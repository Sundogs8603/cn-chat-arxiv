# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Executable Code Actions Elicit Better LLM Agents](https://rss.arxiv.org/abs/2402.01030) | 使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。 |
| [^2] | [ROUTERBENCH: A Benchmark for Multi-LLM Routing System](https://arxiv.org/abs/2403.12031) | 提出了ROUTERBENCH，一个用于评估LLM路由系统性能的基准测试框架，包括超过405k推理结果的数据集，以支持路由策略的开发。 |
| [^3] | [Align and Distill: Unifying and Improving Domain Adaptive Object Detection](https://arxiv.org/abs/2403.12029) | 引入了统一的基准测试和实现框架ALDI以及新的DAOD基准数据集CFC-DAOD，解决了领域自适应目标检测中的基准问题，并支持未来方法的发展。 |
| [^4] | [Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and Detail](https://arxiv.org/abs/2403.12028) | 提出了一种名为“Ultraman”的新方法，实现了从单一图像快速重建带有纹理的3D人体模型，并通过优化的框架提高了重建速度和准确性 |
| [^5] | [From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models](https://arxiv.org/abs/2403.12027) | 近年来，随着大型基础模型的兴起，自动图表理解取得了显著进展，本调查论文概述了在这些基础模型背景下图表理解领域的最新发展、挑战和未来方向 |
| [^6] | [FlexCap: Generating Rich, Localized, and Flexible Captions in Images](https://arxiv.org/abs/2403.12026) | FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。 |
| [^7] | [Supervised Fine-Tuning as Inverse Reinforcement Learning](https://arxiv.org/abs/2403.12017) | 本论文提出将逆强化学习和模仿学习的见解结合，探讨了使用演示数据集对齐大语言模型的方法，并对不同方法的性能进行了分析。 |
| [^8] | [EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents](https://arxiv.org/abs/2403.12014) | EnvGen提出了一种新的框架，利用LLMs的推理能力自适应创建训练环境，帮助小型具身体RL代理在弱点方面学习有用技能。 |
| [^9] | [VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model](https://arxiv.org/abs/2403.12010) | 提出了基于大型视频生成模型的一致性多视角生成框架，通过微调视频生成模型并引入3D感知去噪采样方法来解决多视角图像生成中的数据训练和一致性问题。 |
| [^10] | [Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks](https://arxiv.org/abs/2403.12009) | 本研究通过将图神经网络与胶囊网络相结合，提出了一种创新方法来增强皮肤癌诊断的分类性能。 |
| [^11] | [DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing](https://arxiv.org/abs/2403.12002) | 该方法提出了一种用于零样本视频编辑的新方法，通过匹配原始视频和编辑视频的时空自相似性，在分数蒸馏过程中解决了新内容引入时可能出现的结构和运动偏差问题。 |
| [^12] | [Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance](https://arxiv.org/abs/2403.12000) | Notochord是一种灵活概率模型，能够在实时MIDI表演中生成多音轨MIDI并以低于10毫秒的延迟响应输入，支持多种交互音乐功能。 |
| [^13] | [Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning](https://arxiv.org/abs/2403.11996) | 利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。 |
| [^14] | [Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching](https://arxiv.org/abs/2403.11984) | 本文介绍了一种使用自然语言处理和大型语言模型分析教学评估的新方法，可以从大量反馈中提取、嵌入、聚类和总结SETs。 |
| [^15] | [Informed Spectral Normalized Gaussian Processes for Trajectory Prediction](https://arxiv.org/abs/2403.11966) | 该论文提出了一种用于轨迹预测的新颖正则化持续学习方法，利用通知式先验知识，提高了模型性能和数据效率。 |
| [^16] | [Enhanced Event-Based Video Reconstruction with Motion Compensation](https://arxiv.org/abs/2403.11961) | 通过运动补偿来增强重建质量，提出将输入帧和稀疏编码进行变换，并将流网络与CISTA-LSTC集成，形成CISTA-Flow网络，使系统仅依赖事件。 |
| [^17] | [IVAC-P2L: Enhancing Video Action Counting through Irregular Repetition Priors](https://arxiv.org/abs/2403.11959) | IVAC-P2L提出了一种新颖的视频动作计数方法，通过优化不规则重复模式，实现了在视频中准确计数动作的重要性 |
| [^18] | [Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling](https://arxiv.org/abs/2403.11942) | 该论文通过半监督学习技术生成表情类别伪标签，实现了面部表情识别模型的泛化能力，在面临数据集限制和类别不平衡问题时取得了优异的识别性能。 |
| [^19] | [Tur[k]ingBench: A Challenge Benchmark for Web Agents](https://arxiv.org/abs/2403.11905) | Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。 |
| [^20] | [Larimar: Large Language Models with Episodic Memory Control](https://arxiv.org/abs/2403.11901) | Larimar提出了一种大脑启发的架构，通过分布式情节记忆增强LLMs，实现了动态、一次性的知识更新，无需昂贵的重新训练或微调，且在速度和灵活性上表现出色。 |
| [^21] | [From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?](https://arxiv.org/abs/2403.11894) | 该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。 |
| [^22] | [SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules](https://arxiv.org/abs/2403.11887) | SuperLoRA提出了一个统一且高度灵活的框架，通过引入不同的技巧扩展了不同的LoRA变体，在极少参数情况下特别优异。 |
| [^23] | [QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction](https://arxiv.org/abs/2403.11886) | QueryAgent提出了一种基于环境反馈的自我校正方法ERASER，在语义解析中表现出显著的性能提升和高效性 |
| [^24] | [ReGenNet: Towards Human Action-Reaction Synthesis](https://arxiv.org/abs/2403.11882) | 本文分析了人际互动的非对称、动态、同步和详细性质，并提出了第一个多场景人类行动-反应综合基准。 |
| [^25] | [Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton](https://arxiv.org/abs/2403.11879) | 通过融合技术整合全局背景信息和采用LSTM架构进行时间分析，我们的方法在情感模仿强度预测任务上取得了显着的改进。 |
| [^26] | [Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging](https://arxiv.org/abs/2403.11865) | 本文对神经场景表示在多模态学习中的应用进行了全面评估，并提出了四种不同的策略以将第二模态（非RGB）纳入NeRFs中，通过选择热成像作为第二模态来挑战神经场景表示的整合。 |
| [^27] | [Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay](https://arxiv.org/abs/2403.11852) | 本文提出了一种具有潜在状态推断的强化学习方法，用于解决自动匝道合并问题，在没有详细了解周围车辆意图或驾驶风格的情况下安全执行匝道合并任务，并考虑了观测延迟，以增强代理在动态交通状况中的决策能力。 |
| [^28] | [Fuzzy Rough Choquet Distances for Classification](https://arxiv.org/abs/2403.11843) | 本文提出了一种基于模糊粗糙集合度量的新型Choquet距离，用于捕捉数据中的非线性关系，使得距离度量更加灵活与准确。 |
| [^29] | [Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data](https://arxiv.org/abs/2403.11841) | 提出了一种新的策略学习算法，PESCAL，利用基于前门标准的中介变量消除混杂偏差，并采用悲观原则处理候选策略引起的分布变化。 |
| [^30] | [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838) | 引入Guide-Align，一种两阶段方法，通过安全训练模型识别潜在风险，并制定特定指南，从而建立全面的指导库，用于指导LLMs生成安全和高质量输出。 |
| [^31] | [Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks](https://arxiv.org/abs/2403.11830) | 本文首次提出了针对基于图神经网络的网络入侵检测系统的形式化对抗攻击，并模拟了攻击者在现实场景中执行可行结构攻击所需考虑的问题空间约束。 |
| [^32] | [Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics](https://arxiv.org/abs/2403.11821) | 评估文本到图像合成中，提出了针对图像质量的新评估指标，以确保文本和图像内容的对齐，并提出了新的分类法来归纳这些指标 |
| [^33] | [How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments](https://arxiv.org/abs/2403.11807) | 通过博弈论视角评估LLMs的决策能力，结果表明GPT-3.5在稳健性方面表现良好，但泛化能力有限，而GPT-4则优于其他模型。 |
| [^34] | [Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2403.11793) | 使用抽象和推理语料库（ARC）数据集评估大型语言模型的推理和上下文理解能力，结果显示虽然大型语言模型具有较弱的推理能力，但在逻辑连贯性、组合性和效率方面仍然落后，实验结果有助于提出实现人类水平推理的发展路径。 |
| [^35] | [Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling](https://arxiv.org/abs/2403.11790) | 引入了深度中轴体，一种半隐式表示，通过卷积表面实现形状重建，具有潜力。 |
| [^36] | [Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models](https://arxiv.org/abs/2403.11786) | 使用零样本提示的方法，我们利用OpenAI的GPT-3.5模型从文本中成功提取了超关联知识，尽管精确率较低，但结果显示了未来研究的潜在方向。 |
| [^37] | [Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt](https://arxiv.org/abs/2403.11780) | 提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。 |
| [^38] | [S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention](https://arxiv.org/abs/2403.11772) | 本文介绍了一项关于使用联合嵌入预测架构（JEPAs）实现脑电信号无缝跨数据集转移的探索性研究，提出了Signal-JEPA用于表示脑电记录，并展示了其在精确下游分类中的重要性。 |
| [^39] | [Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs](https://arxiv.org/abs/2403.11755) | 提出了Meta-Prompting for Visual Recognition (MPVR)方法，通过仅需少量信息即可自动化零样本识别中的提示生成过程。 |
| [^40] | [Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$](https://arxiv.org/abs/2403.11734) | 该研究提出了一种参数化版本的关系GNNs，通过在$t$为无穷大时仅使用二次空间的嵌入来近似$3$-GNNs，对于较低的$t$值，通过交换较少的消息实现弱的近似，同时通常产生了几个规划领域中所需的$C_3$特征。 |
| [^41] | [LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images](https://arxiv.org/abs/2403.11703) | 提出了LLaVA-UHD，一个大型多模态模型，通过图像模块化策略、压缩模块和空间模式，可以高效地感知任意长宽比和高分辨率的图像 |
| [^42] | [HDLdebugger: Streamlining HDL debugging with Large Language Models](https://arxiv.org/abs/2403.11671) | 提出了一个LLM辅助的HDL调试框架 HDLdebugger，通过逆向工程方法生成HDL调试数据，提供检索增强生成的搜索引擎以及检索增强LLM微调的方法，以简化HDL调试任务。 |
| [^43] | [Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring](https://arxiv.org/abs/2403.11642) | 通过考虑运行时的时间约束，本研究通过调整基于遗传算法的技术，在预测性流程监控中引入了时间背景知识来生成反事实解释。 |
| [^44] | [QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation](https://arxiv.org/abs/2403.11626) | 提出了一种从四元数视角进行视觉舞蹈生成的四元数增强注意力网络（QEAN），可以更好地学习运动序列和音频序列的特征。 |
| [^45] | [Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits](https://arxiv.org/abs/2403.11598) | 本研究提出了一种基于并行计划的SAT编码，每个时间步骤应用1个SWAP和一组CNOT。利用特定领域信息，在并行计划中保持优化性能的同时，扩展到大型深度电路。我们展示了我们方法的可扩展性，在多项比领先的精确和接近最优方法表现出色（高达100倍）。我们首次能够最优地映射多个8、14和16量子比特 |
| [^46] | [Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines](https://arxiv.org/abs/2403.11585) | Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。 |
| [^47] | [Reinforcement Learning with Token-level Feedback for Controllable Text Generation](https://arxiv.org/abs/2403.11558) | 提出了一种新的强化学习算法TOLE，通过使用标记级别奖励进行文本生成，采用了"先量子化，然后加噪声"的方法来增强算法的鲁棒性。可以在多个约束条件下灵活扩展，避免了过拟合和语义崩溃问题。 |
| [^48] | [LLM^3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning](https://arxiv.org/abs/2403.11552) | LLM^3是一个基于大型语言模型的任务和运动规划框架，利用预训练的LLM具备强大的推理和规划能力，通过接口提出符号动作序列和选择连续动作参数进行运动规划，并通过运动规划的反馈来迭代优化提议，从而简化了处理领域特定消息的设计过程。 |
| [^49] | [OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System](https://arxiv.org/abs/2403.11536) | 引入外部模态引导的数据挖掘框架以解决自动光学检验在工业制造中面临的模型部署挑战和准确性问题。 |
| [^50] | [End-To-End Underwater Video Enhancement: Dataset and Model](https://arxiv.org/abs/2403.11506) | 提出了一个新的端到端水下视频增强模型UVENet，利用帧间关系来提高水下视频的可见性和帧质量，构建了适用于UVE任务的合成数据集SUVE，填补了现有方法中缺乏的监督数据集和模型。 |
| [^51] | [MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning](https://arxiv.org/abs/2403.11504) | MLVICX是一种用于胸部X射线自监督表征学习的方法，通过多级方差和协方差探索策略来捕获丰富的表征。 |
| [^52] | [MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception](https://arxiv.org/abs/2403.11496) | MCD数据集为机器人感知领域带来了全新挑战，包含多种传感模式、高精度地面真值和跨校园的多样化挑战环境，同时还引入了对大规模NRE激光扫描进行的语义注释。 |
| [^53] | [Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics](https://arxiv.org/abs/2403.11495) | 提出了一种名为Toast的新框架，以及增强版DyToast，用于学习路网的通用表示，并增强了时间动态的整合，以提高各种时间敏感下游任务的性能。 |
| [^54] | [SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient Motion Prediction](https://arxiv.org/abs/2403.11492) | SmartRefine提出了一种场景自适应细化策略，可在最小的额外计算量下对运动预测进行细化 |
| [^55] | [Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis](https://arxiv.org/abs/2403.11487) | 提出了一种新方法，利用LLM以及上下文学习，实现了自动生成具身机器人的“行路指示”，并且在多个模拟平台上展示出跨平台特性。 |
| [^56] | [Open-World Semi-Supervised Learning for Node Classification](https://arxiv.org/abs/2403.11483) | 方差不平衡可能对模型性能产生负面影响，提出一种不依赖预训练图编码器的有效方法 |
| [^57] | [Word Order's Impacts: Insights from Reordering and Generation Analysis](https://arxiv.org/abs/2403.11473) | 本文通过顺序重建的角度重新审视了关于词序的假设，并通过选择不同范围的数据集进行了实验证明，ChatGPT依赖于词序进行推断，但不能明确支持或否定词序与词汇语义之间的冗余关系。 |
| [^58] | [Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V](https://arxiv.org/abs/2403.11468) | 通过引入Collage Prompting方法，我们实现了与GPT-4V合作的经济可行的视觉识别方法，通过优化图像排列顺序获得最大的识别准确性。 |
| [^59] | [HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models](https://arxiv.org/abs/2403.11456) | HateCOT数据集通过GPT-3.5-Turbo生成解释，将52,000个样本数据用于预训练模型，显著提升了在不同领域和任务下的攻击性内容检测效果。 |
| [^60] | [Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making](https://arxiv.org/abs/2403.11432) | 本研究致力于研究基于注意力的DRL框架的可解释性，在自主车辆决策中，通过在开源AV仿真环境中添加多头注意力框架，提高了模型的解释性。 |
| [^61] | [Neural network representation of quantum systems](https://arxiv.org/abs/2403.11420) | 利用神经网络的通用逼近定理，我们提出了一个新颖的映射方法，将广泛类的量子力学系统表示为神经网络形式，从而可以对Feynman路径积分中的任意路径进行统计求和。 |
| [^62] | [Variational Sampling of Temporal Trajectories](https://arxiv.org/abs/2403.11418) | 本文介绍了一种通过在函数空间中显式参数化过渡函数来学习轨迹分布的机制，实现了对轨迹的高效合成和推断。 |
| [^63] | [DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation](https://arxiv.org/abs/2403.11415) | DreamSampler框架通过整合反向扩散采样和分数蒸馏，提供了模型无关的图像处理方法，解决了分数蒸馏易崩溃的问题，并在图像编辑和重构中展现了竞争力。 |
| [^64] | [Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT](https://arxiv.org/abs/2403.11402) | 生成AI技术如ChatGPT对网络安全教育具有重大影响，研究强调了大学应对课程进行调整以适应行业需求。 |
| [^65] | [Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning](https://arxiv.org/abs/2403.11401) | 本文介绍了Scene-LLM，一种3D视觉语言模型，通过整合大型语言模型（LLMs）的推理优势，增强了在交互式3D室内环境中具身体存在的代理者能力。 |
| [^66] | [Automated data processing and feature engineering for deep learning and big data applications: a survey](https://arxiv.org/abs/2403.11395) | 现代人工智能方法旨在设计能够直接从数据中学习的算法，自动化数据处理任务的兴起驱动了机器学习和大数据应用中利用大量复杂数据的发展。 |
| [^67] | [Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot](https://arxiv.org/abs/2403.11381) | 本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。 |
| [^68] | [Driving Style Alignment for LLM-powered Driver Agent](https://arxiv.org/abs/2403.11368) | 通过人类驾驶行为演示和反馈，提出了一个多对齐框架，用于将驾驶代理与人类驾驶风格对齐，并通过模拟实验验证了其有效性 |
| [^69] | [IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight](https://arxiv.org/abs/2403.11363) | IGANN Sparse是一种新颖的机器学习模型，通过训练过程中的非线性特征选择促进稀疏性，确保在不影响预测性能的情况下提高模型的可解释性。 |
| [^70] | [Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies](https://arxiv.org/abs/2403.11353) | 该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。 |
| [^71] | [COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits](https://arxiv.org/abs/2403.11348) | 通过概率电路，提出了COLEP框架，实现了可证实鲁棒学习推理一致性预测，其特点在于训练统计模型学习不同语义概念，并利用概率电路实现精确高效推理 |
| [^72] | [CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data](https://arxiv.org/abs/2403.11346) | 提出了CantonMT项目，利用合成反向翻译数据对粤语至英语NMT模型进行微调，并为研究人员提供用户友好的界面和开源工具包，以促进研究 |
| [^73] | [Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective](https://arxiv.org/abs/2403.11345) | 本文从均场视角研究了独立强化学习在合作竞争代理中的应用，提出了一种可实现纳什均衡的线性二次结构RL方法，并通过考虑无限代理数量的情况来解决有限人口环境中的非稳态性问题。 |
| [^74] | [Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction](https://arxiv.org/abs/2403.11337) | 本研究提出了一种基于深度学习的关键点预测框架，通过将关键点预测和视频帧合成结合应用在视频运动传输中，提升了带宽效率。 |
| [^75] | [Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback](https://arxiv.org/abs/2403.11330) | 通过将全局明确标注拆解成本地隐式多模态反馈，提出了一种改进对话代理的方法，并在各种对话度量方面展现出一致的改进 |
| [^76] | [Domain-Guided Masked Autoencoders for Unique Player Identification](https://arxiv.org/abs/2403.11328) | 设计了一种新颖的领域引导遮罩策略d-MAE，用于在存在运动模糊的情况下进行球员识别的稳健特征提取。 |
| [^77] | [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322) | 提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。 |
| [^78] | [Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving](https://arxiv.org/abs/2403.11304) | 该论文提出了一种轻量级等变规划模型，在保证输入空间的旋转平移等变性的同时，将运动预测和轨迹规划结合到一个步骤中，提高了样本效率，并提供了一种高级路线引导方法。 |
| [^79] | [SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://arxiv.org/abs/2403.11299) | 本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。 |
| [^80] | [Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction](https://arxiv.org/abs/2403.11292) | 提出了一种名为GOOD的图神经网络模型，设计用于解决领域外链接预测问题，采用一种新颖的多关系嵌入聚合设计概念。 |
| [^81] | [Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation](https://arxiv.org/abs/2403.11265) | 通过引入合成示例的数据增强方法，可以改善在作者验证任务中对抗性攻击下的分类器预测。 |
| [^82] | [Understanding Diffusion Models by Feynman's Path Integral](https://arxiv.org/abs/2403.11262) | 通过费曼的路径积分引入了扩散模型的新形式，对基于分数的生成模型提供了全面描述，并识别出连接随机和确定性采样方案的插值参数。 |
| [^83] | [A Lie Group Approach to Riemannian Batch Normalization](https://arxiv.org/abs/2403.11261) | 本论文建立了一个Lie群上的统一框架，为黎曼批量归一化（RBN）技术提供了理论保证，并推广了现有的Lie群到对称正定流形上的三类参数化Lie群结构。 |
| [^84] | [A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty](https://arxiv.org/abs/2403.11259) | 通过机器学习模型将用户请求分配给服务器，以解决在移动边缘计算中应用部署问题的二阶段随机规划。 |
| [^85] | [CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations](https://arxiv.org/abs/2403.11220) | 提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能 |
| [^86] | [Causality from Bottom to Top: A Survey](https://arxiv.org/abs/2403.11219) | 本文调查了因果关系在过去五十年中的发展，探讨了因果关系与其他方法的区别，以及其与人工智能等新方法的互动，研究了因果关系对各领域的影响和贡献。 |
| [^87] | [Research on Personal Credit Risk Assessment Methods Based on Causal Inference](https://arxiv.org/abs/2403.11217) | 本文基于范畴论引入新的因果关系定义，演示了因果推断中指标综合的可行性。 |
| [^88] | [MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data](https://arxiv.org/abs/2403.11207) | MindEye2使用共享主题模型，通过仅使用1小时的fMRI训练数据，实现了高质量的fMRI到图像的转换。 |
| [^89] | [Partitioned Neural Network Training via Synthetic Intermediate Labels](https://arxiv.org/abs/2403.11204) | 该研究提出了一种通过将模型分区到不同GPU上，并生成合成中间标签来训练各个部分的方法，以缓解大规模神经网络训练中的内存和计算压力。 |
| [^90] | [Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework](https://arxiv.org/abs/2403.11202) | 提出了一种自动设计数据增强框架，以优化大型语言模型在芯片设计中的应用能力，并解决了Verilog数据匮乏和训练数据准备时间长的问题 |
| [^91] | [Graph Unitary Message Passing](https://arxiv.org/abs/2403.11199) | 提出了一种名为GUMP的图单元消息传递方法，通过应用单元邻接矩阵来缓解图神经网络中的过度压缩问题。 |
| [^92] | [Prior-dependent analysis of posterior sampling reinforcement learning with function approximation](https://arxiv.org/abs/2403.11175) | 该研究提出了首个先验依赖性贝叶斯遗憾上界，并对后验抽样强化学习进行了改进分析，提出了一个新的上界结果，实现了对先前基准的方法论提升。 |
| [^93] | [Correcting misinformation on social media with a large language model](https://arxiv.org/abs/2403.11169) | 提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。 |
| [^94] | [CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion](https://arxiv.org/abs/2403.11162) | 该方法提出了一种通过对比梯度反转实现扩散模型的数字版权认证的新方法，通过利用预训练模型和微调模型之间的概念差异来恢复图像的缺失细节。 |
| [^95] | [Evaluation Ethics of LLMs in Legal Domain](https://arxiv.org/abs/2403.11152) | 评估大型语言模型在法律领域的伦理层面的重要性以确保其有效整合，提出了一种基于法律案例的新颖评估方法，并对大型语言模型的基本语言能力、专业法律知识和法律稳健性进行了全面评估 |
| [^96] | [Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment](https://arxiv.org/abs/2403.11124) | 更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。 |
| [^97] | [PhD: A Prompted Visual Hallucination Evaluation Dataset](https://arxiv.org/abs/2403.11116) | 本研究针对Intrinsic Vision-Language Hallucination（IVL-Hallu）问题进行了深入分析，提出了几种新颖的IVL-Hallu任务，并将其分为四种类型，有助于揭示其产生的原因和反映。 |
| [^98] | [Phasic Diversity Optimization for Population-Based Reinforcement Learning](https://arxiv.org/abs/2403.11114) | 引入了Phasic Diversity Optimization (PDO)算法，采用种群训练框架，将奖励和多样性训练分为不同阶段，并在辅助阶段实现激进的多样性优化。 |
| [^99] | [Self-Supervised Quantization-Aware Knowledge Distillation](https://arxiv.org/abs/2403.11106) | 提出了一种自监督量化感知知识蒸馏(SQAKD)框架，可以在不需要标记的监督情况下，同时最小化全精度和低比特模型之间的KL损失以及量化的离散化误差，从而避免了繁琐的超参数调整和复杂的训练过程。 |
| [^100] | [Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts](https://arxiv.org/abs/2403.11092) | 本研究发现在一个文本到图像模型基准中存在西班牙语、日语和中文中不同程度的翻译错误，提供了纠正，并分析了对基准性能的影响。 |
| [^101] | [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082) | RobustSentEmbed是一个自监督句子嵌入框架，通过对抗对比学习提高了文本表示任务中的泛化性和稳健性，实现了在对抗攻击中的优越表现。 |
| [^102] | [GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment](https://arxiv.org/abs/2403.11075) | GOMA提出了一种面向目标的心智对齐的合作沟通框架，通过最小化智能体心智状态部分之间的不一致性来帮助实现更好的合作。 |
| [^103] | [Audio-Visual Segmentation via Unlabeled Frame Exploitation](https://arxiv.org/abs/2403.11074) | 该研究提出一种通用框架，有效利用邻近帧和远距帧来实现音频-视觉分割 |
| [^104] | [Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping](https://arxiv.org/abs/2403.11073) | Tokensome提出了一种基于染色体标记的创新视觉语言模型，将处理核型问题从传统的视觉感知层升级到认知决策层，通过整合知识图和LLM，显著提升模型的可解释性和促进异常检测。 |
| [^105] | [From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting](https://arxiv.org/abs/2403.11047) | 提出了使用谱图作为时间序列数据的视觉表示，并引入了视觉变换器进行多模态学习，在多个领域的数据集上展示出明显优势。 |
| [^106] | [Regulating Chatbot Output via Inter-Informational Competition](https://arxiv.org/abs/2403.11046) | 本文通过探讨信息间竞争，提出利用信息市场本身作为有效减轻AI聊天机器人输出风险的可能性，并指出监管者在面对新技术不确定性时往往过度谨慎，需重新评估监管策略。 |
| [^107] | [Reward Guided Latent Consistency Distillation](https://arxiv.org/abs/2403.11027) | 该论文提出了一种奖励引导的潜在一致性蒸馏方法，通过在LCD过程中整合奖励模型的反馈，从而有效提高高保真图像生成时的样本质量。 |
| [^108] | [Neuro-Symbolic Video Search](https://arxiv.org/abs/2403.11021) | 提出了一种神经网络符号视频搜索系统，该系统利用视觉-语言模型进行语义理解，并通过状态机和时间逻辑公式对事件的长期演变进行推理，从而实现高效的场景识别。 |
| [^109] | [Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach](https://arxiv.org/abs/2403.11015) | 本文提出了一种基于Zadeh计算与词的新方法，可以有效地在存在巨大不确定性时从时间基因表达数据中稳健地识别吸引子。 |
| [^110] | [DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages](https://arxiv.org/abs/2403.11009) | DIALECTBENCH是第一个面向自然语言处理中的方言、语言变体和密切相关语言的大规模基准测试，为实现对不同语言变体上NLP系统性能的全面评估提供了重要工具。 |
| [^111] | [N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields](https://arxiv.org/abs/2403.10997) | 利用Nested Neural Feature Fields (N2F2) 实现了层次化监督学习，提供了对物理维度或语义维度等不同粒度的场景属性全面和细致的理解。 |
| [^112] | [Edge Private Graph Neural Networks with Singular Value Perturbation](https://arxiv.org/abs/2403.10995) | 提出了一种新的隐私保护GNN训练算法Eclipse，通过观察图结构中邻接矩阵的低秩行为，实现了在保护边缘隐私的同时保持模型良好效用。 |
| [^113] | [Boosting Flow-based Generative Super-Resolution Models via Learned Prior](https://arxiv.org/abs/2403.10988) | 通过引入条件学习的先验知识，成功解决了基于流的超分辨模型中的网格伪影、逆矩阵爆炸和次优结果等问题。 |
| [^114] | [IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning](https://arxiv.org/abs/2403.10984) | 介绍了一种名为\carb 的端到端建模工具，用于在物联网-启用深度学习中精确估算碳足迹，展示了与实际测量值相比最大$\pm21\%$的碳足迹差异。 |
| [^115] | [Enhancing IoT Security Against DDoS Attacks through Federated Learning](https://arxiv.org/abs/2403.10968) | 通过联邦学习，利用物联网设备的集体智慧构建全局模型，保护数据隐私和最小化通信开销，增强物联网对抗DDoS攻击的安全性。 |
| [^116] | [Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization](https://arxiv.org/abs/2403.10967) | 学习上下文世界模型有助于提高在未知上下文下的零样点泛化能力。 |
| [^117] | [SelfIE: Self-Interpretation of Large Language Model Embeddings](https://arxiv.org/abs/2403.10949) | 提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。 |
| [^118] | [Human Centered AI for Indian Legal Text Analytics](https://arxiv.org/abs/2403.10944) | 该论文探讨了在法律文本分析中整合人类专业知识以显著提高大型语言模型性能的潜力，并介绍了一个以人为中心的复合人工智能系统。 |
| [^119] | [Inducing Individual Students' Learning Strategies through Homomorphic POMDPs](https://arxiv.org/abs/2403.10930) | 提出同态POMDP（H-POMDP）模型以适应多种认知模式，并通过参数学习方法自动构建模型，以实现诱导更个性化的学习策略 |
| [^120] | [Interpretable Machine Learning for TabPFN](https://arxiv.org/abs/2403.10923) | TabPFN模型在低数据情况下实现了良好的分类性能，并能够以秒级速度生成后验预测分布，我们提出了几种专为TabPFN设计的可解释性方法的改进，实现了更高效的计算。 |
| [^121] | [DTOR: Decision Tree Outlier Regressor to explain anomalies](https://arxiv.org/abs/2403.10903) | DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。 |
| [^122] | [Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean](https://arxiv.org/abs/2403.10882) | 该研究提出了三种策略来增强基于公开可用MLLMs的资源较少的语言的性能，包括扩展词汇、双语数据预训练和指导微调。 |
| [^123] | [stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation](https://arxiv.org/abs/2403.10863) | stMCDI是一种新颖的条件扩散模型，通过利用空间定位转录组数据中的空间位置信息来填补缺失值，同时保持整体数据分布。 |
| [^124] | [Efficient Domain Adaptation for Endoscopic Visual Odometry](https://arxiv.org/abs/2403.10860) | 这项工作提出了一个高效的内窥镜视觉里程计神经风格迁移框架，将从术前规划到测试阶段的时间缩短至不到五分钟，通过利用有限数量的真实图像和术前先验信息进行训练，以及引入测试时间自适应方法来减小训练和测试之间的光照条件差距。 |
| [^125] | [Just Say the Name: Online Continual Learning with Category Names Only via Data Generation](https://arxiv.org/abs/2403.10853) | 提出了在线连续学习框架G-NoCL，采用生成数据并利用DIverSity和COmplexity enhancing ensemBlER（DISCOBER）进行数据融合，展示了其在在线连续学习基准测试中的优越性能。 |
| [^126] | [GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments](https://arxiv.org/abs/2403.10850) | GAgent 基于视觉语言模型的自适应刚柔握持Agent在复杂照明环境下提供了先进的认知能力，包括Prompt Engineer模块、VLM核心和Workflow模块，同时具备可变硬度软手持器，对物体和材料进行识别和准确抓取，为无人机等场景带来潜在好处。 |
| [^127] | [Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process](https://arxiv.org/abs/2403.10842) | 本研究提出一种新颖的双Transformer模型，结合门控动态可学习注意机制，用于田纳西伊斯曼过程的故障检测与诊断，提高性能通过独立处理输入数据和提取多样化信息，以及动态学习适应性调整注意策略。 |
| [^128] | [SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation](https://arxiv.org/abs/2403.10834) | SF(DA)$^2$是一种新颖的无源域自适应方法，通过在特征空间中构建增强图并利用谱邻域聚类来识别分区，实现了数据增强的好处而避免了挑战。 |
| [^129] | [LookALike: Human Mimicry based collaborative decision making](https://arxiv.org/abs/2403.10824) | 提出了一种新颖的方法，通过在LLM代理之间进行知识蒸馏，实现实时的人类角色扮演，保留独特上下文，而不依赖任何存储数据或预训练，并展示出在模拟真实世界任务中表现优于现有技术。 |
| [^130] | [VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis](https://arxiv.org/abs/2403.10823) | VisionCLIP利用Med-AIGC生成的合成数据，并结合自然语言描述，建立了一种伦理语言-图像模型，在视网膜图像分析中取得了竞争性表现。 |
| [^131] | [Incentivized Exploration of Non-Stationary Stochastic Bandits](https://arxiv.org/abs/2403.10819) | 提出了针对非平稳随机赌博机的激励探索算法，实现了随时间的子线性遗憾和补偿 |
| [^132] | [Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference](https://arxiv.org/abs/2403.10805) | 提出了一种新颖的端到端生成模型 Persona-Gestor，通过模糊特征提取器和非自回归自适应层归一化变压器扩散架构，仅依赖原始语音音频生成高度个性化的3D全身手势。 |
| [^133] | [Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion](https://arxiv.org/abs/2403.10803) | 提出了一种名为MLOD的新框架，利用多测试过程在不同级别的特征中识别测试样本中的分布偏移，无需修改预训练模型结构。 |
| [^134] | [Efficient Pruning of Large Language Model with Adaptive Estimation Fusion](https://arxiv.org/abs/2403.10799) | 提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。 |
| [^135] | [From Words to Routes: Applying Large Language Models to Vehicle Routing](https://arxiv.org/abs/2403.10795) | 该研究探索了应用大型语言模型解决车辆路径规划问题的能力，提出了基于自然语言任务描述的基本提示范例并提出一种使模型进行改进的框架。 |
| [^136] | [Time Series Representation Learning with Supervised Contrastive Temporal Transformer](https://arxiv.org/abs/2403.10787) | 提出了一种名为SCOTT的具有监督对比变换器的时间序列表示学习模型，结合了Transformer和Temporal Convolutional Networks以学习全局和局部特征，并简化了用于标记时间序列数据的监督对比损失。 |
| [^137] | [Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings](https://arxiv.org/abs/2403.10781) | 本文研究了如何利用最先进的语言模型来理解和生成中国幽默，特别是关于训练模型生成典故谚语。他们采用新颖的fine-tuning方法，包含融合拼音嵌入和对比学习，从而成功生成幽默性典故谚语。 |
| [^138] | [Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation](https://arxiv.org/abs/2403.10780) | 提出了一种新颖的域不变的Real-to-Simulation微调策略，以实现多类多实例分割中Segment Any Object Model（SAOM）的“一切”模式工作，并在室内场景理解中发挥关键作用。 |
| [^139] | [From Melting Pots to Misrepresentations: Exploring Harms in Generative AI](https://arxiv.org/abs/2403.10776) | 探索生成式AI中的社会有害影响，提出对多样性和公平性的关切，并引领讨论在这些模型中的重要性，同时提出未来研究方向。 |
| [^140] | [ECRC: Emotion-Causality Recognition in Korean Conversation for GCN](https://arxiv.org/abs/2403.10764) | 本研究提出了ECRC模型，通过结合单词级和句子级嵌入，以及基于新颖图结构的方法，在韩国会话环境中进行情绪因果识别研究。 |
| [^141] | [Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning](https://arxiv.org/abs/2403.10761) | 本文提出了一个通过深度强化学习对无人机和移动充电器进行调度的方法，以解决无人机在观察任务中的航线规划和充电优化问题。 |
| [^142] | [LIGHTCODE: Light Analytical and Neural Codes for Channels with Feedback](https://arxiv.org/abs/2403.10751) | 本文提出了一种LIGHTCODE轻量级神经编码方案，在具备解释性的基础上，在低信噪比区域实现了最先进的可靠性。 |
| [^143] | [Depression Detection on Social Media with Large Language Models](https://arxiv.org/abs/2403.10750) | 提出了名为DORIS的新型抑郁症检测系统，将医学知识和大语言模型的最新进展相结合，通过分析个人在社交媒体上的帖子历史记录来确定抑郁症患者，以提高早期检测和干预。 |
| [^144] | [Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control](https://arxiv.org/abs/2403.10744) | 提出了一种新颖的流行病政策制定模型-政策组合综合（PCS）模型，通过引入对抗学习来避免极端决策，提高政策的人性化，同时最小化次优历史政策对决策模型训练的影响。 |
| [^145] | [Variance-Dependent Regret Bounds for Non-stationary Linear Bandits](https://arxiv.org/abs/2403.10732) | 提出利用奖励分布方差和变化预算的算法，可以实现更紧的遗憾上限界限。 |
| [^146] | [Strict Partitioning for Sporadic Rigid Gang Tasks](https://arxiv.org/abs/2403.10726) | 提出了一种新的严格分区调度策略，用于零星刚性流式任务，通过创建不相交的任务和处理器分区，并尝试将相似容量的任务分配给同一分区，以减少干扰。 |
| [^147] | [Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies](https://arxiv.org/abs/2403.10720) | 本研究通过实现和评估两个MCTS算法变体，发现达芬奇密码棋盘游戏中的分支分歧明显阻碍了GPU执行时的并行性。 |
| [^148] | [Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency](https://arxiv.org/abs/2403.10717) | 通过利用比例预测一致性技术，本研究提出了一种在被毒化数据集中自动识别后门数据的方法，无需额外干净数据或手动定义后门检测阈值。 |
| [^149] | [Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns](https://arxiv.org/abs/2403.10707) | 本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。 |
| [^150] | [PERL: Parameter Efficient Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2403.10704) | 使用低秩适应（LoRA）方法进行参数高效强化学习（PERL），能够在与传统RLHF设置相当的性能下，实现更快的训练和更少的内存占用。 |
| [^151] | [Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation](https://arxiv.org/abs/2403.10700) | 提出了一个新的基准数据集，首次引入了各种类型的指令错误，考虑到潜在的人类原因，以评估连续环境中 VLN 系统的健壮性 |
| [^152] | [Robust Influence-based Training Methods for Noisy Brain MRI](https://arxiv.org/abs/2403.10698) | 该研究提出了两种基于影响函数的稳健训练方法，针对噪声MRI图像进行脑部肿瘤分类，可提高模型的鲁棒性。 |
| [^153] | [EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning](https://arxiv.org/abs/2403.10692) | 提出了一种名为EXPLORER的探索引导推理代理，用于文本强化学习，能够解决智能体在多个游戏中泛化并在已知和未知对象上表现良好的关键挑战。 |
| [^154] | [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://arxiv.org/abs/2403.10691) | MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。 |
| [^155] | [AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs](https://arxiv.org/abs/2403.10686) | AutoHLS提出了一个集成深度神经网络和贝叶斯优化的框架，用于加速HLS硬件设计优化，实现高达70倍的探索时间加速 |
| [^156] | [Improved discrete particle swarm optimization using Bee Algorithm and multi-parent crossover method (Case study: Allocation problem and benchmark functions)](https://arxiv.org/abs/2403.10684) | 本文提出了一种结合了蜂群算法和多父代交叉的离散粒子群优化算法，通过利用观察者蜜蜂进行独立且密集的邻域搜索来改进算法效率。 |
| [^157] | [Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond](https://arxiv.org/abs/2403.10667) | 本文旨在建立一个统一的多模态个性化系统(UniMP)，有效利用多模态数据同时消除与任务和模态特定定制相关的复杂性。 |
| [^158] | [Limits of Approximating the Median Treatment Effect](https://arxiv.org/abs/2403.10618) | 估计中位数差异比估计中位数处理效应更容易的问题是因果推断的基本问题 |
| [^159] | [SurvRNC: Learning Ordered Representations for Survival Prediction using Rank-N-Contrast](https://arxiv.org/abs/2403.10603) | 提出了SurvRNC方法，通过引入损失函数作为正则化器来获得基于生存时间的有序表示，能处理被截尾数据，并可整合到任何生存模型中。 |
| [^160] | [Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems](https://arxiv.org/abs/2403.10596) | 这项研究介绍了一种在人工智能系统中模拟受控神经退行的新方法，通过在LLMs中添加噪音或切除神经元来逐渐降低其性能，在智商测试中展示了神经退行的过程，并与传统的计算机视觉领域不同，是首个使用文本数据模拟神经退行的工作。 |
| [^161] | [S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document](https://arxiv.org/abs/2403.10588) | S3LLM是一个基于LLM的框架，利用生成式人工智能技术，通过用户友好的界面以交互式、对话式的方式，使用户能够检查大规模科学软件的源代码、代码元数据和摘要信息以及文本技术报告，并通过转换自然语言查询为领域特定语言查询来增强代码分析。 |
| [^162] | [From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction](https://arxiv.org/abs/2403.10586) | 机器学习技术在非肌层侵袭性膀胱癌复发预测中具有潜在作用，可以提高准确性，降低治疗成本，并有效规划治疗方案 |
| [^163] | [Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint](https://arxiv.org/abs/2403.10585) | 提出了一种通过后验抽样解决一般性噪声逆问题的方法，采用策略梯度视角，通过扩散策略梯度（DPG）精确估计指导评分函数，实现对多种线性和非线性逆任务的鲁棒性解决，提高了图像恢复质量。 |
| [^164] | [Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction](https://arxiv.org/abs/2403.10581) | 提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。 |
| [^165] | [Exploring Language Model's Code Generation Ability with Auxiliary Functions](https://arxiv.org/abs/2403.10575) | 在这项研究中，我们全面评估了最近代码预训练语言模型中编码的辅助函数利用能力，通过设计实验，并通过实现风格分析，我们发现了模型利用辅助函数的很有前景。 |
| [^166] | [Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare](https://arxiv.org/abs/2403.10570) | 博弈论模型和基础模型在分析、设计和实施网络欺骗策略中发挥关键作用，为提升主动和自动化网络防御机制提供了新思路。 |
| [^167] | [Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment](https://arxiv.org/abs/2403.10569) | 通过在Xception上实施高效的参数缩减策略，该研究在资源受限的边缘环境中实现了DNN的帕累托最优性，提高了模型的准确性，减少了内存利用，且在Caltech-101图像分类中表现优于原始Xception和轻量级模型。 |
| [^168] | [MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts](https://arxiv.org/abs/2403.10568) | 本文提出了MoPE技术，通过解开提示以自适应捕获数据集级和实例级特征，引入了混合Prompt专家来增强表达能力，并且在多模态融合中表现出更大的表达能力和可扩展性。 |
| [^169] | [Cooling-Guide Diffusion Model for Battery Cell Arrangement](https://arxiv.org/abs/2403.10566) | 导入冷却引导扩散模型的生成式AI方法优化了电池单元布局，显著降低了单元的最高温度，并在冷却效率方面具有独特优势。 |
| [^170] | [PTSD-MDNN : Fusion tardive de r\'eseaux de neurones profonds multimodaux pour la d\'etection du trouble de stress post-traumatique](https://arxiv.org/abs/2403.10565) | PTSD-MDNN通过融合多模态深度神经网络，提供了一种更客观、更快速的方法来检测创伤后应激障碍。 |
| [^171] | [Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks](https://arxiv.org/abs/2403.10562) | 无状态策略通过评估反样本对抗黑盒查询，有效引入了防御者有利的不对称性。这种防御方法能够欺骗攻击者寻找对抗性示例、保持模型在合法输入上的准确性，且适用于多种攻击类型。 |
| [^172] | [A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024](https://arxiv.org/abs/2403.10561) | AAAI 2024年人本主义表示学习研讨会的被接受论文集合。部分论文选择退出。 |
| [^173] | [Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI](https://arxiv.org/abs/2403.10559) | 生成模型与联网自动驾驶车辆的整合有望提升自动车辆的预测建模、模拟精度和决策流程，对交通行业的安全和创新具有潜在推动作用。 |
| [^174] | [Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models](https://arxiv.org/abs/2403.10557) | 本论文通过二阶信息（Hessian）的视角重新审视了大型语言模型的机器遗忘问题，提出了遗忘算法，具有较高的鲁棒性。 |
| [^175] | [KARINA: An Efficient Deep Learning Model for Global Weather Forecast](https://arxiv.org/abs/2403.10555) | KARINA模型通过结合ConvNext、SENet和Geocyclic填充，在2.5°分辨率下提升天气预测准确性，只需较少的计算资源，展示出与更高分辨率模型相当的预测精度。 |
| [^176] | [Learning to Watermark LLM-generated Text via Reinforcement Learning](https://arxiv.org/abs/2403.10553) | 通过将信号嵌入LLM的权重中，我们设计一种模型级水印，有效追踪生成文本的滥用情况，并提出基于强化学习的协同训练框架，使水印更准确、更稳健且更适应新攻击。 |
| [^177] | [Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer](https://arxiv.org/abs/2403.10552) | 通过师生无数据知识传输，实现在未知陌生地点上训练自定位模型，不仅可以处理各种类型的开放式老师，还有效避免依赖于师生私人数据可用性。 |
| [^178] | [Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows](https://arxiv.org/abs/2403.10550) | 通过生成伪异常样本和使用双向归一化流模块，实现对异常数据的半监督检测。 |
| [^179] | [Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing](https://arxiv.org/abs/2403.10547) | 研究了在强污染模型中寻找SOSP的问题，提出了一般框架以\emph{独立于维度}的精度保证高效地找到近似SOSP，具有对抗异常值的鲁棒性，同时将该框架应用于低秩矩阵感知问题，发展了能够容忍数据破坏的高效且可证明鲁棒性的算法。 |
| [^180] | [Process-Aware Analysis of Treatment Paths in Heart Failure Patients: A Case Study](https://arxiv.org/abs/2403.10544) | 针对心力衰竭患者的治疗路径，利用流程挖掘技术对稀疏数据进行分析，探究是否能够回答多个研究问题。 |
| [^181] | [MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications](https://arxiv.org/abs/2403.10538) | 该论文介绍了MATADOR，一个用于边缘应用的自动化片上Tsetlin机设计生成系统，实现了将ML模型转换为SoC-FPGA解决方案的高效方法。 |
| [^182] | [VISREAS: Complex Visual Reasoning with Unanswerable Questions](https://arxiv.org/abs/2403.10534) | 提出了新的组合视觉问答数据集VISREAS，通过验证问题在回答之前的有效性，解决了用户提供不完美指示的挑战 |
| [^183] | [PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency](https://arxiv.org/abs/2403.09732) | 提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。 |
| [^184] | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力 |
| [^185] | [Optimistic Verifiable Training by Controlling Hardware Nondeterminism](https://arxiv.org/abs/2403.09603) | 提出了一种方法，结合了在比目标模型更高精度下进行训练、在中间计算步骤后进行四舍五入，并基于自适应阈值存储四舍五入决策，以应对硬件非确定性对训练过程的影响。 |
| [^186] | [Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption](https://arxiv.org/abs/2403.09404) | 通过创新性实验，我们揭示了人工智能系统在精度最大化和努力减少之间的权衡，以及在详尽的逻辑处理和使用认知快捷方式（启发式）之间转换的条件，并区分了启发式的工具性使用和模仿吸收两种方式。 |
| [^187] | [LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection](https://arxiv.org/abs/2403.09209) | 该论文的贡献是提出了一个名为LAN的框架，能够实时在活动级别进行内部威胁检测，并学习活动序列内的时间依赖关系和活动之间的关系。 |
| [^188] | [AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning](https://arxiv.org/abs/2403.09113) | AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。 |
| [^189] | [Bugs in Large Language Models Generated Code](https://arxiv.org/abs/2403.08937) | 本文研究了使用三种主要LLM生成的代码中收集的333个缺陷样本，并识别了10种独特的错误模式。 |
| [^190] | [Fast Inference of Removal-Based Node Influence](https://arxiv.org/abs/2403.08333) | 提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。 |
| [^191] | [Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale](https://arxiv.org/abs/2403.08293) | GPST是一种无监督的句法语言模型，通过联合训练两个模型实现对原始文本的高并行预训练，克服了之前SLM依赖于黄金树和顺序训练的限制，展示了在多个任务中优于同等规模的GPT-2。 |
| [^192] | [Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models](https://arxiv.org/abs/2403.08281) | 通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。 |
| [^193] | [Deep Submodular Peripteral Network](https://arxiv.org/abs/2403.08199) | 引入了深度子模逆点网络（DSPNs），并提出了一种使用对比学习启发的GPC-ready策略进行训练的方法，以应对子模函数学习中的两大挑战。 |
| [^194] | [Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2403.07440) | 该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。 |
| [^195] | [A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees](https://arxiv.org/abs/2403.07363) | 提出了一种新的直觉模糊决策树随机森林集成方法，融合了随机性、模糊逻辑和模糊集的灵活性，以及多分类器系统的稳健性 |
| [^196] | [Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning](https://arxiv.org/abs/2403.06880) | 研究探讨了幼儿启发的奖励转换如何影响强化学习任务的样本效率和成功率，特别是发现了幼儿启发的稀疏转密集（S2D）转换的有效性。 |
| [^197] | [ALaRM: Align Language Models via Hierarchical Rewards Modeling](https://arxiv.org/abs/2403.06754) | ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。 |
| [^198] | [RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion](https://arxiv.org/abs/2403.06095) | RepoHyper提出了Repo级语义图（RSG）和Expand和Refine检索方法，显著优于现有技术，满足仓库级代码补全的需求 |
| [^199] | [Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations](https://arxiv.org/abs/2403.05828) | 混合量子-经典工作流架构QCQ实现了量子模拟中的创新，通过在QPUs上运行VQE算法和在经典硬件上进行量子态分类，结合了cuQuantum SDK和PennyLane's Lightning plugin，为材料和凝聚态物理领域的计算挑战提供了解决方案。 |
| [^200] | [MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process](https://arxiv.org/abs/2403.05751) | 提出了一种新颖的MG-TSD模型，利用数据内在粒度水平作为目标来引导学习过程，实现了状态-of-the-art的预测性能 |
| [^201] | [DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception](https://arxiv.org/abs/2403.05050) | DyRoNet采用低秩动态路由并结合分支网络优化流媒体感知性能，为多种分支选择策略设定了新的性能标杆 |
| [^202] | [Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models](https://arxiv.org/abs/2403.04325) | 引入了Composition Score，一种基于模型的度量标准，用于量化句子理解中的含义合成程度，实验证明这一度量与大脑区域相关，揭示了含义合成在人类句子理解中的多方面性。 |
| [^203] | [Effectiveness Assessment of Recent Large Vision-Language Models](https://arxiv.org/abs/2403.04306) | 本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。 |
| [^204] | [ProMISe: Promptable Medical Image Segmentation using SAM](https://arxiv.org/abs/2403.04164) | 本文提出了一个自动提示模块（APM），为SAM基础模型提供了在目标领域中使用的自适应提示，显著提高了SAM在医学图像分割中的性能。 |
| [^205] | [Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning](https://arxiv.org/abs/2403.03835) | Cobweb是一种类似人类类别学习系统，采用类别效用度量构建分层组织的类似树状结构，能够捕捉心理效应并在单一模型中展现出实例和原型学习的灵活性，为将来研究人类类别学习提供了基础。 |
| [^206] | [DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models](https://arxiv.org/abs/2403.03768) | DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。 |
| [^207] | [Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification](https://arxiv.org/abs/2403.01827) | 本研究引入了一种新颖的双存储器 RC 系统，通过集成不同类型的 memristor，并在处理时间数据分类任务中取得了显著成效。 |
| [^208] | [Fast Low-parameter Video Activity Localization in Collaborative Learning Environments](https://arxiv.org/abs/2403.01281) | 该论文提出了一种快速低参数视频活动定位系统，可在有限数据集上进行训练，并能准确检测并关联学生在现实教室视频中执行的活动。 |
| [^209] | [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://arxiv.org/abs/2403.01232) | Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。 |
| [^210] | [ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies](https://arxiv.org/abs/2403.01139) | 设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。 |
| [^211] | [ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework](https://arxiv.org/abs/2403.00781) | 这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。 |
| [^212] | [Boosting Neural Representations for Videos with a Conditional Decoder](https://arxiv.org/abs/2402.18152) | 引入条件解码器和NeRV-like模块的增强框架，以提高隐式视频表示方法的效果。 |
| [^213] | [Learning-Based Algorithms for Graph Searching Problems](https://arxiv.org/abs/2402.17736) | 本研究提出了针对未知图的图搜索问题的基于学习的算法，首次在未知加权图上建立了形式保证，并设计算法在预测误差上具有最优或几乎最佳依存关系。 |
| [^214] | [Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>](https://arxiv.org/abs/2402.17527) | 评估语言模型在预测下一个词时，是否能够复现人类在这项任务中展示的语言变化性 |
| [^215] | [Deep Homography Estimation for Visual Place Recognition](https://arxiv.org/abs/2402.16086) | 提出了一种基于Transformer的深度单应性估计网络，用于快速和可学习的几何验证。 |
| [^216] | [OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2402.15321) | 提供了OpenSUN3D研讨会上针对开放词汇3D场景理解的挑战概述，包括挑战数据集、评估方法和获胜方法的简要描述 |
| [^217] | [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899) | 该研究评估了多模态LLMs在采用串联推理时的对抗鲁棒性，发现串联推理在一定程度上提高了对抗性鲁棒性，但引入了一种新的停止推理攻击技术成功规避了这种增强。 |
| [^218] | [Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition](https://arxiv.org/abs/2402.14505) | 提出了一种新颖的方法，实现了预训练模型对视觉地点识别的无缝适应 |
| [^219] | [Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph](https://arxiv.org/abs/2402.14424) | 利用大型语言模型和因果图结合的方法，在心理学假设生成中取得了突破，结果显示这种联合方法在新颖性方面明显优于仅使用大型语言模型的假设。 |
| [^220] | [Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving](https://arxiv.org/abs/2402.13602) | 大型语言模型在自动驾驶中的混合推理能力可以通过分析数据、理解规则和法则、提供语境等方式提高自动驶驶的决策能力。 |
| [^221] | [Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware](https://arxiv.org/abs/2402.11780) | 本文提出了CiMNet，一个旨在联合优化深度神经网络架构和配置的框架，以解决计算存储硬件构建中的挑战。 |
| [^222] | [Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2402.11702) | 大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。 |
| [^223] | [HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments](https://arxiv.org/abs/2402.10228) | HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。 |
| [^224] | [Revisiting Recurrent Reinforcement Learning with Memory Monoids](https://arxiv.org/abs/2402.09900) | 这篇论文重新审视了使用内存单子的循环强化学习方法。通过定义新颖的内存单子框架并提出一种新的批处理方法，改进了样本效率、增加了回报并简化了实现过程。 |
| [^225] | [Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2402.05808) | 本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。 |
| [^226] | [Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations](https://arxiv.org/abs/2402.05713) | 该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。 |
| [^227] | [S-Agents: self-organizing agents in open-ended environment](https://arxiv.org/abs/2402.04578) | S-Agents是一个自组织代理系统，通过引入代理树结构、沙漏代理架构和非阻塞协作方法，实现了在无限环境中高效协调代理的能力，提供了优化协作效率和灵活性的解决方案。 |
| [^228] | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544) | LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。 |
| [^229] | [LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization](https://arxiv.org/abs/2401.17500) | LeTO 是一种通过可微分轨迹优化实现受限视觉运动策略的学习方法，它将优化层表示为轨迹优化问题，使模型能以安全可控的方式端到端生成动作。通过引入约束信息，实现了平衡满足约束、平滑轨迹和最小化演示误差的训练目标。在仿真和实际机器人中进行了评估，表明LeTO方法在成功率上与最先进的模仿学习方法相当。 |
| [^230] | [CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2401.11944) | CMMMU是一个旨在评估大型多模型模型在大学级学科知识和深思熟虑推理任务中表现的中文大规模多学科多模态理解基准，为填补在非英语环境中评估先进知识和推理能力的空白而设计。 |
| [^231] | [FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models](https://arxiv.org/abs/2312.08459) | FaceTalk是一种生成方法，利用神经参数头部模型的潜在空间和潜在扩散模型，从输入音频信号中合成高保真3D运动序列，是首个提出这种方法用于合成逼真和高质量运动序列的工作。 |
| [^232] | [Redefining Developer Assistance: Through Large Language Models in Software Ecosystem](https://arxiv.org/abs/2312.05626) | 本研究介绍了一种通过指导调整开发的DevAssistLlama模型，在处理软件相关自然语言查询时表现出优异能力，突出了专门LLM在软件开发中的潜力。 |
| [^233] | [Prompting in Autoregressive Large Language Models](https://arxiv.org/abs/2312.03740) | LLMs通过创新的提示技术实现了预训练和提示范式的转变，以大大提高下游NLP任务的效果。 |
| [^234] | [DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer](https://arxiv.org/abs/2312.03724) | 提出了差分私密离线提示调整（DP-OPT）解决大型语言模型调整提示时的隐私问题，通过在客户端调整提示并应用于云模型实现了隐私保护和数据传输 |
| [^235] | [SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes](https://arxiv.org/abs/2312.03297) | SoftMAC提出了一个不同于以往的可微仿真框架，能够将软体、关节刚体和衣物耦合在一起，并采用基于预测的接触模型和穿透追踪算法，有效地减少了穿透现象。 |
| [^236] | [Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation](https://arxiv.org/abs/2312.03003) | MobileGPT是一种创新的基于LLM的移动任务自动化工具，通过类人应用记忆模拟人类与移动应用的认知过程，实现任务程序的精确高效学习。 |
| [^237] | [Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/abs/2312.00029) | Bergeron提出了一个基于良知的对齆框架，能够提高大型语言模型对抗攻击的鲁棒性，无需额外参数微调。 |
| [^238] | [Dataset Distillation via the Wasserstein Metric](https://arxiv.org/abs/2311.18531) | 通过引入Wasserstein距离及其重心，我们提出一种有效的数据集精炼方法，利用先验知识提高分布匹配效果，实现了新的最先进性能。 |
| [^239] | [Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching](https://arxiv.org/abs/2311.17950) | 本文提出了广义匹配的概念，并在此基础上提出了Generalized Various Backbone and Statistical Matching (G-VBSM)方法，可以创建一个具有丰富信息和更好概括能力的压缩数据集。 |
| [^240] | [Deep Regularized Compound Gaussian Network for Solving Linear Inverse Problems](https://arxiv.org/abs/2311.17248) | 提出了两种允许在复合高斯分布类中进行问题特定统计先验选择的线性逆问题新方法，一种是迭代算法广义复合高斯最小二乘，另一种是通过展开得到的新颖深度正则化神经网络DR-CG-Net。 |
| [^241] | [Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention](https://arxiv.org/abs/2311.16834) | 该论文提出了一种新颖的模块化神经网络模型，用于多变量时间序列预测，通过循环神经网络学习时间依赖关系，并使用基于注意力的特征选择组件实现解释性，实验结果表明其优于当前最先进的解释性神经加性模型（NAM）及其变体。 |
| [^242] | [UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition](https://arxiv.org/abs/2311.15599) | 提出了四项用于设计大卷积神经网络的架构指南，并展示了在多模态领域具有通用感知能力 |
| [^243] | [Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning](https://arxiv.org/abs/2311.15487) | 通过几何调整的梯度下降，在深度学习中以均匀指数速率实现全局$\mathcal{L}^2$最小化，这一方法在过参数化情况下具有明确自然的不变几何含义。 |
| [^244] | [Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification](https://arxiv.org/abs/2311.10319) | 引入了S4MI流程，利用自监督和半监督学习的高效方法，能够简化医学图像的机器监督过程，自监督学习在分类任务中表现明显优于监督方法。 |
| [^245] | [Inherently Interpretable Time Series Classification via Multiple Instance Learning](https://arxiv.org/abs/2311.10049) | 通过多例学习提出了一个新框架MILLET，使现有的深度学习时间序列分类模型变得内在可解释，同时不影响甚至改进预测性能，并在多个数据集上展示出比其他方法更高质量的稀疏解释。 |
| [^246] | [PubDef: Defending Against Transfer Attacks From Public Models](https://arxiv.org/abs/2310.17645) | 本文提出了一个新的实用威胁模型，其中对手依赖于通过公开可用的替代模型的迁移攻击，提出了一种基于博弈论视角的专门防御方法，并在多个公共模型和攻击算法下进行了评估。 |
| [^247] | [Value of Assistance for Grasping](https://arxiv.org/abs/2310.14402) | 提出了一种新颖的协助价值（VOA）度量，用于评估特定观测对机器人完成抓取任务的预期影响，通过模拟和真实环境验证了该度量的有效性。 |
| [^248] | [Llemma: An Open Language Model For Mathematics](https://arxiv.org/abs/2310.10631) | Llemma是一个用于数学的开放语言模型，在MATH基准测试中表现优异，能够进行工具使用和形式定理证明，无需进一步微调。 |
| [^249] | [Spike-based Neuromorphic Computing for Next-Generation Computer Vision](https://arxiv.org/abs/2310.09692) | 基于脉冲的神经形态计算作为对今天主导视觉领域的深度卷积神经网络的可行替代，承诺能够实现数量级的能量效率提升。 |
| [^250] | [WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions](https://arxiv.org/abs/2310.05556) | 提出了一种WeatherDepth模型，通过课程对比学习，逐步适应恶劣天气条件下深度估计需求，并通过逐渐抓住有益深度线索对抗天气影响，实现更好的领域适应。 |
| [^251] | [Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model](https://arxiv.org/abs/2310.05155) | 提出了Toolink，一个通过链式解决方法首先创建工具包，再集成工具规划和调用的框架，成功通过对ChatGPT和CoS-GPT的实验，打造了LLaMA-CoS，一个具有先进工具规划和调用能力的强大开源模型。 |
| [^252] | [Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations](https://arxiv.org/abs/2310.04566) | 本论文介绍了一种自监督学习框架，利用Transformer神经网络使机器人能够从整齐排列的示范中理解和复制整洁的概念，从而实现整理物品的功能。 |
| [^253] | [Can LLM-Generated Misinformation Be Detected?](https://arxiv.org/abs/2309.13788) | LLM生成的虚假信息可能比人类撰写的虚假信息更难以检测，具有更具欺骗性的风格，可能造成更多危害。 |
| [^254] | [LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech](https://arxiv.org/abs/2309.05472) | LeBenchmark 2.0是一个开源框架，用于评估和构建法语语音技术的自监督学习，提供大规模语料库、预训练模型和评估协议，并探讨了预训练SSL模型的独特视角。 |
| [^255] | [MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators](https://arxiv.org/abs/2306.10900) | MotionGPT是一款能够利用多模态控制信号生成连续人类动作的通用运动生成器。 |
| [^256] | [Distributional Reinforcement Learning with Dual Expectile-Quantile Regression](https://arxiv.org/abs/2305.16877) | 提出了一种用双期望分位回归的分布式强化学习方法，能够更高效地学习任意回报分布 |
| [^257] | [Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models](https://arxiv.org/abs/2305.12544) | 本文提出了14个不由大型语言模型直接解决的自然语言处理研究领域，包含45个新的研究方向，为NLP研究人员提供了丰富的探索方向。 |
| [^258] | [Provably Bounding Neural Network Preimages](https://arxiv.org/abs/2302.01404) | 提出了INVPROP算法用于验证神经网络输出集的前像上的属性，结合分支界限以增加精度，并且实现了GPU加速，避免了线性规划求解器的需求。 |
| [^259] | [Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning](https://arxiv.org/abs/2211.08559) | 使用跨域自监督深度学习方法，以医学图像作为输入进行疾病预后建模，提高了预测阿尔茨海默病进展的准确性。 |
| [^260] | [Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection](https://arxiv.org/abs/2210.06891) | 提出了一种基于任务驱动特征选择的多通道成像实验设计方法，通过优化设计和训练机器学习模型执行用户指定的图像分析任务。 |
| [^261] | [Accelerating Laboratory Automation Through Robot Skill Learning For Sample Scraping](https://arxiv.org/abs/2209.14875) | 实验室自动化机器人通过学习技能加速实验室自动化，特别是对于样品刮取过程的自动化实现具有重要意义 |
| [^262] | [MechProNet: Machine Learning Prediction of Mechanical Properties in Metal Additive Manufacturing](https://arxiv.org/abs/2209.12605) | 通过机器学习方法预测金属增材制造中的机械性能，建立了一个全面的框架，包括来自90多篇文章和140个数据表的大量实验数据。 |
| [^263] | [Causal Intervention for Fairness in Multi-behavior Recommendation](https://arxiv.org/abs/2209.04589) | 通过考虑多种用户行为来减轻流行度偏见，处理了多行为推荐中的公平问题。 |
| [^264] | [Fair Division of Multi-layered Cakes](https://arxiv.org/abs/2208.00726) | 该论文介绍了多层蛋糕的公平分配问题，引入了“一对刀”计算模型，展示了针对不同代理和层数的可行比例分配计算过程，并开发了一种计算技术适用于任意数量的代理和层数。 |
| [^265] | [Accelerating Asynchronous Federated Learning Convergence via Opportunistic Mobile Relaying](https://arxiv.org/abs/2206.04742) | 通过机会性移动中继，提出了FedMobile算法，实现了异步联邦学习收敛速率为$O(\frac{1}{\sqrt{NT}})$。 |
| [^266] | [Local Interpretations for Explainable Natural Language Processing: A Survey](https://arxiv.org/abs/2103.11072) | 本文调查了改善深度神经网络在自然语言处理任务中可解释性的各种方法，特别关注局部解释，包括与相关输入特征相关的预测解释、自然语言解释以及探测模型隐藏状态和词表示。 |
| [^267] | [Attention-based Reinforcement Learning for Combinatorial Optimization: Application to Job Shop Scheduling Problem.](http://arxiv.org/abs/2401.16580) | 本论文提出了一种基于注意力的强化学习方法，用于解决作业车间调度问题。通过集成策略梯度强化学习和修改后的Transformer结构，我们的方法在解决大规模问题上表现出色，优于最近研究和广泛采用的启发式规则。 |
| [^268] | [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities.](http://arxiv.org/abs/2401.14405) | 本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。 |
| [^269] | [Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors.](http://arxiv.org/abs/2401.13652) | 本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。 |
| [^270] | [Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model.](http://arxiv.org/abs/2401.12873) | 本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。 |
| [^271] | [Evaluating Language Model Agency through Negotiations.](http://arxiv.org/abs/2401.04536) | 本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。 |
| [^272] | [The Power of Training: How Different Neural Network Setups Influence the Energy Demand.](http://arxiv.org/abs/2401.01851) | 本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。 |
| [^273] | [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review.](http://arxiv.org/abs/2401.01519) | 本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。 |
| [^274] | [SVGDreamer: Text Guided SVG Generation with Diffusion Model.](http://arxiv.org/abs/2312.16476) | 该论文提出了一种名为SVGDreamer的方法，用于基于文本引导的SVG生成。它通过引入语义驱动的图像矢量化过程和基于注意力的元素控制，增强了生成结果的可编辑性和质量。同时，采用基于矢量化粒子分数蒸馏的方法解决了现有方法中的颜色、平滑度和结果多样性方面的挑战。 |
| [^275] | [Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning.](http://arxiv.org/abs/2312.05720) | 本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。 |
| [^276] | [The Expressive Power of Low-Rank Adaptation.](http://arxiv.org/abs/2310.17513) | 本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。 |
| [^277] | [3D Masked Autoencoders for Enhanced Privacy in MRI Scans.](http://arxiv.org/abs/2310.15778) | 本研究提出了一种名为CP-MAE的模型，通过使用面部遮罩来实现MRI扫描中的人脸去识别，提高了隐私保护水平。 |
| [^278] | [Memory-Consistent Neural Networks for Imitation Learning.](http://arxiv.org/abs/2310.06171) | 本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。 |
| [^279] | [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens.](http://arxiv.org/abs/2310.02239) | MiniGPT-5使用生成凭据作为桥梁，引入了一种创新的交错视觉与语言生成技术，并通过独特的两阶段训练策略和无分类器的指导来实现无描述的多模态生成。 |
| [^280] | [L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation.](http://arxiv.org/abs/2310.02003) | L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。 |
| [^281] | [SmartPlay : A Benchmark for LLMs as Intelligent Agents.](http://arxiv.org/abs/2310.01557) | SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。 |
| [^282] | [ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter.](http://arxiv.org/abs/2310.01015) | ETGraph是一个连接以太坊和Twitter的开创性数据集，结合了以太坊交易记录和Twitter关注数据，通过验证OpenSea的Twitter账户与以太坊地址进行绑定，对其进行了详细统计分析，揭示了Twitter匹配和非Twitter匹配的以太坊地址之间的结构差异。 |
| [^283] | [Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients.](http://arxiv.org/abs/2310.01012) | 本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。 |
| [^284] | [Combining Spatial and Temporal Abstraction in Planning for Better Generalization.](http://arxiv.org/abs/2310.00229) | Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。 |
| [^285] | [Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving.](http://arxiv.org/abs/2310.00013) | 这篇论文提出了一个通信的协同感知框架ACC-DA，通过动态调整通信图和自适应数据重构机制来增强自动驾驶中的感知能力。 |
| [^286] | [MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool.](http://arxiv.org/abs/2309.16701) | 本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。 |
| [^287] | [From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity.](http://arxiv.org/abs/2309.16512) | 本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。 |
| [^288] | [Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration.](http://arxiv.org/abs/2309.15723) | 本文通过研究现有数据叙事工具，从人工智能协作的角度出发，总结了关于不同阶段和角色的共同协作模式，以促进人类和人工智能的优势并减少不足。 |
| [^289] | [Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts.](http://arxiv.org/abs/2309.13202) | 本研究使用大型语言模型和控制机制改善了生物医学摘要的文本可读性，具体包括领域微调和基于提示的学习方法，以及应用于编码器-解码器模型和GPT模型的控制令牌机制。 |
| [^290] | [Choice-75: A Dataset on Decision Branching in Script Learning.](http://arxiv.org/abs/2309.11737) | Choice-75是一个关于脚本学习中决策分支的数据集，主要挑战智能系统在描述场景下预测决策。在许多难题中仍有改进的空间。 |
| [^291] | [Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization.](http://arxiv.org/abs/2309.10370) | 本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。 |
| [^292] | [A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection.](http://arxiv.org/abs/2309.00464) | 本文提出了一种评估目标检测中不确定性校准的理论和实践框架，并通过实验证明了所提出指标的有效性。 |
| [^293] | [DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System.](http://arxiv.org/abs/2308.06378) | 本文介绍了一种新的深度卷积神经模糊推理系统（DCNFIS），它通过将模糊逻辑和深度学习模型相结合，实现了提高透明度而不损失准确性的目标。DCNFIS在准确性上与现有卷积神经网络相当，并且胜过了最先进的深度模糊系统。通过模糊规则提取的解释可以提高模型的可解释性。 |
| [^294] | [More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes.](http://arxiv.org/abs/2308.01313) | 本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。 |
| [^295] | [RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment.](http://arxiv.org/abs/2307.12950) | RLCD是一种用于语言模型对齐的强化学习方法，利用对比蒸馏训练偏好模型，可以使语言模型在不使用人类反馈的情况下遵循自然语言规则。在多个对齐任务和不同规模的模型上，RLCD优于其他基线方法。 |
| [^296] | [Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?.](http://arxiv.org/abs/2307.09985) | 该论文通过对MovieLens数据集的分析，发现用户与该平台的交互在不同阶段存在显著差异，并且用户交互受到平台推荐算法推荐的候选电影的影响。 |
| [^297] | [Neural Architecture Retrieval.](http://arxiv.org/abs/2307.07919) | 该论文提出了一个新的问题——神经架构检索，主要解决了研究人员在发现相似神经架构时所遇到的困难，并引入了多层对比学习来实现准确的图表示学习。 |
| [^298] | [In-context Autoencoder for Context Compression in a Large Language Model.](http://arxiv.org/abs/2307.06945) | 在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。 |
| [^299] | [SAMAug: Point Prompt Augmentation for Segment Anything Model.](http://arxiv.org/abs/2307.01187) | SAMAug是一种用于增强交互式图像分割性能的方法，通过生成增强的点提示，结合初始提示，可以提高Segment Anything Model的分割结果。 |
| [^300] | [The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks.](http://arxiv.org/abs/2306.16922) | ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。 |
| [^301] | [Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning.](http://arxiv.org/abs/2306.16001) | 本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。 |
| [^302] | [Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX.](http://arxiv.org/abs/2306.09884) | Jumanji是JAX中一套可扩展的强化学习系统，提供了一系列高度可定制的环境，具有快速、灵活、可扩展和模块化特点，利用硬件加速器赋能更有能力的代理人。 |
| [^303] | [Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots.](http://arxiv.org/abs/2306.05716) | 本研究提出了一种基于语言分割掩模的新方法，用于解决通用型机器人的泛化能力问题，提高了在开放域场景中新对象的抓取操作的学习效率和推广效果。 |
| [^304] | [Proximity-Informed Calibration for Deep Neural Networks.](http://arxiv.org/abs/2306.04590) | 该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。 |
| [^305] | [Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.](http://arxiv.org/abs/2305.15852) | 本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。 |
| [^306] | [Detecting Propaganda Techniques in Code-Switched Social Media Text.](http://arxiv.org/abs/2305.14534) | 本文提出了一个新任务，即在混合语言的社交媒体文本中检测宣传技术。为了支持这一任务，作者创建了一个包含1030个文本的英语和罗马乌尔混合语言的语料库，并进行了一系列实验。 |
| [^307] | [Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees.](http://arxiv.org/abs/2305.11997) | 本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。 |
| [^308] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^309] | [Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization.](http://arxiv.org/abs/2305.11074) | Tram是一种源代码摘要的令牌级别检索增强机制，它在解码器端精细检索帮助神经模型生成更准确的摘要，并在Java和Python源代码摘要任务上表现优异。 |
| [^310] | [Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis.](http://arxiv.org/abs/2305.08339) | 本文研究了使用基于大语言模型的聊天机器人自动标注文本的潜力，重点考察了从本地语法角度观察道歉言语行为构成的功能元素的程度，并比较了不同模型在注释任务中的表现，结果表明Bing聊天机器人在任务中表现优于ChatGPT和人类标注员。 |
| [^311] | [When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks.](http://arxiv.org/abs/2305.06626) | 本文通过预测单个标注者的打分，并结合文本目标群体的预测，模拟了目标群体成员的意见，通过使用他们的人口统计学数据和在线意见预测标注者的打分，在仇恨言论检测等主观任务中提高了模型性能。 |
| [^312] | [The Update Equivalence Framework for Decision-Time Planning.](http://arxiv.org/abs/2304.13138) | 该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。 |
| [^313] | [Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures.](http://arxiv.org/abs/2304.12576) | 该论文介绍了一种新的方法来开发适用于现代CPU体系结构的高效、可移植的深度学习和高性能计算内核，使用高级循环和张量抽象。 |
| [^314] | [Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems.](http://arxiv.org/abs/2304.09444) | 本文提出了一种基于排名学习和局部模型的多目标进化算法，该算法使用分类器进行排名，以解决高维昂贵多目标优化问题。 |
| [^315] | [Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space.](http://arxiv.org/abs/2304.00436) | 本研究提出了一种实例级别的特洛伊攻击方法，在视觉问答中通过神经元激活空间的对抗学习生成多样性特洛伊，有效地适应了微调模型。 |
| [^316] | [Magnushammer: A Transformer-based Approach to Premise Selection.](http://arxiv.org/abs/2303.04488) | Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。 |
| [^317] | [Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging.](http://arxiv.org/abs/2302.01622) | 本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。 |
| [^318] | [Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence.](http://arxiv.org/abs/2301.11476) | 这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。 |
| [^319] | [Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning.](http://arxiv.org/abs/2211.14769) | 本文研究了联邦学习体系下代理人学习中可能出现的攻击和防御策略，建立了全联邦拜占庭鲁棒的代理人学习模型。其中，导航即攻击者所愿（NAW）是一种简单而有效的攻击策略，而基于离群点检测的防御训练方法可以有效减轻NAW攻击的影响，提高代理人学习的全局鲁棒性。 |
| [^320] | [Graph Neural Modeling of Network Flows.](http://arxiv.org/abs/2209.05208) | 本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。 |

# 详细

[^1]: 可执行代码行动能够激发更出色的LLM智能体

    Executable Code Actions Elicit Better LLM Agents

    [https://rss.arxiv.org/abs/2402.01030](https://rss.arxiv.org/abs/2402.01030)

    使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。

    

    大型语言模型（LLM）智能体具备执行广泛行动的能力，如调用工具和控制机器人等，在解决现实世界的挑战中显示出巨大潜力。LLM智能体通常通过生成JSON或文本的预定义格式来产生行动，这通常受限于受限制的行动空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本研究提出使用可执行的Python代码将LLM智能体的行动整合到一个统一的行动空间中（CodeAct）。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互在新的观察中动态修订先前的行动或发出新的行动。我们对17个LLM在API-Bank和新编制的基准测试中进行了广泛分析，结果显示CodeAct的性能超过了广泛使用的替代方案（成功率高出20%）。CodeAct的令人鼓舞的表现激励我们建立了一个开源的...

    Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
    
[^2]: ROUTERBENCH：用于多LLM路由系统的基准测试

    ROUTERBENCH: A Benchmark for Multi-LLM Routing System

    [https://arxiv.org/abs/2403.12031](https://arxiv.org/abs/2403.12031)

    提出了ROUTERBENCH，一个用于评估LLM路由系统性能的基准测试框架，包括超过405k推理结果的数据集，以支持路由策略的开发。

    

    随着大型语言模型（LLMs）的应用范围不断扩大，对有效的服务解决方案的需求变得日益关键。尽管LLMs具有多样性，但没有单一模型可以最优地解决所有任务和应用，特别是在平衡性能和成本之间。为了弥补这一限制，发展了LLM路由系统，这些系统结合了各种模型的优势，以克服单个LLMs的约束。然而，缺乏用于评估LLM路由器性能的标准化基准测试，阻碍了这一领域的进展。为弥合这一差距，我们提出了ROUTERBENCH，这是一个新颖的评估框架，旨在系统评估LLM路由系统的功效，以及一个包括来自代表性LLMs的超过405k推理结果的全面数据集，以支持路由策略的开发。我们进一步提出了一个LLM路由的理论框架，以及...

    arXiv:2403.12031v1 Announce Type: cross  Abstract: As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and del
    
[^3]: 对齐与提炼：统一和改进领域自适应目标检测

    Align and Distill: Unifying and Improving Domain Adaptive Object Detection

    [https://arxiv.org/abs/2403.12029](https://arxiv.org/abs/2403.12029)

    引入了统一的基准测试和实现框架ALDI以及新的DAOD基准数据集CFC-DAOD，解决了领域自适应目标检测中的基准问题，并支持未来方法的发展。

    

    目标检测器通常表现不佳于与其训练集不同的数据。最近，领域自适应目标检测（DAOD）方法已经展示了在应对这一挑战上的强大结果。遗憾的是，我们发现了系统化的基准测试陷阱，这些陷阱对过去的结果提出质疑并阻碍了进一步的进展：（a）由于基线不足导致性能高估，（b）不一致的实现实践阻止了方法的透明比较，（c）由于过时的骨干和基准测试缺乏多样性，导致缺乏普遍性。我们通过引入以下问题来解决这些问题：（1）一个统一的基准测试和实现框架，Align and Distill（ALDI），支持DAOD方法的比较并支持未来发展，（2）一个公平且现代的DAOD训练和评估协议，解决了基准测试的陷阱，（3）一个新的DAOD基准数据集，CFC-DAOD，能够在多样化的真实环境中进行评估。

    arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
    
[^4]: Ultraman：具有超高速和细节的单图像3D人体重建

    Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and Detail

    [https://arxiv.org/abs/2403.12028](https://arxiv.org/abs/2403.12028)

    提出了一种名为“Ultraman”的新方法，实现了从单一图像快速重建带有纹理的3D人体模型，并通过优化的框架提高了重建速度和准确性

    

    3D人体重建一直是计算机视觉领域的一个挑战。先前的方法往往耗时且难以捕捉人体的详细外观。本文提出了一种名为“Ultraman”的新方法，用于从单一图像快速重建带有纹理的3D人体模型。与现有技术相比，“Ultraman”极大地提高了重建速度和准确性，同时保留了高质量的纹理细节。我们提出了一套新的人体重建框架，包括几个部分，几何重建、纹理生成和纹理映射。首先使用了一个网格重建框架，准确地从单一图像中提取3D人体形状。同时，我们提出了一种基于单一图像生成多视角一致的人体图像的方法。最后，结合一种新颖的纹理映射方法来优化纹理。

    arXiv:2403.12028v1 Announce Type: cross  Abstract: 3D human body reconstruction has been a challenge in the field of computer vision. Previous methods are often time-consuming and difficult to capture the detailed appearance of the human body. In this paper, we propose a new method called \emph{Ultraman} for fast reconstruction of textured 3D human models from a single image. Compared to existing techniques, \emph{Ultraman} greatly improves the reconstruction speed and accuracy while preserving high-quality texture details. We present a set of new frameworks for human reconstruction consisting of three parts, geometric reconstruction, texture generation and texture mapping. Firstly, a mesh reconstruction framework is used, which accurately extracts 3D human shapes from a single image. At the same time, we propose a method to generate a multi-view consistent image of the human body based on a single image. This is finally combined with a novel texture mapping method to optimize texture 
    
[^5]: 从像素到洞察: 在大型基础模型时代自动图表理解的调查

    From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models

    [https://arxiv.org/abs/2403.12027](https://arxiv.org/abs/2403.12027)

    近年来，随着大型基础模型的兴起，自动图表理解取得了显著进展，本调查论文概述了在这些基础模型背景下图表理解领域的最新发展、挑战和未来方向

    

    数据可视化以图表形式在数据分析中扮演着关键角色，提供关键洞察并帮助做出明智决策。随着近年大型基础模型的崛起，自动图表理解取得了显著进展。基础模型，如大型语言模型(LLMs)，已经在各种自然语言处理（NLP）任务中实现了革命，并越来越多地应用于图表理解任务。本调查论文全面介绍了最新进展、挑战和未来方向，探讨了这些基础模型背景下图表理解的内容。

    arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
    
[^6]: FlexCap：在图像中生成丰富、本地化和灵活的标题

    FlexCap: Generating Rich, Localized, and Flexible Captions in Images

    [https://arxiv.org/abs/2403.12026](https://arxiv.org/abs/2403.12026)

    FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。

    

    我们介绍了一种多功能的$\textit{灵活字幕}$视觉-语言模型（VLM），能够生成长度不同的特定区域描述。该模型FlexCap经过训练，可为输入的边界框生成长度条件的字幕，从而可以控制其输出的信息密度，描述范围从简洁的对象标签到详细的字幕。为了实现这一点，我们从带字幕的图像开始创建了大规模的图像区域描述训练数据集。这种灵活的字幕功能有几个宝贵的应用。首先，FlexCap在Visual Genome数据集上的密集字幕任务中表现出优越性能。其次，可以通过采用FlexCap生成本地化描述作为大型语言模型的输入来构建视觉问答（VQA）系统。由此产生的系统在许多VQ上实现了最新技术的零样本性能。

    arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
    
[^7]: 监督微调作为逆强化学习

    Supervised Fine-Tuning as Inverse Reinforcement Learning

    [https://arxiv.org/abs/2403.12017](https://arxiv.org/abs/2403.12017)

    本论文提出将逆强化学习和模仿学习的见解结合，探讨了使用演示数据集对齐大语言模型的方法，并对不同方法的性能进行了分析。

    

    大语言模型（LLMs）对齐的主流方法通常依赖于人类或AI反馈，并假设可以访问特定类型的偏好数据集。在我们的工作中，我们质疑这些数据集的有效性，并探讨了各种情景下与专家演示对齐更为现实的方法。我们构建了一个顺序决策框架，以演示数据集为基础来规划对齐LLMs的问题。借鉴逆强化学习和模仿学习的见解，我们引入了各种方法来最小化LLM对齐任务中的差异。我们的分析突出了这些不同方法的覆盖率和寻找模式行为。此外，我们考察了经典监督微调方法的利弊，并详细阐述了不同方法表现突出的情景。

    arXiv:2403.12017v1 Announce Type: cross  Abstract: The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.
    
[^8]: EnvGen: 通过LLMs生成和调整环境以训练具身体的代理

    EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents

    [https://arxiv.org/abs/2403.12014](https://arxiv.org/abs/2403.12014)

    EnvGen提出了一种新的框架，利用LLMs的推理能力自适应创建训练环境，帮助小型具身体RL代理在弱点方面学习有用技能。

    

    最近有关通过互动进行具身体学习的最新方法直接采用大型语言模型（LLMs）作为代理，以确定环境中的下一步。LLM代理由于其世界知识和推理能力，比基于强化学习（RL）的以往较小的代理表现更强；但频繁调用LLMs速度慢且昂贵。我们提出EnvGen，一个处理这个问题的新框架。首先，我们提示一个LLM生成训练环境，使代理可以快速并行学习不同任务。具体而言，LLM获得任务描述和模拟器目标，然后被要求生成一组环境配置。

    arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
    
[^9]: 基于大型视频生成模型的一致性多视角生成

    VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model

    [https://arxiv.org/abs/2403.12010](https://arxiv.org/abs/2403.12010)

    提出了基于大型视频生成模型的一致性多视角生成框架，通过微调视频生成模型并引入3D感知去噪采样方法来解决多视角图像生成中的数据训练和一致性问题。

    

    生成基于文本或单图像提示的多视角图像是创建3D内容的关键能力。本文介绍了一个新颖的框架，对于数据用于训练和如何确保多视角一致性这两个基本问题都做出了贡献。与利用2D扩散模型的图像进行训练不同，我们提出了一种密集的一致性多视角生成模型，该模型是从现成的视频生成模型微调而来的。视频生成模型生成的图像更适合于多视角生成，因为生成这些图像的基础网络架构采用了时间模块来强制帧一致性。此外，用于训练这些模型的视频数据集丰富多样，导致减少了训练微调领域差距。为了增强多视角一致性，我们引入了一个3D感知去噪采样。

    arXiv:2403.12010v1 Announce Type: cross  Abstract: Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first empl
    
[^10]: 利用胶囊网络和图神经网络提取空间和语义特征进行皮肤癌诊断

    Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks

    [https://arxiv.org/abs/2403.12009](https://arxiv.org/abs/2403.12009)

    本研究通过将图神经网络与胶囊网络相结合，提出了一种创新方法来增强皮肤癌诊断的分类性能。

    

    在皮肤病变图像分类领域，复杂的空间和语义特征对传统的卷积神经网络（CNN）方法构成了重大挑战。皮肤病变数据集的不平衡性加剧了这些挑战，这阻碍了模型有效学习少数类特征的能力。本研究通过将图神经网络（GNNs）与胶囊网络相结合，提出了一种创新方法来增强分类性能。GNNs擅长处理图结构数据，为捕获复杂模式和关系提供了一种高级机制，远超传统CNN的能力。胶囊网络进一步通过提供对空间层次的优越识别能力来贡献。

    arXiv:2403.12009v1 Announce Type: cross  Abstract: In the realm of skin lesion image classification, the intricate spatial and semantic features pose significant challenges for conventional Convolutional Neural Network (CNN)-based methodologies. These challenges are compounded by the imbalanced nature of skin lesion datasets, which hampers the ability of models to learn minority class features effectively. Despite augmentation strategies, such as those using Generative Adversarial Networks (GANs), previous attempts have not fully addressed these complexities. This study introduces an innovative approach by integrating Graph Neural Networks (GNNs) with Capsule Networks to enhance classification performance. GNNs, known for their proficiency in handling graph-structured data, offer an advanced mechanism for capturing complex patterns and relationships beyond the capabilities of traditional CNNs. Capsule Networks further contribute by providing superior recognition of spatial hierarchies 
    
[^11]: DreamMotion：用于零样本视频编辑的时空自相似性分数蒸馏

    DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing

    [https://arxiv.org/abs/2403.12002](https://arxiv.org/abs/2403.12002)

    该方法提出了一种用于零样本视频编辑的新方法，通过匹配原始视频和编辑视频的时空自相似性，在分数蒸馏过程中解决了新内容引入时可能出现的结构和运动偏差问题。

    

    arXiv:2403.12002v1 公告类型：跨领域 摘要：基于文本驱动的扩散式视频编辑在图像编辑文献中显现了一项独特挑战：建立真实世界运动。与现有的视频编辑方法不同，我们在这里专注于分数蒸馏采样，以规避标准的反向扩散过程，并从已展现自然运动的视频中启动优化。我们的分析表明，虽然视频分数蒸馏可以有效地引入目标文本指示的新内容，但也可能导致显著的结构和运动偏差。为了抵消这一点，我们提出在分数蒸馏过程中匹配原始视频和编辑视频的时空自相似性。由于使用了分数蒸馏，我们的方法与模型无关，可应用于级联和非级联视频扩散框架。通过与领先方法的广泛比较，我们的方法展示了在视频编辑中的卓越优势。

    arXiv:2403.12002v1 Announce Type: cross  Abstract: Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion. Unlike existing video editing approaches, here we focus on score distillation sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion. Our analysis reveals that while video score distillation can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation. To counteract this, we propose to match space-time self-similarities of the original video and the edited video during the score distillation. Thanks to the use of score distillation, our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks. Through extensive comparisons with leading methods, our approach demonstrates its superiority in alterin
    
[^12]: Notochord：一种用于实时MIDI表演的灵活概率模型

    Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance

    [https://arxiv.org/abs/2403.12000](https://arxiv.org/abs/2403.12000)

    Notochord是一种灵活概率模型，能够在实时MIDI表演中生成多音轨MIDI并以低于10毫秒的延迟响应输入，支持多种交互音乐功能。

    

    音乐数据基于深度学习的概率模型产生了越来越逼真的结果，并有望进入各种创作工作流程中。然而，在表演环境中对其实际效果进行研究还很少，用户行为的结果通常应该即时感知。为了促进这样的研究，我们设计了Notochord，这是一个用于结构化事件序列的深度概率模型，并在Lakh MIDI数据集上对其进行了训练。我们的概率形式允许在子事件级别进行可解释的干预，这使得一个模型可以作为多样交互音乐功能的基础，包括可操控的生成、和声、机器即兴和基于可能性的界面。Notochord可以生成多音轨MIDI并在十毫秒以下的延迟内响应输入。我们提供了训练代码，模型检查点和交互示例作为开源软件。

    arXiv:2403.12000v1 Announce Type: cross  Abstract: Deep learning-based probabilistic models of musical data are producing increasingly realistic results and promise to enter creative workflows of many kinds. Yet they have been little-studied in a performance setting, where the results of user actions typically ought to feel instantaneous. To enable such study, we designed Notochord, a deep probabilistic model for sequences of structured events, and trained an instance of it on the Lakh MIDI dataset. Our probabilistic formulation allows interpretable interventions at a sub-event level, which enables one model to act as a backbone for diverse interactive musical functions including steerable generation, harmonization, machine improvisation, and likelihood-based interfaces. Notochord can generate polyphonic and multi-track MIDI, and respond to inputs with latency below ten milliseconds. Training code, model checkpoints and interactive examples are provided as open source software.
    
[^13]: 利用生成式知识提取、基于图的表示和多模态智能图推理加速科学发现

    Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning

    [https://arxiv.org/abs/2403.11996](https://arxiv.org/abs/2403.11996)

    利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。

    

    利用生成式人工智能，我们将一组涉及生物材料领域的1,000篇科学论文转化为详细的本体知识图，揭示了它们固有的无标度特性。通过基于节点相似性和介数中心性的组合排名，探测不同概念之间的图遍历路径，我们揭示了深入的跨学科关系，可用于回答查询，识别知识中的空白，并提出前所未见的材料设计及其行为。一项比较揭示了生物材料和贝多芬第九交响曲之间的详细结构相似之处，突显了通过同构映射共享复杂性模式。该算法进一步创建了一种创新的基于分级菌丝体的复合材料，将图采样的联合合成原理与康定斯基《第七组成》中提取的原则相结合

    arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
    
[^14]: 使用生成文本模型为教学评估创建定性代码手册

    Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching

    [https://arxiv.org/abs/2403.11984](https://arxiv.org/abs/2403.11984)

    本文介绍了一种使用自然语言处理和大型语言模型分析教学评估的新方法，可以从大量反馈中提取、嵌入、聚类和总结SETs。

    

    反馈是改进的关键方面。然而，当有来自多个来源的大量反馈时，将信息提炼成可操作的见解可能会很困难。本文讨论了一种使用自然语言处理（NLP）和大型语言模型（LLMs）分析教学评估（SETs）的新方法。我们通过将其应用于一所大型公立大学的5000个SETs语料库来演示该方法。

    arXiv:2403.11984v1 Announce Type: cross  Abstract: Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to id
    
[^15]: 通知谱归一化高斯过程用于轨迹预测

    Informed Spectral Normalized Gaussian Processes for Trajectory Prediction

    [https://arxiv.org/abs/2403.11966](https://arxiv.org/abs/2403.11966)

    该论文提出了一种用于轨迹预测的新颖正则化持续学习方法，利用通知式先验知识，提高了模型性能和数据效率。

    

    先前的参数分布为通知式学习提供了一种优雅的方式，以表示先验专家和世界知识。以前的工作表明，使用这样的信息先验来规范概率深度学习（DL）模型会增加它们的性能和数据效率。然而，用于概率DL模型的常用基于采样的近似方法可能计算昂贵，需要多次推理和更长的训练时间。计算高效的最后一层核逼近如谱归一化高斯过程（SNGPs）是有希望的替代方案。我们提出了一种针对SNGPs的基于新颖正则化的持续学习方法，它可以使用代表先前任务学习的先验知识的通知先验。我们的提议建立在成熟方法的基础上，不需要记忆或参数扩展。我们将我们的通知SNGP模型应用于轨迹预测问题。

    arXiv:2403.11966v1 Announce Type: cross  Abstract: Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction proble
    
[^16]: 通过运动补偿增强事件驱动视频重建

    Enhanced Event-Based Video Reconstruction with Motion Compensation

    [https://arxiv.org/abs/2403.11961](https://arxiv.org/abs/2403.11961)

    通过运动补偿来增强重建质量，提出将输入帧和稀疏编码进行变换，并将流网络与CISTA-LSTC集成，形成CISTA-Flow网络，使系统仅依赖事件。

    

    深度神经网络用于事件驱动视频重建通常缺乏可解释性，并且具有高内存需求。最近引入了一种轻量级网络CISTA-LSTC，表明通过系统设计其架构可以实现高质量的重建。然而，其建模假设输入信号和输出重建帧共享相同的稀疏表示，忽视了运动导致的位移。为了解决这一问题，我们提出对输入强度帧和稀疏编码进行变换以增强重建质量。通过将流网络与CISTA-LSTC集成，构建了一个CISTA-Flow网络用于运动补偿。系统仅依赖事件，其中预测的流有助于重建，然后重建的帧用于促进流估计。我们还为该组合系统引入了一个迭代训练框架。结果表明

    arXiv:2403.11961v1 Announce Type: cross  Abstract: Deep neural networks for event-based video reconstruction often suffer from a lack of interpretability and have high memory demands. A lightweight network called CISTA-LSTC has recently been introduced showing that high-quality reconstruction can be achieved through the systematic design of its architecture. However, its modelling assumption that input signals and output reconstructed frame share the same sparse representation neglects the displacement caused by motion. To address this, we propose warping the input intensity frames and sparse codes to enhance reconstruction quality. A CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC for motion compensation. The system relies solely on events, in which predicted flow aids in reconstruction and then reconstructed frames are used to facilitate flow estimation. We also introduce an iterative training framework for this combined system. Results demonstrate tha
    
[^17]: 通过不规则重复先验优化视频动作计数的IVAC-P2L

    IVAC-P2L: Enhancing Video Action Counting through Irregular Repetition Priors

    [https://arxiv.org/abs/2403.11959](https://arxiv.org/abs/2403.11959)

    IVAC-P2L提出了一种新颖的视频动作计数方法，通过优化不规则重复模式，实现了在视频中准确计数动作的重要性

    

    视频动作计数（VAC）在分析运动、健身和日常活动中具有重要意义，通过量化视频中的重复动作。然而，传统的VAC方法忽略了动作重复的复杂性，如中断和周期持续时间的变异性。我们的研究通过引入一种新颖的VAC方法，称为Irregular Video Action Counting（IVAC），来解决这一不足。IVAC优先考虑建模视频中的不规则重复模式，我们通过两个主要方面对其进行定义：跨周期一致性和周期间隔不一致性。跨周期一致性确保周期段的时空表示的同质性，表示周期内动作的一致性。周期间隔不一致性强调基于固有内容差异区分周期段和间隔的重要性。为了概括这些原则，我们提出了一种新方法论

    arXiv:2403.11959v1 Announce Type: cross  Abstract: Video Action Counting (VAC) is crucial in analyzing sports, fitness, and everyday activities by quantifying repetitive actions in videos. However, traditional VAC methods have overlooked the complexity of action repetitions, such as interruptions and the variability in cycle duration. Our research addresses the shortfall by introducing a novel approach to VAC, called Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular repetition patterns in videos, which we define through two primary aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures homogeneity in the spatial-temporal representations of cycle segments, signifying action uniformity within cycles. Cycle-interval inconsistency highlights the importance of distinguishing between cycle segments and intervals based on their inherent content differences. To encapsulate these principles, we propose a new methodology that 
    
[^18]: 通过半监督预训练和时间建模探索面部表情识别

    Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling

    [https://arxiv.org/abs/2403.11942](https://arxiv.org/abs/2403.11942)

    该论文通过半监督学习技术生成表情类别伪标签，实现了面部表情识别模型的泛化能力，在面临数据集限制和类别不平衡问题时取得了优异的识别性能。

    

    面部表情识别（FER）在计算机视觉中起着关键作用，并在各个领域中有着广泛的应用。本文旨在介绍我们针对即将在CVPR2024举行的第6届野外情感行为分析（ABAW）比赛的方法。我们在面部表情识别任务中，利用半监督学习技术为未标记的面部数据生成表情类别伪标签，通过均匀采样标记的面部表情样本，并实施去偏反馈学习策略来解决数据集中类别不平衡的问题和半监督学习中可能存在的数据偏差问题。

    arXiv:2403.11942v1 Announce Type: cross  Abstract: Facial Expression Recognition (FER) plays a crucial role in computer vision and finds extensive applications across various fields. This paper aims to present our approach for the upcoming 6th Affective Behavior Analysis in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024.. In the facial expression recognition task, The limited size of the FER dataset poses a challenge to the expression recognition model's generalization ability, resulting in subpar recognition performance. To address this problem, we employ a semi-supervised learning technique to generate expression category pseudo-labels for unlabeled face data. At the same time, we uniformly sampled the labeled facial expression samples and implemented a debiased feedback learning strategy to address the problem of category imbalance in the dataset and the possible data bias in semi-supervised learning. Moreover, , to further compensate for the limitation and bias of fe
    
[^19]: Tur[k]ingBench：用于网络代理的挑战基准测试

    Tur[k]ingBench: A Challenge Benchmark for Web Agents

    [https://arxiv.org/abs/2403.11905](https://arxiv.org/abs/2403.11905)

    Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。

    

    最近的聊天机器人展示了在原始文本形式下理解和交流的令人印象深刻的能力。然而，世界上不仅仅是原始文本。例如，人们在网页上花费大量时间，在这些网页上，文本与其他形式交织在一起，并以各种复杂互动的形式完成任务。最先进的多模型是否能够推广到这种复杂的领域呢？为了回答这个问题，我们介绍了TurkingBench，一个由包含多模态背景的文本说明制定的任务基准。与现有的使用人工合成的网页的工作不同，这里我们使用最初设计用于各种注释目的的自然HTML页面。每个任务的HTML说明也被实例化为各种值（从众包任务获得）以形成任务的新实例。这个基准包含32.2K个实例。

    arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
    
[^20]: Larimar: 具有情节记忆控制的大型语言模型

    Larimar: Large Language Models with Episodic Memory Control

    [https://arxiv.org/abs/2403.11901](https://arxiv.org/abs/2403.11901)

    Larimar提出了一种大脑启发的架构，通过分布式情节记忆增强LLMs，实现了动态、一次性的知识更新，无需昂贵的重新训练或微调，且在速度和灵活性上表现出色。

    

    本文提出了Larimar - 一种新颖的、受大脑启发的架构，用于增强大型语言模型(LLMs)的分布式情节记忆。 Larimar的记忆允许动态、一次性更新知识，无需进行计算昂贵的重新训练或微调。在多个事实编辑基准测试上的实验结果表明，Larimar在速度方面表现优异 - 根据基础LLM的不同，速度提升为4-10倍，并且由于提出的架构简单、不依赖于LLM，因此具有良好的灵活性和通用性。我们进一步提供了选择性事实遗忘和输入上下文长度概括机制，并展示了它们的有效性。

    arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
    
[^21]: 从可解释到可解释的深度学习在医疗自然语言处理中的应用：现实有多远？

    From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?

    [https://arxiv.org/abs/2403.11894](https://arxiv.org/abs/2403.11894)

    该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。

    

    深度学习（DL）通过解决各种自然语言处理（NLP）任务，极大地增强了医疗保健研究。然而，基于DL的NLP方法日益复杂，需要透明的模型解释性，或至少是可解释性，以进行可靠的决策制定。本文对医疗健康NLP中的可解释和可解释的DL进行了彻底的范围审查。引入了术语“XIAI”（eXplainable和Interpretable Artificial Intelligence）以区分XAI和IAI。方法根据其功能（模型、输入、输出为基础）和范围（局部、全局）进一步分类。我们的分析表明，注意机制是最主要的新兴IAI。此外，IAI越来越多地用于对抗XAI。确定的主要挑战是大多数XIAI不探索“全局”建模过程，缺乏最佳实践，并且需要系统评估和基准测试。

    arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
    
[^22]: SuperLoRA: 多层注意力模块参数高效统一适应

    SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules

    [https://arxiv.org/abs/2403.11887](https://arxiv.org/abs/2403.11887)

    SuperLoRA提出了一个统一且高度灵活的框架，通过引入不同的技巧扩展了不同的LoRA变体，在极少参数情况下特别优异。

    

    低秩自适应（LoRA）及其变体被广泛应用于微调大型模型，包括自然语言处理的大型语言模型和计算机视觉的扩散模型。本文提出了一个名为SuperLoRA的通用框架，统一并扩展了不同的LoRA变体，可以在不同的超参数设置下实现。通过引入分组、折叠、洗牌、投影和张量因子化，SuperLoRA相比其他LoRA变体提供了更高的灵活性，在非常少的参数范围内特别在传递学习任务中表现出卓越性能。

    arXiv:2403.11887v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) and its variants are widely employed in fine-tuning large models, including large language models for natural language processing and diffusion models for computer vision. This paper proposes a generalized framework called SuperLoRA that unifies and extends different LoRA variants, which can be realized under different hyper-parameter settings. Introducing grouping, folding, shuffling, projecting, and tensor factoring, SuperLoRA offers high flexibility compared with other LoRA variants and demonstrates superior performance for transfer learning tasks especially in the extremely few-parameter regimes.
    
[^23]: QueryAgent：一种具有环境反馈的可靠高效推理框架及自我校正

    QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction

    [https://arxiv.org/abs/2403.11886](https://arxiv.org/abs/2403.11886)

    QueryAgent提出了一种基于环境反馈的自我校正方法ERASER，在语义解析中表现出显著的性能提升和高效性

    

    使用大型语言模型（LLMs）进行语义解析取得了显著成功，但在遇到幻觉时现有方法可靠性和效率方面存在不足。本文提出了一个名为QueryAgent的框架，通过逐步解决问题并进行逐步自我校正来解决这些挑战。我们引入了一种基于环境反馈的自我校正方法ERASER。与传统方法不同，ERASER利用中间步骤中的丰富环境反馈，在必要时仅进行选择性和差异化的自我校正。实验结果表明，QueryAgent在GrailQA和GraphQ上仅使用一个例子就比所有先前的少样本方法取得了7.0和15.0的F1值提升。此外，我们的方法在效率方面表现出优势，包括运行时间、查询开销和API调用成本。通过利用ERASER，

    arXiv:2403.11886v1 Announce Type: cross  Abstract: Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we
    
[^24]: ReGenNet：走向人类行动-反应综合

    ReGenNet: Towards Human Action-Reaction Synthesis

    [https://arxiv.org/abs/2403.11882](https://arxiv.org/abs/2403.11882)

    本文分析了人际互动的非对称、动态、同步和详细性质，并提出了第一个多场景人类行动-反应综合基准。

    

    人类不断与周围环境互动。当前以人类为中心的生成模型主要集中在合成人类与静态场景和对象进行可信互动，而对于普遍因果人际互动的动态人类行动-反应综合进行了较少的探索。本文全面分析了人际互动的非对称、动态、同步和详细性质，并提出了第一个多场景人类行动-反应综合基准，以在给定人类行动的条件下生成人类反应。首先，我们提出为NTU120、InterHuman和Chi3D数据集注释交互序列的actor-reactor次序。基于这些数据集，提出了一种基于扩散的生成模型，使用Transformer解码器结构，名为ReGenNet。

    arXiv:2403.11882v1 Announce Type: cross  Abstract: Humans constantly interact with their surrounding environments. Current human-centric generative models mainly focus on synthesizing humans plausibly interacting with static scenes and objects, while the dynamic human action-reaction synthesis for ubiquitous causal human-human interactions is less explored. Human-human interactions can be regarded as asymmetric with actors and reactors in atomic interaction periods. In this paper, we comprehensively analyze the asymmetric, dynamic, synchronous, and detailed nature of human-human interactions and propose the first multi-setting human action-reaction synthesis benchmark to generate human reactions conditioned on given human actions. To begin with, we propose to annotate the actor-reactor order of the interaction sequences for the NTU120, InterHuman, and Chi3D datasets. Based on them, a diffusion-based generative model with a Transformer decoder architecture called ReGenNet together with 
    
[^25]: 单模态多任务融合用于情感模仿预测

    Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton

    [https://arxiv.org/abs/2403.11879](https://arxiv.org/abs/2403.11879)

    通过融合技术整合全局背景信息和采用LSTM架构进行时间分析，我们的方法在情感模仿强度预测任务上取得了显着的改进。

    

    在这项研究中，我们提出了一种方法，用于在第六届户外情感行为分析研讨会和竞赛中进行情感模仿强度（EMI）估计任务。我们的方法利用了Wav2Vec 2.0框架，在一个全面的播客数据集上进行了预训练，以提取涵盖语言和语外元素的广泛音频特征。我们通过一种融合技术增强了特征表示，该技术将个体特征与全局均值向量相结合，引入全局背景信息到我们的分析中。此外，我们从Wav2Vec 2.0模型中引入了一个预训练的valence-arousal-dominance（VAD）模块。我们的融合采用了一种长短期记忆（LSTM）架构，用于对音频数据进行高效的时间分析。仅利用所提供的音频数据，我们的方法在已建立的基准线上表现出显著的改进。

    arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.
    
[^26]: 利用热成像探索多模态神经场景表示并应用

    Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging

    [https://arxiv.org/abs/2403.11865](https://arxiv.org/abs/2403.11865)

    本文对神经场景表示在多模态学习中的应用进行了全面评估，并提出了四种不同的策略以将第二模态（非RGB）纳入NeRFs中，通过选择热成像作为第二模态来挑战神经场景表示的整合。

    

    神经辐射场（NeRFs）在一组RGB图像上训练时迅速发展为新的事实标准，用于新视角合成任务。本文在多模态学习的背景下对神经场景表示（如NeRFs）进行了全面评估。具体而言，我们提出了四种不同的策略，用于如何将第二模态（非RGB）纳入NeRFs中：（1）独立地从头训练每种模态；（2）在RGB上进行预训练，然后在第二模态上进行微调；（3）添加第二分支；（4）添加一个单独的组件来预测（颜色）额外模态的值。我们选择了热成像作为第二模态，因为从辐射度来看，它与RGB有很大差异，这使得将其整合到神经场景表示中具有挑战性。

    arXiv:2403.11865v1 Announce Type: cross  Abstract: Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisti
    
[^27]: 具有潜在状态推断的强化学习在自动匝道合并中的应用

    Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay

    [https://arxiv.org/abs/2403.11852](https://arxiv.org/abs/2403.11852)

    本文提出了一种具有潜在状态推断的强化学习方法，用于解决自动匝道合并问题，在没有详细了解周围车辆意图或驾驶风格的情况下安全执行匝道合并任务，并考虑了观测延迟，以增强代理在动态交通状况中的决策能力。

    

    本文提出了一种解决自动匝道合并问题的新方法，其中自动驾驶车辆需要无缝地融入多车道高速公路上的车流。我们介绍了Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS)代理，旨在在没有关于周围车辆意图或驾驶风格的全面知识的情况下安全执行匝道合并任务。我们还提出了该代理的增强版AL3IS，考虑了观测延迟，使代理能够在具有车辆间通信延迟的现实环境中做出更稳健的决策。通过通过潜在状态建模环境中的不可观察方面，如其他驾驶员的意图，我们的方法增强了代理适应动态交通状况、优化合并操作并确保与其他车辆进行安全互动的能力。

    arXiv:2403.11852v1 Announce Type: cross  Abstract: This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehi
    
[^28]: 模糊粗糙Choquet距离用于分类

    Fuzzy Rough Choquet Distances for Classification

    [https://arxiv.org/abs/2403.11843](https://arxiv.org/abs/2403.11843)

    本文提出了一种基于模糊粗糙集合度量的新型Choquet距离，用于捕捉数据中的非线性关系，使得距离度量更加灵活与准确。

    

    本文介绍了一种基于模糊粗糙集合度量的新型Choquet距离。所提出的距离度量结合了从模糊粗糙集理论中获得的属性信息和Choquet积分的灵活性。这种方法旨在适应性地捕捉数据中的非线性关系，认识到条件属性与决策属性之间的相互作用，并实现更灵活与准确的距离度量。我们探讨了它在机器学习中的应用，特别强调基于距离的分类方法（例如k最近邻）。本文研究了两种基于正区域的模糊粗糙集度量，并探讨了两种根据模糊粗糙集理论导出的用于适用Choquet积分的单调化程序，以及它们之间的差异。

    arXiv:2403.11843v1 Announce Type: cross  Abstract: This paper introduces a novel Choquet distance using fuzzy rough set based measures. The proposed distance measure combines the attribute information received from fuzzy rough set theory with the flexibility of the Choquet integral. This approach is designed to adeptly capture non-linear relationships within the data, acknowledging the interplay of the conditional attributes towards the decision attribute and resulting in a more flexible and accurate distance. We explore its application in the context of machine learning, with a specific emphasis on distance-based classification approaches (e.g. k-nearest neighbours). The paper examines two fuzzy rough set based measures that are based on the positive region. Moreover, we explore two procedures for monotonizing the measures derived from fuzzy rough set theory, making them suitable for use with the Choquet integral, and investigate their differences.
    
[^29]: 基于中介因素的悲观因果强化学习用于混杂的离线数据

    Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data

    [https://arxiv.org/abs/2403.11841](https://arxiv.org/abs/2403.11841)

    提出了一种新的策略学习算法，PESCAL，利用基于前门标准的中介变量消除混杂偏差，并采用悲观原则处理候选策略引起的分布变化。

    

    在现实场景中，由随机实验收集的数据集往往受到时间和预算限制而规模有限。因此，利用大规模的观测数据集成为实现高质量策略学习更具吸引力的选择。然而，大多数现有的离线强化学习（RL）方法依赖于两个关键假设-- 非混杂性和正性-- 这两个假设在观测数据环境中经常不成立。鉴于这些挑战，我们提出了一种新颖的策略学习算法，称为悲观因果学习（PESCAL）。我们利用基于前门标准的中介变量来消除混杂偏差；此外，我们采用悲观原则来解决由候选策略引起的动作分布与生成观测数据的行为策略之间的分布变化。我们的关键观察是，通过融合辅助变量

    arXiv:2403.11841v1 Announce Type: cross  Abstract: In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variable
    
[^30]: 确保安全和高质量输出：面向语言模型的指南库方法

    Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models

    [https://arxiv.org/abs/2403.11838](https://arxiv.org/abs/2403.11838)

    引入Guide-Align，一种两阶段方法，通过安全训练模型识别潜在风险，并制定特定指南，从而建立全面的指导库，用于指导LLMs生成安全和高质量输出。

    

    大型语言模型(LLMs)展示了令人印象深刻的能力，但也存在偏见内容生成和隐私问题等风险。当前的对齐技术之一包括基于原则的集成，但面临由于手工制定规则的不精确性和未经安全训练的模型对风险感知不足而产生的挑战。为了解决这些问题，我们引入了Guide-Align，这是一种两阶段方法。最初，一个经过安全训练的模型识别潜在风险，并为各种输入制定具体指南，从而建立了全面的指南库和用于输入指南检索的模型。随后，检索模型将新输入与相关指南相关联，引导LLMs在响应生成中确保安全和高质量输出，从而与人类价值观一致。另一个额外可选阶段涉及使用经过细致对齐的新数据集对模型进行微调。

    arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the
    
[^31]: 基于图神经网络的网络入侵检测系统的问题空间结构对抗攻击

    Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks

    [https://arxiv.org/abs/2403.11830](https://arxiv.org/abs/2403.11830)

    本文首次提出了针对基于图神经网络的网络入侵检测系统的形式化对抗攻击，并模拟了攻击者在现实场景中执行可行结构攻击所需考虑的问题空间约束。

    

    机器学习算法已经越来越受欢迎地用于支持网络入侵检测系统（NIDS）。然而，大量研究已经表明它们对于对抗性攻击的脆弱性，这些攻击涉及对模型输入的微小扰动，旨在危害其性能。最近的提议有效地利用图神经网络（GNN）来产生基于入侵展示的结构模式的预测，以增强检测的稳健性。然而，采用基于GNN的NIDS引入了新类型的风险。本文首次提出了专门针对GNN在网络入侵检测中形成对抗攻击的形式化。此外，我们概述并模拟了攻击者需要考虑的问题空间约束，以在现实场景中执行可行的结构攻击。作为最终贡献，我们进行了广泛的实验活动。

    arXiv:2403.11830v1 Announce Type: cross  Abstract: Machine Learning (ML) algorithms have become increasingly popular for supporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive research has shown their vulnerability to adversarial attacks, which involve subtle perturbations to the inputs of the models aimed at compromising their performance. Recent proposals have effectively leveraged Graph Neural Networks (GNN) to produce predictions based also on the structural patterns exhibited by intrusions to enhance the detection robustness. However, the adoption of GNN-based NIDS introduces new types of risks. In this paper, we propose the first formalization of adversarial attacks specifically tailored for GNN in network intrusion detection. Moreover, we outline and model the problem space constraints that attackers need to consider to carry out feasible structural attacks in real-world scenarios. As a final contribution, we conduct an extensive experimental campaign in 
    
[^32]: 评估文本到图像合成：图像质量度量的调查与分类

    Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics

    [https://arxiv.org/abs/2403.11821](https://arxiv.org/abs/2403.11821)

    评估文本到图像合成中，提出了针对图像质量的新评估指标，以确保文本和图像内容的对齐，并提出了新的分类法来归纳这些指标

    

    最近，通过利用语言和视觉结合的基础模型，推动了文本到图像合成方面的进展。这些模型在互联网或其他大规模数据库中的海量文本-图像对上进行了预训练。随着对高质量图像生成的需求转向确保文本与图像之间的内容对齐，已开发了新颖的评估度量标准，旨在模拟人类判断。因此，研究人员开始收集具有越来越复杂注释的数据集，以研究视觉语言模型的组成性及其作为文本与图像内容组成对齐质量度量的其纳入。在这项工作中，我们全面介绍了现有的文本到图像评估指标，并提出了一个新的分类法来对这些指标进行分类。我们还审查了经常采用的文本-图像基准数据集

    arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
    
[^33]: LLM的决策水平在多智能体环境中的评估究竟如何？

    How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments

    [https://arxiv.org/abs/2403.11807](https://arxiv.org/abs/2403.11807)

    通过博弈论视角评估LLMs的决策能力，结果表明GPT-3.5在稳健性方面表现良好，但泛化能力有限，而GPT-4则优于其他模型。

    

    决策是一个复杂的任务，需要各种能力，为评估大型语言模型（LLMs）提供了一个极好的框架。我们的研究通过博弈论的视角探究LLMs的决策能力。我们专注于支持多个智能体同时参与的游戏，引入了我们的框架GAMA-Bench，包括八个经典的多智能体游戏。我们设计了一个评分方案，定量评估模型在这些游戏中的表现。通过GAMA-Bench，我们研究了LLMs的稳健性、泛化能力和增强策略。结果显示，虽然GPT-3.5表现出令人满意的稳健性，但其泛化能力相对有限。然而，通过一些方法如“思维链”，其性能可以得到提高。此外，我们对各种LLMs进行评估，发现GPT-4胜过其他模型。

    arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
    
[^34]: 大型语言模型的推理能力：对抽象和推理语料库的深入分析

    Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus

    [https://arxiv.org/abs/2403.11793](https://arxiv.org/abs/2403.11793)

    使用抽象和推理语料库（ARC）数据集评估大型语言模型的推理和上下文理解能力，结果显示虽然大型语言模型具有较弱的推理能力，但在逻辑连贯性、组合性和效率方面仍然落后，实验结果有助于提出实现人类水平推理的发展路径。

    

    评估大型语言模型（LLMs）推理能力的现有方法以结果为中心，使得评估推理过程变得困难。我们引入了一种新方法，使用抽象和推理语料库（ARC）数据集以过程为中心的方式评估大型语言模型的推理和上下文理解能力。ARC要求解决问题时具有严谨的逻辑结构，这使得它成为一个能够促进模型推理能力与人类进行比较的基准。实验结果证实，虽然大型语言模型具有较弱的推理能力，但在逻辑连贯性、组合性和效率方面仍然落后。我们的实验突显了LLMs的推理能力，并提出了实现人类水平推理的发展路径。

    arXiv:2403.11793v1 Announce Type: cross  Abstract: The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.
    
[^35]: 深度中轴体：学习的解剖形状建模中轴近似

    Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling

    [https://arxiv.org/abs/2403.11790](https://arxiv.org/abs/2403.11790)

    引入了深度中轴体，一种半隐式表示，通过卷积表面实现形状重建，具有潜力。

    

    从成像体积中重建形状是医学图像分析中经常需要的操作。常见的工作流程从分割步骤开始，然后进行仔细的后处理，最后使用临时的网格化算法。由于这个序列可能耗时，神经网络被训练用于通过模板变形重建形状。这些网络提供了最先进的结果，无需手动干预，但迄今为止，它们主要在解剖形状上进行评估，而个体之间的拓扑变化很小。相比之下，其他作品偏向于学习隐式形状模型，对于网格化和可视化有多重好处。我们的工作遵循这个方向，引入了深度中轴体，一种忠实地从成像体积中近似表达拓扑骨架的半隐式表示，最终通过卷积表面实现形状重建。我们的重建技术显示出在形状重建中的潜力。

    arXiv:2403.11790v1 Announce Type: cross  Abstract: Shape reconstruction from imaging volumes is a recurring need in medical image analysis. Common workflows start with a segmentation step, followed by careful post-processing and,finally, ad hoc meshing algorithms. As this sequence can be timeconsuming, neural networks are trained to reconstruct shapes through template deformation. These networks deliver state-ofthe-art results without manual intervention, but, so far, they have primarily been evaluated on anatomical shapes with little topological variety between individuals. In contrast, other works favor learning implicit shape models, which have multiple benefits for meshing and visualization. Our work follows this direction by introducing deep medial voxels, a semi-implicit representation that faithfully approximates the topological skeleton from imaging volumes and eventually leads to shape reconstruction via convolution surfaces. Our reconstruction technique shows potential for bo
    
[^36]: 使用预训练大型语言模型构建超关联知识图

    Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models

    [https://arxiv.org/abs/2403.11786](https://arxiv.org/abs/2403.11786)

    使用零样本提示的方法，我们利用OpenAI的GPT-3.5模型从文本中成功提取了超关联知识，尽管精确率较低，但结果显示了未来研究的潜在方向。

    

    提取超关系对于构建全面的知识图至关重要，但目前针对此任务的监督方法有限。为了弥补这一差距，我们引入了一种基于零样本提示的方法，利用OpenAI的GPT-3.5模型从文本中提取超关联知识。通过将我们的模型与基准模型进行比较，我们取得了令人满意的结果，召回率达到了0.77。尽管我们的精确率目前较低，但对模型输出的详细分析揭示了未来研究领域的潜在路径。

    arXiv:2403.11786v1 Announce Type: cross  Abstract: Extracting hyper-relations is crucial for constructing comprehensive knowledge graphs, but there are limited supervised methods available for this task. To address this gap, we introduce a zero-shot prompt-based method using OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text. Comparing our model with a baseline, we achieved promising results, with a recall of 0.77. Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.
    
[^37]: Prompt-Singer: 带自然语言提示的可控唱歌声音合成

    Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt

    [https://arxiv.org/abs/2403.11780](https://arxiv.org/abs/2403.11780)

    提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。

    

    近期的唱歌声音合成(SVS)方法取得了显著的音频质量和自然度，然而它们缺乏显式控制合成唱歌风格属性的能力。我们提出Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的SVS方法。我们采用基于仅解码器的变压器模型架构，具有多尺度层次结构，并设计了一个分离音高表示的范围旋律解耦的方法，从而实现了基于文本的音域控制同时保持了旋律准确性。此外，我们探索了各种实验设置，包括不同类型的文本表示，文本编码器微调，以及引入语音数据以减轻数据稀缺性，旨在促进进一步研究。实验证明，我们的模型具有良好的控制能力和音频质量。音频示例可访问 http://prompt-singer.

    arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
    
[^38]: S-JEPA：通过动态空间注意力实现无缝跨数据集转移

    S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention

    [https://arxiv.org/abs/2403.11772](https://arxiv.org/abs/2403.11772)

    本文介绍了一项关于使用联合嵌入预测架构（JEPAs）实现脑电信号无缝跨数据集转移的探索性研究，提出了Signal-JEPA用于表示脑电记录，并展示了其在精确下游分类中的重要性。

    

    受脑电信号处理中无缝跨数据集转移挑战的启发，本文介绍了关于使用联合嵌入预测架构（JEPAs）的探索性研究。近年来，自监督学习已经成为各个领域中迁移学习的一个有前途的方法。然而，它在脑电信号中的应用仍然是一个未被充分探索的领域。本文介绍了用于表示脑电记录的Signal-JEPA，其中包括一种新颖的领域特定空间块掩蔽策略和三种新颖的用于下游分类的架构。该研究在一个54个受试者数据集上进行，模型的下游性能在三种不同的BCI范式上进行了评估：运动想象、ERP和SSVEP。我们的研究为JEPAs在脑电信号编码中的潜力提供了初步证据。值得注意的是，我们的结果突出了空间滤波对准确下游分类的重要性。

    arXiv:2403.11772v1 Announce Type: cross  Abstract: Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classific
    
[^39]: 使用元提示自动化LLMs进行零样本视觉识别

    Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs

    [https://arxiv.org/abs/2403.11755](https://arxiv.org/abs/2403.11755)

    提出了Meta-Prompting for Visual Recognition (MPVR)方法，通过仅需少量信息即可自动化零样本识别中的提示生成过程。

    

    大型语言模型（LLM）生成的类别特定提示的提示集成已经被证明是增强视觉语言模型（VLMs）零样本识别能力的有效方法。为了获得这些类别特定提示，现有方法依赖于手工为LLMs设计提示，以生成下游任务的VLM提示。然而，这需要手动编写这些任务特定提示，而且它们可能无法涵盖与感兴趣类别相关的各种视觉概念和任务特定风格。为了有效地将人类排除在循环之外，并完全自动化零样本识别的提示生成过程，我们提出了用于视觉识别的元提示（MPVR）。仅以目标任务的少量自然语言描述形式以及一系列相关类别标签作为输入，MPVR自动产生一个多样化的类别提示集。

    arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
    
[^40]: 学习古典规划领域的通用策略：超越$C_2$

    Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$

    [https://arxiv.org/abs/2403.11734](https://arxiv.org/abs/2403.11734)

    该研究提出了一种参数化版本的关系GNNs，通过在$t$为无穷大时仅使用二次空间的嵌入来近似$3$-GNNs，对于较低的$t$值，通过交换较少的消息实现弱的近似，同时通常产生了几个规划领域中所需的$C_3$特征。

    

    基于GNN的方法用于学习跨规划领域的通用策略受到$C_2$表达能力的限制，即一阶逻辑只能包含两个变量和计数。这种限制可以通过转向$k$-GNNs，其中$k=3$，其中物体嵌入被三元组嵌入所替换，来克服。然而，尽管$3$-GNNs具有$C_3$的表达能力，但不同于受限于$C_2$的$1$-和$2$-GNNs，它们需要四次时间进行消息交换和三次空间进行嵌入，使它们变得不切实际。在这项工作中，我们引入了一个参数化版本的关系GNNs。当$t$为无穷大时，R-GNN[$t$]仅使用二次空间的嵌入来近似$3$-GNNs。对于较低的$t$值，例如$t=1$和$t=2$，R-GNN[$t$]通过交换较少的消息实现了更弱的近似，但有趣的是，通常产生了在几个规划领域中所需的$C_3$特征。此外，新的R-GNN[$t$] ar

    arXiv:2403.11734v1 Announce Type: new  Abstract: GNN-based approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting. This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical. In this work, we introduce a parameterized version of relational GNNs. When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains. Furthermore, the new R-GNN[$t$] ar
    
[^41]: LLaVA-UHD：一个能感知任意长宽比和高分辨率图像的LMM

    LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images

    [https://arxiv.org/abs/2403.11703](https://arxiv.org/abs/2403.11703)

    提出了LLaVA-UHD，一个大型多模态模型，通过图像模块化策略、压缩模块和空间模式，可以高效地感知任意长宽比和高分辨率的图像

    

    arXiv:2403.11703v1 公告类型：跨文摘要：视觉编码构成了大型多模态模型（LMM）理解视觉世界的基础。传统的 LMM 处理固定大小和有限分辨率的图像，而最近在这个方向上的探索在适应性、效率甚至正确性方面有所限制。在这项工作中，我们首先以 GPT-4V 和 LLaVA-1.5 为代表例，并揭示了根植于它们的视觉编码策略中的系统性缺陷。为了解决这些挑战，我们提出了LLaVA-UHD，这是一个大型多模态模型，能够高效地感知任意长宽比和高分辨率的图像。LLaVA-UHD 包括三个关键组件：(1)一种图像模块化策略，将原始分辨率的图像分成更小的可变大小片段，以便进行高效和可扩展的编码，(2)一个压缩模块，进一步压缩来自视觉编码器的图像令牌，以及(3)一个用于为 LLMs 组织片段令牌的空间模式。全面的实验表明LLaVA-UHD

    arXiv:2403.11703v1 Announce Type: cross  Abstract: Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show th
    
[^42]: HDLdebugger: 利用大语言模型简化HDL调试

    HDLdebugger: Streamlining HDL debugging with Large Language Models

    [https://arxiv.org/abs/2403.11671](https://arxiv.org/abs/2403.11671)

    提出了一个LLM辅助的HDL调试框架 HDLdebugger，通过逆向工程方法生成HDL调试数据，提供检索增强生成的搜索引擎以及检索增强LLM微调的方法，以简化HDL调试任务。

    

    在芯片设计领域，硬件描述语言（HDLs）发挥着至关重要的作用。然而，由于HDL的复杂语法和在线资源有限的问题，即使是经验丰富的工程师，调试HDL代码仍然是一项困难且耗时的任务。因此，迫切需要开发自动化HDL代码调试模型，以减轻硬件工程师的负担。尽管大语言模型（LLMs）在生成、完成和调试软件代码方面具有强大的能力，但它们在专门领域的HDL调试中的利用受到了限制，并且迄今为止尚未产生令人满意的结果。在本文中，我们提出了一个LLM辅助的HDL调试框架，即HDLdebugger，它包括通过逆向工程方法生成HDL调试数据，用于检索增强生成的搜索引擎，以及用于检索增强LLM微调的方法。

    arXiv:2403.11671v1 Announce Type: cross  Abstract: In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through 
    
[^43]: 引导基于时间背景知识生成可解释性反事实解释来进行预测性流程监控

    Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring

    [https://arxiv.org/abs/2403.11642](https://arxiv.org/abs/2403.11642)

    通过考虑运行时的时间约束，本研究通过调整基于遗传算法的技术，在预测性流程监控中引入了时间背景知识来生成反事实解释。

    

    反事实解释指出了修改输入实例以改变人工智能系统结果应该有什么不同。然而，在预测性流程监控领域处理反事实解释时，必须仔细考虑事件之间的控制流关系。确实，一个反事实不应违反活动之间的控制流关系（即时间背景知识）。在预测性流程监控的可解释性领域中，已经有一系列关于基于结果的预测的反事实解释的作品。然而，其中没有一个在生成这些反事实时考虑了时间背景知识的包含。在这项工作中，我们改进了基于遗传算法的最先进技术，以考虑一系列运行时的时间约束来生成反事实。我们假定这种时间背景知识

    arXiv:2403.11642v1 Announce Type: new  Abstract: Counterfactual explanations suggest what should be different in the input instance to change the outcome of an AI system. When dealing with counterfactual explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered. A counterfactual, indeed, should not violate control flow relationships among activities (temporal background knowledege). Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding counterfactual explanations for outcome-based predictions. However, none of them consider the inclusion of temporal background knowledge when generating these counterfactuals. In this work, we adapt state-of-the-art techniques for counterfactual generation in the domain of XAI that are based on genetic algorithms to consider a series of temporal constraints at runtime. We assume that this temporal background knowle
    
[^44]: QEAN：四元数增强注意力网络用于视觉舞蹈生成

    QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation

    [https://arxiv.org/abs/2403.11626](https://arxiv.org/abs/2403.11626)

    提出了一种从四元数视角进行视觉舞蹈生成的四元数增强注意力网络（QEAN），可以更好地学习运动序列和音频序列的特征。

    

    音乐创生的舞蹈研究是一个新颖且具有挑战性的图像生成任务。本文提出了一种基于四元数的Quaternion-Enhanced Attention Network (QEAN)用于从四元数视角合成视觉舞蹈，其中包括自旋位置嵌入（SPE）模块和四元数旋转注意力（QRA）模块。

    arXiv:2403.11626v1 Announce Type: cross  Abstract: The study of music-generated dance is a novel and challenging Image generation task. It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music. Transformer-based methods face challenges in time series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects. This can lead to issues like joint deformation, role deviation, floating, and inconsistencies in dance movements generated in response to the music. In this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a quaternion perspective, which consists of a Spin Position Embedding (SPE) module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds position information into self-attention in a rotational manner, leading to better learning of features of movement sequences and audio sequences, and
    
[^45]: 面向具有100多个量子比特的NISQ处理器的深度量子电路最佳布局综合

    Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits

    [https://arxiv.org/abs/2403.11598](https://arxiv.org/abs/2403.11598)

    本研究提出了一种基于并行计划的SAT编码，每个时间步骤应用1个SWAP和一组CNOT。利用特定领域信息，在并行计划中保持优化性能的同时，扩展到大型深度电路。我们展示了我们方法的可扩展性，在多项比领先的精确和接近最优方法表现出色（高达100倍）。我们首次能够最优地映射多个8、14和16量子比特

    

    布局综合是将量子电路映射到量子处理器。需要为仅在连接的物理量子比特上安排2比特门的调度进行SWAP门插入。随着NISQ处理器中量子比特数量的不断增加，可扩展的布局综合至关重要。在启发式方法中观察到的优化间隙较大，需要可扩展的精确方法。尽管最近的精确和接近最优方法适用于中等规模电路，但大型深度电路仍超出范围。

    arXiv:2403.11598v1 Announce Type: cross  Abstract: Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP gate insertions are needed for scheduling 2-qubit gates only on connected physical qubits. With the ever-increasing number of qubits in NISQ processors, scalable layout synthesis is of utmost importance. With large optimality gaps observed in heuristic approaches, scalable exact methods are needed. While recent exact and near-optimal approaches scale to moderate circuits, large deep circuits are still out of scope.   In this work, we propose a SAT encoding based on parallel plans that apply 1 SWAP and a group of CNOTs at each time step. Using domain-specific information, we maintain optimality in parallel plans while scaling to large and deep circuits. From our results, we show the scalability of our approach which significantly outperforms leading exact and near-optimal approaches (up to 100x). For the first time, we can optimally map several 8, 14, and 16 qubi
    
[^46]: Linguacodus：一种在机器学习流水线中进行变革性代码生成的协同框架

    Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines

    [https://arxiv.org/abs/2403.11585](https://arxiv.org/abs/2403.11585)

    Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。

    

    在不断发展的机器学习领域中，将自然语言描述无缝转化为可执行代码仍然是一个巨大的挑战。本文介绍了Linguacodus，这是一个创新性框架，旨在通过部署一个动态流水线，通过高级数据塑形指令，将自然语言任务描述迭代地转换为代码来应对这一挑战。Linguacodus的核心是一个经过精细调整的大型语言模型（LLM），能够评估各种问题的多样解决方案，并为特定任务选择最合适的解决方案。本文详细介绍了精细调整过程，并阐明了如何将自然语言描述转化为功能性代码。Linguacodus代表了自动化代码生成的重大飞跃，有效地弥合了任务描述和可执行代码之间的差距。它对推进跨不同领域的机器学习应用具有巨大潜力。

    arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
    
[^47]: 使用标记级反馈的强化学习用于可控文本生成

    Reinforcement Learning with Token-level Feedback for Controllable Text Generation

    [https://arxiv.org/abs/2403.11558](https://arxiv.org/abs/2403.11558)

    提出了一种新的强化学习算法TOLE，通过使用标记级别奖励进行文本生成，采用了"先量子化，然后加噪声"的方法来增强算法的鲁棒性。可以在多个约束条件下灵活扩展，避免了过拟合和语义崩溃问题。

    

    为了满足现实世界应用的需求，控制大型语言模型（LLMs）的生成是至关重要的。先前的研究试图将强化学习（RL）引入可控文本生成，而大多数现有方法存在过拟合问题（微调基础方法）或语义崩溃（后处理方法）。然而，当前的RL方法通常是由粗粒度（句子/段落级别）的反馈引导的，这可能导致句子内语义扭曲或进展，从而导致性能次优。为了解决这个问题，我们提出了一种名为TOLE的新型强化学习算法，该算法制定了TOken-LEvel奖励用于可控文本生成，并采用“先量子化，然后加噪声”范式来增强强化学习算法的鲁棒性。此外，TOLE可以灵活地扩展到多个约束条件，计算成本很低。实验结果表明，我们的al

    arXiv:2403.11558v1 Announce Type: cross  Abstract: To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a "first-quantize-then-noise" paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our al
    
[^48]: LLM^3:基于大型语言模型的任务和运动规划以及运动失败推理

    LLM^3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning

    [https://arxiv.org/abs/2403.11552](https://arxiv.org/abs/2403.11552)

    LLM^3是一个基于大型语言模型的任务和运动规划框架，利用预训练的LLM具备强大的推理和规划能力，通过接口提出符号动作序列和选择连续动作参数进行运动规划，并通过运动规划的反馈来迭代优化提议，从而简化了处理领域特定消息的设计过程。

    

    传统任务和运动规划（TAMP）方法依赖于手工设计的界面，将符号任务规划与连续运动生成连接起来。这些特定领域的、劳动密集型的模块在处理现实世界设置中出现的新任务方面有限。在这里，我们提出了LLM^3，这是一个新颖的基于大型语言模型（LLM）的TAMP框架，具有领域无关的接口。具体来说，我们利用预训练的LLM的强大推理和规划能力来提出符号动作序列，并选择连续动作参数进行运动规划。关键是，LLM^3通过提示将运动规划反馈到其中，使得LLM能够通过对运动失败进行推理来迭代地优化其提议。因此，LLM^3在任务规划和运动规划之间建立接口，减轻了处理它们之间特定领域消息的复杂设计过程。通过一系列仿真

    arXiv:2403.11552v1 Announce Type: cross  Abstract: Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feed- back through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain- specific messages between them. Through a series of simulat
    
[^49]: OCR即为所需：将多模态性引入基于图像的缺陷检测系统

    OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System

    [https://arxiv.org/abs/2403.11536](https://arxiv.org/abs/2403.11536)

    引入外部模态引导的数据挖掘框架以解决自动光学检验在工业制造中面临的模型部署挑战和准确性问题。

    

    arXiv:2403.11536v1 公告类型: 跨领域 摘要: 自动光学检验（AOI）在制造过程中起着关键作用，主要利用高分辨率成像仪器进行扫描。它通过分析图像纹理或图案来检测异常，从而成为工业制造和质量控制中的重要工具。尽管它的重要性，用于AOI的模型部署经常面临挑战，包括有限的样本量阻碍了有效特征学习，源领域之间的差异以及在成像过程中光照和摄像机位置变化对其敏感性。这些因素共同影响模型预测的准确性。传统的AOI通常无法充分利用来自机器或图像内部的丰富机制参数信息，包括经常有益于AOI分类的统计参数。为了解决这个问题，我们引入了一个基于外部模态引导的数据挖掘框架，主要根植于...

    arXiv:2403.11536v1 Announce Type: cross  Abstract: Automatic optical inspection (AOI) plays a pivotal role in the manufacturing process, predominantly leveraging high-resolution imaging instruments for scanning purposes. It detects anomalies by analyzing image textures or patterns, making it an essential tool in industrial manufacturing and quality control. Despite its importance, the deployment of models for AOI often faces challenges. These include limited sample sizes, which hinder effective feature learning, variations among source domains, and sensitivities to changes in lighting and camera positions during imaging. These factors collectively compromise the accuracy of model predictions. Traditional AOI often fails to capitalize on the rich mechanism-parameter information from machines or inside images, including statistical parameters, which typically benefit AOI classification. To address this, we introduce an external modality-guided data mining framework, primarily rooted in o
    
[^50]: 端到端水下视频增强：数据集与模型

    End-To-End Underwater Video Enhancement: Dataset and Model

    [https://arxiv.org/abs/2403.11506](https://arxiv.org/abs/2403.11506)

    提出了一个新的端到端水下视频增强模型UVENet，利用帧间关系来提高水下视频的可见性和帧质量，构建了适用于UVE任务的合成数据集SUVE，填补了现有方法中缺乏的监督数据集和模型。

    

    水下视频增强（UVE）旨在提高水下视频的可见性和帧质量，对海洋研究和探索具有重要意义。然而，现有方法主要集中在开发图像增强算法，以独立增强每一帧。目前缺乏专门针对UVE任务定制的受监督数据集和模型。为填补这一空白，我们构建了合成水下视频增强（SUVE）数据集，包括840个多样化的水下风格视频，配对真实视频作为参考。基于这个数据集，我们训练了一种新颖的水下视频增强模型UVENet，利用帧间关系实现更好的增强性能。通过在合成和真实水下视频上的大量实验，我们展示了我们方法的有效性。据我们所知，这项研究代表了对UVE的首次全面探索。

    arXiv:2403.11506v1 Announce Type: cross  Abstract: Underwater video enhancement (UVE) aims to improve the visibility and frame quality of underwater videos, which has significant implications for marine research and exploration. However, existing methods primarily focus on developing image enhancement algorithms to enhance each frame independently. There is a lack of supervised datasets and models specifically tailored for UVE tasks. To fill this gap, we construct the Synthetic Underwater Video Enhancement (SUVE) dataset, comprising 840 diverse underwater-style videos paired with ground-truth reference videos. Based on this dataset, we train a novel underwater video enhancement model, UVENet, which utilizes inter-frame relationships to achieve better enhancement performance. Through extensive experiments on both synthetic and real underwater videos, we demonstrate the effectiveness of our approach. This study represents the first comprehensive exploration of UVE to our knowledge. The c
    
[^51]: MLVICX: 用于胸部X射线自监督表征学习的多级方差-协方差探索

    MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning

    [https://arxiv.org/abs/2403.11504](https://arxiv.org/abs/2403.11504)

    MLVICX是一种用于胸部X射线自监督表征学习的方法，通过多级方差和协方差探索策略来捕获丰富的表征。

    

    自监督学习（SSL）在减少手动注释需求、使深度学习模型可用于医学图像分析任务方面具有潜在用途。通过利用从未标记数据中学到的表示，自监督模型在无需或只需少量微调的任务上表现良好。然而，对于具有复杂解剖结构和多样临床病情的医学图像，如胸部X射线，需要一种能够编码细粒度细节并保留更广泛上下文信息的表征学习技术。在这种情况下，我们介绍了MLVICX（用于胸部X射线自监督表征学习的多级方差-协方差探索），一种从胸部X射线图像中捕获富表示形式的方法。我们方法的核心是一种新颖的多级方差和协方差探索策略，使其更有能力

    arXiv:2403.11504v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning. However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for representation learning techniques that can encode fine-grained details while preserving the broader contextual information. In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning), an approach to capture rich representations in the form of embeddings from chest X-ray images. Central to our approach is a novel multi-level variance and covariance exploration strategy that empowers t
    
[^52]: MCD: 用于机器人感知的多样性大规模多校区数据集

    MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception

    [https://arxiv.org/abs/2403.11496](https://arxiv.org/abs/2403.11496)

    MCD数据集为机器人感知领域带来了全新挑战，包含多种传感模式、高精度地面真值和跨校园的多样化挑战环境，同时还引入了对大规模NRE激光扫描进行的语义注释。

    

    感知在各种机器人应用中起着至关重要的作用。然而，现有的标注数据集对自动驾驶场景存在偏见，而未标记的SLAM数据集很快被过度拟合，通常缺乏环境和领域变化。为了拓展这些领域的边界，我们引入了一个全面的名为MCD（多校区数据集）的数据集，涵盖了多种传感模式、高精度地面真值以及跨越三个欧亚大学校园的多样化挑战环境。MCD包括经典圆柱旋转（CCS）和非重复回环（NRE）激光雷达、高质量IMU（惯性测量单元）、摄像头和UWB（超宽带）传感器。此外，在开创性的努力下，我们针对三个领域的59k个稀疏NRE激光雷达扫描引入了29类的语义注释，从而为现有的语义分割研究提供了一个全新挑战。

    arXiv:2403.11496v1 Announce Type: cross  Abstract: Perception plays a crucial role in various robot applications. However, existing well-annotated datasets are biased towards autonomous driving scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often lack environment and domain variations. To expand the frontier of these fields, we introduce a comprehensive dataset named MCD (Multi-Campus Dataset), featuring a wide range of sensing modalities, high-accuracy ground truth, and diverse challenging environments across three Eurasian university campuses. MCD comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce semantic annotations of 29 classes over 59k sparse NRE lidar scans across three domains, thus providing a novel challenge to existing semantic segmentation research upon this largely u
    
[^53]: 具有时间动态的路网语义增强表示学习

    Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics

    [https://arxiv.org/abs/2403.11495](https://arxiv.org/abs/2403.11495)

    提出了一种名为Toast的新框架，以及增强版DyToast，用于学习路网的通用表示，并增强了时间动态的整合，以提高各种时间敏感下游任务的性能。

    

    在这项研究中，我们引入了一个名为Toast的新颖框架，用于学习路网的通用表示，以及其增强版DyToast，旨在增强时间动态的整合，提高各种时间敏感下游任务的性能。具体来说，我们提出编码路网固有的两个关键语义特征：交通模式和行驶语义。为实现此目的，我们通过纳入旨在预测与目标路段相关的交通上下文的辅助目标，改进了skip-gram模块。此外，我们利用轨迹数据，并设计基于Transformer的预训练策略，以在路网上提炼行驶语义。DyToast通过使用具有益处特性的统一三角函数进一步增进了该框架，使其能够捕获路网的时间演变和动态性质。

    arXiv:2403.11495v1 Announce Type: cross  Abstract: In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road netwo
    
[^54]: SmartRefine: 一种场景自适应细化框架，用于高效运动预测

    SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient Motion Prediction

    [https://arxiv.org/abs/2403.11492](https://arxiv.org/abs/2403.11492)

    SmartRefine提出了一种场景自适应细化策略，可在最小的额外计算量下对运动预测进行细化

    

    预测周围代理的未来运动对自动驾驶车辆（AVs）在动态的、人机混合环境中安全运行至关重要。上下文信息，如道路地图和周围代理的状态，为运动行为预测提供关键的几何和语义信息。本文引入了一种新颖的场景自适应细化策略，称为SmartRefine，以最小的额外计算量对预测进行细化。具体而言，SmartRefine可以根据每个场景的特性全面调整细化配置，并通过引入一种质量来智能地选择细化迭代的数量

    arXiv:2403.11492v1 Announce Type: cross  Abstract: Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a qualit
    
[^55]: LLM能生成类人行路指示吗？走向跨平台的具身指令综合

    Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis

    [https://arxiv.org/abs/2403.11487](https://arxiv.org/abs/2403.11487)

    提出了一种新方法，利用LLM以及上下文学习，实现了自动生成具身机器人的“行路指示”，并且在多个模拟平台上展示出跨平台特性。

    

    我们提出了一种新颖的方法，用于自动合成“行路指示”以指导具身机器人。与先前的方法相比，这种方法不再依赖于仅设计用于特定模拟平台的人工注释数据集，而是使用上下文学习来调节LLM，以使用少量参考生成指示。我们使用基于LLM的视觉问答策略收集环境的详细信息，LLM用于指令合成。我们将我们的方法实现在多个模拟平台上，包括Matterport3D、AI Habitat和ThreeDWorld，从而展示了其跨平台特性。我们通过用户研究主观评估了我们的方法，观察到83.3%的用户认为合成的指示准确捕捉了环境的细节，并表现出与人类生成的指示类似的特征。

    arXiv:2403.11487v1 Announce Type: cross  Abstract: We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we con
    
[^56]: 开放世界半监督学习用于节点分类

    Open-World Semi-Supervised Learning for Node Classification

    [https://arxiv.org/abs/2403.11483](https://arxiv.org/abs/2403.11483)

    方差不平衡可能对模型性能产生负面影响，提出一种不依赖预训练图编码器的有效方法

    

    开放世界半监督学习 (Open-world SSL) 用于节点分类，在图形社区中是一个实用但未被充分探索的问题，它将未标记的节点分类为已见类或多个新颖类。根据经验和理论分析，我们发现方差不平衡可能对模型性能产生负面影响。预训练特征编码器可以通过为新颖类生成紧凑表示来缓解这个问题。然而，为各种类型的图形数据创建通用预训练编码器被证明是具有挑战性的。因此，需要一种不依赖预训练图编码器的有效方法。

    arXiv:2403.11483v1 Announce Type: cross  Abstract: Open-world semi-supervised learning (Open-world SSL) for node classification, that classifies unlabeled nodes into seen classes or multiple novel classes, is a practical but under-explored problem in the graph community. As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes). Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance. Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes. However, creating general pre-trained encoders for various types of graph data has been proven to be challenging. As such, there is a demand for an effective method that does not rely on pre-trained graph encoders. In this paper, we propose an IMbalanc
    
[^57]: 词序的影响：重新排序和生成分析的洞察

    Word Order's Impacts: Insights from Reordering and Generation Analysis

    [https://arxiv.org/abs/2403.11473](https://arxiv.org/abs/2403.11473)

    本文通过顺序重建的角度重新审视了关于词序的假设，并通过选择不同范围的数据集进行了实验证明，ChatGPT依赖于词序进行推断，但不能明确支持或否定词序与词汇语义之间的冗余关系。

    

    现有研究已经研究了自然语言文本中词语顺序的影响。它们通常通过打乱原始词序来创建一个混乱的序列进行分析，然后比较模型在原始序列和混乱序列之间的表现。实验结果表明存在一些小幅下降。考虑到这一发现，提出了不同的关于词序的假设，包括“词序与词汇语义重复”和“模型不依赖于词序”。在本文中，我们通过增加一个顺序重建的视角，选择不同范围的数据集来重新审视上述假设。具体而言，我们首先选择了四个不同的数据集，然后设计了顺序重建和连续生成任务。实证研究支持ChatGPT依赖于词序进行推断，但无法支持或否定词序与词汇语义之间的冗余关系。

    arXiv:2403.11473v1 Announce Type: cross  Abstract: Existing works have studied the impacts of the order of words within natural text. They usually analyze it by destroying the original order of words to create a scrambled sequence, and then comparing the models' performance between the original and scrambled sequences. The experimental results demonstrate marginal drops. Considering this findings, different hypothesis about word order is proposed, including ``the order of words is redundant with lexical semantics'', and ``models do not rely on word order''. In this paper, we revisit the aforementioned hypotheses by adding a order reconstruction perspective, and selecting datasets of different spectrum. Specifically, we first select four different datasets, and then design order reconstruction and continuing generation tasks. Empirical findings support that ChatGPT relies on word order to infer, but cannot support or negate the redundancy relations between word order lexical semantics.
    
[^58]: Collage Prompting: 与GPT-4V合作的经济可行的视觉识别

    Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V

    [https://arxiv.org/abs/2403.11468](https://arxiv.org/abs/2403.11468)

    通过引入Collage Prompting方法，我们实现了与GPT-4V合作的经济可行的视觉识别方法，通过优化图像排列顺序获得最大的识别准确性。

    

    最近生成式人工智能的进展表明，通过采用视觉提示，GPT-4V可以在图像识别任务中展现出显著的熟练度。尽管其令人印象深刻的能力，但与GPT-4V的推断相关的财务成本构成了其广泛应用的重大障碍。为了解决这一挑战，我们的研究引入了Collage Prompting，这是一种经济实惠的提示方法，将多个图像连接成单个视觉输入。借助拼贴提示，GPT-4V可以同时在多幅图像上执行图像识别。基于GPT-4V的图像识别准确性与拼贴提示中图像顺序明显变化的观察，我们的方法进一步学习优化图像安排以获得最大的识别准确性。训练了一个图预测器来指示每个拼贴提示的准确性，然后我们提出了一种优化方法来导航搜索空间。

    arXiv:2403.11468v1 Announce Type: cross  Abstract: Recent advancements in generative AI have suggested that by taking visual prompt, GPT-4V can demonstrate significant proficiency in image recognition task. Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents a substantial barrier for its wide use. To address this challenge, our work introduces Collage Prompting, a budget-friendly prompting approach that concatenates multiple images into a single visual input. With collage prompt, GPT-4V is able to perform image recognition on several images simultaneously. Based on the observation that the accuracy of GPT-4V's image recognition varies significantly with the order of images within the collage prompt, our method further learns to optimize the arrangement of images for maximum recognition accuracy. A graph predictor is trained to indicate the accuracy of each collage prompt, then we propose an optimization method to navigate the search space
    
[^59]: HateCOT：通过大型语言模型进行泛化攻击性言论检测的解释增强数据集

    HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models

    [https://arxiv.org/abs/2403.11456](https://arxiv.org/abs/2403.11456)

    HateCOT数据集通过GPT-3.5-Turbo生成解释，将52,000个样本数据用于预训练模型，显著提升了在不同领域和任务下的攻击性内容检测效果。

    

    社交媒体的普及导致了对攻击性内容的可靠高效检测的需求，为了限制其有害影响。这导致了大量与检测攻击性内容相关的数据集和模型的出现。本文介绍了HateCOT，这是从多样化现有来源中抽取的5.2万个样本数据集，其中包含由GPT-3.5-Turbo和人工精心制作的解释。我们展示了在HateCOT上为攻击性内容检测预训练模型在零-shot和few-shot设置下显著改进了开源语言模型在三个基准数据集上的表现，尽管在领域和任务方面存在差异。

    arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
    
[^60]: 深度强化学习驱动的自主车辆决策的揭秘

    Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making

    [https://arxiv.org/abs/2403.11432](https://arxiv.org/abs/2403.11432)

    本研究致力于研究基于注意力的DRL框架的可解释性，在自主车辆决策中，通过在开源AV仿真环境中添加多头注意力框架，提高了模型的解释性。

    

    随着强化学习领域中通用函数逼近器的出现，利用深度强化学习（DRL）的实际应用数量激增。自动驾驶任务中的决策制定已成为其中一项主要应用，将传感器数据或高阶运动学变量作为输入，并提供离散选择或连续控制输出。然而，模型的黑盒特性限制了DRL在自主车辆中的实际部署。因此，在这项研究工作中，我们关注基于注意力的DRL框架的可解释性。我们在开源AV仿真环境中使用了基于连续近端策略优化的DRL算法作为基线模型，并添加了一个多头注意力框架。我们提供了一些分析技术来讨论训练模型的可解释性。

    arXiv:2403.11432v1 Announce Type: cross  Abstract: With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment. We provide some analytical techniques for discussing the interpretability of the trained mode
    
[^61]: 神经网络表示量子系统

    Neural network representation of quantum systems

    [https://arxiv.org/abs/2403.11420](https://arxiv.org/abs/2403.11420)

    利用神经网络的通用逼近定理，我们提出了一个新颖的映射方法，将广泛类的量子力学系统表示为神经网络形式，从而可以对Feynman路径积分中的任意路径进行统计求和。

    

    已经提出，接近高斯过程的随机宽神经网络是围绕高斯固定点的量子场论。在本文中，我们提供了一种新颖的映射，通过该映射，可以将一大类量子力学系统表达为具有对网络参数的统计求和形式的神经网络。我们的简单思想是利用神经网络的通用逼近定理生成费曼路径积分中的任意路径。这种映射可以应用于相互作用的量子系统/场论，即使远离高斯极限。我们的发现使机器学习与量子世界更加接近。

    arXiv:2403.11420v1 Announce Type: cross  Abstract: It has been proposed that random wide neural networks near Gaussian process are quantum field theories around Gaussian fixed points. In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters. Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman's path integral. The map can be applied to interacting quantum systems / field theories, even away from the Gaussian limit. Our findings bring machine learning closer to the quantum world.
    
[^62]: 时间轨迹的变分采样

    Variational Sampling of Temporal Trajectories

    [https://arxiv.org/abs/2403.11418](https://arxiv.org/abs/2403.11418)

    本文介绍了一种通过在函数空间中显式参数化过渡函数来学习轨迹分布的机制，实现了对轨迹的高效合成和推断。

    

    一个确定性的时间过程可以通过其轨迹来确定，该轨迹是(a)初始条件$z_0 \in \mathcal{Z}$和(b)过渡函数$f:(\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$的乘积空间中的元素，往往受底层动力系统控制的影响。现有方法通常将过渡函数建模为微分方程或循环神经网络。尽管它们在预测未来测量方面很有效，但很少有结果成功建立使用神经网络进行轨迹采样和统计推断的方法，部分原因是参数化方面的约束。在这项工作中，我们引入了一种机制，通过将过渡函数$f$显式地参数化为函数空间中的一个元素来学习轨迹的分布。我们的框架允许有效地合成新颖的轨迹，同时还直接提供了一个方便的工具来进行推断。

    arXiv:2403.11418v1 Announce Type: cross  Abstract: A deterministic temporal process can be determined by its trajectory, an element in the product space of (a) initial condition $z_0 \in \mathcal{Z}$ and (b) transition function $f: (\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$ often influenced by the control of the underlying dynamical system. Existing methods often model the transition function as a differential equation or as a recurrent neural network. Despite their effectiveness in predicting future measurements, few results have successfully established a method for sampling and statistical inference of trajectories using neural networks, partially due to constraints in the parameterization. In this work, we introduce a mechanism to learn the distribution of trajectories by parameterizing the transition function $f$ explicitly as an element in a function space. Our framework allows efficient synthesis of novel trajectories, while also directly providing a convenient tool for inferen
    
[^63]: DreamSampler：统一扩散采样和分数蒸馏以用于图像处理

    DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation

    [https://arxiv.org/abs/2403.11415](https://arxiv.org/abs/2403.11415)

    DreamSampler框架通过整合反向扩散采样和分数蒸馏，提供了模型无关的图像处理方法，解决了分数蒸馏易崩溃的问题，并在图像编辑和重构中展现了竞争力。

    

    反向采样和分数蒸馏已成为最近几年使用潜在扩散模型（LDMs）进行图像处理的主要工具。虽然反向扩散采样通常需要调整LDM架构或特征工程，分数蒸馏提供了一种简单而强大的与模型无关的方法，但往往容易发生模式崩溃。为了解决这些局限性并利用这两种方法的优势，我们引入了一个称为DreamSampler的新颖框架，通过正则化潜在优化的视角无缝地整合了这两种不同的方法。类似于分数蒸馏，DreamSampler是一种适用于任何LDM架构的模型无关方法，但它允许在图像编辑和重构中进行蒸馏和反向采样，并提供额外的指导。通过涉及图像编辑、SVG重构等实验，我们展示了竞争力

    arXiv:2403.11415v1 Announce Type: cross  Abstract: Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive pe
    
[^64]: 拥抱生成AI革命：用GPT推进网络安全的高等教育

    Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT

    [https://arxiv.org/abs/2403.11402](https://arxiv.org/abs/2403.11402)

    生成AI技术如ChatGPT对网络安全教育具有重大影响，研究强调了大学应对课程进行调整以适应行业需求。

    

    随着生成人工智能（AI）技术的快速发展，特别是如ChatGPT等Generative Pre-trained Transformer（GPT）模型，将对网络安全产生重大影响。本研究调查了GPT，特别是ChatGPT在网络安全高等教育中的影响，并就大学应如何调整课程内容以满足行业不断发展的需求提供建议。我们的研究强调了理解GPT的“心智模型”与人类认知之间的一致性的重要性，以及提高GPT能力以符合布鲁姆分类法中的人类技能。通过分析当前的教育实践以及课程内容与行业需求的一致性，我们得出结论：提供网络安全等实用学位的大学应与行业需求密切对接，拥抱不可避免的生成AI革命，并应用strik

    arXiv:2403.11402v1 Announce Type: cross  Abstract: The rapid advancement of generative Artificial Intelligence (AI) technologies, particularly Generative Pre-trained Transformer (GPT) models such as ChatGPT, has the potential to significantly impact cybersecurity. In this study, we investigated the impact of GPTs, specifically ChatGPT, on tertiary education in cybersecurity, and provided recommendations for universities to adapt their curricula to meet the evolving needs of the industry. Our research highlighted the importance of understanding the alignment between GPT's ``mental model'' and human cognition, as well as the enhancement of GPT capabilities to human skills based on Bloom's taxonomy. By analyzing current educational practices and the alignment of curricula with industry requirements, we concluded that universities providing practical degrees like cybersecurity should align closely with industry demand and embrace the inevitable generative AI revolution, while applying stri
    
[^65]: Scene-LLM：扩展用于3D视觉理解和推理的语言模型

    Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning

    [https://arxiv.org/abs/2403.11401](https://arxiv.org/abs/2403.11401)

    本文介绍了Scene-LLM，一种3D视觉语言模型，通过整合大型语言模型（LLMs）的推理优势，增强了在交互式3D室内环境中具身体存在的代理者能力。

    

    本文介绍了Scene-LLM，这是一种3D视觉语言模型，通过整合大型语言模型（LLMs）的推理优势，增强了在交互式3D室内环境中具身体存在的代理者能力。Scene-LLM采用了混合的3D视觉特征表示，包括了密集的空间信息，并支持场景状态的更新。该模型使用投影层将这些特征高效地投影到预训练的文本嵌入空间中，从而有效解释3D视觉信息。我们方法的独特之处在于整合了场景级和以自我为中心的3D信息。这种组合对于交互式规划至关重要，其中场景级数据支持全局规划，以自我为中心的数据对于定位至关重要。值得注意的是，我们使用以自我为中心的3D帧特征进行特征对齐，这是一种增强模型对小物体特征对齐能力的有效技术。

    arXiv:2403.11401v1 Announce Type: cross  Abstract: This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and ego-centric 3D information. This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization. Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the s
    
[^66]: 用于深度学习和大数据应用的自动化数据处理和特征工程：一项调查

    Automated data processing and feature engineering for deep learning and big data applications: a survey

    [https://arxiv.org/abs/2403.11395](https://arxiv.org/abs/2403.11395)

    现代人工智能方法旨在设计能够直接从数据中学习的算法，自动化数据处理任务的兴起驱动了机器学习和大数据应用中利用大量复杂数据的发展。

    

    现代人工智能（AI）的方法旨在设计能够直接从数据中学习的算法。这种方法取得了令人印象深刻的成果，并在AI的发展中做出了重要贡献，特别是在监督深度学习领域。它也简化了机器学习系统的设计，因为学习过程是高度自动化的。然而，并非所有传统深度学习流程中的数据处理任务都已自动化。在大多数情况下，数据必须在可以用于训练之前经过手动收集、预处理并通过数据增强进一步扩展。最近，出现了用于自动化这些任务的特殊技术。数据处理任务的自动化驱动力是利用大量复杂、异构数据进行机器学习和大数据应用。如今，基于自动化机器学习（A

    arXiv:2403.11395v1 Announce Type: cross  Abstract: Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (A
    
[^67]: LLM增强型自主代理能够合作吗？通过融合盆评估它们的合作能力

    Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot

    [https://arxiv.org/abs/2403.11381](https://arxiv.org/abs/2403.11381)

    本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。

    

    随着人工智能领域的不断发展，其中一个重要维度是发展大型语言模型及其潜力增强多代理人人工智能系统。本文通过使用著名的Meltin Pot环境以及参考模型如GPT4和GPT3.5，探讨了大型语言模型增强的自主代理(LAAs)的合作能力。初步结果表明，尽管这些代理人表现出合作的倾向，但在特定环境中仍然难以实现有效的协作，强调了更强大架构的需求。本研究的贡献包括一个用于适应LLM的Melting Pot游戏场景的抽象化层，一个用于LLM中介代理开发的可重用架构-包括短期和长期记忆以及不同的认知模块，并使用一组方法评估合作能力。

    arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
    
[^68]: 基于LLM的驾驶员代理的驾驶风格调整

    Driving Style Alignment for LLM-powered Driver Agent

    [https://arxiv.org/abs/2403.11368](https://arxiv.org/abs/2403.11368)

    通过人类驾驶行为演示和反馈，提出了一个多对齐框架，用于将驾驶代理与人类驾驶风格对齐，并通过模拟实验验证了其有效性

    

    最近，LLM技术驱动的驾驶代理在自动驾驶领域展现出了相当大的潜力，展示出类似人类的推理和决策能力。然而，关于使驾驶代理行为与人类驾驶风格对齐的当前研究仍然有限，部分原因是由于缺乏来自人类驾驶行为的高质量自然语言数据。为了填补这一研究空白，我们提出了一个多对齐框架，旨在通过演示和反馈将驾驶代理与人类驾驶风格对齐。值得注意的是，我们通过自然驾驶实验和驾驶后访谈构建了一个人类驾驶行为的自然语言数据集，为LLM的对齐提供了高质量的人类演示。该框架的有效性通过在CARLA城市交通模拟器中的仿真实验得以验证，并得到了人类评估的进一步证实。我们的研究提供了有价值的见解，可以指导设计

    arXiv:2403.11368v1 Announce Type: cross  Abstract: Recently, LLM-powered driver agents have demonstrated considerable potential in the field of autonomous driving, showcasing human-like reasoning and decision-making abilities.However, current research on aligning driver agent behaviors with human driving styles remains limited, partly due to the scarcity of high-quality natural language data from human driving behaviors.To address this research gap, we propose a multi-alignment framework designed to align driver agents with human driving styles through demonstrations and feedback. Notably, we construct a natural language dataset of human driver behaviors through naturalistic driving experiments and post-driving interviews, offering high-quality human demonstrations for LLM alignment. The framework's effectiveness is validated through simulation experiments in the CARLA urban traffic simulator and further corroborated by human evaluations. Our research offers valuable insights into desi
    
[^69]: IGANN Sparse: 用非线性洞察力连接稀疏性和可解释性

    IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight

    [https://arxiv.org/abs/2403.11363](https://arxiv.org/abs/2403.11363)

    IGANN Sparse是一种新颖的机器学习模型，通过训练过程中的非线性特征选择促进稀疏性，确保在不影响预测性能的情况下提高模型的可解释性。

    

    特征选择是预测性分析中的关键组成部分，显著影响模型的预测准确性和解释性。本文提出了IGANN Sparse，这是一种新颖的机器学习模型，它来自广义加法模型家族，通过训练过程中的非线性特征选择促进稀疏性，从而确保通过改进模型稀疏性来实现可解释性而不影响预测性能。

    arXiv:2403.11363v1 Announce Type: cross  Abstract: Feature selection is a critical component in predictive analytics that significantly affects the prediction accuracy and interpretability of models. Intrinsic methods for feature selection are built directly into model learning, providing a fast and attractive option for large amounts of data. Machine learning algorithms, such as penalized regression models (e.g., lasso) are the most common choice when it comes to in-built feature selection. However, they fail to capture non-linear relationships, which ultimately affects their ability to predict outcomes in intricate datasets. In this paper, we propose IGANN Sparse, a novel machine learning model from the family of generalized additive models, which promotes sparsity through a non-linear feature selection process during training. This ensures interpretability through improved model sparsity without sacrificing predictive performance. Moreover, IGANN Sparse serves as an exploratory tool
    
[^70]: 溶剂感知的2D核磁共振预测：利用多任务训练和迭代自训练策略

    Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies

    [https://arxiv.org/abs/2403.11353](https://arxiv.org/abs/2403.11353)

    该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。

    

    核磁共振（NMR）光谱在各个科学领域中起着关键作用，提供了有关分子的结构信息、电子性质和动态行为的见解。准确的NMR光谱预测能够高效地生成候选分子，使化学家能够将它们与实际实验光谱进行比较。该过程有助于确认分子结构或指出差异，引导进一步的研究。机器学习（ML）已经成为一种有前途的替代方法，用于根据分子结构预测分子的原子NMR化学位移。虽然在预测一维（1D）NMR方面已经取得了显著进展，但通过机器学习进行二维（2D）NMR预测仍然是一项挑战，因为缺乏用于训练的标注的NMR数据集。为了解决这一差距，我们提出了一种迭代自训练（IST）方法，用于训练深度学习模型，以预测原子2DNMR位移。

    arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
    
[^71]: COLEP: 通过概率电路实现可证实鲁棒学习推理一致性预测

    COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits

    [https://arxiv.org/abs/2403.11348](https://arxiv.org/abs/2403.11348)

    通过概率电路，提出了COLEP框架，实现了可证实鲁棒学习推理一致性预测，其特点在于训练统计模型学习不同语义概念，并利用概率电路实现精确高效推理

    

    参考类型：交叉  摘要：一致性预测已经显示出在为任意黑匣子机器学习模型构建统计严谨的预测集方面表现出色，假设数据是可交换的。然而，即使在推理过程中进行微小的对抗性扰动也可能违反可交换性假设，挑战覆盖率保证，并导致后续实证覆盖率的下降。在这项工作中，我们提出了一个通过概率电路实现可证实鲁棒学习推理的一致性预测框架（COLEP），其中包括一个数据驱动的学习组件，用于训练统计模型以学习不同的语义概念，以及一个用于编码知识并表征训练模型之间关系的推理组件。为了实现精确和高效的推理，我们在推理组件内部使用了概率电路（PCs）。在理论上，我们提供了完整的预测认证

    arXiv:2403.11348v1 Announce Type: cross  Abstract: Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. In this work, we propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning. To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component. Theoretically, we provide end-to-end certification of predict
    
[^72]: CantonMT: 汉英NMT平台，使用合成反向翻译数据对模型进行微调

    CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data

    [https://arxiv.org/abs/2403.11346](https://arxiv.org/abs/2403.11346)

    提出了CantonMT项目，利用合成反向翻译数据对粤语至英语NMT模型进行微调，并为研究人员提供用户友好的界面和开源工具包，以促进研究

    

    arXiv:2403.11346v1 消息类型：跨领域 摘要：对于低资源语言的神经机器翻译(NMT)仍然是自然语言处理研究人员面临的挑战。在这项工作中，我们将一个标准的数据增强方法——反向翻译，应用到了新的语言翻译方向粤语至英语。我们介绍了我们使用有限数量真实数据和生成的合成数据(包括OpusMT, NLLB,和mBART)进行微调的模型。我们使用了一系列不同指标包括基于词汇和嵌入的自动评估。此外，我们为这项\textsc{CantonMT}研究项目中包含的模型创建了一个用户友好的界面，并提供便利实现粤语至英语MT研究。研究人员可以通过我们的开源\textsc{CantonMT}工具包\url{https://github.com/kenrickkung/CantoneseTranslation}向平台添加更多模型。

    arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
    
[^73]: 独立强化学习用于合作竞争Agent：均场视角

    Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective

    [https://arxiv.org/abs/2403.11345](https://arxiv.org/abs/2403.11345)

    本文从均场视角研究了独立强化学习在合作竞争代理中的应用，提出了一种可实现纳什均衡的线性二次结构RL方法，并通过考虑无限代理数量的情况来解决有限人口环境中的非稳态性问题。

    

    在本文中，我们研究了分成团队的代理之间的强化学习（RL），每个团队内部存在合作，但不同团队之间存在非零和的竞争。为了开发一种可以明确实现纳什均衡的RL方法，我们专注于线性二次结构。此外，为了解决有限人口环境中由多智能体交互引起的非稳态性，我们考虑每个团队内代理数量无限的情况，即均场设置。这导致了一个广义和的LQ均场类型博弈（GS-MFTGs）。我们在标准逆可逆条件下表征了GS-MFTG的纳什均衡（NE）。然后证明了这个MFTG NE在有限人口博弈中为$\mathcal{O}(1/M)$-NE，其中$M$是每个团队中代理数量的下界。这些结构性结果推动了一个名为多玩家递进式自然Pol的算法。

    arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
    
[^74]: 利用基于深度学习的关键点预测提升视频运动传输应用的带宽效率

    Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction

    [https://arxiv.org/abs/2403.11337](https://arxiv.org/abs/2403.11337)

    本研究提出了一种基于深度学习的关键点预测框架，通过将关键点预测和视频帧合成结合应用在视频运动传输中，提升了带宽效率。

    

    我们提出了一个基于深度学习的新型预测框架，用于增强运动转移视频应用（如视频会议、虚拟现实游戏和个人健康监测隐私保护）中的带宽减少。为了建模复杂运动，我们使用代表动态对象的第一阶动作模型（FOMM），该模型利用学习到的关键点及其局部仿射变换。关键点由自监督关键点检测器提取，并按照视频帧对应的时间序列组织。关键点的预测是通过变分递归神经网络（VRNN）进行的，以便在源设备上使用更低的帧率进行传输。然后利用光流估计器和生成器网络，将预测的关键点合成为视频帧。在视频动画传输中，利用关键点表示和基于VRNN的预测的有效性

    arXiv:2403.11337v1 Announce Type: cross  Abstract: We propose a deep learning based novel prediction framework for enhanced bandwidth reduction in motion transfer enabled video applications such as video conferencing, virtual reality gaming and privacy preservation for patient health monitoring. To model complex motion, we use the First Order Motion Model (FOMM) that represents dynamic objects using learned keypoints along with their local affine transformations. Keypoints are extracted by a self-supervised keypoint detector and organized in a time series corresponding to the video frames. Prediction of keypoints, to enable transmission using lower frames per second on the source device, is performed using a Variational Recurrent Neural Network (VRNN). The predicted keypoints are then synthesized to video frames using an optical flow estimator and a generator network. This efficacy of leveraging keypoint based representations in conjunction with VRNN based prediction for both video ani
    
[^75]: 通过将一个全局明确标注拆解成本地隐式多模态反馈来改进对话代理

    Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback

    [https://arxiv.org/abs/2403.11330](https://arxiv.org/abs/2403.11330)

    通过将全局明确标注拆解成本地隐式多模态反馈，提出了一种改进对话代理的方法，并在各种对话度量方面展现出一致的改进

    

    我们描述了一种方法，通过全局（即，对话级）奖励对齐基于LLM的对话代理，同时考虑到自然发生的多模态信号。在高层次上，我们的方法（名为GELI）通过将人类提供的全局明确（GE）会话级奖励拆分，利用本地隐式（LI）多模态奖励信号来跨模态地塑造奖励分解步骤。然后将这种分解的奖励模型作为标准RHLF流程的一部分，来改进基于LLM的对话代理。我们进行了定量和定性的人类研究，评估了我们的GELI方法的性能，并发现与基线方法相比，它在各种对话度量方面都表现出一致的改进。

    arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
    
[^76]: 领域引导的遮罩自动编码器用于独特球员识别

    Domain-Guided Masked Autoencoders for Unique Player Identification

    [https://arxiv.org/abs/2403.11328](https://arxiv.org/abs/2403.11328)

    设计了一种新颖的领域引导遮罩策略d-MAE，用于在存在运动模糊的情况下进行球员识别的稳健特征提取。

    

    独特球员识别是视觉驱动体育分析中的一个基本模块。从广播视频中识别球员可以帮助各种下游任务，如球员评估、比赛分析和转播制作。然而，使用深度特征自动检测球衣号码具有挑战性，主要原因包括：a）运动模糊，b）低分辨率视频信号，和c）遮挡。遮罩自动编码器（MAEs）以在各种视觉任务中取得成功而脱颖而出，成为传统特征提取器的优越替代品。然而，大多数MAEs只是简单地将图像补丁置零，要么随机要么关注于在哪里遮罩，而不是如何遮罩。受人类视觉的启发，我们设计了一种新颖的领域引导遮罩策略，用于促进d-MAE以提高在运动模糊存在的情况下进行球员识别的稳健特征提取。

    arXiv:2403.11328v1 Announce Type: cross  Abstract: Unique player identification is a fundamental module in vision-driven sports analytics. Identifying players from broadcast videos can aid with various downstream tasks such as player assessment, in-game analysis, and broadcast production. However, automatic detection of jersey numbers using deep features is challenging primarily due to: a) motion blur, b) low resolution video feed, and c) occlusions. With their recent success in various vision tasks, masked autoencoders (MAEs) have emerged as a superior alternative to conventional feature extractors. However, most MAEs simply zero-out image patches either randomly or focus on where to mask rather than how to mask. Motivated by human vision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to facilitate robust feature extraction in the presence of motion blur for player identification. We further introduce a new spatio-temporal network leveraging our novel d-MAE for 
    
[^77]: 使用StateFlow增强LLM任务解决能力通过状态驱动工作流

    StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows

    [https://arxiv.org/abs/2403.11322](https://arxiv.org/abs/2403.11322)

    提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。

    

    使用大型语言模型（LLM）来解决复杂任务的趋势日益明显，例如需要一系列操作和与工具环境动态交互的任务。本文提出了StateFlow，一种新颖的基于LLM的任务求解范式，将由LLM支持的复杂任务解决过程概念化为状态机。通过正确构建状态和定义状态转换，StateFlow确定了任务求解的进展，确保清晰跟踪和管理LLM在整个任务求解过程中的响应。在每个状态中，StateFlow允许执行一系列动作，不仅包括根据特定提示指导生成LLM响应，还包括根据需要利用外部工具。状态转换由LLM做出的特定规则或决策控制，允许通过任务的预定义StateFlow模型动态自适应地进行进展。

    arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
    
[^78]: 开创性SE(2)-等变轨迹规划用于自动驾驶

    Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving

    [https://arxiv.org/abs/2403.11304](https://arxiv.org/abs/2403.11304)

    该论文提出了一种轻量级等变规划模型，在保证输入空间的旋转平移等变性的同时，将运动预测和轨迹规划结合到一个步骤中，提高了样本效率，并提供了一种高级路线引导方法。

    

    计划控制自车轨迹是自动驾驶中的关键挑战。最近的运动预测方法利用等变神经网络来利用场景中的几何对称性。然而，没有现有方法在保证输入空间的旋转平移等变性的同时，将运动预测和轨迹规划结合到一个步骤中。我们通过提出一个轻量级等变规划模型来填补这一空白，该模型生成所有车辆的多模态联合预测，并选择一个模态作为自车计划。等变网络设计提高了样本效率，保证了输出稳定性，并减少了模型参数。我们进一步提出等变路线吸引力，以引导自车沿着一个由现成GPS导航系统提供的高级路线前进。

    arXiv:2403.11304v1 Announce Type: cross  Abstract: Planning the trajectory of the controlled ego vehicle is a key challenge in automated driving. As for human drivers, predicting the motions of surrounding vehicles is important to plan the own actions. Recent motion prediction methods utilize equivariant neural networks to exploit geometric symmetries in the scene. However, no existing method combines motion prediction and trajectory planning in a joint step while guaranteeing equivariance under roto-translations of the input space. We address this gap by proposing a lightweight equivariant planning model that generates multi-modal joint predictions for all vehicles and selects one mode as the ego plan. The equivariant network design improves sample efficiency, guarantees output stability, and reduces model parameters. We further propose equivariant route attraction to guide the ego vehicle along a high-level route provided by an off-the-shelf GPS navigation system. This module creates
    
[^79]: SQ-LLaVA：自问自答的大型视觉-语言助手

    SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant

    [https://arxiv.org/abs/2403.11299](https://arxiv.org/abs/2403.11299)

    本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。

    

    最近视觉-语言模型的发展在经过视觉指导调整后，在视觉-语言任务中展现出显着的泛化能力。然而，预训练视觉编码器和大型语言模型之间的鸿沟成为整个网络的瓶颈。为了改善跨模态对齐，现有的工作通常考虑涵盖更广泛的视觉任务范围的更多视觉指导数据，对模型进行微调以用于问答，但这种操作成本较高。然而，图像包含大量上下文信息，但这一方面一直鲜有人探索。本文首次尝试利用视觉指导数据内部被忽视的上下文，训练模型自我训练'学习'如何提出高质量问题。通过这种方式，我们引入了一个名为SQ-LLaVA的新颖框架：自问自答的大型视觉-语言助手。SQ-LLaVA在生成灵活且有意义的图像方面表现出高效性。

    arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
    
[^80]: 多关系图神经网络用于领域外链接预测

    Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction

    [https://arxiv.org/abs/2403.11292](https://arxiv.org/abs/2403.11292)

    提出了一种名为GOOD的图神经网络模型，设计用于解决领域外链接预测问题，采用一种新颖的多关系嵌入聚合设计概念。

    

    动态多关系图是一种用于包含不同类型实体和关系的数据的表达形式，在这种数据中，关系允许随时间变化。解决这类数据上的预测任务需要能够找到捕捉所涉及关系多样性以及动态演变的结构嵌入的能力。在这项工作中，我们为动态多关系图建立了一类新颖且具有挑战性的任务，涉及领域外链接预测，其中待预测的关系在输入图中不可用。然后，我们引入了一种新颖的图神经网络模型，命名为GOOD，专门设计来解决领域外泛化问题。GOOD引入了一种新颖的多关系嵌入聚合设计概念，基于这样一个观点，即当能够解开不同关系混合比例时，嵌入才是好的表示。

    arXiv:2403.11292v1 Announce Type: cross  Abstract: Dynamic multi-relational graphs are an expressive relational representation for data enclosing entities and relations of different types, and where relationships are allowed to vary in time. Addressing predictive tasks over such data requires the ability to find structure embeddings that capture the diversity of the relationships involved, as well as their dynamic evolution. In this work, we establish a novel class of challenging tasks for dynamic multi-relational graphs involving out-of-domain link prediction, where the relationship being predicted is not available in the input graph. We then introduce a novel Graph Neural Network model, named GOOD, designed specifically to tackle the out-of-domain generalization problem. GOOD introduces a novel design concept for multi-relation embedding aggregation, based on the idea that good representations are such when it is possible to disentangle the mixing proportions of the different relatio
    
[^81]: 通过数据增强来改善作者验证的方法

    Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation

    [https://arxiv.org/abs/2403.11265](https://arxiv.org/abs/2403.11265)

    通过引入合成示例的数据增强方法，可以改善在作者验证任务中对抗性攻击下的分类器预测。

    

    作者验证（AV）是一个文本分类任务，关注的是推断候选文本是由一个特定作者撰写还是由其他人撰写。已经显示许多AV系统容易受到敌对攻击的影响，其中恶意作者积极尝试欺骗分类器，方法是隐藏他们的写作风格，或者模仿另一位作者的风格。本文研究了将分类器训练集与（负面的）合成示例进行增强的潜在好处。这些合成示例是为了模仿感兴趣的作者的风格而生成的。我们分析了这种增强对在敌对环境下的AV任务中带来的分类器预测改进。具体来说，我们尝试了三种不同的生成器架构（一种基于循环神经网络，另一种基于小规模transformers，另一种基于流行的GPT模型）。

    arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
    
[^82]: 通过费曼的路径积分理解扩散模型

    Understanding Diffusion Models by Feynman's Path Integral

    [https://arxiv.org/abs/2403.11262](https://arxiv.org/abs/2403.11262)

    通过费曼的路径积分引入了扩散模型的新形式，对基于分数的生成模型提供了全面描述，并识别出连接随机和确定性采样方案的插值参数。

    

    基于分数的扩散模型在图像生成方面已被证明具有高效性，并且得到了广泛的应用；然而，随机和确定性（即概率流ODEs）采样方案之间性能差异的潜在因素仍然不清楚。我们引入了使用费曼的路径积分的扩散模型的新形式，这是最初为量子物理学开发的一种形式。我们发现这种形式提供了对基于分数的生成模型的全面描述，并展示了反向随机微分方程和损失函数的推导。这种形式容纳了一个插值参数，连接了随机和确定性采样方案，我们确定这个参数就像是量子物理学中的普朗克常数的对应物。这种类比使我们能够应用温策-克拉墨-布里渊（WKB）展开，这是量子物理学中一个成熟的技术。

    arXiv:2403.11262v1 Announce Type: cross  Abstract: Score-based diffusion models have proven effective in image generation and have gained widespread usage; however, the underlying factors contributing to the performance disparity between stochastic and deterministic (i.e., the probability flow ODEs) sampling schemes remain unclear. We introduce a novel formulation of diffusion models using Feynman's path integral, which is a formulation originally developed for quantum physics. We find this formulation providing comprehensive descriptions of score-based generative models, and demonstrate the derivation of backward stochastic differential equations and loss functions.The formulation accommodates an interpolating parameter connecting stochastic and deterministic sampling schemes, and we identify this parameter as a counterpart of Planck's constant in quantum physics. This analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a well-established technique in quantum ph
    
[^83]: 一种Lie群方法应用于黎曼批量归一化

    A Lie Group Approach to Riemannian Batch Normalization

    [https://arxiv.org/abs/2403.11261](https://arxiv.org/abs/2403.11261)

    本论文建立了一个Lie群上的统一框架，为黎曼批量归一化（RBN）技术提供了理论保证，并推广了现有的Lie群到对称正定流形上的三类参数化Lie群结构。

    

    计算机视觉和机器学习中存在许多应用中的流形值测量。最近的研究将深度神经网络(DNNs)扩展到流形，并且同时，归一化技术也已经被应用到几个流形，称为黎曼归一化。然而，大多数现有的黎曼归一化方法是以临时方式推导出来的，仅适用于特定的流形。本文在Lie群上建立了一个统一的黎曼批量归一化（RBN）技术框架。我们的框架提供了控制黎曼平均值和方差的理论保证。在经验上，我们专注于具有三种不同类型Lie群结构的对称正定(SPD)流形。利用变形概念，我们将现有的SPD流形上的Lie群推广成三类参数化Lie群。

    arXiv:2403.11261v1 Announce Type: cross  Abstract: Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization l
    
[^84]: 移动边缘计算中应用部署问题的基于学习的解决方案

    A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty

    [https://arxiv.org/abs/2403.11259](https://arxiv.org/abs/2403.11259)

    通过机器学习模型将用户请求分配给服务器，以解决在移动边缘计算中应用部署问题的二阶段随机规划。

    

    在移动边缘计算服务器中放置应用程序是一个复杂的挑战，涉及许多服务器、用户及其请求。现有算法需要很长时间解决具有重大不确定性情景的高维问题。因此，需要一种有效的方法来最大化服务质量，同时考虑所有技术约束。其中一种方法是机器学习，它模拟了在边缘服务器中部署应用程序的最佳解决方案。机器学习模型预计将学习如何根据用户和服务器的空间位置将用户请求分配给服务器。本研究将问题构建为二阶段随机规划。通过变化参数如用户位置、请求速率和解决优化模型生成足够数量的训练记录。然后，基于每个用户距离可用服务器的距离特征，

    arXiv:2403.11259v1 Announce Type: cross  Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and 
    
[^85]: CPA-Enhancer：链式思维驱动自适应增强器用于未知退化下的目标检测

    CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations

    [https://arxiv.org/abs/2403.11220](https://arxiv.org/abs/2403.11220)

    提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能

    

    目前，已经广泛研究了在已知单一退化情况下的目标检测方法。然而，现有方法需要先验知识来确定退化类型，并为每种类型训练一个单独的模型，从而限制了它们在不可预测环境中的实际应用。为了解决这一挑战，我们提出了一种链式思维（CoT）驱动的自适应增强器CPA-Enhancer，用于未知退化情况下的目标检测。具体而言，CPA-Enhancer在CoT提示的逐步指导下逐步调整其增强策略，这些提示编码了与退化相关的信息。据我们所知，这是首个利用CoT提示进行目标检测任务的工作。总的来说，CPA-Enhancer是一个即插即用的增强模型，可以集成到任何通用检测器中，在不事先知道退化类型的情况下，在受损图像上实现显著提升。实验结果表明，CPA-E

    arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
    
[^86]: 自底向上的因果关系：一项调查

    Causality from Bottom to Top: A Survey

    [https://arxiv.org/abs/2403.11219](https://arxiv.org/abs/2403.11219)

    本文调查了因果关系在过去五十年中的发展，探讨了因果关系与其他方法的区别，以及其与人工智能等新方法的互动，研究了因果关系对各领域的影响和贡献。

    

    因果关系已经成为解释各个研究领域中事件、现象和结果之间关系的基本方法。它已经渗透到医学、保健、经济学、金融、欺诈检测、网络安全、教育、公共政策、推荐系统、异常检测、机器人学、控制、社会学、营销和广告等各个领域和应用中。本文调查了因果关系在过去五十年中的发展，揭示了因果关系与其他方法之间的区别，以及使用它的先决条件。此外，本文阐明了因果关系如何与新方法如人工智能（AI）、生成人工智能（GAI）、机器学习和深度学习、强化学习（RL）和模糊逻辑相互作用。我们研究了因果关系对各领域的影响、其贡献以及与最先进方法的互动。此外，本论文还举例...

    arXiv:2403.11219v1 Announce Type: new  Abstract: Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study. It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, recommender systems, anomaly detection, robotics, control, sociology, marketing, and advertising. In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it. Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning, Reinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches. Additionally, the paper exempli
    
[^87]: 基于因果推断的个人信用风险评估方法研究

    Research on Personal Credit Risk Assessment Methods Based on Causal Inference

    [https://arxiv.org/abs/2403.11217](https://arxiv.org/abs/2403.11217)

    本文基于范畴论引入新的因果关系定义，演示了因果推断中指标综合的可行性。

    

    人类历史上关于因果关系的讨论可以追溯到古希腊，然而直至今日仍没有共识。从根本上讲，这源于人类认知的本质，因果关系的理解需要超越人类认知局限的抽象工具。近几十年来，数学和计算工具的快速发展为探索因果关系提供了新的理论和技术手段，为研究开辟了更多途径。基于此，本文引入了由Samuel Eilenberg和Saunders Mac Lane于1945年提出的利用范畴论定义因果关系的新概念，以避免集合论中的自指矛盾，特别是罗素悖论。在这一框架内，演示了因果推断中指标综合的可行性。由于范畴论相关技术工具的发展存在局限性，本文采用了广泛使用的概率论

    arXiv:2403.11217v1 Announce Type: new  Abstract: The discussion on causality in human history dates back to ancient Greece, yet to this day, there is still no consensus. Fundamentally, this stems from the nature of human cognition, as understanding causality requires abstract tools to transcend the limitations of human cognition. In recent decades, the rapid development of mathematical and computational tools has provided new theoretical and technical means for exploring causality, creating more avenues for investigation.   Based on this, this paper introduces a new definition of causality using category theory, proposed by Samuel Eilenberg and Saunders Mac Lane in 1945 to avoid the self-referential contradictions in set theory, notably the Russell paradox. Within this framework, the feasibility of indicator synthesis in causal inference is demonstrated. Due to the limitations in the development of category theory-related technical tools, this paper adopts the widely-used probabilistic
    
[^88]: MindEye2：共享主题模型使得只需1小时数据即可实现fMRI到图像的转换

    MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data

    [https://arxiv.org/abs/2403.11207](https://arxiv.org/abs/2403.11207)

    MindEye2使用共享主题模型，通过仅使用1小时的fMRI训练数据，实现了高质量的fMRI到图像的转换。

    

    视觉感知的重建已取得了巨大进步，但是这些方法的实际效用受到了限制。这是因为这些模型是针对每个受试者独立训练的，每个受试者需要数十小时昂贵的fMRI训练数据才能获得高质量的结果。这项研究展示了仅使用1小时fMRI训练数据实现高质量重建的方法。我们在7名受试者中预训练我们的模型，然后在新受试者的部分数据上微调。我们的新颖功能对齐程序将所有脑数据线性映射到一个共享的主题潜在空间，然后通过共享的非线性映射到CLIP图像空间。然后，我们通过微调稳定扩散XL来接受CLIP潜在作为输入而不是文本，将CLIP空间映射到像素空间。这种方法提高了在有限训练数据的情况下的跨受试者泛化能力，并且也达到了最先进的图像检索和重建技术。

    arXiv:2403.11207v1 Announce Type: cross  Abstract: Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruct
    
[^89]: 通过合成中间标签进行分区神经网络训练

    Partitioned Neural Network Training via Synthetic Intermediate Labels

    [https://arxiv.org/abs/2403.11204](https://arxiv.org/abs/2403.11204)

    该研究提出了一种通过将模型分区到不同GPU上，并生成合成中间标签来训练各个部分的方法，以缓解大规模神经网络训练中的内存和计算压力。

    

    大规模神经网络架构的普及，特别是深度学习模型，对资源密集型训练提出了挑战。 GPU 内存约束已经成为训练这些庞大模型的一个明显瓶颈。现有策略，包括数据并行、模型并行、流水线并行和完全分片数据并行，提供了部分解决方案。 特别是模型并行允许将整个模型分布在多个 GPU 上，但随后的这些分区之间的数据通信减慢了训练速度。此外，为在每个 GPU 上存储辅助参数所需的大量内存开销增加了计算需求。 本研究主张不使用整个模型进行训练，而是将模型分区到 GPU 上，并生成合成中间标签来训练各个部分。 通过随机过程生成的这些标签减缓了训练中的内存和计算压力。

    arXiv:2403.11204v1 Announce Type: cross  Abstract: The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate me
    
[^90]: 数据就是你需要的一切：通过自动设计数据增强框架对LLM进行芯片设计微调

    Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework

    [https://arxiv.org/abs/2403.11202](https://arxiv.org/abs/2403.11202)

    提出了一种自动设计数据增强框架，以优化大型语言模型在芯片设计中的应用能力，并解决了Verilog数据匮乏和训练数据准备时间长的问题

    

    最近大型语言模型的进展表明，它们在高层提示下自动生成硬件描述语言（HDL）代码的潜力。研究人员利用微调来增强这些大型语言模型（LLMs）在芯片设计领域的能力。然而，缺乏Verilog数据阻碍了LLMs在Verilog生成质量上的进一步提升。此外，缺少Verilog和电子设计自动化（EDA）脚本数据增强框架显着增加了为LLM训练器准备训练数据集所需的时间。本文提出了一种自动设计数据增强框架，它生成与Verilog和EDA脚本对齐的大量高质量自然语言。对于Verilog生成，它将Verilog文件转换为抽象语法树，然后将节点映射到具有预定义模板的自然语言。对于Verilog修复，它

    arXiv:2403.11202v1 Announce Type: cross  Abstract: Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design. However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers. This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts. For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. For Verilog repair, it 
    
[^91]: 图单元消息传递

    Graph Unitary Message Passing

    [https://arxiv.org/abs/2403.11199](https://arxiv.org/abs/2403.11199)

    提出了一种名为GUMP的图单元消息传递方法，通过应用单元邻接矩阵来缓解图神经网络中的过度压缩问题。

    

    消息传递机制是图神经网络在各种应用中取得成功的原因，但也带来了过度压缩的问题。最近的研究通过改善图谱的重连技术、破坏图中的结构偏见来抵制过度压缩，然而在过度压缩度量方面对过度压缩的改进有所限制。受到单元RNN的启发，我们提出了图单元消息传递（GUMP），通过应用单元邻接矩阵进行消息传递来缓解图神经网络中的过度压缩问题。为设计GUMP，首先提出了一种转换方法，使普通图具有单元邻接矩阵并保持其结构偏差。然后，通过利用单元邻接矩阵的固有结构实现单位化投影算法获得单元邻接矩阵，并允许GUMP是置换等变的。实验结果表明了GUMP在改善各种应用任务上性能的有效性。

    arXiv:2403.11199v1 Announce Type: cross  Abstract: Message passing mechanism contributes to the success of GNNs in various applications, but also brings the oversquashing problem. Recent works combat oversquashing by improving the graph spectrums with rewiring techniques, disrupting the structural bias in graphs, and having limited improvement on oversquashing in terms of oversquashing measure. Motivated by unitary RNN, we propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs by applying unitary adjacency matrix for message passing. To design GUMP, a transformation is first proposed to make general graphs have unitary adjacency matrix and keep its structural bias. Then, unitary adjacency matrix is obtained with a unitary projection algorithm, which is implemented by utilizing the intrinsic structure of unitary adjacency matrix and allows GUMP to be permutation-equivariant. Experimental results show the effectiveness of GUMP in improving the performance on vari
    
[^92]: 先验依赖性分析基于函数逼近的后验抽样强化学习

    Prior-dependent analysis of posterior sampling reinforcement learning with function approximation

    [https://arxiv.org/abs/2403.11175](https://arxiv.org/abs/2403.11175)

    该研究提出了首个先验依赖性贝叶斯遗憾上界，并对后验抽样强化学习进行了改进分析，提出了一个新的上界结果，实现了对先前基准的方法论提升。

    

    这项研究在对线性混合MDPs建模的函数逼近强化学习（RL）中推进了随机探索。我们为具有函数逼近的RL建立了首个先验依赖性贝叶斯遗憾上界；并且改进了用于后验抽样强化学习（PSRL）的贝叶斯遗憾分析，提出了一个上界为${\mathcal{O}}(d\sqrt{H^3 T \log T})$的结果，其中$d$表示转移核的维度，$H$表示规划视野，$T$表示总交互次数。 这表示通过优化$\mathcal{O}(\sqrt{\log T})$因子，我们在之前针对线性混合MDPs的基准（Osband和Van Roy，2014）上取得了方法论上的提升。我们的方法，利用价值定向模型学习的视角，引入了一种解耦论证和方差缩减技术，超越了传统分析依赖于置信区间和集中不等式的限制。

    arXiv:2403.11175v1 Announce Type: cross  Abstract: This work advances randomized exploration in reinforcement learning (RL) with function approximation modeled by linear mixture MDPs. We establish the first prior-dependent Bayesian regret bound for RL with function approximation; and refine the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon, and $T$ the total number of interactions. This signifies a methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$ factor over the previous benchmark (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging a value-targeted model learning perspective, introduces a decoupling argument and a variance reduction technique, moving beyond traditional analyses reliant on confidence sets and concentration inequalities to f
    
[^93]: 使用大型语言模型纠正社交媒体上的错误信息

    Correcting misinformation on social media with a large language model

    [https://arxiv.org/abs/2403.11169](https://arxiv.org/abs/2403.11169)

    提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。

    

    误信息会破坏公众对科学和民主的信任，特别是在社交媒体上，不准确信息会迅速传播。专家和普通人通过手动识别和解释不准确信息已经被证明是有效的纠正误信息的方法。然而，这种方法很难扩展，这是一个担忧，因为大型语言模型（LLMs）等技术使误信息更容易生成。LLMs还具有多功能能力，可以加速纠正误信息；然而，它们由于缺乏最新信息、倾向于生成似是而非的内容和引用以及无法处理多模态信息而面临困难。为了解决这些问题，我们提出了MUSE，这是一个带有最新信息访问和可信度评估的LLM。通过检索上下文证据和反驳，MUSE可以提供准确可信的解释和参考。它还描述

    arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
    
[^94]: CGI-DM：通过对比梯度反转进行扩散模型的数字版权认证

    CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion

    [https://arxiv.org/abs/2403.11162](https://arxiv.org/abs/2403.11162)

    该方法提出了一种通过对比梯度反转实现扩散模型的数字版权认证的新方法，通过利用预训练模型和微调模型之间的概念差异来恢复图像的缺失细节。

    

    扩散模型（DMs）已经发展成为先进的图像生成工具，特别是对于少样本生成，在这种情况下，一个预训练模型被微调到一小组图像上以捕捉特定风格或对象。尽管它们取得了成功，但人们对潜在的版权侵犯问题表示担忧，这主要源于在此过程中使用未经授权的数据。为了应对这一问题，我们提出了通过对比梯度反转进行扩散模型（CGI-DM），这是一种新颖的方法，具有生动的视觉图像，用于数字版权认证。我们的方法涉及删除图像的部分信息，并通过利用预训练模型和微调模型之间的概念差异恢复缺失的细节。我们将两个模型的潜在变量之间的差异构建为给定相同输入图像时的KL散度，可以通过蒙特卡罗采样和投影梯度下降（PGD）进行最大化。原图像和

    arXiv:2403.11162v1 Announce Type: cross  Abstract: Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and
    
[^95]: 法律领域中大型语言模型(LLMs)的评估伦理

    Evaluation Ethics of LLMs in Legal Domain

    [https://arxiv.org/abs/2403.11152](https://arxiv.org/abs/2403.11152)

    评估大型语言模型在法律领域的伦理层面的重要性以确保其有效整合，提出了一种基于法律案例的新颖评估方法，并对大型语言模型的基本语言能力、专业法律知识和法律稳健性进行了全面评估

    

    近年来，大型语言模型在自然语言对话中的应用日益增多，导致它们在各个领域被广泛采用。然而，它们在应对诸如法律等专业领域特定挑战方面的通用能力仍受到质疑。研究人员忽视了将法律伦理纳入模型的重要性。我们主张严格的伦理评估对于确保大型语言模型在法律领域有效整合至关重要，强调了评估领域特定能力和领域特定伦理的必要性。为解决这一问题，我们提出了一种创新的评估方法，利用真实法律案例来评估大型语言模型(LLMs)的基本语言能力、专业法律知识和法律稳健性。我们的全面评估结果对学术界围绕法律领域的讨论做出了重要贡献。

    arXiv:2403.11152v1 Announce Type: cross  Abstract: In recent years, the utilization of large language models for natural language dialogue has gained momentum, leading to their widespread adoption across various domains. However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny. The incorporation of legal ethics into the model has been overlooked by researchers. We asserts that rigorous ethic evaluation is essential to ensure the effective integration of large language models in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic. To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models (LLMs). The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the s
    
[^96]: 在人类对齐中扩展数据多样性以微调语言模型

    Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment

    [https://arxiv.org/abs/2403.11124](https://arxiv.org/abs/2403.11124)

    更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。

    

    与人类偏好对齐可以防止大型语言模型（LLMs）生成误导性或有毒内容，同时需要高成本的人类反馈。假设人工注释资源有限，则可以考虑两种不同的分配方式：更多样化的提示或更多样化的待标记响应。然而，它们对结果的影响的直接比较尚不存在。在这项工作中，我们首先根据微调样本数量控制双方的多样性，这可以直接反映它们的影响。我们发现，与大量提示不同，更多的响应但是更少的提示更能激发LLMs进行人类对齐。此外，提示的多样性概念可能比通常由单个数字量化的响应更复杂。因此，提出了提示多样性的新公式，进一步暗示与微调后LLMs最终性能的线性相关。

    arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
    
[^97]: 博士论文：一个提示的视觉幻觉评估数据集

    PhD: A Prompted Visual Hallucination Evaluation Dataset

    [https://arxiv.org/abs/2403.11116](https://arxiv.org/abs/2403.11116)

    本研究针对Intrinsic Vision-Language Hallucination（IVL-Hallu）问题进行了深入分析，提出了几种新颖的IVL-Hallu任务，并将其分为四种类型，有助于揭示其产生的原因和反映。

    

    大型语言模型（LLMs）的快速增长推动了大型视觉语言模型（LVLMs）的发展。在LLMs中普遍存在的幻觉挑战也出现在LVLMs中。然而，大部分现有研究主要集中在LVLM中的对象幻觉上，忽略了LVLM幻觉的多样化类型。本研究深入探讨了固有视觉语言幻觉（IVL-Hallu）问题，对导致幻觉的不同类型的IVL-Hallu进行了彻底分析。具体来说，我们提出了几个新颖的IVL-Hallu任务，并将它们分为四种类型：（a）对象幻觉，由于对象的误识别而产生，（b）属性幻觉，由于属性的误识别而引起，（c）多模态冲突幻觉，源自文本和视觉信息之间的矛盾，以及（d）反常识幻觉，由于对立之间的矛盾。

    arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
    
[^98]: 种群强化学习的相位多样性优化

    Phasic Diversity Optimization for Population-Based Reinforcement Learning

    [https://arxiv.org/abs/2403.11114](https://arxiv.org/abs/2403.11114)

    引入了Phasic Diversity Optimization (PDO)算法，采用种群训练框架，将奖励和多样性训练分为不同阶段，并在辅助阶段实现激进的多样性优化。

    

    本文介绍了Phasic Diversity Optimization (PDO)算法，这是一个基于种群的训练框架，将奖励和多样性训练分为不同的阶段，而不是优化多目标函数。在辅助阶段，表现较差的agent通过决策者进行多样化，不会取代存档中更好的agent。奖励和多样性的解耦使我们能够在辅助阶段使用激进的多样性优化，而不会降低性能。

    arXiv:2403.11114v1 Announce Type: cross  Abstract: Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degra
    
[^99]: 自监督量化感知知识蒸馏

    Self-Supervised Quantization-Aware Knowledge Distillation

    [https://arxiv.org/abs/2403.11106](https://arxiv.org/abs/2403.11106)

    提出了一种自监督量化感知知识蒸馏(SQAKD)框架，可以在不需要标记的监督情况下，同时最小化全精度和低比特模型之间的KL损失以及量化的离散化误差，从而避免了繁琐的超参数调整和复杂的训练过程。

    

    遗憾地，现有工作将知识蒸馏应用于量化感知训练(QAT)需要繁琐的超参数调整来平衡不同损失项的权重，假定有标记的训练数据可用，并且需要复杂、计算密集的训练程序以获得良好的性能。为了解决这些限制，本文提出了一种新颖的自监督量化感知知识蒸馏(SQAKD)框架。SQAKD首先统一了各种量化函数的前向和反向动态，使其可以灵活地整合各种QAT工作。然后，它将QAT形式化为一个联合优化问题，同时最小化了用于KD的全精度模型和低比特模型之间的KL损失，以及用于量化的离散化误差，而无需来自标签的监督。

    arXiv:2403.11106v1 Announce Type: cross  Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined to achieve competitive performance in creating low-bit deep learning models. However, existing works applying KD to QAT require tedious hyper-parameter tuning to balance the weights of different loss terms, assume the availability of labeled training data, and require complex, computationally intensive training procedures for good performance. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating various QAT works. Then it formulates QAT as a co-optimization problem that simultaneously minimizes the KL-Loss between the full-precision and low-bit models for KD and the discretization error for quantization, without supervision from labels. A comprehensive e
    
[^100]: 翻译困境？跨语言概念上对文本到图像模型公平评估的翻译错误与挑战

    Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts

    [https://arxiv.org/abs/2403.11092](https://arxiv.org/abs/2403.11092)

    本研究发现在一个文本到图像模型基准中存在西班牙语、日语和中文中不同程度的翻译错误，提供了纠正，并分析了对基准性能的影响。

    

    通过将概念列表翻译成七种语言，将刺激生成的图像与预期的图像分布进行比较，评估了文本到图像（T2I）模型的多语言能力基准。不幸的是，我们发现这个基准在西班牙语、日语和中文中存在不同严重程度的翻译错误。我们提供了这些错误的更正，并分析了它们对CoCo-CroLa作为基准的效用和有效性的影响。我们通过修订后重新评估了多个基线T2I模型，比较了在新翻译下引发的输出与在旧翻译下引发的输出，并展示了修正对图像领域基准结果的影响程度是可以预测的。

    arXiv:2403.11092v1 Announce Type: cross  Abstract: Benchmarks of the multilingual capabilities of text-to-image (T2I) models compare generated images prompted in a test language to an expected image distribution over a concept set. One such benchmark, "Conceptual Coverage Across Languages" (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting them to generate pictures from a concept list translated to seven languages and comparing the output image populations. Unfortunately, we find that this benchmark contains translation errors of varying severity in Spanish, Japanese, and Chinese. We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a benchmark. We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction's impactfulness on the image-domain benchmark results can be predicted in t
    
[^101]: RobustSentEmbed：使用对抗自监督对比学习的稳健句子嵌入

    RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning

    [https://arxiv.org/abs/2403.11082](https://arxiv.org/abs/2403.11082)

    RobustSentEmbed是一个自监督句子嵌入框架，通过对抗对比学习提高了文本表示任务中的泛化性和稳健性，实现了在对抗攻击中的优越表现。

    

    预训练语言模型（PLMs）在各种自然语言处理任务中展现出了出色的性能。然而，尽管它们在未见数据上取得成功，但目前基于PLM的表示通常在对抗设置中表现出较差的稳健性。本文引入了RobustSentEmbed，这是一个自监督句子嵌入框架，旨在改善不同文本表示任务中的泛化性和稳健性，以及对抗各种对抗攻击。通过生成高风险的对抗扰动并将其用于新颖的目标函数中，RobustSentEmbed巧妙地学习高质量和稳健的句子嵌入。我们的实验证实了RobustSentEmbed优于最先进表示方法的优越性。具体来说，我们的框架显著降低了各种对抗攻击的成功率，特别是降低了

    arXiv:2403.11082v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing
    
[^102]: GOMA：通过面向目标的心智对齐实现主动合作沟通

    GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment

    [https://arxiv.org/abs/2403.11075](https://arxiv.org/abs/2403.11075)

    GOMA提出了一种面向目标的心智对齐的合作沟通框架，通过最小化智能体心智状态部分之间的不一致性来帮助实现更好的合作。

    

    口头交流在人类合作中起着至关重要的作用，特别是当合作伙伴只对任务、环境和彼此的心理状态具有不完整的信息时。本文提出了一种新颖的合作沟通框架，即面向目标的心智对齐（GOMA）。GOMA将口头交流形式化为一个规划问题，通过最小化与目标相关的智能体心智状态部分之间的不一致性来促进合作。这种方法使得一个具有身体的助手能够推理何时以及如何以自然语言主动开始与人类的口头沟通，从而帮助实现更好的合作。我们在两个具有挑战性的环境，Overcooked（一款多人游戏）和VirtualHome（一个家庭模拟器）中，对我们的方法进行了评估。实验结果表明，大型语言模型在生成基于语境的有意义沟通方面存在困难。

    arXiv:2403.11075v1 Announce Type: cross  Abstract: Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals. This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in th
    
[^103]: 通过未标记帧利用实现音频-视觉分割

    Audio-Visual Segmentation via Unlabeled Frame Exploitation

    [https://arxiv.org/abs/2403.11074](https://arxiv.org/abs/2403.11074)

    该研究提出一种通用框架，有效利用邻近帧和远距帧来实现音频-视觉分割

    

    音频-视觉分割（AVS）旨在分割视频帧中的发出声音的对象。尽管取得了很大进展，但我们通过实验证实当前方法在使用未标记帧时只实现了边际性能提升，导致了未充分利用的问题。为了充分挖掘未标记帧在AVS中的潜力，我们明确将它们基于其时间特征分为两类，即邻近帧（NF）和远距帧（DF）。NF，与标记帧在时间上相邻，通常包含丰富的运动信息，有助于准确定位发声对象。与NF相反，DF与标记帧之间存在很长的时间间隔，其具有外观变化但共享语义相似对象。考虑到它们独特的特点，我们提出了一种通用框架，有效利用它们来处理AVS。具体而言，对于NF，我们利用运动线索

    arXiv:2403.11074v1 Announce Type: cross  Abstract: Audio-visual segmentation (AVS) aims to segment the sounding objects in video frames. Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue. To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories based on their temporal characteristics, i.e., neighboring frame (NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame, often contain rich motion information that assists in the accurate localization of sounding objects. Contrary to NFs, DFs have long temporal distances from the labeled frame, which share semantic-similar objects with appearance variations. Considering their unique characteristics, we propose a versatile framework that effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the motion cues
    
[^104]: Tokensome：面向可解释和认知核型的基因视觉语言GPT

    Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping

    [https://arxiv.org/abs/2403.11073](https://arxiv.org/abs/2403.11073)

    Tokensome提出了一种基于染色体标记的创新视觉语言模型，将处理核型问题从传统的视觉感知层升级到认知决策层，通过整合知识图和LLM，显著提升模型的可解释性和促进异常检测。

    

    自动核型分析通常被定义为一项专注于染色体对象级建模的视觉感知任务，该定义导致大多数现有方法忽视了组件和整体信息，显著限制了模型的性能。此外，当前技术缺乏可解释性阻碍了临床的采用。在本文中，我们介绍了Tokensome，这是一种基于染色体标记的新型视觉语言模型，用于可解释和认知核型。Tokensome将方法从传统的视觉感知层提升到认知决策层。这种提升通过知识图和LLM，使领域知识和认知推理整合在一起，显著增强了模型的可解释性并促进了异常检测。

    arXiv:2403.11073v1 Announce Type: cross  Abstract: Automatic karyotype analysis is often defined as a visual perception task focused solely on chromosomal object-level modeling. This definition has led most existing methods to overlook componential and holistic information, significantly constraining model performance. Moreover, the lack of interpretability in current technologies hinders clinical adoption. In this paper, we introduce Tokensome, a novel vision-language model based on chromosome tokenization for explainable and cognitive karyotyping. Tokensome elevates the method from the conventional visual perception layer to the cognitive decision-making layer. This elevation enables the integration of domain knowledge and cognitive reasoning via knowledge graphs and LLMs, markedly enhancing model's explainability and facilitating abnormality detection.
    
[^105]: 从像素到预测：谱图和视觉变换器在更好的时间序列预测中的应用

    From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting

    [https://arxiv.org/abs/2403.11047](https://arxiv.org/abs/2403.11047)

    提出了使用谱图作为时间序列数据的视觉表示，并引入了视觉变换器进行多模态学习，在多个领域的数据集上展示出明显优势。

    

    时间序列预测在各个领域的决策制定中起着至关重要的作用，但也存在重大挑战。最近的研究探索了使用计算机视觉模型的基于图像的方法来解决这些挑战，通常采用线图作为时间序列数据的视觉表示。本文提出了一种新颖方法，使用时间-频率谱图作为时间序列数据的视觉表示。我们引入了视觉变换器用于多模态学习，展示了我们方法在来自不同领域的多样数据集上的优势。为了评估其有效性，我们将我们的方法与统计基线（EMA和ARIMA）、最先进的基于深度学习的方法（DeepAR）、其他时间序列数据的视觉表示（线图像）以及仅使用时间序列作为输入的消融研究进行了比较。我们的实验表明利用谱图和视觉变换器技术在时间序列预测中具有重要优势。

    arXiv:2403.11047v1 Announce Type: cross  Abstract: Time series forecasting plays a crucial role in decision-making across various domains, but it presents significant challenges. Recent studies have explored image-driven approaches using computer vision models to address these challenges, often employing lineplots as the visual representation of time series data. In this paper, we propose a novel approach that uses time-frequency spectrograms as the visual representation of time series data. We introduce the use of a vision transformer for multimodal learning, showcasing the advantages of our approach across diverse datasets from different domains. To evaluate its effectiveness, we compare our method against statistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based approach (DeepAR), other visual representations of time series data (lineplot images), and an ablation study on using only the time series as input. Our experiments demonstrate the benefits of utilizing s
    
[^106]: 通过信息间竞争调控聊天机器人输出

    Regulating Chatbot Output via Inter-Informational Competition

    [https://arxiv.org/abs/2403.11046](https://arxiv.org/abs/2403.11046)

    本文通过探讨信息间竞争，提出利用信息市场本身作为有效减轻AI聊天机器人输出风险的可能性，并指出监管者在面对新技术不确定性时往往过度谨慎，需重新评估监管策略。

    

    ChatGPT的出现引发了一年多的监管狂潮。然而，少数现有研究严格质疑了这样一个假设：如果不经规范，AI聊天机器人的输出会对人类事务造成实质且严重的伤害。大多数研究人员忽视了信息市场本身可以有效减轻这些风险的关键可能性，并因此倾向于使用监管工具直接解决问题。本文通过关注各种渠道之间的信息竞争，发展了一个重新评估AI相关内容风险和相应监管提议的标准。长达数十年的信息和通信技术监管史表明，监管者在面对新技术带来的不确定性时往往过于谨慎，并在提出过度的监管措施时犯错误。事实上，丰富的经验数据支持了信息市场机制在信息监管方面的作用。

    arXiv:2403.11046v1 Announce Type: cross  Abstract: The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empiric
    
[^107]: 奖励引导的潜在一致性蒸馏

    Reward Guided Latent Consistency Distillation

    [https://arxiv.org/abs/2403.11027](https://arxiv.org/abs/2403.11027)

    该论文提出了一种奖励引导的潜在一致性蒸馏方法，通过在LCD过程中整合奖励模型的反馈，从而有效提高高保真图像生成时的样本质量。

    

    潜在一致性蒸馏(LCD)已成为一种有效的文本到图像合成范式。通过从预训练的教师潜在扩散模型(LDM)中蒸馏出潜在一致性模型(LCM)，LCD在仅需2到4个推理步骤内促进了高保真图像的生成。然而，LCM的高效推理是以样本质量为代价的。本文提出通过在训练过程中将LCM的输出与人类偏好对齐来补偿质量损失。具体而言，我们引入奖励引导的LCD(RG-LCD)，通过将奖励模型(RM)的反馈整合到LCD过程中，通过将原始LCD损失与最大化与LCM单步生成相关联的奖励的目标相结合，来最大化奖励。通过人类评估验证，当使用良好RM的反馈进行训练时，我们的RG-LCM的2步生成被人类青睐，超过了50步DDIM样本。

    arXiv:2403.11027v1 Announce Type: cross  Abstract: Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM's efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from 
    
[^108]: 神经符号视频搜索

    Neuro-Symbolic Video Search

    [https://arxiv.org/abs/2403.11021](https://arxiv.org/abs/2403.11021)

    提出了一种神经网络符号视频搜索系统，该系统利用视觉-语言模型进行语义理解，并通过状态机和时间逻辑公式对事件的长期演变进行推理，从而实现高效的场景识别。

    

    近年来视频数据生产的空前激增需求高效的工具，以从视频中提取有意义的帧供下游任务使用。 长期时间推理是帧检索系统的一个关键要求。 虽然 VideoLLaMA 和 ViCLIP 等最先进的基础模型在短期语义理解方面表现优异，但它们在跨帧的长期推理方面却令人惊讶地失败。 这种失败的一个关键原因是它们将逐帧感知和时间推理交织成单个深度网络。 因此，解耦但共同设计语义理解和时间推理对于高效的场景识别是至关重要的。 我们提出了一种系统，利用视觉-语言模型对单个帧进行语义理解，但有效地通过使用状态机和时间逻辑（TL）公式对事件的长期演变进行推理，这些公式在本质上捕捉了记忆。

    arXiv:2403.11021v1 Announce Type: cross  Abstract: The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reas
    
[^109]: 从表达数据中识别基因调控网络的吸引子：一种可解释方法

    Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach

    [https://arxiv.org/abs/2403.11015](https://arxiv.org/abs/2403.11015)

    本文提出了一种基于Zadeh计算与词的新方法，可以有效地在存在巨大不确定性时从时间基因表达数据中稳健地识别吸引子。

    

    在系统生物学中，基因调控网络的吸引子景观分析被认为是研究从增殖和分化到衰老和凋亡等各种细胞状态的强大计算工具。因此，准确识别吸引子在确定细胞命运方面起着至关重要的作用。本文针对这一问题提出了一种基于Zadeh计算与词的新方法。

    arXiv:2403.11015v1 Announce Type: cross  Abstract: In systems biology, attractor landscape analysis of gene regulatory networks is recognized as a powerful computational tool for studying various cellular states from proliferation and differentiation to senescence and apoptosis. Therefore, accurate identification of attractors plays a critical role in determination of the cell fates. On the other hand, in a real biological circuit, genetic/epigenetic alterations as well as varying environmental factors drastically take effect on the location, characteristics, and even the number of attractors. The central question is: Given a temporal gene expression profile of a real gene regulatory network, how can the attractors be robustly identified in the presence of huge amount of uncertainty? This paper addresses this question using a novel approach based on Zadeh Computing with Words. The proposed scheme could effectively identify the attractors from temporal gene expression data in terms of b
    
[^110]: DIALECTBENCH：一个方言、语言变体和密切相关语言的自然语言处理基准

    DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages

    [https://arxiv.org/abs/2403.11009](https://arxiv.org/abs/2403.11009)

    DIALECTBENCH是第一个面向自然语言处理中的方言、语言变体和密切相关语言的大规模基准测试，为实现对不同语言变体上NLP系统性能的全面评估提供了重要工具。

    

    arXiv:2403.11009v1 语种：交叉  摘要：语言技术应该根据其在实际用例中的有用性来判断。在自然语言处理（NLP）研究和评估中经常被忽视的一个方面是非标准方言或语言变体（以下简称为变体）形式的语言变体. 大多数NLP基准测试仅限于标准语言变体。为填补这一空白，我们提出了DIALECTBENCH，这是首个针对语言变体的大规模NLP基准测试，汇总了一系列多样任务样本的变体数据集（涵盖281种变体的10个文本级任务）。这允许对不同语言变体上NLP系统性能进行全面评估。我们提供了大量证据表明标准语言变体与非标准语言变体之间存在性能差距，并且我们还确定了在任务之间存在大量性能差距的语言类群。我们认为DIALECTBENCH提供了对当前语言NLP状态的全面视图。

    arXiv:2403.11009v1 Announce Type: cross  Abstract: Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for langua
    
[^111]: 嵌套神经特征场的层次场景理解

    N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields

    [https://arxiv.org/abs/2403.10997](https://arxiv.org/abs/2403.10997)

    利用Nested Neural Feature Fields (N2F2) 实现了层次化监督学习，提供了对物理维度或语义维度等不同粒度的场景属性全面和细致的理解。

    

    在计算机视觉中，理解多层抽象的复杂场景仍然是一个巨大挑战。为了解决这个问题，我们引入了嵌套神经特征场 (N2F2)，这是一种新颖的方法，利用分层监督来学习单个特征场，在同一高维特征中的不同维度编码不同粒度的场景属性。我们的方法允许灵活定义层次，可以根据物理维度、语义维度或两者均匹配，从而实现对场景的全面和细致理解。我们利用2D类别无关分割模型在图像空间的任意尺度提供语义有意义的像素分组，并查询CLIP视觉编码器，为这些段落中的每个部分获得与语言对齐的嵌入。我们提出的分层监督方法将不同的嵌套特征场维度分配给提取C

    arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
    
[^112]: 具有奇异值扰动的边缘私有图神经网络

    Edge Private Graph Neural Networks with Singular Value Perturbation

    [https://arxiv.org/abs/2403.10995](https://arxiv.org/abs/2403.10995)

    提出了一种新的隐私保护GNN训练算法Eclipse，通过观察图结构中邻接矩阵的低秩行为，实现了在保护边缘隐私的同时保持模型良好效用。

    

    图神经网络（GNNs）在从图结构化数据中学习表示方面发挥关键作用，并被证明在许多应用中非常有用。然而，GNN训练流程已被显示容易受到节点特征泄漏和边提取攻击的影响。本文研究了一种情景，其中攻击者旨在从训练过的GNN模型中恢复私有边缘信息。先前的研究采用差分隐私（DP）直接向邻接矩阵或紧凑图表示添加噪声。添加的扰动导致图结构被大幅改变，降低了模型的效用。我们提出了一种新的保护隐私的GNN训练算法Eclipse，该算法在提供强大的隐私保护的同时保持了良好的模型效用。Eclipse基于两个关键观察结果。第一，图结构中的邻接矩阵表现出低秩行为。因此，Eclipse使用低秩的方式训练GNNs。

    arXiv:2403.10995v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) play a key role in learning representations from graph-structured data and are demonstrated to be useful in many applications. However, the GNN training pipeline has been shown to be vulnerable to node feature leakage and edge extraction attacks. This paper investigates a scenario where an attacker aims to recover private edge information from a trained GNN model. Previous studies have employed differential privacy (DP) to add noise directly to the adjacency matrix or a compact graph representation. The added perturbations cause the graph structure to be substantially morphed, reducing the model utility. We propose a new privacy-preserving GNN training algorithm, Eclipse, that maintains good model utility while providing strong privacy protection on edges. Eclipse is based on two key observations. First, adjacency matrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains GNNs with a low-r
    
[^113]: 通过学习的先验知识提升基于流的生成超分辨模型

    Boosting Flow-based Generative Super-Resolution Models via Learned Prior

    [https://arxiv.org/abs/2403.10988](https://arxiv.org/abs/2403.10988)

    通过引入条件学习的先验知识，成功解决了基于流的超分辨模型中的网格伪影、逆矩阵爆炸和次优结果等问题。

    

    基于流的超分辨（SR）模型展现了在生成功能高质量图像方面的惊人能力。然而，这些方法在图像生成过程中遇到了几个挑战，比如网格伪影、逆矩阵爆炸和由于固定的采样温度导致的次优结果。为了克服这些问题，本文在基于流的SR模型的推断阶段引入了条件学习的先验知识。这个先验知识是由我们提出的潜在模块根据低分辨率图像预测出的潜在代码，然后通过流模型转换为SR图像。我们的框架旨在无缝地与任何现代基于流的SR模型集成，而无需修改其架构或预训练权重。我们通过大量实验和消融分析评估了提出的框架的有效性。提出的框架成功解决了基于流的SR模型中的所有固有问题。

    arXiv:2403.10988v1 Announce Type: cross  Abstract: Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images. However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model. This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image. Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights. We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses. The proposed framework successfully addresses all the inherent issues in flow-based SR models a
    
[^114]: IoTCO2：评估物联网-启用深度学习的端到端碳足迹

    IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning

    [https://arxiv.org/abs/2403.10984](https://arxiv.org/abs/2403.10984)

    介绍了一种名为\carb 的端到端建模工具，用于在物联网-启用深度学习中精确估算碳足迹，展示了与实际测量值相比最大$\pm21\%$的碳足迹差异。

    

    为了提高隐私性和确保服务质量（QoS），深度学习（DL）模型越来越多地部署在物联网（IoT）设备上进行数据处理，极大地增加了与IoT上DL相关的碳足迹，涵盖了操作和实体方面。现有的操作能量预测器经常忽略了量化的DL模型和新兴的神经处理单元（NPUs），而实体碳足迹建模工具忽略了IoT设备中常见的非计算硬件组件，导致了物联网DL准确碳足迹建模工具的差距。本文介绍了\textit{\carb}，一种用于精确估算物联网DL中碳足迹的端到端建模工具，展示了与各种DL模型的实际测量值相比最大$\pm21\%$的碳足迹差异。此外，\carb的实际应用通过多个用户案例展示。

    arXiv:2403.10984v1 Announce Type: cross  Abstract: To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21\%$ deviation in carbon footprint values compared to actual measurements across various DL models. Additionally, practical applications of \carb are showcased through multiple user case studies.
    
[^115]: 通过联邦学习增强对抗 DDoS 攻击的物联网安全

    Enhancing IoT Security Against DDoS Attacks through Federated Learning

    [https://arxiv.org/abs/2403.10968](https://arxiv.org/abs/2403.10968)

    通过联邦学习，利用物联网设备的集体智慧构建全局模型，保护数据隐私和最小化通信开销，增强物联网对抗DDoS攻击的安全性。

    

    物联网的迅速普及引入了物理设备与数字世界之间的变革性连接。然而，分布式拒绝服务（DDoS）攻击不断升级，危及物联网网络的完整性和可靠性。传统的DDoS缓解方法无法处理物联网生态系统的复杂性，可能 compromise 数据隐私。本文介绍了一种创新策略，通过利用联邦学习的力量，增强物联网网络对抗DDoS攻击的安全性，允许多个物联网设备或边缘节点协作构建全局模型，同时保护数据隐私并最小化通信开销。该研究旨在探究联邦学习在检测和缓解物联网中DDoS攻击中的有效性。我们提出的框架利用物联网设备的集体智慧进行实时攻击检测。

    arXiv:2403.10968v1 Announce Type: cross  Abstract: The rapid proliferation of the Internet of Things (IoT) has ushered in transformative connectivity between physical devices and the digital realm. Nonetheless, the escalating threat of Distributed Denial of Service (DDoS) attacks jeopardizes the integrity and reliability of IoT networks. Conventional DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT ecosystems, potentially compromising data privacy. This paper introduces an innovative strategy to bolster the security of IoT networks against DDoS attacks by harnessing the power of Federated Learning that allows multiple IoT devices or edge nodes to collaboratively build a global model while preserving data privacy and minimizing communication overhead. The research aims to investigate Federated Learning's effectiveness in detecting and mitigating DDoS attacks in IoT. Our proposed framework leverages IoT devices' collective intelligence for real-time attack det
    
[^116]: 梦想中的许多世界：学习上下文世界模型有助于零样点泛化

    Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization

    [https://arxiv.org/abs/2403.10967](https://arxiv.org/abs/2403.10967)

    学习上下文世界模型有助于提高在未知上下文下的零样点泛化能力。

    

    零样点泛化（Zero-shot generalization，ZSG）到未见过的动态对于创建具有普遍能力的体系代理是一个重大挑战。为了解决更广泛的挑战，我们从上下文强化学习（contextual reinforcement learning，cRL）的简单设置开始，假设可观察到参数化系统动态变化的上下文值，如机器人的质量或尺寸，而不对马尔可夫状态的可观察性做进一步简化假设。为了实现对未知上下文变化的ZSG目标，我们提出了上下文循环状态空间模型（contextual recurrent state-space model，cRSSM），它对Dreamer（v3）（Hafner等人，2023年）的世界模型进行了修改，使得世界模型可以融入上下文以从观察中推断潜在的马尔可夫状态并建模潜在动态。我们的实验表明，这种系统性地将上下文纳入其中提高了在“梦境”训练的策略的ZSG能力。

    arXiv:2403.10967v1 Announce Type: cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams
    
[^117]: SelfIE：大型语言模型嵌入的自我解释

    SelfIE: Self-Interpretation of Large Language Model Embeddings

    [https://arxiv.org/abs/2403.10949](https://arxiv.org/abs/2403.10949)

    提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。

    

    arXiv:2403.10949v1 公告类型：交叉摘要：大型语言模型（LLMs）如何获得答案？解释和控制LLM的推理过程对于可靠性、透明度和未来模型发展至关重要。我们提出了SelfIE（嵌入的自我解释），这是一个框架，能够利用LLMs响应关于给定段落的查询的能力，以自然语言解释它们自己的嵌入。SelfIE能够解释隐藏嵌入中的开放世界概念，在案例中揭示LLM的内部推理，如做出道德决策、内化提示注入和回想有害知识。SelfIE对隐藏嵌入的文本描述也开辟了控制LLM推理的新途径。我们提出了监督控制，它允许编辑开放式概念，而只需要计算单个层的梯度。我们将RLHF扩展到隐藏的嵌入，并提出了强化控制来消除有害知识。

    arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
    
[^118]: 面向印度法律文本分析的人工智能

    Human Centered AI for Indian Legal Text Analytics

    [https://arxiv.org/abs/2403.10944](https://arxiv.org/abs/2403.10944)

    该论文探讨了在法律文本分析中整合人类专业知识以显著提高大型语言模型性能的潜力，并介绍了一个以人为中心的复合人工智能系统。

    

    法律研究在法律实践中至关重要，它需要强烈的人力和智力谨慎来研究法律案例并准备论点。近年来生成式人工智能的繁荣并没有导致对影响深远的法律应用的成比例增长，因为其可信度低以及训练大型语言模型(LLMs)的专业数据集稀缺。本文探讨了LLMs在法律文本分析（LTA）中的潜力，重点介绍了人类专业知识在特定领域的整合，如何显著增强其性能，与专家相匹配。我们介绍了一个新颖的数据集，并描述了一个以人为中心的复合人工智能系统，主要结合人类输入，以在LLMs中执行LTA任务。

    arXiv:2403.10944v1 Announce Type: cross  Abstract: Legal research is a crucial task in the practice of law. It requires intense human effort and intellectual prudence to research a legal case and prepare arguments. Recent boom in generative AI has not translated to proportionate rise in impactful legal applications, because of low trustworthiness and and the scarcity of specialized datasets for training Large Language Models (LLMs). This position paper explores the potential of LLMs within Legal Text Analytics (LTA), highlighting specific areas where the integration of human expertise can significantly enhance their performance to match that of experts. We introduce a novel dataset and describe a human centered, compound AI system that principally incorporates human inputs for performing LTA tasks with LLMs.
    
[^119]: 通过同态POMDP诱导个别学生的学习策略

    Inducing Individual Students' Learning Strategies through Homomorphic POMDPs

    [https://arxiv.org/abs/2403.10930](https://arxiv.org/abs/2403.10930)

    提出同态POMDP（H-POMDP）模型以适应多种认知模式，并通过参数学习方法自动构建模型，以实现诱导更个性化的学习策略

    

    优化学生的学习策略是智能辅导系统中的重要组成部分。先前的研究已经证明，通过模拟学生的学习过程，通过部分可观察马尔可夫决策过程（POMDP）为学生制定个性化学习策略的有效性。然而，这项研究假设学生人群遵循统一的认知模式。虽然这种假设简化了POMDP建模过程，但显然偏离了真实场景，因此降低了诱导个别学生学习策略的精度。在本文中，我们提出同态POMDP（H-POMDP）模型以适应多种认知模式，并提出参数学习方法自动构建H-POMDP模型。基于H-POMDP模型，我们可以从数据中表示不同的认知模式并诱导更个性化的学习策略。

    arXiv:2403.10930v1 Announce Type: new  Abstract: Optimizing students' learning strategies is a crucial component in intelligent tutoring systems. Previous research has demonstrated the effectiveness of devising personalized learning strategies for students by modelling their learning processes through partially observable Markov decision process (POMDP). However, the research holds the assumption that the student population adheres to a uniform cognitive pattern. While this assumption simplifies the POMDP modelling process, it evidently deviates from a real-world scenario, thus reducing the precision of inducing individual students' learning strategies. In this article, we propose the homomorphic POMDP (H-POMDP) model to accommodate multiple cognitive patterns and present the parameter learning approach to automatically construct the H-POMDP model. Based on the H-POMDP model, we are able to represent different cognitive patterns from the data and induce more personalized learning strat
    
[^120]: TabPFN的可解释机器学习

    Interpretable Machine Learning for TabPFN

    [https://arxiv.org/abs/2403.10923](https://arxiv.org/abs/2403.10923)

    TabPFN模型在低数据情况下实现了良好的分类性能，并能够以秒级速度生成后验预测分布，我们提出了几种专为TabPFN设计的可解释性方法的改进，实现了更高效的计算。

    

    最近开发的Prior-Data Fitted Networks（PFNs）已经显示出在低数据情况下具有非常有希望的应用结果。TabPFN模型是PFN的一种特殊情况，适用于表格数据，在不需要学习参数或超参数调整的情况下，能够在短短几秒钟内实现多种分类任务的最先进性能，并且能够生成后验预测分布。TabPFN因此成为了许多领域应用中非常吸引人的选择。然而，该方法的一个主要缺点是缺乏可解释性。因此，我们提出了几种针对TabPFN专门设计的流行解释性方法的改进。通过利用该模型的独特性质，我们的改进允许比现有实现更高效的计算。特别是，我们展示了通过避免...

    arXiv:2403.10923v1 Announce Type: cross  Abstract: The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoid
    
[^121]: DTOR：决策树异常值回归器用于解释异常

    DTOR: Decision Tree Outlier Regressor to explain anomalies

    [https://arxiv.org/abs/2403.10903](https://arxiv.org/abs/2403.10903)

    DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。

    

    解释异常值的出现以及其产生机制在各种领域中可能非常重要。故障、欺诈、威胁等问题，除了被正确识别之外，通常需要有效的解释以有效执行可操作的对抗措施。越来越广泛地使用复杂的机器学习方法来识别异常值，使得这样的解释更具挑战性。我们提出了决策树异常值回归器（DTOR），这是一种通过估计异常检测模型生成的异常分数来为单个数据点生成基于规则的解释的技术。这是通过首先应用决策树回归器来计算估计分数，然后提取与数据点分数相关联的相对路径来实现的。我们的结果表明，即使在具有大量特征的数据集中，DTOR的鲁棒性也得到了证实。此外，与其他基于规则的方法相比

    arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
    
[^122]: 优化多语言大语言模型的语言增强：以韩语为例的案例研究

    Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean

    [https://arxiv.org/abs/2403.10882](https://arxiv.org/abs/2403.10882)

    该研究提出了三种策略来增强基于公开可用MLLMs的资源较少的语言的性能，包括扩展词汇、双语数据预训练和指导微调。

    

    大型语言模型（LLMs）使用预训练来预测下一个单词；然而，它们的扩展需要大量计算资源。许多大型科技公司和研究机构已经开发了多语言LLMs（MLLMs）以满足当前需求，但忽视了资源较少的语言（LRLs）。本研究提出了三种策略来增强基于公开可用MLLMs的LRLs的性能。首先，扩展LRLs的MLLM词汇以增强表达性。其次，使用双语数据进行预训练以对齐高资源语言和低资源语言。第三，构建高质量的小规模指导数据集，并进行指导微调以增强LRL。实验采用了Llama2模型，以韩语作为LRL，并在八项任务中对其与其他已开发的LLMs进行了定量评估。此外，基于人类评估进行了定性评估。

    arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva
    
[^123]: stMCDI: 使用图神经网络的条件掩码扩散模型用于空间转录组数据填补

    stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation

    [https://arxiv.org/abs/2403.10863](https://arxiv.org/abs/2403.10863)

    stMCDI是一种新颖的条件扩散模型，通过利用空间定位转录组数据中的空间位置信息来填补缺失值，同时保持整体数据分布。

    

    空间定位转录组学通过提供基因表达数据及其相应的物理位置，为单细胞分析带来了重大进展。然而，这种高度的空间分辨率却带来了一个缺点，即由于缺失值的高发生率，导致了细胞水平上的空间转录组数据明显受到困扰。此外，大多数现有的填补方法要么忽视不同点之间的空间信息，要么牺牲整体基因表达数据分布。为解决这些挑战，我们的主要关注点是有效利用空间定位转录组数据中的空间位置信息来填补缺失值，同时保持整体数据分布。我们引入了一种新颖的条件扩散模型stMCDI，利用使用随机掩码数据部分作为指导来训练去噪网络。

    arXiv:2403.10863v1 Announce Type: cross  Abstract: Spatially resolved transcriptomics represents a significant advancement in single-cell analysis by offering both gene expression data and their corresponding physical locations. However, this high degree of spatial resolution entails a drawback, as the resulting spatial transcriptomic data at the cellular level is notably plagued by a high incidence of missing values. Furthermore, most existing imputation methods either overlook the spatial information between spots or compromise the overall gene expression data distribution. To address these challenges, our primary focus is on effectively utilizing the spatial location information within spatial transcriptomic data to impute missing values, while preserving the overall data distribution. We introduce \textbf{stMCDI}, a novel conditional diffusion model for spatial transcriptomics data imputation, which employs a denoising network trained using randomly masked data portions as guidance
    
[^124]: 内窥镜视觉里程计的高效领域自适应

    Efficient Domain Adaptation for Endoscopic Visual Odometry

    [https://arxiv.org/abs/2403.10860](https://arxiv.org/abs/2403.10860)

    这项工作提出了一个高效的内窥镜视觉里程计神经风格迁移框架，将从术前规划到测试阶段的时间缩短至不到五分钟，通过利用有限数量的真实图像和术前先验信息进行训练，以及引入测试时间自适应方法来减小训练和测试之间的光照条件差距。

    

    arXiv:2403.10860v1 公告类型: 跨领域 摘要: 视觉里程计在内窥镜成像中起着至关重要的作用，然而缺乏具有地面真实性的图像对于学习里程计信息提出了重大挑战。因此，领域自适应为连接术前规划领域和术中实际领域学习里程计信息提供了一种有前途的方法。然而，现有方法在训练时间上存在低效性。本文提出了一种针对内窥镜视觉里程计的高效神经风格迁移框架，将从术前规划到测试阶段的时间缩短至不到五分钟。为了进行高效训练，本研究专注于用有限数量的真实图像训练模块，并利用术前先验信息大大减少训练时间。此外，在测试阶段，我们提出了一种新颖的测试时间自适应（TTA）方法来消除训练和测试之间的光照条件差距。

    arXiv:2403.10860v1 Announce Type: cross  Abstract: Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity of realistic images with ground truth poses poses a significant challenge. Therefore, domain adaptation offers a promising approach to bridge the pre-operative planning domain with the intra-operative real domain for learning odometry information. However, existing methodologies suffer from inefficiencies in the training time. In this work, an efficient neural style transfer framework for endoscopic visual odometry is proposed, which compresses the time from pre-operative planning to testing phase to less than five minutes. For efficient traing, this work focuses on training modules with only a limited number of real images and we exploit pre-operative prior information to dramatically reduce training duration. Moreover, during the testing phase, we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in lighting conditions between training an
    
[^125]: 只说名称：通过数据生成实现仅利用类别名称进行在线连续学习

    Just Say the Name: Online Continual Learning with Category Names Only via Data Generation

    [https://arxiv.org/abs/2403.10853](https://arxiv.org/abs/2403.10853)

    提出了在线连续学习框架G-NoCL，采用生成数据并利用DIverSity和COmplexity enhancing ensemBlER（DISCOBER）进行数据融合，展示了其在在线连续学习基准测试中的优越性能。

    

    在现实世界的场景中，由于成本过高，对于连续学习进行大量手动注释是不切实际的。虽然之前的研究受到大规模网络监督训练的影响，建议在连续学习中利用网络抓取的数据，但这带来了诸如数据不平衡、使用限制和隐私问题等挑战。为了解决连续网络监督训练的风险，我们提出了一种在线连续学习框架 - 仅使用名称的生成式连续学习（G-NoCL）。所提出的G-NoCL使用一组生成器G以及学习者。当遇到新概念（例如，类别）时，G-NoCL采用新颖的样本复杂性引导数据合成技术DIverSity and COmplexity enhancing ensemBlER（DISCOBER）从生成的数据中最优抽样训练数据。通过大量实验，我们展示了DISCOBER在G-NoCL在线连续学习基准测试中表现出的优越性能，涵盖了In-Distributi。

    arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
    
[^126]: GAgent：一种适应复杂照明环境的自适应刚柔握持Agent

    GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments

    [https://arxiv.org/abs/2403.10850](https://arxiv.org/abs/2403.10850)

    GAgent 基于视觉语言模型的自适应刚柔握持Agent在复杂照明环境下提供了先进的认知能力，包括Prompt Engineer模块、VLM核心和Workflow模块，同时具备可变硬度软手持器，对物体和材料进行识别和准确抓取，为无人机等场景带来潜在好处。

    

    这篇论文介绍了GAgent：一种为开放世界环境设计的握持Agent，通过VLM代理提供先进的认知能力，并具备可变硬度软手持器的灵活抓取能力。GAgent包括三个主要组件 - Prompt Engineer模块，Visual-Language Model（VLM）核心和Workflow模块。这三个模块通过识别物体和材料，即使在挑战性的光照条件下也能准确估计抓取区域，提高了夹爪成功率。作为创造性的一部分，研究人员还创建了一种具有可变硬度的仿生混合软手握器，能够夹持重物同时轻柔接触物体。这种特征VLM基础认知处理和仿生设计的智能Agent显示了潜力，因为它可能有助于各种情景中的无人机。

    arXiv:2403.10850v1 Announce Type: cross  Abstract: This paper introduces GAgent: an Gripping Agent designed for open-world environments that provides advanced cognitive abilities via VLM agents and flexible grasping abilities with variable stiffness soft grippers. GAgent comprises three primary components - Prompt Engineer module, Visual-Language Model (VLM) core and Workflow module. These three modules enhance gripper success rates by recognizing objects and materials and accurately estimating grasp area even under challenging lighting conditions. As part of creativity, researchers also created a bionic hybrid soft gripper with variable stiffness capable of gripping heavy loads while still gently engaging objects. This intelligent agent, featuring VLM-based cognitive processing with bionic design, shows promise as it could potentially benefit UAVs in various scenarios.
    
[^127]: 使用门控动态可学习注意机制的双Transformer在田纳西伊斯曼过程中进行故障检测与诊断

    Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process

    [https://arxiv.org/abs/2403.10842](https://arxiv.org/abs/2403.10842)

    本研究提出一种新颖的双Transformer模型，结合门控动态可学习注意机制，用于田纳西伊斯曼过程的故障检测与诊断，提高性能通过独立处理输入数据和提取多样化信息，以及动态学习适应性调整注意策略。

    

    故障检测和诊断（FDD）对于确保工业过程的安全性和效率至关重要。我们提出了一种新颖的FDD方法，适用于田纳西伊斯曼过程（TEP），这是化工过程控制中广泛使用的基准。该模型采用两个独立的Transformer分支，能够独立处理输入数据并提取多样化的信息。引入了一种新颖的注意机制，即门控动态可学习注意（GDLAttention），它集成了门控机制和动态学习能力。门控机制调节注意权重，使模型能够关注输入的最相关部分。动态学习方法在训练过程中调整注意策略，有可能提高性能。注意机制使用双线性相似性函数，提供更大的灵活性来捕捉查询和输入之间的复杂关系。

    arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
    
[^128]: SF(DA)$^2$: 无源域自适应在数据增强的视角下

    SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation

    [https://arxiv.org/abs/2403.10834](https://arxiv.org/abs/2403.10834)

    SF(DA)$^2$是一种新颖的无源域自适应方法，通过在特征空间中构建增强图并利用谱邻域聚类来识别分区，实现了数据增强的好处而避免了挑战。

    

    在深度学习模型面对域偏移的脆弱性时，提出了无源域自适应（SFDA）方法，旨在适应新的、未见的目标域，而无需访问源域数据。尽管将数据增强应用于SFDA的潜在好处具有吸引力，但也会带来一些挑战，比如依赖于保存类别变换的先验知识以及增加内存和计算需求。在本文中，我们提出了SF(DA)$^2$，这是一种新颖的方法，利用数据增强的好处，并避免了这些挑战。我们在预训练模型的特征空间中构建一个增强图，利用目标特征之间的邻居关系，并提出了谱邻域聚类来识别预测空间中的分区。

    arXiv:2403.10834v1 Announce Type: cross  Abstract: In the face of the deep learning model's vulnerability to domain shift, source-free domain adaptation (SFDA) methods have been proposed to adapt models to new, unseen target domains without requiring access to source domain data. Although the potential benefits of applying data augmentation to SFDA are attractive, several challenges arise such as the dependence on prior knowledge of class-preserving transformations and the increase in memory and computational requirements. In this paper, we propose Source-free Domain Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach that leverages the benefits of data augmentation without suffering from these challenges. We construct an augmentation graph in the feature space of the pretrained model using the neighbor relationships between target features and propose spectral neighborhood clustering to identify partitions in the prediction space. Furthermore, we propose im
    
[^129]: LookALike: 基于人类模仿的协作决策

    LookALike: Human Mimicry based collaborative decision making

    [https://arxiv.org/abs/2403.10824](https://arxiv.org/abs/2403.10824)

    提出了一种新颖的方法，通过在LLM代理之间进行知识蒸馏，实现实时的人类角色扮演，保留独特上下文，而不依赖任何存储数据或预训练，并展示出在模拟真实世界任务中表现优于现有技术。

    

    人工通用智能在向其他系统传达特定角色细微差别方面存在不足。在构建能够相互沟通以解决现实世界问题的自主LLM代理时，这一不足更为明显。人类能够传达上下文和领域特定的微小差别以及知识，这导致了技能的进一步完善。在这项工作中，我们提出并评估了一种新颖的方法，实现了LLM代理之间的知识精炼，从而实现了保留独特上下文而无需依赖任何存储数据或预训练的实时人类角色扮演。我们还评估了我们的系统在模拟实际任务中相比最先进技术表现更好的情况。

    arXiv:2403.10824v1 Announce Type: cross  Abstract: Artificial General Intelligence falls short when communicating role specific nuances to other systems. This is more pronounced when building autonomous LLM agents capable and designed to communicate with each other for real world problem solving. Humans can communicate context and domain specific nuances along with knowledge, and that has led to refinement of skills. In this work we propose and evaluate a novel method that leads to knowledge distillation among LLM agents leading to realtime human role play preserving unique contexts without relying on any stored data or pretraining. We also evaluate how our system performs better in simulated real world tasks compared to state of the art.
    
[^130]: VisionCLIP：一种基于Med-AIGC的伦理语言-图像基础模型，用于通用性视网膜图像分析

    VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis

    [https://arxiv.org/abs/2403.10823](https://arxiv.org/abs/2403.10823)

    VisionCLIP利用Med-AIGC生成的合成数据，并结合自然语言描述，建立了一种伦理语言-图像模型，在视网膜图像分析中取得了竞争性表现。

    

    通用基础模型在医学领域引入了新的能力。然而，高质量注释数据需求与患者隐私之间的矛盾持续加剧。利用医学人工智能生成的内容（Med-AIGC）作为一个源源不断的资源库，成为解决上述挑战的潜在解决方案。本文利用100万个开源合成底片图像与自然语言描述，策划了一种名为VisionCLIP的伦理语言-图像基础模型，用于视网膜图像分析。VisionCLIP以零-shot方式在三个外部数据集上实现了与现有方法在真实数据上预训练的竞争性性能。在训练过程中同时使用人工合成图像和相应的文本数据使得这个医学基础模型成功地吸收了疾病知识。

    arXiv:2403.10823v1 Announce Type: cross  Abstract: Generalist foundation model has ushered in newfound capabilities in medical domain. However, the contradiction between the growing demand for high-quality annotated data with patient privacy continues to intensify. The utilization of medical artificial intelligence generated content (Med-AIGC) as an inexhaustible resource repository arises as a potential solution to address the aforementioned challenge. Here we harness 1 million open-source synthetic fundus images paired with natural language descriptions, to curate an ethical language-image foundation model for retina image analysis named VisionCLIP. VisionCLIP achieves competitive performance on three external datasets compared with the existing method pre-trained on real-world data in a zero-shot fashion. The employment of artificially synthetic images alongside corresponding textual data for training enables the medical foundation model to successfully assimilate knowledge of disea
    
[^131]: 非平稳随机赌博机的激励探索

    Incentivized Exploration of Non-Stationary Stochastic Bandits

    [https://arxiv.org/abs/2403.10819](https://arxiv.org/abs/2403.10819)

    提出了针对非平稳随机赌博机的激励探索算法，实现了随时间的子线性遗憾和补偿

    

    我们研究了多臂赌博机（MAB）问题中的激励探索，其中玩家通过探索除了贪婪选择之外的臂获得补偿，并且可能对奖励提供偏倚反馈。我们考虑了两种不同的非平稳环境：突变和持续变化，并提出了相应的激励探索算法。我们展示了所提出的算法实现了随时间的子线性遗憾和补偿，从而有效地激励了探索，尽管存在非平稳性和偏倚或漂移反馈。

    arXiv:2403.10819v1 Announce Type: cross  Abstract: We study incentivized exploration for the multi-armed bandit (MAB) problem with non-stationary reward distributions, where players receive compensation for exploring arms other than the greedy choice and may provide biased feedback on the reward. We consider two different non-stationary environments: abruptly-changing and continuously-changing, and propose respective incentivized exploration algorithms. We show that the proposed algorithms achieve sublinear regret and compensation over time, thus effectively incentivizing exploration despite the nonstationarity and the biased or drifted feedback.
    
[^132]: 由语音驱动的个性化手势合成：利用自动模糊特征推断

    Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference

    [https://arxiv.org/abs/2403.10805](https://arxiv.org/abs/2403.10805)

    提出了一种新颖的端到端生成模型 Persona-Gestor，通过模糊特征提取器和非自回归自适应层归一化变压器扩散架构，仅依赖原始语音音频生成高度个性化的3D全身手势。

    

    语音驱动的手势生成是虚拟人类创造中一个新兴领域。然而，一个重要的挑战在于准确确定和处理多种输入特征（如声学、语义、情感、个性，甚至微妙的未知特征）。传统方法依赖于各种显式特征输入和复杂的多模态处理，限制了生成手势的表现力并限制了它们的适用性。为了解决这些挑战，我们提出Persona-Gestor，这是一个新颖的端到端生成模型，专门设计用于仅依赖原始语音音频生成高度个性化的3D全身手势。该模型结合了模糊特征提取器和非自回归自适应层归一化（AdaLN）变压器扩散架构。模糊特征提取器利用模糊推断策略，自动推断出隐含的连续模糊特征。

    arXiv:2403.10805v1 Announce Type: cross  Abstract: Speech-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw speech audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture. The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy feature
    
[^133]: 利用基于多测试的逐层特征融合增强超出分布检测

    Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion

    [https://arxiv.org/abs/2403.10803](https://arxiv.org/abs/2403.10803)

    提出了一种名为MLOD的新框架，利用多测试过程在不同级别的特征中识别测试样本中的分布偏移，无需修改预训练模型结构。

    

    在开放环境中部署机器学习会遇到一个挑战，即遇到与训练数据显著不同的各种测试输入，这些超出分布的样本可能在局部或全局特征上与训练分布有所偏移。本文提出了一种新颖的框架，名为基于多测试的逐层超出分布（OOD）检测（MLOD），通过严格的多个测试过程在不同级别的特征中鉴别测试样本中的分布偏移。我们的方法不同于现有方法，因为它不需要修改预训练的深度神经网络的结构或微调。

    arXiv:2403.10803v1 Announce Type: cross  Abstract: Deploying machine learning in open environments presents the challenge of encountering diverse test inputs that differ significantly from the training data. These out-of-distribution samples may exhibit shifts in local or global features compared to the training distribution. The machine learning (ML) community has responded with a number of methods aimed at distinguishing anomalous inputs from original training data. However, the majority of previous studies have primarily focused on the output layer or penultimate layer of pre-trained deep neural networks. In this paper, we propose a novel framework, Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to identify distributional shifts in test samples at different levels of features through rigorous multiple testing procedure. Our approach distinguishes itself from existing methods as it does not require modifying the structure or fine-tuning of the pre-trained c
    
[^134]: 使用自适应估计融合高效剪枝大型语言模型

    Efficient Pruning of Large Language Model with Adaptive Estimation Fusion

    [https://arxiv.org/abs/2403.10799](https://arxiv.org/abs/2403.10799)

    提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。

    

    大型语言模型（LLMs）已经成为许多生成性下游任务中至关重要的组成部分，这导致在资源受限设备上高效部署它们成为不可避免的趋势和重大挑战。结构化剪枝是解决这一挑战的广泛应用方法。然而，当处理多个解码器层的复杂结构时，通常的方法往往采用常见的估计方法进行剪枝。这些方法导致特定下游任务精度下降。本文介绍了一种简单而有效的方法，可自适应地模拟每个子结构的重要性。同时，它可以基于复杂和多层结构的结果，自适应地融合粗粒度和细粒度的估计。我们设计的所有方面都无缝集成到端到端的剪枝框架中。与主流数据集上的最先进方法相比，我们的实验结果表明

    arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
    
[^135]: 从单词到路径：将大型语言模型应用于车辆路径规划

    From Words to Routes: Applying Large Language Models to Vehicle Routing

    [https://arxiv.org/abs/2403.10795](https://arxiv.org/abs/2403.10795)

    该研究探索了应用大型语言模型解决车辆路径规划问题的能力，提出了基于自然语言任务描述的基本提示范例并提出一种使模型进行改进的框架。

    

    LLMs在机器人领域（例如操作和导航）中展示出令人印象深刻的进展，其利用自然语言任务描述。LLMs在这些任务中取得成功让我们思考：LLMs在使用自然语言任务描述解决车辆路径规划问题（VRPs）的能力如何？在这项工作中，我们分三步研究这个问题。首先，我们构建了一个包含21种单车或多车路径问题的数据集。其次，我们评估了LLMs在四种基本提示范例文本到代码生成的性能，每种包括不同类型的文本输入。我们发现，直接从自然语言任务描述生成代码的基本提示范例对于GPT-4效果最佳，实现了56%的可行性，40%的优化性和53%的效率。第三，基于观察到LLMs可能无法在初始尝试中提供正确解决方案的现象，我们提出了一个框架，使LLMs能够进行改进。

    arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
    
[^136]: 具有监督对比时间变换器的时间序列表示学习

    Time Series Representation Learning with Supervised Contrastive Temporal Transformer

    [https://arxiv.org/abs/2403.10787](https://arxiv.org/abs/2403.10787)

    提出了一种名为SCOTT的具有监督对比变换器的时间序列表示学习模型，结合了Transformer和Temporal Convolutional Networks以学习全局和局部特征，并简化了用于标记时间序列数据的监督对比损失。

    

    找到时间序列数据的有效表示是一项有用但具有挑战性的任务。有些工作利用自监督或无监督学习方法来解决这个问题。然而，如何利用可用的标签信息来获得更好的表示仍然是一个悬而未决的问题。为了回答这个问题，我们利用时间序列和表示学习领域中的现有技术，开发了一个简单但新颖的融合模型，称为：\textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT)。我们首先研究了适用于各种类型时间序列数据的合适增强方法，以帮助学习具有变化不变性的表示。其次，我们以简单的方式结合了Transformer和Temporal Convolutional Networks，以有效地学习全局和局部特征。最后，我们简化了用于标记时间序列数据表示学习的监督对比损失。

    arXiv:2403.10787v1 Announce Type: cross  Abstract: Finding effective representations for time series data is a useful but challenging task. Several works utilize self-supervised or unsupervised learning methods to address this. However, there still remains the open question of how to leverage available label information for better representations. To answer this question, we exploit pre-existing techniques in time series and representation learning domains and develop a simple, yet novel fusion model, called: \textbf{S}upervised \textbf{CO}ntrastive \textbf{T}emporal \textbf{T}ransformer (SCOTT). We first investigate suitable augmentation methods for various types of time series data to assist with learning change-invariant representations. Secondly, we combine Transformer and Temporal Convolutional Networks in a simple way to efficiently learn both global and local features. Finally, we simplify Supervised Contrastive Loss for representation learning of labelled time series data. We p
    
[^137]: 探索中国幽默生成：关于两句典故谚语的研究

    Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings

    [https://arxiv.org/abs/2403.10781](https://arxiv.org/abs/2403.10781)

    本文研究了如何利用最先进的语言模型来理解和生成中国幽默，特别是关于训练模型生成典故谚语。他们采用新颖的fine-tuning方法，包含融合拼音嵌入和对比学习，从而成功生成幽默性典故谚语。

    

    幽默是人类语言中富有文化内涵的一个方面，对于计算机理解和生成而言具有挑战性，尤其是在相对未被自然语言处理社区探索的中国幽默领域。本文研究了最先进语言模型在理解和生成中国幽默方面的能力，特别聚焦于训练它们创造典故谚语。我们采用了两种显著的训练方法：调整一个中等规模的语言模型和提示一个大型模型。我们的新颖调整方法融合了拼音嵌入以考虑同音异义词，并采用对比学习与合成困难性负例以区分幽默元素。人类注释的结果显示这些模型能够生成幽默的典故谚语，提示法证明是一个实用且有效的方法。然而，在生成与人类创造力匹配的典故谚语方面仍有改进空间。

    arXiv:2403.10781v1 Announce Type: cross  Abstract: Humor, a culturally nuanced aspect of human language, poses challenges for computational understanding and generation, especially in Chinese humor, which remains relatively unexplored in the NLP community. This paper investigates the capability of state-of-the-art language models to comprehend and generate Chinese humor, specifically focusing on training them to create allegorical sayings. We employ two prominent training methods: fine-tuning a medium-sized language model and prompting a large one. Our novel fine-tuning approach incorporates fused Pinyin embeddings to consider homophones and employs contrastive learning with synthetic hard negatives to distinguish humor elements. Human-annotated results show that these models can generate humorous allegorical sayings, with prompting proving to be a practical and effective method. However, there is still room for improvement in generating allegorical sayings that match human creativity.
    
[^138]: 物体任意模型（SAOM）：用于多类多实例分割的实际到仿真微调策略

    Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation

    [https://arxiv.org/abs/2403.10780](https://arxiv.org/abs/2403.10780)

    提出了一种新颖的域不变的Real-to-Simulation微调策略，以实现多类多实例分割中Segment Any Object Model（SAOM）的“一切”模式工作，并在室内场景理解中发挥关键作用。

    

    多类多实例分割是识别图像中多个对象类别和同一类别的多个实例的任务。基础的“Segment Anything Model”（SAM）专门设计用于多类多实例分割，但在各种真实世界应用中往往会以“一切”模式输出部分或子部分掩模。整体物体分割掩模在室内场景理解中发挥着至关重要的作用，尤其在机器人应用中。我们提出了一种新的域不变的Real-to-Simulation（Real-Sim）微调策略用于SAM。我们在微调（真实到仿真）期间使用从Ai2Thor模拟器收集的物体图像和地面真实数据。为了允许我们的Segment Any Object Model（SAOM）在“一切”模式下工作，我们提出了一种新颖的最近邻分配方法，更新每个地面真实掩模的点嵌入。SAOM在我们从Ai2T收集的自己的数据集上进行了评估。

    arXiv:2403.10780v1 Announce Type: cross  Abstract: Multi-class multi-instance segmentation is the task of identifying masks for multiple object classes and multiple instances of the same class within an image. The foundational Segment Anything Model (SAM) is designed for promptable multi-class multi-instance segmentation but tends to output part or sub-part masks in the "everything" mode for various real-world applications. Whole object segmentation masks play a crucial role for indoor scene understanding, especially in robotics applications. We propose a new domain invariant Real-to-Simulation (Real-Sim) fine-tuning strategy for SAM. We use object images and ground truth data collected from Ai2Thor simulator during fine-tuning (real-to-sim). To allow our Segment Any Object Model (SAOM) to work in the "everything" mode, we propose the novel nearest neighbour assignment method, updating point embeddings for each ground-truth mask. SAOM is evaluated on our own dataset collected from Ai2T
    
[^139]: 从大熔炉到误传：探讨生成式AI中的有害影响

    From Melting Pots to Misrepresentations: Exploring Harms in Generative AI

    [https://arxiv.org/abs/2403.10776](https://arxiv.org/abs/2403.10776)

    探索生成式AI中的社会有害影响，提出对多样性和公平性的关切，并引领讨论在这些模型中的重要性，同时提出未来研究方向。

    

    随着Gemini和GPT等先进生成模型的广泛应用，这些模型被纳入AI作为服务（AIaaS）的社会技术系统中，跨越多个领域。尽管如此，对这些模型在各种社会人口维度上的歧视倾向仍存在关切，尤其是倾向于某些“多数群体”人口统计特征。尽管有关媒体呼吁多样化代表的广泛，但在AIaaS背景下，边缘化种族和民族群体仍然面临持续扭曲、刻板印象和忽视。本文对社会有害研究现状进行了关键总结，引领讨论重点放在它们的影响上。我们还提出了本讨论指引下的开放式研究问题，以帮助确定未来研究途径。

    arXiv:2403.10776v1 Announce Type: cross  Abstract: With the widespread adoption of advanced generative models such as Gemini and GPT, there has been a notable increase in the incorporation of such models into sociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite their versatility across diverse sectors, concerns persist regarding discriminatory tendencies within these models, particularly favoring selected `majority' demographics across various sociodemographic dimensions. Despite widespread calls for diversification of media representations, marginalized racial and ethnic groups continue to face persistent distortion, stereotyping, and neglect within the AIaaS context. In this work, we provide a critical summary of the state of research in the context of social harms to lead the conversation to focus on their implications. We also present open-ended research questions, guided by our discussion, to help define future research pathways.
    
[^140]: 韩国会话中情绪因果识别的图卷积网络研究

    ECRC: Emotion-Causality Recognition in Korean Conversation for GCN

    [https://arxiv.org/abs/2403.10764](https://arxiv.org/abs/2403.10764)

    本研究提出了ECRC模型，通过结合单词级和句子级嵌入，以及基于新颖图结构的方法，在韩国会话环境中进行情绪因果识别研究。

    

    在这项多任务学习研究中，我们同时分析了会话环境中的情感及其潜在原因，采用深度神经网络方法有效处理和训练大规模的标记数据集。我们克服了以往嵌入方法的局限性，利用了单词级和句子级嵌入。此外，我们提出了基于新颖图结构的情绪因果识别模型（ECRC），从而借鉴了两种嵌入方法的优势。该模型独特地集成了双向长短期记忆（Bi-LSTM）和图神经网络。

    arXiv:2403.10764v1 Announce Type: cross  Abstract: In this multi-task learning study on simultaneous analysis of emotions and their underlying causes in conversational contexts, deep neural network methods were employed to effectively process and train large labeled datasets. However, these approaches are typically limited to conducting context analyses across the entire corpus because they rely on one of the two methods: word- or sentence-level embedding. The former struggles with polysemy and homonyms, whereas the latter causes information loss when processing long sentences. In this study, we overcome the limitations of previous embeddings by utilizing both word- and sentence-level embeddings. Furthermore, we propose the emotion-causality recognition in conversation (ECRC) model, which is based on a novel graph structure, thereby leveraging the strengths of both embedding methods. This model uniquely integrates the bidirectional long short-term memory (Bi-LSTM) and graph neural netw
    
[^141]: 通过混合动作深度强化学习调度无人机和移动充电器

    Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.10761](https://arxiv.org/abs/2403.10761)

    本文提出了一个通过深度强化学习对无人机和移动充电器进行调度的方法，以解决无人机在观察任务中的航线规划和充电优化问题。

    

    最近，工业界和学术界对使用无线充电器延长无人机操作寿命的兴趣日益增长。本文考虑了一种受到充电器辅助的无人机应用：无人机部署到观察一组兴趣点，而充电器可以移动以为无人机充电。我们专注于无人机和移动充电器的航线和充电计划，以在尽可能短的时间内获得高观察效用，同时确保无人机在任务执行期间保持可操作状态。本文提出的无人机-充电器调度问题本质上是一个多阶段决策过程，在该过程中，无人机和移动充电器充当两个代理商合作完成任务。这两个代理商的离散连续混合动作空间在我们的问题中构成了一个重要挑战。为了解决这个问题，我们提出了一种混合动作d

    arXiv:2403.10761v1 Announce Type: new  Abstract: Recently there has been a growing interest in industry and academia, regarding the use of wireless chargers to prolong the operational longevity of unmanned aerial vehicles (commonly knowns as drones). In this paper we consider a charger-assisted drone application: a drone is deployed to observe a set points of interest, while a charger can move to recharge the drone's battery. We focus on the route and charging schedule of the drone and the mobile charger, to obtain high observation utility with the shortest possible time, while ensuring the drone remains operational during task execution. Essentially, this proposed drone-charger scheduling problem is a multi-stage decision-making process, in which the drone and the mobile charger act as two agents who cooperate to finish a task. The discrete-continuous hybrid action space of the two agents poses a significant challenge in our problem. To address this issue, we present a hybrid-action d
    
[^142]: LIGHTCODE：具有反馈通道的光解析和神经编码

    LIGHTCODE: Light Analytical and Neural Codes for Channels with Feedback

    [https://arxiv.org/abs/2403.10751](https://arxiv.org/abs/2403.10751)

    本文提出了一种LIGHTCODE轻量级神经编码方案，在具备解释性的基础上，在低信噪比区域实现了最先进的可靠性。

    

    通道反馈中可靠且高效的编码方案设计一直是通信理论中一项长期挑战。虽然深度学习技术取得了显著进展，神经编码往往面临计算成本高、缺乏可解释性以及在资源受限环境中的实用性有限等问题。本文旨在设计解释性强且更适用于通信系统的低复杂度编码方案。我们先进了解析编码和神经编码。首先，我们展示了POWERBLAST，一种受Schalkwijk-Kailath（SK）和Gallager-Nakiboglu（GN）方案启发的解析编码方案，在高信噪比（SNR）区域实现了明显的可靠性改进，胜过神经编码。接下来，为了增强低SNR区域的可靠性，我们提出了LIGHTCODE，一种轻量级神经编码，实现了最先进的可靠性。

    arXiv:2403.10751v1 Announce Type: cross  Abstract: The design of reliable and efficient codes for channels with feedback remains a longstanding challenge in communication theory. While significant improvements have been achieved by leveraging deep learning techniques, neural codes often suffer from high computational costs, a lack of interpretability, and limited practicality in resource-constrained settings. We focus on designing low-complexity coding schemes that are interpretable and more suitable for communication systems. We advance both analytical and neural codes. First, we demonstrate that POWERBLAST, an analytical coding scheme inspired by Schalkwijk-Kailath (SK) and Gallager-Nakiboglu (GN) schemes, achieves notable reliability improvements over both SK and GN schemes, outperforming neural codes in high signal-to-noise ratio (SNR) regions. Next, to enhance reliability in low-SNR regions, we propose LIGHTCODE, a lightweight neural code that achieves state-of-the-art reliability
    
[^143]: 社交媒体上利用大语言模型进行抑郁症检测

    Depression Detection on Social Media with Large Language Models

    [https://arxiv.org/abs/2403.10750](https://arxiv.org/abs/2403.10750)

    提出了名为DORIS的新型抑郁症检测系统，将医学知识和大语言模型的最新进展相结合，通过分析个人在社交媒体上的帖子历史记录来确定抑郁症患者，以提高早期检测和干预。

    

    抑郁症造成危害。然而，由于缺乏心理健康意识和对病症耻辱感的恐惧，许多患者并未积极寻求诊断和治疗，导致不利后果。抑郁症检测旨在通过分析社交媒体上个人帖子的历史记录来确定个体是否患有抑郁症，这可显著有助于早期检测和干预。它主要面临两个关键挑战：1）需要专业医学知识，2）需要高准确性和可解释性。为了解决这一问题，我们提出了一个名为DORIS的新型抑郁症检测系统，结合了医学知识和大语言模型的最新进展。具体来说，为了解决第一个挑战，我们提出了一种基于大语言模型的解决方案，首先对高危文本进行标注以确定是否符合医学诊断标准。

    arXiv:2403.10750v1 Announce Type: cross  Abstract: Depression harms. However, due to a lack of mental health awareness and fear of stigma, many patients do not actively seek diagnosis and treatment, leading to detrimental outcomes. Depression detection aims to determine whether an individual suffers from depression by analyzing their history of posts on social media, which can significantly aid in early detection and intervention. It mainly faces two key challenges: 1) it requires professional medical knowledge, and 2) it necessitates both high accuracy and explainability. To address it, we propose a novel depression detection system called DORIS, combining medical knowledge and the recent advances in large language models (LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based solution to first annotate whether high-risk texts meet medical diagnostic criteria. Further, we retrieve texts with high emotional intensity and summarize critical information from the his
    
[^144]: 游戏与参考：流行病防控政策组合综合

    Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control

    [https://arxiv.org/abs/2403.10744](https://arxiv.org/abs/2403.10744)

    提出了一种新颖的流行病政策制定模型-政策组合综合（PCS）模型，通过引入对抗学习来避免极端决策，提高政策的人性化，同时最小化次优历史政策对决策模型训练的影响。

    

    近年来，流行病制定政策模型越来越被用来为各级政府提供制定防控SARS、H1N1和COVID-19等灾难性流行病政策的参考。现有研究目前存在两个问题：首先，既有方法基于效果评估制定政策，由于真实决策中很少能被建模的因素，输出的政策很容易变得极端。其次，人类的主观性和认知局限性使得历史政策并不总是最优的决策模型训练。因此，我们提出了一种新颖的流行病政策制定模型-政策组合综合（PCS）模型。特别地，为预防极端决策，我们引入了模型制定政策和实际政策之间的对抗学习，以迫使输出的政策更符合人类喜好。另一方面，为了减小次优历史政策的影响

    arXiv:2403.10744v1 Announce Type: new  Abstract: In recent years, epidemic policy-making models are increasingly being used to provide reference for governors on prevention and control policies against catastrophic epidemics such as SARS, H1N1 and COVID-19. Existing studies are currently constrained by two issues: First, previous methods develop policies based on effect evaluation, since few of factors in real-world decision-making can be modeled, the output policies will then easily become extreme. Second, the subjectivity and cognitive limitation of human make the historical policies not always optimal for the training of decision models. To these ends, we present a novel Policy Combination Synthesis (PCS) model for epidemic policy-making. Specially, to prevent extreme decisions, we introduce adversarial learning between the model-made policies and the real policies to force the output policies to be more human-liked. On the other hand, to minimize the impact of sub-optimal historica
    
[^145]: 针对非平稳线性赌博机的方差相关遗憾界限

    Variance-Dependent Regret Bounds for Non-stationary Linear Bandits

    [https://arxiv.org/abs/2403.10732](https://arxiv.org/abs/2403.10732)

    提出利用奖励分布方差和变化预算的算法，可以实现更紧的遗憾上限界限。

    

    我们研究了非平稳随机线性赌博机问题，其中奖励分布每一轮都在演变。现有算法通过总变化预算$B_K$来表征非平稳性，该预算是线性赌博机每$K$轮连续特征向量变化的总和。然而，这样的量只衡量了相对于奖励分布期望的非平稳性，这使得现有算法在一般非平稳分布情况下表现不佳。在这项工作中，我们提出了利用奖励分布方差以及$B_K$的算法，并展示它们可以实现更紧的遗憾上限界限。具体来说，我们介绍了两种新算法: 重新启动的加权$\text{OFUL}^+$和重新启动的$\text{SAVE}^+$。这些算法分别处理了奖励方差信息已知和未知的情况。

    arXiv:2403.10732v1 Announce Type: cross  Abstract: We investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. Existing algorithms characterize the non-stationarity by the total variation budget $B_K$, which is the summation of the change of the consecutive feature vectors of the linear bandits over $K$ rounds. However, such a quantity only measures the non-stationarity with respect to the expectation of the reward distribution, which makes existing algorithms sub-optimal under the general non-stationary distribution setting. In this work, we propose algorithms that utilize the variance of the reward distribution as well as the $B_K$, and show that they can achieve tighter regret upper bounds. Specifically, we introduce two novel algorithms: Restarted Weighted$\text{OFUL}^+$ and Restarted $\text{SAVE}^+$. These algorithms address cases where the variance information of the rewards is known and unknown, respectively. Notably, when
    
[^146]: 针对零星刚性流式任务的严格分区方法

    Strict Partitioning for Sporadic Rigid Gang Tasks

    [https://arxiv.org/abs/2403.10726](https://arxiv.org/abs/2403.10726)

    提出了一种新的严格分区调度策略，用于零星刚性流式任务，通过创建不相交的任务和处理器分区，并尝试将相似容量的任务分配给同一分区，以减少干扰。

    

    刚性流式任务模型基于在固定数量的处理器上同时执行多个线程以提高效率和性能的思想。虽然全局刚性流式调度有大量文献，但分区方法具有几个实际优势（例如任务隔离和减少调度开销）。本文提出了一种新的用于刚性流式任务的分区调度策略，称为严格分区。该方法创建任务和处理器的不相交分区，以避免分区间干扰。此外，它尝试将具有相似容量（即并行性）的任务分配给同一分区，以减少分区内干扰。在每个分区内，任务可以使用任何类型的调度器进行调度，这允许使用不那么悲观的可调度测试。大量的合成实验证明和基于Edge TPU基准的案例研究显示

    arXiv:2403.10726v1 Announce Type: cross  Abstract: The rigid gang task model is based on the idea of executing multiple threads simultaneously on a fixed number of processors to increase efficiency and performance. Although there is extensive literature on global rigid gang scheduling, partitioned approaches have several practical advantages (e.g., task isolation and reduced scheduling overheads). In this paper, we propose a new partitioned scheduling strategy for rigid gang tasks, named strict partitioning. The method creates disjoint partitions of tasks and processors to avoid inter-partition interference. Moreover, it tries to assign tasks with similar volumes (i.e., parallelisms) to the same partition so that the intra-partition interference can be reduced. Within each partition, the tasks can be scheduled using any type of scheduler, which allows the use of a less pessimistic schedulability test. Extensive synthetic experiments and a case study based on Edge TPU benchmarks show th
    
[^147]: 开发和应用蒙特卡洛树搜索算法模拟达芬奇密码游戏策略

    Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies

    [https://arxiv.org/abs/2403.10720](https://arxiv.org/abs/2403.10720)

    本研究通过实现和评估两个MCTS算法变体，发现达芬奇密码棋盘游戏中的分支分歧明显阻碍了GPU执行时的并行性。

    

    在这项研究中，我们探讨了蒙特卡洛树搜索（MCTS）的效率，这是一种著名的决策算法，以其在复杂决策环境中的有效性而闻名，取决于进行的模拟数量。尽管该算法具有广泛的适用性，但在某些场景中，特别是在游戏策略开发领域，其性能可能会受到不利影响。本研究认为，达芬奇密码棋盘游戏中固有的分支分歧显著阻碍了在图形处理单元（GPU）上执行时的并行性。为了调查这一假设，我们实现并细致评估了两个变体的MCTS算法，专门设计用于评估分支分歧对计算性能的影响。我们的比较分析显示，与基于CPU的实现相比，在性能方面存在线性改进，与GPU实现形成鲜明对比。

    arXiv:2403.10720v1 Announce Type: new  Abstract: In this study, we explore the efficiency of the Monte Carlo Tree Search (MCTS), a prominent decision-making algorithm renowned for its effectiveness in complex decision environments, contingent upon the volume of simulations conducted. Notwithstanding its broad applicability, the algorithm's performance can be adversely impacted in certain scenarios, particularly within the domain of game strategy development. This research posits that the inherent branch divergence within the Da Vinci Code board game significantly impedes parallelism when executed on Graphics Processing Units (GPUs). To investigate this hypothesis, we implemented and meticulously evaluated two variants of the MCTS algorithm, specifically designed to assess the impact of branch divergence on computational performance. Our comparative analysis reveals a linear improvement in performance with the CPU-based implementation, in stark contrast to the GPU implementation, which 
    
[^148]: 揭示后门秘密：利用优化的比例预测一致性识别后门数据

    Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency

    [https://arxiv.org/abs/2403.10717](https://arxiv.org/abs/2403.10717)

    通过利用比例预测一致性技术，本研究提出了一种在被毒化数据集中自动识别后门数据的方法，无需额外干净数据或手动定义后门检测阈值。

    

    现代机器学习（ML）系统需要大量的训练数据，通常需要借助外部来源。然而，这种做法使它们容易受到后门毒化攻击。先前的后门防御策略主要集中在识别被植入后门的模型或被毒化数据的特征上，通常在假设可以访问干净数据的情况下运作。在这项工作中，我们探讨了一个相对不太被探索的挑战：在被毒化的数据集中自动识别后门数据，而且在现实条件下，即不需要额外的干净数据或不需要手动定义后门检测的阈值。我们从比例预测一致性（SPC）技术中汲取灵感，该技术利用被毒化数据对输入缩放因子的预测不变性。基于此，我们将后门数据识别问题建模为一个分层数据划分优化问题。

    arXiv:2403.10717v1 Announce Type: cross  Abstract: Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimizatio
    
[^149]: 利用LLMs集成揭示社交媒体消息的潜在主题：气候运动案例研究

    Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns

    [https://arxiv.org/abs/2403.10707](https://arxiv.org/abs/2403.10707)

    本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。

    

    本文介绍了一种揭示和分析社交媒体消息主题的新方法。鉴于传统主题级分析的局限性，往往只捕捉到整体模式，本研究强调了对更精细、主题聚焦的探索的需求。传统的主题发现方法，涉及手动流程和人在循环中的方法，具有价值，但在伸缩性、一致性和资源强度方面面临挑战，涉及时间和成本。为了应对这些挑战，我们提出了一种利用大型语言模型（LLMs）先进功能的机器在循环中方法。这种方法允许更深入地调查社交媒体话语的主题方面，使我们能够揭示多样的主题，每个主题具有独特的特征和相关性，从而提供对更广泛主题内有的微妙细节的全面理解。

    arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
    
[^150]: PERL: 从人类反馈中实现参数高效强化学习

    PERL: Parameter Efficient Reinforcement Learning from Human Feedback

    [https://arxiv.org/abs/2403.10704](https://arxiv.org/abs/2403.10704)

    使用低秩适应（LoRA）方法进行参数高效强化学习（PERL），能够在与传统RLHF设置相当的性能下，实现更快的训练和更少的内存占用。

    

    强化学习从人类反馈（RLHF）已被证明是一种将预训练的大型语言模型（LLMs）与人类偏好对齐的有效方法。然而，使用RLHF训练模型计算成本高昂，且整个过程复杂。在本研究中，我们研究了RLHF，其中基础模型使用胡等人提出的低秩适应（LoRA）的参数高效方法进行训练。我们探讨了“参数高效强化学习”（PERL）的设置，在其中我们使用LoRA进行奖励模型训练和强化学习。我们将PERL与传统的微调（全调）在包括2个新数据集在内的7个基准测试中的奖励建模和强化学习方面的各种配置进行了比较。我们发现，PERL的性能与传统的RLHF设置相当，同时训练速度更快，内存占用更少。这使得RLHF具有很高的性能，同时减少了计算成本。

    arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational 
    
[^151]: 注意错误！检测和定位视觉与语言导航中的指令错误

    Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation

    [https://arxiv.org/abs/2403.10700](https://arxiv.org/abs/2403.10700)

    提出了一个新的基准数据集，首次引入了各种类型的指令错误，考虑到潜在的人类原因，以评估连续环境中 VLN 系统的健壮性

    

    Vision-and-Language Navigation in Continuous Environments (VLN-CE) 是一项直观且具有挑战性的体验智能任务。代理人被要求通过执行一系列低级动作、遵循一系列自然语言指令来导航到目标目标。所有文献中的 VLN-CE 方法都假设语言指令是准确的。然而，在实践中，人类给出的指令可能由于不准确的记忆或混淆而包含空间环境描述中的错误。当前 VLN-CE 基准没有解决这种情况，使得 VLN-CE 中的最新方法在面对来自人类用户的错误指令时变得脆弱。我们首次提出了一个引入各种类型指令错误考虑潜在人类原因的新型基准数据集。该基准数据集为连续环境中的 VLN 系统的健壮性提供了宝贵的见解。我们观察到 noticeable...

    arXiv:2403.10700v1 Announce Type: cross  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable 
    
[^152]: 针对噪声脑部MRI的稳健基于影响力的训练方法

    Robust Influence-based Training Methods for Noisy Brain MRI

    [https://arxiv.org/abs/2403.10698](https://arxiv.org/abs/2403.10698)

    该研究提出了两种基于影响函数的稳健训练方法，针对噪声MRI图像进行脑部肿瘤分类，可提高模型的鲁棒性。

    

    正确分类脑部肿瘤对及时和准确治疗患者至关重要。虽然已经有几种基于经典图像处理或深度学习方法的分类算法被提出来快速分类MR图像中的肿瘤，但大多数假定了训练数据是无噪声的不切实际情况。在这项工作中，我们研究了在噪声的MR图像上训练深度学习模型以分类脑部肿瘤的困难但现实的设置。我们提出了两种稳健于噪声MRI训练数据的训练方法，即基于影响力的样本重新加权（ISR）和基于影响力的样本扰动（ISP），这两种方法都基于鲁棒统计中的影响函数。在ISR中，我们根据训练过程中样本对训练的帮助/危害程度自适应地重新加权训练样本，而在ISP中，我们根据影响得分量身定制并注入有帮助的扰动。ISR和ISP均可以提高模型的鲁棒性。

    arXiv:2403.10698v1 Announce Type: cross  Abstract: Correctly classifying brain tumors is imperative to the prompt and accurate treatment of a patient. While several classification algorithms based on classical image processing or deep learning methods have been proposed to rapidly classify tumors in MR images, most assume the unrealistic setting of noise-free training data. In this work, we study a difficult but realistic setting of training a deep learning model on noisy MR images to classify brain tumors. We propose two training methods that are robust to noisy MRI training data, Influence-based Sample Reweighing (ISR) and Influence-based Sample Perturbation (ISP), which are based on influence functions from robust statistics. Using the influence functions, in ISR, we adaptively reweigh training examples according to how helpful/harmful they are to the training process, while in ISP, we craft and inject helpful perturbation proportional to the influence score. Both ISR and ISP harden
    
[^153]: EXPLORER：探索引导的文本强化学习

    EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning

    [https://arxiv.org/abs/2403.10692](https://arxiv.org/abs/2403.10692)

    提出了一种名为EXPLORER的探索引导推理代理，用于文本强化学习，能够解决智能体在多个游戏中泛化并在已知和未知对象上表现良好的关键挑战。

    

    文本游戏（TBGs）已经成为自然语言处理任务的一个重要集合，要求强化学习（RL）代理结合自然语言理解和推理。本文提出了一种名为EXPLORER的探索引导推理代理，用于文本强化学习，旨在解决智能体在多个游戏中泛化并在已知和未知对象上表现良好的关键挑战。

    arXiv:2403.10692v1 Announce Type: cross  Abstract: Text-based games (TBGs) have emerged as an important collection of NLP tasks, requiring reinforcement learning (RL) agents to combine natural language understanding with reasoning. A key challenge for agents attempting to solve such tasks is to generalize across multiple games and demonstrate good performance on both seen and unseen objects. Purely deep-RL-based approaches may perform well on seen objects; however, they fail to showcase the same performance on unseen objects. Commonsense-infused deep-RL agents may work better on unseen data; unfortunately, their policies are often not interpretable or easily transferable. To tackle these issues, in this paper, we present EXPLORER which is an exploration-guided reasoning agent for textual reinforcement learning. EXPLORER is neurosymbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation. It can also learn generalized symbolic policies and 
    
[^154]: MYTE：形态学驱动的字节编码，用于更好、更公平的多语言语言建模

    MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling

    [https://arxiv.org/abs/2403.10691](https://arxiv.org/abs/2403.10691)

    MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。

    

    多语言语言建模中的一个主要考虑因素是如何最好地表示具有不同词汇和文字的语言。尽管当代文本编码方法涵盖了大多数世界文字系统，但它们存在偏向于全球西方高资源语言的问题。因此，少数语言的文本往往被分割为一长串在语言学上毫无意义的单元。为了解决这种不平等，我们引入了一种新的范式，用跨不同语言具有一致大小的片段来编码相同的信息。我们的编码约定（MYTE）基于形态素，因为它们的库存在各种语言中比字符更平衡，而以前的方法使用字符。我们展示MYTE为所有99种分析语言产生了更短的编码，其中非欧洲语言和非拉丁文字的改进最为显著。这进而改善了多语言语言建模的性能。

    arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
    
[^155]: AutoHLS：学习加速HLS设计空间探索

    AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs

    [https://arxiv.org/abs/2403.10686](https://arxiv.org/abs/2403.10686)

    AutoHLS提出了一个集成深度神经网络和贝叶斯优化的框架，用于加速HLS硬件设计优化，实现高达70倍的探索时间加速

    

    高级综合（HLS）是一种设计流程，利用现代语言特性和灵活性，如复杂数据结构，继承，模板等，快速原型化硬件设计。然而，探索各种设计空间参数可能需要硬件工程师花费大量时间和精力，以满足特定设计规格。本文提出了一个名为AutoHLS的新颖框架，将深度神经网络（DNN）与贝叶斯优化（BO）集成，加速HLS硬件设计优化。我们的工具专注于HLS pragma探索和操作转换。它利用集成的DNN来预测在给定FPGA资源预算下的综合性。我们还探讨了量子神经网络（QNNs）的潜力，取代AutoHLS流水线中的经典DNNs。我们的实验结果表明，在探索时间上加速了多达70倍。

    arXiv:2403.10686v1 Announce Type: cross  Abstract: High-level synthesis (HLS) is a design flow that leverages modern language features and flexibility, such as complex data structures, inheritance, templates, etc., to prototype hardware designs rapidly. However, exploring various design space parameters can take much time and effort for hardware engineers to meet specific design specifications. This paper proposes a novel framework called AutoHLS, which integrates a deep neural network (DNN) with Bayesian optimization (BO) to accelerate HLS hardware design optimization. Our tool focuses on HLS pragma exploration and operation transformation. It utilizes integrated DNNs to predict synthesizability within a given FPGA resource budget. We also investigate the potential of emerging quantum neural networks (QNNs) instead of classical DNNs for the AutoHLS pipeline. Our experimental results demonstrate up to a 70-fold speedup in exploration time.
    
[^156]: 使用蜂群算法和多父代交叉方法改进的离散粒子群优化（案例研究：分配问题和基准函数）

    Improved discrete particle swarm optimization using Bee Algorithm and multi-parent crossover method (Case study: Allocation problem and benchmark functions)

    [https://arxiv.org/abs/2403.10684](https://arxiv.org/abs/2403.10684)

    本文提出了一种结合了蜂群算法和多父代交叉的离散粒子群优化算法，通过利用观察者蜜蜂进行独立且密集的邻域搜索来改进算法效率。

    

    与其他技术相比，粒子群优化更常被使用，因为它易于使用且变异性低。然而，在大规模优化问题的搜索空间中找到最佳解决方案是复杂的。此外，改变算法变量并不会对算法的收敛产生很大影响。PSO算法可以与其他算法结合使用，利用它们的优势和运算符来解决问题。因此，本文提出了适用于观察者多父代交叉离散粒子群优化（OMPCDPSO）。为了改进DPSO算法的效率，我们在最佳解决方案上使用了多父代交叉。我们利用蜂群算法的观察者蜜蜂进行独立且密集的邻域搜索。该算法利用观察者蜜蜂和交叉。它们进行局部搜索（开发）和全局搜索（探索）。这些搜索中的每一个都是最佳的。

    arXiv:2403.10684v1 Announce Type: cross  Abstract: Compared to other techniques, particle swarm optimization is more frequently utilized because of its ease of use and low variability. However, it is complicated to find the best possible solution in the search space in large-scale optimization problems. Moreover, changing algorithm variables does not influence algorithm convergence much. The PSO algorithm can be combined with other algorithms. It can use their advantages and operators to solve this problem. Therefore, this paper proposes the onlooker multi-parent crossover discrete particle swarm optimization (OMPCDPSO). To improve the efficiency of the DPSO algorithm, we utilized multi-parent crossover on the best solutions. We performed an independent and intensive neighborhood search using the onlooker bees of the bee algorithm. The algorithm uses onlooker bees and crossover. They do local search (exploitation) and global search (exploration). Each of these searches is among the bes
    
[^157]: 通向统一多模式个性化：大型视觉语言模型用于生成推荐和更多领域

    Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond

    [https://arxiv.org/abs/2403.10667](https://arxiv.org/abs/2403.10667)

    本文旨在建立一个统一的多模态个性化系统(UniMP)，有效利用多模态数据同时消除与任务和模态特定定制相关的复杂性。

    

    开发一个能够有效利用异构资源并满足各种个性化需求的通用模型一直是社区渴望的目标。我们日常的选择，尤其是在时尚和零售等领域，很大程度上受多模态数据的影响，比如图片和文本描述。这些模态不仅提供直观的指导，还迎合个性化用户偏好。然而，当前主流的个性化方法主要聚焦于基于ID或文本的推荐问题，未能理解涵盖各种任务或模态的信息。本文的目标是建立一个统一的多模态个性化系统(UniMP)，能够有效利用多模态数据，同时消除与任务和模态特定定制相关的复杂性。我们认为基础生成建模的进展提供了

    arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
    
[^158]: 近似中位数处理效应的极限

    Limits of Approximating the Median Treatment Effect

    [https://arxiv.org/abs/2403.10618](https://arxiv.org/abs/2403.10618)

    估计中位数差异比估计中位数处理效应更容易的问题是因果推断的基本问题

    

    平均处理效应（ATE）的估计是因果推断中一个经过深入研究的问题。然而，它并不一定能捕捉数据中的异质性，因此提出了几种方法来解决这个问题，包括估计分位处理效应。在包含$n$个个体的有限人群设置中，治疗和对照值用潜在结果向量$\mathbf{a}, \mathbf{b}$表示，大部分先前的工作侧重于估计median$(\mathbf{a}) -$ median$(\mathbf{b})$，其中median($\mathbf x$)表示向量$\mathbf x$中所有值排序后的中位数值。众所周知，估计中位数差异比估计中位数$(\mathbf{a-b})$更容易，即所谓的中位数处理效应（MTE）。因果推断的基本问题是对于每个个体$i$，我们只能观测到潜在结果值之一，即

    arXiv:2403.10618v1 Announce Type: cross  Abstract: Average Treatment Effect (ATE) estimation is a well-studied problem in causal inference. However, it does not necessarily capture the heterogeneity in the data, and several approaches have been proposed to tackle the issue, including estimating the Quantile Treatment Effects. In the finite population setting containing $n$ individuals, with treatment and control values denoted by the potential outcome vectors $\mathbf{a}, \mathbf{b}$, much of the prior work focused on estimating median$(\mathbf{a}) -$ median$(\mathbf{b})$, where median($\mathbf x$) denotes the median value in the sorted ordering of all the values in vector $\mathbf x$. It is known that estimating the difference of medians is easier than the desired estimand of median$(\mathbf{a-b})$, called the Median Treatment Effect (MTE). The fundamental problem of causal inference -- for every individual $i$, we can only observe one of the potential outcome values, i.e., either the
    
[^159]: SurvRNC：使用Rank-N-Contrast学习有序表示来进行生存预测

    SurvRNC: Learning Ordered Representations for Survival Prediction using Rank-N-Contrast

    [https://arxiv.org/abs/2403.10603](https://arxiv.org/abs/2403.10603)

    提出了SurvRNC方法，通过引入损失函数作为正则化器来获得基于生存时间的有序表示，能处理被截尾数据，并可整合到任何生存模型中。

    

    预测生存的可能性对于被诊断为癌症的个体来说至关重要，因为它提供了关于早期预后的宝贵信息。这种知识使得制定有效的治疗方案成为可能，从而带来改善患者结局的结果。在过去几年中，深度学习模型为评估医学图像、电子健康记录和基因组数据以估计癌症风险分数提供了可行的解决方案。然而，这些模型通常没有充分发挥其潜力，因为它们很难学习具有回归意识的特征表示。在本研究中，我们提出Survival Rank-N Contrast（SurvRNC）方法，该方法引入了一种损失函数作为正则化器，以根据生存时间获得一个有序表示。该函数可以处理被截尾的数据，并可以被整合到任何生存模型中，以确保所学得的表示是顺序的。

    arXiv:2403.10603v1 Announce Type: cross  Abstract: Predicting the likelihood of survival is of paramount importance for individuals diagnosed with cancer as it provides invaluable information regarding prognosis at an early stage. This knowledge enables the formulation of effective treatment plans that lead to improved patient outcomes. In the past few years, deep learning models have provided a feasible solution for assessing medical images, electronic health records, and genomic data to estimate cancer risk scores. However, these models often fall short of their potential because they struggle to learn regression-aware feature representations. In this study, we propose Survival Rank-N Contrast (SurvRNC) method, which introduces a loss function as a regularizer to obtain an ordered representation based on the survival times. This function can handle censored data and can be incorporated into any survival model to ensure that the learned representation is ordinal. The model was extensi
    
[^160]: 神经侵蚀：在AI系统中模拟受控神经退行和衰老

    Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems

    [https://arxiv.org/abs/2403.10596](https://arxiv.org/abs/2403.10596)

    这项研究介绍了一种在人工智能系统中模拟受控神经退行的新方法，通过在LLMs中添加噪音或切除神经元来逐渐降低其性能，在智商测试中展示了神经退行的过程，并与传统的计算机视觉领域不同，是首个使用文本数据模拟神经退行的工作。

    

    创建控制方法以模拟人工智能（AI）中的神经退行对于模拟大脑功能下降和认知障碍的应用至关重要。我们利用大型语言模型（LLMs）执行的智商测试，特别是LLaMA 2，引入了“神经侵蚀”的概念。这种有意的侵蚀涉及在训练期间或之后切除突触或神经元，或添加高斯噪声，导致LLMs表现的逐渐下降。我们能够描述智商测试中的神经退行，并显示LLM首先失去其数学能力，然后失去其语言能力，同时进一步失去理解问题的能力。据我们所知，这是首个使用文本数据建模神经退行的工作，与在计算机视觉领域进行操作的其他工作相比。最后，我们对我们的研究与认知进行相似性的研究进行了对比。

    arXiv:2403.10596v1 Announce Type: cross  Abstract: Creating controlled methods to simulate neurodegeneration in artificial intelligence (AI) is crucial for applications that emulate brain function decline and cognitive disorders. We use IQ tests performed by Large Language Models (LLMs) and, more specifically, the LLaMA 2 to introduce the concept of ``neural erosion." This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the LLMs' performance. We are able to describe the neurodegeneration in the IQ tests and show that the LLM first loses its mathematical abilities and then its linguistic abilities, while further losing its ability to understand the questions. To the best of our knowledge, this is the first work that models neurodegeneration with text data, compared to other works that operate in the computer vision domain. Finally, we draw similarities between our study and cogn
    
[^161]: S3LLM：使用源代码、元数据和文档的LLMs进行大规模科学软件理解

    S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document

    [https://arxiv.org/abs/2403.10588](https://arxiv.org/abs/2403.10588)

    S3LLM是一个基于LLM的框架，利用生成式人工智能技术，通过用户友好的界面以交互式、对话式的方式，使用户能够检查大规模科学软件的源代码、代码元数据和摘要信息以及文本技术报告，并通过转换自然语言查询为领域特定语言查询来增强代码分析。

    

    由于其多样的代码库、庞大的代码长度和目标计算架构，大规模科学软件的理解面临着重大挑战。生成式人工智能的出现，特别是大型语言模型（LLMs），为理解这种复杂科学代码提供了新的途径。本文提出了S3LLM，一种基于LLM的框架，旨在通过用户友好的界面以交互式、对话式的方式，使用户能够检查源代码、代码元数据和摘要信息以及文本技术报告。S3LLM利用开源的LLaMA-2模型增强了代码分析，通过将自然语言查询自动转换为特定于领域的语言（DSL）查询。具体来说，它将这些查询转换为特征查询语言（FQL），从而实现对整个代码存储库的高效扫描和解析。此外，S3LLM还能够处理d

    arXiv:2403.10588v1 Announce Type: cross  Abstract: The understanding of large-scale scientific software poses significant challenges due to its diverse codebase, extensive code length, and target computing architectures. The emergence of generative AI, specifically large language models (LLMs), provides novel pathways for understanding such complex scientific codes. This paper presents S3LLM, an LLM-based framework designed to enable the examination of source code, code metadata, and summarized information in conjunction with textual technical reports in an interactive, conversational manner through a user-friendly interface. S3LLM leverages open-source LLaMA-2 models to enhance code analysis through the automatic transformation of natural language queries into domain-specific language (DSL) queries. Specifically, it translates these queries into Feature Query Language (FQL), enabling efficient scanning and parsing of entire code repositories. In addition, S3LLM is equipped to handle d
    
[^162]: 从算法到结果：审视人工智能在非肌层侵袭性膀胱癌复发预测中的作用

    From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction

    [https://arxiv.org/abs/2403.10586](https://arxiv.org/abs/2403.10586)

    机器学习技术在非肌层侵袭性膀胱癌复发预测中具有潜在作用，可以提高准确性，降低治疗成本，并有效规划治疗方案

    

    膀胱癌是英国每天造成15人死亡的领先泌尿道癌症。这种癌症主要表现为非肌层侵袭性膀胱癌（NMIBC），其特点是肿瘤还未渗透到膀胱壁的肌肉层。 NMIBC的复发率非常高，达到70-80％，因此治疗成本最高。目前用于预测复发的工具使用评分系统来高估风险，并具有较低的准确性。对复发的不准确和延迟预测显著提高了死亡的可能性。因此，准确预测复发对于成本效益的管理和治疗计划至关重要。这就是机器学习（ML）技术出现的地方，通过利用分子和临床数据预测NMIBC复发，成为一种有前途的方法。本次审查对预测NMIBC复发的ML方法进行了全面分析。我们的系统评估使

    arXiv:2403.10586v1 Announce Type: cross  Abstract: Bladder cancer, the leading urinary tract cancer, is responsible for 15 deaths daily in the UK. This cancer predominantly manifests as non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very high recurrence rate of 70-80% and hence the costliest treatments. Current tools for predicting recurrence use scoring systems that overestimate risk and have poor accuracy. Inaccurate and delayed prediction of recurrence significantly elevates the likelihood of mortality. Accurate prediction of recurrence is hence vital for cost-effective management and treatment planning. This is where Machine learning (ML) techniques have emerged as a promising approach for predicting NMIBC recurrence by leveraging molecular and clinical data. This review provides a comprehensive analysis of ML approaches for predicting NMIBC recurrence. Our systematic evaluation de
    
[^163]: 通过后验抽样解决一般性噪声逆问题：一个策略梯度的视角

    Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint

    [https://arxiv.org/abs/2403.10585](https://arxiv.org/abs/2403.10585)

    提出了一种通过后验抽样解决一般性噪声逆问题的方法，采用策略梯度视角，通过扩散策略梯度（DPG）精确估计指导评分函数，实现对多种线性和非线性逆任务的鲁棒性解决，提高了图像恢复质量。

    

    解决图像逆问题（例如，超分辨率和修复）需要生成一个与给定输入（低分辨率图像或遮挡图像）相匹配的高保真图像。通过使用输入图像作为指导，我们可以利用预训练的扩散生成模型来解决各种图像逆任务，而无需针对特定任务微调模型。为了精确估计输入图像的指导评分函数，我们提出了扩散策略梯度（DPG），这是一种可行的计算方法，通过将中间噪声图像视为策略，将目标图像视为策略选择的状态。实验表明，我们的方法对多个线性和非线性逆任务上的高斯和泊松噪声退化都具有鲁棒性，从而在FFHQ、ImageNet和LSUN数据集上实现更高的图像恢复质量。

    arXiv:2403.10585v1 Announce Type: cross  Abstract: Solving image inverse problems (e.g., super-resolution and inpainting) requires generating a high fidelity image that matches the given input (the low-resolution image or the masked image). By using the input image as guidance, we can leverage a pretrained diffusion generative model to solve a wide range of image inverse tasks without task specific model fine-tuning. To precisely estimate the guidance score function of the input image, we propose Diffusion Policy Gradient (DPG), a tractable computation method by viewing the intermediate noisy images as policies and the target image as the states selected by the policy. Experiments show that our method is robust to both Gaussian and Poisson noise degradation on multiple linear and non-linear inverse tasks, resulting into a higher image restoration quality on FFHQ, ImageNet and LSUN datasets.
    
[^164]: 大型语言模型指导的心力衰竭风险预测的ECG双注意力网络

    Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction

    [https://arxiv.org/abs/2403.10581](https://arxiv.org/abs/2403.10581)

    提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。

    

    心力衰竭（HF）由于全球死亡率不断上升而构成重大公共卫生挑战。通过早期诊断和预防来解决这一问题可显著减少疾病对社会的影响。本文引入了一种使用临床获取的12导联心电图（ECG）进行HF风险预测的方法。我们提出了一种新颖的、轻量级的双注意力ECG网络，旨在捕捉对早期HF预测至关重要的复杂心电图特征，尽管低风险和高风险组之间存在明显的不平衡。该网络具有一个跨导注意力模块和12个导联特定的时间注意力模块，以捕捉交叉导联交互作用和每个导联内的局部时间动态。为了防止模型过拟合于有限的训练数据，我们利用一个大型语言模型（LLM）与公共ECG-Report数据集进行预训练，用于进行ECG-报告对齐任务。然后对网络进行fine-tune以用于HF风险预测

    arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
    
[^165]: 用辅助功能探索语言模型的代码生成能力

    Exploring Language Model's Code Generation Ability with Auxiliary Functions

    [https://arxiv.org/abs/2403.10575](https://arxiv.org/abs/2403.10575)

    在这项研究中，我们全面评估了最近代码预训练语言模型中编码的辅助函数利用能力，通过设计实验，并通过实现风格分析，我们发现了模型利用辅助函数的很有前景。

    

    辅助函数是改善语言模型代码生成能力的有用组件。然而，对它们如何影响的系统性探索尚未完成。本研究全面评估了最近的代码预训练语言模型中编码的辅助函数利用能力。我们首先构建了一个人工设计的评估集，称为HumanExtension，其中包含一个函数帮助另一个函数的示例。通过HumanExtension，我们设计了几个实验以多方面地检验它们的能力。我们的评估过程使得能够全面理解包括辅助函数在提示中的有效性和鲁棒性。另外，通过实现风格分析捕捉到模型在访问辅助函数时的各种实现模式。通过这一分析，我们发现模型利用辅助函数的能力很有前景。

    arXiv:2403.10575v1 Announce Type: cross  Abstract: Auxiliary function is a helpful component to improve language model's code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other. With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models' various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models' promising ability to utilize auxiliary functions includi
    
[^166]: 战略网络战中的生物共生游戏和基础模型

    Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare

    [https://arxiv.org/abs/2403.10570](https://arxiv.org/abs/2403.10570)

    博弈论模型和基础模型在分析、设计和实施网络欺骗策略中发挥关键作用，为提升主动和自动化网络防御机制提供了新思路。

    

    面临着网络战战术的快速演变、情报不对称性增加和黑客工具的日益易得，我们正面临前所未有的网络战。在这种背景下，网络欺骗作为我们防御策略中的关键组成部分崭露头角，旨在应对日益复杂的攻击。本章旨在强调博弈论模型和基础模型（FMs）在分析、设计和实施网络欺骗策略中的关键作用。博弈模型（GMs）作为一个基础框架，用于建模多样的对抗性交互，使我们能够包容对抗性知识和领域特定的见解。同时，基础模型作为创建针对特定应用的定制机器学习模型的构建块。通过利用博弈模型和基础模型之间的协同效应，我们可以通过不仅保护我们的网络免受攻击，而且提高主动和自动化网络防御机制。

    arXiv:2403.10570v1 Announce Type: cross  Abstract: We are currently facing unprecedented cyber warfare with the rapid evolution of tactics, increasing asymmetry of intelligence, and the growing accessibility of hacking tools. In this landscape, cyber deception emerges as a critical component of our defense strategy against increasingly sophisticated attacks. This chapter aims to highlight the pivotal role of game-theoretic models and foundation models (FMs) in analyzing, designing, and implementing cyber deception tactics. Game models (GMs) serve as a foundational framework for modeling diverse adversarial interactions, allowing us to encapsulate both adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the building blocks for creating tailored machine learning models suited to given applications. By leveraging the synergy between GMs and FMs, we can advance proactive and automated cyber defense mechanisms by not only securing our networks against attacks but als
    
[^167]: 在资源受限的边缘环境中利用高效参数缩减实现DNN的帕累托最优性

    Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment

    [https://arxiv.org/abs/2403.10569](https://arxiv.org/abs/2403.10569)

    通过在Xception上实施高效的参数缩减策略，该研究在资源受限的边缘环境中实现了DNN的帕累托最优性，提高了模型的准确性，减少了内存利用，且在Caltech-101图像分类中表现优于原始Xception和轻量级模型。

    

    本文提出了对现有深度神经网络（DNN）进行优化，改善其硬件利用率，并为资源受限的边缘环境提供便于设备上训练的条件。我们在Xception上实施了高效的参数缩减策略，缩小模型大小而不损失准确性，从而减少训练过程中的内存利用。我们在两个实验中评估了我们的模型：Caltech-101图像分类和PCB缺陷检测，并将其性能与原始的Xception和轻量级模型EfficientNetV2B1和MobileNetV2进行了比较。Caltech-101图像分类的结果显示，我们的模型具有更好的测试准确度（76.21%），比Xception（75.89%）平均使用更少的内存（847.9MB比Xception的874.6MB），并且训练和推理时间更快。轻量级模型存在过拟合问题，EfficientNetV2B1的测试准确度为30.52%，MobileNetV2的测试准确度为58.11%。

    arXiv:2403.10569v1 Announce Type: cross  Abstract: This paper proposes an optimization of an existing Deep Neural Network (DNN) that improves its hardware utilization and facilitates on-device training for resource-constrained edge environments. We implement efficient parameter reduction strategies on Xception that shrink the model size without sacrificing accuracy, thus decreasing memory utilization during training. We evaluate our model in two experiments: Caltech-101 image classification and PCB defect detection and compare its performance against the original Xception and lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the Caltech-101 image classification show that our model has a better test accuracy (76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than Xception (874.6MB), and has faster training and inference times. The lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy and MobileNetV2 having a 58.11% test acc
    
[^168]: MoPE：通过Prompt专家混合实现参数高效和可扩展的多模态融合

    MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts

    [https://arxiv.org/abs/2403.10568](https://arxiv.org/abs/2403.10568)

    本文提出了MoPE技术，通过解开提示以自适应捕获数据集级和实例级特征，引入了混合Prompt专家来增强表达能力，并且在多模态融合中表现出更大的表达能力和可扩展性。

    

    Prompt调整已经证明在融合多模态任务的单模基础模型时具有参数效率性。然而，其有限的适应性和表达能力导致性能不佳与其他调整方法相比。本文通过将简单提示解开以自适应地捕获数据集级和实例级特征来解决这个问题。建立在这种解开的基础上，我们引入了Prompt专家的混合（MoPE）技术来增强表达能力。MoPE利用多模态配对先验在每个实例基础上路由最有效的提示。与简单提示相比，我们基于MoPE的条件提示对多模态融合具有更大的表达能力，在训练数据和可训练参数总数上具有更好的扩展性。我们还研究了一个专家路由的正则化项，导致专家的不断发展专长，不同专家专注于不同的特征。

    arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
    
[^169]: 电池单元布局的冷却引导扩散模型

    Cooling-Guide Diffusion Model for Battery Cell Arrangement

    [https://arxiv.org/abs/2403.10566](https://arxiv.org/abs/2403.10566)

    导入冷却引导扩散模型的生成式AI方法优化了电池单元布局，显著降低了单元的最高温度，并在冷却效率方面具有独特优势。

    

    我们的研究引入了一种生成式人工智能方法，利用冷却引导扩散模型来优化电池单元的布局，这是增强电池热管理系统冷却性能和效率的关键步骤。与传统设计流程相比，我们的创新方法使用参数去噪扩散概率模型（DDPM）与分类器和冷却引导相结合，生成具有增强冷却路径的优化单元布局，显著降低了单元的最高温度。通过结合基于位置的分类器引导，我们确保生成的布局的可行性。同时，冷却引导直接优化冷却效率，使我们的方法独具效果。与两种先进模型相比，我们的方法在提高效率上表现出独特优势。

    arXiv:2403.10566v1 Announce Type: cross  Abstract: Our study introduces a Generative AI method that employs a cooling-guided diffusion model to optimize the layout of battery cells, a crucial step for enhancing the cooling performance and efficiency of battery thermal management systems. Traditional design processes, which rely heavily on iterative optimization and extensive guesswork, are notoriously slow and inefficient, often leading to suboptimal solutions. In contrast, our innovative method uses a parametric denoising diffusion probabilistic model (DDPM) with classifier and cooling guidance to generate optimized cell layouts with enhanced cooling paths, significantly lowering the maximum temperature of the cells. By incorporating position-based classifier guidance, we ensure the feasibility of generated layouts. Meanwhile, cooling guidance directly optimizes cooling-efficiency, making our approach uniquely effective. When compared to two advanced models, the Tabular Denoising Diff
    
[^170]: PTSD-MDNN：融合多模态深度神经网络以更快速地检测创伤后应激障碍

    PTSD-MDNN : Fusion tardive de r\'eseaux de neurones profonds multimodaux pour la d\'etection du trouble de stress post-traumatique

    [https://arxiv.org/abs/2403.10565](https://arxiv.org/abs/2403.10565)

    PTSD-MDNN通过融合多模态深度神经网络，提供了一种更客观、更快速的方法来检测创伤后应激障碍。

    

    为了提供一种更客观、更快速的诊断创伤后应激障碍（PTSD）的方法，我们提出了PTSD-MDNN，它合并了两个单模态卷积神经网络，并且具有较低的检测错误率。该模型仅使用视频和音频作为输入，可用于远程会诊、优化患者就诊过程或用于人机交互配置。

    arXiv:2403.10565v1 Announce Type: cross  Abstract: In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.
    
[^171]: Counter-Samples: 一种抵御黑盒对抗攻击的无状态策略

    Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks

    [https://arxiv.org/abs/2403.10562](https://arxiv.org/abs/2403.10562)

    无状态策略通过评估反样本对抗黑盒查询，有效引入了防御者有利的不对称性。这种防御方法能够欺骗攻击者寻找对抗性示例、保持模型在合法输入上的准确性，且适用于多种攻击类型。

    

    我们的论文提出了一种新型的抵御黑盒攻击的方法，攻击者利用受害者模型作为 Oracle 来构建他们的对抗性样本。与传统的预处理防御方法不同，我们的无状态策略实际上对抗攻击过程本身。我们针对每个查询都评估一个反样本，其中反样本是针对攻击者的目标进行优化的原始样本。通过用有针对性的白盒优化来对抗每一个黑盒查询，我们的策略有效地为防御者引入了对称性，使其处于有利地位。这种防御不仅能够有效地误导攻击者寻找对抗性示例，还能保留在合法输入上的模型准确性，并且适用于多种类型的攻击。我们证明，我们的方法对最先进的黑盒攻击非常有效，并且在 C 上胜过现有的防御方法。

    arXiv:2403.10562v1 Announce Type: cross  Abstract: Our paper presents a novel defence against black box attacks, where attackers use the victim model as an oracle to craft their adversarial examples. Unlike traditional preprocessing defences that rely on sanitizing input samples, our stateless strategy counters the attack process itself. For every query we evaluate a counter-sample instead, where the counter-sample is the original sample optimized against the attacker's objective. By countering every black box query with a targeted white box optimization, our strategy effectively introduces an asymmetry to the game to the defender's advantage. This defence not only effectively misleads the attacker's search for an adversarial example, it also preserves the model's accuracy on legitimate inputs and is generic to multiple types of attacks.   We demonstrate that our approach is remarkably effective against state-of-the-art black box attacks and outperforms existing defences for both the C
    
[^172]: AAAI 2024年人本主义表示学习研讨会的被接受论文集合

    A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024

    [https://arxiv.org/abs/2403.10561](https://arxiv.org/abs/2403.10561)

    AAAI 2024年人本主义表示学习研讨会的被接受论文集合。部分论文选择退出。

    

    arXiv:2403.10561v1 公告类型: 跨项  摘要: 这份非存档索引并不完整，因为一些被接受的论文选择了退出。所有被接受论文的列表可在研讨会网站上找到。

    arXiv:2403.10561v1 Announce Type: cross  Abstract: This non-archival index is not complete, as some accepted papers chose to opt-out of inclusion. The list of all accepted papers is available on the workshop website.
    
[^173]: 生成模型与联网自动驾驶车辆：探索交通和人工智能交叉领域的调查

    Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI

    [https://arxiv.org/abs/2403.10559](https://arxiv.org/abs/2403.10559)

    生成模型与联网自动驾驶车辆的整合有望提升自动车辆的预测建模、模拟精度和决策流程，对交通行业的安全和创新具有潜在推动作用。

    

    这份报告调查了生成模型和联网自动驾驶车辆（CAVs）两种推动技术和交通进步的突破性力量的历史和影响。通过关注生成模型在CAVs背景下的应用，该研究旨在揭示这种整合如何提升自动驾驶车辆的预测建模、模拟精度和决策流程。本文讨论了在交通领域整合生成模型和CAV技术的益处和挑战，旨在强调取得的进展、剩余的障碍以及在安全和创新方面的潜力。

    arXiv:2403.10559v1 Announce Type: cross  Abstract: This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
    
[^174]: 二阶信息很重要：重访大型语言模型的机器遗忘问题

    Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2403.10557](https://arxiv.org/abs/2403.10557)

    本论文通过二阶信息（Hessian）的视角重新审视了大型语言模型的机器遗忘问题，提出了遗忘算法，具有较高的鲁棒性。

    

    随着大型语言模型（LLMs）的快速发展，我们目睹了ChatGPT、LLaMa和Gemini等主要LLM产品之间的激烈竞争。然而，训练语料库的各种问题（如隐私泄露和版权侵犯）仍然未被充分探讨。以LLM从业者的视角来看，处理这些意外的隐私侵犯可能具有挑战性。之前的研究通过使用梯度信息解决了LLMs的“遗忘”问题，但它们大多引入了显著的开销，如数据预处理或缺乏鲁棒性。在本文中，与基于一阶信息的方法形成对比，我们通过二阶信息（Hessian）的视角重新审视了遗忘问题。受经典牛顿更新启发，我们的遗忘算法具有

    arXiv:2403.10557v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are 
    
[^175]: KARINA：一种用于全球天气预测的高效深度学习模型

    KARINA: An Efficient Deep Learning Model for Global Weather Forecast

    [https://arxiv.org/abs/2403.10555](https://arxiv.org/abs/2403.10555)

    KARINA模型通过结合ConvNext、SENet和Geocyclic填充，在2.5°分辨率下提升天气预测准确性，只需较少的计算资源，展示出与更高分辨率模型相当的预测精度。

    

    基于深度学习的数据驱动模型在气候研究中越来越普遍，特别是在全球天气预测方面。然而，以高分辨率训练全球天气数据需要大量的计算资源。因此，我们提出了一种名为KARINA的新模型，以克服该领域典型的巨大计算需求。该模型通过只需4个NVIDIA A100 GPU和不到12小时的训练时间，即可实现与更高分辨率对手相媲美的预测准确性，从而大大减少了计算资源的需求。KARINA结合了ConvNext、SENet和Geocyclic填充以增强以2.5°分辨率进行的天气预测，可以过滤掉高频噪声。地理周期填充保留了输入图像的侧边界像素，从而在球形地球中保持大气流的连续性。SENet动态改善特征响应，推进了大气过程建模。

    arXiv:2403.10555v1 Announce Type: cross  Abstract: Deep learning-based, data-driven models are gaining prevalence in climate research, particularly for global weather prediction. However, training the global weather data at high resolution requires massive computational resources. Therefore, we present a new model named KARINA to overcome the substantial computational demands typical of this field. This model achieves forecasting accuracy comparable to higher-resolution counterparts with significantly less computational resources, requiring only 4 NVIDIA A100 GPUs and less than 12 hours of training. KARINA combines ConvNext, SENet, and Geocyclic Padding to enhance weather forecasting at a 2.5{\deg} resolution, which could filter out high-frequency noise. Geocyclic Padding preserves pixels at the lateral boundary of the input image, thereby maintaining atmospheric flow continuity in the spherical Earth. SENet dynamically improves feature response, advancing atmospheric process modeling,
    
[^176]: 通过强化学习学习给LLM生成的文本添加水印

    Learning to Watermark LLM-generated Text via Reinforcement Learning

    [https://arxiv.org/abs/2403.10553](https://arxiv.org/abs/2403.10553)

    通过将信号嵌入LLM的权重中，我们设计一种模型级水印，有效追踪生成文本的滥用情况，并提出基于强化学习的协同训练框架，使水印更准确、更稳健且更适应新攻击。

    

    我们研究如何给LLM生成的文本添加水印，即将可以通过算法检测到的信号嵌入LLM生成的文本中，以追踪滥用情况。与当前主流方法不同，我们通过将LLM调优阶段纳入水印流程，扩展了水印设计空间。先前的研究侧重于在输出中嵌入信号的标记级水印，我们设计了一种模型级水印，将信号嵌入LLM的权重中，可以被配对的检测器检测到。我们提出了一个基于强化学习的协同训练框架，迭代地（1）训练一个检测器来检测生成的带水印文本，并（2）调整LLM以生成检测器轻松检测到但保持正常效用的文本。我们实验证明，我们的水印更准确、更稳健且更适应（应对新攻击）。它还允许水印模型开源。此外，如果与al一起使用

    arXiv:2403.10553v1 Announce Type: cross  Abstract: We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with al
    
[^177]: 通过师生无数据知识传输在未知陌生地点上训练自定位模型

    Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer

    [https://arxiv.org/abs/2403.10552](https://arxiv.org/abs/2403.10552)

    通过师生无数据知识传输，实现在未知陌生地点上训练自定位模型，不仅可以处理各种类型的开放式老师，还有效避免依赖于师生私人数据可用性。

    

    当一个机器人在一个一般的开放世界中运行时，目前自定位模型的一个典型假设是目标工作空间中有一个带标注的训练数据集。然而，在一般开放世界中，这种情况并不总是成立。本研究引入了一种新颖的开放世界分布式机器人系统训练方案。在我们的方案中，一个机器人（"学生"）可以向它在陌生地点遇到的其他机器人（"老师"）寻求指导。具体来说，从老师模型重建一个伪训练数据集，然后用于学生模型的持续学习。与典型的知识传输方案不同，我们的方案在老师模型上只引入了最小的假设，使其可以处理各种类型的开放式老师，包括不合作的、无法训练的（如图像检索引擎）和黑匣子老师（即数据隐私）。与现有方法中依赖于老师的私人数据可用性不同

    arXiv:2403.10552v1 Announce Type: cross  Abstract: A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available in the target workspace. However, this does not always hold when a robot travels in a general open-world. This study introduces a novel training scheme for open-world distributed robot systems. In our scheme, a robot ("student") can ask the other robots it meets at unfamiliar places ("teachers") for guidance. Specifically, a pseudo-training dataset is reconstructed from the teacher model and thereafter used for continual learning of the student model. Unlike typical knowledge transfer schemes, our scheme introduces only minimal assumptions on the teacher model, such that it can handle various types of open-set teachers, including uncooperative, untrainable (e.g., image retrieval engines), and blackbox teachers (i.e., data privacy). Rather than relying on the availability of private data of teachers as in existing methods
    
[^178]: 通过双向归一化流进行半监督学习的异常流量检测

    Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows

    [https://arxiv.org/abs/2403.10550](https://arxiv.org/abs/2403.10550)

    通过生成伪异常样本和使用双向归一化流模块，实现对异常数据的半监督检测。

    

    随着互联网的快速发展，各种类型的异常流量威胁着网络安全。本文考虑了异常网络流量检测的问题，并提出了一个仅使用正常流量的三阶段异常检测框架。我们的框架可以生成伪异常样本，无需先前对异常进行了解，从而实现对异常数据的检测。首先，我们采用重构方法来学习正常样本的深层表示。其次，利用双向流模块将这些表示归一化为标准正态分布。为了模拟异常样本，我们向归一化后的表示添加噪声，然后将其通过双向流模块的生成方向。最后，在潜在空间中训练一个简单的分类器来区分正常样本和伪异常样本。在推断过程中，我们的框架仅需要两个模块来进行检测。

    arXiv:2403.10550v1 Announce Type: cross  Abstract: With the rapid development of the Internet, various types of anomaly traffic are threatening network security. We consider the problem of anomaly network traffic detection and propose a three-stage anomaly detection framework using only normal traffic. Our framework can generate pseudo anomaly samples without prior knowledge of anomalies to achieve the detection of anomaly data. Firstly, we employ a reconstruction method to learn the deep representation of normal samples. Secondly, these representations are normalized to a standard normal distribution using a bidirectional flow module. To simulate anomaly samples, we add noises to the normalized representations which are then passed through the generation direction of the bidirectional flow module. Finally, a simple classifier is trained to differentiate the normal samples and pseudo anomaly samples in the latent space. During inference, our framework requires only two modules to detec
    
[^179]: 坚韧的二阶非凸优化及其在低秩矩阵感知中的应用

    Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing

    [https://arxiv.org/abs/2403.10547](https://arxiv.org/abs/2403.10547)

    研究了在强污染模型中寻找SOSP的问题，提出了一般框架以\emph{独立于维度}的精度保证高效地找到近似SOSP，具有对抗异常值的鲁棒性，同时将该框架应用于低秩矩阵感知问题，发展了能够容忍数据破坏的高效且可证明鲁棒性的算法。

    

    寻找近似的二阶稳定点（SOSP）是随机非凸优化中一个经过深入研究的基本问题，具有许多在机器学习中的应用。然而，在存在异常值的情况下，这个问题理解不足，限制了现有非凸算法在对抗性环境中的使用。本文研究在强污染模型中寻找SOSPs的问题，解决了一定部分的数据点被任意破坏的情况下有效地寻找近似SOSP的一般框架，具有\emph{独立于维度}的精度保证，使用$\widetilde{O}({D^2}/{\epsilon})$个样本，其中$D$是环境维度，$\epsilon$是被破坏数据点的比例。

    arXiv:2403.10547v1 Announce Type: cross  Abstract: Finding an approximate second-order stationary point (SOSP) is a well-studied and fundamental problem in stochastic nonconvex optimization with many applications in machine learning. However, this problem is poorly understood in the presence of outliers, limiting the use of existing nonconvex algorithms in adversarial settings.   In this paper, we study the problem of finding SOSPs in the strong contamination model, where a constant fraction of datapoints are arbitrarily corrupted. We introduce a general framework for efficiently finding an approximate SOSP with \emph{dimension-independent} accuracy guarantees, using $\widetilde{O}({D^2}/{\epsilon})$ samples where $D$ is the ambient dimension and $\epsilon$ is the fraction of corrupted datapoints.   As a concrete application of our framework, we apply it to the problem of low rank matrix sensing, developing efficient and provably robust algorithms that can tolerate corruptions in both 
    
[^180]: 心力衰竭患者治疗路径的流程感知分析：案例研究

    Process-Aware Analysis of Treatment Paths in Heart Failure Patients: A Case Study

    [https://arxiv.org/abs/2403.10544](https://arxiv.org/abs/2403.10544)

    针对心力衰竭患者的治疗路径，利用流程挖掘技术对稀疏数据进行分析，探究是否能够回答多个研究问题。

    

    在医疗领域进行流程挖掘时面临着一系列挑战，因为医疗过程中涉及的数据类型各异。通过从医疗过程中收集的各种数据，我们发现存在高度多样性：由索赔数据提供的运营过程，手术期间发生的事件集合，与术前和术后护理相关的数据，以及基于定期门诊访问的高级数据收集，其中并没有明显的事件。本案例研究分析了来自最后一类的数据集。我们将稀疏的心力衰竭患者数据应用流程挖掘技术，并探讨是否可以实现对多个研究问题的信息增益。在这里，可用数据被转换成事件日志格式，并应用流程发现和符合性检查。此外，根据合并症，如糖尿病和慢性肾病，患者被分成不同的队列，并对多个实验进行分析。

    arXiv:2403.10544v1 Announce Type: cross  Abstract: Process mining in healthcare presents a range of challenges when working with different types of data within the healthcare domain. There is high diversity considering the variety of data collected from healthcare processes: operational processes given by claims data, a collection of events during surgery, data related to pre-operative and post-operative care, and high-level data collections based on regular ambulant visits with no apparent events. In this case study, a data set from the last category is analyzed. We apply process-mining techniques on sparse patient heart failure data and investigate whether an information gain towards several research questions is achievable. Here, available data are transformed into an event log format, and process discovery and conformance checking are applied. Additionally, patients are split into different cohorts based on comorbidities, such as diabetes and chronic kidney disease, and multiple st
    
[^181]: MATADOR：用于边缘应用的自动化片上Tsetlin机设计生成系统

    MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications

    [https://arxiv.org/abs/2403.10538](https://arxiv.org/abs/2403.10538)

    该论文介绍了MATADOR，一个用于边缘应用的自动化片上Tsetlin机设计生成系统，实现了将ML模型转换为SoC-FPGA解决方案的高效方法。

    

    System-on-Chip Field-Programmable Gate Array（SoC-FPGA）通过设计协处理器加速器系统为机器学习（ML）边缘推断应用提供了显著的吞吐量增益。然而，将ML模型训练并翻译为SoC-FPGA解决方案的设计工作可能是相当大的，需要专业知识来平衡模型性能、功耗、延迟和资源利用率之间的 trade-offs。与其他ML算法相反，Tsetlin机器（TM）通过从Tsetlin自动机（学习元素）和布尔输入特征之间形成逻辑命题来执行分类。经过训练的TM模型通常具有很高的稀疏性，并且这些逻辑命题在类别内部和类别之间都有相当大的重叠。因此，该模型可以使用极少量的AND和NOT门转换为RTL级的设计。本文介绍了MATADOR，一个自动化布尔到si

    arXiv:2403.10538v1 Announce Type: cross  Abstract: System-on-Chip Field-Programmable Gate Arrays (SoC-FPGAs) offer significant throughput gains for machine learning (ML) edge inference applications via the design of co-processor accelerator systems. However, the design effort for training and translating ML models into SoC-FPGA solutions can be substantial and requires specialist knowledge aware trade-offs between model performance, power consumption, latency and resource utilization. Contrary to other ML algorithms, Tsetlin Machine (TM) performs classification by forming logic proposition between boolean actions from the Tsetlin Automata (the learning elements) and boolean input features. A trained TM model, usually, exhibits high sparsity and considerable overlapping of these logic propositions both within and among the classes. The model, thus, can be translated to RTL-level design using a miniscule number of AND and NOT gates. This paper presents MATADOR, an automated boolean-to-si
    
[^182]: VISREAS: 复杂视觉推理与无法回答的问题

    VISREAS: Complex Visual Reasoning with Unanswerable Questions

    [https://arxiv.org/abs/2403.10534](https://arxiv.org/abs/2403.10534)

    提出了新的组合视觉问答数据集VISREAS，通过验证问题在回答之前的有效性，解决了用户提供不完美指示的挑战

    

    在现实世界的应用中，验证问题的有效性在回答之前至关重要，因为用户可能提供不完美的指示。为了解决这一需求，我们引入了一个新的组合视觉问答数据集VISREAS，其中包含了通过遍历和扰动对象、属性和关系的共同点和差异而构成的可回答和无法回答的视觉查询。

    arXiv:2403.10534v1 Announce Type: cross  Abstract: Verifying a question's validity before answering is crucial in real-world applications, where users may provide imperfect instructions. In this scenario, an ideal model should address the discrepancies in the query and convey them to the users rather than generating the best possible answer. Addressing this requirement, we introduce a new compositional visual question-answering dataset, VISREAS, that consists of answerable and unanswerable visual queries formulated by traversing and perturbing commonalities and differences among objects, attributes, and relations. VISREAS contains 2.07M semantically diverse queries generated automatically using Visual Genome scene graphs. The unique feature of this task, validating question answerability with respect to an image before answering, and the poor performance of state-of-the-art models inspired the design of a new modular baseline, LOGIC2VISION that reasons by producing and executing pseudo
    
[^183]: PET-SQL：一个带有交叉一致性的增强提示的两阶段文本到SQL框架

    PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency

    [https://arxiv.org/abs/2403.09732](https://arxiv.org/abs/2403.09732)

    提出了一个两阶段框架，通过引入参考增强表示和少样本演示，解决了在处理冗长的数据库信息和复杂用户意图时的挑战。

    

    最近文本到SQL（Text2SQL）领域的进展强调刺激大型语言模型（LLM）进行上下文学习，取得了显著成果。然而，他们在处理冗长的数据库信息和复杂的用户意图时面临挑战。本文提出了一个两阶段框架，以增强当前基于LLM的自然语言到SQL系统的性能。我们首先引入了一种新颖的提示表示，称为参考增强表示，其中包括模式信息和从表格随机抽样的单元格值，以指导LLM生成SQL查询。然后，在第一阶段，我们检索问题-SQL对作为少量演示，促使LLM生成初步SQL（PreSQL）。之后，解析PreSQL中提到的实体进行模式链接，可以显著压缩有用信息。在第二阶段，利用链接的模式，我们简化了

    arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
    
[^184]: Quiet-STaR: 语言模型可以自己学会思考后再说话

    Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking

    [https://arxiv.org/abs/2403.09629](https://arxiv.org/abs/2403.09629)

    Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力

    

    写作和交谈时，人们有时会停下来思考。尽管以推理为重点的作品通常将推理框定为回答问题或完成代理任务的方法，但推理几乎都隐含在所有书面文本中。例如，这适用于证明中未明确说明的步骤，以及支撑对话的心智理论。在自学习推理者（STaR，Zelikman等，2022）中，通过从少量示例中推断来自问答中有用的思考，并学习那些导致正确答案的思考。这是一个高度受限制的环境--理想情况下, 一个语言模型可以学会从任意文本中推断未明确说明的思考。我们提出Quiet-STaR，这是STaR的一个泛化版本，其中语言模型学会在每个标记处生成解释未来文本的思考过程，从而改善其预测。我们解决了一些关键挑战，包括1）生成连续的计算成本

    arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
    
[^185]: 控制硬件非确定性进行乐观可验证训练

    Optimistic Verifiable Training by Controlling Hardware Nondeterminism

    [https://arxiv.org/abs/2403.09603](https://arxiv.org/abs/2403.09603)

    提出了一种方法，结合了在比目标模型更高精度下进行训练、在中间计算步骤后进行四舍五入，并基于自适应阈值存储四舍五入决策，以应对硬件非确定性对训练过程的影响。

    

    AI系统日益增加的计算需求导致了为缺乏必要资源的客户进行模型训练的服务的出现。然而，确保训练的正确性并防范潜在的训练时攻击，例如数据毒化，都带来了挑战。现有的关于可验证训练的工作主要分为两类：基于证明的系统，由于需要加密技术而难以扩展，以及考虑到一个可信第三方审计员复制训练过程的“乐观”方法。 后者的一个关键挑战是，在训练期间GPU类型之间的硬件非确定性阻止审计员精确复制训练过程，因此这样的方案不够健壮。我们提出了一种方法，将训练在比目标模型更高的精度下进行，中间计算步骤后四舍五入，基于自适应阈值存储四舍五入决策。

    arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
    
[^186]: AI中的启发式推理: 工具性使用与模仿吸收

    Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption

    [https://arxiv.org/abs/2403.09404](https://arxiv.org/abs/2403.09404)

    通过创新性实验，我们揭示了人工智能系统在精度最大化和努力减少之间的权衡，以及在详尽的逻辑处理和使用认知快捷方式（启发式）之间转换的条件，并区分了启发式的工具性使用和模仿吸收两种方式。

    

    我们提出了一种新颖的启发式推理方案，用于人工智能系统。通过一系列创新性实验，包括对经典琳达问题的变体和对美丽比赛游戏的新应用，我们揭示了精度最大化和努力减少之间的权衡，塑造了人工智能在何种条件下在详尽的逻辑处理和使用认知快捷方式（启发式）之间转换。我们区分了启发式的'工具性'使用，以匹配资源与目标，以及'模仿吸收'，即从人类那里学到的启发式，并表现为随机且普遍存在。我们提供证据表明，尽管缺乏内在目标或自我意识，人工智能表现出精度和效率的自适应平衡，符合资源理性人类认知的原则，这是受限理性和双系统理论经典理论的明文阐释。

    arXiv:2403.09404v1 Announce Type: new  Abstract: We propose a novel program of heuristic reasoning within artificial intelligence (AI) systems. Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics). We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally. We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory.
    
[^187]: 学习自适应邻居以实时检测内部威胁

    LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection

    [https://arxiv.org/abs/2403.09209](https://arxiv.org/abs/2403.09209)

    该论文的贡献是提出了一个名为LAN的框架，能够实时在活动级别进行内部威胁检测，并学习活动序列内的时间依赖关系和活动之间的关系。

    

    企业和组织面临来自内部员工可能导致严重后果的潜在威胁。先前关于内部威胁检测（ITD）的研究主要集中在检测异常用户或异常时间段（例如，一周或一天）。然而，用户可能在日志中有数十万条活动，即使在一天内，一个用户也可能存在数千条活动，这需要高昂的调查预算来验证异常用户或活动。另一方面，现有作品主要是事后方法而不是实时检测，无法及时报告内部威胁在引起损失之前。在本文中，我们进行了针对活动级别实时ITD的第一项研究，并提出了一个细粒度和高效的框架LAN。具体而言，LAN同时学习活动序列内的时间依赖关系和活动之间的关系。

    arXiv:2403.09209v1 Announce Type: cross  Abstract: Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between ac
    
[^188]: AutoLoRA：基于元学习的自动调整矩阵秩在低秩适应中的应用

    AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning

    [https://arxiv.org/abs/2403.09113](https://arxiv.org/abs/2403.09113)

    AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。

    

    大规模预训练之后进行任务特定微调在各种自然语言处理任务中取得了巨大成功。然而，对于大型预训练模型的所有参数进行微调存在着巨大的计算和内存挑战，因此研发了几种高效的微调方法。其中，低秩适应（LoRA）通过在冻结的预训练权重之上微调低秩增量更新矩阵，被证明特别有效。然而，LoRA在所有层中均匀分配秩，并且依赖于穷举搜索来找到最佳秩，导致了高计算成本和微调性能不佳。为了解决这些限制，我们引入了AutoLoRA，这是一个基于元学习的框架，用于自动识别每个LoRA层的最佳秩。AutoLoRA将低秩更新矩阵中的每个秩为1的矩阵与选择变量相关联，该变量决定了秩为1的矩阵是否应该被...

    arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
    
[^189]: 大型语言模型生成的代码中的缺陷

    Bugs in Large Language Models Generated Code

    [https://arxiv.org/abs/2403.08937](https://arxiv.org/abs/2403.08937)

    本文研究了使用三种主要LLM生成的代码中收集的333个缺陷样本，并识别了10种独特的错误模式。

    

    最近，用于代码的大型语言模型（LLMs）受到了重视。它们可以基于提供的提示在不同的编程语言中生成代码，实现了软件工程（SE）中长期以来的一个梦想，即自动生成代码。与人类编写的代码类似，LLM生成的代码容易出现错误，但社区尚未对这些错误进行深入研究。鉴于软件工程活动中LLM-based代码生成工具（例如GitHub Copilot）的日益普及，了解LLMs生成代码中包含的缺陷特征至关重要。本文研究了使用三种主要LLM（即CodeGen、PanGu-Coder和Codex）生成的代码中收集的333个缺陷样本，并识别了以下10种独特的错误模式：误解、语法错误、愚蠢错误、提示偏向代码、遗漏角落案例、错误的输入类型、产生幻象对象、错误的属性、不完整（未完结）

    arXiv:2403.08937v1 Announce Type: cross  Abstract: Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomple
    
[^190]: 快速推断基于移除的节点影响

    Fast Inference of Removal-Based Node Influence

    [https://arxiv.org/abs/2403.08333](https://arxiv.org/abs/2403.08333)

    提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。

    

    图神经网络（GNNs）被广泛用于捕获图中信息传播模式。虽然取得了显著的性能，但评估节点影响的新趋势日益受到关注。我们提出了一种评估节点影响的新方法，通过衡量训练好的GNN模型在移除节点后的预测变化。一个真实应用是，“在预测Twitter账户极性的任务中，如果移除特定账户，其他账户的极性会如何改变？”我们将GNN作为一个代理模型，其预测可以模拟移除节点引起的节点或边的变化。为了获得每个节点的影响，一种直接的方法是交替移除每个节点，并在修改后的图上应用训练好的GNN。这是可靠的但耗时，因此我们需要一种高效的方法。

    arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
    
[^191]: 生成预训练结构化Transformer：规模化的无监督句法语言模型

    Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale

    [https://arxiv.org/abs/2403.08293](https://arxiv.org/abs/2403.08293)

    GPST是一种无监督的句法语言模型，通过联合训练两个模型实现对原始文本的高并行预训练，克服了之前SLM依赖于黄金树和顺序训练的限制，展示了在多个任务中优于同等规模的GPT-2。

    

    句法语言模型（SLM）以从左到右的方式逐步生成带有其句法树的句子。我们提出了生成预训练结构化Transformer（GPST），这是一种规模化的无监督SLM，能够在原始文本上从头开始进行高并行预训练。GPST规避了之前SLM的一些限制，比如依赖于黄金树和顺序训练。它由两个组件组成，一个通常的SLM受单向语言建模损失的监督，以及一个额外的组合模型，用于引导句法解析树并计算成分表示，受双向语言建模损失的监督。我们提出了一个表示替代方案，以实现两个模型的联合并行训练，采用硬EM的方式。我们在OpenWebText上对GPST进行了预训练，该语料库包括90亿个token，并展示了GPST在许多任务上的优越性，涵盖了与GPT-2相当规模的内容。

    arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
    
[^192]: 通过融合高度专业化语言模型同时掌握文本、代码和数学

    Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models

    [https://arxiv.org/abs/2403.08281](https://arxiv.org/abs/2403.08281)

    通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。

    

    自然语言、编程代码和数学符号的底层数据分布变化巨大，对于那些努力同时在三个领域实现高性能的大型语言模型（LLMs）提出了复杂挑战。本文提出了一种直接融合已经高度专业化模型的方法。所提出的融合框架UltraFuser包括三个已经在语言、编码和数学上得到充分训练的专家。引入了一个标记级别的门控机制来混合专家的输出。设计了一个伴随平衡采样的两阶段训练策略以确保稳定性。为了有效训练融合模型，我们进一步构建了一个

    arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
    
[^193]: 深度子模逆点网络

    Deep Submodular Peripteral Network

    [https://arxiv.org/abs/2403.08199](https://arxiv.org/abs/2403.08199)

    引入了深度子模逆点网络（DSPNs），并提出了一种使用对比学习启发的GPC-ready策略进行训练的方法，以应对子模函数学习中的两大挑战。

    

    子模函数对各种应用至关重要，但通常缺乏实用的学习方法来获取它们。本文引入了深度子模逆点网络（DSPNs），一种新颖的子模函数参数化族，并提出了使用对比学习启发的GPC-ready策略对其进行训练的方法，以连接并解决上述两个挑战。

    arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripteral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate
    
[^194]: 基于矩阵变换的低秩调整（MTLoRA）：一种受大脑启发的参数高效微调方法

    Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2403.07440](https://arxiv.org/abs/2403.07440)

    该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。

    

    基于大型预训练语言模型（LPLMs）的微调技术已被证明可以显著提高模型在各种下游任务上的性能，并有效控制LPLMs的输出行为。本文受大脑功能受其几何结构塑造的启发，将这一思想融入LoRA技术中，提出了一种新的基于矩阵变换的重新参数化方法，以减少复杂任务适应性、性能、稳定性和算法复杂性方面的改进空间。

    arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
    
[^195]: 一个新的直觉模糊决策树随机森林集成

    A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees

    [https://arxiv.org/abs/2403.07363](https://arxiv.org/abs/2403.07363)

    提出了一种新的直觉模糊决策树随机森林集成方法，融合了随机性、模糊逻辑和模糊集的灵活性，以及多分类器系统的稳健性

    

    分类对于数据挖掘、人工智能和故障检测领域的应用至关重要。在开发准确、适用且高效的分类方法和算法具有广泛适用性方面存在强烈需求。随机森林是一种常用于复杂条件下分类的通用算法。尽管它被广泛采用，但其与不同模糊理论的结合仍有探索的价值。本文提出直觉模糊随机森林（IFRF），它是一种由直觉模糊决策树（IFDT）组成的新的随机森林集成。森林中的这些树使用直觉模糊信息增益来选择特征，并考虑信息传输中的犹豫。所提出的方法融合了来自自助抽样和特征选择的随机性、模糊逻辑和模糊集的灵活性，以及多分类器系统的稳健性。

    arXiv:2403.07363v1 Announce Type: new  Abstract: Classification is essential to the applications in the field of data mining, artificial intelligence, and fault detection. There exists a strong need in developing accurate, suitable, and efficient classification methods and algorithms with broad applicability. Random forest is a general algorithm that is often used for classification under complex conditions. Although it has been widely adopted, its combination with diverse fuzzy theory is still worth exploring. In this paper, we propose the intuitionistic fuzzy random forest (IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees (IFDT). Such trees in forest use intuitionistic fuzzy information gain to select features and consider hesitation in information transmission. The proposed method enjoys the power of the randomness from bootstrapped sampling and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the robustness of multiple classifier syste
    
[^196]: 揭示受幼儿启发的奖励转换在目标导向强化学习中的重要性

    Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning

    [https://arxiv.org/abs/2403.06880](https://arxiv.org/abs/2403.06880)

    研究探讨了幼儿启发的奖励转换如何影响强化学习任务的样本效率和成功率，特别是发现了幼儿启发的稀疏转密集（S2D）转换的有效性。

    

    幼儿从稀疏反馈的自由探索逐渐发展为利用先前经验进行以目标为导向的学习，获得更密集奖励。受此幼儿启发的奖励转换的影响，我们探讨了将不同奖励转换纳入强化学习（RL）任务的意义。我们研究的重点是从稀疏到基于潜在的密集奖励的转换，这两者共享无论奖励变化均为最佳策略。通过包括以自我为中心的导航和机械臂操作任务在内的各种实验，我们发现适当的奖励转换显著影响样本效率和成功率。特别值得注意的是受幼儿启发的稀疏转密集（S2D）转换的有效性。除了这些性能指标外，使用交叉密度可视化技术，我们观察到转换，特别是S2D，使策略损失景观更加平滑，促进

    arXiv:2403.06880v1 Announce Type: cross  Abstract: Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting
    
[^197]: ALaRM: 通过分层奖励建模对齐语言模型

    ALaRM: Align Language Models via Hierarchical Rewards Modeling

    [https://arxiv.org/abs/2403.06754](https://arxiv.org/abs/2403.06754)

    ALaRM是第一个从人类反馈中建模分层奖励的框架，通过整合整体奖励与特定方面的奖励，改善了大型语言模型与人类偏好的对齐性，尤其在复杂文本生成任务中表现出更精确和一致的指导。

    

    我们介绍了ALaRM，第一个在强化学习中从人类反馈模型分层奖励的框架，旨在增强大型语言模型（LLMs）与人类偏好的对齐性。该框架解决了当前对齐方法的限制，这些方法通常难以处理人类监督信号的不一致性和稀疏性，通过将整体奖励与特定方面的奖励相结合。这种整合使得语言模型更加精确和一致地指导朝着期望的结果前进，尤其在复杂和开放的文本生成任务中。通过应用基于一致性的方法来过滤和组合多个奖励，该框架提供了一种可靠的机制来改善模型的对齐性。我们通过在长篇问题回答和机器翻译任务中使用gpt-3.5-turbo进行成对比较来验证我们的方法。

    arXiv:2403.06754v1 Announce Type: cross  Abstract: We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demon
    
[^198]: RepoHyper：更好的上下文检索是仓库级代码补全所需的一切

    RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion

    [https://arxiv.org/abs/2403.06095](https://arxiv.org/abs/2403.06095)

    RepoHyper提出了Repo级语义图（RSG）和Expand和Refine检索方法，显著优于现有技术，满足仓库级代码补全的需求

    

    arXiv:2403.06095v1 公告类型：交叉摘要：代码大型语言模型（CodeLLMs）在代码补全任务中展示出令人印象深刻的熟练程度。然而，它们经常无法完全理解项目仓库的广泛上下文，比如相关文件和类层次结构的复杂性，这可能导致补全不够精确。为了克服这些限制，我们提出了RepoHyper，一个旨在解决与仓库级代码补全相关的复杂挑战的多方面框架。RepoHyper的核心是Repo级语义图（RSG），一种封装代码仓库广泛上下文的新颖语义图结构。此外，RepoHyper利用扩展和细化检索方法，包括应用于RSG的图扩展和链接预测算法，从而实现对相关代码片段的有效检索和优先排序。我们的评估表明，RepoHyper在重新

    arXiv:2403.06095v1 Announce Type: cross  Abstract: Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in re
    
[^199]: 具有多GPU启用的混合量子-经典工作流的Quantum-HPC框架：在量子模拟中的应用

    Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations

    [https://arxiv.org/abs/2403.05828](https://arxiv.org/abs/2403.05828)

    混合量子-经典工作流架构QCQ实现了量子模拟中的创新，通过在QPUs上运行VQE算法和在经典硬件上进行量子态分类，结合了cuQuantum SDK和PennyLane's Lightning plugin，为材料和凝聚态物理领域的计算挑战提供了解决方案。

    

    在量子系统上实现高性能计算面临着巨大挑战，需要弥合量子硬件和经典计算资源之间的能力差距。本研究引入了一种创新的分布感知的Quantum-Classical-Quantum (QCQ)架构，将前沿的量子软件框架与高性能经典计算资源整合在一起，以解决材料和凝聚态物理领域的量子模拟挑战。该架构的核心是在QPUs上运行VQE算法实现高效量子态准备，在经典硬件上进行量子态分类的Tensor Network states和QCNNs的无缝集成。 为了对量子模拟器进行基准测试，QCQ架构利用cuQuantum SDK利用多GPU加速，集成了PennyLane的Lightning插件，展示了计算速度增加多达十倍的能力。

    arXiv:2403.05828v1 Announce Type: cross  Abstract: Achieving high-performance computation on quantum systems presents a formidable challenge that necessitates bridging the capabilities between quantum hardware and classical computing resources. This study introduces an innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture, which integrates cutting-edge quantum software framework works with high-performance classical computing resources to address challenges in quantum simulation for materials and condensed matter physics. At the heart of this architecture is the seamless integration of VQE algorithms running on QPUs for efficient quantum state preparation, Tensor Network states, and QCNNs for classifying quantum states on classical hardware.   For benchmarking quantum simulators, the QCQ architecture utilizes the cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's Lightning plugin, demonstrating up to tenfold increases in computational spe
    
[^200]: MG-TSD：具有引导学习过程的多粒度时间序列扩散模型

    MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process

    [https://arxiv.org/abs/2403.05751](https://arxiv.org/abs/2403.05751)

    提出了一种新颖的MG-TSD模型，利用数据内在粒度水平作为目标来引导学习过程，实现了状态-of-the-art的预测性能

    

    最近，扩散概率模型由于其生成高保真样本的显著能力而在生成式时间序列预测中引起关注。然而，由于其随机特性带来的不稳定性挑战，如何有效利用它们在概率时间序列预测任务中的强大建模能力仍然是一个悬而未决的问题。为了解决这一挑战，我们引入了一种新颖的多粒度时间序列扩散（MG-TSD）模型，通过利用数据内在的粒度水平作为中间扩散步骤的给定目标来指导扩散模型的学习过程，实现了最先进的预测性能。

    arXiv:2403.05751v1 Announce Type: cross  Abstract: Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the p
    
[^201]: DyRoNet：一种低秩适配器增强的动态路由网络，用于流媒体感知

    DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception

    [https://arxiv.org/abs/2403.05050](https://arxiv.org/abs/2403.05050)

    DyRoNet采用低秩动态路由并结合分支网络优化流媒体感知性能，为多种分支选择策略设定了新的性能标杆

    

    自主驾驶系统需要实时、准确的感知来应对复杂环境。为解决这一问题，我们引入了动态路由网络（DyRoNet），这是一个创新性的框架，采用低秩动态路由以增强流媒体感知。通过集成专门预训练的分支网络，针对各种环境条件进行微调，DyRoNet在延迟和精度之间取得了平衡。其核心特征是速度路由模块，智能地将输入数据引导到最适合的分支网络，优化性能。广泛的评估结果显示，DyRoNet有效地适应多种分支选择策略，为各种场景性能设定了新的标杆。DyRoNet不仅为流媒体感知建立了新的标杆，还为未来的工作提供了宝贵的工程洞见。有关更多项目信息，请访问 https://tastevision.github.io/DyRoNet/

    arXiv:2403.05050v1 Announce Type: cross  Abstract: Autonomous driving systems demand real-time, accurate perception to navigate complex environments. Addressing this, we introduce the Dynamic Router Network (DyRoNet), a framework that innovates with low-rank dynamic routing for enhanced streaming perception. By integrating specialized pre-trained branch networks, fine-tuned for various environmental conditions, DyRoNet achieves a balance between latency and precision. Its core feature, the speed router module, intelligently directs input data to the best-suited branch network, optimizing performance. The extensive evaluations reveal that DyRoNet adapts effectively to multiple branch selection strategies, setting a new benchmark in performance across a range of scenarios. DyRoNet not only establishes a new benchmark for streaming perception but also provides valuable engineering insights for future work. More project information is available at https://tastevision.github.io/DyRoNet/
    
[^202]: 使用大型语言模型的组合分数测量人脑中的含义合成

    Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models

    [https://arxiv.org/abs/2403.04325](https://arxiv.org/abs/2403.04325)

    引入了Composition Score，一种基于模型的度量标准，用于量化句子理解中的含义合成程度，实验证明这一度量与大脑区域相关，揭示了含义合成在人类句子理解中的多方面性。

    

    含义合成的过程是指更小的单位如语素或单词组合形成短语和句子的含义，对于人类句子理解至关重要。尽管神经语言学对涉及含义合成的大脑区域进行了大量研究，但仍缺乏一种计算度量来量化合成的程度。借鉴变压器前馈网络块的键值内存解释，我们引入了组合分数，这是一种新颖的基于模型的度量标准，旨在量化句子理解过程中的含义合成程度。实验结果表明，这一度量与大脑簇相关联，这些大脑簇与词频率、结构处理和对单词的一般敏感性有关，这表明了人类句子理解过程中含义合成的多方面性。

    arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
    
[^203]: 最近大型视觉-语言模型的有效性评估

    Effectiveness Assessment of Recent Large Vision-Language Models

    [https://arxiv.org/abs/2403.04306](https://arxiv.org/abs/2403.04306)

    本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。

    

    大型视觉-语言模型(LVLMs)的出现代表着迈向人工通用智能的重要进步。然而，它们在专业和通用任务中的有效性程度需要进一步调查。本文旨在评估流行的LVLMs在专业和通用任务中的能力，旨在提供对这些创新方法的全面理解。为了评估它们在专业任务中的有效性，我们量身定制了一个包含自然、医疗和工业三种不同场景的全面测试平台，涵盖六项具有挑战性的任务。这些任务包括显著、伪装和透明物体检测，以及息肉和皮肤病变检测，以及工业异常检测。我们检验了最近三种开源LVLMs--MiniGPT-v2、LLaVA-1.5和Shikra--在视觉识别和定位领域的表现。

    arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
    
[^204]: ProMISe: 使用SAM进行可提示的医学图像分割

    ProMISe: Promptable Medical Image Segmentation using SAM

    [https://arxiv.org/abs/2403.04164](https://arxiv.org/abs/2403.04164)

    本文提出了一个自动提示模块（APM），为SAM基础模型提供了在目标领域中使用的自适应提示，显著提高了SAM在医学图像分割中的性能。

    

    随着提出了Segment Anything Model (SAM)的建议，对SAM进行医学图像分割(MIS)的微调变得流行起来。然而，由于SAM模型的规模较大，自然图像和医学图像之间存在显著的领域差距，基于微调的策略成本高，存在不稳定性、特征损伤和灾难性遗忘的潜在风险。此外，一些通过微调策略将SAM转移到特定领域MIS的方法禁用了模型的提示能力，严重限制了其使用场景。在本文中，我们提出了一个自动提示模块（APM），为SAM基础模型在目标域中提供了具有欧几里德自适应提示的基础。我们的实验证明，这样的自适应提示显著提高了SAM在MIS中非微调的性能。此外，我们提出了一种名为增量模式移位（IPS）的新型非侵入式方法，用于将SAM调整到特定医疗领域。

    arXiv:2403.04164v1 Announce Type: cross  Abstract: With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medica
    
[^205]: Cobweb：一种增量和分层式的人类类别学习模型

    Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning

    [https://arxiv.org/abs/2403.03835](https://arxiv.org/abs/2403.03835)

    Cobweb是一种类似人类类别学习系统，采用类别效用度量构建分层组织的类似树状结构，能够捕捉心理效应并在单一模型中展现出实例和原型学习的灵活性，为将来研究人类类别学习提供了基础。

    

    Cobweb是一种类似人类的类别学习系统，与其他增量分类模型不同的是，它利用类别效用度量构建分层组织的类似树状结构。先前的研究表明，Cobweb能够捕捉心理效应，如基本水平、典型性和扇形效应。然而，对Cobweb作为人类分类模型的更广泛评估仍然缺乏。本研究填补了这一空白。它确定了Cobweb与经典的人类类别学习效应的一致性。还探讨了Cobweb展现出在单一模型中既有实例又有原型学习的灵活性。这些发现为将来研究Cobweb作为人类类别学习的综合模型奠定了基础。

    arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
    
[^206]: DeepCRE：利用尖端计算模型改革药物研发

    DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models

    [https://arxiv.org/abs/2403.03768](https://arxiv.org/abs/2403.03768)

    DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。

    

    arXiv:2403.03768v1 公告类型：新摘要：药物开发领域和治疗应用领域都面临着重大挑战。治疗领域需要更多的治疗选择，同时大量有前景的临床前药物在临床试验中失败。一个原因是在药物开发的后期阶段交叉药物反应评估（CRE）的不足。尽管计算机模拟的CRE模型为解决这一问题提供了一种解决方案，但现有方法学要么局限于早期开发阶段，要么缺乏对全面CRE分析的能力。在这里，我们介绍了一种名为DeepCRE的新型计算模型，并展示了DeepCRE在推动治疗发现和发展方面的潜力。DeepCRE通过实现患者级别CRE平均性能提高17.7\%，指示级别CRE增加了5倍，优于现有最佳模型。此外，DeepCRE已经确定了六个显示出明显更大优势的药物候选者。

    arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
    
[^207]: 分析和基于全 memristor 的时间数据分类的储层计算

    Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification

    [https://arxiv.org/abs/2403.01827](https://arxiv.org/abs/2403.01827)

    本研究引入了一种新颖的双存储器 RC 系统，通过集成不同类型的 memristor，并在处理时间数据分类任务中取得了显著成效。

    

    arXiv:2403.01827v1 公告类型: 交叉摘要: 储层计算（RC）提供了一个特别适用于处理时空信号的神经形态学框架。RC以其时间处理能力而闻名，与传统的递归神经网络相比，显著降低了训练成本。其硬件部署中的一个关键组成部分是能够生成动态储层状态的能力。我们的研究引入了一种新颖的双重存储器 RC 系统，集成了一种基于 WOx 的 memristor 的短期存储器，能够实现 16 个不同状态的编码超过 4 个比特，并在读出层中使用 TiOx-based memristor 的长期存储器组件。我们彻底研究了两种 memristor 类型，并利用 RC 系统处理时间数据集。所提出的 RC 系统的性能通过两个基准任务进行了验证: 对具有不完整输入的孤立口述数字识别和 Mackey-Glass 时间序列预测。该系统提供了令人印象深刻的 98.84% 准确率.

    arXiv:2403.01827v1 Announce Type: cross  Abstract: Reservoir computing (RC) offers a neuromorphic framework that is particularly effective for processing spatiotemporal signals. Known for its temporal processing prowess, RC significantly lowers training costs compared to conventional recurrent neural networks. A key component in its hardware deployment is the ability to generate dynamic reservoir states. Our research introduces a novel dual-memory RC system, integrating a short-term memory via a WOx-based memristor, capable of achieving 16 distinct states encoded over 4 bits, and a long-term memory component using a TiOx-based memristor within the readout layer. We thoroughly examine both memristor types and leverage the RC system to process temporal data sets. The performance of the proposed RC system is validated through two benchmark tasks: isolated spoken digit recognition with incomplete inputs and Mackey-Glass time series prediction. The system delivered an impressive 98.84% accu
    
[^208]: 在协作学习环境中快速低参数视频活动定位

    Fast Low-parameter Video Activity Localization in Collaborative Learning Environments

    [https://arxiv.org/abs/2403.01281](https://arxiv.org/abs/2403.01281)

    该论文提出了一种快速低参数视频活动定位系统，可在有限数据集上进行训练，并能准确检测并关联学生在现实教室视频中执行的活动。

    

    视频活动检测的研究主要集中在识别短视频片段中明确定义的人类活动。视频活动识别的大部分研究都集中在开发需要在大型视频数据集上训练的大参数系统上。本文开发了一个低参数、模块化系统，具有快速推理能力，可以完全在有限数据集上进行训练，而无需从大参数系统中进行迁移学习。该系统可以准确检测和将特定活动与在现实教室视频中执行活动的学生关联起来。此外，本文开发了一个交互式基于Web的应用程序，用于在长时间的现实教室视频上可视化人类活动地图。

    arXiv:2403.01281v1 Announce Type: cross  Abstract: Research on video activity detection has primarily focused on identifying well-defined human activities in short video segments. The majority of the research on video activity recognition is focused on the development of large parameter systems that require training on large video datasets. This paper develops a low-parameter, modular system with rapid inferencing capabilities that can be trained entirely on limited datasets without requiring transfer learning from large-parameter systems. The system can accurately detect and associate specific activities with the students who perform the activities in real-life classroom videos. Additionally, the paper develops an interactive web-based application to visualize human activity maps over long real-life classroom videos.
    
[^209]: Polynormer: 多项式表达的线性时间图转换器

    Polynormer: Polynomial-Expressive Graph Transformer in Linear Time

    [https://arxiv.org/abs/2403.01232](https://arxiv.org/abs/2403.01232)

    Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。

    

    图转换器（GTs）已经成为一种有前途的架构，理论上它比消息传递图神经网络（GNNs）更具表现力。然而，典型的GT模型至少具有二次复杂度，因此无法扩展到大型图。虽然最近提出了几种线性GTs，但它们在几个热门图数据集上仍落后于GNN对应模型，这对于它们的实际表现力构成了一个重要关注点。为了平衡GTs的表现力和可扩展性之间的权衡，我们提出了Polynormer，一个具有线性复杂度的多项式表达GT模型。Polynormer构建在一个新颖的基础模型上，该模型在输入特征上学习高次多项式。为了使基础模型具有置换等变性，我们将其与图拓扑和节点特征分开集成，从而产生本地和全局等变关注模型。因此，Polynormer采用了线性的局部到全局关注方案。

    arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
    
[^210]: ParallelPARC: 生成自然语言类比的可扩展流水线

    ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies

    [https://arxiv.org/abs/2403.01139](https://arxiv.org/abs/2403.01139)

    设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。

    

    Analogy-making对于人类认知至关重要，使我们能够适应新颖情境--这是当前人工智能系统仍然缺乏的能力。大多数类比数据集今天关注简单的类比（例如，词类比）；包含复杂类型类比的数据集通常是手工策划的，并且非常小。我们认为这限制了计算类比的进展。在这项工作中，我们设计了一个数据生成流水线，ParallelPARC（Parallel Paragraph Creator），利用最先进的大型语言模型（LLM）来创建基于段落的复杂类比，以及简单和具有挑战性的干扰项。我们展示了我们的流水线，并创建了ProPara-Logy，一个关于科学过程间类比的数据集。我们发布了一个由人类验证过的金标准数据集，以及一个自动生成的银标准数据集。我们在二进制和多选环境中测试了LLMs和人类对类比的识别，发现人类胜过最佳模型。

    arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
    
[^211]: ChatDiet：通过LLM增强框架赋能个性化营养导向食品推荐聊天机器人

    ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework

    [https://arxiv.org/abs/2403.00781](https://arxiv.org/abs/2403.00781)

    这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。

    

    食物对健康的深远影响使得先进的营养导向食品推荐服务成为必要。传统方法往往缺乏个性化、可解释性和互动性等关键元素。虽然大型语言模型（LLMs）带来了解释性和可解释性，但它们单独的使用未能实现真正的个性化。本文介绍了ChatDiet，一种新颖的LLM驱动框架，专门设计用于个性化营养导向食品推荐聊天机器人。ChatDiet集成了个人和人群模型，辅以一个协调器，无缝检索和处理相关信息。其结果是动态提供个性化和可解释的食品推荐，根据个人用户喜好定制。我们对ChatDiet进行了评估，包括一个引人入胜的案例研究，在案例研究中建立了一个因果个人模型来估计个人营养效果。

    arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
    
[^212]: 使用条件解码器增强视频的神经表示

    Boosting Neural Representations for Videos with a Conditional Decoder

    [https://arxiv.org/abs/2402.18152](https://arxiv.org/abs/2402.18152)

    引入条件解码器和NeRV-like模块的增强框架，以提高隐式视频表示方法的效果。

    

    隐式神经表示（INRs）已经成为视频存储和处理的一种有前途的方法，在各种视频任务中展现出显著的多功能性。然而，由于目标帧解码过程中中间特征的不足对齐，现有方法往往未能充分利用其表示能力。本文引入了一个通用的增强框架来加强当前的隐式视频表示方法。具体来说，我们利用具有时间感知仿射变换模块的条件解码器，该模块使用帧索引作为先验条件，有效地将中间特征与目标帧对齐。此外，我们引入了一个正弦NeRV-like模块来生成多样化的中间特征，并实现更平衡的参数分布，从而增强了模型的容量。借助高频信息保留的重构损失，我们的方法成功地增强了m

    arXiv:2402.18152v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts m
    
[^213]: 基于学习的图搜索问题算法

    Learning-Based Algorithms for Graph Searching Problems

    [https://arxiv.org/abs/2402.17736](https://arxiv.org/abs/2402.17736)

    本研究提出了针对未知图的图搜索问题的基于学习的算法，首次在未知加权图上建立了形式保证，并设计算法在预测误差上具有最优或几乎最佳依存关系。

    

    我们考虑了Banerjee等人（2022年）最近提出的具有预测的图搜索问题。在这个问题中，一个从某个顶点$r$出发的代理者必须在最小化总行程的同时遍历一个（潜在未知的）图$G$以找到隐藏的目标节点$g$。我们研究了一种设置，在这种设置中，在任意节点$v$处，代理者会接收到到$g$的距离的噪声估计。我们设计了针对未知图的这种搜索任务的算法。我们在未知加权图上建立了第一次形式保证，并提供了显示我们提出的算法在预测误差上具有最优或几乎最佳依存关系的下界。此外，我们进行了数值实验，证明我们的算法除了对抗性误差具有鲁棒性外，还在误差是随机的典型实例中表现良好。最后，我们提供了对Banerjee等人算法的替代简化性能界限。

    arXiv:2402.17736v1 Announce Type: cross  Abstract: We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et 
    
[^214]: 预测下一个词：人类在这项任务中表现出不确定性，而语言模型_____

    Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>

    [https://arxiv.org/abs/2402.17527](https://arxiv.org/abs/2402.17527)

    评估语言模型在预测下一个词时，是否能够复现人类在这项任务中展示的语言变化性

    

    语言模型（LMs）是训练用于为人类生成文本分配概率的统计模型。因此，合理质疑它们是否很好地近似人类展示的语言变化性。这种形式的统计评估在段落级别上很难执行，因为它需要可接受性判断（即，人类评估）或一个强大的自动代理（这是不平凡的）。然而，在单词级别上，通过给定一些上下文，可以通过与一个预先记录的替代单词连续数据集的精确匹配来评估LM的样本。我们利用这一事实，并评估LM重新生成人类（特别是一群英语使用者）在“下一个词预测”任务中展示的变化的能力。这可以被视为一种校准评估，在文本分类的背景下，Baan等人（2022年）将其称为对人类不确定性的校准

    arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint
    
[^215]: 用于视觉地点识别的深度单应性估计

    Deep Homography Estimation for Visual Place Recognition

    [https://arxiv.org/abs/2402.16086](https://arxiv.org/abs/2402.16086)

    提出了一种基于Transformer的深度单应性估计网络，用于快速和可学习的几何验证。

    

    视觉地点识别(VPR)是许多应用程序的基本任务，如机器人定位和增强现实。最近，由于准确性和效率之间的权衡，分层VPR方法受到了广泛关注。它们通常首先使用全局特征来检索候选图像，然后验证匹配的局部特征的空间一致性以进行重新排序。然而，后者通常依赖于RANSAC算法进行单应性拟合，这是耗时且不可微分的。这导致现有方法只能在全局特征提取中训练网络。在这里，我们提出了一种基于Transformer的深度单应性估计(DHE)网络，其以由主干网络提取的密集特征图为输入，并适合于快速和可学习的几何验证。此外，我们设计了一个内点重投影误差损失来训练DHE网络，无需添加额外......

    arXiv:2402.16086v1 Announce Type: cross  Abstract: Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without addit
    
[^216]: OpenSUN3D: 开放词汇的3D场景理解第一次研讨会挑战

    OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding

    [https://arxiv.org/abs/2402.15321](https://arxiv.org/abs/2402.15321)

    提供了OpenSUN3D研讨会上针对开放词汇3D场景理解的挑战概述，包括挑战数据集、评估方法和获胜方法的简要描述

    

    这份报告概述了在2023年ICCV会议上举办的OpenSUN3D Workshop关于开放词汇3D场景理解的挑战。该研讨会系列的目标是为开放词汇3D场景理解任务提供探索和讨论平台，包括但不限于分割、检测和映射。我们提供了研讨会上举办的挑战概述，展示了挑战数据集、评估方法以及获胜方法的简要描述。更多详情请参阅https://opensun3d.github.io/index_iccv23.html。

    arXiv:2402.15321v1 Announce Type: cross  Abstract: This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.
    
[^217]: 停止推理！当多模态LLMs与串联推理遇到对抗性图像

    Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images

    [https://arxiv.org/abs/2402.14899](https://arxiv.org/abs/2402.14899)

    该研究评估了多模态LLMs在采用串联推理时的对抗鲁棒性，发现串联推理在一定程度上提高了对抗性鲁棒性，但引入了一种新的停止推理攻击技术成功规避了这种增强。

    

    最近，多模态LLMs（MLLMs）展示了很强的理解图像的能力。然而，像传统视觉模型一样，它们仍然容易受到对抗性图像的攻击。与此同时，串联推理（CoT）已经被广泛应用在MLLMs上，不仅提高了模型的性能，而且通过提供中间推理步骤来增强模型的可解释性。然而，目前还缺乏关于MLLMs在CoT下的对抗鲁棒性的研究，以及在MLLMs用对抗性图像推断错误答案时推理的合理性。我们的研究评估了采用CoT推理时MLLMs的对抗鲁棒性，发现CoT在一定程度上提高了对抗性鲁棒性，抵抗了已有的攻击方法。此外，我们引入了一种新的停止推理攻击技术，可以有效地规避CoT引起的鲁棒性增强。最后，我们展示了CoT推理的变化。

    arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
    
[^218]: 为实现视觉地点识别的预训练模型的无缝适应

    Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition

    [https://arxiv.org/abs/2402.14505](https://arxiv.org/abs/2402.14505)

    提出了一种新颖的方法，实现了预训练模型对视觉地点识别的无缝适应

    

    最近的研究表明，在大规模数据上用通用的视觉学习任务预训练的视觉模型可以为各种视觉感知问题提供有用的特征表示。然而，很少有尝试利用在视觉地点识别（VPR）中利用预训练的基础模型。由于模型预训练和VPR任务之间在训练目标和数据方面的固有差异，如何弥合差距并充分发挥预训练模型在VPR中的能力仍然是一个关键问题。为此，我们提出了一种新颖的方法，实现了预训练模型对VPR的无缝适应。具体而言，我们设计了一种混合适应方法，以实现全局和局部适应，从而获得既关注显著地标用于区分地点的全局和局部特征。

    arXiv:2402.14505v1 Announce Type: cross  Abstract: Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to gu
    
[^219]: 利用人工智能自动化心理学假设生成：大型语言模型结合因果图

    Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph

    [https://arxiv.org/abs/2402.14424](https://arxiv.org/abs/2402.14424)

    利用大型语言模型和因果图结合的方法，在心理学假设生成中取得了突破，结果显示这种联合方法在新颖性方面明显优于仅使用大型语言模型的假设。

    

    我们的研究利用因果知识图谱和大型语言模型（LLM）之间的协同作用，引入了一种突破性的计算方法来生成心理学假设。我们使用LLM分析了43,312篇心理学文章，提取了因果关系对，生成了一个专门针对心理学的因果图。应用链接预测算法，我们生成了130个关注“幸福”的潜在心理学假设，然后将其与博士学者构思的研究想法和仅由LLM产生的想法进行了比较。有趣的是，我们的LLM和因果图的联合方法在新颖性方面与专家水平的洞察力保持一致，明显优于仅LLM的假设（分别为t(59)=3.34，p=0.007和t(59)=4.32，p<0.001）。这种一致性进一步通过深度语义分析得到证实。我们的结果表明，将LLM与因果图等机器学习技术相结合，可以更好地生成心理学假设。

    arXiv:2402.14424v1 Announce Type: new  Abstract: Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on `well-being', then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) = 4.32, p<0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal k
    
[^220]: 基于大型语言模型的混合推理在自动驾驶中的应用

    Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving

    [https://arxiv.org/abs/2402.13602](https://arxiv.org/abs/2402.13602)

    大型语言模型在自动驾驶中的混合推理能力可以通过分析数据、理解规则和法则、提供语境等方式提高自动驶驶的决策能力。

    

    大型语言模型（LLMs）因其理解文本和图像、生成类人文本以及执行复杂推理任务的能力而受到广泛关注。然而，它们将这种高级推理与自然语言文本相结合以用于动态情况下的决策的泛化能力需要进一步探索。本研究探讨了LLMs在混合算术和常识推理方面的适应能力和应用能力，特别是在自动驾驶场景中。我们假设LLMs的混合推理能力可以通过使它们分析检测到的物体和传感器数据、理解驾驶规定和物理法则，并提供额外的语境来改善自动驾驶。这解决了复杂情景，如低能见度（由于天气条件）下的决策，传统方法可能不足以胜任。我们通过准确性评估基于大型语言模型（LLMs）的这种能力。

    arXiv:2402.13602v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by co
    
[^221]: 面向计算存储硬件的深度神经网络架构和配置联合优化

    Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware

    [https://arxiv.org/abs/2402.11780](https://arxiv.org/abs/2402.11780)

    本文提出了CiMNet，一个旨在联合优化深度神经网络架构和配置的框架，以解决计算存储硬件构建中的挑战。

    

    随着大规模深度神经网络需求的增长，计算存储（CiM）作为一种突出解决方案，有助于缓解约束冯·诺依曼体系结构的带宽和芯片内部连接瓶颈。然而，CiM硬件的构建面临挑战，因为在不同接口的缓存大小和内存带宽方面的任何特定内存层次结构可能不理想地匹配任何神经网络的属性，如张量维度和算术强度，因此导致次优和表现不佳的系统。 尽管神经架构搜索（NAS）技术在为给定硬件度量预算（例如DNN执行时间或延迟）产生高效子网络方面取得成功，但它假设硬件配置被冻结，通常为给定预算产生次优子网络。 本文提出了CiMNet，一个联合搜索最优子网络和

    arXiv:2402.11780v1 Announce Type: cross  Abstract: With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and 
    
[^222]: ChatGPT是否能支持开发者？对大型语言模型用于代码生成的实证评估

    Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation

    [https://arxiv.org/abs/2402.11702](https://arxiv.org/abs/2402.11702)

    大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。

    

    大型语言模型（LLMs）已经展示出在代码生成方面的显著能力，许多先前的研究显示它们在各种开发场景中具有很大潜力。然而，这些研究主要提供了在研究环境中的评估，这在理解LLM在实际世界中能有效支持开发者方面留下了显著的空白。为了解决这个问题，我们对DevGPT中的对话进行了实证分析，这是从开发者与ChatGPT的对话中收集的数据集（通过GitHub等平台上的Share Link功能捕获）。我们的实证结果表明，目前使用LLM生成的代码的实践通常仅限于展示高层概念或提供文档中的示例，而不是作为可用于生产的代码。这些发现表明，在LLM代码生成方面还需要大量未来工作才能使其成为不可或缺的组成部分。

    arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
    
[^223]: HyperAgent：一种简单、可扩展、高效且可证明用于复杂环境的强化学习框架

    HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments

    [https://arxiv.org/abs/2402.10228](https://arxiv.org/abs/2402.10228)

    HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    

    为了在资源约束下解决复杂任务，强化学习（RL）代理需要简单、高效、可扩展、具有大状态空间和不断积累的交互数据。我们提出了HyperAgent，这是一个具有超模型、索引抽样方案和增量更新机制的RL框架，可以在一般价值函数逼近中进行计算高效的顺序后验逼近和数据高效的动作选择，超越了共轭性。HyperAgent的实现简单，只需要在DDQN中添加一个模块和一行额外代码。在实践中，HyperAgent在大规模深度RL基准测试中表现出稳健的性能，无论是在数据还是计算方面都获得了显着的效率提升。在理论上，在实际可扩展的算法中，HyperAgent是第一个能够实现可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
    
[^224]: 重新审视带有内存单子的循环强化学习

    Revisiting Recurrent Reinforcement Learning with Memory Monoids

    [https://arxiv.org/abs/2402.09900](https://arxiv.org/abs/2402.09900)

    这篇论文重新审视了使用内存单子的循环强化学习方法。通过定义新颖的内存单子框架并提出一种新的批处理方法，改进了样本效率、增加了回报并简化了实现过程。

    

    在强化学习中，像RNN和transformers这样的记忆模型通过将轨迹映射到潜在的马尔可夫状态来处理部分可观察的马尔可夫决策过程（POMDPs）。这些模型对于长序列的规模化处理能力并不特别好，尤其是与一类新兴的记忆模型（有时称为线性循环模型）相比。我们发现这些模型的循环更新是一个单子，因此我们正式定义了一个新颖的内存单子框架。我们重新审视了循环强化学习中的传统批处理方法，突出了理论和实证上的不足之处。利用内存单子的特性，我们提出了一种新的批处理方法，改进了样本效率，增加了回报，并简化了循环丢失函数在强化学习中的实施。

    arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
    
[^225]: 通过反向课程强化学习训练大型语言模型进行推理

    Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

    [https://arxiv.org/abs/2402.05808](https://arxiv.org/abs/2402.05808)

    本文提出了一种通过反向课程强化学习训练大型语言模型进行推理的新方法，通过学习正确演示并建立逐步的课程，实现了结果监督和过程监督的优化。

    

    在本文中，我们提出了R$^3$：通过反向课程强化学习（RL）进行推理的学习方法，该方法只使用结果监督来实现大型语言模型的过程监督的好处。将RL应用于复杂推理的核心挑战是确定一系列行动，以获得正向奖励并提供适当的优化监督。结果监督为最终结果提供了稀疏奖励，而不识别错误位置，而过程监督提供了逐步奖励，但需要大量手动注释。R$^3$通过学习正确演示来克服这些限制。具体而言，R$^3$将推理的起始状态从演示的结束滑动到开始，从而在所有阶段都促进了更容易的模型探索。因此，R$^3$建立了一个逐步的课程，使结果监督能够提供阶段级信号并精确定位错误。

    In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7
    
[^226]: 明明就在眼前：对弱势患者群体进行不可检测的对抗性偏见攻击

    Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations

    [https://arxiv.org/abs/2402.05713](https://arxiv.org/abs/2402.05713)

    该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。

    

    人工智能在放射学中的广泛应用揭示了深度学习模型加剧对弱势患者群体的临床偏见的风险。虽然先前的文献主要关注训练的深度学习模型所展示的偏见的量化，但针对特定人口群体的对抗性偏见攻击以及其在临床环境中的影响仍然是一个未被充分研究的医学影像领域。在这项工作中，我们证明了针对人口统计学标签的毒化攻击可以向深度学习模型引入对抗性的诊断不足偏见，并在不影响整体模型性能的情况下降低对被低估群体的性能。此外，我们的结果在多个性能指标和人口群体（如性别、年龄以及其交叉子群）上表明，群体对于不可检测的对抗性偏见攻击的脆弱性与其在模型的训练数据中的表征直接相关。

    The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
    
[^227]: S-Agents: 自组织代理在无限环境中的应用

    S-Agents: self-organizing agents in open-ended environment

    [https://arxiv.org/abs/2402.04578](https://arxiv.org/abs/2402.04578)

    S-Agents是一个自组织代理系统，通过引入代理树结构、沙漏代理架构和非阻塞协作方法，实现了在无限环境中高效协调代理的能力，提供了优化协作效率和灵活性的解决方案。

    

    利用大型语言模型（LLMs），自主代理能够显著提升，具备处理各种任务的能力。在无限环境中，为了提高效率和效果，优化协作需要灵活的调整。然而，当前的研究主要强调固定的、任务导向的工作流程，忽视了以代理为中心的组织结构。受人类组织行为的启发，我们引入了一种自组织代理系统（S-Agents），其中包括动态工作流程的“代理树”结构、平衡信息优先级的“沙漏代理架构”以及允许代理之间异步执行任务的“非阻塞协作”方法。这种结构可以自主协调一组代理，有效地解决无限且动态的环境挑战，无需人工干预。我们的实验表明，S-Agents能够熟练地执行协作建筑任务和资源收集。

    Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection i
    
[^228]: LHRS-Bot：利用VGI增强的大型多模态语言模型赋能遥感领域

    LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model

    [https://arxiv.org/abs/2402.02544](https://arxiv.org/abs/2402.02544)

    LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    

    大型语言模型（LLMs）的革命性能力开创了多模态大型语言模型（MLLMs）并促进了在各个专业领域的多样化应用。然而，在遥感（RS）领域中，近期的MLLM努力未能充分考虑到遥感图像中多样的地理景观和物体。为了弥补这一差距，我们构建了一个大规模的RS图像-文本数据集LHRS-Align，以及一个信息丰富的RS特定指导数据集LHRS-Instruct，利用丰富的自愿地理信息（VGI）和全球可用的RS图像。在此基础上，我们引入了LHRS-Bot，一种针对RS图像理解的MLLM，通过一种新颖的多层次视觉-语言对齐策略和课程学习方法。全面的实验证明，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
    
[^229]: LeTO：通过可微分轨迹优化学习受限视觉运动策略

    LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization

    [https://arxiv.org/abs/2401.17500](https://arxiv.org/abs/2401.17500)

    LeTO 是一种通过可微分轨迹优化实现受限视觉运动策略的学习方法，它将优化层表示为轨迹优化问题，使模型能以安全可控的方式端到端生成动作。通过引入约束信息，实现了平衡满足约束、平滑轨迹和最小化演示误差的训练目标。在仿真和实际机器人中进行了评估，表明LeTO方法在成功率上与最先进的模仿学习方法相当。

    

    本文介绍了一种名为LeTO的方法，通过可微分轨迹优化实现受限视觉运动策略的学习。我们的方法独特地将一个可微分优化层整合到神经网络中。通过将优化层表示为一个轨迹优化问题，我们能够使模型以安全和可控的方式端到端生成动作，而无需额外的模块。我们的方法允许在训练过程中引入约束信息，从而平衡满足约束、平滑轨迹和最小化演示误差的训练目标。这种“灰盒”方法将基于优化的安全性和可解释性与神经网络的强大表达能力结合在一起。我们在仿真和实际机器人上对LeTO进行了定量评估。在仿真中，LeTO的成功率与最先进的模仿学习方法相当，但生成的轨迹的不一致性较小。

    This paper introduces LeTO, a method for learning constrained visuomotor policy via differentiable trajectory optimization. Our approach uniquely integrates a differentiable optimization layer into the neural network. By formulating the optimization layer as a trajectory optimization problem, we enable the model to end-to-end generate actions in a safe and controlled fashion without extra modules. Our method allows for the introduction of constraints information during the training process, thereby balancing the training objectives of satisfying constraints, smoothing the trajectories, and minimizing errors with demonstrations. This "gray box" method marries the optimization-based safety and interpretability with the powerful representational abilities of neural networks. We quantitatively evaluate LeTO in simulation and on the real robot. In simulation, LeTO achieves a success rate comparable to state-of-the-art imitation learning methods, but the generated trajectories are of less un
    
[^230]: CMMMU：一个中国大规模多学科多模态理解基准

    CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark

    [https://arxiv.org/abs/2401.11944](https://arxiv.org/abs/2401.11944)

    CMMMU是一个旨在评估大型多模型模型在大学级学科知识和深思熟虑推理任务中表现的中文大规模多学科多模态理解基准，为填补在非英语环境中评估先进知识和推理能力的空白而设计。

    

    随着大型多模型模型(LMMs)的能力不断提升，评估LMMs的表现日益成为一个迫切的需求。此外，在评估LMMs在中文等非英语环境中先进知识和推理能力方面存在更大差距。我们引入了CMMMU，一个新的中文大规模多学科多模态理解基准，旨在评估LMMs在需要大学水平学科知识和深思熟虑推理的任务中的表现。CMMMU受到了MMMUs的标注和分析模式的启发并严格遵循。CMMMU包括来自大学考试、测验和教科书的1.2万个手动收集的多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文社科以及技术与工程，就像其伙伴MMMMU一样。这些问题涵盖30个学科，包括39个高度异质的图像。

    arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
    
[^231]: FaceTalk: 面部特征驱动的神经参数头部模型的运动扩散

    FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models

    [https://arxiv.org/abs/2312.08459](https://arxiv.org/abs/2312.08459)

    FaceTalk是一种生成方法，利用神经参数头部模型的潜在空间和潜在扩散模型，从输入音频信号中合成高保真3D运动序列，是首个提出这种方法用于合成逼真和高质量运动序列的工作。

    

    我们引入了FaceTalk，这是一种新颖的生成方法，旨在从输入音频信号合成说话人头部的高保真3D运动序列。为了捕捉人头部的富有表现力和细节性质，包括头发、耳朵和更细微的眼睛运动，我们提议将语音信号与神经参数头部模型的潜在空间相结合，以创建高保真、时间连贯的运动序列。我们为这一任务提出了一种新的潜在扩散模型，它在神经参数头部模型的表达空间中运行，以合成受音频驱动的逼真头部序列。鉴于缺乏具有相应NPHM表达的数据集，我们优化这些对应关系，以生成适合音视频记录中说话人的时间优化NPHM表达的数据集。据我们所知，这是首个提出一种生成方法用于生成逼真且高质量运动序列的工作。

    arXiv:2312.08459v2 Announce Type: replace-cross  Abstract: We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality m
    
[^232]: 通过大型语言模型重新定义开发者援助：在软件生态系统中的应用

    Redefining Developer Assistance: Through Large Language Models in Software Ecosystem

    [https://arxiv.org/abs/2312.05626](https://arxiv.org/abs/2312.05626)

    本研究介绍了一种通过指导调整开发的DevAssistLlama模型，在处理软件相关自然语言查询时表现出优异能力，突出了专门LLM在软件开发中的潜力。

    

    在本文中，我们深入探讨了领域特定的大型语言模型（LLMs）的进展，重点关注它们在软件开发中的应用。我们介绍了DevAssistLlama，这是一个通过指导调整开发的模型，可帮助开发人员处理与软件相关的自然语言查询。这个模型，作为指导调整的LLM变体，特别擅长处理复杂的技术文档，增强了开发人员在软件特定任务中的能力。DevAssistLlama的创建涉及从各种软件系统构建了一个广泛的指导数据集，从而有效处理命名实体识别（NER）、关系抽取（RE）和链接预测（LP）。我们的结果表明，在这些任务中，DevAssistLlama相对于其他模型（包括ChatGPT）具有更优越的能力。这项研究不仅突出了专门LLM在软件开发中的潜力

    arXiv:2312.05626v2 Announce Type: replace-cross  Abstract: In this paper, we delve into the advancement of domain-specific Large Language Models (LLMs) with a focus on their application in software development. We introduce DevAssistLlama, a model developed through instruction tuning, to assist developers in processing software-related natural language queries. This model, a variant of instruction tuned LLM, is particularly adept at handling intricate technical documentation, enhancing developer capability in software specific tasks. The creation of DevAssistLlama involved constructing an extensive instruction dataset from various software systems, enabling effective handling of Named Entity Recognition (NER), Relation Extraction (RE), and Link Prediction (LP). Our results demonstrate DevAssistLlama's superior capabilities in these tasks, in comparison with other models including ChatGPT. This research not only highlights the potential of specialized LLMs in software development also t
    
[^233]: 自回归大型语言模型中的提示

    Prompting in Autoregressive Large Language Models

    [https://arxiv.org/abs/2312.03740](https://arxiv.org/abs/2312.03740)

    LLMs通过创新的提示技术实现了预训练和提示范式的转变，以大大提高下游NLP任务的效果。

    

    自回归大型语言模型已经改变了自然语言处理的格局。预训练和提示范式已经取代了许多下游NLP任务常规的预训练和微调方法。这种转变主要得益于LLMs和创新的提示技术。LLMs显示出巨大的潜力用于各种下游任务，这归功于它们在预训练中使用的大量参数和庞大数据集。然而，为了充分发挥它们的潜力，必须引导它们的输出朝着期望的结果。提示，即提供特定的输入或指令来引导LLMs朝着预期输出的方向发展，已成为实现这一目标的工具。本文讨论了已被应用来充分利用LLMs潜力的各种提示技术。我们提出了现有文献中关于提示技术的分类，并根据其进行了简明调查。

    arXiv:2312.03740v1 Announce Type: cross  Abstract: Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre-train and prompt paradigm has replaced the conventional approach of pre-training and fine-tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on. However, in order to fully realize their potential, their outputs must be guided towards the desired outcomes. Prompting, in which a specific input or instruction is provided to guide the LLMs toward the intended output, has become a tool for achieving this goal. In this paper, we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on
    
[^234]: DP-OPT：使大型语言模型成为您的隐私保护提示工程师

    DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer

    [https://arxiv.org/abs/2312.03724](https://arxiv.org/abs/2312.03724)

    提出了差分私密离线提示调整（DP-OPT）解决大型语言模型调整提示时的隐私问题，通过在客户端调整提示并应用于云模型实现了隐私保护和数据传输

    

    大型语言模型（LLMs）已成为各种任务的主要工具，尤其是通过提示调整针对特定目标时。然而，由于调整的提示依赖于敏感的私人信息，围绕数据隐私的担忧提出了障碍。一个实际的解决方案是托管一个本地的LLM，并使用数据私下优化一个软提示。然而，当模型所有权受到保护时，托管一个本地模型就变得有问题。将数据发送给模型提供程序进行培训等替代方法加剧了这些隐私问题，面对一个不受信任的提供者。在本文中，我们提出了一种名为差分私密离线提示调整（DP-OPT）的新颖解决方案以解决这一挑战。我们的方法涉及在客户端调整离散提示，然后将其应用于所需的云模型。我们演示了LLMs本身建议的提示可以在不暴露p的情况下进行传递

    arXiv:2312.03724v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising p
    
[^235]: SoftMAC：基于预测接触模型和与关节刚体和衣物双向耦合的可微软体仿真

    SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes

    [https://arxiv.org/abs/2312.03297](https://arxiv.org/abs/2312.03297)

    SoftMAC提出了一个不同于以往的可微仿真框架，能够将软体、关节刚体和衣物耦合在一起，并采用基于预测的接触模型和穿透追踪算法，有效地减少了穿透现象。

    

    可微物理仿真通过基于梯度的优化，显著提高了解决机器人相关问题的效率。为在各种机器人操纵场景中应用可微仿真，一个关键挑战是将各种材料集成到统一框架中。我们提出了SoftMAC，一个可微仿真框架，将软体与关节刚体和衣物耦合在一起。SoftMAC使用基于连续力学的材料点法来模拟软体。我们提出了一种新颖的基于预测的MPM接触模型，有效减少了穿透，而不会引入其他异常现象，如不自然的反弹。为了将MPM粒子与可变形和非体积衣物网格耦合，我们还提出了一种穿透追踪算法，重建局部区域的有符号距离场。

    arXiv:2312.03297v2 Announce Type: replace-cross  Abstract: Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from prev
    
[^236]: 探索、选择、推导和回忆：为移动任务自动化增加类人记忆的LLM

    Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation

    [https://arxiv.org/abs/2312.03003](https://arxiv.org/abs/2312.03003)

    MobileGPT是一种创新的基于LLM的移动任务自动化工具，通过类人应用记忆模拟人类与移动应用的认知过程，实现任务程序的精确高效学习。

    

    大型语言模型（LLMs）的出现为移动任务自动化领域带来了新的机遇。它们优越的语言理解和推理能力使用户能够自动执行复杂和重复的任务。然而，由于LLMs固有的不可靠性和高运行成本，它们的实际适用性相当有限。为解决这些问题，本文引入了MobileGPT，这是一种创新的基于LLM的移动任务自动化工具，配备了类人应用记忆。MobileGPT模拟了人类与移动应用交互的认知过程--探索、选择、推导和回忆。这种方法通过将任务程序分解为更小、模块化的子任务，允许更精确、高效地学习任务流程，从而实现子任务的重复使用、重新排列和适应各种目标。我们使用在线LLM服务（GPT-3.5和GPT-4）实现了MobileGPT，并在一组数据上评估了其性能。

    arXiv:2312.03003v2 Announce Type: replace-cross  Abstract: The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MobileGPT, an innovative LLM-based mobile task automator equipped with a human-like app memory. MobileGPT emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular sub-tasks that can be re-used, re-arranged, and adapted for various objectives. We implement MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on a datase
    
[^237]: 通过基于良知的对准框架抵御对抗性攻击

    Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework

    [https://arxiv.org/abs/2312.00029](https://arxiv.org/abs/2312.00029)

    Bergeron提出了一个基于良知的对齆框架，能够提高大型语言模型对抗攻击的鲁棒性，无需额外参数微调。

    

    近年来，随着越来越强大的大型语言模型（LLMs）的引入，人工智能对齐的研究取得了可观的进展。不幸的是，现代对齐方法仍然无法完全防止在模型被蓄意攻击时产生有害应对。为了帮助缓解这一问题，我们引入了Bergeron：一个旨在提高LLMs对抗攻击鲁棒性的框架，无需进行额外的参数微调。Bergeron分为两个层次；次要LLM模拟受保护的主要LLM的良知。该框架在监视输出以检测任何有害内容的同时，更好地保护主要模型免受入侵攻击。实证分析表明，使用Bergeron来补充现有对齐训练的模型

    arXiv:2312.00029v2 Announce Type: replace-cross  Abstract: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment traini
    
[^238]: 通过Wasserstein度量进行数据集精炼

    Dataset Distillation via the Wasserstein Metric

    [https://arxiv.org/abs/2311.18531](https://arxiv.org/abs/2311.18531)

    通过引入Wasserstein距离及其重心，我们提出一种有效的数据集精炼方法，利用先验知识提高分布匹配效果，实现了新的最先进性能。

    

    数据集精炼（DD）作为一种强大的策略，将大型数据集的丰富信息封装为明显更小的合成等价物，从而在减少计算开销的同时保留模型性能。为实现这一目标，我们引入了Wasserstein距离，这是一种基于最优输运理论的度量，用于增强DD中的分布匹配。我们的方法利用Wasserstein重心提供了一种在量化分布差异和高效捕获分布集合中心的几何意义方法。通过在预训练分类模型的特征空间中嵌入合成数据，我们促进了有效的分布匹配，利用这些模型固有的先验知识。我们的方法不仅保持了基于分布匹配的技术的计算优势，而且在一系列任务中实现了新的最先进性能。

    arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran
    
[^239]: 通过不同的主干和统计匹配进行广义大规模数据压缩

    Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching

    [https://arxiv.org/abs/2311.17950](https://arxiv.org/abs/2311.17950)

    本文提出了广义匹配的概念，并在此基础上提出了Generalized Various Backbone and Statistical Matching (G-VBSM)方法，可以创建一个具有丰富信息和更好概括能力的压缩数据集。

    

    SRe2L引入的轻量级“局部匹配-全局匹配”成功地创造了一个经过精心筛选的数据集，其中包含来自完整的224x224 ImageNet-1k的全面信息。然而，这种单边方法只适用于特定的主干、层和统计数据，从而限制了精简数据集概括能力的提升。我们建议充分而各异的“局部匹配-全局匹配”比单一匹配更加精确和有效，并能够创造出更丰富信息和更好概括能力的压缩数据集。我们称之为“广义匹配”观点，并在本文中提出了广义不同主干和统计匹配 (G-VBSM)，旨在创建一个具有密度的合成数据集，确保在不同的主干、层和统计数据上与完整数据集保持一致。实验证明，G-VBSM是第一个获得强大性能的算法。

    arXiv:2311.17950v2 Announce Type: replace-cross  Abstract: The lightweight "local-match-global" matching introduced by SRe2L successfully creates a distilled dataset with comprehensive information on the full 224x224 ImageNet-1k. However, this one-sided approach is limited to a particular backbone, layer, and statistics, which limits the improvement of the generalization of a distilled dataset. We suggest that sufficient and various "local-match-global" matching are more precise and effective than a single one and has the ability to create a distilled dataset with richer information and better generalization. We call this perspective "generalized matching" and propose Generalized Various Backbone and Statistical Matching (G-VBSM) in this work, which aims to create a synthetic dataset with densities, ensuring consistency with the complete dataset across various backbones, layers, and statistics. As experimentally demonstrated, G-VBSM is the first algorithm to obtain strong performance a
    
[^240]: 用于解决线性逆问题的深度正则化复合高斯网络

    Deep Regularized Compound Gaussian Network for Solving Linear Inverse Problems

    [https://arxiv.org/abs/2311.17248](https://arxiv.org/abs/2311.17248)

    提出了两种允许在复合高斯分布类中进行问题特定统计先验选择的线性逆问题新方法，一种是迭代算法广义复合高斯最小二乘，另一种是通过展开得到的新颖深度正则化神经网络DR-CG-Net。

    

    将先验信息纳入逆问题，例如通过最大后验估计，是促进稳健逆问题解决方案的重要技术。本文为允许在复合高斯（CG）分布类中进行问题特定统计先验选择的线性逆问题设计了两种新方法。第一种方法是一种迭代算法，称为广义复合高斯最小二乘（G-CG-LS），它最小化了一个正则化最小二乘目标函数，其中正则化强制执行一个CG先验。然后将G-CG-LS展开，提供我们的第二种方法，这是一种新颖的深度正则化（DR）神经网络，称为DR-CG-Net，可以学习先验信息。

    arXiv:2311.17248v2 Announce Type: replace-cross  Abstract: Incorporating prior information into inverse problems, e.g. via maximum-a-posteriori estimation, is an important technique for facilitating robust inverse problem solutions. In this paper, we devise two novel approaches for linear inverse problems that permit problem-specific statistical prior selections within the compound Gaussian (CG) class of distributions. The CG class subsumes many commonly used priors in signal and image reconstruction methods including those of sparsity-based approaches. The first method developed is an iterative algorithm, called generalized compound Gaussian least squares (G-CG-LS), that minimizes a regularized least squares objective function where the regularization enforces a CG prior. G-CG-LS is then unrolled, or unfolded, to furnish our second method, which is a novel deep regularized (DR) neural network, called DR-CG-Net, that learns the prior information. A detailed computational theory on conv
    
[^241]: 模块化神经网络用于时间序列预测：使用注意力进行解释性和特征选择

    Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention

    [https://arxiv.org/abs/2311.16834](https://arxiv.org/abs/2311.16834)

    该论文提出了一种新颖的模块化神经网络模型，用于多变量时间序列预测，通过循环神经网络学习时间依赖关系，并使用基于注意力的特征选择组件实现解释性，实验结果表明其优于当前最先进的解释性神经加性模型（NAM）及其变体。

    

    多变量时间序列在医疗保健、气象学和生命科学等领域有许多应用。虽然深度学习模型在时间序列预测方面表现出色，但被批评为“黑盒”或无法解释。本文提出了一种新颖的用于多变量时间序列预测的模块化神经网络模型，其构造具有解释性。循环神经网络学习数据中的时间依赖关系，而基于注意力的特征选择组件选择最相关的特征并抑制在学习时间依赖性中使用的冗余特征。从选择的特征独立训练模块化深度网络，向用户展示特征如何影响结果，使模型具有解释性。实验结果表明，这种方法可以超过最先进的可解释性神经加性模型（NAM）及其变体。

    arXiv:2311.16834v3 Announce Type: replace-cross  Abstract: Multivariate time series have many applications, from healthcare and meteorology to life science. Although deep learning models have shown excellent predictive performance for time series, they have been criticised for being "black-boxes" or non-interpretable. This paper proposes a novel modular neural network model for multivariate time series prediction that is interpretable by construction. A recurrent neural network learns the temporal dependencies in the data while an attention-based feature selection component selects the most relevant features and suppresses redundant features used in the learning of the temporal dependencies. A modular deep network is trained from the selected features independently to show the users how features influence outcomes, making the model interpretable. Experimental results show that this approach can outperform state-of-the-art interpretable Neural Additive Models (NAM) and variations thereo
    
[^242]: UniRepLKNet：一种用于音频、视频、点云、时间序列和图像识别的通用感知大卷积神经网络

    UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition

    [https://arxiv.org/abs/2311.15599](https://arxiv.org/abs/2311.15599)

    提出了四项用于设计大卷积神经网络的架构指南，并展示了在多模态领域具有通用感知能力

    

    大卷积神经网络(ConvNets)最近受到了广泛的研究关注，但仍有两个未解决的重要问题需要进一步研究。1) 现有大卷积神经网络的架构在很大程度上遵循传统ConvNets或transformers的设计原则，而大卷积神经网络的架构设计仍未得到解决。2) 随着transformers主导了多种模态，仍需探讨ConvNets在视觉以外领域是否具有强大的通用感知能力。在本文中，我们从两个方面做出了贡献。1) 我们提出了四项用于设计大卷积神经网络的架构指南，其中的核心是利用大卷积核的基本特征，区别于小卷积核-它们可以广泛地观察而无需深入。遵循这些指南，我们提出的大卷积神经网络表现出领先的性能

    arXiv:2311.15599v2 Announce Type: replace-cross  Abstract: Large-kernel convolutional neural networks (ConvNets) have recently received extensive research attention, but two unresolved and critical issues demand further investigation. 1) The architectures of existing large-kernel ConvNets largely follow the design principles of conventional ConvNets or transformers, while the architectural design for large-kernel ConvNets remains under-addressed. 2) As transformers have dominated multiple modalities, it remains to be investigated whether ConvNets also have a strong universal perception ability in domains beyond vision. In this paper, we contribute from two aspects. 1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep. Following such guidelines, our proposed large-kernel ConvNet shows leading performanc
    
[^243]: 深度学习中通过几何调整的梯度下降以均匀指数速率全局$\mathcal{L}^2$最小化

    Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning

    [https://arxiv.org/abs/2311.15487](https://arxiv.org/abs/2311.15487)

    通过几何调整的梯度下降，在深度学习中以均匀指数速率实现全局$\mathcal{L}^2$最小化，这一方法在过参数化情况下具有明确自然的不变几何含义。

    

    我们考虑在深度学习网络中广泛使用的用于最小化$\mathcal{L}^2$代价函数的梯度下降流，并引入两个改进版本；一个适用于过参数化设置，另一个适用于欠参数化设置。这两个版本都具有明确自然的不变几何含义，考虑到在过参数化设置中的拉回向量丛结构和在欠参数化设置中的推前向量丛结构。在过参数化情况下，我们证明，只要满足秩条件，改进的梯度下降的所有轨道将以均匀指数收敛速率将$\mathcal{L}^2$代价驱动到全局最小值；因此，对于任何预先指定的接近全局最小值的近似，我们可以得到先验停止时间。我们指出后者与次Riemann几何的关系。

    arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.
    
[^244]: 转向机器监督：用于自动医学图像分割和分类的标注高效的半监督和自监督学习

    Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification

    [https://arxiv.org/abs/2311.10319](https://arxiv.org/abs/2311.10319)

    引入了S4MI流程，利用自监督和半监督学习的高效方法，能够简化医学图像的机器监督过程，自监督学习在分类任务中表现明显优于监督方法。

    

    临床治疗的进展越来越受到监督学习技术的限制，这些技术严重依赖于大量标注数据。标注过程不仅成本高昂，而且需要临床专家大量时间。为了解决这一问题，我们引入了S4MI（医学图像的自监督和半监督）流程，这是一种利用自监督和半监督学习的发展的新方法。这些技术参与不需要标记的辅助任务，从而简化了机器监督的扩展，相比完全监督的方法。我们的研究在三个不同的医学图像数据集上对这些技术进行基准测试，以评估它们在分类和分割任务中的有效性。值得注意的是，我们发现自监督学习在分类任务中明显优于监督方法。

    arXiv:2311.10319v3 Announce Type: replace-cross  Abstract: Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages the advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self-supervised learning significantly surpassed the performance of supervised methods in the classificati
    
[^245]: 通过多例学习实现内在可解释的时间序列分类

    Inherently Interpretable Time Series Classification via Multiple Instance Learning

    [https://arxiv.org/abs/2311.10049](https://arxiv.org/abs/2311.10049)

    通过多例学习提出了一个新框架MILLET，使现有的深度学习时间序列分类模型变得内在可解释，同时不影响甚至改进预测性能，并在多个数据集上展示出比其他方法更高质量的稀疏解释。

    

    传统的时间序列分类（TSC）方法通常是黑匣子，难以理解其决策过程。在这项工作中，我们利用多例学习（MIL）来解决这个问题，并提出了一个名为MILLET的新框架：用于本地可解释时间序列分类的多例学习。我们将MILLET应用于现有的深度学习TSC模型，并展示它们如何变得内在可解释，而不会影响（在某些情况下，甚至提高）预测性能。我们在85个UCR TSC数据集上评估了MILLET，并提出了一个特别设计的新颖合成数据集，以促进可解释性评估。在这些数据集上，我们展示MILLET能够快速产生比其他众所周知的可解释性方法更高质量的稀疏解释。

    arXiv:2311.10049v3 Announce Type: replace-cross  Abstract: Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/J
    
[^246]: PubDef：防御来自公共模型的迁移攻击

    PubDef: Defending Against Transfer Attacks From Public Models

    [https://arxiv.org/abs/2310.17645](https://arxiv.org/abs/2310.17645)

    本文提出了一个新的实用威胁模型，其中对手依赖于通过公开可用的替代模型的迁移攻击，提出了一种基于博弈论视角的专门防御方法，并在多个公共模型和攻击算法下进行了评估。

    

    对抗性攻击一直是行业中一个令人担忧且未解决的威胁。然而，通过十年的鲁棒性评估文献历史，我们了解到发起强大或最优攻击是具有挑战性的，它需要机器学习和领域专业知识。换句话说，过去大多数文献坚定假设的白盒威胁模型是不现实的。在本文中，我们提出了一个新的实用威胁模型，在这个模型中，对手依赖于通过公开可用的替代模型的迁移攻击。我们认为这种设置将成为未来安全敏感应用中最普遍的情况。我们评估了在这种设置下的迁移攻击，并提出了基于博弈论视角的专门防御方法。这些防御方法在24个公共模型和11种攻击算法下进行了评估，涵盖三个数据集（CIFAR-10、CIFAR-100 和 ImageNet）。

    arXiv:2310.17645v2 Announce Type: replace-cross  Abstract: Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under thi
    
[^247]: 抓取过程中协助价值的研究

    Value of Assistance for Grasping

    [https://arxiv.org/abs/2310.14402](https://arxiv.org/abs/2310.14402)

    提出了一种新颖的协助价值（VOA）度量，用于评估特定观测对机器人完成抓取任务的预期影响，通过模拟和真实环境验证了该度量的有效性。

    

    在多种实际设置中，机器人被赋予抓取物体的任务，但不知道物体的确切姿势，依赖对姿势的概率估计来决定如何尝试抓取。我们支持在尝试抓取之前向机器人提供物体观测的设置，但这种可能性有限，需要决定哪种感知行为最有益。我们通过提供一种新颖的协助价值（Value of Assistance, VOA）度量来支持这种决策，这种度量用于评估特定观测将对机器人完成任务的能力产生的预期影响。我们在模拟和真实世界的协作抓取环境中评估了我们提出的度量方法。

    arXiv:2310.14402v2 Announce Type: replace-cross  Abstract: In multiple realistic settings, a robot is tasked with grasping an object without knowing its exact pose and relies on a probabilistic estimation of the pose to decide how to attempt the grasp. We support settings in which it is possible to provide the robot with an observation of the object before a grasp is attempted but this possibility is limited and there is a need to decide which sensing action would be most beneficial. We support this decision by offering a novel Value of Assistance (VOA) measure for assessing the expected effect a specific observation will have on the robot's ability to complete its task. We evaluate our suggested measure in simulated and real-world collaborative grasping settings.
    
[^248]: Llemma: 一种用于数学的开放语言模型

    Llemma: An Open Language Model For Mathematics

    [https://arxiv.org/abs/2310.10631](https://arxiv.org/abs/2310.10631)

    Llemma是一个用于数学的开放语言模型，在MATH基准测试中表现优异，能够进行工具使用和形式定理证明，无需进一步微调。

    

    我们提出了Llemma，一个用于数学的大型语言模型。我们继续在Proof-Pile-2上对Code Llama进行预训练，Proof-Pile-2包含科学论文、包含数学内容的网络数据和数学代码，最终生成了Llemma。在MATH基准测试中，Llemma在同等参数基础上胜过所有已知的开源基准模型，以及尚未发布的Minerva模型套件。此外，Llemma能够进行工具使用和形式定理证明，而无需进一步微调。我们公开发布所有工件，包括70亿和340亿参数模型、Proof-Pile-2以及用于复制我们实验的代码。

    arXiv:2310.10631v3 Announce Type: replace-cross  Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.
    
[^249]: 面向下一代计算机视觉的基于脉冲的神经形态计算

    Spike-based Neuromorphic Computing for Next-Generation Computer Vision

    [https://arxiv.org/abs/2310.09692](https://arxiv.org/abs/2310.09692)

    基于脉冲的神经形态计算作为对今天主导视觉领域的深度卷积神经网络的可行替代，承诺能够实现数量级的能量效率提升。

    

    神经形态计算承诺在能量效率上比传统的冯·诺依曼计算范式有数量级的改进。其目标是通过学习和模拟大脑功能来开发一种自适应、容错、低占地面积、快速、低能耗的智能系统，可以通过不同抽象层次的创新来实现，包括材料、器件、电路、架构和算法。由于在复杂视觉任务中的能量消耗因数据集增大而呈指数级增长，而资源受限的边缘设备变得越来越普遍，基于脉冲的神经形态计算方法可以作为今天主导视觉领域的深度卷积神经网络的可行替代方案。在本书章节中，我们介绍了神经形态计算，概述了设计堆栈的不同层面（器件、电路和算法）中的一些代表性示例，并得出结论。

    arXiv:2310.09692v2 Announce Type: replace-cross  Abstract: Neuromorphic Computing promises orders of magnitude improvement in energy efficiency compared to traditional von Neumann computing paradigm. The goal is to develop an adaptive, fault-tolerant, low-footprint, fast, low-energy intelligent system by learning and emulating brain functionality which can be realized through innovation in different abstraction layers including material, device, circuit, architecture and algorithm. As the energy consumption in complex vision tasks keep increasing exponentially due to larger data set and resource-constrained edge devices become increasingly ubiquitous, spike-based neuromorphic computing approaches can be viable alternative to deep convolutional neural network that is dominating the vision field today. In this book chapter, we introduce neuromorphic computing, outline a few representative examples from different layers of the design stack (devices, circuits and algorithms) and conclude w
    
[^250]: WeatherDepth: 恶劣天气条件下自监督深度估计的课程对比学习

    WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions

    [https://arxiv.org/abs/2310.05556](https://arxiv.org/abs/2310.05556)

    提出了一种WeatherDepth模型，通过课程对比学习，逐步适应恶劣天气条件下深度估计需求，并通过逐渐抓住有益深度线索对抗天气影响，实现更好的领域适应。

    

    深度估计模型在清晰场景上表现出色，但由于光照变化、天气颗粒等无法推广到恶劣天气条件。本文提出了WeatherDepth，一种具有课程对比学习的自监督稳健深度估计模型，以解决在复杂天气条件下性能下降的问题。具体而言，我们首先提出了一种渐进式课程学习方案，使用三种从简单到复杂的课程逐渐使模型从清晰适应到相对恶劣，然后到恶劣天气场景。这鼓励模型逐渐抓住有益的深度线索对抗天气影响，产生更平滑、更好的领域适应。同时，为了防止模型忘记之前的课程，我们将对比学习整合到不同的课程中。通过从以往课程中汲取参考知识，我们的策略建立了一种深度的记忆

    arXiv:2310.05556v2 Announce Type: replace-cross  Abstract: Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. By drawing reference knowledge from the previous course, our strategy establishes a depth consi
    
[^251]: Toolink: 链接工具包创建和使用的链式解决开源模型

    Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model

    [https://arxiv.org/abs/2310.05155](https://arxiv.org/abs/2310.05155)

    提出了Toolink，一个通过链式解决方法首先创建工具包，再集成工具规划和调用的框架，成功通过对ChatGPT和CoS-GPT的实验，打造了LLaMA-CoS，一个具有先进工具规划和调用能力的强大开源模型。

    

    大语言模型（LLMs）在利用工具方面取得了显著进展，但其闭源性和高推理成本对其适应性造成了限制，需要一种有效的方法，利用较小的开源模型。在本文中，我们介绍了Toolink，这是一个全面的框架，通过链式解决（CoS）方法首先创建工具包，然后集成工具的规划和调用。我们首先验证了Toolink在利用模型的创造力和CoS能力方面的有效性。之后，我们策划了CoS-GPT，一个专为工具使用而设计的链式解决数据集，并对LLaMA-7B模型进行了微调。结果是LLaMA-CoS，一个具有先进工具规划和工具调用能力的强大开源模型。通过对BIG-bench的多样任务进行评估，表明其CoS能力与ChatGPT相匹配，而性能超过了其

    arXiv:2310.05155v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses th
    
[^252]: Knolling Bot: 从整洁的示范中学习机器人对象排列

    Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations

    [https://arxiv.org/abs/2310.04566](https://arxiv.org/abs/2310.04566)

    本论文介绍了一种自监督学习框架，利用Transformer神经网络使机器人能够从整齐排列的示范中理解和复制整洁的概念，从而实现整理物品的功能。

    

    地址：arXiv:2310.04566v2  公告类型：replace-cross  摘要：解决家庭空间中散乱物品的整理挑战受到整洁性的多样性和主观性的复杂性影响。正如人类语言的复杂性允许同一理念的多种表达一样，家庭整洁偏好和组织模式变化广泛，因此预设物体位置将限制对新物体和环境的适应性。受自然语言处理（NLP）的进展启发，本文引入一种自监督学习框架，使机器人能够从整洁布局的示范中理解和复制整洁的概念，类似于使用会话数据集训练大语言模型（LLM）。我们利用一个Transformer神经网络来预测后续物体的摆放位置。我们展示了一个“整理”系统，利用机械臂和RGB相机在桌子上组织不同大小和数量的物品。

    arXiv:2310.04566v2 Announce Type: replace-cross  Abstract: Addressing the challenge of organizing scattered items in domestic spaces is complicated by the diversity and subjective nature of tidiness. Just as the complexity of human language allows for multiple expressions of the same idea, household tidiness preferences and organizational patterns vary widely, so presetting object locations would limit the adaptability to new objects and environments. Inspired by advancements in natural language processing (NLP), this paper introduces a self-supervised learning framework that allows robots to understand and replicate the concept of tidiness from demonstrations of well-organized layouts, akin to using conversational datasets to train Large Language Models(LLM). We leverage a transformer neural network to predict the placement of subsequent objects. We demonstrate a ``knolling'' system with a robotic arm and an RGB camera to organize items of varying sizes and quantities on a table. Our 
    
[^253]: 能够检测到LLM生成的虚假信息吗?

    Can LLM-Generated Misinformation Be Detected?

    [https://arxiv.org/abs/2309.13788](https://arxiv.org/abs/2309.13788)

    LLM生成的虚假信息可能比人类撰写的虚假信息更难以检测，具有更具欺骗性的风格，可能造成更多危害。

    

    大型语言模型（LLMs）的出现产生了深远影响。然而，LLMs（如ChatGPT）可能被利用来生成虚假信息，这给在线安全和公众信任带来了严重关切。一个基本的研究问题是：LLM生成的虚假信息是否会比人类撰写的虚假信息造成更大危害?我们提出从检测难度的角度来探讨这个问题。我们首先建立了一个LLM生成的虚假信息分类法。然后，我们对利用LLMs生成虚假信息的潜在真实世界方法进行分类和验证。通过广泛的实证调查，我们发现与具有相同语义的人类撰写的虚假信息相比，LLM生成的虚假信息对人类和检测器来说更难检测，这表明它可能具有更具欺骗性的风格，潜在地造成更多危害。我们还讨论了我们发现的影响。

    arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
    
[^254]: LeBenchmark 2.0：用于自监督法表示法语语音的标准化、可复制和增强框架

    LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech

    [https://arxiv.org/abs/2309.05472](https://arxiv.org/abs/2309.05472)

    LeBenchmark 2.0是一个开源框架，用于评估和构建法语语音技术的自监督学习，提供大规模语料库、预训练模型和评估协议，并探讨了预训练SSL模型的独特视角。

    

    自监督学习（SSL）是许多不同领域，包括计算机视觉和自然语言处理等领域取得了前所未有的进展。语音处理极大受益于SSL，因为当前大部分领域相关任务现在都是用预训练模型处理的。本文介绍了LeBenchmark 2.0，这是一个用于评估和构建配备SSL的法语语音技术的开源框架。它包括有文档记录、大规模和异构语料库，涵盖长达14,000小时的异构语音，十个预训练的SSL wav2vec 2.0 模型，包含从2600万到10亿可学习参数与社区共享，并且包含由六个下游任务组成的评估协议，以补充现有基准。LeBenchmark 2.0 还对于语音的预训练SSL模型提出了独特的视角，探讨了冻结与微调下游模型以及任务不可知的相关性。

    arXiv:2309.05472v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) is at the origin of unprecedented improvements in many different domains including computer vision and natural language processing. Speech processing drastically benefitted from SSL as most of the current domain-related tasks are now being approached with pre-trained models. This work introduces LeBenchmark 2.0 an open-source framework for assessing and building SSL-equipped French speech technologies. It includes documented, large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneous speech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million to one billion learnable parameters shared with the community, and an evaluation protocol made of six downstream tasks to complement existing benchmarks. LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models for speech with the investigation of frozen versus fine-tuned downstream models, task-agnostic ve
    
[^255]: MotionGPT: 微调的LLM是通用的运动生成器

    MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators

    [https://arxiv.org/abs/2306.10900](https://arxiv.org/abs/2306.10900)

    MotionGPT是一款能够利用多模态控制信号生成连续人类动作的通用运动生成器。

    

    由于数字人类的新兴需求，从给定动作描述生成逼真的人类动作已经取得了显著进展。然而，最近的作品在直接从文本动作描述生成动作方面取得了令人印象深刻的成果，但通常只支持单一控制信号模态，这限制了它们在真实数字人类行业中的应用。本文提出了一种 Motion General-Purpose generaTor (MotionGPT)，它可以使用多模态控制信号（例如文本和单帧姿势）来通过将多模态信号视为大型语言模型（LLMs）中的特殊输入标记，生成连续的人类动作。具体来说，我们首先将多模态控制信号量化为离散码，然后将它们制定为统一的提示指令，要求LLMs生成动作答案。我们的 MotionGPT 展示了一个统一的人类动作生成模型，具有多模态支持。

    arXiv:2306.10900v2 Announce Type: replace-cross  Abstract: Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multim
    
[^256]: 用双期望分位回归的分布式强化学习

    Distributional Reinforcement Learning with Dual Expectile-Quantile Regression

    [https://arxiv.org/abs/2305.16877](https://arxiv.org/abs/2305.16877)

    提出了一种用双期望分位回归的分布式强化学习方法，能够更高效地学习任意回报分布

    

    分布式强化学习（RL）已经在多个基准测试中证明其有效性，因为它可以近似整个回报分布，并更好地利用环境样本。常用的基于不对称$L_1$损失的分布式RL的分位回归方法提供了一种灵活而有效的学习任意回报分布的方式。在实践中，通过使用更高效的混合不对称$L_1$-$L_2$ Huber损失来改进往往会提高性能。然而，通过这样做，分布估计保证消失了，我们实证观察到估计的分布会迅速收敛到其均值。事实上，与期望回归相对应的不对称$L_2$损失不能直接用于分布式时序差异学习。受到$L_2$为基础学习效率的启发，我们提出了联合学习回报分布的期望值和分位数的方法。

    arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
    
[^257]: 一切都已解决了吗？大型语言模型尚未解决的开放性自然语言处理研究问题

    Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models

    [https://arxiv.org/abs/2305.12544](https://arxiv.org/abs/2305.12544)

    本文提出了14个不由大型语言模型直接解决的自然语言处理研究领域，包含45个新的研究方向，为NLP研究人员提供了丰富的探索方向。

    

    近期大型语言模型(LLMs)的进展使得许多生成式自然语言处理应用得以部署。与此同时，这也导致了一个误导性的公众话语，“一切都已解决”。不足为奇的是，这反过来使得许多自然语言处理研究人员，特别是那些刚开始职业生涯的人，担心他们应该专注于哪些研究领域。一切都已解决了吗，或者不论大型语言模型(LLMs)如何，我们可以继续研究哪些问题？为了回答这个问题，本文整理了适合深入探究的自然语言处理研究方向。我们确定了包含45个研究方向的14个不需要大型语言模型直接解决的研究领域。虽然我们确定了许多研究领域，但还有许多其他领域存在；我们未涵盖目前由大型语言模型(LLMs)处理的领域，但在性能上落后或者专注于大型语言模型(LLM)发展的领域。我们欢迎对其他研究领域的建议。

    arXiv:2305.12544v2 Announce Type: replace-cross  Abstract: Recent progress in large language models (LLMs) has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has, in turn, made many NLP researchers -- especially those at the beginning of their careers -- worry about what NLP research area they should focus on. Has it all been solved, or what remaining questions can we work on regardless of LLMs? To address this question, this paper compiles NLP research directions rich for exploration. We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs. While we identify many research areas, many others exist; we do not cover areas currently addressed by LLMs, but where LLMs lag behind in performance or those focused on LLM development. We welcome suggestions for other research
    
[^258]: 可证明边界神经网络前像

    Provably Bounding Neural Network Preimages

    [https://arxiv.org/abs/2302.01404](https://arxiv.org/abs/2302.01404)

    提出了INVPROP算法用于验证神经网络输出集的前像上的属性，结合分支界限以增加精度，并且实现了GPU加速，避免了线性规划求解器的需求。

    

    大部分关于神经网络的形式验证工作侧重于限定给定输入集对应的输出集（例如，标准输入的有界扰动）。但是，神经网络验证的许多应用情景需要解决逆问题，或者对导致特定输出的输入集进行过度近似。我们提出了INVPROP算法，用于验证在线性约束输出集的前像上的属性，可以与分支界限结合以增加精度。与其他方法相反，我们的高效算法是GPU加速的，并且不需要线性规划求解器。我们展示了我们的算法用于通过后向可达性分析识别动态系统的安全控制区域，验证对抗鲁棒性，并检测神经网络的超出分布的输入。我们的结果表明，在某些情况下，我们找到了过渡

    arXiv:2302.01404v4 Announce Type: replace-cross  Abstract: Most work on the formal verification of neural networks has focused on bounding the set of outputs that correspond to a given set of inputs (for example, bounded perturbations of a nominal input). However, many use cases of neural network verification require solving the inverse problem, or over-approximating the set of inputs that lead to certain outputs. We present the INVPROP algorithm for verifying properties over the preimage of a linearly constrained output set, which can be combined with branch-and-bound to increase precision. Contrary to other approaches, our efficient algorithm is GPU-accelerated and does not require a linear programming solver. We demonstrate our algorithm for identifying safe control regions for a dynamical system via backward reachability analysis, verifying adversarial robustness, and detecting out-of-distribution inputs to a neural network. Our results show that in certain settings, we find over-a
    
[^259]: 使用跨域自监督深度学习的鲁棒性阿尔茨海默病进展建模

    Robust Alzheimer's Progression Modeling using Cross-Domain Self-Supervised Deep Learning

    [https://arxiv.org/abs/2211.08559](https://arxiv.org/abs/2211.08559)

    使用跨域自监督深度学习方法，以医学图像作为输入进行疾病预后建模，提高了预测阿尔茨海默病进展的准确性。

    

    发展成功的人工智能系统实践上取决于鲁棒的深度学习模型和大规模、高质量的数据。然而，在许多实际应用中获取并标记数据可能太昂贵且耗时，例如临床疾病模型。自监督学习已经展示了在小数据情况下增加模型准确性和鲁棒性的巨大潜力。本研究中，我们开发了一种用医学图像作为输入的跨域自监督学习方法，将疾病预后建模作为回归问题。我们证明了自监督预训练可以提高从医学图像中预测阿尔茨海默病进展的准确性。

    arXiv:2211.08559v2 Announce Type: cross  Abstract: Developing successful artificial intelligence systems in practice depends on both robust deep learning models and large, high-quality data. However, acquiring and labeling data can be prohibitively expensive and time-consuming in many real-world applications, such as clinical disease models. Self-supervised learning has demonstrated great potential in increasing model accuracy and robustness in small data regimes. In addition, many clinical imaging and disease modeling applications rely heavily on regression of continuous quantities. However, the applicability of self-supervised learning for these medical-imaging regression tasks has not been extensively studied. In this study, we develop a cross-domain self-supervised learning approach for disease prognostic modeling as a regression problem using medical images as input. We demonstrate that self-supervised pretraining can improve the prediction of Alzheimer's Disease progression from 
    
[^260]: 基于任务驱动特征选择的多通道成像实验设计

    Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection

    [https://arxiv.org/abs/2210.06891](https://arxiv.org/abs/2210.06891)

    提出了一种基于任务驱动特征选择的多通道成像实验设计方法，通过优化设计和训练机器学习模型执行用户指定的图像分析任务。

    

    本文提出了一种数据驱动的、任务特定的实验设计范式，旨在缩短采集时间、降低成本、加速成像设备的部署。当前实验设计方法主要集中在模型参数估计上，并要求对特定模型进行规范，而在成像领域，其他任务可能驱动设计。此外，这种方法常常在真实世界的成像应用中导致难以求解的优化问题。本文提出了一种新的实验设计范式，同时优化设计（图像通道集）并训练一个机器学习模型来执行用户指定的图像分析任务。该方法在测量空间上密集采样数据（许多图像通道）进行了少量采集，然后识别一个预先指定尺寸的最佳支持任务的通道子集。

    arXiv:2210.06891v3 Announce Type: replace-cross  Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experime
    
[^261]: 通过机器人技能学习加速实验室自动化进行样品刮取

    Accelerating Laboratory Automation Through Robot Skill Learning For Sample Scraping

    [https://arxiv.org/abs/2209.14875](https://arxiv.org/abs/2209.14875)

    实验室自动化机器人通过学习技能加速实验室自动化，特别是对于样品刮取过程的自动化实现具有重要意义

    

    实验室自动化机器人在自主实验中的使用为减轻科学家繁琐的任务提供了一条吸引人的途径，同时加快了针对气候变化和制药等热点问题的材料发现过程。尽管一些实验工作流程已经可以从自动化中受益，但由于处理不同工具、化学品和玻璃器皿时需要高水平的动作功能和灵巧性，样品制备仍然是手动进行的。化学领域的基本工作流程之一是结晶，其中一个应用是多态筛选，即从晶体中获得三维分子结构。对于这一过程，尽可能多地恢复样品至关重要，因为合成分子在时间和金钱上都是昂贵的。为了实现这一目标，化学家在成像板传送之前用刮瓶器刮除样品内容。自动化这一过程具有挑战性。

    arXiv:2209.14875v2 Announce Type: replace-cross  Abstract: The use of laboratory robotics for autonomous experiments offers an attractive route to alleviate scientists from tedious tasks while accelerating material discovery for topical issues such as climate change and pharmaceuticals. While some experimental workflows can already benefit from automation, sample preparation is still carried out manually due to the high level of motor function and dexterity required when dealing with different tools, chemicals, and glassware. A fundamental workflow in chemical fields is crystallisation, where one application is polymorph screening, i.e., obtaining a three dimensional molecular structure from a crystal. For this process, it is of utmost importance to recover as much of the sample as possible since synthesising molecules is both costly in time and money. To this aim, chemists scrape vials to retrieve sample contents prior to imaging plate transfer. Automating this process is challenging 
    
[^262]: MechProNet：金属增材制造中机器学习预测机械性能

    MechProNet: Machine Learning Prediction of Mechanical Properties in Metal Additive Manufacturing

    [https://arxiv.org/abs/2209.12605](https://arxiv.org/abs/2209.12605)

    通过机器学习方法预测金属增材制造中的机械性能，建立了一个全面的框架，包括来自90多篇文章和140个数据表的大量实验数据。

    

    预测金属增材制造（MAM）中的机械性能对于确保印制零部件的性能和可靠性以及其适用于特定应用至关重要。本研究引入了一个全面的框架，用于评估预测机械性能的机器学习模型。我们从90多篇MAM文章和来自多种来源的数据表中收集了大量实验数据，包括140个不同的MAM数据表。这些数据包括MAM处理条件、机器、材料和结果。

    arXiv:2209.12605v2 Announce Type: replace-cross  Abstract: Predicting mechanical properties in metal additive manufacturing (MAM) is essential for ensuring the performance and reliability of printed parts, as well as their suitability for specific applications. However, conducting experiments to estimate mechanical properties in MAM processes can be laborious and expensive, and they are often limited to specific materials and processes. Machine learning (ML) methods offer a more flexible and cost-effective approach to predicting mechanical properties based on processing parameters and material properties. In this study, we introduce a comprehensive framework for benchmarking ML models for predicting mechanical properties. We compiled an extensive experimental dataset from over 90 MAM articles and data sheets from a diverse range of sources, encompassing 140 different MAM data sheets. This dataset includes information on MAM processing conditions, machines, materials, and resulting mech
    
[^263]: 多行为推荐中的公平因果干预

    Causal Intervention for Fairness in Multi-behavior Recommendation

    [https://arxiv.org/abs/2209.04589](https://arxiv.org/abs/2209.04589)

    通过考虑多种用户行为来减轻流行度偏见，处理了多行为推荐中的公平问题。

    

    推荐系统通常从各种用户行为中学习用户兴趣，包括点击和点击后的行为（例如，点赞和收藏）。然而，这些行为不可避免地表现出流行度偏差，导致一些不公平问题：1）对于相似质量的物品，更受欢迎的物品会获得更多曝光；2）更糟糕的是，流行度较低的受欢迎物品可能会获得更多曝光。现有工作在减轻流行度偏差方面盲目消除偏见，通常忽略物品质量的影响。我们认为，不同用户行为之间（例如转化率）的关系实际上反映了物品质量。因此，为了处理不公平问题，我们提出通过考虑多种用户行为来减轻流行度偏见。在这项工作中，我们研究了多行为推荐中交互生成过程背后的因果关系。

    arXiv:2209.04589v2 Announce Type: replace-cross  Abstract: Recommender systems usually learn user interests from various user behaviors, including clicks and post-click behaviors (e.g., like and favorite). However, these behaviors inevitably exhibit popularity bias, leading to some unfairness issues: 1) for items with similar quality, more popular ones get more exposure; and 2) even worse the popular items with lower popularity might receive more exposure. Existing work on mitigating popularity bias blindly eliminates the bias and usually ignores the effect of item quality. We argue that the relationships between different user behaviors (e.g., conversion rate) actually reflect the item quality. Therefore, to handle the unfairness issues, we propose to mitigate the popularity bias by considering multiple user behaviors.   In this work, we examine causal relationships behind the interaction generation procedure in multi-behavior recommendation. Specifically, we find that: 1) item popula
    
[^264]: 多层蛋糕的公平分配

    Fair Division of Multi-layered Cakes

    [https://arxiv.org/abs/2208.00726](https://arxiv.org/abs/2208.00726)

    该论文介绍了多层蛋糕的公平分配问题，引入了“一对刀”计算模型，展示了针对不同代理和层数的可行比例分配计算过程，并开发了一种计算技术适用于任意数量的代理和层数。

    

    我们考虑多层蛋糕切割，旨在在两个约束条件下（连续性和可行性）在一组代理之间公平分配多个可分资源（蛋糕层）。我们首先引入了一个在多层蛋糕中的新计算模型“一对刀”。然后，我们展示了在新计算模型中为两位代理和两层存在确切的多重分配。我们演示了在三层蛋糕上为超过三个代理进行可行且连续的成比例多重分配的计算过程。最后，我们开发了一种用于计算任何数量为$n\geq 2^a3$个代理和$2^a3$层的比例分配的技术，其中$a$是任意正整数。

    arXiv:2208.00726v2 Announce Type: replace  Abstract: We consider multi-layered cake cutting in order to fairly allocate numerous divisible resources (layers of cake) among a group of agents under two constraints: contiguity and feasibility. We first introduce a new computational model in a multi-layered cake named ``a pair of knives''. Then, we show the existence of an exact multi-allocation for two agents and two layers using the new computational model. We demonstrate the computation procedure of a feasible and contiguous proportional multi-allocation over a three-layered cake for more than three agents. Finally, we develop a technique for computing proportional allocations for any number $n\geq 2^a3$ of agents and $2^a3$ layers, where $a$ is any positive integer.
    
[^265]: 通过机会性移动中继加速异步联邦学习收敛

    Accelerating Asynchronous Federated Learning Convergence via Opportunistic Mobile Relaying

    [https://arxiv.org/abs/2206.04742](https://arxiv.org/abs/2206.04742)

    通过机会性移动中继，提出了FedMobile算法，实现了异步联邦学习收敛速率为$O(\frac{1}{\sqrt{NT}})$。

    

    这篇论文探讨了移动网络环境下的异步联邦学习（FL）。大多数FL算法假设客户端和服务器之间的通信始终可用，然而，在许多实际系统中并非如此。为解决这一问题，本文探讨了移动性对异步FL收敛性能的影响。通过利用移动性，研究表明客户端可以通过另一客户端充当中继与服务器间接通信，创造额外的通信机会。这使客户端能够更早地上传本地模型更新或接收更新的全局模型。我们提出了一种新的FL算法，称为FedMobile，该算法融合了机会性中继，并解决了何时以及如何中继的关键问题。我们证明FedMobile实现了收敛速率为$O(\frac{1}{\sqrt{NT}})$，其中$N$是客户端数量，$T$是客户端数量。

    arXiv:2206.04742v2 Announce Type: replace-cross  Abstract: This paper presents a study on asynchronous Federated Learning (FL) in a mobile network setting. The majority of FL algorithms assume that communication between clients and the server is always available, however, this is not the case in many real-world systems. To address this issue, the paper explores the impact of mobility on the convergence performance of asynchronous FL. By exploiting mobility, the study shows that clients can indirectly communicate with the server through another client serving as a relay, creating additional communication opportunities. This enables clients to upload local model updates sooner or receive fresher global models. We propose a new FL algorithm, called FedMobile, that incorporates opportunistic relaying and addresses key questions such as when and how to relay. We prove that FedMobile achieves a convergence rate $O(\frac{1}{\sqrt{NT}})$, where $N$ is the number of clients and $T$ is the numbe
    
[^266]: 可解释自然语言处理的本地解释：一项调查

    Local Interpretations for Explainable Natural Language Processing: A Survey

    [https://arxiv.org/abs/2103.11072](https://arxiv.org/abs/2103.11072)

    本文调查了改善深度神经网络在自然语言处理任务中可解释性的各种方法，特别关注局部解释，包括与相关输入特征相关的预测解释、自然语言解释以及探测模型隐藏状态和词表示。

    

    随着过去十年间深度学习技术在各个领域的应用不断增长，关于黑盒模型不透明性的抱怨也在增加，导致深度学习模型透明度受到更多关注。本研究探讨了改善深度神经网络在自然语言处理（NLP）任务中的可解释性的各种方法，包括机器翻译和情感分析。我们在本研究的开始阶段对可解释性一词及其各个方面进行了全面讨论。本次调查中收集和总结的方法仅涉及局部解释，并具体分为三类：1）通过相关输入特征解释模型的预测；2）通过自然语言解释进行解释；3）探测模型的隐藏状态和单词表示。

    arXiv:2103.11072v3 Announce Type: replace-cross  Abstract: As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for Natural Language Processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are specifically divided into three categories: 1) interpreting the model's predictions through related input features; 2) interpreting through natural language explanation; 3) probing the hidden states of models and word representations.
    
[^267]: 基于注意力的强化学习在组合优化中的应用：以作业车间调度问题为例

    Attention-based Reinforcement Learning for Combinatorial Optimization: Application to Job Shop Scheduling Problem. (arXiv:2401.16580v1 [cs.AI])

    [http://arxiv.org/abs/2401.16580](http://arxiv.org/abs/2401.16580)

    本论文提出了一种基于注意力的强化学习方法，用于解决作业车间调度问题。通过集成策略梯度强化学习和修改后的Transformer结构，我们的方法在解决大规模问题上表现出色，优于最近研究和广泛采用的启发式规则。

    

    作业车间调度问题是一类重要且具有挑战性的组合优化问题，主要通过精确或近似的解决方法来解决。然而，寻找精确解对于实际问题来说是不可行的，即使采用近似解决方法，也可能需要大量的时间来找到近似最优解，并且找到的解决方案通常不能应用于新问题。为了解决这些挑战，我们提出了一种基于注意力的强化学习方法，用于解决作业车间调度问题，该方法将策略梯度强化学习与修改后的Transformer结构进行了集成。一个重要的结果是，我们在所提出的方法中训练的学习者可以用于解决未参与训练的大规模问题，并且证明我们的方法优于最近研究和广泛采用的启发式规则的结果。

    Job shop scheduling problems are one of the most important and challenging combinatorial optimization problems that have been tackled mainly by exact or approximate solution approaches. However, finding an exact solution can be infeasible for real-world problems, and even with an approximate solution approach, it can require a prohibitive amount of time to find a near-optimal solution, and the found solutions are not applicable to new problems in general. To address these challenges, we propose an attention-based reinforcement learning method for the class of job shop scheduling problems by integrating policy gradient reinforcement learning with a modified transformer architecture. An important result is that our trained learners in the proposed method can be reused to solve large-scale problems not used in training and demonstrate that our approach outperforms the results of recent studies and widely adopted heuristic rules.
    
[^268]: 多模态路径：通过其他模态的无关数据来改进Transformer

    Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])

    [http://arxiv.org/abs/2401.14405](http://arxiv.org/abs/2401.14405)

    本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。

    

    我们提出使用来自其他模态的无关数据来改进特定模态的Transformer，例如，使用音频或点云数据集来改进ImageNet模型。我们强调目标模态的数据样本与其他模态无关，这与利用不同模态的配对数据（如CLIP）或交错数据的其他方法不同。我们提出了一种名为多模态路径的方法-给定目标模态和设计用于该模态的Transformer，我们使用使用另一个模态的数据训练的辅助Transformer，并构建路径来连接两个模型的组件，以便目标模态的数据可以被两个模型处理。通过这种方式，我们利用了从两个模态获得的Transformer的通用序列建模能力。作为具体实现，我们通常使用特定模态的tokenizer和任务特定的head，但是利用辅助模型的Transformer block。

    We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
    
[^269]: 基于稀疏网格的不连续性检测的图信息神经网络

    Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])

    [http://arxiv.org/abs/2401.13652](http://arxiv.org/abs/2401.13652)

    本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。

    

    本文提出了一种新颖的方法来检测不连续函数的不连续界面。该方法利用了基于图的神经网络（GINNs）和稀疏网格来解决维度大于3的情况下的不连续性检测。训练过的GINNs在稀疏网格上识别有问题的点，并利用构建在网格上的图结构实现高效准确的不连续性检测性能。我们还引入了一种递归算法用于一般的基于稀疏网格的检测器，具有收敛性和易于应用性。在维度n=2和n=4的函数上进行的数值实验证明了GINNs在检测不连续界面方面的高效性和鲁棒泛化能力。值得注意的是，经过训练的GINNs具有可移植性和多功能性，可以集成到各种算法中并共享给用户。

    In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
    
[^270]: 通过人的反馈改善机器翻译: 将质量估计作为奖励模型的探索

    Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model. (arXiv:2401.12873v1 [cs.CL])

    [http://arxiv.org/abs/2401.12873](http://arxiv.org/abs/2401.12873)

    本研究探索了利用质量估计作为奖励模型来预测人类偏好以改善机器翻译的潜力。我们发现基于质量估计的反馈训练存在过度优化问题，采用启发式规则来检测错误翻译并对质量估计模型进行惩罚以解决该问题。

    

    不充分建模人类偏好导致奖励模型在利用人的反馈提高翻译质量方面成为一个主要障碍。幸运的是，质量估计(QE)在过去两年中无需参考文献就能准确预测给定翻译的质量。在这项工作中，我们探讨了将QE模型作为奖励模型(基于QE的奖励模型)来预测人的偏好以进行反馈训练的潜力。我们首先发现了在基于QE的反馈训练中的过度优化问题，表现为奖励的增加而翻译质量下降。我们研究了这个问题，并认为QE模型的脆弱性可能导致错误翻译的高奖励，从而导致过度优化和错误传播。为解决这个问题，我们采用了一种简单而有效的方法，使用启发式规则检测错误翻译，并为QE模型添加了一个惩罚项。

    Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the Q
    
[^271]: 通过谈判评估语言模型的代理能力

    Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])

    [http://arxiv.org/abs/2401.04536](http://arxiv.org/abs/2401.04536)

    本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。

    

    公司、组织和政府越来越多地利用语言模型（LM）展示类似代理行为的出色能力。随着LM被采用来执行越来越具有自主性的任务，迫切需要可靠且可扩展的评估基准。当前主要是静态的LM基准无法很好地评估此类动态应用。因此，我们提议通过谈判游戏的视角来共同评估LM的性能和对齐。我们认为这个共同任务更好地反映了真实世界的部署条件，并提供了关于LM决策过程的见解。至关重要的是，谈判游戏使我们能够研究多轮次和跨模型交互，调整复杂性，并避免评估中的意外数据泄漏。我们报告了来自几个主要供应商的六个公开可访问的LM在各种谈判游戏上的结果，评估了自我对弈和交叉对弈性能。值得注意的发现包括：（i）开源模式

    Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
    
[^272]: 训练的力量：不同的神经网络设置对能源需求的影响

    The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])

    [http://arxiv.org/abs/2401.01851](http://arxiv.org/abs/2401.01851)

    本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。

    

    本研究探讨机器学习训练方案和学习范式的变化对相应能源消耗的影响。虽然数据的可用性提高和高性能硬件的创新推动了复杂模型的训练，但也支持了能源消耗和碳排放的消隐。因此，本研究的目标是增加人们对一般训练参数和过程（从学习率到批量大小再到知识传输）的能源影响的认识。使用不同的超参数初始化在两种不同的硬件配置上评估多种设置，以获得有意义的结果。在基准结果上进行了预训练和多任务训练实验，以确定它们对可持续机器学习的潜力。

    This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
    
[^273]: 探索LLMs在心理学应用中的前沿：一份综述

    Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])

    [http://arxiv.org/abs/2401.01519](http://arxiv.org/abs/2401.01519)

    本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。

    

    本文探索了大型语言模型（LLMs）在心理学应用中的前沿。心理学经历了几次理论变革，当前人工智能（AI）和机器学习，特别是LLMs的使用有望开启新的研究方向。我们详细探讨了LLMs如ChatGPT在心理学研究中的转变。文章讨论了LLMs在认知与行为心理学、临床与咨询心理学、教育与发展心理学以及社会与文化心理学等心理学分支中的影响，强调了它们模拟人类认知和行为方面的潜力。本文深入探讨了这些模型模拟人类文本生成的能力，为心理学中的文献回顾、假设生成、实验设计、实验对象、数据分析、学术写作和同行评审等提供创新工具。虽然LLMs在推动研究方法学方面起着重要作用，

    This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
    
[^274]: SVGDreamer：基于文本引导的SVG生成与扩散模型

    SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.16476](http://arxiv.org/abs/2312.16476)

    该论文提出了一种名为SVGDreamer的方法，用于基于文本引导的SVG生成。它通过引入语义驱动的图像矢量化过程和基于注意力的元素控制，增强了生成结果的可编辑性和质量。同时，采用基于矢量化粒子分数蒸馏的方法解决了现有方法中的颜色、平滑度和结果多样性方面的挑战。

    

    最近，基于文本引导的可缩放矢量图形（SVG）合成在图标设计和草图等领域展示出了潜力。然而，现有的文本到SVG生成方法存在缺乏可编辑性、视觉质量和结果多样性不足的问题。为了解决这些限制，我们提出了一种新颖的基于文本引导的矢量图形合成方法，称为SVGDreamer。SVGDreamer整合了一种语义驱动的图像矢量化（SIVE）过程，可以将合成过程分解为前景对象和背景，从而增强了可编辑性。具体而言，SIVE过程引入了基于注意力的基本元素控制和注意力掩蔽损失函数，以有效控制和操作各个元素。此外，我们提出了一种基于矢量化粒子分数蒸馏（VPSD）的方法，以解决现有文本到SVG生成方法中颜色过饱和、矢量基元过平滑和结果多样性有限的挑战。

    Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduce attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to tackle the challenges of color over-saturation, vector primitives over-smoothing, and limited result diversity in existing text-to-SVG generation methods. Furthermore, on the basis of 
    
[^275]: 超越梯度和先验知识在隐私攻击中：利用联邦学习中语言模型的池化层输入

    Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05720](http://arxiv.org/abs/2312.05720)

    本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。

    

    联邦学习强调分散式训练，通过本地存储数据并仅发送模型更新，强调用户隐私。最近，一系列有关隐私攻击的工作通过从联邦学习上下文的语言模型中提取敏感的训练文本来损害用户隐私。然而，这些攻击技术面临着不同的障碍：一些工作主要使用有限的批处理大小（例如，批处理大小为1），而其他技术则容易被检测出来。本文介绍了一种创新的方法，具有难以检测的特点，在不同的批处理大小设置下显著提高了文本恢复率。基于基本的梯度匹配和领域先验知识，我们通过恢复语言模型的池化层输入来增强攻击能力，这使我们能够在特征级别提供额外的监督信号。与梯度数据不同，这些信号不会在句子和标记之间进行平均，从而提供更细致和有效的见解。

    Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
    
[^276]: 《低秩适应的表达能力》

    The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])

    [http://arxiv.org/abs/2310.17513](http://arxiv.org/abs/2310.17513)

    本文分析了低秩适应（LoRA）的表达能力，证明了对于全连接神经网络，当LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度）时，LoRA可以使任何模型f准确表示任何较小的目标模型f。对于Transformer网络，通过rank-（嵌入大小/ 2）的LoRA适配器可以使任何模型适应于相同大小的目标模型。

    

    低秩适应（LoRA）是一种参数高效的微调方法，利用矩阵的低秩适应性，在微调预训练模型（如大型语言模型和扩散模型）中得到了广泛应用。尽管在实践中取得了巨大成功，但是LoRA的理论基础在很大程度上尚未得到探索。本文通过从理论角度分析LoRA的表达能力，首次尝试弥合这一差距。我们证明了对于全连接神经网络，如果LoRA-rank≥（f的宽度）×（目标模型的深度/ f的深度），则LoRA可以使任何模型f准确表示任何较小的目标模型f。当LoRA-rank低于阈值时，我们还量化了逼近误差。对于Transformer网络，我们证明任何模型可以通过rank-（嵌入大小/ 2）的LoRA适配器适应于相同大小的目标模型。

    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
    
[^277]: 增强MRI扫描隐私的3D遮罩自编码器

    3D Masked Autoencoders for Enhanced Privacy in MRI Scans. (arXiv:2310.15778v1 [cs.CV])

    [http://arxiv.org/abs/2310.15778](http://arxiv.org/abs/2310.15778)

    本研究提出了一种名为CP-MAE的模型，通过使用面部遮罩来实现MRI扫描中的人脸去识别，提高了隐私保护水平。

    

    MRI扫描提供有价值的医学信息，但也包含敏感和可识别个人信息（PII），需要保护。传统的MRI数据去识别方法通过删除隐私敏感部位（如眼睛、鼻子等）来实现，但会引入领域转换，影响下游分析。本文提出了CP-MAE模型，通过面部遮罩来实现人脸去识别。

    MRI scans provide valuable medical information, however they also contain sensitive and personally identifiable information (PII) that needs to be protected. Whereas MRI metadata is easily sanitized, MRI image data is a privacy risk because it contains information to render highly-realistic 3D visualizations of a patient's head, enabling malicious actors to possibly identify the subject by cross-referencing a database. Data anonymization and de-identification is concerned with ensuring the privacy and confidentiality of individuals' personal information. Traditional MRI de-identification methods remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This comes at the expense of introducing a domain shift that can throw off downstream analyses. Recently, a GAN-based approach was proposed to de-identify a patient's scan by remodeling it (e.g. changing the face) rather than by removing parts. In this work, we propose CP-MAE, a model that de-identifies the face using mask
    
[^278]: 内存一致的神经网络在模仿学习中的应用

    Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])

    [http://arxiv.org/abs/2310.06171](http://arxiv.org/abs/2310.06171)

    本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。

    

    模仿学习利用专家演示大大简化了策略合成的过程。然而，对于这种模仿策略来说，远离训练样本的错误尤为关键。即使在策略的行动输出中出现罕见的错误，由于这些错误会导致不熟悉的未来状态，策略在这些状态下仍更容易出错，最终导致任务失败。本文重新审视了简单的监督式“行为克隆”方法，能够方便地仅通过预先记录的演示来训练策略，并设计了一种能够抵消错误累积现象的模型类。我们的“内存一致神经网络”(MCNN)输出被强制约束在与典型的“内存”训练样本相关的明确指定的允许区域内。我们提供了MCNN策略导致的次优性差距的保证上界。通过在9个模仿学习任务上使用MCNNs，采用MLP、Transformer等方法。

    Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
    
[^279]: MiniGPT-5: 通过生成凭据实现交错的视觉与语言生成

    MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. (arXiv:2310.02239v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.02239](http://arxiv.org/abs/2310.02239)

    MiniGPT-5使用生成凭据作为桥梁，引入了一种创新的交错视觉与语言生成技术，并通过独特的两阶段训练策略和无分类器的指导来实现无描述的多模态生成。

    

    大型语言模型（LLMs）因其在自然语言处理方面的进展而引起了广泛关注，展示了在文本理解和生成方面无与伦比的能力。然而，同时生成具有连贯文本叙述的图像仍然是一个不断发展的前沿。为此，我们引入了一种创新的交错视觉与语言生成技术，以"生成凭据"的概念为基础，作为协调图像文本输出的桥梁。我们的方法特点是独特的两阶段训练策略，重点是无描述的多模态生成，训练过程不需要对图像进行全面的描述。为了增强模型的完整性，我们还引入了无分类器的指导，增强了生成凭据在图像生成方面的效果。我们的模型MiniGPT-5在MMDialog数据集上相比基线Divter模型有显著改进，并始终提供优越或可比的多模态输出。

    Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation. Yet, the simultaneous generation of images with coherent textual narratives remains an evolving frontier. In response, we introduce an innovative interleaved vision-and-language generation technique anchored by the concept of "generative vokens," acting as the bridge for harmonized image-text outputs. Our approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. Our model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs 
    
[^280]: L2MAC：大规模语言模型自动计算机用于无限代码生成

    L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])

    [http://arxiv.org/abs/2310.02003](http://arxiv.org/abs/2310.02003)

    L2MAC是一种基于LLM的存储程序自动计算机，可以用于生成长且逻辑一致的代码。

    

    基于Transformer的大型语言模型（LLM）受到底层Transformer架构固定上下文窗口的限制，阻碍了它们生成长且逻辑一致的代码的能力。增强记忆的LLM是一个有前途的解决方案，但目前的方法无法处理长时间的代码生成任务，因为它们要么只关注于读取内存并将其演变为新内存的连接，要么使用非常专门的内存，无法适应其他领域。本文介绍了L2MAC，这是一种基于LLM的长且一致代码生成的实用存储程序自动计算机。它的内存有两个组成部分：指令注册表，其中填充了一个解决用户给定任务的提示程序，以及文件存储，其中包含最终和中间输出。每个指令由单独的LLM实例执行，其上下文由控制单元管理，能够精确读取和写入内存，以确保有效的整合。

    Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
    
[^281]: SmartPlay: 一种用于评估LLMs作为智能Agent能力的基准

    SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])

    [http://arxiv.org/abs/2310.01557](http://arxiv.org/abs/2310.01557)

    SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。

    

    最近的大型语言模型(LLMs)在智能Agent和下一代自动化方面展示了巨大的潜力，但目前缺乏一个系统化的基准来评估LLMs作为Agent的能力。我们介绍了SmartPlay：一个具有挑战性的基准和评估LLMs作为Agent的方法论。SmartPlay包括6个不同的游戏，包括剪刀石头布、汉诺塔、Minecraft等。每个游戏都具有独特的设置，提供最多20个评估设置和无限的环境变化。SmartPlay中的每个游戏都独特地挑战了智能LLM Agent的9个重要能力的子集，包括对对象依赖的推理、提前规划、空间推理、从历史中学习和理解随机性。每个游戏测试的能力集的区别使我们能够单独分析每个能力。SmartPlay不仅是评估LLM Agent整体性能的严格测试场地，而且也是评估Agent在不同能力方面的性能的一个重要工具。

    Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
    
[^282]: ETGraph：一个连接以太坊和Twitter的开创性数据集

    ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter. (arXiv:2310.01015v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2310.01015](http://arxiv.org/abs/2310.01015)

    ETGraph是一个连接以太坊和Twitter的开创性数据集，结合了以太坊交易记录和Twitter关注数据，通过验证OpenSea的Twitter账户与以太坊地址进行绑定，对其进行了详细统计分析，揭示了Twitter匹配和非Twitter匹配的以太坊地址之间的结构差异。

    

    尽管有大量的公共区块链数据集可用，但它们的效用受限于对区块链数据的单一关注。这种限制限制了相关社交网络数据与区块链分析的结合，从而减少了可以得出的广度和深度洞见。为了解决以上限制，我们介绍了ETGraph，它是一个全新的数据集，真实地连接了以太坊和Twitter，是其类别中第一个且最大的数据集。ETGraph结合了以太坊的交易记录（200万个节点和3000万条边）和Twitter的关注数据（100万个节点和300万条边），将30667个以太坊地址与来自OpenSea的已验证Twitter账户进行了绑定。对ETGraph的详细统计分析突出了与Twitter匹配和非Twitter匹配的以太坊地址之间的结构差异。包括以太坊链路预测、虚假交易以太坊地址检测和Twitter-以太坊匹配链路预测在内的大量实验

    While numerous public blockchain datasets are available, their utility is constrained by a singular focus on blockchain data. This constraint limits the incorporation of relevant social network data into blockchain analysis, thereby diminishing the breadth and depth of insight that can be derived. To address the above limitation, we introduce ETGraph, a novel dataset that authentically links Ethereum and Twitter, marking the first and largest dataset of its kind. ETGraph combines Ethereum transaction records (2 million nodes and 30 million edges) and Twitter following data (1 million nodes and 3 million edges), bonding 30,667 Ethereum addresses with verified Twitter accounts sourced from OpenSea. Detailed statistical analysis on ETGraph highlights the structural differences between Twitter-matched and non-Twitter-matched Ethereum addresses. Extensive experiments, including Ethereum link prediction, wash-trading Ethereum addresses detection, and Twitter-Ethereum matching link prediction
    
[^283]: CCA家族的高效算法：无约束目标与无偏梯度

    Efficient Algorithms for the CCA Family: Unconstrained Objectives with Unbiased Gradients. (arXiv:2310.01012v1 [cs.LG])

    [http://arxiv.org/abs/2310.01012](http://arxiv.org/abs/2310.01012)

    本论文提出了一个新颖的无约束目标，通过应用随机梯度下降（SGD）到CCA目标，实现了一系列快速算法，包括随机PLS、随机CCA和深度CCA。这些方法在各种基准测试中表现出比先前最先进方法更快的收敛速度和更高的相关性恢复。

    

    典型相关分析（CCA）方法在多视角学习中具有基础性作用。正则化线性CCA方法可以看作是偏最小二乘（PLS）的推广，并与广义特征值问题（GEP）框架统一。然而，这些线性方法的传统算法在大规模数据上计算上是不可行的。深度CCA的扩展显示出很大的潜力，但目前的训练过程缓慢且复杂。我们首先提出了一个描述GEPs的顶级子空间的新颖无约束目标。我们的核心贡献是一系列快速算法，用随机梯度下降（SGD）应用于相应的CCA目标，从而获得随机PLS、随机CCA和深度CCA。这些方法在所有标准CCA和深度CCA基准测试中显示出比先前最先进方法更快的收敛速度和更高的相关性恢复。这样的速度使我们能够首次进行大规模生物数据的PLS分析。

    The Canonical Correlation Analysis (CCA) family of methods is foundational in multi-view learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data. Extensions to Deep CCA show great promise, but current training procedures are slow and complicated. First we propose a novel unconstrained objective that characterizes the top subspace of GEPs. Our core contribution is a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives. These methods show far faster convergence and recover higher correlations than the previous state-of-the-art on all standard CCA and Deep CCA benchmarks. This speed allows us to perform a first-of-its-kind PLS analysis of an extremely large bio
    
[^284]: 在规划中结合空间和时间抽象以实现更好的泛化

    Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00229](http://arxiv.org/abs/2310.00229)

    Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。

    

    受到人类有意识规划的启发，我们提出了Skipper，这是一个利用时空抽象来推广在新情境中学到的技能的基于模型的强化学习代理。它自动将给定任务分解为更小、更可管理的子任务，从而实现稀疏决策和对环境相关部分的专注计算。这依赖于从回溯中学习得到的表示为有向图的抽象代理问题的提取。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法在哪些方面有望提供帮助。针对泛化的实验验证了Skipper在零样本泛化方面与现有最先进的分层规划方法相比的显著优势。

    Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
    
[^285]: 自适应驾驶中基于领域匹配的协同感知的通信

    Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving. (arXiv:2310.00013v1 [cs.AI])

    [http://arxiv.org/abs/2310.00013](http://arxiv.org/abs/2310.00013)

    这篇论文提出了一个通信的协同感知框架ACC-DA，通过动态调整通信图和自适应数据重构机制来增强自动驾驶中的感知能力。

    

    通过通信允许车辆交换补充信息，多个连接的自动驾驶车辆之间的协同感知可以极大地增强感知能力。尽管之前的方法取得了进展，但由于通道变化和协同车辆之间的数据异构性，仍然存在挑战。为了解决这些问题，我们提出了ACC-DA，一个通道感知的协同感知框架，它可以动态调整通信图并最小化平均传输延迟，同时减轻数据异构性带来的副作用。我们的创新点包括三个方面。首先，我们设计了一种最小化传输延迟的方法，根据不同的通道信息状态构建通信图并最小化传输延迟。然后，我们提出了一种自适应数据重构机制，可以动态调整码率-畸变折衷以增强感知效率。此外，它最小化了时域丢失。

    Collaborative perception among multiple connected and autonomous vehicles can greatly enhance perceptive capabilities by allowing vehicles to exchange supplementary information via communications. Despite advances in previous approaches, challenges still remain due to channel variations and data heterogeneity among collaborative vehicles. To address these issues, we propose ACC-DA, a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize the average transmission delay while mitigating the side effects from the data heterogeneity. Our novelties lie in three aspects. We first design a transmission delay minimization method, which can construct the communication graph and minimize the transmission delay according to different channel information state. We then propose an adaptive data reconstruction mechanism, which can dynamically adjust the rate-distortion trade-off to enhance perception efficiency. Moreover, it minimizes the temporal
    
[^286]: MVMR: 在多个可靠视频集中评估自然语言视频定位偏差

    MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])

    [http://arxiv.org/abs/2309.16701](http://arxiv.org/abs/2309.16701)

    本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    

    随着近年来多媒体内容的激增，自然语言视频定位成为一个关键问题，它致力于检测与给定自然语言查询匹配的视频片段。然而，以往的研究都没有探索在存在多个正负视频的大量语料库中定位一个时刻。本文提出了一个名为MVMR（Massive Videos Moment Retrieval）的任务，旨在给定文本查询从大量视频集中定位视频帧。对于这个任务，我们提出了一种通过对现有视频定位数据集进行相似性筛选来构建数据集的方法，并引入了三个MVMR数据集。具体来说，我们采用基于嵌入的文本相似度匹配和视频-语言对齐技术来计算目标查询与视频之间的相关性得分，从而定义正负集。针对提出的MVMR任务，我们进一步开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
    
[^287]: 从复杂到清晰：通过Clifford的几何代数和凸优化的分析表达深度神经网络的权重

    From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])

    [http://arxiv.org/abs/2309.16512](http://arxiv.org/abs/2309.16512)

    本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。

    

    本文介绍了一种基于几何（Clifford）代数和凸优化的神经网络分析方法。我们展示了当使用标准正则化损失进行训练时，深度ReLU神经网络的最优权重由训练样本的楔积给出。此外，训练问题可简化为对楔积特征进行凸优化，在其中编码训练数据集的几何结构。该结构以数据向量生成的三角形和平行体的有符号体积表示。凸问题通过$\ell_1$正则化找到样本的一个小子集，以发现仅相关的楔积特征。我们的分析提供了对深度神经网络内部工作机制的新视角，并揭示了隐藏层的作用。

    In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
    
[^288]: 我们目前处于哪个阶段？从人工智能协作的角度理解数据叙事工具

    Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration. (arXiv:2309.15723v1 [cs.HC])

    [http://arxiv.org/abs/2309.15723](http://arxiv.org/abs/2309.15723)

    本文通过研究现有数据叙事工具，从人工智能协作的角度出发，总结了关于不同阶段和角色的共同协作模式，以促进人类和人工智能的优势并减少不足。

    

    数据叙事在传达数据洞察力方面非常强大，但它需要人类创作者具备多样化的技能和相当大的工作量。最近的研究广泛探讨了人工智能支持和增强人类在数据叙事中的作用潜力。然而，缺乏一个系统性的综述来理解从人工智能协作的角度来看数据叙事工具，这阻碍了研究人员对现有的协作工具设计的思考，以促进人类和人工智能的优势并减少他们的不足。本文采用一个框架来研究现有工具，从两个角度来看：其服务于叙事工作流程的阶段，包括分析、规划、实施和沟通；以及在每个阶段人类和人工智能的角色，如创作者、助手、优化器和评审员。通过我们的分析，我们认识到现有工具中的共同协作模式，并总结了从这些模式中得到的经验教训。

    Data storytelling is powerful for communicating data insights, but it requires diverse skills and considerable effort from human creators. Recent research has widely explored the potential for artificial intelligence (AI) to support and augment humans in data storytelling. However, there lacks a systematic review to understand data storytelling tools from the perspective of human-AI collaboration, which hinders researchers from reflecting on the existing collaborative tool designs that promote humans' and AI's advantages and mitigate their shortcomings. This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers. Through our analysis, we recognize the common collaboration patterns in existing tools, summarize lessons learned from these pattern
    
[^289]: 大型语言模型和控制机制提高了生物医学摘要的文本可读性

    Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])

    [http://arxiv.org/abs/2309.13202](http://arxiv.org/abs/2309.13202)

    本研究使用大型语言模型和控制机制改善了生物医学摘要的文本可读性，具体包括领域微调和基于提示的学习方法，以及应用于编码器-解码器模型和GPT模型的控制令牌机制。

    

    生物医学文献通常使用复杂的语言和难以理解的专业术语。因此，简化在提高公共健康素养方面起着重要作用。将自然语言处理（NLP）模型应用于自动化此类任务可以使非专业读者快速直接地获取信息。在本研究中，我们使用公开可用的用于生物医学摘要简化的数据集（PLABA）来调查最先进大型语言模型（LLMs）在生物医学摘要简化任务上的能力。应用的方法包括领域微调和基于提示的学习（PBL）在：1）编码器-解码器模型（T5、SciFive和BART）上，2）仅解码器的GPT模型（GPT-3.5和GPT-4）来自OpenAI和BioGPT，以及3）基于控制令牌机制的基于BART的模型。我们使用了一系列自动评估指标，包括BLEU、ROUGE、SARI和BERTscore，并进行了人工评估。

    Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m
    
[^290]: Choice-75：一个关于脚本学习中决策分支的数据集

    Choice-75: A Dataset on Decision Branching in Script Learning. (arXiv:2309.11737v1 [cs.AI])

    [http://arxiv.org/abs/2309.11737](http://arxiv.org/abs/2309.11737)

    Choice-75是一个关于脚本学习中决策分支的数据集，主要挑战智能系统在描述场景下预测决策。在许多难题中仍有改进的空间。

    

    脚本学习研究日常事件的展开方式。以往的研究往往将脚本视为线性的事件序列，忽略了人们在特定情境中所做出的选择可能带来的分支。因此，我们提出了Choice-75，这是第一个挑战智能系统根据描述场景预测决策的基准，包含75个脚本和600多个场景。尽管大型语言模型在整体上表现不错，但在许多困难的场景中仍有明显的改进空间。

    Script learning studies how daily events unfold. Previous works tend to consider a script as a linear sequence of events while ignoring the potential branches that arise due to people's circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to predict decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. While large language models demonstrate overall decent performances, there is still notable room for improvement in many hard scenarios.
    
[^291]: 浅层神经网络的几何结构和基于${\mathcal L}^2$代价最小化的构造方法

    Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])

    [http://arxiv.org/abs/2309.10370](http://arxiv.org/abs/2309.10370)

    本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。

    

    本文给出了一个几何解释：浅层神经网络的结构由一个隐藏层、一个斜坡激活函数、一个${\mathcal L}^2$谱范类（或者Hilbert-Schmidt）的代价函数、输入空间${\mathbb R}^M$、输出空间${\mathbb R}^Q$（其中$Q\leq M$），以及训练输入样本数量$N>QM$所特征。我们证明了代价函数的最小值具有$O(\delta_P)$的上界，其中$\delta_P$衡量了训练输入的信噪比。我们使用适应于属于同一输出向量$y_j$的训练输入向量$\overline{x_{0,j}}$的投影来获得近似的优化器，其中$j=1,\dots,Q$。在特殊情况$M=Q$下，我们明确确定了代价函数的一个确切退化局部最小值；这个尖锐的值与对于$Q\leq M$所获得的上界之间有一个相对误差$O(\delta_P^2)$。上界证明的方法提供了一个构造性训练的网络；我们证明它测度了$Q$维空间中的给定输出。

    In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
    
[^292]: 评估目标检测中的不确定性校准的理论和实践框架

    A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection. (arXiv:2309.00464v1 [cs.CV])

    [http://arxiv.org/abs/2309.00464](http://arxiv.org/abs/2309.00464)

    本文提出了一种评估目标检测中不确定性校准的理论和实践框架，并通过实验证明了所提出指标的有效性。

    

    深度神经网络的普及导致机器学习系统在各种实际应用中越来越常见。因此，在考虑深度学习的未来时，不确定性校准问题成为至关重要的问题，尤其是在考虑到在自动驾驶和机器人等安全关键应用中常见的目标检测系统。基于此，本研究提出了一种新颖的理论和实践框架，以评估目标检测系统的不确定性校准。通过一系列代表性实验展示了所提出不确定性校准指标的鲁棒性。所提出不确定性校准指标的代码可在以下链接找到：https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection。

    The proliferation of Deep Neural Networks has resulted in machine learning systems becoming increasingly more present in various real-world applications. Consequently, there is a growing demand for highly reliable models in these domains, making the problem of uncertainty calibration pivotal, when considering the future of deep learning. This is especially true when considering object detection systems, that are commonly present in safety-critical application such as autonomous driving and robotics. For this reason, this work presents a novel theoretical and practical framework to evaluate object detection systems in the context of uncertainty calibration. The robustness of the proposed uncertainty calibration metrics is shown through a series of representative experiments. Code for the proposed uncertainty calibration metrics at: https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection.
    
[^293]: DCNFIS：深度卷积神经模糊推理系统

    DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])

    [http://arxiv.org/abs/2308.06378](http://arxiv.org/abs/2308.06378)

    本文介绍了一种新的深度卷积神经模糊推理系统（DCNFIS），它通过将模糊逻辑和深度学习模型相结合，实现了提高透明度而不损失准确性的目标。DCNFIS在准确性上与现有卷积神经网络相当，并且胜过了最先进的深度模糊系统。通过模糊规则提取的解释可以提高模型的可解释性。

    

    在可解释的人工智能中，透明度与准确性之间存在一个著名的权衡。本文介绍了一种新的深度网络设计，通过将模糊逻辑和深度学习模型相结合，实现了提高透明度但不损失准确性的目标。我们设计了一个深度卷积神经模糊推理系统（DCNFIS），并在四个著名数据集上展示了它与三个现有卷积神经网络的相同准确性。我们进一步发现，DCNFIS在性能上胜过了最先进的深度模糊系统。然后，我们利用模糊逻辑的透明度，从DCNFIS中编码的模糊规则中提取解释，以渐变映射的形式展示。我们还利用Fashion-MNIST数据集对这些解释的特性进行了深入研究。

    A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
    
[^294]: 更多上下文，更少干扰：通过推断和调节上下文属性进行视觉分类

    More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])

    [http://arxiv.org/abs/2308.01313](http://arxiv.org/abs/2308.01313)

    本文借鉴了人类视觉感知过程，提出了一种通过推断和调节上下文属性来改进零样本图像分类的方法。通过给CLIP提供上下文属性，可以减轻对虚假特征的依赖，进而提高零样本分类的准确性。

    

    CLIP作为一种基础的视觉语言模型，由于其理解各种视觉概念和自然语言描述的能力，被广泛应用于零样本图像分类。然而，如何充分利用CLIP的前所未有的人类般理解能力来实现更好的零样本分类仍然是一个开放问题。本文从人类的视觉感知过程中得到启发：现代神经科学观点认为，在对物体进行分类时，人类首先推断其与类别无关的属性（如背景和方向），这有助于将前景对象与背景区分开来，然后以此信息为基础进行决策。受此启发，我们观察到为CLIP提供上下文属性可以改善零样本分类并减轻对虚假特征的依赖。我们还观察到CLIP本身可以合理地从图像中推断出这些属性。基于这些观察，我们提出了一种零训练、两步骤的零样本分类方法。

    CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
    
[^295]: RLCD: 基于对比蒸馏的强化学习用于语言模型对齐

    RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12950](http://arxiv.org/abs/2307.12950)

    RLCD是一种用于语言模型对齐的强化学习方法，利用对比蒸馏训练偏好模型，可以使语言模型在不使用人类反馈的情况下遵循自然语言规则。在多个对齐任务和不同规模的模型上，RLCD优于其他基线方法。

    

    我们提出了一种称为Reinforcement Learning from Contrast Distillation (RLCD)的方法，用于无需使用人类反馈即可使语言模型遵循自然语言规则的对齐。RLCD使用模拟的偏好对进行训练，这些对包含了高质量和低质量的示例，其中使用对比的正负提示生成。然后，使用偏好模型通过强化学习来改进基础的无对齐语言模型。在实证上，RLCD在三个不同的对齐任务（无害性、有用性和故事大纲生成）以及7B和30B模型规模的偏好数据模拟上，都优于RLAIF (Bai等人，2022b)和上下文蒸馏 (Huang等人，2022) 的基准方法。

    We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
    
[^296]: 我们的模型在MovieLens上取得了出色的表现：这意味着什么？

    Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])

    [http://arxiv.org/abs/2307.09985](http://arxiv.org/abs/2307.09985)

    该论文通过对MovieLens数据集的分析，发现用户与该平台的交互在不同阶段存在显著差异，并且用户交互受到平台推荐算法推荐的候选电影的影响。

    

    推荐系统评估的典型基准数据集是在某一时间段内在平台上生成的用户-物品交互数据。交互生成机制部分解释了为什么用户与物品进行交互（如喜欢、购买、评分）以及特定交互发生的背景。在本研究中，我们对MovieLens数据集进行了细致的分析，并解释了使用该数据集进行评估推荐算法时可能的影响。我们从分析中得出了一些主要发现。首先，在用户与MovieLens平台交互的不同阶段存在显著差异。早期交互在很大程度上定义了用户画像，影响了后续的交互。其次，用户交互受到平台内部推荐算法推荐的候选电影的很大影响。删除靠近最后几次交互的交互会对结果产生较大影响。

    A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions 
    
[^297]: 神经架构检索

    Neural Architecture Retrieval. (arXiv:2307.07919v1 [cs.AI])

    [http://arxiv.org/abs/2307.07919](http://arxiv.org/abs/2307.07919)

    该论文提出了一个新的问题——神经架构检索，主要解决了研究人员在发现相似神经架构时所遇到的困难，并引入了多层对比学习来实现准确的图表示学习。

    

    随着新型神经架构设计的增加和现有神经架构的大量存在，研究人员很难将自己的贡献与现有的神经架构相比较，或者建立自己的设计与其他相关设计之间的联系。为了以高效且自动的方式发现相似的神经架构，我们定义了一个新的问题——神经架构检索，它检索一组与查询神经架构具有相似设计的现有神经架构。由于神经架构中的图的大小和模式，现有的图预训练策略不能解决计算图问题。为了充分发挥潜力，我们提出将图分成模式，并将其用于重建宏图来解决这些问题，并引入多层对比学习来实现准确的图表示学习。对人工设计和合成神经架构进行了广泛评估。

    With the increasing number of new neural architecture designs and substantial existing neural architectures, it becomes difficult for the researchers to situate their contributions compared with existing neural architectures or establish the connections between their designs and other relevant ones. To discover similar neural architectures in an efficient and automatic manner, we define a new problem Neural Architecture Retrieval which retrieves a set of existing neural architectures which have similar designs to the query neural architecture. Existing graph pre-training strategies cannot address the computational graph in neural architectures due to the graph size and motifs. To fulfill this potential, we propose to divide the graph into motifs which are used to rebuild the macro graph to tackle these issues, and introduce multi-level contrastive learning to achieve accurate graph representation learning. Extensive evaluations on both human-designed and synthesized neural architecture
    
[^298]: 在大型语言模型中的上下文压缩的上下文自编码器

    In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])

    [http://arxiv.org/abs/2307.06945](http://arxiv.org/abs/2307.06945)

    在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。

    

    我们提出了一种用于大型语言模型中上下文压缩的上下文自编码器（ICAE）。 ICAE有两个模块：一个可学习的编码器，通过从LLM中采用LoRA方式将长上下文压缩为有限数量的内存槽，以及一个固定的解码器，作为目标LLM，可以根据内存槽来进行各种目的的条件处理。我们首先使用自编码和语言建模目标在大规模文本数据上预训练ICAE，使其能够生成准确和全面表示原始上下文的内存槽。然后，我们使用少量指导数据对预训练的ICAE进行微调，以增强其与各种提示的交互，从而产生理想的响应。我们的实验结果表明，使用我们提出的预训练和微调范式学习的ICAE可以有效地产生$4\times$上下文压缩的内存槽，目标LLM可以很好地对其进行条件处理，以响应各种提示。

    We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
    
[^299]: SAMAug: Segment Anything Model的点提示增强方法

    SAMAug: Point Prompt Augmentation for Segment Anything Model. (arXiv:2307.01187v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.01187](http://arxiv.org/abs/2307.01187)

    SAMAug是一种用于增强交互式图像分割性能的方法，通过生成增强的点提示，结合初始提示，可以提高Segment Anything Model的分割结果。

    

    本文介绍了SAMAug，一种用于增强交互式图像分割性能的新型视觉点提示增强方法。SAMAug生成增强的点提示，以提供更多关于用户意图的信息给SAM。SAM从一个初始点提示开始生成一个初始掩码，然后将其输入我们提出的SAMAug来生成增强的点提示。通过结合这些额外的点，SAM可以基于增强的点提示和初始提示生成增强的分割掩码，从而提高分割性能。我们使用了四种不同的点增强策略进行评估：随机采样，基于最大差异熵的采样，最大距离和显著性。在COCO、Fundus、COVID QUEx和ISIC2018数据集上的实验证实了SAMAug可以提升SAM的分割结果，尤其是使用最大距离和显著性。SAMAug证明了其应用潜力。

    This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information about the user's intention to SAM. Starting with an initial point prompt, SAM produces an initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on both the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We conducted evaluations using four different point augmentation strategies: random sampling, sampling based on maximum difference entropy, maximum distance, and saliency. Experiment results on the COCO, Fundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency. SAMAug demonstrates the potenti
    
[^300]: ELM神经元：一种高效且表达力强的皮层神经元模型可以解决长时间跨度任务

    The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])

    [http://arxiv.org/abs/2306.16922](http://arxiv.org/abs/2306.16922)

    ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。

    

    传统的大规模神经科学模型和机器学习利用简化的个体神经元模型，依靠集体活动和适当调整的连接来执行复杂的计算。然而，每个生物皮层神经元本质上都是一个复杂的计算设备，最近的一项研究证实了这一点，该研究中，需要一个具有数百万个参数的深度人工神经网络来复制详细生物物理模型的输入输出关系。我们对这些多个参数的必要性提出了质疑，并引入了表达力强的泄漏存储器（ELM）神经元，这是一种受生物启发的计算模型，具有高计算表达力，同时也非常高效。值得注意的是，我们的ELM神经元仅需要8,000个可训练参数就能准确匹配前述的输入输出关系。我们发现，准确的模型需要多个类似于存储器的隐藏状态和复杂的非线性突触整合。

    Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
    
[^301]: 用深度学习简化社交媒体信息检索以支持公共卫生研究

    Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])

    [http://arxiv.org/abs/2306.16001](http://arxiv.org/abs/2306.16001)

    本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。

    

    社交媒体在流行病监测中的利用已经得到了很好的证实。然而，当使用预定义的词汇表来检索相关语料库时，常常会引入偏见。本研究介绍了一个框架，旨在构建医学俗语和统一医学语言系统（UMLS）概念的广泛字典。该框架由三个模块组成：基于BERT的命名实体识别（NER）模型，用于从社交媒体内容中识别出医学实体；深度学习驱动的标准化模块，用于对提取出的实体进行规范化处理；半监督聚类模块，将最可能的UMLS概念分配给每个规范化实体。我们将该框架应用于从2020年2月1日到2022年4月30日期间与COVID-19相关的推文，生成了一个症状词典（可在https://github.com/ningkko/UMLS_colloquialism/上获取），其中包含9,249个标准化实体，映射到876个UMLS概念和38,175个俚语表达。该框架的演示

    The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
    
[^302]: Jumanji: JAX中一套多样化可扩展的强化学习环境

    Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX. (arXiv:2306.09884v1 [cs.LG])

    [http://arxiv.org/abs/2306.09884](http://arxiv.org/abs/2306.09884)

    Jumanji是JAX中一套可扩展的强化学习系统，提供了一系列高度可定制的环境，具有快速、灵活、可扩展和模块化特点，利用硬件加速器赋能更有能力的代理人。

    

    开源强化学习环境在推动AI算法的发展方面起到了关键作用。现代强化学习研究需要模拟环境具备性能、可扩展性和模块化特点，以扩展其在更广泛的实际应用中的可用性。因此，我们提出了Jumanji，这是一套设计用于快速、灵活和可扩展的不同RL环境的强化学习系统。Jumanji提供了一系列的环境，专注于工业中经常遇到的组合问题，以及挑战性的一般决策任务。通过利用JAX和GPU、TPU等硬件加速器的效率，Jumanji能够迅速迭代研究思路和大规模实验，最终赋能更有能力的代理人。与现有的强化学习环境套件不同，Jumanji具有高度可定制性，允许用户根据其需求调整初始状态分布和问题复杂度。

    Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Further
    
[^303]: 为抓住任何物品铺平道路：基于迁移学习的通用抓取放置机器人模型

    Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])

    [http://arxiv.org/abs/2306.05716](http://arxiv.org/abs/2306.05716)

    本研究提出了一种基于语言分割掩模的新方法，用于解决通用型机器人的泛化能力问题，提高了在开放域场景中新对象的抓取操作的学习效率和推广效果。

    

    提高通用型机器人的泛化能力一直是研究社区长期追求的重要挑战。现有的方法通常依赖于收集大规模现实世界机器人数据，如 RT-1 数据集。然而，这些方法通常效率低下，限制了它们在具有新对象和多样背景的开放域场景中的能力。本文提出了一种新的范例，有效地利用最先进的基础模型生成的基于语言的分割掩模，以解决日常场景中广泛的拾放机器人操作任务。通过将掩模传达的精确语义和几何形状集成到我们的多视角策略模型中，我们的方法可以感知准确的物体姿态并实现高效学习，同时也有助于有效的新对象的推广。我们的方法同时可以实现在训练时观察到相似形状的新物体的抓取操作。

    Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
    
[^304]: 深度神经网络的相似性校准

    Proximity-Informed Calibration for Deep Neural Networks. (arXiv:2306.04590v1 [cs.LG])

    [http://arxiv.org/abs/2306.04590](http://arxiv.org/abs/2306.04590)

    该论文提出了一个校准算法，解决了深度神经网络推理过程中低接近度数据和高接近度数据之间不一致的误校准问题。

    

    推理结果的可信度校准对于提供准确和可解释的不确定性估计至关重要，特别是在安全关键场景下。已有的校准方法通常忽略了接近度偏差的问题，即模型在低接近性数据（即分布的稀疏区域）中倾向于更自信，而在高接近性样本中表现出不一致的误校准，我们发现这一问题。我们在ImageNet预训练模型上研究了这一问题，并观察到：1）接近度偏差存在于各种模型架构和大小之间；2）基于Transformer的模型比基于CNN的模型更容易受到接近度偏差的影响；3）即使采用流行的校准算法如温度缩放，接近度偏差也会持续存在；4）模型在低接近性样本上的过拟合程度比高接近性样本更严重。在这些实证发现的基础上，我们提出了ProCal。

    Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, 
    
[^305]: 大型语言模型的自相矛盾幻觉：评估、检测和缓解

    Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])

    [http://arxiv.org/abs/2305.15852](http://arxiv.org/abs/2305.15852)

    本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。

    

    大型语言模型容易产生幻想的文本。自相矛盾是一种重要的幻觉形式，指的是语言模型在同一语境中生成两个矛盾的句子。本文针对最先进、经过指导的语言模型，对自相矛盾进行了全面的分析、评估、检测和缓解。我们设计了一个框架来有效地触发自相矛盾，评估结果表明，无论是对于著名的还是不太出名的话题，不同的语言模型中自相矛盾都经常发生。

    Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
    
[^306]: 在混合语言的社交媒体文本中检测宣传技术

    Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])

    [http://arxiv.org/abs/2305.14534](http://arxiv.org/abs/2305.14534)

    本文提出了一个新任务，即在混合语言的社交媒体文本中检测宣传技术。为了支持这一任务，作者创建了一个包含1030个文本的英语和罗马乌尔混合语言的语料库，并进行了一系列实验。

    

    宣传是一种旨在影响公众舆论和心态以推广特定议程的沟通形式。随着社交媒体的崛起，宣传已经迅速传播，引发了对自动宣传检测系统的需求。大多数宣传检测工作都集中在高资源语言（如英语）上，几乎没有为低资源语言检测宣传做出努力。然而，在社交媒体交流中发现多种语言的混合现象是很常见的，这被称为码混。码混在同一文本中结合了不同的语言，这对于自动系统构成了挑战。考虑到这一点，我们在此提出了检测混合文本中宣传技术的新任务。为了支持这个任务，我们创建了一个包含1030个文本的语料库，这些文本在英语和罗马乌尔都进行了混合，并用20种宣传技巧进行了注释，我们已经公开了这个语料库。我们进行了一系列实验来对比不同的模型和特征集合在此任务上的表现。

    Propaganda is a form of communication intended to influence the opinions and the mindset of the public to promote a particular agenda. With the rise of social media, propaganda has spread rapidly, leading to the need for automatic propaganda detection systems. Most work on propaganda detection has focused on high-resource languages, such as English, and little effort has been made to detect propaganda for low-resource languages. Yet, it is common to find a mix of multiple languages in social media communication, a phenomenon known as code-switching. Code-switching combines different languages within the same text, which poses a challenge for automatic systems. With this in mind, here we propose the novel task of detecting propaganda techniques in code-switched text. To support this task, we create a corpus of 1,030 texts code-switching between English and Roman Urdu, annotated with 20 propaganda techniques, which we make publicly available. We perform a number of experiments contrastin
    
[^307]: 具有概率保证的神经网络鲁棒的反事实解释

    Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])

    [http://arxiv.org/abs/2305.11997](http://arxiv.org/abs/2305.11997)

    本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。

    

    针对神经网络发现偏移，通过使用稳定性度量来量化反事实解释对可能的模型变化的鲁棒性。通过在反事实解释优化中引入正则化项来将生成的反事实解释靠近数据流形，从而实现了对自然发生的模型变化的高概率鲁棒性。新的算法在合成和现实世界数据集上进行实验，证明了其有效性。

    There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
    
[^308]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^309]: Tram：一个源代码摘要的令牌级检索增强机制

    Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization. (arXiv:2305.11074v1 [cs.AI])

    [http://arxiv.org/abs/2305.11074](http://arxiv.org/abs/2305.11074)

    Tram是一种源代码摘要的令牌级别检索增强机制，它在解码器端精细检索帮助神经模型生成更准确的摘要，并在Java和Python源代码摘要任务上表现优异。

    

    自动生成人类可读的文本以描述程序的功能是源代码摘要的目标。虽然神经语言模型在这个领域取得了显著的性能，但结合神经模型和外部知识的新趋势正在兴起。大多数先前的方法依赖于句子级别的检索和组合范式（检索类似的代码片段并使用相应的代码和摘要对来编码）。然而，这种范式是粗粒度的，不能直接利用解码器端高质量的检索摘要令牌。本文中，我们探讨了一种精细的令牌级别检索增强机制，在解码器端帮助原始神经模型生成更好的代码摘要。此外，为了缓解令牌级别检索在捕捉上下文代码语义方面的局限性，我们提出将代码语义集成到摘要令牌中。大量的实验和人类评估表明，我们提出的方法Tram在Java和Python源代码摘要任务上均优于最先进的方法。

    Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although Neural Language Models achieve significant performance in this field, an emerging trend is combining neural models with external knowledge. Most previous approaches rely on the sentence-level retrieval and combination paradigm (retrieval of similar code snippets and use of the corresponding code and summary pairs) on the encoder side. However, this paradigm is coarse-grained and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we explore a fine-grained token-level retrieval-augmented mechanism on the decoder side to help the vanilla neural model generate a better code summary. Furthermore, to mitigate the limitation of token-level retrieval on capturing contextual code semantics, we propose to integrate code semantics into summary tokens. Extensive experiments and human evaluation revea
    
[^310]: 使用LLM辅助注释进行语料库语言学研究：本地语法分析案例研究

    Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08339](http://arxiv.org/abs/2305.08339)

    本文研究了使用基于大语言模型的聊天机器人自动标注文本的潜力，重点考察了从本地语法角度观察道歉言语行为构成的功能元素的程度，并比较了不同模型在注释任务中的表现，结果表明Bing聊天机器人在任务中表现优于ChatGPT和人类标注员。

    

    基于大语言模型（LLMs）的聊天机器人在语言理解方面表现出很强的能力。本研究探索LLMs在协助基于语料库的语言学研究方面的潜力，通过将文本自动标注为特定语言信息类别。具体而言，我们研究了从本地语法的角度观察道歉言语行为构成的功能元素的程度，通过比较基于GPT-3.5的ChatGPT、基于GPT-4的Bing聊天机器人和人类编码器在注释任务中的表现。结果表明，Bing聊天机器人在任务中表现显着优于ChatGPT。与人类标注员相比，Bing聊天机器人的整体表现略低于人类标注员的表现，但已经取得了较高的F1得分:道歉标记99.95％，原因标记91.91％，道歉者标记95.35％，被道歉者标记89.74％和加强标记96.47％。这表明，在语言类别清晰且可以轻松识别的情况下，使用LLM辅助注释进行语料库语言学研究是可行的。

    Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
    
[^311]: 当多数人是错误的：利用标注者不一致性进行主观任务

    When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])

    [http://arxiv.org/abs/2305.06626](http://arxiv.org/abs/2305.06626)

    本文通过预测单个标注者的打分，并结合文本目标群体的预测，模拟了目标群体成员的意见，通过使用他们的人口统计学数据和在线意见预测标注者的打分，在仇恨言论检测等主观任务中提高了模型性能。

    

    在自然语言处理中，虽然通常使用标注者的多数投票来确定标签，但在仇恨言论检测等主观任务中，标注者之间存在不一致性可能反映出群体观点的差异，而不是噪声。因此，仇恨言论检测的一个关键问题是一个语句是否冒犯了它所针对的人群，而这可能只占标注者池的一小部分。本文构建了一个模型，预测可能具有冒犯性文本上每个标注者的打分，并结合文本的预测目标群体来模拟目标群体成员的意见。我们展示了一系列的评估指标，包括提高了22％在预测每个标注者的打分上的性能，提高了33％在预测标注者之间方差上的性能，这提供了下游用来衡量模型不确定性的方法。我们发现可以使用标注者的人口统计信息和其在线意见来预测标注者的打分。

    Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
    
[^312]: 决策时间规划的更新等价框架

    The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])

    [http://arxiv.org/abs/2304.13138](http://arxiv.org/abs/2304.13138)

    该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。

    

    在棋类游戏等完全信息环境中，即时修正（或构建）策略的决策时间规划是实现超人类表现的关键。一些研究将决策时间规划扩展到更普遍的不完全信息环境，从而实现了扑克中的超人类表现。但是，这些方法需要考虑随着非公共信息量的增加而快速增长的子游戏，使得它们在非公共信息量较大时不起作用。为了解决这个问题，我们引入了一种基于更新等价而不是子游戏概念的决策时间规划框架。在这个框架中，决策时间规划算法模拟同步学习算法的更新。这个框架使我们能够引入一系列原则上的决策时间规划算法，这些算法不依赖于公共信息，并为新的一个系列的决策时间规划算法打开了大门。

    The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
    
[^313]: 利用高级循环和张量抽象在CPU架构上通过深度学习和HPC内核

    Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures. (arXiv:2304.12576v1 [cs.DC])

    [http://arxiv.org/abs/2304.12576](http://arxiv.org/abs/2304.12576)

    该论文介绍了一种新的方法来开发适用于现代CPU体系结构的高效、可移植的深度学习和高性能计算内核，使用高级循环和张量抽象。

    

    在过去的十年中，深度学习（DL）算法、编程系统和硬件已经与高性能计算（HPC）相结合。然而，DL和HPC系统的编程方法却停滞不前，依赖于高度优化、特定于平台、僵化的供应商优化库。这项工作介绍了一个框架，用于开发现代CPU架构的高效、可移植的DL和HPC内核。我们将内核开发分解为两个步骤：1）使用张量处理原语（TPP）表达计算核心：一个紧凑、多功能的2D张量运算符，2）以高级、声明性的方式表达TPP周围的逻辑循环，而确切的实例化（顺序，内存布局）则通过将TPL视为黑盒，根据优化目标和约束由自动优化器完成。

    During the past decade, Deep Learning (DL) algorithms, programming systems and hardware have converged with the High Performance Computing (HPC) counterparts. Nevertheless, the programming methodology of DL and HPC systems is stagnant, relying on highly-optimized, yet platform-specific and inflexible vendor-optimized libraries. Such libraries provide close-to-peak performance on specific platforms, kernels and shapes thereof that vendors have dedicated optimizations efforts, while they underperform in the remaining use-cases, yielding non-portable codes with performance glass-jaws. This work introduces a framework to develop efficient, portable DL and HPC kernels for modern CPU architectures. We decompose the kernel development in two steps: 1) Expressing the computational core using Tensor Processing Primitives (TPPs): a compact, versatile set of 2D-tensor operators, 2) Expressing the logical loops around TPPs in a high-level, declarative fashion whereas the exact instantiation (order
    
[^314]: 基于排名学习和局部模型的多目标高维昂贵问题的进化算法

    Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems. (arXiv:2304.09444v1 [cs.NE])

    [http://arxiv.org/abs/2304.09444](http://arxiv.org/abs/2304.09444)

    本文提出了一种基于排名学习和局部模型的多目标进化算法，该算法使用分类器进行排名，以解决高维昂贵多目标优化问题。

    

    近年来，辅以代理模型的进化算法广泛应用于解决复杂而计算代价昂贵的多目标优化问题。但是在处理高维优化问题时，这些辅以代理模型的多目标进化算法的性能会急剧恶化。本文提出了一种新颖的基于分类器辅助的排名学习和局部模型的多目标进化算法 (CLMEA)，用于解决高维昂贵的多目标优化问题。该算法由三部分组成：分类器辅助的排名学习、超体积非支配搜索和相对稀疏目标空间的局部搜索。具体来说，该算法建立了一个概率神经网络作为分类器，将后代划分为几个等级。不同等级的后代使用排名学习策略生成更具有前景性和信息性的候选解用于实际优化函数。

    Surrogate-assisted evolutionary algorithms have been widely developed to solve complex and computationally expensive multi-objective optimization problems in recent years. However, when dealing with high-dimensional optimization problems, the performance of these surrogate-assisted multi-objective evolutionary algorithms deteriorate drastically. In this work, a novel Classifier-assisted rank-based learning and Local Model based multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional expensive multi-objective optimization problems. The proposed algorithm consists of three parts: classifier-assisted rank-based learning, hypervolume-based non-dominated search, and local search in the relatively sparse objective space. Specifically, a probabilistic neural network is built as classifier to divide the offspring into a number of ranks. The offspring in different ranks uses rank-based learning strategy to generate more promising and informative candidates for real funct
    
[^315]: 通过神经元激活空间的对抗学习，在视觉问答中对实例进行的特洛伊攻击

    Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (arXiv:2304.00436v1 [cs.CV])

    [http://arxiv.org/abs/2304.00436](http://arxiv.org/abs/2304.00436)

    本研究提出了一种实例级别的特洛伊攻击方法，在视觉问答中通过神经元激活空间的对抗学习生成多样性特洛伊，有效地适应了微调模型。

    

    恶意扰动被嵌入输入数据中，称为特洛伊攻击，可以导致神经网络表现异常。然而，在模型微调过程中，即从预训练的大规模模型（如视觉问答）向目标模型转移知识的过程中，特洛伊攻击的影响有所减小。为了减轻特洛伊攻击的影响，我们可以替换和微调预训练模型的多个层。本研究聚焦于样本效率、隐蔽性、多样性和鲁棒性。为了解决这些挑战，我们提出了一个针对实例级的特洛伊攻击，该攻击生成跨输入样本和模态的多样化特洛伊。对抗学习建立了指定扰动层和微调模型的异常行为之间的相关性。我们在VQA-v2数据集上进行了大量实验，并使用多种指标进行了评估。实验结果表明，我们提出的方法可以有效适应微调模型。

    Malicious perturbations embedded in input data, known as Trojan attacks, can cause neural networks to misbehave. However, the impact of a Trojan attack is reduced during fine-tuning of the model, which involves transferring knowledge from a pretrained large-scale model like visual question answering (VQA) to the target model. To mitigate the effects of a Trojan attack, replacing and fine-tuning multiple layers of the pretrained model is possible. This research focuses on sample efficiency, stealthiness and variation, and robustness to model fine-tuning. To address these challenges, we propose an instance-level Trojan attack that generates diverse Trojans across input samples and modalities. Adversarial learning establishes a correlation between a specified perturbation layer and the misbehavior of the fine-tuned model. We conducted extensive experiments on the VQA-v2 dataset using a range of metrics. The results show that our proposed method can effectively adapt to a fine-tuned model 
    
[^316]: Magnushammer: 一种基于Transformer的前提选择方法

    Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04488](http://arxiv.org/abs/2303.04488)

    Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。

    

    前提选择是自动定理证明的一个基本问题。以往的方法常常使用复杂的符号方法，依赖领域知识，并需要大量的工程工作来解决这个任务。在这项工作中，我们展示了基于神经转换器的Magnushammer方法可以大幅度地超越传统的符号系统。通过在PISA基准上的测试，Magnushammer的证明率达到了59.5％，而最成熟和流行的基于符号的求解器Sledgehammer的证明率只有38.3％。此外，通过将Magnushammer与基于语言模型的神经形式证明器相结合，我们将先前最先进的证明率从57.0％大幅提高到71.0％。

    Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
    
[^317]: 私密、公平且精确：在医学影像中训练大规模隐私保护的人工智能模型

    Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.01622](http://arxiv.org/abs/2302.01622)

    本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。

    

    人工智能模型在医学领域的应用越来越多。然而，由于医学数据的高度敏感性，需要采取特殊措施确保其保护。保护隐私的黄金标准是引入差分隐私（DP）来进行模型训练。先前的研究表明，DP对模型的准确性和公平性有负面影响，这在医学中是不可接受的，并且是隐私保护技术广泛应用的主要障碍。在这项工作中，我们评估了隐私保护训练人工智能模型对准确性和公平性的影响，与非隐私训练进行了比较。为此，我们使用了两个数据集：（1）一个大规模数据集（N=193,311）的高质量临床胸部X射线图像，和（2）一个数据集（N=1,625）的3D腹部计算机断层扫描（CT）图像，用于分类胰腺导管腺癌（PDAC）的存在。两个数据集均为回顾性采集，并由经验丰富的医学影像专家进行手动标注。

    Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
    
[^318]: 使用Tsallis KL散度的广义Munchausen强化学习

    Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11476](http://arxiv.org/abs/2301.11476)

    这篇论文通过研究广义的Tsallis KL散度，扩展了Munchausen强化学习算法，并提供了一种将KL正则化纳入实际算法的方法。对于Tsallis KL，当$q > 1$时，可以获得新的策略优化选项。

    

    许多强化学习中的策略优化方法都采用Kullback-Leibler（KL）散度到上一个策略，以防止策略变化过快。这个想法最初是在Conservative Policy Iteration的一篇重要论文中提出的，近似算法如TRPO和Munchausen Value Iteration（MVI）给出了有限的方法。我们通过研究一种广义的KL散度 - 称为Tsallis KL散度 - 来继续这一工作，它在定义中使用了$q$-对数。这种方法是一种严格的推广，因为$q = 1$对应于标准的KL散度；$q > 1$提供了一系列新的选项。我们对在Tsallis KL下学习的策略类型进行了表征，并阐述了何时$ q > 1 $可能是有益的。为了获得一个将Tsallis KL正则化纳入实际算法的方法，我们扩展了MVI，它是一种最简单的包含KL正则化的方法之一。我们展示了这种广义MVI（$q$）获得了显著的改进。

    Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q > 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q >1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
    
[^319]: 向导航即攻击者所愿？建立拜占庭鲁棒性的联邦学习体系下的代理人

    Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.14769](http://arxiv.org/abs/2211.14769)

    本文研究了联邦学习体系下代理人学习中可能出现的攻击和防御策略，建立了全联邦拜占庭鲁棒的代理人学习模型。其中，导航即攻击者所愿（NAW）是一种简单而有效的攻击策略，而基于离群点检测的防御训练方法可以有效减轻NAW攻击的影响，提高代理人学习的全局鲁棒性。

    

    联邦化体系下的代理人学习通过在本地客户端（即不同环境）中保持数据来保护个人视觉环境的数据隐私。然而，在联邦学习下，由于本地数据对服务器是不可访问的，攻击者可能轻易地污染本地客户端的训练数据，从而在不被通知的情况下在代理人中建立后门。使用这样的代理人会对人类构成潜在危害，因为攻击者可以轻松地通过后门操纵代理人进行导航和控制。为了实现全联邦拜占庭鲁棒的代理人学习，在本文中，我们研究了视觉与语言导航（VLN）任务中的攻击和防御，其中代理人需要跟随自然语言指令来导航室内环境。首先，我们介绍了一种简单而有效的攻击策略，即导航即攻击者所愿（NAW），其中恶意客户端通过操纵本地轨迹数据来向全局模型植入后门。结果表明，NAW可以实现高攻击成功率，而且性能下降微不足道。为了防止NAW攻击，我们提出了一种防御训练方法，该方法利用离群点检测的概念来识别和删除恶意客户端。我们在VLN任务上的实验表明，所提出的防御方法可以有效地减轻NAW攻击的影响，提高联邦化体系下代理人学习的全局鲁棒性。

    Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Resu
    
[^320]: 网络流的图神经建模

    Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05208](http://arxiv.org/abs/2209.05208)

    本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    

    网络流问题涉及将流量分布在网络中，以使基础设施得到有效利用，这在交通运输和物流中是无处不在的。其中，多商品网络流 (MCNF) 问题是普遍感兴趣的，因为它涉及在多个源和汇之间分配不同大小的多个流，同时实现链路的有效利用。由于数据驱动优化的吸引力，这些问题越来越多地使用图学习方法来解决。本文提出了一种新颖的网络流问题图学习架构 PEW (Per-Edge Weights)。此方法基于图注意力网络，并沿着每个链接使用不同参数化的消息函数。我们通过使用 $17$ 个服务提供商拓扑和 $2$ 个路由方案进行互联网流量路由案例研究，对所提出的解决方案进行了广泛的评估。我们展示了 PEW 相对于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
    

