# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning.](http://arxiv.org/abs/2309.07915) | MMICL提出了一种用于视觉-语言模型的架构和训练数据设计，以解决VLM在理解复杂多模态提示方面的困难。 |
| [^2] | [Beta Diffusion.](http://arxiv.org/abs/2309.07867) | beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。 |
| [^3] | [The Rise and Potential of Large Language Model Based Agents: A Survey.](http://arxiv.org/abs/2309.07864) | 基于大型语言模型的代理的崛起和潜力：一项调查。大型语言模型被认为是构建通用人工智能代理的潜在催化剂，许多研究已经取得重要进展。 |
| [^4] | [CiwaGAN: Articulatory information exchange.](http://arxiv.org/abs/2309.07861) | 本文介绍了CiwaGAN模型，该模型结合了无监督声韵学建模和无监督听觉模态信息交流，是对人类口语习得最现实的近似。 |
| [^5] | [ExpertQA: Expert-Curated Questions and Attributed Answers.](http://arxiv.org/abs/2309.07852) | 本论文介绍了ExpertQA，它是一个专家策划的问题和带有属性的答案系统。该系统通过分析语言模型在领域特定情景中提供的事实准确性和归因等方面来确保提供准确的信息。研究还收集了领域专家的问题并要求他们评估生成的答案。这项工作的目的是确保语言模型在高风险领域中不会传播错误信息，从而避免不良的社会后果。 |
| [^6] | [Applying Deep Learning to Calibrate Stochastic Volatility Models.](http://arxiv.org/abs/2309.07843) | 本研究将深度学习技术应用于校准随机波动性模型，通过训练神经网络对基于Heston模型的标的资产进行定价，并且在校准方面取得了快速和准确的结果。 |
| [^7] | [Two Timin': Repairing Smart Contracts With A Two-Layered Approach.](http://arxiv.org/abs/2309.07841) | 本文提出了一个双层框架来处理智能合约中的漏洞。通过将Slither的漏洞报告与源代码结合，结合预训练的分类器和大型语言模型，实现了智能合约的分类和修复。实验证明了该方法的有效性，可以显著减少漏洞数量。 |
| [^8] | [VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning.](http://arxiv.org/abs/2309.07832) | VAPOR是一种使用离线强化学习的方法，用于在室外复杂植被环境中进行全向腿式机器人的自主导航。通过从未标记的真实植被数据中训练，该方法学习了植被的物理和几何特性，并使用一个自适应规划器生成动态可行机器人动作，在狭窄通道中导航，并避免被高草和灌木丛所困住。在实验中，我们观察到成功率有所提高。 |
| [^9] | [Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery.](http://arxiv.org/abs/2309.07823) | 本文提出一种利用OpenStreetMap道路数据作为弱标签和大规模卫星图像预训练语义分割模型的方法，实验证明对于道路提取而言，预测准确度随着弱标注数据量和道路密度的增加而增加。 |
| [^10] | [What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving.](http://arxiv.org/abs/2309.07808) | 本文提出了一种基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能，并解决了交通规则遵守和传感器感知问题。 |
| [^11] | [Variational Quantum Linear Solver enhanced Quantum Support Vector Machine.](http://arxiv.org/abs/2309.07770) | 提出了一种变分量子线性求解器增强的量子支持向量机算法，能够在嘈杂中间规模量子设备上进行可扩展的分类任务。算法利用变分量子线性求解器解决了最小二乘支持向量机的线性方程组，并在Iris数据集上进行了验证。通过经典计算和量子计算的策略性组合，算法在不同维度的特征空间中表现出了实用性和有效性。 |
| [^12] | [PRE: Vision-Language Prompt Learning with Reparameterization Encoder.](http://arxiv.org/abs/2309.07760) | 这项工作提出了一种名为PRE的方法，通过重新参数化编码器来增强可学习提示的泛化能力，从而解决了大型预训练视觉-语言模型中手动提示工程的挑战。 |
| [^13] | [Generative AI Text Classification using Ensemble LLM Approaches.](http://arxiv.org/abs/2309.07755) | 本文提出了一个利用集成的LLM方法进行生成式AI文本分类的方法，通过使用多个预训练LLM生成概率作为特征来准确检测AI生成的语言，并确定生成文本的特定语言模型的归属。 |
| [^14] | [AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks.](http://arxiv.org/abs/2309.07730) | 本文提出了一个自适应的分布式水声传感器网络入侵检测与防御系统（AIDPS），该系统能够提高水声传感器网络的安全性，有效地检测水下相关攻击，并且通过使用多种机器学习算法进行实验证明了其有效性。 |
| [^15] | [NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches.](http://arxiv.org/abs/2309.07704) | 该论文介绍了NutritionVerse-Synth，这是一个拥有大规模合成食物图像数据集，其中包含了多种视角、模态和饮食注释，旨在解决目前饮食摄入估计方法的准确性和真实性问题。 |
| [^16] | [Tree of Uncertain Thoughts Reasoning for Large Language Models.](http://arxiv.org/abs/2309.07694) | 本研究提出了一种针对大型语言模型的推理框架——不确定思维树（TouT），它通过利用蒙特卡洛丢弃来量化中间步骤上的本地不确定性，提高了模型生成响应的精确性。 |
| [^17] | [Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text.](http://arxiv.org/abs/2309.07689) | 这篇调查论文概述了当前用于区分ChatGPT生成文本和人工文本的方法和数据集，并总结了研究发现。 |
| [^18] | [deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations.](http://arxiv.org/abs/2309.07684) | deepFDEnet是一种新型神经网络架构，能够准确求解各种形式的分数阶微分方程。 |
| [^19] | [Assessing the nature of large language models: A caution against anthropocentrism.](http://arxiv.org/abs/2309.07683) | 通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。 |
| [^20] | [Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation.](http://arxiv.org/abs/2309.07670) | 本文提出了一种用于联邦领域自适应的方法，通过字典学习经验分布来解决客户端间分布偏移和部分无标签数据的问题。该方法通过设计协作通信协议和聚合操作，保护了客户端数据隐私，并成功在目标领域生成了标记数据。 |
| [^21] | [Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning.](http://arxiv.org/abs/2309.07666) | 本文介绍了一个新问题，称为多源领域适应通过数据集字典学习遇见数据集蒸馏（MSDA-DD），通过适应先前的方法以及分配匹配方法，我们实现了仅使用每类1个样本即可实现最先进的适应性能。 |
| [^22] | [Feature Engineering in Learning-to-Rank for Community Question Answering Task.](http://arxiv.org/abs/2309.07610) | 本文研究了在社区问答任务中学习到排名中的特征工程的几个方面。首先，引入了基于BERT的特征，捕捉语义相似性；其次，结合问题和答案两种类型的特征；第三，通过经验性研究探索了不同排名算法。 |
| [^23] | [Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?.](http://arxiv.org/abs/2309.07602) | 在比较推荐系统中的两种模型SASRec和BERT4Rec时，我们的研究发现，如果两个模型都使用相同的损失函数进行训练，SASRec在质量和训练速度方面表现明显优于BERT4Rec。同时，我们还发现，即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但需要更多的负样本。 |
| [^24] | [Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision.](http://arxiv.org/abs/2309.07601) | 本文研究了使用大型语言模型和弱监督的方式来检测虚假信息，证明了这种方法在两个数据集上的效果优于当前最先进的分类器。 |
| [^25] | [C-Pack: Packaged Resources To Advance General Chinese Embedding.](http://arxiv.org/abs/2309.07597) | C-Pack是一套推进普通汉语嵌入领域的资源，包括全面汉语文本嵌入基准、大规模文本嵌入数据集和涵盖多个尺寸的嵌入模型系列。该资源集在C-MTEB基准上实现了最高+10%的表现，并通过整合和优化一套训练方法进一步提升了效果。此外，C-Pack还发布了英语文本嵌入数据和模型，实现了最先进的性能。该资源集可公开获取。 |
| [^26] | [Neuro-Symbolic Recommendation Model based on Logic Query.](http://arxiv.org/abs/2309.07594) | 本文提出了一个基于逻辑查询的神经符号推荐模型，将用户历史交互转化为逻辑表达式，并通过逻辑查询实现推荐过程。 |
| [^27] | [Statistically Valid Variable Importance Assessment through Conditional Permutations.](http://arxiv.org/abs/2309.07593) | 本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。 |
| [^28] | [Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning.](http://arxiv.org/abs/2309.07578) | 这个论文提出了一种用于离线强化学习中泛化的等变数据增强方法，并通过扩展等变集合并增强数据集来提高策略的测试性能。 |
| [^29] | [Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer.](http://arxiv.org/abs/2309.07566) | 本研究提出了一种基于离散单元的语音到语音翻译框架，通过自监督学习和神经编解码器实现风格转换，解决了数据稀缺和音色保留的问题。实验结果表明，我们的模型在之前未见的语言上实现了高质量的跨语言风格转换。 |
| [^30] | [SingFake: Singing Voice Deepfake Detection.](http://arxiv.org/abs/2309.07525) | 本文提出了唱歌声音Deepfake检测任务，并通过提供一个特定的数据集和评估系统的方法，对这一问题进行了研究和解决。 |
| [^31] | [Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions.](http://arxiv.org/abs/2309.07510) | 本论文提出了一个环境感知的可供性框架，考虑了物体级的可行性先验和环境约束，以解决多个遮挡的复杂情况下的三维关节物体操作问题。 |
| [^32] | [Connected Autonomous Vehicle Motion Planning with Video Predictions from Smart, Self-Supervised Infrastructure.](http://arxiv.org/abs/2309.07504) | 本文介绍了一种利用智能自监督基础设施的视频预测来实现连接自主车辆的运动规划的方法。通过修改预测来预测未来的占用情况，而不是原始视频，减少了广播预测的数据占用量，并通过实验证明了该设计可以有效地辅助CAV的运动规划。 |
| [^33] | [HDTR-Net: A Real-Time High-Definition Teeth Restoration Network for Arbitrary Talking Face Generation Methods.](http://arxiv.org/abs/2309.07495) | HDTR-Net是一种实时高清牙齿修复网络，适用于任意说话人脸生成方法。通过使用Fine-Grained Feature Fusion (FGFF)模块来捕捉细纹理特征信息并提高牙齿的清晰度，HDTR-Net可以快速增强牙齿区域，并保持同步和时间上的一致性。 |
| [^34] | [Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects.](http://arxiv.org/abs/2309.07473) | Where2Explore是一种针对未见物体的少样本能力学习框架，通过有效地探索和最少数量的交互，可以推广到具有类似局部几何结构的新类别。 |
| [^35] | [Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection.](http://arxiv.org/abs/2309.07461) | 这项研究介绍了一个针对物联网环境定制的网络入侵检测系统的开放集分类器框架，利用图像表示和堆叠子聚类技术来识别未知攻击。 |
| [^36] | [Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges.](http://arxiv.org/abs/2309.07438) | 本研究探讨了在物联网中实现人工通用智能（AGI）的机遇与挑战，并提出了将AGI无缝集成到物联网中的概念框架。 |
| [^37] | [Semantic Parsing in Limited Resource Conditions.](http://arxiv.org/abs/2309.07429) | 本论文探讨了在有限资源条件下的语义解析的挑战，并提出了解决方案，包括使用自动数据筛选、知识迁移、主动学习和持续学习。具体方法包括合成训练样本、利用源领域知识改进目标领域解析，以及利用有限的人工翻译预算和机器翻译服务来调整解析器。 |
| [^38] | [JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale.](http://arxiv.org/abs/2309.07425) | JSMNet通过自注意力机制和多尺度改进室内点云语义和实例分割。在室内3D点云数据中，JSMNet通过全局特征自注意力模块和多分辨率特征自适应融合模块，实现了更好的室内目标特征表达和语义、实例分割结果，具有较高的质量和准确性。 |
| [^39] | [Client-side Gradient Inversion Against Federated Learning from Poisoning.](http://arxiv.org/abs/2309.07415) | 这项工作提出了一种新的联邦学习攻击方法——客户端污染梯度反演（CGI），能够从客户端发起并在有限的知识条件下恢复训练样本。这项研究首次展示了客户端对FL的攻击可能性。 |
| [^40] | [FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.](http://arxiv.org/abs/2309.07405) | FunCodec是一个基础的、可复现的、可整合的神经语音编解码器工具包，提供了训练方法和预训练模型，可以实现较高的重构质量和整合到下游任务中。 |
| [^41] | [Multi-Grade Deep Learning for Partial Differential Equations with Applications to the Burgers Equation.](http://arxiv.org/abs/2309.07401) | 本文提出了一种多级深度学习方法，用于解决非线性偏微分方程。该方法通过将DNN的学习任务分解为多个堆叠的神经网络，以解决随着网络层数增加而导致的非凸优化问题的复杂度增加的挑战。 |
| [^42] | [Semantic Adversarial Attacks via Diffusion Models.](http://arxiv.org/abs/2309.07398) | 本文提出了一个用于生成语义对抗攻击的框架，并使用扩散模型中的潜在空间中的语义信息。在该框架中，有两个变种方法：语义转换方法(ST)，通过微调潜在空间和/或扩散模型本身来生成图像；潜在屏蔽方法(LM)，利用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。实验结果验证了该框架的有效性。 |
| [^43] | [DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective.](http://arxiv.org/abs/2309.07396) | 本文重新思考了无监督对比句子嵌入学习，并从去偏见的角度提出了DebCSE方法。通过消除各种偏差，包括词频偏差、句子长度偏差和假负样本偏差，DebCSE旨在学习高质量的句子嵌入。 |
| [^44] | [Unleashing the Power of Depth and Pose Estimation Neural Networks by Designing Compatible Endoscopic Images.](http://arxiv.org/abs/2309.07390) | 本研究通过改进图像和神经网络的兼容性，引入了Mask Image Modelling (MIM)模块，从而能够充分发挥当前神经网络在内窥镜导航中的能力。 |
| [^45] | [The kernel-balanced equation for deep neural networks.](http://arxiv.org/abs/2309.07367) | 本文提出了深度神经网络的核平衡方程，解释了数据集分布估计中的不稳定性和尺度机制。网络的输出是数据集的局部平均，平均的尺度随着训练逐渐减小并导致不稳定性。 |
| [^46] | [Hodge-Aware Contrastive Learning.](http://arxiv.org/abs/2309.07364) | 本文提出了一种基于豪奇分解的对比学习方法，用于处理单纯复合体数据，并生成具有特定谱信息的嵌入。通过编码数据不变性和设计合适的增强方法，以及重新权衡负样本的重要性，我们得到了一个反映谱信息的嵌入空间。 |
| [^47] | [Efficient quantum recurrent reinforcement learning via quantum reservoir computing.](http://arxiv.org/abs/2309.07339) | 本研究提出了一种高效的量子循环强化学习方法，通过构建利用基于量子循环神经网络的储水池的QRL代理，解决了QRL与QRNN的低效训练问题。通过数值模拟验证了这种方法的有效性，并在标准基准测试中展示了其潜力。 |
| [^48] | [Learning from Auxiliary Sources in Argumentative Revision Classification.](http://arxiv.org/abs/2309.07334) | 该论文主要研究了在辩论写作中分类理想的推理修改的模型，并提出了多任务学习和迁移学习两种方法来利用辅助修订数据的来源。研究结果表明，这两种方法可以显著改善分类器的性能，并且迁移学习可以更好地表示数据之间的关系。 |
| [^49] | [Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining.](http://arxiv.org/abs/2309.07332) | 创新点：提出了一种基于可靠性的训练数据清洗方法，利用归纳性符合预测来纠正噪声训练数据中的错误标记和异常值。该方法在多个分类任务中验证有效性，并显著提升了分类性能。 |
| [^50] | [Traveling Words: A Geometric Interpretation of Transformers.](http://arxiv.org/abs/2309.07315) | 本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。 |
| [^51] | [AudioSR: Versatile Audio Super-resolution at Scale.](http://arxiv.org/abs/2309.07314) | 本论文介绍了一个基于扩散的生成模型AudioSR，可在各种音频类型上进行鲁棒的音频超分辨率处理。具体而言，它可以将2kHz到16kHz范围内的音频信号上采样为24kHz带宽的高分辨率音频信号，采样率为48kHz。 |
| [^52] | [Language-Conditioned Observation Models for Visual Object Search.](http://arxiv.org/abs/2309.07276) | 本研究提出了一种语言条件下的观测模型（LCOM），通过将目标搜索问题视为部分可观察的马尔可夫决策过程（POMDP），利用基于复杂语言描述的深度神经网络来动态生成物体检测器，从而解决了先前方法中需要为每个对象制作新检测器的问题。 |
| [^53] | [Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach.](http://arxiv.org/abs/2309.07265) | 本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。 |
| [^54] | [Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization.](http://arxiv.org/abs/2309.07235) | 本文提出了一种基于贝叶斯优化的TVM自动调优框架，使用TVM张量表达语言实现了LU分解、Cholesky分解和3mm等线性代数核。在GPU集群上的实验结果表明，该框架在大多数情况下优于传统的AutoTVM框架。 |
| [^55] | [Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck.](http://arxiv.org/abs/2309.07200) | 本文介绍了一种通过时间滞后信息瓶颈的方法，将复杂系统映射到简化表示空间并模拟时间上的大跳跃。实验证明该方法能够准确模拟原始过程的统计特性和动力学，优于现有的时间滞后降维方法。 |
| [^56] | [Predicting Survival Time of Ball Bearings in the Presence of Censoring.](http://arxiv.org/abs/2309.07188) | 本文提出了一种新的方法，使用生存分析来预测滚珠轴承的失效时间，并且通过分析频域数据和训练多个生存模型来实现。模型可以以时间为基础进行风险的概率预测，并允许比较不同组轴承的生存函数。 |
| [^57] | [A Health Monitoring System Based on Flexible Triboelectric Sensors for Intelligence Medical Internet of Things and its Applications in Virtual Reality.](http://arxiv.org/abs/2309.07185) | 本研究设计了一个基于柔性摩擦电传感器和深度学习的智能医疗物联网系统，用于帕金森病患者的监测和互动，包括位置/轨迹追踪、心率监测和身份识别。 |
| [^58] | [CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis.](http://arxiv.org/abs/2309.07178) | CloudBrain-NMR是一个智能云计算平台，用于NMR光谱处理和分析，通过在线访问，无需用户端安装任何程序，使用并行计算来加快处理时间。 |
| [^59] | [HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting.](http://arxiv.org/abs/2309.07174) | 本研究提出了HurriCast，一种使用机器学习和统计建模的自动化框架，通过组合ARIMA模型和K-MEANS算法以更好地捕捉飓风趋势，并结合Autoencoder进行改进的飓风模拟，从而有效模拟历史飓风行为并提供详细的未来预测。这项研究通过利用全面且有选择性的数据集，丰富了对飓风模式的理解，并为风险管理策略提供了可操作的见解。 |
| [^60] | [Exploring Large Language Models for Ontology Alignment.](http://arxiv.org/abs/2309.07172) | 本文研究了大型语言模型在本体对齐中的应用，并发现它们有潜力在谨慎的框架和提示设计下超越现有的本体对齐系统。 |
| [^61] | [Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis.](http://arxiv.org/abs/2309.07168) | 本文介绍了一种通过可达性分析在分层强化学习中对目标空间进行抽象的方法，以自动发现符号目标表示。该方法通过将具有相似任务角色的环境状态集合在一起的紧密关联表示来发现子目标，在导航任务中表现出可解释性和数据效率。 |
| [^62] | [Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion.](http://arxiv.org/abs/2309.07164) | 这个论文介绍了一种专为资源受限机器人设计的混合ASR系统，通过结合HMM和深度学习模型，有效提高语音识别准确性，并在不同的机器人平台上展示了实时和精确的识别能力，具有适应性和与低功耗硬件兼容的特点，为无缝人机交互提供了有前景的可能性。 |
| [^63] | [Recall-driven Precision Refinement: Unveiling Accurate Fall Detection using LSTM.](http://arxiv.org/abs/2309.07154) | 本文提出了一种使用LSTM的精确跌倒检测系统，通过结合加速度计和陀螺仪传感器以及修剪技术进行性能优化，系统具有高召回率和96％的特异度。 |
| [^64] | [Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2309.07153) | 本文提出了一种有效的深度强化学习模型，通过将图神经网络作为编码器、强化学习作为解码器，实现了在复杂网络中寻找影响者的任务上的优越性能，超过了传统的最佳影响力最大化算法。 |
| [^65] | [Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models.](http://arxiv.org/abs/2309.07149) | 本研究提出了一种创新方法，利用脑电图数据解码人脑中的视觉表示。通过将EEG数据转换为频谱图并使用卷积神经网络进行训练，结合基于知识蒸馏的图像分类教师网络，我们的模型在图像分类和重建任务上表现出色。 |
| [^66] | [ETP: Learning Transferable ECG Representations via ECG-Text Pre-training.](http://arxiv.org/abs/2309.07145) | 本论文介绍了ECG-Text预训练（ETP）框架，它通过将ECG信号与文本报告对齐，实现了跨模态ECG特征学习。ETP在线性评估和零样本分类任务中表现出色，并展示了其在跨模态ECG特征学习方面的鲁棒性和可迁移性。 |
| [^67] | [Design of Recognition and Evaluation System for Table Tennis Players' Motor Skills Based on Artificial Intelligence.](http://arxiv.org/abs/2309.07141) | 本研究基于人工智能，设计了一种用于识别和评估乒乓球运动员动作技能的系统，通过改进可穿戴设备，并利用特征工程、降维和不同评估指标的损失函数实现了动作的模式识别和层次化评估。 |
| [^68] | [Masked Transformer for Electrocardiogram Classification.](http://arxiv.org/abs/2309.07136) | 提出了一种基于掩码Transformer的ECG分类方法，命名为MTECG，扩展了掩码自动编码器在ECG时间序列上的应用，该方法在广泛的掩码比例下表现稳定良好，并进行了消融实验验证了重构目标的波动性、训练计划长度、逐层学习率衰减和DropPath率的重要性。 |
| [^69] | [Ontologies for increasing the FAIRness of plant research data.](http://arxiv.org/abs/2309.07129) | 本研究提出了一种增加植物研究数据FAIRness的本体论方法，利用语义标记和相关元数据增加数据的可重用性和互操作性。 |
| [^70] | [On the Local Quadratic Stability of T-S Fuzzy Systems in the Vicinity of the Origin.](http://arxiv.org/abs/2309.06841) | 本文介绍了一种新的局部稳定性条件，该条件基于线性矩阵不等式和二次Lyapunov函数，并结合了原点附近的非线性系统的线性结构，相比于现有方法更为准确和有效。同时，本文还提出了局部指数稳定性的必要和充分条件，并讨论了模糊Lyapunov方法的局限性。 |
| [^71] | [Uncertainty-aware Traffic Prediction under Missing Data.](http://arxiv.org/abs/2309.06800) | 本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。 |
| [^72] | [Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach.](http://arxiv.org/abs/2309.06604) | 本文提出了一种基于代理的层级机器学习平台，用于选择分布式组织的机器学习算法并同时调整其超参数。该方法具有可伸缩性、灵活性和鲁棒性，并支持自动化和协同的功能。 |
| [^73] | [Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs.](http://arxiv.org/abs/2309.05918) | 随机LLMs无法理解语言的原因是它们无法提供可以依赖的事实信息，它们存储的语言知识埋藏在无意义的微特征中，并在某些语言上下文中无法进行正确推理。本文建议在符号化方法中应用有效的自下而上策略 |
| [^74] | [Evaluating Chatbots to Promote Users' Trust -- Practices and Open Problems.](http://arxiv.org/abs/2309.05680) | 本文评估了当前聊天机器人测试的实践和开放问题，旨在解决和减轻与用户信任相关的服务或产品性能、用户满意度以及对社会的长期意外后果问题。 |
| [^75] | [CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning.](http://arxiv.org/abs/2309.04802) | CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。 |
| [^76] | [Physically Grounded Vision-Language Models for Robotic Manipulation.](http://arxiv.org/abs/2309.02561) | 该论文介绍了一个用于机器人操作的具有物理基础的视觉语言模型，通过在物体上微调模型，提高了模型对物理概念的理解，在语言交互框架中展现了良好的性能。 |
| [^77] | [A Survey on Interpretable Cross-modal Reasoning.](http://arxiv.org/abs/2309.01955) | 这篇论文调查了可解释的跨模态推理领域，主要目标是在实现高预测性能的同时为结果提供可理解的解释。调查提供了一种三级分类方法的综合概述，并回顾了现有的具有解释注释的CMR数据集，同时总结了I-CMR的挑战和未来发展方向。 |
| [^78] | [eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models.](http://arxiv.org/abs/2309.00964) | 本文提出了一种内存高效的DKM实现，即eDKM，用于大型语言模型的高效准确的训练时权重聚类方法。它通过减小DKM的内存占用，解决了LLM压缩中的训练开销问题。 |
| [^79] | [Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports.](http://arxiv.org/abs/2309.00917) | 本论文提出一种新颖的轻量级基于图的嵌入方法，用于对放射学报告进行多语言结构化表现。通过连接医学术语和多语言知识库，这种嵌入方法揭示了临床术语之间的关系，提供了对临床医生更易理解、临床更准确的表征。 |
| [^80] | [Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models.](http://arxiv.org/abs/2308.11764) | 本文介绍了一种用于评估和减少开源弱大语言模型中幻觉问题的框架，并探索了知识注入和师生方法等技术来减轻低参数模型中的幻觉问题，实验结果表明，在挑战性领域中，这些模型的幻觉问题得到了减少。 |
| [^81] | [Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction.](http://arxiv.org/abs/2308.08518) | 本文提出了一个具有点对注意力感知机制的双向对应预测网络，通过利用模型点和场景点之间的相关性进行点对匹配学习，解决了传统方法对观察质量和遮挡的依赖性问题，并在实验证明其在物体姿态估计任务上优于其他最先进的方法。 |
| [^82] | [Neural Categorical Priors for Physics-Based Character Control.](http://arxiv.org/abs/2308.07200) | 本文提出了一种新的基于神经分类先验的学习框架，用于控制基于物理的角色，利用强化学习和离散信息瓶颈，生成高质量逼真的行为。 |
| [^83] | [Exploring ChatGPT's Empathic Abilities.](http://arxiv.org/abs/2308.03527) | 这项研究探索了基于GPT-3.5的ChatGPT在展现共情回应和情感表达方面的能力。研究结果表明，在91.7%的情况下，ChatGPT能够准确识别情感并产生适当的回答。 |
| [^84] | [Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats.](http://arxiv.org/abs/2308.01921) | 该论文提出了一种可转移的图神经指纹模型，用于快速应对未来的生物威胁。通过利用包含30万种候选药物和23个冠状病毒蛋白靶的COVID-19药物对接数据集，训练了高通量虚拟COVID-19药物筛选的图神经指纹模型。与传统指纹方法相比，该模型在对接得分上具有较高的预测准确性，并且提出了可转移的图神经指纹方法，能够适用于未知的靶点。 |
| [^85] | [Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System.](http://arxiv.org/abs/2307.16834) | 本论文实现了一个端到端的视频异常检测系统，通过从监控视频输入进行犯罪现场异常检测，并在多个Jetson边缘设备上部署和运行。这是对Jetson平台在深度学习算法执行方面性能的基准测试分析的创新。 |
| [^86] | [Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version).](http://arxiv.org/abs/2307.14799) | 本研究通过混合ASP方法解决了实际半导体制造过程的调度问题，将其具体要求建模，并且实现了灵活的机器加工、设置、批处理和维护操作。 |
| [^87] | [Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations.](http://arxiv.org/abs/2307.10932) | 本文提出了一种名为同卵和异卵对比学习（IFTCL）框架，通过同时适应不同的正样本对生成方式，在无监督学习句子表示中解决了对比学习中存在的语义扭曲和语义间隔问题。 |
| [^88] | [Personality Traits in Large Language Models.](http://arxiv.org/abs/2307.00184) | 该研究介绍了一种综合方法，用于验证大型语言模型（LLMs）生成的文本中展示的人格特质。研究发现，部分LLMs在特定提示配置下模拟的人格可靠且有效，特别是对于更大和经过指导微调的模型。此外，LLMs的输出中的人格特质可以根据需要进行塑造。 |
| [^89] | [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning.](http://arxiv.org/abs/2306.14565) | 本论文通过引入第一个大型多样化的视觉指令调整数据集，提出了一种解决大规模多模态模型中幻觉问题的方法。通过设计包含正负指令的数据集和提出的评估方法，能够更准确地衡量模型产生的幻觉。 |
| [^90] | [COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models.](http://arxiv.org/abs/2306.05659) | 本文提出了一种启发式贪心对抗攻击，针对基于提示的模板在PLMs中可能存在的漏洞，通过字符级和单词级的破坏方法进行攻击，取得了较高的攻击成功率。 |
| [^91] | [Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization.](http://arxiv.org/abs/2306.05064) | 本文首次提出了一个地球科学领域的大型语言模型K2，并开发了各种资源以进一步促进其在地球科学领域中的研究和应用，包括第一个地球科学教学调音数据集GeoSignal和第一个地球科学基准测试GeoBenchmark。我们使用了完整的方法将预先训练的通用领域LLM LLaMA-7B 模型适应到地球科学领域。 |
| [^92] | [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought.](http://arxiv.org/abs/2305.15021) | EmbodiedGPT是一种端到端的多模态基础模型，通过思维链预训练的方式，赋予具有多模态理解和执行能力的实体代理人。 |
| [^93] | [SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction.](http://arxiv.org/abs/2305.11322) | 这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。 |
| [^94] | [PaLM 2 Technical Report.](http://arxiv.org/abs/2305.10403) | PaLM 2 是一种计算效率更高的最先进的语言模型，提供了更好的多语言和推理能力，并且通过使用多种目标进行训练，获得了在不同模型大小的下游任务上显着的改进质量。此外，PaLM 2 还展示了强大的推理能力和稳定的性能表现，使得模型能够更广泛地部署，并且可以控制毒性推理时间，而不会对其他能力产生影响。 |
| [^95] | [Sequential decomposition of propositional logic programs.](http://arxiv.org/abs/2304.13522) | 研究命题逻辑程序的序列分解，通过分析程序之间的Green关系，为逻辑编程代数理论进一步发展作出了贡献。 |
| [^96] | [Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence.](http://arxiv.org/abs/2304.12241) | 设计以幸福为导向的人工智能系统面临着知识和动机两方面的挑战，包括概念化、测量和优化幸福，设计适当的AI行动，以及激励措施、财务和宣传风险的不一致以及数据获取的缺乏。针对这些挑战，需要在科学理解AI系统对幸福影响方面进行研究，并指导设计行动。 |
| [^97] | [Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget.](http://arxiv.org/abs/2304.10520) | 本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。 |
| [^98] | [A Latent Space Theory for Emergent Abilities in Large Language Models.](http://arxiv.org/abs/2304.09960) | 本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。 |
| [^99] | [Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture.](http://arxiv.org/abs/2302.10848) | 该研究提出了一种基于深度强化学习的启发式方法，用于增强组合优化过程，并在自旋玻璃基态问题上取得了改进结果。研究结果表明，相对于传统方法如模拟退火或并行退火，强化学习方法在提供相当质量的结果之前减少了运行时间。 |
| [^100] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^101] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^102] | [TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering.](http://arxiv.org/abs/2212.04953) | TargetCall通过预基调过滤，消除了basecalling中的浪费计算，提高了基因组分析流程的效率。 |
| [^103] | [Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning.](http://arxiv.org/abs/2211.10851) | 这项研究表明，我们可以使用内在动机衡量标准而不依赖于奖励来创建一个具有自我保护能力的智能体。 |
| [^104] | [DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation.](http://arxiv.org/abs/2210.16591) | 本文提出了DisenPOI，一个新颖的基于双图的POI推荐解开框架，通过利用顺序和地理关系并使用自我监督解开这两种影响，以提高推荐性能和可解释性。 |
| [^105] | [ConSpec: honing in on critical steps for rapid learning and generalization in RL.](http://arxiv.org/abs/2210.05845) | ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。 |
| [^106] | [BAFFLE: Backdoor Attack in Offline Reinforcement Learning.](http://arxiv.org/abs/2210.04688) | 本文研究离线增强学习中的后门攻击，通过向数据中添加扰动，使得智能体在注入触发器的观测值上采取低奖励动作，从而提出了BAFFLE方法。 |
| [^107] | [LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings.](http://arxiv.org/abs/2210.00305) | LambdaKG是一个基于预训练语言模型的知识图谱嵌入库，提供了多个预训练语言模型和支持多种任务，如知识图谱补全、问答、推荐和知识探索。 |
| [^108] | [Machine Learning and Computer Vision Techniques in Bee Monitoring Applications.](http://arxiv.org/abs/2208.00085) | 本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。 |
| [^109] | [An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning.](http://arxiv.org/abs/2206.03420) | 该论文提出了一种适应性联邦相关框架，用于处理时空数据中的复杂预测任务，并解决了同时融合空间信息的相互依赖性和动态的时间变化的挑战。 |
| [^110] | [Meta-Learning Regrasping Strategies for Physical-Agnostic Objects.](http://arxiv.org/abs/2205.11110) | 本研究提出了一种元学习算法，ConDex，用于自主识别具有未知物理属性的非均匀对象，并实现精确的抓取点估计。与现有方法相比，ConDex在性能上表现出优势，并且生成了两个新的对象数据集用于进一步研究。 |
| [^111] | [Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning.](http://arxiv.org/abs/2202.10629) | 模型重新编程是一种资源高效的跨领域机器学习方法，通过重新利用和重用预训练模型，无需模型细调即可在目标领域解决任务。这种方法在许多应用中优于迁移学习和从头训练。 |
| [^112] | [PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning.](http://arxiv.org/abs/2202.03609) | 本文提出了PolicyCleanse方法，用于检测和缓解多智能体强化学习系统中的后门攻击。该方法基于激活的特洛伊智能体累积奖励的下降特性进行检测，并尝试缓解其特洛伊行为。 |
| [^113] | [Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze.](http://arxiv.org/abs/2112.07611) | 通过建立$S_n$-等变卷积量子Ansatze，我们证明了其能够在具有SU($d$)对称性的广泛量子机器学习问题中生成任意幺正矩阵，同时验证了4-local SU($d$)对称幺正矩阵的可实现性。 |

# 详细

[^1]: MMICL：多模态上下文学习增强视觉-语言模型

    MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])

    [http://arxiv.org/abs/2309.07915](http://arxiv.org/abs/2309.07915)

    MMICL提出了一种用于视觉-语言模型的架构和训练数据设计，以解决VLM在理解复杂多模态提示方面的困难。

    

    从深度学习的复苏开始，借助大型语言模型（LLM）的视觉-语言模型（VLM）变得非常流行。然而，尽管LLM可以利用丰富的背景知识和任务信息进行上下文学习，大多数VLM在理解复杂的多模态提示（包含多个图像）方面仍然面临困难。这个问题可以追溯到VLM的架构设计或预训练数据。具体来说，当前的VLM主要强调利用带有单个图像的多模态数据，而不是带有交错多个图像和文本的多模态提示。尽管一些新提出的VLM可以处理带有多个图像的用户提示，但预训练数据没有提供比从Web抓取时交错图像和文本更复杂的多模态提示。我们提出了MMICL，从模型和数据的角度来解决这个问题。我们引入了一个精心设计的架构，能够无缝地集成视觉和语言信息，并提供更丰富的多模态训练数据。

    Starting from the resurgence of deep learning, vision-language models (VLMs) benefiting from large language models (LLMs) have never been so popular. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images. The issue can traced back to the architectural design of VLMs or pre-training data. Specifically, the current VLMs primarily emphasize utilizing multi-modal data with a single image some, rather than multi-modal prompts with interleaved multiple images and text. Even though some newly proposed VLMs could handle user prompts with multiple images, pre-training data does not provide more sophisticated multi-modal prompts than interleaved image and text crawled from the web. We propose MMICL to address the issue by considering both the model and data perspectives. We introduce a well-designed architecture capable of seamlessly integrating vis
    
[^2]: Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    [http://arxiv.org/abs/2309.07867](http://arxiv.org/abs/2309.07867)

    beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。

    

    我们引入了beta扩散，一种将去掩盖和去噪集成到一起的新型生成建模方法，用于在有界范围内生成数据。使用了缩放和偏移的beta分布，beta扩散利用了随时间的乘法转换来创建正向和反向的扩散过程，同时维持着正向边缘分布和反向条件分布，给定任意时间点的数据。与传统的基于扩散的生成模型不同，传统模型依赖于加性高斯噪声和重新加权的证据下界（ELBO），beta扩散是乘法的，并且通过从KL散度的凸性推导出来的KL散度上界（KLUB）进行优化。我们证明了所提出的KLUB相对于负ELBO来说对于优化beta扩散更加有效，负ELBO也可以作为相同KL散度的KLUB，只是其两个参数交换了位置。beta扩散的损失函数以Bregman散度为指标来表示。

    We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
    
[^3]: 基于大型语言模型的代理的崛起和潜力：一项调查

    The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])

    [http://arxiv.org/abs/2309.07864](http://arxiv.org/abs/2309.07864)

    基于大型语言模型的代理的崛起和潜力：一项调查。大型语言模型被认为是构建通用人工智能代理的潜在催化剂，许多研究已经取得重要进展。

    

    长期以来，人类一直追求人工智能（AI）达到或超越人类水平的目标，而被认为是实现这一目标的有望方式的AI代理。AI代理是能感知环境、做出决策和采取行动的人工实体。自20世纪中叶以来，人们为开发智能AI代理进行了许多努力。然而，这些努力主要集中在算法或训练策略的进步上，以增强特定能力或在特定任务上的性能。实际上，社区所缺乏的是一个足够通用和强大的模型，作为设计能适应各种场景的AI代理的起点。由于展示出的多功能和显著能力，大型语言模型（LLMs）被视为人工通用智能（AGI）的潜在催化剂，为构建通用AI代理提供了希望。许多研究工作利用LLMs作为构建AI代理的基础，并且已经取得重要的进展。

    For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
    
[^4]: CiwaGAN: 声韵学信息交流

    CiwaGAN: Articulatory information exchange. (arXiv:2309.07861v1 [cs.SD])

    [http://arxiv.org/abs/2309.07861](http://arxiv.org/abs/2309.07861)

    本文介绍了CiwaGAN模型，该模型结合了无监督声韵学建模和无监督听觉模态信息交流，是对人类口语习得最现实的近似。

    

    人类通过控制发音器官将信息编码成声音，并通过听觉装置解码声音的信息。本文介绍了CiwaGAN，这是一个结合了无监督声韵学建模和无监督听觉模态信息交流的人类口语习得模型。尽管之前的研究分别包括了无监督声韵学建模和信息交流，但我们的模型是第一个将这两个组成部分结合在一起的。本文还提出了一个改进的声韵学模型，具有更可解释的内部表示。提出的CiwaGAN模型是使用深度学习对人类口语习得进行最现实的近似。因此，它对于认知上可行的人类言语行为模拟是有用的。

    Humans encode information into sounds by controlling articulators and decode information from sounds using the auditory apparatus. This paper introduces CiwaGAN, a model of human spoken language acquisition that combines unsupervised articulatory modeling with an unsupervised model of information exchange through the auditory modality. While prior research includes unsupervised articulatory modeling and information exchange separately, our model is the first to combine the two components. The paper also proposes an improved articulatory model with more interpretable internal representations. The proposed CiwaGAN model is the most realistic approximation of human spoken language acquisition using deep learning. As such, it is useful for cognitively plausible simulations of the human speech act.
    
[^5]: ExpertQA: 专家策划的问题和带有属性的答案

    ExpertQA: Expert-Curated Questions and Attributed Answers. (arXiv:2309.07852v1 [cs.CL])

    [http://arxiv.org/abs/2309.07852](http://arxiv.org/abs/2309.07852)

    本论文介绍了ExpertQA，它是一个专家策划的问题和带有属性的答案系统。该系统通过分析语言模型在领域特定情景中提供的事实准确性和归因等方面来确保提供准确的信息。研究还收集了领域专家的问题并要求他们评估生成的答案。这项工作的目的是确保语言模型在高风险领域中不会传播错误信息，从而避免不良的社会后果。

    

    随着语言模型被越来越复杂和多样化的用户所采用，确保它们提供基于可验证来源的事实准确信息的重要性在各个领域的研究和职业中都是至关重要的。这特别适用于医学和法律等高风险领域，因为传播错误信息的风险较高，可能导致不良的社会后果。先前的研究关注于事实性和归因方面，并未专注于分析语言模型在特定领域情景中的这些特征。在这项工作中，我们通过将领域专家纳入其中，提出了一个评估研究，分析来自几个系统的响应中提供的事实准确性和归因的各个方面。具体而言，我们先从32个学科领域的484名参与者中收集由专家策划的问题，然后要求这些专家评估对他们自己问题的产生的响应。我们还要求专家修改产生的答案。

    As language models are adapted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study & professions. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying factuality and attribution has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we present an evaluation study analyzing various axes of factuality and attribution provided in responses from a few systems, by bringing domain experts in the loop. Specifically, we first collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. We also ask experts to revise answers produce
    
[^6]: 将深度学习应用于校准随机波动性模型

    Applying Deep Learning to Calibrate Stochastic Volatility Models. (arXiv:2309.07843v1 [q-fin.CP])

    [http://arxiv.org/abs/2309.07843](http://arxiv.org/abs/2309.07843)

    本研究将深度学习技术应用于校准随机波动性模型，通过训练神经网络对基于Heston模型的标的资产进行定价，并且在校准方面取得了快速和准确的结果。

    

    随机波动性模型是一种波动率是随机过程的模型，可以捕捉到隐含波动率曲面的大部分基本特征，并提供更真实的波动率笑曲线或偏斜动态。然而，它们存在一个重要问题，即校准时间过长。最近，基于深度学习（DL）技术的替代校准方法已被用于构建快速且准确的校准解决方案。Huge和Savine开发了一种差分深度学习（DDL）方法，该方法在样本中训练了机器学习模型，其中样本不仅包括特征和标签，还包括标签对特征的微分。本研究旨在将DDL技术应用于定价基本欧洲期权（即校准工具），具体而言，是在基于Heston模型的标的资产上定价看涨期权，并使用训练好的网络对模型进行校准。DDL可以实现快速训练和准确定价。训练好的神经网络戏剧性地

    Stochastic volatility models, where the volatility is a stochastic process, can capture most of the essential stylized facts of implied volatility surfaces and give more realistic dynamics of the volatility smile or skew. However, they come with the significant issue that they take too long to calibrate.  Alternative calibration methods based on Deep Learning (DL) techniques have been recently used to build fast and accurate solutions to the calibration problem. Huge and Savine developed a Differential Deep Learning (DDL) approach, where Machine Learning models are trained on samples of not only features and labels but also differentials of labels to features. The present work aims to apply the DDL technique to price vanilla European options (i.e. the calibration instruments), more specifically, puts when the underlying asset follows a Heston model and then calibrate the model on the trained network. DDL allows for fast training and accurate pricing. The trained neural network dramatic
    
[^7]: 两步走：采用双层方法修复智能合约

    Two Timin': Repairing Smart Contracts With A Two-Layered Approach. (arXiv:2309.07841v1 [cs.CR])

    [http://arxiv.org/abs/2309.07841](http://arxiv.org/abs/2309.07841)

    本文提出了一个双层框架来处理智能合约中的漏洞。通过将Slither的漏洞报告与源代码结合，结合预训练的分类器和大型语言模型，实现了智能合约的分类和修复。实验证明了该方法的有效性，可以显著减少漏洞数量。

    

    鉴于区块链技术的现代重要性，智能合约既带来了巨大的风险，也带来了巨大的好处。其中的漏洞可以引发一系列后果，导致重大损失。许多当前的论文主要集中在对恶意意图进行智能合约的分类，通常依赖于有限的合约特征，如字节码或操作码。本文提出了一个新颖的双层框架：1）进行分类和2）直接修复恶意合约。将Slither的漏洞报告与源代码结合，并通过预训练的随机森林分类器（RFC）和大型语言模型（LLMs）传递，对每个建议的漏洞进行分类和修复。实验证明了经过细调和及时修复工程的LLMs的有效性。基于预训练的GPT-3.5-Turbo和经过细调的Llama-2-7B模型构建的智能合约修复模型分别将整体漏洞计数减少了97.5%和96.7%。对修复的手动检查表明，修复模型的修复正确率高达90.3%。

    Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither's vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repair
    
[^8]: 使用离线强化学习的全向腿式机器人在室外植被中进行导航的VAPOR方法

    VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning. (arXiv:2309.07832v1 [cs.RO])

    [http://arxiv.org/abs/2309.07832](http://arxiv.org/abs/2309.07832)

    VAPOR是一种使用离线强化学习的方法，用于在室外复杂植被环境中进行全向腿式机器人的自主导航。通过从未标记的真实植被数据中训练，该方法学习了植被的物理和几何特性，并使用一个自适应规划器生成动态可行机器人动作，在狭窄通道中导航，并避免被高草和灌木丛所困住。在实验中，我们观察到成功率有所提高。

    

    我们提出了一种名为VAPOR的方法，该方法使用离线强化学习（RL）在结构混乱、密集植被的室外环境中实现全向腿式机器人的自主导航。该方法使用从三维LiDAR点云导出的高度和强度基于成本地图、目标成本地图以及经过处理的固有感知数据作为状态输入，从未标记的实际室外植被数据中训练出一种新的RL策略。该策略学习了周围植被的物理和几何特性，如高度、密度和坚实度/刚性，以进行导航。与使用端到端策略动作不同，完全训练好的RL策略的Q网络用于评估由一种新的自适应规划器生成的动态可行机器人动作，该规划器能够穿过狭窄的通道并防止被植被，例如高草和灌木丛所困住。我们在复杂的室外植被上展示了该方法的能力。我们观察到成功率有所提高。

    We present VAPOR, a novel method for autonomous legged robot navigation in unstructured, densely vegetated outdoor environments using Offline Reinforcement Learning (RL). Our method trains a novel RL policy from unlabeled data collected in real outdoor vegetation. This policy uses height and intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map, and processed proprioception data as state inputs, and learns the physical and geometric properties of the surrounding vegetation such as height, density, and solidity/stiffness for navigation. Instead of using end-to-end policy actions, the fully-trained RL policy's Q network is used to evaluate dynamically feasible robot actions generated from a novel adaptive planner capable of navigating through dense narrow passages and preventing entrapment in vegetation such as tall grass and bushes. We demonstrate our method's capabilities on a legged robot in complex outdoor vegetation. We observe an improvement in success rates
    
[^9]: 从卫星图像中进行道路提取的大规模弱监督学习

    Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery. (arXiv:2309.07823v1 [cs.CV])

    [http://arxiv.org/abs/2309.07823](http://arxiv.org/abs/2309.07823)

    本文提出一种利用OpenStreetMap道路数据作为弱标签和大规模卫星图像预训练语义分割模型的方法，实验证明对于道路提取而言，预测准确度随着弱标注数据量和道路密度的增加而增加。

    

    使用深度学习从卫星图像中自动提取道路是传统手工绘制的可行替代方案。因此，它最近受到了相当大的关注。然而，大部分现有方法都是有监督的，需要像素级别的标注，这一过程繁琐且容易出错。更糟糕的是，地球上的地形、植被和人造物体有着各种各样的差异。众所周知，从一个地区训练出的模型往往很难泛化到其他地区。不同的拍摄条件，如光照和角度，以及不同的图像处理技术进一步增加了这个问题的复杂性。开发覆盖所有图像风格的训练数据是不现实的。本文提出利用OpenStreetMap道路数据作为弱标签和大规模卫星图像进行预训练语义分割模型。我们广泛的实验结果表明，预测准确度随着弱标注数据量的增加以及地区道路密度的增加而增加。

    Automatic road extraction from satellite imagery using deep learning is a viable alternative to traditional manual mapping. Therefore it has received considerable attention recently. However, most of the existing methods are supervised and require pixel-level labeling, which is tedious and error-prone. To make matters worse, the earth has a diverse range of terrain, vegetation, and man-made objects. It is well known that models trained in one area generalize poorly to other areas. Various shooting conditions such as light and angel, as well as different image processing techniques further complicate the issue. It is impractical to develop training data to cover all image styles. This paper proposes to leverage OpenStreetMap road data as weak labels and large scale satellite imagery to pre-train semantic segmentation models. Our extensive experimental results show that the prediction accuracy increases with the amount of the weakly labeled data, as well as the road density in the areas 
    
[^10]: 提升模仿学习用于自动驾驶的交通规则遵守的关键因素

    What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])

    [http://arxiv.org/abs/2309.07808](http://arxiv.org/abs/2309.07808)

    本文提出了一种基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能，并解决了交通规则遵守和传感器感知问题。

    

    最近越来越多的研究关注于全端到端的自动驾驶技术，在这种技术中，整个驾驶流程被替换为一个简单的神经网络，由于其结构简单和推理时间快，因此变得非常吸引人。尽管这种方法大大减少了驾驶流程中的组件，但其简单性也导致解释性问题和安全问题。训练得到的策略并不总是符合交通规则，同时也很难发现其错误的原因，因为缺乏中间输出。同时，传感器对于自动驾驶的安全性和可行性也至关重要，可以帮助感知复杂驾驶场景下的周围环境。本文提出了一种全新的基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能。我们对模型的性能进行了评估。

    More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
    
[^11]: 变分量子线性求解器增强的量子支持向量机

    Variational Quantum Linear Solver enhanced Quantum Support Vector Machine. (arXiv:2309.07770v1 [quant-ph])

    [http://arxiv.org/abs/2309.07770](http://arxiv.org/abs/2309.07770)

    提出了一种变分量子线性求解器增强的量子支持向量机算法，能够在嘈杂中间规模量子设备上进行可扩展的分类任务。算法利用变分量子线性求解器解决了最小二乘支持向量机的线性方程组，并在Iris数据集上进行了验证。通过经典计算和量子计算的策略性组合，算法在不同维度的特征空间中表现出了实用性和有效性。

    

    量子支持向量机在使用量子资源进行监督学习任务（如分类）中起着重要作用。然而，当前的方法在嘈杂中间规模量子（NISQ）设备的可扩展性方面存在严重限制。在本研究中，我们提出了一种新的方法，称为变分量子线性求解器（VQLS）增强的量子支持向量机。这是基于我们利用变分量子线性求解器在NISQ设备上解决最小二乘SVM的线性方程组的思想构建的。我们通过对Iris数据集的大量数值实验评估了我们方法的实现，该数据集包含三个不同的鸢尾植物物种。基于此，我们通过构建一个能够在从一维到七维的特征空间中进行分类的分类器，探索了我们算法的实用性和有效性。此外，通过策略性地利用经典计算和量子计算对各种子程序进行优化，我们的算法在NISQ设备上实现了可扩展的QSVM。

    Quantum Support Vector Machines (QSVM) play a vital role in using quantum resources for supervised machine learning tasks, such as classification. However, current methods are strongly limited in terms of scalability on Noisy Intermediate Scale Quantum (NISQ) devices. In this work, we propose a novel approach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM. This is built upon our idea of utilizing the variational quantum linear solver to solve system of linear equations of a least squares-SVM on a NISQ device. The implementation of our approach is evaluated by an extensive series of numerical experiments with the Iris dataset, which consists of three distinct iris plant species. Based on this, we explore the practicality and effectiveness of our algorithm by constructing a classifier capable of classification in a feature space ranging from one to seven dimensions. Furthermore, by strategically exploiting both classical and quantum computing for various subroutines of
    
[^12]: PRE: 视觉-语言提示学习与重新参数化编码器

    PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])

    [http://arxiv.org/abs/2309.07760](http://arxiv.org/abs/2309.07760)

    这项工作提出了一种名为PRE的方法，通过重新参数化编码器来增强可学习提示的泛化能力，从而解决了大型预训练视觉-语言模型中手动提示工程的挑战。

    

    大型预训练的视觉-语言模型（如CLIP）已经展示出在零样本迁移任务中具有巨大潜力。然而，为了达到最佳性能，需要手动选择提示以改进下游图像分布和文本类描述之间的对齐。这种手动提示工程是将这些模型部署到实践中的主要挑战，因为它需要领域专业知识并且非常耗时。为了避免复杂的提示工程，最近的CoOp工作引入了在视觉领域使用可控文本标记的提示学习概念。虽然CoOp可以在手动提示上取得显著改进，但其学到的上下文在同一数据集中更广泛的未见类别中的泛化能力较差。在这项工作中，我们提出了一种名为Prompt Learning with Reparameterization Encoder (PRE) 的简单高效的方法，改进了可学习提示的泛化能力。

    Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
    
[^13]: 利用集成的LLM方法进行生成式AI文本分类

    Generative AI Text Classification using Ensemble LLM Approaches. (arXiv:2309.07755v1 [cs.CL])

    [http://arxiv.org/abs/2309.07755](http://arxiv.org/abs/2309.07755)

    本文提出了一个利用集成的LLM方法进行生成式AI文本分类的方法，通过使用多个预训练LLM生成概率作为特征来准确检测AI生成的语言，并确定生成文本的特定语言模型的归属。

    

    大型语言模型（LLM）在人工智能和自然语言处理任务中表现出色，如内容创作、报告生成等。然而，对这些模型的不受限制的恶意应用可能导致不良后果，如虚假新闻生成、抄袭等。因此，在负责任地使用LLM时准确检测AI生成的语言非常重要。在本研究中，我们探讨了以下问题：1）某个文本是由AI生成还是人类编写的；2）特定语言模型在生成文本中的归属。我们考虑了英文和西班牙文的文本。本研究中使用的数据集是Automated Text Identification (AuTexTification)共享任务的一部分。针对上述每个研究目标，我们提出了一个基于集成神经模型的方法，该方法从不同的预训练LLM生成概率，这些概率被用作传统机器学习方法的特征。

    Large Language Models (LLMs) have shown impressive performance across a variety of Artificial Intelligence (AI) and natural language processing tasks, such as content creation, report generation, etc. However, unregulated malign application of these models can create undesirable consequences such as generation of fake news, plagiarism, etc. As a result, accurate detection of AI-generated language can be crucial in responsible usage of LLMs. In this work, we explore 1) whether a certain body of text is AI generated or written by human, and 2) attribution of a specific language model in generating a body of text. Texts in both English and Spanish are considered. The datasets used in this study are provided as part of the Automated Text Identification (AuTexTification) shared task. For each of the research objectives stated above, we propose an ensemble neural model that generates probabilities from different pre-trained LLMs which are used as features to a Traditional Machine Learning (T
    
[^14]: AIDPS:自适应水声传感器网络入侵检测与防御系统

    AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks. (arXiv:2309.07730v1 [cs.CR])

    [http://arxiv.org/abs/2309.07730](http://arxiv.org/abs/2309.07730)

    本文提出了一个自适应的分布式水声传感器网络入侵检测与防御系统（AIDPS），该系统能够提高水声传感器网络的安全性，有效地检测水下相关攻击，并且通过使用多种机器学习算法进行实验证明了其有效性。

    

    水声传感器网络主要用于水下环境，并在许多领域中应用。然而，水下环境的不稳定和挑战性、传感器节点资源有限且缺乏安全考虑使得水声传感器网络容易受到攻击。本文提出一种自适应分布式的水声传感器网络入侵检测与防御系统（AIDPS），该系统可以提高水声传感器网络的安全性，有效地检测水下攻击。为确定所提出建筑的最有效配置，我们使用了几种先进的机器学习算法（如自适应随机森林（ARF）、轻量级梯度提升机和K近邻算法）和概念漂移检测进行了一系列实验。

    Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for underwater environments and find applications in many areas. However, a lack of security considerations, the unstable and challenging nature of the underwater environment, and the resource-constrained nature of the sensor nodes used for UW-ASNs (which makes them incapable of adopting security primitives) make the UW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised Intrusion Detection and Prevention System called AIDPS for UW-ASNs. The proposed AIDPS can improve the security of the UW-ASNs so that they can efficiently detect underwater-related attacks (e.g., blackhole, grayhole and flooding attacks). To determine the most effective configuration of the proposed construction, we conduct a number of experiments using several state-of-the-art machine learning algorithms (e.g., Adaptive Random Forest (ARF), light gradient-boosting machine, and K-nearest neighbours) and concept drift detection
    
[^15]: NutritionVerse: 各种饮食摄入估计方法的实证研究

    NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. (arXiv:2309.07704v1 [cs.CV])

    [http://arxiv.org/abs/2309.07704](http://arxiv.org/abs/2309.07704)

    该论文介绍了NutritionVerse-Synth，这是一个拥有大规模合成食物图像数据集，其中包含了多种视角、模态和饮食注释，旨在解决目前饮食摄入估计方法的准确性和真实性问题。

    

    准确的饮食摄入估计对于支持健康饮食的政策和程序至关重要，因为营养不良与生活质量下降直接相关。然而，诸如食物日记之类的自我报告方法存在显著偏差。其他传统的饮食评估技术和新兴的替代方法，如移动应用程序，耗时长，并且可能需要受过训练的人员。最近的研究集中于使用计算机视觉和机器学习来从食物图像中自动估计饮食摄入量，但缺乏具有多样视角、模态和食物注释的综合数据集限制了这种方法的准确性和真实性。为了解决这个局限性，我们引入了NutritionVerse-Synth，这是第一个拥有84,984个逼真的合成2D食物图像及相关饮食信息和多模态标注的大规模数据集（包括深度图像、实例掩膜和语义掩膜）。

    Accurate dietary intake estimation is critical for informing policies and programs to support healthy eating, as malnutrition has been directly linked to decreased quality of life. However self-reporting methods such as food diaries suffer from substantial bias. Other conventional dietary assessment techniques and emerging alternative approaches such as mobile applications incur high time costs and may necessitate trained personnel. Recent work has focused on using computer vision and machine learning to automatically estimate dietary intake from food images, but the lack of comprehensive datasets with diverse viewpoints, modalities and food annotations hinders the accuracy and realism of such methods. To address this limitation, we introduce NutritionVerse-Synth, the first large-scale dataset of 84,984 photorealistic synthetic 2D food images with associated dietary information and multimodal annotations (including depth images, instance masks, and semantic masks). Additionally, we col
    
[^16]: 不确定思维树：大型语言模型的推理方法

    Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])

    [http://arxiv.org/abs/2309.07694](http://arxiv.org/abs/2309.07694)

    本研究提出了一种针对大型语言模型的推理框架——不确定思维树（TouT），它通过利用蒙特卡洛丢弃来量化中间步骤上的本地不确定性，提高了模型生成响应的精确性。

    

    尽管最近引入的不确定思维树（Tree of Thoughts, ToT）在允许大型语言模型（LLMs）通过预见和回溯进行全局决策方面取得了进展，但它忽视了中间决策点或“思维”中的固有局部不确定性。这些固有的局部不确定性，由于LLMs潜在的多样性响应能力，成为推理过程中的重要问题。为了解决这一关键差距，我们引入了不确定思维树（Tree of Uncertain Thoughts, TouT）-一种针对LLMs设计的推理框架。我们的TouT有效地利用了蒙特卡洛丢弃(Monte Carlo Dropout)来量化与LLMs在这些中间步骤上的不同本地响应相关的不确定性得分。通过将这种本地不确定性量化与全局搜索算法结合起来，TouT提高了模型生成响应的精确性。我们通过在两个苛刻的规划任务上进行严格实验证明了我们的方法：24点游戏和迷你填字游戏。

    While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence 
    
[^17]: 检测ChatGPT：对检测ChatGPT生成文本状态的调查

    Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text. (arXiv:2309.07689v1 [cs.CL])

    [http://arxiv.org/abs/2309.07689](http://arxiv.org/abs/2309.07689)

    这篇调查论文概述了当前用于区分ChatGPT生成文本和人工文本的方法和数据集，并总结了研究发现。

    

    随着ChatGPT（OpenAI, 2022）等生成语言模型能力和广泛使用的不断进步，它们生成流利的类人文本带来了各种好处，但区分人类生成文本和大型语言模型（LLM）生成文本的任务也因此成为一个关键问题。这些模型可能通过生成看似由人类生成的人工文本而进行欺骗。在法律、教育和科学等领域，确保文本的真实性尤为重要，因此这个问题尤为显著。本调查概述了当前用于区分人工文本和ChatGPT生成文本的方法，我们介绍了构建用于检测ChatGPT生成文本的不同数据集，使用的各种方法，进行了哪些关于人类与ChatGPT生成文本特征的定性分析，并总结了我们的研究结果。

    While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings int
    
[^18]: deepFDEnet: 一个用于求解分数阶微分方程的新型神经网络架构

    deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations. (arXiv:2309.07684v1 [cs.LG])

    [http://arxiv.org/abs/2309.07684](http://arxiv.org/abs/2309.07684)

    deepFDEnet是一种新型神经网络架构，能够准确求解各种形式的分数阶微分方程。

    

    本研究的主要目标是提出一种新颖的深度神经网络架构，能够准确地求解分数阶微分方程。该设计使用了高斯积分规则和$L_1$离散化技术。在每个方程中，使用深度神经网络近似未知函数。研究还对三个形式的分数阶微分方程进行了检验，以突出该方法的多样性：分数阶常微分方程、分数阶积分微分方程和分数阶偏微分方程。结果表明，所提出的架构能够以极高的精度解决不同形式的分数阶微分方程。

    The primary goal of this research is to propose a novel architecture for a deep neural network that can solve fractional differential equations accurately. A Gaussian integration rule and a $L_1$ discretization technique are used in the proposed design. In each equation, a deep neural network is used to approximate the unknown function. Three forms of fractional differential equations have been examined to highlight the method's versatility: a fractional ordinary differential equation, a fractional order integrodifferential equation, and a fractional order partial differential equation. The results show that the proposed architecture solves different forms of fractional differential equations with excellent precision.
    
[^19]: 评估大规模语言模型的性质：对人类中心主义的警告

    Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])

    [http://arxiv.org/abs/2309.07683](http://arxiv.org/abs/2309.07683)

    通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。

    

    生成式人工智能模型通过OpenAI的聊天机器人ChatGPT的发布引起了公众的关注和猜测。目前存在两种意见阵营：一方对这些模型为人类任务带来的基本变革的可能性感到兴奋，另一方对这些模型的强大能力感到高度关切。为了应对这些关切，我们使用了标准、规范化和经过验证的认知和个性测量工具来评估GPT3.5。在这个初步项目中，我们开发了一套测试，可以估计这些模型的能力边界，它们在短时间内的稳定性以及与人类的比较。我们的结果表明，GPT 3.5很可能没有产生意识，尽管它对个性问卷的回答能力令人感兴趣。它在重复观察过程中显示出认知和个性测量方面的大量变异，这与具有人类般个性的模型是不符合预期的。

    Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
    
[^20]: 多源领域自适应的联邦数据集字典学习

    Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])

    [http://arxiv.org/abs/2309.07670](http://arxiv.org/abs/2309.07670)

    本文提出了一种用于联邦领域自适应的方法，通过字典学习经验分布来解决客户端间分布偏移和部分无标签数据的问题。该方法通过设计协作通信协议和聚合操作，保护了客户端数据隐私，并成功在目标领域生成了标记数据。

    

    本文提出了一种联邦领域自适应的方法，该方法在客户端中存在分布偏移且部分客户端具有无标签的数据。所提出的框架FedDaDiL通过字典学习经验分布来解决这一挑战。在我们的设置中，客户端的分布代表着特定的领域，而FedDaDiL则共同训练了一个联邦经验分布字典。具体而言，我们在数据集字典学习框架上设计了协作通信协议和聚合操作。所选择的协议保护了客户端的数据隐私，相比于集中式方法提高了整体隐私性。我们通过对Caltech-Office、TEP和CWRU基准数据集进行了大量实验证明了我们的方法成功地在目标领域生成了标记数据。此外，我们还将我们的方法与其集中式方法和其他联邦领域基准进行了比较。

    In this article, we propose an approach for federated domain adaptation, a setting where distributional shift exists among clients and some have unlabeled data. The proposed framework, FedDaDiL, tackles the resulting challenge through dictionary learning of empirical distributions. In our setting, clients' distributions represent particular domains, and FedDaDiL collectively trains a federated dictionary of empirical distributions. In particular, we build upon the Dataset Dictionary Learning framework by designing collaborative communication protocols and aggregation operations. The chosen protocols keep clients' data private, thus enhancing overall privacy compared to its centralized counterpart. We empirically demonstrate that our approach successfully generates labeled data on the target domain with extensive experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks. Furthermore, we compare our method to its centralized counterpart and other benchmarks in federated doma
    
[^21]: 多源领域适应通过数据集字典学习遇见数据集蒸馏

    Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])

    [http://arxiv.org/abs/2309.07666](http://arxiv.org/abs/2309.07666)

    本文介绍了一个新问题，称为多源领域适应通过数据集字典学习遇见数据集蒸馏（MSDA-DD），通过适应先前的方法以及分配匹配方法，我们实现了仅使用每类1个样本即可实现最先进的适应性能。

    

    本文考虑了机器学习中两个问题的交集：多源领域适应（MSDA）和数据集蒸馏（DD）。一方面，前者考虑了将多个异质的标注源领域适应到一个未标注的目标领域。另一方面，后者攻击了关于合成一个包含有关数据集的所有信息的小摘要的问题。因此，我们提出了一个新问题称为MSDA-DD。为了解决这个问题，我们适应了MSDA文献中的先前作品，如Wasserstein Barycenter Transport和数据集字典学习，以及DD方法Distribution Matching。我们在四个基准测试集上对这个新问题进行了彻底的实验（Caltech-Office 10， Tennessee-Eastman Process， Continuous Stirred Tank Reactor和Case Western Reserve University），在这些实验中，我们展示了即使每类只有1个样本，我们也达到了最先进的适应性能。

    In this paper, we consider the intersection of two problems in machine learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD). On the one hand, the first considers adapting multiple heterogeneous labeled source domains to an unlabeled target domain. On the other hand, the second attacks the problem of synthesizing a small summary containing all the information about the datasets. We thus consider a new problem called MSDA-DD. To solve it, we adapt previous works in the MSDA literature, such as Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD method Distribution Matching. We thoroughly experiment with this novel problem on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous Stirred Tank Reactor, and Case Western Reserve University), where we show that, even with as little as 1 sample per class, one achieves state-of-the-art adaptation performance.
    
[^22]: 学习到排名中的特征工程在社区问答任务中的应用

    Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])

    [http://arxiv.org/abs/2309.07610](http://arxiv.org/abs/2309.07610)

    本文研究了在社区问答任务中学习到排名中的特征工程的几个方面。首先，引入了基于BERT的特征，捕捉语义相似性；其次，结合问题和答案两种类型的特征；第三，通过经验性研究探索了不同排名算法。

    

    社区问答（CQA）论坛是基于互联网的平台，用户在这里提出问题，其他专家用户试图提供解决方案。许多CQA论坛，如Quora，Stackoverflow，Yahoo！Answer，StackExchange等都有大量用户生成的数据。这些数据在自动化的CQA排名系统中得到利用，以回应用户的查询，呈现类似的问题（和答案）。在这项工作中，我们经验性地调查了该领域的一些方面。首先，除了传统的特征如TF-IDF、BM25等，我们引入了基于BERT的特征，捕捉问题和答案之间的语义相似性。其次，大部分现有研究工作都集中在仅从问题部分提取的特征上，尚未广泛探索从答案中提取的特征。我们以线性方式结合了两种类型的特征。第三，使用我们提出的概念，我们对不同排名算法进行了经验性的研究。

    Community question answering (CQA) forums are Internet-based platforms where users ask questions about a topic and other expert users try to provide solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer, StackExchange exist with a lot of user-generated data. These data are leveraged in automated CQA ranking systems where similar questions (and answers) are presented in response to the query of the user. In this work, we empirically investigate a few aspects of this domain. Firstly, in addition to traditional features like TF-IDF, BM25 etc., we introduce a BERT-based feature that captures the semantic similarity between the question and answer. Secondly, most of the existing research works have focused on features extracted only from the question part; features extracted from answers have not been explored extensively. We combine both types of features in a linear fashion. Thirdly, using our proposed concepts, we conduct an empirical investigation with different rank-lea
    
[^23]: 将废料变为黄金的损失：BERT4Rec真的比SASRec更好吗？

    Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])

    [http://arxiv.org/abs/2309.07602](http://arxiv.org/abs/2309.07602)

    在比较推荐系统中的两种模型SASRec和BERT4Rec时，我们的研究发现，如果两个模型都使用相同的损失函数进行训练，SASRec在质量和训练速度方面表现明显优于BERT4Rec。同时，我们还发现，即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但需要更多的负样本。

    

    最近，在推荐系统领域，顺序推荐和下一个项目预测任务越来越受欢迎。目前，基于Transformer的模型SASRec和BERT4Rec是两种最先进的基准模型。在过去的几年中，有很多发表的论文比较了这两个算法并提出了新的最先进模型。在大多数论文中，BERT4Rec的性能优于SASRec。但是，BERT4Rec对所有项目使用交叉熵，而SASRec使用负采样对一个正样本和一个负样本计算二元交叉熵损失。在我们的工作中，我们展示了如果两个模型都使用BERT4Rec所用的损失进行训练，那么SASRec在质量和训练速度方面将明显优于BERT4Rec。此外，我们还展示了即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但负样本的数量应该比BERT4Rec要大得多。

    Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than on
    
[^24]: 使用LLM预测的可信度信号和弱监督检测虚假信息

    Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])

    [http://arxiv.org/abs/2309.07601](http://arxiv.org/abs/2309.07601)

    本文研究了使用大型语言模型和弱监督的方式来检测虚假信息，证明了这种方法在两个数据集上的效果优于当前最先进的分类器。

    

    可信度信号代表了记者和事实核查员通常用来评估在线内容真实性的一系列启发式方法。然而，自动化可信度信号提取的任务非常具有挑战性，因为它需要训练高准确率的特定信号提取器，而目前没有足够大的数据集对所有可信度信号进行注释。本文研究了是否可以有效地用一组18个可信度信号来提示大型语言模型（LLMs），以产生每个信号的弱标签。然后，我们使用弱监督的方式对这些潜在的噪声标签进行聚合，以预测内容的真实性。我们证明了我们的方法，即结合了零-shot LLM可信度信号标注和弱监督的方法，在两个虚假信息数据集上优于最先进的分类器，而没有使用任何训练标签。

    Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
    
[^25]: C-Pack: 推进普通汉语嵌入的打包资源

    C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])

    [http://arxiv.org/abs/2309.07597](http://arxiv.org/abs/2309.07597)

    C-Pack是一套推进普通汉语嵌入领域的资源，包括全面汉语文本嵌入基准、大规模文本嵌入数据集和涵盖多个尺寸的嵌入模型系列。该资源集在C-MTEB基准上实现了最高+10%的表现，并通过整合和优化一套训练方法进一步提升了效果。此外，C-Pack还发布了英语文本嵌入数据和模型，实现了最先进的性能。该资源集可公开获取。

    

    我们介绍了C-Pack，这是一套显著推进普通汉语嵌入领域的资源。C-Pack包括三个关键资源。1）C-MTEB是一个涵盖6个任务和35个数据集的全面汉语文本嵌入基准。2）C-MTP是一个从标记和未标记的汉语语料库中策划的大规模文本嵌入数据集，用于训练嵌入模型。3）C-TEM是一个涵盖多个尺寸的嵌入模型系列。我们的模型在C-MTEB上的表现优于之前的所有汉语文本嵌入达到了发布时的最高+10%。我们还整合和优化了C-TEM的整套训练方法。除了我们关于普通汉语嵌入的资源外，我们还发布了我们的英语文本嵌入数据和模型。这些英语模型在MTEB基准上实现了最先进的性能；与此同时，我们发布的英语数据比汉语数据大2倍。所有这些资源都可以在https://github.com/FlagOpen/FlagEmbedding上公开获取。

    We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
    
[^26]: 基于逻辑查询的神经符号推荐模型

    Neuro-Symbolic Recommendation Model based on Logic Query. (arXiv:2309.07594v1 [cs.AI])

    [http://arxiv.org/abs/2309.07594](http://arxiv.org/abs/2309.07594)

    本文提出了一个基于逻辑查询的神经符号推荐模型，将用户历史交互转化为逻辑表达式，并通过逻辑查询实现推荐过程。

    

    推荐系统帮助用户找到与他们相关的物品。现有的推荐模型主要基于预测用户和物品之间的关系，并使用复杂的匹配模型或引入大量的外部信息来捕捉数据中的关联模式。然而，推荐不仅是一个利用数据进行归纳统计的问题，也是一个基于从信息中提取的知识进行推理决策的认知任务。因此，在推荐任务中，逻辑系统自然可以用于推理。然而，虽然基于逻辑系统的硬规则方法可以提供强大的推理能力，但它们在处理不一致和不完整的现实任务中很难应对，尤其是对于复杂任务如推荐。因此，在本文中，我们提出了一个神经符号推荐模型，将用户历史交互转化为逻辑表达式，然后将推荐过程变换为逻辑查询。

    A recommendation system assists users in finding items that are relevant to them. Existing recommendation models are primarily based on predicting relationships between users and items and use complex matching models or incorporate extensive external information to capture association patterns in data. However, recommendation is not only a problem of inductive statistics using data; it is also a cognitive task of reasoning decisions based on knowledge extracted from information. Hence, a logic system could naturally be incorporated for the reasoning in a recommendation task. However, although hard-rule approaches based on logic systems can provide powerful reasoning ability, they struggle to cope with inconsistent and incomplete knowledge in real-world tasks, especially for complex tasks such as recommendation. Therefore, in this paper, we propose a neuro-symbolic recommendation model, which transforms the user history interactions into a logic expression and then transforms the recomm
    
[^27]: 通过条件置换进行统计有效的变量重要性评估

    Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])

    [http://arxiv.org/abs/2309.07593](http://arxiv.org/abs/2309.07593)

    本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。

    

    在使用复杂学习器（如深度神经网络）处理大规模数据时，变量重要性评估已成为机器学习应用中的关键步骤。目前，基于移除的重要性评估是参考方法，特别是在需要统计保证来验证变量包含性时。通常，它们使用变量置换方案来实现。然而，这些方法在存在协变量之间的相关性时容易将不重要的变量误识别为重要变量。本文提出了一种系统方法来研究条件置换重要性（Conditional Permutation Importance，CPI），它是模型无关且计算效率高的方法，并提供了基准测试，用于评估当前最先进的变量重要性估计器。理论和实证结果表明，CPI通过提供准确的I型错误控制，克服了标准置换重要性的局限性。当与深度神经网络一起使用时，CPI始终显示出最高的准确性。

    Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
    
[^28]: Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning.（离线强化学习中用于泛化的等变数据增强）

    Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning. (arXiv:2309.07578v1 [cs.LG])

    [http://arxiv.org/abs/2309.07578](http://arxiv.org/abs/2309.07578)

    这个论文提出了一种用于离线强化学习中泛化的等变数据增强方法，并通过扩展等变集合并增强数据集来提高策略的测试性能。

    

    我们提出了一种新颖的方法来解决离线强化学习中的泛化挑战，其中智能体从固定数据集中学习而无需与环境进行额外交互。具体而言，我们旨在提高智能体对于超出分布范围的目标的泛化能力。为实现这一目标，我们提出了一种学习动力学模型并检查其是否等变于固定类型的变换，即状态空间中的平移。然后，我们使用一个熵正则化器来扩展等变集并通过所得到的转换样本对数据集进行增强。最后，我们基于增强的数据集使用一个现成的离线强化学习算法离线学习一个新的策略。我们的实验结果表明，我们的方法能够极大地提高策略在考虑的环境中的测试性能。

    We present a novel approach to address the challenge of generalization in offline reinforcement learning (RL), where the agent learns from a fixed dataset without any additional interaction with the environment. Specifically, we aim to improve the agent's ability to generalize to out-of-distribution goals. To achieve this, we propose to learn a dynamics model and check if it is equivariant with respect to a fixed type of transformation, namely translations in the state space. We then use an entropy regularizer to increase the equivariant set and augment the dataset with the resulting transformed samples. Finally, we learn a new policy offline based on the augmented dataset, with an off-the-shelf offline RL algorithm. Our experimental results demonstrate that our approach can greatly improve the test performance of the policy on the considered environments.
    
[^29]: 基于离散单元的风格转换的语音到语音翻译

    Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer. (arXiv:2309.07566v1 [cs.SD])

    [http://arxiv.org/abs/2309.07566](http://arxiv.org/abs/2309.07566)

    本研究提出了一种基于离散单元的语音到语音翻译框架，通过自监督学习和神经编解码器实现风格转换，解决了数据稀缺和音色保留的问题。实验结果表明，我们的模型在之前未见的语言上实现了高质量的跨语言风格转换。

    

    直接的语音到语音翻译（S2ST）通过离散的自监督表示实现了显著的准确性，但在翻译过程中无法保留源语音的说话人音色。与此同时，高质量说话人平行数据的稀缺性对于学习源语音和目标语音之间的风格转换构成了挑战。我们提出了一个基于自监督模型的离散单元的声学语言模型和风格转换的神经编解码器的S2ST框架。声学语言模型通过自监督上下文学习获得了风格转换的能力，无需依赖于任何说话人平行数据，从而克服了数据稀缺性问题。通过使用大量的训练数据，我们的模型可以在之前未见过的源语言上实现零-shot跨语言风格转换。实验证明，我们的模型生成的翻译语音具有高度的保真度和风格相似性。

    Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech during translation. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer between source and target speech. We propose an S2ST framework with an acoustic language model based on discrete units from a self-supervised model and a neural codec for style transfer. The acoustic language model leverages self-supervised in-context learning, acquiring the ability for style transfer without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at this http URL .
    
[^30]: SingFake：唱歌声音Deepfake检测

    SingFake: Singing Voice Deepfake Detection. (arXiv:2309.07525v1 [cs.SD])

    [http://arxiv.org/abs/2309.07525](http://arxiv.org/abs/2309.07525)

    本文提出了唱歌声音Deepfake检测任务，并通过提供一个特定的数据集和评估系统的方法，对这一问题进行了研究和解决。

    

    合成唱歌声音的兴起给艺术家和行业利益相关者带来了未经授权的声音使用的重要挑战。与合成语音不同，合成唱歌声音通常在包含强烈背景音乐的歌曲中发布，这可能掩盖了合成造成的瑕疵。此外，唱歌声音与语音话语具有不同的声学和语言特征。这些独特的属性使得唱歌声音Deepfake检测成为一个相关但显着不同于合成语音检测的问题。在本文中，我们提出了唱歌声音Deepfake检测任务。首先，我们提供了SingFake，这是一个在野外精选的数据集，包含了40位歌手用五种语言演唱的28.93小时真实音频和29.40小时的Deepfake音频。我们提供了一个训练/验证/测试的划分，其中测试集包括不同的场景。然后，我们使用SingFake评估了四个在语音话语训练的最先进的语音对抗系统。我们发现这些系统在唱歌声音上的表现不同于在语音话语上的表现。

    The rise of singing voice synthesis presents critical challenges to artists and industry stakeholders over unauthorized voice usage. Unlike synthesized speech, synthesized singing voices are typically released in songs containing strong background music that may hide synthesis artifacts. Additionally, singing voices present different acoustic and linguistic characteristics from speech utterances. These unique properties make singing voice deepfake detection a relevant but significantly different problem from synthetic speech detection. In this work, we propose the singing voice deepfake detection task. We first present SingFake, the first curated in-the-wild dataset consisting of 28.93 hours of bonafide and 29.40 hours of deepfake song clips in five languages from 40 singers. We provide a train/val/test split where the test sets include various scenarios. We then use SingFake to evaluate four state-of-the-art speech countermeasure systems trained on speech utterances. We find these sys
    
[^31]: 学习环境感知的遮挡下三维关节物体操作的可供性

    Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])

    [http://arxiv.org/abs/2309.07510](http://arxiv.org/abs/2309.07510)

    本论文提出了一个环境感知的可供性框架，考虑了物体级的可行性先验和环境约束，以解决多个遮挡的复杂情况下的三维关节物体操作问题。

    

    在多样的环境中感知和操作三维关节物体对于家庭助理机器人至关重要。最近的研究表明，点级可供性为下游操作任务提供了可行性先验。然而，现有工作主要集中在单个物体场景中的均质代理，忽视了环境和代理形态所施加的现实约束，如遮挡和物理限制。在本文中，我们提出了一个环境感知的可供性框架，结合了物体级可行性先验和环境约束。与以物体为中心的可供性方法不同，学习环境感知的可供性面临着由各种遮挡的复杂性引起的组合爆炸挑战，这些遮挡以其数量、几何形状、位置和姿势来刻画。为了解决这个问题并提高数据效率，我们引入了一种新颖的对比式可供性学习框架，能够在含有遮挡的场景中进行训练。

    Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
    
[^32]: 通过来自智能自监督基础设施的视频预测实现连接自主车辆的运动规划

    Connected Autonomous Vehicle Motion Planning with Video Predictions from Smart, Self-Supervised Infrastructure. (arXiv:2309.07504v1 [cs.RO])

    [http://arxiv.org/abs/2309.07504](http://arxiv.org/abs/2309.07504)

    本文介绍了一种利用智能自监督基础设施的视频预测来实现连接自主车辆的运动规划的方法。通过修改预测来预测未来的占用情况，而不是原始视频，减少了广播预测的数据占用量，并通过实验证明了该设计可以有效地辅助CAV的运动规划。

    

    连接自主车辆(CAVs)有望提高城市交通的安全性、效率和可持续性。然而，这取决于CAV能否正确预测周围运动的代理者，并安全规划自己的运动。在复杂的城市环境中，由于频繁的遮挡和多个代理者之间的相互作用，这一点具有挑战性。其中一个解决方案是利用智能基础设施来增强CAV的情境意识；本文利用最近提出的“自监督交通指导器”(SSTA)智能传感器框架，让其自己生成和广播有用的路用户视频预测。在本研究中，将SSTA预测修改为预测未来的占用情况，而不是原始视频，从而减少广播预测的数据占用量。得到的预测结果在规划框架中使用，证明这种设计可以有效地辅助CAV的运动规划。多种数值实验研究了关键因素。

    Connected autonomous vehicles (CAVs) promise to enhance safety, efficiency, and sustainability in urban transportation. However, this is contingent upon a CAV correctly predicting the motion of surrounding agents and planning its own motion safely. Doing so is challenging in complex urban environments due to frequent occlusions and interactions among many agents. One solution is to leverage smart infrastructure to augment a CAV's situational awareness; the present work leverages a recently proposed "Self-Supervised Traffic Advisor" (SSTA) framework of smart sensors that teach themselves to generate and broadcast useful video predictions of road users. In this work, SSTA predictions are modified to predict future occupancy instead of raw video, which reduces the data footprint of broadcast predictions. The resulting predictions are used within a planning framework, demonstrating that this design can effectively aid CAV motion planning. A variety of numerical experiments study the key fa
    
[^33]: HDTR-Net：一种用于任意说话人脸生成方法的实时高清牙齿修复网络

    HDTR-Net: A Real-Time High-Definition Teeth Restoration Network for Arbitrary Talking Face Generation Methods. (arXiv:2309.07495v1 [cs.CV])

    [http://arxiv.org/abs/2309.07495](http://arxiv.org/abs/2309.07495)

    HDTR-Net是一种实时高清牙齿修复网络，适用于任意说话人脸生成方法。通过使用Fine-Grained Feature Fusion (FGFF)模块来捕捉细纹理特征信息并提高牙齿的清晰度，HDTR-Net可以快速增强牙齿区域，并保持同步和时间上的一致性。

    

    说话人脸生成 (TFG) 旨在重建面部运动，以实现从音频和面部特征中获得高自然度的唇部运动。现有的 TFG 方法在产生自然和逼真的图像方面取得了显著进展。然而，大多数工作很少考虑视觉质量。在跨模式生成方法中，同时保证唇部同步和避免视觉质量下降是一项具有挑战性的任务。为了解决这个问题，我们提出了一种通用的高清牙齿修复网络，称为HDTR-Net，适用于任意的TFG方法。HDTR-Net可以以极快的速度增强牙齿区域，并保持同步和时间上的一致性。特别地，我们提出了一种Fine-Grained Feature Fusion (FGFF) 模块，有效地捕捉牙齿周围细纹理特征信息，并使用这些特征对特征图进行细粒度处理以提高牙齿的清晰度。大量实验证明...

    Talking Face Generation (TFG) aims to reconstruct facial movements to achieve high natural lip movements from audio and facial features that are under potential connections. Existing TFG methods have made significant advancements to produce natural and realistic images. However, most work rarely takes visual quality into consideration. It is challenging to ensure lip synchronization while avoiding visual quality degradation in cross-modal generation methods. To address this issue, we propose a universal High-Definition Teeth Restoration Network, dubbed HDTR-Net, for arbitrary TFG methods. HDTR-Net can enhance teeth regions at an extremely fast speed while maintaining synchronization, and temporal consistency. In particular, we propose a Fine-Grained Feature Fusion (FGFF) module to effectively capture fine texture feature information around teeth and surrounding regions, and use these features to fine-grain the feature map to enhance the clarity of teeth. Extensive experiments show that
    
[^34]: Where2Explore: 为未见过的关节物体进行少样本能力学习的研究

    Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects. (arXiv:2309.07473v1 [cs.RO])

    [http://arxiv.org/abs/2309.07473](http://arxiv.org/abs/2309.07473)

    Where2Explore是一种针对未见物体的少样本能力学习框架，通过有效地探索和最少数量的交互，可以推广到具有类似局部几何结构的新类别。

    

    关节物体的操作是机器人中一项基本但具有挑战性的任务。由于物体类别之间存在重要的几何和语义差异，以往的操纵模型难以推广到新的类别。少样本学习是缓解这个问题的一种有希望的解决方案，它允许机器人对未见过的物体进行少量交互。然而，现有的方法通常需要与每个未见实例进行昂贵且低效的测试交互。鉴于这一限制，我们观察到，尽管它们具有不同的形状，不同的类别通常共享类似的局部几何结构，这些结构对于操作是必要的，比如可拉动的手柄和可抓取的边缘-这个因素在以前的少样本学习中通常没有充分利用。为了利用这种共性，我们引入了“Where2Explore”，一种探索未知类别的能力学习框架，该框架在有限数量的实例上进行最少交互的求解。

    Articulated object manipulation is a fundamental yet challenging task in robotics. Due to significant geometric and semantic variations across object categories, previous manipulation models struggle to generalize to novel categories. Few-shot learning is a promising solution for alleviating this issue by allowing robots to perform a few interactions with unseen objects. However, extant approaches often necessitate costly and inefficient test-time interactions with each unseen instance. Recognizing this limitation, we observe that despite their distinct shapes, different categories often share similar local geometries essential for manipulation, such as pullable handles and graspable edges - a factor typically underutilized in previous few-shot learning works. To harness this commonality, we introduce 'Where2Explore', an affordance learning framework that effectively explores novel categories with minimal interactions on a limited number of instances. Our framework explicitly estimates
    
[^35]: 在物联网环境中检测未知攻击: 一种用于增强网络入侵检测的开放集分类器

    Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])

    [http://arxiv.org/abs/2309.07461](http://arxiv.org/abs/2309.07461)

    这项研究介绍了一个针对物联网环境定制的网络入侵检测系统的开放集分类器框架，利用图像表示和堆叠子聚类技术来识别未知攻击。

    

    物联网设备在生活的各个方面都得到了广泛的应用，这引入了互联的时代，创造了新的网络安全挑战，并强调了强大的入侵检测系统的需求。然而，传统的安全系统是基于封闭世界视角设计的，往往面临与不断发展的威胁环境中新的、陌生的攻击相处理的挑战。在本文中，我们介绍了一个框架，旨在解决物联网环境下网络入侵检测系统（NIDS）中的开放集识别（OSR）问题。我们的框架利用基于图像的数据表示，从网络流量中提取空间和时间模式。此外，我们还集成了堆叠和子聚类技术，通过有效地建模复杂和多样化的良性行为，实现对未知攻击的识别。

    The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empiric
    
[^36]: 朝着物联网中的人工通用智能（AGI）发展：机遇与挑战

    Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges. (arXiv:2309.07438v1 [cs.AI])

    [http://arxiv.org/abs/2309.07438](http://arxiv.org/abs/2309.07438)

    本研究探讨了在物联网中实现人工通用智能（AGI）的机遇与挑战，并提出了将AGI无缝集成到物联网中的概念框架。

    

    人工通用智能（AGI）具备理解、学习和执行任务的人类认知能力，引发了科学、商业和社会领域的广泛关注。这种关注特别延伸到物联网（IoT）领域，该领域通过将无数设备、传感器和系统连接起来，共同收集和共享数据以实现智能决策和自动化。本研究探讨了在物联网环境中实现AGI的机遇与挑战。具体而言，研究首先概述了物联网的基本原理以及人工智能在物联网系统中的关键作用。随后，研究深入探讨了AGI的基本原理，并提出了一个将AGI无缝集成到物联网中的概念框架。AGI与物联网相结合的应用范围广泛，涵盖了从智能城市到智能医疗等多个领域。

    Artificial General Intelligence (AGI), possessing the capacity to comprehend, learn, and execute tasks with human cognitive abilities, engenders significant anticipation and intrigue across scientific, commercial, and societal arenas. This fascination extends particularly to the Internet of Things (IoT), a landscape characterized by the interconnection of countless devices, sensors, and systems, collectively gathering and sharing data to enable intelligent decision-making and automation. This research embarks on an exploration of the opportunities and challenges towards achieving AGI in the context of the IoT. Specifically, it starts by outlining the fundamental principles of IoT and the critical role of Artificial Intelligence (AI) in IoT systems. Subsequently, it delves into AGI fundamentals, culminating in the formulation of a conceptual framework for AGI's seamless integration within IoT. The application spectrum for AGI-infused IoT is broad, encompassing domains ranging from smart
    
[^37]: 在有限资源条件下的语义解析

    Semantic Parsing in Limited Resource Conditions. (arXiv:2309.07429v1 [cs.CL])

    [http://arxiv.org/abs/2309.07429](http://arxiv.org/abs/2309.07429)

    本论文探讨了在有限资源条件下的语义解析的挑战，并提出了解决方案，包括使用自动数据筛选、知识迁移、主动学习和持续学习。具体方法包括合成训练样本、利用源领域知识改进目标领域解析，以及利用有限的人工翻译预算和机器翻译服务来调整解析器。

    

    本论文探讨了在有限的数据和计算资源条件下的语义解析的挑战，并提供了使用自动数据筛选、知识迁移、主动学习和持续学习等技术的解决方案。对于没有平行训练数据的任务，本论文提出了从结构化数据库模式生成合成训练样本的方法。当源领域有大量数据但目标领域只有有限的平行数据时，利用源领域的知识来改进目标领域的解析。对于目标语言中有限数据的多语言情况，本论文引入了一种使用有限的人工翻译预算来调整解析器的方法。通过主动学习选择源语言样本进行手动翻译，最大化目标语言解析器的性能。此外，本论文还提出了一种替代方法，利用机器翻译服务，并辅以人工翻译的方法。

    This thesis explores challenges in semantic parsing, specifically focusing on scenarios with limited data and computational resources. It offers solutions using techniques like automatic data curation, knowledge transfer, active learning, and continual learning.  For tasks with no parallel training data, the thesis proposes generating synthetic training examples from structured database schemas. When there is abundant data in a source domain but limited parallel data in a target domain, knowledge from the source is leveraged to improve parsing in the target domain.  For multilingual situations with limited data in the target languages, the thesis introduces a method to adapt parsers using a limited human translation budget. Active learning is applied to select source-language samples for manual translation, maximizing parser performance in the target language. In addition, an alternative method is also proposed to utilize machine translation services, supplemented by human-translated d
    
[^38]: JSMNet通过自注意力机制和多尺度改进室内点云语义和实例分割

    JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale. (arXiv:2309.07425v1 [cs.CV])

    [http://arxiv.org/abs/2309.07425](http://arxiv.org/abs/2309.07425)

    JSMNet通过自注意力机制和多尺度改进室内点云语义和实例分割。在室内3D点云数据中，JSMNet通过全局特征自注意力模块和多分辨率特征自适应融合模块，实现了更好的室内目标特征表达和语义、实例分割结果，具有较高的质量和准确性。

    

    室内3D点云数据的语义理解对于一系列后续应用非常重要，包括室内服务机器人、导航系统和数字孪生工程。全局特征对于实现高质量的室内点云语义和实例分割至关重要，因为它们提供了重要的长程上下文信息。为此，我们提出了JSMNet，它结合了多层网络和全局特征自注意力模块，共同分割三维点云的语义和实例。为了更好地表达室内目标的特征，我们设计了一个多分辨率特征自适应融合模块，考虑了由于扫描仪距离目标的变化而导致的点云密度差异。此外，我们提出了一个结合语义和实例特征的联合语义和实例分割框架，以达到优越的结果。我们在S3DIS上进行了实验，该数据集是一个大型的室内地物点云数据集。

    The semantic understanding of indoor 3D point cloud data is crucial for a range of subsequent applications, including indoor service robots, navigation systems, and digital twin engineering. Global features are crucial for achieving high-quality semantic and instance segmentation of indoor point clouds, as they provide essential long-range context information. To this end, we propose JSMNet, which combines a multi-layer network with a global feature self-attention module to jointly segment three-dimensional point cloud semantics and instances. To better express the characteristics of indoor targets, we have designed a multi-resolution feature adaptive fusion module that takes into account the differences in point cloud density caused by varying scanner distances from the target. Additionally, we propose a framework for joint semantic and instance segmentation by integrating semantic and instance features to achieve superior results. We conduct experiments on S3DIS, which is a large thr
    
[^39]: 对抗联邦学习中的客户端梯度反演

    Client-side Gradient Inversion Against Federated Learning from Poisoning. (arXiv:2309.07415v1 [cs.CR])

    [http://arxiv.org/abs/2309.07415](http://arxiv.org/abs/2309.07415)

    这项工作提出了一种新的联邦学习攻击方法——客户端污染梯度反演（CGI），能够从客户端发起并在有限的知识条件下恢复训练样本。这项研究首次展示了客户端对FL的攻击可能性。

    

    联邦学习（FL）使得分布式参与者（如移动设备）能够在不直接共享数据给中央服务器的情况下训练全局模型。最近的研究发现，FL容易受到梯度反演攻击（GIA）的威胁，该攻击旨在重构原始训练样本，并对FL中的客户端隐私构成高风险。然而，大多数现有的GIA方法需要控制服务器并依赖强先验知识，包括批归一化和数据分布信息。我们提出了一种新的攻击方法——客户端污染梯度反演（CGI），可以从客户端发起。首次展示了一个具有有限知识的客户端对损失恶意模型的利用，能够从聚合的全局模型中恢复训练样本的可行性。

    Federated Learning (FL) enables distributed participants (e.g., mobile devices) to train a global model without sharing data directly to a central server. Recent studies have revealed that FL is vulnerable to gradient inversion attack (GIA), which aims to reconstruct the original training samples and poses high risk against the privacy of clients in FL. However, most existing GIAs necessitate control over the server and rely on strong prior knowledge including batch normalization and data distribution information. In this work, we propose Client-side poisoning Gradient Inversion (CGI), which is a novel attack method that can be launched from clients. For the first time, we show the feasibility of a client-side adversary with limited knowledge being able to recover the training samples from the aggregated global model. We take a distinct approach in which the adversary utilizes a malicious model that amplifies the loss of a specific targeted class of interest. When honest clients employ
    
[^40]: FunCodec:一个基础的、可复现的、可整合的神经语音编解码器开源工具包

    FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec. (arXiv:2309.07405v1 [cs.SD])

    [http://arxiv.org/abs/2309.07405](http://arxiv.org/abs/2309.07405)

    FunCodec是一个基础的、可复现的、可整合的神经语音编解码器工具包，提供了训练方法和预训练模型，可以实现较高的重构质量和整合到下游任务中。

    

    本文介绍了FunCodec，一个基础的神经语音编解码器工具包，它是开源语音处理工具包FunASR的扩展。FunCodec提供了可复现的训练方法和推断脚本，用于最新的神经语音编解码器模型，如SoundStream和Encodec。由于与FunASR的统一设计，FunCodec可以轻松地整合到下游任务中，如语音识别。除了FunCodec，还提供了预训练模型，可用于学术或普遍用途。基于该工具包，我们进一步提出了频域编解码器模型FreqCodec，它可以以更低的计算和参数复杂度实现相当的语音质量。实验结果表明，在相同的压缩比下，FunCodec可以实现与其他工具包和发布模型相比更好的重构质量。我们还证明了预训练模型适用于包括自动语音识别在内的下游任务。

    This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pre-trained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech re
    
[^41]: 多级深度学习在解决偏微分方程中的应用，以Burgers方程为例

    Multi-Grade Deep Learning for Partial Differential Equations with Applications to the Burgers Equation. (arXiv:2309.07401v1 [math.NA])

    [http://arxiv.org/abs/2309.07401](http://arxiv.org/abs/2309.07401)

    本文提出了一种多级深度学习方法，用于解决非线性偏微分方程。该方法通过将DNN的学习任务分解为多个堆叠的神经网络，以解决随着网络层数增加而导致的非凸优化问题的复杂度增加的挑战。

    

    本文提出了一种多级深度学习方法，用于解决非线性偏微分方程（PDEs）。深度神经网络在解决PDEs方面表现出超强的性能，除了在自然语言处理、计算机视觉和机器人等领域的卓越成功。然而，训练一个非常深的网络往往是一项具有挑战性的任务。随着DNN的层数增加，解决由PDEs的DNN求解结果导致的大规模非凸优化问题变得越来越困难，这可能导致预测准确性的降低而不是增加。为了克服这一挑战，我们提出了一种两阶段多级深度学习（TS-MGDL）方法，将学习DNN的任务分解为一系列堆叠在彼此上方的神经网络。这种方法可以减轻解决具有大量参数的非凸优化问题的复杂度，并学习残差。

    We develop in this paper a multi-grade deep learning method for solving nonlinear partial differential equations (PDEs). Deep neural networks (DNNs) have received super performance in solving PDEs in addition to their outstanding success in areas such as natural language processing, computer vision, and robotics. However, training a very deep network is often a challenging task. As the number of layers of a DNN increases, solving a large-scale non-convex optimization problem that results in the DNN solution of PDEs becomes more and more difficult, which may lead to a decrease rather than an increase in predictive accuracy. To overcome this challenge, we propose a two-stage multi-grade deep learning (TS-MGDL) method that breaks down the task of learning a DNN into several neural networks stacked on top of each other in a staircase-like manner. This approach allows us to mitigate the complexity of solving the non-convex optimization problem with large number of parameters and learn resid
    
[^42]: 通过扩散模型实现语义对抗攻击

    Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])

    [http://arxiv.org/abs/2309.07398](http://arxiv.org/abs/2309.07398)

    本文提出了一个用于生成语义对抗攻击的框架，并使用扩散模型中的潜在空间中的语义信息。在该框架中，有两个变种方法：语义转换方法(ST)，通过微调潜在空间和/或扩散模型本身来生成图像；潜在屏蔽方法(LM)，利用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。实验结果验证了该框架的有效性。

    

    传统的对抗攻击主要通过在像素空间中添加对抗性扰动来操纵干净的样本。相比之下，语义对抗攻击更加关注改变干净样本的语义属性，例如颜色、上下文和特征，在实际世界中更具可行性。在本文中，我们提出了一个框架，通过利用最近的扩散模型，能够快速生成语义对抗攻击，因为语义信息包含在训练有素的扩散模型的潜在空间中。然后，该框架有两个变种：1) 语义转换方法(ST)通过微调生成图像的潜在空间和/或扩散模型本身；2) 潜在屏蔽方法(LM)使用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。此外，ST方法可在白盒或黑盒设置中应用。在CelebA-HQ和AFHQ数据集上进行了大量实验。

    Traditional adversarial attacks concentrate on manipulating clean examples in the pixel space by adding adversarial perturbations. By contrast, semantic adversarial attacks focus on changing semantic attributes of clean examples, such as color, context, and features, which are more feasible in the real world. In this paper, we propose a framework to quickly generate a semantic adversarial attack by leveraging recent diffusion models since semantic information is included in the latent space of well-trained diffusion models. Then there are two variants of this framework: 1) the Semantic Transformation (ST) approach fine-tunes the latent space of the generated image and/or the diffusion model itself; 2) the Latent Masking (LM) approach masks the latent space with another target image and local backpropagation-based interpretation methods. Additionally, the ST approach can be applied in either white-box or black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ datas
    
[^43]: DebCSE: 以去偏见的角度重新思考无监督对比句子嵌入学习

    DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective. (arXiv:2309.07396v1 [cs.CL])

    [http://arxiv.org/abs/2309.07396](http://arxiv.org/abs/2309.07396)

    本文重新思考了无监督对比句子嵌入学习，并从去偏见的角度提出了DebCSE方法。通过消除各种偏差，包括词频偏差、句子长度偏差和假负样本偏差，DebCSE旨在学习高质量的句子嵌入。

    

    先前的研究指出，词频偏差会导致Bert模型学习到无法区分的句子嵌入。一些对比学习方案，如SimCSE和ConSERT，已成功用于改善无监督句子嵌入的质量，减少偏差。然而，这些方法仍会引入新的偏差，如句子长度偏差和假负样本偏差，这些偏差阻碍了模型学习更精细的语义。本文从去偏见的角度重新审视对比句子嵌入学习的挑战，并认为有效消除各种偏差对于学习高质量句子嵌入至关重要。我们认为，所有这些偏差都是由于对比学习中构建训练数据的简单规则引入的，对比学习句子嵌入的关键是在无监督机器学习中模拟训练数据的分布。

    Several prior studies have suggested that word frequency biases can cause the Bert model to learn indistinguishable sentence embeddings. Contrastive learning schemes such as SimCSE and ConSERT have already been adopted successfully in unsupervised sentence embedding to improve the quality of embeddings by reducing this bias. However, these methods still introduce new biases such as sentence length bias and false negative sample bias, that hinders model's ability to learn more fine-grained semantics. In this paper, we reexamine the challenges of contrastive sentence embedding learning from a debiasing perspective and argue that effectively eliminating the influence of various biases is crucial for learning high-quality sentence embeddings. We think all those biases are introduced by simple rules for constructing training data in contrastive learning and the key for contrastive learning sentence embedding is to mimic the distribution of training data in supervised machine learning in uns
    
[^44]: 通过设计兼容的内窥镜图像来释放深度和姿态估计神经网络的能力

    Unleashing the Power of Depth and Pose Estimation Neural Networks by Designing Compatible Endoscopic Images. (arXiv:2309.07390v1 [cs.CV])

    [http://arxiv.org/abs/2309.07390](http://arxiv.org/abs/2309.07390)

    本研究通过改进图像和神经网络的兼容性，引入了Mask Image Modelling (MIM)模块，从而能够充分发挥当前神经网络在内窥镜导航中的能力。

    

    深度学习模型在未标注数据集上进行深度和姿态估计已成为成功进行内窥镜导航的有效途径。大多数当前的技术致力于开发更先进的神经网络以提高准确性。然而，现有方法忽视了内窥镜图像的特殊属性，导致无法充分发挥神经网络的能力。在本研究中，我们对内窥镜图像的属性进行详细分析，并改善图像和神经网络的兼容性，以释放当前神经网络的能力。首先，我们引入了Mask Image Modelling (MIM)模块，该模块输入部分图像信息而不是完整的图像信息，允许网络从部分像素信息中恢复全局信息。这增强了网络感知全局信息的能力，并减轻了卷积神经网络由于局部伪影而产生的局部过拟合现象。

    Deep learning models have witnessed depth and pose estimation framework on unannotated datasets as a effective pathway to succeed in endoscopic navigation. Most current techniques are dedicated to developing more advanced neural networks to improve the accuracy. However, existing methods ignore the special properties of endoscopic images, resulting in an inability to fully unleash the power of neural networks. In this study, we conduct a detail analysis of the properties of endoscopic images and improve the compatibility of images and neural networks, to unleash the power of current neural networks. First, we introcude the Mask Image Modelling (MIM) module, which inputs partial image information instead of complete image information, allowing the network to recover global information from partial pixel information. This enhances the network' s ability to perceive global information and alleviates the phenomenon of local overfitting in convolutional neural networks due to local artifact
    
[^45]: 深度神经网络的核平衡方程

    The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2309.07367](http://arxiv.org/abs/2309.07367)

    本文提出了深度神经网络的核平衡方程，解释了数据集分布估计中的不稳定性和尺度机制。网络的输出是数据集的局部平均，平均的尺度随着训练逐渐减小并导致不稳定性。

    

    在过去十年中，深度神经网络已经展现出许多有益的应用。通过对有限数据集进行训练，网络可以获得广义函数。广义化程度是数据空间中的近似尺度的实现。特别地，在数据集复杂时，尺度不明确。在本文中，我们考虑了一个用于数据集分布估计的网络。我们展示了估计是不稳定的，并且不稳定性取决于数据密度和训练持续时间。我们推导了核平衡方程，它给出了解的简短现象学描述。该方程告诉我们不稳定性的原因和尺度的机制。网络的输出作为预测是数据集的局部平均，平均的尺度沿着方程确定。尺度在训练过程中逐渐减小，最终导致不稳定性。

    Deep neural networks have shown many fruitful applications in this decade. A network can get the generalized function through training with a finite dataset. The degree of generalization is a realization of the proximity scale in the data space. Specifically, the scale is not clear if the dataset is complicated. Here we consider a network for the distribution estimation of the dataset. We show the estimation is unstable and the instability depends on the data density and training duration. We derive the kernel-balanced equation, which gives a short phenomenological description of the solution. The equation tells us the reason for the instability and the mechanism of the scale. The network outputs a local average of the dataset as a prediction and the scale of averaging is determined along the equation. The scale gradually decreases along training and finally results in instability in our case.
    
[^46]: Hodge-Aware Contrastive Learning（基于豪奇分解的对比学习）

    Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])

    [http://arxiv.org/abs/2309.07364](http://arxiv.org/abs/2309.07364)

    本文提出了一种基于豪奇分解的对比学习方法，用于处理单纯复合体数据，并生成具有特定谱信息的嵌入。通过编码数据不变性和设计合适的增强方法，以及重新权衡负样本的重要性，我们得到了一个反映谱信息的嵌入空间。

    

    单纯复合体在对具有多向依赖关系的数据进行建模方面表现出很好的效果，例如在网络的边缘上定义的数据或其他高阶结构内的数据。通过豪奇分解，可以将其谱分解为三个可解释的子空间，这在许多应用中都具有基础性。我们利用这种分解来开发一种对比自我监督学习方法，用于处理单纯复合体数据并生成蕴含特定谱信息的嵌入。具体地，我们通过单纯神经网络来编码相关数据的不变性，并设计出具有适当谱特性的增强方法，以生成正对比示例。此外，我们通过考虑负样本的豪奇分量与锚点相似性，重新权衡对比损失中负样本的重要性。通过加强较少相似实例之间的分离，我们获得了一个反映谱信息的嵌入空间。

    Simplicial complexes prove effective in modeling data with multiway dependencies, such as data defined along the edges of networks or within other higher-order structures. Their spectrum can be decomposed into three interpretable subspaces via the Hodge decomposition, resulting foundational in numerous applications. We leverage this decomposition to develop a contrastive self-supervised learning approach for processing simplicial data and generating embeddings that encapsulate specific spectral information.Specifically, we encode the pertinent data invariances through simplicial neural networks and devise augmentations that yield positive contrastive examples with suitable spectral properties for downstream tasks. Additionally, we reweight the significance of negative examples in the contrastive loss, considering the similarity of their Hodge components to the anchor. By encouraging a stronger separation among less similar instances, we obtain an embedding space that reflects the spect
    
[^47]: 高效的量子循环强化学习：基于量子储水池计算

    Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])

    [http://arxiv.org/abs/2309.07339](http://arxiv.org/abs/2309.07339)

    本研究提出了一种高效的量子循环强化学习方法，通过构建利用基于量子循环神经网络的储水池的QRL代理，解决了QRL与QRNN的低效训练问题。通过数值模拟验证了这种方法的有效性，并在标准基准测试中展示了其潜力。

    

    量子强化学习（QRL）已经成为解决顺序决策任务的框架，并展示了量子优势的实证结果。一个值得注意的发展是通过量子循环神经网络（QRNN）来处理部分可观察环境等内存密集任务。然而，QRL模型结合QRNN面临着挑战，包括QRL与QRNN的低效训练，因为QRNN中的梯度计算既耗费计算资源又耗时。本研究通过构建利用基于QRNN的储水池的QRL代理来解决这一挑战，具体采用量子长短时记忆（QLSTM）。QLSTM参数是随机初始化并固定不变的。该模型使用异步优势演员-评论家（A3C）算法进行训练。通过数值模拟，我们验证了QLSTM-Reservoir RL框架的有效性。其性能在标准基准测试上得到了评估，证明了其潜力。

    Quantum reinforcement learning (QRL) has emerged as a framework to solve sequential decision-making tasks, showcasing empirical quantum advantages. A notable development is through quantum recurrent neural networks (QRNNs) for memory-intensive tasks such as partially observable environments. However, QRL models incorporating QRNN encounter challenges such as inefficient training of QRL with QRNN, given that the computation of gradients in QRNN is both computationally expensive and time-consuming. This work presents a novel approach to address this challenge by constructing QRL agents utilizing QRNN-based reservoirs, specifically employing quantum long short-term memory (QLSTM). QLSTM parameters are randomly initialized and fixed without training. The model is trained using the asynchronous advantage actor-aritic (A3C) algorithm. Through numerical simulations, we validate the efficacy of our QLSTM-Reservoir RL framework. Its performance is assessed on standard benchmarks, demonstrating 
    
[^48]: 从辅助来源中学习在辩论修订分类中的应用

    Learning from Auxiliary Sources in Argumentative Revision Classification. (arXiv:2309.07334v1 [cs.CL])

    [http://arxiv.org/abs/2309.07334](http://arxiv.org/abs/2309.07334)

    该论文主要研究了在辩论写作中分类理想的推理修改的模型，并提出了多任务学习和迁移学习两种方法来利用辅助修订数据的来源。研究结果表明，这两种方法可以显著改善分类器的性能，并且迁移学习可以更好地表示数据之间的关系。

    

    我们开发了模型来分类辩论写作中希望改进的推理修订。我们探索了两种方法 - 多任务学习和迁移学习 - 利用类似任务的修订数据的辅助来源。内在和外在评估结果表明，这两种方法确实可以提高分类器性能，超过基线。尽管多任务学习显示同时在不同的数据来源上进行训练可能会提高性能，迁移学习更好地表示数据之间的关系。

    We develop models to classify desirable reasoning revisions in argumentative writing. We explore two approaches -- multi-task learning and transfer learning -- to take advantage of auxiliary sources of revision data for similar tasks. Results of intrinsic and extrinsic evaluations show that both approaches can indeed improve classifier performance over baselines. While multi-task learning shows that training on different sources of data at the same time may improve performance, transfer-learning better represents the relationship between the data.
    
[^49]: 多模态生物医学数据挖掘中的基于可靠性的噪声训练标签清洗方法与归纳性符合预测

    Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])

    [http://arxiv.org/abs/2309.07332](http://arxiv.org/abs/2309.07332)

    创新点：提出了一种基于可靠性的训练数据清洗方法，利用归纳性符合预测来纠正噪声训练数据中的错误标记和异常值。该方法在多个分类任务中验证有效性，并显著提升了分类性能。

    

    准确标记生物医学数据是一个挑战。传统的半监督学习方法通常没有充分利用可用的未标记数据。为了解决这个问题，我们提出了一种新的基于可靠性的训练数据清洗方法，采用了归纳性符合预测（ICP）。该方法利用一小部分准确标记的训练数据，并利用ICP计算的可靠性指标来纠正海量噪声训练数据中的错误标记和异常值。该方法的有效性在三个分类任务中得到了验证：使用标题和摘要对药物诱导的肝损伤（DILI）文献进行过滤，通过CT影像学和电子健康记录预测COVID-19患者的重症监护病房（ICU）入院情况，以及使用RNA测序数据对乳腺癌进行亚型分型。通过标签排列引入了不同程度的训练标签噪声。结果显示分类性能显著提升：准确度提高

    Accurately labeling biomedical data presents a challenge. Traditional semi-supervised learning methods often under-utilize available unlabeled data. To address this, we propose a novel reliability-based training data cleaning method employing inductive conformal prediction (ICP). This method capitalizes on a small set of accurately labeled training data and leverages ICP-calculated reliability metrics to rectify mislabeled data and outliers within vast quantities of noisy training data. The efficacy of the method is validated across three classification tasks within distinct modalities: filtering drug-induced-liver-injury (DILI) literature with title and abstract, predicting ICU admission of COVID-19 patients through CT radiomics and electronic health records, and subtyping breast cancer using RNA-sequencing data. Varying levels of noise to the training labels were introduced through label permutation. Results show significant enhancements in classification performance: accuracy enhanc
    
[^50]: 旅行词：一种变压器的几何解释。

    Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])

    [http://arxiv.org/abs/2309.07315](http://arxiv.org/abs/2309.07315)

    本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。

    

    变压器在自然语言处理领域取得了显著的进展，但理解其内部机制仍然是一个挑战。本文介绍了一种新颖的几何视角，阐明了变压器操作的内部机制。我们的主要贡献是说明了层归一化如何将潜在特征限制在一个超球面上，从而使注意力能够在该表面上塑造单词的语义表示。这种几何视点无缝地连接了迭代改进和上下文嵌入等已知属性。我们通过探测一个预训练的124M参数的GPT-2模型验证了我们的见解。我们的发现揭示了早期层中清晰的查询-键注意力模式，并在更深的层次上建立在先前关于注意头的专门性的观察基础上。利用这些几何见解，我们提出了对变压器的直观理解，将其描绘为塑造轨迹的过程。

    Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
    
[^51]: AudioSR: 大规模情境下的多功能音频超分辨率

    AudioSR: Versatile Audio Super-resolution at Scale. (arXiv:2309.07314v1 [cs.SD])

    [http://arxiv.org/abs/2309.07314](http://arxiv.org/abs/2309.07314)

    本论文介绍了一个基于扩散的生成模型AudioSR，可在各种音频类型上进行鲁棒的音频超分辨率处理。具体而言，它可以将2kHz到16kHz范围内的音频信号上采样为24kHz带宽的高分辨率音频信号，采样率为48kHz。

    

    音频超分辨率是一项基础任务，用于预测低分辨率音频的高频组成部分，从而提高数字应用中的音频质量。先前的方法存在一些限制，例如仅适用于特定类型的音频（例如音乐、语音）和特定的频带设置（例如4kHz到8kHz）。本文介绍了一种基于扩散的生成模型AudioSR，它能够在各种音频类型上进行鲁棒的音频超分辨率处理，包括音效、音乐和语音。具体而言，AudioSR可以将2kHz到16kHz范围内的任何输入音频信号上采样为24kHz带宽的高分辨率音频信号，采样率为48kHz。对各种音频超分辨率基准的广泛客观评估表明了所提出模型所取得的强大结果。此外，我们的主观评估表明，AudioSR可以作为即插即用的模块，提高广泛范围的生成质量。

    Audio super-resolution is a fundamental task that predicts high-frequency components for low-resolution audio, enhancing audio quality in digital applications. Previous methods have limitations such as the limited scope of audio types (e.g., music, speech) and specific bandwidth settings they can handle (e.g., 4kHz to 8kHz). In this paper, we introduce a diffusion-based generative model, AudioSR, that is capable of performing robust audio super-resolution on versatile audio types, including sound effects, music, and speech. Specifically, AudioSR can upsample any input audio signal within the bandwidth range of 2kHz to 16kHz to a high-resolution audio signal at 24kHz bandwidth with a sampling rate of 48kHz. Extensive objective evaluation on various audio super-resolution benchmarks demonstrates the strong result achieved by the proposed model. In addition, our subjective evaluation shows that AudioSR can acts as a plug-and-play module to enhance the generation quality of a wide range of
    
[^52]: 语言条件下的视觉目标搜索观测模型

    Language-Conditioned Observation Models for Visual Object Search. (arXiv:2309.07276v1 [cs.RO])

    [http://arxiv.org/abs/2309.07276](http://arxiv.org/abs/2309.07276)

    本研究提出了一种语言条件下的观测模型（LCOM），通过将目标搜索问题视为部分可观察的马尔可夫决策过程（POMDP），利用基于复杂语言描述的深度神经网络来动态生成物体检测器，从而解决了先前方法中需要为每个对象制作新检测器的问题。

    

    目标搜索是一项具有挑战性的任务，因为当给定复杂的语言描述时（例如“在桌子上找到白色的杯子”），机器人必须通过环境移动摄像头并识别描述的对象。先前的研究将语言描述映射到一组固定的物体检测器，并采用预定的噪声模型，但是这些方法很难扩展，因为每个对象都需要制作新的检测器。在这项工作中，我们将现实中的目标搜索作为部分可观察的马尔可夫决策过程（POMDP）来解决。其中，目标检测器和观测模型中的视觉传感器噪声由一个基于复杂语言描述的深度神经网络确定。我们将神经网络的输出纳入到我们的语言条件下的观测模型（LCOM）中，以表示动态变化的传感器噪声。通过LCOM，可以使用任何一个物体的语言描述来生成相应的物体检测器。

    Object search is a challenging task because when given complex language descriptions (e.g., "find the white cup on the table"), the robot must move its camera through the environment and recognize the described object. Previous works map language descriptions to a set of fixed object detectors with predetermined noise models, but these approaches are challenging to scale because new detectors need to be made for each object. In this work, we bridge the gap in realistic object search by posing the search problem as a partially observable Markov decision process (POMDP) where the object detector and visual sensor noise in the observation model is determined by a single Deep Neural Network conditioned on complex language descriptions. We incorporate the neural network's outputs into our language-conditioned observation model (LCOM) to represent dynamically changing sensor noise. With an LCOM, any language description of an object can be used to generate an appropriate object detector and 
    
[^53]: 安全且加速的基于深度强化学习的O-RAN切片: 一种混合迁移学习方法

    Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])

    [http://arxiv.org/abs/2309.07265](http://arxiv.org/abs/2309.07265)

    本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。

    

    开放无线接入网络（O-RAN）架构支持智能网络控制算法作为其核心能力之一。数据驱动应用程序利用这些算法通过无线接入网络智能控制器（RIC）来优化无线接入网络（RAN）功能。在O-RAN文献中，深度强化学习（DRL）算法是解决动态无线资源管理问题的主要方法之一。然而，尽管O-RAN RIC引入了诸多好处，但在真实网络部署中，DRL算法的实际采用却落后。这主要是因为DRL代理在部署和面对之前未见过的网络条件时收敛速度慢、性能不稳定。在本文中，我们通过将迁移学习（TL）作为O-RAN功能的DRL基于闭环控制的训练和部署流程的核心组成部分来解决这些挑战。为此，我们提出并设计了一个混合TL辅助的方法

    The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
    
[^54]: 使用贝叶斯优化自动调优基于Apache TVM的科学应用

    Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization. (arXiv:2309.07235v1 [cs.LG])

    [http://arxiv.org/abs/2309.07235](http://arxiv.org/abs/2309.07235)

    本文提出了一种基于贝叶斯优化的TVM自动调优框架，使用TVM张量表达语言实现了LU分解、Cholesky分解和3mm等线性代数核。在GPU集群上的实验结果表明，该框架在大多数情况下优于传统的AutoTVM框架。

    

    Apache TVM是一个开源的机器学习编译器框架，旨在优化各种硬件平台上的计算。本文提出了一种基于贝叶斯优化的新型TVM自动调优框架，并使用TVM张量表达语言实现了LU分解、Cholesky分解和3mm等线性代数核。我们使用这些科学计算核来评估我们的方法在阿贡国家实验室的GPU集群"Swing"上的有效性。我们将提出的自动调优框架与TVM自动调优框架AutoTVM的四个调优器进行比较，并发现我们的框架在大多数情况下表现优于AutoTVM。

    Apache TVM (Tensor Virtual Machine), an open source machine learning compiler framework designed to optimize computations across various hardware platforms, provides an opportunity to improve the performance of dense matrix factorizations such as LU (Lower Upper) decomposition and Cholesky decomposition on GPUs and AI (Artificial Intelligence) accelerators. In this paper, we propose a new TVM autotuning framework using Bayesian Optimization and use the TVM tensor expression language to implement linear algebra kernels such as LU, Cholesky, and 3mm. We use these scientific computation kernels to evaluate the effectiveness of our methods on a GPU cluster, called Swing, at Argonne National Laboratory. We compare the proposed autotuning framework with the TVM autotuning framework AutoTVM with four tuners and find that our framework outperforms AutoTVM in most cases.
    
[^55]: 通过时间滞后信息瓶颈的潜在表示和马尔可夫过程模拟

    Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])

    [http://arxiv.org/abs/2309.07200](http://arxiv.org/abs/2309.07200)

    本文介绍了一种通过时间滞后信息瓶颈的方法，将复杂系统映射到简化表示空间并模拟时间上的大跳跃。实验证明该方法能够准确模拟原始过程的统计特性和动力学，优于现有的时间滞后降维方法。

    

    马尔可夫过程是描述各个领域中动态系统的广泛使用的数学模型。然而，由于需要准确积分的短时间步长，精确模拟长时间尺度上的大规模系统计算量很大。在本文中，我们引入了一种将复杂系统映射到简化表示空间并模拟时间上的大跳跃的推理过程。为了实现这一点，我们提出了基于信息理论的原则目标-时间滞后信息瓶颈（T-IB），它旨在捕捉相关的时间特征，同时丢弃高频信息以简化模拟任务并最小化推理误差。我们的实验证明，T-IB学习了信息最优的表示，能够准确地模拟原始过程在选择的时间滞后下的统计特性和动力学，并且优于现有的时间滞后降维方法。

    Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck (T-IB), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that T-IB learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.
    
[^56]: 在存在截尾数据的情况下预测滚珠轴承的寿命

    Predicting Survival Time of Ball Bearings in the Presence of Censoring. (arXiv:2309.07188v1 [eess.SP])

    [http://arxiv.org/abs/2309.07188](http://arxiv.org/abs/2309.07188)

    本文提出了一种新的方法，使用生存分析来预测滚珠轴承的失效时间，并且通过分析频域数据和训练多个生存模型来实现。模型可以以时间为基础进行风险的概率预测，并允许比较不同组轴承的生存函数。

    

    滚珠轴承在各种制造和机械领域中得到广泛应用，基于机器学习的方法已经被广泛采用来监测磨损并在故障发生之前发现缺陷。然而，很少有研究涉及到截尾数据的问题，即未观察到故障的情况。本文提出了一种新的方法，使用生存分析来预测滚珠轴承的失效时间。首先，我们在频域中分析轴承数据，并通过比较库尔巴赫-莱布勒散度和其破坏频率区间与其破坏频率区间之间的标准差来标注轴承故障时刻。其次，我们训练了多个生存模型，根据标注数据和从时域提取的协变量（如偏度、峰度和熵）来估计失效时间。这些模型可以以时间为基础进行风险的概率预测，并允许我们比较不同组的轴承的生存函数。我们进行了演示。

    Ball bearings find widespread use in various manufacturing and mechanical domains, and methods based on machine learning have been widely adopted in the field to monitor wear and spot defects before they lead to failures. Few studies, however, have addressed the problem of censored data, in which failure is not observed. In this paper, we propose a novel approach to predict the time to failure in ball bearings using survival analysis. First, we analyze bearing data in the frequency domain and annotate when a bearing fails by comparing the Kullback-Leibler divergence and the standard deviation between its break-in frequency bins and its break-out frequency bins. Second, we train several survival models to estimate the time to failure based on the annotated data and covariates extracted from the time domain, such as skewness, kurtosis and entropy. The models give a probabilistic prediction of risk over time and allow us to compare the survival function between groups of bearings. We demo
    
[^57]: 基于柔性摩擦电传感器的智能医疗物联网健康监测系统及其在虚拟现实中的应用

    A Health Monitoring System Based on Flexible Triboelectric Sensors for Intelligence Medical Internet of Things and its Applications in Virtual Reality. (arXiv:2309.07185v1 [eess.SP])

    [http://arxiv.org/abs/2309.07185](http://arxiv.org/abs/2309.07185)

    本研究设计了一个基于柔性摩擦电传感器和深度学习的智能医疗物联网系统，用于帕金森病患者的监测和互动，包括位置/轨迹追踪、心率监测和身份识别。

    

    医疗物联网（IoMT）是将物联网技术与医疗应用相结合的平台，实现了数字化和智能化时代的精准医学、智能医疗和远程医疗。然而，IoMT面临着可持续的电力供应、传感器的人体适应性和传感器的智能性等多种挑战。本研究通过柔性可穿戴摩擦电传感器和深度学习辅助数据分析的协同集成，设计了一种稳健而智能的IoMT系统。我们将四个摩擦电传感器嵌入到手环中，用于检测和分析帕金森病患者的肢体运动。通过进一步整合深度学习辅助数据分析，我们实现了一个智能医疗监测系统，包括位置/轨迹追踪、心率监测和身份识别，用于帕金森病患者的监测和互动。

    The Internet of Medical Things (IoMT) is a platform that combines Internet of Things (IoT) technology with medical applications, enabling the realization of precision medicine, intelligent healthcare, and telemedicine in the era of digitalization and intelligence. However, the IoMT faces various challenges, including sustainable power supply, human adaptability of sensors and the intelligence of sensors. In this study, we designed a robust and intelligent IoMT system through the synergistic integration of flexible wearable triboelectric sensors and deep learning-assisted data analytics. We embedded four triboelectric sensors into a wristband to detect and analyze limb movements in patients suffering from Parkinson's Disease (PD). By further integrating deep learning-assisted data analytics, we actualized an intelligent healthcare monitoring system for the surveillance and interaction of PD patients, which includes location/trajectory tracking, heart monitoring and identity recognition.
    
[^58]: CloudBrain-NMR: 一种智能云计算平台，用于NMR光谱处理、重建和分析

    CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis. (arXiv:2309.07178v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.07178](http://arxiv.org/abs/2309.07178)

    CloudBrain-NMR是一个智能云计算平台，用于NMR光谱处理和分析，通过在线访问，无需用户端安装任何程序，使用并行计算来加快处理时间。

    

    核磁共振(NMR)光谱学在化学和生物学中作为一种强大的分析工具，已经被广泛应用于研究分子结构和动力学。然而，对从NMR光谱仪获取的原始数据进行处理和后续定量分析涉及各种专门工具，这就需要全面掌握程序和NMR方面的知识。特别是，由于计算设置复杂，新兴的深度学习工具在NMR中的应用并不容易。因此，NMR处理对于化学家和生物学家来说并不是一项容易的任务。在这项工作中，我们提出了CloudBrain-NMR，一种智能在线云计算平台，用于NMR数据的读取，处理，重建和定量分析。该平台通过Web浏览器方便地访问，无需用户端安装任何程序。CloudBrain-NMR使用图形处理单元和中央处理单元进行并行计算，从而大大缩短了处理时间。

    Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful analytical tool for studying molecular structure and dynamics in chemistry and biology. However, the processing of raw data acquired from NMR spectrometers and subsequent quantitative analysis involves various specialized tools, which necessitates comprehensive knowledge in programming and NMR. Particularly, the emerging deep learning tools is hard to be widely used in NMR due to the sophisticated setup of computation. Thus, NMR processing is not an easy task for chemist and biologists. In this work, we present CloudBrain-NMR, an intelligent online cloud computing platform designed for NMR data reading, processing, reconstruction, and quantitative analysis. The platform is conveniently accessed through a web browser, eliminating the need for any program installation on the user side. CloudBrain-NMR uses parallel computing with graphics processing units and central processing units, resulting in significantly shorten
    
[^59]: HurriCast：使用机器学习和统计建模的自动化框架用于飓风预测

    HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])

    [http://arxiv.org/abs/2309.07174](http://arxiv.org/abs/2309.07174)

    本研究提出了HurriCast，一种使用机器学习和统计建模的自动化框架，通过组合ARIMA模型和K-MEANS算法以更好地捕捉飓风趋势，并结合Autoencoder进行改进的飓风模拟，从而有效模拟历史飓风行为并提供详细的未来预测。这项研究通过利用全面且有选择性的数据集，丰富了对飓风模式的理解，并为风险管理策略提供了可操作的见解。

    

    飓风由于其灾害性影响而在美国面临重大挑战。减轻这些风险很重要，保险业在这方面起着重要作用，使用复杂的统计模型进行风险评估。然而，这些模型常常忽视关键的时间和空间飓风模式，并受到数据稀缺的限制。本研究引入了一种改进的方法，将ARIMA模型和K-MEANS相结合，以更好地捕捉飓风趋势，并使用Autoencoder进行改进的飓风模拟。我们的实验证明，这种混合方法有效地模拟了历史飓风行为，同时提供了潜在未来路径和强度的详细预测。此外，通过利用全面而有选择性的数据集，我们的模拟丰富了对飓风模式的当前理解，并为风险管理策略提供了可操作的见解。

    Hurricanes present major challenges in the U.S. due to their devastating impacts. Mitigating these risks is important, and the insurance industry is central in this effort, using intricate statistical models for risk assessment. However, these models often neglect key temporal and spatial hurricane patterns and are limited by data scarcity. This study introduces a refined approach combining the ARIMA model and K-MEANS to better capture hurricane trends, and an Autoencoder for enhanced hurricane simulations. Our experiments show that this hybrid methodology effectively simulate historical hurricane behaviors while providing detailed projections of potential future trajectories and intensities. Moreover, by leveraging a comprehensive yet selective dataset, our simulations enrich the current understanding of hurricane patterns and offer actionable insights for risk management strategies.
    
[^60]: 探索大型语言模型在本体对齐中的应用

    Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])

    [http://arxiv.org/abs/2309.07172](http://arxiv.org/abs/2309.07172)

    本文研究了大型语言模型在本体对齐中的应用，并发现它们有潜力在谨慎的框架和提示设计下超越现有的本体对齐系统。

    

    本文研究了最近的生成式大型语言模型（LLMs）（如GPT系列和Flan-T5）在本体对齐中的适用性，用于识别本体之间的概念等价映射。为了测试Flan-T5-XXL和GPT-3.5-turbo的零样本性能，我们利用了OAEI Bio-ML track的两个等价匹配数据集中具有挑战性的子集，并考虑到概念标签和结构上下文。初步结果表明，LLMs有可能在谨慎的框架和提示设计的情况下，胜过现有的本体对齐系统如BERTMap。

    This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.
    
[^61]: 通过可达性分析在分层强化学习中对目标空间进行抽象

    Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])

    [http://arxiv.org/abs/2309.07168](http://arxiv.org/abs/2309.07168)

    本文介绍了一种通过可达性分析在分层强化学习中对目标空间进行抽象的方法，以自动发现符号目标表示。该方法通过将具有相似任务角色的环境状态集合在一起的紧密关联表示来发现子目标，在导航任务中表现出可解释性和数据效率。

    

    开放式学习极大地受益于使用符号方法进行目标表示，因为它们提供了一种将知识结构化为高效且易于迁移的学习方式。然而，依赖于符号推理的现有分层强化学习方法通常受限于手动目标表示。自主发现符号目标表示的挑战在于它必须保留关键信息，如环境动力学。在这项工作中，我们提出了一种通过紧密关联（即将具有类似任务角色的环境状态集合在一起）的新兴表示发现子目标的发展机制。我们创建了一个逐渐学习此表示以及策略的分层强化学习算法，并使用导航任务进行评估，以展示学到的表示具有可解释性并实现数据效率。

    Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this work, we propose a developmental mechanism for subgoal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We create a HRL algorithm that gradually learns this representation along with the policies and evaluate it on navigation tasks to show the learned representation is interpretable and results in data efficiency.
    
[^62]: 资源受限机器人的混合ASR: HMM和深度学习融合

    Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion. (arXiv:2309.07164v1 [eess.AS])

    [http://arxiv.org/abs/2309.07164](http://arxiv.org/abs/2309.07164)

    这个论文介绍了一种专为资源受限机器人设计的混合ASR系统，通过结合HMM和深度学习模型，有效提高语音识别准确性，并在不同的机器人平台上展示了实时和精确的识别能力，具有适应性和与低功耗硬件兼容的特点，为无缝人机交互提供了有前景的可能性。

    

    本文介绍了一种专为资源受限机器人设计的新型混合自动语音识别（ASR）系统。该方法将隐马尔可夫模型（HMM）与深度学习模型相结合，并利用套接字编程有效地分配处理任务。在该架构中，基于HMM的处理在机器人内部进行，而一个独立的计算机处理深度学习模型。HMM和深度学习之间的协同作用显著提高了语音识别的准确性。我们在各种机器人平台上进行了实验，展示了实时和精确的语音识别能力。值得注意的是，该系统适应性强，能够适应不断变化的声学条件，与低功耗硬件兼容，使其在计算资源有限的环境中非常有效。这种混合的ASR范式为无缝的人机交互打开了有前景的可能性。总之，我们的研究引入了一个开创性的维度

    This paper presents a novel hybrid Automatic Speech Recognition (ASR) system designed specifically for resource-constrained robots. The proposed approach combines Hidden Markov Models (HMMs) with deep learning models and leverages socket programming to distribute processing tasks effectively. In this architecture, the HMM-based processing takes place within the robot, while a separate PC handles the deep learning model. This synergy between HMMs and deep learning enhances speech recognition accuracy significantly. We conducted experiments across various robotic platforms, demonstrating real-time and precise speech recognition capabilities. Notably, the system exhibits adaptability to changing acoustic conditions and compatibility with low-power hardware, making it highly effective in environments with limited computational resources. This hybrid ASR paradigm opens up promising possibilities for seamless human-robot interaction. In conclusion, our research introduces a pioneering dimens
    
[^63]: 使用LSTM揭示精确的跌倒检测：以召回率为驱动的精度细化

    Recall-driven Precision Refinement: Unveiling Accurate Fall Detection using LSTM. (arXiv:2309.07154v1 [eess.SP])

    [http://arxiv.org/abs/2309.07154](http://arxiv.org/abs/2309.07154)

    本文提出了一种使用LSTM的精确跌倒检测系统，通过结合加速度计和陀螺仪传感器以及修剪技术进行性能优化，系统具有高召回率和96％的特异度。

    

    本文提出了一种创新方法来解决老年人跌倒事件的紧迫问题，通过开发一种准确的跌倒检测系统。我们的系统结合了先进的技术，包括加速度计和陀螺仪传感器，与深度学习模型，特别是长短期记忆（LSTM）网络。通过树莓派硬件的集成实现了实时执行能力。我们引入了修剪技术，通过调整LSTM模型的架构和参数以优化系统的性能。我们优先考虑召回率而不是精度，旨在准确识别跌倒事件并最大程度减少误报，以便及时干预。广泛的实验和细致的评估展示了卓越的性能指标，强调了高召回率同时保持96％的特异度。我们的研究最终得到了一种先进的跌倒检测系统，可以及时发送通知，确保老年人的安全。

    This paper presents an innovative approach to address the pressing concern of fall incidents among the elderly by developing an accurate fall detection system. Our proposed system combines state-of-the-art technologies, including accelerometer and gyroscope sensors, with deep learning models, specifically Long Short-Term Memory (LSTM) networks. Real-time execution capabilities are achieved through the integration of Raspberry Pi hardware. We introduce pruning techniques that strategically fine-tune the LSTM model's architecture and parameters to optimize the system's performance. We prioritize recall over precision, aiming to accurately identify falls and minimize false negatives for timely intervention. Extensive experimentation and meticulous evaluation demonstrate remarkable performance metrics, emphasizing a high recall rate while maintaining a specificity of 96\%. Our research culminates in a state-of-the-art fall detection system that promptly sends notifications, ensuring vulner
    
[^64]: 在复杂网络中寻找影响者：一种有效的深度强化学习方法

    Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach. (arXiv:2309.07153v1 [cs.SI])

    [http://arxiv.org/abs/2309.07153](http://arxiv.org/abs/2309.07153)

    本文提出了一种有效的深度强化学习模型，通过将图神经网络作为编码器、强化学习作为解码器，实现了在复杂网络中寻找影响者的任务上的优越性能，超过了传统的最佳影响力最大化算法。

    

    在社交网络分析中，最大化复杂网络中的影响力是一个实际重要但计算上具有挑战性的任务，因为它是一个NP难问题。目前大多数近似或启发式方法要么需要巨大的人工设计工作，要么在效果和效率之间无法达到令人满意的平衡。最近的机器学习尝试只注重速度，而缺乏性能提升。本文中，与以前的尝试不同，我们提出了一种有效的深度强化学习模型，它在传统的最佳影响力最大化算法上表现出优越的性能。具体而言，我们设计了一个端到端的学习框架，将图神经网络作为编码器，强化学习作为解码器，命名为DREIM。通过在小规模合成图上进行大量训练，DREIM在非常大的合成网络和真实世界网络上在解决质量方面超过了最先进的基准方法，我们还通过实验证明了其线性可扩展性。

    Maximizing influences in complex networks is a practically important but computationally challenging task for social network analysis, due to its NPhard nature. Most current approximation or heuristic methods either require tremendous human design efforts or achieve unsatisfying balances between effectiveness and efficiency. Recent machine learning attempts only focus on speed but lack performance enhancement. In this paper, different from previous attempts, we propose an effective deep reinforcement learning model that achieves superior performances over traditional best influence maximization algorithms. Specifically, we design an end-to-end learning framework that combines graph neural network as the encoder and reinforcement learning as the decoder, named DREIM. Trough extensive training on small synthetic graphs, DREIM outperforms the state-of-the-art baseline methods on very large synthetic and real-world networks on solution quality, and we also empirically show its linear sca
    
[^65]: 通过知识蒸馏和潜在扩散模型从脑电图中解码视觉脑表示

    Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])

    [http://arxiv.org/abs/2309.07149](http://arxiv.org/abs/2309.07149)

    本研究提出了一种创新方法，利用脑电图数据解码人脑中的视觉表示。通过将EEG数据转换为频谱图并使用卷积神经网络进行训练，结合基于知识蒸馏的图像分类教师网络，我们的模型在图像分类和重建任务上表现出色。

    

    从人脑活动中解码视觉表示已成为一个蓬勃发展的研究领域，特别是在脑机接口的背景下。我们的研究提出了一种创新方法，利用来自图像Net数据集的脑电图（EEG）数据来分类和重构图像（即“脑解码”）。我们分析了来自6名参与者的EEG记录，每名参与者观看了覆盖40个独特语义类别的50个图像。这些EEG读数被转换为频谱图，然后用于训练一个卷积神经网络（CNN），集成了基于预训练的对比语言-图像预训练（CLIP）图像分类教师网络的知识蒸馏过程。这种策略使我们的模型达到了80%的前五位准确率，显著优于标准CNN和各种基于RNN的基准。此外，我们还引入了一种图像重构机制。

    Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain-computer interfaces. Our study presents an innovative method that employs to classify and reconstruct images from the ImageNet dataset using electroencephalography (EEG) data from subjects that had viewed the images themselves (i.e. "brain decoding"). We analyzed EEG recordings from 6 participants, each exposed to 50 images spanning 40 unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 80%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism ba
    
[^66]: ETP: 通过ECG-Text预训练学习可迁移的ECG表示

    ETP: Learning Transferable ECG Representations via ECG-Text Pre-training. (arXiv:2309.07145v1 [eess.SP])

    [http://arxiv.org/abs/2309.07145](http://arxiv.org/abs/2309.07145)

    本论文介绍了ECG-Text预训练（ETP）框架，它通过将ECG信号与文本报告对齐，实现了跨模态ECG特征学习。ETP在线性评估和零样本分类任务中表现出色，并展示了其在跨模态ECG特征学习方面的鲁棒性和可迁移性。

    

    在心血管保健领域中，心电图（ECG）作为一种重要的非侵入性诊断工具。尽管近年来自我监督学习（SSL）在ECG表示学习方面取得了进展，但这些技术通常需要注释样本，并且在微调阶段难以处理不存在的类别。为了解决这些限制，我们引入了ECG-Text预训练（ETP），这是一种创新的框架，旨在学习将ECG信号与文本报告联系起来的跨模态表示。该框架首次在ECG领域中利用了零样本分类任务。ETP使用ECG编码器和预训练语言模型来将ECG信号与其相应的文本报告对齐。所提出的框架在线性评估和零样本分类任务中表现出色，在PTB-XL和CPSC2018数据集上的实验结果表明其具有鲁棒且可迁移的跨模态ECG特征学习能力。

    In the domain of cardiovascular healthcare, the Electrocardiogram (ECG) serves as a critical, non-invasive diagnostic tool. Although recent strides in self-supervised learning (SSL) have been promising for ECG representation learning, these techniques often require annotated samples and struggle with classes not present in the fine-tuning stages. To address these limitations, we introduce ECG-Text Pre-training (ETP), an innovative framework designed to learn cross-modal representations that link ECG signals with textual reports. For the first time, this framework leverages the zero-shot classification task in the ECG domain. ETP employs an ECG encoder along with a pre-trained language model to align ECG signals with their corresponding textual reports. The proposed framework excels in both linear evaluation and zero-shot classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets, showcasing its ability for robust and generalizable cross-modal ECG feature learning.
    
[^67]: 基于人工智能的乒乓球运动员动作技能识别与评估系统设计

    Design of Recognition and Evaluation System for Table Tennis Players' Motor Skills Based on Artificial Intelligence. (arXiv:2309.07141v1 [eess.SP])

    [http://arxiv.org/abs/2309.07141](http://arxiv.org/abs/2309.07141)

    本研究基于人工智能，设计了一种用于识别和评估乒乓球运动员动作技能的系统，通过改进可穿戴设备，并利用特征工程、降维和不同评估指标的损失函数实现了动作的模式识别和层次化评估。

    

    随着电子科学技术的飞速发展，对可穿戴设备的研究不断更新，但目前来看，对于可穿戴设备识别和分析特定运动的能力还不够全面。基于此，本文改进了乒乓球运动的可穿戴设备，并通过人工智能实现了乒乓球运动员动作技能的模式识别和评估。首先设计了一个设备来收集乒乓球运动员的动作信息，并对实际运动数据进行处理。其次，制作了一个滑动窗口来将收集到的动作数据分割成六个乒乓球基准动作的特征数据库。然后，基于特征工程构建了动作特征，并通过降维后的不同模型来识别不同的动作技能。最后，利用不同评估指标的损失函数建立了动作技能的层次化评估系统。

    With the rapid development of electronic science and technology, the research on wearable devices is constantly updated, but for now, it is not comprehensive for wearable devices to recognize and analyze the movement of specific sports. Based on this, this paper improves wearable devices of table tennis sport, and realizes the pattern recognition and evaluation of table tennis players' motor skills through artificial intelligence. Firstly, a device is designed to collect the movement information of table tennis players and the actual movement data is processed. Secondly, a sliding window is made to divide the collected motion data into a characteristic database of six table tennis benchmark movements. Thirdly, motion features were constructed based on feature engineering, and motor skills were identified for different models after dimensionality reduction. Finally, the hierarchical evaluation system of motor skills is established with the loss functions of different evaluation indexes.
    
[^68]: 基于掩码Transformer的心电图分类研究

    Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])

    [http://arxiv.org/abs/2309.07136](http://arxiv.org/abs/2309.07136)

    提出了一种基于掩码Transformer的ECG分类方法，命名为MTECG，扩展了掩码自动编码器在ECG时间序列上的应用，该方法在广泛的掩码比例下表现稳定良好，并进行了消融实验验证了重构目标的波动性、训练计划长度、逐层学习率衰减和DropPath率的重要性。

    

    心电图（ECG）是临床应用中最重要的诊断工具之一。随着先进算法的出现，各种深度学习模型已被应用于ECG任务。然而，尽管Transformer在计算机视觉和自然语言处理领域取得了广泛成功，但其在ECG数据上的潜力尚未得到实现。在本研究中，我们提出了一种有用的基于掩码Transformer的ECG分类方法，称为MTECG，它将掩码自动编码器的应用扩展到了ECG时间序列上。我们构建了一个包含220,251个ECG记录的数据集，这些记录由医学专家进行了广泛的诊断注释，以探索MTECG的特性。在提出的训练策略下，一个只有5.7M参数的轻量级模型在广泛的掩码比例（5%-75%）下表现稳定良好。消融研究突出了波动重构目标、训练计划长度、逐层学习率衰减和DropPath率的重要性。实验发现MTECG耗时较少且能够有效分类各种心电图。

    Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
    
[^69]: 增加植物研究数据的FAIRness的本体论

    Ontologies for increasing the FAIRness of plant research data. (arXiv:2309.07129v1 [cs.DL])

    [http://arxiv.org/abs/2309.07129](http://arxiv.org/abs/2309.07129)

    本研究提出了一种增加植物研究数据FAIRness的本体论方法，利用语义标记和相关元数据增加数据的可重用性和互操作性。

    

    提高研究数据的FAIRness（可找到性、可访问性、可互操作性、可重用性）的重要性不可否认，特别是面对当前由组学技术产生的大规模复杂数据集。本体论可以作为语义标记数据集的有用工具，通过添加相关元数据来增加对数据生成方式的理解，并提高其互操作性。本体论提供了一个特定领域的概念以及概念之间的关系。通过使用本体论术语对数据进行标记，数据既可以人类解释，也可以机器解释，从而增加重用和互操作性。然而，在特定研究领域或技术中识别相关本体论的任务是具有挑战性的，尤其是在多样化的基础植物研究领域中。

    The importance of improving the FAIRness (findability, accessibility, interoperability, reusability) of research data is undeniable, especially in the face of large, complex datasets currently being produced by omics technologies. Facilitating the integration of a dataset with other types of data increases the likelihood of reuse, and the potential of answering novel research questions. Ontologies are a useful tool for semantically tagging datasets as adding relevant metadata increases the understanding of how data was produced and increases its interoperability. Ontologies provide concepts for a particular domain as well as the relationships between concepts. By tagging data with ontology terms, data becomes both human and machine interpretable, allowing for increased reuse and interoperability. However, the task of identifying ontologies relevant to a particular research domain or technology is challenging, especially within the diverse realm of fundamental plant research. In this re
    
[^70]: 关于T-S模糊系统在原点附近的局部二次稳定性的研究

    On the Local Quadratic Stability of T-S Fuzzy Systems in the Vicinity of the Origin. (arXiv:2309.06841v1 [eess.SY])

    [http://arxiv.org/abs/2309.06841](http://arxiv.org/abs/2309.06841)

    本文介绍了一种新的局部稳定性条件，该条件基于线性矩阵不等式和二次Lyapunov函数，并结合了原点附近的非线性系统的线性结构，相比于现有方法更为准确和有效。同时，本文还提出了局部指数稳定性的必要和充分条件，并讨论了模糊Lyapunov方法的局限性。

    

    本文的主要目标是引入新的局部稳定性条件，用于连续时间的Takagi-Sugeno（T-S）模糊系统。这些稳定性条件基于线性矩阵不等式（LMIs）和二次Lyapunov函数。此外，它们结合了原点处的隶属函数信息，并有效利用了原点附近的非线性系统的线性结构。因此，与文献中使用模糊Lyapunov函数的现有方法相比，提出的条件证明了更少的保守性。此外，我们证明了提出的方法提供了T-S模糊系统局部指数稳定性的必要和充分条件。本文还讨论了模糊Lyapunov方法的固有限制。为了演示理论结果，我们提供了详细的示例，阐明了核心概念，并验证了所提方法的有效性。

    The main goal of this paper is to introduce new local stability conditions for continuous-time Takagi-Sugeno (T-S) fuzzy systems. These stability conditions are based on linear matrix inequalities (LMIs) in combination with quadratic Lyapunov functions. Moreover, they integrate information on the membership functions at the origin and effectively leverage the linear structure of the underlying nonlinear system in the vicinity of the origin. As a result, the proposed conditions are proved to be less conservative compared to existing methods using fuzzy Lyapunov functions in the literature. Moreover, we establish that the proposed methods offer necessary and sufficient conditions for the local exponential stability of T-S fuzzy systems. The paper also includes discussions on the inherent limitations associated with fuzzy Lyapunov approaches. To demonstrate the theoretical results, we provide comprehensive examples that elucidate the core concepts and validate the efficacy of the proposed
    
[^71]: 缺失数据下的不确定性交通预测

    Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])

    [http://arxiv.org/abs/2309.06800](http://arxiv.org/abs/2309.06800)

    本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。

    

    交通预测是一个重要的课题，因为它在交通领域有广泛的应用。近期，许多研究取得了很好的结果。然而，大多数研究假设预测位置有完整或至少部分的历史记录，不能扩展到无历史记录的位置。在现实场景中，由于预算限制和安装可行性问题，传感器的部署可能受限，这使得大多数当前模型不适用。虽然少数文献尝试在缺失位置上插补交通状态，但这些方法需要与传感器位置同时观测的数据，使它们不适用于预测任务。另一个缺点是缺乏对预测不确定性的测量，使得之前的工作不适用于风险敏感的任务或涉及决策的情况。为了填补这一空白，受到先前的归纳图神经网络的启发，本文提出了一种考虑不确定性的方法。

    Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
    
[^72]: 分布式机器学习资源上的混合算法选择和超参数调整: 一种基于层级代理的方法

    Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v1 [cs.LG])

    [http://arxiv.org/abs/2309.06604](http://arxiv.org/abs/2309.06604)

    本文提出了一种基于代理的层级机器学习平台，用于选择分布式组织的机器学习算法并同时调整其超参数。该方法具有可伸缩性、灵活性和鲁棒性，并支持自动化和协同的功能。

    

    算法选择和超参数调整是学术界和应用机器学习中关键的步骤。然而，由于机器学习资源数量的大幅增加、多样性和分布性，这些步骤变得越来越复杂。当将多智能体系统应用于机器学习平台的设计时，会带来可伸缩性、灵活性和鲁棒性等多个独特特性。本文提出了一种完全自动和协同的基于代理的机制，用于选择分布式组织的机器学习算法，并同时调整其超参数。我们的方法基于现有的基于代理的层级机器学习平台，并通过增强其查询结构来支持上述功能，而不限于特定的学习、选择和调整机制。我们进行了理论评估、形式验证和分析。

    Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical st
    
[^73]: 随机LLMs无法理解语言：走向符号化、可解释性和本体论基于的LLMs

    Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])

    [http://arxiv.org/abs/2309.05918](http://arxiv.org/abs/2309.05918)

    随机LLMs无法理解语言的原因是它们无法提供可以依赖的事实信息，它们存储的语言知识埋藏在无意义的微特征中，并在某些语言上下文中无法进行正确推理。本文建议在符号化方法中应用有效的自下而上策略

    

    在我们看来，围绕数据驱动的大型语言模型（LLMs）相对成功的狂热是有些误导的，原因如下：（i）LLMs不能依赖于事实信息，因为对于LLMs来说，摄入的所有文本（事实或非事实）都是平等的；（ii）由于它们的亚符号性质，这些模型对语言的任何“知识”都将永远埋藏在数十亿个微特征（权重）中，其中没有一个本身是有意义的；以及（iii）LLMs在几种语言上下文中常常无法进行正确推理（如名词复合词、共谓词、量词范围模糊和意向性上下文）。我们相信，数据驱动的大型语言模型（LLMs）的相对成功不是符号与亚符号之辩的反映，而是在规模上应用自下而上的逆向工程语言的成功策略的反映。在本文中，我们建议将有效的自下而上策略应用于符号化方法中

    In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts. Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbol-ic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbol
    
[^74]: 评估聊天机器人以促进用户的信任 - 实践和开放问题

    Evaluating Chatbots to Promote Users' Trust -- Practices and Open Problems. (arXiv:2309.05680v1 [cs.HC])

    [http://arxiv.org/abs/2309.05680](http://arxiv.org/abs/2309.05680)

    本文评估了当前聊天机器人测试的实践和开放问题，旨在解决和减轻与用户信任相关的服务或产品性能、用户满意度以及对社会的长期意外后果问题。

    

    聊天机器人是一种人工智能（AI）软件，通过与它们自然地互动来完成任务。尽管聊天机器人从人工智能诞生之初就已经被研究，但自从易于使用且通用的基于大型语言模型的聊天机器人如ChatGPT推出以来，聊天机器人特别吸引了公众和企业的关注。企业将聊天机器人作为一种潜在技术来吸引用户，这些用户可能是最终消费者、供应商，甚至是自己的员工，对聊天机器人进行适当的测试以解决和减轻与服务或产品性能、用户满意度以及对社会的长期意外后果相关的信任问题是重要的。本文回顾了当前聊天机器人测试的实践，确定了追求用户信任的开放问题，并提出了一个前进的路径。

    Chatbots, the common moniker for collaborative assistants, are Artificial Intelligence (AI) software that enables people to naturally interact with them to get tasks done. Although chatbots have been studied since the dawn of AI, they have particularly caught the imagination of the public and businesses since the launch of easy-to-use and general-purpose Large Language Model-based chatbots like ChatGPT. As businesses look towards chatbots as a potential technology to engage users, who may be end customers, suppliers, or even their own employees, proper testing of chatbots is important to address and mitigate issues of trust related to service or product performance, user satisfaction and long-term unintended consequences for society. This paper reviews current practices for chatbot testing, identifies gaps as open problems in pursuit of user trust, and outlines a path forward.
    
[^75]: CPMR: 基于上下文感知的增量顺序推荐与伪多任务学习

    CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])

    [http://arxiv.org/abs/2309.04802](http://arxiv.org/abs/2309.04802)

    CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。

    

    用户进行互动的动机可以分为静态偏好和动态兴趣。为了准确地建模用户随时间变化的表示，最近的顺序推荐研究利用信息传播和演化从批量到达的互动中进行挖掘。然而，他们忽略了在上下文场景中人们很容易受到其他用户的最近行为的影响，并且在所有历史互动中应用演化会稀释最近互动的重要性，从而无法准确地建模兴趣动态的演化。为了解决这个问题，我们提出了一种基于上下文感知的伪多任务推荐系统（CPMR），通过为每个用户和项目创建三个表示（静态嵌入、历史时间状态和上下文时间状态），来建模历史和上下文情境中的演化。为了同时提高时间状态演化和增量推荐的性能。

    The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
    
[^76]: 用于机器人操作的具有物理基础的视觉语言模型

    Physically Grounded Vision-Language Models for Robotic Manipulation. (arXiv:2309.02561v1 [cs.RO])

    [http://arxiv.org/abs/2309.02561](http://arxiv.org/abs/2309.02561)

    该论文介绍了一个用于机器人操作的具有物理基础的视觉语言模型，通过在物体上微调模型，提高了模型对物理概念的理解，在语言交互框架中展现了良好的性能。

    

    最近对于视觉语言模型（VLMs）的研究进展导致在视觉问答和图像描述等任务上的性能得到了提升。因此，这些模型现在可以在物理世界中进行推理，特别是在机器人操作领域。然而，当前的VLMs在对常见物体的物理概念（例如材料、脆弱性）的理解方面存在局限，这限制了它们在涉及与这些物体的相互作用和物理推理的机器人操作任务中的实用性。为了解决这个问题，我们提出了PhysObjects，这是一个以物体为中心的数据集，包含36.9K个众包和417K个自动化的常见家居物品的物理概念注释。我们证明，在PhysObjects上对VLM进行微调可以提高其对物理物体概念的理解，通过从视觉外观中捕捉这些概念的人类先验知识。我们在一个大型的语言交互框架中将这个具有物理基础的VLM结合在一起。

    Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 36.9K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically-grounded VLM in an interactive framework with a large languag
    
[^77]: 关于可解释的跨模态推理的调查

    A Survey on Interpretable Cross-modal Reasoning. (arXiv:2309.01955v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.01955](http://arxiv.org/abs/2309.01955)

    这篇论文调查了可解释的跨模态推理领域，主要目标是在实现高预测性能的同时为结果提供可理解的解释。调查提供了一种三级分类方法的综合概述，并回顾了现有的具有解释注释的CMR数据集，同时总结了I-CMR的挑战和未来发展方向。

    

    最近几年，跨模态推理（CMR）已经成为一个重要领域，涵盖了从多媒体分析到医疗诊断等多个应用领域的理解和推理过程。随着人工智能系统的部署变得更加普遍，对这些系统决策过程的透明度和可理解性的需求也日益增加。本调查深入探讨了可解释的跨模态推理（I-CMR）领域，其目标不仅是实现高预测性能，还要为结果提供人类可理解的解释。本调查提供了一种三级分类方法的全面概述，并回顾了现有的具有解释注释的CMR数据集。最后，本调查总结了I-CMR的挑战，并讨论了潜在的未来发展方向。总之，本调查旨在推动这一新兴领域的进步。

    In recent years, cross-modal reasoning (CMR), the process of understanding and reasoning across different modalities, has emerged as a pivotal area with applications spanning from multimedia analysis to healthcare diagnostics. As the deployment of AI systems becomes more ubiquitous, the demand for transparency and comprehensibility in these systems' decision-making processes has intensified. This survey delves into the realm of interpretable cross-modal reasoning (I-CMR), where the objective is not only to achieve high predictive performance but also to provide human-understandable explanations for the results. This survey presents a comprehensive overview of the typical methods with a three-level taxonomy for I-CMR. Furthermore, this survey reviews the existing CMR datasets with annotations for explanations. Finally, this survey summarizes the challenges for I-CMR and discusses potential future directions. In conclusion, this survey aims to catalyze the progress of this emerging resea
    
[^78]: eDKM:一种用于大型语言模型的高效准确的训练时权重聚类方法

    eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00964](http://arxiv.org/abs/2309.00964)

    本文提出了一种内存高效的DKM实现，即eDKM，用于大型语言模型的高效准确的训练时权重聚类方法。它通过减小DKM的内存占用，解决了LLM压缩中的训练开销问题。

    

    由于大型语言模型（LLMs）在许多复杂语言任务上表现出高质量性能，因此将这些LLMs引入移动设备以实现更快的响应和更好的隐私保护引起了极大兴趣。然而，LLMs的规模（数十亿个参数）需要高效的压缩才能适应存储有限的设备。在众多压缩技术中，权重聚类是LLM压缩的领先候选方法之一，并得到了现代智能手机的支持。然而，其训练开销对LLM的微调来说是难以承受的。特别是，可微分K均值聚类（DKM）已经显示出在压缩比和准确性回归之间的最先进折衷方案，但其较大的内存复杂性使其几乎不可能应用于训练时的LLM压缩。因此，在本文中，我们提出了一种内存高效的DKM实现，即eDKM，通过创新的技术减小了DKM的内存占用。

    Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM 
    
[^79]: 知识图谱嵌入用于多语言结构化表现的放射学报告

    Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports. (arXiv:2309.00917v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.00917](http://arxiv.org/abs/2309.00917)

    本论文提出一种新颖的轻量级基于图的嵌入方法，用于对放射学报告进行多语言结构化表现。通过连接医学术语和多语言知识库，这种嵌入方法揭示了临床术语之间的关系，提供了对临床医生更易理解、临床更准确的表征。

    

    过去几年中，我们分析临床文本的方式发生了重大变化。语言模型（如BERT）的引入导致了针对（生物）医学领域的调整，如PubMedBERT和ClinicalBERT。这些模型依赖于大量存档的医学文档数据库。虽然在准确度方面表现良好，但解释能力的缺乏和跨语言转移的限制限制了它们在临床设置中的使用。我们引入了一种新颖的轻量级基于图的嵌入方法，专门针对放射学报告。它考虑了报告的结构和组成，同时通过多语言SNOMED临床术语知识库连接报告中的医学术语。生成的图嵌入揭示出临床术语之间的潜在关系，实现了对临床医生更易理解、临床更准确的表征，而不依赖于大规模的预训练数据集。我们展示了这种嵌入的使用。

    The way we analyse clinical texts has undergone major changes over the last years. The introduction of language models such as BERT led to adaptations for the (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely on large databases of archived medical documents. While performing well in terms of accuracy, both the lack of interpretability and limitations to transfer across languages limit their use in clinical setting. We introduce a novel light-weight graph-based embedding method specifically catering radiology reports. It takes into account the structure and composition of the report, while also connecting medical terms in the report through the multi-lingual SNOMED Clinical Terms knowledge base. The resulting graph embedding uncovers the underlying relationships among clinical terms, achieving a representation that is better understandable for clinicians and clinically more accurate, without reliance on large pre-training datasets. We show the use of this embedding
    
[^80]: Halo：评估和降低开源弱大语言模型中的幻觉

    Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])

    [http://arxiv.org/abs/2308.11764](http://arxiv.org/abs/2308.11764)

    本文介绍了一种用于评估和减少开源弱大语言模型中幻觉问题的框架，并探索了知识注入和师生方法等技术来减轻低参数模型中的幻觉问题，实验结果表明，在挑战性领域中，这些模型的幻觉问题得到了减少。

    

    大型语言模型(LLMs)已经彻底改变了自然语言处理(NLP)领域。虽然对于研究和实际应用来说方便，但是与其更大规模的对应模型相比，开源的参数较少的LLMs经常出现严重幻觉问题。本文着重于测量和减少BLOOM 7B中的幻觉问题，该模型是公开提供给研究和商业应用的弱开源LLMs的代表。我们引入了HaloCheck，一种轻量级的无需知识的黑盒子框架，用于量化LLMs中幻觉问题的严重程度。此外，我们探索了知识注入和师生方法等技术，以减轻低参数LLMs中的幻觉问题。我们的实验证明了在这些LLMs的挑战性领域中幻觉问题的减少。

    Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
    
[^81]: 基于双向预测的6D物体姿态估计中的点对注意力的利用

    Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v1 [cs.CV])

    [http://arxiv.org/abs/2308.08518](http://arxiv.org/abs/2308.08518)

    本文提出了一个具有点对注意力感知机制的双向对应预测网络，通过利用模型点和场景点之间的相关性进行点对匹配学习，解决了传统方法对观察质量和遮挡的依赖性问题，并在实验证明其在物体姿态估计任务上优于其他最先进的方法。

    

    传统的几何注册估计方法仅隐式地利用CAD模型，这导致它们对观察质量的依赖性和对遮挡的不足。为了解决这个问题，本文提出了一个具有点对注意力感知机制的双向对应预测网络。该网络不仅要求模型点预测对应关系，还明确地对观察和模型先验之间的几何相似性进行建模。我们的关键见解是，每个模型点和场景点之间的相关性为学习点对匹配提供了关键信息。为了进一步解决特征分布分歧带来的相关性噪声，我们设计了一个简单但有效的伪孪生网络来改善特征的一致性。在线MOD、YCB-Video和Occ-LineMOD的公共数据集上的实验结果表明，所提出的方法在性能上优于其他最先进的方法。

    Traditional geometric registration based estimation methods only exploit the CAD model implicitly, which leads to their dependence on observation quality and deficiency to occlusion.To address the problem,the paper proposes a bidirectional correspondence prediction network with a point-wise attention-aware mechanism. This network not only requires the model points to predict the correspondence but also explicitly models the geometric similarities between observations and the model prior.} Our key insight is that the correlations between each model point and scene point provide essential information for learning point-pair matches. To further tackle the correlation noises brought by feature distribution divergence, we design a simple but effective pseudo-siamese network to improve feature homogeneity.Experimental results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that the proposed method achieves better performance than other state-of-the-art methods under the sa
    
[^82]: 基于神经分类先验的基于物理的角色控制研究

    Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2308.07200](http://arxiv.org/abs/2308.07200)

    本文提出了一种新的基于神经分类先验的学习框架，用于控制基于物理的角色，利用强化学习和离散信息瓶颈，生成高质量逼真的行为。

    

    最近在学习可重用运动先验方面取得了一些进展，证明了它们在生成自然行为方面的有效性。在本文中，我们提出了一种新的学习框架，用于控制基于物理的角色，相比现有最先进的方法，显著改进了运动质量和多样性。所提出的方法利用强化学习（RL）来追踪和模仿来自非结构化运动剪辑的逼真动作，使用离散信息瓶颈，如矢量量化变分自动编码器（VQ-VAE）中所采用的那样。该结构将来自运动剪辑的最相关信息压缩成一个紧凑而且信息丰富的潜在空间，即一个离散的向量量化码空间。通过从经过训练的分类先验分布中采样空间中的码，可以生成高质量逼真的行为，类似于在计算机视觉中使用VQ-VAE。虽然这个先验分布可以通过监督方法进行训练，

    Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervis
    
[^83]: 探索ChatGPT的共情能力

    Exploring ChatGPT's Empathic Abilities. (arXiv:2308.03527v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.03527](http://arxiv.org/abs/2308.03527)

    这项研究探索了基于GPT-3.5的ChatGPT在展现共情回应和情感表达方面的能力。研究结果表明，在91.7%的情况下，ChatGPT能够准确识别情感并产生适当的回答。

    

    共情通常被理解为分享和理解他人的心态或情绪的能力。随着聊天机器人在各个领域的增加应用，例如儿童寻求作业帮助、个人寻求医疗建议以及人们将聊天机器人作为日常伴侣，共情在人机交互中的重要性变得更加明显。因此，我们的研究调查了基于GPT-3.5的ChatGPT在展现共情回应和情感表达方面的能力。我们分析了以下三个方面：(1)理解和表达情感、(2)并行情感回应以及(3)共情个性。因此，我们不仅在各个共情方面评估了ChatGPT并将其与人类行为进行比较，还展示了一种分析聊天机器人共情能力的可能方式。我们的研究结果显示，在91.7%的情况下，ChatGPT能够准确识别情感并产生适当的回答。

    Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, e.g., children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. We analyzed the following three aspects: (1) understanding and expressing emotions, (2) parallel emotional response, and (3) empathic personality. Thus, we not only evaluate ChatGPT on various empathy aspects and compare it with human behavior but also show a possible way to analyze the empathy of chatbots in general. Our results show, that in 91.7% of the cases, ChatGPT was able to correctly identify emotions and produces appropriate answers. In c
    
[^84]: 可转移的图神经指纹模型快速应对未来生物威胁

    Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.01921](http://arxiv.org/abs/2308.01921)

    该论文提出了一种可转移的图神经指纹模型，用于快速应对未来的生物威胁。通过利用包含30万种候选药物和23个冠状病毒蛋白靶的COVID-19药物对接数据集，训练了高通量虚拟COVID-19药物筛选的图神经指纹模型。与传统指纹方法相比，该模型在对接得分上具有较高的预测准确性，并且提出了可转移的图神经指纹方法，能够适用于未知的靶点。

    

    基于配体结合亲和力的药物分子快速筛选是药物发现管线中的重要步骤。图神经指纹是一种用于开发高通量和高准确性分子对接代理的有希望方法。在这项研究中，我们建立了一个包含约30万种药物候选物和23个冠状病毒蛋白靶的COVID-19药物对接数据集。利用这个数据集，我们训练了图神经指纹对接模型，用于高通量虚拟COVID-19药物筛选。图神经指纹模型在对接得分上具有很高的预测准确性，对大多数对接靶点的均方误差低于0.21 kcal/mol，相比传统圆形指纹方法有显著改进。为了使神经指纹适用于未知的靶点，我们还提出了一种在多个靶点上训练的可转移的图神经指纹方法。

    Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
    
[^85]: 使用端到端视频异常检测系统来基准测试Jetson边缘设备

    Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.16834](http://arxiv.org/abs/2307.16834)

    本论文实现了一个端到端的视频异常检测系统，通过从监控视频输入进行犯罪现场异常检测，并在多个Jetson边缘设备上部署和运行。这是对Jetson平台在深度学习算法执行方面性能的基准测试分析的创新。

    

    创新的嵌入式系统平台，特别是硬件加速，显着影响了深度学习在现实场景中的应用。这些创新将人类劳动转化为自动化的智能系统，应用于自动驾驶、机器人技术、物联网和许多其他有重大影响的应用领域。NVIDIA的Jetson平台是在执行深度学习算法方面能够提供能效和吞吐率最佳性能的先驱之一。先前的大部分基准测试分析都是基于2D图像，并使用单个深度学习模型进行每个比较结果。在本文中，我们实现了一种从监控视频输入的端到端基于视频的犯罪现场异常检测系统，并将该系统部署在多个Jetson边缘设备上（Nano、AGX Xavier、Orin Nano）。比较分析包括将Torch-TensorRT集成为软件。

    Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a softwar
    
[^86]: 混合ASP方法用于半导体制造过程的多目标调度（扩展版本）

    Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v1 [cs.AI])

    [http://arxiv.org/abs/2307.14799](http://arxiv.org/abs/2307.14799)

    本研究通过混合ASP方法解决了实际半导体制造过程的调度问题，将其具体要求建模，并且实现了灵活的机器加工、设置、批处理和维护操作。

    

    现代半导体制造涉及复杂的生产过程，包括数百个操作，从批次发布到完成可能需要数月时间。这些过程中使用的高科技设备多样化，可以在多个阶段上对单个晶圆、批次或批次进行操作，并需要产品特定的设置和专门的维护程序。这种情况与传统的车间调度场景不同，后者具有较不复杂的生产过程和设备，主要关注解决高度组合但抽象的调度问题。在这项工作中，我们通过使用具有差异逻辑的混合ASP模型来对实际的半导体制造过程的调度进行建模，这包括灵活的机器加工、设置、批处理和维护操作。与现有的使用贪婪启发式算法或独立进行调度半导体制造过程的方法不同，

    Modern semiconductor manufacturing involves intricate production processes consisting of hundreds of operations, which can take several months from lot release to completion. The high-tech machines used in these processes are diverse, operate on individual wafers, lots, or batches in multiple stages, and necessitate product-specific setups and specialized maintenance procedures. This situation is different from traditional job-shop scheduling scenarios, which have less complex production processes and machines, and mainly focus on solving highly combinatorial but abstract scheduling problems. In this work, we address the scheduling of realistic semiconductor manufacturing processes by modeling their specific requirements using hybrid Answer Set Programming with difference logic, incorporating flexible machine processing, setup, batching and maintenance operations. Unlike existing methods that schedule semiconductor manufacturing processes locally with greedy heuristics or by independen
    
[^87]: 同卵和异卵双胞胎：句子表示的细粒度语义对比学习

    Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations. (arXiv:2307.10932v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.10932](http://arxiv.org/abs/2307.10932)

    本文提出了一种名为同卵和异卵对比学习（IFTCL）框架，通过同时适应不同的正样本对生成方式，在无监督学习句子表示中解决了对比学习中存在的语义扭曲和语义间隔问题。

    

    利用对比学习显著提高了无监督学习句子表示的效果。该方法通过将正样本与锚定样本聚类来创建所需的嵌入空间。然而，仅仅依靠对比目标可能会导致次优结果，因为它无法区分正样本对之间的微小语义变化。具体而言，常见的数据增强技术经常引入语义扭曲，导致正样本对之间存在语义间隔。虽然InfoNCE损失函数忽略了语义间隔，并在训练期间优先考虑正样本对之间的相似性最大化，从而导致训练模型的语义理解能力不敏感。本文介绍了一种新颖的同卵和异卵对比学习（称为IFTCL）框架，能够同时适应不同的正样本对生成方式。

    The enhancement of unsupervised learning of sentence representations has been significantly achieved by the utility of contrastive learning. This approach clusters the augmented positive instance with the anchor instance to create a desired embedding space. However, relying solely on the contrastive objective can result in sub-optimal outcomes due to its inability to differentiate subtle semantic variations between positive pairs. Specifically, common data augmentation techniques frequently introduce semantic distortion, leading to a semantic margin between the positive pair. While the InfoNCE loss function overlooks the semantic margin and prioritizes similarity maximization between positive pairs during training, leading to the insensitive semantic comprehension ability of the trained model. In this paper, we introduce a novel Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework, capable of simultaneously adapting to various positive pairs generated by differ
    
[^88]: 大型语言模型中的人格特质

    Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])

    [http://arxiv.org/abs/2307.00184](http://arxiv.org/abs/2307.00184)

    该研究介绍了一种综合方法，用于验证大型语言模型（LLMs）生成的文本中展示的人格特质。研究发现，部分LLMs在特定提示配置下模拟的人格可靠且有效，特别是对于更大和经过指导微调的模型。此外，LLMs的输出中的人格特质可以根据需要进行塑造。

    

    大型语言模型（LLMs）的出现彻底改变了自然语言处理，使得能够生成连贯且上下文相关的文本。随着LLMs越来越多地用于驱动对话代理，这些模型通过训练大量人工生成的数据获得的人格特质引起了人们的关注。由于人格是决定交流效果的重要因素，我们提出了一种全面的方法来进行验证的心理测量测试，并对从广泛使用的LLMs生成的文本中展示的人格特质进行量化、分析和塑造。我们发现：1）某些LLMs的输出中模拟的人格（在特定的提示配置下）是可靠和有效的；2）LLM模拟的人格的可靠性和有效性的证据对于更大的和经过指导微调的模型更强；3）LLM输出中的人格可以根据需要的维度进行塑造，以模仿特定的人格特点。

    The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
    
[^89]: 通过强化指令调整来减轻大规模多模态模型中的幻觉问题

    Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14565](http://arxiv.org/abs/2306.14565)

    本论文通过引入第一个大型多样化的视觉指令调整数据集，提出了一种解决大规模多模态模型中幻觉问题的方法。通过设计包含正负指令的数据集和提出的评估方法，能够更准确地衡量模型产生的幻觉。

    

    尽管多模态任务取得了可喜的进展，但当前的大规模多模态模型（LMM）很容易在描述图像和人类指令时产生不一致的幻觉。本文通过引入第一个大型多样化的视觉指令调整数据集LRV-Instruction来解决这个问题。我们的数据集包含由GPT4生成的12万个视觉指令，涵盖了16个开放式指令和答案的视觉与语言任务。与现有研究主要关注正指令样本不同，我们设计了LRV-Instruction以包含更多针对更强的视觉指令调整的正负指令。我们的负指令在两个语义层次上设计：（i）不存在元素操作和（ii）存在元素操作。为了更有效地衡量LMM所产生的幻觉，我们提出了一种新的方法，即GPT4辅助的视觉指令评估（GAVIE）。

    Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
    
[^90]: COVER：一种启发式贪心对抗攻击预训练语言模型中的基于提示学习

    COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])

    [http://arxiv.org/abs/2306.05659](http://arxiv.org/abs/2306.05659)

    本文提出了一种启发式贪心对抗攻击，针对基于提示的模板在PLMs中可能存在的漏洞，通过字符级和单词级的破坏方法进行攻击，取得了较高的攻击成功率。

    

    基于提示的学习已被证明是预训练语言模型（PLMs）中一种有效的方式，特别是在像少量样本场景这样的低资源情况下。然而，PLMs的可信度至关重要，并且在基于模板的提示中已经显示出了潜在的漏洞，可能会误导语言模型的预测，引起严重的安全问题。本文通过在黑盒场景中提出基于提示的对抗攻击手段，揭示了PLMs的一些漏洞。首先，我们设计了字符级别和单词级别的启发式方法来破坏手动模板。然后，我们基于上述启发式破坏方法提出了一种贪心算法进行攻击。最后，我们使用BERT系列模型的三个变种和八个数据集的分类任务评估了我们的方法。广泛的实验结果证明了我们的方法在攻击成功率方面的有效性。

    Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
    
[^91]: 学习地球科学知识理解和利用的基础语言模型

    Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])

    [http://arxiv.org/abs/2306.05064](http://arxiv.org/abs/2306.05064)

    本文首次提出了一个地球科学领域的大型语言模型K2，并开发了各种资源以进一步促进其在地球科学领域中的研究和应用，包括第一个地球科学教学调音数据集GeoSignal和第一个地球科学基准测试GeoBenchmark。我们使用了完整的方法将预先训练的通用领域LLM LLaMA-7B 模型适应到地球科学领域。

    

    大型语言模型(LLM)在自然语言处理的常规领域取得了巨大成功。本文将LLM引入地球科学领域，旨在推进该领域的研究和应用。为此，我们首次提出了地球科学领域的LLM，命名为K2，并开发了一系列资源，以进一步促进LLM在地球科学研究中的应用。例如，我们为LLM提供了第一个地球科学教学调音数据集GeoSignal，旨在将LLM相应与地球科学相关的用户查询对齐。此外，我们还建立了第一个地质科学基准测试GeoBenchmark，以在地球科学环境中评估LLM。在这项工作中，我们尝试使用完整的方法将预先训练的通用领域LLM适应到地球科学领域。具体而言，我们在超过100万篇地球科学文献上进一步训练了LLaMA-7B模型，并利用GeoSignal的监督数据对模型进行微调。此外，我们还分享了一个可以在不同领域中迁移LLM的协议。

    Large language models (LLMs)have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience, with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pretrained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on over 1 million pieces of geoscience literature and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can e
    
[^92]: EmbodiedGPT: 通过思维链预训练的视觉语言模型

    EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.15021](http://arxiv.org/abs/2305.15021)

    EmbodiedGPT是一种端到端的多模态基础模型，通过思维链预训练的方式，赋予具有多模态理解和执行能力的实体代理人。

    

    赋予具有多模态理解和执行能力的实体代理人，我们介绍了一种端到端的多模态基础模型EmbodiedGPT，用于实体智能。为了实现这一目标，我们做出了以下努力：（i）我们创建了一个大规模的嵌入式规划数据集，命名为EgoCOT，该数据集由Ego4D数据集中精选的视频和相应的高质量语言指令组成。具体而言，我们使用“思维链”模式生成一系列子目标，以实现有效的嵌入式规划。（ii）我们推出了一种高质量计划生成的高效训练方法，通过前缀调优将7B大型语言模型（LLM）调整到EgoCOT数据集上。（iii）我们介绍了一种从LLM生成的计划中提取与任务相关特征的范例方法。

    Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated pla
    
[^93]: SpikeCP: 通过极限预测实现延迟自适应可靠脉冲神经网络

    SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])

    [http://arxiv.org/abs/2305.11322](http://arxiv.org/abs/2305.11322)

    这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。

    

    脉冲神经网络（SNN）通过内部事件驱动的神经动态处理时间序列数据，其能量消耗取决于输入演示期间神经元之间交换的脉冲数量。在典型的SNN分类器实现中，决策是在整个输入序列被处理后产生的，导致延迟和能量消耗水平在输入之间是相对均匀的。最近引入的延迟自适应SNN可根据每个示例的难度来定制推断延迟 - 以及随之而来的能耗 - 通过在SNN模型足够“自信”时产生早期决策来实现。

    Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
    
[^94]: PaLM 2 技术报告

    PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])

    [http://arxiv.org/abs/2305.10403](http://arxiv.org/abs/2305.10403)

    PaLM 2 是一种计算效率更高的最先进的语言模型，提供了更好的多语言和推理能力，并且通过使用多种目标进行训练，获得了在不同模型大小的下游任务上显着的改进质量。此外，PaLM 2 还展示了强大的推理能力和稳定的性能表现，使得模型能够更广泛地部署，并且可以控制毒性推理时间，而不会对其他能力产生影响。

    

    我们介绍了 PaLM 2，这是一种新的最先进的语言模型，比其前身 PaLM 在多语言和推理能力方面更加出色，并且计算效率更高。PaLM 2 是一种基于 Transformer 的模型，使用多种目标进行训练。通过对英语和多语言语言以及推理任务的广泛评估，我们展示了 PaLM 2 在不同模型大小的下游任务上具有显着的改进质量，同时展现了比 PaLM 更快和更有效的推理能力。这种改进的效率使得模型能够更广泛地部署，同时也使得模型能够更快地响应，以获得更自然的交互节奏。PaLM 2 展示了强大的推理能力，在 BIG-Bench 和其他推理任务上相对于 PaLM 有巨大的改进。PaLM 2 在一套负责人的 AI 评估中表现出稳定的性能，并且在没有附加运行开销或对其他能力产生影响的情况下，能够对毒性进行推理时间的控制。

    We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
    
[^95]: 命题逻辑程序的序列分解研究

    Sequential decomposition of propositional logic programs. (arXiv:2304.13522v1 [cs.LO])

    [http://arxiv.org/abs/2304.13522](http://arxiv.org/abs/2304.13522)

    研究命题逻辑程序的序列分解，通过分析程序之间的Green关系，为逻辑编程代数理论进一步发展作出了贡献。

    

    最近引入了命题逻辑程序的序列组合。本文通过研究程序之间的 Green 关系 $\mathcal{L,R,J}$，从而研究程序的序列分解。在更广泛的意义上，本文是逻辑编程代数理论的进一步研究。

    The sequential composition of propositional logic programs has been recently introduced. This paper studies the sequential {\em decomposition} of programs by studying Green's relations $\mathcal{L,R,J}$ -- well-known in semigroup theory -- between programs. In a broader sense, this paper is a further step towards an algebraic theory of logic programming.
    
[^96]: 正向人工智能：设计以幸福为导向的人工智能的关键挑战

    Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence. (arXiv:2304.12241v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2304.12241](http://arxiv.org/abs/2304.12241)

    设计以幸福为导向的人工智能系统面临着知识和动机两方面的挑战，包括概念化、测量和优化幸福，设计适当的AI行动，以及激励措施、财务和宣传风险的不一致以及数据获取的缺乏。针对这些挑战，需要在科学理解AI系统对幸福影响方面进行研究，并指导设计行动。

    

    人工智能（AI）正迅速改变社会，迫切需要确保其积极影响。本文采用积极设计方法来解决这个问题，将其视为设计主动支持人类幸福的AI系统的问题。然而，设计以幸福为导向的AI系统是困难的。本文采用控制论的视角，识别了两个类别中的十二个关键挑战：知识缺乏和动机缺乏。知识障碍包括概念化、测量和优化幸福，并设计适当的AI行动方面的挑战。动机障碍包括不一致的激励措施、财务和宣传风险，以及缺乏数据获取阻止了（第三方）对幸福进行研究。为了应对这些挑战，我们提出了一个研究议程，包括推进对AI系统对幸福影响的科学理解，并指导如何进行AI系统的设计行动。

    Artificial Intelligence (AI) is rapidly transforming society, creating an urgent need to ensure its positive impact. In this article, we take a positive design approach towards this issue, viewing it as a matter of designing AI systems that actively support human wellbeing. However, designing wellbeing-aligned AI systems is difficult. This article adopts a cybernetic perspective to identify twelve key challenges across two categories: lack of knowledge and lack of motivation. Knowledge barriers include challenges in conceptualizing, measuring, and optimizing for wellbeing, then designing appropriate AI actions. Motivation barriers include misaligned incentives, financial and publicity risks, and a lack of data access preventing (third-party) research on wellbeing. To address these challenges we have captured our key takeaways in a research agenda related to 1) advancing the scientific understanding of the impact of AI systems on wellbeing, and 2) guiding design actions on how AI system
    
[^97]: 对比调节: 帮助遗忘掩码自编码器的一点小帮助

    Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])

    [http://arxiv.org/abs/2304.10520](http://arxiv.org/abs/2304.10520)

    本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。

    

    掩码图像建模方法，如掩码自编码器（MAE），可以有效地学习输入的丰富表示。但是，为了适应下游任务，由于其丰富的特征不仅捕获了对象而且还包括不相关的图像背景，因此它们需要足够数量的标记数据。相比之下，实例辨别方法侧重于对象。在这项工作中，我们研究如何将MIM的效率和可伸缩性与ID的能力相结合，以在缺少大量标记数据的情况下执行下游分类。为此，我们引入了掩码自编码器对比调整（MAE-CT），这是一种顺序方法，可以将最近邻对比学习（NNCLR）应用于预先训练的MAE。MAE-CT调整了丰富的特征，使它们形成对象的语义聚类，而不使用任何标签。应用于大型和巨型Vision Transformer（ViT）模型时，MAE-CT在线性探测，k-均值聚类和半监督少量样本学习方面匹配或超越了在ImageNet上训练的先前的自我监督方法。

    Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
    
[^98]: 大规模语言模型中潜在空间理论对应新兴能力

    A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])

    [http://arxiv.org/abs/2304.09960](http://arxiv.org/abs/2304.09960)

    本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。

    

    语言并不是随机生成，而是为了传递信息。语言与其底层含义之间存在强烈的关联，在其相关性方面有着严重偏差的稀疏联合分布。此外，由于稀疏性，这些高峰值恰好与语言的边缘分布匹配。随着大数据和大模型上训练的LLMs的出现，我们现在可以精确评估语言的边缘分布，这提供了一种方便的探索联合分布稀疏结构实现有效推理的方式。在本文中，我们将语言分类为明确与{\epsilon}-模糊，并提出定量结果，以表明LLMs的新兴能力（例如语言理解、上下文学习、思路启发以及有效指令微调）都可以归因于对稀疏联合分布进行贝叶斯推断。

    Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
    
[^99]: 基于深度强化学习的启发式方法在自旋玻璃基态问题上的测试：更广泛的视角

    Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture. (arXiv:2302.10848v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2302.10848](http://arxiv.org/abs/2302.10848)

    该研究提出了一种基于深度强化学习的启发式方法，用于增强组合优化过程，并在自旋玻璃基态问题上取得了改进结果。研究结果表明，相对于传统方法如模拟退火或并行退火，强化学习方法在提供相当质量的结果之前减少了运行时间。

    

    在Changjun Fan等人的研究中，作者们提出了一种基于深度强化学习的方法来增强组合优化启发式方法。具体而言，他们针对几个自旋玻璃基态问题展示了结果，其中非平面网络上的实例通常是NP困难的，与几种基于蒙特卡洛的方法进行比较，如模拟退火（SA）或并行退火（PT）。事实上，这些结果表明，强化学习相对于SA或PT改进了结果，或者至少在获得与其他方法相当质量的结果之前，减少了启发式方法的运行时间。为了证明他们的方法“优越”，作者采取了两种基本策略：（1）调用商业GUROBI求解器获取一些精确基态样本作为测试基准进行比较，以及（2）将他们的方法与其他策略进行了直接对比。

    In Changjun Fan et al. [Nature Communications https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep reinforced learning approach to augment combinatorial optimization heuristics. In particular, they present results for several spin glass ground state problems, for which instances on non-planar networks are generally NP-hard, in comparison with several Monte Carlo based methods, such as simulated annealing (SA) or parallel tempering (PT). Indeed, those results demonstrate that the reinforced learning improves the results over those obtained with SA or PT, or at least allows for reduced runtimes for the heuristics before results of comparable quality have been obtained relative to those other methods. To facilitate the conclusion that their method is ''superior'', the authors pursue two basic strategies: (1) A commercial GUROBI solver is called on to procure a sample of exact ground states as a testbed to compare with, and (2) a head-to-head comparison between th
    
[^100]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^101]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^102]: 通过预基调过滤消除basecalling中的浪费计算的TargetCall

    TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2212.04953](http://arxiv.org/abs/2212.04953)

    TargetCall通过预基调过滤，消除了basecalling中的浪费计算，提高了基因组分析流程的效率。

    

    Basecalling是纳米孔测序分析中的重要步骤，它将纳米孔测序仪的原始信号转换为核酸序列，即reads。最先进的basecallers使用复杂的深度学习模型实现高度的basecalling准确性。这使得basecalling在计算上效率低下且内存消耗大，成为整个基因组分析流程的瓶颈。然而，对于许多应用来说，大多数reads与感兴趣的参考基因组不匹配（即目标参考基因组），因此会在后续的基因组流程步骤中被丢弃，浪费了basecalling的计算。为了解决这个问题，我们提出了TargetCall，这是第一个用于消除basecalling中浪费计算的预基调过滤器。TargetCall的关键思想是在basecalling之前丢弃不会与目标参考基因组匹配的reads（即非目标reads）。TargetCall由两个主要组件组成：（1）LightCall，一个轻量级的神经网络basecaller，产生噪声reads；

    Basecalling is an essential step in nanopore sequencing analysis where the raw signals of nanopore sequencers are converted into nucleotide sequences, i.e., reads. State-of-the-art basecallers employ complex deep learning models to achieve high basecalling accuracy. This makes basecalling computationally-inefficient and memory-hungry; bottlenecking the entire genome analysis pipeline. However, for many applications, the majority of reads do no match the reference genome of interest (i.e., target reference) and thus are discarded in later steps in the genomics pipeline, wasting the basecalling computation. To overcome this issue, we propose TargetCall, the first pre-basecalling filter to eliminate the wasted computation in basecalling. TargetCall's key idea is to discard reads that will not match the target reference (i.e., off-target reads) prior to basecalling. TargetCall consists of two main components: (1) LightCall, a lightweight neural network basecaller that produces noisy reads;
    
[^103]: 奖励并非必要：如何为终身学习创建一个组合性自我保护智能体

    Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.10851](http://arxiv.org/abs/2211.10851)

    这项研究表明，我们可以使用内在动机衡量标准而不依赖于奖励来创建一个具有自我保护能力的智能体。

    

    强化学习认为最大化奖励和避免惩罚是解释目标导向行为的核心。然而，在一生中，生物需要学习关于世界结构的许多不同方面：世界状态和状态转移动力学。随着智能体融入新知识，状态组合的数量以指数级增长，并且对于给定的状态组合，没有明显定义的预设奖励或成本的加权组合，因为这样的加权需要在智能体在世界中的经验之前对好的和坏的组合进行编码。因此，我们必须在大状态空间中开发更自然的行为和动机模型。我们展示了仅使用内在动机衡量标准（即赋予能力）是可能的，该标准衡量智能体在转移操作者下实现许多可能未来的能力。我们建议将赋予能力扩展到分层状态空间中。

    Reinforcement Learning views the maximization of rewards and avoidance of punishments as central to explaining goal-directed behavior. However, over a life, organisms will need to learn about many different aspects of the world's structure: the states of the world and state-vector transition dynamics. The number of combinations of states grows exponentially as an agent incorporates new knowledge, and there is no obvious weighted combination of pre-existing rewards or costs defined for a given combination of states, as such a weighting would need to encode information about good and bad combinations prior to an agent's experience in the world. Therefore, we must develop more naturalistic accounts of behavior and motivation in large state-spaces. We show that it is possible to use only the intrinsic motivation metric of empowerment, which measures the agent's capacity to realize many possible futures under a transition operator. We propose to scale empowerment to hierarchical state-space
    
[^104]: DisenPOI: 解开顺序和地理影响的POI推荐

    DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation. (arXiv:2210.16591v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.16591](http://arxiv.org/abs/2210.16591)

    本文提出了DisenPOI，一个新颖的基于双图的POI推荐解开框架，通过利用顺序和地理关系并使用自我监督解开这两种影响，以提高推荐性能和可解释性。

    

    POI（兴趣点）推荐在各种位置感知服务中起着至关重要的作用。已经观察到POI推荐受到顺序和地理影响的驱动。然而，由于在推荐过程中没有明确指定主导影响的注释标签，现有方法往往会混淆这两种影响，这可能导致次优的推荐性能和差的可解释性。本文通过提出DisenPOI，一个新颖的基于双图的POI推荐解开框架，来解决上述挑战。DisenPOI在两个独立的图上同时利用顺序和地理关系，并通过自我监督解开这两种影响。与现有方法相比，我们的模型的关键创新之处在于使用对比学习提取顺序和地理影响的解开表示。具体而言，我们根据签到序列构建了一个地理图和一个顺序图。

    Point-of-Interest (POI) recommendation plays a vital role in various location-aware services. It has been observed that POI recommendation is driven by both sequential and geographical influences. However, since there is no annotated label of the dominant influence during recommendation, existing methods tend to entangle these two influences, which may lead to sub-optimal recommendation performance and poor interpretability. In this paper, we address the above challenge by proposing DisenPOI, a novel Disentangled dual-graph framework for POI recommendation, which jointly utilizes sequential and geographical relationships on two separate graphs and disentangles the two influences with self-supervision. The key novelty of our model compared with existing approaches is to extract disentangled representations of both sequential and geographical influences with contrastive learning. To be specific, we construct a geographical graph and a sequential graph based on the check-in sequence of a 
    
[^105]: ConSpec: 突出强化学习中的关键步骤，实现快速学习和泛化

    ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05845](http://arxiv.org/abs/2210.05845)

    ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。

    

    在现实生活中，成功往往取决于多个关键步骤，这些步骤在时间上相距较远，与最终奖励也相距甚远。传统的强化学习方法在信用分配方面依赖Bellman方程，很难识别这些关键步骤。本文提出了一种新的强化学习算法，使用离线对比学习来确定关键步骤。这个算法被称为对比内省（ConSpec），可以添加到任何现有的强化学习算法中。ConSpec通过一种新颖的对比损失学习任务中的关键步骤的原型，并在当前状态与这些原型之一匹配时提供内在奖励。ConSpec中的原型在信用分配方面具有两个关键优势：（1）它们使得能够迅速识别所有关键步骤；（2）它们以容易解释的方式实现这一点，使得在感觉特征改变时可以进行超出分布的泛化。与其他当代的强化学习方法不同，

    In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
    
[^106]: BAFFLE: 离线增强学习中的后门攻击

    BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04688](http://arxiv.org/abs/2210.04688)

    本文研究离线增强学习中的后门攻击，通过向数据中添加扰动，使得智能体在注入触发器的观测值上采取低奖励动作，从而提出了BAFFLE方法。

    

    越来越多的研究关注于强化学习（RL）方法，允许智能体通过与环境的交互中收集的试错经验进行学习。最近，离线RL成为一种流行的RL范例，因为它节省了与环境的交互。在离线RL中，数据提供者共享大规模的预先收集的数据集，其他人可以在不与环境交互的情况下训练高质量的智能体。这种范例在机器人控制、自动驾驶等关键任务中表现出有效性。然而，较少关注研究离线RL系统的安全威胁。本文关注后门攻击，其中一些扰动被添加到数据（观测值）中，使得在给定正常观测值的情况下，智能体采取高奖励的动作，在注入触发器的观测值上采取低奖励的动作。在本文中，我们提出了BAFFLE（离线增强学习中的后门攻击），这是一种方法。

    A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
    
[^107]: LambdaKG:基于预训练语言模型的知识图谱嵌入库

    LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.00305](http://arxiv.org/abs/2210.00305)

    LambdaKG是一个基于预训练语言模型的知识图谱嵌入库，提供了多个预训练语言模型和支持多种任务，如知识图谱补全、问答、推荐和知识探索。

    

    知识图谱（KG）通常具有异构的图结构和文本丰富的实体/关系信息。基于文本的KG嵌入可以通过使用预训练语言模型对描述进行编码来表示实体，但目前尚无专门为PLM与KG设计的开源库。本文介绍了LambdaKG，一个带有多个预训练语言模型（如BERT，BART，T5，GPT-3）并支持各种任务（如知识图谱补全，问答，推荐和知识探索）的KGE库。LambdaKG在https://github.com/zjunlp/PromptKG/tree/main/lambdaKG上公开开源，并提供了演示视频和长期维护。

    Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at this http URL and long-term maintenance.
    
[^108]: 机器学习和计算机视觉技术在蜜蜂监测应用中的应用

    Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2208.00085](http://arxiv.org/abs/2208.00085)

    本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。

    

    机器学习和计算机视觉是快速发展的领域，已经证明能够解决非常复杂的任务。它们可以用于监测蜜蜂群体并检查其健康状况，从而在情况变得严重之前，识别出潜在危险状态，或者更好地计划定期蜜蜂群体检查，从而节省重要的成本。本文概述了用于蜜蜂监测的最先进的计算机视觉和机器学习应用，并以自动化蜜蜂计数算法为例展示了这些方法的潜力。本文面向兽医学和蜜蜂学专业人员和专家，旨在向他们介绍机器学习的可能性，因此每个应用类别都以简要的理论介绍和与其基本方法相关的动机开篇。我们希望这篇论文能激发其他科学家的灵感...

    Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
    
[^109]: 适应性联邦相关框架用于时空图学习

    An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning. (arXiv:2206.03420v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03420](http://arxiv.org/abs/2206.03420)

    该论文提出了一种适应性联邦相关框架，用于处理时空数据中的复杂预测任务，并解决了同时融合空间信息的相互依赖性和动态的时间变化的挑战。

    

    由于相关应用在许多领域的快速发展，时空数据包含丰富的信息并近年来受到广泛研究。例如，医疗机构经常使用附着在患者不同部位的电极，分析带有空间和时间特征的电生理数据进行健康评估和疾病诊断。现有研究主要使用深度学习技术，如卷积神经网络（CNN）或循环神经网络（RNN），提取隐藏的时空特征。然而，同时融合空间信息的相互依赖性和动态的时间变化是具有挑战性的。实际上，对于一个利用这些时空特征来完成复杂预测任务的模型，通常需要大量的训练数据才能获得令人满意的模型性能。考虑到上述挑战，我们提出了一种适应性联邦相关框架，名为.....

    Spatial-temporal data contains rich information and has been widely studied in recent years due to the rapid development of relevant applications in many fields. For instance, medical institutions often use electrodes attached to different parts of a patient to analyse the electorencephal data rich with spatial and temporal features for health assessment and disease diagnosis. Existing research has mainly used deep learning techniques such as convolutional neural network (CNN) or recurrent neural network (RNN) to extract hidden spatial-temporal features. Yet, it is challenging to incorporate both inter-dependencies spatial information and dynamic temporal changes simultaneously. In reality, for a model that leverages these spatial-temporal features to fulfil complex prediction tasks, it often requires a colossal amount of training data in order to obtain satisfactory model performance. Considering the above-mentioned challenges, we propose an adaptive federated relevance framework, nam
    
[^110]: 物理-不可知对象的元学习再抓取策略

    Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.11110](http://arxiv.org/abs/2205.11110)

    本研究提出了一种元学习算法，ConDex，用于自主识别具有未知物理属性的非均匀对象，并实现精确的抓取点估计。与现有方法相比，ConDex在性能上表现出优势，并且生成了两个新的对象数据集用于进一步研究。

    

    在真实世界的应用中，抓取非均匀对象仍然是一项具有挑战性的任务，原因是存在未知的物理属性，如质量分布和摩擦系数。在本研究中，我们提出了一种称为ConDex的元学习算法，它将条件神经过程（CNP）与DexNet-2.0相结合，使用深度图像自主识别对象的潜在物理属性。ConDex能够有效地从有限试验中获取物理嵌入，实现精确的抓取点估计。此外，ConDex能够以在线方式迭代地更新预测的抓取质量。据我们所知，我们是第一个生成两个面向非均匀物理属性的对象数据集，包括不同的质量分布和摩擦系数。在仿真环境中进行的广泛评估表明，ConDex在性能上优于DexNet-2.0和现有的基于元学习的抓取流水线。此外，ConDex展示了

    Grasping inhomogeneous objects in real-world applications remains a challenging task due to the unknown physical properties such as mass distribution and coefficient of friction. In this study, we propose a meta-learning algorithm called ConDex, which incorporates Conditional Neural Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical properties of objects using depth images. ConDex efficiently acquires physical embeddings from limited trials, enabling precise grasping point estimation. Furthermore, ConDex is capable of updating the predicted grasping quality iteratively from new trials in an online fashion. To the best of our knowledge, we are the first who generate two object datasets focusing on inhomogeneous physical properties with varying mass distributions and friction coefficients. Extensive evaluations in simulation demonstrate ConDex's superior performance over DexNet-2.0 and existing meta-learning-based grasping pipelines. Furthermore, ConDex shows
    
[^111]: 模型重新编程：资源高效的跨领域机器学习

    Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.10629](http://arxiv.org/abs/2202.10629)

    模型重新编程是一种资源高效的跨领域机器学习方法，通过重新利用和重用预训练模型，无需模型细调即可在目标领域解决任务。这种方法在许多应用中优于迁移学习和从头训练。

    

    在视觉、语言和语音等数据丰富的领域中，深度学习在提供高性能的特定任务模型方面占据主导地位，甚至可以学习通用的任务无关表示以便有效地进行下游任务的细调。然而，在资源有限的领域中，深度学习仍面临多个挑战，包括：（i）数据有限；（ii）模型开发成本受限；（iii）缺乏足够的预训练模型以便有效进行细调。本文概述了模型重新编程的概念来弥合这一差距。模型重新编程通过从源领域重新利用和重用一个精心开发的预训练模型，在目标领域解决任务而无需进行模型细调，从而实现了资源高效的跨领域机器学习，源领域和目标领域可以差异巨大。在许多应用中，模型重新编程优于迁移学习和从头训练。本文阐述了模型重新编程的方法论，并总结了现有的应用情况。

    In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use c
    
[^112]: PolicyCleanse：强化学习中的后门检测与缓解

    PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning. (arXiv:2202.03609v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03609](http://arxiv.org/abs/2202.03609)

    本文提出了PolicyCleanse方法，用于检测和缓解多智能体强化学习系统中的后门攻击。该方法基于激活的特洛伊智能体累积奖励的下降特性进行检测，并尝试缓解其特洛伊行为。

    

    尽管强化学习在现实世界中的应用越来越受欢迎，但RL系统的安全性和稳健性仍值得更多关注和探索。最近的研究揭示了在多智能体强化学习环境中，可以向受害者智能体（即特洛伊智能体）注入后门触发动作，这可能导致灾难性失败。为确保RL智能体对恶意后门的安全性，本文提出了在多智能体竞争性强化学习系统中的后门检测问题，其目标是检测特洛伊智能体以及相应的潜在触发动作，并进一步尝试缓解其特洛伊行为。为解决这个问题，我们提出了基于激活的特洛伊智能体累积奖励在几个时间步之后明显下降的PolicyCleanse。除了PolicyCleanse，我们还设计了一个机器未完成的部分...

    While real-world applications of reinforcement learning are becoming popular, the security and robustness of RL systems are worthy of more attention and exploration. In particular, recent works have revealed that, in a multi-agent RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action. To ensure the security of RL agents against malicious backdoors, in this work, we propose the problem of Backdoor Detection in a multi-agent competitive reinforcement learning system, with the objective of detecting Trojan agents as well as the corresponding potential trigger actions, and further trying to mitigate their Trojan behavior. In order to solve this problem, we propose PolicyCleanse that is based on the property that the activated Trojan agents accumulated rewards degrade noticeably after several timesteps. Along with PolicyCleanse, we also design a machine unl
    
[^113]: 通过群等变卷积量子Ansatze加速学习量子态

    Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2112.07611](http://arxiv.org/abs/2112.07611)

    通过建立$S_n$-等变卷积量子Ansatze，我们证明了其能够在具有SU($d$)对称性的广泛量子机器学习问题中生成任意幺正矩阵，同时验证了4-local SU($d$)对称幺正矩阵的可实现性。

    

    我们基于Schur-Weyl对偶，建立了一个理论框架，用于$S_n$-等变卷积量子电路和SU$(d)$对称性，将Jordan的置换量子计算(PQC)形式主义扩展和泛化。我们利用Okounkov-Vershik方法来证明Harrow在奇异边表示基之间的等价性，并利用Young-Jucys-Murphy(YJM)元素建立了$S_n$-等变卷积量子交替Ansatze($S_n$-CQA)。我们证明了$S_n$-CQA能够在任何给定的$S_n$ irrep扇区中生成任意幺正矩阵，这可以作为具有SU($d$)对称性的广泛量子机器学习问题的通用模型。我们的方法提供了另一种证明量子近似优化算法(QAOA)的普适性的方式，并验证了4-local SU($d$)对称幺正矩阵是可实现的。

    We develop a theoretical framework for $S_n$-equivariant convolutional quantum circuits with SU$(d)$-symmetry, building on and significantly generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In particular, we utilize the Okounkov-Vershik approach to prove Harrow's statement (Ph.D. Thesis 2005 p.160) on the equivalence between $\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the $S_n$-equivariant Convolutional Quantum Alternating Ans\"atze ($S_n$-CQA) using Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate any unitary in any given $S_n$ irrep sector, which may serve as a universal model for a wide array of quantum machine learning problems with the presence of SU($d$) symmetry. Our method provides another way to prove the universality of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local SU($d$) symmetric unitaries are su
    

