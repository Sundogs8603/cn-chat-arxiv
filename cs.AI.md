# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation](https://arxiv.org/abs/2403.07869) | TeleMoMa 是一种面向移动操作的模块化多功能远程操作系统，通过整合多种人机接口、降低门槛且具有通用性，为移动操作器提供了全身远程操作的解决方案。 |
| [^2] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^3] | [MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric](https://arxiv.org/abs/2403.07839) | 提出了Module-wise Pruning Error（MoPE）指标，用于评估CLIP模块重要性，在预训练和任务特定微调压缩阶段提出了统一的剪枝框架。 |
| [^4] | [Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling](https://arxiv.org/abs/2403.07818) | 本文研究利用多个数据集进行深度学习超声心动图分割，在处理部分标记数据时采用改进的交叉熵损失函数。 |
| [^5] | [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816) | BTX方法提供了一种高效的训练大型语言模型以具备多个专业领域能力的方法，通过将种子模型的分支培训成专家，然后在Mixture-of-Expert层中将它们汇集为专家，并利用MoE微调阶段学习基于标记的路由，从而实现了最佳的精度和效率权衡。 |
| [^6] | [Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815) | Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。 |
| [^7] | [Beyond Memorization: The Challenge of Random Memory Access in Language Models](https://arxiv.org/abs/2403.07805) | 本文研究了语言模型在访问内存时的挑战，发现通过背诵和置换等技术可以改善语言模型的随机内存访问能力，从而在开放域问题回答任务中取得显著改进。 |
| [^8] | [Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data](https://arxiv.org/abs/2403.07797) | jam-pgm机制在合成数据生成中能够联合选择公共数据和私密数据，并且能够在公共数据分布存在偏差的情况下优于其他机制。 |
| [^9] | [DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation](https://arxiv.org/abs/2403.07788) | DexCap是一个可移植的手部动作捕捉系统，结合DexIL算法从人类手部运动数据中训练机器人技能，具有精确追踪和复制人类动作的能力。 |
| [^10] | [Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations](https://arxiv.org/abs/2403.07769) | 文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。 |
| [^11] | [Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings](https://arxiv.org/abs/2403.07750) | 这项研究提出了一种新方法，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。 |
| [^12] | [Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph](https://arxiv.org/abs/2403.07748) | 论文研究了移动计算中的探索和会合两个基本问题，提出了分别能在图中$m$个同步时间步实现集体探索和$\frac{3}{2}m$时间步内实现会合的算法。 |
| [^13] | [FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models](https://arxiv.org/abs/2403.07747) | FineMath是一个用于评估中文大型语言模型数学推理能力的细粒度数学评估基准数据集，涵盖小学数学中的主要概念，划分为17类数学问题，并手动注释难度级别，实验证实在数学方面仍有改进空间。 |
| [^14] | [Probabilistic Easy Variational Causal Effect](https://arxiv.org/abs/2403.07745) | 论文提出了一种称为Probabilistic Easy Variational Causal Effect (PEACE)的函数，可以测量X对Y的直接因果效应，适用于连续和离散情况，通过管理概率密度值强度$d\ge 0$来实现干预。 |
| [^15] | [Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs](https://arxiv.org/abs/2403.07743) | 提出了一种专家混合方案，用于在计算病理学系统中检测和排除五种显著的工件，并应用概率阈值处理。 |
| [^16] | [Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation](https://arxiv.org/abs/2403.07741) | 本研究提出了一种使用深度集成来量化多阶段6D物体姿态估计方法不确定性的方法。 |
| [^17] | [DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven Segmentation](https://arxiv.org/abs/2403.07733) | 通过引入数据驱动分割和层次分割程序，DSEG-LIME改进了图像解释能力，提高了图像分类的可解释性。 |
| [^18] | [Balancing Fairness and Accuracy in Data-Restricted Binary Classification](https://arxiv.org/abs/2403.07724) | 研究提出了一个框架，直接分析最优贝叶斯分类器在数据限制的情况下的行为，以平衡准确性和公平性。 |
| [^19] | [Multi-modal Auto-regressive Modeling via Visual Words](https://arxiv.org/abs/2403.07720) | 本研究成功实现了多模态自回归建模，并通过引入视觉词的概念，将视觉特征映射到大语言模型的词汇表上，为视觉建模提供了监督信息。 |
| [^20] | [WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?](https://arxiv.org/abs/2403.07718) | 该研究探究了基于大型语言模型的代理在通过web浏览器与软件交互时的能力，提出了WorkArena和BrowserGym两个工具，在29个任务的基准测试中显示出潜力，但也揭示了实现完全任务自动化仍存在挑战。 |
| [^21] | [SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces](https://arxiv.org/abs/2403.07711) | 提出了一种基于状态空间模型（SSMs）的方法，用于解决使用扩散模型生成长视频序列时注意力层内存消耗增长快、限制较大的问题 |
| [^22] | [Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards](https://arxiv.org/abs/2403.07708) | 引入对比奖励的方法提高了从人类反馈中的强化学习的效果，改善了奖励模型的鲁棒性，鼓励对基准的改善，并能根据任务的难度进行校准。 |
| [^23] | [Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning](https://arxiv.org/abs/2403.07704) | 对称 Q-learning方法通过添加合成噪声来减少贝尔曼误差的偏斜，在在线强化学习中提高了样本效率。 |
| [^24] | [Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization](https://arxiv.org/abs/2403.07693) | 使用大型和小型语言模型的新型数据增强框架，通过重新生成评论来实现观点摘要的去偏见化，避免了大型语言模型数据增强可能存在的问题和高昂成本。 |
| [^25] | [Reference-free Monolithic Preference Optimization with Odds Ratio](https://arxiv.org/abs/2403.07691) | 本文介绍了一种无参考单体赔率比偏好优化算法ORPO，在SFT过程中通过轻微惩罚不受欢迎的生成风格，消除了额外的偏好对齐阶段 |
| [^26] | [Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons](https://arxiv.org/abs/2403.07688) | 重新评估深度神经网络中的死亡神经元现象，提出了Demon Pruning（DemP）方法，通过控制死亡神经元的产生，动态实现网络稀疏化。 |
| [^27] | [Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost](https://arxiv.org/abs/2403.07687) | 提出一种方法来识别数据，以平衡模型性能和注释成本 |
| [^28] | [Scalable Spatiotemporal Prediction with Bayesian Neural Fields](https://arxiv.org/abs/2403.07657) | 该论文提出了贝叶斯神经场（BayesNF），结合了深度神经网络和分层贝叶斯推断，用于处理大规模时空预测问题。 |
| [^29] | [Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2403.07630) | 本研究提出一种上下文原型感知学习策略（CPAL），通过减少知识偏差以增强原型表示能力，从而捕获实例语义的多样和细粒度特征属性。 |
| [^30] | [Multiple Latent Space Mapping for Compressed Dark Image Enhancement](https://arxiv.org/abs/2403.07622) | 该研究提出了一种基于变分自动编码器的新颖潜在空间映射网络，以处理增强压缩暗图像时避免压缩伪影放大的问题。 |
| [^31] | [Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning](https://arxiv.org/abs/2403.07611) | 该论文介绍了一种新颖的机器遗忘算法，分别采用部分失忆式遗忘和逐层部分更新的方法，以更高效地在训练模型中删除知识。 |
| [^32] | [Couler: Unified Machine Learning Workflow Optimization in Cloud](https://arxiv.org/abs/2403.07608) | 设计并实现了Couler系统，用于云中统一机器学习工作流优化，主要见解在于能够使用自然生成ML工作流 |
| [^33] | [Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation](https://arxiv.org/abs/2403.07605) | 提出NegOpt方法，通过监督微调和强化学习优化负面提示的生成，显著提高图像生成质量，超越其他方法并构建了负面提示数据集。 |
| [^34] | [Perennial Semantic Data Terms of Use for Decentralized Web](https://arxiv.org/abs/2403.07587) | 提出了一种新颖的数据使用条款（DToU）的形式化描述，以及一个 DToU 推理器，使用户和应用程序可以使用本地知识指定其DToU政策的部分，从而验证合规性，并防止数据滥用。 |
| [^35] | [Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments](https://arxiv.org/abs/2403.07586) | 本研究提出了在模拟家庭环境中进行联邦学习的新方法，旨在评估机器人行为的社会适宜性，并结合持续学习方法，使机器人可以从彼此的经验中学习社会规范。 |
| [^36] | [Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)](https://arxiv.org/abs/2403.07573) | 本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。 |
| [^37] | [An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning](https://arxiv.org/abs/2403.07566) | 本文提出了一种使用多步深度强化学习改进血糖控制策略的算法，通过转换BG控制问题的形式化，考虑药物作用的延迟和持久性，提高了效率。 |
| [^38] | [Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding](https://arxiv.org/abs/2403.07559) | 提出了Ensembling Prioritized Hybrid Policies (EPH)方法，通过选择性通信模块和三种高级推理策略，提高了基于通信的多智能体路径规划解决方案的性能。 |
| [^39] | [The future of document indexing: GPT and Donut revolutionize table of content processing](https://arxiv.org/abs/2403.07553) | 该论文介绍了一种利用Donut和OpenAI GPT-3.5 Turbo两种前沿AI模型自动提取文件中结构化信息的创新方法，在建筑规格文件的目录处理中取得了显著的准确性，代表了文档索引领域向自动化信息提取迈出的重要一步。 |
| [^40] | [Online Continual Learning For Interactive Instruction Following Agents](https://arxiv.org/abs/2403.07548) | 我们提出了针对具身代理的两种持续学习设置：学习新行为和新环境。同时，我们通过自信度得分来更新存储的信息，从而避免需要任务边界信息的问题。 |
| [^41] | [WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces](https://arxiv.org/abs/2403.07540) | 本文介绍了一种能够安全模拟勒索软件攻击的模拟器，通过生成存储I/O痕迹，并利用这些痕迹来训练机器学习模型，有效地检测勒索软件，为发展负责任的网络安全工具提供了实际应用。 |
| [^42] | [Relevance Score: A Landmark-Like Heuristic for Planning](https://arxiv.org/abs/2403.07510) | 提出了一种新颖的“相关性分数”启发式方法，可以帮助识别在大多数但不是所有计划中出现的事实或行动，相比于传统地标启发式方法，在缺乏明确定义地标的问题上具有更好的性能表现。 |
| [^43] | [Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation](https://arxiv.org/abs/2403.07500) | 提出了分块低秩适应（LoRA）方法，在文本到图像生成中实现了有效的个性化和风格化细粒度微调。 |
| [^44] | [A Deep Learning Approach to Diabetes Diagnosis](https://arxiv.org/abs/2403.07483) | 采用深度学习方法，提出一种无创糖尿病诊断方法，通过反向传播神经网络和数据平衡技术，在准确性、敏感性和特异性方面取得显著改进 |
| [^45] | [Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2403.07440) | 该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。 |
| [^46] | [Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning](https://arxiv.org/abs/2403.07404) | 早期退出网络在持续学习中展现出降低遗忘和在资源利用上表现优异的特点 |
| [^47] | [From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios](https://arxiv.org/abs/2403.07403) | 将食物识别从食堂场景泛化到日常生活场景，提出了两个新基准数据集DailyFood-172和DailyFood-16，旨在评估方法的可转移性。 |
| [^48] | [Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs](https://arxiv.org/abs/2403.07398) | 提出了COM2数据集，通过在常识知识图中抽样多跳逻辑查询并结合大型语言模型，显著提高了语言模型在复杂推理能力方面的性能。 |
| [^49] | [Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images](https://arxiv.org/abs/2403.07389) | 通过引入新的训练设计，从而利用辅助的免疫荧光图像域，我们提出了一种用于从双向到单向IHC图像的任务感知域翻译的方法，该方法在下游分割任务中表现出比基线方法更好的效果。 |
| [^50] | [SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models](https://arxiv.org/abs/2403.07384) | S2L提出了一种通过总结小模型的训练轨迹，来指导大型语言模型数据选择的方法，显著提高了数学问题解决中监督微调的数据效率，并在数据集性能上表现优异。 |
| [^51] | [Gabor-guided transformer for single image deraining](https://arxiv.org/abs/2403.07380) | 本文提出了一种基于Gabor引导的变压器（Gabformer）用于单幅图像去雨，通过融入Gabor滤波器处理的信息提高了对局部纹理特征的关注，以及增强了模型对噪声的鲁棒性。 |
| [^52] | [NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning](https://arxiv.org/abs/2403.07376) | 本文提出了一种名为NavCoT的新策略，在视觉与语言导航中通过学习解耦推理，实现了自主导航决策，有效减轻了领域差距。 |
| [^53] | [A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees](https://arxiv.org/abs/2403.07363) | 提出了一种新的直觉模糊决策树随机森林集成方法，融合了随机性、模糊逻辑和模糊集的灵活性，以及多分类器系统的稳健性 |
| [^54] | [Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning](https://arxiv.org/abs/2403.07362) | 该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。 |
| [^55] | [Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems](https://arxiv.org/abs/2403.07355) | 提出了一种基于矢量量化的深度学习方法，用于大规模MIMO系统中的CSI反馈，通过引入特定的转换函数和可训练的码本设计策略，降低了计算复杂度并提高了CSI重建性能。 |
| [^56] | [KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models](https://arxiv.org/abs/2403.07350) | KEBench提出了一个新的基准测试，采用不同的数据收集方法和新增加的度量标准（可移植性），以全面评估大型视觉-语言模型知识编辑的质量。 |
| [^57] | [Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning](https://arxiv.org/abs/2403.07342) | 提出一种新颖的标记方案，并采用对比学习方法来重新思考ASTE，该方法在性能上优于最先进技术，同时具有更紧凑的设计和降低的计算开销，尤其在少样本学习情景下展现出优越效果。 |
| [^58] | [Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention](https://arxiv.org/abs/2403.07332) | 该论文提出了一个基于大窗口的Mamba UNet用于医学图像分割，相比传统方法，在局部空间建模方面有优势，同时在全局建模方面保持高效率。 |
| [^59] | [A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models](https://arxiv.org/abs/2403.07322) | 通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战 |
| [^60] | [Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer](https://arxiv.org/abs/2403.07309) | 该研究提出了POSNEGDM框架，利用变压器模型和反馈强化器，在败血症治疗中取得显著改进，将患者生存率提高至97.39％，明显优于传统算法。 |
| [^61] | [Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees](https://arxiv.org/abs/2403.07308) | 提出了一种使用验证辅助学习神经网络屏障函数的终止保证方法。 |
| [^62] | [Graph Data Condensation via Self-expressive Graph Structure Reconstruction](https://arxiv.org/abs/2403.07294) | 通过自表达图结构重建的方法解决了图数据压缩中的问题 |
| [^63] | [Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure](https://arxiv.org/abs/2403.07292) | 该论文研究了如何在统一网络结构上进行持续全方位恶劣天气去除任务，以适应现实世界中不断变化的恶劣天气条件，而非传统静态学习范式。 |
| [^64] | [A Bayesian Approach to OOD Robustness in Image Classification](https://arxiv.org/abs/2403.07277) | 本文提出了一种基于贝叶斯方法的图像分类中OOD鲁棒性解决方案，利用扩展的组合神经网络和von Mises-Fisher核来处理真实世界的OOD问题。 |
| [^65] | [Anderson acceleration for iteratively reweighted $\ell_1$ algorithm](https://arxiv.org/abs/2403.07271) | 提出了一种Anderson加速的IRL1算法，将其收敛结果扩展到非光滑场景，不依赖于Kurdyka-Lojasiewicz条件 |
| [^66] | [Advantage-Aware Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2403.07262) | 介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。 |
| [^67] | [Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation](https://arxiv.org/abs/2403.07261) | 通过对抗性数据增强方法，该研究解决了从有限数量策略中学习任务表示的问题，从而将策略与离线任务表示学习分离。 |
| [^68] | [Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication](https://arxiv.org/abs/2403.07255) | 论文提出了一种基于深度学习辅助的并行干扰消除方法，用于在无授权NOMA系统中联合处理活动检测、信道估计和数据检测问题。 |
| [^69] | [Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences](https://arxiv.org/abs/2403.07230) | 提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。 |
| [^70] | [A multi-cohort study on prediction of acute brain dysfunction states using selective state space models](https://arxiv.org/abs/2403.07201) | 该研究利用电子健康记录数据开发了用于ICU病人急性脑功能障碍预测的自动化方法，动态预测谵妄、昏迷和死亡，填补了现有文献中的研究空白。 |
| [^71] | [Improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources](https://arxiv.org/abs/2403.07194) | 使用属性选择和集成方法，结合不同多模态数据源，可以改进智能辅导系统中对学生表现的预测能力。 |
| [^72] | [CuentosIE: can a chatbot about "tales with a message" help to teach emotional intelligence?](https://arxiv.org/abs/2403.07193) | CuentosIE提供了一套高度专业化的寓言故事，并提供了一系列工具，旨在教育用户情感知识，并监测他们的情感发展。 |
| [^73] | [$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model](https://arxiv.org/abs/2403.07191) | 提出了一种$(N,K)$-Puzzle测试平台，用于评估和比较生成语言模型中的强化学习算法。 |
| [^74] | [Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews](https://arxiv.org/abs/2403.07183) | 该研究提出了一种估计大语料库中被大语言模型大幅修改的文本比例的方法，并在AI会议的同行评审中进行了实证分析，发现6.5%至16.9%的文本可能被LLMs大幅修改，揭示了用户行为的一些见解。 |
| [^75] | [Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing](https://arxiv.org/abs/2403.07175) | 本文重建了ROME，提供了更稳定的r-ROME实现，解决了顺序模型编辑过程中的模型崩溃问题。 |
| [^76] | [Don't Forget What I did?: Assessing Client Contributions in Federated Learning](https://arxiv.org/abs/2403.07151) | 提出了一个历史感知的博弈理论框架FLContrib，用来评估联邦学习中的客户贡献。 |
| [^77] | [On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency](https://arxiv.org/abs/2403.07136) | 研究发现基于值函数的表征能力有限，导致在某些情况下基于值函数的方法在统计上低效率，这揭示了价值函数和统计效率之间的关联。 |
| [^78] | [Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation](https://arxiv.org/abs/2403.07131) | 本文提出了一种带有学习激励函数的大图匹配方法，用于多机器人任务分配，通过开发图强化学习框架，学习二部图匹配方法MRTA中的启发式或激励。 |
| [^79] | [Time Series Analysis of Key Societal Events as Reflected in Complex Social Media Data Streams](https://arxiv.org/abs/2403.07090) | 通过时间序列分析探索GAB和Telegram这两个社交媒体平台上的叙事演变，提出了一种新颖的方法来研究多个社交媒体领域，以提取可能被掩盖的关键信息。 |
| [^80] | [LSTM-Based Text Generation: A Study on Historical Datasets](https://arxiv.org/abs/2403.07087) | 本研究探讨了基于LSTM的文本生成在历史数据集上的应用，展示了这些模型在生成语言丰富、语境相关的文本方面的高准确性和高效率。 |
| [^81] | [Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning](https://arxiv.org/abs/2403.07078) | 本文调查了如何借助先验知识和认知模型来改善深度学习，以提升对抗防御、可解释性人工智能（XAI）和零样本学习，弥补现有深度学习模型在领域知识利用、对抗性攻击防御、解释性以及在开放环境推理中的性能限制。 |
| [^82] | [All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)](https://arxiv.org/abs/2403.07040) | 本文介绍了一种新颖的多任务提示方法，用于解决预训练图模型与不同任务之间的差距，启发自NLP中提示学习的成功。 |
| [^83] | [From English to ASIC: Hardware Implementation with Large Language Model](https://arxiv.org/abs/2403.07039) | 大型语言模型的快速发展改变了ASIC工程领域，但现代语言模型在生成硬件描述代码方面性能不佳，研究重点在于通过微调自然语言模型和重组HDL代码数据集来提高精度和准确性。 |
| [^84] | [Interpreting What Typical Fault Signals Look Like via Prototype-matching](https://arxiv.org/abs/2403.07033) | 提出了原型匹配网络（PMN），结合人类固有原型匹配和自编码器（AE），通过匹配特征与原型并选择最相似的原型来解释典型故障信号的特征。 |
| [^85] | [STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning for Real-world Scene Flow](https://arxiv.org/abs/2403.07032) | 提出了一种全局注意力流嵌入和空间时间特征重新嵌入模块相结合的方法，用于解决现实世界场景流预测中的局部依赖匹配和非刚性物体变形的挑战。 |
| [^86] | [An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem](https://arxiv.org/abs/2403.07028) | 该论文提出了一种基于神经网络的解决器，通过引入方向感知技术和监督强化学习，显著缩小了与先进元启发式算法的差距，同时表现出优越的效率。 |
| [^87] | [A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units](https://arxiv.org/abs/2403.07022) | 本文提出了一种One4All-ST框架，可以仅使用一个模型为任意可修改区域单位进行ST预测，有效解决了多尺度预测的成本问题和预测不一致性。 |
| [^88] | [Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence](https://arxiv.org/abs/2403.07017) | 数学在博弈论和人工智能交叉领域中的多智能体学习系统关注进化博弈论和人工智能之间的联系，探讨了个体策略在人群中演化的情况，并研究了智能体在多智能体环境中根据反馈和经验调整策略的过程。 |
| [^89] | [On Globular T-Spherical Fuzzy (G-TSF) Sets with Application to G-TSF Multi-Criteria Group Decision-Making](https://arxiv.org/abs/2403.07010) | 本文介绍了球形T-球形模糊（G-TSF）集合作为T-球形模糊集合和圆形球形模糊集合的创新扩展概念，利用球形/球体边界表示隶属度、不确定度和非隶属度程度，提供更准确的信息描绘，并通过结构化数据点评估决策对象来增强决策过程。 |
| [^90] | [AutoEval Done Right: Using Synthetic Data for Model Evaluation](https://arxiv.org/abs/2403.07008) | 提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。 |
| [^91] | [Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines](https://arxiv.org/abs/2403.07005) | 提出了具有层次结构奖励机器的多智能体强化学习（MAHRM），可以处理更复杂的情况，将任务分解为简单子任务的层次结构，以减少计算复杂性。 |
| [^92] | [Convergence of Some Convex Message Passing Algorithms to a Fixed Point](https://arxiv.org/abs/2403.07004) | 这项研究证明了一些凸消息传递算法会收敛到固定点，并在一定迭代次数内达到特定精度。 |
| [^93] | [Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System](https://arxiv.org/abs/2403.07003) | 提出了一种面向智能城市全覆盖智能应急互动响应系统的疏散管理框架，结合人工智能和机器学习技术，旨在改进现有的应急响应系统，以提高居民的公共服务和生活质量。 |
| [^94] | [Survival modeling using deep learning, machine learning and statistical methods: A comparative analysis for predicting mortality after hospital admission](https://arxiv.org/abs/2403.06999) | 该研究进行了使用多种生存分析方法的比较研究，包括传统统计模型和先进的机器学习算法，旨在预测入院后的死亡率。 |
| [^95] | [On the stochastics of human and artificial creativity](https://arxiv.org/abs/2403.06996) | 论文通过统计表征人类创造力，结合多个领域的先前见解，突出人类创造过程中的随机性质，并利用这一表征评估当代人工智能系统的创造力水平。 |
| [^96] | [Exact algorithms and heuristics for capacitated covering salesman problems](https://arxiv.org/abs/2403.06995) | 本文介绍了容量覆盖推销员问题（CCSP），提出了基于ILP和BRKGA的优化方法。 |
| [^97] | [Physics Sensor Based Deep Learning Fall Detection System](https://arxiv.org/abs/2403.06994) | 本文提出了一个基于物理传感器的深度学习跌倒检测系统TSFallDetect，利用顺序深度学习方法解决跌倒动作预测问题。 |
| [^98] | [Automatic driving lane change safety prediction model based on LSTM](https://arxiv.org/abs/2403.06993) | 通过使用基于深度学习方法的安全敏感深度学习模型，提出了一种基于LSTM的自动驾驶车道变更安全预测模型，旨在提高自动驾驶车辆驾驶安全性。 |
| [^99] | [MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning](https://arxiv.org/abs/2403.06914) | 提出了Meta dEmonstratioN Distillation (MEND)，利用知识蒸馏提高MEND和LLM之间的对齐，实现了高效和有效的上下文学习。 |
| [^100] | [CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin](https://arxiv.org/abs/2403.06670) | CEAT提出了一种用于非示范类增量学习的新架构，通过持续扩展和吸收参数的方式解决了可塑性-稳定性困境和分类器偏差问题 |
| [^101] | [DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2403.06397) | DeepSafeMPC是一种基于深度学习的模型预测控制方法，旨在有效预测多智体环境的复杂动态，并应用MARL原则寻找最优解。 |
| [^102] | [Decoupled Data Consistency with Diffusion Purification for Image Restoration](https://arxiv.org/abs/2403.06054) | 通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。 |
| [^103] | [CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming](https://arxiv.org/abs/2403.06025) | 这项研究介绍了一种利用计算机视觉从地下储存空间几何图像中预测陆地表面位移的新方法，为碳捕集和封存项目中的决策提供支持。 |
| [^104] | [SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data](https://arxiv.org/abs/2403.05918) | 基于残差网络的扩散建模方法能够有效处理不平衡数据，克服了经典过采样方法和基于生成网络的模式塌陷与训练不稳定问题。 |
| [^105] | [GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527) | GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。 |
| [^106] | [ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues](https://arxiv.org/abs/2403.05326) | 本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。 |
| [^107] | [Tell me the truth: A system to measure the trustworthiness of Large Language Models](https://arxiv.org/abs/2403.04964) | 本文提出了一种基于预定义领域知识图的系统化方法来衡量大型语言模型的可信度。 |
| [^108] | [Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology](https://arxiv.org/abs/2403.04558) | 本研究探讨了在计算病理学中减少对比自监督学习复杂性对分类性能的改善，通过利用消费级硬件。 |
| [^109] | ["In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning](https://arxiv.org/abs/2403.03102) | 提出了一种In-Dialogue Learning框架，通过对话历史刻画个人设来完成个性化对话生成任务，无需预定义个人资料，并在实验证明其显著改进对话生成性能。 |
| [^110] | [Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations](https://arxiv.org/abs/2403.02760) | 大型语言模型的兴起对电子商务推荐系统带来了新的发展机遇，突破了基于深度神经网络的推荐方法的局限性。 |
| [^111] | [Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects](https://arxiv.org/abs/2403.02624) | 该论文提出了帕累托最优估计和策略学习的方法，用于确定如何在短期和长期治疗效果之间进行权衡从而实现最佳治疗。 |
| [^112] | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://arxiv.org/abs/2403.01548) | 本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。 |
| [^113] | [Data Interpreter: An LLM Agent For Data Science](https://arxiv.org/abs/2402.18679) | 本研究引入了数据解释器，采用动态规划、工具集成和逻辑错误识别等关键技术，旨在增强数据科学中的问题解决能力。 |
| [^114] | [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603) | 符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。 |
| [^115] | [RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences](https://arxiv.org/abs/2402.17257) | RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。 |
| [^116] | [Information-based Transductive Active Learning](https://arxiv.org/abs/2402.15898) | ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。 |
| [^117] | [Active Few-Shot Fine-Tuning](https://arxiv.org/abs/2402.15441) | 该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。 |
| [^118] | [CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples](https://arxiv.org/abs/2402.13254) | 本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。 |
| [^119] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^120] | [3D Diffuser Actor: Policy Diffusion with 3D Scene Representations](https://arxiv.org/abs/2402.10885) | 通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。 |
| [^121] | [Clifford Group Equivariant Simplicial Message Passing Networks](https://arxiv.org/abs/2402.10011) | 本论文介绍了一种Clifford群等变单体消息传递网络，通过将Clifford群等变层与单体消息传递相结合，实现了在拓扑上更为复杂的E（n）-等变消息传递。实验结果表明，该方法具有良好的效果。 |
| [^122] | [Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model](https://arxiv.org/abs/2402.09786) | 这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。 |
| [^123] | [Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning](https://arxiv.org/abs/2402.09442) | 本文介绍了基于自驱动传感器和深度学习的人工智能应用的最新进展，重点讨论了使用TENG作为自驱动传感器的优势，包括简单结构和高瞬时性能。 |
| [^124] | [WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing](https://arxiv.org/abs/2402.09430) | WiMANS是第一个基于WiFi的多用户活动感知数据集，包含WiFi信道状态信息和同步视频，旨在促进可重复和可比较的研究 |
| [^125] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^126] | [IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images](https://arxiv.org/abs/2402.03227) | IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。 |
| [^127] | [One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications](https://arxiv.org/abs/2312.16145) | 基于一维结构，该研究提出了一种新的擦除框架，旨在解决现有概念消除方法存在的问题，实现非侵入性、精确性、可定制性和可转移性。 |
| [^128] | [Automated Approaches to Detect Self-Admitted Technical Debt: A Systematic Literature Review](https://arxiv.org/abs/2312.15020) | 论文提出了一种特征提取技术和ML/DL算法分类法，旨在比较和基准测试其在技术债务检测中的表现。 |
| [^129] | [On Alternating-Time Temporal Logic, Hyperproperties, and Strategy Sharing](https://arxiv.org/abs/2312.12403) | HyperATLS$^*_S扩展了ATL$^*$，允许比较多个战略互动结果与超性质，并要求一些Agent共享相同策略，捕捉重要AI相关特性。 |
| [^130] | [ACPO: AI-Enabled Compiler-Driven Program Optimization](https://arxiv.org/abs/2312.09982) | 该论文提出了ACPO框架，通过机器学习模型提供给LLVM简单全面的工具，以实现编译器驱动的程序优化。 |
| [^131] | [Lite-Mind: Towards Efficient and Robust Brain Representation Network](https://arxiv.org/abs/2312.03781) | Lite-Mind旨在解决fMRI解码中的挑战，通过提出一种高效稳健的脑表示网络，避免了在实践设备上为每个受试者部署特定模型的问题。 |
| [^132] | [WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images](https://arxiv.org/abs/2311.16480) | 研究提出了一种基于多实例生成模型的方法，能够生成千亿像素全切片图像的病理报告，实验结果表明该模型能够产生包含多个临床线索的病理报告。 |
| [^133] | [FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients](https://arxiv.org/abs/2311.11227) | 提出了一种名为FedRA的联邦调优算法，可以随机生成分配矩阵来应对拥有不同计算和通信资源的异构客户端，在不需要修改原模型的情况下进行微调。 |
| [^134] | [3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition](https://arxiv.org/abs/2310.18511) | 3DCoMPaT$^{++}$提出了一个大规模的多模态2D/3D数据集，包含1.6亿个渲染视图的风格化三维形状，带有详细的部件实例级别标注，用于组合识别。 |
| [^135] | [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2310.02129) | 这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。 |
| [^136] | [Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge](https://arxiv.org/abs/2309.02001) | 使用直方图匹配来转换额外数据在处理域偏移时比简单归一化取得更好的结果 |
| [^137] | [Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770) | 该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。 |
| [^138] | [Rethinking Mobile AI Ecosystem in the LLM Era](https://arxiv.org/abs/2308.14363) | 重新思考移动AI生态系统，引入了一种基于协作管理的范式，通过在NPU内部放置不受应用或操作系统修订影响的基础模型，以及每个应用贡献特定的适配器，为广泛移动AI任务提供服务。 |
| [^139] | [Defending Against Malicious Behaviors in Federated Learning with Blockchain](https://arxiv.org/abs/2307.00543) | 该研究提出了一个基于区块链和分布式分类账技术的安全和可靠的联邦学习系统，包括点对点投票机制和奖励和惩罚机制，以检测和阻止恶意行为，证明了该框架对抗恶意客户的有效性。 |
| [^140] | [MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543) | 本文提出了一种将大型语言模型的知识蒸馏到更小模型的方法，通过使用反向KLD替换标准KD方法中的前向KLD目标，有效避免了学生模型高估教师分布的低概率区域。 |
| [^141] | [The Principles of Data-Centric AI (DCAI)](https://arxiv.org/abs/2211.14611) | 数据中心人工智能（DCAI）强调数据的质量和动态性，提出六项指导原则，并为人工智能系统的未来发展方向指明了道路。 |
| [^142] | [ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning](https://arxiv.org/abs/2211.04118) | 提出的Consprompt结合了提示编码网络、对比采样模块和对比评分模块，实现了差异对比学习，在不同少样本设置下表现出最先进的性能，证实了利用多级对比学习在基于提示的微调过程中的有效性。 |
| [^143] | [WaveNets: Wavelet Channel Attention Networks](https://arxiv.org/abs/2211.02695) | WaveNets利用小波变换压缩作为通道表示的解决方案，提出了一种通道注意力机制，名为WaveNet。 |
| [^144] | [Efficient Diffusion Models for Vision: A Survey](https://arxiv.org/abs/2210.09292) | 扩散模型在内容生成中表现出色，无需对抗性训练，但由于其高计算复杂性和大量计算开销，可能限制了其在现实应用中的推广。 |
| [^145] | [SATformer: Transformer-Based UNSAT Core Learning](https://arxiv.org/abs/2209.00953) | SATformer通过引入基于Transformer的方法，采用对不可满足性进行建模的方式，以识别不可满足的子问题，得到了优于NeuroSAT的性能。 |
| [^146] | [Transformations in Learned Image Compression from a Modulation Perspective](https://arxiv.org/abs/2203.02158) | 本文提出了一种新的学习图像压缩变换方法，将LIC解释为通信系统，通过引入信号调制的方法，实现了各种变换方法的数学简化和扩展，并在实验中验证了其有效性和稳健性。 |
| [^147] | [Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited](https://arxiv.org/abs/2202.03580) | 重新审视了使用切比雪夫多项式逼近谱图卷积的问题，发现ChebNet性能较差主要是由于其学习到的非法系数近似解析滤波器函数 |
| [^148] | [Evaluating a Methodology for Increasing AI Transparency: A Case Study](https://arxiv.org/abs/2201.13224) | 该论文评估了一个针对人工智能透明度增加的方法论，首次将该用户中心方法论应用于实践，报告了在医疗人工智能领域团队的经验。 |
| [^149] | [A Two-Stage Feature Selection Approach for Robust Evaluation of Treatment Effects in High-Dimensional Observational Data](https://arxiv.org/abs/2111.13800) | 提出了一种名为Outcome Adaptive Elastic Net（OAENet）的两阶段特征选择技术，用于在高维观察数据中进行稳健的因果推断决策。 |
| [^150] | [Deep DNA Storage: Scalable and Robust DNA Storage via Coding Theory and Deep Learning](https://arxiv.org/abs/2109.00031) | 通过将基于深度神经网络训练的编码理论和深度学习应用于DNA存储，我们提出了一种可扩展且稳健的DNA存储解决方案，将速度提高了最高3200倍，准确性提高了40%。 |
| [^151] | [CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural Network for Semantic Segmentation](https://arxiv.org/abs/2108.00408) | 通过将常用的卷积操作重新构造为多层卷积稀疏编码块，提出了一种可能用于显著提高语义分割模型性能的新策略。 |
| [^152] | [Understanding and Avoiding AI Failures: A Practical Guide](https://arxiv.org/abs/2104.12582) | 基于正常事故理论、高可靠性理论和开放系统理论，结合人工智能安全原则，建立了一个框架来理解当前人工智能应用的风险，强调了关注当前一代人工智能系统安全性的重要性。 |
| [^153] | [In-context Learning with Retrieved Demonstrations for Language Models: A Survey.](http://arxiv.org/abs/2401.11624) | 本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。 |
| [^154] | [CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents.](http://arxiv.org/abs/2401.10568) | CivRealm是一个受文明游戏启发的环境，要求代理人在复杂的情境中进行学习和推理，以应对变化的游戏规则和随机环境。 |
| [^155] | [A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles.](http://arxiv.org/abs/2311.02099) | 本研究介绍了一种安全的个性化偏好学习方法，应用于自动驾驶汽车。该方法利用信号时态逻辑公式的优先顺序进行学习，并提出了一种解决这个学习问题的方法。通过对比较，我们找到了适合的权重估计，使得首选信号的加权满足度高于非首选信号。在人体试验中证明了该方法的有效性。 |
| [^156] | [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.](http://arxiv.org/abs/2310.06117) | 本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。 |
| [^157] | [Augmenting transformers with recursively composed multi-grained representations.](http://arxiv.org/abs/2309.16319) | ReCAT是一种增强的Transformer模型，使用递归组合和上下文内外层能够模拟文本的层级句法结构，并生成与其他跨度上下文相关的多粒度表示。 |
| [^158] | [MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback.](http://arxiv.org/abs/2309.10691) | MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。 |
| [^159] | [Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation.](http://arxiv.org/abs/2308.13212) | 提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。 |
| [^160] | [A Survey on Large Language Model based Autonomous Agents.](http://arxiv.org/abs/2308.11432) | 该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。 |
| [^161] | [Deep Learning for Diverse Data Types Steganalysis: A Review.](http://arxiv.org/abs/2308.04522) | 本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。 |
| [^162] | [Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models.](http://arxiv.org/abs/2308.02409) | 该论文通过融合多个空间维度的方法，使用脑电信号对心理负荷进行分类和估计连续级别。在时间域中使用了时态卷积网络，而在频率域中引入了新的架构——多维残差块。 |
| [^163] | [Fully $1\times1$ Convolutional Network for Lightweight Image Super-Resolution.](http://arxiv.org/abs/2307.16140) | 本论文提出了一种全卷积轻量级图像超分辨率网络，通过融合$3\times3$和$1\times1$卷积核的优点，以及引入无参数的空间平移操作，在保持计算效率的同时提高了网络的表示能力。 |
| [^164] | [Overthinking the Truth: Understanding how Language Models Process False Demonstrations.](http://arxiv.org/abs/2307.09476) | 该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。 |
| [^165] | [TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences.](http://arxiv.org/abs/2306.14114) | 该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题 |
| [^166] | [Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information.](http://arxiv.org/abs/2306.08762) | 本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。 |
| [^167] | [SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation.](http://arxiv.org/abs/2306.03403) | 本论文提出了SGAT4PASS，一种面向球面几何意识的全景语义分割Transformer，通过加入球面几何感知的约束，能更好地捕捉全景图像的3D属性，从而提高分割性能。 |
| [^168] | [Sequence Modeling is a Robust Contender for Offline Reinforcement Learning.](http://arxiv.org/abs/2305.14550) | 序列建模是离线强化学习中比Q-Learning和Imitation Learning更适合在稀疏奖励和低质量数据设置下的选择，在任务范围增加时，序列建模和模仿学习更可取。 |
| [^169] | [Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference.](http://arxiv.org/abs/2302.09582) | 本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。 |

# 详细

[^1]: TeleMoMa：一种用于移动操作的模块化多功能远程操作系统

    TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation

    [https://arxiv.org/abs/2403.07869](https://arxiv.org/abs/2403.07869)

    TeleMoMa 是一种面向移动操作的模块化多功能远程操作系统，通过整合多种人机接口、降低门槛且具有通用性，为移动操作器提供了全身远程操作的解决方案。

    

    机器人学中限制模仿学习的关键瓶颈是数据的匮乏。这个问题在移动操作中更为严重，因为与静止操作相比，由于缺乏可用且易于使用的远程操作界面，收集演示更加困难。在这项工作中，我们展示了TeleMoMa，这是一种用于全身远程操作移动操作器的通用和模块化界面。TeleMoMa将包括RGB和深度摄像头、虚拟现实控制器、键盘、操纵杆等多个人机接口整合在一起，以及这些接口的任何组合。在其更易访问的版本中， TeleMoMa可以仅使用视觉（如RGB-D相机）即可工作，降低了人类提供移动操作演示的门槛。我们通过在模拟环境和现实世界中远程操作几个现有的移动操作器——PAL Tiago++, Toyota HSR和Fetch来展现TeleMoMa的多功能性。

    arXiv:2403.07869v1 Announce Type: cross  Abstract: A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonst
    
[^2]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^3]: MoPE-CLIP: 结构化剪枝用于高效的视觉-语言模型，带有基于模块的剪枝误差度量

    MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric

    [https://arxiv.org/abs/2403.07839](https://arxiv.org/abs/2403.07839)

    提出了Module-wise Pruning Error（MoPE）指标，用于评估CLIP模块重要性，在预训练和任务特定微调压缩阶段提出了统一的剪枝框架。

    

    视觉-语言预训练模型在各种下游任务中取得了令人印象深刻的性能，然而，它们庞大的模型大小阻碍了在计算资源有限的平台上的利用。我们发现，直接使用更小的预训练模型并对CLIP模型应用基于大小的剪枝会导致僵化和性能较差。最近关于VLP压缩的努力要么采用单模态压缩指标导致性能有限，要么涉及昂贵的掩码搜索过程和可学习掩码。在本文中，我们首次提出了基于模块的剪枝误差（MoPE）指标，通过跨模态任务性能下降准确评估CLIP模块重要性。利用MoPE指标，我们引入了一个统一的剪枝框架，适用于预训练和任务特定微调压缩阶段。对于预训练，MoPE-CLIP有效利用了来自教师模型的知识，显著提高了

    arXiv:2403.07839v1 Announce Type: cross  Abstract: Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly 
    
[^4]: 标签丢失率：利用具有域转移和部分标记的多个数据集改进深度学习超声心动图分割

    Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling

    [https://arxiv.org/abs/2403.07818](https://arxiv.org/abs/2403.07818)

    本文研究利用多个数据集进行深度学习超声心动图分割，在处理部分标记数据时采用改进的交叉熵损失函数。

    

    超声心动图（超声）是评估心脏功能时使用的第一种成像方式。从超声中测量功能生物标志物依赖于对心脏结构进行分割，深度学习模型被提出来自动化这一过程。然而，为了将这些工具转化为广泛的临床应用，重要的是分割模型对各种图像具有鲁棒性（例如，由不同扫描仪获得，由不同级别的专家操作员获得等）。为了实现这种鲁棒性水平，有必要使用多个不同的数据集来训练模型。在使用多个不同的数据集进行训练时面临的一个重要挑战是标签存在的变化，即合并数据通常是部分标记的。已经提出了交叉熵损失函数的改进来处理部分标记数据。在本文中，我们展示了训练的naively

    arXiv:2403.07818v1 Announce Type: cross  Abstract: Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively 
    
[^5]: Branch-Train-MiX: 将专家LLMs混合到混合专家LLM中

    Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM

    [https://arxiv.org/abs/2403.07816](https://arxiv.org/abs/2403.07816)

    BTX方法提供了一种高效的训练大型语言模型以具备多个专业领域能力的方法，通过将种子模型的分支培训成专家，然后在Mixture-of-Expert层中将它们汇集为专家，并利用MoE微调阶段学习基于标记的路由，从而实现了最佳的精度和效率权衡。

    

    我们研究了训练大型语言模型（LLMs）以具备多个专业领域能力的高效方法，例如编码、数学推理和世界知识。我们的方法名为Branch-Train-MiX（BTX），从一个种子模型开始，将其分支训练成专家，以高吞吐量和降低通信成本的尴尬并行方式。在各个专家异步训练后，BTX将它们作为专家在Mixture-of-Expert（MoE）层中汇集其前馈参数，并平均其他参数，随后是一个MoE微调阶段来学习基于标记的路由。BTX概括了两个特殊情况，即Branch-Train-Merge方法，它没有MoE微调阶段来学习路由，以及稀疏升级，它省略了异步训练专家的阶段。与其他方法相比，BTX实现了最佳精度和效率的权衡。

    arXiv:2403.07816v1 Announce Type: cross  Abstract: We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.
    
[^6]: Chronos: 学习时间序列的语言

    Chronos: Learning the Language of Time Series

    [https://arxiv.org/abs/2403.07815](https://arxiv.org/abs/2403.07815)

    Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。

    

    我们介绍了Chronos，一个简单但有效的预训练概率时间序列模型框架。Chronos使用缩放和量化将时间序列值标记化为固定词汇，并通过交叉熵损失在这些标记化的时间序列上训练现有的基于Transformer的语言模型架构。我们在大量公开可用数据集上基于T5系列（参数范围从20M到710M）对Chronos模型进行了预训练，同时通过高斯过程生成了一个合成数据集以提高泛化能力。在包含42个数据集的全面基准测试中，涵盖了传统的本地模型和深度学习方法，我们展示了Chronos模型：（a）在训练语料库中的数据集上明显优于其他方法；（b）相对于专门训练的方法，在新数据集上的零样本性能可比甚至优于其他方法。

    arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
    
[^7]: 超越死记硬背：语言模型中的随机内存访问挑战

    Beyond Memorization: The Challenge of Random Memory Access in Language Models

    [https://arxiv.org/abs/2403.07805](https://arxiv.org/abs/2403.07805)

    本文研究了语言模型在访问内存时的挑战，发现通过背诵和置换等技术可以改善语言模型的随机内存访问能力，从而在开放域问题回答任务中取得显著改进。

    

    最近语言模型(LMs)的发展展示了它们在NLP任务中的有效性，尤其是在知识密集型任务中。然而，在其参数内部的知识存储和内存访问机制仍然令人费解。本文探讨了生成式语言模型（如GPT-2）是否能够顺序或随机地访问其内存。通过精心设计的合成任务，涵盖全面背诵、选择性背诵和基于问题回答的情景，我们揭示了LMs能够顺序访问其内存，同时在随机访问已记忆内容时遇到挑战。我们发现，通过背诵和置换等技术可以提高LMs的随机内存访问能力。此外，通过将这种干预应用于开放域问题回答的现实场景，我们验证了通过背诵来增强随机访问技术对问题回答能力的显著改进。

    arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
    
[^8]: 联合选择：适应性地将公共信息纳入私密合成数据

    Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data

    [https://arxiv.org/abs/2403.07797](https://arxiv.org/abs/2403.07797)

    jam-pgm机制在合成数据生成中能够联合选择公共数据和私密数据，并且能够在公共数据分布存在偏差的情况下优于其他机制。

    

    基于边际和图模型的不同ially私密合成数据生成机制已在各种设置中取得成功。然而，这些方法的一个局限性是它们无法整合公共数据。通过在公共数据上进行预训练来初始化数据生成模型已被证明可以提高合成数据的质量，但当模型结构未事先确定时，该技术无法应用。我们开发了机制jam-pgm，将自适应测量框架扩展到联合选择测量公共数据和私密数据之间。这一技术允许将公共数据纳入基于图模型的机制中。我们展示了，即使在公共数据分布存在偏差的情况下，jam-pgm能够胜过公共辅助和非公共辅助的合成数据生成机制。

    arXiv:2403.07797v1 Announce Type: cross  Abstract: Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased.
    
[^9]: DexCap：用于灵巧操作的可扩展和可移植动作捕捉数据收集系统

    DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation

    [https://arxiv.org/abs/2403.07788](https://arxiv.org/abs/2403.07788)

    DexCap是一个可移植的手部动作捕捉系统，结合DexIL算法从人类手部运动数据中训练机器人技能，具有精确追踪和复制人类动作的能力。

    

    从人类手部运动数据中学习是为机器人赋予类人灵巧在现实操纵任务中的潜在途径，然而，现存手部动作捕捉系统的可移植性以及将动作捕捉数据转化为有效控制策略的困难仍然存在挑战。为了应对这些问题，我们引入了DexCap，一个便携式手部动作捕捉系统，以及DexIL，一种新颖的模仿算法，可直接从人类手部动作捕捉数据训练灵巧机器人技能。DexCap基于SLAM和电磁场以及环境的3D观察，提供了对手腕和手指运动的精确、抗遮挡的跟踪。利用这一丰富的数据集，DexIL采用逆运动学和基于点云的模仿学习来复制人类动作与机器人手。除了从人类运动中学习外，DexCap还提供了一种op

    arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
    
[^10]: 将竞争转化为合作：多Agent系统和语言模型在现代组织中的革命性作用

    Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations

    [https://arxiv.org/abs/2403.07769](https://arxiv.org/abs/2403.07769)

    文章探讨了基于多Agent系统理论结合大型语言模型的计算实体对人类互动的革新影响，提出了一种可能将专门人工代理支持扩展到操作性组织流程和基于知识和人类编排的战略决策的方式。

    

    这篇文章探讨了基于多Agent系统理论（SMA）结合大型语言模型（LLM）的计算实体的动态影响，其特点是能够模拟复杂的人类互动，作为一种革新人类用户交互的可能性，从利用专门的人工代理支持从操作组织流程到基于应用知识和人的编排的战略决策。 先前的调查显示，在处理新挑战和实用任务（如引发逻辑推理和问题解决）时，特别是在人工代理的自主方法方面存在限制。 还考虑到，传统技术，如激发思想链，需要明确的人类指导。 在我们的方法中，我们使用从大型语言模型（LLM）开发的代理，每个代理都有不同

    arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
    
[^11]: Synth$^2$: 用合成标题和图像嵌入提升视觉-语言模型

    Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings

    [https://arxiv.org/abs/2403.07750](https://arxiv.org/abs/2403.07750)

    这项研究提出了一种新方法，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。

    

    高质量的人工标记图像标题数据集的创建在视觉-语言模型（VLMs）的发展中构成了一个重大瓶颈。我们提出了一种新方法，利用大型语言模型（LLMs）和图像生成模型的优势，为高效有效的VLM训练创建合成图像-文本对。我们的方法利用预训练的文本到图像模型，从LLM生成的标题开始合成图像嵌入。然后，这些合成对用于训练VLM。大量实验证明，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。特别是，通过合成数据集的增强，我们超过基线17%。此外，我们展示，在图像嵌入空间合成比在像素空间中快25%。

    arXiv:2403.07750v1 Announce Type: cross  Abstract: The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel sp
    
[^12]: 阿瑞阿德涅和忒修斯：在未知图中探索和会合的两个移动代理

    Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph

    [https://arxiv.org/abs/2403.07748](https://arxiv.org/abs/2403.07748)

    论文研究了移动计算中的探索和会合两个基本问题，提出了分别能在图中$m$个同步时间步实现集体探索和$\frac{3}{2}m$时间步内实现会合的算法。

    

    我们研究移动计算中的两个基本问题：探索和会合，涉及到一个未知图中的两个不同移动代理。这两个代理可以在所有节点上的白板上读写信息。它们每一步都沿着一个相邻的边移动。在探索问题中，两个代理从图中相同的节点出发，必须遍历所有的边。我们展示了深度优先搜索的一个简单变体可以在$m$个同步时间步中实现集体探索，其中$m$是图的边数。这提高了集体图探索的竞争比率。在会合问题中，代理从图中不同的节点出发，必须尽快相遇。我们介绍了一个算法，保证在至多$\frac{3}{2}m$个时间步内会合。这比所谓的“等妈妈”算法需求的$2m$时间步更好。我们所有的保证都是

    arXiv:2403.07748v1 Announce Type: cross  Abstract: We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph. The agents can read and write information on whiteboards that are located at all nodes. They both move along one adjacent edge at every time-step. In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges. We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph. This improves the competitive ratio of collective graph exploration. In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible. We introduce an algorithm guaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps. All our guarantees are
    
[^13]: FineMath：一种用于评估中文大型语言模型的细粒度数学评估基准

    FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models

    [https://arxiv.org/abs/2403.07747](https://arxiv.org/abs/2403.07747)

    FineMath是一个用于评估中文大型语言模型数学推理能力的细粒度数学评估基准数据集，涵盖小学数学中的主要概念，划分为17类数学问题，并手动注释难度级别，实验证实在数学方面仍有改进空间。

    

    为了全面评估大型语言模型（LLMs）的数学推理能力，我们需要精心策划涵盖不同难度级别的各种数学概念和数学问题的评估数据集。为了实现这一目标，我们在本文中提出了FineMath，这是一个用于评估中文LLMs的细粒度数学评估基准数据集。FineMath旨在涵盖小学数学中教授的主要数学概念，进一步划分为17类数学问题，从而深入分析LLMs的数学推理能力。所有17类数学问题均根据解决这些问题所需的推理步骤数量进行手动注释其难度级别。我们在FineMath上对各种LLMs进行了广泛实验，并发现在数学方面仍有相当大的改进空间。

    arXiv:2403.07747v1 Announce Type: cross  Abstract: To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathem
    
[^14]: 概率易变量因果效应研究

    Probabilistic Easy Variational Causal Effect

    [https://arxiv.org/abs/2403.07745](https://arxiv.org/abs/2403.07745)

    论文提出了一种称为Probabilistic Easy Variational Causal Effect (PEACE)的函数，可以测量X对Y的直接因果效应，适用于连续和离散情况，通过管理概率密度值强度$d\ge 0$来实现干预。

    

    论文研究随机向量$X$和$Z$，以及$Y=g(X,Z)$的情况。一方面，对于连续的$X$和$Z$，通过使用总变差和$g$的通量的思想，我们发展了一个在因果推断中能够处理广泛因果问题领域的视角。我们关注一个称为Probabilistic Easy Variational Causal Effect (PEACE)的函数，它可以测量$X$对$Y$的直接因果效应，而同时改变$X$的值，但保持$Z$的值不变。PEACE是关于$d\ge 0$的一个函数，管理着概率密度值$f(x|z)$的强度。另一方面，我们将上述思想推广到离散情况，并展示其与连续情况的兼容性。此外，我们使用测度论概念研究了PEACE的一些性质。此外，我们提供了一些可辨识性标准。

    arXiv:2403.07745v1 Announce Type: cross  Abstract: Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems. Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case. Further, we investigate some properties of PEACE using measure theoretical concepts. Furthermore, we provide some identifiability criteria and s
    
[^15]: 为计算病理学系统配备工件处理流水线：计算与性能权衡的展示

    Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs

    [https://arxiv.org/abs/2403.07743](https://arxiv.org/abs/2403.07743)

    提出了一种专家混合方案，用于在计算病理学系统中检测和排除五种显著的工件，并应用概率阈值处理。

    

    组织病理学是癌症诊断的黄金标准，在显微镜下进行检查。然而，组织病理学处理过程会产生一些工件，最终会转移到玻璃载玻片的数字化版本，即全玻幻灯片。工件是诊断无关的区域，可能导致错误的深度学习算法预测。因此，在计算病理学（CPATH）系统中检测和排除工件对于可靠的自动诊断至关重要。在本文中，我们提出了一种专家混合（MoE）方案，用于检测包括损坏组织、模糊、褶皱组织、气泡和在WSIs中的组织学无关血液等五种显著工件。首先，我们训练独立的二元DL模型作为专家来捕捉特定的工件形态。然后，我们使用融合机制来集成它们的预测。我们对最终的概率进行概率阈值处理

    arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
    
[^16]: 使用深度集成进行6D物体姿态估计的不确定性量化

    Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation

    [https://arxiv.org/abs/2403.07741](https://arxiv.org/abs/2403.07741)

    本研究提出了一种使用深度集成来量化多阶段6D物体姿态估计方法不确定性的方法。

    

    6D物体姿态的估计是许多计算机视觉应用中的基本任务。特别是在人机交互、工业检验和自动化等高风险场景中，可靠的姿态估计至关重要。近年来，提出了越来越精确和鲁棒的基于深度学习的6D物体姿态估计方法。许多表现最佳的方法并非端到端可训练，而是由多个阶段组成。在深度不确定性量化的背景下，深度集成被认为是最先进的，因为它们已被证明能够产生良好校准和鲁棒的不确定性估计。然而，深度集成只能应用于可以端到端训练的方法。在这项工作中，我们提出了一种使用深度集成来量化多阶段6D物体姿态估计方法的不确定性的方法。对于实现，我们选择SurfEmb作为代表，因为它是一种

    arXiv:2403.07741v1 Announce Type: cross  Abstract: The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is on
    
[^17]: DSEG-LIME -- 通过层次化数据驱动分割提升图像解释能力

    DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven Segmentation

    [https://arxiv.org/abs/2403.07733](https://arxiv.org/abs/2403.07733)

    通过引入数据驱动分割和层次分割程序，DSEG-LIME改进了图像解释能力，提高了图像分类的可解释性。

    

    可解释的人工智能在揭示复杂机器学习模型的决策过程中至关重要。LIME (Local Interpretable Model-agnostic Explanations) 是一个广为人知的用于图像分析的XAI框架。它利用图像分割来创建特征以识别相关的分类区域。然而，较差的分割可能会影响解释的一致性并削弱各个区域的重要性，从而影响整体的可解释性。针对这些挑战，我们引入了DSEG-LIME (Data-Driven Segmentation LIME)，具有: i) 用于生成人类可识别特征的数据驱动分割, 和 ii) 通过组合实现的层次分割程序。我们在预训练模型上使用来自ImageNet数据集的图像对DSEG-LIME进行基准测试-这些情景不包含特定领域的知识。分析包括使用已建立的XAI指标进行定量评估，以及进一步的定性评估。

    arXiv:2403.07733v1 Announce Type: cross  Abstract: Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented
    
[^18]: 在数据限制的二分类中平衡公平性和准确性

    Balancing Fairness and Accuracy in Data-Restricted Binary Classification

    [https://arxiv.org/abs/2403.07724](https://arxiv.org/abs/2403.07724)

    研究提出了一个框架，直接分析最优贝叶斯分类器在数据限制的情况下的行为，以平衡准确性和公平性。

    

    处理敏感信息的应用可能对机器学习（ML）分类器可用数据设置限制。本文提出一个框架，模拟准确性和公平性之间的权衡，在四种实际情景下探讨可用于分析的数据类型。与先前研究通过分析经训练以隐式学习数据集的特征向量、类别标签和敏感属性的潜在分布的评分函数的输出来考虑这种权衡不同，我们的框架直接通过从数据集本身构建的离散近似来分析最优贝叶斯分类器在这个潜在分布上的行为。这种方法使我们能够制定多个凸优化问题，以更好地平衡准确性和公平性。

    arXiv:2403.07724v1 Announce Type: cross  Abstract: Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex 
    
[^19]: 通过视觉词进行多模态自回归建模

    Multi-modal Auto-regressive Modeling via Visual Words

    [https://arxiv.org/abs/2403.07720](https://arxiv.org/abs/2403.07720)

    本研究成功实现了多模态自回归建模，并通过引入视觉词的概念，将视觉特征映射到大语言模型的词汇表上，为视觉建模提供了监督信息。

    

    大语言模型（LLMs）受益于在大量未标记文本语料库上进行自回归建模的方法，展现出强大的感知和推理能力。然而，将自回归建模扩展到多模态场景以构建大型多模态模型（LMMs）时，存在一个巨大困难，即图像信息在LMM中以连续的视觉嵌入进行处理，无法获得离散的用于分类的监督标签。本文首次成功地进行了多模态自回归建模，并提出了统一的目标。具体来说，我们提出了视觉词的概念，将视觉特征映射到LLM词汇表上的概率分布，为视觉建模提供了监督信息。我们进一步探讨了LMM中语义空间内视觉特征的分布以及使用文本嵌入来表示的可能性。

    arXiv:2403.07720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities. However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification. In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to repre
    
[^20]: WorkArena：Web代理在解决常见知识工作任务中的能力如何？

    WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?

    [https://arxiv.org/abs/2403.07718](https://arxiv.org/abs/2403.07718)

    该研究探究了基于大型语言模型的代理在通过web浏览器与软件交互时的能力，提出了WorkArena和BrowserGym两个工具，在29个任务的基准测试中显示出潜力，但也揭示了实现完全任务自动化仍存在挑战。

    

    我们研究了基于大型语言模型的代理与软件通过web浏览器交互的应用。与先前的研究不同，我们关注衡量这些代理执行任务的能力，这些任务涵盖了利用企业软件系统的知识工作者的典型日常工作。为此，我们提出了WorkArena，一个基于广泛使用的ServiceNow平台的29个任务的远程主机基准。我们还介绍了BrowserGym，这是一个用于设计和评估这些代理的环境，提供了丰富的行为和多模态观察。我们的实证评估显示，尽管当前的代理在WorkArena上表现出了潜力，但要实现完全任务自动化仍存在相当大的差距。值得注意的是，我们的分析揭示了开源和闭源LLMs之间显著的性能差距，突出了未来探索和发展领域的一个重要领域。

    arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
    
[^21]: SSM遇上视频扩散模型: 结构化状态空间下的高效视频生成

    SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces

    [https://arxiv.org/abs/2403.07711](https://arxiv.org/abs/2403.07711)

    提出了一种基于状态空间模型（SSMs）的方法，用于解决使用扩散模型生成长视频序列时注意力层内存消耗增长快、限制较大的问题

    

    鉴于图像生成通过扩散模型取得的显著成就，研究界对将这些模型扩展到视频生成表现出越来越大的兴趣。最近用于视频生成的扩散模型主要利用注意力层来提取时间特征。然而，由于注意力层的内存消耗随着序列长度的增加呈二次增长，这种限制在尝试使用扩散模型生成更长视频序列时会带来重大挑战。为了克服这一挑战，我们提出利用状态空间模型（SSMs）。由于相对于序列长度，SSMs具有线性内存消耗，最近已经引起了越来越多的关注。在实验中，我们首先通过使用UCF101这一视频生成的标准基准来评估我们基于SSM的模型。此外，为探讨SSMs在更长视频生成中的潜力，

    arXiv:2403.07711v1 Announce Type: cross  Abstract: Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, 
    
[^22]: 使用对比奖励改善从人类反馈中的强化学习

    Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards

    [https://arxiv.org/abs/2403.07708](https://arxiv.org/abs/2403.07708)

    引入对比奖励的方法提高了从人类反馈中的强化学习的效果，改善了奖励模型的鲁棒性，鼓励对基准的改善，并能根据任务的难度进行校准。

    

    强化学习从人类反馈（RLHF）是用来对齐大型语言模型（LLMs）与人类偏好的主流范式。然而现有的RLHF在很大程度上依赖于准确和信息丰富的奖励模型，这些模型容易受到各种来源的噪声，例如人类标注错误，使得流程脆弱。本文通过引入对奖励的惩罚项，命名为“对比奖励”，来提高奖励模型的有效性。我们的方法包括两个步骤：（1）离线抽样步骤，获取用作基准计算的提示响应，以及（2）使用基准响应计算对比奖励，并将其用于Proximal Policy Optimization（PPO）步骤。我们展示了对比奖励使得LLM能够惩罚奖励不确定性，提高鲁棒性，鼓励优于基线的改进，根据任务难度进行校准，并且重新

    arXiv:2403.07708v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and re
    
[^23]: 对称 Q-learning：减少在线强化学习中贝尔曼误差的偏斜

    Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning

    [https://arxiv.org/abs/2403.07704](https://arxiv.org/abs/2403.07704)

    对称 Q-learning方法通过添加合成噪声来减少贝尔曼误差的偏斜，在在线强化学习中提高了样本效率。

    

    在深度强化学习中，估计值函数以评估状态和动作的质量是至关重要的。该值函数通常使用最小二乘法进行训练，隐含地假定一个高斯误差分布。然而，最近的研究表明，由于贝尔曼算子的特性，用于训练值函数的误差分布通常是倾斜的，违反了最小二乘法中对正态误差分布的隐含假设。为了解决这个问题，我们提出了一种称为对称 Q-learning 的方法，其中从零均值分布生成的合成噪声被添加到目标值中，以生成高斯误差分布。我们在MuJoCo的连续控制基准任务上评估了所提出的方法。通过减少错误分布的偏斜，它提高了一种最先进的强化学习方法的样本效率。

    arXiv:2403.07704v1 Announce Type: cross  Abstract: In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.
    
[^24]: 基于语言模型的新型数据增强框架用于去偏见观点摘要

    Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization

    [https://arxiv.org/abs/2403.07693](https://arxiv.org/abs/2403.07693)

    使用大型和小型语言模型的新型数据增强框架，通过重新生成评论来实现观点摘要的去偏见化，避免了大型语言模型数据增强可能存在的问题和高昂成本。

    

    现有观点摘要数据集中超过70％的评论是积极的，当前的观点摘要方法在给定负面文本的情况下不愿生成负面摘要，造成情感偏见。为了解决这种情感偏见，一个直接的方法是基于大型语言模型生成额外的数据，平衡数据集的情感分布，而不过分依赖特定框架。然而，基于大型语言模型的数据增强面临两个缺点：1）增强数据中的潜在问题或毒性；2）昂贵的成本。因此，在本文中，我们提出了一个基于大型和小型语言模型的新型数据增强框架，用于去偏见观点摘要。具体而言，通过大型语言模型重写正面文本获得了小规模合成的负面评论。然后，基于生成的内容训练一个解耦重构模型。

    arXiv:2403.07693v1 Announce Type: cross  Abstract: As more than 70$\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated 
    
[^25]: 具有赔率比的无参考单体偏好优化

    Reference-free Monolithic Preference Optimization with Odds Ratio

    [https://arxiv.org/abs/2403.07691](https://arxiv.org/abs/2403.07691)

    本文介绍了一种无参考单体赔率比偏好优化算法ORPO，在SFT过程中通过轻微惩罚不受欢迎的生成风格，消除了额外的偏好对齐阶段

    

    近期的语言模型偏好对齐算法展现了很好的结果，但是监督微调（SFT）仍然对于成功收敛至关重要。本文研究了在偏好对齐的环境中SFT的关键作用，强调对于偏好对齐的SFT来说，对于不受欢迎的生成风格施加轻微惩罚就足够了。在此基础上，我们引入了一种简单而创新的无参考模型的单体赔率比偏好优化算法ORPO，消除了额外的偏好对齐阶段的必要性。我们通过实证和理论手段证明，赔率比是在125M至7B不同规模下进行SFT时对比受欢迎和不受欢迎风格的明智选择。具体来说，使用ORPO在仅UltraFeedback上对Phi-2（2.7B）、Llama-2（7B）和Mistral（7B）进行微调，超越了性能

    arXiv:2403.07691v1 Announce Type: cross  Abstract: While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance o
    
[^26]: Maxwell的恶魔之工作：通过利用神经元饱和实现有效修剪

    Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons

    [https://arxiv.org/abs/2403.07688](https://arxiv.org/abs/2403.07688)

    重新评估深度神经网络中的死亡神经元现象，提出了Demon Pruning（DemP）方法，通过控制死亡神经元的产生，动态实现网络稀疏化。

    

    在训练深度神经网络时，$\textit{死亡神经元}$现象——在训练期间变得不活跃或饱和，输出为零的单元—传统上被视为不可取的，与优化挑战有关，并导致在不断学习的情况下丧失可塑性。本文重新评估了这一现象，专注于稀疏性和修剪。通过系统地探索各种超参数配置对死亡神经元的影响，我们揭示了它们有助于促进简单而有效的结构化修剪算法的潜力。我们提出了$\textit{Demon Pruning}$（DemP），一种控制死亡神经元扩张，动态导致网络稀疏性的方法。通过在活跃单元上注入噪声和采用单周期调度正则化策略的组合，DemP因其简单性和广泛适用性而脱颖而出。在CIFAR10上的实验中...

    arXiv:2403.07688v1 Announce Type: cross  Abstract: When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 an
    
[^27]: 低成本注释：利用地理数据相似性平衡模型性能和注释成本

    Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost

    [https://arxiv.org/abs/2403.07687](https://arxiv.org/abs/2403.07687)

    提出一种方法来识别数据，以平衡模型性能和注释成本

    

    当前的基础模型在各种任务中表现出色。然而，一些研究表明，由于训练过程中使用的数据在地理和经济上的不平衡表示，这些模型并不对每个人都有效。大多数数据来自西方国家，导致对代表性不足的国家的结果不佳。为了解决这个问题，需要从这些国家收集更多数据，但注释成本可能是一个重大瓶颈。在本文中，我们提出了一种方法来识别需要注释的数据，以平衡模型性能和注释成本。我们的方法首先涉及找到具有最大视觉差异的主题（物体和动作）图像的国家。接下来，我们确定了对于这些主题在视觉上更相似的国家，并表明利用这些国家可以提高模型性能，同时在注释成本方面节省开销。

    arXiv:2403.07687v1 Announce Type: cross  Abstract: Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that usin
    
[^28]: 使用贝叶斯神经场进行可扩展的时空预测

    Scalable Spatiotemporal Prediction with Bayesian Neural Fields

    [https://arxiv.org/abs/2403.07657](https://arxiv.org/abs/2403.07657)

    该论文提出了贝叶斯神经场（BayesNF），结合了深度神经网络和分层贝叶斯推断，用于处理大规模时空预测问题。

    

    时空数据集由空间参考的时间序列表示，广泛应用于许多科学和商业智能领域，例如空气污染监测，疾病跟踪和云需求预测。随着现代数据集规模和复杂性的不断增加，需要新的统计方法来捕捉复杂的时空动态并处理大规模预测问题。本研究介绍了Bayesian Neural Field (BayesNF)，这是一个用于推断时空域上丰富概率分布的通用领域统计模型，可用于包括预测、插值和变异分析在内的数据分析任务。BayesNF将用于高容量函数估计的新型深度神经网络架构与用于鲁棒不确定性量化的分层贝叶斯推断相结合。通过在定义先验分布方面进行序列化

    arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
    
[^29]: 狩猎属性：面向弱监督语义分割的上下文原型感知学习

    Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation

    [https://arxiv.org/abs/2403.07630](https://arxiv.org/abs/2403.07630)

    本研究提出一种上下文原型感知学习策略（CPAL），通过减少知识偏差以增强原型表示能力，从而捕获实例语义的多样和细粒度特征属性。

    

    最近，弱监督语义分割（WSSS）方法致力于整合上下文知识以提高类激活映射（CAM）的完整性。在这项工作中，我们认为实例与上下文之间的知识偏差影响原型充分理解实例语义的能力。受原型学习理论启发，我们提出利用原型感知来捕获实例的多样和细粒度特征属性。我们的假设是，由于这种知识偏差，上下文原型可能会错误地激活相似和频繁共同出现的对象类别。因此，我们提出通过减少偏差来增强原型的表示能力，以更好地捕捉语义对象区域的空间覆盖。基于此目标，我们提出一种上下文原型感知学习（CPAL）策略，利用语义上下文来丰富实例理解。

    arXiv:2403.07630v1 Announce Type: cross  Abstract: Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of t
    
[^30]: 压缩暗图像增强的多个潜在空间映射

    Multiple Latent Space Mapping for Compressed Dark Image Enhancement

    [https://arxiv.org/abs/2403.07622](https://arxiv.org/abs/2403.07622)

    该研究提出了一种基于变分自动编码器的新颖潜在空间映射网络，以处理增强压缩暗图像时避免压缩伪影放大的问题。

    

    暗图像增强旨在将暗图像转换为正常亮度的图像。现有的暗图像增强方法以未经压缩的暗图像为输入，并取得了良好的性能。然而，在实践中，暗图像经常在存储或通过互联网传输之前被压缩。目前的方法在处理压缩暗图像时表现出性能不佳。当前方法放大了隐藏在暗区域中的伪影，这导致观察者感到不适的视觉效果。基于这一观察，该研究旨在增强压缩暗图像，同时避免压缩伪影的放大。由于纹理细节与压缩伪影在压缩暗图像中交织在一起，细节增强与阻塞伪影抑制在图像空间中相互矛盾。因此，我们在潜在空间中处理任务。为此，我们提出了一种基于变分自动编码器的新颖潜在映射网络。

    arXiv:2403.07622v1 Announce Type: cross  Abstract: Dark image enhancement aims at converting dark images to normal-light images. Existing dark image enhancement methods take uncompressed dark images as inputs and achieve great performance. However, in practice, dark images are often compressed before storage or transmission over the Internet. Current methods get poor performance when processing compressed dark images. Artifacts hidden in the dark regions are amplified by current methods, which results in uncomfortable visual effects for observers. Based on this observation, this study aims at enhancing compressed dark images while avoiding compression artifacts amplification. Since texture details intertwine with compression artifacts in compressed dark images, detail enhancement and blocking artifacts suppression contradict each other in image space. Therefore, we handle the task in latent space. To this end, we propose a novel latent mapping network based on variational auto-encoder 
    
[^31]: 通过逐层部分机器遗忘实现训练模型中的有效知识删除

    Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning

    [https://arxiv.org/abs/2403.07611](https://arxiv.org/abs/2403.07611)

    该论文介绍了一种新颖的机器遗忘算法，分别采用部分失忆式遗忘和逐层部分更新的方法，以更高效地在训练模型中删除知识。

    

    arXiv:2403.07611v1 发表类型：cross  摘要：机器遗忘因其能够有选择地擦除已经训练的机器学习模型中从特定训练数据样本获得的知识而受到广泛关注。这种能力使数据持有者能够严格遵守数据保护法规。然而，现有的遗忘技术面临实际约束，通常导致性能下降，需要遗忘后进行简短的微调，并需要大量存储空间。作为回应，本文引入了一种新颖的机器遗忘算法。第一种方法是部分失忆式遗忘，将逐层修剪与失忆式遗忘相结合。在这种方法中，训练过程中对模型进行的更新被修剪并存储，随后用于从训练模型中遗忘特定数据。第二种方法将逐层部分更新集成到标签翻转和基于优化的遗忘中，以减轻由于遗忘而产生的不利影响。

    arXiv:2403.07611v1 Announce Type: cross  Abstract: Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of da
    
[^32]: Couler: 云中统一机器学习工作流优化

    Couler: Unified Machine Learning Workflow Optimization in Cloud

    [https://arxiv.org/abs/2403.07608](https://arxiv.org/abs/2403.07608)

    设计并实现了Couler系统，用于云中统一机器学习工作流优化，主要见解在于能够使用自然生成ML工作流

    

    机器学习（ML）已经变得无处不在，推动着各种组织中的数据驱动应用。与传统观念中研究中的ML相反，ML工作流可能是复杂的，资源密集的，并且耗时的。扩展ML工作流以包含更广泛的数据基础设施和数据类型可能导致更大的工作量和增加的部署成本。目前，有许多工作流引擎可用（其中超过十个得到广泛认可）。这种多样性对于最终用户来说构成了掌握不同引擎API的挑战。虽然目前的努力主要集中在针对特定工作流引擎优化ML操作（MLOps），但当前方法在跨不同引擎进行工作流优化方面很大程度上被忽视。

    arXiv:2403.07608v1 Announce Type: cross  Abstract: Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural 
    
[^33]: 优化负面提示以增强文本到图像生成中的美学和保真度

    Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation

    [https://arxiv.org/abs/2403.07605](https://arxiv.org/abs/2403.07605)

    提出NegOpt方法，通过监督微调和强化学习优化负面提示的生成，显著提高图像生成质量，超越其他方法并构建了负面提示数据集。

    

    在文本到图像生成中，使用描述不良图像特征的负面提示可以显著提高图像质量。然而，生成良好的负面提示是一项手工而繁琐的工作。为了解决这个问题，我们提出了NegOpt，一种新颖的方法，通过监督微调和强化学习来优化负面提示生成，从而增强图像生成。我们的综合方法相对于其他方法大幅提高了25%的Inception Score，并超越了来自测试集的标准负面提示。此外，使用NegOpt，我们可以有选择地优化对我们最重要的指标。最后，我们构建了负面提示数据集Negative Prompts DB。

    arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
    
[^34]: 分布式 Web 的长期语义数据使用条款

    Perennial Semantic Data Terms of Use for Decentralized Web

    [https://arxiv.org/abs/2403.07587](https://arxiv.org/abs/2403.07587)

    提出了一种新颖的数据使用条款（DToU）的形式化描述，以及一个 DToU 推理器，使用户和应用程序可以使用本地知识指定其DToU政策的部分，从而验证合规性，并防止数据滥用。

    

    在当今数字化的环境中，网络日益中心化，引发对用户隐私侵犯的担忧。分布式 Web 架构（如 Solid）为用户提供了更好掌控个人“Pods”数据的解决方案。然而，一个重要挑战仍然存在：用户必须在众多应用程序间进行抉择，决定哪个应用程序可以被信任访问他们的数据 Pods。这通常涉及阅读冗长复杂的使用条款协议，用户往往觉得艰难或干脆忽视。这威胁了用户自主权，并阻碍了对数据滥用的检测。我们提出了一种新颖的数据使用条款（DToU）的形式化描述，以及一个 DToU 推理器。用户和应用程序会使用本地知识指定他们DToU政策的部分，涵盖权限，要求，禁止和义务。自动推理会验证合规性，同时推导政策...

    arXiv:2403.07587v1 Announce Type: new  Abstract: In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives poli
    
[^35]: 在模拟家庭环境中联邦学习社会适宜的机器人行为

    Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments

    [https://arxiv.org/abs/2403.07586](https://arxiv.org/abs/2403.07586)

    本研究提出了在模拟家庭环境中进行联邦学习的新方法，旨在评估机器人行为的社会适宜性，并结合持续学习方法，使机器人可以从彼此的经验中学习社会规范。

    

    随着社交机器人越来越多地融入日常生活，确保它们的行为符合社会规范变得至关重要。为了广泛应用于开放世界，探索个体机器人可以在学习其独特环境的同时也从彼此的经验中学习的联邦学习（FL）设置非常重要。本文提出了一个新颖的FL基准，评估不同策略，使用多标签回归目标，其中每个客户端分别学习预测不同机器人行为的社会适宜性，同时与他人共享其学习。此外，通过将训练数据按不同上下文拆分，使每个客户端逐渐跨上下文学习，我们提出了一个新颖的联邦持续学习（FCL）基准，将FL方法调整为使用最先进的持续学习（CL）方法，以持续学习社会适宜的代理行为。

    arXiv:2403.07586v1 Announce Type: cross  Abstract: As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behav
    
[^36]: 迈向具有可适应性计算和网络融合的动态未来（ACNC）

    Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)

    [https://arxiv.org/abs/2403.07573](https://arxiv.org/abs/2403.07573)

    本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。

    

    在推进6G的背景下，预计会出现实质性的范式转变，突出了由大量连接和严格遵守服务质量/体验（QoS/E）先决条件所特征化的全面的一切对一切交互。即将面临的挑战源于资源稀缺，促使有意识地向计算-网络融合（CNC）过渡，作为联合资源编排的有前途的方法。虽然基于CNC的机制引起了人们的关注，但它们在实现未来服务方面的有效性，特别是在类似Metaverse的使用情景中，可能会由于用户、服务和资源不断变化的特性而受到限制。因此，本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。

    arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
    
[^37]: 使用多步深度强化学习改进血糖控制策略

    An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.07566](https://arxiv.org/abs/2403.07566)

    本文提出了一种使用多步深度强化学习改进血糖控制策略的算法，通过转换BG控制问题的形式化，考虑药物作用的延迟和持久性，提高了效率。

    

    血糖（BG）控制涉及通过体外胰岛素注射将个体的血糖维持在健康范围内，这对于1型糖尿病患者来说是一项重要任务。然而，传统的患者自我管理方式繁琐且危险。最近的研究致力于探索个性化和自动化的BG控制方法，其中深度强化学习（DRL）显示出作为新兴方法的潜力。在本文中，我们使用药物浓度的指数衰减模型将BG控制问题的形式化转换为MDP，考虑到药物作用的延迟和持久性，从PAE-POMDP（持续行动效果-部分可见马尔可夫决策过程）到MDP，并提出一种新颖的基于多步DRL的算法来解决该问题。其中还使用了优先经验回放（PER）采样方法。与单步自举更新相比，多步学习

    arXiv:2403.07566v1 Announce Type: new  Abstract: Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes. However,traditional patient self-management is cumbersome and risky. Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach. In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem. The Prioritized Experience Replay (PER) sampling method is also used in it. Compared to single-step bootstrapped updates, multi-step learning is
    
[^38]: 为多智能体路径规划集成优先级混合策略

    Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding

    [https://arxiv.org/abs/2403.07559](https://arxiv.org/abs/2403.07559)

    提出了Ensembling Prioritized Hybrid Policies (EPH)方法，通过选择性通信模块和三种高级推理策略，提高了基于通信的多智能体路径规划解决方案的性能。

    

    基于多智能体强化学习（MARL）的多智能体路径规划（MAPF）近来因其高效性和可扩展性而受到关注。我们提出了一种新方法，Ensembling Prioritized Hybrid Policies (EPH)，以进一步提高基于通信的MARL-MAPF求解器的性能。我们首先提出了一个选择性通信模块，以在多智能体环境中收集更丰富的信息，从而实现更好的智能体协调，并使用基于Q-learning的算法对模型进行训练。

    arXiv:2403.07559v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Se
    
[^39]: 文档索引的未来：GPT和Donut革新目录内容处理

    The future of document indexing: GPT and Donut revolutionize table of content processing

    [https://arxiv.org/abs/2403.07553](https://arxiv.org/abs/2403.07553)

    该论文介绍了一种利用Donut和OpenAI GPT-3.5 Turbo两种前沿AI模型自动提取文件中结构化信息的创新方法，在建筑规格文件的目录处理中取得了显著的准确性，代表了文档索引领域向自动化信息提取迈出的重要一步。

    

    工业项目严重依赖冗长、复杂的规格文件，手工提取结构化信息繁琐且效率低下。本文介绍一种创新方法来自动化这一过程，利用两种前沿AI模型的能力：Donut，一种可以直接从扫描文档中提取信息而无需OCR的模型，以及OpenAI GPT-3.5 Turbo，一个强大的大型语言模型。所提出的方法首先通过获取建筑规格文件的目录（ToCs），然后将ToCs文本结构化为JSON数据。尤为显著的准确性被实现，Donut在有效组织ToCs方面达到85%，GPT-3.5 Turbo达到89%。这一里程碑式的成就代表了文档索引领域的重大进步，展示了AI自动化信息提取在各种文档类型中的巨大潜力，提升了效率。

    arXiv:2403.07553v1 Announce Type: cross  Abstract: Industrial projects rely heavily on lengthy, complex specification documents, making tedious manual extraction of structured information a major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting-edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specification documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effectively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting effic
    
[^40]: 互动指令跟随代理的在线持续学习

    Online Continual Learning For Interactive Instruction Following Agents

    [https://arxiv.org/abs/2403.07548](https://arxiv.org/abs/2403.07548)

    我们提出了针对具身代理的两种持续学习设置：学习新行为和新环境。同时，我们通过自信度得分来更新存储的信息，从而避免需要任务边界信息的问题。

    

    在通过语言指令执行日常任务的具身代理学习过程中，文献大都假定代理在开始时就学习所有训练数据。我们认为这样的学习场景较不现实，因为机器人代理应该在探索和感知世界的过程中不断地学习。为了朝着更真实的具身代理学习场景迈进一步，我们提出了两种持续学习设置供具身代理使用；学习新行为（行为增量学习，Behavior-IL）和新环境（环境增量学习，Environment-IL）。在任务中，先前基于“数据先验”的持续学习方法维护过去任务的logits。然而，存储的信息往往是不充分学习的信息，需要任务边界信息，而这种信息并不总是可用。在这里，我们提议基于自信度得分而无需任务边界信息来更新它们。

    arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d
    
[^41]: WannaLaugh: 一种可配置的勒索软件模拟器 -- 学习模仿恶意存储痕迹

    WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces

    [https://arxiv.org/abs/2403.07540](https://arxiv.org/abs/2403.07540)

    本文介绍了一种能够安全模拟勒索软件攻击的模拟器，通过生成存储I/O痕迹，并利用这些痕迹来训练机器学习模型，有效地检测勒索软件，为发展负责任的网络安全工具提供了实际应用。

    

    勒索软件作为一种可怕且快速发展的网络安全威胁，持续对全球个人和组织造成严重后果。传统的检测方法，依赖于静态签名和应用程序行为模式，受到这些威胁动态本质的挑战。本文介绍了三个主要贡献，以解决这一挑战。首先，我们介绍了一个勒索软件模拟器。该工具旨在安全地模仿勒索软件攻击，而不引起实际伤害或传播恶意软件，使其成为研究勒索软件行为的独特解决方案。其次，我们演示了如何使用该模拟器创建存储I/O痕迹。然后利用这些痕迹来训练机器学习模型。我们的结果显示这些模型在检测勒索软件方面是有效的，突显了我们的模拟器在开发负责任的网络安全工具方面的实际应用。第三，我们展示了我们的模拟器如何

    arXiv:2403.07540v1 Announce Type: cross  Abstract: Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide. Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats. This paper introduces three primary contributions to address this challenge. First, we introduce a ransomware emulator. This tool is designed to safely mimic ransomware attacks without causing actual harm or spreading malware, making it a unique solution for studying ransomware behavior. Second, we demonstrate how we use this emulator to create storage I/O traces. These traces are then utilized to train machine-learning models. Our results show that these models are effective in detecting ransomware, highlighting the practical application of our emulator in developing responsible cybersecurity tools. Third, we show how our emulator can be
    
[^42]: 相关性分数：规划中的一种类似地标的启发式方法

    Relevance Score: A Landmark-Like Heuristic for Planning

    [https://arxiv.org/abs/2403.07510](https://arxiv.org/abs/2403.07510)

    提出了一种新颖的“相关性分数”启发式方法，可以帮助识别在大多数但不是所有计划中出现的事实或行动，相比于传统地标启发式方法，在缺乏明确定义地标的问题上具有更好的性能表现。

    

    地标是计划问题所有有效解决方案中都出现的事实或行动。它们已成功用于计算引导计划搜索的启发式方法。我们通过定义一种新颖的“相关性分数”来扩展这一概念，以帮助识别出现在大多数但不是所有计划中以实现任何给定目标的事实或行动。我们描述了计算这种相关性分数的方法，并将其用作计划搜索中的启发式方法。我们通过使用基准规划问题实验比较了我们的方法与基于地标的最先进启发式规划方法的性能。尽管原始的基于地标的启发式方法在具有明确定义地标的问题上表现更好，但我们的方法在缺乏非平凡地标的问题上显着提高了性能。

    arXiv:2403.07510v1 Announce Type: new  Abstract: Landmarks are facts or actions that appear in all valid solutions of a planning problem. They have been used successfully to calculate heuristics that guide the search for a plan. We investigate an extension to this concept by defining a novel "relevance score" that helps identify facts or actions that appear in most but not all plans to achieve any given goal. We describe an approach to compute this relevance score and use it as a heuristic in the search for a plan. We experimentally compare the performance of our approach with that of a state of the art landmark-based heuristic planning approach using benchmark planning problems. While the original landmark-based heuristic leads to better performance on problems with well-defined landmarks, our approach substantially improves performance on problems that lack non-trivial landmarks.
    
[^43]: 分块LoRA：重新审视文本到图像生成中的细粒度LoRA以实现有效的个性化和风格化

    Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation

    [https://arxiv.org/abs/2403.07500](https://arxiv.org/abs/2403.07500)

    提出了分块低秩适应（LoRA）方法，在文本到图像生成中实现了有效的个性化和风格化细粒度微调。

    

    个性化和风格化在文本到图像中的目标是指导一个预训练扩散模型分析用户引入的新概念，并将其融入到预期的风格中。 最近，广泛采用了参数高效微调（PEFT）方法来解决这一任务，并极大推动了该领域的发展。 尽管它们很受欢迎，现有的高效微调方法仍然难以实现文本到图像生成中的有效个性化和风格化。 为了解决这个问题，我们提出了分块低秩适应（LoRA）以对SD的不同块执行细粒度微调，可以生成忠实于输入提示和目标身份且具有所需样式的图像。 大量实验证明了所提方法的有效性。

    arXiv:2403.07500v1 Announce Type: cross  Abstract: The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method.
    
[^44]: 一种深度学习方法用于糖尿病诊断

    A Deep Learning Approach to Diabetes Diagnosis

    [https://arxiv.org/abs/2403.07483](https://arxiv.org/abs/2403.07483)

    采用深度学习方法，提出一种无创糖尿病诊断方法，通过反向传播神经网络和数据平衡技术，在准确性、敏感性和特异性方面取得显著改进

    

    糖尿病是由胰岛素产生或利用不足导致的，对身体造成了广泛的危害。现有的诊断方法通常是侵入性的，并伴有诸多缺点，比如成本限制。尽管存在像类间k最近邻(CkNN)和通用回归神经网络(GRNN)这样的机器学习模型，但它们在处理不平衡数据时往往表现不佳。利用传感技术和机器学习的进展，我们提出了一种使用带有批量标准化的反向传播神经网络(BPNN)进行无创糖尿病诊断的方法，结合数据重采样和归一化以实现类平衡。我们的方法解决了传统机器学习中存在的诸多挑战，比如与传统方法相关的性能受限。在三个数据集上的实验结果显示，与传统方法相比，我们在整体准确性、敏感性和特异性方面取得了显著的改进。值得注意的是，我们实现了高准确率

    arXiv:2403.07483v1 Announce Type: cross  Abstract: Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accur
    
[^45]: 基于矩阵变换的低秩调整（MTLoRA）：一种受大脑启发的参数高效微调方法

    Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2403.07440](https://arxiv.org/abs/2403.07440)

    该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。

    

    基于大型预训练语言模型（LPLMs）的微调技术已被证明可以显著提高模型在各种下游任务上的性能，并有效控制LPLMs的输出行为。本文受大脑功能受其几何结构塑造的启发，将这一思想融入LoRA技术中，提出了一种新的基于矩阵变换的重新参数化方法，以减少复杂任务适应性、性能、稳定性和算法复杂性方面的改进空间。

    arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
    
[^46]: 提高推理速度和减少遗忘：早期退出网络在持续学习中的双重好处

    Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning

    [https://arxiv.org/abs/2403.07404](https://arxiv.org/abs/2403.07404)

    早期退出网络在持续学习中展现出降低遗忘和在资源利用上表现优异的特点

    

    arXiv:2403.07404v1 公告类型: 跨界 摘要: 受深度神经网络能源高效利用需求驱动，早期退出方法备受关注。这些策略通过在网络早期做出决定，实现快速预测，从而节省计算时间和资源。然而，迄今为止，早期退出网络仅针对静态数据分布进行了开发，限制了它们在具有持续非静态数据的实际场景中的应用。本研究旨在探讨早期退出网络的持续学习。我们改编现有的持续学习方法以适应早期退出架构，并研究它们在持续设置中的行为。我们注意到，早期网络层表现出减少遗忘，即使使用的资源显著更少，也能胜过标准网络。此外，我们分析任务最近性偏差对早期退出推理的影响，并提出任务...

    arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
    
[^47]: 从食堂食物到日常餐点：将食物识别泛化到更实际的场景

    From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios

    [https://arxiv.org/abs/2403.07403](https://arxiv.org/abs/2403.07403)

    将食物识别从食堂场景泛化到日常生活场景，提出了两个新基准数据集DailyFood-172和DailyFood-16，旨在评估方法的可转移性。

    

    食物类别的精确识别对智能健康管理至关重要，在近年来引起了广泛的研究关注。著名的基准数据集，如Food-101和VIREO Food-172，提供了丰富的食物图像资源，推动了该领域研究的繁荣发展。然而，这些数据集都是从食堂场景精心策划的，与日常生活中的食物外观有所偏差。这种差异给在这些食堂数据集上训练的分类器有效地转移到人类所遇到的更广泛的日常生活场景中带来了巨大挑战。为此，我们提出了两个新的基准数据集，即DailyFood-172和DailyFood-16，专门设计用于策划来自日常餐点的食物图像。这两个数据集被用来评估方法从精心策划的食物图像领域向日常生活食物图像领域的可转移性。此外，我们还提出了一种简单的方法

    arXiv:2403.07403v1 Announce Type: cross  Abstract: The precise recognition of food categories plays a pivotal role for intelligent health management, attracting significant research attention in recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172, provide abundant food image resources that catalyze the prosperity of research in this field. Nevertheless, these datasets are well-curated from canteen scenarios and thus deviate from food appearances in daily life. This discrepancy poses great challenges in effectively transferring classifiers trained on these canteen datasets to broader daily-life scenarios encountered by humans. Toward this end, we present two new benchmarks, namely DailyFood-172 and DailyFood-16, specifically designed to curate food images from everyday meals. These two datasets are used to evaluate the transferability of approaches from the well-curated food image domain to the everyday-life food image domain. In addition, we also propose a simple ye
    
[^48]: 在常识知识图上进行逻辑查询的复杂推理

    Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs

    [https://arxiv.org/abs/2403.07398](https://arxiv.org/abs/2403.07398)

    提出了COM2数据集，通过在常识知识图中抽样多跳逻辑查询并结合大型语言模型，显著提高了语言模型在复杂推理能力方面的性能。

    

    事件常识推理需要具有推理事件之间关系的能力，以及推断在这种关系之下的隐含上下文。然而，数据稀缺使得语言模型难以学会为涉及复杂事件相互作用的背景和问题生成常识推断变得具有挑战性。为了满足这种需求，我们提出了COM2（COMplex COMmonsense），这是一个通过从现有常识知识图（CSKG）中抽样多跳逻辑查询（例如，事件A和B的联合效果或因果关系，或事件C的效果的效果），并利用手工制作的规则和大型语言模型将其用多选和文本生成问题的形式表达出来的新数据集。我们的实验表明，在COM2上训练的语言模型在复杂推理能力方面取得了显著的改进，从而增强了零-shot性能，无论是在领域内还是领域外的任务中。

    arXiv:2403.07398v1 Announce Type: cross  Abstract: Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for
    
[^49]: 辅助CycleGAN引导下的从双向到单向IHC图像的任务感知域翻译

    Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images

    [https://arxiv.org/abs/2403.07389](https://arxiv.org/abs/2403.07389)

    通过引入新的训练设计，从而利用辅助的免疫荧光图像域，我们提出了一种用于从双向到单向IHC图像的任务感知域翻译的方法，该方法在下游分割任务中表现出比基线方法更好的效果。

    

    生成模型使得从一个源图像域到一个在训练中未见过的目标域的转换成为可能。虽然Cycle生成对抗网络（GANs）已经被广泛应用，但其中的循环一致性约束依赖于两个域之间存在可逆映射的情况，而在染色单向和双向免疫组化（IHC）检测的图像之间的转换不是这样的。针对从后者到前者的转换，我们提出了一种新颖的训练设计，引入了一种新的约束，利用一组免疫荧光（IF）图像作为辅助的不配对图像域。在下游分割任务上的定量和定性结果显示，相比基线方法，所提出的方法带来了显著的好处。

    arXiv:2403.07389v1 Announce Type: cross  Abstract: Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches.
    
[^50]: SmallToLarge (S2L): 通过总结小模型的训练轨迹，为大型语言模型的微调提供可伸缩的数据选择

    SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models

    [https://arxiv.org/abs/2403.07384](https://arxiv.org/abs/2403.07384)

    S2L提出了一种通过总结小模型的训练轨迹，来指导大型语言模型数据选择的方法，显著提高了数学问题解决中监督微调的数据效率，并在数据集性能上表现优异。

    

    尽管数据选择在大型语言模型（LLMs）的预训练和指导微调阶段非常有效，但在专业领域的监督微调（SFT）中改善数据效率面临着重大挑战，原因是微调数据的复杂性。为弥合这一差距，我们引入了一种有效且可伸缩的数据选择方法S2L（SmallToLarge），它利用小模型的训练轨迹来指导更大模型的数据选择。通过大量实验，我们证明了S2L显著提高了数学问题解决的SFT数据效率，将训练数据缩减到原始MathInstruct数据集（Yue等人，2023）的仅11%，以达到全数据集的性能，并在6个领域内外评估数据集中平均优于最先进的数据选择算法4.7%。值得注意的是，仅选择50K数据进行SFT，S2L实现...

    arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
    
[^51]: 基于Gabor引导的变压器用于单幅图像去雨

    Gabor-guided transformer for single image deraining

    [https://arxiv.org/abs/2403.07380](https://arxiv.org/abs/2403.07380)

    本文提出了一种基于Gabor引导的变压器（Gabformer）用于单幅图像去雨，通过融入Gabor滤波器处理的信息提高了对局部纹理特征的关注，以及增强了模型对噪声的鲁棒性。

    

    图像去雨已经引起了广泛关注，以应对恶劣天气条件对视觉任务造成的挑战。本文提出了一种基于Gabor引导的变压器（Gabformer）用于单幅图像去雨，通过将Gabor滤波器处理的信息融入查询向量来增强对局部纹理特征的关注，从而提高了模型对噪声的鲁棒性。实验结果表明，我们的方法优于当今最先进的方法。

    arXiv:2403.07380v1 Announce Type: cross  Abstract: Image deraining have have gained a great deal of attention in order to address the challenges posed by the effects of harsh weather conditions on visual tasks. While convolutional neural networks (CNNs) are popular, their limitations in capturing global information may result in ineffective rain removal. Transformer-based methods with self-attention mechanisms have improved, but they tend to distort high-frequency details that are crucial for image fidelity. To solve this problem, we propose the Gabor-guided tranformer (Gabformer) for single image deraining. The focus on local texture features is enhanced by incorporating the information processed by the Gabor filter into the query vector, which also improves the robustness of the model to noise due to the properties of the filter. Extensive experiments on the benchmarks demonstrate that our method outperforms state-of-the-art approaches.
    
[^52]: NavCoT: 通过学习解耦推理提升基于LLM的视觉与语言导航

    NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning

    [https://arxiv.org/abs/2403.07376](https://arxiv.org/abs/2403.07376)

    本文提出了一种名为NavCoT的新策略，在视觉与语言导航中通过学习解耦推理，实现了自主导航决策，有效减轻了领域差距。

    

    视觉与语言导航(VLN)作为具有重要研究价值的具身人工智能问题，需要一个具身代理根据自然语言指示穿越复杂的3D环境。最近的研究突出了大型语言模型(LLMs)在VLN中提高导航推理准确性和可解释性的潜力。然而，它们主要在离线方式下的使用通常在VLN任务和LLM训练语料库之间遭受显著的领域差距。本文引入了一种名为导航思维链(NavCoT)的新型策略，我们通过完成领域内高效参数训练，实现自主导航决策，有效减轻领域差距的成本。具体地，在每个时间步，LLM被提示通过作为世界模型来预测导航思维链：1)根据

    arXiv:2403.07376v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the
    
[^53]: 一个新的直觉模糊决策树随机森林集成

    A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees

    [https://arxiv.org/abs/2403.07363](https://arxiv.org/abs/2403.07363)

    提出了一种新的直觉模糊决策树随机森林集成方法，融合了随机性、模糊逻辑和模糊集的灵活性，以及多分类器系统的稳健性

    

    分类对于数据挖掘、人工智能和故障检测领域的应用至关重要。在开发准确、适用且高效的分类方法和算法具有广泛适用性方面存在强烈需求。随机森林是一种常用于复杂条件下分类的通用算法。尽管它被广泛采用，但其与不同模糊理论的结合仍有探索的价值。本文提出直觉模糊随机森林（IFRF），它是一种由直觉模糊决策树（IFDT）组成的新的随机森林集成。森林中的这些树使用直觉模糊信息增益来选择特征，并考虑信息传输中的犹豫。所提出的方法融合了来自自助抽样和特征选择的随机性、模糊逻辑和模糊集的灵活性，以及多分类器系统的稳健性。

    arXiv:2403.07363v1 Announce Type: new  Abstract: Classification is essential to the applications in the field of data mining, artificial intelligence, and fault detection. There exists a strong need in developing accurate, suitable, and efficient classification methods and algorithms with broad applicability. Random forest is a general algorithm that is often used for classification under complex conditions. Although it has been widely adopted, its combination with diverse fuzzy theory is still worth exploring. In this paper, we propose the intuitionistic fuzzy random forest (IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees (IFDT). Such trees in forest use intuitionistic fuzzy information gain to select features and consider hesitation in information transmission. The proposed method enjoys the power of the randomness from bootstrapped sampling and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the robustness of multiple classifier syste
    
[^54]: 挑战遗忘：揭示机器遗忘中最坏情况遗忘集

    Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning

    [https://arxiv.org/abs/2403.07362](https://arxiv.org/abs/2403.07362)

    该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。

    

    靠谱的机器学习(Machine Learning, ML)社区越来越认识到模型在训练后有选择性地“遗忘”数据点的重要性。这引出了机器遗忘(Machine Unlearning, MU)问题，旨在消除选定数据点对模型性能的影响，同时仍保持模型在遗忘后的实用性。尽管有各种MU方法来擦除数据影响，评估主要集中在随机数据遗忘上，忽视了对于真实衡量遗忘性能的数据子集选择的重要探究。为解决这一问题，我们从对抗的角度引入了一种新的MU评估视角。我们提出确定那些对影响擦除构成最大挑战的数据子集，即找出最坏情况遗忘集。利用双层优化原则，我们增强了在上层优化中的遗忘挑战。

    arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
    
[^55]: 基于矢量量化的深度学习在大规模MIMO系统中用于CSI反馈

    Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems

    [https://arxiv.org/abs/2403.07355](https://arxiv.org/abs/2403.07355)

    提出了一种基于矢量量化的深度学习方法，用于大规模MIMO系统中的CSI反馈，通过引入特定的转换函数和可训练的码本设计策略，降低了计算复杂度并提高了CSI重建性能。

    

    本文介绍了一种基于有限速率深度学习（DL）的信道状态信息（CSI）反馈方法，用于大规模多输入多输出（MIMO）系统。该方法在矢量量化变分自动编码器（VQ-VAE）框架下提供了潜在矢量的有限位表示，同时基于形状增益矢量量化减少了计算复杂度。在这种方法中，潜在矢量的幅度使用合适的转换函数和非均匀标量码本进行量化，而潜在矢量的方向则使用可训练的Grassmann码本进行量化。通过引入一个用于嵌套码本的码字选择规则和损失函数设计，还开发了一种多速率码本设计策略。仿真结果表明，所提出的方法降低了与VQ-VAE相关的计算复杂度，同时改善了CSI重建性能。

    arXiv:2403.07355v1 Announce Type: cross  Abstract: This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction pe
    
[^56]: KEBench: 用于大型视觉-语言模型知识编辑的基准测试

    KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models

    [https://arxiv.org/abs/2403.07350](https://arxiv.org/abs/2403.07350)

    KEBench提出了一个新的基准测试，采用不同的数据收集方法和新增加的度量标准（可移植性），以全面评估大型视觉-语言模型知识编辑的质量。

    

    arXiv:2403.07350v1 公告类型: 跨领域 摘要: 目前，针对大型视觉-语言模型(LVLMs)的知识编辑研究很少。编辑LVLMs面临着有效整合多种模态（图像和文本）的挑战，同时确保修改连贯且与上下文相关。现有基准测试具有三个度量标准（可靠性、局部性和一般性）用于衡量LVLMs的知识编辑。然而，该基准测试在评估中使用的生成图像质量不足，并且无法评估模型是否有效地利用与相关内容相关的编辑知识。我们采用不同的数据收集方法构建了一个新的基准测试$\textbf{KEBench}$，并扩展了新度量标准(可移植性)以进行全面评估。借助多模态知识图，我们的图像数据呈现出明确的给实体方向性。这种方向性可以进一步用于提取与实体相关的知识和进行编辑。

    arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
    
[^57]: 重新思考ASTE:一种极简的标记方案与对比学习

    Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning

    [https://arxiv.org/abs/2403.07342](https://arxiv.org/abs/2403.07342)

    提出一种新颖的标记方案，并采用对比学习方法来重新思考ASTE，该方法在性能上优于最先进技术，同时具有更紧凑的设计和降低的计算开销，尤其在少样本学习情景下展现出优越效果。

    

    Aspect Sentiment Triplet Extraction (ASTE) 是细粒度情感分析的一个新兴子任务，旨在从非结构化文本数据中提取结构化的情感三元组。现有的ASTE方法通常通过额外的结构或外部数据来复杂化任务。在这项研究中，我们提出了一种新颖的标记方案，并采用对比学习方法来缓解这些挑战。所提出的方法在与最先进技术的比较中展示出可比甚至更优越的性能，同时具有更紧凑的设计和降低的计算开销。值得注意的是，在大语言模型(LLMs)时代，我们的方法在少样本学习情景下展现出优于GPT 3.5和GPT 4的效果。本研究还为在大语言模型范式下推进ASTE技术提供了宝贵的见解。

    arXiv:2403.07342v1 Announce Type: cross  Abstract: Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models.
    
[^58]: 基于大窗口的Mamba UNet用于医学图像分割：超越卷积和自注意力

    Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention

    [https://arxiv.org/abs/2403.07332](https://arxiv.org/abs/2403.07332)

    该论文提出了一个基于大窗口的Mamba UNet用于医学图像分割，相比传统方法，在局部空间建模方面有优势，同时在全局建模方面保持高效率。

    

    在临床实践中，医学图像分割提供了有关目标器官或组织轮廓和尺寸的有用信息，有助于改进诊断、分析和治疗。最近几年，卷积神经网络（CNN）和Transformer在这一领域占据主导地位，但它们仍然存在一定问题，如有限的感知范围或昂贵的远程建模。Mamba，作为一种具有线性复杂度的长程依赖性建模的状态空间序列模型（SSM），最近出现为一种有前途的范式。在本文中，我们介绍了一种用于2D和3D医学图像分割的基于大窗口的Mamba U-形网络，即LMa-UNet。我们LMa-UNet的一个突出特点是利用大窗口，在局部空间建模方面优于基于小核的CNN和基于小窗口的Transformer，同时与具有二次复杂度的自注意力相比，在全局建模方面保持卓越的效率。

    arXiv:2403.07332v1 Announce Type: cross  Abstract: In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment. In the past few years, convolutional neural networks (CNNs) and Transformers have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity. In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based CNNs and small window-based Transformers, while maintaining superior efficiency in global modeling compared to self-attention with quadratic co
    
[^59]: 一个问题中心的多专家对比学习框架，用于提高深度序列知识追踪模型的准确性和可解释性

    A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models

    [https://arxiv.org/abs/2403.07322](https://arxiv.org/abs/2403.07322)

    通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战

    

    知识追踪在通过分析学生历史学习过程来预测其未来表现中发挥着至关重要的作用。深度神经网络在解决知识追踪问题方面展现出巨大潜力。然而，将深度学习技术应用于模拟知识追踪过程仍然存在一些重要挑战。第一个挑战在于将问题的个体信息融入建模中。这很关键，因为尽管问题共享相同的知识组件（KC），但学生对同质问题的知识习得可以有显著差异。第二个挑战在于解释现有基于深度学习的知识追踪模型的预测结果。在真实应用中，虽然可能并不需要完全透明和可解释的模型参数，但关键是以老师能理解的方式呈现模型的预测结果。

    arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
    
[^60]: 针对败血症治疗的强化序贯决策：具有死亡分类器和变压器的POSNEGDM框架

    Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer

    [https://arxiv.org/abs/2403.07309](https://arxiv.org/abs/2403.07309)

    该研究提出了POSNEGDM框架，利用变压器模型和反馈强化器，在败血症治疗中取得显著改进，将患者生存率提高至97.39％，明显优于传统算法。

    

    败血症是由机体对感染产生夸张反应引发的一种危及生命的情况，要求紧急干预以防止严重并发症。现有的用于处理败血症的机器学习方法在离线场景中效果不佳，存活率低于50％。本文介绍了POSNEGDM框架，即“用于序贯决策的具有正负示范的强化学习”，利用创新的基于变压器的模型和反馈强化器复制专家行为，同时考虑个体患者特征。具有96.7％准确度的死亡分类器指导治疗决策取得积极结果。POSNEGDM框架显著提高了患者的生存率，挽救了97.39％的患者，优于现有的机器学习算法（决策变压器和行为克隆）的存活率分别为33.4％和43.5％。

    arXiv:2403.07309v1 Announce Type: cross  Abstract: Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making" framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7\% accuracy guides treatment decisions towards positive outcomes. The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respective
    
[^61]: 使用验证辅助学习神经网络屏障函数并具有终止保证

    Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees

    [https://arxiv.org/abs/2403.07308](https://arxiv.org/abs/2403.07308)

    提出了一种使用验证辅助学习神经网络屏障函数的终止保证方法。

    

    屏障函数是为系统建立安全保证的一般框架。然而，目前尚无通用方法找到这些函数。为了解决这一缺点，最近的方法使用自监督学习技术，通过由验证程序周期生成的训练数据来学习这些函数，从而导致一个具有验证辅助的学习框架。尽管验证辅助学习框架在自动合成屏障函数方面有巨大潜力，但该框架缺乏终止保证，并且在实践中可能在找到有效屏障函数的成功率方面存在问题。在本文中，我们提出了一种全面解决这些缺点的方法。通过对屏障函数合成的凸形式化，我们首先提出学习一个经验良好的NN基函数，然后应用一种利用凸性和验证中的反例的微调算法。

    arXiv:2403.07308v1 Announce Type: cross  Abstract: Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verif
    
[^62]: 通过自表达图结构重建对图数据进行压缩

    Graph Data Condensation via Self-expressive Graph Structure Reconstruction

    [https://arxiv.org/abs/2403.07294](https://arxiv.org/abs/2403.07294)

    通过自表达图结构重建的方法解决了图数据压缩中的问题

    

    随着训练大规模图神经网络（GNNs）需求的增加，图数据压缩已经成为在训练阶段减轻存储和时间成本的关键技术。它旨在将原始大规模图压缩为一个更小的合成图，同时保留训练下游GNN所需的基本信息。然而，现有方法要么集中于仅优化节点特征，要么努力独立学习节点特征和图结构生成器。它们无法明确利用原始图结构的信息，并未能为合成数据集构建可解释的图结构。为了解决这些问题，我们引入了一种名为\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})的新型框架。我们的方法突出之处在于

    arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
    
[^63]: 在统一网络结构上通过知识重播进行持续全方位恶劣天气去除

    Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure

    [https://arxiv.org/abs/2403.07292](https://arxiv.org/abs/2403.07292)

    该论文研究了如何在统一网络结构上进行持续全方位恶劣天气去除任务，以适应现实世界中不断变化的恶劣天气条件，而非传统静态学习范式。

    

    在现实世界的应用中，由恶劣天气引起的图像退化总是复杂的，并且随着不同天气条件的变化而变化。现实世界环境中的系统不断遇到以前未曾观察到的恶劣天气条件。因此，恶劣天气去除模型实际上需要不断学习增量收集的反映各种退化类型的数据。现有的恶劣天气去除方法，无论是针对单个还是多个恶劣天气，主要设计为静态学习范式，假设在单一阶段学习过程之前可以精细收集处理所有类型的退化数据。因此，它们无法直接处理增量学习需求。为了解决这个问题，我们最早努力研究了持续全方位恶劣天气去除任务，处于更接近实际的环境设置。

    arXiv:2403.07292v1 Announce Type: cross  Abstract: In real-world applications, image degeneration caused by adverse weather is always complex and changes with different weather conditions from days and seasons. Systems in real-world environments constantly encounter adverse weather conditions that are not previously observed. Therefore, it practically requires adverse weather removal models to continually learn from incrementally collected data reflecting various degeneration types. Existing adverse weather removal approaches, for either single or multiple adverse weathers, are mainly designed for a static learning paradigm, which assumes that the data of all types of degenerations to handle can be finely collected at one time before a single-phase learning process. They thus cannot directly handle the incremental learning requirements. To address this issue, we made the earliest effort to investigate the continual all-in-one adverse weather removal task, in a setting closer to real-wo
    
[^64]: 基于贝叶斯方法的图像分类中OOD鲁棒性解决方案

    A Bayesian Approach to OOD Robustness in Image Classification

    [https://arxiv.org/abs/2403.07277](https://arxiv.org/abs/2403.07277)

    本文提出了一种基于贝叶斯方法的图像分类中OOD鲁棒性解决方案，利用扩展的组合神经网络和von Mises-Fisher核来处理真实世界的OOD问题。

    

    计算机视觉中一个重要且未解决的问题是确保算法对图像领域的变化具有鲁棒性。我们在目标领域中处理此问题的情况下，但没有注释的图像。在面临真实世界的域之外（OOD）干扰和遮挡的OOD-CV基准挑战的激励下，我们引入了一种新颖的贝叶斯方法来实现物体分类的OOD鲁棒性。我们的工作扩展了已被证明在遮挡情况下具有鲁棒性但在OOD数据测试时严重降级的组合神经网络（CompNets）。我们利用了CompNets包含的在von Mises-Fisher（vMF）核表示的特征向量上定义的生成头，这些核大致对应于对象部分，并且可以在无监督的情况下学习。我们观察到不同域之间的某些vMF核是相似的，而另一些则不是。这使我们能够学习一个transiti

    arXiv:2403.07277v1 Announce Type: cross  Abstract: An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transiti
    
[^65]: Anderson加速用于迭代重新加权的$\ell_1$算法

    Anderson acceleration for iteratively reweighted $\ell_1$ algorithm

    [https://arxiv.org/abs/2403.07271](https://arxiv.org/abs/2403.07271)

    提出了一种Anderson加速的IRL1算法，将其收敛结果扩展到非光滑场景，不依赖于Kurdyka-Lojasiewicz条件

    

    迭代重新加权L1（IRL1）算法是一种常见算法，用于解决具有非凸和非光滑正则化的稀疏优化问题。其加速算法的发展，通常采用Nesterov加速，引起了极大兴趣。然而，这些加速算法的收敛性和复杂性分析一直存在重大挑战。最近，Anderson加速因其在加速固定点迭代方面的出色性能而备受瞩目，许多最近的研究将其应用于基于梯度的算法。受到Anderson加速强大影响的启发，我们提出了一种Anderson加速的IRL1算法，并建立了其局部线性收敛速度。我们将这一通常在平滑设置中观察到的收敛结果扩展到非光滑场景。重要的是，我们的理论结果不依赖于Kurdyka-Lojasiewicz条件。

    arXiv:2403.07271v1 Announce Type: cross  Abstract: Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization. The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest. Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges. Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms. Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate. We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario. Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a
    
[^66]: 适应优势的策略优化用于离线强化学习

    Advantage-Aware Policy Optimization for Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.07262](https://arxiv.org/abs/2403.07262)

    介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。

    

    离线强化学习致力于利用离线数据集来制定有效的智能体策略，而无需在线交互，通过在行为策略的支持下施加适当的保守约束来解决分布外问题。本文引入了一种新的适应优势的策略优化（A2PO）方法，以明确构建针对混合质量数据集的离线学习优势感知策略约束。

    arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
    
[^67]: 通过对抗性数据增强来将策略从离线任务表示学习中解藕

    Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation

    [https://arxiv.org/abs/2403.07261](https://arxiv.org/abs/2403.07261)

    通过对抗性数据增强方法，该研究解决了从有限数量策略中学习任务表示的问题，从而将策略与离线任务表示学习分离。

    

    离线元强化学习（OMRL）高效地允许一个代理在仅依赖于静态数据集的情况下处理新颖任务。为了精准高效地识别任务，现有的OMRL研究建议学习单独的任务表示，并将其与策略输入结合，从而形成基于上下文的元策略。训练任务表示的主要方法是采用使用多任务离线数据的对比学习。数据集通常涵盖来自各种策略（即行为策略）的相互作用，从而提供了关于不同任务的大量上下文信息。然而，在现实设置中，收集来自大量策略的数据不仅不切实际，而且通常难以实现。相反，我们转而采取更受限制但更实际的情形，即使用有限数量的策略进行多任务数据收集。我们观察到，学习到的任务表示

    arXiv:2403.07261v1 Announce Type: cross  Abstract: Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representatio
    
[^68]: 基于深度学习辅助的机器类型通信中无授权NOMA的并行干扰消除

    Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication

    [https://arxiv.org/abs/2403.07255](https://arxiv.org/abs/2403.07255)

    论文提出了一种基于深度学习辅助的并行干扰消除方法，用于在无授权NOMA系统中联合处理活动检测、信道估计和数据检测问题。

    

    在这篇论文中，我们提出了一种新颖的方法，用于在上行无授权非正交多址接入（NOMA）系统中联合进行活动检测（AD）、信道估计（CE）和数据检测（DD）。我们的方法采用了一个受并行干扰消除（PIC）启发的迭代和并行干扰移除策略，并结合了深度学习来共同解决AD、CE和DD问题。基于这种方法，我们开发了三种PIC框架，每种框架都设计用于相干或非一致方案。第一个框架在相干方案中使用接收到的导频信号进行联合AD和CE。在此框架基础上，第二个框架利用接收到的导频和数据信号进行CE，进一步增强了相干方案中AD、CE和DD的性能。第三个框架设计用于适应包含少量数据位的非相干方案，同时实现...

    arXiv:2403.07255v1 Announce Type: cross  Abstract: In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously perf
    
[^69]: Curry-DPO：利用课程学习和排名偏好增强对齐

    Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences

    [https://arxiv.org/abs/2403.07230](https://arxiv.org/abs/2403.07230)

    提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。

    

    直接偏好优化(DPO)是一种有效的技术，利用成对偏好数据(通常是每个用户提示选择和拒绝的响应对)将LLMs与人类偏好对齐。在实践中，对于给定提示可能会存在多个响应，这些响应的质量相对于彼此而言有所不同。有了这些多个响应的质量评级，我们提出利用这些响应为给定提示创建多个偏好对。我们的工作侧重于通过课程学习方法系统地利用构建的多个偏好对来进行DPO训练。特别是，我们根据不同的标准将这些多个偏好数据对从易到难(模拟课程训练)排序。我们详细比较了我们提出的方法与标准单一对DPO设置。我们的方法，我们称之为Curry-DPO，在MTbench、Vicuna、Wiz上始终表现出增强的性能收益。

    arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
    
[^70]: 使用选择性状态空间模型预测急性脑功能障碍状态的多队列研究

    A multi-cohort study on prediction of acute brain dysfunction states using selective state space models

    [https://arxiv.org/abs/2403.07201](https://arxiv.org/abs/2403.07201)

    该研究利用电子健康记录数据开发了用于ICU病人急性脑功能障碍预测的自动化方法，动态预测谵妄、昏迷和死亡，填补了现有文献中的研究空白。

    

    评估急性脑功能障碍（包括重症监护室（ICU）中的谵妄和昏迷）是一项重要挑战，目前的诊断方法依赖于不经常的临床观察，我们的研究旨在利用电子健康记

    arXiv:2403.07201v1 Announce Type: cross  Abstract: Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes. Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient's ABD status after onset. Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU. Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small sample size, proprietary single-hospital datasets. Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals thr
    
[^71]: 使用属性选择和不同多模态数据源的集成改进智能辅导系统中预测学生表现

    Improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources

    [https://arxiv.org/abs/2403.07194](https://arxiv.org/abs/2403.07194)

    使用属性选择和集成方法，结合不同多模态数据源，可以改进智能辅导系统中对学生表现的预测能力。

    

    本研究旨在利用智能辅导系统来预测大学生的学习表现，使用了不同来源的数据。我们从40名学生采集和预处理了来自不同多模态来源的数据：系统日志中的学习策略，面部录像中的情绪，眼动追踪中的交互区域，以及最终知识评估的测试表现。我们的目标是通过使用属性选择和分类集成来测试是否可以改进预测。我们通过将六种分类算法应用于数值化和离散化预处理的多模态数据进行了三个实验。结果表明，使用集成和选择最佳属性的方法结合数值数据时可以得到最佳的预测结果。

    arXiv:2403.07194v1 Announce Type: cross  Abstract: The aim of this study was to predict university students' learning performance using different sources of data from an Intelligent Tutoring System. We collected and preprocessed data from 40 students from different multimodal sources: learning strategies from system logs, emotions from face recording videos, interaction zones from eye tracking, and test performance from final knowledge evaluation. Our objective was to test whether the prediction could be improved by using attribute selection and classification ensembles. We carried out three experiments by applying six classification algorithms to numerical and discretized preprocessed multimodal data. The results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.
    
[^72]: CuentosIE：一个关于“寓言寓意”的聊天机器人是否有助于教授情商？

    CuentosIE: can a chatbot about "tales with a message" help to teach emotional intelligence?

    [https://arxiv.org/abs/2403.07193](https://arxiv.org/abs/2403.07193)

    CuentosIE提供了一套高度专业化的寓言故事，并提供了一系列工具，旨在教育用户情感知识，并监测他们的情感发展。

    

    在本文中，我们介绍了CuentosIE（TalesEI：一个关于情商发展的寓言寓意聊天机器人），这是一个关于情感的教育性聊天机器人，同时也为教师和心理学家提供了一个工具，通过CuentosIE编制的指标和数据来监测他们的学生/患者。选择“寓言寓意”的理由在于它们的简单性和易于理解，这归功于它们的道德或相关的隐喻。 CuentosIE的主要贡献在于选择、收集和分类一组高度专业化的故事，以及提供工具（搜索、阅读理解、聊天、推荐和分类）对教育用户情感并监测他们的情感发展都很有用。该工具的初步评估取得了令人鼓舞的结果，这肯定了文章标题中提出的问题。

    arXiv:2403.07193v1 Announce Type: cross  Abstract: In this article, we present CuentosIE (TalesEI: chatbot of tales with a message to develop Emotional Intelligence), an educational chatbot on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of "tales with a message" is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article.
    
[^73]: $\mathbf{(N,K)}$-Puzzle：一种用于基准测试生成语言模型中强化学习算法的成本效益测试平台

    $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model

    [https://arxiv.org/abs/2403.07191](https://arxiv.org/abs/2403.07191)

    提出了一种$(N,K)$-Puzzle测试平台，用于评估和比较生成语言模型中的强化学习算法。

    

    强化学习（RL）算法的最新进展旨在提高规模化语言模型的性能。 然而，缺乏一种成本效益且标准化的测试平台，专门用于评估和比较这些算法。 为填补这一空白，我们提出了24-Puzzle的一般化版本：$(N, K)$-Puzzle，挑战语言模型以使用$N$个整数达到目标值$K$。 我们评估了已建立的RL算法（如Proximal Policy Optimization（PPO）），以及新颖方法（如Identity Policy Optimization（IPO）和Direct Policy Optimization（DPO））的有效性。

    arXiv:2403.07191v1 Announce Type: cross  Abstract: Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).
    
[^74]: 在规模上监测AI修改的内容：AI会议同行评审中ChatGPT影响的案例研究

    Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews

    [https://arxiv.org/abs/2403.07183](https://arxiv.org/abs/2403.07183)

    该研究提出了一种估计大语料库中被大语言模型大幅修改的文本比例的方法，并在AI会议的同行评审中进行了实证分析，发现6.5%至16.9%的文本可能被LLMs大幅修改，揭示了用户行为的一些见解。

    

    我们提出了一种估计大语料库中文本可能被大语言模型（LLM）大幅修改或生成的部分比例的方法。我们的最大似然模型利用专家撰写和AI生成的参考文本，准确高效地检查语料库级别上真实世界LLM使用。我们将这种方法应用于AI会议上科学同行评审的案例研究，该研究发生在ChatGPT发布之后，包括ICLR 2024、NeurIPS 2023、CoRL 2023和EMNLP 2023。我们的研究结果表明，在这些会议提交的同行评审中，6.5%至16.9%的文本可能是由LLMs大幅修改的，即超出拼写检查或小幅更新的范围。生成文本出现的情况为用户行为提供了见解：在报告信心较低、在截止日期前提交的评论以及从评论公司

    arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
    
[^75]: 重建ROME: 解决顺序模型编辑过程中的模型崩溃问题

    Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing

    [https://arxiv.org/abs/2403.07175](https://arxiv.org/abs/2403.07175)

    本文重建了ROME，提供了更稳定的r-ROME实现，解决了顺序模型编辑过程中的模型崩溃问题。

    

    最近关于使用Rank-One Model Editing (ROME)进行模型编辑的研究表明，有一些事实表明该算法无法进行编辑而不破坏模型。这些编辑以前被称为禁用编辑。这些禁用编辑会导致立即模型崩溃，并限制了ROME用于顺序编辑的使用。在本文中，我们做出了两个主要贡献。首先，我们展示了在使用CounterFact数据集进行编辑时，ROME仅在此时发生模型崩溃，并在使用zsRE数据集时不会发生。其次，我们发现禁用编辑是ROME原始实现的产物。通过本文，我们提供了一个更稳定的实现ROME，我们将其称为r-ROME，并展示我们在使用ROME进行大规模顺序编辑时不再观察到模型崩溃。

    arXiv:2403.07175v1 Announce Type: cross  Abstract: Recent work on model editing using Rank-One Model Editing (ROME), a popular model editing method, has shown that there are certain facts that the algorithm is unable to edit without breaking the model. Such edits have previously been called disabling edits. These disabling edits cause immediate model collapse and limits the use of ROME for sequential editing. In this paper, we make two main contributions. Firstly, we show that model collapse with ROME only happens when making edits using the CounterFact dataset and does not happen when using the zsRE dataset. Secondly, we find that disabling edits are an artifact of the original implementation of ROME. With this paper, we provide a more stable implementation ROME, which we call r-ROME and show that we no longer observe model collapse when making large scale sequential edits with ROME.
    
[^76]: 不要忘记我做的事：评估联邦学习中的客户贡献

    Don't Forget What I did?: Assessing Client Contributions in Federated Learning

    [https://arxiv.org/abs/2403.07151](https://arxiv.org/abs/2403.07151)

    提出了一个历史感知的博弈理论框架FLContrib，用来评估联邦学习中的客户贡献。

    

    联邦学习（FL）是一种协作机器学习（ML）方法，多个客户参与训练ML模型，而不暴露私人数据。公平准确评估客户贡献在FL中是一个重要问题，以促进激励分配并鼓励多样化客户参与统一模型训练。本文提出了一个历史感知的博弈理论框架FLContrib，用于评估在每个FL训练时期中的（潜在非独立同分布）客户参与。

    arXiv:2403.07151v1 Announce Type: cross  Abstract: Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client cont
    
[^77]: 价值函数的有限表征能力及其与统计(不)效率的关联

    On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency

    [https://arxiv.org/abs/2403.07136](https://arxiv.org/abs/2403.07136)

    研究发现基于值函数的表征能力有限，导致在某些情况下基于值函数的方法在统计上低效率，这揭示了价值函数和统计效率之间的关联。

    

    在强化学习中，识别基于模型和无模型方法之间的权衡是一个核心问题。 基于值的方法提供了重要的计算优势，并且有时在统计上和基于模型的方法一样有效。 然而，当关注策略评估的核心问题时，我们发现关于转移动态的信息可能无法在价值函数空间中表示。 我们通过一系列着重于许多重要问题中出现的结构的案例研究来探究这一点。 在其中几种情况中，没有信息丢失，基于值的方法与基于模型的方法在统计效率上相当。 在其他相关示例中，信息丢失严重，基于值的方法性能严重不及基于模型的方法。 更深入的研究指出了表征能力的限制作为低效性的驱动因素，而非算法设计上的失误。

    arXiv:2403.07136v1 Announce Type: cross  Abstract: Identifying the trade-offs between model-based and model-free methods is a central question in reinforcement learning. Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods. However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions. We explore this through a series of case studies focused on structures that arises in many important problems. In several, there is no information loss and value-based methods are as statistically efficient as model based ones. In other closely-related examples, information loss is severe and value-based methods are severely outperformed. A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design.
    
[^78]: 带有学习激励函数的大图匹配用于多机器人任务分配

    Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation

    [https://arxiv.org/abs/2403.07131](https://arxiv.org/abs/2403.07131)

    本文提出了一种带有学习激励函数的大图匹配方法，用于多机器人任务分配，通过开发图强化学习框架，学习二部图匹配方法MRTA中的启发式或激励。

    

    大多数现实世界的多机器人任务分配（MRTA）问题需要快速高效的决策制定，通常通过使用启发式辅助方法（如遗传算法、基于拍卖的方法和二部图匹配方法）来实现。这些方法通常具有更好的可解释性形式，相比于MRTA的端到端（学习）神经网络政策。然而，获取适当的启发式方法可能很繁琐、风险较高，并且在某些情况下如果问题过于复杂可能不切实际。这就引发了一个问题：这些启发式方法能否被学习？为此，本文特别开发了一个图强化学习（GRL）框架，用于学习二部图匹配方法MRTA中的启发式或激励。具体地，使用了胶囊注意力策略模型来学习如何为连接任务集和机器人集的二部图中的任务/机器人配对（边）进行加权。

    arXiv:2403.07131v1 Announce Type: new  Abstract: Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule att
    
[^79]: 反映复杂社交媒体数据流中关键社会事件的时间序列分析

    Time Series Analysis of Key Societal Events as Reflected in Complex Social Media Data Streams

    [https://arxiv.org/abs/2403.07090](https://arxiv.org/abs/2403.07090)

    通过时间序列分析探索GAB和Telegram这两个社交媒体平台上的叙事演变，提出了一种新颖的方法来研究多个社交媒体领域，以提取可能被掩盖的关键信息。

    

    arXiv:2403.07090v1 公告类型: 跨界 摘要: 社交媒体平台蕴藏着宝贵的见解，然而提取关键信息可能具有挑战性。传统的自上而下方法通常难以捕捉快速变化事件中的关键信号。随着全球事件迅速演变，包括虚假信息的社交媒体叙事成为重要的见解来源。为了满足归纳策略的需求，我们探索了一种小众社交媒体平台GAB和一个成熟的消息服务Telegram，以开发适用于更广泛范围的方法。本研究使用定量基于语料库的话语分析技术来研究这些平台上的叙事演变。我们的方法是研究多个社交媒体领域以提炼可能被掩盖的关键信息的新颖方式，从而提供有用且可操作的见解。该论文详细介绍了收集和预处理GAB和Telegram数据的技术和方法论方面。

    arXiv:2403.07090v1 Announce Type: cross  Abstract: Social media platforms hold valuable insights, yet extracting essential information can be challenging. Traditional top-down approaches often struggle to capture critical signals in rapidly changing events. As global events evolve swiftly, social media narratives, including instances of disinformation, become significant sources of insights. To address the need for an inductive strategy, we explore a niche social media platform GAB and an established messaging service Telegram, to develop methodologies applicable on a broader scale. This study investigates narrative evolution on these platforms using quantitative corpus-based discourse analysis techniques. Our approach is a novel mode to study multiple social media domains to distil key information which may be obscured otherwise, allowing for useful and actionable insights. The paper details the technical and methodological aspects of gathering and preprocessing GAB and Telegram data 
    
[^80]: 基于LSTM的文本生成：对历史数据集的研究

    LSTM-Based Text Generation: A Study on Historical Datasets

    [https://arxiv.org/abs/2403.07087](https://arxiv.org/abs/2403.07087)

    本研究探讨了基于LSTM的文本生成在历史数据集上的应用，展示了这些模型在生成语言丰富、语境相关的文本方面的高准确性和高效率。

    

    本文探讨了长短期记忆（LSTM）网络在文本生成领域的应用，重点在于利用莎士比亚和尼采的历史数据集。LSTMs以其处理序列数据的有效性而闻名，这里应用它们来建模历史文本中固有的复杂语言模式和结构。研究表明，基于历史数据集训练的LSTM模型不仅能够生成语言丰富且语境相关的文本，还能提供关于语言模式随时间演变的见解。研究结果展示了基于LSTM的模型在预测尼采作品文本时的高准确性和高效率，具有较低的损失值和100次迭代的训练时间。模型的准确性为0.9521，表明其高准确性。模型的损失为0.2518，显示其有效性。

    arXiv:2403.07087v1 Announce Type: cross  Abstract: This paper presents an exploration of Long Short-Term Memory (LSTM) networks in the realm of text generation, focusing on the utilization of historical datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical texts. The study demonstrates that LSTM-based models, when trained on historical datasets, can not only generate text that is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting text from works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting text from the w
    
[^81]: 借助先验知识和认知模型改善深度学习：增强可解释性、对抗鲁棒性和零样本学习的调查

    Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning

    [https://arxiv.org/abs/2403.07078](https://arxiv.org/abs/2403.07078)

    本文调查了如何借助先验知识和认知模型来改善深度学习，以提升对抗防御、可解释性人工智能（XAI）和零样本学习，弥补现有深度学习模型在领域知识利用、对抗性攻击防御、解释性以及在开放环境推理中的性能限制。

    

    我们审查了当前和新兴的知识驱动和脑启发的认知系统，用于实现对抗性防御、可解释的人工智能（XAI）以及零样本或少样本学习。数据驱动的深度学习模型在许多应用中取得了显著的性能，并展示了超越人类专家的能力。然而，它们由于无法利用领域知识而在实际应用中存在严重性能限制。特别是，深度学习系统容易受到对抗性攻击，这可能导致它们做出明显错误的决定。此外，复杂的数据驱动模型通常缺乏解释性，即它们的决策无法被人类主体理解。此外，模型通常是在标准数据集上训练的，具有封闭世界的假设。因此，在实际的开放环境中进行推理时，它们很难推广到未见案例。

    arXiv:2403.07078v1 Announce Type: cross  Abstract: We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environmen
    
[^82]: 一站式：图神经网络的多任务提示（扩展摘要）

    All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)

    [https://arxiv.org/abs/2403.07040](https://arxiv.org/abs/2403.07040)

    本文介绍了一种新颖的多任务提示方法，用于解决预训练图模型与不同任务之间的差距，启发自NLP中提示学习的成功。

    

    本文是我们在KDD23中获得最佳研究论文奖的原始工作的扩展摘要，其中我们介绍了一个新颖的方法，用于弥合预训练图模型和它们应用于的不同任务之间的差距，灵感来源于NLP中提示学习的成功。我们意识到了将预训练模型与各种图任务（节点级、边级和图级）对齐的挑战，这可能导致负迁移和性能下降，因此我们提出了一种用于图的多任务提示方法。该方法涉及统一图和语言提示格式，使NLP的提示策略能够适用于图任务。通过分析图应用的任务空间，我们重新制定问题以适应图级任务，并应用元学习来改进提示初始化。

    arXiv:2403.07040v1 Announce Type: cross  Abstract: This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initializ
    
[^83]: 从英语到ASIC：大型语言模型的硬件实现

    From English to ASIC: Hardware Implementation with Large Language Model

    [https://arxiv.org/abs/2403.07039](https://arxiv.org/abs/2403.07039)

    大型语言模型的快速发展改变了ASIC工程领域，但现代语言模型在生成硬件描述代码方面性能不佳，研究重点在于通过微调自然语言模型和重组HDL代码数据集来提高精度和准确性。

    

    在ASIC工程领域，随着大型语言模型的快速发展，现代数字电路的复杂性也在增加，这加剧了对HDL编码的要求，需要更高的精度和复杂性。然而，由于现代语言模型在生成硬件描述代码方面性能不佳，加之相应高质量的代码数据集稀缺，挑战不断。这些挑战凸显了LLM潜力革新数字电路设计与当前能力准确解释和实施硬件规范之间的差距。为了解决这些挑战，制定了一种专注于领先自然语言模型微调和HDL代码数据集重组的策略。

    arXiv:2403.07039v1 Announce Type: cross  Abstract: In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of LLM, paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. To address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The fine-tu
    
[^84]: 通过原型匹配解释典型故障信号的特征

    Interpreting What Typical Fault Signals Look Like via Prototype-matching

    [https://arxiv.org/abs/2403.07033](https://arxiv.org/abs/2403.07033)

    提出了原型匹配网络（PMN），结合人类固有原型匹配和自编码器（AE），通过匹配特征与原型并选择最相似的原型来解释典型故障信号的特征。

    

    神经网络具有强大的非线性映射和分类能力，在机械故障诊断中得到广泛应用以确保安全。然而，作为典型的黑盒模型，它们在高可靠性要求的场景中的应用受到限制。为了理解分类逻辑并解释典型故障信号的特征，提出了结合人类固有原型匹配和自编码器（AE）的原型匹配网络（PMN）。PMN将AE提取的特征与每个原型进行匹配，并选择最相似的原型作为预测结果。它在分类逻辑、故障原型和匹配贡献方面有三条解释路径。传统诊断和领域泛化实验证明了其具有竞争性的诊断性能以及在表示学习中的显着优势。此外，学习到的典型故障信号（即样本级原型）展示了

    arXiv:2403.07033v1 Announce Type: cross  Abstract: Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical black-box models, their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with autoencoder (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in representation learning. Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase 
    
[^85]: STARFlow: 具有注意力学习的空间时间特征重新嵌入用于现实世界的场景流

    STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning for Real-world Scene Flow

    [https://arxiv.org/abs/2403.07032](https://arxiv.org/abs/2403.07032)

    提出了一种全局注意力流嵌入和空间时间特征重新嵌入模块相结合的方法，用于解决现实世界场景流预测中的局部依赖匹配和非刚性物体变形的挑战。

    

    场景流预测是理解动态场景中的关键任务，因为它提供了基本的运动信息。然而，当代场景流方法面临三大挑战。首先，仅基于局部感受野的流估计缺乏点对的长依赖匹配。为了解决这个问题，我们提出了全局注意力流嵌入，以匹配特征空间和欧几里得空间中的所有点对，提供局部细化之前的全局初始化。其次，在变形后存在非刚性物体的变形，导致连续帧之间的时空关系变化。为了更精确地估计残余流，设计了一个空间时间特征重新嵌入模块，以在变形后获取序列特征。此外，由于合成数据和真实数据之间的显著域差异，先前的方法表现出较差的泛化能力。

    arXiv:2403.07032v1 Announce Type: cross  Abstract: Scene flow prediction is a crucial underlying task in understanding dynamic scenes as it offers fundamental motion information. However, contemporary scene flow methods encounter three major challenges. Firstly, flow estimation solely based on local receptive fields lacks long-dependency matching of point pairs. To address this issue, we propose global attentive flow embedding to match all-to-all point pairs in both feature space and Euclidean space, providing global initialization before local refinement. Secondly, there are deformations existing in non-rigid objects after warping, which leads to variations in the spatiotemporal relation between the consecutive frames. For a more precise estimation of residual flow, a spatial temporal feature re-embedding module is devised to acquire the sequence features after deformation. Furthermore, previous methods perform poor generalization due to the significant domain gap between the synthesi
    
[^86]: 一种与元启发式算法可比的高效学习型解决器，用于容量弧路由问题

    An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem

    [https://arxiv.org/abs/2403.07028](https://arxiv.org/abs/2403.07028)

    该论文提出了一种基于神经网络的解决器，通过引入方向感知技术和监督强化学习，显著缩小了与先进元启发式算法的差距，同时表现出优越的效率。

    

    最近，神经网络（NN）在组合优化方面取得了长足进展。然而，它们在解决容量弧路由问题（CARP）时面临挑战，CARP是指在图上找到覆盖所有必需边的最小成本路径，同时在容量约束内。在解决CARP方面，基于NN的方法往往落后于先进的元启发式算法，因为它们缺乏针对复杂CARP定制的定向弧建模和高效学习方法。在本文中，我们引入了一种基于NN的解决器，以大大缩小与先进元启发式算法之间的差距，同时表现出更高的效率。首先，我们提出了方向感知注意模型（DaAM），将方向性引入嵌入过程，促进更有效的一阶段决策。其次，我们设计了一个监督强化学习方案，涉及监督预训练，为随后的强化学习建立一个强大的初始策略。

    arXiv:2403.07028v1 Announce Type: cross  Abstract: Recently, neural networks (NN) have made great strides in combinatorial optimization. However, they face challenges when solving the capacitated arc routing problem (CARP) which is to find the minimum-cost tour covering all required edges on a graph, while within capacity constraints. In tackling CARP, NN-based approaches tend to lag behind advanced metaheuristics, since they lack directed arc modeling and efficient learning methods tailored for complex CARP. In this paper, we introduce an NN-based solver to significantly narrow the gap with advanced metaheuristics while exhibiting superior efficiency. First, we propose the direction-aware attention model (DaAM) to incorporate directionality into the embedding process, facilitating more effective one-stage decision-making. Second, we design a supervised reinforcement learning scheme that involves supervised pre-training to establish a robust initial policy for subsequent reinforcement 
    
[^87]: 一种统一的模型用于具有任意可修改区域单位的时空预测查询

    A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units

    [https://arxiv.org/abs/2403.07022](https://arxiv.org/abs/2403.07022)

    本文提出了一种One4All-ST框架，可以仅使用一个模型为任意可修改区域单位进行ST预测，有效解决了多尺度预测的成本问题和预测不一致性。

    

    时空（ST）预测对于在城市基于位置的应用中做出明智决策（如顺风车）至关重要。然而，现有的ST模型通常需要区域划分作为先决条件，导致两个主要缺点。首先，基于位置的服务需要为不同目的而定义临时区域，需要支持成本高昂的多个具有不同规模和区域的ST模型。其次，不同的ST模型可能产生冲突的输出，导致混乱的预测。 本文提出了One4All-ST框架，可以仅使用一个模型来为任意可修改的区域单元进行ST预测。为了减少获取多尺度预测的成本，我们设计了具有分层空间建模和规模归一化模块的ST网络，以有效且平等地学习多尺度表示。为了解决跨尺度的预测不一致性，我们提出了一种动态规划sch

    arXiv:2403.07022v1 Announce Type: cross  Abstract: Spatio-Temporal (ST) prediction is crucial for making informed decisions in urban location-based applications like ride-sharing. However, existing ST models often require region partition as a prerequisite, resulting in two main pitfalls. Firstly, location-based services necessitate ad-hoc regions for various purposes, requiring multiple ST models with varying scales and zones, which can be costly to support. Secondly, different ST models may produce conflicting outputs, resulting in confusing predictions. In this paper, we propose One4All-ST, a framework that can conduct ST prediction for arbitrary modifiable areal units using only one model. To reduce the cost of getting multi-scale predictions, we design an ST network with hierarchical spatial modeling and scale normalization modules to efficiently and equally learn multi-scale representations. To address prediction inconsistencies across scales, we propose a dynamic programming sch
    
[^88]: 数学在博弈论和人工智能交叉领域中的多智能体学习系统

    Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence

    [https://arxiv.org/abs/2403.07017](https://arxiv.org/abs/2403.07017)

    数学在博弈论和人工智能交叉领域中的多智能体学习系统关注进化博弈论和人工智能之间的联系，探讨了个体策略在人群中演化的情况，并研究了智能体在多智能体环境中根据反馈和经验调整策略的过程。

    

    进化博弈论(EGT)和人工智能(AI)是两个乍一看可能不同的领域，但它们有显著的联系和交汇。前者专注于人群中行为(或策略)的演化，个体与其他人互动，并根据模仿(或社会学习)更新他们的策略。一个策略越成功，随着时间的推移它就会变得越普遍。后者则集中于机器学习算法和(深度)神经网络。它通常是从单一智能体的角度，但越来越多地涉及多智能体环境，在这种环境中，智能体根据反馈和经验调整他们的策略，有点类似于演化过程，但在其自我学习能力上却有所不同。鉴于解决现实问题所需的关键组成部分，包括(i)学习和适应性，(ii)合作和竞争

    arXiv:2403.07017v1 Announce Type: cross  Abstract: Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two fields that, at first glance, might seem distinct, but they have notable connections and intersections. The former focuses on the evolution of behaviors (or strategies) in a population, where individuals interact with others and update their strategies based on imitation (or social learning). The more successful a strategy is, the more prevalent it becomes over time. The latter, meanwhile, is centered on machine learning algorithms and (deep) neural networks. It is often from a single-agent perspective but increasingly involves multi-agent environments, in which intelligent agents adjust their strategies based on feedback and experience, somewhat akin to the evolutionary process yet distinct in their self-learning capacities. In light of the key components necessary to address real-world problems, including (i) learning and adaptation, (ii) cooperation and competit
    
[^89]: 关于球形T-球形模糊（G-TSF）集合及其在G-TSF多标准群体决策中的应用

    On Globular T-Spherical Fuzzy (G-TSF) Sets with Application to G-TSF Multi-Criteria Group Decision-Making

    [https://arxiv.org/abs/2403.07010](https://arxiv.org/abs/2403.07010)

    本文介绍了球形T-球形模糊（G-TSF）集合作为T-球形模糊集合和圆形球形模糊集合的创新扩展概念，利用球形/球体边界表示隶属度、不确定度和非隶属度程度，提供更准确的信息描绘，并通过结构化数据点评估决策对象来增强决策过程。

    

    在本文中，我们介绍了球形T-球形模糊（G-TSF）集合作为T-球形模糊集合（TSFSs）和圆形球形模糊集合（C-SFSs）的创新扩展概念。G-TSF集合利用一个球形/球体边界来表示隶属度、不确定度和非隶属度程度，可以更准确地描绘模糊、含糊和不精确信息。通过在具有特定中心和半径的球体上结构化表示数据点，这个模型通过在一个灵活区域内对对象进行更全面的评估来增强决策过程。在新定义的G-TSF集合之后，我们建立了一些基本的集合运算，并引入了G-TSF值（G-TSFVs）的基本代数运算。这些运算扩展了决策者的评估能力，有助于在更广泛的区域内进行更敏感的决策过程。为了量化相似度测量（SM）b

    arXiv:2403.07010v1 Announce Type: new  Abstract: In this paper, we give the concept of Globular T-Spherical Fuzzy (G-TSF) Sets (G-TSFSs) as an innovative extension of T-Spherical Fuzzy Sets (TSFSs) and Circular Spherical Fuzzy Sets (C-SFSs). G-TSFSs represent membership, indeterminacy, and non-membership degrees using a globular/sphere bound that can offer a more accurate portrayal of vague, ambiguous, and imprecise information. By employing a structured representation of data points on a sphere with a specific center and radius, this model enhances decision-making processes by enabling a more comprehensive evaluation of objects within a flexible region. Following the newly defined G-TSFSs, we establish some basic set operations and introduce fundamental algebraic operations for G-TSF Values (G-TSFVs). These operations expand the evaluative capabilities of decision-makers, facilitating more sensitive decision-making processes in a broader region. To quantify a similarity measure (SM) b
    
[^90]: 自动评价正确: 使用合成数据进行模型评估

    AutoEval Done Right: Using Synthetic Data for Model Evaluation

    [https://arxiv.org/abs/2403.07008](https://arxiv.org/abs/2403.07008)

    提出了用合成数据进行模型评估的方法，通过高效和统计上合理的算法，在GPT-4实验中有效的人工标记样本大小增加了50%。

    

    机器学习模型的评估使用人工标记的验证数据可能既昂贵又耗时。可以使用AI标记的合成数据来减少此类目的人工注释数量，这一过程称为自动评估。我们提出了用于此目的的高效和统计上合理的算法，可以提高样本效率，同时保持不偏。这些算法在与GPT-4进行的实验中将有效的人工标记样本大小增加了高达50%。

    arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
    
[^91]: 具有奖励机器层次结构的多智能体强化学习

    Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines

    [https://arxiv.org/abs/2403.07005](https://arxiv.org/abs/2403.07005)

    提出了具有层次结构奖励机器的多智能体强化学习（MAHRM），可以处理更复杂的情况，将任务分解为简单子任务的层次结构，以减少计算复杂性。

    

    在本文中，我们研究了利用奖励机器（RMs）来指定奖励函数，并通过这种方式利用任务中高级事件的先验知识来促进学习效率的合作式多智能体强化学习（MARL）问题。我们提出了具有层次结构奖励机器的多智能体强化学习（MAHRM），能够处理更复杂的场景，其中智能体之间的事件可以同时发生且互相依赖。

    arXiv:2403.07005v1 Announce Type: new  Abstract: In this paper, we study the cooperative Multi-Agent Reinforcement Learning (MARL) problems using Reward Machines (RMs) to specify the reward functions such that the prior knowledge of high-level events in a task can be leveraged to facilitate the learning efficiency. Unlike the existing work that RMs have been incorporated into MARL for task decomposition and policy learning in relatively simple domains or with an assumption of independencies among the agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs (MAHRM) that is capable of dealing with more complex scenarios when the events among agents can occur concurrently and the agents are highly interdependent.   MAHRM exploits the relationship of high-level events to decompose a task into a hierarchy of simpler subtasks that are assigned to a small group of agents, so as to reduce the overall computational complexity.   Experimental results in three cooperative MAR
    
[^92]: 一些凸消息传递算法收敛到固定点

    Convergence of Some Convex Message Passing Algorithms to a Fixed Point

    [https://arxiv.org/abs/2403.07004](https://arxiv.org/abs/2403.07004)

    这项研究证明了一些凸消息传递算法会收敛到固定点，并在一定迭代次数内达到特定精度。

    

    在图模型中解决MAP推断问题的一种流行方法是通过（块状）坐标下降最小化从对偶线性规划或Lagrange松弛中获得的一个上界。这样的算法包括最大和扩散以及顺序树重新加权消息传递。这些方法的收敛性质目前尚未完全理解。它们已被证明会收敛到由活跃约束的局部一致性所表征的集合，但收敛速度未知；然而，尚不清楚迭代是否会收敛（到任何一个单一点）。我们证明了一个更强的结果（之前有猜想但从未证明过）：迭代会收敛到算法的一个固定点。此外，我们还展示它们在$\mathcal{O}(1/\varepsilon)$次迭代中达到了精度$\varepsilon>0$。

    arXiv:2403.07004v1 Announce Type: new  Abstract: A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon>0$ in $\mathcal{O}(1/\varepsilon)$ iterations.   We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel p
    
[^93]: 面向智能城市全覆盖智能应急互动响应系统的疏散管理框架

    Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System

    [https://arxiv.org/abs/2403.07003](https://arxiv.org/abs/2403.07003)

    提出了一种面向智能城市全覆盖智能应急互动响应系统的疏散管理框架，结合人工智能和机器学习技术，旨在改进现有的应急响应系统，以提高居民的公共服务和生活质量。

    

    一种面向未来6G网络部署的智能城市解决方案，允许中小企业、行业和政府机构与基础设施连接，并在提高应急准备能力方面发挥关键作用。本文旨在提出一套协调的技术解决方案，将现有的应急响应系统转变为智能互动系统，从而改善居民在家中、在道路上、在医院、交通枢纽等地的公共服务和生活质量。在这一背景下，我们从与人们日常生活密切相关的三个不同应用场景着手考虑城市全景视角，以优化相关部门采取的行动。因此，采用人工智能（AI）和机器学习（ML）技术来实现下一代互联车辆体验，我们专注于发生在工厂的事故。

    arXiv:2403.07003v1 Announce Type: new  Abstract: A smart city solution toward future 6G network deployment allows small and medium sized enterprises (SMEs), industry, and government entities to connect with the infrastructures and play a crucial role in enhancing emergency preparedness with advanced sensors. The objective of this work is to propose a set of coordinated technological solutions to transform an existing emergency response system into an intelligent interactive system, thereby improving the public services and the quality of life for residents at home, on road, in hospitals, transport hubs, etc. In this context, we consider a city wide view from three different application scenes that are closely related to peoples daily life, to optimize the actions taken at relevant departments. Therefore, using artificial intelligence (AI) and machine learning (ML) techniques to enable the next generation connected vehicle experiences, we specifically focus on accidents happening in ind
    
[^94]: 使用深度学习、机器学习和统计方法的生存建模：预测入院后死亡率的比较分析

    Survival modeling using deep learning, machine learning and statistical methods: A comparative analysis for predicting mortality after hospital admission

    [https://arxiv.org/abs/2403.06999](https://arxiv.org/abs/2403.06999)

    该研究进行了使用多种生存分析方法的比较研究，包括传统统计模型和先进的机器学习算法，旨在预测入院后的死亡率。

    

    生存分析对于研究事件发生的时间至关重要，并且能够动态地理解事件随时间发生的概率。各种生存分析技术，从传统的统计模型到最先进的机器学习算法，支持医疗干预和政策决策。然而，它们的比较性能仍在持续讨论中。我们进行了几种生存分析方法的比较研究，包括Cox比例风险模型（CoxPH）、逐步CoxPH、弹性网惩罚Cox模型、随机生存森林（RSF）、梯度提升机器（GBM）学习、AutoScore-Survival、DeepSurv、基于神经网络的时间相关Cox模型（CoxTime）以及DeepHit生存神经网络。我们应用了一致性指数（C指数）进行模型拟合度评估，用积分Brier分数（IBS）进行校准，并考虑了模型的可解释性。

    arXiv:2403.06999v1 Announce Type: cross  Abstract: Survival analysis is essential for studying time-to-event outcomes and providing a dynamic understanding of the probability of an event occurring over time. Various survival analysis techniques, from traditional statistical models to state-of-the-art machine learning algorithms, support healthcare intervention and policy decisions. However, there remains ongoing discussion about their comparative performance. We conducted a comparative study of several survival analysis methods, including Cox proportional hazards (CoxPH), stepwise CoxPH, elastic net penalized Cox model, Random Survival Forests (RSF), Gradient Boosting machine (GBM) learning, AutoScore-Survival, DeepSurv, time-dependent Cox model based on neural network (CoxTime), and DeepHit survival neural network. We applied the concordance index (C-index) for model goodness-of-fit, and integral Brier scores (IBS) for calibration, and considered the model interpretability. As a case 
    
[^95]: 论人类和人工创造力的随机性

    On the stochastics of human and artificial creativity

    [https://arxiv.org/abs/2403.06996](https://arxiv.org/abs/2403.06996)

    论文通过统计表征人类创造力，结合多个领域的先前见解，突出人类创造过程中的随机性质，并利用这一表征评估当代人工智能系统的创造力水平。

    

    什么构成人类创造力，计算机是否能够展现真正的创造力？我们认为，要实现计算机的人类水平智能，或者所谓的人工通用智能，必须同时达到人类水平的创造力。我们通过开发人类创造力的统计表征，结合随机理论、心理学、哲学、神经科学和混沌理论的先前见解，为这一讨论做出贡献。这突显了人类创造过程的随机性质，其中包括一个有偏向导向的随机提案步骤，以及依赖于灵活或可变偏向结构的评估步骤。获得的人类创造力表征随后被用来评估各种当代人工智能系统的创造力水平。我们的分析包括现代人工智能算法，如强化学习、扩散模型和大型语言模型，探讨了这些算法在多大程度上实现了人类创造力的水平。

    arXiv:2403.06996v1 Announce Type: new  Abstract: What constitutes human creativity, and is it possible for computers to exhibit genuine creativity? We argue that achieving human-level intelligence in computers, or so-called Artificial General Intelligence, necessitates attaining also human-level creativity. We contribute to this discussion by developing a statistical representation of human creativity, incorporating prior insights from stochastic theory, psychology, philosophy, neuroscience, and chaos theory. This highlights the stochastic nature of the human creative process, which includes both a bias guided, random proposal step, and an evaluation step depending on a flexible or transformable bias structure. The acquired representation of human creativity is subsequently used to assess the creativity levels of various contemporary AI systems. Our analysis includes modern AI algorithms such as reinforcement learning, diffusion models, and large language models, addressing to what ext
    
[^96]: 容量覆盖推销员问题的精确算法和启发式算法

    Exact algorithms and heuristics for capacitated covering salesman problems

    [https://arxiv.org/abs/2403.06995](https://arxiv.org/abs/2403.06995)

    本文介绍了容量覆盖推销员问题（CCSP），提出了基于ILP和BRKGA的优化方法。

    

    本文介绍了容量覆盖推销员问题（CCSP），探讨了容量车辆路径问题中服务覆盖的概念。在CCSP中，提供了车辆可以经过的位置，其中一些位置有需求的客户。目标是通过驻扎在某一点的车队为客户提供服务，以最小化车辆行驶的总距离。CCSP在客户无需被车辆访问即可提供服务的意义上是独特的。相反，如果客户位于车辆的覆盖区域内，它们就可以得到服务。该假设的动机是某些客户无法到达（例如，无法访问车辆），或者访问每个客户都是不切实际的应用场景。在本文中，基于ILP（整数线性规划）和BRKGA（偏向随机键遗传算法）元启发式，为CCSP提出了优化方法。

    arXiv:2403.06995v1 Announce Type: new  Abstract: This paper introduces the Capacitated Covering Salesman Problem (CCSP), approaching the notion of service by coverage in capacitated vehicle routing problems. In CCSP, locations where vehicles can transit are provided, some of which have customers with demands. The objective is to service customers through a fleet of vehicles based in a depot, minimizing the total distance traversed by the vehicles. CCSP is unique in the sense that customers, to be serviced, do not need to be visited by a vehicle. Instead, they can be serviced if they are within a coverage area of the vehicle. This assumption is motivated by applications in which some customers are unreachable (e.g., forbidden access to vehicles) or visiting every customer is impractical. In this work, optimization methodologies are proposed for the CCSP based on ILP (Integer Linear Programming) and BRKGA (Biased Random-Key Genetic Algorithm) metaheuristic. Computational experiments cond
    
[^97]: 基于物理传感器的深度学习跌倒检测系统

    Physics Sensor Based Deep Learning Fall Detection System

    [https://arxiv.org/abs/2403.06994](https://arxiv.org/abs/2403.06994)

    本文提出了一个基于物理传感器的深度学习跌倒检测系统TSFallDetect，利用顺序深度学习方法解决跌倒动作预测问题。

    

    基于嵌入式传感器的跌倒检测是近年来的一个实用且流行的研究方向。本文提出了一个名为TSFallDetect的完整系统，包括基于嵌入式传感器的数据接收设备、移动深度学习模型部署平台以及一个简单的服务器，用于收集模型和数据以进行未来扩展。此外，我们利用顺序深度学习方法来解决基于惯性和薄膜压力传感器收集的数据的跌倒动作预测问题。

    arXiv:2403.06994v1 Announce Type: cross  Abstract: Fall detection based on embedded sensor is a practical and popular research direction in recent years. In terms of a specific application: fall detection methods based upon physics sensors such as [gyroscope and accelerator] have been exploited using traditional hand crafted features and feed them in machine learning models like Markov chain or just threshold based classification methods. In this paper, we build a complete system named TSFallDetect including data receiving device based on embedded sensor, mobile deep-learning model deploying platform, and a simple server, which will be used to gather models and data for future expansion. On the other hand, we exploit the sequential deep-learning methods to address this falling motion prediction problem based on data collected by inertial and film pressure sensors. We make a empirical study based on existing datasets and our datasets collected from our system separately, which shows tha
    
[^98]: 基于LSTM的自动驾驶车道变更安全预测模型

    Automatic driving lane change safety prediction model based on LSTM

    [https://arxiv.org/abs/2403.06993](https://arxiv.org/abs/2403.06993)

    通过使用基于深度学习方法的安全敏感深度学习模型，提出了一种基于LSTM的自动驾驶车道变更安全预测模型，旨在提高自动驾驶车辆驾驶安全性。

    

    自动驾驶技术可以提高交通安全，减少交通事故。此外，它提高了交通流量，减少拥堵，节约能源并提高出行效率。在相对成熟的自动驾驶技术中，自动驾驶功能分为几个模块：感知、决策、规划和控制，合理的分工可以提高系统的稳定性。因此，自动驾驶车辆需要具备预测周围车辆轨迹的能力，以做出合理的决策规划和安全措施，提高驾驶安全性。本文提出了一种基于深度学习方法的、以短期记忆（LSTM）网络为基础的安全敏感深度学习模型。该模型可以缓解当前自动驾驶轨迹规划的缺点，输出轨迹不仅保证了高准确性，还提升了安全性。

    arXiv:2403.06993v1 Announce Type: cross  Abstract: Autonomous driving technology can improve traffic safety and reduce traffic accidents. In addition, it improves traffic flow, reduces congestion, saves energy and increases travel efficiency. In the relatively mature automatic driving technology, the automatic driving function is divided into several modules: perception, decision-making, planning and control, and a reasonable division of labor can improve the stability of the system. Therefore, autonomous vehicles need to have the ability to predict the trajectory of surrounding vehicles in order to make reasonable decision planning and safety measures to improve driving safety. By using deep learning method, a safety-sensitive deep learning model based on short term memory (LSTM) network is proposed. This model can alleviate the shortcomings of current automatic driving trajectory planning, and the output trajectory not only ensures high accuracy but also improves safety. The cell sta
    
[^99]: MEND：元演示蒸馏用于有效和高效的上下文学习

    MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning

    [https://arxiv.org/abs/2403.06914](https://arxiv.org/abs/2403.06914)

    提出了Meta dEmonstratioN Distillation (MEND)，利用知识蒸馏提高MEND和LLM之间的对齐，实现了高效和有效的上下文学习。

    

    大型语言模型(LLMs)展示了令人印象深刻的上下文学习(ICL)能力，其中LLM为给定的测试输入和少量输入-输出对(演示)进行预测。然而，演示的加入导致自注意机制的计算开销呈二次增加。现有解决方案尝试将冗长的演示蒸馏成紧凑的向量。然而，它们通常需要特定于任务的重新训练或牺牲LLM的上下文学习性能。为了缓解这些挑战，我们提出了Meta dEmonstratioN Distillation (MEND)，其中语言模型学会将任何冗长演示蒸馏为向量，而无需为新的下游任务重新训练。我们利用知识蒸馏增强MEND和LLM之间的对齐，同时实现效率和有效性。MEND具有蒸馏演示的元知识

    arXiv:2403.06914v1 Announce Type: cross  Abstract: Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrat
    
[^100]: CEAT：用于非示范类增量学习的持续扩展和吸收变压器

    CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin

    [https://arxiv.org/abs/2403.06670](https://arxiv.org/abs/2403.06670)

    CEAT提出了一种用于非示范类增量学习的新架构，通过持续扩展和吸收参数的方式解决了可塑性-稳定性困境和分类器偏差问题

    

    在现实世界的应用中，动态场景要求模型具备不断学习新任务而不忘记旧知识的能力。经验重放方法存储一部分旧图像进行联合训练。在更严格的隐私保护场景中，存储旧图像变得不可行，这导致了更为严重的可塑性-稳定性困境和分类器偏差。为应对上述挑战，我们提出了一种新的架构，称为持续扩展和吸收变压器（CEAT）。模型可以通过将扩展-融合层与冻结前期参数并行扩展来学习新知识。任务结束后，我们无损地吸收扩展的参数到主干，以确保参数数量保持恒定。为提高模型的学习能力，我们设计了一种新颖的原型对比损失，以减少旧类和新类之间的重叠。

    arXiv:2403.06670v1 Announce Type: cross  Abstract: In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge. Experience-Replay methods store a subset of the old images for joint training. In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias. To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT). The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters. After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant. To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes 
    
[^101]: DeepSafeMPC: 基于深度学习的安全多智体强化学习模型预测控制

    DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2403.06397](https://arxiv.org/abs/2403.06397)

    DeepSafeMPC是一种基于深度学习的模型预测控制方法，旨在有效预测多智体环境的复杂动态，并应用MARL原则寻找最优解。

    

    安全多智体强化学习（safe MARL）在最近几年逐渐受到关注，强调了智体不仅需要优化全局回报，还需要通过行为约束遵守安全要求的必要性。近期一些工作将控制理论与多智体强化学习相结合，以解决确保安全性的挑战。然而，在这一领域中应用模型预测控制（MPC）方法的应用非常有限，主要是由于多智体环境中复杂且隐式动态的特性。为弥合这一差距，我们提出了一种称为基于深度学习的安全多智体强化学习模型预测控制（DeepSafeMPC）的新方法。DeepSafeMPC 的关键见解是利用集中式深度学习模型很好地预测环境动态。我们的方法应用MARL原则来寻找最优解。

    arXiv:2403.06397v1 Announce Type: cross  Abstract: Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employme
    
[^102]: 具有扩散净化的分离数据一致性的图像恢复

    Decoupled Data Consistency with Diffusion Purification for Image Restoration

    [https://arxiv.org/abs/2403.06054](https://arxiv.org/abs/2403.06054)

    通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。

    

    最近，扩散模型作为一种强大的深度生成先验类别已经引起了人们的关注，由于其出色地建模数据分布的能力，在各种图像恢复任务中表现出色。为了解决图像恢复问题，许多现有技术通过将额外的似然梯度步骤纳入到扩散模型的反向采样过程中来实现数据一致性。然而，这些额外的梯度步骤对于实际应用中存在挑战，因为它们造成了巨大的计算开销，从而增加了推理时间。当使用加速的扩散模型采样器时，这些额外的步骤还会导致额外的困难，因为数据一致性步骤的数量受限于反向采样步骤的数量。在这项工作中，我们提出了一种新颖的基于扩散的图像恢复求解器，通过将反向过程与数据一致性步骤分离来解决这些问题。我们的方法涉及

    arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
    
[^103]: CarbonNet: 计算机视觉在气候变化中的作用是什么？ 应用：学习从地下储存空间几何形状中减缓全球变暖的地质力学

    CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming

    [https://arxiv.org/abs/2403.06025](https://arxiv.org/abs/2403.06025)

    这项研究介绍了一种利用计算机视觉从地下储存空间几何图像中预测陆地表面位移的新方法，为碳捕集和封存项目中的决策提供支持。

    

    我们介绍了一种新方法，使用计算机视觉从地下储存空间几何图像中预测陆地表面位移，以应用于碳捕集和封存（CCS）。CCS已被证明是碳中和社会的关键组成部分。然而，科学家发现存在挑战，包括由于大模型尺度而导致的高计算成本，以及难以泛化具有复杂物理学的预训练模型的限制。我们通过直接从地下储存空间几何图像训练模型来应对这些挑战。我们的目标是理解由碳注入导致的陆地表面位移响应，并利用我们训练的模型来为CCS项目的决策提供信息。

    arXiv:2403.06025v1 Announce Type: cross  Abstract: We introduce a new approach using computer vision to predict the land surface displacement from subsurface geometry images for Carbon Capture and Sequestration (CCS). CCS has been proved to be a key component for a carbon neutral society. However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics. We tackle those challenges by training models directly from the subsurface geometry images. The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects.   We implement multiple models (CNN, ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem. Next, we use the LSTM and transformer for transient mechanics scenario, which is a video prediction problem. It shows ResNetUNe
    
[^104]: 基于残差网络的扩散建模在不平衡数据上的应用

    SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data

    [https://arxiv.org/abs/2403.05918](https://arxiv.org/abs/2403.05918)

    基于残差网络的扩散建模方法能够有效处理不平衡数据，克服了经典过采样方法和基于生成网络的模式塌陷与训练不稳定问题。

    

    在数据挖掘和机器学习领域，通常使用的分类模型在不平衡数据中无法有效学习。为了平衡模型训练前的数据分布，通常使用过采样方法为少数类生成数据，以解决分类不平衡数据的问题。大多数经典的过采样方法基于SMOTE技术，该技术仅关注数据的局部信息，因此生成的数据可能存在不够逼真的问题。在基于生成网络的当前过采样方法中，基于GAN的方法可以捕获数据的真实分布，但训练中存在模式崩溃和不稳定性的问题；基于去噪扩散概率模型的过采样方法中，使用U-Net的逆扩散过程神经网络不适用于表格数据。

    arXiv:2403.05918v1 Announce Type: cross  Abstract: In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and
    
[^105]: GEAR: 一种用于几乎无损生成推断大型语言模型的高效KV缓存压缩方案

    GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM

    [https://arxiv.org/abs/2403.05527](https://arxiv.org/abs/2403.05527)

    GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。

    

    关键-值（KV）缓存已成为加快大型语言模型（LLMs）推断生成速度的事实标准。然而，随着序列长度增加而增长的缓存需求已将LLM推断转变为一个记忆绑定问题，显著地限制了系统吞吐量。现有方法依赖于丢弃不重要的标记或均匀量化所有条目。然而，这种方法往往会产生较高的近似误差来表示压缩后的矩阵。自回归解码过程进一步增加了每个步骤的误差，导致模型生成中的重大偏差和性能恶化。为了解决这一挑战，我们提出了GEAR，一种高效的KV缓存压缩框架，实现几乎无损的高压缩比。

    arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
    
[^106]: ChatASU：唤起LLM的反思，真正理解对话中的方面情绪

    ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues

    [https://arxiv.org/abs/2403.05326](https://arxiv.org/abs/2403.05326)

    本文提出了一个新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索大型语言模型（LLMs）在对话场景中理解方面情绪的能力，并引入了一个子任务Aspect Chain Reasoning（ACR）任务来解决方面共指问题。

    

    在互动场景（例如，问答和对话）中进行方面情绪理解（ASU）近年来引起了越来越多的关注并取得了重要进展。然而，现有研究大多忽略了意见目标（即方面）的共指问题，而这种现象在互动场景特别是对话中普遍存在，限制了ASU的性能。最近，大型语言模型（LLM）展示了将各种NLP任务与聊天范式相结合的强大能力。基于此，本文提出了一项新的基于聊天的方面情绪理解（ChatASU）任务，旨在探索LLMs在对话场景中理解方面情绪的能力。特别是，这项ChatASU任务引入了一个子任务，即方面链推理（ACR）任务，以解决方面共指问题。在此基础上，我们提出了一种可信的自反思方法（TSA）与ChatGLM作为背景。

    arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
    
[^107]: 告诉我实话：一种用于衡量大型语言模型可信度的系统

    Tell me the truth: A system to measure the trustworthiness of Large Language Models

    [https://arxiv.org/abs/2403.04964](https://arxiv.org/abs/2403.04964)

    本文提出了一种基于预定义领域知识图的系统化方法来衡量大型语言模型的可信度。

    

    大型语言模型（LLM）自从2023年11月ChatGPT推出以来，在大多数新闻中占据了重要位置。然而，一年多过去了，公司抵触采用它们的一个主要原因是他们对这些系统的可信度缺乏信心。一项由Baymard（2023）进行的研究发现，ChatGPT-4 在识别网站可用性问题时有80.1%的假阳性错误率。而《JAMA儿科学》杂志（JAMA Pediatrics）于2024年1月的研究发现，ChatGPT 在诊断儿科医疗案例时的准确率为17%（Barile et al., 2024）。那么，何为“信任”？信任是一个相对的、主观的条件，可以根据文化、领域和个体而变化。那么，在给定一个领域的情况下，如何衡量系统的可信度呢？本文提出了一种基于预定义领域知识图表示的系统化方法来衡量可信度。

    arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a 
    
[^108]: 减少自监督学习复杂性改善计算病理学中的弱监督分类性能

    Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology

    [https://arxiv.org/abs/2403.04558](https://arxiv.org/abs/2403.04558)

    本研究探讨了在计算病理学中减少对比自监督学习复杂性对分类性能的改善，通过利用消费级硬件。

    

    深度学习模型已成功应用于从常规可用的组织学数据中提取临床可操作见解。通常，这些模型需要临床医生进行的标注，这种标注稀缺且昂贵。自监督学习（SSL）方法的出现消除了这一障碍，允许对非标注数据进行大规模分析。然而，最近的SSL方法采用日益庞大的模型架构和更大的数据集，导致数据量迅速增加，硬件要求和整体成本增加，使得很少机构能够获得这些资源。因此，我们研究了对比自监督学习在计算病理学中的复杂性与分类性能之间的关系，利用消费级硬件。具体而言，我们分析了数据量、架构和算法的调整对下游分类任务的影响。

    arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, empha
    
[^109]: “在对话中学习”：通过对话中学习实现无需预定义个人资料的个性化对话

    "In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning

    [https://arxiv.org/abs/2403.03102](https://arxiv.org/abs/2403.03102)

    提出了一种In-Dialogue Learning框架，通过对话历史刻画个人设来完成个性化对话生成任务，无需预定义个人资料，并在实验证明其显著改进对话生成性能。

    

    个性化对话系统近年来备受关注，因其能够生成与不同人设一致的响应。然而，大多数现有方法依赖预定义的个人资料，这不仅耗时且劳动密集，还缺乏灵活性。我们提出了In-Dialogue Learning（IDL），一种微调框架，增强了预训练的大型语言模型利用对话历史来刻画个人设，以完成个性化对话生成任务，而无需预定义个人资料。我们在三个数据集上的实验表明，IDL带来了显著的改进，BLEU和ROUGE分数分别增加了高达200%和247%。此外，人工评估的结果进一步验证了我们提出方法的有效性。

    arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
    
[^110]: 大型语言模型与机器学习在电子商务推荐中的新兴协同作用

    Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations

    [https://arxiv.org/abs/2403.02760](https://arxiv.org/abs/2403.02760)

    大型语言模型的兴起对电子商务推荐系统带来了新的发展机遇，突破了基于深度神经网络的推荐方法的局限性。

    

    随着电子商务和网络应用的蓬勃发展，推荐系统已成为我们日常生活中的重要组成部分，根据用户的偏好提供个性化推荐。深度神经网络（DNNs）通过模拟用户和商品之间的互动并融入其文本信息，取得了改善推荐系统的重大进展，但这些基于DNN的方法仍然存在一些限制，如难以有效理解用户的兴趣和捕捉文本信息。此外，大型语言模型（LLMs）的出现如ChatGPT和GPT-4，由于在语言理解等基本任务中具有卓越能力，已经彻底改变了自然语言处理（NLP）和人工智能（AI）领域。

    arXiv:2403.02760v1 Announce Type: new  Abstract: With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences. Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information. It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions. At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understandin
    
[^111]: Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects

    Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects

    [https://arxiv.org/abs/2403.02624](https://arxiv.org/abs/2403.02624)

    该论文提出了帕累托最优估计和策略学习的方法，用于确定如何在短期和长期治疗效果之间进行权衡从而实现最佳治疗。

    

    这篇论文专注于发展帕累托最优估计和策略学习，以确定最有效的治疗方法，从而最大化来自短期和长期效果的总奖励，这可能会相互冲突。 例如，药物剂量的增加可能会提高患者康复速度（短期），但也可能导致严重的长期副作用。虽然最近的研究已经探讨了有关短期或长期效应或两者的问题，但如何在它们之间取得平衡以实现最佳治疗仍然是一个悬而未决的挑战。此外，当使用传统因果表示学习直接估计多个目标时，各种任务之间的优化方向也可能发生冲突。在这篇论文中，我们系统地研究了这些问题，并引入了一个帕累托有效算法，包括帕累托最优估计（POE）和帕累托最优策略学习（POPL）。

    arXiv:2403.02624v1 Announce Type: cross  Abstract: This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL
    
[^112]: 基于内部表征的上下文锐度作为警报：减少幻觉的一个视角

    In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation

    [https://arxiv.org/abs/2403.01548](https://arxiv.org/abs/2403.01548)

    本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。

    

    大型语言模型（LLMs）经常会产生幻觉并产生事实错误，然而我们对它们为什么会犯这些错误的理解仍然有限。在本研究中，我们从内部表征的角度深入探讨LLM幻觉的潜在机制，并发现与幻觉相关的一个突出模式：正确的生成在上下文标记的隐藏状态中具有更清晰的上下文激活，而不正确的生成则没有。利用这一见解，我们提出了一种基于熵的度量来量化上下文隐藏状态之间的“锐度”，并将其纳入解码过程中以制定一种受限解码方法。在各种知识寻求和幻觉基准测试上的实验证明了我们方法的一致有效性，例如，在TruthfulQA上实现了高达8.6点的改进。我们相信这项研究可以提高我们对幻觉的理解。

    arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
    
[^113]: 数据解释器：用于数据科学的LLM代理

    Data Interpreter: An LLM Agent For Data Science

    [https://arxiv.org/abs/2402.18679](https://arxiv.org/abs/2402.18679)

    本研究引入了数据解释器，采用动态规划、工具集成和逻辑错误识别等关键技术，旨在增强数据科学中的问题解决能力。

    

    大型语言模型（LLM）代理已表现出显著的有效性。然而，在需要实时数据调整、优化专业知识以应对各种任务间复杂依赖性以及精确推理的逻辑错误识别的数据科学场景中，它们的性能可能会受到影响。本研究介绍了数据解释器，这是一个设计用于解决强调三种关键技术以增强数据科学中问题解决的方案的代码：1）具有分层图结构的动态规划，用于实时数据适应性；2）工具集成动态化，以增强代码执行过程中的熟练度，丰富必要的专业知识；3）在反馈中识别逻辑不一致性，并通过经验记录来提高效率。我们评估了数据解释器在各种数据科学和现实任务上的表现。与开源基线相比，它展现了s

    arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
    
[^114]: MMSR：符号回归是一个多模态任务

    MMSR: Symbolic Regression is a Multimodal Task

    [https://arxiv.org/abs/2402.18603](https://arxiv.org/abs/2402.18603)

    符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。

    

    数学公式是探索自然规律几千年来人类智慧的结晶。用简洁的数学公式描述复杂的自然规律是科学家不断追求的目标，也是人工智能面临的重大挑战。这一领域被称为符号回归。在本文中，研究人员将从数据到表达式的映射视为翻译问题，并引入了相应的大规模预训练模型。

    arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
    
[^115]: RIME: 具有嘈杂偏好的健壮偏好强化学习

    RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences

    [https://arxiv.org/abs/2402.17257](https://arxiv.org/abs/2402.17257)

    RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。

    

    偏好强化学习（PbRL）通过利用人类偏好作为奖励信号，避免了对奖励设计的需求。然而，当前PbRL算法过度依赖来自领域专家的高质量反馈，导致缺乏鲁棒性。在本文中，我们提出了RIME，一种针对嘈杂偏好的健壮PbRL算法，用于有效地从嘈杂偏好中学习奖励。我们的方法结合了基于样本选择的鉴别器，动态过滤去噪偏好以进行健壮训练。为了减轻选择不正确造成的累积误差，我们提出热启动奖励模型，此外还能填补PbRL中从预训练到在线训练过渡时的性能差距。我们在机器人操纵和运动任务上的实验表明，RIME显著提升了当前最先进的PbRL方法的鲁棒性。消融研究进一步表明，热启动

    arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
    
[^116]: 基于信息的转导式主动学习

    Information-based Transductive Active Learning

    [https://arxiv.org/abs/2402.15898](https://arxiv.org/abs/2402.15898)

    ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。

    

    我们将主动学习推广到解决现实世界中采样受限于可访问域的情况，而预测目标可能位于这个域之外。为此，我们提出了ITL，即基于信息的转导式学习，一种自适应采样的方法，旨在最大化关于指定预测目标的信息获取。在一般正则性假设下，我们展示了ITL收敛到可从可访问数据中获得的最小可能不确定性。我们在两个关键应用中展示了ITL：大型神经网络的少样本微调和安全贝叶斯优化，在两种情况下，ITL明显优于最先进技术。

    arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
    
[^117]: 主动少样本微调

    Active Few-Shot Fine-Tuning

    [https://arxiv.org/abs/2402.15441](https://arxiv.org/abs/2402.15441)

    该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。

    

    我们研究了大型神经网络对下游任务进行主动少样本微调。我们表明少样本微调是传统主动学习和转导主动学习的泛化实例，我们提出了信息基于转导学习（ITL）的方法，该方法自适应地进行采样以最大化获得对指定下游任务的信息。在一般正则性假设下，我们证明ITL均匀收敛到可从可访问数据获取的最小可能的不确定性。据我们所知，我们是首批推导出这种泛化界限的人，这对于主动学习可能是具有独立意义的。我们将ITL应用于大型神经网络的少样本微调中，结果显示ITL明显改进了现有技术。

    arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
    
[^118]: CounterCurate: 通过对照例子增强物理和语义视觉-语言组合推理能力

    CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples

    [https://arxiv.org/abs/2402.13254](https://arxiv.org/abs/2402.13254)

    本研究提出CounterCurate框架，通过对比例子和生成式微调，全面提升视觉-语言组合推理能力，解决了物理推理和语义对照微调方面的关键问题，实现了显著性能改进。

    

    我们提出CounterCurate，一个框架，全面提升对比和生成式多模态模型的视觉-语言组合推理能力。特别地，我们确定了两个尚未充分探讨的关键问题：忽视了基于物理的推理（计数和位置理解），以及利用高性能文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一个解决这些空白的方法。我们首先突出了多模态模型（如CLIP和LLaVA）在基于物理的组合推理中几乎无法胜任的表现。然后，我们应用简单的数据增强，使用基于图像的生成模型GLIGEN生成微调数据，使得性能显著提高：在我们新的策划的Flickr30k-Positions基准测试中，CLIP和LLaVA的性能分别提高了+33%和+37%。此外，我们利用了高性能文本和图像生成模型的能力。

    arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
    
[^119]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^120]: 基于3D场景表示的3D扩散器Actor：通过策略扩散进行机器人操作

    3D Diffuser Actor: Policy Diffusion with 3D Scene Representations

    [https://arxiv.org/abs/2402.10885](https://arxiv.org/abs/2402.10885)

    通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。

    

    我们将扩散策略和3D场景表示相结合，用于机器人操作。扩散策略通过条件扩散模型学习基于机器人和环境状态的动作分布。最近，它们已经表现出优于确定性和其他基于状态的动作分布学习方法。3D机器人策略使用从单个或多个摄像头视角获取的感应深度聚合的3D场景特征表示。它们已经证明比其2D对应物在摄像机视角上具有更好的泛化能力。我们统一了这两条线路的工作，并提出了3D扩散器Actor，这是一个神经策略架构，它在给定语言指令的情况下，构建视觉场景的3D表示，并在其上进行条件迭代去噪机器人末端执行器的3D旋转和平移。在每个去噪迭代中，我们的模型将末端执行器姿态估计表示为3D场景令牌，并预测t

    arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
    
[^121]: Clifford群等变单体消息传递网络

    Clifford Group Equivariant Simplicial Message Passing Networks

    [https://arxiv.org/abs/2402.10011](https://arxiv.org/abs/2402.10011)

    本论文介绍了一种Clifford群等变单体消息传递网络，通过将Clifford群等变层与单体消息传递相结合，实现了在拓扑上更为复杂的E（n）-等变消息传递。实验结果表明，该方法具有良好的效果。

    

    我们引入了Clifford群等变单体消息传递网络，这是一种在单体复合体上进行可控的E（n）-等变消息传递的方法。我们的方法将Clifford群等变层的表达能力与单体消息传递相结合，后者在拓扑上比常规图消息传递更加复杂。Clifford代数包括高阶对象，如双向量和三向量，这些对象通过向量衍生出几何特征（例如面积，体积）。利用这些知识，我们通过顶点的几何乘积表示简单形式特征。为了实现高效的单体消息传递，我们在不同维度之间共享消息网络的参数。此外，我们将最终的消息限制为来自不同维度的传入消息的聚合，从而导致了我们称之为共享单体消息传递的方法。实验结果表明，我们的方法能够输出适当的结果。

    arXiv:2402.10011v1 Announce Type: new  Abstract: We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to ou
    
[^122]: 检查生成对抗网络判别器中的病态偏见：以StyleGAN3模型为例的案例研究

    Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model

    [https://arxiv.org/abs/2402.09786](https://arxiv.org/abs/2402.09786)

    这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。

    

    生成对抗网络可以生成逼真的人脸，往往难以被人类区分出来。我们发现预训练的StyleGAN3模型中的判别器在图像和面部质量上系统地对得分进行分层，并且这不成比例地影响了不同性别、种族和其他类别的图像。我们检查了判别器在色彩和亮度方面对感知的种族和性别的偏见，然后检查了社会心理学中关于刻板印象研究中常见的偏见。

    arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
    
[^123]: 基于自驱动传感器和深度学习的人工智能应用进展

    Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning

    [https://arxiv.org/abs/2402.09442](https://arxiv.org/abs/2402.09442)

    本文介绍了基于自驱动传感器和深度学习的人工智能应用的最新进展，重点讨论了使用TENG作为自驱动传感器的优势，包括简单结构和高瞬时性能。

    

    在物联网时代，如何开发具有可持续电源供应、易于部署和灵活使用的智能传感器系统已成为一个难题。传统的电源供应存在频繁更换或使用时充电等问题，这限制了可穿戴设备的发展。通过使用聚四氟乙烯（PTFE）和铝箔（AI）制备接触-分离摩擦纳米发电机（TENG）来收集人体运动能量，根据输出电信号的变化来监测人体运动姿势。 2012年，王中林院士及其团队发明了摩擦电纳米发电机（TENG），它利用最大位移电流作为驱动力，将机械刺激直接转换为电信号，因此可以用作自驱动传感器。TENG传感器具有结构简单和瞬时性高的优点。

    arXiv:2402.09442v1 Announce Type: cross  Abstract: In the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved. The traditional power supply has problems such as frequent replacement or charging when in use, which limits the development of wearable devices. The contact-to-separate friction nanogenerator (TENG) was prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human motion energy was collected by human body arrangement, and human motion posture was monitored according to the changes of output electrical signals. In 2012, Academician Wang Zhong lin and his team invented the triboelectric nanogenerator (TENG), which uses Maxwell displacement current as a driving force to directly convert mechanical stimuli into electrical signals, so it can be used as a self-driven sensor. Teng-based sensors have the advantages of simple structure and high instant
    
[^124]: WiMANS: WiFi-based多用户活动感知的基准数据集

    WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing

    [https://arxiv.org/abs/2402.09430](https://arxiv.org/abs/2402.09430)

    WiMANS是第一个基于WiFi的多用户活动感知数据集，包含WiFi信道状态信息和同步视频，旨在促进可重复和可比较的研究

    

    WiFi-based human sensing表现出了在不侵入和无需设备的情况下分析用户行为的显着潜力，使得智能家居和医疗保健等应用受益。然而，大多数先前的工作都集中在单用户感知上，在涉及多用户场景时具有有限的实用性。尽管最近的研究已经开始探讨基于WiFi的多用户活动感知，但仍然缺乏基准数据集以促进可重复和可比较的研究。为了弥补这一空白，我们呈现了WiMANS，据我们所知，这是第一个基于WiFi的多用户活动感知数据集。WiMANS包含超过9.4小时的WiFi信道状态信息（CSI），监测多个用户在各种环境中同时进行的活动。与现有数据集相比，WiMANS不仅收集了双WiFi频段的CSI，还包括了同步视频。我们利用WiMANS来进行基准测试

    arXiv:2402.09430v1 Announce Type: cross  Abstract: WiFi-based human sensing has exhibited remarkable potential to analyze user behaviors in a non-intrusive and device-free manner, benefiting applications as diverse as smart homes and healthcare. However, most previous works focus on single-user sensing, which has limited practicability in scenarios involving multiple users. Although recent studies have begun to investigate WiFi-based multi-user activity sensing, there remains a lack of benchmark datasets to facilitate reproducible and comparable research. To bridge this gap, we present WiMANS, to our knowledge, the first dataset for multi-user activity sensing based on WiFi. WiMANS contains over 9.4 hours of WiFi Channel State Information (CSI), monitoring simultaneous activities performed by multiple users in various environments. Compared to existing datasets, WiMANS not only collects the CSI of dual WiFi bands but also includes synchronized videos. We exploit WiMANS to benchmark the
    
[^125]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^126]: IGUANe: 一种适用于脑MR图像多中心协调的三维通用CycleGAN模型

    IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images

    [https://arxiv.org/abs/2402.03227](https://arxiv.org/abs/2402.03227)

    IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。

    

    在MRI研究中，来自多个采集点的图像数据的聚合可以增加样本大小，但可能引入阻碍后续分析一致性的与采集点相关的变异。图像翻译的深度学习方法已经成为协调MR图像跨站点的解决方案。在本研究中，我们引入了IGUANe（具有统一对抗网络的图像生成），这是一种原始的三维模型，它结合了域转换的优势和直接应用样式转移方法来实现多中心脑MR图像协调。IGUANe通过多对一策略，集成了任意数量的域进行训练，扩展了CycleGAN架构。在推断过程中，该模型可以应用于任何图像，甚至来自未知采集点，使其成为协调的通用生成器。在由11台不同扫描仪的T1加权图像组成的数据集上进行训练，IGUANe在未见站点的数据上进行了评估。

    In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
    
[^127]: 一维适配器来统治它们所有：概念、扩散模型和消除应用

    One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications

    [https://arxiv.org/abs/2312.16145](https://arxiv.org/abs/2312.16145)

    基于一维结构，该研究提出了一种新的擦除框架，旨在解决现有概念消除方法存在的问题，实现非侵入性、精确性、可定制性和可转移性。

    

    由于商业和开源扩散模型（DMs）在文本到图像生成中的广泛应用，为防止不良行为，现有的概念擦除方法基于完全参数或基于规范的精细调整，观察到以下问题：1）不断侵蚀朝向的生成：目标消除过程中的参数漂移导致所有生成的变化和潜在变形，甚至在多概念消除时侵蚀其他概念，在不同程度上更加明显；2）转移能力和部署效率：之前基于模型的擦除阻碍了概念的灵活组合和训练免费转移到其他模型，导致部署场景增加时成本线性增长。为了实现无侵入、精确、可定制和可转移的消除，我们将擦除框架建立在一维基础上。

    arXiv:2312.16145v2 Announce Type: replace-cross  Abstract: The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimension
    
[^128]: 自动化方法检测自我承认的技术债务：系统文献综述

    Automated Approaches to Detect Self-Admitted Technical Debt: A Systematic Literature Review

    [https://arxiv.org/abs/2312.15020](https://arxiv.org/abs/2312.15020)

    论文提出了一种特征提取技术和ML/DL算法分类法，旨在比较和基准测试其在技术债务检测中的表现。

    

    技术债务是软件开发中普遍存在的问题，通常源自开发过程中做出的权衡，在影响软件可维护性和阻碍未来开发工作方面起到作用。自我承认的技术债务（SATD）指的是开发人员明确承认代码库中存在的代码质量或设计缺陷。自动检测SATD已经成为一个重要的研究领域，旨在帮助开发人员高效地识别和解决技术债务。然而，文献中广泛采用的NLP特征提取方法和算法种类多样化常常阻碍研究人员试图提高其性能。基于此，本系统文献综述提出了一种特征提取技术和ML/DL算法分类法，其目的是比较和基准测试所考察研究中它们的性能。我们选择......

    arXiv:2312.15020v2 Announce Type: replace-cross  Abstract: Technical debt is a pervasive issue in software development, often arising from trade-offs made during development, which can impede software maintainability and hinder future development efforts. Self-admitted technical debt (SATD) refers to instances where developers explicitly acknowledge suboptimal code quality or design flaws in the codebase. Automated detection of SATD has emerged as a critical area of research, aiming to assist developers in identifying and addressing technical debt efficiently. However, the enormous variety of feature extraction approaches of NLP and algorithms employed in the literature often hinder researchers from trying to improve their performance. In light of this, this systematic literature review proposes a taxonomy of feature extraction techniques and ML/DL algorithms used in technical debt detection: its objective is to compare and benchmark their performance in the examined studies. We select
    
[^129]: 论交替时间时态逻辑，超性质和策略共享

    On Alternating-Time Temporal Logic, Hyperproperties, and Strategy Sharing

    [https://arxiv.org/abs/2312.12403](https://arxiv.org/abs/2312.12403)

    HyperATLS$^*_S扩展了ATL$^*$，允许比较多个战略互动结果与超性质，并要求一些Agent共享相同策略，捕捉重要AI相关特性。

    

    交替时间时态逻辑（ATL$^*$）是一个用于形式推理多Agent系统的良好框架。然而，ATL$^*$可以推理Agent的战略能力（例如，某个联盟$A$可以确保最终实现目标），我们却无法比较多个战略互动，也不能要求多个Agent采用相同的策略。在本文中，我们提出了HyperATLS$^*_S，这是ATL$^*$的扩展，我们可以（1）比较多个战略互动的结果，2）强制一些Agent共享相同的策略。我们展示了HyperATL$^*_S是一个丰富的规范语言，捕捉了现有逻辑无法达到的重要AI相关特性。

    arXiv:2312.12403v2 Announce Type: replace  Abstract: Alternating-time temporal logic (ATL$^*$) is a well-established framework for formal reasoning about multi-agent systems. However, while ATL$^*$ can reason about the strategic ability of agents (e.g., some coalition $A$ can ensure that a goal is reached eventually), we cannot compare multiple strategic interactions, nor can we require multiple agents to follow the same strategy. For example, we cannot state that coalition $A$ can reach a goal sooner (or more often) than some other coalition $A'$. In this paper, we propose HyperATLS$^*_S$, an extension of ATL$^*$ in which we can (1) compare the outcome of multiple strategic interactions w.r.t. a hyperproperty, i.e., a property that refers to multiple paths at the same time, and (2) enforce that some agents share the same strategy. We show that HyperATL$^*_S$ is a rich specification language that captures important AI-related properties that were out of reach of existing logics. We pro
    
[^130]: ACPO: AI-Enabled Compiler-Driven Program Optimization

    ACPO: AI-Enabled Compiler-Driven Program Optimization

    [https://arxiv.org/abs/2312.09982](https://arxiv.org/abs/2312.09982)

    该论文提出了ACPO框架，通过机器学习模型提供给LLVM简单全面的工具，以实现编译器驱动的程序优化。

    

    该论文提出了ACPO：AI-Enabled Compiler-driven Program Optimization，这是一个新颖的框架，为LLVM提供简单全面的工具，以从应用机器学习模型来进行不同的优化通路中获益。首先展示了ACPO的高层视图、类层次结构和功能，然后通过将循环展开和函数内联传递的ML使能化，展示了ACPO的一些用例，描述了ACPO如何发挥作用。

    arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
    
[^131]: Lite-Mind: 高效稳健的脑表示网络

    Lite-Mind: Towards Efficient and Robust Brain Representation Network

    [https://arxiv.org/abs/2312.03781](https://arxiv.org/abs/2312.03781)

    Lite-Mind旨在解决fMRI解码中的挑战，通过提出一种高效稳健的脑表示网络，避免了在实践设备上为每个受试者部署特定模型的问题。

    

    通过非侵入性的fMRI方法解码大脑中的视觉信息的研究正在迅速发展。挑战在于有限的数据可用性和fMRI信号的低信噪比，导致fMRI到图像检索任务的低精度。MindEye技术通过利用高参数计数的深度MLP（每个受试者的996M MLP主干）将fMRI嵌入对齐到CLIP的视觉变换器的最终隐藏层，显着提高了fMRI到图像检索的性能。然而，即使在相同的实验设置内，受试者之间存在显着的个体差异，需要训练特定于受试者的模型。这些大量的参数在实际设备上部署fMRI解码时带来了重大挑战，特别是需要为每个受试者提供特定模型。

    arXiv:2312.03781v2 Announce Type: replace-cross  Abstract: Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind,
    
[^132]: 病理报告的多实例生成用于千亿像素全切片图像

    WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images

    [https://arxiv.org/abs/2311.16480](https://arxiv.org/abs/2311.16480)

    研究提出了一种基于多实例生成模型的方法，能够生成千亿像素全切片图像的病理报告，实验结果表明该模型能够产生包含多个临床线索的病理报告。

    

    全切片图像是用于癌症诊断和治疗的数字病理学的基础。撰写病理报告对经验不足的病理学家来说是费时且容易出错的。为了减少工作量并改善临床自动化，我们研究了如何生成给定全切片图像的病理报告。在数据端，我们整理了最大的WSI-文本数据集（TCGA-PathoText）。具体来说，我们通过识别和清理TCGA中叙述诊断幻灯片的病理报告，收集了近1万对高质量的WSI-文本配对，供视觉-语言模型使用。在模型端，我们提出了可以为千亿像素WSI生成病理报告的多实例生成模型（MI-Gen）。我们在TCGA-PathoText的最大子集上对我们的模型进行了基准测试。实验结果表明，我们的模型可以生成包含多个临床线索的病理报告。此外，WSI-文本预测可被视为一种方法。

    arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
    
[^133]: FedRA:一种用于释放异构客户端强大潜力的随机分配策略

    FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients

    [https://arxiv.org/abs/2311.11227](https://arxiv.org/abs/2311.11227)

    提出了一种名为FedRA的联邦调优算法，可以随机生成分配矩阵来应对拥有不同计算和通信资源的异构客户端，在不需要修改原模型的情况下进行微调。

    

    随着基础模型的日益可用，联邦调优在联邦学习领域引起了关注，利用多个客户端的数据和计算资源共同对基础模型进行微调。然而，在现实世界的联邦场景中，通常存在大量具有不同计算和通信资源的异构客户端，导致它们无法支持整个模型的微调过程。针对这一挑战，我们提出了一种新颖的联邦调优算法FedRA。FedRA的实施简单，可以无缝集成到任何基于Transformer的模型中，无需对原模型进行进一步修改。具体而言，在每一轮通信中，FedRA会随机生成一个分配矩阵。对于资源受限的客户端，它会根据分配情况重新组织原模型中的少量层。

    arXiv:2311.11227v2 Announce Type: replace-cross  Abstract: With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the alloc
    
[^134]: 3DCoMPaT$^{++}$：一个用于组合识别的改进型大规模三维视觉数据集

    3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition

    [https://arxiv.org/abs/2310.18511](https://arxiv.org/abs/2310.18511)

    3DCoMPaT$^{++}$提出了一个大规模的多模态2D/3D数据集，包含1.6亿个渲染视图的风格化三维形状，带有详细的部件实例级别标注，用于组合识别。

    

    在这项工作中，我们提出了3DCoMPaT$^{++}$，这是一个包含1.6亿个以上10百万个风格化三维形状的渲染视图的多模态2D/3D数据集，这些形状在部件实例级别上进行了精心注释，并配有匹配的RGB点云、3D纹理网格、深度图和分割蒙版。3DCoMPaT$^{++}$涵盖了41个形状类别、275个细粒度部分类别和293个细粒度材料类别，这些类别可以组合应用于三维物体的各部分。我们从四个等间距视图和四个随机视图中渲染了一百万个风格化形状的子集，共计1.6亿个渲染。部件在实例级别、粗粒度和细粒度语义级别上进行了分割。我们引入了一个名为Grounded CoMPaT Recognition (GCR)的新任务，旨在共同识别和基于物体部分的材料组合。另外，我们还报告了一个数据挑战活动的结果。

    arXiv:2310.18511v2 Announce Type: replace-cross  Abstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized a
    
[^135]: 揭示大语言模型知识编辑的陷阱

    Unveiling the Pitfalls of Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2310.02129](https://arxiv.org/abs/2310.02129)

    这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。

    

    随着调整大型语言模型（LLMs）成本不断上升，最近的研究工作已经转向开发编辑LLMs内在知识的方法。然而，仍有一个阴云悬在头顶上 - 知识编辑是否会触发蝴蝶效应？因为目前尚不清楚知识编辑是否会引入可能带来潜在风险的副作用。本文首次探讨了与LLMs知识编辑相关的潜在陷阱。为实现此目的，我们引入了新的基准数据集并提出了创新性的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑冲突的事实组可能会放大LLMs固有的不一致性 - 这是以前方法忽略的一个方面。（2）知识扭曲：为了编辑事实知识而更改参数可能会不可逆地扭曲

    arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
    
[^136]: 分析在MICCAI KiTS23挑战中使用额外数据时的域偏移

    Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge

    [https://arxiv.org/abs/2309.02001](https://arxiv.org/abs/2309.02001)

    使用直方图匹配来转换额外数据在处理域偏移时比简单归一化取得更好的结果

    

    使用额外的训练数据已知可以改善结果，特别是对于医学图像3D分割，在那里缺乏训练资料，模型需要在少量数据上有很好的泛化能力。然而，新数据可能是使用其他仪器获取并经过预处理，使得其分布与原始训练数据显著不同。因此，我们研究了在训练过程中改善域偏移的技术，使得额外数据可以更好地用于预处理和与原始数据一起训练。我们的结果表明，使用直方图匹配转换额外数据的效果优于简单的归一化。

    arXiv:2309.02001v2 Announce Type: replace-cross  Abstract: Using additional training data is known to improve the results, especially for medical image 3D segmentation where there is a lack of training material and the model needs to generalize well from few available data. However, the new data could have been acquired using other instruments and preprocessed such its distribution is significantly different from the original training data. Therefore, we study techniques which ameliorate domain shift during training so that the additional data becomes better usable for preprocessing and training together with the original data. Our results show that transforming the additional data using histogram matching has better results than using simple normalization.
    
[^137]: 大型语言模型中的偏见与公平性：一项调查

    Bias and Fairness in Large Language Models: A Survey

    [https://arxiv.org/abs/2309.00770](https://arxiv.org/abs/2309.00770)

    该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。

    

    大型语言模型（LLMs）的快速发展使得人们能够处理、理解和生成类似人类文本，逐渐融入触及我们社交领域的系统。然而，尽管取得成功，这些模型可能学习、延续和放大有害的社会偏见。本文对LLMs的偏见评估和缓解技术进行了全面调查。我们首先整合、形式化和扩展自然语言处理中社会偏见和公平性的概念，定义了伤害的不同方面，并引入了几个实现LLMs公平性的必要条件。然后，我们通过提出三个直观的分类体系统一了文献，其中包括两个用于偏见评估的分类体系，即指标和数据集，以及一个用于缓解的分类体系。

    arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
    
[^138]: 在LLM时代重新思考移动AI生态系统

    Rethinking Mobile AI Ecosystem in the LLM Era

    [https://arxiv.org/abs/2308.14363](https://arxiv.org/abs/2308.14363)

    重新思考移动AI生态系统，引入了一种基于协作管理的范式，通过在NPU内部放置不受应用或操作系统修订影响的基础模型，以及每个应用贡献特定的适配器，为广泛移动AI任务提供服务。

    

    在今天的背景下，智能手机已经演变成了托管多种深度学习模型的中心，旨在进行本地执行。推动这项工作的一个关键意识是这些模型之间的显著分散性，其特点是不同的架构、运算符和实现。这种分散性给硬件、系统设置和算法的全面优化带来了重大负担。在最近的大型基础模型取得重大进展的推动下，这项工作引入了一种移动AI的开创性范式：移动操作系统和硬件之间的协作管理方法，监督具有为广泛移动AI任务提供服务能力的基础模型，即使还不能为所有任务提供服务。这个基础模型驻留在NPU内部，类似于固件，不受应用或操作系统的修订的影响。同时，每个应用程序都会贡献一个简洁的、离线微调的“适配器”，用于特定的下游任务。

    arXiv:2308.14363v2 Announce Type: replace  Abstract: In today's landscape, smartphones have evolved into hubs for hosting a multitude of deep learning models aimed at local execution. A key realization driving this work is the notable fragmentation among these models, characterized by varied architectures, operators, and implementations. This fragmentation imposes a significant burden on the comprehensive optimization of hardware, system settings, and algorithms.   Buoyed by the recent strides in large foundation models, this work introduces a pioneering paradigm for mobile AI: a collaborative management approach between the mobile OS and hardware, overseeing a foundational model capable of serving a broad spectrum of mobile AI tasks, if not all. This foundational model resides within the NPU and remains impervious to app or OS revisions, akin to firmware. Concurrently, each app contributes a concise, offline fine-tuned "adapter" tailored to distinct downstream tasks. From this concept
    
[^139]: 使用区块链防御联邦学习中的恶意行为

    Defending Against Malicious Behaviors in Federated Learning with Blockchain

    [https://arxiv.org/abs/2307.00543](https://arxiv.org/abs/2307.00543)

    该研究提出了一个基于区块链和分布式分类账技术的安全和可靠的联邦学习系统，包括点对点投票机制和奖励和惩罚机制，以检测和阻止恶意行为，证明了该框架对抗恶意客户的有效性。

    

    在深度学习时代，联邦学习(FL)提供了一种有前途的方法，允许多家机构数据所有者或客户共同训练机器学习模型，而不会损害数据隐私。然而，大多数现有的FL方法依赖于用于全局模型聚合的集中式服务器，导致单点故障。这使系统在处理不诚实的客户时容易受到恶意攻击。在这项工作中，我们通过提出基于区块链和分布式分类账技术的安全可靠FL系统来解决这个问题。我们的系统结合了点对点投票机制和奖励和惩罚机制，由链上智能合约提供动力，以检测和阻止恶意行为。我们提出了理论和实证分析，以展示所提出方法的有效性，表明我们的框架对恶意客户是强大的。

    arXiv:2307.00543v2 Announce Type: replace-cross  Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-s
    
[^140]: MiniLLM：大型语言模型的知识蒸馏

    MiniLLM: Knowledge Distillation of Large Language Models

    [https://arxiv.org/abs/2306.08543](https://arxiv.org/abs/2306.08543)

    本文提出了一种将大型语言模型的知识蒸馏到更小模型的方法，通过使用反向KLD替换标准KD方法中的前向KLD目标，有效避免了学生模型高估教师分布的低概率区域。

    

    知识蒸馏（KD）是一种减少大型语言模型（LLMs）高计算需求的有前途的技术。然而，先前的KD方法主要应用于白盒分类模型或训练小模型来模仿如ChatGPT之类的黑盒模型API。如何有效地将白盒LLMs的知识蒸馏到小模型中仍未得到充分探讨，随着开源LLMs的蓬勃发展，这变得更为重要。在这项工作中，我们提出一种KD方法，将LLMs蒸馏到更小的语言模型。

    arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
    
[^141]: 数据中心人工智能（DCAI）的原则

    The Principles of Data-Centric AI (DCAI)

    [https://arxiv.org/abs/2211.14611](https://arxiv.org/abs/2211.14611)

    数据中心人工智能（DCAI）强调数据的质量和动态性，提出六项指导原则，并为人工智能系统的未来发展方向指明了道路。

    

    数据对于人工智能（AI）系统的学习至关重要。然而，迄今为止，这些系统主要以模型为中心，以数据质量为代价。数据质量问题影响了AI系统的性能，尤其是在下游部署和实际应用中。作为一个新兴概念，数据中心人工智能（DCAI）通过迭代和系统化的方法将数据、其质量和动态性置于AI系统考虑的前沿。作为首次概述之一，本文汇集了数据中心的视角和概念，勾勒了DCAI的基础。它特别为研究人员和从业者制定了六项指导原则，并为DCAI的未来发展提供了方向。

    arXiv:2211.14611v2 Announce Type: replace-cross  Abstract: Data is a crucial infrastructure to how artificial intelligence (AI) systems learn. However, these systems to date have been largely model-centric, putting a premium on the model at the expense of the data quality. Data quality issues beset the performance of AI systems, particularly in downstream deployments and in real-world applications. Data-centric AI (DCAI) as an emerging concept brings data, its quality and its dynamism to the forefront in considerations of AI systems through an iterative and systematic approach. As one of the first overviews, this article brings together data-centric perspectives and concepts to outline the foundations of DCAI. It specifically formulates six guiding principles for researchers and practitioners and gives direction for future advancement of DCAI.
    
[^142]: 利用对比样本进行少样本提示学习

    ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning

    [https://arxiv.org/abs/2211.04118](https://arxiv.org/abs/2211.04118)

    提出的Consprompt结合了提示编码网络、对比采样模块和对比评分模块，实现了差异对比学习，在不同少样本设置下表现出最先进的性能，证实了利用多级对比学习在基于提示的微调过程中的有效性。

    

    提示已成为利用预训练语言模型的有效语言工具。然而，在少样本场景中，提示设计中的细微变化总是导致结果差异很大，并且提示学习方法也很容易过拟合有限的样本。为了缓解这一问题，我们探索利用合适的对比样本和多级对比学习方法来改进提示表示的鲁棒性。因此，引入了提出的Consprompt与提示编码网络、对比采样模块和对比评分模块相结合，实现了差异对比学习。我们的结果在不同少样本设置下展现了最先进的性能，消融实验也证明了在基于提示的微调过程中利用多级对比学习的有效性。

    arXiv:2211.04118v3 Announce Type: replace-cross  Abstract: The prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes in the prompt design always make the result widely different, and the prompt learning methods also make it easy to overfit the limited samples. To alleviate this, we explore utilizing suitable contrastive samples and multi-degree contrastive learning methods to improve the robustness of the prompt representation. Therefore, the proposed Consprompt combined with the prompt encoding network, contrastive sampling modules, and contrastive scoring modules, is introduced to realize differential contrastive learning. Our results exhibit state-of-the-art performance in different few-shot settings, and the ablation experiments also certify the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process.
    
[^143]: WaveNets：小波通道注意力网络

    WaveNets: Wavelet Channel Attention Networks

    [https://arxiv.org/abs/2211.02695](https://arxiv.org/abs/2211.02695)

    WaveNets利用小波变换压缩作为通道表示的解决方案，提出了一种通道注意力机制，名为WaveNet。

    

    通道注意力作为计算机视觉领域的一种有效技术至高无上。然而，由于SENet提出的通道注意力在特征学习中存在信息损失，导致使用全局平均池化（GAP）表示通道为标量。因此，设计有效的通道注意力机制需要找到一种解决方案，以增强特征在建模通道相互依赖性中的保留。在这项工作中，我们利用小波变换压缩作为解决通道表示问题的方案。我们首先测试小波变换作为配备传统通道注意力模块的自动编码器模型。接下来，我们测试小波变换作为独立的通道压缩方法。我们证明全局平均池化等效于递归近似的Haar小波变换。有了这个证明，我们推广了使用小波压缩的通道注意力，将其命名为WaveNet。

    arXiv:2211.02695v2 Announce Type: replace-cross  Abstract: Channel Attention reigns supreme as an effective technique in the field of computer vision. However, the proposed channel attention by SENet suffers from information loss in feature learning caused by the use of Global Average Pooling (GAP) to represent channels as scalars. Thus, designing effective channel attention mechanisms requires finding a solution to enhance features preservation in modeling channel inter-dependencies. In this work, we utilize Wavelet transform compression as a solution to the channel representation problem. We first test wavelet transform as an Auto-Encoder model equipped with conventional channel attention module. Next, we test wavelet transform as a standalone channel compression method. We prove that global average pooling is equivalent to the recursive approximate Haar wavelet transform. With this proof, we generalize channel attention using Wavelet compression and name it WaveNet. Implementation o
    
[^144]: 高效的视觉扩散模型：一项调查

    Efficient Diffusion Models for Vision: A Survey

    [https://arxiv.org/abs/2210.09292](https://arxiv.org/abs/2210.09292)

    扩散模型在内容生成中表现出色，无需对抗性训练，但由于其高计算复杂性和大量计算开销，可能限制了其在现实应用中的推广。

    

    扩散模型（DMs）在内容生成方面表现出色，无需对抗性训练。这些模型使用两步训练过程。首先，前向扩散过程逐渐向数据（通常是图像）添加噪声。然后，反向扩散过程逐渐去除噪声，将其转化为被建模目标分布的样本。DMs受非平衡热力学启发，并具有天然的高计算复杂性。由于在高维空间中频繁进行函数评估和梯度计算，这些模型在训练和推理阶段都会产生相当大的计算开销。这不仅可能阻碍基于扩散的建模的民主化，也会阻碍扩散模型在现实应用中的应用。更不用说，计算模型的效率正在迅速成为一种标志。

    arXiv:2210.09292v3 Announce Type: replace-cross  Abstract: Diffusion Models (DMs) have demonstrated state-of-the-art performance in content generation without requiring adversarial training. These models are trained using a two-step process. First, a forward - diffusion - process gradually adds noise to a datum (usually an image). Then, a backward - reverse diffusion - process gradually removes the noise to turn it into a sample of the target distribution being modelled. DMs are inspired by non-equilibrium thermodynamics and have inherent high computational complexity. Due to the frequent function evaluations and gradient calculations in high-dimensional spaces, these models incur considerable computational overhead during both training and inference stages. This can not only preclude the democratization of diffusion-based modelling, but also hinder the adaption of diffusion models in real-life applications. Not to mention, the efficiency of computational models is fast becoming a sign
    
[^145]: SATformer: 基于Transformer的UNSAT核心学习

    SATformer: Transformer-Based UNSAT Core Learning

    [https://arxiv.org/abs/2209.00953](https://arxiv.org/abs/2209.00953)

    SATformer通过引入基于Transformer的方法，采用对不可满足性进行建模的方式，以识别不可满足的子问题，得到了优于NeuroSAT的性能。

    

    本文介绍了SATformer，这是一种用于布尔可满足性（SAT）问题的新型基于Transformer的方法。 SATformer并非直接解决问题，而是从相反的方向入手，着重于不可满足性。具体来说，它通过模拟子句之间的相互作用来识别任何不可满足的子问题。我们利用图神经网络将子句转换为子句嵌入，并采用分层Transformer模型来理解子句之间的相关性。 SATformer通过多任务学习方法进行训练，使用单比特可满足性结果以及最小不可满足核心（MUC）作为子句监督来处理UNSAT问题。作为端到端学习的可满足性分类器，SATformer的性能显著超越了NeuroSAT。此外，我们将SATformer做出的子句预测集成到现代启发式SAT求解器中，并验证了我们的方法。

    arXiv:2209.00953v2 Announce Type: replace  Abstract: This paper introduces SATformer, a novel Transformer-based approach for the Boolean Satisfiability (SAT) problem. Rather than solving the problem directly, SATformer approaches the problem from the opposite direction by focusing on unsatisfiability. Specifically, it models clause interactions to identify any unsatisfiable sub-problems. Using a graph neural network, we convert clauses into clause embeddings and employ a hierarchical Transformer-based model to understand clause correlation. SATformer is trained through a multi-task learning approach, using the single-bit satisfiability result and the minimal unsatisfiable core (MUC) for UNSAT problems as clause supervision. As an end-to-end learning-based satisfiability classifier, the performance of SATformer surpasses that of NeuroSAT significantly. Furthermore, we integrate the clause predictions made by SATformer into modern heuristic-based SAT solvers and validate our approach wit
    
[^146]: 从调制的角度探讨学习图像压缩中的变换

    Transformations in Learned Image Compression from a Modulation Perspective

    [https://arxiv.org/abs/2203.02158](https://arxiv.org/abs/2203.02158)

    本文提出了一种新的学习图像压缩变换方法，将LIC解释为通信系统，通过引入信号调制的方法，实现了各种变换方法的数学简化和扩展，并在实验中验证了其有效性和稳健性。

    

    本文从调制的角度提出了一种统一的学习图像压缩(LIC)变换方法。首先，将LIC中的量化视为带有加性均匀噪声的广义信道。此外，根据结构和优化目标的一致性，将LIC解释为一种特殊的通信系统。因此，可以应用通信系统技术来指导LIC中模块的设计。此外，定义了一种基于信号调制的统一变换方法（TSM）。从TSM的观点来看，现有的变换方法在数学上被简化为线性调制。通过扩展到非线性调制，可以得到一系列变换方法，如TPM和TJM。在各种数据集和骨干架构上的实验结果验证了该方法的有效性和稳健性。

    arXiv:2203.02158v3 Announce Type: replace-cross  Abstract: In this paper, a unified transformation method in learned image compression(LIC) is proposed from the perspective of modulation. Firstly, the quantization in LIC is considered as a generalized channel with additive uniform noise. Moreover, the LIC is interpreted as a particular communication system according to the consistency in structures and optimization objectives. Thus, the technology of communication systems can be applied to guide the design of modules in LIC. Furthermore, a unified transform method based on signal modulation (TSM) is defined. In the view of TSM, the existing transformation methods are mathematically reduced to a linear modulation. A series of transformation methods, e.g. TPM and TJM, are obtained by extending to nonlinear modulation. The experimental results on various datasets and backbone architectures verify that the effectiveness and robustness of the proposed method. More importantly, it further co
    
[^147]: 使用切比雪夫逼近的图卷积神经网络，重新审视

    Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited

    [https://arxiv.org/abs/2202.03580](https://arxiv.org/abs/2202.03580)

    重新审视了使用切比雪夫多项式逼近谱图卷积的问题，发现ChebNet性能较差主要是由于其学习到的非法系数近似解析滤波器函数

    

    在图学习中，设计谱卷积网络是一个具有挑战性的问题。ChebNet是早期尝试之一，它使用切比雪夫多项式近似谱图卷积。GCN简化了ChebNet，仅利用前两个切比雪夫多项式，同时在真实世界数据集上性能优于其。GPR-GNN和BernNet表明，单项式和伯恩斯坦基也在学习谱图卷积方面优于切比雪夫基。这样的结论在逼近理论领域是反直觉的，逼近函数时切比雪夫多项式实现了最佳收敛速率。

    arXiv:2202.03580v5 Announce Type: replace-cross  Abstract: Designing spectral convolutional networks is a challenging problem in graph learning. ChebNet, one of the early attempts, approximates the spectral graph convolutions using Chebyshev polynomials. GCN simplifies ChebNet by utilizing only the first two Chebyshev polynomials while still outperforming it on real-world datasets. GPR-GNN and BernNet demonstrate that the Monomial and Bernstein bases also outperform the Chebyshev basis in terms of learning the spectral graph convolutions. Such conclusions are counter-intuitive in the field of approximation theory, where it is established that the Chebyshev polynomial achieves the optimum convergent rate for approximating a function.   In this paper, we revisit the problem of approximating the spectral graph convolutions with Chebyshev polynomials. We show that ChebNet's inferior performance is primarily due to illegal coefficients learnt by ChebNet approximating analytic filter functio
    
[^148]: 评估增加人工智能透明度的方法论：一项案例研究

    Evaluating a Methodology for Increasing AI Transparency: A Case Study

    [https://arxiv.org/abs/2201.13224](https://arxiv.org/abs/2201.13224)

    该论文评估了一个针对人工智能透明度增加的方法论，首次将该用户中心方法论应用于实践，报告了在医疗人工智能领域团队的经验。

    

    针对人工智能（AI）潜在危害的增长关切，社会已开始要求更多关于AI模型和系统创建和使用方式的透明度。为解决这些问题，一些努力提出了包含模型开发者需要回答的问题的文档模板。这些模板提供了一个有用的起点，但是没有单一模板可以涵盖各种文档使用者的需求。然而，原则上可以创建一种可重复的方法论来生成真正有用的文档。Richards等人[25]提出了这样一种方法，用于识别具体的文档需求，并创建模板来满足这些需求。虽然这是一个有前途的提议，但尚未进行评估。 本文首次在实践中评估了这种以用户为中心的方法论，并报告了一个在医疗人工智能领域团队的经验。

    arXiv:2201.13224v2 Announce Type: replace-cross  Abstract: In reaction to growing concerns about the potential harms of artificial intelligence (AI), societies have begun to demand more transparency about how AI models and systems are created and used. To address these concerns, several efforts have proposed documentation templates containing questions to be answered by model developers. These templates provide a useful starting point, but no single template can cover the needs of diverse documentation consumers. It is possible in principle, however, to create a repeatable methodology to generate truly useful documentation. Richards et al. [25] proposed such a methodology for identifying specific documentation needs and creating templates to address those needs. Although this is a promising proposal, it has not been evaluated.   This paper presents the first evaluation of this user-centered methodology in practice, reporting on the experiences of a team in the domain of AI for healthca
    
[^149]: 一种用于在高维观察数据中稳健评估治疗效果的两阶段特征选择方法

    A Two-Stage Feature Selection Approach for Robust Evaluation of Treatment Effects in High-Dimensional Observational Data

    [https://arxiv.org/abs/2111.13800](https://arxiv.org/abs/2111.13800)

    提出了一种名为Outcome Adaptive Elastic Net（OAENet）的两阶段特征选择技术，用于在高维观察数据中进行稳健的因果推断决策。

    

    随机对照试验（RCT）被认为是评估任何干预或治疗效果的黄金标准。然而，由于伦理、经济和法律考虑，其可行性经常受阻，使得观察数据成为绘制因果结论的宝贵替代方法。然而，由于医疗观察数据具有高维性，这带来了困难挑战，需要仔细考虑以确保无偏、可靠和稳健的因果推断。为了克服这一挑战，本研究提出了一种名为Outcome Adaptive Elastic Net（OAENet）的新颖两阶段特征选择技术，专门设计用于使用匹配技术做出稳健的因果推断决策。OAENet相对于现有方法具有几个关键优势：在相关和高维数据上表现优越于现有方法，并具有选择特定变量集的能力。

    arXiv:2111.13800v2 Announce Type: replace-cross  Abstract: A Randomized Control Trial (RCT) is considered as the gold standard for evaluating the effect of any intervention or treatment. However, its feasibility is often hindered by ethical, economical, and legal considerations, making observational data a valuable alternative for drawing causal conclusions. Nevertheless, healthcare observational data presents a difficult challenge due to its high dimensionality, requiring careful consideration to ensure unbiased, reliable, and robust causal inferences. To overcome this challenge, in this study, we propose a novel two-stage feature selection technique called, Outcome Adaptive Elastic Net (OAENet), explicitly designed for making robust causal inference decisions using matching techniques. OAENet offers several key advantages over existing methods: superior performance on correlated and high-dimensional data compared to the existing methods and the ability to select specific sets of vari
    
[^150]: 深度DNA存储：基于编码理论和深度学习的可扩展和稳健DNA存储

    Deep DNA Storage: Scalable and Robust DNA Storage via Coding Theory and Deep Learning

    [https://arxiv.org/abs/2109.00031](https://arxiv.org/abs/2109.00031)

    通过将基于深度神经网络训练的编码理论和深度学习应用于DNA存储，我们提出了一种可扩展且稳健的DNA存储解决方案，将速度提高了最高3200倍，准确性提高了40%。

    

    DNA存储是一种新兴技术，可以将数字信息存档在DNA分子中。这种方法相对于磁性和光学存储方案具有主要优势，如出色的信息密度、增强的数据耐久性以及维持数据完整性所需的可忽略功耗。为了访问数据，我们使用了信息检索过程，其中一些主要瓶颈是可扩展性和准确性之间的自然权衡。在这里，我们展示了一种模块化和整体性方法，将基于深度神经网络（DNN）在模拟数据上训练、基于张量积（TP）的纠错码（ECC），以及一个安全裕度机制结合到一个统一的流水线中。我们使用两种不同的测序技术展示了我们的解决方案对3.1MB 信息的提升。我们的工作在速度上比当前领先解决方案提高了最高3200倍，准确性提高了40%。

    arXiv:2109.00031v3 Announce Type: replace-cross  Abstract: DNA-based storage is an emerging technology that enables digital information to be archived in DNA molecules. This method enjoys major advantages over magnetic and optical storage solutions such as exceptional information density, enhanced data durability, and negligible power consumption to maintain data integrity. To access the data, an information retrieval process is employed, where some of the main bottlenecks are the scalability and accuracy, which have a natural tradeoff between the two. Here we show a modular and holistic approach that combines Deep Neural Networks (DNN) trained on simulated data, Tensor-Product (TP) based Error-Correcting Codes (ECC), and a safety margin mechanism into a single coherent pipeline. We demonstrated our solution on 3.1MB of information using two different sequencing technologies. Our work improves upon the current leading solutions by up to x3200 increase in speed, 40% improvement in accur
    
[^151]: CSC-Unet：一种基于卷积稀疏编码策略的神经网络用于语义分割

    CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural Network for Semantic Segmentation

    [https://arxiv.org/abs/2108.00408](https://arxiv.org/abs/2108.00408)

    通过将常用的卷积操作重新构造为多层卷积稀疏编码块，提出了一种可能用于显著提高语义分割模型性能的新策略。

    

    准确执行语义分割是一项具有挑战性的任务，这是由于真实图像场景的复杂性。许多基于传统深度学习的语义分割方法未能充分捕捉图像的语义和外观信息，这限制了它们对各种应用场景的普适性和鲁棒性。在本文中，我们提出了一种新颖的策略，将常用的卷积操作重新构造为多层卷积稀疏编码块，以缓解前述缺陷。该策略可能用于显著提高任何涉及卷积操作的语义分割模型的分割性能。为了证明我们想法的有效性，我们选择了广泛使用的U-Net模型进行演示，并基于U-Net设计了CSC-Unet模型系列。通过广泛的分析和实验证据，我们提供了可信的证据显示

    arXiv:2108.00408v2 Announce Type: replace-cross  Abstract: It is a challenging task to accurately perform semantic segmentation due to the complexity of real picture scenes. Many semantic segmentation methods based on traditional deep learning insufficiently captured the semantic and appearance information of images, which put limit on their generality and robustness for various application scenes. In this paper, we proposed a novel strategy that reformulated the popularly-used convolution operation to multi-layer convolutional sparse coding block to ease the aforementioned deficiency. This strategy can be possibly used to significantly improve the segmentation performance of any semantic segmentation model that involves convolutional operations. To prove the effectiveness of our idea, we chose the widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet model series based on U-Net. Through extensive analysis and experiments, we provided credible evidence showing
    
[^152]: 理解并避免人工智能失败：实用指南

    Understanding and Avoiding AI Failures: A Practical Guide

    [https://arxiv.org/abs/2104.12582](https://arxiv.org/abs/2104.12582)

    基于正常事故理论、高可靠性理论和开放系统理论，结合人工智能安全原则，建立了一个框架来理解当前人工智能应用的风险，强调了关注当前一代人工智能系统安全性的重要性。

    

    随着人工智能技术在能力和普及程度上的增加，人工智能事故正在变得越来越常见。基于正常事故理论、高可靠性理论和开放系统理论，我们建立了一个框架来理解与人工智能应用相关的风险。此外，我们还运用人工智能安全原则来量化增加智能和人类特质在人工智能中带来的独特风险。两个领域共同描绘了当代人工智能的风险更完整的图景。通过聚焦事故附近的系统属性而不是寻找事故的根本原因，我们确定了应该关注当前一代人工智能系统安全性的位置。

    arXiv:2104.12582v4 Announce Type: replace-cross  Abstract: As AI technologies increase in capability and ubiquity, AI accidents are becoming more common. Based on normal accident theory, high reliability theory, and open systems theory, we create a framework for understanding the risks associated with AI applications. In addition, we also use AI safety principles to quantify the unique risks of increased intelligence and human-like qualities in AI. Together, these two fields give a more complete picture of the risks of contemporary AI. By focusing on system properties near accidents instead of seeking a root cause of accidents, we identify where attention should be paid to safety for current generation AI systems.
    
[^153]: 通过检索示范进行上下文学习的语言模型：一项综述

    In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11624](http://arxiv.org/abs/2401.11624)

    本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。

    

    语言模型，特别是预训练的大型语言模型，已展示出卓越的能力，可以在输入上下文中进行少量样本的情境学习（ICL），并在新任务上具有适应能力。然而，模型的ICL能力对于少样本示范的选择是敏感的。最近的一项研究进展是检索针对每个输入查询定制的示范。示范检索的实现相对简单，利用现有的数据库和检索系统。这不仅提高了学习过程的效率和可扩展性，而且已经证明可以减少手动示例选择中的偏见。鉴于令人鼓舞的结果和在检索示范的ICL方面不断增长的研究，我们进行了广泛的研究综述。在这项综述中，我们讨论和比较了检索模型的不同设计选择，检索训练

    Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
    
[^154]: CivRealm: 一个在文明游戏中的学习与推理奇幻之旅

    CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents. (arXiv:2401.10568v1 [cs.AI])

    [http://arxiv.org/abs/2401.10568](http://arxiv.org/abs/2401.10568)

    CivRealm是一个受文明游戏启发的环境，要求代理人在复杂的情境中进行学习和推理，以应对变化的游戏规则和随机环境。

    

    决策代理的泛化包括两个基本元素：从过去经验中学习和在新情境中进行推理。然而，在大多数交互环境中，对学习的重视往往以牺牲推理复杂性为代价。在本文中，我们介绍了CivRealm，一个受到文明游戏启发的环境。文明游戏与人类历史和社会的深刻契合需要复杂的学习，而其不断变化的情境则要求强大的推理能力。特别是，CivRealm建立了一个有着不完全信息和变化人数的总和游戏；它呈现了众多复杂的特征，挑战代理人处理开放的、随机的环境，需要外交和谈判技巧。在CivRealm中，我们为两种典型的代理类型提供了接口：基于张量的学习型代理和基于语言的推理型代理。

    The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization's profound alignment with human history and society necessitates sophisticated learning, while its ever-changing situations demand strong reasoning to generalize. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further rese
    
[^155]: 一种安全的个性化偏好学习方法及其在自动驾驶汽车中的应用

    A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles. (arXiv:2311.02099v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.02099](http://arxiv.org/abs/2311.02099)

    本研究介绍了一种安全的个性化偏好学习方法，应用于自动驾驶汽车。该方法利用信号时态逻辑公式的优先顺序进行学习，并提出了一种解决这个学习问题的方法。通过对比较，我们找到了适合的权重估计，使得首选信号的加权满足度高于非首选信号。在人体试验中证明了该方法的有效性。

    

    本研究介绍了一种偏好学习方法，确保符合给定规范，并应用于自动驾驶汽车。我们的方法将描述交通规则的信号时态逻辑(STL)公式的优先顺序纳入学习框架中。通过利用参数加权信号时态逻辑(PWSTL)，我们基于成对比较提出了一种解决这个学习问题的方法，该方法确保了安全保证的偏好学习。我们的方法找到了给定PWSTL公式权重的可行估计，使得使用这些权重时，首选信号的加权定量满足度大于非首选信号。我们的方法得到的权重的可行估计导致了一个加权STL公式，可以用于正确性和定制合成控制器。我们通过两个不同的模拟驾驶场景进行了一项人体试验，验证了我们方法的性能。

    This work introduces a preference learning method that ensures adherence to given specifications, with an application to autonomous vehicles. Our approach incorporates the priority ordering of Signal Temporal Logic (STL) formulas describing traffic rules into a learning framework. By leveraging Parametric Weighted Signal Temporal Logic (PWSTL), we formulate the problem of safety-guaranteed preference learning based on pairwise comparisons and propose an approach to solve this learning problem. Our approach finds a feasible valuation for the weights of the given PWSTL formula such that, with these weights, preferred signals have weighted quantitative satisfaction measures greater than their non-preferred counterparts. The feasible valuation of weights given by our approach leads to a weighted STL formula that can be used in correct-and-custom-by-construction controller synthesis. We demonstrate the performance of our method with a pilot human subject study in two different simulated dri
    
[^156]: 退后一步：通过抽象唤起大型语言模型的推理能力

    Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])

    [http://arxiv.org/abs/2310.06117](http://arxiv.org/abs/2310.06117)

    本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。

    

    我们提出了一种称为“退后提示”的简单提示技术，使得大型语言模型能够通过从包含具体细节的实例中进行抽象，得出高层概念和基本原理。利用这些概念和原理来指导推理步骤，语言模型在正确推理路径上显著提升了能力。我们使用PaLM-2L模型进行了退后提示实验，在包括STEM、知识问答和多跳推理在内的各种具有挑战性的推理密集型任务上观察到了明显的性能提升。例如，在MMLU物理和化学任务上，退后提示可以将PaLM-2L的性能提升7%和11%，在TimeQA任务上提升27%，在MuSiQue任务上提升7%。

    We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
    
[^157]: 基于递归组合多粒度表示的Transformer增强模型

    Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])

    [http://arxiv.org/abs/2309.16319](http://arxiv.org/abs/2309.16319)

    ReCAT是一种增强的Transformer模型，使用递归组合和上下文内外层能够模拟文本的层级句法结构，并生成与其他跨度上下文相关的多粒度表示。

    

    我们提出了一种名为ReCAT的递归组合增强Transformer模型，它能够在学习和推理过程中明确建模原始文本的层级句法结构，而无需依赖于黄金树。现有研究限制数据遵循层级树结构，因此缺乏跨距通信。为了克服这个问题，我们提出了一种新颖的上下文内外(CIO)层，通过自底向上和自顶向下的传递学习跨度的上下文化表示，其中自底向上传递通过组合低级跨度形成高级跨度的表示，而自顶向下传递则结合了跨度内部和外部的信息。通过在嵌入层和注意力层之间叠加多个CIO层，ReCAT模型可以进行跨距内部和跨距间的深层交互，从而生成与其他跨度完全上下文化的多粒度表示。此外，CIO层可以进行联合预训练。

    We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained
    
[^158]: MINT: 评估在与工具和语言反馈进行多轮交互中的LLMs的能力

    MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10691](http://arxiv.org/abs/2309.10691)

    MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。

    

    为了解决复杂任务，大语言模型（LLMs）通常需要与用户进行多轮交互，有时候辅以外部工具的帮助。然而，当前的评估协议常常强调用单轮交流的基准性能，忽略了用户、LLMs和外部工具之间的细致互动，并低估了用户的自然语言反馈的重要性。这些疏忽导致了研究基准评估结果与实际应用情况之间的差异。我们引入了MINT，这是一个通过使用工具和利用用户的自然语言反馈来评估LLMs解决多轮交互任务能力的基准。为了保证可重复性，我们提供了一个评估框架，在这个框架中，LLMs可以通过执行Python代码来访问工具，并接收由GPT-4模拟的用户的自然语言反馈。我们重新利用了一系列多样的已建立评估数据集，重点关注推理、编码和决策方面。

    To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
    
[^159]: 受物理启发的神经图ODE用于长期动力学模拟

    Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])

    [http://arxiv.org/abs/2308.13212](http://arxiv.org/abs/2308.13212)

    提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。

    

    模拟和建模多对象物理系统的长期动态是一项重要且具有挑战性的任务。目前的研究利用具有等变性质的图神经网络(GNNs)对物理系统进行建模。具体而言，他们将动力学建模为一系列具有固定时间间隔的离散状态，并学习所有相邻状态之间的直接映射。然而，这种直接映射忽略了两个状态之间的连续性。换句话说，我们已经验证了在当前基于GNN的直接映射模型中，两个离散动态状态之间存在无数可能的轨迹。这个问题极大地阻碍了模型的泛化能力，导致长期模拟的性能较差。在本文中，为了更好地通过离散监督信号建模潜在轨迹，我们提出了一个受物理启发的神经图ODE(PINGO)算法。

    Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
    
[^160]: 基于大型语言模型的自主代理的调查

    A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])

    [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)

    该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。

    

    自主代理长期以来一直是学术界的研究热点。以往的研究往往集中在对有限知识的代理进行训练，而这与人类的学习过程存在明显差异，因此很难实现人类般的决策。近年来，通过获取大量的网络知识，大型语言模型（LLM）展现出了实现人类水平智能的显著潜力。这引发了对基于LLM的自主代理的研究的高涨兴趣。为了发挥LLM的全部潜力，研究人员设计了各种不同应用的代理体系结构。本论文综述了这些研究，从整体的角度对自主代理领域进行了系统的审查。具体而言，我们的重点是基于LLM的代理构建，为此我们提出了一个统一的框架。

    Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
    
[^161]: 深度学习在不同数据类型隐写分析中的应用：综述

    Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])

    [http://arxiv.org/abs/2308.04522](http://arxiv.org/abs/2308.04522)

    本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。

    

    隐写术和隐写分析是信息安全领域的两个相关方面。隐写术旨在隐藏通信，而隐写分析则旨在找到这些隐藏信息，甚至尝试恢复其所包含的数据。隐写术和隐写分析引起了广泛的关注，特别受到执法部门的关注。隐写术常被网络犯罪分子甚至恐怖分子用来避免在拥有证据时被捕，即使加密也一样，因为在许多国家禁止或限制使用密码学。因此，了解揭示隐藏信息的尖端技术对揭露非法行为至关重要。在过去几年中，文献中引入了许多强大可靠的隐写术和隐写分析技术。本综述论文提供了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的全面概述。

    Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
    
[^162]: 通过融合多空间深度模型的脑电信号来估计心理负荷

    Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models. (arXiv:2308.02409v1 [eess.SP])

    [http://arxiv.org/abs/2308.02409](http://arxiv.org/abs/2308.02409)

    该论文通过融合多个空间维度的方法，使用脑电信号对心理负荷进行分类和估计连续级别。在时间域中使用了时态卷积网络，而在频率域中引入了新的架构——多维残差块。

    

    人脑在工作和休息时都处于持续活动的状态。心理活动是日常过程中的一部分，当大脑过度劳累时，会对人体健康产生负面影响。近年来，人们对于早期检测心理健康问题的重视逐渐增加，因为这可以帮助预防严重的健康问题，并改善生活质量。多种信号被用于评估心理状态，但由于大量提供关于大脑信息的特点，脑电图（EEG）被研究人员广泛使用。本文旨在将心理负荷分为三种状态并估计连续级别。我们的方法通过融合多个空间维度来实现最佳的心理估计结果。在时间域方法中，我们使用了时态卷积网络，而在频率域上，我们提出了一种名为多维残差块的新架构，它结合了残差块。

    The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
    
[^163]: 全卷积轻量级图像超分辨率网络

    Fully $1\times1$ Convolutional Network for Lightweight Image Super-Resolution. (arXiv:2307.16140v1 [cs.CV])

    [http://arxiv.org/abs/2307.16140](http://arxiv.org/abs/2307.16140)

    本论文提出了一种全卷积轻量级图像超分辨率网络，通过融合$3\times3$和$1\times1$卷积核的优点，以及引入无参数的空间平移操作，在保持计算效率的同时提高了网络的表示能力。

    

    深度模型在单图像超分辨率任务中取得了显著的进展，特别是使用了大卷积核（$3\times3$或更大）的大型模型。然而，这些模型的高计算复杂度限制了其在实时、资源受限的环境中的部署。相反，$1\times1$卷积具有较高的计算效率，但在聚合局部空间表示方面表现不佳，这是SISR模型的一个重要能力。为了解决这种对立，我们提出了融合$3\times3$和$1\times1$卷积核优点的轻量级SISR网络，名为Shift-Conv-based Network (SCNet)。通过引入一个无参数的空间平移操作，在保持计算效率的同时，赋予了全卷积轻量级网络强大的表示能力。大量实验证明了网络的有效性。

    Deep models have achieved significant process on single image super-resolution (SISR) tasks, in particular large models with large kernel ($3\times3$ or more). However, the heavy computational footprint of such models prevents their deployment in real-time, resource-constrained environments. Conversely, $1\times1$ convolutions bring substantial computational efficiency, but struggle with aggregating local spatial representations, an essential capability to SISR models. In response to this dichotomy, we propose to harmonize the merits of both $3\times3$ and $1\times1$ kernels, and exploit a great potential for lightweight SISR tasks. Specifically, we propose a simple yet effective fully $1\times1$ convolutional network, named Shift-Conv-based Network (SCNet). By incorporating a parameter-free spatial-shift operation, it equips the fully $1\times1$ convolutional network with powerful representation capability while impressive computational efficiency. Extensive experiments demonstrate th
    
[^164]: 过度思考真相：理解语言模型如何处理虚假演示

    Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])

    [http://arxiv.org/abs/2307.09476](http://arxiv.org/abs/2307.09476)

    该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。

    

    现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。

    Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
    
[^165]: TNPAR: 基于拓扑神经泊松自回归模型的事件序列Granger因果结构学习

    TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])

    [http://arxiv.org/abs/2306.14114](http://arxiv.org/abs/2306.14114)

    该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题

    

    从事件序列中学习Granger因果关系是各种应用中具有挑战性但又至关重要的任务。大多数现有方法都依赖于事件序列独立同分布 (i.i.d.) 的假设。然而，由于事件序列之间的固有依赖关系，这一 i.i.d. 假设经常被违反。幸运的是，在实践中，我们发现这些依赖关系可以被建模成一个拓扑网络，因此可以通过将先验拓扑网络引入Granger因果发现来解决非 i.i.d. 问题。这一发现促使我们解决两个问题：1) 如何在模型事件序列时同时考虑先验拓扑网络和潜在的Granger因果结构；2) 如何学习Granger因果结构。为此，我们设计了一个两阶段的统一拓扑神经泊松自回归模型。在生成阶段，我们采用神经泊松过程的一种变体来建模事件发生的时刻，并通过拓扑关系和现有事件序列推断因果关系。

    Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
    
[^166]: 在具有部分在线状态信息的强化学习中，POMDP的理论难度和可计算性

    Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08762](http://arxiv.org/abs/2306.08762)

    本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。

    

    部分可观察的马尔可夫决策过程（POMDP）被广泛应用于捕捉许多现实世界的应用。然而，现有的理论结果已经表明，在一般的POMDP中学习可能是不可计算的，主要挑战在于缺乏潜在的状态信息。一个关键的基本问题是有多少在线状态信息（OSI）足以实现可计算性。在本文中，我们建立了一个下界，揭示了一个惊人的难度结果：除非我们具有完整的OSI，否则我们需要指数级的采样复杂度才能获得POMDP的$\epsilon$-最优策略解。尽管如此，受到我们下界设计的关键见解的启发，我们发现即使只有部分OSI，也存在重要的可计算的POMDP类别。特别地，对于具有部分OSI的两个新颖的POMDP类别，我们通过建立新的遗憾上下界证明了新的算法是接近最优的。

    Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
    
[^167]: SGAT4PASS：面向球面几何意识的全景语义分割Transformer

    SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation. (arXiv:2306.03403v1 [cs.CV])

    [http://arxiv.org/abs/2306.03403](http://arxiv.org/abs/2306.03403)

    本论文提出了SGAT4PASS，一种面向球面几何意识的全景语义分割Transformer，通过加入球面几何感知的约束，能更好地捕捉全景图像的3D属性，从而提高分割性能。

    

    作为计算机视觉中一个重要且具有挑战性的问题，全景语义分割可以根据超广角观察到的完整场景来进行感知。传统的针对2D全景图像的PASS方法侧重于解决图像畸变问题，但缺乏对原始360°数据的3D属性的考虑。因此，当输入具有3D扰动的全景图像时，它们的性能会大幅下降。为了更好地应对3D扰动，我们提出了一种面向球面几何意识的全景语义分割Transformer，即SGAT4PASS。具体来说，我们提出了一个球面几何意识的分割框架，它包括三个模块，即球面几何感知图像投影，球面可形变补丁嵌入和全景感知损失，它对具有3D扰动的输入图像进行处理，并对已有的可形变补丁嵌入加入了球面几何感知的约束。

    As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates
    
[^168]: 序列建模是离线强化学习的一个强有力的竞争者。

    Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])

    [http://arxiv.org/abs/2305.14550](http://arxiv.org/abs/2305.14550)

    序列建模是离线强化学习中比Q-Learning和Imitation Learning更适合在稀疏奖励和低质量数据设置下的选择，在任务范围增加时，序列建模和模仿学习更可取。

    

    离线强化学习使代理能够从静态数据集中学习有效的最大化收益策略。离线RL的三大范式是Q-Learning、Imitation Learning和Sequence Modeling。一个关键的问题是：在什么条件下，哪种范式被优先选择？我们通过探索代表性算法——保守Q-Learning(CQL)、行为克隆 (BC)和决策Transformer (DT)——在常用的D4RL和Robomimic基准测试中的表现来对这个问题进行了实证研究。我们设计了有针对性的实验来理解它们在数据子优性和任务复杂性方面的行为。我们的主要发现是：(1)序列建模需要比Q-Learning更多的数据来学习竞争性策略，但更加稳健；(2)序列建模在稀疏奖励和低质量数据设置中比Q-Learning和Imitation Learning都要好得多；(3)随着任务范围的增加，序列建模和模仿学习更可取。

    Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
    
[^169]: 语言特定的情绪概念知识表示对情绪推断的因果支持

    Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09582](http://arxiv.org/abs/2302.09582)

    本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。

    

    在情绪科学中，如何理解语言支持情绪推断仍然是一个争议的话题。本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，调查了语言是否会因果支持情绪推断。使用提示技术，发现了14个情绪概念的属性由不同的人工神经元群体表示。通过操纵这些属性相关的神经元，与随机操纵相比，大多数情绪推断任务的表现出现了下降。属性特定的表现下降与人类心理空间中不同属性的重要性有关。我们的发现提供了支持基于语言的情绪推断机制的因果证据，并强调了情绪概念知识的贡献。

    Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
    

