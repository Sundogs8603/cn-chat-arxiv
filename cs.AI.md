# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play.](http://arxiv.org/abs/2303.12076) | T-Dex是一种基于触觉的灵巧性方法，可在自我监督式的触觉编码器和一些灵巧性任务演示的指导下，将触觉和视觉结合起来，相比于传统的纯视觉方法更有效，并在现实世界中具有更好的应用性。 |
| [^2] | [Roots and Requirements for Collaborative AI.](http://arxiv.org/abs/2303.12040) | 论文探讨了AI协同合作的历史和要求，是协同AI研究的动机和背景。 |
| [^3] | [The Representational Status of Deep Learning Models.](http://arxiv.org/abs/2303.12032) | 该论文澄清了深度学习模型的表征状态。尽管通常称为“表征”，但实际上它们更适合理解为高度理想化的模型，这一结果对可解释的AI有着直接影响，也引起了哲学家对其在未来科学研究中的作用的关注。 |
| [^4] | [cTBL: Augmenting Large Language Models for Conversational Tables.](http://arxiv.org/abs/2303.12024) | 本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。 |
| [^5] | [Logical Reasoning over Natural Language as Knowledge Representation: A Survey.](http://arxiv.org/abs/2303.12023) | 本文总结了一种新的逻辑推理方法，它使用自然语言作为知识表示，具有不同于端到端神经方法的优势。这种新模式在未来有着很高的潜力。 |
| [^6] | [Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity.](http://arxiv.org/abs/2303.12003) | 该研究对比了六个生成式人工智能聊天机器人和人类生成的创意，发现有9.4％的人类比最有创造力的GPT-4更有创造力。研究表明，生成式人工智能在创造性过程中是有价值的助手。 |
| [^7] | [Deep trip generation with graph neural networks for bike sharing system expansion.](http://arxiv.org/abs/2303.11977) | 本文提出了一种使用图神经网络进行共享单车系统扩展的深度出行生成方法，用于预测基于多源城市建筑和地理数据的站点需求。 |
| [^8] | [The Power of Nudging: Exploring Three Interventions for Metacognitive Skills Instruction across Intelligent Tutoring Systems.](http://arxiv.org/abs/2303.11965) | 本研究探究了涉及推动、示例和提出三种干预方法教授默认学生何时如何使用哪种策略，研究结果表明Nudge表现最佳。 |
| [^9] | [Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention.](http://arxiv.org/abs/2303.11945) | 本文提出一种无监督的、基于对比学习和交叉注意力机制的跨领域谣言检测模型。该模型不仅可以进行跨领域特征对齐，还可以强制目标样本与源域中的相应原型对齐，并使用聚类方法产生伪标签来学习域不变表示，能够显著提高跨领域谣言检测的性能。 |
| [^10] | [Representational Tenets for Memory Athletics.](http://arxiv.org/abs/2303.11944) | 该论文总结了世界级记忆竞技比赛的现状和方法，并探讨了这些方法的意识学理论视角。研究者提出了一组实验，以帮助更好地了解专家记忆表现的边界。 |
| [^11] | [Sparse Distributed Memory is a Continual Learner.](http://arxiv.org/abs/2303.11934) | 该论文提出了一个使用稀疏分布式内存的修改多层感知器（MLP）模型，它是一个强大的持续学习者，并且该方法不需要记忆重放或任务信息。这是一种训练稀疏网络的新方法，具有广泛的适用性。 |
| [^12] | [Do intermediate feature coalitions aid explainability of black-box models?.](http://arxiv.org/abs/2303.11920) | 本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。 |
| [^13] | [Deephys: Deep Electrophysiology, Debugging Neural Networks under Distribution Shifts.](http://arxiv.org/abs/2303.11912) | 本文介绍了一种名为Deephys的可视化和理解DNN在超出分布范围的场景中失败的工具，使用神经电生理学的概念，通过比较内分布和外分布数据集中的神经活动，无缝分析单个神经元、单个图像和类别图像集，并能揭示假特征和新特征存在导致的失败。 |
| [^14] | [Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network.](http://arxiv.org/abs/2303.11899) | 本文提出了一种新的训练框架 RegionLight，基于交叉口之间的邻接关系将智能体分配到每个区域中。同时，研究人员扩展了BDQ方法为DBDQ，以限制联合动作空间大小的增长并缓解智能体训练问题。 |
| [^15] | [Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving.](http://arxiv.org/abs/2303.11888) | 本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。 |
| [^16] | [Better Understanding Differences in Attribution Methods via Systematic Evaluations.](http://arxiv.org/abs/2303.11884) | 本文提出了三种新的评估方案，通过这些方案，可以更可靠地测量归因方法的可信度。 |
| [^17] | [Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control.](http://arxiv.org/abs/2303.11860) | 该论文采用了滑动窗口注意机制来替代Transformer中的自注意机制，从而使得使用逐个元素地处理序列，更加适用于在线信号处理，并且在手指位置回归数据集上实现了最好的准确性纪录，每个推理步骤仅需要非常短的时间窗口(3.5毫秒)。 |
| [^18] | [Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs.](http://arxiv.org/abs/2303.11858) | 本文介绍了一种新的查询嵌入方法RoConE，它允许学习关系模式并提高了逻辑查询推理的性能。 |
| [^19] | [LoRCoN-LO: Long-term Recurrent Convolutional Network-based LiDAR Odometry.](http://arxiv.org/abs/2303.11853) | 提出了一种基于长短期循环卷积网络的激光雷达里程计估计方法LoRCoN-LO。通过使用CNN和LSTM层处理空间和时间信息，利用点云实现对机器人连续运动的预测。在公共数据集(KITTI)上表现出了精确的里程计预测结果。 |
| [^20] | [Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation.](http://arxiv.org/abs/2303.11848) | Dens-PU是一种基于密度的正样本增强的PU Learning方法，可以用于解决二元分类问题，且在基准图像数据集上表现出最先进的结果。 |
| [^21] | [Addressing Class Variable Imbalance in Federated Semi-supervised Learning.](http://arxiv.org/abs/2303.11809) | 本文介绍了一种称为联邦半监督学习与类变量不平衡（FSSL-CVI）的新方法，它使用动态类别加权方案来处理FSSL中的类别变量不平衡问题，并且在多个数据集上进行了实验验证。通过实验结果，本文表明 FSSL-CVI 方法在各方面性能上优于现有的联邦学习和FSSL 方法。 |
| [^22] | [Lightweight Contrastive Protein Structure-Sequence Transformation.](http://arxiv.org/abs/2303.11783) | 该论文提出了一种新的无监督学习的蛋白质结构表示预训练方法，使用强大的蛋白质语言模型和自监督结构约束，避免了破坏真实的空间结构表示和标记数据的限制。 |
| [^23] | [Poisoning Attacks in Federated Edge Learning for Digital Twin 6G-enabled IoTs: An Anticipatory Study.](http://arxiv.org/abs/2303.11745) | 本文研究了面向数字孪生6G物联网的联邦边缘学习中的毒化攻击的预测性研究，发现攻击者可以在集中式学习和联邦学习中进行攻击，并且成功的攻击会降低模型的准确性。 |
| [^24] | [Unlocking Layer-wise Relevance Propagation for Autoencoders.](http://arxiv.org/abs/2303.11734) | 本文提出了一种解释自编码器的快速解决方案，通过将深度泰勒分解框架与逐层相关传播方法相结合。结果显示，该方法计算和质量方面具有比现有方法更好的优势。 |
| [^25] | [Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices.](http://arxiv.org/abs/2303.11730) | 该论文提出了一种新的代数机器推理框架，能够高效解决Raven渐进矩阵问题，最高准确率可达93.8％。 |
| [^26] | [Online Learning of Wheel Odometry Correction for Mobile Robots with Attention-based Neural Network.](http://arxiv.org/abs/2303.11725) | 本文提出了一种基于注意力神经网络的在线学习方法，用于处理移动机器人轮式里程校正问题，并取得了优异的结果，避免了耗时的数据收集过程，具有良好的实时性。 |
| [^27] | [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?.](http://arxiv.org/abs/2303.11717) | 本文通过完整的调研介绍了生成型AI，包含了从技术到应用的各个方面。ChatGPT虽然是一个有用的工具，但并不足以覆盖所有的AIGC任务，对于实现多样化的内容创造还需要GPT-5等未来的发展。 |
| [^28] | [Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples).](http://arxiv.org/abs/2303.11712) | 本文提出对于约束满足问题，采用一种高效的算法以易于理解的方式解释解决方案，这一方法可以让人类更好地理解机器所做的决策。 |
| [^29] | [Linking generative semi-supervised learning and generative open-set recognition.](http://arxiv.org/abs/2303.11702) | 本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。 |
| [^30] | [A Survey on Class Imbalance in Federated Learning.](http://arxiv.org/abs/2303.11673) | 本文对联邦学习中的类别不平衡问题和解决方案进行了调查和总结，包括平衡感知优化、数据增强和客户端加权等。目前仍存在着一些问题需要解决。 |
| [^31] | [Manipulating Transfer Learning for Property Inference.](http://arxiv.org/abs/2303.11643) | 本文研究了转移学习中的属性推断攻击，攻击者可以操纵上游模型，对受害者调整的下游模型进行高效且特定的推断攻击，需要注意和防范此类攻击。 |
| [^32] | [BoxSnake: Polygonal Instance Segmentation with Box Supervision.](http://arxiv.org/abs/2303.11630) | BoxSnake是一种新的端到端训练技术，可以仅使用框注释实现有效的多边形实例分割，相较于基于掩膜的弱监督方法，BoxSnake显示出显着的优越性。 |
| [^33] | [Assessor-Guided Learning for Continual Environments.](http://arxiv.org/abs/2303.11624) | 本文提出了一种评估者指导学习策略，用于持续学习，其中评估者通过控制学习方向和速度来指导学习过程，以有效地学习新环境并避免灾难性干扰问题的发生。 |
| [^34] | [Heterogeneous-Branch Collaborative Learning for Dialogue Generation.](http://arxiv.org/abs/2303.11621) | 本文提出了一种异构分支协作学习模型，用于对话生成。该模型使用协作学习方法而非传统的知识蒸馏方法，在网络分支的训练中考虑到对话属性，使不同分支的特征多样化。 |
| [^35] | [Low-complexity Deep Video Compression with A Distributed Coding Architecture.](http://arxiv.org/abs/2303.11599) | 本论文提出了一种低复杂度深度视频压缩的分布式编码结构，通过使用一个有效的译码器辅助信息生成模块，实现在设备资源受限的情况下仍能够有效地利用视频帧间的相关性进行高效压缩。 |
| [^36] | [A Review on Machine Theory of Mind.](http://arxiv.org/abs/2303.11594) | 本文回顾了机器心智理论在信念、欲望和意图方面的最新进展，比较了优缺点和适用条件，希望指导研究人员跟上该领域的最新趋势。 |
| [^37] | [Large AI Models in Health Informatics: Applications, Challenges, and the Future.](http://arxiv.org/abs/2303.11568) | 大型AI模型在健康信息学领域具有突破性应用，但其规模和数据量的挑战需要克服，未来仍需深入探索。 |
| [^38] | [Agave crop segmentation and maturity classification with deep learning data-centric strategies using very high-resolution satellite imagery.](http://arxiv.org/abs/2303.11564) | 本研究利用高分辨率卫星图像采用深度学习数据策略解决了龙舌兰作物分割中的问题，并提出了龙舌兰作物成熟度分类方法。 |
| [^39] | [Dynamic Healthcare Embeddings for Improving Patient Care.](http://arxiv.org/abs/2303.11563) | DECENT是一种自动编码异构共同演化的动态神经网络，可以从各种数据流中学习患者、医生、房间和药物的异构动态嵌入，用于改善患者护理，如药物推荐、患者风险分层和医院容量管理。 |
| [^40] | [Fix the Noise: Disentangling Source Feature for Controllable Domain Translation.](http://arxiv.org/abs/2303.11545) | 本文提出了一种新的可控领域翻译方法，通过在目标特征空间的已分解子空间中保留源特征，使得只使用单个模型就能平滑地控制保留源特征的程度，产生更一致、更逼真的图像。 |
| [^41] | [Indeterminate Probability Neural Network.](http://arxiv.org/abs/2303.11536) | 本文提出了一种新型通用模型——不定概率神经网络；它可以进行无监督聚类和使用很小的神经网络处理大规模分类，其理论优势体现在新的概率理论和神经网络框架中。 |
| [^42] | [AI-in-the-Loop -- The impact of HMI in AI-based Application.](http://arxiv.org/abs/2303.11508) | 人机交互(HMI)被加入到AI架构设计过程中可以有效减少AI开发所需的资源，避免训练和评估具有非生产层的AI架构，从而导致轻量级AI架构的产生。 |
| [^43] | [Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking.](http://arxiv.org/abs/2303.11470) | 提出了一种基于背门数字水印的方法，以确保公共数据的安全。通过在数据集中插入极少量的数字水印样本，隐式学习一个隐藏的函数作为数字水印，以跟踪非法使用此数据集的模型。使用“清洁标签背门”方法实现了数字水印，不会破坏原始数据集。实验证明，该方法有效地检测到非法利用数据集的行为。 |
| [^44] | [Mind meets machine: Unravelling GPT-4's cognitive psychology.](http://arxiv.org/abs/2303.11436) | 本研究评估了在广泛使用的CommonsenseQA数据集中的一套常识推理问题上，GPT-4的表现及其对常识知识的处理和整合过程，在此过程中我们也发现了其局限性。 |
| [^45] | [Improving Human-Robot Collaboration via Computational Design.](http://arxiv.org/abs/2303.11425) | 本论文通过厨房设计为例，研究合理的共享空间设计如何提高人机协作能力，使用分散式运动规划器有效解决多智能体运动规划问题，结果表明优化的厨房设计可以明显提高人机协作的性能。 |
| [^46] | [Improving EEG-based Emotion Recognition by Fusing Time-frequency And Spatial Representations.](http://arxiv.org/abs/2303.11421) | 论文提出了一种基于跨域特征融合方法的EEG信号分类网络，其中融合了时频域和空间域的多重表示方法，该方法在EEG情感识别中取得了最先进的结果。 |
| [^47] | [ADCNet: End-to-end perception with raw radar ADC data.](http://arxiv.org/abs/2303.11420) | 本文提出了一种在原始雷达模拟数字（ADC）数据上执行端到端学习的方法，其中一个可学习的信号处理模块被嵌入网络中，实验结果证实了该方法的有效性。 |
| [^48] | [Vibration Signal Denoising Using Deep Learning.](http://arxiv.org/abs/2303.11413) | 本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。 |
| [^49] | [GNN-Ensemble: Towards Random Decision Graph Neural Networks.](http://arxiv.org/abs/2303.11376) | 本文提出了一种名为GNN-Ensemble的方法，它可以构建随机决策图神经网络的集合，以提高GNNs的性能，泛化能力和抗攻击性，并遵循随机建模的原则。 |
| [^50] | [Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement.](http://arxiv.org/abs/2303.11373) | 该研究提出了一种神经网络约束满足的方法，通过层级结构的抽象实现了在组合状态下的物体重新排列任务的泛化能力。 |
| [^51] | [Reflexion: an autonomous agent with dynamic memory and self-reflection.](http://arxiv.org/abs/2303.11366) | 本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。 |
| [^52] | [HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals.](http://arxiv.org/abs/2303.11340) | 本研究提出了一种新的基于高维Transformer的架构HDformer，并利用长距离PPG信号进行糖尿病检测，其中提出了一种新的注意力模块TSA，成功将标记体积减少10倍以上，提高了模型的能力和效率。 |
| [^53] | [FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder.](http://arxiv.org/abs/2303.11339) | 本文提出了一个新的联邦自监督学习框架 FedMAE，可以利用轻量级设备上的大规模未标记图像进行联邦学习。FedMAE可以预训练一个单块遮蔽自编码器，并将多个预训练的单块MAE级联在服务器上构建用于下游任务的多块ViT骨干。实验结果表明，FedMAE相较于最先进的FSSL方法具有卓越的性能。 |
| [^54] | [Recursive Euclidean Distance Based Robust Aggregation Technique For Federated Learning.](http://arxiv.org/abs/2303.11337) | 本文提出了一种递归欧几里得距离计算的鲁棒聚合方法来防御联邦学习中的恶意攻击，该方法分配权重以最小化数据污染效应，实验表明其精度优于现有算法并且时间复杂度降低。 |
| [^55] | [Rotating without Seeing: Towards In-hand Dexterity through Touch.](http://arxiv.org/abs/2303.10880) | 本研究提出了一种新系统Touch Dexterity，通过密集二进制力传感器实现了多指机器人手不看就能旋转物体，同时大大降低了成本和与实际应用的差距。 |
| [^56] | [RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network.](http://arxiv.org/abs/2303.10770) | 本论文提出一种基于储备节点的神经元视觉感知网络（RN-Net）。RN-Net可以以低成本有效地处理异步的时间特征，并在DVS128手势上实现了迄今最高的准确率99.2％，在更小的网络尺寸下实现了DVS Lip数据集的67.5％准确率。 |
| [^57] | [Multi-modal reward for visual relationships-based image captioning.](http://arxiv.org/abs/2303.10766) | 本文提出了一种基于融合图像场景图的视觉关系信息与图像的空间特征映射的深度神经网络，引入多模态奖励函数进行深度强化学习，以优化图像字幕生成任务。实验结果表明，该方法在客观和主观评估指标上均优于基准。 |
| [^58] | [Path Planning for Autonomous Driving: The State of the Art and Perspectives.](http://arxiv.org/abs/2303.09824) | 本文综述了现有的自动驾驶路径规划方法，包括管道规划和端到端规划方法。在挑战和潜在解决方案方面提供了讨论，有助于为智能汽车的发展提供更好的规划方法。 |
| [^59] | [Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries.](http://arxiv.org/abs/2303.09280) | 该论文介绍了一种基于物理知识神经网络的拓扑优化方法，应用于无先验知识的几何结构检测，通过材料密度场表示任意解决方案拓扑，并通过Eikonal正则化实现。该方法可用于医疗和工业应用中的非侵入式成像技术。 |
| [^60] | [Maximum Margin Learning of t-SPNs for Cell Classification with Filtering.](http://arxiv.org/abs/2303.09065) | 本研究提出了一种基于t-SPN算法和滤波技术的细胞分类方法，通过最大化边缘和L2正则化，该方法在HEp-2和Feulgen基准数据集上取得了最高的准确率。 |
| [^61] | [Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection.](http://arxiv.org/abs/2303.09026) | 本文提出了一种通识知识辅助的细粒度目标检测方法，利用通识知识推理模块处理由基准深度学习检测器给出的粗粒度标签，从而提高目标检测的准确性。经过实验验证，该方法相比于现有方法需要更少的计算量和标注资源。 |
| [^62] | [Exploring the Relevance of Data Privacy-Enhancing Technologies for AI Governance Use Cases.](http://arxiv.org/abs/2303.08956) | 探究数据隐私增强技术对AI治理的重要性，将不同的AI治理目标视为系统信息流，强调解决方案之间的互操作性。 |
| [^63] | [WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis.](http://arxiv.org/abs/2303.07543) | 本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。 |
| [^64] | [Fast exploration and learning of latent graphs with aliased observations.](http://arxiv.org/abs/2303.07397) | 本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。 |
| [^65] | [Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation.](http://arxiv.org/abs/2303.06152) | 本文演示了如何使用通用函数表示语言和框架来表示特定对象及其参与支持其设计的过程，从而实现深入的概念理解，可解释性的功能，使系统能够回答“为什么”问题。 |
| [^66] | [CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment.](http://arxiv.org/abs/2303.05725) | CVT-SLR是一种新的手语识别模型，它采用基于对比视觉-文本变换和变分对齐的方法来充分利用跨模态知识，为解决手语识别中缺乏大规模可用数据集的问题提供了一种新的解决方案。 |
| [^67] | [ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?.](http://arxiv.org/abs/2303.05382) | 本文探讨了ChatGPT在解决交通问题方面的应用。通过利用具有跨模态编码器的LLM，可以处理来自不同模态的交通数据并执行交通运营。作者提供了一个基于智能手机的碰撞报告自动生成和分析框架作为用例展示了这种潜力。 |
| [^68] | [QAID: Question Answering Inspired Few-shot Intent Detection.](http://arxiv.org/abs/2303.01593) | 本文提出了一个启发式的Few-shot意图检测方法，通过将意图检测重新定义为一个问题-回答检索任务来解决语义相似的细粒度意图问题，结果在三个few-shot意图检测基准测试上取得了最优表现。 |
| [^69] | [Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption.](http://arxiv.org/abs/2303.01254) | 本研究介绍了一种基于全同态加密的数据隐私保护方法，能够针对加密表格数据进行任意计算，并得到了最新的解决方案，适用于一系列树型模型，包括决策树，随机森林和梯度增强树。此方法已应用在Concrete-ML开源库中，能够在准确性方面接近未受保护的版本。 |
| [^70] | [Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning.](http://arxiv.org/abs/2302.14115) | 本文介绍了 Vid2Seq，这是一个在大规模 narrated 视频数据集上预先训练的多模态密集事件字幕模型。通过将转录语音的句子边界转化为伪事件边界，并使用转录语音句子作为伪事件字幕，我们有效利用未标注 narrated 视频数据集进行了密集视频字幕的训练。该模型在多个基准测试中表现出色，是目前最优秀的模型之一。 |
| [^71] | [Explainable AI does not provide the explanations end-users are asking for.](http://arxiv.org/abs/2302.11577) | 可解释性人工智能对提高人工智能系统信任度的效果有局限性，透明度和严格的验证更适合打造可靠的人工智能系统。 |
| [^72] | [DNG: Taxonomy Expansion by Exploring the Intrinsic Directed Structure on Non-gaussian Space.](http://arxiv.org/abs/2302.11165) | 本文提出了一种基于非高斯空间内在有向结构的分类学扩展算法DNG，通过明确表示每个节点的继承特征和增量特征的组合来挖掘结构信息，并成功捕捉节点之间is-a关系的方向，效果优于现有方法。 |
| [^73] | [Liveness score-based regression neural networks for face anti-spoofing.](http://arxiv.org/abs/2302.09461) | 本文提出了一种新的基于活体得分的方法来克服对第三方网络和用户的依赖，并使用了一种新的标签技术来产生表示与真实图像相关的信息量的离散化标签。大量实验表明，该方法优于以前的方法。 |
| [^74] | [GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis.](http://arxiv.org/abs/2302.08722) | 本文提出 GPT4MIA 方法，利用 GPT-3 作为插入式检验工具进行医学图像分析；该方法在提示结构设计、样本选择及提示排序等方面优化，能有效提高预测准确性。 |
| [^75] | [Adap-$\tau$: Adaptively Modulating Embedding Magnitude for Recommendation.](http://arxiv.org/abs/2302.04775) | 本研究提出了一种自适应归一化方案Adap-$\tau$，通过动态调节每个用户-每个物品对的嵌入幅度，实现了理想的推荐性能，方法在四个真实世界的数据集上都超过了基准方法。 |
| [^76] | [Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction.](http://arxiv.org/abs/2302.02601) | 本文提出了一种基于双层知识图谱的方法来学习嵌入，将三元组之间的关系考虑进去，并使用数据增强策略来增加合理的三元组。 |
| [^77] | [Diversity Induced Environment Design via Self-Play.](http://arxiv.org/abs/2302.02119) | 本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。 |
| [^78] | [Risk-Sensitive Reinforcement Learning with Exponential Criteria.](http://arxiv.org/abs/2212.09010) | 本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。 |
| [^79] | [Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture.](http://arxiv.org/abs/2212.08189) | 本文提出了一种基于逐渐增加子集数量的分区序列的通用的分层学习结构，并使用无梯度随机逼近更新进行在线解决优化问题的方法，可以定义函数逼近问题并使用双时间尺度随机逼近算法的理论解决，模拟了一种退火过程。 |
| [^80] | [Policy Adaptation from Foundation Model Feedback.](http://arxiv.org/abs/2212.07398) | 本文提出了基于基础模型反馈的策略适应（PAFF）方法，通过让策略使用随机生成的指令进行演示，并利用预训练的基础模型提供反馈来重新标记演示，自动提供新的演示-指令数据对进行策略微调，以实现机器人操作的泛化。实验结果表明，PAFF优于现有最先进的方法。 |
| [^81] | [Fine-Grained Selective Similarity Integration for Drug-Target Interaction Prediction.](http://arxiv.org/abs/2212.00543) | 本文提出了一种细粒度选择性相似性集成方法（FGS）来提高药物靶点相互作用预测的效果，采取局部的方法来精细选择性集成药物和靶点相似性，并自适应地学习不同相似性视图的权重。在多个基准数据集上的实验结果表明，FGS优于现有的最先进的相似性集成方法进行DTI预测。 |
| [^82] | [Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand.](http://arxiv.org/abs/2211.14983) | 本论文提出了一种多智能体强化学习的学习框架，用于在城市地图上服务于随机出现的请求，可以产生协调作用并考虑先前可能出现的未来请求，能够适应不同需求分布的变化。 |
| [^83] | [Collecting Interactive Multi-modal Datasets for Grounded Language Understanding.](http://arxiv.org/abs/2211.06552) | 本文提出了一个通过自然语言任务与协作体验智能体交互收集数据集的方法，并收集了首个交互基础语言理解数据集。 |
| [^84] | [Neural Co-Processors for Restoring Brain Function: Results from a Cortical Model of Grasping.](http://arxiv.org/abs/2210.11478) | 本文提出了一种神经协处理器，利用人工神经网络和深度学习学习最优闭环刺激策略，实现了针对性修复和康复。并利用皮层模型的模拟，为神经协处理器的未来体内测试奠定基础。 |
| [^85] | [MixMask: Revisiting Masking Strategy for Siamese ConvNets.](http://arxiv.org/abs/2210.11456) | 本文提出了一种新的填充式遮盖策略MixMask，在Siamese ConvNets中实现遮盖和对比学习目标的匹配，提高了Siamese ConvNets的性能并在多个基准测试中实现了最先进的结果。 |
| [^86] | [Mitigating Covertly Unsafe Text within Natural Language Systems.](http://arxiv.org/abs/2210.09306) | 本文讨论了智能技术中日益普遍的文本安全问题，并强调了一个被忽视的类别：隐蔽不安全文本。该文提出了缓解策略以解决这一问题，以提高智能系统内部的安全性。 |
| [^87] | [Computational Choreography using Human Motion Synthesis.](http://arxiv.org/abs/2210.04366) | 本文介绍了一种利用深度学习模型分析舞蹈动作和生成新动作序列的方法，同时也结合了前人的努力来开发出一套系统。 |
| [^88] | [$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of Features.](http://arxiv.org/abs/2207.02625) | 本文提出了一种$L_2$BN方法，通过等化样本特征的$L_2$范数来增强批量归一化，可以增强内类别特征的紧凑性并扩大跨类别特征的差异，易于实现，可以用作神经网络的基本归一化方法。 |
| [^89] | [Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework.](http://arxiv.org/abs/2207.01955) | 本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。 |
| [^90] | [From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms.](http://arxiv.org/abs/2206.09090) | 这篇论文介绍了一种基于智能重启机制的分布估计算法，该算法可以在基因漂变风险高的情况下停止运行，并寻找良好的参数范围以运行EDA，从而提高性能。 |
| [^91] | [An Abstract View on Optimizations in Propositional Frameworks.](http://arxiv.org/abs/2206.06440) | 本文提出了一个统一的权重系统框架，消除了命题框架中不同优化语句之间的语法差异，具有重要的简化和解释潜力。 |
| [^92] | [GAMR: A Guided Attention Model for (visual) Reasoning.](http://arxiv.org/abs/2206.04928) | 本文介绍了一个新的模块——GAMR，它是一种用于(视觉)推理的引导式注意力模型，以动态选择任务相关的视觉信息并将其路由到记忆中来解决复杂的视觉推理任务，并在各种任务和数据集上取得了成功的实验结果。 |
| [^93] | [MolScribe: Robust Molecular Structure Recognition with Image-To-Graph Generation.](http://arxiv.org/abs/2205.14311) | MolScribe是一种强健的分子结构识别模型，它可以将分子图像转换成图形结构，包括原子和键及其几何布局，并且能够识别手性和扩展缩写结构。该模型在公共基准测试中的准确性为76-93％，并可通过置信度估计和原子级对齐进行验证。 |
| [^94] | [DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data.](http://arxiv.org/abs/2205.00701) | DeepGraviLens是一种多模态神经网络，用于分类属于不同类型的引力透镜数据，具有高精度和优于现有方法的结果。 |
| [^95] | [Pre-trained Token-replaced Detection Model as Few-shot Learner.](http://arxiv.org/abs/2203.03235) | 本文提出了一种使用预训练的token-replaced检测模型的少样本学习器方法，将任务重新定义为token-replaced检测问题，能够优于使用预训练的遮蔽语言模型的少样本学习器。 |
| [^96] | [Holistic Deep Learning.](http://arxiv.org/abs/2110.15829) | 本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。 |
| [^97] | [Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles.](http://arxiv.org/abs/1703.01347) | 本论文研究了具有噪声特征的上下文线性Bandit问题。我们提出了一个算法，通过观察信息，实现了贝叶斯神谕并得到了$\tilde{O}(d\sqrt{T})$的遗憾界。 |

# 详细

[^1]: 触觉训练下的机器人灵巧性

    Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play. (arXiv:2303.12076v1 [cs.RO])

    [http://arxiv.org/abs/2303.12076](http://arxiv.org/abs/2303.12076)

    T-Dex是一种基于触觉的灵巧性方法，可在自我监督式的触觉编码器和一些灵巧性任务演示的指导下，将触觉和视觉结合起来，相比于传统的纯视觉方法更有效，并在现实世界中具有更好的应用性。

    

    在机器人领域，让具有多指的机器人具有灵巧性一直是一个长期的挑战。此前，最重要的工作都集中在学习控制器或策略上，这些控制器或策略要么基于视觉观测，要么基于从视觉推断得到的状态估计。然而，这些方法在需要推理接触力或通过手本身遮挡的物体的细粒度操作任务上表现不佳。本研究提出了一种新的基于触觉的灵巧性方法T-Dex，该方法分为两个阶段：在第一阶段，收集2.5小时的游戏数据并使用这些数据训练自我监督型触觉编码器；在第二阶段，利用少量的灵巧性任务演示，学习将触觉观测和视觉观测相结合的非参数化策略。通过五个具有挑战性的灵巧性任务，我们展示了我们的基于触觉的灵巧性模型比纯视觉方法更有效，并且能够推广到现实世界中视觉方法失败的情况。

    Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vi
    
[^2]: 协同人工智能的根源和要求

    Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])

    [http://arxiv.org/abs/2303.12040](http://arxiv.org/abs/2303.12040)

    论文探讨了AI协同合作的历史和要求，是协同AI研究的动机和背景。

    

    AI协作者的愿景长期以来一直是科幻小说的经典素材，其中人工智能代理理解协作和人类沟通的微妙差别。它们通过贡献特殊的才能给他们的人类合作者和团队带来优势。多年来，政府咨询团体和人工智能领域的领袖一直倡导AIs应该具有人类兼容性和有效协作的能力。然而，具备像才华横溢的人那样协作能力的强大的AI仍然遥不可及。这篇论文依据对人工智能和人类代理有效和强大协作所需认知的分析，概述了公众和AI愿景中关于人工协作者的历史，开始于早期智能增强(IA)和人工智能(AI)的愿景。这篇论文旨在成为协同AI的第二个立场文件(Stefik & Price, 2023)的动机和背景。第二篇论文回顾了多学科的现状，并提出了一个AI协作研究的路线图。

    The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik & Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
    
[^3]: 深度学习模型的表征状态

    The Representational Status of Deep Learning Models. (arXiv:2303.12032v1 [cs.AI])

    [http://arxiv.org/abs/2303.12032](http://arxiv.org/abs/2303.12032)

    该论文澄清了深度学习模型的表征状态。尽管通常称为“表征”，但实际上它们更适合理解为高度理想化的模型，这一结果对可解释的AI有着直接影响，也引起了哲学家对其在未来科学研究中的作用的关注。

    

    本文旨在澄清深度学习模型（DLMs）的表征状态。由于功能和关系概念的混淆，尽管通常称为“表征”，但这意味着含糊不清。本文认为，虽然DLM以关系意义上的表征其目标，但最好理解为高度理想化的模型。这个结果对可解释的AI（XAI）有直接影响，并引导哲学关注DLM表征的理想化性质及其在未来科学研究中的作用。

    This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, they are best understood as highly idealized models. This result has immediate implications for explainable AI (XAI) and directs philosophical attention toward examining the idealized nature of DLM representations and their role in future scientific investigation.
    
[^4]: cTBL：增强大型语言模型用于对话表格

    cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])

    [http://arxiv.org/abs/2303.12024](http://arxiv.org/abs/2303.12024)

    本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。

    

    多模态对话人工智能中一个开放的挑战是如何从文本和非文本来源中增强大型语言模型以进行多轮对话。为了解决这个问题，本文引入了Conversation Table (cTBL)，这是一种三步编码器-解码器方法，用于检索表格信息并生成基于检索信息的对话响应。cTBL使用转换器编码器嵌入进行浓密表检索，并在HyrbiDialogue数据集Top-1和Top-3准确性上相对于稀疏检索提高了最多5%。此外，cTBL使用编码器和解码器模型进行表格知识检索，在HyrbiDialogue上产生了最高46%的ROUGE分数相对改进，并实现了更好的人工评估响应生成。

    An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
    
[^5]: 自然语言作为知识表示的逻辑推理研究：综述

    Logical Reasoning over Natural Language as Knowledge Representation: A Survey. (arXiv:2303.12023v1 [cs.CL])

    [http://arxiv.org/abs/2303.12023](http://arxiv.org/abs/2303.12023)

    本文总结了一种新的逻辑推理方法，它使用自然语言作为知识表示，具有不同于端到端神经方法的优势。这种新模式在未来有着很高的潜力。

    

    逻辑推理是人类认知和智能的核心。以往的人工智能中的逻辑推理研究使用形式化语言作为知识表示（和符号推理器）。然而，使用形式化语言进行推理证明具有困难（例如脆弱性和知识获取瓶颈）。本文总结了一种新的逻辑推理方法的综合概述，它使用自然语言作为知识表示（以及预训练语言模型作为推理器），包括逻辑推理的哲学定义和分类，新模式的优势、基准和方法，未来需要的任务和方法以及与相关 NLP 领域的关系。这种新模式是很有前途的，因为它不仅可以缓解形式化表示的许多挑战，而且也具有优于端到端神经方法的优势。

    Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation~(and symbolic reasoners). However, reasoning with formal language has proved challenging~(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation~(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks & methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.
    
[^6]: 人工缪斯：生成式人工智能聊天机器人已达到人类创造力水平。

    Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity. (arXiv:2303.12003v1 [cs.AI])

    [http://arxiv.org/abs/2303.12003](http://arxiv.org/abs/2303.12003)

    该研究对比了六个生成式人工智能聊天机器人和人类生成的创意，发现有9.4％的人类比最有创造力的GPT-4更有创造力。研究表明，生成式人工智能在创造性过程中是有价值的助手。

    

    人们普遍认为人工智能不具备创造力。我们通过比较人类生成的创意与六个生成式人工智能聊天机器人（Alpa.ai，Copy.ai，ChatGPT（版本3和4），Studio.ai和YouChat）生成的创意来测试这一假设。人类和一种经过特别训练的人工智能独立评估了创意的质量和数量。我们发现，人工智能生成的创意在质量上与人类生成的创意没有定性差异，尽管它们在创意生成方式上存在差异。有趣的是，9.4％的人类比最有创造力的GAI（GPT-4）更有创造力。我们的研究表明，生成式人工智能在创造性过程中是有价值的助手。继续研究和开发生成式人工智能在创意任务中的能力对于充分了解该技术在塑造创意未来的潜在益处和缺陷至关重要。最后，我们讨论了生成式人工智能是否能够真正具有创造力的问题。

    A widespread view is that Artificial Intelligence cannot be creative. We tested this assumption by comparing human-generated ideas with those generated by six Generative Artificial Intelligence (GAI) chatbots: alpa.ai, Copy.ai, ChatGPT (versions 3 and 4), Studio.ai, and YouChat. Humans and a specifically trained AI independently assessed the quality and quantity of ideas. We found no qualitative difference between AI and human-generated creativity, although there are differences in how ideas are generated. Interestingly, 9.4 percent of humans were more creative than the most creative GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the creative process. Continued research and development of GAI in creative tasks is crucial to fully understand this technology's potential benefits and drawbacks in shaping the future of creativity. Finally, we discuss the question of whether GAIs are capable of being truly creative.
    
[^7]: 使用图神经网络进行共享单车系统扩展的深度出行生成

    Deep trip generation with graph neural networks for bike sharing system expansion. (arXiv:2303.11977v1 [cs.LG])

    [http://arxiv.org/abs/2303.11977](http://arxiv.org/abs/2303.11977)

    本文提出了一种使用图神经网络进行共享单车系统扩展的深度出行生成方法，用于预测基于多源城市建筑和地理数据的站点需求。

    

    共享单车正在全球范围内作为一种活跃，方便和可持续的交通方式而兴起。为了计划成功的共享单车系统（BSS），许多城市从小规模试点开始，并逐步扩大系统覆盖更多区域。对于基于站点的BSS，这意味着随着时间的推移基于现有站点规划新站点，这需要预测整个系统中这些新站点产生的旅行次数。先前的研究通常依赖于相对简单的回归或机器学习模型，在捕捉复杂空间关系方面受到限制。尽管在旅行需求预测方面，深度学习方法的文献越来越多，但它们大多是基于时序数据的短期预测，假设系统没有结构性变化。在本研究中，我们重点研究了BSS扩展的出行生成问题，并提出了一种基于图神经网络（GNN）的方法来预测基于多源城市建筑和地理数据的站点需求。

    Bike sharing is emerging globally as an active, convenient, and sustainable mode of transportation. To plan successful bike-sharing systems (BSSs), many cities start from a small-scale pilot and gradually expand the system to cover more areas. For station-based BSSs, this means planning new stations based on existing ones over time, which requires prediction of the number of trips generated by these new stations across the whole system. Previous studies typically rely on relatively simple regression or machine learning models, which are limited in capturing complex spatial relationships. Despite the growing literature in deep learning methods for travel demand prediction, they are mostly developed for short-term prediction based on time series data, assuming no structural changes to the system. In this study, we focus on the trip generation problem for BSS expansion, and propose a graph neural network (GNN) approach to predicting the station-level demand based on multi-source urban bui
    
[^8]: “推动力”：智能辅导系统中元认知技能教学的三种干预方法的探究

    The Power of Nudging: Exploring Three Interventions for Metacognitive Skills Instruction across Intelligent Tutoring Systems. (arXiv:2303.11965v1 [cs.HC])

    [http://arxiv.org/abs/2303.11965](http://arxiv.org/abs/2303.11965)

    本研究探究了涉及推动、示例和提出三种干预方法教授默认学生何时如何使用哪种策略，研究结果表明Nudge表现最佳。

    

    认证域通常具有很多认知技能，其中没有一种单一的解决策略可适用于解决所有问题。研究表明，知道何时如何使用每种策略的学生（StrTime）表现优于那些谁不知道并坚持默认策略（Default）的学生。在这项工作中，学生在逻辑辅导员的训练中使用默认的前向链接和后向链接（BC）策略，然后在概率辅导员中只使用BC支持的策略。我们研究了三种在逻辑辅导员上教授默认学生何时如何使用哪种策略的干预措施：示例、推动和提出。同时，StrTime学生没有接受任何干预措施。总的来说，我们的结果表明，Nudge的表现优于其Default同伴，并在两个辅导员上追赶StrTime。

    Deductive domains are typical of many cognitive skills in that no single problem-solving strategy is always optimal for solving all problems. It was shown that students who know how and when to use each strategy (StrTime) outperformed those who know neither and stick to the default strategy (Default). In this work, students were trained on a logic tutor that supports a default forward-chaining and a backward-chaining (BC) strategy, then a probability tutor that only supports BC. We investigated three types of interventions on teaching the Default students how and when to use which strategy on the logic tutor: Example, Nudge and Presented. Meanwhile, StrTime students received no interventions. Overall, our results show that Nudge outperformed their Default peers and caught up with StrTime on both tutors.
    
[^9]: 无监督跨领域谣言检测：基于对比学习和交叉注意力的方法

    Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention. (arXiv:2303.11945v1 [cs.SI])

    [http://arxiv.org/abs/2303.11945](http://arxiv.org/abs/2303.11945)

    本文提出一种无监督的、基于对比学习和交叉注意力机制的跨领域谣言检测模型。该模型不仅可以进行跨领域特征对齐，还可以强制目标样本与源域中的相应原型对齐，并使用聚类方法产生伪标签来学习域不变表示，能够显著提高跨领域谣言检测的性能。

    

    大规模谣言通常伴随着突发新闻或热门话题而出现，严重阻碍真相的查证。现有的谣言检测方法大多专注于相同领域，因此在跨领域情况下表现不佳。本文提出了一种端到端的基于实例和原型的，带有交叉注意力机制的对比学习模型，用于跨领域谣言检测。该模型不仅可以进行跨领域特征对齐，还可以强制目标样本与给定源域的相应原型对齐。由于目标域中的目标标签不可用，因此我们使用一种聚类的方法，并通过一批源域样本的仔细初始化中心来产生伪标签。此外，我们使用交叉注意力机制处理具有相同标签的一对源数据和目标数据，以学习域不变表示。由于领域对中的样本倾向于表达相似的语义模式，因此这种方法能够提高模型的检测性能。

    Massive rumors usually appear along with breaking news or trending topics, seriously hindering the truth. Existing rumor detection methods are mostly focused on the same domain, and thus have poor performance in cross-domain scenarios due to domain shift. In this work, we propose an end-to-end instance-wise and prototype-wise contrastive learning model with a cross-attention mechanism for cross-domain rumor detection. The model not only performs cross-domain feature alignment but also enforces target samples to align with the corresponding prototypes of a given source domain. Since target labels in a target domain are unavailable, we use a clustering-based approach with carefully initialized centers by a batch of source domain samples to produce pseudo labels. Moreover, we use a cross-attention mechanism on a pair of source data and target data with the same labels to learn domain-invariant representations. Because the samples in a domain pair tend to express similar semantic patterns,
    
[^10]: 记忆竞技的表征原则

    Representational Tenets for Memory Athletics. (arXiv:2303.11944v1 [q-bio.NC])

    [http://arxiv.org/abs/2303.11944](http://arxiv.org/abs/2303.11944)

    该论文总结了世界级记忆竞技比赛的现状和方法，并探讨了这些方法的意识学理论视角。研究者提出了一组实验，以帮助更好地了解专家记忆表现的边界。

    

    我们描述了世界级记忆竞技比赛的当前状态，包括为记忆比赛做准备和参加比赛所使用的方法，基于世界记忆大师和联合作者纳尔逊·德利斯的主观报告。然后，我们通过意识的模拟、情境和结构一致性量子化理论(S3Q)的视角探究了这些报道经验，以提出一组实验，以帮助进一步了解专家记忆表现的界限。

    We describe the current state of world-class memory competitions, including the methods used to prepare for and compete in memory competitions, based on the subjective report of World Memory Championship Grandmaster and co-author Nelson Dellis. We then explore the reported experiences through the lens of the Simulated, Situated, and Structurally coherent Qualia (S3Q) theory of consciousness, in order to propose a set of experiments to help further understand the boundaries of expert memory performance.
    
[^11]: 稀疏分布式内存是一个持续学习者

    Sparse Distributed Memory is a Continual Learner. (arXiv:2303.11934v1 [cs.NE])

    [http://arxiv.org/abs/2303.11934](http://arxiv.org/abs/2303.11934)

    该论文提出了一个使用稀疏分布式内存的修改多层感知器（MLP）模型，它是一个强大的持续学习者，并且该方法不需要记忆重放或任务信息。这是一种训练稀疏网络的新方法，具有广泛的适用性。

    

    持续学习是人工神经网络面临的问题，而它们的生物学对应物擅长解决。在利用稀疏分布式内存（SDM）将核心神经电路与强大的Transformer模型相连接的研究基础上，我们创建了一个改进的多层感知器（MLP），它是一个强大的持续学习者。我们发现，从生物学上翻译过来的我们的MLP变体的每个组成部分都是持续学习所必需的。我们的解决方案也不需要任何记忆重放或任务信息，并引入了训练稀疏网络的新方法，这可能具有广泛的适用性。

    Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable.
    
[^12]: 中间特征联盟能帮助解释黑盒模型吗？

    Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])

    [http://arxiv.org/abs/2303.11920](http://arxiv.org/abs/2303.11920)

    本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。

    

    本文引入了基于级别结构的中间概念，以帮助黑盒模型的可解释性。级别结构是一种分层结构，每个级别对应数据集的特征（即玩家集分区）。从只包含单元素的平凡集合到只包含大联盟的集合，粗糙度的级别逐渐增加。此外，可以通过领域专家建立部分-整体关系来生成抽象级别的解释。我们在一个实际的汽车模型示例和泰坦尼克号的数据集中说明了这种方法的可用性，其中中间概念在不同抽象级别上帮助解释。

    This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
    
[^13]: Deephys：分布漂移下神经网络的调试与可视化工具

    Deephys: Deep Electrophysiology, Debugging Neural Networks under Distribution Shifts. (arXiv:2303.11912v1 [cs.LG])

    [http://arxiv.org/abs/2303.11912](http://arxiv.org/abs/2303.11912)

    本文介绍了一种名为Deephys的可视化和理解DNN在超出分布范围的场景中失败的工具，使用神经电生理学的概念，通过比较内分布和外分布数据集中的神经活动，无缝分析单个神经元、单个图像和类别图像集，并能揭示假特征和新特征存在导致的失败。

    

    深度神经网络（DNN）在超出分布范围的场景下经常会出现失败。本文提出了一个工具来可视化和理解这种失败。我们从神经电生理学的概念中汲取灵感，通过分析单个神经元的特征调谐和不变性，来检查神经网络的内部功能。Deep Electrophysiology，简称Deephys，通过比较可视化内分布和外分布数据集中的神经活动，提供了有关DNN在超出分布范围的场景中失败的见解。Deephys提供了对单个神经元，单个图像以及类别图像集的无缝分析，并且能够揭示由于假特征和新特征的存在而导致的失败。我们通过在几个数据集和分布漂移中使用卷积神经网络和变换器架构进行数量分析，证实了Deephys的定性可视化的有效性。

    Deep Neural Networks (DNNs) often fail in out-of-distribution scenarios. In this paper, we introduce a tool to visualize and understand such failures. We draw inspiration from concepts from neural electrophysiology, which are based on inspecting the internal functioning of a neural networks by analyzing the feature tuning and invariances of individual units. Deep Electrophysiology, in short Deephys, provides insights of the DNN's failures in out-of-distribution scenarios by comparative visualization of the neural activity in in-distribution and out-of-distribution datasets. Deephys provides seamless analyses of individual neurons, individual images, and a set of set of images from a category, and it is capable of revealing failures due to the presence of spurious features and novel features. We substantiate the validity of the qualitative visualizations of Deephys thorough quantitative analyses using convolutional and transformers architectures, in several datasets and distribution shi
    
[^14]: 多智能体强化学习用于大规模格网交通网络区域信号控制

    Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])

    [http://arxiv.org/abs/2303.11899](http://arxiv.org/abs/2303.11899)

    本文提出了一种新的训练框架 RegionLight，基于交叉口之间的邻接关系将智能体分配到每个区域中。同时，研究人员扩展了BDQ方法为DBDQ，以限制联合动作空间大小的增长并缓解智能体训练问题。

    

    多智能体强化学习（MARL）的自适应交通信号控制是当前非常流行的研究领域。大多数现有方法中，一个智能体控制单个路口，这些方法侧重于路口之间的协作。然而，MARL的非稳态性质随着交通网络规模的增长，仍然限制着上述方法的性能。一种妥协的策略是将一名智能体分配到一组路口中，以减少智能体数量。这种策略存在两个挑战，一个是如何将交通网络划分成小区域，另一个是如何搜索区域内的最优联合动作。本文提出了一种新的训练框架RegionLight，其中我们的区域划分规则基于交叉口之间的邻接关系，并扩展了Branching Dueling Q-Network(BDQ)。该方法将BDQ进一步优化为Dynamic Branching Dueling Q-Network(DBDQ)，以限制联合动作空间大小的增长并缓解智能体训练问题。

    Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
    
[^15]: 基于惩罚的模仿学习和跨语义生成传感器融合技术在自动驾驶中的应用

    Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])

    [http://arxiv.org/abs/2303.11888](http://arxiv.org/abs/2303.11888)

    本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。

    

    随着模式识别和计算机视觉技术的快速发展，目标检测、语义分割等任务的准确度已经超过人类。自动驾驶作为一项重要的研究方向，旨在彻底改变未来的交通和出行方式。传感器对于自动驾驶的安全性和环境感知的可行性至关重要。多传感器融合由于其多维感知和集成能力的潜力而成为当前研究的热点。本文提出了一种新的特征级多传感器融合技术，用于端到端的自动驾驶导航和模仿学习。我们的论文主要关注于激光雷达和RGB信息的融合技术。我们还提供了一种全新的基于惩罚的模仿学习方法，以加强模型遵守交通规则的能力并统一模仿学习的目标。

    With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
    
[^16]: 通过系统评估更好地理解归因方法的差异

    Better Understanding Differences in Attribution Methods via Systematic Evaluations. (arXiv:2303.11884v1 [cs.CV])

    [http://arxiv.org/abs/2303.11884](http://arxiv.org/abs/2303.11884)

    本文提出了三种新的评估方案，通过这些方案，可以更可靠地测量归因方法的可信度。

    

    深度神经网络在许多视觉任务上取得了巨大成功，但其黑盒性质使其难以解释。为了克服这一问题，提出了各种后续归因方法来确定对模型决策最有影响力的图像区域。由于不存在基准归因，因此评估这些方法是具有挑战性的。因此，我们提出了三种新的评估方案，以更可靠地测量这些方法的可信度，使它们之间的比较更公平，并使视觉检查更系统化。

    Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For mo
    
[^17]: 使用脉冲神经元的在线Transformer用于快速假肢手控制

    Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control. (arXiv:2303.11860v1 [cs.NE])

    [http://arxiv.org/abs/2303.11860](http://arxiv.org/abs/2303.11860)

    该论文采用了滑动窗口注意机制来替代Transformer中的自注意机制，从而使得使用逐个元素地处理序列，更加适用于在线信号处理，并且在手指位置回归数据集上实现了最好的准确性纪录，每个推理步骤仅需要非常短的时间窗口(3.5毫秒)。

    

    Transformer网络是大多数序列处理任务的最先进网络。然而，Transformer中经常使用的自注意机制需要大的时间窗口来进行每个计算步骤，因此与递归神经网络(RNN)相比，使得它们不太适用于在线信号处理。在本文中，我们使用滑动窗口注意机制来代替自注意机制。我们展示了这种机制对于在输入和目标之间存在有限范围依赖的连续信号更为高效，并且可以用它来逐个元素地处理序列，因此使其适用于在线处理。我们在一个指尖位置回归数据集(NinaproDB8)上测试了模型，该数据集使用在前臂皮肤上测量的Surface Electromyographic (sEMG)信号来估计肌肉活动。我们的方法在每个推理步骤中仅需要非常短的时间窗口(3.5毫秒)就能在这个数据集上取得最新的准确性纪录。

    Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we inc
    
[^18]: 对知识图谱进行逻辑查询应答的关系模式建模

    Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs. (arXiv:2303.11858v1 [cs.DB])

    [http://arxiv.org/abs/2303.11858](http://arxiv.org/abs/2303.11858)

    本文介绍了一种新的查询嵌入方法RoConE，它允许学习关系模式并提高了逻辑查询推理的性能。

    

    对知识图谱（KG）进行一阶逻辑（FOL）查询的回答仍然是一项具有挑战性的任务，主要是由于KG不完整性而导致的。查询嵌入方法通过计算实体、关系和逻辑查询的低维度向量表示来解决这个问题。KG表现出对称性和组合性等关系模式，建模这些模式可以进一步提高查询嵌入模型的性能。然而，这些模式在查询嵌入模型中的作用尚未在文献中进行研究。在本文中，我们填补了这一研究空白，并通过引入允许学习关系模式的归纳偏差，加强FOL查询推理的模式推理能力。为此，我们开发了一种新的查询嵌入方法RoConE，它将查询区域定义为几何锥体，并通过在复杂空间中旋转代数查询算子。RoConE结合了几何锥体作为可以明确定义查询表示的几何表示和旋转代数查询算子的优势。

    Answering first-order logical (FOL) queries over knowledge graphs (KG) remains a challenging task mainly due to KG incompleteness. Query embedding approaches this problem by computing the low-dimensional vector representations of entities, relations, and logical queries. KGs exhibit relational patterns such as symmetry and composition and modeling the patterns can further enhance the performance of query embedding models. However, the role of such patterns in answering FOL queries by query embedding models has not been yet studied in the literature. In this paper, we fill in this research gap and empower FOL queries reasoning with pattern inference by introducing an inductive bias that allows for learning relation patterns. To this end, we develop a novel query embedding method, RoConE, that defines query regions as geometric cones and algebraic query operators by rotations in complex space. RoConE combines the advantages of Cone as a well-specified geometric representation for query e
    
[^19]: LoRCoN-LO:基于长短期循环卷积网络的激光雷达测距仪里程计估计方法

    LoRCoN-LO: Long-term Recurrent Convolutional Network-based LiDAR Odometry. (arXiv:2303.11853v1 [cs.RO])

    [http://arxiv.org/abs/2303.11853](http://arxiv.org/abs/2303.11853)

    提出了一种基于长短期循环卷积网络的激光雷达里程计估计方法LoRCoN-LO。通过使用CNN和LSTM层处理空间和时间信息，利用点云实现对机器人连续运动的预测。在公共数据集(KITTI)上表现出了精确的里程计预测结果。

    

    我们提出了一种基于深度学习的激光雷达里程计估计方法，称为LoRCoN-LO，该方法利用了长期循环卷积网络（LRCN）结构。 LRCN层是一种结构，可以通过使用CNN和LSTM层同时处理空间和时间信息。由于它使用包含空间信息的点云进行连续机器人运动的预测，因此这个特征非常适合。因此，我们使用LRCN层构建了一个LoRCoN-LO模型，并通过该模型预测了机器人的姿态。为了验证性能，我们利用了公共数据集（KITTI）进行了实验。实验结果表明，LoRCoN-LO在数据集中显示了精确的里程计预测。 该代码可在 https://github.com/donghwijung/LoRCoN-LO 上获得。

    We propose a deep learning-based LiDAR odometry estimation method called LoRCoN-LO that utilizes the long-term recurrent convolutional network (LRCN) structure. The LRCN layer is a structure that can process spatial and temporal information at once by using both CNN and LSTM layers. This feature is suitable for predicting continuous robot movements as it uses point clouds that contain spatial information. Therefore, we built a LoRCoN-LO model using the LRCN layer, and predicted the pose of the robot through this model. For performance verification, we conducted experiments exploiting a public dataset (KITTI). The results of the experiment show that LoRCoN-LO displays accurate odometry prediction in the dataset. The code is available at https://github.com/donghwijung/LoRCoN-LO.
    
[^20]: 基于密度的正样本增强的 PU Learning 方法：Dens-PU

    Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation. (arXiv:2303.11848v1 [cs.LG])

    [http://arxiv.org/abs/2303.11848](http://arxiv.org/abs/2303.11848)

    Dens-PU是一种基于密度的正样本增强的PU Learning方法，可以用于解决二元分类问题，且在基准图像数据集上表现出最先进的结果。

    

    本研究提出了一种新的 PU Learning 方法，基于异常检测策略解决该问题。从正样本数据中提取的潜在编码线性组合以获得新样本。这些新样本被用作嵌入以增加正样本数据的密度，从而定义近似正类的边界。样本距离边界越远，则认为它是负样本的可能性越大。一旦获得一组负样本，PU Learning 问题就转化为二元分类。名为 Dens-PU 的方法，由于其依赖于正样本数据的密度，经过基准图像数据集的评估，取得了最先进的结果。

    This study proposes a novel approach for solving the PU learning problem based on an anomaly-detection strategy. Latent encodings extracted from positive-labeled data are linearly combined to acquire new samples. These new samples are used as embeddings to increase the density of positive-labeled data and, thus, define a boundary that approximates the positive class. The further a sample is from the boundary the more it is considered as a negative sample. Once a set of negative samples is obtained, the PU learning problem reduces to binary classification. The approach, named Dens-PU due to its reliance on the density of positive-labeled data, was evaluated using benchmark image datasets, and state-of-the-art results were attained.
    
[^21]: 解决联邦半监督学习中的类变量不平衡问题

    Addressing Class Variable Imbalance in Federated Semi-supervised Learning. (arXiv:2303.11809v1 [cs.LG])

    [http://arxiv.org/abs/2303.11809](http://arxiv.org/abs/2303.11809)

    本文介绍了一种称为联邦半监督学习与类变量不平衡（FSSL-CVI）的新方法，它使用动态类别加权方案来处理FSSL中的类别变量不平衡问题，并且在多个数据集上进行了实验验证。通过实验结果，本文表明 FSSL-CVI 方法在各方面性能上优于现有的联邦学习和FSSL 方法。

    

    联邦半监督学习（FSSL）结合联邦学习和半监督学习的技术，通过使用少量标注数据和大量未标注数据在分布式环境中提高模型的准确性和性能。在不需要将所有数据集中于一处进行训练的情况下，它会在设备本地训练模型后收集模型训练更新，从而可以保护用户数据的隐私。然而，在联邦训练过程中，一些设备无法收集足够的本地训练数据，同时新设备将被添加到组训练中。这导致不平衡的全局数据分布，从而影响全局模型训练的性能。大多数当前的研究着重于固定类别数量的类别不平衡问题，而很少有注意力放在具有可变类别数量的数据不平衡问题上。因此，在本文中，我们提出了联邦半监督学习与类变量不平衡（FSSL-CVI），它使用动态类权重方案来解决FSSL中的类变量不平衡问题。实验结果表明，我们提出的方法在具有类变量不平衡的各种数据集上的分类准确率方面优于现有的联邦学习和FSSL方法。

    Federated Semi-supervised Learning (FSSL) combines techniques from both fields of federated and semi-supervised learning to improve the accuracy and performance of models in a distributed environment by using a small fraction of labeled data and a large amount of unlabeled data. Without the need to centralize all data in one place for training, it collect updates of model training after devices train models at local, and thus can protect the privacy of user data. However, during the federal training process, some of the devices fail to collect enough data for local training, while new devices will be included to the group training. This leads to an unbalanced global data distribution and thus affect the performance of the global model training. Most of the current research is focusing on class imbalance with a fixed number of classes, while little attention is paid to data imbalance with a variable number of classes. Therefore, in this paper, we propose Federated Semi-supervised Learni
    
[^22]: 轻量级对比蛋白质结构-序列变换

    Lightweight Contrastive Protein Structure-Sequence Transformation. (arXiv:2303.11783v1 [q-bio.BM])

    [http://arxiv.org/abs/2303.11783](http://arxiv.org/abs/2303.11783)

    该论文提出了一种新的无监督学习的蛋白质结构表示预训练方法，使用强大的蛋白质语言模型和自监督结构约束，避免了破坏真实的空间结构表示和标记数据的限制。

    

    在大多数蛋白质下游应用中，无标签的预训练蛋白质结构模型是关键基础。传统的结构预训练方法遵循成熟的自然语言预训练方法，例如去噪重构和掩码语言建模，但通常会破坏真实的空间结构表示。其他常见的预训练方法可能会预测一组固定的预定对象类别，其中受限的监督方式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他的蛋白质概念。在这项工作中，我们引入了一种新的无监督蛋白质结构表示预训练方法，其中使用强大的蛋白质语言模型。特别地，我们首先建议利用现有的预训练语言模型通过无监督的对比对齐来指导结构模型的学习。此外，我们提出了一种自监督结构约束，以进一步学习内在的蛋白质结构表示形式。

    Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic i
    
[^23]: 面向数字孪生6G物联网的联邦边缘学习中的毒化攻击：一项预测性研究

    Poisoning Attacks in Federated Edge Learning for Digital Twin 6G-enabled IoTs: An Anticipatory Study. (arXiv:2303.11745v1 [cs.CR])

    [http://arxiv.org/abs/2303.11745](http://arxiv.org/abs/2303.11745)

    本文研究了面向数字孪生6G物联网的联邦边缘学习中的毒化攻击的预测性研究，发现攻击者可以在集中式学习和联邦学习中进行攻击，并且成功的攻击会降低模型的准确性。

    

    联邦边缘学习在支持数字孪生6G物联网环境下的隐私保护、人工智能（AI）应用方面具有重要作用。然而，我们需要考虑到攻击目标是基于AI系统的潜在威胁，例如，攻击者试图在本地更新期间破坏物联网设备上的数据或破坏模型更新过程。因此，我们提出了一项针对数字孪生6G物联网环境下联邦边缘学习中的毒化攻击的预测性研究。具体来说，我们研究了攻击对数字孪生6G物联网环境中联邦学习模型的训练和发展的影响。我们证明了攻击者可以在两种不同的学习设置中（即集中式学习和联邦学习）进行毒化攻击，并且成功的攻击会严重降低模型的准确性。我们在一个新的网络安全数据集上全面评估了这些攻击。

    Federated edge learning can be essential in supporting privacy-preserving, artificial intelligence (AI)-enabled activities in digital twin 6G-enabled Internet of Things (IoT) environments. However, we need to also consider the potential of attacks targeting the underlying AI systems (e.g., adversaries seek to corrupt data on the IoT devices during local updates or corrupt the model updates); hence, in this article, we propose an anticipatory study for poisoning attacks in federated edge learning for digital twin 6G-enabled IoT environments. Specifically, we study the influence of adversaries on the training and development of federated learning models in digital twin 6G-enabled IoT environments. We demonstrate that attackers can carry out poisoning attacks in two different learning settings, namely: centralized learning and federated learning, and successful attacks can severely reduce the model's accuracy. We comprehensively evaluate the attacks on a new cyber security dataset designe
    
[^24]: 解锁自编码器的逐层相关传播

    Unlocking Layer-wise Relevance Propagation for Autoencoders. (arXiv:2303.11734v1 [cs.LG])

    [http://arxiv.org/abs/2303.11734](http://arxiv.org/abs/2303.11734)

    本文提出了一种解释自编码器的快速解决方案，通过将深度泰勒分解框架与逐层相关传播方法相结合。结果显示，该方法计算和质量方面具有比现有方法更好的优势。

    

    自编码器是一种强大而多功能的工具，常用于各种问题，如异常检测、图像处理和机器翻译。然而，它们的重建并不总是易于解释。因此，我们提出了一种通过将深度泰勒分解框架与逐层相关传播方法相结合来快速解释的解决方案。此外，我们引入了一种新的验证技术，用于比较我们的可解释性方法和缺失地面真实数据的基准方法。我们的结果凸显了所提出的可解释性解决方案相对于现有方法的计算和质量优势。

    Autoencoders are a powerful and versatile tool often used for various problems such as anomaly detection, image processing and machine translation. However, their reconstructions are not always trivial to explain. Therefore, we propose a fast explainability solution by extending the Layer-wise Relevance Propagation method with the help of Deep Taylor Decomposition framework. Furthermore, we introduce a novel validation technique for comparing our explainability approach with baseline methods in the case of missing ground-truth data. Our results highlight computational as well as qualitative advantages of the proposed explainability solution with respect to existing methods.
    
[^25]: 抽象视觉推理: 一种用于解决Raven渐进矩阵的代数方法

    Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices. (arXiv:2303.11730v1 [cs.CV])

    [http://arxiv.org/abs/2303.11730](http://arxiv.org/abs/2303.11730)

    该论文提出了一种新的代数机器推理框架，能够高效解决Raven渐进矩阵问题，最高准确率可达93.8％。

    

    我们引入了一种新的推理框架---代数机器推理，该框架非常适合抽象推理。代数机器推理有效地将解决新问题的困难过程简化为例行代数计算。感兴趣的基本代数对象是某些适当初始化的多项式环的理想。我们将解释如何将Raven渐进矩阵（RPM）的解决可以作为代数计算问题实现，这些问题结合了各种众所周知的代数子例程，包括：计算理想的Gröbner基，检查理想包含等。关键是，理想满足的附加代数结构允许进行更多的理想操作，超越了集合理论操作。我们的代数机器推理框架不仅能够从给定的答案集中选择正确的答案，而且还能仅凭问题矩阵给出正确的答案。在I-RAVEN数据集上的实验表明，对于RPM问题，我们的代数机器推理框架的总体准确率达到93.8％，优于传统的机器学习方法和人类表现。

    We introduce algebraic machine reasoning, a new reasoning framework that is well-suited for abstract reasoning. Effectively, algebraic machine reasoning reduces the difficult process of novel problem-solving to routine algebraic computation. The fundamental algebraic objects of interest are the ideals of some suitably initialized polynomial ring. We shall explain how solving Raven's Progressive Matrices (RPMs) can be realized as computational problems in algebra, which combine various well-known algebraic subroutines that include: Computing the Gr\"obner basis of an ideal, checking for ideal containment, etc. Crucially, the additional algebraic structure satisfied by ideals allows for more operations on ideals beyond set-theoretic operations.  Our algebraic machine reasoning framework is not only able to select the correct answer from a given answer set, but also able to generate the correct answer with only the question matrix given. Experiments on the I-RAVEN dataset yield an overall
    
[^26]: 基于注意力神经网络的移动机器人轮式里程校正的在线学习

    Online Learning of Wheel Odometry Correction for Mobile Robots with Attention-based Neural Network. (arXiv:2303.11725v1 [cs.RO])

    [http://arxiv.org/abs/2303.11725](http://arxiv.org/abs/2303.11725)

    本文提出了一种基于注意力神经网络的在线学习方法，用于处理移动机器人轮式里程校正问题，并取得了优异的结果，避免了耗时的数据收集过程，具有良好的实时性。

    

    现代机器人平台需要可靠的定位系统来在人类旁边日常运行。基于滤波轮式和惯性里程计的简单姿态估计算法通常会在出现突然的动力学变化和轮滑时失效。此外，尽管视觉里程计最近取得了成功，但提供服务和辅助机器人任务的环境条件往往具有挑战性，在这些条件下，基于视觉的解决方案由于照明不良或重复特征模式而失败。在本文中，我们提出了一种创新的轮式里程校正在线学习方法，为鲁棒的多源定位系统铺平了道路。我们研究了一种高效的基于注意力的神经网络架构，将精确的性能与实时推断相结合。与标准神经网络和基于滤波的里程校正算法相比，所提出的解决方案显示出令人瞩目的结果。然而，在线学习范式避免了耗时的数据收集过程，并且可以随着时间改善系统的性能。

    Modern robotic platforms need a reliable localization system to operate daily beside humans. Simple pose estimation algorithms based on filtered wheel and inertial odometry often fail in the presence of abrupt kinematic changes and wheel slips. Moreover, despite the recent success of visual odometry, service and assistive robotic tasks often present challenging environmental conditions where visual-based solutions fail due to poor lighting or repetitive feature patterns. In this work, we propose an innovative online learning approach for wheel odometry correction, paving the way for a robust multi-source localization system. An efficient attention-based neural network architecture has been studied to combine precise performances with real-time inference. The proposed solution shows remarkable results compared to a standard neural network and filter-based odometry correction algorithms. Nonetheless, the online learning paradigm avoids the time-consuming data collection procedure and can
    
[^27]: 生成型AI（AIGC）完整调研：ChatGPT从GPT-4到GPT-5是你需要的全部吗？

    A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?. (arXiv:2303.11717v1 [cs.AI])

    [http://arxiv.org/abs/2303.11717](http://arxiv.org/abs/2303.11717)

    本文通过完整的调研介绍了生成型AI，包含了从技术到应用的各个方面。ChatGPT虽然是一个有用的工具，但并不足以覆盖所有的AIGC任务，对于实现多样化的内容创造还需要GPT-5等未来的发展。

    

    随着ChatGPT的流行，生成型AI（AIGC，即AI生成的内容）因其分析和创造文本、图像等能力而在各个领域引起轰动。在如此广泛的媒体报导下，几乎不可能错过从特定角度窥探AIGC的机会。在AI从纯粹的分析转向创造的时代，值得注意的是，ChatGPT仅仅是众多AIGC任务中的一个工具，其最新的语言模型GPT-4。许多人对ChatGPT的能力印象深刻，同时也在思考它的局限性：GPT-5（或其他未来的GPT变种）是否能帮助ChatGPT统一各种AIGC任务，实现多样化的内容创造？为了回答这个问题，需要进行现有AIGC任务的全面审查。因此，我们的工作迅速填补了这一空白，首次介绍了AIGC，涵盖了从技术到应用的所有方面。现代的生成型AI依赖于各种技术基础，包括模型架构等。

    As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and 
    
[^28]: 高效解释 CSPs 的不可满足子集优化（扩展算法和示例）

    Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples). (arXiv:2303.11712v1 [cs.AI])

    [http://arxiv.org/abs/2303.11712](http://arxiv.org/abs/2303.11712)

    本文提出对于约束满足问题，采用一种高效的算法以易于理解的方式解释解决方案，这一方法可以让人类更好地理解机器所做的决策。

    

    我们在最近提出的方法基础上，为逐步以易于理解方式解释约束满足问题（CSP）的解决方案。这里的解释是一系列简单的推断步骤，其中简单性使用成本函数量化。解释生成算法依赖于从导出的不可满足公式中提取最小不满子集（MUS），利用所谓的非冗余解释和 MUS 之间的一一对应关系。然而，MUS 提取算法不提供任何针对给定成本函数的子集最小性或最优性保证。因此，我们在这些形式基础上建立，并着手改进的主要要点，即如何高效地生成可证明是最优的解释（与给定成本度量相关）。为此，我们开发了（1）基于命中集的算法，用于查找最佳受限不可满足子集；（2）一种重用相关信息的方法，可在不同的解释生成阶段提高效率。

    We build on a recently proposed method for stepwise explaining solutions of Constraint Satisfaction Problems (CSP) in a human-understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified using a cost function. The algorithms for explanation generation rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived unsatisfiable formula, exploiting a one-to-one correspondence between so-called non-redundant explanations and MUSs. However, MUS extraction algorithms do not provide any guarantee of subset minimality or optimality with respect to a given cost function. Therefore, we build on these formal foundations and tackle the main points of improvement, namely how to generate explanations efficiently that are provably optimal (with respect to the given cost metric). For that, we developed (1) a hitting set-based algorithm for finding the optimal constrained unsatisfiable subsets; (2) a method for re-using relevant information over m
    
[^29]: 连接生成半监督学习和生成开放集识别

    Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])

    [http://arxiv.org/abs/2303.11702](http://arxiv.org/abs/2303.11702)

    本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。

    

    本研究在生成对抗网络（GANs）的背景下，探究了半监督学习（SSL）和开放集识别（OSR）之间的关系。尽管以前没有正式将SSL和OSR联系起来的研究，但它们各自的方法有惊人的相似之处。具体而言，SSL-GAN和OSR-GAN要求生成器在互补空间中产生样本。随后，通过对生成样本进行正则化，SSL和OSR分类器都可以完全识别开放空间。为了证明SSL和OSR之间的关联，我们在理论上和实验上比较了最先进的SSL-GAN方法和最先进的OSR-GAN方法。结果表明，文献基础更加牢固的SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，并在某些一般的OSR实验中取得了新的最先进的结果。然而，OSR优化的对抗性互惠点（ARP）-GAN在一些OSR任务中仍然略优于SSL-GAN。

    This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
    
[^30]: 联邦学习中类别不平衡问题的调查

    A Survey on Class Imbalance in Federated Learning. (arXiv:2303.11673v1 [cs.LG])

    [http://arxiv.org/abs/2303.11673](http://arxiv.org/abs/2303.11673)

    本文对联邦学习中的类别不平衡问题和解决方案进行了调查和总结，包括平衡感知优化、数据增强和客户端加权等。目前仍存在着一些问题需要解决。

    

    联邦学习是一种新兴的分布式学习技术，允许网络中的多个客户端设备在不直接暴露客户数据的情况下共同训练一个机器学习模型，由于其隐私保护的性质而备受关注。然而，在联邦学习中训练的模型通常表现不如在标准集中式学习模式下训练的模型，特别是在训练数据不平衡的情况下。在联邦学习中，数据失衡可能在单个客户端设备上发生，也可能在许多设备上全局发生。不同类型数据不平衡的复杂性给联邦学习技术的发展带来了挑战，尤其是考虑到需要同时缓解数据不平衡问题和保护数据隐私。因此，许多尝试已经在文献中对联邦学习中的类别不平衡进行了处理。本文对这一问题的最新进展进行了详细回顾，包括平衡感知优化、数据增强和客户端加权。我们还讨论了解决联邦学习中类别不平衡问题的开放问题和潜在研究方向。

    Federated learning, which allows multiple client devices in a network to jointly train a machine learning model without direct exposure of clients' data, is an emerging distributed learning technique due to its nature of privacy preservation. However, it has been found that models trained with federated learning usually have worse performance than their counterparts trained in the standard centralized learning mode, especially when the training data is imbalanced. In the context of federated learning, data imbalance may occur either locally one one client device, or globally across many devices. The complexity of different types of data imbalance has posed challenges to the development of federated learning technique, especially considering the need of relieving data imbalance issue and preserving data privacy at the same time. Therefore, in the literature, many attempts have been made to handle class imbalance in federated learning. In this paper, we present a detailed review of recen
    
[^31]: 利用转移学习来进行属性推断的研究

    Manipulating Transfer Learning for Property Inference. (arXiv:2303.11643v1 [cs.LG])

    [http://arxiv.org/abs/2303.11643](http://arxiv.org/abs/2303.11643)

    本文研究了转移学习中的属性推断攻击，攻击者可以操纵上游模型，对受害者调整的下游模型进行高效且特定的推断攻击，需要注意和防范此类攻击。

    

    转移学习是一种常用的方法，用于利用有限的数据和计算资源来调整预训练的（上游）模型，用于不同的下游任务。我们研究了一个拥有对用于转移学习中的上游模型进行控制的对手如何对受害者调整的下游模型进行属性推断攻击。我们展示了攻击的情况，即对手可以操纵上游模型进行高效且特定的属性推断攻击（AUC得分>0.9），而不会在主要任务中产生显著的性能损失。操纵的主要思想是使上游模型为具有目标属性的样本生成具有不同分布的激活（中间特征），从而使对手能够轻松区分训练有具有目标属性的样本和没有的样本。我们的代码和实验基于使用卷积神经网络作为上游模型的面部识别任务和ColorFeret数据集作为下游模型的训练集。我们的结果表明，从业者在使用转移学习时需要注意属性推断攻击的可能性，并采取措施来防止此类攻击。

    Transfer learning is a popular method for tuning pretrained (upstream) models for different downstream tasks using limited data and computational resources. We study how an adversary with control over an upstream model used in transfer learning can conduct property inference attacks on a victim's tuned downstream model. For example, to infer the presence of images of a specific individual in the downstream training set. We demonstrate attacks in which an adversary can manipulate the upstream model to conduct highly effective and specific property inference attacks (AUC score $> 0.9$), without incurring significant performance loss on the main task. The main idea of the manipulation is to make the upstream model generate activations (intermediate features) with different distributions for samples with and without a target property, thus enabling the adversary to distinguish easily between downstream models trained with and without training examples that have the target property. Our cod
    
[^32]: BoxSnake：使用框注释的多边形实例分割

    BoxSnake: Polygonal Instance Segmentation with Box Supervision. (arXiv:2303.11630v1 [cs.CV])

    [http://arxiv.org/abs/2303.11630](http://arxiv.org/abs/2303.11630)

    BoxSnake是一种新的端到端训练技术，可以仅使用框注释实现有效的多边形实例分割，相较于基于掩膜的弱监督方法，BoxSnake显示出显着的优越性。

    

    带框注释的实例分割因只需要简单的框标注而非昂贵的掩膜或多边形标注而引起了广泛关注。然而，现有的带框实例分割模型主要集中在基于掩膜的框架上。我们提出了一种新的端到端训练技术BoxSnake，首次仅使用框注释实现有效的多边形实例分割。我们的方法包括两个损失函数：（1）基于点的单元损失，约束预测多边形的边界框以实现粗略分割；（2）距离感知的成对损失，促使预测的多边形贴合物体边界。与基于掩膜的弱监督方法相比，BoxSnake进一步降低了预测分割与边界框之间的性能差距，并在Cityscapes数据集上表现出显着的优越性。

    Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset.
    
[^33]: 面向持续环境的评估者指导学习

    Assessor-Guided Learning for Continual Environments. (arXiv:2303.11624v1 [cs.LG])

    [http://arxiv.org/abs/2303.11624](http://arxiv.org/abs/2303.11624)

    本文提出了一种评估者指导学习策略，用于持续学习，其中评估者通过控制学习方向和速度来指导学习过程，以有效地学习新环境并避免灾难性干扰问题的发生。

    

    本文提出了一种评估者指导学习策略，用于持续学习，其中一个评估者通过控制学习过程的方向和速度来指导基础学习者的学习过程，从而有效地学习新环境并防止灾难性干扰问题的发生。评估者以元学习的方式进行训练，其元目标是增强基础学习者的学习过程。它执行每个样本的软加权机制，接受正样本并拒绝负样本。基础学习者的训练目标是最小化交叉熵损失函数、暗体验重放（DER）损失函数和知识蒸馏损失函数的元加权组合，这些交互以获得更好的性能。

    This paper proposes an assessor-guided learning strategy for continual learning where an assessor guides the learning process of a base learner by controlling the direction and pace of the learning process thus allowing an efficient learning of new environments while protecting against the catastrophic interference problem. The assessor is trained in a meta-learning manner with a meta-objective to boost the learning process of the base learner. It performs a soft-weighting mechanism of every sample accepting positive samples while rejecting negative samples. The training objective of a base learner is to minimize a meta-weighted combination of the cross entropy loss function, the dark experience replay (DER) loss function and the knowledge distillation loss function whose interactions are controlled in such a way to attain an improved performance. A compensated over-sampling (COS) strategy is developed to overcome the class imbalanced problem of the episodic memory due to limited memor
    
[^34]: 异构分支协作学习用于对话生成

    Heterogeneous-Branch Collaborative Learning for Dialogue Generation. (arXiv:2303.11621v1 [cs.CL])

    [http://arxiv.org/abs/2303.11621](http://arxiv.org/abs/2303.11621)

    本文提出了一种异构分支协作学习模型，用于对话生成。该模型使用协作学习方法而非传统的知识蒸馏方法，在网络分支的训练中考虑到对话属性，使不同分支的特征多样化。

    

    随着深度学习的发展，高级对话生成方法通常需要更多的计算资源。一种有效的获得高性能和轻量级模型的方法是知识蒸馏，其依赖于预训练的强大教师模型。协作学习，也称为在线知识蒸馏，在缺乏训练良好的大型教师模型的情况下，是进行单阶段群体蒸馏的有效方法。然而，先前的工作由于相同的训练目标和独立相同的训练集存在严重的分支同质性问题。为了缓解这个问题，我们考虑在网络分支的训练中考虑对话属性。每个分支基于所选子集学习与属性相关的特征。此外，我们提出了一个双重基于群体的知识蒸馏方法，包括积极蒸馏和消极蒸馏，进一步使不同分支的特征多样化。

    With the development of deep learning, advanced dialogue generation methods usually require a greater amount of computational resources. One promising approach to obtaining a high-performance and lightweight model is knowledge distillation, which relies heavily on the pre-trained powerful teacher. Collaborative learning, also known as online knowledge distillation, is an effective way to conduct one-stage group distillation in the absence of a well-trained large teacher model. However, previous work has a severe branch homogeneity problem due to the same training objective and the independent identical training sets. To alleviate this problem, we consider the dialogue attributes in the training of network branches. Each branch learns the attribute-related features based on the selected subset. Furthermore, we propose a dual group-based knowledge distillation method, consisting of positive distillation and negative distillation, to further diversify the features of different branches in
    
[^35]: 分布式编码结构的低复杂度深度视频压缩

    Low-complexity Deep Video Compression with A Distributed Coding Architecture. (arXiv:2303.11599v1 [eess.IV])

    [http://arxiv.org/abs/2303.11599](http://arxiv.org/abs/2303.11599)

    本论文提出了一种低复杂度深度视频压缩的分布式编码结构，通过使用一个有效的译码器辅助信息生成模块，实现在设备资源受限的情况下仍能够有效地利用视频帧间的相关性进行高效压缩。

    

    目前的预测编码视频压缩方法依靠复杂的编码器降低时域冗余，导致在资源受限设备上部署变得具有挑战性。传统的分布式编码方法存在性能差距。我们提出了第一个端对端分布式深度视频压缩框架，其中包含一种有效的SI生成模块，有助于在没有计算密集型编码器侧运动估计的情况下有效地利用帧间相关性，从而提高速率失真性能。

    Prevalent predictive coding-based video compression methods rely on a heavy encoder to reduce the temporal redundancy, which makes it challenging to deploy them on resource-constrained devices. Meanwhile, as early as the 1970s, distributed source coding theory has indicated that independent encoding and joint decoding with side information (SI) can achieve high-efficient compression of correlated sources. This has inspired a distributed coding architecture aiming at reducing the encoding complexity. However, traditional distributed coding methods suffer from a substantial performance gap to predictive coding ones. Inspired by the great success of learning-based compression, we propose the first end-to-end distributed deep video compression framework to improve the rate-distortion performance. A key ingredient is an effective SI generation module at the decoder, which helps to effectively exploit inter-frame correlations without computation-intensive encoder-side motion estimation and c
    
[^36]: 机器心智理论综述

    A Review on Machine Theory of Mind. (arXiv:2303.11594v1 [cs.AI])

    [http://arxiv.org/abs/2303.11594](http://arxiv.org/abs/2303.11594)

    本文回顾了机器心智理论在信念、欲望和意图方面的最新进展，比较了优缺点和适用条件，希望指导研究人员跟上该领域的最新趋势。

    

    心智理论 (ToM) 是指将心理状态归因于他人的能力，是人类认知的基础。目前，对具有认知能力的人工智能（AI）在医疗保健和汽车工业等方面的应用日益广泛。信念、欲望和意图是婴儿早期的能力，也是人类和具有心智理论的机器的认知能力的基础。本文回顾了机器心智理论在信念、欲望和意图方面的最新进展，并介绍了在这三个方面的实验、数据集和方法，总结了近年来不同任务和数据集的发展，并比较了表现良好的模型的优缺点和适用条件，希望这项研究能够指导研究人员快速跟上该领域的最新趋势。与具有特定任务和解决方案框架的其他领域不同，机器心智理论缺乏统一的指导和一系列标准评估任务，这给该领域带来了许多挑战。

    Theory of Mind (ToM) is the ability to attribute mental states to others, the basis of human cognition. At present, there has been growing interest in the AI with cognitive abilities, for example in healthcare and the motoring industry. Beliefs, desires, and intentions are the early abilities of infants and the foundation of human cognitive ability, as well as for machine with ToM. In this paper, we review recent progress in machine ToM on beliefs, desires, and intentions. And we shall introduce the experiments, datasets and methods of machine ToM on these three aspects, summarize the development of different tasks and datasets in recent years, and compare well-behaved models in aspects of advantages, limitations and applicable conditions, hoping that this study can guide researchers to quickly keep up with latest trend in this field. Unlike other domains with a specific task and resolution framework, machine ToM lacks a unified instruction and a series of standard evaluation tasks, wh
    
[^37]: 健康信息学中的大型AI模型：应用、挑战和未来展望

    Large AI Models in Health Informatics: Applications, Challenges, and the Future. (arXiv:2303.11568v1 [cs.AI])

    [http://arxiv.org/abs/2303.11568](http://arxiv.org/abs/2303.11568)

    大型AI模型在健康信息学领域具有突破性应用，但其规模和数据量的挑战需要克服，未来仍需深入探索。

    

    大型AI模型是最近出现的模型，具有庞大的参数和数据规模，其规模往往超过数十亿。一旦预训练，大型AI模型在各种下游任务中展现出令人印象深刻的性能。在健康信息学领域，大型AI模型的出现为方法学设计带来了新的范例，并为健康相关领域的突破提供了推动力量。本文对大型AI模型进行了全面综述，包括背景、应用和挑战等方面。

    Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which often reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A concrete example is the recent debut of ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our life. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multimodality data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents an up-to-date comprehensive review of large AI models, from background to their applic
    
[^38]: 利用高分辨率卫星图像的深度学习数据策略进行龙舌兰作物分割和成熟度分类

    Agave crop segmentation and maturity classification with deep learning data-centric strategies using very high-resolution satellite imagery. (arXiv:2303.11564v1 [cs.CV])

    [http://arxiv.org/abs/2303.11564](http://arxiv.org/abs/2303.11564)

    本研究利用高分辨率卫星图像采用深度学习数据策略解决了龙舌兰作物分割中的问题，并提出了龙舌兰作物成熟度分类方法。

    

    负责任和可持续的龙舌兰-龙舌兰酒生产链对墨西哥龙舌兰地区的社会、环境和经济发展至关重要。因此，开发新工具以实现大规模自动龙舌兰地区监测变得尤为重要。本文提出使用高分辨率卫星图像进行龙舌兰作物分割和成熟度分类的方法，该方法对于此任务可能会有所帮助。

    The responsible and sustainable agave-tequila production chain is fundamental for the social, environment and economic development of Mexico's agave regions. It is therefore relevant to develop new tools for large scale automatic agave region monitoring. In this work, we present an Agave tequilana Weber azul crop segmentation and maturity classification using very high resolution satellite imagery, which could be useful for this task. To achieve this, we solve real-world deep learning problems in the very specific context of agave crop segmentation such as lack of data, low quality labels, highly imbalanced data, and low model performance. The proposed strategies go beyond data augmentation and data transfer combining active learning and the creation of synthetic images with human supervision. As a result, the segmentation performance evaluated with Intersection over Union (IoU) value increased from 0.72 to 0.90 in the test set. We also propose a method for classifying agave crop matur
    
[^39]: 动态健康嵌入改善患者护理

    Dynamic Healthcare Embeddings for Improving Patient Care. (arXiv:2303.11563v1 [cs.LG])

    [http://arxiv.org/abs/2303.11563](http://arxiv.org/abs/2303.11563)

    DECENT是一种自动编码异构共同演化的动态神经网络，可以从各种数据流中学习患者、医生、房间和药物的异构动态嵌入，用于改善患者护理，如药物推荐、患者风险分层和医院容量管理。

    

    随着医院向自动化和集成他们的计算系统，越来越多的精细化医院运营数据变得可用。这些数据包括医院建筑图纸，患者和医护人员之间的交互日志，处方数据，程序数据以及关于患者入院、出院和转移的数据。这为改善患者护理开辟了许多迷人的可能性。然而，为了利用现成的机器学习软件进行这些任务，我们需要从异构的、动态的数据流中学习涉及实体的结构表示。在这里，我们提出了DECTEN，一种自动编码异构共同演化的动态神经网络，用于从各种数据流中学习患者、医生、房间和药物的异构动态嵌入。这些嵌入基于静态属性和动态行为捕捉医生、房间、患者和药物之间的相似性，并可用于各种预测任务，如药物推荐、患者风险分层和医院容量管理。

    As hospitals move towards automating and integrating their computing systems, more fine-grained hospital operations data are becoming available. These data include hospital architectural drawings, logs of interactions between patients and healthcare professionals, prescription data, procedures data, and data on patient admission, discharge, and transfers. This has opened up many fascinating avenues for healthcare-related prediction tasks for improving patient care. However, in order to leverage off-the-shelf machine learning software for these tasks, one needs to learn structured representations of entities involved from heterogeneous, dynamic data streams. Here, we propose DECENT, an auto-encoding heterogeneous co-evolving dynamic neural network, for learning heterogeneous dynamic embeddings of patients, doctors, rooms, and medications from diverse data streams. These embeddings capture similarities among doctors, rooms, patients, and medications based on static attributes and dynamic
    
[^40]: 修正噪声：为可控领域翻译分解源特征

    Fix the Noise: Disentangling Source Feature for Controllable Domain Translation. (arXiv:2303.11545v1 [cs.CV])

    [http://arxiv.org/abs/2303.11545](http://arxiv.org/abs/2303.11545)

    本文提出了一种新的可控领域翻译方法，通过在目标特征空间的已分解子空间中保留源特征，使得只使用单个模型就能平滑地控制保留源特征的程度，产生更一致、更逼真的图像。

    

    最近的研究表明，在无条件生成器上使用转移学习技术，特别是在领域翻译方面，表现出了强大的生成能力。但是，使用单个模型控制不同领域特征之间的控制仍然具有挑战性。现有方法通常需要额外的模型，这在计算上是要求很高的，而且会导致不令人满意的视觉质量。此外，它们具有受限控制步骤，从而防止平滑过渡。在本文中，我们提出了一种新的高质量领域翻译方法，具有更好的可控性。其关键思想是在目标特征空间的已分解子空间中保留源特征。这使得我们的方法能够在只使用单个模型的情况下，平滑地控制保留源特征的程度，同时从完全新的领域生成图像。我们广泛的实验表明，所提出的方法可以产生比先前的工作更一致、更逼真的图像，并保持精确的可控性。

    Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllab
    
[^41]: 不定概率神经网络

    Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])

    [http://arxiv.org/abs/2303.11536](http://arxiv.org/abs/2303.11536)

    本文提出了一种新型通用模型——不定概率神经网络；它可以进行无监督聚类和使用很小的神经网络处理大规模分类，其理论优势体现在新的概率理论和神经网络框架中。

    

    本文提出了一个称为IPNN的新型通用模型，它将神经网络和概率论结合在一起。在传统概率论中，概率的计算是基于事件的发生，而这在当前的神经网络中几乎不使用。因此，我们提出了一种新的概率理论，它是经典概率论的扩展，并使经典概率论成为我们理论的一种特殊情况。此外，对于我们提出的神经网络框架，神经网络的输出被定义为概率事件，并基于这些事件的统计分析，推导出分类任务的推理模型。IPNN展现了新的特性：它在进行分类的同时可以执行无监督聚类。此外，IPNN能够使用非常小的神经网络进行非常大的分类，例如100个输出节点的模型可以分类10亿类别。理论优势体现在新的概率理论和神经网络框架中，并且实验结果展示了IPNN在各种应用中的潜力。

    We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
    
[^42]: AI在人机交互中的影响-基于人机交互的应用

    AI-in-the-Loop -- The impact of HMI in AI-based Application. (arXiv:2303.11508v1 [cs.HC])

    [http://arxiv.org/abs/2303.11508](http://arxiv.org/abs/2303.11508)

    人机交互(HMI)被加入到AI架构设计过程中可以有效减少AI开发所需的资源，避免训练和评估具有非生产层的AI架构，从而导致轻量级AI架构的产生。

    

    人工智能(AI)和人机交互(HMI)是通常不适用于嵌入式应用的两个关键词。在将AI应用于解决特定任务之前需要的步骤中，HMI通常在AI架构设计和AI模型训练过程中缺失。人在环路概念在AI开发的所有其他步骤中都普遍存在，从数据分析到数据选择和清洗、性能评估。在AI架构设计过程中，HMI可以立即突出显示架构的非生产层，从而可以轻松创建用于嵌入式应用的轻量级网络架构。我们展示了通过使用这种HMI，用户可以立即区分哪种AI架构应该首先进行训练和评估，因为可以预期在任务中高精度。这种方法通过避免训练和评估具有非生产层的AI架构来减少AI开发所需的资源，从而导致轻量级AI架构。

    Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded applications can be created easily. We show that by using this HMI, users can instantly distinguish which AI architecture should be trained and evaluated first since a high accuracy on the task could be expected. This approach reduces the resources needed for AI development by avoiding training and evaluating AI architectures with unproductive layers and leads to lightweight AI architectures. The
    
[^43]: 你有在使用我的数据集进行训练吗？使用清洁标签背门数字水印实现公共数据集保护

    Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. (arXiv:2303.11470v1 [cs.CR])

    [http://arxiv.org/abs/2303.11470](http://arxiv.org/abs/2303.11470)

    提出了一种基于背门数字水印的方法，以确保公共数据的安全。通过在数据集中插入极少量的数字水印样本，隐式学习一个隐藏的函数作为数字水印，以跟踪非法使用此数据集的模型。使用“清洁标签背门”方法实现了数字水印，不会破坏原始数据集。实验证明，该方法有效地检测到非法利用数据集的行为。

    

    互联网上源源不断的支持训练数据是深度学习模型成功的关键因素。然而，这种大量的公共数据也引起了对数据集被未经授权的用于商业目的的担忧，这是数据集许可证所禁止的。本文提出了一种基于背门数字水印的方法，作为保护公共数据的通用框架。通过向数据集中插入少量的数字水印样本，我们的方法使学习模型能够隐式学习由防御者设置的秘密函数。这个隐藏的函数可以作为数字水印，用于跟踪非法使用数据集的第三方模型。不幸的是，现有的背门插入方法往往涉及向训练集中添加任意的、错误标记的数据，导致性能显著下降，并容易被异常检测算法检测到。为了克服这个挑战，我们引入了一种清洁标记背门方法，实现了数字水印而不破坏原始数据集。我们的方法在几个图像分类任务上进行了评估，证明了它在检测非法数据集使用方面的有效性。

    The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoo
    
[^44]: 心灵与机器: 解开GPT-4的认知心理学之谜

    Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v1 [cs.CL])

    [http://arxiv.org/abs/2303.11436](http://arxiv.org/abs/2303.11436)

    本研究评估了在广泛使用的CommonsenseQA数据集中的一套常识推理问题上，GPT-4的表现及其对常识知识的处理和整合过程，在此过程中我们也发现了其局限性。

    

    常识推理是人类智能的基本成分，使其能够根据环境观察推断结论。大型语言模型(LLMs)正成为越来越能够执行人类级任务的强有力工具。最近开发的GPT-4及其在医学考试、律师考试等人类难以完成的任务中表现出的成功，增加了LLMs成为完美智能工具的信心。然而，尽管GPT-4论文向我们展示了其在某些常识推理任务中的表现，但对GPT-4在常识推理任务上的全面评估，特别是现有的已经确立好的数据集上的评估还是缺失的。为此，我们关注GPT-4在广泛使用的CommonsenseQA数据集中的一套常识推理问题上的表现评估及其认知心理学工具。通过这样做，我们能够理解GPT-4如何在其语言生成过程中处理和整合常识知识，以及其在这方面的局限性。

    Commonsense reasoning is a basic ingredient of intelligence in humans, empowering the ability to deduce conclusions based on the observations of surroundings. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans such as medical exam, bar exam and others has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has shown performance on some common sense reasoning tasks, a comprehensive assessment of GPT-4 on common sense reasoning tasks, particularly on the existing well-established datasets is missing. In this study, we focus on the evaluation of GPT-4's performance on a set of common sense reasoning questions from the widely used CommonsenseQA dataset along with tools from cognitive psychology. In doing so, we understand how GPT-4 processes and integrates common sense k
    
[^45]: 通过计算设计提高人机协作能力

    Improving Human-Robot Collaboration via Computational Design. (arXiv:2303.11425v1 [cs.RO])

    [http://arxiv.org/abs/2303.11425](http://arxiv.org/abs/2303.11425)

    本论文通过厨房设计为例，研究合理的共享空间设计如何提高人机协作能力，使用分散式运动规划器有效解决多智能体运动规划问题，结果表明优化的厨房设计可以明显提高人机协作的性能。

    

    当机器人进入我们的日常生活时，人与机器人共享的空间对于有效的人机协作至关重要。共享空间的设计应满足人类的偏好和机器人的效率。本文以厨房设计为例，说明良好的空间设计在促进人机协作方面的重要性。在给定厨房边界、台面和菜谱的情况下，所提出的方法计算满足厨房设计规则并改善人机协作的台面的最佳摆放位置。主要技术挑战在于优化方法通常要评估数千种设计，而评估函数的一部分——运动规划的计算成本很高。我们使用了一种可高效解决多智能体运动规划的分散式运动规划器。我们的结果表明，优化的厨房设计可以明显提高人机协作的性能。

    When robots entered our day-to-day life, the shared space surrounding humans and robots is critical for effective Human-Robot collaboration. The design of shared space should satisfy humans' preferences and robots' efficiency. This work uses kitchen design as an example to illustrate the importance of good space design in facilitating such collaboration. Given the kitchen boundary, counters, and recipes, the proposed method computes the optimal placement of counters that meet the requirement of kitchen design rules and improve Human-Robot collaboration. The key technical challenge is that the optimization method usually evaluates thousands of designs and the computational cost of motion planning, which is part of the evaluation function, is expensive. We use a decentralized motion planner that can solve multi-agent motion planning efficiently. Our results indicate that optimized kitchen designs can provide noticeable performance improvement to Human-Robot collaboration.
    
[^46]: 融合时频和空间表示来改善基于EEG的情感识别

    Improving EEG-based Emotion Recognition by Fusing Time-frequency And Spatial Representations. (arXiv:2303.11421v1 [eess.SP])

    [http://arxiv.org/abs/2303.11421](http://arxiv.org/abs/2303.11421)

    论文提出了一种基于跨域特征融合方法的EEG信号分类网络，其中融合了时频域和空间域的多重表示方法，该方法在EEG情感识别中取得了最先进的结果。

    

    使用深度学习方法对EEG信号进行分类可以准确识别人们的情绪。然而，现有研究很少考虑将另一个领域的表示中的信息应用于时频域的特征选择。我们提出了一种基于跨域特征融合方法的EEG信号分类网络，通过使用多域注意机制，使网络更专注于与脑活动和思维变化最相关的特征。此外，我们提出了一个两步融合方法，并将这些方法应用于EEG情感识别网络中。实验结果表明，我们提出的融合时频域和空间域的多重表示方法，优于先前公开数据集上的方法，在当前取得了最先进的结果。

    Using deep learning methods to classify EEG signals can accurately identify people's emotions. However, existing studies have rarely considered the application of the information in another domain's representations to feature selection in the time-frequency domain. We propose a classification network of EEG signals based on the cross-domain feature fusion method, which makes the network more focused on the features most related to brain activities and thinking changes by using the multi-domain attention mechanism. In addition, we propose a two-step fusion method and apply these methods to the EEG emotion recognition network. Experimental results show that our proposed network, which combines multiple representations in the time-frequency domain and spatial domain, outperforms previous methods on public datasets and achieves state-of-the-art at present.
    
[^47]: ADCNet：带原始雷达ADC数据的端到端感知

    ADCNet: End-to-end perception with raw radar ADC data. (arXiv:2303.11420v1 [eess.SP])

    [http://arxiv.org/abs/2303.11420](http://arxiv.org/abs/2303.11420)

    本文提出了一种在原始雷达模拟数字（ADC）数据上执行端到端学习的方法，其中一个可学习的信号处理模块被嵌入网络中，实验结果证实了该方法的有效性。

    

    自动驾驶行业对雷达传感器的兴趣重新激发。雷达作为一种相对成熟的技术，在过去几年中得到了稳定的改进，使其成为常用的LiDAR的有吸引力的替代品或补充。一种新兴的趋势是利用丰富的低级别雷达数据进行感知。在这项工作中，我们将这个趋势推向了极端--我们提出了一种在原始雷达模拟数字（ADC）数据上执行端到端学习的方法。具体来说，我们在神经网络中设计了一个可学习的信号处理模块，并提供了一个由传统信号处理算法引导的预训练方法。实验结果证实了端到端学习方法的整体有效性，而消融研究验证了我们个体创新的有效性。

    There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.
    
[^48]: 基于深度学习的振动信号去噪方法

    Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])

    [http://arxiv.org/abs/2303.11413](http://arxiv.org/abs/2303.11413)

    本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。

    

    由脚步引起的结构振动信号被广泛用于人员识别、定位、人类活动推断、结构健康监测等任务。然而，由于环境噪声、电磁干扰等因素的影响，实际采集的信号通常会带有噪声。噪声的存在影响了信号处理过程，从而影响了最终任务的准确性和误差。本文主要探讨了基于深度学习的去除脚步引起的振动信号的噪声的方法。我们考虑了不同类型的噪声，包括高斯噪声和非平稳噪声等。

    Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
    
[^49]: GNN-Ensemble：面向随机决策图神经网络

    GNN-Ensemble: Towards Random Decision Graph Neural Networks. (arXiv:2303.11376v1 [cs.LG])

    [http://arxiv.org/abs/2303.11376](http://arxiv.org/abs/2303.11376)

    本文提出了一种名为GNN-Ensemble的方法，它可以构建随机决策图神经网络的集合，以提高GNNs的性能，泛化能力和抗攻击性，并遵循随机建模的原则。

    

    图神经网络（GNNs）在图结构数据方面广泛应用，但是现有的基于图的应用通常缺乏注释数据。GNNs需要从有限的训练数据中学习潜在的模式，以对大量的测试数据进行推断。GNNs的增加复杂性，以及单点模型参数初始化，通常会导致过度适应和次优性能。此外，众所周知GNNs易受到对抗性攻击。在本文中，我们提出了一种名为GNN-Ensemble的新方法，该方法遵循随机建模的原则，在拓扑上随机选择子结构中构建多个GNNs来构建随机决策图神经网络，其容量可以任意扩展，以提高性能和对抗鲁棒性。

    Graph Neural Networks (GNNs) have enjoyed wide spread applications in graph-structured data. However, existing graph based applications commonly lack annotated data. GNNs are required to learn latent patterns from a limited amount of training data to perform inferences on a vast amount of test data. The increased complexity of GNNs, as well as a single point of model parameter initialization, usually lead to overfitting and sub-optimal performance. In addition, it is known that GNNs are vulnerable to adversarial attacks. In this paper, we push one step forward on the ensemble learning of GNNs with improved accuracy, generalization, and adversarial robustness. Following the principles of stochastic modeling, we propose a new method called GNN-Ensemble to construct an ensemble of random decision graph neural networks whose capacity can be arbitrarily expanded for improvement in performance. The essence of the method is to build multiple GNNs in randomly selected substructures in the topo
    
[^50]: 神经约束满足：层级抽象实现组合式物体重新排列中的泛化能力

    Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement. (arXiv:2303.11373v1 [cs.LG])

    [http://arxiv.org/abs/2303.11373](http://arxiv.org/abs/2303.11373)

    该研究提出了一种神经网络约束满足的方法，通过层级结构的抽象实现了在组合状态下的物体重新排列任务的泛化能力。

    

    对于具有实体机器人智能的问题而言，物体重新排列任务是一个巨大挑战，因为解决这些任务需要泛化到极大组合的实体状态。更糟糕的是，这些实体的表示是未知的，必须从感官输入进行推断。我们提出了一种层次抽象的方法，以揭示这些底层实体，并从非结构化视觉输入中实现组合泛化。通过在像素聚类上构建一个因子化的转换图，我们展示了如何学习代理模型中实体状态和环境物体之间的对应关系。我们利用这种关系开发了一种控制方法，可泛化到不同数量和配置的物体，当在模拟重新排列任务上进行评估时，该方法胜过当前的离线深度强化学习方法。

    Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent's model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.
    
[^51]: Reflexion：具有动态记忆和自我反思的自主智能体

    Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])

    [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366)

    本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。

    

    最近决策大型语言模型（LLM）代理的发展在各种基准测试中展现出卓越的性能。然而，这些最先进的方法通常需要内部模型微调、外部模型微调或在定义的状态空间上进行策略优化。由于高质量训练数据的稀缺性或缺乏良好定义的状态空间，实现这些方法可能会具有挑战性。此外，这些代理没有人类决策过程固有的某些品质，特别是从错误中学习的能力。通过反思，人类可以通过试错过程高效地解决新的问题。在最近的研究基础上，我们提出 Reflexion，一种将动态记忆和自我反思能力赋予智能体的方法，以增强其现有的推理轨迹和任务特定的行动选择能力。为了实现完全自动化，我们介绍了一种简单而有效的方法。

    Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
    
[^52]: HDformer: 一种利用长距离血管信号进行糖尿病检测的高维Transformer

    HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals. (arXiv:2303.11340v1 [cs.LG])

    [http://arxiv.org/abs/2303.11340](http://arxiv.org/abs/2303.11340)

    本研究提出了一种新的基于高维Transformer的架构HDformer，并利用长距离PPG信号进行糖尿病检测，其中提出了一种新的注意力模块TSA，成功将标记体积减少10倍以上，提高了模型的能力和效率。

    

    糖尿病是全球性问题，早期检测有助于预防严重并发症。已出现将心血管信号纳入深度学习模型的低成本、非侵入式检测方法，但限制其临床应用的是有限的准确性。本文提出了一种新的基于Transformer的架构，即Higher Dimensional Transformer（HDformer），它利用长距离光电容积图（PPG）信号来检测糖尿病。相较于现有研究常用的不足一分钟的PPG信号，长距离PPG包含更广泛、更深入的信号上下文信息。为了增加处理长距离数据的能力和效率，我们提出了一种新的注意力模块Time Square Attention（TSA），将标记体积减少10倍以上，同时保留本地/全局依赖关系。它将一维输入 转换为二维表示，并将相邻点组成一个单独的2D标记。

    Diabetes mellitus is a worldwide concern, and early detection can help to prevent serious complications. Low-cost, non-invasive detection methods, which take cardiovascular signals into deep learning models, have emerged. However, limited accuracy constrains their clinical usage. In this paper, we present a new Transformer-based architecture, Higher Dimensional Transformer (HDformer), which takes long-range photoplethysmography (PPG) signals to detect diabetes. The long-range PPG contains broader and deeper signal contextual information compared to the less-than-one-minute PPG signals commonly utilized in existing research. To increase the capability and efficiency of processing the long range data, we propose a new attention module Time Square Attention (TSA), reducing the volume of the tokens by more than 10x, while retaining the local/global dependencies. It converts the 1-dimensional inputs into 2-dimensional representations and groups adjacent points into a single 2D token, using 
    
[^53]: FedMAE：带有单块遮蔽自编码器的联邦自监督学习

    FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder. (arXiv:2303.11339v1 [cs.LG])

    [http://arxiv.org/abs/2303.11339](http://arxiv.org/abs/2303.11339)

    本文提出了一个新的联邦自监督学习框架 FedMAE，可以利用轻量级设备上的大规模未标记图像进行联邦学习。FedMAE可以预训练一个单块遮蔽自编码器，并将多个预训练的单块MAE级联在服务器上构建用于下游任务的多块ViT骨干。实验结果表明，FedMAE相较于最先进的FSSL方法具有卓越的性能。

    

    最新的联邦学习方法开始关注如何利用用户设备中未标记的数据进行训练，原因是用户关注隐私，成本高，或者缺乏专业知识。然而，当前的Federated Semi-Supervised/Self-Supervised Learning（FSSL）方法由于本地客户端的有限计算资源而无法学习大规模图像。本文提出了一个新的框架FedMAE，即Federated Masked AutoEncoder，以解决如何利用未标记的大尺度图像进行联邦学习的问题。具体来说，FedMAE可以使用轻量级客户端设备中的大型图像预训练单块遮蔽自编码器（MAE），然后在服务器中级联多个预训练的单块MAE以构建下游任务的多块ViT骨干。 图像重建和分类的理论分析和实验结果表明，与最先进的FSSL方法相比，我们的FedMAE获得了更优秀的性能。

    Latest federated learning (FL) methods started to focus on how to use unlabeled data in clients for training due to users' privacy concerns, high labeling costs, or lack of expertise. However, current Federated Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn large-scale images because of the limited computing resources of local clients. In this paper, we introduce a new framework FedMAE, which stands for Federated Masked AutoEncoder, to address the problem of how to utilize unlabeled large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked AutoEncoder (MAE) using large images in lightweight client devices, and then cascades multiple pre-trained one-block MAEs in the server to build a multi-block ViT backbone for downstream tasks. Theoretical analysis and experimental results on image reconstruction and classification show that our FedMAE achieves superior performance compared to the state-of-the-art FSSL methods.
    
[^54]: 递归欧几里得距离基于鲁棒聚合技术的联邦学习方法

    Recursive Euclidean Distance Based Robust Aggregation Technique For Federated Learning. (arXiv:2303.11337v1 [cs.LG])

    [http://arxiv.org/abs/2303.11337](http://arxiv.org/abs/2303.11337)

    本文提出了一种递归欧几里得距离计算的鲁棒聚合方法来防御联邦学习中的恶意攻击，该方法分配权重以最小化数据污染效应，实验表明其精度优于现有算法并且时间复杂度降低。

    

    联邦学习作为解决机器学习中数据可用性和隐私挑战的解决方案已经变得非常受欢迎。然而，在联邦学习中，本地模型更新的聚合过程容易受到恶意攻击，如背包攻击、标签翻转和成员推断。恶意用户旨在通过恶意数据训练本地模型来破坏合作学习过程。本文提出了一种基于递归欧几里得距离计算的新型鲁棒聚合方法。我们的方法测量本地模型与之前全局模型的距离，并相应地分配权重。远离全局模型的本地模型被分配较小的权重，以最小化聚合过程中的数据污染效应。我们的实验表明，所提出的算法在精度方面的表现优于现有算法，且时间复杂度减少不超过$55\%$。我们的贡献有两个：1）我们提出了一种新的递归欧几里得距离聚合技术，以防范联邦学习中的恶意攻击。2）我们的实验证明了所提出算法的精度和时间复杂度的有效性。

    Federated learning has gained popularity as a solution to data availability and privacy challenges in machine learning. However, the aggregation process of local model updates to obtain a global model in federated learning is susceptible to malicious attacks, such as backdoor poisoning, label-flipping, and membership inference. Malicious users aim to sabotage the collaborative learning process by training the local model with malicious data. In this paper, we propose a novel robust aggregation approach based on recursive Euclidean distance calculation. Our approach measures the distance of the local models from the previous global model and assigns weights accordingly. Local models far away from the global model are assigned smaller weights to minimize the data poisoning effect during aggregation. Our experiments demonstrate that the proposed algorithm outperforms state-of-the-art algorithms by at least $5\%$ in accuracy while reducing time complexity by less than $55\%$. Our contribut
    
[^55]: 不看就能旋转: 通过触觉实现手部灵活性

    Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.10880](http://arxiv.org/abs/2303.10880)

    本研究提出了一种新系统Touch Dexterity，通过密集二进制力传感器实现了多指机器人手不看就能旋转物体，同时大大降低了成本和与实际应用的差距。

    

    触感信息在人类灵巧性中扮演着至关重要的角色，它可以提供有用的接触信息，直接从视觉中无法推断。这篇论文探讨了是否能够使多指机器人手具备与人类类似的不看就能旋转物体的能力。作者们提出了一个新的系统Touch Dexterity，通过使用覆盖整个机器人手的密集二进制力传感器（触摸或未触摸）代替仅仅在小区域内进行精准的触觉传感，使系统具有低成本、覆盖范围广等优点，并通过强化学习在多样的物体模拟中训练出了一种触感旋转策略，能够在真实的机器人手上直接实施不看就能旋转新型物体。

    Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
    
[^56]: RN-Net: 基于储备节点的神经元视觉感知网络

    RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network. (arXiv:2303.10770v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10770](http://arxiv.org/abs/2303.10770)

    本论文提出一种基于储备节点的神经元视觉感知网络（RN-Net）。RN-Net可以以低成本有效地处理异步的时间特征，并在DVS128手势上实现了迄今最高的准确率99.2％，在更小的网络尺寸下实现了DVS Lip数据集的67.5％准确率。

    

    事件相机受生物视觉系统稀疏和异步脉冲表示的启发。然而，处理事件数据要么需要使用昂贵的特征描述符将脉冲转换成帧，要么使用难以训练的脉冲神经网络。在本文中，我们提出了一种基于简单卷积层和动态时间编码储备的神经网络架构，具有低硬件和训练成本。使用储备节点的神经元视觉感知网络（RN-Net）使网络能够有效处理异步的时间特征，并实现了DVS128手势的迄今最高准确度99.2％，同时在更小的网络尺寸下实现了DVS Lip数据集的67.5％准确度。通过利用记忆电阻器的内部动态，可以以非常低的硬件成本实现异步时间特征编码，而不需要预处理或专用的存储器和算术单元。

    Event-based cameras are inspired by the sparse and asynchronous spike representation of the biological visual system. However, processing the even data requires either using expensive feature descriptors to transform spikes into frames, or using spiking neural networks that are difficult to train. In this work, we propose a neural network architecture based on simple convolution layers integrated with dynamic temporal encoding reservoirs with low hardware and training costs. The Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net) allows the network to efficiently process asynchronous temporal features, and achieves the highest accuracy of 99.2% for DVS128 Gesture reported to date, and one of the highest accuracy of 67.5% for DVS Lip dataset at a much smaller network size. By leveraging the internal dynamics of memristors, asynchronous temporal feature encoding can be implemented at very low hardware cost without preprocessing or dedicated memory and arithmetic units. T
    
[^57]: 基于视觉关系的图像字幕生成中的多模态奖励

    Multi-modal reward for visual relationships-based image captioning. (arXiv:2303.10766v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10766](http://arxiv.org/abs/2303.10766)

    本文提出了一种基于融合图像场景图的视觉关系信息与图像的空间特征映射的深度神经网络，引入多模态奖励函数进行深度强化学习，以优化图像字幕生成任务。实验结果表明，该方法在客观和主观评估指标上均优于基准。

    

    深度神经网络在自动图像字幕生成中取得了很好的效果，其有效的表示学习和基于上下文的内容生成能力是关键原因。然而，虽然 bottom-up 特征是许多最近图像字幕生成的深度功能的重要组成部分，这些特征与从原始图像直接提取的特征图相比提供了不同对象的详细表示，但是它们关于这些对象之间高层次语义信息的缺乏是它们的重要缺点，尽管其提取过程耗时且需耗费大量资源。为了利用视觉关系在字幕生成中的优势，本文提出了一种深度神经网络架构，它基于提取的图像场景图的视觉关系信息与图像的空间特征映射进行融合，然后引入了一个多模态奖励函数来进行深度强化学习，以优化图像字幕生成任务。实验结果表明，我们提出的方法在客观和主观评估指标上均优于基准。

    Deep neural networks have achieved promising results in automatic image captioning due to their effective representation learning and context-based content generation capabilities. As a prominent type of deep features used in many of the recent image captioning methods, the well-known bottomup features provide a detailed representation of different objects of the image in comparison with the feature maps directly extracted from the raw image. However, the lack of high-level semantic information about the relationships between these objects is an important drawback of bottom-up features, despite their expensive and resource-demanding extraction procedure. To take advantage of visual relationships in caption generation, this paper proposes a deep neural network architecture for image captioning based on fusing the visual relationships information extracted from an image's scene graph with the spatial feature maps of the image. A multi-modal reward function is then introduced for deep rei
    
[^58]: 自动驾驶路径规划：现状与展望

    Path Planning for Autonomous Driving: The State of the Art and Perspectives. (arXiv:2303.09824v1 [cs.RO])

    [http://arxiv.org/abs/2303.09824](http://arxiv.org/abs/2303.09824)

    本文综述了现有的自动驾驶路径规划方法，包括管道规划和端到端规划方法。在挑战和潜在解决方案方面提供了讨论，有助于为智能汽车的发展提供更好的规划方法。

    

    智能汽车由于提高的便利性、安全性和潜在的商业价值而受到广泛关注。但由于各种问题，如安全性、可靠性和规划方法的泛化等限制，它们的部署仍局限于小规模验证阶段。本文旨在综述最先进的路径规划方法，包括管道规划和端到端规划方法。针对管道方法，本文提供了选取算法的概述，并讨论了扩展和优化机制；针对端到端方法，本文强调培训和验证方法。此外，本文还讨论了挑战和潜在解决方案，这有助于为智能汽车的发展提供更好的规划方法。

    Intelligent vehicles (IVs) have attracted wide attention thanks to the augmented convenience, safety advantages, and potential commercial value. Although a few of autonomous driving unicorns assert that IVs will be commercially deployable by 2025, their deployment is still restricted to small-scale validation due to various issues, among which safety, reliability, and generalization of planning methods are prominent concerns. Precise computation of control commands or trajectories by planning methods remains a prerequisite for IVs, owing to perceptual imperfections under complex environments, which pose an obstacle to the successful commercialization of IVs. This paper aims to review state-of-the-art planning methods, including pipeline planning and end-to-end planning methods. In terms of pipeline methods, a survey of selecting algorithms is provided along with a discussion of the expansion and optimization mechanisms, whereas in end-to-end methods, the training approaches and verific
    
[^59]: 物理知识神经网络拓扑优化：应用于隐藏几何结构的非侵入式探测。

    Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])

    [http://arxiv.org/abs/2303.09280](http://arxiv.org/abs/2303.09280)

    该论文介绍了一种基于物理知识神经网络的拓扑优化方法，应用于无先验知识的几何结构检测，通过材料密度场表示任意解决方案拓扑，并通过Eikonal正则化实现。该方法可用于医疗和工业应用中的非侵入式成像技术。

    

    在医疗和工业应用中，通过电磁、声学或机械负载从表面测量中检测隐藏的几何结构是非侵入成像技术的目标。由于未知的拓扑和几何形状、数据的稀疏性以及物理规律的复杂性，解决逆问题是具有挑战性的。物理知识神经网络已经表现出许多优点，是一个简单而强大的问题反演工具，但它们尚未应用于具有先验未知拓扑的一般问题。在这里，我们介绍了一个基于PINNs的拓扑优化框架，它可以解决没有形状数量或类型先验知识的几何检测问题。我们允许任意的解决方案拓扑，通过使用材料密度场来表示几何形状，并通过新的Eikonal正则化接近二进制值。我们通过检测隐含虚空和包含物的数量、位置和形状来验证我们的框架。

    Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
    
[^60]: 基于t-SPN和滤波的细胞分类的最大间隔学习

    Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])

    [http://arxiv.org/abs/2303.09065](http://arxiv.org/abs/2303.09065)

    本研究提出了一种基于t-SPN算法和滤波技术的细胞分类方法，通过最大化边缘和L2正则化，该方法在HEp-2和Feulgen基准数据集上取得了最高的准确率。

    

    本文探讨了一种基于深度概率体系结构的算法，称为树形求和产品网络(t-SPN)，用于细胞分类。构建t-SPN的目的是表示未归一化概率作为最相似的细胞类别的条件概率。通过最大化边缘来学习构建的t-SPN体系结构，该边缘是真实标签和最有竞争力的错误标签之间的条件概率差。为了增强体系结构的泛化能力，在学习过程中考虑了L2正则化（REG）和最大间隔（MM）标准。为了突出细胞特征，本文探讨了两种通用的高通滤波器的有效性：理想高通滤波和拉普拉斯滤波(Log)。在HEp-2和Feulgen基准数据集上，基于最大间隔准则与正则化学习的t-SPN体系结构产生了最高的准确率。

    An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
    
[^61]: 通识知识辅助的资源受限和细粒度目标检测的深度学习方法

    Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])

    [http://arxiv.org/abs/2303.09026](http://arxiv.org/abs/2303.09026)

    本文提出了一种通识知识辅助的细粒度目标检测方法，利用通识知识推理模块处理由基准深度学习检测器给出的粗粒度标签，从而提高目标检测的准确性。经过实验验证，该方法相比于现有方法需要更少的计算量和标注资源。

    

    本文考虑边缘计算等资源受限场景下的细粒度图像目标检测问题。针对使用现代深度学习目标检测器时需要使用大型模型和大量数据标注的精准细粒度检测需求，提出一种方法，即利用通识知识辅助粗粒度目标检测器获取精准的细粒度检测结果。引入通识知识推理模块(CKIM)处理由基准深度学习检测器给出的粗粒度标签，从而生成细粒度标签。论文中考虑了模糊规则和清晰规则的推理，前者用于处理目标语义标签的模糊性。实验结果表明所提方法可以有效提高目标检测的准确性，同时相比于现有方法需要更少的计算量和标注资源。

    In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
    
[^62]: 探究数据隐私增强技术在AI治理应用中的相关性

    Exploring the Relevance of Data Privacy-Enhancing Technologies for AI Governance Use Cases. (arXiv:2303.08956v1 [cs.AI])

    [http://arxiv.org/abs/2303.08956](http://arxiv.org/abs/2303.08956)

    探究数据隐私增强技术对AI治理的重要性，将不同的AI治理目标视为系统信息流，强调解决方案之间的互操作性。

    

    随着隐私增强技术的发展，数据交换和分析中隐私与性能之间的权衡已经得到了极大的改善。类似的透明度结构工具对于AI治理也很有用，因为它们可以提供外部审查、审计和源验证等能力。为了避免治理上的重大漏洞和局限性，需要将这些不同的AI治理目标视为信息流系统。由于在本文提到的AI治理用例所需的软件栈中可能存在重叠，因此在整体上看待系统，了解这些不同的AI治理解决方案之间的互操作性变得很重要。因此，在这些标准、审计程序、软件和规范定型之前，首先紧急需要将AI治理中的这些问题作为系统来研究和解决。

    The development of privacy-enhancing technologies has made immense progress in reducing trade-offs between privacy and performance in data exchange and analysis. Similar tools for structured transparency could be useful for AI governance by offering capabilities such as external scrutiny, auditing, and source verification. It is useful to view these different AI governance objectives as a system of information flows in order to avoid partial solutions and significant gaps in governance, as there may be significant overlap in the software stacks needed for the AI governance use cases mentioned in this text. When viewing the system as a whole, the importance of interoperability between these different AI governance solutions becomes clear. Therefore, it is imminently important to look at these problems in AI governance as a system, before these standards, auditing procedures, software, and norms settle into place.
    
[^63]: WDiscOOD：通过白化线性判别分析进行区分度优化的OOD检测

    WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])

    [http://arxiv.org/abs/2303.07543](http://arxiv.org/abs/2303.07543)

    本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。

    

    深度神经网络容易在遇到未知概念的情形下产生过度自信但错误的预测。这个挑战突显了在开放世界中检测OOD样本的重要性。本文提出了一种新颖的特征空间OOD检测分数，同时结合了类别特定和类别不可知的信息。具体地，我们的方法使用白化线性判别分析将特征投影到两个子空间中——判别子空间和残留子空间，其中ID类在判别子空间中被最大化地分离，并在残差子空间中被紧密地聚类。然后，在两个子空间中将来自输入数据与ID分布的偏差组合起来确定OOD分数。我们的方法名为WDiscOOD，在覆盖多种分布偏移的六个OOD数据集上验证了其高效性，包括大规模ImageNet-1k基准测试。WDiscOOD在深度分类器上表现出了优越的性能。

    Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
    
[^64]: 具有别名观测的潜在图的快速探索与学习

    Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])

    [http://arxiv.org/abs/2303.07397](http://arxiv.org/abs/2303.07397)

    本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。

    

    考虑这种场景：一个智能体通过执行操作从一个节点到另一个节点来导航潜在图。所选操作确定了下一个访问节点上的概率分布。在每个节点处，智能体收到一个观测，但该观测不是唯一的，因此它不能唯一地标识节点，这使得问题别名化。本文旨在提供一个政策，该政策约等于最大化探索效率（即在给定的探索预算下如何恢复图表）。在非别名化的情况下，我们展示了相对于现有最先进强化学习基线的改进性能。对于别名化的情况，我们不知道适用的基线，而是展示了在各种拓扑结构下相对于随机策略更快的恢复速度，并且对于具有挑战性的拓扑结构，恢复速度比随机策略快指数倍。我们将该算法称为 eFeX（来自于 efficient exploration 的缩写）。

    Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
    
[^65]: 为什么这是一个好的或不是一个好的煎锅？——对象和工具功能的知识表示，用于设计理解、改进和生成

    Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation. (arXiv:2303.06152v1 [cs.AI])

    [http://arxiv.org/abs/2303.06152](http://arxiv.org/abs/2303.06152)

    本文演示了如何使用通用函数表示语言和框架来表示特定对象及其参与支持其设计的过程，从而实现深入的概念理解，可解释性的功能，使系统能够回答“为什么”问题。

    This paper demonstrates how a particular object and its participation in the processes it is designed to support can be represented in a general function representational language and framework, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions.

    对象和工具的功能方面的理解对于支持智能系统在环境中导航和与各种对象、结构和系统进行交互以帮助实现其目标至关重要。对功能的详细理解也可以导致设计改进和新颖的设计，从而增强人工智能和机器人系统的操作，一方面，增强人类生活。本文演示了如何使用通用函数表示语言和框架来表示特定对象（在本例中为煎锅）及其参与支持其设计的过程（在本例中为煎炸过程），从而可以用于详细说明涉及的过程和功能，从而实现深入的概念理解，可解释性的功能，使系统能够回答“为什么”问题——为什么这是一个好的煎锅，或者为什么某个部件在煎锅中的位置很重要。

    The understanding of the functional aspects of objects and tools is of paramount importance in supporting an intelligent system in navigating around in the environment and interacting with various objects, structures, and systems, to help fulfil its goals. A detailed understanding of functionalities can also lead to design improvements and novel designs that would enhance the operations of AI and robotic systems on the one hand, and human lives on the other. This paper demonstrates how a particular object - in this case, a frying pan - and its participation in the processes it is designed to support - in this case, the frying process - can be represented in a general function representational language and framework, that can be used to flesh out the processes and functionalities involved, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions - why is something a good frying pan, say, or why a certain part on t
    
[^66]: CVT-SLR：基于对比视觉-文本变换与变分对齐的手语识别模型

    CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment. (arXiv:2303.05725v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05725](http://arxiv.org/abs/2303.05725)

    CVT-SLR是一种新的手语识别模型，它采用基于对比视觉-文本变换和变分对齐的方法来充分利用跨模态知识，为解决手语识别中缺乏大规模可用数据集的问题提供了一种新的解决方案。

    

    手语识别 (SLR) 是一项弱监督任务，可以将手语视频注释为文本。最近的研究表明，由于缺乏大规模可用的手语数据集而导致的不充分训练成为 SLR 的主要瓶颈。因此，大多数 SLR 的工作采用预训练的视觉模块，并开发了两种主流解决方案。本文提出了一种新型的对比视觉-文本变换模型 CVT-SLR，充分发挥了视觉和语言模态的预训练知识。

    Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign language datasets becomes the main bottleneck for SLR. The majority of SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variation
    
[^67]: ChatGPT已在地平线上：大语言模型是否就是我们需要的智能交通解决方案？

    ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.05382](http://arxiv.org/abs/2303.05382)

    本文探讨了ChatGPT在解决交通问题方面的应用。通过利用具有跨模态编码器的LLM，可以处理来自不同模态的交通数据并执行交通运营。作者提供了一个基于智能手机的碰撞报告自动生成和分析框架作为用例展示了这种潜力。

    

    ChatGPT是由OpenAI开发的具有60亿参数的重要大语言模型之一。ChatGPT展示了LLM的卓越的语言理解能力，特别是在生成对话响应方面。随着LLM在各种研究或工程领域越来越受到关注，现在是时候设想LLM如何革新我们处理智能交通系统的方式了。本文探讨了LLM在解决关键交通问题方面的未来应用。通过利用具有跨模态编码器的LLM，智能系统还可以处理来自不同模态的交通数据并通过LLM执行交通运营。我们提出并验证了LLM装备的这些潜在的交通应用。为了进一步证明这种潜力，我们还提供了一个具体的基于智能手机的碰撞报告自动生成和分析框架作为用例。尽管存在潜在的益处，但与数据隐私相关的挑战仍然存在。

    ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
    
[^68]: QAID：启发式的Few-shot意图检测方法

    QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01593](http://arxiv.org/abs/2303.01593)

    本文提出了一个启发式的Few-shot意图检测方法，通过将意图检测重新定义为一个问题-回答检索任务来解决语义相似的细粒度意图问题，结果在三个few-shot意图检测基准测试上取得了最优表现。

    

    意图检测涉及到一些语义相似的细粒度意图，是一个具有挑战性的任务。为了解决这个问题，我们将意图检测重新定义为一个问题-回答检索任务，将话语和意图名作为问题和答案。为此，我们利用了一个问题-回答检索体系结构，并采用了一个两阶段培训模式，其中包括批量对比损失。在预训练阶段，我们通过自我监督培训来改善查询表示。然后，在微调阶段中，我们增加了查询和同一意图答案之间的上下文化令牌级相似度分数。我們在三个few-shot意图检测基准测试上的结果达到了最优表现。

    Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.
    
[^69]: 基于全同态加密的隐私保护树型推理

    Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption. (arXiv:2303.01254v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.01254](http://arxiv.org/abs/2303.01254)

    本研究介绍了一种基于全同态加密的数据隐私保护方法，能够针对加密表格数据进行任意计算，并得到了最新的解决方案，适用于一系列树型模型，包括决策树，随机森林和梯度增强树。此方法已应用在Concrete-ML开源库中，能够在准确性方面接近未受保护的版本。

    

    隐私增强技术(PETs)被提出作为一种保护数据隐私同时允许数据分析的方式。在本文中，我们关注一种强大的工具——全同态加密(FHE)，它允许对加密数据进行任意计算。我们展示了如何将FHE应用于基于树型模型的数据分析中，得到了针对加密表格数据的最新解决方案。我们证明了该方法适用于一系列树型模型，包括决策树，随机森林和梯度增强树，并已实现在Concrete-ML库中，该库在https://github.com/zama-ai/concrete-ml. 开源。通过选择一组应用案例，我们证明了我们的FHE版本在准确性方面非常接近未受保护的版本。

    Privacy enhancing technologies (PETs) have been proposed as a way to protect the privacy of data while still allowing for data analysis. In this work, we focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for arbitrary computations to be performed on encrypted data. FHE has received lots of attention in the past few years and has reached realistic execution times and correctness.  More precisely, we explain in this paper how we apply FHE to tree-based models and get state-of-the-art solutions over encrypted tabular data. We show that our method is applicable to a wide range of tree-based models, including decision trees, random forests, and gradient boosted trees, and has been implemented within the Concrete-ML library, which is open-source at https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we demonstrate that our FHE version is very close to the unprotected version in terms of accuracy.
    
[^70]: Vid2Seq: 用于密集视频字幕的视觉语言模型的大规模预训练

    Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.14115](http://arxiv.org/abs/2302.14115)

    本文介绍了 Vid2Seq，这是一个在大规模 narrated 视频数据集上预先训练的多模态密集事件字幕模型。通过将转录语音的句子边界转化为伪事件边界，并使用转录语音句子作为伪事件字幕，我们有效利用未标注 narrated 视频数据集进行了密集视频字幕的训练。该模型在多个基准测试中表现出色，是目前最优秀的模型之一。

    

    本文介绍了 Vid2Seq，这是一个多模态的单级密集事件字幕模型，它是在大规模 narrated 视频数据集上预先训练的。 Vid2Seq 架构通过特殊的时间标记来增强语言模型，使其能够无缝地预测事件边界和文本描述。我们展示了如何利用未标注 narrated 视频数据集进行密集视频字幕的训练，通过将转录语音的句子边界转化为伪事件边界，并使用转录语音句子作为伪事件字幕。使用 YT-Temporal-1B 数据集预训练的 Vid2Seq 模型在各种密集视频字幕基准测试中均表现出色，包括 YouCook2、ViTT 和 ActivityNet Captions。 Vid2Seq 还可以很好地推广到视频段落字幕和视频片段字幕的任务中。

    In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captionin
    
[^71]: 可解释性人工智能无法提供最终用户所要求的解释

    Explainable AI does not provide the explanations end-users are asking for. (arXiv:2302.11577v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.11577](http://arxiv.org/abs/2302.11577)

    可解释性人工智能对提高人工智能系统信任度的效果有局限性，透明度和严格的验证更适合打造可靠的人工智能系统。

    

    可解释性人工智能（XAI）技术经常被许多人工智能系统的用户要求使用，旨在了解复杂模型及其相关预测，并建立信任。虽然在开发的某些特定任务中是适用的，但组织采用这些技术来增强对机器学习系统的信任时，会产生意想不到的后果。在本文中，我们讨论了XAI在部署中的局限性，并得出结论认为透明度和严格的验证更适合获得人工智能系统的信任。

    Explainable Artificial Intelligence (XAI) techniques are frequently required by users in many AI systems with the goal of understanding complex models, their associated predictions, and gaining trust. While suitable for some specific tasks during development, their adoption by organisations to enhance trust in machine learning systems has unintended consequences. In this paper we discuss XAI's limitations in deployment and conclude that transparency alongside with rigorous validation are better suited to gaining trust in AI systems.
    
[^72]: 基于非高斯空间内在有向结构的分类学扩展

    DNG: Taxonomy Expansion by Exploring the Intrinsic Directed Structure on Non-gaussian Space. (arXiv:2302.11165v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11165](http://arxiv.org/abs/2302.11165)

    本文提出了一种基于非高斯空间内在有向结构的分类学扩展算法DNG，通过明确表示每个节点的继承特征和增量特征的组合来挖掘结构信息，并成功捕捉节点之间is-a关系的方向，效果优于现有方法。

    

    分类学扩展是将大量其他节点（即“查询”）纳入现有分类学（即“种子”）的过程，其中最重要的步骤是选择每个查询的适当位置。虽然通过探索种子结构进行了大量努力，但现有方法在两方面挖掘结构信息存在不足：对层次语义的建模较差，无法捕捉is-a关系的方向性。本文试图通过将每个节点明确表示为继承特征（即结构部分）和增量特征（即补充部分）的组合来解决这些问题。具体而言，继承特征源自“父”节点，并加上一个继承因子的权重。有了这种节点表示法，分类学中的语义层次结构（即从“父级”到“子级”的继承和累积特征）可以被体现。此外，基于这些表示，可以捕捉节点之间is-a关系的方向，使模型能够区分不同方向的关系。提出了一种名为DNG（Directed Non-Gaussian）的算法，通过探索非高斯空间上的内在有向结构来扩展分类学。实验结果表明，DNG在效率和有效性方面均优于现有方法。

    Taxonomy expansion is the process of incorporating a large number of additional nodes (i.e., "queries") into an existing taxonomy (i.e., "seed"), with the most important step being the selection of appropriate positions for each query. Enormous efforts have been made by exploring the seed's structure. However, existing approaches are deficient in their mining of structural information in two ways: poor modeling of the hierarchical semantics and failure to capture directionality of is-a relation. This paper seeks to address these issues by explicitly denoting each node as the combination of inherited feature (i.e., structural part) and incremental feature (i.e., supplementary part). Specifically, the inherited feature originates from "parent" nodes and is weighted by an inheritance factor. With this node representation, the hierarchy of semantics in taxonomies (i.e., the inheritance and accumulation of features from "parent" to "child") could be embodied. Additionally, based on this rep
    
[^73]: 基于活体得分的回归神经网络用于面部反欺诈

    Liveness score-based regression neural networks for face anti-spoofing. (arXiv:2302.09461v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09461](http://arxiv.org/abs/2302.09461)

    本文提出了一种新的基于活体得分的方法来克服对第三方网络和用户的依赖，并使用了一种新的标签技术来产生表示与真实图像相关的信息量的离散化标签。大量实验表明，该方法优于以前的方法。

    

    先前的反欺诈方法是使用伪地图或用户定义标签，每种方法的表现都依赖于第三方网络生成伪地图的准确性以及用户定义标签的方式。本文提出了一种基于活体得分的回归网络来克服对第三方网络和用户的依赖。首先，我们引入了一种新的标签技术，称为伪分离标签编码，用于产生表示与真实图像相关的信息量的离散化标签。其次，我们提出了期望的活体得分，基于回归网络训练提出的监督和期望的活体得分的差异。最后，在四个面部反欺诈基准测试上进行了大量实验，验证了我们提出的方法在内和跨数据集测试中的有效性。实验结果表明，我们的方法优于以前的方法。

    Previous anti-spoofing methods have used either pseudo maps or user-defined labels, and the performance of each approach depends on the accuracy of the third party networks generating pseudo maps and the way in which the users define the labels. In this paper, we propose a liveness score-based regression network for overcoming the dependency on third party networks and users. First, we introduce a new labeling technique, called pseudo-discretized label encoding for generating discretized labels indicating the amount of information related to real images. Secondly, we suggest the expected liveness score based on a regression network for training the difference between the proposed supervision and the expected liveness score. Finally, extensive experiments were conducted on four face anti-spoofing benchmarks to verify our proposed method on both intra-and cross-dataset tests. The experimental results show our approach outperforms previous methods.
    
[^74]: GPT4MIA: 利用生成预训练变压器 (GPT-3) 作为插入式检验模型进行医学图像分析

    GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis. (arXiv:2302.08722v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08722](http://arxiv.org/abs/2302.08722)

    本文提出 GPT4MIA 方法，利用 GPT-3 作为插入式检验工具进行医学图像分析；该方法在提示结构设计、样本选择及提示排序等方面优化，能有效提高预测准确性。

    

    本文提出了一种称为 GPT4MIA 的新方法，利用生成预训练变压器 (GPT) 作为插入式检验工具，用于医学图像分析 (MIA)。我们提供了理论分析，解释了为什么像 GPT-3 这样的大型预训练语言模型可以作为插入式检验模型用于 MIA。在方法学层面上，我们开发了几种技术处理方法，包括更好的提示结构设计、样本选择以及代表性样本/特征的提示排序，以提高 GPT4MIA 的效率和有效性。我们呈现了两种具体的 GPT4MIA 使用案例 (带有工作流程)：(1) 检测预测错误和 (2) 改进预测准确性，与已经建立的基于视觉的图像分类模型 (例如 ResNet) 协同工作。实验验证了我们提出的方法对于这两个任务的有效性。我们进一步讨论了利用基于变压器的大型语言模型进行 MIA 任务的机会和挑战。

    In this paper, we propose a novel approach (called GPT4MIA) that utilizes Generative Pre-trained Transformer (GPT) as a plug-and-play transductive inference tool for medical image analysis (MIA). We provide theoretical analysis on why a large pre-trained language model such as GPT-3 can be used as a plug-and-play transductive inference model for MIA. At the methodological level, we develop several technical treatments to improve the efficiency and effectiveness of GPT4MIA, including better prompt structure design, sample selection, and prompt ordering of representative samples/features. We present two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction errors and (2) improving prediction accuracy, working in conjecture with well-established vision-based models for image classification (e.g., ResNet). Experiments validate that our proposed method is effective for these two tasks. We further discuss the opportunities and challenges in utilizing Transformer-based large
    
[^75]: Adap-$\tau$:自适应调整嵌入的幅度用于推荐

    Adap-$\tau$: Adaptively Modulating Embedding Magnitude for Recommendation. (arXiv:2302.04775v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2302.04775](http://arxiv.org/abs/2302.04775)

    本研究提出了一种自适应归一化方案Adap-$\tau$，通过动态调节每个用户-每个物品对的嵌入幅度，实现了理想的推荐性能，方法在四个真实世界的数据集上都超过了基准方法。

    

    最近几年来，基于嵌入的方法在推荐系统中取得了巨大的成功。尽管它们的性能还不错，但我们认为这些方法可能存在一个潜在的限制——嵌入幅度没有明确调节，这可能加剧流行度偏见和训练不稳定性，从而阻碍模型做出好的推荐。这促使我们利用嵌入归一化来推荐。通过将用户/物品嵌入归一化为特定值，我们在四个真实世界的数据集上实证观察到了令人满意的性能提升（平均9％）。虽然这是令人鼓舞的，但我们也揭示了在推荐中应用归一化的严重局限性——性能高度敏感于控制标准化嵌入比例的温度τ的选择。为了充分发挥归一化的优点并避免其局限性，本研究研究了如何自适应设置适当的τ。为此，我们首先提出了一个理论框架，描述了推荐中归一化操作与偏差-方差折衷之间的关系。然后，我们设计了一种自适应归一化方案，名为Adap-$\tau$，它动态调节每个用户-每个物品对的嵌入幅度，旨在实现理想的推荐性能。在四个真实世界的数据集上的广泛实验表明，Adap-$\tau$始终优于强基准方法，并实现了最先进的性能。

    Recent years have witnessed the great successes of embedding-based methods in recommender systems. Despite their decent performance, we argue one potential limitation of these methods -- the embedding magnitude has not been explicitly modulated, which may aggravate popularity bias and training instability, hindering the model from making a good recommendation. It motivates us to leverage the embedding normalization in recommendation. By normalizing user/item embeddings to a specific value, we empirically observe impressive performance gains (9\% on average) on four real-world datasets. Although encouraging, we also reveal a serious limitation when applying normalization in recommendation -- the performance is highly sensitive to the choice of the temperature $\tau$ which controls the scale of the normalized embeddings.  To fully foster the merits of the normalization while circumvent its limitation, this work studied on how to adaptively set the proper $\tau$. Towards this end, we firs
    
[^76]: 学习双层知识图谱的表示以进行超越链接预测的推理

    Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02601](http://arxiv.org/abs/2302.02601)

    本文提出了一种基于双层知识图谱的方法来学习嵌入，将三元组之间的关系考虑进去，并使用数据增强策略来增加合理的三元组。

    

    知识图谱使用三元组来表示已知事实。现有的知识图谱嵌入方法仅考虑实体之间的连接，而本文提出考虑三元组之间的关系。本文定义了一个更高级的三元组来表示三元组之间的关系，例如，$\langle T_1$, PrerequisiteFor, $T_2\rangle$，其中PrerequisiteFor是更高级别的关系。我们定义一个由基本级别和更高级别的三元组组成的双层知识图谱。我们还提出了一种基于双层知识图谱上的随机游走的数据增强策略来增加合理的三元组。我们的模型BiVE通过考虑基本级别和更高级别三元组的结构来学习嵌入。

    Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
    
[^77]: 通过自我对战实现多样化诱导的环境设计

    Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02119](http://arxiv.org/abs/2302.02119)

    本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。

    

    最近关于环境分布设计的研究已经展示出训练有效的通用能力代理的前景。它的成功部分在于一种自适应课程学习的形式，该形式通过生成代理能力的前沿环境实例（或级别）。然而，这种环境设计框架经常在具有挑战性的设计空间中发现有效级别方面存在困难，并需要与环境进行高成本交互。本文的目的是在非监督环境设计（UED）框架中引入多样性。具体来说，我们提出了一种任务不可知的方法来识别对给定级别具有代表性的观察/隐藏状态。然后利用这种方法的结果来表征两个级别之间的多样性，正如我们所展示的，这对于有效性能至关重要。此外，为了提高采样效率，我们加入了自我对战技术，使得环境生成器能够自动生成环境。

    Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
    
[^78]: 风险敏感的强化学习算法：指数标准的应用

    Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.09010](http://arxiv.org/abs/2212.09010)

    本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。

    

    尽管风险中性的强化学习已经在很多应用中得到了实验成功，但是这种方法容易受到噪声和系统参数扰动的影响而不够稳健。因此,对风险敏感的强化学习算法进行了研究，以提高其系统抗干扰性，样本效率和实用性。本文介绍了一种新型的无模型风险敏感学习算法，将广泛使用的策略梯度算法进行变体，其实现过程类似。具体来说，本文研究了指数标准对强化学习代理的策略风险敏感性的影响，并开发了蒙特卡罗策略梯度算法和在线(时间差分)演员-评论家算法的变体。分析结果表明，指数标准的使用能够推广常用的特定正则化方法。作者在摆动杆和摆摆杆任务上进行了测试，验证了所提出的算法的实现性能和稳健性。

    While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
    
[^79]: 多分辨率在线确定性退火：一种分层和渐进学习架构

    Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture. (arXiv:2212.08189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08189](http://arxiv.org/abs/2212.08189)

    本文提出了一种基于逐渐增加子集数量的分区序列的通用的分层学习结构，并使用无梯度随机逼近更新进行在线解决优化问题的方法，可以定义函数逼近问题并使用双时间尺度随机逼近算法的理论解决，模拟了一种退火过程。

    

    随着时间和计算资源的限制，逐步逼近基于数据的优化问题的解决方案的分层学习算法对于决策系统至关重要。本研究提出了一种通用的分层学习结构，基于可能的多分辨率数据空间的渐进分区。最优分区通过解决一系列优化子问题逐步逼近，生成具有逐渐增加的子集数量的分区序列。我们展示对每个优化问题的解可以使用无梯度随机逼近更新进行在线估计。因此，可以在分区的每个子集中定义函数逼近问题，并使用双时间尺度随机逼近算法的理论解决。这模拟了一种退火过程，并定义了一种强大且可解释的启发式方法，逐步增加复杂性。

    Hierarchical learning algorithms that gradually approximate a solution to a data-driven optimization problem are essential to decision-making systems, especially under limitations on time and computational resources. In this study, we introduce a general-purpose hierarchical learning architecture that is based on the progressive partitioning of a possibly multi-resolution data space. The optimal partition is gradually approximated by solving a sequence of optimization sub-problems that yield a sequence of partitions with increasing number of subsets. We show that the solution of each optimization problem can be estimated online using gradient-free stochastic approximation updates. As a consequence, a function approximation problem can be defined within each subset of the partition and solved using the theory of two-timescale stochastic approximation algorithms. This simulates an annealing process and defines a robust and interpretable heuristic method to gradually increase the complexi
    
[^80]: 基于基础模型反馈的策略适应

    Policy Adaptation from Foundation Model Feedback. (arXiv:2212.07398v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07398](http://arxiv.org/abs/2212.07398)

    本文提出了基于基础模型反馈的策略适应（PAFF）方法，通过让策略使用随机生成的指令进行演示，并利用预训练的基础模型提供反馈来重新标记演示，自动提供新的演示-指令数据对进行策略微调，以实现机器人操作的泛化。实验结果表明，PAFF优于现有最先进的方法。

    

    最近在视觉-语言基础模型方面的进展为构建通用机器人带来了显著进步。通过使用预训练模型将场景和指令编码为决策输入，指令条件化策略可以在不同的对象和任务之间进行泛化。尽管这是令人鼓舞的，但策略在遇到未见过的任务或环境时仍然失败。在本工作中，我们提出了一种基于基础模型反馈的策略适应（PAFF）。当将训练好的策略部署到新任务或新环境时，我们首先让策略使用随机生成的指令进行演示。虽然执行可能出现错误，但我们可以利用预训练的基础模型提供反馈来重新标记演示。这自动为策略微调提供了新的演示-指令数据对。我们在机器人操作设置中进行了各种实验的评估，重点是在未见过的对象、任务和未观察到的环境中的泛化。我们的实验结果表明，PAFF在最终任务成功率和训练效率方面优于现有最先进的方法。

    Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unse
    
[^81]: 药物靶点相互作用预测的细粒度选择性相似性集成

    Fine-Grained Selective Similarity Integration for Drug-Target Interaction Prediction. (arXiv:2212.00543v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.00543](http://arxiv.org/abs/2212.00543)

    本文提出了一种细粒度选择性相似性集成方法（FGS）来提高药物靶点相互作用预测的效果，采取局部的方法来精细选择性集成药物和靶点相似性，并自适应地学习不同相似性视图的权重。在多个基准数据集上的实验结果表明，FGS优于现有的最先进的相似性集成方法进行DTI预测。

    

    药物靶点相互作用（DTIs）的发现是药物开发中至关重要的过程。计算方法是从众多候选项中预测新型DTIs的有希望和高效的替代方法，避免了繁琐和昂贵的湿实验。最近，由于有了来自不同数据源的丰富异构生物信息，计算方法已经能够利用多个药物和靶标的相似性来提高DTI预测性能。相似性集成是一种有效和灵活的策略，能够提取补充相似性视图中的关键信息，为任何基于相似性的DTI预测模型提供压缩输入。然而，现有的相似性集成方法从全局角度过滤和融合相似性，忽略了每个药物和靶点相似性视图的实用性。在本研究中，我们提出了一种称为FGS的细粒度选择性相似性集成方法，采用一种局部的方法来精细选择性集成相似性。FGS基于各自的邻域结构集成药物和靶点相似性，并允许模型自适应地学习不同相似性视图的权重。在各种基准数据集上的实验结果表明，FGS始终优于现有最先进的相似性集成方法来进行DTI预测。

    The discovery of drug-target interactions (DTIs) is a pivotal process in pharmaceutical development. Computational approaches are a promising and efficient alternative to tedious and costly wet-lab experiments for predicting novel DTIs from numerous candidates. Recently, with the availability of abundant heterogeneous biological information from diverse data sources, computational methods have been able to leverage multiple drug and target similarities to boost the performance of DTI prediction. Similarity integration is an effective and flexible strategy to extract crucial information across complementary similarity views, providing a compressed input for any similarity-based DTI prediction model. However, existing similarity integration methods filter and fuse similarities from a global perspective, neglecting the utility of similarity views for each drug and target. In this study, we propose a Fine-Grained Selective similarity integration approach, called FGS, which employs a local 
    
[^82]: 多智能体强化学习在自主路由和接送问题中的应用，可适应需求的变化

    Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand. (arXiv:2211.14983v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2211.14983](http://arxiv.org/abs/2211.14983)

    本论文提出了一种多智能体强化学习的学习框架，用于在城市地图上服务于随机出现的请求，可以产生协调作用并考虑先前可能出现的未来请求，能够适应不同需求分布的变化。

    

    我们推导出一个学习框架，用于为一组自主车辆在城市地图上服务于随机出现的请求时生成路由/接送策略。我们着重研究的策略是：1）产生协调作用，从而减少为服务请求等待的时间；2）是非近视策略，并考虑先前可能出现的未来请求；3）可以适应基础需求分布的变化。具体来说，我们感兴趣的策略是适应城市环境中实际需求条件的波动，例如高峰时间和非高峰时间等。我们通过以下方式实现：(i)能够改进离线训练策略性能的在线玩算法，和(ii)一种离线逼近方案，允许适应基于需求模型的变化。特别地，我们通过计算Wasserstein距离的q-valid半径来量化有效区域，从而实现对我们已学习策略对不同需求分布的适应性。我们的实验结果表明，我们提出的框架可以比基线启发式策略改善平均等待时间，并能够适应不断变化的需求模型。

    We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wass
    
[^83]: 收集交互多模态数据集以进行基础语言理解

    Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.06552](http://arxiv.org/abs/2211.06552)

    本文提出了一个通过自然语言任务与协作体验智能体交互收集数据集的方法，并收集了首个交互基础语言理解数据集。

    

    人类智慧能够迅速适应新的任务和环境。从很小的时候开始，人类通过模仿他人的行为或按照提供的自然语言指令学习新技能并学会解决新任务。为了促进研究能够在机器中实现类似功能的方法，本文作出了以下贡献：（1）形式化协作体验智能体使用自然语言任务；（2）开发了一个可进行广泛和可扩展的数据收集工具；（3）收集了首个交互基础语言理解数据集。

    Human intelligence can remarkably adapt quickly to new tasks and environments. Starting from a very young age, humans acquire new skills and learn how to solve new tasks either by imitating the behavior of others or by following provided natural language instructions. To facilitate research which can enable similar capabilities in machines, we made the following contributions (1) formalized the collaborative embodied agent using natural language task; (2) developed a tool for extensive and scalable data collection; and (3) collected the first dataset for interactive grounded language understanding.
    
[^84]: 恢复脑功能的神经协处理器：基于抓取的皮层模型的结果

    Neural Co-Processors for Restoring Brain Function: Results from a Cortical Model of Grasping. (arXiv:2210.11478v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2210.11478](http://arxiv.org/abs/2210.11478)

    本文提出了一种神经协处理器，利用人工神经网络和深度学习学习最优闭环刺激策略，实现了针对性修复和康复。并利用皮层模型的模拟，为神经协处理器的未来体内测试奠定基础。

    

    目的：设计闭环脑机接口的一个主要挑战是找到不同受试者和目标的持续神经活动的最佳刺激模式。方法：为了实现目标导向的闭环神经刺激，我们提出了“神经协处理器”，它使用人工神经网络和深度学习来学习最优闭环刺激策略，塑造神经活动和连接受损的神经回路进行有针对性的修复和康复。协处理器随着生物电路自身适应刺激而调整刺激策略，实现了一种脑-设备协同适应形式。在这里，我们使用模拟为神经协处理器的未来体内测试奠定基础。我们利用一个用于抓取的皮层模型，对其应用了各种形式的模拟损伤，从而使我们能够开发关键的学习算法并研究对非稳态的适应性。主要结果：我们的模拟表明，神经协处理器能够从不同的模拟損傷中，学会自适应的闭环刺激策略，并促进神经电路的修复。

    Objective: A major challenge in designing closed-loop brain-computer interfaces is finding optimal stimulation patterns as a function of ongoing neural activity for different subjects and objectives. Approach: To achieve goal-directed closed-loop neurostimulation, we propose "neural co-processors" which use artificial neural networks and deep learning to learn optimal closed-loop stimulation policies, shaping neural activity and bridging injured neural circuits for targeted repair and rehabilitation. The co-processor adapts the stimulation policy as the biological circuit itself adapts to the stimulation, achieving a form of brain-device co-adaptation. Here we use simulations to lay the groundwork for future in vivo tests of neural co-processors. We leverage a cortical model of grasping, to which we applied various forms of simulated lesions, allowing us to develop the critical learning algorithms and study adaptations to non-stationarity. Main results: Our simulations show the ability
    
[^85]: MixMask: 重新审视Siamese ConvNets的遮盖策略

    MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.11456](http://arxiv.org/abs/2210.11456)

    本文提出了一种新的填充式遮盖策略MixMask，在Siamese ConvNets中实现遮盖和对比学习目标的匹配，提高了Siamese ConvNets的性能并在多个基准测试中实现了最先进的结果。

    

    最近自监督学习的进展将Masked Image Modeling（MIM）和Siamese网络整合成一个统一的框架，利用了两种技术的优点。然而，在Siamese ConvNets中应用传统的基于擦除的遮盖策略时，存在一些未解决的问题，包括（I）在连续处理数据时不能放弃不相关的遮盖区域，导致训练效率低于ViT模型;（II）基于擦除的遮盖与Siamese ConvNets中的对比学习目标不匹配，与MIM方法不同。本文提出了一种称为MixMask的填充式遮盖策略，以防止香草遮盖方法中图像中的随机遮盖区域导致信息不完整。此外，我们引入了一种灵活的损失函数设计，考虑两个不同混合视图之间的语义距离变化，以适应集成架构并防止遮盖和对比学习目标之间的不匹配。实验表明，MixMask显着提高了Siamese ConvNets的性能，并在几个基准测试中实现了最先进的结果。

    Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
    
[^86]: 缓解自然语言系统中隐蔽不安全的文本

    Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.09306](http://arxiv.org/abs/2210.09306)

    本文讨论了智能技术中日益普遍的文本安全问题，并强调了一个被忽视的类别：隐蔽不安全文本。该文提出了缓解策略以解决这一问题，以提高智能系统内部的安全性。

    

    智能技术中一个日益普遍的问题是文本安全性，因为不受控制的系统可能会向用户生成导致伤害或威胁生命的建议。然而，可能导致身体伤害的生成语句的明确程度不同。在本文中，我们区分了可能导致身体伤害的文本类型，并建立了一个尤其未被探索的类别：隐蔽不安全文本。然后，我们进一步分解了这个类别并分析了每个小类别中文本的生成方式。最终，我们的工作定义了导致物理伤害的隐蔽不安全语言问题，并指出这个微妙但危险的问题需要成为相关利益相关者和监管机构的优先考虑问题。我们提出了缓解策略，以启发未来研究人员解决这个具有挑战性的问题，并帮助提高智能系统内部的安全性。

    An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies. In this paper, we distinguish types of text that can lead to physical harm and establish one particularly underexplored category: covertly unsafe text. Then, we further break down this category with respect to the system's information and discuss solutions to mitigate the generation of text in each of these subcategories. Ultimately, our work defines the problem of covertly unsafe language that causes physical harm and argues that this subtle yet dangerous issue needs to be prioritized by stakeholders and regulators. We highlight mitigation strategies to inspire future researchers to tackle this challenging problem and help improve safety within smart systems.
    
[^87]: 利用人体动作合成进行计算编舞

    Computational Choreography using Human Motion Synthesis. (arXiv:2210.04366v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04366](http://arxiv.org/abs/2210.04366)

    本文介绍了一种利用深度学习模型分析舞蹈动作和生成新动作序列的方法，同时也结合了前人的努力来开发出一套系统。

    

    深度学习模型是否应该被训练来分析人体表演艺术？为了回答这个问题，我们探索了深度神经网络在合成艺术人体动作方面的应用。人体运动合成中的问题任务包括预测野外环境中人体运动，以及生成基于这些预测的新动作序列。我们将讨论一个非传统的应用潜力，即将学习模型应用于预测舞蹈动作。最近有一些显著的努力，以计算的方式分析舞蹈动作，例如Everybody Dance Now（EDN）学习模型和Cal Poly硕士论文Take The Lead（TTL）。我们有效地将这两个作品与我们自己的深度神经网络结合起来，生成了一种新的舞蹈动作预测系统、图像到图像的转换和视频生成。

    Should deep learning models be trained to analyze human performance art? To help answer this question, we explore an application of deep neural networks to synthesize artistic human motion. Problem tasks in human motion synthesis can include predicting the motions of humans in-the-wild, as well as generating new sequences of motions based on said predictions. We will discuss the potential of a less traditional application, where learning models are applied to predicting dance movements. There have been notable, recent efforts to analyze dance movements in a computational light, such as the Everybody Dance Now (EDN) learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have effectively combined these two works along with our own deep neural network to produce a new system for dance motion prediction, image-to-image translation, and video generation.
    
[^88]: $L_2$BN: 通过等化特征的$L_2$范数来增强批量归一化

    $L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of Features. (arXiv:2207.02625v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.02625](http://arxiv.org/abs/2207.02625)

    本文提出了一种$L_2$BN方法，通过等化样本特征的$L_2$范数来增强批量归一化，可以增强内类别特征的紧凑性并扩大跨类别特征的差异，易于实现，可以用作神经网络的基本归一化方法。

    This paper proposes an $L_2$BN method to enhance batch normalization by equalizing the $l_2$ norms of sample features, which can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features, easy to implement, and can be used as a basic normalization method for neural networks.

    本文表明，样本特征的$L_2$范数差异会妨碍批量归一化获得更加显著的跨类别特征和更加紧凑的内类别特征。为了解决这个问题，我们提出了一种直观但有效的方法来等化样本特征的$L_2$范数。具体来说，我们在将样本特征输入批量归一化之前对每个样本特征进行$L_2$归一化，因此特征具有相同的数量级。由于所提出的方法结合了$L_2$归一化和批量归一化，因此我们将其命名为$L_2$BN。$L_2$BN可以增强内类别特征的紧凑性并扩大跨类别特征的差异。$L_2$BN易于实现，可以在没有任何额外参数或超参数的情况下发挥其作用。因此，它可以用作神经网络的基本归一化方法。我们通过对各种模型进行广泛的实验评估了$L_2$BN的有效性，用于图像分类。

    In this paper, we show that the difference in $l_2$ norms of sample features can hinder batch normalization from obtaining more distinguished inter-class features and more compact intra-class features. To address this issue, we propose an intuitive but effective method to equalize the $l_2$ norms of sample features. Concretely, we $l_2$-normalize each sample feature before feeding them into batch normalization, and therefore the features are of the same magnitude. Since the proposed method combines the $l_2$ normalization and batch normalization, we name our method $L_2$BN. The $L_2$BN can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features. The $L_2$BN is easy to implement and can exert its effect without any additional parameters or hyper-parameters. Therefore, it can be used as a basic normalization method for neural networks. We evaluate the effectiveness of $L_2$BN through extensive experiments with various models on image classif
    
[^89]: Ask-AC: 一种循环中的主动顾问演员-评论家框架

    Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01955](http://arxiv.org/abs/2207.01955)

    本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。

    

    尽管交互式强化学习方案取得了很多有希望的结果，但目前的方案仍然依赖于来自顾问专家的被动监督信号，形式包括持续监控或预定义规则，这不可避免地导致了一种麻烦而昂贵的学习过程。在本文中，我们介绍了一种新的主动顾问演员-评论家框架，称为Ask-AC，它用一个双向的学习者主动机制替换了单向的顾问指导机制，从而实现了学习者和顾问之间的定制化和有效的信息交换。Ask-AC 的核心是两个互补的组件，分别是动作请求者和自适应状态选择器，可以方便地纳入各种离散的演员-评论家架构中。前者允许代理主动寻求不确定状态下的顾问干预，后者则可以识别漏掉的不稳定状态。

    Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
    
[^90]: 从理解基因漂变到基于智能重启机制的分布估计算法

    From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms. (arXiv:2206.09090v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2206.09090](http://arxiv.org/abs/2206.09090)

    这篇论文介绍了一种基于智能重启机制的分布估计算法，该算法可以在基因漂变风险高的情况下停止运行，并寻找良好的参数范围以运行EDA，从而提高性能。

    

    估计分布算法（EDAs）是一种优化算法，它从搜索空间中学习一个分布，从中可以轻松地采样出好的解决方案。大多数EDAs的关键参数是样本大小（种群大小）。如果种群大小太小，概率模型更新仅基于少量样本，导致不希望出现的基因漂变效应。种群大小过大会避免遗传漂变，但会减缓进程。基于最近量化分析的种群大小如何导致基因漂变，我们设计了EDAs的智能重启机制。当基因漂变风险很高时停止运行，它会自动在良好的参数范围内运行EDA。通过数学运行时间分析，我们为这种智能重启方案证明了一个通用的性能保证。特别地，这表明在许多情况下，如果已知最佳的（问题特定的）参数值，重启方案会自动发现这些值，从而导致更好的性能。

    Estimation-of-distribution algorithms (EDAs) are optimization algorithms that learn a distribution on the search space from which good solutions can be sampled easily. A key parameter of most EDAs is the sample size (population size). If the population size is too small, the update of the probabilistic model builds on few samples, leading to the undesired effect of genetic drift. Too large population sizes avoid genetic drift, but slow down the process.  Building on a recent quantitative analysis of how the population size leads to genetic drift, we design a smart-restart mechanism for EDAs. By stopping runs when the risk for genetic drift is high, it automatically runs the EDA in good parameter regimes.  Via a mathematical runtime analysis, we prove a general performance guarantee for this smart-restart scheme. This in particular shows that in many situations where the optimal (problem-specific) parameter values are known, the restart scheme automatically finds these, leading to the a
    
[^91]: 命题框架中的优化问题的抽象视角

    An Abstract View on Optimizations in Propositional Frameworks. (arXiv:2206.06440v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.06440](http://arxiv.org/abs/2206.06440)

    本文提出了一个统一的权重系统框架，消除了命题框架中不同优化语句之间的语法差异，具有重要的简化和解释潜力。

    

    科学和工程领域中存在大量的搜索优化问题。人工智能长期以来为解决和建模搜索优化问题的搜索算法和声明式编程语言的发展做出了贡献。自动推理和知识表示是人工智能的子领域，特别关注这些发展。许多流行的自动推理范式为用户提供支持优化语句的语言：例如MaxSAT on minone或答案集编程。这些范式在其语言和表达计算解的质量条件的方式上存在重大差异。本文提出了一个称为权重系统的统一框架，消除了范式之间的语法差异，并允许我们看到范式提供的优化语句之间的本质相似性和差异性。这种统一的观点具有重要的简化和解释潜力。

    Search-optimization problems are plentiful in scientific and engineering domains. Artificial intelligence has long contributed to the development of search algorithms and declarative programming languages geared toward solving and modeling search-optimization problems. Automated reasoning and knowledge representation are the subfields of AI that are particularly vested in these developments. Many popular automated reasoning paradigms provide users with languages supporting optimization statements: answer set programming or MaxSAT on minone, to name a few. These paradigms vary significantly in their languages and in the ways they express quality conditions on computed solutions. Here we propose a unifying framework of so-called weight systems that eliminates syntactic distinctions between paradigms and allows us to see essential similarities and differences between optimization statements provided by paradigms. This unifying outlook has significant simplifying and explanatory potential 
    
[^92]: GAMR: 一种用于 (视觉) 推理的引导式注意力模型

    GAMR: A Guided Attention Model for (visual) Reasoning. (arXiv:2206.04928v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.04928](http://arxiv.org/abs/2206.04928)

    本文介绍了一个新的模块——GAMR，它是一种用于(视觉)推理的引导式注意力模型，以动态选择任务相关的视觉信息并将其路由到记忆中来解决复杂的视觉推理任务，并在各种任务和数据集上取得了成功的实验结果。

    

    人类在灵活分析和理解复杂的视觉场景方面仍然优于现代人工智能系统。本文提出了一种新的视觉推理模块——引导式注意力模型(GAMR)，它通过选择和路由与任务相关的视觉信息到记忆中的注意力转移序列来体现主动视觉理论。在各种视觉推理任务和数据集上的实验显示，GAMR能够以稳健且样本高效的方式学习视觉例程。此外，GAMR在全新的推理任务上表现出了零样本泛化的能力。总的来说，我们的研究提供了计算支持，支持认知理论假设需要注意力和记忆之间的关键相互作用，以动态地维护和操作任务相关的视觉信息来解决复杂的视觉推理任务。

    Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR's ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks.
    
[^93]: MolScribe：具有图像到图形生成的强健分子结构识别

    MolScribe: Robust Molecular Structure Recognition with Image-To-Graph Generation. (arXiv:2205.14311v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.14311](http://arxiv.org/abs/2205.14311)

    MolScribe是一种强健的分子结构识别模型，它可以将分子图像转换成图形结构，包括原子和键及其几何布局，并且能够识别手性和扩展缩写结构。该模型在公共基准测试中的准确性为76-93％，并可通过置信度估计和原子级对齐进行验证。

    

    分子结构识别是将分子图像翻译成其图形结构的任务。化学文献中展示的绘图风格和约定的显着变化为自动化此任务带来了重大挑战。在本文中，我们提出了MolScribe，一种新颖的图像到图形生成模型，它明确地预测原子和键以及它们的几何布局来构建分子结构。我们的模型灵活地融合了符号化学约束，以识别手性并扩展缩写结构。我们还开发了数据增强策略，以增强模型对领域变化的稳健性。在合成和现实分子图像的实验中，MolScribe显着优于以前的模型，在公共基准测试中实现了76-93％的准确性。化学家们也可以轻松验证 MolScribe 的预测，该预测受其置信度估计和与输入图像的原子级对齐的控制。MolScribe是公开的。

    Molecular structure recognition is the task of translating a molecular image into its graph structure. Significant variation in drawing styles and conventions exhibited in chemical literature poses a significant challenge for automating this task. In this paper, we propose MolScribe, a novel image-to-graph generation model that explicitly predicts atoms and bonds, along with their geometric layouts, to construct the molecular structure. Our model flexibly incorporates symbolic chemistry constraints to recognize chirality and expand abbreviated structures. We further develop data augmentation strategies to enhance the model robustness against domain shifts. In experiments on both synthetic and realistic molecular images, MolScribe significantly outperforms previous models, achieving 76-93% accuracy on public benchmarks. Chemists can also easily verify MolScribe's prediction, informed by its confidence estimation and atom-level alignment with the input image. MolScribe is publicly availa
    
[^94]: DeepGraviLens：一种用于分类引力透镜数据的多模态网络架构

    DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2205.00701](http://arxiv.org/abs/2205.00701)

    DeepGraviLens是一种多模态神经网络，用于分类属于不同类型的引力透镜数据，具有高精度和优于现有方法的结果。

    

    引力透镜是由大质量物体产生的相对论效应，会弯曲其周围的时空。这是天体物理学中一个深入研究的课题，允许验证理论相对论结果并研究一些否则不可见的微弱天体物体。近年来，机器学习方法已被应用于支持引力透镜现象的分析，通过检测与亮度变化时间序列相关的图像数据集中的透镜效应。然而，当前的方法要么仅考虑图像而忽略时间序列数据，要么在最困难的数据集上实现相对较低的准确性。本文介绍了 DeepGraviLens，这是一种新颖的多模态网络，用于分类属于一个非透镜系统类型和三个透镜系统类型的时空数据。它在准确性方面超过当前的 state-of-art 方法，提高了约 19% 到 43%，具体取决于所考虑的数据集。

    Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx$ 19% to $\approx$ 43%, depending on the considered dat
    
[^95]: 预训练的token-replaced检测模型作为少样本学习器

    Pre-trained Token-replaced Detection Model as Few-shot Learner. (arXiv:2203.03235v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.03235](http://arxiv.org/abs/2203.03235)

    本文提出了一种使用预训练的token-replaced检测模型的少样本学习器方法，将任务重新定义为token-replaced检测问题，能够优于使用预训练的遮蔽语言模型的少样本学习器。

    

    预训练的遮蔽语言模型已经展示出在少样本学习方面具有非凡的能力。本文提出了一种新的方法，使用预训练的token-replaced检测模型（比如ELECTRA）作为少样本学习器。该方法将分类或回归任务重新定义为token-replaced检测问题。具体来说，我们首先为每个任务定义一个模板和标签描述词，并将它们放入输入中形成一个自然语言提示。然后，我们使用预训练的token-replaced检测模型来预测哪个标签描述词在提示中是最原始的（即最少更改的）。对16个数据集的系统评估表明，我们的方法在一句话和两句话的学习任务中，都优于使用预训练的遮蔽语言模型的少样本学习器。

    Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.
    
[^96]: 全面深度学习

    Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15829](http://arxiv.org/abs/2110.15829)

    本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。

    This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.

    本文提出了一种新颖的全面深度学习框架，同时解决了对输入扰动的脆弱性、过度参数化和来自不同训练验证拆分的性能不稳定性等挑战。所提出的框架在标准深度学习模型上全面提高了准确性、鲁棒性、稀疏性和稳定性，这在对表格和图像数据集进行广泛实验中得到了证明。结果进一步通过消融实验和SHAP值分析进行验证，揭示了不同评估指标之间的交互作用和权衡。为了支持实践者应用我们的框架，我们提供了一种指导性方法，根据他们的具体目标，提供选择适当的训练损失函数的建议。所有用于重现结果的代码都可以在https://github.com/kimvc7/HDL找到。

    This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
    
[^97]: 带噪声特征的上下文线性Bandit：朝向贝叶斯神谕前进

    Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles. (arXiv:1703.01347v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1703.01347](http://arxiv.org/abs/1703.01347)

    本论文研究了具有噪声特征的上下文线性Bandit问题。我们提出了一个算法，通过观察信息，实现了贝叶斯神谕并得到了$\tilde{O}(d\sqrt{T})$的遗憾界。

    

    我们研究了带有噪声和缺失项的上下文线性Bandit问题。为了解决噪声的挑战，我们分析了在观测噪声特征的情况下给出的贝叶斯神谕。我们的贝叶斯分析发现，最优假设可能会远离潜在的可实现函数，这取决于噪声特征，这是高度非直观的，并且在经典的无噪声设置下不会发生。这意味着经典方法不能保证非平凡的遗憾界（regret bound）。因此，我们提出了一个算法，旨在从这个模型下的观察信息中实现贝叶斯神谕，当有大量手臂时，可以实现$\tilde{O}(d\sqrt{T})$遗憾界。我们使用合成和实际数据集演示了所提出的算法。

    We study contextual linear bandit problems under feature uncertainty; they are noisy with missing entries. To address the challenges of the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on the noise characteristics, which are highly non-intuitive and do not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.
    

