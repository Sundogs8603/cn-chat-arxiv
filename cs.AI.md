# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CONA: A novel CONtext-Aware instruction paradigm for communication using large language model.](http://arxiv.org/abs/2305.18620) | CONA是一种基于上下文的指令范式，利用大型语言模型，自动优化演示内容并提供上下文感知型答案，具有较高的上下文感知性和易理解性。 |
| [^2] | [Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard.](http://arxiv.org/abs/2305.18618) | 本文比较了三种基于大型语言模型的聊天机器人(ChatGPT-3.5、ChatGPT-4和Google Bard)在解决数学和逻辑问题上的正确性，研究发现这些机器人可以在某些情况下给出正确答案，但在更复杂的问题中需要改进。 |
| [^3] | [Embrace Opportunities and Face Challenges: Using ChatGPT in Undergraduate Students' Collaborative Interdisciplinary Learning.](http://arxiv.org/abs/2305.18616) | 通过在本科生跨学科合作学习中使用ChatGPT，可以促进其跨学科问题解决、身体和认知投入，尤其对STEM学生产生积极作用。 |
| [^4] | [Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders.](http://arxiv.org/abs/2305.18612) | 本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。 |
| [^5] | [How Effective Are Neural Networks for Fixing Security Vulnerabilities.](http://arxiv.org/abs/2305.18607) | 这篇论文比较了使用大型代码语言模型和自动化程序修复技术修复Java漏洞的能力，并提供了新的Java漏洞修复基准。在两个真实的Java漏洞基准上进行评估。 |
| [^6] | [Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models.](http://arxiv.org/abs/2305.18585) | 本文结合可解释性和对抗攻击两个关键方面，研究了仇恨言论检测模型的鲁棒性，并提出了针对性的对抗攻击，对该领域的未来研究具有重要的意义。 |
| [^7] | [Controllable Text-to-Image Generation with GPT-4.](http://arxiv.org/abs/2305.18583) | 利用GPT-4编写TikZ代码可提高文本到图像生成的控制性和保真度。 |
| [^8] | [Search-Based Regular Expression Inference on a GPU.](http://arxiv.org/abs/2305.18575) | 此研究提出了一种基于搜索空间矩阵的算法来进行正则表达式推断，是具有时间换空间的枚举算法；中缀闭包使操作更为高效。 |
| [^9] | [Fairness of ChatGPT.](http://arxiv.org/abs/2305.18569) | 本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。 |
| [^10] | [Controllable Path of Destruction.](http://arxiv.org/abs/2305.18553) | 本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。 |
| [^11] | [RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments.](http://arxiv.org/abs/2305.18510) | RLAD是首个在城市自动驾驶领域应用基于像素的强化学习方法，通过优化图像编码器和路径点编码器等技术，可以提高自动驾驶的性能和效率。 |
| [^12] | [Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models.](http://arxiv.org/abs/2305.18507) | 本文介绍了一种神经符号提示方法——代码提示，该方法可以触发代码作为中间步骤。与自然语言相比，代码提示有着几个独特优势，能够提高符号推理和算术推理的性能，并且通常优于思路链提示。 |
| [^13] | [How to Query Human Feedback Efficiently in RL?.](http://arxiv.org/abs/2305.18505) | 该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。 |
| [^14] | [VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset.](http://arxiv.org/abs/2305.18500) | 本文提出了一种全模态视频文本基础模型VAST及其数据集VAST-27M。该模型可以感知和处理视频中的视觉、音频和字幕模态，通过自动集成多模态字幕和资源，提供更好的支持文本关联的多种任务。 |
| [^15] | [ANPL: Compiling Natural Programs with Interactive Decomposition.](http://arxiv.org/abs/2305.18498) | ANPL是一个编程系统，可以让用户直接操作草图，使用自然语言描述注释模块或孔，并生成一个有机的Python程序，它优于基线。 |
| [^16] | [DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific Clustering Using Side Information.](http://arxiv.org/abs/2305.18492) | 本研究提出了一种基于少量成对样例的侧信息直接学习数据聚类的方法DMS，与以往方法不同，我们无需知道类别数、类中心或者任何相似的距离度量，该方法可以根据侧信息的任务需求将相同的数据点分成不同的聚类，且在固有的和非固有的数据集任务上表现优异。 |
| [^17] | [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets.](http://arxiv.org/abs/2305.18486) | 本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。 |
| [^18] | [Autoencoding Conditional Neural Processes for Representation Learning.](http://arxiv.org/abs/2305.18485) | 本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。 |
| [^19] | [A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection.](http://arxiv.org/abs/2305.18481) | 本论文提出了一种使用无人机辅助的元宇宙网络模型，将资源分配和轨迹控制集成到系统模型中，设计了一种强化学习和凸优化的混合框架，以“合作”方式解决时间顺序优化问题，使得数据收集效率得到提高。 |
| [^20] | [FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition.](http://arxiv.org/abs/2305.18479) | 本论文解决了将Human Action Recognition中最先进的模型之一——X3D映射到任何FPGA设备上的问题，并首次实现了针对这种复杂模型架构进行Human Action Recognition任务的目标。 |
| [^21] | [Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics.](http://arxiv.org/abs/2305.18477) | 本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。 |
| [^22] | [Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2305.18459) | 本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。 |
| [^23] | [Baselines for Identifying Watermarked Large Language Models.](http://arxiv.org/abs/2305.18456) | 该论文介绍了一套基线算法，用于识别带有水印的大型语言模型，这些算法分析了带有和不带有水印的LLM所产生的输出分布和逻辑分布之间的差异。 |
| [^24] | [Shift-Robust Molecular Relational Learning with Causal Substructure.](http://arxiv.org/abs/2305.18451) | 本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。 |
| [^25] | [Taming AI Bots: Controllability of Neural States in Large Language Models.](http://arxiv.org/abs/2305.18449) | 本文提出了一个问题，是否可以通过适当选择提示，控制AI bot到达任何状态，而研究表明训练良好的Bot能几乎确定地到达任何意义子集，具有可控性。 |
| [^26] | [Continual Task Allocation in Meta-Policy Network via Sparse Prompting.](http://arxiv.org/abs/2305.18444) | 本文提出的CoTASP可以通过学习过完备字典来生成稀疏掩码作为提示，从而从元策略网络中提取与每个任务相关的子网络，实现了快速适应新任务，同时保留了之前任务的共同知识。 |
| [^27] | [Optimizing Airbnb Search Journey with Multi-task Learning.](http://arxiv.org/abs/2305.18431) | 本文提出了一种新的多任务深度学习模型架构Journey Ranker，来解决Airbnb搜索过程中的唯一挑战，即客户和主机的偏好，该模型可应用于多个用例。 |
| [^28] | [Employing Explainable Artificial Intelligence (XAI) Methodologies to Analyze the Correlation between Input Variables and Tensile Strength in Additively Manufactured Samples.](http://arxiv.org/abs/2305.18426) | 本研究使用可解释的人工智能方法，探究了增材制造中输入变量和拉伸强度的相关性，发现Infill百分比和挤出温度对于拉伸强度具有最高的正相关和负相关影响。 |
| [^29] | [Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals.](http://arxiv.org/abs/2305.18425) | 本论文提出了一种利用权重残差低秩特性实现精细调整模型高效存储的新方法ERE，并通过额外量化和分层秩分配来提高存储效率，实验结果表明该方法显著减少内存占用，同时保持性能。 |
| [^30] | [HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts.](http://arxiv.org/abs/2305.18421) | 本文提出了一种名为HyperTime的超参数优化方法，用于寻找时间上鲁棒的预测性能超参数，该方法在历史验证数据集上对平均验证损失和最坏情况验证损失设置了词典优先级顺序，并在多个带时间分布偏移的机器学习任务上表现强劲。 |
| [^31] | [Just a Glimpse: Rethinking Temporal Information for Video Continual Learning.](http://arxiv.org/abs/2305.18418) | 本文提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。实验表明，在内存受到极端限制时，视频的多样性比时间信息更重要。 |
| [^32] | [Learning to Learn from APIs: Black-Box Data-Free Meta-Learning.](http://arxiv.org/abs/2305.18413) | 该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。 |
| [^33] | [A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining.](http://arxiv.org/abs/2305.18407) | MoleculeSDE是用于分子多模态预训练的群对称随机微分方程模型，通过在输入空间中直接生成3D几何与2D拓扑之间的转换，它能够更有效地保存分子结构信息。 |
| [^34] | [Dink-Net: Neural Clustering on Large Graphs.](http://arxiv.org/abs/2305.18405) | Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。 |
| [^35] | [A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning.](http://arxiv.org/abs/2305.18400) | 提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。 |
| [^36] | [On the impact of activation and normalization in obtaining isometric embeddings at initialization.](http://arxiv.org/abs/2305.18399) | 本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。 |
| [^37] | [Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?.](http://arxiv.org/abs/2305.18398) | 该论文研究了文本驱动的图像生成模型复制不适当人类行为的问题，并提出了抑制生成不适当内容的策略，该策略利用模型对世界丑陋的表现与人类偏好对齐。 |
| [^38] | [Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks.](http://arxiv.org/abs/2305.18395) | 本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。 |
| [^39] | [Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study.](http://arxiv.org/abs/2305.18384) | 本文提出了增量学习器的后门攻击可能存在的安全风险，并实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性。 |
| [^40] | [Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection.](http://arxiv.org/abs/2305.18381) | 研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。 |
| [^41] | [Assumption Generation for the Verification of Learning-Enabled Autonomous Systems.](http://arxiv.org/abs/2305.18372) | 本文提出了一种为安全保证提供假设的做法，以用于验证具有复杂环境和学习增强组件的自主系统，通过自动生成适当的DNN行为假设，来满足要求的安全属性。 |
| [^42] | [ColibriUAV: An Ultra-Fast, Energy-Efficient Neuromorphic Edge Processing UAV-Platform with Event-Based and Frame-Based Cameras.](http://arxiv.org/abs/2305.18371) | 本文介绍了一种具有帧和事件相机接口的UAV平台ColibriUAV，其围绕一种新颖的低功耗RISC-V SoC Kraken设计，Kraken能够高效地处理来自DVS相机的事件数据和来自RGB相机的帧数据。 |
| [^43] | [What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks.](http://arxiv.org/abs/2305.18365) | 本文建立了包括 8 个实际化学任务的综合基准测试，有力地证明了 LLM 在实际化学中的能力。 |
| [^44] | [Towards Explainable Conversational Recommender Systems.](http://arxiv.org/abs/2305.18363) | 本文提出了衡量对话式推荐系统的可解释性的十种评估视角，并通过构建新数据集来提高解释质量。 |
| [^45] | [Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs.](http://arxiv.org/abs/2305.18362) | 该研究提出了一种基于概念的解释模型敲门技术，可以在图像分类任务中找到显著的概念以避免误解，并控制误发现率（FDR）在某个值下，在合成和真实数据实验中得到验证。 |
| [^46] | [DeepSI: Interactive Deep Learning for Semantic Interaction.](http://arxiv.org/abs/2305.18357) | 本文提出了一种基于语义交互的交互式深度学习方法，可以高效地学习用户和任务特定的数据表示，改善视觉分析应用中的语义交互，并通过比较验证了该方法的优势。 |
| [^47] | [LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations.](http://arxiv.org/abs/2305.18354) | 本文通过分析GPT模型在抽象推理语料库上的表现，发现GPT在抽象推理任务中存在需要核心概念“核心知识”的限制。通过使用基于对象的表示方法和新的1D-ARC基准，GPT在抽象推理任务中取得了较好的表现。 |
| [^48] | [Multi-Objective Genetic Algorithm for Multi-View Feature Selection.](http://arxiv.org/abs/2305.18352) | 多视角数据提高了预测模型的准确性，但也使得高维数据增加，影响模型泛化能力。研究者提出了一种多视角多目标特征选择遗传算法（MMFS-GA），用于从多视角数据中选择最优的特征子集以提高模型精度和可解释性。 |
| [^49] | [Attention Paper: How Generative AI Reshapes Digital Shadow Industry?.](http://arxiv.org/abs/2305.18346) | 本文分析了生成 AI 对数字阴影产业带来的挑战和机遇，从黑色/阴影产业的上游、中游和下游路径进行了技术分析，并提出了改进现有风险的未来方向。 |
| [^50] | [Neural Task Synthesis for Visual Programming.](http://arxiv.org/abs/2305.18342) | 该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。 |
| [^51] | [Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback.](http://arxiv.org/abs/2305.18341) | 本文提出了一种叫做RLCF的方法，使用代码编译器反馈进一步训练预训练的大型语言模型，以生成符合目标分布的代码，并通过所有静态正确性检查，显著提高了性能。 |
| [^52] | [A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions.](http://arxiv.org/abs/2305.18339) | 本论文探讨了AI生成内容的工作原理、安全与隐私威胁、现状和未来挑战，并提供了针对这些问题的最新解决方案。 |
| [^53] | [You Don't Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images.](http://arxiv.org/abs/2305.18337) | 本文研究了合成图像在解决数据稀缺和隐私问题方面的应用，并建立了一个全面的合成图像评估体系，证明了合成图像的实用性。 |
| [^54] | [Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics.](http://arxiv.org/abs/2305.18333) | 研究了物品流行度、质量和位置偏差对用户福利的影响，提出了通过探索减轻流行度偏见负面影响的算法。 |
| [^55] | [#REVAL: a semantic evaluation framework for hashtag recommendation.](http://arxiv.org/abs/2305.18330) | #REVAL是一种新颖的语义评估框架，用于解决传统评估方法无法考虑推荐和实际hashtag之间语义相关性问题。实验证明#REVAL在捕捉语义相关性方面是有效的。 |
| [^56] | [A Study on Deep CNN Structures for Defect Detection From Laser Ultrasonic Visualization Testing Images.](http://arxiv.org/abs/2305.18327) | 本文提出了一种适用于激光超声可视化测试图像中自动缺陷检测和定位的深度神经网络。在实际数据上进行的数值实验表明，该方法在预测性能方面比通用对象检测模型更有效且计算时间更短。 |
| [^57] | [BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation.](http://arxiv.org/abs/2305.18326) | 提出了一个大规模的视频字幕翻译数据集BigVideo， 集成了4.5 million句子对和9981小时视频，设计了有歧义和明确的测试集，引入了一种对比学习方法，实验结果表明，视觉信息可以提高NMT模型的BLEU、BLEURT和COMET得分，有助于消除歧义，数据集和翻译模型公开可用。 |
| [^58] | [Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain.](http://arxiv.org/abs/2305.18324) | 本文介绍了将正则表达式模式作为领域知识特征与领域特定文本一起用于微调预训练语言模型的方法，通过在金融领域中实验，证明这种方法可以改善下游文本分类任务的表现。 |
| [^59] | [ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models.](http://arxiv.org/abs/2305.18323) | ReWOO是一种将推理过程与外部观察分离的模块化范式，从而可以减少标记消耗并提高性能。 |
| [^60] | [REFinD: Relation Extraction Financial Dataset.](http://arxiv.org/abs/2305.18322) | REFinD是第一个完全基于金融文档生成的关系注释的大规模数据集，并对其进行了评估，显示出在金融领域进行关系抽取的特定挑战。 |
| [^61] | [Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students.](http://arxiv.org/abs/2305.18320) | 本研究通过行为forma mentis网络(BFMNs)的方法研究了先进的语言模型(GPT-3，Chat-GPT和GPT-4)对数学和STEM领域的偏见，并发现它们都倾向于将数学和STEM领域视为困难、紧张和引起焦虑。 |
| [^62] | [Automated Feedback Generation for a Chemistry Database and Abstracting Exercise.](http://arxiv.org/abs/2305.18319) | 本研究利用BERT模型，对化学数据库中摘要练习的答案结构进行反馈，该模型在学生提交的句子中将其归为三类，即背景、技术和观察，提供了一种方法对学生作业进行自动化反馈。 |
| [^63] | [Application of Text Analytics in Public Service Co-Creation: Literature Review and Research Framework.](http://arxiv.org/abs/2305.18316) | 本文系统综述了在公共服务领域中应用文本分析技能以支持公共服务协同创作的研究，发掘了TA技术和公共服务的对公共价值的影响，以及在公共服务领域中TA技术的应用前景。 |
| [^64] | [CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities.](http://arxiv.org/abs/2305.18315) | CDJUR-BR是一份稳健的黄金收藏，包含巴西司法文件中的精细命名实体，该收藏涵盖各种法律程序文件，并有助于解决目前命名实体识别（NER）无法轻而易举地识别法律实践文本中实体的问题。 |
| [^65] | [Balancing Test Accuracy and Security in Computerized Adaptive Testing.](http://arxiv.org/abs/2305.18312) | 本文介绍了一种基于双层优化的计算机自适应测试(CAT)框架的约束版本C-BOBCAT，通过权衡测试准确性和问题暴露率及测试重叠率，解决了BOBCAT存在的高问题暴露率和测试重叠率的问题。 |
| [^66] | [Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study.](http://arxiv.org/abs/2305.18307) | 本文研究了认证标签在传达可信AI上的作用。调查结果表明，认证标签可以显著增加终端用户在低风险和高风险场景下使用AI的信任和使用意愿，尤其是在高风险场景中，其效果更为明显。 |
| [^67] | [Multi-View Interactive Collaborative Filtering.](http://arxiv.org/abs/2305.18306) | 提出了基于多视角交互主题回归算法（MV-ICTR）的推荐系统，在不同视角下同时纳入评分和上下文信息来建模物品特定功能的相关性和用户的个人偏好，采用多臂老虎机策略进行持续的在线个性化，显著提高了数据集上性能。 |
| [^68] | [What We Know So Far: Artificial Intelligence in African Healthcare.](http://arxiv.org/abs/2305.18302) | 本文综述了人工智能算法在非洲医疗保健领域中的应用情况和如何利用人工智能在低资源环境下改善非洲医疗保健的可访问性，并讨论了采用人工智能面临的一些重要挑战和机遇。需要政府、私营部门、医疗保健提供者和国际组织协调一致地努力，创建可持续的人工智能解决方案，满足非洲医疗保健系统独特的需求。 |
| [^69] | [Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning.](http://arxiv.org/abs/2305.18158) | 本文提出了一种命名为OOD语义修剪（OSP）的框架，该框架旨在从分布内（ID）特征中修剪掉分布外（OOD）的语义信息，这有助于提高鲁棒半监督学习的性能。 |
| [^70] | [InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion.](http://arxiv.org/abs/2305.17716) | 本文提出了一个基于视错觉的独特数据集InDL，用于测试和评估深度学习模型的图中逻辑解释能力。利用几何光学视错觉，建立可比性框架用于阐明模型可能存在的缺陷和提供改进模型的洞察力。 |
| [^71] | [Incentivizing honest performative predictions with proper scoring rules.](http://arxiv.org/abs/2305.17601) | 本论文研究了当预测可以影响结果时如何利用适当的评分规则激励专家作出准确反映信念的预测，并给出了针对二元预测的评分规则的界限。 |
| [^72] | [A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection.](http://arxiv.org/abs/2305.17480) | 本研究提出了一个多任务深度学习框架，同时检测夸张和隐喻。使用隐喻标签注释了两个夸张数据集，使用夸张标签注释了两个隐喻数据集，实验证明该框架检测夸张的性能比先前方法进步了12%。此外，多任务学习方法比单任务学习方法提高了多达17%。 |
| [^73] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^74] | [mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms.](http://arxiv.org/abs/2305.17152) | mldr.resampling是一个软件包，提供11种多标签重采样方法的参考实现，旨在应对多标签学习中的不平衡情况，并具有高效性。 |
| [^75] | [Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model.](http://arxiv.org/abs/2305.17116) | 本研究使用Retrieval Augmentation（RetA）方法，对比了OpenAI的GPT-3、GPT-4、Bing的Prometheus以及定制的RetA模型在回答19个弥漫大B细胞淋巴瘤（DLBCL）疾病相关问题方面的表现，结果显示RetA模型在准确性和相关性方面表现最佳。 |
| [^76] | [Combining Global and Local Merges in Logic-based Entity Resolution.](http://arxiv.org/abs/2305.16926) | 在Lace框架中，使用逻辑规则和约束识别表示同一实体的实体引用对，该识别结合了全局和局部合并的概念。 |
| [^77] | [GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children.](http://arxiv.org/abs/2305.16809) | 本研究设计了一个智能辅导系统（GenQ），可以根据照顾者和孩子之间的对话促进孩子的阅读理解能力，并通过考虑文化背景和语境变化以提高系统效果。 |
| [^78] | [Do GPTs Produce Less Literal Translations?.](http://arxiv.org/abs/2305.16806) | 本研究比较了GPT和NMT生成翻译的文字积极度差异，发现GPT翻译更不准确，但在MT质量评估指标上表现出相似或更好的分数。 |
| [^79] | [Differentiable Clustering with Perturbed Spanning Forests.](http://arxiv.org/abs/2305.16358) | 介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。 |
| [^80] | [Scaling Data-Constrained Language Models.](http://arxiv.org/abs/2305.16264) | 研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。 |
| [^81] | [Visually grounded few-shot word acquisition with fewer shots.](http://arxiv.org/abs/2305.15937) | 该论文提出了一种基于视觉的语音模型，可以从仅有少量的词-图像示例对中习得新的词汇及其视觉表示，并且与现有方法相比，该模型在使用更少的样本时取得了更好的性能。 |
| [^82] | [The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning.](http://arxiv.org/abs/2305.15703) | 通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。 |
| [^83] | [Learning-Based Automatic Synthesis of Software Code and Configuration.](http://arxiv.org/abs/2305.15642) | 本文介绍了一种基于学习的自动软件代码和配置生成方法，其中通过遗传算法和神经网络训练的适应度函数自动合成程序，使用协方差矩阵适应进化策略完成程序合成，并采用强化学习方法实现配置生成。 |
| [^84] | [SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning.](http://arxiv.org/abs/2305.15486) | SPRING是一个新的方法，能够在开放世界游戏中表现出色，它通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏。。 |
| [^85] | [Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement.](http://arxiv.org/abs/2305.15151) | 本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。 |
| [^86] | [Video Prediction Models as Rewards for Reinforcement Learning.](http://arxiv.org/abs/2305.14343) | 本文提出了VIPER算法，利用预训练的视频预测模型作为强化学习的奖励信号来学习复杂行为，从而实现在广泛任务范围内的专家级控制，同时具有泛化性。 |
| [^87] | [EntRED: Benchmarking Relation Extraction with Fewer Shortcuts.](http://arxiv.org/abs/2305.13551) | 本研究提出了一个名称更为多样、没有捷径、具有挑战性的关系提取基准测试EntRed，并解决了标准基准测试数据集存在的实体注释错误、实体名称多样性较低、从实体名称到基本事实关系的捷径等问题。 |
| [^88] | [Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy.](http://arxiv.org/abs/2305.11616) | 这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。 |
| [^89] | [Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering.](http://arxiv.org/abs/2305.11541) | 本文提供了一个行业云特定QA数据集 MSQA，该数据集可用于评估旨在提高大规模语言模型特定领域能力的方法。本文还提出了一种新的模型交互范式，可以使大规模语言模型在其不擅长的特定任务上取得更好的性能。 |
| [^90] | [SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs.](http://arxiv.org/abs/2305.11461) | 本文提出了 SelfzCoT 自动自我生成的零样本编码，通过使用LLMs和代码级别的自我提示，在六个零样本算术推理任务中实现了巨大的准确度提升。同时，修改的零样本编码 MzCoT 在推理任务中也取得了显著的表现。 |
| [^91] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^92] | [Analysis of Visual Question Answering Algorithms with attention model.](http://arxiv.org/abs/2305.09782) | 本文批评性地检查和审查了使用共同注意力方法的VQA算法的方法，重点关注文本语义生成、对象识别和答案分类技术。 |
| [^93] | [GNNs,You can be Stronger,Deeper and Faster.](http://arxiv.org/abs/2305.05368) | 本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。 |
| [^94] | [A transformer-based method for zero and few-shot biomedical named entity recognition.](http://arxiv.org/abs/2305.04928) | 本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。 |
| [^95] | [New metrics and search algorithms for weighted causal DAGs.](http://arxiv.org/abs/2305.04445) | 本研究提供了针对加权因果 DAGs的新度量和搜索算法，发现了用于自适应干预的因果图，提供了一个新的基准来捕捉搜索算法的最坏干预成本，并提供自适应搜索算法实现对数逼近。 |
| [^96] | [Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.](http://arxiv.org/abs/2305.04111) | 本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。 |
| [^97] | [Towards Weakly-Supervised Hate Speech Classification Across Datasets.](http://arxiv.org/abs/2305.02637) | 该论文提出使用极度弱的监督方法，只依赖于类别名称而不是注释数据中的类别示例，解决当前仇恨言论识别的研究存在的数据创建策略不系统和不同注释方案问题，并展示了有效性。 |
| [^98] | [Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation.](http://arxiv.org/abs/2305.00909) | 提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。 |
| [^99] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^100] | [Analogy-Forming Transformers for Few-Shot 3D Parsing.](http://arxiv.org/abs/2304.14382) | "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。 |
| [^101] | [Contrastive Learning Is Spectral Clustering On Similarity Graph.](http://arxiv.org/abs/2303.15103) | 本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。 |
| [^102] | [RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network.](http://arxiv.org/abs/2303.10770) | 本论文提出一种基于储备节点的神经元视觉感知网络（RN-Net）。RN-Net可以以低成本有效地处理异步的时间特征，并在DVS128手势上实现了迄今最高的准确率99.2％，在更小的网络尺寸下实现了DVS Lip数据集的67.5％准确率。 |
| [^103] | [CB2: Collaborative Natural Language Interaction Research Platform.](http://arxiv.org/abs/2303.08127) | CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。 |
| [^104] | [Transformer-based Planning for Symbolic Regression.](http://arxiv.org/abs/2303.06833) | 该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。 |
| [^105] | [Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization.](http://arxiv.org/abs/2302.13221) | 该论文提出一种将离散特征子集作为连续嵌入空间优化的深度生成可微分特征选择方法，解决了在高维小样本数据集中通用、准确和维度无关的特征选择问题。 |
| [^106] | [Construction numbers: How to build a graph?.](http://arxiv.org/abs/2302.13186) | 论文研究了计算偏序的线性扩展数量问题，并研究了由包含关系确定的图形的顶点和边的偏序，找到了路径、环、星形图、双星形图和完全图的构造序列数量，并提出了公式，同时研究了结构和应用。 |
| [^107] | [Why Target Networks Stabilise Temporal Difference Methods.](http://arxiv.org/abs/2302.12537) | 本文解释了深度强化学习中一种流行的时序差分方法中关键的稳定性问题：为什么目标网络能够有效降低不满足条件时的影响。 |
| [^108] | [Unsupervised Layer-wise Score Aggregation for Textual OOD Detection.](http://arxiv.org/abs/2302.09852) | 提出了一种无监督的逐层聚合异常得分的方法，用于更好地进行文本OOD检测。其能发掘不同层输出的优势，达到更鲁棒的性能，并扩展经典基准测试以反映更现实的设置。 |
| [^109] | [Zero-Shot Batch-Level Anomaly Detection.](http://arxiv.org/abs/2302.07849) | 本文提出了一种名为“自适应中心表示”的方法，用于零样本批次级异常检测。该方法利用批量归一化来训练现成的深度异常检测器，可以自动零样本泛化为未见过的AD任务。在实验中，该方法显示出了在多种数据集上的优秀表现，对表格数据进行了零样本AD。 |
| [^110] | [Are Diffusion Models Vulnerable to Membership Inference Attacks?.](http://arxiv.org/abs/2302.01316) | 本文研究了扩散模型是否易受成员隐私攻击，提出了一种基于查询的成员隐私攻击方法。结果表明现有的成员隐私攻击对扩散模型基本无效。 |
| [^111] | [ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts.](http://arxiv.org/abs/2301.12171) | 这篇论文提出了一种通过最优传输的方法，利用多个文本提示来实现零样本分割，达到了最先进的性能水平。 |
| [^112] | [Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning.](http://arxiv.org/abs/2301.10915) | 这篇论文提出了一种通过使用软提示令牌嵌入来学习任务属性的方法，以实现参数高效的低资源对话状态跟踪，同时在不微调语言模型参数的情况下，取得了比之前方法更好的性能表现。 |
| [^113] | [Mind the Gap: Modelling Difference Between Censored and Uncensored Electric Vehicle Charging Demand.](http://arxiv.org/abs/2301.06418) | 电动汽车充电需求模型的天然偏见导致了观测到的需求与实际需求有差异，使用有关审查的模型来模拟充电需求可以更好地估计真实需求。 |
| [^114] | [SPTS v2: Single-Point Scene Text Spotting.](http://arxiv.org/abs/2301.01635) | 本研究提出的SPTS v2框架，首次证明了可以使用极低成本的单点注释来训练场景文本定位模型，同时保留了自回归Transformer与实例分配解码器（IAD）的优势，并使用并行识别解码器（PRD）进行文本识别。 |
| [^115] | [Continual Contrastive Finetuning Improves Low-Resource Relation Extraction.](http://arxiv.org/abs/2212.10823) | 本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。 |
| [^116] | [ORCA: A Challenging Benchmark for Arabic Language Understanding.](http://arxiv.org/abs/2212.10758) | ORCA 是一个公开可用的阿拉伯语言理解评估基准，利用各种阿拉伯语言和一系列有挑战性的阿拉伯语言理解任务构建。当前使用 ORCA 对 18 个多语言和阿拉伯语言模型进行比较。 |
| [^117] | [When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories.](http://arxiv.org/abs/2212.10511) | 本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。 |
| [^118] | [Prompting Is Programming: A Query Language for Large Language Models.](http://arxiv.org/abs/2212.06094) | LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合，实现了一种新的语言模型编程方式。 |
| [^119] | [World Knowledge in Multiple Choice Reading Comprehension.](http://arxiv.org/abs/2211.07040) | 本文研究了在多项选择阅读理解中利用世界知识的可能性，并提出了基于信息理论的度量方法，这些方法能够评估系统利用世界知识的水平，帮助测试设计人员确保使用世界知识的问题是可接受的，同时可以测试问题的质量并发现设计中可能存在的问题。 |
| [^120] | [FedGen: Generalizable Federated Learning for Sequential Data.](http://arxiv.org/abs/2211.01914) | FedGen为联邦学习带来可推广性，允许分布式设备共同识别和区分伪特征和不变特征，而不需要训练分布的先前知识。 |
| [^121] | [Backtracking Counterfactuals.](http://arxiv.org/abs/2211.00472) | 本文介绍了一种基于回溯式反事实推理的形式化方法，该方法基于图形模型，提供了一种更加自然和直观的推理过去时反事实情景的方法。 |
| [^122] | [Consistent and Truthful Interpretation with Fourier Analysis.](http://arxiv.org/abs/2210.17426) | 该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。 |
| [^123] | [Men Also Do Laundry: Multi-Attribute Bias Amplification.](http://arxiv.org/abs/2210.11924) | 本文研究了计算机视觉中的多属性偏见放大现象，发现现有的度量可能会忽略多个属性之间的相关性，并可能错误地给出度量结果，使得最小或没有偏见放大的情况被误判。 |
| [^124] | [Call Graph Evolution Analytics over a Version Series of an Evolving Software System.](http://arxiv.org/abs/2210.08316) | 该论文提出了一种Call Graph Evolution Analytics的方法，用于从软件系统版本序列的演化调用图中提取信息，通过分析演化调用图的依赖关系模式的演化，帮助软件工程师进行版本演化管理。 |
| [^125] | [Learning Robust Kernel Ensembles with Kernel Average Pooling.](http://arxiv.org/abs/2210.00062) | 本文提出了核均值池（KAP），一种神经网络构建模块，它可以在卷积神经网络中自然地产生具有相似功能的核集合，提高模型对输入扰动的鲁棒性，并在多个数据集上的实验证明了其有效性。 |
| [^126] | [Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization.](http://arxiv.org/abs/2209.03549) | 本论文调查抽取式总结中存在的五种广泛的不忠实问题，并发现30%的抽取式摘要存在至少一种问题。为了自动检测这些问题，提出了新的度量标准 ExtEval。 |
| [^127] | [PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm.](http://arxiv.org/abs/2208.07914) | PD-MORL是一种基于偏好的多目标强化学习算法，其采用偏好作为指导，适应于各种偏好空间的目标，可扩展到连续的机器人任务，并在多目标机器人控制任务上优于现有的MORL方法。 |
| [^128] | [RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents.](http://arxiv.org/abs/2208.06448) | RLang是一种声明性语言，可以为强化学习智能体提供部分世界模型的信息，包括无模型和有模型表格算法、策略梯度和基于价值的方法、分层方法和深度方法。 |
| [^129] | [Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones.](http://arxiv.org/abs/2207.13700) | 本研究提出了一种使用智能手机时间序列数据预测帕金森病患者药物治疗状态的方法，通过考察病人个体的历史记录、使用 Transformer 模型学习注意权重，实现了较好的客观预测结果，为个性化远程健康传感提供了一种创新方法。 |
| [^130] | [When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes.](http://arxiv.org/abs/2207.08336) | 该论文研究在半隐私场景下如何通过本地差分隐私（LDP）实现公平分类，解决了在收集大规模用户敏感属性时的难题。 |
| [^131] | [Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior.](http://arxiv.org/abs/2206.13498) | 本研究评估了可视化方法检测模型异常行为的能力，发现现有方法难以识别微妙的异常行为，并且无法识别导致异常行为的输入。因此，需要开发更可靠的模型透明度方法。 |
| [^132] | [Pyramid Fusion Transformer for Semantic Segmentation.](http://arxiv.org/abs/2201.04019) | 本文提出了一种基于变换器的金字塔融合方法用于语义分割，来挖掘跨特征金字塔的丰富语义信息，取得了有竞争力的表现。 |
| [^133] | [C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System.](http://arxiv.org/abs/2201.02732) | 本文提出了一种新的粗到细对比学习框架，用于对话式推荐系统中多类型外部数据的数据语义融合，在此基础上提高了数据的处理效率。 |
| [^134] | [Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?.](http://arxiv.org/abs/2109.15284) | 在移动设备上部署复杂 AI 模型需要平衡准确性和复杂性，设计决策对实现高准确度和低资源消耗有影响。 |

# 详细

[^1]: CONA: 一种新颖的基于上下文的指令范式，用于使用大型语言模型的通信

    CONA: A novel CONtext-Aware instruction paradigm for communication using large language model. (arXiv:2305.18620v1 [cs.CL])

    [http://arxiv.org/abs/2305.18620](http://arxiv.org/abs/2305.18620)

    CONA是一种基于上下文的指令范式，利用大型语言模型，自动优化演示内容并提供上下文感知型答案，具有较高的上下文感知性和易理解性。

    

    我们介绍了CONA，这是一种新颖的基于上下文的指令范式，利用生成预训练转换器（GPT）模型有效进行知识传播。CONA是一个灵活的框架，旨在利用大型语言模型（LLMs）的能力，并结合DIKW（数据、信息、知识、智慧）层级自动指导和优化演示内容，预测潜在的听众问题，并适应听众群体的知识水平提供上下文感知型答案。CONA范式的独特之处在于其独立咨询机制和根植于DIKW层级的递归反馈循环的组合。这种协同作用显著提高了上下文感知内容，确保听众可以轻松理解和获取。这种范式是探索LLM时代的知识传播和沟通的新方法的早期先驱，为日常知识共享场景提供有效支持。

    We introduce CONA, a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models. CONA is a flexible framework designed to leverage the capabilities of Large Language Models (LLMs) and incorporate DIKW (Data, Information, Knowledge, Wisdom) hierarchy to automatically instruct and optimise presentation content, anticipate potential audience inquiries, and provide context-aware answers that adaptive to the knowledge level of the audience group. The unique aspect of the CONA paradigm lies in its combination of an independent advisory mechanism and a recursive feedback loop rooted on the DIKW hierarchy. This synergy significantly enhances context-aware contents, ensuring they are accessible and easily comprehended by the audience. This paradigm is an early pioneer to explore new methods for knowledge dissemination and communication in the LLM era, offering effective support for everyday knowledge sharing scenarios. We
    
[^2]: 数学与逻辑问题中的聊天机器人: ChatGPT-3.5、ChatGPT-4和Google Bard的初步比较和评估

    Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard. (arXiv:2305.18618v1 [cs.CL])

    [http://arxiv.org/abs/2305.18618](http://arxiv.org/abs/2305.18618)

    本文比较了三种基于大型语言模型的聊天机器人(ChatGPT-3.5、ChatGPT-4和Google Bard)在解决数学和逻辑问题上的正确性，研究发现这些机器人可以在某些情况下给出正确答案，但在更复杂的问题中需要改进。

    

    本文介绍了三个基于大型语言模型的聊天机器人(ChatGPT-3.5、ChatGPT-4和Google Bard)在解决数学和逻辑问题时的正确性对比。我们使用30个清晰、无二义性、仅使用纯文本且具有独特定义的正确答案的问题，分为两组并分别向每个聊天机器人提出三遍。通过记录并讨论问题的答案，我们总结了它们的优点和缺点。研究表明，对于简单的算术和代数表达式...

    A comparison between three chatbots which are based on large language models, namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their ability to give correct answers to mathematics and logic problems. In particular, we check their ability to Understand the problem at hand; Apply appropriate algorithms or methods for its solution; and Generate a coherent response and a correct answer. We use 30 questions that are clear, without any ambiguities, fully described with plain text only, and have a unique, well defined correct answer. The questions are divided into two sets of 15 each. The questions of Set A are 15 "Original" problems that cannot be found online, while Set B contains 15 "Published" problems that one can find online, usually with their solution. Each question is posed three times to each chatbot. The answers are recorded and discussed, highlighting their strengths and weaknesses. It has been found that for straightforward arithmetic, algebraic expressions
    
[^3]: 抓住机遇，迎接挑战：在本科跨学科合作学习中使用ChatGPT

    Embrace Opportunities and Face Challenges: Using ChatGPT in Undergraduate Students' Collaborative Interdisciplinary Learning. (arXiv:2305.18616v1 [cs.CY])

    [http://arxiv.org/abs/2305.18616](http://arxiv.org/abs/2305.18616)

    通过在本科生跨学科合作学习中使用ChatGPT，可以促进其跨学科问题解决、身体和认知投入，尤其对STEM学生产生积极作用。

    

    ChatGPT于2022年11月推出后，受到全球学生和教育工作者的广泛关注，华《2023》的一份在线报告称其是历史上增长最快的消费应用程序。虽然在高等教育中讨论使用ChatGPT的话题很多，但其对跨学科合作学习的影响的实证研究很少。为了探究其潜力，我们进行了一项拟实验性研究，招募了130名本科生（STEM和非STEM），他们在两周内学习数字素养，有些使用ChatGPT，有些不使用。我们进行了有关跨学科问题解决、身体和认知投入以及对Chat GPT使用的个人反思的周度调查。对调查结果的分析显示，话题对跨学科问题解决和身体、认知投入有显著的主效应，学科背景和ChatGPT条件对认知投入有较小的交互效应， ChatGPT对STEM学生具有积极作用。

    ChatGPT, launched in November 2022, has gained widespread attention from students and educators globally, with an online report by Hu (2023) stating it as the fastest-growing consumer application in history. While discussions on the use of ChatGPT in higher education are abundant, empirical studies on its impact on collaborative interdisciplinary learning are rare. To investigate its potential, we conducted a quasi-experimental study with 130 undergraduate students (STEM and non-STEM) learning digital literacy with or without ChatGPT over two weeks. Weekly surveys were conducted on collaborative interdisciplinary problem-solving, physical and cognitive engagement, and individual reflections on ChatGPT use. Analysis of survey responses showed significant main effects of topics on collaborative interdisciplinary problem-solving and physical and cognitive engagement, a marginal interaction effect between disciplinary backgrounds and ChatGPT conditions for cognitive engagement, and a signi
    
[^4]: 基于位置感知图增强变分自编码器的网络时间序列插补

    Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])

    [http://arxiv.org/abs/2305.18612](http://arxiv.org/abs/2305.18612)

    本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。

    

    多元时间序列插补是近年来广泛研究的问题。现有方法可以分为两大类，包括（1）主要关注时间序列特征的深度递归或生成模型，以及（2）基于图神经网络（GNN）的模型，利用MTS固有图结构的拓扑信息作为插补的关系归纳偏差。然而，这些方法要么忽略了拓扑信息，要么假定图结构固定且准确已知。因此，在更具挑战性的网络时间序列（NTS）数据中，它们无法充分利用图动态进行精确的插补，其中底层图不断变化并可能存在缺失边。本文提出了一种新方法来克服这些限制。首先，我们定义了包含节点时间序列特征和图结构中缺失值的NTS插补问题。然后，我们设计了一种名为PGE-VAE的新模型，它利用定位编码技术将时间序列信息合并到图神经网络中。具体而言，我们建议使用自我注意机制来捕捉图中不同时间步骤和不同节点之间的依赖关系。此外，我们引入了一个动态图生成网络来学习图结构的演化，可以处理缺失的边并适应图动态。在合成和真实数据集上的广泛实验表明，我们提出的方法优于现有最先进的方法。

    Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
    
[^5]: 神经网络修复安全漏洞的有效性有多高？

    How Effective Are Neural Networks for Fixing Security Vulnerabilities. (arXiv:2305.18607v1 [cs.SE])

    [http://arxiv.org/abs/2305.18607](http://arxiv.org/abs/2305.18607)

    这篇论文比较了使用大型代码语言模型和自动化程序修复技术修复Java漏洞的能力，并提供了新的Java漏洞修复基准。在两个真实的Java漏洞基准上进行评估。

    

    安全漏洞修复是一项需要自动化的困难任务。两组技术表现出了希望：（1）大型代码语言模型（LLMs），它们已经针对代码完成等任务进行了预训练，以及（2）使用深度学习（DL）模型自动修复软件错误的自动化程序修复（APR）技术。本文是首次研究和比较LLMs和DL-based APR模型在Java漏洞修复能力方面的论文。这项工作的贡献包括：（1）将并评估五个LLMs（Codex、CodeGen、CodeT5、PLBART和InCoder）、四个精细调整的LLMs和四种基于DL的APR技术在两个真实世界的Java漏洞基准（Vul4J和VJBench）上，（2）设计代码转换来解决对Codex的训练和测试数据重叠威胁，（3）创建一个新的Java漏洞修复基准VJBench，以及它的转化版本VJBench-trans，（4）评估LLMs和APR技术对转化的漏洞。

    Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs.  This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities
    
[^6]: 利用可解释性设计对抗攻击和评估仇恨言论检测模型的鲁棒性

    Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])

    [http://arxiv.org/abs/2305.18585](http://arxiv.org/abs/2305.18585)

    本文结合可解释性和对抗攻击两个关键方面，研究了仇恨言论检测模型的鲁棒性，并提出了针对性的对抗攻击，对该领域的未来研究具有重要的意义。

    

    社交媒体的出现带来了许多伦理问题，其中仇恨言论是最重要的问题之一。研究人员正在尝试通过利用仇恨言论检测和使用语言模型来自动审查内容并促进文明讨论来解决这个问题。不幸的是，最近的研究表明，仇恨言论检测系统可能会被对抗攻击所误导，引起其鲁棒性的关注。尽管以前的研究已单独讨论了在对抗攻击下模型的鲁棒性和可解释性，但没有全面研究他们的交集。我们的工作的创新在于结合这两个关键方面，利用可解释性识别潜在的漏洞，从而设计有针对性的对抗攻击。我们对各种仇恨言论检测模型表现出的对抗鲁棒性进行了全面和比较分析。

    The advent of social media has given rise to numerous ethical challenges, with hate speech among the most significant concerns. Researchers are attempting to tackle this problem by leveraging hate-speech detection and employing language models to automatically moderate content and promote civil discourse. Unfortunately, recent studies have revealed that hate-speech detection systems can be misled by adversarial attacks, raising concerns about their resilience. While previous research has separately addressed the robustness of these models under adversarial attacks and their interpretability, there has been no comprehensive study exploring their intersection. The novelty of our work lies in combining these two critical aspects, leveraging interpretability to identify potential vulnerabilities and enabling the design of targeted adversarial attacks. We present a comprehensive and comparative analysis of adversarial robustness exhibited by various hate-speech detection models. Our study e
    
[^7]: 利用GPT-4进行可控文本生成的图像生成

    Controllable Text-to-Image Generation with GPT-4. (arXiv:2305.18583v1 [cs.CV])

    [http://arxiv.org/abs/2305.18583](http://arxiv.org/abs/2305.18583)

    利用GPT-4编写TikZ代码可提高文本到图像生成的控制性和保真度。

    

    当前的文本到图像生成模型往往难以按照文本说明进行操作，特别是那些需要空间推理的模型。另一方面，大语言模型（LLMs），如GPT-4，在生成用于通过TikZ图解文本输入的代码片段方面表现出了极高的准确性。在这项工作中，我们引入了Control-GPT来为基于扩散的文本到图像管道提供指导，GPT-4生成编程草图，增强了他们遵循指示的能力。Control-GPT通过查询GPT-4来编写TikZ代码，生成的草图与文本说明一起用作扩散模型（例如ControlNet）生成逼真图像的参考资料。训练我们的管道的一个主要挑战是缺乏包含对准文本，图像和草图的数据集。我们通过将现有数据集中的实例掩码转换为多边形来模仿测试时使用的草图来解决这个问题。结果，Control-GPT极大地提高了文本到图像生成的可控性和保真度，取得了在几个基准数据集上的惊人成果。

    Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly
    
[^8]: 在GPU上进行基于搜索的正则表达式推断

    Search-Based Regular Expression Inference on a GPU. (arXiv:2305.18575v1 [cs.PL])

    [http://arxiv.org/abs/2305.18575](http://arxiv.org/abs/2305.18575)

    此研究提出了一种基于搜索空间矩阵的算法来进行正则表达式推断，是具有时间换空间的枚举算法；中缀闭包使操作更为高效。

    

    正则表达式推断(REI)是一个监督式机器学习和程序综合问题，它接受 正、反样例 和 正则表达式的成本度量 作为输入，并输出一种精确的正则表达式(即接受所有正样例和拒绝所有反样例)，并相对于成本度量是最小的。我们提出了一种新的REI算法，可用于任意字母表，该算法具有枚举性，并以时间换空间。我们的主要算法思想是将正则表达式的搜索空间简洁地实现为位向量矩阵的连续形式。总体上，在结果矩阵中，所有子语言都被表示为特征序列，对于正、反例子的并集使用 infix-closure。其数学本质上是(正式幂级数的变体)的半环。中缀闭包能够通过使用我们的半环的操作，从底部向上地通过构成更大的正则表达式，最小化数据移动。

    Regular expression inference (REI) is a supervised machine learning and program synthesis problem that takes a cost metric for regular expressions, and positive and negative examples of strings as input. It outputs a regular expression that is precise (i.e., accepts all positive and rejects all negative examples), and minimal w.r.t. to the cost metric. We present a novel algorithm for REI over arbitrary alphabets that is enumerative and trades off time for space. Our main algorithmic idea is to implement the search space of regular expressions succinctly as a contiguous matrix of bitvectors. Collectively, the bitvectors represent, as characteristic sequences, all sub-languages of the infix-closure of the union of positive and negative examples. Mathematically, this is a semiring of (a variant of) formal power series. Infix-closure enables bottom-up compositional construction of larger from smaller regular expressions using the operations of our semiring. This minimises data movement an
    
[^9]: ChatGPT的公平性评估

    Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])

    [http://arxiv.org/abs/2305.18569](http://arxiv.org/abs/2305.18569)

    本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。

    

    理解和解决LLM中不公平的问题对于负责任的AI部署至关重要。然而，在将LLM应用于高风险领域时，尤其是关于公平评估方面，数量分析和深入研究的可用性有限。本文旨在提供一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，我们专注于评估ChatGPT在包括教育、犯罪学、金融和医疗保健等高风险领域的表现。为了进行全面的评估，我们考虑了群体公平性和个人公平性，并观察了在一系列有偏或无偏提示下ChatGPT输出的差异。该研究对于更深入的了解LLM的公平表现，便于偏见缓解，促进负责任的人工智能系统的发展具有意义。

    Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
    
[^10]: 可控毁灭路径

    Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])

    [http://arxiv.org/abs/2305.18553](http://arxiv.org/abs/2305.18553)

    本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。

    

    毁灭路径（PoD）是一种自我监督的迭代生成器学习方法。其核心思想是通过破坏一组物品来产生一个训练集，为每个破坏步骤创建一个与相应修复动作相关的训练实例。在此数据集上训练的生成器可以通过从任意状态“修复”来生成新的物品。PoD方法在原始训练示例方面非常节省，并且非常适合由分类数据组成的功能部件，例如游戏关卡和离散的3D结构。在本文中，我们将毁灭路径方法扩展到允许设计师控制生成的物品的各个方面。通过向构成修复轨迹的状态-动作对添加条件输入来引入可控性。我们在2D地牢设置以及小型3D乐高汽车领域测试了可控PoD方法。

    Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
    
[^11]: RLAD：基于像素的强化学习在城市环境下自动驾驶中的应用

    RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments. (arXiv:2305.18510v1 [cs.CV])

    [http://arxiv.org/abs/2305.18510](http://arxiv.org/abs/2305.18510)

    RLAD是首个在城市自动驾驶领域应用基于像素的强化学习方法，通过优化图像编码器和路径点编码器等技术，可以提高自动驾驶的性能和效率。

    

    当前应用于城市自动驾驶的强化学习（RL）方法主要集中在将感知训练与驾驶策略训练分离。主要原因是为了避免在策略网络旁边训练卷积编码器，因为这种方法在样本效率、特征表示退化和自我过度拟合等方面存在问题。然而，这种方法可能会导致环境表示与下游任务不一致，从而导致次优性能。本文提出了RLAD，这是首个在城市自动驾驶领域应用基于像素的强化学习（RLfP）方法。我们提出了几种技术来提高在此领域中RLfP算法的性能，包括：i）一种图像编码器，利用图像增强和自适应局部信号混合（A-LIX）层的优势；ii）WayConv1D，一种利用2D几何信息的路径点编码器。

    Current approaches of Reinforcement Learning (RL) applied in urban Autonomous Driving (AD) focus on decoupling the perception training from the driving policy training. The main reason is to avoid training a convolution encoder alongside a policy network, which is known to have issues related to sample efficiency, degenerated feature representations, and catastrophic self-overfitting. However, this paradigm can lead to representations of the environment that are not aligned with the downstream task, which may result in suboptimal performances. To address this limitation, this paper proposes RLAD, the first Reinforcement Learning from Pixels (RLfP) method applied in the urban AD domain. We propose several techniques to enhance the performance of an RLfP algorithm in this domain, including: i) an image encoder that leverages both image augmentations and Adaptive Local Signal Mixing (A-LIX) layers; ii) WayConv1D, which is a waypoint encoder that harnesses the 2D geometrical information of
    
[^12]: 代码提示：用于大语言模型中复杂推理的神经符号方法

    Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])

    [http://arxiv.org/abs/2305.18507](http://arxiv.org/abs/2305.18507)

    本文介绍了一种神经符号提示方法——代码提示，该方法可以触发代码作为中间步骤。与自然语言相比，代码提示有着几个独特优势，能够提高符号推理和算术推理的性能，并且通常优于思路链提示。

    

    大语言模型已经通过各种提示方法扩大了规模，以解锁广泛的复杂推理任务。然而，当前的提示方法生成自然语言中间步骤以帮助推理，这可能导致不完善的任务缩减和混淆。为了缓解这样的限制，我们探索了代码提示，一种神经符号提示方法，具有零-shot和少-shot版本，可以触发代码作为中间步骤。我们在涉及符号推理和算术推理的7个广泛使用的基准测试中进行了实验。代码提示通常优于思路链提示。为了进一步了解代码提示的性能和限制，我们进行了广泛的消融研究和错误分析，并确定了使用符号提示相对于自然语言的几个独特优势。我们还考虑了代码提示和思路链提示的集合，以结合两者的优势。最后，我们展示了...

    Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show throu
    
[^13]: 如何有效地在强化学习中进行人类反馈查询？

    How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])

    [http://arxiv.org/abs/2305.18505](http://arxiv.org/abs/2305.18505)

    该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。

    

    人类反馈强化学习（RLHF）是一种范例，在此范例下，RL代理学习使用对轨迹的成对优先级反馈来最优化任务，而不是使用明确的奖励信号。尽管RLHF在微调语言模型方面已经取得了实用成功，但现有的实证研究并未解决如何高效采样轨迹对以查询人类反馈的挑战。在本研究中，我们提出了一种有效的采样方法，用于获取探索性轨迹，在收集任何人类反馈之前，使学习隐藏的奖励函数更加准确。理论分析表明，与现有文献相比，我们的算法在线性参数化和未知过渡的基于偏好模型下学习最优策略所需的人类反馈更少。具体而言，我们的框架可以纳入线性和低秩MDPs。此外，我们研究了使用基于行动比较的反馈的RLHF，并介绍了一种高效的采样方法，以在优化具有有限反馈的任务时获得探索性轨迹。

    Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
    
[^14]: VAST：一种视听字幕文本全模态基础模型与数据集

    VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])

    [http://arxiv.org/abs/2305.18500](http://arxiv.org/abs/2305.18500)

    本文提出了一种全模态视频文本基础模型VAST及其数据集VAST-27M。该模型可以感知和处理视频中的视觉、音频和字幕模态，通过自动集成多模态字幕和资源，提供更好的支持文本关联的多种任务。

    

    当代视频文本基础模型已经完全探索了视觉和文本，而其他模态，如视频中的音频和字幕，却没有得到足够的关注。本文旨在通过探索自动生成的大规模全模态视频字幕数据集VAST-27M，建立多模态视频轨迹之间的连接，包括视觉、音频和字幕，并与文本进行关联。具体而言，我们首先收集了2700万个开放领域视频片段，并分别训练视觉和音频字幕生成器以生成视觉和音频字幕。然后，我们使用一个现有的大语言模型（LLM）将生成的字幕、字幕和指导提示集成到全模态字幕中。基于提出的VAST-27M数据集，我们训练了一种全模态视频文本基础模型VAST，它可以感知和处理视频中的视觉、音频和字幕模态，并更好地支持各种任务，包括视觉和文本之间的关联。

    Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
    
[^15]: ANPL：使用交互式分解编译自然程序

    ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])

    [http://arxiv.org/abs/2305.18498](http://arxiv.org/abs/2305.18498)

    ANPL是一个编程系统，可以让用户直接操作草图，使用自然语言描述注释模块或孔，并生成一个有机的Python程序，它优于基线。

    

    大型语言模型的出现在通过自然交互增强编程方面显示出了潜力。然而，虽然大型语言模型擅长将常见的使用模式编译为编程语言，例如Python，但如何编辑和调试由大型语言模型生成的程序仍然是一个挑战。我们介绍了ANPL，一种编程系统，允许用户分解特定于用户的任务。在ANPL程序中，用户可以直接操作草图，该草图指定生成的程序的数据流。用户使用自然语言描述注释模块或孔，将生成功能的昂贵任务卸载到大型语言模型中。给定一个ANPL程序，ANPL编译器会生成一个有机的Python程序，实现孔中的功能，并遵守草图中指定的数据流。我们将ANPL部署在抽象和推理语料库（ARC）上，它是一组对于最先进的AI系统而言具有挑战性的独特任务，结果表明它优于基线。

    The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
    
[^16]: DMS：基于侧信息的无需知道类别数和距离度量的聚类算法

    DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific Clustering Using Side Information. (arXiv:2305.18492v1 [cs.LG])

    [http://arxiv.org/abs/2305.18492](http://arxiv.org/abs/2305.18492)

    本研究提出了一种基于少量成对样例的侧信息直接学习数据聚类的方法DMS，与以往方法不同，我们无需知道类别数、类中心或者任何相似的距离度量，该方法可以根据侧信息的任务需求将相同的数据点分成不同的聚类，且在固有的和非固有的数据集任务上表现优异。

    

    本研究提出了一种新的聚类方法，通过少量成对样例的侧信息直接学习数据聚类。与以往方法不同，我们无需知道类别数、类中心或者任何相似的距离度量。该方法可以根据侧信息的任务需求将相同的数据点分成不同的聚类。受均值漂移算法启发，我们使用一种自定义的迭代神经网络来实现我们的聚类方法——Differentiable Mean Shift (DMS)，一种最先进的数据集无关聚类方法。我们发现可以训练一个强大的聚类定义，而不必强制要求每个簇在训练期间呈现。DMS在固有的和非固有的数据集任务上表现优异。

    We present a novel approach, in which we learn to cluster data directly from side information, in the form of a small set of pairwise examples. Unlike previous methods, with or without side information, we do not need to know the number of clusters, their centers or any kind of distance metric for similarity. Our method is able to divide the same data points in various ways dependant on the needs of a specific task, defined by the side information. Contrastingly, other work generally finds only the intrinsic, most obvious, clusters. Inspired by the mean shift algorithm, we implement our new clustering approach using a custom iterative neural network to create Differentiable Mean Shift (DMS), a state of the art, dataset agnostic, clustering method. We found that it was possible to train a strong cluster definition without enforcing a constraint that each cluster must be presented during training. DMS outperforms current methods in both the intrinsic and non-intrinsic dataset tasks.
    
[^17]: 基准数据集上 ChatGPT 的系统研究和全面评估

    A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])

    [http://arxiv.org/abs/2305.18486](http://arxiv.org/abs/2305.18486)

    本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。

    

    最近，如 ChatGPT 这样的大型语言模型（LLM）的开发引起了很多关注。然而，由于难以将该模型生成的产出与基本事实进行比较，因此其在基准学术数据集上的评估仍未充分探索。本文旨在对 ChatGPT 在包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务中的表现进行彻底评估。具体而言，我们在 140 个任务中评估了 ChatGPT，并分析了其在这些数据集中生成的 255K 次响应，这使我们的工作成为了在 NLP 基准测试中对 ChatGPT 进行的最大评估。简而言之，我们的研究旨在验证 ChatGPT 在各种任务中的优势和弱点，并为使用 LLM 的未来研究提供见解。我们还报告了一种新的迸发能力，即遵循多个查询指令。

    The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
    
[^18]: 基于自编码器的条件神经过程用于表示学习

    Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])

    [http://arxiv.org/abs/2305.18485](http://arxiv.org/abs/2305.18485)

    本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。

    

    条件神经过程(CNPs)是一种灵活高效的模型族群，可以从观测值中学习出一个随机过程。在视觉领域中，CNPs 在上下文图像补全中得到了特别的应用，即通过观察某些位置的像素值来预测其他未观察位置上的值的分布。然而，学习这样一个 CNP 的像素选择通常是随机的或者是通过一个简单的统计量(例如像素方差)导出的。本文将问题转变一下：一个 CNP 想要观察哪些像素？也就是说，哪些像素允许拟合 CNP，这样的像素能告诉我们一些关于潜在图像的信息吗？将提供给 CNP 的上下文视为固定大小的潜在表示，我们构建了一个一次性变分框架，部分像素空间变分自编码器(Partical Pixel Space VAE, PPS-VAE)，同时预测这个上下文，并学习一个 CNP。我们在一组视觉数据集上评估了 PPS-VAE，发现通过相对大小或变化预测像素的选择可以安排学习，且更准确地进行了上下文预测，并且可以对基本物理和文化概念进行有意义的表示。

    Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
    
[^19]: 一种基于强化学习和凸优化的混合框架，用于基于UAV的自主元宇宙数据收集

    A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection. (arXiv:2305.18481v1 [cs.LG])

    [http://arxiv.org/abs/2305.18481](http://arxiv.org/abs/2305.18481)

    本论文提出了一种使用无人机辅助的元宇宙网络模型，将资源分配和轨迹控制集成到系统模型中，设计了一种强化学习和凸优化的混合框架，以“合作”方式解决时间顺序优化问题，使得数据收集效率得到提高。

    

    无人机（UAV）由于成本和灵活性的优势，在提供通信服务方面具有很大的潜力，尤其是在新兴元宇宙和物联网（IoT）的背景下。本文考虑了一种UAV辅助的元宇宙网络，其中UAV扩展了基站（BS）的覆盖范围，收集路边单元（RSU）产生的元宇宙数据。为了提高数据收集效率，系统模型中集成了资源分配和轨迹控制。优化问题的时间依赖性使传统凸优化方法难以解决。基于所提出的UAV辅助的元宇宙网络系统模型，我们设计了一种强化学习和凸优化的混合框架，以“合作”方式解决时间顺序优化问题。仿真结果表明，所提出的框架能够在保证收集数据质量的同时，减少任务完成时间。

    Unmanned aerial vehicles (UAVs) are promising for providing communication services due to their advantages in cost and mobility, especially in the context of the emerging Metaverse and Internet of Things (IoT). This paper considers a UAV-assisted Metaverse network, in which UAVs extend the coverage of the base station (BS) to collect the Metaverse data generated at roadside units (RSUs). Specifically, to improve the data collection efficiency, resource allocation and trajectory control are integrated into the system model. The time-dependent nature of the optimization problem makes it non-trivial to be solved by traditional convex optimization methods. Based on the proposed UAV-assisted Metaverse network system model, we design a hybrid framework with reinforcement learning and convex optimization to {cooperatively} solve the time-sequential optimization problem. Simulation results show that the proposed framework is able to reduce the mission completion time with a given transmission 
    
[^20]: FMM-X3D：基于FPGA的X3D建模和映射用于人体动作识别

    FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition. (arXiv:2305.18479v1 [cs.CV])

    [http://arxiv.org/abs/2305.18479](http://arxiv.org/abs/2305.18479)

    本论文解决了将Human Action Recognition中最先进的模型之一——X3D映射到任何FPGA设备上的问题，并首次实现了针对这种复杂模型架构进行Human Action Recognition任务的目标。

    

    三维卷积神经网络越来越受到研究人员和实践者的关注，在监控系统、自动驾驶车辆、人体监测系统和视频检索等许多领域都有应用。然而，它们的广泛应用受到了高计算和内存要求的限制，特别是针对资源受限的系统。本文解决了将Human Action Recognition中最先进的模型之一——X3D映射到任何FPGA设备上的问题。提出的工具流生成了一个经过优化的基于流的硬件系统，考虑了FPGA设备的可用资源和外部存储器特性。所生成的设计推进了当前性能和准确性帕累托前沿，并首次实现了针对这种复杂模型架构进行Human Action Recognition任务的目标。

    3D Convolutional Neural Networks are gaining increasing attention from researchers and practitioners and have found applications in many domains, such as surveillance systems, autonomous vehicles, human monitoring systems, and video retrieval. However, their widespread adoption is hindered by their high computational and memory requirements, especially when resource-constrained systems are targeted. This paper addresses the problem of mapping X3D, a state-of-the-art model in Human Action Recognition that achieves accuracy of 95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflow generates an optimised stream-based hardware system, taking into account the available resources and off-chip memory characteristics of the FPGA device. The generated designs push further the current performance-accuracy pareto front, and enable for the first time the targeting of such complex model architectures for the Human Action Recognition task.
    
[^21]: 超越元数据：利用游戏设计参数进行跨版本电子竞技分析

    Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])

    [http://arxiv.org/abs/2305.18477](http://arxiv.org/abs/2305.18477)

    本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。

    

    电子竞技游戏是全球游戏市场的重要组成部分，并且是增长最快的游戏细分领域。这导致了电子竞技分析的领域产生，其使用游戏提取的遥测数据来为玩家、教练、播音员和其他利益相关者提供信息。与传统的体育比赛相比，电子竞技游戏的机制和规则经常发生快速变化。由于游戏参数的频繁更改，电子竞技分析模型的使用寿命可能很短，这在文献中很大程度上被忽略了。本文提取游戏设计信息（即补丁说明），利用聚类技术提出了一种新的角色表征形式。以Dota 2游戏中击杀次数的预测为案例，利用这种创新的角色表征技术训练了一个神经网络模型。然后将此模型的性能与包括常规技术在内的两个不同基线进行了评估。这个模型不仅达到了显著的表现水平，还克服了电子竞技游戏中版本更迭的困境。

    Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
    
[^22]: 扩散模型是多任务强化学习的有效规划器和数据合成器

    Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])

    [http://arxiv.org/abs/2305.18459](http://arxiv.org/abs/2305.18459)

    本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。

    

    扩散模型在视觉和NLP领域中展现出了高度表现力的生成能力。最近的研究表明，在强化学习领域中，扩散模型也能够有效地建模离线数据集中的复杂策略或轨迹。然而，这些研究仅限于单任务设置，没有考虑多任务的情况。本文旨在研究单一扩散模型在建模用于多个任务策略训练的大规模离线数据时的有效性，具有多样化和多模态的数据分布。具体而言，我们提出了Multi-Task Diffusion Model（MTDiff），这是一种基于扩散的方法，结合Transformer骨干和提示学习，用于多任务离线设置中的生成规划和数据合成。MTDiff利用大量可用于多任务数据中的知识，并在任务之间执行隐式知识共享以进行虚拟规划。

    Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
    
[^23]: 识别水印大语言模型的基线

    Baselines for Identifying Watermarked Large Language Models. (arXiv:2305.18456v1 [cs.LG])

    [http://arxiv.org/abs/2305.18456](http://arxiv.org/abs/2305.18456)

    该论文介绍了一套基线算法，用于识别带有水印的大型语言模型，这些算法分析了带有和不带有水印的LLM所产生的输出分布和逻辑分布之间的差异。

    

    我们考虑了一种新兴问题：如何识别广泛使用的，公开托管的，闭源的大型语言模型（LLM）中的水印方案。我们引入了一套基线算法，用于识别LLM水印，这些算法依赖于分析由带有和不带有水印的LLM生成的输出令牌和逻辑分布。值得注意的是，带有水印的LLMs tend to产生分布与标准模型 qualitatively and identifiably不同。此外，我们研究了在不同强度下识别水印的可行性，并考虑了每个识别机制在水印场景中的权衡。在此过程中，我们正式化了识别LLMs中的水印的具体问题，以及LLM水印和水印检测的一般性问题，为研究它们提供了框架和基础。

    We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs). We introduce a suite of baseline algorithms for identifying watermarks in LLMs that rely on analyzing distributions of output tokens and logits generated by watermarked and unmarked LLMs. Notably, watermarked LLMs tend to produce distributions that diverge qualitatively and identifiably from standard models. Furthermore, we investigate the identifiability of watermarks at varying strengths and consider the tradeoffs of each of our identification mechanisms with respect to watermarking scenario. Along the way, we formalize the specific problem of identifying watermarks in LLMs, as well as LLM watermarks and watermark detection in general, providing a framework and foundations for studying them.
    
[^24]: 具有因果亚结构的分子关系学习模型在数据分布变化时的鲁棒性

    Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])

    [http://arxiv.org/abs/2305.18451](http://arxiv.org/abs/2305.18451)

    本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。

    

    最近，分子关系学习引起了分子科学领域的广泛关注，其目标是预测分子对之间的相互作用行为。本文提出了一种鲁棒性强的分子关系学习模型CMRL，它通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。为此，我们首先假定基于分子科学领域知识的因果关系，并构建结构因果模型（SCM）来揭示变量之间的关系。基于SCM，我们引入了一个新的条件干预框架，其干预是基于成对分子条件的。使用条件干预框架，我们的模型成功地从因果亚结构中学习，并减轻了与化学反应虚假相关的快捷亚结构的混淆效应。本文在各种任务和真实和合成数据集上进行了广泛实验。

    Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
    
[^25]: 驯服AI Bot：大型语言模型中神经状态的可控性

    Taming AI Bots: Controllability of Neural States in Large Language Models. (arXiv:2305.18449v1 [cs.AI])

    [http://arxiv.org/abs/2305.18449](http://arxiv.org/abs/2305.18449)

    本文提出了一个问题，是否可以通过适当选择提示，控制AI bot到达任何状态，而研究表明训练良好的Bot能几乎确定地到达任何意义子集，具有可控性。

    

    我们解决了一个问题，即是否可以通过适当选择提示，将AI bot 控制到任何状态。为此，我们首先介绍了一个正式的“意义”定义，便于分析。然后，我们通过一些条件，表征了大型语言模型（LLM）所显然训练的“有意义的数据”和“训练良好的LLM”。虽然训练良好的LLM构建了一个欧几里得意义嵌入空间，但是意义本身并不形成一个向量（线性）子空间，而是一个商空间。我们然后表征了某个输入提示的状态所能到达的意义子集，并展示了训练良好的Bot能够到达任何意义，尽管概率很小。我们接着引入了更强的可控性概念，即“几乎确定可达性”，并展示了限制到意义空间时，AI bot是可控的。我们在引入功能特征的情况下这样做了。

    We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. To that end, we first introduce a formal definition of ``meaning'' that is amenable to analysis. Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs. While a well-trained LLM constructs an embedding space of meanings that is Euclidean, meanings themselves do not form a vector (linear) subspace, but rather a quotient space within. We then characterize the subset of meanings that can be reached by the state of the LLMs for some input prompt, and show that a well-trained bot can reach any meaning albeit with small probability. We then introduce a stronger notion of controllability as {\em almost certain reachability}, and show that, when restricted to the space of meanings, an AI bot is controllable. We do so after introducing a functional characte
    
[^26]: 基于稀疏提示的元策略网络中的持续任务分配

    Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])

    [http://arxiv.org/abs/2305.18444](http://arxiv.org/abs/2305.18444)

    本文提出的CoTASP可以通过学习过完备字典来生成稀疏掩码作为提示，从而从元策略网络中提取与每个任务相关的子网络，实现了快速适应新任务，同时保留了之前任务的共同知识。

    

    如何通过不断学习一系列任务来训练一个具有一般化能力的元策略，是当前强化学习面临的挑战。本文提出了一种名为“连续任务分配的稀疏提示（CoTASP）”的解决方案，通过学习过完备字典来生成稀疏掩码作为提示，从元策略网络中提取与每个任务相关的子网络。通过交替优化子网络和提示，CoTASP更新了元策略，通过训练特定于任务的策略来实现。然后更新字典，以使优化后的提示与任务嵌入相匹配，从而捕捉其语义相关性。因此，相关任务通过相似的提示在元策略网络中共享更多的神经元，而跨任务干扰导致遗忘被有效地约束。给定经过训练的元策略和更新后的字典，我们可以通过推导相应的提示来迅速适应新任务，从而从元策略中提取相关的子网络。我们在一组导航任务上评估了CoTASP，并展示了它在任务完成度、样本效率和泛化能力方面优于现有的基线方法。

    How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti
    
[^27]: 多任务学习优化Airbnb搜索之旅

    Optimizing Airbnb Search Journey with Multi-task Learning. (arXiv:2305.18431v1 [cs.IR])

    [http://arxiv.org/abs/2305.18431](http://arxiv.org/abs/2305.18431)

    本文提出了一种新的多任务深度学习模型架构Journey Ranker，来解决Airbnb搜索过程中的唯一挑战，即客户和主机的偏好，该模型可应用于多个用例。

    

    Airbnb是一个在线住宿和体验市场，客人通常需要花费数周来探索和比较多个物品，并在做出最后的预订请求之前平衡客人和主机的偏好。搜索过程的长期性质以及需要平衡客人和主机的偏好，这些都为Airbnb的搜索排名提供了独特的挑战。本文提出了一种新的多任务深度学习模型架构Journey Ranker，以解决这些挑战。Journey Ranker利用中间的客户操作作为里程碑（无论是积极的还是消极的）来更好地将客户推向成功的预订。它还使用上下文信息（如客户状态和搜索查询）来平衡客人和主机的偏好。其模块化和可扩展的设计包括四个模块，分离明确，可以方便地应用于Airbnb搜索排名以外的用例。

    At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranki
    
[^28]: 应用可解释的人工智能方法探究增材制造样品中输入变量和拉伸强度的相关性

    Employing Explainable Artificial Intelligence (XAI) Methodologies to Analyze the Correlation between Input Variables and Tensile Strength in Additively Manufactured Samples. (arXiv:2305.18426v1 [cs.LG])

    [http://arxiv.org/abs/2305.18426](http://arxiv.org/abs/2305.18426)

    本研究使用可解释的人工智能方法，探究了增材制造中输入变量和拉伸强度的相关性，发现Infill百分比和挤出温度对于拉伸强度具有最高的正相关和负相关影响。

    

    本研究探讨了包括Infill百分比、层高、挤出温度和打印速度在内的各种输入参数对于产生的增材制造物体的拉伸强度的影响。本研究的主要目标是提高我们对输入参数和拉伸强度之间相关性的理解，以及确定影响增材制造过程性能的关键因素。为了实现这一目标，我们首次引入了可解释的人工智能（XAI）技术，通过SHAP（Shapley Additive Explanations）方法解释机器学习模型的预测行为，以分析数据并获得有价值的洞察。我们的发现表明，Infill百分比和挤出温度与拉伸强度具有最高的相关性，分别为正相关和负相关。使用XAI技术可以更好地理解输入参数和增材制造中拉伸强度之间的相关性。

    This research paper explores the impact of various input parameters, including Infill percentage, Layer Height, Extrusion Temperature, and Print Speed, on the resulting Tensile Strength in objects produced through additive manufacturing. The main objective of this study is to enhance our understanding of the correlation between the input parameters and Tensile Strength, as well as to identify the key factors influencing the performance of the additive manufacturing process. To achieve this objective, we introduced the utilization of Explainable Artificial Intelligence (XAI) techniques for the first time, which allowed us to analyze the data and gain valuable insights into the system's behavior. Specifically, we employed SHAP (SHapley Additive exPlanations), a widely adopted framework for interpreting machine learning model predictions, to provide explanations for the behavior of a machine learning model trained on the data. Our findings reveal that the Infill percentage and Extrusion T
    
[^29]: 通过基于权重残差的低秩逼近实现精细调整模型的高效存储

    Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals. (arXiv:2305.18425v1 [cs.LG])

    [http://arxiv.org/abs/2305.18425](http://arxiv.org/abs/2305.18425)

    本论文提出了一种利用权重残差低秩特性实现精细调整模型高效存储的新方法ERE，并通过额外量化和分层秩分配来提高存储效率，实验结果表明该方法显著减少内存占用，同时保持性能。

    

    本文提出了一种利用权重残差的低秩特性来高效存储精细调整模型的方法。我们的关键观察是，大型超参数模型中的权重残差表现出更强的低秩特性。基于此，我们提出了一种名为高效残差编码（ERE）的新方法，通过近似低秩权重残差来实现对精细调整模型权重的高效存储。此外，我们分析了权重残差的稳健性，并通过使用额外的量化和分层秩分配来推动存储效率的极限。我们的实验结果表明，我们的方法可以在各种任务和模态下显著减少内存占用，同时保持性能。我们公开了代码。

    In this paper, we present an efficient method for storing fine-tuned models by leveraging the low-rank properties of weight residuals. Our key observation is that weight residuals in large overparameterized models exhibit even stronger low-rank characteristics. Based on this insight, we propose Efficient Residual Encoding (ERE), a novel approach that achieves efficient storage of fine-tuned model weights by approximating the low-rank weight residuals. Furthermore, we analyze the robustness of weight residuals and push the limit of storage efficiency by utilizing additional quantization and layer-wise rank allocation. Our experimental results demonstrate that our method significantly reduces memory footprint while preserving performance in various tasks and modalities. We release our code.
    
[^30]: HyperTime: 应对时间分布偏移的超参数优化方法

    HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts. (arXiv:2305.18421v1 [cs.LG])

    [http://arxiv.org/abs/2305.18421](http://arxiv.org/abs/2305.18421)

    本文提出了一种名为HyperTime的超参数优化方法，用于寻找时间上鲁棒的预测性能超参数，该方法在历史验证数据集上对平均验证损失和最坏情况验证损失设置了词典优先级顺序，并在多个带时间分布偏移的机器学习任务上表现强劲。

    

    本文提出了一种名为“HyperTime”的超参数优化方法，旨在寻找对未知测试数据的潜在时间分布偏移具有鲁棒性的超参数。我们的工作得到了一个重要观察的启发，即通过超参数优化，在许多情况下可以实现时间上的鲁棒预测性能。基于这一观察，我们借鉴了鲁棒优化文献中的最坏情况导向哲学，帮助找到这样的鲁棒性超参数配置。HyperTime在历史验证数据集上对平均验证损失和最坏情况验证损失设置了词典优先级顺序。我们对预期测试损失的上限进行了理论分析，揭示了我们的方法的独特优势。我们还在多个具有时间分布偏移的机器学习任务上展示了所提出方法的强大实证表现。

    In this work, we propose a hyperparameter optimization method named \emph{HyperTime} to find hyperparameters robust to potential temporal distribution shifts in the unseen test data. Our work is motivated by an important observation that it is, in many cases, possible to achieve temporally robust predictive performance via hyperparameter optimization. Based on this observation, we leverage the `worst-case-oriented' philosophy from the robust optimization literature to help find such robust hyperparameter configurations. HyperTime imposes a lexicographic priority order on average validation loss and worst-case validation loss over chronological validation sets. We perform a theoretical analysis on the upper bound of the expected test loss, which reveals the unique advantages of our approach. We also demonstrate the strong empirical performance of the proposed method on multiple machine learning tasks with temporal distribution shifts.
    
[^31]: 一瞥：重新思考视频不断学习中的时间信息

    Just a Glimpse: Rethinking Temporal Information for Video Continual Learning. (arXiv:2305.18418v1 [cs.CV])

    [http://arxiv.org/abs/2305.18418](http://arxiv.org/abs/2305.18418)

    本文提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。实验表明，在内存受到极端限制时，视频的多样性比时间信息更重要。

    

    增量学习是连续学习研究中最重要的设置之一，因为它与现实世界的应用场景密切相关。随着类别/任务数量的增加，由于受到内存大小的限制，灾难性遗忘会出现。在视频领域研究持续学习面临更大的挑战，因为视频数据包含大量帧，这会使回放记忆负担更重。目前的常见做法是从视频流中对帧进行子采样，并将其存储在回放记忆中。在本文中，我们提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。通过大量实验，我们表明在极端内存限制下，视频的多样性比时间信息更重要。因此，我们的方法侧重于从代表大量独特视频的少量帧中学习。我们在三个代表性视频数据集Kin上进行了实验。

    Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kin
    
[^32]: 从API学习学习：黑盒数据无关元学习

    Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])

    [http://arxiv.org/abs/2305.18413](http://arxiv.org/abs/2305.18413)

    该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。

    

    无数据元学习（DFML）旨在通过从一组预训练模型进行元学习而无需访问训练数据，从而实现高效学习新任务。现有的DFML工作仅能从（i）白盒和（ii）小规模预训练模型（iii）相同的架构中元学习，忽略了更实际的设置，即用户仅能通过任意模型架构和规模的API进行推断。为解决这个问题，我们提出了一个双层数据无关元知识蒸馏（BiDf-MKD）框架，将更通用的元知识从一组黑盒API转移到一个单一的元模型中。

    Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
    
[^33]: 一种用于分子多模态预训练的群对称随机微分方程模型。

    A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining. (arXiv:2305.18407v1 [cs.LG])

    [http://arxiv.org/abs/2305.18407](http://arxiv.org/abs/2305.18407)

    MoleculeSDE是用于分子多模态预训练的群对称随机微分方程模型，通过在输入空间中直接生成3D几何与2D拓扑之间的转换，它能够更有效地保存分子结构信息。

    

    分子预训练已经成为提高基于 AI 的药物发现性能的主流方法。然而，大部分现有的方法只关注单一的模态。最近的研究表明，最大化两种模态之间的互信息（MI）可以增强分子表示能力。而现有的分子多模态预训练方法基于从拓扑和几何编码的表示空间来估计 MI，因此丢失了分子的关键结构信息。为解决这一问题，我们提出了 MoleculeSDE。MoleculeSDE利用群对称（如 SE（3）-等变和反射-反对称）随机微分方程模型在输入空间中直接生成 3D 几何形状与 2D 拓扑之间的转换。它不仅获得更紧的MI界限，而且还能够有效地保存分子结构信息。

    Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multi-modal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous dow
    
[^34]: Dink-Net: 大规模图形神经聚类方法

    Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])

    [http://arxiv.org/abs/2305.18405](http://arxiv.org/abs/2305.18405)

    Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。

    

    近年来，深度图聚类通过深度神经网络将图形的节点进行分组取得了很大的进展，但现有方法无法处理百万节点的大图。为了解决这个问题，我们提出了一种可扩展的Dink-Net深度图聚类方法，利用了膨胀和收缩的思想。首先，通过区分带增强的跟不带增强的节点，自我监督方式学习表示形式。同时，将聚类中心初始化为可学习的神经网络参数。随后，通过对抗性方式最小化提出的集群膨胀损失和集群收缩损失，优化聚类分布。通过这些设置，我们将表示学习和聚类优化两个步骤统一为一个端到端框架，引导网络学习聚类友好的特征。此外，Dink-Net能很好地扩展到大规模的图形上，因为设计的膨胀收缩操作可以有效地减少计算和内存消耗。实验结果表明，Dink-Net在处理百万节点图形的各种基准数据集上优于现有的最先进方法，证明了该方法在大图聚类中的可扩展性和有效性。

    Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
    
[^35]: 一种元学习框架用于调整可信联邦学习保护机制的参数

    A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])

    [http://arxiv.org/abs/2305.18400](http://arxiv.org/abs/2305.18400)

    提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。

    

    可信联邦学习（TFL）通常利用保护机制来保证隐私安全。然而，保护机制不可避免地会引入效用损失或效率降低，同时保护数据隐私。因此，保护机制及其参数应该仔细选择，以在保护隐私泄露、效用损失和效率降低之间取得最佳平衡。为此，联邦学习从业者需要工具来衡量这三个因素，并优化它们之间的权衡，选择最适合手头应用的保护机制。基于这个要求，我们提出了一个框架，它(1)将TFL定义为找到保护机制来优化隐私泄露、效用损失和效率降低三者之间的权衡的问题；(2)正式定义了这三个因素的有界测量。然后，我们提出了一个元学习算法来近似解决此优化问题。

    Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
    
[^36]: 关于激活函数和规范化对初始化等距嵌入的影响

    On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])

    [http://arxiv.org/abs/2305.18399](http://arxiv.org/abs/2305.18399)

    本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。

    

    本文探讨了深度神经网络中倒数第二个 Gram 矩阵的结构，该矩阵包含与一批输入对应的输出之间的成对内积。在几种架构中，观察到在初始化时该 Gram 矩阵会随着深度变得退化，从而严重减缓训练速度。规范化层如批处理规范化或层规范化，在防止秩崩溃问题方面起着关键作用。然而现有的理论结果无法全面覆盖广泛用于 transformer 中的层规范化和有限深度下规范化的量化偏差。为了解决这个问题，我们证明了在初始化时，结合激活函数层使用的层规范化可以使多层感知机的 Gram 矩阵偏向指数级深度等距，并使用激活函数的 Hermite 展开来量化这个速度，从而填补了现有理论的空白。

    In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
    
[^37]: 图像生成中的不当行为缓解：反映世界丑陋是否有价值？

    Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?. (arXiv:2305.18398v1 [cs.CV])

    [http://arxiv.org/abs/2305.18398](http://arxiv.org/abs/2305.18398)

    该论文研究了文本驱动的图像生成模型复制不适当人类行为的问题，并提出了抑制生成不适当内容的策略，该策略利用模型对世界丑陋的表现与人类偏好对齐。

    

    近期，文本驱动的图像生成模型在图像质量和文本对齐方面取得了惊人的成果，并因此在越来越多的应用程序中得到应用。由于它们高度依赖于从网络上随机抓取的数十亿大小的数据集，因此它们还会复制不适当的人类行为。具体而言，我们证明了各种文本到图像生成模型的大规模不当退化，因此需要在部署时对其进行监视和调节。为此，我们评估了推理时的缓解策略，以抑制不合适内容的生成。我们的研究结果表明，我们可以使用模型对世界丑陋的表现来将其与人类偏好进行对齐。

    Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. Specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. To this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. Our findings show that we can use models' representations of the world's ugliness to align them with human preferences.
    
[^38]: 知识增强的推理蒸馏：面向知识密集型任务的小型语言模型

    Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])

    [http://arxiv.org/abs/2305.18395](http://arxiv.org/abs/2305.18395)

    本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。

    

    大型语言模型在需要复合知识理解的知识密集型推理任务中表现出了良好的性能。但是，由于计算要求高且涉及数据隐私，将此类模型部署到现实世界的应用中可能会具有挑战性。以往的研究专注于通过微调具有标记数据或蒸馏大型语言模型来构建任务特定的小型语言模型，但是由于小型语言模型在记忆所需知识方面的能力有限，这些方法不适用于知识密集型推理任务。在理论分析的基础上，我们提出了一种名为知识增强的推理蒸馏 (KARD) 的新方法，该方法微调小型语言模型以生成从外部知识库检索到的增强知识的依据。此外，我们还提出了一个神经重排器，用于获得与依据生成相关的文档。我们实证表明，KARD在三项知识密集型任务上显着优于以前的方法，并且在模型尺寸相同的情况下可以达到与LLMs可比较的结果。

    Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
    
[^39]: 增量学习器的后门攻击：一项实证评估研究

    Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (arXiv:2305.18384v1 [cs.CR])

    [http://arxiv.org/abs/2305.18384](http://arxiv.org/abs/2305.18384)

    本文提出了增量学习器的后门攻击可能存在的安全风险，并实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性。

    

    已经提出了大量的增量学习算法，以缓解在时间序列上处理顺序数据时出现的灾难性遗忘问题。然而，增量学习器的对抗鲁棒性尚未得到广泛验证，存在潜在的安全风险。具体而言，对于基于中毒的后门攻击，我们认为增量学习中流式数据的本质为对手提供了很大的便利，通过数据污染，对手可以在任何时间或时间序列上创建分布式和跨任务攻击，从而影响任何未知的先前或后续任务，并且仅需要注入极小数量的后门样本 (例如，根据我们的观察，仅需要注入0.1%）。为了吸引研究社区的关注，在本文中，我们从三个学习场景出发，实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性，特别是跨任务泛化效应。

    Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \textbf{any unknown} previous or subsequent task by data poisoning \textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect 
    
[^40]: 从大型矿石中提炼黄金: 基于关键样本选择的高效数据集蒸馏

    Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])

    [http://arxiv.org/abs/2305.18381](http://arxiv.org/abs/2305.18381)

    研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。

    

    数据效率学习近年来备受关注，特别是对于拥有大量多模型的现在，数据集蒸馏可以成为有效的解决方案。然而，数据集蒸馏过程本身仍然非常低效。在本文中，我们首先引用信息理论来建模蒸馏问题，观察到数据集蒸馏中存在严重的数据冗余，我们提出了一种方法来扩展现有的蒸馏算法，以便通过选择最有价值的样本来更好地利用这些训练样本。我们进一步对样本选择进行全面分析，并验证了其优化过程。这种新策略能够显著减少训练成本，扩大现有算法范围以对更庞大和多元化的数据集进行数据集蒸馏，例如，在某些情况下，只需要0.04％的训练数据就足以保持可比的蒸馏效果。此外，我们的策略能够持续提高性能，其贡献可能为蒸馏过程的动力学开辟新的分析方法。

    Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
    
[^41]: 学习增强自主系统验证中的假设生成

    Assumption Generation for the Verification of Learning-Enabled Autonomous Systems. (arXiv:2305.18372v1 [cs.AI])

    [http://arxiv.org/abs/2305.18372](http://arxiv.org/abs/2305.18372)

    本文提出了一种为安全保证提供假设的做法，以用于验证具有复杂环境和学习增强组件的自主系统，通过自动生成适当的DNN行为假设，来满足要求的安全属性。

    

    自主系统的安全保证具有挑战性，因为这些系统在需要使用深度神经网络（DNN）等学习增强组件的复杂环境中运行，用于视觉感知。 DNN由于规模大（可能具有成千上万个参数）、缺乏正式规范（DNN通常是从有标签的数据中学习，缺乏任何正式要求）以及对环境中微小变化的敏感性而难以分析。 我们提出了一种假设保证式组合方法，用于验证这种自主系统的系统级安全属性。 我们的洞察力在于，我们可以通过自动合成有保证的DNN行为的假设，在没有DNN感知组件的情况下分析系统，以保证满足所需的安全属性。 合成的假设是最弱的，因为它们表征了所有可能的DNN的输出序列。

    Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, 
    
[^42]: ColibriUAV: 一种具有事件和帧相机接口的超快、能效双优的神经型边缘处理无人机平台

    ColibriUAV: An Ultra-Fast, Energy-Efficient Neuromorphic Edge Processing UAV-Platform with Event-Based and Frame-Based Cameras. (arXiv:2305.18371v1 [cs.CV])

    [http://arxiv.org/abs/2305.18371](http://arxiv.org/abs/2305.18371)

    本文介绍了一种具有帧和事件相机接口的UAV平台ColibriUAV，其围绕一种新颖的低功耗RISC-V SoC Kraken设计，Kraken能够高效地处理来自DVS相机的事件数据和来自RGB相机的帧数据。

    

    动态视觉传感器（DVS）驱动的无人机越来越受到关注，尤其是受到生物启发的事件传感器微秒级反应时间的影响，相比RGB相机，增加了感知任务的稳健性，降低了延迟。本文介绍了ColibriUAV，这是一种具有帧和事件相机接口的UAV平台，可进行高效感知和近传感器处理。所提议的平台围绕Kraken设计，一种新颖的低功耗RISC-V SoC，其具有两个定位于脉冲神经网络和深度三值神经网络的硬件加速器。Kraken能够高效地处理来自DVS相机的事件数据和来自RGB相机的帧数据。Kraken的一个关键特征是其与DVS相机的集成型专用接口。本文对神经型事件UAV子系统的端到端延迟和能量效率进行了基准测试，展现了72的事件数据吞吐量。

    The interest in dynamic vision sensor (DVS)-powered unmanned aerial vehicles (UAV) is raising, especially due to the microsecond-level reaction time of the bio-inspired event sensor, which increases robustness and reduces latency of the perception tasks compared to a RGB camera. This work presents ColibriUAV, a UAV platform with both frame-based and event-based cameras interfaces for efficient perception and near-sensor processing. The proposed platform is designed around Kraken, a novel low-power RISC-V System on Chip with two hardware accelerators targeting spiking neural networks and deep ternary neural networks.Kraken is capable of efficiently processing both event data from a DVS camera and frame data from an RGB camera. A key feature of Kraken is its integrated, dedicated interface with a DVS camera. This paper benchmarks the end-to-end latency and power efficiency of the neuromorphic and event-based UAV subsystem, demonstrating state-of-the-art event data with a throughput of 72
    
[^43]: GPT 模型在化学领域到底有怎样的应用？八个任务的综合基准测试。

    What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])

    [http://arxiv.org/abs/2305.18365](http://arxiv.org/abs/2305.18365)

    本文建立了包括 8 个实际化学任务的综合基准测试，有力地证明了 LLM 在实际化学中的能力。

    

    具有强大自然语言处理能力的大型语言模型已被广泛应用于科学、金融和软件工程等领域。但是，LLM 是否有能力推动化学领域的进展仍不清楚。本文建立了包含 8 个实际化学任务的综合基准测试，包括名称预测、属性预测、产量预测、反应预测、反合成（从产物预测反应物）、基于文本的分子设计、分子字幕和试剂选择。我们使用广泛认可的数据集，包括 BBBP、Tox21、PubChem、USPTO 和 ChEBI，有力地证明了 LLM 在实际化学中的能力。在精心选择的示例中，对三种 GPT 模型（GPT-4、GPT-3.5 和 DaVinci-003）在零样本和少样本有上下文学习的设置中进行了评估。

    Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
    
[^44]: 迈向可解释的对话式推荐系统

    Towards Explainable Conversational Recommender Systems. (arXiv:2305.18363v1 [cs.IR])

    [http://arxiv.org/abs/2305.18363](http://arxiv.org/abs/2305.18363)

    本文提出了衡量对话式推荐系统的可解释性的十种评估视角，并通过构建新数据集来提高解释质量。

    

    传统推荐系统中的解释已经证明在帮助用户理解推荐合理性以及提高系统效率、透明度和可信度方面具有益处。在对话环境中，需要生成多个上下文化解释，这进一步增加了解释的挑战。为了更好地衡量对话式推荐系统的可解释性，我们提出了十种评估视角，结合传统推荐系统的概念和对话式推荐系统的特点。我们使用这些指标评估了五个已有的对话式推荐系统基准数据集，并观察到需要提高对话式推荐系统的解释质量的必要性。为了实现这一目标，我们采用手动和自动方法扩展了这些对话，并构建了一个新的对话式推荐系统数据集，名为“可解释推荐对话”（E-ReDial）。该数据集包括756个对话，超过2000条高质量的重写解释。我们比较了两种基线方法，并进行了实验验证。

    Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in conversational recommender systems (CRS), we propose ten evaluation perspectives based on concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approa
    
[^45]: 基于概念的解释模型敲门技术检验图像分类器的显著统计结论

    Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs. (arXiv:2305.18362v1 [cs.CV])

    [http://arxiv.org/abs/2305.18362](http://arxiv.org/abs/2305.18362)

    该研究提出了一种基于概念的解释模型敲门技术，可以在图像分类任务中找到显著的概念以避免误解，并控制误发现率（FDR）在某个值下，在合成和真实数据实验中得到验证。

    

    基于概念的分类器可以通过人类可理解的概念来解释深度学习模型在图像分类问题上的决策过程。然而，有时概念解释可能会导致误报，将不相关的概念误认为是预测任务的重要因素。我们的目标是找到用于分类的显著概念以防止误解。在本研究中，我们提出了一种方法，使用深度学习模型学习图像概念，然后使用Knockoff样本选择重要概念进行预测，并控制误发现率（FDR）在某个值下。我们在合成和真实数据实验中评估了所提出的方法。结果表明，该方法可以适当地控制FDR，同时选择高度可解释的概念，提高模型的可信度。

    A concept-based classifier can explain the decision process of a deep learning model by human-understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and then using the Knockoff samples to select the important concepts for prediction by controlling the False Discovery Rate (FDR) under a certain value. We evaluate the proposed method in our synthetic and real data experiments. Also, it shows that our method can control the FDR properly while selecting highly interpretable concepts to improve the trustworthiness of the model.
    
[^46]: 基于语义交互的交互式深度学习方法

    DeepSI: Interactive Deep Learning for Semantic Interaction. (arXiv:2305.18357v1 [cs.LG])

    [http://arxiv.org/abs/2305.18357](http://arxiv.org/abs/2305.18357)

    本文提出了一种基于语义交互的交互式深度学习方法，可以高效地学习用户和任务特定的数据表示，改善视觉分析应用中的语义交互，并通过比较验证了该方法的优势。

    

    本文提出了一种新颖的交互式深度学习方法，旨在改善视觉分析应用中的语义交互。语义交互推断分析人员在感知过程中的精确意图取决于底层数据表示的质量。我们提出了$\text{DeepSI}_{\text{finetune}}$框架，将深度学习集成到人在交互式感知管道中，具有两个重要属性。首先，深度学习从原始数据中提取有意义的表示，提高了语义交互推断的质量。其次，利用语义交互来微调深度学习表示，进一步提高了语义交互推断的质量。人机交互和深度学习之间的反馈循环使得可以高效地学习用户和任务特定的表示。为了评估将深度学习嵌入到语义交互循环中的优势，我们比较了$\text{DeepSI}_{\te

    In this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications. The ability of semantic interaction to infer analysts' precise intents during sensemaking is dependent on the quality of the underlying data representation. We propose the $\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into the human-in-the-loop interactive sensemaking pipeline, with two important properties. First, deep learning extracts meaningful representations from raw data, which improves semantic interaction inference. Second, semantic interactions are exploited to fine-tune the deep learning representations, which then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables efficient learning of userand task-specific representations. To evaluate the advantage of embedding the deep learning within the semantic interaction loop, we compare $\text{DeepSI}_{\te
    
[^47]: LLM和抽象推理语料库：成功、失败与基于对象表示的重要性

    LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])

    [http://arxiv.org/abs/2305.18354](http://arxiv.org/abs/2305.18354)

    本文通过分析GPT模型在抽象推理语料库上的表现，发现GPT在抽象推理任务中存在需要核心概念“核心知识”的限制。通过使用基于对象的表示方法和新的1D-ARC基准，GPT在抽象推理任务中取得了较好的表现。

    

    一个大型语言模型（LLM）能否解决简单的抽象推理问题？我们通过系统地分析GPT在抽象推理语料库（ARC）上的表现来探索这个广泛的问题，这是一个代表性的基准，从有限的例子中要求我们有些关于概念（如对象、目标状态、计数和基本几何）的“核心知识”以解决问题。当使用文本编码对二维输入输出网格的ARC任务进行编码时，GPT-4仅解决了50个最简单的任务中的13个。我们的失败分析显示，GPT-4识别对象并推理它们的能力受到表示任务中对象的文本的顺序性的显著影响。为了验证这个假设，我们设计了一个新的基准，1D-ARC，它由更有利于基于GPT的推理的一维（类似数组）任务组成，在这些任务上，GPT的表现确实比在（2D）ARC上更好。为了减轻这个问题，我们提出了一种新的基于对象的编码方案，它保留了对象之间的基本空间关系并实现了更好的推理。使用这种编码，GPT-4在ARC上的成功率大大提高，达到了45/50。我们的工作强调了使用基于对象的表示方法进行抽象推理任务的重要性，并揭示了LLM基于推理系统的局限性和机遇。

    Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we prop
    
[^48]: 多目标遗传算法用于多视角特征选择

    Multi-Objective Genetic Algorithm for Multi-View Feature Selection. (arXiv:2305.18352v1 [cs.NE])

    [http://arxiv.org/abs/2305.18352](http://arxiv.org/abs/2305.18352)

    多视角数据提高了预测模型的准确性，但也使得高维数据增加，影响模型泛化能力。研究者提出了一种多视角多目标特征选择遗传算法（MMFS-GA），用于从多视角数据中选择最优的特征子集以提高模型精度和可解释性。

    

    多视角数据集提供了不同形式的数据，可以通过提供补充信息来增强预测模型。但是，使用多视角数据会导致高维数据的增加，这对可以导致泛化能力差的预测模型带来显著挑战。因此，从多视角数据集中选择相关特征不仅可以解决不良的泛化能力，还可以增强模型的可解释性。尽管传统特征选择方法取得了成功，但它们在利用跨模态的内在信息、缺乏泛化性和适用于特定分类任务方面存在局限性。我们提出了一种新的遗传算法策略，以克服传统特征选择方法在多视角数据上的这些局限性。我们提出的方法称为多视角多目标特征选择遗传算法（MMFS-GA）。这种方法同时选择最优的特征子集。

    Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of feature
    
[^49]: 注意力论文：生成 AI 如何重塑数字阴影产业？

    Attention Paper: How Generative AI Reshapes Digital Shadow Industry?. (arXiv:2305.18346v1 [cs.CY])

    [http://arxiv.org/abs/2305.18346](http://arxiv.org/abs/2305.18346)

    本文分析了生成 AI 对数字阴影产业带来的挑战和机遇，从黑色/阴影产业的上游、中游和下游路径进行了技术分析，并提出了改进现有风险的未来方向。

    

    数字经济的迅速发展导致了各种黑色和阴影互联网产业的出现，这些产业可能存在潜在风险，可以通过数字风险管理（DRM）来识别和管理，其中包括应用机器学习和深度学习等不同技术。DRM 架构的发展已经受到数据形式变化的驱动。然而，AI 生成内容（AIGC）技术的发展，如 ChatGPT 和 Stable Diffusion，为黑色和阴影产业提供了强大的工具，可以个性化数据并生成逼真的图片和对话以进行欺诈活动。这对于 DRM 系统来控制来自数据生成源的风险并迅速应对快速变化的风险环境构成了挑战。本文旨在从黑色/阴影产业的上游、中游和下游路径提供 AIGC 的挑战和机会的技术分析，并提出改进现有风险的未来方向。

    The rapid development of digital economy has led to the emergence of various black and shadow internet industries, which pose potential risks that can be identified and managed through digital risk management (DRM) that uses different techniques such as machine learning and deep learning. The evolution of DRM architecture has been driven by changes in data forms. However, the development of AI-generated content (AIGC) technology, such as ChatGPT and Stable Diffusion, has given black and shadow industries powerful tools to personalize data and generate realistic images and conversations for fraudulent activities. This poses a challenge for DRM systems to control risks from the source of data generation and to respond quickly to the fast-changing risk environment. This paper aims to provide a technical analysis of the challenges and opportunities of AIGC from upstream, midstream, and downstream paths of black/shadow industries and suggest future directions for improving existing risk con
    
[^50]: 可视化编程中神经任务合成

    Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])

    [http://arxiv.org/abs/2305.18342](http://arxiv.org/abs/2305.18342)

    该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。

    

    通过合成新的内容，生成式神经模型在增强编程教育方面具有巨大的潜力。我们旨在设计神经模型，能够根据可视化编程环境下给定的规范自动生成编程任务。尽管近年来像 GPT-4 这样的大型生成模型获得了成功，但我们的初步结果显示，这些模型在合成可视化编程任务方面效果不佳，并且在逻辑和空间推理方面存在困难。我们提出了一种新颖的神经符号技术 NeurTaskSyn，该技术能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，合成编程任务。NeurTaskSyn 由两个部分构成：第一个部分通过模仿学习程序进行训练，生成可能的解决方案代码，第二个部分通过强化学习程序进行训练，指导底层符号执行引擎生成可视化任务。

    Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
    
[^51]: 使用编译器生成的强化学习反馈调整代码模型

    Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback. (arXiv:2305.18341v1 [cs.PL])

    [http://arxiv.org/abs/2305.18341](http://arxiv.org/abs/2305.18341)

    本文提出了一种叫做RLCF的方法，使用代码编译器反馈进一步训练预训练的大型语言模型，以生成符合目标分布的代码，并通过所有静态正确性检查，显著提高了性能。

    

    最近，对代码进行预训练的大型语言模型成为程序合成的主要方法。然而，这些模型生成的代码可能违反基本的语言级别不变性，从而降低下游任务的性能。本文提出了一种称为RLCF的方法，它使用代码编译器的反馈进一步训练预训练的大型语言模型。 RLCF将LLM视为通过RL代理逐步生成代码，并接收以下反馈：（i）编译器派生的反馈与所生成的代码是否通过一组正确性检查有关; （ii）不同LLM的反馈，与训练语料库中一组参考程序相似。这些反馈机制帮助所生成的代码在通过所有静态正确性检查的同时保持在目标分布中。RLCF是模型和语言无关的。我们在Java的MBJP和MathQA任务上进行了实证评估，实验结果表明，RLCF显著提高了性能。

    Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raise
    
[^52]: ChatGPT：AI生成内容的挑战与解决方案综述

    A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. (arXiv:2305.18339v1 [cs.CY])

    [http://arxiv.org/abs/2305.18339](http://arxiv.org/abs/2305.18339)

    本论文探讨了AI生成内容的工作原理、安全与隐私威胁、现状和未来挑战，并提供了针对这些问题的最新解决方案。

    

    随着大型人工智能模型比如ChatGPT的普及使用，AI生成内容（AIGC）日益受到关注，正在引领内容创作和知识表示方式实现范式转变。AIGC利用生成式大型AI算法来辅助或替代人类，根据用户提供的提示以更快的速度和更低的成本创建大规模、高质量和类人的内容。尽管在AIGC方面取得了显著进展，但安全、隐私、伦理和法律挑战仍需解决。本文深入调查了AIGC范式的工作原理、安全和隐私威胁、最新解决方案和未来挑战。具体而言，我们首先探讨了AIGC的技术实现、总体架构，并讨论了其工作模式和关键特征。然后，我们调查了针对AIGC的安全和隐私威胁分类法，并强调了GPT和AIGC技术的伦理和社会影响。接下来，我们全面回顾了现有的解决方案，以解决已确定的挑战，并讨论了AIGC领域未来的研究方向。最后，我们总结了我们的发现，并提出了一些未来发展AIGC的潜在研究方向。

    With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC tec
    
[^53]: 无需完美，也能闪耀：揭示合成图像的应用价值

    You Don't Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images. (arXiv:2305.18337v1 [cs.CV])

    [http://arxiv.org/abs/2305.18337](http://arxiv.org/abs/2305.18337)

    本文研究了合成图像在解决数据稀缺和隐私问题方面的应用，并建立了一个全面的合成图像评估体系，证明了合成图像的实用性。

    

    通过从深度生成模型生成的合成图像，可以解决数据稀缺和数据隐私问题。合成模型的选择主要基于图像质量的度量，大多数研究者偏爱产生逼真图像的合成图像，即具有良好保真度得分（如低的FID和高的PSNR）。然而，合成图像的质量不仅局限于保真度，应该评估一系列指标以全面衡量其质量。此外，图像质量评估指标并不是合成图像实用性的准确预估器，这些评估指标之间的关系也尚不清楚。本文建立了一个包括保真度、多样性、隐私和实用性等多项指标的合成图像全面评估体系。通过对超过10万张胸部X线图片及其合成副本的分析，我们展示了合成图像的实用性。

    Synthetic images generated from deep generative models have the potential to address data scarcity and data privacy issues. The selection of synthesis models is mostly based on image quality measurements, and most researchers favor synthetic images that produce realistic images, i.e., images with good fidelity scores, such as low Fr\'echet Inception Distance (FID) and high Peak Signal-To-Noise Ratio (PSNR). However, the quality of synthetic images is not limited to fidelity, and a wide spectrum of metrics should be evaluated to comprehensively measure the quality of synthetic images. In addition, quality metrics are not truthful predictors of the utility of synthetic images, and the relations between these evaluation metrics are not yet clear. In this work, we have established a comprehensive set of evaluators for synthetic images, including fidelity, variety, privacy, and utility. By analyzing more than 100k chest X-ray images and their synthetic copies, we have demonstrated that ther
    
[^54]: 具有流行度偏见的排名：自增强动态下的用户福利

    Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])

    [http://arxiv.org/abs/2305.18333](http://arxiv.org/abs/2305.18333)

    研究了物品流行度、质量和位置偏差对用户福利的影响，提出了通过探索减轻流行度偏见负面影响的算法。

    

    虽然已经确认流行度偏见在推荐（和其他基于排名的）系统中发挥作用，但其对用户福利的影响的详细分析仍然缺乏。我们提出了一种通用机制，通过它，物品的流行度、质量和位置偏差可以影响用户选择，并且可以负面影响各种推荐策略的集体用户效用。我们将问题表述为非平稳上下文脱靶机，强调不是为了消除流行度偏见而是为了减轻其负面影响而进行探索的重要性。首先，普通的有流行度偏差的推荐系统会通过混淆物品质量和流行度而引发线性遗憾。更一般地，我们展示了即使在线性设置下，由于流行度偏见的混淆效应，物品质量的可识别性也可能无法实现。然而，在足够变异的假设下，我们开发了一种高效的类UCB算法，并证明了有效的遗憾保证。我们通过实验验证了我们提出的算法的有效性，并证实了流行度偏见的负面影响。

    While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
    
[^55]: #REVAL：一种用于hashtag推荐的语义评估框架

    #REVAL: a semantic evaluation framework for hashtag recommendation. (arXiv:2305.18330v1 [cs.IR])

    [http://arxiv.org/abs/2305.18330](http://arxiv.org/abs/2305.18330)

    #REVAL是一种新颖的语义评估框架，用于解决传统评估方法无法考虑推荐和实际hashtag之间语义相关性问题。实验证明#REVAL在捕捉语义相关性方面是有效的。

    

    在许多在线社交网络系统中，自动评估hashtag推荐模型是一项基本任务。传统的评估方法是首先比较算法推荐的hashtag与实际的hashtag的精确对应关系，然后使用精确匹配的数量计算命中率、命中比率、精确度、召回率或F1分数。然而，这种评估方式忽略了推荐和实际hashtag之间的语义相关性。为了解决这个问题，我们提出了一种新颖的语义评估框架#REval，它包括一个称为BERTag的内部模块，可自动学习hashtag嵌入。我们使用提出的#REval-hit-ratio度量标准，研究了#REval框架在不同的词嵌入方法和推荐中的同义词和hashtag数量下的性能。在两个基准数据集上的实验表明，我们提出的框架在捕捉推荐和实际hashtag之间的语义相关性方面是有效的。

    Automatic evaluation of hashtag recommendation models is a fundamental task in many online social network systems. In the traditional evaluation method, the recommended hashtags from an algorithm are firstly compared with the ground truth hashtags for exact correspondences. The number of exact matches is then used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This way of evaluating hashtag similarities is inadequate as it ignores the semantic correlation between the recommended and ground truth hashtags. To tackle this problem, we propose a novel semantic evaluation framework for hashtag recommendation, called #REval. This framework includes an internal module referred to as BERTag, which automatically learns the hashtag embeddings. We investigate on how the #REval framework performs under different word embedding methods and different numbers of synonyms and hashtags in the recommendation using our proposed #REval-hit-ratio measure. Our experiments of the propo
    
[^56]: 激光超声可视化测试图像的缺陷检测深度CNN结构研究

    A Study on Deep CNN Structures for Defect Detection From Laser Ultrasonic Visualization Testing Images. (arXiv:2305.18327v1 [cs.CV])

    [http://arxiv.org/abs/2305.18327](http://arxiv.org/abs/2305.18327)

    本文提出了一种适用于激光超声可视化测试图像中自动缺陷检测和定位的深度神经网络。在实际数据上进行的数值实验表明，该方法在预测性能方面比通用对象检测模型更有效且计算时间更短。

    

    近年来，超声无损检测的重要性不断增加。将激光超声检测与散波可视化技术相结合的激光超声可视化测试具有潜在的应用前景。然而，即使散波被可视化，检验员仍需要仔细检查图像。为了实现自动化，本文提出了一种用于LUVT图像中的自动缺陷检测和定位的深度神经网络。我们将LUVT图像分析问题与通用对象检测问题进行比较，以探索适合该任务的神经网络结构。使用来自SUS304平板的实际数据进行的数值实验表明，该方法在预测性能方面比通用对象检测模型更有效。我们还表明，所需的预测计算时间比通用对象检测模型更快。

    The importance of ultrasonic nondestructive testing has been increasing in recent years, and there are high expectations for the potential of laser ultrasonic visualization testing, which combines laser ultrasonic testing with scattered wave visualization technology. Even if scattered waves are visualized, inspectors still need to carefully inspect the images. To automate this, this paper proposes a deep neural network for automatic defect detection and localization in LUVT images. To explore the structure of a neural network suitable to this task, we compared the LUVT image analysis problem with the generic object detection problem. Numerical experiments using real-world data from a SUS304 flat plate showed that the proposed method is more effective than the general object detection model in terms of prediction performance. We also show that the computational time required for prediction is faster than that of the general object detection model.
    
[^57]: BigVideo:一个用于多模式机器翻译的大规模视频字幕翻译数据集

    BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v1 [cs.CV])

    [http://arxiv.org/abs/2305.18326](http://arxiv.org/abs/2305.18326)

    提出了一个大规模的视频字幕翻译数据集BigVideo， 集成了4.5 million句子对和9981小时视频，设计了有歧义和明确的测试集，引入了一种对比学习方法，实验结果表明，视觉信息可以提高NMT模型的BLEU、BLEURT和COMET得分，有助于消除歧义，数据集和翻译模型公开可用。

    

    我们提出了一个大规模的视频字幕翻译数据集BigVideo，以促进多模式机器翻译的研究。与广泛使用的How2和VaTeX数据集相比，BigVideo超过10倍，包括450万个句子对和9981小时的视频。我们还引入了两个有意设计的测试集来验证视觉信息的必要性：一个是一个有歧义词的不确定集合，另一个是在其中文本上下文对于翻译是自包含的明确集合。为了更好地建模文本和视频共享的共同语义，我们在跨模态编码器中引入了一种对比学习方法。在BigVideo上进行的广泛实验表明：a）视觉信息在歧义和明确的测试集上始终提高NMT模型的BLEU、BLEURT和COMET得分。b）视觉信息对于术语定位得分和人工评估而言，有助于消除歧义，与强文本基线相比。数据集和我们的翻译模型都是公开可用的。

    We present a large-scale video subtitle translation dataset, BigVideo, to facilitate the study of multi-modality machine translation. Compared with the widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: Ambiguous with the presence of ambiguous words, and Unambiguous in which the text context is self-contained for translation. To better model the common semantics shared across texts and videos, we introduce a contrastive learning method in the cross-modal encoder. Extensive experiments on the BigVideo show that: a) Visual information consistently improves the NMT model in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test sets. b) Visual information helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation. Dataset and our 
    
[^58]: 基于预训练语言模型的正则表达式增强的领域迁移主题分类：金融领域中的应用

    Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])

    [http://arxiv.org/abs/2305.18324](http://arxiv.org/abs/2305.18324)

    本文介绍了将正则表达式模式作为领域知识特征与领域特定文本一起用于微调预训练语言模型的方法，通过在金融领域中实验，证明这种方法可以改善下游文本分类任务的表现。

    

    使用大型预训练语言模型进行下游任务的常见方法是使用额外的层进行微调。但当下游领域是一个专业领域而大型语言模型已经在通用语料库上进行了预训练时，这种方法可能效果不佳。本文讨论了在微调过程中使用正则表达式模式作为领域知识的特征，以及领域特定文本。我们在实际生产数据上的实验表明，与仅在领域特定文本上进行微调相比，这种微调方法改善了下游文本分类任务的表现。我们还展示了使用注意力网络进行微调比简单的线性层获得更好的结果。

    A common way to use large pre-trained language models for downstream tasks is to fine tune them using additional layers. This may not work well if downstream domain is a specialized domain whereas the large language model has been pre-trained on a generic corpus. In this paper, we discuss the use of regular expression patterns employed as features for domain knowledge during the process of fine tuning, in addition to domain specific text. Our experiments on real scenario production data show that this method of fine tuning improves the downstream text classification tasks as compared to fine tuning only on domain specific text. We also show that the use of attention network for fine tuning improves results compared to simple linear layers.
    
[^59]: ReWOO：将推理与观察分离，实现高效增强语言模型

    ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])

    [http://arxiv.org/abs/2305.18323](http://arxiv.org/abs/2305.18323)

    ReWOO是一种将推理过程与外部观察分离的模块化范式，从而可以减少标记消耗并提高性能。

    

    增强语言模型（ALMs）将大型语言模型（LLMs）的推理能力与允许知识检索和行动执行的工具相结合。现有的ALM系统以交错方式触发LLM思考过程，同时从这些工具中提取观察。本研究首次提出了一种模块化范式ReWOO（Without Observation Reasoning），将推理过程与外部观察分离，从而显著减少标记消耗。在六个公共NLP基准测试和一个策划数据集中进行全面评估，结果显示采用我们提出的方法可以显著提高性能。

    Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology.
    
[^60]: REFinD: 金融领域关系抽取数据集

    REFinD: Relation Extraction Financial Dataset. (arXiv:2305.18322v1 [cs.CL])

    [http://arxiv.org/abs/2305.18322](http://arxiv.org/abs/2305.18322)

    REFinD是第一个完全基于金融文档生成的关系注释的大规模数据集，并对其进行了评估，显示出在金融领域进行关系抽取的特定挑战。

    

    创建了许多用于关系抽取（RE）的数据集，以帮助下游任务，如信息检索、语义搜索、问答和文本蕴含。然而，这些数据集未能捕捉到金融领域的特定挑战，因为大多数数据集使用维基百科、基于网络的文本和新闻文章等一般知识来源编制，阻碍了在金融世界中的实际进展和采用。为了解决这个限制，我们提出了REFinD，该数据集是第一个完全基于金融文档生成的关系注释的大规模数据集，涵盖8种类型的实体对之间的22个关系和约29K个实例。我们还提供了与各种最先进模型的经验评估，作为RE任务的基准，并强调了我们的数据集所面临的挑战。我们观察到，各种最先进的深度学习模型在数字推理、关系和方向模糊等方面都存在困难。

    A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity.
    
[^61]: 认知网络科学揭示GPT-3、ChatGPT和GPT-4中的偏见：反映高中学生数学焦虑

    Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students. (arXiv:2305.18320v1 [cs.CY])

    [http://arxiv.org/abs/2305.18320](http://arxiv.org/abs/2305.18320)

    本研究通过行为forma mentis网络(BFMNs)的方法研究了先进的语言模型(GPT-3，Chat-GPT和GPT-4)对数学和STEM领域的偏见，并发现它们都倾向于将数学和STEM领域视为困难、紧张和引起焦虑。

    

    大型语言模型正在逐渐融入我们的生活。因此，了解它们输出中的偏见是很重要的，以避免持续流传有害的刻板印象。这种挑战需要开发新的基准和方法来量化情感和语义偏差，同时铭记LLMs是反映社会中普遍存在的观点和倾向的心理社会镜子。其中一种有害的负面影响是青少年全球普遍存在的数学和STEM领域的焦虑现象。在本文中，我们运用网络科学和认知心理学的方法，使用行为forma mentis网络(BFMNs)，研究了先进的LLMs，即GPT-3、Chat-GPT和GPT-4，提供的数学和STEM领域的知识，以及这些LLMs如何将数学和STEM领域与其他概念联系起来。我们使用与数学和STEM相关的提示探测神经语言模型的吞吐量获取的数据，发现GPT-3、Chat-GPT和GPT-4都倾向于将数学和STEM领域视为困难、紧张和引起焦虑，反映了高中学生数学焦虑现象。我们的结果强调了考虑LLMs中存在的偏见的重要性，并突显了利用网络科学和认知心理学开发量化这些偏见的新工具的潜力。

    Large language models are becoming increasingly integrated into our lives. Hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. This challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that LLMs act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. One such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and STEM subjects. Here, we investigate perceptions of math and STEM fields provided by cutting-edge language models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach from network science and cognitive psychology. Specifically, we use behavioral forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM disciplines in relation to other concepts. We use data obtained by probing the thr
    
[^62]: 化学数据库和摘要练习的自动生成反馈

    Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])

    [http://arxiv.org/abs/2305.18319](http://arxiv.org/abs/2305.18319)

    本研究利用BERT模型，对化学数据库中摘要练习的答案结构进行反馈，该模型在学生提交的句子中将其归为三类，即背景、技术和观察，提供了一种方法对学生作业进行自动化反馈。

    

    及时的反馈对于教学和学习来说非常重要。本文描述了如何利用现成的神经网络变换器（机器学习）模型（BERT）来对摘要练习的答案结构进行反馈。在这项任务中，要求学生们从出版数据库中找到一篇文章并对其内容进行总结。数据集包括207个提交品，总共摘要了21篇来自主要文献的文章。该模型使用一个现有的数据集（约15,000个样本）进行了预训练，然后在80%的已提交数据集上进行了微调，这一步骤被认为是重要的。学生提交的句子被归为三类——背景、技术和观察——这使得可以将每个提交品的结构进行比较。通过比较学生的摘要结构以及来自PubMed数据库的大量摘要，可以发现...

    Timely feedback is an important part of teaching and learning. Here we describe how a readily available neural network transformer (machine-learning) model (BERT) can be used to give feedback on the structure of the response to an abstracting exercise where students are asked to summarise the contents of a published article after finding it from a publication database. The dataset contained 207 submissions from two consecutive years of the course, summarising a total of 21 different papers from the primary literature. The model was pre-trained using an available dataset (approx. 15,000 samples) and then fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be important. The sentences in the student submissions are characterised into three classes - background, technique and observation - which allows a comparison of how each submission is structured. Comparing the structure of the students' abstract a large collection of those from the PubMed database shows that stud
    
[^63]: 文本分析在公共服务协同创作中的应用：文献综述和研究框架

    Application of Text Analytics in Public Service Co-Creation: Literature Review and Research Framework. (arXiv:2305.18316v1 [cs.CY])

    [http://arxiv.org/abs/2305.18316](http://arxiv.org/abs/2305.18316)

    本文系统综述了在公共服务领域中应用文本分析技能以支持公共服务协同创作的研究，发掘了TA技术和公共服务的对公共价值的影响，以及在公共服务领域中TA技术的应用前景。

    

    公共部门面临着多个挑战，例如来自内部外部的需求变化，公民对公共部门组织的不满和挫败感，这些问题需要得到解决。公共服务协同创作是传统自上而下的公共服务开发的一种替代方案，它促进了利益相关者之间的合作，旨在创建更好的公共服务并实现公共价值。同时，文本数据的可用性推动了数据分析的发展。尽管协同创作和TA在私营部门中都得到了应用，但本文研究了现有的关于应用文本分析技术支持公共服务协同创作的研究，并对975篇文章进行了系统综述，分析了TA技术和公共服务对公共价值的影响。

    The public sector faces several challenges, such as a number of external and internal demands for change, citizens' dissatisfaction and frustration with public sector organizations, that need to be addressed. An alternative to the traditional top-down development of public services is co-creation of public services. Co-creation promotes collaboration between stakeholders with the aim to create better public services and achieve public values. At the same time, data analytics has been fuelled by the availability of immense amounts of textual data. Whilst both co-creation and TA have been used in the private sector, we study existing works on the application of Text Analytics (TA) techniques on text data to support public service co-creation. We systematically review 75 of the 979 papers that focus directly or indirectly on the application of TA in the context of public service development. In our review, we analyze the TA techniques, the public service they support, public value outcome
    
[^64]: CDJUR-BR -- 带有精细命名实体的巴西司法文件黄金收藏

    CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])

    [http://arxiv.org/abs/2305.18315](http://arxiv.org/abs/2305.18315)

    CDJUR-BR是一份稳健的黄金收藏，包含巴西司法文件中的精细命名实体，该收藏涵盖各种法律程序文件，并有助于解决目前命名实体识别（NER）无法轻而易举地识别法律实践文本中实体的问题。

    

    对于大多数法律人工智能（Legal AI）应用程序而言，命名实体识别（NER）是一项基本任务。然而，法律实践中产生的文本涉及到的实体并非当前可用的NER轻而易举地识别。缺乏法规、判例、证据、惩罚、法律程序中人们的角色（法官、律师、受害者、被告、证人）、位置类型（犯罪地点、被告地址）等的分类。因此，仍需要一个用法律领域的精细实体进行注释的稳健的黄金收藏，涵盖法律程序的各种文件，例如请愿书、调查、投诉、决定和判决。在本文中，我们描述了巴西司法黄金收藏（CDJUR-BR）的开发，该收藏包含一组由法律文献专家注释的精细命名实体。创建CDJUR-BR遵循了自己的

    A basic task for most Legal Artificial Intelligence (Legal AI) applications is Named Entity Recognition (NER). However, texts produced in the context of legal practice make references to entities that are not trivially recognized by the currently available NERs. There is a lack of categorization of legislation, jurisprudence, evidence, penalties, the roles of people in a legal process (judge, lawyer, victim, defendant, witness), types of locations (crime location, defendant's address), etc. In this sense, there is still a need for a robust golden collection, annotated with fine-grained entities of the legal domain, and which covers various documents of a legal process, such as petitions, inquiries, complaints, decisions and sentences. In this article, we describe the development of the Golden Collection of the Brazilian Judiciary (CDJUR-BR) contemplating a set of fine-grained named entities that have been annotated by experts in legal documents. The creation of CDJUR-BR followed its ow
    
[^65]: 计算机自适应测试中平衡测试准确性与安全性

    Balancing Test Accuracy and Security in Computerized Adaptive Testing. (arXiv:2305.18312v1 [cs.CY])

    [http://arxiv.org/abs/2305.18312](http://arxiv.org/abs/2305.18312)

    本文介绍了一种基于双层优化的计算机自适应测试(CAT)框架的约束版本C-BOBCAT，通过权衡测试准确性和问题暴露率及测试重叠率，解决了BOBCAT存在的高问题暴露率和测试重叠率的问题。

    

    计算机自适应测试(CAT)是一种可以准确测量学生知识水平且缩短测试时间的个性化测试形式。基于双层优化的CAT(BOBCAT)是一个最近的框架，它学习了一种数据驱动的问题选择算法，有效地缩短了测试时间并提高了测试准确性。然而，它存在高问题暴露率和测试重叠率的问题，这可能影响测试安全性。本文介绍了BOBCAT的一种约束版本，通过更改其优化设置使我们能够权衡测试准确性和问题暴露率及测试重叠率。我们通过在两个真实的成人测试数据集上进行大量实验，证明了这种方法的有效性。

    Computerized adaptive testing (CAT) is a form of personalized testing that accurately measures students' knowledge levels while reducing test length. Bilevel optimization-based CAT (BOBCAT) is a recent framework that learns a data-driven question selection algorithm to effectively reduce test length and improve test accuracy. However, it suffers from high question exposure and test overlap rates, which potentially affects test security. This paper introduces a constrained version of BOBCAT to address these problems by changing its optimization setup and enabling us to trade off test accuracy for question exposure and test overlap rates. We show that C-BOBCAT is effective through extensive experiments on two real-world adult testing datasets.
    
[^66]: 可信AI认证标签：基于经验杂交方法研究的深刻洞见

    Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study. (arXiv:2305.18307v1 [cs.CY])

    [http://arxiv.org/abs/2305.18307](http://arxiv.org/abs/2305.18307)

    本文研究了认证标签在传达可信AI上的作用。调查结果表明，认证标签可以显著增加终端用户在低风险和高风险场景下使用AI的信任和使用意愿，尤其是在高风险场景中，其效果更为明显。

    

    审计在可信AI的发展中发挥着关键作用。然而，当前研究主要集中在创建可审计的AI文件方面，这些文件是针对监管机构和专家而不是受AI决策影响的终端用户而设计的。如何向公众传达一个经过审计并被视为可信的AI的信息仍然是一个开放性问题。本研究实证调查了认证标签作为一种有前途的解决方案。通过访谈（N = 12）和普查代表性调查（N = 302），我们调查了终端用户对认证标签的态度以及在低风险和高风险AI场景中有效传达可信性的能力。根据调查结果，我们证明标签可以显著增加终端用户在低风险和高风险场景下使用AI的信任和使用意愿。然而，在高风险场景中，终端用户对认证标签的偏好及其对信任和使用AI的影响更为明显。

    Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users' attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users' trust and willingness to use AI in both lowand high-stakes scenarios. However, end-users' preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake sce
    
[^67]: 多视角交互式协同过滤

    Multi-View Interactive Collaborative Filtering. (arXiv:2305.18306v1 [cs.IR])

    [http://arxiv.org/abs/2305.18306](http://arxiv.org/abs/2305.18306)

    提出了基于多视角交互主题回归算法（MV-ICTR）的推荐系统，在不同视角下同时纳入评分和上下文信息来建模物品特定功能的相关性和用户的个人偏好，采用多臂老虎机策略进行持续的在线个性化，显著提高了数据集上性能。

    

    在许多场景下，推荐系统中的用户交互数据（如点击或评分）往往很少，物品的换手率（例如新文章、招聘信息）很高。因此，除了用户-物品评分外，集成上下文“边”信息是非常可取的。虽然存在可以同时处理评分和上下文数据的算法，但这些算法通常仅能进行样本内推荐，受到维度诅咒的限制，并不采用多臂老虎机（MAB）策略进行长期累积收益优化。我们提出了多视角交互主题回归（MV-ICTR）算法，这是一种新颖的部分在线潜在因子推荐算法，同时纳入评分和上下文信息来建模物品特定功能的相关性和用户的个人偏好，采用多臂老虎机策略进行持续在线个性化。该算法在数据集上的性能显著提高。

    In many scenarios, recommender system user interaction data such as clicks or ratings is sparse, and item turnover rates (e.g., new articles, job postings) high. Given this, the integration of contextual "side" information in addition to user-item ratings is highly desirable. Whilst there are algorithms that can handle both rating and contextual data simultaneously, these algorithms are typically limited to making only in-sample recommendations, suffer from the curse of dimensionality, and do not incorporate multi-armed bandit (MAB) policies for long-term cumulative reward optimization. We propose multi-view interactive topic regression (MV-ICTR) a novel partially online latent factor recommender algorithm that incorporates both rating and contextual information to model item-specific feature dependencies and users' personal preferences simultaneously, with multi-armed bandit policies for continued online personalization. The result is significantly increased performance on datasets wi
    
[^68]: 目前为止我们所知道的：人工智能在非洲医疗保健领域中的应用

    What We Know So Far: Artificial Intelligence in African Healthcare. (arXiv:2305.18302v1 [cs.CY])

    [http://arxiv.org/abs/2305.18302](http://arxiv.org/abs/2305.18302)

    本文综述了人工智能算法在非洲医疗保健领域中的应用情况和如何利用人工智能在低资源环境下改善非洲医疗保健的可访问性，并讨论了采用人工智能面临的一些重要挑战和机遇。需要政府、私营部门、医疗保健提供者和国际组织协调一致地努力，创建可持续的人工智能解决方案，满足非洲医疗保健系统独特的需求。

    

    非洲的医疗保健问题受到许多因素的影响，包括贫困、基础设施缺乏和资金不足等。然而，应用于医疗保健领域的人工智能（AI）具有潜力通过提高诊断的准确性和效率、使疾病更早地被发现、支持个性化药物的发布来改变非洲的医疗保健状况。本文综述了目前人工智能算法在改善诊断、治疗和疾病监测方面的应用情况，以及如何利用人工智能在低资源环境下改善非洲医疗保健的可访问性，并讨论了采用人工智能面临的一些重要挑战和机遇。因此，需要政府、私营部门、医疗保健提供者和国际组织协调一致地努力，创建可持续的人工智能解决方案，满足非洲医疗保健系统独特的需求。

    Healthcare in Africa is a complex issue influenced by many factors including poverty, lack of infrastructure, and inadequate funding. However, Artificial intelligence (AI) applied to healthcare, has the potential to transform healthcare in Africa by improving the accuracy and efficiency of diagnosis, enabling earlier detection of diseases, and supporting the delivery of personalized medicine. This paper reviews the current state of how AI Algorithms can be used to improve diagnostics, treatment, and disease monitoring, as well as how AI can be used to improve access to healthcare in Africa as a low-resource setting and discusses some of the critical challenges and opportunities for its adoption. As such, there is a need for a well-coordinated effort by the governments, private sector, healthcare providers, and international organizations to create sustainable AI solutions that meet the unique needs of the African healthcare system.
    
[^69]: 鲁棒半监督学习的分布外语义修剪

    Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning. (arXiv:2305.18158v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18158](http://arxiv.org/abs/2305.18158)

    本文提出了一种命名为OOD语义修剪（OSP）的框架，该框架旨在从分布内（ID）特征中修剪掉分布外（OOD）的语义信息，这有助于提高鲁棒半监督学习的性能。

    

    最近鲁棒半监督学习（SSL）中的进展通常在样本级别过滤分布外（OOD）信息。我们认为鲁棒SSL的一个被忽视的问题是其在语义级别上的损坏信息，实际上限制了该领域的发展。在本文中，我们采取了初步措施，探索并提出了一种统一的框架，称为OOD语义修剪（OSP），旨在从分布内（ID）特征中修剪OOD语义。具体而言，(i)我们提出了一个别名OOD匹配模块，将每个ID样本与一个OOD样本进行语义重叠匹配。(ii)我们设计了一个软正交正则化，它首先通过抑制与配对OOD样本共线的语义成分来转换每个ID特征。然后，它强制在软正交分解之前和之后的预测结果一致。我们的方法在具有挑战性的基准测试中表现出强大的性能，能够进行OOD检测和ID分类。

    Recent advances in robust semi-supervised learning (SSL) typically filter out-of-distribution (OOD) information at the sample level. We argue that an overlooked problem of robust SSL is its corrupted information on semantic level, practically limiting the development of the field. In this paper, we take an initial step to explore and propose a unified framework termed OOD Semantic Pruning (OSP), which aims at pruning OOD semantics out from in-distribution (ID) features. Specifically, (i) we propose an aliasing OOD matching module to pair each ID sample with an OOD sample with semantic overlap. (ii) We design a soft orthogonality regularization, which first transforms each ID feature by suppressing its semantic component that is collinear with paired OOD sample. It then forces the predictions before and after soft orthogonality decomposition to be consistent. Being practically simple, our method shows a strong performance in OOD detection and ID classification on challenging benchmarks.
    
[^70]: InDL: 基于视错觉的图中逻辑解释新数据集和基准的研究

    InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion. (arXiv:2305.17716v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.17716](http://arxiv.org/abs/2305.17716)

    本文提出了一个基于视错觉的独特数据集InDL，用于测试和评估深度学习模型的图中逻辑解释能力。利用几何光学视错觉，建立可比性框架用于阐明模型可能存在的缺陷和提供改进模型的洞察力。

    

    本文提出了一种新的方法来评估深度学习模型在图中逻辑解释方面的能力。通过利用有趣领域的视错觉，我们建立了一个独特的数据集InDL，旨在严格测试和基准这些模型。我们利用六个经典的几何视觉错觉，创建了一个人类和机器视觉感知的可比性框架。这种方法提供了一个可量化的衡量模型的方法，阐明了可能存在的缺陷，并提供了改进模型的行动洞察力。

    This paper introduces a novel approach to evaluating deep learning models' capacity for in-diagram logic interpretation. Leveraging the intriguing realm of visual illusions, we establish a unique dataset, InDL, designed to rigorously test and benchmark these models. Deep learning has witnessed remarkable progress in domains such as computer vision and natural language processing. However, models often stumble in tasks requiring logical reasoning due to their inherent 'black box' characteristics, which obscure the decision-making process. Our work presents a new lens to understand these models better by focusing on their handling of visual illusions -- a complex interplay of perception and logic. We utilize six classic geometric optical illusions to create a comparative framework between human and machine visual perception. This methodology offers a quantifiable measure to rank models, elucidating potential weaknesses and providing actionable insights for model improvements. Our experim
    
[^71]: 利用适当的评分规则激励诚实的表现性预测

    Incentivizing honest performative predictions with proper scoring rules. (arXiv:2305.17601v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.17601](http://arxiv.org/abs/2305.17601)

    本论文研究了当预测可以影响结果时如何利用适当的评分规则激励专家作出准确反映信念的预测，并给出了针对二元预测的评分规则的界限。

    

    适当的评分规则可以激励专家准确报告信念，但我们放宽了此假设，研究了当预测可以影响结果时（例如在股市上公开预测时），预测对专家信念的影响。我们发现在此设定下，最大化预期得分的报告通常不反映专家的信念，并给出了这些报告的不准确度界限。对于二元预测，如果专家预测对结果的影响有界，可以定义评分规则，使得最优报告与固定点非常接近。但对于超过两个结果的预测，这是不可能的。我们还在玩具模型中进行了数值模拟，显示了我们的界限在某些情况下是紧密的。

    Proper scoring rules incentivize experts to accurately report beliefs, assuming predictions cannot influence outcomes. We relax this assumption and investigate incentives when predictions are performative, i.e., when they can influence the outcome of the prediction, such as when making public predictions about the stock market. We say a prediction is a fixed point if it accurately reflects the expert's beliefs after that prediction has been made. We show that in this setting, reports maximizing expected score generally do not reflect an expert's beliefs, and we give bounds on the inaccuracy of such reports. We show that, for binary predictions, if the influence of the expert's prediction on outcomes is bounded, it is possible to define scoring rules under which optimal reports are arbitrarily close to fixed points. However, this is impossible for predictions over more than two outcomes. We also perform numerical simulations in a toy setting, showing that our bounds are tight in some si
    
[^72]: 一桩天作之合：用于夸张和隐喻检测的多任务框架。

    A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection. (arXiv:2305.17480v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17480](http://arxiv.org/abs/2305.17480)

    本研究提出了一个多任务深度学习框架，同时检测夸张和隐喻。使用隐喻标签注释了两个夸张数据集，使用夸张标签注释了两个隐喻数据集，实验证明该框架检测夸张的性能比先前方法进步了12%。此外，多任务学习方法比单任务学习方法提高了多达17%。

    

    夸张和隐喻在日常交流中很常见（例如，“我陷入了麻烦之中”：麻烦怎么可能有深度？），因此它们的检测尤为重要，特别是在对话 AI 设置中。现有的自动检测夸张和隐喻的方法独立地研究了这些语言现象，但它们之间的关系从未被计算化探索过。在本文中，我们提出了一个多任务深度学习框架，同时检测夸张和隐喻。我们假设隐喻有助于夸张检测，反之亦然。为验证这一假设，我们使用隐喻标签注释了两个夸张数据集-HYPO 和 HYPO-L-。同时，我们使用夸张标签注释了两个隐喻数据集-TroFi 和 LCC。使用这些数据集的实验证明，我们的方法比先前的夸张检测方法进步了 12%。此外，我们的多任务学习方法相对于单任务学习方法提高了多达 17%。

    Hyperbole and metaphor are common in day-to-day communication (e.g., "I am in deep trouble": how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these language phenomena independently, but their relationship has hardly, if ever, been explored computationally. In this paper, we propose a multi-task deep learning framework to detect hyperbole and metaphor simultaneously. We hypothesize that metaphors help in hyperbole detection, and vice-versa. To test this hypothesis, we annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels. Simultaneously, we annotate two metaphor datasets- TroFi and LCC- with hyperbole labels. Experiments using these datasets give an improvement of the state of the art of hyperbole detection by 12%. Additionally, our multi-task learning (MTL) approach shows an improvement of up to 17% over single-task learning (S
    
[^73]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^74]: mldr.resampling: 多标签重采样算法有效的参考实现

    mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms. (arXiv:2305.17152v1 [cs.LG])

    [http://arxiv.org/abs/2305.17152](http://arxiv.org/abs/2305.17152)

    mldr.resampling是一个软件包，提供11种多标签重采样方法的参考实现，旨在应对多标签学习中的不平衡情况，并具有高效性。

    

    重采样算法是应对多标签学习中不平衡情况的有用方法。这些方法必须处理多标签数据中的奇异性，例如同一实例中频繁和不频繁标签的出现。这篇原创软件发表介绍了 mldr.resampling，这是一个软件包，提供了11种多标签重采样方法的参考实现，强调效率，因为这些算法通常耗时。

    Resampling algorithms are a useful approach to deal with imbalanced learning in multilabel scenarios. These methods have to deal with singularities in the multilabel data, such as the occurrence of frequent and infrequent labels in the same instance. Implementations of these methods are sometimes limited to the pseudocode provided by their authors in a paper. This Original Software Publication presents mldr.resampling, a software package that provides reference implementations for eleven multilabel resampling methods, with an emphasis on efficiency since these algorithms are usually time-consuming.
    
[^75]: 使用检索增强语言模型提高GPT-3/4在生物医学数据上的准确性

    Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model. (arXiv:2305.17116v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17116](http://arxiv.org/abs/2305.17116)

    本研究使用Retrieval Augmentation（RetA）方法，对比了OpenAI的GPT-3、GPT-4、Bing的Prometheus以及定制的RetA模型在回答19个弥漫大B细胞淋巴瘤（DLBCL）疾病相关问题方面的表现，结果显示RetA模型在准确性和相关性方面表现最佳。

    

    大型语言模型（LLM）在自然语言处理（NLP）方面取得了重大进展。广泛的语料库捕获了多样的模式，但也可能引入无关的信息，而专注的语料库通过减少误导性信息来提高可靠性。针对特定领域中的Retrieval Augmentation（RetA）方法是使用专注语料库训练LLM的替代方法。本文针对弥漫大B细胞淋巴瘤（DLBCL）疾病提出了19个问题并比较了OpenAI的GPT-3、GPT-4、Bing的Prometheus以及定制的RetA模型在回答这些问题方面的表现。 8名独立的评审根据准确性、相关性和可读性（评分1-3）对回答进行评估。结果显示RetA模型在准确性（19项中12项获得3分，总计47分）和相关性（19项中13项，50分）方面表现最佳，其次是GPT-4（19项中8项，43分；11项中49分）。GPT-4获得了最高的可读性评分（19项中17项，55分），其次是GPT-3（19项中15项，53分）和RetA模型（19项中11项，47分）。Prometheus在准确性方面表现不佳。

    Large language models (LLMs) have made significant advancements in natural language processing (NLP). Broad corpora capture diverse patterns but can introduce irrelevance, while focused corpora enhance reliability by reducing misleading information. Training LLMs on focused corpora poses computational challenges. An alternative approach is to use a retrieval-augmentation (RetA) method tested in a specific domain.  To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a custom RetA model were compared using 19 questions on diffuse large B-cell lymphoma (DLBCL) disease. Eight independent reviewers assessed responses based on accuracy, relevance, and readability (rated 1-3).  The RetA model performed best in accuracy (12/19 3-point scores, total=47) and relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4 received the highest readability scores (17/19, 55), followed by GPT-3 (15/19, 53) and the RetA model (11/19, 47). Prometheus underperformed in accu
    
[^76]: 在基于逻辑的实体消解中结合全局和局部合并

    Combining Global and Local Merges in Logic-based Entity Resolution. (arXiv:2305.16926v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2305.16926](http://arxiv.org/abs/2305.16926)

    在Lace框架中，使用逻辑规则和约束识别表示同一实体的实体引用对，该识别结合了全局和局部合并的概念。

    

    在最近提出的集体实体消解框架Lace中，使用逻辑规则和约束来识别表示同一实体的实体引用对（例如作者或论文ID）。这种识别是全局的：这些实体引用的所有出现（可能跨越多个数据库元组）都被视为相等并且可以合并。相比之下，当识别数据值对时，本地合并形式通常更自然，例如，一些出现“J.Smith”的实体可以与“Joe Smith”等同，而其他实体应合并到“Jane Smith”。这激发了我们扩展Lace值的本地合并，并探索所得到的形式化的计算特性。

    In the recently proposed Lace framework for collective entity resolution, logical rules and constraints are used to identify pairs of entity references (e.g. author or paper ids) that denote the same entity. This identification is global: all occurrences of those entity references (possibly across multiple database tuples) are deemed equal and can be merged. By contrast, a local form of merge is often more natural when identifying pairs of data values, e.g. some occurrences of 'J. Smith' may be equated with 'Joe Smith', while others should merge with 'Jane Smith'. This motivates us to extend Lace with local merges of values and explore the computational properties of the resulting formalism.
    
[^77]: GenQ：自动化问答生成器以帮助照顾者与孩子共读故事

    GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])

    [http://arxiv.org/abs/2305.16809](http://arxiv.org/abs/2305.16809)

    本研究设计了一个智能辅导系统（GenQ），可以根据照顾者和孩子之间的对话促进孩子的阅读理解能力，并通过考虑文化背景和语境变化以提高系统效果。

    

    当照顾者询问开放式问题以激发与孩子的对话时，可以促进孩子的阅读理解能力。虽然有利用技术工具来支持这个过程，即所谓的“智能辅导系统”的空间，但目前仍不清楚现有的生成类人语言问题的智能系统是否有益。此外，用于开发这些自动生成问题系统的培训数据通常没有考虑到人口统计学，但具有不同文化背景的人可能会提出不同的问题。作为为拉丁裔儿童设计智能阅读支持应用程序的广泛项目的一部分，我们从来自不同人口统计学的拉丁裔护理人员和非护理人员以及其他人口统计学背景的护理人员和非护理人员中群集大量问题。我们研究了这个数据集中个体、文化和环境因素中介的问题提问的变化。然后我们设计了一个系统来自动产生问题。

    When caregivers ask open--ended questions to motivate dialogue with children, it facilitates the child's reading comprehension skills.Although there is scope for use of technological tools, referred here as "intelligent tutoring systems", to scaffold this process, it is currently unclear whether existing intelligent systems that generate human--language like questions is beneficial. Additionally, training data used in the development of these automated question generation systems is typically sourced without attention to demographics, but people with different cultural backgrounds may ask different questions. As a part of a broader project to design an intelligent reading support app for Latinx children, we crowdsourced questions from Latinx caregivers and noncaregivers as well as caregivers and noncaregivers from other demographics. We examine variations in question--asking within this dataset mediated by individual, cultural, and contextual factors. We then design a system that autom
    
[^78]: GPT是否会产生更不准确的翻译?

    Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])

    [http://arxiv.org/abs/2305.16806](http://arxiv.org/abs/2305.16806)

    本研究比较了GPT和NMT生成翻译的文字积极度差异，发现GPT翻译更不准确，但在MT质量评估指标上表现出相似或更好的分数。

    

    大型语言模型（LLMs），如GPT-3，已经成为通用语言模型，能够处理许多自然语言生成或理解任务。在机器翻译（MT）任务中，已有多项研究探索利用few-shot提示机制从LLMs中引出更好的翻译。然而，人们相对较少地关注这种翻译与标准神经机器翻译（NMT）模型生成翻译的质量差异。本研究从文字对齐和单调性等方面，比较了GPT和NMT生成翻译的文本文字积极度，发现GPT从英语（E-X）翻译的文本更不准确，但在MT质量评估指标上表现出相似或更好的分数。我们证明这一结果在人工评估中也得到了验证。同时，当翻译句子长度增加时，这种差别就尤为显著。

    Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
    
[^79]: 带扰动生成树的可微聚类方法

    Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])

    [http://arxiv.org/abs/2305.16358](http://arxiv.org/abs/2305.16358)

    介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。

    

    我们介绍了一种基于最小权重生成树的可微聚类方法，它是生成树的一种变体，具有多个连通分量。我们的方法依赖于线性规划解的随机扰动，以实现平滑和高效的梯度计算。这使我们能够在端到端可训练的流水线中包含聚类。我们证明了我们的方法即使在嘈杂的数据集和具有挑战性的几何环境下也能良好地工作。我们还利用这种方法制定了一个特别的损失，以有效地从部分聚类数据学习。我们在几个现实世界的数据集上展示了它在监督和半监督任务中的表现。

    We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
    
[^80]: 缩放数据受限的语言模型

    Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16264](http://arxiv.org/abs/2305.16264)

    研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。

    

    现在扩展语言模型的趋势涉及增加参数计数和训练数据集大小。推断这个趋势表明，训练数据集大小可能很快就会受到互联网上可用文本数据的限制。出于此限制的动机，我们研究在数据受限制的情况下缩放语言模型。具体而言，我们运行了大量的实验，变化数据重复程度和计算预算，范围达到了9000亿个训练令牌和9亿参数模型。我们发现，在有限的数据的情况下，使用高达4次重复数据的训练与使用唯一数据相比对损失的贡献微不足道。然而，使用更多的重复数据，添加计算的价值最终会衰减为零。我们提出并经验证了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。最后，我们尝试了缓解数据稀缺的方法。

    The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
    
[^81]: 用更少的数据进行视觉上有依据的少样本词汇习得

    Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL])

    [http://arxiv.org/abs/2305.15937](http://arxiv.org/abs/2305.15937)

    该论文提出了一种基于视觉的语音模型，可以从仅有少量的词-图像示例对中习得新的词汇及其视觉表示，并且与现有方法相比，该模型在使用更少的样本时取得了更好的性能。

    

    我们提出了一种基于视觉的语音模型，它可以从仅有少量的词-图像示例对中习得新的词汇及其视觉表示。给定一组测试图像和一个口头查询，我们要求模型指出哪个图像展示了查询词。先前的工作要么使用数字词-图像对的人造环境来简化该问题，要么使用每个类别大量的示例。我们提出了一种方法，可以在自然的词-图像对上进行，但只需更少的数据，即更少的样本。我们的方法包括使用给定的词-图像示例对从大量未标记的语音和图像中挖掘新的无监督词-图像训练对。另外，我们使用了一种单词到图像的注意力机制来确定词-图像的相似度。通过这种新模型，我们实现了比任何现有方法都更好的性能，而且只需更少的数据量。

    We propose a visually grounded speech model that acquires new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelled speech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than any existing approach.
    
[^82]: 分布式强化学习的好处：小损失边界

    The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])

    [http://arxiv.org/abs/2305.15703](http://arxiv.org/abs/2305.15703)

    通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。

    

    虽然分布式强化学习已经取得了实证成果，但其何时何地有益的问题尚未得到回答。在这项工作中，通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，我们的边界会比非分布式方法更强。作为热身，我们展示了学习成本分布会在情境展开（CB）中导致小损失后悔边界，我们发现分布式CB在三个具有挑战性的任务上比最先进的技术在实证上表现更好。对于在线RL，我们提出了一个分布式版本空间算法，该算法使用最大似然估计构建置信区间，并证明了它在表格MDP中实现了小损失后悔，同时在潜变量模型中享有小损失PAC边界。以类似的见解为基础，我们提出了一个分布式离线RL算法

    While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
    
[^83]: 基于学习的软件代码和配置自动生成

    Learning-Based Automatic Synthesis of Software Code and Configuration. (arXiv:2305.15642v1 [cs.SE])

    [http://arxiv.org/abs/2305.15642](http://arxiv.org/abs/2305.15642)

    本文介绍了一种基于学习的自动软件代码和配置生成方法，其中通过遗传算法和神经网络训练的适应度函数自动合成程序，使用协方差矩阵适应进化策略完成程序合成，并采用强化学习方法实现配置生成。

    

    软件行业的需求增加和软件工程师的稀缺性推动研究人员和从业者自动化软件生成和配置的过程。大规模的自动软件代码生成和配置是一个非常复杂和具有挑战性的任务。本研究将自动软件生成和配置拆分为两个不同的任务进行研究。在第一个任务中，我们提出了使用输入输出规范自动合成软件的方法。这个任务被进一步分为两个子任务。第一个子任务是使用遗传算法合成程序，其驱动神经网络基于程序跟踪和规范训练的适应度函数。对于第二个子任务，我们将程序合成形式化为连续优化问题，并使用协方差矩阵适应进化策略（一种最先进的连续优化方法）来合成程序。最后，对于配置生成，我们采用基于强化学习的方法来实现。

    Increasing demands in software industry and scarcity of software engineers motivates researchers and practitioners to automate the process of software generation and configuration. Large scale automatic software generation and configuration is a very complex and challenging task. In this proposal, we set out to investigate this problem by breaking down automatic software generation and configuration into two different tasks. In first task, we propose to synthesize software automatically with input output specifications. This task is further broken down into two sub-tasks. The first sub-task is about synthesizing programs with a genetic algorithm which is driven by a neural network based fitness function trained with program traces and specifications. For the second sub-task, we formulate program synthesis as a continuous optimization problem and synthesize programs with covariance matrix adaption evolutionary strategy (a state-of-the-art continuous optimization method). Finally, for th
    
[^84]: SPRING: GPT-4通过学习论文和推理在游戏中表现超过RL算法

    SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])

    [http://arxiv.org/abs/2305.15486](http://arxiv.org/abs/2305.15486)

    SPRING是一个新的方法，能够在开放世界游戏中表现出色，它通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏。。

    

    开放世界游戏由于其多任务、深度探索和目标优先级要求，对AI算法提出了重大挑战。尽管强化学习（RL）在解决游戏方面很受欢迎，但其高样本复杂性限制了它在像Crafter或Minecraft这样复杂的开放世界游戏中的有效性。我们提出了一种新颖的方法SPRING，通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏，来解决这个问题。在给定LaTeX源作为游戏语境和代理当前观察的描述的情况下，我们的SPRING框架利用具有游戏相关问题的定向无环图（DAG）作为节点和依赖关系作为边。通过按拓扑顺序遍历DAG并计算每个节点的LLM响应来确定在环境中采取的最优行动，LLM对最终节点的答案直接转化为环境行动。在我们的实验中，我们研究了

    Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
    
[^85]: 知识设计：通过知识提炼推动蛋白质设计的极限

    Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.15151](http://arxiv.org/abs/2305.15151)

    本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。

    

    最近的研究表明，在蛋白质设计中，寻找折叠为所期望结构的氨基酸序列已经取得了竞争优势。然而，大多数研究忽略了预测置信度的重要性，未能覆盖广泛的蛋白质空间，并且没有融入常见的蛋白质知识。本文提出了一种知识感知模块来提炼低质量残基，并引入了一种记忆检索机制来节省超过50%的训练时间。我们在CATH、TS50和TS500数据集上对所提出的方法进行了广泛评估，结果显示我们的知识设计方法在CATH数据集上的性能超过了先前的PiFold方法约9％。具体来说，知识设计是第一个实现了...

    Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
    
[^86]: 作为奖励的视频预测模型用于强化学习

    Video Prediction Models as Rewards for Reinforcement Learning. (arXiv:2305.14343v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14343](http://arxiv.org/abs/2305.14343)

    本文提出了VIPER算法，利用预训练的视频预测模型作为强化学习的奖励信号来学习复杂行为，从而实现在广泛任务范围内的专家级控制，同时具有泛化性。

    

    在强化学习中，制定让代理学习复杂行为的奖励信号一直是一个长期的挑战。一种有前途的方法是从广泛可用于互联网上的无标注视频中提取行为偏好。我们提出了Video Prediction Rewards (VIPER)，这种算法利用预训练的视频预测模型作为不需要行为干预的强化学习奖励信号。具体而言，我们首先在专家视频上训练一个自回归Transformer，然后将视频预测可能性用作强化学习代理的奖励信号。VIPER使得在DMC、Atari和RLBench任务等广泛的任务范围内，在没有编程任务奖励的情况下实现专家级的控制。此外，视频预测模型的泛化使得我们能够为没有专家数据可用的分布外环境导出奖励信号，从而实现桌面操纵的跨体现能力。我们认为我们的工作是具有伸缩性的奖励制定的起点。

    Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning. A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward s
    
[^87]: EntRED: 用更少的捷径进行关系抽取基准测试

    EntRED: Benchmarking Relation Extraction with Fewer Shortcuts. (arXiv:2305.13551v1 [cs.CL])

    [http://arxiv.org/abs/2305.13551](http://arxiv.org/abs/2305.13551)

    本研究提出了一个名称更为多样、没有捷径、具有挑战性的关系提取基准测试EntRed，并解决了标准基准测试数据集存在的实体注释错误、实体名称多样性较低、从实体名称到基本事实关系的捷径等问题。

    

    实体名称在关系抽取中起着有效的作用，并常常影响模型性能。因此，基准测试中测试集中的实体名称显著影响了关系提取模型的评估。本研究发现，标准的关系抽取基准测试数据集存在大量错误的实体注释，实体名称多样性较低，并且容易出现从实体名称到基本事实关系的捷径。这些问题使得标准基准测试与现实世界场景相距甚远。因此，在本研究中，我们提出了EntRED，这是一个具有较少捷径和更高实体多样性的具有挑战性的关系提取基准测试。为构建EntRED，我们提出了一种基于因果推理（CI）的端到端实体替换管道：ERIC。ERIC对实体进行类型约束替换，以减少从实体偏差到基本事实关系的捷径。ERIC在两个方面应用CI：1）针对需要实体替换的实例，2）确定候选实体。

    Entity names play an effective role in relation extraction (RE) and often influence model performance. As a result, the entity names in the benchmarks' test sets significantly influence the evaluation of RE models. In this work, we find that the standard RE benchmarks' datasets have a large portion of incorrect entity annotations, low entity name diversity, and are prone to have shortcuts from entity names to ground-truth relations. These issues make the standard benchmarks far from reflecting the real-world scenarios. Hence, in this work, we present EntRED, a challenging RE benchmark with reduced shortcuts and higher diversity of entities. To build EntRED, we propose an end-to-end entity replacement pipeline based on causal inference (CI): ERIC. ERIC performs type-constrained replacements on entities to reduce the shortcuts from entity bias to ground-truth relations. ERIC applies CI in two aspects: 1) targeting the instances that need entity replacements, and 2) determining the candid
    
[^88]: 多样化深度集成：一种使用显著性图的方法以增强OOD检测、校准和准确性

    Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])

    [http://arxiv.org/abs/2305.11616](http://arxiv.org/abs/2305.11616)

    这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。

    

    深度集成在分类和 OOD 检测方面取得了最先进的成果；然而，由于集成中学习的模式的同质性，它们的效果仍然有限。为了克服这一挑战，本研究引入了一种促进集成成员之间多样性的新方法，该方法利用显著性图。通过整合显著性图多样化，我们的方法在多个分类和OOD检测任务中优于传统的集成技术，同时也提高了校准性。在已建立的OpenOOD基准测试上的实验凸显了我们的方法在实际应用中的潜力。

    Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
    
[^89]: 提升大规模语言模型在工业领域特定问答中的表现

    Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])

    [http://arxiv.org/abs/2305.11541](http://arxiv.org/abs/2305.11541)

    本文提供了一个行业云特定QA数据集 MSQA，该数据集可用于评估旨在提高大规模语言模型特定领域能力的方法。本文还提出了一种新的模型交互范式，可以使大规模语言模型在其不擅长的特定任务上取得更好的性能。

    

    大规模语言模型（LLM）在开放领域任务中获得了广泛应用和卓越的成果，但其在真实的工业特定场景中的表现通常很平庸，因为它缺乏特定领域的知识。这个问题引起了广泛关注，但相关基准测试很少。本文提供了一个名为MSQA的基准问答（QA）数据集，该数据集涉及Microsoft产品和客户遇到的IT技术问题。这个数据集包含了行业云的特定QA知识，这对于一般的LLM来说是不可用的，因此非常适合评估旨在提高LLM特定领域能力的方法。此外，我们提出了一种新的模型交互范式，可以使LLM在其不擅长的特定任务上取得更好的性能。广泛的实验表明，遵循我们的模型融合框架的方法比使用检索方法的常用LLM表现更好。

    Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods.
    
[^90]: 自动自我生成的零样本编码从语义级别到代码级别的 SelfzCoT，更好地利用LLMs

    SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs. (arXiv:2305.11461v1 [cs.AI])

    [http://arxiv.org/abs/2305.11461](http://arxiv.org/abs/2305.11461)

    本文提出了 SelfzCoT 自动自我生成的零样本编码，通过使用LLMs和代码级别的自我提示，在六个零样本算术推理任务中实现了巨大的准确度提升。同时，修改的零样本编码 MzCoT 在推理任务中也取得了显著的表现。

    

    本文通过 SelfzCoT 自动自我生成的零样本编码，研究了如何更好地利用LLMs。具体地，我们将 SelfzCoT 应用于零样本算术推理任务，其准确性从GSM8K的40.50%提高至82.34%，MultiArith从79.3%提高至94.7%，ADDSUB从74.70%提高至94.10%，SingleEq从78.70%提高至91.30%，AQUA从31.90%提高至82.33%，SVAMP从63.70%提高至79.70%。总的来说，使用前两个持久路径激活到LLM，特别是代码级别的自我提示，使 SelfzCoT 在所有六个零样本算术推理任务上实现了巨大的改进。此外，我们修改的零样本编码 MzCoT 在推理任务中也取得了显著的表现。在GSM8K中，MzCoT的准确性从40.50%提高至76.32%，MultiArith从79.3%提高至96.97%，ADDSUB从74.70%提高至92.39%，SingleEq从78.70%提高至94.60%，AQUA从31.90%提高至79.90%，SVAMP从63.70%提高至81.50%。

    This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the
    
[^91]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^92]: 带有注意力模型的视觉问答算法分析

    Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])

    [http://arxiv.org/abs/2305.09782](http://arxiv.org/abs/2305.09782)

    本文批评性地检查和审查了使用共同注意力方法的VQA算法的方法，重点关注文本语义生成、对象识别和答案分类技术。

    

    视觉问答（VQA）使用图像处理算法处理图像，使用自然语言处理方法理解并回答问题。 VQA 对视觉受损者有帮助，可用于安全监控系统和从网络中学习的在线聊天机器人。 它使用自然语言处理方法学习问题的语义并提取文本特征。 计算机视觉技术用于以一种能够识别所问问题涉及的物体的方式生成图像表示。 注意力模型试图模仿人类根据语境关注图像不同区域的行为。 本文批评性地检查和审查了使用共同注意力方法的 VQA 算法的方法，例如生成文本语义，识别对象和答案分类技术。

    Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
    
[^93]: GNNs: 可以更强、更新、更快

    GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])

    [http://arxiv.org/abs/2305.05368](http://arxiv.org/abs/2305.05368)

    本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。

    

    图神经网络（GNNs）是一类可以从图结构数据中学习并通过集成邻居节点的表示学习来表现出色的神经网络。然而，GNN的性能会随着层数增加而逐渐降低。本文引入了一个新的概念——k跳子图聚合，提出了一种新的理解GNN表现能力的视角，揭示了传统深层GNN表现逐渐退化的潜在原因，包括聚合子图的重叠以及基于残差的GNN实际上利用了1到k跳子图聚合结果来提高有效性。此外，我们提出了一种新的采样节点级残差模块SDF，通过理论推导证明其比之前的残差方法具有更优的表现能力，可以利用1到k跳跃子图的信息。

    Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
    
[^94]: 基于Transformer的零样本和少样本生物医学命名实体识别方法

    A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])

    [http://arxiv.org/abs/2305.04928](http://arxiv.org/abs/2305.04928)

    本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。

    

    在生物医学领域中，有监督的命名实体识别（NER）依赖于具有给定命名实体的大量注释文本，其创建可能耗时且昂贵。此外，提取新实体通常需要进行额外的注释任务和重新训练模型。为解决这些挑战，本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。该方法基于将多类标记分类任务转换为二元标记分类（标记包含搜索的实体或不包含搜索的实体），并在更多的数据集和生物医学实体上进行预训练，从而可学习到给定和潜在类别之间的语义关系。在9种不同的生物医学实体上，我们在零样本NER、一次样本NER、10次样本NER和100次样本NER上实现了平均F1得分分别为35.44％、50.10％、69.94％和79.51％。

    Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
    
[^95]: 用于加权因果 DAG 的新度量和搜索算法

    New metrics and search algorithms for weighted causal DAGs. (arXiv:2305.04445v1 [cs.LG])

    [http://arxiv.org/abs/2305.04445](http://arxiv.org/abs/2305.04445)

    本研究提供了针对加权因果 DAGs的新度量和搜索算法，发现了用于自适应干预的因果图，提供了一个新的基准来捕捉搜索算法的最坏干预成本，并提供自适应搜索算法实现对数逼近。

    

    从数据中恢复因果关系是一个重要的问题。在使用观测数据时，只能恢复到一个马尔科夫等价类的因果图，并且需要额外的假设或干预数据来完成恢复。本文在一些标准假设下，通过节点相关干预成本的自适应干预，研究因果图发现。对于这种情况，我们证明没有算法能够比验证次数的顺序更好地实现渐近保证，验证次数是自适应搜索算法的一个成熟基准。在这个负面结果的基础上，我们定义了一个捕捉任何搜索算法最坏干预成本的新基准。此外，针对这个新基准，我们提供了自适应搜索算法，在各种设置下都能实现对数逼近：原子、有界大小的干预和广义成本。

    Recovering causal relationships from data is an important problem. Using observational data, one can typically only recover causal graphs up to a Markov equivalence class and additional assumptions or interventional data are needed for complete recovery. In this work, under some standard assumptions, we study causal graph discovery via adaptive interventions with node-dependent interventional costs. For this setting, we show that no algorithm can achieve an approximation guarantee that is asymptotically better than linear in the number of vertices with respect to the verification number; a well-established benchmark for adaptive search algorithms. Motivated by this negative result, we define a new benchmark that captures the worst-case interventional cost for any search algorithm. Furthermore, with respect to this new benchmark, we provide adaptive search algorithms that achieve logarithmic approximations under various settings: atomic, bounded size interventions and generalized cost o
    
[^96]: 离散扩散建模下的高效和度数引导图生成

    Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])

    [http://arxiv.org/abs/2305.04111](http://arxiv.org/abs/2305.04111)

    本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。

    

    基于扩散的生成图模型已被证明在生成高质量小图方面非常有效。然而，它们需要更可扩展性，以生成包含数千个节点的大图并满足图统计。本文提出了EDGE，一种新的基于扩散的生成图模型，用于生成大型图的生成任务。为了提高计算效率，我们通过在每个时间步长随机删除边来鼓励图的稀疏性，并最终获得一张空白图。EDGE仅在每个去噪步骤中关注图中一部分节点。它比以前的基于扩散的模型更少地进行边预测。此外，EDGE明确地允许对图的节点度数进行建模，进一步提高了模型的性能。实证研究表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。它还在生成质量方面优于基准模型。

    Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
    
[^97]: 面向跨数据集的弱监督仇恨言论分类

    Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])

    [http://arxiv.org/abs/2305.02637](http://arxiv.org/abs/2305.02637)

    该论文提出使用极度弱的监督方法，只依赖于类别名称而不是注释数据中的类别示例，解决当前仇恨言论识别的研究存在的数据创建策略不系统和不同注释方案问题，并展示了有效性。

    

    如多位学者指出的那样，当前针对仇恨言论（HS）识别的研究特点是不系统的数据创建策略和不同的注释方案。因此，监督学习模型往往对它们未被训练的数据集进行泛化性能差，并且不同HS分类法标记的数据集所训练的模型的性能无法比较。为了解决这个问题，我们提出了一种极度弱的监督方法，只依赖于类别名称而不是注释数据中的类别示例。我们展示了一种最先进的弱监督文本分类模型在各种数据集内和跨数据集的情况下的有效性。此外，我们对HS分类模型通用性较差的原因进行了深入的定量和定性分析。

    As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
    
[^98]: 大纲先行，细节后至：基于语法引导的粗-细代码生成

    Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2305.00909](http://arxiv.org/abs/2305.00909)

    提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。

    

    对于一个复杂算法的实现，人类程序员的做法通常是先概述一下控制流程，然后迭代进行丰富，最终生成一些精心加工的语法结构和层次变量。然而，现有的大型语言模型一次性生成代码，没有中间环节，以反映"大纲先行，细节后至"的结构化思维过程。受到思维链提示的最新成功启发，我们提出了ChainCoder，这是一种程序综合语言模型，它逐步生成Python代码，即从粗到细进行多次迭代。我们首先通过抽象语法树解析将源代码分解为布局框架组件和附件组件，以构建层次表示。然后我们将预测目标重新启动，形成多次通过目标，每次生成一个子序列，这些子序列在层次结构中串联起来。最后，我们利用量身定制的Transformer体系结构来实现模型的优化。

    For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
    
[^99]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^100]: 模拟形式转换器用于少样本3D解析

    Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])

    [http://arxiv.org/abs/2304.14382](http://arxiv.org/abs/2304.14382)

    "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。

    

    我们提出了一种称为“模拟网络”的模型，它在一组有标记的结构化3D场景中显式地编码领域知识（作为模型参数的一部分），并通过类比推理对3D物体场景进行分割：我们的模型首先从内存中检索相关场景及其相应的部分结构，然后通过端到端可学习的调制机制为输入场景预测类似的部分结构，而不是直接将场景映射到部分分割。通过对多个检索的记忆进行条件控制，预测混合匹配检索记忆的结构合成。在“模拟网络”中，一发、少发或多发学习被一致地处理，通过对适当的记忆集进行条件谓词，无论是从单个、少数还是许多存储实例中继承相似的解析。我们展示了“模拟网络”在许多样本情况下与最新的3D分割变压器模型相竞争。

    We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
    
[^101]: 对比学习是相似性图谱上的谱聚类

    Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15103](http://arxiv.org/abs/2303.15103)

    本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。

    

    对比学习是一种强大的自监督学习方法，但我们对其运作原理和原因的理论理解有限。本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性。利用这种等价性作为基石，我们将分析扩展到CLIP模型，并严格描述多模态对象如何被嵌入到一起。在理论洞见的推动下，我们引入了核混合损失，结合新颖的核函数，在多个视觉数据集上优于标准高斯核。

    Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
    
[^102]: RN-Net: 基于储备节点的神经元视觉感知网络

    RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network. (arXiv:2303.10770v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10770](http://arxiv.org/abs/2303.10770)

    本论文提出一种基于储备节点的神经元视觉感知网络（RN-Net）。RN-Net可以以低成本有效地处理异步的时间特征，并在DVS128手势上实现了迄今最高的准确率99.2％，在更小的网络尺寸下实现了DVS Lip数据集的67.5％准确率。

    

    事件相机受生物视觉系统稀疏和异步脉冲表示的启发。然而，处理事件数据要么需要使用昂贵的特征描述符将脉冲转换成帧，要么使用难以训练的脉冲神经网络。在本文中，我们提出了一种基于简单卷积层和动态时间编码储备的神经网络架构，具有低硬件和训练成本。使用储备节点的神经元视觉感知网络（RN-Net）使网络能够有效处理异步的时间特征，并实现了DVS128手势的迄今最高准确度99.2％，同时在更小的网络尺寸下实现了DVS Lip数据集的67.5％准确度。通过利用记忆电阻器的内部动态，可以以非常低的硬件成本实现异步时间特征编码，而不需要预处理或专用的存储器和算术单元。

    Event-based cameras are inspired by the sparse and asynchronous spike representation of the biological visual system. However, processing the even data requires either using expensive feature descriptors to transform spikes into frames, or using spiking neural networks that are difficult to train. In this work, we propose a neural network architecture based on simple convolution layers integrated with dynamic temporal encoding reservoirs with low hardware and training costs. The Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net) allows the network to efficiently process asynchronous temporal features, and achieves the highest accuracy of 99.2% for DVS128 Gesture reported to date, and one of the highest accuracy of 67.5% for DVS Lip dataset at a much smaller network size. By leveraging the internal dynamics of memristors, asynchronous temporal feature encoding can be implemented at very low hardware cost without preprocessing or dedicated memory and arithmetic units. T
    
[^103]: CB2：合作自然语言交互研究平台

    CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])

    [http://arxiv.org/abs/2303.08127](http://arxiv.org/abs/2303.08127)

    CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。

    

    CB2 是一个多智能体平台，用于研究基于任务的情境下的合作自然语言交互。它包括一个 3D 游戏环境、一个后端服务器，可为人类智能体提供训练模型，以及各种工具和流程，以实现可扩展性的研究。我们在 https://cb2.ai 上展示了一个具有学习指令跟随模型的系统演示。

    CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
    
[^104]: 基于Transformer的符号回归规划

    Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06833](http://arxiv.org/abs/2303.06833)

    该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。

    

    符号回归是机器学习中一项具有挑战性的任务，它涉及基于函数值查找其数学表达式。最近，符号回归的一些进展表明，预训练的基于Transformer的模型对于生成方程序列是有效的，这些模型从合成数据集的大规模预训练中获益，并在推理时间方面比基于GP的方法具有显著优势。然而，这些模型关注的是借鉴文本生成的监督预训练目标，而忽略了方程的特定目标，如准确性和复杂性。为了解决这个问题，我们提出了TPSR，一种基于Transformer的符号回归规划策略，将蒙特卡罗树搜索融入到Transformer解码过程中。与传统的解码策略不同，TPSR允许将非可微的反馈（如拟合准确性和复杂性）作为知识的外部来源融入到方程生成过程中。

    Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
    
[^105]: 数据中心人工智能：通过离散子集作为连续嵌入空间优化实现深度生成可微分特征选择

    Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization. (arXiv:2302.13221v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13221](http://arxiv.org/abs/2302.13221)

    该论文提出一种将离散特征子集作为连续嵌入空间优化的深度生成可微分特征选择方法，解决了在高维小样本数据集中通用、准确和维度无关的特征选择问题。

    

    特征选择（FS）旨在为给定的下游任务找到最佳特征子集，例如过滤器、包装器和嵌入式方法。但在许多实际应用中，FS的标准在不同领域中变化，并且当数据是高维和小样本时，FS容易出现问题。选择的特征子集是否可以更通用、准确和维度无关？我们将这个问题泛化为一个深度可微分特征选择任务，并提出了一个新的视角：将离散特征子集作为连续嵌入空间优化。我们开发了一个通用和原则性的框架，包括深度特征子集编码器、准确性评估器、解码器和梯度上升优化器。这个框架实现了四个步骤：1) 特征-准确性训练数据准备；2) 深度特征子集嵌入；3) 梯度优化搜索；4) 特征子集重建。我们提出了新的技术洞见：将强化作为训练数据生成器、多样化的集成模型视为搜索加速器、多尺度的特征选择和逐渐增强的探索

    Feature Selection (FS), such as filter, wrapper, and embedded methods, aims to find the optimal feature subset for a given downstream task. However, in many real-world practices, 1) the criteria of FS vary across domains; 2) FS is brittle when data is a high-dimensional and small sample size. Can selected feature subsets be more generalized, accurate, and input dimensionality agnostic? We generalize this problem into a deep differentiable feature selection task and propose a new perspective: discrete feature subsetting as continuous embedding space optimization. We develop a generic and principled framework including a deep feature subset encoder, accuracy evaluator, decoder, and gradient ascent optimizer. This framework implements four steps: 1) features-accuracy training data preparation; 2) deep feature subset embedding; 3) gradient-optimized search; 4) feature subset reconstruction. We develop new technical insights: reinforcement as a training data generator, ensembles of diverse 
    
[^106]: 构造数：如何建立一个图形？

    Construction numbers: How to build a graph?. (arXiv:2302.13186v2 [math.CO] UPDATED)

    [http://arxiv.org/abs/2302.13186](http://arxiv.org/abs/2302.13186)

    论文研究了计算偏序的线性扩展数量问题，并研究了由包含关系确定的图形的顶点和边的偏序，找到了路径、环、星形图、双星形图和完全图的构造序列数量，并提出了公式，同时研究了结构和应用。

    

    约50年前，斯坦利考虑了计算偏序的线性扩展数量问题。对于由包含关系确定的图形的顶点和边的偏序，我们称这样的线性扩展为图形的“构造序列”，因为每个边都遵循其两个端点。我们找到了路径、环、星形图、双星形图和完全图的此类序列数量。对于路径，我们认同斯坦利的想法（切线数），并得到了其他类型的公式。此外还研究了结构和应用。

    Counting the number of linear extensions of a partial order was considered by Stanley about 50 years ago. For the partial order on the vertices and edges of a graph determined by inclusion, we call such linear extensions {\it construction sequences} for the graph as each edge follows both of its endpoints. The number of such sequences for paths, cycles, stars, double-stars, and complete graphs is found. For paths, we agree with Stanley (the Tangent numbers) and get formulas for the other classes. Structure and applications are also studied.
    
[^107]: 目标网络如何稳定时间差分方法

    Why Target Networks Stabilise Temporal Difference Methods. (arXiv:2302.12537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12537](http://arxiv.org/abs/2302.12537)

    本文解释了深度强化学习中一种流行的时序差分方法中关键的稳定性问题：为什么目标网络能够有效降低不满足条件时的影响。

    

    深度强化学习中近期成功的关键在于一类使用不频繁更新目标值进行策略评估的时序差分方法。然而，有关目标网络有效性的完整理论解释仍然难以捉摸。本文针对这种流行算法进行了分析，最终回答了“为什么目标网络可以稳定时间差分学习”的问题。我们规范化了部分拟合的策略评估方法的概念，其中包括目标网络的使用，并且填补了拟合方法和半梯度时序差分算法之间的差距。利用这个框架，我们能够独特地描述所谓的致命三元组，即使用时序差分更新，结合（非线性）函数逼近和处于离线状态的数据，这经常会导致不收敛的算法。这一认识使我们得出结论：目标网络的使用可以减轻条件差时的影响。

    Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditionin
    
[^108]: 无监督的逐层文本OOD检测得分聚合

    Unsupervised Layer-wise Score Aggregation for Textual OOD Detection. (arXiv:2302.09852v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.09852](http://arxiv.org/abs/2302.09852)

    提出了一种无监督的逐层聚合异常得分的方法，用于更好地进行文本OOD检测。其能发掘不同层输出的优势，达到更鲁棒的性能，并扩展经典基准测试以反映更现实的设置。

    

    随着越来越多基于AI的系统增加，OOD检测是一个迅速发展的领域，由于新的鲁棒性和安全性要求。现有的OOD文本检测器通常依赖于在编码器的最后一层输出上计算的异常得分（例如马氏距离）。在这项工作中，我们观察到OOD检测性能因任务和层输出而异。更重要的是，我们表明通常的选择（最后一层）很少是OOD检测的最佳选择，如果选择最佳层，则可以获得更好的结果。为了利用这个观察结果，我们提出了一种数据驱动的无监督方法来结合逐层的异常得分。此外，我们通过包括更多类别的分类任务（高达77）扩展了经典文本OOD基准测试，从而反映更现实的设置。在这个增强的基准测试上，我们展示了所提出的后聚合方法实现了鲁棒的OOD检测性能。

    Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on an anomaly score (e.g., Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results could be achieved if the best layer were picked. To leverage this observation, we propose a data-driven, unsupervised method to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a greater number of classes (up to 77), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust an
    
[^109]: 零样本批次级异常检测

    Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07849](http://arxiv.org/abs/2302.07849)

    本文提出了一种名为“自适应中心表示”的方法，用于零样本批次级异常检测。该方法利用批量归一化来训练现成的深度异常检测器，可以自动零样本泛化为未见过的AD任务。在实验中，该方法显示出了在多种数据集上的优秀表现，对表格数据进行了零样本AD。

    

    异常检测（AD）在许多安全关键的应用领域中发挥着关键作用。适应正常数据分布漂移的异常检测器调整，特别是当没有针对“新正常”进行训练的数据时，这一挑战导致产生了零样本AD技术。在本文中，我们提出了一种名为自适应中心表示（ACR）的简单而有效的方法，用于零样本批次级AD。我们的方法使用批量归一化来训练现成的深度异常检测器（例如深度SVDD）来适应一组相互关联的训练数据分布，使其能够自动零样本泛化为未见过的AD任务。这个简单的方法，批量归一化加元训练，是一种非常有效和多功能的工具。我们的结果展示了对表格数据的第一个零样本AD结果，并在来自专业领域的图像数据的零样本异常检测和分段方面优于现有方法。

    Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
    
[^110]: 扩散模型易受成员隐私攻击吗？

    Are Diffusion Models Vulnerable to Membership Inference Attacks?. (arXiv:2302.01316v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01316](http://arxiv.org/abs/2302.01316)

    本文研究了扩散模型是否易受成员隐私攻击，提出了一种基于查询的成员隐私攻击方法。结果表明现有的成员隐私攻击对扩散模型基本无效。

    

    基于扩散的生成模型在图像合成方面表现出了巨大的潜力，但是它们可能带来安全和隐私风险的研究却很少。本文研究了扩散模型对成员隐私攻击的易受性，这是一种常见的隐私问题。我们的研究结果表明，现有的面向GANs或VAE的成员隐私攻击对于扩散模型来说基本无效，要么是因为适用场景不同（例如需要GANs的判别器），要么是因为假设不当（例如，合成样本和成员样本之间的距离更近）。为了填补这一空白，我们提出了一个基于查询的成员隐私攻击方法Step-wise Error Comparing Membership Inference（SecMI），它通过评估在每个时间步长中前向过程后验估计的匹配情况来推断成员身份。SecMI遵循MIA中的常见过拟合假设，即成员样本通常具有较小的估计误差，而比较保持样本。我们同时考虑了标准的扩散模型和切比雪夫扩展模型，并在多个数据集上进行了评估。

    Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e
    
[^111]: ZegOT:使用文本提示的最优传输实现零样本分割。

    ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts. (arXiv:2301.12171v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.12171](http://arxiv.org/abs/2301.12171)

    这篇论文提出了一种通过最优传输的方法，利用多个文本提示来实现零样本分割，达到了最先进的性能水平。

    

    通过将图像和文本对齐的方法，利用大规模对比性语言-图像预训练（CLIP）的成功为零样本语义分割带来了很大的希望，然而现有的方法通常需要额外的图像编码器或对CLIP模块进行重新训练或微调。本论文提出了一种新的ZegOT方法，通过最优传输将多个文本提示与冻结的图像嵌入匹配，从而实现零样本分割。特别是，通过引入一种新的多提示最优传输求解器（MPOT），该方法为每个文本提示与冻结的图像编码器隐藏层的视觉特征映射之间学习了一种最优映射。这种独特的映射方法有效地使每个文本提示关注不同的视觉语义属性。通过在基准数据集上进行广泛的实验，我们展示了我们的方法优于现有方法，达到了最先进的性能水平。

    Recent success of large-scale Contrastive Language-Image Pre-training (CLIP) has led to great promise in zero-shot semantic segmentation by transferring image-text aligned knowledge to pixel-level classification. However, existing methods usually require an additional image encoder or retraining/tuning the CLIP module. Here, we propose a novel Zero-shot segmentation with Optimal Transport (ZegOT) method that matches multiple text prompts with frozen image embeddings through optimal transport. In particular, we introduce a novel Multiple Prompt Optimal Transport Solver (MPOT), which is designed to learn an optimal mapping between multiple text prompts and visual feature maps of the frozen image encoder hidden layers. This unique mapping method facilitates each of the multiple text prompts to effectively focus on distinct visual semantic attributes. Through extensive experiments on benchmark datasets, we show that our method achieves the state-of-the-art (SOTA) performance over existing 
    
[^112]: 通过提示调整实现参数高效的低资源对话状态跟踪

    Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning. (arXiv:2301.10915v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10915](http://arxiv.org/abs/2301.10915)

    这篇论文提出了一种通过使用软提示令牌嵌入来学习任务属性的方法，以实现参数高效的低资源对话状态跟踪，同时在不微调语言模型参数的情况下，取得了比之前方法更好的性能表现。

    

    对话状态跟踪是对话管理中的一个重要步骤，需要跟踪用户的信念状态。现有的方法需要大量数据和计算资源对所有语言模型参数进行微调来应对对话状态跟踪任务。在实际部署中，需要为不同的领域和任务使用几十个微调了的语言模型，所需的成本呈指数级增长。为了降低参数大小并更好地利用跨任务共享的信息，我们提出使用软提示令牌嵌入来学习任务属性。在不微调语言模型参数的情况下，我们的方法将参数数量大幅减少到少于之前方法的0.5％，同时实现了更好的低资源对话状态跟踪性能。

    Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieves better low-resource DST performance.
    
[^113]: 注意差距：建模被审查和未被审查的电动汽车充电需求的差异。

    Mind the Gap: Modelling Difference Between Censored and Uncensored Electric Vehicle Charging Demand. (arXiv:2301.06418v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.06418](http://arxiv.org/abs/2301.06418)

    电动汽车充电需求模型的天然偏见导致了观测到的需求与实际需求有差异，使用有关审查的模型来模拟充电需求可以更好地估计真实需求。

    

    基于充电记录的电动汽车充电需求模型会天然地对可用充电器的供应产生偏见。这些模型通常无法考虑到被占用充电站和竞争对充电需求的损失。这些损失表明实际需求很可能比充电记录反映的需求更高，即真实需求是潜在的（未观察到的），而观察结果则是被审查的。因此，依赖这些观察结果进行预测充电需求的机器学习模型可能在未来基础设施扩展和供应管理方面受到限制，因为它们无法估计充电的真实需求。我们提出使用有关审查的模型来模拟充电需求以解决此限制。这些模型将审查纳入损失函数中，并从观察到的充电记录中学习真实的潜在需求分布。我们研究了被占用充电站和竞争服务如何使用GPS轨迹审查需求。

    Electric vehicle charging demand models, with charging records as input, will inherently be biased toward the supply of available chargers. These models often fail to account for demand lost from occupied charging stations and competitors. The lost demand suggests that the actual demand is likely higher than the charging records reflect, i.e., the true demand is latent (unobserved), and the observations are censored. As a result, machine learning models that rely on these observed records for forecasting charging demand may be limited in their application in future infrastructure expansion and supply management, as they do not estimate the true demand for charging. We propose using censorship-aware models to model charging demand to address this limitation. These models incorporate censorship in their loss functions and learn the true latent demand distribution from observed charging records. We study how occupied charging stations and competing services censor demand using GPS traject
    
[^114]: SPTS v2: 单点场景文本定位

    SPTS v2: Single-Point Scene Text Spotting. (arXiv:2301.01635v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.01635](http://arxiv.org/abs/2301.01635)

    本研究提出的SPTS v2框架，首次证明了可以使用极低成本的单点注释来训练场景文本定位模型，同时保留了自回归Transformer与实例分配解码器（IAD）的优势，并使用并行识别解码器（PRD）进行文本识别。

    

    由于文本检测和识别之间的内在协同作用，端到端的场景文本定位取得了显著的进展。先前的方法通常将手动标注（如水平矩形、旋转矩形、四边形和多边形）视为必要条件，而这比使用单点要昂贵得多。我们首次证明了通过提出的名为SPTS v2的框架，可以使用极低成本的单点注释来训练场景文本定位模型。SPTS v2通过以顺序预测同一预测序列中所有文本实例的中心点，保留了自回归Transformer与实例分配解码器（IAD）的优势，同时使用并行识别解码器（PRD）进行文本识别。这两个解码器共享相同的参数，并通过简单而有效的信息传输过程进行交互连接，以传递梯度和信息。

    End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. For the first time, we demonstrate that training scene text spotting models can be achieved with an extremely low-cost single-point annotation by the proposed framework, termed SPTS v2. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Compre
    
[^115]: 连续对比微调改进低资源关系提取

    Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2212.10823](http://arxiv.org/abs/2212.10823)

    本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。

    

    关系提取（RE）依赖结构化注释语料库进行模型训练，尤其在低资源情况和领域中，该任务具有挑战性。近期研究通过自监督学习来解决低资源的RE，其中解决方案包括通过RE目标预训练关系嵌入，并通过分类为基础的目标对有标签数据进行微调。然而，这种方法的一个关键挑战是目标之间的差距，它阻止RE模型充分利用预训练表示中的知识。本文旨在弥合差距，并提出使用一致的对比学习目标预训练和微调RE模型。由于在这种表示学习范式中，一个关系可能在表示空间中轻松形成多个聚类，因此我们进一步提出了多中心对比损失，允许一个关系形成多个聚类以更好地对齐预训练。在两个文档中的实验表明，所提出的方法可以在低资源情况和领域中显着提高关系提取性能。

    Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
    
[^116]: ORCA: 一项挑战性的阿拉伯语言理解基准评估

    ORCA: A Challenging Benchmark for Arabic Language Understanding. (arXiv:2212.10758v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10758](http://arxiv.org/abs/2212.10758)

    ORCA 是一个公开可用的阿拉伯语言理解评估基准，利用各种阿拉伯语言和一系列有挑战性的阿拉伯语言理解任务构建。当前使用 ORCA 对 18 个多语言和阿拉伯语言模型进行比较。

    

    由于其在所有 NLP 中的关键作用，已提出了多个基准来评估预训练语言模型。尽管有这些努力，目前尚不存在专门用于评估阿拉伯语的多样化公共基准。这使得同时评估阿拉伯语和多语言语言模型的进展变得具有挑战性。这个挑战还因阿拉伯语不是单一语言而是一系列语言和方言而变得更加困难。在这项工作中，我们介绍了 ORCA，一项公开可用的阿拉伯语言理解评估基准。ORCA 被精心构建，以覆盖多种阿拉伯语言和一系列有挑战性的阿拉伯语言理解任务，利用七个 NLU 任务集群中的 60 种不同数据集。为了衡量当前阿拉伯语 NLU 的进展，我们使用 ORCA 在 18 个多语言和阿拉伯语言模型之间进行了全面对比。我们还提供了一个公共排行榜。

    Due to their crucial role in all NLP, several benchmarks have been proposed to evaluate pretrained language models. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluation of Arabic. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and varieties. In this work, we introduce ORCA, a publicly available benchmark for Arabic language understanding evaluation. ORCA is carefully constructed to cover diverse Arabic varieties and a wide range of challenging Arabic understanding tasks exploiting 60 different datasets across seven NLU task clusters. To measure current progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between 18 multilingual and Arabic language models. We also provide a public leaderboard 
    
[^117]: 何时不信任语言模型：探索参数和非参数记忆的有效性和限制。

    When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10511](http://arxiv.org/abs/2212.10511)

    本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。

    

    尽管大型语言模型在各种任务上表现出色，但仍然难以处理需要丰富世界知识的任务，这暗示了仅依靠其参数来编码丰富的世界知识的局限性。本文旨在通过对10个模型和4种增强方法在PopQA上进行大规模知识探测实验，以了解语言模型在记忆事实知识方面的优点和局限性。我们发现，语言模型难以记忆不太流行的实际知识，并且在长尾中，扩展规模无法明显改善记忆实际知识。然后，我们展示了检索增强的语言模型在很大程度上胜过级别大得多的语言模型，而未经协助的语言模型在涉及高流行实体的问题上仍然具有竞争力。基于这些发现，我们设计了一种简单而有效的强大和高效的检索增强语言模型方法，该方法仅在需要时检索非参数记忆。

    Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
    
[^118]: Prompting就是编程: 一种大语言模型的查询语言

    Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06094](http://arxiv.org/abs/2212.06094)

    LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合，实现了一种新的语言模型编程方式。

    

    大型语言模型在问答和代码生成等各种任务上展现出了优异的表现。从高层次上讲，给定输入，语言模型可以用统计上的可能性自动完成序列。基于此，用户通过语言指令或示例来提示这些模型，以执行各种下游任务。高级提示方法甚至可以暗示模型、用户和计算器等外部工具之间的交互。然而，为了获得最先进的性能或将语言模型适应特定任务，必须实现复杂的任务-和模型特定的程序，这仍然可能需要特定的交互。基于此，我们提出了语言模型编程（LMP）的新概念。LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合。此外，LMP允许指定语言模型的约束条件。

    Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the languag
    
[^119]: 多项选择阅读理解中的世界知识

    World Knowledge in Multiple Choice Reading Comprehension. (arXiv:2211.07040v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.07040](http://arxiv.org/abs/2211.07040)

    本文研究了在多项选择阅读理解中利用世界知识的可能性，并提出了基于信息理论的度量方法，这些方法能够评估系统利用世界知识的水平，帮助测试设计人员确保使用世界知识的问题是可接受的，同时可以测试问题的质量并发现设计中可能存在的问题。

    

    最近的研究表明，在没有上下文语境的情况下，多项选择阅读理解（MCRC）系统能够比随机回答问题要好得多。这些系统使用他们积累的“世界知识”来直接回答问题，而不使用来自段落的信息。本文探讨了利用这个观察结果作为测试设计工具的可能性，以确保在特定的一组问题中使用“世界知识”是可接受的。我们提出了基于信息理论的度量方法，可以评估系统利用"世界知识"的水平。我们描述了两个度量标准：期望选项数，它测量了无需使用上下文信息，系统是否可以利用世界知识来确定一个问题的答案；词汇互信息，它测量了对于给定的问题来说上下文的重要性。我们证明了那些期望选项数较低的问题，即可以利用世界知识来回答的问题，可以被准确地鉴定。我们的结果进一步表明，我们所提出的度量方法可以用于测试问题的质量并确定测试设计中可能存在的问题。

    Recently it has been shown that without any access to the contextual passage, multiple choice reading comprehension (MCRC) systems are able to answer questions significantly better than random on average. These systems use their accumulated "world knowledge" to directly answer questions, rather than using information from the passage. This paper examines the possibility of exploiting this observation as a tool for test designers to ensure that the use of "world knowledge" is acceptable for a particular set of questions. We propose information-theory based metrics that enable the level of "world knowledge" exploited by systems to be assessed. Two metrics are described: the expected number of options, which measures whether a passage-free system can identify the answer a question using world knowledge; and the contextual mutual information, which measures the importance of context for a given question. We demonstrate that questions with low expected number of options, and hence answerabl
    
[^120]: FedGen: 适用于序列数据的可推广联邦学习

    FedGen: Generalizable Federated Learning for Sequential Data. (arXiv:2211.01914v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01914](http://arxiv.org/abs/2211.01914)

    FedGen为联邦学习带来可推广性，允许分布式设备共同识别和区分伪特征和不变特征，而不需要训练分布的先前知识。

    

    现有的遵循标准机器学习风险最小化范式的联邦学习模型在训练数据存在伪相关性的情况下往往难以推广。在许多现实世界的分布式场景中，由于分布式设备或客户端的偏差和数据采样问题，会产生伪相关性，从而错误地影响模型。当前的推广方法是为集中式训练设计的，试图识别具有与目标不变因果关系的特征，从而减少伪特征的影响。然而，这种不变风险最小化方法依赖于训练数据分布的先验知识，在许多应用中难以获得。在这项工作中，我们提出了一个名为FedGen的可推广联邦学习框架，它允许客户端以协作的方式识别和区分伪特征和不变特征，而不需要训练分布的先前知识。

    Existing federated learning models that follow the standard risk minimization paradigm of machine learning often fail to generalize in the presence of spurious correlations in the training data. In many real-world distributed settings, spurious correlations exist due to biases and data sampling issues on distributed devices or clients that can erroneously influence models. Current generalization approaches are designed for centralized training and attempt to identify features that have an invariant causal relationship with the target, thereby reducing the effect of spurious features. However, such invariant risk minimization approaches rely on apriori knowledge of training data distributions which is hard to obtain in many applications. In this work, we present a generalizable federated learning framework called FedGen, which allows clients to identify and distinguish between spurious and invariant features in a collaborative manner without prior knowledge of training distributions. We
    
[^121]: 回溯式反事实推理

    Backtracking Counterfactuals. (arXiv:2211.00472v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.00472](http://arxiv.org/abs/2211.00472)

    本文介绍了一种基于回溯式反事实推理的形式化方法，该方法基于图形模型，提供了一种更加自然和直观的推理过去时反事实情景的方法。

    

    反事实推理是人类思维中广泛存在的一种推理方式——设想一些假设场景或可能存在的情况，这些情况与实际情况不同。传统上，反事实情景被视为局部违反自然规律的“小奇迹”，但它们具有相同的初始条件。而在Pearl的结构因果模型(SCM)框架中，这通过修改因果定律的干预而使得外生变量的值共享得到了数学上的严格化。但近年来，哲学家和心理学家对这种单纯的干预主义反事实观点提出了越来越多的质疑。相反，他们提出了一种回溯式反事实观点，即在反事实世界中因果定律保持不变，将与实际情况的差异“回溯”到改变的初始条件(外生变量)。在本文中，我们提出了一种基于简单但灵活的图形模型的回溯式反事实推理的形式化方法。我们认为，在某些情况下，特别是在推理过去时，我们的模型提供了一种更自然、更直观的反事实推理方法。

    Counterfactual reasoning -- envisioning hypothetical scenarios, or possible worlds, where some circumstances are different from what (f)actually occurred (counter-to-fact) -- is ubiquitous in human cognition. Conventionally, counterfactually-altered circumstances have been treated as "small miracles" that locally violate the laws of nature while sharing the same initial conditions. In Pearl's structural causal model (SCM) framework this is made mathematically rigorous via interventions that modify the causal laws while the values of exogenous variables are shared. In recent years, however, this purely interventionist account of counterfactuals has increasingly come under scrutiny from both philosophers and psychologists. Instead, they suggest a backtracking account of counterfactuals, according to which the causal laws remain unchanged in the counterfactual world; differences to the factual world are instead "backtracked" to altered initial conditions (exogenous variables). In the pres
    
[^122]: 傅立叶分析实现一致且真实的模型解释

    Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2210.17426](http://arxiv.org/abs/2210.17426)

    该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。

    

    对于许多跨学科领域，机器学习的解释需要与当前案例相关的假设情景一致，即如果一个因素改变，模型会如何反应？尽管归因方法由优雅的公理系统支持，但它们主要关注单个输入，并且通常不一致。为支持假设情景，我们引入了一个称为真实解释的新概念，并应用布尔函数的傅立叶分析来获得严格的保证。实验结果表明，对于各种半径的邻域，我们的方法与其他方法相比，可以实现2倍至50倍更低的解释误差。

    For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
    
[^123]: 男性也洗衣服：多属性偏见放大

    Men Also Do Laundry: Multi-Attribute Bias Amplification. (arXiv:2210.11924v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.11924](http://arxiv.org/abs/2210.11924)

    本文研究了计算机视觉中的多属性偏见放大现象，发现现有的度量可能会忽略多个属性之间的相关性，并可能错误地给出度量结果，使得最小或没有偏见放大的情况被误判。

    

    随着计算机视觉系统越来越广泛地部署，研究界和公众对这些系统不仅会复制，而且会放大有害社会偏见的担忧日益增加。本文所关注的偏见放大现象是指模型在测试时放大固有的训练集偏见。现有的指标针对单个注释属性（例如，“电脑”）测量偏见放大。然而，几个视觉数据集由具有多个属性注释的图像组成。我们展示了模型可以学习利用多个属性（例如{“电脑”，“键盘”}）的相关性，而这些相关性不被当前的度量所考虑。此外，我们还展示了当前的度量可能会错误地给出最小或没有偏见放大的印象，因为它们涉及对正值和负值进行聚合。此外，这些指标缺乏明确的期望值，使它们变得困难。

    As computer vision systems become more widely deployed, there is increasing concern from both the research community and the public that these systems are not only reproducing but amplifying harmful social biases. The phenomenon of bias amplification, which is the focus of this work, refers to models amplifying inherent training set biases at test time. Existing metrics measure bias amplification with respect to single annotated attributes (e.g., $\texttt{computer}$). However, several visual datasets consist of images with multiple attribute annotations. We show models can learn to exploit correlations with respect to multiple attributes (e.g., {$\texttt{computer}$, $\texttt{keyboard}$}), which are not accounted for by current metrics. In addition, we show current metrics can give the erroneous impression that minimal or no bias amplification has occurred as they involve aggregating over positive and negative values. Further, these metrics lack a clear desired value, making them diffic
    
[^124]: 一种软件系统版本序列上的调用图演化分析方法

    Call Graph Evolution Analytics over a Version Series of an Evolving Software System. (arXiv:2210.08316v1 [cs.SE] CROSS LISTED)

    [http://arxiv.org/abs/2210.08316](http://arxiv.org/abs/2210.08316)

    该论文提出了一种Call Graph Evolution Analytics的方法，用于从软件系统版本序列的演化调用图中提取信息，通过分析演化调用图的依赖关系模式的演化，帮助软件工程师进行版本演化管理。

    

    调用图演化分析可以帮助软件工程师在维护或演进软件系统时进行更好的决策。该论文提出了一种Call Graph Evolution Analytics的方法，用于从软件系统版本序列 VS = V_1, V_2, … V_N 的演化调用图 ECG = CG_1, CG_2, … CG_N 中提取信息。这是通过使用Call Graph Evolution Rules（CGERs）和Call Graph Evolution Subgraphs（CGESs）完成的。类似于关联规则挖掘，CGERs用于捕获系统中依赖关系的共现。与调用图中的子图模式类似，CGESs用于捕获演化调用图中的依赖关系模式的演化。对这些模式的演化进行调用图分析可以识别出需要关注的潜在受影响的依赖关系（或过程调用）。

    Call Graph evolution analytics can aid a software engineer when maintaining or evolving a software system. This paper proposes Call Graph Evolution Analytics to extract information from an evolving call graph ECG = CG_1, CG_2,... CG_N for their version series VS = V_1, V_2, ... V_N of an evolving software system. This is done using Call Graph Evolution Rules (CGERs) and Call Graph Evolution Subgraphs (CGESs). Similar to association rule mining, the CGERs are used to capture co-occurrences of dependencies in the system. Like subgraph patterns in a call graph, the CGESs are used to capture evolution of dependency patterns in evolving call graphs. Call graph analytics on the evolution in these patterns can identify potentially affected dependencies (or procedure calls) that need attention. The experiments are done on the evolving call graphs of 10 large evolving systems to support dependency evolution management. We also consider results from a detailed study for evolving call graphs of M
    
[^125]: 使用核均值池化学习鲁棒的核集成

    Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00062](http://arxiv.org/abs/2210.00062)

    本文提出了核均值池（KAP），一种神经网络构建模块，它可以在卷积神经网络中自然地产生具有相似功能的核集合，提高模型对输入扰动的鲁棒性，并在多个数据集上的实验证明了其有效性。

    

    模型集成一直被用于机器学习中，以减少个别模型预测的方差，使其对输入扰动更加鲁棒。假集成方法（如dropout）也常用于深度学习模型中以提高泛化能力。然而，利用这些技术提高神经网络对输入扰动的鲁棒性仍未得到充分探索。我们引入了核均值池（KAP），它是一个神经网络构建模块，可沿着层激活张量的核维度应用均值滤波器。我们展示了在使用KAP以及通过反向传播训练的卷积神经网络中，具有相似功能的核集合自然地产生。此外，我们展示了在使用加性高斯噪声扰动的输入上进行训练时，KAP模型对各种形式的对抗性攻击具有显著的鲁棒性。在CIFAR10、CIFAR100、TinyImagenet和Imagenet数据集上的实证评估显示了实质性的结果。

    Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial 
    
[^126]: 抽取式总结的不忠实性：广泛的不忠实问题调查

    Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization. (arXiv:2209.03549v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.03549](http://arxiv.org/abs/2209.03549)

    本论文调查抽取式总结中存在的五种广泛的不忠实问题，并发现30%的抽取式摘要存在至少一种问题。为了自动检测这些问题，提出了新的度量标准 ExtEval。

    

    在抽象式总结的背景下，不忠实总结的问题已经被广泛讨论。虽然相较于抽象式总结，抽取式总结更少倾向于普遍的不忠实问题，但这是否意味着抽取式总结等同于忠实呢？结果证明并非如此。在这项工作中，我们定义了一种五种广泛的不忠实问题（包括和超出非蕴含）的分类，这些问题可能出现在抽取式总结中，包括不正确的共指、不完整的共指、不正确的话语、不完整的话语，以及其他具有误导性的信息。我们要求人类对由16个不同的抽取式系统产生的1600篇英文摘要进行标注，我们发现30%的摘要中至少存在五个问题中的一个。为了自动检测这些问题，我们发现5种现有的总结忠实度评估度量与人类评判的相关性差。为了解决这个问题，我们提出了一个新的度量标准 ExtEval。

    The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1600 English summaries produced by 16 diverse extractive systems. We find that 30% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that
    
[^127]: 基于偏好的多目标强化学习算法：PD-MORL

    PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm. (arXiv:2208.07914v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07914](http://arxiv.org/abs/2208.07914)

    PD-MORL是一种基于偏好的多目标强化学习算法，其采用偏好作为指导，适应于各种偏好空间的目标，可扩展到连续的机器人任务，并在多目标机器人控制任务上优于现有的MORL方法。

    

    多目标强化学习（MORL）方法通过最大化由偏好向量加权的联合目标函数，针对多个冲突目标的实际问题进行了处理。然而，在现实情况下，设计约束和目标通常会动态变化。此外，存储每个潜在偏好的策略是不可扩展的。因此，在给定域中使用单次训练获取整个偏好空间的帕累托前沿解集是至关重要的。为此，我们提出了一种新的MORL算法，训练一个单一的通用网络以覆盖整个偏好空间，并可扩展到连续的机器人任务。所提出的方法“基于偏好的MORL（PD-MORL）”利用偏好以更新网络参数，还采用了一种新颖的并行化方法以提高样本效率并适应当前情况下的各种偏好空间的目标。我们在多目标机器人控制任务上评估PD-MORL，并表明它在帕累托前沿覆盖和样本效率方面优于现有的MORL方法。

    Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample effi
    
[^128]: RLang：一种描述部分领域知识给强化学习智能体的声明性语言

    RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents. (arXiv:2208.06448v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06448](http://arxiv.org/abs/2208.06448)

    RLang是一种声明性语言，可以为强化学习智能体提供部分世界模型的信息，包括无模型和有模型表格算法、策略梯度和基于价值的方法、分层方法和深度方法。

    

    我们介绍了一种名为 RLang 的领域特定语言(DSL)，用于与强化学习智能体通信。与现有的只与决策制定形式中的一个元素(如奖励函数或策略)相关的 DSL 不同，RLang 可以指定关于Markov决策过程的每个元素的信息。我们为RLang定义了精确的语法和基础语义，并提供了一个解析器，将RLang程序基于算法时不变的部分世界模型和策略。我们提供了一系列演示不同RL方法如何利用所得知识的RLang程序，包括无模型和有模型表格算法、策略梯度和基于价值的方法、分层方法和深度方法。

    We introduce RLang, a domain-specific language (DSL) for communicating domain knowledge to an RL agent. Unlike existing RL DSLs that ground to \textit{single} elements of a decision-making formalism (e.g., the reward function or policy), RLang can specify information about every element of a Markov decision process. We define precise syntax and grounding semantics for RLang, and provide a parser that grounds RLang programs to an algorithm-agnostic \textit{partial} world model and policy that can be exploited by an RL agent. We provide a series of example RLang programs demonstrating how different RL methods can exploit the resulting knowledge, encompassing model-free and model-based tabular algorithms, policy gradient and value-based methods, hierarchical approaches, and deep methods.
    
[^129]: 使用智能手机时间序列数据预测帕金森病患者远程用药状态

    Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones. (arXiv:2207.13700v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13700](http://arxiv.org/abs/2207.13700)

    本研究提出了一种使用智能手机时间序列数据预测帕金森病患者药物治疗状态的方法，通过考察病人个体的历史记录、使用 Transformer 模型学习注意权重，实现了较好的客观预测结果，为个性化远程健康传感提供了一种创新方法。

    

    神经系统疾病的药物治疗通常是远程进行的，远离医院的环境对及时准确地收集健康状态数据构成了挑战。穿戴式传感器收集的行为信号的个体差异也导致采用当前的通用机器学习分析流程存在困难。为了应对这些挑战，本研究提出一种使用公共 mPower 数据集的方法，该数据集包含来自 487 名患者的智能手机收集的62,182个远程多模式测试记录，用于预测帕金森病患者的用药状态。提出的方法通过考察病人个体的历史记录，并通过 Transformer 模型学习注意权重，有望在客观上预测三种不同的用药状态：用药前（AUC = 0.95）、用药后（AUC = 0.958）和其他时刻（AUC = 0.976）。本方法为个性化远程健康传感提供了一种创新方法。

    Medication for neurological diseases such as the Parkinson's disease usually happens remotely away from hospitals. Such out-of-lab environments pose challenges in collecting timely and accurate health status data. Individual differences in behavioral signals collected from wearable sensors also lead to difficulties in adopting current general machine learning analysis pipelines. To address these challenges, we present a method for predicting the medication status of Parkinson's disease patients using the public mPower dataset, which contains 62,182 remote multi-modal test records collected on smartphones from 487 patients. The proposed method shows promising results in predicting three medication statuses objectively: Before Medication (AUC=0.95), After Medication (AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historical records with the attention weights learned through a Transformer model. Our method provides an innovative way for personalized remote health sensi
    
[^130]: 当公平遇到隐私：半隐私敏感属性下的公平分类

    When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes. (arXiv:2207.08336v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08336](http://arxiv.org/abs/2207.08336)

    该论文研究在半隐私场景下如何通过本地差分隐私（LDP）实现公平分类，解决了在收集大规模用户敏感属性时的难题。

    

    机器学习模型在许多领域已经展示出良好的性能。然而，对特定人口群体的偏见可能会阻碍它们在高风险应用中的采用。因此，在机器学习模型中确保公平性是至关重要的。大多数先前的努力需要直接访问敏感属性以减轻偏见。然而，考虑到用户对数据收集过程中隐私的担忧，通常难以获取大规模用户的敏感属性。由于法律合规性和人们对隐私的日益关注，隐私机制例如本地差分隐私（LDP）被广泛强制执行于敏感信息的数据收集阶段。因此，一个关键的问题是如何在隐私保护下进行公平预测。我们研究了一个新颖且实用的半隐私场景下的公平分类问题，其中大部分敏感属性是私有的，只有一小部分是可用的敏感属性是干净的。

    Machine learning models have demonstrated promising performance in many areas. However, the concerns that they can be biased against specific demographic groups hinder their adoption in high-stake applications. Thus, it is essential to ensure fairness in machine learning models. Most previous efforts require direct access to sensitive attributes for mitigating bias. Nonetheless, it is often infeasible to obtain large-scale users' sensitive attributes considering users' concerns about privacy in the data collection process. Privacy mechanisms such as local differential privacy (LDP) are widely enforced on sensitive information in the data collection stage due to legal compliance and people's increasing awareness of privacy. Therefore, a critical problem is how to make fair predictions under privacy. We study a novel and practical problem of fair classification in a semi-private setting, where most of the sensitive attributes are private and only a small amount of clean ones are availabl
    
[^131]: 可视化审计：透明方法难以检测异常行为。

    Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior. (arXiv:2206.13498v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13498](http://arxiv.org/abs/2206.13498)

    本研究评估了可视化方法检测模型异常行为的能力，发现现有方法难以识别微妙的异常行为，并且无法识别导致异常行为的输入。因此，需要开发更可靠的模型透明度方法。

    

    模型可视化提供了仅有输出可能会忽略的信息。但我们能相信模型可视化反映了模型行为吗？例如，它们能否诊断出种植的后门或过度正则化等异常行为？为了评估可视化方法，我们测试了它们是否将不正常训练的模型和正常模型分配给不同的可视化。我们发现，虽然现有的方法可以检测到明显异常行为的模型，但它们很难识别更微妙的异常。此外，它们经常无法识别导致异常行为的输入，例如包含虚假提示的图像。这些结果揭示了一些流行模型可视化的盲点和局限性。通过引入一种新的可视化评估框架，我们的工作为未来开发更可靠的模型透明度方法铺平了道路。

    Model visualizations provide information that outputs alone might miss. But can we trust that model visualizations reflect model behavior? For instance, can they diagnose abnormal behavior such as planted backdoors or overregularization? To evaluate visualization methods, we test whether they assign different visualizations to anomalously trained models and normal models. We find that while existing methods can detect models with starkly anomalous behavior, they struggle to identify more subtle anomalies. Moreover, they often fail to recognize the inputs that induce anomalous behavior, e.g. images containing a spurious cue. These results reveal blind spots and limitations of some popular model visualizations. By introducing a novel evaluation framework for visualizations, our work paves the way for developing more reliable model transparency methods in the future.
    
[^132]: 金字塔融合变换器用于语义分割

    Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.04019](http://arxiv.org/abs/2201.04019)

    本文提出了一种基于变换器的金字塔融合方法用于语义分割，来挖掘跨特征金字塔的丰富语义信息，取得了有竞争力的表现。

    

    最近提出的MaskFormer为语义分割任务提供了一种新的视角：它从流行的像素级分类范式转移到了一种面向掩码级别的分类方法。本质上，它生成与类别片段对应的成对概率和掩码，并在推断过程中将它们结合起来生成分割图。在我们的研究中，我们发现仅基于单尺度特征的每个掩码分类解码器不足以提取可靠的概率或掩码。为了挖掘跨特征金字塔的丰富语义信息，我们提出了一种基于变换器的用于单尺度语义分割的金字塔融合变换器（PFT）。所提出的变换器解码器在每个空间特征和可学习查询之间并行执行交叉注意，并使用跨尺度间查询注意来交换补充信息。

    The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely use
    
[^133]: C2-CRS：面向对话推荐系统的粗到细对比学习

    C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.02732](http://arxiv.org/abs/2201.02732)

    本文提出了一种新的粗到细对比学习框架，用于对话式推荐系统中多类型外部数据的数据语义融合，在此基础上提高了数据的处理效率。

    

    对话式推荐系统旨在通过自然语言交互向用户推荐适合的物品。为了开发有效的对话式推荐系统，一个主要的技术问题是如何从非常有限的对话上下文中准确地推断用户偏好。为了解决这个问题，一种有前途的解决方案是结合外部数据来丰富上下文信息。然而，以往的研究主要集中在为一些特定类型的外部数据设计融合模型，这不适用于模型和利用多类型的外部数据。为了有效利用多类型的外部数据，我们提出了一种新的粗到细对比学习框架来改善对话式推荐系统的数据语义融合。在我们的方法中，我们首先从不同的数据信号中提取和表示多种粒度的语义单元，然后以粗到细的方式对齐相关的多种语义单元。为了实现这个框架，我们设计了粗粒度和细粒度的过程来对用户建模。

    Conversational recommender systems (CRS) aim to recommend suitable items to users through natural language conversations. For developing effective CRSs, a major technical issue is how to accurately infer user preference from very limited conversation context. To address issue, a promising solution is to incorporate external data for enriching the context information. However, prior studies mainly focus on designing fusion models tailored for some specific type of external data, which is not general to model and utilize multi-type external data.  To effectively leverage multi-type external data, we propose a novel coarse-to-fine contrastive learning framework to improve data semantic fusion for CRS. In our approach, we first extract and represent multi-grained semantic units from different data signals, and then align the associated multi-type semantic units in a coarse-to-fine way. To implement this framework, we design both coarse-grained and fine-grained procedures for modeling user 
    
[^134]: AI-Enabled 移动应用中的哪些设计决策有助于更环保的 AI？

    Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?. (arXiv:2109.15284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.15284](http://arxiv.org/abs/2109.15284)

    在移动设备上部署复杂 AI 模型需要平衡准确性和复杂性，设计决策对实现高准确度和低资源消耗有影响。

    

    背景：构建、发展和使用复杂的人工智能（AI）模型需要昂贵的计算资源。虽然当前可用的高性能计算环境支持这种复杂性，但将 AI 模型部署到移动设备上（这是一种不断增长的趋势）是具有挑战性的。移动应用程序包括计算资源缺乏的环境，因此意味着在 AI 启用软件工程生命周期中进行设计决策的时候需要平衡移动应用程序的准确性和复杂性的权衡。目标：我们的目标是系统地评估在移动设备上部署复杂的 AI 模型（如神经网络）时准确性和复杂性之间的权衡，它们具有隐式的资源限制。我们旨在涵盖（i）设计决策对实现高准确度和低资源消耗的影响；以及（ii）验证剖析的实用性和可行性，针对在低资源环境下的 AI。

    Background: The construction, evolution and usage of complex artificial intelligence (AI) models demand expensive computational resources. While currently available high-performance computing environments support well this complexity, the deployment of AI models in mobile devices, which is an increasing trend, is challenging. Mobile applications consist of environments with low computational resources and hence imply limitations in the design decisions during the AI-enabled software engineering lifecycle that balance the trade-off between the accuracy and the complexity of the mobile applications.  Objective: Our objective is to systematically assess the trade-off between accuracy and complexity when deploying complex AI models (e.g. neural networks) to mobile devices, which have an implicit resource limitation. We aim to cover (i) the impact of the design decisions on the achievement of high-accuracy and low resource-consumption implementations; and (ii) the validation of profiling to
    

