# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods](https://arxiv.org/abs/2403.20150) | TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。 |
| [^2] | [MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection](https://arxiv.org/abs/2403.19888) | MambaMixer是一种新的架构，提出了具有数据依赖权重的双重选择机制，称为选择性标记和通道混合器，对长序列建模具有潜在优势。 |
| [^3] | [Detecting Generative Parroting through Overfitting Masked Autoencoders](https://arxiv.org/abs/2403.19050) | 本研究提出利用过拟合的遮蔽自编码器(MAE)来检测生成模型中的生成性模仿，建立了基于训练数据集损失的检测阈值，为了确保生成模型的合法使用和提升其法律合规性提供了一种新方法。 |
| [^4] | [Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node](https://arxiv.org/abs/2403.17209) | 通过大型语言模型代理生成AAS实例模型，实现了在数字孪生中的互操作性，降低了手动创建成本和时间。 |
| [^5] | [MatchSeg: Towards Better Segmentation via Reference Image Matching](https://arxiv.org/abs/2403.15901) | 通过引入MatchSeg框架，利用对比语言-图像预训练和联合注意力模块增强了医学图像分割，有效实现了支持集和查询集之间的知识转移。 |
| [^6] | [TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions](https://arxiv.org/abs/2403.15879) | TrustSQL是一个旨在评估文本到SQL模型在处理各种类型问题时的可靠性的新基准，要求模型在SQL预测和放弃预测两种情况下进行评估。 |
| [^7] | [Towards Measuring and Modeling "Culture" in LLMs: A Survey](https://arxiv.org/abs/2403.15412) | 这项研究调查了39篇最新论文，旨在研究大型语言模型中的文化表达和包容性，发现当前研究未对“文化”进行定义，而是在特定设计的数据集上对模型进行探究，研究了某些“文化”的方面，留下许多未被探究的有趣和重要方面，如语义领域和关于性。 |
| [^8] | [ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems](https://arxiv.org/abs/2403.12660) | 深度推荐系统中的特征选择方法研究面临着公平比较、选择属性分析缺乏以及过度关注峰值性能等挑战。 |
| [^9] | [Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot](https://arxiv.org/abs/2403.11381) | 本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。 |
| [^10] | [HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation](https://arxiv.org/abs/2403.10506) | 提出了一个高维度的仿真机器人学习基准测试HumanoidBench，揭示了目前最先进的强化学习算法在大多数任务上面临挑战，而具备鲁棒低级策略支持的分层学习基线表现更优秀。 |
| [^11] | [EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning](https://arxiv.org/abs/2403.09502) | 这项研究提出了一种利用等变性进行音频-视觉对比学习的新框架，通过一个共享的基于注意力的变换预测器来实现特征聚合和嵌入表示，有效提供了强大的监督，且计算开销最小。 |
| [^12] | [Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields](https://arxiv.org/abs/2403.08974) | 提出了一种使用隐式神经表示和去噪扩散来准确捕捉解剖树几何和拓扑结构的新方法 |
| [^13] | [A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models](https://arxiv.org/abs/2403.07322) | 通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战 |
| [^14] | [On the Diminishing Returns of Width for Continual Learning](https://arxiv.org/abs/2403.06398) | 增加神经网络宽度以减少遗忘会带来递减的回报，并且在先前研究中尚未探索的宽度范围内进行了实证验证。 |
| [^15] | [FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning](https://arxiv.org/abs/2403.06131) | 提出一种新颖的联邦算法FedPIT，通过利用LLMs的上下文学习能力自动生成任务特定的合成数据进行训练，采用参数隔离训练来防止数据提取攻击。 |
| [^16] | [Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://arxiv.org/abs/2403.05750) | 大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。 |
| [^17] | [Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering](https://arxiv.org/abs/2403.02966) | 提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。 |
| [^18] | [LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction](https://arxiv.org/abs/2403.00863) | 提出了一种名为LLM-ensemble的算法，用于集成不同大型语言模型，以提高电子商务产品属性值提取的性能。 |
| [^19] | [Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication](https://arxiv.org/abs/2402.18439) | 挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。 |
| [^20] | [On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms](https://arxiv.org/abs/2402.18129) | 分析了标准 DP 基础正则化方法对给定敏感属性的预测标签条件分布的影响，并提出了一种基于敏感属性的分布稳健优化方法来控制归纳偏差。 |
| [^21] | [Evaluating the Performance of ChatGPT for Spam Email Detection](https://arxiv.org/abs/2402.15537) | 该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。 |
| [^22] | [Active Few-Shot Fine-Tuning](https://arxiv.org/abs/2402.15441) | 该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。 |
| [^23] | [Chain-of-Thought Unfaithfulness as Disguised Accuracy](https://arxiv.org/abs/2402.14897) | 了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。 |
| [^24] | [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857) | 系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。 |
| [^25] | [MSynFD: Multi-hop Syntax aware Fake News Detection](https://arxiv.org/abs/2402.14834) | 提出一种新的多跳语法感知假新闻检测方法，通过引入补充的语法信息来处理假新闻中的微妙转折 |
| [^26] | [The FinBen: An Holistic Financial Benchmark for Large Language Models](https://arxiv.org/abs/2402.12659) | 本文引入了FinBen，这是第一个专门设计用于彻底评估LLMs在金融领域能力的全面开源评估基准，对15个代表性LLMs进行评估，揭示了它们的优势和局限性。 |
| [^27] | [SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning](https://arxiv.org/abs/2402.11903) | 提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性 |
| [^28] | [Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint](https://arxiv.org/abs/2402.11893) | 提出了一种自适应解码方法COIECD，用于识别和解决知识冲突，提高模型对冲突上下文的忠实度，并在非冲突情况下保持高性能。 |
| [^29] | [LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs](https://arxiv.org/abs/2402.11804) | 本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。 |
| [^30] | [Prospector Heads: Generalized Feature Attribution for Large Models & Data](https://arxiv.org/abs/2402.11729) | Prospector heads是一种高效且可解释的基于特征归因的替代方法，它可以应用于任何编码器和任何数据形态，并且通过对不同数据形态的实验，表现优越于传统方法。 |
| [^31] | [Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective](https://arxiv.org/abs/2402.11463) | Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。 |
| [^32] | [The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test](https://arxiv.org/abs/2402.11089) | 通过成对刻板印象测试（PST）框架，在文本-图像模型中探究性别偏见，并评估了DALLE-3在性别职业和组织权力方面的偏见。 |
| [^33] | [Physics-informed machine learning as a kernel method](https://arxiv.org/abs/2402.07514) | 物理约束的机器学习结合了数据方法的表达能力与物理模型的可解释性，可以用于正则化经验风险并提高估计器的统计性能。 |
| [^34] | [VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization](https://arxiv.org/abs/2402.07398) | VisLingInstruct通过自主优化指导文本和视觉特征提取模块，显著提高了多模态语言模型在零样本学习中的性能，在TextVQA和HatefulMemes数据集上的准确率分别提高了13.1%和9%。 |
| [^35] | [Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks](https://arxiv.org/abs/2402.05271) | 了解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。前人的研究表明，在训练过程中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这被称为神经特征分析（NFA）。本研究解释了这种相关性的出现，并发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。在早期训练阶段，可以通过解析的方式预测NFA的发展速度。 |
| [^36] | [A Bandit Approach with Evolutionary Operators for Model Selection](https://arxiv.org/abs/2402.05144) | 本文提出了一种使用进化算子的强盗方法来进行模型选择，通过将模型选择问题建模为无穷臂赌博机问题，利用部分训练和准确性作为奖励，最终的算法Mutant-UCB在测试中表现出色，优于固定预算下的最先进技术。 |
| [^37] | [Multi-Sender Persuasion -- A Computational Perspective](https://arxiv.org/abs/2402.04971) | 这项研究考虑了多个具有信息优势的发信者向单个自私行为者传递信号以影响其行为的问题，并提出了一种新颖的可微神经网络方法来近似解决这一问题。通过额外梯度算法，我们发现了超越已有方法的局部均衡解。 |
| [^38] | [LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views](https://arxiv.org/abs/2402.04644) | 本论文中，我们提出了一种名为LEVI的方法，通过以层为单位的多视角集成，实现了对预训练和微调数据中问题的解决，并提升微调模型对未见过的分布的泛化能力。 |
| [^39] | [Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy](https://arxiv.org/abs/2402.03907) | 本研究探讨了将大型语言模型嵌入扩展现实中的机会和挑战，认为通过使用LLMs可以实现扩展现实的更包容和参与，并有望推动其在日常生活中的广泛应用。 |
| [^40] | [Toward Human-AI Alignment in Large-Scale Multi-Player Games](https://arxiv.org/abs/2402.03575) | 本研究提出了一种在大规模多人游戏中评估人工智能与人类协作的方法，通过分析人类游戏数据和训练AI代理来比较和对比人类和AI的行为差异，以识别高级行为概念。 |
| [^41] | [A Survey of Data-Efficient Graph Learning](https://arxiv.org/abs/2402.00447) | 这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。 |
| [^42] | [A Study of Fairness Concerns in AI-based Mobile App Reviews](https://arxiv.org/abs/2401.08097) | 本文研究了AI基于移动应用评价中的公平关注，并通过构建数据集和开发分类器的方法，成功检测出公平性评论，并识别出约92000条公平性评论。 |
| [^43] | [MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation](https://arxiv.org/abs/2401.07314) | MapGPT引入了在线语言形成的地图，帮助GPT理解整体环境，提出自适应规划机制以协助代理执行多步路径规划。 |
| [^44] | [In-Context Reinforcement Learning for Variable Action Spaces](https://arxiv.org/abs/2312.13327) | 本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。 |
| [^45] | [Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks](https://arxiv.org/abs/2312.11560) | 本文研究了积极抑制人工神经网络中的单意义神经元，这对于提高性能具有重要意义，并提出了一种基于自发现的方法来实现抑制。 |
| [^46] | [Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making](https://arxiv.org/abs/2312.04772) | 本文研究了在序贯决策过程中的非马尔可夫公平性，发现公平往往取决于历史，需要在过程中的不同时间点进行评估。 |
| [^47] | [Diffusion model for relational inference.](http://arxiv.org/abs/2401.16755) | 这项研究提出了一种关系推理的扩散模型(DiffRI)，通过条件扩散建模学习推断组件之间连接存在的概率，并在无监督方式下发现地面真实相互作用方面具有很高的能力。 |
| [^48] | [Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach.](http://arxiv.org/abs/2401.10747) | 本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。 |
| [^49] | [DeepEdit: Knowledge Editing as Decoding with Constraints.](http://arxiv.org/abs/2401.10471) | DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。 |
| [^50] | [MISS: A Generative Pretraining and Finetuning Approach for Med-VQA.](http://arxiv.org/abs/2401.05163) | MISS是一种适用于医学视觉问答的生成式预训练与微调方法。相比于现有方法，我们把医学视觉问答作为一个生成式任务处理，通过多任务学习使图像和文本特征对齐，并通过使用大型语言模型扩展单模态图像数据集的转换和字幕方法实现特征空间的扩展。 |
| [^51] | [Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation.](http://arxiv.org/abs/2312.15643) | 这篇论文介绍了一种复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。研究发现，过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。为了解决推广到未见过观察结果的问题，引入了基于知识图谱的强化学习方法（RLF-KG），最小化观察结果与结论之间的差异。 |
| [^52] | [CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models.](http://arxiv.org/abs/2310.08753) | CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。 |
| [^53] | [DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records.](http://arxiv.org/abs/2310.07059) | 本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。 |
| [^54] | [Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology.](http://arxiv.org/abs/2310.05227) | 物理感知机器学习是一种革命性方法，它将物理知识和机器学习相结合，提供了准确的水文学理解和水循环预测，对于管理水资源以应对气候变化等挑战具有重要意义。 |
| [^55] | [Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification.](http://arxiv.org/abs/2310.05128) | 提出了一种层次感知联合监督对比学习方法（HJCL），用于层次化多标签文本分类。该方法通过使用实例级和标签级对比学习技术，以及精心构建批次来处理标签层次结构，解决了在HMTC中使用监督对比学习的问题。 |
| [^56] | [Can GPT-4 Replicate Empirical Software Engineering Research?.](http://arxiv.org/abs/2310.01727) | 本论文研究了GPT-4在新数据上执行实证软件工程研究复制的能力，探讨了其揭示实证软件工程研究方法中假设的潜力和应用前景。 |
| [^57] | [A Survey on Image-text Multimodal Models.](http://arxiv.org/abs/2309.15857) | 图像-文本多模型综述论文全面回顾了其发展历程和当前状态，提出了新的分类方法，并阐明了该领域的挑战和潜在研究方向。 |
| [^58] | [Transferring climate change knowledge.](http://arxiv.org/abs/2309.14780) | 通过转移学习方法，研究表明机器学习，尤其是深度神经网络，可以通过充分利用地球系统模型模拟和历史观测所获得的知识，更准确地预测21世纪的全球表面温度场。 |
| [^59] | [CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought.](http://arxiv.org/abs/2309.11143) | CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。 |
| [^60] | [Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models.](http://arxiv.org/abs/2308.07706) | 本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。 |
| [^61] | [Adaptive Taxonomy Learning and Historical Patterns Modelling for Patent Classification.](http://arxiv.org/abs/2308.05385) | 本文介绍了一种综合考虑专利信息的框架，旨在为专利分类提供更准确的方法。该框架综合了专利的文本描述、权利人信息和IPC代码的相关性，从而为专利分类提供更全面的上下文信息。 |
| [^62] | [A Fast and Optimal Learning-based Path Planning Method for Planetary Rovers.](http://arxiv.org/abs/2308.04792) | 本文提出了一种基于学习的快速路径规划方法，通过学习最优路径示范中的语义信息和地图表示，生成概率分布来搜索最优路径。实验结果表明，该方法能够提高行星探测车的探索效率。 |
| [^63] | [SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation.](http://arxiv.org/abs/2307.01646) | 本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。 |
| [^64] | [Physics-inspired spatiotemporal-graph AI ensemble for gravitational wave detection.](http://arxiv.org/abs/2306.15728) | 本论文提出了一种基于物理启发的时空图神经网络AI集合的方法，用于引力波探测。该方法通过混合膨胀卷积神经网络和图神经网络，准确地建模引力波信号的时空关联性，实现了在分布式环境下训练多个AI模型，并在短时间内获得最佳的分类性能。 |
| [^65] | [Optimal Control of Logically Constrained Partially Observable and Multi-Agent Markov Decision Processes.](http://arxiv.org/abs/2305.14736) | 本文介绍了一个用于部分可观察和多智能体马尔可夫决策过程的最优控制理论，能够使用时间逻辑规范表达约束，并提供了一种结构化的方法来合成策略以最大化累积奖励并保证约束条件的概率足够高。同时我们还提供了对信息不对称的多智能体设置进行最优控制的框架。 |
| [^66] | [Rethinking AI Explainability and Plausibility.](http://arxiv.org/abs/2303.17707) | 本文研究了XAI评估中最普遍的人为概念——解释合理性。虽然一直被制定为AI可解释性任务的重要评估目标，但是评估XAI的合理性有时是有害的，且无法达到模型可理解性、透明度和可信度的目的。 |
| [^67] | [Construction numbers: How to build a graph?.](http://arxiv.org/abs/2302.13186) | 论文研究了计算偏序的线性扩展数量问题，并研究了由包含关系确定的图形的顶点和边的偏序，找到了路径、环、星形图、双星形图和完全图的构造序列数量，并提出了公式，同时研究了结构和应用。 |
| [^68] | [The logic behind desirable sets of things, and its filter representation.](http://arxiv.org/abs/2302.08176) | 本研究确定了合理物品集合的基本逻辑，并提出了一些简单表述方法。 |
| [^69] | [Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey.](http://arxiv.org/abs/2301.12901) | 本文调查了城市空中出行（UAM）在大都市交通中的研究现状，确定了将UAM融入城市交通系统的关键挑战和机遇，包括对现有交通模式和拥堵的影响；安全分析和风险评估；潜在的经济和环境效益；以及为UAM和地面交通开发共享基础设施和路线。同时，我们讨论了UAM的潜在好处，如缩短旅行时间和改善服务不足地区的可达性。 |
| [^70] | [RFold: RNA Secondary Structure Prediction with Decoupled Optimization.](http://arxiv.org/abs/2212.14041) | 所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。 |
| [^71] | [Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data.](http://arxiv.org/abs/2206.02909) | 本研究利用700,000人日的未标记可穿戴传感器数据，通过自监督学习技术，成功构建了一种能够在多个数据集上泛化且效果显著优于基线模型的人体活动识别模型，有望帮助研究人员和开发者开发高性能的可定制和泛化的活动分类器。 |

# 详细

[^1]: TFB：面向时间序列预测方法全面且公平的基准比较

    TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods

    [https://arxiv.org/abs/2403.20150](https://arxiv.org/abs/2403.20150)

    TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。

    

    时间序列会在经济、交通、健康和能源等不同领域中产生，对未来数值的预测在许多重要应用中起着关键作用。不出所料，许多预测方法被提出。为了确保进展，有必要能够以全面且可靠的方式经验性地研究和比较这些方法。为了实现这一目标，我们提出了TFB，一个自动化的时间序列预测（TSF）方法基准测试。TFB通过解决与数据集、比较方法和评估管道相关的缺点，推动了最新技术的发展：1）数据领域覆盖不足，2）对传统方法的刻板印象，3）不一致和不灵活的流程。为了获得更好的领域覆盖率，我们包括了来自10个不同领域的数据集：交通、电力、能源、环境、自然、经济、股票市场、银行、健康和网络。我们还提供了一个时间序列特性

    arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
    
[^2]: MambaMixer：具有双重标记和通道选择的高效选择性状态空间模型

    MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection

    [https://arxiv.org/abs/2403.19888](https://arxiv.org/abs/2403.19888)

    MambaMixer是一种新的架构，提出了具有数据依赖权重的双重选择机制，称为选择性标记和通道混合器，对长序列建模具有潜在优势。

    

    深度学习的最新进展主要依赖于Transformers，因为它们具有数据依赖性并且能够实现大规模学习。然而，这些架构中的注意力模块展现出输入大小的二次时间和空间，限制了它们用于长序列建模的可扩展性。尽管最近有尝试为多维数据设计高效有效的架构主干，例如图像和多变量时间序列，但现有模型要么是数据独立的，要么无法允许跨维度和内部维度之间的通信。最近，状态空间模型（SSMs），尤其是具有高效硬件感知实现的选择性状态空间模型，展现出了用于长序列建模的潜在优势。受到SSMs成功的启发，我们提出了MambaMixer，一种新的具有数据依赖权重的架构，使用跨标记和通道的双重选择机制，称为选择性标记和通道混合器。

    arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
    
[^3]: 通过过拟合的遮蔽自编码器检测生成性模仿

    Detecting Generative Parroting through Overfitting Masked Autoencoders

    [https://arxiv.org/abs/2403.19050](https://arxiv.org/abs/2403.19050)

    本研究提出利用过拟合的遮蔽自编码器(MAE)来检测生成模型中的生成性模仿，建立了基于训练数据集损失的检测阈值，为了确保生成模型的合法使用和提升其法律合规性提供了一种新方法。

    

    生成式人工智能模型的出现彻底改变了数字内容创建的方式，然而由于生成性模仿问题，模型过于模仿其训练数据而给版权完整性带来挑战。本研究提出了一种新方法来解决这个问题，即利用一个过拟合的遮蔽自编码器(MAE)来有效地检测这种模仿样本。我们基于训练数据集上的平均损失建立一个检测阈值，从而精确定位修改后数据集中的模仿内容。初步评估表明了有希望的结果，显示了我们方法确保生成模型的合法使用并加强法律合规性方面的潜力。

    arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
    
[^4]: 利用大型语言模型代理生成资产管理外壳：数字孪生和语义节点中的互操作性

    Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node

    [https://arxiv.org/abs/2403.17209](https://arxiv.org/abs/2403.17209)

    通过大型语言模型代理生成AAS实例模型，实现了在数字孪生中的互操作性，降低了手动创建成本和时间。

    

    这项研究介绍了一种新颖的方法，用于协助在工业4.0背景下为数字孪生建模创建资产管理外壳（AAS）实例，旨在增强智能制造中的互操作性，减少手动工作。我们构建了一个“语义节点”数据结构来捕捉文本数据的语义要义。然后，设计并实现了一个由大型语言模型驱动的系统，用于处理“语义节点”并从文本技术数据生成AAS实例模型。我们的评估表明，有效生成率为62-79%，表明相当比例的手动创建工作可以转换为更容易的验证工作，从而减少创建AAS实例模型的时间和成本。在我们的评估中，对不同LLM的比较分析以及检索增强生成（RAG）机制的深入消融研究提供了有关LLM有效性的见解。

    arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
    
[^5]: 通过参考图像匹配实现更好的分割：MatchSeg

    MatchSeg: Towards Better Segmentation via Reference Image Matching

    [https://arxiv.org/abs/2403.15901](https://arxiv.org/abs/2403.15901)

    通过引入MatchSeg框架，利用对比语言-图像预训练和联合注意力模块增强了医学图像分割，有效实现了支持集和查询集之间的知识转移。

    

    最近，基于深度学习的自动医学图像分割方法取得了巨大成功。然而，它们严重依赖于大量的标注数据集，而获取这些数据集的成本高昂且耗时。Few-shot learning旨在通过使用一个小型标记数据集（称为支持集）来指导预测新的、未标记图像（称为查询集）的标签，从而克服对标注数据的需求。受到这一范式的启发，我们引入了MatchSeg，这是一个通过战略性参考图像匹配增强医学图像分割的新框架。我们利用对比语言-图像预训练（CLIP）在定义支持集时选择高度相关的样本。此外，我们设计了联合注意力模块来加强支持和查询特征之间的交互，促进更有效的知识转移。我们在四个公共数据集上验证了我们的方法。

    arXiv:2403.15901v1 Announce Type: new  Abstract: Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental re
    
[^6]: TrustSQL: 用于具有多样无法回答问题的文本到SQL模型的可靠性基准

    TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions

    [https://arxiv.org/abs/2403.15879](https://arxiv.org/abs/2403.15879)

    TrustSQL是一个旨在评估文本到SQL模型在处理各种类型问题时的可靠性的新基准，要求模型在SQL预测和放弃预测两种情况下进行评估。

    

    最近大型语言模型（LLMs）的进展显著提高了将自然语言问题翻译成SQL查询的准确性。在SQL生成的准确性至关重要的同时，很少有人了解这些文本到SQL模型在真实世界部署过程中能否可靠处理各种类型的问题，包括无法回答的问题。为了探讨这一方面，我们提出了TrustSQL，这是一个新的基准，旨在评估文本到SQL模型在单一数据库和跨数据库设置中的可靠性。基准任务要求模型提供两种结果之一：1）SQL预测；或2）在生成的SQL中可能存在错误或面临无法回答的问题时放弃预测。为了评估模型，我们探讨了专门为这一任务设计的各种建模方法，包括：1）为可回答性优化单独的模型

    arXiv:2403.15879v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings. The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability d
    
[^7]: 在LLMs中测量和建模“文化”：一项调查

    Towards Measuring and Modeling "Culture" in LLMs: A Survey

    [https://arxiv.org/abs/2403.15412](https://arxiv.org/abs/2403.15412)

    这项研究调查了39篇最新论文，旨在研究大型语言模型中的文化表达和包容性，发现当前研究未对“文化”进行定义，而是在特定设计的数据集上对模型进行探究，研究了某些“文化”的方面，留下许多未被探究的有趣和重要方面，如语义领域和关于性。

    

    我们呈现了对39篇最新论文的调查，旨在研究大型语言模型中的文化表达和包容性。我们观察到，没有一篇研究定义“文化”，这是一个复杂、多层面的概念；相反，它们在一些特别设计的数据集上对模型进行探究，这些数据集代表了某些“文化”的方面。我们将这些方面称为文化的代理，并将它们组织在人口统计、语义和语言文化交互代理的三个维度上。我们还对采用的探查方法进行了分类。我们的分析表明，只有“文化”的某些方面，如价值观和目标，被研究了，留下了几个其他有趣且重要的方面，特别是大量语义领域和关于性（Hershcovich等人，2022）的未被探究。另外两个关键的空白是目前方法的鲁棒性和情境性的缺乏。基于这些观察结果，

    arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
    
[^8]: ERASE：深度推荐系统特征选择方法的基准测试

    ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems

    [https://arxiv.org/abs/2403.12660](https://arxiv.org/abs/2403.12660)

    深度推荐系统中的特征选择方法研究面临着公平比较、选择属性分析缺乏以及过度关注峰值性能等挑战。

    

    深度推荐系统(DRS)越来越依赖于大量特征字段来提供更精准的推荐。有效的特征选择方法因此变得至关重要，以进一步提高准确性并优化存储效率，以满足部署需求。研究领域，特别是在DRS的背景下，尚处于初期阶段，面临三个核心挑战：首先，研究论文之间实验设置的差异往往导致不公平比较，遮蔽了实践见解。其次，现有文献缺乏基于大规模数据集的选择属性的详细分析，并且缺乏对选择技术和DRS骨干之间进行全面比较的限制性文章的通用性研究和部署。最后，研究往往专注于比较特征选择方法可达到的峰值性能，这种方法通常在计算方面不足。

    arXiv:2403.12660v1 Announce Type: cross  Abstract: Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infe
    
[^9]: LLM增强型自主代理能够合作吗？通过融合盆评估它们的合作能力

    Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot

    [https://arxiv.org/abs/2403.11381](https://arxiv.org/abs/2403.11381)

    本研究探讨了LLM增强型自主代理在合作方面的表现，结果显示虽然这些代理显示出合作倾向，但仍然存在合作困难，强调了对更健壮架构的需求。

    

    随着人工智能领域的不断发展，其中一个重要维度是发展大型语言模型及其潜力增强多代理人人工智能系统。本文通过使用著名的Meltin Pot环境以及参考模型如GPT4和GPT3.5，探讨了大型语言模型增强的自主代理(LAAs)的合作能力。初步结果表明，尽管这些代理人表现出合作的倾向，但在特定环境中仍然难以实现有效的协作，强调了更强大架构的需求。本研究的贡献包括一个用于适应LLM的Melting Pot游戏场景的抽象化层，一个用于LLM中介代理开发的可重用架构-包括短期和长期记忆以及不同的认知模块，并使用一组方法评估合作能力。

    arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
    
[^10]: HumanoidBench：用于全身运动和操作的仿真人型机器人基准测试

    HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation

    [https://arxiv.org/abs/2403.10506](https://arxiv.org/abs/2403.10506)

    提出了一个高维度的仿真机器人学习基准测试HumanoidBench，揭示了目前最先进的强化学习算法在大多数任务上面临挑战，而具备鲁棒低级策略支持的分层学习基线表现更优秀。

    

    人型机器人在协助人类在不同环境和任务中有着巨大潜力，由于其灵活性和适应性，可以利用类人形态。然而，人型机器人的研究常常受到昂贵且易损的硬件设置的限制。为了加速人型机器人算法研究，我们提出了一个高维度的仿真机器人学习基准测试，HumanoidBench，该测试包括一个配备灵巧手部和各种具有挑战性的全身操作和运动任务的人型机器人。我们的研究发现表明，最先进的强化学习算法在大多数任务上表现不佳，而具备鲁棒的低级策略支持的分层学习基线在行走或到达等任务中表现优异。借助HumanoidBench，我们为机器人社区提供了一个平台，用于识别解决人型机器人在解决各种任务时面临的挑战，促进算法研究。

    arXiv:2403.10506v1 Announce Type: cross  Abstract: Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitatin
    
[^11]: EquiAV: 利用等变性进行音频-视觉对比学习

    EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning

    [https://arxiv.org/abs/2403.09502](https://arxiv.org/abs/2403.09502)

    这项研究提出了一种利用等变性进行音频-视觉对比学习的新框架，通过一个共享的基于注意力的变换预测器来实现特征聚合和嵌入表示，有效提供了强大的监督，且计算开销最小。

    

    自我监督的音频-视觉表示学习最近取得了重大进展，展示出捕捉丰富综合表示的潜力。然而，尽管数据增强在许多学习方法中已经得到验证，音频-视觉学习仍然很难充分利用这些优势，因为增强可能会轻易破坏输入对之间的对应关系。为了解决这一限制，我们引入了EquiAV，一种利用等变性进行音频-视觉对比学习的新框架。我们的方法从扩展等变性开始进行音频-视觉学习，通过一个共享的基于注意力的变换预测器来促进。它使得来自不同增强的特征能够聚合到一个代表性的嵌入中，提供强大的监督。值得注意的是，这是在最小计算开销的情况下实现的。大量消融研究和定性结果验证了我们方法的有效性。

    arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. 
    
[^12]: 用去噪扩散隐式神经场表示解剖树

    Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields

    [https://arxiv.org/abs/2403.08974](https://arxiv.org/abs/2403.08974)

    提出了一种使用隐式神经表示和去噪扩散来准确捕捉解剖树几何和拓扑结构的新方法

    

    解剖树在临床诊断和治疗规划中起着核心作用。然而，由于解剖树的拓扑结构和几何形状多样且复杂，准确表示解剖树具有挑战性。我们提出了一种使用隐式神经表示（INRs）来表示解剖树的新方法，同时通过在INR空间中进行去噪扩散来捕捉一组树的分布。我们可以在任何所需分辨率下准确捕捉解剖树的复杂几何和拓扑结构。

    arXiv:2403.08974v1 Announce Type: cross  Abstract: Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reco
    
[^13]: 一个问题中心的多专家对比学习框架，用于提高深度序列知识追踪模型的准确性和可解释性

    A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models

    [https://arxiv.org/abs/2403.07322](https://arxiv.org/abs/2403.07322)

    通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战

    

    知识追踪在通过分析学生历史学习过程来预测其未来表现中发挥着至关重要的作用。深度神经网络在解决知识追踪问题方面展现出巨大潜力。然而，将深度学习技术应用于模拟知识追踪过程仍然存在一些重要挑战。第一个挑战在于将问题的个体信息融入建模中。这很关键，因为尽管问题共享相同的知识组件（KC），但学生对同质问题的知识习得可以有显著差异。第二个挑战在于解释现有基于深度学习的知识追踪模型的预测结果。在真实应用中，虽然可能并不需要完全透明和可解释的模型参数，但关键是以老师能理解的方式呈现模型的预测结果。

    arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
    
[^14]: 关于持续学习中宽度递减回报的研究

    On the Diminishing Returns of Width for Continual Learning

    [https://arxiv.org/abs/2403.06398](https://arxiv.org/abs/2403.06398)

    增加神经网络宽度以减少遗忘会带来递减的回报，并且在先前研究中尚未探索的宽度范围内进行了实证验证。

    

    深度神经网络在各种设置中展示了突破性的性能，但这些模型在按顺序训练新任务时经常出现“灾难性遗忘”。 一些研究已经经验性地证明增加神经网络宽度会导致灾难性遗忘减少，但尚未准确刻画宽度和持续学习之间的确切关系。我们设计了其中一个最早的框架来分析持续学习理论，并证明宽度与前馈网络（FFN）中的遗忘直接相关。 具体来说，我们证明增加网络宽度以减少遗忘会带来递减的回报。我们在先前研究中尚未探索的宽度上经验性验证了我们的论断，结果显示递减回报如我们的理论所预测的那样清晰可见。

    arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
    
[^15]: FedPIT：面向隐私保护和少样本联邦指令调整

    FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning

    [https://arxiv.org/abs/2403.06131](https://arxiv.org/abs/2403.06131)

    提出一种新颖的联邦算法FedPIT，通过利用LLMs的上下文学习能力自动生成任务特定的合成数据进行训练，采用参数隔离训练来防止数据提取攻击。

    

    指令调整对于增强大型语言模型（LLMs）在生成与人类对齐的响应方面的性能已被证明至关重要。然而，在调整过程中收集多样化、高质量的指令数据存在挑战，特别是在涉及隐私的领域。联邦指令调整（FedIT）已成为一种解决方案，利用来自多个数据所有者的联邦学习，同时保护隐私。然而，由于指令数据有限以及容易受到训练数据提取攻击的脆弱性，它面临挑战。为解决这些问题，我们提出了一种新颖的联邦算法，FedPIT，它利用LLMs的上下文学习能力自动生成用于训练的特定任务合成数据。我们的方法采用参数隔离训练来维护在合成数据上训练的全局参数和在增强本地数据上训练的本地参数，有效地防止数据提取攻击。

    arXiv:2403.06131v1 Announce Type: cross  Abstract: Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive exper
    
[^16]: 解读AI笔: 检测AI生成文本的技术与挑战

    Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text

    [https://arxiv.org/abs/2403.05750](https://arxiv.org/abs/2403.05750)

    大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。

    

    大型语言模型(LLMs)通过展示生成类人文本的惊人能力，彻底颠覆了自然语言生成(NLG)领域。然而，它们广泛的应用带来挑战，需要深入审查、伦理审查和负责任的实践。本研究探讨了这些挑战，探索了现有的缓解策略，重点是识别AI生成文本作为最终解决方案。此外，我们从理论角度评估了检测的可行性，并提出了解决当前领域限制的新颖研究方向。

    arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
    
[^17]: 面向证据的事实摘要化用于知识增强的零-shot问答

    Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering

    [https://arxiv.org/abs/2403.02966](https://arxiv.org/abs/2403.02966)

    提出了一种面向证据的事实摘要化框架EFSum，用于增强LLMs的零-shot QA性能，并确保摘要的有益性和忠实性。

    

    最近的研究探讨了利用知识图谱（KGs）来增强大语言模型（LLMs）的问答（QA）性能，然而结构化的KG形式化仍然具有挑战性。现有方法，如三元组形式或三元组事实的自由文本转换，遇到了一些问题。这些问题包括由于重复实体或关系而导致的证据密度降低，以及由于无法强调关键证据而导致的证据清晰度降低。为解决这些问题，我们提出了EFSum，一个面向证据的事实摘要化框架，用于通过知识增强的LLMs增强QA。我们通过蒸馏和偏好对齐来优化一个开源的LLM作为事实摘要器。我们的广泛实验证明，EFSum提高了LLM的零-shot QA性能，并且可以确保摘要的同时有益和忠实。

    arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
    
[^18]: LLM-Ensemble: 用于电子商务产品属性值提取的最佳大型语言模型集成方法

    LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction

    [https://arxiv.org/abs/2403.00863](https://arxiv.org/abs/2403.00863)

    提出了一种名为LLM-ensemble的算法，用于集成不同大型语言模型，以提高电子商务产品属性值提取的性能。

    

    arXiv:2403.00863v1 公告类型:跨领域摘要: 产品属性值提取是自然语言处理（NLP）和当代电子商务行业中至关重要的组成部分。提供精确的产品属性值在确保高质量推荐和提升客户满意度方面至关重要。最近出现的大型语言模型（LLMs）在许多属性提取任务中表现出最新技术水平，而无需进行领域特定的训练数据。然而，由于数据、架构和超参数的多样性，不同LLMs表现出不同的优势和劣势。这种变化使它们彼此互补，没有哪个LLM能完全压倒其他LLM。考虑到LLMs的多样优势和劣势，开发一种利用它们互补潜力的集成方法变得必要。在本文中，我们提出了一种名为LLM-ensemble的新算法，用于集成不同LLMs。

    arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
    
[^19]: 超越自然语言：LLM利用替代格式进行增强推理和沟通

    Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication

    [https://arxiv.org/abs/2402.18439](https://arxiv.org/abs/2402.18439)

    挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。

    

    自然语言（NL）长期以来一直是人类认知和沟通的主要格式，因此，在大型语言模型（LLMs）的发展和应用中同样至关重要。然而，除了NL之外，LLMs在预训练过程中看到了各种非NL格式，如代码和逻辑表达式。NL作为LLMs的最佳格式，在单一LLM推理和多智能体通信中的地位尚未得到彻底审查。在这项工作中，我们挑战了默认使用NL，通过探索在这些上下文中使用非NL格式的效用。我们展示，允许LLMs在推理或沟通之前自主选择最合适的格式，可导致不同LLMs推理效率提高3.3至5.7％，并且在多智能体通信中最多可减少72.7％的标记使用，同时保持沟通效果。我们的综合分析进一步揭示了LLMs可以......

    arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
    
[^20]: 关于基于人口统计学平等的公平学习算法的归纳偏差

    On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms

    [https://arxiv.org/abs/2402.18129](https://arxiv.org/abs/2402.18129)

    分析了标准 DP 基础正则化方法对给定敏感属性的预测标签条件分布的影响，并提出了一种基于敏感属性的分布稳健优化方法来控制归纳偏差。

    

    公平的监督式学习算法在机器学习领域备受关注，这些算法在分配标签时很少依赖敏感属性。本文分析了标准DP（人口统计学平等）基础正则化方法对给定敏感属性的预测标签条件分布的影响。我们的分析表明，在具有非均匀分布敏感属性的训练数据集中，可能会导致分类规则偏向占据大多数训练数据的敏感属性结果。为了控制DP-based公平学习中的这种归纳偏差，我们提出了基于敏感属性的分布稳健优化（SA）

    arXiv:2402.18129v1 Announce Type: cross  Abstract: Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA
    
[^21]: 评估ChatGPT用于垃圾邮件检测的性能

    Evaluating the Performance of ChatGPT for Spam Email Detection

    [https://arxiv.org/abs/2402.15537](https://arxiv.org/abs/2402.15537)

    该研究评估了ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件检测的性能，并探讨了其在这一领域的潜力。

    

    电子邮件继续是专业和商业领域中至关重要且广泛使用的通信媒介。然而，垃圾邮件的普及给用户带来了重大挑战，扰乱了他们的日常工作并降低了生产率。因此，基于内容准确地识别和过滤垃圾邮件对网络安全至关重要。最近自然语言处理领域的发展，特别是大型语言模型如ChatGPT，在诸如问答和文本生成等任务中表现出色。然而，其在垃圾邮件识别方面的潜力尚未得到充分探索。为了填补这一空白，本研究尝试评估ChatGPT在英文和中文电子邮件数据集中用于垃圾邮件识别的能力。我们利用ChatGPT进行垃圾邮件检测，采用上下文学习，需要提示说明和少量示范。

    arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
    
[^22]: 主动少样本微调

    Active Few-Shot Fine-Tuning

    [https://arxiv.org/abs/2402.15441](https://arxiv.org/abs/2402.15441)

    该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。

    

    我们研究了大型神经网络对下游任务进行主动少样本微调。我们表明少样本微调是传统主动学习和转导主动学习的泛化实例，我们提出了信息基于转导学习（ITL）的方法，该方法自适应地进行采样以最大化获得对指定下游任务的信息。在一般正则性假设下，我们证明ITL均匀收敛到可从可访问数据获取的最小可能的不确定性。据我们所知，我们是首批推导出这种泛化界限的人，这对于主动学习可能是具有独立意义的。我们将ITL应用于大型神经网络的少样本微调中，结果显示ITL明显改进了现有技术。

    arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
    
[^23]: Chain-of-Thought不忠诚作为伪装的准确性

    Chain-of-Thought Unfaithfulness as Disguised Accuracy

    [https://arxiv.org/abs/2402.14897](https://arxiv.org/abs/2402.14897)

    了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。

    

    了解Chain-of-Thought (CoT)生成与大语言模型(LLM)内部计算的一致程度对于决定是否信任LLM的输出至关重要。作为CoT忠实度的代理，arXiv:2307.13702提出了一个度量模型依赖其CoT生成答案的指标。在一个专有模型系列中，他们发现LLM表现出模型大小与其忠实度测量之间的缩放-反向缩放关系，并且130亿参数模型相比于尺寸介于8.1亿到1750亿参数之间的模型表现出增加的忠实度。我们评估这些结果是否作为所有LLM的特性泛化。我们使用三种不同系列的模型复制他们的实验设置，并在特定条件下，成功复制了他们报告的CoT忠实度的缩放趋势。然而，我们发现简单的改变设定会导致这些模式在多大程度上重复。

    arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
    
[^24]: 大型语言模型中的系统消息对越狱是否真的很重要？

    Is the System Message Really Important to Jailbreaks in Large Language Models?

    [https://arxiv.org/abs/2402.14857](https://arxiv.org/abs/2402.14857)

    系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。

    

    大型语言模型（LLMs）的快速发展使它们在现代社会中不可或缺。尽管通常会采取安全措施在发布前将LLMs与人类价值观保持一致，但最近的研究揭示了一个令人担忧的现象，被称为"越狱"。这个术语指的是当LLMs受到恶意问题提示时产生意外且可能有害的响应。现有研究侧重于生成越狱提示，但我们的研究旨在回答一个不同的问题：系统消息对LLMs中的越狱是否真的很重要？为了回答这个问题，我们在一个稳定的GPT版本gpt-3.5-turbo-0613中进行了实验，生成了具有不同系统消息的越狱提示：短，长和无消息。我们发现不同的系统消息通过实验具有不同的抵抗越狱的能力。此外，我们还探讨了越狱在LLMs之间的可转移性。这一发现强调了系统消息在防止LLMs越狱中的重要性。

    arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
    
[^25]: MSynFD: 多跳语法感知假新闻检测

    MSynFD: Multi-hop Syntax aware Fake News Detection

    [https://arxiv.org/abs/2402.14834](https://arxiv.org/abs/2402.14834)

    提出一种新的多跳语法感知假新闻检测方法，通过引入补充的语法信息来处理假新闻中的微妙转折

    

    社交媒体平台的广泛传播助长了假新闻的快速传播，对我们的现实社会构成威胁。现有方法使用多模态数据或上下文信息来增强对假新闻的检测，通过分析新闻内容和/或其社会背景。然而，这些方法常常忽视了基本的文本新闻内容（文章），并且过分依赖序列建模和全局注意力来提取语义信息。这些现有方法无法处理新闻文章中的复杂、微妙的转折，比如句法-语义不匹配和先验偏差，导致性能较低，并在缺失模态或社会背景时可能失败。为了弥合这些重要差距，我们提出了一种新颖的多跳语法感知假新闻检测（MSynFD）方法，该方法融合了补充的语法信息，以处理假新闻中的微妙转折。

    arXiv:2402.14834v1 Announce Type: cross  Abstract: The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical depen
    
[^26]: FinBen：大型语言模型的全面财务基准

    The FinBen: An Holistic Financial Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.12659](https://arxiv.org/abs/2402.12659)

    本文引入了FinBen，这是第一个专门设计用于彻底评估LLMs在金融领域能力的全面开源评估基准，对15个代表性LLMs进行评估，揭示了它们的优势和局限性。

    

    LLMs已经改变了自然语言处理，并显示出在各个领域具有潜力，但由于缺乏彻底的评估和金融任务的复杂性，它们在金融领域的潜力尚未得到充分开发。本文介绍了FinBen，第一个全面的开源评估基准，专门设计用于彻底评估LLMs在金融领域的能力。FinBen包括23种金融任务的35个数据集，这些任务根据卡特尔-霍恩-卡罗尔理论的灵感组织成三个不同难度的谱，以评估LLMs在归纳推理、联想记忆、数量推理、晶体智力等方面的认知能力。我们对15个代表性LLMs（包括GPT-4、ChatGPT和最新的Gemini）进行了评估，揭示了它们的优势和局限性。

    arXiv:2402.12659v1 Announce Type: cross  Abstract: LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations withi
    
[^27]: SoLA: 为了更好的逻辑推理而对LLM进行求解层调整

    SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning

    [https://arxiv.org/abs/2402.11903](https://arxiv.org/abs/2402.11903)

    提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性

    

    针对大型语言模型（LLMs）在逻辑推理上面临的挑战，先前的努力试图通过工具学习来改变问题求解。虽然在小规模问题上已经取得了进展，但由于规模庞大且表达复杂，解决工业案例仍然困难。在本文中，我们提出了一种新颖的求解层适应（SoLA）方法，我们在LLM中引入了一个求解器作为新层，不同地引导解决方案朝向可满足性。在SoLA中，LLM旨在理解自然语言描述的搜索空间，并识别最高质量的局部解，而求解器层则专注于初始解不满足的约束条件。借助MaxSAT作为桥梁，我们定义了前向和后向传递梯度，使最终模型能够收敛到一个满足的解或证明不可满足性。后门理论确保SoLA能够获得准确性

    arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
    
[^28]: 通过自适应解码和上下文信息-熵约束来识别和解决知识冲突

    Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint

    [https://arxiv.org/abs/2402.11893](https://arxiv.org/abs/2402.11893)

    提出了一种自适应解码方法COIECD，用于识别和解决知识冲突，提高模型对冲突上下文的忠实度，并在非冲突情况下保持高性能。

    

    大型语言模型在预训练期间内部化了大量参数化知识。与此同时，现实应用需要外部上下文知识来帮助模型完成基本任务。这引发了一个被称为知识冲突的关键困境，即上下文知识与模型内部知识相冲突。然而，现有的解码方法专门用于解决知识冲突，可能会在没有冲突的情况下无意中降低性能。本文提出了一种自适应解码方法，称为上下文信息-熵约束解码（COIECD），用于识别知识冲突并解决它们。它可以提高模型对冲突上下文的忠实度，同时在非冲突情况下保持高性能。我们的实验表明，COIECD在现实数据集中展现出较强的性能和稳健性。提供源代码。

    arXiv:2402.11893v1 Announce Type: new  Abstract: Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.
    
[^29]: LLM作为提示器：在任意知识图上进行低资源归纳推理

    LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs

    [https://arxiv.org/abs/2402.11804](https://arxiv.org/abs/2402.11804)

    本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。

    

    知识图（KG）归纳推理旨在推断训练期间未见过的新KG中缺失的事实，在各种应用中被广泛采用。KG归纳推理的一个关键挑战是处理在文本和结构方面都稀缺的低资源场景。本文尝试利用大型语言模型（LLMs）来解决这一挑战。具体来说，我们利用最先进的LLMs生成图形结构提示，以增强预训练的图神经网络（GNNs），从而为KG归纳推理方法带来新的方法论见解，以及在实践中具有很高的普适性。在方法论方面，我们引入了一种新颖的预训练和提示框架ProLINK，旨在在任意KG上进行低资源归纳推理，而无需额外训练。在实践方面，我们在36个低资源数据集上对我们的方法进行了实验评估。

    arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
    
[^30]: Prospector Heads:大规模模型和数据的广义特征归因

    Prospector Heads: Generalized Feature Attribution for Large Models & Data

    [https://arxiv.org/abs/2402.11729](https://arxiv.org/abs/2402.11729)

    Prospector heads是一种高效且可解释的基于特征归因的替代方法，它可以应用于任何编码器和任何数据形态，并且通过对不同数据形态的实验，表现优越于传统方法。

    

    特征归因是一种定位输入数据中与分类相关的区域的能力，对于科学和生物医学领域的机器学习模型而言，这是一种重要的能力。当前的特征归因方法依赖于“解释”端到端分类器的预测，存在特征定位不精确以及由于计算挑战而无法在小样本尺寸和高维数据集上使用的问题。我们引入了探寻者头部（prospector heads），这是一种高效且可解释的基于特征归因的替代方法，可以应用于任何编码器和任何数据形态。通过对序列（文本）、图像（病理学）和图（蛋白质结构）的实验，探寻者头部在模态之间进行概括，表现优于基线归因方法，平均局部化AUPRC得分提升了高达49点。我们还演示了探寻者头部如何实现了改进的解释能力。

    arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
    
[^31]: 长期时间序列预测中的吸引子记忆：混沌视角

    Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective

    [https://arxiv.org/abs/2402.11463](https://arxiv.org/abs/2402.11463)

    Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。

    

    在长期时间序列预测（LTSF）任务中，现有的深度学习模型忽视了离散时间序列源自潜在连续动态系统的关键特征，导致缺乏外推和演化能力。 鉴别真实世界数据的混沌性质，我们的模型\textbf{\textit{Attraos}}将混沌理论融入到LTSF中，将实际时间序列视为未知高维混沌动态系统的观测。 在吸引子不变性的概念下，Attraos利用提出的多尺度动态记忆单元来记忆历史动态结构，并通过频率增强的局部演化策略进行预测。 详细的理论分析和丰富的经验证据一致表明，Attraos在主流LTSF数据集和混沌数据集上的表现优于各种LTSF方法。

    arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
    
[^32]: 男性CEO和女性助理：通过成对刻板印象测试探究文本-图像模型中的性别偏见

    The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test

    [https://arxiv.org/abs/2402.11089](https://arxiv.org/abs/2402.11089)

    通过成对刻板印象测试（PST）框架，在文本-图像模型中探究性别偏见，并评估了DALLE-3在性别职业和组织权力方面的偏见。

    

    最近大规模的文本到图像（T2I）模型（如DALLE-3）展示了在新应用中的巨大潜力，但也面临前所未有的公平挑战。先前的研究揭示了单人图像生成中的性别偏见，但T2I模型应用可能需要同时描绘两个或更多人。该设定中的潜在偏见仍未被探究，导致使用中的公平相关风险。为了研究T2I模型中性别偏见的基本方面，我们提出了一种新颖的成对刻板印象测试（PST）偏见评估框架。PST促使模型生成同一图像中的两个个体，用与相反性别刻板印象相关联的两个社会身份来描述他们。通过生成的图像遵从性别刻板印象的程度来衡量偏见。利用PST，我们从两个角度评估DALLE-3：性别职业中的偏见和组织权力中的偏见。

    arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
    
[^33]: 物理约束的机器学习作为一种核方法

    Physics-informed machine learning as a kernel method

    [https://arxiv.org/abs/2402.07514](https://arxiv.org/abs/2402.07514)

    物理约束的机器学习结合了数据方法的表达能力与物理模型的可解释性，可以用于正则化经验风险并提高估计器的统计性能。

    

    物理约束的机器学习将基于数据的方法的表达能力与物理模型的可解释性相结合。在这种背景下，我们考虑一个普通的回归问题，其中经验风险由一个偏微分方程正则化，该方程量化了物理不一致性。我们证明对于线性微分先验，该问题可以被表述为核回归任务。利用核理论，我们推导出正则化风险的最小化器的收敛速度，并表明它至少以Sobolev最小化速度收敛。然而，根据物理误差的不同，可以实现更快的收敛速度。通过一个一维示例来说明这个原理，支持一个论点：使用物理信息来正则化经验风险对估计器的统计性能有益。

    Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.
    
[^34]: VisLingInstruct: 通过自主指导优化提升多模态语言模型中的零样本学习

    VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization

    [https://arxiv.org/abs/2402.07398](https://arxiv.org/abs/2402.07398)

    VisLingInstruct通过自主优化指导文本和视觉特征提取模块，显著提高了多模态语言模型在零样本学习中的性能，在TextVQA和HatefulMemes数据集上的准确率分别提高了13.1%和9%。

    

    本文介绍了VisLingInstruct，这是一种在多模态语言模型中推进零样本学习的新方法。当前的多模态语言模型在多模态任务中展现出令人印象深刻的零样本能力，但它们的性能严重依赖于指导文本的质量。VisLingInstruct通过自主评估和优化指导文本，通过上下文学习改进多模态语言模型中视觉感知和语言表达之间的协同作用。除了指导文本的改进之外，我们还优化了多模态语言模型中的视觉特征提取模块，进一步增强了对文本提示的响应能力。基于FlanT5和Vicuna的综合实验结果显示，VisLingInstruct显著提高了在视觉多模态任务中的零样本性能。值得注意的是，它在TextVQA和HatefulMemes数据集上的准确率分别比之前的最先进方法提高了13.1%和9%。

    This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.
    
[^35]: 梯度下降引发了深度非线性网络权重与经验NTK之间的对齐

    Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks

    [https://arxiv.org/abs/2402.05271](https://arxiv.org/abs/2402.05271)

    了解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。前人的研究表明，在训练过程中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这被称为神经特征分析（NFA）。本研究解释了这种相关性的出现，并发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。在早期训练阶段，可以通过解析的方式预测NFA的发展速度。

    

    理解神经网络从输入-标签对中提取统计信息的机制是监督学习中最重要的未解决问题之一。先前的研究已经确定，在一般结构的训练神经网络中，权重的格拉姆矩阵与模型的平均梯度外积成正比，这个说法被称为神经特征分析（NFA）。然而，这些数量在训练过程中如何相关尚不清楚。在这项工作中，我们解释了这种相关性的出现。我们发现NFA等价于权重矩阵的左奇异结构与与这些权重相关的经验神经切线核的显著成分之间的对齐。我们证明了先前研究中引入的NFA是由隔离这种对齐的中心化NFA驱动的。我们还展示了在早期训练阶段，可以通过解析的方式预测NFA的发展速度。

    Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
    
[^36]: 一种使用进化算子的强盗方法进行模型选择

    A Bandit Approach with Evolutionary Operators for Model Selection

    [https://arxiv.org/abs/2402.05144](https://arxiv.org/abs/2402.05144)

    本文提出了一种使用进化算子的强盗方法来进行模型选择，通过将模型选择问题建模为无穷臂赌博机问题，利用部分训练和准确性作为奖励，最终的算法Mutant-UCB在测试中表现出色，优于固定预算下的最先进技术。

    

    本文将模型选择问题建模为无穷臂赌博机问题。模型是臂，选择一个臂对应部分训练模型（资源分配）。奖励是选择模型在部分训练后的准确性。在这个最佳臂识别问题中，遗憾是最优模型的预期准确性与最终选择模型的准确性之间的差距。我们首先考虑了UCB-E在随机无穷臂赌博机问题上的直接推广，并且证明了在基本假设下，期望遗憾的顺序是$T^{-\alpha}$，其中$\alpha \in (0,1/5)$，$T$是要分配的资源数量。从这个基本算法出发，我们介绍了一种算法Mutant-UCB，它结合了进化算法的操作符。在三个开源图片分类数据集上进行的测试表明了这种新颖的组合方法的相关性，该方法优于固定预算下的国际领先技术。

    This paper formulates model selection as an infinite-armed bandit problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed bandit problem and show that, under basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha \in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget.
    
[^37]: 多发信者说服 - 从计算的角度来看

    Multi-Sender Persuasion -- A Computational Perspective

    [https://arxiv.org/abs/2402.04971](https://arxiv.org/abs/2402.04971)

    这项研究考虑了多个具有信息优势的发信者向单个自私行为者传递信号以影响其行为的问题，并提出了一种新颖的可微神经网络方法来近似解决这一问题。通过额外梯度算法，我们发现了超越已有方法的局部均衡解。

    

    我们考虑到具有信息优势的多个发信者向单个自私行为者传递信号以使其采取某些行动。这些设置是计算经济学，多智能体学习和具有多个目标的机器学习中普遍存在的。核心解决方案概念是发信者信号策略的纳什均衡。理论上，我们证明一般情况下找到一个均衡是PPAD-Hard的;实际上，计算一个发信者的最佳响应甚至是NP-Hard的。鉴于这些固有的困难，我们转而寻找局部纳什均衡。我们提出了一种新颖的可微神经网络来近似该游戏的非线性和不连续效用。结合额外梯度算法，我们发现了超越完全展示均衡和现有神经网络发现的局部均衡。广义上，我们的理论和实证贡献对广泛的类别感兴趣。

    We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
    
[^38]: LEVI:通过以层为单位的多视角集成实现可泛化的微调

    LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views

    [https://arxiv.org/abs/2402.04644](https://arxiv.org/abs/2402.04644)

    本论文中，我们提出了一种名为LEVI的方法，通过以层为单位的多视角集成，实现了对预训练和微调数据中问题的解决，并提升微调模型对未见过的分布的泛化能力。

    

    微调越来越广泛地用于在新的下游任务中利用预训练基础模型的能力。虽然在各种任务上微调取得了许多成功，但最近的研究观察到微调模型在未见过的分布（即，超出分布；OOD）上的泛化存在挑战。为了改善OOB泛化，一些先前的研究确定了微调数据的限制，并调整微调以保留自预训练数据学习到的通用表示。然而，预训练数据和模型中的潜在限制经常被忽视。在本文中，我们认为过度依赖预训练表示可能会阻碍微调学习下游任务的重要表示，从而影响其OOB泛化。当新任务来自于与预训练数据不同的（子）领域时，这可能尤为灾难性。为了解决预训练和微调数据中的问题，我们提出了一种新颖的方法。

    Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove
    
[^39]: 将大型语言模型嵌入扩展现实中：包容性、参与度和隐私的机会和挑战

    Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy

    [https://arxiv.org/abs/2402.03907](https://arxiv.org/abs/2402.03907)

    本研究探讨了将大型语言模型嵌入扩展现实中的机会和挑战，认为通过使用LLMs可以实现扩展现实的更包容和参与，并有望推动其在日常生活中的广泛应用。

    

    计算机图形学、硬件、人工智能和人机交互的最新发展可能导致扩展现实设备的普及。在这篇论文中，我们提出将大型语言模型（LLMs）嵌入扩展现实中，通过将它们嵌入虚拟角色或作为叙事方式，来促进更包容的体验。我们认为这种包容性将有助于扩展现实的多样性使用。此外，我们相信LLMs的多功能对话能力将增加用户与扩展现实环境的参与度，从而帮助扩展现实更广泛地应用于日常生活中。

    Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combin
    
[^40]: 在大规模多人游戏中实现人工智能与人类的协同

    Toward Human-AI Alignment in Large-Scale Multi-Player Games

    [https://arxiv.org/abs/2402.03575](https://arxiv.org/abs/2402.03575)

    本研究提出了一种在大规模多人游戏中评估人工智能与人类协作的方法，通过分析人类游戏数据和训练AI代理来比较和对比人类和AI的行为差异，以识别高级行为概念。

    

    在复杂的多智能体游戏中实现人工智能与人类的协同对于创建增强游戏体验的可信任的人工智能代理至关重要。我们提出了一种方法来评估这种协同，使用可解释的任务集框架，重点关注高级行为任务而非低级策略。我们的方法有三个组成部分。首先，我们分析了来自Xbox的Bleeding Edge（10万+游戏）的大量人类游戏数据，揭示了复杂任务空间中的行为模式。这个任务空间作为行为流形的基础集合，捕捉可解释的轴：战斗-逃跑、探索-利用以及单人-多人智能体。其次，我们训练一个使用生成预训练因果变换器的人工智能代理来玩Bleeding Edge，并测量其行为。第三，我们将人类和人工智能游戏映射到提出的行为流形中进行比较和对比。这样可以解释策略的差异，如，我们发现人类玩家在战斗-逃跑方面表现变化多样。

    Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight
    
[^41]: 数据高效图学习的综述

    A Survey of Data-Efficient Graph Learning

    [https://arxiv.org/abs/2402.00447](https://arxiv.org/abs/2402.00447)

    这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。

    

    图结构化数据在社交网络到生物化学分析等领域中广泛存在，是各种现实世界系统的基础。虽然图神经网络在建模这种数据方面表现出色，但它们的成功往往依赖于大量标记数据，这在标注资源有限的实际场景中构成了挑战。为了解决这个问题，我们致力于通过探索各种最小监督方法来提高低资源设置下的图机器学习性能。本文介绍了一种新颖的数据高效图学习(DEGL)的研究前沿，并提供了对DEGL当前进展的首次综述。我们首先强调了使用大规模标记数据训练模型所固有的挑战，为我们对DEGL的探索铺平了道路。接下来，我们从几个关键方面系统地回顾了这一主题的最新进展，其中包括...

    Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
    
[^42]: AI基于移动应用评价的公平关注研究

    A Study of Fairness Concerns in AI-based Mobile App Reviews

    [https://arxiv.org/abs/2401.08097](https://arxiv.org/abs/2401.08097)

    本文研究了AI基于移动应用评价中的公平关注，并通过构建数据集和开发分类器的方法，成功检测出公平性评论，并识别出约92000条公平性评论。

    

    公平是AI系统中必须解决的社会技术问题之一。不公平的AI系统，特别是不公平的AI基于移动应用，可能给全球很大一部分人口带来困难。本文旨在分析AI基于应用评价中的公平问题。我们首先手动构建了一个基准数据集，包括公平性和非公平性评论的统计样本。利用这个基准数据集，我们开发和评估了一组机器学习和深度学习分类器，用于区分公平性评论和非公平性评论。我们的实验结果显示，我们最佳的分类器可以以94%的精确度检测到公平性评论。然后，我们将最佳分类器应用于从108个AI基于应用收集的约950万条评论，识别出约92000条公平性评论。接下来，我们将K-means聚类技术应用于这92000条公平性评论。

    arXiv:2401.08097v2 Announce Type: replace-cross Abstract: Fairness is one of the socio-technical concerns that must be addressed in AI-based systems. Unfair AI-based systems, particularly unfair AI-based mobile apps, can pose difficulties for a significant proportion of the global population. This paper aims to analyze fairness concerns in AI-based app reviews.We first manually constructed a ground-truth dataset, including a statistical sample of fairness and non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning classifiers that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing classifier can detect fairness reviews with a precision of 94%. We then applied the best-performing classifier on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness r
    
[^43]: MapGPT：具有自适应路径规划的地图引导提示的视觉与语言导航

    MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation

    [https://arxiv.org/abs/2401.07314](https://arxiv.org/abs/2401.07314)

    MapGPT引入了在线语言形成的地图，帮助GPT理解整体环境，提出自适应规划机制以协助代理执行多步路径规划。

    

    具有GPT作为大脑的体验代理表现出在各种任务中的非凡决策和泛化能力。然而，现有的视觉与语言导航（VLN）零-shot代理只促使GPT-4在局部环境中选择潜在位置，而没有为代理构建一个有效的“全局视图”来理解整体环境。在这项工作中，我们提出了一种新颖的地图引导的基于GPT的代理，名为MapGPT，它引入了一个在线语言形成的地图来鼓励全局探索。具体而言，我们构建了一个在线地图，并将其合并到包含节点信息和拓扑关系的提示中，以帮助GPT理解空间环境。从这一设计中获益，我们进一步提出了一种自适应规划机制，以帮助代理根据地图执行多步规划，系统地探索多个候选

    arXiv:2401.07314v2 Announce Type: replace  Abstract: Embodied agents equipped with GPT as their brain have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT-4 to select potential locations within localized environments, without constructing an effective "global-view" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage the global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate 
    
[^44]: 变化的行动空间中的情境式强化学习

    In-Context Reinforcement Learning for Variable Action Spaces

    [https://arxiv.org/abs/2312.13327](https://arxiv.org/abs/2312.13327)

    本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。

    

    最近的研究表明，预先在多样数据集上进行上下文多情节训练的变形金刚网络可以在情境中泛化到新的强化学习任务。先前提出的模型的一个关键限制是它们依赖于预定义的行动空间大小和结构。引入新的行动空间通常需要数据重新收集和模型重新训练，这对于一些应用来说可能是昂贵的。我们的工作表明，通过提出一种只训练一次的Headless-AD模型，可以缓解这个问题，该模型能够泛化到具有可变大小、语义内容和顺序的离散动作空间。通过在伯努利和上下文赌博机以及一个网格世界环境中进行实验，我们展示了Headless-AD在从未遇到的行动空间上表现出显著的泛化能力，甚至在几个环境配置上胜过专门针对特定行动集训练的模型。

    Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
    
[^45]: 学习自发现：关于积极抑制人工神经网络单意义神经元的研究

    Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks

    [https://arxiv.org/abs/2312.11560](https://arxiv.org/abs/2312.11560)

    本文研究了积极抑制人工神经网络中的单意义神经元，这对于提高性能具有重要意义，并提出了一种基于自发现的方法来实现抑制。

    

    最近，随着大型语言模型的成功，自发现受到了研究界的广泛关注。与现有文献不同，我们提出了一个关键因素的假设，即在规模扩大的过程中高度促进性能的因素：减少只能与特定特征形成一对一关系的单意义神经元。单意义神经元往往更稀疏，并对大型模型的性能产生负面影响。受到这一观点的启发，我们提出了一种直观的思路来识别和抑制单意义神经元。然而，实现这一目标是一个非平凡的任务，因为没有统一的定量评估指标，简单地禁止单意义神经元并不能促进神经网络的多意思性。因此，本文提出了从自发现中学习的方法，并展开了关于积极抑制单意义神经元的研究。具体来说，我们首先提出了一种新的方法

    arXiv:2312.11560v2 Announce Type: replace-cross  Abstract: Recently, emergence has received widespread attention from the research community along with the success of large language models. Different from the literature, we hypothesize a key factor that highly promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we propose to learn from emergence and present a study on proactively inhibiting the monosemantic neurons in this paper. More specifically, we first propose a new
    
[^46]: 在序贯决策中记住公平：非马尔可夫公平性

    Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making

    [https://arxiv.org/abs/2312.04772](https://arxiv.org/abs/2312.04772)

    本文研究了在序贯决策过程中的非马尔可夫公平性，发现公平往往取决于历史，需要在过程中的不同时间点进行评估。

    

    公平的决策制定在很大程度上是针对单一决策进行研究的。本文在多个利益相关者可能受到决策结果影响的情况下，研究了顺序决策中的公平概念。我们观察到，公平往往取决于顺序决策过程的历史，从这个意义上讲，它是固有的非马尔可夫性。我们进一步观察到，公平通常需要在过程中的某个时间点进行评估，而不仅仅是在过程结束时。为了推进我们对这类公平性问题的理解，我们探讨了顺序决策背景下非马尔可夫公平的概念。我们确定了非马尔可夫公平的属性，包括长期公平性、任意时刻公平性、周期性公平性和有界公平性等概念。我们进一步探讨了非马尔可夫公平性和记忆之间的相互作用，以及这如何支持制定公平政策。

    arXiv:2312.04772v3 Announce Type: replace  Abstract: Fair decision making has largely been studied with respect to a single decision. In this paper we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points within the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We further explore the interplay between non-Markovian fairness and memory, and how this can support construction of fair policies
    
[^47]: 关系推理的扩散模型

    Diffusion model for relational inference. (arXiv:2401.16755v1 [cs.LG])

    [http://arxiv.org/abs/2401.16755](http://arxiv.org/abs/2401.16755)

    这项研究提出了一种关系推理的扩散模型(DiffRI)，通过条件扩散建模学习推断组件之间连接存在的概率，并在无监督方式下发现地面真实相互作用方面具有很高的能力。

    

    复杂相互作用系统的动态行为，包括大脑活动、金融价格波动和物理集体现象，与系统组成部分之间的相互作用相关。利用可观测的动态来发现这些系统中的相互作用关系被称为关系推理。在本研究中，我们提出了一种关系推理的扩散模型(DiffRI)，它借鉴了一种自监督的概率时间序列插值方法。DiffRI通过条件扩散建模学习推断组件之间连接存在的概率。对于模拟和准真实数据集的实验证明，DiffRI在无监督方式下发现地面真实相互作用方面与其他最先进的模型相比具有很高的能力。我们的代码将很快公开。

    Dynamical behaviors of complex interacting systems, including brain activities, financial price movements, and physical collective phenomena, are associated with underlying interactions between the system's components. The issue of uncovering interaction relations in such systems using observable dynamics is called relational inference. In this study, we propose a Diffusion model for Relational Inference (DiffRI), inspired by a self-supervised method for probabilistic time series imputation. DiffRI learns to infer the probability of the presence of connections between components through conditional diffusion modeling. Experiments on both simulated and quasi-real datasets show that DiffRI is highly competent compared with other state-of-the-art models in discovering ground truth interactions in an unsupervised manner. Our code will be made public soon.
    
[^48]: 缺失模态下的多模态情感分析:一种知识迁移方法

    Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])

    [http://arxiv.org/abs/2401.10747](http://arxiv.org/abs/2401.10747)

    本文提出了一种知识迁移方法，用于在缺失模态下进行多模态情感分析。通过翻译不同模态之间的内容以重构缺失的音频模态，并利用跨模态注意机制进行情感预测，实验证明了该方法在多个数据集上表现出显著的改进和与完整多模态监督方法相媲美的效果。

    

    多模态情感分析旨在通过视觉、语言和声音线索来识别个体表达的情绪。然而，现有研究大多假设在训练和测试过程中所有模态都是可用的，这使得它们的算法容易受到缺失模态的影响。在本文中，我们提出了一种新颖的知识迁移网络，用于在不同模态之间进行翻译，以重构缺失的音频模态。此外，我们还开发了一种跨模态注意机制，以保留重构和观察到的模态的最大信息，用于情感预测。在三个公开数据集上进行的大量实验证明了相对于基线算法的显著改进，并实现了与具有完整多模态监督的先前方法相媲美的结果。

    Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
    
[^49]: DeepEdit: 带有约束的解码式知识编辑

    DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])

    [http://arxiv.org/abs/2401.10471](http://arxiv.org/abs/2401.10471)

    DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。

    

    我们将大型语言模型（LLMs）的知识编辑视为带有约束的解码过程。我们提出了DeepEdit（基于深度优先搜索的渐进式解码知识编辑），这是一种神经符号方法，通过更好的推理一致性、问题相关性和对更新知识的意识来改进知识编辑。DeepEdit可灵活应用于所有黑盒LLMs：不需要访问模型参数、表示或输出词汇分布。DeepEdit逐步产生高质量的推理步骤，以实现有效的知识编辑。它利用深度优先搜索来修改LLMs的输出，从而提高输出对问题的相关性和对更新知识的意识。在知识编辑方面，DeepEdit在控制LLMs产生更简洁的推理方面表现出色。在MQuaKE上，DeepEdit在定量上取得了显著的进展，这是一个具有挑战性的多跳问题数据集。

    We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
    
[^50]: MISS：一种适用于医学视觉问答的生成式预训练与微调方法

    MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])

    [http://arxiv.org/abs/2401.05163](http://arxiv.org/abs/2401.05163)

    MISS是一种适用于医学视觉问答的生成式预训练与微调方法。相比于现有方法，我们把医学视觉问答作为一个生成式任务处理，通过多任务学习使图像和文本特征对齐，并通过使用大型语言模型扩展单模态图像数据集的转换和字幕方法实现特征空间的扩展。

    

    医学视觉问答是一项具有挑战性的多模态任务，视觉语言预训练模型能够有效提高其泛化性能。然而，当前多数方法将医学视觉问答视为一个难以转移到实际应用场景的答案分类任务。另外，由于医学图像的隐私性和昂贵的注释过程，用于预训练的大规模医学图文对数据集严重缺乏。本文中，我们提出了一种基于多任务自监督学习的大规模医学视觉问答（MISS）框架。与现有方法不同，我们将医学视觉问答视为一项生成式任务。我们将文本编码器和多模态编码器统一起来，并通过多任务学习使图像和文本特征对齐。此外，我们提出了一种通过使用大型语言模型（LLMs）扩展单模态图像数据集的转换和字幕方法，从而实现了特征空间的扩展。

    Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision fiel
    
[^51]: 通过复杂逻辑假设生成在知识图谱中推进诱导推理

    Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.15643](http://arxiv.org/abs/2312.15643)

    这篇论文介绍了一种复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。研究发现，过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。为了解决推广到未见过观察结果的问题，引入了基于知识图谱的强化学习方法（RLF-KG），最小化观察结果与结论之间的差异。

    

    诱导推理是通过做出有根据的猜测来解释观察结果的过程。尽管许多应用需要使用知识进行解释，但将诱导推理与结构化知识（如知识图谱）结合使用的方法仍然尚未得到广泛探索。为了填补这一空白，本文介绍了复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。在这个任务中，我们的目标是生成一个复杂的逻辑假设，以解释一组观察结果。我们发现，经过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。然而，当推广到未见过的观察结果时，这种训练目标并不能保证更好的假设生成。为了解决这个问题，我们引入了基于知识图谱的强化学习方法（RLF-KG），该方法最小化观察结果与结论之间的差异。

    Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusio
    
[^52]: CompA: 解决音频-语言模型中的组合推理差距

    CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])

    [http://arxiv.org/abs/2310.08753](http://arxiv.org/abs/2310.08753)

    CompA提出了由两个专家注释的音频-语言模型组合推理基准数据集，用于评估ALMs在理解音频中声音事件的顺序和属性绑定方面的表现。

    

    音频的基本特性是其组合性。使用对比方法（例如CLAP）训练的音频-语言模型（ALMs）能够学习音频和语言模态之间的共享表示，从而在许多下游应用中提高性能，包括零样本音频分类、音频检索等。然而，这些模型在有效执行组合推理方面的能力还很少被探索，需要进一步的研究。本文提出了CompA，这是一个由两个专家注释的基准数据集，其中大多数是真实世界的音频样本，用于评估ALMs的组合推理能力。我们的CompA-order评估ALMs在理解音频中声音事件的顺序或发生时的表现如何，而CompA-attribute评估声音事件的属性绑定。每个基准数据集中的实例包含两个音频-标题对，其中两个音频具有相同的声音事件，但组合方式不同。

    A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
    
[^53]: DKEC: 领域知识增强的电子病历多标签分类

    DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])

    [http://arxiv.org/abs/2310.07059](http://arxiv.org/abs/2310.07059)

    本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。

    

    医学领域的多标签文本分类任务经常面临长尾标签分布，即罕见类别的训练样本少于频繁类别。虽然之前的工作已经探索了不同的模型架构和层次化标签结构来找到重要特征，但大多数忽略了从医学指南中融入领域知识。本文提出了DKEC，一种增强医学诊断预测的领域知识增强分类器，其中包括两个创新点：（1）一个基于标签的注意力机制，结合异构图和领域本体来捕捉医学实体之间的语义关系，（2）一种基于标签相似性的简单而有效的组内训练方法，用于增加罕见类别的样本数量。我们在两个真实的医学数据集上评估了DKEC：RAA数据集，包含来自急救服务（EMS）事件的4,417个患者护理报告的收集，和来自53898报告的子集。

    Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
    
[^54]: 物理感知机器学习革命科学范式对于机器学习和基于过程的水文学的影响

    Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05227](http://arxiv.org/abs/2310.05227)

    物理感知机器学习是一种革命性方法，它将物理知识和机器学习相结合，提供了准确的水文学理解和水循环预测，对于管理水资源以应对气候变化等挑战具有重要意义。

    

    准确的水文学理解和水循环预测对于解决水资源管理中的科学和社会挑战至关重要，特别是在人为气候变化的动态影响下。现有的评论主要关注机器学习在这个领域的发展，然而水文学和机器学习作为独立的范式存在明显的区别。在这里，我们介绍了以物理感知机器学习作为一种变革性方法，克服了这种认知障碍，并革新了这两个领域。具体来说，我们提出了一个综合的物理感知机器学习方法的评论，构建了一个结构化社区（PaML），将先前的物理知识或基于物理的建模与机器学习相结合。我们系统地从物理数据引导的机器学习、物理信息处理的机器学习、物理嵌入式机器学习和物理感知混合学习四个方面分析了这些PaML方法。PaML促进了机器学习辅助的假设推导。

    Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
    
[^55]: 实例和标签: 针对层次化多标签文本分类的层次感知联合监督对比学习

    Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05128](http://arxiv.org/abs/2310.05128)

    提出了一种层次感知联合监督对比学习方法（HJCL），用于层次化多标签文本分类。该方法通过使用实例级和标签级对比学习技术，以及精心构建批次来处理标签层次结构，解决了在HMTC中使用监督对比学习的问题。

    

    层次化多标签文本分类（HMTC）旨在利用标签层次结构进行多标签分类。近期关于HMTC的方法采用对比学习在生成的样本上以半监督的方式将文本和标签嵌入接近，从而解决了对输出空间施加过度约束的问题。然而，样本的生成往往引入噪声，因为它忽略了同一批次中相似样本之间的相关性。解决这个问题的一个方法是使用监督对比学习，但由于其复杂的结构化标签，这仍然是一个未被充分研究的领域。为了克服这一挑战，我们提出了一种称为$\textbf{HJCL}$的层次感知联合监督对比学习方法，用于填补监督对比学习和HMTC之间的差距。具体而言，我们采用实例级和标签级对比学习技术，并仔细构造批次来满足标签层次结构的要求。

    Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
    
[^56]: GPT-4能否复制实证软件工程研究？

    Can GPT-4 Replicate Empirical Software Engineering Research?. (arXiv:2310.01727v1 [cs.SE])

    [http://arxiv.org/abs/2310.01727](http://arxiv.org/abs/2310.01727)

    本论文研究了GPT-4在新数据上执行实证软件工程研究复制的能力，探讨了其揭示实证软件工程研究方法中假设的潜力和应用前景。

    

    对生产系统的实证软件工程研究为从业者和研究人员带来了对软件工程过程的更好理解。然而，只有生产系统的一小部分进行了研究，限制了该研究的影响力。虽然软件工程从业者可以通过复制研究来获得充实自己数据的好处，但这也带来了一系列挑战，因为执行复制需要对研究方法和软件工程数据中的微妙细节有深入的理解。鉴于大型语言模型（LLMs）如GPT-4在处理软件工程和科学任务方面显示出潜力，这些模型可以帮助推广实证软件工程研究。在这篇论文中，我们研究了LLMs在新数据上执行实证软件工程研究复制的能力。我们特别研究了它们揭示实证软件工程研究方法中所做的假设的能力。

    Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help democratize empirical software engineering research.  In this paper, we examine LLMs' abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodolog
    
[^57]: 图像-文本多模型综述论文

    A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])

    [http://arxiv.org/abs/2309.15857](http://arxiv.org/abs/2309.15857)

    图像-文本多模型综述论文全面回顾了其发展历程和当前状态，提出了新的分类方法，并阐明了该领域的挑战和潜在研究方向。

    

    在人工智能不断发展的背景下，图像和文本信息的融合成为一个至关重要的领域，导致了图像-文本多模型的出现。本论文全面回顾了图像-文本多模型的发展历程和当前状态，探讨了它们的应用价值、挑战和潜在研究方向。首先，我们重新审视了这些模型的基本概念和发展里程碑，引入了一种新的分类方法，将它们的发展分为三个不同的阶段，基于它们被引入的时间和对学科的影响。此外，基于任务在学术领域中的重要性和普及性，我们提出了将与图像-文本多模型相关的任务划分为五个主要类型的分类方法，阐明了每个类别内的最新进展和关键技术。尽管这些模型取得了显著的成就，但仍面临着许多挑战。

    Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
    
[^58]: 转移气候变化知识

    Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.14780](http://arxiv.org/abs/2309.14780)

    通过转移学习方法，研究表明机器学习，尤其是深度神经网络，可以通过充分利用地球系统模型模拟和历史观测所获得的知识，更准确地预测21世纪的全球表面温度场。

    

    准确的气候预测对于气候适应和减缓至关重要。用于预测气候变化的地球系统模型模拟在对小尺度物理过程（例如云）的表示中本质上进行了近似，这是全球平均温度对增加的温室气体浓度的响应中不确定性的根源。已经开发了多种方法，用于使用历史观测约束未来预测，并减少气候预测和气候反馈的不确定性。然而，这些方法无法捕捉气候系统固有的非线性复杂性。通过使用转移学习方法，我们展示了机器学习，特别是深度神经网络，可以用于最大程度地利用和整合从地球系统模型模拟和历史观测中获得的知识，以更准确地预测21世纪全球表面温度场。

    Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
    
[^59]: CoT-BERT: 通过思维链条增强无监督句子表示

    CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])

    [http://arxiv.org/abs/2309.11143](http://arxiv.org/abs/2309.11143)

    CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。

    

    无监督句子表示学习旨在将输入句子转化为富含复杂语义信息的固定长度向量，同时消除对标注数据的依赖。近年来，在对比学习和提示工程的推动下，该领域取得了显著进展，极大地缩小了无监督和有监督策略之间的差距。然而，在这个轨迹中，仍然没有充分利用思维链条的潜在能力。为了释放预训练模型（如BERT）中的潜能，我们提出了一个句子表示的两阶段方法：理解和摘要。随后，后一阶段的输出被利用为输入句子的向量化表示。为了进一步提高性能，我们对对比学习损失函数和模板去噪技术进行了精细调整。严格的实验验证了我们的方法CoT-BERT的优越性。

    Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
    
[^60]: 利用视觉-语言模型在医学图像分割中探索迁移学习

    Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])

    [http://arxiv.org/abs/2308.07706](http://arxiv.org/abs/2308.07706)

    本论文提出使用视觉-语言模型进行医学图像分割的迁移学习，并评估了其在医学领域的可迁移性。通过捕捉语义信息和引入新的图像描述变化，实现了对多样化医学图像的分割。

    

    医学图像分割在医学领域的各种临床应用中至关重要。尽管最先进的分割模型已被证明有效，但在这个任务中整合文本指导以增强视觉特征仍然是一个进展有限的领域。现有利用文本指导的分割模型主要在开放领域图像上训练，这引发了在医学领域直接应用的难题，需要手动介入或进行微调。为了解决这些挑战，我们提出使用多模态的视觉-语言模型从图像描述和图像中捕捉语义信息，使得能够对多样化的医学图像进行分割。该研究全面评估了现有的视觉-语言模型在多个数据集上的可迁移性，以评估其从开放领域向医学领域的迁移能力。此外，我们对数据集中以前未见图像的图像描述引入了变化，揭示了显著的变异。

    Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
    
[^61]: 自适应分类学习和历史模式建模用于专利分类

    Adaptive Taxonomy Learning and Historical Patterns Modelling for Patent Classification. (arXiv:2308.05385v1 [cs.AI])

    [http://arxiv.org/abs/2308.05385](http://arxiv.org/abs/2308.05385)

    本文介绍了一种综合考虑专利信息的框架，旨在为专利分类提供更准确的方法。该框架综合了专利的文本描述、权利人信息和IPC代码的相关性，从而为专利分类提供更全面的上下文信息。

    

    专利分类旨在为给定的专利分配多个国际专利分类（IPC）代码。最近用于自动分类专利的方法主要集中于分析专利的文本描述。然而，除了文本之外，每个专利还与一些权利人相关联，了解他们申请的专利对于分类往往是有价值的。此外，IPC系统制定的层次化分类法提供了重要的上下文信息，并且使模型可以利用IPC代码之间的相关性进行更准确的分类。然而，现有方法未能综合考虑上述方面。在本文中，我们提出了一个综合考虑专利信息的整合框架来进行专利分类。具体而言，我们首先提出了一个IPC代码相关性学习模块，通过在同一层级和不同层级之间自适应地传递和聚合消息，从而得到它们的语义表示。

    Patent classification aims to assign multiple International Patent Classification (IPC) codes to a given patent. Recent methods for automatically classifying patents mainly focus on analyzing the text descriptions of patents. However, apart from the texts, each patent is also associated with some assignees, and the knowledge of their applied patents is often valuable for classification. Furthermore, the hierarchical taxonomy formulated by the IPC system provides important contextual information and enables models to leverage the correlations between IPC codes for more accurate classification. However, existing methods fail to incorporate the above aspects. In this paper, we propose an integrated framework that comprehensively considers the information on patents for patent classification. To be specific, we first present an IPC codes correlations learning module to derive their semantic representations via adaptively passing and aggregating messages within the same level and across dif
    
[^62]: 快速和最优的基于学习的行星探测车路径规划方法

    A Fast and Optimal Learning-based Path Planning Method for Planetary Rovers. (arXiv:2308.04792v1 [cs.RO])

    [http://arxiv.org/abs/2308.04792](http://arxiv.org/abs/2308.04792)

    本文提出了一种基于学习的快速路径规划方法，通过学习最优路径示范中的语义信息和地图表示，生成概率分布来搜索最优路径。实验结果表明，该方法能够提高行星探测车的探索效率。

    

    智能自主路径规划对于提高行星探测车的探索效率至关重要。在本文中，我们提出了一种基于学习的方法，在高程图中快速搜索最优路径，称为NNPP。NNPP模型从大量预注释的最优路径示范中学习起始和目标位置的语义信息，以及地图表示，并生成每个像素的概率分布，表示其属于地图上最优路径的可能性。具体而言，本文从DEM获取的坡度、粗糙度和高度差计算每个网格单元的遍历成本。随后，使用高斯分布对起始和目标位置进行编码，并分析不同位置编码参数对模型性能的影响。经过训练，NNPP模型能够在新的地图上执行路径规划。实验证明，NNPP生成的引导场能够准确指导行星探测车的运动。

    Intelligent autonomous path planning is crucial to improve the exploration efficiency of planetary rovers. In this paper, we propose a learning-based method to quickly search for optimal paths in an elevation map, which is called NNPP. The NNPP model learns semantic information about start and goal locations, as well as map representations, from numerous pre-annotated optimal path demonstrations, and produces a probabilistic distribution over each pixel representing the likelihood of it belonging to an optimal path on the map. More specifically, the paper computes the traversal cost for each grid cell from the slope, roughness and elevation difference obtained from the DEM. Subsequently, the start and goal locations are encoded using a Gaussian distribution and different location encoding parameters are analyzed for their effect on model performance. After training, the NNPP model is able to perform path planning on novel maps. Experiments show that the guidance field generated by the 
    
[^63]: SwinGNN:重新思考在图生成的扩散模型中的置换不变性

    SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])

    [http://arxiv.org/abs/2307.01646](http://arxiv.org/abs/2307.01646)

    本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。

    

    基于置换等变网络的扩散模型可以学习图数据的置换不变分布。然而，相对于非不变模型，我们发现这些不变模型遇到了更大的学习挑战，因为1）它们的目标分布更具模态性；2）它们的最优一步去噪得分是具有更多成分的高斯混合物的得分函数。受到这个分析的启发，我们提出了一种非不变的扩散模型，称为“SwinGNN”，它采用了一种高效的边到边的2-WL消息传递网络，并利用SwinTransformers中的移动窗口自注意力。此外，通过系统性的实验和剖析，我们确定了几种关键的训练和采样技术，显著提高了图生成样本的质量。最后，我们引入了一种简单的后处理技巧，即随机置换生成的图，可以证明将任何图转换成图形统计量。

    Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
    
[^64]: 基于物理启发的时空图神经网络AI集合用于引力波探测

    Physics-inspired spatiotemporal-graph AI ensemble for gravitational wave detection. (arXiv:2306.15728v1 [astro-ph.IM])

    [http://arxiv.org/abs/2306.15728](http://arxiv.org/abs/2306.15728)

    本论文提出了一种基于物理启发的时空图神经网络AI集合的方法，用于引力波探测。该方法通过混合膨胀卷积神经网络和图神经网络，准确地建模引力波信号的时空关联性，实现了在分布式环境下训练多个AI模型，并在短时间内获得最佳的分类性能。

    

    我们引入了一种新颖的方法来探测引力波，该方法结合了：1）混合膨胀卷积神经网络，以准确地建模引力波信号的短时和长时序列信息；以及2）图神经网络，以捕捉引力波天文观测站之间的空间关联，以一致地描述和识别探测器网络中的信号存在。这些时空图神经网络AI模型经过测试，用于探测近圆非自旋和近圆自旋非进动的二进制黑洞合并产生的引力波信号。对于后一种情况，我们需要一个包含120万个模拟波形的数据集来密集采样这个信号流形。因此，我们通过在阿贡国家实验室龙头超级计算机Polaris上使用256个NVIDIA A100 GPU进行分布式训练，在1.7小时内实现了训练时间到解决方案的减小，并获得了最佳的分类性能。

    We introduce a novel method for gravitational wave detection that combines: 1) hybrid dilated convolution neural networks to accurately model both shortand long-range temporal sequential information of gravitational wave signals; and 2) graph neural networks to capture spatial correlations among gravitational wave observatories to consistently describe and identify the presence of a signal in a detector network. These spatiotemporal-graph AI models are tested for signal detection of gravitational waves emitted by quasi-circular, non-spinning and quasi-circular, spinning, non-precessing binary black hole mergers. For the latter case, we needed a dataset of 1.2 million modeled waveforms to densely sample this signal manifold. Thus, we reduced time-to-solution by training several AI models in the Polaris supercomputer at the Argonne Leadership Supercomputing Facility within 1.7 hours by distributing the training over 256 NVIDIA A100 GPUs, achieving optimal classification performance. Th
    
[^65]: 逻辑约束下的部分可观察和多智能体马尔可夫决策过程的最优控制

    Optimal Control of Logically Constrained Partially Observable and Multi-Agent Markov Decision Processes. (arXiv:2305.14736v1 [cs.AI])

    [http://arxiv.org/abs/2305.14736](http://arxiv.org/abs/2305.14736)

    本文介绍了一个用于部分可观察和多智能体马尔可夫决策过程的最优控制理论，能够使用时间逻辑规范表达约束，并提供了一种结构化的方法来合成策略以最大化累积奖励并保证约束条件的概率足够高。同时我们还提供了对信息不对称的多智能体设置进行最优控制的框架。

    

    自动化系统通常会产生逻辑约束，例如来自安全、操作或法规要求，可以用时间逻辑规范表达这些约束。系统状态通常是部分可观察的，可能包含具有共同目标但不同信息结构和约束的多个智能体。在本文中，我们首先引入了一个最优控制理论，用于具有有限线性时间逻辑约束的部分可观察马尔可夫决策过程（POMDP）。我们提供了一种结构化方法，用于合成策略，同时确保满足时间逻辑约束的概率足够高时最大化累积回报。我们的方法具有关于近似奖励最优性和约束满足的保证。然后我们在此基础上构建了一个对信息不对称的具有逻辑约束的多智能体设置进行最优控制的框架。我们阐述了该方法并给出了理论保证。

    Autonomous systems often have logical constraints arising, for example, from safety, operational, or regulatory requirements. Such constraints can be expressed using temporal logic specifications. The system state is often partially observable. Moreover, it could encompass a team of multiple agents with a common objective but disparate information structures and constraints. In this paper, we first introduce an optimal control theory for partially observable Markov decision processes (POMDPs) with finite linear temporal logic constraints. We provide a structured methodology for synthesizing policies that maximize a cumulative reward while ensuring that the probability of satisfying a temporal logic constraint is sufficiently high. Our approach comes with guarantees on approximate reward optimality and constraint satisfaction. We then build on this approach to design an optimal control framework for logically constrained multi-agent settings with information asymmetry. We illustrate the
    
[^66]: 重新思考人工智能可解释性与合理性

    Rethinking AI Explainability and Plausibility. (arXiv:2303.17707v1 [cs.AI])

    [http://arxiv.org/abs/2303.17707](http://arxiv.org/abs/2303.17707)

    本文研究了XAI评估中最普遍的人为概念——解释合理性。虽然一直被制定为AI可解释性任务的重要评估目标，但是评估XAI的合理性有时是有害的，且无法达到模型可理解性、透明度和可信度的目的。

    

    为了使可解释人工智能（XAI）算法符合人类交流规范，支持人类推理过程，并满足人类对于AI解释的需求，设定适当的评估目标至关重要。在本文中，我们研究了解释合理性，这是XAI评估中最普遍的人为概念。合理性衡量机器解释与人类解释相比的合理程度。合理性一直被传统地制定为AI可解释性任务的重要评估目标。我们反对这个想法，并展示了如何优化和评估XAI的合理性有时是有害的，且无法达到模型可理解性、透明度和可信度的目的。具体来说，评估XAI算法的合理性会规范机器解释，以表达与人类解释完全相同的内容，这偏离了人类解释的基本动机：表达自己的理解。

    Setting proper evaluation objectives for explainable artificial intelligence (XAI) is vital for making XAI algorithms follow human communication norms, support human reasoning processes, and fulfill human needs for AI explanations. In this article, we examine explanation plausibility, which is the most pervasive human-grounded concept in XAI evaluation. Plausibility measures how reasonable the machine explanation is compared to the human explanation. Plausibility has been conventionally formulated as an important evaluation objective for AI explainability tasks. We argue against this idea, and show how optimizing and evaluating XAI for plausibility is sometimes harmful, and always ineffective to achieve model understandability, transparency, and trustworthiness. Specifically, evaluating XAI algorithms for plausibility regularizes the machine explanation to express exactly the same content as human explanation, which deviates from the fundamental motivation for humans to explain: expres
    
[^67]: 构造数：如何建立一个图形？

    Construction numbers: How to build a graph?. (arXiv:2302.13186v2 [math.CO] UPDATED)

    [http://arxiv.org/abs/2302.13186](http://arxiv.org/abs/2302.13186)

    论文研究了计算偏序的线性扩展数量问题，并研究了由包含关系确定的图形的顶点和边的偏序，找到了路径、环、星形图、双星形图和完全图的构造序列数量，并提出了公式，同时研究了结构和应用。

    

    约50年前，斯坦利考虑了计算偏序的线性扩展数量问题。对于由包含关系确定的图形的顶点和边的偏序，我们称这样的线性扩展为图形的“构造序列”，因为每个边都遵循其两个端点。我们找到了路径、环、星形图、双星形图和完全图的此类序列数量。对于路径，我们认同斯坦利的想法（切线数），并得到了其他类型的公式。此外还研究了结构和应用。

    Counting the number of linear extensions of a partial order was considered by Stanley about 50 years ago. For the partial order on the vertices and edges of a graph determined by inclusion, we call such linear extensions {\it construction sequences} for the graph as each edge follows both of its endpoints. The number of such sequences for paths, cycles, stars, double-stars, and complete graphs is found. For paths, we agree with Stanley (the Tangent numbers) and get formulas for the other classes. Structure and applications are also studied.
    
[^68]: 合理物品集合背后的逻辑及其过滤器表示

    The logic behind desirable sets of things, and its filter representation. (arXiv:2302.08176v2 [math.LO] UPDATED)

    [http://arxiv.org/abs/2302.08176](http://arxiv.org/abs/2302.08176)

    本研究确定了合理物品集合的基本逻辑，并提出了一些简单表述方法。

    

    我们确定了最近一组合理且令人向往的物品集合理论的逻辑，这个理论推广了令人向往的赌注和一致的选择函数，我们证明这种识别允许我们建立各种表示结果，用更简单的方法来描述这种一致性模型。

    We identify the logic behind the recent theory of coherent sets of desirable (sets of) things, which generalise desirable (sets of) gambles and coherent choice functions, and show that this identification allows us to establish various representation results for such coherent models in terms of simpler ones.
    
[^69]: 模拟城市空中出行融入现有交通系统：一项调查

    Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey. (arXiv:2301.12901v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.12901](http://arxiv.org/abs/2301.12901)

    本文调查了城市空中出行（UAM）在大都市交通中的研究现状，确定了将UAM融入城市交通系统的关键挑战和机遇，包括对现有交通模式和拥堵的影响；安全分析和风险评估；潜在的经济和环境效益；以及为UAM和地面交通开发共享基础设施和路线。同时，我们讨论了UAM的潜在好处，如缩短旅行时间和改善服务不足地区的可达性。

    This paper surveys the current state of research on urban air mobility (UAM) in metropolitan-scale traffic using simulation techniques, identifying key challenges and opportunities for integrating UAM into urban transportation systems, including impacts on existing traffic patterns and congestion, safety analysis and risk assessment, potential economic and environmental benefits, and the development of shared infrastructure and routes for UAM and ground-based transportation. The potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas, are also discussed.

    城市空中出行（UAM）有可能彻底改变大都市地区的交通方式，提供一种新的交通方式，缓解拥堵，提高可达性。然而，将UAM融入现有交通系统是一项复杂的任务，需要深入了解其对交通流量和容量的影响。在本文中，我们进行了一项调查，使用模拟技术调查了UAM在大都市交通中的研究现状。我们确定了将UAM融入城市交通系统的关键挑战和机遇，包括对现有交通模式和拥堵的影响；安全分析和风险评估；潜在的经济和环境效益；以及为UAM和地面交通开发共享基础设施和路线。我们还讨论了UAM的潜在好处，如缩短旅行时间和改善服务不足地区的可达性。我们的调查

    Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey 
    
[^70]: RFold：基于解耦优化方法的RNA二级结构预测

    RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2212.14041](http://arxiv.org/abs/2212.14041)

    所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。

    

    核糖核酸（RNA）的二级结构比三级结构更稳定和更易于在细胞中访问，因此对于功能预测至关重要。尽管深度学习在这个领域中显示出了很好的结果，但当前的方法存在泛化性差和复杂性高的问题。在这项工作中，我们提出了一种简单而有效的RNA二级结构预测方法RFold。RFold引入了一种解耦优化的过程，将传统的约束满足问题分解为逐行和逐列优化，简化了求解过程，同时保证了输出的有效性。此外，RFold采用注意力地图作为信息表示，而不是设计手工特征。广泛的实验表明，RFold具有竞争性能，并且比现有最先进的方法具有约8倍的推理效率。代码和Colab演示可在\href{this http URL}{this http UR}上找到。

    The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
    
[^71]: 使用70万人日的可穿戴数据进行自监督学习的人体活动识别

    Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data. (arXiv:2206.02909v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2206.02909](http://arxiv.org/abs/2206.02909)

    本研究利用700,000人日的未标记可穿戴传感器数据，通过自监督学习技术，成功构建了一种能够在多个数据集上泛化且效果显著优于基线模型的人体活动识别模型，有望帮助研究人员和开发者开发高性能的可定制和泛化的活动分类器。

    

    由于缺乏大规模标记数据集，深度学习在人体活动识别方面的进展相对有限。在本研究中，我们利用自监督学习技术对UK-Biobank活动追踪器数据集进行了分析，这是迄今为止最大的数据集，包含超过70万人日的未标记可穿戴传感器数据。我们得到的活动识别模型在七个基准数据集上始终优于强基线模型，F1相对改善2.5%-100%（中位数18.4%），改进最大的是规模较小的数据集。与之前的研究不同的是，我们的结果可以泛化到外部数据集、设备和环境。我们的开源模型将帮助研究人员和开发者构建具有高性能的可定制和泛化的活动分类器。

    Advances in deep learning for human activity recognition have been relatively limited due to the lack of large labelled datasets. In this study, we leverage self-supervised learning techniques on the UK-Biobank activity tracker dataset--the largest of its kind to date--containing more than 700,000 person-days of unlabelled wearable sensor data. Our resulting activity recognition model consistently outperformed strong baselines across seven benchmark datasets, with an F1 relative improvement of 2.5%-100% (median 18.4%), the largest improvements occurring in the smaller datasets. In contrast to previous studies, our results generalise across external datasets, devices, and environments. Our open-source model will help researchers and developers to build customisable and generalisable activity classifiers with high performance.
    

