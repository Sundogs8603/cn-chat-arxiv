# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Simplicial Models for the Epistemic Logic of Faulty Agents.](http://arxiv.org/abs/2311.01351) | 本文研究了故障代理的认知逻辑，并提出了基于单纯模型的新模型，该模型允许世界中参与的代理数量可以变化。这对于容错分布式计算非常有用。 |
| [^2] | [AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models.](http://arxiv.org/abs/2311.01305) | AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。 |
| [^3] | [Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling.](http://arxiv.org/abs/2311.00797) | 该论文通过机器学习辅助的数据驱动建模方法，研究了自适应易感-感染-易感流行病学网络的临界转折点集体动力学。他们识别出了一个有效的随机微分方程以描述网络的演化行为，并观察到了罕见的大幅度集体振荡现象。 |
| [^4] | [MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows.](http://arxiv.org/abs/2311.00334) | MetisFL是一种可扩展和高效的联邦学习系统，重点关注联邦控制器的可扩展性和优化。 |
| [^5] | [OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization.](http://arxiv.org/abs/2310.18122) | 本文提出了一个新的数据集OpinSummEval，对意见摘要进行自动化评估的可靠性进行重新评估。研究发现基于神经网络的度量通常优于非神经网络的度量，但即使是基于强大模型构建的度量也不能在所有维度上始终保持良好的相关性，突出了对意见摘要自动化评估方法的进一步改进的需求。 |
| [^6] | [Managing AI Risks in an Era of Rapid Progress.](http://arxiv.org/abs/2310.17688) | 在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。 |
| [^7] | [Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph.](http://arxiv.org/abs/2310.16452) | 本文提出了一个名为PEARLM的方法，通过语言建模开展基于路径的知识图谱推荐，解决了现有方法中对预训练知识图谱嵌入的依赖以及未充分利用实体和关系之间相互依赖性的问题，还避免了生成不准确的解释。实验结果表明，与现有方法相比，我们的方法效果显著。 |
| [^8] | [Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets.](http://arxiv.org/abs/2310.11715) | 通过利用粗粒度数据集，提出了一种细粒度命名实体识别模型，使用细粒度-粗粒度映射矩阵来显式利用层次结构，并提出了一种不一致性过滤方法，以增强低资源细粒度命名实体识别。 |
| [^9] | [PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection.](http://arxiv.org/abs/2310.11676) | PREM是一种简单而有效的节点级图异常检测方法，它通过简化图异常检测的过程，减少了时间和内存消耗，同时保持了强大的异常检测能力。 |
| [^10] | [Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning.](http://arxiv.org/abs/2310.11670) | 基于原型的超适配器（PHA）框架用于样本高效多任务调整，通过引入实例密集的检索器和样本高效的原型超网络生成条件模块，在多任务学习和少样本迁移学习中取得了可比性能的提升，甚至在数据量较小时也能超过其他强基线方法的性能。 |
| [^11] | [Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations.](http://arxiv.org/abs/2310.10705) | 本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。 |
| [^12] | [Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach.](http://arxiv.org/abs/2310.08660) | 本研究提出了一种批量约束的离策略方法，用于解决联合波束成形的参数优化问题。通过使用深度强化学习技术，克服了传统方法中计算复杂度高和无法准确建模的难题。 |
| [^13] | [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains.](http://arxiv.org/abs/2310.07838) | 本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。 |
| [^14] | [Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization.](http://arxiv.org/abs/2310.05317) | 该论文提出了一种任务自适应分词的方法，通过优化分词过程来增强在心理健康领域中的长文本生成。实验证明，该方法在减少标记数量的情况下显著提高了生成性能，并且可与大型语言模型结合使用。 |
| [^15] | [Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models.](http://arxiv.org/abs/2310.03059) | Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。 |
| [^16] | [De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics.](http://arxiv.org/abs/2310.00023) | 本研究提出了De-SaTE方法，通过利用多个去噪模块以及自注意力变换编码器，准确预测锂离子电池的剩余寿命（RUL），为预防性维护和预测性分析提供关键指标估计。 |
| [^17] | [Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World.](http://arxiv.org/abs/2309.10987) | 本文介绍了SpikingNeRF，它通过将辐射光线与脉冲神经网络的时间维度对齐，以节省能量并减少计算量。 |
| [^18] | [DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving.](http://arxiv.org/abs/2308.15991) | 本文提出了一种基于深度强化学习的轨迹跟踪方法，适用于自主驾驶系统中的运动相关模块。该方法通过DL的表示学习能力和RL的探索性质，提高了轨迹跟踪的准确性和稳健性，在真实系统中具有较好的适应性和效果。 |
| [^19] | [Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets.](http://arxiv.org/abs/2308.13488) | 本文提出了一种新的时空不确定性度量作为动态质量控制（dQC）工具，用于DNN分割自由呼吸DCE-CMRI数据集，并建立了人在回路框架来改善分割结果。 |
| [^20] | [Time Travel in LLMs: Tracing Data Contamination in Large Language Models.](http://arxiv.org/abs/2308.08493) | 该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。 |
| [^21] | [Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic.](http://arxiv.org/abs/2308.07336) | 本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。 |
| [^22] | [Developmental Bootstrapping of AIs.](http://arxiv.org/abs/2308.04586) | 传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。 |
| [^23] | [Anytime Model Selection in Linear Bandits.](http://arxiv.org/abs/2307.12897) | 该论文提出了一种在线性赌博机中进行任意模型选择的方法，通过模拟全信息反馈实现在遗憾方面具有指数改进的性能，并且不依赖时间界限和纯探索阶段。 |
| [^24] | [Tackling the Curse of Dimensionality with Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.12306) | 本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。 |
| [^25] | [The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models.](http://arxiv.org/abs/2307.12166) | 本论文研究了区分人类和AI生成的文本的任务，在不同体裁下进行了比较研究，提出了一个新的数据集，并采用多种机器学习模型进行分类。结果表明这些模型对于区分人类和AI生成的文本具有很高的效力，尽管在区分GPT生成的文本方面存在一定挑战。 |
| [^26] | [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus.](http://arxiv.org/abs/2307.11760) | EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。 |
| [^27] | [A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset.](http://arxiv.org/abs/2307.10455) | 提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。 |
| [^28] | [FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning.](http://arxiv.org/abs/2307.10317) | FedBug是一个自底向上逐渐解冻的联邦学习框架，通过冻结和逐渐解冻模型层，实现了一种有效缓解客户端漂移现象的方法。 |
| [^29] | [No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models.](http://arxiv.org/abs/2307.06440) | 本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。 |
| [^30] | [A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms.](http://arxiv.org/abs/2307.01231) | 本研究重新评估了(深度)学习匹配算法的基准数据集，发现其中大多数数据集都属于相对简单的分类任务。 |
| [^31] | [milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing.](http://arxiv.org/abs/2306.17010) | milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。 |
| [^32] | [OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection.](http://arxiv.org/abs/2306.16045) | OpenNDD是一个用于神经发育障碍检测的开放性识别框架，结合了自动编码器和对抗循环点开放性识别技术，能准确识别已知类别并识别未遇到的类别。 |
| [^33] | [Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset.](http://arxiv.org/abs/2306.13948) | 该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。 |
| [^34] | [CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation.](http://arxiv.org/abs/2306.13761) | 本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。 |
| [^35] | [Learning to Generate Better Than Your LLM.](http://arxiv.org/abs/2306.11816) | 本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。 |
| [^36] | [Taming Diffusion Models for Music-driven Conducting Motion Generation.](http://arxiv.org/abs/2306.10065) | 本文提出了Diffusion-Conductor，一种基于DDIM的方法，用于音乐驱动的指挥运动生成，利用扩散模型整合到一个两阶段学习框架中，并应用随机屏蔽策略和一对几何损失函数来提高运动多样性和鲁棒性。 |
| [^37] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^38] | [Data Augmentation Approaches for Source Code Models: A Survey.](http://arxiv.org/abs/2305.19915) | 本文对源代码的数据增强技术进行了全面的调查和综述，介绍了它们的分类法、优化策略和性能结果，并讨论了未来方向和研究挑战。 |
| [^39] | [Bigger, Better, Faster: Human-level Atari with human-level efficiency.](http://arxiv.org/abs/2305.19452) | 引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。 |
| [^40] | [Visualization for Recommendation Explainability: A Survey and New Perspectives.](http://arxiv.org/abs/2305.11755) | 本文回顾了推荐系统中有关可视化解释的研究，提出了一组可能有益于设计解释性可视化推荐系统的指南。 |
| [^41] | [Uncertainty-Aware Workload Prediction in Cloud Computing.](http://arxiv.org/abs/2303.13525) | 本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。 |
| [^42] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^43] | [Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning.](http://arxiv.org/abs/2303.10180) | 本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。 |
| [^44] | [Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning.](http://arxiv.org/abs/2303.09447) | 本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。 |
| [^45] | [Byte Pair Encoding for Symbolic Music.](http://arxiv.org/abs/2301.11975) | 本文介绍了一种在符号音乐领域中使用的字节对编码技术，它可以显著减小序列长度并增加词汇量，从而提高语言模型的嵌入能力。 |
| [^46] | [Semantics-Empowered Communication: A Tutorial-cum-Survey.](http://arxiv.org/abs/2212.08487) | 本文提供了基于语义的沟通方面的教程兼综述，回顾了文献，介绍了SemCom生态系统，并将研究方向进行了分类。此外，还提供了启用技术的分类和未来应用场景的展望。 |
| [^47] | [Welfare and Fairness in Multi-objective Reinforcement Learning.](http://arxiv.org/abs/2212.01382) | 本论文研究了多目标强化学习中的福利和公平性问题，提出了一种基于非线性福利函数的Q-learning算法，通过非线性标量化学习更新和非稳态动作选择来优化策略。算法被证明是可收敛的。 |
| [^48] | [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations.](http://arxiv.org/abs/2211.13308) | SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。 |
| [^49] | [Multi-Head Adapter Routing for Cross-Task Generalization.](http://arxiv.org/abs/2211.03831) | 本文研究了跨任务泛化中适配器路由的作用，并设计了基于此的新变体Multi-Head Routing (MHR)，通过精细的路由和参数高效的微调实现了更好的性能和更高的参数效率。 |
| [^50] | [Non-contrastive representation learning for intervals from well logs.](http://arxiv.org/abs/2209.14750) | 本文提出了一种新的方法来处理井测数据表示学习问题，采用自我监督学习的方法进行非对比度的表示学习，减少对数据的标注需求，并提高了算法性能。 |
| [^51] | [HyperMixer: An MLP-based Low Cost Alternative to Transformers.](http://arxiv.org/abs/2203.03691) | HyperMixer是一种低成本的基于MLP的Transformer替代方案，通过动态形成标记混合MLP来实现自然语言理解，其性能比替代方案好，并可与Transformer媲美，成本更低。 |

# 详细

[^1]: 故障代理的认知逻辑的单纯模型

    Simplicial Models for the Epistemic Logic of Faulty Agents. (arXiv:2311.01351v1 [cs.LO])

    [http://arxiv.org/abs/2311.01351](http://arxiv.org/abs/2311.01351)

    本文研究了故障代理的认知逻辑，并提出了基于单纯模型的新模型，该模型允许世界中参与的代理数量可以变化。这对于容错分布式计算非常有用。

    

    近年来，一些作者一直在研究单纯模型，这是基于称为单纯复合体的高维结构的认知逻辑模型。在最初的形式中，单纯模型始终被假设为纯粹的，意味着所有世界具有相同的维度。这相当于基于克里普克模型的标准S5n语义的认知逻辑。通过移除模型必须是纯粹的假设，我们可以超越常规的克里普克语义，并研究参与一个世界的代理数量可以变化的认知逻辑。这种方法已在许多论文中得到应用，其中应用于容错分布式计算，其中在系统执行期间可能会发生过程崩溃。一个问题是，在定义不纯的单纯模型时，微妙的设计选择可能会导致所得到的逻辑的不同公理。在本文中，我们系统地对这些设计选择进行分类，并公理化相应的认知逻辑。

    In recent years, several authors have been investigating simplicial models, a model of epistemic logic based on higher-dimensional structures called simplicial complexes. In the original formulation, simplicial models were always assumed to be pure, meaning that all worlds have the same dimension. This is equivalent to the standard S5n semantics of epistemic logic, based on Kripke models. By removing the assumption that models must be pure, we can go beyond the usual Kripke semantics and study epistemic logics where the number of agents participating in a world can vary. This approach has been developed in a number of papers, with applications in fault-tolerant distributed computing where processes may crash during the execution of a system. A difficulty that arises is that subtle design choices in the definition of impure simplicial models can result in different axioms of the resulting logic. In this paper, we classify those design choices systematically, and axiomatize the correspon
    
[^2]: AWEQ：用于大型语言模型的后训练量化和激活权重均衡方法

    AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])

    [http://arxiv.org/abs/2311.01305](http://arxiv.org/abs/2311.01305)

    AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。

    

    大型语言模型(LLMs)在各种任务中表现出色，但其计算和存储成本也相对较高。量化这些模型是缓解这个问题的有效方法。然而，现有方法很难在模型准确性和硬件效率之间取得平衡。因此，我们引入了AWEQ，一种后训练方法，不需要额外的训练开销。AWEQ在超低位量化和8-bit权重和激活(W8A8)量化方面表现出色。观察到权重量化比激活量化更容易。AWEQ通过通道均衡将激活量化的难度转移到权重上，实现了两者量化困难的平衡，从而最大化了性能。我们进一步改进了均衡方法，减小了量化偏差误差，确保模型的鲁棒性。在像LLaMA这样的流行模型上进行了大量实验。

    Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
    
[^3]: 演化流行病学网络的临界转折点：机器学习辅助的数据驱动有效建模

    Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])

    [http://arxiv.org/abs/2311.00797](http://arxiv.org/abs/2311.00797)

    该论文通过机器学习辅助的数据驱动建模方法，研究了自适应易感-感染-易感流行病学网络的临界转折点集体动力学。他们识别出了一个有效的随机微分方程以描述网络的演化行为，并观察到了罕见的大幅度集体振荡现象。

    

    我们通过数据驱动的、机器学习辅助的方式研究自适应易感-感染-易感(SIS)流行病学网络的临界转折点集体动力学。我们通过受数值随机积分器启发的深度学习ResNet架构，识别出一个参数相关的基于物理意义的粗粒度均场变量的有效随机微分方程(eSDE)。我们基于eSDE的确定偏移项构建了一个近似的有效分岔图，并将其与均场SIS模型的分岔图进行对比。我们观察到演化网络的有效SIS动力学中的次临界Hopf分岔，它引起了临界转折行为；这表现为大幅度的集体振荡，它们从(噪声的)固定状态的邻域中自发地、罕见地出现。我们通过重复的暴力模拟和使用已建立的数学工具研究了这些罕见事件的统计特性。

    We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathemati
    
[^4]: MetisFL:一种可扩展和高效的联邦学习工作流的尴尬并行控制器

    MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])

    [http://arxiv.org/abs/2311.00334](http://arxiv.org/abs/2311.00334)

    MetisFL是一种可扩展和高效的联邦学习系统，重点关注联邦控制器的可扩展性和优化。

    

    联邦学习(FL)系统通常由两个核心处理实体组成:联邦控制器和学习器。控制器负责管理在学习器之间执行FL工作流程，学习器负责在其私有数据集上训练和评估联邦模型。在执行FL工作流时，FL系统对参与学习器的计算资源或数据没有控制。尽管最近提出了许多FL系统来促进FL工作流的开发，但这些系统中大多数忽视了控制器的可扩展性。为了满足这一需求，我们设计和开发了一种名为MetisFL的新型FL系统，其中联邦控制器是第一等公民。

    A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations cond
    
[^5]: OpinSummEval:再考自动化评估在意见摘要中的应用

    OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])

    [http://arxiv.org/abs/2310.18122](http://arxiv.org/abs/2310.18122)

    本文提出了一个新的数据集OpinSummEval，对意见摘要进行自动化评估的可靠性进行重新评估。研究发现基于神经网络的度量通常优于非神经网络的度量，但即使是基于强大模型构建的度量也不能在所有维度上始终保持良好的相关性，突出了对意见摘要自动化评估方法的进一步改进的需求。

    

    与其他类型的摘要任务不同，意见摘要专注于观点和情感，因此与众不同。虽然像ROUGE这样的某些自动化评估方法很受欢迎，但我们发现它们对评估意见摘要的质量是不可靠的。在本文中，我们提出了一个数据集OpinSummEval，它包括来自14个意见摘要模型的人工判断和输出。我们进一步探讨了24个自动度量与人工评分之间的相关性，涵盖了四个维度。我们的研究结果表明，基于神经网络的度量通常优于非神经网络的度量。然而，即使是基于强大模型（如BART和GPT-3/3.5）构建的度量也不能在所有维度上始终保持良好的相关性，突出了需要改进意见摘要的自动化评估方法的需求。代码和数据公开可用于https://github.com/A-Chicharito-S/OpinSummEval/tree/main。

    Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
    
[^6]: 在快速发展时代管理人工智能风险

    Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)

    [http://arxiv.org/abs/2310.17688](http://arxiv.org/abs/2310.17688)

    在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。

    

    在这篇简短的共识文中，我们概述了即将到来的先进人工智能系统所带来的风险。我们审查了大规模的社会危害和恶意使用，以及人类对自主人工智能系统失去控制的不可逆转的损失。鉴于人工智能的快速和持续进展，我们提出了人工智能研发和治理的优先事项。

    In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&D and governance.
    
[^7]: 可解释的基于路径的知识图推荐中的忠实路径语言建模

    Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])

    [http://arxiv.org/abs/2310.16452](http://arxiv.org/abs/2310.16452)

    本文提出了一个名为PEARLM的方法，通过语言建模开展基于路径的知识图谱推荐，解决了现有方法中对预训练知识图谱嵌入的依赖以及未充分利用实体和关系之间相互依赖性的问题，还避免了生成不准确的解释。实验结果表明，与现有方法相比，我们的方法效果显著。

    

    针对知识图谱中的路径推理方法在提高推荐系统透明度方面的潜力，本文提出了一种名为PEARLM的新方法，该方法通过语言建模有效捕获用户行为和产品端知识。我们的方法通过语言模型直接从知识图谱上的路径中学习知识图谱嵌入，并将实体和关系统一在同一优化空间中。序列解码的约束保证了路径对知识图谱的忠实性。在两个数据集上的实验证明了我们方法与现有最先进方法的有效性。

    Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
    
[^8]: 利用粗粒度数据集增强低资源细粒度命名实体识别

    Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])

    [http://arxiv.org/abs/2310.11715](http://arxiv.org/abs/2310.11715)

    通过利用粗粒度数据集，提出了一种细粒度命名实体识别模型，使用细粒度-粗粒度映射矩阵来显式利用层次结构，并提出了一种不一致性过滤方法，以增强低资源细粒度命名实体识别。

    

    命名实体识别（NER）在细粒度NER场景下常常面临标注数据不足的问题。虽然可以应用K-shot学习技术，但当注释数量超过几十个标签时，性能往往达到饱和。为了解决这个问题，我们利用现有的粗粒度数据集，其中包含大量的标注。一种直接解决这个问题的方法是预训练，它利用粗粒度数据进行表示学习。然而，它无法直接利用细粒度和粗粒度实体之间的关系，尽管细粒度实体类型很可能是粗粒度实体类型的子类别。我们提出了一个带有细粒度-粗粒度（F2C）映射矩阵的细粒度NER模型，以显式地利用层次结构。此外，我们提出了一种不一致性过滤方法，以消除与细粒度不一致的粗粒度实体。

    Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-gr
    
[^9]: PREM:一种简单而有效的节点级图异常检测方法。

    PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])

    [http://arxiv.org/abs/2310.11676](http://arxiv.org/abs/2310.11676)

    PREM是一种简单而有效的节点级图异常检测方法，它通过简化图异常检测的过程，减少了时间和内存消耗，同时保持了强大的异常检测能力。

    

    节点级图异常检测在识别医学、社交网络和电子商务等各个领域中的图结构数据中的异常节点起着关键作用。然而，由于异常的多样性以及标注数据的匮乏，已有的基于重构和对比学习的方法往往在效率方面存在问题，这源于它们复杂的目标和繁琐的模块。为了提高图异常检测的效率，我们引入了一种简单的方法，称为PREprocessing and Matching（简称PREM）。我们的方法简化了图异常检测，减少了时间和内存的消耗，同时保持了强大的异常检测能力。PREM由两个模块组成：预处理模块和邻居匹配模块。PREM在训练过程中消除了消息传递传播的必要性，并采用了简单的对比损失函数，从而大大减少了训练时间和内存使用量。此外，

    Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
    
[^10]: 基于原型的超适配器用于样本高效多任务调整

    Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])

    [http://arxiv.org/abs/2310.11670](http://arxiv.org/abs/2310.11670)

    基于原型的超适配器（PHA）框架用于样本高效多任务调整，通过引入实例密集的检索器和样本高效的原型超网络生成条件模块，在多任务学习和少样本迁移学习中取得了可比性能的提升，甚至在数据量较小时也能超过其他强基线方法的性能。

    

    参数高效微调（PEFT）已经证明在适应预训练语言模型到下游任务时有效，同时只更新了少量参数。尽管取得了成功，大多数现有方法独立地适应每个任务，没有考虑任务之间的知识传输，并且受限于低数据情景。为了克服这个问题，我们提出了一种基于原型的超适配器（PHA）框架，该框架建立在适配器调整和超网络基础上。它引入了一个实例密集的检索器和一个样本高效的原型超网络来生成条件模块。这导致与现有PEFT方法在多任务学习和少样本迁移学习上相当的性能改进。更重要的是，当可用数据量变小时，我们的方法比其他强基线方法有很大的优势。基于我们在各种数据集上的广泛实证实验，我们证明了PHA在权衡方面取得了更好的结果。

    Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
    
[^11]: 用于识别半导体晶圆地图中缺陷模式的机器学习技术：一项调查、实证和实验评估

    Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])

    [http://arxiv.org/abs/2310.10705](http://arxiv.org/abs/2310.10705)

    本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。

    

    本文综述了利用机器学习（ML）技术识别半导体制造中晶圆缺陷的方法学。尽管越来越多的研究证明了ML在晶圆缺陷识别中的有效性，但在这个主题上缺乏全面的综述。本文试图弥补这个空白，通过整合现有文献，深入分析各种ML算法在晶圆缺陷检测领域的优势、局限性和潜在应用。我们提出了一种创新的方法学分类体系，详细分类了算法，并提供了更细致的子技术划分。这个分类体系从广泛的方法学类别开始，到具体的子技术结束。它帮助研究人员理解不同算法以及它们的技术之间的复杂关系。我们采用严谨的实证和实验评估来验证算法性能。

    This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
    
[^12]: 在不探索的情况下学习联合波束成形的RL策略：一种批量约束的离策略方法

    Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v1 [cs.LG])

    [http://arxiv.org/abs/2310.08660](http://arxiv.org/abs/2310.08660)

    本研究提出了一种批量约束的离策略方法，用于解决联合波束成形的参数优化问题。通过使用深度强化学习技术，克服了传统方法中计算复杂度高和无法准确建模的难题。

    

    在这个项目中，我们考虑了网络参数优化的问题，以实现速率最大化。我们将其构建为功率控制、波束成形和干扰消除的联合优化问题。我们考虑了多个基站与多个用户设备通信的情况。由于穷举搜索的指数计算复杂度，我们使用深度强化学习（RL）技术来解决该非凸优化问题。现代通信系统以其难以准确建模的行为而臭名昭著。这限制了我们使用基于RL的算法，因为需要与环境进行交互以便代理能够高效地探索和学习。此外，由于失败的高成本，将算法部署在现实世界中进行探索和学习是不明智的。与先前提出的基于RL的解决方案（如基于深度Q网络（DQN）的控制）相比，我们提出采用一种离策略方法

    In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an off
    
[^13]: 探索有限领域知识传递的基本限制

    Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])

    [http://arxiv.org/abs/2310.07838](http://arxiv.org/abs/2310.07838)

    本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。

    

    我们对通过从教师到概率化学生分类器的n个样本进行知识传递的统计效率进行了表征，其中输入空间S和标签A为有限域。我们发现，在三个渐进级别上的特权信息可以加快传递的速度。在第一级别上，只有具有困难标签的样本是已知的，最大似然估计器能够达到最小化速率sqrt(|S||A|/n)。第二级别上，除了已知的困难标签样本外，还有采样标签的教师概率可用，这将收敛速度的下界提高到|S||A|/n。然而，在第二个数据采集协议下，最小化交叉熵损失的朴素适应会导致渐近偏差的学生。我们克服了这个限制，并通过使用一种新颖的经验变体的平方误差逻辑损失来实现了基本限制。第三级别进一步赋予学生软标签。

    We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
    
[^14]: 在心理健康领域中通过任务自适应分词来增强长文本生成

    Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05317](http://arxiv.org/abs/2310.05317)

    该论文提出了一种任务自适应分词的方法，通过优化分词过程来增强在心理健康领域中的长文本生成。实验证明，该方法在减少标记数量的情况下显著提高了生成性能，并且可与大型语言模型结合使用。

    

    我们提出了任务自适应分词作为一种方式，将生成流水线适应于下游任务的特定要求，并增强在心理健康领域的长文本生成。受认知科学的启发，我们的任务自适应分词器从多个结果中采样可变的分段，采样概率基于任务特定的数据进行优化。我们引入了一种构建专用词汇的策略，并介绍了一种词汇合并协议，可以将任务特定的标记整合到预训练模型的分词步骤中。通过对中英文心理问答任务进行广泛实验，我们发现我们的任务自适应分词方法在使用更少的标记的情况下带来了显著的生成性能提升，最高可达60%。初步实验表明，使用我们的分词方法与非常大的语言模型结合能够得到有希望的结果。

    We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
    
[^15]: Point-PEFT: 用于3D预训练模型的参数高效微调

    Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])

    [http://arxiv.org/abs/2310.03059](http://arxiv.org/abs/2310.03059)

    Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。

    

    大型预训练模型的流行已经彻底改变了语言、视觉和多模态等领域的下游任务。为了降低下游任务的适应成本，许多参数高效微调（PEFT）技术被提出用于语言和2D图像预训练模型。然而，对于3D预训练模型的专门PEFT方法仍未得到充分探索。为此，我们引入了Point-PEFT，一种用于适应点云预训练模型的新型框架，其具有最少的可学习参数。具体而言，对于预训练的3D模型，我们冻结大部分参数，只微调新增的PEFT模块。这些模块包括Point-prior Prompt和Geometry-aware Adapter。Point-prior Prompt采用一组可学习的提示标记，并提出使用具有领域特定知识的内存库来增强提示标记的参数无关的注意力机制。Geometry-aware Adapter旨在对不同任务或数据进行准确地聚合。

    The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
    
[^16]: De-SaTE：用于锂离子电池健康预测的去噪自注意力变换编码器

    De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v1 [cs.LG])

    [http://arxiv.org/abs/2310.00023](http://arxiv.org/abs/2310.00023)

    本研究提出了De-SaTE方法，通过利用多个去噪模块以及自注意力变换编码器，准确预测锂离子电池的剩余寿命（RUL），为预防性维护和预测性分析提供关键指标估计。

    

    锂离子电池在各个行业中得到了广泛应用，从为便携式电子设备供电到推动电动汽车和支持能源存储系统。有效管理锂离子电池的一个核心挑战是准确预测其剩余寿命（RUL），这是预防性维护和预测性分析的关键指标。本研究提出了一种新的方法，利用多个去噪模块的能量，每个模块都经过训练来处理电池数据中常见的噪声类型。具体而言，我们使用去噪自动编码器和小波去噪器来生成编码/分解表示，然后将其通过专用自注意力变换编码器进行处理。在NASA和CALCE数据集上进行了大量实验后，我们能够表征多种噪声模式下的广泛健康指标估计。我们发现我们报告的误差

    Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error
    
[^17]: Spiking NeRF：使生物启发的神经网络穿透现实世界

    Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])

    [http://arxiv.org/abs/2309.10987](http://arxiv.org/abs/2309.10987)

    本文介绍了SpikingNeRF，它通过将辐射光线与脉冲神经网络的时间维度对齐，以节省能量并减少计算量。

    

    脉冲神经网络（SNN）在许多任务中取得了成功，利用其具有潜在生物学可行性的能量效率和潜力。与此同时，神经辐射场（NeRF）以大量能量消耗渲染高质量的3D场景，但很少有研究深入探索以生物启发的方法进行节能解决方案。本文提出了脉冲NeRF（SpikingNeRF），将辐射光线与SNN的时间维度对齐，以自然地适应SNN对辐射场的重建。因此，计算以基于脉冲、无乘法的方式进行，从而减少能量消耗。在SpikingNeRF中，光线上的每个采样点匹配到特定的时间步，并以混合方式表示，其中体素网格也得到维护。基于体素网格，确定采样点是否在训练和推断过程中被屏蔽以进行更好的处理。然而，这个操作也会产生不可逆性。

    Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
    
[^18]: 基于深度强化学习的自主驾驶中运动相关模块的轨迹跟踪

    DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving. (arXiv:2308.15991v1 [cs.RO])

    [http://arxiv.org/abs/2308.15991](http://arxiv.org/abs/2308.15991)

    本文提出了一种基于深度强化学习的轨迹跟踪方法，适用于自主驾驶系统中的运动相关模块。该方法通过DL的表示学习能力和RL的探索性质，提高了轨迹跟踪的准确性和稳健性，在真实系统中具有较好的适应性和效果。

    

    自主驾驶系统总是建立在运动相关模块（如规划器和控制器）之上。准确而稳健的轨迹跟踪方法对于这些运动相关模块来说是不可或缺的原始例程。当前的方法往往对模型（如上下文和动力学）做出了强烈的假设，这些假设不足以应对真实系统中的场景变化。在本文中，我们提出了一种基于深度强化学习（DRL）的自主驾驶系统中运动相关模块的轨迹跟踪方法。DL的表示学习能力和RL的探索性质既带来了强健性又提高了准确性。与此同时，它通过以无模型和数据驱动的方式运行轨迹跟踪来增强了通用性。通过大量实验，我们证明了我们的方法相比当前方法在效率和效果上的优势。

    Autonomous driving systems are always built on motion-related modules such as the planner and the controller. An accurate and robust trajectory tracking method is indispensable for these motion-related modules as a primitive routine. Current methods often make strong assumptions about the model such as the context and the dynamics, which are not robust enough to deal with the changing scenarios in a real-world system. In this paper, we propose a Deep Reinforcement Learning (DRL)-based trajectory tracking method for the motion-related modules in autonomous driving systems. The representation learning ability of DL and the exploration nature of RL bring strong robustness and improve accuracy. Meanwhile, it enhances versatility by running the trajectory tracking in a model-free and data-driven manner. Through extensive experiments, we demonstrate both the efficiency and effectiveness of our method compared to current methods.
    
[^19]: 临时不确定性定位以实现人在回路分析动态增强心脏磁共振成像数据集

    Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets. (arXiv:2308.13488v1 [eess.IV])

    [http://arxiv.org/abs/2308.13488](http://arxiv.org/abs/2308.13488)

    本文提出了一种新的时空不确定性度量作为动态质量控制（dQC）工具，用于DNN分割自由呼吸DCE-CMRI数据集，并建立了人在回路框架来改善分割结果。

    

    动态增强心脏磁共振成像（DCE-CMRI）是一种广泛用于诊断心肌血流（灌注）异常的模式。在典型的自由呼吸DCE-CMRI扫描中，会在各种对比度“进入/离开”阶段获取接近300幅心肌灌注的时序图像。对于每一帧DCE图像序列中心肌轮廓的手动分割可能是繁琐和耗时的，特别是当非刚性运动校正失败或不可用时。虽然深度神经网络（DNN）显示出对DCE-CMRI数据集进行分析的潜力，但缺乏一种可靠检测失败分割的“动态质量控制”（dQC）技术。在本文中，我们提出了一种新的时空不确定性度量作为基于DNN的自由呼吸DCE-CMRI数据集分割的dQC工具，并通过在外部数据集上验证所提出的度量，并建立一个人在回路框架来改善分割结果。

    Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is a widely used modality for diagnosing myocardial blood flow (perfusion) abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300 time-resolved images of myocardial perfusion are acquired at various contrast "wash in/out" phases. Manual segmentation of myocardial contours in each time-frame of a DCE image series can be tedious and time-consuming, particularly when non-rigid motion correction has failed or is unavailable. While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI datasets, a "dynamic quality control" (dQC) technique for reliably detecting failed segmentations is lacking. Here we propose a new space-time uncertainty metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI datasets by validating the proposed metric on an external dataset and establishing a human-in-the-loop framework to improve the segmentation results. In the proposed approach,
    
[^20]: LLM中的时间旅行：追踪大型语言模型中的数据污染

    Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])

    [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493)

    该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。

    

    数据污染是指大型语言模型（LLMs）的训练数据中存在来自下游任务的测试数据，这可能是理解LLMs在其他任务上有效性的一个重要问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的方法核心是通过识别从小的随机样本中抽取的单个实例中的潜在污染，然后评估整个数据集分区是否受到污染。为了估计单个实例的污染程度，我们使用了“引导指令”：即一个由数据集名称、分区类型和参考实例的初始部分组成的提示，要求LLM完成它。如果LLM的输出与参考实例的后一部分完全或接近匹配，那么该实例被标记为受到污染。为了了解整个分区是否受到污染，我们提出了两个想法。第一个想法是标记一个数据集的分区，该分区中的实例大多数都被判断为受到污染。

    Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
    
[^21]: 从合成语料库和形式逻辑学习演绎推理

    Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])

    [http://arxiv.org/abs/2308.07336](http://arxiv.org/abs/2308.07336)

    本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。

    

    我们研究了一种从合成语料库中学习演绎推理能力的语言模型（LMs）方法。之前的研究使用了具体的演绎规则来生成演绎示例，但这些规则受限或者是任意的。这可能限制了所获得演绎推理能力的泛化能力。我们重新思考并采用基于形式逻辑理论的一组良好基础的演绎规则，当这些规则以多步方式组合时，可以推导出任何其他演绎规则。我们通过实验证明，在提出的语料库上训练的LMs，即$\textbf{FLD}$（$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction），获得了更具泛化性的演绎推理能力。此外，我们确定了演绎推理语料库可以增强LMs的推理能力的方面，以及不同方面无法增强的方面。最后，基于这些结果，我们讨论了将演绎语料库或其他方法应用于每个方面的未来方向。

    We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
    
[^22]: AIs的发展脱靴法

    Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])

    [http://arxiv.org/abs/2308.04586](http://arxiv.org/abs/2308.04586)

    传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。

    

    尽管当前一些AI在封闭的世界，如棋盘游戏中超越了人类能力，但它们在混乱的现实世界中的表现有限。它们会犯奇怪的错误而且没有意识到。它们很难受到指导，不能运用常识，缺乏好奇心。它们不能成为良好的合作者。传统手动构建的符号AI方法构建的系统和使用生成和深度学习AI方法(包括大规模语言模型)构建的系统都无法应对这些挑战。它们不适合创建强大和可信赖的AI。尽管此方法不属于主流的AI方法，但发展脱靴法显示出希望。在发展脱靴法中，AI像人类儿童一样发展能力。它们从先天能力开始。像人类一样，它们与环境互动，并从互动中学习。它们通过自我发展的能力逐步扩展先天能力。它们互动并逐渐将所学应用于实际操作。

    Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
    
[^23]: 线性赌博机中的任意模型选择

    Anytime Model Selection in Linear Bandits. (arXiv:2307.12897v1 [stat.ML])

    [http://arxiv.org/abs/2307.12897](http://arxiv.org/abs/2307.12897)

    该论文提出了一种在线性赌博机中进行任意模型选择的方法，通过模拟全信息反馈实现在遗憾方面具有指数改进的性能，并且不依赖时间界限和纯探索阶段。

    

    在赌博优化中，模型选择是一个具有挑战性的问题，因为它不仅需要在行动选择方面平衡探索和开发，还需要在模型选择方面平衡探索和开发。一种自然的方法是依赖于将不同模型视为专家的在线学习算法。然而，现有方法在遗憾方面与模型数量$M$的规模（$\text{poly}M$）呈不良的关系。我们的关键洞察是，在线性赌博机的模型选择中，我们可以通过有利的偏差-方差权衡来模拟全信息反馈给在线学习者。这使得我们能够开发出具有指数改进（$\log M$）在遗憾方面对$M$依赖性的ALEXP。ALEXP在遗憾方面具有任意保证，并且既不需要对时间界$n$具有知识，也不依赖于初始的纯探索阶段。我们的方法利用了Lasso的一种新颖的时间均匀分析，建立了在线学习和高维统计之间的新连接。

    Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
    
[^24]: 用物理信知的神经网络解决维度诅咒问题

    Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])

    [http://arxiv.org/abs/2307.12306](http://arxiv.org/abs/2307.12306)

    本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。

    

    维度诅咒(CoD)随着维度的增加，以指数级增长的计算成本来极度税费计算资源。这在解决高维偏微分方程(PDEs)中面临极大挑战，正如Richard Bellman在60年前首次指出的那样。尽管近年来在高维度上数值解决偏微分方程(PDEs)取得了一些成功，但这样的计算代价过高，而将一般非线性PDEs扩展到高维度从未实现过。本文提出了一种新方法，将物理信知的神经网络(PINNs)扩展到解决任意高维PDEs。该新方法称为随机维度梯度下降(SDGD)，将PDE的梯度分解为与不同维度对应的部分，并在训练PINNs的每次迭代中随机选择这些维度部分的子集进行采样。我们在理论上证明了所提出方法的收敛保证和其他期望属性。

    The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
    
[^25]: 模仿游戏：在大型语言模型时代检测人类和AI生成的文本

    The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models. (arXiv:2307.12166v1 [cs.CL])

    [http://arxiv.org/abs/2307.12166](http://arxiv.org/abs/2307.12166)

    本论文研究了区分人类和AI生成的文本的任务，在不同体裁下进行了比较研究，提出了一个新的数据集，并采用多种机器学习模型进行分类。结果表明这些模型对于区分人类和AI生成的文本具有很高的效力，尽管在区分GPT生成的文本方面存在一定挑战。

    

    基于人工智能的大型语言模型（LLM）具有革新教育、研究和实践的巨大潜力。然而，区分人类写作和AI生成的文本已经成为一项重要任务。本文介绍了一项比较研究，提出了一个新颖的数据集，包含不同体裁的人类写作和LLM生成的文本：论文、故事、诗歌和Python代码。我们采用了几种机器学习模型来对这些文本进行分类。结果表明，尽管数据集的样本数量有限，这些模型在区分人类和AI生成的文本方面表现出了很高的效力。然而，当分类GPT生成的文本时，任务变得更具挑战性，特别是在故事写作方面。结果表明，与更复杂的多类别任务相比，这些模型在二元分类任务（如区分人类生成文本和特定LLM）方面表现出了更优越的性能。

    The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionizing education, research, and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This paper presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry, and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the dataset's limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-ge
    
[^26]: EmotionPrompt: 通过情感刺激提升大型语言模型的关键心理学方法

    EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])

    [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)

    EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。

    

    大型语言模型（LLMs）在推理、语言理解和数学问题解决等许多领域取得了显著的性能，并被视为人工通用智能（AGI）的关键步骤。然而，LLMs对提示的敏感性仍然是其日常应用的主要瓶颈。本文从心理学中汲取灵感，提出了EmotionPrompt来探索情感智能以提升LLMs的性能。EmotionPrompt基于一个非常简单明了的原则：将情感刺激融入到提示中。实验结果表明，我们的方法在相同的单一提示模板上，与原始的零样本提示和Zero-shot-CoT相比，在8个任务上都显著优于多种模型：ChatGPT、Vicuna-13b、Bloom和T5。此外，观察到EmotionPrompt能够提高真实性和信息量。我们相信EmotionPrompt为探索跨学科知识开辟了一条新的道路。

    Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
    
[^27]: 朝着全球生物多样性评估迈出的一步：BIOSCAN-1M昆虫数据集

    A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])

    [http://arxiv.org/abs/2307.10455](http://arxiv.org/abs/2307.10455)

    提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。

    

    为了对昆虫生物多样性进行编目，我们提出了一个新的大型手工标记昆虫图像数据集，即BIOSCAN-Insect数据集。每个记录都由专家进行分类，并且具有相关的遗传信息，包括原始核苷酸条形码序列和分配的条形码索引号，这些是基于遗传的物种分类的代理。本文介绍了一个精选的百万图像数据集，主要用于训练能够提供基于图像的分类评估的计算机视觉模型，但该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。由于数据集固有的生物性质，展现出了具有长尾类别不平衡分布的特征。此外，分类标签是一个分层分类方案，在较低级别上呈现出高度细粒度的分类问题。除了激发对生物多样性研究的兴趣外，该数据集还促进了对机器学习的深入研究。

    In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
    
[^28]: FedBug: 一种自底向上逐渐解冻的联邦学习框架

    FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])

    [http://arxiv.org/abs/2307.10317](http://arxiv.org/abs/2307.10317)

    FedBug是一个自底向上逐渐解冻的联邦学习框架，通过冻结和逐渐解冻模型层，实现了一种有效缓解客户端漂移现象的方法。

    

    联邦学习（FL）提供了一种协作训练框架，允许多个客户端在不损害数据隐私的情况下为共享模型做出贡献。由于本地数据集的异构性，更新的客户端模型可能会过拟合并与彼此发散，这被称为客户端漂移问题。在本文中，我们提出了FedBug（具有自底向上逐渐解冻的联邦学习），这是一种新颖的FL框架，旨在有效地减轻客户端漂移。FedBug自适应地利用每个全局轮次服务器分发的客户端模型参数作为跨客户端对齐的参考点。具体而言，在客户端上，FedBug从冻结整个模型开始，然后逐渐解冻层，从输入层到输出层。这种自底向上的方法允许模型训练解冻的新层将数据投影到一个潜在空间中，在这个空间中，分离超平面在所有客户端上保持一致。我们在理论上分析了FedBug

    Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze F
    
[^29]: 没有训练就没有收益：重新审视基于Transformer的语言模型的高效训练算法

    No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])

    [http://arxiv.org/abs/2307.06440](http://arxiv.org/abs/2307.06440)

    本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。

    

    近年来，训练Transformer-based语言模型所需的计算量急剧增加。这一趋势促使研究者们开展了针对高效训练算法的研究，旨在比标准训练更快地改善训练、验证和下游性能。在这项工作中，我们重新审视了三类这样的算法：动态架构（层叠、层丢弃）、批量选择（选择性反向传播、RHO损失）和高效优化器（Lion、Sophia）。当使用这些方法在固定计算预算下对BERT和T5进行预训练时，我们发现它们的训练、验证和下游收益相对于一个具有完全衰减学习率的基线而言会消失。我们定义了一个评估协议，可以通过将所有计算时间映射到一个称为参考系统时间的参考机器上，在任意机器上进行计算。我们讨论了我们提出的协议的局限性，并发布了我们的代码，以鼓励对高效训练的严格研究。

    The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
    
[^30]: 对(深度)学习匹配算法的基准数据集的关键重新评估

    A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])

    [http://arxiv.org/abs/2307.01231](http://arxiv.org/abs/2307.01231)

    本研究重新评估了(深度)学习匹配算法的基准数据集，发现其中大多数数据集都属于相对简单的分类任务。

    

    实体解析(ER)是识别在一个或多个数据库中指向相同实体的记录的过程。多年来，已经开发了许多技术来解决ER挑战，近年来，机器学习和深度学习方法在匹配阶段受到了重视。然而，在文献中尚未对实验评估中常用的学习匹配算法的基准数据集的质量进行检查。为了弥补这个空白，我们提出了四种不同的方法来评估13个已建立数据集的难度和适用性：两种理论方法，涉及新的线性度量和现有的复杂度度量，以及两种实际方法：最佳非线性和线性匹配器之间的差异，以及最佳学习匹配器和完美预测器之间的差异。我们的分析表明，大多数流行数据集都提出了相当简单的分类任务。

    Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a
    
[^31]: milliFlow：用于人体运动感知的毫米波雷达点云场景流估计

    milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])

    [http://arxiv.org/abs/2306.17010](http://arxiv.org/abs/2306.17010)

    milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。

    

    随着普适计算时代的到来，人体运动感知在智能系统中起着关键作用，用于决策、用户交互和个性化服务。在传统方法中，人体跟踪、姿势估计、手势识别和活动识别等方面进行了大量研究，这些方法主要基于摄像机。然而，摄像机的侵入性特点限制了它们在智能家居应用中的使用。为了解决这个问题，毫米波雷达由于其保护隐私的特点而受到欢迎。在这项工作中，我们提出了一种新颖的深度学习方法milliFlow，用于对毫米波雷达点云进行场景流估计，作为中间层的特征，直接受益于下游的人体运动感知任务。实验结果表明，我们的方法具有优越的性能，平均3D端点误差为4.6cm，明显超过竞争方法。此外，通过结合...

    Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
    
[^32]: OpenNDD:用于神经发育障碍检测的开放性识别

    OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v1 [cs.CV])

    [http://arxiv.org/abs/2306.16045](http://arxiv.org/abs/2306.16045)

    OpenNDD是一个用于神经发育障碍检测的开放性识别框架，结合了自动编码器和对抗循环点开放性识别技术，能准确识别已知类别并识别未遇到的类别。

    

    神经发育障碍(NDDs)是一组高患病率的障碍，表现出临床行为的相似性，使得精确识别不同的NDDs（如自闭症谱系障碍（ASD）和注意力缺陷多动障碍（ADHD））非常具有挑战性。此外，对于NDDs诊断并没有可靠的生理标志物，而仅依赖于心理评估标准。然而，通过智能辅助诊断来防止误诊和漏诊是至关重要的，这与随后的相应治疗密切相关。为了缓解这些问题，我们提出了一种新颖的用于NDDs筛查和检测的开放性识别框架，这是在该领域中首次应用开放性识别。它结合了自动编码器和对抗循环点开放性识别，能够准确识别已知类别，并能够识别过去未遇到的类别。

    Neurodevelopmental disorders (NDDs) are a highly prevalent group of disorders and represent strong clinical behavioral similarities, and that make it very challenging for accurate identification of different NDDs such as autism spectrum disorder (ASD) and attention-deficit hyperactivity disorder (ADHD). Moreover, there is no reliable physiological markers for NDDs diagnosis and it solely relies on psychological evaluation criteria. However, it is crucial to prevent misdiagnosis and underdiagnosis by intelligent assisted diagnosis, which is closely related to the follow-up corresponding treatment. In order to relieve these issues, we propose a novel open set recognition framework for NDDs screening and detection, which is the first application of open set recognition in this field. It combines auto encoder and adversarial reciprocal points open set recognition to accurately identify known classes as well as recognize classes never encountered. And considering the strong similarities bet
    
[^33]: 实现真实空气质量预测：介绍易于使用的PurpleAirSF数据集

    Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])

    [http://arxiv.org/abs/2306.13948](http://arxiv.org/abs/2306.13948)

    该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。

    

    空气质量预测由于机器学习和深度学习模型的进步引起了人们的极大关注。然而，复杂的数据采集和开放数据集的缺乏给研究人员造成了挑战，从而阻碍了有效的模型验证。本文介绍了PurpleAirSF，这是一个综合全面且易于获取的数据集，从PurpleAir网络中收集而来。该数据集具有高时间分辨率、各种空气质量测量指标和广泛的地理覆盖范围，可作为研究人员开发新型预测模型、研究空气污染模式以及调查其对健康和环境的影响的有用工具。我们介绍了构建PurpleAirSF所采用的数据采集和处理方法。此外，我们还使用经典和现代时空预测模型进行了初步实验，从而为未来空气质量预测模型的制定建立了基准。

    Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
    
[^34]: CeBed: 一个基于深度数据驱动的OFDM信道估计的基准测试

    CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])

    [http://arxiv.org/abs/2306.13761](http://arxiv.org/abs/2306.13761)

    本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。

    

    深度学习广泛应用于无线通信问题中，包括信道估计。尽管存在许多数据驱动方法，但由于实验条件不一致和缺乏标准化的实验设计，对它们进行公正和现实的比较是困难的。此外，数据驱动方法的性能通常基于经验分析进行比较。缺乏可重复性和标准化评估工具（例如数据集、代码库）阻碍了数据驱动方法在信道估计和无线通信等领域的发展和进步。在这项工作中，我们介绍了一个建立基准测试的倡议，统一了几种数据驱动的OFDM信道估计方法。具体而言，我们提出了CeBed（信道估计测试平台），包括涵盖各种系统模型和传播条件的不同数据集，以及十个深度和传统的基线实现。本文旨在为评估和比较不同的数据驱动OFDM信道估计方法提供标准化的基准，解决领域内实验条件不一致和缺乏可重复性的问题。

    Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
    
[^35]: 学会生成比你的LMM更好的文本

    Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])

    [http://arxiv.org/abs/2306.11816](http://arxiv.org/abs/2306.11816)

    本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。

    

    强化学习(RL)已经成为一种强大的范例，用于优化大型语言模型 (LLM) 条件文本生成。特别地，最近的LLM，如ChatGPT和GPT - 4能够与用户进行流畅的对话，并融合了RL和人类反馈。本研究受到学习搜索算法的启发，并利用文本生成的关键特性，探索了超出通用RL算法如PPO之外的强化学习算法。特别地，我们扩展了RL算法，使其能够与动态黑匣子的指导LLM如GPT-3进行交互，并提出了具有引导反馈的RL(RLGF)，这是一套用于LLM微调的RL算法。我们在GRUE基准测试的IMDB正向评论和CommonGen文本生成任务上进行了实验。我们展示了我们的RL算法比监督学习(SL)和默认PPO基线表现更高，证明了与指导LLM互动的好处。

    Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
    
[^36]: 驯服扩散模型生成音乐驱动的指挥动作

    Taming Diffusion Models for Music-driven Conducting Motion Generation. (arXiv:2306.10065v1 [eess.AS])

    [http://arxiv.org/abs/2306.10065](http://arxiv.org/abs/2306.10065)

    本文提出了Diffusion-Conductor，一种基于DDIM的方法，用于音乐驱动的指挥运动生成，利用扩散模型整合到一个两阶段学习框架中，并应用随机屏蔽策略和一对几何损失函数来提高运动多样性和鲁棒性。

    

    从交响乐中生成指挥家的动作是一项具有挑战性的任务，需要学习语义音乐特征并捕捉真实指挥动作的潜在分布。先前的工作已经将生成对抗网络（GAN）应用于此任务，但前景光追迹的模型在训练稳定性和输出质量方面显示出优势，但在此上下文中还未被利用。本文提出了Diffusion-Conductor，一种基于DDIM的新颖方法，用于音乐驱动的指挥运动生成，将扩散模型整合到一个两阶段学习框架中。我们进一步提出了随机屏蔽策略以提高特征的鲁棒性，并使用一对几何损失函数施加附加规则化和增加运动多样性。我们还设计了几个新的度量标准，包括Frechet Gesture Distance（FGD）和Beat Consistency Score（BC），以进行更全面的评估。

    Generating the motion of orchestral conductors from a given piece of symphony music is a challenging task since it requires a model to learn semantic music features and capture the underlying distribution of real conducting motion. Prior works have applied Generative Adversarial Networks (GAN) to this task, but the promising diffusion model, which recently showed its advantages in terms of both training stability and output quality, has not been exploited in this context. This paper presents Diffusion-Conductor, a novel DDIM-based approach for music-driven conducting motion generation, which integrates the diffusion model to a two-stage learning framework. We further propose a random masking strategy to improve the feature robustness, and use a pair of geometric loss functions to impose additional regularizations and increase motion diversity. We also design several novel metrics, including Frechet Gesture Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive evaluati
    
[^37]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^38]: 源代码模型的数据增强方法：一份综述

    Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19915](http://arxiv.org/abs/2305.19915)

    本文对源代码的数据增强技术进行了全面的调查和综述，介绍了它们的分类法、优化策略和性能结果，并讨论了未来方向和研究挑战。

    

    源代码在许多关键任务中的广泛应用促进了数据增强（DA）技术的发展，以增强训练数据并提高这些模型的各种能力（例如健壮性和可泛化性）。虽然已经提出并针对源代码模型进行了一系列DA方法的调整，但缺乏综合性的调查和审查以理解它们的有效性和含义。本文通过对源代码的数据增强进行全面而综合的调查，填补这一空白，我们系统地整理和概述现有文献，以提供该领域的全面概述。我们首先构建了适用于源代码模型的数据增强的分类法，然后讨论了著名的、方法上具有说明性的方法。接下来，我们强调了优化DA质量的一般策略和技术。随后，我们强调了在被广泛接受的基准测试中发挥作用的技术，并呈现了它们的性能结果。最后，我们讨论了DA用于源代码模型的潜在未来方向和开放研究挑战。

    The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
    
[^39]: 更大、更好、更快：具有人类效率的人类级Atari游戏

    Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])

    [http://arxiv.org/abs/2305.19452](http://arxiv.org/abs/2305.19452)

    引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。

    

    我们引入了一个名为BBF的基于价值函数的强化学习代理来实现Atari 100K基准测试的超人类表现。BBF依靠神经网络的价值估计扩展以及其他设计选择，在遵循样本高效的情况下，能够实现这种扩展。我们对这些设计选择进行了广泛的分析并为未来的工作提供了洞见。最后，我们讨论了更新ALE上样本高效的RL研究目标的问题。我们将我们的代码和数据公开在https://github.com/google-research/google-research/tree/master/bigger_better_faster。

    We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
    
[^40]: 推荐系统解释的可视化：综述和新视角

    Visualization for Recommendation Explainability: A Survey and New Perspectives. (arXiv:2305.11755v1 [cs.IR])

    [http://arxiv.org/abs/2305.11755](http://arxiv.org/abs/2305.11755)

    本文回顾了推荐系统中有关可视化解释的研究，提出了一组可能有益于设计解释性可视化推荐系统的指南。

    

    为推荐提供系统生成的解释是实现透明且值得信赖的推荐系统的重要步骤。可解释的推荐系统为输出提供了人类可理解的基础。在过去的20年中，可解释的推荐引起了推荐系统研究社区的广泛关注。本文旨在全面回顾推荐系统中有关可视化解释的研究工作。更具体地，我们根据解释目标、解释范围、解释样式和解释格式这四个维度系统地审查推荐系统中有关解释的文献。认识到可视化的重要性，我们从解释性视觉方式的角度途径推荐系统文献，即使用可视化作为解释的显示样式。因此，我们得出了一组可能有益于设计解释性可视化推荐系统的指南。

    Providing system-generated explanations for recommendations represents an important step towards transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the last two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizat
    
[^41]: 云计算下的不确定性工作负载预测

    Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])

    [http://arxiv.org/abs/2303.13525](http://arxiv.org/abs/2303.13525)

    本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。

    

    在云计算中预测未来的资源需求对于管理云数据中心并保证客户最低服务质量（QoS）水平至关重要。建模未来需求的不确定性可以提高预测的质量并减少由于资源过度分配而带来的浪费。本文提出了单变量和双变量贝叶斯深度学习模型，用于预测未来资源需求的分布及其不确定性。我们设计了不同的训练情景来训练这些模型，其中每个过程是在多个数据集配置上进行预训练和微调步骤的不同组合。我们还将双变量模型与其单变量对应模型进行比较，用单个或多个数据集进行训练，以研究不同组成部分如何影响预测的准确性并影响QoS。最后，我们研究了我们的模型是否具有迁移学习能力。广泛的实验表明，使用多个数据集进行预训练可以提高性能。

    Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
    
[^42]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^43]: 使用深度离线强化学习实现安全的丙泊酚全麻剂量控制

    Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])

    [http://arxiv.org/abs/2303.10180](http://arxiv.org/abs/2303.10180)

    本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。

    

    自动化麻醉有望实现更精确和个性化的麻醉管理，使麻醉师免于重复性任务，专注于患者手术护理的最关键方面。当前的研究通常集中于创建模拟环境，以便智能体进行学习。这些方法已经展示出良好的实验结果，但离临床应用还有很大的差距。本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来解决学习麻醉策略的问题。首先引入了保守Q-Learning方法以缓解脱线情况下Q函数过度估计的问题。在智能体的培训中添加一项策略约束项，以保持智能体和麻醉师的策略分布一致，以确保智能体在全麻情景下做出更安全的决策。通过实验证明了PCQL的有效性。

    Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
    
[^44]: 使用Prompt-Tuning的原型转向针对无需重复训练的连续学习

    Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])

    [http://arxiv.org/abs/2303.09447](http://arxiv.org/abs/2303.09447)

    本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。

    

    原型作为类别嵌入的一种表示，已被探索用于减少连续学习情境下的内存占用或减轻遗忘。然而，基于原型的方法仍然存在语义漂移和原型干扰导致的性能急剧恶化的问题。在本研究中，我们提出了对比原型提示（CPP）方法，并展示了任务特定提示调整，当在对比学习目标上进行优化时，可以有效地解决这两个障碍并显着提高原型的性能。我们的实验表明，CPP在四个具有挑战性的类增量学习基准测试中表现出色，相对于现有最先进方法有4%至6%的绝对提升。此外，CPP不需要重复训练，它极大地缩小了连续学习和离线联合学习之间的性能差距，展示了一种有前途的Transformer体系结构下连续学习系统的设计方案。

    Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
    
[^45]: 符号音乐的字节对编码

    Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11975](http://arxiv.org/abs/2301.11975)

    本文介绍了一种在符号音乐领域中使用的字节对编码技术，它可以显著减小序列长度并增加词汇量，从而提高语言模型的嵌入能力。

    

    当与深度学习结合使用时，符号音乐通常与语言模型架构相结合。为此，音乐需要进行标记化，即转化为一系列离散的标记。可以通过不同的方法实现这一点，因为音乐可以由同时存在的轨道，具有多个属性的同时音符组成。目前，所提出的标记化依赖于描述音符属性和时间事件的小型标记字典，导致标记序列相当长，对语言模型的嵌入空间的使用不够优化。最近的研究致力于通过合并嵌入或组合标记来减少整体序列长度。在本文中，我们展示了字节对编码，一种广泛用于自然语言的压缩技术，其显著减小了序列长度，同时增加了词汇量。通过这样做，我们利用这些模型的嵌入能力与更有表现力的标记结合，从而得到更好的结果。

    When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better 
    
[^46]: 基于语义的沟通：一篇教程兼综述

    Semantics-Empowered Communication: A Tutorial-cum-Survey. (arXiv:2212.08487v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2212.08487](http://arxiv.org/abs/2212.08487)

    本文提供了基于语义的沟通方面的教程兼综述，回顾了文献，介绍了SemCom生态系统，并将研究方向进行了分类。此外，还提供了启用技术的分类和未来应用场景的展望。

    

    随着基于语义的沟通（SemCom）研究的兴起，学术界和工业界对其各个方面（如理论、应用、度量和实现）的兴趣不断增长。本文的目的是提供一个综合性的调查，涵盖了背景和研究分类，以及详细的技术教程。具体而言，我们首先回顾文献并回答关于语义传输的“什么”和“为什么”问题。然后，我们展示了SemCom生态系统，包括历史、理论、度量、数据集和工具包，并介绍了研究方向的分类方式。此外，我们提议通过显式和隐式基于推理的方法对关键的启用技术进行分类，并详细阐述它们如何演变并为现代内容和通道语义驱动的通信做出贡献。除了回顾和总结最新的e技术，我们还提供了未来的展望和应用场景。

    Along with the springing up of the semantics-empowered communication (SemCom) research, it is now witnessing an unprecedentedly growing interest towards a wide range of aspects (e.g., theories, applications, metrics and implementations) in both academia and industry. In this work, we primarily aim to provide a comprehensive survey on both the background and research taxonomy, as well as a detailed technical tutorial. Specifically, we start by reviewing the literature and answering the "what" and "why" questions in semantic transmissions. Afterwards, we present the ecosystems of SemCom, including history, theories, metrics, datasets and toolkits, on top of which the taxonomy for research directions is presented. Furthermore, we propose to categorize the critical enabling techniques by explicit and implicit reasoning-based methods, and elaborate on how they evolve and contribute to modern content & channel semantics-empowered communications. Besides reviewing and summarizing the latest e
    
[^47]: 多目标强化学习中的福利和公平性

    Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.01382](http://arxiv.org/abs/2212.01382)

    本论文研究了多目标强化学习中的福利和公平性问题，提出了一种基于非线性福利函数的Q-learning算法，通过非线性标量化学习更新和非稳态动作选择来优化策略。算法被证明是可收敛的。

    

    我们研究了公平多目标强化学习，其中一个代理必须学习一个能够在多个维度的向量值奖励上同时获得高回报的策略。受公平资源分配文献的启发，我们将其建模为期望福利最大化问题，针对向量的长期累积奖励的非线性公平福利函数。其中一个经典的例子是纳什社会福利函数，或者几何平均数，其对数变换也被称为比例公平目标。我们表明，即使在表格化的情况下，对期望纳什社会福利进行近似最优化也是计算上难以处理的。尽管如此，我们提供了一种创新的Q-learning改进方法，结合非线性标量化学习更新和非稳态动作选择，以学习有效的优化非线性福利函数的策略。我们证明了我们的算法是可收敛的，并进行了实验证明。

    We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental
    
[^48]: SciRepEval：一个用于科学文献表示的多格式基准

    SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.13308](http://arxiv.org/abs/2211.13308)

    SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。

    

    学习的科学文献表示可以作为下游任务的有价值输入特征，无需进一步微调。然而，用于评估这些表示的现有基准未能捕捉到相关任务的多样性。为此，我们介绍了 SciRepEval，第一个用于训练和评估科学文献表示的全面基准。它包括四种格式的 25 个具有挑战性和现实性的任务，其中 11 个是新任务：分类、回归、排名和搜索。我们使用该基准来研究和改进科学文档表示模型的泛化能力。我们展示了最先进的模型如何在任务格式方面缺乏泛化性能，简单的多任务训练也不能改进它们。然而，一种新的方法，学习每个文档的多个嵌入，每个嵌入专门针对不同的格式，可以提高性能。我们尝试使用任务格式特定的控制代码和适配器。

    Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
    
[^49]: 跨任务泛化的多头适配器路由

    Multi-Head Adapter Routing for Cross-Task Generalization. (arXiv:2211.03831v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.03831](http://arxiv.org/abs/2211.03831)

    本文研究了跨任务泛化中适配器路由的作用，并设计了基于此的新变体Multi-Head Routing (MHR)，通过精细的路由和参数高效的微调实现了更好的性能和更高的参数效率。

    

    跨任务泛化的参数高效微调(PEFT)在少样本任务适配之前通过在多任务训练集上预训练适配器来实现。Ponti等人的Polytropon [Ponti et al., 2023] ($\texttt{Poly}$)在预训练和少样本适配期间共同学习了一组适配器和选择每个任务的适配器子集的路由函数。在本文中，我们研究了适配器路由在其成功中的作用，并根据我们的发现设计了基于此的新变体。首先，我们建立在更精细的路由能提供更多表达性的直觉上。因此，我们提出了Multi-Head Routing (MHR)，它结合了适配器参数的子集，并在可比较的参数预算下表现更好；通过仅微调路由函数而不是适配器(MHR-z)，我们实现了与极高参数效率相媲美的性能。其次，我们发现Poly/MHR的交叉模型适配在少样本任务上可以实现更好的性能。

    Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\texttt{MHR}$ (Multi-Head Routing), which combines $\textit{subsets}$ of adapter parameters and outperforms $\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\texttt{MHR}$-$z$), we achieve competitive performance with extreme parameter efficiency. Second, we find that $\texttt{Poly}$/$\texttt{MHR
    
[^50]: 针对井测数据的非对比度表示学习

    Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.14750](http://arxiv.org/abs/2209.14750)

    本文提出了一种新的方法来处理井测数据表示学习问题，采用自我监督学习的方法进行非对比度的表示学习，减少对数据的标注需求，并提高了算法性能。

    

    石油和天然气行业中的表示学习问题旨在构建一个模型，根据钻井数据为井段提供表示形式。以往的尝试主要是有监督的，并且关注于相似性任务，即估计井段之间的相似程度。我们希望在不使用已标记数据的情况下构建信息量丰富的表示形式。其中一个可能的方法是自我监督学习（SSL）。与有监督范式相反，这个方法对数据需要很少或者没有标签。现今，大多数SSL方法要么是对比的，要么是非对比的。对比方法使相似的（正）对象的表示变得更加接近，并将不同的（负）对象与之距离。由于可能存在错误的正负标注，这些方法可能会提供更差的性能。非对比方法不依赖于此类标注，在计算机视觉领域广泛应用。它们仅使用容易识别的相似对象对进行学习。

    The representation learning problem in the oil & gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in 
    
[^51]: HyperMixer：一种基于MLP的低成本Transformer替代方案

    HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.03691](http://arxiv.org/abs/2203.03691)

    HyperMixer是一种低成本的基于MLP的Transformer替代方案，通过动态形成标记混合MLP来实现自然语言理解，其性能比替代方案好，并可与Transformer媲美，成本更低。

    

    Transformer架构是自然语言理解的首选模型，但它们的成本相当高，因为它们在输入长度方面具有二次复杂度，需要大量的训练数据，并且可能难以调整。为了降低成本，我们研究了简单的基于MLP的架构。我们发现现有的架构（例如MLPMixer）通过静态的MLP独立地应用于每个特征，而过于脱离自然语言理解所需的归纳偏差。在本文中，我们提出了一种简单的改进，即HyperMixer，它使用超网络动态地形成标记混合MLP。实验上，我们证明了我们的模型表现优于替代的基于MLP的模型，并与Transformer媲美。与Transformer不同，HyperMixer在处理时间、训练数据和超参数调整方面具有大大降低的成本。

    Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
    

