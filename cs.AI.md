# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Foundation Model is Efficient Multimodal Multitask Model Selector.](http://arxiv.org/abs/2308.06262) | 本文提出了一种高效的多模态多任务模型选择器（EMMS），它使用大规模的基础模型将不同任务的标签格式转换为统一的噪声标签嵌入，通过简单的加权线性回归来估计模型的可迁移性。 |
| [^2] | [Enhancing Network Management Using Code Generated by Large Language Models.](http://arxiv.org/abs/2308.06261) | 本论文介绍了一种利用大型语言模型生成代码的方法来增强网络管理，通过自然语言查询生成特定任务的代码，解决了可解释性、可扩展性和隐私性方面的挑战，并展示了高准确性、成本效益以及后续使用程序综合技术进一步增强的潜力。 |
| [^3] | [ChatGPT-based Investment Portfolio Selection.](http://arxiv.org/abs/2308.06260) | 本文研究了基于ChatGPT的投资组合选择方法，发现ChatGPT对于股票选择有效，但在权重分配方面可能不够理想。然而，当将ChatGPT的股票选择与组合优化模型相结合时，可以获得更好的结果。 |
| [^4] | [Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms.](http://arxiv.org/abs/2308.06221) | 该论文提出了一种多步训练方法，用于设计深度自编码器，并通过修剪和增长方法以及优化隐藏层大小和训练时长来改善其性能。 |
| [^5] | [Safety in Traffic Management Systems: A Comprehensive Survey.](http://arxiv.org/abs/2308.06204) | 本研究综述了交通管理系统安全性的文献，总结了交通管理系统中的安全问题、当前的研究现状以及确保安全性的技术和方法。这项综述还提出了现有研究的局限性，并提出了未来研究的方向。 |
| [^6] | [Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks.](http://arxiv.org/abs/2308.06203) | 这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。 |
| [^7] | [Exploring Predicate Visual Context in Detecting of Human-Object Interactions.](http://arxiv.org/abs/2308.06202) | 本文研究了在人物和物体交互检测中的谓词视觉背景问题，并通过交叉注意力和盒子配对位置嵌入等方式来改进模型，取得了比现有方法更好的性能。 |
| [^8] | [Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features.](http://arxiv.org/abs/2308.06197) | 这项研究提出了一种基于深度知识蒸馏的新颖持续学习方法，通过使用少量训练样本，能够准确识别新的复合表情类别。 |
| [^9] | [Software Doping Analysis for Human Oversight.](http://arxiv.org/abs/2308.06186) | 本文介绍了一个旨在减轻软件可能带来的社会风险的框架，该框架结合了软件滥用分析的形式基础与概率伪造技术，用于识别软件不良效果。作者应用该技术来评估柴油汽车排放清洁系统和高风险评估人类的决策系统，并提出了改进建议。 |
| [^10] | [Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook.](http://arxiv.org/abs/2308.06173) | 本文调查了当前的物理对抗攻击趋势，分析了物理对抗攻击的特点和挑战。研究了不同应用中的攻击方法，并评估了它们的效果和鲁棒性。此外，讨论了未来的研究方向。 |
| [^11] | [Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction.](http://arxiv.org/abs/2308.06155) | 本论文提出了一种基于阶段性深度时空学习方法，用于预测城际高速公路的每日交通量。该方法通过精心的数据规范化、混合模型结合以及考虑来自异构数据的多个要素，有效解决了交通量预测中的时空特征和数据不平衡问题。 |
| [^12] | [Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling.](http://arxiv.org/abs/2308.06138) | 这项研究利用人工神经网络模型成功预测了锌生产压力过滤过程中的滤饼含水率，为锌生产工艺提供了可靠的预测手段。 |
| [^13] | [A Game-Theoretic Framework for Joint Forecasting and Planning.](http://arxiv.org/abs/2308.06137) | 本论文提出了一个新颖的博弈论框架，用于联合规划和预测，以改进机器人动作的安全性。通过使用游戏理论的方法，该框架可以学习预测人类防范对策，并通过实际算法实现了端到端的模型训练。研究结果表明，该算法在人群导航和现实世界的行人运动数据集中可以产生更安全的规划。 |
| [^14] | [Improving Joint Speech-Text Representations Without Alignment.](http://arxiv.org/abs/2308.06125) | 本研究表明，在联合语音-文本表示中，忽略序列长度问题能够自然地实现一致的表示，一致性损失可以提高下游的字错误率。 |
| [^15] | [Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models.](http://arxiv.org/abs/2308.06111) | 这项研究提出了ZeroShotALI，它使用了一种新颖的推荐系统来改进金融审计中的零样本文本匹配。通过采用大型语言模型（LLM）和经过领域优化的基于transformer的文本匹配解决方案，该系统实现了从报告中推荐与法律要求相符的相关文本段落，并在现有方法上取得了显著的性能提升。 |
| [^16] | [Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes.](http://arxiv.org/abs/2308.06095) | 这项综述研究了以强大语言模型为基础的开放领域对话系统，并探讨了如何通过干预底层语言模型的不同方面，如数据、训练制度或解码，来保证模型的流畅性、信息丰富性、一致性、连贯性以及遵循社会准则的特性。 |
| [^17] | [Reinforcement Logic Rule Learning for Temporal Point Processes.](http://arxiv.org/abs/2308.06094) | 该论文提出了一个用于时间点过程的强化逻辑规则学习框架，利用逐步优化的方法扩展解释性的规则集来解释时间事件的发生。通过使用神经搜索策略和强化学习框架，可以高效地生成新的规则内容和权重。 |
| [^18] | [Toward a Better Understanding of Loss Functions for Collaborative Filtering.](http://arxiv.org/abs/2308.06091) | 现有研究已经表明，通过改进对齐和均匀性设计的损失函数可以实现显著的性能提升。本文提出了一种新的损失函数，称为MAWU，它考虑了数据集的独特模式。 |
| [^19] | [An Autoethnographic Exploration of XAI in Algorithmic Composition.](http://arxiv.org/abs/2308.06089) | 本文是一项使用MeasureVAE生成音乐XAI模型进行自传式研究的探索，发现该模型在音乐创作工作流程中突显了训练数据集的音乐特征，并且显示出XAI模型可以在丰富和复杂的工作流程中发挥潜力。 |
| [^20] | [Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters.](http://arxiv.org/abs/2308.06088) | 本研究探究了大型语言模型（LLMs）在自动识别学生错误和简化教师评估中的潜力。使用65份学生实验方案的数据集，研发了基于GPT-3.5和GPT-4系列的人工智能（AI）系统，并与人工评分员进行测试。结果显示，AI系统在错误检测方面能够准确识别许多基本学生错误。 |
| [^21] | [Cost-effective On-device Continual Learning over Memory Hierarchy with Miro.](http://arxiv.org/abs/2308.06053) | 这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。 |
| [^22] | [Learning to Guide Human Experts via Personalized Large Language Models.](http://arxiv.org/abs/2308.06039) | 本论文提出了学习引导（LTG）框架，通过个性化的大型语言模型为人类专家提供有助于指导决策的指导，解决了机器决策和人类决策之间的依赖问题。利用SLOG实现，该框架在医学诊断任务上取得了初步但有希望的结果。 |
| [^23] | [Deep Context Interest Network for Click-Through Rate Prediction.](http://arxiv.org/abs/2308.06037) | 这篇论文提出了一种名为深度上下文兴趣网络（DCIN）的模型，该模型通过完整地建模点击及其展示上下文来学习用户的上下文感知兴趣，以提高点击率预测性能。 |
| [^24] | [Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing.](http://arxiv.org/abs/2308.06035) | 这篇论文研究了多模态大语言模型（mLLMs）在预测语言处理过程中与人类的视觉-语言集成能力是否一致的问题，并通过实验验证了mLLMs的多模态输入方法可以减少认知负荷，提高感知和理解能力。 |
| [^25] | [Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?.](http://arxiv.org/abs/2308.06032) | 本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。 |
| [^26] | [Controlling Character Motions without Observable Driving Source.](http://arxiv.org/abs/2308.06025) | 本文提出了一个系统框架，结合了VQ-VAE和使用强化学习训练的一种新型基于令牌级控制策略，通过在顶部注入高级先验模型来生成无限长且多样化的序列。 |
| [^27] | [Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study.](http://arxiv.org/abs/2308.06017) | 本研究通过消融实验发现，在单个GPU上，参数数量最多的组合并不一定是最有效的，通过减少参数大小可以在不降低翻译质量的情况下训练复杂模型，揭示了超参数选择、模型大小和计算资源需求之间的关系。 |
| [^28] | [Large Language Models for Telecom: Forthcoming Impact on the Industry.](http://arxiv.org/abs/2308.06013) | 大型语言模型在电信行业将产生重要的影响。它们可以提高运营效率，简化任务，并需要解决使用中的挑战。 |
| [^29] | [Deep Task-specific Bottom Representation Network for Multi-Task Recommendation.](http://arxiv.org/abs/2308.05996) | 本文提出了一种深度任务特定的底层表示网络（DTRN），用于解决多任务推荐系统中的负迁移问题，通过明确获取每个任务的底层表示来改善任务特定特征的捕捉能力。 |
| [^30] | [Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model.](http://arxiv.org/abs/2308.05995) | 本文介绍了一种利用WavLM预训练模型的基于语音驱动的手势合成方法，实现只使用原始语音音频生成个性化全身手势，消除了复杂的多模态处理和手动注释的需求。 |
| [^31] | [TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models.](http://arxiv.org/abs/2308.05985) | 本研究提出了一个名为TrajPAC的原型工具，它采用可能近似正确 (PAC) 的框架来验证行人轨迹预测模型的鲁棒性。通过定义标签鲁棒性和纯鲁棒性，并考虑干扰区间中所有点的鲁棒性，TrajPAC不仅可以识别潜在的反例，还可以提供可解释的分析结果。 |
| [^32] | [Contrastive Explanations of Multi-agent Optimization Solutions.](http://arxiv.org/abs/2308.05984) | 本研究提出了MAoE，一种用于多智能体优化问题的对比解释方法。该方法通过生成新的解决方案并突出显示与初始解决方案的差异，帮助智能体理解为什么初始解决方案优于他们的期望。 |
| [^33] | [Face Encryption via Frequency-Restricted Identity-Agnostic Attacks.](http://arxiv.org/abs/2308.05983) | 通过受限频率的身份不可知攻击进行人脸加密，解决了使用外部扰动加密人脸图像的隐私保护问题，并提出了一种弱黑盒场景下可行的解决方案。 |
| [^34] | [CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation.](http://arxiv.org/abs/2308.05978) | CyberForce是一个联邦强化学习框架，用于在物联网设备中协同私密地确定适合缓解各种零日攻击的MTD技术。它整合了设备指纹识别和异常检测，并通过奖励或惩罚FRL agent选择的MTD机制来提高网络安全性。 |
| [^35] | [Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning.](http://arxiv.org/abs/2308.05973) | 这篇论文使用Viterbi算法和迁移学习来提取推特情绪，引入置信度分数和向量作为评估模型的指标，进行模型的内部评估和微调。 |
| [^36] | [Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI.](http://arxiv.org/abs/2308.05962) | 本文探讨了基于基金会模型的人工智能系统在整个生命周期中所面临的治理挑战，并提出了利用区块链实现去中心化治理的架构。 |
| [^37] | [BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents.](http://arxiv.org/abs/2308.05960) | BOLAA通过提供对LAA的综合比较，并引入一种新的编排策略，为设计LAA架构和优化代理编排策略提供了量化建议。 |
| [^38] | [FoodSAM: Any Food Segmentation.](http://arxiv.org/abs/2308.05938) | 本文介绍了FoodSAM，它是一个集成了粗糙语义掩膜和SAM生成掩膜的新型框架，可以提高食物图像分割的质量。同时，它还能进行实例分割、全景分割和可提示分割，是一个全面的食物分割解决方案。 |
| [^39] | [A Deep Recurrent-Reinforcement Learning Method for Intelligent AutoScaling of Serverless Functions.](http://arxiv.org/abs/2308.05937) | 该论文介绍了一种用于智能自动缩放无服务器函数的深度循环强化学习方法，针对波动的工作负载和严格的性能约束，通过建立一个适应性策略来实现最大化期望目标。 |
| [^40] | [LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts.](http://arxiv.org/abs/2308.05935) | 本文提出了一个虚拟的MOOC助教 LittleMu，通过整合异构数据源和教学提示链路来支持广泛范围的准确回答和知识相关的闲聊服务。 |
| [^41] | [Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding.](http://arxiv.org/abs/2308.05893) | 本文综述了在多智能体路径规划中深度强化学习技术的应用。与其他研究不同，我们重点介绍了DRL方法在MAPF中的整合，并解决了MAPF解决方案评估指标缺乏统一性的问题。我们讨论了基于模型的DRL作为未来发展方向，并提供了解决MAPF当前挑战所需的基础理解。 |
| [^42] | [DF2: Distribution-Free Decision-Focused Learning.](http://arxiv.org/abs/2308.05889) | DF2是一种无分布的决策焦点学习方法，特别解决了模型不匹配错误、样本平均逼近误差和梯度逼近误差三个瓶颈问题。 |
| [^43] | [Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips.](http://arxiv.org/abs/2308.05869) | 这项工作提出了一种名为 HaX-CoNN 的方案，可以实现在异构 SoC 上并发执行 DNN 推理工作负载，并考虑了共享内存竞争和加速器之间的转换以找到最优的调度方案。 |
| [^44] | [Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge.](http://arxiv.org/abs/2308.05862) | 本论文介绍了FLARE22挑战，旨在评估速度快、资源少、准确、节约注释成本且具有泛化能力的AI算法在腹部器官分析方面的表现。通过使用50个标记扫描和2000个未标记扫描，一组AI算法达到了90.0\%的中位数Dice相似系数（DSC），显著降低了注释需求。 |
| [^45] | [Knowledge Propagation over Conditional Independence Graphs.](http://arxiv.org/abs/2308.05857) | 这项工作提出了在条件独立图上进行知识传播的算法，并通过在Cora和PubMed数据集上的实验证明了其优于现有方法的效果。 |
| [^46] | [Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks.](http://arxiv.org/abs/2308.05846) | 本研究提出了一种使用领域随机化和目标跟踪神经网络的方法来进行种子核计数。该方法通过使用合成图像作为训练数据的替代品，解决了神经网络模型需要大量有标签训练数据的问题，可以低成本估计谷物产量。 |
| [^47] | [DiLogics: Creating Web Automation Programs With Diverse Logics.](http://arxiv.org/abs/2308.05828) | DiLogics是一个通过演示编程的系统，利用自然语言处理帮助用户创建处理多样化规范的Web自动化程序。 |
| [^48] | [Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception.](http://arxiv.org/abs/2308.05822) | 本研究提出了一种记忆增强系统，它利用自然语言编码视频数据并将其存储在向量数据库中，通过利用大型视觉语言模型的强大功能来进行语言编码的过程。 |
| [^49] | [Optical Script Identification for multi-lingual Indic-script.](http://arxiv.org/abs/2308.05780) | 本论文调查了印度多语言字体的光学脚本识别技术的进展，重点讨论了脚本预处理和文本识别的方法，提出了对于印度字体特征的挑战和可能的解决方案。 |
| [^50] | [Unleashing the Power of Extra-Tree Feature Selection and Random Forest Classifier for Improved Survival Prediction in Heart Failure Patients.](http://arxiv.org/abs/2308.05765) | 通过利用数据预处理技术和Extra-Tree特征选择方法与Random Forest分类器相结合，本研究提高了心力衰竭患者生存预测能力，并取得了98.33%的准确率 |
| [^51] | [Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI.](http://arxiv.org/abs/2308.05764) | 该论文提出了一种通过从心脏MRI中的知识转移解锁心电图的诊断潜力的方法。通过将CMR图像中的领域特定信息转移到ECG嵌入中，该方法实现了仅根据ECG数据进行全面的心脏筛查，并能预测心血管疾病的个体风险和确定心脏表型。 |
| [^52] | [A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals.](http://arxiv.org/abs/2308.05759) | 这项研究提出了一种基于机器学习的睡眠-清醒分类模型，利用光电流血容积波形和活动信号提取的特征，可以评估睡眠质量，识别睡眠问题和改善整体健康。 |
| [^53] | [LLM As DBA.](http://arxiv.org/abs/2308.05481) | LLM变成DBA，提供数据库维护的诊断和优化建议，通过从文本来源中获取经验和多个LLMs的协作诊断。 |
| [^54] | [Homophily-enhanced Structure Learning for Graph Clustering.](http://arxiv.org/abs/2308.05309) | 提出了一种名为HoLe的方法，通过在图结构中增强同类性可以显著改善图聚类任务的性能。 |
| [^55] | [Developmental Bootstrapping of AIs.](http://arxiv.org/abs/2308.04586) | 传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。 |
| [^56] | [Deep Learning for Diverse Data Types Steganalysis: A Review.](http://arxiv.org/abs/2308.04522) | 本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。 |
| [^57] | [AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models.](http://arxiv.org/abs/2308.04241) | 本研究提出了一种利用大型语言模型和深度学习算法的自动化AI驱动的PCF核算框架AutoPCF，该框架具有实现产品碳足迹的自动建模和估算的潜力。 |
| [^58] | [Predicting Drug-Drug Interactions Using Knowledge Graphs.](http://arxiv.org/abs/2308.04172) | 本文提出了一个名为medicX的端到端框架，通过使用知识图谱和机器学习算法，预测药物的相互作用。最好的表现组合是ComplEx嵌入方法和LSTM网络，达到了95.19%的F1分数。 |
| [^59] | [Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket.](http://arxiv.org/abs/2308.02916) | 本文介绍了一种利用对抗抹除的方法来增强图彩票的性能。通过重新考虑修剪信息中的有价值的信息，我们提出了ACE-GLT，这是一种更强大的图彩票方法。 |
| [^60] | [Collaborative filtering to capture AI user's preferences as norms.](http://arxiv.org/abs/2308.02542) | 本论文提出了一种以协同过滤方法构建规范以捕捉AI用户偏好的新视角。 |
| [^61] | [A Survey on Popularity Bias in Recommender Systems.](http://arxiv.org/abs/2308.01118) | 这篇综述论文讨论了推荐系统中的流行偏差问题，并回顾了现有的方法来检测、量化和减少流行偏差。它同时提供了计算度量的概述和主要技术方法的回顾。 |
| [^62] | [Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?.](http://arxiv.org/abs/2308.00158) | 本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。 |
| [^63] | [AI Increases Global Access to Reliable Flood Forecasts.](http://arxiv.org/abs/2307.16104) | 本研究开发了一个人工智能模型，可以准确预测未经测量流域的极端水文事件，从而提高了全球洪水预警的覆盖范围。 |
| [^64] | [ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications.](http://arxiv.org/abs/2307.12488) | 本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。 |
| [^65] | [Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education.](http://arxiv.org/abs/2307.12267) | 本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。 |
| [^66] | [In-IDE Generation-based Information Support with a Large Language Model.](http://arxiv.org/abs/2307.08177) | 本研究探索了一种基于大型语言模型的IDE内生成式信息支持，通过在IDE中内嵌对话式用户界面，向开发者提供关于代码理解的帮助。通过查询大型语言模型来解释代码、提供API调用的详细信息、解释特定领域术语以及为API提供使用示例，该系统在用户研究中获得了积极的评价。 |
| [^67] | [Efficient Domain Adaptation of Sentence Embeddings using Adapters.](http://arxiv.org/abs/2307.03104) | 本论文提出了一种通过训练轻量级适配器来高效域自适应句子嵌入的方法，避免了微调整个句子嵌入模型的资源消耗。通过训练特定领域的适配器，可以在不同领域中使用同一模型获得良好的性能。 |
| [^68] | [Human-to-Human Interaction Detection.](http://arxiv.org/abs/2307.00464) | 这项研究提出了人与人之间的互动检测任务，通过一种模型实现了同时检测主体、识别个人动作和根据互动关系分组的目标。通过在现有的AVA数据集上添加互动关系注释，建立了一个新的HID基准数据集。 |
| [^69] | [Mixed Integer Programming for Time-Optimal Multi-Robot Coverage Path Planning with Efficient Heuristics.](http://arxiv.org/abs/2306.17609) | 本文研究了时间优化多机器人覆盖路径规划问题，并通过混合整数规划模型提出了一个最优解决方案，该方案的覆盖时间最多为最优解的四倍。此外，我们还提出了两个有效的启发式算法来减少问题的规模，并通过模型优化方法进一步提高了效率。 |
| [^70] | [Hard Sample Mining Enabled Supervised Contrastive Feature Learning for Wind Turbine Pitch System Fault Diagnosis.](http://arxiv.org/abs/2306.14701) | 本文提出了一种基于困难样本挖掘的监督对比特征学习方法，用于风力发电机桨叶系统故障诊断。该方法利用余弦相似度识别困难样本，并通过构建困难样本对来学习更具区分性的表示，进一步提高了多层感知机的训练效果。 |
| [^71] | [Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy (with appendix).](http://arxiv.org/abs/2306.10645) | 使用ChatGPT提示开发教育聊天机器人具有潜力，能够实现互动和个性化学习体验。通过在社交媒体素养案例中进行初步测试，我们得出了混合对话机器人交互的见解和初步指南。这些机器人具有适应性强、多样化的教育策略和对话风格的能力。 |
| [^72] | [Trained Transformers Learn Linear Models In-Context.](http://arxiv.org/abs/2306.09927) | 本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。 |
| [^73] | [On the Design Fundamentals of Diffusion Models: A Survey.](http://arxiv.org/abs/2306.04542) | 本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。 |
| [^74] | [Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss.](http://arxiv.org/abs/2305.17271) | 本论文提出了一种鲁棒车道检测流水线，该流水线包括自预训练掩模序列自编码器和使用定制PolyLoss微调的端到端神经网络模型。掩模序列自编码器被采用以通过重构随机掩膜图像中的丢失像素为目标来预训练神经网络模型，提升了车道检测性能。 |
| [^75] | [Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation.](http://arxiv.org/abs/2305.07500) | 本文提出了一种新的基于最优输运（OT）的领域自适应（DA）方法，通过学习一个嵌入空间，使得OT问题的解是最优且计算量较少的，适用于同质和异质的DA设置。 |
| [^76] | [Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders.](http://arxiv.org/abs/2305.02640) | 本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。 |
| [^77] | [Towards Enhancing In-Context Learning for Code Generation.](http://arxiv.org/abs/2303.17780) | 本文提出了一种名为AceCoder的代码生成上下文学习方法，与标准上下文学习相比，它通过示例检索和引导代码生成来提高生成代码的准确性和鲁棒性。 |
| [^78] | [Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations.](http://arxiv.org/abs/2303.16618) | 本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。 |
| [^79] | [ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions.](http://arxiv.org/abs/2303.12364) | ExBEHRT是一种扩展Transformer模型，应用于电子病历数据，将多种类型的记录包括在特征空间中，可以预测不同疾病下游任务的性能更好，并使用预期梯度对结果进行更细粒度的解释。 |
| [^80] | [NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing.](http://arxiv.org/abs/2303.11219) | 提出了一种名为NeTO的方法，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。通过采用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场，可以实现高质量的重建结果。 |
| [^81] | [Collaborative Learning with a Drone Orchestrator.](http://arxiv.org/abs/2303.02266) | 本文研究了无人机辅助的协同学习问题，提出了一种通过智能设备群与无人机协同训练神经网络模型的方法。在考虑数据异质性和通信错误的情况下，导出了协同学习的收敛速度，并通过优化无人机轨迹来提高训练准确率。 |
| [^82] | [Cross-modal Contrastive Learning for Multimodal Fake News Detection.](http://arxiv.org/abs/2302.14057) | 这项研究提出了COOLANT，一个用于跨模态假新闻检测的对比学习框架，旨在提升图像和文本的对齐精度，并通过跨模态融合和注意力机制实现更准确和可解释的特征聚合。 |
| [^83] | [Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models.](http://arxiv.org/abs/2301.11189) | 本论文提出了一种改进神经图像压缩的方法，通过引入非二元鉴别器以及以量化局部图像表示为条件，实现了更好地优化失真度和统计保真度。 |
| [^84] | [Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios.](http://arxiv.org/abs/2212.11419) | 结合使用简单奖励的强化学习和模仿学习，可以显著提高驾驶策略的安全性和可靠性，超过仅用模仿学习学到的策略。 |
| [^85] | [An Efficient Incremental Simple Temporal Network Data Structure for Temporal Planning.](http://arxiv.org/abs/2212.07226) | 本研究提出了一种用于时态规划的高效增量简单时态网络数据结构，通过采用增量重用计算和避免内存复制的方法，提高了时态规划问题求解的效率。实验证明，该数据结构在时态规划问题序列上表现优于其他方法。 |
| [^86] | [RT-1: Robotics Transformer for Real-World Control at Scale.](http://arxiv.org/abs/2212.06817) | 本文提出了机器人变压器模型，通过从大规模、多样化、任务无关的数据集中获取知识，并结合高容量架构，实现了在实际控制领域的高性能泛化能力。 |
| [^87] | [Expeditious Saliency-guided Mix-up through Random Gradient Thresholding.](http://arxiv.org/abs/2212.04875) | 本文介绍了一种新的混合训练方法，名为R-Mix，通过结合随机性和显著性利用的最佳元素，实现了速度、简单性和准确性的平衡。该方法在泛化能力、弱监督目标定位、校准和对抗性攻击的鲁棒性方面具有有效性。同时，作者还通过训练一个强化学习代理来决定混合训练过程，以探索是否存在更好的决策协议。 |
| [^88] | [On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks.](http://arxiv.org/abs/2212.02374) | 过度平滑和过度压缩是深度图神经网络中的关键挑战，我们提出了添加和删除边缘的方法来解决这个问题。 |
| [^89] | [Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia.](http://arxiv.org/abs/2211.00732) | Kuaipedia是一个大规模的多模式短视频百科全书，通过知识视频的形式，能够轻松表达网民对某个项目的各个方面的需求。 |
| [^90] | [There is more than one kind of robustness: Fooling Whisper with adversarial examples.](http://arxiv.org/abs/2210.17316) | 本研究展示了Whisper模型虽然在分布外输入和随机噪声方面显示出了出色的稳健性，但却容易受到对抗干扰的影响。我们通过生成极小的输入扰动来大幅降低Whisper的性能，并对多语言模型的性能产生了影响。这些发现对于实际的安全问题和对抗性稳健ASR的需求具有重要意义。 |
| [^91] | [CodeEditor: Learning to Edit Source Code with Pre-trained Models.](http://arxiv.org/abs/2210.17040) | CodeEditor是一个预训练代码编辑模型，通过专门的预训练任务和代码编辑任务的结合，提高了代码编辑模型的性能和泛化能力。 |
| [^92] | [A Law of Data Separation in Deep Learning.](http://arxiv.org/abs/2210.17020) | 深度学习中存在一个简单而定量的数据分离定律，每一层都以恒定的几何速率改善数据的分离程度。这个定律为架构设计、提高模型的健壮性和样本外性能以及预测的解释提供了实际的指导。 |
| [^93] | [Lib-SibGMU -- A University Library Circulation Dataset for Recommender Systems Developmen.](http://arxiv.org/abs/2208.12356) | Lib-SibGMU是一个开放的大学图书馆借阅数据集，可以用于推荐系统开发。在该数据集上我们发现使用fastText模型作为向量化器可以获得竞争性的结果。 |
| [^94] | [Relational Action Bases: Formalization, Effective Safety Verification, and Invariants (Extended Version).](http://arxiv.org/abs/2208.06377) | 该论文介绍了关系行动基础（RABs）的通用框架，通过取消关系状态和行动的限制，实现了对动态系统的建模和验证，同时提供了参数化安全性的研究方法。 |
| [^95] | [ECLAD: Extracting Concepts with Local Aggregated Descriptors.](http://arxiv.org/abs/2206.04531) | 本文提出了一种使用本地聚合描述符提取概念的方法，并介绍了一种基于合成数据集的验证过程，用于减少概念提取方法的人工干预需求。 |
| [^96] | [Vision-Based UAV Self-Positioning in Low-Altitude Urban Environments.](http://arxiv.org/abs/2201.09201) | 本论文提出了一个新的数据集DenseUAV，为无人机在低空城市环境中的自定位任务设计。该数据集采用了密集采样方法，解决了现有数据集中忽视的密集采样和现实场景中的不确定性问题。 |
| [^97] | [CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition.](http://arxiv.org/abs/2111.11011) | 本文提出了一种名为多域字符距离感知（MDCDP）的新颖模块，用于解决场景文本识别中特征和字符对齐不准确的问题。该模块通过交叉注意机制融合视觉和语义特征，并生成一个内容感知嵌入来感知字符位置。 |
| [^98] | [Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification.](http://arxiv.org/abs/2110.03894) | 本文提出了一种基于相似性映射的神经模型重编程方法，用于低资源口语命令分类。实验证明，在有限的数据条件下，该方法在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果。 |
| [^99] | [Mapping Patterns for Virtual Knowledge Graphs.](http://arxiv.org/abs/2012.01917) | 本文提出了一个全面的虚拟知识图谱映射模式目录，用于支持链接数据库和本体的映射管理，该目录通过分析实际用例和VKG基准，扩展和完善了已有方法和模式，并验证了其在多种场景中可行性。 |
| [^100] | [Towards AI Forensics: Did the Artificial Intelligence System Do It?.](http://arxiv.org/abs/2005.13635) | 本文关注人工智能是否导致特定事件以及触发AI行动的问题，提供了取证调查策略，并使用卷积神经网络评估了识别恶意AI的挑战和思路。 |

# 详细

[^1]: Foundation Model是一个高效的多模态多任务模型选择器

    Foundation Model is Efficient Multimodal Multitask Model Selector. (arXiv:2308.06262v1 [cs.LG])

    [http://arxiv.org/abs/2308.06262](http://arxiv.org/abs/2308.06262)

    本文提出了一种高效的多模态多任务模型选择器（EMMS），它使用大规模的基础模型将不同任务的标签格式转换为统一的噪声标签嵌入，通过简单的加权线性回归来估计模型的可迁移性。

    

    本文研究了一个不常见但非常重要的问题：在给定一组预训练的神经网络的情况下，如何在不对它们进行微调的情况下预测它们在每个多模态任务上的性能，比如图像识别、指代、字幕生成、视觉问答和文字问答。一种蛮力的方法是对所有模型在所有目标数据集上进行微调，这会带来高计算成本。虽然最近的一些先进方法采用轻量级指标来衡量模型的可迁移性，但它们往往严重依赖于单个任务的先验知识，使得它们在多模态多任务的场景中不适用。为了解决这个问题，我们提出了一种高效的多任务模型选择器（EMMS），它使用大规模的基于基础模型，将不同下游任务的各种标签格式，如类别、文本和边界框转换为统一的噪声标签嵌入。EMMS可以通过简单的加权线性回归来估计模型的可迁移性。

    This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression
    
[^2]: 使用大型语言模型生成的代码增强网络管理

    Enhancing Network Management Using Code Generated by Large Language Models. (arXiv:2308.06261v1 [cs.NI])

    [http://arxiv.org/abs/2308.06261](http://arxiv.org/abs/2308.06261)

    本论文介绍了一种利用大型语言模型生成代码的方法来增强网络管理，通过自然语言查询生成特定任务的代码，解决了可解释性、可扩展性和隐私性方面的挑战，并展示了高准确性、成本效益以及后续使用程序综合技术进一步增强的潜力。

    

    分析网络拓扑和通信图在当代网络管理中起着至关重要的作用。然而，缺乏一种一致的方法导致了学习曲线的挑战性增加、错误率的增加和效率的降低。在本文中，我们提出了一种新颖的方法，利用大型语言模型（LLMs）从自然语言查询中生成特定任务的代码，以方便自然语言网络管理体验。这种方法通过允许网络运营商检查生成的代码来解决了可解释性、可扩展性和隐私性方面的挑战，避免了与LLMs共享网络数据，并集中工作于应用程序特定请求与通用程序综合技术相结合。我们设计和评估了一个原型系统，使用基准应用程序展示了高准确性、成本效益以及使用互补程序综合技术进一步增强的潜力。

    Analyzing network topologies and communication graphs plays a crucial role in contemporary network management. However, the absence of a cohesive approach leads to a challenging learning curve, heightened errors, and inefficiencies. In this paper, we introduce a novel approach to facilitate a natural-language-based network management experience, utilizing large language models (LLMs) to generate task-specific code from natural language queries. This method tackles the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, eliminating the need to share network data with LLMs, and concentrating on application-specific requests combined with general program synthesis techniques. We design and evaluate a prototype system using benchmark applications, showcasing high accuracy, cost-effectiveness, and the potential for further enhancements using complementary program synthesis techniques.
    
[^3]: 基于ChatGPT的投资组合选择

    ChatGPT-based Investment Portfolio Selection. (arXiv:2308.06260v1 [q-fin.PM])

    [http://arxiv.org/abs/2308.06260](http://arxiv.org/abs/2308.06260)

    本文研究了基于ChatGPT的投资组合选择方法，发现ChatGPT对于股票选择有效，但在权重分配方面可能不够理想。然而，当将ChatGPT的股票选择与组合优化模型相结合时，可以获得更好的结果。

    

    本文探讨了利用生成式AI模型（如ChatGPT）进行投资组合选择的潜在用途。由于模型的“幻觉”，对来自生成预训练转换器（GPT）模型的投资建议的信任是一个挑战，需要对输出进行仔细验证和验证。因此，我们采用了一种替代方法。我们使用ChatGPT从S&P500市场指数中获取一组有投资吸引力的股票。随后，我们将利用这个由AI生成的交易股票组合与定量组合优化模型进行比较，并与一些流行的投资基金进行对比评估。我们的研究结果表明，ChatGPT在股票选择方面是有效的，但在分配股票组合中的最优权重方面可能表现不佳。但是，当ChatGPT的股票选择与已建立的组合优化模型相结合时，我们取得了更好的结果。

    In this paper, we explore potential uses of generative AI models, such as ChatGPT, for investment portfolio selection. Trusting investment advice from Generative Pre-Trained Transformer (GPT) models is a challenge due to model "hallucinations", necessitating careful verification and validation of the output. Therefore, we take an alternative approach. We use ChatGPT to obtain a universe of stocks from S&P500 market index that are potentially attractive for investing. Subsequently, we compared various portfolio optimization strategies that utilized this AI-generated trading universe, evaluating those against quantitative portfolio optimization models as well as comparing to some of the popular investment funds. Our findings indicate that ChatGPT is effective in stock selection but may not perform as well in assigning optimal weights to stocks within the portfolio. But when stocks selection by ChatGPT is combined with established portfolio optimization models, we achieve even better resu
    
[^4]: 使用二阶算法自动调整和训练高效的深度自编码器

    Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v1 [cs.LG])

    [http://arxiv.org/abs/2308.06221](http://arxiv.org/abs/2308.06221)

    该论文提出了一种多步训练方法，用于设计深度自编码器，并通过修剪和增长方法以及优化隐藏层大小和训练时长来改善其性能。

    

    我们提出了一种多步训练方法，用于设计广义线性分类器。首先，通过回归找到一个初始的多类线性分类器。然后通过修剪不必要的输入来最小化验证误差。同时，通过类似于Ho-Kashyap规则的方法改善期望输出。接下来，将输出判别式缩放为广义线性分类器中S型输出单元的网络函数。然后，我们开发了一族批量训练算法，用于优化多层感知机的隐藏层大小和训练时长。接着，我们将修剪与增长方法相结合。然后，将输入单元缩放为S型输出单元的网络函数，然后将其作为输入馈送到MLP中。最后，我们提出了深度学习模块中的改进，从而提高了深度架构的整体性能。我们讨论了关于d的学习算法的原则和公式。

    We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for d
    
[^5]: 交通管理系统的安全性：一项全面调查

    Safety in Traffic Management Systems: A Comprehensive Survey. (arXiv:2308.06204v1 [eess.SY])

    [http://arxiv.org/abs/2308.06204](http://arxiv.org/abs/2308.06204)

    本研究综述了交通管理系统安全性的文献，总结了交通管理系统中的安全问题、当前的研究现状以及确保安全性的技术和方法。这项综述还提出了现有研究的局限性，并提出了未来研究的方向。

    

    交通管理系统在保障道路上的安全和高效交通方面起着至关重要的作用。然而，交通管理系统中的先进技术引入了新的安全挑战。因此，确保这些系统的安全性是重要的，以防止事故并减小对道路用户的影响。本调查综述了关于交通管理系统安全性的文献，具体讨论了交通管理系统中涉及的不同安全问题、当前关于这些系统安全性的研究现状以及确保这些系统安全性的技术和方法。我们还指出了现有研究的局限性，并提出了未来研究方向的建议。

    Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
    
[^6]: 为机器人堆积方块任务构建因果性概率框架

    Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])

    [http://arxiv.org/abs/2308.06203](http://arxiv.org/abs/2308.06203)

    这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。

    

    现实世界中的不确定性意味着系统设计者无法预测并明确设计出机器人可能遇到的所有场景。因此，以这种方式设计的机器人在高度受控的环境之外容易出现故障。因果模型提供了一个原则性的框架，用于编码机器人与其环境相互作用的因果关系的形式化知识，并结合现实世界机器人通常遇到的噪声和不确定性的概率表示。结合因果推断，这些模型使自主代理能够理解、推理和解释其环境。在这项工作中，我们关注机器人堆积方块任务的问题，因为它展示了许多应用所需的基本感知和操作能力，包括仓库物流和家庭人工支持机器人。我们提出了一个新颖的因果性概率框架，将物理模拟功能嵌入到这个任务中。

    Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
    
[^7]: 在检测人物和物体交互中探索谓词视觉背景

    Exploring Predicate Visual Context in Detecting of Human-Object Interactions. (arXiv:2308.06202v1 [cs.CV])

    [http://arxiv.org/abs/2308.06202](http://arxiv.org/abs/2308.06202)

    本文研究了在人物和物体交互检测中的谓词视觉背景问题，并通过交叉注意力和盒子配对位置嵌入等方式来改进模型，取得了比现有方法更好的性能。

    

    最近，DETR框架已成为人物和物体交互（HOI）研究的主要方法。特别是，基于两阶段变换器的HOI检测器是性能最好和训练最高效的方法之一。然而，这些方法通常以缺乏细粒度上下文信息的物体特征作为HOI分类的条件，而忽视了姿势和方向信息，而更注重关于物体身份和边界的视觉提示。这自然地阻碍了对复杂或模糊交互的识别。本文通过可视化和精心设计的实验研究了这些问题。因此，我们通过交叉注意力重新引入图像特征，并改进了查询设计，广泛探索了键和值，以及使用盒子配对位置嵌入作为空间指导。我们的改进谓词视觉背景（PViC）模型在HICO-DET和V-COCO基准测试上优于现有方法，同时保持了性能。

    Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaini
    
[^8]: 使用深度知识蒸馏基本特征进行复杂面部表情识别

    Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v1 [cs.CV])

    [http://arxiv.org/abs/2308.06197](http://arxiv.org/abs/2308.06197)

    这项研究提出了一种基于深度知识蒸馏的新颖持续学习方法，通过使用少量训练样本，能够准确识别新的复合表情类别。

    

    复杂情绪识别是一项认知任务，迄今为止，其表现在与人类认知水平相等或以上的其他任务中表现优秀的程度已经被证明是较为困难的。由于人脸表达的情绪复杂性，通过面部表情进行情绪识别特别困难。为了使机器在这一领域达到与人类相同的表现水平，它可能需要实时合成知识并理解新概念。人类能够仅仅使用几个例子学习新概念，通过从记忆中提取重要信息并丢弃其余信息。同样，持续学习方法可以在保留已知类别知识的同时学习新类别，而少样本学习方法能够使用非常少的训练样本学习新类别。我们提出了一种新颖的持续学习方法，受到人类认知和学习的启发，可以使用少量的训练样本准确地识别新的复合表情类别。

    Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by
    
[^9]: 软件滥用分析与人类监督

    Software Doping Analysis for Human Oversight. (arXiv:2308.06186v1 [cs.CY])

    [http://arxiv.org/abs/2308.06186](http://arxiv.org/abs/2308.06186)

    本文介绍了一个旨在减轻软件可能带来的社会风险的框架，该框架结合了软件滥用分析的形式基础与概率伪造技术，用于识别软件不良效果。作者应用该技术来评估柴油汽车排放清洁系统和高风险评估人类的决策系统，并提出了改进建议。

    

    本文介绍了一个旨在减轻软件可能带来的社会风险的框架。具体而言，这包括了软件滥用、不公平和高风险决策系统中的歧视。软件滥用是指包含了针对用户利益的秘密添加功能的软件。一个著名的软件滥用示例就是柴油排放丑闻曝光时在全球数百万辆汽车中发现的操纵排放清洁系统。本文的第一部分将软件滥用分析的形式基础与已建立的概率伪造技术相结合，得出了一种用于识别软件不良效果的黑箱分析技术。我们将这种技术应用于柴油汽车排放清洁系统，也应用于以可能不公平或歧视的方式评估人类的高风险系统。我们演示了我们的方法如何可以对决策结果进行评估，并为改进系统提出了建议。

    This article introduces a framework that is meant to assist in mitigating societal risks that software can pose. Concretely, this encompasses facets of software doping as well as unfairness and discrimination in high-risk decision-making systems. The term software doping refers to software that contains surreptitiously added functionality that is against the interest of the user. A prominent example of software doping are the tampered emission cleaning systems that were found in millions of cars around the world when the diesel emissions scandal surfaced. The first part of this article combines the formal foundations of software doping analysis with established probabilistic falsification techniques to arrive at a black-box analysis technique for identifying undesired effects of software. We apply this technique to emission cleaning systems in diesel cars but also to high-risk systems that evaluate humans in a possibly unfair or discriminating way. We demonstrate how our approach can a
    
[^10]: 基于摄像头的智能系统的物理对抗攻击：当前趋势，分类，应用，研究挑战和未来展望

    Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook. (arXiv:2308.06173v1 [cs.CR])

    [http://arxiv.org/abs/2308.06173](http://arxiv.org/abs/2308.06173)

    本文调查了当前的物理对抗攻击趋势，分析了物理对抗攻击的特点和挑战。研究了不同应用中的攻击方法，并评估了它们的效果和鲁棒性。此外，讨论了未来的研究方向。

    

    本文针对物理对抗攻击提出了一个综合调查研究，重点关注当前的趋势。我们旨在提供对物理对抗攻击概念的全面理解，分析其关键特征和区别性特点。此外，我们探讨了在物理世界中执行攻击所涉及的具体需求和挑战。本文深入研究了各种物理对抗攻击方法，根据它们在不同应用中的目标任务进行分类，包括分类，检测，人脸识别，语义分割和深度估计。我们评估了这些攻击方法在效果、隐蔽性和鲁棒性方面的性能。我们研究了每种技术如何努力确保成功操作深度神经网络，同时减小被检测的风险和承受真实世界的干扰。最后，我们讨论了当前的挑战，并勾画了未来的研究方向。

    In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research 
    
[^11]: 基于阶段性深度时空学习的高速公路交通量预测

    Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction. (arXiv:2308.06155v1 [cs.LG])

    [http://arxiv.org/abs/2308.06155](http://arxiv.org/abs/2308.06155)

    本论文提出了一种基于阶段性深度时空学习方法，用于预测城际高速公路的每日交通量。该方法通过精心的数据规范化、混合模型结合以及考虑来自异构数据的多个要素，有效解决了交通量预测中的时空特征和数据不平衡问题。

    

    城际高速公路交通对于现代都市生活至关重要，并生成带有时空特征的异构感知数据。作为交通领域的例行分析，每日交通量估计面临着挑战，包括缺乏对相关时空特征的长期考察和处理数据不平衡的有效手段，后者会恶化预测性能。本文提出了一种阶段性深度时空学习方法，用于预测每日交通量。在特征预处理阶段，根据潜在的长尾分布精心进行数据规范化。在时空学习阶段，采用了一种混合模型，将全卷积网络（FCN）和长短期记忆网络（LSTM）结合起来，考虑了来自异构数据的时间、空间、气象和日历信息。在决策阶段，预测了网络范围收费卡口未来一天的交通量。

    Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll
    
[^12]: 应用人工神经网络研究压力过滤性能的探索和锌浸出滤饼含水率建模

    Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling. (arXiv:2308.06138v1 [cs.LG])

    [http://arxiv.org/abs/2308.06138](http://arxiv.org/abs/2308.06138)

    这项研究利用人工神经网络模型成功预测了锌生产压力过滤过程中的滤饼含水率，为锌生产工艺提供了可靠的预测手段。

    

    机器学习是材料科学应用中的强大工具。人工神经网络是一种能够提供高预测准确性的机器学习技术。本研究旨在开发一种人工神经网络模型，用于预测锌生产的压力过滤过程中的滤饼含水率。滤饼含水率受到七个参数的影响：温度（35摄氏度和65摄氏度），固体浓度（0.2克/升和0.38克/升），pH值（2、3.5和5），吹气时间（2分钟、10分钟和15分钟），滤饼厚度（14毫米、20毫米、26毫米和34毫米），压力和过滤时间。本研究使用两种类型的织物进行了288次测试：聚丙烯（S1）和涤纶（S2）。通过决定系数（R2）、均方误差（MSE）和平均绝对误差（MAE）指标评估了人工神经网络模型在两个数据集上的性能。结果显示，对于S1和S2，R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。

    Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated t
    
[^13]: 一个用于联合预测和规划的博弈论框架

    A Game-Theoretic Framework for Joint Forecasting and Planning. (arXiv:2308.06137v1 [cs.AI])

    [http://arxiv.org/abs/2308.06137](http://arxiv.org/abs/2308.06137)

    本论文提出了一个新颖的博弈论框架，用于联合规划和预测，以改进机器人动作的安全性。通过使用游戏理论的方法，该框架可以学习预测人类防范对策，并通过实际算法实现了端到端的模型训练。研究结果表明，该算法在人群导航和现实世界的行人运动数据集中可以产生更安全的规划。

    

    在存在人的情况下，安全规划机器人的运动需要可靠的对未来人类运动的预测。然而，简单地预测前期交互中最可能的动作并不能保证安全。这样的预测未能模拟出可能事件的长尾部分，这些事件在有限的数据集中很少被观察到。另一方面，为最坏情况下的动作进行规划会导致过分保守的行为和"僵化的机器人"。相反，我们的目标是学习预测人类防范对策的预测模型。我们提出了一个创新的游戏理论框架，用于联合规划和预测，以规划器与演示者之间的绩效作为收益，并提出了实际算法，以端对端的方式训练模型。我们证明了我们提出的算法在人群导航模拟器和现实世界的行人运动数据集中产生了更安全的计划。我们将代码发布在https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning。

    Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a ``frozen robot''. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.
    
[^14]: 在不进行对齐的情况下改进联合语音-文本表示

    Improving Joint Speech-Text Representations Without Alignment. (arXiv:2308.06125v1 [cs.CL])

    [http://arxiv.org/abs/2308.06125](http://arxiv.org/abs/2308.06125)

    本研究表明，在联合语音-文本表示中，忽略序列长度问题能够自然地实现一致的表示，一致性损失可以提高下游的字错误率。

    

    过去一年，基于跨模态表示空间的文本引导图像生成取得了令人瞩目的进展，其中文本和图像领域以联合的方式表示。在ASR中，这个想法被应用为联合语音-文本编码器，通过同时训练不匹配的语音和文本，可以扩展到非常大的参数模型的容量。虽然这些方法表现出了希望，但它们需要特殊处理语音和文本之间的序列长度不匹配问题，要么通过上采样启发式方法，要么通过一个显式的对齐模型。在这项工作中，我们提供了证据表明，联合语音-文本编码器通过忽略序列长度自然而然地实现了一致的表示，并且认为一致性损失可以弥补长度差异，并简单地假设最佳对齐。我们展示了这样的损失在大参数单语言和多语言系统中提高了下游的字错误率。

    The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
    
[^15]: 使用大型语言模型提升金融审计的零样本文本匹配

    Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])

    [http://arxiv.org/abs/2308.06111](http://arxiv.org/abs/2308.06111)

    这项研究提出了ZeroShotALI，它使用了一种新颖的推荐系统来改进金融审计中的零样本文本匹配。通过采用大型语言模型（LLM）和经过领域优化的基于transformer的文本匹配解决方案，该系统实现了从报告中推荐与法律要求相符的相关文本段落，并在现有方法上取得了显著的性能提升。

    

    审计金融文件是一个非常繁琐和耗时的过程。目前，通过使用基于人工智能的解决方案可以简化这一过程，以推荐与严格会计标准的法律要求相符的报告中的相关文本段落。然而，这些方法需要定期进行微调，并且通常在工业环境中缺乏大量的注释数据。因此，我们提出了ZeroShotALI，这是一个新颖的推荐系统，利用了最先进的大型语言模型（LLM）与领域特定的优化的基于transformer的文本匹配解决方案。我们发现，首先使用自定义BERT模型检索与法律要求相符的若干最佳匹配的文档部分，然后使用LLM对这些选择进行过滤，可以显著改善现有方法的性能。

    Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
    
[^16]: 神经对话模型及其控制方法：失败和修复的综述

    Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes. (arXiv:2308.06095v1 [cs.CL])

    [http://arxiv.org/abs/2308.06095](http://arxiv.org/abs/2308.06095)

    这项综述研究了以强大语言模型为基础的开放领域对话系统，并探讨了如何通过干预底层语言模型的不同方面，如数据、训练制度或解码，来保证模型的流畅性、信息丰富性、一致性、连贯性以及遵循社会准则的特性。

    

    最近，以强大语言模型为基础的条件语言模型能够以看似流利的方式延续任何类型的文本来源。这个事实促进了对基于强大语言模型的开放领域对话系统的研究，旨在通过生成适当的对话内容来模仿对话方的行为。然而，从语言学的角度来看，参与对话的复杂性很高。在这项综述中，我们从这一特定研究领域的角度解释了Grice的合作性对话最大规则，并将文献系统化地归纳为一个贡献何种内容是适当的方面：神经对话模型必须流畅、信息丰富、一致、连贯，并遵循社会准则。为了确保这些特性，最近的方法尝试在数据、训练制度或解码等各个干预点上控制底层语言模型。按照这些类别和干预点进行排序，我们讨论了一些有希望的方法。

    Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising at
    
[^17]: 强化逻辑规则学习用于时间点过程

    Reinforcement Logic Rule Learning for Temporal Point Processes. (arXiv:2308.06094v1 [cs.LG])

    [http://arxiv.org/abs/2308.06094](http://arxiv.org/abs/2308.06094)

    该论文提出了一个用于时间点过程的强化逻辑规则学习框架，利用逐步优化的方法扩展解释性的规则集来解释时间事件的发生。通过使用神经搜索策略和强化学习框架，可以高效地生成新的规则内容和权重。

    

    我们提出了一个框架，可以逐步扩展解释性的时间逻辑规则集，以解释时间事件的发生。利用时间点过程建模和学习框架，规则内容和权重将逐渐优化，直到观测事件序列的似然性最优。所提出的算法在当前规则集权重更新的主问题和搜索并包含最佳增加似然性的新规则的子问题之间交替进行。所制定的主问题是凸的，使用连续优化相对容易求解，而子问题则需要搜索巨大的组合规则谓词和关系空间。为解决这个挑战，我们提出了一个神经搜索策略，学习生成新规则内容的一系列动作。策略参数将使用强化学习框架进行端到端训练，其中奖励信号可以高效地得到。

    We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be effic
    
[^18]: 对协同过滤丢失函数的更好理解

    Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])

    [http://arxiv.org/abs/2308.06091](http://arxiv.org/abs/2308.06091)

    现有研究已经表明，通过改进对齐和均匀性设计的损失函数可以实现显著的性能提升。本文提出了一种新的损失函数，称为MAWU，它考虑了数据集的独特模式。

    

    协同过滤（CF）是现代推荐系统中的关键技术。CF模型的学习过程通常由三个组件组成：交互编码器、损失函数和负采样。尽管许多现有研究已经提出了各种CF模型来设计复杂的交互编码器，但最近的工作表明，简单地重新制定损失函数可以实现显著的性能提升。本文深入分析了现有损失函数之间的关系。我们的数学分析揭示了先前的损失函数可以解释为对齐和均匀性函数：（i）对齐匹配用户和物品表示，（ii）均匀性分散用户和物品分布。受到这个分析的启示，我们提出了一种改进对齐和均匀性设计的损失函数，考虑到数据集的独特模式，称为Margin-aware Alignment and Weighted Uniformity（MAWU）。MAWU的关键创新是

    Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
    
[^19]: 自传式探索算法创作中的解释性人工智能

    An Autoethnographic Exploration of XAI in Algorithmic Composition. (arXiv:2308.06089v1 [cs.SD])

    [http://arxiv.org/abs/2308.06089](http://arxiv.org/abs/2308.06089)

    本文是一项使用MeasureVAE生成音乐XAI模型进行自传式研究的探索，发现该模型在音乐创作工作流程中突显了训练数据集的音乐特征，并且显示出XAI模型可以在丰富和复杂的工作流程中发挥潜力。

    

    机器学习模型能够生成各种风格的复杂音乐，从民间音乐到古典音乐。然而，当前的生成音乐人工智能模型通常在意义上难以理解和控制。虽然研究已开始探索如何为音乐创建可解释的生成式人工智能（XAI）模型，但尚未研究过在音乐创作实践中的生成XAI模型。本文介绍了一项关于使用在爱尔兰民间音乐上训练的可解释潜在维度测量变分自编码器（MeasureVAE）生成音乐XAI模型的自传式研究。研究结果表明，音乐创作工作流程的探索性质突显了训练数据集的音乐特征，而非生成模型本身的特征。将XAI模型纳入迭代式工作流程中突显了XAI模型在更丰富、更复杂的工作流程中的潜力，而这些模型最初并不是为此设计的。

    Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
    
[^20]: 使用人工智能和大型语言模型评估学生实验错误：与人工评分比较研究

    Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters. (arXiv:2308.06088v1 [cs.AI])

    [http://arxiv.org/abs/2308.06088](http://arxiv.org/abs/2308.06088)

    本研究探究了大型语言模型（LLMs）在自动识别学生错误和简化教师评估中的潜力。使用65份学生实验方案的数据集，研发了基于GPT-3.5和GPT-4系列的人工智能（AI）系统，并与人工评分员进行测试。结果显示，AI系统在错误检测方面能够准确识别许多基本学生错误。

    

    辨别复杂、不完整甚至矛盾以及整体异质化的数据，如学生的实验方案，是具有挑战性的。鉴于当前评估方法的局限性，我们研究了大型语言模型（LLM）在自动识别学生错误和简化教师评估方面的潜力。我们的目标是为有效的个性化反馈打下基础。通过使用65份学生实验方案的数据集，我们开发了一个基于GPT-3.5和GPT-4系列的人工智能（AI）系统，并将其与人工评分员进行了测试。我们的结果表明，AI系统和人工评分员之间在错误检测方面的准确性存在不同水平。AI系统能够准确识别许多基本的学生错误，例如，当学生把假设重点放在独立变量而不是预期的观察上时，AI系统能够识别出来（准确度=0.90），当学生在进行中的调查中改变试验时。

    Identifying logical errors in complex, incomplete or even contradictory and overall heterogeneous data like students' experimentation protocols is challenging. Recognizing the limitations of current evaluation methods, we investigate the potential of Large Language Models (LLMs) for automatically identifying student errors and streamlining teacher assessments. Our aim is to provide a foundation for productive, personalized feedback. Using a dataset of 65 student protocols, an Artificial Intelligence (AI) system based on the GPT-3.5 and GPT-4 series was developed and tested against human raters. Our results indicate varying levels of accuracy in error detection between the AI system and human raters. The AI system can accurately identify many fundamental student errors, for instance, the AI system identifies when a student is focusing the hypothesis not on the dependent variable but solely on an expected observation (acc. = 0.90), when a student modifies the trials in an ongoing investi
    
[^21]: 在内存层次结构上具有MiRo的成本效益的设备上的持续学习

    Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])

    [http://arxiv.org/abs/2308.06053](http://arxiv.org/abs/2308.06053)

    这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。

    

    持续学习是从持续的任务流中逐步训练神经网络模型。为了记住先前学到的知识，之前的研究将旧样本存储在一个内存层次结构中，并在新任务到来时进行回放。采用持续学习以保护数据隐私的边缘设备通常对能源敏感，因此需要在不损害能源效率的情况下保持高模型准确度，即成本效益。我们的工作是首次探索基于层次内存回放的持续学习的设计空间，以获得在边缘设备上的成本效益。我们提出了Miro，一个新颖的系统运行时，通过使其能够根据资源状态动态配置持续学习系统，从而将我们的见解精确地整合到持续学习框架中，以实现最佳成本效益。为了实现这个目标，Miro还对带有明确准确度-能量平衡的参数进行在线分析，并以低开销地适应最佳值。广泛的评估显示Miro明显优于其他方案。

    Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
    
[^22]: 学习通过个性化的大型语言模型指导人类专家

    Learning to Guide Human Experts via Personalized Large Language Models. (arXiv:2308.06039v1 [cs.AI])

    [http://arxiv.org/abs/2308.06039](http://arxiv.org/abs/2308.06039)

    本论文提出了学习引导（LTG）框架，通过个性化的大型语言模型为人类专家提供有助于指导决策的指导，解决了机器决策和人类决策之间的依赖问题。利用SLOG实现，该框架在医学诊断任务上取得了初步但有希望的结果。

    

    在学习推迟的过程中，一个预测器识别出风险决策并将它们推迟给人类专家。这种设置的一个关键问题是，由于锚定偏见，专家可能会过分依赖于机器的决策。同时，每当机器选择推迟选项时，专家必须完全自主地做出决策。为了解决这个问题，我们提出了学习引导（LTG）的另一种框架，在这个框架中，机器不是提供现成的决策，而是提供有助于指导决策的指导，并且人类完全负责做出决策。我们还介绍了SLOG，这是一个LTG实现，利用（少量）人类监督将通用大型语言模型转化为能够生成文本指导的模块，并在医学诊断任务上展示了初步但有希望的结果。

    In learning to defer, a predictor identifies risky decisions and defers them to a human expert. One key issue with this setup is that the expert may end up over-relying on the machine's decisions, due to anchoring bias. At the same time, whenever the machine chooses the deferral option the expert has to take decisions entirely unassisted. As a remedy, we propose learning to guide (LTG), an alternative framework in which -- rather than suggesting ready-made decisions -- the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision. We also introduce SLOG, an LTG implementation that leverages (a small amount of) human supervision to convert a generic large language model into a module capable of generating textual guidance, and present preliminary but promising results on a medical diagnosis task.
    
[^23]: 深度上下文兴趣网络用于点击率预测

    Deep Context Interest Network for Click-Through Rate Prediction. (arXiv:2308.06037v1 [cs.IR])

    [http://arxiv.org/abs/2308.06037](http://arxiv.org/abs/2308.06037)

    这篇论文提出了一种名为深度上下文兴趣网络（DCIN）的模型，该模型通过完整地建模点击及其展示上下文来学习用户的上下文感知兴趣，以提高点击率预测性能。

    

    点击率（CTR）预测是在线广告等工业应用中的关键问题，它估计用户点击某个项目的概率。许多研究致力于用户行为建模以提高CTR预测性能，但大多数方法只从用户点击项目中建模用户的正向兴趣，忽略了展示项目周围的上下文信息，导致性能较差。本文强调了上下文信息对用户行为建模的重要性，并提出了一个名为深度上下文兴趣网络（DCIN）的新模型，该模型通过完整地建模点击及其展示上下文来学习用户的上下文感知兴趣。DCIN包括三个关键模块：1）位置感知上下文聚合模块（PCAM），通过注意机制对展示项目进行聚合；2）反馈-上下文融合模块（FCFM），通过非线性函数将点击和展示上下文的表示进行融合；

    Click-Through Rate (CTR) prediction, estimating the probability of a user clicking on an item, is essential in industrial applications, such as online advertising. Many works focus on user behavior modeling to improve CTR prediction performance. However, most of those methods only model users' positive interests from users' click items while ignoring the context information, which is the display items around the clicks, resulting in inferior performance. In this paper, we highlight the importance of context information on user behavior modeling and propose a novel model named Deep Context Interest Network (DCIN), which integrally models the click and its display context to learn users' context-aware interests. DCIN consists of three key modules: 1) Position-aware Context Aggregation Module (PCAM), which performs aggregation of display items with an attention mechanism; 2) Feedback-Context Fusion Module (FCFM), which fuses the representation of clicks and display contexts through non-li
    
[^24]: 多模态大语言模型在预测语言处理期间表现出人类视觉-语言集成的证据

    Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])

    [http://arxiv.org/abs/2308.06035](http://arxiv.org/abs/2308.06035)

    这篇论文研究了多模态大语言模型（mLLMs）在预测语言处理过程中与人类的视觉-语言集成能力是否一致的问题，并通过实验验证了mLLMs的多模态输入方法可以减少认知负荷，提高感知和理解能力。

    

    大语言模型（LLMs）的先进语言处理能力引发了关于它们是否能够复制人类认知过程的争议。LLMs和人类在语言处理方面的一个区别在于，语言输入通常建立在多个知觉模态上，而大多数LLMs仅处理基于文本的信息。多模态基础使人类能够整合视觉背景与语言信息，从而对即将出现的单词的空间施加限制，减少认知负荷，提高感知和理解能力。最近的多模态LLMs（mLLMs）结合了视觉和语言嵌入空间，并使用变压器类型的注意机制进行下一个单词的预测。在多大程度上，基于多模态输入的预测语言处理在mLLMs和人类中吻合？为了回答这个问题，200名被试观看了短的视听剪辑，并估计了即将出现的动词或名词的可预测性。

    The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
    
[^25]: 加密货币证券案件中的大型语言模型：ChatGPT能否取代律师？

    Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])

    [http://arxiv.org/abs/2308.06032](http://arxiv.org/abs/2308.06032)

    本研究探讨了在加密货币证券案件中，大型语言模型（LLMs）是否能够准确判断违法行为，并比较了由LLM和律师撰写的投诉书对陪审团决策的影响。研究发现，目前的LLMs在法律推理方面表现较弱，但随着未来模型的改进，其潜力有望提升。

    

    大型语言模型（LLMs）可以增强对法律系统的访问。然而，关于它们在进行法律任务方面的有效性的实证研究非常有限。我们研究涉及加密货币的证券案件，作为AI可以支持法律过程的众多情境之一，研究LLMs的法律推理和起草能力。我们检查以下两个方面：a）LLM能否准确确定事实模式中可能存在的违法行为，b）基于LLM和律师撰写的投诉书，陪审团的决策是否有所差异。我们将真实案例中的事实模式输入GPT-3.5，并评估其确定正确潜在违法行为并排除虚假违法行为的能力。其次，我们请模拟陪审员评估LLM和律师撰写的投诉书。GPT-3.5的法律推理能力较弱，但我们预期未来模型的改进，特别是考虑到它建议的违法行为往往是正确的（它仅仅过于保守）。

    Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
    
[^26]: 不可观测的驱动源下控制角色动作的方法

    Controlling Character Motions without Observable Driving Source. (arXiv:2308.06025v1 [cs.AI])

    [http://arxiv.org/abs/2308.06025](http://arxiv.org/abs/2308.06025)

    本文提出了一个系统框架，结合了VQ-VAE和使用强化学习训练的一种新型基于令牌级控制策略，通过在顶部注入高级先验模型来生成无限长且多样化的序列。

    

    如何在没有任何驱动源的情况下生成多样化、逼真且无限长的头部/身体序列？我们认为这个未经充分探究的研究问题并不是一件轻松的事，并且存在着独特的技术挑战。在没有来自驱动源的语义约束的情况下，使用标准的自回归模型生成无限长序列很容易导致以下问题：1）由于累积误差产生了超出分布（OOD）问题；2）生成的运动序列缺乏多样性，无法产生逼真和生动的动作序列；3）时间上出现了不期望的周期性模式。为了解决上述挑战，我们提出了一个系统框架，将VQ-VAE的优点与使用经过精心设计的奖励函数训练的一种新型基于令牌级控制策略相结合，通过在顶部注入高级先验模型来生成无限长且多样化的序列。尽管我们现在关注于没有驱动源，但我们的框架可以推广到受控合成场景。

    How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthe
    
[^27]: 优化单GPU训练的基于transformer的机器翻译模型：超参数消融研究

    Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study. (arXiv:2308.06017v1 [cs.CL])

    [http://arxiv.org/abs/2308.06017](http://arxiv.org/abs/2308.06017)

    本研究通过消融实验发现，在单个GPU上，参数数量最多的组合并不一定是最有效的，通过减少参数大小可以在不降低翻译质量的情况下训练复杂模型，揭示了超参数选择、模型大小和计算资源需求之间的关系。

    

    在机器翻译任务中，模型复杂性和性能之间的关系通常被认为是线性的，驱动参数数量增加并对多个GPU等计算资源提出要求。为了探索这一假设，本研究通过超参数消融系统地研究了一个基于序列到序列的机器翻译管道在单个NVIDIA A100 GPU上的影响。与预期相反，我们的实验发现具有最多参数的组合未必是最有效的。这一意外的发现促使我们仔细减少参数大小，揭示了能够在单个GPU上训练复杂模型而不损害翻译质量的“甜点”。这些发现展示了超参数选择、模型大小和计算资源需求之间的复杂关系。这项研究的见解对于努力进行机器翻译的改进工作有所贡献。

    In machine translation tasks, the relationship between model complexity and performance is often presumed to be linear, driving an increase in the number of parameters and consequent demands for computational resources like multiple GPUs. To explore this assumption, this study systematically investigates the effects of hyperparameters through ablation on a sequence-to-sequence machine translation pipeline, utilizing a single NVIDIA A100 GPU. Contrary to expectations, our experiments reveal that combinations with the most parameters were not necessarily the most effective. This unexpected insight prompted a careful reduction in parameter sizes, uncovering "sweet spots" that enable training sophisticated models on a single GPU without compromising translation quality. The findings demonstrate an intricate relationship between hyperparameter selection, model size, and computational resource needs. The insights from this study contribute to the ongoing efforts to make machine translation m
    
[^28]: 大型语言模型在电信行业的未来影响

    Large Language Models for Telecom: Forthcoming Impact on the Industry. (arXiv:2308.06013v1 [cs.IT])

    [http://arxiv.org/abs/2308.06013](http://arxiv.org/abs/2308.06013)

    大型语言模型在电信行业将产生重要的影响。它们可以提高运营效率，简化任务，并需要解决使用中的挑战。

    

    大型语言模型（LLMs）已经成为一股变革的力量，不仅在自然语言处理（NLP）的传统领域之外，还在许多领域引起了革命性的关注。随着LLM技术的不断发展，电信行业面临着潜在影响的前景。为了阐明这些影响，我们深入研究LLMs的内部机制，提供了关于它们目前的能力和局限性的见解。我们还研究了在电信行业可以方便实施的使用案例，简化了目前妨碍运营效率并需要大量人力和工程专业知识的任务。此外，我们还揭示了在电信领域利用LLMs所面临的独特挑战的重要研究方向。解决这些挑战是充分利用LLMs潜力和发挥其能力的重要进展。

    Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fulles
    
[^29]: 深度任务特定的底层表示网络用于多任务推荐系统

    Deep Task-specific Bottom Representation Network for Multi-Task Recommendation. (arXiv:2308.05996v1 [cs.AI])

    [http://arxiv.org/abs/2308.05996](http://arxiv.org/abs/2308.05996)

    本文提出了一种深度任务特定的底层表示网络（DTRN），用于解决多任务推荐系统中的负迁移问题，通过明确获取每个任务的底层表示来改善任务特定特征的捕捉能力。

    

    基于神经网络的多任务学习在推荐系统中取得了显著的改进，并成功地应用于推荐系统。最近的深度多任务学习方法（如MMoE、PLE）专注于设计基于软门控的参数共享网络，隐式地学习每个任务的泛化表示。然而，当处理冲突任务时，多任务学习方法可能会遭受性能退化，因为负迁移效应可能发生在任务共享的底层表示上。这可能导致多任务学习方法捕捉任务特定特征的能力降低，最终影响其效果，并妨碍其在所有任务上的泛化能力。在本文中，我们专注于推荐系统中多任务学习的底层表示学习，并提出了深度任务特定的底层表示网络（DTRN）以缓解负迁移问题。DTRN通过使每个任务具有自己的表示学习明确地获取任务特定的底层表示。

    Neural-based multi-task learning (MTL) has gained significant improvement, and it has been successfully applied to recommendation system (RS). Recent deep MTL methods for RS (e.g. MMoE, PLE) focus on designing soft gating-based parameter-sharing networks that implicitly learn a generalized representation for each task. However, MTL methods may suffer from performance degeneration when dealing with conflicting tasks, as negative transfer effects can occur on the task-shared bottom representation. This can result in a reduced capacity for MTL methods to capture task-specific characteristics, ultimately impeding their effectiveness and hindering the ability to generalize well on all tasks. In this paper, we focus on the bottom representation learning of MTL in RS and propose the Deep Task-specific Bottom Representation Network (DTRN) to alleviate the negative transfer problem. DTRN obtains task-specific bottom representation explicitly by making each task has its own representation learni
    
[^30]: 一体化音频：利用WavLM预训练模型进行基于语音驱动的手势合成

    Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model. (arXiv:2308.05995v1 [cs.SD])

    [http://arxiv.org/abs/2308.05995](http://arxiv.org/abs/2308.05995)

    本文介绍了一种利用WavLM预训练模型的基于语音驱动的手势合成方法，实现只使用原始语音音频生成个性化全身手势，消除了复杂的多模态处理和手动注释的需求。

    

    在虚拟人创作领域，生成与语音配套的手势是一个正在兴起的研究方向。先前的研究通过以声学和语义信息作为输入，采用分类方法来识别人物的ID和情感，以驱动与语音配套的手势生成。然而，这项工作仍然面临着重大挑战。这些挑战不仅涉及手势、语音声学和语义之间错综复杂的相互作用，还包括与个性、情感和其他不明确但重要的因素相关的复杂性。本文介绍了“diffmotion-v2”，这是一种基于语音条件扩散和基于非自回归Transformer的生成模型，采用WavLM预训练模型。它可以仅使用原始语音音频生成个性化的全身手势，消除了复杂的多模态处理和手动注释的需求。

    The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acou
    
[^31]: 运动者轨迹预测模型的鲁棒性验证：TrajPAC

    TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models. (arXiv:2308.05985v1 [cs.AI])

    [http://arxiv.org/abs/2308.05985](http://arxiv.org/abs/2308.05985)

    本研究提出了一个名为TrajPAC的原型工具，它采用可能近似正确 (PAC) 的框架来验证行人轨迹预测模型的鲁棒性。通过定义标签鲁棒性和纯鲁棒性，并考虑干扰区间中所有点的鲁棒性，TrajPAC不仅可以识别潜在的反例，还可以提供可解释的分析结果。

    

    鲁棒的行人轨迹预测对于开发安全的自动驾驶车辆至关重要。尽管先前的研究已经研究了轨迹预测中的对抗鲁棒性，但仍存在一些重要的问题尚未解决。本研究试图解决这些关键问题。首先，先前的轨迹预测鲁棒性定义模糊不清，因此我们提供了两种鲁棒性的形式定义，即标签鲁棒性和纯鲁棒性。其次，由于先前的研究未能考虑干扰区间中所有点的鲁棒性，我们采用了一个可能近似正确 (PAC) 的框架进行鲁棒性验证。此外，该框架不仅可以识别潜在的反例，还可以对原始方法进行可解释的分析。我们使用一个名为TrajPAC的原型工具来应用我们的方法。通过TrajPAC，我们评估了四种最先进的轨迹预测模型的鲁棒性- Trajectron++，Mem...

    Robust pedestrian trajectory forecasting is crucial to developing safe autonomous vehicles. Although previous works have studied adversarial robustness in the context of trajectory forecasting, some significant issues remain unaddressed. In this work, we try to tackle these crucial problems. Firstly, the previous definitions of robustness in trajectory prediction are ambiguous. We thus provide formal definitions for two kinds of robustness, namely label robustness and pure robustness. Secondly, as previous works fail to consider robustness about all points in a disturbance interval, we utilise a probably approximately correct (PAC) framework for robustness verification. Additionally, this framework can not only identify potential counterexamples, but also provides interpretable analyses of the original methods. Our approach is applied using a prototype tool named TrajPAC. With TrajPAC, we evaluate the robustness of four state-of-the-art trajectory prediction models -Trajectron++, Mem
    
[^32]: 多智能体优化解决方案的对比解释

    Contrastive Explanations of Multi-agent Optimization Solutions. (arXiv:2308.05984v1 [cs.AI])

    [http://arxiv.org/abs/2308.05984](http://arxiv.org/abs/2308.05984)

    本研究提出了MAoE，一种用于多智能体优化问题的对比解释方法。该方法通过生成新的解决方案并突出显示与初始解决方案的差异，帮助智能体理解为什么初始解决方案优于他们的期望。

    

    在许多现实世界的场景中，智能体参与优化问题。由于大多数情况都是超约束的，最优解并不总能满足所有智能体的要求。有些智能体可能不满意，并提出类似“为什么解决方案S不满足属性P？”的问题。在本文中，我们提出了MAoE，这是一种领域无关的方法，通过（i）生成一个新的解决方案S'，该解决方案强制满足属性P，同时最小化S和S'之间的差异；以及（ii）突出显示这两个解决方案之间的差异。这样的解释旨在帮助智能体理解为什么初始解决方案优于他们的期望。我们进行了计算评估，表明MAoE可以为大型多智能体优化问题生成对比解释。我们还在四个不同的领域进行了广泛的用户研究，结果显示，在提供这些解释后，用户对解决方案的理解有所改善。

    In many real-world scenarios, agents are involved in optimization problems. Since most of these scenarios are over-constrained, optimal solutions do not always satisfy all agents. Some agents might be unhappy and ask questions of the form ``Why does solution $S$ not satisfy property $P$?''. In this paper, we propose MAoE, a domain-independent approach to obtain contrastive explanations by (i) generating a new solution $S^\prime$ where the property $P$ is enforced, while also minimizing the differences between $S$ and $S^\prime$; and (ii) highlighting the differences between the two solutions. Such explanations aim to help agents understanding why the initial solution is better than what they expected. We have carried out a computational evaluation that shows that MAoE can generate contrastive explanations for large multi-agent optimization problems. We have also performed an extensive user study in four different domains that shows that, after being presented with these explanations, h
    
[^33]: 通过受限频率的身份不可知攻击进行人脸加密

    Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v1 [cs.CV])

    [http://arxiv.org/abs/2308.05983](http://arxiv.org/abs/2308.05983)

    通过受限频率的身份不可知攻击进行人脸加密，解决了使用外部扰动加密人脸图像的隐私保护问题，并提出了一种弱黑盒场景下可行的解决方案。

    

    每天有数十亿人在社交媒体上分享他们的日常照片。然而，恶意采集者利用深度人脸识别系统轻松地从这些图片中窃取他们的生物特征信息（例如人脸）。一些研究正在进行中，通过引入难以察觉的扰动来生成加密人脸照片，以减少人脸信息泄漏。然而，现有的研究需要更强的黑盒场景可行性和更自然的视觉外观，这对隐私保护的可行性构成了挑战。为了解决这些问题，我们提出了一种受限频率的身份不可知（FRIA）框架，以从未经授权的人脸识别中加密人脸图像，而无需访问个人信息。对于弱黑盒场景的可行性，我们观察到多个人脸识别模型中的平均特征表示相似，因此我们提出利用通过互联网爬取的数据集中的平均特征作为t

    Billions of people are sharing their daily live images on social media everyday. However, malicious collectors use deep face recognition systems to easily steal their biometric information (e.g., faces) from these images. Some studies are being conducted to generate encrypted face photos using adversarial attacks by introducing imperceptible perturbations to reduce face information leakage. However, existing studies need stronger black-box scenario feasibility and more natural visual appearances, which challenge the feasibility of privacy protection. To address these problems, we propose a frequency-restricted identity-agnostic (FRIA) framework to encrypt face images from unauthorized face recognition without access to personal information. As for the weak black-box scenario feasibility, we obverse that representations of the average feature in multiple face recognition models are similar, thus we propose to utilize the average feature via the crawled dataset from the Internet as the t
    
[^34]: CyberForce: 一个用于恶意软件缓解的联邦强化学习框架

    CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])

    [http://arxiv.org/abs/2308.05978](http://arxiv.org/abs/2308.05978)

    CyberForce是一个联邦强化学习框架，用于在物联网设备中协同私密地确定适合缓解各种零日攻击的MTD技术。它整合了设备指纹识别和异常检测，并通过奖励或惩罚FRL agent选择的MTD机制来提高网络安全性。

    

    互联网物联网(IoT)范例的扩展是不可避免的，但是对于IoT设备对恶意软件事件的脆弱性已成为一个越来越关注的问题。最近的研究显示，将强化学习与移动目标防御(MTD)机制相结合，可以增强IoT设备的网络安全性。然而，大量的新恶意软件攻击和代理人学习和选择有效的MTD技术所需的时间使得这种方法在现实世界的IoT场景中不切实际。为解决这个问题，本研究提出了CyberForce，一个采用联邦强化学习(FRL)的框架，用于集体且保密地确定适合缓解各种零日攻击的MTD技术。CyberForce结合了设备指纹识别和异常检测，通过奖励或惩罚FRL agent选择的MTD机制。该框架在一个由十台真实IoT平台设备组成的联邦中进行了评估。通过六个恶意软件样本进行了一系列实验。

    The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samp
    
[^35]: 使用Viterbi算法和迁移学习进行推特情绪提取

    Tweet Sentiment Extraction using Viterbi Algorithm with Transfer Learning. (arXiv:2308.05973v1 [cs.AI])

    [http://arxiv.org/abs/2308.05973](http://arxiv.org/abs/2308.05973)

    这篇论文使用Viterbi算法和迁移学习来提取推特情绪，引入置信度分数和向量作为评估模型的指标，进行模型的内部评估和微调。

    

    推特情绪提取是提取句子中最重要部分的过程，判断情绪是积极还是消极。本研究旨在识别推特句子中引起情感的部分。为了达到这个目标，我们继续改进作者之前修改的Viterbi算法，使其能够接收预训练的模型参数。我们引入置信度分数和向量作为两个指标，用于在评估最终结果之前对模型进行内部评估。然后，我们介绍了一种微调这个非参数模型的方法。我们发现，随着置信度分数向量准确地显示出最不自信的预测状态的位置，以及修改是否改善了置信度分数或微调是否朝着错误的方向进行，模型变得更易解释。

    Tweet sentiment extraction extracts the most significant portion of the sentence, determining whether the sentiment is positive or negative. This research aims to identify the part of tweet sentences that strikes any emotion. To reach this objective, we continue improving the Viterbi algorithm previously modified by the author to make it able to receive pre-trained model parameters. We introduce the confidence score and vector as two indicators responsible for evaluating the model internally before assessing the final results. We then present a method to fine-tune this nonparametric model. We found that the model gets highly explainable as the confidence score vector reveals precisely where the least confidence predicted states are and if the modifications approved ameliorate the confidence score or if the tuning is going in the wrong direction.
    
[^36]: 基于区块链的基金会模型系统的去中心化治理：探讨区块链在负责任的人工智能中的作用。

    Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI. (arXiv:2308.05962v1 [cs.SE])

    [http://arxiv.org/abs/2308.05962](http://arxiv.org/abs/2308.05962)

    本文探讨了基于基金会模型的人工智能系统在整个生命周期中所面临的治理挑战，并提出了利用区块链实现去中心化治理的架构。

    

    基金会模型因其卓越的能力和潜力在全球范围内越来越受到关注，能够执行各种任务。然而，人们担心基于基金会模型的人工智能系统是否得到了适当的治理，以确保其可信度，并防止可能对人类、社会和环境造成伤害的滥用。在本文中，我们确定了基金会模型人工智能系统在整个生命周期中面临的八个治理挑战，涉及治理的三个基本维度：决策权、激励机制和问责制。此外，我们探讨了区块链作为解决这些挑战的潜力，通过提供分布式账本来促进去中心化的治理。我们提出了一个架构，演示了如何利用区块链实现基金会模型人工智能系统的治理。

    Foundation models are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. Nevertheless, people are concerned about whether foundation model based AI systems are properly governed to ensure trustworthiness of foundation model based AI systems and to prevent misuse that could harm humans, society and the environment. In this paper, we identify eight governance challenges in the entire lifecycle of foundation model based AI systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. Furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. We present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based AI systems.
    
[^37]: BOLAA:基准测试和编排LLM增强的自治代理

    BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents. (arXiv:2308.05960v1 [cs.AI])

    [http://arxiv.org/abs/2308.05960](http://arxiv.org/abs/2308.05960)

    BOLAA通过提供对LAA的综合比较，并引入一种新的编排策略，为设计LAA架构和优化代理编排策略提供了量化建议。

    

    大语言模型（LLMs）的巨大成功鼓励了对LLM增强自治代理（LAAs）的新兴探索。LAAs能够借助其核心LLM生成动作并与环境交互，通过对过去的交互（如观察和动作）进行条件调节，以解决复杂任务。由于对LAA的研究仍然非常新颖，可用的探索有限。因此，我们综合比较了LAA在代理架构和LLM背景方面。此外，我们提出了一种新的策略，以按类型将多个LAAs编排在一起，即BOLAA，其中控制器管理多个代理之间的通信。我们在决策和多步推理环境中进行模拟，全面验证了LAAs的能力。我们的性能结果为设计LAA架构和优化代理编排策略提供了量化建议。

    The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal
    
[^38]: FoodSAM: 任何食物分割

    FoodSAM: Any Food Segmentation. (arXiv:2308.05938v1 [cs.CV])

    [http://arxiv.org/abs/2308.05938](http://arxiv.org/abs/2308.05938)

    本文介绍了FoodSAM，它是一个集成了粗糙语义掩膜和SAM生成掩膜的新型框架，可以提高食物图像分割的质量。同时，它还能进行实例分割、全景分割和可提示分割，是一个全面的食物分割解决方案。

    

    本文探讨了Segment Anything Model (SAM) 在食物图像分割中的零样本能力。为了解决SAM生成的掩膜中缺乏特定类别信息的问题，我们提出了一种新的框架，称为FoodSAM。这种创新的方法将粗糙的语义掩膜与SAM生成的掩膜结合起来，提高语义分割质量。此外，我们意识到食物中的成分可以被视为独立的个体，因此我们对食物图像进行了实例分割。此外，FoodSAM将其零样本能力扩展到全景分割，并结合了物体检测器，使FoodSAM能够有效捕捉非食物对象信息。受到提示分割的最近成功启发，我们还将FoodSAM扩展到了可提示分割，支持各种提示变体。因此，FoodSAM成为一个全面的解决方案，能够在多个层面上分割食物物品。

    In this paper, we explore the zero-shot capability of the Segment Anything Model (SAM) for food image segmentation. To address the lack of class-specific information in SAM-generated masks, we propose a novel framework, called FoodSAM. This innovative approach integrates the coarse semantic mask with SAM-generated masks to enhance semantic segmentation quality. Besides, we recognize that the ingredients in food can be supposed as independent individuals, which motivated us to perform instance segmentation on food images. Furthermore, FoodSAM extends its zero-shot capability to encompass panoptic segmentation by incorporating an object detector, which renders FoodSAM to effectively capture non-food object information. Drawing inspiration from the recent success of promptable segmentation, we also extend FoodSAM to promptable segmentation, supporting various prompt variants. Consequently, FoodSAM emerges as an all-encompassing solution capable of segmenting food items at multiple levels 
    
[^39]: 一种用于智能自动缩放无服务器函数的深度循环强化学习方法

    A Deep Recurrent-Reinforcement Learning Method for Intelligent AutoScaling of Serverless Functions. (arXiv:2308.05937v1 [cs.DC])

    [http://arxiv.org/abs/2308.05937](http://arxiv.org/abs/2308.05937)

    该论文介绍了一种用于智能自动缩放无服务器函数的深度循环强化学习方法，针对波动的工作负载和严格的性能约束，通过建立一个适应性策略来实现最大化期望目标。

    

    函数即服务（FaaS）引入了一种轻量级的基于函数的云执行模型，在物联网边缘数据处理和异常检测等应用中具有相关性。虽然云服务提供商提供了几乎无限的函数弹性，但这些应用经常遇到波动的工作负载和更严格的性能约束。典型的云服务提供商策略是根据基于监控的阈值（如CPU或内存）来经验性地确定和调整所需的函数实例以适应需求和性能，即"自动缩放"。然而，阈值配置要么需要专家知识，要么需要历史数据或对环境的完整视图，使得自动缩放成为缺乏适应性解决方案的性能瓶颈。强化学习算法已被证明在分析复杂的云环境中是有益的，并产生适应性策略以最大化期望目标。

    Function-as-a-Service (FaaS) introduces a lightweight, function-based cloud execution model that finds its relevance in applications like IoT-edge data processing and anomaly detection. While CSP offer a near-infinite function elasticity, these applications often experience fluctuating workloads and stricter performance constraints. A typical CSP strategy is to empirically determine and adjust desired function instances, "autoscaling", based on monitoring-based thresholds such as CPU or memory, to cope with demand and performance. However, threshold configuration either requires expert knowledge, historical data or a complete view of environment, making autoscaling a performance bottleneck lacking an adaptable solution.RL algorithms are proven to be beneficial in analysing complex cloud environments and result in an adaptable policy that maximizes the expected objectives. Most realistic cloud environments usually involve operational interference and have limited visibility, making them
    
[^40]: LittleMu：通过异构数据源整合和教学提示链路部署在线虚拟助教

    LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts. (arXiv:2308.05935v1 [cs.CL])

    [http://arxiv.org/abs/2308.05935](http://arxiv.org/abs/2308.05935)

    本文提出了一个虚拟的MOOC助教 LittleMu，通过整合异构数据源和教学提示链路来支持广泛范围的准确回答和知识相关的闲聊服务。

    

    在教育的漫长历史中，助教在学习中发挥了重要作用。然而，由于真实在线教育场景的复杂性和缺乏训练数据，很少有MOOC平台提供人工或虚拟助教来支持大量在线学生的学习。在本文中，我们提出了一个虚拟的MOOC助教LittleMu，仅使用少量标注训练数据，提供问题回答和闲聊服务。LittleMu由两个交互模块组成，包括异构检索和语言模型提示，首先整合结构化、半结构化和非结构化的知识源，支持广泛范围的问题的准确回答。然后，我们设计了名为“Chain of Teach”提示的精心示范，利用大规模预训练模型处理复杂的未收集问题。除了问题回答，我们还开发了其他教育服务，如知识相关的闲聊。我们通过机器人测试系统的性能。

    Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. We test the system's performance via bot
    
[^41]: 学习基于团队导航：深度强化学习技术在多智能体路径规划中的综述

    Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding. (arXiv:2308.05893v1 [cs.AI])

    [http://arxiv.org/abs/2308.05893](http://arxiv.org/abs/2308.05893)

    本文综述了在多智能体路径规划中深度强化学习技术的应用。与其他研究不同，我们重点介绍了DRL方法在MAPF中的整合，并解决了MAPF解决方案评估指标缺乏统一性的问题。我们讨论了基于模型的DRL作为未来发展方向，并提供了解决MAPF当前挑战所需的基础理解。

    

    多智能体路径规划(MAPF)是许多大规模机器人应用中的关键领域，通常是多智能体系统的基本步骤。然而，在复杂和拥挤的环境中，MAPF的复杂性不断增加，已有解决方案的有效性严重降低。与其他研究不同，我们在本综述论文中重点介绍了DRL方法在MAPF中的应用。此外，我们旨在填补目前在评估MAPF解决方案方面的缺口，通过解决缺乏统一评估指标的问题并对这些指标进行全面阐释。最后，我们的论文讨论了作为未来方向的基于模型的DRL的潜力，并提供了必要的基础理解以应对MAPF中的当前挑战。

    Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our o
    
[^42]: DF2: 无分布的决策焦点学习

    DF2: Distribution-Free Decision-Focused Learning. (arXiv:2308.05889v1 [cs.LG])

    [http://arxiv.org/abs/2308.05889](http://arxiv.org/abs/2308.05889)

    DF2是一种无分布的决策焦点学习方法，特别解决了模型不匹配错误、样本平均逼近误差和梯度逼近误差三个瓶颈问题。

    

    最近决策焦点学习（DFL）作为一种强大的方法在解决预测-优化问题时，通过将预测模型定制到一个下游优化任务。然而，现有的端到端DFL方法受到三个重要瓶颈的制约：模型不匹配错误、样本平均逼近误差和梯度逼近误差。模型不匹配错误源于模型参数化的预测分布与真实概率分布之间的不协调。样本平均逼近误差是使用有限样本来近似期望优化目标时产生的。梯度逼近误差发生在DFL依靠KKT条件进行精确梯度计算时，而大多数方法在非凸目标中近似梯度进行反向传播。在本文中，我们提出DF2 - 第一个明确设计来解决这三个瓶颈的无分布决策焦点学习方法。

    Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending 
    
[^43]: 面向异构 SoC 的共享内存竞争感知的并发 DNN 执行

    Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips. (arXiv:2308.05869v1 [cs.DC])

    [http://arxiv.org/abs/2308.05869](http://arxiv.org/abs/2308.05869)

    这项工作提出了一种名为 HaX-CoNN 的方案，可以实现在异构 SoC 上并发执行 DNN 推理工作负载，并考虑了共享内存竞争和加速器之间的转换以找到最优的调度方案。

    

    当前移动和自主系统的两个显著特点是：1）通常同时连续运行多个工作负载，主要是深度神经网络 (DNN) 推理；2）它们在嵌入了针对特定操作的异构加速器的共享内存系统芯片上运行。目前的技术缺乏高效的性能和资源管理技术，无法最大化总系统吞吐量或最小化端到端工作负载延迟。在本研究中，我们提出了一种名为 HaX-CoNN 的新方案，将并发执行的 DNN 推理工作负载的层进行特征化和映射到 SoC 中的多样加速器。我们的方案独特地考虑了每层的执行特性、共享内存 (SM) 的竞争以及加速器之间的转换，以找到最优的调度方案。我们在 NVIDIA Orin、NVIDIA Xavier 和 Qualcomm Snapdragon 865 SoC 上评估了 HaX-CoNN。我们的实验结果表明，HaX-CoNN 可以最小化

    Two distinguishing features of state-of-the-art mobile and autonomous systems are 1) there are often multiple workloads, mainly deep neural network (DNN) inference, running concurrently and continuously; and 2) they operate on shared memory system-on-chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art lacks efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose HaX-CoNN, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within a SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find optimal schedules. We evaluate HaX-CoNN on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that HaX-CoNN minimiz
    
[^44]: 在泛癌症腹部器官数量化中释放未标记数据的力量：FLARE22挑战

    Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge. (arXiv:2308.05862v1 [eess.IV])

    [http://arxiv.org/abs/2308.05862](http://arxiv.org/abs/2308.05862)

    本论文介绍了FLARE22挑战，旨在评估速度快、资源少、准确、节约注释成本且具有泛化能力的AI算法在腹部器官分析方面的表现。通过使用50个标记扫描和2000个未标记扫描，一组AI算法达到了90.0\%的中位数Dice相似系数（DSC），显著降低了注释需求。

    

    定量器官评估是自动化腹部疾病诊断和治疗计划的关键步骤。人工智能（AI）已经展现出自动化这一过程的巨大潜力。然而，大多数现有的AI算法依赖于许多专家注释，并且在真实世界多国设置中缺乏准确性和效率的综合评估。为了克服这些限制，我们组织了迄今为止最大规模的腹部器官分析挑战——FLARE 2022挑战，旨在评估速度快、资源少、准确、节约注释成本且具有泛化能力的AI算法。我们从50多个医疗团队构建了一个跨洲际和跨国的数据集，包括具有不同种族、疾病、阶段和制造商的计算机断层扫描（CT）。我们独立验证了一组AI算法通过使用50个标记扫描和2000个未标记扫描达到了90.0\%的中位数Dice相似系数（DSC），这可以显著减少注释需求。

    Quantitative organ assessment is an essential step in automated abdominal disease diagnosis and treatment planning. Artificial intelligence (AI) has shown great potential to automatize this process. However, most existing AI algorithms rely on many expert annotations and lack a comprehensive evaluation of accuracy and efficiency in real-world multinational settings. To overcome these limitations, we organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. We constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers. We independently validated that a set of AI algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0\% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation req
    
[^45]: 在条件独立图上的知识传播

    Knowledge Propagation over Conditional Independence Graphs. (arXiv:2308.05857v1 [cs.AI])

    [http://arxiv.org/abs/2308.05857](http://arxiv.org/abs/2308.05857)

    这项工作提出了在条件独立图上进行知识传播的算法，并通过在Cora和PubMed数据集上的实验证明了其优于现有方法的效果。

    

    条件独立（CI）图是一种特殊类型的概率图模型（PGM），其中特征连接使用无向图建模，边权重表示特征之间的部分相关性强度。由于CI图捕捉了特征之间的直接依赖关系，它们在研究社区中引起了越来越多的关注，特别是在发现领域拓扑方面。在这项工作中，我们提出了在CI图上执行知识传播的算法。我们的实验证明，我们的技术在公开的Cora和PubMed数据集上超过了最先进的方法。

    Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
    
[^46]: 使用领域随机化和目标跟踪神经网络进行种子核计数

    Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks. (arXiv:2308.05846v1 [cs.CV])

    [http://arxiv.org/abs/2308.05846](http://arxiv.org/abs/2308.05846)

    本研究提出了一种使用领域随机化和目标跟踪神经网络的方法来进行种子核计数。该方法通过使用合成图像作为训练数据的替代品，解决了神经网络模型需要大量有标签训练数据的问题，可以低成本估计谷物产量。

    

    高通量表型（HTP）对种子的评估是对生长、发育、耐受性、抗性、生态、产量等复杂种子特性的全面评估，以及衡量形成更复杂特性的参数。种子表型的关键之一是谷物产量估计，种子生产行业依赖于这一估计来进行业务运作。目前市场上已有机械化的种子核计数器，但价格往往很高，有时超出小规模种子生产企业的承受范围。目标跟踪神经网络模型(如YOLO)的发展使计算机科学家能够设计出可以低成本估计谷物产量的算法。神经网络模型的关键瓶颈是需要大量有标签的训练数据才能投入使用。我们证明了使用合成图像作为可行替代方案。

    High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping, is the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of parameters that form more complex traits. One of the key aspects of seed phenotyping is cereal yield estimation that the seed production industry relies upon to conduct their business. While mechanized seed kernel counters are available in the market currently, they are often priced high and sometimes outside the range of small scale seed production firms' affordability. The development of object tracking neural network models such as You Only Look Once (YOLO) enables computer scientists to design algorithms that can estimate cereal yield inexpensively. The key bottleneck with neural network models is that they require a plethora of labelled training data before they can be put to task. We demonstrate that the use of synthetic imagery serves as a feasible substitute t
    
[^47]: DiLogics：利用不同逻辑创建Web自动化程序

    DiLogics: Creating Web Automation Programs With Diverse Logics. (arXiv:2308.05828v1 [cs.HC])

    [http://arxiv.org/abs/2308.05828](http://arxiv.org/abs/2308.05828)

    DiLogics是一个通过演示编程的系统，利用自然语言处理帮助用户创建处理多样化规范的Web自动化程序。

    

    知识工作者经常遇到重复的网络数据输入任务，例如更新记录或下订单。网络自动化可以提高生产力，但准确地将任务转化为网络操作并扩展到新的规范是具有挑战性的。现有的工具可以自动化执行相同UI操作的任务（例如，按顺序在每个字段中输入文本），但不支持根据不同的输入条件进行不同执行的任务。我们提出了DiLogics，这是一个通过演示编程的系统，利用自然语言处理帮助用户创建处理多样化规范的Web自动化程序。DiLogics首先将输入数据语义分割为结构化的任务步骤。通过为每个步骤记录用户演示，DiLogics将网络宏泛化为新颖但在语义上相似的任务要求。我们的评估结果显示，非专家可以有效使用DiLogics创建满足多样化输入指令的自动化程序。

    Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provide
    
[^48]: 编码-存储-检索：通过语言编码的自我中心感知增强记忆

    Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])

    [http://arxiv.org/abs/2308.05822](http://arxiv.org/abs/2308.05822)

    本研究提出了一种记忆增强系统，它利用自然语言编码视频数据并将其存储在向量数据库中，通过利用大型视觉语言模型的强大功能来进行语言编码的过程。

    

    我们依赖于自己的记忆来编码、存储和检索我们的经历。然而，记忆间隔有时会发生。实现记忆增强的一种有希望的方法是通过使用增强现实头戴式显示设备来捕捉和保留自我中心的视频，这种做法通常被称为生活记录。然而，由于当前技术缺乏高效编码和存储如此大量的视频数据的能力，从庞大的视频存档中检索特定信息需要大量的计算能力，进一步复杂了快速访问所需内容的任务。

    We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
    
[^49]: 具有多语言印度字体的光学脚本识别

    Optical Script Identification for multi-lingual Indic-script. (arXiv:2308.05780v1 [cs.AI])

    [http://arxiv.org/abs/2308.05780](http://arxiv.org/abs/2308.05780)

    本论文调查了印度多语言字体的光学脚本识别技术的进展，重点讨论了脚本预处理和文本识别的方法，提出了对于印度字体特征的挑战和可能的解决方案。

    

    脚本识别和文本识别是人工智能应用中的重要领域。在数字化时代，使用数字记笔记已经成为常见的做法。然而，传统的纸笔方法仍然是一种主要的书写方式。这导致脚本被基于获取方式进行分类。对当前处理和识别方法的调查将对研究人员有益。本文旨在讨论脚本预处理和文本识别技术的进展。在印度，有十二种杰出的印度字体，与英语不同，这些字体具有多层次的特征。诸如文本形状相似性等复杂特征使得它们难以识别和分析，因此需要先进的预处理方法来进行准确识别。本调查试图提供一份全面的概述，介绍了当前的研究和方法，并展望未来的挑战和可能的解决方案。

    Script identification and text recognition are some of the major domains in the application of Artificial Intelligence. In this era of digitalization, the use of digital note-taking has become a common practice. Still, conventional methods of using pen and paper is a prominent way of writing. This leads to the classification of scripts based on the method they are obtained. A survey on the current methodologies and state-of-art methods used for processing and identification would prove beneficial for researchers. The aim of this article is to discuss the advancement in the techniques for script pre-processing and text recognition. In India there are twelve prominent Indic scripts, unlike the English language, these scripts have layers of characteristics. Complex characteristics such as similarity in text shape make them difficult to recognize and analyze, thus this requires advance preprocessing methods for their accurate recognition. A sincere attempt is made in this survey to provide
    
[^50]: 发挥特征选择和随机森林分类器的作用，提高心力衰竭患者生存预测能力

    Unleashing the Power of Extra-Tree Feature Selection and Random Forest Classifier for Improved Survival Prediction in Heart Failure Patients. (arXiv:2308.05765v1 [cs.LG])

    [http://arxiv.org/abs/2308.05765](http://arxiv.org/abs/2308.05765)

    通过利用数据预处理技术和Extra-Tree特征选择方法与Random Forest分类器相结合，本研究提高了心力衰竭患者生存预测能力，并取得了98.33%的准确率

    

    心力衰竭是一种危及生命的疾病，全球数百万人受到影响。准确预测患者的生存能力可以帮助及早干预，并改善患者预后。本研究探讨利用数据预处理技术和Extra-Tree (ET) 特征选择方法与Random Forest (RF) 分类器相结合，以提高心力衰竭患者的生存预测能力的潜力。通过发挥ET特征选择的优势，我们的目标是识别与心力衰竭生存相关的最重要的预测因子。利用公开的UCL心力衰竭 (HF) 生存数据集，我们采用ET特征选择算法来识别最具信息量的特征。然后，将这些特征用作RF的网格搜索的输入。最后，使用不同的指标对调优后的RF模型进行训练和评估。所采用的方法取得了98.33%的准确率，是目前的最高水平。

    Heart failure is a life-threatening condition that affects millions of people worldwide. The ability to accurately predict patient survival can aid in early intervention and improve patient outcomes. In this study, we explore the potential of utilizing data pre-processing techniques and the Extra-Tree (ET) feature selection method in conjunction with the Random Forest (RF) classifier to improve survival prediction in heart failure patients. By leveraging the strengths of ET feature selection, we aim to identify the most significant predictors associated with heart failure survival. Using the public UCL Heart failure (HF) survival dataset, we employ the ET feature selection algorithm to identify the most informative features. These features are then used as input for grid search of RF. Finally, the tuned RF Model was trained and evaluated using different matrices. The approach was achieved 98.33% accuracy that is the highest over the exiting work.
    
[^51]: 通过从心脏MRI中的知识转移解锁心电图的诊断潜力

    Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI. (arXiv:2308.05764v1 [eess.SP])

    [http://arxiv.org/abs/2308.05764](http://arxiv.org/abs/2308.05764)

    该论文提出了一种通过从心脏MRI中的知识转移解锁心电图的诊断潜力的方法。通过将CMR图像中的领域特定信息转移到ECG嵌入中，该方法实现了仅根据ECG数据进行全面的心脏筛查，并能预测心血管疾病的个体风险和确定心脏表型。

    

    心电图 (ECG) 是一种广泛可用的诊断工具，可以快速和经济高效地评估心血管健康状况。然而，在心血管疾病的诊断中，通常更喜欢使用昂贵的心脏磁共振 (CMR) 成像进行更详细的检查。虽然 CMR 成像可以提供详细的心脏解剖可视化，但由于长时间扫描和高昂的费用，它并不广泛可用。为了解决这个问题，我们提出了一种第一种自监督对比方法，将CMR图像中的领域特定信息转移到ECG嵌入中。我们的方法将多模态对比学习与屏蔽数据建模相结合，实现了仅根据ECG数据进行全面的心脏筛查。在使用来自40044名UK Biobank受试者的数据进行的广泛实验证明了我们方法的实用性和可推广性。我们预测了各种心血管疾病的个体风险，并仅根据ECG数据确定了不同的心脏表型。

    The electrocardiogram (ECG) is a widely available diagnostic tool that allows for a cost-effective and fast assessment of the cardiovascular health. However, more detailed examination with expensive cardiac magnetic resonance (CMR) imaging is often preferred for the diagnosis of cardiovascular diseases. While providing detailed visualization of the cardiac anatomy, CMR imaging is not widely available due to long scan times and high costs. To address this issue, we propose the first self-supervised contrastive approach that transfers domain-specific information from CMR images to ECG embeddings. Our approach combines multimodal contrastive learning with masked data modeling to enable holistic cardiac screening solely from ECG data. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalizability of our method. We predict the subject-specific risk of various cardiovascular diseases and determine distinct cardiac phenotypes solely from E
    
[^52]: 一种利用从光电流血容积波形和活动信号中提取的少量特征的基于机器学习的睡眠-清醒分类模型

    A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals. (arXiv:2308.05759v1 [eess.SP])

    [http://arxiv.org/abs/2308.05759](http://arxiv.org/abs/2308.05759)

    这项研究提出了一种基于机器学习的睡眠-清醒分类模型，利用光电流血容积波形和活动信号提取的特征，可以评估睡眠质量，识别睡眠问题和改善整体健康。

    

    睡眠对我们的整体健康和幸福感至关重要。它在调节我们的心理和身体健康方面起着至关重要的作用，对我们的情绪、记忆和认知功能以及身体的韧性和免疫系统都有影响。睡眠阶段的分类是评估睡眠质量的必要步骤，提供了估计睡眠质量和我们的身体在这个重要的休息时期内功能如何的指标。光电流血容积波形（PPG）已被证明是有效的睡眠阶段推断信号，意味着它可以单独使用或与其他信号结合使用来确定睡眠阶段。这些信息对于识别潜在的睡眠问题和制定改善睡眠质量和整体健康策略非常有价值。在这项工作中，我们提出了一种基于eXtreme Gradient Boosting（XGBoost）算法和从PPG信号和活动计数中提取的特征的机器学习睡眠-清醒分类模型。

    Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our met
    
[^53]: LLM变成DBA

    LLM As DBA. (arXiv:2308.05481v1 [cs.DB])

    [http://arxiv.org/abs/2308.05481](http://arxiv.org/abs/2308.05481)

    LLM变成DBA，提供数据库维护的诊断和优化建议，通过从文本来源中获取经验和多个LLMs的协作诊断。

    

    数据库管理员（DBA）在管理、维护和优化数据库系统以确保数据可用性、性能和可靠性方面起着至关重要的作用。然而，对于DBA来说，管理大量数据库实例（例如，云数据库上的数百万个实例）是困难和繁琐的。最近，大型语言模型（LLMs）已经显示出了理解有价值文件并生成合理答案的巨大潜力。因此，我们提出了D-Bot，一种基于LLM的数据库管理员，它可以持续从文本来源中获取数据库维护经验，并为目标数据库提供合理、有理、及时的诊断和优化建议。本文介绍了一个革命性的以LLM为中心的数据库维护框架，包括（i）从文档和工具中检测数据库维护知识，（ii）根本原因分析的思维树，和（iii）多个LLM之间的协作诊断。我们进行了初步实验。

    Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
    
[^54]: 图聚类的同类性增强结构学习

    Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])

    [http://arxiv.org/abs/2308.05309](http://arxiv.org/abs/2308.05309)

    提出了一种名为HoLe的方法，通过在图结构中增强同类性可以显著改善图聚类任务的性能。

    

    图聚类是图分析中的一个基本任务，在利用图神经网络（GNNs）方面的最新进展已经取得了令人印象深刻的成果。尽管现有的基于GNN的图聚类方法取得了成功，但它们往往忽视了图结构的质量，这是由于现实世界图的稀疏性和多样性所固有的，从而导致了次优的性能。图结构学习可以通过添加缺失的连接和删除错误的连接来优化输入图。然而，以往的图结构学习工作主要集中在有监督的设置上，并且由于缺乏真实标签，不能直接应用于我们的特定聚类任务。为了弥补这个差距，我们提出了一种新颖的方法，称为同类性增强结构学习图聚类（HoLe）。我们的动机源于观察到，微妙地增强图结构中的同类性程度可以显著提升GNNs的性能。

    Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
    
[^55]: AIs的发展脱靴法

    Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])

    [http://arxiv.org/abs/2308.04586](http://arxiv.org/abs/2308.04586)

    传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。

    

    尽管当前一些AI在封闭的世界，如棋盘游戏中超越了人类能力，但它们在混乱的现实世界中的表现有限。它们会犯奇怪的错误而且没有意识到。它们很难受到指导，不能运用常识，缺乏好奇心。它们不能成为良好的合作者。传统手动构建的符号AI方法构建的系统和使用生成和深度学习AI方法(包括大规模语言模型)构建的系统都无法应对这些挑战。它们不适合创建强大和可信赖的AI。尽管此方法不属于主流的AI方法，但发展脱靴法显示出希望。在发展脱靴法中，AI像人类儿童一样发展能力。它们从先天能力开始。像人类一样，它们与环境互动，并从互动中学习。它们通过自我发展的能力逐步扩展先天能力。它们互动并逐渐将所学应用于实际操作。

    Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
    
[^56]: 深度学习在不同数据类型隐写分析中的应用：综述

    Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])

    [http://arxiv.org/abs/2308.04522](http://arxiv.org/abs/2308.04522)

    本综述论文详细综述了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的最新研究进展。

    

    隐写术和隐写分析是信息安全领域的两个相关方面。隐写术旨在隐藏通信，而隐写分析则旨在找到这些隐藏信息，甚至尝试恢复其所包含的数据。隐写术和隐写分析引起了广泛的关注，特别受到执法部门的关注。隐写术常被网络犯罪分子甚至恐怖分子用来避免在拥有证据时被捕，即使加密也一样，因为在许多国家禁止或限制使用密码学。因此，了解揭示隐藏信息的尖端技术对揭露非法行为至关重要。在过去几年中，文献中引入了许多强大可靠的隐写术和隐写分析技术。本综述论文提供了基于深度学习的隐写分析技术在数字媒体中检测隐藏信息的全面概述。

    Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
    
[^57]: AutoPCF:利用大型语言模型进行高效的产品碳足迹核算

    AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models. (arXiv:2308.04241v1 [cs.AI])

    [http://arxiv.org/abs/2308.04241](http://arxiv.org/abs/2308.04241)

    本研究提出了一种利用大型语言模型和深度学习算法的自动化AI驱动的PCF核算框架AutoPCF，该框架具有实现产品碳足迹的自动建模和估算的潜力。

    

    产品碳足迹（PCF）对于减缓供应链的碳排放至关重要，它衡量了产品生命周期中所有活动所导致的直接和间接温室气体排放量。然而，PCF核算通常需要专业知识和大量时间来构建生命周期模型。在本研究中，我们测试比较了五个大型语言模型（LLMs）在建模产品的“摇篮到大门”生命周期并生成输入和输出的库存数据方面的应用能力，揭示了它们作为一个广义PCF知识数据库的局限性。通过利用LLMs，我们提出了一种自动化AI驱动的PCF核算框架，称为AutoPCF，该框架还应用深度学习算法自动匹配计算参数，并最终计算PCF。利用AutoPCF框架对三个案例产品的碳足迹进行估算的结果表明，它在实现PCF的自动建模和估算方面具有潜力。

    The product carbon footprint (PCF) is crucial for decarbonizing the supply chain, as it measures the direct and indirect greenhouse gas emissions caused by all activities during the product's life cycle. However, PCF accounting often requires expert knowledge and significant time to construct life cycle models. In this study, we test and compare the emergent ability of five large language models (LLMs) in modeling the 'cradle-to-gate' life cycles of products and generating the inventory data of inputs and outputs, revealing their limitations as a generalized PCF knowledge database. By utilizing LLMs, we propose an automatic AI-driven PCF accounting framework, called AutoPCF, which also applies deep learning algorithms to automatically match calculation parameters, and ultimately calculate the PCF. The results of estimating the carbon footprint for three case products using the AutoPCF framework demonstrate its potential in achieving automatic modeling and estimation of PCF with a large
    
[^58]: 使用知识图谱预测药物相互作用

    Predicting Drug-Drug Interactions Using Knowledge Graphs. (arXiv:2308.04172v1 [cs.AI])

    [http://arxiv.org/abs/2308.04172](http://arxiv.org/abs/2308.04172)

    本文提出了一个名为medicX的端到端框架，通过使用知识图谱和机器学习算法，预测药物的相互作用。最好的表现组合是ComplEx嵌入方法和LSTM网络，达到了95.19%的F1分数。

    

    在过去几十年里，人们对药物的消费和组合比以前更多，导致了药物相互作用（DDIs）的增加。为了预测未知的DDIs，最近的研究开始将知识图谱（KGs）纳入考虑，因为它们能够捕捉实体之间的关系，提供比单个药物属性更好的药物表示。在本文中，我们提出了medicX端到端框架，它将来自公共药物库的多个药物特征集成到KG中，并使用各种翻译、分解和基于神经网络（NN）的KG Embedding（KGE）方法对图中的节点进行嵌入。最终，我们使用机器学习（ML）算法预测未知的DDIs。在不同的翻译和分解型KGE模型中，我们发现表现最佳的组合是ComplEx嵌入方法和Long Short-Term Memory（LSTM）网络，它在基于DrugBank的DDIs数据集上获得了95.19%的F1分数。

    In the last decades, people have been consuming and combining more drugs than before, increasing the number of Drug-Drug Interactions (DDIs). To predict unknown DDIs, recently, studies started incorporating Knowledge Graphs (KGs) since they are able to capture the relationships among entities providing better drug representations than using a single drug property. In this paper, we propose the medicX end-to-end framework that integrates several drug features from public drug repositories into a KG and embeds the nodes in the graph using various translation, factorisation and Neural Network (NN) based KG Embedding (KGE) methods. Ultimately, we use a Machine Learning (ML) algorithm that predicts unknown DDIs. Among the different translation and factorisation-based KGE models, we found that the best performing combination was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network, which obtained an F1-score of 95.19% on a dataset based on the DDIs found in DrugBank vers
    
[^59]: 利用修剪元素的对抗抹除：迈向更好的图彩票

    Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket. (arXiv:2308.02916v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.02916](http://arxiv.org/abs/2308.02916)

    本文介绍了一种利用对抗抹除的方法来增强图彩票的性能。通过重新考虑修剪信息中的有价值的信息，我们提出了ACE-GLT，这是一种更强大的图彩票方法。

    

    图彩票（GLT）是核心子图和稀疏子网络的组合，旨在减轻大型输入图上深度图神经网络（GNN）的计算成本，同时保持原始性能。然而，现有研究中获胜的GLT是通过应用迭代幅值修剪（IMP）而得到的，而无需重新评估和重新考虑修剪信息，这忽视了在图/模型结构修剪过程中边缘/权重重要性的动态变化，从而限制了获胜的彩票的吸引力。在本文中，我们提出了一个猜想，即修剪图连接和模型参数中存在被忽视的有价值信息，这些信息可以重新分组到GLT中以增强最终性能。具体而言，我们提出了一个对抗性补充抹除（ACE）框架，以从修剪组件中探索有价值的信息，从而开发出更强大的GLT，称为ACE-GLT。

    Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main
    
[^60]: 以协同过滤捕捉AI用户偏好作为规范的方法

    Collaborative filtering to capture AI user's preferences as norms. (arXiv:2308.02542v1 [cs.IR])

    [http://arxiv.org/abs/2308.02542](http://arxiv.org/abs/2308.02542)

    本论文提出了一种以协同过滤方法构建规范以捕捉AI用户偏好的新视角。

    

    将AI技术根据每个用户的偏好进行定制是其良好运行的基础。然而，当前的方法需要用户过多参与，并且未能真正捕捉到他们的真实偏好。事实上，为了避免手动设置偏好的麻烦，用户通常会接受默认设置，即使这些设置与他们的真实偏好不符。规范可以用来调节行为，确保其符合用户的偏好，但是尽管文献已经详细研究了规范，大部分提议都是从形式化的角度出发。实际上，虽然已经有一些关于构建规范以捕捉用户隐私偏好的研究，但是这些方法依赖于领域知识，在AI技术的情况下，这很难获得和维护。我们认为，在构建规范时需要一种新的视角，即利用系统中大量用户的偏好信息。受到推荐系统的启发，我们相信协同过滤可以成为构建规范的方法。

    Customising AI technologies to each user's preferences is fundamental to them functioning well. Unfortunately, current methods require too much user involvement and fail to capture their true preferences. In fact, to avoid the nuisance of manually setting preferences, users usually accept the default settings even if these do not conform to their true preferences. Norms can be useful to regulate behaviour and ensure it adheres to user preferences but, while the literature has thoroughly studied norms, most proposals take a formal perspective. Indeed, while there has been some research on constructing norms to capture a user's privacy preferences, these methods rely on domain knowledge which, in the case of AI technologies, is difficult to obtain and maintain. We argue that a new perspective is required when constructing norms, which is to exploit the large amount of preference information readily available from whole systems of users. Inspired by recommender systems, we believe that co
    
[^61]: 推荐系统中的流行偏差综述

    A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])

    [http://arxiv.org/abs/2308.01118](http://arxiv.org/abs/2308.01118)

    这篇综述论文讨论了推荐系统中的流行偏差问题，并回顾了现有的方法来检测、量化和减少流行偏差。它同时提供了计算度量的概述和主要技术方法的回顾。

    

    推荐系统以个性化的方式帮助人们找到相关内容。这些系统的一个主要承诺是能够增加目录中较少知名的物品的可见性。然而，现有研究表明，在许多情况下，现今的推荐算法反而表现出流行偏差，即它们在推荐中经常关注相当流行的物品。这种偏差不仅可能导致短期内对消费者和提供者的推荐价值有限，而且还可能引起不希望的强化效应。在本文中，我们讨论了流行偏差的潜在原因，并回顾了现有的检测、量化和减少推荐系统中流行偏差的方法。因此，我们的综述既包括了文献中使用的计算度量的概述，也包括了减少偏差的主要技术方法的回顾。我们还对这些方法进行了批判性讨论。

    Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
    
[^62]: 使用Fine-Tuned的OpenAI LLM预测机器翻译输出中的完美质量段落：是否可以从历史数据中捕捉编辑距离模式？

    Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])

    [http://arxiv.org/abs/2308.00158](http://arxiv.org/abs/2308.00158)

    本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。

    

    翻译质量估计（TQE）是将输出翻译部署到使用中之前的重要步骤。 TQE对于评估机器翻译（MT）和人工翻译（HT）的质量也是至关重要的，而不需要查看参考翻译。在这项工作中，我们检查了最先进的大型语言模型（LLMs）是否可以为TQE任务和它们的能力进行Fine-Tune。我们以ChatGPT为例，将TQE视为二元分类任务。使用英意和英德训练语料库，我们的实验结果显示，通过ChatGPT的API Fine-Tuned可以在预测翻译质量方面获得相对较高的得分，即是否需要编辑翻译，但肯定有改进准确性的空间。英意双语摘要可在论文中找到。

    Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
    
[^63]: 人工智能提高了全球可靠洪水预警的覆盖范围

    AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])

    [http://arxiv.org/abs/2307.16104](http://arxiv.org/abs/2307.16104)

    本研究开发了一个人工智能模型，可以准确预测未经测量流域的极端水文事件，从而提高了全球洪水预警的覆盖范围。

    

    洪水是最常见和影响最大的自然灾害之一，对发展中国家尤其具有不对称的影响，这些国家往往缺乏密集的水流监测网络。准确及时的预警对于减轻洪水风险至关重要，但准确的水文模拟模型通常需要根据每个应用的流域中的长时间数据记录进行校准。我们开发了一个人工智能（AI）模型，可以预测7天内的极端水文事件。该模型在所有大洲、前导时间和重现期中均明显优于当前最先进的全球水文模型（Copernicus应急管理服务全球洪水意识系统）。AI在未经测量的流域中的预测尤其有效，这很重要，因为全球只有百分之几的流域具有流量观测站，而发展中国家的未经测量的流域数量占比很高，对人类特别脆弱。

    Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
    
[^64]: ChatGPT用于软件安全：探索ChatGPT在安全应用中的优点和局限性

    ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2307.12488](http://arxiv.org/abs/2307.12488)

    本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。

    

    作为一个多才多艺的大型语言模型，ChatGPT在各个领域应对问题的潜力得到了显著的展示。它能够分析、理解和综合来自在线资源和用户输入的信息，引起了广泛的关注。先前的研究已经探索了ChatGPT在代码生成和代码审查方面的能力。在本文中，我们深入研究了ChatGPT在面向安全的程序分析中的能力，从攻击者和安全分析师的角度进行了探讨。我们通过一个案例研究来评估ChatGPT在几个安全导向的程序分析任务中的回答质量，并有意地引入挑战来评估其响应能力。通过对ChatGPT提供的答案质量的考察，我们对其在安全导向的程序分析领域的优点和局限性有了更清晰的认识。

    ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
    
[^65]: 面向教育中人工智能协作混合论文的自动边界检测

    Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12267](http://arxiv.org/abs/2307.12267)

    本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。

    

    最近的大型语言模型（如ChatGPT）能够在提供具体指导的情况下生成类似于人类的流畅回答。尽管承认技术进步带来的便利，教育者也担心学生可能利用语言模型来完成写作任务并将其假冒为自己的原创作品。虽然有很多AI内容检测研究是基于这些担忧进行的，但大多数之前的研究将AI内容检测建模为一个分类问题，假设一个文本要么完全由人类编写，要么完全由AI生成。在本研究中，我们研究了AI内容检测在一个少有探索但却现实的情况下，即检测的文本由人类和生成性语言模型（即混合文本）协作编写。我们首先将检测任务形式化为从给定的混合文本中识别人类编写内容和AI生成内容之间的转换点（边界检测）。

    The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
    
[^66]: 基于大型语言模型的IDE内生成式信息支持

    In-IDE Generation-based Information Support with a Large Language Model. (arXiv:2307.08177v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2307.08177](http://arxiv.org/abs/2307.08177)

    本研究探索了一种基于大型语言模型的IDE内生成式信息支持，通过在IDE中内嵌对话式用户界面，向开发者提供关于代码理解的帮助。通过查询大型语言模型来解释代码、提供API调用的详细信息、解释特定领域术语以及为API提供使用示例，该系统在用户研究中获得了积极的评价。

    

    理解代码是具有挑战性的，尤其是在新的和复杂的开发环境中工作时。代码注释和文档可以帮助，但往往稀缺或难以导航。大型语言模型（LLM）正在彻底改变编写代码的过程。它们能否对理解代码提供同样的帮助？在这项研究中，我们首次探索了一种基于LLM的直接内嵌式对话式用户界面，并针对代码理解进行了优化。我们的IDE插件通过四个高级请求（无需用户编写显式提示信息）查询OpenAI的GPT-3.5和GPT-4模型，请求内容包括解释代码中的突出部分、提供代码中使用的API调用详细信息，解释关键领域特定术语，以及为API提供使用示例。该插件还支持自由响应式提示，可以自动上下文化到正在编辑的程序中。我们通过32名参与者进行用户研究来评估该系统，结果确认使用我们的插件能够

    Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5 and GPT-4 models with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can
    
[^67]: 使用适配器高效域自适应句子嵌入

    Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])

    [http://arxiv.org/abs/2307.03104](http://arxiv.org/abs/2307.03104)

    本论文提出了一种通过训练轻量级适配器来高效域自适应句子嵌入的方法，避免了微调整个句子嵌入模型的资源消耗。通过训练特定领域的适配器，可以在不同领域中使用同一模型获得良好的性能。

    

    句子嵌入使我们能够捕捉短文本的语义相似性。大多数句子嵌入模型是针对一般语义文本相似性（STS）任务进行训练的。因此，要在特定领域中使用句子嵌入，必须将模型适应于该领域以获得良好的结果。通常，这是通过对感兴趣的域对整个句子嵌入模型进行微调来实现的。虽然这种方法能够产生最先进的结果，但在微调过程中更新了所有模型的权重，使该方法在资源上要求较高。因此，我们提出了训练轻量级适配器的方法，而不是单独为每个目标领域微调整个句子嵌入模型。这些特定领域的适配器不需要微调所有底层句子嵌入模型的参数。相反，我们只训练少量的额外参数，同时保持底层句子嵌入模型的权重不变。训练特定领域的适配器可以始终使用同一模型并在不同领域中获得良好的性能。

    Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
    
[^68]: 人与人之间的互动检测

    Human-to-Human Interaction Detection. (arXiv:2307.00464v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.00464](http://arxiv.org/abs/2307.00464)

    这项研究提出了人与人之间的互动检测任务，通过一种模型实现了同时检测主体、识别个人动作和根据互动关系分组的目标。通过在现有的AVA数据集上添加互动关系注释，建立了一个新的HID基准数据集。

    

    对于视频流中的人与人之间的互动行为（例如排队、握手、打斗和追逐）的全面理解，对于校园、广场和公园等地区的公共安全监控至关重要。与使用编排视频作为输入、忽略并发互动群体，并将检测和识别分为不同阶段的传统人际互动识别不同，我们引入了一个称为人与人之间的互动检测（HID）的新任务。HID致力于在一个模型中检测主体、识别个人动作，并根据他们的互动关系将人员分组。首先，我们基于用于动作检测的流行AVA数据集，在逐帧的方式上添加了互动关系的注释，建立了一个名为AVA-Interaction（AVA-I）的新的HID基准。AVA-I包括85,254帧和86,338个互动群体，每个图像最多包含4个并发互动群体。

    A comprehensive understanding of interested human-to-human interactions in video streams, such as queuing, handshaking, fighting and chasing, is of immense importance to the surveillance of public security in regions like campuses, squares and parks. Different from conventional human interaction recognition, which uses choreographed videos as inputs, neglects concurrent interactive groups, and performs detection and recognition in separate stages, we introduce a new task named human-to-human interaction detection (HID). HID devotes to detecting subjects, recognizing person-wise actions, and grouping people according to their interactive relations, in one model. First, based on the popular AVA dataset created for action detection, we establish a new HID benchmark, termed AVA-Interaction (AVA-I), by adding annotations on interactive relations in a frame-by-frame manner. AVA-I consists of 85,254 frames and 86,338 interactive groups, and each image includes up to 4 concurrent interactive g
    
[^69]: 混合整数规划用于高效启发式算法的时间优化多机器人覆盖路径规划

    Mixed Integer Programming for Time-Optimal Multi-Robot Coverage Path Planning with Efficient Heuristics. (arXiv:2306.17609v1 [cs.RO])

    [http://arxiv.org/abs/2306.17609](http://arxiv.org/abs/2306.17609)

    本文研究了时间优化多机器人覆盖路径规划问题，并通过混合整数规划模型提出了一个最优解决方案，该方案的覆盖时间最多为最优解的四倍。此外，我们还提出了两个有效的启发式算法来减少问题的规模，并通过模型优化方法进一步提高了效率。

    

    我们研究了旨在最小化覆盖时间（定义为所有机器人的最大行程时间）的未加权和加权地形的时间优化多机器人覆盖路径规划（MCPP）。具体而言，我们将MCPP减少到了根最小最大树覆盖（RMMTC）。首次，我们提出了一个混合整数规划（MIP）模型来最优解决RMMTC问题，从而得到一个MCPP解决方案，其覆盖时间最多为最优解的四倍。此外，我们提出了两个次优但有效的启发式算法，用于减少MIP模型中的变量数量，从而提高其在大规模MCPP实例上的效率。我们展示了这两种启发式算法导致了缩小后的MIP模型，对于所有RMMTC实例来说仍然是完整的（即保证找到解决方案）。此外，我们还探索了模型优化的热启动方法，进一步提高了原始MIP模型和缩小后的MIP模型的效率。

    We investigate time-optimal Multi-Robot Coverage Path Planning (MCPP) for both unweighted and weighted terrains, which aims to minimize the coverage time, defined as the maximum travel time of all robots. Specifically, we focus on a reduction from MCPP to Rooted Min-Max Tree Cover (RMMTC). For the first time, we propose a Mixed Integer Programming (MIP) model to optimally solve RMMTC, resulting in an MCPP solution with a coverage time that is provably at most four times the optimal. Moreover, we propose two suboptimal yet effective heuristics that reduce the number of variables in the MIP model, thus improving its efficiency for large-scale MCPP instances. We show that both heuristics result in reduced-size MIP models that remain complete (i.e., guarantee to find a solution if one exists) for all RMMTC instances. Additionally, we explore the use of model optimization warm-startup to further improve the efficiency of both the original MIP model and the reduced-size MIP models. We valida
    
[^70]: 基于困难样本挖掘的监督对比特征学习用于风力发电机桨叶系统故障诊断

    Hard Sample Mining Enabled Supervised Contrastive Feature Learning for Wind Turbine Pitch System Fault Diagnosis. (arXiv:2306.14701v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14701](http://arxiv.org/abs/2306.14701)

    本文提出了一种基于困难样本挖掘的监督对比特征学习方法，用于风力发电机桨叶系统故障诊断。该方法利用余弦相似度识别困难样本，并通过构建困难样本对来学习更具区分性的表示，进一步提高了多层感知机的训练效果。

    

    风力发电机有效利用风能依赖于其桨叶系统能够根据风速变化调整桨叶角度的能力。然而，由于长期磨损导致的桨叶系统中存在多种健康问题，给准确分类造成了挑战，进而增加了风力发电机的维护成本甚至可能损坏它们。本文提出了一种基于困难样本挖掘的监督对比学习（HSMSCL）的新方法来解决这个问题。该方法利用余弦相似度识别困难样本，然后利用监督对比学习通过构建困难样本对来学习更具区分性的表示。此外，该方法中的困难样本挖掘框架还用学到的表示构建困难样本，使多层感知机（MLP）的训练过程更具挑战性并使其成为更有效的分类器。

    The efficient utilization of wind power by wind turbines relies on the ability of their pitch systems to adjust blade pitch angles in response to varying wind speeds. However, the presence of multiple health conditions in the pitch system due to the long-term wear and tear poses challenges in accurately classifying them, thus increasing the maintenance cost of wind turbines or even damaging them. This paper proposes a novel method based on hard sample mining-enabled supervised contrastive learning (HSMSCL) to address this problem. The proposed method employs cosine similarity to identify hard samples and subsequently, leverages supervised contrastive learning to learn more discriminative representations by constructing hard sample pairs. Furthermore, the hard sample mining framework in the proposed method also constructs hard samples with learned representations to make the training process of the multilayer perceptron (MLP) more challenging and make it a more effective classifier. The
    
[^71]: 使用ChatGPT提示开发有效的教育聊天机器人：基于社交媒体素养案例初步测试的insights（含附录）

    Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy (with appendix). (arXiv:2306.10645v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2306.10645](http://arxiv.org/abs/2306.10645)

    使用ChatGPT提示开发教育聊天机器人具有潜力，能够实现互动和个性化学习体验。通过在社交媒体素养案例中进行初步测试，我们得出了混合对话机器人交互的见解和初步指南。这些机器人具有适应性强、多样化的教育策略和对话风格的能力。

    

    教育聊天机器人承诺提供互动和个性化学习体验，然而，它们的发展受到了现有平台受限的自由互动能力和知识以适宜格式编码的困难的限制。最近的语言学习模型的进展表明，使用基于提示的方法开发教育聊天机器人是一个新的可能性，如ChatGPT等具有零样本学习能力的模型。我们提供了一个利用简单系统实现混合对话机器人交互的案例研究，并讨论了从初始测试中获得的见解和初步指南。我们检查了ChatGPT追求多个相互关联的学习目标的能力，根据用户的特征（如文化、年龄和教育水平）来调整教育活动的能力，以及使用多种教育策略和对话风格的能力。尽管结果是令人鼓舞的，但由于受到维护的历史数据的限制，也面临着挑战。

    Educational chatbots come with a promise of interactive and personalized learning experiences, yet their development has been limited by the restricted free interaction capabilities of available platforms and the difficulty of encoding knowledge in a suitable format. Recent advances in language learning models with zero-shot learning capabilities, such as ChatGPT, suggest a new possibility for developing educational chatbots using a prompt-based approach. We present a case study with a simple system that enables mixed-turn chatbot interactions and discuss the insights and preliminary guidelines obtained from initial tests. We examine ChatGPT's ability to pursue multiple interconnected learning objectives, adapt the educational activity to users' characteristics, such as culture, age, and level of education, and its ability to use diverse educational strategies and conversational styles. Although the results are encouraging, challenges are posed by the limited history maintained for the
    
[^72]: 训练好的Transformer在上下文中学习线性模型

    Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])

    [http://arxiv.org/abs/2306.09927](http://arxiv.org/abs/2306.09927)

    本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。

    

    基于注意力的神经网络，例如Transformers，在上下文学习（ICL）方面表现出了非凡的能力：给定一个来自未见过的任务的短语序列的提示，它们可以制定相关的每个令牌和下一个令牌的预测，而不需要任何参数更新。通过将标记的训练数据和未标记的测试数据序列嵌入到提示中，这使得Transformer表现得像有监督学习算法。事实上，最近的工作表明，在随机实例上训练Transformer体系结构的线性回归问题时，这些模型的预测会模仿普通最小二乘法的预测。

    Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
    
[^73]: 关于扩散模型的设计基础：综述

    On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])

    [http://arxiv.org/abs/2306.04542](http://arxiv.org/abs/2306.04542)

    本文综述了扩散模型的设计基础，即其三个关键组件：正向过程、逆向过程和采样过程，为未来的研究提供了有益的细粒度透视。

    

    扩散模型是一种生成模型，通过逐渐添加和删除噪声来学习训练数据的潜在分布以生成数据。扩散模型的组成部分已经受到了广泛的关注，许多设计选择被提出。现有的评论主要关注高层次的解决方案，对组件的设计基础覆盖较少。本研究旨在通过提供一个全面而连贯的综述，针对扩散模型的组件设计选择进行分析。具体来说，我们将这个综述按照三个关键组件进行组织，即正向过程、逆向过程和采样过程。这使得我们可以提供扩散模型的细粒度透视，有助于未来研究分析个体组件、设计选择的适用性以及扩散模型的实现。

    Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
    
[^74]: 通过自预训练掩模序列自编码器和使用定制PolyLoss微调的方法实现鲁棒车道检测

    Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v1 [cs.CV])

    [http://arxiv.org/abs/2305.17271](http://arxiv.org/abs/2305.17271)

    本论文提出了一种鲁棒车道检测流水线，该流水线包括自预训练掩模序列自编码器和使用定制PolyLoss微调的端到端神经网络模型。掩模序列自编码器被采用以通过重构随机掩膜图像中的丢失像素为目标来预训练神经网络模型，提升了车道检测性能。

    

    车道检测是车辆定位的关键，是实现自动驾驶和许多智能高级驾驶辅助系统的基础。现有的基于视觉的车道检测方法未充分利用有价值的特征和聚合的上下文信息，尤其是车道线和图像中其他区域之间的相互关系。为填补这一研究空白并提升车道检测性能，本文提出了一种流水线，其中包括使用自预训练掩模序列自编码器和使用定制PolyLoss微调的端到端神经网络模型。掩模序列自编码器被采用以通过重构随机掩模图像中的丢失像素为目标来预训练神经网络模型。然后，在细调分割阶段中，连续的图像帧被用作输入，

    Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline consisting of self pre-training with masked sequential autoencoders and fine-tuning with customized PolyLoss for the end-to-end neural network models using multi-continuous image frames. The masked sequential autoencoders are adopted to pre-train the neural network models with reconstructing the missing pixels from a random masked image as the objective. Then, in the fine-tuning segmentation phase where lane detection segmentation is performed, the continuous image frames are served as the inputs,
    
[^75]: 超越不变表示学习：线性可对齐的潜在空间用于高效闭合形式领域自适应

    Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation. (arXiv:2305.07500v1 [cs.LG])

    [http://arxiv.org/abs/2305.07500](http://arxiv.org/abs/2305.07500)

    本文提出了一种新的基于最优输运（OT）的领域自适应（DA）方法，通过学习一个嵌入空间，使得OT问题的解是最优且计算量较少的，适用于同质和异质的DA设置。

    

    最优输运（OT）是一种强大的几何工具，用于比较和对齐概率测度，遵循最小努力原则。在机器学习（ML）中，OT的许多成功应用之一是领域自适应（DA），这是一种研究领域，其目标是将分类器从一个带标签的领域转移到另一个类似但不同的未标记或稀疏标记的领域。我们提出了一种全新的基于OT的DA方法，该方法使用由仿射映射给出的OT问题的闭式解，并学习了一个嵌入空间，使得该解是最优且计算量较少。我们展示了我们的方法适用于同质和异质的DA设置。

    Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Among many successful applications of OT in machine learning (ML), domain adaptation (DA) -- a field of study where the goal is to transfer a classifier from one labelled domain to another similar, yet different unlabelled or scarcely labelled domain -- has been historically among the most investigated ones. This success is due to the ability of OT to provide both a meaningful discrepancy measure to assess the similarity of two domains' distributions and a mapping that can project source domain data onto the target one. In this paper, we propose a principally new OT-based approach applied to DA that uses the closed-form solution of the OT problem given by an affine mapping and learns an embedding space for which this solution is optimal and computationally less complex. We show that our approach works in both homogeneous and heterogeneous DA settings 
    
[^76]: 学习在存在隐性混淆因素的情况下从不确定数据中恢复因果关系

    Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])

    [http://arxiv.org/abs/2305.02640](http://arxiv.org/abs/2305.02640)

    本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。

    

    在具有潜在变量的因果发现中，我们定义了两个数据范式：确定数据：具有观察节点单值的单个骨架结构，和不确定数据：具有观察节点多值的一组多骨架结构。多个骨架引入低样本利用率，多个值引入了分布假设的无能力，这两者导致从不确定数据中恢复因果关系至今仍然未被充分探索。我们设计了因果强度变分模型来解决这两个问题。具体地，我们利用因果强度而不是独立噪声作为潜变量来调节证据下界。通过这种设计思想，不同骨架的因果强度被看作是一个分布，并可以表示为单值因果图矩阵。此外，考虑到潜在混淆因素，我们将因果图G分解为两个相关子图O和C。O包含观察节点之间的纯关系，而C表示混淆因素。

    In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
    
[^77]: 《增强上下文学习提高代码生成能力》

    Towards Enhancing In-Context Learning for Code Generation. (arXiv:2303.17780v1 [cs.SE])

    [http://arxiv.org/abs/2303.17780](http://arxiv.org/abs/2303.17780)

    本文提出了一种名为AceCoder的代码生成上下文学习方法，与标准上下文学习相比，它通过示例检索和引导代码生成来提高生成代码的准确性和鲁棒性。

    

    基于预先训练的语言模型的上下文学习已经在代码生成领域表现出了强大的成功。通过这种方法，无需训练，模型只需要输入一个由少量需求-代码示例和一个新需求组成的提示，就能生成出新的程序。但是，现有的研究仅仅将上下文学习用于自然语言生成，忽略了代码生成的独特特性。我们称这些研究为标准上下文学习。本文通过对人类编码过程的观察，提出了一种新的名为AceCoder的代码生成上下文学习方法。与标准上下文学习相比，AceCoder有两个新颖之处。(1)示例检索。它检索类似程序作为示例，并从中学习编程技能(如算法、API)。(2)引导代码生成。它鼓励预训练的语言模型生成中间预备代码(如测试用例、API)并帮助模型理解需求和指导下一步代码生成。我们将AceCoder应用到大量代码生成任务中，实验结果表明，与现有的代码生成系统相比，AceCoder具有更高的准确性和鲁棒性。

    In-context learning (ICL) with pre-trained language models (PTLMs) has shown great success in code generation. ICL does not require training. PTLMs take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse ICL techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard ICL.  Inspired by observations of the human coding process, we propose a novel ICL approach for code generation named AceCoder. Compared to standard ICL, AceCoder has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, APIs) from them. (2) Guided Code Generation. It encourages PTLMs to output an intermediate preliminary (e.g., test cases, APIs) before generating programs. The preliminary can help PTLMs understand requirements and guide the next code generation. We apply AceCode
    
[^78]: 使用丰富的元数据注释的屏幕角色的个性化语言建模

    Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])

    [http://arxiv.org/abs/2303.16618](http://arxiv.org/abs/2303.16618)

    本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。

    

    语言模型的个性化为对话敏感，能更好地捕捉特定特征的人员和/或特定环境中的说话模式。然而，丰富的角色注释难以得到和成功利用。在此工作中，我们发布并描述了一组新颖的手动注释，涵盖了来自流行的 Cornell 电影对话语料库的 863 名演讲者，包括特征引用和角色描述，以及超过 95％ 的特色电影的一组自动提取的六个元数据。我们对两个语料库进行了广泛的实验，并表明可以有效地使用此类注释来个性化语言模型，将困惑减少高达 8.5％。我们的方法甚至可以应用于零样本的演讲者，即对于没有先前培训数据的演讲者，依赖于角色的人口特征的组合。由于收集此类元数据成本高昂，因此我们还贡献了一项成本效益分析，以突出显示

    Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
    
[^79]: ExBEHRT：基于电子病历的扩展Transformer预测疾病亚型和进展

    ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions. (arXiv:2303.12364v1 [cs.LG])

    [http://arxiv.org/abs/2303.12364](http://arxiv.org/abs/2303.12364)

    ExBEHRT是一种扩展Transformer模型，应用于电子病历数据，将多种类型的记录包括在特征空间中，可以预测不同疾病下游任务的性能更好，并使用预期梯度对结果进行更细粒度的解释。

    

    本研究引入了ExBEHRT，它是BEHRT（应用于电子病历的BERT）的扩展版本，并应用不同的算法来解释其结果。我们将特征空间从仅考虑诊断和患者年龄扩展到包括多种类型的记录，包括人口统计学、临床特征、生命体征、吸烟状态、诊断、手术、药物和实验室检查，并采用一种新方法来统一不同特征的频率和时间维度。我们展示了附加特征可以显著改善不同疾病下游任务的模型性能。为了保证模型的稳健性，我们使用了预期梯度的改进方法对模型预测结果进行解释，该方法以前未应用于将EHR数据与Transformer相结合，提供了比以前方法更细粒度的解释，如特征和令牌重要性。此外，通过对肿瘤学患者的模型表示进行聚类，我们展示了ExBEHRT可以用于预测疾病亚型和进展。

    In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
    
[^80]: NeTO: 透明物体的神经重建与自遮挡感知折射追踪

    NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11219](http://arxiv.org/abs/2303.11219)

    提出了一种名为NeTO的方法，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。通过采用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场，可以实现高质量的重建结果。

    

    我们提出了一种新颖的方法，称为NeTO，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。透明物体的重建是一项非常具有挑战性的任务，不适合通用的重建技术，因为其困扰于镜面光传输现象。虽然现有的基于折射追踪的方法在这个任务上取得了令人印象深刻的结果，但它们仍然存在优化不稳定和细节缺失的问题，因为它们采用的显式表面表示难以优化，且忽略了自遮挡问题。在本文中，我们提出利用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场。隐式表示使得我们的方法能够在有限的图像集合下重建出高质量的重建结果。

    We present a novel method, called NeTO, for capturing 3D geometry of solid transparent objects from 2D images via volume rendering. Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena. Although existing refraction-tracing based methods, designed specially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details, since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing. In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation, and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of images, and the s
    
[^81]: 采用机载协调器进行协同学习

    Collaborative Learning with a Drone Orchestrator. (arXiv:2303.02266v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2303.02266](http://arxiv.org/abs/2303.02266)

    本文研究了无人机辅助的协同学习问题，提出了一种通过智能设备群与无人机协同训练神经网络模型的方法。在考虑数据异质性和通信错误的情况下，导出了协同学习的收敛速度，并通过优化无人机轨迹来提高训练准确率。

    

    本文考虑了无人机辅助协同学习的问题。在这种场景下，智能无线设备群通过无人机共同训练一个共享的神经网络模型。每个设备使用其传感器记录来自环境的样本，以获取用于训练的本地数据集。由于各设备的数据量和传感器噪声水平不同，训练数据具有严重的异质性。智能设备对其本地数据集进行迭代训练，并将模型参数与无人机进行交换以进行聚合。在考虑数据异质性、传感器噪声水平和通信错误的情况下，导出了协同学习的收敛速率，并获得了最大化训练的神经网络的最终准确率的无人机轨迹。所提出的轨迹优化方法考虑了设备的数据特性（即本地数据集大小和噪声水平）以及其无线通信条件。

    In this paper, the problem of drone-assisted collaborative learning is considered. In this scenario, swarm of intelligent wireless devices train a shared neural network (NN) model with the help of a drone. Using its sensors, each device records samples from its environment to gather a local dataset for training. The training data is severely heterogeneous as various devices have different amount of data and sensor noise level. The intelligent devices iteratively train the NN on their local datasets and exchange the model parameters with the drone for aggregation. For this system, the convergence rate of collaborative learning is derived while considering data heterogeneity, sensor noise levels, and communication errors, then, the drone trajectory that maximizes the final accuracy of the trained NN is obtained. The proposed trajectory optimization approach is aware of both the devices data characteristics (i.e., local dataset size and noise level) and their wireless channel conditions, 
    
[^82]: 跨模态对比学习用于多模态假新闻检测

    Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14057](http://arxiv.org/abs/2302.14057)

    这项研究提出了COOLANT，一个用于跨模态假新闻检测的对比学习框架，旨在提升图像和文本的对齐精度，并通过跨模态融合和注意力机制实现更准确和可解释的特征聚合。

    

    自动检测多模态假新闻近来引起了广泛关注。许多现有方法致力于融合单模特征以产生多模态新闻表达。然而，强大的跨模态对比学习方法在假新闻检测方面的潜力尚未充分利用。此外，如何聚合不同模态的特征以提升决策过程的性能仍然是一个未解决的问题。为了解决这个问题，我们提出了COOLANT，一个用于多模态假新闻检测的跨模态对比学习框架，旨在实现更准确的图像-文本对齐。为了进一步提高对齐精度，我们利用辅助任务在对比过程中软化负样本的损失项。我们开发了一个跨模态融合模块来学习跨模态之间的相关性。我们实现了一个带有注意力引导模块的注意力机制，以帮助有效且可解释地聚合对齐的单模信息。

    Automatic detection of multimodal fake news has gained a widespread attention recently. Many existing approaches seek to fuse unimodal features to produce multimodal news representations. However, the potential of powerful cross-modal contrastive learning methods for fake news detection has not been well exploited. Besides, how to aggregate features from different modalities to boost the performance of the decision-making process is still an open question. To address that, we propose COOLANT, a cross-modal contrastive learning framework for multimodal fake news detection, aiming to achieve more accurate image-text alignment. To further improve the alignment precision, we leverage an auxiliary task to soften the loss term of negative samples during the contrast process. A cross-modal fusion module is developed to learn the cross-modality correlations. An attention mechanism with an attention guidance module is implemented to help effectively and interpretably aggregate the aligned unimo
    
[^83]: 使用隐式局部概率模型改进神经图像压缩的统计保真度

    Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models. (arXiv:2301.11189v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.11189](http://arxiv.org/abs/2301.11189)

    本论文提出了一种改进神经图像压缩的方法，通过引入非二元鉴别器以及以量化局部图像表示为条件，实现了更好地优化失真度和统计保真度。

    

    有损图像压缩旨在在尽可能少的比特数下保持对原始图像的保真度。理论结果表明，优化失真度量（如PSNR或MS-SSIM）必然导致原始图像与重建图像的统计信息存在差异，特别是在低比特率下，这常常表现为压缩图像的模糊。先前的工作利用对抗性鉴别器提高了统计保真度。然而，从生成建模任务中采用的这些二元鉴别器可能不适用于图像压缩。在本文中，我们引入了一种非二元的鉴别器，它以通过VQ-VAE自动编码器获得的量化局部图像表示为条件。我们在CLIC2020、DIV2K和Kodak数据集上的评估表明，与目前最先进的HiFiC模型的PatchGAN相比，我们的鉴别器更有效地联合优化失真度（例如PSNR）和统计保真度（例如FID）。

    Lossy image compression aims to represent images in as few bits as possible while maintaining fidelity to the original. Theoretical results indicate that optimizing distortion metrics such as PSNR or MS-SSIM necessarily leads to a discrepancy in the statistics of original images from those of reconstructions, in particular at low bitrates, often manifested by the blurring of the compressed images. Previous work has leveraged adversarial discriminators to improve statistical fidelity. Yet these binary discriminators adopted from generative modeling tasks may not be ideal for image compression. In this paper, we introduce a non-binary discriminator that is conditioned on quantized local image representations obtained via VQ-VAE autoencoders. Our evaluations on the CLIC2020, DIV2K and Kodak datasets show that our discriminator is more effective for jointly optimizing distortion (e.g., PSNR) and statistical fidelity (e.g., FID) than the PatchGAN of the state-of-the-art HiFiC model. On CLIC
    
[^84]: 不仅仅是模仿：结合强化学习用于具有挑战性的驾驶情景的鲁棒化模仿

    Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios. (arXiv:2212.11419v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.11419](http://arxiv.org/abs/2212.11419)

    结合使用简单奖励的强化学习和模仿学习，可以显著提高驾驶策略的安全性和可靠性，超过仅用模仿学习学到的策略。

    

    模仿学习(imitation learning, IL)是一种简单而强大的方法，利用大规模的高质量人类驾驶数据产生类人驾驶行为。然而，仅基于模仿学习的策略往往无法充分考虑到安全性和可靠性的问题。本文中，我们展示了如何通过结合使用简单奖励的强化学习和模仿学习，可以显著提高驾驶策略的安全性和可靠性，超过仅用模仿学习学到的策略。具体来说，我们在超过10万英里的城市驾驶数据上训练了一个策略，并在不同碰撞可能性水平下的测试情景中测量其效果。我们的分析表明，虽然模仿学习在由示范数据很好覆盖的低难度情景中表现良好，但我们提出的方法在最具挑战性的情景中显著提高了鲁棒性(故障减少了超过38%)。据我们所知，这是第一个将结合imitation learning和reinforcement learning应用于驾驶情景的工作。

    Imitation learning (IL) is a simple and powerful way to use high-quality human driving data, which can be collected at scale, to produce human-like behavior. However, policies based on imitation learning alone often fail to sufficiently account for safety and reliability concerns. In this paper, we show how imitation learning combined with reinforcement learning using simple rewards can substantially improve the safety and reliability of driving policies over those learned from imitation alone. In particular, we train a policy on over 100k miles of urban driving data, and measure its effectiveness in test scenarios grouped by different levels of collision likelihood. Our analysis shows that while imitation can perform well in low-difficulty scenarios that are well-covered by the demonstration data, our proposed approach significantly improves robustness on the most challenging scenarios (over 38% reduction in failures). To our knowledge, this is the first application of a combined imit
    
[^85]: 一种用于时态规划的高效增量简单时态网络数据结构

    An Efficient Incremental Simple Temporal Network Data Structure for Temporal Planning. (arXiv:2212.07226v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.07226](http://arxiv.org/abs/2212.07226)

    本研究提出了一种用于时态规划的高效增量简单时态网络数据结构，通过采用增量重用计算和避免内存复制的方法，提高了时态规划问题求解的效率。实验证明，该数据结构在时态规划问题序列上表现优于其他方法。

    

    解决时态规划问题的一种常用技术是将因果决策与启发式搜索分离，将时态决策交由简单时态网络(STN)求解器处理。在这种架构中，需要检查一系列互相关联的STN的一致性，因此具有增量重用之前的计算和避免昂贵的内存复制的方法非常重要。本文详细描述了STN在时态规划中的使用方法，确定了支持此用例的明确接口，并介绍了一种既时间又内存高效的实现了此接口的高效数据结构---\deltastn。我们展示了我们的数据结构在时态规划问题序列上优于其他最先进方法。

    One popular technique to solve temporal planning problems consists in decoupling the causal decisions, demanding them to heuristic search, from temporal decisions, demanding them to a simple temporal network (STN) solver. In this architecture, one needs to check the consistency of a series of STNs that are related one another, therefore having methods to incrementally re-use previous computations and that avoid expensive memory duplication is of paramount importance. In this paper, we describe in detail how STNs are used in temporal planning, we identify a clear interface to support this use-case and we present an efficient data-structure implementing this interface that is both time- and memory-efficient. We show that our data structure, called \deltastn, is superior to other state-of-the-art approaches on temporal planning sequences of problems.
    
[^86]: RT-1: 用于实际控制的机器人变压器.

    RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.06817](http://arxiv.org/abs/2212.06817)

    本文提出了机器人变压器模型，通过从大规模、多样化、任务无关的数据集中获取知识，并结合高容量架构，实现了在实际控制领域的高性能泛化能力。

    

    通过从大规模、多样化、任务无关的数据集中获取知识，现代机器学习模型可以在零样本学习或使用少量特定任务的数据集来高水平地解决具体的下游任务。虽然这种能力已经在计算机视觉、自然语言处理或语音识别等其他领域得到证明，但在机器人领域尚未展示出来。这是因为模型的泛化能力在机器人领域尤为关键，由于收集现实世界的机器人数据的难度较大。我们认为，这种通用机器人模型成功的一个关键因素在于任务不可知的开放式训练，结合可以吸收所有多样化机器人数据的高容量架构。在本文中，我们提出了一种称为机器人变压器的模型类，具有良好的可扩展模型特性。我们通过研究不同模型类别及其随数据大小而推广的能力来验证我们的结论。

    By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, mod
    
[^87]: 快速基于显著性引导的随机梯度阈值化的混合训练方法

    Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (arXiv:2212.04875v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04875](http://arxiv.org/abs/2212.04875)

    本文介绍了一种新的混合训练方法，名为R-Mix，通过结合随机性和显著性利用的最佳元素，实现了速度、简单性和准确性的平衡。该方法在泛化能力、弱监督目标定位、校准和对抗性攻击的鲁棒性方面具有有效性。同时，作者还通过训练一个强化学习代理来决定混合训练过程，以探索是否存在更好的决策协议。

    

    混合训练方法已被证明可以提高深度神经网络的泛化能力。多年来，研究界将混合方法扩展为两个方向，旨在改进显著性引导过程并最小化对任意路径的关注，将随机性领域留给进一步探索。在本文中，受到每个方向相互优越的特性的启发，我们引入了一种新颖的方法，位于两个路线的交汇处。通过结合随机性和显著性利用的最佳元素，我们的方法在速度、简单性和准确性之间取得平衡。我们将方法命名为R-Mix，意为“随机混合”。我们证明了该方法在泛化性能、弱监督目标定位、校准和对抗性攻击的鲁棒性方面的有效性。最后，为了回答是否存在更好的决策协议这个问题，我们训练了一个强化学习代理来决定混合训练过程。

    Mix-up training approaches have proven to be effective in improving the generalization ability of Deep Neural Networks. Over the years, the research community expands mix-up methods into two directions, with extensive efforts to improve saliency-guided procedures but minimal focus on the arbitrary path, leaving the randomization domain unexplored. In this paper, inspired by the superior qualities of each direction over one another, we introduce a novel method that lies at the junction of the two routes. By combining the best elements of randomness and saliency utilization, our method balances speed, simplicity, and accuracy. We name our method R-Mix following the concept of "Random Mix-up". We demonstrate its effectiveness in generalization, weakly supervised object localization, calibration, and robustness to adversarial attacks. Finally, in order to address the question of whether there exists a better decision protocol, we train a Reinforcement Learning agent that decides the mix-up
    
[^88]: 深度图神经网络中过度平滑和过度压缩的权衡研究

    On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks. (arXiv:2212.02374v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02374](http://arxiv.org/abs/2212.02374)

    过度平滑和过度压缩是深度图神经网络中的关键挑战，我们提出了添加和删除边缘的方法来解决这个问题。

    

    图神经网络（GNN）在各种计算机科学应用中取得了成功，但是深度GNN在应用中表现不佳，尽管深度学习在其他领域取得了成功。当堆叠图卷积层时，过度平滑和过度压缩是深度表示学习和从远处节点传播信息的关键挑战。我们的工作揭示了过度平滑和过度压缩与图拉普拉斯算符的谱间隔有内在联系，导致这两个问题之间存在必然的权衡，无法同时缓解。为了达到合适的折中，我们提出了添加和删除边缘作为可行的方法。我们引入了随机Jost和Liu曲率重连（SJLR）算法，它在速度上具有计算效率，并与以前基于曲率的方法相比保持了基本特性。与现有方法不同，SJLR在GNN训练过程中执行边缘添加和删除，同时保持了基本特性。

    Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining
    
[^89]: Kuaipedia:一个大规模的多模式短视频百科全书

    Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.00732](http://arxiv.org/abs/2211.00732)

    Kuaipedia是一个大规模的多模式短视频百科全书，通过知识视频的形式，能够轻松表达网民对某个项目的各个方面的需求。

    

    过去20年中，在线百科全书（如维基百科）得到了很好的发展和研究。人们可以在由志愿者社区编辑的维基页面上找到维基项的任何属性或其他信息。然而，传统的文本、图片和表格很难表达维基项的某些方面。例如，当我们谈论“柴犬”时，人们可能更关心“如何喂养它”或“如何训练它不保护食物”。目前，短视频平台已成为在线世界的标志。无论你使用的是TikTok、Instagram、快手还是YouTube Shorts，短视频应用程序已改变了我们今天的内容消费和创作方式。除了为娱乐制作短视频外，我们越来越多地看到作者们在各行各业广泛分享有见解的知识。这些短视频，我们称之为知识视频，可以轻松表达消费者想了解有关某个项目（例如柴犬）的任何方面（例如毛发或如何喂养）。

    Online encyclopedias, such as Wikipedia, have been well-developed and researched in the last two decades. One can find any attributes or other information of a wiki item on a wiki page edited by a community of volunteers. However, the traditional text, images and tables can hardly express some aspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may care more about ``How to feed it'' or ``How to train it not to protect its food''. Currently, short-video platforms have become a hallmark in the online world. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts, short-video apps have changed how we consume and create content today. Except for producing short videos for entertainment, we can find more and more authors sharing insightful knowledge widely across all walks of life. These short videos, which we call knowledge videos, can easily express any aspects (e.g. hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and they can b
    
[^90]: 有不止一种稳健性：用对抗样本欺骗Whisper模型

    There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.17316](http://arxiv.org/abs/2210.17316)

    本研究展示了Whisper模型虽然在分布外输入和随机噪声方面显示出了出色的稳健性，但却容易受到对抗干扰的影响。我们通过生成极小的输入扰动来大幅降低Whisper的性能，并对多语言模型的性能产生了影响。这些发现对于实际的安全问题和对抗性稳健ASR的需求具有重要意义。

    

    Whisper是一种最近的自动语音识别（ASR）模型，对于分布外输入和随机噪声都展示出了令人印象深刻的稳健性。在这项工作中，我们展示了这种稳健性在对抗干扰下并不适用。我们展示了通过生成极小的输入扰动（信噪比为35-45dB），我们可以大幅降低Whisper的性能，甚至转录我们选择的目标句子。我们还展示了通过欺骗Whisper语言检测器，我们可以轻松降低多语言模型的性能。这些对一个广受欢迎的开源模型的脆弱性具有实际的安全影响，并强调了对抗性稳健ASR的需求。

    Whisper is a recent Automatic Speech Recognition (ASR) model displaying impressive robustness to both out-of-distribution inputs and random noise. In this work, we show that this robustness does not carry over to adversarial noise. We show that we can degrade Whisper performance dramatically, or even transcribe a target sentence of our choice, by generating very small input perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling the Whisper language detector we can very easily degrade the performance of multilingual models. These vulnerabilities of a widely popular open-source model have practical security implications and emphasize the need for adversarially robust ASR.
    
[^91]: CodeEditor: 使用预训练模型学习编辑源代码

    CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.17040](http://arxiv.org/abs/2210.17040)

    CodeEditor是一个预训练代码编辑模型，通过专门的预训练任务和代码编辑任务的结合，提高了代码编辑模型的性能和泛化能力。

    

    在软件开发过程中，开发人员经常需要进行重复的代码编辑活动，例如代码重构等。预训练的代码编辑模型已经取得了最先进的结果。预训练模型首先使用预训练任务进行预训练，然后使用代码编辑任务进行微调。现有的预训练任务主要是代码填充任务（例如，掩码语言模型），这些任务来自自然语言处理领域，不适用于自动代码编辑。本文提出了一个专门用于代码编辑的新型预训练任务，并介绍了一个名为CodeEditor的有效预训练代码编辑模型。我们的预训练任务进一步提高了代码编辑模型的性能和泛化能力。具体来说，我们收集了大量的真实代码片段作为参考，并使用一个强大的生成器将它们重写成变异版本。然后，我们使用CodeEditor对变异版本进行预训练，将其编辑为正确的代码。

    Developers often perform repetitive code editing activities for various reasons (e.g., code refactoring) during software development. Pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.  This paper proposes a novel pre-training task specialized in code editing and presents an effective pre-trained code editing model named CodeEditor. Our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the c
    
[^92]: 深度学习中的数据分离定律

    A Law of Data Separation in Deep Learning. (arXiv:2210.17020v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17020](http://arxiv.org/abs/2210.17020)

    深度学习中存在一个简单而定量的数据分离定律，每一层都以恒定的几何速率改善数据的分离程度。这个定律为架构设计、提高模型的健壮性和样本外性能以及预测的解释提供了实际的指导。

    

    虽然深度学习在科学的许多领域取得了重大进展，但其黑盒特性阻碍了未来人工智能应用的架构设计和高风险决策的解释。我们通过研究深度神经网络在中间层中如何处理数据来解决这个问题。我们的发现是一个简单而定量的定律，它规定了深度神经网络如何根据类别成员将数据在所有层中分离出来进行分类。这个定律表明，每一层都以恒定的几何速率改善数据的分离程度，并且在训练过程中观察到了它的出现，无论是在一系列网络架构还是数据集上。这个定律为架构设计、提高模型的健壮性和样本外性能以及预测的解释提供了实际的指导。

    While deep learning has enabled significant advances in many areas of science, its black-box nature hinders architecture design for future artificial intelligence applications and interpretation for high-stakes decision makings. We addressed this issue by studying the fundamental question of how deep neural networks process data in the intermediate layers. Our finding is a simple and quantitative law that governs how deep neural networks separate data according to class membership throughout all layers for classification. This law shows that each layer improves data separation at a constant geometric rate, and its emergence is observed in a collection of network architectures and datasets during training. This law offers practical guidelines for designing architectures, improving model robustness and out-of-sample performance, as well as interpreting the predictions.
    
[^93]: Lib-SibGMU -- 用于推荐系统开发的大学图书馆借阅数据集

    Lib-SibGMU -- A University Library Circulation Dataset for Recommender Systems Developmen. (arXiv:2208.12356v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2208.12356](http://arxiv.org/abs/2208.12356)

    Lib-SibGMU是一个开放的大学图书馆借阅数据集，可以用于推荐系统开发。在该数据集上我们发现使用fastText模型作为向量化器可以获得竞争性的结果。

    

    我们以CC BY 4.0许可证开源了Lib-SibGMU的大学图书馆借阅数据集，供广大研究社区使用，并在该数据集上评估了主要的推荐系统算法。我们展示了一个由将借阅书籍历史转化为向量的向量化器和一个基于邻域的推荐器组成的推荐体系结构，分别进行训练。我们证明使用fastText模型作为向量化器可以获得竞争性的结果。

    We opensource under CC BY 4.0 license Lib-SibGMU - a university library circulation dataset - for a wide research community, and benchmark major algorithms for recommender systems on this dataset. For a recommender architecture that consists of a vectorizer that turns the history of the books borrowed into a vector, and a neighborhood-based recommender, trained separately, we show that using the fastText model as a vectorizer delivers competitive results.
    
[^94]: 关系行动基础：形式化、有效安全验证和不变量（扩展版）

    Relational Action Bases: Formalization, Effective Safety Verification, and Invariants (Extended Version). (arXiv:2208.06377v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06377](http://arxiv.org/abs/2208.06377)

    该论文介绍了关系行动基础（RABs）的通用框架，通过取消关系状态和行动的限制，实现了对动态系统的建模和验证，同时提供了参数化安全性的研究方法。

    

    在人工智能、业务流程管理和数据库理论中，对基于关系表示的动态系统的建模和验证是越来越受关注的问题。为了使这些系统适于验证，需要限制每个关系状态中存储的信息量，或者对行动的先决条件和影响施加限制。我们引入了关系行动基础（RABs）的通用框架，通过取消这些限制从而推广现有模型：无限制的关系状态可以通过可以对数据进行存在量词和全称量词量化的行动演变，并且可以利用带有算术谓词的数字数据类型。然后，我们通过（近似）基于SMT的反向搜索研究了RABs的参数化安全性，挑出了结果过程的基本元属性，并展示了它如何通过现有验证模块的组合实现。

    Modeling and verification of dynamic systems operating over a relational representation of states are increasingly investigated problems in AI, Business Process Management, and Database Theory. To make these systems amenable to verification, the amount of information stored in each relational state needs to be bounded, or restrictions are imposed on the preconditions and effects of actions. We introduce the general framework of relational action bases (RABs), which generalizes existing models by lifting both these restrictions: unbounded relational states can be evolved through actions that can quantify both existentially and universally over the data, and that can exploit numerical datatypes with arithmetic predicates. We then study parameterized safety of RABs via (approximated) SMT-based backward search, singling out essential meta-properties of the resulting procedure, and showing how it can be realized by an off-the-shelf combination of existing verification modules of the state-o
    
[^95]: ECLAD: 使用本地聚合描述符提取概念

    ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.04531](http://arxiv.org/abs/2206.04531)

    本文提出了一种使用本地聚合描述符提取概念的方法，并介绍了一种基于合成数据集的验证过程，用于减少概念提取方法的人工干预需求。

    

    卷积神经网络（CNN）在关键系统中的应用越来越多，其中鲁棒性和对齐性至关重要。在这个背景下，可解释的人工智能领域提出了通过概念提取来生成CNN预测过程的高级解释。虽然这些方法可以检测图像中是否存在概念，但无法确定其位置。此外，由于缺乏适当的验证程序，很难对这些方法进行公平比较。为了解决这些问题，我们提出了一种基于CNN激活图像的像素聚合表示的自动概念提取和定位方法。此外，我们引入了一种基于合成数据集的概念提取技术验证过程，该数据集具有其主要组件的像素注释，减少了人工干预的需求。我们在合成和真实数据上进行了广泛的实验验证。

    Convolutional neural networks (CNNs) are increasingly being used in critical systems, where robustness and alignment are crucial. In this context, the field of explainable artificial intelligence has proposed the generation of high-level explanations of the prediction process of CNNs through concept extraction. While these methods can detect whether or not a concept is present in an image, they are unable to determine its location. What is more, a fair comparison of such approaches is difficult due to a lack of proper validation procedures. To address these issues, we propose a novel method for automatic concept extraction and localization based on representations obtained through pixel-wise aggregations of CNN activation maps. Further, we introduce a process for the validation of concept-extraction techniques based on synthetic datasets with pixel-wise annotations of their main components, reducing the need for human intervention. Extensive experimentation on both synthetic and real-w
    
[^96]: 基于视觉的低空城市环境中无人机自定位

    Vision-Based UAV Self-Positioning in Low-Altitude Urban Environments. (arXiv:2201.09201v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.09201](http://arxiv.org/abs/2201.09201)

    本论文提出了一个新的数据集DenseUAV，为无人机在低空城市环境中的自定位任务设计。该数据集采用了密集采样方法，解决了现有数据集中忽视的密集采样和现实场景中的不确定性问题。

    

    无人机依赖于卫星系统进行稳定定位。然而，由于卫星覆盖有限或通信中断，无人机可能会失去来自卫星定位系统的信号。在这种情况下，基于视觉的技术可以作为替代方案，确保无人机的自定位能力。然而，大多数现有数据集都是为无人机识别的地理定位任务而开发的，而不是为无人机的自定位任务而设计的。此外，当前的无人机数据集使用了对合成数据（如Google Maps）进行离散采样，因此忽略了在现实场景中经常遇到的密集采样和不确定性的关键因素。为了解决这些问题，本文提出了一个新的数据集DenseUAV，这是第一个公开可用的为无人机自定位任务设计的数据集。DenseUAV在低空城市环境中采用了对无人机图像的密集采样。总共，超过27K份无人机图像被采集。

    Unmanned Aerial Vehicles (UAVs) rely on satellite systems for stable positioning. However, due to limited satellite coverage or communication disruptions, UAVs may lose signals from satellite-based positioning systems. In such situations, vision-based techniques can serve as an alternative, ensuring the self-positioning capability of UAVs. However, most of the existing datasets are developed for the geo-localization tasks of the objects identified by UAVs, rather than the self-positioning task of UAVs. Furthermore, the current UAV datasets use discrete sampling on synthetic data, such as Google Maps, thereby neglecting the crucial aspects of dense sampling and the uncertainties commonly experienced in real-world scenarios. To address these issues, this paper presents a new dataset, DenseUAV, which is the first publicly available dataset designed for the UAV self-positioning task. DenseUAV adopts dense sampling on UAV images obtained in low-altitude urban settings. In total, over 27K UA
    
[^97]: CDistNet：感知多域字符距离的鲁棒文本识别

    CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition. (arXiv:2111.11011v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.11011](http://arxiv.org/abs/2111.11011)

    本文提出了一种名为多域字符距离感知（MDCDP）的新颖模块，用于解决场景文本识别中特征和字符对齐不准确的问题。该模块通过交叉注意机制融合视觉和语义特征，并生成一个内容感知嵌入来感知字符位置。

    

    基于Transformer的编码-解码框架在场景文本识别中越来越流行，主要是因为它能够自然地整合来自视觉和语义领域的识别线索。然而，最近的研究表明这两种线索并不总是很好地注册，因此在困难文本（例如，具有罕见形状的文本）中，特征和字符可能不对齐。因此，引入了字符位置等约束来缓解这个问题。尽管在一定程度上取得了成功，但视觉和语义仍然是分别建模的，它们之间只是松散关联。在本文中，我们提出了一个新颖的模块，称为多域字符距离感知（MDCDP），用于建立一个视觉上和语义上相关的位置嵌入。MDCDP使用位置嵌入通过交叉注意机制查询视觉和语义特征。这两种线索被融合到位置分支中，生成一个能够很好地感知字符的内容感知嵌入。

    The Transformer-based encoder-decoder framework is becoming popular in scene text recognition, largely because it naturally integrates recognition clues from both visual and semantic domains. However, recent studies show that the two kinds of clues are not always well registered and therefore, feature and character might be misaligned in difficult text (e.g., with a rare shape). As a result, constraints such as character position are introduced to alleviate this problem. Despite certain success, visual and semantic are still separately modeled and they are merely loosely associated. In this paper, we propose a novel module called Multi-Domain Character Distance Perception (MDCDP) to establish a visually and semantically related position embedding. MDCDP uses the position embedding to query both visual and semantic features following the cross-attention mechanism. The two kinds of clues are fused into the position branch, generating a content-aware embedding that well perceives characte
    
[^98]: 基于相似性映射的神经模型重编程用于低资源口语命令分类

    Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2110.03894](http://arxiv.org/abs/2110.03894)

    本文提出了一种基于相似性映射的神经模型重编程方法，用于低资源口语命令分类。实验证明，在有限的数据条件下，该方法在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果。

    

    本研究提出了一种新颖的对抗重编程（AR）方法，用于低资源口语命令识别（SCR），并构建了一个AR-SCR系统。AR过程旨在修改来自目标领域的声学信号，以重新调整预训练的SCR模型，从而实现重编程。为了解决源域和目标域之间的标签不匹配问题，并进一步提高AR的稳定性，我们提出了一种新颖的基于相似性的标签映射技术来对齐类别。此外，将迁移学习（TL）技术与原始AR过程相结合，以提高模型的适应能力。我们在三个低资源SCR数据集上评估了提出的AR-SCR系统，包括阿拉伯语、立陶宛语和言语障碍性普通话。实验结果表明，在大规模英语数据集上训练的预训练AM的基础上，提出的AR-SCR系统在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果，且仅使用有限数量的数据。

    In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of 
    
[^99]: 虚拟知识图谱的映射模式

    Mapping Patterns for Virtual Knowledge Graphs. (arXiv:2012.01917v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2012.01917](http://arxiv.org/abs/2012.01917)

    本文提出了一个全面的虚拟知识图谱映射模式目录，用于支持链接数据库和本体的映射管理，该目录通过分析实际用例和VKG基准，扩展和完善了已有方法和模式，并验证了其在多种场景中可行性。

    

    虚拟知识图谱（VKG）是集成和访问遗留数据源的最有前景的范式之一。集成过程中的一个关键瓶颈是定义、验证和维护将数据源与领域本体链接起来的映射。为了支持整个映射生命周期的管理，我们提出了一个全面的映射模式目录，用于连接数据库和本体时出现的复杂映射模式。为了实现这一目标，我们借鉴了数据管理、数据分析和概念建模领域中的成熟方法和模式，并通过分析具体的VKG基准和实际用例，考虑了数据源和本体之间固有的阻抗不匹配问题进行了扩展和完善。我们在考虑的VKG场景中验证了我们的目录，证明它涵盖了其中绝大部分的模式。

    Virtual Knowledge Graphs (VKG) constitute one of the most promising paradigms for integrating and accessing legacy data sources. A critical bottleneck in the integration process involves the definition, validation, and maintenance of mappings that link data sources to a domain ontology. To support the management of mappings throughout their entire lifecycle, we propose a comprehensive catalog of sophisticated mapping patterns that emerge when linking databases to ontologies. To do so, we build on well-established methodologies and patterns studied in data management, data analysis, and conceptual modeling. These are extended and refined through the analysis of concrete VKG benchmarks and real-world use cases, and considering the inherent impedance mismatch between data sources and ontologies. We validate our catalog on the considered VKG scenarios, showing that it covers the vast majority of patterns present therein.
    
[^100]: 朝向人工智能取证：人工智能系统是否做到了？(arXiv:2005.13635v3 [cs.AI] UPDATED)

    Towards AI Forensics: Did the Artificial Intelligence System Do It?. (arXiv:2005.13635v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2005.13635](http://arxiv.org/abs/2005.13635)

    本文关注人工智能是否导致特定事件以及触发AI行动的问题，提供了取证调查策略，并使用卷积神经网络评估了识别恶意AI的挑战和思路。

    

    人工智能（AI）以日益自主的方式做出影响我们日常生活的决策。它们的行为可能会导致事故、伤害，或者更普遍地违反规定。确定AI是否导致了特定事件，以及如果是这样，是什么触发了AI的行动，是关键的取证问题。我们提供了对这些问题的概念化和取证调查策略。我们重点研究可能是“恶意设计”的AI和灰盒分析。我们使用卷积神经网络进行评估，揭示了识别恶意AI的挑战和思路。

    Artificial intelligence (AI) makes decisions impacting our daily lives in an increasingly autonomous manner. Their actions might cause accidents, harm, or, more generally, violate regulations. Determining whether an AI caused a specific event and, if so, what triggered the AI's action, are key forensic questions. We provide a conceptualization of the problems and strategies for forensic investigation. We focus on AI that is potentially ``malicious by design'' and grey box analysis. Our evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious AI.
    

