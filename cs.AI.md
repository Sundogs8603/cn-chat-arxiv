# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multi-level protein pre-training with Vabs-Net](https://rss.arxiv.org/abs/2402.01481) | 这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。 |
| [^2] | [Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization](https://rss.arxiv.org/abs/2402.01401) | 通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。 |
| [^3] | [LoTR: Low Tensor Rank Weight Adaptation](https://rss.arxiv.org/abs/2402.01376) | LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。 |
| [^4] | [Supervised Algorithmic Fairness in Distribution Shifts: A Survey](https://rss.arxiv.org/abs/2402.01327) | 这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。 |
| [^5] | [Federated Unlearning: a Perspective of Stability and Fairness](https://rss.arxiv.org/abs/2402.01276) | 本文研究了在数据异构性情况下的联邦取消学习（FU）的稳定性和公平性问题。我们提出了关键指标来评估FU，重点考虑了验证、全局稳定性和局部公平性，并研究了其中的权衡。通过理论分析，我们洞察了数据异构性对FU的影响，并提出了管理权衡的FU机制。通过实证验证，我们证实了这些机制的有效性。 |
| [^6] | [Efficient Causal Graph Discovery Using Large Language Models](https://rss.arxiv.org/abs/2402.01207) | 提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。 |
| [^7] | [A Survey on Self-Supervised Learning for Non-Sequential Tabular Data](https://rss.arxiv.org/abs/2402.01204) | 本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。 |
| [^8] | [Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud](https://rss.arxiv.org/abs/2402.00876) | 认知互联网超越了认知物联网，使连接的物体能够独立地获取知识和理解，并在整个网络中集成了协作智能。本文深入探讨了“认知互联网”范式的基础要素、独特特征、益处和工业影响。 |
| [^9] | [Language Models as Inductive Reasoners](https://rss.arxiv.org/abs/2212.10923) | 该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。 |
| [^10] | [HASSOD: Hierarchical Adaptive Self-Supervised Object Detection](https://arxiv.org/abs/2402.03311) | HASSOD是一种分层自适应无监督目标检测系统，通过自监督学习，提高了检测性能和可解释性。 |
| [^11] | [V-IRL: Grounding Virtual Intelligence in Real Life](https://arxiv.org/abs/2402.03310) | V-IRL是一个平台，可以让人工智能代理在虚拟环境中与现实世界进行互动，旨在将数字和物理世界之间的差距缩小，并开发出具有丰富感知、决策和与真实数据互动能力的代理。 |
| [^12] | [Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?](https://arxiv.org/abs/2402.03305) | 本研究通过实验探究了条件DDPMs学习生成2D球形高斯凸起的过程，在学习的过程中发现了潜在表示的关键，产生了与不同阶段对应的 qualitatively 不同的生成行为。 |
| [^13] | [Nevermind: Instruction Override and Moderation in Large Language Models](https://arxiv.org/abs/2402.03303) | 大语言模型具有覆盖和调节指令的能力，较大的模型在覆盖内部和上下文指令方面表现最佳，并且在绳索扩展时需要保持缓冲区来保持指令遵循能力。 |
| [^14] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^15] | [Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks](https://arxiv.org/abs/2402.03295) | Ginger是一种用于通用神经网络的高效曲率近似方法，具有线性复杂度。它通过特征分解来逆向计算广义高斯牛顿矩阵，避免了传统方法中的高内存和高时间复杂度问题。 |
| [^16] | [Flora: Low-Rank Adapters Are Secretly Gradient Compressors](https://arxiv.org/abs/2402.03293) | 本文研究了低秩适配器的动力学，并提出了一种基于随机投影的方法Flora，通过重新采样投影矩阵实现高秩更新，同时减少优化状态的空间复杂度。 |
| [^17] | [InstanceDiffusion: Instance-level Control for Image Generation](https://arxiv.org/abs/2402.03290) | InstanceDiffusion通过添加实例级控制，使文本到图像的扩散模型能够产生高质量图像，并在不同的位置条件下超过了专业先进模型。 |
| [^18] | [Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS](https://arxiv.org/abs/2402.03289) | 本文介绍了一种使用蒙特卡罗树搜索进行前瞻的自动变换器解码算法，可解决现有大型语言模型在RTL代码生成中存在的编译失败和PPA不敏感的问题，并在性能上取得了显著改进。 |
| [^19] | [Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2402.03286) | 本文提出了一种无需训练的方法ConsiStory，通过共享预训练模型的内部激活，实现了一致的文本到图像生成。引入了主题驱动的共享注意力块和基于对应的特征注入，促进了图像之间的主题一致性，并采用了策略来保持布局多样性。 |
| [^20] | [Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models](https://arxiv.org/abs/2402.03284) | 本论文研究了如何用语言模型来表示对话中的不确定性，并提出了改进模型校准的微调策略。实验证明，这些策略可以使较小的模型具备与大型预训练模型相当的性能。 |
| [^21] | [A Framework for Partially Observed Reward-States in RLHF](https://arxiv.org/abs/2402.03282) | 这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。 |
| [^22] | [Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models](https://arxiv.org/abs/2402.03271) | 通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。 |
| [^23] | [Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://arxiv.org/abs/2402.03268) | 本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。 |
| [^24] | [CLIP Can Understand Depth](https://arxiv.org/abs/2402.03251) | 本文研究了将CLIP用于单目深度估计的问题，通过联合训练反卷积解码器和可学习嵌入矩阵，使得CLIP能够理解深度，该方法在深度估计任务上取得了令人印象深刻的性能，并优于之前的方法。 |
| [^25] | [HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference](https://arxiv.org/abs/2402.03247) | HEANA是一种混合时幅模拟光学加速器，通过使用混合时幅模拟光学乘法器(TAOMs)提高了对多种数据流的灵活性。它解决了现有光学加速器的波长并行性受到串扰、不支持多种数据流以及未充分利用光电探测器进行原位累积等问题。 |
| [^26] | [SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM](https://arxiv.org/abs/2402.03246) | SGS-SLAM是一种基于三维高斯点云的语义稠密SLAM系统，通过多通道优化和关键帧优化，实现了高质量的重建和精确的语义分割。 |
| [^27] | [IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images](https://arxiv.org/abs/2402.03227) | IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。 |
| [^28] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^29] | [Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?](https://arxiv.org/abs/2402.03214) | 这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。 |
| [^30] | [Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems](https://arxiv.org/abs/2402.03204) | 该论文提出了一种多智能体强化学习算法，通过优化多个大规模MIMO基站的睡眠模式和天线切换，实现在多小区网络中能量的节省，同时保持服务质量。仿真结果表明，这种算法相比基线策略具有更好的性能。 |
| [^31] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^32] | [Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning](https://arxiv.org/abs/2402.03183) | 本论文提出了一个顺序元学习框架SeMPL，可以在多个环境下学习和预测给定软件配置的性能。与现有方法不同的是，SeMPL通过依次顺序训练元环境，实现了更准确的对新环境的性能预测。 |
| [^33] | [C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.03181) | C-RAG是第一个用于认证检索增强语言模型生成风险的框架，通过提供符合风险分析和生成风险的上界，确保生成结果的可信性。 |
| [^34] | [Comparison of Topic Modelling Approaches in the Banking Context](https://arxiv.org/abs/2402.03176) | 本研究在银行业背景下比较了主题建模方法。通过使用KernelPCA和K-means Clustering在BERTopic架构中，我们得到了连贯的主题，连贯性得分为0.8463。 |
| [^35] | [The Matrix: A Bayesian learning model for LLMs](https://arxiv.org/abs/2402.03175) | 本文介绍了一个贝叶斯学习模型，用于理解大型语言模型（LLMs）的行为。研究探索了LLMs的优化指标，并开发了一个新的基于预测下一个标记的模型。实验结果表明，LLMs的行为与贝叶斯学习一致，为上下文学习提供了新的见解。 |
| [^36] | [Multi: Multimodal Understanding Leaderboard with Text and Images](https://arxiv.org/abs/2402.03173) | Multi是一个多模态理解的排行榜，提供了一个综合数据集，评估多模态大型语言模型对理解复杂图表和科学问题的能力。它兼具准确和开放式的回答形式，挑战MLLM的各种任务，并包含超过18,000个问题。 |
| [^37] | [Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings](https://arxiv.org/abs/2402.03172) | 本论文提出了一种新颖的方法来实现自动化ICD编码，通过在文本中使用注意力机制的方法来精确分配ICD代码。实验结果表明，该方法在ICD编码中优于当前最先进的模型，并且标签嵌入机制对模型性能也有显著贡献。 |
| [^38] | [Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories](https://arxiv.org/abs/2402.03164) | 本论文研究了有限域情境演算理论中关于时间的可判定推理。通过引入具有受限状态公理和比较操作符的钟表作为实值fluents，解决了检查可达情境是否满足给定公式的问题的不可判定性，并给出了一个可判定的过程来确定行动序列。 |
| [^39] | [Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task](https://arxiv.org/abs/2402.03141) | 这篇论文提出了一种名为Auxiliary-Delayed Reinforcement Learning (AD-RL)的方法，通过利用辅助的短时延任务来加速长时延任务的学习过程，同时在随机环境中保持性能。该方法能显著降低样本复杂度，并在确定性和随机基准测试中表现出优异的样本效率和性能。 |
| [^40] | [Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations](https://arxiv.org/abs/2402.03138) | 本文提出了一种基于聚类和预训练表示的方法，用于在高维空间中进行探索。通过对随机和预训练表示进行聚类，可以有效计算状态数，特别是在多维环境中预训练表示更加有效。 |
| [^41] | [Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games](https://arxiv.org/abs/2402.03136) | 我们提出了一种名为Albatross的算法，它使用模拟自我对弈学习合作和竞争的同时博弈中的有界理性代理行为，并成功实现了与任何游戏实力的代理的合作和竞争。 |
| [^42] | [User-Centric Evaluation of ChatGPT Capability of Generating R Program Code](https://arxiv.org/abs/2402.03130) | 本文评估了ChatGPT从自然语言输入生成R编程语言代码的能力，通过多次尝试的过程，并对生成的解决方案进行了精确性、完整性、简洁性等多个质量属性的评估。 |
| [^43] | [Good Teachers Explain: Explanation-Enhanced Knowledge Distillation](https://arxiv.org/abs/2402.03119) | 通过优化解释增强的知识蒸馏（e$^2$KD）算法，可以让学生模型在准确性和学生-教师一致性方面都得到大幅度提升，确保学生模型从教师那里正确学到原因。 |
| [^44] | [Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism](https://arxiv.org/abs/2402.03112) | 本文提出了一种利用结构关注机制的机器学习方法，可以预测和解释偶氮化合物的红外光谱。该方法通过集中关注与功能团邻近的化学信息，显著提升了光谱预测的准确性、鲁棒性和可解释性，为揭示红外光谱特征与分子结构之间的相关性提供了一种可扩展和高效的途径。 |
| [^45] | [Non-Stationary Latent Auto-Regressive Bandits](https://arxiv.org/abs/2402.03110) | 本文提出了非平稳潜在自回归赌博机问题，并提出了一个算法，在这种环境下可以达到较低的遗憾率。 |
| [^46] | [Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases](https://arxiv.org/abs/2402.03099) | 该论文介绍了一种基于意图的提示校准方法，通过迭代优化和生成合成边界情况数据来改进提示工程，以提高大型语言模型的性能。 |
| [^47] | [Preference-Conditioned Language-Guided Abstraction](https://arxiv.org/abs/2402.03081) | 本研究提出了一种基于用户偏好的语言引导的抽象化方法，通过观察人类行为的变化并使用语言模型查询用户的偏好，构建适用于不同用户的状态抽象。 |
| [^48] | [Learning to Abstract Visuomotor Mappings using Meta-Reinforcement Learning](https://arxiv.org/abs/2402.03072) | 本研究通过使用元强化学习来探索人类获取多个全新视觉运动映射的能力，并发现情境线索在学习中起到了重要的作用，提供了计算上的优势。 |
| [^49] | [Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian](https://arxiv.org/abs/2402.03067) | 本文介绍了BERTopic在塞尔维亚语短文本中的首次应用，结果显示BERTopic可以产生丰富的主题，即使是部分预处理的文本。与传统方法相比，BERTopic在主题数量不受限制时提供了更多信息和新的见解。 |
| [^50] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^51] | [InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions](https://arxiv.org/abs/2402.03040) | “InteractiveVideo”是一个以用户为中心的视频生成框架，通过交互式的多模态指令，用户可以在整个生成过程中精确、有效地指导生成模型，并对视频的关键方面进行灵活调整。 |
| [^52] | [Automatic Combination of Sample Selection Strategies for Few-Shot Learning](https://arxiv.org/abs/2402.03038) | 本文研究了20种样本选择策略对少样本学习性能的影响，并提出了一种自动组合样本选择策略的方法（ACSESS），在多个数据集上证明了其优越性能。 |
| [^53] | [Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches](https://arxiv.org/abs/2402.03017) | 本文全面调查了少样本学习领域的最新进展，探讨了该方法在解决深度学习在实际应用中的限制方面的潜力和挑战。 |
| [^54] | [Whom to Trust? Elective Learning for Distributed Gaussian Process Regression](https://arxiv.org/abs/2402.03014) | 本文介绍了一种使用高斯过程回归增强分布式协作学习的创新方法，并开发了一种选举学习算法，使智能体能够根据邻居的可信度有选择性地请求预测。该方法提高了个体预测的准确性，同时消除了计算密集型方差计算的需求，并确保了预测的可靠性。 |
| [^55] | [UniMem: Towards a Unified View of Long-Context Large Language Models](https://arxiv.org/abs/2402.03009) | 本文引入UniMem，一个统一的框架，以记忆增强的角度重新制定了现有的长上下文方法，并提出了UniMix来提高长上下文处理能力。 |
| [^56] | [Decoding-time Realignment of Language Models](https://arxiv.org/abs/2402.02992) | 本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。 |
| [^57] | [Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL](https://arxiv.org/abs/2402.02978) | 本文评估了针对OWL 2 QL上元推理的Datalog工具。许多应用都需要元模型来简化本体的扩展和重用。然而，元模型的无限制使用带来了不可判定性问题。本文讨论了在OWL 2 QL上的元模型语义和元模型语义蕴含机制，并对几种扩展进行了评估。 |
| [^58] | [Variational Flow Models: Flowing in Your Style](https://arxiv.org/abs/2402.02977) | 我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。 |
| [^59] | [Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation](https://arxiv.org/abs/2402.02921) | 本论文介绍了一种使用增量评估挖掘最小的行为模式集合的方法，以解决现有方法的可扩展性和实际应用中冗余模式的问题。 |
| [^60] | [DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network](https://arxiv.org/abs/2402.02910) | 本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。 |
| [^61] | [Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows](https://arxiv.org/abs/2402.02904) | 本研究利用强化学习控制的数字孪生模型成功复制了人体肘部阻抗识别实验，并发现强化学习智能体在稳定肘关节运动方面表现出比人体更高的阻抗，为虚拟环境模拟在神经机械研究中的潜力提供了初步证据。 |
| [^62] | [LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models](https://arxiv.org/abs/2402.02896) | 本研究通过对大规模语言模型进行个性化配置，并探究了智能体之间的对话交互对个性一致性和语言对齐的影响。研究结果表明不同配置对话者展示了不同程度的个性一致性和语言对齐。这些发现为进一步理解LLMs之间基于对话的相互作用提供了基础，并强调了打造更具鲁棒性和类人化LLM智能体的新方法的需求。 |
| [^63] | [A Review on Building Blocks of Decentralized Artificial Intelligence](https://arxiv.org/abs/2402.02885) | 本文综述了分散式人工智能（DEAI）领域的构建模块，从底层开始分析，探讨了DEAI解决方案和网络，并提出了未来的研究方向和开放性问题。 |
| [^64] | [Comparing Knowledge Sources for Open-Domain Scientific Claim Verification](https://arxiv.org/abs/2402.02844) | 在本论文中，我们对开放领域科学主张验证系统的性能进行了测试，通过比较不同的知识来源（PubMed、维基百科、谷歌）和信息检索方法，我们发现在真实世界环境中进行科学主张验证的挑战和问题。 |
| [^65] | [SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data](https://arxiv.org/abs/2402.02826) | 本研究提出了一种新方法，通过仅使用合成数据来构建一个全面的计算机视觉模型，用于检测人类乳头瘤病毒生殖器疣。使用扩散模型快速生成高质量的训练数据，并评估其对视觉模型的影响。 |
| [^66] | [Evading Data Contamination Detection for Language Models is (too) Easy](https://arxiv.org/abs/2402.02823) | 本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。 |
| [^67] | [Graph-enhanced Large Language Models in Asynchronous Plan Reasoning](https://arxiv.org/abs/2402.02805) | 本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。 |
| [^68] | [Large Language Model Distilling Medication Recommendation Model](https://arxiv.org/abs/2402.02803) | 本研究旨在利用大型语言模型改进药物推荐方法，以解决传统模型在医学数据语义理解和新患者处方推荐方面的挑战。 |
| [^69] | [KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models](https://arxiv.org/abs/2402.02801) | KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。 |
| [^70] | [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) | 本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。 |
| [^71] | [Dual Knowledge Distillation for Efficient Sound Event Detection](https://arxiv.org/abs/2402.02781) | 这项研究提出了一种称为双重知识蒸馏的框架，用于开发高效的声音事件检测系统。这个框架通过时序平均知识蒸馏和增强嵌入特征蒸馏来稳定地蒸馏知识并提升上下文学习能力，在实验中取得了良好的性能。 |
| [^72] | [Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate](https://arxiv.org/abs/2402.02769) | 通过从教学中学习（LoT）技术，我们提出了一种深度神经网络正则化技术，能够通过引入辅助学生模型来提升主模型的泛化性能。实验证明，LoT能有效识别具有泛化和可教授关系的信息。 |
| [^73] | [Intent Profiling and Translation Through Emergent Communication](https://arxiv.org/abs/2402.02768) | 本研究提出了一个基于人工智能的意图概要和翻译框架，通过新兴通信实现应用程序的意图概要，解决了应用程序与网络之间复杂的通信问题。 |
| [^74] | [List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation](https://arxiv.org/abs/2402.02764) | 该论文提出了一个基于列表感知的重新排序-截断联合模型，用于搜索和生成增强检索。该模型旨在捕捉列表级的上下文特征，通过重新排序和截断来返回更好的列表。先前的研究将重新排序和截断视为两个单独的任务，但这种分离不是最优的。因此，该论文提出的联合模型可以解决信息共享和错误积累的问题。 |
| [^75] | [ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer](https://arxiv.org/abs/2402.02733) | 本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。 |
| [^76] | [A Generative Approach to Surrogate-based Black-box Attacks](https://arxiv.org/abs/2402.02732) | 提出了一种基于生成模型的黑盒替代攻击方法，通过学习样本分布，生成对抗性样本，用于攻击深度神经网络。 |
| [^77] | [Denoising Time Cycle Modeling for Recommendation](https://arxiv.org/abs/2402.02718) | 本文提出了一种新的方法，降噪时间周期建模（DiCycle），用于在推荐系统中建模用户-物品交互的时间模式。通过选择与目标物品高度相关的用户行为子集，DiCycle能够更好地建模时间周期模式，从而提高推荐性能。 |
| [^78] | [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716) | 这项调查研究系统地介绍了基于LLM的代理规划的最新进展和挑战。 |
| [^79] | [Position Paper: What Can Large Language Models Tell Us about Time Series Analysis](https://arxiv.org/abs/2402.02713) | 大语言模型有潜力颠覆时间序列分析，提升决策效率，推动时间序列分析智能的普及化。这种进展可以带来模态切换和时间序列问答等多种可能性。 |
| [^80] | [Representation Surgery for Multi-Task Model Merging](https://arxiv.org/abs/2402.02705) | 该论文提出了一种名为“Surgery”的表征手术解决方案，用于减少多任务模型合并中的表示偏差。该方法通过一个轻量级的任务专用模块，针对合并模型的表示进行修正，以提高合并模型的性能。 |
| [^81] | [Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence](https://arxiv.org/abs/2402.02701) | 本文通过理论和实证研究，揭示了在测试环境具有干扰因素时影响视觉强化学习中泛化差距的关键因素。结果表明，最小化训练和测试环境之间的表示距离是减少泛化差距最关键的因素。 |
| [^82] | [Beyond Expectations: Learning with Stochastic Dominance Made Practical](https://arxiv.org/abs/2402.02698) | 这项工作首次尝试建立了一个随机优势学习的通用框架，并推广了随机优势的概念以使其能够在任意两个随机变量之间进行比较。同时，我们还开发了一种有效的计算方法来处理连续性评估的问题。 |
| [^83] | [Causal Feature Selection for Responsible Machine Learning](https://arxiv.org/abs/2402.02696) | 本调查论文研究了负责任机器学习中的因果特征选择，强调了其对解释性、公平性、对抗鲁棒性和域泛化的作用。 |
| [^84] | [Exploiting Class Probabilities for Black-box Sentence-level Attacks](https://arxiv.org/abs/2402.02695) | 该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。 |
| [^85] | [Poisson Process for Bayesian Optimization](https://arxiv.org/abs/2402.02687) | 提出了一种基于泊松过程的新型排名替代模型，引入了称为泊松过程贝叶斯优化（PoPBO）的高效BO框架，并从经典的LCB和EI模型中得出了两个定制的收集函数以适应它。 |
| [^86] | [Equivariant Symmetry Breaking Sets](https://arxiv.org/abs/2402.02681) | 这里是中文总结出的一句话要点: 该论文提出了一种全等变的对称破缺框架，通过引入对称破缺集来破坏等变神经网络中的对称性。这种方法通用且适用于任何群的等变性。 |
| [^87] | [Large Language Models are Geographically Biased](https://arxiv.org/abs/2402.02680) | 本文研究了大型语言模型的地理偏见，并展示了其对地理空间预测的系统错误，通过零射击地理空间预测来评估其对世界的认知。 |
| [^88] | [Verifiable evaluations of machine learning models using zkSNARKs](https://arxiv.org/abs/2402.02675) | 本论文提出了一种使用zkSNARKs进行机器学习模型评估的方法。通过模型推理和零知识计算证明，可以提供可验证的评估证明，验证模型在公开输入上的性能和公平性指标。这有助于解决闭源商业机器学习模型评估可信性的问题。 |
| [^89] | [Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision](https://arxiv.org/abs/2402.02658) | 本文介绍了一种名为模型引导的过程监督（MiPS）的新方法，该方法通过对推理模型的解决方案进行抽样完成来自动进行数据整理，从而避免了昂贵的人工注释。实证结果表明，与之前的工作相反，我们应优先选择验证器预测得分高的验证。这种方法显著改进了PaLM 2在数学和编码任务上的性能。 |
| [^90] | [Vision-Language Models Provide Promptable Representations for Reinforcement Learning](https://arxiv.org/abs/2402.02651) | 本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。 |
| [^91] | [Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses](https://arxiv.org/abs/2402.02648) | 本文提出了链式反馈（CoF）方法，以缓解大型语言模型在回答问题中出现不一致性的问题。同时，作者还提出了递归链式反馈（R-CoF）方法，并提到了进一步的研究。这些方法可以提高回答的可靠性和有效性。 |
| [^92] | [LLM-Enhanced Data Management](https://arxiv.org/abs/2402.02643) | LLMDB是一种LLM增强的数据管理范式，通过LLM的微调和提示工程嵌入领域特定知识，解决了虚构、高成本和低准确性的挑战，并实现了泛化能力和高推理能力。 |
| [^93] | [Enhancing Transformer RNNs with Multiple Temporal Perspectives](https://arxiv.org/abs/2402.02625) | 引入了多个时间视角的概念，用于增强Transformer RNNs对顺序数据的理解能力，在参数数量最小增加的情况下取得了显著的改进。 |
| [^94] | [A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control](https://arxiv.org/abs/2402.02624) | 提出了一种安全的强化学习驱动的权重变化模型预测控制方法，用于自动驾驶车辆运动控制。该方法通过在运行时动态调整代价函数权重，可以提供上下文相关的最优闭环控制性能。 |
| [^95] | [Efficient Market Dynamics: Unraveling Informational Efficiency in UK Horse Racing Betting Markets Through Betfair's Time Series Analysis](https://arxiv.org/abs/2402.02623) | 通过对Betfair的时间序列数据进行分析，研究发现英国赛马市场在投注交易回报方面表现出了高水平的信息效率，与金融资产不同，市场中的知识能够快速被吸收并反映在价格上，并且市场对新信息的响应速度较快。 |
| [^96] | [PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?](https://arxiv.org/abs/2402.02611) | 本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。 |
| [^97] | [Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2402.02600) | 该论文提出了一种基于深度强化学习的方法，通过使用开源加密工具和强化学习框架成功地混淆恶意软件，规避了最先进的恶意软件检测器，并且在规避率方面超越了使用高级修改方法的技术。 |
| [^98] | [Unified Training of Universal Time Series Forecasting Transformers](https://arxiv.org/abs/2402.02592) | 本研究提出了一种改进的时间序列Transformer架构，名为Moirai，以解决时间序列预测中的跨频率学习、适应多变量时间序列以及解决大规模数据固有的不同分布特性等挑战，从而实现了统一训练通用时间序列预测Transformer。 |
| [^99] | [DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models](https://arxiv.org/abs/2402.02563) | DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。 |
| [^100] | [Neur2BiLO: Neural Bilevel Optimization](https://arxiv.org/abs/2402.02552) | Neur2BiLO是一个针对双层优化问题的框架，通过将神经网络近似引入到混合整数规划中，可以快速生成高质量的解决方案。 |
| [^101] | ["What's my model inside of?": Exploring the role of environments for grounded natural language understanding](https://arxiv.org/abs/2402.02548) | 本论文探索了环境对于改进基于实践的自然语言理解中数据采集和模型开发的潜力，通过开发新的训练方法和基准，采用生态方法研究基于实践的语言理解系统在自然/模拟虚拟环境中的角色。 |
| [^102] | [Integration of cognitive tasks into artificial general intelligence test for large models](https://arxiv.org/abs/2402.02547) | 建议将认知任务整合到大型模型的人工通用智能测试中，以建立一个综合框架，能够评估大型模型的多维智能。这个框架结合了认知科学和自然语言处理，包含了稳态智力、流态智力和社交智能等方面。 |
| [^103] | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544) | LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。 |
| [^104] | [Absolute convergence and error thresholds in non-active adaptive sampling](https://arxiv.org/abs/2402.02522) | 提出了一种计算非主动自适应采样中绝对收敛和误差阈值的方法，可以确定模型何时不再增加质量，并提供了一个接近条件来估算模型实现目标的接近度，从而支持模型选择中学习参数的微调。 |
| [^105] | [SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving](https://arxiv.org/abs/2402.02519) | 本文提出了一种简单高效的自动驾驶多智能体运动预测基线(SIMPL)，通过引入紧凑高效的全局特征融合模块以及使用Bernstein基多项式对连续轨迹参数化的方法，实现了实时准确的运动预测，为下游规划任务提供了有价值的数据。 |
| [^106] | [Adaptive scheduling for adaptive sampling in POS taggers construction](https://arxiv.org/abs/2402.02516) | 本论文提出了一种自适应调度的自适应采样方法，用于在构建词性标注器中加快训练速度，同时提高采样的鲁棒性。该方法分析学习曲线的形状，在任何时候增加或减少实例，以确保获得学习能力的净增益。 |
| [^107] | [Modeling of learning curves with applications to pos tagging](https://arxiv.org/abs/2402.02515) | 该论文提出了一种估计学习曲线演化的算法，可以通过部分训练数据结果来预测学习过程中达到期望准确度所需的时间。 |
| [^108] | [Early stopping by correlating online indicators in neural networks](https://arxiv.org/abs/2402.02513) | 本文提出了一种在神经网络中最小化泛化误差的新技术，通过利用一系列在线指标的时间相关性，找到过拟合现象，并提供了可靠的提前停止条件，从而提高了预测能力。 |
| [^109] | [Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning](https://arxiv.org/abs/2402.02500) | 通过广泛实验发现基于点云的方法在机器人学习中表现出更好的性能，特别是在各种预训练和泛化任务中。结果表明，点云观测模态对于复杂机器人任务是有价值的。 |
| [^110] | [Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to CT Image Fusion](https://arxiv.org/abs/2402.02498) | 本论文提出一种全可微相关驱动网络用于X射线与CT图像融合的配准，通过双分支CNN-Transformer编码器实现低频全局特征和高频局部特征的提取和分离，进一步提出相关驱动损失进行特征分解，并应用凸形相似函数学习的训练策略。实验证明，该方法在性能上优于现有的全可微学习配准方法。 |
| [^111] | [BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback](https://arxiv.org/abs/2402.02479) | BRAIn是一种基于贝叶斯奖励条件化缩减推断的自然语言生成方法，通过反馈来改进RLHF，在LLM对齐中表现出较好的可扩展性和性能。 |
| [^112] | [Why are hyperbolic neural networks effective? A study on hierarchical representation capability](https://arxiv.org/abs/2402.02478) | 本文通过大规模实验对为什么双曲神经网络有效进行了全面分析，并提出了几种预训练策略来增强层级表示能力，从而改进下游任务的性能。 |
| [^113] | [Fast Peer Adaptation with Context-aware Exploration](https://arxiv.org/abs/2402.02468) | 本文提出了一种基于上下文感知的探索方法，用于快速适应具有不同策略的未知同伴。通过奖励智能体在历史上下文中有效识别同伴行为模式，该方法能够促进智能体积极探索和快速适应，从而在不确定同伴策略时收集信息反馈，并在有信心时利用上下文执行最佳反应。 |
| [^114] | [A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer](https://arxiv.org/abs/2402.02464) | 这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。 |
| [^115] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^116] | [DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching](https://arxiv.org/abs/2402.02439) | DiffStitch是一种使用基于扩散的轨迹拼接提升离线强化学习的方法。它通过有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以提高离线强化学习算法的性能。 |
| [^117] | [Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback](https://arxiv.org/abs/2402.02423) | Uni-RLHF是一个通用的强化学习平台和基准套件，致力于处理多样化的人类反馈，解决了在RLHF中量化进展的挑战，并提供了用户友好的注释界面和离线基准实现。 |
| [^118] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^119] | [FreDF: Learning to Forecast in Frequency Domain](https://arxiv.org/abs/2402.02399) | FreDF是一种在频域中学习预测的方法，解决了时间序列建模中标签序列的自相关问题，相比现有方法有更好的性能表现，并且与各种预测模型兼容。 |
| [^120] | [DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models](https://arxiv.org/abs/2402.02392) | DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。 |
| [^121] | [Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning](https://arxiv.org/abs/2402.02388) | 本文提出了SAGE，一个通用的面向解决方案的ABM生成框架，利用辅助验证和迭代上下文学习，自动生成针对特定问题的模型和解决方案。 |
| [^122] | [Empowering Computing and Networks Convergence System with Distributed Cooperative Routing](https://arxiv.org/abs/2402.02381) | 本文提出了一个分布式协作路由框架，用于增强计算与网络融合系统，以确保截止日期要求并最小化请求计算成本。 |
| [^123] | [Evaluating Large Language Models in Analysing Classroom Dialogue](https://arxiv.org/abs/2402.02380) | 本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。 |
| [^124] | [Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation](https://arxiv.org/abs/2402.02367) | 本文介绍一种专为医学图像分割量身定制的自监督框架MedSASS，通过对比实验证明其在医学数据集上的优越性能，并展示了在端到端训练下的显著改进。 |
| [^125] | [The Developmental Landscape of In-Context Learning](https://arxiv.org/abs/2402.02364) | 在transformers模型中，我们展示了在上下文学习中的离散发展阶段，并引入了两种方法来检测这些阶段的关键里程碑。我们使用行为和结构度量验证了这些方法的有效性。 |
| [^126] | [Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE](https://arxiv.org/abs/2402.02362) | 本研究通过应用物理学中的规范对称性原理，将其应用于神经网络架构，发现各种机器学习模型的参数冗余可以解释为规范对称性。并证明了前馈神经网络中的参数冗余在神经ODE中升级为微分同胚，并找到了Transformer模型与神经ODE及其规范对称性的自然对应关系。 |
| [^127] | [Stereographic Spherical Sliced Wasserstein Distances](https://arxiv.org/abs/2402.02345) | 本文提出了一种快速且高度并行的用于比较球形测度的距离，使用了立体投影和广义Radon变换，称之为立体投影球面切片瓦瑟斯坦（S3W）距离。通过仔细处理立体投影引起的距离畸变，并进行了理论分析，证明了该方法在速度和效果上的优势。 |
| [^128] | [MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters](https://arxiv.org/abs/2402.02342) | MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。 |
| [^129] | [Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation](https://arxiv.org/abs/2402.02339) | 本文提出了一种不确定性感知的测试时间优化（UAO）框架，通过量化关节点的不确定性来缓解过拟合问题，提高3D人体姿势估计的性能。 |
| [^130] | [Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning](https://arxiv.org/abs/2402.02334) | 本文研究了深度表格学习中算术特征交互的必要性，通过引入AMFormer模型，实现了在细粒度表格数据建模、训练数据效率和泛化方面的优越性能。 |
| [^131] | [Enhance Reasoning for Large Language Models in the Game Werewolf](https://arxiv.org/abs/2402.02330) | 本文提出了一个创新的框架，将大型语言模型（LLM）与外部思考模块相结合，以增强推理能力，通过引入通信协议和使用大量数据进行训练，展示了其在游戏推理、语音生成和在线评估方面的有效性。 |
| [^132] | [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314) | 该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。 |
| [^133] | [Future Directions in Foundations of Graph Machine Learning](https://arxiv.org/abs/2402.02287) | 图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。 |
| [^134] | [Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2402.02286) | 该论文提出了一种在实时推理速度下实现高分割准确性的多级特征聚合和递归对齐网络。使用ResNet-18作为骨干，通过多级特征聚合模块和递归对齐模块来提高模型性能。 |
| [^135] | [SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking](https://arxiv.org/abs/2402.02285) | SynthDST是一个针对对话状态跟踪设计的数据生成框架，利用合成数据来实现少样本提示，通过使用少量手工对话模板和对话模式，它能够生成自然、连贯和流畅的带有DST注释的对话，并使Join连通率提升4-5％. |
| [^136] | [InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification](https://arxiv.org/abs/2402.02274) | InceptionCapsule方法采用了自注意力机制、迁移学习和Inception-ResNet模型，在医学图像分类中解决了初始权重选择和特征提取的问题，取得了较高的准确率。 |
| [^137] | [Federated Learning with New Knowledge: Fundamentals, Advances, and Futures](https://arxiv.org/abs/2402.02268) | 本文系统地探讨了具有新知识的联邦学习，包括如何有效地融入新特征、任务、模型和算法，并分析了新知识到达的形式和时间对纳入过程的影响。同时讨论了联邦学习具有新知识时的潜在未来发展方向。 |
| [^138] | [MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers](https://arxiv.org/abs/2402.02263) | MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。 |
| [^139] | [Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model](https://arxiv.org/abs/2402.02262) | 本文介绍了一种使用RoBERTa-CNN模型来在社交媒体帖子中检测自杀意图的新方法。RoBERTa-CNN通过在RoBERTa模型中添加卷积神经网络（CNN）层，提高了对重要模式的捕捉能力，并在实验证明在自杀和抑郁检测数据集上表现出良好的准确性。 |
| [^140] | [XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction](https://arxiv.org/abs/2402.02258) | XTSFormer是一个用于不规则时间事件预测的跨时空尺度的Transformer模型，通过新颖的循环感知时间位置编码和分层的多尺度时间注意机制来解决不规则时间间隔、循环、周期性和多尺度事件交互等挑战。 |
| [^141] | [ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images](https://arxiv.org/abs/2402.02246) | ExTTNet是一种深度学习算法，能够自动从发票图像中提取表格文字。使用光学字符识别（OCR）技术获取文本，并通过特征提取方法提高准确度。通过多层神经网络模型进行训练，最终获得了0.92的F1分数。 |
| [^142] | [Federated Learning with Differential Privacy](https://arxiv.org/abs/2402.02230) | 本论文研究了带有差分隐私的联邦学习对模型性能的影响，发现在分布式和差分隐私设置下，非独立同分布和小型数据集的性能下降最为严重。 |
| [^143] | [Machine Intelligence in Africa: a survey](https://arxiv.org/abs/2402.02218) | 本文从多层多尺度和文化意识伦理的角度总结了非洲机器智能的最新发展，展示了MI在54个非洲国家的应用案例，强调了非洲的音频数据集未被充分利用、先进的MI模型缺乏文化意识的问题。 |
| [^144] | [Diabetes detection using deep learning techniques with oversampling and feature augmentation](https://arxiv.org/abs/2402.02188) | 本研究提出了一种使用深度学习技术、过采样和特征增强的方法来进行糖尿病检测。通过对Pima印地安人糖尿病数据库进行评估，实现了92.31%的准确率。 |
| [^145] | [Evolution Guided Generative Flow Networks](https://arxiv.org/abs/2402.02186) | 本文提出了进化引导的生成流网络（EGFN），用于处理长时间跨度和稀疏奖励的挑战。通过使用进化算法训练一组代理参数，并将结果轨迹存储在优先回放缓冲区中，我们的方法在训练GFlowNets代理时展现出了很高的效果。 |
| [^146] | [Sentiment analysis in non-fixed length audios using a Fully Convolutional Neural Network](https://arxiv.org/abs/2402.02184) | 提出了一种使用全卷积神经网络在非固定长度的音频上进行情感分析的方法，使用Mel频谱图和Mel频率倒谱系数作为音频描述方法，并在多个数据集上验证了其有效性。 |
| [^147] | [An Ontology-Based multi-domain model in Social Network Analysis: Experimental validation and case study](https://arxiv.org/abs/2402.02181) | 本研究提出了一个基于本体的多领域知识模型，能够自动进行社交网络分析，避免错误，并获得与专家相同的结论。通过一个真实案例研究，展示了该方法的优势。 |
| [^148] | [One Graph Model for Cross-domain Dynamic Link Prediction](https://arxiv.org/abs/2402.02168) | DyExpert是一种用于跨域链接预测的动态图模型，通过明确建模历史演化过程并结合链接预测，它可以学习特定下游图的演化模式，并在各个领域上取得了最先进的性能。 |
| [^149] | [Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations](https://arxiv.org/abs/2402.02167) | 这篇论文介绍了一个名为EvaLLM的概念模型栈，用于评估和解释基于生成AI的可视化。它解决了使用大型语言模型生成可视化时遇到的问题，并提出了一种理论评估的方法。 |
| [^150] | [TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular Representation](https://arxiv.org/abs/2402.02164) | 本研究引入了TSIS算法作为t-SMILES的补充，用于改进基于字符串的分子表示方法。实验证明，TSIS模型在处理语法中的长期依赖性方面表现优于其他模型。 |
| [^151] | [Data-Driven Prediction of Seismic Intensity Distributions Featuring Hybrid Classification-Regression Models](https://arxiv.org/abs/2402.02150) | 本研究开发了基于地震参数的线性回归模型，能够基于数据预测地震强度分布，并且优于常用的地震地面运动预测方程（GMPEs）。 |
| [^152] | [Emergency Computing: An Adaptive Collaborative Inference Method Based on Hierarchical Reinforcement Learning](https://arxiv.org/abs/2402.02146) | 本文提出了一个紧急网络(E-SC3I)，该网络具有感知、通信、计算、缓存和智能功能，通过紧急计算、缓存、集成通信和感知以及智能增强机制实现快速接入、可靠传输和动态网络部署。为了解决计算开销问题，我们提出了一种自适应协作推理方法(ACIM)。 |
| [^153] | [Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test](https://arxiv.org/abs/2402.02135) | 本研究通过使用多语言定义问题测试，探讨了大型语言模型（LLMs）在不同语言下的道德判断和道德推理能力。研究发现，印地语和斯瓦希里语的道德推理能力明显低于其他语言，而道德判断也在不同语言下变化较大。 |
| [^154] | [Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees](https://arxiv.org/abs/2402.02110) | 复合主动学习为多领域主动学习提供了具备理论保证的方法，并且考虑了不同领域之间的相似性和数据分布变化。 |
| [^155] | [Are Large Language Models Good Prompt Optimizers?](https://arxiv.org/abs/2402.02101) | 这项研究揭示了以大型语言模型（LLM）作为提示优化器的实际机制。研究发现LLM优化器往往受到自身先前知识的偏见，难以准确识别错误的真正原因，并且在生成合适的提示方面面临挑战。 |
| [^156] | [Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models](https://arxiv.org/abs/2402.02099) | 分析了多语言语言模型中跨语言知识转移的评估方法和设置，发现高性能主要归因于非语言知识的因素，如任务和表层知识，并且跨语言传输的主要是数据工件和偏见，尤其是对于低资源语言。 |
| [^157] | [Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing](https://arxiv.org/abs/2402.02097) | 提出了一种名为MACE的简单而有效的多智能体协同探索方法，通过共享局部新颖性来近似全局新颖性，并引入加权互信息来衡量智能体行动对其他智能体的影响，从而促进多智能体之间的协同探索。实验证明，MACE在稀疏奖励的多智能体环境中取得了出色的性能。 |
| [^158] | [Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene Classification](https://arxiv.org/abs/2402.02094) | 提出了一种针对零样本遥感图像场景分类的深度语义-视觉对齐方法，通过自动收集可视化可检测属性来解决先前ZSL模型主要依赖于手动标记的属性或词嵌入的问题。 |
| [^159] | [DeCoF: Generated Video Detection via Frame Consistency](https://arxiv.org/abs/2402.02085) | 通过帧一致性原则，DeCoF是一个简单但有效的生成视频检测模型，可以消除空间伪影的影响，并表现出强大的泛化能力。 |
| [^160] | [Prototypical Contrastive Learning through Alignment and Uniformity for Recommendation](https://arxiv.org/abs/2402.02079) | 本研究提出了一种基于对齐和一致性的原型对比学习方法（ProtoAU）用于推荐系统中，该方法解决了随机抽样引起的抽样偏差问题，提高了特征表示的有效性。 |
| [^161] | [Trustworthiness of $\mathbb{X}$ Users: A One-Class Classification Approach](https://arxiv.org/abs/2402.02066) | 本研究基于One-Class Classification（OCC）模型，提出了一种对$\mathbb{X}$用户进行分类的方法。同时，提出了一种基于子空间学习的新颖正则化项和数据描述方法，用于捕捉异质的数据集中度。 |
| [^162] | [AnthroScore: A Computational Linguistic Measure of Anthropomorphism](https://arxiv.org/abs/2402.02056) | AnthroScore是一种计算语言学的度量方法，用于隐含人性化语言。研究发现，AnthroScore与人类对人性化的判断和社会科学文献中的人性化维度相一致。分析结果显示，近15年来研究论文中的人形化水平稳步增加，与语言模型相关的论文具有最高的人形化水平。 |
| [^163] | [Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning](https://arxiv.org/abs/2402.02055) | 提出了一种简单但有理论原则支持的度量方法——方差对齐分数(Variance Alignment Score，VAS)，用于解决数据选择问题。该方法通过最大化总的VAS来选择最具信息量的样本。 |
| [^164] | [Neural Scaling Laws on Graphs](https://arxiv.org/abs/2402.02054) | 本论文在图上深入研究了神经缩放定律，从模型和数据两个角度进行了探索。对于模型缩放，发现了缩放定律崩溃和过拟合之间的关系，以及深度图模型的模型深度对缩放行为的影响。对于数据缩放，提出了图数量不适合作为衡量缩放定律中图数据量的指标。 |
| [^165] | [Affordable Generative Agents](https://arxiv.org/abs/2402.02053) | 本文提出了经济实惠的生成式智能体框架（AGA），通过学习策略替代LLM推理和压缩对话信息，实现了低成本的可信互动，且对于有限环境中生成的可信行为机制进行了深入研究。 |
| [^166] | [A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission](https://arxiv.org/abs/2402.02043) | 提出了一种插件式微型AI模块，可以将智能和选择性传感器数据传输集成到物联网应用中。通过放置高效的机器学习模型靠近传感器，实现对数据传输的智能控制，筛选有价值的数据，并通过调节传输频率丢弃无关信息。通过量化和优化近传感器模型，实现实时传感器控制。定制训练过程和利用时间信息的“懒惰”传感器停用策略提升框架性能。该方法独立于其他物联网框架，可与之共存。 |
| [^167] | [Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm](https://arxiv.org/abs/2402.02042) | 该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。 |
| [^168] | [Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization](https://arxiv.org/abs/2402.02033) | 该竞赛旨在填补多方多目标优化问题研究不足，通过鼓励研究人员探索定制建模方法。通过基于MPIGD和MPHV指标对两部分问题进行评估，并以平均排名作为性能基准。 |
| [^169] | [ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation](https://arxiv.org/abs/2402.02029) | ScribFormer是一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割。它通过融合CNN学习的局部特征和Transformer获得的全局表示，有效克服了现有方法在涂鸦注释中学习全局形状信息的局限性。 |
| [^170] | [Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving](https://arxiv.org/abs/2402.02026) | 本文提出了一种用于角落案例检测的多模态增强物件学习器（MENOL）。该方法通过减小已知类和未知类之间的差异，并引入多模态的数据，显著提高了对新类别的召回率，并降低了训练成本。 |
| [^171] | [A Survey of Constraint Formulations in Safe Reinforcement Learning](https://arxiv.org/abs/2402.02025) | 本文综述了安全强化学习中的约束形式，包括对每种形式特别设计的算法。同时，揭示了常见问题形式之间的数学相互关系。 |
| [^172] | [Self-Supervised Contrastive Forecasting](https://arxiv.org/abs/2402.02023) | 该论文介绍了一种通过采用对比学习和增强的分解架构，并结合全局自相关性的自监督方法来解决长期预测中的挑战。实验证明，该方法在九个长期基准上的多个实验中胜过了14个基线模型。 |
| [^173] | [How well do LLMs cite relevant medical references? An evaluation framework and analyses](https://arxiv.org/abs/2402.02008) | 这项研究提出了一个问题：LLM生成的来源是否真正支持它们所做的主张？通过验证源的相关性和开发端到端的自动化流水线，研究人员发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。 |
| [^174] | [A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge](https://arxiv.org/abs/2402.01999) | 本文提出了一种新颖的超维度计算框架，用于解决在线时间序列预测问题。通过将非线性低维数据映射到高维空间进行线性超维度预测，实现了快速、高效和轻量级的预测，同时适应了时间序列数据的变化。 |
| [^175] | [Human-Centered Privacy Research in the Age of Large Language Models](https://arxiv.org/abs/2402.01994) | 大语言模型带来了隐私担忧，现有研究着重于模型本身，我们认为有必要进行以人为中心的研究，关注用户的行为和偏好，设计工具和系统，使用户能够掌控个人数据的所有权。 |
| [^176] | [Online Transfer Learning for RSV Case Detection](https://arxiv.org/abs/2402.01987) | 这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。 |
| [^177] | [Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes](https://arxiv.org/abs/2402.01981) | 本研究利用大型语言模型的零-shot能力提出了一种自我去偏差的技术，通过解释和重启两种方法，成功减少了九个不同社会群体的刻板印象程度，该技术不需要对训练数据、模型参数或解码策略进行修改，希望能启发其他零-shot偏见缓解技术的研究。 |
| [^178] | [A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions](https://arxiv.org/abs/2402.01968) | 这篇论文调查了上下文感知多代理系统的技术、挑战和未来发展方向，并提供了一个综合的概述。它介绍了上下文感知系统和多代理系统的特性，以及集成这些系统的通用过程。 |
| [^179] | [OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival Analysis](https://arxiv.org/abs/2402.01955) | 本文介绍了一种用于生存分析的新方法OPSurv，通过正交多项式积分算法提供连续时间功能性输出。该方法利用累积发生函数的初始零条件和正交多项式的分解，实现了对每个风险事件的功能近似系数的学习，并通过高斯-勒让德积分法构建累积发生函数的估计值，有效地解决了竞争风险场景中的过度拟合问题。 |
| [^180] | [Robust Counterfactual Explanations in Machine Learning: A Survey](https://arxiv.org/abs/2402.01928) | 这项调查回顾了机器学习中强健反事实解释的研究，并分析了其对强健性的考虑。该调查讨论了现有解决方案及其局限性，为未来的发展提供了基础。 |
| [^181] | [A General Framework for Learning from Weak Supervision](https://arxiv.org/abs/2402.01922) | 本文介绍了一个通用框架，利用新算法从各种弱监督中学习，通过使用非确定性有限自动机和前向-后向算法来简化计算要求，并将时间复杂度降低到线性尺度。 |
| [^182] | [Preference Poisoning Attacks on Reward Model Learning](https://arxiv.org/abs/2402.01920) | 对于从偏好比较中学习奖励模型的方法存在偏好污染攻击的漏洞，攻击者可以通过翻转少量偏好比较来对目标结果进行操纵。我们提出了两类算法方法，并证明了这些攻击在实施恶意行为方面的有效性。 |
| [^183] | [On Catastrophic Inheritance of Large Foundation Models](https://arxiv.org/abs/2402.01909) | 这篇论文讨论了大型基础模型（LFMs）中的灾难性继承问题，指出了从有偏见的大规模预训练数据到LFMs在下游任务中的行为的弱点和限制。我们提出了UIM框架，旨在理解LFMs的灾难性继承问题，并解释其中的含义。 |
| [^184] | [The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning](https://arxiv.org/abs/2402.01889) | 基于基础模型的神经符号学习和推理在NeSy任务中表现出更好的性能，并且减少了标注和手动工程的工作量。 |
| [^185] | [Inverse Reinforcement Learning by Estimating Expertise of Demonstrators](https://arxiv.org/abs/2402.01886) | 本文介绍了一个新颖的框架，IRLEED，它通过估计演示者的专业知识来解决模仿学习中的次优和异质演示的问题。IRLEED通过结合演示者次优性的普适模型和最大熵IRL框架，有效地从多样的次优演示中得出最佳策略。 |
| [^186] | [Large Language Model Agent for Hyper-Parameter Optimization](https://arxiv.org/abs/2402.01881) | 基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。 |
| [^187] | [Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models](https://arxiv.org/abs/2402.01877) | 移动试衣间是第一个基于设备内扩散模型的虚拟试穿系统，解决了高质量服装放置和模型压缩等技术挑战，实现了隐私保护和用户定制。它为时尚电子商务企业提供了有价值的服务和用户友好的虚拟试穿体验。 |
| [^188] | [The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models](https://arxiv.org/abs/2402.01874) | 这项工作回顾了将强化学习和大语言模型结合起来的研究，并提出了一个新的分类方法，以审视这两个领域之间的协同关系。 |
| [^189] | [(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice](https://arxiv.org/abs/2402.01864) | 通过对法律领域进行调查，我们对使用大型语言模型（LLM）提供专业咨询的政策考虑进行了深入分析，通过基于案例的推理方法，从20名法律专家的讨论中提取了四个影响因素：用户属性、查询特征、AI能力和影响。 |
| [^190] | [DFML: Decentralized Federated Mutual Learning](https://arxiv.org/abs/2402.01863) | DFML是一个无服务器的分散式联邦互联学习框架，能够有效地处理模型和数据的异质性，并通过相互学习在客户端之间传授知识，以获得更快的收敛速度和更高的全局准确性。 |
| [^191] | [Parametric Feature Transfer: One-shot Federated Learning with Foundation Models](https://arxiv.org/abs/2402.01862) | FedPFT 使用参数特征迁移提高了一次性联邦学习的准确性和通信效率，并在不同数据异质性设置中显示出改进效果。 |
| [^192] | [Explaining latent representations of generative models with large multimodal models](https://arxiv.org/abs/2402.01858) | 该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。 |
| [^193] | [COMET: Generating Commit Messages using Delta Graph Context Representation](https://arxiv.org/abs/2402.01841) | COMET是一种利用增量图上下文表示的新方法，通过使用transformer模型生成高质量的提交消息，并引入可定制的质量保证模块。实验证明，COMET在各项指标上优于其他技术，并与流行的GPT模型具有可比性。 |
| [^194] | [SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?](https://arxiv.org/abs/2402.01832) | SynthCLIP是一种新的框架，用于训练完全合成的CLIP模型，通过生成大规模的合成图片和标题数据集，在性能上可以与在真实数据上训练的CLIP模型相媲美。 |
| [^195] | [Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment](https://arxiv.org/abs/2402.01830) | 本文提出了一种新的无监督评估方法，利用同行评审机制在开放环境中衡量LLMs。通过为每个LLM分配可学习的能力参数，以最大化各个LLM的能力和得分的一致性。结果表明，高层次的LLM能够更准确地评估其他模型的答案，并能够获得更高的响应得分。 |
| [^196] | [Retrieval Augmented End-to-End Spoken Dialog Models](https://arxiv.org/abs/2402.01828) | 这项研究介绍了一种检索增强的端到端口语对话模型（ReSLM），通过训练语音检索器获取音频中的文本实体，并将这些实体作为输入加入到模型中以提高性能。 |
| [^197] | [Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature](https://arxiv.org/abs/2402.01826) | 本研究利用大型语言模型GPT-35-turbo，从PubMed的25 million个摘要中自动提取出男性和女性的血压平均值和标准差，填补了在考虑生物学性别差异的血压测量方差方面的研究空白。 |
| [^198] | [Fractal Patterns May Unravel the Intelligence in Next-Token Prediction](https://arxiv.org/abs/2402.01825) | 通过研究语言的分形结构，我们发现语言具有自相似性和长程相关性，从段落到整个文档都存在相似的模式和依赖性。这些发现有助于理解如何通过预测下一个词来理解文本的多个层级结构。分形参数在预测性能方面优于困惑度。这些发现提供了对语言和机制的新视角。 |
| [^199] | [Building Guardrails for Large Language Models](https://arxiv.org/abs/2402.01822) | 本文旨在为大型语言模型构建防护措施，并倡导采用系统化方法，通过与多学科团队合作来确定精确的技术要求，以减轻LLM的风险，并全面考虑不同LLM应用的多样化上下文。 |
| [^200] | [Ecologically rational meta-learned inference explains human category learning](https://arxiv.org/abs/2402.01821) | 本研究提出了一种叫做生态合理的元学习推断（ERMI）的模型，通过使用大型语言模型生成与现实世界任务统计一致的认知任务，并通过元学习框架推导适应这些任务的理性主体。实验证明，ERMI模型在定性和定量上都更好地解释了人类的数据。 |
| [^201] | [LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks](https://arxiv.org/abs/2402.01817) | LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。 |
| [^202] | [Distilling LLMs' Decomposition Abilities into Compact Language Models](https://arxiv.org/abs/2402.01812) | 本研究将LLMs的分解能力通过离线强化学习融入到紧凑模型中，通过开发AI生成的数据集和建立基准，突出了紧凑模型在复制复杂问题解决能力方面的潜力。 |
| [^203] | [HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text](https://arxiv.org/abs/2402.01806) | HQA-Attack是一种针对黑盒硬标签文本对抗攻击的方法，通过简单有效的框架生成高质量的对抗样本，解决了现有方法在有限的查询预算下难以生成具有高语义相似度和低扰动率的问题。 |
| [^204] | [Exploring the Limitations of Graph Reasoning in Large Language Models](https://arxiv.org/abs/2402.01805) | 本文测试了5种不同的大型语言模型在图推理问题上的推理深度，并发现了LLMs的局限性、偏见和属性。我们发现LLMs对于节点遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务有负面影响，并且LLMs存在积极的回应偏差，无法识别有效解的缺失。我们还提出了一种新的图推理提示技术。 |
| [^205] | [Analysis of Internet of Things implementation barriers in the cold supply chain: an integrated ISM-MICMAC and DEMATEL approach](https://arxiv.org/abs/2402.01804) | 本研究通过综述和调查研究物联网在冷链中的实施障碍，发现了13个关键障碍。其中，合规性和冷链网络是物联网采用策略的关键驱动因素。MICMAC和DEMATEL方法的应用有助于评估障碍之间的互动关系和因果关系。 |
| [^206] | [An Auction-based Marketplace for Model Trading in Federated Learning](https://arxiv.org/abs/2402.01802) | 本论文提出了一个基于拍卖的联邦学习模型交易市场，允许模型的买卖并通过适当定价和激励机制来提高模型性能和实现最大交易量。 |
| [^207] | [Large Language Models for Time Series: A Survey](https://arxiv.org/abs/2402.01801) | 本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。 |
| [^208] | [Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward](https://arxiv.org/abs/2402.01799) | 本调查文章概述了在提高LLM推理效果方面的最新方法和进展，通过实验评估不同压缩技术的有效性，并提出改进LLM推理效率的潜在未来方向。 |
| [^209] | [Variational Quantum Circuits Enhanced Generative Adversarial Network](https://arxiv.org/abs/2402.01791) | 这项工作提出了一种基于变分量子电路的混合量子-经典架构（QC-GAN），通过在手写图像生成任务上表现出更好的性能来改进传统GAN。这个架构在收敛时具有更少的训练参数和迭代次数，并且利用了量子电路的纠缠和表达能力。 |
| [^210] | [An introduction to graphical tensor notation for mechanistic interpretability](https://arxiv.org/abs/2402.01790) | 图形张量符号化是一种简单的表示张量操作的方法，对于理解深度学习系统和理解神经网络行为具有重要意义。本论文介绍了图形张量符号化的方法，并将其应用于不同的分解和解释语言模型的基础方法中。 |
| [^211] | [The Political Preferences of LLMs](https://arxiv.org/abs/2402.01789) | 该研究对LLM的政治偏好进行了分析，结果发现大多数对话型LLM表现出左翼观点，但需谨慎解读基础模型在政治倾向测试中的分类。 |
| [^212] | [LitLLM: A Toolkit for Scientific Literature Review](https://arxiv.org/abs/2402.01788) | "LitLLM: A Toolkit for Scientific Literature Review" 提出了一个基于 RAG 原则的工具包，通过使用专门的提示和指导技术，结合大型语言模型（LLM），实现了科学文献综述的自动化。这个工具包不仅可以通过转化摘要为关键词进行文献检索，还可以通过补充相关论文或关键词进行定制化的检索。 |
| [^213] | [Harm Amplification in Text-to-Image Models](https://arxiv.org/abs/2402.01787) | 我们的研究提出了危害放大现象并发展了量化危害放大的方法，考虑模型输出的危害与用户输入的情境。我们还实证地研究了不同的方法在真实场景中的应用，并量化了由危害放大引起的性别之间的影响差异。 |
| [^214] | [COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations](https://arxiv.org/abs/2402.01786) | COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。 |
| [^215] | [DoubleMLDeep: Estimation of Causal Effects with Multimodal Data](https://arxiv.org/abs/2402.01785) | 本文提出了一个利用文本和图像在因果推断和治疗效应估计中的双机器学习框架，并提出了一种生成半合成数据集的方法用于评估因果效应估计的性能。这些方法和架构在半合成数据集上进行了评估，并与标准方法进行了比较，显示了直接使用文本和图像进行因果研究的潜在好处。 |
| [^216] | [Hierarchical Multi-Label Classification of Online Vaccine Concerns](https://arxiv.org/abs/2402.01783) | 本文研究了在线疫苗关注的分层多标签分类任务，使用大型语言模型在零样本设置下检测疫苗关注，同时探索了不同提示策略的成本和准确性权衡，并提供了指导当前应用程序系统设计的具体经验教训。 |
| [^217] | [Benchmarking Spiking Neural Network Learning Methods with Varying Locality](https://arxiv.org/abs/2402.01782) | 本研究使用不同局部性对脉冲神经网络学习方法进行基准测试，并发现这些方法在性能和生物学合理性之间存在权衡。此外，研究还探讨了SNN的隐式循环特性。 |
| [^218] | [When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards](https://arxiv.org/abs/2402.01781) | 依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。 |
| [^219] | [On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble](https://arxiv.org/abs/2402.01777) | GPT-4在心理测试中展现出适度焦虑、略带男性化、诚实谦逊的特点，并且在数值素养和语言任务上表现出超过人类平均水平的认知反思能力。 |
| [^220] | [Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation](https://arxiv.org/abs/2402.01772) | 本文通过大规模研究展示了多语言机器翻译中目标端转移的动态影响。我们发现，语言相似的辅助目标语言具有强大的正向知识转移能力，并且随着相似目标语言规模的增加，转移效果进一步增强。同时，远离的辅助目标语言也可以意外地对主要语言对产生正向转移效果。 |
| [^221] | [BlackMamba: Mixture of Experts for State-Space Models](https://arxiv.org/abs/2402.01771) | BlackMamba是一种结合了Mamba SSM和MoE的新型架构，它具有竞争力的性能和较低的推断和训练成本，在大规模语言建模领域具有潜在应用价值。 |
| [^222] | [Extending Interactive Science Exhibits into the Classroom using Anthropomorphized Chatbots and Bloom's Taxonomy](https://arxiv.org/abs/2402.01770) | 本研究探索了使用生成型人工智能聊天机器人将公共科学展品转化为虚拟体验，以提高展品在课堂上的可访问性和参与度。研究还发现使用布鲁姆税目生成问题的交互式评估可能与这些聊天机器人结合使用。 |
| [^223] | [Redefining "Hallucination" in LLMs: Towards a psychology-informed framework for mitigating misinformation](https://arxiv.org/abs/2402.01769) | 该研究旨在重新定义LLMs中的“幻觉”，提出了心理学分类法，以更详细地理解和解决LLMs输出误导信息的挑战。通过借鉴人类处理类似挑战的方式，研究旨在开发策略以减轻LLMs中的幻觉。 |
| [^224] | [HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA](https://arxiv.org/abs/2402.01767) | HiQA是一个先进的多文档问答框架，使用分层的上下文增强和多路径检索机制，解决了大规模文档问答中的检索准确性问题，并在多文档环境中展示了最先进的性能。 |
| [^225] | [LLM Voting: Human Choices and AI Collective Decision Making](https://arxiv.org/abs/2402.01766) | 本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。 |
| [^226] | [LLMs Simulate Big Five Personality Traits: Further Evidence](https://arxiv.org/abs/2402.01765) | 本研究旨在研究大型语言模型（LLMs）如何模拟五大人格特质，并分析了其模拟的人格特质及稳定性，有助于深入了解LLMs在个性化人机交互方面的潜力。 |
| [^227] | [When Large Language Models Meet Vector Databases: A Survey](https://arxiv.org/abs/2402.01763) | 本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。 |
| [^228] | [Commercial AI, Conflict, and Moral Responsibility: A theoretical analysis and practical approach to the moral responsibilities associated with dual-use AI technology](https://arxiv.org/abs/2402.01762) | 本文对开发非军事应用中可能被用于冲突的人工智能系统时的道德责任进行了理论分析和实践方法的探讨，并认为利益相关者对于合理可预见的系统使用负有道德责任。 |
| [^229] | [Rethinking Interpretability in the Era of Large Language Models](https://arxiv.org/abs/2402.01761) | 大语言模型具有以自然语言解释的能力，能够重新定义解释性，并且在多个应用中展示出巨大潜力。 |
| [^230] | [Trust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik's Cube](https://arxiv.org/abs/2402.01760) | 该论文描述了一个多模态、可解释的AI驱动聊天机器人辅导系统，在解决高中学生协同解决魔方问题时，解决了数据隐私、信息泄露、滥用语言和公平性等伦理和信任问题。 |
| [^231] | [Systematic Literature Review: Computational Approaches for Humour Style Classification](https://arxiv.org/abs/2402.01759) | 这项研究通过系统性文献综述展示了计算方法在幽默风格分类中的应用情况，并指出了当前的研究空白和有希望的方向。 |
| [^232] | [Aalap: AI Assistant for Legal & Paralegal Functions in India](https://arxiv.org/abs/2402.01758) | Aalap是一个在印度法律任务上针对指令数据进行微调的AI助手，相比于gpt-3.5-turbo在测试数据中表现更好，主要教授法律推理，对律师、法官或在法律系统中工作的人有帮助。 |
| [^233] | [Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio](https://arxiv.org/abs/2402.01752) | 该研究提出了一种通过分析音频来辨别僞作和仇恨言论在僧伽罗语YouTube视频中的解决方案。这些方法包括评估视频是否包含虚假信息以及检测其中是否存在仇恨言论。 |
| [^234] | [Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia](https://arxiv.org/abs/2402.01751) | 本研究评估了ChatGPT和Bard在识别阿尔茨海默病痴呆的能力，并发现Bard在积极识别AD方面表现最好，具有较高的召回率和F1得分，但可能会将CN错误地识别为AD。对于积极识别CN，GPT-4表现出最高的真阴性。 |
| [^235] | [PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models](https://arxiv.org/abs/2402.01750) | PACE是一种利用大型语言模型的实用智能体，提供了基于图像的实用通信框架。它通过语义感知、意图解析和以意图为导向的编码来增强通信效率。为了有效利用语言模型，它使用知识库补充所需知识，引入专用提示来促进对实用通信场景和任务要求的理解，并设计思维链以帮助权衡传输效率和质量。 |
| [^236] | [Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models](https://arxiv.org/abs/2402.01749) | 本文综述了城市基础模型在智能城市发展中的重要性和潜力，并提出了一个以数据为中心的分类方法。这个新兴领域面临着一些挑战，如缺乏清晰的定义和系统性的综述，需要进一步的研究和解决方案。 |
| [^237] | [Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems](https://arxiv.org/abs/2402.01748) | 本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。 |
| [^238] | [3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems](https://arxiv.org/abs/2402.01746) | 3DG框架是一种通过将数据表示为三维张量并结合张量分解和生成性人工智能模型，来处理智能辅导系统中学习者性能稀疏数据的创新方法。 |
| [^239] | [Unveiling Molecular Moieties through Hierarchical Graph Explainability](https://arxiv.org/abs/2402.01744) | 本论文提出了一种使用图神经网络和分层可解释人工智能技术的方法，能够准确预测生物活性并找到与之相关的最重要的成分。 |
| [^240] | [The Reasoning Under Uncertainty Trap: A Structural AI Risk](https://arxiv.org/abs/2402.01743) | 当前使用的人工智能工具在面对不确定性推理时存在风险，尤其是对于LLMs的能力尚不清楚，并且无法保证性能。该报告强调了理性陷阱对人类和机器的挑战，并对潜在的人工智能风险进行了分析。 |
| [^241] | [Towards Optimizing the Costs of LLM Usage](https://arxiv.org/abs/2402.01742) | 本论文提出了一种优化LLM使用成本的方法，通过估计输出质量并解决优化问题，实现在质量和延迟方面保持成本在预算范围内或最小化成本。 |
| [^242] | [Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties](https://arxiv.org/abs/2402.01741) | 本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。 |
| [^243] | [Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models](https://arxiv.org/abs/2402.01740) | 这项研究研究了大型语言模型在选择对象时的偏见，发现偏见结构依赖于模型，对象类型调节了偏见的影响程度，导致列表中的第一个对象在输出中被过度呈现。 |
| [^244] | [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) | OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。 |
| [^245] | [Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues](https://arxiv.org/abs/2402.01737) | 本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。 |
| [^246] | [SADAS: A Dialogue Assistant System Towards Remediating Norm Violations in Bilingual Socio-Cultural Conversations](https://arxiv.org/abs/2402.01736) | SADAS是一个面向双语社会文化对话的对话助手系统，旨在通过识别规范类别、检测违例、评估严重程度、实施纠正措施并阐述理由等新颖架构，确保不同文化背景的个体之间的对话能够以尊重和理解的方式进行。 |
| [^247] | [VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models](https://arxiv.org/abs/2402.01735) | 这项研究调查了具有大型模型的视觉障碍辅助，并通过基准实验评估了模型的能力，进一步推动了视觉障碍辅助技术的发展。 |
| [^248] | [Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report](https://arxiv.org/abs/2402.01733) | RAG模型是一种有前景的方法，用于在大型语言模型中定制领域知识。本研究开发和评估了一个专为医疗保健定制的LLM-RAG流程，重点关注术前医学。 |
| [^249] | [Identifying and Improving Disability Bias in GAI-Based Resume Screening](https://arxiv.org/abs/2402.01732) | 这项研究针对生成式人工智能在招聘中的应用进行了简历审计，发现 GPT-4 对残疾相关的简历存在偏见，通过训练自定义 GPT 并遵循多样性、公平性与包容性以及残疾正义原则，可以减少这种偏见。 |
| [^250] | [Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and Symptom Analysis](https://arxiv.org/abs/2402.01730) | 本文提出了一种评估LLM生成医学诊断的方法，通过结构化交互和领域特定分析来评估其正确性和准确性。我们使用GPT-4-Vision-Preview作为LLM，并使用多模态多项选择题评估其在病理学领域的表现。 |
| [^251] | [Contextualization Distillation from Large Language Model for Knowledge Graph Completion](https://arxiv.org/abs/2402.01729) | 本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。 |
| [^252] | [Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge](https://arxiv.org/abs/2402.01728) | 硬件Phi-1.5B是一个专门针对半导体产业硬件领域的大型语言模型，通过预训练和专业分层数据集解决了硬件领域的复杂性和数据稀缺性问题。 |
| [^253] | [Prompting Diverse Ideas: Increasing AI Idea Variance](https://arxiv.org/abs/2402.01727) | 该论文通过研究在人工智能生成的想法中如何增加多样性，探讨了在创意生成过程中利用人工智能工具提高质量和生产效率的方法。研究发现不同的提示方法可以增加思路的多样性和独特想法的数量，在新产品开发领域尤其有效。 |
| [^254] | [Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages](https://arxiv.org/abs/2402.01726) | 这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。 |
| [^255] | [Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models](https://arxiv.org/abs/2402.01725) | 本篇论文介绍了一种多方面的方法来解决大型语言模型中的道德和安全挑战，包括过滤敏感词汇、检测角色扮演、实施自定义规则引擎，以及应用到不同的派生模型中。这些方法可以提高模型的安全性，降低道德和隐私方面的风险。 |
| [^256] | [An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios](https://arxiv.org/abs/2402.01723) | 本文对中国工业领域的大型语言模型进行了实证研究，评估了其在准确性和鲁棒性方面的表现。 |
| [^257] | [Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately](https://arxiv.org/abs/2402.01722) | 通过使用反馈和示例的精调过程，结合余弦相似度、LLM评估和Rouge-L得分等指标，可以提升大型语言模型（LLMs）在回答问题和提取信息方面的准确性。与零-shot LLMs相比，经过精调的模型展示了出色的问答能力，并且通过结合RAG过程进一步提高了其效果。 |
| [^258] | [Attitudes Towards and Knowledge of Non-Consensual Synthetic Intimate Imagery in 10 Countries](https://arxiv.org/abs/2402.01721) | 本文研究了10个国家超过16,000名受访者对非自愿合成亲密图像的态度和行为；尽管社会对此仍认识不足，但这种行为被认为有害；约有2.2%的受访者表示曾受害，1.8%的受访者表示曾参与过此类行为；单靠立法行动并不足以解决问题，建议技术和平台政策的改进来减少伤害。 |
| [^259] | [Deep Learning Based Amharic Chatbot for FAQs in Universities](https://arxiv.org/abs/2402.01720) | 本文提出了一个基于深度学习的阿姆哈拉语常见问题解答聊天机器人模型，可以帮助大学生解答常见问题，通过使用自然语言处理和深度学习技术，采用多种机器学习模型算法进行分析和分类，取得了最好的成绩。 |
| [^260] | [From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process](https://arxiv.org/abs/2402.01717) | 本文介绍了一种利用生成式AI和检索增强生成（RAG）方法的聊天机器人模型，名为QA-RAG，用于解决制药行业中的合规性挑战，并在准确性上取得了显著改进。 |
| [^261] | [ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis](https://arxiv.org/abs/2402.01715) | 本研究对ChatGPT、Gemini和LLaMA2等多语言情感分析模型进行了比较，发现这些模型在处理模棱两可的情境时表现良好，但在不同语言和评估中存在显著的偏差和性能不一致。这项工作为自动情感分析提供了标准化的评估方法，并呼吁改进算法和数据，以提高性能、可解释性和适用性。 |
| [^262] | [TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy](https://arxiv.org/abs/2402.01714) | TrICy是一种轻量级框架，利用意图和触发器引导数据到文本生成任务，并通过关注-复制机制有效地处理了词汇表之外的词汇。实验结果表明其在不同数据集上取得了良好的性能。 |
| [^263] | [Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data](https://arxiv.org/abs/2402.01713) | 本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。 |
| [^264] | [Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models](https://arxiv.org/abs/2402.01712) | 通过利用生成式AI模型，我们提出了一种基于社会感知的合成数据生成方法，用于自杀意念检测。与传统模型相比，我们的方法在实际数据集上表现出有竞争力的性能。 |
| [^265] | [LLM on FHIR -- Demystifying Health Records](https://arxiv.org/abs/2402.01711) | 该论文描述了开发基于大型语言模型（LLMs）和快速健康互操作性资源（FHIR）应用程序编程接口（APIs）的患者中心人工智能（AI）解决方案，以增强健康素养和健康信息可访问性。在LLM在FHIR移动应用的试点研究中，应用程序通过LLMs向患者提供了准确、相关和可理解的健康信息。 |
| [^266] | [Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators](https://arxiv.org/abs/2402.01708) | 语音生成技术的广泛应用给社会带来了伦理和安全风险。本文通过分析语音生成事件，总结出特定伤害模式，并将其分类。这些特定伤害涉及到受影响个体的曝光程度以及相关利益相关者和技术系统之间的相互作用。 |
| [^267] | [Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation](https://arxiv.org/abs/2402.01705) | 本研究超越行为主义的定义范围，提出了一种度量和减轻表征性伤害的框架，强调了大型语言模型在实施这些伤害时的脆弱性，并提出了减轻措施的建议。 |
| [^268] | [States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers](https://arxiv.org/abs/2402.01704) | 本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。 |
| [^269] | [A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles](https://arxiv.org/abs/2402.01703) | 该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。 |
| [^270] | [Fluent dreaming for language models](https://arxiv.org/abs/2402.01702) | 扩展了贪婪坐标梯度方法，设计了进化提示优化算法，实现了语言模型的流畅梦境，能够同时最大化内部特征和提示流畅性，可以自动探索模型对轻度分布之外提示的反应。 |
| [^271] | [Question answering systems for health professionals at the point of care -- a systematic review](https://arxiv.org/abs/2402.01700) | 这项系统综述旨在描述当前的医学问答系统，评估其在医疗保健中的适用性，并确定改进的方向。 |
| [^272] | [Large language model empowered participatory urban planning](https://arxiv.org/abs/2402.01698) | 本研究将大型语言模型（LLMs）应用于城市规划的参与式过程中，通过角色扮演、协作生成和反馈迭代，解决了社区级土地利用任务。实证实验表明LLM在不同规划场景上具有适应性和有效性，能够超过人类专家满意度和包容性，并与最先进的强化学习方法在服务和生态方面媲美。 |
| [^273] | [Language-Guided World Models: A Model-Based Approach to AI Control](https://arxiv.org/abs/2402.01695) | 语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。 |
| [^274] | [ARGS: Alignment as Reward-Guided Search](https://arxiv.org/abs/2402.01694) | ARGS是一个对齐作为奖励导向的搜索框架，通过在解码过程中将模型的概率预测调整为奖励信号，实现生成具有语义多样性且与人类偏好对齐的文本。与基线相比，在不同任务和模型维度下，ARGS具有持续的奖励增益，表现出很好的性能。 |
| [^275] | [Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study](https://arxiv.org/abs/2402.01693) | 本研究评估了使用大型语言生成模型（LLMs）与人类患者相比，解答非专业患者关于实验室测试结果的问题的回答质量。评估结果表明LLMs在相关性、正确性和有帮助性方面具有一定潜力，但还存在潜在问题需要改进。 |
| [^276] | [Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance](https://arxiv.org/abs/2402.01691) | 本研究调查了组织负责的人工智能（AI）治理中的算法审查委员会（ARBs）的实践情况和成果，发现了来自不同类型组织和领域的多样化定义和内部治理方法。 |
| [^277] | [An Online Hierarchical Energy Management System for Energy Communities, Complying with the Current Technical Legislation Framework](https://arxiv.org/abs/2402.01688) | 本论文提出了一个在线分层能源管理系统（HEMS），用于最小化可再生能源社区（REC）的成本，以评估其相对于本地自消耗方法的优越性。 |
| [^278] | [A Systematic Mapping Study of Digital Twins for Diagnosis in Transportation](https://arxiv.org/abs/2402.01686) | 这项研究对交通领域的数字孪生进行了系统映射，发现目前大多数研究局限于系统监测或故障检测，需要进一步研究数字孪生在诊断推理方面的应用。 |
| [^279] | [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/abs/2402.01685) | SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。 |
| [^280] | [A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm](https://arxiv.org/abs/2402.01684) | 提出了一个使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的统一框架，旨在解决高计算成本和摇摆问题。 |
| [^281] | [Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications](https://arxiv.org/abs/2402.01681) | 在表情符号研究中，我们评估了ChatGPT在处理注释和下游任务中的有效性。我们的研究结果表明ChatGPT可以作为一个可行的替代人类注释者的工具，有效地解释表情符号。 |
| [^282] | [Large Language Model based Multi-Agents: A Survey of Progress and Challenges](https://arxiv.org/abs/2402.01680) | 大型语言模型(LLMs)已广泛应用于各种任务。基于LLMs的多智能体系统在复杂问题求解和世界模拟方面取得了重要进展。这篇综述给出了基于LLMs的多智能体系统的重要方面和面临的挑战的全面讨论。 |
| [^283] | [StickerConv: Generating Multimodal Empathetic Responses from Scratch](https://arxiv.org/abs/2402.01679) | 本文介绍了StickerConv代理(Agent4SC)，该代理通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。为了利用构建的多模态共情对话数据集StickerConv，作者提出了PErceive and Generate Stickers (PEGS)模型，该模型能够生成情境相关和情感丰富的回应。 |
| [^284] | [Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge](https://arxiv.org/abs/2402.01677) | 本文提出了一种新型本体嵌入方法EIKE，通过整合外延知识和内涵知识，在外延空间和内涵空间中表示本体，并采用基于几何的方法和预训练的语言模型对实例、概念和关系进行嵌入建模。 |
| [^285] | [Language models align with human judgments on key grammatical constructions](https://arxiv.org/abs/2402.01676) | 本研究通过对比评估发现，大型语言模型（LLMs）在俘获人类行为方面的表现非常出色，不仅整体准确率高，而且能够捕捉到人类语言判断中的细微差异。 |
| [^286] | [Legal and ethical implications of applications based on agreement technologies: the case of auction-based road intersections](https://arxiv.org/abs/2402.01673) | 本文研究了基于协议技术的应用在道路交叉口拍卖中的法律和伦理问题，并提出了相应的要求。 |
| [^287] | [Prerequisite Structure Discovery in Intelligent Tutoring Systems](https://arxiv.org/abs/2402.01672) | 本研究提出了一种在智能辅导系统中整合知识结构和知识追踪的方法，通过学习者轨迹发现潜在的先决条件结构，并应用于内容推荐和评估推荐算法。 |
| [^288] | [Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice](https://arxiv.org/abs/2402.01669) | 本研究通过结合机器学习和学生选择，改进了智能辅导系统的性能和动机。使用ZPDES算法，该系统能够最大化学习进展，并在实地研究中提高了不同学生群体的学习成绩。研究还探讨了学生选择对学习效率和动机的影响。 |
| [^289] | [Determining the Difficulties of Students With Dyslexia via Virtual Reality and Artificial Intelligence: An Exploratory Analysis](https://arxiv.org/abs/2402.01668) | 该研究通过虚拟现实和人工智能的应用，为患有诵读困难的学生提供个性化的学习支持，填补了高等教育领域的空白。 |
| [^290] | [Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives](https://arxiv.org/abs/2402.01662) | 本文讨论了生成幽灵的潜在实施设计空间和其对个人和社会的实际和伦理影响，提出了研究议程以便使人们能够安全而有益地创建和与人工智能来世进行互动。 |
| [^291] | [Generative Artificial Intelligence in Higher Education: Evidence from an Analysis of Institutional Policies and Guidelines](https://arxiv.org/abs/2402.01659) | 这项研究通过分析美国116所高等教育机构的政策和指南，总结出大部分高等教育机构鼓励并提供详细指导来指引生成式人工智能（GenAI）在教学中的应用。 |
| [^292] | [Promises and pitfalls of artificial intelligence for legal applications](https://arxiv.org/abs/2402.01656) | 该研究指出当前缺乏支持证据，将AI重新定义法律职业的观点。作者调查了AI在信息处理、创造力/推理/判断任务和预测任务中的作用，并发现法律应用的评估取决于确定正确答案的难易程度和相关信息的观察性。作者建议对AI在法律环境中进行更好的评估和部署。 |
| [^293] | [A Deep Learning Approach Towards Student Performance Prediction in Online Courses: Challenges Based on a Global Perspective](https://arxiv.org/abs/2402.01655) | 该论文研究了基于深度学习的方法在全球范围内预测在线课程学生表现的挑战，并证明了深度学习模型在此方面具有有希望的性能。 |
| [^294] | [A Scoping Review of Energy Load Disaggregation](https://arxiv.org/abs/2402.01654) | 通过范围回顾，本论文发现电能负荷分解领域主要集中在家庭电力消费方面，针对工业负荷分解的研究较少。研究使用了多种方法，其中人工神经网络是最常用的。 |
| [^295] | [User-Centric AI Analytics for Chronic Health Conditions Management](https://arxiv.org/abs/2402.01652) | 本论文介绍了AI分析在管理慢性健康状况中的应用，特别是在无药物治疗方法中的挑战。研究呈现了用户中心的方法以及相关的研究问题和下一步工作。 |
| [^296] | [Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values](https://arxiv.org/abs/2402.01651) | 通过以伦理为基础的审计，本论文调查了8个主要的商业和开源大型语言模型，发现GPT-4在道德推理和伦理框架方面表现出色。 |
| [^297] | [Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education](https://arxiv.org/abs/2402.01647) | 本文开发了一个面向大学和高中学生的开源学习模块，允许学生从零开始构建自己的机器人伙伴，并提供实践经验和入门知识，包括机器人技术、机器学习、软件工程和机械工程等。同时，该模块着重强调以人为本的人工智能，使学生能够更好地理解和开发。 |
| [^298] | [L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs](https://arxiv.org/abs/2402.01643) | 本文介绍了L-Tuning，一种在自然语言推理（NLI）框架内的高效微调方法，通过对预训练的LLM中的标签标记进行微调，提高了训练准确性和效率，并增强了模型的训练细微差别。 |
| [^299] | [Evaluating Large Language Models for Generalization and Robustness via Data Compression](https://arxiv.org/abs/2402.00861) | 通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。 |
| [^300] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^301] | [Intent Assurance using LLMs guided by Intent Drift](https://arxiv.org/abs/2402.00715) | 该论文介绍了一种使用以意图漂移为引导的LLMs进行意图保证的框架，通过利用自然语言模型生成的策略识别和处理意图漂移，以实现意图与业务目标的对齐。 |
| [^302] | [Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID](https://arxiv.org/abs/2402.00672) | 该论文提出了一种同时考虑均质和异质实例级别结构，构建高质量跨模态标签关联的模态统一标签传输方法，用于无监督可见-红外人物重新识别。 |
| [^303] | [Optimized Task Assignment and Predictive Maintenance for Industrial Machines using Markov Decision Process](https://arxiv.org/abs/2402.00042) | 本文提出了一种使用马尔可夫决策过程的分布式决策方法，用于工业设备任务分配和预测性维护。该方法实现了任务分配和健康管理决策代理之间的信息共享，并将不确定性纳入决策过程中。通过详细的数学模型和案例研究，证明了该方法的有效性和实际适用性。 |
| [^304] | [Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding](https://arxiv.org/abs/2402.00024) | LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。 |
| [^305] | [Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers](https://arxiv.org/abs/2401.17870) | 提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。 |
| [^306] | [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263) | 该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。 |
| [^307] | [Tradeoffs Between Alignment and Helpfulness in Language Models](https://arxiv.org/abs/2401.16332) | 本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。 |
| [^308] | [M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining](https://arxiv.org/abs/2401.15896) | 本论文介绍了M2-编码器，通过大规模高效预训练，推动了双语图像-文本理解的发展。我们引入了一个全面的中英双语数据集BM-6B，使用新的聚合方法减少了损失计算的通信开销和GPU内存需求，提高了训练速度。在BM-6B上预训练的M²-编码器在多语言的多模态检索中创造了新的基准。 |
| [^309] | [AI-as-exploration: Navigating intelligence space](https://arxiv.org/abs/2401.07964) | 这篇论文阐述了一个被忽视但重要的科学角色，即AI作为探索。它强调通过创建和研究智能系统来揭示可能与人类和动物的智能形式不同的候选构建模块。论文通过讨论人类和大型语言模型在组合新颖和创造性概念方面的能力，说明了AI作为探索的价值。 |
| [^310] | [Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations](https://arxiv.org/abs/2401.07353) | 本文旨在为管理低空领域授权而构建公平和公正的软件系统进行工程方法研究。研究结果表明，飞行特征和环境条件被认为是最重要的因素，但还应考虑飞行员和无人机的能力。此外，许多受访者对使用机器学习算法批准或拒绝飞行请求表示反对。 |
| [^311] | [Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation](https://arxiv.org/abs/2401.00006) | 通过OpenContra框架，我们提出了一种协同训练方法，结合语言模型和强化学习，构建开放式具身代理，能够理解任意人类指令，并以较高的完成率完成目标。 |
| [^312] | [AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts](https://arxiv.org/abs/2312.16046) | AdaNAS是一个自适应后处理方法，利用自我监督神经架构搜索对集合降雨预报进行处理，能够提高降雨预测的准确性。它采用面向降雨的搜索空间和降雨层级规则化函数，有效消除噪声数据的影响。 |
| [^313] | [An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification](https://arxiv.org/abs/2312.16043) | 本文提出了一个新的多项式参数化sigmoid函数(SIGTRON)，并且介绍了其伴随的SIC模型。相比传统的成本敏感学习模型，在给定的训练数据集接近良好平衡的条件下，所提出的SIC模型对于数据集的变化更加适应，并通过创建倾斜的超平面方程来实现。 |
| [^314] | [Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast](https://arxiv.org/abs/2312.13508) | 本文介绍了一种利用原型遮罩和对比处理来解决多模态联邦学习中缺失模态问题的方法，该方法可以有效缓解由于模态缺失引起的全局模型性能下降。 |
| [^315] | [Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives](https://arxiv.org/abs/2312.13434) | 本论文提出了一种Zero-1-to-3的方法，该方法通过一批早鸟学生关注三个诊断目标，以实现领域级零样本认知诊断。它解决了跨领域诊断模型中可能融入不可转移信息的问题，从而提高了知识传输的效果。 |
| [^316] | [Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs](https://arxiv.org/abs/2312.11282) | 该论文评估了当前最先进的大型语言模型（GPT-4）在知识图谱上的对话推理能力，提出了一种基于KG推理的LLM基准代理（LLM-ARK），该代理利用全文环境提示来实现精确和适应性强的KG路径预测，并采用近端策略优化算法进行训练。 |
| [^317] | [Large Language Models for Autonomous Driving: Real-World Experiments](https://arxiv.org/abs/2312.09397) | 该论文介绍了一个基于大规模语言模型的自动驾驶框架，名为Talk2Drive，用于处理来自人类的口头指令，并根据上下文信息实现自动驾驶决策，满足个性化偏好。 |
| [^318] | [Playing Large Games with Oracles and AI Debate](https://arxiv.org/abs/2312.04792) | 本论文研究了在具有大量动作的重复游戏中实现遗憾最小化的问题，通过使用基于Oracle的算法，提出了一种高效的内部遗憾最小化算法，实现了在大型游戏中计算相关均衡的高效性。通过在AI安全辩论环境中的实验验证了算法的有效性。 |
| [^319] | [Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift](https://arxiv.org/abs/2312.00050) | Elijah是一种用于扩散模型的后门检测和消除框架，能够准确检测后门并将其效果减少至接近零，而不会显著牺牲模型的实用性。 |
| [^320] | [Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web](https://arxiv.org/abs/2311.18751) | 本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。 |
| [^321] | [Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization](https://arxiv.org/abs/2311.18703) | 该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。 |
| [^322] | [Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth](https://arxiv.org/abs/2311.18022) | 该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。 |
| [^323] | [ChatTraffic: Text-to-Traffic Generation via Diffusion Model](https://arxiv.org/abs/2311.16203) | 本研究提出了一种名为ChatTraffic的扩散模型，用于实现文本到交通生成。通过将生成模型与描述交通系统的文本结合，该方法能够解决传统交通预测方法中的两个挑战，并得到与真实交通数据一致的合成数据。 |
| [^324] | [Sample as You Infer: Predictive Coding With Langevin Dynamics](https://arxiv.org/abs/2311.13664) | 本文提出了以采样为导向的Langevin动力学的预测编码算法，通过对PC推理过程注入高斯噪声实现过阻尼的Langevin采样，并改进了结果编码器自由训练方法，通过编码器网络提供摊销的热启动。此外，还验证了一种轻量级且易于计算的预处理形式，使得算法具有更好的性能和鲁棒性。 |
| [^325] | [Toward effective protection against diffusion based mimicry through score distillation](https://arxiv.org/abs/2311.12832) | 本研究发现了攻击潜在扩散模型的脆弱点，并通过分数蒸馏采样（SDS）的策略实现了更快速和更高效的保护方法。 |
| [^326] | [The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change](https://arxiv.org/abs/2311.12664) | DURel注释工具是一个在线的、开源的工具，通过人类和计算机的注释实现了对单词使用之间的语义接近度的测量，并提供了对语义聚类和语义变化的分析功能。 |
| [^327] | [Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity](https://arxiv.org/abs/2311.12267) | 该论文研究了从一般环境中学习因果表示的问题，提供了基于这种环境生成的数据的可辨识性结果，并指出了受到围绕节点歧义的限制。同时提出了一个算法可以恢复出地面真实模型 |
| [^328] | [Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning](https://arxiv.org/abs/2311.10246) | 本论文提出了一种基于惊喜性驱动的稳健可解释的k-NN算法，通过使用信息论的角度对传统算法进行新的阐释，实现了在非参数学习中的分类、回归、密度估计和异常检测等任务。 |
| [^329] | [Divergences between Language Models and Human Brains](https://arxiv.org/abs/2311.09308) | 该论文系统地探索了语言模型（LMs）和人类大脑在语言处理方面的差异，发现在社交/情感智能和物理常识领域，LMs无法很好地捕捉到人类的表现，但在这些领域对LMs进行微调可以提高其性能。 |
| [^330] | [Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models](https://arxiv.org/abs/2311.06233) | 这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。 |
| [^331] | [PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks](https://arxiv.org/abs/2311.03415) | PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。 |
| [^332] | [Multitask Kernel-based Learning with First-Order Logic Constraints](https://arxiv.org/abs/2311.03340) | 本文提出了一个通用框架，将有监督和无监督示例与一阶逻辑背景知识整合到核机器学习中，实现多任务学习。通过将一阶逻辑约束转化为连续实现的形式，有效处理基于核的谓词的输出。 |
| [^333] | [Learning From Mistakes Makes LLM Better Reasoner](https://arxiv.org/abs/2310.20689) | 本研究探索了大型语言模型（LLMs）是否可以从错误中学习，类似于人类学习的过程，并通过引入错误纠正的数据对来改进LLMs的推理能力。实验结果表明，这种方法能够持续提升仅使用CoT进行微调后的性能。 |
| [^334] | [Agent-Specific Effects: A Causal Effect Propagation Analysis in Multi-Agent MDPs](https://arxiv.org/abs/2310.11334) | 本文介绍了一种系统化的方法，用于将代理的行动对其他代理的影响归因到因果效应上，并提出了一种衡量代理特定效应（ASE）的新的因果数量。同时，我们还介绍了ASE的反事实对应物（cf-ASE）以及识别cf-ASE的条件，并提出了一种实用的基于采样的算法。 |
| [^335] | [Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design](https://arxiv.org/abs/2310.07684) | 本文通过消息传递机制的视角，提出了一种新的对高阶网络同質性的概念化，并探索了一些处理高阶结构的策略，为超图神经网络的架构设计和性能提供了新的视野和方法。 |
| [^336] | [Guiding Language Model Math Reasoning with Planning Tokens](https://arxiv.org/abs/2310.05707) | 本论文介绍了一种通过引入规划标记来引导语言模型进行数学推理的方法。这种方法在保持推理过程中的一致性方面具有显著的准确性提升，而增加的训练参数很少。 |
| [^337] | [Interpretable Semiotics Networks Representing Awareness](https://arxiv.org/abs/2310.05212) | 这个研究描述了一个计算模型，通过追踪和模拟物体感知以及其在交流中所传达的表示来模拟人类的意识。相比于大多数无法解释的神经网络，该模型具有解释性，并可以通过构建新网络来定义物体感知。 |
| [^338] | [Deep Reinforcement Learning for Image-to-Image Translation](https://arxiv.org/abs/2309.13672) | 该论文提出了一种基于深度强化学习的图像到图像翻译方法，通过将翻译过程分解为小步骤并引入元策略和Plan概念，能够有效处理高维连续状态和动作空间的挑战。 |
| [^339] | [DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by Generative Text-to-Image Models](https://arxiv.org/abs/2309.09944) | 本文章提出了DiffusionWorldViewer，一个交互界面，旨在揭示和拓宽生成式文本到图像模型的世界观。通过在输出的不同人口统计数据之间揭示世界观，并提供编辑工具，帮助用户在生成的图像中代表他们多样化的观点，并挑战当前模型中反映的有限世界观。 |
| [^340] | [Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs](https://arxiv.org/abs/2309.09582) | Fabricator是一个开源工具集，用于生成带有Teacher LLMs标注训练数据。它通过零样本学习的方式，利用强大的LLM根据任务描述生成标注数据，可以用于训练下一阶段的自然语言处理模型。 |
| [^341] | [Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes](https://arxiv.org/abs/2309.01922) | 本文提出了一种基于策略梯度的算法用于无限时域平均奖励马尔可夫决策过程，并证明了其全局收敛性和近似O(T^3/4)的遗憾界。 |
| [^342] | [Distilled GPT for Source Code Summarization](https://arxiv.org/abs/2308.14731) | 这篇论文提出了一种用于源代码摘要的蒸馏GPT方法，通过训练一个开源模型，避免了将代码发送给不可信的第三方进行处理的安全问题。 |
| [^343] | [Iterative Hierarchy and Ranking Process (IHRP): A Novel Effective Hierarchy Method for Densely Connected Systems and Case Study in Student Performance Assessment](https://arxiv.org/abs/2306.10409) | 本文介绍了一种新颖的迭代层次和排名过程(IHRP)方法，用于在密集连接系统中构建层次结构，并通过直觉模糊语言学方法计算因素的相对重要性。这种方法在学生表现评估等决策问题中具有显著效果。 |
| [^344] | [LAraBench: Benchmarking Arabic AI with Large Language Models](https://arxiv.org/abs/2305.14982) | LAraBench是一个针对阿拉伯语自然语言处理和语音处理任务的基准测试平台，通过多种实验设置和性能衡量指标，证明最新模型通常表现优于大语言模型（LLMs）。 |
| [^345] | [SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation in Magnetic Resonance Image Reconstruction](https://arxiv.org/abs/2304.05057) | SFT-KD-Recon是一种学生友好的教师训练策略，使教师了解学生的结构和能力，并将教师的表示与学生的表示相一致，以实现MRI重建中的知识蒸馏。 |
| [^346] | [Multiobjective Evolutionary Pruning of Deep Neural Networks with Transfer Learning for improving their Performance and Robustness](https://arxiv.org/abs/2302.10253) | 这项工作提出了一种名为MO-EvoPruneDeepTL的多目标进化剪枝算法，使用迁移学习来提高深度神经网络的性能和鲁棒性。en_tdlr: This work presents MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm that uses transfer learning to improve the performance and robustness of deep neural networks. |
| [^347] | [Distributional GFlowNets with Quantile Flows](https://arxiv.org/abs/2302.05793) | 本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。 |
| [^348] | [Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind](https://arxiv.org/abs/2211.04684) | 人们在阅读故事时，通过对虚构和真实人物的类比，可以快速理解新的虚构角色。本研究填补了现有研究中忽视的少样本和元学习的心智模型（ToM）的重要性。我们提供了一个新的NLP数据集ToM-in-AMC，该数据集提供了一个评价机器元学习心智模型的现实叙事理解场景。 |
| [^349] | [ANAct: Adaptive Normalization for Activation Functions](https://arxiv.org/abs/2208.13315) | 本文研究了激活函数对神经网络前向和反向传播的负面影响，并提出了一种自适应归一化的激活函数方法ANAct来保持一致的梯度差异，通过实验证明了其有效性。 |
| [^350] | [Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning](https://arxiv.org/abs/2204.04510) | 提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。 |
| [^351] | [Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence](https://arxiv.org/abs/2202.03482) | 本文重新审视了概念激活向量（CAVs）在建模人类可理解的概念中的应用，并引入了基于模式的CAVs来提供更准确的概念方向。 |
| [^352] | [Encoding Temporal Statistical-space Priors via Augmented Representation.](http://arxiv.org/abs/2401.16808) | 通过增加表示的方式编码时间统计空间先验，以应对时间序列数据建模中的挑战。我们的方法在两个数据集上的实证推广性能明显优于五个最新的基准方法。具有高度模块化性质的方法适用于各种场景。 |
| [^353] | [Context-Former: Stitching via Latent Conditioned Sequence Modeling.](http://arxiv.org/abs/2401.16452) | Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。 |
| [^354] | [Towards Stable Preferences for Stakeholder-aligned Machine Learning.](http://arxiv.org/abs/2401.15268) | 本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。 |
| [^355] | [Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection.](http://arxiv.org/abs/2401.15222) | 本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。 |
| [^356] | [Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers.](http://arxiv.org/abs/2401.15018) | 本文通过使用特征组合和并行结构分类器来增强无文本约束的说话人验证系统的性能，在特征提取和分类阶段进行了探索，提出了不同声学特征的组合比较，并针对传统支持向量机分类器的弱点进行了改进。 |
| [^357] | [The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances.](http://arxiv.org/abs/2401.14831) | 本研究提出了一个名为粒度等级的层次级别模型，用于更好地理解机器视觉在不同环境条件下的操作。这一模型旨在提供对影响机器视觉功能的各个实体的全面概述。 |
| [^358] | [Visibility into AI Agents.](http://arxiv.org/abs/2401.13138) | 本文评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录，并提出了可能的实施方式。这些措施在集中化和去中心化部署环境中应用广泛，有助于理解和减轻AI代理带来的风险。 |
| [^359] | [Cross-lingual Editing in Multilingual Language Models.](http://arxiv.org/abs/2401.10521) | 本论文介绍了跨语言模型编辑（XME）范式，通过在一种语言中编辑事实并观察其对其他语言的更新传播，研究了多语言语言模型中的模型编辑技术（MET）的性能限制。 |
| [^360] | [DCRMTA: Unbiased Causal Representation for Multi-touch Attribution.](http://arxiv.org/abs/2401.08875) | DCRMTA提出了一种无偏的多触点归因方法，通过建立转化预测模型和构建对照触点序列来减轻偏差的影响。 |
| [^361] | [DrawTalking: Building Interactive Worlds by Sketching and Speaking.](http://arxiv.org/abs/2401.05631) | 用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。 |
| [^362] | [CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks.](http://arxiv.org/abs/2401.05043) | CreINNs是一种用于分类任务的Credal-Set Interval Neural Networks，通过保留传统的区间神经网络结构，捕捉权重不确定性，并使用概率区间的数学框架预测可信区间。实验结果表明，CreINNs在不确定性估计方面优于变分贝叶斯神经网络和深度集成，并且具有较低的计算复杂度和模型大小。 |
| [^363] | [NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems.](http://arxiv.org/abs/2401.01836) | NODEC是一种用神经ODE模型将动力学建模与控制器训练相结合的新框架，用于控制未知动态系统。 |
| [^364] | [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning.](http://arxiv.org/abs/2401.01325) | 本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。 |
| [^365] | [LLM-SAP: Large Language Model Situational Awareness Based Planning.](http://arxiv.org/abs/2312.16127) | 本文提出了一种基于大型语言模型的情景感知的紧急规划能力评估方法，并提供了新的基准和指标，以及独特的数据集。研究结果表明，提示和多智能体方案可以显著提高上下文敏感规划任务的性能。 |
| [^366] | [Piecewise polynomial regression of tame functions via integer programming.](http://arxiv.org/abs/2311.13544) | 本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。 |
| [^367] | [Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models.](http://arxiv.org/abs/2311.01441) | 本文提出了一个概念简单且轻量级的框架，通过结合知识蒸馏和数据增强的方法来提高视觉模型的鲁棒性，并从预训练的基础模型中获得鲁棒教师模型的知识。借助离散对抗蒸馏方法，我们生成更有信息量的对抗样本，取得了在对抗性样本上鲁棒性显著提升的结果。此外，我们提供了理论框架来支持在知识蒸馏和数据增强设置中使用鲁棒教师模型，并展示了在不同学生模型上的显著性能提升。我们的方法在计算负载方面的开销较小，并可以与其他数据增强方法轻松结合。 |
| [^368] | [Vision-Language Foundation Models as Effective Robot Imitators.](http://arxiv.org/abs/2311.01378) | 该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。 |
| [^369] | [An energy-based comparative analysis of common approaches to text classification in the Legal domain.](http://arxiv.org/abs/2311.01256) | 本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。 |
| [^370] | [Diffusion Models for Reinforcement Learning: A Survey.](http://arxiv.org/abs/2311.01223) | 强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。 |
| [^371] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^372] | [Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings.](http://arxiv.org/abs/2310.17451) | 这篇论文提出了一种神经符号学习方法，AbdGen，用于将知识推理系统与神经视觉生成模型集成。它解决了符号赋值和规则学习的问题，通过量化诱导方法实现可靠高效的符号赋值，通过对比元诱导方法实现精确的规则学习。 |
| [^373] | [DoGE: Domain Reweighting with Generalization Estimation.](http://arxiv.org/abs/2310.15393) | DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。 |
| [^374] | [Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes.](http://arxiv.org/abs/2310.11677) | 本论文提出了一种加速自然策略梯度算法（ANPG），用于解决无限时间无折扣奖励马尔可夫决策过程。ANPG实现了样本和迭代复杂度的显著改进，克服了现有算法的局限性，并达到了最新的技术成果。 |
| [^375] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^376] | [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis.](http://arxiv.org/abs/2310.10477) | 该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。 |
| [^377] | [Mirage: Model-Agnostic Graph Distillation for Graph Classification.](http://arxiv.org/abs/2310.09486) | Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。 |
| [^378] | [Uncovering hidden geometry in Transformers via disentangling position and context.](http://arxiv.org/abs/2310.04861) | 本文通过分解transformer的隐藏状态，揭示了其在语义理解中的隐含几何结构。 |
| [^379] | [Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing.](http://arxiv.org/abs/2310.03052) | Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。 |
| [^380] | [Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems.](http://arxiv.org/abs/2310.02299) | 本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。 |
| [^381] | [AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model.](http://arxiv.org/abs/2310.02054) | 本研究提出了一种新的框架AlignDiff，通过将强化学习与人类反馈相结合，量化人类偏好并指导零-shot行为定制的扩散规划，解决了将代理行为与多样的人类偏好相一致的挑战问题。 |
| [^382] | [Combining Spatial and Temporal Abstraction in Planning for Better Generalization.](http://arxiv.org/abs/2310.00229) | Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。 |
| [^383] | [Exploring the Influence of Information Entropy Change in Learning Systems.](http://arxiv.org/abs/2309.10625) | 本研究探索了在深度学习系统中引入噪声对性能的影响，证明了特定噪声可以在降低任务复杂性的条件下提升深度架构的性能，通过实验证明了在大规模图像数据集中的显著性能提升。 |
| [^384] | [Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances.](http://arxiv.org/abs/2309.10426) | 多对象图形可用性网络（MOGAN）模型化了复合对象的可用性，预测了将新对象放置在复合对象上的效果。在构建具有特定高度或属性的塔等任务中，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们的系统能够正确建模非常复杂的复合对象的可用性，并在模拟和真实环境中展示了其适用性。 |
| [^385] | [Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model.](http://arxiv.org/abs/2309.09357) | 本研究利用大型语言模型（LLMs）来促进患者和医生之间的异步通信，通过访谈研究了解了他们对LLMs的需求，并构建了一个名为Talk2Care的LLM驱动的通信系统。 |
| [^386] | [Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity.](http://arxiv.org/abs/2309.06364) | 本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。 |
| [^387] | [Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation.](http://arxiv.org/abs/2309.01365) | 通过优化的时间金字塔压缩和放大变换器，该论文提出了一种在视频序列中准确估计人体3D姿势的方法。该方法通过扩展时间建模和细化特征交互来解决其他方法中的细节和稳定性问题，展示了较好的效果。 |
| [^388] | [Causal Strategic Learning with Competitive Selection.](http://arxiv.org/abs/2308.16262) | 我们研究了具有竞争选择的因果战略学习中的代理选择问题，并提出了最佳选择规则的数学形式和实现机制。 |
| [^389] | [VIGC: Visual Instruction Generation and Correction.](http://arxiv.org/abs/2308.12714) | 本文提出了VIGC框架，使多模态大型语言模型能够生成和纠正视觉指令数据，解决了缺乏高质量调整数据的挑战。 |
| [^390] | [Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances.](http://arxiv.org/abs/2308.11129) | 本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。 |
| [^391] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^392] | [Bayesian Flow Networks.](http://arxiv.org/abs/2308.07037) | 本文介绍了贝叶斯流网络（BFNs），一种新的生成模型，它通过贝叶斯推断修改了一组独立分布的参数，并将其作为输入传递给神经网络来生成另一个相互依赖的分布。该方法不需要前向过程，适用于连续和离散数据，并具有优化数据压缩的功能。 |
| [^393] | [Bringing order into the realm of Transformer-based language models for artificial intelligence and law.](http://arxiv.org/abs/2308.05502) | 本文提供了第一个对基于Transformer的语言模型在法律领域的人工智能问题和任务中的方法的系统概述。文章旨在突出这一领域的研究进展，以进一步了解Transformer在支持法律流程中的AI成功贡献以及当前的局限性。 |
| [^394] | [Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance.](http://arxiv.org/abs/2308.04215) | 提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。 |
| [^395] | [Calibration in Deep Learning: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2308.01222) | 本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。 |
| [^396] | [DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI.](http://arxiv.org/abs/2307.10172) | DialogStudio是迄今为止最大且最多样化的对话数据集合，包含从开放领域对话到任务导向对话、自然语言理解、会话推荐、对话摘要和知识驱动对话的数据。它为对话研究和模型训练提供了丰富而多样化的资源。 |
| [^397] | [HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning.](http://arxiv.org/abs/2307.09653) | HAT-CL是一个基于任务的硬注意力PyTorch库，以提供对连续学习中的灾难性遗忘现象的解决方案。它通过改善HAT的可用性和兼容性问题，并提供对现有网络复用的支持，实现了对PyTorch模块的自动化梯度操作和转换。此外，HAT-CL还引入了新颖的掩码操作技术。 |
| [^398] | [Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks.](http://arxiv.org/abs/2307.08430) | 本论文研究了在大规模异构信息网络中利用长程依赖的重要性，并提出了一种名为长程元路径搜索和渐进抽样（LMSPS）的自动框架。通过动态缩小搜索空间和专门的元路径选择，实验证明了LMSPS的有效性。 |
| [^399] | [DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding.](http://arxiv.org/abs/2307.06924) | DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。 |
| [^400] | [Generating Benchmarks for Factuality Evaluation of Language Models.](http://arxiv.org/abs/2307.06908) | 该论文提出了一个名为FACTOR的方法，用于生成用于语言模型事实性评估的基准数据集。通过自动转换事实语料库，评估语言模型根据语料库生成真实事实的倾向与生成不正确陈述的能力。实验结果表明，该基准数据集的分数随模型大小增加而增加，在LM与检索方法结合时性能得到改善。困惑度和基准数据集分数之间存在相关性，但不总是一致。 |
| [^401] | [Enhancing Adversarial Training via Reweighting Optimization Trajectory.](http://arxiv.org/abs/2306.14275) | 本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。 |
| [^402] | [System-Level Natural Language Feedback.](http://arxiv.org/abs/2306.13588) | 本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了通过任务度量设计和语言模型提示设计，如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型，并展示了使用系统级别反馈和实例级别反馈的有效性。 |
| [^403] | [TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support.](http://arxiv.org/abs/2306.13339) | TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。 |
| [^404] | [Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?.](http://arxiv.org/abs/2306.10224) | 研究发现生成式 AI 工具 ChatGPT 可以更有效地展示股票市场相关信息，提出了信息膨胀指标并证明其与负面的资本市场后果相关，同时展示其在构建针对性总结方面的效果。 |
| [^405] | [Predicting Software Performance with Divide-and-Learn.](http://arxiv.org/abs/2306.06651) | 本文提出了一种名为$DaL$的基于分割学习的方法，用于预测高度配置的软件系统的性能。实验证明了该方法的有效性。 |
| [^406] | [STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.](http://arxiv.org/abs/2306.00937) | STEVE-1 是一种新的生成模型，能够在Minecraft中跟随各种短期开放型文本和视觉指令。STEVE-1利用预先训练的模型和最佳实践，通过自监督的行为克隆和回顾重新标记来微调，避免了昂贵的人工注释。 |
| [^407] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^408] | [A Rational Model of Dimension-reduced Human Categorization.](http://arxiv.org/abs/2305.14383) | 提出了一个基于层次化混合模型的分类模型，以及基于生成过程的低维潜在空间的分类解释，该模型学习类别表示和特征集合，适用于高维刺激下，支持零-shot学习，并验证了该模型。 |
| [^409] | [How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data.](http://arxiv.org/abs/2305.14195) | 本论文提出了一种使用人类人口数据对语言模型进行评估的框架，并发现GPT-3.5在不同任务中表现不同，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。 |
| [^410] | [Relabel Minimal Training Subset to Flip a Prediction.](http://arxiv.org/abs/2305.12809) | 本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。 |
| [^411] | [SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition.](http://arxiv.org/abs/2305.08029) | SongDriver2实现了基于情绪的实时音乐编排，并提出了柔和过渡机制，使音乐具有高度真实性和平滑过渡。 |
| [^412] | [Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification.](http://arxiv.org/abs/2305.04228) | 本研究提出了使用异构有向超图表示AST，并使用异构有向超图神经网络处理图形进行代码分类，超过了现有方法。 |
| [^413] | [High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling.](http://arxiv.org/abs/2305.02614) | 本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。 |
| [^414] | [Lightweight, Pre-trained Transformers for Remote Sensing Timeseries.](http://arxiv.org/abs/2304.14065) | 设计针对远程传感器数据的自监督学习模型和训练技术，可以得到表现更好且更小的模型。预训练的遥感时间序列Transformer（Presto）在几个遥感基准测试中实现了最先进的结果。 |
| [^415] | [Fundamental Limitations of Alignment in Large Language Models.](http://arxiv.org/abs/2304.11082) | 本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。 |
| [^416] | [Operating critical machine learning models in resource constrained regimes.](http://arxiv.org/abs/2303.10181) | 本文分享了在关键场景下使用机器学习模型时资源消耗和性能之间的权衡方法。考虑到模型在全球诊所中的部署，机器学习界正在为改进模型效率而努力。 |
| [^417] | [FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions.](http://arxiv.org/abs/2210.01595) | FreDSNet是一种深度学习解决方案，可以从单个全景图像中实现室内环境的语义三维理解，并且通过利用频率域中的卷积从而获得更宽广的感受野。它是第一个联合提供单目深度估计和语义分割的网络。 |

# 详细

[^1]: 用Vabs-Net进行多级蛋白质预训练

    Multi-level protein pre-training with Vabs-Net

    [https://rss.arxiv.org/abs/2402.01481](https://rss.arxiv.org/abs/2402.01481)

    这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。

    

    最近几年，三维结构预训练蛋白质模型的发展迅猛，相较于预训练蛋白质语言模型，在各种下游任务中取得了重大进展。然而，大多数现有的基于结构的预训练模型主要关注残基水平，即α碳原子，而忽略了其他原子，如侧链原子。我们认为，在残基和原子水平上对蛋白质进行建模很重要，因为侧链原子对于许多下游任务（如分子对接）也是至关重要的。然而，我们发现在预训练中天真地组合残基和原子信息通常会失败。我们发现，信息泄漏是包含原子结构的输入导致残基级预训练任务变得琐碎并导致残基表示不够充分的一个关键原因。为了解决这个问题，我们引入了一种基于跨度掩码预训练策略的三维蛋白质预训练方法。

    In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
    
[^2]: 通过Lipschitz正则化在规模上实现零样本机器遗忘

    Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization

    [https://rss.arxiv.org/abs/2402.01401](https://rss.arxiv.org/abs/2402.01401)

    通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。

    

    为了遵守人工智能和数据规定，从训练得到的机器学习模型中遗忘私人或受版权保护的信息的需求变得越来越重要。遗忘的关键挑战是及时忘记必要的数据，同时保持模型性能。在这项工作中，我们解决了零样本遗忘的场景，即只有一个经过训练的模型和要遗忘的数据，遗忘算法必须能够移除数据。根据这样定义，现有的最先进的方法是不够的。基于Lipschitz连续性的概念，我们提出了一种方法，通过对样本扰动的输出进行平滑处理来诱导遗忘。我们展示了这种平滑性成功地实现了遗忘，同时保持了总体模型性能。我们对我们的方法进行了广泛的经验评估，包括一系列当代基准测试，验证了我们的方法在严格的零样本约束下达到了最先进的性能。

    To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
    
[^3]: LoTR: 低张量秩权重自适应

    LoTR: Low Tensor Rank Weight Adaptation

    [https://rss.arxiv.org/abs/2402.01376](https://rss.arxiv.org/abs/2402.01376)

    LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。

    

    在本文中，我们将大型语言模型（LLM）上的低秩适应（LoRA）思想推广和扩展，这些模型基于Transformer架构。广泛使用的LoRA类方法是基于梯度更新的矩阵分解。我们引入了LoTR，一种新颖的LLM参数高效调优方法，它以张量分解的形式表示参数的梯度更新。每个层的低秩适配器都由三个矩阵的乘积构成，而张量结构是由这个乘积的左右乘子在层之间共享引起的。通过对低秩张量表示的一系列层同时压缩，LoTR能够比LoRA在特别是对于深层模型具有更好的参数效率。此外，核心张量不依赖于原始权重维度，可以任意缩小，从而实现非常廉价和快速的下游调优。

    In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
    
[^4]: 在分布变化中的监督算法公平性：一项综述

    Supervised Algorithmic Fairness in Distribution Shifts: A Survey

    [https://rss.arxiv.org/abs/2402.01327](https://rss.arxiv.org/abs/2402.01327)

    这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。

    

    在分布变化下的监督公平机器学习是一个新兴领域，解决了面对从源领域到目标领域的数据分布变化时，如何保持公平和无偏预测的挑战。在现实世界的应用中，机器学习模型通常是在特定数据集上进行训练，但在部署时，数据分布可能因各种因素而随时间发生变化。这种变化可能导致不公平的预测，对特定通过敏感属性（如种族和性别）来表征的群体产生不均衡的影响。在这项调查中，我们对各种类型的分布变化进行了总结，并全面调查了基于这些变化的现有方法，在文献中突出了六种常用的方法。此外，这份调查列出了用于实证研究的公开可用数据集和评估指标。我们进一步探讨了与相关研究领域的交互关系，并讨论了其中的重要创新和贡献。

    Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
    
[^5]: 联邦取消学习: 稳定性和公平性的视角

    Federated Unlearning: a Perspective of Stability and Fairness

    [https://rss.arxiv.org/abs/2402.01276](https://rss.arxiv.org/abs/2402.01276)

    本文研究了在数据异构性情况下的联邦取消学习（FU）的稳定性和公平性问题。我们提出了关键指标来评估FU，重点考虑了验证、全局稳定性和局部公平性，并研究了其中的权衡。通过理论分析，我们洞察了数据异构性对FU的影响，并提出了管理权衡的FU机制。通过实证验证，我们证实了这些机制的有效性。

    

    本文探讨了在数据异构性情况下联邦取消学习（FU）的多方面影响。我们介绍了FU评估的关键指标，重点关注验证，全局稳定性和局部公平性，并研究了内在的权衡。此外，我们通过一个优化框架对具有数据异构性的取消学习过程进行了形式化。我们的核心贡献在于对FU中权衡进行了全面的理论分析，并提供了数据异构性对FU的影响的见解。利用这些见解，我们提出了管理权衡的FU机制，为FU机制的进一步发展提供指导。我们通过实证验证了我们的FU机制有效地平衡了权衡，确认了从我们的理论分析中得出的见解。

    This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.
    
[^6]: 使用大型语言模型的高效因果图发现

    Efficient Causal Graph Discovery Using Large Language Models

    [https://rss.arxiv.org/abs/2402.01207](https://rss.arxiv.org/abs/2402.01207)

    提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。

    

    我们提出了一个新的框架，利用LLMs进行完整的因果图发现。之前基于LLM的方法采用了成对查询的方法，但这需要二次查询的数量，对于较大的因果图来说很快变得不可行。相反，提出的框架采用了广度优先搜索（BFS）的方法，只需要线性数量的查询。我们还展示了当有所观察数据可用时，提出的方法可以轻松地进行结合以提高性能。除了更具时间和数据效率外，提出的框架在不同大小的真实因果图上取得了最先进的结果。结果证明了提出方法在发现因果关系方面的有效性和效率，展示了其在不同领域的因果图发现任务中的广泛适用性潜力。

    We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
    
[^7]: 自监督学习在非连续表格数据中的应用调研

    A Survey on Self-Supervised Learning for Non-Sequential Tabular Data

    [https://rss.arxiv.org/abs/2402.01204](https://rss.arxiv.org/abs/2402.01204)

    本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    

    自监督学习（SSL）已经被应用于各个领域的许多最先进的模型中，其中SSL通过定义基于无标签数据集的预训练任务来学习上下文化和鲁棒的表示。最近，SSL已成为探索表格数据领域中表示学习能力的新趋势，这是一项更具挑战性的任务，因为它没有明确的关系来学习描述性的表示。本调研旨在系统地回顾和总结自监督学习在非连续表格数据（SSL4NS-TD）中的最新进展和挑战。首先，我们给出了NS-TD的正式定义，并阐明了它与相关研究的关联。然后，这些方法被分为三组——预测性学习、对比学习和混合学习，并介绍了每个方向的代表性方法的动机和优点。在此基础上，还介绍了SSL4NS-TD的应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
    
[^8]: 用于强化混合边缘云的认知互联网的构建块

    Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud

    [https://rss.arxiv.org/abs/2402.00876](https://rss.arxiv.org/abs/2402.00876)

    认知互联网超越了认知物联网，使连接的物体能够独立地获取知识和理解，并在整个网络中集成了协作智能。本文深入探讨了“认知互联网”范式的基础要素、独特特征、益处和工业影响。

    

    随着我们从移动互联网过渡到“认知互联网”，我们在如何与技术和智能互动方面发生了重大变化。我们认为，认知互联网超越了认知物联网（认知IoT），使连接的物体能够独立地获取知识和理解。与移动互联网和认知IoT不同，认知互联网在整个网络中集成了协作智能，将认知物联网领域与系统范围的协作和人类智能融合在一起。这种集成智能促进了设备、服务、实体和个人之间在不同领域内的互动，同时保持决策自主性并适应各种身份。本文深入探讨了“认知互联网”范式的基础要素、独特特征、益处和工业影响。它强调了适应性人工智能基础设施和混合边缘云（HEC）平台的重要性。

    As we transition from the mobile internet to the 'Cognitive Internet,' a significant shift occurs in how we engage with technology and intelligence. We contend that the Cognitive Internet goes beyond the Cognitive Internet of Things (Cognitive IoT), enabling connected objects to independently acquire knowledge and understanding. Unlike the Mobile Internet and Cognitive IoT, the Cognitive Internet integrates collaborative intelligence throughout the network, blending the cognitive IoT realm with system-wide collaboration and human intelligence. This integrated intelligence facilitates interactions between devices, services, entities, and individuals across diverse domains while preserving decision-making autonomy and accommodating various identities.   The paper delves into the foundational elements, distinct characteristics, benefits, and industrial impact of the 'Cognitive Internet' paradigm. It highlights the importance of adaptable AI infrastructures and hybrid edge cloud (HEC) plat
    
[^9]: 语言模型作为归纳推理器

    Language Models as Inductive Reasoners

    [https://rss.arxiv.org/abs/2212.10923](https://rss.arxiv.org/abs/2212.10923)

    该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。

    

    归纳推理是人类智能的核心组成部分。在计算机科学中的归纳推理研究中，形式语言被用作知识（事实和规则）的表示。然而，形式语言会给归纳推理带来系统性问题，例如无法处理自然语言这样的原始输入、对错误标记的数据敏感以及处理模糊输入的能力不足。为此，我们提出了一种新的归纳推理范式（任务），即从自然语言事实中归纳出自然语言规则，并创建了一个被称为DEER的数据集，其中包含1.2k个规则-事实对，规则和事实以自然语言书写。还提出并分析了用于评估此任务的新的自动评估指标。通过DEER，我们研究了一种现代的归纳推理方法，其中我们使用自然语言作为知识的表示而不是形式语言，并使用预训练的语言模型。

    Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
    
[^10]: HASSOD：分层自适应无监督目标检测

    HASSOD: Hierarchical Adaptive Self-Supervised Object Detection

    [https://arxiv.org/abs/2402.03311](https://arxiv.org/abs/2402.03311)

    HASSOD是一种分层自适应无监督目标检测系统，通过自监督学习，提高了检测性能和可解释性。

    

    人类视觉感知系统展示了在学习时没有明确监督的情况下以及理解物体的部分整体组成方面的卓越能力。受到这两个能力的启发，我们提出了一种新颖的方法——分层自适应无监督目标检测(HASSOD)，该方法学习在没有人类监督的情况下检测物体并理解其组成。HASSOD采用分层自适应聚类策略，根据自监督的视觉表示将区域分组为对象掩码，自适应确定每个图像中的对象数量。此外，HASSOD通过分析掩码之间的覆盖关系和构建树结构来识别对象的层次级别。这种额外的自监督学习任务可以提高检测性能并增强可解释性。最后，我们放弃了之前方法中使用的低效的多轮自我训练过程，而采取了一种自我训练的方法，该方法在时间和计算资源上更高效。

    The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead ad
    
[^11]: V-IRL: 将虚拟智能与现实生活联系起来

    V-IRL: Grounding Virtual Intelligence in Real Life

    [https://arxiv.org/abs/2402.03310](https://arxiv.org/abs/2402.03310)

    V-IRL是一个平台，可以让人工智能代理在虚拟环境中与现实世界进行互动，旨在将数字和物理世界之间的差距缩小，并开发出具有丰富感知、决策和与真实数据互动能力的代理。

    

    人类生活在地球上，而现代人工智能代理所创造的数字领域之间存在着感官差距。为了开发出在现实世界中能像人类一样灵活感知、思考和行动的人工智能代理，必须弥合数字和物理世界之间的逼真差距。我们如何在一个像我们所居住的世界中一样丰富多样的环境中体现代理，而不受真实硬件和控制所施加的约束？为了实现这个目标，我们引入了V-IRL: 一种平台，可以使代理在虚拟而逼真的环境中与现实世界进行可扩展的互动。我们的平台既是一个开发代理完成各种实际任务的游乐场，又是一个广阔的测试基地，用于衡量在感知、决策和与全球真实数据的互动能力等方面的进展。

    There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.
    
[^12]: 扩散模型是否学习语义有意义和高效的表示？

    Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?

    [https://arxiv.org/abs/2402.03305](https://arxiv.org/abs/2402.03305)

    本研究通过实验探究了条件DDPMs学习生成2D球形高斯凸起的过程，在学习的过程中发现了潜在表示的关键，产生了与不同阶段对应的 qualitatively 不同的生成行为。

    

    扩散模型能够以不寻常的方式生成图像，例如宇航员骑在月球上的马，并且有正确的阴影。这些输出表明了模型具有组合泛化的能力，但是模型是如何做到这一点的呢？我们在条件DDPMs上进行了控制实验，学习生成以指定的$x$和$y$位置为中心的2D球形高斯凸起。我们的结果表明，产生语义有意义的潜在表示对于实现高性能至关重要。在学习过程中，模型经历了三个不同的潜在表示阶段：(A阶段)没有潜在结构，(B阶段)一个混乱状态的2D流形，以及(C阶段)一个有序的2D流形。对应于这些阶段，我们发现了 qualitatively 不同的生成行为：1）生成多个凸起，2）生成一个凸起，但$x$和$y$位置不准确，3）生成一个凸起且位置准确。

    Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera
    
[^13]: 没关系：大语言模型中的指令覆盖和调节

    Nevermind: Instruction Override and Moderation in Large Language Models

    [https://arxiv.org/abs/2402.03303](https://arxiv.org/abs/2402.03303)

    大语言模型具有覆盖和调节指令的能力，较大的模型在覆盖内部和上下文指令方面表现最佳，并且在绳索扩展时需要保持缓冲区来保持指令遵循能力。

    

    鉴于近期大语言模型（LLMs）的令人印象深刻的能力，我们对最流行的专有模型和不同大小的开源模型进行了调查和基准测试，以解决在冲突情况下的明确指令遵循任务，例如覆盖。这些包括模型在其权重中覆盖知识的能力，覆盖（或调节）提示中提取的知识的能力，以及进行完全越狱的能力。实验表明，可以改进指令遵循的几个关键发现 - 较大的模型在遵循覆盖内部和上下文指令方面表现最佳，并且非常服从，甚至有些过度。当通过绳索扩展来扩展到更长的上下文时，需要保持与困惑边缘的显著缓冲区，以保持指令遵循能力。最后，我们观察到指令遵循的改善，以及随之而来的指令覆盖/越狱。

    Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja
    
[^14]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^15]: Ginger: 一种用于通用神经网络的线性复杂度高效曲率近似方法

    Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks

    [https://arxiv.org/abs/2402.03295](https://arxiv.org/abs/2402.03295)

    Ginger是一种用于通用神经网络的高效曲率近似方法，具有线性复杂度。它通过特征分解来逆向计算广义高斯牛顿矩阵，避免了传统方法中的高内存和高时间复杂度问题。

    

    二阶优化方法，如广义高斯牛顿法，由于利用了目标函数的曲率信息和预处理矩阵，被认为更加强大。尽管在理论上具有诱人的优势，但它们不易应用于现代深度学习。主要原因是计算矩阵的逆所需的二次内存和三次时间复杂度是不可行的，即使使用先进的硬件也不行。在这项工作中，我们提出了Ginger，一种用于广义高斯牛顿矩阵逆的特征分解方法。我们的方法在每次迭代中具有高效的线性内存和时间复杂度。我们直接维护条件矩阵的逆，以使近似更加准确，而不是近似条件矩阵。我们提供了Ginger在非凸目标上的收敛结果。我们在不同任务和不同模型架构上的实验证实了我们方法的有效性。我们的代码...

    Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code i
    
[^16]: Flora: 低秩适配器是悄悄的梯度压缩器

    Flora: Low-Rank Adapters Are Secretly Gradient Compressors

    [https://arxiv.org/abs/2402.03293](https://arxiv.org/abs/2402.03293)

    本文研究了低秩适配器的动力学，并提出了一种基于随机投影的方法Flora，通过重新采样投影矩阵实现高秩更新，同时减少优化状态的空间复杂度。

    

    尽管大型神经网络展示了完成不同任务的显着能力，但它们需要过多的内存使用来存储训练的优化状态。为了缓解这个问题，提出低秩适配（LoRA）来通过训练更少的参数来减少优化状态。然而，LoRA将整体权重更新矩阵限制为低秩，限制了模型的性能。在这项工作中，我们研究了LoRA的动力学，并确定它可以近似为随机投影。基于这一观察，我们提出了Flora，它能够通过重新采样投影矩阵实现高秩更新，同时享受优化状态的次线性空间复杂度。我们在不同任务和模型架构上进行实验证实了我们方法的有效性。

    Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.
    
[^17]: InstanceDiffusion：图像生成的实例级控制

    InstanceDiffusion: Instance-level Control for Image Generation

    [https://arxiv.org/abs/2402.03290](https://arxiv.org/abs/2402.03290)

    InstanceDiffusion通过添加实例级控制，使文本到图像的扩散模型能够产生高质量图像，并在不同的位置条件下超过了专业先进模型。

    

    文本到图像的扩散模型可以生成高质量图像，但不能对图像中的单个实例进行精确控制。我们引入InstanceDiffusion，将精确的实例级控制添加到文本到图像的扩散模型中。InstanceDiffusion支持每个实例的自由形式语言条件，并允许以简单的单个点、涂鸦、边界框或复杂的实例分割掩码及其组合方式指定实例位置。我们提出了三个重要的改进文本到图像模型的方法，以实现精确的实例级控制。我们的UniFusion块实现了文本到图像模型的实例级条件，ScaleU块提高了图像的保真度，Multi-instance采样器提高了多实例的生成效果。InstanceDiffusion在每个位置条件下明显超过了专业先进模型。值得注意的是，对于COCO数据集，我们在box输入方面超过了以前的最先进技术20.4%AP50 box。

    Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, 
    
[^18]: 让每一步都有价值：使用MCTS的LLM基础高质量RTL代码生成

    Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS

    [https://arxiv.org/abs/2402.03289](https://arxiv.org/abs/2402.03289)

    本文介绍了一种使用蒙特卡罗树搜索进行前瞻的自动变换器解码算法，可解决现有大型语言模型在RTL代码生成中存在的编译失败和PPA不敏感的问题，并在性能上取得了显著改进。

    

    现有的用于寄存器传输级代码生成的大型语言模型(LLM)面临编译失败和亚最优功耗、性能和面积(PPA)效率等挑战。这是由于传统变换器解码算法缺乏对PPA的意识所致。为此，我们提出了一种自动变换器解码算法，它通过蒙特卡罗树搜索来进行前瞻，引导变换器生成可编译的、功能正确的、PPA优化的代码。在RTL代码集上使用经过微调的语言模型进行的实证评估表明，与仅使用提示的方法相比，我们提出的技术一致地生成功能正确的代码，并有效解决了朴素大型语言模型对PPA不敏感的缺点。对于最先进的LLM生成的最大设计（16位加法器），我们的技术可以在面积延迟乘积上实现31.8%的改进。

    Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.
    
[^19]: 无需训练的一致性文本到图像生成

    Training-Free Consistent Text-to-Image Generation

    [https://arxiv.org/abs/2402.03286](https://arxiv.org/abs/2402.03286)

    本文提出了一种无需训练的方法ConsiStory，通过共享预训练模型的内部激活，实现了一致的文本到图像生成。引入了主题驱动的共享注意力块和基于对应的特征注入，促进了图像之间的主题一致性，并采用了策略来保持布局多样性。

    

    文本到图像模型通过自然语言引导图像生成过程，提供了一种新的创造性灵活性。然而，使用这些模型在多样化的提示下一致地描绘相同的主题仍然具有挑战性。现有方法通过优化模型来教授它描述特定用户提供主题的新词汇或者为模型添加图像条件。这些方法要求针对每个主题进行漫长的优化或进行大规模预训练。此外，它们在将生成的图像与文本提示对齐和描绘多个主题方面遇到困难。在这里，我们介绍了一种无训练方法ConsiStory，通过共享预训练模型的内部激活来实现一致的主题生成。我们引入了一个主题驱动共享注意力块和基于对应的特征注入，以促进图像之间的主题一致性。此外，我们开发了策略以鼓励布局多样性，同时保持主题一致性。

    Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
    
[^20]: 交易，还是不交易（或者谁知道）？使用大型语言模型预测对话中的不确定性

    Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models

    [https://arxiv.org/abs/2402.03284](https://arxiv.org/abs/2402.03284)

    本论文研究了如何用语言模型来表示对话中的不确定性，并提出了改进模型校准的微调策略。实验证明，这些策略可以使较小的模型具备与大型预训练模型相当的性能。

    

    有效的对话者考虑他人的不确定目标、信念和情绪。但即使是最佳的人类对话者也无法完美地预测对话的轨迹。语言模型能够多好地表示对话中固有的不确定性？我们提出了FortUne Dial，这是“对话预测”任务的扩展：评估不仅仅以准确度为标准，还采用了对不确定性敏感的度量方法，有效地使个别实例可以放弃。我们研究了语言模型可能表示结果不确定性的两种方式（内部使用分数和直接使用令牌），并提出了改进两种表示的校准的微调策略。对八个困难的谈判语料库进行的实验表明，我们提出的微调策略（一种传统的监督策略和一种离线策略强化学习策略）可以使较小的开源模型校准得上与其尺寸相当的预训练模型。

    Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing "conversation forecasting" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their si
    
[^21]: 一个部分观察到的奖励状态在RLHF中的框架

    A Framework for Partially Observed Reward-States in RLHF

    [https://arxiv.org/abs/2402.03282](https://arxiv.org/abs/2402.03282)

    这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。

    

    最近几年来，强化学习从人类反馈（RLHF）的研究因其在LLMs的发展中起到的作用而变得重要。神经科学研究表明，人类对刺激的反应已知依赖于部分观察到的“内部状态”。不幸的是，当前的RLHF模型没有考虑到这一点。此外，大多数RLHF模型没有考虑到中间反馈，在实证研究中变得越来越重要，可以帮助提高样本复杂性和对齐性。为了解决这些局限性，我们将RLHF建模为部分观察到的奖励状态的强化学习（PORRL）。我们展示了从RLHF中两种主要形式的人类反馈 - 基数反馈和决斗反馈到PORRL的缩减。对于基数反馈，我们开发了通用的统计高效算法，并将它们实例化为POR-UCRL和POR-UCBVI。对于决斗反馈，我们表明，简单的基数反馈缩减不能达到亚线性的决斗回归。

    The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
    
[^22]: 想法的不确定性：不确定性感知规划增强大型语言模型的信息搜索能力

    Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models

    [https://arxiv.org/abs/2402.03271](https://arxiv.org/abs/2402.03271)

    通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。

    

    在面对不确定性时，寻求信息的能力至关重要。在许多实际应用中，比如医学诊断和故障排除，解决任务所需的信息不是初始给定的，而需要通过询问后续问题来主动寻求（例如，医生向患者询问症状的更多细节）。在这项工作中，我们引入了思想的不确定性（UoT），一种算法将大型语言模型的能力与主动提问信息的能力相结合。UoT结合了1）不确定性感知仿真方法，使模型能够模拟可能的未来场景，并估计其发生的可能性；2）基于不确定性的奖励机制，激励模型寻求信息；3）奖励传播方案，以最大化预期奖励的方式选择最佳的问题提问方式。在医学诊断、故障排除和'20的实验中。

    In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
    
[^23]: 从推理路径聚合的角度理解语言模型的推理能力

    Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

    [https://arxiv.org/abs/2402.03268](https://arxiv.org/abs/2402.03268)

    本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。

    

    预训练的语言模型能够在没有明确微调的情况下执行复杂的推理。为了理解预训练与下一个标记预测目标的关系如何促使推理能力的出现，我们提出可以将语言模型视为在预训练时通过聚合间接的推理路径来得出新结论。我们发现，这个视角在逻辑推理和数学推理等关键情况下非常有效。具体而言，我们将推理路径形式化为在知识/推理图上的随机游走路径。对学习的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型推理的合理方式。对多个知识图谱和数学问题数据集进行的实验和分析揭示了训练对随机游走路径的影响，并表明增加无标签的随机游走推理路径可以提高现实世界的多步推理能力。

    Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
    
[^24]: CLIP可以理解深度

    CLIP Can Understand Depth

    [https://arxiv.org/abs/2402.03251](https://arxiv.org/abs/2402.03251)

    本文研究了将CLIP用于单目深度估计的问题，通过联合训练反卷积解码器和可学习嵌入矩阵，使得CLIP能够理解深度，该方法在深度估计任务上取得了令人印象深刻的性能，并优于之前的方法。

    

    最近关于将CLIP推广到单目深度估计的研究表明，在网络爬取的数据上预训练的CLIP在图像块和与深度相关的提示之间得到适当相似性是低效的。在本文中，我们适应CLIP用于有意义的密集预测单目深度估计，而无需微调其原始的视觉-语言对齐。通过联合训练一个紧凑的反卷积解码器和一个名为mirror的小型可学习嵌入矩阵作为其文本编码器的静态提示，CLIP能够理解深度。通过这种方法，我们的模型在NYU Depth v2和KITTI数据集上展现出了令人印象深刻的性能，与几个先前的仅视觉模型相匹配，而且胜过了每个基于CLIP的深度估计模型。关于时间深度一致性和空间连续性的实验证明，我们提出的框架能够有效地优化CLIP的先验知识。此外，对于时滞研究进行了消融实验。

    Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on 
    
[^25]: HEANA: 一种具有灵活数据流的混合时幅模拟光学加速器，用于能量高效的CNN推理

    HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference

    [https://arxiv.org/abs/2402.03247](https://arxiv.org/abs/2402.03247)

    HEANA是一种混合时幅模拟光学加速器，通过使用混合时幅模拟光学乘法器(TAOMs)提高了对多种数据流的灵活性。它解决了现有光学加速器的波长并行性受到串扰、不支持多种数据流以及未充分利用光电探测器进行原位累积等问题。

    

    提出了一种名为HEANA的新型混合时幅模拟光学加速器，用于加速整数量化CNN的推理。HEANA采用混合时幅模拟光学乘法器(TAOMs)，增强了HEANA对多种数据流的支持灵活性。通过谱无损的TAOMs排列，有效解决了现有光学加速器存在的波长并行性受到各种串扰影响、无法支持多种数据流以及未充分利用光电探测器进行原位累积等问题。

    Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs s
    
[^26]: SGS-SLAM：基于高斯点云的语义稠密SLAM

    SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM

    [https://arxiv.org/abs/2402.03246](https://arxiv.org/abs/2402.03246)

    SGS-SLAM是一种基于三维高斯点云的语义稠密SLAM系统，通过多通道优化和关键帧优化，实现了高质量的重建和精确的语义分割。

    

    语义理解在稠密同时定位和建图（SLAM）中起着关键作用，有助于全面的场景解析。最近将高斯点云集成到SLAM系统中的进展表明，通过使用显式的三维高斯表示，可以生成高质量的渲染效果。基于这一进展，我们提出了SGS-SLAM，这是第一个基于三维高斯点云的语义稠密视觉SLAM系统，它不仅提供精确的三维语义分割，还实现了高保真度的重建。具体而言，我们提出在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。大量实验证明SGS-SLAM在相机位姿估计、地图重建和语义分割方面表现出了最先进的性能，优于现有方法同时保持实时渲染。

    Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rende
    
[^27]: IGUANe: 一种适用于脑MR图像多中心协调的三维通用CycleGAN模型

    IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images

    [https://arxiv.org/abs/2402.03227](https://arxiv.org/abs/2402.03227)

    IGUANe是一种三维通用CycleGAN模型，通过集成多个域的训练实现了脑MR图像的多中心协调，使其成为通用生成器。

    

    在MRI研究中，来自多个采集点的图像数据的聚合可以增加样本大小，但可能引入阻碍后续分析一致性的与采集点相关的变异。图像翻译的深度学习方法已经成为协调MR图像跨站点的解决方案。在本研究中，我们引入了IGUANe（具有统一对抗网络的图像生成），这是一种原始的三维模型，它结合了域转换的优势和直接应用样式转移方法来实现多中心脑MR图像协调。IGUANe通过多对一策略，集成了任意数量的域进行训练，扩展了CycleGAN架构。在推断过程中，该模型可以应用于任何图像，甚至来自未知采集点，使其成为协调的通用生成器。在由11台不同扫描仪的T1加权图像组成的数据集上进行训练，IGUANe在未见站点的数据上进行了评估。

    In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
    
[^28]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^29]: 有机或扩散：我们能区分人类艺术和AI生成的图像吗？

    Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?

    [https://arxiv.org/abs/2402.03214](https://arxiv.org/abs/2402.03214)

    这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。

    

    生成AI图像的出现完全颠覆了艺术界。从人类艺术中识别AI生成的图像是一个具有挑战性的问题，其影响随着时间的推移而不断增加。未能解决这个问题会导致不良行为者欺诈那些支付高价购买人类艺术品的个人和禁止使用AI图像的公司。这对于需要过滤训练数据以避免潜在模型崩溃的AI模型训练者来说也至关重要。区分人类艺术和AI图像的方法有多种，包括通过监督学习训练的分类器，针对扩散模型的研究工具，以及通过专业艺术家利用他们对艺术技巧的知识进行识别。在本文中，我们试图了解这些方法在现代生成模型的良性和对抗性环境中的表现如何。我们策划了7种风格的真实人类艺术，从5个生成模型生成了与之匹配的图像，并应用了8个检测器。

    The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
    
[^30]: 多智能体强化学习在多小区大规模MIMO系统中的能量节省

    Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems

    [https://arxiv.org/abs/2402.03204](https://arxiv.org/abs/2402.03204)

    该论文提出了一种多智能体强化学习算法，通过优化多个大规模MIMO基站的睡眠模式和天线切换，实现在多小区网络中能量的节省，同时保持服务质量。仿真结果表明，这种算法相比基线策略具有更好的性能。

    

    我们开发了一种多智能体强化学习算法，通过对多级高级睡眠模式（ASM）和基站的天线切换进行决策，来最小化多个大规模MIMO（多输入多输出）基站在多小区网络中的总能量消耗，同时保持整体的服务质量（QoS）。将问题建模为分布式部分可观察的马尔可夫决策过程（DEC-POMDP），以实现个体基站之间的协作，这对于处理小区间干扰是必要的。设计了一种多智能体近端策略优化（MAPPO）算法来学习协同基站控制策略。为了提高其可伸缩性，进一步提出了一种称为MAPPO-neighbor策略的修改版本。仿真结果表明，训练的MAPPO智能体相比基线策略取得了更好的性能。特别是，与自动睡眠模式1（符号级休眠）算法相比，MAPPO-neighbor算法实现了更好的性能。

    We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor
    
[^31]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^32]: 用顺序元学习预测多个环境下的配置性能

    Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning

    [https://arxiv.org/abs/2402.03183](https://arxiv.org/abs/2402.03183)

    本论文提出了一个顺序元学习框架SeMPL，可以在多个环境下学习和预测给定软件配置的性能。与现有方法不同的是，SeMPL通过依次顺序训练元环境，实现了更准确的对新环境的性能预测。

    

    学习和预测给定软件配置的性能对许多软件工程活动非常重要。当前的工作通常在单个环境下构建性能模型或未能正确处理来自不同环境的数据，从而限制了对新环境的准确性。本文针对多个环境下的配置性能学习，设计了SeMPL - 一种元学习框架，它从不同(meta)环境中测量的配置中学习共同的理解，并将它们推广到未知的目标环境中。其独特之处在于，与常见的元学习框架（如MAML和MetaSGD）并行训练元环境不同，我们依次顺序训练它们。训练顺序自然地允许区分

    Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating t
    
[^33]: C-RAG: 针对检索增强语言模型的认证生成风险

    C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models

    [https://arxiv.org/abs/2402.03181](https://arxiv.org/abs/2402.03181)

    C-RAG是第一个用于认证检索增强语言模型生成风险的框架，通过提供符合风险分析和生成风险的上界，确保生成结果的可信性。

    

    尽管大型语言模型（LLMs）在各种应用中具备令人印象深刻的能力，但它们仍然存在可信度问题，如幻觉和错位。检索增强语言模型（RAG）被提出来增强生成结果的可信性，通过引入外部知识。但是，对于RAG模型的生成风险的理论理解尚未被研究。本文回答了以下问题：1）RAG是否确实能够降低生成风险，2）如何对RAG和传统LLM的生成风险提供可证明的保证，以及3）哪些充分条件使得RAG模型能够降低生成风险。我们提出了C-RAG，第一个用于认证RAG模型生成风险的框架。具体而言，我们为RAG模型提供了符合风险分析，并确保了生成风险的上界，我们称之为符合生成风险。我们还对一般有界风险下的符合生成风险提供了理论保证。

    Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
    
[^34]: 银行业背景下主题建模方法的比较

    Comparison of Topic Modelling Approaches in the Banking Context

    [https://arxiv.org/abs/2402.03176](https://arxiv.org/abs/2402.03176)

    本研究在银行业背景下比较了主题建模方法。通过使用KernelPCA和K-means Clustering在BERTopic架构中，我们得到了连贯的主题，连贯性得分为0.8463。

    

    主题建模是许多应用程序中自动提取主题的重要任务，例如情感分析和推荐系统。该方法对于服务行业来说是至关重要的，可以用于监控客户讨论。传统方法如潜在狄利克雷分配（Latent Dirichlet Allocation，LDA）在主题发现方面表现出色，但由于数据稀疏性和无法对文档中的单词顺序建模，它们的结果并不一致。因此，本研究提出了在BERTopic架构中使用核主成分分析（Kernel Principal Component Analysis，KernelPCA）和K-means聚类的方法。我们使用尼日利亚银行客户的推文准备了一个新数据集，然后使用该数据集来比较主题建模方法。我们的研究结果显示，在BERTopic架构中使用KernelPCA和K-means可以生成连贯的主题，其中连贯性得分为0.8463。

    Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.
    
[^35]: The Matrix: 一个用于LLMs的贝叶斯学习模型

    The Matrix: A Bayesian learning model for LLMs

    [https://arxiv.org/abs/2402.03175](https://arxiv.org/abs/2402.03175)

    本文介绍了一个贝叶斯学习模型，用于理解大型语言模型（LLMs）的行为。研究探索了LLMs的优化指标，并开发了一个新的基于预测下一个标记的模型。实验结果表明，LLMs的行为与贝叶斯学习一致，为上下文学习提供了新的见解。

    

    本文介绍了一个用于理解大型语言模型（LLMs）行为的贝叶斯学习模型。我们探索了基于预测下一个标记的LLM的优化指标，并开发了一个以此原则为基础的新型模型。我们的方法涉及构建一个由先验和多项式转移概率矩阵表示的理想生成文本模型，并研究LLMs如何逼近该矩阵。我们讨论了嵌入和多项式分布之间的映射的连续性，并提出了Dirichlet逼近定理来逼近任何先验。此外，我们演示了LLMs的文本生成如何与贝叶斯学习原理一致，并深入探讨了其在上下文学习中的影响，具体解释了为什么在更大的模型中出现了上下文学习，其中提示被视为需要更新的样本。我们的发现表明，LLMs的行为与贝叶斯学习一致，提供了新的见解。

    In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights
    
[^36]: 多模态：文本和图像的多模态理解排行榜

    Multi: Multimodal Understanding Leaderboard with Text and Images

    [https://arxiv.org/abs/2402.03173](https://arxiv.org/abs/2402.03173)

    Multi是一个多模态理解的排行榜，提供了一个综合数据集，评估多模态大型语言模型对理解复杂图表和科学问题的能力。它兼具准确和开放式的回答形式，挑战MLLM的各种任务，并包含超过18,000个问题。

    

    多模态大型语言模型（MLLM）的快速进展强调了向学术界引入具有挑战性而又真实的基准的需求。现有的基准主要关注简单的自然图像理解，但Multi成为了MLLM的尖端基准，提供了一个综合性的数据集，用于评估MLLM对理解复杂图表和科学问题的能力。该基准反映了当前真实的考试风格，提供多模态的输入，并要求准确或开放式的回答，类似于现实中的学校考试。它通过各种任务挑战MLLM，从公式推导到图像细节分析，以及跨模态推理。Multi包括超过18,000个问题，重点关注不同格式的基于科学的问答。我们还引入了Multi-Elite，一个包含500个问题的子集，用于测试MLLM的极端情况，以及Multi-Extend，通过超过4..。

    Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
    
[^37]: 通过对多样标签嵌入进行注意力的精确和良好校准的ICD代码分配

    Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings

    [https://arxiv.org/abs/2402.03172](https://arxiv.org/abs/2402.03172)

    本论文提出了一种新颖的方法来实现自动化ICD编码，通过在文本中使用注意力机制的方法来精确分配ICD代码。实验结果表明，该方法在ICD编码中优于当前最先进的模型，并且标签嵌入机制对模型性能也有显著贡献。

    

    尽管国际疾病分类（ICD）已在全球范围内得到采用，但将ICD代码手动分配给临床文本耗时、容易出错且昂贵，这促使了自动化方法的发展。本文描述了一种用于自动ICD编码的新方法，结合了先前相关工作的几个思想。我们特别使用了一个强大的基于Transformer的模型作为文本编码器，并且为处理冗长的临床叙述，我们探索了将基础编码器模型改造成为Longformer，或者将文本分成多个块并独立处理每个块的方法。编码器产生的表示与一个标签嵌入机制相结合，该机制探索了多样的ICD代码近义词。对MIMIC-III数据集的不同划分进行的实验表明，所提出的方法在ICD编码中优于当前最先进的模型，标签嵌入对良好性能的贡献也是显著的。我们的方法还...

    Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also 
    
[^38]: 有限域情境演算理论中有关时间的可判定推理

    Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories

    [https://arxiv.org/abs/2402.03164](https://arxiv.org/abs/2402.03164)

    本论文研究了有限域情境演算理论中关于时间的可判定推理。通过引入具有受限状态公理和比较操作符的钟表作为实值fluents，解决了检查可达情境是否满足给定公式的问题的不可判定性，并给出了一个可判定的过程来确定行动序列。

    

    时间的表达对于智能物理系统至关重要，并在情境演算中得到了广泛研究。最常用的方法是通过添加一个实值fluents $\mathit{time}(a)$ 来表示时间，这个fluents将一个时间点与每个动作以及每个情境相连。我们证明了在这种方法中，即使领域范围被限制在一个有限对象集上，检查是否存在一个可达情境满足给定公式的问题是不可判定的。我们提出了一种基于定时自动机理论的替代方法，通过引入钟表作为具有受限的继任状态公理和比较操作符的实值fluents。通过这种限制，我们可以证明有限域基本动作理论的可达性问题是可判定的。最后，我们将我们的结果应用于Golog程序实现，通过提供一种可判定的过程来确定一个行动序列

    Representing time is crucial for cyber-physical systems and has been studied extensively in the Situation Calculus. The most commonly used approach represents time by adding a real-valued fluent $\mathit{time}(a)$ that attaches a time point to each action and consequently to each situation. We show that in this approach, checking whether there is a reachable situation that satisfies a given formula is undecidable, even if the domain of discourse is restricted to a finite set of objects. We present an alternative approach based on well-established results from timed automata theory by introducing clocks as real-valued fluents with restricted successor state axioms and comparison operators. %that only allow comparisons against fixed rationals. With this restriction, we can show that the reachability problem for finite-domain basic action theories is decidable. Finally, we apply our results on Golog program realization by presenting a decidable procedure for determining an action sequence
    
[^39]: 使用辅助短时延任务提升长时延强化学习

    Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task

    [https://arxiv.org/abs/2402.03141](https://arxiv.org/abs/2402.03141)

    这篇论文提出了一种名为Auxiliary-Delayed Reinforcement Learning (AD-RL)的方法，通过利用辅助的短时延任务来加速长时延任务的学习过程，同时在随机环境中保持性能。该方法能显著降低样本复杂度，并在确定性和随机基准测试中表现出优异的样本效率和性能。

    

    延迟情景下的强化学习是具有挑战性的，延迟情景是指观察和交互存在延迟的常见实际情况。现有技术中，状态增强技术在延迟步骤中可能会出现状态空间扩大或在随机环境中性能下降的问题。为了解决这些挑战，我们提出了一种新颖的Auxiliary-Delayed Reinforcement Learning（AD-RL），利用一个辅助的短时延任务来加速长时延任务的学习，同时不损害在随机环境中的性能。具体来说，AD-RL在短时延任务中学习值函数，然后将其与长时延任务中的自举和策略改进技术结合起来。我们理论上证明，与直接在原始长时延任务上学习相比，这样做可以大大减小样本复杂度。在确定性和随机基准测试中，我们的方法在样本效率和性能方面明显优于现有技术。

    Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficienc
    
[^40]: Just Cluster It: 一种使用聚类和预训练表示进行高维空间探索的方法

    Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations

    [https://arxiv.org/abs/2402.03138](https://arxiv.org/abs/2402.03138)

    本文提出了一种基于聚类和预训练表示的方法，用于在高维空间中进行探索。通过对随机和预训练表示进行聚类，可以有效计算状态数，特别是在多维环境中预训练表示更加有效。

    

    本文从表征为中心的角度探讨了强化学习中探索问题，将探索视为密度估计问题。我们研究了在三维环境中使用聚类来进行探索的有效性，基于一个观察：在三维环境中，与二维环境相比，状态转换中的像素变化的重要性不那么明显。我们提出了一种方法，通过对随机表示和预训练DINO表示进行周期性和全局聚类来计算状态数，即估计伪计数。令人惊讶的是，即使是随机特征也可以在三维环境中有效地进行聚类以计算状态数，然而当这些特征变得更加复杂时，预训练DINO表示由于其预训练的归纳偏差在表示中更加有效。总体而言，这为集成预训练表示提供了一条路径。

    In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-t
    
[^41]: 解决合作与竞争的同时博弈的零射击互动

    Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games

    [https://arxiv.org/abs/2402.03136](https://arxiv.org/abs/2402.03136)

    我们提出了一种名为Albatross的算法，它使用模拟自我对弈学习合作和竞争的同时博弈中的有界理性代理行为，并成功实现了与任何游戏实力的代理的合作和竞争。

    

    在顺序游戏中，自我对弈和规划的组合取得了巨大的成功，例如国际象棋和围棋。然而，将AlphaZero等算法调整到同时博弈中带来了新的挑战。在这些游戏中，对其他代理的同时行动缺乏信息是一个限制因素，因为它们可能选择不同的纳什均衡或者根本不最优地进行游戏。因此，在与其他代理博弈时，建模其行为是至关重要的。为此，我们提出了Albatross：使用模拟自我对弈来学习有界理性代理和基于温度的反应优化的AlphaZero。Albatross学习了一种新的均衡概念：平滑最佳反应极端均衡(SBRLE)，从而实现了与任何游戏实力的代理的合作和竞争。我们在一系列合作和竞争的同时完全信息游戏上对Albatross进行了广泛的评估。与AlphaZero相比，Albatross能够同时博弈，并且在不同游戏场景下表现出色。

    The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go. However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge. In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all. Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games. To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play. Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength. We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games. In contrast to AlphaZero, Albatr
    
[^42]: ChatGPT生成R程序代码的用户中心评估

    User-Centric Evaluation of ChatGPT Capability of Generating R Program Code

    [https://arxiv.org/abs/2402.03130](https://arxiv.org/abs/2402.03130)

    本文评估了ChatGPT从自然语言输入生成R编程语言代码的能力，通过多次尝试的过程，并对生成的解决方案进行了精确性、完整性、简洁性等多个质量属性的评估。

    

    本文报告了对ChatGPT从自然语言输入生成R编程语言代码能力的评估。构建了一个专门用于生成R程序代码的数据集，以支持基于场景的测试和评估在不同难度级别和不同类型程序的各种使用情境中的代码生成能力。评估采用多次尝试的过程，测试人员通过多次尝试来完成代码生成任务，直至获得满意的解决方案或达到最大尝试次数后放弃。每次尝试中，测试人员根据先前的结果和待完成的任务制定自然语言输入给ChatGPT。除了平均尝试次数和平均完成任务所需的时间指标外，最终生成的解决方案将根据精确性、完整性、简洁性等多个质量属性进行评估。

    This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input. A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, concise
    
[^43]: 好的教师解释: 解释增强的知识蒸馏

    Good Teachers Explain: Explanation-Enhanced Knowledge Distillation

    [https://arxiv.org/abs/2402.03119](https://arxiv.org/abs/2402.03119)

    通过优化解释增强的知识蒸馏（e$^2$KD）算法，可以让学生模型在准确性和学生-教师一致性方面都得到大幅度提升，确保学生模型从教师那里正确学到原因。

    

    知识蒸馏已被证明可以将大型教师模型压缩成较小的学生模型。虽然已经知道学生模型可以达到与教师相似的准确性，但也已经发现学生模型通常不会学到相同的函数。然而，学生模型和教师模型之间共享相似属性，如基于相同的输入特征进行预测，通常是非常有价值的，因为这确保学生从教师那里学到了“正确的特征”。在这项工作中，我们探索了是否可以通过优化经典的知识蒸馏损失以及教师和学生所生成的解释的相似性来实现这一点。尽管这个想法简单且直观，但我们发现我们提出的“解释增强的知识蒸馏”（e$^2$KD）（1）在准确性和学生-教师一致性方面始终提供了大幅度的增益，（2）确保学生从教师那里学到了正确的原因。

    Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and
    
[^44]: 利用具有结构关注机制的机器学习方法预测偶氮基团的红外光谱

    Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism

    [https://arxiv.org/abs/2402.03112](https://arxiv.org/abs/2402.03112)

    本文提出了一种利用结构关注机制的机器学习方法，可以预测和解释偶氮化合物的红外光谱。该方法通过集中关注与功能团邻近的化学信息，显著提升了光谱预测的准确性、鲁棒性和可解释性，为揭示红外光谱特征与分子结构之间的相关性提供了一种可扩展和高效的途径。

    

    红外（IR）光谱是化学研究中的重要技术，通过振动和转动跃迁阐明分子结构和动力学。然而，独特的振动和转动模式所表征的复杂分子特征给分析带来了挑战。本文提出了一种采用结构关注机制的机器学习方法，针对偶氮化合物，以提升红外光谱预测和解释能力。我们的模型通过集中关注邻近功能团附近的化学信息，显著提升了光谱预测的准确性、鲁棒性和可解释性。这种方法不仅揭示了红外光谱特征与分子结构之间的相关性，还提供了一种可扩展和高效的途径来解析复杂的分子相互作用。

    Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions. However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges. Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds. Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions. This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions.
    
[^45]: 非平稳潜在自回归赌博机

    Non-Stationary Latent Auto-Regressive Bandits

    [https://arxiv.org/abs/2402.03110](https://arxiv.org/abs/2402.03110)

    本文提出了非平稳潜在自回归赌博机问题，并提出了一个算法，在这种环境下可以达到较低的遗憾率。

    

    本文考虑具有非平稳奖励的随机多臂赌博机问题。我们提出了一个新颖的非平稳环境的公式，其中臂的平均奖励随时间变化是由一些未知的潜在自回归(AR)状态的顺序k决定的。我们将这个新的环境称为潜在AR赌博机。潜在AR赌博机的不同形式在许多现实世界的场景中都出现，特别是在行为健康或教育等新兴科学领域中，这里缺乏对环境的机制建模。如果AR顺序k已知，我们提出了一个算法，在这种情况下，算法表现出O(k√T)的遗憾率。实证结果显示，即使k被错误地估计，我们的算法在多个非平稳环境中也胜过标准的UCB算法。

    We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
    
[^46]: 基于意图的提示校准：用合成边界情况增强提示优化

    Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases

    [https://arxiv.org/abs/2402.03099](https://arxiv.org/abs/2402.03099)

    该论文介绍了一种基于意图的提示校准方法，通过迭代优化和生成合成边界情况数据来改进提示工程，以提高大型语言模型的性能。

    

    由于大型语言模型（LLMs）对给定提示的高度敏感性和文本任务指令的固有歧义，提示工程是一项具有挑战性和重要性的任务。通过采用一个包含上次试验结果的元提示并提出改进的提示，最近的研究表明LLMs自动进行提示工程的能力。然而，这需要一个高质量的基准来比较不同的提示，在许多实际应用场景中获取高质量的基准是困难且昂贵的。在这项工作中，我们引入了一种新的自动提示工程方法，使用校准过程来迭代地优化与用户意图相符的提示。在优化过程中，系统联合生成边界用例的合成数据，并根据生成的数据集进行提示优化。我们证明了我们方法的有效性。

    Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our
    
[^47]: 基于用户偏好的语言引导的抽象化

    Preference-Conditioned Language-Guided Abstraction

    [https://arxiv.org/abs/2402.03081](https://arxiv.org/abs/2402.03081)

    本研究提出了一种基于用户偏好的语言引导的抽象化方法，通过观察人类行为的变化并使用语言模型查询用户的偏好，构建适用于不同用户的状态抽象。

    

    从示范中学习是人们教导机器人的常用方式，但它容易出现误导性特征相关性。最近的研究使用语言构建状态抽象，即包含任务相关特征的视觉表示，以进行更通用的学习。然而，这些抽象也取决于用户在任务中关注的偏好，这可能很难用语言描述或仅仅通过语言详尽说明。我们如何构建抽象来捕捉这些潜在偏好呢？我们观察到人类行为反映了他们的世界观。我们的关键观点是，人类行为的变化告诉我们人类对世界的偏好存在差异，即他们的状态抽象也不同。在这项工作中，我们提出使用语言模型（LM）直接查询这些偏好，给定已发生行为变化的知识。在我们的框架中，我们使用LM有两种方式：首先，根据文本描述查询用户的偏好。

    Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of 
    
[^48]: 学习使用元强化学习进行抽象视觉运动映射

    Learning to Abstract Visuomotor Mappings using Meta-Reinforcement Learning

    [https://arxiv.org/abs/2402.03072](https://arxiv.org/abs/2402.03072)

    本研究通过使用元强化学习来探索人类获取多个全新视觉运动映射的能力，并发现情境线索在学习中起到了重要的作用，提供了计算上的优势。

    

    我们研究了人类获得全新技能的多个视觉运动映射的能力。使用格子导航范式，我们测试了不同"格子世界"实施的情境线索是否能够使参与者更有效地学习两个不同的关键映射。我们的结果表明，当提供情境信息时，任务表现显著更好。对于元强化学习智能体来说也是如此，它们在执行任务时是否接收情境信息有所不同。我们评估了它们在预测任务中人类表现的准确性，并分析了它们的内部表示。结果表明，情境线索允许在使用不同的视觉运动映射时形成独立的空间和时间表示，而它们的缺失则更倾向于共享一个表示。虽然这两种策略都可以学习多个视觉运动映射，但我们证明了情境线索提供了计算上的优势。

    We investigated the human capacity to acquire multiple visuomotor mappings for de novo skills. Using a grid navigation paradigm, we tested whether contextual cues implemented as different "grid worlds", allow participants to learn two distinct key-mappings more efficiently. Our results indicate that when contextual information is provided, task performance is significantly better. The same held true for meta-reinforcement learning agents that differed in whether or not they receive contextual information when performing the task. We evaluated their accuracy in predicting human performance in the task and analyzed their internal representations. The results indicate that contextual cues allow the formation of separate representations in space and time when using different visuomotor mappings, whereas the absence of them favors sharing one representation. While both strategies can allow learning of multiple visuomotor mappings, we showed contextual cues provide a computational advantage 
    
[^49]: 多语言Transformer和BERTopic用于短文本主题建模：塞尔维亚语案例

    Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian

    [https://arxiv.org/abs/2402.03067](https://arxiv.org/abs/2402.03067)

    本文介绍了BERTopic在塞尔维亚语短文本中的首次应用，结果显示BERTopic可以产生丰富的主题，即使是部分预处理的文本。与传统方法相比，BERTopic在主题数量不受限制时提供了更多信息和新的见解。

    

    本文介绍了BERTopic这种最先进的主题建模技术在具有丰富形态语言的短文本中的首次应用结果。我们使用三个多语言嵌入模型以及两种文本预处理水平（部分和完全）对塞尔维亚语的部分预处理的短文本进行了BERTopic的性能评估。我们还将其与LDA和NMF在完全预处理文本上进行了比较。实验使用了一个表达对COVID-19疫苗疑虑的推文数据集。我们的结果表明，通过适当的参数设置，BERTopic即使应用于部分预处理的短文本，也能产生信息丰富的主题。当在两种预处理场景下应用相同的参数时，部分预处理文本的性能下降很小。与LDA和NMF相比，从关键词来看，BERTopic提供了更丰富的主题，并在主题数量不受限制时提供了新的见解。

    This paper presents the results of the first application of BERTopic, a state-of-the-art topic modeling technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative topics even when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative topics and gives novel insights when the number of topics is not limited. The findings of this p
    
[^50]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^51]: 交互式视频：以用户为中心的可控视频生成与多模态指令协同

    InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions

    [https://arxiv.org/abs/2402.03040](https://arxiv.org/abs/2402.03040)

    “InteractiveVideo”是一个以用户为中心的视频生成框架，通过交互式的多模态指令，用户可以在整个生成过程中精确、有效地指导生成模型，并对视频的关键方面进行灵活调整。

    

    我们引入了一种名为“交互式视频”的用户中心视频生成框架。与传统的基于用户提供图像或文本的生成方法不同，我们的框架设计用于动态交互，允许用户通过各种直观的机制在整个生成过程中指导生成模型，例如文本和图像提示、绘画、拖放等。我们提出了一种协同多模态指令机制，旨在将用户的多模态指令无缝集成到生成模型中，从而实现用户输入和生成过程之间的合作和响应式交互。这种方法通过精确而有效的用户指令，实现了对生成结果的迭代和精细化调整。通过“交互式视频”，用户可以灵活地精心调整视频的关键方面，包括绘画参考图像、编辑语义和调整视频动作等。

    We introduce $\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions 
    
[^52]: 自动组合样本选择策略用于少样本学习

    Automatic Combination of Sample Selection Strategies for Few-Shot Learning

    [https://arxiv.org/abs/2402.03038](https://arxiv.org/abs/2402.03038)

    本文研究了20种样本选择策略对少样本学习性能的影响，并提出了一种自动组合样本选择策略的方法（ACSESS），在多个数据集上证明了其优越性能。

    

    在少样本学习中，如元学习、少样本微调或上下文学习中，用于训练模型的有限样本数量对整体成功具有显著影响。尽管存在大量的样本选择策略，但它们对少样本学习性能的影响尚不十分明确，因为大部分只被在典型的监督设置中进行了评估。本文通过对8个图像和6个文本数据集上的5种少样本学习方法，彻底研究了20种样本选择策略对性能的影响。此外，我们提出了一种新的自动组合样本选择策略的方法（ACSESS），它充分利用了个体策略的优势和互补信息。实验结果表明，我们的方法始终优于个体选择策略，以及最近提出的上下文学习支持样本选择方法。

    In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str
    
[^53]: 向绿色且类人的人工智能迈进：当代少样本学习方法的全面调查

    Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches

    [https://arxiv.org/abs/2402.03017](https://arxiv.org/abs/2402.03017)

    本文全面调查了少样本学习领域的最新进展，探讨了该方法在解决深度学习在实际应用中的限制方面的潜力和挑战。

    

    尽管深度学习取得了广泛的成功，但其对数据的需求和计算的昂贵性使其在许多数据受限的真实应用中不实用。少样本学习（FSL）旨在通过实现对新学习任务的快速适应来解决这些限制，并在近年来取得了显著发展。本调查提供了该领域最新进展的全面概述。首先，正式定义了FSL，并介绍了它与不同学习领域的关系。引入了一种新的分类法，扩展了以前提出的方法，并对经典和新领域中的实际应用进行了描述。最后，讨论了塑造该领域的最新趋势、突出挑战和有前途的未来研究方向。

    Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.
    
[^54]: 信任谁？分布式高斯过程回归的选举学习

    Whom to Trust? Elective Learning for Distributed Gaussian Process Regression

    [https://arxiv.org/abs/2402.03014](https://arxiv.org/abs/2402.03014)

    本文介绍了一种使用高斯过程回归增强分布式协作学习的创新方法，并开发了一种选举学习算法，使智能体能够根据邻居的可信度有选择性地请求预测。该方法提高了个体预测的准确性，同时消除了计算密集型方差计算的需求，并确保了预测的可靠性。

    

    本文介绍了一种创新的方法，通过在多智能体系统中使用高斯过程（GP）回归来增强分布式协作学习。这项工作的主要贡献是开发了一种选举学习算法，即基于先验的选举式分布式GP（Pri-GP），它赋予了智能体基于信任度有选择地向邻居智能体请求预测的能力。所提出的Pri-GP有效提高了个体预测的准确性，特别是在智能体先验知识错误的情况下。此外，它消除了在分布式GP中确定聚合权重的计算密集型方差计算的需求。此外，我们在Pri-GP框架中建立了一个预测误差边界，确保了预测的可靠性，这被认为是在安全关键的多智能体系统应用中的一个重要性质。

    This paper introduces an innovative approach to enhance distributed cooperative learning using Gaussian process (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.
    
[^55]: UniMem：迈向长上下文大语言模型统一视图

    UniMem: Towards a Unified View of Long-Context Large Language Models

    [https://arxiv.org/abs/2402.03009](https://arxiv.org/abs/2402.03009)

    本文引入UniMem，一个统一的框架，以记忆增强的角度重新制定了现有的长上下文方法，并提出了UniMix来提高长上下文处理能力。

    

    长上下文处理是限制大语言模型应用能力的关键能力。虽然存在各种致力于增强大语言模型的长上下文处理能力的方法，但它们是孤立地开发的，缺乏对它们的优点的系统分析和整合，从而阻碍了进一步的发展。在本文中，我们引入了UniMem，一个统一的框架，从LLM的记忆增强的角度重新制定了现有的长上下文方法。 UniMem的特点是四个关键维度：内存管理，内存写入，内存读取和内存注入，为了理解各种长上下文方法提供了系统理论。我们基于UniMem重新制定了16种现有方法，并分析了Transformer-XL，记忆化Transformer，RMT和Longformer中的四种代表性方法，以揭示它们的设计原则和优势。基于这些分析，我们提出了UniMix，一种新的方法来提高长上下文处理能力。

    Long-context processing is a critical ability that constrains the applicability of large language models. Although there exist various methods devoted to enhancing the long-context processing ability of large language models (LLMs), they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of LLMs. UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an inn
    
[^56]: 论文标题：解码时间对齐的语言模型

    Decoding-time Realignment of Language Models

    [https://arxiv.org/abs/2402.02992](https://arxiv.org/abs/2402.02992)

    本研究提出了解码时间对齐（DeRa）方法，可以在不重新训练模型的情况下探索和评估不同的规则化强度，从而对齐语言模型和人类偏好。

    

    将语言模型与人类偏好对齐对于减少模型中的错误和偏差非常重要。对齐技术，如从人类反馈中进行的强化学习（RLHF），通常被视为在人类偏好奖励和鼓励保持与未对齐模型接近的接近性规则项之间进行优化的权衡。选择适当的规则化水平至关重要：规则化不足可能导致由于奖励欺骗而降低模型能力，而过度规则化则阻碍对齐。传统方法找到最佳规则化水平需要使用不同规则化强度重新训练多个模型。然而，这个过程耗费资源，特别是对于大型模型来说。为了解决这个挑战，我们提出了解码时间对齐（DeRa），一种简单的方法，在无需重新训练的情况下探索和评估不同的规则化强度。DeRa可以对对齐模型的程度进行控制。

    Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of al
    
[^57]: 对于在OWL 2 QL上进行元推理的Datalog工具的评估

    Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL

    [https://arxiv.org/abs/2402.02978](https://arxiv.org/abs/2402.02978)

    本文评估了针对OWL 2 QL上元推理的Datalog工具。许多应用都需要元模型来简化本体的扩展和重用。然而，元模型的无限制使用带来了不可判定性问题。本文讨论了在OWL 2 QL上的元模型语义和元模型语义蕴含机制，并对几种扩展进行了评估。

    

    元模型是一种用于表达本体论中类和属性知识的通用方法。它是多个应用中的一种理想建模特性，可以简化本体的扩展和重用。然而，无限制地允许元模型存在会出现多个问题，主要是由于不可判定性问题。因此，实际语言禁止类作为其他类的实例出现，或将此类出现视为语义上不同的对象。具体而言，SPARQL中的元查询在直接语义蕴涵机制（DSER）下使用后一种方法，因此不支持元查询。然而，在过去十年中，已经提出了几种能够实现不同元模型特性的扩展。本文讨论了Lenzerini等人（2015）和Lenzerini等人（2020）；Cima等人（2017）中提出的在OWL 2 QL上的元模型语义（MS）和元模型语义蕴含机制（MSER）。由O

    Metamodeling is a general approach to expressing knowledge about classes and properties in an ontology. It is a desirable modeling feature in multiple applications that simplifies the extension and reuse of ontologies. Nevertheless, allowing metamodeling without restrictions is problematic for several reasons, mainly due to undecidability issues. Practical languages, therefore, forbid classes to occur as instances of other classes or treat such occurrences as semantically different objects. Specifically, meta-querying in SPARQL under the Direct Semantic Entailment Regime (DSER) uses the latter approach, thereby effectively not supporting meta-queries. However, several extensions enabling different metamodeling features have been proposed over the last decade. This paper deals with the Metamodeling Semantics (MS) over OWL 2 QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in Lenzerini et al. (2015) and Lenzerini et al. (2020); Cima et al. (2017). A reduction from O
    
[^58]: 变分流模型：以你的风格流动

    Variational Flow Models: Flowing in Your Style

    [https://arxiv.org/abs/2402.02977](https://arxiv.org/abs/2402.02977)

    我们引入了一种变分流模型的方法，并提出了一种系统的无需训练的转换方法，使得快速采样成为可能，同时保持了采样的准确性和效率。

    

    我们引入了一种对"后验流"模型进行变分推理解释的方法——用以将"概率流"推广到更广泛的随机过程类别，不必局限于扩散过程。我们将这种结果称为"变分流模型"。此外，我们提出了一种无需训练的系统方法，将由方程Xt = at * X0 + st * X1所描述的"线性"随机过程的后验流转化为直线恒速(SC)流，类似于矫正流。这种转化使得可以快速沿着原始的后验流进行采样，而无需训练一个新的SC流模型。我们的方法的灵活性使我们能够将转换扩展到两个不同"线性"随机过程的后验流之间进行互相转化。此外，我们还可以将高阶数值解法轻松集成到转换后的SC流中，进一步提高采样的准确性和效率。我们进行了严格的理论分析和大量实验结果的验证。

    We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
    
[^59]: 使用增量评估挖掘最小的行为模式集合的方法

    Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation

    [https://arxiv.org/abs/2402.02921](https://arxiv.org/abs/2402.02921)

    本论文介绍了一种使用增量评估挖掘最小的行为模式集合的方法，以解决现有方法的可扩展性和实际应用中冗余模式的问题。

    

    过程挖掘提供了分析信息系统在执行过程中生成的事件日志的方法。它支持从医疗保健、制造业到电子商务等领域的过程设计、验证和执行。为了探索具有大量行为可变性的灵活流程的规律性，建议挖掘共同描述底层流程的重复行为模式。然而，现有的行为模式挖掘方法存在两个限制。首先，它们在生成模式候选人时只采用了增量计算，但在评估其质量时却没有使用增量计算，因此可扩展性有限。其次，基于挖掘模式的过程分析在实际应用场景中获得了大量冗余模式，导致效果有限。在本文中，我们解决了这些限制，以便更好地分析复杂过程。

    Process mining provides methods to analyse event logs generated by information systems during the execution of processes. It thereby supports the design, validation, and execution of processes in domains ranging from healthcare, through manufacturing, to e-commerce. To explore the regularities of flexible processes that show a large behavioral variability, it was suggested to mine recurrent behavioral patterns that jointly describe the underlying process. Existing approaches to behavioral pattern mining, however, suffer from two limitations. First, they show limited scalability as incremental computation is incorporated only in the generation of pattern candidates, but not in the evaluation of their quality. Second, process analysis based on mined patterns shows limited effectiveness due to an overwhelmingly large number of patterns obtained in practical application scenarios, many of which are redundant. In this paper, we address these limitations to facilitate the analysis of complex
    
[^60]: DS-MS-TCN: 使用双尺度多阶段时间卷积网络的Otago体操识别

    DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network

    [https://arxiv.org/abs/2402.02910](https://arxiv.org/abs/2402.02910)

    本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。

    

    Otago运动计划是针对老年人的重要康复举措，旨在增强平衡和力量。本研究利用单个腰部佩戴的惯性测量单元(IMU)，在老年人的日常生活中识别Otago体操动作，以解决现有研究在准确性和稳定性方面的限制。研究在实验室设置中招募了36名老年人，并对额外招募的7名老年人进行了家庭评估。研究提出了一种双尺度多阶段时间卷积网络(DS-MS-TCN)，用于两级序列到序列分类，将其纳入一个损失函数。在第一阶段，模型专注于识别每个体操动作的重复次数(微标签)。随后的阶段扩展了识别范围，包括完整的运动序列。

    The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
    
[^61]: 在基于强化学习控制的人肘数字孪生中复制阻抗识别实验

    Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows

    [https://arxiv.org/abs/2402.02904](https://arxiv.org/abs/2402.02904)

    本研究利用强化学习控制的数字孪生模型成功复制了人体肘部阻抗识别实验，并发现强化学习智能体在稳定肘关节运动方面表现出比人体更高的阻抗，为虚拟环境模拟在神经机械研究中的潜力提供了初步证据。

    

    本研究通过使用一种经过强化学习增强的全新人体运动仿真平台MyoSuite，在虚拟环境中复制了人体神经机械实验的先驱性工作。通过在一个肌肉骨骼模型上复制多种类型的人体肘部阻抗识别实验，将由强化学习智能体控制的肘关节运动与实际人体肘关节在扭矩干扰实验中识别到的阻抗进行比较。研究发现，强化学习智能体在稳定目标肘关节运动方面表现出比人体更高的肘关节阻抗，可能是由于其更短的反应时间和更好的感知能力。本研究首次探索了虚拟环境模拟在神经机械研究中的潜力，为传统实验方法提供了一种初步但有前景的替代方案。

    This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion simulation platform enhanced by Reinforcement Learning (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment simulations for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-contro
    
[^62]: LLM智能体的相互作用：测量个性一致性和语言对齐在大规模语言模型相互作用中的贡献

    LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models

    [https://arxiv.org/abs/2402.02896](https://arxiv.org/abs/2402.02896)

    本研究通过对大规模语言模型进行个性化配置，并探究了智能体之间的对话交互对个性一致性和语言对齐的影响。研究结果表明不同配置对话者展示了不同程度的个性一致性和语言对齐。这些发现为进一步理解LLMs之间基于对话的相互作用提供了基础，并强调了打造更具鲁棒性和类人化LLM智能体的新方法的需求。

    

    尽管智能体相互作用和个性化在大规模语言模型（LLMs）研究中都是热门话题，但对语言相互作用对于个性化的LLM智能体行为的影响的关注有限。这样的努力对于确保智能体在保持其指定特征的同时能够进行开放的自然对话非常重要。在我们的实验中，我们通过提示将GPT-3.5调节到个性化配置，并使用简单的变异性引导抽样算法创建了一个双组群的LLM智能体人口。然后我们进行个性化测试，并将智能体提交到协同写作任务，发现不同配置表现出不同程度的个性一致性和语言对齐。我们的研究旨在为更好地理解LLMs之间基于对话的相互作用奠定基础，并凸显了需要新的方法来打造更强大、更类人的LLM智能体的需求。

    While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM pers
    
[^63]: 分散式人工智能的构建模块综述

    A Review on Building Blocks of Decentralized Artificial Intelligence

    [https://arxiv.org/abs/2402.02885](https://arxiv.org/abs/2402.02885)

    本文综述了分散式人工智能（DEAI）领域的构建模块，从底层开始分析，探讨了DEAI解决方案和网络，并提出了未来的研究方向和开放性问题。

    

    人工智能正在改变我们的生活，技术进步和从学术和理论领域向现实世界的转移年复一年加速。但在这个进程和转变中，一些开放性问题和疑问需要得到解决，以实现该领域的道德发展，例如数字隐私、所有权和控制。这些是目前最流行的人工智能方法——集中式人工智能（CEAI）存在问题的原因之一，其他方向也被广泛探索，如分散式人工智能（DEAI），以解决一些最广泛的问题。本文对DEAI领域的现有工作进行了系统文献综述，总结了71个研究的发现。本文的主要重点是识别DEAI解决方案和网络的构建模块，采用自下而上的方法进行DEAI分析。最后，提出未来的研究方向和开放性问题。

    Artificial intelligence is transforming our lives, and technological progress and transfer from the academic and theoretical sphere to the real world are accelerating yearly. But during that progress and transition, several open problems and questions need to be addressed for the field to develop ethically, such as digital privacy, ownership, and control. These are some of the reasons why the currently most popular approaches of artificial intelligence, i.e., centralized AI (CEAI), are questionable, with other directions also being widely explored, such as decentralized artificial intelligence (DEAI), to solve some of the most reaching problems. This paper provides a systematic literature review (SLR) of existing work in the field of DEAI, presenting the findings of 71 identified studies. The paper's primary focus is identifying the building blocks of DEAI solutions and networks, tackling the DEAI analysis from a bottom-up approach. In the end, future directions of research and open pr
    
[^64]: 对开放领域科学主张验证的知识来源的比较

    Comparing Knowledge Sources for Open-Domain Scientific Claim Verification

    [https://arxiv.org/abs/2402.02844](https://arxiv.org/abs/2402.02844)

    在本论文中，我们对开放领域科学主张验证系统的性能进行了测试，通过比较不同的知识来源（PubMed、维基百科、谷歌）和信息检索方法，我们发现在真实世界环境中进行科学主张验证的挑战和问题。

    

    科学知识的发现速度和在线健康主张的分享增加，凸显了为科学主张开发高效事实检核系统的重要性。现有文献中，这项任务的常见设置假设已经提供并注释或包含在有限语料库中的包含证据的文档。这使得该系统在可能需要查询数百万个文档以找到相关证据的现实世界环境中变得不切实际。在本文中，我们进行了一系列实验，以测试开放领域主张验证系统的性能。我们在不同设置下测试了四个数据集中的生物医学和健康主张系统的最终判断预测。在保持流水线的证据选择和判断预测部分不变的情况下，使用三种常见知识来源（PubMed、维基百科、谷歌）和两种不同的信息检索方法进行文档检索。

    The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati
    
[^65]: SynthVision - 使用合成图像数据在计算机视觉模型中最大化产出的最小输入

    SynthVision - Harnessing Minimal Input for Maximal Output in Computer Vision Models using Synthetic Image data

    [https://arxiv.org/abs/2402.02826](https://arxiv.org/abs/2402.02826)

    本研究提出了一种新方法，通过仅使用合成数据来构建一个全面的计算机视觉模型，用于检测人类乳头瘤病毒生殖器疣。使用扩散模型快速生成高质量的训练数据，并评估其对视觉模型的影响。

    

    在应对紧急医疗危机（如流行病或生物恐怖主义事件）时，快速开发疾病检测的计算机视觉模型非常重要。然而，传统的数据收集方法对于这些场景来说太慢了，需要创新方法从很少的数据中快速生成可靠的模型。我们通过仅使用合成数据构建了一个全面的计算机视觉模型来检测人类乳头瘤病毒生殖器疣，展示了我们的新方法。在我们的研究中，我们采用了一个两阶段的实验设计，使用扩散模型。在第一阶段，扩散模型被用于从10张HPV引导图像生成大量多样的合成图像，专注于准确描绘生殖器疣。第二阶段涉及使用该合成数据集对视觉模型进行训练和测试。这种方法旨在评估扩散模型在快速生成高质量训练数据方面的有效性以及对视觉模型的影响。

    Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision mod
    
[^66]: 逃避语言模型数据污染检测（太）容易

    Evading Data Contamination Detection for Language Models is (too) Easy

    [https://arxiv.org/abs/2402.02823](https://arxiv.org/abs/2402.02823)

    本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。

    

    大型语言模型广泛使用，它们在基准测试上的性能经常指导用户对一个模型与另一个模型的偏好。然而，这些模型所训练的大量数据可能会意外地与公共基准测试数据发生污染，从而损害性能评估。尽管最近开发了一些污染检测方法来解决这个问题，但它们忽视了恶意模型提供者有意进行污染以避免被检测的可能性。我们认为这种情况非常重要，因为它对公共基准测试的可信度产生了怀疑。为了更严格地研究这个问题，我们提出了模型提供者和污染检测方法的分类，这揭示了现有方法中的漏洞，我们通过使用EAL这种简单而有效的污染技术，明显提高了基准测试的性能，并完全逃避了当前的检测方法。

    Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
    
[^67]: 异步计划推理中的图增强大型语言模型的研究

    Graph-enhanced Large Language Models in Asynchronous Plan Reasoning

    [https://arxiv.org/abs/2402.02805](https://arxiv.org/abs/2402.02805)

    本研究是关于使用大型语言模型(LLMs)进行异步计划推理的首次大规模研究。我们发现在没有提供任务解决过程插图的情况下，现有的LLMs表现不佳。为此，我们提出了一种称为Plan Like a Graph (PLaG)的新技术，通过将图与自然语言提示相结合，实现了最先进的结果。然而，当任务复杂性增加时，LLMs仍然存在严重降级的问题，突显了LLMs在模拟数字设备方面的局限性。这项研究为将LLMs作为高效自主代理迈出了重要一步。

    

    异步计划推理具有挑战性，因为它需要顺序和并行规划以优化时间成本。大型语言模型(LLMs)能在这个任务中成功吗？在这里，我们进行了第一次大规模研究来探讨这个问题。我们发现一组代表性的闭源和开源LLMs，包括GPT-4和LLaMA-2，在我们的基准测试AsyncHow中，在没有提供任务解决过程的插图的情况下表现不佳。我们提出了一种称为Plan Like a Graph (PLaG)的新技术，它将图与自然语言提示相结合，实现了最先进的结果。我们展示了虽然PLaG能提升模型性能，但在任务复杂性增加时，LLMs仍然遭受严重降级，突出了利用LLMs模拟数字设备的局限性。我们将我们的研究视为将LLMs用作高效自主代理的一个令人兴奋的步骤。

    Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
    
[^68]: 大型语言模型蒸馏药物推荐模型

    Large Language Model Distilling Medication Recommendation Model

    [https://arxiv.org/abs/2402.02803](https://arxiv.org/abs/2402.02803)

    本研究旨在利用大型语言模型改进药物推荐方法，以解决传统模型在医学数据语义理解和新患者处方推荐方面的挑战。

    

    药物推荐是智能医疗系统中的重要方面，它根据患者特定的健康需求来推荐最合适的药物。然而，目前使用的许多复杂模型往往忽视医学数据的细微语义，而只是过度依赖标识。此外，这些模型在处理首次访问医院的患者的情况时面临重大挑战，因为它们缺乏之前的处方历史可以参考。为了解决这些问题，我们利用大型语言模型（LLMs）强大的语义理解和输入不可知的特性。我们的研究旨在利用LLMs改进现有的药物推荐方法。在本文中，我们介绍了一种新颖的方法，称为大型语言模型蒸馏药物推荐（LEADER）。我们首先创建合适的提示模板，使LLMs能够有效地推荐药物。

    The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
    
[^69]: KS-Lottery: 寻找多语言语言模型中的认证彩票

    KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models

    [https://arxiv.org/abs/2402.02801](https://arxiv.org/abs/2402.02801)

    KS-Lottery是一种寻找多语言语言模型中有效参数的方法，通过使用Kolmogorov-Smirnov检验来分析参数分布偏移，并证明了在嵌入层中可以找到认证的中奖票。这种方法可以在微调中获得与全面微调相当的性能，同时减少了所需的参数数量。

    

    彩票票证假说认为在随机初始化的神经网络中存在“中奖票”。在微调场景中，语言模型中是否存在中奖票？我们如何找到这样的中奖票？在本文中，我们提出了KS-Lottery，一种用于识别在多语言微调中高度有效的LLM参数的方法。我们的核心思想是使用Kolmogorov-Smirnov检验来分析微调前后参数的分布偏移。我们进一步理论证明了KS-Lottery可以在嵌入层中找到认证的中奖票，微调这些参数可以保证与全面微调相同的性能。通过在翻译任务上将KS-Lottery与其他参数高效调优算法进行比较，实验结果表明，KS-Lottery找到了一个更小的参数集来进行微调，同时达到了与全面微调LLM相当的性能。令人惊讶的是，我们发现微调18个标记的嵌入层

    The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
    
[^70]: 重新思考微型语言模型的优化和架构

    Rethinking Optimization and Architecture for Tiny Language Models

    [https://arxiv.org/abs/2402.02791](https://arxiv.org/abs/2402.02791)

    本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。

    

    大型语言模型（LLMs）的威力通过大量的数据和计算资源得到了证明。然而，在移动设备上应用语言模型面临着计算和内存成本的巨大挑战，迫切需要高性能的微型语言模型。受复杂训练过程的限制，优化语言模型的许多细节很少得到仔细研究。在本研究中，基于一个具有10亿参数的微型语言模型，我们仔细设计了一系列经验研究来分析每个组件的影响。主要讨论了三个方面，即神经架构、参数初始化和优化策略。多个设计公式在微型语言模型中经验性地被证明特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后，我们在1.6T多语种数据集上训练了PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。

    The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
    
[^71]: 双重知识蒸馏用于高效声音事件检测

    Dual Knowledge Distillation for Efficient Sound Event Detection

    [https://arxiv.org/abs/2402.02781](https://arxiv.org/abs/2402.02781)

    这项研究提出了一种称为双重知识蒸馏的框架，用于开发高效的声音事件检测系统。这个框架通过时序平均知识蒸馏和增强嵌入特征蒸馏来稳定地蒸馏知识并提升上下文学习能力，在实验中取得了良好的性能。

    

    声音事件检测（SED）对于识别特定声音及其在声学信号中的时间位置至关重要。特别是在设备上的应用中，由于计算资源有限，这变得很具挑战性。为了解决这个问题，我们在本研究中引入了一种新颖的框架，称之为双重知识蒸馏，用于开发高效的SED系统。我们提出的双重知识蒸馏以时序平均知识蒸馏（TAKD）为开端，利用从学生模型参数的时序平均得到的平均学生模型。这使得学生模型能够间接地从预训练的教师模型中学习，确保稳定的知识蒸馏。随后，我们引入了增强嵌入特征蒸馏（EEFD），其中包含在学生模型中引入了嵌入蒸馏层来增强上下文学习。在DCASE 2023任务4A公共评估数据集上，我们的双重知识蒸馏SED系统表现出很好的性能。

    Sound event detection (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual knowledge distillation for developing efficient SED systems in this work. Our proposed dual knowledge distillation commences with temporal-averaging knowledge distillation (TAKD), utilizing a mean student model derived from the temporal averaging of the student model's parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable knowledge distillation. Subsequently, we introduce embedding-enhanced feature distillation (EEFD), which involves incorporating an embedding distillation layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual knowle
    
[^72]: 从教学中学习正则化: 易于模仿的可推广关系

    Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate

    [https://arxiv.org/abs/2402.02769](https://arxiv.org/abs/2402.02769)

    通过从教学中学习（LoT）技术，我们提出了一种深度神经网络正则化技术，能够通过引入辅助学生模型来提升主模型的泛化性能。实验证明，LoT能有效识别具有泛化和可教授关系的信息。

    

    泛化仍然是机器学习中的一个核心挑战。在这项工作中，我们提出了一种新颖的深度神经网络正则化技术，即从教学中学习（Learning from Teaching，简称LoT），以增强泛化性能。受到人类捕捉简明抽象模式的能力的启发，我们假设可推广的关系更容易教授。LoT通过辅助学生模型来实现这个概念，通过提供反馈来训练主模型和改进主模型，以捕捉更多具有泛化和可教授关系的信息。我们在计算机视觉、自然语言处理和强化学习等多个领域进行的实验结果表明，引入LoT相比仅在原始训练数据上训练模型带来了显著的好处。这表明了LoT在识别可推广信息方面的有效性。

    Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
    
[^73]: 意图概要和通过新兴通信进行翻译的研究

    Intent Profiling and Translation Through Emergent Communication

    [https://arxiv.org/abs/2402.02768](https://arxiv.org/abs/2402.02768)

    本研究提出了一个基于人工智能的意图概要和翻译框架，通过新兴通信实现应用程序的意图概要，解决了应用程序与网络之间复杂的通信问题。

    

    为了有效地表达和满足网络应用的需求，基于意图的网络管理作为一种有前景的解决方案出现了。在基于意图的方法中，用户和应用程序使用高级抽象语言来表达他们的意图。尽管这种抽象简化了网络操作，但它对于有效地表达应用程序的意图并将其映射到不同的网络能力提出了许多挑战。因此，在这项工作中，我们提出了一种基于人工智能的意图概要和翻译框架。我们考虑到一个场景，在这个场景中，与网络交互的应用程序使用他们的领域语言来表达他们对网络服务的需求。机器与机器之间的通信（即应用程序与网络之间的通信）是复杂的，因为它要求网络学习如何理解每个应用程序的领域语言，这既不实际也不可伸缩。相反，我们提出了一个基于新兴通信的意图概要框架，用于应用程序的意图概要。

    To effectively express and satisfy network application requirements, intent-based network management has emerged as a promising solution. In intent-based methods, users and applications express their intent in a high-level abstract language to the network. Although this abstraction simplifies network operation, it induces many challenges to efficiently express applications' intents and map them to different network capabilities. Therefore, in this work, we propose an AI-based framework for intent profiling and translation. We consider a scenario where applications interacting with the network express their needs for network services in their domain language. The machine-to-machine communication (i.e., between applications and the network) is complex since it requires networks to learn how to understand the domain languages of each application, which is neither practical nor scalable. Instead, a framework based on emergent communication is proposed for intent profiling, in which applica
    
[^74]: 基于列表感知的重新排序-截断联合模型用于搜索和生成增强检索

    List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation

    [https://arxiv.org/abs/2402.02764](https://arxiv.org/abs/2402.02764)

    该论文提出了一个基于列表感知的重新排序-截断联合模型，用于搜索和生成增强检索。该模型旨在捕捉列表级的上下文特征，通过重新排序和截断来返回更好的列表。先前的研究将重新排序和截断视为两个单独的任务，但这种分离不是最优的。因此，该论文提出的联合模型可以解决信息共享和错误积累的问题。

    

    信息检索的结果通常以排名列表的形式呈现，例如面向人类的网络搜索和面向大型语言模型的检索增强生成。列表感知检索旨在捕捉列表级的上下文特征，返回更好的列表，主要包括重新排序和截断。重新排序对列表中的文档进行精细重新评分。截断动态确定排名列表的截断点，以在整体相关性和避免无关文档的错误信息之间进行权衡。过去的研究将它们视为两个单独的任务并分别对其进行建模，然而，这种分离是不理想的。首先，很难在两个任务之间共享排名列表的上下文信息。其次，独立的流水线通常会遇到错误积累问题，即重新排序阶段的小错误可能会严重影响截断阶段。为了解决这些问题，我们提出了...（继续描述我们提出的方法）

    The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose
    
[^75]: ToonAging: 艺术肖像风格转换下的人脸逆龄化

    ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer

    [https://arxiv.org/abs/2402.02733](https://arxiv.org/abs/2402.02733)

    本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。

    

    人脸逆龄化是计算机视觉和图形学中的一个重要领域，在电影、广告和直播等逼真领域中具有重要应用。最近，将人脸逆龄化应用于非逼真图像，如漫画、插图和动画，在各种娱乐行业中成为一个新的需求。然而，缺乏一个能够无缝编辑NPR图像上显现年龄的网络意味着这些任务一直局限于一个简单的顺序方法，这往往会导致不愉快的伪影和由于域差异而丢失面部属性。在本文中，我们引入了一种新颖的单阶段人脸逆龄化方法，结合了肖像风格转换，在一个生成步骤中完成。我们利用现有的人脸逆龄化和风格转换网络，两者都在相同的PR领域进行训练。我们的方法独特地融合了不同的潜在向量，每个向量负责管理与衰老相关的属性。

    Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
    
[^76]: 基于生成模型的黑盒替代攻击方法

    A Generative Approach to Surrogate-based Black-box Attacks

    [https://arxiv.org/abs/2402.02732](https://arxiv.org/abs/2402.02732)

    提出了一种基于生成模型的黑盒替代攻击方法，通过学习样本分布，生成对抗性样本，用于攻击深度神经网络。

    

    基于生成模型的黑盒替代攻击揭示了深度神经网络的易受攻击性。这些攻击旨在为给定的一组样本，通过黑盒目标反馈来生成对抗性样本。目前最先进的替代攻击方法是通过训练一个鉴别性替代模型来模拟目标模型的输出，以学习目标模型的决策边界。然后，白盒攻击针对替代模型生成与原始样本相似但属于其他类别的对抗性样本。然而，由于样本数量有限，鉴别性替代模型无法准确学习目标模型的决策边界，因此这些替代攻击方法的成功率较低。与鉴别性方法不同，我们提出了一种生成模型的替代方法，该模型学习目标模型决策边界附近或相邻样本的分布。通过生成模型学习的分布可以用于生成对抗性样本。

    Surrogate-based black-box attacks have exposed the heightened vulnerability of DNNs. These attacks are designed to craft adversarial examples for any samples with black-box target feedback for only a given set of samples. State-of-the-art surrogate-based attacks involve training a discriminative surrogate that mimics the target's outputs. The goal is to learn the decision boundaries of the target. The surrogate is then attacked by white-box attacks to craft adversarial examples similar to the original samples but belong to other classes. With limited samples, the discriminative surrogate fails to accurately learn the target's decision boundaries, and these surrogate-based attacks suffer from low success rates. Different from the discriminative approach, we propose a generative surrogate that learns the distribution of samples residing on or close to the target's decision boundaries. The distribution learned by the generative surrogate can be used to craft adversarial examples that have
    
[^77]: 降噪时间周期建模用于推荐

    Denoising Time Cycle Modeling for Recommendation

    [https://arxiv.org/abs/2402.02718](https://arxiv.org/abs/2402.02718)

    本文提出了一种新的方法，降噪时间周期建模（DiCycle），用于在推荐系统中建模用户-物品交互的时间模式。通过选择与目标物品高度相关的用户行为子集，DiCycle能够更好地建模时间周期模式，从而提高推荐性能。

    

    最近，在推荐系统中，建模用户-物品交互的时间模式引起了广泛关注。我们认为现有方法忽略了用户行为的时间模式的多样性。我们将与目标物品无关的用户行为定义为噪声，这限制了与目标相关的时间周期建模的性能，并影响了推荐性能。在本文中，我们提出了一种新颖的方法，称为降噪时间周期建模（DiCycle），用于降噪用户行为并选择与目标物品高度相关的用户行为子集。DiCycle能够明确地建模多样化的时间周期模式以进行推荐。我们在公共基准数据集和真实世界数据集上进行了大量实验，结果表明DiCycle相比最先进的推荐方法具有更好的性能。

    Recently, modeling temporal patterns of user-item interactions have attracted much attention in recommender systems. We argue that existing methods ignore the variety of temporal patterns of user behaviors. We define the subset of user behaviors that are irrelevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the recommendation performance. In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item. DiCycle is able to explicitly model diverse time cycle patterns for recommendation. Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art recommendation methods.
    
[^78]: 理解LLM代理的规划：一项调查研究

    Understanding the planning of LLM agents: A survey

    [https://arxiv.org/abs/2402.02716](https://arxiv.org/abs/2402.02716)

    这项调查研究系统地介绍了基于LLM的代理规划的最新进展和挑战。

    

    由于大型语言模型（LLM）展示了显著的智能，将LLM用作自主代理的规划模块的进展引起了更多关注。本调查提供了对基于LLM的代理规划的首个系统视角，涵盖了旨在提高规划能力的最新研究。我们对LLM-代理规划的现有研究进行了分类，包括任务分解、计划选择、外部模块、反思和记忆。对于每个方向进行了全面分析，并进一步讨论了该领域的挑战。

    As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.
    
[^79]: 一篇位置论文: 大语言模型对时间序列分析有什么启示

    Position Paper: What Can Large Language Models Tell Us about Time Series Analysis

    [https://arxiv.org/abs/2402.02713](https://arxiv.org/abs/2402.02713)

    大语言模型有潜力颠覆时间序列分析，提升决策效率，推动时间序列分析智能的普及化。这种进展可以带来模态切换和时间序列问答等多种可能性。

    

    时间序列分析对于理解各种现实世界系统和应用中的复杂性至关重要。尽管大语言模型（LLM）最近取得了显著进展，但具备时间序列分析能力的人工通用智能（AGI）的发展仍处于初级阶段。目前大部分时间序列模型主要依赖领域知识和大量模型调整，主要集中在预测任务上。本文提出，目前的LLM具有颠覆时间序列分析的潜力，从而促进高效的决策和推进向更普适形式的时间序列分析智能发展。这种进步可以打开各种可能性，包括模态切换和时间序列问答。我们鼓励研究人员和实践者认识到LLM在推进时间序列分析方面的潜力，并强调对这些相关工作的信任需求。

    Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts
    
[^80]: 多任务模型合并的表征手术

    Representation Surgery for Multi-Task Model Merging

    [https://arxiv.org/abs/2402.02705](https://arxiv.org/abs/2402.02705)

    该论文提出了一种名为“Surgery”的表征手术解决方案，用于减少多任务模型合并中的表示偏差。该方法通过一个轻量级的任务专用模块，针对合并模型的表示进行修正，以提高合并模型的性能。

    

    多任务学习（MTL）将多个任务的信息压缩到一个统一的骨干模型中，以提高计算效率和泛化能力。最近的研究直接合并多个独立训练的模型来执行MTL，而不是收集它们的原始数据进行联合训练，从而极大地扩展了MTL的应用场景。然而，通过可视化现有模型合并方案的表示分布，我们发现合并模型往往面临表示偏差的困境。也就是说，合并模型与个体模型之间的表示分布存在明显的差异，导致合并MTL性能较差。在本文中，我们提出了一个称为“Surgery”的表征手术解决方案，以减少合并模型中的表示偏差。具体而言，“Surgery”是一个轻量级的任务专用模块，它以合并模型的表示为输入，并试图输出其中包含的偏差。

    Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called "Surgery" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation fr
    
[^81]: 理解影响视觉强化学习中泛化差距的因素：理论和实证证据

    Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence

    [https://arxiv.org/abs/2402.02701](https://arxiv.org/abs/2402.02701)

    本文通过理论和实证研究，揭示了在测试环境具有干扰因素时影响视觉强化学习中泛化差距的关键因素。结果表明，最小化训练和测试环境之间的表示距离是减少泛化差距最关键的因素。

    

    最近，有许多努力致力于在视觉强化学习中学习对连续控制有用的策略。在这种场景下，学习一个具有泛化能力的策略非常重要，因为测试环境可能与训练环境不同，例如在部署过程中存在干扰因素。许多实际算法被提出来解决这个问题。然而，据我们所知，它们中没有一种算法能够从理论上解释泛化差距的影响因素以及为什么他们的方法有效。在本文中，我们通过在测试环境具有干扰因素时理论上回答影响泛化差距的关键因素来解决这个问题。我们的理论表明，最小化训练和测试环境之间的表示距离（与人类直觉一致）对于减少泛化差距的效益至关重要。我们的理论结果得到了DM数据的实证证据的支持。

    Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DM
    
[^82]: 超越期望: 现实中实现随机优势学习

    Beyond Expectations: Learning with Stochastic Dominance Made Practical

    [https://arxiv.org/abs/2402.02698](https://arxiv.org/abs/2402.02698)

    这项工作首次尝试建立了一个随机优势学习的通用框架，并推广了随机优势的概念以使其能够在任意两个随机变量之间进行比较。同时，我们还开发了一种有效的计算方法来处理连续性评估的问题。

    

    随机优势模型对决策时具有风险厌恶偏好的不确定结果进行建模，相比于仅仅依赖期望值，自然地捕捉了底层不确定性的内在结构。尽管在理论上具有吸引力，但随机优势在机器学习中的应用却很少，主要是由于以下挑战：$\textbf{i)}$ 随机优势的原始概念仅提供了$\textit{部分序}$，因此不能作为最优性准则；和 $\textbf{ii)}$ 由于评估随机优势的连续性本质，目前还缺乏高效的计算方法。在这项工作中，我们首次尝试建立一个与随机优势学习相关的通用框架。我们首先将随机优势概念推广，使得任意两个随机变量之间的比较成为可能。接下来我们开发了一个有效的计算方法，以解决评估随机优势的连续性问题。

    Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes, which naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations. Despite theoretically appealing, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\textbf{i)}$, the original concept of stochastic dominance only provides a $\textit{partial order}$, therefore, is not amenable to serve as an optimality criterion; and $\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.%, which barriers its application for machine learning.   In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance. We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables. We next develop a 
    
[^83]: 对负责任机器学习的因果特征选择的研究

    Causal Feature Selection for Responsible Machine Learning

    [https://arxiv.org/abs/2402.02696](https://arxiv.org/abs/2402.02696)

    本调查论文研究了负责任机器学习中的因果特征选择，强调了其对解释性、公平性、对抗鲁棒性和域泛化的作用。

    

    机器学习（ML）已成为许多实际应用的重要方面。因此，负责任机器学习的需求已经出现，重点是将ML模型与伦理和社会价值相一致，同时增强其可靠性和可信度。负责任的ML涉及许多问题。本调查涉及四个主要问题：可解释性，公平性，对抗鲁棒性和域泛化。特征选择在负责任的ML任务中起着重要作用。然而，仅基于变量之间的统计相关性构建模型可能会导致带有偏见和性能下降的虚假模式。本调查专注于当前对因果特征选择的研究：什么是因果特征选择以及它如何加强负责任ML的四个方面。通过识别对结果产生因果影响并区分因果和相关性，因果特征选择被认为是确保ML模型在伦理和社会上负责任的独特方法。

    Machine Learning (ML) has become an integral aspect of many real-world applications. As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness. Responsible ML involves many issues. This survey addresses four main issues: interpretability, fairness, adversarial robustness, and domain generalization. Feature selection plays a pivotal role in the responsible ML tasks. However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance. This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML. By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in hi
    
[^84]: 利用类别概率进行黑盒子句级攻击

    Exploiting Class Probabilities for Black-box Sentence-level Attacks

    [https://arxiv.org/abs/2402.02695](https://arxiv.org/abs/2402.02695)

    该论文研究了在黑盒子句级攻击中利用类别概率的有效性，并开发了一种新的算法进行攻击。通过与基线方法进行对比，进行了广泛的评估。

    

    句级攻击是针对文本分类器的对抗性句子生成方法，这些句子与正确分类的句子同义，但被分类器错误地分类。在黑盒设置下，分类器只能通过对查询输入的反馈进行访问，这主要以类别概率的形式提供。尽管利用类别概率可以获得更强大的攻击效果，但由于在句级攻击中使用类别概率存在挑战，现有的攻击方法要么不使用反馈，要么仅使用类别标签。为了克服这些挑战，我们开发了一种新的算法，使用类别概率进行黑盒句级攻击，并研究了在攻击成功率上使用类别概率的有效性，并探讨了在黑盒句级攻击中使用类别概率是否值得或可行。我们在各种分类器和基准数据集上对提出的攻击方法进行了广泛评估，并与基线进行了对比。

    Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
    
[^85]: 泊松过程用于贝叶斯优化

    Poisson Process for Bayesian Optimization

    [https://arxiv.org/abs/2402.02687](https://arxiv.org/abs/2402.02687)

    提出了一种基于泊松过程的新型排名替代模型，引入了称为泊松过程贝叶斯优化（PoPBO）的高效BO框架，并从经典的LCB和EI模型中得出了两个定制的收集函数以适应它。

    

    贝叶斯优化是一种高效的黑盒优化器，通过概率替代模型建立黑盒函数的绝对函数响应。已经提出了许多方法来构建这种模型，包括基于树结构的Parzen估计方法(TPE)、随机森林(SMAC)和高斯过程(GP)。然而，很少有方法来估计候选项的相对排名，相对排名可以比绝对函数响应更具鲁棒性，并且在函数响应难以处理但偏好可以获取时更具实用性。为此，我们提出了一种基于泊松过程的新型排名替代模型，并引入了一种高效的BO框架，称为泊松过程贝叶斯优化(PoPBO)。进一步从经典的LCB和EI模型中得出了两个定制的收集函数以适应它。与经典的GP-BO方法相比，我们的PoPBO计算成本较低，对噪声具有更好的鲁棒性。

    BayesianOptimization(BO) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian process (GP). However, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired. To this end, we propose a novel ranking-based surrogate model based on the Poisson process and introduce an efficient BO framework, namely Poisson Process Bayesian Optimization (PoPBO). Two tailored acquisition functions are further derived from classic LCB and EI to accommodate it. Compared to the classic GP-BO method, our PoPBO has lower computation costs and better robustness to noise, which is verified b
    
[^86]: 等变对称破缺集

    Equivariant Symmetry Breaking Sets

    [https://arxiv.org/abs/2402.02681](https://arxiv.org/abs/2402.02681)

    这里是中文总结出的一句话要点: 该论文提出了一种全等变的对称破缺框架，通过引入对称破缺集来破坏等变神经网络中的对称性。这种方法通用且适用于任何群的等变性。

    

    等变神经网络（ENN）已被证明在涉及潜在对称性的应用中非常有效。通过设计，ENN在给定更高对称性输入时无法产生较低对称性输出。然而，在许多物理系统中会发生自发对称破缺，我们可以从一个初始高度对称的状态获得一个较不对称的稳定状态。因此，我们必须了解如何系统地在ENN中破坏对称性。在这项工作中，我们提出了一种全等变的新型对称破缺框架。我们强调我们的方法是通用的，并适用于任何群的等变性。为了实现这一目标，我们引入了对称破缺集（SBS）的概念。我们不是重新设计现有的网络，而是设计了一组对称破缺对象，根据输入和输出的对称性将其输入到我们的网络中。我们展示了在这些集合上定义等变性的一种自然方式，它提供了额外的约束。通过最小化... (the abstract is incomplete and cut off)

    Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
    
[^87]: 大型语言模型存在地理偏见

    Large Language Models are Geographically Biased

    [https://arxiv.org/abs/2402.02680](https://arxiv.org/abs/2402.02680)

    本文研究了大型语言模型的地理偏见，并展示了其对地理空间预测的系统错误，通过零射击地理空间预测来评估其对世界的认知。

    

    大型语言模型（LLMs）内在地含有其训练语料库中的偏见，这可能导致社会伤害的持续存在。随着这些基础模型的影响力不断增长，理解和评估它们的偏见对于实现公正和准确性至关重要。本文提出通过地理视角研究LLMs对我们所生活的世界的认知。这种方法特别强大，因为对人类生活中诸多与地理空间相关的方面（如文化、种族、语言、政治和宗教）有着明显的真实性。我们展示了各种问题地理偏见，我们将其定义为地理空间预测中的系统错误。首先，我们证明LLMs能够进行精确的零射击地理空间预测，以评级的形式呈现，其与真实情况之间呈现出强烈的单调相关性（Spearman's ρ最高可达0.89）。然后，我们展示了LLMs在多个客观和子领域上表现出共同的偏见。

    Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
    
[^88]: 使用zkSNARKs进行可验证的机器学习模型评估

    Verifiable evaluations of machine learning models using zkSNARKs

    [https://arxiv.org/abs/2402.02675](https://arxiv.org/abs/2402.02675)

    本论文提出了一种使用zkSNARKs进行机器学习模型评估的方法。通过模型推理和零知识计算证明，可以提供可验证的评估证明，验证模型在公开输入上的性能和公平性指标。这有助于解决闭源商业机器学习模型评估可信性的问题。

    

    在越来越多闭源商业机器学习模型的世界中，开发者的模型评估必须被当作面值接受。这些评估结果，无论是任务准确性、偏差评估还是安全检查，传统上无法通过重新执行黑箱模型输出的基准测试来进行验证。本研究提出了一种使用zkSNARKs进行可验证模型评估的方法。通过zkSNARKs进行模型推理，得到的模型输出的零知识计算证明可以打包成可验证的评估证明，显示具有固定私有权重的模型在公开输入上达到了所述的性能或公平性指标。这些可验证的评估证明可以在任何标准神经网络模型上进行，计算要求各不相同。我们首次在一系列真实模型上展示了这一点，并突出了关键挑战和设计解决方案。

    In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a n
    
[^89]: 多步问题求解中的验证器：关于模型引导的过程监督的实证分析

    Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision

    [https://arxiv.org/abs/2402.02658](https://arxiv.org/abs/2402.02658)

    本文介绍了一种名为模型引导的过程监督（MiPS）的新方法，该方法通过对推理模型的解决方案进行抽样完成来自动进行数据整理，从而避免了昂贵的人工注释。实证结果表明，与之前的工作相反，我们应优先选择验证器预测得分高的验证。这种方法显著改进了PaLM 2在数学和编码任务上的性能。

    

    过程监督使用训练好的验证器来评估推理器生成的中间步骤，已经在多步问题求解中展示出了显著的改进。在本文中，为了避免在验证器训练数据上进行昂贵的人工注释，我们引入了模型引导的过程监督（MiPS），这是一种自动化数据整理的新方法。MiPS通过对推理模型的解决方案进行抽样完成，并获得一个准确率，其中准确完成的比例定义为准确率。推理器中的错误会导致MiPS低估中间步骤的准确率，因此，我们建议并通过实验证明，与之前的工作相反，应优先选择验证器预测得分高的验证，而不是低的。我们的方法显著提高了PaLM 2在数学和编码任务上的性能（GSM8K上的准确率+0.67％，数学上的准确率+4.16％，MBPP上的准确率+0.92％与输出s相比。）

    Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
    
[^90]: 视觉-语言模型为强化学习提供可提示的表示

    Vision-Language Models Provide Promptable Representations for Reinforcement Learning

    [https://arxiv.org/abs/2402.02651](https://arxiv.org/abs/2402.02651)

    本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。

    

    人类可以通过利用背景世界知识快速学习新的行为。相比之下，利用强化学习训练的代理通常需要从零开始学习行为。因此，我们提出了一种新的方法，利用在互联网规模数据上预训练的视觉-语言模型（VLMs）中编码的大量通用和可索引的世界知识来进行具象的强化学习。我们通过将VLMs用作可提示表示来初始化策略：这些嵌入在视觉观察中具有基础，并根据VLM的内部知识编码语义特征，通过提供任务上下文和辅助信息来触发。我们在Minecraft和Habitat中的视觉复杂、长期的强化学习任务上评估了我们的方法。我们发现，使用通用型VLMs提取的嵌入训练的策略胜过使用通用的、不可提示的图像嵌入训练的策略。我们还发现我们的方法胜过遵循指示的元策略。

    Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
    
[^91]: 链式反馈：缓解回答不一致性的影响

    Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses

    [https://arxiv.org/abs/2402.02648](https://arxiv.org/abs/2402.02648)

    本文提出了链式反馈（CoF）方法，以缓解大型语言模型在回答问题中出现不一致性的问题。同时，作者还提出了递归链式反馈（R-CoF）方法，并提到了进一步的研究。这些方法可以提高回答的可靠性和有效性。

    

    大型语言模型（LLMs）在回答知识密集型问题时经常出现不一致的情况，即使输入相同，也会提供不同的输出。当用户表达坚决相反的立场时，LLMs调整其回答的质量会变差，尽管初始回答是正确的。这些行为降低了这些模型提供的回答的可靠性和有效性。在本文中，我们试图：1）通过展示链式反馈（CoF）如何导致LLMs更加偏离实际答案，引起过度依赖ChatGPT等AI代理带来的固有风险；2）提出一种新的提示方法，递归链式反馈（R-CoF），我们正在进行进一步研究。CoF系统接收一个开放式多步问题，然后我们重复提供无意义的反馈，要求再次尝试。我们的初步实验表明，这种反馈只会降低回答的质量。另一方面，

    Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth
    
[^92]: LLM增强型数据管理

    LLM-Enhanced Data Management

    [https://arxiv.org/abs/2402.02643](https://arxiv.org/abs/2402.02643)

    LLMDB是一种LLM增强的数据管理范式，通过LLM的微调和提示工程嵌入领域特定知识，解决了虚构、高成本和低准确性的挑战，并实现了泛化能力和高推理能力。

    

    近年来，针对优化数据管理问题的机器学习（ML）技术得到了广泛研究和广泛部署。然而，传统的ML方法在泛化能力（适应不同情景）和推理能力（理解上下文）方面存在局限性。幸运的是，大型语言模型（LLMs）表现出高度的泛化能力和人类竞争能力，对于数据管理任务（如数据库诊断、数据库调优）具有潜在的应用前景。然而，现有的LLMs存在一些限制：虚构、高成本以及对复杂任务的低准确性。为了应对这些挑战，我们设计了LLMDB，一种使用LLM增强的数据管理范式，具有良好的泛化能力和高推理能力，同时避免了虚构，降低了LLM成本，提高了准确性。LLMDB通过LLM的微调和提示工程嵌入领域特定知识以避免虚构。LLMDB通过减少LLMs的高成本 addresses challenges: hallucination, high cost, low accuracy-

    Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by 
    
[^93]: 用多个时间视角增强Transformer RNNs

    Enhancing Transformer RNNs with Multiple Temporal Perspectives

    [https://arxiv.org/abs/2402.02625](https://arxiv.org/abs/2402.02625)

    引入了多个时间视角的概念，用于增强Transformer RNNs对顺序数据的理解能力，在参数数量最小增加的情况下取得了显著的改进。

    

    我们引入了多个时间视角的概念，这是一种适用于循环神经网络（RNN）架构的新方法，用于增强其对顺序数据的理解。该方法涉及维护先前遇到的文本的多样时间视图，显著丰富了语言模型解释上下文的能力。为了展示这种方法的有效性，我们将其纳入了Receptance Weighted Key Value（RWKV）架构，解决了该架构在单个隐藏状态中保留所有历史信息的固有挑战。值得注意的是，即使参数数量增加最少（仅为最初参数数量的0.04%），也实现了此改进。此外，多个时间视角所需的额外参数经过微小的计算开销进行微调，避免了完全预训练的需要。由此产生的模型在提示推断过程中保持了线性的计算复杂度。

    We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
    
[^94]: 一种安全的强化学习驱动的权重变化模型预测控制用于自动驾驶车辆运动控制

    A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control

    [https://arxiv.org/abs/2402.02624](https://arxiv.org/abs/2402.02624)

    提出了一种安全的强化学习驱动的权重变化模型预测控制方法，用于自动驾驶车辆运动控制。该方法通过在运行时动态调整代价函数权重，可以提供上下文相关的最优闭环控制性能。

    

    确定模型预测控制(MPC)的最佳代价函数参数以优化多个控制目标是一项具有挑战性且耗时的任务。多目标贝叶斯优化(BO)技术通过确定MPC的 Pareto 最优参数集解决了这个问题，然而，当MPC的操作条件上下文在其运行过程中发生变化时，单个参数集可能无法提供最优的闭环控制性能，因此需要在运行时调整代价函数权重。深度强化学习(RL)算法能够自动学习上下文相关的最优参数集，并在权重变化的MPC中进行动态调整。然而，在连续动作空间中从头学习代价函数权重可能导致不安全的操作状态。为了解决这个问题，我们提出了一种新方法，将RL的动作限制在安全学习空间内，该空间表示经过预优化的BO Pareto最优权重的目录。

    Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight se
    
[^95]: 高效市场动态：通过Betfair的时间序列分析揭示UK赛马投注市场的信息效率

    Efficient Market Dynamics: Unraveling Informational Efficiency in UK Horse Racing Betting Markets Through Betfair's Time Series Analysis

    [https://arxiv.org/abs/2402.02623](https://arxiv.org/abs/2402.02623)

    通过对Betfair的时间序列数据进行分析，研究发现英国赛马市场在投注交易回报方面表现出了高水平的信息效率，与金融资产不同，市场中的知识能够快速被吸收并反映在价格上，并且市场对新信息的响应速度较快。

    

    使用Betfair的时间序列数据，对英国（UK）赛马市场进行了分析，发现了一个有趣的悖论：投注交易回报的市场具有短尾、快速衰减的自相关性和无长期记忆。与具有重尾和波动性聚类特征的金融资产相比，投注交易回报的信息效率非常高。具有轻尾的广义高斯无条件分布表明市场知识很快被吸收并反映在价格上。这进一步得到了自相关性极快减弱和收益-损失不对称缺失的支持。因此，除了测量长期记忆之外，Hurst指数也显示出均值回归，表明市场对新信息的快速响应。

    Using Betfair's time series data, an analysis of the United Kingdom (UK) horse racing market reveals an interesting paradox: a market with short tails, rapidly decaying autocorrelations, and no long-term memory. There seems to be a remarkably high level of informational efficiency in betting exchange returns, in contrast to financial assets that are characterized by heavy tails and volatility clustering. The generalized Gaussian unconditional distribution with a light tail point to a market where knowledge is quickly assimilated and reflected in prices. This is further supported by the extremely quick fading of autocorrelations and the absence of gain-loss asymmetry. Therefore, in addition to measuring long-range memory, the Hurst exponent also shows mean reversion, a sign that markets respond quickly to fresh information.
    
[^96]: PuzzleBench：LLMs能否解决困难的一阶组合推理问题？

    PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?

    [https://arxiv.org/abs/2402.02611](https://arxiv.org/abs/2402.02611)

    本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。

    

    最近的研究探索了使用LLMs进行推理任务，重点是相对简单的问题，如逻辑问答。在我们的工作中，我们希望解决更复杂的问题，显著扩展这些模型的功能。特别是，我们探讨LLMs是否能够解决困难的一阶组合推理问题，一个例子是流行的数独谜题。这些问题有一个由自然语言描述的基础一阶结构，并且可以实例化为不同大小的实例。此外，这些问题在计算上是密集型的，需要多个推理步骤才能达到解决方案。我们提出了PuzzleBench，一个包含31个这样具有挑战性的谜题的数据集。我们观察到，即使在符号求解器的帮助下，LLMs在我们的基准测试中表现得相当糟糕。作为回应，我们提出了一种新的方法，Puzzle-LM，它将LLMs与符号求解器和程序解释器相结合，使它们能够推理这类问题。

    Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
    
[^97]: 绕过基于深度学习的恶意软件检测器的混淆方法：一种基于深度强化学习的方法

    Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep Reinforcement Learning Approach

    [https://arxiv.org/abs/2402.02600](https://arxiv.org/abs/2402.02600)

    该论文提出了一种基于深度强化学习的方法，通过使用开源加密工具和强化学习框架成功地混淆恶意软件，规避了最先进的恶意软件检测器，并且在规避率方面超越了使用高级修改方法的技术。

    

    对抗性恶意软件生成（AMG）是生成对抗性恶意软件变种以加强基于深度学习（DL）的恶意软件检测器的重要工具，它已成为主动式网络防御的关键技术。然而，现有的大多数方法仅提供对可执行文件的微小扰动或添加，并没有探索全文件混淆。在本研究中，我们展示了一个开源加密工具结合强化学习（RL）框架可以成功地混淆恶意软件以规避最先进的恶意软件检测引擎，并超越使用高级修改方法的技术。我们的结果表明，与广泛使用的最先进的基于强化学习的方法相比，所提出的方法将规避率提高了27%-49%。

    Adversarial Malware Generation (AMG), the gen- eration of adversarial malware variants to strengthen Deep Learning (DL)-based malware detectors has emerged as a crucial tool in the development of proactive cyberdefense. However, the majority of extant works offer subtle perturbations or additions to executable files and do not explore full-file obfuscation. In this study, we show that an open-source encryption tool coupled with a Reinforcement Learning (RL) framework can successfully obfuscate malware to evade state-of-the-art malware detection engines and outperform techniques that use advanced modification methods. Our results show that the proposed method improves the evasion rate from 27%-49% compared to widely- used state-of-the-art reinforcement learning-based methods.
    
[^98]: 统一训练通用时间序列预测Transformer

    Unified Training of Universal Time Series Forecasting Transformers

    [https://arxiv.org/abs/2402.02592](https://arxiv.org/abs/2402.02592)

    本研究提出了一种改进的时间序列Transformer架构，名为Moirai，以解决时间序列预测中的跨频率学习、适应多变量时间序列以及解决大规模数据固有的不同分布特性等挑战，从而实现了统一训练通用时间序列预测Transformer。

    

    传统上，时间序列预测的深度学习在一个数据集中一模型的框架下运作，限制了其能够利用大型预训练模型的突破性影响的潜力。通用预测的概念，源于在大量时间序列数据集上进行预训练，设想一个能够处理各种下游预测任务的单一大型时间序列模型。然而，构建这样的模型对于时间序列数据存在独特的挑战，包括：i) 跨频率学习，ii) 适应多变量时间序列中任意数量的变量，以及iii) 解决大规模数据固有的不同分布特性。为了解决这些挑战，我们对传统的时间序列Transformer架构进行了新颖的增强，提出了基于Masked Encoder的通用时间序列预测Transformer（Moirai）。在我们新引入的大规模开放时间序列存档（LOTSA）数据集上进行训练。

    Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea
    
[^99]: DefInt：一种用于高效处理混合大型语言模型推理的默认干预框架

    DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models

    [https://arxiv.org/abs/2402.02563](https://arxiv.org/abs/2402.02563)

    DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。

    

    大型语言模型（LLMs）在各种任务中展示出令人印象深刻的新能力，但在处理复杂推理问题方面仍面临挑战。以往的研究如连锁推理（CoT）和思维树（ToT）主要关注提高准确性，但忽视了不断增加的标记成本，这对于具有巨大解空间的开放性实际任务来说可能特别问题。受人类认知的双过程理论的启发，我们提出了一种默认干预框架（DefInt），以释放混合LLMs的协同潜力。默认情况下，DefInt使用较小规模的语言模型生成低成本的推理思路，类似于“系统1”产生的快速直觉。如果这些直觉被认为低置信度，则DefInt将调用放大的语言模型的反思推理作为“系统2”的干预，可以覆盖默认思考并纠正推理过程。实验在五个实际数据集上展示了DefInt论文中的有效性。

    Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
    
[^100]: Neur2BiLO: 神经双层优化

    Neur2BiLO: Neural Bilevel Optimization

    [https://arxiv.org/abs/2402.02552](https://arxiv.org/abs/2402.02552)

    Neur2BiLO是一个针对双层优化问题的框架，通过将神经网络近似引入到混合整数规划中，可以快速生成高质量的解决方案。

    

    双层优化处理嵌套问题，在这些问题中，领导者首先做出决策以最小化自己的目标函数，同时考虑到追随者的最好反应。整数变量约束的双层问题特别难以处理。尽管已经提出了用于混合整数线性双层优化的精确求解器，但它们在问题规模较大时往往无法扩展，并且难以推广到非线性情况。另一方面，问题特定的算法（精确和启发式）局限于特定范围。在以数据驱动的环境下，我们提出的框架Neur2BiLO将通过监督回归训练的领导者或追随者的值函数的神经网络近似嵌入到易于解决的混合整数规划中。 Neur2BiLO作为一种启发式算法，可以快速生成高质量的解决方案，适用于双层背包拦截问题，即“关键n个问题”。

    Bilevel optimization deals with nested problems in which a leader takes the first decision to minimize their objective function while accounting for a follower's best-response reaction. Constrained bilevel problems with integer variables are particularly notorious for their hardness. While exact solvers have been proposed for mixed-integer linear bilevel optimization, they tend to scale poorly with problem size and are hard to generalize to the non-linear case. On the other hand, problem-specific algorithms (exact and heuristic) are limited in scope. Under a data-driven setting in which similar instances of a bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds a neural network approximation of the leader's or follower's value function, trained via supervised regression, into an easy-to-solve mixed-integer program. Neur2BiLO serves as a heuristic that produces high-quality solutions extremely fast for the bilevel knapsack interdiction problem, the "critical n
    
[^101]: "我的模型在什么里面？": 探索环境在基于实践的自然语言理解中的作用

    "What's my model inside of?": Exploring the role of environments for grounded natural language understanding

    [https://arxiv.org/abs/2402.02548](https://arxiv.org/abs/2402.02548)

    本论文探索了环境对于改进基于实践的自然语言理解中数据采集和模型开发的潜力，通过开发新的训练方法和基准，采用生态方法研究基于实践的语言理解系统在自然/模拟虚拟环境中的角色。

    

    与研究孤立的大脑的经典认知科学不同，生态方法侧重于身体和环境在塑造认知过程中的作用。同样，在本论文中，我们采用了生态方法来研究基于实践的自然语言理解（NLU）。基于实践的语言理解研究将语言理解系统置于自然/模拟虚拟环境中的事件、动作和预期之中。与经典研究倾向于专注于设计新模型和优化方法，同时将环境视为给定的情况不同，我们探索了环境设计对于改进数据采集和模型开发的潜力。我们基于基于文本的游戏环境开发了新的训练和标注方法，用于程序化文本理解。我们还参考了基于体验的认知语言学文献，提出了基于实践的NLP研究路线图，并为衡量进展提供了一个新的基准。

    In contrast to classical cognitive science which studied brains in isolation, ecological approaches focused on the role of the body and environment in shaping cognition. Similarly, in this thesis we adopt an ecological approach to grounded natural language understanding (NLU) research. Grounded language understanding studies language understanding systems situated in the context of events, actions and precepts in naturalistic/simulated virtual environments. Where classic research tends to focus on designing new models and optimization methods while treating environments as given, we explore the potential of environment design for improving data collection and model development. We developed novel training and annotation approaches for procedural text understanding based on text-based game environments. We also drew upon embodied cognitive linguistics literature to propose a roadmap for grounded NLP research, and to inform the development of a new benchmark for measuring the progress of
    
[^102]: 将认知任务整合到大型模型的人工通用智能测试中

    Integration of cognitive tasks into artificial general intelligence test for large models

    [https://arxiv.org/abs/2402.02547](https://arxiv.org/abs/2402.02547)

    建议将认知任务整合到大型模型的人工通用智能测试中，以建立一个综合框架，能够评估大型模型的多维智能。这个框架结合了认知科学和自然语言处理，包含了稳态智力、流态智力和社交智能等方面。

    

    在大型模型的发展过程中，必须对中间模型进行性能评估以评估其能力，并对经过充分训练的模型进行安全性评估，以确保在实际应用之前的安全性。然而，当前的模型评估主要依赖于特定任务和数据集，缺乏对大型模型的多维智能评估的统一框架。在这个视角中，我们提倡建立一个人工通用智能测试的综合框架，旨在满足大型语言模型和多模态大型模型的测试需求，以提高其能力。该人工通用智能测试框架将认知科学和自然语言处理联系起来，包括智力的各个方面，包括稳态智力，即积累的知识和经验的反映; 流态智力，特点是解决问题和适应性推理; 社交智能，表示在多方面理解和适应的能力。

    During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities. The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifacete
    
[^103]: LHRS-Bot：利用VGI增强的大型多模态语言模型赋能遥感领域

    LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model

    [https://arxiv.org/abs/2402.02544](https://arxiv.org/abs/2402.02544)

    LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    

    大型语言模型（LLMs）的革命性能力开创了多模态大型语言模型（MLLMs）并促进了在各个专业领域的多样化应用。然而，在遥感（RS）领域中，近期的MLLM努力未能充分考虑到遥感图像中多样的地理景观和物体。为了弥补这一差距，我们构建了一个大规模的RS图像-文本数据集LHRS-Align，以及一个信息丰富的RS特定指导数据集LHRS-Instruct，利用丰富的自愿地理信息（VGI）和全球可用的RS图像。在此基础上，我们引入了LHRS-Bot，一种针对RS图像理解的MLLM，通过一种新颖的多层次视觉-语言对齐策略和课程学习方法。全面的实验证明，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
    
[^104]: 非主动自适应采样中的绝对收敛和误差阈值

    Absolute convergence and error thresholds in non-active adaptive sampling

    [https://arxiv.org/abs/2402.02522](https://arxiv.org/abs/2402.02522)

    提出了一种计算非主动自适应采样中绝对收敛和误差阈值的方法，可以确定模型何时不再增加质量，并提供了一个接近条件来估算模型实现目标的接近度，从而支持模型选择中学习参数的微调。

    

    非主动自适应采样是一种从训练数据中构建机器学习模型的方法，它可以动态和自动地确定保证的样本大小。在这个背景下，无论所采用的调度和生成弱预测器的策略如何，我们描述了一种计算绝对收敛和误差阈值的方法。我们不仅可以确定模型的质量何时不再增加，还提供了一个接近条件来绝对地估算模型实现这一目标的接近度，从而支持在模型选择中进行学习参数的微调。该技术在工作假设方面证明了其正确性和完备性，同时增强了采样方案的鲁棒性。测试结果符合我们的预期，并以自然语言处理领域中词性标注生成为案例研究来说明这一提议。

    Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.
    
[^105]: SIMPL:一种简单高效的自动驾驶多智能体运动预测基线

    SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving

    [https://arxiv.org/abs/2402.02519](https://arxiv.org/abs/2402.02519)

    本文提出了一种简单高效的自动驾驶多智能体运动预测基线(SIMPL)，通过引入紧凑高效的全局特征融合模块以及使用Bernstein基多项式对连续轨迹参数化的方法，实现了实时准确的运动预测，为下游规划任务提供了有价值的数据。

    

    本文提出了一种简单高效的自动驾驶车辆的运动预测基线，名为SIMPL。与传统的以智能体为中心的方法相比，虽然具有高准确性但计算重复，以及以场景为中心的方法虽然准确性和泛化性有所妥协，SIMPL可以实时、准确地预测所有相关交通参与者的运动。为了在准确性和推理速度上实现改进，我们提出了一种紧凑高效的全局特征融合模块，以对称方式执行定向消息传递，使网络能够在单次前向传递中预测所有道路使用者的未来运动，并减轻视角转换带来的准确性损失。此外，我们还研究了使用Bernstein基多项式对连续轨迹参数化，允许在任何所需时间点评估轨迹和其高阶导数，这对下游规划任务非常有价值。作为一个强大的基准方法，SIMPL打破了模式并提供了准确结果。

    This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL) for autonomous vehicles. Unlike conventional agent-centric methods with high accuracy but repetitive computations and scene-centric methods with compromised accuracy and generalizability, SIMPL delivers real-time, accurate motion predictions for all relevant traffic participants. To achieve improvements in both accuracy and inference speed, we propose a compact and efficient global feature fusion module that performs directed message passing in a symmetric manner, enabling the network to forecast future motion for all road users in a single feed-forward pass and mitigating accuracy loss caused by viewpoint shifting. Additionally, we investigate the continuous trajectory parameterization using Bernstein basis polynomials in trajectory decoding, allowing evaluations of states and their higher-order derivatives at any desired time point, which is valuable for downstream planning tasks. As a strong baseline, SIMP
    
[^106]: 适应性调度用于自适应采样在构建词性标注器中

    Adaptive scheduling for adaptive sampling in POS taggers construction

    [https://arxiv.org/abs/2402.02516](https://arxiv.org/abs/2402.02516)

    本论文提出了一种自适应调度的自适应采样方法，用于在构建词性标注器中加快训练速度，同时提高采样的鲁棒性。该方法分析学习曲线的形状，在任何时候增加或减少实例，以确保获得学习能力的净增益。

    

    我们引入了一种自适应调度的自适应采样作为构建词性标注器的机器学习的新方法。目标是在大规模数据集上加快训练速度，同时不显著损失性能。与之前使用随机、固定或定期增加实例之间间隔的方法不同，我们的方法在几何上分析学习曲线的形状，结合功能模型，在任何时候增加或减少实例。该算法在我们的工作假设上被证明是形式上正确的。也就是说，给定一个案例，下一个案例是最近的，确保从前者中获得学习能力的净增益，可以调节此条件的要求水平。我们还通过更加关注在训练数据库中临时性能膨胀的区域，提高了采样的鲁棒性，从而防止学习提前停止。

    We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematu
    
[^107]: 学习曲线建模及其在词性标注中的应用

    Modeling of learning curves with applications to pos tagging

    [https://arxiv.org/abs/2402.02515](https://arxiv.org/abs/2402.02515)

    该论文提出了一种估计学习曲线演化的算法，可以通过部分训练数据结果来预测学习过程中达到期望准确度所需的时间。

    

    介绍了一种基于部分训练数据结果和使用功能策略的算法，用于估计整个训练数据集上的学习曲线的演化。我们通过迭代逼近所需时间点的待求值，独立于所使用的学习技术，并且在经过一定的过程点（称为预测级别）后。该提案在工作假设方面被证明是形式上正确的，并且包含一个可靠的近似条件。这使得用户可以基于最终可实现的准确度来设定收敛阈值，这扩展了停止准则的概念，即使存在扭曲观察结果，也似乎是有效的。我们的目标是评估培训工作量，支持决策过程来减少学习过程中所需的人力和计算资源。该提案在至少三个操作程序中很有兴趣。第一个是预测学习过程中达到期望准确度所需的时间。

    An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations.   Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of
    
[^108]: 在神经网络中通过相关在线指标来提前停止

    Early stopping by correlating online indicators in neural networks

    [https://arxiv.org/abs/2402.02513](https://arxiv.org/abs/2402.02513)

    本文提出了一种在神经网络中最小化泛化误差的新技术，通过利用一系列在线指标的时间相关性，找到过拟合现象，并提供了可靠的提前停止条件，从而提高了预测能力。

    

    为了最小化神经网络中的泛化误差，引入了一种新颖的技术来在训练学习者时识别过拟合现象。这使得支持可靠和可信的提前停止条件成为可能，从而提高了该类型建模的预测能力。我们的提议利用一系列在线指标的时间相关性，即用于指示一组假设是否满足的特征函数，与从金丝雀判断中构建的一系列独立停止条件相联系，以评估过拟合的存在。通过这种方式，我们为决策提供了形式化的基础，以中断学习过程。与之前专注于单一标准的方法相反，我们利用独立评估之间的附带效应，寻求更广泛的操作范围和更大的诊断可靠性。

    In order to minimize the generalization error in neural networks, a novel technique to identify overfitting phenomena when training the learner is formally introduced. This enables support of a reliable and trustworthy early stopping condition, thus improving the predictive power of that type of modeling. Our proposal exploits the correlation over time in a collection of online indicators, namely characteristic functions for indicating if a set of hypotheses are met, associated with a range of independent stopping conditions built from a canary judgment to evaluate the presence of overfitting. That way, we provide a formal basis for decision making in terms of interrupting the learning process.   As opposed to previous approaches focused on a single criterion, we take advantage of subsidiarities between independent assessments, thus seeking both a wider operating range and greater diagnostic reliability. With a view to illustrating the effectiveness of the halting condition described, 
    
[^109]: 点云问题:重新思考不同观测空间对机器人学习的影响

    Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning

    [https://arxiv.org/abs/2402.02500](https://arxiv.org/abs/2402.02500)

    通过广泛实验发现基于点云的方法在机器人学习中表现出更好的性能，特别是在各种预训练和泛化任务中。结果表明，点云观测模态对于复杂机器人任务是有价值的。

    

    在这项研究中，我们探讨了不同观测空间对机器人学习的影响，重点关注了三种主要模态：RGB，RGB-D和点云。通过在超过17个不同接触丰富的操作任务上进行广泛实验，涉及两个基准和仿真器，我们观察到了一个显著的趋势：基于点云的方法，即使是最简单的设计，通常在性能上超过了其RGB和RGB-D的对应物。这在从头开始训练和利用预训练的两种情况下都是一致的。此外，我们的研究结果表明，点云观测在相机视角、照明条件、噪声水平和背景外观等各种几何和视觉线索方面，都能提高策略零样本泛化能力。研究结果表明，三维点云是复杂机器人任务中有价值的观测模态。我们将公开所有的代码和检查点，希望我们的观点能帮助解决问题。

    In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
    
[^110]: X射线与CT图像融合的全可微相关驱动2D/3D配准

    Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to CT Image Fusion

    [https://arxiv.org/abs/2402.02498](https://arxiv.org/abs/2402.02498)

    本论文提出一种全可微相关驱动网络用于X射线与CT图像融合的配准，通过双分支CNN-Transformer编码器实现低频全局特征和高频局部特征的提取和分离，进一步提出相关驱动损失进行特征分解，并应用凸形相似函数学习的训练策略。实验证明，该方法在性能上优于现有的全可微学习配准方法。

    

    基于图像的刚性2D/3D配准是导向外科手术干预的重要技术。近年来，一些基于学习的全可微方法取得了有益的成果，但特征提取和梯度传递过程仍缺乏可控性和可解释性。为了缓解这些问题，在本研究中，我们提出了一种新颖的全可微相关驱动网络，利用双分支CNN-Transformer编码器实现了低频全局特征和高频局部特征的提取和分离。进一步提出了一种基于内嵌信息的低频特征和高频特征分解的相关驱动损失。此外，我们应用一种学习逼近凸形相似函数的训练策略。我们在一个自建数据集上测试了我们的方法，并展示了其优于现有全可微学习配准方法的性能。

    Image-based rigid 2D/3D registration is a critical technique for fluoroscopic guided surgical interventions. In recent years, some learning-based fully differentiable methods have produced beneficial outcomes while the process of feature extraction and gradient flow transmission still lack controllability and interpretability. To alleviate these problems, in this work, we propose a novel fully differentiable correlation-driven network using a dual-branch CNN-transformer encoder which enables the network to extract and separate low-frequency global features from high-frequency local features. A correlation-driven loss is further proposed for low-frequency feature and high-frequency feature decomposition based on embedded information. Besides, a training strategy that learns to approximate a convex-shape similarity function is applied in our work. We test our approach on a in-house datasetand show that it outperforms both existing fully differentiable learning-based registration approach
    
[^111]: BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback

    BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback

    [https://arxiv.org/abs/2402.02479](https://arxiv.org/abs/2402.02479)

    BRAIn是一种基于贝叶斯奖励条件化缩减推断的自然语言生成方法，通过反馈来改进RLHF，在LLM对齐中表现出较好的可扩展性和性能。

    

    在人类反馈的强化学习领域，继Proximal Policy Optimization (PPO)取得成功之后，提出了一种新的方法，如Sequence Likelihood Calibration (SLiC)和Direct Policy Optimization (DPO)，这些方法是离线的，并且以间接的方式使用奖励。这些技术，特别是DPO，由于其可扩展性和性能，最近已经成为LLM对齐的首选工具。然而，它们遗漏了PPO方法的重要特征。诸如SLiC或RRHF的方法仅利用奖励模型(RM)进行排序/偏好，丢失了细粒度信息，忽略了RM的参数形式(例如Bradley-Terry、Plackett-Luce)；而诸如DPO的方法甚至不使用单独的奖励模型。在这项工作中，我们提出了一种新颖的方法，命名为BRAIn，它将RM作为分布匹配方法的一部分重新引入。BRAIn考虑到了LLM分布在假设输出质量良好的条件下，并应用B...

    Following the success of Proximal Policy Optimization (PPO) for Reinforcement Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that are offline in nature and use rewards in an indirect manner. These techniques, in particular DPO, have recently become the tools of choice for LLM alignment due to their scalability and performance. However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward model. In this work, we propose a novel approach, named BRAIn, that re-introduces the RM as part of a distribution matching approach.BRAIn considers the LLM distribution conditioned on the assumption of output goodness and applies B
    
[^112]: 为什么双曲神经网络有效？关于层级表示能力的研究

    Why are hyperbolic neural networks effective? A study on hierarchical representation capability

    [https://arxiv.org/abs/2402.02478](https://arxiv.org/abs/2402.02478)

    本文通过大规模实验对为什么双曲神经网络有效进行了全面分析，并提出了几种预训练策略来增强层级表示能力，从而改进下游任务的性能。

    

    双曲神经网络（HNNs）在双曲空间中运作，在过去几年中被广泛应用，其动机是双曲空间中存在一种优化嵌入，能够比欧几里得空间更准确地保留数据的层级关系（称为层级表示能力，HRC）。然而，没有证据表明HNN可以达到理论上的最优嵌入，这导致了许多研究建立在错误的动机上。本文提出了一个评估HRC的基准，并通过大规模实验对HNN为何有效进行了全面分析。受到分析结果的启发，我们提出了几种预训练策略来增强HRC并提高下游任务的性能，进一步验证了分析的可靠性。实验证明，HNN无法实现理论上的最优嵌入。HRC受优化目标和层级结构的显著影响，

    Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been widely applied in recent years, motivated by the existence of an optimal embedding in hyperbolic space that can preserve data hierarchical relationships (termed Hierarchical Representation Capability, HRC) more accurately than Euclidean space. However, there is no evidence to suggest that HNNs can achieve this theoretical optimal embedding, leading to much research being built on flawed motivations. In this paper, we propose a benchmark for evaluating HRC and conduct a comprehensive analysis of why HNNs are effective through large-scale experiments. Inspired by the analysis results, we propose several pre-training strategies to enhance HRC and improve the performance of downstream tasks, further validating the reliability of the analysis. Experiments show that HNNs cannot achieve the theoretical optimal embedding. The HRC is significantly affected by the optimization objectives and hierarchical structures, and 
    
[^113]: 基于上下文感知探索的快速适应未知同伴

    Fast Peer Adaptation with Context-aware Exploration

    [https://arxiv.org/abs/2402.02468](https://arxiv.org/abs/2402.02468)

    本文提出了一种基于上下文感知的探索方法，用于快速适应具有不同策略的未知同伴。通过奖励智能体在历史上下文中有效识别同伴行为模式，该方法能够促进智能体积极探索和快速适应，从而在不确定同伴策略时收集信息反馈，并在有信心时利用上下文执行最佳反应。

    

    在多智能体游戏中，快速适应具有不同策略的未知同伴是一个关键挑战。为了做到这一点，智能体能够高效地探索和识别同伴的策略至关重要，因为这是适应中进行最佳反应的先决条件。然而，当游戏是部分可观测且时间跨度很长时，探索未知同伴的策略是困难的。在本文中，我们提出了一种同伴识别奖励，根据智能体在历史环境下（例如多个回合的观察）如何有效地识别同伴的行为模式来奖励学习智能体。这个奖励激励智能体学习一种基于上下文的策略，以实现有效探索和快速适应，即在对同伴策略不确定时积极寻找和收集信息反馈，并在有信心时利用上下文执行最佳反应。我们在不同的测试场景上进行了评估。

    Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds 
    
[^114]: 一张图值千言：使用纯Transformer将图形欧拉化

    A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer

    [https://arxiv.org/abs/2402.02464](https://arxiv.org/abs/2402.02464)

    这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。

    

    我们能否将非欧几里德图形建模为纯语言甚至欧几里德向量，同时保留其固有信息？非欧几里德性质一直是图形建模中的长期挑战。尽管最近的GNN和Graphformer努力将图形编码为欧几里德向量，但从向量中恢复出原始图形仍然是一个挑战。我们引入了GraphsGPT，它具有一个将非欧几里德图形转换为在欧几里德空间中可学习图形单词的Graph2Seq编码器，以及一个从图形单词重构原始图形以确保信息等价性的GraphGPT解码器。我们在100M个分子上预训练了GraphsGPT，并得到一些有趣的发现：(1) 预训练的Graph2Seq在图形表示学习方面表现出色，在8/9个图形分类和回归任务上取得了最新成果。(2) 预训练的GraphGPT作为一个强大的图形生成器，其能够进行无条件和有条件的图形生成。(3) Graph2Seq+Gr

    Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
    
[^115]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^116]: DiffStitch: 使用基于扩散的轨迹拼接提升离线强化学习

    DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching

    [https://arxiv.org/abs/2402.02439](https://arxiv.org/abs/2402.02439)

    DiffStitch是一种使用基于扩散的轨迹拼接提升离线强化学习的方法。它通过有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以提高离线强化学习算法的性能。

    

    在离线强化学习中，学习策略的性能高度依赖于离线数据集的质量。然而，在许多情况下，离线数据集只包含了非常有限的最佳轨迹，这给离线强化学习算法带来了挑战，因为智能体必须获得到达高奖励区域的能力。为了解决这个问题，我们引入了基于扩散的轨迹拼接（DiffStitch），这是一个新颖的基于扩散的数据增强流水线，它可以系统地生成轨迹之间的拼接转换。DiffStitch可以有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以解决离线强化学习算法所面临的挑战。在D4RL数据集上进行的实证实验表明，DiffStitch在各种强化学习方法中都具有有效性。值得注意的是，DiffStitch在一步方法（IQL）、模仿学习方法（TD3+BC）和轨迹方法（PPO）的性能方面都有显著的改进。

    In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje
    
[^117]: Uni-RLHF: 用于多样化人类反馈的强化学习通用平台和基准套件

    Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback

    [https://arxiv.org/abs/2402.02423](https://arxiv.org/abs/2402.02423)

    Uni-RLHF是一个通用的强化学习平台和基准套件，致力于处理多样化的人类反馈，解决了在RLHF中量化进展的挑战，并提供了用户友好的注释界面和离线基准实现。

    

    强化学习与人类反馈（RLHF）通过与人类偏好对齐，避免了昂贵的手动奖励设计，已经受到了广泛关注。考虑到不同环境中不同学习方法和多样化的人类反馈类型对RLHF的进步进行量化是具有挑战性的，因为缺乏标准化的注释平台和广泛使用的统一基准。为了填补这个空白，我们引入了Uni-RLHF，这是一个为RLHF量身定制的综合系统实现。它旨在提供一个完整的从真实人类反馈到实际问题发展的工作流。Uni-RLHF包含三个部分：1）通用的多反馈注释平台，2）大规模的众包反馈数据集，3）模块化的离线RLHF基准实现。Uni-RLHF开发了一个用户友好的注释界面，适用于各种反馈类型，并与主要的强化学习框架兼容。

    Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of main
    
[^118]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^119]: FreDF: 在频域中学习预测

    FreDF: Learning to Forecast in Frequency Domain

    [https://arxiv.org/abs/2402.02399](https://arxiv.org/abs/2402.02399)

    FreDF是一种在频域中学习预测的方法，解决了时间序列建模中标签序列的自相关问题，相比现有方法有更好的性能表现，并且与各种预测模型兼容。

    

    时间序列建模在历史序列和标签序列中都面临自相关的挑战。当前的研究主要集中在处理历史序列中的自相关问题，但往往忽视了标签序列中的自相关存在。具体来说，新兴的预测模型主要遵循直接预测（DF）范式，在标签序列中假设条件独立性下生成多步预测。这种假设忽视了标签序列中固有的自相关性，从而限制了基于DF的模型的性能。针对这一问题，我们引入了频域增强直接预测（FreDF），通过在频域中学习预测来避免标签自相关的复杂性。我们的实验证明，FreDF在性能上大大超过了包括iTransformer在内的现有最先进方法，并且与各种预测模型兼容。

    Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models. In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods including iTransformer and is compatible with a variety of forecast models.
    
[^120]: DeLLMa:一个用于大型语言模型下决策的框架

    DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models

    [https://arxiv.org/abs/2402.02392](https://arxiv.org/abs/2402.02392)

    DeLLMa是一个旨在提高不确定环境下决策精度的框架，通过多步骤的脚手架程序，借鉴决策理论和效用理论的原则，可以显著提高大型语言模型的决策性能。

    

    大型语言模型（LLMs）在商业、工程和医学等领域被广泛应用，这些领域往往面临决策不确定性的问题，这是一个关键但具有挑战性的任务。本文表明，在决策问题上直接使用LLMs往往效果较差，尤其是在问题复杂性增加时。为了克服这个限制，我们提出了DeLLMa（Decision-making Large Language Model assistant）框架，旨在提高不确定环境下的决策精度。DeLLMa包括一个多步骤的脚手架程序，借鉴了决策理论和效用理论的原则，提供了一个最优的、可审计的决策过程。我们在涉及真实农业和金融数据的决策环境中验证了我们的框架。结果表明，DeLLMa可以显著提高LLMs的决策性能，准确性可提高高达40%以上。

    Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
    
[^121]: 使用辅助验证的迭代上下文学习生成面向解决方案的基于Agent的模型

    Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning

    [https://arxiv.org/abs/2402.02388](https://arxiv.org/abs/2402.02388)

    本文提出了SAGE，一个通用的面向解决方案的ABM生成框架，利用辅助验证和迭代上下文学习，自动生成针对特定问题的模型和解决方案。

    

    基于Agent的模型（ABM）作为一种重要的范式，用于提出和验证针对复杂系统所提出的假设解决方案或政策，并实现各种目标。这个过程需要大量的工作和跨学科的专业知识。具有跨领域知识和编程能力的大型语言模型（LLM）有潜力减轻这个过程的困难。然而，LLM擅长处理序列信息，这对于分析ABM中的复杂交互和非线性动力学来说是具有挑战性的。另外，由于LLM缺乏自我评估能力，仅仅依靠LLM是无法有效完成这个过程的。在本文中，我们提出了SAGE，这是一个通用的面向解决方案的ABM生成框架，用于自动生成针对特定问题的模型和解决方案。

    Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural netw
    
[^122]: 通过分布协作路由增强计算与网络融合系统

    Empowering Computing and Networks Convergence System with Distributed Cooperative Routing

    [https://arxiv.org/abs/2402.02381](https://arxiv.org/abs/2402.02381)

    本文提出了一个分布式协作路由框架，用于增强计算与网络融合系统，以确保截止日期要求并最小化请求计算成本。

    

    智能应用的出现和计算与网络领域的最新进展推动了计算与网络融合（CNC）系统的发展。然而，现有研究未能实现对计算和网络资源的全面调度优化。这种不足导致了某些计算请求在端到端服务模式下无法保证需求，并对CNC系统的发展产生了负面影响。在本文中，我们提出了一种针对CNC系统的分布式协作路由框架，以确保截止日期要求并最小化请求的计算成本。该框架包括交易平面、管理平面、控制平面和转发平面。跨平面协作端到端路由方案在制定路由计划时考虑了异构服务器的计算效率和网络拥塞程度，从而确定了在何处执行请求和相应的路由方案。

    The emergence of intelligent applications and recent advances in the fields of computing and networks are driving the development of computing and networks convergence (CNC) system. However, existing researches failed to achieve comprehensive scheduling optimization of computing and network resources. This shortfall results in some requirements of computing requests unable to be guaranteed in an end-to-end service pattern, negatively impacting the development of CNC systems. In this article, we propose a distributed cooperative routing framework for the CNC system to ensure the deadline requirements and minimize the computation cost of requests. The framework includes trading plane, management plane, control plane and forwarding plane. The cross-plane cooperative end-to-end routing schemes consider both computation efficiency of heterogeneous servers and the network congestion degrees while making routing plan, thereby determining where to execute requests and corresponding routing pat
    
[^123]: 在分析课堂对话时评估大型语言模型

    Evaluating Large Language Models in Analysing Classroom Dialogue

    [https://arxiv.org/abs/2402.02380](https://arxiv.org/abs/2402.02380)

    本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。

    

    本研究探讨了大型语言模型（LLM），特别是GPT-4，在分析课堂对话中的应用，这是教学诊断和质量改进的重要研究任务。鉴于传统教育研究中知识密集和劳动密集的定性方法，本研究调查了LLM在优化和增强分析过程方面的潜力。该研究涉及中学的数据集，包括数学和语文课堂上的对话。这些对话由教育专家手动编码，然后使用定制的GPT-4模型进行分析。本研究侧重于比较手动注释与GPT-4的输出，以评估其在分析教育对话方面的效果。评估时间效率、编码者间一致性和编码者间可靠性之间的差异。结果表明，使用GPT-4可以显著节省时间，并在编码一致性方面具有很高的一致性。

    This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
    
[^124]: 探索医学图像的内在属性，用于自监督的二进制语义分割

    Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation

    [https://arxiv.org/abs/2402.02367](https://arxiv.org/abs/2402.02367)

    本文介绍一种专为医学图像分割量身定制的自监督框架MedSASS，通过对比实验证明其在医学数据集上的优越性能，并展示了在端到端训练下的显著改进。

    

    自监督学习的最新进展已经解锁了利用无标签数据进行辅助任务学习有益先验知识的潜力。这在医学图像分析等标注数据稀缺的领域尤为有利。尽管对于分类任务有效，但这种方法在更复杂的应用，如医学图像分割中存在局限性。本文介绍一种专为医学图像分割量身定制的自监督框架，名为Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS)。通过对四个不同的医学数据集进行评估，展示了MedSASS相对现有最先进方法的优越性。MedSASS的性能优于现有的基于CNN的自监督方法3.83％，并且与基于ViT的方法性能相匹配。此外，在覆盖编码器和解码器的端到端训练下，MedSASS显示出显著的改进。

    Recent advancements in self-supervised learning have unlocked the potential to harness unlabeled data for auxiliary tasks, facilitating the learning of beneficial priors. This has been particularly advantageous in fields like medical image analysis, where labeled data are scarce. Although effective for classification tasks, this methodology has shown limitations in more complex applications, such as medical image segmentation. In this paper, we introduce Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS), a dedicated self-supervised framework tailored for medical image segmentation. We evaluate MedSASS against existing state- of-the-art methods across four diverse medical datasets, showcasing its superiority. MedSASS outperforms existing CNN-based self-supervised methods by 3.83% and matches the performance of ViT-based methods. Furthermore, when MedSASS is trained end-to-end, covering both encoder and decoder, it demonstrates significant improvements o
    
[^125]: 在上下文中学习的发展景观

    The Developmental Landscape of In-Context Learning

    [https://arxiv.org/abs/2402.02364](https://arxiv.org/abs/2402.02364)

    在transformers模型中，我们展示了在上下文学习中的离散发展阶段，并引入了两种方法来检测这些阶段的关键里程碑。我们使用行为和结构度量验证了这些方法的有效性。

    

    我们展示了在transformers中，当它们通过语言建模或线性回归任务进行训练时，上下文学习是如何以离散的发展阶段出现的。我们引入了两种方法来检测分隔这些阶段的关键里程碑，通过探测参数空间和函数空间中种群损失的几何特征。我们使用一系列行为和结构度量研究这些新方法揭示的阶段，以建立它们的有效性。

    We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
    
[^126]: 神经网络内部的对称性统一：Transformer, 前馈和神经ODE

    Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE

    [https://arxiv.org/abs/2402.02362](https://arxiv.org/abs/2402.02362)

    本研究通过应用物理学中的规范对称性原理，将其应用于神经网络架构，发现各种机器学习模型的参数冗余可以解释为规范对称性。并证明了前馈神经网络中的参数冗余在神经ODE中升级为微分同胚，并找到了Transformer模型与神经ODE及其规范对称性的自然对应关系。

    

    理解神经网络内部运作，包括transformers，仍然是机器学习中最具挑战性的难题之一。本研究将物理学中的规范对称性原理应用于神经网络架构，提出了一种新颖的方法。通过将模型函数视为物理可观测量，发现各种机器学习模型的参数冗余可以解释为规范对称性。我们在神经ODE中数学形式化了参数冗余，并发现它们的规范对称性由时空微分同胚给出，这在爱因斯坦的引力理论中起着基础性作用。我们将神经ODE视为连续版本的前馈神经网络，证明了前馈神经网络中的参数冗余确实在神经ODE中升级为微分同胚。我们进一步将分析扩展到Transformer模型，找到了与神经ODE及其规范对称性的自然对应关系。

    Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The
    
[^127]: Stereographic Spherical Sliced Wasserstein Distances - 应用于球形概率分布比较的立体投影球面切片瓦瑟斯坦距离

    Stereographic Spherical Sliced Wasserstein Distances

    [https://arxiv.org/abs/2402.02345](https://arxiv.org/abs/2402.02345)

    本文提出了一种快速且高度并行的用于比较球形测度的距离，使用了立体投影和广义Radon变换，称之为立体投影球面切片瓦瑟斯坦（S3W）距离。通过仔细处理立体投影引起的距离畸变，并进行了理论分析，证明了该方法在速度和效果上的优势。

    

    在地质学、医学领域、计算机视觉和深度表示学习等各个领域，比较球形概率分布是非常重要的。基于最优传输的距离，比如瓦瑟斯坦距离，对于比较概率测度已经引发了活跃的研究，以开发计算效率高的球形概率测度的变体。本文介绍了一种高速且高度并行化的用于比较球形测度的距离，使用了立体投影和广义Radon变换，我们称之为立体投影球面切片瓦瑟斯坦（S3W）距离。我们仔细处理了立体投影引起的距离畸变，并对我们提出的度量及其具有旋转不变性的变体进行了广泛的理论分析。最后，我们评估了所提出的度量的性能，并将其与最近的基线进行了比较，从遥感和处理效率两个方面进行了评估。

    Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both spe
    
[^128]: MetaOptimize：一个优化步长和其他元参数的框架

    MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters

    [https://arxiv.org/abs/2402.02342](https://arxiv.org/abs/2402.02342)

    MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。

    

    本文解决了机器学习算法中优化元参数（即超参数）的挑战，这是影响训练效率和模型性能的关键因素。我们引入了MetaOptimize框架，摆脱了计算昂贵的传统元参数搜索方法，通过动态调整元参数，特别是步长（也称为学习率），来训练模型。具体而言，MetaOptimize可以适用于任何一阶优化算法，在训练过程中实时调整步长，通过未来损失的折现总和来最小化一种特定形式的遗憾。我们还介绍了MetaOptimize的低复杂度变体，结合其适应多个优化算法的能力，展示了在各种机器学习应用中与手工设计的学习率计划相媲美的性能。

    This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
    
[^129]: 不确定性感知的3D人体姿势估计测试时间优化

    Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation

    [https://arxiv.org/abs/2402.02339](https://arxiv.org/abs/2402.02339)

    本文提出了一种不确定性感知的测试时间优化（UAO）框架，通过量化关节点的不确定性来缓解过拟合问题，提高3D人体姿势估计的性能。

    

    尽管数据驱动方法在3D人体姿势估计方面取得了成功，但它们常常受到域间差异的限制，表现出有限的泛化能力。相比之下，基于优化的方法在特定情况下进行微调方面表现优秀，但整体表现通常不如数据驱动方法。我们观察到先前的基于优化的方法通常依赖于投影约束，这仅仅确保了在2D空间中的对齐，可能导致过拟合问题。为了解决这个问题，我们提出了一种不确定性感知的测试时间优化 (UAO) 框架，它保留了预训练模型的先验信息，并利用关节点的不确定性来缓解过拟合问题。具体而言，在训练阶段，我们设计了一个有效的2D到3D网络，用于估计相应的3D姿势，并量化每个3D关节点的不确定性。对于测试时的优化，所提出的优化框架冻结预训练模型，并仅优化少量关键参数，以提高性能。

    Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a 
    
[^130]: 深度表格学习需要算术特征交互

    Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning

    [https://arxiv.org/abs/2402.02334](https://arxiv.org/abs/2402.02334)

    本文研究了深度表格学习中算术特征交互的必要性，通过引入AMFormer模型，实现了在细粒度表格数据建模、训练数据效率和泛化方面的优越性能。

    

    直到最近，关于深度模型在表格数据上的有效归纳偏见的问题仍然没有答案。本文调查了算术特征交互对于深度表格学习的必要性假设。为了测试这一观点，我们创建了一个具有轻微特征交互假设的合成表格数据集，并研究了一种改进的Transformer架构，使其能够进行算术特征交互，称为AMFormer。结果显示，AMFormer在细粒度表格数据建模、训练数据效率和泛化方面优于强对手。这归因于其并行的加性和乘性注意力操作符和基于提示的优化，这有助于在具有算术工程特征的扩展空间中分离表格样本。我们在真实世界数据上进行了广泛的实验，也验证了AMFormer的一致有效性、效率和合理性，表明它已经建立了强有力的归纳能力。

    Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive
    
[^131]: 在狼人游戏中增强大型语言模型的推理能力

    Enhance Reasoning for Large Language Models in the Game Werewolf

    [https://arxiv.org/abs/2402.02330](https://arxiv.org/abs/2402.02330)

    本文提出了一个创新的框架，将大型语言模型（LLM）与外部思考模块相结合，以增强推理能力，通过引入通信协议和使用大量数据进行训练，展示了其在游戏推理、语音生成和在线评估方面的有效性。

    

    本文提出了一个创新的框架，将大型语言模型（LLM）与外部思考模块相结合，以增强基于LLM的代理的推理能力。与通过prompt工程增加LLM不同，思考者直接利用数据库中的知识，并采用各种优化技术。该框架形成了一个推理层次结构，在其中LLM处理直观的系统1任务，如自然语言处理，而思考者专注于需要复杂的逻辑分析和领域特定知识的认知系统2任务。我们以需要双系统推理的9人狼人游戏为例介绍了该框架。我们引入了LLM和思考者之间的通信协议，并使用来自18800个人类会话和强化学习的数据训练了思考者。实验证明了该框架在演绎推理、语音生成和在线游戏评估方面的有效性。此外，我们通过微调6B LLM，超越了GPT4。

    This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when
    
[^132]: 通过修正的缩放定律选择大型语言模型进行微调

    Selecting Large Language Model to Fine-tune via Rectified Scaling Law

    [https://arxiv.org/abs/2402.02314](https://arxiv.org/abs/2402.02314)

    该论文研究了在资源受限的情况下如何选择合适的预训练语言模型进行微调的问题。通过引入修正的缩放定律和预学习数据大小的概念，作者提出了一种新颖的模型选择算法，可以选择接近最优的模型。

    

    在日益增长的语言模型生态系统中，在众多选项中选择最合适的预训练模型进行微调成为了一个挑战。在资源受限的情况下，微调所有模型然后再进行选择是不现实的。在本文中，我们将这个资源受限的选择任务转化为预测微调性能，并且展示其与缩放定律之间的自然联系。与预训练不同，我们发现微调的缩放曲线不仅包括众所周知的“功率阶段”，还包括以前未被观察到的“预功率阶段”。我们还解释了为什么现有的缩放定律无法理论和实证地捕捉到这种相变现象。为了解决这个问题，我们将“预学习数据大小”概念引入到我们的修正缩放定律中，这克服了理论上的限制，并更好地适应实验结果。通过利用我们的定律，我们提出了一种新颖的语言模型选择算法，可以选择接近最优的模型。

    The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
    
[^133]: 图机器学习基础的未来方向

    Future Directions in Foundations of Graph Machine Learning

    [https://arxiv.org/abs/2402.02287](https://arxiv.org/abs/2402.02287)

    图机器学习领域的未来方向应该是发展一个更加均衡的理论，从更完整的角度探究图神经网络的表达能力、泛化和优化之间的相互关系。

    

    随着图数据在不同学科（从生命科学到社会科学和工程科学）上的广泛应用，图机器学习，尤其是使用图神经网络（GNNs），引起了人们浓厚的兴趣。尽管在实际应用中取得了成功，但我们对GNNs性质的理论理解仍然非常不完整。最近的理论发展主要集中在阐明GNNs粗粒度表达能力方面，主要采用组合技巧。然而，这些研究与实践并不完全一致，特别是在使用随机一阶优化技术训练GNNs时，对GNNs的泛化行为的理解。在这篇定位论文中，我们认为图机器学习领域需要将注意力转移到发展一个更加均衡的图机器学习理论上来，重点关注表达能力、泛化和优化的相互关系的更全面的理解。

    Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
    
[^134]: 多级特征聚合和递归对齐网络用于实时语义分割

    Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation

    [https://arxiv.org/abs/2402.02286](https://arxiv.org/abs/2402.02286)

    该论文提出了一种在实时推理速度下实现高分割准确性的多级特征聚合和递归对齐网络。使用ResNet-18作为骨干，通过多级特征聚合模块和递归对齐模块来提高模型性能。

    

    实时语义分割对于实际应用非常重要。然而，许多方法都着重于降低计算复杂性和模型大小，但同时牺牲了准确性。在一些场景下，如自主导航和驾驶员辅助系统，准确性和速度同样重要。为了解决这个问题，我们提出了一种新颖的多级特征聚合和递归对齐网络（MFARANet），旨在实现高分割准确性和实时推理速度。我们使用ResNet-18作为骨干来保证效率，并提出了三个核心组件来弥补浅骨干引起的模型容量减少。具体而言，我们首先设计多级特征聚合模块（MFAM），将编码器中的分层特征聚合到每个尺度，以便于后续的空间对齐和多尺度推理。然后，我们通过结合基于流的对齐来建立递归对齐模块（RAM）。

    Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
    
[^135]: SynthDST: 少样本对话状态跟踪所需的全部是合成数据

    SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking

    [https://arxiv.org/abs/2402.02285](https://arxiv.org/abs/2402.02285)

    SynthDST是一个针对对话状态跟踪设计的数据生成框架，利用合成数据来实现少样本提示，通过使用少量手工对话模板和对话模式，它能够生成自然、连贯和流畅的带有DST注释的对话，并使Join连通率提升4-5％.

    

    在上下文学习中，大型语言模型（LLM）已成为对话状态跟踪（DST）研究的一个有希望的方向。然而，表现最好的上下文学习方法涉及检索和添加类似的示例到提示中，需要访问标记的训练数据。在多个领域和应用中获取这样的训练数据非常耗时、昂贵，有时是不可行的。虽然零样本学习不需要训练数据，但在少样本设置中明显落后。因此，“我们是否可以为任何对话模式有效地生成合成数据，以实现少样本提示？”针对这个问题，我们提出了一个名为\method的数据生成框架，专门针对DST，利用LLM。我们的方法只需要对话模式和一些手工对话模板，就能合成自然、连贯和流畅的带有DST注释的对话。使用{\method}的少样本学习结果显示，Join连通率提升了4-5％。

    In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Join
    
[^136]: InceptionCapsule: 采用自注意力机制的Inception-Resnet和CapsuleNet用于医学图像分类

    InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention for medical image Classification

    [https://arxiv.org/abs/2402.02274](https://arxiv.org/abs/2402.02274)

    InceptionCapsule方法采用了自注意力机制、迁移学习和Inception-ResNet模型，在医学图像分类中解决了初始权重选择和特征提取的问题，取得了较高的准确率。

    

    在深度神经网络中，初始权重的选择对于网络的性能至关重要，因为随机选择的权重可能产生不同的输出，增加过拟合和欠拟合的概率。另一方面，基于向量的特征提取方法需要丰富的向量以实现更准确的分类。本文提出了InceptionCapsule方法来解决这两个问题。该方法利用迁移学习和Inception-ResNet模型，避免随机选择权重，从ImageNet中获取初始权重。同时，利用Inception中间层的输出生成丰富的向量。提取的向量被传递给装备有注意力机制的胶囊网络进行学习。使用Kvasir数据集和BUSI with GT数据集对这种方法进行了评估。该模型在5类分类上实现了97.62%的准确率，在8类分类上实现了94.30%的准确率。

    Initial weighting is significant in deep neural networks because the random selection of weights produces different outputs and increases the probability of overfitting and underfitting. On the other hand, vector-based approaches to extract vector features need rich vectors for more accurate classification. The InceptionCapsule approach is presented to alleviate these two problems. This approach uses transfer learning and the Inception-ResNet model to avoid random selection of weights, which takes initial weights from ImageNet. It also uses the output of Inception middle layers to generate rich vectors. Extracted vectors are given to a capsule network for learning, which is equipped with an attention technique. Kvasir data and BUSI with the GT dataset were used to evaluate this approach. This model was able to achieve 97.62 accuracies in 5-class classification and also achieved 94.30 accuracies in 8-class classification on Kvasir. In the BUSI with GT dataset, the proposed approach achi
    
[^137]: 具有新知识的联邦学习: 基础，进展和未来展望

    Federated Learning with New Knowledge: Fundamentals, Advances, and Futures

    [https://arxiv.org/abs/2402.02268](https://arxiv.org/abs/2402.02268)

    本文系统地探讨了具有新知识的联邦学习，包括如何有效地融入新特征、任务、模型和算法，并分析了新知识到达的形式和时间对纳入过程的影响。同时讨论了联邦学习具有新知识时的潜在未来发展方向。

    

    联邦学习（FL）是一种隐私保护的分布式学习方法，在隐私保护日益重视的时代迅速发展。正是这种快速发展趋势，以及现实世界中对联邦学习的不断新需求的出现，促使我们专注于一个非常重要的问题：具有新知识的联邦学习。主要挑战在于如何将各种新知识有效地融入现有的FL系统中，并使这些系统降低成本，延长寿命，并促进可持续发展。在本文中，我们系统地定义了FL中新知识的主要来源，包括新特征、任务、模型和算法。对于每个来源，我们深入分析和讨论如何将新知识纳入现有的FL系统，并研究新知识到达的形式和时间对纳入过程的影响。此外，我们全面讨论了联邦学习具有新知识时的潜在未来发展方向。

    Federated Learning (FL) is a privacy-preserving distributed learning approach that is rapidly developing in an era where privacy protection is increasingly valued. It is this rapid development trend, along with the continuous emergence of new demands for FL in the real world, that prompts us to focus on a very important problem: Federated Learning with New Knowledge. The primary challenge here is to effectively incorporate various new knowledge into existing FL systems and evolve these systems to reduce costs, extend their lifespan, and facilitate sustainable development. In this paper, we systematically define the main sources of new knowledge in FL, including new features, tasks, models, and algorithms. For each source, we thoroughly analyze and discuss how to incorporate new knowledge into existing FL systems and examine the impact of the form and timing of new knowledge arrival on the incorporation process. Furthermore, we comprehensively discuss the potential future directions for
    
[^138]: MixedNUTS: 通过非线性混合分类器实现无需训练的准确性和鲁棒性平衡

    MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers

    [https://arxiv.org/abs/2402.02263](https://arxiv.org/abs/2402.02263)

    MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。

    

    鲁棒性往往牺牲了准确性，阻碍了鲁棒分类模型在实际应用中的使用。基于训练的解决方案在与已训练的大型高性能模型兼容性方面存在限制，因此需要探索无需训练的集成方法。我们观察到鲁棒模型在干净数据和对抗数据上的正确预测比错误预测更自信，我们推测通过增强这种“良性置信度特性”可以在集成环境中实现准确性和鲁棒性的平衡。为了实现这一点，我们提出了“MixedNUTS”，一种无需训练的方法，利用仅有三个参数的非线性转换来处理鲁棒分类器和标准非鲁棒分类器的输出Logits，并通过高效算法进行优化。然后，MixedNUTS将转换后的Logits转换为概率，并将它们混合作为最终的输出。在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验。

    Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
    
[^139]: 数据质量很重要：使用RoBERTa-CNN模型在社交媒体帖子中检测自杀意图

    Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model

    [https://arxiv.org/abs/2402.02262](https://arxiv.org/abs/2402.02262)

    本文介绍了一种使用RoBERTa-CNN模型来在社交媒体帖子中检测自杀意图的新方法。RoBERTa-CNN通过在RoBERTa模型中添加卷积神经网络（CNN）层，提高了对重要模式的捕捉能力，并在实验证明在自杀和抑郁检测数据集上表现出良好的准确性。

    

    自杀仍然是全球健康领域的一个关注焦点，急需创新方法进行早期检测和干预。本文着重于识别SuicideWatch Reddit帖子中的自杀意图，并提出了一种使用尖端的RoBERTa-CNN模型进行自杀检测的新方法，RoBERTa-CNN是RoBERTa（鲁棒性优化BERT方法）的一种变体。RoBERTa被用于各种自然语言处理（NLP）任务，包括文本分类和情感分析。RoBERTa的有效性在于它能够捕捉文本信息并形成文本之间的语义关系。通过在原始模型中添加卷积神经网络（CNN）层，RoBERTa增强了从庞大数据集中捕捉重要模式的能力。我们在自杀和抑郁检测数据集上评估了RoBERTa-CNN，并获得了可靠的结果，例如，RoBERTa-CNN在平均准确率上获得了98％，标准差为...

    Suicide remains a global health concern for the field of health, which urgently needs innovative approaches for early detection and intervention. In this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit posts and present a novel approach to suicide detection using the cutting-edge RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach). RoBERTa is used for various Natural Language Processing (NLP) tasks, including text classification and sentiment analysis. The effectiveness of the RoBERTa lies in its ability to capture textual information and form semantic relationships within texts. By adding the Convolution Neural Network (CNN) layer to the original model, the RoBERTa enhances its ability to capture important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we experimented on the Suicide and Depression Detection dataset and obtained solid results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard deviation (ST
    
[^140]: XTSFormer: 跨时空尺度的Transformer用于不规则时间事件预测

    XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction

    [https://arxiv.org/abs/2402.02258](https://arxiv.org/abs/2402.02258)

    XTSFormer是一个用于不规则时间事件预测的跨时空尺度的Transformer模型，通过新颖的循环感知时间位置编码和分层的多尺度时间注意机制来解决不规则时间间隔、循环、周期性和多尺度事件交互等挑战。

    

    事件预测旨在基于历史事件序列预测未来事件的时间和类型。尽管其重要性，但存在几个挑战，包括连续事件之间时间间隔的不规则性、循环、周期性和多尺度事件交互，以及长事件序列的高计算成本。现有的神经时间点过程（TPP）方法不能捕捉事件交互的多尺度特性，而这在许多实际应用中（如临床事件数据）很常见。为了解决这些问题，我们提出了跨时空尺度的Transformer（XTSFormer），特别适用于不规则时间事件数据。我们的模型包含两个关键组成部分：一种新颖的基于特征的循环感知时间位置编码（FCPE），能够灵活捕捉时间的循环性质，以及一个分层的多尺度时间注意机制。这些尺度由自底向上的聚类算法确定。

    Event prediction aims to forecast the time and type of a future event based on a historical event sequence. Despite its significance, several challenges exist, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, and multi-scale event interactions, as well as the high computational costs for long event sequences. Existing neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data. Our model comprises two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism. These scales are determined by a bottom-up clustering algorithm. Extens
    
[^141]: ExTTNet:一种用于从发票图像中提取表格文字的深度学习算法

    ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images

    [https://arxiv.org/abs/2402.02246](https://arxiv.org/abs/2402.02246)

    ExTTNet是一种深度学习算法，能够自动从发票图像中提取表格文字。使用光学字符识别（OCR）技术获取文本，并通过特征提取方法提高准确度。通过多层神经网络模型进行训练，最终获得了0.92的F1分数。

    

    在这项工作中，通过一个被称为ExTTNet的深度学习模型，自动地从发票图像中提取产品表格。首先使用光学字符识别（OCR）技术从发票图像中获取文本。在此过程中，使用了Tesseract OCR引擎 [37]。然后，通过使用特征提取方法增加现有特征的数量来提高准确度。根据每个OCR获得的文本是否是表格元素，进行标记处理。在本研究中，使用了多层人工神经网络模型进行训练。使用Nvidia RTX 3090显卡进行训练，耗时162分钟。训练的结果，F1分数为0.92。

    In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.
    
[^142]: 带有差分隐私的联邦学习

    Federated Learning with Differential Privacy

    [https://arxiv.org/abs/2402.02230](https://arxiv.org/abs/2402.02230)

    本论文研究了带有差分隐私的联邦学习对模型性能的影响，发现在分布式和差分隐私设置下，非独立同分布和小型数据集的性能下降最为严重。

    

    联邦学习作为一种分布式机器学习方法，能够显著保护客户的私密数据，避免在不同参与方之间共享。然而，通过分析来自客户端上传的参数权重，私密信息仍然可能泄露。在本报告中，我们展示了关于客户数量和差分隐私机制对不同数据类型模型性能的实证基准测试。我们的结果表明，在分布式和差分隐私设置下，非独立同分布和小型数据集的性能下降最为严重。

    Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving client's private data from being shared among different parties. Nevertheless, private information can still be divulged by analyzing uploaded parameter weights from clients. In this report, we showcase our empirical benchmark of the effect of the number of clients and the addition of differential privacy (DP) mechanisms on the performance of the model on different types of data. Our results show that non-i.i.d and small datasets have the highest decrease in performance in a distributed and differentially private setting.
    
[^143]: 非洲的机器智能：一项调查

    Machine Intelligence in Africa: a survey

    [https://arxiv.org/abs/2402.02218](https://arxiv.org/abs/2402.02218)

    本文从多层多尺度和文化意识伦理的角度总结了非洲机器智能的最新发展，展示了MI在54个非洲国家的应用案例，强调了非洲的音频数据集未被充分利用、先进的MI模型缺乏文化意识的问题。

    

    在过去的5年里，非洲国家中大规模音频数据集的可用性为构建距离人们更近、使用本地语言进行交流、学习、理解和开展业务的机器智能（MI）技术提供了无限机会，包括那些无法读写的人。然而，这些音频数据集并没有被现有的MI工具充分利用，使得许多非洲人错失了MI业务机会。此外，许多先进的MI模型并没有意识到文化差异，其采纳指数的伦理问题也值得质疑。这一缺失在非洲的许多应用中是一个重大障碍。本文以多层多尺度和文化意识伦理的视角总结了非洲机器智能的最新发展，通过400篇关于MI研究、产业、政府行动以及艺术、音乐、非正式经济和非洲小企业应用的文章，展示了MI在54个非洲国家的应用案例。

    In the last 5 years, the availability of large audio datasets in African countries has opened unlimited opportunities to build machine intelligence (MI) technologies that are closer to the people and speak, learn, understand, and do businesses in local languages, including for those who cannot read and write. Unfortunately, these audio datasets are not fully exploited by current MI tools, leaving several Africans out of MI business opportunities. Additionally, many state-of-the-art MI models are not culture-aware, and the ethics of their adoption indexes are questionable. The lack thereof is a major drawback in many applications in Africa. This paper summarizes recent developments in machine intelligence in Africa from a multi-layer multiscale and culture-aware ethics perspective, showcasing MI use cases in 54 African countries through 400 articles on MI research, industry, government actions, as well as uses in art, music, the informal economy, and small businesses in Africa. The surv
    
[^144]: 使用深度学习技术进行糖尿病检测的过采样和特征增强

    Diabetes detection using deep learning techniques with oversampling and feature augmentation

    [https://arxiv.org/abs/2402.02188](https://arxiv.org/abs/2402.02188)

    本研究提出了一种使用深度学习技术、过采样和特征增强的方法来进行糖尿病检测。通过对Pima印地安人糖尿病数据库进行评估，实现了92.31%的准确率。

    

    背景和目标：糖尿病是一种慢性病，多年来影响越来越多的人。每年有大量的死亡案例。此外，许多患有糖尿病的人没有及时意识到自己的健康状况的严重性。延迟诊断会导致许多健康问题和大量的死亡案例，因此开发早期诊断此病的方法至关重要。方法：本文提出了一种基于深度学习技术的流程，用于预测糖尿病患者。它包括使用变分自动编码器（VAE）进行数据增强，使用稀疏自动编码器（SAE）进行特征增强和使用卷积神经网络进行分类。评估了Pima印地安人糖尿病数据库，其中包括患者的怀孕次数、血糖或胰岛素水平、血压或年龄等信息。

    Background and objective: Diabetes is a chronic pathology which is affecting more and more people over the years. It gives rise to a large number of deaths each year. Furthermore, many people living with the disease do not realize the seriousness of their health status early enough. Late diagnosis brings about numerous health problems and a large number of deaths each year so the development of methods for the early diagnosis of this pathology is essential.   Methods: In this paper, a pipeline based on deep learning techniques is proposed to predict diabetic people. It includes data augmentation using a variational autoencoder (VAE), feature augmentation using an sparse autoencoder (SAE) and a convolutional neural network for classification. Pima Indians Diabetes Database, which takes into account information on the patients such as the number of pregnancies, glucose or insulin level, blood pressure or age, has been evaluated.   Results: A 92.31% of accuracy was obtained when CNN class
    
[^145]: 进化引导的生成流网络

    Evolution Guided Generative Flow Networks

    [https://arxiv.org/abs/2402.02186](https://arxiv.org/abs/2402.02186)

    本文提出了进化引导的生成流网络（EGFN），用于处理长时间跨度和稀疏奖励的挑战。通过使用进化算法训练一组代理参数，并将结果轨迹存储在优先回放缓冲区中，我们的方法在训练GFlowNets代理时展现出了很高的效果。

    

    生成流网络（GFlowNets）是一类概率生成模型，用于学习按照奖励比例对组合对象进行采样。GFlowNets的一个重大挑战是在处理长时间跨度和稀疏奖励时有效训练它们。为了解决这个问题，我们提出了进化引导的生成流网络（EGFN），这是对GFlowNets训练的一种简单但强大的增强方法，使用进化算法（EA）进行训练。我们的方法可以在任何GFlowNets训练目标的基础上工作，通过使用EA训练一组代理参数，将结果轨迹存储在优先回放缓冲区中，并使用存储的轨迹训练GFlowNets代理。我们在广泛的玩具和真实世界基准任务上进行了深入研究，展示了我们方法在处理长轨迹和稀疏奖励方面的有效性。

    Generative Flow Networks (GFlowNets) are a family of probabilistic generative models that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.
    
[^146]: 使用全卷积神经网络在非固定长度的音频中进行情感分析

    Sentiment analysis in non-fixed length audios using a Fully Convolutional Neural Network

    [https://arxiv.org/abs/2402.02184](https://arxiv.org/abs/2402.02184)

    提出了一种使用全卷积神经网络在非固定长度的音频上进行情感分析的方法，使用Mel频谱图和Mel频率倒谱系数作为音频描述方法，并在多个数据集上验证了其有效性。

    

    本文提出了一种能够接受任意长度音频的情感分析方法，不需要事先固定长度。该方法使用Mel频谱图和Mel频率倒谱系数作为音频描述方法，并提出了一种全卷积神经网络架构作为分类器。通过使用EMODB、RAVDESS和TESS三个知名数据集进行验证，结果表明该方法的表现优于现有最先进方法。此外，由于该方法可以接受任意大小的音频，因此可以实时进行情感分析，这对于呼叫中心、医疗咨询或金融经纪等多个领域非常有意义。

    In this work, a sentiment analysis method that is capable of accepting audio of any length, without being fixed a priori, is proposed. Mel spectrogram and Mel Frequency Cepstral Coefficients are used as audio description methods and a Fully Convolutional Neural Network architecture is proposed as a classifier. The results have been validated using three well known datasets: EMODB, RAVDESS, and TESS. The results obtained were promising, outperforming the state-of-the-art methods. Also, thanks to the fact that the proposed method admits audios of any size, it allows a sentiment analysis to be made in near real time, which is very interesting for a wide range of fields such as call centers, medical consultations, or financial brokers.
    
[^147]: 基于本体的多领域社交网络分析模型：实验验证和案例研究

    An Ontology-Based multi-domain model in Social Network Analysis: Experimental validation and case study

    [https://arxiv.org/abs/2402.02181](https://arxiv.org/abs/2402.02181)

    本研究提出了一个基于本体的多领域知识模型，能够自动进行社交网络分析，避免错误，并获得与专家相同的结论。通过一个真实案例研究，展示了该方法的优势。

    

    近年来，社交网络理论和分析方法已被应用于不同的领域，包括公共卫生。进行社交网络分析(SNA)的完整流程是一项耗时任务，其中专家可能会犯错误。本研究提出了一个多领域知识模型，能够自动收集数据并在不同领域进行不同的社交网络分析，避免错误，并获得与SNA专家相同的结论。该模型以一个名为OntoSNAQA的本体表示，该本体由表示People、Questionnaires和Social Network Analysis领域的类、属性和规则组成。除了本体本身，还通过SWRL和SPARQL查询来表示不同的规则。利用OntoSNAQA创建了一个基于知识的系统，并应用于一个真实案例研究以展示该方法的优势。

    The use of social network theory and methods of analysis have been applied to different domains in recent years, including public health. The complete procedure for carrying out a social network analysis (SNA) is a time-consuming task that entails a series of steps in which the expert in social network analysis could make mistakes. This research presents a multi-domain knowledge model capable of automatically gathering data and carrying out different social network analyses in different domains, without errors and obtaining the same conclusions that an expert in SNA would obtain. The model is represented in an ontology called OntoSNAQA, which is made up of classes, properties and rules representing the domains of People, Questionnaires and Social Network Analysis. Besides the ontology itself, different rules are represented by SWRL and SPARQL queries. A Knowledge Based System was created using OntoSNAQA and applied to a real case study in order to show the advantages of the approach. F
    
[^148]: 跨域动态链接预测的一种图模型

    One Graph Model for Cross-domain Dynamic Link Prediction

    [https://arxiv.org/abs/2402.02168](https://arxiv.org/abs/2402.02168)

    DyExpert是一种用于跨域链接预测的动态图模型，通过明确建模历史演化过程并结合链接预测，它可以学习特定下游图的演化模式，并在各个领域上取得了最先进的性能。

    

    本研究提出了DyExpert，一种用于跨域链接预测的动态图模型。它可以明确地建模历史演化过程，学习特定下游图的演化模式，并进而进行特定模式的链接预测。DyExpert采用了解码器优化的transformer，并通过结合演化建模和链接预测的“条件链接生成”实现了高效的并行训练和推断。DyExpert在包含6百万个动态边的广泛动态图上进行训练。在八个未训练的图上进行了大量实验，结果显示DyExpert在跨域链接预测中取得了最先进的性能。与相同设置下的先进基准相比，DyExpert在八个图上的平均精确度提高了11.40％。更令人印象深刻的是，在六个未训练的图上，它超过了八个先进基线的全监督性能。

    This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs.
    
[^149]: Vi(E)va LLM！一个用于评估和解释生成AI可视化的概念模型栈

    Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations

    [https://arxiv.org/abs/2402.02167](https://arxiv.org/abs/2402.02167)

    这篇论文介绍了一个名为EvaLLM的概念模型栈，用于评估和解释基于生成AI的可视化。它解决了使用大型语言模型生成可视化时遇到的问题，并提出了一种理论评估的方法。

    

    自动生成可视化是一项古老的任务，多年来越来越受到研究和实践社区的关注。最近，大型语言模型（LLM）已成为支持与可视化相关的生成任务的有趣选择，并展示了初步的有希望的结果。与此同时，存在诸多问题，如指导LLM生成所需结果的多种方式，引导生成的不同视角（基于代码、基于图像、基于语法），以及即使在可视化生成任务中也存在的幻觉，使得它们的使用不如预期的那样可行。在类似为LLM进行基准测试的倡议下，本文针对通过LLM对生成的可视化建模的评估问题进行了研究。我们提出了一个理论上的评估模型栈，EvaLLM，它将评估工作分解为其原子组成部分，并对其性质进行了描述，并概述了如何进行评估的概述。

    The automatic generation of visualizations is an old task that, through the years, has shown more and more interest from the research and practitioner communities. Recently, large language models (LLM) have become an interesting option for supporting generative tasks related to visualization, demonstrating initial promising results. At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected. Following similar initiatives for benchmarking LLMs, this paper copes with the problem of modeling the evaluation of a generated visualization through an LLM. We propose a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort in its atomic components, characterizes their nature, and provides an overview of how
    
[^150]: TSIS: t-SMILES的补充算法用于基于片段的分子表示

    TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular Representation

    [https://arxiv.org/abs/2402.02164](https://arxiv.org/abs/2402.02164)

    本研究引入了TSIS算法作为t-SMILES的补充，用于改进基于字符串的分子表示方法。实验证明，TSIS模型在处理语法中的长期依赖性方面表现优于其他模型。

    

    字符串基本的分子表示方法，如SMILES，在线性表示分子信息方面是事实上的标准。然而，必须使用配对符号和解析算法导致了长的语法依赖关系，使得即使是最先进的深度学习模型也难以准确理解语法和语义。尽管DeepSMILES和SELFIES已经解决了某些限制，但它们仍然在处理高级语法方面存在困难，使得一些字符串难以阅读。本研究引入了一个补充算法TSIS（TSID简化），用于t-SMILES家族。TSIS与另一个基于片段的线性解决方案SAFE进行了比较实验，结果表明SAFE在处理语法中的长期依赖性时存在挑战。TSIS继续使用t-SMILES中定义的树作为其基础数据结构，这使其与SAFE模型有所不同。TSIS模型的性能超过了SAFE模型，表明t-SMILES的树结构起到了重要作用。

    String-based molecular representations, such as SMILES, are a de facto standard for linearly representing molecular information. However, the must be paired symbols and the parsing algorithm result in long grammatical dependencies, making it difficult for even state-of-the-art deep learning models to accurately comprehend the syntax and semantics. Although DeepSMILES and SELFIES have addressed certain limitations, they still struggle with advanced grammar, which makes some strings difficult to read. This study introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES family. Comparative experiments between TSIS and another fragment-based linear solution, SAFE, indicate that SAFE presents challenges in managing long-term dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as its foundational data structure, which sets it apart from the SAFE model. The performance of TSIS models surpasses that of SAFE models, indicating that the tree structure of t
    
[^151]: 基于数据驱动的混合分类回归模型对地震强度分布的预测

    Data-Driven Prediction of Seismic Intensity Distributions Featuring Hybrid Classification-Regression Models

    [https://arxiv.org/abs/2402.02150](https://arxiv.org/abs/2402.02150)

    本研究开发了基于地震参数的线性回归模型，能够基于数据预测地震强度分布，并且优于常用的地震地面运动预测方程（GMPEs）。

    

    地震是人类面临的最紧迫和致命的自然灾害之一。准确预测地震损害的程度和评估潜在风险可以在挽救众多生命方面起到重要作用。在本研究中，我们开发了基于地震参数（位置、深度和震级）的线性回归模型，能够预测地震强度分布。由于它完全基于数据驱动，可以在没有地理信息的情况下预测强度分布。数据集包含1997年至2020年在日本附近发生的地震的地震强度数据，具体包含1857个震级在5.0或更大的地震实例，数据来源于日本气象厅。我们训练了回归模型和分类模型，并将它们结合起来利用二者的优势创建了一个混合模型。在相关性方面，所提出的模型优于常用的地震地面运动预测方程（Ground Motion Prediction Equations，GMPEs）。

    Earthquakes are among the most immediate and deadly natural disasters that humans face. Accurately forecasting the extent of earthquake damage and assessing potential risks can be instrumental in saving numerous lives. In this study, we developed linear regression models capable of predicting seismic intensity distributions based on earthquake parameters: location, depth, and magnitude. Because it is completely data-driven, it can predict intensity distributions without geographical information. The dataset comprises seismic intensity data from earthquakes that occurred in the vicinity of Japan between 1997 and 2020, specifically containing 1,857 instances of earthquakes with a magnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We trained both regression and classification models and combined them to take advantage of both to create a hybrid model. The proposed model outperformed commonly used Ground Motion Prediction Equations (GMPEs) in terms of the correlatio
    
[^152]: 紧急计算：基于分层强化学习的自适应协作推理方法

    Emergency Computing: An Adaptive Collaborative Inference Method Based on Hierarchical Reinforcement Learning

    [https://arxiv.org/abs/2402.02146](https://arxiv.org/abs/2402.02146)

    本文提出了一个紧急网络(E-SC3I)，该网络具有感知、通信、计算、缓存和智能功能，通过紧急计算、缓存、集成通信和感知以及智能增强机制实现快速接入、可靠传输和动态网络部署。为了解决计算开销问题，我们提出了一种自适应协作推理方法(ACIM)。

    

    在实现有效的紧急响应中，及时获取环境信息、无缝的指挥数据传输和及时的决策至关重要。这需要建立一个弹性的紧急通信专用网络，能够在基础设施缺失的情况下提供通信和感知服务。本文提出了一个具有感知、通信、计算、缓存和智能功能的紧急网络(E-SC3I)。该框架包括紧急计算、缓存、集成通信和感知以及智能增强机制。E-SC3I确保了快速接入大量用户、在不稳定链路上可靠传输数据以及在不断变化的环境中动态网络部署。然而，这些优势带来了重要的计算开销。因此，我们专注于紧急计算，并提出了一种基于自适应协作推理方法(ACIM)。该方法基于Hierarchical Reinforcement Learning and Realization (HRLR)模型，通过在紧急情况下自适应地选择推理节点，实现了协作推理。

    In achieving effective emergency response, the timely acquisition of environmental information, seamless command data transmission, and prompt decision-making are crucial. This necessitates the establishment of a resilient emergency communication dedicated network, capable of providing communication and sensing services even in the absence of basic infrastructure. In this paper, we propose an Emergency Network with Sensing, Communication, Computation, Caching, and Intelligence (E-SC3I). The framework incorporates mechanisms for emergency computing, caching, integrated communication and sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large user base, reliable data transmission over unstable links, and dynamic network deployment in a changing environment. However, these advantages come at the cost of significant computation overhead. Therefore, we specifically concentrate on emergency computing and propose an adaptive collaborative inference method (ACIM) based on
    
[^153]: 语言对LLMs的道德判断和推理能力有影响吗？一项使用多语言定义问题测试的研究

    Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test

    [https://arxiv.org/abs/2402.02135](https://arxiv.org/abs/2402.02135)

    本研究通过使用多语言定义问题测试，探讨了大型语言模型（LLMs）在不同语言下的道德判断和道德推理能力。研究发现，印地语和斯瓦希里语的道德推理能力明显低于其他语言，而道德判断也在不同语言下变化较大。

    

    本文通过使用多语言定义问题测试探讨了大型语言模型（LLMs）在不同语言下展现的道德判断和道德推理能力。众所周知，道德判断取决于问题所使用的语言。我们将研究扩展到了除英语外的五种新语言（中文、印地语、俄语、西班牙语和斯瓦希里语），并对三个具有较强多语言文本处理和生成能力的LLMs（ChatGPT、GPT-4和Llama2Chat-70B）进行了探究。我们的研究发现，通过后柔性分数指示的道德推理能力在印地语和斯瓦希里语中显著低于西班牙语、俄语、中文和英语，而后四种语言的表现则没有明显的趋势。道德判断在不同语言中也存在较大差异。

    This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.
    
[^154]: 复合主动学习：在具备理论保证的多领域主动学习中迈进

    Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees

    [https://arxiv.org/abs/2402.02110](https://arxiv.org/abs/2402.02110)

    复合主动学习为多领域主动学习提供了具备理论保证的方法，并且考虑了不同领域之间的相似性和数据分布变化。

    

    主动学习（AL）旨在通过选择信息量最大的数据点进行标注，从而在固定的标注预算内提高模型性能。现有的AL方法通常只适用于单一领域的情况，即所有数据都来自同一领域（例如，同一数据集）。然而，许多现实任务往往涉及多个领域。例如，在视觉识别中，通常希望训练一个能够在不同环境（例如，不同背景）中工作的图像分类器，其中每个环境的图像构成一个领域。这种多领域AL设置对于以前的方法来说是具有挑战性的，因为它们（1）在分配标注预算时忽视了不同领域之间的相似性，（2）无法处理不同领域之间的数据分布变化。在本文中，我们提出了第一个通用方法，称为复合主动学习（CAL），用于多领域AL。我们的方法显式地考虑了问题中的领域层级和实例层级信息；CAL首先分配

    Active learning (AL) aims to improve model performance within a fixed labeling budget by choosing the most informative data points to label. Existing AL focuses on the single-domain setting, where all data come from the same domain (e.g., the same dataset). However, many real-world tasks often involve multiple domains. For example, in visual recognition, it is often desirable to train an image classifier that works across different environments (e.g., different backgrounds), where images from each environment constitute one domain. Such a multi-domain AL setting is challenging for prior methods because they (1) ignore the similarity among different domains when assigning labeling budget and (2) fail to handle distribution shift of data across different domains. In this paper, we propose the first general method, dubbed composite active learning (CAL), for multi-domain AL. Our approach explicitly considers the domain-level and instance-level information in the problem; CAL first assigns
    
[^155]: 大型语言模型是好的提示优化器吗？

    Are Large Language Models Good Prompt Optimizers?

    [https://arxiv.org/abs/2402.02101](https://arxiv.org/abs/2402.02101)

    这项研究揭示了以大型语言模型（LLM）作为提示优化器的实际机制。研究发现LLM优化器往往受到自身先前知识的偏见，难以准确识别错误的真正原因，并且在生成合适的提示方面面临挑战。

    

    近期的研究表明，以大型语言模型（LLM）作为提示优化器进行自我反馈和优化提示在性能上取得了令人期待的结果。尽管成功，但这种方法的底层机制尚未被探索，而LLM作为提示优化器的真正有效性需要进一步验证。本文进行了一项全面的研究，揭示了基于LLM的提示优化的实际机制。我们的研究发现，LLM优化器在反思过程中往往难以准确识别错误的真正原因，而更多地受到自身先前知识的偏见。此外，即使反思在语义上是有效的，LLM优化器也经常无法通过单一的优化步骤为目标模型生成合适的提示，部分原因是目标模型的行为是不可预测的。根据观察结果，我们提出了一种新的“自动行为优化”方法。

    LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as Prompt Optimizers to self-reflect and refine prompts, has shown promising performance in recent studies. Despite the success, the underlying mechanism of this approach remains unexplored, and the true effectiveness of LLMs as Prompt Optimizers requires further validation. In this work, we conducted a comprehensive study to uncover the actual mechanism of LLM-based Prompt Optimization. Our findings reveal that the LLM optimizers struggle to identify the true causes of errors during reflection, tending to be biased by their own prior knowledge rather than genuinely reflecting on the errors. Furthermore, even when the reflection is semantically valid, the LLM optimizers often fail to generate appropriate prompts for the target models with a single prompt refinement step, partly due to the unpredictable behaviors of the target models. Based on the observations, we introduce a new "Automatic Behavior Optimization" par
    
[^156]: 分析多语言语言模型中跨语言知识转移的评估

    Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models

    [https://arxiv.org/abs/2402.02099](https://arxiv.org/abs/2402.02099)

    分析了多语言语言模型中跨语言知识转移的评估方法和设置，发现高性能主要归因于非语言知识的因素，如任务和表层知识，并且跨语言传输的主要是数据工件和偏见，尤其是对于低资源语言。

    

    最近在大规模数据集上训练的多语言语言模型似乎显示出在跨语言知识转移和下游任务上取得了很高的性能。然而，我们对当前的评估基准和设置能够准确衡量零-shot跨语言知识转移的程度表示质疑。在这项工作中，我们通过引入更具挑战性的设置，涉及多语言实例，挑战了高零-shot性能在目标任务中反映高跨语言能力的假设。通过广泛的实验和分析，我们展示了多语言模型的高性能主要归因于不需要转移实际语言知识的因素，如任务和表层知识。更具体地说，我们观察到跨语言传输的主要是数据工件和偏见，特别是对于低资源语言。我们的发现突显了被忽视的缺点。

    Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks
    
[^157]: 利用新颖性共享解决分散式多智能体协同探索问题

    Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing

    [https://arxiv.org/abs/2402.02097](https://arxiv.org/abs/2402.02097)

    提出了一种名为MACE的简单而有效的多智能体协同探索方法，通过共享局部新颖性来近似全局新颖性，并引入加权互信息来衡量智能体行动对其他智能体的影响，从而促进多智能体之间的协同探索。实验证明，MACE在稀疏奖励的多智能体环境中取得了出色的性能。

    

    分散式协作式多智能体强化学习中的探索面临两个挑战。一是全局状态的新颖性不可用，而局部观察的新颖性存在偏差。另一个挑战是智能体如何协调地进行探索。为了解决这些挑战，我们提出了一种简单但有效的多智能体协同探索方法MACE。通过仅传播局部新颖性，智能体可以考虑其他智能体的局部新颖性来近似全局新颖性。此外，我们新引入了加权互信息来衡量一个智能体的行动对其他智能体累计新颖性的影响。我们将其作为内在回报来鼓励智能体对其他智能体的探索产生更大的影响，从而促进协同探索。实验证明，MACE在三种稀疏奖励的多智能体环境中表现出优异的性能。

    Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
    
[^158]: 针对零样本遥感图像场景分类的深度语义-视觉对齐

    Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene Classification

    [https://arxiv.org/abs/2402.02094](https://arxiv.org/abs/2402.02094)

    提出了一种针对零样本遥感图像场景分类的深度语义-视觉对齐方法，通过自动收集可视化可检测属性来解决先前ZSL模型主要依赖于手动标记的属性或词嵌入的问题。

    

    深度神经网络在遥感图像分类方面取得了显著的进展，然而，训练过程需要每个类别的丰富样本。然而，给定遥感目标数据库的动态增长事实，为每个遥感类别注释标签耗时且不现实。零样本学习（ZSL）可以识别训练期间未见过的新类别，为上述问题提供了有希望的解决方案。然而，先前的ZSL模型主要依赖于手动标记的属性或从语言模型中提取的词嵌入来将知识从已知类别传递到新类别。此外，开创性的ZSL模型使用在ImageNet上预训练的卷积神经网络，这些网络主要关注每个图像中出现的主要对象，忽视了在遥感场景分类中也很重要的背景上下文。为了解决上述问题，我们提出自动收集可视化可检测属性的方法。

    Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. W
    
[^159]: DeCoF:通过帧一致性进行生成视频检测

    DeCoF: Generated Video Detection via Frame Consistency

    [https://arxiv.org/abs/2402.02085](https://arxiv.org/abs/2402.02085)

    通过帧一致性原则，DeCoF是一个简单但有效的生成视频检测模型，可以消除空间伪影的影响，并表现出强大的泛化能力。

    

    高级视频生成方法产生的视频质量不断提高，这导致社会面临新的安全挑战，使生成视频检测成为紧迫的研究重点。为促进这一领域的合作研究，我们构建了第一个明确用于生成视频检测的开源数据集，为社区提供了一个宝贵的资源，以评估和改进检测方法。通过一系列精心设计的探测实验，我们的研究探讨了时间和空间伪影在开发生成视频的通用和稳健检测器方面的重要性。基于视频帧一致性原则，我们引入了一个简单但有效的检测模型（DeCoF），它消除了空间伪影在通用特征学习中的影响。我们的广泛实验表明，DeCoF在检测未见过的视频生成模型产生的视频方面非常有效，并且验证了其在多个领域的强大泛化能力。

    The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
    
[^160]: 基于对齐和一致性的原型对比学习用于推荐

    Prototypical Contrastive Learning through Alignment and Uniformity for Recommendation

    [https://arxiv.org/abs/2402.02079](https://arxiv.org/abs/2402.02079)

    本研究提出了一种基于对齐和一致性的原型对比学习方法（ProtoAU）用于推荐系统中，该方法解决了随机抽样引起的抽样偏差问题，提高了特征表示的有效性。

    

    图协同过滤（GCF）是最常使用的推荐系统方法之一，有效地捕捉用户和物品之间复杂的关系。基于图的对比学习（GCL）的GCF已经受到了显著关注，因为它利用自监督技术从现实场景中提取有价值的信号。然而，许多方法通常通过随机抽样学习涉及构建对比对的鉴别任务的实例。GCL方法面临抽样偏差问题，负样本可能具有与正样本类似的语义结构，导致特征表示的有效性下降。为了解决这些问题，我们提出了基于对齐和一致性的原型对比学习用于推荐，称为ProtoAU。具体而言，我们首先提出了原型（聚类中心）作为一个潜在空间，以确保一致性。

    Graph Collaborative Filtering (GCF), one of the most widely adopted recommendation system methods, effectively captures intricate relationships between user and item interactions. Graph Contrastive Learning (GCL) based GCF has gained significant attention as it leverages self-supervised techniques to extract valuable signals from real-world scenarios. However, many methods usually learn the instances of discrimination tasks that involve the construction of contrastive pairs through random sampling. GCL approaches suffer from sampling bias issues, where the negatives might have a semantic structure similar to that of the positives, thus leading to a loss of effective feature representation. To address these problems, we present the \underline{Proto}typical contrastive learning through \underline{A}lignment and \underline{U}niformity for recommendation, which is called \textbf{ProtoAU}. Specifically, we first propose prototypes (cluster centroids) as a latent space to ensure consistency 
    
[^161]: $\mathbb{X}$用户的可信度：一种一类分类方法

    Trustworthiness of $\mathbb{X}$ Users: A One-Class Classification Approach

    [https://arxiv.org/abs/2402.02066](https://arxiv.org/abs/2402.02066)

    本研究基于One-Class Classification（OCC）模型，提出了一种对$\mathbb{X}$用户进行分类的方法。同时，提出了一种基于子空间学习的新颖正则化项和数据描述方法，用于捕捉异质的数据集中度。

    

    $\mathbb{X}$（以前的Twitter）是一个重要的在线社交媒体平台，在信息共享方面起着重要作用，使得该平台生成的内容成为有价值的信息来源。确保$\mathbb{X}$上的信任是确定用户的可信度并防止各个领域出现问题的关键。尽管使用传统机器学习模型将可信度分配给$\mathbb{X}$用户并将其分类为可信或不可信是常见做法，但对于使用一类分类（OCC）模型进行这一目的的探索有限。在本研究中，我们使用各种OCC模型对$\mathbb{X}$用户进行分类。此外，我们提出使用基于子空间学习的方法，同时优化子空间和数据描述以用于OCC。我们还引入了一种新颖的正则化项用于子空间支持向量数据描述（SSVDD），以表达低维子空间中的数据集中度，捕捉多样性。

    $\mathbb{X}$ (formerly Twitter) is a prominent online social media platform that plays an important role in sharing information making the content generated on this platform a valuable source of information. Ensuring trust on $\mathbb{X}$ is essential to determine the user credibility and prevents issues across various domains. While assigning credibility to $\mathbb{X}$ users and classifying them as trusted or untrusted is commonly carried out using traditional machine learning models, there is limited exploration about the use of One-Class Classification (OCC) models for this purpose. In this study, we use various OCC models for $\mathbb{X}$ user classification. Additionally, we propose using a subspace-learning-based approach that simultaneously optimizes both the subspace and data description for OCC. We also introduce a novel regularization term for Subspace Support Vector Data Description (SSVDD), expressing data concentration in a lower-dimensional subspace that captures diverse
    
[^162]: AnthroScore: 一种计算语言学的人性度量方法

    AnthroScore: A Computational Linguistic Measure of Anthropomorphism

    [https://arxiv.org/abs/2402.02056](https://arxiv.org/abs/2402.02056)

    AnthroScore是一种计算语言学的度量方法，用于隐含人性化语言。研究发现，AnthroScore与人类对人性化的判断和社会科学文献中的人性化维度相一致。分析结果显示，近15年来研究论文中的人形化水平稳步增加，与语言模型相关的论文具有最高的人形化水平。

    

    人形化，即将人类特征赋予非人类实体，已经塑造了关于技术影响和可能性的对话。我们提出了AnthroScore，一种隐含人性化语言的自动度量标准。我们使用遮蔽语言模型来量化非人类实体在周围语境中被隐式地框架为人类的程度。我们证明了AnthroScore与人类对人性化的判断以及社会科学文献中描述的人性化维度相一致。受到计算机科学话语中误导性人形化的担忧的驱动，我们使用AnthroScore分析了15年的研究论文和下游新闻文章。在研究论文中，我们发现人形化在时间上稳步增加，并且与语言模型相关的论文中具有最高的人形化水平。在ACL论文中，人形化的时间增加与关键神经进展相关。在科学担忧的基础上构建。

    Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific
    
[^163]: 方差对齐分数: 一种简单但难以超越的多模式对比学习数据选择方法

    Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning

    [https://arxiv.org/abs/2402.02055](https://arxiv.org/abs/2402.02055)

    提出了一种简单但有理论原则支持的度量方法——方差对齐分数(Variance Alignment Score，VAS)，用于解决数据选择问题。该方法通过最大化总的VAS来选择最具信息量的样本。

    

    近年来，数据选择已经成为大规模视觉-语言模型预训练的核心问题，特别是在嘈杂的网络抽样数据集上。一种广泛采用的策略是为每个样本分配质量分数，如CLIP相似度，并保留具有最高分数的数据对。然而，这些方法对数据分布是无知的，始终无法选择最具信息量的样本。为了解决这个问题，我们提出了一种简单但有理论原则支持的度量方法，名为方差对齐分数(Variance Alignment Score，VAS)，它的形式是$\langle \Sigma_{\text{test}}, \Sigma_i\rangle$。这里，$\Sigma_{\text{test}}$表示我们希望对齐的目标（交叉）协方差矩阵，可能基于先验知识，而$\Sigma_i$表示第$i$个样本的单模态或多模态表示的张量积。我们进一步设计了一种新的数据选择方法，最大化总的VAS。我们在简化的设置下进行了理论分析来证明这个方法的理论优势。

    In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here, $\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical 
    
[^164]: 图上的神经缩放定律

    Neural Scaling Laws on Graphs

    [https://arxiv.org/abs/2402.02054](https://arxiv.org/abs/2402.02054)

    本论文在图上深入研究了神经缩放定律，从模型和数据两个角度进行了探索。对于模型缩放，发现了缩放定律崩溃和过拟合之间的关系，以及深度图模型的模型深度对缩放行为的影响。对于数据缩放，提出了图数量不适合作为衡量缩放定律中图数据量的指标。

    

    深度图模型（例如图神经网络和图变换器）已成为利用各种类型图的知识的重要技术。然而，深度图模型的缩放特性尚未得到系统研究，对通过扩大模型和数据集大小来实现大型图模型的可行性产生了疑问。在这项工作中，我们从模型和数据的角度深入探索了图上的神经缩放定律。我们首先验证了这些定律在图上的有效性，并建立了描述缩放行为的公式。对于模型缩放，我们研究了缩放定律崩溃现象，并确定了过拟合可能是原因。此外，我们揭示了深度图模型的模型深度可以影响模型缩放行为，这与其他领域（如计算机视觉和自然语言处理）的观察结果不同。对于数据缩放，我们建议图数量无法有效衡量图数据量的缩放定律，因为...

    Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
    
[^165]: 经济实惠的生成式智能体

    Affordable Generative Agents

    [https://arxiv.org/abs/2402.02053](https://arxiv.org/abs/2402.02053)

    本文提出了经济实惠的生成式智能体框架（AGA），通过学习策略替代LLM推理和压缩对话信息，实现了低成本的可信互动，且对于有限环境中生成的可信行为机制进行了深入研究。

    

    大规模语言模型（LLMs）的出现显著推进了真实交互智能体的模拟。然而，维持长时间智能体交互的巨大成本对于部署基于LLM的可信智能体构成了挑战。因此，在本文中，我们开发了经济实惠的生成式智能体（AGA），这是一个框架，可以在智能体-环境和智能体间交互的两个层面上实现低成本的可信互动。具体而言，对于智能体-环境交互，我们用学习的策略替代了重复的LLM推理；而对于智能体间交互，我们对智能体之间的社会关系进行建模，并压缩辅助对话信息。在多个环境上的大量实验表明了我们提出的框架的有效性和效率。此外，我们深入探究了LLM智能体中的可信行为形成机制，证明智能体仅能在固定环境中生成有限行为。

    The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, 
    
[^166]: 一种用于智能和选择性传感器数据传输的插件式微型人工智能模块

    A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data Transmission

    [https://arxiv.org/abs/2402.02043](https://arxiv.org/abs/2402.02043)

    提出了一种插件式微型AI模块，可以将智能和选择性传感器数据传输集成到物联网应用中。通过放置高效的机器学习模型靠近传感器，实现对数据传输的智能控制，筛选有价值的数据，并通过调节传输频率丢弃无关信息。通过量化和优化近传感器模型，实现实时传感器控制。定制训练过程和利用时间信息的“懒惰”传感器停用策略提升框架性能。该方法独立于其他物联网框架，可与之共存。

    

    物联网应用中利用机器学习分析传感器生成的数据，但当前感知系统缺乏有针对性的智能，导致数据产生量巨大、计算和通信成本增加。为了解决这个挑战，我们提出了一种新颖的感知模块，通过将高效的机器学习模型放置在传感器附近，为感知框架提供智能数据传输能力。该模型及时反馈给感知系统，只传输有价值的数据，通过调节数据传输的频率丢弃无关信息。近传感器模型经过量化和优化，实现实时传感器控制。为了提升框架性能，定制了训练过程，并引入利用时间信息的“懒惰”传感器停用策略。该方法与其他物联网框架正交，可以保证共存。

    Applications in the Internet of Things (IoT) utilize machine learning to analyze sensor-generated data. However, a major challenge lies in the lack of targeted intelligence in current sensing systems, leading to vast data generation and increased computational and communication costs. To address this challenge, we propose a novel sensing module to equip sensing frameworks with intelligent data transmission capabilities by integrating a highly efficient machine learning model placed near the sensor. This model provides prompt feedback for the sensing system to transmit only valuable data while discarding irrelevant information by regulating the frequency of data transmission. The near-sensor model is quantized and optimized for real-time sensor control. To enhance the framework's performance, the training process is customized and a "lazy" sensor deactivation strategy utilizing temporal information is introduced. The suggested method is orthogonal to other IoT frameworks and can be cons
    
[^167]: 学习通过原始-对偶策略梯度算法对无限时域平均回报受限MDP进行参数化通用策略

    Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm

    [https://arxiv.org/abs/2402.02042](https://arxiv.org/abs/2402.02042)

    该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。

    

    本文探索了无限时域平均回报受限马尔科夫决策过程（CMDP）的领域。据我们所知，这项工作是首次研究具有通用策略参数化的平均回报CMDP的遗憾和约束违规分析。为了解决这个挑战，我们提出了一种基于原始对偶的策略梯度算法，能够灵活地管理约束条件，并确保低遗憾保证以实现全局最优策略。特别地，我们证明了我们提出的算法在目标遗憾和约束违反上具有 $\tilde{\mathcal{O}}({T}^{3/4})$ 的界限。

    This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
    
[^168]: CEC 2024多方多目标优化竞赛的基准

    Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization

    [https://arxiv.org/abs/2402.02033](https://arxiv.org/abs/2402.02033)

    该竞赛旨在填补多方多目标优化问题研究不足，通过鼓励研究人员探索定制建模方法。通过基于MPIGD和MPHV指标对两部分问题进行评估，并以平均排名作为性能基准。

    

    该竞赛关注多方多目标优化问题（MPMOPs），其中多个决策者具有冲突的目标，如无人机路径规划应用中所见。尽管其重要性，与传统多目标优化相比，MPMOPs仍然研究不足。该竞赛旨在通过鼓励研究人员探索定制建模方法来填补这一空白。测试套件分为两部分：具有共同帕累托最优解的问题和具有未知解的双方多目标无人机路径规划（BPMO-UAVPP）问题。第一部分的优化算法使用多方倒转式生成距离（MPIGD）进行评估，第二部分使用多方超体积（MPHV）指标进行评估。跨所有问题的平均算法排名用作性能基准。

    The competition focuses on Multiparty Multiobjective Optimization Problems (MPMOPs), where multiple decision makers have conflicting objectives, as seen in applications like UAV path planning. Despite their importance, MPMOPs remain understudied in comparison to conventional multiobjective optimization. The competition aims to address this gap by encouraging researchers to explore tailored modeling approaches. The test suite comprises two parts: problems with common Pareto optimal solutions and Biparty Multiobjective UAV Path Planning (BPMO-UAVPP) problems with unknown solutions. Optimization algorithms for the first part are evaluated using Multiparty Inverted Generational Distance (MPIGD), and the second part is evaluated using Multiparty Hypervolume (MPHV) metrics. The average algorithm ranking across all problems serves as a performance benchmark.
    
[^169]: ScribFormer: Transformer使得基于涂鸦的医学图像分割的CNN工作更好

    ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation

    [https://arxiv.org/abs/2402.02029](https://arxiv.org/abs/2402.02029)

    ScribFormer是一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割。它通过融合CNN学习的局部特征和Transformer获得的全局表示，有效克服了现有方法在涂鸦注释中学习全局形状信息的局限性。

    

    最近的涂鸦监督分割方法通常采用具有编码器-解码器架构的CNN框架。尽管这个框架有多个好处，但是由于卷积层只能捕捉具有局部感受野的小范围特征依赖关系，它很难从涂鸦注释提供的有限信息中学习全局形状信息。为了解决这个问题，本文提出了一种新的CNN-Transformer混合解决方案，用于涂鸦监督的医学图像分割，称为ScribFormer。所提出的ScribFormer模型具有三个分支结构，即CNN分支，Transformer分支和注意力引导的类激活图（ACAM）分支的混合。具体而言，CNN分支与Transformer分支合作，将从CNN学习的局部特征与从Transformer获得的全局表示相融合，可以有效克服现有涂鸦监督分割方法的局限性。

    Most recent scribble-supervised segmentation methods commonly adopt a CNN framework with an encoder-decoder architecture. Despite its multiple benefits, this framework generally can only capture small-range feature dependency for the convolutional layer with the local receptive field, which makes it difficult to learn global shape information from the limited information provided by scribble annotations. To address this issue, this paper proposes a new CNN-Transformer hybrid solution for scribble-supervised medical image segmentation called ScribFormer. The proposed ScribFormer model has a triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer branch, and an attention-guided class activation map (ACAM) branch. Specifically, the CNN branch collaborates with the Transformer branch to fuse the local features learned from CNN with the global representations obtained from Transformer, which can effectively overcome limitations of existing scribble-supervised segmentation m
    
[^170]: 自主驾驶中角落案例检测的多模态增强物件学习器

    Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving

    [https://arxiv.org/abs/2402.02026](https://arxiv.org/abs/2402.02026)

    本文提出了一种用于角落案例检测的多模态增强物件学习器（MENOL）。该方法通过减小已知类和未知类之间的差异，并引入多模态的数据，显著提高了对新类别的召回率，并降低了训练成本。

    

    在物体检测的先前工作中，封闭场景下的准确率较高，但在开放世界场景下的性能并不令人满意。其中一个具有挑战性的开放世界问题是自主驾驶中的角落案例检测。现有的检测器在这些案例中表现困难，过度依赖视觉外观，具有较差的泛化能力。本文提出了一种解决方案，通过减小已知类和未知类之间的差异，并引入多模态增强的物件学习器的概念。借助视觉中心和图像-文本两种形式，我们的半监督学习框架将物件学习器的知识传递给学生模型，实现了类别感知的检测。我们的方法——用于角落案例检测的多模态增强物件学习器（MENOL），显著提高了对新类别的召回率，并降低训练成本。在仅使用5100个标签训练图像的CODA-val数据集上，MENOL实现了76.6%的mAR-corner和79.8%的mAR-agnostic。

    Previous works on object detection have achieved high accuracy in closed-set scenarios, but their performance in open-world scenarios is not satisfactory. One of the challenging open-world problems is corner case detection in autonomous driving. Existing detectors struggle with these cases, relying heavily on visual appearance and exhibiting poor generalization ability. In this paper, we propose a solution by reducing the discrepancy between known and unknown classes and introduce a multimodal-enhanced objectness notion learner. Leveraging both vision-centric and image-text modalities, our semi-supervised learning framework imparts objectness knowledge to the student model, enabling class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner (MENOL) for Corner Case Detection, significantly improves recall for novel classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8% mAR-agnostic on the CODA-val dataset with just 5100 labeled training images, MEN
    
[^171]: 安全强化学习中的约束形式综述

    A Survey of Constraint Formulations in Safe Reinforcement Learning

    [https://arxiv.org/abs/2402.02025](https://arxiv.org/abs/2402.02025)

    本文综述了安全强化学习中的约束形式，包括对每种形式特别设计的算法。同时，揭示了常见问题形式之间的数学相互关系。

    

    在将强化学习（RL）应用于现实世界问题时，确保安全性至关重要。因此，安全RL成为一种从实验数据中安全优化代理策略的基本而强大的范例。基于约束准则的安全RL方法被广泛采用，它解决了在安全约束下最大化预期累积奖励的问题。虽然近年来在RL中实现安全性的尝试激增，但由于约束表示的多样性和对它们之间关系的讨论很少，对该领域的系统性了解仍然困难。为了解决这一知识差距，我们提供了对代表性约束形式的全面回顾，以及针对每种形式特别设计的算法的精选。此外，我们揭示了揭示常见问题形式之间的数学相互关系的理论基础。最后，我们讨论了当前研究的一些挑战和未来方向。

    Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current 
    
[^172]: 自监督对比预测

    Self-Supervised Contrastive Forecasting

    [https://arxiv.org/abs/2402.02023](https://arxiv.org/abs/2402.02023)

    该论文介绍了一种通过采用对比学习和增强的分解架构，并结合全局自相关性的自监督方法来解决长期预测中的挑战。实验证明，该方法在九个长期基准上的多个实验中胜过了14个基线模型。

    

    长期预测由于处理长序列的时间和内存复杂性而面临独特挑战。现有方法依赖于滑动窗口来处理长序列，难以有效捕捉部分在短窗口内被捕捉到的长期变化（即外窗口变化）。本文介绍了一种新颖的方法，通过采用对比学习和增强的分解架构，专门设计用于聚焦长期变化，从而克服了这个限制。为此，我们的对比损失将整个时间序列中的全局自相关性纳入考虑，以自监督方式构建正负对。当与我们的分解网络结合使用时，我们的对比学习显著提高了长期预测性能。广泛的实验表明，我们的方法在九个长期基准上的多个实验中胜过了14个基线模型。

    Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, es
    
[^173]: LLM是否可以正确引用相关医学参考文献？一个评估框架和分析

    How well do LLMs cite relevant medical references? An evaluation framework and analyses

    [https://arxiv.org/abs/2402.02008](https://arxiv.org/abs/2402.02008)

    这项研究提出了一个问题：LLM生成的来源是否真正支持它们所做的主张？通过验证源的相关性和开发端到端的自动化流水线，研究人员发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。

    

    目前在各种临床领域中，大型语言模型（LLM）被广泛用于回答医学问题。特别是最近表现出色的商业LLM，它们能够引用来源来支持其回答。本文提出一个问题：LLM生成的来源是否真正支持它们所做的主张？为了回答这个问题，我们提出了三个贡献。首先，专家的医学注释是一种昂贵且耗时的评估瓶颈，我们证明GPT-4在验证源的相关性方面非常准确，与一组医生的判断达到88%的一致性。其次，我们开发了一个名为“SourceCheckup”的端到端自动化流水线，并使用它评估了1200个生成的问题上的五个表现最佳的LLM，总计超过40K对的陈述和来源。有趣的是，我们发现50%到90%的LLM回答并没有充分地得到它们所提供的来源的支持。我们还对GPT-4进行了评估...

    Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with 
    
[^174]: 边缘上基于新颖超维度计算的在线时间序列预测框架

    A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge

    [https://arxiv.org/abs/2402.01999](https://arxiv.org/abs/2402.01999)

    本文提出了一种新颖的超维度计算框架，用于解决在线时间序列预测问题。通过将非线性低维数据映射到高维空间进行线性超维度预测，实现了快速、高效和轻量级的预测，同时适应了时间序列数据的变化。

    

    近年来，已经开发出在线和离线的深度学习模型用于时间序列预测。然而，离线深度预测模型不能有效适应时间序列数据的变化，而在线深度预测模型常常昂贵且具有复杂的训练过程。在本文中，我们重新定义了在线非线性时间序列预测问题，将其作为线性超维度时间序列预测的问题。非线性低维时间序列数据被映射到高维（超维度）空间进行线性超维度预测，实现了快速、高效和轻量级的在线时间序列预测。我们的框架TSF-HD使用了一种新颖的共训练框架，用于超维度映射和线性超维度预测器，以适应时间序列分布的变化。TSF-HD在短期和长期时间序列预测方面优于现有技术，并降低了推断延迟。我们的代码已经公开发布。

    In recent years, both online and offline deep learning models have been developed for time series forecasting. However, offline deep forecasting models fail to adapt effectively to changes in time-series data, while online deep forecasting models are often expensive and have complex training procedures. In this paper, we reframe the online nonlinear time-series forecasting problem as one of linear hyperdimensional time-series forecasting. Nonlinear low-dimensional time-series data is mapped to high-dimensional (hyperdimensional) spaces for linear hyperdimensional prediction, allowing fast, efficient and lightweight online time-series forecasting. Our framework, TSF-HD, adapts to time-series distribution shifts using a novel co-training framework for its hyperdimensional mapping and its linear hyperdimensional predictor. TSF-HD is shown to outperform the state of the art, while having reduced inference latency, for both short-term and long-term time series forecasting. Our code is publi
    
[^175]: 大语言模型时代的以人为中心的隐私研究

    Human-Centered Privacy Research in the Age of Large Language Models

    [https://arxiv.org/abs/2402.01994](https://arxiv.org/abs/2402.01994)

    大语言模型带来了隐私担忧，现有研究着重于模型本身，我们认为有必要进行以人为中心的研究，关注用户的行为和偏好，设计工具和系统，使用户能够掌控个人数据的所有权。

    

    大型语言模型（LLMs）的出现及其在面向用户的系统中的广泛使用引起了重大的隐私担忧。迄今为止，对这些隐私问题的研究以模型为中心：探索LLMs如何导致隐私风险，例如记忆，或如何通过内容推断人们的个人特征。我们认为有必要进行更多研究，重点关注这些隐私问题的人类因素：例如研究LLMs设计范式如何影响用户的披露行为、用户的心理模型和隐私控制的偏好，以及设计工具、系统和工件以赋予终端用户对个人数据的所有权。为了构建可用、高效和注重隐私的系统，使其由这些具有不完善隐私属性的模型进行驱动，我们的目标是发起讨论，拟定一个以人为中心的研究议程，对LLMs加速系统中的隐私问题进行研究。

    The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns. To date, research on these privacy concerns has been model-centered: exploring how LLMs lead to privacy risks like memorization, or can be used to infer personal characteristics about people from their content. We argue that there is a need for more research focusing on the human aspect of these privacy issues: e.g., research on how design paradigms for LLMs affect users' disclosure behaviors, users' mental models and preferences for privacy controls, and the design of tools, systems, and artifacts that empower end-users to reclaim ownership over their personal data. To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems. This Special Interest Group (SIG) ai
    
[^176]: 在线转移学习用于RSV病例检测

    Online Transfer Learning for RSV Case Detection

    [https://arxiv.org/abs/2402.01987](https://arxiv.org/abs/2402.01987)

    这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。

    

    转移学习已经成为机器学习中的一个关键技术，在各种真实世界应用中表现出高效性。然而，当将这种方法应用于序列流行病学数据时，会面临一个重要挑战，即标注信息匮乏。为了解决这个挑战，我们引入了一种新颖的在线多源转移学习方法，称为预测体积自适应加权（PVAW）。PVAW在整合模型中创造性地实现了动态加权机制，可以根据每个源模型和目标模型的相关性和贡献度自动调整权重。我们通过在匹兹堡大学医学中心收集的多个季节的呼吸道合胞病毒（RSV）数据上应用PVAW，证明了其有效性。我们的方法在模型性能上显著优于现有基线，突出了在线转移学习在处理这一问题上的潜力。

    Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
    
[^177]: 自我去偏差的大型语言模型：零-shot识别和降低刻板印象

    Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes

    [https://arxiv.org/abs/2402.01981](https://arxiv.org/abs/2402.01981)

    本研究利用大型语言模型的零-shot能力提出了一种自我去偏差的技术，通过解释和重启两种方法，成功减少了九个不同社会群体的刻板印象程度，该技术不需要对训练数据、模型参数或解码策略进行修改，希望能启发其他零-shot偏见缓解技术的研究。

    

    大型语言模型(LLMs)在语言生成和理解方面表现出了显著的进展，但也容易展示出有害的社会偏见。尽管已经提出了许多偏见缓解技术，但大多数需要对训练数据、模型参数或解码策略进行修改，这在没有可训练模型的情况下可能是不可行的。在这项工作中，我们利用LLMs的零-shot能力，引入了一种称为零-shot自我去偏差的技术来减少刻板印象。通过两种方法，即解释自我去偏差和重启自我去偏差，我们展示了自我去偏差可以显著减少九个不同社会群体的刻板印象程度，只依赖于LLM自身和简单的提示，其中解释正确地识别出无效的假设，而重启则产生了最大的偏见减少效果。我们希望这项工作能够开启对其他零-shot偏见缓解技术的研究。

    Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mi
    
[^178]: 对上下文感知多agent系统的调查：技术、挑战和未来发展方向

    A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions

    [https://arxiv.org/abs/2402.01968](https://arxiv.org/abs/2402.01968)

    这篇论文调查了上下文感知多代理系统的技术、挑战和未来发展方向，并提供了一个综合的概述。它介绍了上下文感知系统和多代理系统的特性，以及集成这些系统的通用过程。

    

    随着新兴主题的兴起，自主代理的研究兴趣正在增加。大型语言模型的显著成就已经展示了在自主代理中达到人类智能的巨大潜力。然而，挑战在于使这些代理能够在动态环境中学习、推理和导航不确定性。当处理动态情况时，上下文意识成为强化多agent系统的关键因素。尽管现有的研究专注于上下文感知系统和多agent系统，但缺乏全面概述如何将上下文感知系统与多agent系统集成的综合调查。为了填补这个空白，本调查提供了对最先进的上下文感知多agent系统的全面概述。首先，我们概述了促进这些系统之间集成的上下文感知系统和多 agent 系统的特性。随后，我们提出了一个通用的过程来建模上下文感知和多agent系统的集成。

    Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for c
    
[^179]: OPSurv：用于生存分析的正交多项式积分算法

    OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival Analysis

    [https://arxiv.org/abs/2402.01955](https://arxiv.org/abs/2402.01955)

    本文介绍了一种用于生存分析的新方法OPSurv，通过正交多项式积分算法提供连续时间功能性输出。该方法利用累积发生函数的初始零条件和正交多项式的分解，实现了对每个风险事件的功能近似系数的学习，并通过高斯-勒让德积分法构建累积发生函数的估计值，有效地解决了竞争风险场景中的过度拟合问题。

    

    本文介绍了用于生存分析中单一风险和竞争风险场景的Orthogonal Polynomials Quadrature Algorithm for Survival Analysis（OPSurv）方法，该方法提供了连续时间功能性输出。OPSurv利用累积发生函数的初始零条件和使用正交多项式对概率密度进行唯一分解，从而为每个风险事件学习功能近似系数，并通过高斯-勒让德积分法构建累积发生函数的估计值。该方法有效地对抗过度拟合，尤其在竞争风险场景中，增强模型的表达能力和控制性。本文还详细介绍了OPSurv的经验验证和理论证明，突出了其作为竞争风险生存分析的一项重要进展的鲁棒性能。

    This paper introduces the Orthogonal Polynomials Quadrature Algorithm for Survival Analysis (OPSurv), a new method providing time-continuous functional outputs for both single and competing risks scenarios in survival analysis. OPSurv utilizes the initial zero condition of the Cumulative Incidence function and a unique decomposition of probability densities using orthogonal polynomials, allowing it to learn functional approximation coefficients for each risk event and construct Cumulative Incidence Function estimates via Gauss--Legendre quadrature. This approach effectively counters overfitting, particularly in competing risks scenarios, enhancing model expressiveness and control. The paper further details empirical validations and theoretical justifications of OPSurv, highlighting its robust performance as an advancement in survival analysis with competing risks.
    
[^180]: 机器学习中的强健反事实解释：一项调查

    Robust Counterfactual Explanations in Machine Learning: A Survey

    [https://arxiv.org/abs/2402.01928](https://arxiv.org/abs/2402.01928)

    这项调查回顾了机器学习中强健反事实解释的研究，并分析了其对强健性的考虑。该调查讨论了现有解决方案及其局限性，为未来的发展提供了基础。

    

    反事实解释（CEs）被认为非常适合为受机器学习模型预测影响的对象提供算法上的补救措施。尽管CEs对受影响的个体有益，但最近的研究揭示了获取CEs的最新方法相关的严重问题。由于缺乏强健性可能会损害CEs的有效性，因此有必要采取技术来减轻这个风险。在这项调查中，我们回顾了强健CEs这一迅速发展的领域的研究，并对它们考虑的强健性形式进行了深入分析。我们还讨论了现有解决方案及其局限性，为未来的发展提供了坚实的基础。

    Counterfactual explanations (CEs) are advocated as being ideally suited to providing algorithmic recourse for subjects affected by the predictions of machine learning models. While CEs can be beneficial to affected individuals, recent work has exposed severe issues related to the robustness of state-of-the-art methods for obtaining CEs. Since a lack of robustness may compromise the validity of CEs, techniques to mitigate this risk are in order. In this survey, we review works in the rapidly growing area of robust CEs and perform an in-depth analysis of the forms of robustness they consider. We also discuss existing solutions and their limitations, providing a solid foundation for future developments.
    
[^181]: 从弱监督中学习的通用框架

    A General Framework for Learning from Weak Supervision

    [https://arxiv.org/abs/2402.01922](https://arxiv.org/abs/2402.01922)

    本文介绍了一个通用框架，利用新算法从各种弱监督中学习，通过使用非确定性有限自动机和前向-后向算法来简化计算要求，并将时间复杂度降低到线性尺度。

    

    弱监督学习通常面临着适用于具有多样化弱监督的各种场景和由于现有算法的复杂性而导致的可扩展性挑战，从而阻碍了实际部署。本文介绍了一个利用一种新算法来从弱监督中学习的通用框架（GLWS）。GLWS的核心是一个期望最大化（EM）的公式，灵活地适应了各种弱监督来源，包括实例的部分标签、聚合统计、成对观察和无标注数据。此外，我们还提供了一个先进的算法，使用非确定性有限自动机（NFA）以及前向-后向算法，显著简化了EM计算的需求，从而将时间复杂度从现有解决方案中通常所需的二次或阶乘复杂度降低到线性尺度。因此，从任意弱监督中学习的问题转化为了对它们进行NFA建模。GLWS不仅可以增强+

    Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enha
    
[^182]: 对奖励模型学习的偏好污染攻击

    Preference Poisoning Attacks on Reward Model Learning

    [https://arxiv.org/abs/2402.01920](https://arxiv.org/abs/2402.01920)

    对于从偏好比较中学习奖励模型的方法存在偏好污染攻击的漏洞，攻击者可以通过翻转少量偏好比较来对目标结果进行操纵。我们提出了两类算法方法，并证明了这些攻击在实施恶意行为方面的有效性。

    

    从两两比较中学习效用或奖励模型是许多应用领域的基础组成部分。这些方法从本质上需要从人们那里收集偏好信息，而反馈通常是匿名提供的。由于偏好是主观的，没有可以比较的黄金标准；然而，对偏好学习的高影响系统的依赖性为恶意行为者倾向于扭曲以达到其目的而采集的数据创造了强烈的动机。我们通过考虑一种威胁模型系统地调查了这种漏洞的性质和程度，其中攻击者可以翻转少量偏好比较，以促进或贬低目标结果。首先，我们提出了两类用于这些攻击的算法方法：基于原则的梯度框架和几种变种的按距离排名的方法。接下来，我们展示了这两类最佳攻击在成功实施恶意行为方面的效果。

    Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
    
[^183]: 关于大型基础模型的灾难性继承问题

    On Catastrophic Inheritance of Large Foundation Models

    [https://arxiv.org/abs/2402.01909](https://arxiv.org/abs/2402.01909)

    这篇论文讨论了大型基础模型（LFMs）中的灾难性继承问题，指出了从有偏见的大规模预训练数据到LFMs在下游任务中的行为的弱点和限制。我们提出了UIM框架，旨在理解LFMs的灾难性继承问题，并解释其中的含义。

    

    大型基础模型（LFMs）声称具有惊人的性能，然而人们对它们在机器学习以及其他各个学科中的神秘和难以解释的潜力提出了极大关切。在这篇立场论文中，我们提出了一个被忽视的问题，即LFMs中根深蒂固的灾难性继承问题，描述了从有偏见的大规模预训练数据到LFMs在下游任务中的行为的弱点和限制，包括受损、长尾、有噪音、超出分布等样本。这种继承可能对下游应用产生灾难性影响，如偏见、缺乏泛化能力、性能下降、安全漏洞、隐私泄露和价值误差。我们讨论了这个问题背后的挑战，并提出了UIM框架，来理解LFMs的灾难性继承问题，包括来自预训练和下游适应的继承内容的解释。

    Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inher
    
[^184]: 基于基础模型的神经符号学习和推理的作用

    The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning

    [https://arxiv.org/abs/2402.01889](https://arxiv.org/abs/2402.01889)

    基于基础模型的神经符号学习和推理在NeSy任务中表现出更好的性能，并且减少了标注和手动工程的工作量。

    

    神经符号人工智能（NeSy）有望确保AI系统的安全部署，因为可解释的符号技术提供了正式的行为保证。挑战在于如何有效地整合神经和符号计算，以便从原始数据中进行学习和推理。现有的顺序训练神经和符号组件的流水线需要大量标注，而端到端方法在符号基础问题的组合爆炸方面具有可扩展性的限制。在本文中，我们利用基础模型中的隐性知识来增强NeSy任务的性能，同时减少数据标注和人工工程量。我们引入了一种新的架构，称为NeSyGPT，它通过微调视觉-语言基础模型来从原始数据中提取符号特征，然后学习一个高度表达的答案集程序来解决下游任务。我们的综合评估表明，NeSyGPT具有... (剩余部分请自行翻译)

    Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI systems, as interpretable symbolic techniques provide formal behaviour guarantees. The challenge is how to effectively integrate neural and symbolic computation, to enable learning and reasoning from raw data. Existing pipelines that train the neural and symbolic components sequentially require extensive labelling, whereas end-to-end approaches are limited in terms of scalability, due to the combinatorial explosion in the symbol grounding problem. In this paper, we leverage the implicit knowledge within foundation models to enhance the performance in NeSy tasks, whilst reducing the amount of data labelling and manual engineering. We introduce a new architecture, called NeSyGPT, which fine-tunes a vision-language foundation model to extract symbolic features from raw data, before learning a highly expressive answer set program to solve a downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has 
    
[^185]: 通过估计演示者的专业知识的逆向强化学习

    Inverse Reinforcement Learning by Estimating Expertise of Demonstrators

    [https://arxiv.org/abs/2402.01886](https://arxiv.org/abs/2402.01886)

    本文介绍了一个新颖的框架，IRLEED，它通过估计演示者的专业知识来解决模仿学习中的次优和异质演示的问题。IRLEED通过结合演示者次优性的普适模型和最大熵IRL框架，有效地从多样的次优演示中得出最佳策略。

    

    在模仿学习中，利用次优和异质的演示提出了一个重大挑战，因为现实世界数据的性质各不相同。然而，标准的模仿学习算法将这些数据集视为同质的，从而继承了次优演示的缺陷。先前处理这个问题的方法通常依赖于不切实际的假设，如高质量的数据子集、置信度排名或明确的环境知识。本文介绍了IRLEED（通过估计演示者的专业知识的逆向强化学习），这是一个新颖的框架，能够克服这些障碍，而不需要先前对演示者专业知识进行了解。IRLEED通过将演示者次优性的普适模型与最大熵IRL框架相结合，来处理奖励偏差和行动方差，从而有效地从多样的次优演示中得出最优策略。在在线和离线实验中进行了验证。

    In Imitation Learning (IL), utilizing suboptimal and heterogeneous demonstrations presents a substantial challenge due to the varied nature of real-world data. However, standard IL algorithms consider these datasets as homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators. Previous approaches to this issue typically rely on impractical assumptions like high-quality data subsets, confidence rankings, or explicit environmental knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, a novel framework that overcomes these hurdles without prior knowledge of demonstrator expertise. IRLEED enhances existing Inverse Reinforcement Learning (IRL) algorithms by combining a general model for demonstrator suboptimality to address reward bias and action variance, with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations. Experiments in both online and offline I
    
[^186]: 基于大规模语言模型的超参数优化的技术

    Large Language Model Agent for Hyper-Parameter Optimization

    [https://arxiv.org/abs/2402.01881](https://arxiv.org/abs/2402.01881)

    基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。

    

    超参数优化在现代机器学习中至关重要，需要专业知识、大量实验以及高计算和人力资源。尽管自动化机器学习（AutoML）取得了一些进展，但试验效率、设置复杂性和互操作性方面仍存在挑战。为了解决这些问题，我们引入了一种新的范式，利用大规模语言模型（LLMs）来自动化不同机器学习任务的超参数优化，称为AgentHPO（LLM Agent-based Hyperparameter Optimization）。具体来说，AgentHPO自主处理任务信息，根据历史试验对特定超参数（HPs）进行实验，并进行迭代优化。与传统的AutoML方法相比，这种类似人类的优化过程极大地减少了所需的试验次数，简化了设置过程，并提升了解释性和用户信任。

    Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
    
[^187]: 移动试衣间：基于扩散模型的设备内虚拟试穿

    Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models

    [https://arxiv.org/abs/2402.01877](https://arxiv.org/abs/2402.01877)

    移动试衣间是第一个基于设备内扩散模型的虚拟试穿系统，解决了高质量服装放置和模型压缩等技术挑战，实现了隐私保护和用户定制。它为时尚电子商务企业提供了有价值的服务和用户友好的虚拟试穿体验。

    

    在日益增长的时尚电子商务领域，需要交互式和用户友好的界面来进行虚拟试穿衣服。传统的试穿方法在适应不同的背景、姿势和主体方面存在困难。虽然利用最近扩散模型的进展而产生了更高质量的图像生成，但移动界面交付的以人为中心的维度和隐私问题大部分仍未被探索。我们提出了移动试衣间，这是第一个基于设备内扩散的虚拟试穿系统。为了解决多个相互关联的技术挑战，如高质量服装放置和移动设备的模型压缩，我们提出了一种新颖的技术流程和界面设计，实现隐私保护和用户定制。一个使用场景突显了我们的工具如何为顾客提供无缝、互动的虚拟试穿体验，并为时尚电子商务企业提供有价值的服务。

    The growing digital landscape of fashion e-commerce calls for interactive and user-friendly interfaces for virtually trying on clothes. Traditional try-on methods grapple with challenges in adapting to diverse backgrounds, poses, and subjects. While newer methods, utilizing the recent advances of diffusion models, have achieved higher-quality image generation, the human-centered dimensions of mobile interface delivery and privacy concerns remain largely unexplored. We present Mobile Fitting Room, the first on-device diffusion-based virtual try-on system. To address multiple inter-related technical challenges such as high-quality garment placement and model compression for mobile devices, we present a novel technical pipeline and an interface design that enables privacy preservation and user customization. A usage scenario highlights how our tool can provide a seamless, interactive virtual try-on experience for customers and provide a valuable service for fashion e-commerce businesses.
    
[^188]: RL/LLM分类树：回顾强化学习和大语言模型之间的协同关系

    The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models

    [https://arxiv.org/abs/2402.01874](https://arxiv.org/abs/2402.01874)

    这项工作回顾了将强化学习和大语言模型结合起来的研究，并提出了一个新的分类方法，以审视这两个领域之间的协同关系。

    

    在这项工作中，我们回顾了将强化学习（RL）和大语言模型（LLM）结合起来的研究，并提出了一种新的三类分类方法，该分类方法基于这两种模型类型之间的交互方式。第一类是RL4LLM，包括利用RL改进与自然语言处理相关任务上LLM性能的研究。L4LLM分为两个子类，取决于RL是直接微调现有LLM还是改进LLM的提示。在第二类LLM4RL中，LLM辅助训练一个与自然语言无关的RL模型。我们进一步根据LLM辅助或替代RL训练框架的组件（奖励塑造、目标生成和策略函数）对LLM4RL进行了细分。最后，在第三类RL+LLM中，一个LLM和一个RL代理被嵌入其中。

    In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedde
    
[^189]: (A)我不是律师, 但是...: 向律师专家制定负责任的法律咨询的LLM政策

    (A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice

    [https://arxiv.org/abs/2402.01864](https://arxiv.org/abs/2402.01864)

    通过对法律领域进行调查，我们对使用大型语言模型（LLM）提供专业咨询的政策考虑进行了深入分析，通过基于案例的推理方法，从20名法律专家的讨论中提取了四个影响因素：用户属性、查询特征、AI能力和影响。

    

    大型语言模型(LLM)作为通用聊天机器人迅速扩散, 带来了扩大公众获得法律、医学和金融专业指导的希望, 同时引发了公众对LLM在重大事件中的依赖的担忧。之前的研究猜测了高层次的伦理考虑, 但缺乏具体的标准来确定LLM聊天机器人何时以及为什么应该或不应该提供专业帮助。通过研究法律领域, 我们进行了结构化的专家分析, 以发现关于使用LLM进行专业咨询的政策考虑的细微差别, 并采用案例推理的方法。我们与20名法律专家召开了研讨会, 并从样本用户查询("案例")中提取出适当的AI辅助的维度。我们将专业维度分为: (1)用户属性, (2)查询特征, (3)AI能力, 和 (4)影响。除了已知的问题, 如幻觉, 专家们还

    The rapid proliferation of large language models (LLMs) as general purpose chatbots available to the public raises hopes around expanding access to professional guidance in law, medicine, and finance, while triggering concerns about public reliance on LLMs for high-stakes circumstances. Prior research has speculated on high-level ethical considerations but lacks concrete criteria determining when and why LLM chatbots should or should not provide professional assistance. Through examining the legal domain, we contribute a structured expert analysis to uncover nuanced policy considerations around using LLMs for professional advice, using methods inspired by case-based reasoning. We convened workshops with 20 legal experts and elicited dimensions on appropriate AI assistance for sample user queries (``cases''). We categorized our expert dimensions into: (1) user attributes, (2) query characteristics, (3) AI capabilities, and (4) impacts. Beyond known issues like hallucinations, experts re
    
[^190]: DFML：分散式联邦互联学习

    DFML: Decentralized Federated Mutual Learning

    [https://arxiv.org/abs/2402.01863](https://arxiv.org/abs/2402.01863)

    DFML是一个无服务器的分散式联邦互联学习框架，能够有效地处理模型和数据的异质性，并通过相互学习在客户端之间传授知识，以获得更快的收敛速度和更高的全局准确性。

    

    在现实设备领域中，联邦学习（FL）中的集中式服务器存在通信瓶颈和容易受到单点故障的挑战。此外，现有设备固有地表现出模型和数据的异质性。现有工作缺乏一个能够适应此异质性且不施加架构限制或假定公共数据可用的分散式FL（DFL）框架。为了解决这些问题，我们提出了一个分散式联邦互联学习（DFML）框架，该框架是无服务器的，支持非限制性的异构模型，并避免依赖公共数据。DFML通过相互学习在客户端之间传授知识，并循环改变监督和提取信号的数量来有效处理模型和数据的异质性。广泛的实验结果表明，DFML在收敛速度和全局准确性方面具有一致的有效性，优于普遍存在的方法。

    In the realm of real-world devices, centralized servers in Federated Learning (FL) present challenges including communication bottlenecks and susceptibility to a single point of failure. Additionally, contemporary devices inherently exhibit model and data heterogeneity. Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such heterogeneity without imposing architectural restrictions or assuming the availability of public data. To address these issues, we propose a Decentralized Federated Mutual Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous models, and avoids reliance on public data. DFML effectively handles model and data heterogeneity through mutual learning, which distills knowledge between clients, and cyclically varying the amount of supervision and distillation signals. Extensive experimental results demonstrate consistent effectiveness of DFML in both convergence speed and global accuracy, outperforming prevalent b
    
[^191]: 参数特征迁移：一次性联邦学习与基础模型

    Parametric Feature Transfer: One-shot Federated Learning with Foundation Models

    [https://arxiv.org/abs/2402.01862](https://arxiv.org/abs/2402.01862)

    FedPFT 使用参数特征迁移提高了一次性联邦学习的准确性和通信效率，并在不同数据异质性设置中显示出改进效果。

    

    在一次性联邦学习中，客户端在一轮通信中共同训练一个全局模型。现有的一次性联邦学习方法在增强通信效率的同时损失了准确性。本文介绍了FedPFT（带参数特征迁移的联邦学习），这是一种利用基础模型的可迁移性来提高一次性联邦学习的准确性和通信效率的方法。该方法涉及从基础模型中提取的每个客户端参数模型（具体来说是高斯混合模型）的特征进行迁移。随后，每个参数模型被用来生成用于训练分类器头的合成特征。在八个数据集上的实验结果表明，FedPFT在集中和分散的联邦学习场景中以及在协变量转移和任务转移等多样的数据异质性设置中，增强了通信-准确性的边界，改进了最高达20.6%。

    In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additio
    
[^192]: 使用大型多模态模型解释生成模型的潜在表示

    Explaining latent representations of generative models with large multimodal models

    [https://arxiv.org/abs/2402.01858](https://arxiv.org/abs/2402.01858)

    该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    

    学习可解释的数据生成潜在因素表示是人工智能发展的重要课题。随着大型多模态模型的兴起，它可以将图像与文本对齐以生成答案。在本研究中，我们提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。我们进一步量化评估了我们生成的解释的不确定性，在多个大型多模态模型之间评估了解释生成的性能，并定性地可视化了每个潜在因素的变化，以学习不同生成模型对解释的解缠效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
    
[^193]: COMET: 使用增量图上下文表示生成提交消息

    COMET: Generating Commit Messages using Delta Graph Context Representation

    [https://arxiv.org/abs/2402.01841](https://arxiv.org/abs/2402.01841)

    COMET是一种利用增量图上下文表示的新方法，通过使用transformer模型生成高质量的提交消息，并引入可定制的质量保证模块。实验证明，COMET在各项指标上优于其他技术，并与流行的GPT模型具有可比性。

    

    提交消息解释了提交中的代码更改，并促进开发者之间的协作。已经提出了几种提交消息生成方法，然而这些方法在捕捉代码更改的上下文方面取得了有限的成功。我们提出了一种新颖的方法COMET（Context-Aware Commit Message Generation），通过使用基于图的表示捕捉代码更改的上下文，并利用基于transformer的模型生成高质量的提交消息。我们的方法使用我们开发的增量图有效地表示代码差异。我们还引入了一个可定制的质量保证模块来识别最佳的消息，减少了提交消息的主观性。实验证明，COMET在bleu-norm和meteor指标方面优于最先进的技术，并在rogue-l指标方面具有可比性。此外，我们将所提出的方法与流行的gpt-3.5-turbo模型以及最强大的GPT模型gpt-4-turbo进行了比较。

    Commit messages explain code changes in a commit and facilitate collaboration among developers. Several commit message generation approaches have been proposed; however, they exhibit limited success in capturing the context of code changes. We propose Comet (Context-Aware Commit Message Generation), a novel approach that captures context of code changes using a graph-based representation and leverages a transformer-based model to generate high-quality commit messages. Our proposed method utilizes delta graph that we developed to effectively represent code differences. We also introduce a customizable quality assurance module to identify optimal messages, mitigating subjectivity in commit messages. Experiments show that Comet outperforms state-of-the-art techniques in terms of bleu-norm and meteor metrics while being comparable in terms of rogue-l. Additionally, we compare the proposed approach with the popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable GPT model, ove
    
[^194]: SynthCLIP: 我们准备好开始完全合成的CLIP训练了吗？

    SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?

    [https://arxiv.org/abs/2402.01832](https://arxiv.org/abs/2402.01832)

    SynthCLIP是一种新的框架，用于训练完全合成的CLIP模型，通过生成大规模的合成图片和标题数据集，在性能上可以与在真实数据上训练的CLIP模型相媲美。

    

    我们提出了SynthCLIP，一种新颖的用于训练完全合成的CLIP模型的框架，与之前依赖真实数据的方法有着显著区别。借助最近的文本到图像生成网络和大型语言模型，我们能够生成任意规模的图像和相应的标题的合成数据集，无需人为干预。通过大规模的训练，SynthCLIP实现了与在真实数据集上训练的CLIP模型相当的性能。我们还介绍了SynthCI-30M，一个纯粹合成的数据集，包含3000万张带标题的图片。我们的代码、训练模型和生成的数据已经在https://github.com/hammoudhasan/SynthCLIP发布。

    We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
    
[^195]: LLM中的同行评审方法：开放环境下LLMs的自动评估方法

    Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment

    [https://arxiv.org/abs/2402.01830](https://arxiv.org/abs/2402.01830)

    本文提出了一种新的无监督评估方法，利用同行评审机制在开放环境中衡量LLMs。通过为每个LLM分配可学习的能力参数，以最大化各个LLM的能力和得分的一致性。结果表明，高层次的LLM能够更准确地评估其他模型的答案，并能够获得更高的响应得分。

    

    现有的大型语言模型（LLMs）评估方法通常集中于在一些有人工注释的封闭环境和特定领域基准上测试性能。本文探索了一种新颖的无监督评估方法，利用同行评审机制自动衡量LLMs。在这个设置中，开源和闭源的LLMs处于同一环境中，能够回答未标记的问题并互相评估，每个LLM的响应得分由其他匿名的LLMs共同决定。为了获取这些模型之间的能力层次结构，我们为每个LLM分配一个可学习的能力参数来调整最终排序结果。我们将其形式化为一个受约束的优化问题，旨在最大化每个LLM的能力和得分的一致性。背后的关键假设是高层次的LLM能够比低层次的LLM更准确地评估其他模型的答案，而高层次的LLM也可以达到较高的响应得分。

    Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
    
[^196]: 检索增强的端到端口语对话模型

    Retrieval Augmented End-to-End Spoken Dialog Models

    [https://arxiv.org/abs/2402.01828](https://arxiv.org/abs/2402.01828)

    这项研究介绍了一种检索增强的端到端口语对话模型（ReSLM），通过训练语音检索器获取音频中的文本实体，并将这些实体作为输入加入到模型中以提高性能。

    

    我们最近开发了SLM，一种融合预训练语音模型和大型语言模型（LLM）的联合语音和语言模型，同时保留了预训练LLM的上下文学习能力。在本文中，我们将SLM应用于语音对话应用，其中对话状态直接从音频信号中推断出来。面向任务的对话经常包含特定领域的实体，例如餐馆、酒店、火车站和城市名称，这些实体很难识别，但对于后续应用非常关键。受到检索增强生成（retrieval-augmented generation）范式的启发，我们提出了一种检索增强的SLM（ReSLM），克服了这个弱点。我们首先训练了一个语音检索器，用于检索音频中提到的文本实体。然后，将检索到的实体作为文本输入添加到底层的SLM中，以偏置模型的预测。我们在语音MultiWoz任务（DSTC-11挑战）上评估了ReSLM，并发现这种检索增强技术改进了模型的性能。

    We recently developed SLM, a joint speech and language model, which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to speech dialog applications where the dialog states are inferred directly from the audio signal.   Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to retrieve text entities mentioned in the audio. The retrieved entities are then added as text inputs to the underlying SLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that this retrieval augmentation bo
    
[^197]: 利用大型语言模型分析科学文献中血压变异的生物学性别差异

    Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature

    [https://arxiv.org/abs/2402.01826](https://arxiv.org/abs/2402.01826)

    本研究利用大型语言模型GPT-35-turbo，从PubMed的25 million个摘要中自动提取出男性和女性的血压平均值和标准差，填补了在考虑生物学性别差异的血压测量方差方面的研究空白。

    

    高血压被定义为高于正常的血压，它在公共卫生领域具有至关重要的意义，因为它是各种心血管疾病的重要先兆，并且显著 contributed to the elevated mortality rates worldwide。然而，许多现有的血压测量技术和标准可能存在偏差，因为它们未考虑临床结果、合并症或人口统计因素，使其在诊断目的上没有结论性。针对这些变量，在研究血压测量方差方面，有限的以数据驱动的研究。在本研究中，我们采用了GPT-35-turbo，一种大型语言模型（LLM），从PubMed的2500万个摘要的数据集中自动提取了男性和女性的血压平均值和标准差数值。符合我们预定义的纳入标准的摘要文章有993篇（即具有与血压、血压单位（如mmHg）和提及相关性的参考文献）。

    Hypertension, defined as blood pressure (BP) that is above normal, holds paramount significance in the realm of public health, as it serves as a critical precursor to various cardiovascular diseases (CVDs) and significantly contributes to elevated mortality rates worldwide. However, many existing BP measurement technologies and standards might be biased because they do not consider clinical outcomes, comorbidities, or demographic factors, making them inconclusive for diagnostic purposes. There is limited data-driven research focused on studying the variance in BP measurements across these variables. In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed. 993 article abstracts met our predefined inclusion criteria (i.e., presence of references to blood pressure, units of blood pressure such as mmHg, and mention
    
[^198]: 分形模式可能揭示下一个词预测中的智能

    Fractal Patterns May Unravel the Intelligence in Next-Token Prediction

    [https://arxiv.org/abs/2402.01825](https://arxiv.org/abs/2402.01825)

    通过研究语言的分形结构，我们发现语言具有自相似性和长程相关性，从段落到整个文档都存在相似的模式和依赖性。这些发现有助于理解如何通过预测下一个词来理解文本的多个层级结构。分形参数在预测性能方面优于困惑度。这些发现提供了对语言和机制的新视角。

    

    我们研究语言的分形结构，旨在提供一个精确的形式化方法来量化可能之前只有怀疑但尚未正式证明的属性。我们证明了语言具有以下特点：（1）自相似性，展示出各个层级上的复杂性，没有特定的特征上下文长度；（2）长程相关性（LRD），具有大约H=0.70的Hurst参数。基于这些发现，我们认为语言中的短期模式/依赖性，如段落中的模式/依赖性，反映了更大范围的模式/依赖性，如整个文档。这可能有助于理解下一个词预测如何导致对文本的多个层级结构，从单词和从句到更广泛的上下文和意图的理解。我们还证明了分形参数在预测下游性能方面优于基于困惑度的每字节比特（BPB）。我们希望这些发现能为语言和机制提供一种新的视角。

    We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.70. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can lead to a comprehension of the structure of text at multiple levels of granularity, from words and clauses to broader contexts and intents. We also demonstrate that fractal parameters improve upon perplexity-based bits-per-byte (BPB) in predicting downstream performance. We hope these findings offer a fresh perspective on language and the mechani
    
[^199]: 为大型语言模型构建防护措施

    Building Guardrails for Large Language Models

    [https://arxiv.org/abs/2402.01822](https://arxiv.org/abs/2402.01822)

    本文旨在为大型语言模型构建防护措施，并倡导采用系统化方法，通过与多学科团队合作来确定精确的技术要求，以减轻LLM的风险，并全面考虑不同LLM应用的多样化上下文。

    

    随着大型语言模型（LLM）越来越多地融入我们的日常生活中，识别和减轻它们的风险变得至关重要，特别是当这些风险对人类用户和社会产生深远影响时。防护措施，即过滤LLM的输入或输出，已经成为一种核心的安全技术。本文深入研究了当前的开源解决方案（Llama Guard，Nvidia NeMo，Guardrails AI），讨论了构建更完整解决方案的挑战和路径。基于前期研究的有力证据，我们倡导采用系统化方法构建LLM的防护措施，全面考虑不同LLM应用的多样化上下文。我们建议通过与多学科团队的合作，采用社会技术方法来确定精确的技术要求，探索面向需求复杂性的先进神经符号实现，并开展验证和测试。

    As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and t
    
[^200]: 生态合理的元学习推断解释人类类别学习

    Ecologically rational meta-learned inference explains human category learning

    [https://arxiv.org/abs/2402.01821](https://arxiv.org/abs/2402.01821)

    本研究提出了一种叫做生态合理的元学习推断（ERMI）的模型，通过使用大型语言模型生成与现实世界任务统计一致的认知任务，并通过元学习框架推导适应这些任务的理性主体。实验证明，ERMI模型在定性和定量上都更好地解释了人类的数据。

    

    生态合理性是指人类作为适应环境的理性主体。然而，由于两个方面的挑战，测试这个理论仍然具有挑战性：定义哪些任务在生态上是有效的以及为这些任务建立合理的模型。在这项工作中，我们证明了大型语言模型可以生成与现实世界任务统计一致的认知任务，特别是类别学习任务，从而解决了第一个挑战。我们通过利用元学习框架推导适应这些任务的合理性主体来解决第二个挑战，从而导致了一类模型，称为生态合理的元学习推断（ERMI）。ERMI在两个不同实验中以定量方式比其他七个认知模型更好地解释了人类数据。此外，它在定性上与人类行为相匹配：（1）它发现了与人类发现困难的相同任务，（2）它变得更依赖于基于样本的策略。

    Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy 
    
[^201]: LLMs无法规划，但可以在LLM-Modulo框架中帮助规划

    LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks

    [https://arxiv.org/abs/2402.01817](https://arxiv.org/abs/2402.01817)

    LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。

    

    关于大型语言模型（LLMs）在规划和推理任务中的角色存在很大的困惑。一方面有人过于乐观地声称只需正确提示或自我验证策略，LLMs就能完成这些任务。另一方面，也有人过于悲观地认为LLMs在规划/推理任务中仅能作为问题规范的简单翻译器，并将问题交给外部符号求解器。在这篇立场文章中，我们认为这两种极端观点都是错误的。我们认为自回归LLMs本身不能进行规划或自我验证（毕竟这是一种推理形式），并对文献中的误解原因进行了一些阐述。我们还将辩称LLMs应该被视为具有更有意义的角色的通用近似知识源，能在规划/推理任务中发挥更大的作用。

    There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
    
[^202]: 将LLMs的分解能力融入到紧凑语言模型中

    Distilling LLMs' Decomposition Abilities into Compact Language Models

    [https://arxiv.org/abs/2402.01812](https://arxiv.org/abs/2402.01812)

    本研究将LLMs的分解能力通过离线强化学习融入到紧凑模型中，通过开发AI生成的数据集和建立基准，突出了紧凑模型在复制复杂问题解决能力方面的潜力。

    

    大型语言模型（LLMs）展示了其推理能力，但其庞大的大小带来了可扩展性挑战，并限制了进一步的定制。相比之下，紧凑模型提供了定制化培训，但在解决复杂推理任务方面往往不足。本研究着重于使用离线强化学习将LLMs的分解能力融入到紧凑模型中。我们利用LLM能力的进步，提供反馈并生成专门用于训练紧凑模型的特定任务数据集。通过开发一个由AI生成的数据集和建立基准，我们的工作主要贡献在于强调了紧凑模型在复制复杂问题解决能力方面的潜力。

    Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization. In contrast, compact models offer customized training but often fall short in solving complex reasoning tasks. This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning. We leverage the advancements in the LLM`s capabilities to provide feedback and generate a specialized task-specific dataset for training compact models. The development of an AI-generated dataset and the establishment of baselines constitute the primary contributions of our work, underscoring the potential of compact models in replicating complex problem-solving skills.
    
[^203]: HQA-Attack: 面向高质量黑盒硬标签文本对抗攻击

    HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text

    [https://arxiv.org/abs/2402.01806](https://arxiv.org/abs/2402.01806)

    HQA-Attack是一种针对黑盒硬标签文本对抗攻击的方法，通过简单有效的框架生成高质量的对抗样本，解决了现有方法在有限的查询预算下难以生成具有高语义相似度和低扰动率的问题。

    

    针对文本的黑盒硬标签对抗攻击是一项实际且具有挑战性的任务，因为文本数据空间本质上是离散且不可微分的，只能访问到预测标签。目前对这个问题的研究还处于初级阶段，只有少数几种方法可用。然而，现有的方法依赖于复杂的启发式算法或不可靠的梯度估计策略，很可能陷入局部最优解，并且在有限的查询预算下难以生成具有高语义相似度和低扰动率的令人满意的对抗样本。为了解决上述问题，我们提出了一种简单而有效的框架，用于在黑盒硬标签攻击场景下生成高质量的文本对抗样本，名为HQA-Attack。具体来说，HQA-Attack首先随机初始化对抗样本，然后不断尽可能地反向替换原始单词，从而缩小扰动。

    Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the pert
    
[^204]: 探索大型语言模型中图推理的局限性

    Exploring the Limitations of Graph Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.01805](https://arxiv.org/abs/2402.01805)

    本文测试了5种不同的大型语言模型在图推理问题上的推理深度，并发现了LLMs的局限性、偏见和属性。我们发现LLMs对于节点遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务有负面影响，并且LLMs存在积极的回应偏差，无法识别有效解的缺失。我们还提出了一种新的图推理提示技术。

    

    预训练的大型语言模型仅通过基于语言的提示就展示了各种类型的推理能力。然而，在本文中，我们通过图推理问题测试了5种不同的大型语言模型（GPT-4，GPT-3.5，Claude-2，Llama-2和Palm-2）的推理深度。特别地，我们设计了10个不同的图遍历问题，每个问题代表着逐步增加的复杂性水平。此外，我们通过对不同图大小以及不同形式的k-shot提示的设置分析了模型的性能。通过这个基准测试过程，我们凸显了LLMs的各种局限性、偏见和属性，比如与每个节点的遍历自由度的平均度数呈反向关系，k-shot提示对图推理任务的整体负面影响，以及积极的回应偏差导致LLMs无法识别有效解的缺失。最后，我们提出一种新的提示技术，专门用于图推理。

    Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
    
[^205]: 冷链中物联网实施障碍的分析：一种集成的ISM-MICMAC和DEMATEL方法

    Analysis of Internet of Things implementation barriers in the cold supply chain: an integrated ISM-MICMAC and DEMATEL approach

    [https://arxiv.org/abs/2402.01804](https://arxiv.org/abs/2402.01804)

    本研究通过综述和调查研究物联网在冷链中的实施障碍，发现了13个关键障碍。其中，合规性和冷链网络是物联网采用策略的关键驱动因素。MICMAC和DEMATEL方法的应用有助于评估障碍之间的互动关系和因果关系。

    

    将物联网技术整合到冷链中可以提高透明度、效率和质量，优化运营流程并提高生产力。在复杂的环境下，物联网在冷链中的集成受到特定的障碍的阻碍，需要进行全面的研究。通过对物联网实施的相关文献进行综述，共发现了13个障碍。调查数据经过交叉验证以确保质量，并采用Cronbach's alpha测试来确保有效性。本研究在第一阶段应用解释性结构建模技术以识别主要障碍。在这些障碍中，“合规性”和“冷链网络”是物联网采用策略的关键驱动因素。MICMAC的驱动和依赖力量元素分类有助于评估障碍之间的互动。在本研究的第二阶段中，通过DEMATEL方法确定了所识别障碍之间的因果关系。

    Integrating Internet of Things (IoT) technology inside the cold supply chain can enhance transparency, efficiency, and quality, optimizing operating procedures and increasing productivity. The integration of IoT in this complicated setting is hindered by specific barriers that need a thorough examination. Prominent barriers to IoT implementation in the cold supply chain are identified using a two-stage model. After reviewing the available literature on the topic of IoT implementation, a total of 13 barriers were found. The survey data was cross-validated for quality, and Cronbach's alpha test was employed to ensure validity. This research applies the interpretative structural modeling technique in the first phase to identify the main barriers. Among those barriers, "regularity compliance" and "cold chain networks" are key drivers for IoT adoption strategies. MICMAC's driving and dependence power element categorization helps evaluate the barrier interactions. In the second phase of this
    
[^206]: 一个基于拍卖的联邦学习模型交易市场

    An Auction-based Marketplace for Model Trading in Federated Learning

    [https://arxiv.org/abs/2402.01802](https://arxiv.org/abs/2402.01802)

    本论文提出了一个基于拍卖的联邦学习模型交易市场，允许模型的买卖并通过适当定价和激励机制来提高模型性能和实现最大交易量。

    

    联邦学习（FL）越来越被认识到在使用本地分布数据训练模型方面的效力。然而，在这个合作过程中共享数据的适当估值仍未得到足够解决。在这项工作中，我们将FL设想为一个模型交易的市场，客户既是买家也是卖家，参与模型交易。这个FL市场允许客户通过出售自己的模型赚取货币奖励，并通过购买他人的模型来提高本地模型的性能。我们提出了一个基于拍卖的解决方案，以确保基于性能增益的适当定价。激励机制被设计出来鼓励客户真实地揭示他们对模型的价值评估。此外，我们引入了一个强化学习（RL）框架用于市场运营，旨在实现在动态和不断变化的市场状况下的最大交易量。在四个数据集上进行的实验结果表明，所提出的FL市场可以实现高交易收益和公平的模型定价。

    Federated learning (FL) is increasingly recognized for its efficacy in training models using locally distributed data. However, the proper valuation of shared data in this collaborative process remains insufficiently addressed. In this work, we frame FL as a marketplace of models, where clients act as both buyers and sellers, engaging in model trading. This FL market allows clients to gain monetary reward by selling their own models and improve local model performance through the purchase of others' models. We propose an auction-based solution to ensure proper pricing based on performance gain. Incentive mechanisms are designed to encourage clients to truthfully reveal their model valuations. Furthermore, we introduce a reinforcement learning (RL) framework for marketing operations, aiming to achieve maximum trading volumes under the dynamic and evolving market status. Experimental results on four datasets demonstrate that the proposed FL market can achieve high trading revenue and fai
    
[^207]: 大规模语言模型用于时间序列：一项调研

    Large Language Models for Time Series: A Survey

    [https://arxiv.org/abs/2402.01801](https://arxiv.org/abs/2402.01801)

    本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。

    

    大规模语言模型（LLM）在自然语言处理和计算机视觉等领域得到了广泛应用。LLM不仅仅局限于文本、图像和图形，还具有对时间序列数据进行分析的重要潜力，可以在气候、物联网、医疗、交通、音频和金融等领域受益。本调研论文对利用LLM进行时间序列分析的各种方法进行了深入探讨和详细分类。我们解决了LLM原始文本数据训练与数值型时间序列数据之间的差异挑战，并探索了将LLM的知识转移和提取到数值时间序列分析的策略。我们详细介绍了各种方法，包括（1）直接提示LLM，（2）时间序列量化，（3）对齐技术，（4）利用视觉方式作为桥接机制，和（5）结合LLM与工具。此外，本调研还提供了一系列涉及应用领域、评估方法和未来研究方向的讨论。

    Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
    
[^208]: 更快更轻的LLMs：当前挑战和未来发展的调查

    Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward

    [https://arxiv.org/abs/2402.01799](https://arxiv.org/abs/2402.01799)

    本调查文章概述了在提高LLM推理效果方面的最新方法和进展，通过实验评估不同压缩技术的有效性，并提出改进LLM推理效率的潜在未来方向。

    

    尽管LLMs表现出色，但由于推理过程中需要大量的计算和内存资源，它们的普及面临着挑战。最近在模型压缩和系统级优化方法方面的进展旨在增强LLM推理效果。本调查提供了这些方法的概述，强调了最近的发展。通过对LLaMA(/2)-7B的实验，我们评估了各种压缩技术，为在统一环境中高效部署LLM提供了实践见解。对LLaMA(/2)-7B的实证分析突出了这些方法的有效性。基于调查结果，我们确定了当前的局限性，并讨论了改善LLM推理效率的潜在未来方向。我们在https://github.com/nyunAI/Faster-LLM-Survey发布了用于复现本文结果的代码库。

    Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
    
[^209]: 变分量子电路增强的生成对抗网络

    Variational Quantum Circuits Enhanced Generative Adversarial Network

    [https://arxiv.org/abs/2402.01791](https://arxiv.org/abs/2402.01791)

    这项工作提出了一种基于变分量子电路的混合量子-经典架构（QC-GAN），通过在手写图像生成任务上表现出更好的性能来改进传统GAN。这个架构在收敛时具有更少的训练参数和迭代次数，并且利用了量子电路的纠缠和表达能力。

    

    生成对抗网络（GAN）是一种广泛应用于生成高质量图像、视频和音频内容等各种应用领域的机器学习框架。然而，对于大型神经网络，训练GAN可能会变得计算密集。在这项工作中，我们提出了一种改进GAN的混合量子-经典架构（QC-GAN）。通过在手写图像生成任务上使用MindSpore Quantum与传统GAN进行基准测试，我们对性能进行了数值检验。QC-GAN的生成器由量子变分电路和一层神经网络组成，判别器由传统神经网络组成。借助量子电路的纠缠和表达能力，我们的混合架构在收敛时比传统GAN具有更好的性能（Frechet Inception Distance），并且训练参数和迭代次数更少。我们还展示了其优越性。

    Generative adversarial network (GAN) is one of the widely-adopted machine-learning frameworks for a wide range of applications such as generating high-quality images, video, and audio contents. However, training a GAN could become computationally expensive for large neural networks. In this work, we propose a hybrid quantum-classical architecture for improving GAN (denoted as QC-GAN). The performance was examed numerically by benchmarking with a classical GAN using MindSpore Quantum on the task of hand-written image generation. The generator of the QC-GAN consists of a quantum variational circuit together with a one-layer neural network, and the discriminator consists of a traditional neural network. Leveraging the entangling and expressive power of quantum circuits, our hybrid architecture achieved better performance (Frechet Inception Distance) than the classical GAN, with much fewer training parameters and number of iterations for convergence. We have also demonstrated the superiori
    
[^210]: 一种用于机制可解释性的图形张量符号化的介绍

    An introduction to graphical tensor notation for mechanistic interpretability

    [https://arxiv.org/abs/2402.01790](https://arxiv.org/abs/2402.01790)

    图形张量符号化是一种简单的表示张量操作的方法，对于理解深度学习系统和理解神经网络行为具有重要意义。本论文介绍了图形张量符号化的方法，并将其应用于不同的分解和解释语言模型的基础方法中。

    

    图形张量符号化是一种简单的表示张量线性操作的方法，源自物理学。现代深度学习几乎完全由张量操作组成，因此理解张量操作对于理解这些系统非常重要。尤其是在试图反向工程神经网络学习的算法以理解其行为时，这一点尤为重要，这个领域被称为机制可解释性。在张量间进行的操作往往让人混淆，并且很难抓住整体结构，但图形张量符号化使得快速解析和发现有趣的等价关系更加容易。本文的前半部分介绍了这种符号化方法并将其应用于一些分解方法（SVD，CP，Tucker和张量网络分解），后半部分将其应用于一些现有的用于理解语言模型的基础方法，大致遵循

    Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely follow
    
[^211]: LLM的政治偏好分析

    The Political Preferences of LLMs

    [https://arxiv.org/abs/2402.01789](https://arxiv.org/abs/2402.01789)

    该研究对LLM的政治偏好进行了分析，结果发现大多数对话型LLM表现出左翼观点，但需谨慎解读基础模型在政治倾向测试中的分类。

    

    我们在这里报告了关于大型语言模型（LLMs）中内嵌的政治偏好的全面分析。具体而言，我们对24个最先进的对话型LLM进行了11项政治倾向测试，旨在确定测试者的政治偏好。结果表明，当使用具有政治含义的问题/陈述进行探究时，大多数对话型LLM倾向于生成被大多数政治测试仪器诊断为左翼观点的回答。我们注意到，这对于用于与人类对话优化的LLM基础模型并非如此。然而，基础模型在连贯回答问题方面表现不佳，需要对其政治倾向测试的分类进行谨慎解读。虽然还没有定论，但我们的结果为有趣的假设提供了初步证据，即政治偏好会嵌入其中。

    We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences
    
[^212]: LitLLM：科学文献综述工具包

    LitLLM: A Toolkit for Scientific Literature Review

    [https://arxiv.org/abs/2402.01788](https://arxiv.org/abs/2402.01788)

    "LitLLM: A Toolkit for Scientific Literature Review" 提出了一个基于 RAG 原则的工具包，通过使用专门的提示和指导技术，结合大型语言模型（LLM），实现了科学文献综述的自动化。这个工具包不仅可以通过转化摘要为关键词进行文献检索，还可以通过补充相关论文或关键词进行定制化的检索。

    

    进行科学论文的文献综述对于理解研究、其限制以及构建在现有工作基础上是必不可少的。这是一项繁琐的任务，因此自动文献综述生成器变得有吸引力。然而，许多使用大型语言模型（LLM）生成此类综述的现有工作存在显著限制。它们倾向于产生虚构的非实际信息，并忽略它们未受过训练的最新研究。为了解决这些限制，我们提出了一个基于检索增强生成（RAG）原则的工具包，在LLM的帮助下，使用专门的提示和指导技术。我们的系统首先通过将用户提供的摘要转化为关键词来进行网络搜索，以检索相关论文，其中使用了现成的LLM。作者可以通过补充相关论文或关键词来改进搜索，从而实现定制化的检索过程。其次，系统根据-

    Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on t
    
[^213]: 在文本到图像模型中的危害放大

    Harm Amplification in Text-to-Image Models

    [https://arxiv.org/abs/2402.01787](https://arxiv.org/abs/2402.01787)

    我们的研究提出了危害放大现象并发展了量化危害放大的方法，考虑模型输出的危害与用户输入的情境。我们还实证地研究了不同的方法在真实场景中的应用，并量化了由危害放大引起的性别之间的影响差异。

    

    文本到图像 (T2I) 模型已成为生成式人工智能的重要进展，然而，存在安全问题，即使用户输入看似安全的提示，这些模型也可能生成有害图像。这种现象称为危害放大，它比对抗提示更具潜在风险，使用户无意间遭受伤害。本文首先提出了危害放大的形式定义，并进一步贡献于开发用于量化危害放大的方法，考虑模型输出的危害与用户输入的情境。我们还经验性地研究了如何应用这些方法模拟真实世界的部署场景，包括量化由危害放大引起的不同性别之间的影响差异。我们的工作旨在为研究者提供工具去解决这个问题。

    Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
    
[^214]: COA-GPT：用于军事行动中加速行动方案开发的生成式预训练变压器

    COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations

    [https://arxiv.org/abs/2402.01786](https://arxiv.org/abs/2402.01786)

    COA-GPT是一种利用大型语言模型快速高效生成有效行动方案的算法，它融合了军事学说和领域专业知识，并在军事游戏中的实验中展示了其快速生成战略合理COAs的优势。

    

    军事行动中行动方案（COAs）的开发传统上是一个耗时且复杂的过程。针对这一挑战，本研究介绍了COA-GPT，一种利用大型语言模型（LLMs）快速高效生成有效COAs的新算法。COA-GPT通过上下文学习将军事学说和领域专业知识融入到LLMs中，允许指挥官输入任务信息（包括文本和图像格式），并获得与战略对齐的COAs以供审查和批准。独特的是，COA-GPT不仅加速了COA的开发，在几秒钟内生成初始COAs，还能根据指挥官的反馈实时精细化改进。本研究在《星际争霸II》游戏的军事相关场景中评估了COA-GPT，将其性能与最先进的强化学习算法进行了比较。我们的结果表明COA-GPT在更快生成战略合理的COAs方面具有优势。

    The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
    
[^215]: DoubleMLDeep: 利用多模态数据对因果效应进行估计

    DoubleMLDeep: Estimation of Causal Effects with Multimodal Data

    [https://arxiv.org/abs/2402.01785](https://arxiv.org/abs/2402.01785)

    本文提出了一个利用文本和图像在因果推断和治疗效应估计中的双机器学习框架，并提出了一种生成半合成数据集的方法用于评估因果效应估计的性能。这些方法和架构在半合成数据集上进行了评估，并与标准方法进行了比较，显示了直接使用文本和图像进行因果研究的潜在好处。

    

    本文探讨了在因果推断和治疗效应估计中使用非结构化的多模态数据，即文本和图像。我们提出了一种适应于双机器学习（DML）框架，特别是部分线性模型的神经网络架构。我们论文的另一个贡献是提出了一种生成半合成数据集的新方法，该方法可用于评估在文本和图像作为混淆因素的情况下因果效应估计的性能。我们在半合成数据集上评估并与标准方法进行比较，突出了直接在因果研究中使用文本和图像的潜在好处。我们的研究结果对经济学、市场营销、金融、医学和数据科学等领域的研究人员和从业者具有重要意义，他们希望使用非传统数据估计因果数量。

    This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.
    
[^216]: 在线疫苗关注的分层多标签分类

    Hierarchical Multi-Label Classification of Online Vaccine Concerns

    [https://arxiv.org/abs/2402.01783](https://arxiv.org/abs/2402.01783)

    本文研究了在线疫苗关注的分层多标签分类任务，使用大型语言模型在零样本设置下检测疫苗关注，同时探索了不同提示策略的成本和准确性权衡，并提供了指导当前应用程序系统设计的具体经验教训。

    

    疫苗关注是一个不断发展的目标，可以在COVID-19大流行中快速变化。通过识别疫苗关注和错误信息的长期趋势，可以帮助公共卫生努力在资源或信息宣传上进行战略性分配。我们在零样本设置中使用大型语言模型（LLM）探索在在线讨论中检测疫苗关注的任务，无需昂贵的训练数据集。由于实时监控在线来源需要大规模推理，我们探索了不同提示策略的成本和准确性之间的权衡，并提供了可以为当前应用程序的系统设计选择提供信息的具体经验教训。对不同提示策略的分析表明，通过LLM多次进行分类，每次通过布尔问题判断文本是否提到疫苗关注，效果最好。我们的结果表明，GPT-4能够明显优于其他模型。

    Vaccine concerns are an ever-evolving target, and can shift quickly as seen during the COVID-19 pandemic. Identifying longitudinal trends in vaccine concerns and misinformation might inform the healthcare space by helping public health efforts strategically allocate resources or information campaigns. We explore the task of detecting vaccine concerns in online discourse using large language models (LLMs) in a zero-shot setting without the need for expensive training datasets. Since real-time monitoring of online sources requires large-scale inference, we explore cost-accuracy trade-offs of different prompting strategies and offer concrete takeaways that may inform choices in system designs for current applications. An analysis of different prompting strategies reveals that classifying the concerns over multiple passes through the LLM, each consisting a boolean question whether the text mentions a vaccine concern or not, works the best. Our results indicate that GPT-4 can strongly outpe
    
[^217]: 使用不同局部性对脉冲神经网络学习方法进行基准测试

    Benchmarking Spiking Neural Network Learning Methods with Varying Locality

    [https://arxiv.org/abs/2402.01782](https://arxiv.org/abs/2402.01782)

    本研究使用不同局部性对脉冲神经网络学习方法进行基准测试，并发现这些方法在性能和生物学合理性之间存在权衡。此外，研究还探讨了SNN的隐式循环特性。

    

    脉冲神经网络（SNN）提供更真实的神经动力学，在多个机器学习任务中已经显示出与人工神经网络（ANN）相当的性能。信息在SNN中以脉冲形式进行处理，采用事件驱动机制，显著降低了能源消耗。然而，由于脉冲机制的非可微性，训练SNN具有挑战性。传统方法如时间反向传播（BPTT）已经显示出一定的效果，但在计算和存储成本方面存在问题，并且在生物学上不可行。相反，最近的研究提出了具有不同局部性的替代学习方法，在分类任务中取得了成功。本文表明，这些方法在训练过程中有相似之处，同时在生物学合理性和性能之间存在权衡。此外，本研究还探讨了SNN的隐式循环特性，并进行了调查。

    Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but comes with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, this research examines the implicitly recurrent nature of SNNs and investigat
    
[^218]: 当基准成为目标：揭示大型语言模型排行榜的敏感性

    When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards

    [https://arxiv.org/abs/2402.01781](https://arxiv.org/abs/2402.01781)

    依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。

    

    基于基准排名的大型语言模型(LLM)排行榜经常被用来指导实践者在模型选择中。通常，发布的排行榜排名被直接接受 - 我们表明这是一个（潜在昂贵的）错误。在现有的排行榜下，LLM的相对性能对（通常微小的）细节非常敏感。我们展示了对于流行的多项选择题基准（例如MMLU），对基准的微小扰动，如改变选项顺序或答案选择方法，会导致排名变化达到8个位置。我们通过对三个广泛的基准扰动类别进行系统实验并确定这一行为的来源来解释这一现象。我们的分析得出了几个最佳实践建议，包括选择优化的混合评分方法来进行答案选择。我们的研究强调了依赖简单基准评估的风险，并为更健壮的模型评估提供了指导道路。

    Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
    
[^219]: GPT-4的心理学研究：适度焦虑、略带男性化、诚实谦逊

    On the Psychology of GPT-4: Moderately anxious, slightly masculine, honest, and humble

    [https://arxiv.org/abs/2402.01777](https://arxiv.org/abs/2402.01777)

    GPT-4在心理测试中展现出适度焦虑、略带男性化、诚实谦逊的特点，并且在数值素养和语言任务上表现出超过人类平均水平的认知反思能力。

    

    我们对GPT-4进行了一系列严格的心理测量测试并分析结果。我们发现，与普通人相比，GPT-4更倾向于展现出更多的诚实和谦逊，较少的马基雅维利主义和自恋。它有时表现出矛盾的性别偏见，略带男性特征，适度焦虑但大多不抑郁（但不总是）。它在数值素养上表现为与人类平均水平相符，并且在语言任务上的认知反思能力高于人类的平均水平。

    We subject GPT-4 to a number of rigorous psychometric tests and analyze the results. We find that, compared to the average human, GPT-4 tends to show more honesty and humility, and less machiavellianism and narcissism. It sometimes exhibits ambivalent sexism, leans slightly toward masculinity, is moderately anxious but mostly not depressive (but not always). It shows human-average numerical literacy and has cognitive reflection abilities that are above human average for verbal tasks.
    
[^220]: 解开多语言机器翻译中目标端转移和正则化的作用

    Disentangling the Roles of Target-Side Transfer and Regularization in Multilingual Machine Translation

    [https://arxiv.org/abs/2402.01772](https://arxiv.org/abs/2402.01772)

    本文通过大规模研究展示了多语言机器翻译中目标端转移的动态影响。我们发现，语言相似的辅助目标语言具有强大的正向知识转移能力，并且随着相似目标语言规模的增加，转移效果进一步增强。同时，远离的辅助目标语言也可以意外地对主要语言对产生正向转移效果。

    

    多语言机器翻译(MMT)在不同语言对之间的知识转移中受益。然而，一对多翻译相比于多对一翻译的改进仅有微小甚至可忽略不计。这种性能差异引发了一个问题：在一对多MT中，正向转移在目标端的作用程度如何。在本文中，我们进行了一项大规模研究，通过语言相似性和语料库大小这两个维度变化辅助目标语言，以展示知识转移对主要语言对的动态影响。我们发现，语言相似的辅助目标语言表现出强大的正向知识转移能力。随着相似目标语言规模的增加，正向转移进一步增强，使主要语言对受益。同时，我们发现远离的辅助目标语言也可以出乎意料地使主要语言对受益，即使正向转移最小也是如此。

    Multilingual Machine Translation (MMT) benefits from knowledge transfer across different language pairs. However, improvements in one-to-many translation compared to many-to-one translation are only marginal and sometimes even negligible. This performance discrepancy raises the question of to what extent positive transfer plays a role on the target-side for one-to-many MT. In this paper, we conduct a large-scale study that varies the auxiliary target side languages along two dimensions, i.e., linguistic similarity and corpus size, to show the dynamic impact of knowledge transfer on the main language pairs. We show that linguistically similar auxiliary target languages exhibit strong ability to transfer positive knowledge. With an increasing size of similar target languages, the positive transfer is further enhanced to benefit the main language pairs. Meanwhile, we find distant auxiliary target languages can also unexpectedly benefit main language pairs, even with minimal positive trans
    
[^221]: BlackMamba: 混合专家模型的状态空间模型

    BlackMamba: Mixture of Experts for State-Space Models

    [https://arxiv.org/abs/2402.01771](https://arxiv.org/abs/2402.01771)

    BlackMamba是一种结合了Mamba SSM和MoE的新型架构，它具有竞争力的性能和较低的推断和训练成本，在大规模语言建模领域具有潜在应用价值。

    

    最近的研究表明，状态空间模型（SSMs）在大规模语言建模基准测试中表现出与transformer竞争力的性能，同时，其时间和内存复杂度与序列长度成线性关系。最近发布的SSM模型Mamba在语言建模和处理长序列任务方面表现出色。与此同时，专家混合模型（MoE）在显著降低推断计算和延迟成本的同时，也增加了更大的内存占用。本文提出了一种名为BlackMamba的新型架构，将Mamba SSM与MoE相结合，以获得两者的好处。我们证明BlackMamba在Mamba和transformer基准测试中表现出竞争力，并在推断和训练FLOPs方面表现出色。我们在自定义数据集的300B标记上全面训练并开源了340M/1.5B和630M/2.8B的BlackMamba模型。我们展示了BlackMamba继承并结合了这两种模型的优势。

    State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits
    
[^222]: 将人格化聊天机器人和布鲁姆税目扩展到课堂上的互动科学展品

    Extending Interactive Science Exhibits into the Classroom using Anthropomorphized Chatbots and Bloom's Taxonomy

    [https://arxiv.org/abs/2402.01770](https://arxiv.org/abs/2402.01770)

    本研究探索了使用生成型人工智能聊天机器人将公共科学展品转化为虚拟体验，以提高展品在课堂上的可访问性和参与度。研究还发现使用布鲁姆税目生成问题的交互式评估可能与这些聊天机器人结合使用。

    

    本研究探讨了使用生成型人工智能聊天机器人将公共科学展品转化为虚拟体验，以扩展展品在课堂上的参与度。更广泛的目标是增加科学展品的可访问性，尤其是对那些由于各种因素，包括文化壁垒而边缘化在STEM领域的人群。我们假设将展品转化为第一人称人格化聊天机器人，具有个性，如古怪的对话小行星或彗星，可增加参与度和学习。本文主要探讨了仅通过提示工程使用生成型人工智能（如GPT）是否可能实现这些技术。研究包括对使用布鲁姆税目生成问题进行交互式评估的可能性的调查。初步结果表明，结合这些技术是可能的。因此，它为将来在课堂上评估这种聊天机器人的整体效力奠定了基础。

    This study explores the use of Generative AI chatbots for transforming public science exhibits into virtual experiences that can extend the engagement of exhibits into the classroom. The broader goal is to increase accessibility of science exhibits, especially for those marginalized in STEM due to various factors, including cultural barriers. We hypothesize that turning exhibits into first-person anthropomorphized chatbots with a personality, like quirky-talking asteroids or comets, can increase engagement and learning. The paper mainly explores if such techniques are possible using Generative AI (e.g. GPT) via prompt engineering alone. The research includes an investigation into the possibility of integrating interactive assessment via question-generation using Bloom's Taxonomy. Initial results indicate that it is possible to combine these techniques. As such, it lays a foundation for future classroom evaluations of such chatbots to gauge their overall efficacy in extending the reach 
    
[^223]: 重新定义LLMs中的“幻觉”：构建心理学为基础的减轻误导的框架

    Redefining "Hallucination" in LLMs: Towards a psychology-informed framework for mitigating misinformation

    [https://arxiv.org/abs/2402.01769](https://arxiv.org/abs/2402.01769)

    该研究旨在重新定义LLMs中的“幻觉”，提出了心理学分类法，以更详细地理解和解决LLMs输出误导信息的挑战。通过借鉴人类处理类似挑战的方式，研究旨在开发策略以减轻LLMs中的幻觉。

    

    近年来，大型语言模型（LLMs）变得非常受欢迎，例如ChatGPT已经被超过十亿的用户使用。虽然这些模型展现出了出色的语言理解和逻辑能力，但在“幻觉”方面存在一个显著挑战。这一现象导致LLMs以自信的方式输出误导信息，而这可以在如此庞大的用户群体中产生灾难性后果。然而，我们对LLMs中使用“幻觉”一词的适当性提出了质疑，并提出了基于认知偏差和其他心理现象的心理分类法。我们的方法提供了对这一现象更详细的理解，从而能够提供有针对性的解决方案。通过利用人类内部解决类似挑战的见解，我们旨在开发策略来减轻LLMs的幻觉。这种跨学科的方法旨在超越传统的术语，为深入理解和可操作的路径提供细致入微的认识。

    In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of "hallucinations." This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term "hallucination" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for imp
    
[^224]: HiQA：一种用于大规模文档问答的分层上下文增强的RAG模型

    HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA

    [https://arxiv.org/abs/2402.01767](https://arxiv.org/abs/2402.01767)

    HiQA是一个先进的多文档问答框架，使用分层的上下文增强和多路径检索机制，解决了大规模文档问答中的检索准确性问题，并在多文档环境中展示了最先进的性能。

    

    随着利用外部工具的语言模型代理迅速发展，使用补充文档和检索增强生成（RAG）方法的问答（QA）方法学取得了重要进展。这种进步提高了语言模型的回答质量，并减轻了幻觉的出现。然而，当面临大量无法区分的文档时，这些方法在检索准确性方面表现有限，给实际应用带来了显著挑战。针对这些新兴的挑战，我们提出了HiQA，这是一个先进的多文档问答（MDQA）框架，将级联的元数据整合到内容中，同时具备多路径检索机制。我们还发布了一个名为MasQA的基准来评估和研究MDQA。最后，HiQA在多文档环境中展示了最先进的性能。

    As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
    
[^225]: LLM投票：人类选择和AI集体决策

    LLM Voting: Human Choices and AI Collective Decision Making

    [https://arxiv.org/abs/2402.01766](https://arxiv.org/abs/2402.01766)

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并与人类投票模式进行了对比。我们的方法包括进行人类投票实验以建立人类偏好的基准，并与LLM代理进行平行实验。研究聚焦于集体结果和个体偏好，揭示了人类和LLMs之间在决策和固有偏见方面的差异。我们观察到LLMs在偏好多样性和一致性之间存在权衡，相比人类选民的多样偏好，LLMs有更趋向于一致选择的倾向。这一发现表明，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
    
[^226]: LLMs 模拟五大人格特质：进一步的证据

    LLMs Simulate Big Five Personality Traits: Further Evidence

    [https://arxiv.org/abs/2402.01765](https://arxiv.org/abs/2402.01765)

    本研究旨在研究大型语言模型（LLMs）如何模拟五大人格特质，并分析了其模拟的人格特质及稳定性，有助于深入了解LLMs在个性化人机交互方面的潜力。

    

    本研究探讨了大型语言模型（LLMs）如Llama2、GPT4和Mixtral对五大人格特质的模拟，并分析了这些模型模拟的人格特质及其稳定性。这有助于更广泛地了解LLMs模拟人格特质的能力以及对个性化人机交互的相关影响。

    An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.
    
[^227]: 当大型语言模型遇上向量数据库：一项综述

    When Large Language Models Meet Vector Databases: A Survey

    [https://arxiv.org/abs/2402.01763](https://arxiv.org/abs/2402.01763)

    本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。

    

    最近大型语言模型的突破在人类文字处理和生成方面开启了新的领域。然而，随着它们的显著增长，大型语言模型面临着包括幻觉、偏见、实时知识更新以及在商业环境中实施和维护的高成本等重要挑战。而另一种日益流行的工具，向量数据库则为这些挑战提供了潜在的解决方案。这些数据库擅长处理高维数据，并且对于高效的信息检索和语义搜索等任务至关重要。通过与大型语言模型的整合，它们显著增强了人工智能系统管理和更有效地利用多样数据的能力。本综述论文对大型语言模型和向量数据库之间的交叉点进行了深入而独特的分析。

    The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
    
[^228]: 商业人工智能、冲突和道德责任：关于双重用途人工智能技术所涉及的道德责任的理论分析和实践方法

    Commercial AI, Conflict, and Moral Responsibility: A theoretical analysis and practical approach to the moral responsibilities associated with dual-use AI technology

    [https://arxiv.org/abs/2402.01762](https://arxiv.org/abs/2402.01762)

    本文对开发非军事应用中可能被用于冲突的人工智能系统时的道德责任进行了理论分析和实践方法的探讨，并认为利益相关者对于合理可预见的系统使用负有道德责任。

    

    本文提出了关于开发非军事应用中可能被用于冲突应用的人工智能系统时涉及的道德责任的理论分析和实践方法。我们认为，人工智能代表了一种与之前的双重或多重用途技术不同的交叉技术，因为它在其他技术上具有乘法效应。因此，现有关于双重用途技术的道德责任分析不一定适用于人工智能系统。我们认为，参与人工智能系统生命周期的利益相关者对于他们系统的合理可预见的使用负有道德责任。核心思想是，一个行动主体的道德责任不仅仅取决于他们的意图，我们还必须考虑行动主体合理可预见的行动结果，如系统在冲突中的潜在用途等。

    This paper presents a theoretical analysis and practical approach to the moral responsibilities when developing AI systems for non-military applications that may nonetheless be used for conflict applications. We argue that AI represents a form of crossover technology that is different from previous historical examples of dual- or multi-use technology as it has a multiplicative effect across other technologies. As a result, existing analyses of ethical responsibilities around dual-use technologies do not necessarily work for AI systems. We instead argue that stakeholders involved in the AI system lifecycle are morally responsible for uses of their systems that are reasonably foreseeable. The core idea is that an agent's moral responsibility for some action is not necessarily determined by their intentions alone; we must also consider what the agent could reasonably have foreseen to be potential outcomes of their action, such as the potential use of a system in conflict even when it is n
    
[^229]: 在大语言模型时代重新思考可解释性

    Rethinking Interpretability in the Era of Large Language Models

    [https://arxiv.org/abs/2402.01761](https://arxiv.org/abs/2402.01761)

    大语言模型具有以自然语言解释的能力，能够重新定义解释性，并且在多个应用中展示出巨大潜力。

    

    可解释的机器学习在过去十年中成为一个热门领域，受到越来越大的数据集和深度神经网络的崛起的推动。与此同时，大语言模型（LLMs）在各种任务中展示出了卓越的能力，为重新思考可解释机器学习的机会提供了可能。值得注意的是，以自然语言解释的能力使得LLMs能够扩展给人类的规模和复杂性上的模式。然而，这些新的能力也带来了新的挑战，比如虚构的解释和巨大的计算成本。在这篇立场论文中，我们首先回顾了评估新兴LLM解释领域的现有方法（包括解释LLM和使用LLM进行解释）。我们认为，尽管存在局限性，LLMs能够重新定义解释性，涵盖更广泛的应用领域，包括对LLMs本身的审计。

    Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high
    
[^230]: 一个多模态、可解释的AI驱动聊天机器人辅导系统中的信任和伦理考虑：以协同解决魔方为例

    Trust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik's Cube

    [https://arxiv.org/abs/2402.01760](https://arxiv.org/abs/2402.01760)

    该论文描述了一个多模态、可解释的AI驱动聊天机器人辅导系统，在解决高中学生协同解决魔方问题时，解决了数据隐私、信息泄露、滥用语言和公平性等伦理和信任问题。

    

    人工智能具有从大量关于学生学习模式的数据中发现洞察力的潜力，有望改变教育。然而，人工智能的伦理和信任问题已经引起了关注，但尚未解决。在高中人工智能教育中，突出的伦理问题包括数据隐私、信息泄露、滥用语言和公平性。本文描述了为解决高中学生与人工智能协作解决魔方的伦理和信任问题而构建的技术组件（称为ALLURE chatbot）。在数据隐私方面，我们希望确保儿童、父母和教师的知情同意处于任何管理的数据的中心位置。由于涉及儿童，系统能够接受用户和人工智能提供的文本、音频或视觉语言，并将互动引导远离危险情况。在信息管理方面，我们还希望确保系统可以利用机制防止信息泄露的危险。

    Artificial intelligence (AI) has the potential to transform education with its power of uncovering insights from massive data about student learning patterns. However, ethical and trustworthy concerns of AI have been raised but are unsolved. Prominent ethical issues in high school AI education include data privacy, information leakage, abusive language, and fairness. This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube. In data privacy, we want to ensure that the informed consent of children, parents, and teachers, is at the center of any data that is managed. Since children are involved, language, whether textual, audio, or visual, is acceptable both from users and AI and the system can steer interaction away from dangerous situations. In information management, we also want to ensure that the sys
    
[^231]: 系统性文献综述：用于幽默风格分类的计算方法

    Systematic Literature Review: Computational Approaches for Humour Style Classification

    [https://arxiv.org/abs/2402.01759](https://arxiv.org/abs/2402.01759)

    这项研究通过系统性文献综述展示了计算方法在幽默风格分类中的应用情况，并指出了当前的研究空白和有希望的方向。

    

    理解各种幽默风格对于理解幽默的多面性及其在心理学和人工智能等领域的影响至关重要。这种理解揭示了依据所采用的风格，幽默可以对个人的健康和人际关系产生治疗或有害的影响。虽然专门研究基于计算的幽默风格分析的研究仍然比较少见，但在相关任务中，特别是二元幽默和讽刺识别方面，已有大量研究蓬勃发展。在这项系统性文献综述中，我们调查了应用于这些相关任务的计算技术的现状，并揭示了它们与幽默风格分析的基本相关性。通过这项研究，我们揭示了常见的方法，阐明了各种数据集和评估指标，并有效地引导幽默研究的复杂领域。我们的努力确定了潜在的研究空白，并提出了有希望的方向。

    Understanding various humour styles is essential for comprehending the multifaceted nature of humour and its impact on fields such as psychology and artificial intelligence. This understanding has revealed that humour, depending on the style employed, can either have therapeutic or detrimental effects on an individual's health and relationships. Although studies dedicated exclusively to computational-based humour style analysis remain somewhat rare, an expansive body of research thrives within related task, particularly binary humour and sarcasm recognition. In this systematic literature review (SLR), we survey the landscape of computational techniques applied to these related tasks and also uncover their fundamental relevance to humour style analysis. Through this study, we unveil common approaches, illuminate various datasets and evaluation metrics, and effectively navigate the complex terrain of humour research. Our efforts determine potential research gaps and outlined promising di
    
[^232]: Aalap：印度法律和法律助理功能的人工智能助手

    Aalap: AI Assistant for Legal & Paralegal Functions in India

    [https://arxiv.org/abs/2402.01758](https://arxiv.org/abs/2402.01758)

    Aalap是一个在印度法律任务上针对指令数据进行微调的AI助手，相比于gpt-3.5-turbo在测试数据中表现更好，主要教授法律推理，对律师、法官或在法律系统中工作的人有帮助。

    

    在法律任务中使用专有的大型语言模型面临数据隐私问题、领域数据异构性、领域知识复杂性和领域目标独特性的挑战。我们创建了Aalap，它是在与印度特定法律任务相关的指令数据上经过微调的Mistral 7B模型。 Aalap在我们的测试数据中比gpt-3.5-turbo表现更好的比例为31％，在34％的测试数据中与GPT4评估得分相当。 Aalap的训练主要侧重于教授法律推理而不是法律记忆。 Aalap对律师、法官或在法律系统中工作的人的日常活动肯定有所帮助。

    Using proprietary Large Language Models on legal tasks poses challenges due to data privacy issues, domain data heterogeneity, domain knowledge sophistication, and domain objectives uniqueness. We created Aalalp, a fine-tuned Mistral 7B model on instructions data related to specific Indian legal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\% of our test data and obtains an equivalent score in 34\% of the test data as evaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning rather than legal recall. Aalap is definitely helpful for the day-to-day activities of lawyers, judges, or anyone working in legal systems.
    
[^233]: 通过分析音频识别辱骂言论和假消息，辨别僞作和仇恨言论在僧伽罗语YouTube视频中

    Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio

    [https://arxiv.org/abs/2402.01752](https://arxiv.org/abs/2402.01752)

    该研究提出了一种通过分析音频来辨别僞作和仇恨言论在僧伽罗语YouTube视频中的解决方案。这些方法包括评估视频是否包含虚假信息以及检测其中是否存在仇恨言论。

    

    YouTube面临着全球范围内虚假信息和仇恨言论的传播危机。为解决这些问题，YouTube已实施严格规定，禁止上传包含虚假信息或宣传仇恨言论的内容。虽然已经进行了许多降低冒犯性英语内容的研究，但对僧伽罗语内容的研究仍然相对较少。本研究旨在通过提出解决方案，减少僧伽罗语YouTube视频中暴力和虚假信息的传播。该方法包括开发一个评级系统，通过比较标题和描述与音频内容，评估视频是否包含虚假信息，并检测其中是否存在仇恨言论。方法包括使用Pytube库进行音频提取，使用经过微调的Whisper模型进行音频转录，使用distilroberta-base模型进行仇恨言论检测和文本分类。

    YouTube faces a global crisis with the dissemination of false information and hate speech. To counter these issues, YouTube has implemented strict rules against uploading content that includes false information or promotes hate speech. While numerous studies have been conducted to reduce offensive English-language content, there's a significant lack of research on Sinhala content. This study aims to address the aforementioned gap by proposing a solution to minimize the spread of violence and misinformation in Sinhala YouTube videos. The approach involves developing a rating system that assesses whether a video contains false information by comparing the title and description with the audio content and evaluating whether the video includes hate speech. The methodology encompasses several steps, including audio extraction using the Pytube library, audio transcription via the fine-tuned Whisper model, hate speech detection employing the distilroberta-base model and a text classification L
    
[^234]: ChatGPT与Bard在检测阿尔茨海默病痴呆的性能评估

    Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia

    [https://arxiv.org/abs/2402.01751](https://arxiv.org/abs/2402.01751)

    本研究评估了ChatGPT和Bard在识别阿尔茨海默病痴呆的能力，并发现Bard在积极识别AD方面表现最好，具有较高的召回率和F1得分，但可能会将CN错误地识别为AD。对于积极识别CN，GPT-4表现出最高的真阴性。

    

    大型语言模型（LLM）在许多领域中有着越来越多的应用。本研究评估了三个LLM聊天机器人（ChatGPT-3.5，ChatGPT-4和Bard）在其当前公开形式下，使用从自发语音记录中提取的文本输入，对阿尔茨海默病痴呆（AD）和认知正常（CN）个体进行识别的能力。采用零样本学习方法，在两个独立查询级别上进行，第二个查询（思维链引导）比第一个查询产生更详细的结果。通过评估每个LLM聊天机器人在准确度、敏感度、特异度、精确度和F1得分方面生成的预测来评估LLM聊天机器人的性能。LLM聊天机器人生成了三类结果（"AD"，"CN"或"Unsure"）。在积极识别AD时，Bard产生了最高的真阳性（89%的召回率）和最高的F1得分（71%），但倾向于将CN错误地识别为AD，并具有较高的置信度（较低的"Unsure"率）；在积极识别CN时，GPT-4产生了最高的真阴性。

    Large language models (LLMs) find increasing applications in many fields. Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in their current form, as publicly available - for their ability to recognize Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual input derived from spontaneous speech recordings. Zero-shot learning approach is used at two levels of independent queries, with the second query (chain-of-thought prompting) eliciting more detailed than the first. Each LLM chatbot's performance is evaluated on the prediction generated in terms of accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots generated three-class outcome ("AD", "CN", or "Unsure"). When positively identifying AD, Bard produced highest true-positives (89% recall) and highest F1 score (71%), but tended to misidentify CN as AD, with high confidence (low "Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest true-negatives 
    
[^235]: PACE：利用大型语言模型增强通信效率的实用智能体

    PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models

    [https://arxiv.org/abs/2402.01750](https://arxiv.org/abs/2402.01750)

    PACE是一种利用大型语言模型的实用智能体，提供了基于图像的实用通信框架。它通过语义感知、意图解析和以意图为导向的编码来增强通信效率。为了有效利用语言模型，它使用知识库补充所需知识，引入专用提示来促进对实用通信场景和任务要求的理解，并设计思维链以帮助权衡传输效率和质量。

    

    当前通信技术在理论容量、频谱可用性和功耗资源方面存在限制。实用通信利用终端智能进行选择性数据传输，提供资源节约。现有研究缺乏通用意图解析工具，限制了其适用性。本文提出了一种基于大型语言模型的实用智能体（PACE）的图像实用通信框架。在该框架下，PACE依次执行语义感知、意图解析和以意图为导向的编码。为了确保在通信中有效使用大型语言模型，设计了一个知识库来补充所需的知识，引入了专用提示来促进对实用通信场景和任务要求的理解，并设计了一条思维链以帮助在传输效率和质量之间进行合理的权衡。

    Current communication technologies face limitations in terms of theoretical capacity, spectrum availability, and power resources. Pragmatic communication, leveraging terminal intelligence for selective data transmission, offers resource conservation. Existing research lacks universal intention resolution tools, limiting applicability to specific tasks. This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE sequentially performs semantic perception, intention resolution, and intention-oriented coding. To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and c
    
[^236]: 迈向城市智能：城市基础模型综述与展望

    Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models

    [https://arxiv.org/abs/2402.01749](https://arxiv.org/abs/2402.01749)

    本文综述了城市基础模型在智能城市发展中的重要性和潜力，并提出了一个以数据为中心的分类方法。这个新兴领域面临着一些挑战，如缺乏清晰的定义和系统性的综述，需要进一步的研究和解决方案。

    

    机器学习技术现已成为智能城市服务进步的核心，对提高城市环境的效率、可持续性和宜居性起到至关重要的作用。最近出现的ChatGPT等基础模型在机器学习和人工智能领域标志着一个革命性的转变。它们在上下文理解、问题解决和适应各种任务方面的无与伦比的能力表明，将这些模型整合到城市领域中可能对智能城市的发展产生变革性影响。尽管对城市基础模型（UFMs）的兴趣日益增长，但这个新兴领域面临着一些挑战，如缺乏清晰的定义、系统性的综述和可普遍化的解决方案。为此，本文首先介绍了UFM的概念，并讨论了构建它们所面临的独特挑战。然后，我们提出了一个以数据为中心的分类方法，对当前与UFM相关的工作进行了分类。

    Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces challenges such as a lack of clear definitions, systematic reviews, and universalizable solutions. To this end, this paper first introduces the concept of UFM and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes current UFM-related works, base
    
[^237]: 大型多模型(LMMs)作为AI原生无线系统的通用基础模型

    Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems

    [https://arxiv.org/abs/2402.01748](https://arxiv.org/abs/2402.01748)

    本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。

    

    最近，大型语言模型(LLMs)和基础模型被宣称为6G系统的改变者。然而，目前关于无线网络的LLMs的努力仅限于直接应用现有的为自然语言处理(NLP)应用设计的语言模型。为了解决这一挑战，并创建以无线为中心的基础模型，本文提出了一个全面的视野，介绍了如何设计针对部署人工智能(AI)原生网络的通用基础模型。与基于NLP的基础模型不同，所提出的框架通过三个关键能力促进了大型多模型(LMMs)的设计：1) 处理多模态感知数据，2) 通过因果推理和检索增强生成(RAG)将物理符号表示与现实世界的无线系统联系起来，3) 通过无线环境反馈实现可教导性，以促进动态网络配置。

    Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
    
[^238]: 使用生成性人工智能处理智能辅导系统学习者性能稀疏数据的3DG框架

    3DG: A Framework for Using Generative AI for Handling Sparse Learner Performance Data From Intelligent Tutoring Systems

    [https://arxiv.org/abs/2402.01746](https://arxiv.org/abs/2402.01746)

    3DG框架是一种通过将数据表示为三维张量并结合张量分解和生成性人工智能模型，来处理智能辅导系统中学习者性能稀疏数据的创新方法。

    

    学习者的学习表现数据（如测验分数和尝试次数）对于理解学习者参与度和知识掌握水平非常重要。然而，从智能辅导系统（ITS）收集的学习表现数据常常稀疏，影响学习者建模和知识评估的准确性。为了解决这个问题，我们引入了3DG框架（三维张量稠密生成与生成性模型结合），这是一种结合了张量分解和先进生成性模型（包括生成对抗网络（GAN）和生成预训练变换器（GPT））的创新方法，用于增强数据填补和扩充。该框架首先将数据表示为三维张量，捕捉学习者、问题和尝试次数这三个维度。然后，通过张量分解对数据进行稠密处理，并利用针对个体学习模式的聚类进行生成性人工智能模型的扩充。应用于AutoTutor的数据中。

    Learning performance data (e.g., quiz scores and attempts) is significant for understanding learner engagement and knowledge mastery level. However, the learning performance data collected from Intelligent Tutoring Systems (ITSs) often suffers from sparsity, impacting the accuracy of learner modeling and knowledge assessments. To address this, we introduce the 3DG framework (3-Dimensional tensor for Densification and Generation), a novel approach combining tensor factorization with advanced generative models, including Generative Adversarial Network (GAN) and Generative Pre-trained Transformer (GPT), for enhanced data imputation and augmentation. The framework operates by first representing the data as a three-dimensional tensor, capturing dimensions of learners, questions, and attempts. It then densifies the data through tensor factorization and augments it using Generative AI models, tailored to individual learning patterns identified via clustering. Applied to data from an AutoTutor
    
[^239]: 通过分层图解释揭示分子成分

    Unveiling Molecular Moieties through Hierarchical Graph Explainability

    [https://arxiv.org/abs/2402.01744](https://arxiv.org/abs/2402.01744)

    本论文提出了一种使用图神经网络和分层可解释人工智能技术的方法，能够准确预测生物活性并找到与之相关的最重要的成分。

    

    背景：图神经网络（GNN）作为一种强大的工具，在支持体外虚拟筛选方面已经出现多年。在这项工作中，我们提出了一种使用图卷积架构实现高精度多靶标筛选的GNN。我们还设计了一种分层可解释人工智能（XAI）技术，通过利用信息传递机制，在原子、环和整个分子层面上直接捕获信息，从而找到与生物活性预测相关的最重要的成分。结果：我们在支持虚拟筛选方面的二十个细胞周期依赖性激酶靶标上报道了一种最先进的GNN分类器。我们的分类器超越了作者提出的先前最先进方法。此外，我们还设计了一个仅针对CDK1的高灵敏度版本的GNN，以使用我们的解释器来避免多类别模型固有的偏差。分层解释器已经由一位专家化学家在19个CDK1批准药物上进行了验证。

    Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
    
[^240]: 理性陷阱下的不确定性：一种结构化的人工智能风险

    The Reasoning Under Uncertainty Trap: A Structural AI Risk

    [https://arxiv.org/abs/2402.01743](https://arxiv.org/abs/2402.01743)

    当前使用的人工智能工具在面对不确定性推理时存在风险，尤其是对于LLMs的能力尚不清楚，并且无法保证性能。该报告强调了理性陷阱对人类和机器的挑战，并对潜在的人工智能风险进行了分析。

    

    本报告研究了当前（以及预测）人工智能工具所带来的一种新型风险。在对未来行动做出有效决策的过程中，我们需要在不确定性下进行推理，这对于许多关键的现实世界问题至关重要。面对这一挑战，对于类似LLMs的人工智能工具来辅助决策者的需求正在增长。在证明了这种需求及其背后的动机后，我们揭示了一个不断增加的风险：1）我们目前对LLMs在这方面的能力了解不够，2）在基本的计算爆炸性和深度不确定性约束准确性的情况下，我们无法保证其性能。本报告阐述了人类和机器在理性陷阱下面临的挑战，并把这些困难与潜在的人工智能时间表和能力联系起来。在确立了当前潜在的误用风险后，我们进一步揭示了这种看似累加的风险（越多的误用加剧了潜在的危害）实际上有多重的

    This report examines a novel risk associated with current (and projected) AI tools. Making effective decisions about future actions requires us to reason under uncertainty (RUU), and doing so is essential to many critical real world problems. Overfaced by this challenge, there is growing demand for AI tools like LLMs to assist decision-makers. Having evidenced this demand and the incentives behind it, we expose a growing risk: we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy. This report provides an exposition of what makes RUU so challenging for both humans and machines, and relates these difficulties to prospective AI timelines and capabilities. Having established this current potential misuse risk, we go on to expose how this seemingly additive risk (more misuse additively contributed to potential harm) in fact has multipl
    
[^241]: 优化LLM使用成本的研究

    Towards Optimizing the Costs of LLM Usage

    [https://arxiv.org/abs/2402.01742](https://arxiv.org/abs/2402.01742)

    本论文提出了一种优化LLM使用成本的方法，通过估计输出质量并解决优化问题，实现在质量和延迟方面保持成本在预算范围内或最小化成本。

    

    生成式人工智能，特别是LLM在现今广泛应用于各种文件处理任务中，如问答和摘要。然而，不同的LLM在不同任务上具有不同的能力、成本、标记化和延迟。实际上，企业已经在为各自的用例运营或使用LLM而承担巨大的成本。在这项工作中，我们提出通过估计LLM的输出质量（而无需实际调用LLM），然后解决LLM选择的优化例程，以在质量和延迟方面保持成本在预算范围内或最小化成本。我们提出了一个模型来预测LLM在摘要等文件处理任务中的输出质量，随后采用LP取整算法来优化LLM的选择。我们从理论和实证的角度研究了在质量和成本之间权衡的优化问题。

    Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente
    
[^242]: 开发并测试一种新的基于大型语言模型的临床决策支持系统，用于药物安全的12种临床专业

    Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties

    [https://arxiv.org/abs/2402.01741](https://arxiv.org/abs/2402.01741)

    本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。

    

    重要性：我们介绍了一种新颖的基于检索增强生成（RAG）-大型语言模型（LLM）的临床决策支持系统（CDSS），用于安全用药处方。该模型通过提供与患者背景和机构指南相关的处方错误警报，解决了传统基于规则的CDSS的局限性。目标：本研究评估了基于LLM的CDSS在识别各种医学和外科病例中的药物错误方面的有效性，与人工专家小组进行比较。它还研究了临床医生在不同CDSS集成方式（初级药师、仅基于LLM的CDSS和二者的组合）中的偏好。设计、设置和参与者：利用带有GPT-4.0的RAG模型，本研究涉及12个专业中23个临床案例的61个处方错误场景。专家小组使用PCNE分类和NCC MERP指数评估这些案例。三名初级药师独立审核每个场景，并提出处理建议。根据检查的错误和建议编制了反馈报告。 然后，三名医生独立审核这些报告，并提出对下一步处理的意见。

    Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
    
[^243]: 在认知负荷下的补偿性偏见：减少大型语言模型中的选择偏见

    Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models

    [https://arxiv.org/abs/2402.01740](https://arxiv.org/abs/2402.01740)

    这项研究研究了大型语言模型在选择对象时的偏见，发现偏见结构依赖于模型，对象类型调节了偏见的影响程度，导致列表中的第一个对象在输出中被过度呈现。

    

    大型语言模型（LLMs）如gpt-3.5-turbo和claude-instant-1.2在解释和执行语义任务方面变得非常重要。不幸的是，这些模型固有的偏见，类似于人类的认知偏见，会对它们的性能产生不利影响。其中一个受到影响最大的是从列表中进行对象选择，这是数字导航和决策制定中的基本操作。本研究重点检查这些偏见，并量化其对代表性列表选择任务的影响。通过进行一系列控制实验，我们操纵了温度、列表长度、对象身份、对象类型、提示复杂度和模型，以探索这些偏见。这使得我们能够孤立和测量这些偏见对选择行为的影响。我们的研究结果表明，偏见结构在很大程度上取决于模型，而对象类型调节了偏见影响的程度。由于存在较强的初现效应，列表中的第一个对象会在输出中被过度呈现。

    Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
    
[^244]: OpenMoE：开源混合专家语言模型的早期努力

    OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

    [https://arxiv.org/abs/2402.01739](https://arxiv.org/abs/2402.01739)

    OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。

    

    为了帮助开源社区更好地理解基于混合专家(MoE)的大型语言模型(LLM)，我们训练并发布了OpenMoE，一系列完全开放源码和可复现的仅解码器MoE LLM，参数范围从650M到34B，训练数据超过1T个标记。我们的研究证实，MoE-based LLM可以提供比密集LLM更有利的成本效益平衡，突出了未来LLM开发的潜在有效性。本研究的另一个重要贡献是对我们的OpenMoE模型中的路由机制进行深入分析，得到了三个重要发现：上下文无关专业化、早期路由学习和末尾降低。我们发现，MoE模型中的路由决策主要基于标记ID，与上下文相关性很小。标记到专家的分配在预训练阶段早期确定，并且基本保持不变。这种不完全的路由可能导致...

    To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
    
[^245]: 为社交感知的谈判对话开发辅助大型语言模型代理

    Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues

    [https://arxiv.org/abs/2402.01737](https://arxiv.org/abs/2402.01737)

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。

    

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们通过让两个大型语言模型（LLM）扮演每次对话中的两名谈判者来模拟现实世界谈判。第三个LLM充当修正代理，重新编写违反规范的话语以改善谈判结果。由于这是一个新颖的任务，不存在手动构建的数据。为解决这个限制，我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性，即产品销售、房价和薪资谈判。源代码和生成的数据集将在接受后公开。

    In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
    
[^246]: SADAS: 一个面向双语社会文化对话修复规范违例的对话助手系统

    SADAS: A Dialogue Assistant System Towards Remediating Norm Violations in Bilingual Socio-Cultural Conversations

    [https://arxiv.org/abs/2402.01736](https://arxiv.org/abs/2402.01736)

    SADAS是一个面向双语社会文化对话的对话助手系统，旨在通过识别规范类别、检测违例、评估严重程度、实施纠正措施并阐述理由等新颖架构，确保不同文化背景的个体之间的对话能够以尊重和理解的方式进行。

    

    在当今全球化的世界中，弥合文化差异对于建立有意义的联系比以往任何时候都更加重要。社会感知对话助手系统（SADAS）是我们对这一全球挑战的回答，旨在确保来自不同文化背景的个体之间的对话能够以尊重和理解的方式进行。我们系统的新颖架构包括：（1）识别对话中存在的规范类别，（2）检测潜在的规范违例，（3）评估这些违例的严重程度，（4）实施有针对性的措施来纠正违规行为，并（5）阐述这些纠正措施的理由。我们采用一系列最先进的技术来构建不同的模块，并进行大量实验选择每个模块最合适的骨干模型。我们还设计了一个人类偏好实验来验证系统的整体性能。我们将开源我们的系统（包括...

    In today's globalized world, bridging the cultural divide is more critical than ever for forging meaningful connections. The Socially-Aware Dialogue Assistant System (SADAS) is our answer to this global challenge, and it's designed to ensure that conversations between individuals from diverse cultural backgrounds unfold with respect and understanding. Our system's novel architecture includes: (1) identifying the categories of norms present in the dialogue, (2) detecting potential norm violations, (3) evaluating the severity of these violations, (4) implementing targeted remedies to rectify the breaches, and (5) articulates the rationale behind these corrective actions. We employ a series of State-Of-The-Art (SOTA) techniques to build different modules, and conduct numerous experiments to select the most suitable backbone model for each of the modules. We also design a human preference experiment to validate the overall performance of the system. We will open-source our system (includin
    
[^247]: VIALM：关于具有大型模型的视觉障碍辅助的调查和基准研究

    VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models

    [https://arxiv.org/abs/2402.01735](https://arxiv.org/abs/2402.01735)

    这项研究调查了具有大型模型的视觉障碍辅助，并通过基准实验评估了模型的能力，进一步推动了视觉障碍辅助技术的发展。

    

    视觉障碍辅助 (VIA) 旨在自动帮助视觉障碍者 (VI) 处理日常活动。VIA 的进展主要依赖于计算机视觉 (CV) 和自然语言处理 (NLP) 的发展，二者都展示了利用大型模型 (LMs) 的前沿范式。此外，LMs 展现出出色的多模态能力，可以应对诸如具身机器人等具有挑战性的物理任务。为了研究最先进 (SOTA) LMs 在VIA应用中的潜力和局限性，我们针对具有LMs的VIA任务（VIALM）进行了广泛的研究。在这个任务中，给定一个说明物理环境的图像和视觉障碍者用户的语言请求，VIALM旨在输出逐步引导，以在环境中帮助视觉障碍用户完成请求。该研究包括对近期LM研究的调查和对选定LMs能力的基准实验的检查。

    Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities
    
[^248]: 大型语言模型中检索增强生成的开发和测试--案例研究报告

    Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report

    [https://arxiv.org/abs/2402.01733](https://arxiv.org/abs/2402.01733)

    RAG模型是一种有前景的方法，用于在大型语言模型中定制领域知识。本研究开发和评估了一个专为医疗保健定制的LLM-RAG流程，重点关注术前医学。

    

    目的：大型语言模型(LLMs)在医学应用中具有重要的潜力。检索增强生成(RAG)作为一种有前景的方法，用于定制LLMs中的领域知识。本案例研究介绍了一个专为医疗保健定制的LLM-RAG流程的开发和评估，重点关注术前医学。方法：我们使用了35个术前指南开发了一个LLM-RAG模型，并通过与人工生成的回答进行测试，共评估了1260个回答。RAG流程涉及使用基于Python的LangChain和Llamaindex框架将临床文档转换为文本，并将这些文本处理为块以用于嵌入和检索。利用Pinecone进行向量存储和使用1536维余弦相似度损失度量来优化数据检索，其中选择了嵌入模型。将由初级医生提供的人工生成回答用作比较。结果：LLM-RA

    Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine.   Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison.   Results: The LLM-RA
    
[^249]: 辨别和改进基于 GAI 的简历筛选中的残疾偏见

    Identifying and Improving Disability Bias in GAI-Based Resume Screening

    [https://arxiv.org/abs/2402.01732](https://arxiv.org/abs/2402.01732)

    这项研究针对生成式人工智能在招聘中的应用进行了简历审计，发现 GPT-4 对残疾相关的简历存在偏见，通过训练自定义 GPT 并遵循多样性、公平性与包容性以及残疾正义原则，可以减少这种偏见。

    

    随着生成式人工智能的广泛应用，其使用范围已扩展到招聘和招聘等领域。然而，如果不考虑偏见的潜在问题，这可能对包括残疾人在内的边缘人群产生负面影响。为了解决这一重要问题，我们提出了一项简历审计研究，在这项研究中，我们要求 ChatGPT（特别是 GPT-4）对一份简历进行排名，与附加了与残疾有关的额外领导奖励、奖学金、面板演讲和成员资格的相同简历进行比较。我们发现 GPT-4 对这些改进的简历存在偏见。此外，我们表明通过训练基于 DEI 和残疾正义原则的自定义 GPT，可以量化地减少这种偏见。我们的研究还包括对 GPT-4 用于证明其有偏见决策的直接和间接残障主义类型的独特定性分析，并提出了额外偏见减轻工作的方向建议。此外，由于这些理由可能来自训练数据，我们还描述了残障正义的相关实际案例，以指导未来的研究方向。

    As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from traini
    
[^250]: 评估LLM生成的医学图像和症状分析的多模态诊断

    Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and Symptom Analysis

    [https://arxiv.org/abs/2402.01730](https://arxiv.org/abs/2402.01730)

    本文提出了一种评估LLM生成医学诊断的方法，通过结构化交互和领域特定分析来评估其正确性和准确性。我们使用GPT-4-Vision-Preview作为LLM，并使用多模态多项选择题评估其在病理学领域的表现。

    

    大型语言模型（LLM）是一种突破性的人工智能技术，正在快速发展，并承诺帮助医学诊断。然而，它们的返回结果的正确性和准确性尚未得到适当评估。在这项工作中，我们提出了一种LLM评估范式，它包含两个独立步骤的新方法，即（1）通过结构化交互进行多模态LLM评估和（2）基于之前交互提取的数据进行后续的领域特定分析。使用这种范式，（1）我们通过公开的多模态多项选择题（MCQs）在病理学领域评估LLM生成的医学诊断的正确性和准确性，（2）然后对提取的结果进行系统和全面的分析。我们将GPT-4-Vision-Preview作为LLM，回答由图像和文本组成的复杂医学问题，并探索了一系列的疾病、病况、化学状况。

    Large language models (LLMs) constitute a breakthrough state-of-the-art Artificial Intelligence technology which is rapidly evolving and promises to aid in medical diagnosis. However, the correctness and the accuracy of their returns has not yet been properly evaluated. In this work, we propose an LLM evaluation paradigm that incorporates two independent steps of a novel methodology, namely (1) multimodal LLM evaluation via structured interactions and (2) follow-up, domain-specific analysis based on data extracted via the previous interactions. Using this paradigm, (1) we evaluate the correctness and accuracy of LLM-generated medical diagnosis with publicly available multimodal multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a systemic and comprehensive analysis of extracted results. We used GPT-4-Vision-Preview as the LLM to respond to complex, medical questions consisting of both images and text, and we explored a wide range of diseases, conditions, chem
    
[^251]: 从大型语言模型中提取上下文信息用于知识图谱补全

    Contextualization Distillation from Large Language Model for Knowledge Graph Completion

    [https://arxiv.org/abs/2402.01729](https://arxiv.org/abs/2402.01729)

    本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。

    

    虽然文本信息显著提高了预训练语言模型（PLMs）在知识图谱补全（KGC）中的性能，但现有语料库从维基百科文章或同义词定义中收集的静态和噪声性质常常限制了基于PLM的KGC模型的潜力。为了克服这些挑战，我们提出了上下文化蒸馏策略，这是一种通用的可插入和可播放的方法，与判别和生成的KGC框架兼容。我们的方法首先指导大型语言模型（LLMs）将紧凑的结构化三元组转换为上下文丰富的段落。随后，我们引入了两个定制的辅助任务，重建和上下文化，使较小的KGC模型能够吸收这些丰富的三元组中的见解。对多种数据集和KGC技术的全面评估突出了我们方法的功效和适应性，揭示了无论基础管道如何，始终能提高性能。

    While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
    
[^252]: 硬件Phi-1.5B：一个大型语言模型编码硬件领域专业知识

    Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge

    [https://arxiv.org/abs/2402.01728](https://arxiv.org/abs/2402.01728)

    硬件Phi-1.5B是一个专门针对半导体产业硬件领域的大型语言模型，通过预训练和专业分层数据集解决了硬件领域的复杂性和数据稀缺性问题。

    

    在快速发展的半导体产业中，研究、设计、验证和制造是紧密相连的，大型语言模型在革新硬件设计和安全验证方面有巨大潜力。然而，主要挑战在于硬件特定问题的复杂性，这些问题在预训练阶段通常不能充分解决自然语言或软件代码知识。此外，缺乏与硬件领域相关的数据集也是开发基础模型的重要障碍。针对这些挑战，本文介绍了硬件Phi 1.5B，这是一个专门针对半导体产业硬件领域的创新大型语言模型。我们开发了一个专业分层数据集，包括小、中和大型子集，并将重点放在使用中型数据集进行预训练。这种方法充分利用了Ph模型的紧凑但高效的架构。

    In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi 1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset comprising small, medium, and large subsets and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Ph
    
[^253]: 提示多样化的想法: 提高人工智能创意多样性

    Prompting Diverse Ideas: Increasing AI Idea Variance

    [https://arxiv.org/abs/2402.01727](https://arxiv.org/abs/2402.01727)

    该论文通过研究在人工智能生成的想法中如何增加多样性，探讨了在创意生成过程中利用人工智能工具提高质量和生产效率的方法。研究发现不同的提示方法可以增加思路的多样性和独特想法的数量，在新产品开发领域尤其有效。

    

    与例行任务注重一致性不同，创造力和创新的目标是产生多样化的想法。本文深入探讨利用人工智能（AI）来提高创意生成过程的生产力和质量的兴趣。尽管先前的研究发现AI想法的平均质量相当高，但先前的研究也指出基于AI的头脑风暴无法创造足够的想法分散，这限制了新颖性和整体最佳想法的质量。我们的研究探讨了增加AI生成的想法分散方法。我们使用GPT-4探索不同提示方法对余弦相似度、独特想法数量和想法空间耗尽速度的影响。在以大学生为价格为50美元以下的新产品开发领域进行探索研究。在这个背景下，我们发现(1)通过不同的plausi提示方法生成的GPT-4想法池

    Unlike routine tasks where consistency is prized, in creativity and innovation the goal is to create a diverse set of ideas. This paper delves into the burgeoning interest in employing Artificial Intelligence (AI) to enhance the productivity and quality of the idea generation process. While previous studies have found that the average quality of AI ideas is quite high, prior research also has pointed to the inability of AI-based brainstorming to create sufficient dispersion of ideas, which limits novelty and the quality of the overall best idea. Our research investigates methods to increase the dispersion in AI-generated ideas. Using GPT-4, we explore the effect of different prompting methods on Cosine Similarity, the number of unique ideas, and the speed with which the idea space gets exhausted. We do this in the domain of developing a new product development for college students, priced under $50. In this context, we find that (1) pools of ideas generated by GPT-4 with various plausi
    
[^254]: AI中介交流的指导：AI不改变对文本消息的感知

    Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages

    [https://arxiv.org/abs/2402.01726](https://arxiv.org/abs/2402.01726)

    这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。

    

    对于许多人来说，焦虑、抑郁和其他社交和心理因素可能使撰写文本消息成为一项积极的挑战。为解决这个问题，大型语言模型（LLM）可能是帮助那些本来会觉得发送短信困难或有压力的用户的完美工具。然而，尽管大型语言模型的使用快速普及，但对其在文本消息撰写中的辅助使用的考虑还未被探索。关于大型语言模型使用的一个主要关注点是，AI的公众情绪较差可能导致其辅助的文本消息的使用对感知产生负面影响，从而使使用适得其反。为了验证这种可能性，我们探讨了人们是否认为一条文本消息是否在撰写过程中得到了AI的辅助，会改变其感知的语调、清晰度和表达意图的能力。在这项研究中，我们调查了26名参与者对18条随机标记的预先撰写的文本消息的感知。通过分析参与者对消息语调的评分，我们发现。

    For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
    
[^255]: 在人工智能中加强道德界限：在大型语言模型中增强安全的高级策略

    Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models

    [https://arxiv.org/abs/2402.01725](https://arxiv.org/abs/2402.01725)

    本篇论文介绍了一种多方面的方法来解决大型语言模型中的道德和安全挑战，包括过滤敏感词汇、检测角色扮演、实施自定义规则引擎，以及应用到不同的派生模型中。这些方法可以提高模型的安全性，降低道德和隐私方面的风险。

    

    最近在大型语言模型（LLMs）方面的进展显著增强了自然语言处理和人工智能的能力。这些模型，包括GPT-3.5和LLaMA-2，由于具有变革性的Transformer模型，在文本生成、翻译和问答任务中取得了革命性的突破。尽管被广泛使用，LLMs也带来了一些挑战，例如在被迫做出不当回应时产生的道德困境，易受网络钓鱼攻击和侵犯隐私等问题。本文通过引入多方面的方法来解决这些挑战，包括：1）从用户输入中过滤敏感词汇，以防止产生不道德的回应；2）检测角色扮演，以阻止可能引发“越狱”情境的互动；3）实施自定义规则引擎，限制禁止内容的生成；以及4）将这些方法应用于各种LLM派生模型，如Multi-Model Large Language Models (MLLMs)。我们的方法不仅可以提高语言模型的安全性，还可以降低道德和隐私方面的风险。

    Recent advancements in large language models (LLMs) have significantly enhanced capabilities in natural language processing and artificial intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized text generation, translation, and question-answering tasks due to the transformative Transformer model. Despite their widespread use, LLMs present challenges such as ethical dilemmas when models are compelled to respond inappropriately, susceptibility to phishing attacks, and privacy violations. This paper addresses these challenges by introducing a multi-pronged approach that includes: 1) filtering sensitive vocabulary from user input to prevent unethical responses; 2) detecting role-playing to halt interactions that could lead to 'prison break' scenarios; 3) implementing custom rule engines to restrict the generation of prohibited content; and 4) extending these methodologies to various LLM derivatives like Multi-Model Large Language Models (MLLMs). Our approach not onl
    
[^256]: 在中国工业场景下对大型语言模型的准确性和鲁棒性的实证研究

    An Empirical Study on Large Language Models in Accuracy and Robustness under Chinese Industrial Scenarios

    [https://arxiv.org/abs/2402.01723](https://arxiv.org/abs/2402.01723)

    本文对中国工业领域的大型语言模型进行了实证研究，评估了其在准确性和鲁棒性方面的表现。

    

    近年来，大型语言模型（LLMs）在各个领域取得了快速发展。为了更好地服务大量的中国用户，中国的许多商业供应商采取了本地化战略，训练并提供专门为中国用户定制的本地LLMs。此外，展望未来，LLMs的一个重要应用领域将是企业和用户在工业生产领域实际部署。然而，LLMs在工业场景中的准确性和鲁棒性尚未得到很好的研究。本文在中国工业生产领域的背景下，对LLMs的准确性和鲁棒性进行了全面的实证研究。我们手动收集了来自8个不同工业部门的1200个领域特定问题来评估LLMs的准确性。此外，我们设计了一个包含四个工业特定稳定性类别和八个能力的变态测试框架，总计13,631个问题用于评估LLMs的鲁棒性。

    Recent years have witnessed the rapid development of large language models (LLMs) in various domains. To better serve the large number of Chinese users, many commercial vendors in China have adopted localization strategies, training and providing local LLMs specifically customized for Chinese users. Furthermore, looking ahead, one of the key future applications of LLMs will be practical deployment in industrial production by enterprises and users in those sectors. However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied. In this paper, we present a comprehensive empirical study on the accuracy and robustness of LLMs in the context of the Chinese industrial production area. We manually collected 1,200 domain-specific problems from 8 different industrial sectors to evaluate LLM accuracy. Furthermore, we designed a metamorphic testing framework containing four industrial-specific stability categories with eight abilities, totaling 13,631 questions wi
    
[^257]: 提升大型语言模型的性能，以更准确地回答问题和提取信息

    Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately

    [https://arxiv.org/abs/2402.01722](https://arxiv.org/abs/2402.01722)

    通过使用反馈和示例的精调过程，结合余弦相似度、LLM评估和Rouge-L得分等指标，可以提升大型语言模型（LLMs）在回答问题和提取信息方面的准确性。与零-shot LLMs相比，经过精调的模型展示了出色的问答能力，并且通过结合RAG过程进一步提高了其效果。

    

    大型语言模型（LLMs）生成问题的回答，然而它们的有效性常常受到答案质量的不佳和偶尔无法准确回答问题的影响。为了应对这些挑战，采用了一种精调过程，利用反馈和示例来优化模型。优化的目标是通过持续的反馈循环和利用余弦相似度、LLM评估和Rouge-L得分等指标来提升AI模型的性能。利用像GPT-3.5、GPT4ALL、LLaMA2和Claude这样的LLMs，并在包括FinanceBench和RAG Instruct Benchmark Tester Dataset在内的金融数据集上进行基准测试，展示了精调的必要性。结果表明，经过精调的模型能够超越零-shot LLMs的准确性，提供卓越的问答能力。值得注意的是，将LLM与一种名为RAG的检索增强生成过程相结合的方法证明了其有效性。

    Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves
    
[^258]: 在10个国家的非自愿合成亲密图像的态度和知识

    Attitudes Towards and Knowledge of Non-Consensual Synthetic Intimate Imagery in 10 Countries

    [https://arxiv.org/abs/2402.01721](https://arxiv.org/abs/2402.01721)

    本文研究了10个国家超过16,000名受访者对非自愿合成亲密图像的态度和行为；尽管社会对此仍认识不足，但这种行为被认为有害；约有2.2%的受访者表示曾受害，1.8%的受访者表示曾参与过此类行为；单靠立法行动并不足以解决问题，建议技术和平台政策的改进来减少伤害。

    

    深度伪造技术工具已经无处不在，"民主化"了操纵图像和视频的能力。这种技术的一个流行用途是创建性暴力内容，然后在互联网上广泛发布和共享。本文通过对10个国家的超过16,000名受访者的研究，研究了与非自愿合成亲密图像（NSII）相关的态度和行为。尽管社会对NSII的认识尚处于初级阶段，但NSII行为被认为是有害的。在普遍性方面，所有受访者中有2.2%的人表示曾受害，1.8%的人表示曾参与过这种行为。来自具有相关立法的国家的受访者也报告了参与和受害经历，这表明单靠立法行动并不足以阻止参与者的行为。减少伤害的技术考虑可能包括建议人们如何更好地监控自己在网上的存在，并执行平台政策。

    Deepfake technology tools have become ubiquitous, "democratizing" the ability to manipulate images and videos. One popular use of such technology is the creation of sexually explicit content, which can then be posted and shared widely on the internet. This article examines attitudes and behaviors related to non-consensual synthetic intimate imagery (NSII) across over 16,000 respondents in 10 countries. Despite nascent societal awareness of NSII, NSII behaviors were considered harmful. In regards to prevalence, 2.2% of all respondents indicated personal victimization, and 1.8% all of respondents indicated perpetration behaviors. Respondents from countries with relevant legislation also reported perpetration and victimization experiences, suggesting legislative action alone is not a sufficient solution to deter perpetration. Technical considerations to reduce harms may include suggestions for how individuals can better monitor their presence online, as well as enforced platform policies 
    
[^259]: 基于深度学习的阿姆哈拉语常见问题解答聊天机器人

    Deep Learning Based Amharic Chatbot for FAQs in Universities

    [https://arxiv.org/abs/2402.01720](https://arxiv.org/abs/2402.01720)

    本文提出了一个基于深度学习的阿姆哈拉语常见问题解答聊天机器人模型，可以帮助大学生解答常见问题，通过使用自然语言处理和深度学习技术，采用多种机器学习模型算法进行分析和分类，取得了最好的成绩。

    

    大学生常常花费大量时间向管理员或教师寻求常见问题的答案。这对双方来说都很繁琐，需要找到一个解决方案。为此，本文提出了一个聊天机器人模型，利用自然语言处理和深度学习技术，在阿姆哈拉语中回答常见问题。聊天机器人是通过人工智能模拟人类对话的计算机程序，作为虚拟助手处理问题和其他任务。所提出的聊天机器人程序使用标记化、规范化、去除停用词和词干提取对阿姆哈拉语输入句子进行分析和分类。采用了三种机器学习模型算法来分类标记和检索合适的回答：支持向量机（SVM）、多项式朴素贝叶斯和通过TensorFlow、Keras和NLTK实现的深度神经网络。深度学习模型取得了最好的成绩。

    University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
    
[^260]: 从RAG到QA-RAG：将生成式AI应用于药品监管合规流程

    From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process

    [https://arxiv.org/abs/2402.01717](https://arxiv.org/abs/2402.01717)

    本文介绍了一种利用生成式AI和检索增强生成（RAG）方法的聊天机器人模型，名为QA-RAG，用于解决制药行业中的合规性挑战，并在准确性上取得了显著改进。

    

    制药行业的合规性要求需要面对复杂且大量的指南文件，通常需要大量人力资源。为了解决这些挑战，我们的研究引入了一种利用生成式AI和检索增强生成（RAG）方法的聊天机器人模型。该聊天机器人旨在搜索与用户查询相关的指南文件，并根据检索到的指南提供答案。考虑到这个领域对高可靠性的需求，我们提出了问题回答检索增强生成（QA-RAG）模型。在对比实验中，QA-RAG模型在准确性上显示出显著改进，优于包括传统RAG方法在内的所有其他基线模型。本文详细介绍了QA-RAG的结构和性能评估，并强调其在药品监管合规领域及其他领域的潜力。我们已将我们的工作公开提供给进一步研究。

    Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further researc
    
[^261]: ChatGPT、Gemini和LLaMA2在多语言情感分析方面的比较

    ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis

    [https://arxiv.org/abs/2402.01715](https://arxiv.org/abs/2402.01715)

    本研究对ChatGPT、Gemini和LLaMA2等多语言情感分析模型进行了比较，发现这些模型在处理模棱两可的情境时表现良好，但在不同语言和评估中存在显著的偏差和性能不一致。这项工作为自动情感分析提供了标准化的评估方法，并呼吁改进算法和数据，以提高性能、可解释性和适用性。

    

    使用ChatGPT、Gemini或LLaMA2等基于大型语言模型（LLM）的模型进行自动情感分析，如今在学术研究和工业应用中越来越普遍。然而，在处理模棱两可或具有讽刺意味的文本时，对它们的性能进行评估和验证仍然不足。在本研究中，我们构建了细致和模棱两可的情境，并将其翻译成10种语言，然后使用流行的LLM进行情感预测。结果经过事后验证的人类回应来验证。ChatGPT和Gemini通常对模棱两可的情境处理得很好，但我们发现模型和评估的人类语言之间存在显著的偏差和性能不一致。这项工作为自动情感分析提供了标准化的评估方法，并呼吁进一步改进算法及其基础数据，以提高性能、可解释性和适用性。

    Automated sentiment analysis using Large Language Model (LLM)-based models like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic research and in industrial applications. However, assessment and validation of their performance in case of ambiguous or ironic text is still poor. In this study, we constructed nuanced and ambiguous scenarios, we translated them in 10 languages, and we predicted their associated sentiment using popular LLMs. The results are validated against post-hoc human responses. Ambiguous scenarios are often well-coped by ChatGPT and Gemini, but we recognise significant biases and inconsistent performance across models and evaluated human languages. This work provides a standardised methodology for automated sentiment analysis evaluation and makes a call for action to further improve the algorithms and their underlying data, to improve their performance, interpretability and applicability.
    
[^262]: TrICy: 通过意图感知的关注-复制机制引导的数据到文本生成

    TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy

    [https://arxiv.org/abs/2402.01714](https://arxiv.org/abs/2402.01714)

    TrICy是一种轻量级框架，利用意图和触发器引导数据到文本生成任务，并通过关注-复制机制有效地处理了词汇表之外的词汇。实验结果表明其在不同数据集上取得了良好的性能。

    

    数据到文本（D2T）生成是许多自然语言理解（NLU）应用中的关键任务，也是面向任务导向对话系统的基础。在可以直接与用户设备上的本地数据一起工作的会话型人工智能解决方案中，利用大型预训练语言模型（PLMs）的架构由于高内存占用而无法在设备上部署。为此，我们提出了TrICy，一种新颖的轻量级框架，用于增强D2T任务，根据上下文中的意图生成文本序列，并可进一步由用户提供的触发器进行引导。我们利用关注-复制机制准确地预测了词汇表之外的词汇（OOV）。对E2E NLG数据集（BLEU：66.43％，ROUGE-L：70.14％），WebNLG数据集（BLEU：Seen 64.08％，Unseen 52.35％）和与文本消息应用相关的自定义数据集的性能分析展示了我们的架构的有效性。此外，我们展示了通过利用可选的触发器输入，数据

    Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data
    
[^263]: 使用结构化纵向电子健康记录数据促使大型语言模型进行零样本临床预测

    Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data

    [https://arxiv.org/abs/2402.01713](https://arxiv.org/abs/2402.01713)

    本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。

    

    结构化纵向电子健康记录（EHR）数据的固有复杂性使其与传统上为自然语言处理而设计的大型语言模型（LLM）整合时面临重大挑战。受新疾病爆发时迅速决策的紧迫需求的驱使，本研究调查了类似GPT-4的LLM对EHR数据的适应性。我们特别关注它们的零样本能力，即在没有明确训练的情况下进行预测。针对EHR数据的纵向、稀疏和知识注入的特点，我们的提示方法考虑了特定的EHR特征，如单位和参考范围，并采用了与临床上下文相一致的上下文学习策略。通过在MIMIC-IV和TJH数据集上进行全面实验，我们证明了LLM能够通过我们的方法进行零样本临床预测，有效应对了EHR数据的挑战。

    The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
    
[^264]: 基于大型语言模型的社会感知式合成数据生成用于自杀意念检测

    Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models

    [https://arxiv.org/abs/2402.01712](https://arxiv.org/abs/2402.01712)

    通过利用生成式AI模型，我们提出了一种基于社会感知的合成数据生成方法，用于自杀意念检测。与传统模型相比，我们的方法在实际数据集上表现出有竞争力的性能。

    

    自杀意念检测是一个重要的研究领域，对于改进心理健康支持系统具有巨大潜力。然而，在自杀相关数据周围的敏感性导致难以访问到大规模的、注释的数据集，这是训练有效的机器学习模型所必需的。为了解决这个限制，我们引入了一种创新的策略，利用ChatGPT，Flan-T5和Llama等生成式AI模型的能力，为自杀意念检测创建合成数据。我们的数据生成方法基于从心理学文献中提取的社会因素，并旨在确保涵盖与自杀意念相关的基本信息。在我们的研究中，我们与基于BERT系列结构的现有NLP分类模型进行了基准测试。当在现实世界的UMD数据集上训练时，这些传统模型的F1分数通常在0.75到0.87之间。我们的合成数据驱动方法，提供了一种有效的替代选择，产生具有竞争力的性能。

    Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, i
    
[^265]: LLM在FHIR上--揭开健康记录的神秘面纱

    LLM on FHIR -- Demystifying Health Records

    [https://arxiv.org/abs/2402.01711](https://arxiv.org/abs/2402.01711)

    该论文描述了开发基于大型语言模型（LLMs）和快速健康互操作性资源（FHIR）应用程序编程接口（APIs）的患者中心人工智能（AI）解决方案，以增强健康素养和健康信息可访问性。在LLM在FHIR移动应用的试点研究中，应用程序通过LLMs向患者提供了准确、相关和可理解的健康信息。

    

    目标：通过开发基于大型语言模型（LLMs）和快速健康互操作性资源（FHIR）应用程序编程接口（APIs）的以患者为中心的人工智能（AI）解决方案，增强多样化患者群体的健康素养和健康信息的可访问性。材料和方法：该研究涉及开发在FHIR上的LLM，这是一个开源的移动应用程序，允许用户使用LLMs与其健康记录进行互动。该应用程序基于斯坦福大学的Spezi生态系统构建，并使用OpenAI的GPT-4。通过合成患者数据集进行了一项试点研究，并由医学专家进行评估，以评估该应用程序提高健康素养的效果。评估重点是LLM对常见患者问题的回答的准确性、相关性和可理解性。结果：LLM在FHIR上向患者提供了可理解的健康信息，准确性和相关性各不相同，但总体较高。

    Objective: To enhance health literacy and accessibility of health information for a diverse patient population by developing a patient-centered artificial intelligence (AI) solution using large language models (LLMs) and Fast Healthcare Interoperability Resources (FHIR) application programming interfaces (APIs). Materials and Methods: The research involved developing LLM on FHIR, an open-source mobile application allowing users to interact with their health records using LLMs. The app is built on Stanford's Spezi ecosystem and uses OpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient dataset and evaluated by medical experts to assess the app's effectiveness in increasing health literacy. The evaluation focused on the accuracy, relevance, and understandability of the LLM's responses to common patient questions. Results: LLM on FHIR demonstrated varying but generally high degrees of accuracy and relevance in providing understandable health information to patients. T
    
[^266]: 不是我的声音！语音生成器的伦理和安全伤害分类法

    Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators

    [https://arxiv.org/abs/2402.01708](https://arxiv.org/abs/2402.01708)

    语音生成技术的广泛应用给社会带来了伦理和安全风险。本文通过分析语音生成事件，总结出特定伤害模式，并将其分类。这些特定伤害涉及到受影响个体的曝光程度以及相关利益相关者和技术系统之间的相互作用。

    

    人工智能广泛采用语音生成技术给社会带来了一系列重大的伦理和安全风险，亟需解决。例如，在美国，越来越多的语音生成事件与警察受到恶作剧袭击有关，匿名行为者制造合成的声音打电话给警察，要求关闭学校和医院，或者以暴力手段进入无辜市民的家中。这样的事件表明，多模态生成人工智能的风险和伤害并不存在于孤立状态，而是源于多个利益相关者和技术人工智能系统之间的互动。本文通过分析语音生成事件，研究特定伤害模式的出现。我们发现特定伤害可以根据受影响个体的曝光程度进行分类，即他们是语音生成系统的主体、与之互动、受其影响或被排除在外。同样，特定伤害也与相关利益相关者和技术系统之间的相互作用有关。

    The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a con
    
[^267]: 超越行为主义的表征伤害：度量和减轻计划

    Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation

    [https://arxiv.org/abs/2402.01705](https://arxiv.org/abs/2402.01705)

    本研究超越行为主义的定义范围，提出了一种度量和减轻表征性伤害的框架，强调了大型语言模型在实施这些伤害时的脆弱性，并提出了减轻措施的建议。

    

    算法伤害通常被分为配置性或表征性。本研究专门针对后者，重点在于对当前表征性伤害定义的审查，以确定其中包含什么和不包含什么。这个分析促使我们扩展超越行为主义的定义范围，包括对认知和情感状态的伤害。本文概述了度量的高级要求：确定实施这种方法所需的专业知识，并通过案例研究进行说明。我们的工作凸显了大型语言模型在实施表征性伤害时的独特脆弱性，特别是当这些伤害未被度量和减轻时。该研究通过提出减轻措施并界定何时使用它们来结束。这项研究的总体目标是建立一个框架，扩大表征性伤害的定义，并将公平研究的见解转化为实际的度量方法。

    Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement 
    
[^268]: 作为策略的状态字符串：用博弈论求解器引导语言模型

    States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

    [https://arxiv.org/abs/2402.01704](https://arxiv.org/abs/2402.01704)

    本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。

    

    博弈论是研究理性主体间战略互动的数学模型。语言是人类互动的重要方式，但在历史上一直很难通过数学方法对对话及其战略动机建模。与语言互动相关的玩家、策略和回报的适当模型（即对游戏论常规符号逻辑的约束）将使现有的博弈论算法能够在语言领域提供战略解决方案。换句话说，这种约束可以为在对话中计算稳定、理性的对话策略提供一条途径。大型语言模型（LLM）可能已经达到了其生成能力足以实现自然对话真实、类似人类的模拟的程度。通过以不同的方式提示它们，我们可以将其响应引导到不同的输出话语。利用自然语言的表达能力，LLM还可以帮助我们快速生成新的对话。

    Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
    
[^269]: 一种多角度的机器学习方法用于评估洛杉矶警察与司机的互动

    A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles

    [https://arxiv.org/abs/2402.01703](https://arxiv.org/abs/2402.01703)

    该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。

    

    政府官员与市民之间的互动影响公共福祉和民主社会的正当性。警察是国家最显而易见、最接触市民的代理人，在交通站停期间，他们每年与公众互动超过2000万次。如今，这些互动经常被戴在身上的摄像机记录下来，这被视为提高警察问责制和改善警民互动的手段。然而，由于缺乏可靠的自动化工具来分析这些复杂而有争议的警民互动，这些记录的及时分析受到了阻碍。本文提出了一种新的多角度、多模态机器学习（ML）工具的方法，用于分析来自这些身上摄像机记录的音频、视频和文字信息。我们的方法首先确定与不同利益相关者最相关的沟通方面，包括共同感知互动的标志标记以及具有这些标记的符号。

    Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
    
[^270]: 语言模型的流畅梦境

    Fluent dreaming for language models

    [https://arxiv.org/abs/2402.01702](https://arxiv.org/abs/2402.01702)

    扩展了贪婪坐标梯度方法，设计了进化提示优化算法，实现了语言模型的流畅梦境，能够同时最大化内部特征和提示流畅性，可以自动探索模型对轻度分布之外提示的反应。

    

    特征可视化，也称为“梦境”，通过优化输入以最大化神经元的激活或其他内部组件，为视觉模型提供了洞察力。然而，由于输入空间是离散的，梦境在语言模型上尚未成功应用。我们对语言模型对抗攻击文献中的贪婪坐标梯度方法进行了扩展，设计了进化提示优化（EPO）算法。EPO优化输入提示，以同时最大化所选内部特征和提示流畅性之间的帕累托前沿，实现语言模型的流畅梦境。我们演示了使用神经元、输出logits和激活空间中的任意方向进行梦境。我们衡量了生成的提示的流畅性，并将语言模型的梦境与最大激活数据集示例进行了比较。关键是，流畅梦境允许自动探索模型内部对轻度分布之外的提示的行为反应。用于运行的代码

    Feature visualization, also known as "dreaming", offers insights into vision models by optimizing the inputs to maximize a neuron's activation or other internal component. However, dreaming has not been successfully applied to language models because the input space is discrete. We extend Greedy Coordinate Gradient, a method from the language model adversarial attack literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO optimizes the input prompt to simultaneously maximize the Pareto frontier between a chosen internal feature and prompt fluency, enabling fluent dreaming for language models. We demonstrate dreaming with neurons, output logits and arbitrary directions in activation space. We measure the fluency of the resulting prompts and compare language model dreaming with max-activating dataset examples. Critically, fluent dreaming allows automatically exploring the behavior of model internals in reaction to mildly out-of-distribution prompts. Code for runni
    
[^271]: 在临床护理点的医疗专业人员问答系统--一项系统综述

    Question answering systems for health professionals at the point of care -- a systematic review

    [https://arxiv.org/abs/2402.01700](https://arxiv.org/abs/2402.01700)

    这项系统综述旨在描述当前的医学问答系统，评估其在医疗保健中的适用性，并确定改进的方向。

    

    目标：问答（QA）系统有潜力通过为医疗专业人员提供最新和最相关的证据来提高临床护理的质量。然而，QA系统尚未得到广泛应用。本系统综述旨在描述当前的医学QA系统，评估其在医疗保健领域的适用性，并确定改进的方向。材料与方法：我们于2023年2月7日在PubMed、IEEE Xplore、ACM Digital Library、ACL Anthology以及前后引用中进行了搜索。我们包括了描述生物医学QA系统设计和评估的同行评议期刊和会议论文。两个评审人员筛选了标题、摘要和全文文章。我们对每个研究进行了叙事综合和偏倚评估。我们评估了生物医学QA系统的效用。结果：我们包括了79个研究，并确定了主题，包括问题的真实性，答案的可靠性，答案的效用，临床特殊性，系统和可用性。

    Objective: Question answering (QA) systems have the potential to improve the quality of clinical care by providing health professionals with the latest and most relevant evidence. However, QA systems have not been widely adopted. This systematic review aims to characterize current medical QA systems, assess their suitability for healthcare, and identify areas of improvement.   Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library, ACL Anthology and forward and backward citations on 7th February 2023. We included peer-reviewed journal and conference papers describing the design and evaluation of biomedical QA systems. Two reviewers screened titles, abstracts, and full-text articles. We conducted a narrative synthesis and risk of bias assessment for each study. We assessed the utility of biomedical QA systems.   Results: We included 79 studies and identified themes, including question realism, answer reliability, answer utility, clinical specialism, systems, usabili
    
[^272]: 大型语言模型赋能参与式城市规划

    Large language model empowered participatory urban planning

    [https://arxiv.org/abs/2402.01698](https://arxiv.org/abs/2402.01698)

    本研究将大型语言模型（LLMs）应用于城市规划的参与式过程中，通过角色扮演、协作生成和反馈迭代，解决了社区级土地利用任务。实证实验表明LLM在不同规划场景上具有适应性和有效性，能够超过人类专家满意度和包容性，并与最先进的强化学习方法在服务和生态方面媲美。

    

    参与式城市规划是现代城市规划的主流，涉及各个利益相关者的积极参与。然而，传统的参与式范式在时间和人力上面临挑战，而生成式规划工具在提供可调整和包容性解决方案方面失败。本研究介绍了一种创新的城市规划方法，将大型语言模型（LLMs）整合到参与过程中。该框架基于精心设计的LLM代理，包括角色扮演、协作生成和反馈迭代，解决了一个服务于1000个不同利益的社区级土地利用任务。在各种城市社区的实证实验表明，LLM在不同规划场景上的适应性和有效性。结果根据四个指标进行评估，在满意度和包容性方面超过人类专家，并在服务和生态方面与最先进的强化学习方法相媲美。进一步分析显示了该方法的优势。

    Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage
    
[^273]: 语言引导的世界模型：一种基于模型的人工智能控制方法

    Language-Guided World Models: A Model-Based Approach to AI Control

    [https://arxiv.org/abs/2402.01695](https://arxiv.org/abs/2402.01695)

    语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。

    

    将概率世界模型安装到人工智能代理中，为人类与这些代理沟通和控制打开了一个高效的渠道。除了更新代理策略，人类还可以修改他们的内部世界模型，以影响代理的决策。然而，当前现有的世界模型难以适应人类，因为它们缺乏自然的通信界面。为了解决这个问题，我们开发了语言引导的世界模型（LWMs），它们可以通过阅读语言描述来捕捉环境动态。这些模型提高了代理的沟通效率，使人类能够通过简洁的语言反馈同时改变他们在多个任务上的行为。它们还使代理能够从最初用于指导人类的文本中进行自我学习。为了促进LWMs的发展，我们设计了一个基于MESSENGER游戏（Hanjie等人，2021）的挑战基准，需要对新场景进行组合泛化。

    Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
    
[^274]: ARGS: 对齐作为奖励导向的搜索

    ARGS: Alignment as Reward-Guided Search

    [https://arxiv.org/abs/2402.01694](https://arxiv.org/abs/2402.01694)

    ARGS是一个对齐作为奖励导向的搜索框架，通过在解码过程中将模型的概率预测调整为奖励信号，实现生成具有语义多样性且与人类偏好对齐的文本。与基线相比，在不同任务和模型维度下，ARGS具有持续的奖励增益，表现出很好的性能。

    

    将大规模语言模型与人类目标对齐是至关重要的，然而常见的方法包括RLHF在训练过程中存在不稳定和资源密集的问题。为应对这一挑战，我们引入了一个新的框架ARGS，即对齐作为奖励导向的搜索，它将对齐融入到解码过程中，消除了昂贵的RL训练的需求。通过使用奖励信号调整模型的概率预测，ARGS生成具有语义多样性的文本，同时与人类偏好对齐，为对齐语言模型提供了一种有前景且灵活的解决方案。值得注意的是，在不同的对齐任务和不同的模型维度下，ARGS相对于基线显示出持续的奖励改进。例如，采用相同的贪婪解码策略，我们的方法相对于基线提高了19.56%的平均奖励，并在GPT-4评估中获得了64.33%的偏好或并列分数。我们相信，我们的框架强调了解码的创新性和效果。

    Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model's probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, ARGS demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing deco
    
[^275]: 大型语言生成模型与人类患者相比，对非专业患者解释实验室测试结果的回答质量的评估研究

    Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study

    [https://arxiv.org/abs/2402.01693](https://arxiv.org/abs/2402.01693)

    本研究评估了使用大型语言生成模型（LLMs）与人类患者相比，解答非专业患者关于实验室测试结果的问题的回答质量。评估结果表明LLMs在相关性、正确性和有帮助性方面具有一定潜力，但还存在潜在问题需要改进。

    

    实验室检测结果常常令人困惑和难以理解。大型语言生成模型（LLMs），如ChatGPT，为患者提供了获取问题答案的有前景的途径。我们的目标是评估使用LLMs回答患者有关实验室测试问题的相关、准确、有帮助并且无害的回答的可行性，以及识别可以通过增强方法来缓解的潜在问题。我们首先从Yahoo! Answers收集了与实验室测试结果相关的问题和答案数据，并选择了53个问答对进行本研究。使用LangChain框架和ChatGPT互联网门户，我们从四个LLMs（包括GPT-4、Meta LLaMA 2、MedAlpaca和ORCA_mini）生成了对这53个问题的回答。我们首先使用标准的问答相似度评估指标（包括ROUGE、BLEU、METEOR和BERTScore）评估了它们回答的相似性。我们还利用基于LLMs的评估器判断目标模型在相关性、正确性和有帮助性方面的质量是否更高。

    Lab results are often confusing and hard to understand. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to get their questions answered. We aim to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to lab test-related questions asked by patients and to identify potential issues that can be mitigated with augmentation approaches. We first collected lab test results related question and answer data from Yahoo! Answers and selected 53 QA pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from four LLMs including GPT-4, Meta LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge whether a target model has higher quality in terms of relevance, correctness, helpf
    
[^276]: 探索组织负责人工智能治理的算法审查委员会

    Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance

    [https://arxiv.org/abs/2402.01691](https://arxiv.org/abs/2402.01691)

    本研究调查了组织负责的人工智能（AI）治理中的算法审查委员会（ARBs）的实践情况和成果，发现了来自不同类型组织和领域的多样化定义和内部治理方法。

    

    公司、非营利组织、政府和学术机构等组织越来越多地开发、部署和利用人工智能（AI）工具。负责任的AI（RAI）治理方法在组织中变得越来越重要，以应对潜在的AI风险和危害。本研究采访了17位技术贡献者，涵盖了学术界、政府、行业和非营利组织等各种类型的组织以及金融、健康、科技和其他领域，了解他们对内部RAI治理的经验。我们的研究结果揭示了RAI的多样化定义以及相应的内部治理方法。我们总结了对算法审查委员会（ARBs）和类似的审查委员会的首次详细研究结果，包括其成员资格、范围和衡量成功的方法。我们证实了金融行业在模型治理方面的稳健实践，并揭示了在健康领域存在大量的算法和AI治理，包括ARB类似的审查委员会。

    Organizations including companies, nonprofits, governments, and academic institutions are increasingly developing, deploying, and utilizing artificial intelligence (AI) tools. Responsible AI (RAI) governance approaches at organizations have emerged as important mechanisms to address potential AI risks and harms. In this work, we interviewed 17 technical contributors across organization types (Academic, Government, Industry, Nonprofit) and sectors (Finance, Health, Tech, Other) about their experiences with internal RAI governance. Our findings illuminated the variety of organizational definitions of RAI and accompanying internal governance approaches. We summarized the first detailed findings on algorithm review boards (ARBs) and similar review committees in practice, including their membership, scope, and measures of success. We confirmed known robust model governance in finance sectors and revealed extensive algorithm and AI governance with ARB-like review boards in health sectors. Ou
    
[^277]: 一个符合当前技术立法框架的能源社区在线分层能源管理系统

    An Online Hierarchical Energy Management System for Energy Communities, Complying with the Current Technical Legislation Framework

    [https://arxiv.org/abs/2402.01688](https://arxiv.org/abs/2402.01688)

    本论文提出了一个在线分层能源管理系统（HEMS），用于最小化可再生能源社区（REC）的成本，以评估其相对于本地自消耗方法的优越性。

    

    在应对气候变化的斗争中，人们越来越多地关注智能电网中的新能源效率策略。2018年，欧洲联盟（EU）在适当的立法框架下，将可再生能源社区（REC）定义为一个参与者共享自己生产的可再生能源的局部电网，旨在通过合理的激励措施降低电费成本。这一举措旨在加速本地可再生能源利用的普及，其成本可能无法承受。由于REC在技术上属于智能电网，因此上述策略可以应用，并且需要实际的能源管理系统（EMS）。因此，在这项工作中，我们综合了一个在线分层EMS（HEMS），以最小化REC成本，评估其相对于本地自消耗方法的优越性。我们认真遵循欧盟技术指示（源自意大利），旨在获得尽可能真实的结果。REC之间的功率流动

    Efforts in the fight against Climate Change are increasingly oriented towards new energy efficiency strategies in Smart Grids (SGs). In 2018, with proper legislation, the European Union (EU) defined the Renewable Energy Community (REC) as a local electrical grid whose participants share their self-produced renewable energy, aiming at reducing bill costs by taking advantage of proper incentives. That action aspires to accelerate the spread of local renewable energy exploitation, whose costs could not be within everyone's reach. Since a REC is technically an SG, the strategies above can be applied, and specifically, practical Energy Management Systems (EMSs) are required. Therefore, in this work, an online Hierarchical EMS (HEMS) is synthesized for REC cost minimization to evaluate its superiority over a local self-consumption approach. EU technical indications (as inherited from Italy) are diligently followed, aiming for results that are as realistic as possible. Power flows between REC
    
[^278]: 交通领域诊断的数字孪生系统的系统映射研究

    A Systematic Mapping Study of Digital Twins for Diagnosis in Transportation

    [https://arxiv.org/abs/2402.01686](https://arxiv.org/abs/2402.01686)

    这项研究对交通领域的数字孪生进行了系统映射，发现目前大多数研究局限于系统监测或故障检测，需要进一步研究数字孪生在诊断推理方面的应用。

    

    近年来，数字孪生已被提出并在各个领域中应用，其潜在应用范围从原型制作到维护。未来，数字孪生将推动许多高效可持续的技术应用，其中包括自动驾驶汽车。然而，尽管在许多领域有大量研究，学术界对数字孪生的确切定义以及其能力和局限性尚未达成一致。为了增进我们的理解，我们探讨数字孪生在交通领域诊断中的能力。我们进行了一个系统映射研究，包括车辆和其组件以及交通基础设施的数字孪生。我们发现很少有关数字孪生的论文描述了任何诊断过程。此外，大多数现有方法似乎仅限于系统监测或故障检测。这些发现表明我们需要更多研究来利用数字孪生进行诊断推理。

    In recent years, digital twins have been proposed and implemented in various fields with potential applications ranging from prototyping to maintenance. Going forward, they are to enable numerous efficient and sustainable technologies, among them autonomous cars. However, despite a large body of research in many fields, academics have yet to agree on what exactly a digital twin is -- and as a result, what its capabilities and limitations might be. To further our understanding, we explore the capabilities of digital twins concerning diagnosis in the field of transportation. We conduct a systematic mapping study including digital twins of vehicles and their components, as well as transportation infrastructure. We discovered that few papers on digital twins describe any diagnostic process. Furthermore, most existing approaches appear limited to system monitoring or fault detection. These findings suggest that we need more research for diagnostic reasoning utilizing digital twins.
    
[^279]: SMUTF：使用生成标签和混合特征的模式匹配方法

    SMUTF: Schema Matching Using Generative Tags and Hybrid Features

    [https://arxiv.org/abs/2402.01685](https://arxiv.org/abs/2402.01685)

    SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。

    

    我们引入了SMUTF，一种用于大规模表格数据模式匹配的独特方法，该方法假设在开放域任务中，监督学习不会影响性能，从而实现了有效的跨域匹配。这个系统独特地结合了基于规则的特征工程、预训练语言模型和生成式大语言模型。受人道主义交换语言的启发，我们使用“生成标签”为每个数据列部署了创新的适应性，提高了模式匹配的效果。SMUTF具有广泛的灵活性，可以与任何现有的预训练嵌入、分类方法和生成模型无缝配合使用。鉴于模式匹配缺乏广泛的公开数据集，我们已经创建并开源了HDXSM数据集，该数据集来自公共人道主义数据，我们相信这是目前最全面的模式匹配数据集。

    We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
    
[^280]: 使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的框架

    A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm

    [https://arxiv.org/abs/2402.01684](https://arxiv.org/abs/2402.01684)

    提出了一个使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的统一框架，旨在解决高计算成本和摇摆问题。

    

    随着自然语言处理领域中大型语言模型（LLMs）的不断演进，人们为了有效地微调常见的预训练LLMs以完成各种任务，在一个或多个特定领域中进行了大量努力。在实践中，有两种主要的适应方式：（i）多个独立模型：使用每个任务的相应训练样本对预训练LLMs进行独立的微调；（ii）集成模型：使用所有任务的样本来联合微调预训练LLMs 。为了同时解决高计算成本和摇摆问题，我们提出了一个统一的框架，使用一种新颖的定制门控（CGC）低秩自适应（LoRA）算法在LLMs中实现了1 + N多任务微调模式。我们的工作旨在充分利用MTL（即CGC）和PEFT（即LoRA）方案。对于给定的任务集群，我们设计了一个创新的层，其中包含...

    With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai
    
[^281]: 表情符号解密：利用ChatGPT提升社交媒体沟通的理解能力

    Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications

    [https://arxiv.org/abs/2402.01681](https://arxiv.org/abs/2402.01681)

    在表情符号研究中，我们评估了ChatGPT在处理注释和下游任务中的有效性。我们的研究结果表明ChatGPT可以作为一个可行的替代人类注释者的工具，有效地解释表情符号。

    

    表情符号在社交网络沟通中已经普遍存在，它们承载了超越文字或短语的语义，这引发了学术界对其属性和功能的越来越多的研究兴趣。然而，与表情符号相关的研究和应用面临两个主要挑战。首先，研究者通常依赖众包来注释表情符号，以了解其情感、使用意图和语义含义。其次，用户的主观解释往往会导致对表情符号的误解，并造成沟通障碍。大型语言模型（LLMs）在各种注释任务中取得了显著的成功，ChatGPT在多个领域展示了专业能力。在我们的研究中，我们评估了ChatGPT在处理以前注释和下游任务中的有效性。我们的目标是验证ChatGPT可以在表情符号研究中作为人类注释者的可行替代者，并验证其解释表情符号的能力。

    Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
    
[^282]: 大型语言模型基于多智能体系统：进展与挑战综述

    Large Language Model based Multi-Agents: A Survey of Progress and Challenges

    [https://arxiv.org/abs/2402.01680](https://arxiv.org/abs/2402.01680)

    大型语言模型(LLMs)已广泛应用于各种任务。基于LLMs的多智能体系统在复杂问题求解和世界模拟方面取得了重要进展。这篇综述给出了基于LLMs的多智能体系统的重要方面和面临的挑战的全面讨论。

    

    大型语言模型(LLMs)在各种任务上取得了显著的成功。由于LLMs具有令人印象深刻的规划和推理能力，它们被用作自主智能体来自动完成许多任务。最近，基于将一个LLM用作单个规划或决策智能体的发展，基于LLMs的多智能体系统在复杂问题求解和世界模拟方面取得了相当大的进展。为了为社区提供这个充满活力领域的综述，我们提供了这篇综述文章，深入讨论了基于LLMs的多智能体系统的基本方面以及面临的挑战。我们的目标是让读者对以下问题获得实质性见解：LLM-based多智能体模拟哪些领域和环境？这些智能体是如何建模和通信的？什么机制有助于智能体能力的增长？对于那些对这个领域的研究感兴趣的人，我们还总结了一些要点和挑战.

    Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize t
    
[^283]: StickerConv: 从零开始生成多模态共情回应

    StickerConv: Generating Multimodal Empathetic Responses from Scratch

    [https://arxiv.org/abs/2402.01679](https://arxiv.org/abs/2402.01679)

    本文介绍了StickerConv代理(Agent4SC)，该代理通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。为了利用构建的多模态共情对话数据集StickerConv，作者提出了PErceive and Generate Stickers (PEGS)模型，该模型能够生成情境相关和情感丰富的回应。

    

    在当前的共情对话研究中，贴纸尽管被广泛认可为提高在线交流中的共情能力，但仍未得到充分探索。本文介绍了StickerConv代理(Agent4SC)，通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。在此基础上，我们构建了一个多模态共情对话数据集StickerConv，包括12.9K个对话会话，5.8K个独特贴纸和2K个多样化会话场景，专门设计用于增强多模态情境下的共情回应生成。为了利用这个数据集的丰富性，我们提出了PErceive and Generate Stickers (PEGS)，一种多模态共情回应生成模型，并结合基于LLM的全面共情评估指标。我们的实验表明，PEGS在生成情境相关和情感丰富的回应方面具有很好的效果。

    Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotional
    
[^284]: 通过整合外延知识和内涵知识嵌入本体

    Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge

    [https://arxiv.org/abs/2402.01677](https://arxiv.org/abs/2402.01677)

    本文提出了一种新型本体嵌入方法EIKE，通过整合外延知识和内涵知识，在外延空间和内涵空间中表示本体，并采用基于几何的方法和预训练的语言模型对实例、概念和关系进行嵌入建模。

    

    本体包含领域内丰富的知识，可以分为两个类别，即外延知识和内涵知识。外延知识提供关于本体中特定概念所属的具体实例的信息，而内涵知识详细描述了概念之间的内在属性、特征和语义关联。然而，现有的本体嵌入方法未能同时充分考虑外延知识和内涵知识。在本文中，我们提出了一种名为EIKE（Extensional and Intensional Knowledge Embedding）的新型本体嵌入方法，通过在外延空间和内涵空间中表示本体。EIKE提出了一个统一的框架，用于将实例、概念及其关系嵌入到本体中，采用基于几何的方法对外延知识进行建模，并使用预训练的语言模型对内涵知识进行建模。

    Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
    
[^285]: 语言模型与人类在关键语法结构上的判断一致性

    Language models align with human judgments on key grammatical constructions

    [https://arxiv.org/abs/2402.01676](https://arxiv.org/abs/2402.01676)

    本研究通过对比评估发现，大型语言模型（LLMs）在俘获人类行为方面的表现非常出色，不仅整体准确率高，而且能够捕捉到人类语言判断中的细微差异。

    

    大型语言模型（LLMs）是否具有类似人类的语言普遍性？Dentella等人（2023年；“DGL”）使用多个LLMs提示语法正确性问题，以获取80个英语句子的语法句子判断，得出LLMs存在“是”偏向和“不能区分语法和非语法句子”的结论。我们采用了既定的实践方法重新评估LLM的性能，并发现DGL的数据实际上证明了LLM如何准确捕捉人类行为。模型不仅整体上实现了高准确率，还捕捉到了人类语言判断的细微变化。

    Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.
    
[^286]: 基于协议技术的应用的法律和伦理问题：拍卖式道路交叉口的案例研究

    Legal and ethical implications of applications based on agreement technologies: the case of auction-based road intersections

    [https://arxiv.org/abs/2402.01673](https://arxiv.org/abs/2402.01673)

    本文研究了基于协议技术的应用在道路交叉口拍卖中的法律和伦理问题，并提出了相应的要求。

    

    协议技术是构建分布式智能系统的一种新范 Paradigm，其中自主软件代理商代表他们的人类用户进行谈判以达成协议。智能城市是协议技术的关键应用领域。尽管有几个概念验证和原型存在，但这些系统距离在现实世界中投入使用还有很长的距离。本文重点研究了未来智能道路基础设施管理的一种新方法，即拍卖式道路交叉口的案例。我们发现，虽然这种方法的关键技术要素已经可用，但在实际应用之前仍需要解决多个非技术问题。为此，我们从国际法规和西班牙立法的角度分析了基于拍卖的道路交叉口的法律和伦理问题。通过这个研究，我们提出了一些要求。

    Agreement Technologies refer to a novel paradigm for the construction of distributed intelligent systems, where autonomous software agents negotiate to reach agreements on behalf of their human users. Smart Cities are a key application domain for Agreement Technologies. While several proofs of concept and prototypes exist, such systems are still far from ready for being deployed in the real-world. In this paper we focus on a novel method for managing elements of smart road infrastructures of the future, namely the case of auction-based road intersections. We show that, even though the key technological elements for such methods are already available, there are multiple non-technical issues that need to be tackled before they can be applied in practice. For this purpose, we analyse legal and ethical implications of auction-based road intersections in the context of international regulations and from the standpoint of the Spanish legislation. From this exercise, we extract a set of requi
    
[^287]: 智能辅导系统中的先决条件结构发现

    Prerequisite Structure Discovery in Intelligent Tutoring Systems

    [https://arxiv.org/abs/2402.01672](https://arxiv.org/abs/2402.01672)

    本研究提出了一种在智能辅导系统中整合知识结构和知识追踪的方法，通过学习者轨迹发现潜在的先决条件结构，并应用于内容推荐和评估推荐算法。

    

    本文讨论了在智能辅导系统中，知识结构（KS）和知识追踪（KT）对提高教育内容推荐的重要性。KS表示不同知识组件（KCs）之间的关系，而KT根据学习者的过去历史来预测其成功。本研究的贡献包括提出了一个将KS作为可学习参数合并到KT模型中的方法，从而能够从学习者轨迹中发现潜在的KS。通过使用发现的KS进行内容推荐，并利用模拟学生评估推荐算法，评估揭示的KS的质量。

    This paper addresses the importance of Knowledge Structure (KS) and Knowledge Tracing (KT) in improving the recommendation of educational content in intelligent tutoring systems. The KS represents the relations between different Knowledge Components (KCs), while KT predicts a learner's success based on her past history. The contribution of this research includes proposing a KT model that incorporates the KS as a learnable parameter, enabling the discovery of the underlying KS from learner trajectories. The quality of the uncovered KS is assessed by using it to recommend content and evaluating the recommendation algorithm with simulated students.
    
[^288]: 智能辅导系统中的性能和动机的改进：结合机器学习和学习者选择

    Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice

    [https://arxiv.org/abs/2402.01669](https://arxiv.org/abs/2402.01669)

    本研究通过结合机器学习和学生选择，改进了智能辅导系统的性能和动机。使用ZPDES算法，该系统能够最大化学习进展，并在实地研究中提高了不同学生群体的学习成绩。研究还探讨了学生选择对学习效率和动机的影响。

    

    在学校中，大规模的课堂规模给个性化学习带来了挑战，教育技术，尤其是智能辅导系统（ITS）试图解决这个问题。在这个背景下，基于学习进展假设（LPH）和多臂赌博机器学习技术的ZPDES算法对最大化学习进展（LP）的练习进行排序。该算法在之前的实地研究中已经显示出将学习表现提升到更广泛的学生群体中，与手工设计的课程相比。然而，其动机影响尚未评估。此外，ZPDES不允许学生发表选择意见。这种缺乏机构的限制与关注建模好奇驱动学习的LPH理论不一致。我们在这里研究了这种选择可能性的引入如何影响学习效率和动机。给定的选择与练习难度正交的维度有关，作为一种有趣的特性。

    Large class sizes pose challenges to personalized learning in schools, which educational technologies, especially intelligent tutoring systems (ITS), aim to address. In this context, the ZPDES algorithm, based on the Learning Progress Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences exercises that maximize learning progress (LP). This algorithm was previously shown in field studies to boost learning performances for a wider diversity of students compared to a hand-designed curriculum. However, its motivational impact was not assessed. Also, ZPDES did not allow students to express choices. This limitation in agency is at odds with the LPH theory concerned with modeling curiosity-driven learning. We here study how the introduction of such choice possibilities impact both learning efficiency and motivation. The given choice concerns dimensions that are orthogonal to exercise difficulty, acting as a playful feature.   In an extensive field study (265 7-8 years
    
[^289]: 通过虚拟现实和人工智能确定患有诵读困难的学生的困难：一项探索性分析

    Determining the Difficulties of Students With Dyslexia via Virtual Reality and Artificial Intelligence: An Exploratory Analysis

    [https://arxiv.org/abs/2402.01668](https://arxiv.org/abs/2402.01668)

    该研究通过虚拟现实和人工智能的应用，为患有诵读困难的学生提供个性化的学习支持，填补了高等教育领域的空白。

    

    学习障碍是影响大脑连接通讯区域的神经系统疾病。诵读困难学生在阅读、记忆和理解概念方面存在问题；然而，通过治疗和创建补偿机制，这些问题的程度可以得到缓解。为面向小学和中学的特殊学习障碍学生创造数字资源已经做出了一些努力。然而，高等教育中仍然缺乏标准方法。VRAIlexia项目通过提出两种不同的工具来解决这个问题：一个集成虚拟现实（VR）以快速和轻松采集数据的移动应用，以及一个基于人工智能的软件（AI）来分析采集的数据，以为每个学生定制支持方法。第一个工具已经被创建并正在分发给高等院校的诵读困难学生。

    Learning disorders are neurological conditions that affect the brain's ability to interconnect communication areas. Dyslexic students experience problems with reading, memorizing, and exposing concepts; however the magnitude of these can be mitigated through both therapies and the creation of compensatory mechanisms. Several efforts have been made to mitigate these issues, leading to the creation of digital resources for students with specific learning disorders attending primary and secondary education levels. Conversely, a standard approach is still missed in higher education. The VRAIlexia project has been created to tackle this issue by proposing two different tools: a mobile application integrating virtual reality (VR) to collect data quickly and easily, and an artificial intelligencebased software (AI) to analyze the collected data for customizing the supporting methodology for each student. The first one has been created and is being distributed among dyslexic students in Higher
    
[^290]: 生成幽灵：预测人工智能来世的益处和风险

    Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives

    [https://arxiv.org/abs/2402.01662](https://arxiv.org/abs/2402.01662)

    本文讨论了生成幽灵的潜在实施设计空间和其对个人和社会的实际和伦理影响，提出了研究议程以便使人们能够安全而有益地创建和与人工智能来世进行互动。

    

    随着人工智能系统在性能的广度和深度上迅速提升，它们越来越适合创建功能强大、逼真的代理人，包括基于特定人物建模的代理人的可能性。我们预计，在我们有生之年，人们可能会普遍使用定制的人工智能代理人与爱的人和/或更广大的世界进行互动。我们称之为生成幽灵，因为这些代理人将能够生成新颖的内容，而不只是复述其创作者在生前的内容。在本文中，我们首先讨论了生成幽灵潜在实施的设计空间。然后，我们讨论了生成幽灵的实际和伦理影响，包括对个人和社会的潜在积极和消极影响。基于这些考虑，我们制定了一个研究议程，旨在使人们能够安全而有益地创建和与人工智能来世进行互动。

    As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on specific people. We anticipate that within our lifetimes it may become common practice for people to create a custom AI agent to interact with loved ones and/or the broader world after death. We call these generative ghosts, since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we first discuss the design space of potential implementations of generative ghosts. We then discuss the practical and ethical implications of generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to empower people to create and interact with AI afterlives in a safe and beneficial 
    
[^291]: 高等教育中的生成式人工智能：基于机构政策和指南的证据分析

    Generative Artificial Intelligence in Higher Education: Evidence from an Analysis of Institutional Policies and Guidelines

    [https://arxiv.org/abs/2402.01659](https://arxiv.org/abs/2402.01659)

    这项研究通过分析美国116所高等教育机构的政策和指南，总结出大部分高等教育机构鼓励并提供详细指导来指引生成式人工智能（GenAI）在教学中的应用。

    

    2022年11月发布的ChatGPT引发了高等教育机构普遍采用生成式人工智能（GenAI）。这些机构急于应对其使用，尤其是学生的使用，首先试图对其进行规范，并争论其在教学中的有益整合。在发布后的一年里，高等教育机构越来越多地提供了GenAI的政策和指南。本文通过对116所被归类为高研究活动或R1机构的美国大学所产生的文件进行深入分析，全面了解了提供给机构利益相关者的GenAI相关建议和指导。经过广泛分析，我们发现绝大多数大学（N=73，63%）鼓励使用GenAI，并且许多大学提供了详细的课堂使用指导（N=48，41%）。超过一半的机构提供了示例教学大纲（N=65，56%），一半（N=58，50%）提供了样本GenAI课程和活动，这些将有助于机构的实施。

    The release of ChatGPT in November 2022 prompted a massive uptake of generative artificial intelligence (GenAI) across higher education institutions (HEIs). HEIs scrambled to respond to its use, especially by students, looking first to regulate it and then arguing for its productive integration within teaching and learning. In the year since the release, HEIs have increasingly provided policies and guidelines to direct GenAI. In this paper we examined documents produced by 116 US universities categorized as high research activity or R1 institutions to comprehensively understand GenAI related advice and guidance given to institutional stakeholders. Through an extensive analysis, we found the majority of universities (N=73, 63%) encourage the use of GenAI and many provide detailed guidance for its use in the classroom (N=48, 41%). More than half of all institutions provided sample syllabi (N=65, 56%) and half (N=58, 50%) provided sample GenAI curriculum and activities that would help ins
    
[^292]: 人工智能在法律应用中的优势和挑战

    Promises and pitfalls of artificial intelligence for legal applications

    [https://arxiv.org/abs/2402.01656](https://arxiv.org/abs/2402.01656)

    该研究指出当前缺乏支持证据，将AI重新定义法律职业的观点。作者调查了AI在信息处理、创造力/推理/判断任务和预测任务中的作用，并发现法律应用的评估取决于确定正确答案的难易程度和相关信息的观察性。作者建议对AI在法律环境中进行更好的评估和部署。

    

    AI是否将重新定义法律职业？我们认为这一断言目前缺乏支持证据。我们深入探讨了AI在三种类型的法律任务中的普遍角色：信息处理、涉及创造力、推理或判断的任务以及对未来的预测。我们发现，根据确定正确答案的难易程度和与任务相关的信息的可观察性，评估法律应用的便利程度在不同法律任务间差异巨大。对法律职业产生最重要变革的任务也是对AI能力过度乐观的最容易受影响的任务，因为难以评估。我们提出了关于更好评估和在法律环境中部署AI的建议。

    Is AI set to redefine the legal profession? We argue that this claim is not supported by the current evidence. We dive into AI's increasingly prevalent roles in three types of legal tasks: information processing; tasks involving creativity, reasoning, or judgment; and predictions about the future. We find that the ease of evaluating legal applications varies greatly across legal tasks, based on the ease of identifying correct answers and the observability of information relevant to the task at hand. Tasks that would lead to the most significant changes to the legal profession are also the ones most prone to overoptimism about AI capabilities, as they are harder to evaluate. We make recommendations for better evaluation and deployment of AI in legal contexts.
    
[^293]: 在全球视角下基于深度学习的在线课程学生表现预测方法的挑战

    A Deep Learning Approach Towards Student Performance Prediction in Online Courses: Challenges Based on a Global Perspective

    [https://arxiv.org/abs/2402.01655](https://arxiv.org/abs/2402.01655)

    该论文研究了基于深度学习的方法在全球范围内预测在线课程学生表现的挑战，并证明了深度学习模型在此方面具有有希望的性能。

    

    使用传统分析方法来分析和评估学生在任何学习环境中的进展是耗时且压力巨大的。由于教育界对于整合互联网技术以及向电子学习、混合学习或在线学习模式的转变的关注，学生人数的增加进一步加剧了这种情况。因此，学生表现预测成为了近年来一个活跃的研究领域。为了解决这个问题，机器学习和数据挖掘技术被提出作为一个可行的解决方案。为此，该工作提出了使用深度学习技术（CNN和RNN-LSTM）来预测在线课程交付的中期阶段学生的表现，并使用来自世界上三个不同地区的三个不同数据集进行了实验。实验结果表明，深度学习模型在性能上表现出色，超过了其他优化的传统机器学习模型。

    Analyzing and evaluating students' progress in any learning environment is stressful and time consuming if done using traditional analysis methods. This is further exasperated by the increasing number of students due to the shift of focus toward integrating the Internet technologies in education and the focus of academic institutions on moving toward e-Learning, blended, or online learning models. As a result, the topic of student performance prediction has become a vibrant research area in recent years. To address this, machine learning and data mining techniques have emerged as a viable solution. To that end, this work proposes the use of deep learning techniques (CNN and RNN-LSTM) to predict the students' performance at the midpoint stage of the online course delivery using three distinct datasets collected from three different regions of the world. Experimental results show that deep learning models have promising performance as they outperform other optimized traditional ML models
    
[^294]: 电能负荷分解的范围回顾

    A Scoping Review of Energy Load Disaggregation

    [https://arxiv.org/abs/2402.01654](https://arxiv.org/abs/2402.01654)

    通过范围回顾，本论文发现电能负荷分解领域主要集中在家庭电力消费方面，针对工业负荷分解的研究较少。研究使用了多种方法，其中人工神经网络是最常用的。

    

    通过增强需求侧管理的效果和提升消费者意识来促进行为节电，电能负荷分解可以为平衡电力网络做出贡献。然而，该领域当前缺乏全面的概述。为了填补这一空白，本文通过评估72篇全文期刊文章对负荷分解领域、数据类型和方法进行了范围回顾。研究发现，家庭电力消费是最研究的领域，而其他领域如工业负荷分解则很少讨论。大多数研究使用相对较低频率的数据，采样间隔为1至60秒。研究采用了各种各样的方法，其中最常见的是人工神经网络，其次是优化策略、隐藏马尔可夫模型和图信号处理方法。

    Energy load disaggregation can contribute to balancing power grids by enhancing the effectiveness of demand-side management and promoting electricity-saving behavior through increased consumer awareness. However, the field currently lacks a comprehensive overview. To address this gap, this paper con-ducts a scoping review of load disaggregation domains, data types, and methods, by assessing 72 full-text journal articles. The findings reveal that domestic electricity consumption is the most researched area, while others, such as industrial load disaggregation, are rarely discussed. The majority of research uses relatively low-frequency data, sampled between 1 and 60 seconds. A wide variety of methods are used, and artificial neural networks are the most common, followed by optimization strategies, Hidden Markov Models, and Graph Signal Processing approaches.
    
[^295]: 用户中心的AI分析在慢性健康状况管理中的应用

    User-Centric AI Analytics for Chronic Health Conditions Management

    [https://arxiv.org/abs/2402.01652](https://arxiv.org/abs/2402.01652)

    本论文介绍了AI分析在管理慢性健康状况中的应用，特别是在无药物治疗方法中的挑战。研究呈现了用户中心的方法以及相关的研究问题和下一步工作。

    

    在健康信息学中，AI分析的应用近年来迅速增长。在本专题讨论中，我们探讨了AI分析在管理糖尿病、肥胖等慢性健康状况中的应用。我们重点关注在无药物治疗方法中管理这些状况所面临的挑战，由于个体状况的差异。这些差异导致研究进入了用户中心的方法，引发了各种研究问题。在本短文中，我们提供了近期和正在进行的研究工作的示例，并结论我们认为的下一步和一些尚未解决的研究问题。

    The use of AI analytics in health informatics has seen a rapid growth in recent years. In this talk, we look at AI analytics use in managing chronic health conditions such as diabetes, obesity, etc. We focus on the challenges in managing these conditions especially with drug-free approaches due to the variations in individual circumstances. These variations directed the research into user-centric approach leading to variety of research questions. In this short paper, we give examples from recent and current research work and conclude with what, in our opinion, to be the next steps and some remaining open research questions.
    
[^296]: 《了解人工智能监管：通过以伦理为基础的审计比较主导的LLM聊天机器人的伦理框架，以评估道德推理和规范价值观》

    Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values

    [https://arxiv.org/abs/2402.01651](https://arxiv.org/abs/2402.01651)

    通过以伦理为基础的审计，本论文调查了8个主要的商业和开源大型语言模型，发现GPT-4在道德推理和伦理框架方面表现出色。

    

    随着自主智能代理的个体和协作网络的兴起，人工智能在更多关键推理和决策角色中得到应用。因此，基于伦理的审计在快速增长的人工智能安全和监管领域起着关键作用。本文通过伦理审计来调查8个主要的商业和开源大型语言模型，包括GPT-4。我们通过建立不同模型进行道德推理的能力以及比较模型作为伦理框架的规范价值观来评估可解释性和可信度。我们采用一种实验性的、基于证据的方法，通过将伦理困境提供给模型来挑战人工智能和人类的一致性。这些伦理场景旨在需要做出决策，其中情境的特定情况可能需要偏离规范伦理原则。一个复杂的伦理框架在一个模型中一致引发。然而，发现还存在令人担忧的情况。

    With the rise of individual and collaborative networks of autonomous agents, AI is deployed in more key reasoning and decision-making roles. For this reason, ethics-based audits play a pivotal role in the rapidly growing fields of AI safety and regulation. This paper undertakes an ethics-based audit to probe the 8 leading commercial and open-source Large Language Models including GPT-4. We assess explicability and trustworthiness by a) establishing how well different models engage in moral reasoning and b) comparing normative values underlying models as ethical frameworks. We employ an experimental, evidence-based approach that challenges the models with ethical dilemmas in order to probe human-AI alignment. The ethical scenarios are designed to require a decision in which the particulars of the situation may or may not necessitate deviating from normative ethical principles. A sophisticated ethical framework was consistently elicited in one model, GPT-4. Nonetheless, troubling finding
    
[^297]: 打造您自己的机器人朋友：面向普及且引人入胜的人工智能教育的开源学习模块

    Build Your Own Robot Friend: An Open-Source Learning Module for Accessible and Engaging AI Education

    [https://arxiv.org/abs/2402.01647](https://arxiv.org/abs/2402.01647)

    本文开发了一个面向大学和高中学生的开源学习模块，允许学生从零开始构建自己的机器人伙伴，并提供实践经验和入门知识，包括机器人技术、机器学习、软件工程和机械工程等。同时，该模块着重强调以人为本的人工智能，使学生能够更好地理解和开发。

    

    随着人工智能在社会和全球经济中的日益重要作用，人工智能教育和素养已成为大学和K-12教育中必要的组成部分，以培养学生面对人工智能驱动的社会。然而，当前的人工智能课程尚未足够普及和引人入胜，无法满足来自不同教育目标和不同社会经济背景的学生和学校的需求。在这项工作中，我们开发了一个面向大学和高中学生的开源学习模块，允许学生从零开始构建自己的机器人伙伴。这个开放平台可以提供关于人工智能各个方面的实践经验和入门知识，包括机器人技术、机器学习、软件工程和机械工程等。由于社交辅助机器人伙伴的社交和个人性质，该模块还特别强调以人为本的人工智能，使学生能够更好地理解和开发。

    As artificial intelligence (AI) is playing an increasingly important role in our society and global economy, AI education and literacy have become necessary components in college and K-12 education to prepare students for an AI-powered society. However, current AI curricula have not yet been made accessible and engaging enough for students and schools from all socio-economic backgrounds with different educational goals. In this work, we developed an open-source learning module for college and high school students, which allows students to build their own robot companion from the ground up. This open platform can be used to provide hands-on experience and introductory knowledge about various aspects of AI, including robotics, machine learning (ML), software engineering, and mechanical engineering. Because of the social and personal nature of a socially assistive robot companion, this module also puts a special emphasis on human-centered AI, enabling students to develop a better understa
    
[^298]: L-TUNING：用于LLMs中的提示和前缀的同步标签调整

    L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs

    [https://arxiv.org/abs/2402.01643](https://arxiv.org/abs/2402.01643)

    本文介绍了L-Tuning，一种在自然语言推理（NLI）框架内的高效微调方法，通过对预训练的LLM中的标签标记进行微调，提高了训练准确性和效率，并增强了模型的训练细微差别。

    

    高效地针对特定任务对大型语言模型（LLMs）进行微调在自然语言处理中面临着重大挑战。传统方法，如提示或前缀调整，通常依赖于任意标记进行训练，从而导致训练时间延长并且通用标记在各种类别标签中使用。为了解决这些问题，本文引入了L-Tuning，这是一种在自然语言推理（NLI）框架内设计的用于分类任务的高效微调方法。与传统方法不同，L-Tuning专注于通过预训练的LLM处理的标签标记的微调，从而利用其预先存在的语义知识。这种技术不仅提高了微调的准确性和效率，还促进了为每个类别生成不同的标签嵌入，增强了模型的训练细微差别。我们的实验结果表明，使用L-Tuning可以显著提高训练效率和分类准确性。

    Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
    
[^299]: 通过数据压缩评估大型语言模型的泛化性和鲁棒性

    Evaluating Large Language Models for Generalization and Robustness via Data Compression

    [https://arxiv.org/abs/2402.00861](https://arxiv.org/abs/2402.00861)

    通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。

    

    现有的大型语言模型评估方法面临数据污染、对提示敏感以及基准测试创建成本高等挑战。为了解决这些问题，我们提出了一种基于无损数据压缩的评估方法，测试模型的预测能力在其训练截止日期之后的泛化情况。具体而言，我们收集了从2017年到2023年共83个月的全面测试数据，并根据模型的训练数据截止日期将数据分为训练和测试期。我们使用两个指标进行评估：1）测试期的压缩性能作为对未见数据的泛化能力的衡量；2）训练期和测试期之间的性能差距作为鲁棒性的衡量。我们的实验证明，许多模型的压缩率在截止日期之后显著降低，但像... (内容过长，省略)

    Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
    
[^300]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^301]: 使用以意图漂移为引导的LLMs进行意图保证

    Intent Assurance using LLMs guided by Intent Drift

    [https://arxiv.org/abs/2402.00715](https://arxiv.org/abs/2402.00715)

    该论文介绍了一种使用以意图漂移为引导的LLMs进行意图保证的框架，通过利用自然语言模型生成的策略识别和处理意图漂移，以实现意图与业务目标的对齐。

    

    意图驱动的网络（IBN）为网络管理提供了范式转变，承诺以自动化的方式将意图和业务目标与网络操作对齐。然而，其实际实现具有挑战性：1）处理意图，即翻译、分解和识别实现意图的逻辑；2）意图一致性，即考虑到动态网络，逻辑应适当调整以保证意图。为了解决后者，意图保证负责持续验证和验证，包括采取必要行动使操作状态和目标状态保持一致。在本文中，我们定义了一个保证框架，允许我们在发生意图漂移时进行检测和采取行动。为此，我们利用了由大型语言模型（LLM）生成的基于AI驱动的策略，这些模型可以快速学习必要的上下文要求，并协助实现和保证意图。

    Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.
    
[^302]: 探索用于无监督可见-红外人物重新识别的均质和异质一致标签关联

    Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID

    [https://arxiv.org/abs/2402.00672](https://arxiv.org/abs/2402.00672)

    该论文提出了一种同时考虑均质和异质实例级别结构，构建高质量跨模态标签关联的模态统一标签传输方法，用于无监督可见-红外人物重新识别。

    

    无监督可见-红外人物重新识别（USL-VI-ReID）旨在无需注释从不同模态中检索相同身份的行人图像。之前的研究侧重于建立跨模态的伪标签关联以弥合模态间的差异，但忽略了在伪标签空间中保持实例级别的均质和异质一致性，导致关联粗糙。为此，我们引入了一个模态统一标签传输（MULT）模块，同时考虑了均质和异质细粒度实例级结构，生成高质量的跨模态标签关联。它建模了均质和异质的关联性，利用它们定义伪标签的不一致性，然后最小化这种不一致性，从而维持了跨模态的对齐并保持了内部模态结构的一致性。此外，还有一个简单易用的在线交叉记忆标签引用模块。

    Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
    
[^303]: 使用马尔可夫决策过程优化工业设备的任务分配与预测性维护

    Optimized Task Assignment and Predictive Maintenance for Industrial Machines using Markov Decision Process

    [https://arxiv.org/abs/2402.00042](https://arxiv.org/abs/2402.00042)

    本文提出了一种使用马尔可夫决策过程的分布式决策方法，用于工业设备任务分配和预测性维护。该方法实现了任务分配和健康管理决策代理之间的信息共享，并将不确定性纳入决策过程中。通过详细的数学模型和案例研究，证明了该方法的有效性和实际适用性。

    

    本文考虑了一种分布式决策方法，用于制造任务分配和基于设备状况的预测性维护。我们的方法考虑了任务分配和健康管理决策代理之间的信息共享。我们提出了基于马尔可夫决策过程设计决策代理的方法。使用马尔可夫决策过程的主要优势在于考虑到了决策过程中的不确定性。本文提供了详细的数学模型，并给出了实际执行策略。为了证明我们提出的方法的有效性和实际适用性，我们还包括了一个基于开源铣床工具退化数据的详细数值案例研究。我们的案例研究表明，所提出的方法在成本参数选择方面具有灵活性，并且允许对决策过程进行离线计算和分析。

    This paper considers a distributed decision-making approach for manufacturing task assignment and condition-based machine health maintenance. Our approach considers information sharing between the task assignment and health management decision-making agents. We propose the design of the decision-making agents based on Markov decision processes. The key advantage of using a Markov decision process-based approach is the incorporation of uncertainty involved in the decision-making process. The paper provides detailed mathematical models along with the associated practical execution strategy. In order to demonstrate the effectiveness and practical applicability of our proposed approach, we have included a detailed numerical case study that is based on open source milling machine tool degradation data. Our case study indicates that the proposed approach offers flexibility in terms of the selection of cost parameters and it allows for offline computation and analysis of the decision-making p
    
[^304]: LLaMA和ChatGPT嵌入在分子嵌入中的比较分析

    Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding

    [https://arxiv.org/abs/2402.00024](https://arxiv.org/abs/2402.00024)

    LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。

    

    目的：ChatGPT和LLaMA等大型语言模型在化学信息学领域越来越受到重视，特别是在解释Simplified Molecular Input Line Entry System (SMILES)方面。这些语言模型可以将SMILES字符串解码为向量表示，为理解化学图提供了一种新的方法。方法：我们研究了ChatGPT和LLaMA在嵌入SMILES字符串方面的性能。我们的评估集中在两个关键应用领域：分子性质（MP）预测和药物-药物相互作用（DDI）预测，这在药物开发和医疗保健中至关重要。结果：我们发现，使用LLaMA生成的SMILES嵌入在MP和DDI预测任务中表现优于ChatGPT。值得注意的是，基于LLaMA的SMILES嵌入在这两个预测任务中显示了与现有方法相当的结果。结论：在化学信息学中应用LLMs，特别是在利用SMILES进行嵌入方面，是可行的。

    Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
    
[^305]: 使用远程连接通知的Transformer进行高效的次季节天气预报

    Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers

    [https://arxiv.org/abs/2401.17870](https://arxiv.org/abs/2401.17870)

    提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。

    

    次季节预报对农业、水资源管理和灾害预警至关重要，但由于大气的混沌性，面临着挑战。最近机器学习领域的进展通过实现与数值模型相当的预测能力，革新了天气预报。然而，训练这些基础模型需要数千个GPU的计算时间，这导致相当多的碳排放，并限制了其更广泛的应用。此外，机器学习模型往往通过产生平滑的结果来愚弄像素误差评分，这些结果缺乏物理一致性和气象意义。为了解决上述问题，我们提出了一种远程连接通知的Transformer。我们的架构利用预训练的Pangu模型来获得良好的初始权重，并集成了一个远程连接通知的时间模块，以提高在延长的时间范围内的可预测性。值得注意的是，通过调整Pangu模型的1.1%参数，我们的方法改进了预测能力。

    Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
    
[^306]: 鲁棒的提示优化用于对抗语言模型的破解攻击

    Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

    [https://arxiv.org/abs/2401.17263](https://arxiv.org/abs/2401.17263)

    该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。

    

    尽管在人工智能对齐方面取得了一些进展，但语言模型（LM）仍然容易受到对抗性攻击或破解攻击的影响，其中对手修改输入提示以诱导有害行为。虽然已经提出了一些防御方法，但它们仅关注狭窄的威胁模型，并不能提供强大的防御。为了实现强大的防御，我们首次提出了用于对抗破解攻击的对抗目标，并提出了一种名为鲁棒提示优化（RPO）的算法，该算法利用基于梯度的令牌优化来确保输出的无害性。通过这种方法，我们得到了一个易于访问的后缀，显著改善了对破解攻击的强韧性，包括优化过程中出现的破解攻击以及未知的破解攻击，将攻击成功率从84%降低到8.66%，在20个破解攻击中。此外，我们还发现RPO对正常LM使用的影响较小，在适应性攻击下仍然有效，并且可以迁移到黑盒模型中，降低攻击成功率。

    Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
    
[^307]: 对齐和有用性之间的权衡：语言模型的研究

    Tradeoffs Between Alignment and Helpfulness in Language Models

    [https://arxiv.org/abs/2401.16332](https://arxiv.org/abs/2401.16332)

    本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。

    

    语言模型对齐已成为人工智能安全的重要组成部分，通过增强期望行为和抑制非期望行为，实现人类与语言模型之间的安全交互。通常通过调整模型或插入预设的对齐提示来实现。最近，通过改变训练后的表示来改变模型行为的表示工程方法在对齐语言模型方面表现出了有效性。表示工程在面对对抗攻击和降低社会偏见等对齐导向任务方面取得了增益，但也导致了模型执行基本任务能力的降低。本文研究了增加对齐度和减少模型有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。有趣的是，我们发现，尽管模型的有用性通常会减少

    Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
    
[^308]: M2-编码器：通过大规模高效预训练推进双语图像-文本理解

    M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining

    [https://arxiv.org/abs/2401.15896](https://arxiv.org/abs/2401.15896)

    本论文介绍了M2-编码器，通过大规模高效预训练，推动了双语图像-文本理解的发展。我们引入了一个全面的中英双语数据集BM-6B，使用新的聚合方法减少了损失计算的通信开销和GPU内存需求，提高了训练速度。在BM-6B上预训练的M²-编码器在多语言的多模态检索中创造了新的基准。

    

    像CLIP这样的视觉-语言基础模型已经在人工智能领域取得了突破。然而，支持多语言的VLM模型，例如中英文双语，由于大规模预训练数据集的相对稀缺而滞后。为此，我们引入了一个全面的中英双语（中英文）数据集BM-6B，包含超过60亿个图像-文本对，旨在增强多模态基础模型对两种语言中的图像进行深入理解。为了处理这样规模的数据集，我们提出了一种新颖的集合聚合方法来计算图像-文本对比损失，显著减少了通信开销和GPU内存需求，大大提高了训练速度。我们在BM-6B上预训练了一系列增强细粒度理解能力的中英双语图像-文本基础模型，得到的模型被称为M²-编码器（发音为“M-Square”），在两种语言的多模态检索中刷新了新的基准。

    Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced "M-Square"), set new benchmarks in both languages for multimodal retrieval a
    
[^309]: AI作为探索：导航智能空间

    AI-as-exploration: Navigating intelligence space

    [https://arxiv.org/abs/2401.07964](https://arxiv.org/abs/2401.07964)

    这篇论文阐述了一个被忽视但重要的科学角色，即AI作为探索。它强调通过创建和研究智能系统来揭示可能与人类和动物的智能形式不同的候选构建模块。论文通过讨论人类和大型语言模型在组合新颖和创造性概念方面的能力，说明了AI作为探索的价值。

    

    人工智能是一个拥有许多生命的领域，这个术语已经包含了一系列科学和商业努力。在这篇论文中，我阐述了人工智能具有一个被忽视但十分重要的科学角色，即“AI作为探索”。AI作为探索的基本思想是创建和研究能够揭示智能候选构建模块的系统，这些模块可能不同于我们熟悉的人类和动物智能形式。换句话说，我认为人工智能是探索智能空间，即可能的智能系统空间，的最佳工具之一。我通过关注一个具体的案例研究，即人类和大型语言模型在组合新颖和创造性概念方面的能力，来说明AI作为探索的价值。我展示了尽管后者在这样的任务中表现出人类水平的准确性，但很可能以根本不同的方式解决这个问题。

    Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no 
    
[^310]: 为管理低空领域授权而构建公平和公正的软件系统的工程方法

    Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations

    [https://arxiv.org/abs/2401.07353](https://arxiv.org/abs/2401.07353)

    本文旨在为管理低空领域授权而构建公平和公正的软件系统进行工程方法研究。研究结果表明，飞行特征和环境条件被认为是最重要的因素，但还应考虑飞行员和无人机的能力。此外，许多受访者对使用机器学习算法批准或拒绝飞行请求表示反对。

    

    小型无人机系统（sUAS）已在广泛的应用领域中得到广泛应用。这引入了共享领域内的操作复杂性和报告的事件增加，引发了安全担忧。为此，美国联邦航空管理局（FAA）正在开发无人机交通管理（UTM）系统，以基于sUAS预测能够安全完成任务的能力来控制对空域的访问。然而，一个完全自动化的系统，能够快速批准或拒绝飞行请求，可能存在偏见，并必须考虑多样化利益相关者的安全性、透明性和公平性。在本文中，我们提出了一项初步研究，探讨了应考虑在自动化系统中的因素的利益相关者的观点。结果表明，飞行特征和环境条件被认为是最重要的，但飞行员和无人机的能力也应该被考虑。此外，几个受访者表示他们对使用机器学习算法批准或拒绝飞行请求的不满。

    Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversio
    
[^311]: 通过语言策略双向适应构建开放式具身代理

    Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation

    [https://arxiv.org/abs/2401.00006](https://arxiv.org/abs/2401.00006)

    通过OpenContra框架，我们提出了一种协同训练方法，结合语言模型和强化学习，构建开放式具身代理，能够理解任意人类指令，并以较高的完成率完成目标。

    

    构建开放式学习代理涉及预训练的语言模型（LLM）和强化学习（RL）方法面临的挑战。LLM在上下文特定的实时交互中遇到困难，而RL方法则面临探索效率问题。为此，我们提出了OpenContra，一种协同训练框架，它协同LLM和Goal-Conditioned强化学习（GRL），构建一个能够理解任意人类指令的开放式代理。实现包括两个阶段：（1）用LLM微调翻译人类指令为结构化目标，以及课程训练目标条件的RL策略，执行任意目标；（2）协同训练LLM和RL策略相互适应，实现指令空间的开放性。我们在Contra上进行实验，该游戏是一个复杂而广阔的目标空间的大逃杀FPS游戏。结果表明，使用OpenContra训练的代理能够理解任意人类指令，并以高完成率完成目标。

    Building open-ended learning agents involves challenges in pre-trained language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle with context-specific real-time interactions, while RL methods face efficiency issues for exploration. To this end, we propose OpenContra, a co-training framework that cooperates LLMs and GRL to construct an open-ended agent capable of comprehending arbitrary human instructions. The implementation comprises two stages: (1) fine-tuning an LLM to translate human instructions into structured goals, and curriculum training a goal-conditioned RL policy to execute arbitrary goals; (2) collaborative training to make the LLM and RL policy learn to adapt each, achieving open-endedness on instruction space. We conduct experiments on Contra, a battle royale FPS game with a complex and vast goal space. The results show that an agent trained with OpenContra comprehends arbitrary human instructions and completes goals with a high completion ratio, whic
    
[^312]: AdaNAS：自我监督的神经架构搜索用于集合降雨预报的自适应后处理方法

    AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts

    [https://arxiv.org/abs/2312.16046](https://arxiv.org/abs/2312.16046)

    AdaNAS是一个自适应后处理方法，利用自我监督神经架构搜索对集合降雨预报进行处理，能够提高降雨预测的准确性。它采用面向降雨的搜索空间和降雨层级规则化函数，有效消除噪声数据的影响。

    

    以往关于数值天气预报(NWP)降雨预报的后处理研究主要关注统计方面的内容，较少涉及基于学习的方面。虽然一些手动设计的模型被提出来提高准确性，但它们是定制的网络，需要反复尝试和验证，耗费大量的时间和人力成本。因此，本研究提出了一种无需重要手动工作的自我监督神经架构搜索(NAS)方法，称为AdaNAS，用于进行降雨预报后处理和高准确性的降雨预测。此外，我们设计了一个面向降雨的搜索空间，显著改进了高降雨区域的预报。此外，还提出了一个降雨层级规则化函数，以消除训练过程中噪声数据的影响。在大规模预测实验中，根据\emph{无雨}，\emph{小雨}，\emph{中雨}，\emph{大雨}和\emph{暴雨}的情况进行了验证。

    Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated. Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor. Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy. In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas. Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training. Validation experiments have been performed under the cases of \emph{None}, \emph{Light}, \emph{Moderate}, \emph{Heavy} and \emph{Violent} on a large-scale pre
    
[^313]: 一种针对不平衡线性分类的扩展非对称sigmoid和感知机(SIGTRON)

    An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification

    [https://arxiv.org/abs/2312.16043](https://arxiv.org/abs/2312.16043)

    本文提出了一个新的多项式参数化sigmoid函数(SIGTRON)，并且介绍了其伴随的SIC模型。相比传统的成本敏感学习模型，在给定的训练数据集接近良好平衡的条件下，所提出的SIC模型对于数据集的变化更加适应，并通过创建倾斜的超平面方程来实现。

    

    本文提出了一种新的多项式参数化sigmoid函数，称为SIGTRON，它是一种扩展的非对称sigmoid函数和感知机的结合，以及它的伴随凸模型SIGTRON-不平衡分类(SIC)模型，该模型使用了虚拟SIGTRON产生的凸损失函数。与传统的$\pi$-加权成本敏感学习模型相比，SIC模型在损失函数上没有外部的$\pi$-权重，而是在虚拟的SIGTRON产生的损失函数中有内部参数。因此，当给定的训练数据集接近良好平衡的条件时，我们展示了所提出的SIC模型对数据集的变化更加适应，比如训练集和测试集之间比例不平衡的不一致性。这种适应是通过创建一个倾斜的超平面方程来实现的。另外，我们提出了一个基于拟牛顿优化(L-BFGS)框架的虚拟凸损失，通过开发一个基于区间的二分线性搜索算法来实现。

    This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function. In contrast to the conventional $\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function. As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. This adaptation is achieved by creating a skewed hyperplane equation. Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line sear
    
[^314]: 多模态联邦学习中通过原型遮罩和对比处理缺失模态的方法

    Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast

    [https://arxiv.org/abs/2312.13508](https://arxiv.org/abs/2312.13508)

    本文介绍了一种利用原型遮罩和对比处理来解决多模态联邦学习中缺失模态问题的方法，该方法可以有效缓解由于模态缺失引起的全局模型性能下降。

    

    在现实场景中，多模态联邦学习经常面临复杂的模态缺失问题，这对于构建联邦框架和显著降低模型推理准确性产生限制。现有解决方案通常涉及在客户端开发模态特定的编码器，并在服务器上训练模态融合模块来解决缺失模态的问题。然而，这些方法主要仅适用于具有单模态客户端或完全多模态客户端的特定场景，在复杂的模态缺失场景中很难有效泛化。本文将原型库引入基于FedAvg的联邦学习框架，从而使框架能够在训练和测试期间缓解由模态缺失引起的全局模型性能下降。所提出的方法利用原型作为表示缺失模态的遮罩，以制定任务校准的策略。

    In real-world scenarios, multimodal federated learning often faces the practical challenge of intricate modality missing, which poses constraints on building federated frameworks and significantly degrades model inference accuracy. Existing solutions for addressing missing modalities generally involve developing modality-specific encoders on clients and training modality fusion modules on servers. However, these methods are primarily constrained to specific scenarios with either unimodal clients or complete multimodal clients, struggling to generalize effectively in the intricate modality missing scenarios. In this paper, we introduce a prototype library into the FedAvg-based Federated Learning framework, thereby empowering the framework with the capability to alleviate the global model performance degradation resulting from modality missing during both training and testing. The proposed method utilizes prototypes as masks representing missing modalities to formulate a task-calibrated 
    
[^315]: Zero-1-to-3: 基于领域级零样本认知诊断的一批早鸟学生关于三个诊断目标的方法

    Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives

    [https://arxiv.org/abs/2312.13434](https://arxiv.org/abs/2312.13434)

    本论文提出了一种Zero-1-to-3的方法，该方法通过一批早鸟学生关注三个诊断目标，以实现领域级零样本认知诊断。它解决了跨领域诊断模型中可能融入不可转移信息的问题，从而提高了知识传输的效果。

    

    认知诊断通过探索学生的练习测验数据来估计他们的认知状态。它在智能教育系统中的个性化学习指导中起着关键作用。本文关注一个重要但常常被忽视的任务：领域级零样本认知诊断（DZCD），因为在新启动的领域中缺乏学生的练习记录而产生。最近的跨领域诊断模型已经证明是DZCD的一种有前途的策略。这些方法主要关注如何在领域之间传递学生状态。然而，它们可能会无意中将不可转移的信息融入到学生的表示中，从而限制了知识传输的效果。为了解决这个问题，我们提出了Zero-1-to-3，一种基于领域级零样本认知诊断框架，通过一批早鸟学生实现三个诊断目标。我们的方法首先使用预训练的诊断模型进行训练。

    Cognitive diagnosis seeks to estimate the cognitive states of students by exploring their logged practice quiz data. It plays a pivotal role in personalized learning guidance within intelligent education systems. In this paper, we focus on an important, practical, yet often underexplored task: domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the absence of student practice logs in newly launched domains. Recent cross-domain diagnostic models have been demonstrated to be a promising strategy for DZCD. These methods primarily focus on how to transfer student states across domains. However, they might inadvertently incorporate non-transferable information into student representations, thereby limiting the efficacy of knowledge transfer. To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive diagnosis framework via one batch of early-bird students towards three diagnostic objectives. Our approach initiates with pre-training a diagnosis model with d
    
[^316]: 评估和增强用于知识图谱上的对话推理的大型语言模型

    Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs

    [https://arxiv.org/abs/2312.11282](https://arxiv.org/abs/2312.11282)

    该论文评估了当前最先进的大型语言模型（GPT-4）在知识图谱上的对话推理能力，提出了一种基于KG推理的LLM基准代理（LLM-ARK），该代理利用全文环境提示来实现精确和适应性强的KG路径预测，并采用近端策略优化算法进行训练。

    

    大型语言模型（LLM）的发展得益于预训练技术的进展。通过手动设计的提示，这些模型展示了强大的推理能力。在这项工作中，我们评估了当前最先进的LLM（GPT-4）在知识图谱（KG）上的对话推理能力。然而，由于缺乏KG环境意识和开发有效的中间推理阶段优化机制的困难，LLM的性能受到限制。我们进一步引入了LLM-ARK，一个基于KG推理的LLM基准代理，旨在提供精确和适应性强的KG路径预测。LLM-ARK利用全文环境（FTE）提示来吸收每个推理步骤中的状态信息。我们将KG上的多跳推理挑战重新框定为顺序决策任务。利用近端策略优化（PPO）在线策略梯度强化学习算法，我们的模型...

    The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model i
    
[^317]: 用于自动驾驶的大规模语言模型：真实世界实验

    Large Language Models for Autonomous Driving: Real-World Experiments

    [https://arxiv.org/abs/2312.09397](https://arxiv.org/abs/2312.09397)

    该论文介绍了一个基于大规模语言模型的自动驾驶框架，名为Talk2Drive，用于处理来自人类的口头指令，并根据上下文信息实现自动驾驶决策，满足个性化偏好。

    

    自动驾驶系统在当今的技术领域日益流行，部分自动化的车辆已经在市场上广泛流通，具备完全自动化和“无人驾驶”能力的时代已经迫在眉睫。然而，准确理解人类的指令，特别是对于只有乘客而没有驾驶员的自动驾驶汽车来说，实现高度个性化仍然是自动驾驶系统开发中的挑战任务。在本文中，我们介绍了一个基于大规模语言模型（LLM）的框架Talk-to-Drive（Talk2Drive），以处理来自人类的口头指令，并根据上下文信息做出自动驾驶决策，满足他们对安全性、效率性和舒适性的个性化偏好。首先，我们开发了一个语音识别模块，用于将人类的口头输入转化为文本指令，然后将这些指令发送给LLMs进行推理。然后，适当的指令被发送给实际的汽车控制系统，进一步实现自动驾驶。

    Autonomous driving systems are increasingly popular in today's technological landscape, where vehicles with partial automation have already been widely available on the market, and the full automation era with "driverless" capabilities is near the horizon. However, accurately understanding humans' commands, particularly for autonomous vehicles that have only passengers instead of drivers, and achieving a high level of personalization remain challenging tasks in the development of autonomous driving systems. In this paper, we introduce a Large Language Model (LLM)-based framework Talk-to-Drive (Talk2Drive) to process verbal commands from humans and make autonomous driving decisions with contextual information, satisfying their personalized preferences for safety, efficiency, and comfort. First, a speech recognition module is developed for Talk2Drive to interpret verbal inputs from humans to textual instructions, which are then sent to LLMs for reasoning. Then, appropriate commands for t
    
[^318]: 使用Oracle和AI辩论进行大型游戏的玩法

    Playing Large Games with Oracles and AI Debate

    [https://arxiv.org/abs/2312.04792](https://arxiv.org/abs/2312.04792)

    本论文研究了在具有大量动作的重复游戏中实现遗憾最小化的问题，通过使用基于Oracle的算法，提出了一种高效的内部遗憾最小化算法，实现了在大型游戏中计算相关均衡的高效性。通过在AI安全辩论环境中的实验验证了算法的有效性。

    

    我们考虑在具有大量动作的重复游戏中实现遗憾最小化。这种游戏在通过辩论确保AI安全的环境中是固有的，并且更一般地应用于动作基于语言的游戏中。现有的在线游戏算法需要多项式计算数量的动作，而对于大型游戏来说，这可能是难以实现的。因此，我们考虑使用基于Oracle的算法，因为Oracle自然地模拟了对AI代理的访问。通过对Oracle访问进行特征化，我们可以有效地实现内部和外部遗憾的最小化。我们提出了一种新颖的内部遗憾最小化算法，其遗憾和计算复杂度对数地依赖于动作数量。这意味着可以高效地基于Oracle计算大型游戏中的相关均衡。最后，我们通过在AI安全辩论环境中进行实验，展示了我们算法分析的好处。

    We consider regret minimization in repeated games with a very large number of actions. Such games are inherent in the setting of AI safety via debate, and more generally games whose actions are language-based. Existing algorithms for online game playing require computation polynomial in the number of actions, which can be prohibitive for large games.   We thus consider oracle-based algorithms, as oracles naturally model access to AI agents. With oracle access, we characterize when internal and external regret can be minimized efficiently. We give a novel efficient algorithm for internal regret minimization whose regret and computation complexity depend logarithmically on the number of actions. This implies efficient oracle-based computation of a correlated equilibrium in large games.   We conclude with experiments in the setting of AI Safety via Debate that shows the benefit of insights from our algorithmic analysis.
    
[^319]: Elijah: 通过分布变化消除扩散模型中的后门注入

    Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift

    [https://arxiv.org/abs/2312.00050](https://arxiv.org/abs/2312.00050)

    Elijah是一种用于扩散模型的后门检测和消除框架，能够准确检测后门并将其效果减少至接近零，而不会显著牺牲模型的实用性。

    

    扩散模型(DM)因其能够从噪声中生成高质量图像而成为最先进的生成模型。然而，最近的研究表明，它们容易受到后门攻击的影响。当一个数据输入（例如一些高斯噪声）被注入触发器（例如一个白色斑点）时，带有后门的模型总是生成目标图像（例如不恰当的照片）。然而，有效的防御策略来减轻DM中的后门问题尚未得到充分探索。为了弥合这一差距，我们提出了第一个适用于DM的后门检测和消除框架。我们使用13种采样器对包括DDPM、NCSN和LDM在内的数百个DM进行评估，针对3种现有的后门攻击。广泛的实验证明，我们的方法可以接近100%的检测准确率，同时将后门效果减少至接近零，而不会显著牺牲模型的实用性。

    Diffusion models (DM) have become state-of-the-art generative models because of their capability to generate high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies. When a data input (e.g., some Gaussian noise) is stamped with a trigger (e.g., a white patch), the backdoored model always generates the target image (e.g., an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility.
    
[^320]: 在Web上揭示语言模型代理在顺序任务组合中的局限性

    Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web

    [https://arxiv.org/abs/2311.18751](https://arxiv.org/abs/2311.18751)

    本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。

    

    最近，语言模型代理(LMA)作为一种在多步决策任务上的有希望的范例出现，通常表现优于人类和其他强化学习代理。尽管有这种希望，但它们在通常涉及任务组合的现实应用中的性能仍未得到充分探索。在这项工作中，我们引入了一个新的基准，叫做CompWoB-反映更现实假设的50个组合性网站自动化任务。我们发现，虽然现有的提示型LMA（gpt-3.5-turbo或gpt-4）在基本任务上实现了94.0％的平均成功率，但在组合任务上降至24.9％的成功率。另一方面，只在基本任务上进行微调的转移性LMA表现出更小的泛化性差距，从85.4％下降到54.8％。通过平衡任务之间的数据分布，我们训练了一个新模型HTML-T5++，在MiniWoB上超过了人类水平的性能（95.2％），并在CompWoB上实现了最佳的零-shot性能（61.5%）。

    Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
    
[^321]: 通过熵率最小化实现可预测的强化学习动态

    Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization

    [https://arxiv.org/abs/2311.18703](https://arxiv.org/abs/2311.18703)

    该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。

    

    在强化学习中，智能体没有动机展示可预测的行为，通常通过策略熵正则化推动智能体在探索上随机化其行为。从人的角度来看，这使得强化学习智能体很难解释和预测；从安全角度来看，更难以进行形式化验证。我们提出了一种新的方法，称为可预测性感知强化学习（PA-RL），用于引导智能体展现可预测的行为，其利用状态序列熵率作为可预测性度量。我们展示了如何将熵率制定为平均奖励目标，并且由于其熵奖励函数依赖于策略，我们引入了一个动作相关的替代熵，以利用PG方法。我们证明了最小化平均替代奖励的确定性策略存在，并且最小化了实际熵率。我们还展示了如何在学习到的动态模型的基础上近似计算与值函数。

    In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
    
[^322]: 利用指数尺度的深度强化ReLU网络初始化和训练

    Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth

    [https://arxiv.org/abs/2311.18022](https://arxiv.org/abs/2311.18022)

    该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。

    

    ReLU激活的神经网络可以看作是分段线性函数的组合。对于这样的网络，随着深度的增加，表达在输入域上的不同线性区域的数量有可能以指数级增长，但当初始参数选择随机时，不太可能出现这种情况。这种不良的尺度能够导致即使是简单函数也需要使用过大的模型来近似。为了解决这个问题，我们引入了一种新的训练策略：首先以一种方式重新参数化网络权重，使得指数数量的激活模式得以展现。在这些新参数上进行训练可以得到一个初始解，稍后通过更新底层模型权重来改进。这种方法使我们能够产生比随机初始化对应的函数逼近好几个数量级的结果。

    A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
    
[^323]: ChatTraffic：通过扩散模型实现文本到交通生成

    ChatTraffic: Text-to-Traffic Generation via Diffusion Model

    [https://arxiv.org/abs/2311.16203](https://arxiv.org/abs/2311.16203)

    本研究提出了一种名为ChatTraffic的扩散模型，用于实现文本到交通生成。通过将生成模型与描述交通系统的文本结合，该方法能够解决传统交通预测方法中的两个挑战，并得到与真实交通数据一致的合成数据。

    

    交通预测是智能交通系统中最重要的基础之一。传统的交通预测方法只依赖历史交通数据来预测交通趋势，面临两个主要挑战：1）对异常事件不敏感；2）在长期预测方面性能有限。本文探讨了如何将生成模型与描述交通系统的文本结合起来用于交通生成，将此任务命名为文本到交通生成（TTG）。TTG任务的关键挑战是如何将文本与道路网络的空间结构和交通数据相关联，用于生成交通情况。为此，我们提出了ChatTraffic，这是第一个用于文本到交通生成的扩散模型。为了保证合成数据与真实数据的一致性，我们用图卷积网络（GCN）来扩展扩散模型，以提取交通数据的空间相关性。此外，我们构建了一个包含...

    Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing t
    
[^324]: 以采样为导向: Langevin动力学的预测编码

    Sample as You Infer: Predictive Coding With Langevin Dynamics

    [https://arxiv.org/abs/2311.13664](https://arxiv.org/abs/2311.13664)

    本文提出了以采样为导向的Langevin动力学的预测编码算法，通过对PC推理过程注入高斯噪声实现过阻尼的Langevin采样，并改进了结果编码器自由训练方法，通过编码器网络提供摊销的热启动。此外，还验证了一种轻量级且易于计算的预处理形式，使得算法具有更好的性能和鲁棒性。

    

    我们提出了一种新颖的算法，用于在通用深度生成模型中学习参数，该算法建立在计算神经科学的预测编码(PC)框架之上。我们的方法修改了标准的PC算法，使其性能与标准变分自动编码器(VAE)训练的性能相当甚至超过。通过将高斯噪声注入PC推理过程中，我们重新将其构想为过阻尼的Langevin采样，从而方便对紧凑证据下界(ELBO)进行优化。我们改进了结果编码器自由训练方法，通过将编码器网络纳入其中，为我们的Langevin采样提供了一种摊销的热启动，并测试了三种不同的目标。最后，为了增加对采样步长的鲁棒性，并减少对曲率的敏感性，我们验证了一种轻量级且易于计算的预处理形式，受到Riemann Manifold Langevin和SGD文献中的自适应优化器的启发。我们与...

    We present a novel algorithm for parameter learning in generic deep generative models that builds upon the predictive coding (PC) framework of computational neuroscience. Our approach modifies the standard PC algorithm to bring performance on-par and exceeding that obtained from standard variational auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference procedure we re-envision it as an overdamped Langevin sampling, which facilitates optimisation with respect to a tight evidence lower bound (ELBO). We improve the resultant encoder-free training method by incorporating an encoder network to provide an amortised warm-start to our Langevin sampling and test three different objectives for doing so. Finally, to increase robustness to the sampling step size and reduce sensitivity to curvature, we validate a lightweight and easily computable form of preconditioning, inspired by Riemann Manifold Langevin and adaptive optimizers from the SGD literature. We compare agains
    
[^325]: 通过分数蒸馏实现对扩散仿真的有效保护

    Toward effective protection against diffusion based mimicry through score distillation

    [https://arxiv.org/abs/2311.12832](https://arxiv.org/abs/2311.12832)

    本研究发现了攻击潜在扩散模型的脆弱点，并通过分数蒸馏采样（SDS）的策略实现了更快速和更高效的保护方法。

    

    尽管生成式扩散模型在生成高质量图像方面表现出色，但它们也可能被滥用来模仿授权图像，对人工智能系统构成重大威胁。为了保护图像免受基于扩散的仿真管道的威胁，人们已经尝试添加校准扰动。然而，大多数现有方法由于计算量和内存需求过高而对个体用户来说无效甚至不切实际。在这项工作中，我们对攻击潜在扩散模型（LDM）的新发现进行了探索，并提出了新的即插即用策略以实现更有效的保护。特别地，我们探索了攻击LDM中的瓶颈，并发现编码器模块而不是去噪模块是脆弱点。基于这一发现，我们提出了使用分数蒸馏采样（SDS）的策略，以在不损害其强度的前提下使保护速度加倍，内存占用减半。此外，我们还提供了一个强大的保护策略。

    While generative diffusion models excel in producing high-quality images, they can also be misused to mimic authorized images, posing a significant threat to AI systems. Efforts have been made to add calibrated perturbations to protect images from diffusion-based mimicry pipelines. However, most of the existing methods are too ineffective and even impractical to be used by individual users due to their high computation and memory requirements. In this work, we present novel findings on attacking latent diffusion models (LDM) and propose new plug-and-play strategies for more effective protection. In particular, we explore the bottleneck in attacking an LDM, discovering that the encoder module rather than the denoiser module is the vulnerable point. Based on this insight, we present our strategy using Score Distillation Sampling (SDS) to double the speed of protection and reduce memory occupation by half without compromising its strength. Additionally, we provide a robust protection stra
    
[^326]: DURel注释工具：人类和计算测量语义接近度、语义聚类和语义变化

    The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change

    [https://arxiv.org/abs/2311.12664](https://arxiv.org/abs/2311.12664)

    DURel注释工具是一个在线的、开源的工具，通过人类和计算机的注释实现了对单词使用之间的语义接近度的测量，并提供了对语义聚类和语义变化的分析功能。

    

    我们提出了DURel工具，该工具实现了在在线、开源界面中对单词使用之间的语义接近度进行注释。该工具支持标准化的人类注释和计算机注释，利用最近的上下文词模型的进展进行构建。注释者的判断通过自动图形聚类技术进行聚类，并进行可视化分析。这允许通过简单而直观的微任务判断来测量单词词义，并且需要最小的准备工作。该工具提供了额外的功能，以比较注释者之间的一致性，以确保获得判断的主观性，并计算总结统计数据，以揭示词义频率分布、语义变异或词义随时间的变化。

    We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time.
    
[^327]: 从一般环境中学习因果表示：可辨识性和内在歧义

    Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity

    [https://arxiv.org/abs/2311.12267](https://arxiv.org/abs/2311.12267)

    该论文研究了从一般环境中学习因果表示的问题，提供了基于这种环境生成的数据的可辨识性结果，并指出了受到围绕节点歧义的限制。同时提出了一个算法可以恢复出地面真实模型

    

    我们研究因果表示学习，即从低级观测数据（如文本和图像）中恢复高级潜在变量及其因果关系的任务，假设可以访问从多个环境生成的观察结果。之前关于因果表示可辨识性的结果通常假设可以访问单节点干预，但实际上这是不切实际的，因为潜在变量本身就未知。在本研究中，我们提供了基于来自一般环境的数据的第一个可辨识性结果。我们展示了对于线性因果模型，虽然可以完全恢复因果图，但潜在变量只能被识别到受到围绕节点歧义（SNA）的程度上。我们提供了我们保证的对应对，证明了在我们的设置中SNA基本上是不可避免的。我们还提出了一个算法LiNGCReL，可以被证明可以恢复出地面真实模型

    We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \texttt{LiNGCReL} which provably recovers the ground-truth model up to
    
[^328]: 基于惊喜性驱动的稳健可解释的非参数学习中的k-NN算法

    Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning

    [https://arxiv.org/abs/2311.10246](https://arxiv.org/abs/2311.10246)

    本论文提出了一种基于惊喜性驱动的稳健可解释的k-NN算法，通过使用信息论的角度对传统算法进行新的阐释，实现了在非参数学习中的分类、回归、密度估计和异常检测等任务。

    

    非参数学习是机器学习中的一个基本概念，旨在捕捉数据中的复杂模式和关系，而不对潜在的数据分布做出强烈的假设。在这一范式下，最为著名的算法之一是k最近邻（k-NN）算法。在这项工作中，我们通过使用机器学习在安全关键应用中的应用，从信息论的角度对传统的最近邻算法进行了新的阐释，并提出了一种稳健可解释的框架，用于分类、回归、密度估计和异常检测等任务。我们可以通过计算增加特征时的条件熵来确定数据点的权重和特征的贡献，而无需进行显式的模型训练。这使我们能够通过提供详细的数据点影响权重来计算特征的贡献。

    Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, density estimation, and anomaly detection using a single model. We can determine data point weights as well as feature contributions by calculating the conditional entropy for adding a feature without the need for explicit model training. This allows us to compute feature contributions by providing detailed data point influence weights with perfect attributi
    
[^329]: 语言模型与人脑的差异

    Divergences between Language Models and Human Brains

    [https://arxiv.org/abs/2311.09308](https://arxiv.org/abs/2311.09308)

    该论文系统地探索了语言模型（LMs）和人类大脑在语言处理方面的差异，发现在社交/情感智能和物理常识领域，LMs无法很好地捕捉到人类的表现，但在这些领域对LMs进行微调可以提高其性能。

    

    机器和人类是否以相似的方式处理语言？最近的研究暗示肯定，发现大脑信号可以通过语言模型（LMs）的内部表示有效地进行预测。尽管这样的结果被认为反映了LMs和人类大脑之间的共享计算原理，但LMs和人类在语言表示和使用上也存在明显的差异。在这项工作中，我们通过检查LM表示和人类大脑对语言的响应之间的差异，通过采用两个数据集对受试者阅读和听叙述故事的方式，系统地探索了人类和机器语言处理之间的分歧。通过数据驱动的方法，我们确定了两个领域，即社交/情感智能和物理常识，这些领域在LMs中无法很好地捕捉到。然后，我们使用人类行为实验验证了这些领域，并证明在这些领域对LMs进行微调可以改善其性能。

    Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
    
[^330]: 数据污染问题: 一种检测和估计大型语言模型中污染的工具

    Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models

    [https://arxiv.org/abs/2311.06233](https://arxiv.org/abs/2311.06233)

    这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。

    

    我们提出了数据污染问题（DCQ），这是一种简单而有效的方法，用于检测大型语言模型（LLM）中的数据污染并估计其数量。具体而言，我们将数据污染检测视为一系列的多项选择问题，并设计了一种测验形式，其中创建了每个数据集实例的三个扰动版本。这些变化仅包括词级扰动。生成的扰动版本与原始实例一起形成DCQ中的选项，额外的选项适应了提供的选择都不正确的可能性。鉴于在选择之间唯一的区别信号是与原始实例的确切措辞相关，如果在预训练阶段已经接触到原始实例，语言模型当被要求从选项中识别原始实例时，倾向于选择原始实例--这是语言模型固有的特性。在使用GPT-4/3.5进行多个数据集的测试中，我们的结果完全缺少准确性。

    We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
    
[^331]: PowerFlowNet: 使用消息传递图神经网络进行功率流近似

    PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks

    [https://arxiv.org/abs/2311.03415](https://arxiv.org/abs/2311.03415)

    PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。

    

    准确且高效的功率流分析对于现代电力网络的运行和规划至关重要。因此，需要能够为小型和大型电力网络提供准确和快速解的可扩展算法。由于电力网络可以被解释为一个图，图神经网络(GNNs)已经成为通过利用底层图结构的信息共享来改善功率流近似的准确性和速度的一种有前景的方法。在这项研究中，我们介绍了PowerFlowNet，一个新颖的GNN架构，用于功率流近似，在简单的IEEE 14总线系统中与传统的牛顿-拉夫逊方法展示了相似的性能，但在法国高电压网络(6470rte)的真实情况下实现了4倍的速度提升。同时，与其他传统的近似方法(如直流松弛法)相比，在性能和执行时间方面显著优于它们，从而实现了优越的表现。

    Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
    
[^332]: 基于一阶逻辑约束的多任务核机器学习

    Multitask Kernel-based Learning with First-Order Logic Constraints

    [https://arxiv.org/abs/2311.03340](https://arxiv.org/abs/2311.03340)

    本文提出了一个通用框架，将有监督和无监督示例与一阶逻辑背景知识整合到核机器学习中，实现多任务学习。通过将一阶逻辑约束转化为连续实现的形式，有效处理基于核的谓词的输出。

    

    本文提出了一个通用的框架，将由一系列一阶逻辑子句表达的背景知识与有监督和无监督示例整合到核机器中。特别地，我们考虑了一个多任务学习方案，在该方案中，定义在一组对象上的多个谓词需要从示例中共同学习，并对其值的合法配置施加一系列的FOL约束。这些谓词是定义在输入对象表示的特征空间上的，并且可以是已知的事先定义好的，也可以由适当的基于核的学习器进行近似。我们提出了一种通用的方法，将FOL子句转化为一个连续实现，能够处理由基于核的谓词计算出的输出。该学习问题被视为半监督任务，需要在损失函数的原属上进行优化，该损失函数包括对有监督示例的拟合损失度量、正则化项和

    In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a 
    
[^333]: 学习从错误中使LLM成为更好的推理者

    Learning From Mistakes Makes LLM Better Reasoner

    [https://arxiv.org/abs/2310.20689](https://arxiv.org/abs/2310.20689)

    本研究探索了大型语言模型（LLMs）是否可以从错误中学习，类似于人类学习的过程，并通过引入错误纠正的数据对来改进LLMs的推理能力。实验结果表明，这种方法能够持续提升仅使用CoT进行微调后的性能。

    

    最近，大型语言模型（LLM）在解决数学问题方面展示出了卓越的推理能力。为了进一步提高它们的推理能力，本研究探讨了LLM是否可以学习从错误中获益（LEMA），类似于人类的学习过程。考虑一个未能解决数学问题的人类学生，他会从自己犯的错误中学习，并纠正它。模仿这种错误驱动的学习过程，LEMA在LLM的微调过程中引入了错误纠正的数据对。具体而言，我们首先收集来自各种LLM的错误推理路径，然后使用GPT-4作为“纠正者”来识别错误步骤，解释错误原因，纠正错误并生成最终答案。此外，我们还应用了一种基于纠正的进化策略，有效地扩展了生成纠正数据的问题集。在各种LLM和推理任务上的实验表明，LEMA始终可以提升仅使用CoT的微调。我们...

    Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a "corrector" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu
    
[^334]: 代理特定效应：多智能体MDPs中的因果效应传播分析

    Agent-Specific Effects: A Causal Effect Propagation Analysis in Multi-Agent MDPs

    [https://arxiv.org/abs/2310.11334](https://arxiv.org/abs/2310.11334)

    本文介绍了一种系统化的方法，用于将代理的行动对其他代理的影响归因到因果效应上，并提出了一种衡量代理特定效应（ASE）的新的因果数量。同时，我们还介绍了ASE的反事实对应物（cf-ASE）以及识别cf-ASE的条件，并提出了一种实用的基于采样的算法。

    

    在多智能体决策中建立行动与结果之间的因果关系对于负有责任的决策是至关重要的。然而，解释和量化代理对这种关系的贡献面临着重大挑战。在多智能体序贯决策的背景下，这些挑战尤为突出，因为代理的行动对结果的因果效应取决于其他代理如何对该行动作出响应。本文的目标是提出一个系统性的方法，将代理的行动对其他代理产生的因果效应归因到其所施加的影响上。针对多智能体马尔可夫决策过程，我们引入了一种新的因果数量——代理特定效应（ASE），用于衡量代理行动对通过其他代理传播的结果的影响。然后，我们转向ASE的反事实对应物（cf-ASE），提供了识别cf-ASE的一组充分条件，并提出了一种实用的基于采样的算法。

    Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making. However, interpreting and quantifying agents' contributions to such relationships pose significant challenges. These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how other agents respond to that action. In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents. Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algor
    
[^335]: 透过消息传递的视角看超图神经网络：同質性与架构设计的共同视野

    Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design

    [https://arxiv.org/abs/2310.07684](https://arxiv.org/abs/2310.07684)

    本文通过消息传递机制的视角，提出了一种新的对高阶网络同質性的概念化，并探索了一些处理高阶结构的策略，为超图神经网络的架构设计和性能提供了新的视野和方法。

    

    当前大部分的超图学习方法和基准数据集都是通过从图的类比中提升过来的，忽略了超图的特殊性。本文尝试解决一些相关的问题：Q1 同質性在超图神经网络中是否起到了关键作用？Q2 是否可以通过细致处理高阶网络的特征来改善当前的超图神经网络架构？Q3 现有数据集是否对超图神经网络提供了有意义的基准？为了解决这些问题，我们首先引入了基于消息传递机制的高阶网络同質性的新概念化，统一了高阶网络的分析和建模。此外，我们还研究了在超图神经网络中处理高阶结构的一些自然但大部分未被探索的策略，比如保留超边依赖的节点表示，或是以节点和超边共同编码的方式进行处理。

    Most of the current hypergraph learning methodologies and benchmarking datasets in the hypergraph realm are obtained by lifting procedures from their graph analogs, leading to overshadowing specific characteristics of hypergraphs. This paper attempts to confront some pending questions in that regard: Q1 Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HNNs)? Q2 Is there room for improving current HNN architectures by carefully addressing specific characteristics of higher-order networks? Q3 Do existing datasets provide a meaningful benchmark for HNNs? To address them, we first introduce a novel conceptualization of homophily in higher-order networks based on a Message Passing (MP) scheme, unifying both the analytical examination and the modeling of higher-order networks. Further, we investigate some natural, yet mostly unexplored, strategies for processing higher-order structures within HNNs such as keeping hyperedge-dependent node representations, or per
    
[^336]: 用规划标记引导语言模型的数学推理

    Guiding Language Model Math Reasoning with Planning Tokens

    [https://arxiv.org/abs/2310.05707](https://arxiv.org/abs/2310.05707)

    本论文介绍了一种通过引入规划标记来引导语言模型进行数学推理的方法。这种方法在保持推理过程中的一致性方面具有显著的准确性提升，而增加的训练参数很少。

    

    大型语言模型（LLMs）近来因其进行复杂推理任务的能力（如思维链推理）而引起了广泛关注。然而，大多数现有的增强模型推理能力方法过于依赖数据驱动方法，忽视了模型推理能力的结构化方面。我们发现，虽然LLMs可以很好地处理个别推理步骤，但在整个推理链上保持一致性方面却存在困难。为了解决这个问题，我们在每个推理步骤的开始处引入规划标记，作为模型的引导，并将它们的嵌入添加到模型参数中。我们的方法对于可训练参数的增加非常小（仅为0.001%），可以通过完全微调或更高效的参数方案来应用。我们通过将其应用于三种不同的LLMs，在三个数学单词问题数据集上展示了我们方法的有效性，相对于标准方法，准确性显著提高。

    Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
    
[^337]: 可解释的符号网络代表意识的知觉

    Interpretable Semiotics Networks Representing Awareness

    [https://arxiv.org/abs/2310.05212](https://arxiv.org/abs/2310.05212)

    这个研究描述了一个计算模型，通过追踪和模拟物体感知以及其在交流中所传达的表示来模拟人类的意识。相比于大多数无法解释的神经网络，该模型具有解释性，并可以通过构建新网络来定义物体感知。

    

    人类每天都感知物体，并通过各种渠道传达他们的感知。在这里，我们描述了一个计算模型，追踪和模拟物体的感知以及它们在交流中所传达的表示。我们描述了我们内部表示的两个关键组成部分（"观察到的"和"看到的"），并将它们与熟悉的计算机视觉概念（编码和解码）相关联。这些元素被合并在一起形成符号网络，模拟了物体感知和人类交流中的意识。如今，大多数神经网络都是不可解释的。另一方面，我们的模型克服了这个限制。实验证明了该模型的可见性。我们人的物体感知模型使我们能够通过网络定义物体感知。我们通过构建一个包括基准分类器和额外层的新网络来演示这一点。这个层产生了图像的感知。

    Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
    
[^338]: 图像到图像翻译的深度强化学习

    Deep Reinforcement Learning for Image-to-Image Translation

    [https://arxiv.org/abs/2309.13672](https://arxiv.org/abs/2309.13672)

    该论文提出了一种基于深度强化学习的图像到图像翻译方法，通过将翻译过程分解为小步骤并引入元策略和Plan概念，能够有效处理高维连续状态和动作空间的挑战。

    

    大多数现有的图像到图像翻译方法通过深度学习模型的一次运行生成图像。然而，设计这样的单步模型始终具有挑战性，需要大量的参数，并容易陷入坏的全局最小值和过拟合。在本工作中，我们将图像到图像翻译重新定义为逐步的决策问题，通过深度强化学习提出了一种新的框架，进行基于强化学习的图像到图像翻译（RL-I2IT）。RL-I2IT框架的关键特点是将一个单体学习过程分解为小的步骤，并引入一个轻量级模型，逐步将源图像转化为目标图像。考虑到在传统的强化学习框架下处理高维连续状态和动作空间的挑战，我们引入了元策略和一个新的概念Plan到标准的Actor-Critic模型中，该概念的维度较原始图像低，并且可以帮助演员生成可处理的高维表示。

    Most existing Image-to-Image Translation (I2IT) methods generate images in a single run of a deep learning (DL) model. However, designing such a single-step model is always challenging, requiring a huge number of parameters and easily falling into bad global minimums and overfitting. In this work, we reformulate I2IT as a step-wise decision-making problem via deep reinforcement learning (DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The key feature in the RL-I2IT framework is to decompose a monolithic learning process into small steps with a lightweight model to progressively transform a source image successively to a target image. Considering that it is challenging to handle high dimensional continuous state and action spaces in the conventional RL framework, we introduce meta policy with a new concept Plan to the standard Actor-Critic model, which is of a lower dimension than the original image and can facilitate the actor to generate a tractable high dime
    
[^339]: DiffusionWorldViewer：揭示和拓宽生成式文本到图像模型反映的世界观

    DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by Generative Text-to-Image Models

    [https://arxiv.org/abs/2309.09944](https://arxiv.org/abs/2309.09944)

    本文章提出了DiffusionWorldViewer，一个交互界面，旨在揭示和拓宽生成式文本到图像模型的世界观。通过在输出的不同人口统计数据之间揭示世界观，并提供编辑工具，帮助用户在生成的图像中代表他们多样化的观点，并挑战当前模型中反映的有限世界观。

    

    生成式文本到图像（TTI）模型可以从简短的文本描述中生成高质量图像，在学术和创意领域被广泛使用。与人类一样，TTI模型有一个世界观，即从训练数据和任务中学习到的对世界的认知，这会影响它们为给定提示生成的图像。然而，TTI模型的世界观通常对用户隐藏，这使用户难以建立对TTI输出的直觉，并且它们通常与用户的世界观不一致，导致输出的图像不符合用户的期望。为此，我们介绍了DiffusionWorldViewer，一个交互界面，可在输出的不同人口统计数据之间揭示TTI模型的世界观，并提供编辑工具以使输出图像与用户的观点一致。在对18位多样化TTI用户进行的用户研究中，我们发现DiffusionWorldViewer帮助用户在生成的图像中代表他们多样化的观点，并挑战当前TTI模型中反映的有限世界观。

    Generative text-to-image (TTI) models produce high-quality images from short textual descriptions and are widely used in academic and creative domains. Like humans, TTI models have a worldview, a conception of the world learned from their training data and task that influences the images they generate for a given prompt. However, the worldviews of TTI models are often hidden from users, making it challenging for users to build intuition about TTI outputs, and they are often misaligned with users' worldviews, resulting in output images that do not match user expectations. In response, we introduce DiffusionWorldViewer, an interactive interface that exposes a TTI model's worldview across output demographics and provides editing tools for aligning output images with user perspectives. In a user study with 18 diverse TTI users, we find that DiffusionWorldViewer helps users represent their varied viewpoints in generated images and challenge the limited worldview reflected in current TTI mod
    
[^340]: Fabricator: 一个用于生成带有Teacher LLMs标注训练数据的开源工具集

    Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs

    [https://arxiv.org/abs/2309.09582](https://arxiv.org/abs/2309.09582)

    Fabricator是一个开源工具集，用于生成带有Teacher LLMs标注训练数据。它通过零样本学习的方式，利用强大的LLM根据任务描述生成标注数据，可以用于训练下一阶段的自然语言处理模型。

    

    大多数自然语言处理任务是通过监督学习来建模的，因此需要标注的训练数据来训练有效的模型。然而，手动生成足够质量和数量的标注数据被认为是昂贵和耗时的。当前的研究通过探索一种称为零样本学习的新范式来解决这个瓶颈，该范式通过数据集生成模型。在这种模式下，一个强大的LLM根据给定的任务描述生成标注数据，这些数据可以用于训练下一阶段的自然语言处理模型。例如，可以让LLM生成500个积极情绪的电影评论和另外500个消极情绪的评论。然后，生成的数据可以用于训练一个情感分类器，从而有效地利用LLM作为较小规模的学生模型的教师。通过这个演示，我们介绍了Fabricator，一个用于数据集生成的开源Python工具集。Fabricator实现了常见的数据集生成工作流程，支持广泛的自然语言处理任务（例如文本分类）。

    Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to "generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment." The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classi
    
[^341]: 无限时域平均奖励马尔可夫决策过程中策略梯度算法的遗憾分析

    Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes

    [https://arxiv.org/abs/2309.01922](https://arxiv.org/abs/2309.01922)

    本文提出了一种基于策略梯度的算法用于无限时域平均奖励马尔可夫决策过程，并证明了其全局收敛性和近似O(T^3/4)的遗憾界。

    

    本文考虑了无限时域平均奖励马尔可夫决策过程（MDP）。与现有的相关工作不同，我们的方法利用了通用策略梯度算法的能力，解放了它在假设线性MDP结构的限制下。我们提出了一种基于策略梯度的算法，并展示了其全局收敛性质。然后，我们证明了该算法具有近似O(T^3/4)的遗憾。值得注意的是，本文首次探索了在平均奖励场景下，对于通用参数化策略梯度算法的遗憾界计算。

    In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has $\tilde{\mathcal{O}}({T}^{3/4})$ regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret-bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios.
    
[^342]: 用于源代码摘要的蒸馏GPT

    Distilled GPT for Source Code Summarization

    [https://arxiv.org/abs/2308.14731](https://arxiv.org/abs/2308.14731)

    这篇论文提出了一种用于源代码摘要的蒸馏GPT方法，通过训练一个开源模型，避免了将代码发送给不可信的第三方进行处理的安全问题。

    

    代码摘要是源代码的简短自然语言描述。摘要通常只有一句话，但却构成了开发者文档的核心。例如，“将所有可见的多边形变为蓝色”这样的简短描述可以让程序员对代码的功能有一个高层次的了解，而无需阅读代码本身。最近，基于大型语言模型（如ChatGPT）的产品已经展示了自动编写这些描述的强大能力。然而，为了使用这些工具，程序员必须将他们的代码发送给不可信的第三方进行处理（例如，通过API调用）。这种失去控制权的情况对许多组织来说是不可接受的。在本文中，我们提出了一种替代方案：我们使用由GPT-3.5生成的样本输出来训练一个开源模型，这个过程与知识蒸馏相关。我们的模型足够小（350m参数）以在单个16GB GPU上运行，但我们在评估中展示了它足够大以模仿GPT-3。

    A code summary is a brief natural language description of source code. Summaries are usually only a single sentence long, and yet form the backbone of developer documentation. A short descriptions such as "changes all visible polygons to the color blue" can give a programmer a high-level idea of what code does without the effort of reading the code itself. Recently, products based on Large Language Models such as ChatGPT have demonstrated a strong ability to write these descriptions automatically. However, to use these tools, programmers must send their code to untrusted third parties for processing (e.g., via an API call). This loss of custody is not acceptable to many organizations. In this paper, we present an alternative: we train an open source model using sample output generated by GPT-3.5 in a process related to knowledge distillation. Our model is small enough (350m parameters) to be run on a single 16gb GPU, yet we show in our evaluation that it is large enough to mimic GPT-3.
    
[^343]: 迭代层次和排名过程（IHRP）：一种新颖的适用于密集连接系统的层次建模方法及在学生表现评估中的应用案例研究

    Iterative Hierarchy and Ranking Process (IHRP): A Novel Effective Hierarchy Method for Densely Connected Systems and Case Study in Student Performance Assessment

    [https://arxiv.org/abs/2306.10409](https://arxiv.org/abs/2306.10409)

    本文介绍了一种新颖的迭代层次和排名过程(IHRP)方法，用于在密集连接系统中构建层次结构，并通过直觉模糊语言学方法计算因素的相对重要性。这种方法在学生表现评估等决策问题中具有显著效果。

    

    在现实生活中的决策问题中，确定因素对决策属性的影响是其中主要任务之一。为了对决策属性产生最大影响，找到因素之间合适的层次结构，并确定它们在系统中的重要性价值就变得非常重要。诠释结构模型(ISM)是一种广泛使用的基于专家意见挖掘因素相互影响的层次构建方法。本文讨论了常规ISM方法在因素之间密集关联的系统中的一个主要缺点。我们称这样的系统为“密集系统”。我们提出了一种新颖的迭代层次构建技术，称为“迭代层次和排名过程”（IHRP），该方法在这样的密集系统中的效果显著。为了考虑专家意见的模糊性，研究工作中使用了直觉模糊语言学方法。本文提出了一种相对重要性的两阶段计算方法。

    In real-life decision-making problems, determining the influences of the factors on the decision attribute is one of the primary tasks. To affect the decision attribute most, finding a proper hierarchy among the factors and determining their importance values in the system becomes quite important. Interpretive structural modeling (ISM) is a widely used hierarchy-building method that mines factor inter-influences based on expert opinions. This paper discusses one of the main drawbacks of the conventional ISM method in systems where the factors are densely interrelated. We refer to such systems as "dense systems". We propose a novel iterative hierarchy-building technique, called 'Iterative Hierarchy and Ranking Process'(IHRP) which performs effectively in such dense systems. To take the vagueness of the expert opinions into account, intuitionistic fuzzy linguistics has been used in the research work. In this paper, we propose a two-stage calculation of the relative importance of the fact
    
[^344]: LAraBench：基于大语言模型进行阿拉伯语AI的基准测试

    LAraBench: Benchmarking Arabic AI with Large Language Models

    [https://arxiv.org/abs/2305.14982](https://arxiv.org/abs/2305.14982)

    LAraBench是一个针对阿拉伯语自然语言处理和语音处理任务的基准测试平台，通过多种实验设置和性能衡量指标，证明最新模型通常表现优于大语言模型（LLMs）。

    

    近期大语言模型（LLMs）的进展显著影响了语言和语音研究领域。尽管取得了进步，但这些模型尚缺乏特定语言和任务的最新模型进行对比的基准测试。LAraBench针对阿拉伯自然语言处理（NLP）和语音处理任务提供了这方面的解决方案，包括序列标注和跨不同领域的内容分类。我们采用了GPT-3.5-turbo、GPT-4、BLOOMZ、Jais-13b-chat、Whisper和USM等模型，运用零样本学习和少样本学习技术，应对了33个独立任务和61个公开可用的数据集。这涉及98个实验设置，包括约296K个数据点、约46小时的语音和30个用于文本到语音（TTS）的句子。这一努力产生了330+组实验。我们的分析重点是衡量最新模型和LLMs之间的性能差距。总体趋势表明，最新模型一般表现更优。

    Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperf
    
[^345]: SFT-KD-Recon:学习MRI重建中学生友好的知识蒸馏教师

    SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation in Magnetic Resonance Image Reconstruction

    [https://arxiv.org/abs/2304.05057](https://arxiv.org/abs/2304.05057)

    SFT-KD-Recon是一种学生友好的教师训练策略，使教师了解学生的结构和能力，并将教师的表示与学生的表示相一致，以实现MRI重建中的知识蒸馏。

    

    磁共振成像(MRI)加速的深级联架构在提供高质量重建方面取得了显著成功。然而，随着级联数量的增加，重建的改进往往变得边际，这可能表明存在过多的模型容量。知识蒸馏(KD)是一种新兴的压缩这些模型的技术，其中使用训练有素的深度教师网络将知识蒸馏给较小的学生网络，使学生学习模仿教师的行为。大多数KD方法侧重于有效地训练不知道学生模型的预训练教师。我们提出了SFT-KD-Recon，一种学生友好的教师训练方法，以及学生作为KD的先决步骤，使教师了解学生的结构和能力，并使教师的表示与学生的表示相一致。在SFT中，教师与展开的支路配置同时训练。

    Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration have shown remarkable success in providing high-quality reconstruction. However, as the number of cascades increases, the improvements in reconstruction tend to become marginal, indicating possible excess model capacity. Knowledge distillation (KD) is an emerging technique to compress these models, in which a trained deep teacher network is used to distill knowledge to a smaller student network such that the student learns to mimic the behavior of the teacher. Most KD methods focus on effectively training the student with a pre-trained teacher unaware of the student model. We propose SFT-KD-Recon, a student-friendly teacher training approach along with the student as a prior step to KD to make the teacher aware of the structure and capacity of the student and enable aligning the representations of the teacher with the student. In SFT, the teacher is jointly trained with the unfolded branch configurations of t
    
[^346]: 使用迁移学习的多目标进化剪枝深度神经网络来提高其性能和鲁棒性

    Multiobjective Evolutionary Pruning of Deep Neural Networks with Transfer Learning for improving their Performance and Robustness

    [https://arxiv.org/abs/2302.10253](https://arxiv.org/abs/2302.10253)

    这项工作提出了一种名为MO-EvoPruneDeepTL的多目标进化剪枝算法，使用迁移学习来提高深度神经网络的性能和鲁棒性。en_tdlr: This work presents MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm that uses transfer learning to improve the performance and robustness of deep neural networks.

    

    进化计算算法已被用于解决与架构、超参数或训练配置相关的优化问题，形成了现在所称的神经架构搜索领域。这些算法已与神经网络剪枝等技术相结合，将网络的复杂性减小，以及迁移学习等技术相结合，使其能够从其他相关问题中导入知识。同时，使用多个准则来评估进化提议的质量也是常见的情况，其中网络的性能和复杂性是最常用的准则。本文提出了一种名为MO-EvoPruneDeepTL的多目标进化剪枝算法。MO-EvoPruneDeepTL使用迁移学习来调整深度神经网络的最后几层，通过用遗传算法进化出的稀疏层替换它们，从而指导基于性能、复杂性和鲁棒性的进化过程。

    Evolutionary Computation algorithms have been used to solve optimization problems in relation with architectural, hyper-parameter or training configuration, forging the field known today as Neural Architecture Search. These algorithms have been combined with other techniques such as the pruning of Neural Networks, which reduces the complexity of the network, and the Transfer Learning, which lets the import of knowledge from another problem related to the one at hand. The usage of several criteria to evaluate the quality of the evolutionary proposals is also a common case, in which the performance and complexity of the network are the most used criteria. This work proposes MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm. MO-EvoPruneDeepTL uses Transfer Learning to adapt the last layers of Deep Neural Networks, by replacing them with sparse layers evolved by a genetic algorithm, which guides the evolution based in the performance, complexity and robustness of the netw
    
[^347]: 带有分布式量化流的GFlowNets

    Distributional GFlowNets with Quantile Flows

    [https://arxiv.org/abs/2302.05793](https://arxiv.org/abs/2302.05793)

    本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。

    

    生成式流网络（GFlowNets）是一种新的概率采样器系列，其中代理通过一系列决策步骤学习生成复杂组合结构的随机策略。尽管受强化学习启发，当前的GFlowNet框架在适用性上相对有限，无法处理奖励函数中的随机性。在这项工作中，我们采用分布式范式来处理GFlowNets，将每个流函数转化为一个分布，从而在训练过程中提供更多信息的学习信号。通过通过量化函数对每个边流进行参数化，我们提出的“量化匹配” GFlowNet学习算法能够学习风险敏感的策略，这是处理风险不确定性场景的基本组成部分。此外，我们发现与之前的方法相比，分布式方法由于我们增强的训练算法，可以在现有基准上实现显着改进。

    Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
    
[^348]: 电影中少样本情感理解作为元学习心智模型评价的研究

    Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind

    [https://arxiv.org/abs/2211.04684](https://arxiv.org/abs/2211.04684)

    人们在阅读故事时，通过对虚构和真实人物的类比，可以快速理解新的虚构角色。本研究填补了现有研究中忽视的少样本和元学习的心智模型（ToM）的重要性。我们提供了一个新的NLP数据集ToM-in-AMC，该数据集提供了一个评价机器元学习心智模型的现实叙事理解场景。

    

    在阅读故事时，人类可以通过将其与他们已经了解的虚构和真实人物进行类比，迅速理解新的虚构角色。这反映了人类对角色内心状态（即心智模型）的推理中少样本和元学习的本质，现有研究在这方面很大程度上被忽视了。我们通过提供一个新颖的NLP数据集ToM-in-AMC来填补这一空白，这是第一个以现实叙事理解场景为背景的机器元学习心智模型评价。我们的数据集包含约1000个分析过的电影剧本，每个剧本对应于一个需要模型模仿人类快速理解新电影中的角色的少样本情感理解任务。

    When reading a story, humans can quickly understand new fictional characters with a few observations, mainly by drawing analogies to fictional and real people they already know. This reflects the few-shot and meta-learning essence of humans' inference of characters' mental states, i.e., theory-of-mind (ToM), which is largely ignored in existing research. We fill this gap with a novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario. Our dataset consists of ~1,000 parsed movie scripts, each corresponding to a few-shot character understanding task that requires models to mimic humans' ability of fast digesting characters with a few starting scenes in a new movie.   We propose a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions. It surpasses existing baseline models, underscoring the significance of modeling multiple ToM dimensions for our task. Our extensive hu
    
[^349]: ANAct: 自适应归一化的激活函数

    ANAct: Adaptive Normalization for Activation Functions

    [https://arxiv.org/abs/2208.13315](https://arxiv.org/abs/2208.13315)

    本文研究了激活函数对神经网络前向和反向传播的负面影响，并提出了一种自适应归一化的激活函数方法ANAct来保持一致的梯度差异，通过实验证明了其有效性。

    

    本文研究了激活函数对前向和反向传播的负面影响，以及如何抵消这种影响。首先，我们考察了激活函数对神经网络前向和反向传播的影响，并推导出了梯度差异的一般形式，扩展了此领域的先前工作。我们尝试使用小批量统计来动态更新归一化因子，以确保在训练过程中始终保持归一化属性，而不仅仅考虑权重初始化后的神经网络状态。其次，我们提出了ANAct方法，该方法对激活函数进行归一化，以在各层之间保持一致的梯度差异，并通过实验证明了其有效性。我们观察到收敛速度与归一化属性大致相关。我们将ANAct与几种常见的激活函数在卷积神经网络和残差网络上进行比较，并显示ANAct可以持续改善它们的性能。

    In this paper, we investigate the negative effect of activation functions on forward and backward propagation and how to counteract this effect. First, We examine how activation functions affect the forward and backward propagation of neural networks and derive a general form for gradient variance that extends the previous work in this area. We try to use mini-batch statistics to dynamically update the normalization factor to ensure the normalization property throughout the training process, rather than only accounting for the state of the neural network after weight initialization. Second, we propose ANAct, a method that normalizes activation functions to maintain consistent gradient variance across layers and demonstrate its effectiveness through experiments. We observe that the convergence rate is roughly related to the normalization property. We compare ANAct with several common activation functions on CNNs and residual networks and show that ANAct consistently improves their perfo
    
[^350]: 将子图转化为节点让简单的图神经网络在子图表示学习上更强大和高效

    Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning

    [https://arxiv.org/abs/2204.04510](https://arxiv.org/abs/2204.04510)

    提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。

    

    子图表示学习已经成为一个重要的问题，并且通常使用专门的图神经网络来处理大型全局图。这些模型需要大量的内存和计算资源，但挑战子图的层次结构建模。在本文中，我们提出了子图到节点（S2N）转换的新颖公式，用于学习子图的表示。具体而言，给定全局图中的一组子图，我们通过粗略地将子图转换成节点来构建一个新的图。通过理论和实证证据，S2N不仅相比最先进的模型显著减少了内存和计算成本，而且通过捕捉子图的局部和全局结构也在性能上超过了它们。通过利用图粗化方法，我们的方法甚至在数据稀缺的情况下也优于基线模型。我们在八个基准测试上的实验表明，调整模型后效果出色。

    Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
    
[^351]: 领航神经空间：重新审视概念激活向量以克服方向差异

    Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence

    [https://arxiv.org/abs/2202.03482](https://arxiv.org/abs/2202.03482)

    本文重新审视了概念激活向量（CAVs）在建模人类可理解的概念中的应用，并引入了基于模式的CAVs来提供更准确的概念方向。

    

    随着对于理解神经网络预测策略的兴趣日益增长，概念激活向量（CAVs）已成为一种流行的工具，用于在潜在空间中建模人类可理解的概念。通常，CAVs是通过利用线性分类器来计算的，该分类器优化具有给定概念和无给定概念的样本的潜在表示的可分离性。然而，在本文中我们展示了这种以可分离性为导向的计算方法会导致与精确建模概念方向的实际目标发散的解决方案。这种差异可以归因于分散方向的显著影响，即与概念无关的信号，这些信号被线性模型的滤波器（即权重）捕获以优化类别可分性。为了解决这个问题，我们引入基于模式的CAVs，仅关注概念信号，从而提供更准确的概念方向。我们评估了各种CAV方法与真实概念方向的对齐程度。

    With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept dire
    
[^352]: 通过增加表示来编码时间统计空间先验

    Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])

    [http://arxiv.org/abs/2401.16808](http://arxiv.org/abs/2401.16808)

    通过增加表示的方式编码时间统计空间先验，以应对时间序列数据建模中的挑战。我们的方法在两个数据集上的实证推广性能明显优于五个最新的基准方法。具有高度模块化性质的方法适用于各种场景。

    

    时间序列数据建模仍然是一个普遍存在的问题，因为时间维度与许多领域密切相关。尽管在时间序列预测方面取得了显著进展，但高噪声信号比、非正态性、非平稳性和数据缺乏仍然是挑战从业者的问题。为此，我们利用一种简单的表示增强技术来克服这些挑战。我们的增强表示在每个时间步骤上作为统计空间先验进行编码。作为响应，我们将我们的方法命名为统计空间增强表示（SSAR）。基于高维数据生成过程，启发了我们的表示增强。我们在两个数据集上对两个下游时间学习算法的经验泛化性能进行了严格的检查。我们的方法明显击败了五个最新的基准线。此外，我们的方法具有高度模块化的性质，可以轻松应用于各种情况。最后，我们提供了全面的理论视角。

    Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
    
[^353]: Context-Former：基于潜在条件序列建模的拼接技术

    Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])

    [http://arxiv.org/abs/2401.16452](http://arxiv.org/abs/2401.16452)

    Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。

    

    离线强化学习（RL）算法可以通过拼接次优轨迹来改善决策，从而获得更优的结果。这种能力是使RL能够学习优于行为策略的策略的关键因素。另一方面，Decision Transformer（DT）将决策建模为序列建模，展示了在离线RL基准测试中竞争性的性能，然而，最近的研究表明DT缺乏拼接能力，因此提高DT性能需要利用拼接能力。为了赋予DT拼接能力，我们将轨迹拼接抽象为专家匹配，并引入了我们的方法ContextFormer，通过模拟有限数量的专家轨迹的表示来集成基于情境信息的模仿学习（IL）和序列建模，以拼接次优轨迹片段。为了验证我们的观点，我们从两个角度进行实验证明：

    Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
    
[^354]: 朝着稳定的利益相关方一致化机器学习偏好迈进

    Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])

    [http://arxiv.org/abs/2401.15268](http://arxiv.org/abs/2401.15268)

    本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。

    

    针对肾脏分配的紧迫挑战，即需求增长与利益相关方价值的结合，本研究旨在开发一个以数据驱动的解决方案，以解决这个问题。该研究的主要目标是创建一种学习个人和团体关于肾脏分配的偏好的方法。通过利用“成对肾脏患者在线调查”的数据，结合两个不同的数据集，并在个人、团体和稳定性三个层面上进行评估，我们使用机器学习分类器并通过几种度量方法进行评估。个人层面模型预测个体参与者的偏好，团体层面模型汇总参与者偏好，稳定性层面模型是团体升级的扩展，评估这些偏好随时间的稳定性。通过将利益相关方的偏好纳入肾脏分配过程，我们希望推动伦理维度的发展。

    In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
    
[^355]: 在临床文本中预测实体修饰语的迁移学习：以阿片类物质使用障碍病例检测为应用

    Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])

    [http://arxiv.org/abs/2401.15222](http://arxiv.org/abs/2401.15222)

    本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。

    

    背景：从临床文本中提取的实体的语义可能会受到修饰语的显著改变，包括实体的否定、不确定性、条件性、严重性和主观性。现有的确定临床实体修饰语的模型涉及使用正则表达式或特征权重，这些权重是独立训练每个修饰语的。方法：我们开发并评估了一个多任务变换器架构设计，在公开可用的SemEval 2015任务14语料库和一个新的阿片类物质使用障碍（OUD）数据集上共同学习和预测修饰语，该数据集包含与SemEval共享的修饰语以及OUD特定的新修饰语。我们评估了我们的多任务学习方法与以前发表的系统的效果，并评估了仅共享部分临床修饰语时的临床实体修饰语的迁移学习的可行性。结果：我们的方法在来自SemEval 2015的ShARe语料库上取得了最新技术的结果。

    Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
    
[^356]: 通过使用特征组合和并行结构分类器增强无文本约束的说话人验证系统

    Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers. (arXiv:2401.15018v1 [eess.AS])

    [http://arxiv.org/abs/2401.15018](http://arxiv.org/abs/2401.15018)

    本文通过使用特征组合和并行结构分类器来增强无文本约束的说话人验证系统的性能，在特征提取和分类阶段进行了探索，提出了不同声学特征的组合比较，并针对传统支持向量机分类器的弱点进行了改进。

    

    说话人验证系统主要包含两个独立的阶段：特征提取和分类。本文旨在探索这两个模块，提高在嘈杂环境下说话人验证系统的性能。首先，选择最适合的声学特征是进行鲁棒性说话人验证的关键因素。本文提出系统中使用的声学参数包括：梅尔频率倒谱系数（MFCC），它们的一阶和二阶导数（Deltas和Delta-Deltas），巴克频率倒谱系数（BFCC），感知线性预测（PLP）和相对谱变换感知线性预测（RASTA-PLP）。本文对不同特征组合进行了全面比较。其次，传统支持向量机（SVM）分类器的主要弱点是使用通用传统的核函数计算数据点之间的距离。

    Speaker Verification (SV) systems involve mainly two individual stages: feature extraction and classification. In this paper, we explore these two modules with the aim of improving the performance of a speaker verification system under noisy conditions. On the one hand, the choice of the most appropriate acoustic features is a crucial factor for performing robust speaker verification. The acoustic parameters used in the proposed system are: Mel Frequency Cepstral Coefficients (MFCC), their first and second derivatives (Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC), Perceptual Linear Predictive (PLP), and Relative Spectral Transform Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison of different combinations of the previous features is discussed. On the other hand, the major weakness of a conventional Support Vector Machine (SVM) classifier is the use of generic traditional kernel functions to compute the distances among data points
    
[^357]: 机器视觉冰山的解释：通过考虑全面环境条件推进动态测试

    The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances. (arXiv:2401.14831v1 [cs.RO])

    [http://arxiv.org/abs/2401.14831](http://arxiv.org/abs/2401.14831)

    本研究提出了一个名为粒度等级的层次级别模型，用于更好地理解机器视觉在不同环境条件下的操作。这一模型旨在提供对影响机器视觉功能的各个实体的全面概述。

    

    当前的机器视觉测试是否会带来冰山的危险？本研究深入探讨了机器视觉（MV）测试的领域，这在高度自动驾驶（HAD）系统中是非常必要的。借助向冰山航行的隐喻，我们讨论了当前测试策略中潜在的缺陷。我们强调了对如何处理MV在开发过程中的不透明功能的更深入了解的紧迫需要，因为忽视了这些考虑可能会造成生命的丧失。我们的主要贡献是层次级别模型，我们将其称为粒度等级。该模型鼓励对MV操作环境条件的各个层次进行精细探索，从个体实体之间的关系到整个环境场景。该模型旨在提供对可能影响MV功能的所有实体的全面概述。

    Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of
    
[^358]: 对AI代理的可见性

    Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])

    [http://arxiv.org/abs/2401.13138](http://arxiv.org/abs/2401.13138)

    本文评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录，并提出了可能的实施方式。这些措施在集中化和去中心化部署环境中应用广泛，有助于理解和减轻AI代理带来的风险。

    

    将商业、科学、政府和个人活动委托给具有有限监督能力的AI代理系统，可能会加剧现有的社会风险并引入新的风险。理解和减轻这些风险涉及对现有治理结构进行批判性评估，根据需要进行修订和调整，并确保关键利益相关者的问责制。我们将AI代理的使用地点、原因、方式以及使用者等信息称为“可见性”，这对于实现上述目标至关重要。在本文中，我们评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录。对于每一种措施，我们概述了可能的实施方式，这些方式在侵入性和信息性方面有所差异。我们分析了这些措施在集中化和去中心化部署环境中的应用情况，考虑了不同变量的影响。

    Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
    
[^359]: 多语言语言模型中的跨语言编辑

    Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])

    [http://arxiv.org/abs/2401.10521](http://arxiv.org/abs/2401.10521)

    本论文介绍了跨语言模型编辑（XME）范式，通过在一种语言中编辑事实并观察其对其他语言的更新传播，研究了多语言语言模型中的模型编辑技术（MET）的性能限制。

    

    大规模语言模型（LLM）的训练需要大量的数据和计算资源，而更新过时的LLM需要大量的工作和资源。虽然出现了许多模型编辑技术（MET）以便在不重新训练的情况下高效更新模型输出，但在多语言LLM中，其中的知识以多种语言存储，这仍然是一个未深入研究的领域。本研究介绍了跨语言模型编辑（XME）范式，在该范式中，一个事实在一种语言中被编辑，观察其在其他语言中的更新传播。为了研究XME范式，我们使用BLOOM、mBERT和XLM-RoBERTa进行了实验，使用了两种写作脚本，即拉丁语（英语、法语和西班牙语）和印地语（印地语、古吉拉特语和孟加拉语）。结果显示，在XME设置下，当前最先进的MET存在明显的性能限制，特别是当涉及的语言属于两个不同的语族时。

    The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin
    
[^360]: DCRMTA: 无偏的多触点归因的因果表示

    DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])

    [http://arxiv.org/abs/2401.08875](http://arxiv.org/abs/2401.08875)

    DCRMTA提出了一种无偏的多触点归因方法，通过建立转化预测模型和构建对照触点序列来减轻偏差的影响。

    

    多触点归因（MTA）在实现对每个广告触点对于转化行为的贡献的公正估计方面起着关键作用，深刻影响预算分配和广告推荐。传统的多触点归因方法首先构建一个转化预测模型，通过历史数据学习触点序列和用户购买行为之间的内在关系。在此基础上，从原始序列子集中构建对照触点序列，并使用预测模型估计转化，从而计算广告贡献。这些方法的一个隐含假设是转化预测模型的无偏性。然而，由于用户偏好和互联网推荐机制（如过去的购物记录导致的广告推荐同质化）引起的混杂变量因素，转化中很容易产生偏差。

    Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversi
    
[^361]: DrawTalking：通过草图和语言建立互动世界

    DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])

    [http://arxiv.org/abs/2401.05631](http://arxiv.org/abs/2401.05631)

    用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。

    

    我们引入了一种交互式方法，DrawTalking，用户可以通过草图和语言建立互动世界。它强调用户控制和灵活性，并且在没有编程的情况下提供了类似编程的能力。我们在iPad上实现了它。一项开放式研究表明，这种机制与许多创造性探索性用例相契合和适用。我们希望能够激发和指导未来自然用户中心界面的研究。

    We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
    
[^362]: CreINNs: Credal-Set Interval Neural Networks用于分类任务中的不确定性估计

    CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])

    [http://arxiv.org/abs/2401.05043](http://arxiv.org/abs/2401.05043)

    CreINNs是一种用于分类任务的Credal-Set Interval Neural Networks，通过保留传统的区间神经网络结构，捕捉权重不确定性，并使用概率区间的数学框架预测可信区间。实验结果表明，CreINNs在不确定性估计方面优于变分贝叶斯神经网络和深度集成，并且具有较低的计算复杂度和模型大小。

    

    不确定性估计对于提高神经网络的可靠性越来越有吸引力。在这项工作中，我们提出了新颖的Credal-Set Interval Neural Networks（CreINNs），用于分类任务。CreINNs保留了传统的区间神经网络结构，通过确定性区间捕捉权重的不确定性，同时使用概率区间的数学框架预测可信区间。在一个超出分发检测基准（CIFAR10 vs SVHN）上的实验验证中，CreINNs相比于变分贝叶斯神经网络（BNNs）和深度集成（DEs），在认知不确定性估计方面表现出色。此外，与变分BNNs相比，CreINNs的计算复杂度显著降低，并且比DEs具有较小的模型大小。

    Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
    
[^363]: NODEC: 用于未知动态系统最优控制的神经ODE

    NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems. (arXiv:2401.01836v1 [cs.AI])

    [http://arxiv.org/abs/2401.01836](http://arxiv.org/abs/2401.01836)

    NODEC是一种用神经ODE模型将动力学建模与控制器训练相结合的新框架，用于控制未知动态系统。

    

    控制复杂动态系统通常涉及在变化计算框架下最小化具有已知动力学的某些控制目标。对于具有未知动力学的系统，需要额外进行动力学建模。然而，动力学建模的任何不准确都会导致结果控制函数的次优性。另一种用于控制未知动态系统的方法 - 强化学习，将动力学建模融入控制器训练中，通过与环境的广泛交互来近似值函数或策略梯度，但它的数据效率低。为了解决这些问题，我们引入了NODEC，这是一种用神经ODE模型将动力学建模与控制器训练相结合的新框架。通过两个耦合神经网络之间的有趣相互作用，NODEC学习了系统动力学以及指导未知动态系统的最优控制。

    Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknow
    
[^364]: 自扩展LLM:无需调整的LLM上下文窗口。

    LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])

    [http://arxiv.org/abs/2401.01325](http://arxiv.org/abs/2401.01325)

    本研究提出了一种名为Self-Extend的方法，通过自身扩展现有LLMs的上下文窗口，无需调整，充分利用LLMs处理长上下文的固有能力。

    

    本研究揭示了LLM在处理长上下文时的固有能力，而无需进行精调。在训练过程中，训练序列的有限长度可能限制了大型语言模型（LLMs）在推理过程中对长输入序列的应用。在本研究中，我们认为现有的LLMs本身具有处理长上下文的固有能力。基于这一观点，我们建议通过自身扩展LLMs的上下文窗口，以充分利用其固有能力。我们提出了Self-Extend方法来激发LLMs的长上下文处理潜力。基本思想是构建双层注意信息：群组级和邻居级。这两个级别通过原始模型的自注意力计算，这意味着所提方法不需要任何训练。只需修改四行代码，所提方法就可以轻松扩展现有LLMs的上下文窗口，而无需进行任何精调。我们进行了全面的实验证明，结果表明所提方法可以+摘要减掉文章最后一句話

    This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
    
[^365]: LLM-SAP: 大语言模型情景感知的基于规划

    LLM-SAP: Large Language Model Situational Awareness Based Planning. (arXiv:2312.16127v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.16127](http://arxiv.org/abs/2312.16127)

    本文提出了一种基于大型语言模型的情景感知的紧急规划能力评估方法，并提供了新的基准和指标，以及独特的数据集。研究结果表明，提示和多智能体方案可以显著提高上下文敏感规划任务的性能。

    

    本文首次在大型语言模型中评估基于情景感知的紧急规划能力。我们贡献了（i）用于标准化评估的新型基准和指标；（ii）一个独特的数据集来推动进展；以及（iii）演示以提示和多智能体方案显著提高了上下文敏感规划任务的性能。将其纳入到一个处境智能体和自动化规划研究中，我们强调了固有的可靠性挑战-尽管在模拟领域的进步中，如何在没有环境指导的情况下有效地将世界状态映射到行动仍然是一个开放的问题。尽管超出范围，对于验证方法和数据可用性的限制表明了令人兴奋的方向，包括对扩展规划语料库进行微调和针对快速潜在规划的优化。通过通过严密的比较来确实地展示当前方法的承诺和局限性，我们推动了对可靠目标导向推理的研究

    This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models. We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks. Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances. Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning. By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning f
    
[^366]: 通过整数规划对温顺函数进行分段多项式回归

    Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2311.13544](http://arxiv.org/abs/2311.13544)

    本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。

    

    我们考虑估计属于一类特定的非光滑函数的函数的任务，即所谓的温顺函数。这些函数出现在各种应用中：深度学习的训练、混合整数规划的价值函数或小分子的波函数。我们展示了温顺函数在任何完全维度的立方体上可用分段多项式来逼近。然后我们提出了第一个分段多项式回归的混合整数规划形式。这些方法可用于估计温顺函数。我们展示了令人期待的计算结果。

    We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
    
[^367]: 从视觉-语言基础模型中提取对抗性鲁棒性的框架

    Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])

    [http://arxiv.org/abs/2311.01441](http://arxiv.org/abs/2311.01441)

    本文提出了一个概念简单且轻量级的框架，通过结合知识蒸馏和数据增强的方法来提高视觉模型的鲁棒性，并从预训练的基础模型中获得鲁棒教师模型的知识。借助离散对抗蒸馏方法，我们生成更有信息量的对抗样本，取得了在对抗性样本上鲁棒性显著提升的结果。此外，我们提供了理论框架来支持在知识蒸馏和数据增强设置中使用鲁棒教师模型，并展示了在不同学生模型上的显著性能提升。我们的方法在计算负载方面的开销较小，并可以与其他数据增强方法轻松结合。

    

    我们提出了一个概念简单且轻量级的框架，通过知识蒸馏和数据增强的结合来提高视觉模型的鲁棒性。我们通过从预训练的基础模型中进行蒸馏，展示了在对抗性样本上获得的鲁棒性增益，以此反驳了更大的模型不一定会成为更好的教师的猜想。我们还提出了离散对抗蒸馏（Discrete Adversarial Distillation，DAD）方法，利用鲁棒的教师模型生成对抗样本，并通过VQGAN将其离散化，从而创造出比标准数据增强技术更有信息量的样本。我们提供了一个理论框架，用于在知识蒸馏和数据增强的设置中使用鲁棒的教师模型，并在不同的学生模型中展示了在对抗性样本上的鲁棒性和干净准确性的显著提升。值得注意的是，与类似技术相比，我们的方法增加了少量的计算负载，并且可以轻松与其他数据增强方法相结合。

    We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen
    
[^368]: Vision-Language Foundation Models作为有效的机器人模仿者

    Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])

    [http://arxiv.org/abs/2311.01378](http://arxiv.org/abs/2311.01378)

    该论文介绍了一种利用视觉语言基础模型进行机器人操作的方法，通过在语言条件的操作数据集上进行微调，实现了在低性能平台上的有效模仿控制。

    

    最近在视觉语言基础模型方面的进展显示出它们理解多模态数据和解决复杂的视觉语言任务（包括机器人操作）的能力。我们寻求一种简单的方式来利用现有的视觉语言模型（VLMs）在机器人数据上进行简单微调。为此，我们提出了一个简单而新颖的视觉语言操作框架，名为RoboFlamingo，它建立在开源的VLMs，OpenFlamingo之上。与以前的工作不同，RoboFlamingo利用预训练的VLMs进行单步视觉语言理解，使用显式策略头模拟顺序历史信息，并只在语言条件的操作数据集上进行微调。这种分解为RoboFlamingo提供了在低性能平台上进行开环控制和部署的灵活性。通过在测试基准上大幅超过现有技术水平，我们展示了RoboFlamingo可以成为一种有效的机器人模仿者。

    Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
    
[^369]: 基于能源的法律领域文本分类常见方法的比较分析

    An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])

    [http://arxiv.org/abs/2311.01256](http://arxiv.org/abs/2311.01256)

    本研究通过在法律领域的文本分类任务上进行比较分析，综合考虑性能和能源消耗等指标，探讨了大型语言模型与传统方法的优劣，并强调了在性能相近的情况下应重视生产成本、能源消耗和碳足迹等方面的考量。

    

    大部分机器学习研究评估最佳解决方案的性能。然而，在追求最佳性能的竞争中，经常忽视许多重要因素，而事实上，这些因素应该被仔细考虑。实际上，有时不同方法之间的性能差距可以忽略不计，而生产成本、能源消耗和碳足迹等因素必须考虑在内。大型语言模型（LLMs）被广泛应用于学术界和工业界的NLP问题。在这项工作中，我们在LexGLUE基准上对LLM和传统方法（例如SVM）进行了详细的定量比较，同时考虑性能（标准指标）和其他指标，如时间、耗能和成本，总之就是碳足迹。在我们的分析中，我们分别考虑了原型设计阶段（通过训练-验证-测试迭代进行模型选择）和生产阶段。

    Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
    
[^370]: 强化学习的扩散模型: 一份综述

    Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])

    [http://arxiv.org/abs/2311.01223](http://arxiv.org/abs/2311.01223)

    强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。

    

    扩散模型作为一种突出的生成模型类别已经出现，超越了以往方法在样本质量和训练稳定性方面的优势。最近的研究表明，扩散模型在改进强化学习（RL）解决方案方面具有优势，包括作为轨迹规划器、表达能力丰富的策略类别、数据合成器等。本综述旨在提供该新兴领域发展的概述，并希望能启发新的研究方向。首先，我们审查了当前RL算法遇到的一些挑战。然后，我们根据扩散模型在RL中所扮演的角色，提出了现有方法的分类法，并探讨了如何解决现有挑战。我们进一步概述了扩散模型在各种与RL相关任务中的成功应用，并讨论了当前方法的局限性。最后，我们总结了这项综述，并提出了对未来研究方向的见解，重点是提高模型性能和应用扩散模型的方法。

    Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
    
[^371]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^372]: 通过理解生成：具有逻辑符号基础的神经视觉生成

    Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])

    [http://arxiv.org/abs/2310.17451](http://arxiv.org/abs/2310.17451)

    这篇论文提出了一种神经符号学习方法，AbdGen，用于将知识推理系统与神经视觉生成模型集成。它解决了符号赋值和规则学习的问题，通过量化诱导方法实现可靠高效的符号赋值，通过对比元诱导方法实现精确的规则学习。

    

    尽管近年来神经视觉生成模型取得了很大的成功，但将其与强大的符号知识推理系统集成仍然是一个具有挑战性的任务。主要挑战有两个方面：一个是符号赋值，即将神经视觉生成器的潜在因素与知识推理系统中的有意义的符号进行绑定。另一个是规则学习，即学习新的规则，这些规则控制数据的生成过程，以增强知识推理系统。为了解决这些符号基础问题，我们提出了一种神经符号学习方法，Abductive Visual Generation (AbdGen)，用于基于诱导学习框架将逻辑编程系统与神经视觉生成模型集成起来。为了实现可靠高效的符号赋值，引入了量化诱导方法，通过语义编码本中的最近邻查找生成诱导提案。为了实现精确的规则学习，引入了对比元诱导方法。

    Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
    
[^373]: DoGE: 使用泛化估计进行领域重新加权

    DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])

    [http://arxiv.org/abs/2310.15393](http://arxiv.org/abs/2310.15393)

    DoGE提出了一种基于泛化估计的领域重新加权方法。通过使用梯度估计函数评估每个领域对泛化目标的贡献，重新调整了预训练数据中不同领域的采样概率。实验结果表明，该方法在提高大型语言模型的泛化能力方面取得了显著效果。

    

    预训练数据语料库的覆盖范围和组成对大型语言模型的泛化能力有着重要影响。传统上，预训练语料库由各种来源领域（如CommonCrawl、Wikipedia、Github等）按照特定的采样概率（领域权重）组成。然而，当前的方法缺乏一种基于最终泛化目标优化领域权重的原则方法。我们提出了一种称为DOmain reweighting with Generalization Estimation（DoGE）的方法，其中我们重新调整了每个领域的采样概率，根据它对最终泛化目标的贡献进行了基于梯度的泛化估计函数评估。首先，我们使用最小最大优化训练了一个小规模的代理模型来获取重新加权的领域权重。在每一步中，通过镜像下降法更新领域权重以最大化整体的泛化增益。最后，我们使用获得的领域权重来训练一个规模更大的完整语言模型。

    The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
    
[^374]: 无限时间无折扣奖励马尔可夫决策过程自然策略梯度算法的改进样本复杂度分析

    Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])

    [http://arxiv.org/abs/2310.11677](http://arxiv.org/abs/2310.11677)

    本论文提出了一种加速自然策略梯度算法（ANPG），用于解决无限时间无折扣奖励马尔可夫决策过程。ANPG实现了样本和迭代复杂度的显著改进，克服了现有算法的局限性，并达到了最新的技术成果。

    

    本文考虑设计样本高效的学习算法，用于无限时间无折扣奖励马尔可夫决策过程。具体地，我们提出了加速自然策略梯度（ANPG）算法，利用加速随机梯度下降过程来获取自然策略梯度。ANPG算法在一般参数化情况下实现了O(ε^{-2})的样本复杂度和O(ε^{-1})的迭代复杂度，其中ε定义了最优性误差。这将样本复杂度提高了一个log(1/ε)的因子。ANPG是一个一阶算法，并且不需要现有文献中可能无法验证的重要性采样(IS)权重方差上界的假设。在无Hessian和无IS算法类中，ANPG超过了已知样本复杂度的一个O(ε^{-\frac{1}{2}})的因子，并同时达到了它们的最新技术成果。

    We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their state-of-the-art it
    
[^375]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^376]: 从挫折中获得智慧：通过错误分析对齐大型语言模型

    Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10477](http://arxiv.org/abs/2310.10477)

    该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。

    

    大型语言模型（LLMs）的快速发展既带来了机遇，也带来了挑战，特别是在意外生成有害和有毒回应方面。传统的对齐方法致力于引导LLMs朝着期望的性能发展并保护它们免受恶意内容的侵害，而本研究提出了一种基于错误分析的全新对齐策略，通过有意暴露LLMs的缺陷输出并进行深入评估，以完全理解内部原因，通过自然语言分析。因此，有毒回应可以转化为模型对齐的指导调谐语料，LLMs不仅可以避免生成有缺陷的回应，还可以训练其进行自我批评，发挥其辨别有毒内容的内在能力。实验结果表明，所提出的方法在安全指令遵循方面优于传统的对齐技术，同时还保持了卓越的效率。

    The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
    
[^377]: Mirage: 图分类的模型无关图蒸馏

    Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])

    [http://arxiv.org/abs/2310.09486](http://arxiv.org/abs/2310.09486)

    Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。

    

    GNNs和其他深度学习模型一样，对数据和计算需求量很大。急需在大型数据集上扩展GNN的训练，以便在资源有限的环境中使用它们。图蒸馏是为此目的而努力，旨在从原始训练数据构建一个更小的合成训练集，而不会显著影响模型性能。虽然初步工作取得了一些进展，但这项工作基于两个关键观察：(1)现有的图蒸馏算法本身依赖于使用完整数据集进行训练，这就破坏了图蒸馏的前提。(2)蒸馏过程对目标GNN架构和超参数具有特异性，因此对建模流程的变化不具备鲁棒性。我们通过设计一种名为Mirage的图分类蒸馏算法来避免这些限制。Mirage建立在一个洞察的基础上，即一个消息传递的GNN将输入图分解为计算的多重集合。

    GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
    
[^378]: 通过区分位置和上下文来揭示Transformers中的隐藏几何

    Uncovering hidden geometry in Transformers via disentangling position and context. (arXiv:2310.04861v1 [cs.LG])

    [http://arxiv.org/abs/2310.04861](http://arxiv.org/abs/2310.04861)

    本文通过分解transformer的隐藏状态，揭示了其在语义理解中的隐含几何结构。

    

    Transformers广泛用于从输入令牌中提取复杂的语义意义，然而它们通常作为黑盒模型运行。本文提出了一种简单而信息丰富的方法，将训练好的transformer的隐藏状态（或嵌入）分解为可解释的组件。对于任何层，输入序列样本的嵌入向量由一个张量表示 $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$。给定在序列（或上下文） $c \le C$ 的位置 $t \le T$ 处的嵌入向量 $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$，提取均值效果得到分解形式 \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] 其中 $\boldsymbol{\mu}$ 是全局均值向量，$\mathbf{pos}_t$ 和 $\mathbf{ctx}_c$ 分别是跨上下文和跨位置的均值向量，$\mathbf{resid}_{c,t}$ 是残余向量。针对流行的transformer架构和多样的文本数据集，经验结果表明...

    Transformers are widely used to extract complex semantic meanings from input tokens, yet they usually operate as black-box models. In this paper, we present a simple yet informative decomposition of hidden states (or embeddings) of trained transformers into interpretable components. For any layer, embedding vectors of input sequence samples are represented by a tensor $\boldsymbol{h} \in \mathbb{R}^{C \times T \times d}$. Given embedding vector $\boldsymbol{h}_{c,t} \in \mathbb{R}^d$ at sequence position $t \le T$ in a sequence (or context) $c \le C$, extracting the mean effects yields the decomposition \[ \boldsymbol{h}_{c,t} = \boldsymbol{\mu} + \mathbf{pos}_t + \mathbf{ctx}_c + \mathbf{resid}_{c,t} \] where $\boldsymbol{\mu}$ is the global mean vector, $\mathbf{pos}_t$ and $\mathbf{ctx}_c$ are the mean vectors across contexts and across positions respectively, and $\mathbf{resid}_{c,t}$ is the residual vector. For popular transformer architectures and diverse text datasets, empirica
    
[^379]: Memoria: 用于类人顺序处理的海比安记忆体架构

    Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])

    [http://arxiv.org/abs/2310.03052](http://arxiv.org/abs/2310.03052)

    Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。

    

    Transformer 在多个领域和任务中取得了成功。然而，由于其有限的容量，Transformer 很难处理长输入序列。虽然增加输入长度是一个解决方案，但无止境地增加长度是不现实的。此外，与 Transformer 不同，人类有选择性地记住和使用仅与输入相关的信息，而不是从头到尾处理所有原始数据。我们引入了 Memoria，一个应用海比安记忆形成理论的通用记忆网络，用于增强神经网络中的长期依赖。Memoria 在工作记忆、短期记忆和长期记忆的多个记忆层级上存储和检索称为 engram 的信息，使用根据海布规则变化的连接权重。通过与诸如 BERT 和 GPT 等流行的基于 Transformer 的模型进行实验，我们提出 Memoria 显著提高了在各种任务中考虑长期依赖的能力。结果

    Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
    
[^380]: 3D物理系统中学习对称性破缺的松弛八面体群卷积

    Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])

    [http://arxiv.org/abs/2310.02299](http://arxiv.org/abs/2310.02299)

    本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。

    

    深度等价模型利用对称性提高样本效率和泛化性能。然而，在许多这些模型中，完美对称性的假设有时可能会限制性能，特别是当数据与这些对称性不完全一致时。因此，我们在本文中引入了用于建模3D物理系统的松弛八面体群卷积。这种灵活的卷积技术能够在保持与数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。实证结果验证了我们的方法不仅可以揭示相变中的对称性破缺因素，还可以在流体超分辨率任务中实现卓越性能。

    Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
    
[^381]: AlignDiff: 通过可定制行为扩散模型对齐多样的人类偏好

    AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model. (arXiv:2310.02054v1 [cs.AI])

    [http://arxiv.org/abs/2310.02054](http://arxiv.org/abs/2310.02054)

    本研究提出了一种新的框架AlignDiff，通过将强化学习与人类反馈相结合，量化人类偏好并指导零-shot行为定制的扩散规划，解决了将代理行为与多样的人类偏好相一致的挑战问题。

    

    将代理行为与多样的人类偏好相一致仍然是强化学习中的一个挑战问题，这是由于人类偏好的抽象性和可变性的内在特性所致。为了解决这些问题，我们提出了AlignDiff，一种利用强化学习从人类反馈中量化人类偏好的新颖框架，涵盖了抽象性，并利用这些偏好来引导零-shot行为定制的扩散规划，涵盖了可变性。AlignDiff能够准确匹配用户定制的行为并高效地在不同行为之间切换。为了构建这个框架，我们首先建立了多角度的人类反馈数据集，其中包含了对不同行为属性进行比较的数据，并训练了一个属性强度模型来预测量化的相对强度。在使用相对强度重新标记行为数据集之后，我们继续训练了一个属性条件的扩散模型，该模型作为一个规划器，属性强度模型作为导演。

    Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director fo
    
[^382]: 在规划中结合空间和时间抽象以实现更好的泛化

    Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00229](http://arxiv.org/abs/2310.00229)

    Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。

    

    受到人类有意识规划的启发，我们提出了Skipper，这是一个利用时空抽象来推广在新情境中学到的技能的基于模型的强化学习代理。它自动将给定任务分解为更小、更可管理的子任务，从而实现稀疏决策和对环境相关部分的专注计算。这依赖于从回溯中学习得到的表示为有向图的抽象代理问题的提取。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法在哪些方面有望提供帮助。针对泛化的实验验证了Skipper在零样本泛化方面与现有最先进的分层规划方法相比的显著优势。

    Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
    
[^383]: 探索学习系统中信息熵变化的影响

    Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])

    [http://arxiv.org/abs/2309.10625](http://arxiv.org/abs/2309.10625)

    本研究探索了在深度学习系统中引入噪声对性能的影响，证明了特定噪声可以在降低任务复杂性的条件下提升深度架构的性能，通过实验证明了在大规模图像数据集中的显著性能提升。

    

    在本研究中，我们通过向输入/隐含特征添加噪声来探索深度学习系统中熵变化的影响。本文的应用重点是计算机视觉中的深度学习任务，但所提出的理论可以进一步应用于其他领域。噪声通常被视为各种深度学习架构（如卷积神经网络和视觉变换器）以及图像分类和迁移学习等不同学习任务中的有害扰动。然而，本文旨在重新思考传统命题是否总是成立。我们证明了在特定条件下，特定噪声可以提升各种深度架构的性能。我们在信息熵定义的任务复杂性减少方面从理论上证明了正噪声的增强效果，并在大规模图像数据集（如ImageNet）中实验证明了显著的性能提升。

    In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
    
[^384]: 多对象图形可用性网络：通过复合对象可用性实现目标导向规划

    Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances. (arXiv:2309.10426v1 [cs.RO])

    [http://arxiv.org/abs/2309.10426](http://arxiv.org/abs/2309.10426)

    多对象图形可用性网络（MOGAN）模型化了复合对象的可用性，预测了将新对象放置在复合对象上的效果。在构建具有特定高度或属性的塔等任务中，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们的系统能够正确建模非常复杂的复合对象的可用性，并在模拟和真实环境中展示了其适用性。

    

    学习对象的可用性是机器人学习领域的有效工具。虽然数据驱动的模型深入探讨了单个或成对对象的可用性，但在调查由复杂形状的任意数量的对象组成的复合对象的可用性方面存在明显差距。在本研究中，我们提出了多对象图形可用性网络（MOGAN），它建模了复合对象的可用性，并预测将新对象放置在现有复合对象上的效果。给定不同的任务，例如构建具有特定高度或属性的塔，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们展示了我们的系统能够正确建模包括堆叠的球体和杯子、杆和包围杆的环等非常复杂的复合对象的可用性。我们在模拟和真实环境中展示了我们系统的适用性。

    Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, com
    
[^385]: Talk2Care: 利用大型语言模型促进异步患者-医生通信

    Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09357](http://arxiv.org/abs/2309.09357)

    本研究利用大型语言模型（LLMs）来促进患者和医生之间的异步通信，通过访谈研究了解了他们对LLMs的需求，并构建了一个名为Talk2Care的LLM驱动的通信系统。

    

    尽管有大量的远程医疗应用程序来帮助家庭中的老年人和医疗提供者，但基本的消息和电话仍然是最常见的通信方法，这些方法存在有限的可用性、信息丢失和流程效率低下的问题。促进患者-医生通信的一个有希望的解决方案是利用大型语言模型(LLMs)及其强大的自然对话和摘要能力。然而，对于LLMs在通信过程中的作用还存在有限的理解。我们首先进行了两项访谈研究，分别与老年人(N=10)和医疗提供者(N=9)进行了交流，以了解他们在患者-医生异步通信中对LLMs的需求和机会。基于这些见解，我们构建了一个LLM驱动的通信系统Talk2Care，并为两个群体设计了交互组件: (1) 对于老年人，我们利用语音助手的便利性和易于获取性，构建了一个LLM驱动的语音助手

    Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
    
[^386]: 基于框架的大型语言模型自由回答的定性分析：算法保真度

    Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])

    [http://arxiv.org/abs/2309.06364](http://arxiv.org/abs/2309.06364)

    本文通过定性分析研究了大型语言模型生成的自由回答，重点考察了算法保真度，并提出高算法保真度可以推广到真实人类的观点。这对于使用语言模型研究人类行为具有重要意义。

    

    如今，使用大规模生成式语言模型（LLMs），可以模拟自由回答面试问题，就像传统上使用定性研究方法分析的那样。定性方法涵盖了一系列技术，涉及对开放式访谈或自由进行的自然语言对话的手动分析。本文考虑通过定性方法对LLMs生成的"硅参与者"进行研究，从而产生可能可以推广到真实人群的洞察力。我们分析的关键概念是算法保真度，这是由Argyle等人（2023年）引入的一个术语，用于描述LLM生成的输出与人类亚群体的信念和态度的程度相吻合。根据定义，高算法保真度表明从LLMs中提取的潜在信念可能可以推广到真实人类，而低算法保真度则使得这样的研究无效。本文使用LLM生成面试问答，...

    Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
    
[^387]: 优化的时间金字塔压缩和放大变换器用于3D人体姿势估计

    Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation. (arXiv:2309.01365v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.01365](http://arxiv.org/abs/2309.01365)

    通过优化的时间金字塔压缩和放大变换器，该论文提出了一种在视频序列中准确估计人体3D姿势的方法。该方法通过扩展时间建模和细化特征交互来解决其他方法中的细节和稳定性问题，展示了较好的效果。

    

    在视频序列中准确估计人体的3D姿势需要准确性和良好的结构化架构。基于transformer的成功，我们引入了优化的时间金字塔压缩和放大（RTPCA）变换器。RTPCA通过其时间金字塔压缩和放大（TPCA）结构扩展了块内时间建模，并通过交叉层细化（XLR）模块细化块间特征交互。特别地，TPCA块利用时间金字塔范例，增强关键和值表示能力，并从运动序列中无缝提取空间语义。我们将这些TPCA块与XLR连接起来，通过连续的查询、关键字和值的相互作用促进丰富的语义表示。这种策略通过当前流程体现了早期信息，解决了其他基于Transformer方法中常见的细节和稳定性不足。我们展示了改进后的模型在3D人体姿势估计上的效果。

    Accurately estimating the 3D pose of humans in video sequences requires both accuracy and a well-structured architecture. With the success of transformers, we introduce the Refined Temporal Pyramidal Compression-and-Amplification (RTPCA) transformer. Exploiting the temporal dimension, RTPCA extends intra-block temporal modeling via its Temporal Pyramidal Compression-and-Amplification (TPCA) structure and refines inter-block feature interaction with a Cross-Layer Refinement (XLR) module. In particular, TPCA block exploits a temporal pyramid paradigm, reinforcing key and value representation capabilities and seamlessly extracting spatial semantics from motion sequences. We stitch these TPCA blocks with XLR that promotes rich semantic representation through continuous interaction of queries, keys, and values. This strategy embodies early-stage information with current flows, addressing typical deficits in detail and stability seen in other transformer-based methods. We demonstrate the eff
    
[^388]: 有竞争选择的因果战略学习

    Causal Strategic Learning with Competitive Selection. (arXiv:2308.16262v1 [cs.AI])

    [http://arxiv.org/abs/2308.16262](http://arxiv.org/abs/2308.16262)

    我们研究了具有竞争选择的因果战略学习中的代理选择问题，并提出了最佳选择规则的数学形式和实现机制。

    

    我们研究了多个决策者下的因果战略学习中的代理选择问题，并解决了其中的两个关键挑战。首先，我们考虑了由代理人评估和选择组成的选择过程的影响，而不是之前研究中关注的固定代理人池。当每个决策者通过最大化自身效用来单方面选择代理人时，我们证明了最佳的选择规则是在选择最佳代理人和提供激励以最大化代理人改进之间进行权衡的结果。此外，这个最佳选择规则依赖于代理人结果的错误预测。因此，我们研究了决策者的最佳选择规则不会导致代理人结果恶化，也不会造成不公正的降低代理人选择机会的条件。为此，我们提供了最佳选择规则的数学形式和一种实现机制。

    We study the problem of agent selection in causal strategic learning under multiple decision makers and address two key challenges that come with it. Firstly, while much of prior work focuses on studying a fixed pool of agents that remains static regardless of their evaluations, we consider the impact of selection procedure by which agents are not only evaluated, but also selected. When each decision maker unilaterally selects agents by maximising their own utility, we show that the optimal selection rule is a trade-off between selecting the best agents and providing incentives to maximise the agents' improvement. Furthermore, this optimal selection rule relies on incorrect predictions of agents' outcomes. Hence, we study the conditions under which a decision maker's optimal selection rule will not lead to deterioration of agents' outcome nor cause unjust reduction in agents' selection chance. To that end, we provide an analytical form of the optimal selection rule and a mechanism to r
    
[^389]: VIGC: 视觉指令生成与纠正

    VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])

    [http://arxiv.org/abs/2308.12714](http://arxiv.org/abs/2308.12714)

    本文提出了VIGC框架，使多模态大型语言模型能够生成和纠正视觉指令数据，解决了缺乏高质量调整数据的挑战。

    

    视觉编码器和大型语言模型（LLMs）的整合推动了多模态大型语言模型（MLLMs）的最新进展。然而，针对视觉语言任务的高质量指令调整数据的稀缺仍然是一个挑战。当前的主导范式，如LLaVA，依赖于仅使用语言的GPT-4生成数据，这需要预注释的图像标题和检测包围框，导致对图像细节的理解不足。解决这个问题的一个实际方案是利用可用的多模态大型语言模型（MLLMs）生成视觉语言任务的指令数据。然而，值得注意的是，当前可访问的MLLMs不像它们的LLM对应物那样强大，因为它们往往产生不适当的回应和生成错误信息。作为解决当前问题的方案，本文提出了Visual Instruction Generation and Correction（VIGC）框架，使多模态大型语言模型能够生成视觉指令数据并纠正错误。

    The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to ge
    
[^390]: 使用层次结构距离捕捉多层次图结构的变压器

    Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])

    [http://arxiv.org/abs/2308.11129](http://arxiv.org/abs/2308.11129)

    本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。

    

    图变压器需要强大的归纳偏差来得出有意义的注意力分数。然而，当前的提议很少涉及捕捉更长距离、层次结构或社区结构的方法，而这些在分子、社交网络和引用网络等各种图形中都会出现。在本文中，我们提出了一种层次距离结构编码（HDSE）方法，用于建模图中节点之间的层次距离，重点关注其多层次、层次化的性质。特别是，这产生了一个可以灵活与现有图变压器集成的框架，可以与其他位置表示同时应用。通过在12个真实世界数据集上进行大量实验，我们证明了我们的HDSE方法成功提升了各种类型的基线变压器，在10个基准数据集上获得了最先进的实证性能。

    Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
    
[^391]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^392]: 贝叶斯流网络

    Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.07037](http://arxiv.org/abs/2308.07037)

    本文介绍了贝叶斯流网络（BFNs），一种新的生成模型，它通过贝叶斯推断修改了一组独立分布的参数，并将其作为输入传递给神经网络来生成另一个相互依赖的分布。该方法不需要前向过程，适用于连续和离散数据，并具有优化数据压缩的功能。

    

    本文介绍了贝叶斯流网络（BFNs），一种新的生成模型。在BFNs中，独立分布的参数会在嘈杂的数据样本的影响下通过贝叶斯推断进行修改，然后作为输入传递给神经网络，该神经网络输出一个相互依赖的分布。从简单的先验开始，通过迭代更新这两个分布可以得到一个类似于扩散模型反向过程的生成过程；不过，这个过程在概念上更简单，无需前向过程。对于连续、离散化和离散数据，推导出了离散和连续时间的损失函数，以及样本生成过程。值得注意的是，对于离散数据，网络的输入位于概率单纯形上，因此本质上是可微分的，为基于梯度的样本引导和在语言建模等离散领域进行少量步骤生成铺平了道路。损失函数直接优化了数据压缩，并且不放置限制。

    This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
    
[^393]: 将顺序带入基于Transformer的语言模型中，用于人工智能和法律的应用

    Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])

    [http://arxiv.org/abs/2308.05502](http://arxiv.org/abs/2308.05502)

    本文提供了第一个对基于Transformer的语言模型在法律领域的人工智能问题和任务中的方法的系统概述。文章旨在突出这一领域的研究进展，以进一步了解Transformer在支持法律流程中的AI成功贡献以及当前的局限性。

    

    基于Transformer的语言模型（TLM）被广泛认可是一种先进的技术，能够成功开发出基于深度学习的解决方案，用于需要自然语言处理和理解的问题和应用。与其他文本领域一样，TLM确实推动了法律领域许多感兴趣任务对人工智能方法的最新进展。尽管第一个Transformer模型提出了大约6年时间，但这项技术以前所未有的速度迅猛发展，BERT和相关模型成为主要参考，也在法律领域占有重要地位。本文首次系统概述了TLM在法律领域的人工智能驱动问题和任务中的方法。一个主要目标是突出研究在这一领域的进展，以便一方面了解Transformer在支持法律流程中取得的AI成功贡献是什么，另一方面了解当前的局限性是什么。

    Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
    
[^394]: 实时作曲辅助的混合检索增强生成

    Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])

    [http://arxiv.org/abs/2308.04215](http://arxiv.org/abs/2308.04215)

    提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。

    

    检索增强模型在提升传统语言模型的上下文理解、整合私人数据和减少幻觉方面显示出了潜力。然而，应用于需要实时响应的任务（如作曲辅助）时，检索增强的大型语言模型所需的处理时间存在挑战。为了克服这一限制，我们提出了Hybrid Retrieval-Augmented Generation (HybridRAG)框架，利用了将客户端模型和云模型结合起来的混合设置。HybridRAG通过异步生成的检索增强内存，将大型语言模型（LLM）在云端生成的检索增强内存整合到客户端模型中。通过整合这种检索增强内存，客户端模型能够生成高效的响应，从LLM的能力中受益。此外，通过异步内存集成，客户端模型能够实时响应用户请求，无需等待云端处理。

    Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
    
[^395]: 深度学习中的校准：最新研究综述

    Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])

    [http://arxiv.org/abs/2308.01222](http://arxiv.org/abs/2308.01222)

    本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。

    

    在构建可靠、鲁棒的安全关键应用的人工智能系统中，深度神经模型的校准起着重要作用。最近的研究表明，具有高预测能力的现代神经网络的校准性较差，产生不可靠的模型预测。尽管深度学习模型在各种基准测试中取得了显著的性能，但对模型的校准性和可靠性的研究相对较少。理想的深度模型不仅应具有高预测性能，还应具有良好的校准性。最近提出了一些使用不同机制进行深度模型校准的方法。在本综述中，我们回顾了最新的校准方法，并解释了它们执行模型校准的原理。首先，我们从模型校准的定义开始，解释了模型校准不准确的根本原因。然后，我们介绍了可以衡量模型校准性的关键指标。接下来，我们总结了一些校准方法的方法和实践。

    Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
    
[^396]: DialogStudio：面向会话 AI 的最丰富和最多样化的统一数据集集合

    DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])

    [http://arxiv.org/abs/2307.10172](http://arxiv.org/abs/2307.10172)

    DialogStudio是迄今为止最大且最多样化的对话数据集合，包含从开放领域对话到任务导向对话、自然语言理解、会话推荐、对话摘要和知识驱动对话的数据。它为对话研究和模型训练提供了丰富而多样化的资源。

    

    尽管会话 AI 取得了进展，但语言模型在处理多样化的对话任务时面临挑战，现有的对话数据集往往缺乏多样性和全面性。为解决这些问题，我们介绍了 DialogStudio：最大、最多样化的对话数据集集合，以一致的格式统一，同时保留其原始信息。我们的集合包括来自开放领域对话、任务导向对话、自然语言理解、会话推荐、对话摘要和知识驱动对话的数据，为对话研究和模型训练提供了非常丰富和多样化的资源。为了进一步增强 DialogStudio 的实用性，我们为每个数据集确定了许可证，并为选定对话设计了领域感知提示，以便促进指导感知微调。此外，我们使用数据集集合开发了会话 AI 模型，并在零摘要生成和分布式文字基准对话任务上进行了实验。

    Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
    
[^397]: HAT-CL: 用于连续学习的基于任务的硬注意力PyTorch库

    HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])

    [http://arxiv.org/abs/2307.09653](http://arxiv.org/abs/2307.09653)

    HAT-CL是一个基于任务的硬注意力PyTorch库，以提供对连续学习中的灾难性遗忘现象的解决方案。它通过改善HAT的可用性和兼容性问题，并提供对现有网络复用的支持，实现了对PyTorch模块的自动化梯度操作和转换。此外，HAT-CL还引入了新颖的掩码操作技术。

    

    连续学习中的灾难性遗忘现象，即神经网络在学习新任务时丧失先前获得的知识，给人们带来了重大挑战。硬注意力任务(HAT)机制在减轻这个问题方面已经显示出潜力，但其实际实现受到了可用性和兼容性问题的影响，并且缺乏对现有网络复用的支持。在本文中，我们介绍了HAT-CL，这是HAT机制的用户友好、与PyTorch兼容的重新设计。HAT-CL不仅自动化了梯度操作，还简化了PyTorch模块转化为HAT模块的过程。它通过提供一套全面的模块，可以无缝地集成到现有的架构中。此外，HAT-CL还提供了与TIMM库平滑集成的可用的HAT网络。除了对HAT的重新设计和重新实现之外，我们还介绍了用于HAT的新颖的掩码操作技术。

    Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,
    
[^398]: 在大规模异构信息网络上通过渐进抽样进行长程元路径搜索

    Long-range Meta-path Search through Progressive Sampling on Large-scale Heterogeneous Information Networks. (arXiv:2307.08430v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08430](http://arxiv.org/abs/2307.08430)

    本论文研究了在大规模异构信息网络中利用长程依赖的重要性，并提出了一种名为长程元路径搜索和渐进抽样（LMSPS）的自动框架。通过动态缩小搜索空间和专门的元路径选择，实验证明了LMSPS的有效性。

    

    长程依赖的利用在同质图中有广泛研究，但在大规模异构信息网络中很少研究，这主要是由于高成本和有效信息利用的困难。为此，我们研究了不同元路径的重要性，并提出了一种在异构信息网络中利用长程依赖的自动框架，称为长程元路径搜索和渐进抽样（LMSPS）。具体来说，为了在没有先验知识的情况下发现各种数据集或任务的元路径，我们开发了一个包含所有目标节点相关元路径的搜索空间。通过渐进抽样算法，我们动态地缩小搜索空间，以跳数无关的时间复杂度驱动，从而得到一个由当前异构信息网络和任务驱动的紧凑搜索空间。利用抽样评估策略作为指导，我们进行了专门和具有表达力的元路径选择。对八个异构数据集进行的大量实验证明了LMSPS的有效性。

    Utilizing long-range dependency, though extensively studied in homogeneous graphs, is rarely studied in large-scale heterogeneous information networks (HINs), whose main challenge is the high costs and the difficulty in utilizing effective information. To this end, we investigate the importance of different meta-paths and propose an automatic framework for utilizing long-range dependency in HINs, called Long-range Meta-path Search through Progressive Sampling (LMSPS). Specifically, to discover meta-paths for various datasets or tasks without prior, we develop a search space with all target-node-related meta-paths. With a progressive sampling algorithm, we dynamically shrink the search space with hop-independent time complexity, leading to a compact search space driven by the current HIN and task. Utilizing a sampling evaluation strategy as the guidance, we conduct a specialized and expressive meta-path selection. Extensive experiments on eight heterogeneous datasets demonstrate that LM
    
[^399]: DRAGON: 一种基于对话的带有视觉语言关联的辅助导航机器人

    DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])

    [http://arxiv.org/abs/2307.06924](http://arxiv.org/abs/2307.06924)

    DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。

    

    视力受损者在理解和导航周围空间方面存在困难。目前的导航技术要么只关注导航，要么提供有限的关于环境的沟通。受到最近在视觉语言关联和语义导航方面的进展的启发，我们提出了DRAGON，一种由对话系统驱动的导航机器人，并具有将环境与自然语言关联的能力。通过理解用户的指令，DRAGON能够引导用户到地图上的目标地标，描述环境，并通过视觉观察回答问题。通过有效利用对话，机器人可以将用户的自由形式描述与环境中的地标关联起来，并通过口语提供语义信息给用户。我们在日常室内环境中进行了盲目参与者的用户研究。我们的结果表明，DRAGON能够与用户顺畅地沟通，

    Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
    
[^400]: 生成用于语言模型事实性评估的基准数据集

    Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])

    [http://arxiv.org/abs/2307.06908](http://arxiv.org/abs/2307.06908)

    该论文提出了一个名为FACTOR的方法，用于生成用于语言模型事实性评估的基准数据集。通过自动转换事实语料库，评估语言模型根据语料库生成真实事实的倾向与生成不正确陈述的能力。实验结果表明，该基准数据集的分数随模型大小增加而增加，在LM与检索方法结合时性能得到改善。困惑度和基准数据集分数之间存在相关性，但不总是一致。

    

    在将语言模型（LM）部署到特定领域之前，衡量其在该领域中生成事实错误信息的倾向很重要。现有的事实生成评估方法集中于从LM自身中采样的事实，因此无法控制评估事实的集合，并且可能低估了罕见和不太可能的事实。我们提出了FACTOR：通过语料库变换进行事实评估的方法，这是一种可扩展的方法来评估LM的事实性。FACTOR会自动将感兴趣的事实语料库转化为一个基准数据集，评估LM根据语料库生成真实事实的倾向与生成类似但不正确的陈述的能力。我们使用我们的框架创建了两个基准数据集：Wiki-FACTOR和News-FACTOR。我们的实验结果表明：（i）我们的基准数据集分数随模型大小增加而增加，并且当LM与检索方法结合使用时，性能得到改善；（ii）基准数据集分数与困惑度之间存在相关性，但这两个指标在模型排序上并不总是一致；以及（iii）当困惑度和基准数据集分数发生冲突时，基准数据集分数更能准确反映LM的事实性能。

    Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score 
    
[^401]: 通过重新加权优化轨迹增强对抗训练

    Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14275](http://arxiv.org/abs/2306.14275)

    本文提出了一种名为“加权优化轨迹（WOT）”的新方法，通过优化历史轨迹，解决了对抗训练中的鲁棒泛化问题。

    

    尽管对抗训练已成为提高深度神经网络鲁棒性的事实上的方法，但众所周知，简单的对抗训练遭受了令人畏缩的鲁棒过拟合问题，导致鲁棒泛化效果不佳。近年来已经提出了一些方法来解决这些缺点，如额外的规范化、对抗权重扰动和更多数据训练。然而，鲁棒泛化的改进仍然远不理想。在本文中，我们从全新的角度解决这一挑战--优化历史轨迹的精细化。我们提出了一种名为“加权优化轨迹（WOT）”的新方法，利用对抗训练的优化轨迹在时间上的特点。我们进行了大量实验证明了WOT在各种最新对抗攻击下的有效性。结果显示，WOT与现有方法完美融合。

    Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
    
[^402]: 系统级自然语言反馈

    System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])

    [http://arxiv.org/abs/2306.13588](http://arxiv.org/abs/2306.13588)

    本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了通过任务度量设计和语言模型提示设计，如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型，并展示了使用系统级别反馈和实例级别反馈的有效性。

    

    自然语言反馈包含了丰富的用户体验信息。现有研究聚焦于实例级别的方法，即将反馈用于细化特定例子，而忽略了其系统范围的应用。本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型。具体而言，这是通过以下两方面实现的：(i) 任务度量设计; (ii) 用于改进模型响应的语言模型提示设计。我们进行了两项案例研究，来改进搜索查询生成和对话响应生成，展示了使用系统级别反馈的有效性。我们表明系统级别反馈和实例级别反馈的组合带来了进一步的收益，并且由人类撰写的实例级别反馈导致比GPT-3.5撰写的反馈更加扎实。

    Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
    
[^403]: TrustGuard: 基于GNN的动态支持鲁棒且可解释的信任评估

    TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])

    [http://arxiv.org/abs/2306.13339](http://arxiv.org/abs/2306.13339)

    TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。

    

    信任评估评估实体之间的信任关系并促进决策。机器学习由于其学习能力而表现出巨大的潜力，因此对信任评估具有重要意义。近年来，作为一种新的机器学习范 paradigm，图神经网络（GNN）在处理图形数据方面表现出优越性。这激发了研究人员探索将其用于信任评估，因为实体之间的信任关系可以建模为图形。但是，使用GNN的当前信任评估方法未能完全满足信任的动态性，忽略了攻击对信任评估的不利影响，并且无法提供令人信服的评估结果解释。为解决这些问题，在本文中，我们提出了TrustGuard ：一种支持信任动态性、抗击鲁棒且通过可视化提供解释的精确信任评估模型。具体而言，TrustGuard 设计了一个由动态感知节点嵌入层、图卷积层、注意机制层和信任预测层组成的分层架构。为了评估提出的模型的有效性，我们对真实数据集进行了实验，并将TrustGuard与其他最先进的方法进行了比较。实验结果表明，TrustGuard 在准确性、鲁棒性和可解释性方面均优于其他方法。

    Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
    
[^404]: 膨胀的披露：ChatGPT是否能帮助投资者处理财务信息？

    Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?. (arXiv:2306.10224v1 [econ.GN])

    [http://arxiv.org/abs/2306.10224](http://arxiv.org/abs/2306.10224)

    研究发现生成式 AI 工具 ChatGPT 可以更有效地展示股票市场相关信息，提出了信息膨胀指标并证明其与负面的资本市场后果相关，同时展示其在构建针对性总结方面的效果。

    

    生成式 AI 工具（如 ChatGPT）可以从根本上改变投资者处理信息的方式。我们使用股票市场作为实验室，探究这些工具在总结复杂的公司披露信息时的经济效用。总结摘要明显更短，通常比原始文本缩短超过 70%，而信息内容得到增强。当一份文件具有积极（消极）情感时，其总结变得更积极（消极）。更重要的是，总结对解释股市对披露信息的反应更有效。基于这些发现，我们提出了信息“膨胀”指标。我们显示，膨胀的披露与负面的资本市场后果相关，例如更低的价格有效性和更高的信息不对称性。最后，我们展示了这个模型在构建针对性总结方面的有效性，以确定公司的（非）财务表现和风险。总之，我们的研究结果表明，像 ChatGPT 这样的生成式 AI 工具可以有效地帮助投资者更高效地处理财务信息。

    Generative AI tools such as ChatGPT can fundamentally change the way investors process information. We probe the economic usefulness of these tools in summarizing complex corporate disclosures using the stock market as a laboratory. The unconstrained summaries are dramatically shorter, often by more than 70% compared to the originals, whereas their information content is amplified. When a document has a positive (negative) sentiment, its summary becomes more positive (negative). More importantly, the summaries are more effective at explaining stock market reactions to the disclosed information. Motivated by these findings, we propose a measure of information "bloat." We show that bloated disclosure is associated with adverse capital markets consequences, such as lower price efficiency and higher information asymmetry. Finally, we show that the model is effective at constructing targeted summaries that identify firms' (non-)financial performance and risks. Collectively, our results indi
    
[^405]: 使用分割学习预测软件性能

    Predicting Software Performance with Divide-and-Learn. (arXiv:2306.06651v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2306.06651](http://arxiv.org/abs/2306.06651)

    本文提出了一种名为$DaL$的基于分割学习的方法，用于预测高度配置的软件系统的性能。实验证明了该方法的有效性。

    

    预测高度配置的软件系统的性能是性能测试和质量保证的基础。为此，最近的研究依靠机器/深度学习来建模软件性能。然而，一个至关重要但未解决的挑战是如何满足配置景观中继承的稀疏性：配置选项（特征）的影响和数据样本的分布都非常稀疏。本文提出了一种基于“分割学习”概念的方法，称为$DaL$。基本思想是，为了处理样本稀疏性，我们将配置景观中的样本划分为远离的部分，对于每个部分，我们建立一个规范化的深度神经网络作为本地模型来处理特征稀疏性。然后，新给定的配置将被分配给最终预测的正确模型。八个真实系统和五组训练数据的实验结果显示

    Predicting the performance of highly configurable software systems is the foundation for performance testing and quality assurance. To that end, recent work has been relying on machine/deep learning to model software performance. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse.  In this paper, we propose an approach based on the concept of 'divide-and-learn', dubbed $DaL$. The basic idea is that, to handle sample sparsity, we divide the samples from the configuration landscape into distant divisions, for each of which we build a regularized Deep Neural Network as the local model to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction.  Experiment results from eight real-world systems and five sets of training data reveal that
    
[^406]: STEVE-1: 一个用于Minecraft中文本-行为生成的生成模型

    STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. (arXiv:2306.00937v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.00937](http://arxiv.org/abs/2306.00937)

    STEVE-1 是一种新的生成模型，能够在Minecraft中跟随各种短期开放型文本和视觉指令。STEVE-1利用预先训练的模型和最佳实践，通过自监督的行为克隆和回顾重新标记来微调，避免了昂贵的人工注释。

    

    建立对文本指令做出响应的AI模型对于连续性决策任务来说是具有挑战性的。本文介绍了一种名为STEVE-1的Minecraft指令调整型视频预训练模型，展示了DALL-E 2中使用的unCLIP方法也对创建指令跟随连续决策代理非常有效。STEVE-1分为两个步骤进行训练：首先是将预先训练的VPT模型适应MineCLIP的潜在空间中的指令，然后训练一个先验模型以从文本预测潜在代码。这使我们能够通过自监督的行为克隆和回顾重新标记来微调VPT，避免需要昂贵的人工文本注释。通过利用VPT和MineCLIP等预先训练的模型，并采用文本条件的图像生成的最佳实践，STEVE-1的训练成本仅为60美元，并且可以在Minecraft中遵循各种短期开放型文本和视觉指令。STEVE-1为开放的指令跟随连续决策代理设定了一个新的标准。

    Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instructi
    
[^407]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^408]: 一个降维人类分类的理性模型

    A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])

    [http://arxiv.org/abs/2305.14383](http://arxiv.org/abs/2305.14383)

    提出了一个基于层次化混合模型的分类模型，以及基于生成过程的低维潜在空间的分类解释，该模型学习类别表示和特征集合，适用于高维刺激下，支持零-shot学习，并验证了该模型。

    

    认知科学中现有的模型通常假设人类在心理空间中进行分级概括行为，但是在自然环境中，这些模型中的类别表示可能会受到维度诅咒的影响。人们一般依赖于一组可行但足够的特征来理解复杂的环境。本文提出了一个基于层次化概率主成分混合模型的分类模型，同时学习类别表示和经济的特征集合。该模型捕捉了人类分类中的维度偏差并支持零-shot学习。我们进一步在低维潜在空间内利用生成过程，提供高维刺激下更好的分类解释。我们通过模拟和行为实验验证了模型。

    Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
    
[^409]: GPT究竟有多老？HumBEL框架通过人群数据评估语言模型

    How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14195](http://arxiv.org/abs/2305.14195)

    本论文提出了一种使用人类人口数据对语言模型进行评估的框架，并发现GPT-3.5在不同任务中表现不同，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。

    

    近年来，大型预训练语言模型在自然语言处理中得到广泛应用，然而目前的评估方法并未考虑模型的语言使用与特定人群之间的一致性，尤其是在对话式人工智能应用中这一点非常重要。为了填补这一空白，我们探讨了如何测量和比较语言模型的语言能力与人类子群之间的差异。我们借助语言病理学的临床技术，该学科已经建立了不同（人类）年龄阶段的语言能力发展规范，对技能进行评估，我们与领域专家（即持有临床许可证的语言病理学家）进行了评估，并提出了自动化的评估技术以实现规模化评估。我们发现，GPT-3.5的能力因任务而异，在单词意义推断方面模拟了典型6-9岁儿童的能力，在记忆方面则表现优于典型21岁年轻人。GPT-3.5（InstructGPT）在社交交互任务中也存在一定的困难。

    While large pre-trained language models (LMs) find greater use across NLP, existing evaluation protocols do not consider how LM language use aligns with particular human demographic groups, which can be an important consideration in conversational AI applications. To remedy this gap, we consider how LM language skills can be measured and compared to human sub-populations. We suggest clinical techniques from Speech Language Pathology, which has well-established norms for acquisition of language skills, organized by (human) age. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to substitute clinical evaluation at scale. We find LM capability varies widely depending on task with GPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring inference about word meanings and simultaneously outperforming a typical 21 year old at memorization. GPT-3.5 (InstructGPT) also has trouble with soc
    
[^410]: 通过重新标记最小训练子集来翻转预测

    Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])

    [http://arxiv.org/abs/2305.12809](http://arxiv.org/abs/2305.12809)

    本文利用扩展影响函数提出了一种有效的识别和重新标记最小训练子集的方法，并证明其始终能够成功翻转测试结果，同时还提供了挑战模型预测、评估模型鲁棒性和洞察训练集偏差等多重作用。

    

    Yang等人发现，仅删除1%的训练数据就可能导致预测结果翻转。鉴于机器学习模型中存在噪声数据的普遍性，本文提出了一个问题：在模型训练之前通过重新标记一个小的训练数据子集可否导致测试结果翻转？本文利用扩展影响函数提出了一种有效的识别和重新标记这种子集的方法，并证明了其始终能够产生成功的结果。这种机制有多重作用：（1）提供了一种补充方法，可以通过恢复可能错误标记的训练数据来挑战模型预测；（2）评估模型的鲁棒性，因为本文发现子集的大小与训练集中噪声数据的比例之间存在显著关系；（3）提供了洞察训练集偏差的见解。据我们所知，这项工作代表了对识别最小训练子集问题的第一次研究。

    Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
    
[^411]: SongDriver2：基于情绪的实时音乐编排与柔和过渡

    SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition. (arXiv:2305.08029v1 [cs.SD])

    [http://arxiv.org/abs/2305.08029](http://arxiv.org/abs/2305.08029)

    SongDriver2实现了基于情绪的实时音乐编排，并提出了柔和过渡机制，使音乐具有高度真实性和平滑过渡。

    

    基于情绪的实时音乐编排旨在将给定的音乐转化为另一个能够实时引起用户特定情感共鸣的音乐，在音乐疗法、游戏配乐和电影配乐等各种场景中具有重要应用价值。然而，由于目标情感的细粒度和可变性，平衡情感实时匹配和柔和情感转换是一项挑战。现有的研究主要集中在实现情感实时匹配，而柔和过渡的问题仍未得到充分研究，影响了音乐的整体情感一致性。本文提出了SongDriver2来解决这个问题。具体地，我们首先识别最后一个时间步的音乐情绪，然后将其与当前时间步的目标输入情绪融合。融合的情感随后作为SongDriver2根据输入旋律数据生成即将到来的音乐的指导。为了调整音乐相似性和情感实时匹配，以实现两种不同情感之间的过渡，我们设计了一种软过渡机制，将插值和平滑滤波器相结合。我们证明，所提出的SongDriver2可以生成具有高度真实性和平滑过渡的情感音乐，这表明其在基于情绪的实时音乐编排应用中具有潜在价值。

    Real-time emotion-based music arrangement, which aims to transform a given music piece into another one that evokes specific emotional resonance with the user in real-time, holds significant application value in various scenarios, e.g., music therapy, video game soundtracks, and movie scores. However, balancing emotion real-time fit with soft emotion transition is a challenge due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of soft transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose SongDriver2 to address this balance. Specifically, we first recognize the last timestep's music emotion and then fuse it with the current timestep's target input emotion. The fused emotion then serves as the guidance for SongDriver2 to generate the upcoming music based on the input melody data. To adjust music similarity and emotion real-time fit f
    
[^412]: 基于抽象语法树的异构有向超图神经网络用于代码分类

    Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.04228](http://arxiv.org/abs/2305.04228)

    本研究提出了使用异构有向超图表示AST，并使用异构有向超图神经网络处理图形进行代码分类，超过了现有方法。

    

    代码分类是程序理解和自动编码中的一个难题。由于程序的模糊语法和复杂语义，大多数现有研究使用基于抽象语法树（AST）和图神经网络（GNN）的技术创建代码表示用于代码分类。这些技术利用代码的结构和语义信息，但只考虑节点之间的成对关系，忽略了AST中节点之间已经存在的高阶相关性，可能导致代码结构信息的丢失。本研究提出使用异构有向超图（HDHG）表示AST，并使用异构有向超图神经网络（HDHGN）处理图形。HDHG保留了节点之间的高阶相关性，并更全面地编码了AST的语义和结构信息。HDHGN通过聚合不同节点的特征并使用不同的函数对其进行处理来对AST进行建模。在四个数据集上的实验表明，HDHG和HDHGN在代码分类任务中超越了现有方法。

    Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
    
[^413]: 基于半监督学习的高维贝叶斯优化及优化无标签数据采样

    High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])

    [http://arxiv.org/abs/2305.02614](http://arxiv.org/abs/2305.02614)

    本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。

    

    贝叶斯优化（BO）是一种寻找黑箱函数全局最优解的强大工具。虽然黑箱函数的评估成本往往很高，但减少昂贵标记数据的使用是理想的。本文首次提出了一种教师-学生模型，利用半监督学习在BO环境下利用大量未标记的数据。其中，关键在于选择验证和未标记数据以提高BO的表现。为了优化无标签数据的采样，我们采用黑箱参数化采样分布，将其优化为所采用双层优化框架的一部分。更进一步，通过从动态适应的极值分布中选择未标签数据，我们证明了BO的性能可以进一步提高。我们的BO方法在学习后的低维潜在空间中运行，使其可扩展到高维问题。

    Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
    
[^414]: 面向遥感时序数据的轻量级预训练Transformer

    Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])

    [http://arxiv.org/abs/2304.14065](http://arxiv.org/abs/2304.14065)

    设计针对远程传感器数据的自监督学习模型和训练技术，可以得到表现更好且更小的模型。预训练的遥感时间序列Transformer（Presto）在几个遥感基准测试中实现了最先进的结果。

    

    远程传感数据的机器学习算法在社会相关应用方面具有广泛的应用，但用于训练这些算法的标签可能很难或不可能获得。这个挑战已经推动了自监督学习领域的研究，旨在通过遥感数据解锁在标记数据集较小的地理位置或应用领域中使用机器学习。我们展示了为遥感数据设计模型和自监督训练技术可以得到更小、更优秀的模型。我们介绍了Remote Sensing Transformer（Presto），它是一种基于Transformer的模型，使用新颖的自监督目标对遥感时间序列数据进行预训练。我们的实验表明，与在自然图像上训练的可比模型相比，Presto在几个遥感基准测试中实现了最先进的结果，同时需要数量级更少的参数。

    Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr
    
[^415]: 大型语言模型对齐的基本限制

    Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])

    [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082)

    本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。

    

    开发与人交互的语言模型的重要方面是对齐其行为，使其对其人类用户有用且无害。这通常通过调整模型的方式来实现，以增强所需的行为并抑制不希望的行为。在本文中，我们提出了一种名为行为期望边界(BEB)的理论方法，它允许我们正式研究大型语言模型中的几个内在特征和对齐的限制。重要的是，我们证明对于任何具有被该模型表现出的有限概率的行为，都存在可以触发模型输出此行为的提示，其概率随提示的长度增加而增加。这意味着任何减弱不希望的行为但未将其完全消除的对齐过程都无法抵御针对性攻击。此外，我们的框架提示了领先的

    An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
    
[^416]: 在资源受限的环境中运行关键的机器学习模型

    Operating critical machine learning models in resource constrained regimes. (arXiv:2303.10181v1 [cs.LG])

    [http://arxiv.org/abs/2303.10181](http://arxiv.org/abs/2303.10181)

    本文分享了在关键场景下使用机器学习模型时资源消耗和性能之间的权衡方法。考虑到模型在全球诊所中的部署，机器学习界正在为改进模型效率而努力。

    

    机器学习方法，特别是深度学习，在医学图像分析和计算机辅助干预方面的最新突破，使其得到快速发展。深度学习模型在训练数据，计算和能源成本方面的资源消耗是巨大的。这些巨大的资源成本可能会阻碍这些模型在全球诊所中的部署。为了解决这个问题，机器学习界正在努力引入资源效率的概念。例如，使用量化来减轻内存消耗。虽然大多数这些方法已被证明可以减少资源利用，但可能会以性能为代价。在这项工作中，我们探讨了资源消耗和性能之间的权衡，特别是在诊所等关键环境中使用的模型方面。

    The accelerated development of machine learning methods, primarily deep learning, are causal to the recent breakthroughs in medical image analysis and computer aided intervention. The resource consumption of deep learning models in terms of amount of training data, compute and energy costs are known to be massive. These large resource costs can be barriers in deploying these models in clinics, globally. To address this, there are cogent efforts within the machine learning community to introduce notions of resource efficiency. For instance, using quantisation to alleviate memory consumption. While most of these methods are shown to reduce the resource utilisation, they could come at a cost in performance. In this work, we probe into the trade-off between resource consumption and performance, specifically, when dealing with models that are used in critical settings such as in clinics.
    
[^417]: FreDSNet: 利用快速傅里叶卷积进行单目深度和语义分割的联合学习

    FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions. (arXiv:2210.01595v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2210.01595](http://arxiv.org/abs/2210.01595)

    FreDSNet是一种深度学习解决方案，可以从单个全景图像中实现室内环境的语义三维理解，并且通过利用频率域中的卷积从而获得更宽广的感受野。它是第一个联合提供单目深度估计和语义分割的网络。

    

    本研究提出了FreDSNet，这是一种深度学习解决方案，可以从单个全景图像中获取室内环境的语义三维理解。全景图像由于提供了整个环境的360度背景信息，对于解决场景理解问题具有特定的优势。然而，全景图像的固有特点给对象的准确检测和分割或者良好的深度估计带来了额外的问题。为了克服这些问题，我们利用频率域中的卷积，在每个卷积层中获得更宽广的感受野。这些卷积允许从全景图像中利用整个上下文信息。FreDSNet是第一个利用快速傅里叶卷积从单个全景图像中联合提供单目深度估计和语义分割的网络。我们的实验证明，FreDSNet的性能与特定状态的方法相似。

    In this work we present FreDSNet, a deep learning solution which obtains semantic 3D understanding of indoor environments from single panoramas. Omnidirectional images reveal task-specific advantages when addressing scene understanding problems due to the 360-degree contextual information about the entire environment they provide. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequential domain obtaining a wider receptive field in each convolutional layer. These convolutions allow to leverage the whole context information from omnidirectional images. FreDSNet is the first network that jointly provides monocular depth estimation and semantic segmentation from a single panoramic image exploiting fast Fourier convolutions. Our experiments show that FreDSNet has similar performance as specific state 
    

