# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On First-Order Meta-Reinforcement Learning with Moreau Envelopes.](http://arxiv.org/abs/2305.12216) | 本文提出了一种Moreau包络元强化学习算法（MEMRL），通过利用Moreau包络代理正则化器，可以学习一个可以适应任务分布的元策略。 |
| [^2] | [ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios.](http://arxiv.org/abs/2305.12200) | 本文构建了一个用于低资源情境中的单口喜剧文本生成语音系统ComedicSpeech，该系统使用语调编码器提取语调表示并以灵活的方式将其定向到TTS模型，同时通过有条件的持续时间预测器来增强个人韵律建模，并通过引入与喜剧演员相关的特殊标记来模拟个人填充语。实验表明，ComedicSpeech在仅有十分钟训练数据情况下比基线模型表现出更好的表现力。 |
| [^3] | [Do We Need an Encoder-Decoder to Model Dynamical Systems on Networks?.](http://arxiv.org/abs/2305.12185) | 本论文揭示了使用潜在嵌入来模拟网络动力系统的不足，并提出了一种无嵌入的替代方案，通过三个正确长期行为测试的验证表明该方案的可行性。 |
| [^4] | [The Case Against Explainability.](http://arxiv.org/abs/2305.12167) | 研究发现人工智能系统的解释权利难以充分解释法律目的，涉及到公正判决、正当程序、人类机构认证和决策者权威提升。 |
| [^5] | [A Scalable Neural Network for DSIC Affine Maximizer Auction Design.](http://arxiv.org/abs/2305.12162) | 该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。 |
| [^6] | [LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4.](http://arxiv.org/abs/2305.12147) | 该论文提出了LogiCoT, 一个基于GPT-4的逻辑思维指令调整数据集，用于教授模型逻辑推理和引出一般推理技能。 |
| [^7] | [DiffCap: Exploring Continuous Diffusion on Image Captioning.](http://arxiv.org/abs/2305.12144) | DiffCap通过应用连续扩散，以自然的方式转换离散标记，并成功融合提取的图像特征进行扩散字幕生成，其具有更多的解码多样性。 |
| [^8] | [The Scope of ChatGPT in Software Engineering: A Thorough Investigation.](http://arxiv.org/abs/2305.12138) | ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。 |
| [^9] | [Privacy in Multimodal Federated Human Activity Recognition.](http://arxiv.org/abs/2305.12134) | 本文研究了多模式联邦人类活动识别中的隐私问题。通过一个特定的系统，联邦学习可以提供更好的隐私保护，同时不会损失人类活动识别的准确性。 |
| [^10] | [DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training.](http://arxiv.org/abs/2305.12127) | 本文提出了一种分散的基于人口的训练(PBT)算法，使单臂或双臂机器人在模拟环境中可以学习灵巧的物体操作技能，包括重新抓取、抓取-投掷和物体重新定位，相较于常规的端到端学习，该算法可大大提高深度强化学习的探索能力和发现稳健控制策略的能力。 |
| [^11] | [A Framework for Provably Stable and Consistent Training of Deep Feedforward Networks.](http://arxiv.org/abs/2305.12125) | 该论文提出了一种用于深度神经网络的稳定一致训练框架，其核心在于结合了标准随机梯度下降和梯度裁剪方法以及使用自动稳定的压缩激活函数，从而提高了深度神经网络的稳定性和训练效果。 |
| [^12] | [Annealing Self-Distillation Rectification Improves Adversarial Training.](http://arxiv.org/abs/2305.12118) | 本研究提出了退火自蒸馏校正(ADR)方法，其能生成软标签用作更好的指导机制，准确反映在对抗训练中攻击下的分布变化，提高模型的鲁棒性，并实现了平滑的插入性整合到其他对抗性训练技术中。 |
| [^13] | [GFDC: A Granule Fusion Density-Based Clustering with Evidential Reasoning.](http://arxiv.org/abs/2305.12114) | 提出了一种基于证据推理的颗粒聚类算法GFDC，该算法可以克服基于密度的聚类算法在衡量全局密度、确定合理的聚类中心或结构、准确地分配样本以及处理具有大密度差异的数据方面表现不佳的缺点。与此同时，GFDC 采用了三种新颖的颗粒融合策略来将颗粒组合成稳定的聚类结构，有助于检测具有任意形状的聚类。 |
| [^14] | [Human labeling errors and their impact on ConvNets for satellite image scene classification.](http://arxiv.org/abs/2305.12106) | 本文针对卫星图像场景分类中人工标注数据集的不可避免的误差问题，收集了真实标签数据，并探讨这些误差如何影响三个ConvNets的分类结果。研究发现，人类标注误差具有显着的类和实例依赖性，并且对不同CNN结构的影响不同，因此需要仔细考虑这些误差并开发更强大的ConvNets来解决这一挑战。 |
| [^15] | [UP5: Unbiased Foundation Model for Fairness-aware Recommendation.](http://arxiv.org/abs/2305.12090) | 本研究提出了一种新颖的基础模型UP5，它采用反事实公平促进技术来消除大型语言模型中的偏见，从而实现面向公平性的推荐。 |
| [^16] | [Game-Theoretical Analysis of Reviewer Rewards in Peer-Review Journal Systems: Analysis and Experimental Evaluation using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.12088) | 本文针对同行评审期刊中代金券奖励制度可能会导致评审人员二元决策的问题，提出了一种替代奖励系统，有效地促进了更全面的审查，实验结果表明该系统更加平衡且稳定。 |
| [^17] | [AnyPredict: Foundation Model for Tabular Prediction.](http://arxiv.org/abs/2305.12081) | 本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。 |
| [^18] | [GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance.](http://arxiv.org/abs/2305.12073) | 本文对GELU激活函数进行了全面的数学分析和广泛的实验比较，证明了它在深度学习模型中具有优越的性能和适用性。 |
| [^19] | [Technical outlier detection via convolutional variational autoencoder for the ADMANI breast mammogram dataset.](http://arxiv.org/abs/2305.12068) | 该论文介绍了如何使用卷积变分自编码器来检测乳腺X线图像中的技术异常点，以保证一个乳腺癌检测算法的准确性和稳定性。 |
| [^20] | [DADIN: Domain Adversarial Deep Interest Network for Cross Domain Recommender Systems.](http://arxiv.org/abs/2305.12058) | 论文提出了一种创新性的深度跨领域点击率预测模型——领域对抗深度兴趣网络（DADIN），该模型通过引入领域不可知层和特别设计的损失，创新地实现了两个领域的联合分布对齐，并采用对抗训练的方式与点击率预测损失一起进行优化，相比竞争基线算法提升明显。 |
| [^21] | [CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring.](http://arxiv.org/abs/2305.12050) | CodeCompose是一个基于InCoder LLM的AI辅助的代码编写工具，已经在Meta内部部署，可在10多种编程语言和几个编码表面上使用。 CodeCompose已成功帮助提高Meta的开发人员生产力。 |
| [^22] | [Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding.](http://arxiv.org/abs/2305.12031) | 临床骆驼是一种基于对话的知识编码的开源医学语言模型，具有很高的可解释性和临床相关性，并在多个基准数据集上取得了最先进的结果。 |
| [^23] | [Learning Continually on a Sequence of Graphs -- The Dynamical System Way.](http://arxiv.org/abs/2305.12030) | 该论文研究的是如何进行图数据的持续学习，并面临着由于数据基础特性所导致的理论与方法上的挑战。 |
| [^24] | [MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup.](http://arxiv.org/abs/2305.12029) | 本研究提出了MultiTurnCleanup任务，收集了新的数据集MultiTurnCleanup1，针对口语会话转录中的不连续现象进行探讨并提供了两个可用于未来研究的基准测试模型。 |
| [^25] | [Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings.](http://arxiv.org/abs/2305.12027) | 本文提出了 DUCK 方法，通过使用实体类型的先前知识在实体表示空间中注入结构信息。把盒嵌入概念引入到极坐标中，将关系表示为超球面上的盒子，将具有相似类型的实体放置在对应于它们关系的盒子内，实现聚类。在实体链接方面取得了最先进的结果，尤其在低资源环境下效果显著。 |
| [^26] | [Energy-efficient memcapacitive physical reservoir computing system for temporal data processing.](http://arxiv.org/abs/2305.12025) | 本文研究了一种基于能效随态电容器的物理储层计算系统，解决了时间数据的分类任务和分析时间序列数据的问题。在实验中，系统能够实现较高的准确率和较小的均方误差。 |
| [^27] | [Inventing painting styles through natural inspiration.](http://arxiv.org/abs/2305.12015) | 本文提出了两种方法，通过仅使用自然图像训练的模型创建绘画风格，而不需要艺术训练数据，这能为艺术中的生成AI合法使用铺平道路。 |
| [^28] | [Constructing Dreams using Generative AI.](http://arxiv.org/abs/2305.12013) | 本文介绍了一个“与AI一同做梦”的学习车间，让高中生了解和使用了生成式AI工具，并反思了潜在利益和伤害，提出了政策意见。 |
| [^29] | [Advising OpenMP Parallelization via a Graph-Based Approach with Transformers.](http://arxiv.org/abs/2305.11999) | 本文提出了一种名为OMPify的新方法，该方法基于Transformer模型，通过对串行代码的分析，自动检测和预测并行代码中的OpenMP编译指示符和共享内存属性。 |
| [^30] | [Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees.](http://arxiv.org/abs/2305.11997) | 本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。 |
| [^31] | [EEG and EMG dataset for the detection of errors introduced by an active orthosis device.](http://arxiv.org/abs/2305.11996) | 该论文介绍了一个包含EEG和EMG记录的数据集，这些记录来自被协助使用主动矫形器移动右臂的受试者。研究者故意引入了一些错误，以检测新的错误检测方法。 |
| [^32] | ["Sch\"one neue Lieferkettenwelt": Workers' Voice und Arbeitsstandards in Zeiten algorithmischer Vorhersage.](http://arxiv.org/abs/2305.11981) | 面对日益复杂的供应链，压力来自于消费者、批评的公众以及供应链法规等，领先的公司正在尝试通过算法预测商业、环境和社会风险来改善劳工条件，并为不同利益相关者群体提供政策选择。 |
| [^33] | [Logic-Based Benders Decomposition in Answer Set Programming for Chronic Outpatients Scheduling.](http://arxiv.org/abs/2305.11969) | 本文提出并评估了一种基于逻辑的Benders分解方法，用于解决慢性门诊预约等大规模优化问题，相比于其他方法具有更高的效率、可扩展性和最优性保证。 |
| [^34] | [Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization.](http://arxiv.org/abs/2305.11965) | 本文提出了一种具有个性化温度的对比损失用于自监督学习，根据数据分布自动调整温度以使得训练更加有效。 |
| [^35] | [PyTorch Hyperparameter Tuning -- A Tutorial for spotPython.](http://arxiv.org/abs/2305.11930) | 本文介绍了如何将spotPython超参数调谐器集成到PyTorch训练工作流中，以提高机器或深度学习模型的性能，以CIFAR10图像分类器为例。 |
| [^36] | [Energy-frugal and Interpretable AI Hardware Design using Learning Automata.](http://arxiv.org/abs/2305.11928) | 本论文通过使用学习自动机实现了节能的AI硬件设计，同时保持了模型的解释性和准确性。 |
| [^37] | [An Approach to Multiple Comparison Benchmark Evaluations that is Stable Under Manipulation of the Comparate Set.](http://arxiv.org/abs/2305.11921) | 本文提出了一种新的基准比较结果展示方法——多元比较矩阵（MCM），使得比较集合稳定无误差，可避免常用方法存在的无意和有意的操纵空间，并且其采用Python实现，已在公开提供。 |
| [^38] | [Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data.](http://arxiv.org/abs/2305.11913) | 本文提出了一种基于神经网络的方法，利用高度现实的合成训练数据对稀疏雷达数据进行相位相关的波浪表面重建。 |
| [^39] | [Long-lead forecasts of wintertime air stagnation index in southern China using oceanic memory effects.](http://arxiv.org/abs/2305.11901) | 该研究基于海洋记忆效应开发了一个LSTM模型，结合过去的ASI和尼娜指数可以实现更好的ASI预测，为提前制定空气质量管理计划提供了帮助。 |
| [^40] | [ChatGPT and the Labor Market: Unraveling the Effect of AI Discussions on Students' Earnings Expectations.](http://arxiv.org/abs/2305.11900) | 本文研究了 ChatGPT人工智能（AI）讨论对美国学生预期劳动市场结果的因果影响，结果发现学生信心会降低，对未来的收入前景保持悲观态度，这种影响广泛存在于不同学生群体中。这个研究给教育工作者、管理者和政策制定者提供了一个机会，以更好地了解学生的担忧并改进教育课程，使学生更好地准备未来，这个未来必然会被AI改变。 |
| [^41] | [Neural information coding for efficient spike-based image denoising.](http://arxiv.org/abs/2305.11898) | 本文研究了使用脉冲神经网络进行高斯图像去噪的方法，提出了使用Leaky Integrate and Fire (LIF)神经元进行信息处理的方案，相对于传统速率编码机制，在保证性能的同时起到了降低计算负载的作用。 |
| [^42] | [Critical Appraisal of Artificial Intelligence-Mediated Communication.](http://arxiv.org/abs/2305.11897) | 本文探讨了人工智能在语言教育中介交流的优缺点，并强调语言教师积极参与CALL教师教育和专业发展的重要性。 |
| [^43] | [Hyper-automation-The next peripheral for automation in IT industries.](http://arxiv.org/abs/2305.11896) | 本文简要介绍了超级自动化在IT行业的重要性。通过将AI工具与RPA相结合，超级自动化为几乎所有由业务用户执行的重复操作提供自动化。同时，本文还讨论了如何利用脑机接口和传感器来提高超级自动化的效率和自动化流程部署的质量。 |
| [^44] | [Designing for Meaningful Human Control in Military Human-Machine Teams.](http://arxiv.org/abs/2305.11892) | 本论文从军事人机团队的角度提出了有意义的人类控制（MHC）对于防御技术的重要性，并提出了宏观设计选项团队设计模式。同时，提供了一个搜索与搜救任务的案例研究。 |
| [^45] | [Exploring the Efficacy of ChatGPT in Analyzing Student Teamwork Feedback with an Existing Taxonomy.](http://arxiv.org/abs/2305.11882) | 本研究探索了使用ChatGPT分析学生团队合作反馈的有效性，结果表明ChatGPT可以实现90％以上的标签精度，为分析团队项目的反馈提供了一个可能有价值的工具。 |
| [^46] | [Challenges and Trends in User Trust Discourse in AI.](http://arxiv.org/abs/2305.11876) | 本文阐述在人工智能话语中用户信任面临的挑战和趋势，呼吁澄清概念以避免可能的信任差距和误解现象。 |
| [^47] | [Judgments of research co-created by generative AI: experimental evidence.](http://arxiv.org/abs/2305.11873) | 本研究探讨了把研究中部分内容交由生成式AI完成，会导致人们不信任和贬低研究人员和科学输出，并可能影响到生成式AI使用的报告问题。 |
| [^48] | [Scaling laws for language encoding models in fMRI.](http://arxiv.org/abs/2305.11863) | 本文揭示了基于fMRI的语言编码模型预测性能与模型大小呈对数线性关系，在125M到30B参数模型进行规模扩展时，表现提高了约15％。 |
| [^49] | [Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews.](http://arxiv.org/abs/2305.11828) | 本文研究了使用LLM协助制作医学证据综述的潜在用途和风险，指出LLM有可能自动生成文献综述，但由于可能出现虚构或遗漏信息的情况，LLM的使用需要谨慎。 |
| [^50] | [Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate.](http://arxiv.org/abs/2305.11595) | 本文提出了通过辩论探究大型语言模型之间的内部一致性问题，实验证明通过严格的辩论框架可以提高模型性能和常识知识的结构化学习。 |
| [^51] | [RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration.](http://arxiv.org/abs/2305.11474) | 本文提出了轻量级图像恢复的互惠式注意力混合Transformer（RAMiT）。通过使用双向注意力以及一种新的数据增强类型——强度掩码，有效地提高了恢复效果，同时大大减少了参数数量。 |
| [^52] | [SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation.](http://arxiv.org/abs/2305.11130) | 本文提出了一种简单而有效的两阶段SimOAP策略，通过过采样和后评估来提高角色扮演对话生成中的连贯性和一致性，并在三个公共角色扮演对话基准上取得了最先进的结果。 |
| [^53] | [Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions.](http://arxiv.org/abs/2305.10435) | 生成的预训练变形器是一种基于变形器架构的深度神经网络，能够在自然语言处理任务中表现出色且有效地进行对话，具有广泛的潜在应用，但仍面临新兴挑战和局限性。 |
| [^54] | [Rethinking Data Augmentation for Tabular Data in Deep Learning.](http://arxiv.org/abs/2305.10308) | 本研究提出了一种新的表格数据增强方法“随机连续嵌入”（Random Continuous Embedding，RCE），能够提高 Transformer-based 预训练模型的自监督学习性能，大幅优于现有方法，并使得自监督学习模型能够在监督表格学习中优于树形方法。 |
| [^55] | [UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective.](http://arxiv.org/abs/2305.10306) | UniEX是一种能适用于各种模式格式的信息抽取框架，并能同时解决命名实体识别、关系抽取、事件提取和情感分析等任务，在性能和推理速度上优于其他通用信息抽取模型。 |
| [^56] | [MemoryBank: Enhancing Large Language Models with Long-Term Memory.](http://arxiv.org/abs/2305.10250) | MemoryBank 提出了一种新型内存机制，旨在为大型语言模型提供类人的长期记忆。它可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。 |
| [^57] | [Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer.](http://arxiv.org/abs/2305.09480) | 本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。 |
| [^58] | [Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability.](http://arxiv.org/abs/2305.08746) | BIMT方法使得神经网络更加模块化和可诠释，并且能够直接展示模块化结构，为许多简单任务提供了有用的信息，并可以补充当前的机理解释策略。 |
| [^59] | [On Strategies in Synthesis Over Finite Traces.](http://arxiv.org/abs/2305.08319) | 本研究显示，相较于终止自动机，在LTLf模型检查中使用非终止自动机会更加困难。 |
| [^60] | [Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering.](http://arxiv.org/abs/2305.08135) | 提出了一种基于概念中心提示的对比解释生成模型CPACE，旨在将获得的符号知识转化为对比解释，用于更好地区分常识问答中的差异。 |
| [^61] | [Understanding Model Averaging in Federated Learning on Heterogeneous Data.](http://arxiv.org/abs/2305.07845) | 本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。 |
| [^62] | [Generative AI: Implications and Applications for Education.](http://arxiv.org/abs/2305.07605) | 本文研究了一种生成式人工智能技术——基于大型语言模型的聊天机器人（C-LLM），以及它在复杂学生作品的人工智能评估和审查方面的应用，同时探讨了这种技术在教育中的应用范围。 |
| [^63] | [Temporal Network Creation Games.](http://arxiv.org/abs/2305.07494) | 本研究结合了时间图和博弈论网络形成，旨在解决时间路径保证所有对之间的可达性的算法问题。 |
| [^64] | [Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species.](http://arxiv.org/abs/2305.06695) | 本文提出了一种利用对齐的视觉-遗传推理空间来提高少量图像数据珍稀物种分类的方法，该方法通过深度嵌入模型实现对齐，适用于提高稀有物种的长尾识别，并且可以显著有益于仅基于视觉的稀有物种识别。 |
| [^65] | [Autonomous GIS: the next-generation AI-powered GIS.](http://arxiv.org/abs/2305.06453) | 自主GIS是一种AI动力地理信息系统，采用大型语言模型作为推理核心，具有自动空间数据收集、分析和可视化的能力，旨在实现五个自主目标：自动生成、自组织、自验证、自执行和自生长。 |
| [^66] | [A Framework for Designing Foundation Model based Systems.](http://arxiv.org/abs/2305.05352) | 本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。 |
| [^67] | [Distilling Script Knowledge from Large Language Models for Constrained Language Planning.](http://arxiv.org/abs/2305.05252) | 本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。 |
| [^68] | [X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages.](http://arxiv.org/abs/2305.04160) | 本论文提出了一种名为X-LLM的方法，将多模态信息转换为外语并输入到大型语言模型中，从而赋予LLM多模态能力，对于LLM加入多模态信息的能力进行了探究和拓展。 |
| [^69] | [GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.](http://arxiv.org/abs/2305.03403) | 介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。 |
| [^70] | [Improving Contrastive Learning of Sentence Embeddings from AI Feedback.](http://arxiv.org/abs/2305.01918) | 本文提出了一种利用人工智能反馈改进句子嵌入对比学习方法的方式，可以提高对比学习样本对的质量，并结合人类反馈来提供更好的监督信号。 |
| [^71] | [Are Emergent Abilities of Large Language Models a Mirage?.](http://arxiv.org/abs/2304.15004) | 研究指出大型语言模型所谓的新兴技能是研究者分析的产物，不是模型行为的基本变化。研究还展示了度量标准选择和可能研究人员的偏见，可能导致这种新兴技能的出现。 |
| [^72] | [Understanding accountability in algorithmic supply chains.](http://arxiv.org/abs/2304.14749) | 本文研究了算法供应链及其对算法系统治理和责任所带来的困难影响，认为算法责任的讨论必须考虑到供应链。 |
| [^73] | [Interpretable Neural-Symbolic Concept Reasoning.](http://arxiv.org/abs/2304.14068) | 本文提出了第一个基于概念嵌入的可解释概念模型DCR，能够在多个数据集上实现接近最先进的准确性，相对于最先进的可解释概念模型提高了高达+25％，并产生能够解释其预测的人类可理解规则和真值度，适应性强。 |
| [^74] | [IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies.](http://arxiv.org/abs/2304.10573) | 本文重新解释隐式Q学习(IQL)作为Actor-Critic方法，提出使用扩散行为策略和评判器权重来平衡奖励最大化和与行为策略的分歧。这个方法能够处理复杂和多峰特征的Actor问题。 |
| [^75] | [Why think step-by-step? Reasoning emerges from the locality of experience.](http://arxiv.org/abs/2304.03843) | 本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。 |
| [^76] | [Subject-driven Text-to-Image Generation via Apprenticeship Learning.](http://arxiv.org/abs/2304.00186) | 该论文提出了一种基于徒弟学习的面向主题的文本到图像生成器SuTI，能够通过将大量基于主题的专家模型的数据输入徒弟模型，学习并推断出新主题的最佳专家模型，从而生成高品质的自定义图像，且速度比传统方法更快。 |
| [^77] | [DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving.](http://arxiv.org/abs/2303.17144) | DAMO-StreamNet是一个优化自动驾驶中流式感知的框架，它融合了YOLO系列的最新进展，并通过颈部结构、双分支结构、蒸馏机制和实时预测机制等关键创新点，提供了尖端的解决方案。 |
| [^78] | [Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving.](http://arxiv.org/abs/2303.11888) | 本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。 |
| [^79] | [Reflexion: an autonomous agent with dynamic memory and self-reflection.](http://arxiv.org/abs/2303.11366) | 本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。 |
| [^80] | [Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes.](http://arxiv.org/abs/2303.09124) | 本文研究了利用纤维束形状特征预测非成像表型方面的潜力，并在人类连接组计划数据集上进行了实验，表明纤维束形状特征可以显著提高非成像表型的预测性能，为传统微结构和连接特征提供了补充信息。 |
| [^81] | [Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?.](http://arxiv.org/abs/2303.07242) | 工作场所感知技术的应用有助于提高生产力和福祉，但如何获取工人的有效同意是需要解决的难题，本文探讨了工人面临的挑战，并提出了可能的解决方案。 |
| [^82] | [Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning.](http://arxiv.org/abs/2303.02783) | 本文改进了分布式鲁棒强化学习的样本复杂度界限，提出了稳健分阶段价值学习（RPVL）算法来解决表格剧情学习环境下的不确定性问题。 |
| [^83] | [EvoTorch: Scalable Evolutionary Computation in Python.](http://arxiv.org/abs/2302.12600) | EvoTorch是一个Python库，为高维优化问题提供可扩展、可重用和实用的进化算法实现，支持GPU和高并行性能。 |
| [^84] | [Change is Hard: A Closer Look at Subpopulation Shift.](http://arxiv.org/abs/2302.12254) | 本文分析了子群体转变的各种机制，对20个最先进的算法在12个领域内进行了全面的基准测试，发现现有算法只能应对某些转变，进一步地，提出一种简单易行的选择标准来改善现有算法性能。 |
| [^85] | [One Fits All:Power General Time Series Analysis by Pretrained LM.](http://arxiv.org/abs/2302.11939) | 本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。 |
| [^86] | [Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?.](http://arxiv.org/abs/2302.08143) | 本文研究了元Prompt Tuning（MPT）如何帮助改善跨任务泛化能力。使用元学习可以从其他相关任务中学习初始化Prompt嵌入，我们提出并实验了代表性的元学习算法，并在大量的少样本任务中证明了MPT的有效性，特别是在分类任务中。 |
| [^87] | [Equilibrium and Learning in Fixed-Price Data Markets with Externality.](http://arxiv.org/abs/2302.08012) | 这篇论文研究了固定价格数据市场中的买家之间的竞争策略，发现其中存在负面影响。文章揭示了买家之间的负外部性及其影响，并提出了相应的市场干预措施，实现了纯策略均衡，并保证了强福利性质。 |
| [^88] | [Generalizable End-to-End Deep Learning Frameworks for Real-Time Attitude Estimation Using 6DoF Inertial Measurement Units.](http://arxiv.org/abs/2302.06037) | 本文提出了一种端到端深度学习框架，针对使用6DoF IMU测量值的实时惯性姿态估计。我们提出的模型对于不同的运动模式、采样率和环境干扰具有很好的通用性，在七个公开数据集上取得了最先进的性能。 |
| [^89] | [HDFormer: High-order Directed Transformer for 3D Human Pose Estimation.](http://arxiv.org/abs/2302.01825) | HDFormer是一种面向3D人体姿势估计的新方法，它结合了自我注意力和高阶注意力形成了一个多阶注意力模块，解决了复杂和遮挡较重情况下姿态估计中的问题。 |
| [^90] | [User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks.](http://arxiv.org/abs/2302.01471) | 本文提出了一种用户中心异构行动深度强化学习（UCHA-DRL）算法，可以使用在无线网络的元宇宙虚拟现实中。该算法可以联合优化服务器向用户下行通信的信道访问安排和传输功率来提高用户的效用。 |
| [^91] | [Which Experiences Are Influential for Your Agent? Policy Iteration with Turn-over Dropout.](http://arxiv.org/abs/2301.11168) | 本文提出了一种名为PI+ToD的方法，它通过淘汰正则化来高效估算经验对RL代理性能的影响。 |
| [^92] | [The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks.](http://arxiv.org/abs/2301.07068) | 本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。 |
| [^93] | [PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination.](http://arxiv.org/abs/2301.06387) | 本文提出了一种策略合奏的方法，以增加伙伴的多样性，结合上下文感知的方法使得智能体能够分析并识别伙伴的潜在策略基元，从而通过零样本学习更广泛的协作行为。 |
| [^94] | [KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales.](http://arxiv.org/abs/2212.09721) | 通过KNIFE，可以从自由文本理由中提取推理知识，进而在小型语言模型中提高推理能力。 |
| [^95] | [Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games.](http://arxiv.org/abs/2212.06027) | 该论文提出了一种针对多人不完全信息博弈的贝叶斯对手建模方法，在三人 Kuhn poker 中应用这种方法可以明显超过所有的代理商，包括准确的纳什均衡策略。 |
| [^96] | [Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next.](http://arxiv.org/abs/2212.04068) | 本文探究用于汉语漏字检查的字形音标信息的效果和应用方向，提出一个更具挑战性和实用性的测试设置，并公开了所有代码。 |
| [^97] | [Fairness in Multi-Agent Planning.](http://arxiv.org/abs/2212.00506) | 本论文介绍了两种基于公平性的方法来生成考虑成本的公平规划，并在多个标准多智能体规划基准测试中取得了优异的效果。研究表明，生成公平计划不需要牺牲太多的计划成本。 |
| [^98] | [Towards Adversarially Robust Recommendation from Adaptive Fraudster Detection.](http://arxiv.org/abs/2211.11534) | 本文提出了一种针对推荐系统的MetaC恶意攻击，并设计了一种自适应欺诈者检测模块PDR，明确考虑标签的不确定性，提高了推荐系统的鲁棒性。 |
| [^99] | [[RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations.](http://arxiv.org/abs/2211.11093) | 本文提出了一种名为[RE]VER的系统，使用基于transformer的模型来学习实体和关系的自然语言表示，能够生成一个能够表示实体与其他实体关系的句子，相比于之前的最先进方法有了显著改进。 |
| [^100] | [GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective.](http://arxiv.org/abs/2211.08073) | 本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs上对8个经典NLP任务进行评估。 |
| [^101] | [What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives.](http://arxiv.org/abs/2211.06009) | 本文综述了智能网格生成（IMG）现状，概括了113种IMG方法的核心技术、应用范围、代理学习目标、数据类型、目标挑战、优势和局限性，并提出了三个不同的分类法。 |
| [^102] | [The Sample Complexity of Online Contract Design.](http://arxiv.org/abs/2211.05732) | 本文解决了在线合同设计中一个悬而未决的问题，证明了指数级的$m$个样本就足以学习一个近乎最优的合同。 |
| [^103] | [Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control.](http://arxiv.org/abs/2211.03157) | 本研究提出了基于层次化复杂系统框架的模型，用于模拟风险和提供未来分析模板，深入探讨了机器学习领域的AI决策透明度和集成度的担忧，并强调了增强系统能力和自治性可能会导致权力动态的意外改变或灾难性故障。 |
| [^104] | [Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning.](http://arxiv.org/abs/2211.01576) | 本文提出一种基于学习的任务和动作规划（TAMP）算法，通过序列预测辅助搜索，显著提高了规划效率，运行时间缩短了80%。 |
| [^105] | [A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech.](http://arxiv.org/abs/2210.15368) | 为解决语音增强领域中“无清晰语音”的挑战，提出了使用增强语音作为目标的训练和推理策略，即使在域内和域外噪声差异较大的情况下仍能有效，实验结果优于基线方法。 |
| [^106] | [DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models.](http://arxiv.org/abs/2210.14896) | 介绍了DiffusionDB数据集，这是一个规模庞大的文本到图像提示数据集，总计包含1400万张图像和180万个唯一提示。该数据集被用来帮助研究人员解决文本提示生成图像时所需的适当提示的问题，并指出了一些特定的提示样式和超参数值可能导致模型错误，甚至生成误导信息。 |
| [^107] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^108] | [GNM: A General Navigation Model to Drive Any Robot.](http://arxiv.org/abs/2210.03370) | 通用目标条件模型GNM可以通过多样化的机器人数据实现更强大、更健壮的导航性能，并能驱动任何具有适当视觉感知输入的机器人。 |
| [^109] | [Hierarchical Adversarial Inverse Reinforcement Learning.](http://arxiv.org/abs/2210.01969) | 本文提出了一种分层对抗逆强化学习算法，能够在复杂任务中学习到具有层次结构的最优策略，比现有的方法更加有效。 |
| [^110] | [L2XGNN: Learning to Explain Graph Neural Networks.](http://arxiv.org/abs/2209.14402) | L2XGNN提出了一个框架来解释图神经网络，通过选择解释子图（模体）实现忠实的解释。该框架能够识别负责预测图属性的模体，并实现与基线方法相同的分类精度。 |
| [^111] | [Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models.](http://arxiv.org/abs/2209.08141) | 本文介绍了一种在大语言模型中使用心理学引导思维的方法来增强隐喻理解的性能。这种方法使用了隐含变量和关系来选择正确的释义。 |
| [^112] | [Differentiable Agent-based Epidemiology.](http://arxiv.org/abs/2207.09714) | 本文介绍一种可扩展、可微分的基于Agent的流行病模拟设计——GradABM，能够在较短时间内快速模拟百万级别的人口，并与深度神经网络集成和接受异构数据源，为校准、预测和评估政策干预提供了便利。 |
| [^113] | [Learning Deep Time-index Models for Time Series Forecasting.](http://arxiv.org/abs/2207.06046) | 本文提出了DeepTime框架，一种用于学习深度时间索引模型的元优化框架，能够有效地预测时间序列，并在真实世界数据集上获得了与先进方法相当的结果。 |
| [^114] | [Good Time to Ask: A Learning Framework for Asking for Help in Embodied Visual Navigation.](http://arxiv.org/abs/2206.10606) | 该论文提出了一种学习框架，使得机器人可以主动寻求帮助以解决表达需求式视觉导航任务中存在的难题，并展示了其在缺乏反馈信息的情况下依然稳健的抗干扰能力。 |
| [^115] | [PrivHAR: Recognizing Human Actions From Privacy-preserving Lens.](http://arxiv.org/abs/2206.03891) | 该论文提出了一种优化框架，通过参数化摄像头镜头，对视频进行处理以保护个人隐私，并在维护活动识别特征的同时进行了模拟和硬件实验的验证。 |
| [^116] | [Towards Power-Efficient Design of Myoelectric Controller based on Evolutionary Computation.](http://arxiv.org/abs/2204.02179) | 本文提出了一种基于核化SVM分类器的监督学习方法，通过约束的多目标优化问题实现肌电信号的高效控制，实验结果表明该方法在功率和分类准确性方面优于现有技术。 |
| [^117] | [Rethinking Efficient Lane Detection via Curve Modeling.](http://arxiv.org/abs/2203.02431) | 本文提出了一种新的参数曲线车道检测方法，在计算上更加简单且容易处理优化问题，并且在多个数据集中实现了最优的性能表现，可以作为未来车道检测领域的新基准。 |
| [^118] | [Deep Discriminative to Kernel Generative Networks for Calibrated Inference.](http://arxiv.org/abs/2201.13001) | 该论文提出了将判别网络转换为生成网络的方法，用高斯核替换多面体中的仿射函数来生成模型，解决了内部和外部数据校准问题，并在 CIFAR-10，CIFAR-100 和 SVHN 等基准数据集上测试了方法的有效性。 |
| [^119] | [PoNet: Pooling Network for Efficient Token Mixing in Long Sequences.](http://arxiv.org/abs/2110.02442) | PoNet是一种池化网络，可用于长序列中的token混合，其具有线性复杂度，并可以比Transformer更好地处理长序列。该方法提供了一种高效且有效的自注意替代方案。 |
| [^120] | [From Statistical Relational to Neural Symbolic Artificial Intelligence: a Survey.](http://arxiv.org/abs/2108.11451) | 这篇调查综述了NeSy和StarAI两种人工智能方法中，学习和推理的集成方法。共有七个维度用于对两种方法进行分类和比较。 |
| [^121] | [Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline.](http://arxiv.org/abs/2106.16046) | 本文研究了时空人群流量预测中的上下文泛化性，建立了基准，提出了通用分类法，为上下文选择和建模提供了指南。 |
| [^122] | [Communicating Natural Programs to Humans and Machines.](http://arxiv.org/abs/2106.07824) | LARC是通过自然语言描述的任务解决说明，可用于测试代理程序解决新问题的能力，并分析自然程序的独特之处。 |
| [^123] | [And/or trade-off in artificial neurons: impact on adversarial robustness.](http://arxiv.org/abs/2102.07389) | 本文研究了人工神经元中与/或函数的连续性对分类鲁棒性的影响，提出了增加类AND神经元在网络中比例的措施，实验结果显示该方法具有潜在的应用前景。 |
| [^124] | [Natural Language Specification of Reinforcement Learning Policies through Differentiable Decision Trees.](http://arxiv.org/abs/2101.07140) | 本文提出了一个新的可微分决策树框架，允许人们通过自然语言指定初始行为模型，并将其转换为词汇决策树，为机器人的强化学习策略提供指导。 |

# 详细

[^1]: 关于Moreau包络的一阶元强化学习

    On First-Order Meta-Reinforcement Learning with Moreau Envelopes. (arXiv:2305.12216v1 [cs.LG])

    [http://arxiv.org/abs/2305.12216](http://arxiv.org/abs/2305.12216)

    本文提出了一种Moreau包络元强化学习算法（MEMRL），通过利用Moreau包络代理正则化器，可以学习一个可以适应任务分布的元策略。

    

    元强化学习(MRL)是一种训练智能体在新环境和任务中快速适应的有前途的框架。在本文中，我们研究了基于策略梯度的MRL问题，提出了一种利用Moreau包络代理正则化器来共同学习可以适应每个任务环境的元策略的新算法。我们的算法称为Moreau包络元强化学习(MEMRL)，它通过梯度优化和Moreau包络正则化的组合有效地更新策略参数，学习可以适应任务分布的元策略。Moreau包络提供了策略优化问题的平滑近似，使我们能够应用标准优化技术并收敛到适当的稳定点。我们对MEMRL算法进行了详细的分析，展示了对于非凸问题，它可以以亚线性的收敛速度达到一阶稳定点。

    Meta-Reinforcement Learning (MRL) is a promising framework for training agents that can quickly adapt to new environments and tasks. In this work, we study the MRL problem under the policy gradient formulation, where we propose a novel algorithm that uses Moreau envelope surrogate regularizers to jointly learn a meta-policy that is adjustable to the environment of each individual task. Our algorithm, called Moreau Envelope Meta-Reinforcement Learning (MEMRL), learns a meta-policy that can adapt to a distribution of tasks by efficiently updating the policy parameters using a combination of gradient-based optimization and Moreau Envelope regularization. Moreau Envelopes provide a smooth approximation of the policy optimization problem, which enables us to apply standard optimization techniques and converge to an appropriate stationary point. We provide a detailed analysis of the MEMRL algorithm, where we show a sublinear convergence rate to a first-order stationary point for non-convex p
    
[^2]: ComedicSpeech: 用于低资源情境下的单口喜剧文本生成语音系统

    ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios. (arXiv:2305.12200v1 [cs.SD])

    [http://arxiv.org/abs/2305.12200](http://arxiv.org/abs/2305.12200)

    本文构建了一个用于低资源情境中的单口喜剧文本生成语音系统ComedicSpeech，该系统使用语调编码器提取语调表示并以灵活的方式将其定向到TTS模型，同时通过有条件的持续时间预测器来增强个人韵律建模，并通过引入与喜剧演员相关的特殊标记来模拟个人填充语。实验表明，ComedicSpeech在仅有十分钟训练数据情况下比基线模型表现出更好的表现力。

    

    文本语音合成（TTS）模型可生成自然且高质量的语音，但在合成具有戏剧性表现力的语音时（如单口喜剧），其表现力不够。考虑到喜剧演员有着多样的个人演讲风格，包括个人语调、韵律和填充语，这需要真实世界的数据集和强大的语音风格建模能力，这带来了挑战。本文构建了一个新的数据集，并开发了ComedicSpeech，这是一个专门用于低资源场景中的单口喜剧合成的TTS系统。首先，我们使用语调编码器提取语调表示，并以灵活的方式将其定向到TTS模型。其次，我们通过有条件的持续时间预测器增强了个人韵律建模。第三，我们通过引入与喜剧演员相关的特殊标记来模拟个人填充语。实验表明，ComedicSpeech在每个喜剧演员仅有十分钟训练数据情况下，比基线模型表现出更好的表现力。音频样本可在https://bit.ly/3dxm5Ol获得。

    Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are ava
    
[^3]: 我们需要编码器-解码器来模拟网络上的动力系统吗？

    Do We Need an Encoder-Decoder to Model Dynamical Systems on Networks?. (arXiv:2305.12185v1 [cs.LG])

    [http://arxiv.org/abs/2305.12185](http://arxiv.org/abs/2305.12185)

    本论文揭示了使用潜在嵌入来模拟网络动力系统的不足，并提出了一种无嵌入的替代方案，通过三个正确长期行为测试的验证表明该方案的可行性。

    

    随着深度学习在模拟动力系统方面的普及，我们揭示了一个与网络动力学建模相关且未被重视的误解。受图神经网络的强烈影响，潜在顶点嵌入被自然地采用在许多神经动力网络模型中。然而，我们证明了嵌入倾向于产生适应观测良好但同时具有错误动力行为的模型。我们提出了三个正确长期行为的测试，并说明了嵌入式动力模型如何未通过这些测试，并分析其中的原因，特别是通过拓扑共轭的角度。通过这样做，我们表明避免使用嵌入可以避免困难。我们提出了一种简单的基于参数化两个加性矢量场分量的无嵌入替代方案。通过广泛的实验，我们验证了所提出的替代方案的优越性。

    As deep learning gains popularity in modelling dynamical systems, we expose an underappreciated misunderstanding relevant to modelling dynamics on networks. Strongly influenced by graph neural networks, latent vertex embeddings are naturally adopted in many neural dynamical network models. However, we show that embeddings tend to induce a model that fits observations well but simultaneously has incorrect dynamical behaviours. Recognising that previous studies narrowly focus on short-term predictions during the transient phase of a flow, we propose three tests for correct long-term behaviour, and illustrate how an embedding-based dynamical model fails these tests, and analyse the causes, particularly through the lens of topological conjugacy. In doing so, we show that the difficulties can be avoided by not using embedding. We propose a simple embedding-free alternative based on parametrising two additive vector-field components. Through extensive experiments, we verify that the proposed
    
[^4]: 反对解释性论文

    The Case Against Explainability. (arXiv:2305.12167v1 [cs.AI])

    [http://arxiv.org/abs/2305.12167](http://arxiv.org/abs/2305.12167)

    研究发现人工智能系统的解释权利难以充分解释法律目的，涉及到公正判决、正当程序、人类机构认证和决策者权威提升。

    

    随着人工智能(AI)的普及，监管机构对于这些系统所做决策的解释要求日益增加。然而，在可解释性机器学习系统实现有意义的解释权利的需求和能力之间仍存在巨大的差距。AI系统“解释权利”的监管呼声可归因于法律中解释的重要作用。因此，在本文中，我们研究了法律中解释的目的，以分析终端用户可解释性所提供的原因是否足以满足这些目的。我们发现法律中解释的目的包括：(a)做出更好的、更公正的决定，(b)促进正当程序，(c)鉴定人类机构，(d)增强决策者的权威。通过这种方法，我们证明了终端用户可解释性的不足之处。

    As artificial intelligence (AI) becomes more prevalent there is a growing demand from regulators to accompany decisions made by such systems with explanations. However, a persistent gap exists between the need to execute a meaningful right to explanation vs. the ability of Machine Learning systems to deliver on such a legal requirement. The regulatory appeal towards "a right to explanation" of AI systems can be attributed to the significant role of explanations, part of the notion called reason-giving, in law. Therefore, in this work we examine reason-giving's purposes in law to analyze whether reasons provided by end-user Explainability can adequately fulfill them.  We find that reason-giving's legal purposes include: (a) making a better and more just decision, (b) facilitating due-process, (c) authenticating human agency, and (d) enhancing the decision makers' authority. Using this methodology, we demonstrate end-user Explainabilty's inadequacy to fulfil reason-giving's role in law, 
    
[^5]: 一种可扩展的神经网络用于DSIC仿射极大价拍卖设计

    A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])

    [http://arxiv.org/abs/2305.12162](http://arxiv.org/abs/2305.12162)

    该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。

    

    自动拍卖设计旨在通过机器学习寻找经验上高收入的机制。现有的多物品拍卖情景的工作可以粗略地分为RegretNet类和仿射极大价（AMAs）方法。然而，前者不能严格保证占优策略激励兼容性（DSIC），而后者因为分配候选人数过多而面临可扩展性问题。为解决这些限制，我们提出了AMenuNet，一种可扩展的神经网络，它从出价人和物品表示中构造AMA参数（甚至包括分配菜单）。由于AMA的属性，AMenuNet始终是DSIC和个人理性（IR）的，通过神经网络生成候选分配来增强可伸缩性。此外，AMenuNet是置换等变的，其参数数量不受拍卖规模的影响。我们进行了大量实验，证明AMenuNet在协商一致的价值和社会残余价值方面优于强基线模型。

    Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
    
[^6]: LogiCoT：基于GPT-4的逻辑思维指令调整数据收集。

    LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])

    [http://arxiv.org/abs/2305.12147](http://arxiv.org/abs/2305.12147)

    该论文提出了LogiCoT, 一个基于GPT-4的逻辑思维指令调整数据集，用于教授模型逻辑推理和引出一般推理技能。

    

    生成式预训练变压器4（GPT-4）展示了令人印象深刻的思维链推理能力。最近的自我指导调整研究（如Alpaca）侧重于增强模型的通用能力。这些指令使模型在一般任务（如开放领域文本生成和释义）上能够达到与GPT-3.5相当的性能。然而，它们不能帮助模型处理复杂的推理任务。为填补这一差距，本文提出了LogiCoT，一种新的逻辑思维指令调整数据集，用于GPT-4的逻辑思维链推理。我们详细阐述了收集指令以提示GPT-4生成思维链推理的过程。LogiCoT作为教授逻辑推理模型的指令集，并引出了一般推理技能。

    Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
    
[^7]: DiffCap：探索图像字幕生成中的连续扩散

    DiffCap: Exploring Continuous Diffusion on Image Captioning. (arXiv:2305.12144v1 [cs.CV])

    [http://arxiv.org/abs/2305.12144](http://arxiv.org/abs/2305.12144)

    DiffCap通过应用连续扩散，以自然的方式转换离散标记，并成功融合提取的图像特征进行扩散字幕生成，其具有更多的解码多样性。

    

    当前图像字幕生成主要集中在以自回归方式生成描述上。然而，针对非自回归方式生成描述的研究仍然有限，这种方式可以带来更多的解码多样性。受扩散模型在生成自然图像方面的成功启发，我们提出了一种新方法DiffCap，在图像字幕生成中应用连续扩散。与输出大小连续的图像生成不同，图像描述长度随着离散标记而变化。我们的方法以自然的方式转换离散标记，并对它们应用连续扩散，以成功融合提取的图像特征进行扩散字幕生成。我们在COCO数据集上的实验表明，我们的方法使用更简单的结构，即可达到与以往非自回归作品相当的结果。除了质量，DiffCap的一个有趣特性是其在生成过程中高多样性，这在许多自回归作品中都缺少。

    Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive 
    
[^8]: ChatGPT在软件工程中的应用范围：全面探究

    The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])

    [http://arxiv.org/abs/2305.12138](http://arxiv.org/abs/2305.12138)

    ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。

    

    ChatGPT展示了在软件工程中转化的巨大潜力，表现出在代码和文档生成等任务中的卓越性能。然而，软件工程需要高可靠性和风险控制，使ChatGPT的缺乏可解释性成为一个问题。为了解决这个问题，我们进行了一项研究，评估了ChatGPT在软件工程中的能力和局限性。我们将AI模型应对SE任务所需的能力分为三类：1）语法理解，2）静态行为理解，和3）动态行为理解。我们的调查重点是ChatGPT理解代码语法和语义结构的能力，包括抽象语法树（AST）、控制流程图（CFG）和调用图（CG）。我们评估了ChatGPT在涉及C、Java、Python和Solidity的跨语言任务中的表现。我们的发现表明，虽然ChatGPT表现出了对理解代码语法（AST）的出色能力，但在理解代码语义和部分功能上存在问题。

    ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
    
[^9]: 多模式联邦人类活动识别中的隐私问题

    Privacy in Multimodal Federated Human Activity Recognition. (arXiv:2305.12134v1 [cs.LG])

    [http://arxiv.org/abs/2305.12134](http://arxiv.org/abs/2305.12134)

    本文研究了多模式联邦人类活动识别中的隐私问题。通过一个特定的系统，联邦学习可以提供更好的隐私保护，同时不会损失人类活动识别的准确性。

    

    人类活动识别（HAR）的训练数据往往包含隐私信息或由不合作实体持有。联邦学习（FL）通过在边缘设备上训练机器学习模型来解决这些问题。本文研究了在用户、环境和传感器级别上隐私对联邦HAR的影响。我们表明，FL对HAR的性能取决于FL系统的隐私保护程度，并且主要取决于来自不同传感器的数据的配置。尽管避免数据共享并在人类或环境级别上假设隐私，如之前的工作所做的那样，精度会降低5-7％。然而，将这种隐私延伸到模态级别并严格分离多个客户端之间的传感器数据可能会导致精度降低19-42％。由于这种形式的隐私是HAR中被要求的道德利用被动传感方法所必需的，因此我们实现了一种系统，在该系统中客户端相互训练一个通用的FL模型和一个每种模态一个的组级模型。我们的评估表明，这种方法可以在不牺牲HAR准确性的情况下提高隐私保护。

    Human Activity Recognition (HAR) training data is often privacy-sensitive or held by non-cooperative entities. Federated Learning (FL) addresses such concerns by training ML models on edge clients. This work studies the impact of privacy in federated HAR at a user, environment, and sensor level. We show that the performance of FL for HAR depends on the assumed privacy level of the FL system and primarily upon the colocation of data from different sensors. By avoiding data sharing and assuming privacy at the human or environment level, as prior works have done, the accuracy decreases by 5-7%. However, extending this to the modality level and strictly separating sensor data between multiple clients may decrease the accuracy by 19-42%. As this form of privacy is necessary for the ethical utilisation of passive sensing methods in HAR, we implement a system where clients mutually train both a general FL model and a group-level one per modality. Our evaluation shows that this method leads to
    
[^10]: DexPBT：基于人口的训练为手臂系统的灵巧操作扩展规模

    DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training. (arXiv:2305.12127v1 [cs.RO])

    [http://arxiv.org/abs/2305.12127](http://arxiv.org/abs/2305.12127)

    本文提出了一种分散的基于人口的训练(PBT)算法，使单臂或双臂机器人在模拟环境中可以学习灵巧的物体操作技能，包括重新抓取、抓取-投掷和物体重新定位，相较于常规的端到端学习，该算法可大大提高深度强化学习的探索能力和发现稳健控制策略的能力。

    

    本文提出了算法和方法，利用带有多指手末端执行器的单臂或双臂机器人进行模拟的灵巧物体操作学习。使用并行的GPU加速物理模拟器(Isaac Gym)，我们为这些机器人实现了具有挑战性的任务，包括重新抓取、抓取-投掷和物体重新定位。为了解决这些问题，我们引入了一种分散的基于人口的训练(PBT)算法，使我们能够大规模地扩展深度强化学习的探索能力。我们发现，这种方法明显优于常规的端到端学习，并能够在具有挑战性的任务中发现稳健的控制策略。可以在 https://sites.google.com/view/dexpbt 找到所学行为的视频演示和代码。

    In this work, we propose algorithms and methods that enable learning dexterous object manipulation using simulated one- or two-armed robots equipped with multi-fingered hand end-effectors. Using a parallel GPU-accelerated physics simulator (Isaac Gym), we implement challenging tasks for these robots, including regrasping, grasp-and-throw, and object reorientation. To solve these problems we introduce a decentralized Population-Based Training (PBT) algorithm that allows us to massively amplify the exploration capabilities of deep reinforcement learning. We find that this method significantly outperforms regular end-to-end learning and is able to discover robust control policies in challenging tasks. Video demonstrations of learned behaviors and the code can be found at https://sites.google.com/view/dexpbt
    
[^11]: 深度前馈网络的稳定一致训练框架

    A Framework for Provably Stable and Consistent Training of Deep Feedforward Networks. (arXiv:2305.12125v1 [cs.LG])

    [http://arxiv.org/abs/2305.12125](http://arxiv.org/abs/2305.12125)

    该论文提出了一种用于深度神经网络的稳定一致训练框架，其核心在于结合了标准随机梯度下降和梯度裁剪方法以及使用自动稳定的压缩激活函数，从而提高了深度神经网络的稳定性和训练效果。

    

    我们提出了一种新算法，用于监督（分类和回归）和非监督（强化学习）场景下的深度神经网络训练。该算法结合了标准随机梯度下降和梯度裁剪方法。输出层使用裁剪梯度更新，其余神经网络使用标准梯度更新。使用裁剪梯度更新输出层可以使其稳定。我们表明，只要神经网络仅由压缩（紧凑范围）激活函数组成，其余层将自动被稳定。我们还提出了一种新颖的压缩激活函数 - 通过修改高斯误差线性单元（GELU）来获得 - 我们称之为Truncated GELU（tGELU）。与其他压缩激活函数（如Sigmoid）不同，tGELU的范围可以明确指定。因此，在一些激活函数（例如Sigmod）的范围较小时会出现的梯度消失问题可以避免。

    We present a novel algorithm for training deep neural networks in supervised (classification and regression) and unsupervised (reinforcement learning) scenarios. This algorithm combines the standard stochastic gradient descent and the gradient clipping method. The output layer is updated using clipped gradients, the rest of the neural network is updated using standard gradients. Updating the output layer using clipped gradient stabilizes it. We show that the remaining layers are automatically stabilized provided the neural network is only composed of squashing (compact range) activations. We also present a novel squashing activation function - it is obtained by modifying a Gaussian Error Linear Unit (GELU) to have compact range - we call it Truncated GELU (tGELU). Unlike other squashing activations, such as sigmoid, the range of tGELU can be explicitly specified. As a consequence, the problem of vanishing gradients that arise due to a small range, e.g., in the case of a sigmoid activat
    
[^12]: 退火自蒸馏校正改进了对抗训练

    Annealing Self-Distillation Rectification Improves Adversarial Training. (arXiv:2305.12118v1 [cs.LG])

    [http://arxiv.org/abs/2305.12118](http://arxiv.org/abs/2305.12118)

    本研究提出了退火自蒸馏校正(ADR)方法，其能生成软标签用作更好的指导机制，准确反映在对抗训练中攻击下的分布变化，提高模型的鲁棒性，并实现了平滑的插入性整合到其他对抗性训练技术中。

    

    标准的对抗训练中，模型被优化以适应可接受的对抗扰动预算内的一热标签。然而，忽略由扰动带来的基础分布变化，导致了强健的过拟合问题。为了解决这个问题，增强对抗性鲁棒性，我们分析了强健模型的特征，并确定强健模型倾向于生成更平滑和更良好校准的输出。基于这一观测结果，我们提出了一种简单而有效的方法——退火自蒸馏校正(ADR)，该方法生成软标签作为更好的指导机制，能准确反映在对抗训练中攻击下的分布变化。通过使用ADR，我们可以获得修正的分布，显著改善模型的鲁棒性，而不需要预训练模型或额外的计算。此外，我们的方法通过替换卷积层以实现平滑的插入性整合到其他对抗性训练技术中。

    In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by repl
    
[^13]: GFDC：一种基于证据推理的颗粒聚类算法

    GFDC: A Granule Fusion Density-Based Clustering with Evidential Reasoning. (arXiv:2305.12114v1 [cs.LG])

    [http://arxiv.org/abs/2305.12114](http://arxiv.org/abs/2305.12114)

    提出了一种基于证据推理的颗粒聚类算法GFDC，该算法可以克服基于密度的聚类算法在衡量全局密度、确定合理的聚类中心或结构、准确地分配样本以及处理具有大密度差异的数据方面表现不佳的缺点。与此同时，GFDC 采用了三种新颖的颗粒融合策略来将颗粒组合成稳定的聚类结构，有助于检测具有任意形状的聚类。

    

    目前，基于密度的聚类算法因其能够检测具有任意形状的类群而被广泛应用。然而，它们在衡量全局密度、确定合理的聚类中心或结构、准确地分配样本以及处理具有大密度差异的数据方面表现不佳。为了克服它们的缺点，本文提出了一种基于证据推理的颗粒聚类算法（GFDC）。首先，通过一种稀疏度量指标来测量样本的局部和全局密度。然后在高密度和低密度区域中生成信息颗粒，帮助处理具有显著密度差异的聚类。此外，采用了三种新颖的颗粒融合策略来将颗粒组合成稳定的聚类结构，有助于检测具有任意形状的聚类。最后，通过基于Dempster-Shafer理论开发的分配方法来分配不稳定的样本。使用GFDC后，可以得到合理的聚类结果。

    Currently, density-based clustering algorithms are widely applied because they can detect clusters with arbitrary shapes. However, they perform poorly in measuring global density, determining reasonable cluster centers or structures, assigning samples accurately and handling data with large density differences among clusters. To overcome their drawbacks, this paper proposes a granule fusion density-based clustering with evidential reasoning (GFDC). Both local and global densities of samples are measured by a sparse degree metric first. Then information granules are generated in high-density and low-density regions, assisting in processing clusters with significant density differences. Further, three novel granule fusion strategies are utilized to combine granules into stable cluster structures, helping to detect clusters with arbitrary shapes. Finally, by an assignment method developed from Dempster-Shafer theory, unstable samples are assigned. After using GFDC, a reasonable clustering
    
[^14]: 人类标注误差及其对卫星图像场景分类的ConvNets的影响

    Human labeling errors and their impact on ConvNets for satellite image scene classification. (arXiv:2305.12106v1 [cs.CV])

    [http://arxiv.org/abs/2305.12106](http://arxiv.org/abs/2305.12106)

    本文针对卫星图像场景分类中人工标注数据集的不可避免的误差问题，收集了真实标签数据，并探讨这些误差如何影响三个ConvNets的分类结果。研究发现，人类标注误差具有显着的类和实例依赖性，并且对不同CNN结构的影响不同，因此需要仔细考虑这些误差并开发更强大的ConvNets来解决这一挑战。

    

    卷积神经网络(ConvNets)已成功应用于卫星图像场景分类。对于ConvNets执行准确分类，人工标注的训练数据集至关重要。然而，由于卫星图像的复杂性，人工标注数据集中的误差是不可避免的。然而，人类标注误差在卫星图像上的分布及其对ConvNets的影响尚未进行研究。为了填补这一研究空白，本研究首次收集了来自32位参与者的真实标签，并探讨了它们的误差如何影响高分辨率卫星图像场景分类中的三个ConvNets（VGG16，GoogleNet和ResNet-50）。我们发现：（1）人类标注误差具有显着的类和实例依赖性，这与以前研究中的模拟噪声本质上不同，（2）关于所有类的总体精度，当训练数据的人工标注误差增加1个单位时，ConvNets的总体精度会降低0.67%~1.02%；（3）人类标注误差对不同CNN结构的影响各不相同。本研究强调了仔细检查卫星图像场景分类中的人类标注误差的重要性，并呼吁开发强大的ConvNets来解决这一挑战。

    Convolutional neural networks (ConvNets) have been successfully applied to satellite image scene classification. Human-labeled training datasets are essential for ConvNets to perform accurate classification. Errors in human-labeled training datasets are unavoidable due to the complexity of satellite images. However, the distribution of human labeling errors on satellite images and their impact on ConvNets have not been investigated. To fill this research gap, this study, for the first time, collected real-world labels from 32 participants and explored how their errors affect three ConvNets (VGG16, GoogleNet and ResNet-50) for high-resolution satellite image scene classification. We found that: (1) human labeling errors have significant class and instance dependence, which is fundamentally different from the simulation noise in previous studies; (2) regarding the overall accuracy of all classes, when human labeling errors in training data increase by one unit, the overall accuracy of Co
    
[^15]: UP5: 面向公平性推荐的无偏基础模型

    UP5: Unbiased Foundation Model for Fairness-aware Recommendation. (arXiv:2305.12090v1 [cs.IR])

    [http://arxiv.org/abs/2305.12090](http://arxiv.org/abs/2305.12090)

    本研究提出了一种新颖的基础模型UP5，它采用反事实公平促进技术来消除大型语言模型中的偏见，从而实现面向公平性的推荐。

    

    基于大型语言模型（LLM）等基础模型的最新进展，已将它们推到了推荐系统（RS）的前沿。此外，RS中的公平性很关键，因为许多用户将其用于决策和需求履行。然而，目前尚缺乏对推荐基础模型展示公平性水平和公平处理不同用户群组的适当方法的理解。本文侧重于用户方面的不公平问题，并通过彻底检查表明，LLMs中存在不公平性，导致不公平的推荐结果。为了消除LLM中的偏差以实现面向公平性的推荐，我们引入了一种基于反事实公平促进技术的新型无偏P5（UP5）基础模型。CFP包括两个子模块：个性化前缀提示和Prompt混合，从而增强了个体敏感属性的公平性。

    Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that int
    
[^16]: 基于深度强化学习的同行评审期刊系统中评审奖励的博弈论分析与实验评估

    Game-Theoretical Analysis of Reviewer Rewards in Peer-Review Journal Systems: Analysis and Experimental Evaluation using Deep Reinforcement Learning. (arXiv:2305.12088v1 [cs.AI])

    [http://arxiv.org/abs/2305.12088](http://arxiv.org/abs/2305.12088)

    本文针对同行评审期刊中代金券奖励制度可能会导致评审人员二元决策的问题，提出了一种替代奖励系统，有效地促进了更全面的审查，实验结果表明该系统更加平衡且稳定。

    

    本文利用数学准确性和博弈论策略洞察力，探讨了开放获取学术出版中的评审奖励领域。我们将现有的代金券奖励系统概念化为一个两个玩家的博弈，并识别了可能导致评审人员倾向于二元决策的潜在缺点。为解决这个问题，我们提出并数学上形式化了一种替代奖励系统，旨在减轻这种偏见，促进更全面的审查。我们运用严格的博弈论分析和深度强化学习模拟，对两种系统的属性和结果进行了详细的研究。我们的结果强调了两种系统之间的显着差异，我们提出的系统展现出了更平衡的决策分配和更加稳定的特点。这项研究不仅扩充了有关评审奖励系统的数学理解，而且提供了解决当前代金券奖励系统偏见问题的潜在解决方案。

    In this paper, we navigate the intricate domain of reviewer rewards in open-access academic publishing, leveraging the precision of mathematics and the strategic acumen of game theory. We conceptualize the prevailing voucher-based reviewer reward system as a two-player game, subsequently identifying potential shortcomings that may incline reviewers towards binary decisions. To address this issue, we propose and mathematically formalize an alternative reward system with the objective of mitigating this bias and promoting more comprehensive reviews. We engage in a detailed investigation of the properties and outcomes of both systems, employing rigorous game-theoretical analysis and deep reinforcement learning simulations. Our results underscore a noteworthy divergence between the two systems, with our proposed system demonstrating a more balanced decision distribution and enhanced stability. This research not only augments the mathematical understanding of reviewer reward systems, but it
    
[^17]: AnyPredict: 表格预测的基础模型

    AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])

    [http://arxiv.org/abs/2305.12081](http://arxiv.org/abs/2305.12081)

    本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。

    

    基础模型是在大规模数据上预先训练的模型，可以在许多下游任务中表现良好。它们在自然语言处理和计算机视觉方面取得了显著的成功。然而，这种模型在表格预测任务中的使用受到限制，主要问题包括 (1) 缺乏大规模和多样化的带有标准标签的表格数据集，以及 (2) 不同领域之间的模式不匹配和预测目标的异质性。本文提出了一种方法，用于构建基于 AnyPredict 的表格预测基础模型的大规模训练数据，包括领域内和广泛的领域外数据集。该方法使用数据引擎，利用大型语言模型 (LLM) 来整合表格样本，克服了不同模式表格之间的障碍，并使用“学习，注释和审计”流程将领域外数据与目标任务对齐。扩展的训练数据使预训练的 AnyPredict 能够支持每个表格领域。

    Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
    
[^18]: 深度学习中的GELU激活函数：全面的数学分析和性能评估

    GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance. (arXiv:2305.12073v1 [cs.LG])

    [http://arxiv.org/abs/2305.12073](http://arxiv.org/abs/2305.12073)

    本文对GELU激活函数进行了全面的数学分析和广泛的实验比较，证明了它在深度学习模型中具有优越的性能和适用性。

    

    在深度学习模型中，选择最合适的激活函数是影响其学习能力、稳定性和计算效率的关键因素。近年来，高斯误差线性单元（GELU）激活函数已经成为一种主流方法，在各种应用中超越了传统的激活函数，如修正线性单元（ReLU）。本文对GELU激活函数进行了严格的数学分析，详细探讨了其可微性、有界性、平稳性和光滑性等性质。此外，我们对GELU函数进行了广泛的实验比较，利用在CIFAR-10、CIFAR-100和STL-10数据集上训练的残差卷积网络作为实证测试基础。我们的结果证明了GELU相对于其他激活函数的卓越性能，确立了它在广泛的深度学习模型中的适用性。

    Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide ra
    
[^19]: 利用卷积变分自编码器进行技术异常点检测：以ADMAMI乳腺X线数据集为例

    Technical outlier detection via convolutional variational autoencoder for the ADMANI breast mammogram dataset. (arXiv:2305.12068v1 [eess.IV])

    [http://arxiv.org/abs/2305.12068](http://arxiv.org/abs/2305.12068)

    该论文介绍了如何使用卷积变分自编码器来检测乳腺X线图像中的技术异常点，以保证一个乳腺癌检测算法的准确性和稳定性。

    

    澳大利亚BreastScreen Victoria运营着Transforming Breast Cancer Screening with AI计划，收集了注释数字乳腺X线图像及其相关非图像数据，组成了大规模、临床筛选、真实世界的ADMANI数据集。该数据集可望为乳腺癌检测、早期诊断和其他应用开发临床相关的人工智能算法提供帮助。然而，在进行近下游算法开发之前，必须删除技术异常值以确保高质量的数据。我们首先随机选取3万个乳腺X线图像，使用深度生成神经网络中一种类型——卷积变分自编码器（CVAE）来检测异常值。尽管CVAE在不同类型的异常点检测性能上有所不同，但它预计可检测到各种异常值类型。对于CVAE的性能不足，传统的图像处理技术，如腐蚀和胸大肌分析，则可进行补偿。

    The ADMANI datasets (annotated digital mammograms and associated non-image datasets) from the Transforming Breast Cancer Screening with AI programme (BRAIx) run by BreastScreen Victoria in Australia are multi-centre, large scale, clinically curated, real-world databases. The datasets are expected to aid in the development of clinically relevant Artificial Intelligence (AI) algorithms for breast cancer detection, early diagnosis, and other applications. To ensure high data quality, technical outliers must be removed before any downstream algorithm development. As a first step, we randomly select 30,000 individual mammograms and use Convolutional Variational Autoencoder (CVAE), a deep generative neural network, to detect outliers. CVAE is expected to detect all sorts of outliers, although its detection performance differs among different types of outliers. Traditional image processing techniques such as erosion and pectoral muscle analysis can compensate for the poor performance of CVAE 
    
[^20]: DADIN: 面向跨域推荐系统的领域对抗深度兴趣网络

    DADIN: Domain Adversarial Deep Interest Network for Cross Domain Recommender Systems. (arXiv:2305.12058v1 [cs.IR])

    [http://arxiv.org/abs/2305.12058](http://arxiv.org/abs/2305.12058)

    论文提出了一种创新性的深度跨领域点击率预测模型——领域对抗深度兴趣网络（DADIN），该模型通过引入领域不可知层和特别设计的损失，创新地实现了两个领域的联合分布对齐，并采用对抗训练的方式与点击率预测损失一起进行优化，相比竞争基线算法提升明显。

    

    点击率预测是推荐系统的主要任务之一，用户针对不同项目进行点击以获取推荐结果。针对数据稀疏性、用户-项目交互的长尾分布和项目或用户的冷启动等问题，提出了跨领域点击率预测模型。为了使源域到目标域的知识转移更加顺畅，提出了创新性的深度跨领域点击率预测模型——领域对抗深度兴趣网络 (DADIN)，将跨域推荐任务转化为领域适应问题。通过引入领域不可知层和特别设计的损失，创新地实现了两个领域的联合分布对齐，并采用对抗训练的方式与点击率预测损失一起进行优化。实验结果表明，在华为数据集上，DADIN 的曲线下面积 (AUC) 比最具竞争力的基线高出0.08％，高出0.7％。

    Click-Through Rate (CTR) prediction is one of the main tasks of the recommendation system, which is conducted by a user for different items to give the recommendation results. Cross-domain CTR prediction models have been proposed to overcome problems of data sparsity, long tail distribution of user-item interactions, and cold start of items or users. In order to make knowledge transfer from source domain to target domain more smoothly, an innovative deep learning cross-domain CTR prediction model, Domain Adversarial Deep Interest Network (DADIN) is proposed to convert the cross-domain recommendation task into a domain adaptation problem. The joint distribution alignment of two domains is innovatively realized by introducing domain agnostic layers and specially designed loss, and optimized together with CTR prediction loss in a way of adversarial training. It is found that the Area Under Curve (AUC) of DADIN is 0.08% higher than the most competitive baseline on Huawei dataset and is 0.7
    
[^21]: CodeCompose：基于AI辅助的代码编写工具的大规模工业部署

    CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring. (arXiv:2305.12050v1 [cs.SE])

    [http://arxiv.org/abs/2305.12050](http://arxiv.org/abs/2305.12050)

    CodeCompose是一个基于InCoder LLM的AI辅助的代码编写工具，已经在Meta内部部署，可在10多种编程语言和几个编码表面上使用。 CodeCompose已成功帮助提高Meta的开发人员生产力。

    

    大型语言模型（LLMs）的崛起解锁了该技术在软件开发中的各种应用。特别是，已经证明生成式LLMs可以有效地驱动AI基础的代码编写工具，在编写代码期间可以建议整个语句或代码块。在本文中，我们介绍了基于InCoder LLM的CodeCompose，这是一个Meta内部开发和部署的AI辅助代码编写工具。CodeCompose基于合并生成能力和双向性的LLM。我们已经将CodeCompose扩展到Meta的数以万计的开发人员，在10多种编程语言和几个编码表面上使用。我们讨论了在大规模工业环境中部署此类工具时出现的用户体验和指标方面的独特挑战。我们介绍了CodeCompose的模型和系统架构的设计决策，以解决这些挑战。最后，我们提供了我们对CodeCompose的大规模部署的指标，并展示它如何帮助提高Meta的开发人员生产力。

    The rise of large language models (LLMs) has unlocked various applications of this technology in software development. In particular, generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 10+ programming languages and several coding surfaces.  We discuss unique challenges in terms of user experience and metrics that arise when deploying such tools in large-scale industrial settings. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges. Finally, we present metrics from our large-scale deployment of C
    
[^22]: 临床骆驼：一种具有基于对话的知识编码的开源专家级医学语言模型

    Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v1 [cs.CL])

    [http://arxiv.org/abs/2305.12031](http://arxiv.org/abs/2305.12031)

    临床骆驼是一种基于对话的知识编码的开源医学语言模型，具有很高的可解释性和临床相关性，并在多个基准数据集上取得了最先进的结果。

    

    大型语言模型（LLM）在医疗领域具有巨大潜力，但数据隐私、监管合规性和模型稳定性等问题限制了它们的广泛应用。为了应对这些挑战，我们提出了基于对话的知识编码（DBKE）。DBKE增强了模型的隐式知识库，使其具有更强的对话能力，为后续用例提供了软对齐。我们提出了Clinical Camel，这是一个开源的、专注于医疗保健的会话模型，来展示DBKE的有效性。Clinical Camel在几个基准数据集上实现了最先进的结果，同时保持了高水平的可解释性和临床相关性。它还为医疗应用提供了一个可信赖的、开放源代码的替代品。

    Large Language Models (LLMs) present immense potential in the medical field, yet concerns over data privacy, regulatory compliance, and model stability restrict their widespread adoption. Although the distillation of high-performing closed-source LLMs has proven effective for general tasks, their application in healthcare is limited due to reduced domain knowledge and remnants of alignment behavior hindering clinical tasks. To address these challenges, we propose Dialogue-Based Knowledge Encoding (DBKE). DBKE enhances models' implicit knowledge base and primes them for conversational recall, augmenting their conversational capabilities and enabling a soft alignment for subsequent use cases. By transforming dense academic source text into synthetic dialogue, DBKE broadens the model's knowledge base and enables a soft alignment that guides downstream behaviours. We present Clinical Camel, an open-source, healthcare-focused conversational model, to showcase the effectiveness of DBKE. Clin
    
[^23]: 学习连续的图序列 -- 动力系统方法

    Learning Continually on a Sequence of Graphs -- The Dynamical System Way. (arXiv:2305.12030v1 [cs.LG])

    [http://arxiv.org/abs/2305.12030](http://arxiv.org/abs/2305.12030)

    该论文研究的是如何进行图数据的持续学习，并面临着由于数据基础特性所导致的理论与方法上的挑战。

    

    持续学习(CL)是一个领域，关注于学习一系列相互关联的任务，这些任务通常是以回归或分类的方式定义的。近年来，当这些任务是使用欧几里得数据定义的时，如图像，CL已经被广泛研究。然而，当与CL任务相对应的数据是非欧几里德的时，如图形、点云或流形，欧几里得度量意义下的相似性概念并不适用。因此，为非欧几里德数据开发CL面临着几个理论和方法上的挑战。特别是，对于图中的CL需要显式地模拟节点和边的非平稳行为。

    Continual learning~(CL) is a field concerned with learning a series of inter-related task with the tasks typically defined in the sense of either regression or classification. In recent years, CL has been studied extensively when these tasks are defined using Euclidean data-- data, such as images, that can be described by a set of vectors in an n-dimensional real space. However, the literature is quite sparse, when the data corresponding to a CL task is nonEuclidean-- data , such as graphs, point clouds or manifold, where the notion of similarity in the sense of Euclidean metric does not hold. For instance, a graph is described by a tuple of vertices and edges and similarities between two graphs is not well defined through a Euclidean metric. Due to this fundamental nature of the data, developing CL for nonEuclidean data presents several theoretical and methodological challenges. In particular, CL for graphs requires explicit modelling of nonstationary behavior of vertices and edges an
    
[^24]: MultiTurnCleanup：用于多轮口语会话转录清理的基准测试

    MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])

    [http://arxiv.org/abs/2305.12029](http://arxiv.org/abs/2305.12029)

    本研究提出了MultiTurnCleanup任务，收集了新的数据集MultiTurnCleanup1，针对口语会话转录中的不连续现象进行探讨并提供了两个可用于未来研究的基准测试模型。

    

    目前的语调不连续检测模型侧重于单个说话者的每个话语。然而，口语会话转录中的许多不连续现象都发生在多轮对话中，这影响了人类的可读性和下游 NLP 任务的性能。本研究通过提出创新的“MultiTurnCleanup”任务，针对口语会话转录中的不连续现象进行探讨，并收集了新的数据集MultiTurnCleanup1。我们设计了一种数据标注模式以收集高质量的数据集，提供了广泛的数据分析。此外，我们利用两种建模方法进行实验评估，作为未来研究的基准测试。

    Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
    
[^25]: 极地鸭子的发现之旅：使用鸭式辨析和极坐标盒嵌入增强实体链接

    Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v1 [cs.CL])

    [http://arxiv.org/abs/2305.12027](http://arxiv.org/abs/2305.12027)

    本文提出了 DUCK 方法，通过使用实体类型的先前知识在实体表示空间中注入结构信息。把盒嵌入概念引入到极坐标中，将关系表示为超球面上的盒子，将具有相似类型的实体放置在对应于它们关系的盒子内，实现聚类。在实体链接方面取得了最先进的结果，尤其在低资源环境下效果显著。

    

    基于密集检索的实体链接方法是大规模应用中高效且广泛使用的解决方案，但它们在性能上不如生成模型，因为它们对嵌入空间的结构敏感。为解决此问题，本文提出了 DUCK 方法，通过使用实体类型的先前知识在实体表示空间中注入结构信息。受编程语言中鸭式辨析的启发，我们提议根据实体在知识图中的关系定义实体的类型。然后，将盒嵌入的概念移植到球形极坐标中，我们提议将关系表示为超球面上的盒子。我们通过将具有相似类型的实体放置在对应于它们关系的盒子内来优化模型以聚类实体。我们的实验证明，我们的方法在标准实体消歧基准测试上设置了新的最先进结果，它提高了密集检索方法的性能，并且特别不适用于生成模型不可行的低资源环境。

    Entity linking methods based on dense retrieval are an efficient and widely used solution in large-scale applications, but they fall short of the performance of generative models, as they are sensitive to the structure of the embedding space. In order to address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we propose to define the type of an entity based on the relations that it has with other entities in a knowledge graph. Then, porting the concept of box embeddings to spherical polar coordinates, we propose to represent relations as boxes on the hypersphere. We optimize the model to cluster entities of similar type by placing them inside the boxes corresponding to their relations. Our experiments show that our method sets new state-of-the-art results on standard entity-disambiguation benchmarks, it improves the perf
    
[^26]: 基于能效随态电容器的物理储层计算系统用于时间数据处理

    Energy-efficient memcapacitive physical reservoir computing system for temporal data processing. (arXiv:2305.12025v1 [cs.LG])

    [http://arxiv.org/abs/2305.12025](http://arxiv.org/abs/2305.12025)

    本文研究了一种基于能效随态电容器的物理储层计算系统，解决了时间数据的分类任务和分析时间序列数据的问题。在实验中，系统能够实现较高的准确率和较小的均方误差。

    

    储层计算是一种高效的机器学习框架，通过从输入信号中提取特征并将其映射到高维空间来处理时间数据。物理储层可使用磁旋电子、原子开关网络、硅光学模块、铁电晶体管和易失性存储器来实现。然而，这些设备由于其电阻性质本质上存在能量耗散问题，导致功耗增加。因此，采用电容存储器设备可提供更为能效的解决方案。在这里，我们利用模拟和实验中近似某些短期突触可塑性功能的易失生物膜基质量作为储层，解决分类任务和分析时间序列数据。我们的系统在口音数字分类中实现了98％的准确率，在二阶非线性回归任务中获得了0.0012的归一化均方误差。

    Reservoir computing is a highly efficient machine learning framework for processing temporal data by extracting features from the input signal and mapping them into higher dimensional spaces. Physical reservoir layers have been realized using spintronic oscillators, atomic switch networks, silicon photonic modules, ferroelectric transistors, and volatile memristors. However, these devices are intrinsically energy-dissipative due to their resistive nature, which leads to increased power consumption. Therefore, capacitive memory devices can provide a more energy-efficient approach. Here, we leverage volatile biomembrane-based memcapacitors that closely mimic certain short-term synaptic plasticity functions as reservoirs to solve classification tasks and analyze time-series data in simulation and experimentally. Our system achieves a 98% accuracy rate for spoken digit classification and a normalized mean square error of 0.0012 in a second-order non-linear regression task. Further, to demo
    
[^27]: 通过自然灵感发明绘画风格

    Inventing painting styles through natural inspiration. (arXiv:2305.12015v1 [cs.CV])

    [http://arxiv.org/abs/2305.12015](http://arxiv.org/abs/2305.12015)

    本文提出了两种方法，通过仅使用自然图像训练的模型创建绘画风格，而不需要艺术训练数据，这能为艺术中的生成AI合法使用铺平道路。

    

    我们提出了两种通过仅使用自然图像训练的模型创建绘画风格的方法，从而提供了客观证据证明模型没有剽窃人类艺术风格。在第一种方法中，我们使用艺术媒介的归纳偏置来实现创造性的表达。通过使用重建损失来实现抽象。第二种方法使用额外的自然图像作为灵感来创建新的风格。这两种方法使得可以发明新的绘画风格，而无需艺术训练数据。我们相信，我们的方法可以帮助为艺术中生成AI的合法使用铺平道路，而不侵犯人类创造者的独创性。

    We propose two procedures to create painting styles using models trained only on natural images, providing objective proof that the model is not plagiarizing human art styles. In the first procedure we use the inductive bias from the artistic medium to achieve creative expression. Abstraction is achieved by using a reconstruction loss. The second procedure uses an additional natural image as inspiration to create a new style. These two procedures make it possible to invent new painting styles with no artistic training data. We believe that our approach can help pave the way for the ethical employment of generative AI in art, without infringing upon the originality of human creators.
    
[^28]: 用生成式AI构建梦想

    Constructing Dreams using Generative AI. (arXiv:2305.12013v1 [cs.HC])

    [http://arxiv.org/abs/2305.12013](http://arxiv.org/abs/2305.12013)

    本文介绍了一个“与AI一同做梦”的学习车间，让高中生了解和使用了生成式AI工具，并反思了潜在利益和伤害，提出了政策意见。

    

    生成式AI工具为青少年引入了新的、易于接触的媒体创作形式。它们也引发了有关虚假媒体生成、数据保护、隐私和AI生成艺术所有权等方面的伦理问题。鉴于生成式AI已经被应用于青少年使用的产品中，使他们理解这些工具的工作原理及其使用和误用方法具有重要性。在本研究中，我们通过表达他们想象中的未来身份来促进学生生成式AI学习。我们设计了一个学习车间，“与AI一同做梦”，学生们了解了生成式AI工具的内部工作原理，使用文本转图像生成算法创建了他们的形象未来梦想，并反思了生成式AI工具的潜在利益和伤害，并就课堂中使用这些工具的政策发表了意见。本文介绍了34名高中学生参加我们工作坊的学习活动和体验。学生们达到了创意性的高度，并反思了AI工具的利弊。

    Generative AI tools introduce new and accessible forms of media creation for youth. They also raise ethical concerns about the generation of fake media, data protection, privacy and ownership of AI-generated art. Since generative AI is already being used in products used by youth, it is critical that they understand how these tools work and how they can be used or misused. In this work, we facilitated students' generative AI learning through expression of their imagined future identities. We designed a learning workshop - Dreaming with AI - where students learned about the inner workings of generative AI tools, used text-to-image generation algorithms to create their imaged future dreams, reflected on the potential benefits and harms of generative AI tools and voiced their opinions about policies for the use of these tools in classrooms. In this paper, we present the learning activities and experiences of 34 high school students who engaged in our workshops. Students reached creative l
    
[^29]: 一种基于Transformer的图形方法为OpenMP并行化提供建议

    Advising OpenMP Parallelization via a Graph-Based Approach with Transformers. (arXiv:2305.11999v1 [cs.DC])

    [http://arxiv.org/abs/2305.11999](http://arxiv.org/abs/2305.11999)

    本文提出了一种名为OMPify的新方法，该方法基于Transformer模型，通过对串行代码的分析，自动检测和预测并行代码中的OpenMP编译指示符和共享内存属性。

    

    利用多核架构的全部潜力，需要共享内存并行化方案。当前最常见的解决方案是OpenMP并行编程接口。虽然手动编写并行代码是复杂和费力的，但是许多确定性源到源（S2S）编译器已经涌现，旨在自动化将串行代码转换为并行代码的过程。然而，最近的研究表明，在许多情况下这些编译器是不实际的。在本文中，我们将AI和自然语言处理（NLP）领域的最新进展与大量的开源代码相结合，以解决自动并行化的问题。具体而言，我们提出了一种新方法，称为OMPify，通过串行代码来检测和预测并行代码中的OpenMP编译指示符和共享内存属性，OMPify基于基于Transformer的模型，利用源代码的基于图形的表示。

    There is an ever-present need for shared memory parallelization schemes to exploit the full potential of multi-core architectures. The most common parallelization API addressing this need today is OpenMP. Nevertheless, writing parallel code manually is complex and effort-intensive. Thus, many deterministic source-to-source (S2S) compilers have emerged, intending to automate the process of translating serial to parallel code. However, recent studies have shown that these compilers are impractical in many scenarios. In this work, we combine the latest advancements in the field of AI and natural language processing (NLP) with the vast amount of open-source code to address the problem of automatic parallelization. Specifically, we propose a novel approach, called OMPify, to detect and predict the OpenMP pragmas and shared-memory attributes in parallel code, given its serial version. OMPify is based on a Transformer-based model that leverages a graph-based representation of source code that
    
[^30]: 具有概率保证的神经网络鲁棒的反事实解释

    Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])

    [http://arxiv.org/abs/2305.11997](http://arxiv.org/abs/2305.11997)

    本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。

    

    针对神经网络发现偏移，通过使用稳定性度量来量化反事实解释对可能的模型变化的鲁棒性。通过在反事实解释优化中引入正则化项来将生成的反事实解释靠近数据流形，从而实现了对自然发生的模型变化的高概率鲁棒性。新的算法在合成和现实世界数据集上进行实验，证明了其有效性。

    There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
    
[^31]: EEG和EMG数据集，用于检测主动矫形器引入的错误

    EEG and EMG dataset for the detection of errors introduced by an active orthosis device. (arXiv:2305.11996v1 [cs.HC])

    [http://arxiv.org/abs/2305.11996](http://arxiv.org/abs/2305.11996)

    该论文介绍了一个包含EEG和EMG记录的数据集，这些记录来自被协助使用主动矫形器移动右臂的受试者。研究者故意引入了一些错误，以检测新的错误检测方法。

    

    本文介绍了包含电脑图谱（EEG）和肌电图（EMG）记录的数据集，这些记录来自8名被协助使用主动矫形器移动右臂的受试者。该设备支持肘关节运动，即右臂的屈曲和伸展。当矫形器主动移动被试者的手臂时，故意引入了一些错误，持续时间很短。在此期间，矫形器向相反的方向移动。本文解释了实验设置，并展示了跨所有受试者的一些行为分析。此外，我们还为一个受试者提供了平均事件相关电位分析，以提供关于数据质量和错误引入所引起的EEG活动的见解。在此介绍的数据集是公开可访问的。本研究的目的是为研究社区提供数据集，特别是针对异步检测错误的新方法的开发。

    This paper presents a dataset containing recordings of the electroencephalogram (EEG) and the electromyogram (EMG) from eight subjects who were assisted in moving their right arm by an active orthosis device. The supported movements were elbow joint movements, i.e., flexion and extension of the right arm. While the orthosis was actively moving the subject's arm, some errors were deliberately introduced for a short duration of time. During this time, the orthosis moved in the opposite direction. In this paper, we explain the experimental setup and present some behavioral analyses across all subjects. Additionally, we present an average event-related potential analysis for one subject to offer insights into the data quality and the EEG activity caused by the error introduction. The dataset described herein is openly accessible. The aim of this study was to provide a dataset to the research community, particularly for the development of new methods in the asynchronous detection of erroneo
    
[^32]: “美好的新供应链世界”：算法预测时代下的工人发声和劳工标准

    "Sch\"one neue Lieferkettenwelt": Workers' Voice und Arbeitsstandards in Zeiten algorithmischer Vorhersage. (arXiv:2305.11981v1 [cs.CY])

    [http://arxiv.org/abs/2305.11981](http://arxiv.org/abs/2305.11981)

    面对日益复杂的供应链，压力来自于消费者、批评的公众以及供应链法规等，领先的公司正在尝试通过算法预测商业、环境和社会风险来改善劳工条件，并为不同利益相关者群体提供政策选择。

    

    供应链的复杂性和越来越紧密的耦合对于领先的公司构成了重大的后勤挑战。另一个挑战是，领先的公司在消费者、批评的公众以及供应链法规等压力下需要比以前更多地对他们的供应商劳工标准负责。本文讨论了领先企业用于应对这些挑战的一种新方法：算法预测商业风险，以及环境和社会风险。我们描述了算法预测的技术和文化条件，并解释了从领先公司的角度来看，它如何帮助解决这两个挑战。然后，我们开发了一些场景，说明和哪种社会后果下，领先的公司可以使用算法预测。通过这些场景，我们为不同利益相关者群体提供了政策选择，以帮助发展算法预测并改善劳工条件。

    The complexity and increasingly tight coupling of supply chains poses a major logistical challenge for leading companies. Another challenge is that leading companies -- under pressure from consumers, a critical public and legislative measures such as supply chain laws -- have to take more responsibility than before for their suppliers' labour standards. In this paper, we discuss a new approach that leading companies are using to try to address these challenges: algorithmic prediction of business risks, but also environmental and social risks. We describe the technical and cultural conditions for algorithmic prediction and explain how -- from the perspective of leading companies -- it helps to address both challenges. We then develop scenarios on how and with what kind of social consequences algorithmic prediction can be used by leading companies. From the scenarios, we derive policy options for different stakeholder groups to help develop algorithmic prediction towards improving labour
    
[^33]: 基于逻辑的Benders分解在答案集规划中用于慢性门诊预约的研究

    Logic-Based Benders Decomposition in Answer Set Programming for Chronic Outpatients Scheduling. (arXiv:2305.11969v1 [cs.AI])

    [http://arxiv.org/abs/2305.11969](http://arxiv.org/abs/2305.11969)

    本文提出并评估了一种基于逻辑的Benders分解方法，用于解决慢性门诊预约等大规模优化问题，相比于其他方法具有更高的效率、可扩展性和最优性保证。

    

    在答案集规划（ASP）中，用户可以声明性地定义问题，并使用高效的求解器来解决它; ASP的实际应用无数，且已成功地解决了几个约束问题。但是，解决时间通常以超线性方式增长（通常是指数级）相对于实例的大小，这对于大型实例是不实际的。一个广泛使用的方法是将优化问题分割为顺序解决的子问题，某些子问题提交到由其他子问题指定的值，并通过并置单个子问题的解的解决方案重构整个问题的有效分配。这种方法一方面更快，由于超线性行为;另一方面，它不提供任何最优性保证：承诺解决一个子问题的分配可能会将最优解从搜索空间中排除。在其他研究领域中，基于逻辑的Benders分解（LBBD）证明是解决大规模优化问题的有效方法，确保最优性保证;但是，在ASP中的影响尚未完全探索。在本文中，我们提出并评估了一种LBBD方法，用于管理异构资源和目标的实际问题慢性门诊的预约。实验结果突出了我们的方法在效率，可扩展性和最优性方面的有效性，并与最先进的基于ASP和基于CP的求解器进行了比较。

    In Answer Set Programming (ASP), the user can define declaratively a problem and solve it with efficient solvers; practical applications of ASP are countless and several constraint problems have been successfully solved with ASP. On the other hand, solution time usually grows in a superlinear way (often, exponential) with respect to the size of the instance, which is impractical for large instances. A widely used approach is to split the optimization problem into sub-problems that are solved in sequence, some committing to the values assigned by others, and reconstructing a valid assignment for the whole problem by juxtaposing the solutions of the single sub-problems. On the one hand this approach is much faster, due to the superlinear behavior; on the other hand, it does not provide any guarantee of optimality: committing to the assignment of one sub-problem can rule out the optimal solution from the search space. In other research areas, Logic-Based Benders Decomposition (LBBD) prove
    
[^34]: 不是所有的语义都是平等的：具有自定义温度的对比自监督学习

    Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization. (arXiv:2305.11965v1 [cs.LG])

    [http://arxiv.org/abs/2305.11965](http://arxiv.org/abs/2305.11965)

    本文提出了一种具有个性化温度的对比损失用于自监督学习，根据数据分布自动调整温度以使得训练更加有效。

    

    本文旨在通过原则性和系统性的方式，优化具有个性化温度的对比损失，用于自监督学习。普遍做法是将全局温度参数τ用于所有数据，忽略了“不是所有的语义都是平等的”这个事实，特别是在数据展示长尾分布时，不同的锚点数据可能具有不同数量的类似语义的样本。我们提出了一种基于分布鲁棒性优化（DRO）的新型鲁棒对比损失，为我们提供了有关τ的影响的直觉和自动温度个性化的机制。然后，我们提出了一种有效的随机算法来优化鲁棒性对比损失，具有可证明的收敛保证，而不需要使用大型小批量大小。理论和实验结果表明，我们的算法自动学习每个样本的合适τ。具体来说，具有频繁语义的样本使用较大温度以保持难度。

    In this paper, we aim to optimize a contrastive loss with individualized temperatures in a principled and systematic manner for self-supervised learning. The common practice of using a global temperature parameter $\tau$ ignores the fact that ``not all semantics are created equal", meaning that different anchor data may have different numbers of samples with similar semantics, especially when data exhibits long-tails. First, we propose a new robust contrastive loss inspired by distributionally robust optimization (DRO), providing us an intuition about the effect of $\tau$ and a mechanism for automatic temperature individualization. Then, we propose an efficient stochastic algorithm for optimizing the robust contrastive loss with a provable convergence guarantee without using large mini-batch sizes. Theoretical and experimental results show that our algorithm automatically learns a suitable $\tau$ for each sample. Specifically, samples with frequent semantics use large temperatures to k
    
[^35]: PyTorch的超参数调整——面向spotPython的教程

    PyTorch Hyperparameter Tuning -- A Tutorial for spotPython. (arXiv:2305.11930v1 [cs.LG])

    [http://arxiv.org/abs/2305.11930](http://arxiv.org/abs/2305.11930)

    本文介绍了如何将spotPython超参数调谐器集成到PyTorch训练工作流中，以提高机器或深度学习模型的性能，以CIFAR10图像分类器为例。

    

    超参数调整（或超参数优化）的目标是优化超参数以提高机器或深度学习模型的性能。spotPython是知名超参数调谐器SPOT的Python版本，SPOT已经在R编程环境中为统计分析开发了十年以上。PyTorch是一种基于GPU和CPU的深度学习优化张量库。本文展示了如何将spotPython超参数调谐器集成到PyTorch训练工作流中。以CIFAR10图像分类器为例，介绍了spotPython以及与Ray Tune的简短比较。本文讨论了两种方法的优缺点。我们展示了spotPython的使用经验，以及如何使用hook在训练过程中自动调整参数。

    The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPytho
    
[^36]: 使用学习自动机的节能且可解释AI硬件设计

    Energy-frugal and Interpretable AI Hardware Design using Learning Automata. (arXiv:2305.11928v1 [cs.AI])

    [http://arxiv.org/abs/2305.11928](http://arxiv.org/abs/2305.11928)

    本论文通过使用学习自动机实现了节能的AI硬件设计，同时保持了模型的解释性和准确性。

    

    在微边缘计算环境下，能效是实现强大人工智能应用的重要需求。通过节能的计算资源配置实现硬件加速是降低能耗的有效方法。然而，许多新兴应用还需要采用可解释决策模型，以确立责任和透明度。在真实数据场景中提供可达状态需要额外的资源，这给能效设计带来了冲突性的挑战。最近，提出了一种新的机器学习算法——Tsetlin机器，该算法基于有限状态自动机原理，与算术不同，受益于自然逻辑支撑。本文研究了如何通过适当调整超参数来实现节能的人工智能硬件设计，并保持高效的学习效果。为了展示其潜力，我们在不同的优化技术下在可编程逻辑门阵列（FPGAs）上实现了Tsetlin机器算法，并使用标准基准数据集评估其性能。实验结果表明，通过利用学习自动机，我们可以在不牺牲模型的解释性和准确性的情况下，实现显著的能源节约。

    Energy efficiency is a crucial requirement for enabling powerful artificial intelligence applications at the microedge. Hardware acceleration with frugal architectural allocation is an effective method for reducing energy. Many emerging applications also require the systems design to incorporate interpretable decision models to establish responsibility and transparency. The design needs to provision for additional resources to provide reachable states in real-world data scenarios, defining conflicting design tradeoffs between energy efficiency. is challenging.  Recently a new machine learning algorithm, called the Tsetlin machine, has been proposed. The algorithm is fundamentally based on the principles of finite-state automata and benefits from natural logic underpinning rather than arithmetic. In this paper, we investigate methods of energy-frugal artificial intelligence hardware design by suitably tuning the hyperparameters, while maintaining high learning efficacy. To demonstrate i
    
[^37]: 一种对比集合稳定无误差的多重比较基准评估方法

    An Approach to Multiple Comparison Benchmark Evaluations that is Stable Under Manipulation of the Comparate Set. (arXiv:2305.11921v1 [stat.ME])

    [http://arxiv.org/abs/2305.11921](http://arxiv.org/abs/2305.11921)

    本文提出了一种新的基准比较结果展示方法——多元比较矩阵（MCM），使得比较集合稳定无误差，可避免常用方法存在的无意和有意的操纵空间，并且其采用Python实现，已在公开提供。

    

    基准评估是计算机科学和机器学习中广泛使用的衡量进步的方法。然而，目前常用的方法对于多个算法在多个数据集上的基准比较结果分析和展示，如Dem\v{s}ar（2006）引入的关键差异图存在重大缺陷，并且我们发现这些方法存在无意和有意的操纵空间。为了解决这些问题，我们提出了一种新的基准比较结果展示方法——多元比较矩阵（MCM），该方法优先考虑成对比较，排除了现有方法中操纵实验结果的方式。MCM可用于显示全对比结果，或显示一个或多个选择的算法与技术的对比结果。MCM采用Python实现，并公开提供。

    The measurement of progress using benchmarks evaluations is ubiquitous in computer science and machine learning. However, common approaches to analyzing and presenting the results of benchmark comparisons of multiple algorithms over multiple datasets, such as the critical difference diagram introduced by Dem\v{s}ar (2006), have important shortcomings and, we show, are open to both inadvertent and intentional manipulation. To address these issues, we propose a new approach to presenting the results of benchmark comparisons, the Multiple Comparison Matrix (MCM), that prioritizes pairwise comparisons and precludes the means of manipulating experimental results in existing approaches. MCM can be used to show the results of an all-pairs comparison, or to show the results of a comparison between one or more selected algorithms and the state of the art. MCM is implemented in Python and is publicly available.
    
[^38]: 用于从稀疏远程传感器数据重建非线性海洋波浪表面相位的机器学习方法

    Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data. (arXiv:2305.11913v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.11913](http://arxiv.org/abs/2305.11913)

    本文提出了一种基于神经网络的方法，利用高度现实的合成训练数据对稀疏雷达数据进行相位相关的波浪表面重建。

    

    准确预测相位相关的水波条件对于海洋工程的决策至关重要。然而，远程监测波浪预测模型的初始化首先需要从类似雷达的稀疏测量中重建波浪表面。现有的重建方法要么依赖于计算密集型的优化过程，要么依赖于简化的模型假设，这会影响整个预测过程的实时性或准确性。因此，我们提出了一种基于U-Net和Fourier神经算子（FNO）结构的神经网络方法，用于相位相关的波浪表面重建。我们的方法利用具有高度现实性的合成训练数据，这些数据在均匀的一维网格上由波浪模拟的高阶谱方法和几何雷达建模方法生成。研究结果表明，两种模型都可以提供准确的波浪重建结果。

    Accurate short-term prediction of phase-resolved water wave conditions is crucial for decision-making in ocean engineering. However, the initialization of remote-sensing-based wave prediction models first requires a reconstruction of wave surfaces from sparse measurements like radar. Existing reconstruction methods either rely on computationally intensive optimization procedures or simplistic modeling assumptions that compromise real-time capability or accuracy of the entire prediction process. We therefore address these issues by proposing a novel approach for phase-resolved wave surface reconstruction using neural networks based on the U-Net and Fourier neural operator (FNO) architectures. Our approach utilizes synthetic yet highly realistic training data on uniform one-dimensional grids, that is generated by the high-order spectral method for wave simulation and a geometric radar modeling approach. The investigation reveals that both models deliver accurate wave reconstruction resul
    
[^39]: 利用海洋记忆效应预测中国南方冬季空气稳定指数的长期预报

    Long-lead forecasts of wintertime air stagnation index in southern China using oceanic memory effects. (arXiv:2305.11901v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.11901](http://arxiv.org/abs/2305.11901)

    该研究基于海洋记忆效应开发了一个LSTM模型，结合过去的ASI和尼娜指数可以实现更好的ASI预测，为提前制定空气质量管理计划提供了帮助。

    

    不利于空气污染物的稀释和清除，是导致空气污染的主要因素之一。空气稳定指数（ASI）是测量大气清除空气污染物能力的一项重要气象指标。因此，进行长期ASI预报对于提前制定空气质量管理计划至关重要。本研究发现，由海表温度异常推导出的秋季尼娜指数与中国南方冬季ASI呈负相关，为预测冬季ASI提供了前景。我们开发了一个基于LSTM的模型来预测未来的ASI。结果表明，多元输入（过去的ASI和尼娜指数）比单元输入（仅过去的ASI）具有更好的预测性能。该模型在实际和预测ASI之间实现了0.778的相关系数，表现出高度的一致性。

    Stagnant weather condition is one of the major contributors to air pollution as it is favorable for the formation and accumulation of pollutants. To measure the atmosphere's ability to dilute air pollutants, Air Stagnation Index (ASI) has been introduced as an important meteorological index. Therefore, making long-lead ASI forecasts is vital to make plans in advance for air quality management. In this study, we found that autumn Ni\~no indices derived from sea surface temperature (SST) anomalies show a negative correlation with wintertime ASI in southern China, offering prospects for a prewinter forecast. We developed an LSTM-based model to predict the future wintertime ASI. Results demonstrated that multivariate inputs (past ASI and Ni\~no indices) achieve better forecast performance than univariate input (only past ASI). The model achieves a correlation coefficient of 0.778 between the actual and predicted ASI, exhibiting a high degree of consistency.
    
[^40]: ChatGPT与劳动力市场：揭示AI讨论对学生收入预期影响

    ChatGPT and the Labor Market: Unraveling the Effect of AI Discussions on Students' Earnings Expectations. (arXiv:2305.11900v1 [econ.GN])

    [http://arxiv.org/abs/2305.11900](http://arxiv.org/abs/2305.11900)

    本文研究了 ChatGPT人工智能（AI）讨论对美国学生预期劳动市场结果的因果影响，结果发现学生信心会降低，对未来的收入前景保持悲观态度，这种影响广泛存在于不同学生群体中。这个研究给教育工作者、管理者和政策制定者提供了一个机会，以更好地了解学生的担忧并改进教育课程，使学生更好地准备未来，这个未来必然会被AI改变。

    

    本文研究了负面和正面 ChatGPT人工智能（AI）讨论对美国学生预期劳动市场结果的因果影响。我们的研究发现，在接触AI讨论后，学生信心会降低，特别是在阅读负面情绪的讨论摘录时，这种影响更加明显。与STEM专业不同，非STEM领域的学生表现出不对称和悲观的信仰变化，表明他们可能感受到新兴AI技术的影响更强。对于未来收入的悲观信仰更新也普遍存在于各个性别和GPA水平之间，这表明所有学生群体都存在广泛的AI担忧。教育工作者、管理者和政策制定者可以定期与学生互动以解决他们的担忧，并改进教育课程，使他们更好地准备未来，这个未来必然会被AI改变。

    This paper investigates the causal impact of negatively and positively framed ChatGPT Artificial Intelligence (AI) discussions on US students' anticipated labor market outcomes. Our findings reveal students reduce their confidence regarding their future earnings prospects after exposure to AI debates, and this effect is more pronounced after reading discussion excerpts with a negative tone. Unlike STEM majors, students in Non-STEM fields show asymmetric and pessimistic belief changes, suggesting that they might feel more vulnerable to emerging AI technologies. Pessimistic belief updates regarding future earnings are also prevalent across gender and GPA levels, indicating widespread AI concerns among all student subgroups. Educators, administrators, and policymakers may regularly engage with students to address their concerns and enhance educational curricula to better prepare them for a future that will be inevitably shaped by AI.
    
[^41]: 高效的脉冲编码图像去噪神经信息处理研究

    Neural information coding for efficient spike-based image denoising. (arXiv:2305.11898v1 [cs.CV])

    [http://arxiv.org/abs/2305.11898](http://arxiv.org/abs/2305.11898)

    本文研究了使用脉冲神经网络进行高斯图像去噪的方法，提出了使用Leaky Integrate and Fire (LIF)神经元进行信息处理的方案，相对于传统速率编码机制，在保证性能的同时起到了降低计算负载的作用。

    

    近年来，深度卷积神经网络（DCNNs）已经超越了传统算法在图像恢复任务上的表现。然而，这些方法大多数并不适合计算效率，因此执行起来过于昂贵，无法在嵌入式和移动设备上执行。在本文中，我们研究了脉冲神经网络（SNNs）在高斯去噪方面的应用，以达到与传统DCNN相当的性能，同时降低计算负载。我们提出了Leaky Integrate and Fire (LIF)神经元进行信息转换处理的形式分析，并将其性能与传统的速率编码机制进行了比较。然后，通过实验评估了神经编码方案在最先进的深度卷积神经网络的去噪性能和计算效率。我们的结果表明，具有LIF神经元的SNNs可以提供具有竞争力的去噪性能，同时降低计算成本。

    In recent years, Deep Convolutional Neural Networks (DCNNs) have outreached the performance of classical algorithms for image restoration tasks. However most of these methods are not suited for computational efficiency and are therefore too expensive to be executed on embedded and mobile devices. In this work we investigate Spiking Neural Networks (SNNs) for Gaussian denoising, with the goal of approaching the performance of conventional DCNN while reducing the computational load. We propose a formal analysis of the information conversion processing carried out by the Leaky Integrate and Fire (LIF) neurons and we compare its performance with the classical rate-coding mechanism. The neural coding schemes are then evaluated through experiments in terms of denoising performance and computation efficiency for a state-of-the-art deep convolutional neural network. Our results show that SNNs with LIF neurons can provide competitive denoising performance but at a reduced computational cost.
    
[^42]: 人工智能中介交流的关键评估

    Critical Appraisal of Artificial Intelligence-Mediated Communication. (arXiv:2305.11897v1 [cs.HC])

    [http://arxiv.org/abs/2305.11897](http://arxiv.org/abs/2305.11897)

    本文探讨了人工智能在语言教育中介交流的优缺点，并强调语言教师积极参与CALL教师教育和专业发展的重要性。

    

    在过去的20年中，语言学习和教学中的技术应用得到了显著发展，现在被称为计算机辅助语言学习（CALL）。最近，将人工智能（AI）整合到CALL中，显著改变了语言教育在课堂内外的传统方法。在本文中，我探讨了AI中介交流在语言教育中的优缺点。我首先简要回顾了教育中的AI技术。然后，我介绍了ICALL，并对AI驱动的自动语音识别（ASR）、机器翻译（MT）、智能辅导系统（ITSs）、AI驱动的聊天机器人和扩展现实（XR）的潜力进行了关键评估。最后，我认为，语言教师积极参与CALL教师教育和专业发展至关重要，以跟上不断发展的技术景观并提高自己的教学效果。

    Over the last two decades, technology use in language learning and teaching has significantly advanced and is now referred to as Computer-Assisted Language Learning (CALL). Recently, the integration of Artificial Intelligence (AI) into CALL has brought about a significant shift in the traditional approach to language education both inside and outside the classroom. In line with this book's scope, I explore the advantages and disadvantages of AI-mediated communication in language education. I begin with a brief review of AI in education. I then introduce the ICALL and give a critical appraisal of the potential of AI-powered automatic speech recognition (ASR), Machine Translation (MT), Intelligent Tutoring Systems (ITSs), AI-powered chatbots, and Extended Reality (XR). In conclusion, I argue that it is crucial for language teachers to engage in CALL teacher education and professional development to keep up with the ever-evolving technology landscape and improve their teaching effectivene
    
[^43]: 超级自动化——IT行业自动化的下一个外围设备

    Hyper-automation-The next peripheral for automation in IT industries. (arXiv:2305.11896v1 [cs.HC])

    [http://arxiv.org/abs/2305.11896](http://arxiv.org/abs/2305.11896)

    本文简要介绍了超级自动化在IT行业的重要性。通过将AI工具与RPA相结合，超级自动化为几乎所有由业务用户执行的重复操作提供自动化。同时，本文还讨论了如何利用脑机接口和传感器来提高超级自动化的效率和自动化流程部署的质量。

    

    将传统业务流程自动化扩展到特定流程之外的行为被称为超级自动化。超级自动化通过将AI工具与RPA相结合，为几乎所有由业务用户执行的重复操作提供自动化。它自动化了一家公司的顶尖人才可能无法完成的复杂IT业务流程部署，实现了标准业务流程部署的端到端自动化。它通过将脑机接口（BCI）与AI和RPA自动化工具相结合来实现任务数字化。BCI与自动化工具结合会将自动化流程的检测和生成提升到一个新的水平。它使企业能够结合业务智能系统，解决复杂的需求，并提高人员的专业能力和自动化体验。本文简要讨论了超级自动化及其在当今环境中的重要性。接下来讨论了BCI和传感器如何帮助超级自动化并增强IT行业部署的自动化流程。

    The extension of legacy business process automation beyond the bounds of specific processes is known as hyperautomation. Hyperautomation provides automation for nearly any repetitive action performed by business users by combining AI tools with RPA. It automates complex IT business processes that a company's top brains might not be able to complete. This is an end-to-end automation of a standard business process deployment. It enables automation to perform task digitalization by combining a brain computer interface (BCI) with AI and RPA automation tools. BCI, in conjunction with automation tools, will advance the detection and generation of automation processes to the next level. It allows enterprises to combine business intelligence systems, address complex requirements, and enhance human expertise and automation experience. Hyperautomation and its importance in today's environment are briefly discussed in this paper. The article then goes on to discuss how BCI and sensors might aid H
    
[^44]: 军事人机团队中有意义的人类控制的设计

    Designing for Meaningful Human Control in Military Human-Machine Teams. (arXiv:2305.11892v1 [cs.HC])

    [http://arxiv.org/abs/2305.11892](http://arxiv.org/abs/2305.11892)

    本论文从军事人机团队的角度提出了有意义的人类控制（MHC）对于防御技术的重要性，并提出了宏观设计选项团队设计模式。同时，提供了一个搜索与搜救任务的案例研究。

    

    我们提出了分析、设计和评估有意义的人类控制（MHC）的方法，从军事人机团队（HMT）的角度来审视防御技术。我们的方法基于三个原则。首先，MHC应被视为指导所有分析、设计和评估阶段的核心目标。其次，MHC影响到社会技术系统的所有部分，包括人类、机器、人工智能、互动和背景。最后，MHC应被视为一个跨越较长时间的属性，包括多个参与者的事先和实时控制。为了描述实现MHC的宏观设计选项，我们提出了各种团队设计模式。此外，我们提出了一个案例研究，在其中应用了这些方法来构想一种涉及机器人和士兵在军事背景下进行搜救任务的HMT。

    We propose methods for analysis, design, and evaluation of Meaningful Human Control (MHC) for defense technologies from the perspective of military human-machine teaming (HMT). Our approach is based on three principles. Firstly, MHC should be regarded as a core objective that guides all phases of analysis, design and evaluation. Secondly, MHC affects all parts of the socio-technical system, including humans, machines, AI, interactions, and context. Lastly, MHC should be viewed as a property that spans longer periods of time, encompassing both prior and realtime control by multiple actors. To describe macrolevel design options for achieving MHC, we propose various Team Design Patterns. Furthermore, we present a case study, where we applied some of these methods to envision HMT, involving robots and soldiers in a search and rescue task in a military context.
    
[^45]: 探索使用ChatGPT分析现有分类法中的学生团队合作反馈的有效性

    Exploring the Efficacy of ChatGPT in Analyzing Student Teamwork Feedback with an Existing Taxonomy. (arXiv:2305.11882v1 [cs.HC])

    [http://arxiv.org/abs/2305.11882](http://arxiv.org/abs/2305.11882)

    本研究探索了使用ChatGPT分析学生团队合作反馈的有效性，结果表明ChatGPT可以实现90％以上的标签精度，为分析团队项目的反馈提供了一个可能有价值的工具。

    

    团队合作是许多学术和专业环境中至关重要的组成部分。在这些情境下，团队成员之间的反馈是促进成功和可持续团队协作的重要元素。然而，在课堂上，随着团队数量和成员数量以及评估频率的增加，评论的数量可能会令教师无法阅读和跟踪，从而难以识别模式和学生改进的领域。为了解决这个挑战，我们探索了使用生成AI模型，具体来说是ChatGPT，分析团队学习环境下学生的评论。我们的研究旨在评估ChatGPT根据由积极和消极评论组成的现有框架，准确识别学生评论的能力。我们的结果表明，ChatGPT可以实现90％以上的标签精度，为分析团队项目的反馈提供了一个可能有价值的工具。本研究对学术界和实践界的团队项目反馈分析做出了贡献。

    Teamwork is a critical component of many academic and professional settings. In those contexts, feedback between team members is an important element to facilitate successful and sustainable teamwork. However, in the classroom, as the number of teams and team members and frequency of evaluation increase, the volume of comments can become overwhelming for an instructor to read and track, making it difficult to identify patterns and areas for student improvement. To address this challenge, we explored the use of generative AI models, specifically ChatGPT, to analyze student comments in team based learning contexts. Our study aimed to evaluate ChatGPT's ability to accurately identify topics in student comments based on an existing framework consisting of positive and negative comments. Our results suggest that ChatGPT can achieve over 90\% accuracy in labeling student comments, providing a potentially valuable tool for analyzing feedback in team projects. This study contributes to the gro
    
[^46]: 人工智能中用户信任话语的挑战与趋势。

    Challenges and Trends in User Trust Discourse in AI. (arXiv:2305.11876v1 [cs.HC])

    [http://arxiv.org/abs/2305.11876](http://arxiv.org/abs/2305.11876)

    本文阐述在人工智能话语中用户信任面临的挑战和趋势，呼吁澄清概念以避免可能的信任差距和误解现象。

    

    1990年的互联网革命和数据驱动的信息革命改变了我们所知道的世界。现在，曾经被视为科幻想法（即机器统治世界）的事情被认为是可能的。这场革命也引发了对新的监管实践的需求，其中用户信任和人工智能（AI）话语发挥了核心作用。本研究旨在澄清关于人工智能话语中用户信任的一些误解，反对倾向于设计容易引起信任破裂的交互的趋势，包括真实和感知上的。研究结果表明，人们对于用户信任理解不清晰，并已经影响到计算机科学，尤其是在测量用户信任特征方面。研究呼吁澄清这些概念，以避免在人工智能采用与适用中出现潜在的信任差距和误解现象。

    The Internet revolution in 1990, followed by the data-driven and information revolution, has transformed the world as we know it. Nowadays, what seam to be 10 to 20 years ago, a science fiction idea (i.e., machines dominating the world) is seen as possible. This revolution also brought a need for new regulatory practices where user trust and artificial Intelligence (AI) discourse has a central role. This work aims to clarify some misconceptions about user trust in AI discourse and fight the tendency to design vulnerable interactions that lead to further breaches of trust, both real and perceived. Findings illustrate the lack of clarity in understanding user trust and its effects on computer science, especially in measuring user trust characteristics. It argues for clarifying those notions to avoid possible trust gaps and misinterpretations in AI adoption and appropriation.
    
[^47]: 由生成式AI合作创作的研究的评价：实验证据

    Judgments of research co-created by generative AI: experimental evidence. (arXiv:2305.11873v1 [cs.HC])

    [http://arxiv.org/abs/2305.11873](http://arxiv.org/abs/2305.11873)

    本研究探讨了把研究中部分内容交由生成式AI完成，会导致人们不信任和贬低研究人员和科学输出，并可能影响到生成式AI使用的报告问题。

    

    ChatGPT的引入引发了关于生成式AI（大型语言模型；LLMs）的使用的公共争论，包括研究人员的使用。在本研究中，我们测试了将研究过程的某些部分委托给LLMs是否会导致人们不信任和贬低研究人员和科学输出。参与者（N=402）考虑一个将研究过程的元素委托给博士生或LLM的研究人员，并对以下进行评分：（1）道德可接受性，（2）相信科学家监督未来项目，以及（3）输出的准确性和质量。人们认为将任务委托给LLMs比委托给人类不太可接受（d=-0.78）。委托给LLMs也会降低人们对于监督未来研究项目的信任（d=-0.80），而且人们认为结果的准确性和质量会更低（d=-0.85）。我们讨论了这种贬值如何转化为LLMs使用的低估问题。

    The introduction of ChatGPT has fuelled a public debate on the use of generative AI (large language models; LLMs), including its use by researchers. In the current work, we test whether delegating parts of the research process to LLMs leads people to distrust and devalue researchers and scientific output. Participants (N=402) considered a researcher who delegates elements of the research process to a PhD student or LLM, and rated (1) moral acceptability, (2) trust in the scientist to oversee future projects, and (3) the accuracy and quality of the output. People judged delegating to an LLM as less acceptable than delegating to a human (d = -0.78). Delegation to an LLM also decreased trust to oversee future research projects (d = -0.80), and people thought the results would be less accurate and of lower quality (d = -0.85). We discuss how this devaluation might transfer into the underreporting of generative AI use.
    
[^48]: 基于fMRI的语言编码模型的规模定律研究

    Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])

    [http://arxiv.org/abs/2305.11863](http://arxiv.org/abs/2305.11863)

    本文揭示了基于fMRI的语言编码模型预测性能与模型大小呈对数线性关系，在125M到30B参数模型进行规模扩展时，表现提高了约15％。

    

    基于变压器的单向语言模型的表示已被证明能够有效地预测大脑对自然语言的反应。然而，大多数比较语言模型与大脑的研究都使用了类似GPT-2大小的语言模型。本研究测试了是否更大的开源模型（如OPT和LLaMA系列）更适用于预测使用fMRI记录的大脑反应。结果显示，在从125M到30B参数模型进行规模扩展时，大脑预测性能与模型大小呈对数线性关系，跨3个受试者的保留测试集相关性表现提高了约15％。当扩展fMRI训练集的大小时，我们也观察到了类似的对数线性行为。我们还对使用HuBERT，WavLM和Whisper的声学编码模型进行了规模定律研究，发现模型大小的增加带来了类似的改进。我们还使用噪音天花板分析了这些大规模且高性能的编码模型。

    Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
    
[^49]: LLM在医学系统综述中的潜在用途和风险评估

    Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])

    [http://arxiv.org/abs/2305.11828](http://arxiv.org/abs/2305.11828)

    本文研究了使用LLM协助制作医学证据综述的潜在用途和风险，指出LLM有可能自动生成文献综述，但由于可能出现虚构或遗漏信息的情况，LLM的使用需要谨慎。

    

    医学系统综述对于制定临床决策和医疗政策至关重要。但是制作这样的综述很费力且耗时。因此，很多问题缺乏高质量的证据综述，即使这些综述可用，在审查过程中可能已经过时。现在，大型语言模型（LLM）已经能够生成长篇文本，这意味着自动生成文献综述的诱人可能性。然而，由于虚构或遗漏重要信息，LLM有时会产生不准确（甚至可能具有误导性）的文本。在医疗保健环境中，这可能使LLM在最好情况下无法使用，在最坏情况下会带来危险。对于LLM的益处和风险的大多数讨论与具体应用脱离了关系。在这项工作中，我们试图定性描述LLM在协助制作医学证据综述方面的潜在用途和风险。我们对16位国际专家进行了半结构化访谈。

    Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
    
[^50]: 大型语言模型中的内部一致性问题研究：通过辩论进行深入分析

    Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])

    [http://arxiv.org/abs/2305.11595](http://arxiv.org/abs/2305.11595)

    本文提出了通过辩论探究大型语言模型之间的内部一致性问题，实验证明通过严格的辩论框架可以提高模型性能和常识知识的结构化学习。

    

    大型语言模型LLMs在各种自然语言处理NLP任务中展现出了惊人的零样本或少量样本通识推理性能。然而，尽管它们拥有强大的常识推理能力，但它们仍然存在各种不一致问题。本研究提出探索两个或多个LLMs之间的内部一致性问题，这对于不同和精确的决策过程至关重要。通过严格的辩论框架，在7个常识推理数据集上进行了广泛的实验。LLMs不仅通过妥协和反驳变得更具内部一致性，而且还实现了更高的性能和常识知识的结构化学习。

    Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
    
[^51]: RAMiT：轻量级图像恢复的互惠式注意力混合Transformer

    RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])

    [http://arxiv.org/abs/2305.11474](http://arxiv.org/abs/2305.11474)

    本文提出了轻量级图像恢复的互惠式注意力混合Transformer（RAMiT）。通过使用双向注意力以及一种新的数据增强类型——强度掩码，有效地提高了恢复效果，同时大大减少了参数数量。

    

    尽管近年来许多工作在图像恢复（IR）领域取得了进展，但它们往往面临参数过多的问题。另一个问题是，大多数基于Transformer的IR方法只依靠本地或全局特征，导致接受域有限或存在参数不足的问题。为了解决这些问题，我们提出了一种轻量级IR网络：互惠注意力混合Transformer（RAMiT）。它采用我们提出的维度互惠注意力混合Transformer（D-RAMiT）块，在使用不同数量的多头并行计算双向（空间和通道）自注意力的情况下。双向关注帮助彼此弥补对方的缺点，然后混合。此外，我们引入了一种分层互惠注意力混合（H-RAMi）层，它补偿像素级信息丢失并利用语义信息，同时保持高效的分层结构。此外，在IR任务中，我们重新审视了数据增强策略并提出了一种新的数据增强类型——强度掩码，以提高所提出模型的鲁棒性。广泛的实验表明，在各种IR任务中，包括图像去噪、图像去模糊和JPEG图像去块，我们的所提出的方法在大大减少参数的情况下，优于现有最先进的方法。

    Although many recent works have made advancements in the image restoration (IR) field, they often suffer from an excessive number of parameters. Another issue is that most Transformer-based IR methods focus only on either local or global features, leading to limited receptive fields or deficient parameter issues. To address these problems, we propose a lightweight IR network, Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which compute bi-dimensional (spatial and channel) self-attentions in parallel with different numbers of multi-heads. The bi-dimensional attentions help each other to complement their counterpart's drawbacks and are then mixed. Additionally, we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that compensates for pixel-level information losses and utilizes semantic information while maintaining an efficient hierarchical structure. Furthermore, we revisit 
    
[^52]: SimOAP：通过过采样和后评估提高角色扮演对话生成中的连贯性和一致性

    SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation. (arXiv:2305.11130v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.11130](http://arxiv.org/abs/2305.11130)

    本文提出了一种简单而有效的两阶段SimOAP策略，通过过采样和后评估来提高角色扮演对话生成中的连贯性和一致性，并在三个公共角色扮演对话基准上取得了最先进的结果。

    

    在大规模语料库上训练的语言模型在开放域对话中可以生成非常流畅的结果。然而，在以角色为基础的对话生成任务中，连贯性和一致性也是关键因素，这对语言模型来说是巨大的挑战。现有的工作主要集中在有价值数据过滤、模型结构修改或目标函数设计上，但它们的改进有限，并且很难推广到所有类型的预训练语言模型。然而，我们发现，如果我们考虑足够多的生成，语言模型可以产生连贯和一致的响应。因此，问题在于大规模响应生成和目标响应选择。本文提出了一种简单但有效的两阶段SimOAP策略，即过采样和后评估。过采样阶段通过现有训练模型的现成蒸馏和压缩方法高效地获取大规模响应，后评估阶段在过采样生成的候选响应中选择一个好的响应，并提高最终响应的连贯性和一致性。该方法在三个公共角色扮演对话基准上取得了最先进的结果。

    Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a goo
    
[^53]: 生成的预训练变形器：启用技术、潜在应用、新兴挑战和未来方向的综述

    Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])

    [http://arxiv.org/abs/2305.10435](http://arxiv.org/abs/2305.10435)

    生成的预训练变形器是一种基于变形器架构的深度神经网络，能够在自然语言处理任务中表现出色且有效地进行对话，具有广泛的潜在应用，但仍面临新兴挑战和局限性。

    

    生成的预训练变形器模型代表了自然语言处理领域的一项重大突破，将我们推向开发能够像人类一样理解和使用语言进行交流的机器。生成的预训练变形器模型基于变形器架构，这是一种专门设计用于自然语言处理任务的深度神经网络。由于在自然语言处理任务上表现出色且能够有效地进行对话，生成的预训练变形器模型在研究人员和工业界社区中获得了显著的知名度，成为自然语言处理及相关领域中最广泛使用和有效的模型之一，这促使进行了本综述。本综述详细介绍了生成预训练变形器，包括其架构、工作过程、训练过程、启用技术以及在各个领域的潜在应用。同时，本综述还讨论了该模型面临的新兴挑战和局限性，并提供了未来研究的可能方向。

    The Generative Pre-trained Transformer models represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. Generative Pre-trained Transformer models are based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, Generative Pre-trained Transformer models have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, an
    
[^54]: 深度学习中考虑表格数据数据增强的新思路

    Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])

    [http://arxiv.org/abs/2305.10308](http://arxiv.org/abs/2305.10308)

    本研究提出了一种新的表格数据增强方法“随机连续嵌入”（Random Continuous Embedding，RCE），能够提高 Transformer-based 预训练模型的自监督学习性能，大幅优于现有方法，并使得自监督学习模型能够在监督表格学习中优于树形方法。

    

    表格数据是机器学习中最广泛使用的数据格式。虽然在有监督学习中，树形方法优于深度学习方法；但最近的文献报告称，Transformer-based 预训练模型的自监督学习优于树形方法。在关于表格数据的自监督学习的现有文献中，对比学习是主导方法。然而，由于表格数据的独特结构和高复杂性，表格数据的数据增强一直是困难的。此外，现有方法将模型结构、自监督学习方法和数据增强三个主要组成部分一起提出。因此，以往的研究在综合考虑这些组成部分的情况下进行对比，每个组成部分对实际性能的影响还不清楚。本研究关注数据增强，以解决这些限制。具体地，我们提出了一种新的数据增强方法“随机连续嵌入”（RCE），通过向连续变量注入噪声来生成增强的表格数据。我们在几个基准数据集上评估了我们的方法，并表明 RCE 在使用 Transformer-based 模型进行自监督学习时一致优于现有的数据增强方法。我们还进行筛选研究以显示 RCE 的有效性，并证明 RCE 使 Transformer-based 模型的自监督学习可在监督表格学习中优于树形方法。

    Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.  In this study, we focus on data augmentation to address these 
    
[^55]: UniEX：一种基于跨度提取的统一信息抽取的有效高效框架

    UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])

    [http://arxiv.org/abs/2305.10306](http://arxiv.org/abs/2305.10306)

    UniEX是一种能适用于各种模式格式的信息抽取框架，并能同时解决命名实体识别、关系抽取、事件提取和情感分析等任务，在性能和推理速度上优于其他通用信息抽取模型。

    

    我们提出了一种新的通用信息抽取范式，它与任何模式格式兼容，并适用于一系列信息抽取任务，如命名实体识别、关系抽取、事件提取和情感分析。我们的方法将以文本为基础的信息抽取任务转化为 token-pair 问题，使用一种统一的提取框架 UniEX，将所有提取目标都统一分解为联合跨度检测、分类和关联问题。UniEX 可以同时编码基于模式的提示和文本信息，并使用自动编码器语言模型协同学习预定义信息的广义知识。我们开发了 traffine 注意机制，将包括任务、标签和内部 token 在内的异构因素集成起来，并通过评分矩阵获得提取目标。实验结果表明，UniEX 在 $14$个基准测试数据集上的表现和推理速度都优于基于生成的通用信息抽取模型。

    We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
    
[^56]: MemoryBank: 用长期记忆增强大型语言模型

    MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])

    [http://arxiv.org/abs/2305.10250](http://arxiv.org/abs/2305.10250)

    MemoryBank 提出了一种新型内存机制，旨在为大型语言模型提供类人的长期记忆。它可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。

    

    大型语言模型的革命性进展极大地改变了我们与人工智能系统的互动方式。尽管如此，其中一个明显的不足之处是这些模型缺乏长期记忆机制。这在需要持续互动的情况下尤为明显，例如个人伴侣系统和心理咨询。因此，我们提出了MemoryBank，这是一种专为LLM量身定制的新型内存机制。MemoryBank可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。为了模仿人类行为并有选择地保存记忆，MemoryBank采用了受Ebbinghaus遗忘曲线理论启发的记忆更新机制，这样人工智能可以根据时间和记忆的相对重要性来遗忘和加强记忆，从而为LLM提供类似于人类的长期记忆。

    Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
    
[^57]: 交叉门控多层感知机下的蛋白质复合物不变嵌入是一种一次性抗体设计器

    Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09480](http://arxiv.org/abs/2305.09480)

    本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。

    

    抗体是由免疫系统产生的针对外来物质或抗原的重要蛋白质。抗体的特异性由其互补决定区（CDR）决定，CDR位于抗体链的可变区域中，形成与抗原结合的位点。以往的研究利用复杂的技术生成CDR，但它们遭受了几何建模不足的问题。此外，常见的迭代精化策略导致了低效的推断。本文提出了一种深度生成模型，可以一次性地共同设计CDR的1D序列和3D结构。为了实现这一目标，我们将抗体CDR设计分为两个阶段：（i）蛋白质结构的几何建模和（ii）序列结构共学习。我们开发了一种蛋白质复合物不变嵌入，可捕捉蛋白质骨架原子（包括Cα、N、C和O原子）之间的内部和外部组分相互作用，以实现全面的几何建模。

    Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
    
[^58]: 见证就是信仰：脑启发模块化训练促进机理诠释

    Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability. (arXiv:2305.08746v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.08746](http://arxiv.org/abs/2305.08746)

    BIMT方法使得神经网络更加模块化和可诠释，并且能够直接展示模块化结构，为许多简单任务提供了有用的信息，并可以补充当前的机理解释策略。

    

    我们提出了一种名为脑启发模块化训练（Brain-Inspired Modular Training, BIMT）的方法，旨在使神经网络更加模块化和可诠释。BIMT从大脑受启发，将神经元嵌入到几何空间中，并通过成本与神经元连接长度成正比的方式增强损失函数。我们证明了BIMT可以为许多简单任务发现有用的模块化神经网络，揭示了符号公式中的组合结构、可解释的决策边界和分类特征，以及算法数据集中的数学结构。直接眼睛看到模块的能力可以补充当前的机理解释策略，例如探针，干预或凝视所有权重。

    We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.
    
[^59]: 关于有限轨迹上综合策略的策略研究

    On Strategies in Synthesis Over Finite Traces. (arXiv:2305.08319v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2305.08319](http://arxiv.org/abs/2305.08319)

    本研究显示，相较于终止自动机，在LTLf模型检查中使用非终止自动机会更加困难。

    

    该论文研究了LTLf（线性时序逻辑中的有限轨迹）的综合方法运用于验证策略的正确性的有效性。然而LTLf模型检查并不简单，由LTLf综合生成的策略可以用到有限但无界长度或无限长度的终止和非终止自动机中。本研究阐述了非终止自动机与终止自动机在模型检查中的区别，论文主要贡献是显示出LTLf模型检查中使用非终止自动机比终止自动机指数级更难。

    The innovations in reactive synthesis from {\em Linear Temporal Logics over finite traces} (LTLf) will be amplified by the ability to verify the correctness of the strategies generated by LTLf synthesis tools. This motivates our work on {\em LTLf model checking}. LTLf model checking, however, is not straightforward. The strategies generated by LTLf synthesis may be represented using {\em terminating} transducers or {\em non-terminating} transducers where executions are of finite-but-unbounded length or infinite length, respectively. For synthesis, there is no evidence that one type of transducer is better than the other since they both demonstrate the same complexity and similar algorithms.  In this work, we show that for model checking, the two types of transducers are fundamentally different. Our central result is that LTLf model checking of non-terminating transducers is \emph{exponentially harder} than that of terminating transducers. We show that the problems are EXPSPACE-complete
    
[^60]: 区分先于回答：生成对比解释作为常识问答的知识

    Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])

    [http://arxiv.org/abs/2305.08135](http://arxiv.org/abs/2305.08135)

    提出了一种基于概念中心提示的对比解释生成模型CPACE，旨在将获得的符号知识转化为对比解释，用于更好地区分常识问答中的差异。

    

    现有的知识增强方法通过从不同的知识库获取不同的知识，在某些问答任务中取得了显著的成果。然而，受到检索知识的特性限制，它们仍然难以同时从知识相关性和区分性方面受益。为了解决这个挑战，我们提出了CPACE，一种基于概念中心提示的对比解释生成模型，旨在将获得的符号知识转化为对比解释，用于更好地区分给定候选者之间的差异。

    Existing knowledge-enhanced methods have achieved remarkable results in certain QA tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into a contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledg
    
[^61]: 理解异构数据联邦学习中的模型平均

    Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])

    [http://arxiv.org/abs/2305.07845](http://arxiv.org/abs/2305.07845)

    本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    

    模型平均是联邦学习中广泛采用的一种技术，它会聚集训练于异构数据上的多个客户端模型以获得表现良好的全局模型。然而，其成功背后的原理尚不是很清楚。本文通过可视化损失/错误景观来研究模型平均的几何特性，揭示了客户端模型环绕全局模型在一个共同的盆地内，并且即使全局模型表现优异，也可能偏离盆地底部。进一步的分析表明，全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
    
[^62]: 生成式人工智能：教育的影响和应用

    Generative AI: Implications and Applications for Education. (arXiv:2305.07605v1 [cs.CY])

    [http://arxiv.org/abs/2305.07605](http://arxiv.org/abs/2305.07605)

    本文研究了一种生成式人工智能技术——基于大型语言模型的聊天机器人（C-LLM），以及它在复杂学生作品的人工智能评估和审查方面的应用，同时探讨了这种技术在教育中的应用范围。

    

    2022年11月ChatGPT的推出引发了一些教育工作者的恐慌，同时也引发了其他人的热情。在生成式人工智能这个大伞下，ChatGPT是一系列技术的例子，用于提供计算机生成的文本、图像和其他数字化媒体。本文研究了生成式人工智能技术之一——来自大型语言模型的聊天机器人(C-LLM)，以及其在复杂学生作品的人工智能评估和审查方面的应用。在讨论中，本文探讨了生成式人工智能的内在限制，即绑定于语料库及其通过二进制表示的文本表示。在这些限制之内，我们提出了生成式人工智能在教育中新兴和潜在的应用范围。

    The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.
    
[^63]: 时间网络创建游戏

    Temporal Network Creation Games. (arXiv:2305.07494v1 [cs.GT])

    [http://arxiv.org/abs/2305.07494](http://arxiv.org/abs/2305.07494)

    本研究结合了时间图和博弈论网络形成，旨在解决时间路径保证所有对之间的可达性的算法问题。

    

    大多数网络不是静态的对象，而是随时间变化的。这个观察结果在最近几年内引发了对时间图的严格研究。在时间图中，我们有一组固定的节点，它们之间的连通性只在某些时间步骤上可用。这引发了大量算法问题，其中最为突出的是寻找时间张量问题，即通过时间路径保证所有对之间的可达性的计算。据我们所知，目前仅知道用于解决这个问题的集中方法。然而，许多现实世界中的网络并不是由中央设计师塑造的，而是通过许多战略代理人的互动而产生和发展的。这个观察结果是最近对博弈论网络形成模型进行密集研究的推动力量。在本工作中，我们将这两个近期的研究方向结合在一起：时间图和博弈论网络形成。

    Most networks are not static objects, but instead they change over time. This observation has sparked rigorous research on temporal graphs within the last years. In temporal graphs, we have a fixed set of nodes and the connections between them are only available at certain time steps. This gives rise to a plethora of algorithmic problems on such graphs, most prominently the problem of finding temporal spanners, i.e., the computation of subgraphs that guarantee all pairs reachability via temporal paths. To the best of our knowledge, only centralized approaches for the solution of this problem are known. However, many real-world networks are not shaped by a central designer but instead they emerge and evolve by the interaction of many strategic agents. This observation is the driving force of the recent intensive research on game-theoretic network formation models.  In this work we bring together these two recent research directions: temporal graphs and game-theoretic network formation. 
    
[^64]: 深度视觉和遗传生物测定用于少量图像数据珍稀物种分类

    Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])

    [http://arxiv.org/abs/2305.06695](http://arxiv.org/abs/2305.06695)

    本文提出了一种利用对齐的视觉-遗传推理空间来提高少量图像数据珍稀物种分类的方法，该方法通过深度嵌入模型实现对齐，适用于提高稀有物种的长尾识别，并且可以显著有益于仅基于视觉的稀有物种识别。

    

    在生物应用中，视觉和遗传生物测定通常用于识别物种和个体。然而，在计算上增强少量图像数据稀有类别的视觉分类方面，该领域尚未进行尝试。因此，本文提出了对齐的视觉-遗传推理空间，旨在隐式编码跨域关联以提高性能。我们首次证明了这种对齐可以通过深度嵌入模型实现，并且该方法直接适用于提高稀有物种的长尾识别（LTR）。我们通过应用于32个物种、超过30,000个浮游有孔虫壳的显微图像并与独立的遗传数据样本一起使用来实验室展现了该概念的效力。最重要的是，对从业者而言，我们展示了视觉-遗传对齐可以显著有益于仅基于视觉的稀有物种识别。

    Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
    
[^65]: 自主GIS：下一代基于人工智能的GIS

    Autonomous GIS: the next-generation AI-powered GIS. (arXiv:2305.06453v1 [cs.AI])

    [http://arxiv.org/abs/2305.06453](http://arxiv.org/abs/2305.06453)

    自主GIS是一种AI动力地理信息系统，采用大型语言模型作为推理核心，具有自动空间数据收集、分析和可视化的能力，旨在实现五个自主目标：自动生成、自组织、自验证、自执行和自生长。

    

    大型语言模型（LLM）如 ChatGPT ，展示了对人类自然语言的强大理解能力，在推理、创造性写作、代码生成、翻译和信息检索等领域得到了应用与探索。我们采用LLM作为推理核心，提出了一种称之为“自主GIS”的AI动力地理信息系统（GIS），以自动空间数据收集、分析和可视化来解决空间问题。我们设想，自主GIS将需要实现五个自主目标，包括自动生成、自组织、自验证、自执行和自生长。我们引入了自主GIS的设计原则来实现这五个自主目标，从信息充分性、LLM能力和代理架构三个方面进行。我们开发了一个原型系统称为LLM-Geo ，它在Python环境中使用GPT-4 API。

    Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting LLM as the reasoning core, we propose Autonomous GIS, an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning and coding for addressing spatial problems with automatic spatial data collection, analysis and visualization. We envision that autonomous GIS will need to achieve five autonomous goals including self-generating, self-organizing, self-verifying, self-executing, and self-growing. We introduce the design principles of autonomous GIS to achieve these five autonomous goals from the aspects of information sufficiency, LLM ability, and agent architecture. We developed a prototype system called LLM-Geo using GPT-4 API in a Python environme
    
[^66]: 基于基础模型的系统设计框架

    A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])

    [http://arxiv.org/abs/2305.05352](http://arxiv.org/abs/2305.05352)

    本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    

    最近推出了大型语言模型(LLM)的聊天机器人，如ChatGPT，这引起了人们对基础模型的广泛关注。基础模型被广泛认为将成为未来人工智能系统的基石。由于基础模型处于早期阶段，基于基础模型的系统设计尚未得到系统地探索。人们对在软件架构中引入基础模型的影响知之甚少。因此，在本文中，我们提出了一个基于基础模型的系统分类法，对基础模型和基于基础模型的系统的特点进行了分类和比较。我们的分类法包括三个类别：基础模型预训练和微调、基于基础模型的系统架构设计和负责任的AI设计。这个分类法为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
    
[^67]: 从大型语言模型中提取脚本知识以进行受限语言规划

    Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])

    [http://arxiv.org/abs/2305.05252](http://arxiv.org/abs/2305.05252)

    本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。

    

    在日常生活中，人们经常通过遵循目标导向的脚本形式的逐步说明来规划自己的行动。以往的工作利用语言模型（LM）来为立体活动的抽象目标（例如，“制作蛋糕”）进行规划，但对于具有多方面约束的更具体目标（例如，“为糖尿病患者制作蛋糕”）鲜有研究。本文首次定义了受限语言规划任务。我们提出了一种过度生成并过滤的方法来改善大型语言模型（LLM）在这个任务中的表现，并利用它来提取一种新颖的受限语言规划数据集CoScript，其中包括55,000个脚本。实验证明，我们的方法显著提高了LLM在受限语言规划方面的能力，特别是在约束忠实度方面。此外，CoScript被证明对赋予较小的LM受限语言规划能力是非常有效的。

    In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
    
[^68]: X-LLM: 通过将多模态视为外语引入大型语言模型来启动高级大型语言模型

    X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])

    [http://arxiv.org/abs/2305.04160](http://arxiv.org/abs/2305.04160)

    本论文提出了一种名为X-LLM的方法，将多模态信息转换为外语并输入到大型语言模型中，从而赋予LLM多模态能力，对于LLM加入多模态信息的能力进行了探究和拓展。

    

    大型语言模型（LLM）展示了卓越的语言能力。基于高级LLM的GPT-4表现出超常的多模态能力，超越了以往的视觉语言模型。我们将这归功于与以前的多模态模型相比使用了更先进的LLM。但不幸的是，GPT-4的模型架构和训练策略是未知的。为了赋予LLM多模态能力，我们提出了X-LLM，通过使用X2L接口将多模态（图像、语音、视频）转换为外语并将其输入到大型语言模型（ChatGLM）中。具体而言，X-LLM使用X2L接口将多个冻结的单模态编码器和冻结的LLM对齐，其中“X”表示多模态，例如图像、语音和视频，“L”表示语言。X-LLM的训练由三个阶段组成：（1）转换多模态信息：第一阶段分别训练每个X2L接口与其各自的单模态编码器对齐，将多模态信息转换为外语输入到ChatGLM中。...

    Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimod
    
[^69]: GPT用于半自动化数据科学：引入CAAFE实现上下文感知自动特征工程

    GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])

    [http://arxiv.org/abs/2305.03403](http://arxiv.org/abs/2305.03403)

    介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。

    

    随着自动化机器学习（AutoML）领域的发展，将领域知识纳入这些系统中变得越来越重要。我们利用大型语言模型（LLMs）的强大功能提出了一种方法来实现这一目标。具体地，我们介绍了一种用于表格数据的特征工程方法，名为上下文感知自动特征工程（CAAFE），它利用LLM根据数据集的描述生成更多具有语义意义的特征。该方法产生用于创建新特征的Python代码，并提供生成特征的效用说明。尽管方法论上很简单，但CAAFE提高了14个数据集中11个数据集的性能，与2个数据集并列，只有1个数据集性能下降，从而使所有数据集的平均ROC AUC表现从0.798提升至0.822。对于所评估的数据集，这一改进与使用随机森林（AUC 0.782）代替逻辑回归（AUC 0.754）所获得的平均改进相似。此外，

    As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
    
[^70]: 改进句子嵌入的对比学习方法，利用人工智能反馈

    Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])

    [http://arxiv.org/abs/2305.01918](http://arxiv.org/abs/2305.01918)

    本文提出了一种利用人工智能反馈改进句子嵌入对比学习方法的方式，可以提高对比学习样本对的质量，并结合人类反馈来提供更好的监督信号。

    

    对比学习已成为自然语言处理中句子嵌入学习中的流行方法。然而，自然语言的离散性使得通过数据增强方法生成的正负样本对的质量难以保证。虽然有监督的对比学习可以通过人类反馈标签生成更准确的样本对，但仍缺乏细粒度的训练信号。本文提出了一种基于AI反馈来改进句子嵌入对比学习的方法（CLAIF），利用大型预训练语言模型 (LLMs) 的AI反馈构建带有细粒度样本相似度分数的样本对，以改进对比学习。此外，我们结合人工反馈和AI反馈为对比学习中的句子嵌入提供更好的监督信号。实验结果表明，我们的方法达到了最先进的水平。

    Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves stat
    
[^71]: 大型语言模型所表现的新兴技能是否为幻觉？

    Are Emergent Abilities of Large Language Models a Mirage?. (arXiv:2304.15004v1 [cs.AI])

    [http://arxiv.org/abs/2304.15004](http://arxiv.org/abs/2304.15004)

    研究指出大型语言模型所谓的新兴技能是研究者分析的产物，不是模型行为的基本变化。研究还展示了度量标准选择和可能研究人员的偏见，可能导致这种新兴技能的出现。

    

    最近的研究声称，大型语言模型展示了新兴技能，这些技能在更小规模的模型中不存在，但在更大规模的模型中存在。新兴技能让人感到困惑的是两方面：它们的清晰度，似乎瞬间从不存在到存在，以及它们的不可预测性，似乎在不可预见的模型规模下出现。本文提出了新兴技能的另一种解释，即对于特定任务和模型族，当分析固定的模型输出时，可以选择导致推断出新兴技能或不导致推断出新兴技能的度量标准。因此，我们的解释表明，现有的新兴技能声明是研究人员分析的产物，而不是特定任务中模型行为的基本变化。我们在一个简单的数学模型中提出了我们的解释，然后通过三种互补的方式进行了测试：我们(1)制作、测试并验证了关于报告的新兴技能的度量选择的三个预测效应；(2)展示了模型架构和训练程序的简单变化会在一个已经确定的任务中产生大的新兴能力差异；(3)展示所谓的新兴技能可以通过有意优化所选择的评估指标来实现。总的来说，我们认为目前大型语言模型中新兴能力的声明很可能并不是真实存在的，而是度量标准任意选择和可能的研究人员偏见的产物。

    Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metri
    
[^72]: 理解算法供应链中的责任问题

    Understanding accountability in algorithmic supply chains. (arXiv:2304.14749v1 [cs.CY])

    [http://arxiv.org/abs/2304.14749](http://arxiv.org/abs/2304.14749)

    本文研究了算法供应链及其对算法系统治理和责任所带来的困难影响，认为算法责任的讨论必须考虑到供应链。

    

    学术和政策上对算法责任的提议常常试图在社会技术背景下理解算法系统，认识到它们由“多方”共同制造。然而，越来越多的算法系统也是由多个参与者组成的供应链生产、部署和使用的，它们之间通过数据流联系在一起。在这种情况下，算法供应链中不同参与者之间的协作是推动系统并产生特定结果的主要因素。我们认为，算法责任的讨论必须考虑到供应链以及它们对算法系统治理和责任所带来的困难影响。因此，本文研究算法供应链，将其定位于广泛的技术和政治经济背景中，并确定了一些关键特征，应在未来的算法治理和责任研究中加以理解。

    Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by 'many hands'. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accou
    
[^73]: 可解释的神经符号概念推理

    Interpretable Neural-Symbolic Concept Reasoning. (arXiv:2304.14068v1 [cs.AI])

    [http://arxiv.org/abs/2304.14068](http://arxiv.org/abs/2304.14068)

    本文提出了第一个基于概念嵌入的可解释概念模型DCR，能够在多个数据集上实现接近最先进的准确性，相对于最先进的可解释概念模型提高了高达+25％，并产生能够解释其预测的人类可理解规则和真值度，适应性强。

    

    深度学习方法具有高度的准确性，但它们不透明的决策过程阻止了它们获得完全的人类信任。概念模型旨在通过学习一组人类可理解的概念来解决这个问题。然而，最先进的概念模型依赖于高维概念嵌入表示，缺乏明确的语义含义，因此质疑其决策过程的可解释性。为了克服这个限制，我们提出了Deep Concept Reasoner(DCR)，这是第一个基于概念嵌入的可解释概念模型。在DCR中，神经网络不直接进行任务预测，而是使用概念嵌入建立语法规则结构。然后DCR在有意义的概念真值度上执行这些规则，以不可微分的方式提供最终的可解释和语义一致的预测。我们的实验表明，DCR：(i)在多个数据集上实现接近最先进的准确性，同时相对于最先进的可解释概念模型提高了高达+25％;(ii)产生能够解释其预测的人类可理解规则和真值度;(iii)很容易适应新领域。

    Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based
    
[^74]: IDQL: 作为一种扩散策略的Actor-Critic方法的隐式Q学习。 (arXiv:2304.10573v1 [cs.LG])

    IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies. (arXiv:2304.10573v1 [cs.LG])

    [http://arxiv.org/abs/2304.10573](http://arxiv.org/abs/2304.10573)

    本文重新解释隐式Q学习(IQL)作为Actor-Critic方法，提出使用扩散行为策略和评判器权重来平衡奖励最大化和与行为策略的分歧。这个方法能够处理复杂和多峰特征的Actor问题。

    

    有效的离线RL方法需要正确处理超出分布的行为。隐式Q学习（IQL）通过仅使用数据集行动通过修改后的Bellman Backup来训练Q函数来解决此问题。但是，不清楚哪个策略实际上实现了此隐含训练的Q函数所代表的值。在本文中，我们将IQL重新解释为Actor-Critic方法，通过广义化评判目标并将其连接到行为规范化的隐式Actor来实现。这种泛化显示了引入的Actor如何平衡奖励最大化和与行为策略的分歧，具体的损失选择决定了这种权衡的性质。值得注意的是，这个Actor可以表现出复杂和多峰的特征，这表明了利用优势加权回归（AWR）中使用的条件高斯Actor的拟合问题。相反，我们建议使用来自参数化扩散行为策略的样本和由评判器计算的权重，然后将其导入。

    Effective offline RL methods require properly handling out-of-distribution actions. Implicit Q-learning (IQL) addresses this by training a Q-function using only dataset actions through a modified Bellman backup. However, it is unclear which policy actually attains the values represented by this implicitly trained Q-function. In this paper, we reinterpret IQL as an actor-critic method by generalizing the critic objective and connecting it to a behavior-regularized implicit actor. This generalization shows how the induced actor balances reward maximization and divergence from the behavior policy, with the specific loss choice determining the nature of this tradeoff. Notably, this actor can exhibit complex and multimodal characteristics, suggesting issues with the conditional Gaussian actor fit with advantage weighted regression (AWR) used in prior methods. Instead, we propose using samples from a diffusion parameterized behavior policy and weights computed from the critic to then importa
    
[^75]: 为什么要逐步思考？推理源于经验的局部性。

    Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])

    [http://arxiv.org/abs/2304.03843](http://arxiv.org/abs/2304.03843)

    本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。

    

    人类有着强大而神秘的推理能力。通过一系列纯粹的思维步骤，我们可以推理出我们无法直接得出的推论 - 尽管我们从世界上没有得到任何额外数据。同样地，大型语言模型可以通过一步步的推理，在回答问题之前生成中间步骤，从而更好地完成复杂的任务。我们使用语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。这些训练条件能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。我们使用贝叶斯网络定义的联合分布的样品对自回归变压器进行训练，但每个样品只包括其中的一部分变量。我们比较使用推理生成的变量子集与使用完整集合进行训练的方案的性能。

    Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
    
[^76]: 基于徒弟学习的面向主题的文本到图像生成器

    Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])

    [http://arxiv.org/abs/2304.00186](http://arxiv.org/abs/2304.00186)

    该论文提出了一种基于徒弟学习的面向主题的文本到图像生成器SuTI，能够通过将大量基于主题的专家模型的数据输入徒弟模型，学习并推断出新主题的最佳专家模型，从而生成高品质的自定义图像，且速度比传统方法更快。

    

    最近的文本到图像生成模型（如DreamBooth）在通过针对目标主题微调“专家模型”，生成高度自定义的图像方面取得了显著进展。然而，这个过程很昂贵，因为每个主题都必须学习一个新的专家模型。在本文中，我们提出了一个替代主题特定微调的面向主题的文本到图像生成器SuTI。给定一个新主题的少量演示，SuTI可以即时生成不同场景中主题的新版本，而无需进行任何主题特定的优化。SuTI由“徒弟学习”驱动，其中从大量基于主题的专家模型生成的数据中学习单个的徒弟模型。具体而言，我们从互联网挖掘了数百万个图像簇，每个图像簇都聚焦于一个特定的视觉主题。我们采用这些簇来训练大量专门针对不同视觉主题的专家模型。徒弟模型通过推断基于其文本描述的新主题的最佳专家模型并生成图像来学习。我们在各种基准数据集上展示了SuTI的有效性，表明它可以生成高品质的不同主题的图像，同时比基于微调的方法快得多。

    Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
    
[^77]: DAMO-StreamNet：自动驾驶中流式感知的优化

    DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving. (arXiv:2303.17144v1 [cs.CV])

    [http://arxiv.org/abs/2303.17144](http://arxiv.org/abs/2303.17144)

    DAMO-StreamNet是一个优化自动驾驶中流式感知的框架，它融合了YOLO系列的最新进展，并通过颈部结构、双分支结构、蒸馏机制和实时预测机制等关键创新点，提供了尖端的解决方案。

    

    实时感知，或者说流式感知，是自动驾驶中一个至关重要但尚未得到充分研究的方面。为了填补这一空白，我们提出了DAMO-StreamNet，它将YOLO系列的最新进展与空间和时间感知机制的全面分析相结合，提供了一个尖端的解决方案。DAMO-StreamNet的关键创新点包括：(1)一个鲁棒的颈部结构，融合了可变形卷积，增强了感受野和特征对齐能力。(2)一个双分支结构，整合了短通道语义特征和长通道时序特征，提高了运动状态预测的精度。(3)一个在logits级别上进行的蒸馏机制，对齐教师网络和学生网络的语义空间。(4)一个实时预测机制，更新支持帧的特征与当前帧，确保推理过程中的无缝流式感知。

    Real-time perception, or streaming perception, is a crucial aspect of autonomous driving that has yet to be thoroughly explored in existing research. To address this gap, we present DAMO-StreamNet, an optimized framework that combines recent advances from the YOLO series with a comprehensive analysis of spatial and temporal perception mechanisms, delivering a cutting-edge solution. The key innovations of DAMO-StreamNet are: (1) A robust neck structure incorporating deformable convolution, enhancing the receptive field and feature alignment capabilities. (2) A dual-branch structure that integrates short-path semantic features and long-path temporal features, improving motion state prediction accuracy. (3) Logits-level distillation for efficient optimization, aligning the logits of teacher and student networks in semantic space. (4) A real-time forecasting mechanism that updates support frame features with the current frame, ensuring seamless streaming perception during inference. Our ex
    
[^78]: 基于惩罚的模仿学习和跨语义生成传感器融合技术在自动驾驶中的应用

    Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])

    [http://arxiv.org/abs/2303.11888](http://arxiv.org/abs/2303.11888)

    本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。

    

    随着模式识别和计算机视觉技术的快速发展，目标检测、语义分割等任务的准确度已经超过人类。自动驾驶作为一项重要的研究方向，旨在彻底改变未来的交通和出行方式。传感器对于自动驾驶的安全性和环境感知的可行性至关重要。多传感器融合由于其多维感知和集成能力的潜力而成为当前研究的热点。本文提出了一种新的特征级多传感器融合技术，用于端到端的自动驾驶导航和模仿学习。我们的论文主要关注于激光雷达和RGB信息的融合技术。我们还提供了一种全新的基于惩罚的模仿学习方法，以加强模型遵守交通规则的能力并统一模仿学习的目标。

    With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
    
[^79]: Reflexion：具有动态记忆和自我反思的自主智能体

    Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])

    [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366)

    本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。

    

    最近决策大型语言模型（LLM）代理的发展在各种基准测试中展现出卓越的性能。然而，这些最先进的方法通常需要内部模型微调、外部模型微调或在定义的状态空间上进行策略优化。由于高质量训练数据的稀缺性或缺乏良好定义的状态空间，实现这些方法可能会具有挑战性。此外，这些代理没有人类决策过程固有的某些品质，特别是从错误中学习的能力。通过反思，人类可以通过试错过程高效地解决新的问题。在最近的研究基础上，我们提出 Reflexion，一种将动态记忆和自我反思能力赋予智能体的方法，以增强其现有的推理轨迹和任务特定的行动选择能力。为了实现完全自动化，我们介绍了一种简单而有效的方法。

    Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
    
[^80]: 纤维束形状测量信息可用于预测非成像表型

    Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes. (arXiv:2303.09124v1 [cs.CV])

    [http://arxiv.org/abs/2303.09124](http://arxiv.org/abs/2303.09124)

    本文研究了利用纤维束形状特征预测非成像表型方面的潜力，并在人类连接组计划数据集上进行了实验，表明纤维束形状特征可以显著提高非成像表型的预测性能，为传统微结构和连接特征提供了补充信息。

    

    大脑白质连接的神经影像学测量可以预测诸如人口统计学和认知测量等非成像表型的变化。本文研究了纤维束形状特征在预测非成像表型方面的潜力，包括长度、直径和伸长比等三个基本形状特征。本研究使用了传统回归方法和基于深度学习的预测方法，利用微结构、连接和形状测量的高效两阶段融合策略进行预测。实验结果表明，纤维束形状特征可以显著提高非成像表型的预测性能，并为传统微结构和连接特征提供了补充信息。

    Neuroimaging measures of the brain's white matter connections can enable the prediction of non-imaging phenotypes, such as demographic and cognitive measures. Existing works have investigated traditional microstructure and connectivity measures from diffusion MRI tractography, without considering the shape of the connections reconstructed by tractography. In this paper, we investigate the potential of fiber tract shape features for predicting non-imaging phenotypes, both individually and in combination with traditional features. We focus on three basic shape features: length, diameter, and elongation. Two different prediction methods are used, including a traditional regression method and a deep-learning-based prediction method. Experiments use an efficient two-stage fusion strategy for prediction using microstructure, connectivity, and shape measures. To reduce predictive bias due to brain size, normalized shape features are also investigated. Experimental results on the Human Connect
    
[^81]: 工人是否能够有效同意工作场所健康科技？

    Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?. (arXiv:2303.07242v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2303.07242](http://arxiv.org/abs/2303.07242)

    工作场所感知技术的应用有助于提高生产力和福祉，但如何获取工人的有效同意是需要解决的难题，本文探讨了工人面临的挑战，并提出了可能的解决方案。

    

    工作场所部署的感知技术可以收集个人活动和群体互动的详细数据，否则很难捕捉。这些技术的一个有希望的应用是它们可以帮助企业和工人优化生产力和福祉。然而，鉴于工作场所中的内在和结构性权力动态，接受含蓄的遵从以监测工作活动而不是寻求工人的有意义的同意的普遍做法引发了隐私和伦理方面的担忧。本文阐述了工人在同意工作场所健康科技方面面临的一系列挑战。利用一个虚拟案例，引导15位参与者参与了6个多利益相关方焦点小组，我们探讨了参与者同意工作场所感知技术的期望和能力。我们描绘了可能更好地支持更有意义的同意工作场所健康科技的干预措施，这些措施 drawing on critical

    Sensing technologies deployed in the workplace can collect detailed data about individual activities and group interactions that are otherwise difficult to capture. A hopeful application of these technologies is that they can help businesses and workers optimize productivity and wellbeing. However, given the inherent and structural power dynamics in the workplace, the prevalent approach of accepting tacit compliance to monitor work activities rather than seeking workers' meaningful consent raises privacy and ethical concerns. This paper unpacks a range of challenges that workers face when consenting to workplace wellbeing technologies. Using a hypothetical case to prompt reflection among six multi-stakeholder focus groups involving 15 participants, we explored participants' expectations and capacity to consent to workplace sensing technologies. We sketched possible interventions that could better support more meaningful consent to workplace wellbeing technologies by drawing on critical
    
[^82]: 分布式鲁棒强化学习的样本复杂性界限的改进

    Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. (arXiv:2303.02783v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02783](http://arxiv.org/abs/2303.02783)

    本文改进了分布式鲁棒强化学习的样本复杂度界限，提出了稳健分阶段价值学习（RPVL）算法来解决表格剧情学习环境下的不确定性问题。

    

    本文考虑了在训练环境与测试环境之间参数不匹配的情况下学习控制策略的问题。我们将其制定为一个分布式鲁棒强化学习(DR-RL)问题，其中目标是学习在不确定集中针对环境最坏的随机模型下最大化价值函数的策略。我们专注于表格剧情学习环境，在不确定集被定义在名义（训练）环境的生成模型周围的情况下，算法可以访问该环境。我们提出了稳健分阶段价值学习(RPVL)算法来解决用四种不同发散度指定的不确定集的问题: 全变分、卡方、Kullback-Leibler和Wasserstein。我们证明了我们的算法达到了 $\tilde{\mathcal{O}}(|\mathcal{S}||\mathcal{A}| H^{5})$ 样本复杂性，这比现有结果平均好了一倍的 $|\mathcal{S}|$

    We consider the problem of learning a control policy that is robust against the parameter mismatches between the training environment and testing environment. We formulate this as a distributionally robust reinforcement learning (DR-RL) problem where the objective is to learn the policy which maximizes the value function against the worst possible stochastic model of the environment in an uncertainty set. We focus on the tabular episodic learning setting where the algorithm has access to a generative model of the nominal (training) environment around which the uncertainty set is defined. We propose the Robust Phased Value Learning (RPVL) algorithm to solve this problem for the uncertainty sets specified by four different divergences: total variation, chi-square, Kullback-Leibler, and Wasserstein. We show that our algorithm achieves $\tilde{\mathcal{O}}(|\mathcal{S}||\mathcal{A}| H^{5})$ sample complexity, which is uniformly better than the existing results by a factor of $|\mathcal{S}|
    
[^83]: EvoTorch：Python中可扩展的进化计算

    EvoTorch: Scalable Evolutionary Computation in Python. (arXiv:2302.12600v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2302.12600](http://arxiv.org/abs/2302.12600)

    EvoTorch是一个Python库，为高维优化问题提供可扩展、可重用和实用的进化算法实现，支持GPU和高并行性能。

    

    进化计算是人工智能研究、强化学习、机器人、工业自动化和优化、工程设计等各个领域中的重要组成部分。考虑到现代优化问题的计算需求和维度的增加，对可扩展、可重用和实用的进化算法实现的需求不断增加。为了解决这个问题，我们介绍了 EvoTorch：一个旨在处理高维优化问题、支持 GPU 和高并行性能的进化计算库。EvoTorch 基于 PyTorch 库，并能够无缝地与之配合，因此允许用户使用众所周知的 API 定义自己的优化问题。

    Evolutionary computation is an important component within various fields such as artificial intelligence research, reinforcement learning, robotics, industrial automation and/or optimization, engineering design, etc. Considering the increasing computational demands and the dimensionalities of modern optimization problems, the requirement for scalable, re-usable, and practical evolutionary algorithm implementations has been growing. To address this requirement, we present EvoTorch: an evolutionary computation library designed to work with high-dimensional optimization problems, with GPU support and with high parallelization capabilities. EvoTorch is based on and seamlessly works with the PyTorch library, and therefore, allows the users to define their optimization problems using a well-known API.
    
[^84]: 改变很难：子群体转变的深入探究

    Change is Hard: A Closer Look at Subpopulation Shift. (arXiv:2302.12254v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12254](http://arxiv.org/abs/2302.12254)

    本文分析了子群体转变的各种机制，对20个最先进的算法在12个领域内进行了全面的基准测试，发现现有算法只能应对某些转变，进一步地，提出一种简单易行的选择标准来改善现有算法性能。

    

    机器学习模型通常在训练数据中代表性不足的子群体上表现不佳。然而，对于导致子群体转变的机制以及算法在如此不同的转变中如何进行普遍化，我们知之甚少。在这项工作中，我们对子群体转变进行了细致的分析。首先，我们提出了一个统一的框架来剖析和解释子群体中的常见转变。然后，我们在视觉、语言和医疗领域的12个真实数据集上对20个最先进的算法进行了全面的基准测试。通过训练10,000多个模型得到的结果，我们揭示了未来在这个领域取得进展的有趣观察结果。首先，现有算法仅能在某些类型的转变上提高子群体的鲁棒性，而在其他类型的转变上则不能。此外，虽然当前算法依赖于群体标注的验证数据进行模型选择，但我们发现基于最差类别准确度的简单选择标准其实非常有效。

    Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly
    
[^85]: 一站式解决方案：利用预训练 LM 进行强大的时间序列分析

    One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11939](http://arxiv.org/abs/2302.11939)

    本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。

    

    尽管预训练模型在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了巨大成功，但在通用时间序列分析领域取得的进展有限。与 NLP 和 CV 不同的是，这些领域采用统一模型即可执行不同的任务，而在每个时间序列分析任务中，专门设计的方法仍然占据主导地位，如分类、异常检测、预测和少样本学习。阻碍预训练模型发展的主要挑战是缺乏大量用于训练的数据。在本文中，我们通过利用从数十亿标记训练出来的语言或 CV 模型，来解决这一挑战，用于时间序列分析。具体而言，我们避免改变预训练语言或图像模型中残差块中的自注意力和前向传递层。这种模型被称为冻结的预训练变压器 (FPT)，通过对涉及时间序列分析的所有主要类型的任务进行微调进行评估，包括分类、异常检测、预测和少样本学习等。实验结果证明，FPT 在所有任务中都具有最先进的性能和泛化能力。

    Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
    
[^86]: 学习初始化：元学习能否提高Prompt Tuning跨任务泛化能力？

    Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08143](http://arxiv.org/abs/2302.08143)

    本文研究了元Prompt Tuning（MPT）如何帮助改善跨任务泛化能力。使用元学习可以从其他相关任务中学习初始化Prompt嵌入，我们提出并实验了代表性的元学习算法，并在大量的少样本任务中证明了MPT的有效性，特别是在分类任务中。

    

    Prompt Tuning (PT)是一种只调整每个任务的一个额外标记序列的嵌入，同时保持预训练完成的语言模型（PLM）不变，已经在少样本学习中展现出了优异的性能。尽管如此，PT已经被证明极大地依赖于很好的Prompt嵌入的初始化。本文研究了元Prompt Tuning (MPT) 来系统地探索元学习如何帮助通过从其他相关任务学习初始化Prompt嵌入来改善（如果可以）PT中的跨任务泛化能力。我们在大量的少样本任务上使用广泛的实验和分析来经验分析了一系列代表性的元学习算法，分析不同源/目标任务配置下的各种调整设置。通过广泛的实验和分析，我们证明了MPT的有效性。我们发现它特别在分类任务上的提升是显著的。对于其他类型的任务，例如问题回答，我们观察到，虽然MPT可以在大多数情况下优于PT，

    Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most case
    
[^87]: 带有外部性的固定价格数据市场的均衡和学习

    Equilibrium and Learning in Fixed-Price Data Markets with Externality. (arXiv:2302.08012v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.08012](http://arxiv.org/abs/2302.08012)

    这篇论文研究了固定价格数据市场中的买家之间的竞争策略，发现其中存在负面影响。文章揭示了买家之间的负外部性及其影响，并提出了相应的市场干预措施，实现了纯策略均衡，并保证了强福利性质。

    

    我们提出将现实世界中的数据市场建模为一个买家之间的同时移动博弈，其中卖家发布固定价格，而买家可以自由地从任何一组卖家购买。该模型的一个关键组成部分是买家通过购买具有竞争优势的数据对彼此产生的负外部性，这种现象因数据易于复制而加剧。我们考虑了两种情况。在更简单的完全信息设置中，我们确定了存在于存在买家外部性的纯策略纳什均衡的福利性质。我们证明，对于一类标准的外部性函数，以交易成本的形式的市场干预可以导致强福利保证的纯策略均衡。接着，我们考虑买家起始估价未知的更一般情况。

    We propose modeling real-world data markets, where sellers post fixed prices and buyers are free to purchase from any set of sellers, as a simultaneous-move game between the buyers. A key component of this model is the negative externality buyers induce on one another due to purchasing data with a competitive advantage, a phenomenon exacerbated by data's easy replicability. We consider two settings. In the simpler complete-information setting, where all buyers know their valuations, we characterize both the existence and welfare properties of the pure-strategy Nash equilibrium in the presence of buyer externality. While this picture is bleak without any market intervention, reinforcing the limitations of current data markets, we prove that for a standard class of externality functions, market intervention in the form of a transaction cost can lead to a pure-strategy equilibrium with strong welfare guarantees. We next consider a more general setting where buyers start with unknown valua
    
[^88]: 适用于6DoF惯性测量单元的实时姿态估计的通用端到端深度学习框架

    Generalizable End-to-End Deep Learning Frameworks for Real-Time Attitude Estimation Using 6DoF Inertial Measurement Units. (arXiv:2302.06037v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.06037](http://arxiv.org/abs/2302.06037)

    本文提出了一种端到端深度学习框架，针对使用6DoF IMU测量值的实时惯性姿态估计。我们提出的模型对于不同的运动模式、采样率和环境干扰具有很好的通用性，在七个公开数据集上取得了最先进的性能。

    

    本文提出了一种针对使用6DoF IMU测量值的实时惯性姿态估计的新型端到端深度学习框架。本文提出两种深度学习模型，将加速度计和陀螺仪读数作为输入。这些模型旨在针对不同的运动模式、采样率和环境干扰进行通用化设计。我们的模型由卷积神经网络层和双向长短期记忆组成，接着使用全向前神经网络来估计四元数。我们在七个公开数据集上评估了所提出的方法，总计超过120小时和200公里的IMU测量数据。我们的结果表明，所提出的深度学习模型优于传统滤波器，并在所有数据集上实现了最先进的性能，证明了我们方法的有效性和通用性。

    This paper presents a novel end-to-end deep learning framework for real-time inertial attitude estimation using 6DoF IMU measurements. Inertial Measurement Units are widely used in various applications, including engineering and medical sciences. However, traditional filters used for attitude estimation suffer from poor generalization over different motion patterns and environmental disturbances. To address this problem, we propose two deep learning models that incorporate accelerometer and gyroscope readings as inputs. These models are designed to be generalized to different motion patterns, sampling rates, and environmental disturbances. Our models consist of convolutional neural network layers combined with Bi-Directional Long-Short Term Memory followed by a Fully Forward Neural Network to estimate the quaternion. We evaluate the proposed method on seven publicly available datasets, totaling more than 120 hours and 200 kilometers of IMU measurements. Our results show that the propos
    
[^89]: HDFormer: 面向3D人体姿势估计的高阶有向Transformer

    HDFormer: High-order Directed Transformer for 3D Human Pose Estimation. (arXiv:2302.01825v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01825](http://arxiv.org/abs/2302.01825)

    HDFormer是一种面向3D人体姿势估计的新方法，它结合了自我注意力和高阶注意力形成了一个多阶注意力模块，解决了复杂和遮挡较重情况下姿态估计中的问题。

    

    由于人体姿态数据序列的结构性质，人体姿态估计是一项具有挑战性的任务。现有方法主要集中于身体关节之间的成对交互，这对于涉及重叠关节和姿势快速变化的场景是不足够的。为了克服这些问题，我们引入了一种新的方法，即高阶有向Transformer (HDFormer)，它利用高阶骨骼和关节关系来改进姿态估计。具体而言，HDFormer结合了自我注意力和高阶注意力，形成了一个多阶注意力模块。此模块促进了第一阶“关节$\leftrightarrow$关节”，第二阶“骨$\leftrightarrow$关节”和高阶“超骨$\leftrightarrow$关节”的相互作用，有效地解决了复杂和遮挡较重情况下的问题。此外，现代CNN技术被集成到基于Transformer的结构中，平衡了性能和效率之间的权衡。HDFormer在性能方面明显优于目前其他方法。

    Human pose estimation is a challenging task due to its structured data sequence nature. Existing methods primarily focus on pair-wise interaction of body joints, which is insufficient for scenarios involving overlapping joints and rapidly changing poses. To overcome these issues, we introduce a novel approach, the High-order Directed Transformer (HDFormer), which leverages high-order bone and joint relationships for improved pose estimation. Specifically, HDFormer incorporates both self-attention and high-order attention to formulate a multi-order attention module. This module facilitates first-order "joint$\leftrightarrow$joint", second-order "bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint" interactions, effectively addressing issues in complex and occlusion-heavy situations. In addition, modern CNN techniques are integrated into the transformer-based architecture, balancing the trade-off between performance and efficiency. HDFormer significantly outperfo
    
[^90]: 用户中心异构行动深度强化学习在无线网络的元宇宙虚拟现实中的应用

    User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks. (arXiv:2302.01471v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2302.01471](http://arxiv.org/abs/2302.01471)

    本文提出了一种用户中心异构行动深度强化学习（UCHA-DRL）算法，可以使用在无线网络的元宇宙虚拟现实中。该算法可以联合优化服务器向用户下行通信的信道访问安排和传输功率来提高用户的效用。

    

    随着各种技术的发展，元宇宙正在崛起。虚拟现实技术是元宇宙中虚拟宇宙的支撑，能够为用户带来高度沉浸式的体验。在元宇宙中，移动性备受强调，虚拟现实设备通过减少本地的计算能力来减轻重量。针对元宇宙服务器和多个虚拟现实用户的系统，本文考虑了两种情况：（i）服务器生成帧并将它们传输给用户；（ii）用户在本地生成帧，因此耗费设备能量。此外，在元宇宙中的多用户虚拟现实场景中，用户对于帧率有不同的特点和需求。因此，我们联合优化服务器向用户下行通信的信道访问安排（包括帧生成位置的决策）和传输功率来提高用户的效用。这个联合优化问题被形式化为一个用户中心异构行动深度强化学习（UCHA-DRL）问题，并采用一种新的Q网络体系结构，称为异构行动Q网络（HAQN）来求解。仿真结果表明，我们提出的UCHA-DRL算法在收敛速度、用户切换次数和用户平均帧率方面优于传统的Q学习算法和深度Q网络（DQN）算法。

    The Metaverse is emerging as maturing technologies are empowering the different facets. Virtual Reality (VR) technologies serve as the backbone of the virtual universe within the Metaverse to offer a highly immersive user experience. As mobility is emphasized in the Metaverse context, VR devices reduce their weights at the sacrifice of local computation abilities. In this paper, for a system consisting of a Metaverse server and multiple VR users, we consider two cases of (i) the server generating frames and transmitting them to users, and (ii) users generating frames locally and thus consuming device energy. Moreover, in our multi-user VR scenario for the Metaverse, users have different characteristics and demands for Frames Per Second (FPS). Then the channel access arrangement (including the decisions on frame generation location), and transmission powers for the downlink communications from the server to the users are jointly optimized to improve the utilities of users. This joint op
    
[^91]: 哪些经验可以影响机器人的行为？具有淘汰正则化的策略迭代方法研究

    Which Experiences Are Influential for Your Agent? Policy Iteration with Turn-over Dropout. (arXiv:2301.11168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11168](http://arxiv.org/abs/2301.11168)

    本文提出了一种名为PI+ToD的方法，它通过淘汰正则化来高效估算经验对RL代理性能的影响。

    

    在经验回放的强化学习（RL）中，存储在回放缓冲区中的经验会影响RL代理的性能。有关经验影响的信息对于经验清理和分析等各种目的都非常有价值。一个估计单个经验影响的方法是代理比较，但当经验数量很多时，这种方法的成本是难以承受的。在本文中，我们提出了PI+ToD作为一种有效估算经验影响的方法。 PI+ToD是一种策略迭代方法，通过利用淘汰正则化来高效估算经验的影响。我们在MuJoCo环境中的实验中展示了PI + ToD的效率。

    In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance. Information about the influence is valuable for various purposes, including experience cleansing and analysis. One method for estimating the influence of individual experiences is agent comparison, but it is prohibitively expensive when there is a large number of experiences. In this paper, we present PI+ToD as a method for efficiently estimating the influence of experiences. PI+ToD is a policy iteration that efficiently estimates the influence of experiences by utilizing turn-over dropout. We demonstrate the efficiency of PI+ToD with experiments in MuJoCo environments.
    
[^92]: 深度神经网络不安全输入计数的#DNN-Verification问题

    The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.07068](http://arxiv.org/abs/2301.07068)

    本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。

    

    深度神经网络（DNN）在需要高度安全性的关键任务中，例如自动驾驶中越来越被采用。虽然最先进的验证器可以用来检查DNN是否不安全，即是否存在至少一种不安全的输入配置，但它们的是/否输出对于其他目的（如屏蔽、模型选择或培训改进）的信息不足够详细。在本文中，我们介绍了#DNN-Verification问题，它涉及计算导致DNN违反特定安全性质的输入配置数量。我们分析了这个问题的复杂性，并提出了一种新的方法，它返回确切的违规计数。由于该问题的#P完备性，我们还提出了一种随机的近似方法，该方法提供了正确计数的可证明概率界，同时显著降低了计算要求。我们在一组安全关键基准测试上呈现了实验结果，比较了我们的方法与最先进的验证器和基于计数的启发式算法。

    Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
    
[^93]: PECAN：利用策略合奏实现上下文感知的零样本人机协同

    PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination. (arXiv:2301.06387v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.06387](http://arxiv.org/abs/2301.06387)

    本文提出了一种策略合奏的方法，以增加伙伴的多样性，结合上下文感知的方法使得智能体能够分析并识别伙伴的潜在策略基元，从而通过零样本学习更广泛的协作行为。

    

    无需人类数据即可与人类合作的零样本人机协同大有前途。目前的方法通常通过自我对弈训练智能体，以训练出能够与多种人类伙伴协同的智能体。然而这样做存在两个问题：1）有限的伙伴数量会限制训练出的智能体的协作能力；2）当前的方法只能针对众多伙伴提供一个公共的最佳答案，导致在面对新伙伴或人类时零样本协同表现不佳。为解决这些问题，本文首先提出了策略合奏的方法，以增加伙伴的多样性，然后使用一种上下文感知的方法，使得智能体能够分析并识别伙伴的潜在策略基元，从而采取不同的动作。通过这种方式，智能体将能够学习更加普遍的合作行为，与不同的伙伴协同。

    Zero-shot human-AI coordination holds the promise of collaborating with humans without human data. Prevailing methods try to train the ego agent with a population of partners via self-play. However, these methods suffer from two problems: 1) The diversity of a population with finite partners is limited, thereby limiting the capacity of the trained ego agent to collaborate with a novel human; 2) Current methods only provide a common best response for every partner in the population, which may result in poor zero-shot coordination performance with a novel partner or humans. To address these issues, we first propose the policy ensemble method to increase the diversity of partners in the population, and then develop a context-aware method enabling the ego agent to analyze and identify the partner's potential policy primitives so that it can take different actions accordingly. In this way, the ego agent is able to learn more universal cooperative behaviors for collaborating with diverse par
    
[^94]: KNIFE: 从自由文本理由中提取推理知识

    KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales. (arXiv:2212.09721v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09721](http://arxiv.org/abs/2212.09721)

    通过KNIFE，可以从自由文本理由中提取推理知识，进而在小型语言模型中提高推理能力。

    

    语言模型在许多语言推理任务中表现出色，但它们的意外错误引起了对它们的推理能力的怀疑。因此，越来越多的人对微调/提示语言模型感兴趣，这些语言模型包含任务实例和其关联的自由文本理由（FTR），这些理由解释了预测正确任务输出的正确推理过程。然而，现有的微调方法无法提高语言模型的性能，而提示需要过大（即>50B）的语言模型才能良好工作。我们提出了KNIFE，证明从FTR中可以有效地提取推理知识，将其灌输到小型（即<1B）的语言模型中，从而提高语言模型的性能。首先，KNIFE对一个师生语言模型进行微调（给定任务输入和FTR），以预测任务输出，将推理知识从FTR转移至师生隐藏状态。其次，KNIFE对一个学生语言模型进行微调（仅给定任务输入），以使其隐藏状态类似于师生模型的隐藏状态，从而提高学生模型的性能。

    Language models (LMs) have yielded impressive results on many language reasoning tasks, but their unexpected errors raise doubts about their reasoning abilities. In light of this, there is growing interest in finetuning/prompting LMs with both task instances and their associated free-text rationales (FTRs), which explain the correct reasoning process for predicting the correct task output (i.e., how to be "right for the right reasons"). However, existing finetuning methods fail to improve LM performance, while prompting needs prohibitively large (i.e., >50B) LMs to work well. We propose KNIFE, which shows that reasoning knowledge can be effectively distilled from FTRs into a small (i.e., <1B) LM and improve the LM's performance. First, KNIFE finetunes a teacher LM (given task input and FTR) to predict the task output, transferring reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE finetunes a student LM (given task input only) such that its hidden states ar
    
[^95]: 多人不完全信息博弈中的贝叶斯对手建模

    Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games. (arXiv:2212.06027v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.06027](http://arxiv.org/abs/2212.06027)

    该论文提出了一种针对多人不完全信息博弈的贝叶斯对手建模方法，在三人 Kuhn poker 中应用这种方法可以明显超过所有的代理商，包括准确的纳什均衡策略。

    

    在许多现实世界的情境中，代理商与多个对立代理商进行战略互动，对手可能采用各种策略。对于这样的情境，设计代理的标准方法是计算或逼近相关的博弈理论解，如纳什均衡，然后遵循规定的策略。然而，这样的策略忽略了对手玩游戏的任何观察，这些观察可能表明可以利用的缺点。我们提出了一种多人不完全信息博弈中的对手建模方法，通过重复交互收集对手玩游戏的观察。我们对三人 Kuhn 扑克展开了对许多真实对手和准确的纳什均衡策略的实验，结果表明我们的算法明显优于所有的代理商，包括准确的纳什均衡策略。

    In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
    
[^96]: 探究用于汉语漏字检查的字形音标信息：有效性和未来方向

    Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next. (arXiv:2212.04068v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04068](http://arxiv.org/abs/2212.04068)

    本文探究用于汉语漏字检查的字形音标信息的效果和应用方向，提出一个更具挑战性和实用性的测试设置，并公开了所有代码。

    

    虽然预训练的中文语言模型在各种自然语言处理任务中表现出色，但汉语漏字检查（CSC）仍然是一个挑战。先前的研究探讨了使用字形和音标等信息来改善区分错拼字符的能力，取得了良好的结果。然而，这些模型的泛化能力并不好理解：不清楚它们是否包含字形音标信息，以及如果包含这些信息，是否充分利用了。在本文中，我们旨在更好地了解字形音标信息在 CSC 任务中的作用，并提出改进的方向。此外，我们提出了一个新的、更具挑战性和实用的 CSC 模型泛化性测试设置。所有代码都已公开发布。

    While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability to distinguish misspelled characters, with good results. However, the generalization ability of these models is not well understood: it is unclear whether they incorporate glyph-phonetic information and, if so, whether this information is fully utilized. In this paper, we aim to better understand the role of glyph-phonetic information in the CSC task and suggest directions for improvement. Additionally, we propose a new, more challenging, and practical setting for testing the generalizability of CSC models. All code is made publicly available.
    
[^97]: 多智能体规划中的公平性

    Fairness in Multi-Agent Planning. (arXiv:2212.00506v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.00506](http://arxiv.org/abs/2212.00506)

    本论文介绍了两种基于公平性的方法来生成考虑成本的公平规划，并在多个标准多智能体规划基准测试中取得了优异的效果。研究表明，生成公平计划不需要牺牲太多的计划成本。

    

    在协作的多智能体规划中，一组智能体需要完成一组目标。无论是在目标分配之前还是直接搜索解决方案，大多数先前的作品都没有关注智能体之间目标的公平分配/完成。本文将众所周知的公平性方案应用于多智能体规划，并介绍了两种生成考虑成本的公平计划的新方法。第一种方法通过解决优化问题来对智能体进行目标预分配，然后使用该分配来解决集中式多智能体规划任务。第二种方法是一种以计划为基础的编译方法，允许同时解决目标分配和规划的联合问题，同时考虑给定的公平性方案。在多个标准多智能体规划基准测试中的实证结果表明，这些方法优于不同的基线。他们还表明，生成公平计划不需要牺牲太多的计划成本。

    In cooperative Multi-Agent Planning (MAP), a set of goals has to be achieved by a set of agents. Independently of whether they perform a pre-assignment of goals to agents or they directly search for a solution without any goal assignment, most previous works did not focus on a fair distribution/achievement of goals by agents. This paper adapts well-known fairness schemes to MAP, and introduces two novel approaches to generate cost-aware fair plans. The first one solves an optimization problem to pre-assign goals to agents, and then solves a centralized MAP task using that assignment. The second one consists of a planning-based compilation that allows solving the joint problem of goal assignment and planning while taking into account the given fairness scheme. Empirical results in several standard MAP benchmarks show that these approaches outperform different baselines. They also show that there is no need to sacrifice much plan cost to generate fair plans.
    
[^98]: 从自适应欺诈者检测探究对抗鲁棒的推荐系统

    Towards Adversarially Robust Recommendation from Adaptive Fraudster Detection. (arXiv:2211.11534v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.11534](http://arxiv.org/abs/2211.11534)

    本文提出了一种针对推荐系统的MetaC恶意攻击，并设计了一种自适应欺诈者检测模块PDR，明确考虑标签的不确定性，提高了推荐系统的鲁棒性。

    

    推荐系统在节点注入攻击下的鲁棒性备受关注。最近，提出了基于GNN的推荐系统GraphRfi，它有效减轻了注入的虚假用户的影响。但是，我们展示了GraphRfi仍然容易受到攻击，因为其欺诈者检测组件的监督性质，在实践中很难获得干净的标签。我们提出了一个强大的MetaC恶意攻击，针对GNN-based和MF-based推荐系统。根据我们从易受攻击性分析中得到的见解，我们设计了一种自适应欺诈者检测模块，明确考虑了标签不确定性。该模块可以作为不同推荐系统的插件，形成一个稳健的框架（PDR）。全面的实验表明，我们的防御方法在攻击下优于其他基准方法。总体而言，我们的工作强调了在构建欺诈者检测模块时考虑标签不确定性的重要性，并提供了改善推荐系统对节点注入攻击鲁棒性的实用解决方案。

    The robustness of recommender systems under node injection attacks has garnered significant attention. Recently, GraphRfi, a GNN-based recommender system, was proposed and shown to effectively mitigate the impact of injected fake users. However, we demonstrate that GraphRfi remains vulnerable to attacks due to the supervised nature of its fraudster detection component, where obtaining clean labels is challenging in practice. In particular, we propose a powerful poisoning attack, MetaC, against both GNN-based and MF-based recommender systems. Furthermore, we analyze why GraphRfi fails under such an attack. Then, based on our insights obtained from vulnerability analysis, we design an adaptive fraudster detection module that explicitly considers label uncertainty. This module can serve as a plug-in for different recommender systems, resulting in a robust framework named PDR. Comprehensive experiments show that our defense approach outperforms other benchmark methods under attacks. Overal
    
[^99]: [RE]VER：学习自然语言表示以阐述实体和关系

    [RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations. (arXiv:2211.11093v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11093](http://arxiv.org/abs/2211.11093)

    本文提出了一种名为[RE]VER的系统，使用基于transformer的模型来学习实体和关系的自然语言表示，能够生成一个能够表示实体与其他实体关系的句子，相比于之前的最先进方法有了显著改进。

    

    实体及实体之间的关系是现实世界中至关重要的。人们通过理解实体和关系来了解世界。本文提出了一种名为[RE]VER的系统，通过使用基于transformer的模型来学习实体和关系的自然语言表示，并生成一个能够表示实体与其他实体关系的句子。我们在多个基准数据集上评估了我们的模型，并证明其相比于之前的最先进方法有了显著改进。

    Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and relations, humans may refer to natural language descriptions. For instance, when learning a new scientific term, people usually start by reading its definition in dictionaries or encyclopedias. To know the relationship between two entities, humans tend to create a sentence to connect them. In this paper, we propose [RE]VER: A Unified Model for Verbalizing Entities and Relations. Specifically, we attempt to build a system that takes any entity or entity set as input and generates a sentence to repres
    
[^100]: GLUE-X: 从ODD普适性角度评估自然语言理解模型

    GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08073](http://arxiv.org/abs/2211.08073)

    本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs上对8个经典NLP任务进行评估。

    

    预训练语言模型（PLMs）通过利用大量的训练数据，已知可以提高自然语言理解模型的泛化性能。然而，许多NLP任务中的ODD普适性问题仍然存在，这限制了这些方法在现实世界中的部署。本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，强调OOD鲁棒性的重要性，并提供如何衡量模型的鲁棒性以及如何改善模型的见解。该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs（包括GPT-3和GPT-3.5）上对8个经典NLP任务进行评估。我们的研究结果确认了在所有设置下，与ID准确度相比，存在显着的性能下降，需要改善NLP任务中的OOD准确度。

    Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
    
[^101]: 智能网格生成的现状：综述与前景展望

    What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives. (arXiv:2211.06009v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.06009](http://arxiv.org/abs/2211.06009)

    本文综述了智能网格生成（IMG）现状，概括了113种IMG方法的核心技术、应用范围、代理学习目标、数据类型、目标挑战、优势和局限性，并提出了三个不同的分类法。

    

    智能网格生成（IMG）是一种机器学习生成网格的技术，是一个相对较新和有前途的研究领域。在短暂的历史中，IMG极大地拓展了网格生成技术的通用性和实用性，取得了许多突破，并为网格生成创造了潜在可能性。然而，关于IMG的综述研究还不够。本文致力于系统综述和描述当下IMG的研究现状。我们搜集了113种IMG方法，从多个角度（包括算法的核心技术和应用范围、代理学习目标、数据类型、目标挑战、优势和局限性）进行了深入分析。基于内容提取，我们提出了三个不同的分类法：关键技术、输出网格单元元素、所针对的问题，以进行文献收集和分类整理。

    Intelligent mesh generation (IMG) refers to a technique for generating mesh by machine learning, which is a relatively new and promising research field. Within its short lifespan, IMG has greatly expanded the generalizability and practicality of mesh generation techniques, achieved many breakthroughs and created potential possibilities for mesh generation. However, there is a lack of surveys that focus on IMG methods in recent works. In this paper, we are committed to a systematic and comprehensive survey that describes the contemporary IMG landscape. Focusing on 113 preliminary IMG methods, we conducted an in-depth analysis from multiple perspectives, including the core technique and application scope of the algorithm, agent learning goals, data types, targeting challenges, advantages, and limitations. With the aim of literature collection and classification based on content extraction, we propose three different taxonomies from three views: key techniques, output mesh unit elements, 
    
[^102]: 在线合同设计的样本复杂度

    The Sample Complexity of Online Contract Design. (arXiv:2211.05732v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2211.05732](http://arxiv.org/abs/2211.05732)

    本文解决了在线合同设计中一个悬而未决的问题，证明了指数级的$m$个样本就足以学习一个近乎最优的合同。

    

    本文研究在线情境下的隐藏-行动委托问题。在每轮中，委托人发布一份合同，根据每个结果规定代理人的支付。代理人然后做出一个最大化她自己效用的战略行动选择，但直接观察不到行动。委托人观察结果并从代理人的行动选择中获得效用。根据过去的观察，委托人动态地调整合同，目标是最大化其效用。我们引入了一种在线学习算法，并给出了其Stackelberg遗憾的上界。我们证明，在合同空间为$[0,1]^m$时，Stackelberg遗憾的上界为$\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$，下界为$\Omega(T^{1-1/(m+2)})$，其中$\widetilde O$排除对数因子。 这个结果表明，指数级的$m$个样本就足以学习一个近乎最优的合同，解决了在线合同设计中的一个悬而未决的问题。

    We study the hidden-action principal-agent problem in an online setting. In each round, the principal posts a contract that specifies the payment to the agent based on each outcome. The agent then makes a strategic choice of action that maximizes her own utility, but the action is not directly observable by the principal. The principal observes the outcome and receives utility from the agent's choice of action. Based on past observations, the principal dynamically adjusts the contracts with the goal of maximizing her utility.  We introduce an online learning algorithm and provide an upper bound on its Stackelberg regret. We show that when the contract space is $[0,1]^m$, the Stackelberg regret is upper bounded by $\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$, and lower bounded by $\Omega(T^{1-1/(m+2)})$, where $\widetilde O$ omits logarithmic factors. This result shows that exponential-in-$m$ samples are sufficient and necessary to learn a near-optimal contract, resolving an open probl
    
[^103]: 检验高级人工智能差异风险和控制问题

    Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control. (arXiv:2211.03157v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.03157](http://arxiv.org/abs/2211.03157)

    本研究提出了基于层次化复杂系统框架的模型，用于模拟风险和提供未来分析模板，深入探讨了机器学习领域的AI决策透明度和集成度的担忧，并强调了增强系统能力和自治性可能会导致权力动态的意外改变或灾难性故障。

    

    人工智能（AI）是21世纪最具变革性的技术之一。未来AI能力的程度和范围仍存在关键不确定性，各方对时间表和潜在影响有不同的看法。随着国家和技术公司竞相追求更复杂和自主的AI系统，人们担心半透明AI决策过程的集成和监督程度。这在机器学习（ML）领域尤其如此，其中系统学习优化目标而无需人类帮助。目标可能无法完美地制定或以意外或潜在有害的方式执行。随着系统增加功率和自主性，这变得更加令人担忧，一个突然的能力跃升可能导致权力动态意外转变甚至灾难性故障。本研究提出了一个层次化复杂系统框架来模拟AI风险，并提供替代未来分析的模板。

    Artificial Intelligence (AI) is one of the most transformative technologies of the 21st century. The extent and scope of future AI capabilities remain a key uncertainty, with widespread disagreement on timelines and potential impacts. As nations and technology companies race toward greater complexity and autonomy in AI systems, there are concerns over the extent of integration and oversight of opaque AI decision processes. This is especially true in the subfield of machine learning (ML), where systems learn to optimize objectives without human assistance. Objectives can be imperfectly specified or executed in an unexpected or potentially harmful way. This becomes more concerning as systems increase in power and autonomy, where an abrupt capability jump could result in unexpected shifts in power dynamics or even catastrophic failures. This study presents a hierarchical complex systems framework to model AI risk and provide a template for alternative futures analysis. Survey data were co
    
[^104]: 基于序列的计划可行性预测用于高效的任务和动作规划

    Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning. (arXiv:2211.01576v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.01576](http://arxiv.org/abs/2211.01576)

    本文提出一种基于学习的任务和动作规划（TAMP）算法，通过序列预测辅助搜索，显著提高了规划效率，运行时间缩短了80%。

    

    本文提出了一种学习可用的任务和动作规划（TAMP）算法，用于在存在多个关节和可移动障碍物的环境中解决移动操作问题。我们的思路是通过学习的计划可行性预测器对传统TAMP规划器的搜索过程进行偏置。算法的核心是PIGINet，这是一种新颖的基于Transformer的学习方法，它接收任务计划、目标和初始状态，并预测与任务计划相关联的运动轨迹的概率。我们将PIGINet集成到一个TAMP规划器中，该规划器生成一组多样化的高层任务计划，按照预测的可行性排序，并依次进行细化。我们对七种厨房重排问题的TAMP算法运行时间进行评估，将其性能与非学习基准进行比较。实验结果表明，PIGINet显着提高了规划效率，在状态较小的问题上缩短了运行时间80%。

    We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state
    
[^105]: 一种使用噪声增强语音作为目标的训练和推理策略，用于无清晰语音的语音增强

    A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech. (arXiv:2210.15368v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2210.15368](http://arxiv.org/abs/2210.15368)

    为解决语音增强领域中“无清晰语音”的挑战，提出了使用增强语音作为目标的训练和推理策略，即使在域内和域外噪声差异较大的情况下仍能有效，实验结果优于基线方法。

    

    无清晰语音是发展语音增强系统的实际挑战，这意味着它们的训练准则和评估指标之间存在不可避免的不匹配。为了应对这种不利情况，我们提出了一种训练和推理策略，其中使用增强语音作为目标来改进先前提出的噪声目标训练（NyTT）。由于域内噪声与外部噪声的同质性是NyTT有效性的关键，我们通过混音训练多个学生模型，包括：1）使用教师模型估计的语音和噪声进行增强目标训练，或者2）使用原始的噪声语音和教师模型估计的噪声进行噪声目标训练。实验结果表明，我们提出的方法优于几种基线方法，特别是在教师/学生推理方面，其中预测的清晰语音是通过教师和最终学生模型成功地推导出来的。

    The lack of clean speech is a practical challenge to the development of speech enhancement systems, which means that there is an inevitable mismatch between their training criterion and evaluation metric. In response to this unfavorable situation, we propose a training and inference strategy that additionally uses enhanced speech as a target by improving the previously proposed noisy-target training (NyTT). Because homogeneity between in-domain noise and extraneous noise is the key to the effectiveness of NyTT, we train various student models by remixing 1) the teacher model's estimated speech and noise for enhanced-target training or 2) raw noisy speech and the teacher model's estimated noise for noisy-target training. Experimental results show that our proposed method outperforms several baselines, especially with the teacher/student inference, where predicted clean speech is derived successively through the teacher and final student models.
    
[^106]: DiffusionDB: 文本到图像生成模型的大规模提示画廊数据集

    DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.14896](http://arxiv.org/abs/2210.14896)

    介绍了DiffusionDB数据集，这是一个规模庞大的文本到图像提示数据集，总计包含1400万张图像和180万个唯一提示。该数据集被用来帮助研究人员解决文本提示生成图像时所需的适当提示的问题，并指出了一些特定的提示样式和超参数值可能导致模型错误，甚至生成误导信息。

    

    随着扩散模型的最新进展，用户可以通过编写自然语言提示生成高质量图像。然而，生成具有所需细节的图像需要适当的提示，而且往往不清楚模型对不同提示的反应或最佳提示是什么。为了帮助研究人员解决这些关键挑战，我们介绍了DiffusionDB，这是第一个大规模的文本到图像提示数据集，总计6.5TB，包含使用Stable Diffusion生成的1400万张图像，180万个唯一提示和由真实用户指定的超参数。我们分析了提示的语法和语义特征，并指出了可能导致模型错误的特定超参数值和提示样式，并提供了潜在有害模型使用的证据，如生成误导信息。这个人为驱动的数据集的空前规模和多样性为了解提示和生成图像之间相互作用提供了激动人心的研究机会。

    With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generat
    
[^107]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^108]: GNM: 通用导航模型驱动任何机器人

    GNM: A General Navigation Model to Drive Any Robot. (arXiv:2210.03370v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.03370](http://arxiv.org/abs/2210.03370)

    通用目标条件模型GNM可以通过多样化的机器人数据实现更强大、更健壮的导航性能，并能驱动任何具有适当视觉感知输入的机器人。

    

    学习是视觉导航的强有力工具，但基于学习的策略的能力受到有限训练数据的限制。本文研究如何在来自多个不同但结构相似的机器人的数据基础上训练面向视觉导航的通用目标条件模型，并实现在各种环境和机身上的广泛泛化。我们分析了有效的机器人间数据共享的必要设计决策，包括使用时间上下文和标准化的动作空间，并证明了从异构数据集训练的全局策略优于任何单一数据集上训练的策略。我们收集了6个不同机器人的60小时导航轨迹，并在一系列新机器人上部署训练后的GNM，包括一个欠驱动的四旋翼飞行器。我们发现，训练多样化的数据可以提高和更加稳健的导航性能，而GNM可以驱动任何具有适当视觉感知输入的机器人。

    Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diver
    
[^109]: 分层对抗逆强化学习

    Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01969](http://arxiv.org/abs/2210.01969)

    本文提出了一种分层对抗逆强化学习算法，能够在复杂任务中学习到具有层次结构的最优策略，比现有的方法更加有效。

    

    模仿学习（IL）一般用于从演示中恢复专家策略。然而，对于高度复杂的、长时程任务，恢复单一整体策略是困难的，而专家策略通常包含子任务层次结构。因此，研究者开发了分层模仿学习（HIL）方法，通过在选项框架中显式地建模任务中的活动结构来学习分层策略。现有的HIL方法要么忽视了子任务结构与学习策略之间的因果关系，要么无法同时在分层框架中学习高级别和低级别策略，导致亚最优。本文提出了一种新的HIL算法——分层对抗逆强化学习（H-AIRL），它在最新的IL算法AIRL上扩展了一步选项框架，重新定义了AIRL目标。

    Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
    
[^110]: L2XGNN：学习解释图神经网络

    L2XGNN: Learning to Explain Graph Neural Networks. (arXiv:2209.14402v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14402](http://arxiv.org/abs/2209.14402)

    L2XGNN提出了一个框架来解释图神经网络，通过选择解释子图（模体）实现忠实的解释。该框架能够识别负责预测图属性的模体，并实现与基线方法相同的分类精度。

    

    图神经网络（GNNs）是一类流行的机器学习模型。在学习解释（L2X）范式的启发下，我们提出了L2XGNN，这是一个可解释的GNN框架，通过设计提供忠实的解释。L2XGNN学习一种机制，用于选择解释子图（模体），这些子图仅用于GNN的信息传递操作中。对模体施加这样的限制通常会导致更易解释和更有效的解释。在几种数据集上的实验表明，L2XGNN实现了与基线方法相同的分类精度，同时确保仅使用提供的解释来进行预测。此外，我们还表明L2XGNN能够识别负责预测图属性的模体。

    Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.
    
[^111]: 大语言模型中用心理学启发的思维链触发识别隐含变量和推理关系进行隐喻理解

    Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. (arXiv:2209.08141v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.08141](http://arxiv.org/abs/2209.08141)

    本文介绍了一种在大语言模型中使用心理学引导思维的方法来增强隐喻理解的性能。这种方法使用了隐含变量和关系来选择正确的释义。

    

    语言理解的概率模型是研究人们语言使用的有价值的工具，但它们需要手动设计。相比之下，大语言模型（LLMs）是用跨领域的文本进行训练的，但它们缺乏概率模型的结构和可解释性。本文采用思维链触发方式来将概率模型中的结构引入LLMs中，以隐喻理解为例来探究这一方法。我们的思维链触发方式导致语言模型推断隐含变量，并思考它们之间的关系，以选择适当的隐喻释义。所选择的隐含变量和关系都基于认知心理学中的隐喻理解理论。我们将这些提示应用于GPT-3的两个最大版本，并显示它们可以提高释义选择任务的性能。

    Probabilistic models of language understanding are valuable tools for investigating human language use. However, they need to be hand-designed for a particular domain. In contrast, large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. We explore this approach in the case of metaphor understanding. Our chain-of-thought prompts lead language models to infer latent variables and reason about their relationships in order to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve performance in a paraphrase selection task.
    
[^112]: 可微分的基于Agent的流行病模拟

    Differentiable Agent-based Epidemiology. (arXiv:2207.09714v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09714](http://arxiv.org/abs/2207.09714)

    本文介绍一种可扩展、可微分的基于Agent的流行病模拟设计——GradABM，能够在较短时间内快速模拟百万级别的人口，并与深度神经网络集成和接受异构数据源，为校准、预测和评估政策干预提供了便利。

    

    机械模拟器是流行病学探索复杂，动态感染在不同条件下并在不确定的环境中导航的不可或缺工具。 基于代理的模型（ABM）是一种越来越受欢迎的仿真范例，可以用细节表示接触交互的异质性和个体行为的代理。 然而，传统ABM框架不可微分并且在可扩展性上存在挑战；因此，将它们连接到辅助数据源是不平凡的。 在本文中，我们介绍了GradABM：一种可扩展的，可微分的基于代理模型的设计，适合于使用自动微分进行基于梯度的学习。 GradABM可以在商品硬件上快速模拟数百万规模的人口，并与深度神经网络集成和接受异构数据源。 这为校准，预测和评估政策干预提供了一系列实际的好处。

    Mechanistic simulators are an indispensable tool for epidemiology to explore the behavior of complex, dynamic infections under varying conditions and navigate uncertain environments. Agent-based models (ABMs) are an increasingly popular simulation paradigm that can represent the heterogeneity of contact interactions with granular detail and agency of individual behavior. However, conventional ABM frameworks are not differentiable and present challenges in scalability; due to which it is non-trivial to connect them to auxiliary data sources. In this paper, we introduce GradABM: a scalable, differentiable design for agent-based modeling that is amenable to gradient-based learning with automatic differentiation. GradABM can quickly simulate million-size populations in few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous data sources. This provides an array of practical benefits for calibration, forecasting, and evaluating policy interventions. We
    
[^113]: 学习深度时间索引模型用于时间序列预测

    Learning Deep Time-index Models for Time Series Forecasting. (arXiv:2207.06046v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06046](http://arxiv.org/abs/2207.06046)

    本文提出了DeepTime框架，一种用于学习深度时间索引模型的元优化框架，能够有效地预测时间序列，并在真实世界数据集上获得了与先进方法相当的结果。

    

    深度学习被广泛应用于时间序列预测中，导致历史价值模型类别中涌现出大量新方法。虽然时间索引模型具有吸引人的特性，比如能够对底层时间序列动态性建模，但仍未受到足够的关注。本文提出了DeepTime框架，这是一种元优化框架，用于学习克服这些限制的深度时间索引模型，在长序列时间序列预测设置下在真实世界数据集上展开了广泛的实验。实验结果表明，我们的方法与现有技术相比具有竞争力，并且具有高效和准确的预测模型。

    Deep learning has been actively applied to time series forecasting, leading to a deluge of new methods, belonging to the class of historical-value models. Yet, despite the attractive properties of time-index models, such as being able to model the continuous nature of underlying time series dynamics, little attention has been given to them. Indeed, while naive deep time-index models are far more expressive than the manually predefined function representations of classical time-index models, they are inadequate for forecasting, being unable to generalize to unseen time steps due to the lack of inductive bias. In this paper, we propose DeepTime, a meta-optimization framework to learn deep time-index models which overcome these limitations, yielding an efficient and accurate forecasting model. Extensive experiments on real world datasets in the long sequence time-series forecasting setting demonstrate that our approach achieves competitive results with state-of-the-art methods, and is hig
    
[^114]: 好时机: 表达需求式视觉导航中启示求助的学习框架

    Good Time to Ask: A Learning Framework for Asking for Help in Embodied Visual Navigation. (arXiv:2206.10606v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10606](http://arxiv.org/abs/2206.10606)

    该论文提出了一种学习框架，使得机器人可以主动寻求帮助以解决表达需求式视觉导航任务中存在的难题，并展示了其在缺乏反馈信息的情况下依然稳健的抗干扰能力。

    

    实际上，询问求助比在未知位置的空间内搜索要更加高效。我们提出了一个学习框架，使得代理能够在这种感知视觉任务中主动地寻求帮助，其中反馈信息告知代理目标在其视野中的位置。为了模拟现实情况下老师不总是在场的场景，我们提出了一个训练课程，在这个过程中反馈信息并不总是可用的。我们制定了一个不确定性度量目标位置，在经验结果的基础上展示了该方法的有效性，即使在缺乏反馈信息的情况下代理依然保持了抗干扰能力。

    In reality, it is often more efficient to ask for help than to search the entire space to find an object with an unknown location. We present a learning framework that enables an agent to actively ask for help in such embodied visual navigation tasks, where the feedback informs the agent of where the goal is in its view. To emulate the real-world scenario that a teacher may not always be present, we propose a training curriculum where feedback is not always available. We formulate an uncertainty measure of where the goal is and use empirical results to show that through this approach, the agent learns to ask for help effectively while remaining robust when feedback is not available.
    
[^115]: PrivHAR：从隐私保护角度识别人类活动

    PrivHAR: Recognizing Human Actions From Privacy-preserving Lens. (arXiv:2206.03891v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2206.03891](http://arxiv.org/abs/2206.03891)

    该论文提出了一种优化框架，通过参数化摄像头镜头，对视频进行处理以保护个人隐私，并在维护活动识别特征的同时进行了模拟和硬件实验的验证。

    

    数字摄像机的广泛使用引发了人们对隐私和安全的日益关注，特别是在识别人类活动的应用中。本文提出了一种优化框架，通过参数化摄像头镜头，成功地降低了视频质量，以抑制隐私属性并防止敌对攻击，同时保留了活动识别相关特征，提供强大的视觉隐私保护。我们进行了大量模拟和硬件实验来验证我们的方法。

    The accelerated use of digital cameras prompts an increasing concern about privacy and security, particularly in applications such as action recognition. In this paper, we propose an optimizing framework to provide robust visual privacy protection along the human action recognition pipeline. Our framework parameterizes the camera lens to successfully degrade the quality of the videos to inhibit privacy attributes and protect against adversarial attacks while maintaining relevant features for activity recognition. We validate our approach with extensive simulations and hardware experiments.
    
[^116]: 基于进化计算的肌电控制器的功耗设计

    Towards Power-Efficient Design of Myoelectric Controller based on Evolutionary Computation. (arXiv:2204.02179v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2204.02179](http://arxiv.org/abs/2204.02179)

    本文提出了一种基于核化SVM分类器的监督学习方法，通过约束的多目标优化问题实现肌电信号的高效控制，实验结果表明该方法在功率和分类准确性方面优于现有技术。

    

    肌电模式识别是各种应用程序（包括上肢假肢和生物机器人手部运动系统）的控制策略设计中的重要方面。本文提出了一种方法，通过考虑使用核化SVM分类器对表面肌电图（sEMG）信号的信息进行解码来推断潜在的肌肉运动，设计一种能量高效的基于EMG的控制器。为了实现EMG控制器的优化性能，我们的主要分类器设计策略是减少整个系统误动作（当EMG控制器处于“休息”位置时）。为此，与传统的软间隔核化SVM单个训练目标不同，我们将所提出的监督学习系统的训练算法制定为一般约束多目标优化问题。采用精英多目标进化算法NSGA-II来找到受限制的多目标优化问题的帕累托最优解，并根据支持向量的数量和分类准确率之间的权衡选择最终分类器。在公开可用的sEMG数据集上的实验结果表明，所提出的方法在功率效率和分类精度方面优于现有技术的方法。

    Myoelectric pattern recognition is one of the important aspects in the design of the control strategy for various applications including upper-limb prostheses and bio-robotic hand movement systems. The current work has proposed an approach to design an energy-efficient EMG-based controller by considering a supervised learning framework using a kernelized SVM classifier for decoding the information of surface electromyography (sEMG) signals to infer the underlying muscle movements. In order to achieve the optimized performance of the EMG-based controller, our main strategy of classifier design is to reduce the false movements of the overall system (when the EMG-based controller is at the `Rest' position). To this end, unlike the traditional single training objective of soft margin kernelized SVM, we have formulated the training algorithm of the proposed supervised learning system as a general constrained multi-objective optimization problem. An elitist multi-objective evolutionary algor
    
[^117]: 曲线建模下的车道检测方法重新思考

    Rethinking Efficient Lane Detection via Curve Modeling. (arXiv:2203.02431v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.02431](http://arxiv.org/abs/2203.02431)

    本文提出了一种新的参数曲线车道检测方法，在计算上更加简单且容易处理优化问题，并且在多个数据集中实现了最优的性能表现，可以作为未来车道检测领域的新基准。

    

    本论文提出了一种新的基于参数曲线的RGB图像车道检测方法。与现有的基于分割和基于点检测的方法不同，曲线方法可以自然地学习整体车道表示，而不需要解码预测或制定大量锚点的启发式方法。为了处理现有多项式曲线方法的优化困难，我们提出利用参数Bézier曲线，因为它易于计算、稳定且具有较高的自由变换度数。此外，我们还提出了一种可变形卷积的特征翻转融合方法，以利用驾驶场景中车道的对称性质。所提出的方法在流行的LLAMAS基准测试上实现了新的最优性能。它也在TuSimple和CULane数据集上实现了良好的准确性，同时保持较低的延迟（> 150 FPS）和小模型大小（<10M）。我们的方法可以作为新的基准，以在路标检测领域推动进一步的研究。

    This paper presents a novel parametric curve-based method for lane detection in RGB images. Unlike state-of-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curve-based methods can learn holistic lane representations naturally. To handle the optimization difficulties of existing polynomial curve methods, we propose to exploit the parametric B\'ezier curve due to its ease of computation, stability, and high freedom degrees of transformations. In addition, we propose the deformable convolution-based feature flip fusion, for exploiting the symmetry properties of lanes in driving scenes. The proposed method achieves a new state-of-the-art performance on the popular LLAMAS benchmark. It also achieves favorable accuracy on the TuSimple and CULane datasets, while retaining both low latency (> 150 FPS) and small model size (< 10M). Our method can serve as a new baseline, to shed 
    
[^118]: 深度判别到核生成网络的定标推断方法

    Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.13001](http://arxiv.org/abs/2201.13001)

    该论文提出了将判别网络转换为生成网络的方法，用高斯核替换多面体中的仿射函数来生成模型，解决了内部和外部数据校准问题，并在 CIFAR-10，CIFAR-100 和 SVHN 等基准数据集上测试了方法的有效性。

    

    判别与生成网络在人工智能和自然智能的研究中都有其重要性，我们提出了一种将二者相结合的方法，将深度判别网络转换为核生成网络。我们将深度模型视为广义的划分规则，并使用高斯核替换由训练数据构成的多面体中的仿射函数，来获得生成模型。实验证明了我们方法的有效性。

    The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
    
[^119]: PoNet：长序列中高效Token混合的池化网络

    PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.02442](http://arxiv.org/abs/2110.02442)

    PoNet是一种池化网络，可用于长序列中的token混合，其具有线性复杂度，并可以比Transformer更好地处理长序列。该方法提供了一种高效且有效的自注意替代方案。

    

    基于Transformer的模型在各种NLP、视觉和语音任务中取得了巨大的成功。然而，Transformer的核心自注意机制和序列长度的平方时间和内存复杂度，阻碍了Transformer-based模型在处理长序列时的应用。为了缓解这个问题，有许多方法被提出，例如稀疏注意机制、低秩矩阵近似和可扩展的核函数，以及替代自注意的token混合。我们提出了一种用于长序列中token混合的新型池化网络(PoNet)，其复杂度为线性。我们设计了多粒度池化和池化融合来捕捉不同级别的上下文信息，并将其与token的交互结合起来。在长序列基准测试中，PoNet显著优于Transformer，并实现了有竞争力的准确性，而且在所有在GPU上测量的序列长度上，它仅比最快的模型FNet稍慢。我们还能可视化PoNet的注意力图，以展示它可以有效地混合具有不同上下文信息的token。我们的方法为处理长序列提供了一种高效且有效的自注意替代方案。

    Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also c
    
[^120]: 从统计关系到神经符号人工智能：一项调查

    From Statistical Relational to Neural Symbolic Artificial Intelligence: a Survey. (arXiv:2108.11451v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2108.11451](http://arxiv.org/abs/2108.11451)

    这篇调查综述了NeSy和StarAI两种人工智能方法中，学习和推理的集成方法。共有七个维度用于对两种方法进行分类和比较。

    

    本调查探讨了两个不同人工智能领域中学习和推理的集成方法：神经符号计算（NeSy）和统计关系人工智能（StarAI）。NeSy旨在将符号推理与神经网络相结合，而StarAI则专注于将逻辑与概率图模型相结合。该调查关注了两种方法之间的七个共同维度。这些维度用于对两种领域进行分类，包括：（1）逻辑推理方法，无论是基于模型还是基于证明；（2）逻辑理论的语法；（3）系统的逻辑语义及其扩展以促进学习；（4）学习的范围，包括仅涉及参数还是涉及整个逻辑理论；（5）表示法中符号和子符号成分的存在；（6）系统捕捉原始逻辑、概率和神经范例的程度；和（7）任务类别。

    This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neural-symbolic computation (NeSy) and statistical relational artificial intelligence (StarAI). NeSy aims to integrate symbolic reasoning and neural networks while StarAI focuses on integrating logic with probabilistic graphical models. The survey brings attention to seven shared dimensions between the two approaches. These dimensions are employed to categorize both fields and include: (1) the approach to logic inference, whether model or proof-based; (2) the syntax of logical theories; (3) the logic semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either the parameters alone or the entire logic theory; (5) the presence of symbolic and subsymbolic components in representations; (6) the degree to which the systems can capture the original logic, probabilistic, and neural paradigms; and (7) the classes of tasks the
    
[^121]: 探索时空人群流量预测中的上下文泛化性：基准和指南

    Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline. (arXiv:2106.16046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.16046](http://arxiv.org/abs/2106.16046)

    本文研究了时空人群流量预测中的上下文泛化性，建立了基准，提出了通用分类法，为上下文选择和建模提供了指南。

    

    上下文特征是构建时空人群流量预测（STCFP）模型的重要数据来源。然而，应用上下文的困难在于不同场景中上下文特征（例如天气、假日和兴趣点）和上下文建模技术的未知泛化性。本文建立了一个基准，由大规模时空人群流量数据、上下文数据和最先进的时空预测模型组成。我们在几个城市人群流量预测场景中进行了全面的实验研究，以定量研究不同上下文特征和建模技术的泛化性。特别地，我们基于对流行研究的广泛调查，开发了上下文建模技术的通用分类法。我们使用了数百万条记录和丰富的上下文数据，训练和测试了数百种模型以捕捉上下文泛化性。我们的研究为STCFP中的上下文选择和建模提供了指南。

    Contextual features are important data sources for building spatiotemporal crowd flow prediction (STCFP) models. However, the difficulty of applying context lies in the unknown generalizability of both contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we build a benchmark composed of large-scale spatiotemporal crowd flow data, contextual data, and state-of-the-art spatiotemporal prediction models. We conduct a comprehensive experimental study to quantitatively investigate the generalizability of different contextual features and modeling techniques in several urban crowd flow prediction scenarios (including bike flow, metro passenger flow, electric vehicle charging demand and so on). In particular, we develop a general taxonomy of context modeling techniques based on extensive investigations in prevailing research. With millions of records and rich context data, we have trained and tested hun
    
[^122]: 向人类和机器传达自然程序

    Communicating Natural Programs to Humans and Machines. (arXiv:2106.07824v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.07824](http://arxiv.org/abs/2106.07824)

    LARC是通过自然语言描述的任务解决说明，可用于测试代理程序解决新问题的能力，并分析自然程序的独特之处。

    

    抽象推理语料库（ARC）是一组程序任务，用于测试代理程序解决新问题的能力。虽然大多数ARC任务对于人类来说很容易，但对于最先进的人工智能来说却很具有挑战性。建立能够推广到诸如ARC等新情况的智能系统的难点是什么？我们认为，答案可能在于研究\emph{语言}的差异：虽然人类容易生成和解释通用语言的指令，但计算机系统却被束缚在狭窄的领域特定语言中，只能精确执行。我们提出了LARC（Language-complete ARC）：一组通过自然语言描述的任务解决说明，由一组参与者使用语言独立指导彼此如何解决ARC任务，其中成功的说明占88％的ARC任务。我们将收集到的说明分析为“自然程序”，发现它们虽然类似于计算机程序，但在两个方面是独特的。

    The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of \emph{language}: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the \textit{Language-complete ARC}: a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in tw
    
[^123]: 人工神经元中的与/或权衡：对抗鲁棒性影响的研究

    And/or trade-off in artificial neurons: impact on adversarial robustness. (arXiv:2102.07389v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.07389](http://arxiv.org/abs/2102.07389)

    本文研究了人工神经元中与/或函数的连续性对分类鲁棒性的影响，提出了增加类AND神经元在网络中比例的措施，实验结果显示该方法具有潜在的应用前景。

    

    尽管神经网络取得了很大的成功，但分类鲁棒性仍然是一个问题，特别是在对抗性样本上表现尤为突出。本文通过关注人工神经元中实现的函数连续性，从纯AND门到纯OR门的范围来解决这个挑战。我们的假设是，在网络中存在足够数量的类OR的神经元会导致分类的脆弱性和增加对抗攻击的脆弱性。我们定义了类AND的神经元，并提出了增加它们在网络中比例的措施。这些措施涉及将输入放缩到[-1,1]区间并减少S型激活函数陡峭部分的点数。我们方法的一个关键部分是比较当神经元用实际数据集和称为“乱序数据集”的随机版本输入时的输出分布。在MNIST数据集上的实验结果表明，我们的方法很有前途。

    Despite the success of neural networks, the issue of classification robustness remains, particularly highlighted by adversarial examples. In this paper, we address this challenge by focusing on the continuum of functions implemented in artificial neurons, ranging from pure AND gates to pure OR gates. Our hypothesis is that the presence of a sufficient number of OR-like neurons in a network can lead to classification brittleness and increased vulnerability to adversarial attacks. We define AND-like neurons and propose measures to increase their proportion in the network. These measures involve rescaling inputs to the [-1,1] interval and reducing the number of points in the steepest section of the sigmoidal activation function. A crucial component of our method is the comparison between a neuron's output distribution when fed with the actual dataset and a randomised version called the "scrambled dataset." Experimental results on the MNIST dataset suggest that our approach holds promise a
    
[^124]: 可微分决策树通过自然语言规范RL策略

    Natural Language Specification of Reinforcement Learning Policies through Differentiable Decision Trees. (arXiv:2101.07140v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.07140](http://arxiv.org/abs/2101.07140)

    本文提出了一个新的可微分决策树框架，允许人们通过自然语言指定初始行为模型，并将其转换为词汇决策树，为机器人的强化学习策略提供指导。

    

    本文提出了一种新的人工智能政策规范过程，可以使人类与机器人共同启动强化学习策略。该过程包含两个步骤：政策规范与政策优化。我们开发了一个新的协作框架，允许人通过非结构化自然语言指定初始行为模型，并将其转换为词汇决策树来启动和解释一个自主代理的行为。

    Human-AI policy specification is a novel procedure we define in which humans can collaboratively warm-start a robot's reinforcement learning policy. This procedure is comprised of two steps; (1) Policy Specification, i.e. humans specifying the behavior they would like their companion robot to accomplish, and (2) Policy Optimization, i.e. the robot applying reinforcement learning to improve the initial policy. Existing approaches to enabling collaborative policy specification are often unintelligible black-box methods, and are not catered towards making the autonomous system accessible to a novice end-user. In this paper, we develop a novel collaborative framework to allow humans to initialize and interpret an autonomous agent's behavior. Through our framework, we enable humans to specify an initial behavior model via unstructured, natural language (NL), which we convert to lexical decision trees. Next, we leverage these translated specifications, to warm-start reinforcement learning an
    

