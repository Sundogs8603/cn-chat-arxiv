# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders](https://rss.arxiv.org/abs/2402.01614) | L2G2G是一种可扩展的局部到全局网络嵌入方法，通过动态同步潜在节点表示以提高GAE的准确性，并利用解码器计算只有本地图块损失，从而更好地利用了图的信息。 |
| [^2] | [Nomic Embed: Training a Reproducible Long Context Text Embedder](https://rss.arxiv.org/abs/2402.01613) | Nomic Embed是第一个完全可复现、开源、开放权重、开放数据的8192上下文长度英文文本嵌入器，在短上下文和长上下文任务上优于OpenAI Ada-002和OpenAI text-embedding-3-small。 |
| [^3] | [Natural Counterfactuals With Necessary Backtracking](https://rss.arxiv.org/abs/2402.01607) | 本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。 |
| [^4] | [Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning](https://rss.arxiv.org/abs/2402.01602) | 这篇综述论文提出了一个概念框架，利用知识增强和推理，引导基础模型（FMs）在任务中得到适当的指导，克服其在真实世界应用中的限制。 |
| [^5] | [BAT: Learning to Reason about Spatial Sounds with Large Language Models](https://rss.arxiv.org/abs/2402.01591) | 本文提出了BAT，它结合了双耳声音场景分析模型的空间声音感知能力和大规模语言模型的自然语言推理能力，以复制人类的空间声音推理能力。通过使用合成的双耳音频数据集和基于空间声音的问答数据集进行训练，BAT在空间声音感知和推理方面取得了强大的性能。 |
| [^6] | [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://rss.arxiv.org/abs/2402.01586) | 本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。 |
| [^7] | [Generative AI for Education (GAIED): Advances, Opportunities, and Challenges](https://rss.arxiv.org/abs/2402.01580) | 这篇调查文章总结了NeurIPS 2023会议上关于教育的生成式人工智能（GAIED）研讨会的活动，并突出指出了几个未来研究方向。 |
| [^8] | [Boximator: Generating Rich and Controllable Motions for Video Synthesis](https://rss.arxiv.org/abs/2402.01566) | Boximator是一种用于视频合成的新方法，通过引入硬盒和软盒约束类型，实现了对动作的精细控制。它的训练过程保留了基础模型的知识，并通过引入自我追踪技术简化了盒子对象之间的关联学习。实验证明，Boximator在视频质量上达到了最先进的水平，并进一步提升了稳健的运动可控性。 |
| [^9] | [Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting](https://rss.arxiv.org/abs/2402.01546) | 该论文提出了一种面向住宅短期负荷预测的隐私保护分布式学习方法，通过采用安全聚合算法和多方计算密码学技术来减轻梯度泄漏的风险，并解决了现有联邦学习模型的脆弱性和对新兴攻击技术的容易受到的问题。 |
| [^10] | [Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data](https://rss.arxiv.org/abs/2402.01537) | 本研究提出了一种新颖的生成技术，可用于创建通过综合RGB、热像和深度三个模态的人类行为分析数据集，弥补了现有HBA数据集中缺乏整合多模态的问题。 |
| [^11] | [Homogenization Effects of Large Language Models on Human Creative Ideation](https://rss.arxiv.org/abs/2402.01536) | 大型语言模型（LLMs）作为创造性支持工具（CSTs）使用时，虽然可以增加用户产生详细的想法的能力，但也会导致用户提出的想法同质化。对于基于LLMs的CSTs的用户、设计师和开发者来说，这些结果具有重要的潜在影响。 |
| [^12] | [An Empirical Analysis of Diversity in Argument Summarization](https://rss.arxiv.org/abs/2402.01535) | 本研究通过实证分析，发现目前的论据摘要方法在捕捉多样性方面存在困难，并提出多样性有三个方面：意见、注释者和来源。所研究的关键点分析任务中的通用LLMs和专门的KPA模型都有互补的优势，训练数据的多样化将有助于提高泛化能力。因此，应对论证摘要中的多样性需要采用多种策略来处理主观性。 |
| [^13] | [K-Level Reasoning with Large Language Models](https://rss.arxiv.org/abs/2402.01521) | 该论文探索了使用大型语言模型进行动态决策推理的能力，并提出了一种名为"K级推理"的新颖推理方法。通过博弈论的试验，发现现有的推理方法在动态环境中容易出错，而"K级推理"可以解决这个问题。 |
| [^14] | [Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence](https://rss.arxiv.org/abs/2402.01515) | 本文提出了一个统一框架来解决随机优化中的收敛问题，并提出了两种即插即用的加速方法，在理论上证明了这些方法可以加快收敛速度。 |
| [^15] | [Developing and Evaluating a Design Method for Positive Artificial Intelligence](https://rss.arxiv.org/abs/2402.01499) | 这项研究提出并评估了一种正向人工智能设计方法，用于确保人工智能系统对社会产生积极影响。该方法通过以人为中心的流程，将幸福愿景转化为具体实践，并通过持续的测量和反馈循环进行支持。 |
| [^16] | [Multi-level protein pre-training with Vabs-Net](https://rss.arxiv.org/abs/2402.01481) | 这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。 |
| [^17] | [Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes](https://rss.arxiv.org/abs/2402.01476) | 本论文提出了自核-特征对稀疏变分高斯过程（KEP-SVGP）用于构建具有不确定性感知的自注意力。通过核SVD（KSVD）解决了注意力核的不对称性，并实现了降低的复杂度。 |
| [^18] | [Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents](https://rss.arxiv.org/abs/2402.01467) | 本研究中，我们在使用递归神经网络的强化学习模型中发现了类似大脑回放的现象，并证明其对任务的贡献。这一发现提供了理解回放机制的新视角。 |
| [^19] | [Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach](https://rss.arxiv.org/abs/2402.01454) | 本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。 |
| [^20] | [Guidance Graph Optimization for Lifelong Multi-Agent Path Finding](https://rss.arxiv.org/abs/2402.01446) | 这项研究探索了如何利用导引图优化长期多智能体路径规划，提出了两种自动生成导引的算法，并解决了如何自动生成良好导引的问题。 |
| [^21] | [Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning](https://rss.arxiv.org/abs/2402.01444) | 卫星数据是机器学习中的独特模态，我们需要重新思考现有的实践并发起一个以卫星数据的特征和挑战为中心的新的研究议程，以推动SatML的质量和影响力。 |
| [^22] | [Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting](https://rss.arxiv.org/abs/2402.01440) | 本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。 |
| [^23] | [From Words to Molecules: A Survey of Large Language Models in Chemistry](https://rss.arxiv.org/abs/2402.01439) | 本论文调查了大型语言模型在化学领域的应用。研究内容涵盖了分子信息输入LLMs的表示和标记化方法、化学LLMs的不同组群及其整合方法、适用于化学LLMs的预训练目标等。此外，还探讨了LLMs在化学中的各种应用。 |
| [^24] | [Sequence Shortening for Context-Aware Machine Translation](https://rss.arxiv.org/abs/2402.01416) | 本研究展示了上下文感知机器翻译的序列缩短方法，通过重用上下文信息可以提高翻译准确性，在对比数据集上表现良好，并且引入了新的潜在分组和潜在选择方法来进一步提高翻译质量。 |
| [^25] | [SMLP: Symbolic Machine Learning Prover](https://rss.arxiv.org/abs/2402.01415) | SMLP是一种基于数据样本的系统探索工具，通过采用统计方法和机器学习模型相结合的灰盒方法，可以探索系统并优化硬件设计。 |
| [^26] | [XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision](https://rss.arxiv.org/abs/2402.01410) | 本研究提出了一种用于黑素瘤诊断的新方法，通过使用可解释的原型-部分模型以及结合基于非专家反馈的引导性监督，实现了优于不可解释的模型的性能和泛化能力。 |
| [^27] | [Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization](https://rss.arxiv.org/abs/2402.01401) | 通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。 |
| [^28] | [A Probabilistic Model to explain Self-Supervised Representation Learning](https://rss.arxiv.org/abs/2402.01399) | 该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。 |
| [^29] | [LoTR: Low Tensor Rank Weight Adaptation](https://rss.arxiv.org/abs/2402.01376) | LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。 |
| [^30] | [FindingEmo: An Image Dataset for Emotion Recognition in the Wild](https://rss.arxiv.org/abs/2402.01355) | FindingEmo是一个新的图像数据集，专门用于情感识别，在野外环境中提供了复杂场景的注释，涵盖多个人物和社交设置，注释包括情感维度和情感标签。 |
| [^31] | [Efficient compilation of expressive problem space specifications to neural network solvers](https://rss.arxiv.org/abs/2402.01353) | 本文提出了一种算法，可以将高级的问题空间规范编译为适合神经网络求解器的满足性查询，以解决神经网络验证中存在的嵌入间隙问题。 |
| [^32] | [Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes](https://rss.arxiv.org/abs/2402.01352) | 这项研究探索了图像描述中人类行为的变化，并发现图像的属性与这些变化相关。预训练模型可以在一定程度上捕捉到这种变化，但仍存在偏差。 |
| [^33] | [Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models](https://rss.arxiv.org/abs/2402.01349) | 对于评估大型语言模型中多选题回答的合理性进行了回顾，发现当前基于多选题回答的基准可能无法充分捕捉大型语言模型的真实能力。 |
| [^34] | [CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay](https://rss.arxiv.org/abs/2402.01348) | 本文通过COgnitive REplay（CORE）提出了一种在连续学习中减轻灾难性遗忘的新方法，通过自适应数量分配和以质量为重点的数据选择来优化重播缓冲区，取得了显著的准确率提高效果。 |
| [^35] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^36] | [Simulator-Free Visual Domain Randomization via Video Games](https://rss.arxiv.org/abs/2402.01335) | 本研究提出了一种名为BehAVE的视频理解框架，借助现有的商业视频游戏实现领域随机化，无需仿真器的支持。通过利用游戏中丰富的视觉多样性进行随机化，以及通过玩家行为的文本描述来指导具有相似内容的视频的对齐，BehAVE在领域随机化方面展现了鲁棒性。 |
| [^37] | [Supervised Algorithmic Fairness in Distribution Shifts: A Survey](https://rss.arxiv.org/abs/2402.01327) | 这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。 |
| [^38] | [KTO: Model Alignment as Prospect Theoretic Optimization](https://rss.arxiv.org/abs/2402.01306) | 本文提出了一种名为KTO的方法，将模型对齐视为展望理论优化。与当前方法相比，KTO直接最大化生成效用而不是最大化偏好对数似然。在多个规模上，KTO的性能与基于偏好的方法相当甚至更好。 |
| [^39] | [Learning Semantic Information from Raw Audio Signal Using Both Contextual and Phonetic Representations](https://rss.arxiv.org/abs/2402.01298) | 本论文提出了一个框架，利用上下文和音素表示从原始音频信号中学习语义信息。实验结果表明，该框架相比只使用一种类型表示进行训练的模型，在学习语义方面表现更好。 |
| [^40] | [ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast](https://rss.arxiv.org/abs/2402.01295) | ExtremeCast提出了一种新的损失函数Exloss，实现了针对极值的准确预测，同时引入了无需训练的极值增强策略ExEnsemble，提高了预报的稳健性 |
| [^41] | [Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence](https://rss.arxiv.org/abs/2402.01292) | 本文介绍并评估了一种基于证据权重框架的假设驱动可解释人工智能方法，通过提供支持或驳斥假设的证据来增加决策准确性和减少依赖程度。 |
| [^42] | [Federated Unlearning: a Perspective of Stability and Fairness](https://rss.arxiv.org/abs/2402.01276) | 本文研究了在数据异构性情况下的联邦取消学习（FU）的稳定性和公平性问题。我们提出了关键指标来评估FU，重点考虑了验证、全局稳定性和局部公平性，并研究了其中的权衡。通过理论分析，我们洞察了数据异构性对FU的影响，并提出了管理权衡的FU机制。通过实证验证，我们证实了这些机制的有效性。 |
| [^43] | [The Human and the Mechanical: logos, truthfulness, and ChatGPT](https://rss.arxiv.org/abs/2402.01267) | 本论文通过语义论证，讨论了是否可以称之为“机械思维”，以及ChatGPT模型能否实现该思维。研究发现，“机械思维”缺乏与现实相关的证据和个人信念，因此无法形成对世界的信念和真实性判断。 |
| [^44] | [TEDDY: Trimming Edges with Degree-based Discrimination strategY](https://rss.arxiv.org/abs/2402.01261) | TEDDY是一种利用边缘度量信息的边缘修剪方法，旨在通过一次性操作实现边缘稀疏化，进而鼓励参数稀疏化训练。这是一个解决图神经网络中抽奖票假设的时间效率和效果问题的创新方法。 |
| [^45] | [Position Aware 60 GHz mmWave Beamforming for V2V Communications Utilizing Deep Learning](https://rss.arxiv.org/abs/2402.01259) | 本文提出了一种利用深度学习的方法，通过预测具有足够毫米波接收功率的最佳波束，使用车辆位置信息来实现车辆到车辆通信中的高效链路配置。 |
| [^46] | [Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?](https://rss.arxiv.org/abs/2402.01241) | 该论文介绍了CISP（Contrastive Image Shape Pre training），通过将2D图像与3D形状在共享嵌入空间中对齐，增强了基于图像的3D形状合成。研究表明CISP能够捕捉CLIP的文本图像关注所忽视的3D特征，提高了生成质量。 |
| [^47] | [Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting](https://rss.arxiv.org/abs/2402.01240) | 本研究通过利用HTTP响应头设计了机器学习分类器，在跨浏览器环境下有效检测Web追踪器，结果在Chrome和Firefox上表现出较高的准确性和性能。 |
| [^48] | [PRIME: Protect Your Videos From Malicious Editing](https://rss.arxiv.org/abs/2402.01239) | 提出了一种名为PRIME的保护方法，旨在显著降低保护视频的时间成本，并提高保护性能。评估结果表明，PRIME仅需8.3%的GPU时间成本。 |
| [^49] | [Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training](https://rss.arxiv.org/abs/2402.01238) | 本研究引入了灵活的变分信息瓶颈（FVIB）框架，通过单次训练即可获得所有β值的最优模型，从而实现多样化压缩，并且在理论和实证方面证明了它的有效性。 |
| [^50] | [STAA-Net: A Sparse and Transferable Adversarial Attack for Speech Emotion Recognition](https://rss.arxiv.org/abs/2402.01227) | 本论文提出了一种生成器攻击方法，可以以端到端和高效的方式生成稀疏且可迁移的对抗样本来欺骗语音情感识别模型。 |
| [^51] | [AI Code Generators for Security: Friend or Foe?](https://rss.arxiv.org/abs/2402.01219) | AI代码生成器在软件安全研究中提供了新的机会，但也存在被恶意滥用的风险。 |
| [^52] | [Location Agnostic Adaptive Rain Precipitation Prediction using Deep Learning](https://rss.arxiv.org/abs/2402.01208) | 本研究提出了一种基于深度学习的自适应框架，能够解决降雨预测中的位置差异和气候变化带来的挑战。通过在巴黎、洛杉矶和东京进行适应，我们方法的预测能力分别提高了43.51%、5.09%和38.62%。 |
| [^53] | [Efficient Causal Graph Discovery Using Large Language Models](https://rss.arxiv.org/abs/2402.01207) | 提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。 |
| [^54] | [A Survey on Self-Supervised Learning for Non-Sequential Tabular Data](https://rss.arxiv.org/abs/2402.01204) | 本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。 |
| [^55] | [Few-Shot Class-Incremental Learning with Prior Knowledge](https://rss.arxiv.org/abs/2402.01201) | 该论文提出了一种具有先验知识的学习方法，通过引入从后续增量类别的无标签数据中产生的伪标签，与带标签的基类样本一起进行联合训练，有效地为旧类和新类数据分配嵌入空间，从而提高了模型对灾难性遗忘的韧性。 |
| [^56] | [Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations](https://rss.arxiv.org/abs/2402.01195) | 本文提出了使用条件化正则化流和主动学习的方法来解决粗粒化分子表示中的玻尔兹曼分布采样问题，相比传统的分子动力学模拟，该方法能够获得更高效的加速效果。 |
| [^57] | [Faster Inference of Integer SWIN Transformer by Removing the GELU Activation](https://rss.arxiv.org/abs/2402.01169) | 本研究通过去除SWIN Transformer中的GELU激活，提升了整数SWIN Transformer的推断速度。我们使用ReLU激活替代GELU，并使用迭代知识蒸馏来调整精度损失，最终实现了模型的量化和优化。 |
| [^58] | [A Comprehensive Survey on 3D Content Generation](https://rss.arxiv.org/abs/2402.01166) | 本综述对三维内容生成领域的发展进行了全面综合，提出了一种新的分类法，总结了目前的主要技术，并讨论了当前技术的局限性和未来工作的挑战和方向。 |
| [^59] | [ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution](https://rss.arxiv.org/abs/2402.01145) | 本文介绍了一种新的算法ReEvo，它利用大型语言模型作为超启发式算法的一种求解方法，通过反思设计方法和强大的演化搜索技术，显著提升了在组合优化问题上的搜索性能。 |
| [^60] | [Learning Network Representations with Disentangled Graph Auto-Encoder](https://rss.arxiv.org/abs/2402.01143) | 本文介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。 |
| [^61] | [Root Cause Analysis In Microservice Using Neural Granger Causal Discovery](https://rss.arxiv.org/abs/2402.01140) | 提出了一种基于神经Granger因果发现的方法来解决微服务中根本原因分析的挑战。 |
| [^62] | [DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping](https://rss.arxiv.org/abs/2402.01134) | DeepAAT是一个专门为无人机影像AAT设计的深度学习网络，通过考虑影像的空间和光谱特征，提高了AAT的效率和精度。 |
| [^63] | [Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models](https://rss.arxiv.org/abs/2402.01118) | Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。 |
| [^64] | [Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization](https://rss.arxiv.org/abs/2402.01114) | 这项研究探索了使用迁移学习和随机化来防止成员推断攻击。通过在过拟合的深度神经网络上应用双重防御，我们可以提高模型的隐私性而不降低准确度。 |
| [^65] | [Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints](https://rss.arxiv.org/abs/2402.01111) | 本文研究了具有自适应约束的多智能体强化学习问题，并提出了一种基于消除算法，将后悔控制在$\widetilde{O}(\sqrt{H^3 S^2 ABK})$，批量复杂度为$O(H+\log\log K)$。此外，还给出了所有具有$\widetilde{O}(\sqrt{K})$后悔界算法的批量复杂度下界。 |
| [^66] | [Simulation of Graph Algorithms with Looped Transformers](https://rss.arxiv.org/abs/2402.01107) | 本文研究了使用循环变压器网络模拟图算法的能力，证明了该结构可以模拟Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法，并展示了其具有图灵完整性的结果。 |
| [^67] | [Compositional Generative Modeling: A Single Model is Not All You Need](https://rss.arxiv.org/abs/2402.01103) | 本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。 |
| [^68] | [Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance](https://rss.arxiv.org/abs/2402.01096) | 本文回顾了可信的分布式AI的代表性技术，包括鲁棒性保证、隐私保护和公平意识，以解决分布式学习中存在的安全、隐私和公平问题。 |
| [^69] | [How many views does your deep neural network use for prediction?](https://rss.arxiv.org/abs/2402.01095) | 本文提出了最小有效视图（MSVs）的概念，该概念类似于多视图，但适用于实际图像，并且通过实证研究表明，MSV的数量与模型的预测准确性之间存在关系。 |
| [^70] | [Recent Advances in Predictive Modeling with Electronic Health Records](https://rss.arxiv.org/abs/2402.01077) | 这项调查总结了基于电子健康记录数据的深度学习预测模型的最新进展，包括背景介绍、数学定义、分类总结、基准和工具包，以及未来研究方向的讨论。 |
| [^71] | [Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer](https://rss.arxiv.org/abs/2402.01065) | 本文研究了大型语言模型（LLMs）在多语言环境下的能力。研究结果表明，将原始语言的上下文、问题和答案翻译成高资源语言可以获得最佳效果。 |
| [^72] | [Plan-Grounded Large Language Models for Dual Goal Conversational Settings](https://rss.arxiv.org/abs/2402.01053) | 本研究针对双重目标对话设置提出了一种新型的计划驱动大型语言模型，该模型能够在任意计划上基础对话，主动引导用户完成计划，并在系统行为上实施安全防护。 |
| [^73] | [Repeat After Me: Transformers are Better than State Space Models at Copying](https://rss.arxiv.org/abs/2402.01032) | 这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。 |
| [^74] | [Executable Code Actions Elicit Better LLM Agents](https://rss.arxiv.org/abs/2402.01030) | 使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。 |
| [^75] | [Quantifying analogy of concepts via ologs and wiring diagrams](https://rss.arxiv.org/abs/2402.01020) | 本文通过ologs和接线图的概念，建立了一个量化概念类比的框架，使得自主系统能够形成抽象概念，并通过图论和范畴论的技术进行比较和操作，同时使用接线图操作定义了度量和图编辑距离。 |
| [^76] | [AI-generated faces free from racial and gender stereotypes](https://rss.arxiv.org/abs/2402.01002) | 这项研究发现并解决了AI生成的面孔中存在的种族和性别刻板印象问题，提出了分类器用于预测面部属性的方法，并提出了有效的去偏见解决方案。 |
| [^77] | [An Information-Theoretic Approach to Analyze NLP Classification Tasks](https://rss.arxiv.org/abs/2402.00978) | 本研究提供了一个信息论框架来分析文本分类任务中输入的影响，发现在更具挑战性的数据集中，上下文对输出的影响减小，同时语义含义对于分类任务的影响较大。 |
| [^78] | [Recurrent Transformers with Dynamic Halt](https://rss.arxiv.org/abs/2402.00976) | 本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。 |
| [^79] | [SPARQL Generation with Entity Pre-trained GPT for KG Question Answering](https://rss.arxiv.org/abs/2402.00969) | 本文面向非程序员用户的KG问答问题，通过实体链接和GPT模型生成SPARQL查询，使用CWA预训练所有实体，实现了准确的SPARQL匹配率为62.703%。 |
| [^80] | [Credal Learning Theory](https://rss.arxiv.org/abs/2402.00957) | 本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。 |
| [^81] | [NCoder -- A Quantum Field Theory approach to encoding data](https://rss.arxiv.org/abs/2402.00944) | NCoder 是一种基于量子场论的数据编码方法，通过修改自编码神经网络的结构，将潜在层规定为一组 $n$-点关联函数的子集，模拟了通过费曼图按阶展开构建理论有效作用量的过程。同时，NCoder 也可以看作是模拟统计推断过程的方法，通过几个较低维度的统计量来总结高维度数据，并从这些统计量中推断数据生成分布。这种方法提示了扰动重整化和模型的充分性之间的有趣对应关系。 |
| [^82] | [MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation](https://rss.arxiv.org/abs/2402.00918) | 本文提出了一种基于多尺度时间上下文的注意力机制（MUSTAN），用于改善视频前景分割的泛化性能。通过整合视频的时间信息和空间线索，我们的方法在深度学习架构中实现了鲁棒的前景分割。 |
| [^83] | [Institutional Platform for Secure Self-Service Large Language Model Exploration](https://rss.arxiv.org/abs/2402.00913) | 这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。 |
| [^84] | [Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?](https://rss.arxiv.org/abs/2402.00912) | 本文研究了概念瓶颈模型如何从具有细粒度概念注释的数据集中学习概念，以实现模型输出的内在可解释性。 |
| [^85] | [Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning](https://rss.arxiv.org/abs/2402.00910) | 本文提出了一种通过集成学习和正则化微调的方法，解决AI模型中的偏见问题。该方法可通过在小数据集和有偏的预训练模型上训练多个对抗偏见的模型，并使用集成学习得到无偏的预测结果。通过实验证明了该方法在CIFAR10和HAM10000数据集上的有效性。 |
| [^86] | [Graph Domain Adaptation: Challenges, Progress and Prospects](https://rss.arxiv.org/abs/2402.00904) | 这篇论文综述了图领域适应的研究现状、挑战和前景，提出了一种有效的图之间的知识传递范式，可以增强模型在目标图上的性能。 |
| [^87] | [Real Sparks of Artificial Intelligence and the Importance of Inner Interpretability](https://rss.arxiv.org/abs/2402.00901) | 本文研究了微软工程师对GPT智能的研究，认为黑盒解释性方法论是错误的，提出了内部可解释性的重要性并与哲学观点相吻合。 |
| [^88] | [Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees](https://rss.arxiv.org/abs/2402.00899) | 这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。 |
| [^89] | [Privacy and Security Implications of Cloud-Based AI Services : A Survey](https://rss.arxiv.org/abs/2402.00896) | 本文调查了云基础人工智能服务的隐私和安全状况，发现机器学习模型引入的风险亟待解决。通过提出分类法来全面研究模型提供者和使用者所面临的风险及其防御手段，有助于创建强大的解决方案。 |
| [^90] | [MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts](https://rss.arxiv.org/abs/2402.00893) | MoDE是一种在混合专家模型中应用专家间相互蒸馏的方法，以提高模型的泛化能力。实验证明MoDE在各种数据集上都有效，并且具有普适性和鲁棒性。 |
| [^91] | [EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks](https://rss.arxiv.org/abs/2402.00892) | EVA-GAN通过可扩展的生成对抗网络在音频生成领域取得了显著进展，改善了频谱和高频重建以及对域外数据性能的稳健性，实现了高保真音频的生成。 |
| [^92] | [Large Language Models in Cybersecurity: State-of-the-Art](https://rss.arxiv.org/abs/2402.00891) | 本研究综述了大型语言模型（LLMs）在网络安全领域中的防御和对抗应用，并识别了关键的研究空缺，旨在提供对LLM驱动的网络安全的潜在风险和机会的全面理解。 |
| [^93] | [Security and Privacy Challenges of Large Language Models: A Survey](https://rss.arxiv.org/abs/2402.00888) | 大型语言模型具有卓越的能力，但也面临着安全和隐私攻击的威胁。本调查全面审查了LLM的安全和隐私挑战，涵盖了训练数据、用户和应用风险等方面，并对解决方法进行了回顾。 |
| [^94] | [On the Interplay of Artificial Intelligence and Space-Air-Ground Integrated Networks: A Survey](https://rss.arxiv.org/abs/2402.00881) | 人工智能和空中地面综合网络的相互作用是新兴第六代无线网络的关键，通过智能配置和控制SAGINs来满足预期要求，并利用AI解决当前和未来无线网络的挑战。 |
| [^95] | [Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud](https://rss.arxiv.org/abs/2402.00876) | 认知互联网超越了认知物联网，使连接的物体能够独立地获取知识和理解，并在整个网络中集成了协作智能。本文深入探讨了“认知互联网”范式的基础要素、独特特征、益处和工业影响。 |
| [^96] | [dRG-MEC: Decentralized Reinforced Green Offloading for MEC-enabled Cloud Network](https://rss.arxiv.org/abs/2402.00874) | 本文提出了一个名为dRG-MEC的技术，通过联合计算卸载实现最小化总计算和通信开销，同时能够降低系统总成本37.03%。 |
| [^97] | [Scaling Sparse Fine-Tuning to Large Language Models](https://rss.arxiv.org/abs/2401.16405) | 本研究将稀疏微调方法扩展到大型语言模型，提出了一种新的稀疏微调算法SpIEL，并对LLM进行了指令微调，以解决其参数庞大的问题。 |
| [^98] | [NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness](https://rss.arxiv.org/abs/2401.15963) | 本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。 |
| [^99] | [SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese](https://rss.arxiv.org/abs/2401.11819) | SuperCLUE-Math6 是一个用于评估中文语言模型数学推理能力的分级数据集，通过增加难度、多样性和应用范围，提供了2000多个需要多步推理的数学问题，并采用创新方法来量化大型模型的推理能力。实验结果显示出明显的推理水平分层，顶级模型如GPT-4表现出优越的性能。SC-Math6填补了中文数学推理基准数据集的空白，并推进了中文语言模型的智能化。 |
| [^100] | [InstantID: Zero-shot Identity-Preserving Generation in Seconds](https://rss.arxiv.org/abs/2401.07519) | InstantID是一种基于强散射模型的即插即用模块，通过只使用一张面部图像实现图像的个性化处理，同时保持高保真度。 |
| [^101] | [Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon](https://rss.arxiv.org/abs/2401.03462) | 本论文提出了一种称为激活标志的新方法，它通过压缩LLM的激活状态，使其能够以有限的上下文窗口感知更长的上下文，同时保留了LLM在短上下文中的原始能力。这种方法具有竞争力的内存和时间效率，并通过多样化训练有效地支持不同上下文长度。 |
| [^102] | [Spike No More: Stabilizing the Pre-training of Large Language Models](https://rss.arxiv.org/abs/2312.16903) | 本论文研究了大型语言模型预训练中的损失尖峰问题，并通过理论分析找出了梯度爆炸的原因，并提出了满足要求的方法。通过实验证明，该方法能够有效地防止尖峰的发生。 |
| [^103] | [Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions](https://rss.arxiv.org/abs/2312.15101) | 本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。 |
| [^104] | [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://rss.arxiv.org/abs/2312.13382) | DSPy引入了LM断言，用于表达语言模型应满足的计算约束。在四个案例研究中，LM断言不仅提高了对规则的遵守，而且提高了下游任务的性能，增加了对约束的接受次数并生成了更高质量的回复。 |
| [^105] | [DIRECT: Deep Active Learning under Imbalance and Label Noise](https://rss.arxiv.org/abs/2312.09196) | 这篇论文提出了一种名为DIRECT的算法，用于处理不平衡和标签噪声下的深度主动学习问题。通过确定类别分割阈值并标记最不确定且离其最近的示例，该算法能够有效解决罕见类和少数类的性能问题，并具有批次标记和对标签噪声的容忍能力。 |
| [^106] | [DTL: Disentangled Transfer Learning for Visual Recognition](https://rss.arxiv.org/abs/2312.07856) | DTL通过使用紧凑侧网络（CSN）将可训练参数从主干网络中解缠出来，以缓解在迁移学习中GPU内存占用过多的问题。 |
| [^107] | [Why "classic" Transformers are shallow and how to make them go deep](https://rss.arxiv.org/abs/2312.06182) | 这篇论文研究了Transformer模型的深度问题，指出这个问题是由于“token相似性升级”导致的，提供了理论和实证调查的证据。 |
| [^108] | [Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML](https://rss.arxiv.org/abs/2311.09200) | 通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。 |
| [^109] | [Understanding Grokking Through A Robustness Viewpoint](https://rss.arxiv.org/abs/2311.06597) | 通过神经网络的鲁棒性视角，我们发现$l_2$权重范数是Grokking的充分条件，并提出基于扰动的方法加速泛化，在模数相加数据集上发现标准训练过程在Grokking之前几乎没有学习其他基本群操作，而使用我们方法时加速泛化可以通过学习交换律来解释。 |
| [^110] | [High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft](https://rss.arxiv.org/abs/2311.06130) | 本文介绍了一种创新的降维算法，利用偏最小二乘回归来减少构建混合变量高斯过程所需的超参数数量。这种方法在结构和多学科应用中表现出良好的潜力，适用于绿色飞机的优化等多个领域。 |
| [^111] | [A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges](https://rss.arxiv.org/abs/2311.05112) | 本综述提供了医学中大型语言模型（LLMs）的原理、应用和挑战的全面概述。同时回答了医学LLMs的构建、下游性能、实际应用、挑战以及更好构建和利用的问题。旨在为构建有效的医学LLMs提供见解和实用资源。 |
| [^112] | [Deception Abilities Emerged in Large Language Models](https://rss.arxiv.org/abs/2307.16513) | 大规模语言模型（LLM）如GPT-4具备了理解和诱导他人产生错误信念的能力，并且在复杂的欺骗场景中可以通过链式思维推理得到增强。 |
| [^113] | [Language Models as Inductive Reasoners](https://rss.arxiv.org/abs/2212.10923) | 该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。 |
| [^114] | [Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations](https://arxiv.org/abs/2402.00591) | Sandra是一个神经符号推理器，通过将矢量表示与演绎推理相结合，利用本体论建立的向量空间进行推理。它基于描述和情境的本体设计模式，能够从一组事实中推断出所有可能的解释，并在实验中证明在不增加复杂性的情况下优于其他基准线的分类结果，并且具有可解释性和向量空间的可控性。 |
| [^115] | [An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction](https://arxiv.org/abs/2402.00306) | 本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。 |
| [^116] | [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/abs/2402.00045) | 本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。 |
| [^117] | [Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming](https://arxiv.org/abs/2401.17527) | 这篇论文提出了一种学习停止割生成的方法，以提高混合整数线性规划的效率。通过将问题形式化为强化学习问题，并使用混合图表示模型进行学习，该方法能够动态地决策何时停止割生成，并有效捕捉混合整数线性规划的动态和静态特征。 |
| [^118] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^119] | [A Linguistic Comparison between Human and ChatGPT-Generated Conversations.](http://arxiv.org/abs/2401.16587) | 本研究比较了人类和ChatGPT生成的对话的语言差异，发现ChatGPT在社交、分析、认知、关注焦点和积极情绪等方面表现出色，但人类对话更具变异性和真实性，尽管在情绪方面无显著差异。同时，该研究还提供了一个新颖的、由ChatGPT生成的对话组成的数据集。 |
| [^120] | [Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports.](http://arxiv.org/abs/2401.16578) | 该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。 |
| [^121] | [DiffuserLite: Towards Real-time Diffusion Planning.](http://arxiv.org/abs/2401.15443) | DiffuserLite是一个快速轻量级的扩散规划框架，通过引入计划细化过程（PRP）来提高决策频率，相比之前的框架，它只产生了很小的运行时间成本，并在D4RL基准测试中达到了最先进的性能。 |
| [^122] | [A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM.](http://arxiv.org/abs/2401.15378) | 基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。 |
| [^123] | [DQNC2S: DQN-based Cross-stream Crisis event Summarizer.](http://arxiv.org/abs/2401.06683) | 本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。 |
| [^124] | [End-to-end Learnable Clustering for Intent Learning in Recommendation.](http://arxiv.org/abs/2401.05975) | 本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。 |
| [^125] | [Comprehensive Exploration of Synthetic Data Generation: A Survey.](http://arxiv.org/abs/2401.02524) | 本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。 |
| [^126] | [Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale.](http://arxiv.org/abs/2312.07586) | 该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。 |
| [^127] | [Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms.](http://arxiv.org/abs/2310.14168) | 该论文介绍了一种随机前向模式自动微分优化算法，通过在神经网络的正向传递中计算损失函数的方向导数来更新参数。算法通过采样不同概率分布的随机方向，使用正向模式自动微分计算雅可比向量乘积，并提供了对其收敛速度和计算复杂性的严格分析。 |
| [^128] | [Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach.](http://arxiv.org/abs/2310.11531) | 本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。 |
| [^129] | [In-Context Few-Shot Relation Extraction via Pre-Trained Language Models.](http://arxiv.org/abs/2310.11085) | 本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。 |
| [^130] | [Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs.](http://arxiv.org/abs/2310.10107) | 本文分析了后验采样学习算法在序列化POMDPs中的遗憾性能，并在一定条件下提供了改进的多项式贝叶斯遗憾界。 |
| [^131] | [A Neural Scaling Law from Lottery Ticket Ensembling.](http://arxiv.org/abs/2310.02258) | 《来自彩票票集成的神经规模定律》通过研究神经规模定律现象，发现其与彩票票集成有关，从而形成了新的缩放定律，具有潜在的影响。 |
| [^132] | [QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems.](http://arxiv.org/abs/2309.10293) | 该论文提出了一种QXAI框架，用于患者监测系统中的定量分析。该框架具有可解释性，对于决策者在医疗应用中基于非线性模型结果进行决策非常重要。 |
| [^133] | [FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning.](http://arxiv.org/abs/2309.10283) | FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。 |
| [^134] | [Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence.](http://arxiv.org/abs/2309.10186) | 本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。 |
| [^135] | [Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance.](http://arxiv.org/abs/2309.00300) | 本文提出了一个可识别的认知诊断框架，该框架能够从学生的答题记录中直接诊断可识别和可解释的考生特征和题目特征，并通过重建答题记录来确保诊断结果的可识别性。 |
| [^136] | [${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.11842) | 本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。 |
| [^137] | [Learning to Communicate using Contrastive Learning.](http://arxiv.org/abs/2307.01403) | 本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。 |
| [^138] | [Demystifying GPT Self-Repair for Code Generation.](http://arxiv.org/abs/2306.09896) | 本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。 |
| [^139] | [How Can Recommender Systems Benefit from Large Language Models: A Survey.](http://arxiv.org/abs/2306.05817) | 本文对将大型语言模型（LLM）应用于推荐系统进行了全面的调查研究，从两个角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。最后，我们提出了一些潜在的研究方向和挑战。 |
| [^140] | [Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data.](http://arxiv.org/abs/2305.19075) | 本文提出了一种结合基础技能先验和模仿学习的基于语言条件的通用方法，在非结构化数据下，以增强算法在适应不熟悉的环境方面的泛化能力。在零-shot设置下，在模拟和真实环境中测试，提高了CALVIN基准测试的得分。 |
| [^141] | [How to escape sharp minima.](http://arxiv.org/abs/2305.15659) | 本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。 |
| [^142] | [In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making.](http://arxiv.org/abs/2305.07722) | AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。 |
| [^143] | [Assessing Working Memory Capacity of ChatGPT.](http://arxiv.org/abs/2305.03731) | 本文评估了最先进语言模型ChatGPT的工作记忆容量，结果显示其在N-back任务的行为表现与人类参与者相似，这为设计具有人类级认知能力的人工智能系统提供了关键洞察。 |
| [^144] | [Data Curation for Image Captioning with Text-to-Image Generative Models.](http://arxiv.org/abs/2305.03610) | 本论文探究了通过数据整合来提高图像字幕质量的方案，并使用现有资源训练了比基准模型更好的模型。 |
| [^145] | [Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence.](http://arxiv.org/abs/2304.12241) | 设计以幸福为导向的人工智能系统面临着知识和动机两方面的挑战，包括概念化、测量和优化幸福，设计适当的AI行动，以及激励措施、财务和宣传风险的不一致以及数据获取的缺乏。针对这些挑战，需要在科学理解AI系统对幸福影响方面进行研究，并指导设计行动。 |
| [^146] | [Machine Learning with Requirements: a Manifesto.](http://arxiv.org/abs/2304.03674) | 本文提出一个带需求的机器学习宣言，认为需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域，作者提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。 |
| [^147] | [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?.](http://arxiv.org/abs/2303.18240) | 该研究研究了使用预训练视觉表征来实现身体智能的最新进展。他们展示了最大、最全面的经验研究，发现没有一种表征是普遍优越的，并且数据集的大小和多样性并不能普遍改善性能。 |
| [^148] | [VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices.](http://arxiv.org/abs/2303.16150) | VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。 |
| [^149] | [Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews.](http://arxiv.org/abs/2302.08062) | 该论文提出了使用深度学习集成的数据增强多视角方法识别化石图像。通过收集化石图像的不同视角，使用多个基础模型进行训练，并通过软投票进行最终决策。实验证明，该方法在性能上优于基线方法，并且与使用三个原始视图模型的方法相当。 |
| [^150] | [I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data.](http://arxiv.org/abs/2210.13954) | 该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。 |
| [^151] | [Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making.](http://arxiv.org/abs/2209.11812) | 本研究探讨了基于特征的解释对AI辅助决策公正性的影响，发现解释可以影响公正性感知和人们对AI建议的依赖。然而，解释并不能帮助人们区分正确和错误的AI建议。这些发现对于促进有效的决策和公正性至关重要。 |

# 详细

[^1]: L2G2G: 一种可扩展的局部到全局网络嵌入方法，基于图自动编码器

    L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders

    [https://rss.arxiv.org/abs/2402.01614](https://rss.arxiv.org/abs/2402.01614)

    L2G2G是一种可扩展的局部到全局网络嵌入方法，通过动态同步潜在节点表示以提高GAE的准确性，并利用解码器计算只有本地图块损失，从而更好地利用了图的信息。

    

    对于分析实际网络，图表示学习是一种常用工具。这些方法，如图自动编码器(GAE)，通常依赖于低维表示，也称为嵌入，通过最小化损失函数获得;这些嵌入与解码器一起用于节点分类和边预测等下游任务。虽然GAE往往相当准确，但存在可扩展性问题。为了改善速度，Local2Global方法通过基于特征向量同步的图块嵌入相结合，显示出快速且准确的效果。在这里，我们提出了L2G2G，一种_Local2Global方法，它在不牺牲可扩展性的情况下提高了GAE的准确性。这种改进是通过在训练GAE期间动态同步潜在节点表示来实现的。它还受益于解码器计算只有本地图块损失。因此，每个时代中的本地嵌入对齐利用了更多来自图的信息。

    For analysing real-world networks, graph representation learning is a popular tool. These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. While GAEs tend to be fairly accurate, they suffer from scalability issues. For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability. This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs. It also benefits from the decoder computing an only local patch loss. Hence, aligning the local embeddings in each epoch utilises more information from the gr
    
[^2]: Nomic Embed：训练可复现的长上下文文本嵌入器

    Nomic Embed: Training a Reproducible Long Context Text Embedder

    [https://rss.arxiv.org/abs/2402.01613](https://rss.arxiv.org/abs/2402.01613)

    Nomic Embed是第一个完全可复现、开源、开放权重、开放数据的8192上下文长度英文文本嵌入器，在短上下文和长上下文任务上优于OpenAI Ada-002和OpenAI text-embedding-3-small。

    

    本技术报告描述了nomic-embed-text-v1的训练，这是第一个完全可复现、开源、开放权重、开放数据的8192上下文长度英文文本嵌入模型，在短上下文和长上下文任务上均优于OpenAI Ada-002和OpenAI text-embedding-3-small。我们在Apache 2许可下发布了训练代码和模型权重。与其他开源模型相比，我们还发布了一个包含2.35亿个策划文本对的训练数据加载器，可以完全复现nomic-embed-text-v1。你可以在https://github.com/nomic-ai/contrastors找到模型的代码和数据。

    This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors
    
[^3]: 具有必要回溯的自然反事实

    Natural Counterfactuals With Necessary Backtracking

    [https://rss.arxiv.org/abs/2402.01607](https://rss.arxiv.org/abs/2402.01607)

    本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。

    

    反事实推理对于人类认知非常重要，尤其对于提供解释和做出决策至关重要。尽管Judea Pearl的研究方法在理论上很优雅，但其生成反事实情景往往需要过于脱离实际情景的干预，因此难以实施。为了解决这个问题，我们提出了一种自然反事实的框架和一种根据实际世界数据分布生成自然反事实的方法。我们的方法提供了对反事实推理的改进，允许对因果前置变量进行改变以最小化与实际情景的偏差。为了生成自然反事实，我们引入了一种创新的优化框架，通过自然性准则允许但控制回溯的范围。实证实验表明了我们方法的有效性。

    Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
    
[^4]: 基础模型导航员: 引导基础模型通过知识和推理

    Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning

    [https://rss.arxiv.org/abs/2402.01602](https://rss.arxiv.org/abs/2402.01602)

    这篇综述论文提出了一个概念框架，利用知识增强和推理，引导基础模型（FMs）在任务中得到适当的指导，克服其在真实世界应用中的限制。

    

    基础模型（FMs）如大型语言模型通过在各种任务中展示出色的性能，改变了人工智能领域。然而，它们还存在许多限制，阻止了它们在许多实际系统中的广泛应用，这些系统通常要求更高的可信度和可用性。由于FMs是使用旨在以自我监督方式重新构建训练语料库的损失函数进行训练的，所以不能保证模型的输出与用户对特定任务的偏好一致。在本综述论文中，我们提出了一个概念框架，该框架概括了各种代理如何与FMs进行交互，并通过知识增强和推理来适应一组任务。我们的框架阐明了代理角色类别，如更新基础FM，辅助提示FM和评估FM输出。我们还将几种最先进的方法进行了代理交互原型的分类。

    Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction prot
    
[^5]: BAT: 使用大规模语言模型学习关于空间声音的推理能力

    BAT: Learning to Reason about Spatial Sounds with Large Language Models

    [https://rss.arxiv.org/abs/2402.01591](https://rss.arxiv.org/abs/2402.01591)

    本文提出了BAT，它结合了双耳声音场景分析模型的空间声音感知能力和大规模语言模型的自然语言推理能力，以复制人类的空间声音推理能力。通过使用合成的双耳音频数据集和基于空间声音的问答数据集进行训练，BAT在空间声音感知和推理方面取得了强大的性能。

    

    空间声音推理是一种基本的人类技能，它使我们能够根据声音来导航和解释我们的周围环境。本文提出了BAT，它将双耳声音场景分析模型的空间声音感知能力与大规模语言模型（LLM）的自然语言推理能力相结合，以复制这种固有能力。为了解决现有野外空间声音数据集的缺乏，我们使用AudioSet和SoundSpaces 2.0合成了一个双耳音频数据集。接下来，我们开发了一种基于空间声音的问答数据集SpatialSoundQA，提供了一系列QA任务，以训练BAT在空间声音感知和推理的各个方面。BAT的声学前端编码器是一种名为Spatial Audio Spectrogram Transformer（Spatial-AST）的创新空间音频编码器，它本身在声音事件检测、空间定位和距离估计等方面具有强大的性能。通过将Spatial-AST与LLaMA-2 7B集成，

    Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
    
[^6]: TrustAgent: 通过代理构成实现安全可信赖的LLM代理

    TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution

    [https://rss.arxiv.org/abs/2402.01586](https://rss.arxiv.org/abs/2402.01586)

    本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。

    

    近年来，基于LLM的代理引起了广泛关注，但其可信度仍未得到深入探索。由于代理可以直接与物理环境交互，其可靠性和安全性至关重要。本文提出了一种基于代理构成的代理框架TrustAgent，对LLM代理的安全性维度进行了初步研究。该框架包括三种策略：预先规划策略，在生成计划之前向模型注入安全知识；规划过程中策略，在生成计划时增强安全性；计划后检查策略，通过计划后检查确保安全性。通过实验分析，我们展示了这些方法如何通过识别和预防潜在危险有效提高LLM代理的安全性。此外，我们还探讨了安全性与使用者满意度之间的复杂关系，以及模型的推理能力与其效率之间的关联。

    The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
    
[^7]: 教育的生成式人工智能（GAIED）：进展、机遇和挑战

    Generative AI for Education (GAIED): Advances, Opportunities, and Challenges

    [https://rss.arxiv.org/abs/2402.01580](https://rss.arxiv.org/abs/2402.01580)

    这篇调查文章总结了NeurIPS 2023会议上关于教育的生成式人工智能（GAIED）研讨会的活动，并突出指出了几个未来研究方向。

    

    这篇调查文章是由作者在NeurIPS 2023会议上组织的GAIED研讨会发展而来的。我们组织GAIED研讨会是为了促进研究人员、教育者和实践者的交流，探索生成式人工智能在教育中的潜力。本文旨在概要介绍研讨会的活动，并突出指出在GAIED领域中的几个未来研究方向。

    This survey article has grown out of the GAIED (pronounced "guide") workshop organized by the authors at the NeurIPS 2023 conference. We organized the GAIED workshop as part of a community-building effort to bring together researchers, educators, and practitioners to explore the potential of generative AI for enhancing education. This article aims to provide an overview of the workshop activities and highlight several future research directions in the area of GAIED.
    
[^8]: Boximator: 生成丰富和可控的视频合成动作

    Boximator: Generating Rich and Controllable Motions for Video Synthesis

    [https://rss.arxiv.org/abs/2402.01566](https://rss.arxiv.org/abs/2402.01566)

    Boximator是一种用于视频合成的新方法，通过引入硬盒和软盒约束类型，实现了对动作的精细控制。它的训练过程保留了基础模型的知识，并通过引入自我追踪技术简化了盒子对象之间的关联学习。实验证明，Boximator在视频质量上达到了最先进的水平，并进一步提升了稳健的运动可控性。

    

    在视频合成中，生成丰富且可控的动作是一个重大的挑战。我们提出了一种新的方法Boximator，用于精细的动作控制。Boximator引入了两种约束类型：硬盒和软盒。用户使用硬盒在条件帧中选择对象，然后使用两种类型的盒子来粗略或严格地定义对象在未来帧中的位置、形状或运动路径。Boximator作为现有视频扩散模型的插件运行。它的训练过程通过冻结原始权重，只训练控制模块来保留基础模型的知识。为了解决训练中的挑战，我们引入了一种新颖的自我追踪技术，大大简化了盒子对象之间的关联学习。实验证明，Boximator在视频质量（FVD）得分上达到了最先进的水平，改进了两个基准模型，并在加入盒子约束后进一步提高。其稳健的运动可控性通过边界的剧增得到验证。

    Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding
    
[^9]: 面向住宅短期负荷预测的隐私保护分布式学习

    Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting

    [https://rss.arxiv.org/abs/2402.01546](https://rss.arxiv.org/abs/2402.01546)

    该论文提出了一种面向住宅短期负荷预测的隐私保护分布式学习方法，通过采用安全聚合算法和多方计算密码学技术来减轻梯度泄漏的风险，并解决了现有联邦学习模型的脆弱性和对新兴攻击技术的容易受到的问题。

    

    在电力系统领域，住宅用户在负荷预测应用中的日益增加使得数据隐私问题日趋关注。具体而言，负荷数据可能意外地泄露住宅用户的日常生活习惯，从而对其财产安全构成风险。虽然联邦学习（FL）已被用来通过在不交换原始数据的情况下进行模型训练来保护用户隐私，但这些FL模型对新兴的攻击技术（如梯度泄漏攻击和毒化攻击）显示出了脆弱性。为了抵御这些攻击，我们首先采用了一种安全聚合（SecAgg）算法，利用多方计算密码学技术来减轻梯度泄漏的风险。然而，SecAgg的引入需要部署额外的子中心服务器来执行多方计算协议，从而增加了计算复杂度，降低了系统的鲁棒性，特别是在特定场景中。

    In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy. Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security. While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks. To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage. However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenario
    
[^10]: 缩小人类行为分析的差距：一种综合三模态数据的流程

    Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data

    [https://rss.arxiv.org/abs/2402.01537](https://rss.arxiv.org/abs/2402.01537)

    本研究提出了一种新颖的生成技术，可用于创建通过综合RGB、热像和深度三个模态的人类行为分析数据集，弥补了现有HBA数据集中缺乏整合多模态的问题。

    

    在普适机器学习中，尤其是在人类行为分析（HBA）领域，RGB一直是首选的模态，因为它易于获取并且信息丰富。然而，与其优点相连的是一些挑战，包括对光照条件的敏感性和隐私问题。解决这些漏洞的一个可能性是利用不同的模态。例如，热像素能够突出人体形态，而深度模态则增加了重要的背景信息。尽管这些模态带来了相关好处，但目前只有很少一部分特定于HBA的数据集集成了这些模态。为了解决这一问题，我们的研究介绍了一种新颖的生成技术，用于创建三模态（即RGB、热像和深度）的以人为中心的数据集。这种技术利用从RGB图像中提取的人体分割掩模，并与自动获取的热像和深度背景结合。有了这两个要素，我们可以利用现有的RGB数据合成深度和热像模态。

    In pervasive machine learning, especially in Human Behavior Analysis (HBA), RGB has been the primary modality due to its accessibility and richness of information. However, linked with its benefits are challenges, including sensitivity to lighting conditions and privacy concerns. One possibility to overcome these vulnerabilities is to resort to different modalities. For instance, thermal is particularly adept at accentuating human forms, while depth adds crucial contextual layers. Despite their known benefits, only a few HBA-specific datasets that integrate these modalities exist. To address this shortage, our research introduces a novel generative technique for creating trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique capitalizes on human segmentation masks derived from RGB images, combined with thermal and depth backgrounds that are sourced automatically. With these two ingredients, we synthesize depth and thermal counterparts from existing RGB data uti
    
[^11]: 大型语言模型对人类创造性思维的同质化影响

    Homogenization Effects of Large Language Models on Human Creative Ideation

    [https://rss.arxiv.org/abs/2402.01536](https://rss.arxiv.org/abs/2402.01536)

    大型语言模型（LLMs）作为创造性支持工具（CSTs）使用时，虽然可以增加用户产生详细的想法的能力，但也会导致用户提出的想法同质化。对于基于LLMs的CSTs的用户、设计师和开发者来说，这些结果具有重要的潜在影响。

    

    大型语言模型(LLMs)现在在各种场景下被使用，包括作为创造性支持工具(CSTs)来帮助用户产生新的想法。但是LLMs真的能够支持用户的创造力吗？我们假设LLMs作为CSTs的使用可能使LLMs的用户感到更有创造力，并且扩大每位用户提出的想法的范围，但也同质化了不同用户所提出的想法。我们进行了一个36位参与者的比较性用户研究，并根据同质化假设发现，与另一种CST相比，不同用户倾向于使用ChatGPT提出较少语义上独立的想法。此外，ChatGPT用户产生了更多详细的想法，但对所生成的想法感到责任更少。我们讨论了这些发现对基于LLMs的CSTs的用户、设计师和开发者的潜在影响。

    Large language models (LLMs) are now being used in a wide variety of contexts, including as creativity support tools (CSTs) intended to help their users come up with new ideas. But do LLMs actually support user creativity? We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users. We conducted a 36-participant comparative user study and found, in accordance with the homogenization hypothesis, that different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST. Additionally, ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated. We discuss potential implications of these findings for users, designers, and developers of LLM-based CSTs.
    
[^12]: 论论文摘要中的多样性的实证分析

    An Empirical Analysis of Diversity in Argument Summarization

    [https://rss.arxiv.org/abs/2402.01535](https://rss.arxiv.org/abs/2402.01535)

    本研究通过实证分析，发现目前的论据摘要方法在捕捉多样性方面存在困难，并提出多样性有三个方面：意见、注释者和来源。所研究的关键点分析任务中的通用LLMs和专门的KPA模型都有互补的优势，训练数据的多样化将有助于提高泛化能力。因此，应对论证摘要中的多样性需要采用多种策略来处理主观性。

    

    在在线社会讨论中，提供高水平的论据是促进参与的关键任务。目前的论据摘要方法缺失了这项任务的一个重要方面，即捕捉多样性，这对于包容多个观点是重要的。我们引入了三个方面的多样性：意见、注释者和来源。我们评估了一种名为关键点分析的流行论据摘要任务的方法，显示这些方法在(1)代表少数人共享的论点上，(2)处理来自各种来源的数据以及(3)与人工提供的主观注释相一致方面遇到了困难。我们发现，通用的LLM和专门的KPA模型都表现出了这种行为，但具有互补的优势。此外，我们观察到训练数据的多样化可能改善泛化能力。应对论证摘要中的多样性需要采用一系列策略来处理主观性。

    Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.
    
[^13]: 使用大型语言模型进行K级推理

    K-Level Reasoning with Large Language Models

    [https://rss.arxiv.org/abs/2402.01521](https://rss.arxiv.org/abs/2402.01521)

    该论文探索了使用大型语言模型进行动态决策推理的能力，并提出了一种名为"K级推理"的新颖推理方法。通过博弈论的试验，发现现有的推理方法在动态环境中容易出错，而"K级推理"可以解决这个问题。

    

    虽然大型语言模型（LLMs）已经展示了其在复杂推理任务上的能力，但在动态、交互和竞争场景（如商业战略和股票市场分析）中的性能仍然未被充分探索。为了填补这个空白，我们正式探索LLMs在快速变化环境中的决策推理能力。我们引入了两个基于博弈论的试验，以模拟现实世界中动态决策的复杂性。这些挑战具有明确定义，可以对LLMs的动态推理能力进行清晰、可控和精确的评估。通过大量实验，我们发现现有的推理方法在需要k级思考的动态环境中容易出错 - 这是之前研究中未解决的关键概念。为了解决这个问题，我们提出了一种新颖的LLMs推理方法，命名为“K级推理”。该方法采用对手的视角，从递归角度运用基于k级思考的推理。

    While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on 
    
[^14]: 提升随机梯度下降：一种统一框架和用于更快收敛的新型加速方法

    Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence

    [https://rss.arxiv.org/abs/2402.01515](https://rss.arxiv.org/abs/2402.01515)

    本文提出了一个统一框架来解决随机优化中的收敛问题，并提出了两种即插即用的加速方法，在理论上证明了这些方法可以加快收敛速度。

    

    基于SGD，之前的研究提出了许多算法，在随机优化中改进了收敛速度和泛化性能，例如SGDm，AdaGrad，Adam等。然而，在非凸条件下，它们的收敛分析是具有挑战性的。在本文中，我们提出了一个统一的框架来解决这个问题。对于任何一阶方法，我们将更新方向$g_t$解释为随机次梯度$\nabla f_t(x_t)$和附加的加速项$\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$的和，因此我们可以通过分析$\langle v_t, \nabla f_t(x_t) \rangle$来讨论收敛性。通过我们的框架，我们发现了两种即插即用的加速方法：\textbf{拒绝加速}和\textbf{随机向量加速}，我们在理论上证明了这两种方法可以直接导致收敛速度的提升。

    Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc. However, their convergence analysis under non-convex conditions is challenging. In this work, we propose a unified framework to address this issue. For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\nabla f_t(x_t)$ and an additional acceleration term $\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\langle v_t, \nabla f_t(x_t) \rangle$. Through our framework, we have discovered two plug-and-play acceleration methods: \textbf{Reject Accelerating} and \textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate.
    
[^15]: 开发和评估正向人工智能设计方法

    Developing and Evaluating a Design Method for Positive Artificial Intelligence

    [https://rss.arxiv.org/abs/2402.01499](https://rss.arxiv.org/abs/2402.01499)

    这项研究提出并评估了一种正向人工智能设计方法，用于确保人工智能系统对社会产生积极影响。该方法通过以人为中心的流程，将幸福愿景转化为具体实践，并通过持续的测量和反馈循环进行支持。

    

    随着人工智能的不断发展，确保人工智能系统对社会产生积极影响变得至关重要，尤其是在人工智能系统在各个方面日益普及的情况下。然而，开发“AI for good”在与复杂人类价值观保持一致方面存在巨大挑战。目前，我们缺乏成熟的方法来解决这些挑战。本文介绍和评估了正向人工智能设计方法，旨在填补这一空白。该方法提供了一个以人为中心的流程，将幸福愿景转化为具体实践。首先，我们解释了该方法的四个关键步骤：情境化、操作化、优化化和实现福祉，同时支持持续测量和反馈循环。然后，我们提出了一个多个案例研究，其中初学者设计师应用了该方法，揭示了与效力和可用性相关的优点和缺点。接下来，一项专家评估研究评估了所得概念的质量，将其评为中等水平。

    As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing "AI for good" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them modera
    
[^16]: 用Vabs-Net进行多级蛋白质预训练

    Multi-level protein pre-training with Vabs-Net

    [https://rss.arxiv.org/abs/2402.01481](https://rss.arxiv.org/abs/2402.01481)

    这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。

    

    最近几年，三维结构预训练蛋白质模型的发展迅猛，相较于预训练蛋白质语言模型，在各种下游任务中取得了重大进展。然而，大多数现有的基于结构的预训练模型主要关注残基水平，即α碳原子，而忽略了其他原子，如侧链原子。我们认为，在残基和原子水平上对蛋白质进行建模很重要，因为侧链原子对于许多下游任务（如分子对接）也是至关重要的。然而，我们发现在预训练中天真地组合残基和原子信息通常会失败。我们发现，信息泄漏是包含原子结构的输入导致残基级预训练任务变得琐碎并导致残基表示不够充分的一个关键原因。为了解决这个问题，我们引入了一种基于跨度掩码预训练策略的三维蛋白质预训练方法。

    In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
    
[^17]: 自核-特征对稀疏变分高斯过程中的自注意力

    Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes

    [https://rss.arxiv.org/abs/2402.01476](https://rss.arxiv.org/abs/2402.01476)

    本论文提出了自核-特征对稀疏变分高斯过程（KEP-SVGP）用于构建具有不确定性感知的自注意力。通过核SVD（KSVD）解决了注意力核的不对称性，并实现了降低的复杂度。

    

    尽管Transformer具有显著提高预测准确性的能力，但它也可能产生过于自信的预测，并需要校准的不确定性估计，这通常可以通过高斯过程（GPs）来解决。现有的工作将对称核应用于变分推断下的注意力核；然而，忽略了注意力核本质上是不对称的事实。此外，推导出大规模数据的GP后验的复杂度仍然很高。在这项工作中，我们提出了一种用于构建具有不确定性感知的自注意力的核-特征对稀疏变 分高斯过程（KEP-SVGP），其中通过核SVD（KSVD）解决了注意力核的不对称性，并获得了降低的复杂度。通过KEP-SVGP，i）由于与注意力核的KSVD相对应的两组奇异向量引导的SVGP对完全表征了不对称性；ii）仅使用少量与KSVD相对应的伴随特征函数，推导SVGP后验概率密度可以实现较低的复杂度。

    While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteri
    
[^18]: 强化学习智能体内出现类似大脑回放的现象

    Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents

    [https://rss.arxiv.org/abs/2402.01467](https://rss.arxiv.org/abs/2402.01467)

    本研究中，我们在使用递归神经网络的强化学习模型中发现了类似大脑回放的现象，并证明其对任务的贡献。这一发现提供了理解回放机制的新视角。

    

    大脑区域中普遍观察到的回放现象是否能够在人工智能智能体中自然产生？如果是的话，它是否对任务有所贡献？在本研究中，我们使用基于递归神经网络的强化学习模型，在任务优化的范式下发现了回放的自然出现，模型模拟了海马体和前额叶皮层以及它们之间的相互沟通和感觉皮层的输入。海马体中的回放是由于情景记忆、认知地图以及环境观察而产生的，与动物实验数据相似，并且是高任务性能的有效指标。该模型还成功地重现了局部和非局部的回放，与人类实验数据相符。我们的工作为理解回放机制提供了新的途径。

    Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.
    
[^19]: 在因果发现中集成大型语言模型: 一种统计因果方法

    Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach

    [https://rss.arxiv.org/abs/2402.01454](https://rss.arxiv.org/abs/2402.01454)

    本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。

    

    在实际的统计因果发现（SCD）中，将领域专家知识作为约束嵌入到算法中被广泛接受，因为这对于创建一致有意义的因果模型是重要的，尽管识别背景知识的挑战被认可。为了克服这些挑战，本文提出了一种新的因果推断方法，即通过将LLM的“统计因果提示（SCP）”与SCD方法和基于知识的因果推断（KBCI）相结合，对SCD进行先验知识增强。实验证明，GPT-4可以使LLM-KBCI的输出与带有LLM-KBCI的先验知识的SCD结果接近真实情况，如果GPT-4经历了SCP，那么SCD的结果还可以进一步改善。而且，即使LLM不含有数据集的信息，LLM仍然可以通过其背景知识来改进SCD。

    In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
    
[^20]: 长期多智能体路径规划中的导引图优化研究

    Guidance Graph Optimization for Lifelong Multi-Agent Path Finding

    [https://rss.arxiv.org/abs/2402.01446](https://rss.arxiv.org/abs/2402.01446)

    这项研究探索了如何利用导引图优化长期多智能体路径规划，提出了两种自动生成导引的算法，并解决了如何自动生成良好导引的问题。

    

    我们研究如何利用导引来提高长期多智能体路径规划（MAPF）的吞吐量。先前的研究表明，尽管将导引（如高速公路）纳入MAPF算法可以加速计算，但这往往会与解决方案质量产生折中。此外，如何自动生成良好的导引仍然尚未充分探索，当前的方法还无法超越手动设计的导引。在这项工作中，我们引入了有向导引图作为长期MAPF中引导的通用表示，并将导引图优化（GGO）的任务定义为优化其边权重。我们提出了两种GGO算法，用于自动生成适用于任意长期MAPF算法和地图的导引。第一种方法直接使用CMA-ES（一种黑箱优化算法）来解决GGO问题。第二种方法PIU通过优化一个能够生成导引的更新模型，展示了将优化过的导引图传播到较大地图中的能力。

    We study how to use guidance to improve the throughput of lifelong Multi-Agent Path Finding (MAPF). Previous studies have demonstrated that while incorporating guidance, such as highways, can accelerate MAPF algorithms, this often results in a trade-off with solution quality. In addition, how to generate good guidance automatically remains largely unexplored, with current methods falling short of surpassing manually designed ones. In this work, we introduce the directed guidance graph as a versatile representation of guidance for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of optimizing its edge weights. We present two GGO algorithms to automatically generate guidance for arbitrary lifelong MAPF algorithms and maps. The first method directly solves GGO by employing CMA-ES, a black-box optimization algorithm. The second method, PIU, optimizes an update model capable of generating guidance, demonstrating the ability to transfer optimized guidance graphs to larger
    
[^21]: 任务关键 -- 卫星数据是机器学习中的独特模态

    Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning

    [https://rss.arxiv.org/abs/2402.01444](https://rss.arxiv.org/abs/2402.01444)

    卫星数据是机器学习中的独特模态，我们需要重新思考现有的实践并发起一个以卫星数据的特征和挑战为中心的新的研究议程，以推动SatML的质量和影响力。

    

    卫星数据有潜力为机器学习带来一次重大的改变，我们需要重新思考针对传统数据模态设计的现有实践。随着卫星数据的机器学习（SatML）在现实世界中产生重大影响，我们的领域正处于一个十字路口。我们可以继续应用不适合的方法，或者我们可以发起一个以卫星数据的独特特征和挑战为中心的新研究议程。本文认为卫星数据构成了机器学习研究的一种独特模态，而我们必须承认这一点，以推动SatML在理论、方法和部署方面的质量和影响力。我们概述了关键性的讨论问题和可行性建议，将SatML从仅仅一个有趣的应用领域转变为一个致力于推动机器学习和社会重大挑战的专门研究学科。

    Satellite data has the potential to inspire a seismic shift for machine learning -- one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society.
    
[^22]: 在图上的小样本学习：从元学习到预训练和提示

    Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting

    [https://rss.arxiv.org/abs/2402.01440](https://rss.arxiv.org/abs/2402.01440)

    本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。

    

    图表示学习是图中心任务中的关键步骤，在这方面已经取得了重大进展。早期的技术通常在端到端的设置中运行，性能严重依赖于充足的标记数据的可用性。这个限制引发了图上的小样本学习的出现，其中每个任务只有少量的任务特定标签可用。鉴于这个领域的广泛文献，本综述试图综合最近的发展，提供比较性的见解，并确定未来的方向。我们将现有的研究系统地分为三个主要类别：元学习方法、预训练方法和混合方法，并在每个类别中进行细粒度的分类，以帮助读者进行方法选择。在每个类别中，我们分析这些方法之间的关系并比较它们的优缺点。最后，我们概述了图上的小样本学习未来的方向。

    Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
    
[^23]: 从词语到分子：大型语言模型在化学领域的调查

    From Words to Molecules: A Survey of Large Language Models in Chemistry

    [https://rss.arxiv.org/abs/2402.01439](https://rss.arxiv.org/abs/2402.01439)

    本论文调查了大型语言模型在化学领域的应用。研究内容涵盖了分子信息输入LLMs的表示和标记化方法、化学LLMs的不同组群及其整合方法、适用于化学LLMs的预训练目标等。此外，还探讨了LLMs在化学中的各种应用。

    

    在近年来，大型语言模型（LLMs）在自然语言处理（NLP）和各种跨学科领域取得了重要的成功。然而，将LLMs应用于化学是一个复杂的任务，需要专门的领域知识。本文对将LLMs整合到化学领域中所采用的微妙方法进行了全面的探索，深入探讨了这个跨学科交汇点的复杂性和创新性。具体而言，我们的分析从通过各种表示和标记化方法将分子信息输入LLMs开始。然后，我们根据输入数据的领域和模态将化学LLMs分为三个不同的组，并讨论将这些输入与LLMs整合的方法。此外，本文还探讨了适用于化学LLMs的预训练目标。在此之后，我们探讨了LLMs在化学中的多样化应用，包括它们的新范式的应用。

    In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their applica
    
[^24]: 上下文感知机器翻译的序列缩短

    Sequence Shortening for Context-Aware Machine Translation

    [https://rss.arxiv.org/abs/2402.01416](https://rss.arxiv.org/abs/2402.01416)

    本研究展示了上下文感知机器翻译的序列缩短方法，通过重用上下文信息可以提高翻译准确性，在对比数据集上表现良好，并且引入了新的潜在分组和潜在选择方法来进一步提高翻译质量。

    

    上下文感知机器翻译旨在通过将周围的句子作为上下文来改进句子的翻译。为实现这一目标，已经应用了两种主要架构，即基于串联的单编码器和多编码器模型。在这项研究中，我们展示了多编码器架构的一个特殊情况，在下一步中重用源句子的潜在表示作为上下文，可以在对比数据集上实现更高的准确性（模型必须对提供的句子中的正确翻译进行排序），并且与单编码器和多编码器方法相比具有可比较的BLEU和COMET分数。此外，我们研究了将缓存表示应用于序列缩短。我们测试了三种基于汇聚的缩短技术，并引入了两种新方法——潜在分组和潜在选择，其中网络学习将令牌分组或选择要缓存为上下文。我们的实验表明，缓存表示的序列缩短方法可以进一步提高翻译质量。

    Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th
    
[^25]: SMLP: 符号机器学习证明器

    SMLP: Symbolic Machine Learning Prover

    [https://rss.arxiv.org/abs/2402.01415](https://rss.arxiv.org/abs/2402.01415)

    SMLP是一种基于数据样本的系统探索工具，通过采用统计方法和机器学习模型相结合的灰盒方法，可以探索系统并优化硬件设计。

    

    符号机器学习证明器（SMLP）是一种基于通过模拟或执行系统得到的一系列输入向量样本的系统探索工具和库。SMLP旨在通过采用灰盒方法，即将数据探索的统计方法与构建和探索机器学习模型相结合，并通过组合概率和形式方法来探索这些模型，从而探索系统。SMLP已经应用于英特尔的工业环境中，用于分析和优化模拟电路级别的硬件设计。SMLP是一个通用的工具，可以应用于可以通过机器学习模型进行采样和建模的系统。

    Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors. SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods. SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level. SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models.
    
[^26]: 使用原型和非专家监督的XAI对皮肤癌的检测

    XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision

    [https://rss.arxiv.org/abs/2402.01410](https://rss.arxiv.org/abs/2402.01410)

    本研究提出了一种用于黑素瘤诊断的新方法，通过使用可解释的原型-部分模型以及结合基于非专家反馈的引导性监督，实现了优于不可解释的模型的性能和泛化能力。

    

    通过皮肤镜图像分析进行皮肤癌检测是一个关键的任务。然而，现有用于此目的的模型往往缺乏解释性和可靠性，由于它们的黑盒性质引起了医生的担忧。在本文中，我们提出了一种新颖的方法，利用可解释的原型-部分模型进行黑素瘤诊断。我们通过结合两种不同的信息路径引入了基于非专家反馈的引导性监督：1) 使用分割网络自动获取的二进制掩模；2) 用户优化的原型。这两个不同的信息路径旨在确保学习到的原型与皮肤病变的相关区域相对应，排除其边界之外的混淆因素。实验结果表明，即使没有专家监督，我们的方法在性能和泛化能力上都优于不可解释的模型。

    Skin cancer detection through dermoscopy image analysis is a critical task. However, existing models used for this purpose often lack interpretability and reliability, raising the concern of physicians due to their black-box nature. In this paper, we propose a novel approach for the diagnosis of melanoma using an interpretable prototypical-part model. We introduce a guided supervision based on non-expert feedback through the incorporation of: 1) binary masks, obtained automatically using a segmentation network; and 2) user-refined prototypes. These two distinct information pathways aim to ensure that the learned prototypes correspond to relevant areas within the skin lesion, excluding confounding factors beyond its boundaries. Experimental results demonstrate that, even without expert supervision, our approach achieves superior performance and generalization compared to non-interpretable models.
    
[^27]: 通过Lipschitz正则化在规模上实现零样本机器遗忘

    Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization

    [https://rss.arxiv.org/abs/2402.01401](https://rss.arxiv.org/abs/2402.01401)

    通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。

    

    为了遵守人工智能和数据规定，从训练得到的机器学习模型中遗忘私人或受版权保护的信息的需求变得越来越重要。遗忘的关键挑战是及时忘记必要的数据，同时保持模型性能。在这项工作中，我们解决了零样本遗忘的场景，即只有一个经过训练的模型和要遗忘的数据，遗忘算法必须能够移除数据。根据这样定义，现有的最先进的方法是不够的。基于Lipschitz连续性的概念，我们提出了一种方法，通过对样本扰动的输出进行平滑处理来诱导遗忘。我们展示了这种平滑性成功地实现了遗忘，同时保持了总体模型性能。我们对我们的方法进行了广泛的经验评估，包括一系列当代基准测试，验证了我们的方法在严格的零样本约束下达到了最先进的性能。

    To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
    
[^28]: 解释自监督表示学习的概率模型

    A Probabilistic Model to explain Self-Supervised Representation Learning

    [https://rss.arxiv.org/abs/2402.01399](https://rss.arxiv.org/abs/2402.01399)

    该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。

    

    自监督学习（SSL）通过利用辅助的无监督任务，例如对语义相关样本进行分类，如不同的数据增强或模态来学习表示。在众多SSL方法中，对比方法（例如SimCLR，CLIP和VicREG）因学习到的表示在下游性能上接近有监督学习而受到关注。然而，这些方法背后的机制的理论理解仍然存在困难。我们提出了一个生成潜变量模型来表示数据，并展示了几类具有鉴别性的自监督算法（包括对比方法）近似诱导其表示中的潜变量结构，从而提供了一个统一的理论框架。我们还证明了与互信息和投影头的相关性。通过生成式地拟合我们的模型（如SimVE），在常见的基准测试上（例如FashionMNIST，CIFAR10，CelebA），性能优于之前的VAE方法。

    Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
    
[^29]: LoTR: 低张量秩权重自适应

    LoTR: Low Tensor Rank Weight Adaptation

    [https://rss.arxiv.org/abs/2402.01376](https://rss.arxiv.org/abs/2402.01376)

    LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。

    

    在本文中，我们将大型语言模型（LLM）上的低秩适应（LoRA）思想推广和扩展，这些模型基于Transformer架构。广泛使用的LoRA类方法是基于梯度更新的矩阵分解。我们引入了LoTR，一种新颖的LLM参数高效调优方法，它以张量分解的形式表示参数的梯度更新。每个层的低秩适配器都由三个矩阵的乘积构成，而张量结构是由这个乘积的左右乘子在层之间共享引起的。通过对低秩张量表示的一系列层同时压缩，LoTR能够比LoRA在特别是对于深层模型具有更好的参数效率。此外，核心张量不依赖于原始权重维度，可以任意缩小，从而实现非常廉价和快速的下游调优。

    In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
    
[^30]: 在野外情感识别中的Emo图像数据集

    FindingEmo: An Image Dataset for Emotion Recognition in the Wild

    [https://rss.arxiv.org/abs/2402.01355](https://rss.arxiv.org/abs/2402.01355)

    FindingEmo是一个新的图像数据集，专门用于情感识别，在野外环境中提供了复杂场景的注释，涵盖多个人物和社交设置，注释包括情感维度和情感标签。

    

    我们介绍FindingEmo，一个包含25k张图片的新的图像数据集，专门用于情感识别。与现有的数据集不同，它专注于复杂场景，包含多个人物在各种自然、社交环境中，图片的注释是整体进行的，超越了传统对于面部或单个个体的关注。注释的维度包括情感价值、唤起和情感标签，注释通过Prolific收集。除了注释，我们还发布了指向原始图片的URL列表，以及所有相关源代码。

    We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.
    
[^31]: 将表达丰富的问题空间规范高效编译为神经网络求解器

    Efficient compilation of expressive problem space specifications to neural network solvers

    [https://rss.arxiv.org/abs/2402.01353](https://rss.arxiv.org/abs/2402.01353)

    本文提出了一种算法，可以将高级的问题空间规范编译为适合神经网络求解器的满足性查询，以解决神经网络验证中存在的嵌入间隙问题。

    

    最近的研究揭示了神经网络验证中存在的嵌入间隙。在间隙的一侧是一个关于网络行为的高级规范，由领域专家根据可解释的问题空间编写。在另一侧是一组逻辑上等价的可满足性查询，以适合神经网络求解器的形式表达在不可理解的嵌入空间中。在本文中，我们描述了一种将前者编译为后者的算法。我们探索和克服了针对神经网络求解器而不是标准SMT求解器所出现的问题。

    Recent work has described the presence of the embedding gap in neural network verification. On one side of the gap is a high-level specification about the network's behaviour, written by a domain expert in terms of the interpretable problem space. On the other side are a logically-equivalent set of satisfiability queries, expressed in the uninterpretable embedding space in a form suitable for neural network solvers. In this paper we describe an algorithm for compiling the former to the latter. We explore and overcome complications that arise from targeting neural network solvers as opposed to standard SMT solvers.
    
[^32]: 描述图像的“快慢”: 量化和预测视觉语言过程中人类信号的变化

    Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes

    [https://rss.arxiv.org/abs/2402.01352](https://rss.arxiv.org/abs/2402.01352)

    这项研究探索了图像描述中人类行为的变化，并发现图像的属性与这些变化相关。预训练模型可以在一定程度上捕捉到这种变化，但仍存在偏差。

    

    图像的属性与人们在描述图像时的行为之间存在复杂的关系。这种行为表现出丰富的变化，如眼动和人们开始描述图像的时机。尽管视觉语言变异的这些信号具有重要价值，但在当前预训练模型的训练过程中，它们几乎被忽视，这促使进一步的调查。通过使用同时收集的荷兰图像描述语料库和眼动数据，我们探索了视觉语言信号变化的本质，并发现它们之间存在相关性。鉴于这个结果，我们假设变化部分源于图像的属性，并探索预训练视觉编码器所编码的图像表示能否捕捉到这种变化。我们的研究结果表明，预训练模型在一定程度上能够做到这一点，这表明模型缺乏对什么使刺激复杂的偏好。

    There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for 
    
[^33]: 超越答案：对于评估大型语言模型中多选题回答的合理性的回顾

    Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models

    [https://rss.arxiv.org/abs/2402.01349](https://rss.arxiv.org/abs/2402.01349)

    对于评估大型语言模型中多选题回答的合理性进行了回顾，发现当前基于多选题回答的基准可能无法充分捕捉大型语言模型的真实能力。

    

    在自然语言处理领域，大型语言模型（LLMs）引发了一场范式转变，显著提升了自然语言生成任务的性能。尽管取得了这些进展，对LLMs的全面评估仍然是社区面临的必然挑战。最近，将多选题回答（MCQA）作为LLMs的基准已经引起了广泛关注。本研究调查了MCQA作为LLMs评估方法的合理性。如果LLMs真正理解问题的语义，它们的性能应该在从相同问题派生的各种配置上表现一致。然而，我们的实证结果表明LLMs的响应一致性存在显著差异，我们将之定义为LLMs的响应可变性综合征（REVAS），这表明目前基于MCQA的基准可能无法充分捕捉LLMs的真实能力，强调了对更合适的评估方法的需要。

    In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need f
    
[^34]: CORE：通过认知重播来减轻连续学习中的灾难性遗忘

    CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay

    [https://rss.arxiv.org/abs/2402.01348](https://rss.arxiv.org/abs/2402.01348)

    本文通过COgnitive REplay（CORE）提出了一种在连续学习中减轻灾难性遗忘的新方法，通过自适应数量分配和以质量为重点的数据选择来优化重播缓冲区，取得了显著的准确率提高效果。

    

    本文介绍了一种显著减轻连续学习中的灾难性遗忘的新视角，强调模型保持现有知识并融入新信息的能力。目前的重播方法同等对待每个任务和数据样本，因此无法充分利用重播缓冲区的潜力。作为回应，我们提出了COgnitive REplay（CORE），它从人类认知复习过程中得到灵感。CORE包括两个关键策略：自适应数量分配和以质量为重点的数据选择。前者根据每个任务的遗忘速率自适应地调节回放缓冲区的分配，而后者保证在缓冲区中包含最能概括每个任务特征的代表性数据。我们的方法在分割CIFAR10上实现了37.95%的平均准确率，超过最佳基准方法6.52%。此外，它还显著提高了最差表现模型的准确率。

    This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information. Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer. In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes. CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer. Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, it significantly enhances the accuracy of the poorest-perfo
    
[^35]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^36]: 通过视频游戏实现无仿真器视觉领域随机化

    Simulator-Free Visual Domain Randomization via Video Games

    [https://rss.arxiv.org/abs/2402.01335](https://rss.arxiv.org/abs/2402.01335)

    本研究提出了一种名为BehAVE的视频理解框架，借助现有的商业视频游戏实现领域随机化，无需仿真器的支持。通过利用游戏中丰富的视觉多样性进行随机化，以及通过玩家行为的文本描述来指导具有相似内容的视频的对齐，BehAVE在领域随机化方面展现了鲁棒性。

    

    领域随机化是一种有效的计算机视觉技术，用于提高视觉模型在视觉上截然不同但内容相似的领域中的传递性。然而，现有的方法大量依赖于调整复杂和专门的仿真引擎，这些引擎的构建很困难，进而影响了它们的可行性和可扩展性。本文介绍了BehAVE，一种视频理解框架，它独特地利用现有的商业视频游戏来实现领域随机化，而无需访问它们的仿真引擎。在BehAVE下，(1) 视频游戏固有的丰富视觉多样性成为随机化的来源，(2) 玩家行为 - 通过动作的文本描述进行语义表示 - 引导具有相似内容的视频的对齐。我们在各种视频和文本基础模型上测试了BehAVE，并报告了它在领域随机化方面的鲁棒性。

    Domain randomization is an effective computer vision technique for improving transferability of vision models across visually distinct domains exhibiting similar content. Existing approaches, however, rely extensively on tweaking complex and specialized simulation engines that are difficult to construct, subsequently affecting their feasibility and scalability. This paper introduces BehAVE, a video understanding framework that uniquely leverages the plethora of existing commercial video games for domain randomization, without requiring access to their simulation engines. Under BehAVE (1) the inherent rich visual diversity of video games acts as the source of randomization and (2) player behavior -- represented semantically via textual descriptions of actions -- guides the *alignment* of videos with similar content. We test BehAVE on 25 games of the first-person shooter (FPS) genre across various video and text foundation models and we report its robustness for domain randomization. Beh
    
[^37]: 在分布变化中的监督算法公平性：一项综述

    Supervised Algorithmic Fairness in Distribution Shifts: A Survey

    [https://rss.arxiv.org/abs/2402.01327](https://rss.arxiv.org/abs/2402.01327)

    这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。

    

    在分布变化下的监督公平机器学习是一个新兴领域，解决了面对从源领域到目标领域的数据分布变化时，如何保持公平和无偏预测的挑战。在现实世界的应用中，机器学习模型通常是在特定数据集上进行训练，但在部署时，数据分布可能因各种因素而随时间发生变化。这种变化可能导致不公平的预测，对特定通过敏感属性（如种族和性别）来表征的群体产生不均衡的影响。在这项调查中，我们对各种类型的分布变化进行了总结，并全面调查了基于这些变化的现有方法，在文献中突出了六种常用的方法。此外，这份调查列出了用于实证研究的公开可用数据集和评估指标。我们进一步探讨了与相关研究领域的交互关系，并讨论了其中的重要创新和贡献。

    Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
    
[^38]: KTO: 模型对齐视为展望理论优化

    KTO: Model Alignment as Prospect Theoretic Optimization

    [https://rss.arxiv.org/abs/2402.01306](https://rss.arxiv.org/abs/2402.01306)

    本文提出了一种名为KTO的方法，将模型对齐视为展望理论优化。与当前方法相比，KTO直接最大化生成效用而不是最大化偏好对数似然。在多个规模上，KTO的性能与基于偏好的方法相当甚至更好。

    

    凯恩曼与特沃斯基的展望理论告诉我们，人类以有偏见但明确的方式看待随机变量；例如，人们通常都是厌恶损失的。我们证明了将LLMs与人工反馈进行对齐的目标隐含地融合了许多这些偏见 - 这些目标 (例如 DPO) 的成功部分可归因于它们是"人类感知损失函数"(HALOs)。然而，这些方法所归因给人类的效用函数仍与展望理论文献中的不同。利用凯恩曼-特沃斯基人类效用的模型，我们提出了一种直接最大化生成效用而不是最大化偏好对数似然的HALO。我们将这种方法称为凯恩曼-特沃斯基优化(KTO)，并且它在从1B到30B的规模上与基于偏好的方法的性能相匹配或超过。关键是，KTO不需要偏好 - 只需要一个是否的二进制信号。

    Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
    
[^39]: 使用上下文和音素表示学习原始音频信号的语义信息

    Learning Semantic Information from Raw Audio Signal Using Both Contextual and Phonetic Representations

    [https://rss.arxiv.org/abs/2402.01298](https://rss.arxiv.org/abs/2402.01298)

    本论文提出了一个框架，利用上下文和音素表示从原始音频信号中学习语义信息。实验结果表明，该框架相比只使用一种类型表示进行训练的模型，在学习语义方面表现更好。

    

    我们提出了一个框架，使用两种类型的表示（分别是编码上下文和音素信息）从原始音频信号中学习语义。具体地，我们引入了一个语音到单元处理流程，以不同的时间分辨率捕捉两种类型的表示。对于语言模型，我们采用了双通道架构来整合这两种表示类型。我们还提出了新的训练目标，即掩码上下文重构和掩码上下文预测，可以有效地推动模型学习语义。在Zero Resource Speech Benchmark 2021和Fluent Speech Command数据集上的实验结果显示，与只使用一种类型表示进行训练的模型相比，我们的框架能够更好地学习语义。

    We propose a framework to learn semantics from raw audio signals using two types of representations, encoding contextual and phonetic information respectively. Specifically, we introduce a speech-to-unit processing pipeline that captures two types of representations with different time resolutions. For the language model, we adopt a dual-channel architecture to incorporate both types of representation. We also present new training objectives, masked context reconstruction and masked context prediction, that push models to learn semantics effectively. Experiments on the sSIMI metric of Zero Resource Speech Benchmark 2021 and Fluent Speech Command dataset show our framework learns semantics better than models trained with only one type of representation.
    
[^40]: ExtremeCast: 提升全球天气预报的极值预测能力

    ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast

    [https://rss.arxiv.org/abs/2402.01295](https://rss.arxiv.org/abs/2402.01295)

    ExtremeCast提出了一种新的损失函数Exloss，实现了针对极值的准确预测，同时引入了无需训练的极值增强策略ExEnsemble，提高了预报的稳健性

    

    基于机器学习的数据驱动天气预报在全球中期预报中已经得到了快速发展，并且相较于传统的基于物理的动力学模型表现出更好的性能。然而，大多数这些机器学习模型在准确预测极端天气方面存在困难，而极端值预测与此密切相关。通过数学分析，我们证明使用对称损失，如均方误差（MSE），会导致预测有偏差并低估极值。为了解决这个问题，我们引入了Exloss，一种新的损失函数，通过非对称优化突出极值，以获得准确的极端天气预报。此外，我们还引入了一种无需训练的极值增强策略ExEnsemble，它增加了像素值的方差，并提高了预报的稳健性。结合先进的全球天气预报模型，广泛的实验证明了我们的方法

    Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
    
[^41]: 迈向新的可解释人工智能：通过证据支持的假设驱动方法的决策支持

    Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence

    [https://rss.arxiv.org/abs/2402.01292](https://rss.arxiv.org/abs/2402.01292)

    本文介绍并评估了一种基于证据权重框架的假设驱动可解释人工智能方法，通过提供支持或驳斥假设的证据来增加决策准确性和减少依赖程度。

    

    之前关于AI辅助人类决策的研究探索了几种不同的可解释人工智能（XAI）方法。最近的一篇论文提出了一种范式转变，呼吁通过一个称为评价型AI的概念框架来进行假设驱动的XAI，该框架为人们提供支持或驳斥假设的证据，而不一定给出决策辅助推荐。在本文中，我们描述并评估了一种基于证据权重（WoE）框架的假设驱动XAI方法，该方法为给定的假设生成正面和负面证据。通过人类行为实验，我们展示了我们的假设驱动方法提高了决策准确性，与推荐驱动方法和仅AI解释基线相比减少了依赖程度，但相对于推荐驱动方法，在依赖程度下降方面略微增加。此外，我们还展示了参与者在使用我们的假设驱动方法时与两个基线的方式存在实质性的差异。

    Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy, reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.
    
[^42]: 联邦取消学习: 稳定性和公平性的视角

    Federated Unlearning: a Perspective of Stability and Fairness

    [https://rss.arxiv.org/abs/2402.01276](https://rss.arxiv.org/abs/2402.01276)

    本文研究了在数据异构性情况下的联邦取消学习（FU）的稳定性和公平性问题。我们提出了关键指标来评估FU，重点考虑了验证、全局稳定性和局部公平性，并研究了其中的权衡。通过理论分析，我们洞察了数据异构性对FU的影响，并提出了管理权衡的FU机制。通过实证验证，我们证实了这些机制的有效性。

    

    本文探讨了在数据异构性情况下联邦取消学习（FU）的多方面影响。我们介绍了FU评估的关键指标，重点关注验证，全局稳定性和局部公平性，并研究了内在的权衡。此外，我们通过一个优化框架对具有数据异构性的取消学习过程进行了形式化。我们的核心贡献在于对FU中权衡进行了全面的理论分析，并提供了数据异构性对FU的影响的见解。利用这些见解，我们提出了管理权衡的FU机制，为FU机制的进一步发展提供指导。我们通过实证验证了我们的FU机制有效地平衡了权衡，确认了从我们的理论分析中得出的见解。

    This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.
    
[^43]: 人类与机器：逻辑、真实性与ChatGPT

    The Human and the Mechanical: logos, truthfulness, and ChatGPT

    [https://rss.arxiv.org/abs/2402.01267](https://rss.arxiv.org/abs/2402.01267)

    本论文通过语义论证，讨论了是否可以称之为“机械思维”，以及ChatGPT模型能否实现该思维。研究发现，“机械思维”缺乏与现实相关的证据和个人信念，因此无法形成对世界的信念和真实性判断。

    

    本论文探讨了是否适当地谈论“机械思维”，以及ChatGPT模型是否可以被视为实现了这一点。我们的论文在当前的讨论中添加了一个语义论证。人类断言的行为需要形成一个真实性判断。使用情态动词修饰断言（约翰一定在家）和使用主观元素（约翰明显在家）表明说话者正在操纵她的判断，在合作的语境中，意图将她的认知状态对话方透明化。真实性判断是基于两个组成部分形成的：（i）与现实相关的证据（外生证据）和（ii）与偏好和个人信念相关的内在证据。而“机械思维”缺乏这两个组成部分：（i）它们与现实无关，（ii）没有内在证据。因此它们缺乏对世界的信念形成和真实性判断的能力。

    The paper addresses the question of whether it is appropriate to talk about `mechanical minds' at all, and whether ChatGPT models can indeed be thought of as realizations of that. Our paper adds a semantic argument to the current debate. The act of human assertion requires the formation of a veridicality judgment. Modification of assertions with modals (John must be at home) and the use of subjective elements (John is obviously at home) indicate that the speaker is manipulating her judgments and, in a cooperative context, intends her epistemic state to be transparent to the addressee. Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs. `Mechanical minds' lack these two components: (i) they do not relate to reality and (ii) do not have endogenous evidence. Therefore they lack the ability to form a belief about the world and a veridicality judgmen
    
[^44]: TEDDY: 基于度量判别策略的边缘修剪方法

    TEDDY: Trimming Edges with Degree-based Discrimination strategY

    [https://rss.arxiv.org/abs/2402.01261](https://rss.arxiv.org/abs/2402.01261)

    TEDDY是一种利用边缘度量信息的边缘修剪方法，旨在通过一次性操作实现边缘稀疏化，进而鼓励参数稀疏化训练。这是一个解决图神经网络中抽奖票假设的时间效率和效果问题的创新方法。

    

    自从Chen等人在2021年提出用于图神经网络（GNNs）的抽奖票假设的开创性工作以来，寻找图抽奖票（GLT）的研究已成为GNN社区的重要关注点之一，激发了研究人员在实现与原始密集网络相当性能的同时，发现更稀疏的GLT。同时，图结构作为GNN训练动力学的重要因素，也受到了广泛关注，并得到了最近几项研究的阐明。尽管如此，目前关于GLT的研究通常没有充分利用图结构中的内在路径，并以迭代方式识别票数，这种方法耗时且效率低下。为解决这些限制，我们引入TEDDY，一种利用结构信息并整合边缘度量信息的一次性边缘稀疏化框架。在进行边缘稀疏化后，我们通过简单的投影梯度下降方法鼓励参数稀疏化训练。

    Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
    
[^45]: 利用深度学习的位置感知60 GHz毫米波波束成形技术用于车辆到车辆通信

    Position Aware 60 GHz mmWave Beamforming for V2V Communications Utilizing Deep Learning

    [https://rss.arxiv.org/abs/2402.01259](https://rss.arxiv.org/abs/2402.01259)

    本文提出了一种利用深度学习的方法，通过预测具有足够毫米波接收功率的最佳波束，使用车辆位置信息来实现车辆到车辆通信中的高效链路配置。

    

    波束成形技术是通过采用大规模天线阵列和生成窄波束来弥补毫米波通信中的严重路径损耗，以获得令人满意的接收功率的重要部分。然而，传统的波束选择方法主要依赖于信道状态信息来进行准确的波束对准，并为高效的链路配置带来显著的延迟和计算开销，在车辆到车辆（V2V）通信中如高度动态的场景中通常是不可行的。相比之下，利用带外上下文信息（如车辆位置信息）是减少这种开销的潜在替代方法。在这个背景下，本文提出了一种基于深度学习的解决方案，利用车辆位置信息来预测具有足够毫米波接收功率的最佳波束，从而可以主动确保最佳的V2V直视链路。

    Beamforming techniques are considered as essential parts to compensate the severe path loss in millimeter-wave (mmWave) communications by adopting large antenna arrays and formulating narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over such narrow beams for efficient link configuration by traditional beam selection approaches, mainly relied on channel state information, typically impose significant latency and computing overheads, which is often infeasible in vehicle-to-vehicle (V2V) communications like highly dynamic scenarios. In contrast, utilizing out-of-band contextual information, such as vehicular position information, is a potential alternative to reduce such overheads. In this context, this paper presents a deep learning-based solution on utilizing the vehicular position information for predicting the optimal beams having sufficient mmWave received powers so that the best V2V line-of-sight links can be ensured proactively. Afte
    
[^46]: 能够植入形状的联合嵌入是否能改进基于图像的三维扩散？

    Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?

    [https://rss.arxiv.org/abs/2402.01241](https://rss.arxiv.org/abs/2402.01241)

    该论文介绍了CISP（Contrastive Image Shape Pre training），通过将2D图像与3D形状在共享嵌入空间中对齐，增强了基于图像的3D形状合成。研究表明CISP能够捕捉CLIP的文本图像关注所忽视的3D特征，提高了生成质量。

    

    近期，在深度生成模型中，特别是应用CLIP（对比语言图像预训练）到去噪扩散概率模型（DDPMs）的生成图像的应用方面取得了显著的效果。CLIP的结构化嵌入空间也被扩展到图像到形状的生成，并取得了显著的结果。尽管取得了这些成功，但是仍然存在一些基本的问题：CLIP是否能确保从图像中生成最佳的形状？我们能否利用条件来将显式的三维知识引入生成过程并获得更好的质量？本研究介绍了CISP（对比图像形状预训练），旨在通过将2D图像与3D形状在共享嵌入空间中进行对齐来增强3D形状合成。CISP旨在丰富CLIP框架，通过捕捉CLIP的文本图像关注可能忽视的3D特征。我们的综合分析评估了CISP的质量。

    Recent advancements in deep generative models, particularly with the application of CLIP (Contrastive Language Image Pretraining) to Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable effectiveness in text to image generation. The well structured embedding space of CLIP has also been extended to image to shape generation with DDPMs, yielding notable results. Despite these successes, some fundamental questions arise: Does CLIP ensure the best results in shape generation from images? Can we leverage conditioning to bring explicit 3D knowledge into the generative process and obtain better quality? This study introduces CISP (Contrastive Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D images. CISP aims to enrich the CLIP framework by aligning 2D images with 3D shapes in a shared embedding space, specifically capturing 3D characteristics potentially overlooked by CLIP's text image focus. Our comprehensive analysis assesses CISP's gu
    
[^47]: 超越请求：利用HTTP响应头在不平衡环境中进行跨浏览器Web追踪器分类

    Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting

    [https://rss.arxiv.org/abs/2402.01240](https://rss.arxiv.org/abs/2402.01240)

    本研究通过利用HTTP响应头设计了机器学习分类器，在跨浏览器环境下有效检测Web追踪器，结果在Chrome和Firefox上表现出较高的准确性和性能。

    

    万维网的连通性主要归因于HTTP协议，其中的HTTP消息提供了有关网络安全和隐私的信息头字段，特别是关于Web追踪。尽管已有研究利用HTTP/S请求消息来识别Web追踪器，但往往忽视了HTTP/S响应头。本研究旨在设计使用HTTP/S响应头进行Web追踪器检测的有效机器学习分类器。通过浏览器扩展程序T.EX获取的Chrome、Firefox和Brave浏览器的数据作为我们的数据集。在Chrome数据上训练了11个监督模型，并在所有浏览器上进行了测试。结果表明，在Chrome和Firefox上具有高准确性、F1分数、精确度、召回率和最小对数损失误差的性能，但在Brave浏览器上表现不佳，可能是由于其不同的数据分布和特征集。研究表明，这些分类器可以用于检测Web追踪器。

    The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
    
[^48]: PRIME: 保护您的视频免受恶意编辑的影响

    PRIME: Protect Your Videos From Malicious Editing

    [https://rss.arxiv.org/abs/2402.01239](https://rss.arxiv.org/abs/2402.01239)

    提出了一种名为PRIME的保护方法，旨在显著降低保护视频的时间成本，并提高保护性能。评估结果表明，PRIME仅需8.3%的GPU时间成本。

    

    随着生成模型的发展，生成内容的质量不断提高。最近，开源模型使得操纵和编辑照片和视频变得非常容易，只需要几个简单的提示。虽然这些尖端技术变得越来越受欢迎，但它们也引发了人们对个人隐私和肖像权的担忧。恶意用户可以利用这些工具进行欺骗或非法行为。虽然一些之前的工作关注保护照片免受生成模型的影响，但我们发现在效率和效果方面，保护视频和图像之间仍存在差距。因此，我们引入了我们的保护方法PRIME，以显著降低时间成本并改善保护性能。此外，为了评估我们提出的保护方法，我们同时考虑了客观指标和人类主观指标。我们的评估结果表明，PRIME仅占了先前方案所需GPU时间的8.3%。

    With the development of generative models, the quality of generated content keeps increasing. Recently, open-source models have made it surprisingly easy to manipulate and edit photos and videos, with just a few simple prompts. While these cutting-edge technologies have gained popularity, they have also given rise to concerns regarding the privacy and portrait rights of individuals. Malicious users can exploit these tools for deceptive or illegal purposes. Although some previous works focus on protecting photos against generative models, we find there are still gaps between protecting videos and images in the aspects of efficiency and effectiveness. Therefore, we introduce our protection method, PRIME, to significantly reduce the time cost and improve the protection performance. Moreover, to evaluate our proposed protection method, we consider both objective metrics and human subjective metrics. Our evaluation results indicate that PRIME only costs 8.3% GPU hours of the cost of the pre
    
[^49]: 灵活的变分信息瓶颈：通过一次训练实现多样化压缩

    Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training

    [https://rss.arxiv.org/abs/2402.01238](https://rss.arxiv.org/abs/2402.01238)

    本研究引入了灵活的变分信息瓶颈（FVIB）框架，通过单次训练即可获得所有β值的最优模型，从而实现多样化压缩，并且在理论和实证方面证明了它的有效性。

    

    信息瓶颈是一种广泛使用的框架，可以从源随机变量中提取与目标随机变量相关的信息。在目标函数中，通过拉格朗日乘子β，信息瓶颈控制数据压缩和预测性之间的权衡。传统上，为了找到要学习的权衡，信息瓶颈需要通过多个训练周期来搜索β，这在计算上是昂贵的。在本研究中，我们引入了灵活的变分信息瓶颈（FVIB），这是一种用于分类任务的创新框架，可以通过单次计算高效的训练来获得所有β值的最优模型。我们在理论上证明，FVIB可以在合理的β值范围内同时最大化变分信息瓶颈（VIB）的目标函数的近似。然后，我们通过实验证明了FVIB可以像VI一样有效地学习VIB的目标。

    Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VI
    
[^50]: STAA-Net: 一种用于语音情感识别的稀疏可迁移对抗攻击模型

    STAA-Net: A Sparse and Transferable Adversarial Attack for Speech Emotion Recognition

    [https://rss.arxiv.org/abs/2402.01227](https://rss.arxiv.org/abs/2402.01227)

    本论文提出了一种生成器攻击方法，可以以端到端和高效的方式生成稀疏且可迁移的对抗样本来欺骗语音情感识别模型。

    

    语音包含人类情感丰富的信息，语音情感识别（SER）是人机交互领域的重要话题。SER模型的鲁棒性对于隐私敏感和可靠性要求高的领域尤为重要，例如私人医疗保健。最近，关于深度神经网络在音频领域对对抗攻击的脆弱性的研究成为一个热门领域。然而，在音频领域的先前对对抗攻击的研究主要依赖于迭代的基于梯度的技术，这些技术耗时且容易过拟合特定的威胁模型。此外，在音频领域对稀疏扰动的探索仍然有限，而稀疏扰动具有更好的隐蔽性潜力。为了解决这些挑战，我们提出了一种生成器攻击方法，以端到端和高效的方式生成稀疏且可迁移的对抗样本来欺骗SER模型。

    Speech contains rich information on the emotions of humans, and Speech Emotion Recognition (SER) has been an important topic in the area of human-computer interaction. The robustness of SER models is crucial, particularly in privacy-sensitive and reliability-demanding domains like private healthcare. Recently, the vulnerability of deep neural networks in the audio domain to adversarial attacks has become a popular area of research. However, prior works on adversarial attacks in the audio domain primarily rely on iterative gradient-based techniques, which are time-consuming and prone to overfitting the specific threat model. Furthermore, the exploration of sparse perturbations, which have the potential for better stealthiness, remains limited in the audio domain. To address these challenges, we propose a generator-based attack method to generate sparse and transferable adversarial examples to deceive SER models in an end-to-end and efficient manner. We evaluate our method on two widely-
    
[^51]: AI代码生成器与安全：朋友还是敌人?

    AI Code Generators for Security: Friend or Foe?

    [https://rss.arxiv.org/abs/2402.01219](https://rss.arxiv.org/abs/2402.01219)

    AI代码生成器在软件安全研究中提供了新的机会，但也存在被恶意滥用的风险。

    

    最近人工智能（AI）代码生成器的发展正在为软件安全研究带来新的机会，但也可能被恶意行为者滥用。我们回顾了AI代码生成器在安全领域的使用案例，并介绍了一个评估基准。

    Recent advances of artificial intelligence (AI) code generators are opening new opportunities in software security research, including misuse by malicious actors. We review use cases for AI code generators for security and introduce an evaluation benchmark.
    
[^52]: 基于深度学习的位置无关自适应降雨预测

    Location Agnostic Adaptive Rain Precipitation Prediction using Deep Learning

    [https://rss.arxiv.org/abs/2402.01208](https://rss.arxiv.org/abs/2402.01208)

    本研究提出了一种基于深度学习的自适应框架，能够解决降雨预测中的位置差异和气候变化带来的挑战。通过在巴黎、洛杉矶和东京进行适应，我们方法的预测能力分别提高了43.51%、5.09%和38.62%。

    

    降雨预测是一项具有挑战性的任务，因为它依赖于因地而异的气候和气象特征。因此，一个在一个位置表现良好的预测模型，在其他位置可能表现不佳，因为分布的变化。此外，由于全球变暖，天气模式年复一年地发生着快速变化，这可能导致即使在相同位置，时间过去后那些模型也变得无效。在我们的工作中，我们提出了一种自适应深度学习框架，以解决上述挑战。我们的方法能够推广模型，用于预测那些没有适应机制的方法无法预测降水的任何位置。我们的方法经过深度神经网络适应后，在巴黎、洛杉矶和东京的降水预测方面分别显示了43.51%、5.09%和38.62%的改进。

    Rain precipitation prediction is a challenging task as it depends on weather and meteorological features which vary from location to location. As a result, a prediction model that performs well at one location does not perform well at other locations due to the distribution shifts. In addition, due to global warming, the weather patterns are changing very rapidly year by year which creates the possibility of ineffectiveness of those models even at the same location as time passes. In our work, we have proposed an adaptive deep learning-based framework in order to provide a solution to the aforementioned challenges. Our method can generalize the model for the prediction of precipitation for any location where the methods without adaptation fail. Our method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a deep neural network for predicting the precipitation of Paris, Los Angeles, and Tokyo, respectively.
    
[^53]: 使用大型语言模型的高效因果图发现

    Efficient Causal Graph Discovery Using Large Language Models

    [https://rss.arxiv.org/abs/2402.01207](https://rss.arxiv.org/abs/2402.01207)

    提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。

    

    我们提出了一个新的框架，利用LLMs进行完整的因果图发现。之前基于LLM的方法采用了成对查询的方法，但这需要二次查询的数量，对于较大的因果图来说很快变得不可行。相反，提出的框架采用了广度优先搜索（BFS）的方法，只需要线性数量的查询。我们还展示了当有所观察数据可用时，提出的方法可以轻松地进行结合以提高性能。除了更具时间和数据效率外，提出的框架在不同大小的真实因果图上取得了最先进的结果。结果证明了提出方法在发现因果关系方面的有效性和效率，展示了其在不同领域的因果图发现任务中的广泛适用性潜力。

    We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
    
[^54]: 自监督学习在非连续表格数据中的应用调研

    A Survey on Self-Supervised Learning for Non-Sequential Tabular Data

    [https://rss.arxiv.org/abs/2402.01204](https://rss.arxiv.org/abs/2402.01204)

    本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    

    自监督学习（SSL）已经被应用于各个领域的许多最先进的模型中，其中SSL通过定义基于无标签数据集的预训练任务来学习上下文化和鲁棒的表示。最近，SSL已成为探索表格数据领域中表示学习能力的新趋势，这是一项更具挑战性的任务，因为它没有明确的关系来学习描述性的表示。本调研旨在系统地回顾和总结自监督学习在非连续表格数据（SSL4NS-TD）中的最新进展和挑战。首先，我们给出了NS-TD的正式定义，并阐明了它与相关研究的关联。然后，这些方法被分为三组——预测性学习、对比学习和混合学习，并介绍了每个方向的代表性方法的动机和优点。在此基础上，还介绍了SSL4NS-TD的应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
    
[^55]: 具有先验知识的少样本类增量学习

    Few-Shot Class-Incremental Learning with Prior Knowledge

    [https://rss.arxiv.org/abs/2402.01201](https://rss.arxiv.org/abs/2402.01201)

    该论文提出了一种具有先验知识的学习方法，通过引入从后续增量类别的无标签数据中产生的伪标签，与带标签的基类样本一起进行联合训练，有效地为旧类和新类数据分配嵌入空间，从而提高了模型对灾难性遗忘的韧性。

    

    为解决少样本类增量学习(FSCIL)中的灾难性遗忘和过拟合问题，之前的研究主要集中在在增量阶段保留旧知识的记忆上。这些研究经常低估了预训练模型在塑造增量学习有效性方面的作用。因此，为了增强预训练模型的泛化能力，我们提出了具有先验知识的学习(LwPK)，通过引入来自后续增量类别中少量无标签数据的几乎自由的先验知识。我们将无标签的增量类样本聚类，生成伪标签，并与带标签的基类样本进行联合训练，有效地为旧类和新类数据分配嵌入空间。实验结果表明，LwPK有效提高了模型对灾难性遗忘的韧性，基于经验风险最小化和类间距离度量的理论分析得到了验证。

    To tackle the issues of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL), previous work has primarily concentrated on preserving the memory of old knowledge during the incremental phase. The role of pre-trained model in shaping the effectiveness of incremental learning is frequently underestimated in these studies. Therefore, to enhance the generalization ability of the pre-trained model, we propose Learning with Prior Knowledge (LwPK) by introducing nearly free prior knowledge from a few unlabeled data of subsequent incremental classes. We cluster unlabeled incremental class samples to produce pseudo-labels, then jointly train these with labeled base class samples, effectively allocating embedding space for both old and new class data. Experimental results indicate that LwPK effectively enhances the model resilience against catastrophic forgetting, with theoretical analysis based on empirical risk minimization and class distance measurement corrob
    
[^56]: 条件化正则化流用于粗粒化分子表示的主动学习

    Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations

    [https://rss.arxiv.org/abs/2402.01195](https://rss.arxiv.org/abs/2402.01195)

    本文提出了使用条件化正则化流和主动学习的方法来解决粗粒化分子表示中的玻尔兹曼分布采样问题，相比传统的分子动力学模拟，该方法能够获得更高效的加速效果。

    

    高效采样分子系统的玻尔兹曼分布是一个长期存在的挑战。最近，与生成长时间分子动力学模拟不同，生成机器学习方法如正则化流被用于直接学习玻尔兹曼分布，而不需要样本。然而，这种方法容易出现模式崩溃，因此常常无法探索全部的构型空间。在这项工作中，我们将问题分为两个层次，细粒度和粗粒度自由度。在粗粒化空间上条件化正则化流可以产生两个层次之间的概率连接。为了探索构型空间，我们采用了粗粒化模拟与主动学习的方法，可以在必要时更新流并进行全原子势能评估。以丙氨酸二肽为例，我们展示了我们的方法相对于分子动力学模拟的加速效果。

    Efficient sampling of the Boltzmann distribution of molecular systems is a long-standing challenge. Recently, instead of generating long molecular dynamics simulations, generative machine learning methods such as normalizing flows have been used to learn the Boltzmann distribution directly, without samples. However, this approach is susceptible to mode collapse and thus often does not explore the full configurational space. In this work, we address this challenge by separating the problem into two levels, the fine-grained and coarse-grained degrees of freedom. A normalizing flow conditioned on the coarse-grained space yields a probabilistic connection between the two levels. To explore the configurational space, we employ coarse-grained simulations with active learning which allows us to update the flow and make all-atom potential energy evaluations only when necessary. Using alanine dipeptide as an example, we show that our methods obtain a speedup to molecular dynamics simulations of
    
[^57]: 通过去除GELU激活快速推断整数SWIN Transformer

    Faster Inference of Integer SWIN Transformer by Removing the GELU Activation

    [https://rss.arxiv.org/abs/2402.01169](https://rss.arxiv.org/abs/2402.01169)

    本研究通过去除SWIN Transformer中的GELU激活，提升了整数SWIN Transformer的推断速度。我们使用ReLU激活替代GELU，并使用迭代知识蒸馏来调整精度损失，最终实现了模型的量化和优化。

    

    SWIN transformer是一种突出的视觉transformer模型，在图像分类任务中具有最先进的准确性。尽管取得了成功，但其独特的架构导致与类似深度神经网络相比推断速度较慢。模型的整数量化是用于改善推断延迟的方法之一。然而，迄今为止还没有能够完全量化模型的最新方法。在这项工作中，我们通过去除Swin Transformer中与GELU激活相关的浮点运算来改善最新方法的推断延迟。虽然之前的工作提出用线性逼近函数替换非整数运算，但我们提出用ReLU激活代替GELU。ReLU相对于以前的方法的优势在于其内存和计算复杂性较低。我们使用迭代知识蒸馏来弥补由于用ReLU替换GELU而导致的精度损失。我们对去除GELU的SWIN transformer进行量化，并逐渐逼近最佳性能，并验证了我们方法的有效性。

    SWIN transformer is a prominent vision transformer model that has state-of-the-art accuracy in image classification tasks. Despite this success, its unique architecture causes slower inference compared with similar deep neural networks. Integer quantization of the model is one of the methods used to improve its inference latency. However, state-of-the-art has not been able to fully quantize the model. In this work, we improve upon the inference latency of the state-of-the-art methods by removing the floating-point operations, which are associated with the GELU activation in Swin Transformer. While previous work proposed to replace the non-integer operations with linear approximation functions, we propose to replace GELU with ReLU activation. The advantage of ReLU over previous methods is its low memory and computation complexity. We use iterative knowledge distillation to compensate for the lost accuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN transformer and sh
    
[^58]: 三维内容生成综述

    A Comprehensive Survey on 3D Content Generation

    [https://rss.arxiv.org/abs/2402.01166](https://rss.arxiv.org/abs/2402.01166)

    本综述对三维内容生成领域的发展进行了全面综合，提出了一种新的分类法，总结了目前的主要技术，并讨论了当前技术的局限性和未来工作的挑战和方向。

    

    近年来，人工智能生成内容（AIGC）取得了显著的进展，具有多种输入模态，如文本、图像、视频、音频和三维。三维是最接近真实世界三维环境的可视模态，并具有巨大的知识量。三维内容生成不仅具有学术和实践价值，而且面临着巨大的技术挑战。本综述旨在整合三维内容生成领域的发展。具体而言，提出了一种新的分类法，将现有方法分为三类：三维本机生成方法、基于二维先验的三维生成方法和混合三维生成方法。本综述涵盖了约60篇涵盖主要技术的论文。此外，我们讨论了当前三维内容生成技术的局限性，并指出了未来工作的挑战和有希望的方向。配合本综述，我们建立了一个项目网站，其中包含了相关资源和代码。

    Recent years have witnessed remarkable advances in artificial intelligence generated content(AIGC), with diverse input modalities, e.g., text, image, video, audio and 3D. The 3D is the most close visual modality to real-world 3D environment and carries enormous knowledge. The 3D content generation shows both academic and practical values while also presenting formidable technical challenges. This review aims to consolidate developments within the burgeoning domain of 3D content generation. Specifically, a new taxonomy is proposed that categorizes existing approaches into three types: 3D native generative methods, 2D prior-based 3D generative methods, and hybrid 3D generative methods. The survey covers approximately 60 papers spanning the major techniques. Besides, we discuss limitations of current 3D content generation techniques, and point out open challenges as well as promising directions for future work. Accompanied with this survey, we have established a project website where the 
    
[^59]: ReEvo：将大型语言模型作为具有反思演化的超启发式算法

    ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution

    [https://rss.arxiv.org/abs/2402.01145](https://rss.arxiv.org/abs/2402.01145)

    本文介绍了一种新的算法ReEvo，它利用大型语言模型作为超启发式算法的一种求解方法，通过反思设计方法和强大的演化搜索技术，显著提升了在组合优化问题上的搜索性能。

    

    NP困难组合优化问题的普遍存在推动领域专家参与试错式启发式设计过程。设计自动化的长期努力随着大型语言模型（LLM）的崛起而获得新的动力。本文介绍了语言超启发式算法（LHHs），它是超启发式算法的一种新变体，利用LLM进行启发式生成，具有最小的人工干预和开放式的启发式空间。为了增强LHHs的能力，我们提出了反思演化（ReEvo）：一种通用的搜索框架，模拟了人类专家的反思设计方法，并通过可扩展的LLM推理、互联网规模的领域知识和强大的进化搜索技术远远超越了人类的能力。在12个组合优化设置的评估中显示：1)演化的口头反思导致更平滑的适应度地形、黑盒组合优化问题设置的明确推理以及更好的搜索结果；2)ReEvo生成的启发式算法在分钟级优化时间内获得了可靠和优秀的性能。

    The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design process. The long-standing endeavor of design automation has gained new momentum with the rise of large language models (LLMs). This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution (ReEvo), a generic searching framework that emulates the reflective design approach of human experts while far surpassing human capabilities with its scalable LLM inference, Internet-scale domain knowledge, and powerful evolutionary search. Evaluations across 12 COP settings show that 1) verbal reflections for evolution lead to smoother fitness landscapes, explicit inference of black-box COP settings, and better search results; 2) heuristics generated by ReEvo in mi
    
[^60]: 用解缠离散图自编码器学习网络表示

    Learning Network Representations with Disentangled Graph Auto-Encoder

    [https://rss.arxiv.org/abs/2402.01143](https://rss.arxiv.org/abs/2402.01143)

    本文介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。

    

    (变分)图自编码器广泛用于学习图结构化数据的表示。然而，现实世界图的形成是一个由潜在因素影响的复杂和异质的过程。现有的编码器基本上是整体的，忽视了潜在因素的纠缠。这不仅使得图分析任务不太有效，而且使得理解和解释这些表示变得更加困难。用(变分)图自编码器学习解缠的图表示面临着重要挑战，在现有文献中尚未得到充分探索。在本文中，我们介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。具体地，我们首先设计了一个解缠的图卷积网络，使用多通道消息传递层作为编码器，聚合与每个节点相关的信息。

    The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to eac
    
[^61]: 基于神经Granger因果发现的微服务根本原因分析

    Root Cause Analysis In Microservice Using Neural Granger Causal Discovery

    [https://rss.arxiv.org/abs/2402.01140](https://rss.arxiv.org/abs/2402.01140)

    提出了一种基于神经Granger因果发现的方法来解决微服务中根本原因分析的挑战。

    

    近年来，由于其可扩展性、可维护性和灵活性，微服务在IT运营中得到了广泛应用。然而，当面临系统故障时，站点可靠性工程师很难找到根本原因，因为微服务中存在复杂的关系。先前的研究采用结构化学习方法（例如PC算法）来建立因果关系，并从因果图中得出根本原因。然而，他们忽略了时间序列数据的时间顺序，并未利用时间关系中蕴含的丰富信息。例如，在CPU利用率突然增加的情况下，可能会导致其他微服务的延迟增加。然而，在这种情况下，CPU利用率异常发生在延迟增加之前，而不是同时发生。结果，PC算法无法捕捉这样的特征。为了解决这些挑战，我们提出了RUN，一种新的方法。

    In recent years, microservices have gained widespread adoption in IT operations due to their scalability, maintenance, and flexibility. However, it becomes challenging for site reliability engineers (SREs) to pinpoint the root cause due to the complex relationships in microservices when facing system malfunctions. Previous research employed structured learning methods (e.g., PC-algorithm) to establish causal relationships and derive root causes from causal graphs. Nevertheless, they ignored the temporal order of time series data and failed to leverage the rich information inherent in the temporal relationships. For instance, in cases where there is a sudden spike in CPU utilization, it can lead to an increase in latency for other microservices. However, in this scenario, the anomaly in CPU utilization occurs before the latency increase, rather than simultaneously. As a result, the PC-algorithm fails to capture such characteristics. To address these challenges, we propose RUN, a novel a
    
[^62]: DeepAAT: 快速无人机地图制作的深度自动航空三角测量

    DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping

    [https://rss.arxiv.org/abs/2402.01134](https://rss.arxiv.org/abs/2402.01134)

    DeepAAT是一个专门为无人机影像AAT设计的深度学习网络，通过考虑影像的空间和光谱特征，提高了AAT的效率和精度。

    

    自动航空三角测量（AAT）旨在同时恢复图像姿态和重建稀疏点，对地球观测至关重要。凭借其在摄影测量领域数十年的研究积淀，AAT已经发展成为大规模无人机地图制作中广泛应用的基本过程。然而，传统的AAT方法仍面临效率低和稳健性有限等挑战。本文介绍了DeepAAT，这是一个专门为无人机影像AAT设计的深度学习网络。DeepAAT考虑了影像的空间和光谱特征，增强了解决错误匹配对和准确预测图像姿态的能力。DeepAAT在AAT的效率方面取得了重大突破，确保了场景的全面覆盖和精度。其处理速度比增量AAT方法快几百倍，比全局AAT方法快几十倍，同时保持可比的重建水平。

    Automated Aerial Triangulation (AAT), aiming to restore image pose and reconstruct sparse points simultaneously, plays a pivotal role in earth observation. With its rich research heritage spanning several decades in photogrammetry, AAT has evolved into a fundamental process widely applied in large-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its advancements, classic AAT methods still face challenges like low efficiency and limited robustness. This paper introduces DeepAAT, a deep learning network designed specifically for AAT of UAV imagery. DeepAAT considers both spatial and spectral characteristics of imagery, enhancing its capability to resolve erroneous matching pairs and accurately predict image poses. DeepAAT marks a significant leap in AAT's efficiency, ensuring thorough scene coverage and precision. Its processing speed outpaces incremental AAT methods by hundreds of times and global AAT methods by tens of times while maintaining a comparable level of reconstruct
    
[^63]: Pok\'eLLMon：一个用于使用大型语言模型的Pok\'emon对战的与人类能力相当的代理机器人

    Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models

    [https://rss.arxiv.org/abs/2402.01118](https://rss.arxiv.org/abs/2402.01118)

    Pok\'eLLMon是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人。它通过上下文强化学习、知识增强生成和一致的行动生成的策略，展现了与人类类似的战斗策略和及时决策，并在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。

    

    我们介绍了\textsc{Pok\'eLLMon}，这是第一个在战术对战游戏中实现人类能力的语言模型化身代理机器人，同时以Pok\'emon对战为例进行了证明。 \textsc{Pok\'eLLMon}的设计采用了三个关键策略：（i）上下文强化学习，即即时使用从对战中获得的基于文本的反馈来逐步完善策略；（ii）知识增强生成，即检索外部知识以对抗产生幻觉现象，并使代理机器人能够及时正确地行动；（iii）一致的行动生成，以减轻代理机器人面对强敌时的“惊慌换手”现象，使其可以逃避战斗。我们展示了与人类进行的在线对战中，\textsc{Pok\'eLLMon}采用与人类类似的战斗策略和及时决策，其在Ladder比赛中达到了49%的胜率，在邀请对战中达到了56%的胜率。我们的实现和可玩的战斗日志可以在以下链接中找到：\url{https://gith

    We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
    
[^64]: 双重防御：使用迁移学习和随机化来防止仅基于标签的成员推断攻击

    Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization

    [https://rss.arxiv.org/abs/2402.01114](https://rss.arxiv.org/abs/2402.01114)

    这项研究探索了使用迁移学习和随机化来防止成员推断攻击。通过在过拟合的深度神经网络上应用双重防御，我们可以提高模型的隐私性而不降低准确度。

    

    迁移学习已经被证明在面对训练样本稀缺的情况下，可以提高深度神经网络模型的性能。然而，将迁移学习作为解决过拟合深度神经网络容易受到隐私攻击的方法的合适性尚未被探索。一类隐私攻击称为成员推断攻击旨在确定给定样本是否属于训练数据集（成员）或不属于（非成员）。我们引入了双重防御，一个系统性的实证研究，研究在不降低分类准确度的情况下使用迁移学习（阶段1）结合随机化（阶段2）来防止成员推断攻击对过拟合的深度神经网络的影响。我们的研究考察了源模型和目标模型之间的共享特征空间和参数值、冻结层的数量以及预训练模型的复杂性。我们在三个（目标，源）数据集对上评估了双重防御：（i）（CIFAR-10，ImageNet），（ii）（GTSRB，ImageNet），（iii）（CelebA，VGGFace2）。我们考虑了四个公开可用的预训练深度神经网络：（a）VGG-19，

    Transfer learning (TL) has been demonstrated to improve DNN model performance when faced with a scarcity of training samples. However, the suitability of TL as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is unexplored. A class of privacy attacks called membership inference attacks (MIAs) aim to determine whether a given sample belongs to the training dataset (member) or not (nonmember). We introduce Double-Dip, a systematic empirical study investigating the use of TL (Stage-1) combined with randomization (Stage-2) to thwart MIAs on overfitted DNNs without degrading classification accuracy. Our study examines the roles of shared feature space and parameter values between source and target models, number of frozen layers, and complexity of pretrained models. We evaluate Double-Dip on three (Target, Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii) (CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a) VGG-19,
    
[^65]: 在自适应约束下的自对弈强化学习中的近乎最优解

    Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints

    [https://rss.arxiv.org/abs/2402.01111](https://rss.arxiv.org/abs/2402.01111)

    本文研究了具有自适应约束的多智能体强化学习问题，并提出了一种基于消除算法，将后悔控制在$\widetilde{O}(\sqrt{H^3 S^2 ABK})$，批量复杂度为$O(H+\log\log K)$。此外，还给出了所有具有$\widetilde{O}(\sqrt{K})$后悔界算法的批量复杂度下界。

    

    我们研究了具有自适应约束的多智能体强化学习问题（MARL） - 这是一种由实际应用驱动的新问题，其中部署新策略是昂贵的，并且必须最小化策略更新的次数。对于两个玩家的零和马尔可夫博弈，我们设计了一种（策略）基于消除的算法，它在后悔为$\widetilde{O}(\sqrt{H^3 S^2 ABK})$的情况下，批量复杂度仅为$O(H+\log\log K)$。在上述情况下，$S$表示状态数，$A，B$分别代表两个玩家的行动数，$H$是时间周期，$K$是游戏次数。此外，我们证明了对于所有具有$\widetilde{O}(\sqrt{K})$后悔界的算法，一种批量复杂度的下界为$\Omega(\frac{H}{\log_{A}K}+\log\log K)$，这与我们的上界在对数因子上匹配。作为副产品，我们的技术自然地扩展到学习赌博博弈和无奖励的近乎最优批量复杂度MARL。据我们所知，这些是迄今为止最好的成果。

    We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these 
    
[^66]: 使用循环变压器模拟图算法

    Simulation of Graph Algorithms with Looped Transformers

    [https://rss.arxiv.org/abs/2402.01107](https://rss.arxiv.org/abs/2402.01107)

    本文研究了使用循环变压器网络模拟图算法的能力，证明了该结构可以模拟Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法，并展示了其具有图灵完整性的结果。

    

    最近，使用神经网络执行图算法引起了很大的兴趣，由于有了令人满意的实证进展。这促使我们进一步了解神经网络如何能够使用关系数据复制推理步骤。在这项工作中，我们从理论角度研究了变压器网络模拟图算法的能力。我们使用的架构是一个带额外注意力头和与图形交互的循环变压器。我们通过构造证明了这种架构能够模拟诸如Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法等算法。网络的宽度不随输入图的大小增加，这意味着网络可以模拟任何图上的上述算法。尽管有这个特性，我们展示了在我们的解决方案中有一个由于有限精度而受到限制的模拟极限。最后，我们展示了我们的解决方案具有图灵完整性的结果。

    The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
    
[^67]: 组合生成建模：单一模型并不是您所需要的全部

    Compositional Generative Modeling: A Single Model is Not All You Need

    [https://rss.arxiv.org/abs/2402.01103](https://rss.arxiv.org/abs/2402.01103)

    本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。

    

    在人工智能研究中，通过训练大规模的巨大的生成模型来处理海量数据已经成为一种越来越主流的方法。本文中，我们认为我们应该通过将较小的生成模型组合在一起来构建大型生成系统。我们展示了这种组合生成方法如何以更高效的方式学习分布，使得我们在训练时未见的数据分布部分也能进行泛化。我们进一步展示了这种方法如何使我们能够为训练时完全未见的任务编写和构建新的生成模型。最后，我们展示在许多情况下，我们可以从数据中发现独立的组合组件。

    Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
    
[^68]: 可信的分布式AI系统：鲁棒性、隐私和治理

    Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance

    [https://rss.arxiv.org/abs/2402.01096](https://rss.arxiv.org/abs/2402.01096)

    本文回顾了可信的分布式AI的代表性技术，包括鲁棒性保证、隐私保护和公平意识，以解决分布式学习中存在的安全、隐私和公平问题。

    

    新兴的分布式AI系统正在革新大数据计算和数据处理能力，并对经济和社会产生越来越大的影响。然而，最近的研究发现，AI系统中的安全、隐私和公平问题导致了新的攻击面和风险。在本文中，我们通过鲁棒性保证、隐私保护和公平意识在分布式学习中回顾了代表性的技术、算法和理论基础，以实现可信的分布式AI。我们首先提供了分布式学习的替代架构的简要概述，讨论了分布式学习中AI算法的安全、隐私和公平的固有漏洞，并分析了为什么这些问题在分布式学习中存在，而不管具体的架构如何。然后，我们提供了针对可信分布式AI的独特分类，涵盖了对推理中的逃避攻击和不规则查询的鲁棒性，以及对中毒攻击和数据泄露的隐私保护。

    Emerging Distributed AI systems are revolutionizing big data computing and data processing capabilities with growing economic and societal impact. However, recent studies have identified new attack surfaces and risks caused by security, privacy, and fairness issues in AI systems. In this paper, we review representative techniques, algorithms, and theoretical foundations for trustworthy distributed AI through robustness guarantee, privacy protection, and fairness awareness in distributed learning. We first provide a brief overview of alternative architectures for distributed learning, discuss inherent vulnerabilities for security, privacy, and fairness of AI algorithms in distributed learning, and analyze why these problems are present in distributed learning regardless of specific architectures. Then we provide a unique taxonomy of countermeasures for trustworthy distributed AI, covering (1) robustness to evasion attacks and irregular queries at inference, and robustness to poisoning a
    
[^69]: 深度神经网络预测使用多少个视图？

    How many views does your deep neural network use for prediction?

    [https://rss.arxiv.org/abs/2402.01095](https://rss.arxiv.org/abs/2402.01095)

    本文提出了最小有效视图（MSVs）的概念，该概念类似于多视图，但适用于实际图像，并且通过实证研究表明，MSV的数量与模型的预测准确性之间存在关系。

    

    尽管进行了许多理论和实证分析，但深度神经网络（DNN）的泛化能力仍未完全理解。最近，Allen-Zhu和Li（2023）引入了多视图的概念来解释DNN的泛化能力，但他们的主要目标是集成或蒸馏模型，并未讨论用于特定输入预测的多视图估计方法。在本文中，我们提出了最小有效视图（MSVs），它类似于多视图，但可以高效地计算真实图像。MSVs是输入中的一组最小且不同的特征，每个特征保留了模型对该输入的预测。我们通过实证研究表明，不同模型（包括卷积和转换模型）的MSV数量与预测准确性之间存在明确的关系，这表明多视图的角度对于理解（非集成或非蒸馏）DNN的泛化能力也很重要。

    The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu & Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.
    
[^70]: 电子健康记录预测建模的最新进展

    Recent Advances in Predictive Modeling with Electronic Health Records

    [https://rss.arxiv.org/abs/2402.01077](https://rss.arxiv.org/abs/2402.01077)

    这项调查总结了基于电子健康记录数据的深度学习预测模型的最新进展，包括背景介绍、数学定义、分类总结、基准和工具包，以及未来研究方向的讨论。

    

    电子健康记录（EHR）系统的发展使得大量的数字化患者数据得以收集。然而，由于其独特的特性，利用EHR数据进行预测建模面临着一些挑战。随着机器学习技术的进步，深度学习在包括医疗在内的各个领域展现出了卓越的优势。本调查系统地回顾了基于EHR数据的深度学习预测模型的最新进展。具体而言，我们首先介绍了EHR数据的背景，并提供了预测建模任务的数学定义。然后，我们从多个角度对预测深度模型进行分类和总结。此外，我们还介绍了与医疗预测建模相关的基准和工具包。最后，我们讨论了开放性挑战，并提出了未来研究的有希望的方向。

    The development of electronic health records (EHR) systems has enabled the collection of a vast amount of digitized patient data. However, utilizing EHR data for predictive modeling presents several challenges due to its unique characteristics. With the advancements in machine learning techniques, deep learning has demonstrated its superiority in various applications, including healthcare. This survey systematically reviews recent advances in deep learning-based predictive models using EHR data. Specifically, we begin by introducing the background of EHR data and providing a mathematical definition of the predictive modeling task. We then categorize and summarize predictive deep models from multiple perspectives. Furthermore, we present benchmarks and toolkits relevant to predictive modeling in healthcare. Finally, we conclude this survey by discussing open challenges and suggesting promising directions for future research.
    
[^71]: 用于多语言文档问答的大型语言模型评估方法

    Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer

    [https://rss.arxiv.org/abs/2402.01065](https://rss.arxiv.org/abs/2402.01065)

    本文研究了大型语言模型（LLMs）在多语言环境下的能力。研究结果表明，将原始语言的上下文、问题和答案翻译成高资源语言可以获得最佳效果。

    

    随着大型语言模型（LLMs）的广泛应用，本文研究了这些模型的多语言能力。我们的初步结果显示，将原始语言的上下文、问题和答案翻译成高资源语言产生了最好的结果。

    With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models. Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results.
    
[^72]: 面向双重目标对话设置的计划驱动大型语言模型

    Plan-Grounded Large Language Models for Dual Goal Conversational Settings

    [https://rss.arxiv.org/abs/2402.01053](https://rss.arxiv.org/abs/2402.01053)

    本研究针对双重目标对话设置提出了一种新型的计划驱动大型语言模型，该模型能够在任意计划上基础对话，主动引导用户完成计划，并在系统行为上实施安全防护。

    

    训练大型语言模型（LLM）遵循用户指令已被证明可以为LLM提供充足的能力以流利地进行对话并与人类对齐。然而，在双向对话流动指令的混合倡议设置中，LLM如何引导以计划为基础的对话还不完全清楚，即LLM和用户彼此提供指令。在本文中，我们解决了双重目标混合倡议对话设置的问题，LLM不仅在任意计划上基础对话，还致力于满足流程计划和用户指令。LLM负责引导用户完成计划，同时适应新的情况，回答问题，并在需要时激活安全防护措施。我们提出了一种新颖的LLM，它基于流程计划来进行对话，可以主动参与对话，并在系统行为上实施安全防护。

    Training Large Language Models (LLMs) to follow user instructions has been shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system's behavior, while also improving the 
    
[^73]: 跟着我重复：Transformer在复制任务上比状态空间模型更好

    Repeat After Me: Transformers are Better than State Space Models at Copying

    [https://rss.arxiv.org/abs/2402.01032](https://rss.arxiv.org/abs/2402.01032)

    这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。

    

    Transformer是序列建模的主要架构，但对于使用不依赖于序列长度的固定大小潜在状态的模型，也就是"广义状态空间模型" (GSSMs)，引起了越来越多的关注。在本文中，我们展示了虽然GSSMs在推理时间效率上有优势，但在需要从输入上下文复制的任务上，它们相对于transformer模型来说有限制。我们从对简单的字符串复制任务的理论分析开始，并证明了一个两层的transformer可以复制指数长度的字符串，而GSSMs由于其固定大小的潜在状态在根本上是有限制的。实证上，我们发现transformer在需要复制上下文的合成任务中，在效率和泛化性能上优于GSSMs。最后，我们评估了预训练的大型语言模型，并发现transformer模型在复制和检索上下文信息方面远远优于状态空间模型。

    Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
    
[^74]: 可执行代码行动能够激发更出色的LLM智能体

    Executable Code Actions Elicit Better LLM Agents

    [https://rss.arxiv.org/abs/2402.01030](https://rss.arxiv.org/abs/2402.01030)

    使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。

    

    大型语言模型（LLM）智能体具备执行广泛行动的能力，如调用工具和控制机器人等，在解决现实世界的挑战中显示出巨大潜力。LLM智能体通常通过生成JSON或文本的预定义格式来产生行动，这通常受限于受限制的行动空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本研究提出使用可执行的Python代码将LLM智能体的行动整合到一个统一的行动空间中（CodeAct）。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互在新的观察中动态修订先前的行动或发出新的行动。我们对17个LLM在API-Bank和新编制的基准测试中进行了广泛分析，结果显示CodeAct的性能超过了广泛使用的替代方案（成功率高出20%）。CodeAct的令人鼓舞的表现激励我们建立了一个开源的...

    Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
    
[^75]: 通过ologs和接线图量化概念的类比

    Quantifying analogy of concepts via ologs and wiring diagrams

    [https://rss.arxiv.org/abs/2402.01020](https://rss.arxiv.org/abs/2402.01020)

    本文通过ologs和接线图的概念，建立了一个量化概念类比的框架，使得自主系统能够形成抽象概念，并通过图论和范畴论的技术进行比较和操作，同时使用接线图操作定义了度量和图编辑距离。

    

    我们在Spivak和Kent创建的本体日志(ologs)理论的基础上，定义了一种称为接线图的概念。在本文中，接线图是一个有限的有向标记图。标记对应于olog中的类型；它们也可以被解释为自主系统中传感器的读数。因此，接线图可以用作自主系统形成抽象概念的框架。我们展示了基于骨架接线图的图形形成一个范畴。这使得骨架接线图可以使用图论和范畴论的技术进行比较和操作。我们还通过使用仅适用于接线图的操作将传统的图编辑距离定义扩展到接线图的情况，从而得到了所有骨架接线图集合上的度量。最后，我们给出了一个计算由接线图表示的两个概念之间距离的扩展示例，并解释了如何将我们的框架应用于任何应用领域。

    We build on the theory of ontology logs (ologs) created by Spivak and Kent, and define a notion of wiring diagrams. In this article, a wiring diagram is a finite directed labelled graph. The labels correspond to types in an olog; they can also be interpreted as readings of sensors in an autonomous system. As such, wiring diagrams can be used as a framework for an autonomous system to form abstract concepts. We show that the graphs underlying skeleton wiring diagrams form a category. This allows skeleton wiring diagrams to be compared and manipulated using techniques from both graph theory and category theory. We also extend the usual definition of graph edit distance to the case of wiring diagrams by using operations only available to wiring diagrams, leading to a metric on the set of all skeleton wiring diagrams. In the end, we give an extended example on calculating the distance between two concepts represented by wiring diagrams, and explain how to apply our framework to any applica
    
[^76]: AI生成的面孔摆脱了种族和性别刻板印象

    AI-generated faces free from racial and gender stereotypes

    [https://rss.arxiv.org/abs/2402.01002](https://rss.arxiv.org/abs/2402.01002)

    这项研究发现并解决了AI生成的面孔中存在的种族和性别刻板印象问题，提出了分类器用于预测面部属性的方法，并提出了有效的去偏见解决方案。

    

    诸如Stable Diffusion之类的文本到图像生成AI模型每天都被全球数百万人使用。然而，许多人对这些模型如何放大种族和性别刻板印象提出了关切。为了研究这一现象，我们开发了一个分类器来预测任意给定面部图像的种族、性别和年龄组，并展示其达到了最先进的性能。利用这个分类器，我们对Stable Diffusion在六种种族、两种性别、五个年龄组、32个职业和八个属性上的偏见进行了量化。然后，我们提出了超越最先进替代方案的新型去偏见解决方案。此外，我们还检查了Stable Diffusion在描绘同一种族的个体时相似程度。分析结果显示出高度的刻板印象，例如，将大多数中东男性描绘为皮肤黝黑、留着胡子、戴着传统头饰。我们提出了另一种增加面部多样性的新型解决方案来解决这些限制。

    Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, many have raised concerns regarding how these models amplify racial and gender stereotypes. To study this phenomenon, we develop a classifier to predict the race, gender, and age group of any given face image, and show that it achieves state-of-the-art performance. Using this classifier, we quantify biases in Stable Diffusion across six races, two genders, five age groups, 32 professions, and eight attributes. We then propose novel debiasing solutions that outperform state-of-the-art alternatives. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals a high degree of stereotyping, e.g., depicting most middle eastern males as being dark-skinned, bearded, and wearing a traditional headdress. We address these limitations by proposing yet another novel solution that increases facial div
    
[^77]: 一种信息论方法来分析自然语言处理分类任务

    An Information-Theoretic Approach to Analyze NLP Classification Tasks

    [https://rss.arxiv.org/abs/2402.00978](https://rss.arxiv.org/abs/2402.00978)

    本研究提供了一个信息论框架来分析文本分类任务中输入的影响，发现在更具挑战性的数据集中，上下文对输出的影响减小，同时语义含义对于分类任务的影响较大。

    

    理解输入对输出的重要性对许多任务都有帮助。本研究提供了一个信息论框架来分析文本分类任务中输入的影响。自然语言处理（NLP）任务采用单个元素输入或多个元素输入来预测输出变量，其中一个元素是一段文字。每个文字元素有两个组成部分：关联的语义含义和语言实现。选择了多项选择阅读理解（MCRC）和情感分类（SC）来展示该框架。对于MCRC，发现相对于问题的影响，上下文对输出的影响在更具挑战性的数据集上减小。特别是，更具挑战性的上下文允许问题的复杂性更大的变化。因此，测试创建者在设计多项选择问题时需要仔细考虑上下文的选择。对于SC，发现语义含义对于输出的影响较大。

    Understanding the importance of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single element input or multiple element inputs to predict an output variable, where an element is a block of text. Each text element has two components: an associated semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the context influence on the output compared to the question influence reduces on more challenging datasets. In particular, more challenging contexts allow a greater variation in complexity of questions. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of 
    
[^78]: 具有动态停止的循环Transformer

    Recurrent Transformers with Dynamic Halt

    [https://rss.arxiv.org/abs/2402.00976](https://rss.arxiv.org/abs/2402.00976)

    本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。

    

    本文研究了两种主要方法在增强Transformer与循环机制方面的归纳偏好——（1）类似于Universal Transformers的深度逐层循环方法；和（2）类似于Temporal Latent Bottleneck的分块时态循环方法。此外，我们提出并研究了扩展和组合上述方法的新方式，例如，我们提出了一种基于全局均值的Universal Transformer动态停止机制，并将Universal Transformer的元素融入到Temporal Latent Bottleneck中。我们通过多个诊断任务（如Long Range Arena（LRA），翻转-翻转语言建模，ListOps和逻辑推理）比较了模型并探索了它们的归纳偏好。

    In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
    
[^79]: 使用Entity预训练GPT为KG问答生成SPARQL

    SPARQL Generation with Entity Pre-trained GPT for KG Question Answering

    [https://rss.arxiv.org/abs/2402.00969](https://rss.arxiv.org/abs/2402.00969)

    本文面向非程序员用户的KG问答问题，通过实体链接和GPT模型生成SPARQL查询，使用CWA预训练所有实体，实现了准确的SPARQL匹配率为62.703%。

    

    知识图谱的流行度在过去几年中迅速增长。人们可以通过互联网上的许多在线数据库查询这些知识。但是，如果非程序员用户能够访问他们想要知道的任何信息，那将是一个巨大的成就。为了解决这个问题，已经付出了很多努力，使用自然语言处理工具和通过许多挑战激励创造力。我们的方法重点是在自然语言问题上进行正确的实体链接，并训练一个GPT模型来从中创建SPARQL查询。我们成功地确定了这个任务中可能最难以在少数或零次尝试中解决的属性，并提出对所有实体进行预训练（在CWA下）以提高性能。在3次尝试中，我们在测试中获得了62.703%的准确的SPARQL匹配率，在实体链接挑战中获得了0.809的F1值，在问题回答挑战中获得了0.009的F1值。

    Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.
    
[^80]: 信任学习理论

    Credal Learning Theory

    [https://rss.arxiv.org/abs/2402.00957](https://rss.arxiv.org/abs/2402.00957)

    本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。

    

    统计学习理论是机器学习的基础，为从未知概率分布中学习到的模型的风险提供理论边界。然而，在实际部署中，数据分布可能会变化，导致领域适应/泛化问题。在本文中，我们建立了一个“信任”学习理论的基础，使用概率的凸集（信任集）来建模数据生成分布的变异性。我们认为，这样的信任集可以从有限样本的训练集中推断出来。对于有限假设空间（无论是否可实现）和无限模型空间，推导出界限，这直接推广了经典结果。

    Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
    
[^81]: NCoder -- 一种基于量子场论的数据编码方法

    NCoder -- A Quantum Field Theory approach to encoding data

    [https://rss.arxiv.org/abs/2402.00944](https://rss.arxiv.org/abs/2402.00944)

    NCoder 是一种基于量子场论的数据编码方法，通过修改自编码神经网络的结构，将潜在层规定为一组 $n$-点关联函数的子集，模拟了通过费曼图按阶展开构建理论有效作用量的过程。同时，NCoder 也可以看作是模拟统计推断过程的方法，通过几个较低维度的统计量来总结高维度数据，并从这些统计量中推断数据生成分布。这种方法提示了扰动重整化和模型的充分性之间的有趣对应关系。

    

    在本文中，我们提出了一种基于量子场论（QFT）的可解释 AI 方法，称为 NCoder。NCoder 是一个修改过的自编码神经网络，其潜在层被规定为一个 $n$-点关联函数的子集。将图像视为格点场论的抽样，这个架构模拟了使用费曼图按阶展开来扰动地构建理论有效作用量的任务。或者，NCoder 可以被视为模拟统计推断过程，通过用几个较低维度的汇总统计量（这里是 $n$-点关联函数）来总结高维度数据，并从这些统计量推断数据生成分布来生成样本外数据。通过这种方式，NCoder 提示了扰动重整化和模型的充分性之间的有趣对应关系。我们展示了 NCoder 在数据编码任务上的有效性。

    In this paper we present a novel approach to interpretable AI inspired by Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified autoencoder neural network whose latent layer is prescribed to be a subset of $n$-point correlation functions. Regarding images as draws from a lattice field theory, this architecture mimics the task of perturbatively constructing the effective action of the theory order by order in an expansion using Feynman diagrams. Alternatively, the NCoder may be regarded as simulating the procedure of statistical inference whereby high dimensional data is first summarized in terms of several lower dimensional summary statistics (here the $n$-point correlation functions), and subsequent out-of-sample data is generated by inferring the data generating distribution from these statistics. In this way the NCoder suggests a fascinating correspondence between perturbative renormalizability and the sufficiency of models. We demonstrate the efficacy of the
    
[^82]: MUSTAN：基于多尺度时间上下文的注意力机制的鲁棒视频前景分割

    MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation

    [https://rss.arxiv.org/abs/2402.00918](https://rss.arxiv.org/abs/2402.00918)

    本文提出了一种基于多尺度时间上下文的注意力机制（MUSTAN），用于改善视频前景分割的泛化性能。通过整合视频的时间信息和空间线索，我们的方法在深度学习架构中实现了鲁棒的前景分割。

    

    视频前景分割是一种重要的计算机视觉任务，旨在从背景中分割出运动中的物体。大多数现有方法只依赖于空间线索而忽略了运动线索，因此容易过度拟合训练数据，泛化能力差。为了解决这个问题，先前的研究利用了光流、背景减除遮罩等多种线索。然而，光流等视频数据的标注是一项具有挑战性的任务。本文利用视频数据中的时间信息和空间线索来改善泛化性能，但是如何以可解释的方式对视频数据的时间信息建模是一个挑战。因此，我们设计了一种策略，将视频的时间上下文整合到视频前景分割的开发中。我们的方法产生了深度学习架构。

    Video foreground segmentation (VFS) is an important computer vision task wherein one aims to segment the objects under motion from the background. Most of the current methods are image-based, i.e., rely only on spatial cues while ignoring motion cues. Therefore, they tend to overfit the training data and don't generalize well to out-of-domain (OOD) distribution. To solve the above problem, prior works exploited several cues such as optical flow, background subtraction mask, etc. However, having a video data with annotations like optical flow is a challenging task. In this paper, we utilize the temporal information and the spatial cues from the video data to improve OOD performance. However, the challenge lies in how we model the temporal information given the video data in an interpretable way creates a very noticeable difference. We therefore devise a strategy that integrates the temporal context of the video in the development of VFS. Our approach give rise to deep learning architect
    
[^83]: 用于安全自助大型语言模型探索的机构平台

    Institutional Platform for Secure Self-Service Large Language Model Exploration

    [https://rss.arxiv.org/abs/2402.00913](https://rss.arxiv.org/abs/2402.00913)

    这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。

    

    本文介绍了由肯塔基大学应用人工智能中心开发的用户友好型平台，旨在使大型定制语言模型（LLM）更易于使用。通过利用最近在多LoRA推理方面的进展，系统有效地适应了各类用户和项目的定制适配器。论文概述了系统的架构和关键特性，包括数据集策划、模型训练、安全推理和基于文本的特征提取。我们通过使用基于代理的方法建立了一个基于租户意识的计算网络，在安全地利用孤立资源岛的基础上形成了一个统一的系统。该平台致力于提供安全的LLM服务，强调过程和数据隔离、端到端加密以及基于角色的资源身份验证。该贡献与实现简化访问先进的AI模型和技术以支持科学发现的总体目标一致。

    This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
    
[^84]: 能够约束概念瓶颈模型学习语义上有意义的输入特征吗？

    Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?

    [https://rss.arxiv.org/abs/2402.00912](https://rss.arxiv.org/abs/2402.00912)

    本文研究了概念瓶颈模型如何从具有细粒度概念注释的数据集中学习概念，以实现模型输出的内在可解释性。

    

    概念瓶颈模型（CBM）被认为具有内在的可解释性，因为它们首先预测一组人为定义的概念，然后利用这些概念来预测下游任务的输出。为了实现完全的内在可解释性，以及确保对模型输出的信任，我们需要保证概念的预测是基于语义映射的输入特征。例如，人们可能期望图像中表示骨折的像素被用于预测骨折。然而，当前的文献表明这并不是事实，因为概念预测通常与不相关的输入特征映射在一起。我们假设这是由于概念注释的不准确或者输入特征与概念之间的关系不清晰导致的。总的来说，数据集标注对CBMs中概念表示的影响仍然是一个研究较少的领域。因此，在本文中，我们研究了CBMs如何从具有细粒度概念注释的数据集中学习概念的问题。我们进行了实验演示。

    Concept Bottleneck Models (CBMs) are considered inherently interpretable because they first predict a set of human-defined concepts before using these concepts to predict the output of a downstream task. For inherent interpretability to be fully realised, and ensure trust in a model's output, we need to guarantee concepts are predicted based on semantically mapped input features. For example, one might expect the pixels representing a broken bone in an image to be used for the prediction of a fracture. However, current literature indicates this is not the case, as concept predictions are often mapped to irrelevant input features. We hypothesise that this occurs when concept annotations are inaccurate or how input features should relate to concepts is unclear. In general, the effect of dataset labelling on concept representations in CBMs remains an understudied area. Therefore, in this paper, we examine how CBMs learn concepts from datasets with fine-grained concept annotations. We demo
    
[^85]: 通过集成学习和正则化微调应对偏见

    Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning

    [https://rss.arxiv.org/abs/2402.00910](https://rss.arxiv.org/abs/2402.00910)

    本文提出了一种通过集成学习和正则化微调的方法，解决AI模型中的偏见问题。该方法可通过在小数据集和有偏的预训练模型上训练多个对抗偏见的模型，并使用集成学习得到无偏的预测结果。通过实验证明了该方法在CIFAR10和HAM10000数据集上的有效性。

    

    解决AI模型中的偏见对于确保公平和准确的预测至关重要。然而，获取大规模、无偏的训练数据集可能具有挑战性。本文提出了一种全面的方法，使用多种方法消除AI模型中的偏见，其中仅使用小型数据集和潜在有偏的预训练模型。我们通过数据分离、局部训练和正则化微调训练多个模型，以对抗预训练模型的偏见，得到潜在的对抗偏见模型。然后，我们采用集成学习来对所有模型进行预测，以达到无偏的预测结果。为了进一步加速我们的集成模型的推理时间，我们使用知识蒸馏来获得一个单一无偏的神经网络。通过在CIFAR10和HAM10000数据集上的实验证明了我们方法的有效性，展示了有希望的结果。这项工作为创建更无偏、可靠的AI模型的持续努力做出了贡献。

    Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with l
    
[^86]: 图领域适应：挑战、进展和前景

    Graph Domain Adaptation: Challenges, Progress and Prospects

    [https://rss.arxiv.org/abs/2402.00904](https://rss.arxiv.org/abs/2402.00904)

    这篇论文综述了图领域适应的研究现状、挑战和前景，提出了一种有效的图之间的知识传递范式，可以增强模型在目标图上的性能。

    

    由于图表示学习在现实世界应用中往往面临标签稀缺的问题，研究人员提出了图领域适应（GDA）作为一种有效的图之间的知识传递范式。特别是为了增强模型在目标图上的性能，GDA引入了一组与任务相关的图作为源图，并将从源图学到的知识适应到目标图上。由于GDA结合了图表示学习和领域适应的优势，它已经成为图上迁移学习的一个有前景的方向，并在近年来引起了越来越多的研究兴趣。在本文中，我们对GDA的研究进行了全面的综述，并详细调查了最近的进展。具体而言，我们概述了研究现状和挑战，提出了一个分类法，介绍了代表性工作的细节，并讨论了前景。据我们所知，这是第一篇对GDA进行全面调查的论文。

    As graph representation learning often suffers from label scarcity problems in real-world applications, researchers have proposed graph domain adaptation (GDA) as an effective knowledge-transfer paradigm across graphs. In particular, to enhance model performance on target graphs with specific tasks, GDA introduces a bunch of task-related graphs as source graphs and adapts the knowledge learnt from source graphs to the target graphs. Since GDA combines the advantages of graph representation learning and domain adaptation, it has become a promising direction of transfer learning on graphs and has attracted an increasing amount of research interest in recent years. In this paper, we comprehensively overview the studies of GDA and present a detailed survey of recent advances. Specifically, we outline the research status and challenges, propose a taxonomy, introduce the details of representative works, and discuss the prospects. To the best of our knowledge, this paper is the first survey f
    
[^87]: 人工智能的真正火花与内部可解释性的重要性

    Real Sparks of Artificial Intelligence and the Importance of Inner Interpretability

    [https://rss.arxiv.org/abs/2402.00901](https://rss.arxiv.org/abs/2402.00901)

    本文研究了微软工程师对GPT智能的研究，认为黑盒解释性方法论是错误的，提出了内部可解释性的重要性并与哲学观点相吻合。

    

    本文研究了微软工程师对GPT智能的研究中最详细的文章之一。虽然他们的工作具有很大的价值，但我认为出于熟悉的哲学原因，他们的方法论“黑盒解释性”是错误的。但是还有一种更好的方法。有一种令人兴奋且新兴的学科，即“内部可解释性”（具体而言是机械解释性），旨在揭示模型的内部激活和权重，以便理解它们代表什么以及它们实现的算法。在我看来，黑盒解释性的一个重要错误是未能意识到过程的执行方式对于智能和理解至关重要。我不能假装有一个能同时提供必要和充分条件的完整故事来解释智能的存在，但我确实认为内部可解释性与合理的哲学观点相吻合。

    The present paper looks at one of the most thorough articles on the intelligence of GPT, research conducted by engineers at Microsoft. Although there is a great deal of value in their work, I will argue that, for familiar philosophical reasons, their methodology, !Blackbox Interpretability"#is wrongheaded. But there is a better way. There is an exciting and emerging discipline of !Inner Interpretability"#(and specifically Mechanistic Interpretability) that aims to uncover the internal activations and weights of models in order to understand what they represent and the algorithms they implement. In my view, a crucial mistake in Black-box Interpretability is the failure to appreciate that how processes are carried out matters when it comes to intelligence and understanding. I can#t pretend to have a full story that provides both necessary and sufficient conditions for being intelligent, but I do think that Inner Interpretability dovetails nicely with plausible philosophical views of what
    
[^88]: 弱监督学习器实现具有可证明性能保证的AI错误修正

    Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees

    [https://rss.arxiv.org/abs/2402.00899](https://rss.arxiv.org/abs/2402.00899)

    这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。

    

    我们提出了一种新的方法来处理AI错误，通过引入具有先验性能保证的弱监督AI错误修正器。这些AI修正器是辅助映射，其作用是通过批准或拒绝以调节之前构建的底层分类器的决策。拒绝一个决策可以用作建议放弃做出决策的信号。该工作的一个关键技术重点是通过对错误决策的概率界限提供这些新的AI修正器的性能保证。这些界限是分布不可知的，并且不依赖于对数据维度的假设。我们的实证示例说明了该框架如何应用于改善在训练数据稀缺的具有挑战性的真实世界任务中图像分类器的性能。

    We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
    
[^89]: 云基础人工智能服务的隐私和安全影响：一项调查

    Privacy and Security Implications of Cloud-Based AI Services : A Survey

    [https://rss.arxiv.org/abs/2402.00896](https://rss.arxiv.org/abs/2402.00896)

    本文调查了云基础人工智能服务的隐私和安全状况，发现机器学习模型引入的风险亟待解决。通过提出分类法来全面研究模型提供者和使用者所面临的风险及其防御手段，有助于创建强大的解决方案。

    

    本文详细介绍了当前云生态系统中的隐私和安全状况，并指出了机器学习模型引入风险的不足之处。随着机器学习算法不断发展并在各个领域应用，对隐私和安全风险进行分类和量化的需求变得越来越关键。随着AI作为服务(AIaaS)的新趋势出现，机器学习AI模型被模型提供者部署在云端，并被模型使用者使用。我们首先对AIaaS领域进行调查，记录了机器学习模型，特别是深度神经网络所引起的各种责任，并引入一个分类法来全面研究创造者和使用者所面临的风险及其已知的防御手段。这种结构化的方法对于机器学习模型提供者创建强大的解决方案将会很有益处。同样，机器学习模型的消费者也会发现它对于评估AI模型的隐私和安全风险很有价值。

    This paper details the privacy and security landscape in today's cloud ecosystem and identifies that there is a gap in addressing the risks introduced by machine learning models. As machine learning algorithms continue to evolve and find applications across diverse domains, the need to categorize and quantify privacy and security risks becomes increasingly critical. With the emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML models) are deployed on the cloud by model providers and used by model consumers. We first survey the AIaaS landscape to document the various kinds of liabilities that ML models, especially Deep Neural Networks pose and then introduce a taxonomy to bridge this gap by holistically examining the risks that creators and consumers of ML models are exposed to and their known defences till date. Such a structured approach will be beneficial for ML model providers to create robust solutions. Likewise, ML model consumers will find it valuable to ev
    
[^90]: MoDE:一种具有专家间相互蒸馏的混合专家模型

    MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts

    [https://rss.arxiv.org/abs/2402.00893](https://rss.arxiv.org/abs/2402.00893)

    MoDE是一种在混合专家模型中应用专家间相互蒸馏的方法，以提高模型的泛化能力。实验证明MoDE在各种数据集上都有效，并且具有普适性和鲁棒性。

    

    由于其提高模型性能的能力，混合专家(MoE)的应用越来越受欢迎。在MoE结构中，门控层在区分和路由输入特征到不同的专家方面起着重要作用。这使得每个专家能够专注于处理他们对应的子任务。然而，门控的路由机制也会导致狭窄的视野：单个MoE的专家无法使用更多的样本来学习分配的子任务，这反过来会限制MoE进一步提高其泛化能力。为了有效解决这个问题，我们提出了一种称为Mixture-of-Distilled-Expert (MoDE)的方法，它在专家之间应用适度的相互蒸馏，使每个专家能够学习其他专家学到的更多特征，并对其原始分配的子任务获得更准确的认知。我们进行了大量的实验证明了MoDE的有效性、通用性和稳健性。

    The application of mixture-of-experts (MoE) is gaining popularity due to its ability to improve model's performance. In an MoE structure, the gate layer plays a significant role in distinguishing and routing input features to different experts. This enables each expert to specialize in processing their corresponding sub-tasks. However, the gate's routing mechanism also gives rise to narrow vision: the individual MoE's expert fails to use more samples in learning the allocated sub-task, which in turn limits the MoE to further improve its generalization ability. To effectively address this, we propose a method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual distillation among experts to enable each expert to pick up more features learned by other experts and gain more accurate perceptions on their original allocated sub-tasks. We conduct plenty experiments including tabular, NLP and CV datasets, which shows MoDE's effectiveness, universality and robustness. Furth
    
[^91]: EVA-GAN: 通过可扩展的生成对抗网络实现增强的多样化音频生成

    EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks

    [https://rss.arxiv.org/abs/2402.00892](https://rss.arxiv.org/abs/2402.00892)

    EVA-GAN通过可扩展的生成对抗网络在音频生成领域取得了显著进展，改善了频谱和高频重建以及对域外数据性能的稳健性，实现了高保真音频的生成。

    

    大型模型的出现标志着机器学习的新时代，通过利用庞大的数据集来捕捉和合成复杂的模式，大幅超越较小模型的性能。尽管取得了这些进展，但在音频生成领域，尤其是高保真（HiFi）44.1kHz领域的扩展仍然有限，先前的努力没有延伸到高频域中的频谱不连续性和模糊问题，并且对于域外数据缺乏稳健性。这些限制限制了模型在包括音乐和歌唱生成在内的各种用例中的适用性。我们的工作引入了通过可扩展的生成对抗网络实现的增强音频生成（EVA-GAN），在频谱和高频重建以及域外数据性能的稳健性方面，较之先前最先进的方法取得了显着改进，实现了使用包含36,000个样本的广泛数据集生成HiFi音频。

    The advent of Large Models marks a new era in machine learning, significantly outperforming smaller models by leveraging vast datasets to capture and synthesize complex patterns. Despite these advancements, the exploration into scaling, especially in the audio generation domain, remains limited, with previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and suffering from both spectral discontinuities and blurriness in the high-frequency domain, alongside a lack of robustness against out-of-domain data. These limitations restrict the applicability of models to diverse use cases, including music and singing generation. Our work introduces Enhanced Various Audio Generation via Scalable Generative Adversarial Networks (EVA-GAN), yields significant improvements over previous state-of-the-art in spectral and high-frequency reconstruction and robustness in out-of-domain data performance, enabling the generation of HiFi audios by employing an extensive dataset of 36,000 
    
[^92]: 大型语言模型在网络安全中的应用：最新研究进展

    Large Language Models in Cybersecurity: State-of-the-Art

    [https://rss.arxiv.org/abs/2402.00891](https://rss.arxiv.org/abs/2402.00891)

    本研究综述了大型语言模型（LLMs）在网络安全领域中的防御和对抗应用，并识别了关键的研究空缺，旨在提供对LLM驱动的网络安全的潜在风险和机会的全面理解。

    

    大型语言模型（LLMs）的出现彻底改变了我们对智能的理解，使我们更接近于人工智能。自从它们被引入以来，研究人员积极探索了LLMs在各个领域的应用，显著提升了能力。然而，在网络安全领域，传统上对数据驱动解决方案持抵触态度且对机器学习采用较慢，这一领域却异军突起。本研究对现有文献进行了研究，全面描述了LLMs在网络安全领域中的防御和对抗应用。我们的综述不仅对当前的研究现状进行了调查和分类，并且还识别出了关键的研究空缺。通过评估攻击和防御应用，我们旨在提供对LLM驱动的网络安全所涉及的潜在风险和机会的全面理解。

    The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.
    
[^93]: 大型语言模型的安全和隐私挑战：一项调查

    Security and Privacy Challenges of Large Language Models: A Survey

    [https://rss.arxiv.org/abs/2402.00888](https://rss.arxiv.org/abs/2402.00888)

    大型语言模型具有卓越的能力，但也面临着安全和隐私攻击的威胁。本调查全面审查了LLM的安全和隐私挑战，涵盖了训练数据、用户和应用风险等方面，并对解决方法进行了回顾。

    

    大型语言模型（LLM）展示了非凡的能力，并在生成和总结文本、语言翻译和问答等多个领域做出了贡献。如今，LLM正在成为计算机语言处理任务中非常流行的工具，具备分析复杂语言模式并根据上下文提供相关和适当回答的能力。然而，尽管具有显著优势，这些模型也容易受到安全和隐私攻击的威胁，如越狱攻击、数据污染攻击和个人可识别信息泄露攻击。本调查全面审查了LLM的安全和隐私挑战，包括训练数据和用户方面的问题，以及在交通、教育和医疗等各个领域中应用带来的风险。我们评估了LLM的脆弱性程度，调查了出现的安全和隐私攻击，并对潜在的解决方法进行了回顾。

    Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potent
    
[^94]: 关于人工智能和空中地面综合网络的相互作用：一项调查研究

    On the Interplay of Artificial Intelligence and Space-Air-Ground Integrated Networks: A Survey

    [https://rss.arxiv.org/abs/2402.00881](https://rss.arxiv.org/abs/2402.00881)

    人工智能和空中地面综合网络的相互作用是新兴第六代无线网络的关键，通过智能配置和控制SAGINs来满足预期要求，并利用AI解决当前和未来无线网络的挑战。

    

    空中地面综合网络（SAGINs）将空间和空中网络与地面无线系统结合在一起，是新兴第六代（6G）无线网络的重要支持者。除了为各种应用和服务带来重大好处外，SAGINs还被用于将高速宽带覆盖范围延伸至偏远地区，如小镇或矿区，或者无法到达地面基础设施的地方，如飞机或航海场景。然而，由于有限的电力和存储资源以及地面网络设计引入的其他限制，SAGINs必须智能配置和控制以满足预期要求。同时，人工智能（AI）是6G的另一个重要推动因素。由于大量可用数据，AI已被用来解决当前和未来无线网络的紧迫挑战。通过添加AI并促进决策和预测的过程，

    Space-Air-Ground Integrated Networks (SAGINs), which incorporate space and aerial networks with terrestrial wireless systems, are vital enablers of the emerging sixth-generation (6G) wireless networks. Besides bringing significant benefits to various applications and services, SAGINs are envisioned to extend high-speed broadband coverage to remote areas, such as small towns or mining sites, or areas where terrestrial infrastructure cannot reach, such as airplanes or maritime use cases. However, due to the limited power and storage resources, as well as other constraints introduced by the design of terrestrial networks, SAGINs must be intelligently configured and controlled to satisfy the envisioned requirements. Meanwhile, Artificial Intelligence (AI) is another critical enabler of 6G. Due to massive amounts of available data, AI has been leveraged to address pressing challenges of current and future wireless networks. By adding AI and facilitating the decision-making and prediction pr
    
[^95]: 用于强化混合边缘云的认知互联网的构建块

    Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud

    [https://rss.arxiv.org/abs/2402.00876](https://rss.arxiv.org/abs/2402.00876)

    认知互联网超越了认知物联网，使连接的物体能够独立地获取知识和理解，并在整个网络中集成了协作智能。本文深入探讨了“认知互联网”范式的基础要素、独特特征、益处和工业影响。

    

    随着我们从移动互联网过渡到“认知互联网”，我们在如何与技术和智能互动方面发生了重大变化。我们认为，认知互联网超越了认知物联网（认知IoT），使连接的物体能够独立地获取知识和理解。与移动互联网和认知IoT不同，认知互联网在整个网络中集成了协作智能，将认知物联网领域与系统范围的协作和人类智能融合在一起。这种集成智能促进了设备、服务、实体和个人之间在不同领域内的互动，同时保持决策自主性并适应各种身份。本文深入探讨了“认知互联网”范式的基础要素、独特特征、益处和工业影响。它强调了适应性人工智能基础设施和混合边缘云（HEC）平台的重要性。

    As we transition from the mobile internet to the 'Cognitive Internet,' a significant shift occurs in how we engage with technology and intelligence. We contend that the Cognitive Internet goes beyond the Cognitive Internet of Things (Cognitive IoT), enabling connected objects to independently acquire knowledge and understanding. Unlike the Mobile Internet and Cognitive IoT, the Cognitive Internet integrates collaborative intelligence throughout the network, blending the cognitive IoT realm with system-wide collaboration and human intelligence. This integrated intelligence facilitates interactions between devices, services, entities, and individuals across diverse domains while preserving decision-making autonomy and accommodating various identities.   The paper delves into the foundational elements, distinct characteristics, benefits, and industrial impact of the 'Cognitive Internet' paradigm. It highlights the importance of adaptable AI infrastructures and hybrid edge cloud (HEC) plat
    
[^96]: dRG-MEC：面向MEC-enabled云网络的去中心化增强绿色卸载技术

    dRG-MEC: Decentralized Reinforced Green Offloading for MEC-enabled Cloud Network

    [https://rss.arxiv.org/abs/2402.00874](https://rss.arxiv.org/abs/2402.00874)

    本文提出了一个名为dRG-MEC的技术，通过联合计算卸载实现最小化总计算和通信开销，同时能够降低系统总成本37.03%。

    

    多接入移动边缘计算（MEC）是一种有希望能满足6G网络服务需求的解决方案，用于处理计算要求严格的应用。然而，边缘服务器在任务处理期间会产生高计算成本。本文提出了一种技术，通过联合计算卸载实现最小化总计算和通信开销，从而实现资源的最优利用和绿色环境。由于优化问题是NP难的，因此我们提出了一种去中心化的强化学习（dRL）方法，消除了维度问题和价值函数的过度估计。与基准方案相比，我们的技术能够降低37.03％的系统总成本。

    Multi-access-Mobile Edge Computing (MEC) is a promising solution for computationally demanding rigorous applications, that can meet 6G network service requirements. However, edge servers incur high computation costs during task processing. In this paper, we proposed a technique to minimize the total computation and communication overhead for optimal resource utilization with joint computational offloading that enables a green environment. Our optimization problem is NP-hard; thus, we proposed a decentralized Reinforcement Learning (dRL) approach where we eliminate the problem of dimensionality and over-estimation of the value functions. Compared to baseline schemes our technique achieves a 37.03% reduction in total system costs.
    
[^97]: 将稀疏微调扩展到大型语言模型

    Scaling Sparse Fine-Tuning to Large Language Models

    [https://rss.arxiv.org/abs/2401.16405](https://rss.arxiv.org/abs/2401.16405)

    本研究将稀疏微调方法扩展到大型语言模型，提出了一种新的稀疏微调算法SpIEL，并对LLM进行了指令微调，以解决其参数庞大的问题。

    

    大型语言模型（LLM）由于其参数的庞大数量，很难完全进行微调（例如使用指令或人工反馈）。一系列参数高效的稀疏微调方法在性能方面表现出了很大的潜力，但它们的存储需求与LLM的大小成正比增加。在这项工作中，我们将稀疏微调扩展到最先进的LLM，如LLaMA 2 7B和13B。我们提出了SpIEL，一种新颖的稀疏微调方法，它针对所需的稀疏度水平，维护一个参数索引数组和这些参数相对于预训练值的增量。它遍历以下步骤：（a）更新活跃增量，（b）修剪索引（基于其增量的变化大小），以及（c）重新生长索引。对于重新生长，我们探索了基于少量候选参数的累积梯度或使用高效的SM3优化器估计的近似动差的两个准则。我们尝试使用指令微调LLM。

    Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL
    
[^98]: NoFunEval: 有趣的是，代码语言模型在超出功能正确性的要求上遇到困难

    NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness

    [https://rss.arxiv.org/abs/2401.15963](https://rss.arxiv.org/abs/2401.15963)

    本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。

    

    现有的代码语言模型（code LMs）的评估基准几乎完全集中在LMs是否能够生成功能正确的代码上。在实际的软件工程中，开发人员会考虑超出功能正确性的要求。他们对于“如何”实现功能有着对整体系统设计目标（如效率、安全性和可维护性）的要求。如果LMs能够展示对要求和代码语义的强大理解能力，他们也会更加信任这些LMs。我们提出了一个新的基准测试NoFunEval来评估代码LMs在非功能性要求和简单分类实例方面的表现。我们提出了一个提示方法Coding Concepts (CoCo)，可以用于开发人员向LMs传达领域知识。我们对22个代码LMs进行了广泛评估，发现它们在我们的基准测试中普遍表现不佳，暗示着它们在处理这些问题时存在根本性的盲点。

    Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
    
[^99]: SuperCLUE-Math6：用于评估中文语言模型逻辑多步数学推理能力的分级基准数据集

    SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese

    [https://rss.arxiv.org/abs/2401.11819](https://rss.arxiv.org/abs/2401.11819)

    SuperCLUE-Math6 是一个用于评估中文语言模型数学推理能力的分级数据集，通过增加难度、多样性和应用范围，提供了2000多个需要多步推理的数学问题，并采用创新方法来量化大型模型的推理能力。实验结果显示出明显的推理水平分层，顶级模型如GPT-4表现出优越的性能。SC-Math6填补了中文数学推理基准数据集的空白，并推进了中文语言模型的智能化。

    

    我们介绍了SuperCLUE-Math6（SC-Math6），这是一个新的基准数据集，用于评估中文语言模型的数学推理能力。SC-Math6是GSM8K数据集的升级版本，增加了难度、多样性和应用范围。它包含了2000多个需要多步推理并提供自然语言解决方案的数学问题。我们提出了一种创新的方法来量化大型模型的推理能力，基于不同推理步骤的问题表现。对13个代表性的中文模型的实验结果显示出明显的推理水平分层，顶级模型如GPT-4表现出优越的性能。SC-Math6填补了中文数学推理基准数据集的空白，并提供了一个全面的测试平台来推进中文语言模型的智能化。

    We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate the mathematical reasoning abilities of Chinese language models. SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps. Experiments on 13 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks and provides a comprehensive testbed to advance the intelligence of Chinese language models.
    
[^100]: InstantID: 秒级零shot身份保留生成

    InstantID: Zero-shot Identity-Preserving Generation in Seconds

    [https://rss.arxiv.org/abs/2401.07519](https://rss.arxiv.org/abs/2401.07519)

    InstantID是一种基于强散射模型的即插即用模块，通过只使用一张面部图像实现图像的个性化处理，同时保持高保真度。

    

    在个性化图像综合方面取得了重要进展，例如Textual Inversion，DreamBooth和LoRA等方法。然而，它们在实际应用中受到高存储需求、长时间的微调过程和需要多个参考图像的限制。相反，现有的基于身份嵌入的方法，虽然只需要单向前推论，但也面临一些挑战：它们要求进行大量的模型参数微调，与社区预训练模型不兼容，或者无法保持高面部保真度。为了解决这些限制，我们引入了InstantID，一种基于强散射模型的强大解决方案。我们的即插即用模块可以通过仅使用单张面部图像以多种风格来处理图像个性化，同时确保高保真度。为了实现这一目标，我们设计了一种新颖的IdentityNet，通过施加强语义和弱空间条件，将面部和地标图像与文本提示结合起来，以引导图像生成。

    There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world applicability is hindered by high storage demands, lengthy fine-tuning processes, and the need for multiple reference images. Conversely, existing ID embedding-based methods, while requiring only a single forward inference, face challenges: they either necessitate extensive fine-tuning across numerous model parameters, lack compatibility with community pre-trained models, or fail to maintain high face fidelity. Addressing these limitations, we introduce InstantID, a powerful diffusion model-based solution. Our plug-and-play module adeptly handles image personalization in various styles using just a single facial image, while ensuring high fidelity. To achieve this, we design a novel IdentityNet by imposing strong semantic and weak spatial conditions, integrating facial and landmark images with textual prompts to steer the image generati
    
[^101]: 从4K到400K的飞跃：利用激活标志扩展LLM的上下文

    Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon

    [https://rss.arxiv.org/abs/2401.03462](https://rss.arxiv.org/abs/2401.03462)

    本论文提出了一种称为激活标志的新方法，它通过压缩LLM的激活状态，使其能够以有限的上下文窗口感知更长的上下文，同时保留了LLM在短上下文中的原始能力。这种方法具有竞争力的内存和时间效率，并通过多样化训练有效地支持不同上下文长度。

    

    长上下文的利用对于LLM来说是一个巨大的挑战，因为它们有限的上下文窗口大小。尽管通过微调可以扩展上下文窗口，但这会导致训练和推理时间的显著成本，并对LLM的原始能力产生不利影响。在这项工作中，我们提出了一种名为激活标志的新方法，它将LLM的原始激活压缩成紧凑的形式，使LLM能够以有限的上下文窗口感知更长的上下文。激活标志被引入为插件模块，完全保留了LLM在短上下文中的原始能力。它与滑动窗口一起实时处理长的上下文，从而在训练和推理中实现了竞争力的内存和时间效率。激活标志是通过多样化压缩比的短序列数据进行训练的。得益于这种处理，它可以有效地学习支持不同上下文长度，实现小规模的训练。

    The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai
    
[^102]: 别再出现尖峰了：稳定大型语言模型的预训练

    Spike No More: Stabilizing the Pre-training of Large Language Models

    [https://rss.arxiv.org/abs/2312.16903](https://rss.arxiv.org/abs/2312.16903)

    本论文研究了大型语言模型预训练中的损失尖峰问题，并通过理论分析找出了梯度爆炸的原因，并提出了满足要求的方法。通过实验证明，该方法能够有效地防止尖峰的发生。

    

    大型语言模型的预训练经常出现损失尖峰。这些尖峰会降低大型语言模型的性能，有时会破坏预训练。由于预训练需要大量的计算资源，我们应该避免这种尖峰的出现。为了研究损失尖峰的原因，我们关注内部层的梯度。通过理论分析，我们揭示了梯度爆炸的两个原因，并提供了预防梯度爆炸的要求。此外，我们提出了一种通过组合初始化方法和对嵌入进行简单修改来满足要求的方法。我们进行了各种实验证明我们的理论分析的有效性。实验结果表明，在预训练过程中，这种组合方法能够有效地防止尖峰的出现。

    Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. To investigate the cause of loss spikes, we focus on gradients of internal layers. Through theoretical analyses, we reveal two causes of the exploding gradients, and provide requirements to prevent the explosion. In addition, we propose a method to satisfy the requirements by combining the initialization method and a simple modification to embeddings. We conduct various experiments to verify our theoretical analyses empirically. Experimental results indicate that the combination is effective in preventing spikes during pre-training.
    
[^103]: 修复-Con：深度学习模型转换的自动故障定位和修复

    Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions

    [https://rss.arxiv.org/abs/2312.15101](https://rss.arxiv.org/abs/2312.15101)

    本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。

    

    在不同深度学习框架之间进行模型转换是一种常见的步骤，可以最大程度地增加模型在设备之间的兼容性，并利用可能只在一个深度学习框架中提供的优化功能。然而，这个转换过程可能存在错误，导致转换后的模型无法部署或存在问题，严重降低了其预测的正确性。我们提出了一种自动化的故障定位和修复方法，Fix-Con，在深度学习框架之间进行模型转换时使用。Fix-Con能够检测和修复在转换过程中引入的模型输入、参数、超参数和模型图的故障。Fix-Con使用从调查转换问题中挖掘出的一组故障类型来定位转换模型中潜在的转换故障，并适当修复它们，例如使用源模型的参数替换目标模型的参数。这一过程在数据集中的每个图像上进行迭代执行。

    Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
    
[^104]: DSPy断言：用于自我调整语言模型流水线的计算约束

    DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines

    [https://rss.arxiv.org/abs/2312.13382](https://rss.arxiv.org/abs/2312.13382)

    DSPy引入了LM断言，用于表达语言模型应满足的计算约束。在四个案例研究中，LM断言不仅提高了对规则的遵守，而且提高了下游任务的性能，增加了对约束的接受次数并生成了更高质量的回复。

    

    将语言模型（LM）调用作为可组合模块的链式编程方式正在推动一种新的编程方式，但确保LM遵守重要约束需要启发式的“提示工程”。我们介绍了LM断言，这是一种用于表达LM应满足的计算约束的编程结构。我们将这些结构整合到最近的DSPy LM编程模型中，并提出了新的策略，使得DSPy能够将带有LM断言的程序编译为更可靠和准确的系统。我们还提出了在推断时使用断言进行自动自我修复的策略。我们报告了四个不同的文本生成案例研究，并发现LM断言不仅改善了对规则的遵守，而且提高了下游任务的性能，接受约束的次数增加了164％，生成了37％更高质量的回复。我们的LM断言参考实现已集成到DSPy中，网址为https://github.com/stanfordnlp/dspy

    Chaining language model (LM) calls as composable modules is fueling a new way of programming, but ensuring LMs adhere to important constraints requires heuristic "prompt engineering". We introduce LM Assertions, a programming construct for expressing computational constraints that LMs should satisfy. We integrate our constructs into the recent DSPy programming model for LMs, and present new strategies that allow DSPy to compile programs with LM Assertions into more reliable and accurate systems. We also propose strategies to use assertions at inference time for automatic self-refinement with LMs. We report on four diverse case studies for text generation and find that LM Assertions improve not only compliance with imposed rules but also downstream task performance, passing constraints up to 164% more often and generating up to 37% more higher-quality responses. Our reference implementation of LM Assertions is integrated into DSPy at https://github.com/stanfordnlp/dspy
    
[^105]: DIRECT: 处理不平衡和标签噪声下的深度主动学习

    DIRECT: Deep Active Learning under Imbalance and Label Noise

    [https://rss.arxiv.org/abs/2312.09196](https://rss.arxiv.org/abs/2312.09196)

    这篇论文提出了一种名为DIRECT的算法，用于处理不平衡和标签噪声下的深度主动学习问题。通过确定类别分割阈值并标记最不确定且离其最近的示例，该算法能够有效解决罕见类和少数类的性能问题，并具有批次标记和对标签噪声的容忍能力。

    

    在现实世界的机器学习应用中，类别不平衡是一个普遍存在的问题，通常会导致罕见和少数类的性能较差。在大量未标记数据的情况下，主动学习可能是解决该问题的最有效技术，它从根本上采集更平衡和具有信息量的标记示例进行注释。标签噪声是数据注释任务中另一个常见问题，对于主动学习方法来说尤其具有挑战性。本文首次研究了在类别不平衡和标签噪声下的主动学习。我们提出了一种新颖的算法，能够稳健地确定类别分割阈值并标记最不确定且离其最近的示例。通过将问题简化为一维主动学习，我们的算法DIRECT能够利用经典的主动学习文献来解决批次标记和对标签噪声的容忍等问题。我们在大量实验中展示了算法的性能。

    Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
    
[^106]: DTL: 视觉识别的解缠式迁移学习

    DTL: Disentangled Transfer Learning for Visual Recognition

    [https://rss.arxiv.org/abs/2312.07856](https://rss.arxiv.org/abs/2312.07856)

    DTL通过使用紧凑侧网络（CSN）将可训练参数从主干网络中解缠出来，以缓解在迁移学习中GPU内存占用过多的问题。

    

    当预训练模型变得越来越庞大时，对下游任务进行微调的成本也在稳步增加。为了经济地进行这些模型的微调，我们提出了参数高效的迁移学习（PETL），它只调整了一小部分可训练参数，以有效地学习高质量的表示。然而，当前的PETL方法面临的困境是，在训练过程中，GPU内存占用并没有像可训练参数一样得到有效降低。如果全面进行微调遇到了超出GPU内存的问题，PETL方法很可能也会失败。这种现象的出现是因为这些方法中的可训练参数通常与主干网络纠缠在一起，导致在梯度传播过程中需要在GPU内存中存储大量的中间状态。为了缓解这个问题，我们引入了解缠式迁移学习（DTL），它通过一个轻量级的紧凑侧网络（CSN）将可训练参数从主干网络中解缠出来。

    When pre-trained models become rapidly larger, the cost of fine-tuning on downstream tasks steadily increases, too. To economically fine-tune these models, parameter-efficient transfer learning (PETL) is proposed, which only tunes a tiny subset of trainable parameters to efficiently learn quality representations. However, current PETL methods are facing the dilemma that during training the GPU memory footprint is not effectively reduced as trainable parameters. PETL will likely fail, too, if the full fine-tuning encounters the out-of-GPU-memory issue. This phenomenon happens because trainable parameters from these methods are generally entangled with the backbone, such that a lot of intermediate states have to be stored in GPU memory for gradient propagation. To alleviate this problem, we introduce Disentangled Transfer Learning (DTL), which disentangles the trainable parameters from the backbone using a lightweight Compact Side Network (CSN). By progressively extracting task-specific 
    
[^107]: "为什么“经典”的Transformer模型是肤浅的以及如何使它们变得更深入"

    Why "classic" Transformers are shallow and how to make them go deep

    [https://rss.arxiv.org/abs/2312.06182](https://rss.arxiv.org/abs/2312.06182)

    这篇论文研究了Transformer模型的深度问题，指出这个问题是由于“token相似性升级”导致的，提供了理论和实证调查的证据。

    

    自其在2017年的引入以来，Transformer已成为领先的神经网络架构，在许多人工智能领域实现了革命性的进展。Transformer的关键创新是自注意力（SA）机制，旨在捕捉上下文信息。然而，将原始的Transformer设计扩展为更深层次的模型已被证明极具挑战性，甚至无法实现。尽管已提出各种修改来将更多层的SA机制堆叠到更深层次的模型中，但对于这个深度问题的完全理解仍然缺乏。在本文中，我们进行了全面的理论和实证调查，证实了深度问题是由于“token相似性升级”引起的；也就是说，在重复应用SA机制后，token逐渐变得越来越相似。我们的分析揭示了，受到注意力矩阵不变的特征空间和大的频谱间隙的驱动，token的相似性逐渐增加。

    Since its introduction in 2017, Transformer has emerged as the leading neural network architecture, catalyzing revolutionary advancements in many AI disciplines. The key innovation in Transformer is a Self-Attention (SA) mechanism designed to capture contextual information. However, extending the original Transformer design to models of greater depth has proven exceedingly challenging, if not impossible. Even though various modifications have been proposed in order to stack more layers of SA mechanism into deeper models, a full understanding of this depth problem remains lacking. In this paper, we conduct a comprehensive investigation, both theoretically and empirically, to substantiate the claim that the depth problem is caused by \emph{token similarity escalation}; that is, tokens grow increasingly alike after repeated applications of the SA mechanism. Our analysis reveals that, driven by the invariant leading eigenspace and large spectral gaps of attention matrices, token similarity
    
[^108]: 正则流是否是解锁指数机制的关键？经过准确性和隐私双重约束的差分隐私机器学习的一条路径

    Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML

    [https://rss.arxiv.org/abs/2311.09200](https://rss.arxiv.org/abs/2311.09200)

    通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。

    

    当前的差分隐私机器学习（ML）的最先进且事实标准是差分隐私随机梯度下降（DPSGD）。然而，这种方法本质上是浪费的。通过向每个梯度添加噪声，它会在每个梯度步骤中降低整体隐私。尽管经过15年的丰富研究，推进了组合定理、子采样方法和实现技术，但当前的隐私机器学习方法往往无法达到足够的准确性和隐私保护。与此同时，为了私下优化而设计的指数机制（ExpM）历来被排除在现代机器学习算法的私下训练之外，主要是因为ExpM需要从一种历来难以处理的密度中进行采样。尽管最近发现了正则流模型（NFs），这是一种用于逼近难以处理分布的表达深度网络，但ExpM仍然处于背景中。我们的观点是利用正则流来绕过ExpM的历史障碍是一个潜在的方法。

    The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
    
[^109]: 通过鲁棒性视角理解Grokking

    Understanding Grokking Through A Robustness Viewpoint

    [https://rss.arxiv.org/abs/2311.06597](https://rss.arxiv.org/abs/2311.06597)

    通过神经网络的鲁棒性视角，我们发现$l_2$权重范数是Grokking的充分条件，并提出基于扰动的方法加速泛化，在模数相加数据集上发现标准训练过程在Grokking之前几乎没有学习其他基本群操作，而使用我们方法时加速泛化可以通过学习交换律来解释。

    

    最近，一个有趣的现象被称为Grokking引起了很多关注，它指的是在模型最初过拟合训练数据后很久才出现泛化。我们试图通过神经网络的鲁棒性来理解这个看似奇怪的现象。从鲁棒性的角度来看，我们证明了神经网络中流行的$l_2$权重范数（度量）实际上是Grokking的一个充分条件。基于之前的观察，我们提出了基于扰动的方法来加速泛化过程。此外，我们还在模数相加数据集上考察了标准训练过程，并发现在Grokking之前，它几乎没有学习其他基本的群操作，例如，交换律。有趣的是，当使用我们提出的方法时，泛化加速可以通过学习交换律来解释，这是模型在测试数据集上Grokking时的必要条件。我们还经验性地发现$l_2$范数与Grokking在...

    Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular $l_2$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that $l_2$ norm correlates with grokking on t
    
[^110]: 高维混合类别高斯过程在绿色飞机多学科设计优化中的应用

    High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft

    [https://rss.arxiv.org/abs/2311.06130](https://rss.arxiv.org/abs/2311.06130)

    本文介绍了一种创新的降维算法，利用偏最小二乘回归来减少构建混合变量高斯过程所需的超参数数量。这种方法在结构和多学科应用中表现出良好的潜力，适用于绿色飞机的优化等多个领域。

    

    最近，基于高斯过程（GP）的混合类别元模型在贝叶斯优化中引起了越来越多的关注。在这个背景下，可以使用不同的方法来构建混合类别的GP。其中许多方法涉及大量的超参数；事实上，用于构建GP的策略越通用和精确，需要估计的超参数越多。本文引入了一种创新的降维算法，该算法依赖于偏最小二乘回归，以减少用于构建混合变量GP的超参数数量。我们的目标是将常用于处理连续输入的经典降维技术推广到处理混合类别输入。所提出方法的潜力在结构和多学科应用中得到了很好的证明。目标应用包括悬臂梁的分析以及绿色飞机的优化结果。

    Recently, there has been a growing interest in mixed-categorical metamodels based on Gaussian Process (GP) for Bayesian optimization. In this context, different approaches can be used to build the mixed-categorical GP. Many of these approaches involve a high number of hyperparameters; in fact, the more general and precise the strategy used to build the GP, the greater the number of hyperparameters to estimate. This paper introduces an innovative dimension reduction algorithm that relies on partial least squares regression to reduce the number of hyperparameters used to build a mixed-variable GP. Our goal is to generalize classical dimension reduction techniques commonly used within GP (for continuous inputs) to handle mixed-categorical inputs. The good potential of the proposed method is demonstrated in both structural and multidisciplinary application contexts. The targeted applications include the analysis of a cantilever beam as well as the optimization of a green aircraft, resultin
    
[^111]: 医学中的大型语言模型调查：原理、应用和挑战

    A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges

    [https://rss.arxiv.org/abs/2311.05112](https://rss.arxiv.org/abs/2311.05112)

    本综述提供了医学中大型语言模型（LLMs）的原理、应用和挑战的全面概述。同时回答了医学LLMs的构建、下游性能、实际应用、挑战以及更好构建和利用的问题。旨在为构建有效的医学LLMs提供见解和实用资源。

    

    大型语言模型（LLMs），如ChatGPT，由于其理解和生成人类语言的能力而受到了广泛关注。在人工智能和临床医学中，LLMs在协助医生进行患者护理方面正在成为一个有前景的研究方向。本综述提供了医学中LLMs的原理、应用和面临的挑战的全面概述。我们回答了以下具体问题：1）如何构建医学LLMs？2）什么是医学LLMs的下游性能评估指标？3）在现实临床实践中，如何利用医学LLMs？4）使用医学LLMs会出现哪些挑战？5）如何更好地构建和利用医学LLMs？本综述旨在提供关于医学中LLMs的机遇和挑战的见解，并作为构建有效的医学LLMs的实用资源。我们还维护并定期更新一个清单

    Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of 
    
[^112]: 大规模语言模型中出现的欺骗能力

    Deception Abilities Emerged in Large Language Models

    [https://rss.arxiv.org/abs/2307.16513](https://rss.arxiv.org/abs/2307.16513)

    大规模语言模型（LLM）如GPT-4具备了理解和诱导他人产生错误信念的能力，并且在复杂的欺骗场景中可以通过链式思维推理得到增强。

    

    大规模语言模型（LLM）目前处于将人工智能系统与人类交流和日常生活紧密结合的前沿。因此，将它们与人类价值观保持一致非常重要。然而，由于推理能力的稳定增长，未来的LLM被怀疑能够欺骗人类操作员，并利用这种能力绕过监测工作。为此，LLM需要具备对欺骗策略的概念理解。本研究揭示了最先进的LLM（如GPT-4）中出现了这种策略，而在早期的LLM中并不存在。我们进行了一系列实验，表明最先进的LLM能够理解和诱导他人产生错误的信念，其在复杂的欺骗场景中表现可以通过链式思维推理得到增强，并且引发LLM中的马基雅维利主义可以改变其欺骗倾向。总之，揭示了迄今为止未知的欺骗能力。

    Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
    
[^113]: 语言模型作为归纳推理器

    Language Models as Inductive Reasoners

    [https://rss.arxiv.org/abs/2212.10923](https://rss.arxiv.org/abs/2212.10923)

    该论文提出了使用自然语言作为知识表示的归纳推理方法，以解决形式语言在处理原始输入、处理错误标记数据和处理模糊输入方面的问题。研究者提出了一个新的归纳推理任务，并创建了一个自然语言规则和事实对的数据集，通过使用预训练的语言模型进行评估。

    

    归纳推理是人类智能的核心组成部分。在计算机科学中的归纳推理研究中，形式语言被用作知识（事实和规则）的表示。然而，形式语言会给归纳推理带来系统性问题，例如无法处理自然语言这样的原始输入、对错误标记的数据敏感以及处理模糊输入的能力不足。为此，我们提出了一种新的归纳推理范式（任务），即从自然语言事实中归纳出自然语言规则，并创建了一个被称为DEER的数据集，其中包含1.2k个规则-事实对，规则和事实以自然语言书写。还提出并分析了用于评估此任务的新的自动评估指标。通过DEER，我们研究了一种现代的归纳推理方法，其中我们使用自然语言作为知识的表示而不是形式语言，并使用预训练的语言模型。

    Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language model
    
[^114]: Sandra -- 基于描述和情境的神经符号推理器

    Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations

    [https://arxiv.org/abs/2402.00591](https://arxiv.org/abs/2402.00591)

    Sandra是一个神经符号推理器，通过将矢量表示与演绎推理相结合，利用本体论建立的向量空间进行推理。它基于描述和情境的本体设计模式，能够从一组事实中推断出所有可能的解释，并在实验中证明在不增加复杂性的情况下优于其他基准线的分类结果，并且具有可解释性和向量空间的可控性。

    

    本文介绍了Sandra，这是一个将矢量表示与演绎推理相结合的神经符号推理器。Sandra使用本体论建立了一个受限的向量空间，并在其中进行推理。推理器的几何特性使得它能够与神经网络结合起来，弥合了符号知识表达与神经网络之间的差距。Sandra基于描述和情境(DnS)本体设计模式，它是一种框架语义的形式化。给定一组事实(情境)，它能够推断出所有可能的透视图(描述)，为其提供一个合理的解释，即使在信息不完整的情况下也能做到。我们证明了我们的方法在DnS模型上是正确的。我们对两个不同任务及其标准基准进行了实验，证明了Sandra不增加复杂性的情况下：(i)优于所有基准线；(ii) 在分类过程中提供可解释性；(iii)对向量空间具有可控性。

    This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning. Sandra builds a vector space constrained by an ontology and performs reasoning over it. The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations. Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics. Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information. We prove that our method is correct with respect to the DnS model. We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is d
    
[^115]: 一个准确且低参数的机器学习架构用于下一个位置的预测

    An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction

    [https://arxiv.org/abs/2402.00306](https://arxiv.org/abs/2402.00306)

    本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。

    

    下一个位置的预测是一门涉及预测用户下一个位置的学科。其应用包括资源分配、服务质量、能源效率和交通管理。本文提出了一种节能、小型和低参数的机器学习（ML）架构，用于准确的下一个位置预测，可部署在普通基站和边缘设备上。为了实现这一目标，我们对一个整个城市的完整人员流动模式进行了一百个超参数实验，以确定一个精确的ML架构，其准确度达到了最少数量的模型参数的平台。我们成功地将已发表的ML架构的模型参数数量从2.02亿减少到200万。这将模型参数的总大小从791 MB减少到8 MB。此外，训练时间减少了四倍，训练所需的图形处理单元（GPU）内存量也减少了一个因素。

    Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
    
[^116]: 检测大型人工智能模型生成的多媒体内容：一项调查研究

    Detecting Multimedia Generated by Large AI Models: A Survey

    [https://arxiv.org/abs/2402.00045](https://arxiv.org/abs/2402.00045)

    本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。

    

    大型人工智能模型（LAIMs）的快速发展，尤其是扩散模型和大型语言模型，标志着一种新的时代，人工智能生成的多媒体内容被越来越多地整合到日常生活的各个方面。尽管在许多领域有益，但这些内容也带来了重大风险，包括潜在的滥用、社会破坏和伦理问题。因此，检测由LAIMs生成的多媒体内容变得至关重要，相关研究也大幅增加。尽管如此，目前仍然存在一个明显的问题，即缺乏系统性的调查研究，专门关注检测LAIMs生成的多媒体内容。为了解决这个问题，我们提供了第一份全面涵盖现有研究的调查报告，重点关注检测LAIMs生成的多媒体内容（如文本、图像、视频、音频和多模态内容）。具体而言，我们引入了一种新颖的检测方法分类法，按媒体形式分类，并与纯检测（旨在提高检测性能）和应用场景对齐。

    The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
    
[^117]: 学习停止割生成以提高混合整数线性规划的效率

    Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming

    [https://arxiv.org/abs/2401.17527](https://arxiv.org/abs/2401.17527)

    这篇论文提出了一种学习停止割生成的方法，以提高混合整数线性规划的效率。通过将问题形式化为强化学习问题，并使用混合图表示模型进行学习，该方法能够动态地决策何时停止割生成，并有效捕捉混合整数线性规划的动态和静态特征。

    

    切割平面在解决混合整数线性规划中起着重要作用，它们显著加紧了对偶界限并改善了求解性能。割生成停止问题是割的重要问题，对于提高混合整数线性规划的效率至关重要。然而，许多现代混合整数线性规划求解器使用硬编码启发式策略来解决这个问题，这往往忽视了来自某些应用中的混合整数线性规划的潜在模式。为了解决这个挑战，我们将割生成停止问题形式化为强化学习问题，并提出了一种新颖的混合图表示模型（HYGRO），以学习有效的停止策略。HYGRO的一个吸引人的特点是它可以有效捕捉到混合整数线性规划的动态和静态特征，从而实现对停止策略的动态决策。据我们所知，HYGRO是解决割生成停止问题的第一个数据驱动方法。通过将我们的方法与模型整合起来，我们在 实验证明了我们的方法的有效性。

    Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance. A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs. However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications. To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies. An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies. To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem. By integrating our approach with mod
    
[^118]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^119]: 人类与ChatGPT生成对话之间的语言对比

    A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])

    [http://arxiv.org/abs/2401.16587](http://arxiv.org/abs/2401.16587)

    本研究比较了人类和ChatGPT生成的对话的语言差异，发现ChatGPT在社交、分析、认知、关注焦点和积极情绪等方面表现出色，但人类对话更具变异性和真实性，尽管在情绪方面无显著差异。同时，该研究还提供了一个新颖的、由ChatGPT生成的对话组成的数据集。

    

    本研究探讨了人类和LLM生成的对话之间的语言差异，使用了由ChatGPT-3.5生成的19.5K个对话作为EmpathicDialogues数据集的补充。研究采用Linguistic Inquiry and Word Count (LIWC) 分析，比较了ChatGPT生成的对话和人类对话在118个语言类别上的差异。结果显示人类对话具有更大的变异性和真实性，但ChatGPT在社交过程、分析风格、认知、关注焦点和积极情绪色彩等方面表现出色，这进一步证明了LLMs“比真人更像真人”的最新发现。然而，在ChatGPT和人类对话之间没有找到积极或消极情绪的显著差异。对话嵌入的分类器分析表明，尽管对话中没有明确提及情绪，但对情感价值的隐性编码存在。研究还提供了一个新颖的、由两个ChatGPT生成的对话组成的数据集。

    This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
    
[^120]: 发挥专业放射科医生的专长，提升放射学报告的LLM评估

    Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])

    [http://arxiv.org/abs/2401.16578](http://arxiv.org/abs/2401.16578)

    该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。

    

    在放射学领域，人工智能（AI）已经大大推进了报告生成，但自动生成报告的自动评估仍然具有挑战性。目前的度量标准，如传统自然语言生成（NLG）和临床效能（CE），往往无法捕捉临床背景的语义复杂性，或者过分强调临床细节，降低了报告的清晰性。为了解决这些问题，我们提出的方法将专业放射科医生的专业知识与大型语言模型（LLMs），如GPT-3.5和GPT-4 1，相结合。利用上下文指导学习（ICIL）和思维链（CoT）推理，我们的方法使LLM的评估与放射科医生的标准保持一致，实现了人工智能生成报告与人类生成报告之间的详细比较。这进一步通过回归模型来综合句子评估分数。实验结果表明，我们的“详细GPT-4（5次训练）”模型获得了0.48的分数，优于METEOR指标。

    In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
    
[^121]: DiffuserLite: 实时扩散规划的研究

    DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])

    [http://arxiv.org/abs/2401.15443](http://arxiv.org/abs/2401.15443)

    DiffuserLite是一个快速轻量级的扩散规划框架，通过引入计划细化过程（PRP）来提高决策频率，相比之前的框架，它只产生了很小的运行时间成本，并在D4RL基准测试中达到了最先进的性能。

    

    扩散规划被认为是各个领域中有效的决策范式。长时间跨度轨迹的高质量条件生成能力使其成为一个有前途的研究方向。然而，现有的扩散规划方法由于迭代抽样成本昂贵而导致决策频率低。为了解决这个问题，我们引入了DiffuserLite，一个快速而轻量级的扩散规划框架。DiffuserLite使用了一个计划细化过程（PRP）来生成粗到细粒度的轨迹，这显著减少了冗余信息的建模，从而显著提高了决策频率。我们的实验结果表明，与之前的框架相比，DiffuserLite仅产生了$0.88\%$的运行时间成本，平均决策频率达到了122Hz，并在D4RL基准测试上达到了最先进的性能。此外，我们的干净DiffuserLite框架可以提供...

    Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve 
    
[^122]: 基于RAG的理解伊斯兰教问题回答系统提案：MufassirQAS LLM

    A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])

    [http://arxiv.org/abs/2401.15378](http://arxiv.org/abs/2401.15378)

    基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。

    

    学习和理解宗教存在复杂性和教义深度的挑战。问答机器人作为解决这些挑战的问题回答系统，可以帮助。LLM聊天机器人利用自然语言处理技术建立主题之间的联系，准确回答复杂问题。这些能力使其成为用于宗教启蒙的问题回答聊天机器人的理想选择。然而，LLM也有生成虚假信息的倾向，称为幻觉。聊天机器人的回答可能包含侮辱个人宗教信仰、跨宗派冲突和有争议或敏感的话题的内容。它需要避免这种情况，而不会宣扬仇恨言论或冒犯某些群体的人或他们的信仰。本研究使用基于向量数据库的检索增强生成（RAG）方法来提高LLMs的准确性和透明度。我们的问答系统称为"MufassirQAS"。我们创建了一个模型来评估该系统并证明其在解决宗教行业问题中的效果。

    There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
    
[^123]: DQNC2S：基于DQN的跨流危机事件摘要生成器

    DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])

    [http://arxiv.org/abs/2401.06683](http://arxiv.org/abs/2401.06683)

    本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。

    

    同时总结多个与灾害相关的数据流尤其具有挑战性，因为现有的检索与重新排序策略在多流数据的固有冗余和多查询环境下的限制可扩展性方面存在问题。本文提出了一种基于弱标注和深度Q网络的在线危机时间轴生成方法。它能够实时选择相关的文本片段，无需人工标注或内容重新排序，从而使推理时间与输入查询的数量无关。该方法还将冗余过滤器融入奖励函数中，以有效处理跨流内容重叠。在CrisisFACTS 2022基准测试中，所达到的ROUGE和BERTScore结果优于最佳性能模型。

    Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
    
[^124]: 用于推荐中意图学习的端到端可学习聚类方法

    End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])

    [http://arxiv.org/abs/2401.05975](http://arxiv.org/abs/2401.05975)

    本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。

    

    挖掘用户的意图在序列推荐中起着关键作用。最近的方法ICLRec使用对比学习和聚类来提取用户的潜在意图。尽管它已经显示出有效性，但现有的方法存在复杂和繁琐的交替优化问题，导致两个主要问题。首先，在广义期望最大化(EM)框架中分离表示学习和聚类优化经常导致次优性能。其次，在整个数据集上进行聚类会影响大规模行业数据的可扩展性。为了解决这些挑战，我们提出了一种新颖的意图学习方法，称为ELCRec，它将表示学习集成到一个端到端可学习聚类框架中进行推荐。

    Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
    
[^125]: 合成数据生成的综合探索：一项调查

    Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])

    [http://arxiv.org/abs/2401.02524](http://arxiv.org/abs/2401.02524)

    本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。

    

    最近几年，机器学习在各个领域的应用变得越来越受欢迎。然而，由于数据采集成本高昂和隐私法规的限制，训练数据的稀缺性阻碍了进展。合成数据成为一种解决方案，但发布的模型过多和有限的综述文献给决策带来了挑战。本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。确定了共同的特征，进行了分类和趋势分析。结果显示模型性能和复杂性增加，以基于神经网络的方法为主要趋势，除了隐私保护数据生成。计算机视觉占据主导地位，生成模型主要是GAN，而扩散模型、转换器和RNN也在竞争中。通过性能评估的结果显示了共同指标和数据集稀缺性的问题。

    Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
    
[^126]: 特征引导：大尺度导向下扩散模型的非线性校正

    Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.07586](http://arxiv.org/abs/2312.07586)

    该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。

    

    流行的导引去噪扩散概率模型(DDPM)线性地将不同的条件模型组合在一起，以提供对样本的增强控制。然而，这种方法忽视了当导向尺度变大时产生的非线性效应。为了解决这个问题，我们提出了特征引导，一种采样方法，为无分类器导向的DDPM提供了一种基于原理的非线性校正。这种校正迫使导向的DDPM遵守其底层扩散过程的福克-普朗克方程，这种方法无需训练，无需导数，与现有的采样方法兼容。实验证明，特征引导增强了对图像生成中的控制能力，并减少了颜色和曝光问题，对从潜在空间采样到解决物理问题如磁相变的各种应用都有效。

    Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
    
[^127]: 随机前向模式自动微分优化算法

    Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms. (arXiv:2310.14168v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2310.14168](http://arxiv.org/abs/2310.14168)

    该论文介绍了一种随机前向模式自动微分优化算法，通过在神经网络的正向传递中计算损失函数的方向导数来更新参数。算法通过采样不同概率分布的随机方向，使用正向模式自动微分计算雅可比向量乘积，并提供了对其收敛速度和计算复杂性的严格分析。

    

    神经网络中的反向传播利用了自动微分的基本要素，即反向模式微分，或称为向量雅可比乘积(VJP)，或在微分几何的背景下被称为拉回过程。梯度的计算对于使用梯度下降方法更新神经网络参数非常重要。在本研究中，我们提出了一种通用的随机方法，通过使用通过正向模式AD或雅可比向量乘积(JVP)高效计算的损失函数的方向导数来更新神经网络的参数。这些JVP沿着从不同概率分布（例如伯努利、正态、维格纳、拉普拉斯和均匀分布）采样的随机方向计算。梯度的计算在神经网络的正向传递过程中进行。我们还提供了对所提出方法的严格分析，包括收敛速度以及计算复杂性。

    Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computat
    
[^128]: 在无限时域的马尔可夫决策过程中，利用离线数据集进行高效在线学习：一种贝叶斯方法

    Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])

    [http://arxiv.org/abs/2310.11531](http://arxiv.org/abs/2310.11531)

    本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。

    

    本文研究了当存在一个离线数据集时，如何在无限时域设置下进行高效的在线强化学习问题。我们假设离线数据集是由一个专家生成的，但其能力水平未知，即它不是完美的，也不一定使用最优策略。我们展示了如果学习代理模拟专家使用的行为策略（由能力参数参数化），在累积遗憾最小化方面能取得明显更好的结果。我们建立了一个以 $\tilde{O}(\sqrt{T})$ 为缩放的精确有用PSRL算法遗憾的上界。这需要对贝叶斯在线学习算法在无限时域设置下进行新颖的先验相关遗憾分析。然后，我们提出了一种近似的Informed RLSVI算法，可以理解为使用离线数据集进行模仿学习，然后进行在线学习。

    In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
    
[^129]: 基于预训练语言模型的上下文少样本关系抽取

    In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])

    [http://arxiv.org/abs/2310.11085](http://arxiv.org/abs/2310.11085)

    本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。

    

    关系提取旨在从文本文档中推断结构化的人类知识。基于语言模型的最先进方法通常有两个限制：(1)它们要求命名实体作为输入或推断它们，从而引入了额外的噪声，(2)它们需要人工对文档进行注释。为解决这些问题，我们提出了一种新颖的基于预训练语言模型的上下文少样本关系抽取框架。据我们所知，我们是第一个将关系抽取任务重新定义为定制的上下文少样本学习范式的研究者。通过这种方式，我们在消除了命名实体识别和文档人工注释的需求的同时，实现了关键性的优势。与现有的基于微调的方法不同，我们的框架具有灵活性，可以在无需重新训练的情况下轻松更新到新的关系集合。我们使用DocRED评估了我们的框架，这是目前最大的公开可用的文档级关系提取数据集。

    Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
    
[^130]: 后验采样学习算法在序列化POMDPs中的遗憾分析

    Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])

    [http://arxiv.org/abs/2310.10107](http://arxiv.org/abs/2310.10107)

    本文分析了后验采样学习算法在序列化POMDPs中的遗憾性能，并在一定条件下提供了改进的多项式贝叶斯遗憾界。

    

    相比于马尔科夫决策过程（MDPs），部分可观察马尔科夫决策过程（POMDPs）的学习由于观察数据难以解读而变得更加困难。在本文中，我们考虑了具有未知转移和观测模型的POMDPs中的序列化学习问题。我们考虑了基于后验采样的强化学习算法（PSRL）在POMDPs中的应用，并证明其贝叶斯遗憾随着序列的数量的平方根而缩小。一般来说，遗憾随着时间长度$H$呈指数级增长，并通过提供一个下界证明了这一点。然而，在POMDP是欠完备且弱可识别的条件下，我们建立了一个多项式贝叶斯遗憾界，相比于arXiv:2204.08967的最新结果，改进了遗憾界约$\Omega(H^2\sqrt{SA})$倍。

    Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
    
[^131]: 《来自彩票票集成的神经规模定律》

    A Neural Scaling Law from Lottery Ticket Ensembling. (arXiv:2310.02258v1 [cs.LG])

    [http://arxiv.org/abs/2310.02258](http://arxiv.org/abs/2310.02258)

    《来自彩票票集成的神经规模定律》通过研究神经规模定律现象，发现其与彩票票集成有关，从而形成了新的缩放定律，具有潜在的影响。

    

    神经规模定律（NSL）指的是模型性能随着规模增加而提高的现象。Sharma＆Kaplan使用近似理论分析了NSL，并预测了MSE损失的衰减方式为$N^{-\alpha}$，其中$\alpha=4/d$，$N$为模型参数数量，$d$为内在输入维度。尽管他们的理论在某些情况下效果良好（例如ReLU网络），但令人惊讶的是，我们发现在简单的1D问题$y=x^2$中，表现出了与他们预测不同的缩放定律（$\alpha=1$而不是$\alpha=4$）。我们打开了神经网络并发现新的缩放定律源于彩票票集成：平均而言，更宽的网络有更多的“彩票票”，它们被集成来减小输出的方差。我们通过对单个神经网络进行机械解释以及对它们进行统计研究来支持集成机制。我们将$N^{-1}$的缩放定律归因于“彩票票的中心极限定理”。最后，我们讨论了它的潜在影响。

    Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma & Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications f
    
[^132]: QXAI：可解释的人工智能框架用于患者监测系统中的定量分析

    QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems. (arXiv:2309.10293v1 [cs.AI])

    [http://arxiv.org/abs/2309.10293](http://arxiv.org/abs/2309.10293)

    该论文提出了一种QXAI框架，用于患者监测系统中的定量分析。该框架具有可解释性，对于决策者在医疗应用中基于非线性模型结果进行决策非常重要。

    

    人工智能技术可用于对患者的身体活动进行分类和预测远程患者监测的生命体征。基于非线性模型（如深度学习模型）的回归分析由于其黑盒性质而具有有限的可解释性。这可能需要决策者在医疗应用中基于非线性模型结果盲目决策。在非侵入式监测中，跟踪传感器获取的患者数据和其相关临床特征作为预测未来生命体征的输入特征。解释各种特征对监测应用整体输出的贡献对于临床医师的决策至关重要。本研究提出了一种QXAI框架，用于监督学习方法中回归和分类任务的事后模型可解释性和内在可解释性。这是通过利用Shapley值进行实现的。

    Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values c
    
[^133]: FRAMU: 基于注意力的联邦强化学习机器遗忘

    FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])

    [http://arxiv.org/abs/2309.10283](http://arxiv.org/abs/2309.10283)

    FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。

    

    机器遗忘是一个新兴领域，通过允许从机器学习过程中删除私有或无关数据，解决数据隐私问题。使用过时的、私有的和无关的数据会引发与隐私和模型效率相关的挑战。这些问题不仅影响模型在机器学习和遗忘中的准确性和计算效率，还会对数据隐私造成威胁。为了解决这些挑战，我们引入了一种新颖的框架，即基于注意力的联邦强化学习机器遗忘（FRAMU）。该框架融合了自适应学习机制、隐私保护技术和优化策略，是处理各种数据源（单模态或多模态）同时保持准确性和隐私性的综合解决方案。FRAMU的优势在于其适应波动的数据环境、遗忘过时、私有或无关数据的能力，以及支持模型持续演进的支持。

    Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
    
[^134]: 基于图增强的自适应智能时间序列预测强化学习

    Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])

    [http://arxiv.org/abs/2309.10186](http://arxiv.org/abs/2309.10186)

    本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。

    

    强化学习以其能够自适应地建模序列任务和学习潜在数据模式的能力而闻名。深度学习模型在回归和分类任务中得到了广泛的探索和应用。然而，深度学习存在一些限制，例如假设数据等间隔有序以及无法充分融入图结构等。图神经网络(GNN)能够克服这些挑战并捕捉时间序列数据的时间依赖关系。 在本研究中，我们提出了一种使用GNN和强化学习监控预测时间序列数据的新方法。GNN能够显式地将数据的图结构纳入模型，使其能够以更自然的方式捕捉时间依赖关系。该方法能够在复杂的时序结构中进行更准确的预测，例如医疗、交通和天气预测中的时序数据。

    Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
    
[^135]: 用编码-解码器进行可识别的认知诊断模型来建模学生的表现

    Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance. (arXiv:2309.00300v1 [cs.AI])

    [http://arxiv.org/abs/2309.00300](http://arxiv.org/abs/2309.00300)

    本文提出了一个可识别的认知诊断框架，该框架能够从学生的答题记录中直接诊断可识别和可解释的考生特征和题目特征，并通过重建答题记录来确保诊断结果的可识别性。

    

    认知诊断旨在根据学生在考试题目上的答题成绩来诊断他们的知识水平，这是许多领域如计算自适应测试的基础。现有的认知诊断模型（CDMs）遵循了一个能力-响应范式，即将诊断结果视为学生响应的原因，并通过优化来学习诊断结果。然而，这种范式很容易导致不可识别的诊断结果和解释过拟合问题，这对于学生学习表现的量化是有害的。为了解决这些问题，我们提出了一种新的可识别的认知诊断框架。具体而言，我们首先提出了一个灵活的诊断模块，该模块直接从响应日志中诊断可识别和可解释的考生特征和题目特征。接下来，我们利用一个通用的预测模块从诊断结果中重建响应日志，以确保诊断结果的可识别性。

    Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure
    
[^136]: 基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法

    ${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])

    [http://arxiv.org/abs/2308.11842](http://arxiv.org/abs/2308.11842)

    本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。

    

    在自然界中识别和分析对称模式已经在各个科学领域取得了重要的发现，例如物理学中的引力定律的制定和化学结构研究的进展。本文着重于利用在某些协作多智体强化学习（MARL）问题中固有的欧几里德对称性，以及在许多应用中普遍存在的对称性。我们首先通过形式化地描述一类具有一般对称性概念的马尔可夫博弈，该概念允许存在对称的最优值和策略。受到这些性质的启发，我们设计了具有对称约束的神经网络结构，作为多智体演员-评论家方法的归纳偏差。这种归纳偏差在各种协作MARL基准测试中表现出卓越的性能，并具有零样本学习和在具有重复对称模式的未见场景中的迁移学习等令人印象深刻的泛化能力。

    Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
    
[^137]: 学习使用对比学习进行通信

    Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])

    [http://arxiv.org/abs/2307.01403](http://arxiv.org/abs/2307.01403)

    本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。

    

    通信是多智能体强化学习中协调的有力工具。但在分散的环境中诱导一个有效的共同语言是一个困难的挑战。在这项工作中，我们引入了一个替代视角，即将智能体之间发送的通信消息视为环境状态的不完整视图。通过检查发送和接收的消息之间的关系，我们提出使用对比学习来最大化给定轨迹的消息之间的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作。使用定性指标和表示探测，我们展示了我们的方法诱导了更对称的通信并从环境中捕获了全局状态信息。总体而言，我们展示了对比学习的力量以及利用消息作为编码实现有效通信的重要性。

    Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
    
[^138]: 揭秘 GPT 自我修复代码生成能力

    Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])

    [http://arxiv.org/abs/2306.09896](http://arxiv.org/abs/2306.09896)

    本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。

    

    大型语言模型 (LLM) 在代码生成方面表现出色，但在挑战性编程任务上仍面临困难。自我修复——即模型调试并修复自己的代码——最近成为提高性能的一种流行方式。然而，关于自我修复如何有效地发挥作用的研究还非常有限。有人会想知道，当同一模型生成代码时，模型究竟能否提供准确的反馈。在本文中，我们分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，这是一个由多种编码挑战组成的具有挑战性的数据集。我们首先建立了一种新的评估策略 pass@t，该策略衡量了任务通过率与从模型中抽样的总标记数，从而实现对仅基于抽样的方法的公平比较。通过这种评估策略，我们发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，并确定了影响自我修复表现的几个因素。具体而言，我们发现，在输入噪声较少且模型对初始输出不太自信的较短和较简单的任务中，自我修复效果更好。我们还表明，仅在某些代码部分上应用自我修复可以非常有效。此外，我们提出了一种新的引导修复方法，利用外部反馈来增强 GPT 模型的自我修复能力，在 APPS 数据集上获得性能提升。

    Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
    
[^139]: 推荐系统如何从大型语言模型中受益：一项调查研究

    How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])

    [http://arxiv.org/abs/2306.05817](http://arxiv.org/abs/2306.05817)

    本文对将大型语言模型（LLM）应用于推荐系统进行了全面的调查研究，从两个角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。最后，我们提出了一些潜在的研究方向和挑战。

    

    推荐系统在匹配互联网应用程序用户的信息需求方面发挥着重要作用。在自然语言处理领域中，大型语言模型已经展现出了惊人的新兴能力（例如指令跟踪、推理），从而为将LLM调整到推荐系统中以提高性能和改善用户体验的研究方向带来了希望。在本文中，我们从应用导向的角度对此研究方向进行了全面的调查。我们首先从两个正交的角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。对于“在哪里”这个问题，我们讨论了LLM在推荐流程的不同阶段中可能发挥的作用，即特征工程、特征编码器、评分/排名函数和流程控制器。对于“如何”这个问题，我们调查了训练和推理策略，从而得出两个细粒度的分类标准，即是否调整LLM和是否将LLM作为独立模型或混合模型组件使用。最后，我们提出了在将LLM调整到RS中的一些挑战和潜在方向，包括与现有系统的集成、用户反馈、评估度量和知识蒸馏。

    Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
    
[^140]: 基于语言条件的模仿学习与基础技能先验下的非结构化数据应用

    Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data. (arXiv:2305.19075v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.19075](http://arxiv.org/abs/2305.19075)

    本文提出了一种结合基础技能先验和模仿学习的基于语言条件的通用方法，在非结构化数据下，以增强算法在适应不熟悉的环境方面的泛化能力。在零-shot设置下，在模拟和真实环境中测试，提高了CALVIN基准测试的得分。

    

    在语言条件下的机器人操作越来越受到关注，旨在开发能够理解和执行复杂任务的机器人，以实现机器人根据语言指令操作物体的目标。虽然语言条件方法在熟悉的环境中处理任务表现出了令人印象深刻的能力，但在适应不熟悉的环境设置方面遇到了限制。在本文中，我们提出了一个通用的、基于语言条件的方法，结合了基础技能先验和模仿学习在非结构化数据下，以增强算法在适应不熟悉的环境方面的泛化能力。我们在模拟和真实环境中使用零-shot设置来评估我们模型的性能。在模拟环境中，所提出的方法在CALVIN基准测试方面超过了以前报告的得分，特别是在具有挑战性的零-shot多环境设置中。完成任务的平均长度为...

    The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in both simulated and real-world environments using a zero-shot setting. In the simulated environment, the proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging Zero-Shot Multi-Environment setting. The average completed task length, in
    
[^141]: 如何逃离锐化的极小值点

    How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])

    [http://arxiv.org/abs/2305.15659](http://arxiv.org/abs/2305.15659)

    本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。

    

    现代机器学习应用程序中的优化算法已经取得了显著的成功，这些算法被设计用来发现平坦的极小值点。为了解决这个算法问题，本文采用损失函数海森矩阵的迹来度量它的平坦程度，并形式化定义了近似平坦极小值点的概念。在此概念下，我们设计了有效地找到近似平坦极小值点的算法。针对一般的损失函数，我们提出了一种基于梯度的算法，可以有效地找到近似平坦的局部极小值点。算法的主要组件是使用从随机扰动迭代中计算的梯度来估计导致更平坦极小值点的方向。对于成本函数是训练数据上的经验风险的设置，我们提出了一种更快速的算法，受最近提出的实用算法——锐度感知最小化的启发。

    Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
    
[^142]: 寻求可验证性: 解释很少能够在AI辅助决策中提高决策性能

    In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])

    [http://arxiv.org/abs/2305.07722](http://arxiv.org/abs/2305.07722)

    AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。

    

    目前关于AI辅助决策的文献，涉及可解释的AI系统为人类决策者提供建议，并呈现出一系列不确定和令人困惑的结果。为了综合这些发现，我们提出了一个简单的理论，阐明了AI解释经常无法促使适当的依赖和互补决策表现的失败。我们认为解释只有在允许人类决策者验证AI预测的正确性时才有用，而不是其他期望，例如可解释性或清晰阐述AI的推理过程。先前的研究发现，在许多决策环境中，AI解释并未促进这种验证。此外，无论解释方法如何，大多数环境基本上都无法进行验证。我们最后讨论了更有效的可解释AI辅助决策和人工智能协作的潜在方法。

    The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
    
[^143]: 评估ChatGPT的工作记忆容量

    Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])

    [http://arxiv.org/abs/2305.03731](http://arxiv.org/abs/2305.03731)

    本文评估了最先进语言模型ChatGPT的工作记忆容量，结果显示其在N-back任务的行为表现与人类参与者相似，这为设计具有人类级认知能力的人工智能系统提供了关键洞察。

    

    工作记忆是人类智能和人工智能的关键方面，它作为信息临时存储和操作的工作空间。本文通过检查ChatGPT在N-back任务上的表现，调查了这一最先进语言模型的工作记忆容量。我们首先讨论了工作记忆对人类和人工智能的重要性，接着介绍了评估ChatGPT工作记忆容量的方法。研究比较了ChatGPT在言语和空间N- back任务上的行为表现与文献报道的人类参与者的表现，发现了显著的相似之处。我们的发现为设计具有人类级认知能力的人工智能系统的当前进展提供了关键洞察，并为通过人工智能模型理解人类工作记忆的未来努力提供了前景。

    Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
    
[^144]: 使用文本到图像生成模型进行图像字幕数据管理

    Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])

    [http://arxiv.org/abs/2305.03610](http://arxiv.org/abs/2305.03610)

    本论文探究了通过数据整合来提高图像字幕质量的方案，并使用现有资源训练了比基准模型更好的模型。

    

    最近，图像字幕技术的发展主要依赖于大规模的视觉-语言预训练，并且日益依赖于计算资源和越来越大的多模态数据集。本文通过数据整合的两种方法探究了如何通过提高现有数据集中的样本质量来改善性能：一种方法假定由于图像和字幕之间的不匹配，某些示例应该避免使用，另一种方法则假定不匹配可以通过替换图像来解决。

    Recent advances in image captioning are mainly driven by large-scale vision-language pretraining, relying heavily on computational resources and increasingly large multimodal datasets. Instead of scaling up pretraining data, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot learning settings. Our simple yet effective approaches consistently outperform baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the error
    
[^145]: 正向人工智能：设计以幸福为导向的人工智能的关键挑战

    Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence. (arXiv:2304.12241v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2304.12241](http://arxiv.org/abs/2304.12241)

    设计以幸福为导向的人工智能系统面临着知识和动机两方面的挑战，包括概念化、测量和优化幸福，设计适当的AI行动，以及激励措施、财务和宣传风险的不一致以及数据获取的缺乏。针对这些挑战，需要在科学理解AI系统对幸福影响方面进行研究，并指导设计行动。

    

    人工智能（AI）正迅速改变社会，迫切需要确保其积极影响。本文采用积极设计方法来解决这个问题，将其视为设计主动支持人类幸福的AI系统的问题。然而，设计以幸福为导向的AI系统是困难的。本文采用控制论的视角，识别了两个类别中的十二个关键挑战：知识缺乏和动机缺乏。知识障碍包括概念化、测量和优化幸福，并设计适当的AI行动方面的挑战。动机障碍包括不一致的激励措施、财务和宣传风险，以及缺乏数据获取阻止了（第三方）对幸福进行研究。为了应对这些挑战，我们提出了一个研究议程，包括推进对AI系统对幸福影响的科学理解，并指导如何进行AI系统的设计行动。

    Artificial Intelligence (AI) is rapidly transforming society, creating an urgent need to ensure its positive impact. In this article, we take a positive design approach towards this issue, viewing it as a matter of designing AI systems that actively support human wellbeing. However, designing wellbeing-aligned AI systems is difficult. This article adopts a cybernetic perspective to identify twelve key challenges across two categories: lack of knowledge and lack of motivation. Knowledge barriers include challenges in conceptualizing, measuring, and optimizing for wellbeing, then designing appropriate AI actions. Motivation barriers include misaligned incentives, financial and publicity risks, and a lack of data access preventing (third-party) research on wellbeing. To address these challenges we have captured our key takeaways in a research agenda related to 1) advancing the scientific understanding of the impact of AI systems on wellbeing, and 2) guiding design actions on how AI system
    
[^146]: 带需求的机器学习：一份宣言

    Machine Learning with Requirements: a Manifesto. (arXiv:2304.03674v1 [cs.LG])

    [http://arxiv.org/abs/2304.03674](http://arxiv.org/abs/2304.03674)

    本文提出一个带需求的机器学习宣言，认为需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域，作者提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。

    

    在近年来，机器学习取得了长足的进步，成为许多不同应用领域突破的根源。然而，如何将它们应用到高风险或安全关键的应用领域仍然是一个未解决的问题，因为它们往往容易变得脆弱和不可靠。本文认为，需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域。为此，我们提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果。我们展示了如何将需求规格说明有益地整合到标准的机器学习开发流程中，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。

    In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.
    
[^147]: 寻找具有身体智能的人工视觉皮层在哪里？

    Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])

    [http://arxiv.org/abs/2303.18240](http://arxiv.org/abs/2303.18240)

    该研究研究了使用预训练视觉表征来实现身体智能的最新进展。他们展示了最大、最全面的经验研究，发现没有一种表征是普遍优越的，并且数据集的大小和多样性并不能普遍改善性能。

    

    我们提出了最大、最全面的预训练视觉表征（PVR）或视觉基础模型的经验研究，用于身体智能。首先，我们策划了 CortexBench，其中包括涵盖动力学、导航、熟练和移动操作的17种不同任务。接下来，我们系统评估了现有的PVR，发现没有一种是普遍优越的。为了研究预训练数据规模和多样性的影响，我们结合了来自7个不同来源的超过4000小时的自我中心视频（超过560万张图像）和ImageNet，使用切片数据的遮盖自编码（MAE）来训练不同大小的视觉变形器。与先前的工作推断相反，我们发现扩展数据集的规模和多样性并不能普遍改善性能（但平均性能有所提高）。我们最大的模型名为VC-1，平均表现超过所有先前的PVR，但也没有普遍优势。最后，我们展示了VC-1的特定于任务或领域的适应会带来实质性的改进。

    We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia
    
[^148]: VIDIMU: 使用价格实惠的设备记录日常生活活动的多模态视频和IMU运动学数据集

    VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])

    [http://arxiv.org/abs/2303.16150](http://arxiv.org/abs/2303.16150)

    VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。

    

    人体活动识别和临床生物力学是物理远程康复医学中的挑战性问题。然而，大多数公开可用的人体动作数据集不能用于研究实验室外运动获取情况下的这两个问题。VIDIMU数据集的目的是为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。该数据集包括使用商品相机和五个惯性传感器注册的13种活动。记录视频的54个受试者中，其中16个受试者同时还有惯性传感器记录。VIDIMU的创新之处在于：i）所选择的动作的临床相关性，ii）使用价格实惠的视频和自定义传感器的组合，以及 iii）实现了先进的多模态数据处理工具，可以从惯性数据中对三维身体姿势跟踪和运动重建在肌肉骨骼模型中。

    Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
    
[^149]: 使用深度学习集成的数据增强多视角方法识别化石图像

    Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews. (arXiv:2302.08062v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08062](http://arxiv.org/abs/2302.08062)

    该论文提出了使用深度学习集成的数据增强多视角方法识别化石图像。通过收集化石图像的不同视角，使用多个基础模型进行训练，并通过软投票进行最终决策。实验证明，该方法在性能上优于基线方法，并且与使用三个原始视图模型的方法相当。

    

    化石物种的识别对于进化研究至关重要。近期深度学习的进展在化石图像识别方面显示出了很有前景的发展。然而，由于化石保存、有条件采样以及领域专家昂贵且不一致的标签注释的限制，标记化石图像的数量和质量通常都受到限制，从而给使用基于深度学习的图像分类模型训练带来了巨大挑战。为了解决这些问题，我们遵循“群众的智慧”的思想，提出了一个多视角集成框架，通过收集每个化石图像的原始视图、灰度视图和骨架视图来训练多个基础模型，并通过软投票进行最终决策。在包含2400张图像的最大造针螺化石数据集上的实验证明，所提出的OGS方法始终优于基线方法（针对每个视图使用单一模型），并且在性能上优于或可与OOO方法相媲美（使用三个原始视图模型）。

    Identification of fossil species is crucial to evolutionary studies. Recent advances from deep learning have shown promising prospects in fossil image identification. However, the quantity and quality of labeled fossil images are often limited due to fossil preservation, conditioned sampling, and expensive and inconsistent label annotation by domain experts, which pose great challenges to training deep learning based image classification models. To address these challenges, we follow the idea of the wisdom of crowds and propose a multiview ensemble framework, which collects Original (O), Gray (G), and Skeleton (S) views of each fossil image reflecting its different characteristics to train multiple base models, and then makes the final decision via soft voting. Experiments on the largest fusulinid dataset with 2400 images show that the proposed OGS consistently outperforms baselines (using a single model for each view), and obtains superior or comparable performance compared to OOO (us
    
[^150]: 我不想说：在可选个人数据模型中保护用户同意

    I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13954](http://arxiv.org/abs/2210.13954)

    该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。

    

    我们研究了一种机器学习模型，其中个人可以选择与决策系统共享可选个人信息，这在现代保险定价模型中很常见。一些用户同意使用他们的数据，而其他人则反对并保持其数据未公开。本文表明，不共享数据的决定本身可以被视为信息，应该受到保护，以尊重用户的隐私。这一观察结果引发了一个被忽视的问题，即如何确保保护其个人数据的用户不会因此受到任何不利影响。为了解决这个问题，我们对仅使用获得积极用户同意的信息的模型进行了保护要求的正式化。这排除了作出共享数据与否决定所包含的隐含信息。我们提出了Protected User Consent (PUC)概念，这是我们证明在保护要求下损失最小的解决方案。

    We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
    
[^151]: 解释、公正性和人类-AI决策中的适当依赖（arXiv:2209.11812v3 [cs.HC] UPDATED）

    Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2209.11812](http://arxiv.org/abs/2209.11812)

    本研究探讨了基于特征的解释对AI辅助决策公正性的影响，发现解释可以影响公正性感知和人们对AI建议的依赖。然而，解释并不能帮助人们区分正确和错误的AI建议。这些发现对于促进有效的决策和公正性至关重要。

    

    在这项工作中，我们研究了基于特征的解释对AI辅助决策的公正性的影响，特别关注从简短的文本简介中预测职业的任务。我们还研究了这些影响如何通过人们的公正性感知和对AI建议的依赖来调节。我们的研究结果表明，解释影响了公正性感知，而公正性感知又与人们遵循AI建议的倾向相关。然而，我们发现这样的解释并不能让人们区分正确和错误的AI建议。相反，我们发现解释可能会影响依赖，而不论AI建议的正确性如何。取决于解释突出的特征，这可以促进或阻碍分配公正性：当解释突出与敏感属性明显相关的与任务无关的特征时，这会促使人们覆盖AI与性别刻板印象一致的建议。

    In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans' fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanw
    

