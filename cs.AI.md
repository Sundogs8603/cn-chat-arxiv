# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Control-Centric Benchmark for Video Prediction.](http://arxiv.org/abs/2304.13723) | 本文提出了一个以控制为中心的视频预测基准，评估给定模型在通过采样规划对模拟机器人操作的表现。该基准包含有11个任务类别和310个任务实例定义的模拟环境，以及完整的规划实现和训练数据集，以解决现有指标在预测任务执行成功方面不可靠的问题。 |
| [^2] | [Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery.](http://arxiv.org/abs/2304.13714) | 本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。 |
| [^3] | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.](http://arxiv.org/abs/2304.13712) | 本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。 |
| [^4] | [HeySQuAD: A Spoken Question Answering Dataset.](http://arxiv.org/abs/2304.13689) | HeySQuAD 是一个大规模口语化问答数据集，旨在衡量机器理解并回答嘈杂的口语提问的能力，同时使用转录的人类口语提问进行训练能显著提高模型表现。 |
| [^5] | [Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning.](http://arxiv.org/abs/2304.13688) | 联邦机器学习是一种新的人工智能范式，可以从数据孤岛中创建AI模型，挑战在于建立多方合作业务模式。本研究系统化了联邦机器学习项目的社会技术挑战和业务模式。 |
| [^6] | [Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process.](http://arxiv.org/abs/2304.13671) | 本文研究了自动化ATM现金补充流程，提出了一个数学模型并给出了一个工具来评估各种不同的情况。在模拟数据集上，该模型与方法可以削减ATM现金运营成本。 |
| [^7] | [Using Implicit Feedback to Improve Question Generation.](http://arxiv.org/abs/2304.13664) | 本研究提出了一个名为GEN的系统，利用一小组句子/问题对作为输入，通过基于模式的方法生成问题。该系统可以从用户的反馈中学习并提高生成问题的质量。 |
| [^8] | [PVP: Pre-trained Visual Parameter-Efficient Tuning.](http://arxiv.org/abs/2304.13639) | 本文提出了一种名为PVP的新方法，通过利用预训练模型的统计信息和无监督聚类方法初始化提示模块，可以在极少的标记数据（每类一到两个示例）的情况下获得具有竞争力的性能。 |
| [^9] | [AutoCure: Automated Tabular Data Curation Technique for ML Pipelines.](http://arxiv.org/abs/2304.13636) | AutoCure是一种无需配置的数据整理管道，可提高表格数据的质量，通过自适应集成的误差检测方法和数据增强模块，从合乎规范的数据部分中增强了清晰数据分数的密度。 |
| [^10] | [The Roles of Symbols in Neural-based AI: They are Not What You Think!.](http://arxiv.org/abs/2304.13626) | 符号在基于神经网络的人工智能中并不只用于代表某个概念，还可作为外部和内部的沟通工具，提高知识传递效率，并对学习世界提供有益约束。 |
| [^11] | [Impact of Position Bias on Language Models in Token Classification.](http://arxiv.org/abs/2304.13567) | 研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。 |
| [^12] | [Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning.](http://arxiv.org/abs/2304.13545) | 本文提出了一种分布式学习中的综合解决方案，基于量化同时实现通讯效率和隐私保护，并且向均匀量化的梯度添加二项噪声以达到所需的差分隐私级别。 |
| [^13] | [Tensor Decomposition for Model Reduction in Neural Networks: A Review.](http://arxiv.org/abs/2304.13539) | 本文综述了六种张量分解方法，并讨论了将神经网络层替换为低秩张量逼近的方案。实验结果表明这种方法能够显著降低模型大小、运行时间和能量消耗，并且适合于在边缘设备上实现神经网络。 |
| [^14] | [ElegansNet: a brief scientific report and initial experiments.](http://arxiv.org/abs/2304.13538) | ElegansNet是一种神经网络，利用Caenorhabditis elegans的连接组的结构作为参考，设计生成了具有类似于自然网络拓扑的深度学习系统，在效率、性能上均胜过随机连线网络和人工网络。 |
| [^15] | [Key-value information extraction from full handwritten pages.](http://arxiv.org/abs/2304.13530) | 这篇论文提出了一种从手写文档中提取信息的方法，该方法基于Transformer，在同一模型中结合了特征提取、手写识别和命名实体识别步骤。该方法不需要预先分割并在三个公共数据库上表现出色。 |
| [^16] | [Sequential decomposition of propositional logic programs.](http://arxiv.org/abs/2304.13522) | 研究命题逻辑程序的序列分解，通过分析程序之间的Green关系，为逻辑编程代数理论进一步发展作出了贡献。 |
| [^17] | [Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation.](http://arxiv.org/abs/2304.13513) | 本文提出了一种对于病理图像分割领域偏移问题的解决方法，通过使用簇熵来选择有效的WSI并可以显著提高半监督域自适应的性能。 |
| [^18] | [Towards clinical AI fairness: A translational perspective.](http://arxiv.org/abs/2304.13493) | 研究讨论了AI在医疗保健领域中公平性问题，指出了技术和临床视角之间的不匹配，并提出了跨学科合作以解决这一问题的可能性。 |
| [^19] | [Fundamental Tradeoffs in Learning with Prior Information.](http://arxiv.org/abs/2304.13479) | 本文研究了先验信息准确性和学习性能之间的基本权衡，引入了优先风险概念，并为统计估计问题提供了下界，展现了框架在不同问题中的应用。 |
| [^20] | [Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning.](http://arxiv.org/abs/2304.13443) | 本文提出了一种基于策略的强化学习方法，通过重新安排地铁时刻表和调整列车的停靠时间和巡航速度，优化扰动下的地铁系统能源效率，该方法在模拟环境下实验证明其优于基线方法，最高可达降低10.9%的牵引能量消耗和最高达提高47.9%的再生制动能量利用率，为城市轨道交通的节能问题提供了有效的解决方案。 |
| [^21] | [Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories.](http://arxiv.org/abs/2304.13424) | 本文研究了强化学习代理对超出分布的“可控”状态的“接力泛化”性能。通过让测试代理从其他陌生代理的轨迹中间开始，发现这种泛化普遍存在泛化失效问题。 |
| [^22] | [Conjunctive Query Based Constraint Solving For Feature Model Configuration.](http://arxiv.org/abs/2304.13422) | 本文介绍了一种使用联结查询解决约束满足问题的方法，特别是用于特征模型配置任务，使得能够利用数据库技术来解决配置问题并提供新的算法方法。 |
| [^23] | [Filter Pruning via Filters Similarity in Consecutive Layers.](http://arxiv.org/abs/2304.13397) | 本论文提出通过连续层中的滤波器相似度进行滤波器剪枝的方法（FSCL），并取得了在准确性、模型大小和运算量等方面显着的改进。 |
| [^24] | [VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs.](http://arxiv.org/abs/2304.13386) | 本论文介绍了一种名为VGOS的方法，可以快速从稀疏输入（3-10个视图）中重建放射场，以解决体素网格更容易过度拟合训练视图的问题，并提出了增量体素训练策略来避免过度拟合。 |
| [^25] | [Blockchain-based Access Control for Secure Smart Industry Management Systems.](http://arxiv.org/abs/2304.13379) | 本文提出了一种基于角色的访问控制方法，通过在区块链和智能合约中实现来保障基于云的智能制造系统数据的安全性。 |
| [^26] | [Feed-Forward Optimization With Delayed Feedback for Neural Networks.](http://arxiv.org/abs/2304.13372) | 本文提出了一种延迟反馈的前馈神经网络优化方法F^3，使用延迟的误差信息来缩放梯度从而提高生物可行性和计算效率，具有较高的预测性能，为低能量训练和并行化提供了新思路。 |
| [^27] | [LoRaWAN-enabled Smart Campus: The Dataset and a People Counter Use Case.](http://arxiv.org/abs/2304.13366) | 本文介绍一个基于LoRaWAN的智能校园数据集，使用k近邻和LSTM方法处理丢失值和预测未来读数，并构建一个深度神经网络来预测房间内人数，准确率达到95％。 |
| [^28] | [Neuro-symbolic Zero-Shot Code Cloning with Cross-Language Intermediate Representation.](http://arxiv.org/abs/2304.13350) | 本研究介绍了一种可以跨越多种编程语言，通过神经符号学方法进行零样本跨编程语言代码搜索的中间表示方式，并在COBOL编程语言上进行了验证，使得代码克隆任务的性能获得了12.85 MAP @ 2的提升。 |
| [^29] | [Discrepancy-Guided Reconstruction Learning for Image Forgery Detection.](http://arxiv.org/abs/2304.13349) | 本论文提出了一种基于差异引导的图像篡改检测方法，能够提升模型对篡改敏感且具有紧凑视觉模式的学习能力，具有较广泛的推广性。 |
| [^30] | [Evaluation of Regularization-based Continual Learning Approaches: Application to HAR.](http://arxiv.org/abs/2304.13327) | 本研究评估了基于正则化的连续学习方法在HAR领域的应用，并比较了三种方法的优缺点。实验证明，这些方法提高了模型学习新类别的能力，同时保持了模型在先前学习的类别上的准确性。 |
| [^31] | [A Portrait of Emotion: Empowering Self-Expression through AI-Generated Art.](http://arxiv.org/abs/2304.13324) | 探究了AI在创造性表达中表达情感方面的潜力与局限，AI可以促进创造力和情感的自我表达。 |
| [^32] | [Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs.](http://arxiv.org/abs/2304.13312) | 本文提出了一种通过量化输入变量之间的编码交互来准确且简明地解释深度神经网络(DNN)的推理逻辑的方法。针对此目的，作者提出了两种交互方式，即AND交互和OR交互，并利用它们设计出一系列技术来提高解释的简洁性，同时不会损害准确性。 |
| [^33] | [HiQ -- A Declarative, Non-intrusive, Dynamic and Transparent Observability and Optimization System.](http://arxiv.org/abs/2304.13302) | HiQ是一种可透明监控Python程序运行时信息的系统，具有非侵入性和动态性，可应用于离线/在线应用程序和分布式系统，我们可以使用它来优化神经网络模型并捕捉瓶颈，而不影响代码的干净程度和性能表现。 |
| [^34] | [A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL.](http://arxiv.org/abs/2304.13301) | 本文提出了一个基于案例推理框架的跨域文本到SQL自适应提示的解决方案，可以精确控制与案例相关和不相关的知识，解决了大型语言模型提示设计不良限制性能的问题。 |
| [^35] | [Game-based Platforms for Artificial Intelligence Research.](http://arxiv.org/abs/2304.13269) | 本文回顾了基于游戏的人工智能研究平台，讨论了不同研究领域和创意设计在其中的应用和发展，并探讨了其未来趋势。 |
| [^36] | [Bayesian Federated Learning: A Survey.](http://arxiv.org/abs/2304.13267) | 本文综述了贝叶斯联邦学习（BFL）的基本概念和分类，包括客户端、服务器端和基于FL的BFL方法。BFL是解决现有FL方法中受限和动态的数据和条件、异质性和不确定性以及分析解释能力挑战的有前途方法。 |
| [^37] | [Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation.](http://arxiv.org/abs/2304.13224) | 本文提出了一种基于BSDE的扩散模型，可以通过调整现有的分数函数确定到达所需终端分布所需的初始条件，为扩散建模提供了一种新的方法，并在扩散反演，条件扩散和不确定性量化等领域具有潜在应用。 |
| [^38] | [Reinforcement Learning with Partial Parametric Model Knowledge.](http://arxiv.org/abs/2304.13223) | 该论文研究了在环境完全无知和完美知识之间的机遇，提出了一种利用局部模型和保持数据驱动调整的强化学习方法，已在线性二次调节器上得到验证。 |
| [^39] | [Exploiting CNNs for Semantic Segmentation with Pascal VOC.](http://arxiv.org/abs/2304.13216) | 本文在Pascal VOC数据集上进行了语义分割的研究，使用了FCN作为基准并改进了其问题，最终发现使用ResNet进行迁移学习的表现最佳。 |
| [^40] | [Towards Explainable and Safe Conversational Agents for Mental Health: A Survey.](http://arxiv.org/abs/2304.13191) | 这篇论文调查了现有的心理健康会话型智能助手，提出了改进的新见解，并介绍了如何构建责任VMHA，以提出后续问题或提供知情回应，丰富用户体验。 |
| [^41] | [Onboard Science Instrument Autonomy for the Detection of Microscopy Biosignatures on the Ocean Worlds Life Surveyor.](http://arxiv.org/abs/2304.13189) | 在太阳系的冰卫星寻找外星生命，需要用一套补充仪器对多个独立的生物标志进行采样。机载科学仪器自主性（OSIA）是一种新兴学科，可以评估、总结和优先考虑观测仪器数据以最大化科学回报。 |
| [^42] | [AI-assisted coding: Experiments with GPT-4.](http://arxiv.org/abs/2304.13187) | 该论文报告了几个基于GPT-4的人工智能辅助编程实验，发现使用当前工具的人工智能代码生成虽然强大，但需要大量人类验证以确保准确性和有效性，同时GPT-4可以通过重构现有代码显著提高代码质量，并生成具有实质性覆盖范围的测试，但应用于相关代码时，许多测试失败了。 |
| [^43] | [Towards Compute-Optimal Transfer Learning.](http://arxiv.org/abs/2304.13164) | 本文提出一种计算优化的迁移学习方法，通过对预训练模型进行零-shot结构剪枝，使其在最小降低性能的情况下提高计算效率，实现了20%以上的性能提升。 |
| [^44] | [Roll-Drop: accounting for observation noise with a single parameter.](http://arxiv.org/abs/2304.13150) | 本文提出了一种单一参数的策略Roll-Drop，在深度强化学习中使用dropout来考虑观测噪声，而不需要显式建模噪声分布，从而提升了模拟到真实环境的转移能力。 |
| [^45] | [Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection.](http://arxiv.org/abs/2304.13148) | 这篇论文介绍了MBIB，一个将不同类型媒体偏见分为共同框架的全面基准测试，并提供了相关数据集以评估媒体偏见检测技术，结果显示没有单一技术可以显著优于其他技术，同时发现研究兴趣和资源分配不均匀分布。 |
| [^46] | [Self-Supervised Temporal Analysis of Spatiotemporal Data.](http://arxiv.org/abs/2304.13143) | 本文提出了一种自监督方法，能根据移动活动时间序列对景观进行分层，通过深度语义分割实现了对地理空间任务的多模态建模，适用于分类居民区和商业区等不同任务。 |
| [^47] | [Quantum Machine Learning Approach for the Prediction of Surface Roughness in Additive Manufactured Specimens.](http://arxiv.org/abs/2304.13142) | 首次针对增材制造试件的表面粗糙度使用三种量子算法（QNN、Q-Forest和VQC）进行回归预测，其中Q-Forest算法表现最优，具有较低的MSE和MAE和较高的EVS。 |
| [^48] | [The Update Equivalence Framework for Decision-Time Planning.](http://arxiv.org/abs/2304.13138) | 该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。 |
| [^49] | [Precision Spectroscopy of Fast, Hot Exotic Isotopes Using Machine Learning Assisted Event-by-Event Doppler Correction.](http://arxiv.org/abs/2304.13120) | 提出一种新的实验方案，结合机器学习辅助的事件逐事件多普勒修正，实现对快速、热的异位同位素进行高精度的激光光谱学研究，在极端温度下仍能实现kHz级别的不确定性。 |
| [^50] | [Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI.](http://arxiv.org/abs/2304.13109) | 本文提出了一个联邦深度强化学习的方法，可以用于在有限CSI的情况下迅速进行THz波束搜索，以克服THz信号的传播衰减。 |
| [^51] | [Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI.](http://arxiv.org/abs/2304.13107) | 这篇论文提出了一种使用基于WiFi信道状态信息提取人体移动和空间特征的无设备多房间人体存在检测系统，能够通过时间-selective特征提取算法区分有直觉视线路径阻塞和无视线路径阻塞的情况。 |
| [^52] | [Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System.](http://arxiv.org/abs/2304.13105) | 本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。 |
| [^53] | [LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid.](http://arxiv.org/abs/2304.13104) | 本文研究了LSTM神经网络在微电网负荷预测中受到噪声注入攻击的鲁棒性。使用低通滤波器消除了攻击，以提高模型的性能。 |
| [^54] | [HyMo: Vulnerability Detection in Smart Contracts using a Novel Multi-Modal Hybrid Model.](http://arxiv.org/abs/2304.13103) | 本文提出了一种名为 HyMo 的多模态混合深度学习模型，结合了 FastText 词嵌入技术和 BiGRU 深度学习技术，用于智能合约漏洞检测。 |
| [^55] | [Organizational Governance of Emerging Technologies: AI Adoption in Healthcare.](http://arxiv.org/abs/2304.13081) | 该研究通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，制定了AI在医疗保健中的组织治理框架，包括关键控制点和决策标准，为卫生系统领导人做出更加明智的决策提供了支持。 |
| [^56] | [Optimizing Deep Learning Models For Raspberry Pi.](http://arxiv.org/abs/2304.13039) | 针对树莓派优化深度学习模型包括修剪技术和模型参数结构优化，以适应其硬件特点并提高能效。 |
| [^57] | [A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction.](http://arxiv.org/abs/2304.13032) | 提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。 |
| [^58] | [Investigations into Proof Structures.](http://arxiv.org/abs/2304.12827) | 介绍了一种新的形式主义来操作和分析证明，用于生成更短的证明和减少搜索工作量。 |
| [^59] | [Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic and Structure Constraints.](http://arxiv.org/abs/2304.12591) | 本文采用对比学习和一致的语义和结构约束来减少合成和细化图像之间的语义失真，进一步提高了性能。 |
| [^60] | [Learning Partial Correlation based Deep Visual Representation for Image Classification.](http://arxiv.org/abs/2304.11597) | 本文提出了一种基于偏相关的深度视觉表示学习方法，解决了使用协方差矩阵表征相关性在存在混淆效应时的误导问题。 |
| [^61] | [Boosting Theory-of-Mind Performance in Large Language Models via Prompting.](http://arxiv.org/abs/2304.11490) | 本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。 |
| [^62] | [Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance.](http://arxiv.org/abs/2304.11127) | 该论文介绍了一种广泛使用的贝叶斯优化方法 Tree-structured Parzen estimator (TPE)，并对其控制参数的作用和算法直觉进行了讨论和分析，提供了一组推荐设置并证明其能够提高TPE的性能表现。 |
| [^63] | [Segment Anything Model for Medical Image Analysis: an Experimental Study.](http://arxiv.org/abs/2304.10517) | 本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。 |
| [^64] | [A baseline on continual learning methods for video action recognition.](http://arxiv.org/abs/2304.10335) | 本文基于视频动作识别场景，提出最先进的连续学习方法的基准研究，并证明回顾方法优于其他方法。此外，提出的内存效率变体可有效地保持一定水平的性能。 |
| [^65] | [LLM as A Robotic Brain: Unifying Egocentric Memory and Control.](http://arxiv.org/abs/2304.09349) | 本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。 |
| [^66] | [TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation.](http://arxiv.org/abs/2304.07728) | 本文提出了一种可解释的基于Transformer的LiDAR-惯性融合里程计估计方法，通过多头注意力融合模块展示了不同融合方法，实验表明该方法在KITTI和EuRoC数据集上表现优于现有方法。 |
| [^67] | [The Second Monocular Depth Estimation Challenge.](http://arxiv.org/abs/2304.07051) | 本文介绍了单目深度估计挑战赛的第二届比赛结果，高质量的SYNS-Patches数据集提高了比赛难度，所有提交作品都超过了基准水平。 |
| [^68] | [Regulatory Markets: The Future of AI Governance.](http://arxiv.org/abs/2304.04914) | 提出一种监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务，以克服过度依赖行业自律和立法机构缺乏专业知识的局限性，从而逐步实现人工智能的恰当监管。 |
| [^69] | [DiffMimic: Efficient Motion Mimicking with Differentiable Physics.](http://arxiv.org/abs/2304.03274) | 本文提出了DiffMimic，一种基于可微分物理的高效运动模仿方法。与传统强化学习方法相比，其有更快更稳定的收敛速度；同时通过演示重播机制避免陷入局部最优解。 |
| [^70] | [Friend Ranking in Online Games via Pre-training Edge Transformers.](http://arxiv.org/abs/2302.10043) | 本文提出了一种使用边缘Transformer和预训练的链接预测方法，用于在在线游戏中进行好友排名，达到了最先进的结果。 |
| [^71] | [Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most.](http://arxiv.org/abs/2302.09195) | 该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。 |
| [^72] | [Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation.](http://arxiv.org/abs/2302.02561) | 该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。 |
| [^73] | [Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2.](http://arxiv.org/abs/2301.11719) | 本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。 |
| [^74] | [BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI.](http://arxiv.org/abs/2212.10802) | 本文提出了一种基于半监督学习的双折叠师生网络，该网络通过利用部分标记和未标记的数据集智能地学习空间和时间特征，有效地解决了基于CSI的室内存在检测受到环境变化和有监督学习方法需要耗时标注的问题。 |
| [^75] | [Prompting Is Programming: A Query Language for Large Language Models.](http://arxiv.org/abs/2212.06094) | LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合，实现了一种新的语言模型编程方式。 |
| [^76] | [VISEM-Tracking, a human spermatozoa tracking dataset.](http://arxiv.org/abs/2212.02842) | 本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。 |
| [^77] | [Neural Fourier Filter Bank.](http://arxiv.org/abs/2212.01735) | 该论文提出了一个新的神经傅里叶滤波器组方法，该方法既在空间上又在频率上分解信号，通过使用傅里叶编码特定频率来存储每个网格，这一方法在2D图像拟合、3D形状重建和神经辐射场等多个任务上，表现出优于现有技术的模型紧凑性和收敛速度。 |
| [^78] | [CrossSplit: Mitigating Label Noise Memorization through Data Splitting.](http://arxiv.org/abs/2212.01674) | 本文提出了一种名为CrossSplit的新训练程序，通过使用交叉分割的标签修正和半监督训练两个主要组成部分，缓解了深度学习算法中标签噪声记忆问题，具有良好的效果。 |
| [^79] | [Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models.](http://arxiv.org/abs/2211.17091) | 本文提出了“鉴别器引导”方法，通过在评分训练之后训练鉴别器，使模型评估更加准确，从而改善预训练扩散模型的样本生成。在 ImageNet 256x256 数据集上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID 和召回率。 |
| [^80] | [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.](http://arxiv.org/abs/2211.05105) | 该论文提出了一种名为安全潜向扩散的方法，可以在图像生成过程中移除和抑制不当的图像部分，从而缓解基于文本的图像生成模型因不当数据集带来的不良影响。 |
| [^81] | [Learning image representations for anomaly detection: application to discovery of histological alterations in drug development.](http://arxiv.org/abs/2210.07675) | 该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。 |
| [^82] | [Augmented Driver Behavior Models for High-Fidelity Simulation Study of Crash Detection Algorithms.](http://arxiv.org/abs/2208.05540) | 本文提出了一种基于增强型驾驶员行为模型的混合运输系统仿真平台，模拟驾驶员与自动驾驶车辆的交互，在高保真的仿真研究中可用于评估碰撞检测算法的性能。 |
| [^83] | [One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training.](http://arxiv.org/abs/2207.10283) | 本文提出了一种名为SOVR的对抗训练损失函数，可以聚焦重要样本，增加对抗攻击下的对数几率间隔，从而在实验中表现出对抗攻击的有效性。 |
| [^84] | [Language Modelling with Pixels.](http://arxiv.org/abs/2207.06991) | 本文介绍了一个名为PIXEL的基于像素的预训练语言模型，它可以将文本渲染为图像，解决了扩展支持的语言数量时出现的词汇瓶颈问题，且在形态学和语义任务上显著优于BERT。 |
| [^85] | [Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks.](http://arxiv.org/abs/2110.08260) | 该论文提出了一种新的抽象解释框架，使我们能够精确地过度逼近数值Fixpoint迭代器。使用一种新的抽象域，CH-Zonotope，可以实现有效的传播和包含检查。该框架在研究monDEQ神经网络结构时表现出色，是一种有前途的验证技术。 |

# 详细

[^1]: 一种以控制为中心的视频预测基准

    A Control-Centric Benchmark for Video Prediction. (arXiv:2304.13723v1 [cs.CV])

    [http://arxiv.org/abs/2304.13723](http://arxiv.org/abs/2304.13723)

    本文提出了一个以控制为中心的视频预测基准，评估给定模型在通过采样规划对模拟机器人操作的表现。该基准包含有11个任务类别和310个任务实例定义的模拟环境，以及完整的规划实现和训练数据集，以解决现有指标在预测任务执行成功方面不可靠的问题。

    

    视频是学习世界动态模型的体现代理人的有希望的知识来源。大型深度网络在自我监督的情况下越来越有效地建模复杂的视频数据，评估基于人类感知相似性或像素比较的指标。然而，目前的指标是否准确预测下游任务的表现仍不清楚。我们实验证明，对于规划机器人操作来说，现有指标在预测任务执行成功方面可能不可靠。为了解决这个问题，我们提出了一种行动条件下的视频预测基准，即通过采样规划对给定模型进行评估的控制基准。我们的基准，用于视觉规划的视频预测 ($VP^2$)，包括11个任务类别和310个任务实例定义的模拟环境、完整的规划实现和包含脚本交互轨迹的训练数据集。

    Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($VP^2$), includes simulated environments with 11 task categories and 310 task instance definitions, a full planning implementation, and training datasets containing scripted interaction traje
    
[^2]: 评估GPT-3.5和GPT-4在支持医疗保健信息需求方面的实际作用

    Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])

    [http://arxiv.org/abs/2304.13714](http://arxiv.org/abs/2304.13714)

    本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。

    

    尽管在医疗保健领域使用大型语言模型(LLMs)越来越受关注，但当前的探索并未评估LLMs在临床环境中的实用性和安全性。我们的目标是确定两个LLM是否可以以安全和一致的方式满足由医生提交的信息需求问题。我们将66个来自信息技术咨询服务的问题通过简单的提示提交给GPT-3.5和GPT-4。12名医生评估了LLM响应对患者造成伤害的可能性以及与信息技术咨询服务的现有报告的一致性。医生的评估基于多数票汇总。对于没有任何问题，大多数医生认为任何一个LLM响应都不会造成伤害。对于GPT-3.5，8个问题的响应与信息技术咨询报告一致，20个不一致，9个无法评估。有29个响应没有多数票表示“同意”、“不同意”和“无法评估”。

    Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
    
[^3]: 发挥LLMs在实践中的力量：ChatGPT及其应用的综述调查

    Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])

    [http://arxiv.org/abs/2304.13712](http://arxiv.org/abs/2304.13712)

    本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。

    

    本文为从事下游自然语言处理（NLP）任务的从业人员和最终用户提供了一个全面实用的指南，介绍了如何利用Large Language Models（LLMs）。我们从模型、数据和下游任务的角度提供了LLMs的使用讨论和见解。首先，我们介绍了当前的GPT和BERT样式的LLMs。然后，讨论了预训练数据、训练数据和测试数据的影响。最重要的是，我们详细讨论了大型语言模型在各种自然语言处理任务中的使用和非使用情况，例如知识密集型任务、传统自然语言理解任务、自然语言生成任务、紧急能力以及特定任务的考虑。我们呈现了各种使用和非使用情况，以说明LLMs在实际情况下的实际应用和限制。我们还试图了解数据对于LLMs应用的重要性。

    This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
    
[^4]: HeySQuAD: 一个口语化问答数据集

    HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])

    [http://arxiv.org/abs/2304.13689](http://arxiv.org/abs/2304.13689)

    HeySQuAD 是一个大规模口语化问答数据集，旨在衡量机器理解并回答嘈杂的口语提问的能力，同时使用转录的人类口语提问进行训练能显著提高模型表现。

    

    人类口语提问对于评估口语问答系统的性能至关重要，尤其是数字助手等多个实际应用场景。本文提出了一个新的大规模社区共享的口语问答数据集 HeySQuAD，它由76k个人类口语提问、97k个机器生成的问题以及相应的文本答案组成，这些答案源自 SQuAD QA 数据集。HeySQuAD 的目标是衡量机器理解嘈杂的口语提问并准确回答问题的能力。为此，我们在人类口语和机器生成的问题上进行了广泛的基准测试，以量化来自两方面噪声的差异及对模型和回答准确度的影响。重要的是，在口语问答任务中，我们希望回答的是人类口语提问，我们观察到使用转录的人类口语提问和原始 SQuAD 问题进行训练，能够显著提高（12.51%）模型的表现，而不是仅使用原始 SQuAD 数据集。

    Human-spoken questions are critical to evaluating the performance of spoken question answering (SQA) systems that serve several real-world use cases including digital assistants. We present a new large-scale community-shared SQA dataset, HeySQuAD that consists of 76k human-spoken questions and 97k machine-generated questions and corresponding textual answers derived from the SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to understand noisy spoken questions and answer the questions accurately. To this end, we run extensive benchmarks on the human-spoken and machine-generated questions to quantify the differences in noise from both sources and its subsequent impact on the model and answering accuracy. Importantly, for the task of SQA, where we want to answer human-spoken questions, we observe that training using the transcribed human-spoken and original SQuAD questions leads to significant improvements (12.51%) over training using only the original SQuAD te
    
[^5]: 合作人工智能的潜力释放：关于联邦机器学习的社会技术挑战

    Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning. (arXiv:2304.13688v1 [cs.AI])

    [http://arxiv.org/abs/2304.13688](http://arxiv.org/abs/2304.13688)

    联邦机器学习是一种新的人工智能范式，可以从数据孤岛中创建AI模型，挑战在于建立多方合作业务模式。本研究系统化了联邦机器学习项目的社会技术挑战和业务模式。

    

    AI系统的颠覆性潜力源于大数据的出现，但是很大一部分数据分散在数据孤岛中，其潜力未能得到释放。联邦机器学习是一种新的人工智能范式，可以从分散的、潜在的数据孤岛中创建AI模型。因此，联邦机器学习在技术上可以打开数据孤岛，从而释放经济潜力。然而，这需要多个拥有数据孤岛的方之间的合作。建立合作业务模式是复杂的，通常是失败的原因。当前的文献缺乏成功实现合作AI项目所必须考虑的指南。本研究通过系统文献回顾、焦点小组和专家访谈，探讨了当前合作业务模式的挑战和联邦机器学习的不同方面。我们提供了一个系统化的社会技术挑战和扩展的业务模式，以实现联邦机器学习项目。

    The disruptive potential of AI systems roots in the emergence of big data. Yet, a significant portion is scattered and locked in data silos, leaving its potential untapped. Federated Machine Learning is a novel AI paradigm enabling the creation of AI models from decentralized, potentially siloed data. Hence, Federated Machine Learning could technically open data silos and therefore unlock economic potential. However, this requires collaboration between multiple parties owning data silos. Setting up collaborative business models is complex and often a reason for failure. Current literature lacks guidelines on which aspects must be considered to successfully realize collaborative AI projects. This research investigates the challenges of prevailing collaborative business models and distinct aspects of Federated Machine Learning. Through a systematic literature review, focus group, and expert interviews, we provide a systemized collection of socio-technical challenges and an extended Busin
    
[^6]: 自动化ATM现金补充流程的多目标物流优化

    Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process. (arXiv:2304.13671v1 [math.OC])

    [http://arxiv.org/abs/2304.13671](http://arxiv.org/abs/2304.13671)

    本文研究了自动化ATM现金补充流程，提出了一个数学模型并给出了一个工具来评估各种不同的情况。在模拟数据集上，该模型与方法可以削减ATM现金运营成本。

    

    在数字化转型的时代，将数字技术整合到银行运营的各个方面可以改善流程自动化、成本效益和服务水平提升。虽然ATM现金物流是影响运营成本和消费者满意度的重要任务，但却很少有努力来加以改进。特别是在越南，拥有超过2万台ATM的市场上，解决这个问题的研究和技术解决方案仍然较少。在本文中，我们将ATM现金补充的车辆路径问题进行了概括，提出了一个数学模型，然后提供了一个工具来评估各种不同的情况。在模拟数据集上进行评估时，我们提出的模型和方法产生了令人鼓舞的结果，可以削减ATM现金运营成本。

    In the digital transformation era, integrating digital technology into every aspect of banking operations improves process automation, cost efficiency, and service level improvement. Although logistics for ATM cash is a crucial task that impacts operating costs and consumer satisfaction, there has been little effort to enhance it. Specifically, in Vietnam, with a market of more than 20,000 ATMs nationally, research and technological solutions that can resolve this issue remain scarce. In this paper, we generalized the vehicle routing problem for ATM cash replenishment, suggested a mathematical model and then offered a tool to evaluate various situations. When being evaluated on the simulated dataset, our proposed model and method produced encouraging results with the benefits of cutting ATM cash operating costs.
    
[^7]: 使用隐式反馈提高问答生成质量

    Using Implicit Feedback to Improve Question Generation. (arXiv:2304.13664v1 [cs.CL])

    [http://arxiv.org/abs/2304.13664](http://arxiv.org/abs/2304.13664)

    本研究提出了一个名为GEN的系统，利用一小组句子/问题对作为输入，通过基于模式的方法生成问题。该系统可以从用户的反馈中学习并提高生成问题的质量。

    

    问答生成(QG)是自然语言处理(NLP)的一个任务，旨在从文本中自动生成问题。许多应用程序可以从自动生成的问题中受益，但通常需要通过选择或编辑这些问题来使其更准确。本研究提出了一个名为GEN的系统，该系统从这些（隐式）反馈中学习。该系统采用基于模式的方法，将一小组句子/问题对作为输入，并创建模式，然后将其应用于新的未见过的句子。每个由用户纠正后生成的问题都作为下一次迭代的新种子使用，因此每次都会创建更多的模式。我们还利用用户所做的更正来评分模式，从而对生成的问题进行排序。实验表明，该系统优于现有的QG模型，可以生成高质量的问题。

    Question Generation (QG) is a task of Natural Language Processing (NLP) that aims at automatically generating questions from text. Many applications can benefit from automatically generated questions, but often it is necessary to curate those questions, either by selecting or editing them. This task is informative on its own, but it is typically done post-generation, and, thus, the effort is wasted. In addition, most existing systems cannot incorporate this feedback back into them easily. In this work, we present a system, GEN, that learns from such (implicit) feedback. Following a pattern-based approach, it takes as input a small set of sentence/question pairs and creates patterns which are then applied to new unseen sentences. Each generated question, after being corrected by the user, is used as a new seed in the next iteration, so more patterns are created each time. We also take advantage of the corrections made by the user to score the patterns and therefore rank the generated qu
    
[^8]: PVP: 预训练的视觉参数高效微调

    PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])

    [http://arxiv.org/abs/2304.13639](http://arxiv.org/abs/2304.13639)

    本文提出了一种名为PVP的新方法，通过利用预训练模型的统计信息和无监督聚类方法初始化提示模块，可以在极少的标记数据（每类一到两个示例）的情况下获得具有竞争力的性能。

    

    大规模预训练变换器在各种计算机视觉任务中取得了显著的成功。然而，由于其高计算和存储成本，在下游任务中完全微调这些模型仍然面临着极大的挑战。我们首次通过经验探究发现，大多数PETuning方法仍需要大量的下游任务训练数据才能取得良好的结果。为了克服这个问题，我们提出了PVP，利用预训练模型的统计信息，利用无监督聚类方法初始化提示模块。PVP可在各种图像分类和少样本学习基准上仅使用少量标记数据（例如每类一到两个示例）实现具有竞争力的性能。

    Large-scale pre-trained transformers have demonstrated remarkable success in various computer vision tasks. However, it is still highly challenging to fully fine-tune these models for downstream tasks due to their high computational and storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques, e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have significantly reduced the computation and storage cost by inserting lightweight prompt modules into the pre-trained models and tuning these prompt modules with a small number of trainable parameters, while keeping the transformer backbone frozen. Although only a few parameters need to be adjusted, most PETuning methods still require a significant amount of downstream task training data to achieve good results. The performance is inadequate on low-data regimes, especially when there are only one or two examples per class. To this end, we first empirically identify the poor performance is mainly due to the inappr
    
[^9]: AutoCure：面向机器学习管线的自动表格数据整理技术

    AutoCure: Automated Tabular Data Curation Technique for ML Pipelines. (arXiv:2304.13636v1 [cs.DB])

    [http://arxiv.org/abs/2304.13636](http://arxiv.org/abs/2304.13636)

    AutoCure是一种无需配置的数据整理管道，可提高表格数据的质量，通过自适应集成的误差检测方法和数据增强模块，从合乎规范的数据部分中增强了清晰数据分数的密度。

    

    机器学习算法在自动驾驶、医疗保健和金融等多个领域变得越来越普遍。在这些领域中，数据准备仍然是发展准确模型的一个重大挑战，需要大量的专业知识和时间投资来搜索适合的数据整理和转换工具。为了解决这个挑战，我们提出了AutoCure，一种新颖且无需配置的数据整理管道，可提高表格数据的质量。与传统的数据整理方法不同，AutoCure通过自适应集成的误差检测方法和数据增强模块，从合乎规范的数据部分中增强了清晰数据分数的密度。在实践中，AutoCure可以与开源工具（例如Auto-sklearn、H2O和TPOT）集成，以推动机器学习的民主化。作为一个概念验证，我们提供了AutoCure与28种传统数据整理工具组合的比较评估。

    Machine learning algorithms have become increasingly prevalent in multiple domains, such as autonomous driving, healthcare, and finance. In such domains, data preparation remains a significant challenge in developing accurate models, requiring significant expertise and time investment to search the huge search space of well-suited data curation and transformation tools. To address this challenge, we present AutoCure, a novel and configuration-free data curation pipeline that improves the quality of tabular data. Unlike traditional data curation methods, AutoCure synthetically enhances the density of the clean data fraction through an adaptive ensemble-based error detection method and a data augmentation module. In practice, AutoCure can be integrated with open source tools, e.g., Auto-sklearn, H2O, and TPOT, to promote the democratization of machine learning. As a proof of concept, we provide a comparative evaluation of AutoCure against 28 combinations of traditional data curation tool
    
[^10]: 基于神经网络的人工智能中符号的作用：它们并不是你认为的那样！

    The Roles of Symbols in Neural-based AI: They are Not What You Think!. (arXiv:2304.13626v1 [cs.AI])

    [http://arxiv.org/abs/2304.13626](http://arxiv.org/abs/2304.13626)

    符号在基于神经网络的人工智能中并不只用于代表某个概念，还可作为外部和内部的沟通工具，提高知识传递效率，并对学习世界提供有益约束。

    

    我们提出符号首先是智能体之间进行知识传递的外部沟通工具，比起直接经验世界，使用符号可以更加高效和有效。但符号也可以在智能体内部通过自我沟通的形式使用，来帮助描述和证明真正实施思考的次符号模式的神经活动。符号和使用符号的语言不仅允许我们向自己和他人阐述我们的思维，还为学习世界提供有益的约束（引导偏差）。在本文中，我们从神经科学和认知科学中提出了相关的见解，介绍了人类大脑如何表示符号及其所指的概念，以及现代人工神经网络如何实现相同的功能。接着提出一个新的神经符号假设和可能的智能体架构，结合了次符号表示和符号表示。

    We propose that symbols are first and foremost external communication tools used between intelligent agents that allow knowledge to be transferred in a more efficient and effective manner than having to experience the world directly. But, they are also used internally within an agent through a form of self-communication to help formulate, describe and justify subsymbolic patterns of neural activity that truly implement thinking. Symbols, and our languages that make use of them, not only allow us to explain our thinking to others and ourselves, but also provide beneficial constraints (inductive bias) on learning about the world. In this paper we present relevant insights from neuroscience and cognitive science, about how the human brain represents symbols and the concepts they refer to, and how today's artificial neural networks can do the same. We then present a novel neuro-symbolic hypothesis and a plausible architecture for intelligent agents that combines subsymbolic representations
    
[^11]: 位置偏差对token分类中的语言模型的影响

    Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])

    [http://arxiv.org/abs/2304.13567](http://arxiv.org/abs/2304.13567)

    研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。

    

    语言模型在自然语言处理任务中表现出了最先进的性能。命名实体识别(NER)或词性标注等下游任务已知存在数据不平衡问题，特别是在正负示例的比例和类不平衡方面。本文研究了语言模型的另一个特定问题，即token分类任务中正示例的位置偏差。因此，我们对基于Token分类基准测试的语言模型的性能进行了深入的位置偏差评估。我们的研究包括CoNLL03和OntoNote5.0用于NER，English Tree Bank UD_en和TweeBank用于POS标记。我们提出了一种评估方法，以研究Transformer模型中的位置偏差。我们发现像BERT、ERNIE、ELECTRA这样的编码器和像GPT2 和BLOOM这样的解码器平均性能下降了3%和9%。

    Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
    
[^12]: 一箭双雕：量化在分布式学习中实现隐私保护与通讯效率

    Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning. (arXiv:2304.13545v1 [cs.LG])

    [http://arxiv.org/abs/2304.13545](http://arxiv.org/abs/2304.13545)

    本文提出了一种分布式学习中的综合解决方案，基于量化同时实现通讯效率和隐私保护，并且向均匀量化的梯度添加二项噪声以达到所需的差分隐私级别。

    

    分布式机器学习中的通讯效率和隐私保护是两个关键问题。现有的方法分开解决这两个问题，可能在资源有限的环境下应用上受到限制。本文提出了一种基于量化的综合解决方案，能够同时实现通讯效率和隐私保护，并提供了与通讯和隐私相关性质的新见解。具体而言，我们向均匀量化的梯度添加二项噪声以达到所需的差分隐私级别，从而在稍微牺牲通讯效率的情况下证明了所提出的解决方案在分布式随机梯度下降 (SGD) 框架中的有效性。我们理论上捕捉了通讯、隐私和学习性能之间的新权衡。

    Communication efficiency and privacy protection are two critical issues in distributed machine learning. Existing methods tackle these two issues separately and may have a high implementation complexity that constrains their application in a resource-limited environment. We propose a comprehensive quantization-based solution that could simultaneously achieve communication efficiency and privacy protection, providing new insights into the correlated nature of communication and privacy. Specifically, we demonstrate the effectiveness of our proposed solutions in the distributed stochastic gradient descent (SGD) framework by adding binomial noise to the uniformly quantized gradients to reach the desired differential privacy level but with a minor sacrifice in communication efficiency. We theoretically capture the new trade-offs between communication, privacy, and learning performance.
    
[^13]: 张量分解用于神经网络模型简化：一项综述

    Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])

    [http://arxiv.org/abs/2304.13539](http://arxiv.org/abs/2304.13539)

    本文综述了六种张量分解方法，并讨论了将神经网络层替换为低秩张量逼近的方案。实验结果表明这种方法能够显著降低模型大小、运行时间和能量消耗，并且适合于在边缘设备上实现神经网络。

    

    现代神经网络在计算机视觉和自然语言处理等领域取得了革命性的进展。它们被广泛用于解决复杂的计算机视觉任务和自然语言处理任务，如图像分类、图像生成和机器翻译。大多数最先进的神经网络都是过度参数化的，需要高昂的计算成本。一种直接的解决方案是使用不同的张量分解方法将网络的层替换为其低秩张量近似。本文综述了六种张量分解方法并阐述了它们在卷积神经网络、循环神经网络和Transformer的模型参数压缩中的能力。一些压缩模型的准确性甚至可以高于原始版本。评估表明，张量分解可以在模型大小、运行时间和能量消耗方面实现显著降低，并且非常适合在边缘设备上实现神经网络。

    Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices.
    
[^14]: ElegansNet：一篇简要的科学报告和初步实验

    ElegansNet: a brief scientific report and initial experiments. (arXiv:2304.13538v1 [cs.NE])

    [http://arxiv.org/abs/2304.13538](http://arxiv.org/abs/2304.13538)

    ElegansNet是一种神经网络，利用Caenorhabditis elegans的连接组的结构作为参考，设计生成了具有类似于自然网络拓扑的深度学习系统，在效率、性能上均胜过随机连线网络和人工网络。

    

    本研究报告介绍了ElegansNet，一种神经网络，它模仿现实世界的神经网络电路，旨在更好地理解连通性拓扑和深度学习系统之间的相互作用。该方法利用生物神经电路的强大表达能力来设计和生成具有类似于自然网络拓扑的改进型深度学习系统。由于其完整性、合理的大小和功能神经元类别注释，Caenorhabditis elegans的连接组被用作参考。证明了简单生物的连通图在神经元之间具有特定的功能关系，一旦转化为可学习的张量网络并集成到现代架构中，它们提供了能够高效解决复杂任务的生物可行结构。该模型的性能与随机连线网络进行了比较，并与按全球基准排名的人工网络进行了比较。

    This research report introduces ElegansNet, a neural network that mimics real-world neuronal network circuitry, with the goal of better understanding the interplay between connectome topology and deep learning systems. The proposed approach utilizes the powerful representational capabilities of living beings' neuronal circuitry to design and generate improved deep learning systems with a topology similar to natural networks. The Caenorhabditis elegans connectome is used as a reference due to its completeness, reasonable size, and functional neuron classes annotations. It is demonstrated that the connectome of simple organisms exhibits specific functional relationships between neurons, and once transformed into learnable tensor networks and integrated into modern architectures, it offers bio-plausible structures that efficiently solve complex tasks. The performance of the models is demonstrated against randomly wired networks and compared to artificial networks ranked on global benchmar
    
[^15]: 从全手写页面中提取键值信息

    Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])

    [http://arxiv.org/abs/2304.13530](http://arxiv.org/abs/2304.13530)

    这篇论文提出了一种从手写文档中提取信息的方法，该方法基于Transformer，在同一模型中结合了特征提取、手写识别和命名实体识别步骤。该方法不需要预先分割并在三个公共数据库上表现出色。

    

    我们提出了一种基于Transformer的方法，用于从数字化的手写文档中提取信息。我们的方法结合了目前由独立模型执行的不同步骤：特征提取、手写识别和命名实体识别。我们将这种综合方法与传统的两阶段方法进行比较，传统方法在命名实体识别之前进行手写识别，并在不同层次上呈现结果：行、段落和页面。我们的实验表明，在应用于整个页面时，基于注意力的模型特别有趣，因为它们不需要任何预先分割步骤。最后，我们展示了它们能够从键值注释中进行学习：即重要单词和相应命名实体的列表。我们将我们的模型与三个公共数据库（IAM、ESPOSALLES和POPP）的最新方法进行比较，并在所有三个数据集上优于以前的表现。

    We propose a Transformer-based approach for information extraction from digitized handwritten documents. Our approach combines, in a single model, the different steps that were so far performed by separate models: feature extraction, handwriting recognition and named entity recognition. We compare this integrated approach with traditional two-stage methods that perform handwriting recognition before named entity recognition, and present results at different levels: line, paragraph, and page. Our experiments show that attention-based models are especially interesting when applied on full pages, as they do not require any prior segmentation step. Finally, we show that they are able to learn from key-value annotations: a list of important words with their corresponding named entities. We compare our models to state-of-the-art methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform previous performances on all three datasets.
    
[^16]: 命题逻辑程序的序列分解研究

    Sequential decomposition of propositional logic programs. (arXiv:2304.13522v1 [cs.LO])

    [http://arxiv.org/abs/2304.13522](http://arxiv.org/abs/2304.13522)

    研究命题逻辑程序的序列分解，通过分析程序之间的Green关系，为逻辑编程代数理论进一步发展作出了贡献。

    

    最近引入了命题逻辑程序的序列组合。本文通过研究程序之间的 Green 关系 $\mathcal{L,R,J}$，从而研究程序的序列分解。在更广泛的意义上，本文是逻辑编程代数理论的进一步研究。

    The sequential composition of propositional logic programs has been recently introduced. This paper studies the sequential {\em decomposition} of programs by studying Green's relations $\mathcal{L,R,J}$ -- well-known in semigroup theory -- between programs. In a broader sense, this paper is a further step towards an algebraic theory of logic programming.
    
[^17]: 簇熵：病理图像分割中的主动域自适应

    Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation. (arXiv:2304.13513v1 [cs.CV])

    [http://arxiv.org/abs/2304.13513](http://arxiv.org/abs/2304.13513)

    本文提出了一种对于病理图像分割领域偏移问题的解决方法，通过使用簇熵来选择有效的WSI并可以显著提高半监督域自适应的性能。

    

    病理图像分割中的域偏移是一个重要问题，源域（在特定医院收集的图像）训练的网络在目标域（来自不同医院）中由于不同的图像特征表现不佳。由于病理学中的类别不平衡和不同类别先验分布的问题，典型的无监督域自适应方法不能通过对齐源域和目标域的分布来很好地处理该问题。本文提出了一种簇熵方法，用于选择用于半监督域自适应的有效WSI，该方法可以通过计算每个簇的熵来度量WSI的图像特征如何覆盖目标域的整个分布，并可以显著提高域自适应的性能。我们的方法在来自两家医院的数据集上取得了竞争性的结果。

    The domain shift in pathological segmentation is an important problem, where a network trained by a source domain (collected at a specific hospital) does not work well in the target domain (from different hospitals) due to the different image features. Due to the problems of class imbalance and different class prior of pathology, typical unsupervised domain adaptation methods do not work well by aligning the distribution of source domain and target domain. In this paper, we propose a cluster entropy for selecting an effective whole slide image (WSI) that is used for semi-supervised domain adaptation. This approach can measure how the image features of the WSI cover the entire distribution of the target domain by calculating the entropy of each cluster and can significantly improve the performance of domain adaptation. Our approach achieved competitive results against the prior arts on datasets collected from two hospitals.
    
[^18]: 实现临床AI公平性: 一个翻译的视角

    Towards clinical AI fairness: A translational perspective. (arXiv:2304.13493v1 [cs.CY])

    [http://arxiv.org/abs/2304.13493](http://arxiv.org/abs/2304.13493)

    研究讨论了AI在医疗保健领域中公平性问题，指出了技术和临床视角之间的不匹配，并提出了跨学科合作以解决这一问题的可能性。

    

    人工智能（AI）已经展示了从数据中提取洞见的能力，但是在诸如医疗保健等高利益领域，公平性仍然是一个问题。尽管在算法开发方面进行了广泛的讨论和努力，但AI公平性和临床问题还没有得到充分解决。在本文中，我们讨论了AI公平性的技术和临床视角之间的不匹配，强调了AI公平性在转化为医疗保健方面所面临的障碍，主张跨学科合作以弥补知识差距，并提供了解决关于AI公平性的临床问题的可能解决方案。

    Artificial intelligence (AI) has demonstrated the ability to extract insights from data, but the issue of fairness remains a concern in high-stakes fields such as healthcare. Despite extensive discussion and efforts in algorithm development, AI fairness and clinical concerns have not been adequately addressed. In this paper, we discuss the misalignment between technical and clinical perspectives of AI fairness, highlight the barriers to AI fairness' translation to healthcare, advocate multidisciplinary collaboration to bridge the knowledge gap, and provide possible solutions to address the clinical concerns pertaining to AI fairness.
    
[^19]: 先验信息学习中的基本权衡

    Fundamental Tradeoffs in Learning with Prior Information. (arXiv:2304.13479v1 [cs.LG])

    [http://arxiv.org/abs/2304.13479](http://arxiv.org/abs/2304.13479)

    本文研究了先验信息准确性和学习性能之间的基本权衡，引入了优先风险概念，并为统计估计问题提供了下界，展现了框架在不同问题中的应用。

    

    本文旨在探讨学习者在所学问题上的先验信息的准确性和其学习性能之间的基本权衡。我们引入了优先风险的概念，它不同于传统的极小极大和贝叶斯风险，可以让我们研究现实不一定符合学习者先验的情况下这些基本权衡。我们提出了一种基于缩减的方法来扩展经典的极小极大下界技术，以便为统计估计问题的优先风险提供下界。我们还介绍了一种新颖的法诺不等式的推广（可能具有独立的兴趣），用于在涉及无限损失的更一般的设置下，下界优先风险。我们展示了我们的框架揭示了在估计、回归和强化学习问题中，先验信息与学习性能之间权衡的能力。

    We seek to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. We present a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems. We also introduce a novel generalization of Fano's inequality (which may be of independent interest) for lower bounding the prioritized risk in more general settings involving unbounded losses. We illustrate the ability of our framework to provide insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.
    
[^20]: 利用强化学习优化地铁系统能源效率在不确定扰动下的表现

    Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning. (arXiv:2304.13443v1 [cs.AI])

    [http://arxiv.org/abs/2304.13443](http://arxiv.org/abs/2304.13443)

    本文提出了一种基于策略的强化学习方法，通过重新安排地铁时刻表和调整列车的停靠时间和巡航速度，优化扰动下的地铁系统能源效率，该方法在模拟环境下实验证明其优于基线方法，最高可达降低10.9%的牵引能量消耗和最高达提高47.9%的再生制动能量利用率，为城市轨道交通的节能问题提供了有效的解决方案。

    

    在城市交通领域，地铁系统是关键的可持续公共交通工具。然而，它们的巨大能源消耗对可持续性目标构成了挑战。延误和乘客流变化等扰动进一步加剧了这个问题，因为它们会对地铁系统的能源效率产生负面影响。为了解决这个问题，我们提出了一种基于策略的强化学习方法，通过调整列车的停靠时间和巡航速度，重新安排地铁时刻表，并优化受扰动影响的地铁系统的能源效率。在模拟环境中进行的实验表明，我们的方法优于基线方法，实现了高达10.9％的牵引能量消耗降低和最高达47.9％的再生制动能量利用率提高。本研究为城市轨道交通节能问题提供了有效的解决方案。

    In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.
    
[^21]: 与陌生人一起跑接力赛？强化学习在超出分布轨迹上的泛化能力

    Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories. (arXiv:2304.13424v1 [cs.LG])

    [http://arxiv.org/abs/2304.13424](http://arxiv.org/abs/2304.13424)

    本文研究了强化学习代理对超出分布的“可控”状态的“接力泛化”性能。通过让测试代理从其他陌生代理的轨迹中间开始，发现这种泛化普遍存在泛化失效问题。

    

    本文定义、评估和改进各种状态下的强化学习（RL）代理对超出分布的“可控”状态的“接力泛化”性能。通过将测试代理从其他独立训练良好的“陌生”代理的轨迹的中间开始，我们实际评估了这种泛化类型。通过大量实验评估，我们展示了来自陌生代理的可控状态几乎普遍存在泛化失效。例如，在人形环境中，我们观察到一个经过良好训练的PPO代理，在正常测试期间只有3.9％的失败率，但在10个陌生代理的轨迹中，失败率升高到31.4％。

    In this paper, we define, evaluate, and improve the ``relay-generalization'' performance of reinforcement learning (RL) agents on the out-of-distribution ``controllable'' states. Ideally, an RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. For example, a self-driving system should be able to take over the control from humans in the middle of driving and must continue to drive the car safely. To practically evaluate this type of generalization, we start the test agent from the middle of other independently well-trained \emph{stranger} agents' trajectories. With extensive experimental evaluation, we show the prevalence of \emph{generalization failure} on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\% failure rate during regular testing, failed on
    
[^22]: 基于联结查询的特征模型配置约束求解

    Conjunctive Query Based Constraint Solving For Feature Model Configuration. (arXiv:2304.13422v1 [cs.AI])

    [http://arxiv.org/abs/2304.13422](http://arxiv.org/abs/2304.13422)

    本文介绍了一种使用联结查询解决约束满足问题的方法，特别是用于特征模型配置任务，使得能够利用数据库技术来解决配置问题并提供新的算法方法。

    

    特征模型配置可以基于各种推理方法进行支持。例如，SAT求解、约束求解和答案集编程(ASP)。使用这些方法需要技术专业知识来定义和解决底层配置问题。本文展示了如何应用通常由当今的关系数据库系统支持的联结查询来解决约束满足问题(CSP)，更具体地说，是解决特征模型配置任务。这种方法允许应用广泛的数据库技术来解决配置任务，并在识别和解决不一致性时提供了新的算法方法。

    Feature model configuration can be supported on the basis of various types of reasoning approaches. Examples thereof are SAT solving, constraint solving, and answer set programming (ASP). Using these approaches requires technical expertise of how to define and solve the underlying configuration problem. In this paper, we show how to apply conjunctive queries typically supported by today's relational database systems to solve constraint satisfaction problems (CSP) and -- more specifically -- feature model configuration tasks. This approach allows the application of a wide-spread database technology to solve configuration tasks and also allows for new algorithmic approaches when it comes to the identification and resolution of inconsistencies.
    
[^23]: 通过连续层中的滤波器相似度进行滤波器剪枝

    Filter Pruning via Filters Similarity in Consecutive Layers. (arXiv:2304.13397v1 [cs.CV])

    [http://arxiv.org/abs/2304.13397](http://arxiv.org/abs/2304.13397)

    本论文提出通过连续层中的滤波器相似度进行滤波器剪枝的方法（FSCL），并取得了在准确性、模型大小和运算量等方面显着的改进。

    

    滤波器剪枝是压缩和加速卷积神经网络（CNN）的广泛采用方法，但大多数先前的工作忽略了不同层中滤波器和通道之间的关系。独立处理每层未能利用跨层之间的协作关系。在本文中，我们直观地提出了一种新颖的剪枝方法，通过明确利用连续层中的滤波器相似性（FSCL）来压缩模型。FSCL通过剪枝那些在模型中对应特征更加不重要的滤波器。广泛的实验表明了FSCL的有效性，它在几个基准模型和数据集上在准确性，FLOPs和参数减少方面产生了显着的改进，优于现有技术。

    Filter pruning is widely adopted to compress and accelerate the Convolutional Neural Networks (CNNs), but most previous works ignore the relationship between filters and channels in different layers. Processing each layer independently fails to utilize the collaborative relationship across layers. In this paper, we intuitively propose a novel pruning method by explicitly leveraging the Filters Similarity in Consecutive Layers (FSCL). FSCL compresses models by pruning filters whose corresponding features are more worthless in the model. The extensive experiments demonstrate the effectiveness of FSCL, and it yields remarkable improvement over state-of-the-art on accuracy, FLOPs and parameter reduction on several benchmark models and datasets.
    
[^24]: VGOS：来自稀疏输入的体素网格优化视角合成

    VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs. (arXiv:2304.13386v1 [cs.CV])

    [http://arxiv.org/abs/2304.13386](http://arxiv.org/abs/2304.13386)

    本论文介绍了一种名为VGOS的方法，可以快速从稀疏输入（3-10个视图）中重建放射场，以解决体素网格更容易过度拟合训练视图的问题，并提出了增量体素训练策略来避免过度拟合。

    

    神经放射场（NeRF）由于其优异的质量和灵活性在新颖视角合成中取得了很大的成功。但是，NeRF需要密集的输入视图（几十到几百个）和长时间的训练时间（几小时到几天）才能为单个场景生成高保真度的图像。虽然使用体素网格表示放射场可以显著加速优化过程，但我们注意到，在稀疏输入的情况下，体素网格更容易过度拟合训练视图，会产生空洞和漂浮物，导致伪影。在本文中，我们提出了VGOS，一种从稀疏输入（3-10个视图）快速（3-5分钟）重建放射场的方法来解决这些问题。为了改善基于体素的放射场在稀疏输入场景中的性能，我们提出了两种方法：（a）我们引入了增量体素训练策略，在重建早期抑制周边体素的优化，以防止过度拟合。

    Neural Radiance Fields (NeRF) has shown great success in novel view synthesis due to its state-of-the-art quality and flexibility. However, NeRF requires dense input views (tens to hundreds) and a long training time (hours to days) for a single scene to generate high-fidelity images. Although using the voxel grids to represent the radiance field can significantly accelerate the optimization process, we observe that for sparse inputs, the voxel grids are more prone to overfitting to the training views and will have holes and floaters, which leads to artifacts. In this paper, we propose VGOS, an approach for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10 views) to address these issues. To improve the performance of voxel-based radiance field in sparse input scenarios, we propose two methods: (a) We introduce an incremental voxel training strategy, which prevents overfitting by suppressing the optimization of peripheral voxels in the early stage of reconstructio
    
[^25]: 基于区块链的智能工业管理系统访问控制

    Blockchain-based Access Control for Secure Smart Industry Management Systems. (arXiv:2304.13379v1 [cs.CR])

    [http://arxiv.org/abs/2304.13379](http://arxiv.org/abs/2304.13379)

    本文提出了一种基于角色的访问控制方法，通过在区块链和智能合约中实现来保障基于云的智能制造系统数据的安全性。

    

    智能制造系统涉及大量互联设备，导致大量数据生成。云计算技术近年来在智能制造系统中得到越来越多的关注，以促进成本效益服务的提供和大量数据的管理。在基于云的制造系统中，确保对数据的授权访问至关重要。云平台由单一权威运营。因此，云平台容易成为单点故障，并容易受到敌对势力的攻击。内部或外部敌对势力可以轻易修改用户的访问权限，以允许未经授权的用户访问数据。本文提出了一种基于角色的访问控制方法，借助区块链和智能合约在基于云的智能制造系统中防止修改攻击。该基于角色的访问控制被开发出来来确定智能合约中用户的角色和权限。然后将智能合约部署到私有区块链网络中。我们对所提出的方法进行了评估。

    Smart manufacturing systems involve a large number of interconnected devices resulting in massive data generation. Cloud computing technology has recently gained increasing attention in smart manufacturing systems for facilitating cost-effective service provisioning and massive data management. In a cloud-based manufacturing system, ensuring authorized access to the data is crucial. A cloud platform is operated under a single authority. Hence, a cloud platform is prone to a single point of failure and vulnerable to adversaries. An internal or external adversary can easily modify users' access to allow unauthorized users to access the data. This paper proposes a role-based access control to prevent modification attacks by leveraging blockchain and smart contracts in a cloud-based smart manufacturing system. The role-based access control is developed to determine users' roles and rights in smart contracts. The smart contracts are then deployed to the private blockchain network. We evalua
    
[^26]: 延迟反馈的前馈优化神经网络

    Feed-Forward Optimization With Delayed Feedback for Neural Networks. (arXiv:2304.13372v1 [cs.LG])

    [http://arxiv.org/abs/2304.13372](http://arxiv.org/abs/2304.13372)

    本文提出了一种延迟反馈的前馈神经网络优化方法F^3，使用延迟的误差信息来缩放梯度从而提高生物可行性和计算效率，具有较高的预测性能，为低能量训练和并行化提供了新思路。

    

    反向传播长期以来一直受到生物学上的批评，因为它依赖于自然学习过程中不可行的概念。本文提出了一种替代方法来解决两个核心问题，即权重传输和更新锁定，以实现生物可行性和计算效率。我们引入了延迟反馈的前馈（F^3），通过利用延迟的误差信息作为样本级缩放因子来更准确地近似梯度，改进了先前的工作。我们发现，F^3将生物可行性训练算法和反向传播之间的预测性能差距缩小了高达96％。这证明了生物可行性训练的适用性，并为低能量训练和并行化开辟了有 promising 的新方向。

    Backpropagation has long been criticized for being biologically implausible, relying on concepts that are not viable in natural learning processes. This paper proposes an alternative approach to solve two core issues, i.e., weight transport and update locking, for biological plausibility and computational efficiency. We introduce Feed-Forward with delayed Feedback (F$^3$), which improves upon prior work by utilizing delayed error information as a sample-wise scaling factor to approximate gradients more accurately. We find that F$^3$ reduces the gap in predictive performance between biologically plausible training algorithms and backpropagation by up to 96%. This demonstrates the applicability of biologically plausible training and opens up promising new avenues for low-energy training and parallelization.
    
[^27]: 基于LoRaWAN的智能校园：数据集和人流计数器案例

    LoRaWAN-enabled Smart Campus: The Dataset and a People Counter Use Case. (arXiv:2304.13366v1 [cs.LG])

    [http://arxiv.org/abs/2304.13366](http://arxiv.org/abs/2304.13366)

    本文介绍一个基于LoRaWAN的智能校园数据集，使用k近邻和LSTM方法处理丢失值和预测未来读数，并构建一个深度神经网络来预测房间内人数，准确率达到95％。

    

    物联网在智能校园中具有重要作用。本文提供了一个基于LoRaWAN的智能校园数据集的详细描述。LoRaWAN是一种新兴技术，可以为数百个物联网设备提供服务。首先，我们描述了将设备连接到服务器的LoRa网络。然后，我们分析了丢失的传输并提出了k近邻解决方案来处理缺失值。然后，我们使用长短期记忆（LSTM）来预测未来的读数。最后，作为一个示例应用程序，我们构建了一个深度神经网络来基于所选传感器的读数预测房间内人数。我们的结果显示，在预测人数方面，我们的模型达到了95％的准确率。此外，数据集是公开可用的，并且有详细说明，这是探索其他功能和应用的机会。

    IoT has a significant role in the smart campus. This paper presents a detailed description of the Smart Campus dataset based on LoRaWAN. LoRaWAN is an emerging technology that enables serving hundreds of IoT devices. First, we describe the LoRa network that connects the devices to the server. Afterward, we analyze the missing transmissions and propose a k-nearest neighbor solution to handle the missing values. Then, we predict future readings using a long short-term memory (LSTM). Finally, as one example application, we build a deep neural network to predict the number of people inside a room based on the selected sensor's readings. Our results show that our model achieves an accuracy of $95 \: \%$ in predicting the number of people. Moreover, the dataset is openly available and described in detail, which is opportunity for exploration of other features and applications.
    
[^28]: 跨语言中间表示的神经符号式零样本代码克隆

    Neuro-symbolic Zero-Shot Code Cloning with Cross-Language Intermediate Representation. (arXiv:2304.13350v1 [cs.AI])

    [http://arxiv.org/abs/2304.13350](http://arxiv.org/abs/2304.13350)

    本研究介绍了一种可以跨越多种编程语言，通过神经符号学方法进行零样本跨编程语言代码搜索的中间表示方式，并在COBOL编程语言上进行了验证，使得代码克隆任务的性能获得了12.85 MAP @ 2的提升。

    

    本文提出了一种神经符号式的方法来解决COBOL编程语言中的代码语义相似的克隆任务，无需训练数据。我们定义了一个元模型，采用在C和COBOL中的代码都可用的ASTs（抽象语法树）作为中间表示。使用基于结构遍历（SBT）的方法将IRs线性化，以创建顺序输入。进一步对UnixCoder进行微调，在CodeNet数据集上使用SBT IR的C代码对进行零样本跨编程语言代码搜索的任务，以进行代码克隆。使用这个精调的UnixCoder，相对于预先训练的UniXCoder模型，在COBOL测试数据中获得了12.85 MAP @ 2的性能提升。

    In this paper, we define a neuro-symbolic approach to address the task of finding semantically similar clones for the codes of the legacy programming language COBOL, without training data. We define a meta-model that is instantiated to have an Intermediate Representation (IR) in the form of Abstract Syntax Trees (ASTs) common across codes in C and COBOL. We linearize the IRs using Structure Based Traversal (SBT) to create sequential inputs. We further fine-tune UnixCoder, the best-performing model for zero-shot cross-programming language code search, for the Code Cloning task with the SBT IRs of C code-pairs, available in the CodeNet dataset. This allows us to learn latent representations for the IRs of the C codes, which are transferable to the IRs of the COBOL codes. With this fine-tuned UnixCoder, we get a performance improvement of 12.85 MAP@2 over the pre-trained UniXCoder model, in a zero-shot setting, on the COBOL test split synthesized from the CodeNet dataset. This demonstrate
    
[^29]: 基于差异引导重建学习的图像篡改检测方法

    Discrepancy-Guided Reconstruction Learning for Image Forgery Detection. (arXiv:2304.13349v1 [cs.CV])

    [http://arxiv.org/abs/2304.13349](http://arxiv.org/abs/2304.13349)

    本论文提出了一种基于差异引导的图像篡改检测方法，能够提升模型对篡改敏感且具有紧凑视觉模式的学习能力，具有较广泛的推广性。

    

    本论文提出了一种新的图像篡改检测范式，旨在提高模型对于既敏感于篡改又具有紧凑视觉模式的学习能力。相对于现有方法仅关注篡改特定模式（例如噪声、纹理和频率）的问题，我们的方法更具有广泛的推广性。具体而言，我们首先提出了一个差异引导编码器（DisGE）来提取敏感于篡改的视觉模式。DisGE由两个分支组成，其中主流的骨干分支用于提取一般语义特征，而辅助的差异性外部注意力分支用于提取明确的篡改提示。此外，还提出了一个双头重建（DouHR）模块，用于增强不同颗粒空间中的真实紧凑视觉模式。在DouHR下，我们进一步引入了一种差异聚合检测器（DisAD）来聚合这些真实紧凑的视觉模式，从而提高对未知模式的篡改检测能力。

    In this paper, we propose a novel image forgery detection paradigm for boosting the model learning capacity on both forgery-sensitive and genuine compact visual patterns. Compared to the existing methods that only focus on the discrepant-specific patterns (\eg, noises, textures, and frequencies), our method has a greater generalization. Specifically, we first propose a Discrepancy-Guided Encoder (DisGE) to extract forgery-sensitive visual patterns. DisGE consists of two branches, where the mainstream backbone branch is used to extract general semantic features, and the accessorial discrepant external attention branch is used to extract explicit forgery cues. Besides, a Double-Head Reconstruction (DouHR) module is proposed to enhance genuine compact visual patterns in different granular spaces. Under DouHR, we further introduce a Discrepancy-Aggregation Detector (DisAD) to aggregate these genuine compact visual patterns, such that the forgery detection capability on unknown patterns can
    
[^30]: 基于正则化的连续学习方法的评估：应用于HAR

    Evaluation of Regularization-based Continual Learning Approaches: Application to HAR. (arXiv:2304.13327v1 [cs.AI])

    [http://arxiv.org/abs/2304.13327](http://arxiv.org/abs/2304.13327)

    本研究评估了基于正则化的连续学习方法在HAR领域的应用，并比较了三种方法的优缺点。实验证明，这些方法提高了模型学习新类别的能力，同时保持了模型在先前学习的类别上的准确性。

    

    普适计算在许多重要的领域中提供服务，包括健康和福利这个相关且动态的领域。在这个领域中，人类活动识别（HAR）在近年来引起了很多关注。目前的解决方案依赖于机器学习（ML）模型并取得了令人印象深刻的结果。然而，这些模型的演进仍然很困难，除非进行完整的重新训练。为了解决这个问题，连续学习的概念在今天非常有前途，尤其是基于正则化的技术。这些技术非常有趣，因为它们很简单并且成本低。已经进行了初步的研究，并展示了有希望的结果。然而，它们仍然非常专业化并且难以比较。在本文中，我们提供了三种基于正则化的方法在HAR领域的全面比较，并突出它们的优点和局限性。我们的实验基于公开可用的数据集进行，结果表明这些方法提高了模型学习新类别的能力，同时保持了模型在先前学习的类别上的准确性。

    Pervasive computing allows the provision of services in many important areas, including the relevant and dynamic field of health and well-being. In this domain, Human Activity Recognition (HAR) has gained a lot of attention in recent years. Current solutions rely on Machine Learning (ML) models and achieve impressive results. However, the evolution of these models remains difficult, as long as a complete retraining is not performed. To overcome this problem, the concept of Continual Learning is very promising today and, more particularly, the techniques based on regularization. These techniques are particularly interesting for their simplicity and their low cost. Initial studies have been conducted and have shown promising outcomes. However, they remain very specific and difficult to compare. In this paper, we provide a comprehensive comparison of three regularization-based methods that we adapted to the HAR domain, highlighting their strengths and limitations. Our experiments were con
    
[^31]: 情感的画像：通过AI生成的艺术赋予自我表达力

    A Portrait of Emotion: Empowering Self-Expression through AI-Generated Art. (arXiv:2304.13324v1 [cs.AI])

    [http://arxiv.org/abs/2304.13324](http://arxiv.org/abs/2304.13324)

    探究了AI在创造性表达中表达情感方面的潜力与局限，AI可以促进创造力和情感的自我表达。

    

    本研究调查了生成型人工智能（AI）在通过创造性表达反映作者的认知过程中的潜力和局限性。重点在于AI生成的艺术作品理解人类意图（对齐）和根据创意、美感、新颖性、娱乐性和深度等标准视觉化表现情感的能力。结果表明，基于作者情感描述的图像在事件中的表现方面更受欢迎。我们还发现，过分强调某些元素或刻板印象的图像会对AI的对齐产生负面影响。我们的研究结果表明，AI可以促进创造力和情感的自我表达。我们的研究框架可用于设计相关领域（例如心理健康教育、治疗和咨询）中基于AI的干预。

    We investigated the potential and limitations of generative artificial intelligence (AI) in reflecting the authors' cognitive processes through creative expression. The focus is on the AI-generated artwork's ability to understand human intent (alignment) and visually represent emotions based on criteria such as creativity, aesthetic, novelty, amusement, and depth. Results show a preference for images based on the descriptions of the authors' emotions over the main events. We also found that images that overrepresent specific elements or stereotypes negatively impact AI alignment. Our findings suggest that AI could facilitate creativity and the self-expression of emotions. Our research framework with generative AIs can help design AI-based interventions in related fields (e.g., mental health education, therapy, and counseling).
    
[^32]: 技术笔记：定义和量化DNN的AND-OR交互以进行准确和简明的解释

    Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])

    [http://arxiv.org/abs/2304.13312](http://arxiv.org/abs/2304.13312)

    本文提出了一种通过量化输入变量之间的编码交互来准确且简明地解释深度神经网络(DNN)的推理逻辑的方法。针对此目的，作者提出了两种交互方式，即AND交互和OR交互，并利用它们设计出一系列技术来提高解释的简洁性，同时不会损害准确性。

    

    本文旨在通过量化输入变量之间的编码交互来解释深度神经网络(DNN)的推理逻辑。具体而言，我们首先重新思考交互的定义，然后正式定义了基于交互的解释的准确性和简洁性。为此，我们提出了两种交互方式，即AND交互和OR交互。针对准确性，我们证明了AND（OR）交互在量化输入变量之间的AND（OR）关系效应方面的唯一性。此外，基于AND-OR交互，我们设计了技术来提高解释的简洁性，同时不会损害准确性。因此，DNN的推理逻辑可以通过一组符号概念准确而简明地解释。

    In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
    
[^33]: HiQ -- 一种声明性、非侵入式、动态和透明的可观察性和优化系统。

    HiQ -- A Declarative, Non-intrusive, Dynamic and Transparent Observability and Optimization System. (arXiv:2304.13302v1 [cs.DC])

    [http://arxiv.org/abs/2304.13302](http://arxiv.org/abs/2304.13302)

    HiQ是一种可透明监控Python程序运行时信息的系统，具有非侵入性和动态性，可应用于离线/在线应用程序和分布式系统，我们可以使用它来优化神经网络模型并捕捉瓶颈，而不影响代码的干净程度和性能表现。

    

    本文提出了一种名为“HiQ”的非侵入式、声明性、动态和透明系统，用于跟踪Python程序的运行时信息，而不会影响运行时系统性能和损失洞察力。 HiQ可以用于单片和分布式系统、离线和在线应用程序，我们已经将其用于优化使用Python编写的大型深度神经网络（DNN）模型，并可推广到任何Python程序或分布式系统，甚至是Java等其他语言。 我们已经在深度学习模型生命周期管理系统中实现了HiQ，并采用它来捕捉瓶颈，同时保持我们的生产代码干净且高性能。 我们的实现已经在 [https://github.com/oracle/hiq](https://github.com/oracle/hiq) 开源。

    This paper proposes a non-intrusive, declarative, dynamic and transparent system called `HiQ` to track Python program runtime information without compromising on the run-time system performance and losing insight. HiQ can be used for monolithic and distributed systems, offline and online applications. HiQ is developed when we optimize our large deep neural network (DNN) models which are written in Python, but it can be generalized to any Python program or distributed system, or even other languages like Java. We have implemented the system and adopted it in our deep learning model life cycle management system to catch the bottleneck while keeping our production code clean and highly performant. The implementation is open-sourced at: [https://github.com/oracle/hiq](https://github.com/oracle/hiq).
    
[^34]: 跨域文本到SQL自适应提示的基于案例推理框架

    A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])

    [http://arxiv.org/abs/2304.13301](http://arxiv.org/abs/2304.13301)

    本文提出了一个基于案例推理框架的跨域文本到SQL自适应提示的解决方案，可以精确控制与案例相关和不相关的知识，解决了大型语言模型提示设计不良限制性能的问题。

    

    最近流行的大型语言模型（例如Codex、ChatGPT和GPT-4）在AI社区方面有了显著的进展，包括文本到SQL的任务。一些关于大型语言模型的评估和分析表明，它们有潜力生成SQL查询，但是它们所使用的提示设计不良（例如简单的结构或随机抽样）限制了大型语言模型的性能，并可能导致不必要或无关的输出。为了解决这些问题，我们提出了CBR-ApSQL，这是一个基于案例推理（CBR）的框架，与GPT-3.5相结合，用于在文本到SQL任务中对与案例相关和不相关的知识进行精确控制。我们设计了自适应提示，以灵活调整GPT-3.5的输入，其中涉及（1）通过去语义化输入问题来自适应检索案例，根据问题意图，以及（2）自适应回退机制，以确保提示的信息量和案例与提示之间的相关性。在去语义化阶段中，我们设计了Semantic D

    Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have significantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs' performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic D
    
[^35]: 基于游戏的人工智能研究平台

    Game-based Platforms for Artificial Intelligence Research. (arXiv:2304.13269v1 [cs.AI])

    [http://arxiv.org/abs/2304.13269](http://arxiv.org/abs/2304.13269)

    本文回顾了基于游戏的人工智能研究平台，讨论了不同研究领域和创意设计在其中的应用和发展，并探讨了其未来趋势。

    

    游戏具有现实世界场景的广泛特征，成为了人工智能研究的理想测试基地，共同的研究领域包括学习和优化、动态和不确定环境下的决策制定、博弈论、计划与排程、设计和教育等。已实施了许多开源游戏或基于游戏的环境用于研究人工智能。除了单人或多人、合作或对抗性游戏外，在创意设计方面也越来越受到关注。这些平台为探索和比较人工智能的思想和技术提供了理想基准。本文回顾了基于游戏的人工智能研究平台，讨论了由这些平台演变引起的研究趋势，并展望了未来。

    Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous open-sourced games or game-based environments have been implemented for studying artificial intelligence. In addition to single- or multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, discusses the research trend induced by the evolution of those platforms, and gives an outlook.
    
[^36]: 贝叶斯联邦学习：综述

    Bayesian Federated Learning: A Survey. (arXiv:2304.13267v1 [cs.LG])

    [http://arxiv.org/abs/2304.13267](http://arxiv.org/abs/2304.13267)

    本文综述了贝叶斯联邦学习（BFL）的基本概念和分类，包括客户端、服务器端和基于FL的BFL方法。BFL是解决现有FL方法中受限和动态的数据和条件、异质性和不确定性以及分析解释能力挑战的有前途方法。

    

    联邦学习（FL）以隐私保护的方式整合了分布式基础设施、通信、计算和学习，展示了其优势。然而，现有FL方法的鲁棒性和能力受到有限和动态的数据和条件，包括异质性和不确定性，以及分析解释能力的挑战。贝叶斯联邦学习（BFL）已成为解决这些问题的一种有前途的方法。本综述对BFL进行了批判性的概述，包括其基本概念，其在FL上下文中与贝叶斯学习的关系，以及从贝叶斯和联邦学习的角度对BFL进行分类和讨论。我们将客户端和服务器端的BFL方法以及基于FL的BFL方法进行了分类和讨论，分析了它们的优缺点。本文还讨论了现有BFL方法的局限性以及BFL研究的未来方向，以进一步满足现实生活中FL应用的复杂需求。

    Federated learning (FL) demonstrates its advantages in integrating distributed infrastructure, communication, computing and learning in a privacy-preserving manner. However, the robustness and capabilities of existing FL methods are challenged by limited and dynamic data and conditions, complexities including heterogeneities and uncertainties, and analytical explainability. Bayesian federated learning (BFL) has emerged as a promising approach to address these issues. This survey presents a critical overview of BFL, including its basic concepts, its relations to Bayesian learning in the context of FL, and a taxonomy of BFL from both Bayesian and federated perspectives. We categorize and discuss client- and server-side and FL-based BFL methods and their pros and cons. The limitations of the existing BFL methods and the future directions of BFL research further address the intricate requirements of real-life FL applications.
    
[^37]: 基于分数的BSDE扩散模型：反演和生成。

    Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation. (arXiv:2304.13224v1 [cs.LG])

    [http://arxiv.org/abs/2304.13224](http://arxiv.org/abs/2304.13224)

    本文提出了一种基于BSDE的扩散模型，可以通过调整现有的分数函数确定到达所需终端分布所需的初始条件，为扩散建模提供了一种新的方法，并在扩散反演，条件扩散和不确定性量化等领域具有潜在应用。

    

    所提出的基于BSDE的扩散模型为扩散建模提供了一种新的方法，扩展了随机微分方程在机器学习中的应用。与传统的SDE-based扩散模型不同，我们的模型可以通过调整现有的分数函数确定到达所需终端分布所需的初始条件。我们展示了该模型的理论保证，使用Lipschitz网络进行分数匹配的优势，以及在扩散反演，条件扩散和不确定性量化等各个领域的潜在应用。我们的工作对于基于分数的生成学习领域做出了贡献，为解决现实问题提供了有前途的方向。

    The proposed BSDE-based diffusion model represents a novel approach to diffusion modeling, which extends the application of stochastic differential equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion models, our model can determine the initial conditions necessary to reach a desired terminal distribution by adapting an existing score function. We demonstrate the theoretical guarantees of the model, the benefits of using Lipschitz networks for score matching, and its potential applications in various areas such as diffusion inversion, conditional diffusion, and uncertainty quantification. Our work represents a contribution to the field of score-based generative learning and offers a promising direction for solving real-world problems.
    
[^38]: 具有局部参数模型知识的强化学习

    Reinforcement Learning with Partial Parametric Model Knowledge. (arXiv:2304.13223v1 [eess.SY])

    [http://arxiv.org/abs/2304.13223](http://arxiv.org/abs/2304.13223)

    该论文研究了在环境完全无知和完美知识之间的机遇，提出了一种利用局部模型和保持数据驱动调整的强化学习方法，已在线性二次调节器上得到验证。

    

    我们将强化学习方法应用于连续控制，以填补在环境完全无知和完美知识之间的差距。我们的方法，Partial Knowledge Least Squares Policy Iteration(PLSPI)，既借鉴了模型无关的强化学习，也借鉴了模型基础控制。它利用局部模型的不完全信息，并保留强化学习朝向最优性能的数据驱动调整。我们以线性二次调节器为案例研究；数值实验证明了所提方法的有效性和带来的好处。

    We adapt reinforcement learning (RL) methods for continuous control to bridge the gap between complete ignorance and perfect knowledge of the environment. Our method, Partial Knowledge Least Squares Policy Iteration (PLSPI), takes inspiration from both model-free RL and model-based control. It uses incomplete information from a partial model and retains RL's data-driven adaption towards optimal performance. The linear quadratic regulator provides a case study; numerical experiments demonstrate the effectiveness and resulting benefits of the proposed method.
    
[^39]: 利用CNN进行Pascal VOC的语义分割

    Exploiting CNNs for Semantic Segmentation with Pascal VOC. (arXiv:2304.13216v1 [cs.CV])

    [http://arxiv.org/abs/2304.13216](http://arxiv.org/abs/2304.13216)

    本文在Pascal VOC数据集上进行了语义分割的研究，使用了FCN作为基准并改进了其问题，最终发现使用ResNet进行迁移学习的表现最佳。

    

    本文在Pascal VOC数据集上对语义分割进行了全面研究。我们使用了完全卷积网络（FCN）作为基准，并针对其问题进行改进，包括余弦退火学习率调度器、数据增强和类别不平衡权重。此外，我们还探索了三种不同的架构，并发现使用ResNet进行迁移学习的表现最佳。

    In this paper, we present a comprehensive study on semantic segmentation with the Pascal VOC dataset. Here, we have to label each pixel with a class which in turn segments the entire image based on the objects/entities present. To tackle this, we firstly use a Fully Convolution Network (FCN) baseline which gave 71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and working and subsequently address the issues in the baseline with three improvements: a) cosine annealing learning rate scheduler(pixel accuracy: 72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585) c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from these changes in training pipeline, we also explore three different architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%, IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649). We observe that
    
[^40]: 面向心理健康的可解释和安全的会话型智能助手：一项调查

    Towards Explainable and Safe Conversational Agents for Mental Health: A Survey. (arXiv:2304.13191v1 [cs.AI])

    [http://arxiv.org/abs/2304.13191](http://arxiv.org/abs/2304.13191)

    这篇论文调查了现有的心理健康会话型智能助手，提出了改进的新见解，并介绍了如何构建责任VMHA，以提出后续问题或提供知情回应，丰富用户体验。

    

    虚拟心理健康助手（VMHA）在持续推进，以支持每年6000万次初级医疗保健就诊和600万次急诊室就诊的负担过重的全球医疗保健系统。这些系统是由临床心理学家、精神科医师和人工智能（AI）研究人员为认知行为疗法（CBT）构建的。目前，VMHA的作用是通过信息提供情感支持，重点不在与患者进行深入的反思对话。需要更全面、安全和可解释的方法来构建负责任的VMHA，以提出后续问题或提供知情回应。这项调查对现有的心理健康会话型智能助手进行了系统的批判性审查，随后提出了关于VMHA改进的新见解，包括环境知识、数据集和它们在临床决策支持中的新兴角色。我们还提供了丰富用户体验的新方向。

    Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to support the overburdened global healthcare system that gets 60 million primary care visits, and 6 million Emergency Room (ER) visits annually. These systems are built by clinical psychologists, psychiatrists, and Artificial Intelligence (AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role of VMHAs is to provide emotional support through information, focusing less on developing a reflective conversation with the patient. A more comprehensive, safe and explainable approach is required to build responsible VMHAs to ask follow-up questions or provide a well-informed response. This survey offers a systematic critical review of the existing conversational agents in mental health, followed by new insights into the improvements of VMHAs with contextual knowledge, datasets, and their emerging role in clinical decision support. We also provide new directions toward enriching the user experience
    
[^41]: 搜寻海洋世界生命探测器上用于检测显微生物生物标志的仪器自主性

    Onboard Science Instrument Autonomy for the Detection of Microscopy Biosignatures on the Ocean Worlds Life Surveyor. (arXiv:2304.13189v1 [astro-ph.IM])

    [http://arxiv.org/abs/2304.13189](http://arxiv.org/abs/2304.13189)

    在太阳系的冰卫星寻找外星生命，需要用一套补充仪器对多个独立的生物标志进行采样。机载科学仪器自主性（OSIA）是一种新兴学科，可以评估、总结和优先考虑观测仪器数据以最大化科学回报。

    

    寻找外星生命是一项重要的科学探索，对文明级别具有影响。太阳系中的冰卫星因其液态海洋使其成为微观生命潜在寄生地，是探索重要目标。然而，生命的精确定义缺乏使制定探测策略面临基本挑战。为增加无歧义探测的机会，一套补充仪器必须对多个独立的生物标志进行采样（例如，组成，运动/行为，可见结构）。这样的仪器套件可以产生比像土卫六或木卫二这样的遥远海洋世界传输的原始数据多10,000倍。为解决这种带宽限制，机载科学仪器自主性（OSIA）是一种评估、总结和优先考虑观测仪器数据以最大化科学回报的飞行系统的新兴学科。我们描述了两个开发的OSIA实现方案。

    The quest to find extraterrestrial life is a critical scientific endeavor with civilization-level implications. Icy moons in our solar system are promising targets for exploration because their liquid oceans make them potential habitats for microscopic life. However, the lack of a precise definition of life poses a fundamental challenge to formulating detection strategies. To increase the chances of unambiguous detection, a suite of complementary instruments must sample multiple independent biosignatures (e.g., composition, motility/behavior, and visible structure). Such an instrument suite could generate 10,000x more raw data than is possible to transmit from distant ocean worlds like Enceladus or Europa. To address this bandwidth limitation, Onboard Science Instrument Autonomy (OSIA) is an emerging discipline of flight systems capable of evaluating, summarizing, and prioritizing observational instrument data to maximize science return. We describe two OSIA implementations developed a
    
[^42]: 基于GPT-4的人工智能辅助编程实验

    AI-assisted coding: Experiments with GPT-4. (arXiv:2304.13187v1 [cs.AI])

    [http://arxiv.org/abs/2304.13187](http://arxiv.org/abs/2304.13187)

    该论文报告了几个基于GPT-4的人工智能辅助编程实验，发现使用当前工具的人工智能代码生成虽然强大，但需要大量人类验证以确保准确性和有效性，同时GPT-4可以通过重构现有代码显著提高代码质量，并生成具有实质性覆盖范围的测试，但应用于相关代码时，许多测试失败了。

    

    基于大型语言模型的人工智能工具在某些计算机编程任务上已经达到了人类水平的表现。我们报告了几个使用GPT-4生成计算机代码的实验。这些实验表明，使用当前工具的人工智能代码生成虽然强大，但需要人类的大量验证来确保其准确性。我们还展示了GPT-4重构现有代码可以显著提高该代码在几个已有代码质量指标上的表现，并展示了GPT-4可以生成具有实质性覆盖范围的测试，但是应用于相关代码时，许多测试失败了。这些发现表明，尽管人工智能编码工具非常强大，但它们仍需要人类在其中发挥作用，以确保结果的有效性和准确性。

    Artificial intelligence (AI) tools based on large language models have acheived human-level performance on some computer programming tasks. We report several experiments using GPT-4 to generate computer code. These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance. We also demonstrate that GPT-4 refactoring of existing code can significantly improve that code along several established metrics for code quality, and we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code. These findings suggest that while AI coding tools are very powerful, they still require humans in the loop to ensure validity and accuracy of the results.
    
[^43]: 迈向计算优化的迁移学习

    Towards Compute-Optimal Transfer Learning. (arXiv:2304.13164v1 [cs.LG])

    [http://arxiv.org/abs/2304.13164](http://arxiv.org/abs/2304.13164)

    本文提出一种计算优化的迁移学习方法，通过对预训练模型进行零-shot结构剪枝，使其在最小降低性能的情况下提高计算效率，实现了20%以上的性能提升。

    

    预训练模型的出现使得迁移学习领域发生了重大变革，这些模型在下游任务中展现了强大的适应性。但是，微调或使用这些模型的高计算和存储要求可能会阻碍它们的广泛使用。在本研究中，我们提出了一个解决方案，通过提出一种简单而有效的方法，将计算效率与渐近性能之间的权衡，我们将其定义为学习算法在计算趋近于无穷大时实现的性能。具体而言，我们认为对预训练模型进行零-shot结构剪枝，可以使它们在最小降低性能的情况下提高计算效率。我们在提供多种迁移场景的Nevis'22连续学习基准上评估了我们的方法。我们的结果表明，在低计算范围内，剪枝预先训练模型的卷积过滤器可以带来超过20%的性能提高。

    The field of transfer learning is undergoing a significant shift with the introduction of large pretrained models which have demonstrated strong adaptability to a variety of downstream tasks. However, the high computational and memory requirements to finetune or use these models can be a hindrance to their widespread use. In this study, we present a solution to this issue by proposing a simple yet effective way to trade computational efficiency for asymptotic performance which we define as the performance a learning algorithm achieves as compute tends to infinity. Specifically, we argue that zero-shot structured pruning of pretrained models allows them to increase compute efficiency with minimal reduction in performance. We evaluate our method on the Nevis'22 continual learning benchmark that offers a diverse set of transfer scenarios. Our results show that pruning convolutional filters of pretrained models can lead to more than 20% performance improvement in low computational regimes.
    
[^44]: Roll-Drop：使用单一参数考虑观测噪声的方法在深度强化学习中的应用

    Roll-Drop: accounting for observation noise with a single parameter. (arXiv:2304.13150v1 [cs.RO])

    [http://arxiv.org/abs/2304.13150](http://arxiv.org/abs/2304.13150)

    本文提出了一种单一参数的策略Roll-Drop，在深度强化学习中使用dropout来考虑观测噪声，而不需要显式建模噪声分布，从而提升了模拟到真实环境的转移能力。

    

    本文提出了一种称为Roll-Drop的简单策略，它在模拟中使用dropout来考虑部署期间观测噪声而不需要显式地模拟每个状态下的噪声分布，从而提升了深度强化学习中模拟到真实环境的转移能力。实验表明，在注入了高达25%的观测噪声时，该方法的成功率高达80%，鲁棒性比基线方法高出两倍。

    This paper proposes a simple strategy for sim-to-real in Deep-Reinforcement Learning (DRL) -- called Roll-Drop -- that uses dropout during simulation to account for observation noise during deployment without explicitly modelling its distribution for each state. DRL is a promising approach to control robots for highly dynamic and feedback-based manoeuvres, and accurate simulators are crucial to providing cheap and abundant data to learn the desired behaviour. Nevertheless, the simulated data are noiseless and generally show a distributional shift that challenges the deployment on real machines where sensor readings are affected by noise. The standard solution is modelling the latter and injecting it during training; while this requires a thorough system identification, Roll-Drop enhances the robustness to sensor noise by tuning only a single parameter. We demonstrate an 80% success rate when up to 25% noise is injected in the observations, with twice higher robustness than the baseline
    
[^45]: MBIB--首个媒体偏见识别基准测试任务和数据集集合的介绍

    Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection. (arXiv:2304.13148v1 [cs.IR])

    [http://arxiv.org/abs/2304.13148](http://arxiv.org/abs/2304.13148)

    这篇论文介绍了MBIB，一个将不同类型媒体偏见分为共同框架的全面基准测试，并提供了相关数据集以评估媒体偏见检测技术，结果显示没有单一技术可以显著优于其他技术，同时发现研究兴趣和资源分配不均匀分布。

    

    尽管媒体偏见检测是一个复杂的多任务问题，但目前还没有一个统一的基准来分组这些评估任务。我们引入了媒体偏见识别基准测试（MBIB），这是一个全面的基准测试，将不同类型的媒体偏见（例如，语言、认知、政治）分为一个共同的框架，以测试预测检测技术的概括化程度。在评估了115个数据集后，我们选择了9个任务，仔细提出了22个相关数据集，以评估媒体偏见检测技术。我们使用最先进的Transformer技术（例如T5、BART）评估MBIB。我们的结果表明，尽管仇恨言论、种族偏见和性别偏见更容易检测，但模型难以处理某些偏见类型，例如，认知和政治偏见。但是，我们的结果表明，没有单一技术可以显着优于其他技术。我们还发现研究兴趣和资源分配在媒体偏见识别的个别任务上存在不均匀分布。

    Although media bias detection is a complex multi-task problem, there is, to date, no unified benchmark grouping these evaluation tasks. We introduce the Media Bias Identification Benchmark (MBIB), a comprehensive benchmark that groups different types of media bias (e.g., linguistic, cognitive, political) under a common framework to test how prospective detection techniques generalize. After reviewing 115 datasets, we select nine tasks and carefully propose 22 associated datasets for evaluating media bias detection techniques. We evaluate MBIB using state-of-the-art Transformer techniques (e.g., T5, BART). Our results suggest that while hate speech, racial bias, and gender bias are easier to detect, models struggle to handle certain bias types, e.g., cognitive and political bias. However, our results show that no single technique can outperform all the others significantly. We also find an uneven distribution of research interest and resource allocation to the individual tasks in media 
    
[^46]: 自监督的时空数据分析

    Self-Supervised Temporal Analysis of Spatiotemporal Data. (arXiv:2304.13143v1 [cs.AI])

    [http://arxiv.org/abs/2304.13143](http://arxiv.org/abs/2304.13143)

    本文提出了一种自监督方法，能根据移动活动时间序列对景观进行分层，通过深度语义分割实现了对地理空间任务的多模态建模，适用于分类居民区和商业区等不同任务。

    

    地理活动的时间模式与用地类型之间存在着相关性。本文提出了一种新的自监督方法，根据移动活动时间序列对景观进行分层。首先，将时间序列信号转换为频域，然后通过压缩自编码器将其压缩为任务无关的时间嵌入，该嵌入保留了时间序列中观察到的周期性时间模式。像素级的嵌入被转换为类似图像的通道，可以用于基于任务的下游地理空间任务的多模态建模，其中采用了深度语义分割。实验证明，时间嵌入是时间序列数据的语义有意义的表征，并且对于分类居民区和商业区等不同任务是有效的。

    There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas.
    
[^47]: 面向增材制造试件表面粗糙度预测的量子机器学习方法

    Quantum Machine Learning Approach for the Prediction of Surface Roughness in Additive Manufactured Specimens. (arXiv:2304.13142v1 [quant-ph])

    [http://arxiv.org/abs/2304.13142](http://arxiv.org/abs/2304.13142)

    首次针对增材制造试件的表面粗糙度使用三种量子算法（QNN、Q-Forest和VQC）进行回归预测，其中Q-Forest算法表现最优，具有较低的MSE和MAE和较高的EVS。

    

    表面粗糙度是影响增材制造零件性能和功能的重要因素。准确预测表面粗糙度对于优化制造过程和确保最终产品的质量至关重要。最近，量子计算作为解决复杂问题和创建精确预测模型的潜在解决方案引起了关注。在本研究论文中，我们首次针对增材制造试件的表面粗糙度进行了三种量子算法，包括量子神经网络（QNN）、量子森林（Q-Forest）和变分量子分类器（VQC）的回归适应比较。我们使用均方误差（MSE）、平均绝对误差（MAE）和解释方差得分（EVS）作为评估指标来评估算法的性能。我们的研究结果表明，Q-Forest算法超越了其他算法，MSE为56.905，MAE为7.479，EVS为0.2957。

    Surface roughness is a crucial factor influencing the performance and functionality of additive manufactured components. Accurate prediction of surface roughness is vital for optimizing manufacturing processes and ensuring the quality of the final product. Quantum computing has recently gained attention as a potential solution for tackling complex problems and creating precise predictive models. In this research paper, we conduct an in-depth comparison of three quantum algorithms i.e. the Quantum Neural Network (QNN), Quantum Forest (Q-Forest), and Variational Quantum Classifier (VQC) adapted for regression for predicting surface roughness in additive manufactured specimens for the first time. We assess the algorithms performance using Mean Squared Error (MSE), Mean Absolute Error (MAE), and Explained Variance Score (EVS) as evaluation metrics. Our findings show that the Q-Forest algorithm surpasses the other algorithms, achieving an MSE of 56.905, MAE of 7.479, and an EVS of 0.2957. I
    
[^48]: 决策时间规划的更新等价框架

    The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])

    [http://arxiv.org/abs/2304.13138](http://arxiv.org/abs/2304.13138)

    该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。

    

    在棋类游戏等完全信息环境中，即时修正（或构建）策略的决策时间规划是实现超人类表现的关键。一些研究将决策时间规划扩展到更普遍的不完全信息环境，从而实现了扑克中的超人类表现。但是，这些方法需要考虑随着非公共信息量的增加而快速增长的子游戏，使得它们在非公共信息量较大时不起作用。为了解决这个问题，我们引入了一种基于更新等价而不是子游戏概念的决策时间规划框架。在这个框架中，决策时间规划算法模拟同步学习算法的更新。这个框架使我们能够引入一系列原则上的决策时间规划算法，这些算法不依赖于公共信息，并为新的一个系列的决策时间规划算法打开了大门。

    The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
    
[^49]: 利用机器学习辅助的事件逐事件多普勒修正进行快速、热的异位同位素的高精度光谱学研究

    Precision Spectroscopy of Fast, Hot Exotic Isotopes Using Machine Learning Assisted Event-by-Event Doppler Correction. (arXiv:2304.13120v1 [nucl-ex])

    [http://arxiv.org/abs/2304.13120](http://arxiv.org/abs/2304.13120)

    提出一种新的实验方案，结合机器学习辅助的事件逐事件多普勒修正，实现对快速、热的异位同位素进行高精度的激光光谱学研究，在极端温度下仍能实现kHz级别的不确定性。

    

    我们提出了一个实验方案,用于在快速的异位同位素上进行敏感、高精度的激光光谱学研究。通过在电场内诱导原子的逐步共振电离,然后检测离子和相应的电子,可以进行时间和位置敏感的结果粒子的测量。使用混合密度网络 (MDN),我们可以利用这些信息对单个原子的初始能量进行预测,从而在事件逐事件的基础上应用多普勒修正所观察到的跃迁频率。我们对该提议的实验方案进行数值模拟,并表明可以实现kHz级别的不确定性,对在极端温度 ($> 10^8$ K) 下产生的离子束进行，在能量分散最大可达10 keV和速度分布不均匀的情况下。能够直接在高能束上进行飞行谱学研究,为研究短寿命的离子的物理和化学性质提供了独特的机会。

    We propose an experimental scheme for performing sensitive, high-precision laser spectroscopy studies on fast exotic isotopes. By inducing a step-wise resonant ionization of the atoms travelling inside an electric field and subsequently detecting the ion and the corresponding electron, time- and position-sensitive measurements of the resulting particles can be performed. Using a Mixture Density Network (MDN), we can leverage this information to predict the initial energy of individual atoms and thus apply a Doppler correction of the observed transition frequencies on an event-by-event basis. We conduct numerical simulations of the proposed experimental scheme and show that kHz-level uncertainties can be achieved for ion beams produced at extreme temperatures ($> 10^8$ K), with energy spreads as large as $10$ keV and non-uniform velocity distributions. The ability to perform in-flight spectroscopy, directly on highly energetic beams, offers unique opportunities to studying short-lived i
    
[^50]: 有限CSI下的THz波束搜索的联邦深度强化学习

    Federated Deep Reinforcement Learning for THz-Beam Search with Limited CSI. (arXiv:2304.13109v1 [cs.AI])

    [http://arxiv.org/abs/2304.13109](http://arxiv.org/abs/2304.13109)

    本文提出了一个联邦深度强化学习的方法，可以用于在有限CSI的情况下迅速进行THz波束搜索，以克服THz信号的传播衰减。

    

    THz通信是下一代无线网络中高数据率的有前途的技术，然而其严重的传播衰减限制了其实际应用。本文提出了一种联邦深度强化学习（FDRL）的新方法，用于在运营商网络中，由边缘服务器协调的多个基站（BS）迅速执行THz波束搜索。所有BS都进行基于DDPG（深度确定性策略梯度）的DRL以获得具有有限信道状态信息（CSI）的THz波束成形策略。他们使用隐藏信息更新他们的DDPG模型，以减轻跨小区干扰。

    Terahertz (THz) communication with ultra-wide available spectrum is a promising technique that can achieve the stringent requirement of high data rate in the next-generation wireless networks, yet its severe propagation attenuation significantly hinders its implementation in practice. Finding beam directions for a large-scale antenna array to effectively overcome severe propagation attenuation of THz signals is a pressing need. This paper proposes a novel approach of federated deep reinforcement learning (FDRL) to swiftly perform THz-beam search for multiple base stations (BSs) coordinated by an edge server in a cellular network. All the BSs conduct deep deterministic policy gradient (DDPG)-based DRL to obtain THz beamforming policy with limited channel state information (CSI). They update their DDPG models with hidden information in order to mitigate inter-cell interference. We demonstrate that the cell network can achieve higher throughput as more THz CSI and hidden neurons of DDPG a
    
[^51]: 基于WiFi CSI的无设备多房间人体存在检测的时间选择性循环神经网络

    Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v1 [cs.AI])

    [http://arxiv.org/abs/2304.13107](http://arxiv.org/abs/2304.13107)

    这篇论文提出了一种使用基于WiFi信道状态信息提取人体移动和空间特征的无设备多房间人体存在检测系统，能够通过时间-selective特征提取算法区分有直觉视线路径阻塞和无视线路径阻塞的情况。

    

    人类存在检测是各种应用的重要技术，包括家居自动化、安全和医疗保健。虽然传统上采用基于摄像机的系统来实现这一目的，但会引发隐私问题。为了解决这个问题，最近的研究探讨了利用商用WiFi接入点提供的信道状态信息(CSI)方法，提供详细的信道特征。在本论文中，我们提出了一个基于时间选择性条件双特征提取递归网络(TCD-FERN)的无设备多房间人体存在检测系统。我们的系统旨在使用动态和静态(DaS)数据预处理技术，在条件人体特征下捕捉重要的时间特征，提取人的移动和空间特征，并区分有直接视线路径阻塞和无视线路径阻塞的情况。为了减少房间隔断造成的特征衰减问题，我们使用基于 LSTM 的 NCoV-DaS 技术。

    Human presence detection is a crucial technology for various applications, including home automation, security, and healthcare. While camera-based systems have traditionally been used for this purpose, they raise privacy concerns. To address this issue, recent research has explored the use of channel state information (CSI) approaches that can be extracted from commercial WiFi access points (APs) and provide detailed channel characteristics. In this thesis, we propose a device-free human presence detection system for multi-room scenarios using a time-selective conditional dual feature extract recurrent Network (TCD-FERN). Our system is designed to capture significant time features with the condition on current human features using a dynamic and static (DaS) data preprocessing technique to extract moving and spatial features of people and differentiate between line-of-sight (LoS) path blocking and non-blocking cases. To mitigate the feature attenuation problem caused by room partitions,
    
[^52]: 利用室内WiFi系统进行无设备穿墙存在检测的注意增强深度学习

    Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])

    [http://arxiv.org/abs/2304.13105](http://arxiv.org/abs/2304.13105)

    本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。

    

    在室内环境中准确检测人员存在对于各种应用非常重要，例如能源管理和安全。本文提出了一种利用WiFi信号的通道状态信息（CSI）进行人员存在检测的新系统。我们的系统名为注意力增强深度学习（ALPD），采用关注机制从CSI数据中自动选择有信息量的子载波，并采用双向长短时记忆（LSTM）网络捕捉CSI中的时间依赖性。此外，我们利用一个静态特征来提高静态状态下人员存在检测的准确性。我们通过部署一对WiFi接入点（AP）来收集CSI数据集来评估所提出的ALPD系统，该系统进一步与几个基准进行比较。结果表明，我们的ALPD系统在准确性方面优于基准，特别是在存在干扰的情况下。此外，双向传输数据不会影响我们系统的性能，证明了其在现实场景中的稳健性。

    Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
    
[^53]: 基于LSTM的微电网负荷预测对抗注入攻击的鲁棒性研究

    LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid. (arXiv:2304.13104v1 [cs.LG])

    [http://arxiv.org/abs/2304.13104](http://arxiv.org/abs/2304.13104)

    本文研究了LSTM神经网络在微电网负荷预测中受到噪声注入攻击的鲁棒性。使用低通滤波器消除了攻击，以提高模型的性能。

    

    本文研究了LSTM神经网络在理想微电网中的电力负荷预测中对抗注入攻击的鲁棒性。研究了在不同信噪比下进行的黑盒高斯噪声攻击下LSTM模型的性能。假设攻击者只能访问LSTM模型的输入数据。结果表明，噪声攻击影响了LSTM模型的性能。对于正常预测，负荷预测的平均绝对误差（MAE）为0.047 MW，而对于信噪比为6 dB的高斯噪声插入，该值增加到了0.097 MW。为了使LSTM模型对噪声攻击具有鲁棒性，将最佳截止频率的低通滤波器应用于模型的输入以消除噪声攻击。该滤波器在低信噪比噪声的情况下表现更好，对于小噪声的效果较差。

    In this paper, we investigate the robustness of an LSTM neural network against noise injection attacks for electric load forecasting in an ideal microgrid. The performance of the LSTM model is investigated under a black-box Gaussian noise attack with different SNRs. It is assumed that attackers have just access to the input data of the LSTM model. The results show that the noise attack affects the performance of the LSTM model. The load prediction means absolute error (MAE) is 0.047 MW for a healthy prediction, while this value increases up to 0.097 MW for a Gaussian noise insertion with SNR= 6 dB. To robustify the LSTM model against noise attack, a low-pass filter with optimal cut-off frequency is applied at the model's input to remove the noise attack. The filter performs better in case of noise with lower SNR and is less promising for small noises.
    
[^54]: HyMo: 一种新型多模态混合模型在智能合约漏洞检测中的应用

    HyMo: Vulnerability Detection in Smart Contracts using a Novel Multi-Modal Hybrid Model. (arXiv:2304.13103v1 [cs.CR])

    [http://arxiv.org/abs/2304.13103](http://arxiv.org/abs/2304.13103)

    本文提出了一种名为 HyMo 的多模态混合深度学习模型，结合了 FastText 词嵌入技术和 BiGRU 深度学习技术，用于智能合约漏洞检测。

    

    随着区块链技术的快速发展，智能合约已成为金融、医疗保健、保险和游戏等许多行业中的常用工具。智能合约数量已经成倍增加，同时由于智能合约漏洞带来的经济损失，智能合约的安全性引起了相当大的关注。现有的分析技术能够识别大量的智能合约安全漏洞，但它们过于依赖专家建立的刚性标准，检测过程随着智能合约的复杂程度增加而变得更加耗时。在本文中，我们提出了 HyMo 作为一种多模态混合深度学习模型，它智能地考虑多种输入表示来实现多模态，并使用 FastText 词嵌入技术将每个单词表示为字符的 n-gram，与由两个 GRUs 组成的序列处理模型的 BiGRU 深度学习技术搭配，以实现智能合约漏洞检测的最佳结果。

    With blockchain technology rapidly progress, the smart contracts have become a common tool in a number of industries including finance, healthcare, insurance and gaming. The number of smart contracts has multiplied, and at the same time, the security of smart contracts has drawn considerable attention due to the monetary losses brought on by smart contract vulnerabilities. Existing analysis techniques are capable of identifying a large number of smart contract security flaws, but they rely too much on rigid criteria established by specialists, where the detection process takes much longer as the complexity of the smart contract rises. In this paper, we propose HyMo as a multi-modal hybrid deep learning model, which intelligently considers various input representations to consider multimodality and FastText word embedding technique, which represents each word as an n-gram of characters with BiGRU deep learning technique, as a sequence processing model that consists of two GRUs to achiev
    
[^55]: 新兴技术的组织治理：AI在医疗保健中的应用

    Organizational Governance of Emerging Technologies: AI Adoption in Healthcare. (arXiv:2304.13081v1 [cs.AI])

    [http://arxiv.org/abs/2304.13081](http://arxiv.org/abs/2304.13081)

    该研究通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，制定了AI在医疗保健中的组织治理框架，包括关键控制点和决策标准，为卫生系统领导人做出更加明智的决策提供了支持。

    

    私营和公共部门的结构和规范精细化了新兴技术的实际应用。在医疗保健中，尽管出现了大量的AI采用方式，但是其使用和整合周围的组织治理往往被认为不可行。健康AI合作伙伴关系（HAIP）旨在通过此研究更好地定义医疗保健中AI系统的充分组织治理要求，并支持卫生系统领导人做出更加明智的决策。要达到这个目标，我们首先确定了AI在医疗保健中采用的标准如何易于使用和高效运作。然后，我们在特定的卫生系统中，绘制出实际机构采用AI技术的具体决策点。在实践中，我们通过与美国主要医疗保健系统的领导人和相关领域的主要知情人合作，实现了这一目标。通过这种合作，我们制定了AI在医疗保健中的组织治理框架，其中包括关键控制点和决策标准。

    Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working w
    
[^56]: 针对树莓派优化深度学习模型

    Optimizing Deep Learning Models For Raspberry Pi. (arXiv:2304.13039v1 [eess.SY])

    [http://arxiv.org/abs/2304.13039](http://arxiv.org/abs/2304.13039)

    针对树莓派优化深度学习模型包括修剪技术和模型参数结构优化，以适应其硬件特点并提高能效。

    

    深度学习模型在计算机视觉、自然语言处理和语音识别等领域越来越受欢迎。但这类模型需要大量的计算资源，使得在低功耗设备上运行（如树莓派）变得具有挑战性。针对这个问题的解决方案包括修剪（pruning）技术和优化模型以适合树莓派等硬件架构。在训练期间或训练后修剪可以减小模型大小，让模型更高效。优化模型则包括调整模型参数和结构适应树莓派硬件特点，例如树莓派的CPU和GPU，并通过最小化计算量以实现能耗的优化等方面。

    Deep learning models have become increasingly popular for a wide range of applications, including computer vision, natural language processing, and speech recognition. However, these models typically require large amounts of computational resources, making them challenging to run on low-power devices such as the Raspberry Pi. One approach to addressing this challenge is to use pruning techniques to reduce the size of the deep learning models. Pruning involves removing unimportant weights and connections from the model, resulting in a smaller and more efficient model. Pruning can be done during training or after the model has been trained. Another approach is to optimize the deep learning models specifically for the Raspberry Pi architecture. This can include optimizing the model's architecture and parameters to take advantage of the Raspberry Pi's hardware capabilities, such as its CPU and GPU. Additionally, the model can be optimized for energy efficiency by minimizing the amount of c
    
[^57]: 一个统一的主动学习框架，用于注释图形数据并应用于软件代码性能预测

    A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])

    [http://arxiv.org/abs/2304.13032](http://arxiv.org/abs/2304.13032)

    提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。

    

    大多数机器学习和数据分析应用程序，包括软件系统中的性能工程，需要大量注释和标记数据，这些可能事先并不可用。获取注释通常需要显着的时间、精力和计算资源，这使得任务变得具有挑战性。我们开发了一个统一的主动学习框架，专门针对软件性能预测来解决这个任务。我们从将源代码解析成抽象语法树（AST）开始，然后用数据和控制流边来增强它。然后，我们将源代码的树形表示转换为流增强抽象语法树图（FA-AST）表示法。基于图形表示，我们将各种图嵌入（无监督和有监督）构造成一个潜在空间。鉴于这样的嵌入，该框架变得任务不可知，因为可以使用任何回归方法和适用于回归的查询策略来执行主动学习。在该框架内，我们引入并评估了几种用于软件性能预测的主动学习查询策略和回归算法，证明了我们的方法优于现有方法。

    Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
    
[^58]: 证明结构的研究

    Investigations into Proof Structures. (arXiv:2304.12827v1 [cs.AI])

    [http://arxiv.org/abs/2304.12827](http://arxiv.org/abs/2304.12827)

    介绍了一种新的形式主义来操作和分析证明，用于生成更短的证明和减少搜索工作量。

    

    我们引入并详细阐述了一种新型形式主义来操作和分析证明作为一个整体的对象。在这第一次尝试中，这个形式主义仅限于由浓缩推导特征的一阶问题。我们以一个全面的形式重构和分析历史上{\L}ukasiewicz广泛研究过的问题的证明为例进行了阐述。这种方法为在证明搜索过程中生成引理提供了新的系统方法，以减少搜索工作量并找到更短的证明。在这条路线上报告了许多实验，其中自动发现了一个证明{\L}ukasiewicz的问题，它比以前任何由人或机器发现的证明都要短得多。

    We introduce and elaborate a novel formalism for the manipulation and analysis of proofs as objects in a global manner. In this first approach the formalism is restricted to first-order problems characterized by condensed detachment. It is applied in an exemplary manner to a coherent and comprehensive formal reconstruction and analysis of historical proofs of a widely-studied problem due to {\L}ukasiewicz. The underlying approach opens the door towards new systematic ways of generating lemmas in the course of proof search to the effects of reducing the search effort and finding shorter proofs. Among the numerous reported experiments along this line, a proof of {\L}ukasiewicz's problem was automatically discovered that is much shorter than any proof found before by man or machine.
    
[^59]: 通过对比学习和一致的语义和结构约束进行无监督合成图像细化

    Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic and Structure Constraints. (arXiv:2304.12591v1 [cs.CV])

    [http://arxiv.org/abs/2304.12591](http://arxiv.org/abs/2304.12591)

    本文采用对比学习和一致的语义和结构约束来减少合成和细化图像之间的语义失真，进一步提高了性能。

    

    确保计算机生成的合成图像的真实性对于深度神经网络（DNN）的训练至关重要。由于合成和真实数据集之间存在不同的语义分布，因此合成和细化图像之间存在语义不匹配，进而导致语义失真。最近，对比学习（CL）已成功地用于将相关补丁拉在一起并将不相关的补丁推开。在这项工作中，我们利用合成和精细图像之间的语义和结构一致性，并采用CL来减少语义失真。此外，我们还采用了硬负采样来进一步提高性能。我们使用定性和定量措施比较了我们方法与几种其他基准方法的性能，并表明我们的方法提供了最先进的性能。

    Ensuring the realism of computer-generated synthetic images is crucial to deep neural network (DNN) training. Due to different semantic distributions between synthetic and real-world captured datasets, there exists semantic mismatch between synthetic and refined images, which in turn results in the semantic distortion. Recently, contrastive learning (CL) has been successfully used to pull correlated patches together and push uncorrelated ones apart. In this work, we exploit semantic and structural consistency between synthetic and refined images and adopt CL to reduce the semantic distortion. Besides, we incorporate hard negative mining to improve the performance furthermore. We compare the performance of our method with several other benchmarking methods using qualitative and quantitative measures and show that our method offers the state-of-the-art performance.
    
[^60]: 基于偏相关的深度视觉表示学习用于图像分类

    Learning Partial Correlation based Deep Visual Representation for Image Classification. (arXiv:2304.11597v1 [cs.CV])

    [http://arxiv.org/abs/2304.11597](http://arxiv.org/abs/2304.11597)

    本文提出了一种基于偏相关的深度视觉表示学习方法，解决了使用协方差矩阵表征相关性在存在混淆效应时的误导问题。

    

    基于协方差矩阵的视觉表示已经证明了其在图像分类中的有效性，通过对卷积特征映射中不同通道之间的成对相关性进行建模。然而，如果存在另一个通道与感兴趣的两个通道相关，则成对相关性将变得误导人，导致“混淆”效应。针对这种情况，应该估计“偏相关”，以消除混淆效应。然而，可靠地估计偏相关需要解决一个对称正定矩阵优化问题，即稀疏逆协方差矩阵估计（SICE）。如何将此过程融入CNN中仍然是一个开放问题。在这项工作中，我们将SICE制定为CNN的一个新结构层。为确保端到端的可训练性，我们开发了一种迭代方法，在前向和后向传播步骤中解决上述矩阵优化问题。我们的工作获得了基于偏相关的深度视觉表示。

    Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representa
    
[^61]: 通过提示提高大型语言模型的心智理论表现

    Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])

    [http://arxiv.org/abs/2304.11490](http://arxiv.org/abs/2304.11490)

    本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。

    

    2023年，大型语言模型（LLMs）在许多任务中表现出色，但在复杂推理方面仍面临挑战。心智理论（ToM）任务需要理解代理人的信念、目标和心理状态，对于涉及人类的常识推理至关重要，因此提高LLM在这方面的表现至关重要。本研究测量了GPT-4和三个GPT-3.5变体（Davinci-2、Davinci-3、GPT-3.5-Turbo）的ToM表现，并研究了上下文学习提高它们的ToM理解力的有效性。我们评估了包含两步思维推理和逐步思考说明的提示。我们发现，通过人类反馈的强化学习（RLHF）训练的LLMs（除Davinci-2外的所有模型）通过上下文学习提高了它们的ToM准确性。GPT-4在零轮情况下表现最佳，达到了近80%的ToM准确性，但仍不足测试集上87%的人类准确性。然而，当提供上下文学习的提示时，GPT-4和三个GPT-3.5变体的ToM准确性显著高于无提示时，其中表现最好的模型（GPT-3.5-Turbo）达到了92%的准确性。我们的研究展示了上下文学习提升LLM在复杂推理尤其是ToM任务中表现的潜力。

    Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
    
[^62]: 树状Parzen估计器：理解其算法组成部分及其在提高实证表现中的作用

    Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])

    [http://arxiv.org/abs/2304.11127](http://arxiv.org/abs/2304.11127)

    该论文介绍了一种广泛使用的贝叶斯优化方法 Tree-structured Parzen estimator (TPE)，并对其控制参数的作用和算法直觉进行了讨论和分析，提供了一组推荐设置并证明其能够提高TPE的性能表现。

    

    许多领域中最近的进展要求更加复杂的实验设计。这种复杂的实验通常有许多参数，需要参数调整。Tree-structured Parzen estimator (TPE) 是一种贝叶斯优化方法，在最近的参数调整框架中被广泛使用。尽管它很受欢迎，但控制参数的角色和算法直觉尚未得到讨论。在本教程中，我们将确定每个控制参数的作用以及它们对超参数优化的影响，使用多种基准测试。我们将从剖析研究中得出的推荐设置与基准方法进行比较，并证明我们的推荐设置提高了TPE的性能。我们的TPE实现可在https://github.com/nabenabe0928/tpe/tree/single-opt中获得。

    Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
    
[^63]: 医学图像分析中的任意分割模型：一项实验研究

    Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])

    [http://arxiv.org/abs/2304.10517](http://arxiv.org/abs/2304.10517)

    本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。

    

    由于数据注释的有限可用性和获取成本，训练医学图像分割模型仍然具有挑战性。Segment Anything Model（SAM）是一种基础模型，经过超过10亿个注释的训练，主要用于自然图像，旨在能够以交互方式分割用户定义的感兴趣的对象。尽管SAM在自然图像上表现出色，但不清楚该模型在转换到医学图像领域时会受到多大影响。在这里，我们对SAM在各种模态和解剖学的11个医学图像数据集上进行了广泛的评估。在我们的实验中，我们使用标准方法生成点提示来模拟交互分割。实验结果表明，SAM基于单点提示的表现在任务和数据集方面高度变化，即从脊柱MRI数据集的0.1135到髋关节X射线数据集的0.8650。

    Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
    
[^64]: 视频动作识别中连续学习方法的基线研究

    A baseline on continual learning methods for video action recognition. (arXiv:2304.10335v1 [cs.CV])

    [http://arxiv.org/abs/2304.10335](http://arxiv.org/abs/2304.10335)

    本文基于视频动作识别场景，提出最先进的连续学习方法的基准研究，并证明回顾方法优于其他方法。此外，提出的内存效率变体可有效地保持一定水平的性能。

    

    近年来，连续学习吸引了研究界的关注，因为它旨在解决经典监督模型的长期限制。然而，大多数关于这个主题的研究都是针对简单的图像分类场景进行的。在本文中，我们提出了关于视频动作识别状态下最先进的连续学习方法的基准研究。除了由于时间维度而增加的复杂性外，在视频环境中，为了实现最佳性能的回顾方法对计算资源要求更高。为了对抗增加的内存要求，我们提出了两种去方法都通用的回顾方法变体，利用模型置信度或数据信息的指标来选择可记忆的样本。我们的实验表明，与文献中预期的一样，回顾方法优于其他方法；此外，所提出的内存效率变体被证明在保持一定水平的性能方面是有效的。

    Continual learning has recently attracted attention from the research community, as it aims to solve long-standing limitations of classic supervisedly-trained models. However, most research on this subject has tackled continual learning in simple image classification scenarios. In this paper, we present a benchmark of state-of-the-art continual learning methods on video action recognition. Besides the increased complexity due to the temporal dimension, the video setting imposes stronger requirements on computing resources for top-performing rehearsal methods. To counteract the increased memory requirements, we present two method-agnostic variants for rehearsal methods, exploiting measures of either model confidence or data information to select memorable samples. Our experiments show that, as expected from the literature, rehearsal methods outperform other approaches; moreover, the proposed memory-efficient variants are shown to be effective at retaining a certain level of performance 
    
[^65]: LLM作为机器人的大脑：统一自我中心记忆与控制

    LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])

    [http://arxiv.org/abs/2304.09349](http://arxiv.org/abs/2304.09349)

    本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。

    

    体感人工智能研究和开发具备物理或虚拟实体（即机器人）并能够与环境动态交互的智能系统。记忆和控制是体感系统的两个基本部分，通常需要分别使用框架进行建模。本文提出了一个新的、可推广的框架，称为LLM-Brain：使用大规模语言模型作为机器人大脑，统一自我中心记忆和控制。LLM-Brain框架集成了多个多模态语言模型用于机器人任务，利用零-shot学习方法。LLM-Brain中的所有组件使用自然语言进行封闭式多轮对话，包括感知、规划、控制和记忆。系统的核心是一个具备自我中心记忆和控制机器人的实体LLM。我们通过研究两个下游任务：主动探索和实体问答来演示LLM-Brain。

    Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
    
[^66]: 可解释的基于Transformer的LiDAR-惯性融合里程计估计

    TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation. (arXiv:2304.07728v1 [cs.RO])

    [http://arxiv.org/abs/2304.07728](http://arxiv.org/abs/2304.07728)

    本文提出了一种可解释的基于Transformer的LiDAR-惯性融合里程计估计方法，通过多头注意力融合模块展示了不同融合方法，实验表明该方法在KITTI和EuRoC数据集上表现优于现有方法。

    

    增强里程计估计性能的常用方法是多模态传感器融合，这也是移动机器人的基本模块。然而，在监督式传感器融合里程计估计任务中，如何在不同模态之间执行融合仍然是具有挑战性的问题。一些简单的操作，如逐元素求和和连接，并不具备分配自适应关注权重以有效合并不同模态的能力，这使得获得具有竞争力的里程计结果变得困难。最近，Transformer架构显示出在视觉与语言领域的多模态融合任务中具有潜力。在本文中，我们提出了一种基于Transformer的端到端的激光雷达-惯性融合框架(即TransFusionOdom)来进行里程计估计。多头注意力融合模块展示了同构和异构模态的不同融合方法。所提出的TransFusionOdom模型在KITTI和EuRoC数据集上显示出比现有方法更好的性能。

    Multi-modal fusion of sensors is a commonly used approach to enhance the performance of odometry estimation, which is also a fundamental module for mobile robots. However, the question of \textit{how to perform fusion among different modalities in a supervised sensor fusion odometry estimation task?} is still one of challenging issues remains. Some simple operations, such as element-wise summation and concatenation, are not capable of assigning adaptive attentional weights to incorporate different modalities efficiently, which make it difficult to achieve competitive odometry results. Recently, the Transformer architecture has shown potential for multi-modal fusion tasks, particularly in the domains of vision with language. In this work, we propose an end-to-end supervised Transformer-based LiDAR-Inertial fusion framework (namely TransFusionOdom) for odometry estimation. The multi-attention fusion module demonstrates different fusion approaches for homogeneous and heterogeneous modalit
    
[^67]: 第二届单目深度估计挑战赛

    The Second Monocular Depth Estimation Challenge. (arXiv:2304.07051v1 [cs.CV])

    [http://arxiv.org/abs/2304.07051](http://arxiv.org/abs/2304.07051)

    本文介绍了单目深度估计挑战赛的第二届比赛结果，高质量的SYNS-Patches数据集提高了比赛难度，所有提交作品都超过了基准水平。

    

    本文介绍了单目深度估计挑战赛（MDEC）的第二届比赛结果。本次比赛接受任何形式方式的监督，包括全监督、自监督、多任务或代理深度。比赛的数据集基于SYNS-Patches，其中包含高质量的密集真实数据，并具有广泛的环境多样性。比赛收到了8个独特的提交作品，所有基于点云或基于图像的指标表现都超过了基准水平。最佳监督提交作品的相对F-分数提高了27.62％，而最佳自监督提交作品提高了16.61％，这些结果代表了真正的进步。

    This paper discusses the results for the second edition of the Monocular Depth Estimation Challenge (MDEC). This edition was open to methods using any form of supervision, including fully-supervised, self-supervised, multi-task or proxy depth. The challenge was based around the SYNS-Patches dataset, which features a wide diversity of environments with high-quality dense ground-truth. This includes complex natural environments, e.g. forests or fields, which are greatly underrepresented in current benchmarks.  The challenge received eight unique submissions that outperformed the provided SotA baseline on any of the pointcloud- or image-based metrics. The top supervised submission improved relative F-Score by 27.62%, while the top self-supervised improved it by 16.61%. Supervised submissions generally leveraged large collections of datasets to improve data diversity. Self-supervised submissions instead updated the network architecture and pretrained backbones. These results represent a si
    
[^68]: 监管市场：人工智能治理的未来

    Regulatory Markets: The Future of AI Governance. (arXiv:2304.04914v1 [cs.AI])

    [http://arxiv.org/abs/2304.04914](http://arxiv.org/abs/2304.04914)

    提出一种监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务，以克服过度依赖行业自律和立法机构缺乏专业知识的局限性，从而逐步实现人工智能的恰当监管。

    

    恰当地监管人工智能是一个日益紧迫的政策挑战。立法机构和监管机构缺乏翻译公众需求为法律要求所需的专业知识。过度依赖行业自律未能使AI系统的生产者和使用者对民主要求负责。提出了监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务。这种方法可以克服命令和控制监管和自我监管的局限性。监管市场可以使政府为AI监管建立政策优先级，同时依靠市场力量和行业研发努力来开创最能实现政策制定者声明目标的监管方法。

    Appropriately regulating artificial intelligence is an increasingly urgent policy challenge. Legislatures and regulators lack the specialized knowledge required to best translate public demands into legal requirements. Overreliance on industry self-regulation fails to hold producers and users of AI systems accountable to democratic demands. Regulatory markets, in which governments require the targets of regulation to purchase regulatory services from a private regulator, are proposed. This approach to AI regulation could overcome the limitations of both command-and-control regulation and self-regulation. Regulatory market could enable governments to establish policy priorities for the regulation of AI, whilst relying on market forces and industry R&D efforts to pioneer the methods of regulation that best achieve policymakers' stated objectives.
    
[^69]: DiffMimic: 基于可微分物理的高效运动模仿

    DiffMimic: Efficient Motion Mimicking with Differentiable Physics. (arXiv:2304.03274v1 [cs.CV])

    [http://arxiv.org/abs/2304.03274](http://arxiv.org/abs/2304.03274)

    本文提出了DiffMimic，一种基于可微分物理的高效运动模仿方法。与传统强化学习方法相比，其有更快更稳定的收敛速度；同时通过演示重播机制避免陷入局部最优解。

    

    运动模仿是基于物理的角色动画中的基础任务，然而大多数现有的运动模仿方法都建立在强化学习（RL）之上，存在重度奖励工程、高方差和难以探索的收敛速度缓慢等问题。本文提出了一种基于可微分物理模拟器（DPS）的运动模仿方法，名为DiffMimic，通过分析梯度和基于真实物理先验学习稳定策略，从而实现显著更快和更稳定的收敛。此外，为了避免陷入局部最优解，我们还利用演示重播机制，在长时间跨度内实现稳定梯度反向传播。

    Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed DiffMimic. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize a Demonstration Replay mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments o
    
[^70]: 利用预训练的边缘Transformer在在线游戏中进行好友排名

    Friend Ranking in Online Games via Pre-training Edge Transformers. (arXiv:2302.10043v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.10043](http://arxiv.org/abs/2302.10043)

    本文提出了一种使用边缘Transformer和预训练的链接预测方法，用于在在线游戏中进行好友排名，达到了最先进的结果。

    

    在线游戏中，好友回忆是提高每日活跃用户数量的重要途径。本文将好友回忆问题视为链接预测问题，并探讨了可以使用（活跃的和失落的）玩家特征以及历史事件的几种链接预测方法。此外，我们提出了一种新颖的边缘Transformer模型，并通过掩码自动编码器进行预训练。我们的方法在三款腾讯游戏的离线实验和在线A/B测试中取得了最先进的结果。

    Friend recall is an important way to improve Daily Active Users (DAU) in online games. The problem is to generate a proper lost friend ranking list essentially. Traditional friend recall methods focus on rules like friend intimacy or training a classifier for predicting lost players' return probability, but ignore feature information of (active) players and historical friend recall events. In this work, we treat friend recall as a link prediction problem and explore several link prediction methods which can use features of both active and lost players, as well as historical events. Furthermore, we propose a novel Edge Transformer model and pre-train the model via masked auto-encoders. Our method achieves state-of-the-art results in the offline experiments and online A/B Tests of three Tencent games.
    
[^71]: 数据高效的对比自监督学习：易于学习的样本起到最大的作用。

    Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09195](http://arxiv.org/abs/2302.09195)

    该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。

    

    自监督学习（SSL）从大量的无标签训练数据中学习高质量的表示。随着数据集变得越来越大，识别对学习此类表示最有用的示例变得至关重要。这可以通过减少学习高质量表示所需的数据量来实现有效的SSL。然而，对于SSL的价值如何量化一直是一个悬而未决的问题。在本文中，我们首次解决了这个问题，证明在期望意义下，对比SSL中对学习做出最大贡献的示例是具有最相似数据增强的示例。我们对这些子集的SSL的广义性能提供了严格的保证。实验证明，令人惊讶的是，对SSL做出最大贡献的子集是对监督学习做出最小贡献的子集。通过广泛的实验，我们证明了我们的子集在CIFAR100、CIFAR中的表现优于随机子集3%以上。

    Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
    
[^72]: 域索引变分贝叶斯：可解释的域索引用于域自适应

    Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02561](http://arxiv.org/abs/2302.02561)

    该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。

    

    先前的研究表明，利用域索引可以显著提高域自适应性能。然而，并非总是有这样的域索引可用。为解决这一挑战，我们首先从概率角度提供了域索引的正式定义，然后提出了一个对抗性变分贝叶斯框架，从多域数据中推断出域索引，从而提供额外的域关系洞察，并提高域自适应性能。我们的理论分析表明，我们的对抗性变分贝叶斯框架在平衡点处找到了最优的域索引。对合成和真实数据的实证结果验证了我们的模型可以产生可解释的域索引，使我们可以实现优于现有域适应方法的性能。代码可在https://github.com/Wang-ML-Lab/VDI获得。

    Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
    
[^73]: 将知识纳入文档摘要生成中：基于GPT-2的前缀调整应用

    Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11719](http://arxiv.org/abs/2301.11719)

    本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。

    

    尽管现在文档摘要技术得到了很大的发展，但是生成的摘要和原始文本之间的事实不一致仍然时有发生。本研究探索了采用提示来将事实知识纳入生成的摘要的可能性。我们具体研究了前缀调整，它使用一组可训练的连续前缀提示和离散自然语言提示来帮助摘要生成。实验结果表明，可训练的前缀可以帮助摘要模型准确地从离散提示中提取信息，从而生成保留知识的摘要，这些摘要在事实上与离散提示一致。生成的摘要的ROUGE改进表明，将事实知识明确地添加到摘要生成过程中可以提升整体性能，显示出在其他自然语言处理任务中应用的巨大潜力。

    Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
    
[^74]: BTS：基于半监督学习的室内两房间存在检测中的双折叠师生网络

    BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10802](http://arxiv.org/abs/2212.10802)

    本文提出了一种基于半监督学习的双折叠师生网络，该网络通过利用部分标记和未标记的数据集智能地学习空间和时间特征，有效地解决了基于CSI的室内存在检测受到环境变化和有监督学习方法需要耗时标注的问题。

    

    近年来，基于有监督学习和信道状态信息（CSI）的室内人体存在检测引起了广泛的关注。然而，现有的研究依赖于CSI的空间信息，容易受到环境变化的影响，如物体移动、大气因素和机器重启，从而降低了预测精度。此外，基于有监督学习的方法需要进行耗时的标注来重新训练模型。因此，使用半监督学习方案设计一个连续监控的模型生命周期是必要的。在本文中，我们构思了一种双折叠师生（BTS）学习方法来检测存在于系统中的存在。该方法结合了半监督学习，利用部分标记和未标记的数据集。所提出的原始对偶师生网络从标记和未标记的CSI中智能地学习空间和时间特征。此外，增强的惩罚损失函数利用熵和距离测量来区分深层特征，降低噪声的影响。

    In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
    
[^75]: Prompting就是编程: 一种大语言模型的查询语言

    Prompting Is Programming: A Query Language for Large Language Models. (arXiv:2212.06094v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06094](http://arxiv.org/abs/2212.06094)

    LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合，实现了一种新的语言模型编程方式。

    

    大型语言模型在问答和代码生成等各种任务上展现出了优异的表现。从高层次上讲，给定输入，语言模型可以用统计上的可能性自动完成序列。基于此，用户通过语言指令或示例来提示这些模型，以执行各种下游任务。高级提示方法甚至可以暗示模型、用户和计算器等外部工具之间的交互。然而，为了获得最先进的性能或将语言模型适应特定任务，必须实现复杂的任务-和模型特定的程序，这仍然可能需要特定的交互。基于此，我们提出了语言模型编程（LMP）的新概念。LMP将语言模型提示从纯文本提示扩展为文本提示和脚本的直观组合。此外，LMP允许指定语言模型的约束条件。

    Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the languag
    
[^76]: VISEM-Tracking，一份人类精子跟踪数据集

    VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02842](http://arxiv.org/abs/2212.02842)

    本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。

    

    精子运动的手动评估需要显微镜观察，由于所观察的精子在视野中的快速移动，这是具有挑战性的。为了获得正确的结果，手动评估需要进行广泛的培训。因此，在诊所中，计算机辅助精子分析（CASA）变得越来越常用。尽管如此，需要更多数据来训练监督式机器学习方法，以提高在评估精子运动和运动学方面的精度和可靠性。在这方面，我们提供了一个名为VISEM-Tracking的数据集，其中包含20个30秒的视频记录（包括29,196帧）的湿性精子制备物，具备手动注释的包围框坐标和由该领域的专家分析的一组精子特征。除了已注释的数据，我们还提供了未标记的视频剪辑，以便通过自监督或无监督学习等方法轻松访问和分析数据。作为本文的一部分，我们提出了基线精子检测性能。

    A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
    
[^77]: 神经傅里叶滤波器组

    Neural Fourier Filter Bank. (arXiv:2212.01735v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.01735](http://arxiv.org/abs/2212.01735)

    该论文提出了一个新的神经傅里叶滤波器组方法，该方法既在空间上又在频率上分解信号，通过使用傅里叶编码特定频率来存储每个网格，这一方法在2D图像拟合、3D形状重建和神经辐射场等多个任务上，表现出优于现有技术的模型紧凑性和收敛速度。

    

    我们提出了一种新的方法，以提供高效且高度详细的重构。受小波启发，我们学习了一个神经场，既在空间上又在频率上分解信号。我们遵循最近的基于网格的空间分解范例，但与现有的工作不同，通过傅里叶特征编码鼓励在每个网格中存储特定的频率。然后，我们应用带正弦激活函数的多层感知器，以在适当的层次上接受这些傅里叶编码的特征，以使高频组件依次累积在低频组件之上，最后将它们相加以形成最终输出。我们证明了我们的方法在多个任务上（2D图像拟合，3D形状重建和神经辐射场）优于现有技术，包括模型紧凑性和收敛速度。我们的代码可以在https://github.com/ubc-vision/NFFB获得。

    We present a novel method to provide efficient and highly detailed reconstructions. Inspired by wavelets, we learn a neural field that decompose the signal both spatially and frequency-wise. We follow the recent grid-based paradigm for spatial decomposition, but unlike existing work, encourage specific frequencies to be stored in each grid via Fourier features encodings. We then apply a multi-layer perceptron with sine activations, taking these Fourier encoded features in at appropriate layers so that higher-frequency components are accumulated on top of lower-frequency components sequentially, which we sum up to form the final output. We demonstrate that our method outperforms the state of the art regarding model compactness and convergence speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
    
[^78]: CrossSplit: 通过数据分割缓解标签噪声记忆问题

    CrossSplit: Mitigating Label Noise Memorization through Data Splitting. (arXiv:2212.01674v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.01674](http://arxiv.org/abs/2212.01674)

    本文提出了一种名为CrossSplit的新训练程序，通过使用交叉分割的标签修正和半监督训练两个主要组成部分，缓解了深度学习算法中标签噪声记忆问题，具有良好的效果。

    

    我们的研究旨在解决标签噪声存在情况下深度学习算法鲁棒性不足的问题。基于现有的标签修正和共同教学方法，我们提出了一种新的训练程序——CrossSplit，以缓解噪声标签的记忆问题。CrossSplit使用在两个标记数据集的不相交部分上训练的一对神经网络。该方法组合了两个主要组成部分：(i)交叉分割标签修正：由于在数据集的一部分上训练的模型不能记忆来自其他部分的示例-标签对，因此可以使用对等网络的预测平滑调整每个网络呈现的训练标签；(ii)交叉分割半监督训练：在一个部分的数据上训练的网络也使用另一个部分的未标记输入。在CIFAR-10、CIFAR-100、Tiny-ImageNet和mini-WebVision数据集上的大量实验表明，我们的方法可以在广泛的噪声和干扰下比当前最先进的方法更好地完成任务。

    We approach the problem of improving robustness of deep learning algorithms in the presence of label noise. Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit, which uses a pair of neural networks trained on two disjoint parts of the labelled dataset. CrossSplit combines two main ingredients: (i) Cross-split label correction. The idea is that, since the model trained on one part of the data cannot memorize example-label pairs from the other part, the training labels presented to each network can be smoothly adjusted by using the predictions of its peer network; (ii) Cross-split semi-supervised training. A network trained on one part of the data also uses the unlabeled inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of no
    
[^79]: 利用鉴别器引导在基于评分的扩散模型中完善生成过程

    Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models. (arXiv:2211.17091v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.17091](http://arxiv.org/abs/2211.17091)

    本文提出了“鉴别器引导”方法，通过在评分训练之后训练鉴别器，使模型评估更加准确，从而改善预训练扩散模型的样本生成。在 ImageNet 256x256 数据集上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID 和召回率。

    

    本文提出的“鉴别器引导”方法旨在改善预训练扩散模型的样本生成。该方法引入了一个鉴别器，明确地监督去噪样本路径是否真实。与 GAN 不同的是，我们的方法不需要联合训练评分和鉴别器网络。相反，在评分训练之后训练鉴别器，使鉴别器训练稳定且快速收敛。在样本生成中，我们向预训练的评分添加一个辅助项以欺骗鉴别器。该项将模型评分矫正为最优鉴别器处的数据评分，这意味着鉴别器以补充的方式帮助更好地评估分数。使用我们的算法，在 ImageNet 256x256 上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID（1.68）和召回率（0.66）。我们在 https://github.com/alsdudrla10/DG 上公开了代码。

    The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.
    
[^80]: 安全潜向扩散：缓解扩散模型中不当退化

    Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. (arXiv:2211.05105v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05105](http://arxiv.org/abs/2211.05105)

    该论文提出了一种名为安全潜向扩散的方法，可以在图像生成过程中移除和抑制不当的图像部分，从而缓解基于文本的图像生成模型因不当数据集带来的不良影响。

    

    最近，基于文本的图像生成模型在图像质量和文本对齐方面取得了惊人的成果，并因此被广泛应用于越来越多的应用程序。由于它们高度依赖于随机从互联网上抓取的数十亿大小的数据集，因此它们也面临来自退化和偏见的人类行为的不良影响，正如我们所展示的那样。反过来，它们甚至可能强化这些偏见。为了帮助应对这些不良影响，我们提出了安全潜向扩散（SLD）。具体而言，为了衡量由于未过滤和不平衡的训练集而引起的不当退化，我们建立了一个新颖的图像生成测试平台——包含专门的、覆盖裸露和暴力等概念的实际图像到文本提示的不当图像提示（I2P）。正如我们详尽的实证评估所证明的那样，引入的SLD在扩散过程中移除和抑制了不当的图像部分，无需额外的训练，并且对图像质量没有不良影响。

    Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse e
    
[^81]: 学习图像表示以进行异常检测：在药物开发中发现组织学改变的应用

    Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07675](http://arxiv.org/abs/2210.07675)

    该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。

    

    我们提出了一种用于组织病理学图像异常检测的系统。在组织学中，正常样本通常是大量存在的，而异常（病理）情况通常很少或不可用。在这种情况下，使用在健康数据上训练的单类分类器可以检测到分布外的异常样本。这样的方法与预训练的卷积神经网络（CNN）图像表示相结合，以前已经用于异常检测（AD）。但是，预训练的现成CNN表示可能对组织中的异常情况不敏感，而健康组织的自然变异可能导致远离的表示。为了使表示适应健康组织中的相关细节，我们建议在辅助任务上训练CNN，该任务区分不同物种、器官和染色试剂的健康组织。几乎不需要额外的标注工作量，因为健康样本可以自动获得上述标签。在训练中，我们强制执行

    We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
    
[^82]: 基于增强型驾驶员行为模型的高保真碰撞检测算法仿真研究

    Augmented Driver Behavior Models for High-Fidelity Simulation Study of Crash Detection Algorithms. (arXiv:2208.05540v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2208.05540](http://arxiv.org/abs/2208.05540)

    本文提出了一种基于增强型驾驶员行为模型的混合运输系统仿真平台，模拟驾驶员与自动驾驶车辆的交互，在高保真的仿真研究中可用于评估碰撞检测算法的性能。

    

    开发连接和自动驾驶车辆（CAVs）的安全性和有效性应用程序需要大量的测试和评估。在关键和危险的情况下运行这些系统的需求使得评估成本非常昂贵、可能存在危险，并且耗时较长。作为替代方案，研究人员试图使用仿真平台来研究和评估其算法和设计。对于与CAVs或其他车辆互动的驾驶员或人类操作者的行为建模是这种模拟的主要挑战之一。虽然为人类行为开发完美模型是一项具有挑战性的任务和一个开放问题，但我们在现有模拟器用于驾驶员行为的模型的基础上进行了显著的扩展。在本文中，我们提出了一个混合运输系统的仿真平台，包括人驾驶和自动驾驶车辆。此外，我们将人类驾驶任务进行分解，并提供了一种模块化的仿真方法。

    Developing safety and efficiency applications for Connected and Automated Vehicles (CAVs) require a great deal of testing and evaluation. The need for the operation of these systems in critical and dangerous situations makes the burden of their evaluation very costly, possibly dangerous, and time-consuming. As an alternative, researchers attempt to study and evaluate their algorithms and designs using simulation platforms. Modeling the behavior of drivers or human operators in CAVs or other vehicles interacting with them is one of the main challenges of such simulations. While developing a perfect model for human behavior is a challenging task and an open problem, we present a significant augmentation of the current models used in simulators for driver behavior. In this paper, we present a simulation platform for a hybrid transportation system that includes both human-driven and automated vehicles. In addition, we decompose the human driving task and offer a modular approach to simulat
    
[^83]: 一对其余损失函数在对抗训练中聚焦重要样本的作用

    One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training. (arXiv:2207.10283v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10283](http://arxiv.org/abs/2207.10283)

    本文提出了一种名为SOVR的对抗训练损失函数，可以聚焦重要样本，增加对抗攻击下的对数几率间隔，从而在实验中表现出对抗攻击的有效性。

    

    本文提出了一种新的对抗训练损失函数。由于对抗训练存在困难，如需要高模型容量，通过加权交叉熵损失关注重要数据点已引起广泛关注。然而，它们容易受到复杂攻击的影响，如Auto-Attack。本文实验表明，它们的易受攻击的原因是真实标签和其他标签之间的对数几率之间的较小间隔。由于神经网络是根据对数几率对数据点进行分类的，所以对数几率的间隔应该足够大，以避免攻击翻转最大的对数几率。重要性感知方法不会增加重要样本的对数几率间隔，但与交叉熵损失相比会减少较不重要样本的对数几率间隔。为了增加重要样本的对数几率间隔，我们提出了一种切换一对其余（SOVR）损失函数，该损失函数在具有较小对数几率间隔的重要样本中从交叉熵切换到一对其余损失。我们提供理论分析、消融研究和实验，证明SOVR对抗抗击和其他最先进的攻击方法的有效性。

    This paper proposes a new loss function for adversarial training. Since adversarial training has difficulties, e.g., necessity of high model capacity, focusing on important data points by weighting cross-entropy loss has attracted much attention. However, they are vulnerable to sophisticated attacks, e.g., Auto-Attack. This paper experimentally reveals that the cause of their vulnerability is their small margins between logits for the true label and the other labels. Since neural networks classify the data points based on the logits, logit margins should be large enough to avoid flipping the largest logit by the attacks. Importance-aware methods do not increase logit margins of important samples but decrease those of less-important samples compared with cross-entropy loss. To increase logit margins of important samples, we propose switching one-vs-the-rest loss (SOVR), which switches from cross-entropy to one-vs-the-rest loss for important samples that have small logit margins. We prov
    
[^84]: 使用像素的语言建模

    Language Modelling with Pixels. (arXiv:2207.06991v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.06991](http://arxiv.org/abs/2207.06991)

    本文介绍了一个名为PIXEL的基于像素的预训练语言模型，它可以将文本渲染为图像，解决了扩展支持的语言数量时出现的词汇瓶颈问题，且在形态学和语义任务上显著优于BERT。

    

    语言模型在有限的输入集合上进行定义，这导致在尝试扩展支持的语言数量时出现词汇瓶颈。解决这个瓶颈会导致在嵌入矩阵中所能表示的内容与输出层的计算问题之间的权衡。本文引入了名为PIXEL的基于像素的语言编码器。PIXEL是预训练的语言模型，可以将文本渲染为图像，从而可以基于拼写相似性或像素的共同激活来跨语言传递表示。PIXEL训练时不是预测标记分布，而是重构被屏蔽的块的像素。我们对与BART相同的英语数据进行了86M参数PIXEL模型的预训练，并在形态学和语义任务上进行了评估，涵盖了不同类型的语言，包括各种非拉丁文字。我们发现，在语法和语义处理方面，PIXEL的性能显著优于BERT。

    Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing
    
[^85]: 抽象解释下的Fixpoint迭代器及其在神经网络中的应用

    Abstract Interpretation of Fixpoint Iterators with Applications to Neural Networks. (arXiv:2110.08260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08260](http://arxiv.org/abs/2110.08260)

    该论文提出了一种新的抽象解释框架，使我们能够精确地过度逼近数值Fixpoint迭代器。使用一种新的抽象域，CH-Zonotope，可以实现有效的传播和包含检查。该框架在研究monDEQ神经网络结构时表现出色，是一种有前途的验证技术。

    

    我们提出了一个新的抽象解释框架，以精确地过度逼近数值Fixpoint迭代器。我们的关键观察是，在这种情况下，与通常用于逼近所有可达程序状态的标准抽象解释（AI）不同，人们只需要抽象具体的Fixpoints，即最终的程序状态。我们的框架针对具体收敛性和唯一性保证的数值Fixpoint迭代器，基于两个主要的技术贡献：（i）理论上的洞察力，允许我们计算出不使用交汇点的声音和精确的Fixpoint抽象，（ii）一种新的抽象域，CH- Zonotope，在保持高精度的同时，允许有效的传播和包含检查。我们在一个名为CRAFT的工具中实现了我们的框架，并在一个特别具有挑战性的基于Fixpoint的神经网络架构（monDEQ）上进行了评估。我们的广泛评估表明，CRAFT超出了最先进的工具。

    We present a new abstract interpretation framework for the precise over-approximation of numerical fixpoint iterators. Our key observation is that unlike in standard abstract interpretation (AI), typically used to over-approximate all reachable program states, in this setting, one only needs to abstract the concrete fixpoints, i.e., the final program states. Our framework targets numerical fixpoint iterators with convergence and uniqueness guarantees in the concrete and is based on two major technical contributions: (i) theoretical insights which allow us to compute sound and precise fixpoint abstractions without using joins, and (ii) a new abstract domain, CH-Zonotope, which admits efficient propagation and inclusion checks while retaining high precision. We implement our framework in a tool called CRAFT and evaluate it on a novel fixpoint-based neural network architecture (monDEQ) that is particularly challenging to verify. Our extensive evaluation demonstrates that CRAFT exceeds the
    

