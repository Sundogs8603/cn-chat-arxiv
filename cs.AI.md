# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore.](http://arxiv.org/abs/2308.04430) | SILO是一种新的语言模型，通过在推理过程中对非参数化的数据存储进行查询，实现在面临法律风险和模型性能之间的权衡，并支持数据归属和数据生产者退出模型的功能。 |
| [^2] | [Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach.](http://arxiv.org/abs/2308.04419) | 本论文介绍了一种基于混合LSTM和顺序自注意力的方法，旨在预测股票未来价格，并通过实验证明了该方法的有效性。 |
| [^3] | [Probabilistic Invariant Learning with Randomized Linear Classifiers.](http://arxiv.org/abs/2308.04412) | 本文介绍了一种使用随机线性分类器进行概率不变学习的方法，通过接受概率化的普遍逼近和不变性，设计了能同时具有表达能力和不变性的模型，并且使用更少的资源。通过实验证明了这种方法在分类任务中的有效性。 |
| [^4] | [Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models.](http://arxiv.org/abs/2308.04399) | 本文提出了一个模型，探讨了通用模型的微调过程中的利润分享问题，为一般类的成本和收入函数描述了解决方案的条件。 |
| [^5] | [Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining.](http://arxiv.org/abs/2308.04396) | 本论文提出了一种定制的企业协作系统的事件抽象方法，通过比较实际用户活动和系统生成的低级别跟踪来训练模型，并将低级别跟踪转换为抽象的高级别日志，以支持社会流程挖掘。 |
| [^6] | [Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making.](http://arxiv.org/abs/2308.04375) | 本研究利用因果解释方法帮助人类分析审查人工智能建议，减少对人工智能的过度依赖，提高了临床决策的质量和表现。 |
| [^7] | [Some Options for Instantiation of Bipolar Argument Graphs with Deductive Arguments.](http://arxiv.org/abs/2308.04372) | 本文讨论了双极论证图的实例化，通过使用逻辑论证实例化双极论证图，并考虑到论证的内部结构和论证之间的关系类型。 |
| [^8] | [Cumulative Reasoning With Large Language Models.](http://arxiv.org/abs/2308.04371) | 本文提出了一种名为累积推理（CR）的新方法，利用语言模型以累积和迭代的方式模拟人类思维过程，通过将任务分解为较小的组件，简化问题解决过程，取得了优于现有方法的性能，并在逻辑推理和24点游戏中实现了显著提升。 |
| [^9] | [Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs.](http://arxiv.org/abs/2308.04356) | 本文通过使用普通膝关节X光片重新审视使用深度学习进行膝关节骨解剖结构分割的方法，揭示出可见的性别和种族偏见，并提出了缓解策略以确保公正和无偏见的分割结果。同时，该研究促进了多样化患者人群获得准确诊断和治疗结果的平等机会，推动了公平和包容的医疗保障。 |
| [^10] | [Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra.](http://arxiv.org/abs/2308.04337) | 本研究开发了一个基于图像分类的模型，用于检测珊瑚礁的损伤。该模型利用卷积神经网络识别和区分健康珊瑚和白化珊瑚的视觉模式。 |
| [^11] | [Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs.](http://arxiv.org/abs/2308.04314) | 本文提出了一种新的合作赌博机算法，实现了最佳的个体遗憾和恒定的通信成本。 |
| [^12] | [Apple Vision Pro for Healthcare: "The Ultimate Display"?.](http://arxiv.org/abs/2308.04313) | 苹果推出了Vision Pro，一款具有混合现实和增强现实功能的虚拟现实设备，拥有独特的特点，例如内部屏幕展示佩戴者的眼睛以及数字皇冠按钮的融合功能。这款无线设备可能实现了“终极显示器”的潜力。 |
| [^13] | [Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios.](http://arxiv.org/abs/2308.04312) | 本论文提出了一个可解释的目标导向模型，通过结合离散选择模型的可解释性和基于神经网络的模型的高准确性，在交互环境中预测车辆轨迹。使用INTERACTION数据集验证了该模型的有效性。 |
| [^14] | [Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps.](http://arxiv.org/abs/2308.04303) | 本文研究了车辆运动预测的挑战，提出了一种使用先验信息和语义辅助占据网格地图的框架来预测车辆行为。实验证明，该模型在预测静态和动态车辆方面表现出优越性能，并评估了语义标签和地图在模型中的作用。 |
| [^15] | [Actor-Critic with variable time discretization via sustained actions.](http://arxiv.org/abs/2308.04299) | 这篇论文提出了一种称为SusACER的演员-评论者算法，可以通过使用持续动作，在变化的时间离散化过程中取得更好的机器人控制性能。 |
| [^16] | [Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding.](http://arxiv.org/abs/2308.04292) | 本文通过改进LaCAM*算法，解决了实时、大规模、接近最优的多智能体路径规划的挑战。改进技术的融合显著提高了LaCAM*的解决方案质量，推动了MAPF算法的边界。 |
| [^17] | [In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning.](http://arxiv.org/abs/2308.04275) | 本文提出了一种在微调之前与纯净语言模型进行对话的方法，通过上下文学习实现了推理时的对齐。实验证明，这种方法将纯净语言模型的胜率提高了7倍，使其可以与通过对齐微调的强基准模型媲美。 |
| [^18] | [Lossy and Lossless (L$^2$) Post-training Model Size Compression.](http://arxiv.org/abs/2308.04269) | 提出了一种将有损和无损压缩结合的训练后模型尺寸压缩方法，通过统一的参数化权重变换和引入可微分计数器来实现高效高压缩比的模型压缩。 |
| [^19] | [Teacher-Student Architecture for Knowledge Distillation: A Survey.](http://arxiv.org/abs/2308.04268) | 本文综述了知识蒸馏的师生架构，在多个蒸馏目标领域中取得了有效并广泛的应用，包括知识压缩、知识扩展、知识适应和知识增强，通过轻量化和通用化的学生网络实现多个蒸馏目标。 |
| [^20] | [FLIRT: Feedback Loop In-context Red Teaming.](http://arxiv.org/abs/2308.04265) | 本论文提出了一个自动红队行动框架，通过反馈循环和背景学习来评估和暴露生成模型的漏洞，特别是对不安全和不适当内容的生成。对于文本到图像模型，采用不同的背景攻击策略可以学习出有效和多样化的对抗提示。相比基线方法，该策略在暴露漏洞方面更加有效，甚至在模型增加安全功能的情况下仍然能够发现漏洞。该框架也适用于红队行动文本到文本模型。 |
| [^21] | [MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion.](http://arxiv.org/abs/2308.04249) | MindDiffuser是一个用于从人脑活动中重建图像的两阶段模型，通过结合语义和结构信息实现精确可控的图像重建。 |
| [^22] | [Gloss Alignment Using Word Embeddings.](http://arxiv.org/abs/2308.04248) | 这篇论文提出了一种使用大型口语语言模型对定位结果与字幕进行对齐的方法，解决了手语翻译数据中字幕和手势不匹配的问题。 |
| [^23] | [AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models.](http://arxiv.org/abs/2308.04241) | 本研究提出了一种利用大型语言模型和深度学习算法的自动化AI驱动的PCF核算框架AutoPCF，该框架具有实现产品碳足迹的自动建模和估算的潜力。 |
| [^24] | [Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction.](http://arxiv.org/abs/2308.04237) | 本论文提出了一种基于联合符合预测的无线通道联合推理方法，通过设备到服务器的通信提高了服务器的推理决策的可靠性和准确性。 |
| [^25] | [Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models.](http://arxiv.org/abs/2308.04220) | 本论文提出了一种方法来在GNN模型中增强可解释性，通过引入语义关注和建立特征重要性权重与模型准确性之间的相关性。这对于图深度学习任务具有重要意义。 |
| [^26] | [Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance.](http://arxiv.org/abs/2308.04215) | 提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。 |
| [^27] | [A Differential Datalog Interpreter.](http://arxiv.org/abs/2308.04214) | 本文研究了差分Datalog解释器的材料化性能，特别是增量维护和工作分配方面的表现。 |
| [^28] | [Adding Why to What? Analyses of an Everyday Explanation.](http://arxiv.org/abs/2308.04187) | 该论文研究了在解释技术产品时应用的双重性理论，通过关注架构和相关性，对解释进行了快速结构化和比较。通过分析解释内容和视频回想，探索了解释者如何进行解释的合理化。研究发现... |
| [^29] | [Assistive Chatbots for healthcare: a succinct review.](http://arxiv.org/abs/2308.04178) | 医疗保健领域的辅助聊天机器人在过去10年取得了一些进展，但在患者信任和与人类交互能力方面仍存在挑战。 |
| [^30] | [Predicting Drug-Drug Interactions Using Knowledge Graphs.](http://arxiv.org/abs/2308.04172) | 本文提出了一个名为medicX的端到端框架，通过使用知识图谱和机器学习算法，预测药物的相互作用。最好的表现组合是ComplEx嵌入方法和LSTM网络，达到了95.19%的F1分数。 |
| [^31] | [Current and Future Challenges in Knowledge Representation and Reasoning.](http://arxiv.org/abs/2308.04161) | 知识表示与推理是人工智能领域的一个核心领域，近年来在机器学习和不确定推理等方面得到了进一步发展。该论文总结了一项关于知识表示与推理的研讨会的结果，提出了该领域的起源、目标、里程碑和挑战，以及未来十年的重点优先事项。 |
| [^32] | [Heterogeneous 360 Degree Videos in Metaverse: Differentiated Reinforcement Learning Approaches.](http://arxiv.org/abs/2308.04083) | 本文提出了一种为异构360度视频设计的服务质量模型，并使用差异化深度强化学习算法进行逐帧优化。通过引入分离输入差异输出（SIDO）和合并输入差异输出（MIDO）两种结构，该方法在异构场景下表现出了有效性。 |
| [^33] | [Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients.](http://arxiv.org/abs/2308.04077) | 本论文提出了一种使用轨迹信息的代理梯度方法，用于解决联邦零阶优化中的查询和通信效率问题。 |
| [^34] | [Path Signatures for Diversity in Probabilistic Trajectory Optimisation.](http://arxiv.org/abs/2308.04071) | 本文提出了一种基于路径签名的算法，用于解决并行轨迹优化中的模式塌陷问题，并实现更好的全局性能。 |
| [^35] | [Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation.](http://arxiv.org/abs/2308.04061) | 本文提出了一种适用于标记数据稀缺环境的半监督对抗训练算法，通过引入适用于无标签数据的正则化项和知识蒸馏，实现了对抗鲁棒性的增强，并在实验证明了其显著优势。 |
| [^36] | [SODFormer: Streaming Object Detection with Transformer Using Events and Frames.](http://arxiv.org/abs/2308.04047) | SODFormer是一个使用事件和帧进行流式目标检测的Transformer模型，通过设计时空Transformer架构和引入基于注意力的融合模块，实现了对目标的连续检测，提高了检测性能。 |
| [^37] | [InfeRE: Step-by-Step Regex Generation via Chain of Inference.](http://arxiv.org/abs/2308.04041) | InfeRE是一种通过推理链逐步生成正则表达式的新方法，它考虑了生成最终表达式背后的逐步内部文本匹配过程，并引入了自一致性解码机制来提高鲁棒性。实验结果表明，InfeRE在生成正则表达式方面取得了显著的改进。 |
| [^38] | [Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications.](http://arxiv.org/abs/2308.04033) | 本文介绍了NextGen Communications Copilot，这是一个用于无线通信规范信息综合的对话式人工智能工具。它采用基础模型，并引入了领域特定的数据库、上下文提取器和反馈机制，能够提供准确且相关的上下文信息，并结合专家反馈和数据贡献工具。在基准数据集的评估中，该系统展示了更多的优势。 |
| [^39] | [Measure of Uncertainty in Human Emotions.](http://arxiv.org/abs/2308.04032) | 本研究探讨了情绪分类的不确定性信息展示对人类决策过程的影响，结果显示展示更多的不确定性信息可以增强用户决策的自信度。 (This study investigated the impact of displaying more uncertainty information on human decision making in emotion classification, and the results showed that it can increase users' confidence in decision making.) |
| [^40] | [Gentopia: A Collaborative Platform for Tool-Augmented LLMs.](http://arxiv.org/abs/2308.04030) | Gentopia是一个协作平台，用于增强语言模型，并具有灵活的定制、协作民主化和综合评估的特性。 |
| [^41] | [Top K Relevant Passage Retrieval for Biomedical Question Answering.](http://arxiv.org/abs/2308.04028) | 这篇论文提出了一种用于生物医学问题回答的Top K相关段落检索方法，传统的稀疏向量空间模型不适用于这个任务。然而，对于临床领域来说，这个问题还没有得到很好的解决。 |
| [^42] | [AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.](http://arxiv.org/abs/2308.04026) | AgentSims是一个开放源码的沙盒平台，用于评估大型语言模型的能力。它通过任务驱动的评估方法，解决了现有评估方法的局限性，并提供了易于使用的界面，供研究人员测试特定能力。演示版本可在https://agentsims.com获得。 |
| [^43] | [MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition.](http://arxiv.org/abs/2308.04025) | 本研究针对语音情感识别(SER)提出了MSAC方法，通过构建新颖的CNN-based SER模型和多语音属性控制方法MSAC，实现了对情感的更精细控制和捕捉，从而提升了SER的可靠性和效果。 |
| [^44] | [Scope Loss for Imbalanced Classification and RL Exploration.](http://arxiv.org/abs/2308.04024) | 本文介绍了范围损失，这是一种新的适用于强化学习和监督分类的损失函数，它能够解决探索利用权衡和数据集不平衡问题，并在实验中表现出优于其他损失函数的性能。 |
| [^45] | [Improving Performance of Semi-Supervised Learning by Adversarial Attacks.](http://arxiv.org/abs/2308.04018) | 本文提出了一个名为SCAR的通用框架，通过对预训练模型进行对抗攻击来改进半监督学习算法的性能，显著提高了图像分类的准确性。 |
| [^46] | [Multi-Granularity Attention Model for Group Recommendation.](http://arxiv.org/abs/2308.04017) | 多粒度注意力模型为群组推荐提供了一种新方法，通过利用多个粒度层次来揭示群组成员的潜在偏好并减少推荐噪声。 |
| [^47] | [Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning.](http://arxiv.org/abs/2308.03999) | 本文提供了一种使用结构化背景知识和演绎推理的方法，用于解释CNN隐藏神经元的激活。该方法能够提供有意义的解释，解决了深度学习系统黑盒特性的问题。 |
| [^48] | [Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks.](http://arxiv.org/abs/2308.03995) | 本文提出了一种合作式多类型多智能体深度强化学习方法，应用于空天地一体化网络的资源管理，实验结果表明其有效性和潜在价值。 |
| [^49] | [AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in CS Education.](http://arxiv.org/abs/2308.03992) | 本研究研究了使用人工智能驱动的多角色聊天机器人作为提升计算机科学教育学习体验和增强参与度的手段。通过四个不同的角色和探究式学习范式，满足学生的内在心理需求，并通过与人类导师和单个聊天机器人的条件进行比较，证实了该系统的有效性。 |
| [^50] | [NEOLAF, an LLM-powered neural-symbolic cognitive architecture.](http://arxiv.org/abs/2308.03990) | NEOLAF是一种基于LLM的神经符号认知架构，相比于纯连接主义和纯符号方法，它具有解释性、增量学习、协作和分布式学习、人工协助和自我改进的优势。通过实验证明NEOLAF具有优秀的学习能力，有潜力推动认知架构和自适应教学系统领域的发展。 |
| [^51] | [SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool.](http://arxiv.org/abs/2308.03983) | SimplyRetrieve是一款私密且轻量级的生成型AI工具，使用了基于检索的生成方法，通过无需额外模型微调将私有数据集成进生成型AI系统，并提供了一个本地化、轻量级和用户友好的界面，以方便用户探索RCG在提升生成型AI性能方面的潜力。 |
| [^52] | [CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification.](http://arxiv.org/abs/2308.03968) | 本文介绍了一种名为CheXFusion的基于Transformer的融合模块，可用于长尾胸部X射线分类。该模块利用自注意力和交叉注意力机制，有效地聚合多视角特征并考虑标签共现性。通过数据平衡和自训练方法，该解决方案在MIMIC-CXR测试集上取得了0.372的mAP，成为竞赛的冠军。本研究强调了在医学图像分类中考虑多视角设置、类别不平衡和标签共现性的重要性。 |
| [^53] | [ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals.](http://arxiv.org/abs/2308.03936) | ALFA使用各个层次的特征抽象来增强组织病理学图像分类在未知医院中的泛化能力，并通过自监督学习和域对齐实现了对不变特征的提取，以及对参与医院的高度特定特征的表示与分类。 |
| [^54] | [Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links.](http://arxiv.org/abs/2308.03929) | 本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。 |
| [^55] | [ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition.](http://arxiv.org/abs/2308.03908) | 本论文提出了一种基于姿势增强的视觉语言模型(VLM)，用于视频动作识别。实验结果表明，该模型能够取得令人期待的准确率，并在两个流行的动作识别数据集上取得了优异的性能。 |
| [^56] | [Intelligent Assistant Language Understanding On Device.](http://arxiv.org/abs/2308.03905) | 本论文描述了一种在设备上运行的自然语言理解系统的设计，相比于基于服务器的助手，该系统更加私密、可靠、快速、表达更强、准确性更高。通过分享实践经验，为研究界的未来工作提供参考。 |
| [^57] | [FLIPS: Federated Learning using Intelligent Participant Selection.](http://arxiv.org/abs/2308.03901) | 本文介绍了FLIPS，这是一个用于管理联邦学习中数据和参与者异质性的中间件系统。FLIPS通过标签分布聚类和智能参与者选择，并使用可信执行环境来确保隐私保护。实证评估表明，FLIPS相比随机方法有更好的性能。 |
| [^58] | [Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations.](http://arxiv.org/abs/2308.03882) | 本文提出了一种利用未见状态增强的策略，在离线强化学习中通过基于价值的扰动和过滤，实现了对离线数据之外的状态的利用和泛化。 |
| [^59] | [Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse.](http://arxiv.org/abs/2308.03880) | 这项研究介绍了一种自动化工具，用于全面分析儿童性虐待报告。通过自动化分析和分类，降低了接触有害内容的风险，并利用多学科团队的专业知识进行更深入的分析，以改善对基本模式和趋势的理解，帮助执法部门和决策者制定针对性的政策和行动。 |
| [^60] | [Trusting Language Models in Education.](http://arxiv.org/abs/2308.03866) | 本研究在教育中使用语言模型，提出了使用XGBoost和BERT的结合来校准语言模型的置信度，通过基于注意力机制的特征来输出校正的概率。 |
| [^61] | [Mobile Supply: The Last Piece of Jigsaw of Recommender System.](http://arxiv.org/abs/2308.03855) | 本研究提出了一种新的推荐系统模块，称为移动供应，旨在解决分页机制问题。通过在用户设备上部署推荐算法，可以有效减少数据传输延迟和提升用户的沉浸式体验。 |
| [^62] | [Revisiting Prompt Engineering via Declarative Crowdsourcing.](http://arxiv.org/abs/2308.03854) | 本研究通过借鉴声明式众包的思想，提出了一种声明式提示工程的方法，旨在解决大型语言模型在数据处理中的质量优化和成本控制问题。 |
| [^63] | [Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection.](http://arxiv.org/abs/2308.03826) | 本研究提出了一个循环多尺度Transformer网络，用于高分辨率显著目标检测，同时引入了一个新的HRS10K数据集，该数据集是目前规模最大的用于HRSOD任务的数据集。 |
| [^64] | [High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers.](http://arxiv.org/abs/2308.03813) | 该论文提出了一种新的方法来增加个性化颅骨重建的可用性，通过点云完成任务实现高分辨率颅缺损重建，并在训练和推理过程中快速且资源高效。 |
| [^65] | [Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables.](http://arxiv.org/abs/2308.03805) | 提出了一种使用弱监督多任务表示学习的方法，通过将数据映射到多个表示空间并根据多个方面对数据进行聚类，实现了同时解决多个任务的能力。 |
| [^66] | [Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach.](http://arxiv.org/abs/2308.03800) | 该论文提出了一种深度学习方法，用于分析金融欺诈文本并进行分类。通过比较多种神经网络模型的准确性，该研究对金融欺诈检测具有重要意义 |
| [^67] | [Semantic Channel Equalizer: Modelling Language Mismatch in Multi-User Semantic Communications.](http://arxiv.org/abs/2308.03789) | 本文提出了一个新的语义信道均衡器，用于解决多用户语义通信中由于不同语言导致的语义噪声问题。 |
| [^68] | [Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction.](http://arxiv.org/abs/2308.03782) | 该研究比较了Bio+Clinical BERT、BERT Base和CNN在预测药物评价满意度方面的性能，结果显示医学领域特定的Bio+Clinical BERT模型在整体性能上优于通用领域的BERT Base模型，提高了11%的Macro F1和召回率得分。 |
| [^69] | [Goodness-of-Fit of Attributed Probabilistic Graph Generative Models.](http://arxiv.org/abs/2308.03773) | 本文研究了属性概率图生成模型的拟合优度，并以均方互信息系数为指标进行了评估和验证。 |
| [^70] | [Improved Neural Radiance Fields Using Pseudo-depth and Fusion.](http://arxiv.org/abs/2308.03772) | 本文提出了一种改进神经光辐射场的方法，通过构建多尺度编码体和提供多尺度几何信息，同时进行深度预测和辐射场重建，以实现对真实场景中物体的准确建模和视角合成。 |
| [^71] | [Visual Saliency Detection in Advanced Driver Assistance Systems.](http://arxiv.org/abs/2308.03770) | 这项研究介绍了一种在高级驾驶辅助系统中结合驾驶员嗜睡检测和基于显著性的场景理解的智能系统。通过采用特殊的3D深度网络进行语义分割，并在嵌入式平台上实现，该系统可以识别驾驶员的注意力和环境中的显著元素。 |
| [^72] | [Towards Integrated Traffic Control with Operating Decentralized Autonomous Organization.](http://arxiv.org/abs/2308.03769) | 本论文提出了一种基于分权自治组织（DAO）框架的集成交通控制方法，通过共识和激励机制实现了能源消耗效率的全局共识，并优化了所有智能代理的本地目标。同时，针对DAO中的结构僵化问题，提出了一种操作算法，扩展了基于DAO的控制能力。 |
| [^73] | [Enhancing image captioning with depth information using a Transformer-based framework.](http://arxiv.org/abs/2308.03767) | 本研究提出了一个基于Transformer的框架，将深度信息与RGB图像结合，以增强图像字幕生成任务。通过使用多句字幕描述3D场景，我们的框架能够更好地理解输入场景，并在不同的融合方法下进行实验验证。 |
| [^74] | [AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection.](http://arxiv.org/abs/2308.03766) | AMaizeD是一种自动化框架，利用无人机获取的多光谱图像，通过卷积神经网络和分割技术实现了对玉米作物病害的早期检测。 |
| [^75] | [Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach.](http://arxiv.org/abs/2308.03764) | 本文研究在智能工作区应用中部署领导者-跟随者自动驾驶车辆系统，并提出了一种基于排队的交通分配方法来优化ATMA车辆的路径规划，从而减少慢速运行导致的系统成本。 |
| [^76] | [MedMine: Examining Pre-trained Language Models on Medication Mining.](http://arxiv.org/abs/2308.03629) | MedMine通过检验预训练语言模型在药物挖掘中的应用，发现了它们在不同实体类型和临床事件上的不平衡表现，并提供了解决这些问题的研究方向。 |
| [^77] | [RecycleGPT: An Autoregressive Language Model with Recyclable Module.](http://arxiv.org/abs/2308.03421) | RecycleGPT是一种生成式语言模型，通过回收预生成的模型状态而无需多次运行整个模型，并且证明了其降低推理延迟的有效性，实现高速解码和保持高性能。 |
| [^78] | [Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks.](http://arxiv.org/abs/2308.02632) | 本研究提出了一种使用生成对抗网络生成合成毫米波雷达数据的快速方法，可以增加数据增强的潜力，并进一步开发雷达数据处理算法。 |
| [^79] | [Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting.](http://arxiv.org/abs/2308.02582) | 该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。 |
| [^80] | [Digital twin brain: a bridge between biological intelligence and artificial intelligence.](http://arxiv.org/abs/2308.01941) | 本文提出了数字双胞胎脑作为一个桥梁，将生物智能和人工智能联系在一起，通过将神经科学和人工智能的进展结合起来，更好地理解大脑的复杂性和如何产生智能。 |
| [^81] | [NBIAS: A Natural Language Processing Framework for Bias Identification in Text.](http://arxiv.org/abs/2308.01681) | 本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。 |
| [^82] | [FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving.](http://arxiv.org/abs/2308.01006) | FusionAD是第一个将来自相机和激光雷达的信息融合起来用于自动驾驶预测和规划任务的统一框架，在常用数据集上的实验中达到了最先进的性能。 |
| [^83] | [MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.](http://arxiv.org/abs/2308.00352) | MetaGPT是一个用于多智能体协作的创新框架，将有效的人工工作流引入到大型语言模型驱动的协作中。它采用元编程方法，将标准操作规程编码为提示，促进结构化协调，并要求模块化输出，赋予智能体领域专业知识，以验证输出并减少错误。这种框架利用了流水线工作模式来分配任务。 |
| [^84] | [ProtoFL: Unsupervised Federated Learning via Prototypical Distillation.](http://arxiv.org/abs/2307.12450) | 本文提出了ProtoFL，一种基于原型蒸馏的无监督联邦学习方法，用于提高全局模型的表示能力并减少通信成本。此外，引入了基于正规流的本地单类分类器以提高有限数据下的性能。在五个基准数据集上的实验证明了该方法相较于先前方法具有超越性能。 |
| [^85] | [Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?.](http://arxiv.org/abs/2307.12344) | 本研究在胸透诊断任务中评估了五种事后解释技术和一种本质上可解释的方法对于检测虚假相关性的能力，发现SHAP和Attri-Net可以有效地检测出这种类型的依赖关系。 |
| [^86] | [Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts.](http://arxiv.org/abs/2307.11661) | 本文展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，在专门细粒度数据集上显示了较大的0-shot迁移准确性改进。 |
| [^87] | [Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education.](http://arxiv.org/abs/2307.09426) | 本论文讨论了在开发人工智能算法时，在组织病理学中平衡隐私和进步的挑战，并提出了解决方案。 |
| [^88] | [Can We Trust Race Prediction?.](http://arxiv.org/abs/2307.08496) | 本文研究了在没有敏感的种族和族裔数据的情况下，使用代理模型进行种族预测的问题。研究者训练了一个双向长短时记忆（BiLSTM）模型，并创建了一个集成模型，其在外样本（OOS）F1得分上比文献中表现最好的机器学习模型高出36.8%。此外，还构建了美国最全面的姓氏和名字分布数据库，并提供了第一个高质量的基准数据集，以公正比较现有模型，并帮助未来的模型开发者。 |
| [^89] | [A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations.](http://arxiv.org/abs/2307.02947) | 本文提出了一种新颖的神经网络架构，用于从实值观测中进行强化学习。该模型采用了多层事件驱动聚类、时间差分误差调制和资格痕迹等方法，并在经典RL环境中取得了优于表格方法的性能表现。 |
| [^90] | [MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition.](http://arxiv.org/abs/2307.02227) | 本研究提出了一种高效的自监督方法MAE-DFER，通过大规模自监督预训练和显式时间面部运动建模，推进了动态面部表情识别的发展。 |
| [^91] | [Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views.](http://arxiv.org/abs/2306.09841) | 本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。 |
| [^92] | [InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models.](http://arxiv.org/abs/2306.03082) | InstructZero通过优化低维软提示而非离散指令，实现了对黑盒语言模型的高效指令优化，通过贝叶斯优化生成改善零样本性能的新软提示，并在不同任务上优于现有自动指令方法。 |
| [^93] | [Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs.](http://arxiv.org/abs/2306.02864) | LLMs在公共事务文件分类中的表现进行了分析。通过使用正则表达式工具收集了一个包含超过33K个样本和22.5M个标记的公共事务文件数据库。实验结果表明，LLMs可以有效处理和理解这类文件中使用的复杂语言，从而提高了公共事务文件的分析能力。 |
| [^94] | [AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap.](http://arxiv.org/abs/2306.01941) | 在当前LLMs话题的讨论中，负责任人工智能的关键要素--透明度--被忽视了。这篇论文提出了在以人为中心的视角下，追求LLMs的透明度对于支持适当的人类理解是至关重要的。 |
| [^95] | [Measuring and Modeling Physical Intrinsic Motivation.](http://arxiv.org/abs/2305.13452) | 本文对身体内在动机进行了量化建模，发现对抗性奖励模型可以最好地预测人类对物理情境的趣味反应，还发现简单场景特征模型无法在所有情境中预测人类反应，将对抗模型和场景中碰撞数量进行线性组合，能够显著提高对人类反应的预测能力，表明人类追求高信息增益和身体活动的情况。 |
| [^96] | [Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders.](http://arxiv.org/abs/2305.02640) | 本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。 |
| [^97] | [Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects.](http://arxiv.org/abs/2304.08275) | 本文讨论了负责任人工智能伦理原则之间的紧张关系和权衡，并提出一个目录以帮助人们提高对相互作用的认识。 |
| [^98] | [GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization.](http://arxiv.org/abs/2303.16459) | GNNBuilder是一个自动化的、通用的、端到端的GNN加速器生成框架，它可以为用户任意定义的广泛的GNN模型自动生成加速器，且运行速度比软件基准快多达12.95倍。 |
| [^99] | [Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction.](http://arxiv.org/abs/2303.00286) | 通过引入领域和范围约束，我们提出了基于语义的损失函数来区分不同质量的负样本，实验证明在链接预测任务上有效。 |
| [^100] | [SPTS v2: Single-Point Scene Text Spotting.](http://arxiv.org/abs/2301.01635) | 本研究提出的SPTS v2框架，首次证明了可以使用极低成本的单点注释来训练场景文本定位模型，同时保留了自回归Transformer与实例分配解码器（IAD）的优势，并使用并行识别解码器（PRD）进行文本识别。 |
| [^101] | [Learning To Rank Diversely At Airbnb.](http://arxiv.org/abs/2210.07774) | 这篇论文研究了在Airbnb上学习多样化的排序方法。通过纠正传统排序框架中的假设，提出了一种基于高效神经网络架构的排序策略，以实现更准确的房源匹配，并加入了考虑房源相似性的方法以实现多样化的排序。 |
| [^102] | [Machine Learning and Computer Vision Techniques in Bee Monitoring Applications.](http://arxiv.org/abs/2208.00085) | 本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。 |
| [^103] | [Semi-supervised Deep Multi-view Stereo.](http://arxiv.org/abs/2207.11699) | 本文针对学习式多视图立体问题，探讨了仅有一部分带有深度准确值的数据的半监督设置。通过引入新的半监督分布增强框架，提出了一种方法来解决MVS中的分布间隙多义性问题。 |
| [^104] | [ORC: Network Group-based Knowledge Distillation using Online Role Change.](http://arxiv.org/abs/2206.01186) | 本文提出了一种利用在线角色变换的网络群组知识蒸馏方法，通过将多个网络分成教师组和学生组，学生组学习教师的知识，使用在线角色变换策略将学生组中排名靠前的网络晋升为教师组，通过训练教师组以改进其知识，实现了更好的知识蒸馏效果。 |

# 详细

[^1]: SILO语言模型：在非参数化数据存储中隔离法律风险

    SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])

    [http://arxiv.org/abs/2308.04430](http://arxiv.org/abs/2308.04430)

    SILO是一种新的语言模型，通过在推理过程中对非参数化的数据存储进行查询，实现在面临法律风险和模型性能之间的权衡，并支持数据归属和数据生产者退出模型的功能。

    

    在对将语言模型（LMs）训练在受版权或受其他限制的数据上的合法性进行激烈辩论的同时，我们展示了仅在低风险文本（例如过期版权图书或政府文件）上训练时，模型性能显著下降的问题，原因是该文本的规模和领域覆盖有限。我们提出了SILO，一种新的语言模型，在推理过程中管理这种风险-性能权衡。SILO通过以下方式构建：（1）在我们策划的新语料库“开放许可证语料库”（OLC）上训练参数化的LM，该语料库包含228B个公共领域和许可文本。（2）通过非参数化的数据存储（例如包含受版权保护的图书或新闻的数据）对其进行扩充，该数据存储仅在推理过程中被查询。该数据存储允许使用高风险数据而无需对其进行训练，支持句级数据归属，并使数据生产者可以通过从存储中删除内容来选择退出模型。这些功能可以促进对数据使用规范的遵循。

    The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
    
[^2]: 股市价格预测：基于混合LSTM和顺序自注意力的方法

    Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach. (arXiv:2308.04419v1 [cs.AI])

    [http://arxiv.org/abs/2308.04419](http://arxiv.org/abs/2308.04419)

    本论文介绍了一种基于混合LSTM和顺序自注意力的方法，旨在预测股票未来价格，并通过实验证明了该方法的有效性。

    

    股市是最令人着迷的研究领域之一，预测股票价格可以帮助投资者在正确的时间做出最佳决策从而获利。深度学习策略已经成为金融市场领域的关键技术。股市受两个方面的影响，一方面是地缘政治、社会和全球事件，这些事件可能影响价格趋势。与此同时，第二个方面纯粹关注历史价格趋势和季节性，使我们能够预测股票价格。本文旨在专注于第二个方面，建立一个能够以最小误差预测未来价格的模型。为了提供更好的股票价格预测结果，我们提出了一个名为长短期记忆 (LSTM) 和顺序自注意力机制 (LSTM-SSAM) 的新模型。最后，我们对三个股票数据集进行了广泛的实验：SBIN、HDFCBANK 和 BANKBARODA。实验结果证明了模型的有效性。

    One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness a
    
[^3]: 使用随机线性分类器进行概率不变学习

    Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v1 [cs.LG])

    [http://arxiv.org/abs/2308.04412](http://arxiv.org/abs/2308.04412)

    本文介绍了一种使用随机线性分类器进行概率不变学习的方法，通过接受概率化的普遍逼近和不变性，设计了能同时具有表达能力和不变性的模型，并且使用更少的资源。通过实验证明了这种方法在分类任务中的有效性。

    

    设计既具有表达能力又能保持任务已知不变性的模型是一个越来越困难的问题。现有解决方案在不变性和计算或内存资源之间进行权衡。在这项工作中，我们展示了如何利用随机性设计既具表达能力又具不变性但使用更少资源的模型。受随机算法的启发，我们的关键洞察是接受概率化的普遍逼近和不变性可以减少资源需求。具体而言，我们提出了一类称为随机线性分类器 (RLCs) 的二分类模型。我们给出了参数和样本大小的条件，在这些条件下，RLCs 可以以高概率逼近任何（平滑）函数，并保持对紧致群变换的不变性。利用这一结果，我们设计了三种可验证地概率不变的 RLCs，用于集合、图和球形数据的分类任务。我们展示了这些模型如何实现概率不变性。

    Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over sets, graphs, and spherical data. We show how these models can achieve probabilistic invari
    
[^4]: Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models

    Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models. (arXiv:2308.04399v1 [cs.GT])

    [http://arxiv.org/abs/2308.04399](http://arxiv.org/abs/2308.04399)

    本文提出了一个模型，探讨了通用模型的微调过程中的利润分享问题，为一般类的成本和收入函数描述了解决方案的条件。

    

    机器学习（ML）和人工智能（AI）方面的重大进展越来越多地采用开发和发布通用模型的形式。这些模型旨在由其他企业和机构进行适应，以执行特定的领域专用功能。这个过程被称为适应或微调。本文提供了一个微调过程的模型，其中一位通用专家将技术产品（即ML模型）提升到一定的性能水平，并且一位或多位领域专家将其调整适用于特定领域。这两个实体都是追求利润的，当他们投资于技术时会产生成本，在技术进入市场前，他们必须就如何分享收入达成谈判协议。对于相对一般的成本和收入函数类，我们刻画了微调博弈产生利润分享解决方案的条件。我们观察到，任何潜在的领域专业化都会产生...

    Major advances in Machine Learning (ML) and Artificial Intelligence (AI) increasingly take the form of developing and releasing general-purpose models. These models are designed to be adapted by other businesses and agencies to perform a particular, domain-specific function. This process has become known as adaptation or fine-tuning. This paper offers a model of the fine-tuning process where a Generalist brings the technological product (here an ML model) to a certain level of performance, and one or more Domain-specialist(s) adapts it for use in a particular domain. Both entities are profit-seeking and incur costs when they invest in the technology, and they must reach a bargaining agreement on how to share the revenue for the technology to reach the market. For a relatively general class of cost and revenue functions, we characterize the conditions under which the fine-tuning game yields a profit-sharing solution. We observe that any potential domain-specialization will either contri
    
[^5]: 企业协作系统的事件抽象以支持社会流程挖掘

    Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining. (arXiv:2308.04396v1 [cs.LG])

    [http://arxiv.org/abs/2308.04396](http://arxiv.org/abs/2308.04396)

    本论文提出了一种定制的企业协作系统的事件抽象方法，通过比较实际用户活动和系统生成的低级别跟踪来训练模型，并将低级别跟踪转换为抽象的高级别日志，以支持社会流程挖掘。

    

    流程挖掘的一个目标是从信息系统的事件日志中发现流程模型。流程挖掘已成功应用于面向流程的企业系统，但对于面向通信和文档的企业协作系统（ECS）来说不太适用。ECS事件日志非常细粒度，对其日志应用流程挖掘会导致混乱的模型。一个常见的解决方案是事件抽象，即在运行发现算法之前将低级别日志转换为更抽象的高级别日志。迄今为止，既有的事件抽象方法尚未完全解决ECS日志的特殊特征。我们旨在通过定制的ECS事件抽象（ECSEA）方法来弥补这一差距，该方法通过比较记录的实际用户活动（高级别跟踪）与系统生成的低级别跟踪（从ECS中提取）来训练模型。该模型使我们能够自动将未来的低级别跟踪转换为抽象的高级别日志。

    One aim of Process Mining (PM) is the discovery of process models from event logs of information systems. PM has been successfully applied to process-oriented enterprise systems but is less suited for communication- and document-oriented Enterprise Collaboration Systems (ECS). ECS event logs are very fine-granular and PM applied to their logs results in spaghetti models. A common solution for this is event abstraction, i.e., converting low-level logs into more abstract high-level logs before running discovery algorithms. ECS logs come with special characteristics that have so far not been fully addressed by existing event abstraction approaches. We aim to close this gap with a tailored ECS event abstraction (ECSEA) approach that trains a model by comparing recorded actual user activities (high-level traces) with the system-generated low-level traces (extracted from the ECS). The model allows us to automatically convert future low-level traces into an abstracted high-level log that can 
    
[^6]: 理解因果解释对于人工智能与人类协作临床决策中信任和依赖的影响

    Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making. (arXiv:2308.04375v1 [cs.HC])

    [http://arxiv.org/abs/2308.04375](http://arxiv.org/abs/2308.04375)

    本研究利用因果解释方法帮助人类分析审查人工智能建议，减少对人工智能的过度依赖，提高了临床决策的质量和表现。

    

    人工智能（AI）越来越被考虑用于辅助高风险领域（如健康）中的人类决策。然而，研究人员讨论了一个问题，即人类可能会过度依赖错误的AI模型建议，而非实现人工智能与人类的互补性能。在这项工作中，我们利用显著特征解释和假设性因果解释，使人类能更分析地审查AI建议，以减少对AI的过度依赖，并探讨这些解释对临床决策中的信任和依赖的影响。我们在七位治疗师和十位非专业人士中进行了一项实验，任务是评估中风后幸存者的运动质量，并分析了他们的表现、任务上的一致性水平以及在无解释和有两种AI解释的情况下对AI的依赖。结果显示，具有显著特征和因果解释的AI模型帮助治疗师和非专业人士改善了他们的表现。

    Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their 
    
[^7]: 用逻辑论证实例化双极论证图的若干选项

    Some Options for Instantiation of Bipolar Argument Graphs with Deductive Arguments. (arXiv:2308.04372v1 [cs.AI])

    [http://arxiv.org/abs/2308.04372](http://arxiv.org/abs/2308.04372)

    本文讨论了双极论证图的实例化，通过使用逻辑论证实例化双极论证图，并考虑到论证的内部结构和论证之间的关系类型。

    

    论证图提供了一种论证情境的抽象表示。双极论证图是一个有向图，其中每个节点表示一个论证，每个弧表示一个论证对另一个论证的影响。在双极论证图中，每个论证都是原子的，没有内部结构。然而，为了更好地理解个体论证的性质以及它们之间的相互作用，考虑它们的内部结构十分重要。为了解决这个问题，本文提出了一个基于逻辑论证的框架，用于实例化双极论证图，并提供了一组可能的约束条件，考虑到论证的内部结构和论证之间的关系类型。

    Argument graphs provide an abstract representation of an argumentative situation. A bipolar argument graph is a directed graph where each node denotes an argument, and each arc denotes the influence of one argument on another. Here we assume that the influence is supporting, attacking, or ambiguous. In a bipolar argument graph, each argument is atomic and so it has no internal structure. Yet to better understand the nature of the individual arguments, and how they interact, it is important to consider their internal structure. To address this need, this paper presents a framework based on the use of logical arguments to instantiate bipolar argument graphs, and a set of possible constraints on instantiating arguments that take into account the internal structure of the arguments, and the types of relationship between arguments.
    
[^8]: 用大型语言模型进行累积推理的论文

    Cumulative Reasoning With Large Language Models. (arXiv:2308.04371v1 [cs.AI])

    [http://arxiv.org/abs/2308.04371](http://arxiv.org/abs/2308.04371)

    本文提出了一种名为累积推理（CR）的新方法，利用语言模型以累积和迭代的方式模拟人类思维过程，通过将任务分解为较小的组件，简化问题解决过程，取得了优于现有方法的性能，并在逻辑推理和24点游戏中实现了显著提升。

    

    虽然语言模型强大且多功能，但它们通常无法解决高度复杂的问题。这是因为解决复杂问题需要深思熟虑，而在训练过程中对此只有最小程度的指导。在本文中，我们提出了一种新方法，称为累积推理（CR），它以累积和迭代的方式利用语言模型来模拟人类的思维过程。通过将任务分解为较小的组件，我们的方法简化了问题解决过程，使其更易管理和更有效。对于逻辑推理任务，CR在性能上始终超过现有方法，提高了多达9.3％，并在经过策划的FOLIO维基数据集上实现了惊人的98.04％的准确率。在24点游戏的背景下，CR实现了94％的准确率，相比先前最先进的方法，提升了20％。

    While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, \ournameb streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3\%, and achieves the astonishing accuracy of 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94\%, which signifies a substantial enhancement of 20\% over the previous state-of-the-art method.
    
[^9]: 学习无偏见的图像分割：以普通膝关节X光片为例的案例研究

    Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs. (arXiv:2308.04356v1 [cs.CV])

    [http://arxiv.org/abs/2308.04356](http://arxiv.org/abs/2308.04356)

    本文通过使用普通膝关节X光片重新审视使用深度学习进行膝关节骨解剖结构分割的方法，揭示出可见的性别和种族偏见，并提出了缓解策略以确保公正和无偏见的分割结果。同时，该研究促进了多样化患者人群获得准确诊断和治疗结果的平等机会，推动了公平和包容的医疗保障。

    

    在骨科学中，自动分割膝关节骨部解剖结构十分重要，并且在术前和术后环境中已经有了几年的应用。深度学习算法在医学图像分析中已经展示出了卓越的性能，但对于这些模型中的公平性和潜在偏见的评估仍然有限。本研究的目标是重新审视使用常规X光片进行深度学习驱动的膝关节骨解剖结构分割，以揭示可见的性别和种族偏见。本研究的贡献在于提供了一个推进我们对偏见的理解的机会，并为医学影像研究人员和从业者提供了实用的见解。提出的缓解策略可以缓解性别和种族偏见，确保公正和无偏见的分割结果。此外，本研究促进了多样化患者人群获得准确诊断和治疗结果的平等机会，推动了公平和包容的医疗保障。

    Automatic segmentation of knee bony anatomy is essential in orthopedics, and it has been around for several years in both pre-operative and post-operative settings. While deep learning algorithms have demonstrated exceptional performance in medical image analysis, the assessment of fairness and potential biases within these models remains limited. This study aims to revisit deep learning-powered knee-bony anatomy segmentation using plain radiographs to uncover visible gender and racial biases. The current contribution offers the potential to advance our understanding of biases, and it provides practical insights for researchers and practitioners in medical imaging. The proposed mitigation strategies mitigate gender and racial biases, ensuring fair and unbiased segmentation results. Furthermore, this work promotes equal access to accurate diagnoses and treatment outcomes for diverse patient populations, fostering equitable and inclusive healthcare provision.
    
[^10]: 基于图像分类的珊瑚礁损伤检测模型的开发

    Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra. (arXiv:2308.04337v1 [cs.CV])

    [http://arxiv.org/abs/2308.04337](http://arxiv.org/abs/2308.04337)

    本研究开发了一个基于图像分类的模型，用于检测珊瑚礁的损伤。该模型利用卷积神经网络识别和区分健康珊瑚和白化珊瑚的视觉模式。

    

    印度尼西亚水域丰富的珊瑚礁生物多样性是一项宝贵的资产，需要得到保护。快速的气候变化和不受控制的人类活动导致了珊瑚礁生态系统的退化，包括珊瑚白化，这是珊瑚健康状况的一个关键指标。因此，本研究旨在开发一个准确的分类模型，用于区分健康珊瑚和经历白化的珊瑚。本研究利用了一个专门的数据集，其中包含从Flickr收集的923个图像，使用Flickr API进行检索。该数据集包含两个不同的类别：健康珊瑚（438个图像）和白化珊瑚（485个图像）。这些图像已被调整大小，宽度或高度最大为300像素，以保持数据集中的一致尺寸。本研究采用的方法涉及使用机器学习模型，特别是卷积神经网络（CNN），来识别和区分相关的视觉模式。

    The abundant biodiversity of coral reefs in Indonesian waters is a valuable asset that needs to be preserved. Rapid climate change and uncontrolled human activities have led to the degradation of coral reef ecosystems, including coral bleaching, which is a critical indicator of coral health conditions. Therefore, this research aims to develop an accurate classification model to distinguish between healthy corals and corals experiencing bleaching. This study utilizes a specialized dataset consisting of 923 images collected from Flickr using the Flickr API. The dataset comprises two distinct classes: healthy corals (438 images) and bleached corals (485 images). These images have been resized to a maximum of 300 pixels in width or height, whichever is larger, to maintain consistent sizes across the dataset.  The method employed in this research involves the use of machine learning models, particularly convolutional neural networks (CNN), to recognize and differentiate visual patterns asso
    
[^11]: 合作式多智能体赌博机：具有最佳个体遗憾和恒定通信成本的分布式算法

    Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs. (arXiv:2308.04314v1 [cs.LG])

    [http://arxiv.org/abs/2308.04314](http://arxiv.org/abs/2308.04314)

    本文提出了一种新的合作赌博机算法，实现了最佳的个体遗憾和恒定的通信成本。

    

    最近，对合作式多智能体多臂赌博机进行了广泛研究，其中一组分布式智能体合作玩相同的多臂赌博游戏。目标是开发具有最佳群体和个体遗憾以及智能体之间通信成本低的赌博机算法。在前期工作中，使用了两种范式来解决这个问题：领导者-跟随者和完全分布式算法。在这两种范式中，以前的算法都能达到最佳群体遗憾。领导者-跟随者算法实现了恒定的通信成本，但未能达到最佳个体遗憾。目前最先进的完全分布式算法实现了最佳个体遗憾，但未能实现恒定的通信成本。本文提出了一种简单而有效的通信策略，并将其整合到合作赌博机的学习算法中。我们的算法同时实现了两种范式的最优个体遗憾和恒定通信成本。

    Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
    
[^12]: Apple Vision Pro for Healthcare: “终极显示器”？（arXiv:2308.04313v1 [cs.AI]）

    Apple Vision Pro for Healthcare: "The Ultimate Display"?. (arXiv:2308.04313v1 [cs.AI])

    [http://arxiv.org/abs/2308.04313](http://arxiv.org/abs/2308.04313)

    苹果推出了Vision Pro，一款具有混合现实和增强现实功能的虚拟现实设备，拥有独特的特点，例如内部屏幕展示佩戴者的眼睛以及数字皇冠按钮的融合功能。这款无线设备可能实现了“终极显示器”的潜力。

    

    在2023年6月的全球开发者大会（WWDC）上，苹果推出了Vision Pro。Vision Pro是一款混合现实（MR）头盔，更具体地说，它是一款具有额外视频透视（VST）能力的虚拟现实（VR）设备。通过将真实世界通过摄像头传输到用户眼前的（VR）屏幕，使得Vision Pro也成为了增强现实（AR）设备。当然，这并不独特，与Varjo XR-3等其他设备类似。尽管如此，Vision Pro具有一些有趣的特点，例如内部屏幕可以向“外界”显示佩戴头盔者的眼睛，或者顶部的一个按钮称为“数字皇冠”，可以通过旋转无缝地融合数字内容与物理空间。此外，Vision Pro是无线的，只有电池的电缆连接，这使得头盔比Varjo XR-3更加灵活。这可能更接近“终极显示器”。

    At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more specifically it is a Virtual Reality (VR) device with an additional Video See-Through (VST) capability. The VST capability turns the Vision Pro also into an Augmented Reality (AR) device. The AR feature is enabled by streaming the real world via cameras to the (VR) screens in front of the user's eyes. This is of course not unique and similar to other devices, like the Varjo XR-3. Nevertheless, the Vision Pro has some interesting features, like an inside-out screen that can show the headset wearers' eyes to "outsiders" or a button on the top, called "Digital Crown", that allows you to seamlessly blend digital content with your physical space by turning it. In addition, it is untethered, except for the cable to the battery, which makes the headset more agile, compared to the Varjo XR-3. This could actually come closer to the "Ultimate Display",
    
[^13]: 可解释的目标导向模型用于交互场景中的车辆轨迹预测

    Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios. (arXiv:2308.04312v1 [cs.AI])

    [http://arxiv.org/abs/2308.04312](http://arxiv.org/abs/2308.04312)

    本论文提出了一个可解释的目标导向模型，通过结合离散选择模型的可解释性和基于神经网络的模型的高准确性，在交互环境中预测车辆轨迹。使用INTERACTION数据集验证了该模型的有效性。

    

    在自动驾驶中，理解车辆与周围环境之间的社交互动行为并预测其在城市环境中的轨迹是确保道路安全的关键。由于不确定性，社交互动很难解释。近年来，基于神经网络的方法已广泛用于轨迹预测，并且已被证明优于手工设计的方法。然而，这些方法缺乏可解释性。为了克服这一局限性，我们将离散选择模型的可解释性与基于神经网络的模型的高准确性相结合，用于交互环境中的车辆轨迹预测任务。我们使用INTERACTION数据集实现和评估了我们的模型，并展示了我们提出的架构在解释预测时不会影响准确性的有效性。

    The abilities to understand the social interaction behaviors between a vehicle and its surroundings while predicting its trajectory in an urban environment are critical for road safety in autonomous driving. Social interactions are hard to explain because of their uncertainty. In recent years, neural network-based methods have been widely used for trajectory prediction and have been shown to outperform hand-crafted methods. However, these methods suffer from their lack of interpretability. In order to overcome this limitation, we combine the interpretability of a discrete choice model with the high accuracy of a neural network-based model for the task of vehicle trajectory prediction in an interactive environment. We implement and evaluate our model using the INTERACTION dataset and demonstrate the effectiveness of our proposed architecture to explain its predictions without compromising the accuracy.
    
[^14]: 使用先验信息和语义辅助占据网格地图进行车辆运动预测

    Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps. (arXiv:2308.04303v1 [cs.CV])

    [http://arxiv.org/abs/2308.04303](http://arxiv.org/abs/2308.04303)

    本文研究了车辆运动预测的挑战，提出了一种使用先验信息和语义辅助占据网格地图的框架来预测车辆行为。实验证明，该模型在预测静态和动态车辆方面表现出优越性能，并评估了语义标签和地图在模型中的作用。

    

    由于传感器数据的不确定性、未来的非确定性和代理行为的复杂性，运动预测对于自动驾驶车辆来说是一项具有挑战性的任务。本文通过将场景表示为动态占据网格地图（DOGMs），将语义标签与占据的单元格关联，并结合地图信息来解决这个问题。我们提出了一个新颖的框架，将基于深度学习的时空和概率方法相结合，预测车辆行为。与传统的OGM预测方法相反，我们的工作是根据真实注释进行评估的。我们在真实世界的NuScenes数据集上进行实验和验证，并展示了我们的模型在预测静态和动态车辆方面与OGM预测相比具有优越的能力。此外，我们还进行了消融研究，评估了语义标签和地图在架构中的作用。

    Motion prediction is a challenging task for autonomous vehicles due to uncertainty in the sensor data, the non-deterministic nature of future, and complex behavior of agents. In this paper, we tackle this problem by representing the scene as dynamic occupancy grid maps (DOGMs), associating semantic labels to the occupied cells and incorporating map information. We propose a novel framework that combines deep-learning-based spatio-temporal and probabilistic approaches to predict vehicle behaviors.Contrary to the conventional OGM prediction methods, evaluation of our work is conducted against the ground truth annotations. We experiment and validate our results on real-world NuScenes dataset and show that our model shows superior ability to predict both static and dynamic vehicles compared to OGM predictions. Furthermore, we perform an ablation study and assess the role of semantic labels and map in the architecture.
    
[^15]: 变化的时间离散化下的演员-评论者算法：通过持续动作来实现

    Actor-Critic with variable time discretization via sustained actions. (arXiv:2308.04299v1 [cs.AI])

    [http://arxiv.org/abs/2308.04299](http://arxiv.org/abs/2308.04299)

    这篇论文提出了一种称为SusACER的演员-评论者算法，可以通过使用持续动作，在变化的时间离散化过程中取得更好的机器人控制性能。

    

    强化学习方法在离散时间下工作。为了将强化学习应用于像机器人控制这样的连续问题，需要定义特定的时间离散化。这是在稀疏时间控制和精细时间控制之间的选择，稀疏时间控制可能更容易训练，而精细时间控制可能使最终性能更好。在这项工作中，我们提出了SusACER，一种离策略强化学习算法，结合了不同时间离散化设置的优点。最初，它使用稀疏时间离散化，并逐渐切换到精细时间离散化。我们分析了在机器人控制环境中改变时间离散化的影响：Ant，HalfCheetah，Hopper和Walker2D。在所有情况下，我们提出的算法优于现有技术。

    Reinforcement learning (RL) methods work in discrete time. In order to apply RL to inherently continuous problems like robotic control, a specific time discretization needs to be defined. This is a choice between sparse time control, which may be easier to train, and finer time control, which may allow for better ultimate performance. In this work, we propose SusACER, an off-policy RL algorithm that combines the advantages of different time discretization settings. Initially, it operates with sparse time discretization and gradually switches to a fine one. We analyze the effects of the changing time discretization in robotic control environments: Ant, HalfCheetah, Hopper, and Walker2D. In all cases our proposed algorithm outperforms state of the art.
    
[^16]: 工程化LaCAM$: 实现实时、大规模、接近最优的多智能体路径规划

    Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding. (arXiv:2308.04292v1 [cs.AI])

    [http://arxiv.org/abs/2308.04292](http://arxiv.org/abs/2308.04292)

    本文通过改进LaCAM*算法，解决了实时、大规模、接近最优的多智能体路径规划的挑战。改进技术的融合显著提高了LaCAM*的解决方案质量，推动了MAPF算法的边界。

    

    本文通过对最近提出的LaCAM*算法的改进，解决了实时、大规模和接近最优的多智能体路径规划(MAPF)的挑战。LaCAM*是一种可扩展的基于搜索的算法，它保证对于累计转移成本能够最终找到最优解决方案。尽管它表现出了显著的规划成功率，超过了各种最先进的MAPF方法，但其初始解决方案的质量远非最优，并且其收敛速度较慢。为了克服这些局限性，本文引入了一些改进技术，部分借鉴了其他MAPF方法的灵感。我们提供了实证证据，证明这些技术的融合显著提高了LaCAM*的解决方案质量，进一步推动了MAPF算法的边界。

    This paper addresses the challenges of real-time, large-scale, and near-optimal multi-agent pathfinding (MAPF) through enhancements to the recently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithm that guarantees the eventual finding of optimal solutions for cumulative transition costs. While it has demonstrated remarkable planning success rates, surpassing various state-of-the-art MAPF methods, its initial solution quality is far from optimal, and its convergence speed to the optimum is slow. To overcome these limitations, this paper introduces several improvement techniques, partly drawing inspiration from other MAPF methods. We provide empirical evidence that the fusion of these techniques significantly improves the solution quality of LaCAM*, thus further pushing the boundaries of MAPF algorithms.
    
[^17]: 在微调之前与纯净语言模型对话进行上下文对齐

    In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])

    [http://arxiv.org/abs/2308.04275](http://arxiv.org/abs/2308.04275)

    本文提出了一种在微调之前与纯净语言模型进行对话的方法，通过上下文学习实现了推理时的对齐。实验证明，这种方法将纯净语言模型的胜率提高了7倍，使其可以与通过对齐微调的强基准模型媲美。

    

    在这个说明中，我们通过上下文学习来探索推理时的对齐。我们考虑了一个纯净的预训练语言模型 Llama-2，在进行任何微调之前，当模型被要求按照聊天式的指令进行操作时，我们检索到了平均9个对齐演示示例。与直接提示相比，不改变模型权重的上下文对齐导致了与OpenAI的text-davinci-003模型相比，胜率提高了7倍，使得纯净语言模型可以媲美通过对齐微调的强基准模型。

    In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
    
[^18]: 高压缩比训练后模型尺寸压缩的有损和无损方法

    Lossy and Lossless (L$^2$) Post-training Model Size Compression. (arXiv:2308.04269v1 [cs.CV])

    [http://arxiv.org/abs/2308.04269](http://arxiv.org/abs/2308.04269)

    提出了一种将有损和无损压缩结合的训练后模型尺寸压缩方法，通过统一的参数化权重变换和引入可微分计数器来实现高效高压缩比的模型压缩。

    

    深度神经网络在各种视觉任务中取得了显著的性能，并被广泛应用。然而，它们庞大的尺寸给传输和存储带来了很大不便。许多先前的研究探索了模型尺寸压缩的方法。然而，这些研究常常独立地处理各种有损和无损压缩方法，导致在高效实现高压缩比方面存在挑战。本文提出了一种将有损和无损压缩以统一方式结合的训练后模型尺寸压缩方法。我们首先提出了一个统一的参数化权重变换，确保可以以训练后的方式联合执行不同的有损压缩方法。然后，引入了一个专门的可微分计数器来引导有损压缩的优化，以达到更适合后续无损压缩的点。此外，我们的方法可以轻松控制所需的全局压缩比，并为不同的层分配自适应比例。

    Deep neural networks have delivered remarkable performance and have been widely used in various visual tasks. However, their huge size causes significant inconvenience for transmission and storage. Many previous studies have explored model size compression. However, these studies often approach various lossy and lossless compression methods in isolation, leading to challenges in achieving high compression ratios efficiently. This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. We first propose a unified parametric weight transformation, which ensures different lossy compression methods can be performed jointly in a post-training manner. Then, a dedicated differentiable counter is introduced to guide the optimization of lossy compression to arrive at a more suitable point for later lossless compression. Additionally, our method can easily control a desired global compression ratio and allocate adaptive ratios for
    
[^19]: 知识蒸馏的师生架构：综述

    Teacher-Student Architecture for Knowledge Distillation: A Survey. (arXiv:2308.04268v1 [cs.LG])

    [http://arxiv.org/abs/2308.04268](http://arxiv.org/abs/2308.04268)

    本文综述了知识蒸馏的师生架构，在多个蒸馏目标领域中取得了有效并广泛的应用，包括知识压缩、知识扩展、知识适应和知识增强，通过轻量化和通用化的学生网络实现多个蒸馏目标。

    

    尽管深度神经网络（DNN）在许多领域的大规模问题中展现出强大的解决能力，但由于参数庞大，这种DNN很难应用于现实世界的系统中。为解决这个问题，提出了师生架构，其中简单的学生网络只有少量参数，却能达到与拥有许多参数的深度师傅网络相当的性能。最近，师生架构已被广泛应用于知识蒸馏的多个目标领域，包括知识压缩、知识扩展、知识适应和知识增强。借助师生架构，目前的研究能够通过轻量化和通用化的学生网络实现多个蒸馏目标。与现有的主要关注知识压缩的蒸馏调查不同，本调查首次探讨了跨多个蒸馏目标的师生架构。

    Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This surv
    
[^20]: FLIRT: 反馈循环背景下的红队行动

    FLIRT: Feedback Loop In-context Red Teaming. (arXiv:2308.04265v1 [cs.AI])

    [http://arxiv.org/abs/2308.04265](http://arxiv.org/abs/2308.04265)

    本论文提出了一个自动红队行动框架，通过反馈循环和背景学习来评估和暴露生成模型的漏洞，特别是对不安全和不适当内容的生成。对于文本到图像模型，采用不同的背景攻击策略可以学习出有效和多样化的对抗提示。相比基线方法，该策略在暴露漏洞方面更加有效，甚至在模型增加安全功能的情况下仍然能够发现漏洞。该框架也适用于红队行动文本到文本模型。

    

    警告：本论文内容可能不合适或冒犯人。随着生成模型在各种应用中可供公众使用，测试和分析这些模型的漏洞已成为一项优先任务。在这里，我们提出了一个自动红队行动框架，对给定模型进行评估，并暴露其对不安全和不适当内容生成的漏洞。我们的框架使用反馈循环中的背景学习来进行红队行动，并激发模型生成不安全内容。我们提出了不同的背景攻击策略，用于自动学习用于文本到图像模型的有效和多样化的对抗提示。我们的实验表明，与基线方法相比，我们提出的策略在暴露稳定扩散(SD)模型的漏洞时显著更有效，即使后者采用了安全功能的增强。此外，我们证明了所提出的框架对于红队行动文本到文本模型也是有效的。

    Warning: this paper contains content that may be inappropriate or offensive.  As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text mo
    
[^21]: MindDiffuser: 从人脑活动中控制图像重建的语义和结构扩散

    MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2308.04249v1 [cs.CV])

    [http://arxiv.org/abs/2308.04249](http://arxiv.org/abs/2308.04249)

    MindDiffuser是一个用于从人脑活动中重建图像的两阶段模型，通过结合语义和结构信息实现精确可控的图像重建。

    

    从脑电记录中重建视觉刺激是一个有意义且具有挑战性的任务。特别是在推动脑机接口的进展和利用方面，实现精确可控的图像重建具有重要意义。尽管复杂图像重建技术有了进展，但如何在图像刺激中实现语义（概念和对象）和结构（位置、方向和大小）的统一对齐仍然是一个挑战。为了解决这个问题，我们提出了一个名为MindDiffuser的两阶段图像重建模型。在第一阶段，将从fMRI解码得到的VQ-VAE潜在表示和CLIP文本嵌入传入稳定扩散，生成包含语义信息的初步图像。在第二阶段，利用从fMRI解码得到的CLIP视觉特征作为监督信息，通过反向传播不断调整第一阶段解码得到的两个特征向量，以实现对齐。

    Reconstructing visual stimuli from brain recordings has been a meaningful and challenging task. Especially, the achievement of precise and controllable image reconstruction bears great significance in propelling the progress and utilization of brain-computer interfaces. Despite the advancements in complex image reconstruction techniques, the challenge persists in achieving a cohesive alignment of both semantic (concepts and objects) and structure (position, orientation, and size) with the image stimuli. To address the aforementioned issue, we propose a two-stage image reconstruction model called MindDiffuser. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into Stable Diffusion, which yields a preliminary image that contains semantic information. In Stage 2, we utilize the CLIP visual feature decoded from fMRI as supervisory information, and continually adjust the two feature vectors decoded in Stage 1 through backpropagation to alig
    
[^22]: 使用词嵌入进行词汇对齐

    Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])

    [http://arxiv.org/abs/2308.04248](http://arxiv.org/abs/2308.04248)

    这篇论文提出了一种使用大型口语语言模型对定位结果与字幕进行对齐的方法，解决了手语翻译数据中字幕和手势不匹配的问题。

    

    捕捉和注释手语数据集是一项耗时且昂贵的过程。目前的数据集规模远远不足以成功训练无约束的手语翻译模型。因此，研究已转向电视广播内容作为大规模训练数据的来源，包括手语翻译者和相关音频字幕。然而，缺乏手语注释限制了这些数据的可用性，并导致了自动注释技术（如手语定位）的发展。这些定位与视频对齐，而不是与字幕对齐，这往往导致字幕和定位的手势不匹配。在本文中，我们提出了一种使用大型口语语言模型将定位与其对应字幕对齐的方法。使用单一模态意味着我们的方法在计算上开销小，并且可以与现有的定位技术结合使用。我们定量地证明了该方法的有效性。

    Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effective
    
[^23]: AutoPCF:利用大型语言模型进行高效的产品碳足迹核算

    AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models. (arXiv:2308.04241v1 [cs.AI])

    [http://arxiv.org/abs/2308.04241](http://arxiv.org/abs/2308.04241)

    本研究提出了一种利用大型语言模型和深度学习算法的自动化AI驱动的PCF核算框架AutoPCF，该框架具有实现产品碳足迹的自动建模和估算的潜力。

    

    产品碳足迹（PCF）对于减缓供应链的碳排放至关重要，它衡量了产品生命周期中所有活动所导致的直接和间接温室气体排放量。然而，PCF核算通常需要专业知识和大量时间来构建生命周期模型。在本研究中，我们测试比较了五个大型语言模型（LLMs）在建模产品的“摇篮到大门”生命周期并生成输入和输出的库存数据方面的应用能力，揭示了它们作为一个广义PCF知识数据库的局限性。通过利用LLMs，我们提出了一种自动化AI驱动的PCF核算框架，称为AutoPCF，该框架还应用深度学习算法自动匹配计算参数，并最终计算PCF。利用AutoPCF框架对三个案例产品的碳足迹进行估算的结果表明，它在实现PCF的自动建模和估算方面具有潜力。

    The product carbon footprint (PCF) is crucial for decarbonizing the supply chain, as it measures the direct and indirect greenhouse gas emissions caused by all activities during the product's life cycle. However, PCF accounting often requires expert knowledge and significant time to construct life cycle models. In this study, we test and compare the emergent ability of five large language models (LLMs) in modeling the 'cradle-to-gate' life cycles of products and generating the inventory data of inputs and outputs, revealing their limitations as a generalized PCF knowledge database. By utilizing LLMs, we propose an automatic AI-driven PCF accounting framework, called AutoPCF, which also applies deep learning algorithms to automatically match calculation parameters, and ultimately calculate the PCF. The results of estimating the carbon footprint for three case products using the AutoPCF framework demonstrate its potential in achieving automatic modeling and estimation of PCF with a large
    
[^24]: 基于符合预测的可靠不确定性量化的无线通道联合推理

    Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction. (arXiv:2308.04237v1 [cs.IT])

    [http://arxiv.org/abs/2308.04237](http://arxiv.org/abs/2308.04237)

    本论文提出了一种基于联合符合预测的无线通道联合推理方法，通过设备到服务器的通信提高了服务器的推理决策的可靠性和准确性。

    

    在设备和服务器共享预训练模型的设置中，服务器希望根据模型对新输入进行推理。设备可以通过共同的无线信道与服务器通信，并且可以访问以前未用于训练的数据。如果设备无法访问新输入，设备到服务器的通信是否可以提高服务器的推理决策质量？最近的研究引入了联合符合预测（CP），利用设备到服务器的通信来提高服务器决策的可靠性。在联合CP中，设备向服务器传递关于本地数据上共享的预训练模型损失的信息，服务器利用这些信息来校准一个决策区间，以便其在预定义的目标可靠性水平下保证包含正确答案。

    Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a 
    
[^25]: 对GNN模型基于图Attention的解释的语义解释和验证

    Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models. (arXiv:2308.04220v1 [cs.LG])

    [http://arxiv.org/abs/2308.04220](http://arxiv.org/abs/2308.04220)

    本论文提出了一种方法来在GNN模型中增强可解释性，通过引入语义关注和建立特征重要性权重与模型准确性之间的相关性。这对于图深度学习任务具有重要意义。

    

    在这项工作中，我们提出了一种方法来研究在图神经网络（GNN）模型中应用语义关注以增强可解释性，引入语义信息的扰动，并建立预测特征重要性权重与模型准确性之间的相关性。图深度学习（GDL）已经成为一种应用于场景解释等任务的有前途的领域，利用灵活的图结构来简洁地描述复杂的特征和关系。由于传统的解释性AI（XAI）中使用的解释方法不能直接应用于这种结构，因此引入了图特定的方法。注意力机制在估计深度学习模型中输入特征的重要性方面表现出了很好的效果，因此先前已经使用它们为GNN预测提供基于特征的解释。基于这些见解，我们扩展了现有的基于注意力的图解释方法，研究了使用语义信息的图Attention方法。

    In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use o
    
[^26]: 实时作曲辅助的混合检索增强生成

    Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])

    [http://arxiv.org/abs/2308.04215](http://arxiv.org/abs/2308.04215)

    提出了一种混合检索增强生成的框架，通过将云模型的检索增强内存整合到客户端模型中，实现实时响应的作曲辅助。

    

    检索增强模型在提升传统语言模型的上下文理解、整合私人数据和减少幻觉方面显示出了潜力。然而，应用于需要实时响应的任务（如作曲辅助）时，检索增强的大型语言模型所需的处理时间存在挑战。为了克服这一限制，我们提出了Hybrid Retrieval-Augmented Generation (HybridRAG)框架，利用了将客户端模型和云模型结合起来的混合设置。HybridRAG通过异步生成的检索增强内存，将大型语言模型（LLM）在云端生成的检索增强内存整合到客户端模型中。通过整合这种检索增强内存，客户端模型能够生成高效的响应，从LLM的能力中受益。此外，通过异步内存集成，客户端模型能够实时响应用户请求，无需等待云端处理。

    Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
    
[^27]: 《一种差分Datalog解释器》

    A Differential Datalog Interpreter. (arXiv:2308.04214v1 [cs.DB])

    [http://arxiv.org/abs/2308.04214](http://arxiv.org/abs/2308.04214)

    本文研究了差分Datalog解释器的材料化性能，特别是增量维护和工作分配方面的表现。

    

    Datalog引擎的核心推理任务是材料化，即在数据库中评估Datalog程序并将其物理纳入数据库本身。计算它的事实上的方法是通过递归应用推理规则。由于这是一个昂贵的操作，Datalog引擎必须提供增量材料化，即将计算调整到新数据上，而不是从头开始。其中一个主要的缺陷就是删除数据比添加数据复杂得多，因为必须考虑所有可能从正在删除的数据中推导出的数据。差分数据流是一种提供有效的增量维护的计算模型，尤其在添加和删除之间拥有相等的性能，并且能够处理迭代数据流的工作分配。在本文中，我们研究了使用三个参考Datalog实现进行材料化的性能情况。

    The core reasoning task for datalog engines is materialization, the evaluation of a datalog program over a database alongside its physical incorporation into the database itself. The de-facto method of computing it, is through the recursive application of inference rules. Due to it being a costly operation, it is a must for datalog engines to provide incremental materialization, that is, to adjust the computation to new data, instead of restarting from scratch. One of the major caveats, is that deleting data is notoriously more involved than adding, since one has to take into account all possible data that has been entailed from what is being deleted. Differential Dataflow is a computational model that provides efficient incremental maintenance, notoriously with equal performance between additions and deletions, and work distribution, of iterative dataflows. In this paper we investigate the performance of materialization with three reference datalog implementations, out of which one is
    
[^28]: 为何添加到什么中？对一种日常解释的分析。

    Adding Why to What? Analyses of an Everyday Explanation. (arXiv:2308.04187v1 [cs.AI])

    [http://arxiv.org/abs/2308.04187](http://arxiv.org/abs/2308.04187)

    该论文研究了在解释技术产品时应用的双重性理论，通过关注架构和相关性，对解释进行了快速结构化和比较。通过分析解释内容和视频回想，探索了解释者如何进行解释的合理化。研究发现...

    

    在可解释性人工智能（XAI）中，重要的是要考虑到，与专业观众的解释不同，当为一般人解释时，不能假设有共同的专业知识。但是这样人与人之间的解释差异很大，很难研究解释的共同点。我们使用了双重性理论，这是一种技术哲学方法，来应对这些挑战。根据该理论，可以通过关注架构（例如算法逻辑）或相关性（例如决策的严重性，推荐的影响等）来解释XAI的决策。我们使用该理论作为分析框架研究了20种游戏解释。我们详细阐述了如何使用该理论快速结构化和比较技术产品的解释。我们通过分析解释内容的结果，并结合视频回想的结果，探索解释者是如何为他们的解释进行合理化的。我们发现，解释者...

    In XAI it is important to consider that, in contrast to explanations for professional audiences, one cannot assume common expertise when explaining for laypeople. But such explanations between humans vary greatly, making it difficult to research commonalities across explanations. We used the dual nature theory, a techno-philosophical approach, to cope with these challenges. According to it, one can explain, for example, an XAI's decision by addressing its dual nature: by focusing on the Architecture (e.g., the logic of its algorithms) or the Relevance (e.g., the severity of a decision, the implications of a recommendation). We investigated 20 game explanations using the theory as an analytical framework. We elaborate how we used the theory to quickly structure and compare explanations of technological artifacts. We supplemented results from analyzing the explanation contents with results from a video recall to explore how explainers justified their explanation. We found that explainers
    
[^29]: 卫生保健辅助聊天机器人：一个简洁的综述

    Assistive Chatbots for healthcare: a succinct review. (arXiv:2308.04178v1 [cs.AI])

    [http://arxiv.org/abs/2308.04178](http://arxiv.org/abs/2308.04178)

    医疗保健领域的辅助聊天机器人在过去10年取得了一些进展，但在患者信任和与人类交互能力方面仍存在挑战。

    

    人工智能在支持医疗服务方面的需求，从最近的全球大流行病来看，从未达到如此之高。在这里，我们回顾了过去10年（2013-2023年）提出的医疗保健人工智能聊天机器人的最新技术。对于人工智能技术的关注是因为它具有提高人机交互质量、减少对人际交互的依赖和节省人力的潜力。我们的综述表明，目前有一些（商业化的）聊天机器人正在被用于患者支持，同时还有一些（非商业化的）聊天机器人正在临床试验阶段。然而，关于这项技术在患者安全和数据保护方面缺乏信任，以及医护人员对其好处缺乏更广泛的认知。此外，患者在与人类相比，对聊天机器人的自然语言处理（NLP）技能表示不满。尽管如此，对于改善医疗服务和患者支持的潜力，辅助聊天机器人仍有许多待解决的问题。

    Artificial Intelligence (AI) for supporting healthcare services has never been more necessitated than by the recent global pandemic. Here, we review the state-of-the-art in AI-enabled Chatbots in healthcare proposed during the last 10 years (2013-2023). The focus on AI-enabled technology is because of its potential for enhancing the quality of human-machine interaction via Chatbots, reducing dependence on human-human interaction and saving man-hours. Our review indicates that there are a handful of (commercial) Chatbots that are being used for patient support, while there are others (non-commercial) that are in the clinical trial phases. However, there is a lack of trust on this technology regarding patient safety and data protection, as well as a lack of wider awareness on its benefits among the healthcare workers and professionals. Also, patients have expressed dissatisfaction with Natural Language Processing (NLP) skills of the Chatbots in comparison to humans. Notwithstanding the r
    
[^30]: 使用知识图谱预测药物相互作用

    Predicting Drug-Drug Interactions Using Knowledge Graphs. (arXiv:2308.04172v1 [cs.AI])

    [http://arxiv.org/abs/2308.04172](http://arxiv.org/abs/2308.04172)

    本文提出了一个名为medicX的端到端框架，通过使用知识图谱和机器学习算法，预测药物的相互作用。最好的表现组合是ComplEx嵌入方法和LSTM网络，达到了95.19%的F1分数。

    

    在过去几十年里，人们对药物的消费和组合比以前更多，导致了药物相互作用（DDIs）的增加。为了预测未知的DDIs，最近的研究开始将知识图谱（KGs）纳入考虑，因为它们能够捕捉实体之间的关系，提供比单个药物属性更好的药物表示。在本文中，我们提出了medicX端到端框架，它将来自公共药物库的多个药物特征集成到KG中，并使用各种翻译、分解和基于神经网络（NN）的KG Embedding（KGE）方法对图中的节点进行嵌入。最终，我们使用机器学习（ML）算法预测未知的DDIs。在不同的翻译和分解型KGE模型中，我们发现表现最佳的组合是ComplEx嵌入方法和Long Short-Term Memory（LSTM）网络，它在基于DrugBank的DDIs数据集上获得了95.19%的F1分数。

    In the last decades, people have been consuming and combining more drugs than before, increasing the number of Drug-Drug Interactions (DDIs). To predict unknown DDIs, recently, studies started incorporating Knowledge Graphs (KGs) since they are able to capture the relationships among entities providing better drug representations than using a single drug property. In this paper, we propose the medicX end-to-end framework that integrates several drug features from public drug repositories into a KG and embeds the nodes in the graph using various translation, factorisation and Neural Network (NN) based KG Embedding (KGE) methods. Ultimately, we use a Machine Learning (ML) algorithm that predicts unknown DDIs. Among the different translation and factorisation-based KGE models, we found that the best performing combination was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network, which obtained an F1-score of 95.19% on a dataset based on the DDIs found in DrugBank vers
    
[^31]: 知识表示与推理中的当前和未来挑战

    Current and Future Challenges in Knowledge Representation and Reasoning. (arXiv:2308.04161v1 [cs.AI])

    [http://arxiv.org/abs/2308.04161](http://arxiv.org/abs/2308.04161)

    知识表示与推理是人工智能领域的一个核心领域，近年来在机器学习和不确定推理等方面得到了进一步发展。该论文总结了一项关于知识表示与推理的研讨会的结果，提出了该领域的起源、目标、里程碑和挑战，以及未来十年的重点优先事项。

    

    知识表示与推理是人工智能领域的一个核心、长期存在且活跃的领域。多年来，它有了显著的发展；近年来，它在机器学习和不确定推理等领域的研究挑战和补充下得到了进一步发展。2022年7月，Dagstuhl的一个Perspectives研讨会就知识表示与推理领域进行了探讨。这个研讨会的目标是描述该领域的最新进展，包括其与其他领域的关系、其优缺点以及对未来进展的建议。我们基于Dagstuhl研讨会上的演讲、小组讨论和讨论开发了这份宣言。它是我们对知识表示的观点的表述：它的起源、目标、里程碑以及当前的重点；它与其他学科的关系，特别是与人工智能的关系；以及它面临的挑战，以及未来十年的关键优先事项。

    Knowledge Representation and Reasoning is a central, longstanding, and active area of Artificial Intelligence. Over the years it has evolved significantly; more recently it has been challenged and complemented by research in areas such as machine learning and reasoning under uncertainty. In July 2022 a Dagstuhl Perspectives workshop was held on Knowledge Representation and Reasoning. The goal of the workshop was to describe the state of the art in the field, including its relation with other areas, its shortcomings and strengths, together with recommendations for future progress. We developed this manifesto based on the presentations, panels, working groups, and discussions that took place at the Dagstuhl Workshop. It is a declaration of our views on Knowledge Representation: its origins, goals, milestones, and current foci; its relation to other disciplines, especially to Artificial Intelligence; and on its challenges, along with key priorities for the next decade.
    
[^32]: 元宇宙中的异构360度视频：差异化强化学习方法

    Heterogeneous 360 Degree Videos in Metaverse: Differentiated Reinforcement Learning Approaches. (arXiv:2308.04083v1 [cs.NI])

    [http://arxiv.org/abs/2308.04083](http://arxiv.org/abs/2308.04083)

    本文提出了一种为异构360度视频设计的服务质量模型，并使用差异化深度强化学习算法进行逐帧优化。通过引入分离输入差异输出（SIDO）和合并输入差异输出（MIDO）两种结构，该方法在异构场景下表现出了有效性。

    

    先进的视频技术推动着未来元宇宙的发展，旨在连接从任何地方的用户。因此，用户的使用场景将更加多样化，导致了非VR和VR 360度视频的混合。本文提出了一种新颖的为具有不同帧率和晕动症要求的异构360度视频设计的服务质量模型。我们提出了一种帧隙结构，并使用自行设计的差异化深度强化学习算法进行逐帧优化。具体而言，我们设计了两种结构，即分离输入差异输出（SIDO）和合并输入差异输出（MIDO），用于处理这种异构场景。我们还进行了全面的实验以证明其有效性。

    Advanced video technologies are driving the development of the futuristic Metaverse, which aims to connect users from anywhere and anytime. As such, the use cases for users will be much more diverse, leading to a mix of 360-degree videos with two types: non-VR and VR 360-degree videos. This paper presents a novel Quality of Service model for heterogeneous 360-degree videos with different requirements for frame rates and cybersickness. We propose a frame-slotted structure and conduct frame-wise optimization using self-designed differentiated deep reinforcement learning algorithms. Specifically, we design two structures, Separate Input Differentiated Output (SIDO) and Merged Input Differentiated Output (MIDO), for this heterogeneous scenario. We also conduct comprehensive experiments to demonstrate their effectiveness.
    
[^33]: 使用轨迹信息的代理梯度进行联邦零阶优化

    Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (arXiv:2308.04077v1 [cs.LG])

    [http://arxiv.org/abs/2308.04077](http://arxiv.org/abs/2308.04077)

    本论文提出了一种使用轨迹信息的代理梯度方法，用于解决联邦零阶优化中的查询和通信效率问题。

    

    联邦优化是一种新兴的范式，广泛应用于联邦学习等实际场景中，它使多个客户端（如边缘设备）能够共同优化一个全局函数。这些客户端不共享本地数据集，通常只共享本地梯度。然而，在许多联邦优化应用中，梯度信息是不可用的，因此引出了联邦零阶优化（ZOO）的范式。现有的联邦ZOO算法存在查询和通信效率的限制，这可以归因于：（a）它们对梯度估计需要大量的函数查询；（b）它们的实际本地更新与预期全局更新存在显著差异。为此，我们引入了基于轨迹信息的梯度代理，它能够利用优化过程中的函数查询历史进行准确且高效的梯度估计。

    Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimatio
    
[^34]: 路径签名在概率轨迹优化中的多样性

    Path Signatures for Diversity in Probabilistic Trajectory Optimisation. (arXiv:2308.04071v1 [cs.RO])

    [http://arxiv.org/abs/2308.04071](http://arxiv.org/abs/2308.04071)

    本文提出了一种基于路径签名的算法，用于解决并行轨迹优化中的模式塌陷问题，并实现更好的全局性能。

    

    运动规划可以被看作是一个轨迹优化问题，其中成本被最小化，以生成轨迹为函数。在具有多个障碍物和复杂几何的复杂环境中，这个优化问题通常很难解决，并容易陷入局部最小值。然而，最近计算硬件的进步使得可以进行并行轨迹优化，其中同时得到多个解，每个解从不同的起始点初始化。不幸的是，如果没有一个策略防止两个解塌陷在一起，简单的并行优化会遭受模式塌陷的问题，降低方法的效率和找到全局解的可能性。在本文中，我们利用最近在粗路径理论方面的进展，设计了一种用于促进多样性的并行轨迹优化算法，从而避免模式塌陷并实现更好的全局特性。我们的方法...

    Motion planning can be cast as a trajectory optimisation problem where a cost is minimised as a function of the trajectory being generated. In complex environments with several obstacles and complicated geometry, this optimisation problem is usually difficult to solve and prone to local minima. However, recent advancements in computing hardware allow for parallel trajectory optimisation where multiple solutions are obtained simultaneously, each initialised from a different starting point. Unfortunately, without a strategy preventing two solutions to collapse on each other, naive parallel optimisation can suffer from mode collapse diminishing the efficiency of the approach and the likelihood of finding a global solution. In this paper we leverage on recent advances in the theory of rough paths to devise an algorithm for parallel trajectory optimisation that promotes diversity over the range of solutions, therefore avoiding mode collapses and achieving better global properties. Our appro
    
[^35]: 通过自适应加权正则化和知识蒸馏提高低标签环境下的对抗鲁棒性

    Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation. (arXiv:2308.04061v1 [cs.LG])

    [http://arxiv.org/abs/2308.04061](http://arxiv.org/abs/2308.04061)

    本文提出了一种适用于标记数据稀缺环境的半监督对抗训练算法，通过引入适用于无标签数据的正则化项和知识蒸馏，实现了对抗鲁棒性的增强，并在实验证明了其显著优势。

    

    对抗鲁棒性是近年来受到广泛关注的研究领域，它与构建可信人工智能密切相关。然而，现有的对抗鲁棒性研究主要集中在有大量标记数据的监督学习环境下。本文研究在标记数据稀缺的半监督对抗训练环境下进行。我们提出了两个鲁棒风险的上界，并通过这两个上界提出了一个适用于无标签数据的正则化项。然后，我们开发了一个半监督对抗训练算法，通过将提出的正则化项与使用半监督学习算法训练的半监督教师模型进行知识蒸馏相结合。我们的实验结果表明，我们提出的算法在性能上具有显著优势，超过了现有算法。尤其是与监督学习算法相比，我们的算法表现出更高的性能。

    Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorit
    
[^36]: SODFormer：使用事件和帧的Transformer进行流式目标检测

    SODFormer: Streaming Object Detection with Transformer Using Events and Frames. (arXiv:2308.04047v1 [cs.CV])

    [http://arxiv.org/abs/2308.04047](http://arxiv.org/abs/2308.04047)

    SODFormer是一个使用事件和帧进行流式目标检测的Transformer模型，通过设计时空Transformer架构和引入基于注意力的融合模块，实现了对目标的连续检测，提高了检测性能。

    

    DAVIS相机通过异步事件和帧的两种互补感知模态逐渐被用于解决主要的目标检测挑战（例如，快速运动模糊和低光）。然而，如何有效地利用丰富的时间线索和融合两个异构的视觉流仍然是一项具有挑战性的工作。为了解决这个问题，我们提出了一种新颖的基于Transformer的流式目标检测器，即SODFormer，它首先整合事件和帧以异步的方式连续地检测目标。我们首先构建了一个大规模的多模态神经形态目标检测数据集（即PKU-DAVIS-SOD），拥有1080.1k个手动标注。然后，我们设计了一个时空Transformer架构，通过端到端的序列预测问题来检测目标，其中新颖的时序Transformer模块利用两个视觉流的丰富时间线索来提高检测性能。最后，引入了一个异步的基于注意力的融合模块。

    DAVIS camera, streaming two complementary sensing modalities of asynchronous events and frames, has gradually been used to address major object detection challenges (e.g., fast motion blur and low-light). However, how to effectively leverage rich temporal cues and fuse two heterogeneous visual streams remains a challenging endeavor. To address this challenge, we propose a novel streaming object detector with Transformer, namely SODFormer, which first integrates events and frames to continuously detect objects in an asynchronous manner. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1k manual labels. Then, we design a spatiotemporal Transformer architecture to detect objects via an end-to-end sequence prediction problem, where the novel temporal Transformer module leverages rich temporal cues from two visual streams to improve the detection performance. Finally, an asynchronous attention-based fusion module is p
    
[^37]: InfeRE：通过推理链逐步生成正则表达式

    InfeRE: Step-by-Step Regex Generation via Chain of Inference. (arXiv:2308.04041v1 [cs.AI])

    [http://arxiv.org/abs/2308.04041](http://arxiv.org/abs/2308.04041)

    InfeRE是一种通过推理链逐步生成正则表达式的新方法，它考虑了生成最终表达式背后的逐步内部文本匹配过程，并引入了自一致性解码机制来提高鲁棒性。实验结果表明，InfeRE在生成正则表达式方面取得了显著的改进。

    

    自然语言描述（NL2RE）生成正则表达式（regexes）的自动化已经成为一个新兴的研究领域。以往的研究将正则表达式视为一个线性的令牌序列，并通过单次自回归生成最终的表达式。然而，他们没有考虑到生成最终结果背后逐步的内部文本匹配过程。这严重影响了神经语言模型生成正则表达式的效能和可解释性。在本文中，我们提出了一种新的范式，称为InfeRE，它将正则表达式的生成分解成一系列逐步推理的过程。为了增强鲁棒性，我们引入了一个自一致性解码机制，它将从不同的模型中采样得到的多个输出进行集成。我们在两个公开可用的数据集NL-RX-Turk和KB13上对InfeRE进行了评估，并将结果与最先进的方法和流行的基于树的生成方法TRANX进行了比较。实验结果表明，InfeRE显著提高了生成正则表达式的性能。

    Automatically generating regular expressions (abbrev. regexes) from natural language description (NL2RE) has been an emerging research area. Prior studies treat regex as a linear sequence of tokens and generate the final expressions autoregressively in a single pass. They did not take into account the step-by-step internal text-matching processes behind the final results. This significantly hinders the efficacy and interpretability of regex generation by neural language models. In this paper, we propose a new paradigm called InfeRE, which decomposes the generation of regexes into chains of step-by-step inference. To enhance the robustness, we introduce a self-consistency decoding mechanism that ensembles multiple outputs sampled from different models. We evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and compare the results with state-of-the-art approaches and the popular tree-based generation approach TRANX. Experimental results show that InfeRE substantially
    
[^38]: 适应无线通信规范信息综合的基础模型

    Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications. (arXiv:2308.04033v1 [cs.NI])

    [http://arxiv.org/abs/2308.04033](http://arxiv.org/abs/2308.04033)

    本文介绍了NextGen Communications Copilot，这是一个用于无线通信规范信息综合的对话式人工智能工具。它采用基础模型，并引入了领域特定的数据库、上下文提取器和反馈机制，能够提供准确且相关的上下文信息，并结合专家反馈和数据贡献工具。在基准数据集的评估中，该系统展示了更多的优势。

    

    理解、开发和研究现代无线通信技术的现有方法涉及耗时且繁琐的过程，需要筛选大量的网页和技术规范文件，收集所需信息并进行综合。本文提出了NextGen Communications Copilot，这是一个用于无线通信规范信息综合的对话式人工智能工具。该系统基于最新的基础模型进展，并包括三个关键的附加组件：一个领域特定的数据库，一个上下文提取器和一个反馈机制。该系统可以从无线技术规范数据库中提取简洁的、与查询相关的上下文信息，并结合专家反馈和数据贡献工具。在使用由专家创建的查询和参考响应的基准数据集进行评估时，该系统展示了更多的优势。

    Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependent contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more 
    
[^39]: 人类情绪的不确定性测量

    Measure of Uncertainty in Human Emotions. (arXiv:2308.04032v1 [cs.AI])

    [http://arxiv.org/abs/2308.04032](http://arxiv.org/abs/2308.04032)

    本研究探讨了情绪分类的不确定性信息展示对人类决策过程的影响，结果显示展示更多的不确定性信息可以增强用户决策的自信度。 (This study investigated the impact of displaying more uncertainty information on human decision making in emotion classification, and the results showed that it can increase users' confidence in decision making.)

    

    许多研究探讨计算机如何能够检测人类展示的情绪并利用这些数据执行不同的任务。然而，很少有研究评估计算机生成情绪分类信息的能力，以帮助用户做出决策或执行任务。这是一个关键领域需要研究，因为它对人与计算机之间的双向交流至关重要。本研究进行了实验，探讨了情绪分类的不确定性信息展示对人类决策过程的影响。结果表明，展示更多的不确定性信息可以帮助用户在做决策时更加自信。

    Many research explore how well computers are able to examine emotions displayed by humans and use that data to perform different tasks. However, there have been very few research which evaluate the computers ability to generate emotion classification information in an attempt to help the user make decisions or perform tasks. This is a crucial area to explore as it is paramount to the two way communication between humans and computers. This research conducted an experiment to investigate the impact of different uncertainty information displays of emotion classification on the human decision making process. Results show that displaying more uncertainty information can help users to be more confident when making decisions.
    
[^40]: Gentopia: 一个用于增强语言模型的协作平台

    Gentopia: A Collaborative Platform for Tool-Augmented LLMs. (arXiv:2308.04030v1 [cs.AI])

    [http://arxiv.org/abs/2308.04030](http://arxiv.org/abs/2308.04030)

    Gentopia是一个协作平台，用于增强语言模型，并具有灵活的定制、协作民主化和综合评估的特性。

    

    增强语言模型（ALM）赋予了大型语言模型使用工具的能力，将其转化为能够进行真实世界交互的智能代理。然而，现有的大部分ALM框架在以下关键特性方面存在不足：灵活的定制化、协作民主化和整体评估。我们提出了Gentopia，一种ALM框架，通过简单的配置实现代理的灵活定制，将各种语言模型、任务格式、提示模块和插件无缝集成到一个统一的范式中。此外，我们建立了Gentpool，一个公共平台，用于注册和共享用户定制的代理。在Gentpool中注册的代理可组合在一起进行代理协作，推动人工智能的民主化。为了确保高质量的代理，Gentbench是Gentpool的一个重要组成部分，旨在全面评估用户定制的代理。

    Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customi
    
[^41]: 生物医学问题回答的Top K相关段落检索

    Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])

    [http://arxiv.org/abs/2308.04028](http://arxiv.org/abs/2308.04028)

    这篇论文提出了一种用于生物医学问题回答的Top K相关段落检索方法，传统的稀疏向量空间模型不适用于这个任务。然而，对于临床领域来说，这个问题还没有得到很好的解决。

    

    问答是一项利用大量文档回答事实性问题的任务。它旨在以自然语言回答用户的问题并提供准确的答案。问答依赖于高效的段落检索来选择候选上下文，传统的稀疏向量空间模型，如TF-IDF或BM25，是事实上的方法。在网络上，没有一篇文章可以提供所有可能的答案，以回答用户所提出的问题。现有的稠密段落检索模型已经对维基百科2018年12月20日的倾销进行了训练，用作回答问题的源文档。问答系统在多个开放领域和机器理解系统上取得了重大进展，使用了大规模的注释数据集。然而，在临床领域，这个问题仍然相对未被探索。根据多项调查，无法从维基百科准确回答生物医学问题。

    Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia 
    
[^42]: AgentSims: 一个用于大型语言模型评估的开源沙盒平台

    AgentSims: An Open-Source Sandbox for Large Language Model Evaluation. (arXiv:2308.04026v1 [cs.AI])

    [http://arxiv.org/abs/2308.04026](http://arxiv.org/abs/2308.04026)

    AgentSims是一个开放源码的沙盒平台，用于评估大型语言模型的能力。它通过任务驱动的评估方法，解决了现有评估方法的局限性，并提供了易于使用的界面，供研究人员测试特定能力。演示版本可在https://agentsims.com获得。

    

    在 ChatGPT 等大型语言模型占据主导地位的情况下，如何评估这些模型的能力成为一个开放问题。现有的评估方法存在以下问题：（1）有限的评估能力，（2）脆弱的评估基准，（3）不客观的评估指标。我们提出了一种基于任务的评估方法，即在模拟环境中让语言模型代理完成任务，这是解决上述问题的一个综合性解决方案。我们推出了 AgentSims，这是一个易于使用的基础设施，供各学科的研究人员测试他们感兴趣的特定能力。研究人员可以通过交互式图形界面添加代理和建筑物来构建评估任务，也可以通过几行代码部署和测试新的支持机制，例如记忆、规划和工具使用系统。我们的演示版本可以在 https://agentsims.com上获得。

    With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .
    
[^43]: MSAC：用于语音情感识别的多语音属性控制方法

    MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition. (arXiv:2308.04025v1 [cs.SD])

    [http://arxiv.org/abs/2308.04025](http://arxiv.org/abs/2308.04025)

    本研究针对语音情感识别(SER)提出了MSAC方法，通过构建新颖的CNN-based SER模型和多语音属性控制方法MSAC，实现了对情感的更精细控制和捕捉，从而提升了SER的可靠性和效果。

    

    尽管取得了显著进展，但由于情感属性的复杂性和歧义性，尤其是在自然环境下，语音情感识别（SER）仍然具有挑战性。而当前的研究主要关注识别和泛化能力，本文首次探索了SER方法的可靠性，并研究了如何通过各种语音属性的数据分布来建模语音情感。具体来说，我们首先构建了一种新颖的基于CNN的SER模型，采用了加性边界最大化软件最大化损失函数，扩大了不同类别特征之间的距离，从而增强了它们的区分能力。其次，我们提出了一种新颖的多语音属性控制方法MSAC，以明确控制语音属性，使模型受情感无关属性的影响较小，并捕捉到更细粒度的情感相关特征。第三，我们首次尝试测试和分析了所提出的SER工作流程的可靠性。

    Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using 
    
[^44]: 不平衡分类和RL探索的范围损失

    Scope Loss for Imbalanced Classification and RL Exploration. (arXiv:2308.04024v1 [cs.LG])

    [http://arxiv.org/abs/2308.04024](http://arxiv.org/abs/2308.04024)

    本文介绍了范围损失，这是一种新的适用于强化学习和监督分类的损失函数，它能够解决探索利用权衡和数据集不平衡问题，并在实验中表现出优于其他损失函数的性能。

    

    我们展示了强化学习问题和监督分类问题之间的等价性。我们因此将强化学习中的探索利用权衡等同于监督分类中的数据集不平衡问题，并找出了它们在解决方式上的相似之处。通过对上述问题的分析，我们提出了一种新的适用于强化学习和监督分类的损失函数- 范围损失。范围损失可以调整梯度，以防止过度利用和数据集不平衡引起的性能损失，而无需进行任何调整。我们在一系列基准强化学习任务和一个偏斜的分类数据集上测试了范围损失，结果显示范围损失优于其他损失函数。

    We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.
    
[^45]: 通过对抗攻击改进半监督学习的性能

    Improving Performance of Semi-Supervised Learning by Adversarial Attacks. (arXiv:2308.04018v1 [cs.LG])

    [http://arxiv.org/abs/2308.04018](http://arxiv.org/abs/2308.04018)

    本文提出了一个名为SCAR的通用框架，通过对预训练模型进行对抗攻击来改进半监督学习算法的性能，显著提高了图像分类的准确性。

    

    半监督学习算法是建立在一个现实的假设上，即访问大量标记数据很困难。本研究提出了一个名为SCAR的通用框架，用于改进最近的半监督学习算法的性能。通过对预训练模型进行对抗攻击，我们的框架在图像分类方面取得了显著进展。我们介绍了对抗攻击如何成功选择高置信度的未标记数据，并与当前预测的进行标记。在CIFAR10数据集上，三个最近的半监督学习算法与SCAR框架相结合，显著提高了图像分类的性能。

    Semi-supervised learning (SSL) algorithm is a setup built upon a realistic assumption that access to a large amount of labeled data is tough. In this study, we present a generalized framework, named SCAR, standing for Selecting Clean samples with Adversarial Robustness, for improving the performance of recent SSL algorithms. By adversarially attacking pre-trained models with semi-supervision, our framework shows substantial advances in classifying images. We introduce how adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions. On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification.
    
[^46]: 多粒度注意力模型用于群组推荐

    Multi-Granularity Attention Model for Group Recommendation. (arXiv:2308.04017v1 [cs.IR])

    [http://arxiv.org/abs/2308.04017](http://arxiv.org/abs/2308.04017)

    多粒度注意力模型为群组推荐提供了一种新方法，通过利用多个粒度层次来揭示群组成员的潜在偏好并减少推荐噪声。

    

    群组推荐是基于共同兴趣、偏好和特征为一组用户提供个性化推荐的方法。目前的研究已经探索了不同的方法，用于整合个人偏好并做出有利于整个群组的集体决策。然而，大部分方法过于依赖行为丰富的用户，忽视了行为相对稀疏的用户的潜在偏好，导致个人兴趣的学习不足。为了解决这个问题，我们提出了多粒度注意力模型（MGAM），这是一种利用多个粒度层次（即子集、群组和超集）来揭示群组成员潜在偏好和减少推荐噪声的新方法。特别地，我们提出了一个子集偏好提取模块，通过整合用户与物品的交互信息和使用层次化机制，增强了用户潜在的子集级别偏好的表示。

    Group recommendation provides personalized recommendations to a group of users based on their shared interests, preferences, and characteristics. Current studies have explored different methods for integrating individual preferences and making collective decisions that benefit the group as a whole. However, most of them heavily rely on users with rich behavior and ignore latent preferences of users with relatively sparse behavior, leading to insufficient learning of individual interests. To address this challenge, we present the Multi-Granularity Attention Model (MGAM), a novel approach that utilizes multiple levels of granularity (i.e., subsets, groups, and supersets) to uncover group members' latent preferences and mitigate recommendation noise. Specially, we propose a Subset Preference Extraction module that enhances the representation of users' latent subset-level preferences by incorporating their previous interactions with items and utilizing a hierarchical mechanism. Additionall
    
[^47]: 使用结构化背景知识和演绎推理理解CNN隐藏神经元激活

    Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])

    [http://arxiv.org/abs/2308.03999](http://arxiv.org/abs/2308.03999)

    本文提供了一种使用结构化背景知识和演绎推理的方法，用于解释CNN隐藏神经元的激活。该方法能够提供有意义的解释，解决了深度学习系统黑盒特性的问题。

    

    Explainable AI中的一个主要挑战是准确解释隐藏神经元的激活：准确的解释将为深度学习系统内部检测到的输入相关内容提供洞察力，揭示深度学习系统的黑盒特性。现有技术表明，在某些情况下，隐藏节点的激活可以被人类理解，但是对隐藏神经元激活的解释进行假设和验证的系统化自动化方法尚未充分研究。在本文中，我们提供了这样一种方法，并证明它提供了有意义的解释。我们的方法基于使用大规模的背景知识，从维基百科概念层次结构中筛选出的约200万个类别，以及一个称为概念归纳的符号推理方法，这种方法最初是为语义Web领域的应用而开发的。

    A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, de-mystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Ou
    
[^48]: 合作式多类型多智能体深度强化学习在空天地一体化网络资源管理中的应用

    Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks. (arXiv:2308.03995v1 [cs.MA])

    [http://arxiv.org/abs/2308.03995](http://arxiv.org/abs/2308.03995)

    本文提出了一种合作式多类型多智能体深度强化学习方法，应用于空天地一体化网络的资源管理，实验结果表明其有效性和潜在价值。

    

    空天地一体化网络（SAGIN）将包括低地球轨道卫星（LEO）、无人机（UAV）和地面用户（GU）在内的异构设备整合起来，有望推动智能城市应用的发展。然而，SAGIN的资源管理是一个挑战，需要紧急研究，因为不恰当的资源管理会导致数据传输质量差，进而影响智能城市的服务。本文开发了一个综合的SAGIN系统，包括五种不同的通信链路，并提出了一种高效的合作式多类型多智能体深度强化学习（CMT-MARL）方法来解决资源管理问题。实验结果突出了所提出的CMT-MARL的有效性，如整体传输速率和传输成功率等关键性能指标。这些结果强调了未来SAGIN实施的潜在价值和可行性。

    The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of the proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.
    
[^49]: AI聊天机器人作为多角色教育代理：改变计算机科学教育中的参与度

    AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in CS Education. (arXiv:2308.03992v1 [cs.AI])

    [http://arxiv.org/abs/2308.03992](http://arxiv.org/abs/2308.03992)

    本研究研究了使用人工智能驱动的多角色聊天机器人作为提升计算机科学教育学习体验和增强参与度的手段。通过四个不同的角色和探究式学习范式，满足学生的内在心理需求，并通过与人类导师和单个聊天机器人的条件进行比较，证实了该系统的有效性。

    

    本研究调查了使用人工智能 (AI) 驱动的多角色聊天机器人作为增强学习体验和促进计算机科学教育参与度的手段。利用基于设计的研究方法，我们开发、实施和评估了一个新颖的学习环境，其中包含四个不同的聊天机器人角色：导师机器人、同伴机器人、职业咨询机器人和情感支持机器人。这些角色基于自我决定理论的原则设计，满足学习者的三种内在心理需求：能力、自主性和关联性。此外，该系统采用了探究式学习范式，鼓励学生提问、寻求解决方案并探索他们的好奇心。我们在一个高等教育环境中以200名参与学生为期一个月的时间测试了该系统，并将结果与涉及人类导师和单个聊天机器人的条件进行了比较。我们的研究采用了混合方法的方法，包括数量和质量的数据收集和分析。

    This study investigates the use of Artificial Intelligence (AI)-powered, multi-role chatbots as a means to enhance learning experiences and foster engagement in computer science education. Leveraging a design-based research approach, we develop, implement, and evaluate a novel learning environment enriched with four distinct chatbot roles: Instructor Bot, Peer Bot, Career Advising Bot, and Emotional Supporter Bot. These roles, designed around the tenets of Self-Determination Theory, cater to the three innate psychological needs of learners - competence, autonomy, and relatedness. Additionally, the system embraces an inquiry-based learning paradigm, encouraging students to ask questions, seek solutions, and explore their curiosities.  We test this system in a higher education context over a period of one month with 200 participating students, comparing outcomes with conditions involving a human tutor and a single chatbot. Our research utilizes a mixed-methods approach, encompassing quan
    
[^50]: NEOLAF,一种基于LLM的神经符号认知架构

    NEOLAF, an LLM-powered neural-symbolic cognitive architecture. (arXiv:2308.03990v1 [cs.AI])

    [http://arxiv.org/abs/2308.03990](http://arxiv.org/abs/2308.03990)

    NEOLAF是一种基于LLM的神经符号认知架构，相比于纯连接主义和纯符号方法，它具有解释性、增量学习、协作和分布式学习、人工协助和自我改进的优势。通过实验证明NEOLAF具有优秀的学习能力，有潜力推动认知架构和自适应教学系统领域的发展。

    

    本文介绍了永不停止的开放学习自适应框架(NEOLAF)，这是一种集成了神经符号认知架构，用于建模和构建智能代理的方法。NEOLAF框架相比纯连接主义和纯符号方法更具有解释性、增量学习、高效性、协作和分布式学习、人工协助和自我改进的优势。本文还提出了一个引人注目的实验，其中一个作为问题解决代理的NEOLAF代理被输入了来自开源MATH数据集的复杂数学问题。结果表明NEOLAF具有优秀的学习能力，并有潜力在认知架构和自适应教学系统领域引起革命。

    This paper presents the Never Ending Open Learning Adaptive Framework (NEOLAF), an integrated neural-symbolic cognitive architecture that models and constructs intelligent agents. The NEOLAF framework is a superior approach to constructing intelligent agents than both the pure connectionist and pure symbolic approaches due to its explainability, incremental learning, efficiency, collaborative and distributed learning, human-in-the-loop enablement, and self-improvement. The paper further presents a compelling experiment where a NEOLAF agent, built as a problem-solving agent, is fed with complex math problems from the open-source MATH dataset. The results demonstrate NEOLAF's superior learning capability and its potential to revolutionize the field of cognitive architectures and self-improving adaptive instructional systems.
    
[^51]: SimplyRetrieve: 一款私密且轻量级的基于检索为中心的生成型AI工具

    SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])

    [http://arxiv.org/abs/2308.03983](http://arxiv.org/abs/2308.03983)

    SimplyRetrieve是一款私密且轻量级的生成型AI工具，使用了基于检索的生成方法，通过无需额外模型微调将私有数据集成进生成型AI系统，并提供了一个本地化、轻量级和用户友好的界面，以方便用户探索RCG在提升生成型AI性能方面的潜力。

    

    近年来，基于大型语言模型（LLM）的生成型AI系统取得了显著的进展。将知识检索架构整合进公开可用的预训练LLM中，可以无需额外的模型微调，将私有数据无缝集成进生成型AI系统。此外，检索为中心的生成（RCG）方法，作为一种有前景的未来研究方向，明确区分了LLM和检索器在上下文解释和知识记忆中的作用，潜在地导致更高效的实现。SimplyRetrieve是一个面向机器学习社区的开源工具，旨在为这些先进技术提供一个本地化、轻量级和用户友好的界面。SimplyRetrieve提供基于GUI和API的RCG平台，辅以私密知识库构建器和检索调优模块。通过利用这些功能，用户可以探索RCG在改进生成型AI性能方面的潜力。

    Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI per
    
[^52]: CheXFusion: 用Transformer有效地融合多视角特征进行长尾胸部X射线分类

    CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification. (arXiv:2308.03968v1 [cs.CV])

    [http://arxiv.org/abs/2308.03968](http://arxiv.org/abs/2308.03968)

    本文介绍了一种名为CheXFusion的基于Transformer的融合模块，可用于长尾胸部X射线分类。该模块利用自注意力和交叉注意力机制，有效地聚合多视角特征并考虑标签共现性。通过数据平衡和自训练方法，该解决方案在MIMIC-CXR测试集上取得了0.372的mAP，成为竞赛的冠军。本研究强调了在医学图像分类中考虑多视角设置、类别不平衡和标签共现性的重要性。

    

    医学图像分类由于疾病的长尾分布、诊断结果的共现性和每个研究或患者可用的多视角而面临独特的挑战。本文介绍了我们对ICCV CVAMD 2023共享任务CX-LT：胸部X射线的多标签长尾分类的解决方案。我们的方法引入了一个基于Transformer的融合模块CheXFusion，该模块能够有效地聚合多视角特征，并在考虑标签共现性的同时采用自注意力和交叉注意力机制进行引导。此外，我们还探索了数据平衡和自训练方法来优化模型的性能。我们的解决方案在MIMIC-CXR测试集中达到了0.372的mAP，获得了竞赛的第一名。我们在任务中取得的成功突显了在医学图像分类中考虑多视角设置、类别不平衡和标签共现性的重要性。

    Medical image classification poses unique challenges due to the long-tailed distribution of diseases, the co-occurrence of diagnostic findings, and the multiple views available for each study or patient. This paper introduces our solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed Classification on Chest X-Rays. Our approach introduces CheXFusion, a transformer-based fusion module incorporating multi-view images. The fusion module, guided by self-attention and cross-attention mechanisms, efficiently aggregates multi-view features while considering label co-occurrence. Furthermore, we explore data balancing and self-training methods to optimize the model's performance. Our solution achieves state-of-the-art results with 0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our success in the task underscores the significance of considering multi-view settings, class imbalance, and label co-occurrence in medical image classification. Publi
    
[^53]: ALFA - 利用各个层次的特征抽象来增强组织病理学图像分类在未知医院中的泛化能力

    ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals. (arXiv:2308.03936v1 [cs.CV])

    [http://arxiv.org/abs/2308.03936](http://arxiv.org/abs/2308.03936)

    ALFA使用各个层次的特征抽象来增强组织病理学图像分类在未知医院中的泛化能力，并通过自监督学习和域对齐实现了对不变特征的提取，以及对参与医院的高度特定特征的表示与分类。

    

    我们提出了一种全面的方法，利用各个层次的特征抽象，旨在提高图像分类在未知医院中的泛化能力。我们的方法结合了基于增强的自监督学习和病理学场景中的常见分布偏移作为前提任务。这使我们能够从训练图像中提取不变特征，而不依赖于训练标签，从而涵盖不同的抽象层次。在进一步的抽象层次上，我们利用域对齐模块来进一步提取在不同训练医院中的不变特征。为了表示参与医院的高度特定特征，我们训练了一个编码器来对医院标签进行分类，而不考虑诊断标签。然后，我们对来自每个编码器的特征进行解缠以最小化冗余并分离特征。这种表示涵盖了广泛的语义特征。

    We propose an exhaustive methodology that leverages all levels of feature abstraction, targeting an enhancement in the generalizability of image classification to unobserved hospitals. Our approach incorporates augmentation-based self-supervision with common distribution shifts in histopathology scenarios serving as the pretext task. This enables us to derive invariant features from training images without relying on training labels, thereby covering different abstraction levels. Moving onto the subsequent abstraction level, we employ a domain alignment module to facilitate further extraction of invariant features across varying training hospitals. To represent the highly specific features of participating hospitals, an encoder is trained to classify hospital labels, independent of their diagnostic labels. The features from each of these encoders are subsequently disentangled to minimize redundancy and segregate the features. This representation, which spans a broad spectrum of semanti
    
[^54]: ChatGPT生物医学生成文本中建立信任的方法：基于本体的知识图谱用于验证疾病-症状关系

    Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])

    [http://arxiv.org/abs/2308.03929](http://arxiv.org/abs/2308.03929)

    本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。

    

    方法：通过创新的方法，我们从真实的医学文献和人工智能生成的内容构建了基于本体的知识图谱。我们的目标是区分事实信息和未经验证的数据。我们收集了两个数据集：一个是使用“人类疾病和症状”查询从生物医学文献中编译的，另一个是由ChatGPT生成的模拟文章。利用这些数据集（PubMed和ChatGPT），我们随机选择了10组每组250个摘要，并使用特定的种子。我们的方法主要是利用疾病本体（DOID）和症状本体（SYMP）构建知识图谱，这是一种强大的数学模型，可以进行无偏差的比较。通过使用我们的事实核查算法和网络中心度指标，我们进行了GPT疾病-症状链接分析，以量化在噪声、假设和重要发现中的事实知识的准确性。结果：通过比较不同ChatGPT知识图谱及其PubMed计数获得的结果，我们发现...

    Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
    
[^55]: ViLP：使用视觉、语言和姿势嵌入进行视频动作识别的知识探索

    ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition. (arXiv:2308.03908v1 [cs.CV])

    [http://arxiv.org/abs/2308.03908](http://arxiv.org/abs/2308.03908)

    本论文提出了一种基于姿势增强的视觉语言模型(VLM)，用于视频动作识别。实验结果表明，该模型能够取得令人期待的准确率，并在两个流行的动作识别数据集上取得了优异的性能。

    

    视频动作识别是一项具有挑战性的任务，由于其固有的复杂性。尽管文献中已经探索了不同的方法，但设计一个统一的框架来识别大量的人类动作仍然是一个具有挑战性的问题。最近，在这个领域中，多模态学习(MML)显示出了令人期待的结果。在文献中，2D骨骼或姿势模态经常被用于这个任务，要么独立使用，要么与视频中存在的视觉信息（RGB模态）结合使用。然而，尽管文本和姿势属性在许多计算机视觉任务中都被证明是有效的，但是尚未探索过姿势、视觉信息和文本属性的组合。在本文中，我们提出了第一个用于VAR的姿势增强的视觉语言模型(VLM)。值得注意的是，我们的方案在两个流行的人类视频动作识别基准数据集UCF-101和HMDB-51上分别达到了92.81%和73.02%的准确率。

    Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even w
    
[^56]: 在设备上的智能助手语言理解

    Intelligent Assistant Language Understanding On Device. (arXiv:2308.03905v1 [cs.CL])

    [http://arxiv.org/abs/2308.03905](http://arxiv.org/abs/2308.03905)

    本论文描述了一种在设备上运行的自然语言理解系统的设计，相比于基于服务器的助手，该系统更加私密、可靠、快速、表达更强、准确性更高。通过分享实践经验，为研究界的未来工作提供参考。

    

    最近，将个人数字助手应用于手机和其他个人设备已成为可能。在本文中，我们描述了一个在设备上运行的自然语言理解系统的设计。与基于服务器的助手相比，该系统更具私密性、可靠性、速度快、表达更强、准确性更高。我们描述了在架构和技术方面做出的关键选择。例如，对话系统文献中的一些方法在部署环境中难以长期维护。我们希望通过分享实践经验，为研究界的未来工作提供参考。

    It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
    
[^57]: FLIPS: 使用智能参与者选择的联邦学习

    FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])

    [http://arxiv.org/abs/2308.03901](http://arxiv.org/abs/2308.03901)

    本文介绍了FLIPS，这是一个用于管理联邦学习中数据和参与者异质性的中间件系统。FLIPS通过标签分布聚类和智能参与者选择，并使用可信执行环境来确保隐私保护。实证评估表明，FLIPS相比随机方法有更好的性能。

    

    本文介绍了FLIPS的设计和实现，这是一个用于管理联邦学习中数据和参与者异质性的中间件系统。特别地，我们研究了标签分布聚类在联邦学习中参与者选择中的好处。FLIPS根据数据的标签分布预先对参与FL训练作业的各方进行聚类，并在FL训练期间确保每个聚类在被选中的参与者中公平地表示。FLIPS可以支持最常见的FL算法，包括FedAvg，FedProx，FedDyn，FedOpt和FedYogi。为了管理平台的异构性和动态资源可用性，FLIPS还结合了一种处理分布式智能社区应用中容量变化的拖累管理机制。标签分布、聚类和参与者选择的隐私通过可信执行环境(TEE)来确保。我们全面的实证评估将FLIPS与随机方法进行了比较。

    This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random p
    
[^58]: 通过未见过的状态增强利用广义化在离线强化学习中

    Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])

    [http://arxiv.org/abs/2308.03882](http://arxiv.org/abs/2308.03882)

    本文提出了一种利用未见状态增强的策略，在离线强化学习中通过基于价值的扰动和过滤，实现了对离线数据之外的状态的利用和泛化。

    

    离线强化学习方法通过对未见过的状态和动作进行保守价值评估来平衡探索和利用。无模型方法会对所有未见过的动作进行惩罚，而有模型方法可以进一步通过模型展开对未见过的状态进行利用。然而，由于两个因素，这些方法在找到离线数据之外的未见过的状态时存在困难：(a)由于级联模型误差，模型的展开范围非常短，(b)模型展开仅以离线数据中观察到的状态为起点。我们放宽了第二个假设，并提出了一种新颖的未见过状态增强策略，以允许学得的模型和价值估计在未见状态中泛化。我们的策略通过对观察到的状态进行基于价值的扰动来找到未见过的状态，然后通过过滤具有过高的启发性不确定性估计（高误差）或过低的（过于相似）

    Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to
    
[^59]: 保护守护者：在线儿童性虐待的自动化分析

    Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse. (arXiv:2308.03880v1 [cs.AI])

    [http://arxiv.org/abs/2308.03880](http://arxiv.org/abs/2308.03880)

    这项研究介绍了一种自动化工具，用于全面分析儿童性虐待报告。通过自动化分析和分类，降低了接触有害内容的风险，并利用多学科团队的专业知识进行更深入的分析，以改善对基本模式和趋势的理解，帮助执法部门和决策者制定针对性的政策和行动。

    

    最近全球范围内对儿童的在线暴力事件有所增加，要求我们予以紧急关注。有关部门需要手动分析滥用投诉以了解犯罪动态并识别模式。然而，这些投诉的手动分析存在挑战，因为在审查过程中暴露分析员接触到有害内容。鉴于这些挑战，我们提出了一种创新的解决方案，即一种专为全面分析儿童性虐待报告而设计的自动化工具。通过自动化分析过程，我们的工具通过对报告在主题、犯罪程度和伤害程度三个维度进行分类，极大地降低了接触有害内容的风险。此外，利用我们多学科团队的专业知识，我们引入了一种新方法来对收集的数据进行注释，实现对报告的更深入分析。这种方法改善了对基本模式和趋势的理解，使执法部门和决策者能够创建

    Online violence against children has increased globally recently, demanding urgent attention. Competent authorities manually analyze abuse complaints to comprehend crime dynamics and identify patterns. However, the manual analysis of these complaints presents a challenge because it exposes analysts to harmful content during the review process. Given these challenges, we present a novel solution, an automated tool designed to analyze children's sexual abuse reports comprehensively. By automating the analysis process, our tool significantly reduces the risk of exposure to harmful content by categorizing the reports on three dimensions: Subject, Degree of Criminality, and Damage. Furthermore, leveraging our multidisciplinary team's expertise, we introduce a novel approach to annotate the collected data, enabling a more in-depth analysis of the reports. This approach improves the comprehension of fundamental patterns and trends, enabling law enforcement agencies and policymakers to create 
    
[^60]: 在教育中信任语言模型

    Trusting Language Models in Education. (arXiv:2308.03866v1 [cs.CL])

    [http://arxiv.org/abs/2308.03866](http://arxiv.org/abs/2308.03866)

    本研究在教育中使用语言模型，提出了使用XGBoost和BERT的结合来校准语言模型的置信度，通过基于注意力机制的特征来输出校正的概率。

    

    语言模型在教育领域被广泛应用。尽管现代深度学习模型在问答任务上表现出色，但有时会出现错误。为了避免向学生展示错误答案，重要的是校准这些模型的置信度 - 即预测概率。在我们的工作中，我们提出使用一种基于注意力机制的特征的XGBoost在BERT之上输出校正的概率。我们的假设是注意力流中包含的不确定性水平与模型的响应质量相关。

    Language Models are being widely used in Education. Even though modern deep learning models achieve very good performance on question-answering tasks, sometimes they make errors. To avoid misleading students by showing wrong answers, it is important to calibrate the confidence - that is, the prediction probability - of these models. In our work, we propose to use an XGBoost on top of BERT to output the corrected probabilities, using features based on the attention mechanism. Our hypothesis is that the level of uncertainty contained in the flow of attention is related to the quality of the model's response itself.
    
[^61]: 移动供应：推荐系统的最后一块拼图

    Mobile Supply: The Last Piece of Jigsaw of Recommender System. (arXiv:2308.03855v1 [cs.IR])

    [http://arxiv.org/abs/2308.03855](http://arxiv.org/abs/2308.03855)

    本研究提出了一种新的推荐系统模块，称为移动供应，旨在解决分页机制问题。通过在用户设备上部署推荐算法，可以有效减少数据传输延迟和提升用户的沉浸式体验。

    

    推荐系统是在线平台的基本功能。随着手机计算能力的发展，一些研究人员已经将推荐算法部署在用户设备上，以解决数据传输延迟和分页机制等问题。然而，现有的边缘端移动排名不能完全解决分页机制问题。移动排名只能对当前页面上的项目进行排序，所以如果只调用一两次是不起作用的。此外，在用户查看了当前页面上的感兴趣的项目后，用户会刷新页面获取新的项目。这会使移动排名模型做很多无用功，影响用户的沉浸式体验。为了解决分页机制问题，我们在推荐系统的流水线中提出了一个全新的模块，称为移动供应。

    Recommendation system is a fundamental functionality of online platforms. With the development of computing power of mobile phones, some researchers have deployed recommendation algorithms on users' devices to solve the problems of data transmission delay and pagination mechanism. However, the existing edge-side mobile rankings cannot completely solve the problem of pagination mechanism. The mobile rankings can only sort the items on the current page, so it will not work if it is called once or twice. Besides, after the user has viewed the items of interest to the user on the current page, the user refresh to get a new page of items. This will make the mobile ranking model do a lot of useless work and affect the user's immersive experience. In order to solve the pagination mechanism problem, we propose a completely new module in the pipeline of recommender named Mobile Supply. The pipeline of recommender system is extended to "retrival->pre-ranking->ranking->re-ranking->Mobile Supply->
    
[^62]: 通过声明式众包重新审视提示工程

    Revisiting Prompt Engineering via Declarative Crowdsourcing. (arXiv:2308.03854v1 [cs.DB])

    [http://arxiv.org/abs/2308.03854](http://arxiv.org/abs/2308.03854)

    本研究通过借鉴声明式众包的思想，提出了一种声明式提示工程的方法，旨在解决大型语言模型在数据处理中的质量优化和成本控制问题。

    

    大型语言模型(LLM)在理解和生成文本数据方面具有极大的能力，但也容易脆弱和错误。近年来涌现了以所谓的提示工程为中心的工具包和技术方法，通过一系列提示向LLM提出要求。然而，在LLM驱动的数据处理工作流中，优化质量并保持成本有限是一项繁琐的手动过程。我们提出了一个声明式提示工程的愿景。我们将LLM视为众包工人，并借鉴了声明式众包文献中的思想，包括利用多种提示策略，确保内部一致性以及探索混合LLM非LLM方法，以使提示工程过程更加原则性。对排序、实体解析和插补的初步案例研究展示了我们方法的潜力。

    Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach
    
[^63]: 《高分辨率显著目标检测的循环多尺度Transformer》

    Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection. (arXiv:2308.03826v1 [cs.CV])

    [http://arxiv.org/abs/2308.03826](http://arxiv.org/abs/2308.03826)

    本研究提出了一个循环多尺度Transformer网络，用于高分辨率显著目标检测，同时引入了一个新的HRS10K数据集，该数据集是目前规模最大的用于HRSOD任务的数据集。

    

    显著目标检测（SOD）旨在识别和分割图像或视频中最显著的对象。随着成像设备的进步，对高分辨率图像的SOD需求日益迫切。然而，传统的SOD方法主要局限于低分辨率图像，并难以适应高分辨率SOD（HRSOD）的发展。尽管出现了一些HRSOD方法，但目前没有足够大的数据集用于训练和评估。此外，当前的HRSOD方法通常会产生不完整的对象区域和不规则的对象边界。为解决上述问题，我们首先提出了一个新的HRS10K数据集，其中包含10500个2K-8K分辨率的高质量注释图像。据我们所知，这是目前用于HRSOD任务的最大数据集，它将在很大程度上帮助未来的训练和评估工作。此外，为了改进方法，在本研究中，我们提出了一种循环多尺度Transformer网络

    Salient Object Detection (SOD) aims to identify and segment the most conspicuous objects in an image or video. As an important pre-processing step, it has many potential applications in multimedia and vision tasks. With the advance of imaging devices, SOD with high-resolution images is of great demand, recently. However, traditional SOD methods are largely limited to low-resolution images, making them difficult to adapt to the development of High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no large enough datasets for training and evaluating. Besides, current HRSOD methods generally produce incomplete object regions and irregular object boundaries. To address above issues, in this work, we first propose a new HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K resolution. As far as we know, it is the largest dataset for the HRSOD task, which will significantly help future works in training and evaluating models. Furthermore, to improve
    
[^64]: 通过迭代的低分辨率点云完成变换器进行高分辨率颅缺损重建

    High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers. (arXiv:2308.03813v1 [eess.IV])

    [http://arxiv.org/abs/2308.03813](http://arxiv.org/abs/2308.03813)

    该论文提出了一种新的方法来增加个性化颅骨重建的可用性，通过点云完成任务实现高分辨率颅缺损重建，并在训练和推理过程中快速且资源高效。

    

    每年都有成千上万的人遭受各种类型的颅骨伤害，需要个性化植入物，手工设计昂贵且费时。因此，一个自动化的专用系统来增加个性化颅骨重建的可用性非常有必要。自动颅骨缺损重建的问题可以被描述为形状完成任务，并使用专用深度网络来解决。目前，最常见的方法是使用体积表示法并应用于图像分割的深度网络。然而，这种方法存在一些限制，不能很好地适应高分辨率体积，并没有考虑到数据的稀疏性。在我们的工作中，我们将问题重新表述为点云完成任务。我们提出了一种迭代的基于变换器的方法，可以在任何分辨率下重建颅缺损，并在训练和推理过程中快速且资源高效。我们比较了所提出的方法。

    Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed meth
    
[^65]: 通过可穿戴设备进行人体活动分析的弱监督多任务表示学习

    Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables. (arXiv:2308.03805v1 [cs.LG])

    [http://arxiv.org/abs/2308.03805](http://arxiv.org/abs/2308.03805)

    提出了一种使用弱监督多任务表示学习的方法，通过将数据映射到多个表示空间并根据多个方面对数据进行聚类，实现了同时解决多个任务的能力。

    

    来自可穿戴设备和智能环境的传感器数据流在人体活动识别（HAR）、人员识别或健康监测等领域得到广泛研究。然而，在活动和传感器流分析方面的大部分先前工作都侧重于数据的某一方面，例如仅识别活动类型或仅识别执行活动的人员。相反，我们提出一种方法，该方法使用弱监督的多输出孪生网络，学习将数据映射到多个表示空间，其中每个表示空间侧重于数据的一个方面。数据样本的表示向量在空间中的位置使得在该方面具有相同语义意义的数据彼此靠近。因此，正如一系列实验所证明的那样，训练好的模型可以基于多个方面为数据聚类提供度量指标，从而使其能够同时解决多个任务。

    Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and eve
    
[^66]: 金融欺诈检测的文本数据挖掘：一种深度学习方法

    Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])

    [http://arxiv.org/abs/2308.03800](http://arxiv.org/abs/2308.03800)

    该论文提出了一种深度学习方法，用于分析金融欺诈文本并进行分类。通过比较多种神经网络模型的准确性，该研究对金融欺诈检测具有重要意义

    

    在本报告中，我提出了一种深度学习方法，用于进行自然语言处理（以下简称NLP）的二元分类任务，以分析金融欺诈文本。首先，我搜索了港交所新闻的监管公告和执法公告，以定义欺诈公司并提取其MD＆A报告，然后整理了报告中的句子，并标记了报告时间。我的方法包括各种神经网络模型，包括具有嵌入层的多层感知器（MLP），基本循环神经网络（RNN），长短期记忆（LSTM）和门限循环单元（GRU）用于文本分类任务。通过利用这个多样化的模型集合，我旨在全面比较它们在检测金融欺诈方面的准确性。我的结果对于金融欺诈检测具有重要意义，因为这项工作为深度学习，NLP和金融交叉研究的不断增长贡献了有价值的研究成果

    In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuabl
    
[^67]: 语义信道均衡器: 在多用户语义通信中建模语言不匹配

    Semantic Channel Equalizer: Modelling Language Mismatch in Multi-User Semantic Communications. (arXiv:2308.03789v1 [cs.AI])

    [http://arxiv.org/abs/2308.03789](http://arxiv.org/abs/2308.03789)

    本文提出了一个新的语义信道均衡器，用于解决多用户语义通信中由于不同语言导致的语义噪声问题。

    

    我们考虑一个多用户语义通信系统，在这个系统中，代理（发送者和接收者）通过交换语义消息进行交流并传达意义。在这种情况下，语言在构建和巩固知识方面起着关键的作用，影响概念表示和语义提取和解释。然而，在语义通信中，语言的关键作用经常被忽视。当不考虑这一点时，代理的语言被假设为兼容和一致的，忽视了由于语言不匹配可能引起的实际限制。这是本文的重点。当代理使用不同的语言时，消息解释容易受到语义噪声的影响，这是由于语义通道引入的严重失真而引起的。为了解决这个问题，本文提出了一种新的语义信道均衡器，以抵消并限制消息解释中的严重不确定性。我们提出的解决方案模拟了不匹配的情况。

    We consider a multi-user semantic communications system in which agents (transmitters and receivers) interact through the exchange of semantic messages to convey meanings. In this context, languages are instrumental in structuring the construction and consolidation of knowledge, influencing conceptual representation and semantic extraction and interpretation. Yet, the crucial role of languages in semantic communications is often overlooked. When this is not the case, agent languages are assumed compatible and unambiguously interoperable, ignoring practical limitations that may arise due to language mismatching. This is the focus of this work. When agents use distinct languages, message interpretation is prone to semantic noise resulting from critical distortion introduced by semantic channels. To address this problem, this paper proposes a new semantic channel equalizer to counteract and limit the critical ambiguity in message interpretation. Our proposed solution models the mismatch o
    
[^68]: Bio+Clinical BERT、BERT Base和CNN在预测药物评论满意度方面的性能比较

    Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction. (arXiv:2308.03782v1 [cs.CL])

    [http://arxiv.org/abs/2308.03782](http://arxiv.org/abs/2308.03782)

    该研究比较了Bio+Clinical BERT、BERT Base和CNN在预测药物评价满意度方面的性能，结果显示医学领域特定的Bio+Clinical BERT模型在整体性能上优于通用领域的BERT Base模型，提高了11%的Macro F1和召回率得分。

    

    该研究的目标是开发能够分析患者药物评论并准确分类满意程度为积极、中性或消极的自然语言处理（NLP）模型。这样的模型将减轻医疗保健专业人员的工作负担，并提供更多关于患者生活质量的见解，这是治疗效果的重要指标。为了实现这一目标，我们实施和评估了多个分类模型，包括BERT base模型、Bio+Clinical BERT以及一个更简单的CNN。结果表明，医学领域特定的Bio+Clinical BERT模型在整体性能上显著优于通用领域的BERT base模型，如表2所示，它在Macro F1和召回率得分上提升了11%。未来的研究可以探索如何充分利用每个模型的特定优势。Bio+Clinical BERT在整体性能上表现出色，特别是在处理医学行话方面，而更简单的CNN则展现出识别关键词和上下文的能力。

    The objective of this study is to develop natural language processing (NLP) models that can analyze patients' drug reviews and accurately classify their satisfaction levels as positive, neutral, or negative. Such models would reduce the workload of healthcare professionals and provide greater insight into patients' quality of life, which is a critical indicator of treatment effectiveness. To achieve this, we implemented and evaluated several classification models, including a BERT base model, Bio+Clinical BERT, and a simpler CNN. Results indicate that the medical domain-specific Bio+Clinical BERT model significantly outperformed the general domain base BERT model, achieving macro f1 and recall score improvement of 11%, as shown in Table 2. Future research could explore how to capitalize on the specific strengths of each model. Bio+Clinical BERT excels in overall performance, particularly with medical jargon, while the simpler CNN demonstrates the ability to identify crucial words and a
    
[^69]: 属性概率图生成模型的拟合优度研究

    Goodness-of-Fit of Attributed Probabilistic Graph Generative Models. (arXiv:2308.03773v1 [cs.LG])

    [http://arxiv.org/abs/2308.03773](http://arxiv.org/abs/2308.03773)

    本文研究了属性概率图生成模型的拟合优度，并以均方互信息系数为指标进行了评估和验证。

    

    图的概率生成模型是能够进行表示和抽样的重要工具。许多最近的研究已经创建了能够表示实体间相互作用和属性的图的概率模型。然而，对于一个随机属性图的生成模型，确定其拟合优度的一般条件并不明确。在本文中，我们以随机二进制网络的均方互信息系数为基准来定义拟合优度。对于这个统计量，我们概述了一种用于评估学习到的属性图结构质量的过程，确保均方互信息系数的偏差最小，并且高概率下保持恒定或随机。我们将这些准则应用于验证概率生成模型在各种流行的图模型中的表示能力。

    Probabilistic generative models of graphs are important tools that enable representation and sampling. Many recent works have created probabilistic models of graphs that are capable of representing not only entity interactions but also their attributes. However, given a generative model of random attributed graph(s), the general conditions that establish goodness of fit are not clear a-priori. In this paper, we define goodness of fit in terms of the mean square contingency coefficient for random binary networks. For this statistic, we outline a procedure for assessing the quality of the structure of a learned attributed graph by ensuring that the discrepancy of the mean square contingency coefficient (constant, or random) is minimal with high probability. We apply these criteria to verify the representation capability of a probabilistic generative model for various popular types of graph models.
    
[^70]: 使用伪深度和融合技术改进神经光辐射场

    Improved Neural Radiance Fields Using Pseudo-depth and Fusion. (arXiv:2308.03772v1 [cs.CV])

    [http://arxiv.org/abs/2308.03772](http://arxiv.org/abs/2308.03772)

    本文提出了一种改进神经光辐射场的方法，通过构建多尺度编码体和提供多尺度几何信息，同时进行深度预测和辐射场重建，以实现对真实场景中物体的准确建模和视角合成。

    

    自从神经光辐射场技术问世以来，新颖的视角合成引起了广泛的关注。现有的辐射场重建通常通过从附近的源图像中构建一个编码体作为额外的输入来实现。然而，这些方法无法有效地编码真实场景中各种规模的物体/结构的几何信息。本文提出了构建多尺度编码体并为NeRF模型提供多尺度几何信息的方法。为使构建的编码体尽可能接近场景中物体的表面和渲染深度更准确，我们提出同时进行深度预测和辐射场重建。预测的深度图将用于监督渲染深度、缩小深度范围并引导点采样。最后，由于遮挡、光照等原因，点体特征中包含的几何信息可能不准确。

    Since the advent of Neural Radiance Fields, novel view synthesis has received tremendous attention. The existing approach for the generalization of radiance field reconstruction primarily constructs an encoding volume from nearby source images as additional inputs. However, these approaches cannot efficiently encode the geometric information of real scenes with various scale objects/structures. In this work, we propose constructing multi-scale encoding volumes and providing multi-scale geometry information to NeRF models. To make the constructed volumes as close as possible to the surfaces of objects in the scene and the rendered depth more accurate, we propose to perform depth prediction and radiance field reconstruction simultaneously. The predicted depth map will be used to supervise the rendered depth, narrow the depth range, and guide points sampling. Finally, the geometric information contained in point volume features may be inaccurate due to occlusion, lighting, etc. To this en
    
[^71]: 高级驾驶辅助系统中的视觉显著性检测

    Visual Saliency Detection in Advanced Driver Assistance Systems. (arXiv:2308.03770v1 [cs.CV])

    [http://arxiv.org/abs/2308.03770](http://arxiv.org/abs/2308.03770)

    这项研究介绍了一种在高级驾驶辅助系统中结合驾驶员嗜睡检测和基于显著性的场景理解的智能系统。通过采用特殊的3D深度网络进行语义分割，并在嵌入式平台上实现，该系统可以识别驾驶员的注意力和环境中的显著元素。

    

    视觉显著性是指人类从观察环境中聚焦并提取重要特征的内在机制。近年来，关于估计视觉显著性的汽车研究领域出现了明显的兴趣激增。在驾驶汽车时，驾驶员自然而然地将注意力集中在特定对象上，利用大脑驱动的显著性机制来优先考虑某些元素。在本研究中，我们提出了一种智能系统，将驾驶员的嗜睡检测系统与基于显著性的场景理解管线相结合。为了实现这一目标，我们实现了一个专门的3D深度网络用于语义分割，该网络经过预训练和定制，用于处理汽车级外部摄像头捕获的帧。所提出的管线运行在一个嵌入式平台上，采用STA1295核心，具有ARM A7双核，并嵌入了硬件加速器。

    Visual Saliency refers to the innate human mechanism of focusing on and extracting important features from the observed environment. Recently, there has been a notable surge of interest in the field of automotive research regarding the estimation of visual saliency. While operating a vehicle, drivers naturally direct their attention towards specific objects, employing brain-driven saliency mechanisms that prioritize certain elements over others. In this investigation, we present an intelligent system that combines a drowsiness detection system for drivers with a scene comprehension pipeline based on saliency. To achieve this, we have implemented a specialized 3D deep network for semantic segmentation, which has been pretrained and tailored for processing the frames captured by an automotive-grade external camera. The proposed pipeline was hosted on an embedded platform utilizing the STA1295 core, featuring ARM A7 dual-cores, and embeds an hardware accelerator. Additionally, we employ a
    
[^72]: 实现与操作分权自治组织集成的交通控制方法

    Towards Integrated Traffic Control with Operating Decentralized Autonomous Organization. (arXiv:2308.03769v1 [eess.SY])

    [http://arxiv.org/abs/2308.03769](http://arxiv.org/abs/2308.03769)

    本论文提出了一种基于分权自治组织（DAO）框架的集成交通控制方法，通过共识和激励机制实现了能源消耗效率的全局共识，并优化了所有智能代理的本地目标。同时，针对DAO中的结构僵化问题，提出了一种操作算法，扩展了基于DAO的控制能力。

    

    随着智能交通系统（ITS）的日益复杂化，人们需要一种能够考虑到各种异构智能代理的集成控制方法。然而，现有的控制方法在同时考虑到最优性和可扩展性方面并未表现出竞争力。为了解决这个问题，我们提出了一种基于分权自治组织（DAO）框架的集成控制方法。所提出的方法通过共识和激励机制，在达到能源消耗效率（ECE）的全局共识的同时，优化涉及的所有智能代理的本地目标。此外，针对DAO中的结构僵化问题，提出了一种操作算法。具体而言，所提出的操作方法识别出在DAO中执行智能合约的关键代理，从而最终扩展了基于DAO的控制能力。

    With a growing complexity of the intelligent traffic system (ITS), an integrated control of ITS that is capable of considering plentiful heterogeneous intelligent agents is desired. However, existing control methods based on the centralized or the decentralized scheme have not presented their competencies in considering the optimality and the scalability simultaneously. To address this issue, we propose an integrated control method based on the framework of Decentralized Autonomous Organization (DAO). The proposed method achieves a global consensus on energy consumption efficiency (ECE), meanwhile to optimize the local objectives of all involved intelligent agents, through a consensus and incentive mechanism. Furthermore, an operation algorithm is proposed regarding the issue of structural rigidity in DAO. Specifically, the proposed operation approach identifies critical agents to execute the smart contract in DAO, which ultimately extends the capability of DAO-based control. In additi
    
[^73]: 使用基于Transformer的框架结合深度信息增强图像字幕生成

    Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])

    [http://arxiv.org/abs/2308.03767](http://arxiv.org/abs/2308.03767)

    本研究提出了一个基于Transformer的框架，将深度信息与RGB图像结合，以增强图像字幕生成任务。通过使用多句字幕描述3D场景，我们的框架能够更好地理解输入场景，并在不同的融合方法下进行实验验证。

    

    图像字幕生成是连接计算机视觉和自然语言处理的具有挑战性的场景理解任务。虽然图像字幕生成模型在生成优秀描述方面取得了成功，但该领域主要集中于为2D图像生成单个句子。本文研究了通过将深度信息与RGB图像集成，能否增强字幕生成任务并生成更好的描述。为此，我们提出了一个基于Transformer的编码器-解码器框架，用于生成3D场景的多句字幕描述。RGB图像及其对应的深度图作为我们框架的输入，将它们结合起来以更好地理解输入场景。深度图可以是真实值或估计值，这使得我们的框架在任何RGB字幕数据集上都可以广泛应用。我们探索了不同的融合方法来融合RGB和深度图像。实验在NYU-v2数据集和Stanford图像集上进行。

    Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image para
    
[^74]: AMaizeD: 一种自动玉米病害检测的端到端流程

    AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection. (arXiv:2308.03766v1 [cs.CV])

    [http://arxiv.org/abs/2308.03766](http://arxiv.org/abs/2308.03766)

    AMaizeD是一种自动化框架，利用无人机获取的多光谱图像，通过卷积神经网络和分割技术实现了对玉米作物病害的早期检测。

    

    本研究论文提出了一种名为AMaizeD的自动化框架，用于利用来自无人机的多光谱图像对玉米作物中病害的早期检测。专家研究人员和农学家通过精心收集的数据集，聚焦于玉米作物，收集了各种玉米品种、栽培实践和环境条件，捕捉了玉米生长和病害发展的各个阶段。通过利用多光谱图像，该框架可以获得改进的光谱分辨率和对植物健康微小变化的增强敏感性。所提出的框架采用卷积神经网络（CNN）作为特征提取器和分割技术，识别玉米植株及其相关病害。实验结果表明，该框架在检测各种玉米病害方面具有有效性。

    This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, incl
    
[^75]: 在智能工作区应用中部署领导者-跟随者自动驾驶车辆系统并采用基于排队的交通分配方法

    Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach. (arXiv:2308.03764v1 [eess.SY])

    [http://arxiv.org/abs/2308.03764](http://arxiv.org/abs/2308.03764)

    本文研究在智能工作区应用中部署领导者-跟随者自动驾驶车辆系统，并提出了一种基于排队的交通分配方法来优化ATMA车辆的路径规划，从而减少慢速运行导致的系统成本。

    

    新兴技术自主卡车装配器(ATMA)是一种领导者-跟随者风格的车辆系统，利用连接和自动驾驶技术来增强交通基础设施维护中的安全性。然而，ATMA车辆与普通车辆之间的速度差异造成了移动瓶颈，降低了通行能力并增加了排队长度，进而导致额外的延误。ATMA采取不同路线导致了时间变化的不同容量降低模式，可能影响用户均衡交通分配并导致不同的系统成本。本文旨在优化ATMA车辆在网络中的路径规划，以最小化与慢速运行相关的系统成本。为此，提出了一种基于排队的交通分配方法来识别由ATMA系统引起的系统成本。引入了考虑容量降低的基于排队的时间相关(QBTD)行程时间函数。

    The emerging technology of the Autonomous Truck Mounted Attenuator (ATMA), a leader-follower style vehicle system, utilizes connected and automated vehicle capabilities to enhance safety during transportation infrastructure maintenance in work zones. However, the speed difference between ATMA vehicles and general vehicles creates a moving bottleneck that reduces capacity and increases queue length, resulting in additional delays. The different routes taken by ATMA cause diverse patterns of time-varying capacity drops, which may affect the user equilibrium traffic assignment and lead to different system costs. This manuscript focuses on optimizing the routing for ATMA vehicles in a network to minimize the system cost associated with the slow-moving operation.  To achieve this, a queuing-based traffic assignment approach is proposed to identify the system cost caused by the ATMA system. A queuing-based time-dependent (QBTD) travel time function, considering capacity drop, is introduced a
    
[^76]: MedMine: 检验预训练语言模型在药物挖掘中的应用

    MedMine: Examining Pre-trained Language Models on Medication Mining. (arXiv:2308.03629v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03629](http://arxiv.org/abs/2308.03629)

    MedMine通过检验预训练语言模型在药物挖掘中的应用，发现了它们在不同实体类型和临床事件上的不平衡表现，并提供了解决这些问题的研究方向。

    

    自动从临床和生物医学文本中进行药物挖掘已成为一个热门话题，这是由于其对医疗应用的真实影响以及强大语言模型的最新发展。然而，全自动提取模型仍然面临一些障碍，以便可以直接部署到临床实践中以获得更好的影响。这些障碍包括它们在不同实体类型和临床事件上的不平衡表现。在本研究中，我们通过微调，包括基于单语言模型Med7和多语言大型语言模型XLM-RoBERTa的方式，检验了当前最先进的预训练语言模型在这些任务上的表现。我们使用n2c2-2018挑战赛的历史药物挖掘共享任务数据集进行了它们的优劣比较。我们报告了这些微调实验的结果，以便促进未来研究解决这些问题，比如如何结合它们的输出，合并这些模型，或者改进其性能。

    Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or impr
    
[^77]: RecycleGPT：一种具有可回收模块的自回归语言模型

    RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03421](http://arxiv.org/abs/2308.03421)

    RecycleGPT是一种生成式语言模型，通过回收预生成的模型状态而无需多次运行整个模型，并且证明了其降低推理延迟的有效性，实现高速解码和保持高性能。

    

    现有的大型语言模型必须运行K次才能生成K个令牌的序列。在本文中，我们提出了RecycleGPT，这是一种具有快速解码速度的生成式语言模型，通过回收预生成的模型状态而无需将整个模型运行多次步骤。我们的方法基于这样的观察：序列中相邻的令牌通常具有很强的相关性，并且可以根据前面的令牌合理猜测或推断出下一个令牌。实验证明了我们的方法在降低推理延迟方面的有效性，实现了高达1.4倍的加速，同时保持高性能。

    Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
    
[^78]: 使用生成对抗网络生成逼真的合成毫米波雷达数据，用于自动驾驶应用

    Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v1 [cs.CV])

    [http://arxiv.org/abs/2308.02632](http://arxiv.org/abs/2308.02632)

    本研究提出了一种使用生成对抗网络生成合成毫米波雷达数据的快速方法，可以增加数据增强的潜力，并进一步开发雷达数据处理算法。

    

    目前模拟FMCW雷达的主要方法是基于射线追踪，通常计算密集且不能考虑背景噪声。本研究提出了一种更快速的FMCW雷达模拟方法，能够使用生成对抗网络（GAN）生成合成的原始雷达数据。代码和预训练的权重是开源的，并可在GitHub上获得。该方法生成了16个同时的脉冲，可以用于进一步开发用于处理雷达数据（滤波和聚类）的算法。这可以增加数据增强的潜力，例如通过生成在实际生活中不可复现的不存在或安全关键场景的数据。在本研究中，使用摩托车的雷达测量数据对GAN进行训练，并生成摩托车直线行驶的合成原始雷达数据。生成这些数据时，使用了摩托车的距离和高斯噪声作为输入。

    The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the 
    
[^79]: 通过领域适应的最少到最多提示的方式实现文本到SQL的高效泛化

    Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])

    [http://arxiv.org/abs/2308.02582](http://arxiv.org/abs/2308.02582)

    该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。

    

    跨领域和跨组合式的文本到SQL语义解析的泛化是一项具有挑战性的任务。现有的基于大型语言模型（LLM）的解决方案依赖于从训练集中推理出少量样本，以合成每个自然语言（NL）测试查询的运行时提示。与此相反，我们设计了一种算法，该算法通过离线抽样从训练数据中获取少量样本，完全覆盖SQL子句、运算符和函数，并在允许的令牌长度范围内实现最大领域覆盖。这样可以合成一个固定的通用提示（GP），其中包含NL测试查询之间共用的多样化样本集，避免了昂贵的测试时间样本检索。我们还将GP自适应到目标数据库领域（DA-GP），以更好地处理跨领域泛化；然后采用分解的最少到最多提示（LTMP-DA-GP）来处理跨组合泛化。LTMP-DA-GP的合成是离线任务，

    Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
    
[^80]: 数字双胞胎脑：生物智能和人工智能之间的桥梁

    Digital twin brain: a bridge between biological intelligence and artificial intelligence. (arXiv:2308.01941v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.01941](http://arxiv.org/abs/2308.01941)

    本文提出了数字双胞胎脑作为一个桥梁，将生物智能和人工智能联系在一起，通过将神经科学和人工智能的进展结合起来，更好地理解大脑的复杂性和如何产生智能。

    

    最近几年，神经科学和人工智能的进展为理解大脑的复杂性以及通过计算系统来模拟大脑铺平了道路。神经科学研究的前沿进展揭示了大脑结构和功能之间错综复杂的关系，而人工神经网络的成功强调了网络架构的重要性。现在是将它们结合起来更好地揭示智能是如何从大脑的多尺度存储库中产生的时候了。在这篇综述中，我们提出数字双胞胎脑(DTB)作为一个将生物智能和人工智能联系起来的转变性平台。它包括三个核心元素：对双胞胎过程至关重要的大脑结构、用于生成大脑功能的底层模型，以及其广泛的应用领域。关键是，大脑图谱提供了一个重要的约束条件，保持了大脑的网络组织。

    In recent years, advances in neuroscience and artificial intelligence have paved the way for unprecedented opportunities for understanding the complexity of the brain and its emulation by computational systems. Cutting-edge advancements in neuroscience research have revealed the intricate relationship between brain structure and function, while the success of artificial neural networks highlights the importance of network architecture. Now is the time to bring them together to better unravel how intelligence emerges from the brain's multiscale repositories. In this review, we propose the Digital Twin Brain (DTB) as a transformative platform that bridges the gap between biological and artificial intelligence. It consists of three core elements: the brain structure that is fundamental to the twinning process, bottom-layer models to generate brain functions, and its wide spectrum of applications. Crucially, brain atlases provide a vital constraint, preserving the brain's network organizat
    
[^81]: NBIAS: 用于文本中偏见识别的自然语言处理框架

    NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])

    [http://arxiv.org/abs/2308.01681](http://arxiv.org/abs/2308.01681)

    本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。

    

    在文本数据中存在偏见可能导致数据使用时产生倾斜的解释和结果。这些偏见可能会持续强化刻板印象、歧视或其他形式的不公平待遇。在有偏见的数据上训练的算法最终会做出不平等影响某个群体的决策。因此，检测和消除这些偏见至关重要，以确保对数据的公平和道德使用。为此，我们开发了一个全面而强大的框架"NBIAS"，它包括数据层、语料库构建、模型开发层和评估层。数据集由从各个领域收集的多样化数据构建，包括社交媒体、医疗保健和职位招聘门户网站。因此，我们应用了基于Transformer的令牌分类模型，通过一个唯一的命名实体能够识别出偏见词/短语。在评估过程中，我们结合了定量和定性评估方法来评估我们模型的效果。

    Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
    
[^82]: FusionAD: 自动驾驶预测和规划任务的多模态融合

    FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])

    [http://arxiv.org/abs/2308.01006](http://arxiv.org/abs/2308.01006)

    FusionAD是第一个将来自相机和激光雷达的信息融合起来用于自动驾驶预测和规划任务的统一框架，在常用数据集上的实验中达到了最先进的性能。

    

    在自动驾驶感知任务中，构建一个多模态多任务神经网络以实现准确和稳健的性能已成为铁板一块的标准。然而，利用来自多个传感器的数据来联合优化预测和规划任务仍然几乎未被探索。本文提出了FusionAD，据我们所知，这是第一个将来自两个最关键传感器相机和激光雷达的信息融合起来超越感知任务的统一框架。具体来说，我们首先构建了一个基于转换器的多模态融合网络，以有效地产生基于融合的特征。然后，与基于相机的端到端方法UniAD相比，我们建立了一个融合辅助的模态感知预测和状态感知规划模块，称为FMSPnP，充分利用多模态特征的优势。我们在常用的nuScenes数据集上进行了大量实验，结果表明我们的FusionAD达到了最先进的性能，并优于基准线平均15%的性能。

    Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce
    
[^83]: MetaGPT: 元编程用于多智能体协作框架

    MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])

    [http://arxiv.org/abs/2308.00352](http://arxiv.org/abs/2308.00352)

    MetaGPT是一个用于多智能体协作的创新框架，将有效的人工工作流引入到大型语言模型驱动的协作中。它采用元编程方法，将标准操作规程编码为提示，促进结构化协调，并要求模块化输出，赋予智能体领域专业知识，以验证输出并减少错误。这种框架利用了流水线工作模式来分配任务。

    

    最近，在多个大型语言模型驱动的智能体协作中，自动任务解决取得了显著进展。然而，现有的工作主要集中在简单任务上，缺乏对复杂任务的探索和研究，主要是由于幻觉问题。这种幻觉在多个智能体相互作用时被无限放大，导致在解决复杂问题时失败。因此，我们引入了MetaGPT，这是一个创新的框架，在LLM驱动的多智能体协作中采用有效的人工工作流作为元编程方法。具体而言，MetaGPT首先将标准操作规程（SOPs）编码为提示，促进结构化协调。然后，它进一步要求模块化输出，赋予智能体领域专业知识，与人类专业人员平行验证输出并减少错误。通过这种方式，MetaGPT利用流水线工作模式来分配任务

    Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
    
[^84]: ProtoFL: 通过原型蒸馏实现无监督的联邦学习

    ProtoFL: Unsupervised Federated Learning via Prototypical Distillation. (arXiv:2307.12450v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12450](http://arxiv.org/abs/2307.12450)

    本文提出了ProtoFL，一种基于原型蒸馏的无监督联邦学习方法，用于提高全局模型的表示能力并减少通信成本。此外，引入了基于正规流的本地单类分类器以提高有限数据下的性能。在五个基准数据集上的实验证明了该方法相较于先前方法具有超越性能。

    

    联邦学习（FL）是一种提高数据隐私保护的有潜力的方法，特别适用于身份验证系统。然而，有限的通信轮次、稀缺的表示和可扩展性给其部署带来了重大挑战，限制了其发挥全部潜力的能力。在本文中，我们提出了"ProtoFL"，基于原型表示蒸馏的无监督联邦学习，以增强全局模型的表示能力并降低通信成本。此外，我们引入了基于正规流的本地单类分类器，以在有限数据下改善性能。我们的研究首次探讨了使用FL来提高单类分类性能。我们在广泛使用的五个基准数据集上进行了大量实验，分别是MNIST、CIFAR-10、CIFAR-100、ImageNet-30和Keystroke-Dynamics，以展示我们提出的框架在文献中的先前方法上具有卓越的性能。

    Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
    
[^85]: 错误的原因而正确的：可解释的机器学习技术能够检测到虚假相关性吗？

    Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?. (arXiv:2307.12344v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12344](http://arxiv.org/abs/2307.12344)

    本研究在胸透诊断任务中评估了五种事后解释技术和一种本质上可解释的方法对于检测虚假相关性的能力，发现SHAP和Attri-Net可以有效地检测出这种类型的依赖关系。

    

    虽然深度神经网络模型可以提供无与伦比的分类性能，但它们很容易学习到数据中的虚假相关性。如果测试数据与训练数据来自相同的分布，那么这种对混淆信息的依赖会很难通过性能指标来检测出来。可解释的机器学习方法，如事后解释或本质上可解释的分类器，承诺可以识别出错误的模型推理。然而，目前对于这些技术是否真的能够做到这一点存在着一些混合的证据。本文提出了一种严格的评估策略，以评估解释技术正确识别虚假相关性的能力。使用这种策略，我们评估了五种事后解释技术和一种本质上可解释的方法对于在胸透诊断任务中检测三种人为添加的混淆因子的能力。我们发现事后解释技术SHAP以及本质上可解释的Attri-Net方法可以有效地检测出虚假相关性。

    While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net pr
    
[^86]: 用GPT-4增强CLIP：利用视觉描述作为提示

    Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])

    [http://arxiv.org/abs/2307.11661](http://arxiv.org/abs/2307.11661)

    本文展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，在专门细粒度数据集上显示了较大的0-shot迁移准确性改进。

    

    对比预训练的大型视觉-语言模型（VLMs）如CLIP在下游数据集上提供了良好性能，从而革新了视觉表示学习。VLMs通过设计与数据集相关的提示来0-shot适应下游数据集。这种提示工程利用了领域专业知识和验证数据集。同时，像GPT-4这样的生成预训练模型的最新发展意味着它们可以用作先进的互联网搜索工具。它们还可以被操作以提供任何结构化的视觉信息。在这项工作中，我们展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，我们在专门细粒度数据集（如EuroSAT（~7％）、DTD（~7％）、SUN397（~4.6％）和CUB（~3.3％））上显示出了较大的0-shot迁移准确性改进。我们还设计了一个简单的少量样本适配器，它可以学习选择最佳的s

    Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
    
[^87]: 在人工智能中平衡隐私和进步：基于组织病理学的医学研究和教育中的匿名化

    Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education. (arXiv:2307.09426v1 [cs.AI])

    [http://arxiv.org/abs/2307.09426](http://arxiv.org/abs/2307.09426)

    本论文讨论了在开发人工智能算法时，在组织病理学中平衡隐私和进步的挑战，并提出了解决方案。

    

    生物医学研究的进展严重依赖于大量医学数据的获取。在组织病理学的情况下，全切片图像（WSI）和临床病理学信息对于开发数字病理学（DP）的人工智能（AI）算法非常有价值。将医学数据传输“尽量开放”提高了数据用于次级目的的可用性，但同时也对患者隐私构成风险。同时，现有法规要求保持医学数据“尽量封闭”以避免再识别风险。通常，这些法规要求删除敏感数据，但不考虑现代图像匹配算法可能导致的数据链接攻击的可能性。此外，DP的标准化不足使得在所有WSI格式上建立单一解决方案变得更加困难。这些挑战给生物信息学研究者在开发AI算法时在隐私与进步之间取得平衡带来了问题。本文探讨了这些挑战，并提出了一些解决方案。

    The advancement of biomedical research heavily relies on access to large amounts of medical data. In the case of histopathology, Whole Slide Images (WSI) and clinicopathological information are valuable for developing Artificial Intelligence (AI) algorithms for Digital Pathology (DP). Transferring medical data "as open as possible" enhances the usability of the data for secondary purposes but poses a risk to patient privacy. At the same time, existing regulations push towards keeping medical data "as closed as necessary" to avoid re-identification risks. Generally, these legal regulations require the removal of sensitive data but do not consider the possibility of data linkage attacks due to modern image-matching algorithms. In addition, the lack of standardization in DP makes it harder to establish a single solution for all formats of WSIs. These challenges raise problems for bio-informatics researchers in balancing privacy and progress while developing AI algorithms. This paper explo
    
[^88]: 我们能相信种族预测吗？

    Can We Trust Race Prediction?. (arXiv:2307.08496v1 [cs.LG])

    [http://arxiv.org/abs/2307.08496](http://arxiv.org/abs/2307.08496)

    本文研究了在没有敏感的种族和族裔数据的情况下，使用代理模型进行种族预测的问题。研究者训练了一个双向长短时记忆（BiLSTM）模型，并创建了一个集成模型，其在外样本（OOS）F1得分上比文献中表现最好的机器学习模型高出36.8%。此外，还构建了美国最全面的姓氏和名字分布数据库，并提供了第一个高质量的基准数据集，以公正比较现有模型，并帮助未来的模型开发者。

    

    在没有敏感的种族和族裔数据的情况下，研究人员、监管机构和公司都借助代理模型。在本文中，我使用来自美国50个州的选民注册数据训练了一个双向长短时记忆（BiLSTM）模型，并创建了一个集成模型，其在外样本（OOS）F1得分上比文献中表现最好的机器学习模型高出36.8%。此外，我构建了美国最全面的姓氏和名字分布数据库，以改进贝叶斯改进姓氏地理编码（BISG）和贝叶斯改进名字姓氏地理编码（BIFSG）的覆盖和准确性。最后，我提供了第一个高质量的基准数据集，以公正比较现有模型，并帮助未来的模型开发者。

    In the absence of sensitive race and ethnicity data, researchers, regulators, and firms alike turn to proxies. In this paper, I train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models in the literature. Additionally, I construct the most comprehensive database of first and surname distributions in the US in order to improve the coverage and accuracy of Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG). Finally, I provide the first high-quality benchmark dataset in order to fairly compare existing models and aid future model developers.
    
[^89]: 一种从实值观测中进行强化学习的神经形态架构

    A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations. (arXiv:2307.02947v1 [cs.NE])

    [http://arxiv.org/abs/2307.02947](http://arxiv.org/abs/2307.02947)

    本文提出了一种新颖的神经网络架构，用于从实值观测中进行强化学习。该模型采用了多层事件驱动聚类、时间差分误差调制和资格痕迹等方法，并在经典RL环境中取得了优于表格方法的性能表现。

    

    强化学习（RL）为复杂环境下的决策提供了一个强大的框架。然而，以高效且生物启发的方式实现RL仍然是一个挑战。本文提出了一种新颖的脉冲神经网络（SNN）架构，用于解决具有实值观测的RL问题。所提出的模型结合了多层事件驱动聚类，增加了时间差分（TD）误差调制和资格痕迹, 并基于之前的工作进行改进。消融研究证实了这些组成部分对所提出的模型性能的重要影响。在经典RL环境中，我们的网络不断优于表格方法，并成功发现了稳定的控制策略：山车、倒立摆和摆臂。所提出的模型在计算效率上具有吸引力的折中方案。

    Reinforcement Learning (RL) provides a powerful framework for decision-making in complex environments. However, implementing RL in hardware-efficient and bio-inspired ways remains a challenge. This paper presents a novel Spiking Neural Network (SNN) architecture for solving RL problems with real-valued observations. The proposed model incorporates multi-layered event-based clustering, with the addition of Temporal Difference (TD)-error modulation and eligibility traces, building upon prior work. An ablation study confirms the significant impact of these components on the proposed model's performance. A tabular actor-critic algorithm with eligibility traces and a state-of-the-art Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our network consistently outperforms the tabular approach and successfully discovers stable control policies on classic RL environments: mountain car, cart-pole, and acrobot. The proposed model offers an appealing trade-off in terms of computa
    
[^90]: MAE-DFER：高效的面具自编码器用于自监督动态面部表情识别

    MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition. (arXiv:2307.02227v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.02227](http://arxiv.org/abs/2307.02227)

    本研究提出了一种高效的自监督方法MAE-DFER，通过大规模自监督预训练和显式时间面部运动建模，推进了动态面部表情识别的发展。

    

    动态面部表情识别（DFER）对于智能和移情的机器发展至关重要。本文受到最近面具自编码器（例如VideoMAE）的前所未有的成功启发，提出了一种新颖的自监督方法MAE-DFER，该方法利用大规模的自监督预训练和丰富的未标记数据大力推进DFER的发展。由于VideoMAE中使用的Vanilla Vision Transformer（ViT）在微调过程中需要大量计算，因此MAE-DFER开发了一种高效的局部-全局交互Transformer（LGI-Former）作为编码器。此外，在VideoMAE的独立外观内容重构基础上，MAE-DFER还引入了显式的时间面部运动建模，以鼓励LGI-Former挖掘静态外观和动态运动信息。

    Dynamic facial expression recognition (DFER) is essential to the development of intelligent and empathetic machines. Prior efforts in this field mainly fall into supervised learning paradigm, which is severely restricted by the limited labeled data in existing datasets. Inspired by recent unprecedented success of masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel self-supervised method which leverages large-scale self-supervised pre-training on abundant unlabeled data to largely advance the development of DFER. Since the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial computation during fine-tuning, MAE-DFER develops an efficient local-global interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to the standalone appearance content reconstruction in VideoMAE, MAE-DFER also introduces explicit temporal facial motion modeling to encourage LGI-Former to excavate both static appearance and dynamic motion information. 
    
[^91]: 大型语言模型真的是良好的逻辑推理者吗？基于演绎、归纳和阿布达斯观点的全面评估。

    Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])

    [http://arxiv.org/abs/2306.09841](http://arxiv.org/abs/2306.09841)

    本文评估了大型语言模型的逻辑推理能力，选择了15个典型数据集，考虑了演绎、归纳、阿布达斯和混合推理形式，并选择了三个代表性的LLMs进行零样本、一次和三次的设置下评估。提出精细级别的评估方法。

    

    大型语言模型(LLMs)在各种自然语言任务中取得了巨大成功。对LLMs的具体推理能力进行评估，如多语言推理和数学推理，引起了广泛关注。然而，作为关键推理视角之一，逻辑推理能力还没有得到彻底评估。本文旨在填补这些差距并提供全面的评估。首先，为了进行系统化评估，本文选择了15个典型的逻辑推理数据集，并将它们组织成演绎、归纳、阿布达斯和混合形式的推理设置。考虑评估的全面性，我们选择了三个代表性的LLMs（text-davinci-003，ChatGPT和BARD），并在零样本、一次和三次的设置下对所有选择的数据集进行评估。其次，与以往仅依赖简单指标（如准确性）的评估不同，我们提出了从目标推理角度进行的精细级别评估。

    Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
    
[^92]: 《InstructZero: 针对黑盒大型语言模型的高效指令优化》

    InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models. (arXiv:2306.03082v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03082](http://arxiv.org/abs/2306.03082)

    InstructZero通过优化低维软提示而非离散指令，实现了对黑盒语言模型的高效指令优化，通过贝叶斯优化生成改善零样本性能的新软提示，并在不同任务上优于现有自动指令方法。

    

    大型语言模型是指令跟随者，但对于不同情况下最佳指令的选择可能面临挑战，特别是对于禁止反向传播的黑盒语言模型。我们不直接优化离散指令，而是优化一个应用于开源语言模型的低维软提示，以生成黑盒语言模型的指令。在所提出的InstructZero方法的每个迭代中，软提示被转换为一个指令，然后通过开源语言模型提交给黑盒语言模型进行零样本评估，并将性能发送给贝叶斯优化以生成改善零样本性能的新软提示。我们在不同的开源语言模型和API组合上评估了InstructZero，包括Vicuna和ChatGPT。我们的结果显示，在各种下游任务中，InstructZero优于现有自动指令方法。我们的代码和数据公开可用于https://gith。

    Large language models~(LLMs) are instruction followers, but it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. On each iteration of the proposed method, which we call InstructZero, a soft prompt is converted into an instruction using the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, and the performance is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our results show that InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks. Our code and data are publicly available at https://gith
    
[^93]: 利用大型语言模型在公共事务领域进行主题分类

    Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs. (arXiv:2306.02864v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.02864](http://arxiv.org/abs/2306.02864)

    LLMs在公共事务文件分类中的表现进行了分析。通过使用正则表达式工具收集了一个包含超过33K个样本和22.5M个标记的公共事务文件数据库。实验结果表明，LLMs可以有效处理和理解这类文件中使用的复杂语言，从而提高了公共事务文件的分析能力。

    

    分析公共事务文件对公民至关重要，它促进透明度、问责制和决策。它使公民能够了解政府政策，参与公共话语，并追究代表的责任。对于依靠某些规定运营的公司来说，这是至关重要的，有时甚至关乎生死。大型语言模型（LLM）有潜力通过有效处理和理解这类文件中使用的复杂语言，大大增强公共事务文件的分析能力。在这项工作中，我们分析了LLM在分类公共事务文件中的性能。作为一项自然的多标签任务，这些文件的分类具有重要挑战。在这项工作中，我们使用了一个基于正则表达式的工具来收集了一个包含超过33K个样本和22.5M个标记的公共事务文件数据库。我们的实验评估了4个不同的西班牙语LLM在最多30个不同类别的文件分类中的性能。

    The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 diff
    
[^94]: LLM时代的人工智能透明度：以人为中心的研究路线图

    AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap. (arXiv:2306.01941v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2306.01941](http://arxiv.org/abs/2306.01941)

    在当前LLMs话题的讨论中，负责任人工智能的关键要素--透明度--被忽视了。这篇论文提出了在以人为中心的视角下，追求LLMs的透明度对于支持适当的人类理解是至关重要的。

    

    强大的大型语言模型(LLMs)的兴起带来了巨大的创新机会，但也存在着对个人和社会的潜在风险。确保负责任地开发和部署LLMs和基于LLMs的应用程序变得至关重要。然而，负责任人工智能的核心支柱--透明度--在当前关于LLMs的讨论中大部分都缺失了。追求新的透明度方法对于LLMs至关重要，多年来在人工智能和人机交互(HCI)交叉领域的研究表明，我们必须以人为中心的视角来做到这一点：透明度基本上是支持适当人类理解的问题，不同利益相关者在不同环境下追求不同目标的理解。在这个新的LLM时代，我们必须考虑新兴LLM生态系统中利益相关者的需求，开发和设计透明度的方法。

    The rise of powerful large language models (LLMs) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. We have reached a pivotal moment for ensuring that LLMs and LLM-infused applications are developed and deployed responsibly. However, a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs. It is paramount to pursue new approaches to provide transparency for LLMs, and years of research at the intersection of AI and human-computer interaction (HCI) highlight that we must do so with a human-centered perspective: Transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. In this new era of LLMs, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging LLM ecosystem, the novel types of LLM-infused 
    
[^95]: 测量和建模身体内在动机

    Measuring and Modeling Physical Intrinsic Motivation. (arXiv:2305.13452v1 [cs.AI])

    [http://arxiv.org/abs/2305.13452](http://arxiv.org/abs/2305.13452)

    本文对身体内在动机进行了量化建模，发现对抗性奖励模型可以最好地预测人类对物理情境的趣味反应，还发现简单场景特征模型无法在所有情境中预测人类反应，将对抗模型和场景中碰撞数量进行线性组合，能够显著提高对人类反应的预测能力，表明人类追求高信息增益和身体活动的情况。

    

    人类是有驱动力的互动性代理，他们追求有趣的物理动力学情境。本文探讨了形式化的物理内在动机形式。我们首先收集了人类对多种物理情境的评分。接着，我们通过实现依赖于简单场景特征的模型到依赖于前向物理预测的模型的各种内在动机假设来建模人类的趣味反应。我们发现，对于人类反应的单一最佳预测器是针对物理预测损失推导出的对抗性奖励模型。我们还发现，简单的场景特征模型不能在所有情境中推广他们对人类反应的预测。最后，将对抗模型与场景中碰撞数量进行线性组合，可显著提高对人类反应的预测能力，表明人类倾向于追求高信息增益和身体活动的情况。

    Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity.
    
[^96]: 学习在存在隐性混淆因素的情况下从不确定数据中恢复因果关系

    Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])

    [http://arxiv.org/abs/2305.02640](http://arxiv.org/abs/2305.02640)

    本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。

    

    在具有潜在变量的因果发现中，我们定义了两个数据范式：确定数据：具有观察节点单值的单个骨架结构，和不确定数据：具有观察节点多值的一组多骨架结构。多个骨架引入低样本利用率，多个值引入了分布假设的无能力，这两者导致从不确定数据中恢复因果关系至今仍然未被充分探索。我们设计了因果强度变分模型来解决这两个问题。具体地，我们利用因果强度而不是独立噪声作为潜变量来调节证据下界。通过这种设计思想，不同骨架的因果强度被看作是一个分布，并可以表示为单值因果图矩阵。此外，考虑到潜在混淆因素，我们将因果图G分解为两个相关子图O和C。O包含观察节点之间的纯关系，而C表示混淆因素。

    In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
    
[^97]: 负责任人工智能的实施：伦理方面的紧张和权衡

    Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects. (arXiv:2304.08275v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2304.08275](http://arxiv.org/abs/2304.08275)

    本文讨论了负责任人工智能伦理原则之间的紧张关系和权衡，并提出一个目录以帮助人们提高对相互作用的认识。

    

    针对人工智能/机器学习系统的滥用和不当使用引起的担忧，已经提出了许多负责任人工智能的伦理原则。这些准则的基本方面包括隐私、准确性、公平性、稳健性、可解释性和透明度。然而，这些方面之间存在潜在的紧张关系，这给寻求遵循这些原则的AI/ML开发者带来了困难。例如，提高AI/ML系统的准确性可能会降低其可解释性。本文旨在汇编和讨论10个突出的紧张关系、权衡和其他基本方面之间的相互作用，以便在持续努力将这些原则转化为实践的过程中，提高对可能出现的伦理原则方面之间相互作用的认识，并通过在广泛文献中的支持进行双面互动的重点讨论。这个目录对于提高人们对伦理准则方面之间可能相互作用的认识以及促进设计人员和开发人员做出有充分依据的判断可能有所帮助。

    Many sets of ethics principles for responsible AI have been proposed to allay concerns about misuse and abuse of AI/ML systems. The underlying aspects of such sets of principles include privacy, accuracy, fairness, robustness, explainability, and transparency. However, there are potential tensions between these aspects that pose difficulties for AI/ML developers seeking to follow these principles. For example, increasing the accuracy of an AI/ML system may reduce its explainability. As part of the ongoing effort to operationalise the principles into practice, in this work we compile and discuss a catalogue of 10 notable tensions, trade-offs and other interactions between the underlying aspects. We primarily focus on two-sided interactions, drawing on support spread across a diverse literature. This catalogue can be helpful in raising awareness of the possible interactions between aspects of ethics principles, as well as facilitating well-supported judgements by the designers and develo
    
[^98]: GNNBuilder：通用图神经网络加速器生成、模拟和优化的自动化框架

    GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization. (arXiv:2303.16459v1 [cs.AR])

    [http://arxiv.org/abs/2303.16459](http://arxiv.org/abs/2303.16459)

    GNNBuilder是一个自动化的、通用的、端到端的GNN加速器生成框架，它可以为用户任意定义的广泛的GNN模型自动生成加速器，且运行速度比软件基准快多达12.95倍。

    

    目前有很多图神经网络（GNN）加速器被提出。然而，它们高度依赖于用户的硬件专业知识，并且通常针对一种特定的GNN模型进行优化，使它们难以实际使用。因此，在这项工作中，我们提出了GNNBuilder，这是第一个自动化的、通用的、端到端的GNN加速器生成框架。它具有四个优点：（1）GNNBuilder可以自动为用户任意定义的广泛的GNN模型生成GNN加速器；（2）GNNBuilder采用标准的PyTorch编程接口，为算法开发人员提供零开销；（3）GNNBuilder支持端到端的代码生成、模拟、加速器优化和硬件部署，实现了GNN加速器设计的一键式操作；（4）GNNBuilder配备了其所生成的加速器的准确性能模型，使得设计空间探索（DSE）快速而灵活。在实验中，首先我们展示了我们的加速器在6个基准数据集上与最先进的GNN加速器相比表现出色，运行速度比软件基准快了多达12.95倍。其次，对不规则图处理的案例研究展示了GNNBuilder的出色可扩展性和适应性。最后，在单个GPU的服务器上，我们对400个GNN模型进行了30分钟的DSE研究，验证了GNNBuilder的效率和有效性。

    There are plenty of graph neural network (GNN) accelerators being proposed. However, they highly rely on users' hardware expertise and are usually optimized for one specific GNN model, making them challenging for practical use . Therefore, in this work, we propose GNNBuilder, the first automated, generic, end-to-end GNN accelerator generation framework. It features four advantages: (1) GNNBuilder can automatically generate GNN accelerators for a wide range of GNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorch programming interface, introducing zero overhead for algorithm developers; (3) GNNBuilder supports end-to-end code generation, simulation, accelerator optimization, and hardware deployment, realizing a push-button fashion for GNN accelerator design; (4) GNNBuilder is equipped with accurate performance models of its generated accelerator, enabling fast and flexible design space exploration (DSE). In the experiments, first, we show that our accelerator pe
    
[^99]: 针对链接预测，对待不同的负样本有差异性：利用领域和范围约束丰富损失函数

    Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00286](http://arxiv.org/abs/2303.00286)

    通过引入领域和范围约束，我们提出了基于语义的损失函数来区分不同质量的负样本，实验证明在链接预测任务上有效。

    

    知识图谱嵌入模型（KGEMs）用于与知识图谱（KGs）相关的各种任务，包括链接预测。它们使用考虑了一批得分三元组及其相应标签的损失函数进行训练。传统方法认为三元组的标签要么为真，要么为假。然而，最近的研究表明，并非所有的负样本应该被平等对待。与这一最近的假设一致，我们认为基于领域和范围约束在语义上有效的负样本可能是高质量的负样本。因此，损失函数应该将它们与语义上无效的负样本区别对待。为此，我们针对链接预测的三个主要损失函数提出了基于语义的版本。通过广泛和受控的实验设置，我们展示了所提出的损失函数在三个具有不同模式的公共基准KG上系统地提供了令人满意的结果。

    Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic
    
[^100]: SPTS v2: 单点场景文本定位

    SPTS v2: Single-Point Scene Text Spotting. (arXiv:2301.01635v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.01635](http://arxiv.org/abs/2301.01635)

    本研究提出的SPTS v2框架，首次证明了可以使用极低成本的单点注释来训练场景文本定位模型，同时保留了自回归Transformer与实例分配解码器（IAD）的优势，并使用并行识别解码器（PRD）进行文本识别。

    

    由于文本检测和识别之间的内在协同作用，端到端的场景文本定位取得了显著的进展。先前的方法通常将手动标注（如水平矩形、旋转矩形、四边形和多边形）视为必要条件，而这比使用单点要昂贵得多。我们首次证明了通过提出的名为SPTS v2的框架，可以使用极低成本的单点注释来训练场景文本定位模型。SPTS v2通过以顺序预测同一预测序列中所有文本实例的中心点，保留了自回归Transformer与实例分配解码器（IAD）的优势，同时使用并行识别解码器（PRD）进行文本识别。这两个解码器共享相同的参数，并通过简单而有效的信息传输过程进行交互连接，以传递梯度和信息。

    End-to-end scene text spotting has made significant progress due to its intrinsic synergy between text detection and recognition. Previous methods commonly regard manual annotations such as horizontal rectangles, rotated rectangles, quadrangles, and polygons as a prerequisite, which are much more expensive than using single-point. For the first time, we demonstrate that training scene text spotting models can be achieved with an extremely low-cost single-point annotation by the proposed framework, termed SPTS v2. SPTS v2 reserves the advantage of the auto-regressive Transformer with an Instance Assignment Decoder (IAD) through sequentially predicting the center points of all text instances inside the same predicting sequence, while with a Parallel Recognition Decoder (PRD) for text recognition in parallel. These two decoders share the same parameters and are interactively connected with a simple but effective information transmission process to pass the gradient and information. Compre
    
[^101]: 在Airbnb上学习多样化的排序方法

    Learning To Rank Diversely At Airbnb. (arXiv:2210.07774v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.07774](http://arxiv.org/abs/2210.07774)

    这篇论文研究了在Airbnb上学习多样化的排序方法。通过纠正传统排序框架中的假设，提出了一种基于高效神经网络架构的排序策略，以实现更准确的房源匹配，并加入了考虑房源相似性的方法以实现多样化的排序。

    

    Airbnb是一个双边市场，将出租房源的房东与来自全球的潜在客人联系在一起。将基于神经网络的学习排序技术应用到匹配客人与房东的过程中已经取得了显著的改进。这些排序改进是通过一种核心策略驱动的：按照其预计的预订概率对房源进行排序，然后迭代地改进这些预订概率估计的技术。然而，这一策略暗含的一个假设是，搜索结果中的每个房源的预订概率可以独立确定。本文讨论了这个假设的错误，并提供了一个纠正这个假设的理论基础，以及基于该理论的高效神经网络架构。通过显式考虑房源之间的可能相似性并减小它们的方法，实现了多样化排序。

    Airbnb is a two-sided marketplace, bringing together hosts who own listings for rent, with prospective guests from around the globe. Applying neural network-based learning to rank techniques has led to significant improvements in matching guests with hosts. These improvements in ranking were driven by a core strategy: order the listings by their estimated booking probabilities, then iterate on techniques to make these booking probability estimates more and more accurate. Embedded implicitly in this strategy was an assumption that the booking probability of a listing could be determined independently of other listings in search results. In this paper we discuss how this assumption, pervasive throughout the commonly-used learning to rank frameworks, is false. We provide a theoretical foundation correcting this assumption, followed by efficient neural network architectures based on the theory. Explicitly accounting for possible similarities between listings, and reducing them to diversify
    
[^102]: 机器学习和计算机视觉技术在蜜蜂监测应用中的应用

    Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2208.00085](http://arxiv.org/abs/2208.00085)

    本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。

    

    机器学习和计算机视觉是快速发展的领域，已经证明能够解决非常复杂的任务。它们可以用于监测蜜蜂群体并检查其健康状况，从而在情况变得严重之前，识别出潜在危险状态，或者更好地计划定期蜜蜂群体检查，从而节省重要的成本。本文概述了用于蜜蜂监测的最先进的计算机视觉和机器学习应用，并以自动化蜜蜂计数算法为例展示了这些方法的潜力。本文面向兽医学和蜜蜂学专业人员和专家，旨在向他们介绍机器学习的可能性，因此每个应用类别都以简要的理论介绍和与其基本方法相关的动机开篇。我们希望这篇论文能激发其他科学家的灵感...

    Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
    
[^103]: 半监督深度多视图立体

    Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.11699](http://arxiv.org/abs/2207.11699)

    本文针对学习式多视图立体问题，探讨了仅有一部分带有深度准确值的数据的半监督设置。通过引入新的半监督分布增强框架，提出了一种方法来解决MVS中的分布间隙多义性问题。

    

    在监督和无监督设置下，学习为基础的多视图立体(MVS)取得了显著进展。为了将它们各自的优点(准确性和完整性)结合起来，同时减少对昂贵有标签数据的需求，本文探讨了仅有一小部分MVS数据附带稠密深度准确值的半监督设置下的学习式MVS问题。然而，由于视图中的场景巨大变化和灵活设置，这可能破坏了经典半监督学习的基本假设，即无标签数据和有标签数据共享相同的标签空间和数据分布，在MVS问题中称为半监督分布间隙多义性。为了处理这些问题，我们提出了一种新颖的半监督分布增强MVS框架，即SDA-MVS。对于MVS数据中基本假设适用的简单情况，一致性正则化鼓励模型预测在原始样本和...

    Significant progress has been witnessed in learning-based Multi-view Stereo (MVS) under supervised and unsupervised settings. To combine their respective merits in accuracy and completeness, meantime reducing the demand for expensive labeled data, this paper explores the problem of learning-based MVS in a semi-supervised setting that only a tiny part of the MVS data is attached with dense depth ground truth. However, due to huge variation of scenarios and flexible settings in views, it may break the basic assumption in classic semi-supervised learning, that unlabeled data and labeled data share the same label space and data distribution, named as semi-supervised distribution-gap ambiguity in the MVS problem. To handle these issues, we propose a novel semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the simple case that the basic assumption works in MVS data, consistency regularization encourages the model predictions to be consistent between original sample and
    
[^104]: ORC: 利用在线角色变换的网络群组知识蒸馏

    ORC: Network Group-based Knowledge Distillation using Online Role Change. (arXiv:2206.01186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01186](http://arxiv.org/abs/2206.01186)

    本文提出了一种利用在线角色变换的网络群组知识蒸馏方法，通过将多个网络分成教师组和学生组，学生组学习教师的知识，使用在线角色变换策略将学生组中排名靠前的网络晋升为教师组，通过训练教师组以改进其知识，实现了更好的知识蒸馏效果。

    

    在知识蒸馏中，由于单一的全能教师网络无法解决所有问题，最近研究了基于多个教师的知识蒸馏。然而，有时它们的改进效果并不如预期，因为一些不成熟的教师可能会将错误的知识传输给学生。为了克服这个限制并利用多个网络的能力，本文将多个网络分成教师组和学生组。学生组由需要学习教师知识的不成熟网络组成，而教师组则由能够成功教授的选中网络组成。我们提出了在线角色变换策略，在每次迭代中将学生组中排名靠前的网络晋升为教师组。通过使用学生组的错误样本训练教师组以改进其知识，我们转移协作知识。

    In knowledge distillation, since a single, omnipotent teacher network cannot solve all problems, multiple teacher-based knowledge distillations have been studied recently. However, sometimes their improvements are not as good as expected because some immature teachers may transfer the false knowledge to the student. In this paper, to overcome this limitation and take the efficacy of the multiple networks, we divide the multiple networks into teacher and student groups, respectively. That is, the student group is a set of immature networks that require learning the teacher's knowledge, while the teacher group consists of the selected networks that are capable of teaching successfully. We propose our online role change strategy where the top-ranked networks in the student group are able to promote to the teacher group at every iteration. After training the teacher group using the error samples of the student group to refine the teacher group's knowledge, we transfer the collaborative kno
    

