# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs.](http://arxiv.org/abs/2304.10532) | 通过使用新的数据集和评估程序，提出了一种使用3D扩散优先级别加上新颖的基于密度的得分蒸馏采样损失的方法来防止NeRF优化过程中出现图形伪影的解决方案。 |
| [^2] | [Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget.](http://arxiv.org/abs/2304.10520) | 本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。 |
| [^3] | [Segment Anything Model for Medical Image Analysis: an Experimental Study.](http://arxiv.org/abs/2304.10517) | 本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。 |
| [^4] | [Distributed Neural Representation for Reactive in situ Visualization.](http://arxiv.org/abs/2304.10516) | 本研究开发了一种分布式体积数据的隐式神经表示技术，结合到反应式编程系统中构建了一个原位时间缓存系统，并在Ascent基础架构中使用实际模拟评估了其100倍容量的性能。 |
| [^5] | [CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network.](http://arxiv.org/abs/2304.10515) | 本文提出一种基于人脑核心-周边原则的类脑设计原则以指导CNN的设计。通过在网络布线模式和卷积操作中实现核心-周边原则，我们提出了一种全新的CP-CNN架构，它在各种视觉任务中表现出色。 |
| [^6] | [Why Does ChatGPT Fall Short in Answering Questions Faithfully?.](http://arxiv.org/abs/2304.10513) | 该论文分析了ChatGPT在问答系统中的失误，归纳并确定其失败的原因类型和关键能力，进一步提出了潜在方法来提高其准确性。 |
| [^7] | [OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets.](http://arxiv.org/abs/2304.10511) | 本文提出了一种名为OutCenTR的半监督框架，用于预测高维度数据集中可能被利用的漏洞，同时提出一种降维技术OutCenTR，可以增强基本的离群点检测模型，实验证明了OutCenTR的有效性和效率，平均F1分数提高了5倍。 |
| [^8] | [Wasserstein Loss for Semantic Editing in the Latent Space of GANs.](http://arxiv.org/abs/2304.10508) | 本论文提出了一种基于Wasserstein损失的GAN潜空间语义编辑方法，避免了对分类器的依赖，并在两个数据集上证明了其有效性。 |
| [^9] | [Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts.](http://arxiv.org/abs/2304.10505) | 本文提出了一种视频预训练Transformer，它使用多个最先进的编码器模型将视频转换成通用表示，可以结合多种模式获得最佳性能。 |
| [^10] | [Autonomic Architecture for Big Data Performance Optimization.](http://arxiv.org/abs/2304.10503) | 本文提出了KERMIT，一种大数据自主架构系统。KERMIT能够自动调优Apache Spark和Hadoop，并在人工经验优化上达到快30%的速度提升，与穷举搜索的性能优化方法相比快92%。KERMIT能够高达99%的准确率检测重要的工作负载变化，并预测未来工作负载类型的准确率高达96%。 |
| [^11] | [Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code.](http://arxiv.org/abs/2304.10500) | 本文探索使用 Transformer 模型进行形式语言的推理。通过使用编程语言最基本的特性——术语和类型之间的关系，提出了一种基准测试，并提出了一种基于 Transformer 的类型推理模型。实验结果表明，TTI 模型在推理准确性和速度方面都优于现有的最先进方法。 |
| [^12] | [Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?.](http://arxiv.org/abs/2304.10494) | 深度学习方法在构象生成中的表现受到质疑，本文研究了基于物理的方法在这个问题中的表现，发现仍是一个有力选项。 |
| [^13] | [Efficient Deep Reinforcement Learning Requires Regulating Overfitting.](http://arxiv.org/abs/2304.10466) | 深度强化学习的主要瓶颈在于高时间差误差的验证集上出现了严重过拟合问题。 |
| [^14] | [Implicit Temporal Modeling with Learnable Alignment for Video Recognition.](http://arxiv.org/abs/2304.10465) | 本文提出了一种隐式学习对齐方法，通过预测交互点并增强周围特征，实现两帧视频的隐式对齐，同时最小化时间建模的工作量。 |
| [^15] | [Phoenix: Democratizing ChatGPT across Languages.](http://arxiv.org/abs/2304.10453) | Phoenix 是一款大型语言模型，实现了跨语言民主化，不仅在英文和中文中表现良好，也在资源有限的语言中表现出色，使得 ChatGPT 在更多的国家和地区变得更加可用。 |
| [^16] | [On the Potential of Artificial Intelligence Chatbots for Data Exploration of Federated Bioinformatics Knowledge Graphs.](http://arxiv.org/abs/2304.10427) | 本文研究了人工智能聊天机器人在联邦生物信息学知识图谱数据挖掘中的潜力，通过与领域专家对话的方式，实现了对跨数据集的查询和解释。 |
| [^17] | [Fully Autonomous Programming with Large Language Models.](http://arxiv.org/abs/2304.10423) | 本文讨论了基于大语言模型的完全自主编程方法，提出了Synthesize、Execute、Debug（SED）方法来解决程序生成的低准确度问题，并比较了不同的调试策略和提示生成方法。 |
| [^18] | [How the Move Acceptance Hyper-Heuristic Copes With Local Optima: Drastic Differences Between Jumps and Cliffs.](http://arxiv.org/abs/2304.10414) | MAHH方法通过解决跳跃函数在小的间隙大小下的表现问题，显着优于简单的精英进化算法。当处理宽悬崖问题时，MAHH仍然是解决局部最优解的高效方法。 |
| [^19] | [Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review.](http://arxiv.org/abs/2304.10410) | 本文为雷达相机融合提供了一份综合指南，主要关注感知任务中的目标检测和语义分割。雷达和相机通过互补的方式提供成本效益高的周围环境感知，本综述深入探讨了数据处理和表示，详细总结了雷达相机融合数据集的情况，并解决了许多方法学问题。 |
| [^20] | [PDL on Steroids: on Expressive Extensions of PDL with Intersection and Converse.](http://arxiv.org/abs/2304.10381) | 介绍了CPDL+，它是一种表达式强大的逻辑系统，扩展了PDL的交集和转置，超越了CQ、CRPQ和已知扩展的表达力。树宽为2的CPDL+公式等价于ICPDL，但增加树宽会增加表达力。 |
| [^21] | [Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.10375) | 提出了一种基于条件注意力的分布式注意力演员架构，利用显著性向量重用环境的条件状态来提高条件协同行为的可解释性和代理性能。 |
| [^22] | [Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.10351) | 该论文提出了一个基于多智能体强化学习的时空序贯决策结构，实现了斯塔克伯格均衡策略的异步行动协调，并在参数共享的同时实现了不同智能体之间的不对称训练。 |
| [^23] | [Controllable Neural Symbolic Regression.](http://arxiv.org/abs/2304.10336) | 本文提出了一种称为带假设的神经符号回归（NSRwH）的方法，能够将用户定义的先验知识纳入到生成分析表达式的预测模型中，我们的方法允许用户指定目标表达式中使用的数学符号类型和范围，并对表达式的复杂性施加约束。 |
| [^24] | [A baseline on continual learning methods for video action recognition.](http://arxiv.org/abs/2304.10335) | 本文基于视频动作识别场景，提出最先进的连续学习方法的基准研究，并证明回顾方法优于其他方法。此外，提出的内存效率变体可有效地保持一定水平的性能。 |
| [^25] | [Towards a Benchmark for Scientific Understanding in Humans and Machines.](http://arxiv.org/abs/2304.10327) | 该论文提出了一个框架来创建衡量人类和人工智能科学理解的基准。他们使用了行为观念，提出了一组问题以衡量不同水平的科学理解。这个框架可以帮助评估和比较不同水平和方法的科学理解。 |
| [^26] | [LA3: Efficient Label-Aware AutoAugment.](http://arxiv.org/abs/2304.10310) | LA3 提出了一种新颖的两阶段数据增强算法，利用标签信息，为不同标签的样本分别学习增强策略，提高深度神经网络训练的泛化性能。 |
| [^27] | [FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits.](http://arxiv.org/abs/2304.10306) | 本论文提出了FIANCEE，通过添加早期退出分支并根据输出难度动态切换计算路径的方法来加速对抗性网络推断，经过验证该方法对于生成任务可以在保持高质量的同时减少计算量，特别适合实时应用。 |
| [^28] | [SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning.](http://arxiv.org/abs/2304.10297) | 本文提出了一个新的自监督学习模型SARF，通过利用同义关系辅助提高了少样本关系推理的泛化性能和准确性，经过实验验证超越了目前最先进的方法。 |
| [^29] | [A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty.](http://arxiv.org/abs/2304.10284) | 本文提出了一种元启发式方法，它可以以人类和机器学习决策都互相关联的因素来表征一个实例的复杂性，以估计分类错误的风险。 |
| [^30] | [The Seven Worlds and Experiences of the Wireless Metaverse: Challenges and Opportunities.](http://arxiv.org/abs/2304.10282) | 本文提出了一个无限的、无线的元宇宙视野，将元宇宙浓缩为七个世界和体验的交叉点，支持人类和虚拟化角色之间以及连接的智能系统和其数字孪生之间的新颖互动。 |
| [^31] | [Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation.](http://arxiv.org/abs/2304.10260) | 该论文提出了一种使用循环一致性生成对抗方法进行领域自适应轨迹模仿的深度强化学习代理DATI，可以学习动态系统中的代表性轨迹，识别交通中异常运动模式，并在各种合成的参考轨迹家族上表现优异。 |
| [^32] | [Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?.](http://arxiv.org/abs/2304.10224) | 本文提出了一种针对3D点云分类的少样本学习网络MvNet，它能够利用现有的2D预训练模型来缓解现有基线模型对大规模注释3D点云数据的过度依赖问题。 |
| [^33] | [Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout.](http://arxiv.org/abs/2304.10191) | 本文提出了一种利用时间步机制实现MC-dropout的高效脉冲神经网络不确定性估计方法，为神经形态学硬件的节能应用提供了新的研究思路。 |
| [^34] | [Using Text-to-Image Generation for Architectural Design Ideation.](http://arxiv.org/abs/2304.10182) | 本研究首次探索了文本到图像生成技术在建筑设计早期阶段支持创造力的潜力。实验结果表明，生成器支持意外的创意发现和富有想象力的心态，丰富了设计过程。 |
| [^35] | [Robust Deep Reinforcement Learning Scheduling via Weight Anchoring.](http://arxiv.org/abs/2304.10176) | 本研究提出了一种使用权重锚定来实现深度强化学习调度的方法，可避免在优化环境下忽略或忘记期望行为，提高了鲁棒性和可操纵性。 |
| [^36] | [Deep Reinforcement Learning Using Hybrid Quantum Neural Network.](http://arxiv.org/abs/2304.10159) | 该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。 |
| [^37] | [Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks.](http://arxiv.org/abs/2304.10145) | 本文研究了ChatGPT在社交计算任务中是否可以复制人类生成的标签注释，结果表明ChatGPT有潜力处理这些数据注释任务，尽管仍存在许多挑战。 |
| [^38] | [Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One.](http://arxiv.org/abs/2304.10126) | 本论文提出将多层图神经网络解耦为多个简单模块的方法，以实现更高效的训练。该方法包括经典的前向训练和设计的反向训练。每个模块都可以通过随机算法在前向训练中高效地训练，并且通过反向训练机制来使前面的模块能够感知后面的模块，从而充分训练浅层模块和更深层的模块。 |
| [^39] | [Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning.](http://arxiv.org/abs/2304.10124) | 该论文提出了一种非对称进化训练（AET）框架，使用自适应数据调整（ADA）和环境随机化（ER）优化AET过程，使得AI可以在复杂的AMP游戏中击败顶级人类玩家，而不需要使用任何人类数据。 |
| [^40] | [Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk.](http://arxiv.org/abs/2304.10118) | 本文提出了一个基于量子游走的算法解决多臂赌博问题，将探索和利用与量子游走行为联系起来，相比于经典随机游走策略实现了高性能。 |
| [^41] | [Two-Memory Reinforcement Learning.](http://arxiv.org/abs/2304.10098) | 本文提出了双记忆强化学习代理 (2M)，它结合了情节记忆和强化学习的优点来提高学习速度和准确性。 |
| [^42] | [ID-MixGCL: Identity Mixup for Graph Contrastive Learning.](http://arxiv.org/abs/2304.10045) | 本文提出了一种基于身份混合的图形对比学习方法，旨在解决通过图形增强得到的不同但相似的图形结构和标签的不匹配问题，以实现更好的表示捕获。 |
| [^43] | [Topological Guided Actor-Critic Modular Learning of Continuous Systems with Temporal Objectives.](http://arxiv.org/abs/2304.10041) | 本文介绍了一种基于拓扑指导的 Actor-Critic 模块化学习方法，用于解决连续系统中的最优策略综合问题，并提出了一种广义的最优备份顺序来加速学习过程。 |
| [^44] | [Open-World Continual Learning: Unifying Novelty Detection and Continual Learning.](http://arxiv.org/abs/2304.10038) | 本文从理论上证明，分布外检测对于类别增量学习是必要的，因为类别增量学习可以分解成任务内预测和任务 ID 预测，并且任务 ID 预测与分布外检测相关。 |
| [^45] | [A Survey on Deep Neural Network Partition over Cloud, Edge and End Devices.](http://arxiv.org/abs/2304.10020) | 本文给出了对云、边缘和端设备上DNN分区方法的全面综述，并提出了一个五维分类框架。这项研究将有助于解决当计算资源有限且数据远程传输代价高昂时提高DNN推理性能的问题。 |
| [^46] | [Separability, Contextuality, and the Quantum Frame Problem.](http://arxiv.org/abs/2304.10010) | 本文研究了可分离性假设和上下文性的关系以及它们与框架问题的关系，并提出了后者的量子模拟和不可判定性证明，同时表明上下文性在状态准备和测量中如何被引导，并指出细微的调整假设在非上下文环境中普遍存在。 |
| [^47] | [Power Law Trends in Speedrunning and Machine Learning.](http://arxiv.org/abs/2304.10004) | 该论文发现了速通世界纪录改进的幂律模式，并利用这一发现提高了预测速通世界纪录精度的方法，并在机器学习基准上得到了类似的结果。结果表明，ML基准远未饱和，而机器学习中的改进具有突然性。 |
| [^48] | [Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion.](http://arxiv.org/abs/2304.09994) | 本研究提出了一种基于CNN-RNN混合特征融合建模的贝叶斯优化城市洪水预测模型，实现了静态和动态的预测，并通过结合多个CNN和RNN模型，在精度上取得了显著提高。 |
| [^49] | [Supporting Human-AI Collaboration in Auditing LLMs with LLMs.](http://arxiv.org/abs/2304.09991) | 本论文通过对安全和公正人工智能专家的采访以及对人工智能协作和感知文献的研究，增强了“AdaTest”审计工具，这个工具可以通过利用人和生成模型的协同优势，进行更严格的大型语言模型审计。 |
| [^50] | [Beyond Transformers for Function Learning.](http://arxiv.org/abs/2304.09979) | 本文通过给Transformer模型增加两个简单的归纳学习偏见探究了学习和预测简单函数的能力，并在大型神经网络模型的背景下发现这些偏见的帮助，同时指出这些偏见可能有助于人类在外推能力方面的归纳学习。 |
| [^51] | [Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision.](http://arxiv.org/abs/2304.09975) | 本文提出了一个无监督的图神经网络解决肾移植问题，可以近似解决NP难问题，并且比其他方法更好。 |
| [^52] | [SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery.](http://arxiv.org/abs/2304.09974) | 本文提出了一种端到端的语言-视觉GPT模型，增强GPT2模型以包括视觉输入，然后微调注意力机制，以在手术VQA任务中更好地理解视觉环境。 |
| [^53] | [Learning policies for resource allocation in business processes.](http://arxiv.org/abs/2304.09970) | 本文提出了两种基于学习的方法来进行企业流程资源分配，具有优于常见启发式方法的效果。 |
| [^54] | [A Latent Space Theory for Emergent Abilities in Large Language Models.](http://arxiv.org/abs/2304.09960) | 本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。 |
| [^55] | [Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers.](http://arxiv.org/abs/2304.09948) | 本研究使用GPT-3模型，比以往的传统机器学习模型（如逻辑回归和支持向量机）性能更出色。同时，本研究揭示虚假评论与真实评论在语言表达上存在的差异，分析了虚假评论的模式和特征。 |
| [^56] | [\"Uberpr\"ufung von Integrit\"atsbedingungen in Deduktiven Datenbanken.](http://arxiv.org/abs/2304.09944) | 本文提出了从逻辑编程的角度解决演绎数据库完整性问题的方法，使用证明树代替SLDNF树更方便，可以使用一组最小的条件指定知识库中更改对完整性约束的有效性的影响。 |
| [^57] | [LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing.](http://arxiv.org/abs/2304.09938) | LARD数据集是用于着陆和进场阶段的跑道检测任务的高质量航拍图像数据集。它由大量合成图像和一些人工标注的实际着陆镜头图像组成，同时还提供了生成器以产生此类合成图像并自动注释跑道拐角点。 |
| [^58] | [Stopping Criteria for Value Iteration on Stochastic Games with Quantitative Objectives.](http://arxiv.org/abs/2304.09930) | 本文提供了具有总奖励和平均收益的随机博弈的第一个价值迭代停止准则，为这些设置中提供了第一个随时算法。 |
| [^59] | [The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages.](http://arxiv.org/abs/2304.09919) | 介绍了一个名为eBible的圣经翻译语料库，包含1009个圣经部分翻译的数据集，涵盖了833种不同语言的数据，分布在75个语言家族中。同时还提供了基于NLLB神经机器翻译模型的性能基准，并讨论了在圣经翻译领域中的一些问题。 |
| [^60] | [MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation.](http://arxiv.org/abs/2304.09913) | 该论文提出了一种名为MARS的方法，可以在弱监督数据中自动去除有偏对象，而无需额外的监督。 |
| [^61] | [GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models.](http://arxiv.org/abs/2304.09875) | 本文提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。该分数捕捉了所有样本中的平均认证防攻击扰动水平，无需运行对抗性攻击。 |
| [^62] | [ChatGPT as a Therapist Assistant: A Suitability Study.](http://arxiv.org/abs/2304.09873) | 本文提出使用ChatGPT作为心理治疗师的助手，可以积极参与对话、整理信息、提供验证和潜在应对策略，并帮助治疗师从多次对话中发现新的见解。 |
| [^63] | [A Theory on Adam Instability in Large-Scale Machine Learning.](http://arxiv.org/abs/2304.09871) | Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。 |
| [^64] | [Heterogeneous-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.09870) | 提出了一种异构智能体强化学习（HARL）算法，解决了协作多智能体强化学习中的参数共享限制，同时通过引入多智能体优势分解引理和序列更新方案，建立了异构智能体信任区域学习（HATRL）算法及其易处理的逼近方式 HATRPO 和 HAPPO。此外，发现了一种名为异构智能体镜像学习（HAML）的新型框架，加强了对HATRPO和HAPPO的理论保证。 |
| [^65] | [Evolving Constrained Reinforcement Learning Policy.](http://arxiv.org/abs/2304.09869) | 本文提出了一种新颖的进化约束强化学习算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中取得优异性能。 |
| [^66] | [Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?.](http://arxiv.org/abs/2304.09868) | 本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。 |
| [^67] | [Introducing Construct Theory as a Standard Methodology for Inclusive AI Models.](http://arxiv.org/abs/2304.09867) | 构建论为人工智能模型的建构提供了更全面多元的视角，可有效解决现有模型中存在的偏见与歧视问题。 |
| [^68] | [Towards Designing a ChatGPT Conversational Companion for Elderly People.](http://arxiv.org/abs/2304.09866) | 本文为解决老年人的孤独和社交孤立问题，设计了一种基于ChatGPT的对话伴侣系统，并通过初步研究进行了评估，该系统能够生成与老年人相关的回应，但需注意其局限性和伦理影响。 |
| [^69] | [Safer Conversational AI as a Source of User Delight.](http://arxiv.org/abs/2304.09865) | 本文研究了对话型AI系统管理对用户体验的影响。研究结果表明，通过适度的管理可以提高用户留存率，证明了采用AI安全机制构建能让用户喜悦的对话型AI的重要性。 |
| [^70] | [End-User Development for Artificial Intelligence: A Systematic Literature Review.](http://arxiv.org/abs/2304.09863) | 本文综述了终端用户开发（EUD）对人工智能系统的影响，目的是使非技术用户直接以满足其需求的方式参与到人工智能的定义和个性化中。此外，文章还评估了EUD面临的挑战、潜在的好处以及将其整合到整个人工智能开发中的未来影响。 |
| [^71] | [NRTS: A Client-Server architecture for supporting data recording, transmission and evaluation of multidisciplinary teams during the neonatal resuscitation simulation scenario.](http://arxiv.org/abs/2304.09860) | 介绍了一种名为NRTS的Android移动应用程序，可以自动将新生儿复苏模拟课程期间记录的所有数据从医院的新生儿重症监护室发送到位于意大利皮埃蒙特东部大学的服务器上。医学教练可以使用该应用程序查看有关模拟练习的统计信息，以对涉及模拟场景的多学科团队进行评估。 |
| [^72] | [The Future of ChatGPT-enabled Labor Market: A Preliminary Study.](http://arxiv.org/abs/2304.09823) | 本研究从人工智能协作的角度，通过分析大规模职位发布数据及基于职业知识图谱开发的协同过滤算法，预测了ChatGPT对未来劳动力市场的影响，发现目前约28％的职业需要ChatGPT相关技能。 |
| [^73] | [Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach.](http://arxiv.org/abs/2304.08495) | 本论文介绍了一种名为SCAIR的算法，可以优化群体效用，解决行程规划中的多个用户排队时间和人群水平优化的问题。 |
| [^74] | [CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction.](http://arxiv.org/abs/2304.07072) | CornerFormer是一种新的方法，它利用不同建模策略于单个模型中融合角点检测和边缘预测来提升精细结构重建的表现，并在挑战性的基准测试中取得了最好的结果。 |
| [^75] | [The Second Monocular Depth Estimation Challenge.](http://arxiv.org/abs/2304.07051) | 本文介绍了单目深度估计挑战赛的第二届比赛结果，高质量的SYNS-Patches数据集提高了比赛难度，所有提交作品都超过了基准水平。 |
| [^76] | [Emergence of Symbols in Neural Networks for Semantic Understanding and Communication.](http://arxiv.org/abs/2304.06377) | 本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。 |
| [^77] | [Deep Active Alignment of Knowledge Graph Entities and Schemata.](http://arxiv.org/abs/2304.04389) | 本文提出了一种基于深度学习和主动学习的KG对齐方法DAAKG，可以联合对齐不仅实体，还包括关系和类别；通过主动学习选择最佳批次进行人工标注，实验结果显示其优越精度和泛化性。 |
| [^78] | [Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4.](http://arxiv.org/abs/2304.03439) | 本文分析了多个逻辑推理数据集，评估了ChatGPT和GPT-4在逻辑推理任务上的表现，并构造了一个逻辑推理的分布之外的数据集来研究它们的鲁棒性。实验结果显示，ChatGPT在大多数逻辑推理基准测试中的表现远优于RoBERTa微调方法，而GPT-4的表现则更高。 |
| [^79] | [Machine Learning for Economics Research: When What and How?.](http://arxiv.org/abs/2304.00086) | 本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。 |
| [^80] | [Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects.](http://arxiv.org/abs/2303.13769) | 本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。 |
| [^81] | [MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder.](http://arxiv.org/abs/2303.04651) | MCTS-GEB是一个通用的重写系统，使用强化学习和蒙特卡洛树搜索来构建最优的E图，有效消除了E图构建中的顺序问题，并在评估中表现出很好的性能。 |
| [^82] | [CUREE: A Curious Underwater Robot for Ecosystem Exploration.](http://arxiv.org/abs/2303.03943) | CUREE是一种水下机器人，通过机器人行为和感知算法提供了一组独特的能力，例如低空视觉调查、声景调查、栖息地表征和动物跟踪。在珊瑚礁上的两次实地部署中，CUREE成功识别了拍摄虾的首选栖息地类型，并追踪和跟踪了一群觅食的外科医生鱼，收集了它们的行为数据。 |
| [^83] | [Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning.](http://arxiv.org/abs/2303.00396) | 本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。 |
| [^84] | ["Correct answers" from the psychology of artificial intelligence.](http://arxiv.org/abs/2302.07267) | 本文使用OpenAI的GPT3.5模型重新复制了Many Labs 2复制项目中的14项研究，其中8项研究的结果被成功复制。然而，对于剩下的6项研究，GPT3.5以极其预定的方式回答了调查问题，导致无法分析这些研究。 |
| [^85] | [Evolving Flying Machines in Minecraft Using Quality Diversity.](http://arxiv.org/abs/2302.00782) | 本文使用优质多样性算法进化Minecraft游戏中的飞行器，比传统适应度算法更可靠地工作。 |
| [^86] | [Robust Attributed Graph Alignment via Joint Structure Learning and Optimal Transport.](http://arxiv.org/abs/2301.12721) | 本文提出了一种称为SLOTAlign的无监督图对齐框架，其通过结构学习和最优输运来解决图之间的结构和特征不一致性问题，同时具有更好的对齐精度和鲁棒性。 |
| [^87] | [MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid.](http://arxiv.org/abs/2212.14454) | 该论文提出了一种适用于元模态混合的多模式实体对齐变压器方法，通过动态预测模态之间的相互关联系数以进行实体级特征聚合，进一步提出了一种模态感知的硬实体重播策略，用于解决模糊实体细节的问题。该模型在多个训练场景中实现了SOTA性能并有效提高了MMEA的鲁棒性。 |
| [^88] | [MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling.](http://arxiv.org/abs/2212.05989) | MegaCRN是一个使用时空元图学习机制的元图卷积递归网络，对于时空建模具有良好的性能。 |
| [^89] | [Effects of Spectral Normalization in Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2212.05331) | 本文研究多智能体强化学习中评论者的规范化方法，发现使用谱归一化(SN)可以使评论家更稳健地学习，特别是在稀疏奖励场景下。实验结果表明，在规范化评论家的情况下，能够快速轻松地从那些复杂情境中学习，这一点对于实现稳定学习非常重要。 |
| [^90] | [PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks.](http://arxiv.org/abs/2212.02397) | 本论文提出了PowRL框架来通过强化学习来有效应对电力网络中的不确定情况，并保持电网的可靠运行。 |
| [^91] | [Continuous Episodic Control.](http://arxiv.org/abs/2211.15183) | CEC是一种新颖的非参数情景记忆算法，用于连续性行动空间问题中的序列决策制定，其在几个稀疏奖励连续控制环境中比最先进的RL和记忆增强RL算法学习更快，是学习连续控制任务的快速方法。 |
| [^92] | [Model free variable importance for high dimensional data.](http://arxiv.org/abs/2211.08414) | 该论文提出了一种无模型的变量重要性方法，可以与任意预测函数一起使用，不需要访问预测函数，可用于研究模型残差。引入了Cohort Shapley的积分梯度版本（IGCS），使得在二元预测器情况下也可以使用IG方法。 |
| [^93] | [Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation.](http://arxiv.org/abs/2211.02658) | 机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。 |
| [^94] | [Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses.](http://arxiv.org/abs/2211.01131) | 本文展示了利用磁隧道结将多频率RF信号进行分类的方法，成为实现嵌入式射频人工智能的关键一步。 |
| [^95] | [Federated Learning with Intermediate Representation Regularization.](http://arxiv.org/abs/2210.15827) | 本文提出一种新的联邦学习方法FedIntR，可以通过将中间层的表示集成到本地训练过程中，提供更细粒度的正则化，从而限制本地训练过程中偏离全局模型的程度，以提高模型性能。 |
| [^96] | [To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing.](http://arxiv.org/abs/2209.02166) | 本文提出了一种计算和通信延迟估计优化框架，并采用基于强化学习的方法来动态分配边缘的感知和计算资源，从而使智能传感器网络在资源受限的边缘计算中实现更佳的性能。 |
| [^97] | [Negative Human Rights as a Basis for Long-term AI Safety and Regulation.](http://arxiv.org/abs/2208.14788) | 本文探讨了负面人权如何作为一种通用原则，为未来AI系统的技术安全约束提供基础，并支持国际监管体系的制定。 |
| [^98] | [An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System.](http://arxiv.org/abs/2207.07886) | 该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。 |
| [^99] | [Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes.](http://arxiv.org/abs/2207.02715) | 本文提出了一种使用多项式Zonotopes进行开环和闭环神经网络验证的新方法。其主要应用于神经网络控制系统的可达性分析，并且能够捕捉非凸性，相较于其他方法表现更出色。 |
| [^100] | [A Search-Based Testing Approach for Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2206.07813) | 本文提出一种基于搜索的测试方法，探索状态空间以检测DRL代理的安全性，并在三个基准测试中取得了比基线方法更高的状态空间覆盖率。 |
| [^101] | [Independent and Decentralized Learning in Markov Potential Games.](http://arxiv.org/abs/2205.14590) | 独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。 |
| [^102] | [FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning.](http://arxiv.org/abs/2205.11039) | FLEX是一种新型KGR框架，能够真正处理包括合取、析取、否定等所有FOL操作并支持各种特征空间。实验证明，FLEX优于现有最先进算法。 |
| [^103] | [Communication-Efficient Adaptive Federated Learning.](http://arxiv.org/abs/2205.02719) | 本文提出了一种新的通信效率高的自适应联邦学习方法（FedCAMS），可以解决联邦学习中由于重复的服务器-客户端同步而产生的大量通信开销和基于 SGD 的模型更新缺乏适应性等诸多问题。 |
| [^104] | [AI based Log Analyser: A Practical Approach.](http://arxiv.org/abs/2203.10960) | 本研究使用Transformer构建仅使用正常的日志条目来训练新模型，并通过多种形式的扰动来增强日志，以应对日志来源的异构性和缺乏标签用于训练分类器的限制；该模型进一步使用一种有限的标签样本进行强化学习的方式进行微调，实验结果表现良好。 |
| [^105] | [Probabilistic Approach for Road-Users Detection.](http://arxiv.org/abs/2112.01360) | 本文介绍了一种用于缓解深度目标检测模型中过于自信预测问题的方法，通过引入一种新颖的概率层来避免传统的预测层，实验证明该方法能够减少假阳性中的过度自信。 |
| [^106] | [Boundary Graph Neural Networks for 3D Simulations.](http://arxiv.org/abs/2106.11299) | 本文提出了一种新型边界图神经网络（BGNNs），能够有效地表示三维颗粒流动的复杂几何形状，从而实现了高效的预测和计算。 |
| [^107] | [The ERA of FOLE: Foundation.](http://arxiv.org/abs/1512.07430) | 本文讨论本体在FOLE一阶逻辑环境中的表示，特别是提供了ERA数据模型的严格数学表示，作为本体论的基础探讨。 |

# 详细

[^1]: Nerfbusters：从随意捕获的NeRF中去除幽灵似的图像伪影

    Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs. (arXiv:2304.10532v1 [cs.CV])

    [http://arxiv.org/abs/2304.10532](http://arxiv.org/abs/2304.10532)

    通过使用新的数据集和评估程序，提出了一种使用3D扩散优先级别加上新颖的基于密度的得分蒸馏采样损失的方法来防止NeRF优化过程中出现图形伪影的解决方案。

    

    随意捕获的神经辐射场（NeRF）在渲染摄像机轨迹之外时会出现浮点错误或有缺陷的几何图形等伪影。现有的评估协议通常无法捕捉这些效应，因为通常仅在训练抓取的每个第八帧评估图像质量。为了推动新视角合成的进展，我们提出了一个新的数据集和评估程序，其中记录了场景的两个摄像机轨迹：一个用于训练，另一个用于评估。在这种更具挑战性的实际环境中，我们发现现有的手工制作的规则不仅不能去除浮点错误，而且也不能改善场景几何形状。因此，我们提出了一种基于3D扩散的方法，该方法利用本地3D先验和新颖的基于密度的得分蒸馏采样损失，在NeRF优化过程中防止出现伪像现象。我们展示了这种基于数据的优先级别能够去除浮点错误并改善随意捕获的场景几何形状。

    Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To push forward progress in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.
    
[^2]: 对比调节: 帮助遗忘掩码自编码器的一点小帮助

    Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])

    [http://arxiv.org/abs/2304.10520](http://arxiv.org/abs/2304.10520)

    本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。

    

    掩码图像建模方法，如掩码自编码器（MAE），可以有效地学习输入的丰富表示。但是，为了适应下游任务，由于其丰富的特征不仅捕获了对象而且还包括不相关的图像背景，因此它们需要足够数量的标记数据。相比之下，实例辨别方法侧重于对象。在这项工作中，我们研究如何将MIM的效率和可伸缩性与ID的能力相结合，以在缺少大量标记数据的情况下执行下游分类。为此，我们引入了掩码自编码器对比调整（MAE-CT），这是一种顺序方法，可以将最近邻对比学习（NNCLR）应用于预先训练的MAE。MAE-CT调整了丰富的特征，使它们形成对象的语义聚类，而不使用任何标签。应用于大型和巨型Vision Transformer（ViT）模型时，MAE-CT在线性探测，k-均值聚类和半监督少量样本学习方面匹配或超越了在ImageNet上训练的先前的自我监督方法。

    Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
    
[^3]: 医学图像分析中的任意分割模型：一项实验研究

    Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])

    [http://arxiv.org/abs/2304.10517](http://arxiv.org/abs/2304.10517)

    本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。

    

    由于数据注释的有限可用性和获取成本，训练医学图像分割模型仍然具有挑战性。Segment Anything Model（SAM）是一种基础模型，经过超过10亿个注释的训练，主要用于自然图像，旨在能够以交互方式分割用户定义的感兴趣的对象。尽管SAM在自然图像上表现出色，但不清楚该模型在转换到医学图像领域时会受到多大影响。在这里，我们对SAM在各种模态和解剖学的11个医学图像数据集上进行了广泛的评估。在我们的实验中，我们使用标准方法生成点提示来模拟交互分割。实验结果表明，SAM基于单点提示的表现在任务和数据集方面高度变化，即从脊柱MRI数据集的0.1135到髋关节X射线数据集的0.8650。

    Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
    
[^4]: 分布式神经表示技术用于反应式原位可视化

    Distributed Neural Representation for Reactive in situ Visualization. (arXiv:2304.10516v1 [cs.DC])

    [http://arxiv.org/abs/2304.10516](http://arxiv.org/abs/2304.10516)

    本研究开发了一种分布式体积数据的隐式神经表示技术，结合到反应式编程系统中构建了一个原位时间缓存系统，并在Ascent基础架构中使用实际模拟评估了其100倍容量的性能。

    

    利用反应式编程实现计算模型的原位可视化和控制十分高效，它利用时间抽象和数据缓存机制来创建动态工作流。然而，对于大规模模拟，实现时间缓存可能存在挑战。隐式神经网络已被证明在压缩大型数据方面是有效的。然而，它们在分布式数据上的应用还没有被充分探索。在本研究中，我们开发了一种分布式体积数据的隐式神经表示，并将其结合到DIVA反应式编程系统中。这种实现使我们能够构建一个原位时间缓存系统，其容量比以前的容量大100倍。我们将这种方法集成到Ascent基础架构中，并使用实际模拟来评估其性能。

    In situ visualization and steering of computational modeling can be effectively achieved using reactive programming, which leverages temporal abstraction and data caching mechanisms to create dynamic workflows. However, implementing a temporal cache for large-scale simulations can be challenging. Implicit neural networks have proven effective in compressing large volume data. However, their application to distributed data has yet to be fully explored. In this work, we develop an implicit neural representation for distributed volume data and incorporate it into the DIVA reactive programming system. This implementation enables us to build an in situ temporal caching system with a capacity 100 times larger than previously achieved. We integrate our implementation into the Ascent infrastructure and evaluate its performance using real-world simulations.
    
[^5]: CP-CNN: 基于核心-周边原则指导的卷积神经网络

    CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network. (arXiv:2304.10515v1 [cs.NE])

    [http://arxiv.org/abs/2304.10515](http://arxiv.org/abs/2304.10515)

    本文提出一种基于人脑核心-周边原则的类脑设计原则以指导CNN的设计。通过在网络布线模式和卷积操作中实现核心-周边原则，我们提出了一种全新的CP-CNN架构，它在各种视觉任务中表现出色。

    

    卷积神经网络(CNN)的发展很大程度上归因于其架构设计，即网络布线模式。神经结构搜索(NAS)通过自动化搜索最优网络架构推进了这一进展，但所得到的网络实例在不同任务中可能无法泛化。为了解决这个问题，探索跨任务通用的网络设计原则是更实际的解决方案。本研究基于人脑网络的核心-周边属性探索了一种新颖的类脑设计原则，以指导CNN的设计。我们的工作从最近的研究中汲取灵感，这些研究表明人工和生物神经网络在优化网络架构方面可能具有共同的原则。我们在网络布线模式的设计和卷积操作的稀疏化中实现了核心-周边原则。结果，使用核心-周边原则指导的CNN (CP-CNN)在各种视觉任务上得到了良好的表现。

    The evolution of convolutional neural networks (CNNs) can be largely attributed to the design of its architecture, i.e., the network wiring pattern. Neural architecture search (NAS) advances this by automating the search for the optimal network architecture, but the resulting network instance may not generalize well in different tasks. To overcome this, exploring network design principles that are generalizable across tasks is a more practical solution. In this study, We explore a novel brain-inspired design principle based on the core-periphery property of the human brain network to guide the design of CNNs. Our work draws inspiration from recent studies suggesting that artificial and biological neural networks may have common principles in optimizing network architecture. We implement the core-periphery principle in the design of network wiring patterns and the sparsification of the convolution operation. The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated on t
    
[^6]: 为什么ChatGPT无法准确回答问题？

    Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])

    [http://arxiv.org/abs/2304.10513](http://arxiv.org/abs/2304.10513)

    该论文分析了ChatGPT在问答系统中的失误，归纳并确定其失败的原因类型和关键能力，进一步提出了潜在方法来提高其准确性。

    

    最近，大型语言模型，如ChatGPT，展示出对人类生活各方面产生重大影响的巨大潜力。然而，ChatGPT在诚实性等方面仍然面临挑战。以问答系统为代表应用程序，我们试图了解为什么ChatGPT在准确回答问题方面有所不足。为了回答这个问题，我们试图分析ChatGPT在复杂的开放领域问答中失败的原因，并确定与这些失败有关的能力。具体来说，我们将ChatGPT的失败归为四种类型：理解、事实性、具体性和推理。我们进一步确定了与QA失败有关的三个关键能力：知识记忆、知识关联和知识推理。此外，我们还进行了围绕这些能力的实验，并提出了提高准确性的潜在方法。结果表明，向模型提供细粒度的外部知识、给予提示来帮助它聚焦并加强关键能力，这都有助于提高其准确性。

    Recent advancements in Large Language Models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life. However, ChatGPT still faces challenges in aspects like faithfulness. Taking question answering as a representative application, we seek to understand why ChatGPT falls short in answering questions faithfully. To address this question, we attempt to analyze the failures of ChatGPT in complex open-domain question answering and identifies the abilities under the failures. Specifically, we categorize ChatGPT's failures into four types: comprehension, factualness, specificity, and inference. We further pinpoint three critical abilities associated with QA failures: knowledge memorization, knowledge association, and knowledge reasoning. Additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance faithfulness. The results indicate that furnishing the model with fine-grained external knowledge, hints for
    
[^7]: OutCenTR: 一种用于高维数据集中预测漏洞利用的半监督框架

    OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets. (arXiv:2304.10511v1 [cs.CR])

    [http://arxiv.org/abs/2304.10511](http://arxiv.org/abs/2304.10511)

    本文提出了一种名为OutCenTR的半监督框架，用于预测高维度数据集中可能被利用的漏洞，同时提出一种降维技术OutCenTR，可以增强基本的离群点检测模型，实验证明了OutCenTR的有效性和效率，平均F1分数提高了5倍。

    

    每天都有越来越多的漏洞被报道，然而这些漏洞并不都是相同的，有些比其他的更具有针对性。正确估计漏洞被利用的可能性是系统管理员的关键任务。本文利用离群点检测技术来预测高度不平衡和高维度数据集中可能被利用的漏洞，例如国家漏洞数据库。我们提出了一种降维技术OutCenTR，可以增强基本的离群点检测模型。我们在4个基准数据集和12个合成数据集上通过实验证明了OutCenTR的有效性和效率。实验结果显示，与PCA和GRP等最先进的降维技术相比，平均F1分数提高了5倍。

    An ever-growing number of vulnerabilities are reported every day. Yet these vulnerabilities are not all the same; Some are more targeted than others. Correctly estimating the likelihood of a vulnerability being exploited is a critical task for system administrators. This aids the system administrators in prioritizing and patching the right vulnerabilities. Our work makes use of outlier detection techniques to predict vulnerabilities that are likely to be exploited in highly imbalanced and high-dimensional datasets such as the National Vulnerability Database. We propose a dimensionality reduction technique, OutCenTR, that enhances the baseline outlier detection models. We further demonstrate the effectiveness and efficiency of OutCenTR empirically with 4 benchmark and 12 synthetic datasets. The results of our experiments show on average a 5-fold improvement of F1 score in comparison with state-of-the-art dimensionality reduction techniques such as PCA and GRP.
    
[^8]: GAN潜空间语义编辑中的Wasserstein Loss

    Wasserstein Loss for Semantic Editing in the Latent Space of GANs. (arXiv:2304.10508v1 [cs.CV])

    [http://arxiv.org/abs/2304.10508](http://arxiv.org/abs/2304.10508)

    本论文提出了一种基于Wasserstein损失的GAN潜空间语义编辑方法，避免了对分类器的依赖，并在两个数据集上证明了其有效性。

    

    GAN的潜在空间包含了训练数据的丰富语义信息。不同的方法提出在潜空间中学习与语义属性相对应的编辑方法，从而可以修改生成的图像。大多数监督方法依赖于分类器的指导来产生编辑。然而，分类器可能会导致超出分布区域，同时被对抗样本欺骗。我们提出了一种基于Wasserstein损失的替代公式，避免了这些问题，同时在性能上与基于分类器的方法相当。我们使用StyleGAN2在两个数据集（数字和人脸）上展示了我们方法的有效性。

    The latent space of GANs contains rich semantics reflecting the training data. Different methods propose to learn edits in latent space corresponding to semantic attributes, thus allowing to modify generated images. Most supervised methods rely on the guidance of classifiers to produce such edits. However, classifiers can lead to out-of-distribution regions and be fooled by adversarial samples. We propose an alternative formulation based on the Wasserstein loss that avoids such problems, while maintaining performance on-par with classifier-based approaches. We demonstrate the effectiveness of our method on two datasets (digits and faces) using StyleGAN2.
    
[^9]: 视频预训练Transformer: 多模态预训练专家混合体

    Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts. (arXiv:2304.10505v1 [cs.CV])

    [http://arxiv.org/abs/2304.10505](http://arxiv.org/abs/2304.10505)

    本文提出了一种视频预训练Transformer，它使用多个最先进的编码器模型将视频转换成通用表示，可以结合多种模式获得最佳性能。

    

    本文提出了视频预训练Transformer (VPT)。VPT使用来自之前工作的四个最先进的编码器模型将视频转换成紧凑的嵌入序列。我们的骨干网络基于参考Flan-T5-11B架构，学习视频的通用表示形式，这是编码器模型的非线性总和。它使用自回归因果语言建模损失通过预测YouTube视频中讲话的单词进行学习。最后，我们通过为每个任务训练全连接预测头，在标准的下游基准测试中进行评估。据我们所知，这是第一个在“嵌入->骨干网络->预测头”设计模式中使用多个冻结的最先进模型作为编码器的研究，而其他研究都是训练自己的联合编码器模型。此外，我们通过添加显式的场景图信息，包含了比当前SOTA Merlot Reserve更多的模态。出于这两个原因，我们相信它可以将世界上最好的开源模型结合起来，实现SOTA性能。

    We present Video Pre-trained Transformer. VPT uses four SOTA encoder models from prior work to convert a video into a sequence of compact embeddings. Our backbone, based on a reference Flan-T5-11B architecture, learns a universal representation of the video that is a non-linear sum of the encoder models. It learns using an autoregressive causal language modeling loss by predicting the words spoken in YouTube videos. Finally, we evaluate on standard downstream benchmarks by training fully connected prediction heads for each task. To the best of our knowledge, this is the first use of multiple frozen SOTA models as encoders in an "embedding -> backbone -> prediction head" design pattern - all others have trained their own joint encoder models. Additionally, we include more modalities than the current SOTA, Merlot Reserve, by adding explicit Scene Graph information. For these two reasons, we believe it could combine the world's best open-source models to achieve SOTA performance. Initial 
    
[^10]: 大数据性能优化的自主架构

    Autonomic Architecture for Big Data Performance Optimization. (arXiv:2304.10503v1 [cs.DC])

    [http://arxiv.org/abs/2304.10503](http://arxiv.org/abs/2304.10503)

    本文提出了KERMIT，一种大数据自主架构系统。KERMIT能够自动调优Apache Spark和Hadoop，并在人工经验优化上达到快30%的速度提升，与穷举搜索的性能优化方法相比快92%。KERMIT能够高达99%的准确率检测重要的工作负载变化，并预测未来工作负载类型的准确率高达96%。

    

    基于Apache Spark和Hadoop开发的大数据软件栈已成为许多企业的重要任务。Spark和Hadoop作业的性能取决于许多配置设置。手动调整是昂贵且易碎的。过去已经努力开发在线和离线自动调优方法，使大数据栈不再依赖于手动调优。然而，这些方法只能在小数据集上进行非常简单的单用户工作负载，合理性能优化方面贡献有限。本文提出了KERMIT - 大数据自主架构系统，能够自动调优Apache Spark和Hadoop，并实现比人工管理员的经验优化效果快30%，与执行调优参数空间的穷举搜索的性能优化方法相比，可实现高达92%的速度提升。KERMIT能够高达99%的准确率检测重要的工作负载变化，并预测未来工作负载类型的准确率高达96%。

    The big data software stack based on Apache Spark and Hadoop has become mission critical in many enterprises. Performance of Spark and Hadoop jobs depends on a large number of configuration settings. Manual tuning is expensive and brittle. There have been prior efforts to develop on-line and off-line automatic tuning approaches to make the big data stack less dependent on manual tuning. These, however, demonstrated only modest performance improvements with very simple, single-user workloads on small data sets. This paper presents KERMIT - the autonomic architecture for big data capable of automatically tuning Apache Spark and Hadoop on-line, and achieving performance results 30% faster than rule-of-thumb tuning by a human administrator and up to 92% as fast as the fastest possible tuning established by performing an exhaustive search of the tuning parameter space. KERMIT can detect important workload changes with up to 99% accuracy, and predict future workload types with up to 96% accu
    
[^11]: 使用 Transformer 模型进行 Simply Typed Lambda Calculus 的类型推理：深度学习在代码中的案例研究

    Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code. (arXiv:2304.10500v1 [cs.PL])

    [http://arxiv.org/abs/2304.10500](http://arxiv.org/abs/2304.10500)

    本文探索使用 Transformer 模型进行形式语言的推理。通过使用编程语言最基本的特性——术语和类型之间的关系，提出了一种基准测试，并提出了一种基于 Transformer 的类型推理模型。实验结果表明，TTI 模型在推理准确性和速度方面都优于现有的最先进方法。

    

    尽管深度学习和形式语言的交叉领域的研究越来越多，但系统性地探索使用 Transformer 模型推理有类型 lambda 演算方面的研究相对较少。这是一个有趣的研究领域，有两个原因。首先，有类型的 lambda 演算是编程语言的基础。将各种有类型 lambda 演算与有效神经网络结构相关联的一组启发式方法会提供一种将语言特征（例如多态性，子类型，继承等）映射到架构选择的系统方法。其次，Transformer 模型广泛用于应用于代码的深度学习架构中，但其设计和超参数空间在编程语言应用中尚未得到充分探索。因此，我们建议通过也许是编程语言最简单和最基本的特性，即术语和类型之间的关系，来探索这一点的基准测试。

    Despite a growing body of work at the intersection of deep learning and formal languages, there has been relatively little systematic exploration of transformer models for reasoning about typed lambda calculi. This is an interesting area of inquiry for two reasons. First, typed lambda calculi are the lingua franc of programming languages. A set of heuristics that relate various typed lambda calculi to effective neural architectures would provide a systematic method for mapping language features (e.g., polymorphism, subtyping, inheritance, etc.) to architecture choices. Second, transformer models are widely used in deep learning architectures applied to code, but the design and hyperparameter space for them is large and relatively unexplored in programming language applications. Therefore, we suggest a benchmark that allows us to explore exactly this through perhaps the simplest and most fundamental property of a programming language: the relationship between terms and types. Consequent
    
[^12]: 无限物理猴子：深度学习方法在构象生成中真的比传统方法表现更好吗？

    Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?. (arXiv:2304.10494v1 [q-bio.BM])

    [http://arxiv.org/abs/2304.10494](http://arxiv.org/abs/2304.10494)

    深度学习方法在构象生成中的表现受到质疑，本文研究了基于物理的方法在这个问题中的表现，发现仍是一个有力选项。

    

    构象生成是药物发现和化学信息学中的基本问题，有机分子构象生成，特别是在真空和蛋白质口袋环境中，与药物设计最相关。最近，随着几何神经网络的发展，数据驱动的方案已成功应用于这一领域，包括分子构象生成（在真空中）和结合位姿生成（在蛋白质口袋中）。前者打败了传统的ETKDG方法，而后者达到了与广泛使用的分子对接软件相似的精度。尽管这些方法已经显示出了有希望的结果，但一些研究人员最近质疑深度学习（DL）方法是否通过无参数方法更好地在分子构象生成中表现。令人惊讶的是，他们设计的方法有点类似于著名的无限猴子定理，这些猴子甚至装备了物理教育。本文旨在讨论该方法的可行性，并且通过简单的实验，我们发现传统的基于物理的方法仍然是构象生成问题中的有力选项。

    Conformation Generation is a fundamental problem in drug discovery and cheminformatics. And organic molecule conformation generation, particularly in vacuum and protein pocket environments, is most relevant to drug design. Recently, with the development of geometric neural networks, the data-driven schemes have been successfully applied in this field, both for molecular conformation generation (in vacuum) and binding pose generation (in protein pocket). The former beats the traditional ETKDG method, while the latter achieves similar accuracy compared with the widely used molecular docking software. Although these methods have shown promising results, some researchers have recently questioned whether deep learning (DL) methods perform better in molecular conformation generation via a parameter-free method. To our surprise, what they have designed is some kind analogous to the famous infinite monkey theorem, the monkeys that are even equipped with physics education. To discuss the feasib
    
[^13]: 高效深度强化学习需要抑制过拟合

    Efficient Deep Reinforcement Learning Requires Regulating Overfitting. (arXiv:2304.10466v1 [cs.LG])

    [http://arxiv.org/abs/2304.10466](http://arxiv.org/abs/2304.10466)

    深度强化学习的主要瓶颈在于高时间差误差的验证集上出现了严重过拟合问题。

    

    通过与环境的交互收集有限的数据进行策略学习的深度强化学习算法，需要正确的正则化技巧才能实现数据高效利用。本文通过检验几种假设，如非稳态性、过度动作分布偏移和过拟合等，试图理解在样本高效的深度强化学习中主要的瓶颈。我们对DeepMind控制套件（DMC）任务进行了彻底的实证分析，以一种有控制、系统的方式展示了对转换的验证集的高时间差（TD）误差是严重影响深度强化学习算法性能的主要罪魁祸首，而先前的方法......(未完整翻译)

    Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and systematic way to show that high temporal-difference (TD) error on the validation set of transitions is the main culprit that severely affects the performance of deep RL algorithms, and prior method
    
[^14]: 可学习对齐的隐式时间建模用于视频识别

    Implicit Temporal Modeling with Learnable Alignment for Video Recognition. (arXiv:2304.10465v1 [cs.CV])

    [http://arxiv.org/abs/2304.10465](http://arxiv.org/abs/2304.10465)

    本文提出了一种隐式学习对齐方法，通过预测交互点并增强周围特征，实现两帧视频的隐式对齐，同时最小化时间建模的工作量。

    

    对比语言-图像预训练(CLIP)在各种图像任务中都取得了显著的成功。然而，如何有效地扩展CLIP以进行时间建模仍然是一个开放且关键的问题。本文提出了一种新颖的隐式学习对齐(ILA)方法，通过最小化时间建模的工作量，同时实现了极高的性能。具体而言，对于一帧对，每帧都预测一个交互点，作为彼此信息丰富的区域。通过增强交互点周围的特征，两帧被隐式对齐。对齐的特征然后被汇集成一个令牌，用于后续的空间建模。

    Contrastive language-image pretraining (CLIP) has demonstrated remarkable success in various image tasks. However, how to extend CLIP with effective temporal modeling is still an open and crucial problem. Existing factorized or joint spatial-temporal modeling trades off between the efficiency and performance. While modeling temporal information within straight through tube is widely adopted in literature, we find that simple frame alignment already provides enough essence without temporal attention. To this end, in this paper, we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizes the temporal modeling effort while achieving incredibly high performance. Specifically, for a frame pair, an interactive point is predicted in each frame, serving as a mutual information rich region. By enhancing the features around the interactive point, two frames are implicitly aligned. The aligned features are then pooled into a single token, which is leveraged in the subsequent sp
    
[^15]: Phoenix: 实现 ChatGPT 的跨语言民主化

    Phoenix: Democratizing ChatGPT across Languages. (arXiv:2304.10453v1 [cs.CL])

    [http://arxiv.org/abs/2304.10453](http://arxiv.org/abs/2304.10453)

    Phoenix 是一款大型语言模型，实现了跨语言民主化，不仅在英文和中文中表现良好，也在资源有限的语言中表现出色，使得 ChatGPT 在更多的国家和地区变得更加可用。

    

    本文介绍了我们努力实现 ChatGPT 跨语言民主化的工作。我们发布了一款大型语言模型“Phoenix”，在开源的英文和中文模型中实现了有竞争力的性能，并在资源有限的语言（包括拉丁和非拉丁语言）中表现出色。我们相信这项工作将有利于让 ChatGPT 在更多的国家和地区变得更加可用，特别是在由于OpenAI或当地政府的限制而无法使用ChatGPT的国家。

    This paper presents our efforts to democratize ChatGPT across language. We release a large language model "Phoenix", achieving competitive performance among open-source English and Chinese models while excelling in languages with limited resources (covering both Latin and non-Latin languages). We believe this work will be beneficial to make ChatGPT more accessible, especially in countries where people cannot use ChatGPT due to restrictions from OpenAI or local goverments. Our data, code, and models are available at https://github.com/FreedomIntelligence/LLMZoo.
    
[^16]: 人工智能聊天机器人在联邦生物信息学知识图谱数据挖掘中的潜力论文翻译

    On the Potential of Artificial Intelligence Chatbots for Data Exploration of Federated Bioinformatics Knowledge Graphs. (arXiv:2304.10427v1 [cs.AI])

    [http://arxiv.org/abs/2304.10427](http://arxiv.org/abs/2304.10427)

    本文研究了人工智能聊天机器人在联邦生物信息学知识图谱数据挖掘中的潜力，通过与领域专家对话的方式，实现了对跨数据集的查询和解释。

    

    本文展示了目前关于人工智能聊天机器人（例如ChatGPT）在联邦化知识图谱数据访问中的作用研究。特别是提供了生物信息学领域的示例，说明了对话式人工智能用于描述数据集以及生成和解释（联邦化）跨数据集查询的潜在用途，以便为领域专家提供帮助。

    In this paper, we present work in progress on the role of artificial intelligence (AI) chatbots, such as ChatGPT, in facilitating data access to federated knowledge graphs. In particular, we provide examples from the field of bioinformatics, to illustrate the potential use of Conversational AI to describe datasets, as well as generate and explain (federated) queries across datasets for the benefit of domain experts.
    
[^17]: 基于大语言模型的完全自主编程

    Fully Autonomous Programming with Large Language Models. (arXiv:2304.10423v1 [cs.SE])

    [http://arxiv.org/abs/2304.10423](http://arxiv.org/abs/2304.10423)

    本文讨论了基于大语言模型的完全自主编程方法，提出了Synthesize、Execute、Debug（SED）方法来解决程序生成的低准确度问题，并比较了不同的调试策略和提示生成方法。

    

    目前基于大语言模型（LLMs）的程序综合方法存在“几乎成功综合”的问题：它们生成的程序在语义上类似于正确答案（通过文本相似性度量或人工评估来衡量），但由于输入输出格式错误等细微差异，对于单元测试而言的准确性很低甚至为零。这需要一种称为Synthesize、Execute、Debug（SED）的方法，首先生成解决方案的草稿，然后进行程序修复阶段以解决未通过的测试。为了有效地将这种方法应用于指令驱动的LLMs，需要确定哪些提示作为LLMs的指令表现最佳，并在修复未成功的程序和用新生成的程序替换它们之间取得平衡。我们通过比较以替换为重点、以修复为重点和混合调试策略以及不同的基于模板和基于模型的提示生成方法来实证探讨这些权衡。

    Current approaches to program synthesis with Large Language Models (LLMs) exhibit a "near miss syndrome": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug (SED), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven LLMs, one needs to determine which prompts perform best as instructions for LLMs, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generati
    
[^18]: 运动接受超启发式方法如何应对局部最优解问题：跃迁与悬崖之间的巨大差异

    How the Move Acceptance Hyper-Heuristic Copes With Local Optima: Drastic Differences Between Jumps and Cliffs. (arXiv:2304.10414v1 [cs.NE])

    [http://arxiv.org/abs/2304.10414](http://arxiv.org/abs/2304.10414)

    MAHH方法通过解决跳跃函数在小的间隙大小下的表现问题，显着优于简单的精英进化算法。当处理宽悬崖问题时，MAHH仍然是解决局部最优解的高效方法。

    

    在最近的研究中，Lissovoi、Oliveto和Warwicker（《人工智能》（2023））证明了运动接受超启发式方法（MAHH）以显着的效率离开多峰悬崖基准的局部最优解。对于几乎所有悬崖宽度$d$，MAHH的$O(n^3)$运行时间大大优于简单精英进化算法（EAs）的$\Theta(n^d)$运行时间。对于最主要的多峰基准跳跃函数，对于间隙大小$m\geq 2$的运行时间估计为$O(n^{2m}m^{-\Theta(m)})$和$\Omega(2^{\Omega(m)})$，两者的差距很大，MAHH的实际表现仍然是一个未解之谜。在本文中，我们解决了这个问题。我们证明了对于任何MAHH选择参数$p$的选择，MAHH在具有间隙大小$m = o(n^{1/2})$的跳跃函数上的预期运行时间至少为$\Omega(n^{2m-1}/(2m-1)!)$。这使得MAHH比简单的精英进化算法更慢，后者通常有$O(n^m)$的运行时间。

    In recent work, Lissovoi, Oliveto, and Warwicker (Artificial Intelligence (2023)) proved that the Move Acceptance Hyper-Heuristic (MAHH) leaves the local optimum of the multimodal cliff benchmark with remarkable efficiency. With its $O(n^3)$ runtime, for almost all cliff widths $d,$ the MAHH massively outperforms the $\Theta(n^d)$ runtime of simple elitist evolutionary algorithms (EAs). For the most prominent multimodal benchmark, the jump functions, the given runtime estimates of $O(n^{2m} m^{-\Theta(m)})$ and $\Omega(2^{\Omega(m)})$, for gap size $m \ge 2$, are far apart and the real performance of MAHH is still an open question.  In this work, we resolve this question. We prove that for any choice of the MAHH selection parameter~$p$, the expected runtime of the MAHH on a jump function with gap size $m = o(n^{1/2})$ is at least $\Omega(n^{2m-1} / (2m-1)!)$. This renders the MAHH much slower than simple elitist evolutionary algorithms with their typical $O(n^m)$ runtime.  We also show
    
[^19]: 自动驾驶中的雷达相机融合：综述与对比

    Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review. (arXiv:2304.10410v1 [cs.CV])

    [http://arxiv.org/abs/2304.10410](http://arxiv.org/abs/2304.10410)

    本文为雷达相机融合提供了一份综合指南，主要关注感知任务中的目标检测和语义分割。雷达和相机通过互补的方式提供成本效益高的周围环境感知，本综述深入探讨了数据处理和表示，详细总结了雷达相机融合数据集的情况，并解决了许多方法学问题。

    

    受深度学习技术推动，自动驾驶领域的感知技术近年来得到了快速发展。为了实现精准和稳健的感知能力，自动驾驶汽车通常配备多种传感器，使得传感器融合成为感知系统的关键部分。在这些传感器中，雷达和相机通过互补的方式，在任何光照和天气条件下都能够提供成本效益高的周围环境感知。本文旨在提供一份雷达相机融合的综合指南，特别关注与目标检测和语义分割相关的感知任务。基于雷达和相机传感器的原理，我们深入探讨数据处理过程和表示，接着详细分析和总结了雷达相机融合数据集的情况。在探讨雷达相机融合的方法学时，我们回答了许多问题，包括“为什么融合”，“融合什么”，“在哪里融合”

    Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation. Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including "why to fuse", "what to fuse", "where to fus
    
[^20]: PDL在拓展上更上一层楼：具有交集和转置的PDL表达的拓展

    PDL on Steroids: on Expressive Extensions of PDL with Intersection and Converse. (arXiv:2304.10381v1 [cs.LO])

    [http://arxiv.org/abs/2304.10381](http://arxiv.org/abs/2304.10381)

    介绍了CPDL+，它是一种表达式强大的逻辑系统，扩展了PDL的交集和转置，超越了CQ、CRPQ和已知扩展的表达力。树宽为2的CPDL+公式等价于ICPDL，但增加树宽会增加表达力。

    

    我们介绍了CPDL+，这是一族表达式强大的逻辑系统，根植于命题动态逻辑（PDL）。在表达力上，CPDL+严格包含扩展了交集和转置的PDL（即ICPDL），以及连接查询（CQ）、连通正则路径查询（CRPQ）或某些已知的扩展（正则查询和CQPDL）。我们研究了CPDL+的表达力、双模拟的特征、可满足性和模型检查。我们认为，CPDL+的自然子集可以用公式的基础图的树宽来定义。我们证明了树宽为2的CPDL+公式的类等价于ICPDL，它也与树宽为1的CPDL+公式相同。然而，在树宽大于2的情况下，增加树宽严格增加了表达力。我们通过带石子的双模拟博弈来表征了每个固定树宽公式类的表达力。基于这个表征，我们给出了算法，用于判断具有最小或特定树宽的公式的可满足性和模型检查。

    We introduce CPDL+, a family of expressive logics rooted in Propositional Dynamic Logic (PDL). In terms of expressive power, CPDL+ strictly contains PDL extended with intersection and converse (a.k.a. ICPDL) as well as Conjunctive Queries (CQ), Conjunctive Regular Path Queries (CRPQ), or some known extensions thereof (Regular Queries and CQPDL). We investigate the expressive power, characterization of bisimulation, satisfiability, and model checking for CPDL+.  We argue that natural subclasses of CPDL+ can be defined in terms of the tree-width of the underlying graphs of the formulas. We show that the class of CPDL+ formulas of tree-width 2 is equivalent to ICPDL, and that it also coincides with CPDL+ formulas of tree-width 1. However, beyond tree-width 2, incrementing the tree-width strictly increases the expressive power. We characterize the expressive power for every class of fixed tree-width formulas in terms of a bisimulation game with pebbles. Based on this characterization, we s
    
[^21]: 多智能体强化学习中有条件协同行为的可解释性研究

    Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning. (arXiv:2304.10375v1 [cs.LG])

    [http://arxiv.org/abs/2304.10375](http://arxiv.org/abs/2304.10375)

    提出了一种基于条件注意力的分布式注意力演员架构，利用显著性向量重用环境的条件状态来提高条件协同行为的可解释性和代理性能。

    

    我们提出了一种无模型强化学习架构，称为基于条件注意力(DA6-X)的分布式注意力演员架构，以提供更好的条件协同行为可解释性。其基本原理涉及重用显著性向量，该向量表示环境的条件状态，例如代理的全局位置。因此，具有嵌入其策略中的DA6-X灵活性的代理通过在决策过程中考虑条件状态中的附加信息表现出优越的性能。通过在对象收集游戏中将提出的方法与传统方法进行比较，实验评估了所提出方法的有效性。通过可视化DA6-X的注意权重，我们确认代理成功地学习了情境依赖性协同行为，通过正确识别各种条件状态，提高了代理的可解释性和性能。

    We propose a model-free reinforcement learning architecture, called distributed attentional actor architecture after conditional attention (DA6-X), to provide better interpretability of conditional coordinated behaviors. The underlying principle involves reusing the saliency vector, which represents the conditional states of the environment, such as the global position of agents. Hence, agents with DA6-X flexibility built into their policy exhibit superior performance by considering the additional information in the conditional states during the decision-making process. The effectiveness of the proposed method was experimentally evaluated by comparing it with conventional methods in an objects collection game. By visualizing the attention weights from DA6-X, we confirmed that agents successfully learn situation-dependent coordinated behaviors by correctly identifying various conditional states, leading to improved interpretability of agents along with superior performance.
    
[^22]: 多智能体强化学习中的时空序贯决策制导斯塔克伯格均衡的产生

    Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning. (arXiv:2304.10351v1 [cs.MA])

    [http://arxiv.org/abs/2304.10351](http://arxiv.org/abs/2304.10351)

    该论文提出了一个基于多智能体强化学习的时空序贯决策结构，实现了斯塔克伯格均衡策略的异步行动协调，并在参数共享的同时实现了不同智能体之间的不对称训练。

    

    在多智能体强化学习(MARL)中，自利的智能体试图建立均衡并根据游戏结构实现协调。然而，现有的MARL方法大多受制于所有智能体在马可夫博弈(MG)框架中的同时行动，很少有作品考虑通过异步行动协调形成均衡策略。鉴于斯塔克伯格均衡(SE)相对纳什均衡的优势，我们构建了一个从MG导出的时空序贯决策结构，并提出了一个基于所有智能体共享的条件超网络的N级策略模型。这种方法允许不对称训练和对称执行，每个智能体响应于上级智能体决策的最优反应。智能体可以学习不同的SE策略，同时仍然保持参数共享，这导致了学习和存储成本的降低以及扩展性的提高，因为智能体数量增加时，存储和计算成本不会显著增加。

    In multi-agent reinforcement learning (MARL), self-interested agents attempt to establish equilibrium and achieve coordination depending on game structure. However, existing MARL approaches are mostly bound by the simultaneous actions of all agents in the Markov game (MG) framework, and few works consider the formation of equilibrium strategies via asynchronous action coordination. In view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium, we construct a spatio-temporal sequential decision-making structure derived from the MG and propose an N-level policy model based on a conditional hypernetwork shared by all agents. This approach allows for asymmetric training with symmetric execution, with each agent responding optimally conditioned on the decisions made by superior agents. Agents can learn heterogeneous SE policies while still maintaining parameter sharing, which leads to reduced cost for learning and storage and enhanced scalability as the number of agents in
    
[^23]: 可控的神经符号回归

    Controllable Neural Symbolic Regression. (arXiv:2304.10336v1 [cs.LG])

    [http://arxiv.org/abs/2304.10336](http://arxiv.org/abs/2304.10336)

    本文提出了一种称为带假设的神经符号回归（NSRwH）的方法，能够将用户定义的先验知识纳入到生成分析表达式的预测模型中，我们的方法允许用户指定目标表达式中使用的数学符号类型和范围，并对表达式的复杂性施加约束。

    

    在符号回归中，目标是找到一个分析表达式，能够在使用尽可能少的数学符号（例如运算符、变量和常量）的情况下精确地拟合实验数据。然而，由于可能表达式的组合空间很大，传统进化算法很难在合理的时间内找到正确的表达式。为了解决这个问题，研究人员开发了一种名为神经符号回归（NSR）算法，能够快速识别数据中的模式并生成分析表达式。然而，这些方法目前还缺乏将用户定义的先验知识纳入其中的能力，而这种能力在自然科学和工程领域通常是必需的。为了克服这种限制，我们提出了一种新的神经符号回归方法，名为带假设的神经符号回归（NSRwH），它允许显式地将关于真实表达式期望结构的假设纳入预测模型中。

    In symbolic regression, the goal is to find an analytical expression that accurately fits experimental data with the minimal use of mathematical symbols such as operators, variables, and constants. However, the combinatorial space of possible expressions can make it challenging for traditional evolutionary algorithms to find the correct expression in a reasonable amount of time. To address this issue, Neural Symbolic Regression (NSR) algorithms have been developed that can quickly identify patterns in the data and generate analytical expressions. However, these methods, in their current form, lack the capability to incorporate user-defined prior knowledge, which is often required in natural sciences and engineering fields. To overcome this limitation, we propose a novel neural symbolic regression method, named Neural Symbolic Regression with Hypothesis (NSRwH) that enables the explicit incorporation of assumptions about the expected structure of the ground-truth expression into the pre
    
[^24]: 视频动作识别中连续学习方法的基线研究

    A baseline on continual learning methods for video action recognition. (arXiv:2304.10335v1 [cs.CV])

    [http://arxiv.org/abs/2304.10335](http://arxiv.org/abs/2304.10335)

    本文基于视频动作识别场景，提出最先进的连续学习方法的基准研究，并证明回顾方法优于其他方法。此外，提出的内存效率变体可有效地保持一定水平的性能。

    

    近年来，连续学习吸引了研究界的关注，因为它旨在解决经典监督模型的长期限制。然而，大多数关于这个主题的研究都是针对简单的图像分类场景进行的。在本文中，我们提出了关于视频动作识别状态下最先进的连续学习方法的基准研究。除了由于时间维度而增加的复杂性外，在视频环境中，为了实现最佳性能的回顾方法对计算资源要求更高。为了对抗增加的内存要求，我们提出了两种去方法都通用的回顾方法变体，利用模型置信度或数据信息的指标来选择可记忆的样本。我们的实验表明，与文献中预期的一样，回顾方法优于其他方法；此外，所提出的内存效率变体被证明在保持一定水平的性能方面是有效的。

    Continual learning has recently attracted attention from the research community, as it aims to solve long-standing limitations of classic supervisedly-trained models. However, most research on this subject has tackled continual learning in simple image classification scenarios. In this paper, we present a benchmark of state-of-the-art continual learning methods on video action recognition. Besides the increased complexity due to the temporal dimension, the video setting imposes stronger requirements on computing resources for top-performing rehearsal methods. To counteract the increased memory requirements, we present two method-agnostic variants for rehearsal methods, exploiting measures of either model confidence or data information to select memorable samples. Our experiments show that, as expected from the literature, rehearsal methods outperform other approaches; moreover, the proposed memory-efficient variants are shown to be effective at retaining a certain level of performance 
    
[^25]: 向着人类和机器科学理解的基准迈进

    Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v1 [cs.AI])

    [http://arxiv.org/abs/2304.10327](http://arxiv.org/abs/2304.10327)

    该论文提出了一个框架来创建衡量人类和人工智能科学理解的基准。他们使用了行为观念，提出了一组问题以衡量不同水平的科学理解。这个框架可以帮助评估和比较不同水平和方法的科学理解。

    

    科学理解是科学的基本目标，它使我们能够解释世界。目前还没有好的方法来衡量代理人的科学理解，无论它们是人类还是人工智能系统。缺乏清晰的基准，难以评估和比较不同水平和方法的科学理解。在此路线图中，我们提出了一个框架，利用科学哲学工具创建科学理解的基准。我们采用行为观念，认为真正的理解应该被认为是执行某些任务的能力。我们通过考虑一组问题来扩展这个概念，这些问题可以衡量不同水平的科学理解，包括信息检索，安排信息以生成解释的能力以及在不同情况下推断事物会有哪些不同。Scientific Understanding Benchmark（SUB）由

    Scientific understanding is a fundamental goal of science, allowing us to explain the world. There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding. In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. The Scientific Understanding Benchmark (SUB), which is formed by 
    
[^26]: LA3: 高效的标签感知自动增强技术

    LA3: Efficient Label-Aware AutoAugment. (arXiv:2304.10310v1 [cs.CV])

    [http://arxiv.org/abs/2304.10310](http://arxiv.org/abs/2304.10310)

    LA3 提出了一种新颖的两阶段数据增强算法，利用标签信息，为不同标签的样本分别学习增强策略，提高深度神经网络训练的泛化性能。

    

    自动增强技术是一种提高深度神经网络训练泛化性能的新兴有效技术。然而，现有大部分工作侧重于构建适用于数据集中所有数据样本的统一增强策略，而忽略了样本或类别之间的差异性。本文提出了一种新颖的两阶段数据增强算法 LA3，利用标签信息，针对不同标签的样本分别学习增强策略。

    Automated augmentation is an emerging and effective technique to search for data augmentation policies to improve generalizability of deep neural network training. Most existing work focuses on constructing a unified policy applicable to all data samples in a given dataset, without considering sample or class variations. In this paper, we propose a novel two-stage data augmentation algorithm, named Label-Aware AutoAugment (LA3), which takes advantage of the label information, and learns augmentation policies separately for samples of different labels. LA3 consists of two learning stages, where in the first stage, individual augmentation methods are evaluated and ranked for each label via Bayesian Optimization aided by a neural predictor, which allows us to identify effective augmentation techniques for each label under a low search cost. And in the second stage, a composite augmentation policy is constructed out of a selection of effective as well as complementary augmentations, which 
    
[^27]: FIANCEE: 通过条件早期退出加速对抗性网络推断

    FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits. (arXiv:2304.10306v1 [cs.CV])

    [http://arxiv.org/abs/2304.10306](http://arxiv.org/abs/2304.10306)

    本论文提出了FIANCEE，通过添加早期退出分支并根据输出难度动态切换计算路径的方法来加速对抗性网络推断，经过验证该方法对于生成任务可以在保持高质量的同时减少计算量，特别适合实时应用。

    

    生成式深度神经网络是图像合成的有力工具，但它们受到计算负载的限制。另一方面，针对训练好的模型和任务（例如在特定特征范围内生成人脸），不同特征下的图像质量会存在差异，因此可以在某些实例上限制模型的复杂度，保持高质量。我们提出了一种方法，通过在原始架构中添加所谓的早期退出分支，并根据输出难度动态切换计算路径，来减少计算量。我们将该方法应用于两个不同的最先进模型，执行生成任务：从语义映射中生成和面部表情的交叉重现，表明它能够输出具有自定义低质量阈值的图像。对于LPIPS <=0.1的阈值，我们将计算量减少了一半。这对实时应用程序尤其重要，其中快速推断至关重要。

    Generative DNNs are a powerful tool for image synthesis, but they are limited by their computational load. On the other hand, given a trained model and a task, e.g. faces generation within a range of characteristics, the output image quality will be unevenly distributed among images with different characteristics. It follows, that we might restrain the models complexity on some instances, maintaining a high quality. We propose a method for diminishing computations by adding so-called early exit branches to the original architecture, and dynamically switching the computational path depending on how difficult it will be to render the output. We apply our method on two different SOTA models performing generative tasks: generation from a semantic map, and cross-reenactment of face expressions; showing it is able to output images with custom lower-quality thresholds. For a threshold of LPIPS <=0.1, we diminish their computations by up to a half. This is especially relevant for real-time app
    
[^28]: SARF: 利用同义关系辅助的自监督学习以进行少样本关系推理

    SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning. (arXiv:2304.10297v1 [cs.LG])

    [http://arxiv.org/abs/2304.10297](http://arxiv.org/abs/2304.10297)

    本文提出了一个新的自监督学习模型SARF，通过利用同义关系辅助提高了少样本关系推理的泛化性能和准确性，经过实验验证超越了目前最先进的方法。

    

    知识图谱中针对少量数据、长尾的关系推理（FS-KGR）近年来因其实用性而备受关注。之前的方法需要手动构建元关系集来预训练，导致了大量的劳动成本。自监督学习（SSL）被视为解决这个问题的方案，但在FS-KGR任务中仍处于早期阶段。此外，大多数现有方法忽略了利用与目标数据稀少关系具有类似上下文语义的同义关系（AR）的有益信息。因此，我们提出了一种使用同义关系辅助的自监督学习模型，命名为SARF。具体地，我们的模型设计了四个主要组成部分，即SSL推理模块、AR辅助机制、融合模块和评分函数。我们首先以生成式的方式生成共现模式的表示，同时设计AR辅助机制来捕捉数据稀少关系的相似上下文语义。然后，这两个表示被融合起来生成综合特征表示。最后，我们采用评分函数来预测基于此特征表示的目标关系。两个广泛使用的基准测试上的实验结果表明SARF优于最先进的方法。

    Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to infer long-tail data-poor relations, which has drawn increasing attention these years due to its practicalities. The pre-training of previous methods needs to manually construct the meta-relation set, leading to numerous labor costs. Self-supervised learning (SSL) is treated as a solution to tackle the issue, but still at an early stage for FS-KGR task. Moreover, most of the existing methods ignore leveraging the beneficial information from aliasing relations (AR), i.e., data-rich relations with similar contextual semantics to the target data-poor relation. Therefore, we proposed a novel Self-Supervised Learning model by leveraging Aliasing Relations to assist FS-KGR, termed SARF. Concretely, four main components are designed in our model, i.e., SSL reasoning module, AR-assisted mechanism, fusion module, and scoring function. We first generate the representation of the co-occurrence patterns in a generative manner. Meanwh
    
[^29]: 一种估算并解释分类器不确定性的元启发式方法

    A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty. (arXiv:2304.10284v1 [cs.LG])

    [http://arxiv.org/abs/2304.10284](http://arxiv.org/abs/2304.10284)

    本文提出了一种元启发式方法，它可以以人类和机器学习决策都互相关联的因素来表征一个实例的复杂性，以估计分类错误的风险。

    

    信任是影响机器学习模型采用的重要因素。定性研究表明，终端用户，特别是在医疗领域，需要能够在决策时表达不确定性的模型，以使用户知道何时忽略模型的建议。然而，现有的量化决策不确定性的方法不是模型无关的，就是依赖于不容易让普通人或终端用户理解的复杂统计推导，这使它们在解释模型的决策过程时不太有用。本文提出了一组类独立的元启发式方法，可以以人类和机器学习决策都互相关联的因素来表征一个实例的复杂性。这些度量被集成到一个元学习框架中，该框架估计了分类错误风险。所提出的框架在鉴别那些有可能被错误分类的实例方面，表现优于预测概率。

    Trust is a crucial factor affecting the adoption of machine learning (ML) models. Qualitative studies have revealed that end-users, particularly in the medical domain, need models that can express their uncertainty in decision-making allowing users to know when to ignore the model's recommendations. However, existing approaches for quantifying decision-making uncertainty are not model-agnostic, or they rely on complex statistical derivations that are not easily understood by laypersons or end-users, making them less useful for explaining the model's decision-making process. This work proposes a set of class-independent meta-heuristics that can characterize the complexity of an instance in terms of factors are mutually relevant to both human and ML decision-making. The measures are integrated into a meta-learning framework that estimates the risk of misclassification. The proposed framework outperformed predicted probabilities in identifying instances at risk of being misclassified. The
    
[^30]: 无线元宇宙的七个世界和体验：挑战和机遇

    The Seven Worlds and Experiences of the Wireless Metaverse: Challenges and Opportunities. (arXiv:2304.10282v1 [cs.IT])

    [http://arxiv.org/abs/2304.10282](http://arxiv.org/abs/2304.10282)

    本文提出了一个无限的、无线的元宇宙视野，将元宇宙浓缩为七个世界和体验的交叉点，支持人类和虚拟化角色之间以及连接的智能系统和其数字孪生之间的新颖互动。

    

    无线元宇宙将会在实体、数字和虚拟世界的交叉点创造多样化的用户体验。这些体验将会促进这三个世界的构成要素（如扩展现实（XR）用户和虚拟化角色）之间的新颖互动。然而，令人瞩目的是，迄今为止没有一个全面的愿景能够确定元宇宙的全部世界、构成要素和体验的集合，以及它们相关互动对未来通信和计算系统的影响。在本文中，我们提出了一个无尽的、无线的、综合的元宇宙视野，将元宇宙浓缩为七个世界和体验的交叉点，包括：i）实体、数字和虚拟世界，以及ii）网络、扩展、实时和平行的体验。然后，我们阐述了这些经验如何呈现不同的元宇宙构成要素之间的互动，即a）人类和虚拟化角色，和b）连接的智能系统及其数字孪生。

    The wireless metaverse will create diverse user experiences at the intersection of the physical, digital, and virtual worlds. These experiences will enable novel interactions between the constituents (e.g., extended reality (XR) users and avatars) of the three worlds. However, remarkably, to date, there is no holistic vision that identifies the full set of metaverse worlds, constituents, and experiences, and the implications of their associated interactions on next-generation communication and computing systems. In this paper, we present a holistic vision of a limitless, wireless metaverse that distills the metaverse into an intersection of seven worlds and experiences that include the: i) physical, digital, and virtual worlds, along with the ii) cyber, extended, live, and parallel experiences. We then articulate how these experiences bring forth interactions between diverse metaverse constituents, namely, a) humans and avatars and b) connected intelligence systems and their digital tw
    
[^31]: 通过领域自适应模仿学习动态系统的代表性轨迹

    Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation. (arXiv:2304.10260v1 [cs.LG])

    [http://arxiv.org/abs/2304.10260](http://arxiv.org/abs/2304.10260)

    该论文提出了一种使用循环一致性生成对抗方法进行领域自适应轨迹模仿的深度强化学习代理DATI，可以学习动态系统中的代表性轨迹，识别交通中异常运动模式，并在各种合成的参考轨迹家族上表现优异。

    

    领域适应的轨迹模仿是一种某些捕食动物为生存学习的技能，通过将动态信息从一个域（它们的速度和转向方向）映射到不同的域（移动猎物的当前位置）。具有此技能的智能代理可以用于各种任务，包括学习模仿代表性轨迹后识别交通中的异常运动。为此，我们提出DATI，一种使用循环一致性生成对抗方法进行领域自适应轨迹模仿的深度强化学习代理。我们在各种合成的参考轨迹家族上的实验表明，DATI在模仿学习和最优控制中的基准方法在这个设置中表现更好，保持相同的每一项任务的超参数。通过发现海上交通的异常运动模式，它在真实世界场景中的泛化能力已得到证明，为深度学习在领域适应轨迹模仿中的应用开辟了道路。

    Domain-adaptive trajectory imitation is a skill that some predators learn for survival, by mapping dynamic information from one domain (their speed and steering direction) to a different domain (current position of the moving prey). An intelligent agent with this skill could be exploited for a diversity of tasks, including the recognition of abnormal motion in traffic once it has learned to imitate representative trajectories. Towards this direction, we propose DATI, a deep reinforcement learning agent designed for domain-adaptive trajectory imitation using a cycle-consistent generative adversarial method. Our experiments on a variety of synthetic families of reference trajectories show that DATI outperforms baseline methods for imitation learning and optimal control in this setting, keeping the same per-task hyperparameters. Its generalization to a real-world scenario is shown through the discovery of abnormal motion patterns in maritime traffic, opening the door for the use of deep r
    
[^32]: 多视图视觉提示融合网络：2D预训练模型能否增强3D点云数据稀缺学习？

    Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?. (arXiv:2304.10224v1 [cs.CV])

    [http://arxiv.org/abs/2304.10224](http://arxiv.org/abs/2304.10224)

    本文提出了一种针对3D点云分类的少样本学习网络MvNet，它能够利用现有的2D预训练模型来缓解现有基线模型对大规模注释3D点云数据的过度依赖问题。

    

    基于点云的3D深度模型在许多领域中具有广泛的应用，例如自动驾驶、家庭机器人等。本文提出了一种新颖的多视图视觉提示融合网络（MvNet），用于少样本3D点云分类，灵感源自于最近在自然语言处理中的提示性学习。MvNet 探讨了利用现有2D预训练模型实现少样本分类的可能性，这可以缓解现有基线模型对大规模注释3D点云数据的过度依赖问题。具体而言，MvNet首先将3D点云编码成多视图图像特征，然后开发了一种新的多视图提示融合模块，以有效地融合来自不同视角的信息，以弥合3D点云数据和2D预训练模型之间的差距。然后可以派生一组2D图像提示以更好地描述适当的先验知识以进行大规模预训练。

    Point cloud based 3D deep model has wide applications in many applications such as autonomous driving, house robot, and so on. Inspired by the recent prompt learning in natural language processing, this work proposes a novel Multi-view Vision-Prompt Fusion Network (MvNet) for few-shot 3D point cloud classification. MvNet investigates the possibility of leveraging the off-the-shelf 2D pre-trained models to achieve the few-shot classification, which can alleviate the over-dependence issue of the existing baseline models towards the large-scale annotated 3D point cloud data. Specifically, MvNet first encodes a 3D point cloud into multi-view image features for a number of different views. Then, a novel multi-view prompt fusion module is developed to effectively fuse information from different views to bridge the gap between 3D point cloud data and 2D pre-trained models. A set of 2D image prompts can then be derived to better describe the suitable prior knowledge for a large-scale pre-train
    
[^33]: 基于MC-dropout的脉冲神经网络高效不确定性估计

    Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout. (arXiv:2304.10191v1 [cs.NE])

    [http://arxiv.org/abs/2304.10191](http://arxiv.org/abs/2304.10191)

    本文提出了一种利用时间步机制实现MC-dropout的高效脉冲神经网络不确定性估计方法，为神经形态学硬件的节能应用提供了新的研究思路。

    

    脉冲神经网络(SNN)作为生物神经元稀疏和事件驱动通信模型的模拟器已引起关注，并因此在神经形态学硬件中显示出越来越多的节能应用前景。与经典人工神经网络(ANNs)一样，预测不确定性对于高风险应用（如自动驾驶车辆、医学诊断和高频交易）的决策非常重要。然而，关于SNNs中的不确定性估计的讨论很少，而人工神经网络(ANNs)中的不确定性估计方法不能直接应用于SNNs。在这里，我们提出了一种基于MC-dropout的脉冲神经网络不确定性估计的高效方法。我们的方法利用SNNs的时间步机制，以一种计算高效的方式实现MC-dropout，不会在训练和推断过程中引入重大开销，同时展示了高准确率和不确定性质量。

    Spiking neural networks (SNNs) have gained attention as models of sparse and event-driven communication of biological neurons, and as such have shown increasing promise for energy-efficient applications in neuromorphic hardware. As with classical artificial neural networks (ANNs), predictive uncertainties are important for decision making in high-stakes applications, such as autonomous vehicles, medical diagnosis, and high frequency trading. Yet, discussion of uncertainty estimation in SNNs is limited, and approaches for uncertainty estimation in artificial neural networks (ANNs) are not directly applicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout based approach for uncertainty estimation in SNNs. Our approach exploits the time-step mechanism of SNNs to enable MC-dropout in a computationally efficient manner, without introducing significant overheads during training and inference while demonstrating high accuracy and uncertainty quality.
    
[^34]: 利用文本到图像生成技术进行建筑设计构思

    Using Text-to-Image Generation for Architectural Design Ideation. (arXiv:2304.10182v1 [cs.HC])

    [http://arxiv.org/abs/2304.10182](http://arxiv.org/abs/2304.10182)

    本研究首次探索了文本到图像生成技术在建筑设计早期阶段支持创造力的潜力。实验结果表明，生成器支持意外的创意发现和富有想象力的心态，丰富了设计过程。

    

    最近文本到图像生成技术在建筑设计中得到了认可。我们的研究首次探讨了文本到图像生成器在建筑设计早期阶段支持创造力的潜力。我们与17名建筑学生进行了实验室研究，使用了三种流行的文本到图像生成器——Midjourney、稳定扩散和DALL-E，设计了一个文化中心的概念。通过标准问卷和小组访谈，我们发现当仔细考虑设计约束时，图像生成可以成为设计过程的重要组成部分。生成工具支持意外的创意发现和富有想象力的心态，丰富了设计过程。我们确定了图像生成器面临的一些挑战，并提出了软件开发和教育工作者的考虑，以支持创造力并强调设计师的想象力。通过了解文本到图像生成技术的限制和潜力，建筑师可以更好地利用这项技术来促进创造力。

    The recent progress of text-to-image generation has been recognized in architectural design. Our study is the first to investigate the potential of text-to-image generators in supporting creativity during the early stages of the architectural design process. We conducted a laboratory study with 17 architecture students, who developed a concept for a culture center using three popular text-to-image generators: Midjourney, Stable Diffusion, and DALL-E. Through standardized questionnaires and group interviews, we found that image generation could be a meaningful part of the design process when design constraints are carefully considered. Generative tools support serendipitous discovery of ideas and an imaginative mindset, enriching the design process. We identified several challenges of image generators and provided considerations for software development and educators to support creativity and emphasize designers' imaginative mindset. By understanding the limitations and potential of tex
    
[^35]: 通过权重锚定实现鲁棒深度强化学习调度

    Robust Deep Reinforcement Learning Scheduling via Weight Anchoring. (arXiv:2304.10176v1 [cs.LG])

    [http://arxiv.org/abs/2304.10176](http://arxiv.org/abs/2304.10176)

    本研究提出了一种使用权重锚定来实现深度强化学习调度的方法，可避免在优化环境下忽略或忘记期望行为，提高了鲁棒性和可操纵性。

    

    当学习方法从仿真到现实中跨越鸿沟时，数据驱动学习方法的鲁棒性仍存在问题。本文提出了一种名为权重锚定的方法，该方法在持续学习中已知，并用于培养和固定神经网络中期望的行为。可以使用权重锚定方法找到一个与另一个学习问题的解接近的学习问题的解。这样，在优化环境下进行学习时，不会忽略或忘记期望的行为。我们通过学习混合的QoS高效离散资源调度与不频繁的优先消息的示例来演示此方法。结果表明，该方法提供了与增加仿真环境的现有技术相当的性能，同时显著提高了鲁棒性和可操纵性。

    Questions remain on the robustness of data-driven learning methods when crossing the gap from simulation to reality. We utilize weight anchoring, a method known from continual learning, to cultivate and fixate desired behavior in Neural Networks. Weight anchoring may be used to find a solution to a learning problem that is nearby the solution of another learning problem. Thereby, learning can be carried out in optimal environments without neglecting or unlearning desired behavior. We demonstrate this approach on the example of learning mixed QoS-efficient discrete resource scheduling with infrequent priority messages. Results show that this method provides performance comparable to the state of the art of augmenting a simulation environment, alongside significantly increased robustness and steerability.
    
[^36]: 基于混合量子神经网络的深度强化学习

    Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])

    [http://arxiv.org/abs/2304.10159](http://arxiv.org/abs/2304.10159)

    该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。

    

    量子计算对于促进当前机器学习算法处理更高数据维度或减少深度神经网络模型的总体训练参数的限制具有强烈的影响。本研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并采用深度 Q-Learning 方法。该研究评估了其潜力。因此，设计并培训了一个基于最新的 Qiskit 和 PyTorch 框架的新型 PQC，以与完全经典的深度神经网络进行比较，带或不带集成 PQC。研究最后总结了其关于开发深度量子学习解决迷宫问题或其他强化学习问题的前景和结论。

    Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
    
[^37]: ChatGPT能否复制人类生成的标签？对社交计算任务的研究

    Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])

    [http://arxiv.org/abs/2304.10145](http://arxiv.org/abs/2304.10145)

    本文研究了ChatGPT在社交计算任务中是否可以复制人类生成的标签注释，结果表明ChatGPT有潜力处理这些数据注释任务，尽管仍存在许多挑战。

    

    ChatGPT的发布揭示了语言模型可以取代人类智慧的各种可能性。本文旨在了解ChatGPT是否有潜力在社交计算任务中复制人类生成的标签注释。这样的成就可以显著降低社交计算研究的成本和复杂性。因此，我们使用ChatGPT重新标记了五个具有里程碑意义的数据集，涉及立场检测（2个）、情感分析、仇恨言论和机器人检测。我们的结果表明，ChatGPT有潜力处理这些数据注释任务，尽管仍存在许多挑战。ChatGPT获得了平均精度0.609。 ChatGPT对情感分析数据集的表现最佳，正确注释了64.9％的推文。然而，我们显示性能在不同标签之间有很大差异。我们认为这项工作可以开辟新的分析线路，并作为未来利用ChatGPT进行数据注释的基础。

    The release of ChatGPT has uncovered a range of possibilities whereby large language models (LLMs) can substitute human intelligence. In this paper, we seek to understand whether ChatGPT has the potential to reproduce human-generated label annotations in social computing tasks. Such an achievement could significantly reduce the cost and complexity of social computing research. As such, we use ChatGPT to re-label five seminal datasets covering stance detection (2x), sentiment analysis, hate speech, and bot detection. Our results highlight that ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain. ChatGPT obtains an average precision 0.609. Performance is highest for the sentiment analysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we show that performance varies substantially across individual labels. We believe this work can open up new lines of analysis and act as a basis for future research into the exploi
    
[^38]: 解耦图神经网络：同时训练多个简单的GNN，而不是一个。

    Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One. (arXiv:2304.10126v1 [cs.LG])

    [http://arxiv.org/abs/2304.10126](http://arxiv.org/abs/2304.10126)

    本论文提出将多层图神经网络解耦为多个简单模块的方法，以实现更高效的训练。该方法包括经典的前向训练和设计的反向训练。每个模块都可以通过随机算法在前向训练中高效地训练，并且通过反向训练机制来使前面的模块能够感知后面的模块，从而充分训练浅层模块和更深层的模块。

    

    图神经网络（GNN）存在严重的效率问题，主要是由于节点依赖随着层数增加呈指数级增长。这极大地限制了随机优化算法的应用，使得GNN的训练通常很耗时。为了解决这个问题，我们提出了将多层GNN解耦为多个简单模块的方法，以实现更高效的训练。该方法由经典的前向训练（FT）和设计的反向训练（BT）组成。在所提出的框架下，每个模块都可以通过随机算法在FT中高效地训练，由于其简单性，不会扭曲图形信息。为避免FT的只单向信息传递，并充分训练浅层模块和更深层的模块，我们开发了一种反向训练机制，使前面的模块能够感知后面的模块。这种反向训练引入了反向信息传递到解耦模块中，同时也会有前向信息传递。

    Graph neural networks (GNN) suffer from severe inefficiency. It is mainly caused by the exponential growth of node dependency with the increase of layers. It extremely limits the application of stochastic optimization algorithms so that the training of GNN is usually time-consuming. To address this problem, we propose to decouple a multi-layer GNN as multiple simple modules for more efficient training, which is comprised of classical forward training (FT)and designed backward training (BT). Under the proposed framework, each module can be trained efficiently in FT by stochastic algorithms without distortion of graph information owing to its simplicity. To avoid the only unidirectional information delivery of FT and sufficiently train shallow modules with the deeper ones, we develop a backward training mechanism that makes the former modules perceive the latter modules. The backward training introduces the reversed information delivery into the decoupled modules as well as the forward i
    
[^39]: 用多智能体非对称进化强化学习掌握非对称多人游戏

    Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning. (arXiv:2304.10124v1 [cs.AI])

    [http://arxiv.org/abs/2304.10124](http://arxiv.org/abs/2304.10124)

    该论文提出了一种非对称进化训练（AET）框架，使用自适应数据调整（ADA）和环境随机化（ER）优化AET过程，使得AI可以在复杂的AMP游戏中击败顶级人类玩家，而不需要使用任何人类数据。

    

    非对称多人游戏（AMP）是一种流行的游戏类型，涉及多种类型的代理在游戏中相互竞争或合作。由于非对称环境中有不平衡的特性，因此使用典型的自我博弈训练方法训练强大的代理以击败顶级人类玩家在AMP游戏中是困难的。我们提出了非对称进化训练（AET），这是一种新颖的多智能体强化学习框架，可以同时训练多种代理在AMP游戏中。我们设计了自适应数据调整（ADA）和环境随机化（ER）来优化AET过程。我们在一个名为Tom＆Jerry的复杂AMP游戏中测试了我们的方法，我们训练出的AI对65个比赛取得了98.5％的胜率，而没有使用任何人类数据。消融实验表明，所提出的模块有益于该框架。

    Asymmetrical multiplayer (AMP) game is a popular game genre which involves multiple types of agents competing or collaborating with each other in the game. It is difficult to train powerful agents that can defeat top human players in AMP games by typical self-play training method because of unbalancing characteristics in their asymmetrical environments. We propose asymmetric-evolution training (AET), a novel multi-agent reinforcement learning framework that can train multiple kinds of agents simultaneously in AMP game. We designed adaptive data adjustment (ADA) and environment randomization (ER) to optimize the AET process. We tested our method in a complex AMP game named Tom \& Jerry, and our AIs trained without using any human data can achieve a win rate of 98.5% against top human players over 65 matches. The ablation experiments indicated that the proposed modules are beneficial to the framework.
    
[^40]: 一个由经典随机游走和量子游走驱动的赌博算法

    Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk. (arXiv:2304.10118v1 [quant-ph])

    [http://arxiv.org/abs/2304.10118](http://arxiv.org/abs/2304.10118)

    本文提出了一个基于量子游走的算法解决多臂赌博问题，将探索和利用与量子游走行为联系起来，相比于经典随机游走策略实现了高性能。

    

    量子游走具有经典随机游走所不具备的属性——线性传播和局部化共存——并且这种属性被用于实现各种应用。本文提出了一种基于量子游走的多臂赌博问题算法，将使赌博问题困难的两种操作——探索和利用——与这两种量子游走行为联系起来。我们展示了这种基于量子游走的新策略相比于相应的随机游走策略实现了高性能。

    Quantum walks (QWs) have the property that classical random walks (RWs) do not possess -- coexistence of linear spreading and localization -- and this property is utilized to implement various kinds of applications. This paper proposes a quantum-walk-based algorithm for multi-armed-bandit (MAB) problems by associating the two operations that make MAB problems difficult -exploration and exploitation -- with these two behaviors of QWs. We show that this new policy based on the QWs realizes high performance compared with the corresponding RW-based one.
    
[^41]: 双记忆强化学习

    Two-Memory Reinforcement Learning. (arXiv:2304.10098v1 [cs.LG])

    [http://arxiv.org/abs/2304.10098](http://arxiv.org/abs/2304.10098)

    本文提出了双记忆强化学习代理 (2M)，它结合了情节记忆和强化学习的优点来提高学习速度和准确性。

    

    虽然深度强化学习取得了重要的经验性成功，但由于奖励信息传播和参数神经网络更新的速度较慢，它倾向于学习得比较慢。另一方面，非参数化的情节记忆提供了相对较快的学习替代方案，它不需要表示学习，并使用最大情节回报作为状态-动作值进行行动选择。情节记忆和强化学习都有各自的优点和缺点。值得注意的是，人类可以同时利用多个记忆系统进行学习，并从中获益。在这项工作中，我们提出了一种称为双记忆强化学习代理（2M）的方法，它结合了情节记忆和强化学习的优点。 2M 代理利用情节记忆部分的速度和强化学习部分的最优性和广泛适用性相互补充。我们的实验表明，

    While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that 
    
[^42]: ID-MixGCL: 基于身份混合的图形对比学习

    ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v1 [cs.LG])

    [http://arxiv.org/abs/2304.10045](http://arxiv.org/abs/2304.10045)

    本文提出了一种基于身份混合的图形对比学习方法，旨在解决通过图形增强得到的不同但相似的图形结构和标签的不匹配问题，以实现更好的表示捕获。

    

    最近发展的图形对比学习（GCL）方法是比较同一个图形的两个不同的“视图”以学习节点/图形表示。这些方法的核心假设是通过图形增强，可以生成几个结构不同但语义相似的图形结构，因此原始和增强的图形/节点的身份标签应该是相同的。然而，在本文中，我们发现这个假设并不总是成立，例如分子图中对节点或边的任何扰动都会在一定程度上改变图形标签。因此，我们认为增强图形结构应该伴随着对对比损失使用的标签的适应。基于这个想法，我们提出了 ID-MixGCL，它允许同时调节输入图形和相应的身份标签，具有可控的改变程度，从而捕获细粒度的表示。

    Recently developed graph contrastive learning (GCL) approaches compare two different "views" of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations 
    
[^43]: 面向具有时间目标的连续系统的拓扑指导的 Actor-Critic 模块化学习

    Topological Guided Actor-Critic Modular Learning of Continuous Systems with Temporal Objectives. (arXiv:2304.10041v1 [cs.AI])

    [http://arxiv.org/abs/2304.10041](http://arxiv.org/abs/2304.10041)

    本文介绍了一种基于拓扑指导的 Actor-Critic 模块化学习方法，用于解决连续系统中的最优策略综合问题，并提出了一种广义的最优备份顺序来加速学习过程。

    

    本文研究了在线性时间逻辑中给定高级规范的连续状态随机动态系统的正式策略综合。为了学习最大化满足概率的最优策略，我们对动态系统和翻译出的自动机进行了乘积构造，从而构建了一个乘积系统，然后在其中解决了最优计划问题。由于乘积系统具有混合乘积状态空间，导致奖励稀疏，因此我们引入了一种广义的最优备份顺序，与拓扑顺序相反，以指导值备份并加速学习过程。我们提供了使用广义最优备份顺序进行最优计划问题的最优性证明。此外，本文提出了一种基于拓扑顺序的演员-评论家强化学习算法。该算法利用先进的数学技术，并具有超参数自调整的特性。我们提供了该算法的最优性和收敛性证明。

    This work investigates the formal policy synthesis of continuous-state stochastic dynamic systems given high-level specifications in linear temporal logic. To learn an optimal policy that maximizes the satisfaction probability, we take a product between a dynamic system and the translated automaton to construct a product system on which we solve an optimal planning problem. Since this product system has a hybrid product state space that results in reward sparsity, we introduce a generalized optimal backup order, in reverse to the topological order, to guide the value backups and accelerate the learning process. We provide the optimality proof for using the generalized optimal backup order in this optimal planning problem. Further, this paper presents an actor-critic reinforcement learning algorithm when topological order applies. This algorithm leverages advanced mathematical techniques and enjoys the property of hyperparameter self-tuning. We provide proof of the optimality and conver
    
[^44]: 开放世界持续学习：统一新颖性检测与持续学习

    Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])

    [http://arxiv.org/abs/2304.10038](http://arxiv.org/abs/2304.10038)

    本文从理论上证明，分布外检测对于类别增量学习是必要的，因为类别增量学习可以分解成任务内预测和任务 ID 预测，并且任务 ID 预测与分布外检测相关。

    

    随着 AI agent 在未知或新奇的真实开放世界中的使用增加，它们需要具备 (1) 认识已经学习过的物体和检测到之前未见或学习的物体的能力，以及 (2) 增量地学习新物品，逐渐变得更有知识和更强大。 (1) 称为新颖性检测或分布外 (OOD) 检测，而 (2) 称为类别增量学习 (CIL)，是持续学习 (CL) 的一种设置。在现有的研究中，OOD 检测和 CIL 被视为两个完全不同的问题。本文从理论上证明了 OOD 检测实际上对于 CIL 是必要的。我们首先展示 CIL 可以分解为两个子问题：任务内预测 (WP) 和任务 ID 预测(TP)。然后我们证明了 TP 与 OOD 检测相关。关键的理论结果是，无论 WP 和 OOD 检测（或 TP）是否由 CIL 算法显式或隐式地定义，好的 WP 和良好的 OOD 检测或 TP 总是存在嵌入在任何 CIL 算法中的。

    As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
    
[^45]: 云、边缘和终端设备上的深度神经网络分区综述

    A Survey on Deep Neural Network Partition over Cloud, Edge and End Devices. (arXiv:2304.10020v1 [cs.DC])

    [http://arxiv.org/abs/2304.10020](http://arxiv.org/abs/2304.10020)

    本文给出了对云、边缘和端设备上DNN分区方法的全面综述，并提出了一个五维分类框架。这项研究将有助于解决当计算资源有限且数据远程传输代价高昂时提高DNN推理性能的问题。

    

    深度神经网络（DNN）分区是一个研究问题，涉及将DNN拆分为多个部分并将它们卸载到特定位置。由于多接入边缘计算和边缘智能的最新进展，当边缘和终端设备的计算资源有限且从这些设备到云的远程传输数据代价高昂时，DNN分区被认为是提高DNN推理性能的有力工具。本文通过对相关文献的详细文献收集，提供了对云、边缘和端设备上DNN分区方法的最新进展和挑战的全面综述。我们回顾了DNN分区在各种应用场景中的工作方式，并提供了DNN分区问题的统一数学模型。我们开发了一个DNN分区方法的五维分类框架，包括部署位置、分区粒度、分区约束、优化目标和通信协议。最后，我们讨论了DNN分区和其与云和边缘计算的集成面临的挑战和未来研究方向。

    Deep neural network (DNN) partition is a research problem that involves splitting a DNN into multiple parts and offloading them to specific locations. Because of the recent advancement in multi-access edge computing and edge intelligence, DNN partition has been considered as a powerful tool for improving DNN inference performance when the computing resources of edge and end devices are limited and the remote transmission of data from these devices to clouds is costly. This paper provides a comprehensive survey on the recent advances and challenges in DNN partition approaches over the cloud, edge, and end devices based on a detailed literature collection. We review how DNN partition works in various application scenarios, and provide a unified mathematical model of the DNN partition problem. We developed a five-dimensional classification framework for DNN partition approaches, consisting of deployment locations, partition granularity, partition constraints, optimization objectives, and 
    
[^46]: 可分性、上下文性及量子框架问题

    Separability, Contextuality, and the Quantum Frame Problem. (arXiv:2304.10010v1 [quant-ph])

    [http://arxiv.org/abs/2304.10010](http://arxiv.org/abs/2304.10010)

    本文研究了可分离性假设和上下文性的关系以及它们与框架问题的关系，并提出了后者的量子模拟和不可判定性证明，同时表明上下文性在状态准备和测量中如何被引导，并指出细微的调整假设在非上下文环境中普遍存在。

    

    我们研究了状态可分离性假设与准备和测量上下文性之间的关系，以及它们与框架问题——预测由于动作不会改变的内容的问题之间的关系。我们提出了后者的量子模拟，并证明了它的不可判定性。我们展示了状态准备和测量中上下文性是如何通过基选择、热力学交换和先验因果模型的强制实施普遍引导的，并且如何调整假设在被描述为无环境上下文的情况下出现普遍性。

    We study the relationship between assumptions of state separability and both preparation and measurement contextuality, and the relationship of both of these to the frame problem, the problem of predicting what does not change in consequence of an action. We state a quantum analog of the latter and prove its undecidability. We show how contextuality is generically induced in state preparation and measurement by basis choice, thermodynamic exchange, and the imposition of a priori causal models, and how fine-tuning assumptions appear ubiquitously in settings characterized as non-contextual.
    
[^47]: 速通与机器学习中的幂律趋势

    Power Law Trends in Speedrunning and Machine Learning. (arXiv:2304.10004v1 [cs.LG])

    [http://arxiv.org/abs/2304.10004](http://arxiv.org/abs/2304.10004)

    该论文发现了速通世界纪录改进的幂律模式，并利用这一发现提高了预测速通世界纪录精度的方法，并在机器学习基准上得到了类似的结果。结果表明，ML基准远未饱和，而机器学习中的改进具有突然性。

    

    我们发现，在速通世界纪录的改进中存在幂律模式。利用这一观察结果，我们回答了之前研究中的一个未解决问题：如何在预测某个时间跨度（如一个月）内的速通世界纪录时，提高基线预测不改进的精度？通过使用随机效应模型，在预测样本外的世界纪录改进的相对均方误差上，我们在$p<10^{-5}$的显著性水平上提高了基线预测的精度。尽管使用的数据点远少于先前表现最佳的指数移动平均预测模型，但相同的设置在$p=0.15$的显著性水平上提高了预测的准确率。我们将这种方法应用于机器学习基准并取得了超过基线的预测效果。最后，通过解释所得到的模型，我们认为1）ML基准远未饱和，2）机器学习中的突然大幅改进

    We find that improvements in speedrunning world records follow a power law pattern. Using this observation, we answer an outstanding question from previous work: How do we improve on the baseline of predicting no improvement when forecasting speedrunning world records out to some time horizon, such as one month? Using a random effects model, we improve on this baseline for relative mean square error made on predicting out-of-sample world record improvements as the comparison metric at a $p < 10^{-5}$ significance level. The same set-up improves \textit{even} on the ex-post best exponential moving average forecasts at a $p = 0.15$ significance level while having access to substantially fewer data points. We demonstrate the effectiveness of this approach by applying it to Machine Learning benchmarks and achieving forecasts that exceed a baseline. Finally, we interpret the resulting model to suggest that 1) ML benchmarks are far from saturation and 2) sudden large improvements in Machine 
    
[^48]: 基于LSTM-DeepLabv3+和时空特征融合的贝叶斯优化城市洪水预测模型改进

    Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG])

    [http://arxiv.org/abs/2304.09994](http://arxiv.org/abs/2304.09994)

    本研究提出了一种基于CNN-RNN混合特征融合建模的贝叶斯优化城市洪水预测模型，实现了静态和动态的预测，并通过结合多个CNN和RNN模型，在精度上取得了显著提高。

    

    深度学习模型因其相对传统方法更高的准确性和效率，逐渐成为流行的洪水预测方法。但是，当前的机器学习方法通常依赖于单独的空间或时间特征分析，并对输入数据的类型、数量和维度存在限制。本研究提出了一个基于CNN-RNN的混合特征融合建模方法，用于城市洪水预测，将CNN在处理空间特征方面的优势和RNN在分析不同维度的时间序列方面的优势整合起来。这种方法允许进行静态和动态的洪水预测。应用贝叶斯优化来确定七个最具影响力的洪水驱动因素，并确定最佳组合策略。通过结合四个CNN（FCN，UNet，SegNet，DeepLabv3+）和三个RNN（LSTM，BiLSTM，GRU），最优混合模型被确定为LSTM-DeepLabv3+。该模型实现了最高的预测准确性（MAE、RMSE、NSE和KGE率）。

    Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE wer
    
[^49]: 支持人工智能协作审计LLM的LLM

    Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])

    [http://arxiv.org/abs/2304.09991](http://arxiv.org/abs/2304.09991)

    本论文通过对安全和公正人工智能专家的采访以及对人工智能协作和感知文献的研究，增强了“AdaTest”审计工具，这个工具可以通过利用人和生成模型的协同优势，进行更严格的大型语言模型审计。

    

    大型语言模型通过部署在社会技术系统中变得越来越普遍和普及。然而，这些语言模型，无论是用于分类还是生成，都表现出有偏差和不负责任的行为，对人类造成了规模性的伤害。因此，对这些语言模型进行严格审计至关重要。现有的审计工具利用人和或AI来发现失败。在这项工作中，我们借鉴了人工智能协作和感知的文献，并采访了安全和公正人工智能的研究专家，以增强审计工具“AdaTest”（Ribeiro和Lundberg，2022），该工具由生成大型语言模型（LLM）驱动。通过设计过程，我们强调了感知和人工智能通信在协作审计中利用人与生成模型的互补优势的重要性。为了评估增强工具AdaTest ++的有效性，我们进行了用户研究，使参与者进行审计

    Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
    
[^50]: 超越Transformer的函数学习

    Beyond Transformers for Function Learning. (arXiv:2304.09979v1 [cs.LG])

    [http://arxiv.org/abs/2304.09979](http://arxiv.org/abs/2304.09979)

    本文通过给Transformer模型增加两个简单的归纳学习偏见探究了学习和预测简单函数的能力，并在大型神经网络模型的背景下发现这些偏见的帮助，同时指出这些偏见可能有助于人类在外推能力方面的归纳学习。

    

    学习和预测简单函数的能力是人类智能的关键方面。最近的一些研究开始使用Transformer架构来探索这种能力，然而仍不清楚这是否足以重现人们在这个领域的外推能力。在这里，我们提出通过增加两个简单的归纳学习偏见来弥补这一差距，这些偏见直接来自于认知科学中最近的抽象推理模型。我们报道的结果表明，在大型神经网络模型的背景下，这些偏见是有帮助的，同时也揭示了可能有助于人类外推能力的归纳学习偏见的类型。

    The ability to learn and predict simple functions is a key aspect of human intelligence. Recent works have started to explore this ability using transformer architectures, however it remains unclear whether this is sufficient to recapitulate the extrapolation abilities of people in this domain. Here, we propose to address this gap by augmenting the transformer architecture with two simple inductive learning biases, that are directly adapted from recent models of abstract reasoning in cognitive science. The results we report demonstrate that these biases are helpful in the context of large neural network models, as well as shed light on the types of inductive learning biases that may contribute to human abilities in extrapolation.
    
[^51]: 无监督图神经网络解决肾移植问题

    Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision. (arXiv:2304.09975v1 [cs.LG])

    [http://arxiv.org/abs/2304.09975](http://arxiv.org/abs/2304.09975)

    本文提出了一个无监督的图神经网络解决肾移植问题，可以近似解决NP难问题，并且比其他方法更好。

    

    本文引入了一种新的基于学习的方法，近似解决了图上的肾移植问题（KEP），这是一个NP难问题。该问题涉及到从一组肾脏供体和等待肾脏捐赠的患者中，选择一组捐赠以最大化移植数量和质量，同时遵守一些有关这些捐赠安排的约束条件。该技术包括两个主要步骤：第一步是无监督训练的图神经网络（GNN）；第二步是确定性的非学习搜索启发式，它利用GNN的输出来查找路径和循环。为了进行比较，我们还实现并测试了一种精确的解决方案方法，使用整数规划、两个贪心搜索启发式，不使用机器学习模块的GNN。我们分析和比较了这些方法，并得出结论，即基于学习的两阶段方法是最佳解决方案。

    This paper introduces a new learning-based approach for approximately solving the Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problem consists of, given a pool of kidney donors and patients waiting for kidney donations, optimally selecting a set of donations to optimize the quantity and quality of transplants performed while respecting a set of constraints about the arrangement of these donations. The proposed technique consists of two main steps: the first is a Graph Neural Network (GNN) trained without supervision; the second is a deterministic non-learned search heuristic that uses the output of the GNN to find paths and cycles. To allow for comparisons, we also implemented and tested an exact solution method using integer programming, two greedy search heuristics without the machine learning module, and the GNN alone without a heuristic. We analyze and compare the methods and conclude that the learning-based two-stage approach is the best solution quality, outp
    
[^52]: 论文标题：SurgicalGPT：端到端的语言-视觉GPT模型用于手术中的视觉问答

    SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery. (arXiv:2304.09974v1 [cs.CV])

    [http://arxiv.org/abs/2304.09974](http://arxiv.org/abs/2304.09974)

    本文提出了一种端到端的语言-视觉GPT模型，增强GPT2模型以包括视觉输入，然后微调注意力机制，以在手术VQA任务中更好地理解视觉环境。

    

    基于GPT的大型语言模型（LLM）的进展正在彻底改变自然语言处理，并在各个领域的使用呈指数级增长。这些自回归LLM可以生成长而连贯的段落，但是在需要同时处理视觉及语言信息的视觉问答（VQA）任务中，通常需要双向注意力或融合技术来捕获多种模态的环境信息。由于GPT本身不支持视觉标记处理，为了充分利用GPT模型在机器人手术VQA中的优势，我们设计了一种端到端可训练的语言-视觉GPT（LV-GPT）模型，将GPT2模型扩展为包括视觉输入（图像）。所提出的LV-GPT包括功能提取器（视觉标记器）和视觉标记嵌入（标记类型和姿态）。鉴于GPT模型中单向关注的限制以及生成连贯长段落的能力，我们提出了一种新方法，通过微调GPT模型的注意力机制，加入双向关注，从而更好地理解手术VQA任务中的视觉环境。

    Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long p
    
[^53]: 学习策略在企业流程资源分配中的应用

    Learning policies for resource allocation in business processes. (arXiv:2304.09970v1 [cs.AI])

    [http://arxiv.org/abs/2304.09970](http://arxiv.org/abs/2304.09970)

    本文提出了两种基于学习的方法来进行企业流程资源分配，具有优于常见启发式方法的效果。

    

    资源分配是将资源分配到必须在运行时刻执行的业务流程活动中。虽然资源分配在制造等其他领域中已经得到深入研究，但在业务流程管理中却只存在少量的方法。现有方法不适用于大型企业流程的应用或是只针对单个实例进行资源分配的优化。本文提出了两种基于学习的方法来进行企业流程资源分配：一种基于深度强化学习的方法和一种基于评分的价值函数逼近方法。在代表典型业务流程结构的一组情景以及在代表现实业务流程的完整网络上，将两种方法与现有的启发式方法进行比较。结果表明，我们的学习方法在大多数情景中优于或与常见的启发式方法竞争力相当。

    Resource allocation is the assignment of resources to activities that must be executed in a business process at a particular moment at run-time. While resource allocation is well-studied in other fields, such as manufacturing, there exist only a few methods in business process management. Existing methods are not suited for application in large business processes or focus on optimizing resource allocation for a single case rather than for all cases combined. To fill this gap, this paper proposes two learning-based methods for resource allocation in business processes: a deep reinforcement learning-based approach and a score-based value function approximation approach. The two methods are compared against existing heuristics in a set of scenarios that represent typical business process structures and on a complete network that represents a realistic business process. The results show that our learning-based methods outperform or are competitive with common heuristics in most scenarios a
    
[^54]: 大规模语言模型中潜在空间理论对应新兴能力

    A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])

    [http://arxiv.org/abs/2304.09960](http://arxiv.org/abs/2304.09960)

    本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。

    

    语言并不是随机生成，而是为了传递信息。语言与其底层含义之间存在强烈的关联，在其相关性方面有着严重偏差的稀疏联合分布。此外，由于稀疏性，这些高峰值恰好与语言的边缘分布匹配。随着大数据和大模型上训练的LLMs的出现，我们现在可以精确评估语言的边缘分布，这提供了一种方便的探索联合分布稀疏结构实现有效推理的方式。在本文中，我们将语言分类为明确与{\epsilon}-模糊，并提出定量结果，以表明LLMs的新兴能力（例如语言理解、上下文学习、思路启发以及有效指令微调）都可以归因于对稀疏联合分布进行贝叶斯推断。

    Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
    
[^55]: 抓住你的话语：使用生成式预训练Transformer识别虚假医师评论

    Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers. (arXiv:2304.09948v1 [cs.CL])

    [http://arxiv.org/abs/2304.09948](http://arxiv.org/abs/2304.09948)

    本研究使用GPT-3模型，比以往的传统机器学习模型（如逻辑回归和支持向量机）性能更出色。同时，本研究揭示虚假评论与真实评论在语言表达上存在的差异，分析了虚假评论的模式和特征。

    

    医生虚假评论的泛滥可能会对患者福利产生潜在危害，并引起消费者保护组织和监管机构的关注。尽管机器学习和自然语言处理领域已取得重大进展，但虚假评论与真实评论之间的差异特征还是理解有限。本研究利用了一个新的标记好的数据集，包括38048个医师评论，以验证大型语言模型对评论分类的有效性。具体来说，我们比较了传统的机器学习模型（如逻辑回归和支持向量机）与生成式预训练Transformer模型之间的性能差异。此外，我们使用GPT4，GPT系列中最新的模型，来揭示虚假和真实医生评论的关键维度。我们的研究发现，GPT-3在这个领域比传统机器学习模型表现优异。此外，我们还分析了虚假评论与真实评论在语言表达上的差异， 进一步揭示了虚假评论的模式和特征。

    The proliferation of fake reviews of doctors has potentially detrimental consequences for patient well-being and has prompted concern among consumer protection groups and regulatory bodies. Yet despite significant advancements in the fields of machine learning and natural language processing, there remains limited comprehension of the characteristics differentiating fraudulent from authentic reviews. This study utilizes a novel pre-labeled dataset of 38048 physician reviews to establish the effectiveness of large language models in classifying reviews. Specifically, we compare the performance of traditional ML models, such as logistic regression and support vector machines, to generative pre-trained transformer models. Furthermore, we use GPT4, the newest model in the GPT family, to uncover the key dimensions along which fake and genuine physician reviews differ. Our findings reveal significantly superior performance of GPT-3 over traditional ML models in this context. Additionally, ou
    
[^56]: 在演绎数据库中验证完整性条件

    \"Uberpr\"ufung von Integrit\"atsbedingungen in Deduktiven Datenbanken. (arXiv:2304.09944v1 [cs.DB])

    [http://arxiv.org/abs/2304.09944](http://arxiv.org/abs/2304.09944)

    本文提出了从逻辑编程的角度解决演绎数据库完整性问题的方法，使用证明树代替SLDNF树更方便，可以使用一组最小的条件指定知识库中更改对完整性约束的有效性的影响。

    

    计算机科学和人工智能的进步导致了更大、更复杂的知识库的开发。这些知识库容易出现矛盾，特别是在涉及多个专家时。为了保证变化期间的完整性，需要一些程序。本文从逻辑编程的角度解决了这个问题。完整性违规可被解释为完整性约束证明上的特殊操作，其中SLDNF证明是重点。我们将证明树定义为一种特殊的数据结构，并通过这样一棵树演示存在SLDNF证明的含义。证明树比SLDNF树更方便，并允许对证明进行集合取向的考虑。它们也更清晰地展示了证明结构，从而实现了进一步的应用。使用此结构，我们确定了一个最小的条件集，指定在知识库中的更改影响完整性约束的有效性的情况。此外，这种方法允许重用大型数据集。

    Advancements in computer science and AI lead to the development of larger, more complex knowledge bases. These are susceptible to contradictions, particularly when multiple experts are involved. To ensure integrity during changes, procedures are needed. This work addresses the problem from a logical programming perspective. Integrity violations can be interpreted as special operations on proofs of integrity constraints, with SLDNF proofs being the focus. We define a proof tree as a special data structure and demonstrate the implication of the existence of an SLDNF proof through such a tree. Proof trees are more convenient than SLDNF trees and allow set-oriented considerations of proofs. They also present the proof structure more clearly, enabling further applications. Using this structure, we determine a minimal set of conditions that specify when a change in the knowledge base affects the validity of an integrity constraint. Additionally, this approach allows for the reuse of large pa
    
[^57]: 基于视觉的着陆数据集-- LARD（Landing Approach Runway Detection Dataset）

    LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing. (arXiv:2304.09938v1 [cs.CV])

    [http://arxiv.org/abs/2304.09938](http://arxiv.org/abs/2304.09938)

    LARD数据集是用于着陆和进场阶段的跑道检测任务的高质量航拍图像数据集。它由大量合成图像和一些人工标注的实际着陆镜头图像组成，同时还提供了生成器以产生此类合成图像并自动注释跑道拐角点。

    

    随着对自主系统的兴趣不断增长，收集足够且代表性的真实世界数据成为一个关键挑战。尽管航空航天领域的自主着陆系统具有强烈的实用和商业价值，但缺乏开源的航空影像数据集。为了解决这一问题，我们提出了一个高质量的航拍图像数据集-LARD，用于着陆和进场阶段的跑道检测任务。数据集大部分由合成图像组成，但我们还提供了人工标注的实际着陆镜头图像，以扩展检测任务到更现实的场景。此外，我们提供了生成器，可以产生这样的合成前视图像，并通过几何变换实现跑道拐角点的自动注释。该数据集为进一步研究，如数据集质量分析或开发模型应对检测任务，铺平了道路。

    As the interest in autonomous systems continues to grow, one of the major challenges is collecting sufficient and representative real-world data. Despite the strong practical and commercial interest in autonomous landing systems in the aerospace field, there is a lack of open-source datasets of aerial images. To address this issue, we present a dataset-lard-of high-quality aerial images for the task of runway detection during approach and landing phases. Most of the dataset is composed of synthetic images but we also provide manually labelled images from real landing footages, to extend the detection task to a more realistic setting. In addition, we offer the generator which can produce such synthetic front-view images and enables automatic annotation of the runway corners through geometric transformations. This dataset paves the way for further research such as the analysis of dataset quality or the development of models to cope with the detection tasks. Find data, code and more up-to
    
[^58]: 具有定量目标的随机博弈价值迭代的停止准则

    Stopping Criteria for Value Iteration on Stochastic Games with Quantitative Objectives. (arXiv:2304.09930v1 [cs.AI])

    [http://arxiv.org/abs/2304.09930](http://arxiv.org/abs/2304.09930)

    本文提供了具有总奖励和平均收益的随机博弈的第一个价值迭代停止准则，为这些设置中提供了第一个随时算法。

    

    Markov决策过程（MDP）和随机博弈（SG）的经典解决技术是价值迭代（VI）。尽管在最近之前无法给出结果不准确性的实际界限，但由于其良好的实际性能，这种近似方法通常优于确切技术。因此，即使是最常用的模型检查器也可能返回任意错误的结果。在过去的十年中，不同的作品为各种设置派生了停止准则，特别是具有可达性、总奖励和平均收益的MDP，以及具有可达性的SG。在本文中，我们为具有总奖励和平均收益的SG提供了第一个VI停止准则，从而在这些设置中提供了第一个随时算法。为此，我们以两种方式提供解决方案：第一种是通过将其约化为MDP案例，第二种是直接在SG上。前者更简单，可以自动利用MDP停止准则的所有进展。我们展示了它在有限次VI迭代后具有概率为1的近似SG值，可以完全接近所需的SG值。后者其实更强大，因为它直接考虑了游戏结构，不需要将其归约为MDP。我们在不同的领域中展示了这两种算法，并将它们应用于不同领域，例如POMDP中的可达性目标和来自自适应脉冲耦合振荡器的平均收益游戏。

    A classic solution technique for Markov decision processes (MDP) and stochastic games (SG) is value iteration (VI). Due to its good practical performance, this approximative approach is typically preferred over exact techniques, even though no practical bounds on the imprecision of the result could be given until recently. As a consequence, even the most used model checkers could return arbitrarily wrong results. Over the past decade, different works derived stopping criteria, indicating when the precision reaches the desired level, for various settings, in particular MDP with reachability, total reward, and mean payoff, and SG with reachability.  In this paper, we provide the first stopping criteria for VI on SG with total reward and mean payoff, yielding the first anytime algorithms in these settings. To this end, we provide the solution in two flavours: First through a reduction to the MDP case and second directly on SG. The former is simpler and automatically utilizes any advances 
    
[^59]: The eBible语料库：用于面向低资源语言圈的圣经翻译的数据和模型基准

    The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages. (arXiv:2304.09919v1 [cs.CL])

    [http://arxiv.org/abs/2304.09919](http://arxiv.org/abs/2304.09919)

    介绍了一个名为eBible的圣经翻译语料库，包含1009个圣经部分翻译的数据集，涵盖了833种不同语言的数据，分布在75个语言家族中。同时还提供了基于NLLB神经机器翻译模型的性能基准，并讨论了在圣经翻译领域中的一些问题。

    

    无论采用手动、自动或两者结合的策略，高效准确地将语料库翻译成低资源语言仍然是一项挑战。许多基督教组织致力于将圣经翻译成缺乏现代翻译的语言。我们介绍了eBible语料库：一个包含1009个圣经部分翻译的数据集，其中包含833种不同语言的数据，分布在75个语言家族中。除了圣经翻译基准数据集，我们还介绍了基于No Language Left Behind（NLLB）神经机器翻译（NMT）模型的模型性能基准。最后，我们描述了圣经翻译领域特有的若干问题，并考虑已建立的数据和模型基准如何用于未来的翻译工作。对于使用NLLB进行训练的BT任务，南岛和新几内亚传输语系的语言表现不佳，而印欧语系和非洲亚洲语系的语言表现更好。

    Efficiently and accurately translating a corpus into a low-resource language remains a challenge, regardless of the strategies employed, whether manual, automated, or a combination of the two. Many Christian organizations are dedicated to the task of translating the Holy Bible into languages that lack a modern translation. Bible translation (BT) work is currently underway for over 3000 extremely low resource languages. We introduce the eBible corpus: a dataset containing 1009 translations of portions of the Bible with data in 833 different languages across 75 language families. In addition to a BT benchmarking dataset, we introduce model performance benchmarks built on the No Language Left Behind (NLLB) neural machine translation (NMT) models. Finally, we describe several problems specific to the domain of BT and consider how the established data and model benchmarks might be used for future translation efforts. For a BT task trained with NLLB, Austronesian and Trans-New Guinea languag
    
[^60]: MARS: 无需额外监督进行模型无关的有偏对象去除，用于弱监督语义分割

    MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation. (arXiv:2304.09913v1 [cs.CV])

    [http://arxiv.org/abs/2304.09913](http://arxiv.org/abs/2304.09913)

    该论文提出了一种名为MARS的方法，可以在弱监督数据中自动去除有偏对象，而无需额外的监督。

    

    弱监督语义分割旨在通过使用弱监督（如图像级别类别标签）训练语义分割模型来降低标签成本。然而，大多数方法在产生准确的定位图和在类相关背景（如检测具有火车类的铁路）中遭受假预测的有偏对象方面存在困难。 最近的去除偏见对象的方法需要额外的监督以手动识别每个有问题的类的有偏对象并通过审查预测收集它们的数据集，这限制了它们在具有多个标签和复杂关系以偏倚的真实数据集中的适用性。在第一个观察到偏见特征可以通过将偏见对象与相同数据集中的背景匹配来分离和消除的观察之后，我们提出了一个完全自动化/模型无关的有偏去除框架MARS（无需额外监督的模型无关的有偏对象去除），它利用弱监督数据中固有的偏倚模式以有效去除有偏对象，无需任何额外的监督。

    Weakly-supervised semantic segmentation aims to reduce labeling costs by training semantic segmentation models using weak supervision, such as image-level class labels. However, most approaches struggle to produce accurate localization maps and suffer from false predictions in class-related backgrounds (i.e., biased objects), such as detecting a railroad with the train class. Recent methods that remove biased objects require additional supervision for manually identifying biased objects for each problematic class and collecting their datasets by reviewing predictions, limiting their applicability to the real-world dataset with multiple labels and complex relationships for biasing. Following the first observation that biased features can be separated and eliminated by matching biased objects with backgrounds in the same dataset, we propose a fully-automatic/model-agnostic biased removal framework called MARS (Model-Agnostic biased object Removal without additional Supervision), which ut
    
[^61]: GREAT分数：使用生成模型对对抗性扰动进行全局鲁棒性评估

    GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])

    [http://arxiv.org/abs/2304.09875](http://arxiv.org/abs/2304.09875)

    本文提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。该分数捕捉了所有样本中的平均认证防攻击扰动水平，无需运行对抗性攻击。

    

    目前对于对抗性鲁棒性的研究主要集中在聚合一组数据样本的局部鲁棒性结果上，以评估和排名不同的模型。然而，局部统计量可能无法很好地代表基础未知数据分布的真正全局鲁棒性。为了解决这一挑战，本文首次尝试提出了一个新的框架——GREAT分数，用于使用生成模型对对抗性扰动进行全局鲁棒性评估。GREAT分数正式具有一个全局统计量的物理意义，捕捉来自生成模型的所有样本中的平均认证防攻击扰动水平。对于有限样本评估，我们还推导出样本复杂度和样本均值与真实均值之间的概率保证。GREAT分数有几个优点：（1）使用GREAT分数进行鲁棒性评估高效而且规模可扩展，无需运行对抗性攻击。

    Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
    
[^62]: ChatGPT作为治疗师助手的适应性研究

    ChatGPT as a Therapist Assistant: A Suitability Study. (arXiv:2304.09873v1 [cs.HC])

    [http://arxiv.org/abs/2304.09873](http://arxiv.org/abs/2304.09873)

    本文提出使用ChatGPT作为心理治疗师的助手，可以积极参与对话、整理信息、提供验证和潜在应对策略，并帮助治疗师从多次对话中发现新的见解。

    

    本文提出使用ChatGPT作为心理治疗师的助手，这是一种具有多种应用的创新技术。ChatGPT可以作为患者信息的收集器，在治疗会话之间陪伴患者，并整理收集的信息供治疗师使用，促进治疗过程。研究确定了五个研究问题，并发现了用于微调助手的有用提示，这表明ChatGPT可以积极参与对话，聆听全神贯注，提供验证和潜在应对策略，而不提供明确的医疗建议，并帮助治疗师从多次与同一患者的对话中发现新的见解。将ChatGPT用作心理治疗师的助手存在多个需要解决的挑战，包括技术和以人为本的挑战。

    This paper proposes using ChatGPT, an innovative technology with various applications, as an assistant for psychotherapy. ChatGPT can serve as a patient information collector, a companion for patients in between therapy sessions, and an organizer of gathered information for therapists to facilitate treatment processes. The research identifies five research questions and discovers useful prompts for fine-tuning the assistant, which shows that ChatGPT can participate in positive conversations, listen attentively, offer validation and potential coping strategies without providing explicit medical advice, and help therapists discover new insights from multiple conversations with the same patient. Using ChatGPT as an assistant for psychotherapy poses several challenges that need to be addressed, including technical as well as human-centric challenges which are discussed.
    
[^63]: 大规模机器学习中Adam不稳定性的理论研究

    A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])

    [http://arxiv.org/abs/2304.09871](http://arxiv.org/abs/2304.09871)

    Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。

    

    本文提出了一个之前未被解释的现象的理论，该现象出现在大型语言模型训练时的发散行为中。我们认为这种现象是由于主流的优化算法 Adam 导致的。我们观察到 Adam 可能会进入一种状态，其中参数更新向量有比较大的范数，并且与训练损失景观下的下降方向基本无关，从而导致发散。这种现象更容易在大批量情况下出现，这也是大型语言模型训练的典型设置。为了证明该理论，我们对规模不同的语言模型（70亿，300亿，650亿和5460亿参数）进行了训练运行的观察。

    We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
    
[^64]: 异构智能体强化学习

    Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])

    [http://arxiv.org/abs/2304.09870](http://arxiv.org/abs/2304.09870)

    提出了一种异构智能体强化学习（HARL）算法，解决了协作多智能体强化学习中的参数共享限制，同时通过引入多智能体优势分解引理和序列更新方案，建立了异构智能体信任区域学习（HATRL）算法及其易处理的逼近方式 HATRPO 和 HAPPO。此外，发现了一种名为异构智能体镜像学习（HAML）的新型框架，加强了对HATRPO和HAPPO的理论保证。

    

    协作多智能体强化学习（MARL）在人工智能研究中越来越受欢迎，然而，许多研究仍然严重依赖于智能体之间的参数共享，这将它们限制在同质异构智能体设置下，从而导致训练不稳定和缺乏收敛保证。为了在一般的异构智能体设置下实现有效的协作，我们提出了解决上述问题的异构智能体强化学习（HARL）算法。我们的发现核心是多智能体优势分解引理和序列更新方案。基于这些，我们开发了经过验证的无参数共享约束的异构智能体信任区域学习（HATRL）算法，并通过易处理的逼近方式得出了HATRPO和HAPPO。此外，我们发现了一种名为异构智能体镜像学习（HAML）的新型框架，它加强了对HATRPO和HAPPO的理论保证。

    The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP
    
[^65]: 进化约束强化学习策略

    Evolving Constrained Reinforcement Learning Policy. (arXiv:2304.09869v1 [cs.NE])

    [http://arxiv.org/abs/2304.09869](http://arxiv.org/abs/2304.09869)

    本文提出了一种新颖的进化约束强化学习算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中取得优异性能。

    

    进化算法已被用于演化出一组执行者，以产生多样化的体验来训练强化学习智能体，从而解决时间信用分配问题并提高探索效率。然而，当将这种方法应用于解决约束问题时，很难平衡奖励和约束违规之间的权衡。本文提出了一种新颖的进化约束强化学习（ECRL）算法，采用随机排名自适应平衡奖励和约束违规，并同时通过维护一组拉格朗日松弛系数及一个约束缓冲区来限制策略行为。在机器人控制基准测试中进行的广泛实验表明，我们的ECRL相比最先进的算法取得了优异的性能。消融分析表明引入随机排名和约束缓冲区的优势。

    Evolutionary algorithms have been used to evolve a population of actors to generate diverse experiences for training reinforcement learning agents, which helps to tackle the temporal credit assignment problem and improves the exploration efficiency. However, when adapting this approach to address constrained problems, balancing the trade-off between the reward and constraint violation is hard. In this paper, we propose a novel evolutionary constrained reinforcement learning (ECRL) algorithm, which adaptively balances the reward and constraint violation with stochastic ranking, and at the same time, restricts the policy's behaviour by maintaining a set of Lagrange relaxation coefficients with a constraint buffer. Extensive experiments on robotic control benchmarks show that our ECRL achieves outstanding performance compared to state-of-the-art algorithms. Ablation analysis shows the benefits of introducing stochastic ranking and constraint buffer.
    
[^66]: 通过保留谱的数据压缩加速支持向量聚类

    Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])

    [http://arxiv.org/abs/2304.09868](http://arxiv.org/abs/2304.09868)

    本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。

    

    支持向量聚类是一种重要的聚类方法，但是由于其计算昂贵的簇分配步骤，它面临着可伸缩性问题。在本文中，我们通过保留谱的数据压缩来加速支持向量聚类。具体而言，我们将原始数据集压缩成少量谱表示的聚合数据点，然后在压缩后的数据集上执行标准的支持向量聚类，最后将压缩数据集的聚类结果映射回原始数据集以发现簇。我们在真实数据集上的大量实验结果表明，相较于标准支持向量聚类，我们的方法大大提高了速度，而不会损失聚类质量。

    Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
    
[^67]: 引入构建论作为包容型人工智能模型的标准方法论

    Introducing Construct Theory as a Standard Methodology for Inclusive AI Models. (arXiv:2304.09867v1 [cs.CV])

    [http://arxiv.org/abs/2304.09867](http://arxiv.org/abs/2304.09867)

    构建论为人工智能模型的建构提供了更全面多元的视角，可有效解决现有模型中存在的偏见与歧视问题。

    

    社会心理学家George Kelly开发的构建论是一种用来预测和预期事件的心理构建。构建是人类解释、筛选、预测和验证数据和信息的方式。目前AI存在偏见，因为它是根据训练数据标签所定义的狭窄构建来进行训练的。目前人脸识别的机器学习算法存在着对较暗肤色的人的歧视，并且在Buolamwini、Joy 和 Timnit Gebru的开创性研究论文《Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification》中，提出了采用表型标签作为有效解决方案。在构建论中，表型只是构建出脸部特征的许多子元素之一。本文介绍了脸部构建的15个主要元素，50个子元素，并使用目前包括7个种族、性别和年龄数据的FairFace数据集测试了Google Cloud Vision API和Microsoft Cognitive Services API。

    Construct theory in social psychology, developed by George Kelly are mental constructs to predict and anticipate events. Constructs are how humans interpret, curate, predict and validate data; information. AI today is biased because it is trained with a narrow construct as defined by the training data labels. Machine Learning algorithms for facial recognition discriminate against darker skin colors and in the ground breaking research papers (Buolamwini, Joy and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. FAT (2018), the inclusion of phenotypic labeling is proposed as a viable solution. In Construct theory, phenotype is just one of the many subelements that make up the construct of a face. In this paper, we present 15 main elements of the construct of face, with 50 subelements and tested Google Cloud Vision API and Microsoft Cognitive Services API using FairFace dataset that currently has data for 7 races, genders and ages, and w
    
[^68]: 面向老年人的ChatGPT对话伴侣系统设计

    Towards Designing a ChatGPT Conversational Companion for Elderly People. (arXiv:2304.09866v1 [cs.HC])

    [http://arxiv.org/abs/2304.09866](http://arxiv.org/abs/2304.09866)

    本文为解决老年人的孤独和社交孤立问题，设计了一种基于ChatGPT的对话伴侣系统，并通过初步研究进行了评估，该系统能够生成与老年人相关的回应，但需注意其局限性和伦理影响。

    

    孤独和社交孤立是老年人普遍存在的严重问题，会影响其身体和心理健康、生活质量和寿命。本文提出了一种基于ChatGPT的老年人对话伴侣系统，旨在为老年人提供陪伴，并帮助减少孤独和社交孤立感。该系统通过初步研究进行了评估，结果表明该系统能够生成与创建的老年人形象相关的回应。然而，需要注意ChatGPT的局限性，例如潜在的偏见和不准确性，并考虑使用基于人工智能的伴侣系统对老年人的伦理影响，包括隐私问题。

    Loneliness and social isolation are serious and widespread problems among older people, affecting their physical and mental health, quality of life, and longevity. In this paper, we propose a ChatGPT-based conversational companion system for elderly people. The system is designed to provide companionship and help reduce feelings of loneliness and social isolation. The system was evaluated with a preliminary study. The results showed that the system was able to generate responses that were relevant to the created elderly personas. However, it is essential to acknowledge the limitations of ChatGPT, such as potential biases and misinformation, and to consider the ethical implications of using AI-based companionship for the elderly, including privacy concerns.
    
[^69]: 安全的对话型AI作为用户喜悦的源泉

    Safer Conversational AI as a Source of User Delight. (arXiv:2304.09865v1 [cs.HC])

    [http://arxiv.org/abs/2304.09865](http://arxiv.org/abs/2304.09865)

    本文研究了对话型AI系统管理对用户体验的影响。研究结果表明，通过适度的管理可以提高用户留存率，证明了采用AI安全机制构建能让用户喜悦的对话型AI的重要性。

    

    本文研究了管理对话型AI系统对用户体验的影响。尽管近期在大规模语言模型的发展已经使得对话型AI系统在现实世界中得到了广泛应用，但对AI安全性的担忧以及需要对系统进行管理以鼓励安全用语和防止危害的需求也随之增加。然而，一些用户认为当前的管理方法限制了技术的发展、妥协了自由表达并且限制了技术所提供的价值。本研究采取客观的立场并显示出管理并不一定会影响到用户体验。粗暴的管理方式确实有负面影响，但是通过管理的方法使AI更安全可以带来更好的用户体验。通过将不同的对话型AI部署在Chai平台中，本研究发现通过适度管理和安全系统设计可以提高用户留存率。这些结果证明了采用AI安全机制构建能让用户喜悦的对话型AI的重要性。

    This work explores the impact of moderation on users' enjoyment of conversational AI systems. While recent advancements in Large Language Models (LLMs) have led to highly capable conversational AIs that are increasingly deployed in real-world settings, there is a growing concern over AI safety and the need to moderate systems to encourage safe language and prevent harm. However, some users argue that current approaches to moderation limit the technology, compromise free expression, and limit the value delivered by the technology. This study takes an unbiased stance and shows that moderation does not necessarily detract from user enjoyment. Heavy handed moderation does seem to have a nefarious effect, but models that are moderated to be safer can lead to a better user experience. By deploying various conversational AIs in the Chai platform, the study finds that user retention can increase with a level of moderation and safe system design. These results demonstrate the importance of appr
    
[^70]: 人工智能的终端用户开发：系统性文献综述

    End-User Development for Artificial Intelligence: A Systematic Literature Review. (arXiv:2304.09863v1 [cs.HC])

    [http://arxiv.org/abs/2304.09863](http://arxiv.org/abs/2304.09863)

    本文综述了终端用户开发（EUD）对人工智能系统的影响，目的是使非技术用户直接以满足其需求的方式参与到人工智能的定义和个性化中。此外，文章还评估了EUD面临的挑战、潜在的好处以及将其整合到整个人工智能开发中的未来影响。

    

    近年来，人工智能在我们社会中变得越来越重要。然而，创建人工智能系统几乎总是IT和人工智能专家的专属权利。本文提出了一种新的方法——终端用户开发（EUD），使得非技术人员可以直接参与定义和个性化人工智能技术，从而增强AI系统的效果。本文通过系统性文献综述阐述了当前EUD对AI系统的发展格局，即如何使用户在没有人工智能和/或编程技能的情况下，定制人工智能的行为来满足他们的需求。此外，本研究还讨论了EUD面对的当前挑战、潜在的好处以及将EUD整合到整个人工智能开发过程中的未来影响。

    In recent years, Artificial Intelligence has become more and more relevant in our society. Creating AI systems is almost always the prerogative of IT and AI experts. However, users may need to create intelligent solutions tailored to their specific needs. In this way, AI systems can be enhanced if new approaches are devised to allow non-technical users to be directly involved in the definition and personalization of AI technologies. End-User Development (EUD) can provide a solution to these problems, allowing people to create, customize, or adapt AI-based systems to their own needs. This paper presents a systematic literature review that aims to shed the light on the current landscape of EUD for AI systems, i.e., how users, even without skills in AI and/or programming, can customize the AI behavior to their needs. This study also discusses the current challenges of EUD for AI, the potential benefits, and the future implications of integrating EUD into the overall AI development process
    
[^71]: NRTS：一种用于支持新生儿复苏模拟场景中多学科团队数据记录、传输和评估的客户端-服务器架构

    NRTS: A Client-Server architecture for supporting data recording, transmission and evaluation of multidisciplinary teams during the neonatal resuscitation simulation scenario. (arXiv:2304.09860v1 [cs.HC])

    [http://arxiv.org/abs/2304.09860](http://arxiv.org/abs/2304.09860)

    介绍了一种名为NRTS的Android移动应用程序，可以自动将新生儿复苏模拟课程期间记录的所有数据从医院的新生儿重症监护室发送到位于意大利皮埃蒙特东部大学的服务器上。医学教练可以使用该应用程序查看有关模拟练习的统计信息，以对涉及模拟场景的多学科团队进行评估。

    

    本技术报告介绍了新生儿复苏培训模拟器（NRTS），一款旨在支持医学专家在新生儿复苏的高保真模拟课程中输入、传输和记录数据的Android移动应用。该移动应用程序允许自动发送所有记录的数据，包括来自意大利Casale Monferrato儿童医院的新生儿重症监护室（NICU）的数据，到位于意大利皮埃蒙特东部大学科学与技术创新系（DiSIT）的服务器上。最后，医学教练可以在模拟演习期间查看统计信息，这些统计信息可用于多学科团队在模拟场景中的评估和展示。

    In this technical report, we describe Neonatal Resuscitation Training Simulator (NRTS), an Android mobile app designed to support medical experts to input, transmit and record data during a High-Fidelity Simulation course for neonatal resuscitation. This mobile app allows one to automatically send all the recorded data from "Neonatal Intensive Care Unit" (NICU) of Casale Monferrato Children's Hospital, (Italy) to a server located at the Department of Science and Technological Innovation (DiSIT), University of Piemonte Orientale (Italy). Finally, the medical instructor can view statistics on a simulation exercise that may be used during the de-briefing phase for the evaluation of multidisciplinary teams involved in the simulation scenarios.
    
[^72]: ChatGPT启用的劳动力市场的未来：初步研究

    The Future of ChatGPT-enabled Labor Market: A Preliminary Study. (arXiv:2304.09823v1 [cs.CY])

    [http://arxiv.org/abs/2304.09823](http://arxiv.org/abs/2304.09823)

    本研究从人工智能协作的角度，通过分析大规模职位发布数据及基于职业知识图谱开发的协同过滤算法，预测了ChatGPT对未来劳动力市场的影响，发现目前约28％的职业需要ChatGPT相关技能。

    

    作为一个非凡的大型语言模型，ChatGPT在各种现实任务中取得了无与伦比的成功并越来越在我们的日常生活和工作中扮演重要角色。然而，人们也提出了广泛的担忧，特别是关于ChatGPT样的人工通用智能（AGI）是否会取代人类工作。因此，在本文中，我们从人工智能协作而不是对立的角度介绍了ChatGPT启用的劳动力市场的未来的初步数据驱动研究。具体来说，我们首先对中国最大的在线招聘平台BOSS直聘中的大规模职位发布数据进行深入分析。结果表明，当前劳动力市场约有28％的职业需要ChatGPT相关技能。此外，基于大规模基于职业的知识图谱，我们开发了一个语义信息增强的协同过滤算法，以预测未来职业技能。

    As a phenomenal large language model, ChatGPT has achieved unparalleled success in various real-world tasks and increasingly plays an important role in our daily lives and work. However, extensive concerns are also raised about the potential ethical issues, especially about whether ChatGPT-like artificial general intelligence (AGI) will replace human jobs. To this end, in this paper, we introduce a preliminary data-driven study on the future of ChatGPT-enabled labor market from the view of Human-AI Symbiosis instead of Human-AI Confrontation. To be specific, we first conduct an in-depth analysis of large-scale job posting data in BOSS Zhipin, the largest online recruitment platform in China. The results indicate that about 28% of occupations in the current labor market require ChatGPT-related skills. Furthermore, based on a large-scale occupation-centered knowledge graph, we develop a semantic information enhanced collaborative filtering algorithm to predict the future occupation-skill
    
[^73]: 行程规划中的群体效用优化：一种策略性和众包意识方法

    Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach. (arXiv:2304.08495v1 [cs.AI])

    [http://arxiv.org/abs/2304.08495](http://arxiv.org/abs/2304.08495)

    本论文介绍了一种名为SCAIR的算法，可以优化群体效用，解决行程规划中的多个用户排队时间和人群水平优化的问题。

    

    行程推荐是一个具有许多实际应用的复杂的序列预测问题。当考虑到优化多个用户排队时间和人群水平时，这项任务变得更具挑战性，因为涉及到诸多参数，如景点受欢迎程度、排队时间、步行时间和营业时间等。现有的解决方案通常集中在单人视角上，未能解决自然人群行为引起的现实问题，如贪婪路由问题。本文介绍了一种名为“战略和众包意识行程推荐（SCAIR）”算法，该算法在现实环境中优化群体效用。我们将路线推荐策略建模为马尔可夫决策过程，并提出了一种状态编码机制，使得可以在线性时间内实现实时规划和分配。我们使用主题公园数据集对我们的算法进行各种竞争性和现实的基线测试，证明SCAIR优于其他算法。

    Itinerary recommendation is a complex sequence prediction problem with numerous real-world applications. This task becomes even more challenging when considering the optimization of multiple user queuing times and crowd levels, as well as numerous involved parameters, such as attraction popularity, queuing time, walking time, and operating hours. Existing solutions typically focus on single-person perspectives and fail to address real-world issues resulting from natural crowd behavior, like the Selfish Routing problem. In this paper, we introduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR) algorithm, which optimizes group utility in real-world settings. We model the route recommendation strategy as a Markov Decision Process and propose a State Encoding mechanism that enables real-time planning and allocation in linear time. We evaluate our algorithm against various competitive and realistic baselines using a theme park dataset, demonstrating that SCAIR outperforms th
    
[^74]: CornerFormer: 提升角点表征以进行精细结构重建

    CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v1 [cs.CV])

    [http://arxiv.org/abs/2304.07072](http://arxiv.org/abs/2304.07072)

    CornerFormer是一种新的方法，它利用不同建模策略于单个模型中融合角点检测和边缘预测来提升精细结构重建的表现，并在挑战性的基准测试中取得了最好的结果。

    

    结构化重建是一种非平凡的密集预测问题，它从栅格图像中提取结构信息（例如，建筑角点和边缘），然后相应地重建为二维平面图。与常见的分割或检测问题相比，它显著依赖于利用整体几何信息进行结构推理的能力。目前，基于transformer的方法采用两阶段方式解决这个具有挑战性的问题，在第一个模型中检测角点，并在第二个模型中分类拟议边缘（角对）。然而，它们将两个阶段分开成不同的模型，并且只共享主干编码器。与现有的建模策略不同，我们提出了增强的角点表示方法：1）通过在不同的粒度中共享特征，它在角点检测和边缘预测之间融合知识；2）角点候选者根据其方向作为四个热图通道提出。定性和定量评估均表明，我们的CornerFormer明显优于以前的transformer-based模型，在具有挑战性的基准测试中取得了最先进的结果。

    Structured reconstruction is a non-trivial dense prediction problem, which extracts structural information (\eg, building corners and edges) from a raster image, then reconstructs it to a 2D planar graph accordingly. Compared with common segmentation or detection problems, it significantly relays on the capability that leveraging holistic geometric information for structural reasoning. Current transformer-based approaches tackle this challenging problem in a two-stage manner, which detect corners in the first model and classify the proposed edges (corner-pairs) in the second model. However, they separate two-stage into different models and only share the backbone encoder. Unlike the existing modeling strategies, we present an enhanced corner representation method: 1) It fuses knowledge between the corner detection and edge prediction by sharing feature in different granularity; 2) Corner candidates are proposed in four heatmap channels w.r.t its direction. Both qualitative and quantita
    
[^75]: 第二届单目深度估计挑战赛

    The Second Monocular Depth Estimation Challenge. (arXiv:2304.07051v1 [cs.CV])

    [http://arxiv.org/abs/2304.07051](http://arxiv.org/abs/2304.07051)

    本文介绍了单目深度估计挑战赛的第二届比赛结果，高质量的SYNS-Patches数据集提高了比赛难度，所有提交作品都超过了基准水平。

    

    本文介绍了单目深度估计挑战赛（MDEC）的第二届比赛结果。本次比赛接受任何形式方式的监督，包括全监督、自监督、多任务或代理深度。比赛的数据集基于SYNS-Patches，其中包含高质量的密集真实数据，并具有广泛的环境多样性。比赛收到了8个独特的提交作品，所有基于点云或基于图像的指标表现都超过了基准水平。最佳监督提交作品的相对F-分数提高了27.62％，而最佳自监督提交作品提高了16.61％，这些结果代表了真正的进步。

    This paper discusses the results for the second edition of the Monocular Depth Estimation Challenge (MDEC). This edition was open to methods using any form of supervision, including fully-supervised, self-supervised, multi-task or proxy depth. The challenge was based around the SYNS-Patches dataset, which features a wide diversity of environments with high-quality dense ground-truth. This includes complex natural environments, e.g. forests or fields, which are greatly underrepresented in current benchmarks.  The challenge received eight unique submissions that outperformed the provided SotA baseline on any of the pointcloud- or image-based metrics. The top supervised submission improved relative F-Score by 27.62%, while the top self-supervised improved it by 16.61%. Supervised submissions generally leveraged large collections of datasets to improve data diversity. Self-supervised submissions instead updated the network architecture and pretrained backbones. These results represent a si
    
[^76]: 神经网络中符号的出现与语义理解和交流

    Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])

    [http://arxiv.org/abs/2304.06377](http://arxiv.org/abs/2304.06377)

    本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。

    

    能够创造有意义的符号，并熟练地将它们用于更高的认知功能，如交流、推理、规划等，是人类智能的重要和独特之处。 目前，深度神经网络仍远远落后于人类创造符号进行这些高级认知功能的能力。本文提出了一种名为SEA-net的解决方案，使神经网络具有符号创造、语义理解和交流能力。SEA-net生成动态配置网络以执行特定任务的符号。这些符号捕捉了组成性语义信息，使系统能够通过纯符号操作或交流获得新功能。此外，我们发现这些自动生成的符号呈现出类似自然语言的内在结构，表明在人类大脑和人工神经网络中生成和理解符号的共同框架。我们希望这将成为将来发展人工智能的助推器。

    Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
    
[^77]: 知识图谱实体和架构的深度主动对齐

    Deep Active Alignment of Knowledge Graph Entities and Schemata. (arXiv:2304.04389v1 [cs.DB])

    [http://arxiv.org/abs/2304.04389](http://arxiv.org/abs/2304.04389)

    本文提出了一种基于深度学习和主动学习的KG对齐方法DAAKG，可以联合对齐不仅实体，还包括关系和类别；通过主动学习选择最佳批次进行人工标注，实验结果显示其优越精度和泛化性。

    

    知识图谱（KG）存储了关于真实世界的丰富事实。本文研究KG对齐，旨在在不同的KG中找到不仅实体，还包括关系和类别的对齐。实体级别的对齐可以促进架构级别的对齐。我们提出了一种新的KG对齐方法，称为DAAKG，基于深度学习和主动学习。通过深度学习，它学习实体、关系和类别的嵌入，并在半监督方式下联合对齐它们；而通过主动学习，它估计实体、关系或类别对的推断可能性，并选择最佳的批次进行人工标注。我们设计了两个近似算法以高效选择批次解决问题。我们在基准数据集上进行的实验显示了DAAKG的优越精度和泛化性，并验证了其所有模块的有效性。

    Knowledge graphs (KGs) store rich facts about the real world. In this paper, we study KG alignment, which aims to find alignment between not only entities but also relations and classes in different KGs. Alignment at the entity level can cross-fertilize alignment at the schema level. We propose a new KG alignment approach, called DAAKG, based on deep learning and active learning. With deep learning, it learns the embeddings of entities, relations and classes, and jointly aligns them in a semi-supervised manner. With active learning, it estimates how likely an entity, relation or class pair can be inferred, and selects the best batch for human labeling. We design two approximation algorithms for efficient solution to batch selection. Our experiments on benchmark datasets show the superior accuracy and generalization of DAAKG and validate the effectiveness of all its modules.
    
[^78]: 评估ChatGPT和GPT-4的逻辑推理能力

    Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])

    [http://arxiv.org/abs/2304.03439](http://arxiv.org/abs/2304.03439)

    本文分析了多个逻辑推理数据集，评估了ChatGPT和GPT-4在逻辑推理任务上的表现，并构造了一个逻辑推理的分布之外的数据集来研究它们的鲁棒性。实验结果显示，ChatGPT在大多数逻辑推理基准测试中的表现远优于RoBERTa微调方法，而GPT-4的表现则更高。

    

    利用逻辑推理能力是一个全面的自然语言理解任务。随着先进的生成预训练转换器4（GPT-4）的发布，我们渴望了解GPT-4在各种逻辑推理任务上的表现。本文分析了多个逻辑推理数据集，包括LogiQA和ReClor等常用基准测试，以及像AR-LSAT这样的新发布的数据集。我们对需要逻辑推理的基准测试进行了多项选择阅读理解和自然语言推理任务测试。我们进一步构造了一个逻辑推理的分布之外的数据集，以研究ChatGPT和GPT-4的鲁棒性。我们还进行了ChatGPT和GPT-4之间的性能比较。实验结果表明，在大多数逻辑推理基准测试中，ChatGPT的表现远远优于RoBERTa微调方法。GPT-4在我们的手动测试中表现更高。在基准测试中，ChatGPT和GPT-4的表现相对较为均衡。

    Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
    
[^79]: 机器学习在经济研究中的应用：何时、什么和如何运用？

    Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])

    [http://arxiv.org/abs/2304.00086](http://arxiv.org/abs/2304.00086)

    本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。

    

    本文对使用机器学习工具进行经济学研究和政策分析的重要经济期刊上发表的文章进行了精选综述。综述回答了三个关键问题：（1）何时在经济学中使用机器学习，（2）常用的机器学习模型是什么，以及（3）如何将它们用于经济应用。综述强调了机器学习特别适用于处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性。深度学习模型适用于非传统数据，而集成学习模型适用于传统数据集。尽管传统的计量经济学模型在分析低复杂性数据时可能足够，但由于快速数字化和不断增长的文献，经济数据的复杂性增加，机器学习正成为计量经济学家工具箱中不可或缺的一部分。

    This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
    
[^80]: 未知嗅探器用于目标检测：不要对未知对象视而不见

    Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])

    [http://arxiv.org/abs/2303.13769](http://arxiv.org/abs/2303.13769)

    本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。

    

    近期提出的开放世界目标和开放集检测在寻找从未见过的物体并将其与已知类别区分开方面取得了突破。然而，他们对从已知类别向未知类别的知识传递的研究需要更深入，从而导致探测隐藏在背景中的未知物体的能力不足。本文中，我们提出了未知嗅探器(UnSniffer)来寻找未知和已知的目标。首先，引入广义物体置信度(GOC)分数，仅使用已知类别样本进行监督和避免在背景中不适当地压制未知物体。值得注意的是，从已知物体学习到的这种置信度分数可以推广到未知物体。此外，我们提出了负能量抑制损失来进一步限制背景中非物体样本。接下来，在推断过程中由于缺乏它们在训练中的语义信息，难以获得每个未知目标的最佳框。为了解决这个问题，

    The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
    
[^81]: MCTS-GEB：蒙特卡洛树搜索是一个好的E图构建器

    MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder. (arXiv:2303.04651v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04651](http://arxiv.org/abs/2303.04651)

    MCTS-GEB是一个通用的重写系统，使用强化学习和蒙特卡洛树搜索来构建最优的E图，有效消除了E图构建中的顺序问题，并在评估中表现出很好的性能。

    

    重写系统广泛使用等式饱和技术来优化重写顺序，但是当E图没有饱和时，无法代表所有可能的重写机会，会重新引入问题。为了解决这个问题，我们提出了MCTS-GEB，一个应用强化学习于E图构建的通用重写系统。MCTS-GEB使用蒙特卡洛树搜索（MCTS）高效规划最优的E图构建，有效地消除了E图构建阶段的顺序问题，并且在合理时间内取得了更好的性能。在两个不同领域的评估中，MCTS-GEB都表现出很好的性能。

    Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains 
    
[^82]: CUREE: 一种用于生态系统探索的好奇水下机器人

    CUREE: A Curious Underwater Robot for Ecosystem Exploration. (arXiv:2303.03943v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.03943](http://arxiv.org/abs/2303.03943)

    CUREE是一种水下机器人，通过机器人行为和感知算法提供了一组独特的能力，例如低空视觉调查、声景调查、栖息地表征和动物跟踪。在珊瑚礁上的两次实地部署中，CUREE成功识别了拍摄虾的首选栖息地类型，并追踪和跟踪了一群觅食的外科医生鱼，收集了它们的行为数据。

    

    当前探索和监测复杂的水下生态系统（如珊瑚礁）的方法是使用潜水员手持或静态相机进行调查，或者部署传感器浮标。这些方法通常无法捕捉不同珊瑚礁生物及其栖息地之间相互作用的全部变化和复杂性。本文介绍了CUREE平台，它通过机器人行为和感知算法提供了一组独特的能力，使科学家能够探索生态系统的不同方面。这些能力的示例包括低空视觉调查、声景调查、栖息地表征和动物跟踪。我们通过描述在美国维尔京群岛的珊瑚礁上进行的两次实地部署来展示这些能力。在第一次部署中，我们展示了CUREE可以通过视觉调查、栖息地表征和声景调查相结合的方式，识别出珊瑚礁中拍摄虾的首选栖息地类型。在第二次部署中，我们展示了CUREE追踪和跟踪一群觅食的外科医生鱼并收集其行为数据的能力。

    The current approach to exploring and monitoring complex underwater ecosystems, such as coral reefs, is to conduct surveys using diver-held or static cameras, or deploying sensor buoys. These approaches often fail to capture the full variation and complexity of interactions between different reef organisms and their habitat. The CUREE platform presented in this paper provides a unique set of capabilities in the form of robot behaviors and perception algorithms to enable scientists to explore different aspects of an ecosystem. Examples of these capabilities include low-altitude visual surveys, soundscape surveys, habitat characterization, and animal following. We demonstrate these capabilities by describing two field deployments on coral reefs in the US Virgin Islands. In the first deployment, we show that CUREE can identify the preferred habitat type of snapping shrimp in a reef through a combination of a visual survey, habitat characterization, and a soundscape survey. In the second d
    
[^83]: 通过受限代理学习控制深度序数分类中的类布局

    Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00396](http://arxiv.org/abs/2303.00396)

    本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。

    

    对于深度序数分类任务，学习特定于序数分类的良好结构化特征空间有助于恰当地捕捉类之间的序数属性。本文提出了一种新颖的受限代理学习方法，该方法可以为每个序数类学习一个代理，然后通过限制这些代理来调整类的全局布局。我们提出了两种策略：硬布局约束和软布局约束。硬布局约束通过直接控制代理的生成来实现，以强制将其放置在严格的线性布局或半圆形布局（即严格序数布局的两种实例）中。软布局约束通过引入正则化项到损失函数中来实现，该项惩罚偏离理想序数布局的情况。在基准数据集上的实验结果证明了所提出的CPL方法在深度序数分类中的有效性。

    For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
    
[^84]: 人工智能心理学中的“正确答案”

    "Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07267](http://arxiv.org/abs/2302.07267)

    本文使用OpenAI的GPT3.5模型重新复制了Many Labs 2复制项目中的14项研究，其中8项研究的结果被成功复制。然而，对于剩下的6项研究，GPT3.5以极其预定的方式回答了调查问题，导致无法分析这些研究。

    This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.

    大型语言模型的能力已经大大增强。这种AI系统的一个提出的应用是支持社会和认知科学中的数据收集，目前完美的实验控制是不可行的，而大规模、代表性数据集的收集通常是昂贵的。在本文中，我们使用OpenAI的text-davinci-003模型（俗称GPT3.5）重新复制了Many Labs 2复制项目中的14项研究。我们通过将每项研究的调查作为文本输入，从GPT3.5的默认设置中收集了响应。在我们可以分析的八项研究中，我们的GPT样本复制了原始结果的37.5%以及Many Labs 2结果的37.5%。出乎意料的是，我们无法像预先注册的计划那样分析剩下的六项研究。这是因为对于这六项研究中的每一项，GPT3.5以极其预定的方式回答了调查问题（无论是因变量还是条件变量）：一个未知的

    Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
    
[^85]: 在Minecraft中使用优质多样性进化飞行器

    Evolving Flying Machines in Minecraft Using Quality Diversity. (arXiv:2302.00782v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2302.00782](http://arxiv.org/abs/2302.00782)

    本文使用优质多样性算法进化Minecraft游戏中的飞行器，比传统适应度算法更可靠地工作。

    

    Minecraft是一个极好的人类创造力测试平台，启发了各种结构甚至包括飞行器的设计。EvoCraft是一个可编程生成Minecraft结构的API，但是该领域的初步工作并不能生成飞行器。本文应用基于适应度的进化和优质多样性搜索，以进化飞行器。虽然仅仅应用适应度有时可以产生飞行器，但由于使用了比以前更复杂的适应度函数，优质多样性算法MAP-Elites能够更可靠地发现飞行器，至少在使用适当的行为表征来指导多样化解决方案的搜索时是可以的。

    Minecraft is a great testbed for human creativity that has inspired the design of various structures and even functioning machines, including flying machines. EvoCraft is an API for programmatically generating structures in Minecraft, but the initial work in this domain was not capable of evolving flying machines. This paper applies fitness-based evolution and quality diversity search in order to evolve flying machines. Although fitness alone can occasionally produce flying machines, thanks in part to a more sophisticated fitness function than was used previously, the quality diversity algorithm MAP-Elites is capable of discovering flying machines much more reliably, at least when an appropriate behavior characterization is used to guide the search for diverse solutions.
    
[^86]: 基于结构学习和最优输运的鲁棒属性图对齐方法

    Robust Attributed Graph Alignment via Joint Structure Learning and Optimal Transport. (arXiv:2301.12721v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2301.12721](http://arxiv.org/abs/2301.12721)

    本文提出了一种称为SLOTAlign的无监督图对齐框架，其通过结构学习和最优输运来解决图之间的结构和特征不一致性问题，同时具有更好的对齐精度和鲁棒性。

    

    鉴于图之间的结构和特征不一致性在现实世界应用中十分普遍，本文提出了一种称为SLOTAlign的无监督图对齐框架，其中将图对齐转化为两个内部图矩阵之间的最优输运问题，同时结合多视角结构学习以增强其鲁棒性。实验证明，该算法在对齐精度和鲁棒性方面优于现有的其他方法。

    Graph alignment, which aims at identifying corresponding entities across multiple networks, has been widely applied in various domains. As the graphs to be aligned are usually constructed from different sources, the inconsistency issues of structures and features between two graphs are ubiquitous in real-world applications. Most existing methods follow the ``embed-then-cross-compare'' paradigm, which computes node embeddings in each graph and then processes node correspondences based on cross-graph embedding comparison. However, we find these methods are unstable and sub-optimal when structure or feature inconsistency appears. To this end, we propose SLOTAlign, an unsupervised graph alignment framework that jointly performs Structure Learning and Optimal Transport Alignment. We convert graph alignment to an optimal transport problem between two intra-graph matrices without the requirement of cross-graph comparison. We further incorporate multi-view structure learning to enhance graph r
    
[^87]: MEAformer: 多模式实体对齐变压器用于元模态混合

    MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.14454](http://arxiv.org/abs/2212.14454)

    该论文提出了一种适用于元模态混合的多模式实体对齐变压器方法，通过动态预测模态之间的相互关联系数以进行实体级特征聚合，进一步提出了一种模态感知的硬实体重播策略，用于解决模糊实体细节的问题。该模型在多个训练场景中实现了SOTA性能并有效提高了MMEA的鲁棒性。

    

    作为实体对齐（EA）的一个重要变体，多模式实体对齐（MMEA）旨在发现不同知识图谱（KGs）中具有相关图像的相同实体。 我们注意到，当前的MMEA算法都全局采用KG级模态融合策略进行多模式实体表示，但忽略了个体实体的模态偏好变化，从而削弱了对模态（例如模糊图像和关系）中潜在噪声的鲁棒性。在本文中，我们提出了MEAformer，一种适用于元模态混合的多模式实体对齐变压器方法，该方法动态预测模态之间的相互关联系数以进行实体级特征聚合。进一步提出了一种模态感知的硬实体重播策略，用于解决模糊实体细节的问题。实验结果表明，我们的模型不仅在多个训练场景（包括有监督、无监督、迭代和低资源设置）中实现了SOTA性能，而且通过利用模态偏好变化有效提高了MMEA的鲁棒性。

    As an important variant of entity alignment (EA), multi-modal entity alignment (MMEA) aims to discover identical entities across different knowledge graphs (KGs) with relevant images attached. We noticed that current MMEA algorithms all globally adopt the KG-level modality fusion strategies for multi-modal entity representation but ignore the variation in modality preferences for individual entities, hurting the robustness to potential noise involved in modalities (e.g., blurry images and relations). In this paper, we present MEAformer, a multi-modal entity alignment transformer approach for meta modality hybrid, which dynamically predicts the mutual correlation coefficients among modalities for entity-level feature aggregation. A modal-aware hard entity replay strategy is further proposed for addressing vague entity details. Experimental results show that our model not only achieves SOTA performance on multiple training scenarios including supervised, unsupervised, iterative, and low 
    
[^88]: MegaCRN：用于时空建模的元图卷积递归网络

    MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling. (arXiv:2212.05989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05989](http://arxiv.org/abs/2212.05989)

    MegaCRN是一个使用时空元图学习机制的元图卷积递归网络，对于时空建模具有良好的性能。

    

    时空建模是AI社区中重要的多元时间序列预测任务。本研究提出了时空元图学习作为处理图流中的异质性和非平稳性的新型图结构学习机制。通过将元图学习器嵌入GCRN编码器-解码器中，我们实现了这个思想，并创建了Meta-Graph Convolutional Recurrent Network (MegaCRN)。我们对两个基准数据集（METR-LA和PEMS-BAY）以及一个包含各种非平稳现象的大规模时空数据集进行了全面评估。我们的模型在所有三个数据集上性能都大为优越（误差率超过27％ MAE和34％ RMSE）。此外，通过一系列定性评估，我们证明了我们的模型能够明确地解开具有不同模式的位置和时间插槽。

    Spatio-temporal modeling as a canonical task of multivariate time series forecasting has been a significant research topic in AI community. To address the underlying heterogeneity and non-stationarity implied in the graph streams, in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel Graph Structure Learning mechanism on spatio-temporal data. Specifically, we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRN encoder-decoder. We conduct a comprehensive evaluation on two benchmark datasets (METR-LA and PEMS-BAY) and a large-scale spatio-temporal dataset that contains a variaty of non-stationary phenomena. Our model outperformed the state-of-the-arts to a large degree on all three datasets (over 27% MAE and 34% RMSE). Besides, through a series of qualitative evaluations, we demonstrate that our model can explicitly disentangle locations and time slots with different patt
    
[^89]: 多智能体强化学习中的谱归一化效应

    Effects of Spectral Normalization in Multi-agent Reinforcement Learning. (arXiv:2212.05331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05331](http://arxiv.org/abs/2212.05331)

    本文研究多智能体强化学习中评论者的规范化方法，发现使用谱归一化(SN)可以使评论家更稳健地学习，特别是在稀疏奖励场景下。实验结果表明，在规范化评论家的情况下，能够快速轻松地从那些复杂情境中学习，这一点对于实现稳定学习非常重要。

    

    在多智能体稀疏奖励场景下，一个可靠的评论者对于在策略上实现演员-评论者学习至关重要。然而，由于两个因素，学习一个可靠的评论者变得具有挑战性：1）随着智能体数量的增加，联合作用空间呈指数增长；2）这个因素结合奖励稀疏和环境噪声，需要大量的样本数才能实现准确的学习。我们表明，用谱归一化(SN)对评论家进行规范化，使它能够在多智能体策略上的稀疏奖励场景中更加稳健地学习。我们的实验表明，规范化的评论家能够快速地从复杂的SMAC和RWARE领域的稀缺奖励经历中学习到。这些发现强调了评论家规范化在稳定学习中的重要性。

    A reliable critic is central to on-policy actor-critic learning. But it becomes challenging to learn a reliable critic in a multi-agent sparse reward scenario due to two factors: 1) The joint action space grows exponentially with the number of agents 2) This, combined with the reward sparseness and environment noise, leads to large sample requirements for accurate learning. We show that regularising the critic with spectral normalization (SN) enables it to learn more robustly, even in multi-agent on-policy sparse reward scenarios. Our experiments show that the regularised critic is quickly able to learn from the sparse rewarding experience in the complex SMAC and RWARE domains. These findings highlight the importance of regularisation in the critic for stable learning.
    
[^90]: PowRL：用于稳健管理电力网络的强化学习框架

    PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks. (arXiv:2212.02397v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02397](http://arxiv.org/abs/2212.02397)

    本论文提出了PowRL框架来通过强化学习来有效应对电力网络中的不确定情况，并保持电网的可靠运行。

    

    世界各地的电力网络通过为多个行业、企业和家庭消费者提供不间断、可靠和无暂态电力发挥着重要的社会和经济作用。随着可再生能源和电动车产生的不确定发电和高度动态负载需求的出现，通过适当的瞬态稳定问题管理来确保电力网络的稳健运行变得越来越重要，并将停电事件限制在地方范围内。本文引入了一个名为PowRL的强化学习（RL）框架，以缓解意外网络事件的影响，并可靠地在网络上随时维持电力。PowRL利用新颖的超负荷管理启发式以及基于RL提供的最优拓扑选择决策，以确保电网在变化和不确定条件下安全、可靠地运行（无超载线路和无停电事件）。

    Power grids, across the world, play an important societal and economical role by providing uninterrupted, reliable and transient-free power to several industries, businesses and household consumers. With the advent of renewable power resources and EVs resulting into uncertain generation and highly dynamic load demands, it has become ever so important to ensure robust operation of power networks through suitable management of transient stability issues and localize the events of blackouts. In the light of ever increasing stress on the modern grid infrastructure and the grid operators, this paper presents a reinforcement learning (RL) framework, PowRL, to mitigate the effects of unexpected network events, as well as reliably maintain electricity everywhere on the network at all times. The PowRL leverages a novel heuristic for overload management, along with the RL-guided decision making on optimal topology selection to ensure that the grid is operated safely and reliably (with no overloa
    
[^91]: 连续情景控制

    Continuous Episodic Control. (arXiv:2211.15183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15183](http://arxiv.org/abs/2211.15183)

    CEC是一种新颖的非参数情景记忆算法，用于连续性行动空间问题中的序列决策制定，其在几个稀疏奖励连续控制环境中比最先进的RL和记忆增强RL算法学习更快，是学习连续控制任务的快速方法。

    

    非参数情景记忆可以用于快速锁定强化学习任务中高奖励的经验。与参数深度强化学习方法相比，在参数需要缓慢地反向传递奖励信号的方法中，这些方法只需要发现一次解决方案，然后就可以反复解决任务。然而，情景控制解决方案存储在离散表中，这种方法迄今只应用于离散行动空间问题。因此，本文介绍了连续情景控制（CEC），这是一种新颖的非参数情景记忆算法，可用于连续性行动空间问题中的序列决策制定。在几个稀疏奖励连续控制环境中的结果表明，我们提出的方法比最先进的无模型RL和记忆增强RL算法学习更快，同时保持良好的长期性能。简而言之，CEC可以是学习连续控制任务的快速方法。

    Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.
    
[^92]: 高维数据的无模型变量重要性方法

    Model free variable importance for high dimensional data. (arXiv:2211.08414v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08414](http://arxiv.org/abs/2211.08414)

    该论文提出了一种无模型的变量重要性方法，可以与任意预测函数一起使用，不需要访问预测函数，可用于研究模型残差。引入了Cohort Shapley的积分梯度版本（IGCS），使得在二元预测器情况下也可以使用IG方法。

    

    模型不可知的变量重要性方法可与任意预测函数一起使用。在这里，我们提供了一些无模型方法，不需要访问预测函数。这在预测函数是专有的且不可用或极其昂贵时很有用。当对模型的残差进行研究时也很有用。Cohort Shapley（CS）方法是无模型方法，但在输入空间的维数上具有指数成本。Frye等人（2020）的监督流形上Shapley方法也是无模型的，但要求输入第二个黑匣子模型，该模型必须为Shapley值问题进行训练。我们引入了Cohort Shapley的积分梯度（IG）版本，称为IGCS，成本为$\mathcal{O}(nd)$。我们表明，在绝大多数相关单元的立方体上，IGCS值函数接近多线性函数，其中IGCS匹配CS。IGCS的另一个好处是它允许使用二元预测器进行IG方法。我们使用一些面积...

    A model-agnostic variable importance method can be used with arbitrary prediction functions. Here we present some model-free methods that do not require access to the prediction function. This is useful when that function is proprietary and not available, or just extremely expensive. It is also useful when studying residuals from a model. The cohort Shapley (CS) method is model-free but has exponential cost in the dimension of the input space. A supervised on-manifold Shapley method from Frye et al. (2020) is also model free but requires as input a second black box model that has to be trained for the Shapley value problem. We introduce an integrated gradient (IG) version of cohort Shapley, called IGCS, with cost $\mathcal{O}(nd)$. We show that over the vast majority of the relevant unit cube that the IGCS value function is close to a multilinear function for which IGCS matches CS. Another benefit of IGCS is that is allows IG methods to be used with binary predictors. We use some area 
    
[^93]: 利用生命周期自适应处理学习自适应系统中适应空间的漂移

    Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02658](http://arxiv.org/abs/2211.02658)

    机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。

    

    最近，机器学习 (ML) 已成为支持自适应的热门方法。ML 已被用来处理自适应中的几个问题，例如在不确定性下维护最新的运行时模型和可扩展的决策制定。然而，利用 ML 存在固有的挑战。在本文中，我们着重讨论面向基于学习的自适应系统的一个特别重要的挑战：适应空间中的漂移。通过适应空间，我们指的是自适应系统在某一特定时间可以选择的适应选项的集合，以根据适应选项的质量属性进行适应。适应空间的漂移源于影响适应选项质量属性的不确定性。这种漂移可能意味着最终没有适应选项能够满足最初的适应目标，从而降低系统的质量，或者可能出现允许增强适应目标的适应选项。在 ML 中，这种漂移通常被称为概念漂移或实例漂移。为了解决这个挑战，我们提出了一种名为“生命周期自适应”的新方法。生命周期自适应对 ML powered self-adaptation 进行了扩展，使其能够更好地处理适应空间的漂移。

    Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
    
[^94]: 利用磁隧道结将多频率射频信号进行极限学习分类

    Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses. (arXiv:2211.01131v2 [cond-mat.mes-hall] UPDATED)

    [http://arxiv.org/abs/2211.01131](http://arxiv.org/abs/2211.01131)

    本文展示了利用磁隧道结将多频率RF信号进行分类的方法，成为实现嵌入式射频人工智能的关键一步。

    

    在诸如雷达和健康等各种应用中，以低能耗的人工神经网络从射频(RF)信号中提取信息是一项关键需求。这些RF输入由多个频率组成。本文展示了磁隧道结可以并行处理具有多个频率的模拟RF输入并执行突触操作。使用一种无反向传播的方法称为极限学习，我们利用作为突触和神经元的磁隧道结的实验数据，对由RF信号编码的噪声图像进行分类。我们实现了与等效软件神经网络相同的准确性。这些结果为嵌入式射频人工智能迈出了关键的一步。

    Extracting information from radiofrequency (RF) signals using artificial neural networks at low energy cost is a critical need for a wide range of applications from radars to health. These RF inputs are composed of multiples frequencies. Here we show that magnetic tunnel junctions can process analogue RF inputs with multiple frequencies in parallel and perform synaptic operations. Using a backpropagation-free method called extreme learning, we classify noisy images encoded by RF signals, using experimental data from magnetic tunnel junctions functioning as both synapses and neurons. We achieve the same accuracy as an equivalent software neural network. These results are a key step for embedded radiofrequency artificial intelligence.
    
[^95]: 带有中间表示正则化的联邦学习

    Federated Learning with Intermediate Representation Regularization. (arXiv:2210.15827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15827](http://arxiv.org/abs/2210.15827)

    本文提出一种新的联邦学习方法FedIntR，可以通过将中间层的表示集成到本地训练过程中，提供更细粒度的正则化，从而限制本地训练过程中偏离全局模型的程度，以提高模型性能。

    

    与涉及数据收集的集中式模型训练相反，联邦学习使远程客户端能够在不暴露其私有数据的情况下协同训练模型。然而，联邦学习中异构数据可能会导致模型性能下降。很多时候，为了保持较好的性能，限制本地训练过程中偏离全局模型的程度是一种有效策略。以往的研究通过正则化全局和本地学习的表示之间的距离以实现这一点，但是这种方法只考虑了模型的早期层或输出层前面的层的表示。本研究介绍了FedIntR，它通过将中间层的表示集成到本地训练过程中提供了更细粒度的正则化。具体而言，FedIntR计算一个正则化项来鼓励中间层表示和全局模型之间的接近度。

    In contrast to centralized model training that involves data collection, federated learning (FL) enables remote clients to collaboratively train a model without exposing their private data. However, model performance usually degrades in FL due to the heterogeneous data generated by clients of diverse characteristics. One promising strategy to maintain good performance is by limiting the local training from drifting far away from the global model. Previous studies accomplish this by regularizing the distance between the representations learned by the local and global models. However, they only consider representations from the early layers of a model or the layer preceding the output layer. In this study, we introduce FedIntR, which provides a more fine-grained regularization by integrating the representations of intermediate layers into the local training process. Specifically, FedIntR computes a regularization term that encourages the closeness between the intermediate layer represent
    
[^96]: 在资源受限的边缘计算中进行自适应智能传感：计算还是不计算？

    To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing. (arXiv:2209.02166v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2209.02166](http://arxiv.org/abs/2209.02166)

    本文提出了一种计算和通信延迟估计优化框架，并采用基于强化学习的方法来动态分配边缘的感知和计算资源，从而使智能传感器网络在资源受限的边缘计算中实现更佳的性能。

    

    本文考虑了一个智能传感器网络，应用于边缘计算中，对感兴趣的信号进行采样并向基站发送更新以进行远程全局监控。传感器配备有感知和计算功能，可以在传输之前在板上处理原始数据或直接发送原始数据。边缘的硬件资源有限，产生了基本的延迟 - 精度权衡：原始测量不准确但及时，而经过计算延迟后，准确的处理更新就可用。另外，如果传感器板上处理涉及数据压缩，则由于无线通信引起的延迟可能会更高。因此，需要决定何时传感器应该传输原始测量数据或依赖本地处理以最大化整体网络性能。为了解决这个传感设计问题，我们建立了一个嵌入计算和通信延迟的估计优化框架，并提出了一种基于强化学习的方法来动态分配边缘的感知和计算资源。仿真结果表明，我们提出的方法通过实现更低的估计误差、更高的吞吐量和更低的能耗，优于传统的传输方案。

    We consider a network of smart sensors for edge computing application that sample a signal of interest and send updates to a base station for remote global monitoring. Sensors are equipped with sensing and compute, and can either send raw data or process them on-board before transmission. Limited hardware resources at the edge generate a fundamental latency-accuracy trade-off: raw measurements are inaccurate but timely, whereas accurate processed updates are available after computational delay. Also, if sensor on-board processing entails data compression, latency caused by wireless communication might be higher for raw measurements. Hence, one needs to decide when sensors should transmit raw measurements or rely on local processing to maximize overall network performance. To tackle this sensing design problem, we model an estimation-theoretic optimization framework that embeds computation and communication delays, and propose a Reinforcement Learning-based approach to dynamically alloc
    
[^97]: 以负面人权为基础的长期AI安全和监管

    Negative Human Rights as a Basis for Long-term AI Safety and Regulation. (arXiv:2208.14788v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2208.14788](http://arxiv.org/abs/2208.14788)

    本文探讨了负面人权如何作为一种通用原则，为未来AI系统的技术安全约束提供基础，并支持国际监管体系的制定。

    

    如果自主的AI系统要在新领域内保持可靠的安全性，它们需要融入一些通用原则来指导其识别和避免有害行为。这些原则可能需要由一套有约束力的监管体系支持，而监管体系需要具备被广泛接受的基础性原则。同时，这些原则也需要具体到可技术实现的层面。本文从法律的角度阐述了负面人权如何胜任这些原则的角色，不仅能够作为国际监管体系的基础，而且能构建为未来AI系统的技术安全约束。

    If autonomous AI systems are to be reliably safe in novel situations, they will need to incorporate general principles guiding them to recognize and avoid harmful behaviours. Such principles may need to be supported by a binding system of regulation, which would need the underlying principles to be widely accepted. They should also be specific enough for technical implementation. Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems.
    
[^98]: 基于处理内存系统的机器学习训练的实验评估

    An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2207.07886](http://arxiv.org/abs/2207.07886)

    该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。

    

    训练机器学习算法是一种计算密集型的过程，由于不断访问大型训练数据集，这种过程通常会受到内存限制。因此，以处理器为中心的系统（例如CPU，GPU）在内存单元和处理单元之间的数据传输方面存在昂贵的瓶颈，这会消耗大量的能量和执行周期。具有处理内存（PIM）功能的内存中心计算系统可以缓解这种数据移动瓶颈。我们的目标是了解现代通用PIM架构加速ML训练的潜力。为此，我们（1）在实际通用PIM架构上实现了几种代表性的传统ML算法（即线性回归，逻辑回归，决策树，K-Means聚类），（2）严格评估和表征这些算法的准确性，性能和扩展性，并且（3）与它们在CPU和GPU上的相应实现进行比较。我们在实际内存中心计算平台上的评估表明，与相应的CPU和GPU方法相比，基于PIM的ML训练实现了显着的加速和能量效率。

    Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
    
[^99]: 使用多项式Zonotopes进行开环和闭环神经网络验证

    Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes. (arXiv:2207.02715v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2207.02715](http://arxiv.org/abs/2207.02715)

    本文提出了一种使用多项式Zonotopes进行开环和闭环神经网络验证的新方法。其主要应用于神经网络控制系统的可达性分析，并且能够捕捉非凸性，相较于其他方法表现更出色。

    

    我们提出了一种新方法，利用多项式Zonotopes高效地计算具有ReLU、sigmoid或双曲正切激活函数的神经网络中图像的紧密非凸包围。特别地，我们通过多项式逼近来抽象每个神经元的输入-输出关系，并使用多项式Zonotopes以集合的方式进行求值。虽然我们的方法也适用于开环神经网络验证，但我们的主要应用是神经网络控制系统的可达性分析，其中多项式Zonotopes能够捕捉由神经网络及系统动态引起的非凸性。我们在各种基准测试中证明，这比其他方法表现更优秀。

    We present a novel approach to efficiently compute tight non-convex enclosures of the image through neural networks with ReLU, sigmoid, or hyperbolic tangent activation functions. In particular, we abstract the input-output relation of each neuron by a polynomial approximation, which is evaluated in a set-based manner using polynomial zonotopes. While our approach can also can be beneficial for open-loop neural network verification, our main application is reachability analysis of neural network controlled systems, where polynomial zonotopes are able to capture the non-convexity caused by the neural network as well as the system dynamics. This results in a superior performance compared to other methods, as we demonstrate on various benchmarks.
    
[^100]: 一种基于搜索的测试方法，用于深度强化学习代理

    A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2206.07813](http://arxiv.org/abs/2206.07813)

    本文提出一种基于搜索的测试方法，探索状态空间以检测DRL代理的安全性，并在三个基准测试中取得了比基线方法更高的状态空间覆盖率。

    

    近十年来，深度强化学习算法越来越多地被应用于解决自动驾驶和机器人等各种决策问题。然而，由于深度强化学习代理在生命安全环境中经常表现出错误行为，导致潜在的重大错误，因此它们面临着巨大的挑战。为了评估DRL代理的安全性，一种方法是对它们进行测试，以检测可能导致关键故障的故障。这就提出了一个问题，即我们如何有效地测试DRL策略，以确保它们的正确性和遵守安全要求。本文提出了一种基于搜索的测试方法，通过引导代理生成满足安全要求的状态序列变化，以探索环境的状态空间。我们的方法在三种不同的DRL基准测试中进行了评估，并在保持相似或更好的测试效果的同时，实现了比基线方法更高的状态空间覆盖率。

    Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac
    
[^101]: 马尔科夫潜在博弈中的独立和去中心化学习

    Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14590](http://arxiv.org/abs/2205.14590)

    独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。

    

    我们提出了一种多智能体强化学习机制，并分析了它在无限时间折扣马尔科夫潜在博弈中的收敛性。我们专注于独立和去中心化的设置，在这种设置下，玩家不了解游戏模型，也不能进行协调。在每个阶段，玩家通过异步方式更新他们的打扰Q函数的估计值，该函数根据实现的一阶段奖励评估他们的总体条件付款。然后，玩家通过将基于估计Q函数的平滑最优一阶段偏差策略纳入其策略中来独立地更新其策略。学习动态的关键特征是Q函数估计是以比策略更快的时间尺度进行更新的。我们证明了我们的学习动态引导的策略在概率1的情况下收敛到马尔科夫潜在博弈的稳定纳什平衡。我们的结果凸显了简单学习动态在达到马尔可夫潜在博弈的稳定纳什平衡方面的功效，即使是在独立和去中心化代理环境中。

    We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
    
[^102]: FLEX: 用于复杂知识图谱推理的特征逻辑嵌入框架

    FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning. (arXiv:2205.11039v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.11039](http://arxiv.org/abs/2205.11039)

    FLEX是一种新型KGR框架，能够真正处理包括合取、析取、否定等所有FOL操作并支持各种特征空间。实验证明，FLEX优于现有最先进算法。

    

    知识图谱推理（KGR）的当前最佳模型引入几何对象或概率分布将实体和一阶逻辑（FOL）查询嵌入到低维向量空间中。它们可以总结为一个中心-尺寸框架（点/盒式/锥形，Beta/Gaussian分布等），但是它们的逻辑推理能力有限。为了解决这些难题，我们提出了一种名为特征逻辑嵌入框架（FLEX）的新型KGR框架，它是首个真正能够处理包括合取、析取、否定等所有FOL操作并支持各种特征空间的KGR框架。具体来说，特征逻辑框架的逻辑部分基于向量逻辑，自然地建模了所有FOL操作。实验证明，FLEX在各种基准数据集上显著优于现有的最先进算法。

    Current best performing models for knowledge graph reasoning (KGR) introduce geometry objects or probabilistic distributions to embed entities and first-order logical (FOL) queries into low-dimensional vector spaces. They can be summarized as a center-size framework (point/box/cone, Beta/Gaussian distribution, etc.). However, they have limited logical reasoning ability. And it is difficult to generalize to various features, because the center and size are one-to-one constrained, unable to have multiple centers or sizes. To address these challenges, we instead propose a novel KGR framework named Feature-Logic Embedding framework, FLEX, which is the first KGR framework that can not only TRULY handle all FOL operations including conjunction, disjunction, negation and so on, but also support various feature spaces. Specifically, the logic part of feature-logic framework is based on vector logic, which naturally models all FOL operations. Experiments demonstrate that FLEX significantly outp
    
[^103]: 通信效率高的自适应联邦学习

    Communication-Efficient Adaptive Federated Learning. (arXiv:2205.02719v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02719](http://arxiv.org/abs/2205.02719)

    本文提出了一种新的通信效率高的自适应联邦学习方法（FedCAMS），可以解决联邦学习中由于重复的服务器-客户端同步而产生的大量通信开销和基于 SGD 的模型更新缺乏适应性等诸多问题。

    

    联邦学习是一种机器学习训练方式，使得客户端可以在不共享本地数据的情况下共同训练模型。然而，实际中实现联邦学习仍面临许多挑战，如由于重复的服务器-客户端同步而产生的大量通信开销以及基于 SGD 的模型更新缺乏适应性。尽管已经提出了各种方法来通过梯度压缩或量化来减少通信成本，并提出了FedAdam等联邦版本的自适应优化器来增加更多的适应性，但当前的联邦学习框架仍无法同时解决上述所有挑战。本文提出了一种新的通信效率高的自适应联邦学习方法（FedCAMS），具有理论上的收敛保证。

    Federated learning is a machine learning training paradigm that enables clients to jointly train models without sharing their own localized data. However, the implementation of federated learning in practice still faces numerous challenges, such as the large communication overhead due to the repetitive server-client synchronization and the lack of adaptivity by SGD-based model updates. Despite that various methods have been proposed for reducing the communication cost by gradient compression or quantization, and the federated versions of adaptive optimizers such as FedAdam are proposed to add more adaptivity, the current federated learning framework still cannot solve the aforementioned challenges all at once. In this paper, we propose a novel communication-efficient adaptive federated learning method (FedCAMS) with theoretical convergence guarantees. We show that in the nonconvex stochastic optimization setting, our proposed FedCAMS achieves the same convergence rate of $O(\frac{1}{\s
    
[^104]: 基于AI的日志分析器: 一种实用的方法

    AI based Log Analyser: A Practical Approach. (arXiv:2203.10960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.10960](http://arxiv.org/abs/2203.10960)

    本研究使用Transformer构建仅使用正常的日志条目来训练新模型，并通过多种形式的扰动来增强日志，以应对日志来源的异构性和缺乏标签用于训练分类器的限制；该模型进一步使用一种有限的标签样本进行强化学习的方式进行微调，实验结果表现良好。

    

    日志分析是进行故障或网络事件检测、调查和技术取证分析以实现系统和网络韧性的重要活动。AI算法在日志分析中的潜在应用可以增强这种复杂和繁重的任务。然而，这种解决方案存在一些限制，如日志来源的异构性和没有或很少标签用于训练分类器。当这些标签变得可用时，需要更新分类器。这项基于实践的研究旨在通过使用Transformer构建来仅使用正常的日志条目训练新模型来解决这些挑战。通过多种形式的扰动来增强日志，以作为自监督训练用于特征学习。该模型进一步使用一种有限的标签样本进行强化学习的方式进行微调，以模仿实际情况下有标签可用的情况。我们的模型结构的实验结果显示出很好的性能。

    The analysis of logs is a vital activity undertaken for fault or cyber incident detection, investigation and technical forensics analysis for system and cyber resilience. The potential application of AI algorithms for Log analysis could augment such complex and laborious tasks. However, such solution has its constraints the heterogeneity of log sources and limited to no labels for training a classifier. When such labels become available, the need for the classifier to be updated. This practice-based research seeks to address these challenges with the use of Transformer construct to train a new model with only normal log entries. Log augmentation through multiple forms of perturbation is applied as a form of self-supervised training for feature learning. The model is further finetuned using a form of reinforcement learning with a limited set of label samples to mimic real-world situation with the availability of labels. The experimental results of our model construct show promise with c
    
[^105]: 路用户检测的概率方法

    Probabilistic Approach for Road-Users Detection. (arXiv:2112.01360v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.01360](http://arxiv.org/abs/2112.01360)

    本文介绍了一种用于缓解深度目标检测模型中过于自信预测问题的方法，通过引入一种新颖的概率层来避免传统的预测层，实验证明该方法能够减少假阳性中的过度自信。

    

    自动驾驶应用中的目标检测意味着对语义对象的检测和跟踪通常是城市驾驶环境的特色，如行人和车辆。目前最先进的基于深度学习的目标检测中存在假阳性问题，这些问题通常带有过于自信的得分。在自动驾驶和其他关键的机器人感知领域，这是非常不希望看到的，因为涉及安全问题。本文提出了一种方法来缓解过于自信的预测问题，通过在测试中引入一种新颖的概率层，向深度目标检测网络中添加这种概率层。建议的方法避免了传统的Sigmoid或Softmax预测层，这些层通常会产生过于自信的预测。实验证明，所提出的技术能够减少假阳性中的过度自信，而不会降低真阳性的性能。该方法在2D-KITTI目标检测中进行了验证，使用了YOLOV4和S。

    Object detection in autonomous driving applications implies that the detection and tracking of semantic objects are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and S
    
[^106]: 边界图神经网络用于 3D 模拟

    Boundary Graph Neural Networks for 3D Simulations. (arXiv:2106.11299v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11299](http://arxiv.org/abs/2106.11299)

    本文提出了一种新型边界图神经网络（BGNNs），能够有效地表示三维颗粒流动的复杂几何形状，从而实现了高效的预测和计算。

    

    大量数据的出现使得机器学习在自然科学和工程学方面具有了可观的动力，然而对物理过程进行建模通常很困难。其中一个特别棘手的问题是如何有效地表示几何边界。三角化的几何边界在工程应用中得到了广泛理解和使用。然而，基于它们的尺寸和方向的异质性，将它们集成到机器学习方法中通常十分困难。在本文中，我们引入了一种有效的理论来建模粒子与边界的相互作用，从而推导出了我们的新型边界图神经网络（BGNNs），该网络动态地修改图结构以满足边界条件。新的 BGNNs 在复杂的三维颗粒流动过程（如漏斗、旋转鼓和搅拌器）中进行了测试，这些过程都是现代工业机械的标准组件，但其几何形状仍然十分复杂。BGNNs 在计算效率和预测准确率方面都得到了评估。

    The abundance of data has given machine learning considerable momentum in natural sciences and engineering, though modeling of physical processes is often difficult. A particularly tough problem is the efficient representation of geometric boundaries. Triangularized geometric boundaries are well understood and ubiquitous in engineering applications. However, it is notoriously difficult to integrate them into machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce an effective theory to model particle-boundary interactions, which leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify graph structures to obey boundary conditions. The new BGNNs are tested on complex 3D granular flow processes of hoppers, rotating drums and mixers, which are all standard components of modern industrial machinery but still have complicated geometry. BGNNs are evaluated in terms of computational efficiency as well as pre
    
[^107]: FOLE ERA：基础探讨

    The ERA of FOLE: Foundation. (arXiv:1512.07430v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/1512.07430](http://arxiv.org/abs/1512.07430)

    本文讨论本体在FOLE一阶逻辑环境中的表示，特别是提供了ERA数据模型的严格数学表示，作为本体论的基础探讨。

    

    本文讨论本体在FOLE（Kent 2013）一阶逻辑环境中的表示。本体定义了用于为话语社区建模知识资源的原语（Gruber 2009）。这些原语包括类、关系和属性，由实体-关系-属性（ERA）数据模型（Chen 1976）表示。本文是三篇论文中的第一篇，它在FOLE的第一阶逻辑环境中提供了ERA数据模型的严格数学表示，特别是本体论的基础探讨。

    This paper discusses the representation of ontologies in the first-order logical environment FOLE (Kent 2013). An ontology defines the primitives with which to model the knowledge resources for a community of discourse (Gruber 2009). These primitives, consisting of classes, relationships and properties, are represented by the entity-relationship-attribute ERA data model (Chen 1976). An ontology uses formal axioms to constrain the interpretation of these primitives. In short, an ontology specifies a logical theory. This paper is the first in a series of three papers that provide a rigorous mathematical representation for the ERA data model in particular, and ontologies in general, within the first-order logical environment FOLE. The first two papers show how FOLE represents the formalism and semantics of (many-sorted) first-order logic in a classification form corresponding to ideas discussed in the Information Flow Framework (IFF). In particular, this first paper provides a foundation 
    

