# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluating Large Language Models for Generalization and Robustness via Data Compression](https://arxiv.org/abs/2402.00861) | 通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。 |
| [^2] | [SymbolicAI: A framework for logic-based approaches combining generative models and solvers](https://arxiv.org/abs/2402.00854) | SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。 |
| [^3] | [X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System](https://arxiv.org/abs/2402.00839) | 本论文介绍了一种名为X-CBA的新颖可解释型IDS方法，该方法利用图神经网络处理网络流量数据，并采用了新的可解释人工智能方法。它通过利用更广泛的流量数据，包括边属性，以处理网络威胁的挑战。 |
| [^4] | [ALISON: Fast and Effective Stylometric Authorship Obfuscation](https://arxiv.org/abs/2402.00835) | ALISON是一种快速有效的风格学作者身份混淆方法，通过攻击AA模型来保护隐私，相比竞争方法，具有更快的混淆速度和更好的混淆成功率。 |
| [^5] | [A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks](https://arxiv.org/abs/2402.00831) | 该论文介绍了一种使用YANG数据模型与黑洞敏感度量矩阵分析的方法，用于在骨干网络中进行黑洞检测。这种方法填补了骨干网络黑洞检测方法的空白，提供了有效的检测策略。 |
| [^6] | [Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters](https://arxiv.org/abs/2402.00828) | 本文研究了使用软适配器混合实现音频频谱变换的高效微调，证明了该方法在音频和语音任务中的优越性能。 |
| [^7] | [SLIM: Skill Learning with Multiple Critics](https://arxiv.org/abs/2402.00823) | SLIM是一种多判别器学习方法，通过在机器人操作中组合多个判别器的奖励函数，显著改善了潜变量技能发现，克服了奖励之间的干扰。 |
| [^8] | [WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework](https://arxiv.org/abs/2402.00822) | WiOpen是一种稳健的基于Wi-Fi的开放式手势识别框架，通过消除不确定性和定义精确的决策边界来解决在实际应用中未见手势被错误分类的问题。 |
| [^9] | [Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments](https://arxiv.org/abs/2402.00816) | 本文提出了在连续环境中利用近似基于模型的屏蔽技术实现概率安全保证的方法，并通过与约束强化学习算法的对比实验证明了其通用性和稳定性。 |
| [^10] | [Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario](https://arxiv.org/abs/2402.00808) | 本研究探索了在协作装配场景中，Cobot的生产节奏对参与者的经验性定位控制和情绪状态的影响。虽然在情绪状态和定位控制方面未发现差异，但考虑到其他心理变量，结果表明需要考虑个体的心理特征来提供更好的互动体验。 |
| [^11] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^12] | [LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law](https://arxiv.org/abs/2402.00795) | 本文研究了预训练语言模型LLMs对动力系统行为的外推能力，发现LLaMA 2能够准确预测动力系统的时间序列。此外，输入上下文窗口的长度越长，学习到的物理规则的准确性越高，揭示了一种上下文中的神经比例定律。 |
| [^13] | [ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models](https://arxiv.org/abs/2402.00794) | 本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。 |
| [^14] | [Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction](https://arxiv.org/abs/2402.00793) | 本论文介绍了一种新的框架，将人类专业知识纳入算法预测中，重点在于利用人的判断力区分对于任何可行的预测算法来说“看起来相同”的输入。 |
| [^15] | [Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2402.00789) | Graph-Mamba是第一个尝试通过将Mamba模块与输入相关的节点选择机制集成来增强图网络中长程上下文建模的方法。 |
| [^16] | [Building Expressive and Tractable Probabilistic Generative Models: A Review](https://arxiv.org/abs/2402.00759) | 本文综述了富有表现力和可处理的概率生成建模领域的进展和技术，并重点关注了概率电路。文章提供了关于表达能力和可处理性之间权衡的统一视角，并说明了设计原则和算法扩展，成功地构建了富有表现力和高效的概率电路。此外，文章还讨论了最新的深度和混合概率电路研究，并概述了未来研究的挑战和开放性问题。 |
| [^17] | [Unlearnable Algorithms for In-context Learning](https://arxiv.org/abs/2402.00751) | 本文提出了一种针对预先训练的大型语言模型的高效去学习方法，通过选择少量训练示例来实现任务适应训练数据的精确去学习，并与微调方法进行了比较和讨论。 |
| [^18] | [Transforming and Combining Rewards for Aligning Large Language Models](https://arxiv.org/abs/2402.00742) | 本研究主要研究了对齐大规模语言模型的方法中出现的两个问题：奖励模型的选择以及多个奖励模型的组合。通过引入概率解释，我们提出了一种从Bradley-Terry偏好模型中学习的奖励的自然变换选择，该变换强调改善表现不佳的输出，从而减轻了欠拟合和奖励欺骗。 |
| [^19] | [FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game](https://arxiv.org/abs/2402.00738) | 本论文提出了一种名为FM3Q的新型多智能体强化学习框架，采用个体全局最小最大原则，通过分解联合最小最大Q函数为个体函数，并解决了团队内信用分配、数据利用和计算复杂性不足的问题。 |
| [^20] | [Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators](https://arxiv.org/abs/2402.00722) | 本论文提出了一种基于神经风格转移和TD3网络的共享控制方法，可以将多种风格应用于机器人操纵器的运动中。该方法通过使用自动编码器和双延迟深度确定性策略梯度网络来生成机器人控制策略，实现了机器人运动的风格转移。 |
| [^21] | [Intent Assurance using LLMs guided by Intent Drift](https://arxiv.org/abs/2402.00715) | 该论文介绍了一种使用以意图漂移为引导的LLMs进行意图保证的框架，通过利用自然语言模型生成的策略识别和处理意图漂移，以实现意图与业务目标的对齐。 |
| [^22] | [Non-Exchangeable Conformal Language Generation with Nearest Neighbors](https://arxiv.org/abs/2402.00707) | 本文介绍了一种利用最近邻方法扩展的非交换的共形语言生成框架，用于量化自动生成文本的不确定性，并提供带有统计保证的预测集。 |
| [^23] | [PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software](https://arxiv.org/abs/2402.00699) | 本研究介绍了PeaTMOSS数据集，用于记录和分析开源软件中预训练模型的元数据和应用情况。这对于了解预训练模型的采用和重复使用的影响具有重要意义。 |
| [^24] | [Ocassionally Secure: A Comparative Analysis of Code Generation Assistants](https://arxiv.org/abs/2402.00689) | 本文通过比较分析四种先进的LLMs在9个任务上的表现，确定和理解了在真实场景中有效且安全地部署LLMs生成优质代码的条件和环境。 |
| [^25] | [Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications](https://arxiv.org/abs/2402.00678) | 以往的机器人编程方法对非专家用户不友好，这篇论文提出了使用连续目标导向动作来实现真实评估的方法，通过编码动作的特征变化来适应智能城市应用中的各种特征，并使用进化算法来计算机器人的关节轨迹。 |
| [^26] | [Neural Policy Style Transfer](https://arxiv.org/abs/2402.00677) | 本研究提出了神经策略风格转换算法，通过深度强化学习来实现控制策略的风格转换。通过训练不同的网络来最大化预期奖励，同时编码了行为的目标和风格，从而将一个策略的风格转移到另一个策略而保持其内容不变。通过逆强化学习和用户演示实现模型的训练和风格的编码。 |
| [^27] | [Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching](https://arxiv.org/abs/2402.00676) | 本研究提出在艺术机器人应用中引入深度Q学习网络，旨在改进艺术机器人应用的控制策略。 |
| [^28] | [Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID](https://arxiv.org/abs/2402.00672) | 该论文提出了一种同时考虑均质和异质实例级别结构，构建高质量跨模态标签关联的模态统一标签传输方法，用于无监督可见-红外人物重新识别。 |
| [^29] | [Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing](https://arxiv.org/abs/2402.00658) | 本文提出了一种通过直接偏好优化在收集到的轨迹上学习基于规划的推理的框架，以解决大型语言模型在复杂推理任务中的虚幻和缺陷问题。 |
| [^30] | [CapHuman: Capture Your Moments in Parallel Universes](https://arxiv.org/abs/2402.00627) | CapHuman是一个新框架，通过“编码然后学习对齐”的范式实现了可泛化的身份保留能力，用于在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。 |
| [^31] | [Are Synthetic Time-series Data Really not as Good as Real Data?](https://arxiv.org/abs/2402.00607) | 本研究引入了InfoBoost，一种高度通用的跨领域数据合成框架，具备时间序列表示学习能力，并开发了一种基于合成数据的方法，可以实现超越真实数据训练的模型性能。此外，我们还训练了一个通用特征提取器，适用于所有时间序列数据。实验证明，我们的方法能够克服多个干扰源的影响，提高泛化能力。 |
| [^32] | [Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations](https://arxiv.org/abs/2402.00591) | Sandra是一个神经符号推理器，通过将矢量表示与演绎推理相结合，利用本体论建立的向量空间进行推理。它基于描述和情境的本体设计模式，能够从一组事实中推断出所有可能的解释，并在实验中证明在不增加复杂性的情况下优于其他基准线的分类结果，并且具有可解释性和向量空间的可控性。 |
| [^33] | [BrainSLAM: SLAM on Neural Population Activity Data](https://arxiv.org/abs/2402.00588) | BrainSLAM是一种使用大鼠不同脑区的人群活动数据进行SLAM的方法，通过卷积神经网络解码速度和熟悉度信息，并使用吸引子网络进行路径积分和闭环检测，从而构建地图。 |
| [^34] | [EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models](https://arxiv.org/abs/2402.00518) | 该论文介绍了一种经济且可扩展的解决方案EE-Tuning，可以使用较少的计算资源和训练数据针对早期终止大型语言模型进行调整，通过性能优化和3D并行性实现卓越的训练效率。实验证实，即使在有限的训练预算下，也可以实现有效的早期终止LLM推理。 |
| [^35] | [EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data Configurations](https://arxiv.org/abs/2402.00491) | 本研究通过定量和定性研究发现，全局模型导向解释在数据配置过程中的指导作用不足，数据导向解释提高了对配置后系统变化的理解，两种解释类型的混合融合表现出最高的有效性。 |
| [^36] | [A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems](https://arxiv.org/abs/2402.00485) | 这篇论文提出了一个个性化推荐系统框架，通过优化算法实现了消费者和生产者两方的公平性约束。该框架具有可泛化性和灵活性，可以根据不同的群体分割、推荐模型选择和领域设置实现公平性优化。 |
| [^37] | [SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models](https://arxiv.org/abs/2402.00474) | SA-MDKIF是一种可扩展和适应性强的医学领域知识注入框架，通过指令调整并训练医学技能，并在推理中将其与大型语言模型集成，提高了医学任务的性能。 |
| [^38] | [RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum Radiation Exposure Pathway](https://arxiv.org/abs/2402.00468) | 本文提出了一种基于深度 Q 学习的架构 RadDQN，在辐射区域中提供时间高效的最小辐射暴露路径。通过使用辐射感知奖励函数和独特的探索策略，该架构可以优化人员辐射暴露，为自主无人机领域带来辐射防护的创新解决方案。 |
| [^39] | [Genetic-based Constraint Programming for Resource Constrained Job Scheduling](https://arxiv.org/abs/2402.00459) | 本论文提出了一种基于遗传算法的约束编程方法来解决资源受限作业调度问题，该方法通过进化编程发现高效的搜索策略，并通过新的表示方法、适应度评估方案和预选择机制实现了创新。 |
| [^40] | [Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection](https://arxiv.org/abs/2402.00448) | 这篇论文介绍了一种用于无监督异常检测的双学生知识蒸馏网络，通过引入两个具有相同规模但结构相反的学生网络和一个单一预训练的教师网络，提高了对正常数据的识别一致性，并同时引入了异常表示的多样性。 |
| [^41] | [A Survey of Data-Efficient Graph Learning](https://arxiv.org/abs/2402.00447) | 这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。 |
| [^42] | [A practical existence theorem for reduced order models based on convolutional autoencoders](https://arxiv.org/abs/2402.00435) | 本论文提出了基于卷积自编码器的降阶模型的实用存在定理，解决了在处理复杂非线性问题方面传统方法的不足，并讨论了如何学习潜在特征的挑战。 |
| [^43] | [Prompt-Time Symbolic Knowledge Capture with Large Language Models](https://arxiv.org/abs/2402.00414) | 本文研究利用大型语言模型来实现基于提示驱动的知识捕获，特别关注提示到三元组的生成，并通过专门的合成数据集评估了三种方法的性能。 |
| [^44] | [Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection](https://arxiv.org/abs/2402.00412) | 本文旨在对抗评估AI生成的学生论文检测，通过构建AIG-ASAP数据集和使用文本扰动方法，揭示现有检测器易被自动对抗攻击所绕过的问题。 |
| [^45] | [LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model](https://arxiv.org/abs/2402.00411) | 本文通过提出LM-HT模型，一个可学习的多层次阈值模型，增强了脉冲神经网络（SNN）与人工神经网络（ANN）的性能对应关系。 |
| [^46] | [Investigating Bias Representations in Llama 2 Chat via Activation Steering](https://arxiv.org/abs/2402.00402) | 我们通过激活导向技术研究了Llama 2 Chat中的偏见表示问题，发现该模型存在固有的性别偏见，并观察到偏见与模型拒绝回应的倾向之间存在负相关关系。 |
| [^47] | [Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting](https://arxiv.org/abs/2402.00397) | 我们提出了一种跨城市少样本交通预测的解决方案，利用多尺度交通模式库从数据丰富的源城市学习并预测其他城市的交通情况。 |
| [^48] | [Efficient Exploration for LLMs](https://arxiv.org/abs/2402.00396) | 高效探索在改善大型语言模型方面具有显著优势，可以以较少的查询实现较高的性能水平。不确定性估计和探索方案的选择是关键因素。 |
| [^49] | [Loss Function Considering Dead Zone for Neural Networks](https://arxiv.org/abs/2402.00393) | 本文提出了一种新的损失函数，考虑机械手臂死区内的逆动力学计算。该方法能够提高训练可用的运动数据量，并提高逆动力学计算的准确性。 |
| [^50] | [EASRec: Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems](https://arxiv.org/abs/2402.00390) | EASRec是一个针对顺序推荐系统的弹性架构搜索方法，通过自动剪枝技术和先进模型架构结合，以及资源受限神经架构搜索技术，实现了降低计算成本和资源消耗的同时保持或增强准确性。 |
| [^51] | [On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the Dimension](https://arxiv.org/abs/2402.00389) | 这项研究探讨了RMSProp及其动量扩展方法的收敛速度，并发现使用$\ell_1$范数测度时，收敛速度为$O(\frac{\sqrt{d}}{T^{1/4}})$，在维度极大的问题中具有改进依赖性。 |
| [^52] | [Cumulative Distribution Function based General Temporal Point Processes](https://arxiv.org/abs/2402.00388) | 本研究引入了CuFun模型，基于累积分布函数的通用时间点过程，解决了深度时间点过程模型中的强度函数建模、积分计算复杂性和长期时序依赖性捕捉的问题。 |
| [^53] | [Legged Robot State Estimation With Invariant Extended Kalman Filter Using Neural Measurement Network](https://arxiv.org/abs/2402.00366) | 本文介绍了一种新型的基于模型的滤波器和深度神经网络结合的腿式机器人状态估计器。我们通过将神经测量网络与不变扩展卡尔曼滤波器结合，提高了状态估计的性能。与现有的研究不同的是，我们的方法仅使用仿真数据，通过调整学习技术和正则化来缩小模拟到真实的差距。 |
| [^54] | [Climate Trends of Tropical Cyclone Intensity and Energy Extremes Revealed by Deep Learning](https://arxiv.org/abs/2402.00362) | 本研究利用深度学习方法重建了过去的热带气旋观测，并获得了全球热带气旋风场数据，揭示了热带气旋强度和能量的气候趋势。 |
| [^55] | [Adaptive Primal-Dual Method for Safe Reinforcement Learning](https://arxiv.org/abs/2402.00355) | 本论文提出了一种自适应原始-对偶方法用于安全强化学习，通过调整自适应学习速率以优化策略，实现了算法的收敛性、最优性和可行性。实验结果表明，该方法在安全强化学习中具有更好的性能和稳定性。 |
| [^56] | [Large Language Models Based Fuzzing Techniques: A Survey](https://arxiv.org/abs/2402.00350) | 这篇论文对基于大语言模型的模糊测试技术进行了综述，提供了对LLMs、模糊测试和基于LLMs的模糊测试方法的系统概述，并讨论了相关的挑战和未来的研究方向。 |
| [^57] | [ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update](https://arxiv.org/abs/2402.00348) | ODICE研究了离线强化学习和模仿学习中重要的分布校正估计（DICE）方法，并发现在使用真梯度更新学习值函数时存在前向梯度和后向梯度两个梯度项。为了解决这个问题，他们提出了一种简单但有效的修正方法。 |
| [^58] | [Multi-agent Path Finding for Cooperative Autonomous Driving](https://arxiv.org/abs/2402.00334) | 本论文将多智能体路径规划和协同自动驾驶相结合，提出了优化交叉口驶入顺序的完整算法OBS-KATS，该算法在各种情况下都表现出了显著优势。 |
| [^59] | [SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling](https://arxiv.org/abs/2402.00319) | SCO-VIST是一个基于社交互动常识的视觉故事创作框架，通过表示图形和使用加权故事图的方法，能够生成连贯、引人入胜的视觉故事。 |
| [^60] | [The whack-a-mole governance challenge for AI-enabled synthetic biology: literature review and emerging frameworks](https://arxiv.org/abs/2402.00312) | 本文综述了AI赋能的合成生物学面临的治理挑战，提出了新兴框架，结合创新和生物安全的共同目标，探讨了命令与控制、管护、自底向上和自由放任等治理选择。 |
| [^61] | [An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction](https://arxiv.org/abs/2402.00306) | 本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。 |
| [^62] | [PAP-REC: Personalized Automatic Prompt for Recommendation Language Model](https://arxiv.org/abs/2402.00284) | 本研究提出了PAP-REC框架，用于生成个性化自动提示的推荐语言模型。该框架通过自动生成个性化提示标记来减轻手动设计提示所带来的效率和效果问题。 |
| [^63] | [Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective](https://arxiv.org/abs/2402.00262) | 本调查和展望介绍了将计算实验与基于大型语言模型的智能体相结合的研究方法，并讨论了这种方法对解决社会科学中的问题具有的潜力。 |
| [^64] | [Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs](https://arxiv.org/abs/2402.00260) | 本文提出了一种以Large Language Model (LLM)为基础的社交机器人，用于与自闭症谱系障碍儿童进行口头交流，教授透视能力。通过比较不同的LLM管道，发现GPT-2 + BART管道可以更好地生成问题和选择项。这种研究有助于改善自闭症儿童的社交能力。 |
| [^65] | [Vertical Symbolic Regression via Deep Policy Gradient](https://arxiv.org/abs/2402.00254) | 通过使用深度策略梯度的垂直符号回归（VSR-DPG），我们能够从实验数据中发现涉及多个独立变量的符号方程，超过了以前的方法和变体。 |
| [^66] | [Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning](https://arxiv.org/abs/2402.00251) | 本文提出了一种高效的非参数化不确定性量化方法，用于黑盒大型语言模型和决策规划。该方法可以有效地估计输入-决策之间的逐点依赖关系，并提供统计上对决策可信度的解释。另外，本文还提出了一个系统化的决策代理设计，根据用户提示生成动作，并在存在多个高估计逐点依赖性的动作时要求用户提供偏好。 |
| [^67] | [Are Generative AI systems Capable of Supporting Information Needs of Patients?](https://arxiv.org/abs/2402.00234) | 生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。 |
| [^68] | [Learning Label Hierarchy with Supervised Contrastive Learning](https://arxiv.org/abs/2402.00232) | 本文介绍了一种将标签层次结构融入有监督对比学习的方法，通过调整实例之间的距离和引入额外的对比，创建一个更为良好结构化和有区分性的特征空间。 |
| [^69] | [FedCore: Straggler-Free Federated Learning with Distributed Coresets](https://arxiv.org/abs/2402.00219) | FedCore是一种通过分布式选择核心集解决联邦学习中慢速客户端问题的算法，可以显著减少训练时间，并保护隐私。 |
| [^70] | [Improving Object Detection Quality in Football Through Super-Resolution Techniques](https://arxiv.org/abs/2402.00163) | 通过超分辨率技术提高的目标检测准确性在足球比赛中具有重要的应用价值。 |
| [^71] | [Decomposable Submodular Maximization in Federated Setting](https://arxiv.org/abs/2402.00138) | 该论文提出了一种联邦优化设置用于在计算上不可行的可分解子模函数优化问题中。在这个设置中，客户端拥有私有的组件函数，通过并行计算和集中聚合的方式来求解最大化问题。 |
| [^72] | [A Reinforcement Learning Based Controller to Minimize Forces on the Crutches of a Lower-Limb Exoskeleton](https://arxiv.org/abs/2402.00135) | 本研究使用深度强化学习方法开发了一种最小化拐杖上地面反作用力的运动控制器，以减轻外骨骼用户上半身努力的目标。 |
| [^73] | [Deep Neural Networks: A Formulation Via Non-Archimedean Analysis](https://arxiv.org/abs/2402.00094) | 该论文引入了一种新的深度神经网络（DNNs）类别，其采用多层树状结构的架构并使用非阿基米德局部域的整数环进行编码。这些DNNs是稳健的对实值函数和实值平方可积函数的普遍逼近器。 |
| [^74] | [Episodic-free Task Selection for Few-shot Learning](https://arxiv.org/abs/2402.00092) | 本文提出了一种超越分集训练的元训练框架，通过选择一些无分集任务对元学习器进行训练，并使用亲和性标准来评估其有效性。实验结果显示，这种方法在少样本学习中取得了更好的效果。 |
| [^75] | [SCAPE: Searching Conceptual Architecture Prompts using Evolution](https://arxiv.org/abs/2402.00089) | SCAPE是一个将进化搜索与生成式人工智能相结合的工具，能够帮助用户通过简单的操作探索由初始输入启发的创造性和高质量设计，同时提高了图像的新颖性、质量和使用效果。 |
| [^76] | [Retrosynthesis prediction enhanced by in-silico reaction data augmentation](https://arxiv.org/abs/2402.00086) | RetroWISE 是一个利用计算模拟的反应数据增强的 retrosynthesis 预测的框架，通过使用易于获取的非配对数据生成配对数据，提高了模型的训练性能。 |
| [^77] | [Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning](https://arxiv.org/abs/2402.00085) | 本论文提出了Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)框架，通过引入预定学习和好奇心，显著提高了基于强化学习的任务导向对话代理的效果。 |
| [^78] | [EPSD: Early Pruning with Self-Distillation for Efficient Model Compression](https://arxiv.org/abs/2402.00084) | 提出了一种早期修剪与自蒸馏相结合的框架，实现了高效的模型压缩。 |
| [^79] | [Modeling Access Differences to Reduce Disparity in Resource Allocation](https://arxiv.org/abs/2402.00083) | 本论文研究了具有准入差异的资源分配问题，以减少资源分配差异，并通过开发准入模型和准入感知的分配方法来实现。实验结果表明，这种方法能够在不知道准入差距的情况下有效减少资源差异。 |
| [^80] | [Exploitation Strategies in Conditional Markov Chain Search: A case study on the three-index assignment problem](https://arxiv.org/abs/2402.00076) | 这个研究探索了扩展有条件马尔可夫链搜索（CMCS）框架的几种方法，以提高其在开发方面的能力。实验结果表明，两阶段CMCS优于单一阶段CMCS。 |
| [^81] | [EvoMerge: Neuroevolution for Large Language Models](https://arxiv.org/abs/2402.00070) | EvoMerge是一种针对大型语言模型训练和合并的系统性方法，通过权重交叉和微调进行权重变异，旨在将模型推向超越传统微调限制的进化过程。 |
| [^82] | [Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators](https://arxiv.org/abs/2402.00069) | 本文介绍了使用抽象计算机体系结构描述语言（ACAD）对AI硬件加速器进行建模的研究。通过使用ACAD，工程师可以更好地理解不同加速器设计方案的性能特性，从而在选择和配置加速器时做出更准确的决策。 |
| [^83] | [GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries](https://arxiv.org/abs/2402.00068) | 本论文提出了一种基于LLM的框架，可以适应不同类型的锂离子电池，实现准确的健康状态估计。这项工作解决了生成训练数据的时间和资源成本高的挑战，并在实际应用中具有良好的泛化能力。 |
| [^84] | [TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting](https://arxiv.org/abs/2402.00066) | TrackGPT是一种基于GPT的实体轨迹预测模型，能够生成准确的预测结果，包括长期预测和短期预测，并在多个领域展现出良好的性能。 |
| [^85] | [A technical note for the 91-clauses SAT resolution with Indirect QAOA based approach](https://arxiv.org/abs/2402.00065) | 本文针对3-SAT问题提出了一种基于QAOA的间接方法，通过建模解决方案的排名来实现高效的解析，可以处理大规模的问题。 |
| [^86] | [Merging plans with incomplete knowledge about actions and goals through an agent-based reputation system](https://arxiv.org/abs/2402.00064) | 本论文介绍了一种通过基于代理的声誉系统合并具有关于动作和目标的不完整知识的计划的方法，该方法提供了一个自动化生成过渡计划的工具，对认知障碍人士具有帮助。 |
| [^87] | [Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory](https://arxiv.org/abs/2402.00060) | 本文提出了一种基于Dempster-Shafer理论的方法，用于在共同数据信息中建模认知不确定性和根据置信度对共同事件进行分类。通过构建概率箱和DSt结构，可以计算特定碰撞概率的置信度和可能性。 |
| [^88] | [FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting](https://arxiv.org/abs/2402.00059) | FengWu-GHR是全球首个以数据驱动方式运行的公里级全球天气预报模型，具有更高的分辨率和可比甚至更高的预报技巧。 |
| [^89] | [Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors](https://arxiv.org/abs/2402.00053) | 这篇论文提出了一种快速、准确的知识图谱链接预测评估框架，解决了现有方法中随机抽样带来的排名指标不准确的问题。 |
| [^90] | [Zero-shot Sequential Neuro-symbolic Reasoning for Automatically Generating Architecture Schematic Designs](https://arxiv.org/abs/2402.00052) | 提出了一种利用神经推理和符号推理相结合的零射击顺序神经符号推理方法，用于自动生成架构原理图设计，解决了专家见解依赖和技术挑战的问题。 |
| [^91] | [IICONGRAPH: improved Iconographic and Iconological Statements in Knowledge Graphs](https://arxiv.org/abs/2402.00048) | 改进的知识图谱IICONGRAPH填补了当前知识图谱中关于图标学和图象学陈述的空缺，并通过改进和扩展ArCo和Wikidata实现了更好的性能。 |
| [^92] | [Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning](https://arxiv.org/abs/2402.00046) | PetriRL是一个创新框架，将Petri网和基于事件的强化学习集成，用于解决工业作业车间调度问题（JSSP）。该框架利用Petri网进行建模，提高了可解释性，并通过事件驱动控制和动作屏蔽的集成取得了竞争性的性能。 |
| [^93] | [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/abs/2402.00045) | 本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。 |
| [^94] | [Interactive and Intelligent Root Cause Analysis in Manufacturing with Causal Bayesian Networks and Knowledge Graphs](https://arxiv.org/abs/2402.00043) | 本文提出了一个互动智能的制造业根本原因分析工具，通过结合电动汽车制造过程的专家知识和数据驱动的机器学习方法，利用大规模知识图谱进行推理并学习一个因果贝叶斯网络。 |
| [^95] | [Optimized Task Assignment and Predictive Maintenance for Industrial Machines using Markov Decision Process](https://arxiv.org/abs/2402.00042) | 本文提出了一种使用马尔可夫决策过程的分布式决策方法，用于工业设备任务分配和预测性维护。该方法实现了任务分配和健康管理决策代理之间的信息共享，并将不确定性纳入决策过程中。通过详细的数学模型和案例研究，证明了该方法的有效性和实际适用性。 |
| [^96] | [Spatial-temporal-demand clustering for solving large-scale vehicle routing problems with time windows](https://arxiv.org/abs/2402.00041) | 该论文提出了一种基于聚类的解决大规模车辆路径问题的方法，通过结合空间、时间、需求数据，将问题分解为子路径问题，然后分别解决，并通过局部搜索来改进整体解决方案。 |
| [^97] | [Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity](https://arxiv.org/abs/2402.00037) | 本文探讨了利用生成性人工智能在STEM团队中促进多样性与包容性的潜力。主要通过包容性分析和个性化的自适应人工智能系统实现，在此基础上提出了四个政策建议，包括规范化合作评估、包容性分析、为社会认知研究筹集资金以及人工智能与人类团队合作进行包容性培训。 |
| [^98] | [Why does Prediction Accuracy Decrease over Time? Uncertain Positive Learning for Cloud Failure Prediction](https://arxiv.org/abs/2402.00034) | 本论文针对云故障预测中不确定正样本学习问题，设计了一种不确定正样本学习风险估算器（Uptake）方法，实验结果表明重新训练模型后预测准确性可能下降约9%。 |
| [^99] | [LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition](https://arxiv.org/abs/2402.00033) | LF-ViT模型通过局部化和聚焦的方式减少计算需求，同时提高图像识别效率，显著降低了Deit-S模型的FLOPs。 |
| [^100] | [An Integrated Framework for Team Formation and Winner Prediction in the FIRST Robotics Competition: Model, Algorithm, and Analysis](https://arxiv.org/abs/2402.00031) | 本研究提出了一个集成框架，通过在团队形成之前的队员技能数据上进行优化，预测团队表现，填补了FIRST Robotics Competition领域的研究空白。 |
| [^101] | [Evolution-Bootstrapped Simulation: Artificial or Human Intelligence: Which Came First?](https://arxiv.org/abs/2402.00030) | 人类创造了人工智能，但在一个由自然选择驱动的世界中，神经网络可能比人类更早进化，并且能够通过进化引导获得超级智能。 |
| [^102] | [Exploring Public Opinion on Responsible AI Through The Lens of Cultural Consensus Theory](https://arxiv.org/abs/2402.00029) | 应用文化共识理论，研究发现美国公众对负责任人工智能存在共享和对比的观点，这为开发者和政策制定者在决策过程中更有效地考虑个体差异和群体文化观点提供了重要参考。 |
| [^103] | [Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition](https://arxiv.org/abs/2402.00025) | 本研究提出了一种加速W4A16量化推断的Triton融合内核的实现方法，通过使用SplitK工作分解实现解量化和GEMM操作，显著提高了瘦矩阵乘法的执行速度。 |
| [^104] | [Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding](https://arxiv.org/abs/2402.00024) | LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。 |
| [^105] | [Deploying ADVISER: Impact and Lessons from Using Artificial Intelligence for Child Vaccination Uptake in Nigeria](https://arxiv.org/abs/2402.00017) | 这个论文描述了我们与尼日利亚政府合作部署了ADVISE：AI-驱动的疫苗接种干预优化器，该框架基于整数线性规划，旨在最大化成功疫苗接种的累计概率，在尼日利亚是首次成功部署的AI工具链。 |
| [^106] | [Maintaining User Trust Through Multistage Uncertainty Aware Inference](https://arxiv.org/abs/2402.00015) | 本文介绍了一种多阶段的人工智能部署方法，其中每个阶段都使用更准确的推理方法，以维护用户的信任。作者提出了一种量化模型不确定性的方法，以促进对推理结果的有信心的决策。该方法已经在印度的数千名棉农中进行了实际部署，并且可以适用于其他低资源环境中的人工智能部署。 |
| [^107] | [Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications](https://arxiv.org/abs/2401.17434) | 引入生成式人工智能的黑客马拉松在软件行业中发挥重要作用，并在教育领域带来了机遇和挑战。 |
| [^108] | [SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning](https://arxiv.org/abs/2401.16013) | 这个论文介绍了SERL软件套件，它是一个用于样本高效的机器人强化学习的库。该库包含了一个离线深度强化学习方法、计算奖励和重置环境的方法，高质量的机器人控制器，以及一些具有挑战性的示例任务。这个软件套件的目标是解决机器人强化学习的难以使用和获取性的挑战。 |
| [^109] | [Reliability and Interpretability in Science and Deep Learning](https://arxiv.org/abs/2401.07359) | 这篇论文强调了科学与深度学习中模型假设的重要性，并提供了对模型假设认识论复杂性的分析，同时结合标准错误分析与深度神经网络模型的特点，来评估模型可靠性。 |
| [^110] | [Small LLMs Are Weak Tool Learners: A Multi-LLM Agent](https://arxiv.org/abs/2401.07324) | 本论文提出了一种新的策略，将大型语言模型代理（LLMs）的能力分解为计划器、调用器和总结器模块，以克服小型模型性能限制和工具更新的问题。 |
| [^111] | [Commonsense for Zero-Shot Natural Language Video Localization](https://arxiv.org/abs/2312.17429) | 本文研究了零样本自然语言-视频定位中常识推理的有效性，并提出了CORONET框架，该框架利用常识进行视频和生成的伪查询之间的桥接。实验证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。 |
| [^112] | [Emergence and Causality in Complex Systems: A Survey on Causal Emergence and Related Quantitative Studies](https://arxiv.org/abs/2312.16815) | 本文综述了关于因果出现和相关定量研究的最新进展，重点解决了量化因果出现和在数据中识别因果出现的两个问题，并建立了因果出现与人工智能之间的联系。 |
| [^113] | [Parrot Captions Teach CLIP to Spot Text](https://arxiv.org/abs/2312.14232) | 本研究发现在图像-文本数据集LAION-2B中，标题密集地“模仿”图像中嵌入的视觉文本，导致CLIP模型受到文本定位偏差的影响。通过使用以“模仿标题”为标准的训练集，可以有效解决这一问题。 |
| [^114] | [Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration](https://arxiv.org/abs/2312.06643) | 通过分析参与者的注视行为，我们发现向协作机器人的注视可以作为启动联合活动的触发器。这一发现对于改进工业人机协作和操作体验具有重要意义。 |
| [^115] | [EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism](https://arxiv.org/abs/2312.04916) | EE-LLM是一个用于大规模训练和推理早退出大型语言模型的框架，具有三维并行性和多项算法创新。研究发现EE-LLM在训练效率上表现出色，计算开销极小。 |
| [^116] | [RLHF and IIA: Perverse Incentives](https://arxiv.org/abs/2312.01057) | RLHF算法中的IIA假设导致了倒置激励，限制了查询格式和学习算法的创新。 |
| [^117] | [ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach](https://arxiv.org/abs/2311.16136) | 本文提出了ERASER，一种通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习。 |
| [^118] | [Trustworthy Large Models in Vision: A Survey](https://arxiv.org/abs/2311.09680) | 这篇综述研究了可信的大模型在视觉领域的问题，包括人为误用、漏洞性、固有问题和可解释性，并提出了对应的挑战和对策，旨在推动可信的LMs与人类期望的对齐 |
| [^119] | [Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks](https://arxiv.org/abs/2311.05085) | 大型语言模型在知识密集型任务中的理性化能力有待探索，通过使用专家编写的示例，可以生成更受欢迎的基于世界知识的理性化方式。这些理性化方式需要进一步改进，在错误预测的理性化方面会损害人类对模型的信任。 |
| [^120] | [SugarViT -- Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet](https://arxiv.org/abs/2311.03076) | SugarViT是一个基于视觉转换和深度标签分布学习的框架，用于自动化大规模田间图像的疾病严重程度评分。将遥感数据与环境参数结合，实现了对糖菜叶斑病严重程度的预测。 |
| [^121] | [InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining](https://arxiv.org/abs/2310.07713) | InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。 |
| [^122] | [Generative quantum machine learning via denoising diffusion probabilistic models](https://arxiv.org/abs/2310.05866) | 通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。 |
| [^123] | [On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space](https://arxiv.org/abs/2310.04915) | 本文研究在SE(3)不变空间中加速扩散机制，提出新的加速方案，可以以50倍到100倍的速度生成高质量的分子构象。 |
| [^124] | [Deep Learning Methods for Calibrated Photometric Stereo and Beyond](https://arxiv.org/abs/2212.08414) | 本文综述了基于深度学习的校准光度立体方法，并总结了它们在最广泛使用的基准数据集上的性能，展示了深度学习在光度立体领域的高级表现。 |
| [^125] | [Block size estimation for data partitioning in HPC applications using machine learning techniques](https://arxiv.org/abs/2211.10819) | 这项工作介绍了一种名为BLEST-ML的方法，通过机器学习技术进行块大小估计，以加速并行数据密集型应用和提高可伸缩性。这种方法在分布式计算库dislib上进行了评估。 |
| [^126] | [Weak Collocation Regression method: fast reveal hidden stochastic dynamics from high-dimensional aggregate data](https://arxiv.org/abs/2209.02628) | 该论文提出了一种名为弱相关回归（WCR）方法的新方法，能够在没有轨迹的情况下快速有效地揭示高维聚合数据中的隐藏随机动态。 |
| [^127] | [Fair Machine Learning in Healthcare: A Review](https://arxiv.org/abs/2206.14397) | 这篇综述论文研究了机器学习在医疗保健领域的公平问题。通过采用基于分配公正原则的框架，将公平问题分为资源平均分配和性能平等两个类别，并从机器学习的角度对相关的公正度量进行了批判性的回顾。论文还讨论了机器学习生命周期各个阶段的偏见和缓解策略，探讨了偏见与其对策之间的关系。 |
| [^128] | [A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics.](http://arxiv.org/abs/2401.15122) | 提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。 |
| [^129] | [Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning.](http://arxiv.org/abs/2401.15098) | Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。 |
| [^130] | [Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed.](http://arxiv.org/abs/2401.13346) | UMBRELLA是一个大规模的物联网试验平台，具有多个应用案例，包括自动街灯监控、数字孪生环境、联邦学习框架和容器化应用入侵检测。未来，UMBRELLA还有潜力用于智能城市和多机器人群感知应用。 |
| [^131] | [Balancing the AI Strength of Roles in Self-Play Training with Regret Matching+.](http://arxiv.org/abs/2401.12557) | 通过引入后悔匹配+，本论文提出了一种简单的方法来平衡自博弈训练中不同角色的AI能力。 |
| [^132] | [Emergent Dominance Hierarchies in Reinforcement Learning Agents.](http://arxiv.org/abs/2401.12258) | 本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。 |
| [^133] | [Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation.](http://arxiv.org/abs/2401.11864) | 本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。 |
| [^134] | [Towards Learning from Graphs with Heterophily: Progress and Future.](http://arxiv.org/abs/2401.09769) | 本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。 |
| [^135] | [Meta Prompting for AGI Systems.](http://arxiv.org/abs/2311.11482) | 本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。 |
| [^136] | [Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis.](http://arxiv.org/abs/2311.10776) | 本研究提出了一种转变性的人工智能代理，利用检索增强生成（RAG）技术自动化化学中的反应条件推荐（RCR）任务，通过模拟专家化学家的策略，使用大型语言模型（LLM）和新反应指纹，显著优于传统人工智能。此系统可以减轻化学家的工作负担，使他们能够更专注于更基础和创造性的科学问题。 |
| [^137] | [Evolutionary Tabletop Game Design: A Case Study in the Risk Game.](http://arxiv.org/abs/2310.20008) | 本研究提出了进化游戏设计方法的扩展，通过生成《风险》游戏的新变体来验证该方法。结果显示生成的新变体拥有更小的地图和更短的比赛时间，并产生更加平衡的游戏对局。 |
| [^138] | [Auction-Based Scheduling.](http://arxiv.org/abs/2310.11798) | 该论文提出了一种基于拍卖的调度框架，用于解决多目标决策问题。该框架的创新之处在于将每个目标的实现分配给单独的策略，并且可以独立创建、修改和替换这些策略。使用拍卖机制来解决冲突和组合策略，确保长期的调度公平性。 |
| [^139] | [Instilling Inductive Biases with Subnetworks.](http://arxiv.org/abs/2310.10899) | 通过子网络注入归纳偏置，这项研究探索了理解和控制神经网络行为的方法。通过发现功能子网络并利用它们，可以显著减少训练模型所需的数据量。 |
| [^140] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^141] | [SELF: Language-Driven Self-Evolution for Large Language Model.](http://arxiv.org/abs/2310.00533) | SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。 |
| [^142] | [HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning.](http://arxiv.org/abs/2310.00113) | HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。 |
| [^143] | [Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies.](http://arxiv.org/abs/2309.13063) | 通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。 |
| [^144] | [ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning.](http://arxiv.org/abs/2309.05915) | 这篇论文提出了一种通过将动态规划应用于决策Transformer来增强其能力的方法。作者提出了三个步骤来实现这一目标：使用样本内值迭代获得近似值函数，结合估计的优势评估动作质量，并训练ACT生成基于估计优势的动作。该方法在测试中表现出良好的性能。 |
| [^145] | [Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance.](http://arxiv.org/abs/2309.00300) | 本文提出了一个可识别的认知诊断框架，该框架能够从学生的答题记录中直接诊断可识别和可解释的考生特征和题目特征，并通过重建答题记录来确保诊断结果的可识别性。 |
| [^146] | [FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning.](http://arxiv.org/abs/2308.12264) | FECoM是一个用于细粒度深度学习能耗测量的框架，通过静态仪器分析和考虑计算负载和温度稳定性等因素，为研究人员和开发人员提供了对深度学习API进行概要分析的机制。 |
| [^147] | ["Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion.](http://arxiv.org/abs/2308.10974) | "引用GPT的“豚鼠试验”是一种创新的智能代理建模方法，利用智能代理代表企业进行竞争和勾结研究。它比使用人类主体进行实验更具成本效益和灵活性，并展现出超越传统代理建模方法的能力。" |
| [^148] | [SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2308.02594) | 本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。 |
| [^149] | [Artificial intelligence is algorithmic mimicry: why artificial "agents" are not (and won't be) proper agents.](http://arxiv.org/abs/2307.07515) | 本研究通过对比生物系统和算法系统，指出了生物系统具有自我制造自主能力、符号和物理方面没有区分以及体验到模糊问题的大世界等特点，而算法系统则与此相反。 |
| [^150] | [Identifiability of direct effects from summary causal graphs.](http://arxiv.org/abs/2306.16958) | 该论文研究了在缺乏完整时间因果图的情况下，直接因果效应如何从总结因果图中进行可辨识，并提出了一个完整的可辨识性结果。 |
| [^151] | [Towards Open Vocabulary Learning: A Survey.](http://arxiv.org/abs/2306.15880) | 该论文调研了在视觉场景理解领域的开放词汇学习，在与零样本学习和开放集识别等相关概念的比较中，总结和分析了该领域的最新发展。 |
| [^152] | [VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores.](http://arxiv.org/abs/2306.01879) | 我们提出了VisualGPTScore方法，能够使用多模态生成分数捕捉文本标题可能性，并在图像条件语言模型上进行计算，具备组合推理能力。 |
| [^153] | [A closer look at the training dynamics of knowledge distillation.](http://arxiv.org/abs/2303.11098) | 本文对知识蒸馏的训练动态进行了详细研究，实验证明投影器的设计决策、表示的标准化和软最大函数的选择对学生的性能有着重要影响，同时提出了一种解决容量差异问题的简单方法，以及与当前最先进的知识蒸馏技术相媲美的计算效率更高的方法。 |
| [^154] | [Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient.](http://arxiv.org/abs/2302.13144) | 本文从递推视角重新审视了LQR控制问题，并应用递推-视角策略梯度（RHPG）模型提供了一种采样复杂度分析，通过无需任何先验信息进行优化求解，并展示了RHPG在线性控制和估计中的普适性。 |

# 详细

[^1]: 通过数据压缩评估大型语言模型的泛化性和鲁棒性

    Evaluating Large Language Models for Generalization and Robustness via Data Compression

    [https://arxiv.org/abs/2402.00861](https://arxiv.org/abs/2402.00861)

    通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。

    

    现有的大型语言模型评估方法面临数据污染、对提示敏感以及基准测试创建成本高等挑战。为了解决这些问题，我们提出了一种基于无损数据压缩的评估方法，测试模型的预测能力在其训练截止日期之后的泛化情况。具体而言，我们收集了从2017年到2023年共83个月的全面测试数据，并根据模型的训练数据截止日期将数据分为训练和测试期。我们使用两个指标进行评估：1）测试期的压缩性能作为对未见数据的泛化能力的衡量；2）训练期和测试期之间的性能差距作为鲁棒性的衡量。我们的实验证明，许多模型的压缩率在截止日期之后显著降低，但像... (内容过长，省略)

    Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a
    
[^2]: SymbolicAI: 一个结合生成模型和求解器的基于逻辑的方法的框架

    SymbolicAI: A framework for logic-based approaches combining generative models and solvers

    [https://arxiv.org/abs/2402.00854](https://arxiv.org/abs/2402.00854)

    SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。

    

    我们介绍了SymbolicAI，这是一个多功能且模块化的框架，采用基于逻辑的方法来处理生成过程中的概念学习和流程管理。SymbolicAI通过将大型语言模型（LLM）作为语义解析器来执行基于自然语言和形式语言指令的任务，从而弥合了符号推理和生成式人工智能之间的差距，使生成模型与各种求解器无缝集成。我们利用概率编程原理来处理复杂任务，并利用可微分和经典编程范 paradigms 的各自优势。该框架引入了一系列多态的、组合的和自指的数据流操作，将LLM的输出与用户的目标对齐。因此，我们可以在具有零次和少次学习能力的各种基础模型之间进行过渡，并与擅长解决特定问题的专业化调优模型或求解器配合使用。

    We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
    
[^3]: X-CBA: 基于可解释性的CatBoosted Anomal-E用于入侵检测系统

    X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System

    [https://arxiv.org/abs/2402.00839](https://arxiv.org/abs/2402.00839)

    本论文介绍了一种名为X-CBA的新颖可解释型IDS方法，该方法利用图神经网络处理网络流量数据，并采用了新的可解释人工智能方法。它通过利用更广泛的流量数据，包括边属性，以处理网络威胁的挑战。

    

    在网络威胁日益复杂的时代，入侵检测系统（IDS）的效果至关重要。机器学习（ML）和深度学习（DL）模型为识别计算机网络中的攻击和异常提供了高效准确的解决方案。然而，在IDS中使用ML和DL模型导致了信任赤字，因为它们的决策过程不透明。这种IDS研究中的透明度差距显著，影响了信心和问责制。为了解决这个问题，本文引入了一种新颖的可解释型IDS方法，称为X-CBA，它利用图神经网络（GNN）的结构优势来有效处理网络流量数据，并采用新的可解释人工智能（XAI）方法。与大多数以GNN为基础的IDS不同，我们的方法不仅依赖于标记的网络流量和节点特征，还通过网络流量，包括边属性，来利用更广泛的流量数据。

    The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to
    
[^4]: ALISON: 快速有效的风格学作者身份混淆

    ALISON: Fast and Effective Stylometric Authorship Obfuscation

    [https://arxiv.org/abs/2402.00835](https://arxiv.org/abs/2402.00835)

    ALISON是一种快速有效的风格学作者身份混淆方法，通过攻击AA模型来保护隐私，相比竞争方法，具有更快的混淆速度和更好的混淆成功率。

    

    作者归属度（AA）和作者身份混淆（AO）是隐私研究中日益重要的两项竞争任务。现代AA利用作者的一贯写作风格，使用AA分类器将文本与其作者匹配。AO是相应的对抗性任务，旨在以一种方式修改文本，使其语义得到保留，但AA模型无法正确推断其作者。为了解决最先进的AA方法引发的隐私问题，提出了新的AO方法，但由于其训练和混淆速度过慢（通常需要数小时），使用起来仍然不太实际。面对这一挑战，我们提出了一种实用的AO方法ALISON，它（1）大大减少了训练/混淆时间，演示了比最先进的AO方法快10倍以上的混淆速度，（2）通过攻击两个基准数据集上的三种基于Transformer的AA方法，实现了更好的混淆成功率，通常比竞争方法表现好15%。

    Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing method
    
[^5]: YANG辅助下的骨干网络黑洞检测统一策略

    A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks

    [https://arxiv.org/abs/2402.00831](https://arxiv.org/abs/2402.00831)

    该论文介绍了一种使用YANG数据模型与黑洞敏感度量矩阵分析的方法，用于在骨干网络中进行黑洞检测。这种方法填补了骨干网络黑洞检测方法的空白，提供了有效的检测策略。

    

    尽管在互联网骨干网络中解决黑洞故障的重要性不可忽视，但骨干网络中的有效检测策略仍然缺乏。这主要是因为先前的研究主要集中在移动自组网(MANETs)上，而MANETs在动态、协议和拓扑上有着完全不同的操作，因此其研究结果无法直接应用于骨干网络。此外，骨干网络中的黑洞故障检测是一项特别具有挑战性的任务。由于需要考虑多样的条件，这需要收集大量的网络数据，使得数据收集和分析变得并不直观。为填补这一空白，我们的研究引入了一种新颖的方法，使用专门的Yet Another Next Generation (YANG)数据模型与黑洞敏感度量矩阵(BHMM)分析来进行骨干网络中的黑洞检测。本文详细介绍了我们选择和分析了与黑洞检测相关的四个YANG模型的方法。

    Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole de
    
[^6]: 通过软适配器混合实现音频频谱变换的高效微调

    Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters

    [https://arxiv.org/abs/2402.00828](https://arxiv.org/abs/2402.00828)

    本文研究了使用软适配器混合实现音频频谱变换的高效微调，证明了该方法在音频和语音任务中的优越性能。

    

    混合专家（MoE）架构近年来开始兴起，因为它们能够在保持计算成本可承受的情况下扩展模型容量。此外，它们可以应用于变换器和状态空间模型，这些是当前众多领域中颇有成就的模型。虽然MoE主要用于预训练阶段，但其在参数高效的迁移学习设置中的应用尚未得到充分探索。为了填补这一差距，本文试图揭示使用MoE进行参数高效音频频谱变换微调到音频和语音下游任务的方法。具体而言，我们提出了软适配器混合（Soft-MoA）方法。它使用适配器作为专家，并利用最近的软MoE方法，在输入记号和专家之间进行软分配，以保持计算时间有限。对4个基准任务的大量实验证明，Soft-MoA优于单一适配器方法，并在性能上表现出色。

    Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable. Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored. To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on pa
    
[^7]: SLIM: 多判别器在技能学习中的应用

    SLIM: Skill Learning with Multiple Critics

    [https://arxiv.org/abs/2402.00823](https://arxiv.org/abs/2402.00823)

    SLIM是一种多判别器学习方法，通过在机器人操作中组合多个判别器的奖励函数，显著改善了潜变量技能发现，克服了奖励之间的干扰。

    

    自我监督的技能学习旨在获取利用环境的底层动态的有用行为。基于互信息最大化的潜变量模型在此任务中取得了显著的成功，但在机器人操作领域仍存在困难。由于机器人操作可能涉及到环境中很多自由度，单纯的互信息最大化无法产生有用的操作行为。为了解决这个问题，我们引入了SLIM，一种针对机器人操作的多判别器学习方法。我们的主要观点是，在演员-评论者框架中利用多个判别器来优雅地组合多个奖励函数，能够显著改善机器人操作的潜变量技能发现，同时克服奖励之间可能发生的干扰，阻碍对有用技能的收敛。

    Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop man
    
[^8]: WiOpen: 一种稳健的基于Wi-Fi的开放式手势识别框架

    WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework

    [https://arxiv.org/abs/2402.00822](https://arxiv.org/abs/2402.00822)

    WiOpen是一种稳健的基于Wi-Fi的开放式手势识别框架，通过消除不确定性和定义精确的决策边界来解决在实际应用中未见手势被错误分类的问题。

    

    近年来，基于Wi-Fi的手势识别引起了广泛关注。然而，现有的工作主要集中在封闭集范式上，其中所有测试手势在训练期间都是预定义的。这在实际应用中存在重大挑战，因为在测试中，未见过的手势可能会被错误分类为已知类别。为了解决这个问题，我们提出了WiOpen，一种稳健的基于Wi-Fi的开放式手势识别框架。实现开放式手势识别需要解决Wi-Fi感测中的不确定性带来的挑战。这种不确定性由噪声和领域引起，在收集的Wi-Fi感测数据中导致了广泛散布和不规则的数据分布。因此，类别之间的数据模糊以及定义适当的决策边界以识别未知手势的挑战也随之出现。为了解决这些挑战，WiOpen采用了一个双重方法来消除不确定性并定义精确的决策边界。

    Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncerta
    
[^9]: 在连续环境中利用近似基于模型的屏蔽技术实现概率安全保证

    Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments

    [https://arxiv.org/abs/2402.00816](https://arxiv.org/abs/2402.00816)

    本文提出了在连续环境中利用近似基于模型的屏蔽技术实现概率安全保证的方法，并通过与约束强化学习算法的对比实验证明了其通用性和稳定性。

    

    屏蔽技术是实现安全增强学习的一种流行技术。然而，传统的屏蔽方法存在相当严格的假设，使其难以在复杂环境中部署，特别是在具有连续状态或行动空间的环境中。本文将更通用的近似基于模型的屏蔽（AMBS）框架扩展到连续环境中。我们使用Safety Gym作为我们的测试平台，可以更直接地将AMBS与流行的约束强化学习算法进行比较。我们还为连续环境提供了强大的概率安全保证。此外，我们提出了两种新颖的惩罚技术，直接修改策略梯度，在我们的实验中实现了更稳定的收敛。

    Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.
    
[^10]: 在协作装配场景中，探索Cobot的生产节奏、控制定位和情绪状态之间的动态关系

    Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario

    [https://arxiv.org/abs/2402.00808](https://arxiv.org/abs/2402.00808)

    本研究探索了在协作装配场景中，Cobot的生产节奏对参与者的经验性定位控制和情绪状态的影响。虽然在情绪状态和定位控制方面未发现差异，但考虑到其他心理变量，结果表明需要考虑个体的心理特征来提供更好的互动体验。

    

    在工业场景中，协作机器人（cobots）被广泛使用，并且对评估和测量cobots的某些特征对人的影响产生了日益增长的兴趣。在本次试点研究中，研究了一个cobots的生产节奏（C1-慢、C2-快、C3-根据参与者的步调调整）对31名参与者的经验性定位控制（ELoC）和情绪状态的影响。还考虑了操作人员的绩效、基本内部定位控制程度和对机器人的态度。关于情绪状态和ELoC，在三种条件下没有发现差异，但考虑到其他心理变量，出现了更复杂的情况。总体而言，结果似乎表明需要考虑个体的心理特征，以提供不同和最佳的互动体验。

    In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor. In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined. The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered. No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges. Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience.
    
[^11]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^12]: LLMs学习动力系统的控制原理，揭示了上下文中的神经比例定律

    LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law

    [https://arxiv.org/abs/2402.00795](https://arxiv.org/abs/2402.00795)

    本文研究了预训练语言模型LLMs对动力系统行为的外推能力，发现LLaMA 2能够准确预测动力系统的时间序列。此外，输入上下文窗口的长度越长，学习到的物理规则的准确性越高，揭示了一种上下文中的神经比例定律。

    

    预训练的大型语言模型（LLMs）在零-shot任务，包括时间序列预测方面表现出惊人的有效性。然而，由于模型的复杂性，理解其背后的机制仍然极具挑战性。本文研究了LLMs对受物理原理控制的动力系统行为的外推能力。我们的结果表明，主要在文本上进行训练的语言模型LLaMA 2在没有微调或提示工程的情况下，能够准确预测动力系统的时间序列。此外，学习到的物理规则的准确性随着输入上下文窗口的长度增加而增加，揭示了一种上下文中的神经比例定律。同时，我们还提出了一种灵活高效的算法，用于直接从LLMs中提取多位数的概率密度函数。

    Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
    
[^13]: ReAGent: 一个面向生成语言模型的模型无关特征归因方法

    ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models

    [https://arxiv.org/abs/2402.00794](https://arxiv.org/abs/2402.00794)

    本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。

    

    特征归因方法（FAs），如梯度和注意力机制，被广泛应用于确定所有输入特征对模型预测的重要性。现有自然语言处理领域的研究主要集中在为仅有编码器的语言模型（LMs）开发和测试FAs，用于分类任务。然而，尚不清楚在文本生成上是否可以使用这些FAs来处理仅有解码器的模型，因为模型架构和任务设置之间存在固有差异。此外，先前的研究已经证明，没有一个通用的FA适用于所有模型和任务。这使得针对大型LMs选择FA计算上非常昂贵，因为输入重要性的推导通常需要多个前向和反向传递，包括可能是限制性的梯度计算。为了解决这些问题，我们提出了一种面向生成LMs的模型无关FA，称为递归归因生成器（ReAGent）。

    Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
    
[^14]: 无法区分的区分：算法预测中的人类专业知识

    Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction

    [https://arxiv.org/abs/2402.00793](https://arxiv.org/abs/2402.00793)

    本论文介绍了一种新的框架，将人类专业知识纳入算法预测中，重点在于利用人的判断力区分对于任何可行的预测算法来说“看起来相同”的输入。

    

    我们引入了一种将人类专业知识纳入算法预测的新框架。我们的方法主要关注利用人的判断力来区分那些对于任何可行的预测算法来说“看起来相同”的输入。我们认为，这种框架能够澄清人工智能与人类协作预测任务中的问题，因为专家通常具有信息的访问权限——特别是主观信息——而这些信息是算法训练数据中没有编码的。基于这一认识，我们开发了一组有原则的算法，仅在任何可行的预测器的性能有所改善时才选择性地纳入人类反馈。经验结果表明，尽管算法在平均水平上往往优于人类对应任务的能力，但人类判断在特定情况下（可以预先确定）能够显著提高算法预测的性能。在一个X射线分类任务中，我们发现这个子集在患者群体中占据了近30%。我们的方法提供了一种自然的方式，

    We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
    
[^15]: Graph-Mamba: 通过选择性状态空间进行长程图序列建模

    Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces

    [https://arxiv.org/abs/2402.00789](https://arxiv.org/abs/2402.00789)

    Graph-Mamba是第一个尝试通过将Mamba模块与输入相关的节点选择机制集成来增强图网络中长程上下文建模的方法。

    

    注意力机制在图变换器中广泛用于捕捉节点之间的长程依赖关系。由于二次计算成本的限制，注意力机制在大型图中无法扩展。最近的计算效率改进主要通过使用随机或基于启发式的图子采样进行注意力稀疏化实现，但在数据相关的上下文推理方面效果不佳。状态空间模型（SSM）（如Mamba）因其在序列数据中建模长程依赖关系的效果和效率而受到关注。然而，将SSM适应非序列图数据是一个显著的挑战。在这项工作中，我们介绍了图-Mamba，这是第一个通过将Mamba模块与基于输入的节点选择机制集成，以增强图网络中的长程上下文建模的尝试。具体而言，我们制定了以图为中心的节点优先级和排列策略来增强上下文感知推理，从而实现了实质性的效果改进。

    Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantia
    
[^16]: 构建富有表现力和可处理的概率生成模型：一项综述

    Building Expressive and Tractable Probabilistic Generative Models: A Review

    [https://arxiv.org/abs/2402.00759](https://arxiv.org/abs/2402.00759)

    本文综述了富有表现力和可处理的概率生成建模领域的进展和技术，并重点关注了概率电路。文章提供了关于表达能力和可处理性之间权衡的统一视角，并说明了设计原则和算法扩展，成功地构建了富有表现力和高效的概率电路。此外，文章还讨论了最新的深度和混合概率电路研究，并概述了未来研究的挑战和开放性问题。

    

    我们对可处理的概率生成建模领域中的进展和技术进行了全面的调查，重点关注概率电路（PCs）。我们提供了关于表达能力和可处理性之间固有权衡的统一视角，突出了使PCs富有表现力和高效的设计原则和算法扩展，并提供了该领域的分类法。我们还讨论了最近通过融合深度神经模型概念来构建深度和混合PCs的努力，并概述了指导未来研究的挑战和开放性问题。

    We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
    
[^17]: 无法学习的算法用于上下文学习

    Unlearnable Algorithms for In-context Learning

    [https://arxiv.org/abs/2402.00751](https://arxiv.org/abs/2402.00751)

    本文提出了一种针对预先训练的大型语言模型的高效去学习方法，通过选择少量训练示例来实现任务适应训练数据的精确去学习，并与微调方法进行了比较和讨论。

    

    随着模型被越来越多地部署在未知来源的数据上，机器去学习变得越来越受欢迎。然而，要实现精确的去学习——在没有使用要遗忘的数据的情况下获得与模型分布匹配的模型——是具有挑战性或低效的，通常需要大量的重新训练。在本文中，我们专注于预先训练的大型语言模型（LLM）的任务适应阶段的高效去学习方法。我们观察到LLM进行任务适应的上下文学习能力可以实现任务适应训练数据的高效精确去学习。我们提供了一种算法，用于选择少量训练示例加到LLM的提示前面（用于任务适应），名为ERASE，它的去学习操作成本与模型和数据集的大小无关，意味着它适用于大型模型和数据集。我们还将我们的方法与微调方法进行了比较，并讨论了两种方法之间的权衡。这使我们得到了以下结论：

    Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to p
    
[^18]: 改变和组合奖励以对齐大规模语言模型

    Transforming and Combining Rewards for Aligning Large Language Models

    [https://arxiv.org/abs/2402.00742](https://arxiv.org/abs/2402.00742)

    本研究主要研究了对齐大规模语言模型的方法中出现的两个问题：奖励模型的选择以及多个奖励模型的组合。通过引入概率解释，我们提出了一种从Bradley-Terry偏好模型中学习的奖励的自然变换选择，该变换强调改善表现不佳的输出，从而减轻了欠拟合和奖励欺骗。

    

    将语言模型与人类偏好对齐的常见方法是首先从偏好数据中学习奖励模型，然后使用该奖励模型来更新语言模型。我们研究了这种方法中出现的两个密切相关的问题。首先，奖励模型的任何单调变换都保持偏好排名；是否有一种比其他选择“更好”的选择？其次，我们经常希望将语言模型与多个特性对齐：我们如何组合多个奖励模型？通过对齐过程的概率解释，我们确定了从Bradley-Terry偏好模型学习的奖励（常见情况）的自然变换选择。这个派生的变换具有两个重要的属性。首先，它强调改进表现不佳的输出，而不是已经得分良好的输出。这既减轻了欠拟合（其中一些提示没有得到改进），又减少了奖励欺骗（模型学习利用错误指定）。

    A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of
    
[^19]: FM3Q：分解的多智能体最小最大Q学习用于两个团队的零和马尔可夫游戏

    FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game

    [https://arxiv.org/abs/2402.00738](https://arxiv.org/abs/2402.00738)

    本论文提出了一种名为FM3Q的新型多智能体强化学习框架，采用个体全局最小最大原则，通过分解联合最小最大Q函数为个体函数，并解决了团队内信用分配、数据利用和计算复杂性不足的问题。

    

    许多现实世界的应用涉及到一些智能体分为两个团队，同一团队内的回报相等，而对手团队之间的回报则相反。这种所谓的两个团队零和马尔可夫游戏（2t0sMGs）可以在最近几年通过强化学习来解决。然而，现有方法在考虑团队内信用分配、数据利用和计算复杂性不足方面效率低下。在本文中，我们提出了个体全局最小最大（IGMM）原则，通过Q函数在2t0sMGs中确保两个团队最小最大行为和个体贪婪行为之间的一致性。基于此，我们提出了一种新型的多智能体强化学习框架，分解的多智能体最小最大Q学习（FM3Q），它可以将联合最小最大Q函数分解为个体函数，并迭代解出满足IGMM的2t0sMGs最小最大Q函数。此外，我们提出了一种使用神经网络的在线学习算法。

    Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is prop
    
[^20]: 基于双延迟DDPG的神经风格转移用于机器人操纵器的共享控制

    Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators

    [https://arxiv.org/abs/2402.00722](https://arxiv.org/abs/2402.00722)

    本论文提出了一种基于神经风格转移和TD3网络的共享控制方法，可以将多种风格应用于机器人操纵器的运动中。该方法通过使用自动编码器和双延迟深度确定性策略梯度网络来生成机器人控制策略，实现了机器人运动的风格转移。

    

    神经风格转移（NST）是一类能够使元素（通常为图像）采用另一个元素的外观或风格的算法。每个元素由内容和风格组成：内容可以概念上定义为元素的“是什么”，而风格则是元素的“如何”。在这个背景下，我们提出了一个定制的NST框架，用于将一组风格转移到机器人操纵器的运动中，例如，相同的机器人任务可以采用愤怒、快乐、平静或悲伤的方式执行。一个自动编码器架构提取和定义目标机器人运动的内容和风格。双延迟深度确定性策略梯度（TD3）网络使用自动编码器定义的损失生成机器人控制策略。所提出的神经策略风格转移TD3（NPST3）通过引入训练风格来改变机器人运动。这种方法可以在动态环境中以在线或离线的方式实现自主机器人运动。

    Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments
    
[^21]: 使用以意图漂移为引导的LLMs进行意图保证

    Intent Assurance using LLMs guided by Intent Drift

    [https://arxiv.org/abs/2402.00715](https://arxiv.org/abs/2402.00715)

    该论文介绍了一种使用以意图漂移为引导的LLMs进行意图保证的框架，通过利用自然语言模型生成的策略识别和处理意图漂移，以实现意图与业务目标的对齐。

    

    意图驱动的网络（IBN）为网络管理提供了范式转变，承诺以自动化的方式将意图和业务目标与网络操作对齐。然而，其实际实现具有挑战性：1）处理意图，即翻译、分解和识别实现意图的逻辑；2）意图一致性，即考虑到动态网络，逻辑应适当调整以保证意图。为了解决后者，意图保证负责持续验证和验证，包括采取必要行动使操作状态和目标状态保持一致。在本文中，我们定义了一个保证框架，允许我们在发生意图漂移时进行检测和采取行动。为此，我们利用了由大型语言模型（LLM）生成的基于AI驱动的策略，这些模型可以快速学习必要的上下文要求，并协助实现和保证意图。

    Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.
    
[^22]: 非交换的共形语言生成与最近邻

    Non-Exchangeable Conformal Language Generation with Nearest Neighbors

    [https://arxiv.org/abs/2402.00707](https://arxiv.org/abs/2402.00707)

    本文介绍了一种利用最近邻方法扩展的非交换的共形语言生成框架，用于量化自动生成文本的不确定性，并提供带有统计保证的预测集。

    

    在自动生成的文本中量化不确定性对于让人们检查潜在的错觉和使系统更可靠是很重要的。共形预测是一个有吸引力的框架，能够提供带有统计保证的预测，然而，将其应用于文本生成是具有挑战性的，因为任何独立同分布的假设都是不现实的。在本文中，我们通过利用最近关于非交换的共形预测的结果来填补这一差距，该方法仍然确保覆盖范围。结果--非交换的共形核采样，是对基于最近邻的生成的共形预测框架的一种新颖扩展。我们的方法可以用于任意模型的事后处理，无需额外训练，并提供带有统计保证的标记级别、校准的预测集。在机器翻译和语言建模的实验中，我们展示了令人鼓舞的生成质量结果。通过同时产生具有良好覆盖度的更紧密的预测集，

    Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove
    
[^23]: PeaTMOSS: 一个开源软件中预训练模型的数据集和初步分析

    PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software

    [https://arxiv.org/abs/2402.00699](https://arxiv.org/abs/2402.00699)

    本研究介绍了PeaTMOSS数据集，用于记录和分析开源软件中预训练模型的元数据和应用情况。这对于了解预训练模型的采用和重复使用的影响具有重要意义。

    

    深度学习模型的开发和训练变得越来越昂贵和复杂。因此，软件工程师正在采用预训练模型(PTMs)来进行后续应用。PTM供应链的动态仍然很少被探索，这表明需要结构化的数据集，不仅记录元数据，还记录这些模型的后续应用。没有这样的数据，MSR社区无法全面理解PTM的采用和重复使用的影响。本文提出了PeaTMOSS数据集，其中包括281,638个PTM的元数据和超过50个月下载量的所有PTM的详细快照(14,296个PTMs)，以及利用这些模型的28,575个来自GitHub的开源软件代码库。此外，数据集还包括15,129个GitHub代码库到它们使用的2,530个PTMs的44,337个映射。为了提高数据集的全面性，我们为一个大型语言模型开发了提示，以自动地进行摘要生成。

    The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatical
    
[^24]: 偶尔安全：代码生成辅助工具的比较分析

    Ocassionally Secure: A Comparative Analysis of Code Generation Assistants

    [https://arxiv.org/abs/2402.00689](https://arxiv.org/abs/2402.00689)

    本文通过比较分析四种先进的LLMs在9个任务上的表现，确定和理解了在真实场景中有效且安全地部署LLMs生成优质代码的条件和环境。

    

    大型语言模型(LLMs)在各种应用中的应用越来越广泛，代码生成就是一个显著的例子。以往的研究表明LLMs有能力生成安全和不安全的代码，但文献没有考虑到什么因素有助于生成安全和有效的代码。因此，本文重点是确定和理解在真实场景中LLMs能够有效和安全地部署来生成优质代码的条件和环境。我们对四个先进的LLMs进行了比较分析——使用ChatGPT和Bard的GPT-3.5和GPT-4，以及来自Google的Gemini——使用9个独立任务来评估每个模型的代码生成能力。我们将我们的研究置于一个典型的使用场景中，代表了开发人员在工作中使用LLMs进行日常任务的情况。此外，我们还强调了安全意识，通过使用我们开发的两个不同版本的工具来体现。

    $ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our develop
    
[^25]: 使用智能城市应用中的连续目标导向动作来实现真实评估的可追踪性

    Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications

    [https://arxiv.org/abs/2402.00678](https://arxiv.org/abs/2402.00678)

    以往的机器人编程方法对非专家用户不友好，这篇论文提出了使用连续目标导向动作来实现真实评估的方法，通过编码动作的特征变化来适应智能城市应用中的各种特征，并使用进化算法来计算机器人的关节轨迹。

    

    智能城市应用的一个重要挑战是使系统适应与非专家用户的交互。机器人模仿框架旨在通过允许用户通过示范直接编程来简化和减少机器人编程的时间。在传统的框架中，动作被建模为关节或笛卡尔空间轨迹。然而，使用这些纯几何方法并不能很好地表示其他特征，比如视觉特征。连续目标导向动作(CGDA)是这些方法的一种替代方案，它将动作编码为可以从环境中提取的任何特征的变化。为了满足这种无关特征的编码，机器人执行的关节轨迹必须完全计算。为了实现这一点，通常需要使用进化算法(EA)进行计算，但这通常需要过多的评估才能在实际机器人中进行这个进化步骤。目前的策略是在模拟中进行评估，然后将评估结果转移到实际机器人上。

    One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations. In classical frameworks, actions are modeled using joint or Cartesian space trajectories. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. Current strategies involve performing evaluations in a simulation, transferring 
    
[^26]: 神经策略风格转换

    Neural Policy Style Transfer

    [https://arxiv.org/abs/2402.00677](https://arxiv.org/abs/2402.00677)

    本研究提出了神经策略风格转换算法，通过深度强化学习来实现控制策略的风格转换。通过训练不同的网络来最大化预期奖励，同时编码了行为的目标和风格，从而将一个策略的风格转移到另一个策略而保持其内容不变。通过逆强化学习和用户演示实现模型的训练和风格的编码。

    

    风格转换已经在许多领域中被提出：美术、自然语言处理和固定轨迹。本文将这个概念扩展到了深度强化学习架构中的控制策略。每个网络都被训练成最大化预期的奖励，通常编码了一种行为的目标，可以描述为内容。深度神经网络的表达能力使得可以编码第二个任务，可以描述为风格。提出了神经策略风格转换（NPST）算法，用于将一个策略的风格转移到另一个策略，同时保持后者的内容。通过深度 Q-Network 架构定义了不同的策略。使用逆强化学习通过演示进行模型训练。进行了两组不同的用户演示，一组为内容，另一组为风格。不同的风格通过用户演示进行编码。生成的策略是通过将内容策略输送到一个生成器中来得到的。

    Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content poli
    
[^27]: 深度机器人素描：深度Q学习网络在人类素描中的应用

    Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching

    [https://arxiv.org/abs/2402.00676](https://arxiv.org/abs/2402.00676)

    本研究提出在艺术机器人应用中引入深度Q学习网络，旨在改进艺术机器人应用的控制策略。

    

    当前强化学习算法在复杂环境中的性能取得了巨大成功，这激发了许多最新的认知科学理论方法。艺术环境被认知科学界视为丰富、自然、多感官、多文化的环境。在这项工作中，我们提出使用强化学习改进艺术机器人应用的控制。深度Q学习神经网络（DQN）是在机器人中实现强化学习最成功的算法之一。DQN方法在各种环境中生成复杂的控制策略来执行复杂的机器人应用。当前的艺术绘画机器人应用使用简单的控制法则，限制了框架的适应性。本研究提出在艺术绘画机器人应用中引入DQN。目标是研究如何引入复杂的控制策略来改进艺术机器人应用的控制。

    The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control pol
    
[^28]: 探索用于无监督可见-红外人物重新识别的均质和异质一致标签关联

    Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID

    [https://arxiv.org/abs/2402.00672](https://arxiv.org/abs/2402.00672)

    该论文提出了一种同时考虑均质和异质实例级别结构，构建高质量跨模态标签关联的模态统一标签传输方法，用于无监督可见-红外人物重新识别。

    

    无监督可见-红外人物重新识别（USL-VI-ReID）旨在无需注释从不同模态中检索相同身份的行人图像。之前的研究侧重于建立跨模态的伪标签关联以弥合模态间的差异，但忽略了在伪标签空间中保持实例级别的均质和异质一致性，导致关联粗糙。为此，我们引入了一个模态统一标签传输（MULT）模块，同时考虑了均质和异质细粒度实例级结构，生成高质量的跨模态标签关联。它建模了均质和异质的关联性，利用它们定义伪标签的不一致性，然后最小化这种不一致性，从而维持了跨模态的对齐并保持了内部模态结构的一致性。此外，还有一个简单易用的在线交叉记忆标签引用模块。

    Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
    
[^29]: 通过收集轨迹和合成奖励来学习基于规划的推理

    Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing

    [https://arxiv.org/abs/2402.00658](https://arxiv.org/abs/2402.00658)

    本文提出了一种通过直接偏好优化在收集到的轨迹上学习基于规划的推理的框架，以解决大型语言模型在复杂推理任务中的虚幻和缺陷问题。

    

    大型语言模型（LLM）通过逐步合理化生成，展示了处理复杂推理任务的重要潜力。然而，最近的研究对它们的推理过程中的虚幻和缺陷提出了担忧。为了提高生成合理化的可靠性和忠实性，正在进行大量工作。有些方法将推理建模为规划，而其他方法则专注于注释的过程监督。然而，基于规划的搜索过程往往由于频繁评估中间推理状态和广泛的探索空间而导致高延迟。此外，使用人工注释监督推理过程对于LLM训练来说是昂贵且具有挑战性的。为了解决这些问题，我们在本文中提出了一种通过直接偏好优化（DPO）来学习基于规划的推理的框架，其中轨迹直接根据合成的过程奖励进行排名。

    Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. 
    
[^30]: CapHuman: 在平行宇宙中捕捉你的瞬间

    CapHuman: Capture Your Moments in Parallel Universes

    [https://arxiv.org/abs/2402.00627](https://arxiv.org/abs/2402.00627)

    CapHuman是一个新框架，通过“编码然后学习对齐”的范式实现了可泛化的身份保留能力，用于在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。

    

    我们关注一种新颖的以人为中心的图像合成任务，即仅给定一个参考面部照片，期望能够在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。为了实现这一目标，我们认为我们的生成模型应具备以下有利特征：（1）对世界和人类社会有强大的视觉和语义理解，用于基本物体和人类图像的生成；（2）可泛化的身份保留能力；（3）灵活细粒度的头部控制。最近，大型预训练的文本到图像扩散模型展现了显著的结果，成为一个强大的生成基础。基于此，我们旨在释放预训练模型的上述两种能力。在这项工作中，我们提出了一个名为CapHuman的新框架。我们采用了“编码然后学习对齐”的范式，为新个体实现了可泛化的身份保留能力。

    We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align" paradigm, which enables generalizable identity preservation for new individuals without cum
    
[^31]: 人工合成的时间序列数据真的不如真实数据吗？

    Are Synthetic Time-series Data Really not as Good as Real Data?

    [https://arxiv.org/abs/2402.00607](https://arxiv.org/abs/2402.00607)

    本研究引入了InfoBoost，一种高度通用的跨领域数据合成框架，具备时间序列表示学习能力，并开发了一种基于合成数据的方法，可以实现超越真实数据训练的模型性能。此外，我们还训练了一个通用特征提取器，适用于所有时间序列数据。实验证明，我们的方法能够克服多个干扰源的影响，提高泛化能力。

    

    时间序列数据存在数据质量问题、偏见和脆弱性以及泛化问题。整合通用的数据合成方法有望提高泛化能力。然而，当前方法无法保证生成器的输出包含所有未见过的真实数据。在本文中，我们引入了InfoBoost -- 一种高度通用的跨领域数据合成框架，具备时间序列表示学习能力。我们开发了一种基于合成数据的方法，可以实现模型训练而无需真实数据，超越了使用真实数据训练的模型的性能。此外，我们基于合成数据训练了一个适用于所有时间序列数据的通用特征提取器。我们的方法克服了多个源的节奏信号、噪声干扰和超过采样窗口能力的长周期特征的干扰。通过实验证明，我们的非深度学习合成数据实现了

    Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem. Integrating universal data synthesis methods holds promise in improving generalization. However, current methods cannot guarantee that the generator's output covers all unseen real data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability. We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data. Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data. Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities. Through experiments, our non-deep-learning synthetic data enables 
    
[^32]: Sandra -- 基于描述和情境的神经符号推理器

    Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations

    [https://arxiv.org/abs/2402.00591](https://arxiv.org/abs/2402.00591)

    Sandra是一个神经符号推理器，通过将矢量表示与演绎推理相结合，利用本体论建立的向量空间进行推理。它基于描述和情境的本体设计模式，能够从一组事实中推断出所有可能的解释，并在实验中证明在不增加复杂性的情况下优于其他基准线的分类结果，并且具有可解释性和向量空间的可控性。

    

    本文介绍了Sandra，这是一个将矢量表示与演绎推理相结合的神经符号推理器。Sandra使用本体论建立了一个受限的向量空间，并在其中进行推理。推理器的几何特性使得它能够与神经网络结合起来，弥合了符号知识表达与神经网络之间的差距。Sandra基于描述和情境(DnS)本体设计模式，它是一种框架语义的形式化。给定一组事实(情境)，它能够推断出所有可能的透视图(描述)，为其提供一个合理的解释，即使在信息不完整的情况下也能做到。我们证明了我们的方法在DnS模型上是正确的。我们对两个不同任务及其标准基准进行了实验，证明了Sandra不增加复杂性的情况下：(i)优于所有基准线；(ii) 在分类过程中提供可解释性；(iii)对向量空间具有可控性。

    This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning. Sandra builds a vector space constrained by an ontology and performs reasoning over it. The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations. Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics. Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information. We prove that our method is correct with respect to the DnS model. We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is d
    
[^33]: BrainSLAM: 神经人群活动数据上的SLAM

    BrainSLAM: SLAM on Neural Population Activity Data

    [https://arxiv.org/abs/2402.00588](https://arxiv.org/abs/2402.00588)

    BrainSLAM是一种使用大鼠不同脑区的人群活动数据进行SLAM的方法，通过卷积神经网络解码速度和熟悉度信息，并使用吸引子网络进行路径积分和闭环检测，从而构建地图。

    

    同时定位与映射 (SLAM) 算法常用于机器人系统中，用于学习新环境的地图。大脑似乎也学习地图，但机制尚不清楚，并且如何从神经活动数据中推断这些地图也不明确。我们提出了BrainSLAM；一种仅使用从大鼠的三个脑区（海马体，前额皮质和顶叶皮质）同时记录的人群活动（局部场电位，LFP）数据进行SLAM的方法。该系统使用卷积神经网络 (CNN) 从大鼠在2D迷宫中导航时记录的神经局部场电位数据的小波图解来解码速度和熟悉度信息。CNN的输出驱动RatSLAM启发式架构，驱动执行路径积分的吸引子网络以及执行“闭环检测”（检测先前访问过的位置并纠正地图别名错误）的独立系统。这三个组件共同可以构建出地图。

    Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construc
    
[^34]: EE-Tuning:一种经济且可扩展的调整早期终止大型语言模型的解决方案

    EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models

    [https://arxiv.org/abs/2402.00518](https://arxiv.org/abs/2402.00518)

    该论文介绍了一种经济且可扩展的解决方案EE-Tuning，可以使用较少的计算资源和训练数据针对早期终止大型语言模型进行调整，通过性能优化和3D并行性实现卓越的训练效率。实验证实，即使在有限的训练预算下，也可以实现有效的早期终止LLM推理。

    

    本文介绍了EE-Tuning，一种轻量且经济实用的解决方案，可以训练/调整早期终止的大型语言模型（LLMs）。与完整参数的预训练常见方法不同，EE-Tuning通过在参数高效方式下增加额外的早期终止层，与任何预训练（可能是微调）的标准LLM相结合，从而大大降低了计算资源和训练数据的需求。我们通过广泛的性能优化和完全兼容3D并行性的可扩展性，实现了EE-Tuning的卓越训练效率。系统实验证实了EE-Tuning的有效性，证明了在有限的训练预算下可以实现有效的早期终止LLM推理。为了将早期终止LLMs推广到社区，我们在https://github.com/pan-x-c/EE-LLM上发布了EE-Tuning的源代码。

    This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.
    
[^35]: EXMOS: 通过多方面解释和数据配置来解释模型的导向

    EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data Configurations

    [https://arxiv.org/abs/2402.00491](https://arxiv.org/abs/2402.00491)

    本研究通过定量和定性研究发现，全局模型导向解释在数据配置过程中的指导作用不足，数据导向解释提高了对配置后系统变化的理解，两种解释类型的混合融合表现出最高的有效性。

    

    交互式机器学习系统中的解释可以帮助调试和改进预测模型。然而，各种全局的模型导向和数据导向的解释，在帮助领域专家检测和解决潜在数据问题以改进模型方面的有效性尚未探讨。本研究调查了数据导向和模型导向全局解释在支持医疗专家通过自动化和手动数据配置优化模型的系统中的影响。我们通过定量（n = 70）和定性（n = 30）研究与医疗专家一起探索不同解释对信任、可理解性和模型改进的影响。我们的结果显示，在数据配置过程中，全局模型导向的解释不足以指导用户。虽然数据导向的解释增加了对配置后系统变化的理解，但两种解释类型的混合融合表现出最高的有效性。

    Explanations in interactive machine-learning systems facilitate debugging and improving prediction models. However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored. This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations. We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement. Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration. Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness. 
    
[^36]: 个性化的消费者和生产者群体公平优化的推荐系统框架

    A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems

    [https://arxiv.org/abs/2402.00485](https://arxiv.org/abs/2402.00485)

    这篇论文提出了一个个性化推荐系统框架，通过优化算法实现了消费者和生产者两方的公平性约束。该框架具有可泛化性和灵活性，可以根据不同的群体分割、推荐模型选择和领域设置实现公平性优化。

    

    近年来，人们越来越意识到，当机器学习算法用于自动化决策时，可能会对个人或群体进行不公平待遇，从而产生法律、伦理或经济方面的影响。推荐系统是这些机器学习系统的重要例子，它们帮助用户做出决策。过去大部分关于推荐系统公平性的文献研究都是将用户和物品的公平性问题独立对待，忽视了推荐系统在双边市场中的作用。在本文中，我们提出了CP-FairRank，这是一种基于优化的重新排名算法，在联合目标框架中无缝集成了消费者和生产者两方的公平性约束。该框架具有可泛化性，并可以根据群体分割、推荐模型选择和领域考虑多样的公平性设置，这是其一个重要特点。例如，我们展示了该系统可以同时提高消费者和生产者的满意度，并在实验中取得了良好的结果。

    In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications. Recommender systems are prominent examples of these machine learning (ML) systems that aid users in making decisions. The majority of past literature research on RS fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace. In this paper, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework. The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics. For instance, we demonstrate that the system may jointly increase consum
    
[^37]: SA-MDKIF：一种可扩展和适应性强的大型语言模型医学领域知识注入框架

    SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models

    [https://arxiv.org/abs/2402.00474](https://arxiv.org/abs/2402.00474)

    SA-MDKIF是一种可扩展和适应性强的医学领域知识注入框架，通过指令调整并训练医学技能，并在推理中将其与大型语言模型集成，提高了医学任务的性能。

    

    大型语言模型(LLMs)的最新进展在各种自然语言处理(NLP)任务中展现出了卓越的性能。然而，它们在医学领域的有效应用受到医学领域知识的缺乏的限制。在本研究中，我们提出了一种可扩展和适应性强的框架SA-MDKIF，旨在通过指令调整将医学知识注入通用型LLMs中，从而实现对各种下游任务的适应性。SA-MDKIF包括两个阶段：技能训练和技能适应。在第一阶段，我们定义了12种基本的医学技能，并使用AdaLoRA根据我们构建的统一格式的指令数据集来训练这些技能。在下一个阶段，我们使用特定任务的下游数据来训练技能路由器，并在推理过程中使用该路由器将获取的技能与LLMs集成。对9个不同的医学任务的实验结果显示，与原始模型相比，SA-MDKIF的性能提高了10-20％。

    Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the origina
    
[^38]: RadDQN: 一种基于深度 Q 学习的架构，用于寻找时间高效的最小辐射暴露路径

    RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum Radiation Exposure Pathway

    [https://arxiv.org/abs/2402.00468](https://arxiv.org/abs/2402.00468)

    本文提出了一种基于深度 Q 学习的架构 RadDQN，在辐射区域中提供时间高效的最小辐射暴露路径。通过使用辐射感知奖励函数和独特的探索策略，该架构可以优化人员辐射暴露，为自主无人机领域带来辐射防护的创新解决方案。

    

    最近深度强化学习 (DRL) 技术的进步引发了其在自动化领域的多方面应用。在核工业中使用 DRL 处理复杂的决策问题鼓励其用于在正常运行和潜在事故情况下优化人员辐射暴露。然而，缺乏高效的奖励函数和有效的探索策略阻碍了其在开发辐射感知的自主无人机领域的实现，以实现最大限度的辐射防护。在本文中，我们解决了这些有趣的问题，并引入了一种基于深度 Q 学习的架构 (RadDQN)，该架构基于辐射感知奖励函数，在辐射区域中提供时间高效的最小辐射暴露路径。我们提出了一套独特的探索策略，根据各个状态的变化调整探索和利用的程度。

    Recent advancements in deep reinforcement learning (DRL) techniques have sparked its multifaceted applications in the automation sector. Managing complex decision-making problems with DRL encourages its use in the nuclear industry for tasks such as optimizing radiation exposure to the personnel during normal operating conditions and potential accidental scenarios. However, the lack of efficient reward function and effective exploration strategy thwarted its implementation in the development of radiation-aware autonomous unmanned aerial vehicle (UAV) for achieving maximum radiation protection. Here, in this article, we address these intriguing issues and introduce a deep Q-learning based architecture (RadDQN) that operates on a radiation-aware reward function to provide time-efficient minimum radiation-exposure pathway in a radiation zone. We propose a set of unique exploration strategies that fine-tune the extent of exploration and exploitation based on the state-wise variation in radi
    
[^39]: 基于遗传的约束编程在资源受限作业调度中的应用

    Genetic-based Constraint Programming for Resource Constrained Job Scheduling

    [https://arxiv.org/abs/2402.00459](https://arxiv.org/abs/2402.00459)

    本论文提出了一种基于遗传算法的约束编程方法来解决资源受限作业调度问题，该方法通过进化编程发现高效的搜索策略，并通过新的表示方法、适应度评估方案和预选择机制实现了创新。

    

    资源受限的作业调度是一种困难的组合优化问题，源自于采矿行业。通用求解器无法在合理的时间内令人满意地解决这个问题，而其他解法方法如许多进化计算方法和元启发式方法不能保证最优性，并且需要低级定制和专门的启发式技巧才能有效。本文通过提出一种基于遗传编程的算法来发现资源受限作业调度约束编程的高效搜索策略来填补这个缺口。在所提出的算法中，进化的程序代表了在约束编程的搜索过程中要使用的变量选择器，它们的适应度由针对训练实例获得的解质量来确定。该算法的创新点有：（1）新的变量选择器表示方法，（2）新的适应度评估方案，和（3）预选择机制。

    Resource constrained job scheduling is a hard combinatorial optimisation problem that originates in the mining industry. Off-the-shelf solvers cannot solve this problem satisfactorily in reasonable timeframes, while other solution methods such as many evolutionary computation methods and matheuristics cannot guarantee optimality and require low-level customisation and specialised heuristics to be effective. This paper addresses this gap by proposing a genetic programming algorithm to discover efficient search strategies of constraint programming for resource-constrained job scheduling. In the proposed algorithm, evolved programs represent variable selectors to be used in the search process of constraint programming, and their fitness is determined by the quality of solutions obtained for training instances. The novelties of this algorithm are (1) a new representation of variable selectors, (2) a new fitness evaluation scheme, and (3) a pre-selection mechanism. Tests with a large set of
    
[^40]: 双学生知识蒸馏网络用于无监督异常检测

    Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection

    [https://arxiv.org/abs/2402.00448](https://arxiv.org/abs/2402.00448)

    这篇论文介绍了一种用于无监督异常检测的双学生知识蒸馏网络，通过引入两个具有相同规模但结构相反的学生网络和一个单一预训练的教师网络，提高了对正常数据的识别一致性，并同时引入了异常表示的多样性。

    

    由于数据不平衡和缺陷的多样性，在无监督异常检测中，学生-教师网络（S-T）在探索由知识蒸馏过程得出的特征表示中的差异以识别异常方面得到了青睐。然而，传统的S-T网络不够稳定。采用相同的结构构建S-T网络可能会削弱对异常的代表性差异。但使用不同的结构可以增加在正常数据上产生差异性性能的可能性。为了解决这个问题，我们提出了一种新颖的双学生知识蒸馏（DSKD）架构。与其他S-T网络不同，我们使用两个具有相同规模但结构相反的学生网络和一个单一预训练的教师网络。该框架可以增强蒸馏效果，提高对正常数据的识别一致性，并同时引入异常表示的多样性。

    Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semanti
    
[^41]: 数据高效图学习的综述

    A Survey of Data-Efficient Graph Learning

    [https://arxiv.org/abs/2402.00447](https://arxiv.org/abs/2402.00447)

    这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。

    

    图结构化数据在社交网络到生物化学分析等领域中广泛存在，是各种现实世界系统的基础。虽然图神经网络在建模这种数据方面表现出色，但它们的成功往往依赖于大量标记数据，这在标注资源有限的实际场景中构成了挑战。为了解决这个问题，我们致力于通过探索各种最小监督方法来提高低资源设置下的图机器学习性能。本文介绍了一种新颖的数据高效图学习(DEGL)的研究前沿，并提供了对DEGL当前进展的首次综述。我们首先强调了使用大规模标记数据训练模型所固有的挑战，为我们对DEGL的探索铺平了道路。接下来，我们从几个关键方面系统地回顾了这一主题的最新进展，其中包括...

    Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
    
[^42]: 基于卷积自编码器的降阶模型的实用存在定理

    A practical existence theorem for reduced order models based on convolutional autoencoders

    [https://arxiv.org/abs/2402.00435](https://arxiv.org/abs/2402.00435)

    本论文提出了基于卷积自编码器的降阶模型的实用存在定理，解决了在处理复杂非线性问题方面传统方法的不足，并讨论了如何学习潜在特征的挑战。

    

    近年来，深度学习在偏微分方程和降阶建模领域越发受欢迎，提供了基于物理知识的神经网络、神经算子、深度算子网络和深度学习降阶模型等强大的数据驱动技术。在这种情况下，基于卷积神经网络的深度自编码器表现出极高的效果，在处理复杂的非线性问题时，优于传统的降阶方法。然而，尽管基于CNN的自编码器在实践中取得了成功，但目前只有少数理论结果支持这些架构，通常以万能逼近定理的形式陈述。尤其是，尽管现有文献为设计卷积自编码器提供了指导方针，但学习潜在特征的后续挑战几乎没有被探究。

    In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems. However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems. In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely inv
    
[^43]: 使用大型语言模型实现基于提示时间的符号知识捕获

    Prompt-Time Symbolic Knowledge Capture with Large Language Models

    [https://arxiv.org/abs/2402.00414](https://arxiv.org/abs/2402.00414)

    本文研究利用大型语言模型来实现基于提示驱动的知识捕获，特别关注提示到三元组的生成，并通过专门的合成数据集评估了三种方法的性能。

    

    为了实际应用，如个人AI助手，将大型语言模型（LLMs）与用户特定的知识相结合至关重要。然而，LLMs本身缺乏基于提示驱动的知识捕获机制。本文研究利用现有的LLMs能力实现基于提示驱动的知识捕获，特别关注知识图谱。我们通过关注提示到三元组（P2T）生成来应对这个挑战。我们探索了零射距、少射距和微调三种方法，并通过一个专门的合成数据集对它们的性能进行评估。我们的代码和数据集可以在https://github.com/HaltiaAI/paper-PTSKC上公开获取。

    Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants. However, LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs. We address this challenge by focusing on prompt-to-triple (P2T) generation. We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.
    
[^44]: 隐藏代笔者：对AI生成的学生论文检测进行对抗评估

    Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection

    [https://arxiv.org/abs/2402.00412](https://arxiv.org/abs/2402.00412)

    本文旨在对抗评估AI生成的学生论文检测，通过构建AIG-ASAP数据集和使用文本扰动方法，揭示现有检测器易被自动对抗攻击所绕过的问题。

    

    大型语言模型(LLM)在文本生成任务中展示了出色的能力。然而，利用这些模型存在固有的风险，包括但不限于抄袭、传播假新闻以及教育习题中的问题。尽管已有几种检测器被提出来解决这些问题，但它们在对抗扰动方面的效果，特别是在学生论文写作的背景下，仍然被较少探索。本文旨在填补这一空白，通过构建AIG-ASAP，一个基于AI生成的学生论文数据集，采用一系列预计能生成高质量论文的文本扰动方法，同时躲避检测。通过实证实验，我们评估了当前AIGC检测器在AIG-ASAP数据集上的性能。结果表明，现有的检测器可以很容易地被直接的自动对抗攻击所规避。具体来说，我们探索了词替换和句子替换方法。

    Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitu
    
[^45]: LM-HT SNN: 通过可学习的多层次阈值模型增强SNN与ANN的性能对应关系

    LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model

    [https://arxiv.org/abs/2402.00411](https://arxiv.org/abs/2402.00411)

    本文通过提出LM-HT模型，一个可学习的多层次阈值模型，增强了脉冲神经网络（SNN）与人工神经网络（ANN）的性能对应关系。

    

    与传统的人工神经网络（ANN）相比，脉冲神经网络（SNN）因其更具生物启发和能量效率的信息传递能力而引起了广泛的学术兴趣。然而，尽管之前通过各种方法对SNN的学习梯度和模型结构进行了优化，但在性能方面SNN仍然在一定程度上落后于ANN。最近提出的多阈值模型为进一步增强SNN的学习能力提供了更多可能性。在本文中，我们从数学的角度严格分析了多阈值模型、原始脉冲模型和量化ANN之间的关系，然后提出了一种新的LM-HT模型，这是一个等距多层次模型，可以在时间维度上动态调节全局输入电流和膜电位泄漏。此外，我们指出基于LM-HT模型的直接训练算法可以无缝地连接两个阶段的学习。

    Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessl
    
[^46]: 通过激活导向研究 Llama 2 Chat 中的偏见表示

    Investigating Bias Representations in Llama 2 Chat via Activation Steering

    [https://arxiv.org/abs/2402.00402](https://arxiv.org/abs/2402.00402)

    我们通过激活导向技术研究了Llama 2 Chat中的偏见表示问题，发现该模型存在固有的性别偏见，并观察到偏见与模型拒绝回应的倾向之间存在负相关关系。

    

    我们解决了大型语言模型（LLMs）中的社会偏见挑战，重点关注 Llama 2 7B Chat 模型。随着 LLMs 被越来越多地整合到具有重大社会影响的决策过程中，确保这些模型不会强化现有的偏见变得至关重要。我们的方法采用激活导向技术来探测和减轻与性别、种族和宗教有关的偏见。该方法通过操纵模型的激活来指导回应朝向或远离有偏见的输出，利用从StereoSet数据集和自定义GPT4生成的性别偏见提示得到的导向向量。我们的研究结果揭示了Llama 2 7B Chat中固有的性别偏见，即使在通过人类反馈的强化学习之后仍然存在。我们还观察到偏见与模型拒绝回应的倾向之间存在可预测的负相关关系。值得注意的是，我们的研究揭示了强化学习从人类反馈中 tend 趋向增加模型对不同形式社会偏见的表示的相似性。

    We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal bia
    
[^47]: 跨城市少样本交通预测的多尺度交通模式库

    Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting

    [https://arxiv.org/abs/2402.00397](https://arxiv.org/abs/2402.00397)

    我们提出了一种跨城市少样本交通预测的解决方案，利用多尺度交通模式库从数据丰富的源城市学习并预测其他城市的交通情况。

    

    交通预测对智能交通系统具有重要意义，可以帮助高效分配资源和有效控制交通。然而，其有效性往往严重依赖于丰富的交通数据，而许多城市由于设备支持有限而缺乏足够的数据，这对交通预测构成了重大挑战。鉴于这一挑战，我们做出了一个显著的观察：交通模式在不同城市之间存在相似性。基于这一关键洞察，我们提出了一种解决跨城市少样本交通预测问题的方法，称为多尺度交通模式库（MTPB）。主要上，MTPB通过利用数据丰富的源城市启动其学习过程，通过空间-时间感知的预训练过程有效获取全面的交通知识。随后，该框架采用先进的聚类技术从学习到的知识中系统生成一个多尺度交通模式库。接下来，该框架使用准确的交通模式检索机制进行跨城市的少样本交通预测。

    Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, th
    
[^48]: LLMs的高效探索

    Efficient Exploration for LLMs

    [https://arxiv.org/abs/2402.00396](https://arxiv.org/abs/2402.00396)

    高效探索在改善大型语言模型方面具有显著优势，可以以较少的查询实现较高的性能水平。不确定性估计和探索方案的选择是关键因素。

    

    我们提供了证据，表明高效探索在获取人类反馈以改善大型语言模型方面具有显著优势。在我们的实验中，一个代理程序在收到反馈时将奖励模型拟合到查询上。我们表现最佳的代理程序使用双Thompson采样生成查询，不确定性由认知神经网络表示。我们的结果表明，高效探索使得性能水平可以在较少的查询下达到较高水平。此外，不确定性估计和探索方案的选择起着关键作用。

    We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
    
[^49]: 考虑死区的神经网络损失函数

    Loss Function Considering Dead Zone for Neural Networks

    [https://arxiv.org/abs/2402.00393](https://arxiv.org/abs/2402.00393)

    本文提出了一种新的损失函数，考虑机械手臂死区内的逆动力学计算。该方法能够提高训练可用的运动数据量，并提高逆动力学计算的准确性。

    

    揭示机械手臂的逆动力学是提高基于模型控制的控制性能的重要任务。神经网络是表示复杂逆动力学的有望技术，但需要大量的运动数据。然而，在执行机构的死区中的运动数据不适合用于模型训练，这降低了可用于训练的有用数据量。本研究基于机械手臂关节在死区内不工作的事实，提出了一种新的损失函数，只考虑不在死区的关节的误差。该方法使得可用于训练的运动数据量增加，并提高了逆动力学计算的准确性。通过使用三自由度机械手臂的实际设备进行的实验表明，该方法的准确性优于传统方法。我们还验证并讨论了在死区中使用所提出方法的模型的行为。

    It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control. Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data. However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data. In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones. The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation. Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods. We also confirmed and discussed the behavior of the model of the proposed method in dead zones.
    
[^50]: EASRec：用于高效长期顺序推荐系统的弹性架构搜索

    EASRec: Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems

    [https://arxiv.org/abs/2402.00390](https://arxiv.org/abs/2402.00390)

    EASRec是一个针对顺序推荐系统的弹性架构搜索方法，通过自动剪枝技术和先进模型架构结合，以及资源受限神经架构搜索技术，实现了降低计算成本和资源消耗的同时保持或增强准确性。

    

    在数据丰富的时代，从海量信息中提取有意义的见解的能力至关重要。我们的研究解决了当前顺序推荐系统（SRSs）在计算和资源效率方面存在的问题，特别是那些采用了基于注意力模型（如SASRec）的系统。这些系统旨在为各种应用提供下一个项目的推荐，从电子商务到社交网络。然而，这些系统在推理阶段会产生相当大的计算成本和资源消耗。为了解决这些问题，我们的研究提出了一种结合自动剪枝技术和先进模型架构的新方法。我们还探索了在推荐系统领域中流行的资源受限神经架构搜索（NAS）技术的潜力，以调整模型以减少FLOPs、延迟和能量使用，同时保持或增强准确性。我们的工作的主要贡献是开发了一种

    In this age where data is abundant, the ability to distill meaningful insights from the sea of information is essential. Our research addresses the computational and resource inefficiencies that current Sequential Recommender Systems (SRSs) suffer from. especially those employing attention-based models like SASRec, These systems are designed for next-item recommendations in various applications, from e-commerce to social networks. However, such systems suffer from substantial computational costs and resource consumption during the inference stage. To tackle these issues, our research proposes a novel method that combines automatic pruning techniques with advanced model architectures. We also explore the potential of resource-constrained Neural Architecture Search (NAS), a technique prevalent in the realm of recommendation systems, to fine-tune models for reduced FLOPs, latency, and energy usage while retaining or even enhancing accuracy. The main contribution of our work is developing 
    
[^51]: 关于RMSProp及其动量扩展方法的$O(\frac{\sqrt{d}}{T^{1/4}})$收敛速度和对维度的改进依赖性

    On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the Dimension

    [https://arxiv.org/abs/2402.00389](https://arxiv.org/abs/2402.00389)

    这项研究探讨了RMSProp及其动量扩展方法的收敛速度，并发现使用$\ell_1$范数测度时，收敛速度为$O(\frac{\sqrt{d}}{T^{1/4}})$，在维度极大的问题中具有改进依赖性。

    

    尽管自适应梯度方法在深度学习中被广泛使用，但其收敛速度尚未得到彻底研究，特别是对于其对维度的依赖性。本文考虑了经典的RMSProp及其动量扩展方法，并通过$\ell_1$范数建立了收敛率$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$，无需假设梯度有界，其中$d$是优化变量的维度，$T$是迭代次数。由于对于维度极大的问题，$\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$，因此我们的收敛速度可以类比为SGD的$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$，测度为$\ell_1$范数。

    Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$ measured by $\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$ one of SGD measured by $\ell_1$ norm.
    
[^52]: 基于累积分布函数的通用时间点过程

    Cumulative Distribution Function based General Temporal Point Processes

    [https://arxiv.org/abs/2402.00388](https://arxiv.org/abs/2402.00388)

    本研究引入了CuFun模型，基于累积分布函数的通用时间点过程，解决了深度时间点过程模型中的强度函数建模、积分计算复杂性和长期时序依赖性捕捉的问题。

    

    时间点过程在建模各个领域中的事件序列（包括社交网络和电子商务）中发挥着关键作用，并对推荐系统和信息检索策略的进展做出了重大贡献。通过分析用户交互和交易等事件，时间点过程提供了有价值的行为模式洞察，有助于预测未来的趋势。然而，由于这些模式的复杂性，准确预测未来事件仍然是一个巨大挑战。将神经网络与时间点过程相结合，开发了先进的深度时间点过程模型。虽然这些模型在处理复杂和非线性时间数据方面表现出色，但在建模强度函数、复杂积分计算和有效捕捉长期时序依赖方面存在局限性。在本研究中，我们介绍了CuFun模型，该模型代表了一种新的方法来解决这些问题。

    Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends. However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models. While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. In this study, we introduce the CuFun model, representing
    
[^53]: 使用不变扩展卡尔曼滤波器和神经测量网络进行的腿式机器人状态估计

    Legged Robot State Estimation With Invariant Extended Kalman Filter Using Neural Measurement Network

    [https://arxiv.org/abs/2402.00366](https://arxiv.org/abs/2402.00366)

    本文介绍了一种新型的基于模型的滤波器和深度神经网络结合的腿式机器人状态估计器。我们通过将神经测量网络与不变扩展卡尔曼滤波器结合，提高了状态估计的性能。与现有的研究不同的是，我们的方法仅使用仿真数据，通过调整学习技术和正则化来缩小模拟到真实的差距。

    

    本文介绍了一种新型的基于模型的滤波器和深度神经网络结合的腿式机器人本体感知状态估计器。最近的研究表明，如多层感知器或循环神经网络等神经网络可以估计机器人状态，包括接触概率和线速度。受此启发，我们开发了一种状态估计框架，将神经测量网络(NMN)与不变扩展卡尔曼滤波器相结合。我们证明了我们的框架在不同地形下改善了估计性能。现有的基于模型的滤波器和基于学习的方法的研究通常使用真实世界的数据。然而，我们的方法仅依赖于仿真数据，因为它可以轻松获得大量数据。这种差异导致了学习和推理域之间的差距，通常被称为模拟到真实差距。我们通过调整现有的学习技术和正则化来解决这一挑战。为了验证我们的方法，我们进行了广泛的仿真和实验，

    This paper introduces a novel proprioceptive state estimator for legged robots that combines model-based filters and deep neural networks. Recent studies have shown that neural networks such as multi-layer perceptron or recurrent neural networks can estimate the robot states, including contact probability and linear velocity. Inspired by this, we develop a state estimation framework that integrates a neural measurement network (NMN) with an invariant extended Kalman filter. We show that our framework improves estimation performance in various terrains. Existing studies that combine model-based filters and learning-based approaches typically use real-world data. However, our approach relies solely on simulation data, as it allows us to easily obtain extensive data. This difference leads to a gap between the learning and the inference domain, commonly referred to as a sim-to-real gap. We address this challenge by adapting existing learning techniques and regularization. To validate our p
    
[^54]: 深度学习揭示了热带气旋强度和能量极值的气候趋势

    Climate Trends of Tropical Cyclone Intensity and Energy Extremes Revealed by Deep Learning

    [https://arxiv.org/abs/2402.00362](https://arxiv.org/abs/2402.00362)

    本研究利用深度学习方法重建了过去的热带气旋观测，并获得了全球热带气旋风场数据，揭示了热带气旋强度和能量的气候趋势。

    

    人类活动已与热带气旋向极地迁移、气旋极端降水以及重大飓风比例增加相关联。了解过去热带气旋的趋势和变化对于预测气候变化对人类社会的未来影响至关重要。然而，基于有限观测数据，过去的热带气旋结构和能量趋势仍存在不确定性。主观分析和时空异质的“最佳路径”数据集导致对气候变化对气旋响应的评估缺乏信心。本文利用深度学习重建了过去的“观测”，并在1981年至2020年期间获得了客观的全球热带气旋风场资料，有助于全面研究热带气旋的结构和能量。通过使用包含最佳路径和数值模型分析2004年至2018年热带气旋的独特标记数据进行训练，我们的模型将多通道卫星图像转换为0-750公里的轴对称地表风速风场。该模型通过比较复杂的深度神经网络提取特征的方法，可以更精确地区分不同的天气系统，包括台风强度、反射率、风速和气温等。

    Anthropogenic influences have been linked to tropical cyclone (TC) poleward migration, TC extreme precipitation, and an increased proportion of major hurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is critical for projecting future TC impacts on human society considering the changing climate [5]. However, past trends of TC structure/energy remain uncertain due to limited observations; subjective-analyzed and spatiotemporal-heterogeneous "best-track" datasets lead to reduced confidence in the assessed TC repose to climate change [6, 7]. Here, we use deep learning to reconstruct past "observations" and yield an objective global TC wind profile dataset during 1981 to 2020, facilitating a comprehensive examination of TC structure/energy. By training with uniquely labeled data integrating best tracks and numerical model analysis of 2004 to 2018 TCs, our model converts multichannel satellite imagery to a 0-750-km wind profile of axisymmetric surface winds. The model per
    
[^55]: 自适应原始-对偶方法用于安全强化学习

    Adaptive Primal-Dual Method for Safe Reinforcement Learning

    [https://arxiv.org/abs/2402.00355](https://arxiv.org/abs/2402.00355)

    本论文提出了一种自适应原始-对偶方法用于安全强化学习，通过调整自适应学习速率以优化策略，实现了算法的收敛性、最优性和可行性。实验结果表明，该方法在安全强化学习中具有更好的性能和稳定性。

    

    原始-对偶方法在安全强化学习中有自然应用，被提出作为一个约束策略优化问题。然而，在实践中，将原始-对偶方法应用于安全强化学习是具有挑战性的，因为每次解决嵌入的无约束强化学习问题时，学习速率（LR）和拉格朗日乘子（对偶变量）之间存在相互依赖。在本文中，我们提出、分析和评估了适应性原始-对偶（APD）方法用于安全强化学习，在每次迭代中，调整两个自适应LR以使之优化策略。我们从理论上建立了APD算法的收敛性、最优性和可行性。最后，我们使用Bullet-Safey-Gym中的四个知名环境，利用两个先进的安全强化学习算法（PPO-Lagrangian和DDPG-Lagrangian）对实际APD算法进行了数值评估。所有实验表明，实际APD算法的性能优于（或达到可比较的性能），并且具有更稳定的效果。

    Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable
    
[^56]: 基于大语言模型的模糊测试技术：一项综述

    Large Language Models Based Fuzzing Techniques: A Survey

    [https://arxiv.org/abs/2402.00350](https://arxiv.org/abs/2402.00350)

    这篇论文对基于大语言模型的模糊测试技术进行了综述，提供了对LLMs、模糊测试和基于LLMs的模糊测试方法的系统概述，并讨论了相关的挑战和未来的研究方向。

    

    在现代软件发挥关键作用的时代，软件安全和漏洞分析对于软件开发变得至关重要。作为一种高效的软件测试方法，模糊测试被广泛应用于各个领域。此外，大语言模型（LLMs）的快速发展使它们可以在软件测试领域中应用，并展现出卓越的性能。考虑到现有的模糊测试技术并不完全自动化，软件漏洞不断演化，越来越多的趋势是采用基于大语言模型生成的模糊测试。本综述提供了关于融合LLMs和模糊测试的软件测试方法的系统概述。通过总结2024年之前的最新方法，对LLMs、模糊测试和基于LLMs的模糊测试进行统计分析和讨论。我们的综述还研究了LLMs和模糊测试在软件测试领域的应用，并讨论了相关的挑战和未来的研究方向。

    In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. Fuzzing test, as an efficient software testing method, are widely used in various domains. Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing. In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024. Our survey also invest
    
[^57]: ODICE: 通过正交梯度更新揭示分布校正估计的奥秘

    ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update

    [https://arxiv.org/abs/2402.00348](https://arxiv.org/abs/2402.00348)

    ODICE研究了离线强化学习和模仿学习中重要的分布校正估计（DICE）方法，并发现在使用真梯度更新学习值函数时存在前向梯度和后向梯度两个梯度项。为了解决这个问题，他们提出了一种简单但有效的修正方法。

    

    在这项研究中，我们调查了分布校正估计（DICE）方法，这是离线强化学习（RL）和模仿学习（IL）中重要的研究方向。基于DICE的方法对状态行为级别的行为约束施加了，这对于离线学习是一个理想的选择。然而，它们通常比仅使用动作级别行为约束的当前最先进方法表现得更差。在重新审视了基于DICE的方法后，我们发现在使用真梯度更新学习值函数时存在两个梯度项：前向梯度（在当前状态上）和后向梯度（在下一个状态上）。使用前向梯度与许多离线RL方法有很大的相似之处，因此可以被视为应用动作级别约束。然而，如果这两个梯度有相互冲突的方向，直接加上后向梯度可能会退化或抵消其效果。为了解决这个问题，我们提出了一个简单但有效的修正方法。

    In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modi
    
[^58]: 多智能体路径规划用于协同自动驾驶

    Multi-agent Path Finding for Cooperative Autonomous Driving

    [https://arxiv.org/abs/2402.00334](https://arxiv.org/abs/2402.00334)

    本论文将多智能体路径规划和协同自动驾驶相结合，提出了优化交叉口驶入顺序的完整算法OBS-KATS，该算法在各种情况下都表现出了显著优势。

    

    随着联网和自动驾驶汽车（CAV）可能在未来的部署，控制理论和智能交通领域的许多研究对交叉口的协同自动驾驶进行了研究。同时，在机器人领域的最近并行工作中，已经开发出了用于多智能体路径规划（MAPF）的高效算法，尽管环境具有简化的运动学。在这项工作中，我们将MAPF的洞察和算法与优化CAV在无信号交叉口的交叉顺序的结构和启发式方法进行了混合。我们设计了一种优化和完整的算法，基于顺序的搜索与运动学到达时间调度（OBS-KATS），该算法在性能上显著优于现有的算法、固定的启发式方法和优先计划与KATS。该算法的性能在不同的车辆到达率、车道长度、交叉速度和控制范围下都得到了保持。通过消融和解剖分析，我们对OBS-KATS的贡献因素提供了见解。

    Anticipating possible future deployment of connected and automated vehicles (CAVs), cooperative autonomous driving at intersections has been studied by many works in control theory and intelligent transportation across decades. Simultaneously, recent parallel works in robotics have devised efficient algorithms for multi-agent path finding (MAPF), though often in environments with simplified kinematics. In this work, we hybridize insights and algorithms from MAPF with the structure and heuristics of optimizing the crossing order of CAVs at signal-free intersections. We devise an optimal and complete algorithm, Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which significantly outperforms existing algorithms, fixed heuristics, and prioritized planning with KATS. The performance is maintained under different vehicle arrival rates, lane lengths, crossing speeds, and control horizon. Through ablations and dissections, we offer insight on the contributing factors to O
    
[^59]: SCO-VIST: 基于社交互动常识的视觉故事创作

    SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling

    [https://arxiv.org/abs/2402.00319](https://arxiv.org/abs/2402.00319)

    SCO-VIST是一个基于社交互动常识的视觉故事创作框架，通过表示图形和使用加权故事图的方法，能够生成连贯、引人入胜的视觉故事。

    

    视觉故事创作旨在根据给定的图像序列自动生成一个连贯的故事。与图像描述任务不同，视觉故事应包含事实描述、世界观和人类社交常识，将零散的元素组合成一个连贯而引人入胜的人工编写故事。然而，大多数模型主要侧重于应用事实信息和使用分类/词汇外部知识来创建故事。本文介绍了SCO-VIST，一种将图像序列表示为对象和关系的图形框架，包括人类行为动机及其社交互动常识知识。SCO-VIST然后将这个表示情节点的图形与语义和基于事件发生的边缘权重之间建立桥梁。这个加权故事图使用Floyd-Warshall算法以事件序列形式产生故事情节。我们提出的框架在多个指标上产生出色的故事。

    Visual storytelling aims to automatically generate a coherent story based on a given image sequence. Unlike tasks like image captioning, visual stories should contain factual descriptions, worldviews, and human social commonsense to put disjointed elements together to form a coherent and engaging human-writeable story. However, most models mainly focus on applying factual information and using taxonomic/lexical external knowledge when attempting to create stories. This paper introduces SCO-VIST, a framework representing the image sequence as a graph with objects and relations that includes human action motivation and its social interaction commonsense knowledge. SCO-VIST then takes this graph representing plot points and creates bridges between plot points with semantic and occurrence-based edge weights. This weighted story graph produces the storyline in a sequence of events using Floyd-Warshall's algorithm. Our proposed framework produces stories superior across multiple metrics in t
    
[^60]: AI赋能合成生物学中的打地鼠治理挑战：文献综述与新兴框架

    The whack-a-mole governance challenge for AI-enabled synthetic biology: literature review and emerging frameworks

    [https://arxiv.org/abs/2402.00312](https://arxiv.org/abs/2402.00312)

    本文综述了AI赋能的合成生物学面临的治理挑战，提出了新兴框架，结合创新和生物安全的共同目标，探讨了命令与控制、管护、自底向上和自由放任等治理选择。

    

    AI赋能的合成生物学具有巨大的潜力，但也显著增加了生物风险，并带来了一系列新的双重使用问题。鉴于将新兴技术相结合，AI赋能的合成生物学潜在地将生物工程扩大到工业生物制造之中，这一局面变得复杂。然而，文献综述表明，如维持合理的创新范围，或者更有野心地培育巨大的生物经济并不一定与生物安全相对立，而是需要相辅相成。本文对相关问题进行了文献综述，并描述了横跨命令与控制、管护、自底向上和自由放任治理选择的政策和实践新兴框架。如何实现早期预警系统，以便预防和减轻AI赋能的生物危害，包括来自实验室、恶意滥用或公共领域，将不断需要探索。

    AI-enabled synthetic biology has tremendous potential but also significantly increases biorisks and brings about a new set of dual use concerns. The picture is complicated given the vast innovations envisioned to emerge by combining emerging technologies, as AI-enabled synthetic biology potentially scales up bioengineering into industrial biomanufacturing. However, the literature review indicates that goals such as maintaining a reasonable scope for innovation, or more ambitiously to foster a huge bioeconomy don't necessarily contrast with biosafety, but need to go hand in hand. This paper presents a literature review of the issues and describes emerging frameworks for policy and practice that transverse the options of command-and control, stewardship, bottom-up, and laissez-faire governance. How to achieve early warning systems that enable prevention and mitigation of future AI-enabled biohazards from the lab, from deliberate misuse, or from the public realm, will constantly need to e
    
[^61]: 一个准确且低参数的机器学习架构用于下一个位置的预测

    An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction

    [https://arxiv.org/abs/2402.00306](https://arxiv.org/abs/2402.00306)

    本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。

    

    下一个位置的预测是一门涉及预测用户下一个位置的学科。其应用包括资源分配、服务质量、能源效率和交通管理。本文提出了一种节能、小型和低参数的机器学习（ML）架构，用于准确的下一个位置预测，可部署在普通基站和边缘设备上。为了实现这一目标，我们对一个整个城市的完整人员流动模式进行了一百个超参数实验，以确定一个精确的ML架构，其准确度达到了最少数量的模型参数的平台。我们成功地将已发表的ML架构的模型参数数量从2.02亿减少到200万。这将模型参数的总大小从791 MB减少到8 MB。此外，训练时间减少了四倍，训练所需的图形处理单元（GPU）内存量也减少了一个因素。

    Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
    
[^62]: PAP-REC: 个性化自动提示的推荐语言模型

    PAP-REC: Personalized Automatic Prompt for Recommendation Language Model

    [https://arxiv.org/abs/2402.00284](https://arxiv.org/abs/2402.00284)

    本研究提出了PAP-REC框架，用于生成个性化自动提示的推荐语言模型。该框架通过自动生成个性化提示标记来减轻手动设计提示所带来的效率和效果问题。

    

    最近出现的基于提示的推荐语言模型（RLM）可以统一解决多个推荐任务。这些RLM充分利用了从丰富的预训练数据中学到的遗传知识，通过提示来解决下游推荐任务，而不需要引入额外的参数或网络训练。然而，手工设计的提示需要显著的专业知识和人力投入，稍微改写提示就可能导致性能的巨大变化。在本文中，我们提出了PAP-REC，一个用于生成个性化自动提示的推荐语言模型的框架，以缓解手动设计提示导致的低效率和低效果问题。具体而言，个性化自动提示允许不同的用户在相同任务中具有不同的提示标记，这些标记是使用梯度下降法自动生成的。个性化自动提示生成推荐语言模型的一个挑战是庞大的搜索空间。

    Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly. The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training. However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes. In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts. Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method. One challenge for personalized automatic prompt generation for recommendation language models is the extremely large sear
    
[^63]: 计算实验证明遇到基于大型语言模型的智能体：一项调查和展望

    Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective

    [https://arxiv.org/abs/2402.00262](https://arxiv.org/abs/2402.00262)

    本调查和展望介绍了将计算实验与基于大型语言模型的智能体相结合的研究方法，并讨论了这种方法对解决社会科学中的问题具有的潜力。

    

    计算实验已经成为研究复杂系统的有价值方法，涉及对反事实情况的算法化。然而，由于人类的多样性和复杂特性（包括有限理性和异质性），准确地表示真实社会系统在基于代理模型的建模中具有挑战性。为了解决这个局限性，提出了将大型语言模型（LLM）整合到模型中，使代理能够具备人类特征，比如复杂推理和自主学习能力。这些被称为LLM-based Agent的智能体有潜力增强基于代理模型的人类特征。然而，LLM中缺乏明确解释性极大地限制了它们在社会科学中的应用。相反，计算实验在提供个体行为和复杂现象的因果分析方面表现卓越。因此，将计算实验与基于LLM的智能体相结合具有重大的研究潜力。

    Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. 
    
[^64]: 以LLM为基础实现面向自闭症谱系障碍儿童的可扩展机器人干预

    Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs

    [https://arxiv.org/abs/2402.00260](https://arxiv.org/abs/2402.00260)

    本文提出了一种以Large Language Model (LLM)为基础的社交机器人，用于与自闭症谱系障碍儿童进行口头交流，教授透视能力。通过比较不同的LLM管道，发现GPT-2 + BART管道可以更好地生成问题和选择项。这种研究有助于改善自闭症儿童的社交能力。

    

    本文提出了一种能够与自闭症谱系障碍(ASD)儿童进行口头交流的社交机器人。这种交流旨在通过使用Large Language Model (LLM)生成的文本来教授透视能力。社交机器人NAO扮演了一个刺激器(口头描述社交情景并提问)、提示器(提供三个选择项供选择)和奖励器(当答案正确时给予称赞)的角色。对于刺激器的角色，社交情境、问题和选择项是使用我们的LLM管道生成的。我们比较了两种方法：GPT-2 + BART和GPT-2 + GPT-2，其中第一个GPT-2在管道中是用于无监督社交情境生成的。我们使用SOCIALIQA数据集对所有LLM管道进行微调。我们发现，GPT-2 + BART管道在通过结合各自的损失函数来生成问题和选择项方面具有较好的BERTscore。这种观察结果也与儿童在交互过程中的合作水平一致。

    In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the h
    
[^65]: 通过深度策略梯度进行垂直符号回归

    Vertical Symbolic Regression via Deep Policy Gradient

    [https://arxiv.org/abs/2402.00254](https://arxiv.org/abs/2402.00254)

    通过使用深度策略梯度的垂直符号回归（VSR-DPG），我们能够从实验数据中发现涉及多个独立变量的符号方程，超过了以前的方法和变体。

    

    最近提出了垂直符号回归（VSR），用于从实验数据中快速发现具有多个独立变量的符号方程。VSR通过构建由涉及一部分独立变量的简化形式方程到完整方程的垂直发现路径来减少搜索空间。深度神经网络已经被许多符号回归器证明是成功的，预计能进一步扩大VSR的规模。然而，直接将VSR与深度神经网络结合将导致梯度传递困难和其他工程问题。我们提出了使用深度策略梯度的垂直符号回归（VSR-DPG），并证明了VSR-DPG可以恢复涉及多个输入变量的真实方程，显著超过基于深度强化学习的方法和先前的VSR变体。我们的VSR-DPG将符号回归建模为一个顺序决策过程，其中方程是通过多次应用来构建的。

    Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data. VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones. Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR. Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues. We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants. Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of 
    
[^66]: 高效的非参数化不确定性量化方法用于黑盒大型语言模型和决策规划

    Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning

    [https://arxiv.org/abs/2402.00251](https://arxiv.org/abs/2402.00251)

    本文提出了一种高效的非参数化不确定性量化方法，用于黑盒大型语言模型和决策规划。该方法可以有效地估计输入-决策之间的逐点依赖关系，并提供统计上对决策可信度的解释。另外，本文还提出了一个系统化的决策代理设计，根据用户提示生成动作，并在存在多个高估计逐点依赖性的动作时要求用户提供偏好。

    

    大型语言模型(LLMs)的逐步决策规划在人工智能代理的发展中受到关注。本文主要研究带有不确定性估计的决策规划，以解决语言模型中的幻觉问题。现有方法要么是白盒方法，要么是计算复杂，限制了黑盒专有LLMs的使用。本文的第一个贡献是一种非参数化的LLMs不确定性量化方法，通过单一推理有效地估计输入-决策之间的逐点依赖关系，而不需要访问令牌logits。该估计器提供了对决策可信度的统计解释。第二个贡献是一个系统化的决策代理设计，根据用户提示如“打开浴室灯”，生成动作。当有多个动作的估计逐点依赖性都很高时，用户将被要求提供偏好。总结地说，我们的未参数化不确定性量化方法提供了一种高效的决策规划方法，可以在黑盒大型语言模型中应用。

    Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our un
    
[^67]: 生成式AI系统能否支持患者的信息需求？

    Are Generative AI systems Capable of Supporting Information Needs of Patients?

    [https://arxiv.org/abs/2402.00234](https://arxiv.org/abs/2402.00234)

    生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。

    

    患有复杂疾病如癌症的患者面临复杂的信息挑战，他们不仅需要了解他们的疾病，还需要学会如何管理它。与医疗专家（放射科医师、肿瘤科医师）密切互动可以提高患者的学习能力，从而改善疾病预后。然而，这种方法资源密集且占用了专家的时间，使他们无法完成其他关键任务。鉴于生成式AI模型在改进医疗系统方面的最新进展，我们的工作研究了生成式视觉问答系统在放射学成像数据背景下如何负责任地支持患者的信息需求。我们进行了一项形成性需求发现研究，参与者讨论了一个虚构近亲的胸部计算机断层扫描（CT）图像和相关的放射学报告。通过对参与者和医疗专家之间的对话的主题分析，我们确定常见的医学信息需求和问题。

    Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
    
[^68]: 使用有监督对比学习学习标签层次结构

    Learning Label Hierarchy with Supervised Contrastive Learning

    [https://arxiv.org/abs/2402.00232](https://arxiv.org/abs/2402.00232)

    本文介绍了一种将标签层次结构融入有监督对比学习的方法，通过调整实例之间的距离和引入额外的对比，创建一个更为良好结构化和有区分性的特征空间。

    

    有监督对比学习（SCL）框架将每个类别视为独立的，因此认为所有类别同等重要。这忽略了标签层次存在的一般情况，即同一类别下的细粒度类别之间比非常不同的类别更相似。本文引入了一系列标签感知的SCL方法（LASCL），通过利用类别之间的相似性将层次信息融入SCL中，从而创建一个更为良好结构化和有区分性的特征空间。这是通过先根据类别之间的接近程度调整实例之间的距离来实现的，通过缩放的实例-实例对比。引入了一个额外的实例-中心对比，将同类别的示例移动到它们的中心附近，中心由一组可学习的标签参数表示。学习到的标签参数可以直接用作最近邻分类器，无需进一步微调。

    Supervised contrastive learning (SCL) frameworks treat each class as independent and thus consider all classes to be equally important. This neglects the common scenario in which label hierarchy exists, where fine-grained classes under the same category show more similarity than very different ones. This paper introduces a family of Label-Aware SCL methods (LASCL) that incorporates hierarchical information to SCL by leveraging similarities between classes, resulting in creating a more well-structured and discriminative feature space. This is achieved by first adjusting the distance between instances based on measures of the proximity of their classes with the scaled instance-instance-wise contrastive. An additional instance-center-wise contrastive is introduced to move within-class examples closer to their centers, which are represented by a set of learnable label parameters. The learned label parameters can be directly used as a nearest neighbor classifier without further finetuning. 
    
[^69]: FedCore: 使用分布式核心集解决无拖车现象的联邦学习

    FedCore: Straggler-Free Federated Learning with Distributed Coresets

    [https://arxiv.org/abs/2402.00219](https://arxiv.org/abs/2402.00219)

    FedCore是一种通过分布式选择核心集解决联邦学习中慢速客户端问题的算法，可以显著减少训练时间，并保护隐私。

    

    联邦学习（FL）是一种机器学习范例，允许多个客户端在保留自己的数据的前提下，共同训练一个共享模型。然而，由于慢速客户端，拖车现象经常影响FL的效率和可扩展性。本文提出了FedCore，一种通过分布式选择核心集（数据集的代表子集）创新地解决拖车问题的算法。与现有的集中式核心集方法不同，FedCore以分布式方式直接在每个客户端上创建核心集，确保在FL中保护隐私。FedCore将核心集优化问题转化为更易处理的k-medoids聚类问题，并在每个客户端上进行分布式操作。理论分析证实了FedCore的收敛性，实际评估显示FL训练时间减少了8倍，而模型准确性没有降低。我们广泛的评估还表明，FedCore对现有的FL框架具有很好的泛化性。

    Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise. However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL. This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset. Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL. FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client. Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy. Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks.
    
[^70]: 通过超分辨率技术提高足球目标检测质量

    Improving Object Detection Quality in Football Through Super-Resolution Techniques

    [https://arxiv.org/abs/2402.00163](https://arxiv.org/abs/2402.00163)

    通过超分辨率技术提高的目标检测准确性在足球比赛中具有重要的应用价值。

    

    本研究探索了在足球比赛中利用超分辨率技术提高目标检测准确性的潜力。考虑到足球比赛的快节奏以及精确物体（如球、球员）跟踪对于分析和广播的重要性，超分辨率可以带来显著的改进。我们研究了通过超分辨率的先进图像处理对处理足球比赛录像中物体检测算法准确性和可靠性的影响。我们的方法包括将最先进的超分辨率技术应用于来自SoccerNet的多样化的足球比赛录像中，然后使用Faster R-CNN进行目标检测。结果表明，当应用超分辨率预处理时，目标检测的准确性有显著提高。

    This study explores the potential of super-resolution techniques in enhancing object detection accuracy in football. Given the sport's fast-paced nature and the critical importance of precise object (e.g. ball, player) tracking for both analysis and broadcasting, super-resolution could offer significant improvements. We investigate how advanced image processing through super-resolution impacts the accuracy and reliability of object detection algorithms in processing football match footage.   Our methodology involved applying state-of-the-art super-resolution techniques to a diverse set of football match videos from SoccerNet, followed by object detection using Faster R-CNN. The performance of these algorithms, both with and without super-resolution enhancement, was rigorously evaluated in terms of detection accuracy.   The results indicate a marked improvement in object detection accuracy when super-resolution preprocessing is applied. The improvement of object detection through the in
    
[^71]: 在联邦设置中可分解子模函数的最大化

    Decomposable Submodular Maximization in Federated Setting

    [https://arxiv.org/abs/2402.00138](https://arxiv.org/abs/2402.00138)

    该论文提出了一种联邦优化设置用于在计算上不可行的可分解子模函数优化问题中。在这个设置中，客户端拥有私有的组件函数，通过并行计算和集中聚合的方式来求解最大化问题。

    

    在机器学习、推荐系统和福利最大化等众多应用中，子模函数以及可分解子模函数及其优化问题都得到了广泛应用。然而，对于具有数百万个组分函数的可分解子模函数的优化问题，在计算上是不可行的。此外，组分函数可能是私有的（例如可能表示用户偏好函数），不能广泛共享。为了解决这些问题，我们提出了一种适用于可分解子模函数优化的“联邦优化”设置。在这种设置下，客户端拥有自己的偏好函数，需要最大化这些偏好的加权和。我们在该设置中实现了流行的“连续贪婪”算法，其中客户端以并行的方式朝着局部解向前迈出小的局部步骤，然后将局部变化聚合到一个中央服务器上。

    Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is 
    
[^72]: 基于强化学习的控制器以最小化下肢外骨骼拐杖上的力量

    A Reinforcement Learning Based Controller to Minimize Forces on the Crutches of a Lower-Limb Exoskeleton

    [https://arxiv.org/abs/2402.00135](https://arxiv.org/abs/2402.00135)

    本研究使用深度强化学习方法开发了一种最小化拐杖上地面反作用力的运动控制器，以减轻外骨骼用户上半身努力的目标。

    

    身体各部分的力量主要来自上半身的努力，而下半身被认为是被动的。然而，在设计运动控制器时，文献中往往忽视了使用者上半身的努力。本研究使用深度强化学习开发一种运动控制器，其目标是最小化拐杖上的地面反作用力，以减轻使用者的上半身努力。我们设计了一个人体-外骨骼系统的模型和学习框架，并制定了一种奖励函数，以鼓励人体-外骨骼系统的前向位移并满足物理机器人的预定约束。我们使用MuJoCo物理模拟器和多个实验进行了Proximal Policy Optimization，即最先进的深度强化学习方法的新框架的评估。

    Metabolic energy consumption of a powered lower-limb exoskeleton user mainly comes from the upper body effort since the lower body is considered to be passive. However, the upper body effort of the users is largely ignored in the literature when designing motion controllers. In this work, we use deep reinforcement learning to develop a locomotion controller that minimizes ground reaction forces (GRF) on crutches. The rationale for minimizing GRF is to reduce the upper body effort of the user. Accordingly, we design a model and a learning framework for a human-exoskeleton system with crutches. We formulate a reward function to encourage the forward displacement of a human-exoskeleton system while satisfying the predetermined constraints of a physical robot. We evaluate our new framework using Proximal Policy Optimization, a state-of-the-art deep reinforcement learning (RL) method, on the MuJoCo physics simulator with different hyperparameters and network architectures over multiple tria
    
[^73]: 深度神经网络: 非阿基米德分析的表述方式

    Deep Neural Networks: A Formulation Via Non-Archimedean Analysis

    [https://arxiv.org/abs/2402.00094](https://arxiv.org/abs/2402.00094)

    该论文引入了一种新的深度神经网络（DNNs）类别，其采用多层树状结构的架构并使用非阿基米德局部域的整数环进行编码。这些DNNs是稳健的对实值函数和实值平方可积函数的普遍逼近器。

    

    我们引入了一种新的深度神经网络（DNNs），采用多层树状结构的架构。这些架构使用非阿基米德局部域的整数环中的数字进行编码。这些环具有自然的层次结构，类似无限根树。这些环上的自然态射使我们能够构建有限的多层架构。新的DNNs是对在所提到的环上定义的实值函数的稳健的普遍逼近器。我们还证明了DNNs也是对在单位区间上定义的实值平方可积函数的稳健的普遍逼近器。

    We introduce a new class of deep neural networks (DNNs) with multilayered tree-like architectures. The architectures are codified using numbers from the ring of integers of non-Archimdean local fields. These rings have a natural hierarchical organization as infinite rooted trees. Natural morphisms on these rings allow us to construct finite multilayered architectures. The new DNNs are robust universal approximators of real-valued functions defined on the mentioned rings. We also show that the DNNs are robust universal approximators of real-valued square-integrable functions defined in the unit interval.
    
[^74]: 无分集任务选择对于少样本学习的影响

    Episodic-free Task Selection for Few-shot Learning

    [https://arxiv.org/abs/2402.00092](https://arxiv.org/abs/2402.00092)

    本文提出了一种超越分集训练的元训练框架，通过选择一些无分集任务对元学习器进行训练，并使用亲和性标准来评估其有效性。实验结果显示，这种方法在少样本学习中取得了更好的效果。

    

    分集训练是少样本学习中主流的训练策略。然而，在少样本的场景下，这种策略往往劣于一些非分集训练策略，如邻域分量分析（NCA），这挑战了训练条件必须与测试条件匹配的原则。因此，自然而然地提出了一个问题：如何搜索无分集任务从而获得更好的少样本学习效果？在这项工作中，我们提出了一种超越分集训练的新型元训练框架。在这个框架中，分集任务不直接用于训练，而是用于评估从任务集中选择的一些无分集任务对元学习器的训练效果。选择标准是通过亲和性设计的，亲和性衡量了在使用选定任务训练后，执行目标任务时损失降低的程度。在实验中，训练任务集包含了一些有前景的类型，如对比学习和分类。

    Episodic training is a mainstream training strategy for few-shot learning. In few-shot scenarios, however, this strategy is often inferior to some non-episodic training strategy, e. g., Neighbourhood Component Analysis (NCA), which challenges the principle that training conditions must match testing conditions. Thus, a question is naturally asked: How to search for episodic-free tasks for better few-shot learning? In this work, we propose a novel meta-training framework beyond episodic training. In this framework, episodic tasks are not used directly for training, but for evaluating the effectiveness of some selected episodic-free tasks from a task set that are performed for training the meta-learners. The selection criterion is designed with the affinity, which measures the degree to which loss decreases when executing the target tasks after training with the selected tasks. In experiments, the training task set contains some promising types, e. g., contrastive learning and classifica
    
[^75]: SCAPE: 使用进化搜索概念架构提示

    SCAPE: Searching Conceptual Architecture Prompts using Evolution

    [https://arxiv.org/abs/2402.00089](https://arxiv.org/abs/2402.00089)

    SCAPE是一个将进化搜索与生成式人工智能相结合的工具，能够帮助用户通过简单的操作探索由初始输入启发的创造性和高质量设计，同时提高了图像的新颖性、质量和使用效果。

    

    概念架构涉及对来自其他学科的新颖理念进行高度创造性的探索，建筑师考虑将建筑的形式、材料、质地和颜色进行激进的创新。虽然如今的生成式人工智能系统能够产生出色的结果，但它们缺乏几十年来进化算法所展示的创造性。我们提出的工具SCAPE将进化搜索与生成式人工智能结合起来，使用户能够通过简单的点按界面探索由初始输入启发的创造性和高质量设计。SCAPE将随机性注入到生成式人工智能中，并利用GPT-4的内置语言能力通过基于文本的突变和交叉操作来变化提示。我们证明，与DALL-E 3相比，SCAPE在图像的新颖性方面提高了67%，同时在质量和使用效果方面也有所改进；我们展示，仅经过3次迭代，SCAPE的图像新颖性增加了24%，使有效的探索和优化成为可能。

    Conceptual architecture involves a highly creative exploration of novel ideas, often taken from other disciplines as architects consider radical new forms, materials, textures and colors for buildings. While today's generative AI systems can produce remarkable results, they lack the creativity demonstrated for decades by evolutionary algorithms. SCAPE, our proposed tool, combines evolutionary search with generative AI, enabling users to explore creative and good quality designs inspired by their initial input through a simple point and click interface. SCAPE injects randomness into generative AI, and enables memory, making use of the built-in language skills of GPT-4 to vary prompts via text-based mutation and crossover. We demonstrate that compared to DALL-E 3, SCAPE enables a 67% improvement in image novelty, plus improvements in quality and effectiveness of use; we show that in just 3 iterations SCAPE has a 24% image novelty increase enabling effective exploration, plus optimization
    
[^76]: 利用计算模拟反应数据增强的 retrosynthesis 预测

    Retrosynthesis prediction enhanced by in-silico reaction data augmentation

    [https://arxiv.org/abs/2402.00086](https://arxiv.org/abs/2402.00086)

    RetroWISE 是一个利用计算模拟的反应数据增强的 retrosynthesis 预测的框架，通过使用易于获取的非配对数据生成配对数据，提高了模型的训练性能。

    

    机器学习在 retrosynthesis 研究中取得了重要进展，帮助化学家更高效地设计实验。然而，所有基于机器学习的方法都需要大量成对的训练数据（即化学反应：产物-反应物对），而这些数据获取成本高昂。此外，公司将反应数据视为有价值的资产，并限制对研究人员的可访问性。这些问题阻碍了更强大的 retrosynthesis 模型的创建，因为这些模型依赖于数据。为了解决这个问题，我们利用易于获取的非配对数据（即产物-反应物对中的一个组分）生成计算模拟的配对数据，为模型训练提供帮助。具体地，我们提出了 RetroWISE，一个自我增强的框架，它使用从真实配对数据推断出的基础模型，利用非配对数据执行计算模拟的反应生成和增强，最终实现了一个卓越的模型。在三个基准数据集上，RetroWISE 实现最佳的整体性能。

    Recent advances in machine learning (ML) have expedited retrosynthesis research by assisting chemists to design experiments more efficiently. However, all ML-based methods consume substantial amounts of paired training data (i.e., chemical reaction: product-reactant(s) pair), which is costly to obtain. Moreover, companies view reaction data as a valuable asset and restrict the accessibility to researchers. These issues prevent the creation of more powerful retrosynthesis models due to their data-driven nature. As a response, we exploit easy-to-access unpaired data (i.e., one component of product-reactant(s) pair) for generating in-silico paired data to facilitate model training. Specifically, we present RetroWISE, a self-boosting framework that employs a base model inferred from real paired data to perform in-silico reaction generation and augmentation using unpaired data, ultimately leading to a superior model. On three benchmark datasets, RetroWISE achieves the best overall performan
    
[^77]: 预定好奇心-深度动态-Q：对话策略学习的高效探索

    Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning

    [https://arxiv.org/abs/2402.00085](https://arxiv.org/abs/2402.00085)

    本论文提出了Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)框架，通过引入预定学习和好奇心，显著提高了基于强化学习的任务导向对话代理的效果。

    

    基于强化学习的任务导向对话代理的培训是耗时的，并需要与真实用户进行大量的交互。如何在有限的对话经验中掌握对话策略仍然是使代理培训过程更加高效的障碍。此外，大多数先前的框架通过随机选择培训样本开始培训，这与人类学习方法不同，损害了培训的效率和稳定性。因此，我们提出了一种基于最先进的基于模型的强化学习对话模型Deep Dyna-Q(DDQ)的预定好奇心-深度动态-Q (SC-DDQ)的好奇心驱动课程学习框架。此外，我们分别为SC-DDQ和DDQ设计了学习计划，遵循两种相反的培训策略：经典课程学习及其逆向版本。我们的结果表明，通过引入预定学习和好奇心，新框架在DDQ和Dee的基础上取得了显著的改进。

    Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee
    
[^78]: EPSD: 早期修剪与自蒸馏相结合的高效模型压缩

    EPSD: Early Pruning with Self-Distillation for Efficient Model Compression

    [https://arxiv.org/abs/2402.00084](https://arxiv.org/abs/2402.00084)

    提出了一种早期修剪与自蒸馏相结合的框架，实现了高效的模型压缩。

    

    神经网络压缩技术，如知识蒸馏和网络修剪，受到越来越多的关注。最近的研究“先修剪，然后蒸馏”表明，修剪后的适合学生的教师网络可以提高知识蒸馏的性能。然而，传统的教师-学生流程，涉及繁琐的教师预训练和复杂的压缩步骤，使得使用知识蒸馏的修剪变得不够高效。除了压缩模型，最近的压缩技术也强调效率方面的考量。早期修剪与传统的修剪方法相比，要求的计算成本要小得多，因为它不需要一个大型预训练模型。同样，自蒸馏作为知识蒸馏的一种特殊情况，更加高效，因为它不需要预训练或学生-教师对的选择。这激发了我们将早期修剪与自蒸馏相结合，实现高效的模型压缩。在这项工作中，我们提出了名为“早期修剪”的框架。

    Neural network compression techniques, such as knowledge distillation (KD) and network pruning, have received increasing attention. Recent work `Prune, then Distill' reveals that a pruned student-friendly teacher network can benefit the performance of KD. However, the conventional teacher-student pipeline, which entails cumbersome pre-training of the teacher and complicated compression steps, makes pruning with KD less efficient. In addition to compressing models, recent compression techniques also emphasize the aspect of efficiency. Early pruning demands significantly less computational cost in comparison to the conventional pruning methods as it does not require a large pre-trained model. Likewise, a special case of KD, known as self-distillation (SD), is more efficient since it requires no pre-training or student-teacher pair selection. This inspires us to collaborate early pruning with SD for efficient model compression. In this work, we propose the framework named Early Pruning wi
    
[^79]: 减少资源分配差异的准入差异建模

    Modeling Access Differences to Reduce Disparity in Resource Allocation

    [https://arxiv.org/abs/2402.00083](https://arxiv.org/abs/2402.00083)

    本论文研究了具有准入差异的资源分配问题，以减少资源分配差异，并通过开发准入模型和准入感知的分配方法来实现。实验结果表明，这种方法能够在不知道准入差距的情况下有效减少资源差异。

    

    在COVID-19疫苗分配中，易受影响的亚人群在健康和接种疫苗准入方面更具劣势。为此，我们系统地形式化和研究了在存在与优势和劣势相关的固有准入差异的情况下的资源分配问题。我们确定减少资源差异是这一背景下的关键目标，并展示了其作为更细致的下游影响的代理。我们开发了一个具体的准入模型，帮助量化给定分配对于有利地区和不利地区的资源流转的影响，该模型基于它们之间的准入差距。然后，我们提供了一个准入感知的分配方法。直观上讲，由此产生的分配利用更多疫苗分配给易受损人群更多的地区，以减少准入差距和整体差异。令人惊讶的是，通常不需要了解准入差距就可以执行准入感知的分配。为了支持这个形式化的方法，我们所提出的具体建模和方法能够实现平衡公平性和效率性。

    Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are simultaneously more impacted in terms of health and more disadvantaged in terms of access to the vaccine, we formalize and study the problem of resource allocation when there are inherent access differences that correlate with advantage and disadvantage. We identify reducing resource disparity as a key goal in this context and show its role as a proxy to more nuanced downstream impacts. We develop a concrete access model that helps quantify how a given allocation translates to resource flow for the advantaged vs. the disadvantaged, based on the access gap between them. We then provide a methodology for access-aware allocation. Intuitively, the resulting allocation leverages more vaccines in locations with higher vulnerable populations to mitigate the access gap and reduce overall disparity. Surprisingly, knowledge of the access gap is often not needed to perform access-aware allocation. To support this formal
    
[^80]: 有条件马尔可夫链搜索中的开发策略：基于三指数分配问题的案例研究

    Exploitation Strategies in Conditional Markov Chain Search: A case study on the three-index assignment problem

    [https://arxiv.org/abs/2402.00076](https://arxiv.org/abs/2402.00076)

    这个研究探索了扩展有条件马尔可夫链搜索（CMCS）框架的几种方法，以提高其在开发方面的能力。实验结果表明，两阶段CMCS优于单一阶段CMCS。

    

    有条件马尔可夫链搜索（CMCS）是用于离散组合优化问题的元启发式自动设计框架。给定一组算法组件，如山爬和突变，CMCS决定以何种顺序应用这些组件。决策由离线学习的CMCS配置指导。CMCS没有接受标准，框架接受任何移动。因此，它在探索方面表现出色，但在开发方面不如其他方法。在本研究中，我们探索了几种扩展CMCS的方法以提高其开发能力。为了进行计算研究，我们将该框架应用于三指数分配问题。我们实验的结果表明，两阶段CMCS的确优于单一阶段CMCS。

    The Conditional Markov Chain Search (CMCS) is a framework for automated design of metaheuristics for discrete combinatorial optimisation problems. Given a set of algorithmic components such as hill climbers and mutations, CMCS decides in which order to apply those components. The decisions are dictated by the CMCS configuration that can be learnt offline. CMCS does not have an acceptance criterion; any moves are accepted by the framework. As a result, it is particularly good in exploration but is not as good at exploitation. In this study, we explore several extensions of the framework to improve its exploitation abilities. To perform a computational study, we applied the framework to the three-index assignment problem. The results of our experiments showed that a two-stage CMCS is indeed superior to a single-stage CMCS.
    
[^81]: EvoMerge:针对大型语言模型的神经进化

    EvoMerge: Neuroevolution for Large Language Models

    [https://arxiv.org/abs/2402.00070](https://arxiv.org/abs/2402.00070)

    EvoMerge是一种针对大型语言模型训练和合并的系统性方法，通过权重交叉和微调进行权重变异，旨在将模型推向超越传统微调限制的进化过程。

    

    在大型语言模型的广泛微调中，并不总能取得更好的结果。往往模型更擅长模仿一种数据形式而不具备更强的推理能力，甚至可能失去一些智能。这里我介绍了EvoMerge，一种用于大型语言模型训练和合并的系统性方法。通过利用权重交叉和微调进行权重变异，EvoMerge建立了一个旨在将模型推向超越传统微调限制的进化过程。

    Extensive fine-tuning on Large Language Models does not always yield better results. Oftentimes, models tend to get better at imitating one form of data without gaining greater reasoning ability and may even end up losing some intelligence. Here I introduce EvoMerge, a systematic approach to large language model training and merging. Leveraging model merging for weight crossover and fine-tuning for weight mutation, EvoMerge establishes an evolutionary process aimed at pushing models beyond the limits of conventional fine-tuning.
    
[^82]: 使用抽象计算机体系结构描述语言对AI硬件加速器进行建模

    Using the Abstract Computer Architecture Description Language to Model AI Hardware Accelerators

    [https://arxiv.org/abs/2402.00069](https://arxiv.org/abs/2402.00069)

    本文介绍了使用抽象计算机体系结构描述语言（ACAD）对AI硬件加速器进行建模的研究。通过使用ACAD，工程师可以更好地理解不同加速器设计方案的性能特性，从而在选择和配置加速器时做出更准确的决策。

    

    人工智能（AI）在深度神经网络（DNN）的普及中取得了显著的增长。这些强大的模型推动了各个领域中的技术进步。然而，为了在实际应用中发挥它们的潜力，专门的硬件加速器是必不可少的。这种需求引发了不同供应商提供的可参数化AI硬件加速器市场。集成AI产品制造商面临一个关键挑战：选择与其产品性能要求相匹配的加速器。这个决策包括选择合适的硬件和配置一组合适的参数。然而，比较不同的加速器设计方案仍然是一项复杂的任务。通常，工程师依赖于数据表、电子表格计算或缓慢的黑盒模拟器，这些只能提供对性能特性的粗略了解。

    Artificial Intelligence (AI) has witnessed remarkable growth, particularly through the proliferation of Deep Neural Networks (DNNs). These powerful models drive technological advancements across various domains. However, to harness their potential in real-world applications, specialized hardware accelerators are essential. This demand has sparked a market for parameterizable AI hardware accelerators offered by different vendors.   Manufacturers of AI-integrated products face a critical challenge: selecting an accelerator that aligns with their product's performance requirements. The decision involves choosing the right hardware and configuring a suitable set of parameters. However, comparing different accelerator design alternatives remains a complex task. Often, engineers rely on data sheets, spreadsheet calculations, or slow black-box simulators, which only offer a coarse understanding of the performance characteristics.   The Abstract Computer Architecture Description Language (ACAD
    
[^83]: GPT4Battery: 一种基于LLM驱动的自适应锂离子电池健康状态估计框架

    GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries

    [https://arxiv.org/abs/2402.00068](https://arxiv.org/abs/2402.00068)

    本论文提出了一种基于LLM的框架，可以适应不同类型的锂离子电池，实现准确的健康状态估计。这项工作解决了生成训练数据的时间和资源成本高的挑战，并在实际应用中具有良好的泛化能力。

    

    健康状态（SOH）是评估电池退化水平的关键指标，无法直接测量但需要估计。准确的SOH估计提升了锂离子电池的检测、控制和反馈能力，实现安全高效的能源管理，并指导新一代电池的发展。尽管在数据驱动的SOH估计方面取得了显著进展，但为生成寿命长期训练数据而进行的耗时且资源密集的退化实验在建立一个能处理多样化锂离子电池（例如，跨化学、跨制造商和跨容量）的大型模型方面存在挑战。因此，本文利用大型语言模型（LLM）的强大泛化能力，提出了一种适用于不同电池的可调整SOH估计的新型框架。为了适应实际情景，其中未标记的数据按顺序以及分布变化的方式到达，所提出的模型在测试时进行了修改。

    State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
    
[^84]: TrackGPT -- 用于跨领域实体轨迹预测的生成式预训练变换器

    TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting

    [https://arxiv.org/abs/2402.00066](https://arxiv.org/abs/2402.00066)

    TrackGPT是一种基于GPT的实体轨迹预测模型，能够生成准确的预测结果，包括长期预测和短期预测，并在多个领域展现出良好的性能。

    

    在商业和国防领域的应用中，对未来时间点的实体轨迹进行预测是一个关键的能力缺口。近年来，变换器和特别是生成式预训练变换器（GPT）网络已经在人工智能的几个领域革命化，其中最为著名的是使用开放AI的ChatGPT等大型语言模型（LLM）的自然语言处理（NLP）。在本研究论文中，我们介绍了TrackGPT，一种基于GPT的实体轨迹预测模型，该模型在海上和航空领域都显示出了实用性，并且我们预计在其他领域也能表现出色。TrackGPT是一种开创性的GPT模型，能够在不同的实体时间序列数据集上产生准确的预测，展示了长期预测具有持续准确性和短期预测具有高精度的能力。我们与最先进的深度学习技术进行的基准测试显示，TrackGPT的预测能力很高。

    The forecasting of entity trajectories at future points in time is a critical capability gap in applications across both Commercial and Defense sectors. Transformers, and specifically Generative Pre-trained Transformer (GPT) networks have recently revolutionized several fields of Artificial Intelligence, most notably Natural Language Processing (NLP) with the advent of Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we introduce TrackGPT, a GPT-based model for entity trajectory forecasting that has shown utility across both maritime and air domains, and we expect to perform well in others. TrackGPT stands as a pioneering GPT model capable of producing accurate predictions across diverse entity time series datasets, demonstrating proficiency in generating both long-term forecasts with sustained accuracy and short-term forecasts with high precision. We present benchmarks against state-of-the-art deep learning techniques, showing that TrackGPT's forecasting capa
    
[^85]: 一种基于间接QAOA方法的91个子句SAT解析的技术注释

    A technical note for the 91-clauses SAT resolution with Indirect QAOA based approach

    [https://arxiv.org/abs/2402.00065](https://arxiv.org/abs/2402.00065)

    本文针对3-SAT问题提出了一种基于QAOA的间接方法，通过建模解决方案的排名来实现高效的解析，可以处理大规模的问题。

    

    本文通过使用一种类似QAOA的方法解决3-SAT问题。所选择的原则涉及对3-SAT问题的解决方案进行建模，这在这种特定情况下直接代表一种解决方案。这导致了一个非常紧凑的电路，门数量较少，可以对大规模的3-SAT问题进行建模。数值实验表明，该方法可以使用Qiskit实现解决由91个子句和20个变量组成的实例。

    This paper addresses the resolution of the 3-SAT problem using a QAOA-like approach. The chosen principle involves modeling the solution ranks of the 3-SAT problem, which, in this particular case, directly represent a solution. This results in a highly compact circuit with few gates, enabling the modeling of large-sized 3-SAT problems. Numerical experimentation demonstrates that the approach can solve instances composed of 91 clauses and 20 variables with an implementation based on Qiskit.
    
[^86]: 通过基于代理的声誉系统合并具有关于动作和目标的不完整知识的计划

    Merging plans with incomplete knowledge about actions and goals through an agent-based reputation system

    [https://arxiv.org/abs/2402.00064](https://arxiv.org/abs/2402.00064)

    本论文介绍了一种通过基于代理的声誉系统合并具有关于动作和目标的不完整知识的计划的方法，该方法提供了一个自动化生成过渡计划的工具，对认知障碍人士具有帮助。

    

    管理过渡计划是认知障碍人士面临的主要问题之一。因此，找到一种自动化生成这种计划的方法将是对这个群体有益的工具。在本文中，我们特别提出并比较了不同的替代方式，通过合并由多个操作代理执行的目标和动作之间的未知相似性序列形成的计划，这些代理之间进行合作，对一些 passsive 元素（节点代理）应用这些行动，在使用一段时间后需要执行另一个计划。这种对计划动作和目标相似性的无知可以解释为使用分布式推荐系统，该系统会根据其他操作代理以前执行不同计划的已知结果，为特定目标的给定操作代理提供一个有用的计划。在这里，我们提供了执行的总体框架（代理系统）以及不同的合并算法。

    Managing transition plans is one of the major problems of people with cognitive disabilities. Therefore, finding an automated way to generate such plans would be a helpful tool for this community. In this paper we have specifically proposed and compared different alternative ways to merge plans formed by sequences of actions of unknown similarities between goals and actions executed by several operator agents which cooperate between them applying such actions over some passive elements (node agents) that require additional executions of another plan after some time of use. Such ignorance of the similarities between plan actions and goals would justify the use of a distributed recommendation system that would provide an useful plan to be applied for a certain goal to a given operator agent, generated from the known results of previous executions of different plans by other operator agents. Here we provide the general framework of execution (agent system), and the different merging algor
    
[^87]: 在Dempster-Shafer理论中处理共同分析中的认知不确定性

    Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory

    [https://arxiv.org/abs/2402.00060](https://arxiv.org/abs/2402.00060)

    本文提出了一种基于Dempster-Shafer理论的方法，用于在共同数据信息中建模认知不确定性和根据置信度对共同事件进行分类。通过构建概率箱和DSt结构，可以计算特定碰撞概率的置信度和可能性。

    

    本文提出了一种在共同数据信息（CDM）中建模认知不确定性和根据碰撞概率的置信度对共同事件进行分类的方法。本文提出的方法基于Dempster-Shafer论证的理论，假设观察到的CDMs来自一组未知分布的家族。使用Dvoretzky-Kiefer-Wolfowitz不等式从CDMs的时间序列构建鲁棒界限。然后使用DKW不等式构建的概率箱派生DSt结构。该DSt结构封装了时间序列上每个点的CDMs的不确定性，并允许计算特定碰撞概率实现的置信度和可能性。本文提出的方法在一些实际事件上进行了测试，并与欧洲现有的实践进行了比较。

    The paper presents an approach to the modelling of epistemic uncertainty in Conjunction Data Messages (CDM) and the classification of conjunction events according to the confidence in the probability of collision. The approach proposed in this paper is based on the Dempster-Shafer Theory (DSt) of evidence and starts from the assumption that the observed CDMs are drawn from a family of unknown distributions. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality is used to construct robust bounds on such a family of unknown distributions starting from a time series of CDMs. A DSt structure is then derived from the probability boxes constructed with DKW inequality. The DSt structure encapsulates the uncertainty in the CDMs at every point along the time series and allows the computation of the belief and plausibility in the realisation of a given probability of collision. The methodology proposed in this paper is tested on a number of real events and compared against existing practices in the Eu
    
[^88]: FengWu-GHR: 学习公里级中程全球天气预报

    FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting

    [https://arxiv.org/abs/2402.00059](https://arxiv.org/abs/2402.00059)

    FengWu-GHR是全球首个以数据驱动方式运行的公里级全球天气预报模型，具有更高的分辨率和可比甚至更高的预报技巧。

    

    公里级别的全球大气动力学建模可以实现精细化的天气预报，降低灾害性天气和气候活动的风险。因此，建立公里级全球预报模型是气象领域一直以来的追求。过去几十年，国际社会积极努力改善数值天气模型的空间分辨率。然而，由于计算资源的消耗巨大，发展更高分辨率的数值模型仍然是一个长期存在的挑战。最近，数据驱动的全球天气预报模型利用再分析数据进行模型训练，并展示出与数值模型相当甚至更高的预报技巧。然而，它们都受限于再分析数据的分辨率，无法生成更高分辨率的预报。本文介绍了FengWu-GHR，这是首个以数据驱动方式运行、0.09$^{\circ}$水平分辨率的全球天气预报模型。

    Kilometer-scale modeling of global atmosphere dynamics enables fine-grained weather forecasting and decreases the risk of disastrous weather and climate activity. Therefore, building a kilometer-scale global forecast model is a persistent pursuit in the meteorology domain. Active international efforts have been made in past decades to improve the spatial resolution of numerical weather models. Nonetheless, developing the higher resolution numerical model remains a long-standing challenge due to the substantial consumption of computational resources. Recent advances in data-driven global weather forecasting models utilize reanalysis data for model training and have demonstrated comparable or even higher forecasting skills than numerical models. However, they are all limited by the resolution of reanalysis data and incapable of generating higher-resolution forecasts. This work presents FengWu-GHR, the first data-driven global weather forecasting model running at the 0.09$^{\circ}$ horizo
    
[^89]: 我们在浪费时间吗？一种快速、准确的知识图谱链接预测评估框架

    Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors

    [https://arxiv.org/abs/2402.00053](https://arxiv.org/abs/2402.00053)

    这篇论文提出了一种快速、准确的知识图谱链接预测评估框架，解决了现有方法中随机抽样带来的排名指标不准确的问题。

    

    用于衡量知识图谱完善方法质量的标准评估协议通常包括对知识图谱的每个实体进行排名，以评估它们作为候选链接头部或尾部的适合程度。然而，在规模较大的知识图谱中，这个任务很快变得难以承受。先前的方法通过对实体进行随机抽样来评估预测或建议方法的链接质量，但我们展示了这种方法存在严重局限性，因为产生的排名指标不正确地反映了真实结果。本文对这些效应进行了彻底分析，并得出以下发现。首先，我们通过实证研究和理论论证找出了为什么随机均匀抽样极大地高估了方法的排名表现。我们展示了这可以归因于易/难度负候选者的影响。

    The standard evaluation protocol for measuring the quality of Knowledge Graph Completion methods - the task of inferring new links to be added to a graph - typically involves a step which ranks every entity of a Knowledge Graph to assess their fit as a head or tail of a candidate link to be added. In Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively heavy. Previous approaches mitigate this problem by using random sampling of entities to assess the quality of links predicted or suggested by a method. However, we show that this approach has serious limitations since the ranking metrics produced do not properly reflect true outcomes. In this paper, we present a thorough analysis of these effects along with the following findings. First, we empirically find and theoretically motivate why sampling uniformly at random vastly overestimates the ranking performance of a method. We show that this can be attributed to the effect of easy versus hard negative candidates. S
    
[^90]: 零射击顺序神经符号推理用于自动生成架构原理图设计

    Zero-shot Sequential Neuro-symbolic Reasoning for Automatically Generating Architecture Schematic Designs

    [https://arxiv.org/abs/2402.00052](https://arxiv.org/abs/2402.00052)

    提出了一种利用神经推理和符号推理相结合的零射击顺序神经符号推理方法，用于自动生成架构原理图设计，解决了专家见解依赖和技术挑战的问题。

    

    本文介绍了一种新颖的自动化系统，用于生成旨在简化多户型房地产项目的复杂决策的架构原理图设计。该方法利用生成AI（神经推理）和数学规划求解器（符号推理）的综合优势，解决了架构原理图设计中专家见解依赖和技术挑战两个方面。为了处理设计整个建筑所需要的大规模和相互关联性决策的特点，我们提出了一种新颖的顺序神经符号推理方法，模拟了从初始概念到详细布局的传统架构设计过程。为了消除手工制作逼近所需目标的成本函数的需要，我们提出了一种解决方案，使用神经推理生成符号求解器可以使用的约束和成本函数进行求解。我们还为每个设计阶段引入了反馈循环，以确保设计的准确性和可迭代性。

    This paper introduces a novel automated system for generating architecture schematic designs aimed at streamlining complex decision-making at the multifamily real estate development project's outset. Leveraging the combined strengths of generative AI (neuro reasoning) and mathematical program solvers (symbolic reasoning), the method addresses both the reliance on expert insights and technical challenges in architectural schematic design. To address the large-scale and interconnected nature of design decisions needed for designing a whole building, we proposed a novel sequential neuro-symbolic reasoning approach, emulating traditional architecture design processes from initial concept to detailed layout. To remove the need to hand-craft a cost function to approximate the desired objectives, we propose a solution that uses neuro reasoning to generate constraints and cost functions that the symbolic solvers can use to solve. We also incorporate feedback loops for each design stage to ensu
    
[^91]: IICONGRAPH: 改进的知识图谱中图标学和图象学陈述

    IICONGRAPH: improved Iconographic and Iconological Statements in Knowledge Graphs

    [https://arxiv.org/abs/2402.00048](https://arxiv.org/abs/2402.00048)

    改进的知识图谱IICONGRAPH填补了当前知识图谱中关于图标学和图象学陈述的空缺，并通过改进和扩展ArCo和Wikidata实现了更好的性能。

    

    图标学和图象学是理解文化遗产中的艺术品的基本领域。图标学涉及对艺术品中描绘的视觉元素及其象征意义的研究和解释，而图象学更深入探索其中的文化和历史意义。尽管利用链接开放数据（LOD）来表示文化遗产取得了进展，但最近的研究表明当前知识图谱（KGs）中的图标学和图象学陈述仍存在差距。为了解决这些问题，本文介绍了IICONGRAPH，它是通过改进和扩展ArCo（意大利文化遗产知识图谱）和Wikidata的图标学和图象学陈述而创建的KG。IICONGRAPH的开发也受到一系列研究案例的要求的驱动，这些要求在非重新设计版本的KG中无法实现。评估结果表明，IICONGRAPH不仅超越了ArCo和W

    Iconography and iconology are fundamental domains when it comes to understanding artifacts of cultural heritage. Iconography deals with the study and interpretation of visual elements depicted in artifacts and their symbolism, while iconology delves deeper, exploring the underlying cultural and historical meanings. Despite the advances in representing cultural heritage with Linked Open Data (LOD), recent studies show persistent gaps in the representation of iconographic and iconological statements in current knowledge graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was created by refining and extending the iconographic and iconological statements of ArCo (the Italian KG of cultural heritage) and Wikidata. The development of IICONGRAPH was also driven by a series of requirements emerging from research case studies that were unattainable in the non-reengineered versions of the KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms ArCo and W
    
[^92]: 引入PetriRL：将Petri网和基于事件的强化学习集成的JSSP解决方案的创新框架

    Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning

    [https://arxiv.org/abs/2402.00046](https://arxiv.org/abs/2402.00046)

    PetriRL是一个创新框架，将Petri网和基于事件的强化学习集成，用于解决工业作业车间调度问题（JSSP）。该框架利用Petri网进行建模，提高了可解释性，并通过事件驱动控制和动作屏蔽的集成取得了竞争性的性能。

    

    在工业车间中，优质调度至关重要。尽管神经网络在解决这些问题方面表现出色，但其有限的可解释性阻碍了其在工业中的广泛应用。在本研究中，我们介绍了一个创新的框架来解决作业车间调度问题（JSSP）。我们的方法利用Petri网对作业车间进行建模，不仅提高了可解释性，还能直接将原始数据纳入其中，无需对JSSP实例进行预处理成非交替图。Petri网的控制能力还可以管理过程中的自动化组件，使代理人能够专注于关键决策，特别是资源分配。我们的方法中事件驱动控制和动作屏蔽的集成在公共测试基准上取得了竞争性的性能。跨一系列优化解决方案（包括启发式算法、元启发式算法和学习算法）进行的比较分析突出了其竞争性。

    Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitivene
    
[^93]: 检测大型人工智能模型生成的多媒体内容：一项调查研究

    Detecting Multimedia Generated by Large AI Models: A Survey

    [https://arxiv.org/abs/2402.00045](https://arxiv.org/abs/2402.00045)

    本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。

    

    大型人工智能模型（LAIMs）的快速发展，尤其是扩散模型和大型语言模型，标志着一种新的时代，人工智能生成的多媒体内容被越来越多地整合到日常生活的各个方面。尽管在许多领域有益，但这些内容也带来了重大风险，包括潜在的滥用、社会破坏和伦理问题。因此，检测由LAIMs生成的多媒体内容变得至关重要，相关研究也大幅增加。尽管如此，目前仍然存在一个明显的问题，即缺乏系统性的调查研究，专门关注检测LAIMs生成的多媒体内容。为了解决这个问题，我们提供了第一份全面涵盖现有研究的调查报告，重点关注检测LAIMs生成的多媒体内容（如文本、图像、视频、音频和多模态内容）。具体而言，我们引入了一种新颖的检测方法分类法，按媒体形式分类，并与纯检测（旨在提高检测性能）和应用场景对齐。

    The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
    
[^94]: 利用因果贝叶斯网络和知识图谱进行制造业根本原因分析的互动智能方法

    Interactive and Intelligent Root Cause Analysis in Manufacturing with Causal Bayesian Networks and Knowledge Graphs

    [https://arxiv.org/abs/2402.00043](https://arxiv.org/abs/2402.00043)

    本文提出了一个互动智能的制造业根本原因分析工具，通过结合电动汽车制造过程的专家知识和数据驱动的机器学习方法，利用大规模知识图谱进行推理并学习一个因果贝叶斯网络。

    

    电动汽车制造之中，根本原因分析(RCA)是识别故障原因的过程。传统上，RCA是依靠过程专家知识手工进行的。同时，传感器网络在制造过程中收集了大量的数据，利用这些数据进行RCA能够提高效率。然而，纯数据驱动的方法(如因果贝叶斯网络)在大规模的实际制造过程中具有扩展性问题，因为存在大量的潜在因果关系(CERs)。此外，纯数据驱动的方法有可能忽略已知的CERs或学习到错误的CERs。本文提出了一个互动智能的RCA工具，将电动汽车制造过程的专家知识和数据驱动的机器学习方法相结合。该工具利用制造过程的大规模知识图谱进行推理，同时学习一个因果贝叶斯网络。

    Root Cause Analysis (RCA) in the manufacturing of electric vehicles is the process of identifying fault causes. Traditionally, the RCA is conducted manually, relying on process expert knowledge. Meanwhile, sensor networks collect significant amounts of data in the manufacturing process. Using this data for RCA makes it more efficient. However, purely data-driven methods like Causal Bayesian Networks have problems scaling to large-scale, real-world manufacturing processes due to the vast amount of potential cause-effect relationships (CERs). Furthermore, purely data-driven methods have the potential to leave out already known CERs or to learn spurious CERs. The paper contributes by proposing an interactive and intelligent RCA tool that combines expert knowledge of an electric vehicle manufacturing process and a data-driven machine learning method. It uses reasoning over a large-scale Knowledge Graph of the manufacturing process while learning a Causal Bayesian Network. In addition, an I
    
[^95]: 使用马尔可夫决策过程优化工业设备的任务分配与预测性维护

    Optimized Task Assignment and Predictive Maintenance for Industrial Machines using Markov Decision Process

    [https://arxiv.org/abs/2402.00042](https://arxiv.org/abs/2402.00042)

    本文提出了一种使用马尔可夫决策过程的分布式决策方法，用于工业设备任务分配和预测性维护。该方法实现了任务分配和健康管理决策代理之间的信息共享，并将不确定性纳入决策过程中。通过详细的数学模型和案例研究，证明了该方法的有效性和实际适用性。

    

    本文考虑了一种分布式决策方法，用于制造任务分配和基于设备状况的预测性维护。我们的方法考虑了任务分配和健康管理决策代理之间的信息共享。我们提出了基于马尔可夫决策过程设计决策代理的方法。使用马尔可夫决策过程的主要优势在于考虑到了决策过程中的不确定性。本文提供了详细的数学模型，并给出了实际执行策略。为了证明我们提出的方法的有效性和实际适用性，我们还包括了一个基于开源铣床工具退化数据的详细数值案例研究。我们的案例研究表明，所提出的方法在成本参数选择方面具有灵活性，并且允许对决策过程进行离线计算和分析。

    This paper considers a distributed decision-making approach for manufacturing task assignment and condition-based machine health maintenance. Our approach considers information sharing between the task assignment and health management decision-making agents. We propose the design of the decision-making agents based on Markov decision processes. The key advantage of using a Markov decision process-based approach is the incorporation of uncertainty involved in the decision-making process. The paper provides detailed mathematical models along with the associated practical execution strategy. In order to demonstrate the effectiveness and practical applicability of our proposed approach, we have included a detailed numerical case study that is based on open source milling machine tool degradation data. Our case study indicates that the proposed approach offers flexibility in terms of the selection of cost parameters and it allows for offline computation and analysis of the decision-making p
    
[^96]: 空间-时间-需求聚类用于解决具有时间窗口的大规模车辆路径问题

    Spatial-temporal-demand clustering for solving large-scale vehicle routing problems with time windows

    [https://arxiv.org/abs/2402.00041](https://arxiv.org/abs/2402.00041)

    该论文提出了一种基于聚类的解决大规模车辆路径问题的方法，通过结合空间、时间、需求数据，将问题分解为子路径问题，然后分别解决，并通过局部搜索来改进整体解决方案。

    

    几种元启发式方法使用分解和修剪策略来解决车辆路径问题（VRP）的大规模实例。这些复杂性减少技术通常依赖于简单的、问题特定的规则。然而，可用数据的增长和计算机硬件的进步使得可以使用机器学习（ML）的基于数据的方法来改善解决方案算法的可扩展性。我们提出了一个分解-路径改进（DRI）框架，使用聚类将客户分组。其相似度度量指标包括客户的空间、时间和需求数据，并且被制定成反映问题目标函数和约束的形式。导致的子路径问题可以使用任何合适的算法独立地解决。我们在解决的子问题之间应用修剪的局部搜索（LS）来改进整体解。修剪基于在分解阶段获得的客户相似度信息。在计算研究中，我们参数化并比较现有的聚类方法。

    Several metaheuristics use decomposition and pruning strategies to solve large-scale instances of the vehicle routing problem (VRP). Those complexity reduction techniques often rely on simple, problem-specific rules. However, the growth in available data and advances in computer hardware enable data-based approaches that use machine learning (ML) to improve scalability of solution algorithms. We propose a decompose-route-improve (DRI) framework that groups customers using clustering. Its similarity metric incorporates customers' spatial, temporal, and demand data and is formulated to reflect the problem's objective function and constraints. The resulting sub-routing problems can independently be solved using any suitable algorithm. We apply pruned local search (LS) between solved subproblems to improve the overall solution. Pruning is based on customers' similarity information obtained in the decomposition phase. In a computational study, we parameterize and compare existing clustering
    
[^97]: 在STEM团队中促进公平：利用生成性人工智能实现包容和多样性

    Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity

    [https://arxiv.org/abs/2402.00037](https://arxiv.org/abs/2402.00037)

    本文探讨了利用生成性人工智能在STEM团队中促进多样性与包容性的潜力。主要通过包容性分析和个性化的自适应人工智能系统实现，在此基础上提出了四个政策建议，包括规范化合作评估、包容性分析、为社会认知研究筹集资金以及人工智能与人类团队合作进行包容性培训。

    

    合作是STEM的关键，多学科团队研究可以解决复杂问题。然而，STEM领域的不平等阻碍了其充分发展，因为在公平与包容团队中存在着持续的心理障碍。本文记录了STEM团队合作，并探讨了计算建模和生成性人工智能在促进STEM团队多样性和包容性方面的转变潜力。利用生成性人工智能，本文概述了两个推进多样性、公平与包容的主要领域。首先，通过包容性分析，规范化合作评估可以捕捉细粒度学习者行为。其次，个性化的自适应人工智能系统可以支持STEM团队的多样性与包容性。四个政策建议突显了人工智能的能力：规范化合作技能评估、包容性分析、为社会认知研究筹集资金、人工智能与人类团队合作进行包容性培训。研究人员、教育者、决策者可以共同构建一个公平的STEM生态系统。

    Collaboration is key to STEM, where multidisciplinary team research can solve complex problems. However, inequality in STEM fields hinders their full potential, due to persistent psychological barriers in underrepresented students' experience. This paper documents teamwork in STEM and explores the transformative potential of computational modeling and generative AI in promoting STEM-team diversity and inclusion. Leveraging generative AI, this paper outlines two primary areas for advancing diversity, equity, and inclusion. First, formalizing collaboration assessment with inclusive analytics can capture fine-grained learner behavior. Second, adaptive, personalized AI systems can support diversity and inclusion in STEM teams. Four policy recommendations highlight AI's capacity: formalized collaborative skill assessment, inclusive analytics, funding for socio-cognitive research, human-AI teaming for inclusion training. Researchers, educators, policymakers can build an equitable STEM ecosys
    
[^98]: 为什么预测准确性会随时间降低？云故障预测的不确定正样本学习

    Why does Prediction Accuracy Decrease over Time? Uncertain Positive Learning for Cloud Failure Prediction

    [https://arxiv.org/abs/2402.00034](https://arxiv.org/abs/2402.00034)

    本论文针对云故障预测中不确定正样本学习问题，设计了一种不确定正样本学习风险估算器（Uptake）方法，实验结果表明重新训练模型后预测准确性可能下降约9%。

    

    随着云计算的快速发展，各种软件服务被部署在云中。为了保证云服务的可靠性，之前的研究主要关注故障实例（磁盘、节点和交换机等）的预测。一旦预测的输出结果为正，就会采取应对措施快速解决潜在故障。在我们在Microsoft Azure的实际应用中，我们发现在重新训练模型之后，预测准确性可能会下降约9%。考虑到应对措施可能会导致不确定的正实例，因为在应对之后无法验证它们，这可能在更新预测模型时引入更多噪音。据我们所知，在实际云故障预测场景中，我们是首次发现这个不确定正样本学习问题。为了解决这个问题，我们设计了一种不确定正样本学习风险估算器（Uptake）方法。使用两个实际的磁盘故障数据集进行实验验证。

    With the rapid growth of cloud computing, a variety of software services have been deployed in the cloud. To ensure the reliability of cloud services, prior studies focus on failure instance (disk, node, and switch, etc.) prediction. Once the output of prediction is positive, mitigation actions are taken to rapidly resolve the underlying failure. According to our real-world practice in Microsoft Azure, we find that the prediction accuracy may decrease by about 9% after retraining the models. Considering that the mitigation actions may result in uncertain positive instances since they cannot be verified after mitigation, which may introduce more noise while updating the prediction model. To the best of our knowledge, we are the first to identify this Uncertain Positive Learning (UPLearning) issue in the real-world cloud failure prediction scenario. To tackle this problem, we design an Uncertain Positive Learning Risk Estimator (Uptake) approach. Using two real-world datasets of disk fai
    
[^99]: LF-ViT: 为提高图像识别效率减少视觉变换器中的空间冗余

    LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition

    [https://arxiv.org/abs/2402.00033](https://arxiv.org/abs/2402.00033)

    LF-ViT模型通过局部化和聚焦的方式减少计算需求，同时提高图像识别效率，显著降低了Deit-S模型的FLOPs。

    

    视觉变换器（ViT）在处理高分辨率图像时具有出色的准确性，但面临显著的空间冗余挑战，导致计算和内存需求增加。为解决这个问题，我们提出了局部化和聚焦视觉变换器（LF-ViT）。该模型通过有策略地削减计算需求，而不影响性能的方式进行操作。在局部化阶段，我们处理降低分辨率的图像；如果仍无法得出明确的预测结果，我们采用创新的邻近全局类别注意力（NGCA）机制，根据初步发现有效地识别和突出特定类别区域。随后，在聚焦阶段，我们利用原始图像中的指定区域来增强识别结果。独特的是，LF-ViT在两个阶段都使用一致的参数，确保无缝的端到端优化。我们的实证测试证实了LF-ViT的威力：它将Deit-S的FLOPs显著减少。

    The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by
    
[^100]: FIRST Robotics Competition中团队组建和冠军预测的综合框架：模型，算法和分析

    An Integrated Framework for Team Formation and Winner Prediction in the FIRST Robotics Competition: Model, Algorithm, and Analysis

    [https://arxiv.org/abs/2402.00031](https://arxiv.org/abs/2402.00031)

    本研究提出了一个集成框架，通过在团队形成之前的队员技能数据上进行优化，预测团队表现，填补了FIRST Robotics Competition领域的研究空白。

    

    本研究旨在开发一种分析方法，基于团队成员技能的数据，优化团队组建和预测团队表现，以在竞争环境中最大化团队效能。目前，科学文献中有几种优化团队表现和预测的方法。然而，大多数研究都使用了个体成员的细粒度技能统计或团队成员组合的约束条件。目前还没有研究涉及到高度约束的FIRST Robotics Competition领域。本研究旨在填补这一空白，通过提供一种分析方法，在允许这些约束条件的情况下，仅利用先前团队表现的指标来优化和预测团队表现。我们将这种方法应用于FIRST Robotics竞赛的选秀过程，这个领域的技能每年改变，团队成员也会发生变化。

    This research work aims to develop an analytical approach for optimizing team formation and predicting team performance in a competitive environment based on data on the competitors' skills prior to the team formation. There are several approaches in scientific literature to optimize and predict a team's performance. However, most studies employ fine-grained skill statistics of the individual members or constraints such as teams with a set group of members. Currently, no research tackles the highly constrained domain of the FIRST Robotics Competition. This research effort aims to fill this gap by providing an analytical method for optimizing and predicting team performance in a competitive environment while allowing these constraints and only using metrics on previous team performance, not on each individual member's performance. We apply our method to the drafting process of the FIRST Robotics competition, a domain in which the skills change year-over-year, team members change through
    
[^101]: 进化引导模拟: 人工智能还是人类智能：哪个先来？

    Evolution-Bootstrapped Simulation: Artificial or Human Intelligence: Which Came First?

    [https://arxiv.org/abs/2402.00030](https://arxiv.org/abs/2402.00030)

    人类创造了人工智能，但在一个由自然选择驱动的世界中，神经网络可能比人类更早进化，并且能够通过进化引导获得超级智能。

    

    人类创造了人工智能（AI），而不是相反。 这个陈述看似显而易见。 在这篇论文中，我们决定挑战这个陈述，以一种小而轻松的“Gedankenexperiment”。 我们提出一个简单的问题：在一个由自然选择驱动的世界中，神经网络或人类更有可能首先进化？ 我们比较了两者的Solomonoff-Kolmogorov-Chaitin复杂性，并发现神经网络（甚至是LLMs）比人类要简单得多。 此外，我们声称，并不需要任何复杂的人造设备存在才能有神经网络的存在。 神经网络可能早于人类作为一种基于化学反应或酶反应计算的自然存在的对象而进化。 现在我们知道神经网络可以通过图灵测试，还怀疑它们可能具备超级智能，我们问能否从纯粹的自然选择进化到通过进化引导的神经网络。

    Humans have created artificial intelligence (AI), not the other way around. This statement is deceptively obvious. In this note, we decided to challenge this statement as a small, lighthearted Gedankenexperiment. We ask a simple question: in a world driven by evolution by natural selection, would neural networks or humans be likely to evolve first? We compare the Solomonoff--Kolmogorov--Chaitin complexity of the two and find neural networks (even LLMs) to be significantly simpler than humans. Further, we claim that it is unnecessary for any complex human-made equipment to exist for there to be neural networks. Neural networks may have evolved as naturally occurring objects before humans did as a form of chemical reaction-based or enzyme-based computation. Now that we know that neural networks can pass the Turing test and suspect that they may be capable of superintelligence, we ask whether the natural evolution of neural networks could lead from pure evolution by natural selection to w
    
[^102]: 通过文化共识理论探索公众对负责任人工智能的看法

    Exploring Public Opinion on Responsible AI Through The Lens of Cultural Consensus Theory

    [https://arxiv.org/abs/2402.00029](https://arxiv.org/abs/2402.00029)

    应用文化共识理论，研究发现美国公众对负责任人工智能存在共享和对比的观点，这为开发者和政策制定者在决策过程中更有效地考虑个体差异和群体文化观点提供了重要参考。

    

    随着人工智能（AI）的社会影响不断增长，追求负责任的人工智能需要公众参与其发展和治理过程。这种参与对于捕捉各种观点和促进公平实践和结果至关重要。我们应用文化共识理论（CCT）对一份关于人工智能各个方面的全国代表性调查数据集进行分析，以了解美国关于负责任人工智能的信念和态度。我们的结果通过识别共享和对比的负责任人工智能观点提供了宝贵的洞见。此外，在制定重要决策和解决公众关切时，这些发现可作为开发者和政策制定者的关键参考点，使他们能够更有效地考虑个体差异和群体文化观点。

    As the societal implications of Artificial Intelligence (AI) continue to grow, the pursuit of responsible AI necessitates public engagement in its development and governance processes. This involvement is crucial for capturing diverse perspectives and promoting equitable practices and outcomes. We applied Cultural Consensus Theory (CCT) to a nationally representative survey dataset on various aspects of AI to discern beliefs and attitudes about responsible AI in the United States. Our results offer valuable insights by identifying shared and contrasting views on responsible AI. Furthermore, these findings serve as critical reference points for developers and policymakers, enabling them to more effectively consider individual variances and group-level cultural perspectives when making significant decisions and addressing the public's concerns.
    
[^103]: 使用SplitK工作分解加速W4A16量化推断的Triton融合内核

    Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition

    [https://arxiv.org/abs/2402.00025](https://arxiv.org/abs/2402.00025)

    本研究提出了一种加速W4A16量化推断的Triton融合内核的实现方法，通过使用SplitK工作分解实现解量化和GEMM操作，显著提高了瘦矩阵乘法的执行速度。

    

    我们提出了一种有效的融合矩阵乘法内核的实现，用于W4A16量化推断，在融合内核中执行解量化和GEMM操作，并使用SplitK工作分解。我们的实现对于基于foundation模型推断工作负载中的瘦矩阵乘法有所改进。具体而言，本文调查了瘦激活矩阵和方形权重矩阵之间的矩阵乘法类型。我们的结果显示，在一系列矩阵维度（包括llama-style模型中的维度，其中m < n = k）上，A100的平均速度提升了65％，H100的平均速度提升了124％（峰值为295％）。

    We propose an implementation of an efficient fused matrix multiplication kernel for W4A16 quantized inference, where we perform dequantization and GEMM in a fused kernel using a SplitK work decomposition. Our implementation shows improvement for the type of skinny matrix-matrix multiplications found in foundation model inference workloads. In particular, this paper surveys the type of matrix multiplication between a skinny activation matrix and a square weight matrix. Our results show an average of 65% speed improvement on A100, and an average of 124% speed improvement on H100 (with a peak of 295%) for a range of matrix dimensions including those found in a llama-style model, where m < n = k.
    
[^104]: LLaMA和ChatGPT嵌入在分子嵌入中的比较分析

    Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding

    [https://arxiv.org/abs/2402.00024](https://arxiv.org/abs/2402.00024)

    LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。

    

    目的：ChatGPT和LLaMA等大型语言模型在化学信息学领域越来越受到重视，特别是在解释Simplified Molecular Input Line Entry System (SMILES)方面。这些语言模型可以将SMILES字符串解码为向量表示，为理解化学图提供了一种新的方法。方法：我们研究了ChatGPT和LLaMA在嵌入SMILES字符串方面的性能。我们的评估集中在两个关键应用领域：分子性质（MP）预测和药物-药物相互作用（DDI）预测，这在药物开发和医疗保健中至关重要。结果：我们发现，使用LLaMA生成的SMILES嵌入在MP和DDI预测任务中表现优于ChatGPT。值得注意的是，基于LLaMA的SMILES嵌入在这两个预测任务中显示了与现有方法相当的结果。结论：在化学信息学中应用LLMs，特别是在利用SMILES进行嵌入方面，是可行的。

    Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
    
[^105]: 使用人工智能提高尼日利亚儿童疫苗接种率的ADVISE工具的部署：影响与经验教训

    Deploying ADVISER: Impact and Lessons from Using Artificial Intelligence for Child Vaccination Uptake in Nigeria

    [https://arxiv.org/abs/2402.00017](https://arxiv.org/abs/2402.00017)

    这个论文描述了我们与尼日利亚政府合作部署了ADVISE：AI-驱动的疫苗接种干预优化器，该框架基于整数线性规划，旨在最大化成功疫苗接种的累计概率，在尼日利亚是首次成功部署的AI工具链。

    

    每年有500多万五岁以下的儿童死于可预防或可治疗的医疗状况，其中绝大部分死亡发生在接种疫苗率低的欠发达国家。联合国的可持续发展目标之一（SDG 3）旨在结束新生儿和五岁以下儿童的可预防死亡。我们关注尼日利亚，该国婴儿死亡率令人震惊。特别是在尼日利亚，低疫苗接种率是每天导致2000多名五岁以下儿童死亡的主要原因。本文描述了我们与尼日利亚政府合作部署ADVISE：AI-驱动的疫苗接种干预优化器。该框架基于整数线性规划，旨在最大化成功疫苗接种的累计概率，在尼日利亚优化卫生干预配置方面是首次成功部署AI工具链。

    More than 5 million children under five years die from largely preventable or treatable medical conditions every year, with an overwhelmingly large proportion of deaths occurring in underdeveloped countries with low vaccination uptake. One of the United Nations' sustainable development goals (SDG 3) aims to end preventable deaths of newborns and children under five years of age. We focus on Nigeria, where the rate of infant mortality is appalling. In particular, low vaccination uptake in Nigeria is a major driver of more than 2,000 daily deaths of children under the age of five years. In this paper, we describe our collaboration with government partners in Nigeria to deploy ADVISER: AI-Driven Vaccination Intervention Optimiser. The framework, based on an integer linear program that seeks to maximize the cumulative probability of successful vaccination, is the first successful deployment of an AI-enabled toolchain for optimizing the allocation of health interventions in Nigeria. In this
    
[^106]: 通过多阶段不确定性感知推断来维护用户信任

    Maintaining User Trust Through Multistage Uncertainty Aware Inference

    [https://arxiv.org/abs/2402.00015](https://arxiv.org/abs/2402.00015)

    本文介绍了一种多阶段的人工智能部署方法，其中每个阶段都使用更准确的推理方法，以维护用户的信任。作者提出了一种量化模型不确定性的方法，以促进对推理结果的有信心的决策。该方法已经在印度的数千名棉农中进行了实际部署，并且可以适用于其他低资源环境中的人工智能部署。

    

    本文描述并评估了一种多阶段的人工智能部署方法。每个阶段都涉及一种更准确的推理方法，但每个阶段的开销也逐渐增加。在概述架构时，我们提出了一种量化模型不确定性的方法，以促进有信心的延期决策。该架构目前正在印度的数千名棉农中进行积极部署。然而，这个更广泛的想法也适用于在具有挑战性的低资源环境中进行人工智能部署的不断增长的领域。

    This paper describes and evaluates a multistage approach to AI deployment. Each stage involves a more accurate method of inference, yet engaging each comes with an increasing cost. In outlining the architecture, we present a method for quantifying model uncertainty that facilitates confident deferral decisions. The architecture is currently under active deployment to thousands of cotton farmers across India. The broader idea however is applicable to a growing sector of AI deployments in challenging low resources settings.
    
[^107]: 在黑客马拉松中集成生成式人工智能: 机遇，挑战和教育影响

    Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications

    [https://arxiv.org/abs/2401.17434](https://arxiv.org/abs/2401.17434)

    引入生成式人工智能的黑客马拉松在软件行业中发挥重要作用，并在教育领域带来了机遇和挑战。

    

    黑客马拉松和软件竞赛在软件行业中变得越来越重要，它们对组织和学生的创新和技能发展起到重要推动作用。这些平台使公司能够迅速原型化想法，而学生则获得丰富的学习经验，增强他们的实践技能。多年来，黑客马拉松已经从简单的竞争活动转变为重要的教育工具，将理论知识与实际问题解决相结合。将黑客马拉松纳入计算机科学和软件工程课程的整合旨在在合作的环境中对齐教育能力，通过产学合作促进同行之间的连接和丰富学习。然而，高级技术，特别是人工智能（AI）和机器学习的融合进黑客马拉松正在改变它们的结构和结果。这种演变带来了机遇，如增强的学习体验，

    Hackathons and software competitions, increasingly pivotal in the software industry, serve as vital catalysts for innovation and skill development for both organizations and students. These platforms enable companies to prototype ideas swiftly, while students gain enriched learning experiences, enhancing their practical skills. Over the years, hackathons have transitioned from mere competitive events to significant educational tools, fusing theoretical knowledge with real-world problem-solving. The integration of hackathons into computer science and software engineering curricula aims to align educational proficiencies within a collaborative context, promoting peer connectivity and enriched learning via industry-academia collaborations. However, the infusion of advanced technologies, notably artificial intelligence (AI), and machine learning, into hackathons is revolutionizing their structure and outcomes. This evolution brings forth both opportunities, like enhanced learning experienc
    
[^108]: SERL: 用于样本高效的机器人强化学习的软件套件

    SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning

    [https://arxiv.org/abs/2401.16013](https://arxiv.org/abs/2401.16013)

    这个论文介绍了SERL软件套件，它是一个用于样本高效的机器人强化学习的库。该库包含了一个离线深度强化学习方法、计算奖励和重置环境的方法，高质量的机器人控制器，以及一些具有挑战性的示例任务。这个软件套件的目标是解决机器人强化学习的难以使用和获取性的挑战。

    

    近年来，在机器人强化学习领域取得了显著进展，使得可以处理复杂的图像观察，实际训练，并结合辅助数据（如示范和先前经验）。然而，尽管取得了这些进展，机器人强化学习仍然难以使用。从实践者中认识到，这些算法的具体实现细节对性能的影响常常与算法选择同样重要（如果不是更重要）。我们认为，机器人强化学习被广泛采用以及进一步发展机器人强化学习方法的一个重要挑战是这些方法的相对难以获取性。为了解决这个挑战，我们开发了一个精心实现的库，其中包含了一种高效样本离线深度强化学习方法，以及计算奖励和重置环境的方法，针对广泛采用的机器人的高质量控制器，以及一些具有挑战性的示例任务。

    In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task
    
[^109]: 科学与深度学习中的可靠性和解释性

    Reliability and Interpretability in Science and Deep Learning

    [https://arxiv.org/abs/2401.07359](https://arxiv.org/abs/2401.07359)

    这篇论文强调了科学与深度学习中模型假设的重要性，并提供了对模型假设认识论复杂性的分析，同时结合标准错误分析与深度神经网络模型的特点，来评估模型可靠性。

    

    近年来，机器学习（ML）方法的可靠性问题日益重要，并且与此相关的不确定性分析已经激发了大量的研究。然而，大部分研究都仅将标准错误分析应用于深度神经网络（DNN）模型，这在很大程度上与标准科学建模有所不同。因此，有必要将标准错误分析与对DNN模型与标准科学建模的可能差异以及这些差异在可靠性评估中可能产生的影响的更深层次的认识论分析相结合。本文提供了几个贡献。首先，强调了模型假设（在ML和传统科学中均存在）在无理论科学的错觉下的普遍作用。其次，从（认识论的）复杂性角度分析了模型假设，同时还展示了模型假设在可靠性评估中的作用。

    In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown 
    
[^110]: 小型LLMs是弱工具学习者：多LLM代理

    Small LLMs Are Weak Tool Learners: A Multi-LLM Agent

    [https://arxiv.org/abs/2401.07324](https://arxiv.org/abs/2401.07324)

    本论文提出了一种新的策略，将大型语言模型代理（LLMs）的能力分解为计划器、调用器和总结器模块，以克服小型模型性能限制和工具更新的问题。

    

    大型语言模型（LLM）代理大大扩展了独立LLMs的能力，使它们能够与外部工具（例如API，函数）进行交互，并自主完成复杂任务。工具使用的挑战要求LLMs不仅能理解用户查询并生成答案，还要在任务规划、记忆管理、工具调用和结果总结方面表现出色。传统方法集中于训练单个具备所有这些功能的LLM，但在小型模型上会出现性能限制的问题，此外，当工具更新时，整个LLM可能需要重新训练。为了克服这些挑战，我们提出了一种新的策略，将上述能力分解为计划器、调用器和总结器。每个组件由一个单独的LLM实现，专注于特定的能力，并与其他组件合作完成任务。这种模块化框架便于进行个体更新和...

    Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and t
    
[^111]: 零样本自然语言视频定位中的常识推理

    Commonsense for Zero-Shot Natural Language Video Localization

    [https://arxiv.org/abs/2312.17429](https://arxiv.org/abs/2312.17429)

    本文研究了零样本自然语言-视频定位中常识推理的有效性，并提出了CORONET框架，该框架利用常识进行视频和生成的伪查询之间的桥接。实验证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。

    

    零样本自然语言-视频定位（NLVL）方法通过动态生成视频片段和伪查询注释，在仅用原始视频数据训练NLVL模型方面取得了令人期待的结果。然而，现有的伪查询经常缺乏对源视频的扎实基础，导致内容不结构化和不连贯。本文研究了零样本NLVL中常识推理的有效性。具体而言，我们提出了CORONET，一个零样本NLVL框架，通过常识增强模块桥接视频和生成的伪查询之间的差距。CORONET使用图卷积网络（GCN）来编码从知识图中提取的常识信息，条件是视频，以及交叉注意机制来增强编码视频和伪查询表示以进行定位。通过对两个基准数据集进行实证评估，我们证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。

    Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and w
    
[^112]: 复杂系统中的出现和因果关系：关于因果出现和相关定量研究的调查

    Emergence and Causality in Complex Systems: A Survey on Causal Emergence and Related Quantitative Studies

    [https://arxiv.org/abs/2312.16815](https://arxiv.org/abs/2312.16815)

    本文综述了关于因果出现和相关定量研究的最新进展，重点解决了量化因果出现和在数据中识别因果出现的两个问题，并建立了因果出现与人工智能之间的联系。

    

    出现和因果关系是理解复杂系统的两个基本概念，它们是相互关联的。出现一方面指的是宏观属性不能仅归因于个体属性的原因。另一方面，因果关系可以表现出出现，意味着随着抽象层次的增加，可能会出现新的因果定律。因果出现理论旨在连接这两个概念，甚至采用因果度量来量化出现。本文综述了因果出现定量理论和应用的最新进展。重点解决了两个关键问题：量化因果出现和在数据中识别因果出现。解决后者需要使用机器学习技术，从而建立了因果出现与人工智能之间的联系。我们强调了用于识别因果出现的架构与因果表示学习的共享。

    Emergence and causality are two fundamental concepts for understanding complex systems. They are interconnected. On one hand, emergence refers to the phenomenon where macroscopic properties cannot be solely attributed to the cause of individual properties. On the other hand, causality can exhibit emergence, meaning that new causal laws may arise as we increase the level of abstraction. Causal emergence theory aims to bridge these two concepts and even employs measures of causality to quantify emergence. This paper provides a comprehensive review of recent advancements in quantitative theories and applications of causal emergence. Two key problems are addressed: quantifying causal emergence and identifying it in data. Addressing the latter requires the use of machine learning techniques, thus establishing a connection between causal emergence and artificial intelligence. We highlighted that the architectures used for identifying causal emergence are shared by causal representation learn
    
[^113]: Parrot Captions Teach CLIP to Spot Text

    Parrot Captions Teach CLIP to Spot Text

    [https://arxiv.org/abs/2312.14232](https://arxiv.org/abs/2312.14232)

    本研究发现在图像-文本数据集LAION-2B中，标题密集地“模仿”图像中嵌入的视觉文本，导致CLIP模型受到文本定位偏差的影响。通过使用以“模仿标题”为标准的训练集，可以有效解决这一问题。

    

    尽管CLIP在很多视觉语言应用中是基础模型，但它存在严重的文本定位偏差。这种偏差导致CLIP模型“模仿”图像中嵌入的视觉文本，而忽视了真实的视觉语义。我们发现在最流行的图像-文本数据集LAION-2B中，标题也密集地“模仿”图像中嵌入的文本。我们的分析表明，大约50%的图像嵌入了视觉文本内容，而约30%的标题单词属于这些嵌入的视觉内容。基于这样的观察，我们彻底检查了不同发布版本的CLIP模型，并验证了视觉文本是衡量这些模型的LAION风格图像-文本相似性的主要因素。为了检查这些“模仿”的标题是否塑造了文本定位偏差，我们使用不同的“模仿标题”为标准，训练了一系列以LAION子集为基础的CLIP模型。我们展示了使用“模仿”标题进行训练可以很容易地解决文本定位偏差。

    Despite CLIP being the foundation model in numerous vision-language applications, the CLIP suffers from a severe text spotting bias. Such bias causes CLIP models to `Parrot' the visual text embedded within images while disregarding the authentic visual semantics. We uncover that in the most popular image-text dataset LAION-2B, the captions also densely parrot (spell) the text embedded in images. Our analysis shows that around 50% of images are embedded with visual text content, and around 30% of captions words are in these embedded visual content. Based on such observation, we thoroughly inspect the different released versions of CLIP models and verify that the visual text is the dominant factor in measuring the LAION-style image-text similarity for these models. To examine whether these parrot captions shape the text spotting bias, we train a series of CLIP models with LAION subsets curated by different parrot-caption-oriented criteria. We show that training with parrot captions easil
    
[^114]: 检测与分析注视以启动工业人机协作的研究

    Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration

    [https://arxiv.org/abs/2312.06643](https://arxiv.org/abs/2312.06643)

    通过分析参与者的注视行为，我们发现向协作机器人的注视可以作为启动联合活动的触发器。这一发现对于改进工业人机协作和操作体验具有重要意义。

    

    协作机器人（cobots）广泛应用于工业领域，但仍需进行深入研究以提升人机协作和操作体验。一种提高协作体验的潜在方法是基于操作者的自然线索调整协作机器人的行为。受到人际互动研究的启发，我们进行了一项巫师试验，以探究向协作机器人的注视是否能够成为启动联合活动的触发器。在这项研究中，37名参与者在进行组装任务时，我们分析了他们的注视行为。我们采用基于注视的注意力识别模型来确定参与者何时看向协作机器人。研究结果表明，在大多数情况下（84.88\%），联合活动前会出现对协作机器人的注视。此外，在整个组装过程中，参与者往往在联合活动期间注视协作机器人。据我们所知，

    Collaborative robots (cobots) are widely used in industrial applications, yet extensive research is still needed to enhance human-robot collaborations and operator experience. A potential approach to improve the collaboration experience involves adapting cobot behavior based on natural cues from the operator. Inspired by the literature on human-human interactions, we conducted a wizard-of-oz study to examine whether a gaze towards the cobot can serve as a trigger for initiating joint activities in collaborative sessions. In this study, 37 participants engaged in an assembly task while their gaze behavior was analyzed. We employ a gaze-based attention recognition model to identify when the participants look at the cobot. Our results indicate that in most cases (84.88\%), the joint activity is preceded by a gaze towards the cobot. Furthermore, during the entire assembly cycle, the participants tend to look at the cobot around the time of the joint activity. To the best of our knowledge, 
    
[^115]: EE-LLM: 大规模训练和推理具有三维并行性的早退出大型语言模型

    EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism

    [https://arxiv.org/abs/2312.04916](https://arxiv.org/abs/2312.04916)

    EE-LLM是一个用于大规模训练和推理早退出大型语言模型的框架，具有三维并行性和多项算法创新。研究发现EE-LLM在训练效率上表现出色，计算开销极小。

    

    我们提出了EE-LLM，这是一个用于大规模训练和推理早退出大型语言模型（LLM）的框架。虽然最近的研究已经初步证明了早退出在加速LLM推理方面的有效性，但EE-LLM通过支持大规模的三维并行性来推动早退出LLM的规模化。基于Megatron-LM构建的EE-LLM实现了各种算法创新和性能优化，以适应早退出，包括一种轻量级的方法，利用流水线并行性促进早退出训练目标的反向传播，利用原始流水线调度中的空闲资源进行与早退出层相关的计算的技术，以及两种与KV缓存兼容的早退出推理方法，用于自回归生成。我们的分析和实证研究表明，与忽略的计算开销相比，EE-LLM在训练效率上取得了很好的效果。

    We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared
    
[^116]: RLHF和IIA：倒置激励

    RLHF and IIA: Perverse Incentives

    [https://arxiv.org/abs/2312.01057](https://arxiv.org/abs/2312.01057)

    RLHF算法中的IIA假设导致了倒置激励，限制了查询格式和学习算法的创新。

    

    现有的基于人类反馈的强化学习算法（RLHF）可以激励与偏好不符的回应，因为它们基于假设无关概括的模型（IIA）。IIA引发的倒置激励阻碍了查询格式和学习算法的创新。

    Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
    
[^117]: ERASER: 通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习

    ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach

    [https://arxiv.org/abs/2311.16136](https://arxiv.org/abs/2311.16136)

    本文提出了ERASER，一种通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习。

    

    过去几年，机器学习作为一种服务（MLaaS）在支持基于机器学习的服务方面有着大量需求，以在各种应用领域提供革命性的用户体验。MLaaS基于从众多个体数据所有者收集的训练数据集训练的机器学习模型提供低推理延迟的推理服务。最近，为了保护数据所有者的隐私并遵守数据保护法的“被遗忘权（RTBF）”，许多机器去学习方法被提出，以在请求去学习时从训练模型中删除数据所有者的数据。然而，尽管它们具有很高的效率，几乎所有现有的机器去学习方法都独立处理去学习请求和推理请求，这不幸地引入了推理服务过时性的安全问题和机器去学习中极易遭受不必要的曝光的隐私漏洞。

    Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the "right to be forgotten (RTBF)" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the 
    
[^118]: 可信的大模型在视觉领域的研究综述

    Trustworthy Large Models in Vision: A Survey

    [https://arxiv.org/abs/2311.09680](https://arxiv.org/abs/2311.09680)

    这篇综述研究了可信的大模型在视觉领域的问题，包括人为误用、漏洞性、固有问题和可解释性，并提出了对应的挑战和对策，旨在推动可信的LMs与人类期望的对齐

    

    大模型（LMs）的快速发展在深度学习的各个领域中带来了革命性的进展，从自然语言处理（NLP）到计算机视觉（CV）都有显著的成绩。然而，由于其强大的性能但不可信的行为，LMs越来越受到学术界和工业界的挑战和批评，迫切需要可靠的方法加以缓解。尽管关于可信的NLP领域中的LMs已有大量文献，但系统研究关于CV领域中LMs的可信性的文献仍然缺乏。为了弥补这个空白，我们在本综述中总结了四个影响在视觉领域中使用可信LMs的相关问题，包括：1）人为误用，2）漏洞性，3）固有问题和4）可解释性。通过突出每个主题中的相应挑战、对策和讨论，我们希望本综述能够促进读者对这一领域的理解，推动LMs与人类期望的对齐

    The rapid progress of Large Models (LMs) has recently revolutionized various fields of deep learning with remarkable grades, ranging from Natural Language Processing (NLP) to Computer Vision (CV). However, LMs are increasingly challenged and criticized by academia and industry due to their powerful performance but untrustworthy behavior, which urgently needs to be alleviated by reliable methods. Despite the abundance of literature on trustworthy LMs in NLP, a systematic survey specifically delving into the trustworthiness of LMs in CV remains absent. In order to mitigate this gap, we summarize four relevant concerns that obstruct the trustworthy usage in vision of LMs in this survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4) interpretability. By highlighting corresponding challenge, countermeasures, and discussion in each topic, we hope this survey will facilitate readers' understanding of this field, promote alignment of LMs with human expectations and enab
    
[^119]: 将大型语言模型定性为知识密集型任务的理性化工具

    Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks

    [https://arxiv.org/abs/2311.05085](https://arxiv.org/abs/2311.05085)

    大型语言模型在知识密集型任务中的理性化能力有待探索，通过使用专家编写的示例，可以生成更受欢迎的基于世界知识的理性化方式。这些理性化方式需要进一步改进，在错误预测的理性化方面会损害人类对模型的信任。

    

    大型语言模型(LLMs)在几乎没有任务特定监督的情况下能够生成流畅的文本。然而，它们在提供基于知识密集型任务的充分理性支持方面的能力尚未得到充分探索。这类任务，比如常识性多项选择题，需要基于世界知识的理性来支持预测并推翻备选选项。我们通过使用专家编写的样例以少量样本的方式在自然语言中生成知识引导的理性化任务。令人惊讶的是，工人群体更喜欢基于知识的理性化方式，认为其具有事实性、充分性和全面性的反驳。虽然LLMs生成的理性化方式更受欢迎，但还需要在简洁性和新颖性方面进一步改进。在另一项研究中，我们展示了错误模型预测的理性化如何侵蚀人类对LLMs生成的理性化的信任。在这些观察的基础上，我们创建了一个两阶段的流程。

    Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline t
    
[^120]: SugarViT - 基于视觉转换和深度标签分布学习的多目标回归无人机图像，以甜菜疾病严重程度预测为例

    SugarViT -- Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet

    [https://arxiv.org/abs/2311.03076](https://arxiv.org/abs/2311.03076)

    SugarViT是一个基于视觉转换和深度标签分布学习的框架，用于自动化大规模田间图像的疾病严重程度评分。将遥感数据与环境参数结合，实现了对糖菜叶斑病严重程度的预测。

    

    遥感和人工智能是现代精准农业的关键技术。大规模田间图像的高效检索结合机器学习技术在物候学、除草、作物和病害控制等多个任务中取得了成功。本研究将介绍一个用于自动化大规模植物特定特征注释的机器学习框架，以糖菜叶斑病(CLS)的疾病严重程度评分为例。通过深度标签分布学习(DLDL)的概念、特殊损失函数和量身定制的模型架构，我们开发了一种名为SugarViT的高效视觉转换模型，用于疾病严重程度评分。本工作的一个创新之处是将遥感数据与实验场地的环境参数结合起来，用于疾病严重程度预测。尽管该模型在特定的用例上进行了评估，但尽可能保持通用性，可适用于各种基于图像的应用场景。

    Remote sensing and artificial intelligence are pivotal technologies of precision agriculture nowadays. The efficient retrieval of large-scale field imagery combined with machine learning techniques shows success in various tasks like phenotyping, weeding, cropping, and disease control. This work will introduce a machine learning framework for automatized large-scale plant-specific trait annotation for the use case disease severity scoring for Cercospora Leaf Spot (CLS) in sugar beet. With concepts of Deep Label Distribution Learning (DLDL), special loss functions, and a tailored model architecture, we develop an efficient Vision Transformer based model for disease severity scoring called SugarViT. One novelty in this work is the combination of remote sensing data with environmental parameters of the experimental sites for disease severity prediction. Although the model is evaluated on this special use case, it is held as generic as possible to also be applicable to various image-based 
    
[^121]: InstructRetro: 检索增强的预训练中指令调优

    InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining

    [https://arxiv.org/abs/2310.07713](https://arxiv.org/abs/2310.07713)

    InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。

    

    使用检索增强技术对自回归大型语言模型（LLM）进行预训练可以提高困惑度和事实准确性。然而，现有的预训练检索增强LLM的规模仍然有限（如Retro具有75亿个参数），这限制了指令调优和零样例泛化的效果。本文介绍了Retro 48B，这是目前规模最大的使用检索预训练的LLM。具体来说，我们使用检索技术从1.2万亿个标记中继续预训练一个43B的GPT模型，并借助Retro方法将其扩展到4800亿个参数。值得注意的是，所得到的基础模型Retro 48B在困惑度方面显著优于仅使用1.2万亿个标记进行训练的43B GPT模型，且只增加了2.58%的GPU使用时间，展示了该方法的显著扩展潜力。在对Retro进行指令调优后，InstructRetro在各种零样例任务上表现出显著的改进。

    Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
    
[^122]: 通过去噪扩散概率模型进行生成性量子机器学习

    Generative quantum machine learning via denoising diffusion probabilistic models

    [https://arxiv.org/abs/2310.05866](https://arxiv.org/abs/2310.05866)

    通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。

    

    深度生成模型是计算机视觉、文本生成和大型语言模型的关键技术。最近，由于其能够生成多样化和高质量的样本，以及结构灵活、训练简单的特点，去噪扩散概率模型（DDPMs）在许多计算机视觉任务中受到了广泛关注。量子生成模型利用纠缠和叠加的能力为学习经典和量子数据带来了新的见解。受经典模型的启发，我们提出了“量子去噪扩散概率模型”（QuDDPM），以实现对量子数据的高效可训练的生成学习。QuDDPM采用足够层数的电路来保证表达能力，同时引入多个中间训练任务，将目标分布与噪声之间的插值作为训练过程，以避免贫瘠平原并保证高效的训练。我们给出了学习误差的上界和...（未完待续）

    Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
    
[^123]: 关于加速SE(3)不变空间中基于扩散的分子构象生成的研究

    On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space

    [https://arxiv.org/abs/2310.04915](https://arxiv.org/abs/2310.04915)

    本文研究在SE(3)不变空间中加速扩散机制，提出新的加速方案，可以以50倍到100倍的速度生成高质量的分子构象。

    

    在SE(3)不变空间中，基于扩散的生成模型在分子构象生成方面表现出有希望的性能，但通常需要解决具有数千个更新步骤的随机微分方程(SDEs)。迄今为止，如何在SE(3)不变空间中明确有效地加速这个过程仍然不清楚，这极大地阻碍了其在实际环境中的广泛应用。在本文中，我们通过现有方法引起的近似误差来系统研究SE(3)不变空间中的扩散机制。因此，我们在投影微分方程的上下文中发展了更精确的SE(3)内的近似方法。我们还提供了理论分析，并通过实证验证将超参数与这些误差联系起来。总体而言，我们提出了一种新的加速方案，用于在SE(3)不变空间中生成分子构象。实验结果表明，我们的方案可以以50倍到100倍的速度生成高质量的构象。

    Diffusion-based generative models in SE(3)-invariant space have demonstrated promising performance in molecular conformation generation, but typically require solving stochastic differential equations (SDEs) with thousands of update steps. Till now, it remains unclear how to effectively accelerate this procedure explicitly in SE(3)-invariant space, which greatly hinders its wide application in the real world. In this paper, we systematically study the diffusion mechanism in SE(3)-invariant space via the lens of approximate errors induced by existing methods. Thereby, we develop more precise approximate in SE(3) in the context of projected differential equations. Theoretical analysis is further provided as well as empirical proof relating hyper-parameters with such errors. Altogether, we propose a novel acceleration scheme for generating molecular conformations in SE(3)-invariant space. Experimentally, our scheme can generate high-quality conformations with 50x--100x speedup compared to
    
[^124]: 深度学习方法在校准光度立体及其他领域的应用

    Deep Learning Methods for Calibrated Photometric Stereo and Beyond

    [https://arxiv.org/abs/2212.08414](https://arxiv.org/abs/2212.08414)

    本文综述了基于深度学习的校准光度立体方法，并总结了它们在最广泛使用的基准数据集上的性能，展示了深度学习在光度立体领域的高级表现。

    

    光度立体是从多个具有不同阴影线索的图像中恢复物体的表面法线，即建立每个像素点的表面方向和亮度之间的关系模型。光度立体在每个像素点的分辨率和细节重建方面具有优势。然而，由于非兰伯特表面反射引起的非线性关系，光度立体是一个复杂的问题。最近，各种深度学习方法在非兰伯特表面的光度立体环境中展现了强大的能力。本文综述了现有的基于深度学习的校准光度立体方法。首先，我们从不同的角度对这些方法进行了分析，包括输入处理、监督和网络架构。然后，我们总结了深度学习光度立体模型在最广泛使用的基准数据集上的性能。这证明了基于深度学习的光度立体方法的先进性能。

    Photometric stereo recovers the surface normals of an object from multiple images with varying shading cues, i.e., modeling the relationship between surface orientation and intensity at each pixel. Photometric stereo prevails in superior per-pixel resolution and fine reconstruction details. However, it is a complicated problem because of the non-linear relationship caused by non-Lambertian surface reflectance. Recently, various deep learning methods have shown a powerful ability in the context of photometric stereo against non-Lambertian surfaces. This paper provides a comprehensive review of existing deep learning-based calibrated photometric stereo methods. We first analyze these methods from different perspectives, including input processing, supervision, and network architecture. We summarize the performance of deep learning photometric stereo models on the most widely-used benchmark data set. This demonstrates the advanced performance of deep learning-based photometric stereo meth
    
[^125]: 使用机器学习技术进行高性能计算应用中的数据分区的块大小估计

    Block size estimation for data partitioning in HPC applications using machine learning techniques

    [https://arxiv.org/abs/2211.10819](https://arxiv.org/abs/2211.10819)

    这项工作介绍了一种名为BLEST-ML的方法，通过机器学习技术进行块大小估计，以加速并行数据密集型应用和提高可伸缩性。这种方法在分布式计算库dislib上进行了评估。

    

    高性能计算基础设施和框架的广泛使用促使对数据分区技术和策略的兴趣不断增长。实际上，应用性能可能受数据分区方式的影响，而这又取决于选择的数据块大小，即块大小。因此，找到一种有效的分区即合适的块大小是加速并行数据密集型应用和提高可伸缩性的关键策略。本文描述了一种名为BLEST-ML（通过机器学习的块大小估计）的方法，该方法依赖于监督机器学习技术进行块大小估计。提出的方法通过针对基于PyCOMPSs框架的高度专注于机器学习算法的分布式计算库dislib设计的实现进行评估。我们通过广泛的实验评估来评估提供的实现的有效性。

    The extensive use of HPC infrastructures and frameworks for running dataintensive applications has led to a growing interest in data partitioning techniques and strategies. In fact, application performance can be heavily affected by how data are partitioned, which in turn depends on the selected size for data blocks, i.e. the block size. Therefore, finding an effective partitioning, i.e. a suitable block size, is a key strategy to speed-up parallel data-intensive applications and increase scalability. This paper describes a methodology, namely BLEST-ML (BLock size ESTimation through Machine Learning), for block size estimation that relies on supervised machine learning techniques. The proposed methodology was evaluated by designing an implementation tailored to dislib, a distributed computing library highly focused on machine learning algorithms built on top of the PyCOMPSs framework. We assessed the effectiveness of the provided implementation through an extensive experimental evaluat
    
[^126]: 弱相关回归方法：快速揭示高维聚合数据中隐藏的随机动态

    Weak Collocation Regression method: fast reveal hidden stochastic dynamics from high-dimensional aggregate data

    [https://arxiv.org/abs/2209.02628](https://arxiv.org/abs/2209.02628)

    该论文提出了一种名为弱相关回归（WCR）方法的新方法，能够在没有轨迹的情况下快速有效地揭示高维聚合数据中的隐藏随机动态。

    

    从随机数据中揭示隐藏的动态是一个具有挑战性的问题，因为随机性参与了数据的演化过程。当随机数据的轨迹在许多情况下缺失时，这个问题变得异常复杂。在这里，我们提出了一种基于弱形式Fokker-Planck（FP）方程的方法，用于在没有轨迹的情况下有效地建模随机数据的动态。该方程控制着布朗运动中密度函数的演化。我们将高斯函数的相关性作为FP方程的弱形式的测试函数，将导数转化为高斯函数，从而通过对数据的期望和来近似弱形式。通过未知项的字典表示，构建一个线性系统，然后通过回归解决，揭示数据的未知动态。因此，我们将该方法命名为弱相关回归（WCR）方法，因为它有三个关键的组成部分：弱形式，相关性，回归。

    Revealing hidden dynamics from the stochastic data is a challenging problem as randomness takes part in the evolution of the data. The problem becomes exceedingly complex when the trajectories of the stochastic data are absent in many scenarios. Here we present an approach to effectively modeling the dynamics of the stochastic data without trajectories based on the weak form of the Fokker-Planck (FP) equation, which governs the evolution of the density function in the Brownian process. Taking the collocations of Gaussian functions as the test functions in the weak form of the FP equation, we transfer the derivatives to the Gaussian functions and thus approximate the weak form by the expectational sum of the data. With a dictionary representation of the unknown terms, a linear system is built and then solved by the regression, revealing the unknown dynamics of the data. Hence, we name the method with the Weak Collocation Regression (WCR) method for its three key components: weak form, c
    
[^127]: 在医疗保健中实现公平的机器学习：一项综述

    Fair Machine Learning in Healthcare: A Review

    [https://arxiv.org/abs/2206.14397](https://arxiv.org/abs/2206.14397)

    这篇综述论文研究了机器学习在医疗保健领域的公平问题。通过采用基于分配公正原则的框架，将公平问题分为资源平均分配和性能平等两个类别，并从机器学习的角度对相关的公正度量进行了批判性的回顾。论文还讨论了机器学习生命周期各个阶段的偏见和缓解策略，探讨了偏见与其对策之间的关系。

    

    医疗数据的数字化与计算能力的进步推动了机器学习在医疗保健领域的应用。然而，这些方法可能会加剧或甚至加重现有的差异，导致资源不均和不同人群之间的诊断准确性不一致等公平问题。解决这些公平问题对于防止社会不公正的进一步巩固至关重要。在本综述中，我们分析了机器学习与医疗保健不公平的交叉点。我们采用了一个基于分配公正原则的框架，将公平问题分为两个不同的类别：资源平均分配和性能平等。我们从机器学习的角度对相关的公正度量进行了批判性的回顾，并讨论了机器学习生命周期各个阶段的偏见和缓解策略，探讨了偏见与其对策之间的关系。

    The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problem is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in machine learning and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a machine learning standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. T
    
[^128]: 一种多级对称微分方程模型用于学习蛋白质-配体结合动力学

    A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])

    [http://arxiv.org/abs/2401.15122](http://arxiv.org/abs/2401.15122)

    提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。

    

    在药物发现中，蛋白质-配体结合的分子动力学（MD）模拟提供了一种强大的工具，用于预测结合亲和力，估计运输性能和探索口袋位点。通过改进数值方法以及最近通过机器学习（ML）方法增强MD模拟的效率已经有了很长的历史。然而，仍然存在一些挑战，例如准确建模扩展时间尺度的模拟。为了解决这个问题，我们提出了NeuralMD，这是第一个能够促进数值MD并提供准确的蛋白质-配体结合动力学模拟的ML辅助工具。我们提出了一个合理的方法，将一种新的物理信息多级对称框架纳入模型中。具体而言，我们提出了（1）一个使用向量框架满足群对称性并捕获多级蛋白质-配体相互作用的BindingNet模型，以及（2）一个增强的神经微分方程求解器，学习轨迹的演化。

    In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
    
[^129]: Hi-Core: 面向连续强化学习的层次化知识迁移

    Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])

    [http://arxiv.org/abs/2401.15098](http://arxiv.org/abs/2401.15098)

    Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。

    

    连续强化学习（Continual Reinforcement Learning, CRL）赋予强化学习智能体从一系列任务中学习的能力，保留先前的知识并利用它来促进未来的学习。然而，现有的方法往往专注于在类似任务之间传输低层次的知识，忽视了人类认知控制的层次结构，导致在各种任务之间的知识迁移不足。为了增强高层次的知识迁移，我们提出了一种名为Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)的新框架，它由两层结构组成：1) 利用大型语言模型（Large Language Model, LLM）的强大推理能力设定目标的高层策略制定和2) 通过强化学习按照高层目标导向的低层策略学习。此外，构建了一个知识库（策略库）来存储可以用于层次化知识迁移的策略。在MiniGr实验中进行了实验。

    Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
    
[^130]: 过去、现在、未来：对UMBRELLA物联网试验平台中人工智能应用案例的全面探索

    Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed. (arXiv:2401.13346v1 [cs.NI])

    [http://arxiv.org/abs/2401.13346](http://arxiv.org/abs/2401.13346)

    UMBRELLA是一个大规模的物联网试验平台，具有多个应用案例，包括自动街灯监控、数字孪生环境、联邦学习框架和容器化应用入侵检测。未来，UMBRELLA还有潜力用于智能城市和多机器人群感知应用。

    

    UMBRELLA是一个大规模的开放式物联网生态系统，包括200多个多传感器多无线节点、20个协作机器人和支持边缘智能的设备。本文提供了UMBRELLA在实际物联网系统中已实现和潜在的人工智能能力的指南。详细介绍了四个现有的UMBRELLA应用程序：1）用于检测问题并触发维护警报的自动街灯监控；2）提供增强的空气质量感知和降低成本的建筑环境数字孪生；3）用于减少通信开销的大规模联邦学习框架；4）用于识别恶意活动的容器化应用入侵检测。此外，还概述了UMBRELLA在未来智能城市和多机器人群感知应用中的潜力，增强了语义通信和多智能体规划。最后，为了实现上述用例，我们讨论了UMBRELLA的要求。

    UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative robots, and edge-intelligence-enabled devices. This paper provides a guide to the implemented and prospective artificial intelligence (AI) capabilities of UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are presented in detail: 1) An automated streetlight monitoring for detecting issues and triggering maintenance alerts; 2) A Digital twin of building environments providing enhanced air quality sensing with reduced cost; 3) A large-scale Federated Learning framework for reducing communication overhead; and 4) An intrusion detection for containerised applications identifying malicious activities. Additionally, the potential of UMBRELLA is outlined for future smart city and multi-robot crowdsensing applications enhanced by semantic communications and multi-agent planning. Finally, to realise the above use-cases we disc
    
[^131]: 平衡自博弈训练中角色的AI能力与后悔匹配+的方法

    Balancing the AI Strength of Roles in Self-Play Training with Regret Matching+. (arXiv:2401.12557v1 [cs.AI])

    [http://arxiv.org/abs/2401.12557](http://arxiv.org/abs/2401.12557)

    通过引入后悔匹配+，本论文提出了一种简单的方法来平衡自博弈训练中不同角色的AI能力。

    

    在为涵盖多个角色的游戏训练人工智能时，开发一个能够控制游戏内任意角色的通用模型是一个可行的选择。这种策略不仅可以在训练阶段节省计算资源和时间，而且可以减少部署时的资源需求。然而，训练这样一个通用模型经常遇到控制不同角色时能力不平衡的挑战。本文提出了一种基于后悔匹配+的简单方法，可以促进模型在控制不同角色时更平衡的性能。

    When training artificial intelligence for games encompassing multiple roles, the development of a generalized model capable of controlling any character within the game presents a viable option. This strategy not only conserves computational resources and time during the training phase but also reduces resource requirements during deployment. training such a generalized model often encounters challenges related to uneven capabilities when controlling different roles. A simple method is introduced based on Regret Matching+, which facilitates a more balanced performance of strength by the model when controlling various roles.
    
[^132]: 强化学习代理中的新兴支配等级

    Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])

    [http://arxiv.org/abs/2401.12258](http://arxiv.org/abs/2401.12258)

    本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。

    

    现代强化学习算法在各种任务中能够胜过人类。多智能体强化学习(MARL)设置提出了额外的挑战，成功的混合动机代理协作取决于个体和群体目标之间的微妙平衡。社会习惯和规范，往往受到人类机构的启发，被用作实现这种平衡的工具。在本文中，我们研究了一种基本且经过深入研究的社会习惯，即支配等级，它在动物和人类社会中都存在。我们将支配等级的行为理论应用于人工智能代理，并尽可能少地修改现有的术语和定义。我们证明，在没有明确编程或内在奖励的情况下，强化学习代理的群体能够发明、学习、实施和传递支配等级给新的群体。所产生的支配等级有一个

    Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
    
[^133]: 通过思维方程蒸馏改进小型语言模型的数学推理能力

    Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11864](http://arxiv.org/abs/2401.11864)

    本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。

    

    本研究解决了将先进的大型语言模型（LLMs）的数学推理能力压缩到具有小于十亿参数的小型语言模型（SLMs）中的挑战，同时不损害性能。我们引入了一种新颖的思维方程蒸馏（EoTD）技术，将推理过程封装为基于方程的表示，构建了一个EoTD数据集来对SLMs进行微调。此外，我们提出了集合思维蒸馏（ETD）框架，以提升SLMs的推理性能。这包括创建一个包含多个思维过程（包括思维链、思维程序和思维方程）的推理数据集，并将其用于微调。我们的实验证明，EoTD显著提升了SLMs的推理能力，而ETD使这些模型实现了最先进的推理性能。

    This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
    
[^134]: 走向异质图学习：进展与未来

    Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])

    [http://arxiv.org/abs/2401.09769](http://arxiv.org/abs/2401.09769)

    本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。

    

    图是用来模拟现实世界实体之间复杂关系的结构化数据。最近，异质图，其中连接的节点往往具有不同的标签或不同的特征，引起了广泛关注并找到了许多应用。与此同时，人们也在不断努力推进从异质图中学习。虽然有关该主题的调查存在，但它们只关注于异质图神经网络（GNNs），而忽略了异质图学习的其他子主题。在本调查中，我们全面回顾了关于从带有异质性的图中学习的现有研究。首先，我们收集了180多篇论文，介绍了该领域的发展。然后，我们根据层次分类法对现有方法进行了系统分类，包括学习策略、模型架构和实际应用。最后，我们讨论了现有研究的主要挑战，并突出了未来研究的潜在方向。

    Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
    
[^135]: AGI系统的元提示

    Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.11482](http://arxiv.org/abs/2311.11482)

    本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。

    

    本文介绍了元提示(meta prompting)的全面研究，这是一种创新技术，重新塑造了大型语言模型(LLMs)、多模态基础模型和人工智能系统在问题解决和数据解释方面的利用。基于类型理论和范畴论，元提示注重信息的结构和句法，而不是传统以内容为中心的方法。本文探讨了元提示的形式定义，并将其与少样本提示(few-shot prompting)区分开来，并强调其在各种人工智能应用中的有效性。重点关注将元提示扩展到复杂推理任务上，展示如何将复杂问题拆分成较为简单的子问题，提高令牌效率，并使问题求解的比较更加公平，尤其是与少样本示例方法相比。此外，本文还引入了元提示用于提示任务，允许LLMs以迭代的元编程形式自动生成新的提示。

    This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
    
[^136]: 在化学合成中的反应条件推荐中，检索增强生成代理

    Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2311.10776](http://arxiv.org/abs/2311.10776)

    本研究提出了一种转变性的人工智能代理，利用检索增强生成（RAG）技术自动化化学中的反应条件推荐（RCR）任务，通过模拟专家化学家的策略，使用大型语言模型（LLM）和新反应指纹，显著优于传统人工智能。此系统可以减轻化学家的工作负担，使他们能够更专注于更基础和创造性的科学问题。

    

    最近的人工智能研究为化学社会中的自动化化学反应铺平了一个有前途的未来。本研究提出了一种转变性的人工智能代理，利用检索增强生成（RAG）技术自动化化学中的反应条件推荐（RCR）任务。通过模拟专家化学家的搜索和分析策略，该代理使用大型语言模型（LLM）来查询分子数据库，并从在线文献中提取关键数据。此外，该人工智能代理还配备了我们为RCR任务开发的新反应指纹。由于RAG技术的使用，我们的代理使用更新的在线数据库作为知识源，显著优于仅受其训练数据固定知识限制的传统人工智能。由此产生的系统可以显著减轻化学家的工作负担，使他们能够更专注于更基础和创造性的科学问题。这一重大进展将计算技术与化学社会更紧密联系起来。

    Recent artificial intelligence (AI) research plots a promising future of automatic chemical reactions within the chemistry society. This study presents a transformative AI agent that automates the reaction condition recommendation (RCR) task in chemistry using retrieval-augmented generation (RAG) technology. By emulating expert chemists search and analysis strategies, the agent employs large language models (LLMs) to interrogate molecular databases and distill critical data from online literature. Further, the AI agent is equipped with our novel reaction fingerprint developed for the RCR task. Thanks to the RAG technology, our agent uses updated online databases as knowledge sources, significantly outperforming conventional AIs confined to the fixed knowledge within its training data. The resulting system can significantly reduce chemists workload, allowing them to focus on more fundamental and creative scientific problems. This significant advancement brings closer computational techn
    
[^137]: 进化桌面游戏设计：以“风险游戏”为案例研究

    Evolutionary Tabletop Game Design: A Case Study in the Risk Game. (arXiv:2310.20008v1 [cs.AI])

    [http://arxiv.org/abs/2310.20008](http://arxiv.org/abs/2310.20008)

    本研究提出了进化游戏设计方法的扩展，通过生成《风险》游戏的新变体来验证该方法。结果显示生成的新变体拥有更小的地图和更短的比赛时间，并产生更加平衡的游戏对局。

    

    手动创造和评估游戏是一项艰巨而费时的任务。程序生成内容可以通过创建游戏元素来提供帮助，但通常不能生成完整的游戏。进化游戏设计将进化算法与自动化测试相结合，已用于创建具备简单设备的新颖桌面游戏；然而，原有方法并不包括带有骰子、卡牌和地图等复杂桌面游戏。本研究提出了将该方法扩展到桌面游戏的方式，并通过生成《风险》这款军事策略游戏的变体来评估该过程。我们使用遗传算法进化所选参数，并利用规则中心代理测试游戏，并使用多个质量标准评估生成的新变体。结果显示我们创造了原始游戏的新变体，这些变体具有较小的地图，导致比赛时间更短。此外，这些变体产生了更加平衡的比赛。

    Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, main
    
[^138]: 基于拍卖的调度

    Auction-Based Scheduling. (arXiv:2310.11798v1 [cs.AI])

    [http://arxiv.org/abs/2310.11798](http://arxiv.org/abs/2310.11798)

    该论文提出了一种基于拍卖的调度框架，用于解决多目标决策问题。该框架的创新之处在于将每个目标的实现分配给单独的策略，并且可以独立创建、修改和替换这些策略。使用拍卖机制来解决冲突和组合策略，确保长期的调度公平性。

    

    许多顺序决策任务需要满足多个部分矛盾的目标。现有方法是整体化的，即通过一个函数来选择一系列动作来满足所有目标。我们提出了基于拍卖的调度，这是一个模块化的多目标决策框架。每个目标都使用单独的策略来实现，这些策略可以独立创建、修改和替换。可以理解的是，具有冲突目标的不同策略可能在给定时间选择冲突的动作。为了解决冲突和组合策略，我们采用了一种新颖的基于拍卖的机制。我们给每个策略分配一个有限的预算，在每一步，策略同时从可用的预算中出价来获取调度和选择动作的特权。策略使用其出价来表达调度的紧迫性，有限的预算确保了长期的调度公平性。

    Many sequential decision-making tasks require satisfaction of multiple, partially contradictory objectives. Existing approaches are monolithic, namely all objectives are fulfilled using a single policy, which is a function that selects a sequence of actions. We present auction-based scheduling, a modular framework for multi-objective decision-making problems. Each objective is fulfilled using a separate policy, and the policies can be independently created, modified, and replaced. Understandably, different policies with conflicting goals may choose conflicting actions at a given time. In order to resolve conflicts, and compose policies, we employ a novel auction-based mechanism. We allocate a bounded budget to each policy, and at each step, the policies simultaneously bid from their available budgets for the privilege of being scheduled and choosing an action. Policies express their scheduling urgency using their bids and the bounded budgets ensure long-run scheduling fairness. We lay 
    
[^139]: 通过子网络注入归纳偏置

    Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])

    [http://arxiv.org/abs/2310.10899](http://arxiv.org/abs/2310.10899)

    通过子网络注入归纳偏置，这项研究探索了理解和控制神经网络行为的方法。通过发现功能子网络并利用它们，可以显著减少训练模型所需的数据量。

    

    尽管人工神经网络在各种任务上取得了最近的成功，但对于这些模型的精确解决方案，我们几乎没有知识或控制能力。注入归纳偏置--对一些解决方案偏好--是理解和控制这些模型行为的一个有前景的途径。已经进行了大量工作来研究模型固有的归纳偏置，并通过手动设计的结构或精心策划的训练方式注入不同的归纳偏置。在这项工作中，我们探索了一种更机械的方法：子任务归纳。我们的方法发现了一个在训练模型中实现特定子任务的功能子网络，并使用它来注入对利用该子任务的解决方案的归纳偏置。子任务归纳灵活高效，在两个实验中我们证明了它的有效性。

    Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a
    
[^140]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^141]: SELF：基于语言驱动的大型语言模型自主进化

    SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00533](http://arxiv.org/abs/2310.00533)

    SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。

    

    大型语言模型（LLM）展示了在多个领域中的卓越适应能力。然而，实现人类水平的学习和推动自主人工智能的关键——模型自主进化的路径仍然未知。我们引入了一种创新的方法，名为"SELF"（带有语言反馈的自主进化）。这种方法使LLM能够不断地自我进化。此外，SELF利用语言反馈作为一种多功能、全面的评估工具，精确定位响应改进的领域，并提高自主进化训练的稳定性。SELF首先进行元技能学习，专注于自我反馈和自我精炼。这些元技能是关键，引导模型在自制数据的持续训练周期中进行后续的自我进化，从而增强其内在能力。在给定无标签指令的情况下，SELF使模型具备了能够...

    Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
    
[^142]: HyperMask: 自适应的基于超网络的掩码用于持续学习

    HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])

    [http://arxiv.org/abs/2310.00113](http://arxiv.org/abs/2310.00113)

    HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。

    

    当人工神经网络在多个任务上顺序训练时，往往会出现灾难性遗忘的问题。为了克服这个问题，已经存在许多持续学习策略，其中最有效的之一是基于超网络的方法。超网络根据任务的特征生成目标模型的权重。然而，该模型的主要限制是超网络对于每个任务可以产生完全不同的网络结构，因此每个任务都是单独解决的。模型在学习后续任务时不使用之前任务所关联的网络信息，并实际上产生了新的网络架构。为了解决这个问题，我们使用了彩票票证假设，该假设认为存在稀疏的子网络（即中奖票），可以保持完整网络的性能。在本文中，我们提出了一种名为HyperMask的方法，该方法为所有任务训练一个单一网络。超网络产生半二进制掩码，以获取目标子网络。

    Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
    
[^143]: 使用大型语言模型生成、验证和应用用户意图分类方法

    Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])

    [http://arxiv.org/abs/2309.13063](http://arxiv.org/abs/2309.13063)

    通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。

    

    日志数据可以揭示用户与网络搜索服务的交互方式、用户的需求以及满意程度等宝贵信息。然而，分析日志数据中的用户意图并不容易，尤其是对于新的网络搜索形式，如人工智能驱动的聊天。为了理解日志数据中的用户意图，我们需要一种能够用有意义的分类方式标记它们的方法，以捕捉其多样性和动态性。现有的方法依赖于手动或基于机器学习的标注，这些方法对于大型且不断变化的数据集而言，要么代价高昂要么不够灵活。我们提出了一种使用大型语言模型(LLM)的新方法，这种模型能够生成丰富且相关的概念、描述和示例来表示用户意图。然而，使用LLM生成用户意图分类并将其应用于日志分析可能存在两个主要问题：这样的分类得不到外部验证，并且可能存在不良的反馈回路。为了克服这些问题，我们提出了一种新的方法，通过人工专家和评估者来验证。

    Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
    
[^144]: 通过优势调节使用动态规划增强决策Transformer

    ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning. (arXiv:2309.05915v1 [cs.LG])

    [http://arxiv.org/abs/2309.05915](http://arxiv.org/abs/2309.05915)

    这篇论文提出了一种通过将动态规划应用于决策Transformer来增强其能力的方法。作者提出了三个步骤来实现这一目标：使用样本内值迭代获得近似值函数，结合估计的优势评估动作质量，并训练ACT生成基于估计优势的动作。该方法在测试中表现出良好的性能。

    

    决策Transformer (DT) 利用表达丰富的序列建模技术来执行动作生成，已成为离线策略优化的一种有前景的方法。然而，DT 生成的动作是基于期望未来回报的条件，已知具有某些弱点，比如易受环境随机性影响。为了克服DT的弱点，我们提出了在DT中增加动态规划能力的方法。我们的方法包括三个步骤。首先，我们使用样本内值迭代来获得近似值函数，这涉及到MDP结构上的动态规划。第二，我们结合估计的优势来评估动作的质量。我们引入了两种优势估计器，分别适用于不同的任务。第三，我们训练了一个以估计的优势为条件生成动作的优势条件Transformer (ACT)。最后，在测试阶段，ACT根据所需的优势生成动作。我们的评估结果表明...

    Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation resul
    
[^145]: 用编码-解码器进行可识别的认知诊断模型来建模学生的表现

    Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance. (arXiv:2309.00300v1 [cs.AI])

    [http://arxiv.org/abs/2309.00300](http://arxiv.org/abs/2309.00300)

    本文提出了一个可识别的认知诊断框架，该框架能够从学生的答题记录中直接诊断可识别和可解释的考生特征和题目特征，并通过重建答题记录来确保诊断结果的可识别性。

    

    认知诊断旨在根据学生在考试题目上的答题成绩来诊断他们的知识水平，这是许多领域如计算自适应测试的基础。现有的认知诊断模型（CDMs）遵循了一个能力-响应范式，即将诊断结果视为学生响应的原因，并通过优化来学习诊断结果。然而，这种范式很容易导致不可识别的诊断结果和解释过拟合问题，这对于学生学习表现的量化是有害的。为了解决这些问题，我们提出了一种新的可识别的认知诊断框架。具体而言，我们首先提出了一个灵活的诊断模块，该模块直接从响应日志中诊断可识别和可解释的考生特征和题目特征。接下来，我们利用一个通用的预测模块从诊断结果中重建响应日志，以确保诊断结果的可识别性。

    Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure
    
[^146]: FECoM: 朝着深度学习的细粒度能耗测量迈进

    FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning. (arXiv:2308.12264v1 [cs.LG])

    [http://arxiv.org/abs/2308.12264](http://arxiv.org/abs/2308.12264)

    FECoM是一个用于细粒度深度学习能耗测量的框架，通过静态仪器分析和考虑计算负载和温度稳定性等因素，为研究人员和开发人员提供了对深度学习API进行概要分析的机制。

    

    随着深度学习模型的使用、规模和复杂性增加，其能源消耗迅速增长已成为一个关键问题。促进绿色发展和不同粒度的能源意识，以限制深度学习系统的碳排放是当务之急。然而，缺乏准确测量和优化细粒度（例如方法级别）能耗的标准和可重复工具阻碍了该领域的进展。在本文中，我们介绍了FECoM（细粒度能耗测量仪），这是一个用于细粒度深度学习能耗测量的框架。具体而言，FECoM为研究人员和开发人员提供了一种对深度学习API进行概要分析的机制。FECoM通过使用静态仪器分析和考虑计算负载和温度稳定性等各种因素来解决细粒度能耗测量的挑战。我们评估了FECoM在最常用的深度学习模型之一上测量细粒度能耗的能力。

    With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most p
    
[^147]: "引用GPT的“豚鼠试验”：一种研究企业竞争和勾结的创新智能代理建模方法"

    "Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion. (arXiv:2308.10974v1 [cs.AI])

    [http://arxiv.org/abs/2308.10974](http://arxiv.org/abs/2308.10974)

    "引用GPT的“豚鼠试验”是一种创新的智能代理建模方法，利用智能代理代表企业进行竞争和勾结研究。它比使用人类主体进行实验更具成本效益和灵活性，并展现出超越传统代理建模方法的能力。"

    

    企业竞争和勾结涉及复杂的动态，尤其是考虑到企业之间的沟通。这些问题可以被建模为复杂系统的问题，传统上通过涉及人类主体或基于代理的建模方法进行探究。我们提出了一种创新的框架，称为智能代理建模（SABM），其中由GPT-4技术支持的智能代理代表企业并相互交互。我们进行了一项控制实验，研究了不同条件下企业价格竞争和勾结行为。与使用人类主体进行实验相比，SABM更具成本效益和灵活性。智能代理拥有决策的广泛知识库，展现出类似人类的战略能力，超越了传统的基于代理的建模方法。此外，智能代理能够模拟人类对话并个性化，使其成为研究涉及沟通的复杂情况的理想选择。我们的结果表明...

    Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate th
    
[^148]: SMARLA：一种用于深度强化学习智能体的安全监测方法

    SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])

    [http://arxiv.org/abs/2308.02594](http://arxiv.org/abs/2308.02594)

    本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。

    

    深度强化学习算法(DRL)越来越多地应用于安全关键系统。确保DRL智能体的安全性在这种情况下是一个关键问题。然而，仅依靠测试是不足以确保安全性的，因为它不能提供保证。构建安全监测器是缓解这一挑战的一种解决方案。本文提出了SMARLA，一种基于机器学习的安全监测方法，专为DRL智能体设计。出于实际原因，SMARLA被设计为黑盒子(因为它不需要访问智能体的内部)，并利用状态抽象来减少状态空间，从而促进从智能体的状态学习安全违规预测模型。我们在两个知名的RL案例研究中验证了SMARLA。经验分析表明，SMARLA具有准确的违规预测能力，误报率低，并且可以在智能体执行的一半左右的早期阶段预测安全违规。

    Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
    
[^149]: 人工智能是算法模仿：为什么人工“代理”不是（也不会成为）真正的代理

    Artificial intelligence is algorithmic mimicry: why artificial "agents" are not (and won't be) proper agents. (arXiv:2307.07515v1 [cs.AI])

    [http://arxiv.org/abs/2307.07515](http://arxiv.org/abs/2307.07515)

    本研究通过对比生物系统和算法系统，指出了生物系统具有自我制造自主能力、符号和物理方面没有区分以及体验到模糊问题的大世界等特点，而算法系统则与此相反。

    

    这篇论文通过对比生物系统和算法系统，重点探讨“代理”概念，来探讨人工通用智能（AGI）的发展前景。作者指出了三个基本的差异：（1）生物系统具有自我制造的自主能力，能够设定自身的内在目标，而算法系统存在于一个由外部代理提供目标函数的计算环境中。（2）生物系统是具体体现的，即其符号和物理方面没有区分，而算法运行在计算结构上，最大限度地将软件与硬件隔离。（3）生物系统体验到一个庞大的世界，其中大多数问题是模糊的（并非全部可定义），而算法系统存在于一个小世界中，其中所有问题都是明确的。这三个差异说明了生物和算法系统具有非常不同的能力。

    What is the prospect of developing artificial general intelligence (AGI)? I investigate this question by systematically comparing living and algorithmic systems, with a special focus on the notion of "agency." There are three fundamental differences to consider: (1) Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent. (2) Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware. (3) Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined. These three differences imply that living and algorithmic systems have very different capab
    
[^150]: 直接效应在总结因果图中的可辨识性

    Identifiability of direct effects from summary causal graphs. (arXiv:2306.16958v1 [cs.AI])

    [http://arxiv.org/abs/2306.16958](http://arxiv.org/abs/2306.16958)

    该论文研究了在缺乏完整时间因果图的情况下，直接因果效应如何从总结因果图中进行可辨识，并提出了一个完整的可辨识性结果。

    

    动态结构因果模型（SCMs）是一个强大的框架，用于推理动态系统中的直接效应，即衡量一个变量的变化如何影响另一个变量，同时保持其他变量不变。动态结构因果模型中的因果关系可以用完全时间因果图来进行定性表示。假设线性和因果充分性，并给定完全时间因果图，直接因果效应总是可辨识的，并可以通过调整由所谓的单门准则给出的任何变量集合来从数据中估计。然而，在许多应用中，由于各种原因没有此类图形可用，但专家仍然可以访问完全时间因果图的一个抽象，该抽象表示了时间序列之间的因果关系，同时省略了时间信息。本文提出了一个完整的可辨识性结果，其中详细描述了所有直接效应在总结因果图中可辨识的情况。

    Dynamic structural causal models (SCMs) are a powerful framework for reasoning in dynamic systems about direct effects which measure how a change in one variable affects another variable while holding all other variables constant. The causal relations in a dynamic structural causal model can be qualitatively represented with a full-time causal graph. Assuming linearity and causal sufficiency and given the full-time causal graph, the direct causal effect is always identifiable and can be estimated from data by adjusting on any set of variables given by the so-called single-door criterion. However, in many application such a graph is not available for various reasons but nevertheless experts have access to an abstraction of the full-time causal graph which represents causal relations between time series while omitting temporal information. This paper presents a complete identifiability result which characterizes all cases for which the direct effect is graphically identifiable from summa
    
[^151]: 面向开放词汇学习的调研

    Towards Open Vocabulary Learning: A Survey. (arXiv:2306.15880v1 [cs.CV])

    [http://arxiv.org/abs/2306.15880](http://arxiv.org/abs/2306.15880)

    该论文调研了在视觉场景理解领域的开放词汇学习，在与零样本学习和开放集识别等相关概念的比较中，总结和分析了该领域的最新发展。

    

    在视觉场景理解领域，深度神经网络在分割、跟踪和检测等各种核心任务上取得了令人瞩目的进展。然而，大多数方法基于封闭集的假设，即模型只能识别训练集中已定义的类别。最近，由于视觉语言预训练的快速进展，提出了开放词汇设置。这些新方法旨在定位和识别超出注释标签空间的类别。与弱监督和零样本设置相比，开放词汇方法更加通用、实用和有效。本文对开放词汇学习进行了全面的回顾，总结和分析了近期在该领域的发展。特别是，我们首先将其与零样本学习、开放集识别和超出分布检测等相关概念进行了比较。然后，在分割任务的几个紧密相关的任务中进行了回顾。

    In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective compared to weakly supervised and zero-shot settings. This paper provides a thorough review of open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by comparing it to related concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Then, we review several closely related tasks in the case of segmentati
    
[^152]: VisualGPTScore: 多模态生成预训练分数的视觉语义推理。

    VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])

    [http://arxiv.org/abs/2306.01879](http://arxiv.org/abs/2306.01879)

    我们提出了VisualGPTScore方法，能够使用多模态生成分数捕捉文本标题可能性，并在图像条件语言模型上进行计算，具备组合推理能力。

    

    本文提出了一种名为 VisualGPTScore 的方法，使用多模态生成分数来捕捉文本标题可能性，并使用图像条件语言模型在图像上运算。与传统观点认为的VLM只是无意义的单词袋模型不同，我们的 VisualGPTScore 在 ARO 和 Crepe 等最近提出的图像文本检索基准测试中展现了顶尖的性能，证明了其具备组合推理能力。

    Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
    
[^153]: 对知识蒸馏的训练动态进行详细研究

    A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11098](http://arxiv.org/abs/2303.11098)

    本文对知识蒸馏的训练动态进行了详细研究，实验证明投影器的设计决策、表示的标准化和软最大函数的选择对学生的性能有着重要影响，同时提出了一种解决容量差异问题的简单方法，以及与当前最先进的知识蒸馏技术相媲美的计算效率更高的方法。

    

    本文重新审视将知识蒸馏作为函数匹配和度量学习问题时的有效性。通过验证三个重要设计决策，即标准化、软最大函数和投影层作为关键要素，我们有理论地显示出投影器隐含地编码了关于过去样本的信息，从而为学生提供了关联梯度。然后，我们展示了表示的标准化与投影器的训练动态密切相关，这可能对学生的性能产生重大影响。最后，我们展示了简单的软最大函数可以用来解决任何显著容量差异的问题。在各种基准数据集上的实验结果表明，利用这些见解可以实现与最先进的知识蒸馏技术相媲美或优于其性能，同时计算效率更高。特别是在图像分类任务上取得了这些结果。

    In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classificati
    
[^154]: 从递推视角重新审视LQR控制

    Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient. (arXiv:2302.13144v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.13144](http://arxiv.org/abs/2302.13144)

    本文从递推视角重新审视了LQR控制问题，并应用递推-视角策略梯度（RHPG）模型提供了一种采样复杂度分析，通过无需任何先验信息进行优化求解，并展示了RHPG在线性控制和估计中的普适性。

    

    本文从递推视角重新审视了离散时间线性二次调节器（LQR）问题。结合递推-视角策略梯度（RHPG）模型无需任何先验信息进行优化求解，提供了一种采样复杂度分析，能够学习到在ε-范数意义下接近LQR最优解的优化控制策略。在最近将RHPG应用于学习卡尔曼滤波中进行拓展分析之后，我们展示了RHPG在线性控制和估计中的普适性。

    We revisit in this paper the discrete-time linear quadratic regulator (LQR) problem from the perspective of receding-horizon policy gradient (RHPG), a newly developed model-free learning framework for control applications. We provide a fine-grained sample complexity analysis for RHPG to learn a control policy that is both stabilizing and $\epsilon$-close to the optimal LQR solution, and our algorithm does not require knowing a stabilizing control policy for initialization. Combined with the recent application of RHPG in learning the Kalman filter, we demonstrate the general applicability of RHPG in linear control and estimation with streamlined analyses.
    

