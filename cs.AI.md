# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Compositional Generative Modeling: A Single Model is Not All You Need](https://rss.arxiv.org/abs/2402.01103) | 本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。 |
| [^2] | [RLVF: Learning from Verbal Feedback without Overgeneralization](https://arxiv.org/abs/2402.10893) | 研究了如何在大型语言模型中利用口头反馈进行定制化调整而不发生过度泛化，并提出了一种新的方法C3PO。 |
| [^3] | [Instruction Diversity Drives Generalization To Unseen Tasks](https://arxiv.org/abs/2402.10891) | 指导调整通过增加指令集的多样性来推动模型对未见任务的泛化。 |
| [^4] | [When is Tree Search Useful for LLM Planning? It Depends on the Discriminator](https://arxiv.org/abs/2402.10890) | 当前研究通过实验分析了大型语言模型在多步问题求解中使用树搜索的可行性，指出高级规划方法需要鉴别器至少90%准确性才能显著提高性能。 |
| [^5] | [Explainability for Machine Learning Models: From Data Adaptability to User Perception](https://arxiv.org/abs/2402.10888) | 本文旨在为已部署的机器学习模型生成局部解释并确保这些解释对用户具有可理解性，主要创新在于开发具有数据适应性和用户感知要求站点解释方法。 |
| [^6] | [3D Diffuser Actor: Policy Diffusion with 3D Scene Representations](https://arxiv.org/abs/2402.10885) | 通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。 |
| [^7] | [Multi-modal preference alignment remedies regression of visual instruction tuning on language model](https://arxiv.org/abs/2402.10884) | 通过收集轻量级VQA偏好数据集并使用Direct Preference Optimization，我们能够在语言模型的指导能力上取得显著提升，在小规模数据下比其他方法实现了更高的分数。 |
| [^8] | [Robust agents learn causal world models](https://arxiv.org/abs/2402.10877) | 智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。 |
| [^9] | [FedD2S: Personalized Data-Free Federated Knowledge Distillation](https://arxiv.org/abs/2402.10846) | 提出了FedD2S方法，通过深到浅的层丢弃机制，在无数据联邦知识蒸馏中增强了本地模型的个性化，表现出卓越性能和改善客户间公平性。 |
| [^10] | [Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg](https://arxiv.org/abs/2402.10837) | 本文探讨了使用四足机器人的腿进行操作的概念，通过训练强化学习策略实现了具有鲁棒性和灵活工作空间的Pedipulation控制器，并在实际任务中展示了其广泛应用性能。 |
| [^11] | [RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model](https://arxiv.org/abs/2402.10828) | RAG-Driver 提出了一种适用于自动驾驶的通用化驾驶解释系统，通过检索增强上下文学习，解决了多模态大语言模型训练成本高、数据稀缺和泛化能力限制等问题。 |
| [^12] | [Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond](https://arxiv.org/abs/2402.10805) | 提出了一种生成式跨模态检索框架，在多模态语言模型中实现了存储和检索图像的能力 |
| [^13] | [Modelling crypto markets by multi-agent reinforcement learning](https://arxiv.org/abs/2402.10803) | 该研究通过引入多智能体强化学习模型，成功模拟加密货币市场，弥补了之前基于零智能代理或单个自主智能体方法的不足。 |
| [^14] | [VATr++: Choose Your Words Wisely for Handwritten Text Generation](https://arxiv.org/abs/2402.10798) | 该研究探讨了风格化手写文本生成中输入对模型训练的影响，提出了改善性能和泛化能力的准备和训练规范化策略，并通过广泛的分析验证了这些方面。此外，还针对HTG研究中的标准化评估协议不足问题提出了解决方案。 |
| [^15] | [Masked Attention is All You Need for Graphs](https://arxiv.org/abs/2402.10793) | 提出了一种在图上学习的简单替代方法，称为掩码注意力（MAG），其利用注意力矩阵来创建定制的注意力模式，在长距离任务上表现出色并胜过其他方法。 |
| [^16] | [In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss](https://arxiv.org/abs/2402.10790) | 通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理长达 1000 万个元素的任务，这是迄今为止处理最长输入的开放神经网络模型，并展示了对长序列处理能力的显著改进。 |
| [^17] | [EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge](https://arxiv.org/abs/2402.10787) | 本文提出了EdgeQAT，使用熵和分布引导的量化感知训练方法来优化轻量级LLMs，在边缘设备上实现推理加速。 |
| [^18] | [AutoGPT+P: Affordance-based Task Planning with Large Language Models](https://arxiv.org/abs/2402.10778) | 提出了AutoGPT+P，它结合了基于Affordance的场景表示和规划系统，可以解决具有不确定性的任务规划问题。 |
| [^19] | [Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants](https://arxiv.org/abs/2402.10774) | 本文研究了一种现代形式的错误反馈EF21，将其依赖的通信复杂度从平方平均值改进为更小的算术平均值，在实践中表现良好。 |
| [^20] | [How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?](https://arxiv.org/abs/2402.10770) | 本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。 |
| [^21] | [Distillation Enhanced Generative Retrieval](https://arxiv.org/abs/2402.10769) | 通过蒸馏方法增强生成式检索系统，提出了一种名为DGR的框架，利用先进排名模型和蒸馏RankNet损失来优化模型。 |
| [^22] | [Inference to the Best Explanation in Large Language Models](https://arxiv.org/abs/2402.10767) | 该论文提出了一个受哲学启发设计的框架IBE-Eval，用于推进对大型语言模型解释的解释和评估，在因果问答实验中显示出高达77%的准确率。 |
| [^23] | [Policy Learning for Off-Dynamics RL with Deficient Support](https://arxiv.org/abs/2402.10765) | 该论文主要研究在离线动力学强化学习中如何应对源环境和目标环境之间的动态差异挑战。 |
| [^24] | [On Explaining Unfairness: An Overview](https://arxiv.org/abs/2402.10762) | 本文提出了算法公平性和可解释性在实现负责任人工智能中的重要性，并系统性地探讨了它们之间的关系、不同类型的公平性解释以及研究中的空白。 |
| [^25] | [Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering](https://arxiv.org/abs/2402.10756) | 提出了一种对比公平正则化的个体公平非负矩阵三因子分解模型，能够实现平衡和凝聚的簇，并允许用户在精度和公平度之间进行权衡。 |
| [^26] | [ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages](https://arxiv.org/abs/2402.10753) | ToolSword提出了一个专门用于细致调查大型语言模型在工具学习中安全问题的全面框架，揭示了在工具学习中持久存在的安全挑战。 |
| [^27] | [Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting](https://arxiv.org/abs/2402.10747) | 提出了一种完全可微的拉格朗日卷积神经网络模型，实现了物理信息与数据驱动学习相结合，在降水预报中表现优秀，为其他拉格朗日机器学习模型提供了新思路。 |
| [^28] | [GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models](https://arxiv.org/abs/2402.10744) | GenRES提出了一种多维度评估生成式关系抽取结果的方法，填补了使用传统指标评估GRE方法时的不足之处。 |
| [^29] | [Learning Planning Action Models from State Traces](https://arxiv.org/abs/2402.10726) | 该论文探讨了一种从状态轨迹中学习规划行动模型的方法，尤其关注在参数未提供的情况下进行学习，并提出了两种不同级别的跟踪质量以及相应的算法。 |
| [^30] | [Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process](https://arxiv.org/abs/2402.10725) | Cloud Kitchen平台利用基于规划的复合AI优化食品配送流程，通过车辆路径问题和真实历史数据集降低延迟配送量，提高顾客满意度。 |
| [^31] | [BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion](https://arxiv.org/abs/2402.10717) | BioFusionNet是一个深度学习框架，将图像特征与基因和临床数据融合，实现ER+乳腺癌患者的生存风险分层。 |
| [^32] | [An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference](https://arxiv.org/abs/2402.10712) | 通过实证研究，本文探讨了各种跨语言词汇适应方法对提高生成LLM推理效率的影响。 |
| [^33] | [AutoSAT: Automatically Optimize SAT Solvers via Large Language Models](https://arxiv.org/abs/2402.10705) | AutoSAT通过大型语言模型自动优化SAT求解器中的启发式，减少人为干预，提升求解器能力，实现了即插即用操作，保证了容错性，在广泛实验中表现出优越性能。 |
| [^34] | [Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas?](https://arxiv.org/abs/2402.10701) | 研究揭示了双生车联网络在密集区域中通过虚拟双生显著降低网络延迟、保持低延迟和提高计算速度的优势。 |
| [^35] | [Unlink to Unlearn: Simplifying Edge Unlearning in GNNs](https://arxiv.org/abs/2402.10695) | 研究揭示了GNN中边解除过程的关键问题，即过度遗忘现象，提出了解决方法来解决损失函数引起的问题。 |
| [^36] | [LongHeads: Multi-Head Attention is Secretly a Long Context Processor](https://arxiv.org/abs/2402.10685) | LongHeads 提出了一个无需训练的框架，通过释放多头注意力的潜力来增强大型语言模型(LLM)处理长上下文的能力。 |
| [^37] | [Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes](https://arxiv.org/abs/2402.10681) | 提出了物理相关的MeshGraphNets（PI-MGNs），可以在任意网格上进行非定态和非线性仿真，利用PINNs来减少对大量昂贵训练数据的依赖 |
| [^38] | [Network Formation and Dynamics Among Multi-LLMs](https://arxiv.org/abs/2402.10659) | 分析了多个LLM在社交网络中的行为，发现它们在给定网络结构并被询问形成网络偏好时表现出与人类社交动态一致的原则。 |
| [^39] | [Can Separators Improve Chain-of-Thought Prompting?](https://arxiv.org/abs/2402.10645) | 分隔符的引入在思维链提示中显著提高了大型语言模型（LLMs）在复杂推理任务上的表现。 |
| [^40] | [`Keep it Together': Enforcing Cohesion in Extractive Summaries by Simulating Human Memory](https://arxiv.org/abs/2402.10643) | 本文通过模拟人类记忆来保持主题连贯性，实现了在提取式摘要中强化连贯性的目标，同时保持信息量和减少冗余。 |
| [^41] | [Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model](https://arxiv.org/abs/2402.10642) | 通过将生成目标重定向到小波域，我们成功将语音DDPMs的训练和推理速度提高了一倍。 |
| [^42] | [ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling](https://arxiv.org/abs/2402.10635) | 将vanilla Transformer的关系建模扩展到连续时间领域，提出了ContiFormer。 |
| [^43] | [Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling](https://arxiv.org/abs/2402.10634) | 通过 hierarchical spatiotemporal downsampling 处理缺失数据问题，结合可解释的注意机制，实现对时空预测的有效建模 |
| [^44] | [Multitask Kernel-based Learning with Logic Constraints](https://arxiv.org/abs/2402.10617) | 将逻辑约束融合到多任务核心学习中，提出了一种通用方法来转换逻辑陈述为连续实现，以实现核心谓词计算输出。 |
| [^45] | [Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements](https://arxiv.org/abs/2402.10614) | 本文通过辩论调节LLMs，使其生成可控的支持用户定义论点的声明，改进了LLMs的可控性，并提出了DEBATunE流程。通过两个LLMs之间的多轮辩论生成高质量的训练数据，以支持生成有更高质量和更突出的声明。 |
| [^46] | [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher](https://arxiv.org/abs/2402.10601) | 本文使用密码技术编码了越狱提示，成功地绕过了大型语言模型对有害问题的检测，实验结果显示攻击成功率高达59.42%。 |
| [^47] | [Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks](https://arxiv.org/abs/2402.10597) | 研究了不同Parameter Efficient Fine-tuning (PEFT)方法在临床决策任务中的适用性，发现除了LoRA外，大多数PEFT方法在各个模型规模和任务中性能不稳定，而LoRA在所有情况下性能都相对较高。PEFT方法在临床领域特别有效，尤其适用于可以操作的专门模型。 |
| [^48] | [Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation](https://arxiv.org/abs/2402.10580) | 本研究将不同的不确定性量化方法与联合语义分割和单目深度估计相结合，介绍了一种新的学生-教师蒸馏方法 EMUFormer，揭示了多任务学习对不确定性质量的益处。 |
| [^49] | [Symbolic Autoencoding for Self-Supervised Sequence Learning](https://arxiv.org/abs/2402.10575) | 符号自编码（$\Sigma$AE）是一个自监督框架，通过最小化重构损失和平行数据的监督损失来优化连接两个生成模型，实现了在转导任务上显著提升性能。 |
| [^50] | [Direct Preference Optimization with an Offset](https://arxiv.org/abs/2402.10571) | 提出了一种新颖的直接偏好优化方法，即具有偏置的DPO（ODPO），在微调过程中不同对待每个偏好对。 |
| [^51] | [InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?](https://arxiv.org/abs/2402.10567) | 本研究在印度法律领域探讨了大型语言模型（LLMs）在处理社会因素时的能力，提出了结合公平性和准确性的新指标$LSS_{\beta}$，并评估了模型在二元法律推理任务中的表现以及在印度社会各种不平等方面的公平性展示。 |
| [^52] | [Strong hallucinations from negation and how to fix them](https://arxiv.org/abs/2402.10543) | 论文针对语言模型在推理中造成的强幻觉问题，提出了一种处理否定的新方法，可以改善模型性能而无需使用稀疏负数据训练。 |
| [^53] | [Properties and Challenges of LLM-Generated Explanations](https://arxiv.org/abs/2402.10532) | 该研究探讨了大型语言模型生成的解释在多领域指导微调数据集上的特性，发现生成的解释表现出选择性和包含说明性元素，但较少是主观或误导性的。 |
| [^54] | [Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528) | 通过推理链来预测大型语言模型输出的准确性，我们引入了一个新的基准R2PE，并提出了处理可辨识性评分（PDS）框架。 |
| [^55] | [LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models](https://arxiv.org/abs/2402.10524) | LLM Comparator是一种用于交互式分析自动并行评估结果的新型可视化工具，支持用户理解模型表现优劣和不同之处，解决了大型语言模型评估中的可扩展性和可解释性挑战。 |
| [^56] | [Generative AI for Controllable Protein Sequence Design: A Survey](https://arxiv.org/abs/2402.10516) | 人工智能领域的进步推动了蛋白质设计领域朝着前所未有的革命方向发展，这篇论文系统地审查了用于可控蛋白质序列设计的生成AI的最新进展。 |
| [^57] | [Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA](https://arxiv.org/abs/2402.10515) | 提出并实现了一种新颖的低功耗通道感知动态频率DL-TDOA测距算法，主要应对室内定位中的非直射信道路径和信号中断效应。 |
| [^58] | [Can Transformers Predict Vibrations?](https://arxiv.org/abs/2402.10511) | 提出了一种新颖的基于Transformer的模型Resoformer，用于预测电动汽车传动轴上的扭振，解决了当前阻尼技术只能在共振发生后检测到的问题 |
| [^59] | [Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability](https://arxiv.org/abs/2402.10510) | 通过贝叶斯框架探讨了人类目标识别中行动、时序和目标可解性的作用，并开发了与人类推断更匹配的目标识别模型。 |
| [^60] | [Provably Sample Efficient RLHF via Active Preference Optimization](https://arxiv.org/abs/2402.10500) | 通过Active Preference Optimization算法，在Bradley-Terry-Luce偏好模型下实现了RLHF的样本效率提高，优化了对提示收集偏好数据的策略。 |
| [^61] | [Comparing Hallucination Detection Metrics for Multilingual Generation](https://arxiv.org/abs/2402.10496) | 本研究比较了多语言生成中不同幻觉检测指标的效果，发现基于自然语言推理（NLI）的指标在高资源语言的句子级别表现良好，但通常无法检测到原子事实幻觉。 |
| [^62] | [Developing an Optimal Model for Predicting the Severity of Wheat Stem Rust (Case study of Arsi and Bale Zone)](https://arxiv.org/abs/2402.10492) | 通过比较三种不同的人工神经网络方法，研究发现通用回归神经网络（GRNN）在预测小麦枯叶病严重程度方面表现出有效性，并需要较少的训练时间。 |
| [^63] | [Random Projection Layers for Multidimensional Time Sires Forecasting](https://arxiv.org/abs/2402.10487) | 提出了一种全MLP时间序列预测架构RPMixer，通过将随机投影层集成到模型中，增加了块输出之间的多样性，提高了整体性能 |
| [^64] | [Emoji Driven Crypto Assets Market Reactions](https://arxiv.org/abs/2402.10481) | 该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。 |
| [^65] | [Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation](https://arxiv.org/abs/2402.10468) | 提出一种对抗课程图对比学习（ACGCL）框架，利用成对增强生成具有可控相似性的图级正负样本，同时通过子图对比学习来识别有效的图模式 |
| [^66] | [Large Language Models as Zero-shot Dialogue State Tracker through Function Calling](https://arxiv.org/abs/2402.10466) | 本研究提出了一种通过函数调用将大型语言模型用于零-shot对话状态追踪的新方法，能够在任务导向对话中取得出色的性能，适应不同领域而无需大量数据收集或模型调整。 |
| [^67] | [Evaluating and Improving Continual Learning in Spoken Language Understanding](https://arxiv.org/abs/2402.10427) | 提出了一种评估方法来统一评估口语理解中的持续学习算法在稳定性、可塑性和泛化能力方面的整体表现，并展示了引入不同知识蒸馏如何改善模型性能。 |
| [^68] | [Understanding In-Context Learning with a Pelican Soup Framework](https://arxiv.org/abs/2402.10424) | 提出了一个鹈鹕汤框架，包括常识知识库、自然语言分类任务的形式化以及意义关联的概念，并建立了一个$O(1/T)$的上下文学习损失界限，能够解释对未见任务的泛化。 |
| [^69] | [Connect the dots: Dataset Condensation, Differential Privacy, and Adversarial Uncertainty](https://arxiv.org/abs/2402.10423) | 我们连接数据集浓缩、差分隐私和对抗不确定性，提出通过对抗不确定性选择最优噪声水平$\epsilon$的方案，以保证高保真数据的同时提供隐私保护。 |
| [^70] | [Grounding Language about Belief in a Bayesian Theory-of-Mind](https://arxiv.org/abs/2402.10416) | 语义基础置于贝叶斯心灵理论中，通过模拟人们共同推断出解释代理人行为的一致性目标、信念和计划集合，再通过认识逻辑评估有关代理人信念的陈述，解释了人类信念归因的分级性和组合性，以及其与目标和计划的密切联系。 |
| [^71] | [Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting](https://arxiv.org/abs/2402.10412) | 提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。 |
| [^72] | [Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning](https://arxiv.org/abs/2402.10409) | 通过图结构信息在共类别图上利用图表示学习技术，可以在LLMs的预训练模型微调和零-shot/few-shot分类方面显著优于语言模型，揭示了弱标签微调LLMs的潜力。 |
| [^73] | [Explaining generative diffusion models via visual analysis for interpretable decision-making process](https://arxiv.org/abs/2402.10404) | 通过视觉分析，这项研究提出了三个研究问题，解释生成扩散模型的过程，设计了可视化工具，并展示了如何逐步生成输出并强调与基础视觉概念的关系。 |
| [^74] | [Polyhedral Complex Derivation from Piecewise Trilinear Networks](https://arxiv.org/abs/2402.10403) | 本文以三线性插值方法作为位置编码，提出了理论见解和分析网格提取方法，将高维曲面转换为平面，并引入了一种近似交点的方法，拓展了更广泛的应用。 |
| [^75] | [Darwin Turing Dawkins: Building a General Theory of Evolution](https://arxiv.org/abs/2402.10393) | 进化论不仅适用于基因，也适用于存储在大脑中的模因和计算机中的信息，这本书探讨了这一普遍的进化理论对自然、社会、文化和个体的影响。 |
| [^76] | [Pretext Training Algorithms for Event Sequence Data](https://arxiv.org/abs/2402.10392) | 提出了针对事件序列数据的自监督预训练框架，通过引入新颖的对齐验证任务，构建了适用于事件序列的基础表示，可以推广到不同的下游任务和数据领域。 |
| [^77] | [UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style](https://arxiv.org/abs/2402.10381) | UMAIR-FPS提出了一种新的用户感知多模态动画插画推荐系统，通过融合图像绘画风格特征和语义特征来增强表示。 |
| [^78] | [Subgraph-level Universal Prompt Tuning](https://arxiv.org/abs/2402.10380) | 设计了一种可适用于任何预训练策略的简单提示调整方法，通过位于输入图特征空间内的功能来实现，从而增加了其在各种下游应用中的通用性 |
| [^79] | [BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains](https://arxiv.org/abs/2402.10373) | BioMistral是一种面向生物医学领域的开源预训练大型语言模型集合，在医学问答任务中表现出优越性能并具有竞争优势。 |
| [^80] | [Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review](https://arxiv.org/abs/2402.10350) | 大型语言模型在预测和异常检测领域展现出显著潜力，但面临着挑战包括依赖历史数据集、泛化困难、模型幻觉等问题，提出了整合多模态数据等解决方案。 |
| [^81] | [On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities](https://arxiv.org/abs/2402.10340) | 论文突出探讨了在机器人应用中整合大型语言模型和视觉语言模型所带来的安全性和健壮性关键问题，指出这种整合可能容易受到恶意攻击并导致严重后果。 |
| [^82] | [HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting](https://arxiv.org/abs/2402.10334) | 提出了一种名为HI-GAN的层次化修复GAN，通过三个GAN以层次结构的方式进行RGBD修复，其中EdgeGAN和LabelGAN分别修复遮罩边缘和分割标签图像，而CombinedRGBD-GAN结合它们的潜在表示输出并进行RGB和深度修复。 |
| [^83] | [LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing](https://arxiv.org/abs/2402.10294) | LAVE通过整合大型语言模型（LLMs），提供LLM动力的代理辅助和语言增强编辑功能，减少视频编辑的障碍，帮助用户实现编辑目标 |
| [^84] | [Experiments with Encoding Structured Data for Neural Networks](https://arxiv.org/abs/2402.10290) | 该论文着眼于探索为神经网络编码结构化数据的技术，以便在游戏领域中创建一个能够选择良好行动的AI代理。 |
| [^85] | [Backdoor Attack against One-Class Sequential Anomaly Detection Models](https://arxiv.org/abs/2402.10283) | 本文提出了一种新型后门攻击策略，可以通过在良性正常数据中制作几乎不可察觉的触发器，并将其注入模型，成功妥协了两个一类异常检测模型。 |
| [^86] | [Brant-2: Foundation Model for Brain Signals](https://arxiv.org/abs/2402.10251) | Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。 |
| [^87] | [A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals](https://arxiv.org/abs/2402.10248) | 该论文提出了一种基于数据驱动的监督机器学习方法，可用于估计全球环境空气污染浓度，并提供预测区间，为各利益相关者提供更全面的数据，并通过检验模型在不同地理位置上的性能为研究提供洞见。 |
| [^88] | [A Dynamical View of the Question of Why](https://arxiv.org/abs/2402.10240) | 提出了一种在时间过程中直接建立事件之间因果关系的学习范式，并提出了用于计算因果贡献的两个关键引理，可以揭示和量化扩散过程中的因果关系。 |
| [^89] | [Discovering Sensorimotor Agency in Cellular Automata using Diversity Search](https://arxiv.org/abs/2402.10236) | 本论文利用多样性搜索、课程学习和梯度下降等算法，自动搜索元胞自动机中能够自组织出具有基础感觉运动机构的“个体”，为寻找这种基本机构自组织的环境条件提供了新方法。 |
| [^90] | [HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments](https://arxiv.org/abs/2402.10228) | HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。 |
| [^91] | [Human-Centric Goal Reasoning with Ripple-Down Rules](https://arxiv.org/abs/2402.10224) | 本文通过使用Ripple-Down Rules（RDR）扩展了ActorSim目标推理框架，使其能够通过示范学习并建立新的决策规则，以便在未来正确处理类似情况。 |
| [^92] | [Autonomous Vehicle Patrolling Through Deep Reinforcement Learning: Learning to Communicate and Cooperate](https://arxiv.org/abs/2402.10222) | 本文通过强化互代学习方法，使多代理车辆在面临环境因素、限制和合作问题时学会沟通协作，从而解决自主车辆巡逻的挑战性问题。 |
| [^93] | [Clustering Inductive Biases with Unrolled Networks](https://arxiv.org/abs/2402.10213) | 提出了一种自动编码器架构（WLSC），其潜在表示是隐含的，用于对聚类归纳偏好进行建模 |
| [^94] | [Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model](https://arxiv.org/abs/2402.09786) | 这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。 |
| [^95] | [CodeMind: A Framework to Challenge Large Language Models for Code Reasoning](https://arxiv.org/abs/2402.09664) | CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。 |
| [^96] | [API Pack: A Massive Multilingual Dataset for API Call Generation](https://arxiv.org/abs/2402.09615) | 这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成 |
| [^97] | [Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation](https://arxiv.org/abs/2402.09604) | 本文提出了一种使用单个未标记测试图像来调整医学图像分割模型的方法。相比于直接最小化预测熵的其他方法，在这种设置下并不能显著提高性能。为了克服这个问题，我们使用各种目标域统计估计的预测进行集成，并基于权重进行加权。 |
| [^98] | [WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing](https://arxiv.org/abs/2402.09430) | WiMANS是第一个基于WiFi的多用户活动感知数据集，包含WiFi信道状态信息和同步视频，旨在促进可重复和可比较的研究 |
| [^99] | [Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop](https://arxiv.org/abs/2402.09346) | 本文提出了一种使用人机协同的方法开发大型语言模型审计框架，通过使用不同版本的相同问题来探测模型可能存在的偏见或幻觉，实现了自动化和可扩展的审计方法。 |
| [^100] | [Mitigating Reward Hacking via Information-Theoretic Reward Modeling](https://arxiv.org/abs/2402.09345) | 本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。 |
| [^101] | [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091) | 通过提供隐含的线索，Puzzler通过绕过LLM的防御策略，在间接方式下实现了越狱攻击，成功率高达96.6%。 |
| [^102] | [Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis through Rapid Prototyping, Iteration and Curation](https://arxiv.org/abs/2402.08812) | 该论文提出了一种"类似设计"的智能画布环境，通过将生成式AI组件集成到数据分析中，实现了快速原型设计、迭代和比较可视化管理。通过用户研究，论文验证了画布界面的有效性。 |
| [^103] | [Anchor-based Large Language Models](https://arxiv.org/abs/2402.07616) | 基于锚点的大型语言模型（AnLLM）通过引入创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略，将序列信息压缩到锚点标记中，减少键/值缓存，提高推理效率。 |
| [^104] | [Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900) | 本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。 |
| [^105] | [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782) | 本文研究了更弱的语言模型是否能评估更强的模型的正确性。研究发现，通过进行辩论，非专家模型和人类回答问题的准确性都有所提高。 |
| [^106] | [GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding](https://arxiv.org/abs/2402.06764) | 该论文提出了GLaM方法，通过邻域划分和生成子图编码，对领域知识图进行大型语言模型的微调。该方法的创新之处在于能够实现对实际应用中的多步推理，并减少虚构。 |
| [^107] | [NICE: To Optimize In-Context Examples or Not?](https://arxiv.org/abs/2402.06733) | 通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。 |
| [^108] | [LLM Agents can Autonomously Hack Websites](https://arxiv.org/abs/2402.06664) | 这项研究展示了LLM代理可以自主进行网站黑客攻击，包括盲目数据库模式提取和SQL注入，而且不需要人工反馈。这种能力是由高度工具使用和利用扩展上下文能力的前沿模型赋予的。 |
| [^109] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^110] | [Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications](https://arxiv.org/abs/2402.01681) | 在表情符号研究中，我们评估了ChatGPT在处理注释和下游任务中的有效性。我们的研究结果表明ChatGPT可以作为一个可行的替代人类注释者的工具，有效地解释表情符号。 |
| [^111] | [StickerConv: Generating Multimodal Empathetic Responses from Scratch](https://arxiv.org/abs/2402.01679) | 本文介绍了StickerConv代理(Agent4SC)，该代理通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。为了利用构建的多模态共情对话数据集StickerConv，作者提出了PErceive and Generate Stickers (PEGS)模型，该模型能够生成情境相关和情感丰富的回应。 |
| [^112] | [Combining Hierachical VAEs with LLMs for clinically meaningful timeline summarisation in social media](https://arxiv.org/abs/2401.16240) | 利用混合的分层变分自动编码器与LLMs结合的方法实现了从社交媒体用户时间轴生成具有临床意义的摘要，通过对时间轴的时间敏感性和举重有力的抽象摘要，TH-VAE生成的摘要在捕捉随时间变化方面优于仅使用LLM方法。 |
| [^113] | [Text Embedding Inversion Security for Multilingual Language Models](https://arxiv.org/abs/2401.12192) | 该研究探讨了多语言语言模型的文本嵌入逆转安全性问题，发现多语言模型更容易受到逆转攻击的影响，并提出了简单的掩蔽防御方法。 |
| [^114] | [Small LLMs Are Weak Tool Learners: A Multi-LLM Agent](https://arxiv.org/abs/2401.07324) | 本论文提出了一种新的策略，将大型语言模型代理（LLMs）的能力分解为计划器、调用器和总结器模块，以克服小型模型性能限制和工具更新的问题。 |
| [^115] | [Communication-Efficient Federated Learning for LEO Satellite Networks Integrated with HAPs Using Hybrid NOMA-OFDM](https://arxiv.org/abs/2401.00685) | 本文提出了NomaFedHAP这一新型FL-SatCom方法，利用HAPs作为PS来增强卫星可见性，并引入NOMA技术实现快速高效的模型传输。 |
| [^116] | [Response Enhanced Semi-supervised Dialogue Query Generation](https://arxiv.org/abs/2312.12713) | 提出了一种新的半监督学习框架--SemiDQG，通过未标记的对话来改善模型性能，训练响应增强的查询生成器 (RA)。 |
| [^117] | [KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know](https://arxiv.org/abs/2312.11539) | KGLens 是一个旨在衡量知识图与大型语言模型（LLMs）之间对齐程度的框架，帮助找出LLMs相对于知识图的知识不足之处。 |
| [^118] | [Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge](https://arxiv.org/abs/2311.09731) | 本研究系统地调查了大型语言模型在缺乏足够参数化知识的情况下如何表达对超出其知识范围的问题的不确定性，并强调了诚实与帮助性之间的权衡。 |
| [^119] | [Digital Socrates: Evaluating LLMs through Explanation Critiques](https://arxiv.org/abs/2311.09613) | 通过定义新的解释批评任务、创建人工验证过的数据集并训练开源自动批评模型，数字苏格拉底有助于揭示学生模型的见解。 |
| [^120] | [Fusion-Eval: Integrating Evaluators with LLMs](https://arxiv.org/abs/2311.09204) | Fusion-Eval是一种创新方法，利用LLMs整合不同辅助评估器的见解，极大提升自然语言系统评估的有效性。 |
| [^121] | [Generative quantum machine learning via denoising diffusion probabilistic models](https://arxiv.org/abs/2310.05866) | 通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。 |
| [^122] | [Interpretable Semiotics Networks Representing Awareness](https://arxiv.org/abs/2310.05212) | 这个研究描述了一个计算模型，通过追踪和模拟物体感知以及其在交流中所传达的表示来模拟人类的意识。相比于大多数无法解释的神经网络，该模型具有解释性，并可以通过构建新网络来定义物体感知。 |
| [^123] | [Redundancy and Concept Analysis for Code-trained Language Models](https://arxiv.org/abs/2305.00875) | 本文针对代码训练的语言模型进行冗余和概念分析，通过消除与给定任务不相关的神经元，帮助理解网络中哪些神经元和层可以被消除，并在哪里找到重要的代码属性。 |
| [^124] | [Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation](https://arxiv.org/abs/2302.03038) | 本文探讨了如何将单细胞视为空间标记，并利用Transformer模型促进空间转录组数据填充。 |
| [^125] | [Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments](https://arxiv.org/abs/2301.04195) | Orbit是一个统一的模块化机器人学习框架，提供了逼真的场景和高保真的刚性和可变形体模拟，支持多样化的任务和传感器，能够通过GPU并行化快速训练强化学习策略和收集大型演示数据集。 |
| [^126] | [Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior](https://arxiv.org/abs/2212.03733) | 层级奖励函数提出了一种解决奖励设计问题的方法，能够保证诱导出根据偏好关系是帕累托最优的策略，并在多个环境中展示其快速学习的能力。 |
| [^127] | [Hijack Vertical Federated Learning Models As One Party](https://arxiv.org/abs/2212.00322) | 这项研究探讨了水平联邦学习模型的安全性，弥补了现有研究中对于该模型安全性的不足。 |
| [^128] | [Decorrelative Network Architecture for Robust Electrocardiogram Classification](https://arxiv.org/abs/2207.09031) | 我们提出了一种基于特征装饰和Fourier分区的新型集成方法，用于教授网络各种互补特征，减少基于扰动的愚弄的机会。 |
| [^129] | [Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents.](http://arxiv.org/abs/2401.16461) | 通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。 |
| [^130] | [Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks.](http://arxiv.org/abs/2401.05949) | 本研究发现上下文学习范式在大型语言模型中存在漏洞，攻击者可以通过污染示范上下文来操控模型行为，而无需进行微调。这项研究设计了一种名为ICLAttack的后门攻击方法，可以通过污染示范样本和提示来使模型按照预定义的意图行事。 |
| [^131] | [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning.](http://arxiv.org/abs/2401.05268) | AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。 |
| [^132] | [Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models.](http://arxiv.org/abs/2310.12481) | 本文研究了大型语言模型中的文化主导问题，发现由于在模型训练中主要使用英语数据，当用户使用非英语语言提问时，模型往往提供与预期文化不相关的不恰当答案。我们提出了通过多样化数据预训练和文化感知提示两种方法来解决这个问题。 |
| [^133] | [A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems.](http://arxiv.org/abs/2310.08644) | 这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。 |
| [^134] | [OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models.](http://arxiv.org/abs/2310.07637) | OpsEval是一个全面任务导向的AIOps基准测试，评估了大型语言模型在有线网络操作、5G通信操作和数据库操作等关键场景下的能力水平，为提供针对AIOps定制的LLMs的优化方向。 |
| [^135] | [(Dynamic) Prompting might be all you need to repair Compressed LLMs.](http://arxiv.org/abs/2310.00867) | 提出了一种动态提示(IDP)的机制，它可以作为一种轻量级的适应工具，修复压缩的大型语言模型(LLMs)在一些实际的下游任务中的性能下降。 |
| [^136] | [ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving.](http://arxiv.org/abs/2309.17452) | ToRA是一种集成工具的数学问题求解推理代理，通过结合语言的分析能力和工具的计算效率，能够显著提高数学推理的性能，在多个数学推理数据集上取得了13%-19%的平均绝对改进率，并在竞赛级数据集MATH上达到了44.6%的性能。 |
| [^137] | [User Experience Design Professionals' Perceptions of Generative Artificial Intelligence.](http://arxiv.org/abs/2309.15237) | 经验丰富的设计师认为生成性人工智能（GenAI）在用户体验设计（UXD）实践中具有辅助作用。然而，初级设计师可能会面临技能退化、工作替代和创造力枯竭等问题。这篇论文讨论了人-GenAI协作的一些影响，包括版权与所有权、人类创造力和代理性以及AI素养和获取。 |
| [^138] | [Large Language Models for Automated Open-domain Scientific Hypotheses Discovery.](http://arxiv.org/abs/2309.02726) | 这项研究提出了用于社会科学学术假设发现的第一个自然语言处理数据集，旨在开发一个系统，能够基于原始网络语料库自动生成有效、新颖且对人类研究者有帮助的假设。 |
| [^139] | [Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT.](http://arxiv.org/abs/2308.07876) | 该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。 |
| [^140] | [Fair Machine Unlearning: Data Removal while Mitigating Disparities.](http://arxiv.org/abs/2307.14754) | 本研究提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的机器学习方法。 |
| [^141] | [Predict-AI-bility of how humans balance self-interest with the interest of others.](http://arxiv.org/abs/2307.12776) | 生成式AI能够准确预测人类在决策中平衡自身利益与他人利益的行为模式，但存在高估他人关注行为的倾向，这对AI的开发者和用户具有重要意义。 |
| [^142] | [Bandits with Deterministically Evolving States.](http://arxiv.org/abs/2307.11655) | 该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。 |
| [^143] | [FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets.](http://arxiv.org/abs/2307.10928) | FLASK是一种基于对齐技能集的细粒度语言模型评估协议，通过将粗级评分分解为每个指令的技能集级评分，实现了对模型性能的全面视角和提高评估的可靠性。 |
| [^144] | [Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL.](http://arxiv.org/abs/2305.17342) | 本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。 |
| [^145] | [CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing.](http://arxiv.org/abs/2305.11738) | 本文提出了一个名为CRITIC的框架，使得大型语言模型可以通过与工具的交互校正自己的错误，从而避免生成出现不一致和问题行为的结果。 |
| [^146] | [Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks.](http://arxiv.org/abs/2305.10544) | GSPNs是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询，并通过权重共享和树状计算图的优势获得了纯概率模型的效率和深度图网络的效果。 |
| [^147] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^148] | [Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks.](http://arxiv.org/abs/2305.01713) | 本文介绍了一种使用可逆神经网络将BERT-GPT2自动编码器的隐藏空间转换为更可分离的语义空间的方法，实验结果表明此方法可以改进模型的可解释性和可控性，并取得了比最先进模型更好的性能表现。 |
| [^149] | [Contrastive Learning Is Spectral Clustering On Similarity Graph.](http://arxiv.org/abs/2303.15103) | 本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。 |
| [^150] | [Logical Reasoning over Natural Language as Knowledge Representation: A Survey.](http://arxiv.org/abs/2303.12023) | 本文总结了一种新的逻辑推理方法，它使用自然语言作为知识表示，具有不同于端到端神经方法的优势。这种新模式在未来有着很高的潜力。 |
| [^151] | [Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++.](http://arxiv.org/abs/2301.11118) | Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。 |
| [^152] | [Efficient Multi-order Gated Aggregation Network.](http://arxiv.org/abs/2211.03295) | 本文探索了现代卷积神经网络的表征能力，使用多阶博弈论交互的新视角，提出了一种新的纯卷积神经网络架构MogaNet，它表现出优异的可扩展性，并在多种典型视觉基准中以更高效的参数利用达到了与最先进模型竞争的效果。 |
| [^153] | [Data Augmentation techniques in time series domain: A survey and taxonomy.](http://arxiv.org/abs/2206.13508) | 本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。 |
| [^154] | [A survey on GANs for computer vision: Recent research, analysis and taxonomy.](http://arxiv.org/abs/2203.11242) | 本文综述了GAN的最新架构、损失函数优化、验证指标和应用领域，并提出了一个分类法以更好地理解计算机视觉中GAN的现状。 |
| [^155] | [A Review of Deep Learning-based Approaches for Deepfake Content Detection.](http://arxiv.org/abs/2202.06095) | 本文综述了最近基于深度学习方法的深伪造内容检测的研究，系统地回顾了不同类别的伪造内容检测，并报告了所考察工作的优点和缺点，以及深伪造检测领域仍未解决的问题和不足之处的未来研究方向。 |

# 详细

[^1]: 组合生成建模：单一模型并不是您所需要的全部

    Compositional Generative Modeling: A Single Model is Not All You Need

    [https://rss.arxiv.org/abs/2402.01103](https://rss.arxiv.org/abs/2402.01103)

    本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。

    

    在人工智能研究中，通过训练大规模的巨大的生成模型来处理海量数据已经成为一种越来越主流的方法。本文中，我们认为我们应该通过将较小的生成模型组合在一起来构建大型生成系统。我们展示了这种组合生成方法如何以更高效的方式学习分布，使得我们在训练时未见的数据分布部分也能进行泛化。我们进一步展示了这种方法如何使我们能够为训练时完全未见的任务编写和构建新的生成模型。最后，我们展示在许多情况下，我们可以从数据中发现独立的组合组件。

    Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
    
[^2]: RLVF: 学习如何在没有泛化的情况下从口头反馈中学习

    RLVF: Learning from Verbal Feedback without Overgeneralization

    [https://arxiv.org/abs/2402.10893](https://arxiv.org/abs/2402.10893)

    研究了如何在大型语言模型中利用口头反馈进行定制化调整而不发生过度泛化，并提出了一种新的方法C3PO。

    

    大型语言模型（LLMs）部署的不同情境的多样性要求能够修改或定制默认模型行为，以满足细微的要求和偏好。规定这种模型调整的方便界面是高层次口头反馈，比如"在给老板起草邮件时不要使用表情符号"。然而，尽管撰写高层反馈比从人类反馈中收集强化学习注释（RLHF）简单得多，但我们发现只是用这种反馈提示模型会导致反馈在不相关的情境中产生泛化。我们研究了如何在没有这种泛化的情况下整合口头反馈的问题，并启发了一个新方法：带约束偏好优化的情境化评论（C3PO）。C3PO使用一段高层次反馈生成一个小的合成偏好数据集，指定了反馈应该如何（以及不应该如何）进行。

    arXiv:2402.10893v1 Announce Type: cross  Abstract: The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as "Don't use emojis when drafting emails to my boss." However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should no
    
[^3]: 指导多样性推动对未见任务的泛化

    Instruction Diversity Drives Generalization To Unseen Tasks

    [https://arxiv.org/abs/2402.10891](https://arxiv.org/abs/2402.10891)

    指导调整通过增加指令集的多样性来推动模型对未见任务的泛化。

    

    指导调整——在指令和期望结果之间微调大型语言模型（LLM）的方法——是一种使预训练语言模型执行现实世界任务并遵循人类指令的方法。其实际成功取决于模型学习比其训练时更广泛的指令集。然而，决定模型对这种“未见任务”的泛化的因素尚不十分清楚。为了了解泛化的驱动因素，本文通过字符串重写进行实验，这是一个符号任务，是图灵完整马尔可夫算法的基本组成部分，同时允许实验对“输入”和“指令”进行控制。我们调查了模型接受的指令数量和为每个指令提供的训练样本数量之间的权衡，并观察到指令集的多样性确定了泛化。

    arXiv:2402.10891v1 Announce Type: cross  Abstract: Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of "inputs" and "instructions". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalizati
    
[^4]: LLM规划中树搜索何时有用？取决于鉴别器

    When is Tree Search Useful for LLM Planning? It Depends on the Discriminator

    [https://arxiv.org/abs/2402.10890](https://arxiv.org/abs/2402.10890)

    当前研究通过实验分析了大型语言模型在多步问题求解中使用树搜索的可行性，指出高级规划方法需要鉴别器至少90%准确性才能显著提高性能。

    

    在本文中，我们通过一个语言代理框架研究了大型语言模型（LLMs）如何在多步问题下解决问题，该框架包括生成器、鉴别器和规划方法三个部分。我们研究了两种先进规划方法，迭代校正和树搜索的实际效用。我们全面分析了鉴别准确性如何影响代理在使用这两种方法或更简单的重新排序方法时的整体性能。在两项任务，文本到SQL解析和数学推理上的实验表明：（1）高级规划方法需要至少90%准确性的鉴别器才能实现显著改进；（2）当前LLMs的鉴别能力尚未满足高级规划方法实现这种改进的需求；（3）采用基于LLM的鉴别器时，高级规划方法可能无法充分平衡准确性和效率。

    arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
    
[^5]: 机器学习模型的可解释性：从数据适应性到用户感知

    Explainability for Machine Learning Models: From Data Adaptability to User Perception

    [https://arxiv.org/abs/2402.10888](https://arxiv.org/abs/2402.10888)

    本文旨在为已部署的机器学习模型生成局部解释并确保这些解释对用户具有可理解性，主要创新在于开发具有数据适应性和用户感知要求站点解释方法。

    

    这篇论文探讨了为已部署的机器学习模型生成局部解释，旨在确定产生有意义解释的最佳条件，考虑到数据和用户需求。主要目标是开发方法，生成任何模型的解释，同时确保这些解释保持忠实于基础模型并对用户具有可理解性。论文分为两部分。第一部分增强了一个广泛使用的基于规则的解释方法。然后引入了一个评估线性解释逼近模型适宜性的新方法。此外，对比了两种反事实解释方法族以分析其中一种相对另一种的优势。第二部分侧重于用户实验，评估三种解释方法和两种不同表示的影响。这些实验测量了用户理解解释的速度，可信度和关注程度。

    arXiv:2402.10888v1 Announce Type: new  Abstract: This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure ho
    
[^6]: 基于3D场景表示的3D扩散器Actor：通过策略扩散进行机器人操作

    3D Diffuser Actor: Policy Diffusion with 3D Scene Representations

    [https://arxiv.org/abs/2402.10885](https://arxiv.org/abs/2402.10885)

    通过策略扩散和3D场景表示相结合，提出了3D Diffuser Actor，一个神经策略架构，可以根据语言指令构建3D视觉场景表示，并对机器人末端执行器的3D旋转和平移进行迭代去噪。

    

    我们将扩散策略和3D场景表示相结合，用于机器人操作。扩散策略通过条件扩散模型学习基于机器人和环境状态的动作分布。最近，它们已经表现出优于确定性和其他基于状态的动作分布学习方法。3D机器人策略使用从单个或多个摄像头视角获取的感应深度聚合的3D场景特征表示。它们已经证明比其2D对应物在摄像机视角上具有更好的泛化能力。我们统一了这两条线路的工作，并提出了3D扩散器Actor，这是一个神经策略架构，它在给定语言指令的情况下，构建视觉场景的3D表示，并在其上进行条件迭代去噪机器人末端执行器的3D旋转和平移。在每个去噪迭代中，我们的模型将末端执行器姿态估计表示为3D场景令牌，并预测t

    arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
    
[^7]: 多模式偏好对齐修复了语言模型在视觉指令调整上的回归

    Multi-modal preference alignment remedies regression of visual instruction tuning on language model

    [https://arxiv.org/abs/2402.10884](https://arxiv.org/abs/2402.10884)

    通过收集轻量级VQA偏好数据集并使用Direct Preference Optimization，我们能够在语言模型的指导能力上取得显著提升，在小规模数据下比其他方法实现了更高的分数。

    

    在实际应用中，多模式大型语言模型（MLLMs）被期望能够支持图像和文本模态的交换式多轮查询。然而，当前使用视觉问题回答（VQA）数据集训练的MLLMs可能会出现退化，因为VQA数据集缺乏原始文本指令数据集的多样性和复杂性，后者是底层语言模型训练的数据集。为了解决这一具有挑战性的退化问题，我们首先收集了一个轻量级（6k条记录）的VQA偏好数据集，其中答案由Gemini以细粒度方式注释了5个质量指标，然后研究了标准的监督微调、拒绝抽样、直接偏好优化（DPO）和SteerLM。我们的研究结果表明，通过DPO，我们能够超越语言模型的指导能力，实现了6.73的MT-Bench分数，而Vicuna的6.57和LLaVA的5.99，尽管数据规模较小。

    arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
    
[^8]: 强健的智能体学习因果世界模型

    Robust agents learn causal world models

    [https://arxiv.org/abs/2402.10877](https://arxiv.org/abs/2402.10877)

    智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。

    

    一直有人假设因果推理在强健且具有通用智能中起着基础作用，然而不清楚智能体是否必须学习因果模型才能推广到新的领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布转变下满足后悔界限的智能体必须学习数据生成过程的近似因果模型，对于优化智能体来说，该近似模型会收敛到真实的因果模型。我们讨论了这一结果对于多个研究领域，包括迁移学习和因果推断的影响。

    arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
    
[^9]: FedD2S: 个性化无数据联邦知识蒸馏

    FedD2S: Personalized Data-Free Federated Knowledge Distillation

    [https://arxiv.org/abs/2402.10846](https://arxiv.org/abs/2402.10846)

    提出了FedD2S方法，通过深到浅的层丢弃机制，在无数据联邦知识蒸馏中增强了本地模型的个性化，表现出卓越性能和改善客户间公平性。

    

    本文解决了联邦学习（FL）框架中客户端数据异构性的挑战。我们提出了一种名为FedD2S的新方法，用于个性化联邦学习（pFL），利用知识蒸馏。FedD2S在无数据知识蒸馏过程中结合了深到浅的层丢弃机制，以增强本地模型的个性化。通过在不同图像数据集（FEMNIST、CIFAR10、CINIC0和CIFAR100）上进行大量模拟，我们将FedD2S与最先进的FL基线进行了比较。所提出的方法表现出卓越性能，具有加速收敛和改善客户间公平性的特点。引入的层丢弃技术有效捕捉

    arXiv:2402.10846v1 Announce Type: cross  Abstract: This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation. FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization. Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients. The introduced layer-dropping technique effectively capture
    
[^10]: Pedipulate: 使用四足机器人腿部实现操作技能

    Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg

    [https://arxiv.org/abs/2402.10837](https://arxiv.org/abs/2402.10837)

    本文探讨了使用四足机器人的腿进行操作的概念，通过训练强化学习策略实现了具有鲁棒性和灵活工作空间的Pedipulation控制器，并在实际任务中展示了其广泛应用性能。

    

    四足机器人具有在维护、家庭支持和探索场景中变得至关重要的潜力。为了与环境互动和操作，大多数四足机器人都配备了专用机器人臂，这意味着与标准四足机器人相比具有额外的质量和机械复杂性。在这项工作中，我们探讨了“Pedipulation”-使用四足机器人的腿进行操作。通过训练一个强化学习策略来追踪一个足部的位置目标，我们实现了一个专用的Pedipulation控制器，它对干扰具有鲁棒性，通过全身行为具有大工作空间，并且可以通过步态的出现到达远距离目标，从而实现了运动-操作一体化。通过将我们的控制器部署在一个四足机器人上使用远程操作，我们展示了各种现实世界任务，例如开门、采集样本和推动障碍物。我们展示了足部负载携带超过2.0公斤。

    arXiv:2402.10837v1 Announce Type: cross  Abstract: Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additi
    
[^11]: RAG-Driver：在多模态大语言模型中通过检索增强上下文学习实现可泛化的驾驶解释

    RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model

    [https://arxiv.org/abs/2402.10828](https://arxiv.org/abs/2402.10828)

    RAG-Driver 提出了一种适用于自动驾驶的通用化驾驶解释系统，通过检索增强上下文学习，解决了多模态大语言模型训练成本高、数据稀缺和泛化能力限制等问题。

    

    由“黑匣子”模型驱动的机器人需要提供人类可信赖的可理解解释。因此，可解释性在可信任的自主决策中扮演着关键角色，以促进透明度和最终用户的接受度，特别是在复杂的自动驾驶场景中。最近多模态大语言模型（MLLMs）的进展显示出了增强解释性作为驾驶代理的潜力，通过产生控制预测以及自然语言解释。然而，由于昂贵的注释成本和不同数据集之间的显著领域差异，导致的严重数据稀缺使得开发一个强大且具有通用性的系统变得异常具有挑战性。此外，MLLM的训练要求昂贵，而灾难性遗忘问题的尚未解决也限制了它们在部署后的泛化能力。为了解决这些挑战，我们提出了RAG-Driver。

    arXiv:2402.10828v1 Announce Type: cross  Abstract: Robots powered by 'blackbox' models need to provide human-understandable explanations which we can trust. Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a n
    
[^12]: 生成式跨模态检索：在多模态语言模型中存储图像用于检索及更多应用

    Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond

    [https://arxiv.org/abs/2402.10805](https://arxiv.org/abs/2402.10805)

    提出了一种生成式跨模态检索框架，在多模态语言模型中实现了存储和检索图像的能力

    

    近期生成式语言模型的进展表明其能够记忆文档中的知识并有效地回答用户查询。在此能力基础上，我们提出了使多模态大型语言模型（MLLMs）能够在其参数内存储和检索图像的方法。给定用户对视觉内容的查询，MLLM被期望能够从其参数中“回忆”相关图像作为响应。实现这一目标面临着显著挑战，其中包括MLLM内置的视觉记忆和视觉检索方案。为解决这些挑战，我们引入了一个生成式跨模态检索框架，该框架为图像分配唯一标识符字符串，并涉及两个训练步骤：学习记忆和学习检索。第一步侧重于训练MLLM记忆图像与其标识符之间的关联。

    arXiv:2402.10805v1 Announce Type: cross  Abstract: The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to "recall" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter ste
    
[^13]: 通过多智能体强化学习模型对加密市场进行建模

    Modelling crypto markets by multi-agent reinforcement learning

    [https://arxiv.org/abs/2402.10803](https://arxiv.org/abs/2402.10803)

    该研究通过引入多智能体强化学习模型，成功模拟加密货币市场，弥补了之前基于零智能代理或单个自主智能体方法的不足。

    

    构建在之前的基础工作（Lussange等人，2020年）之上，本研究引入了一种多智能体强化学习（MARL）模型，模拟加密货币市场，该模型校准为 2018 年至 2022 年间不间断交易的 Binance 的153种加密货币的每日收盘价。与先前依赖于零智能代理或单个自主智能体方法的代理基础模型（ABM）或多智能体系统（MAS）不同，我们的方法依赖于赋予代理强化学习（RL）技术，以模拟加密市场。这种整合旨在通过自下而上的复杂性推理，模拟个体和集体代理，确保在这类市场近期波动剧烈且在 COVID-19 时代期间的稳健性。我们模型的一个关键特征还在于其自主代理根据两种信息源进行资产价格估值：

    arXiv:2402.10803v1 Announce Type: cross  Abstract: Building on a previous foundation work (Lussange et al. 2020), this study introduces a multi-agent reinforcement learning (MARL) model simulating crypto markets, which is calibrated to the Binance's daily closing prices of $153$ cryptocurrencies that were continuously traded between 2018 and 2022. Unlike previous agent-based models (ABM) or multi-agent systems (MAS) which relied on zero-intelligence agents or single autonomous agent methodologies, our approach relies on endowing agents with reinforcement learning (RL) techniques in order to model crypto markets. This integration is designed to emulate, with a bottom-up approach to complexity inference, both individual and collective agents, ensuring robustness in the recent volatile conditions of such markets and during the COVID-19 era. A key feature of our model also lies in the fact that its autonomous agents perform asset price valuation based on two sources of information: the mar
    
[^14]: VATr++：明智地选择您的字词进行手写文本生成

    VATr++: Choose Your Words Wisely for Handwritten Text Generation

    [https://arxiv.org/abs/2402.10798](https://arxiv.org/abs/2402.10798)

    该研究探讨了风格化手写文本生成中输入对模型训练的影响，提出了改善性能和泛化能力的准备和训练规范化策略，并通过广泛的分析验证了这些方面。此外，还针对HTG研究中的标准化评估协议不足问题提出了解决方案。

    

    arXiv:2402.10798v1 公告类型: 跨领域 摘要: 近年来，基于学习的解决方案，如GANs、Transformers和初步的Diffusion模型的成功推动了风格化手写文本生成（HTG）引起了极大关注。尽管对此趋势产生了浓厚兴趣，但一个关键且尚未得到充分研究的方面是输入（包括视觉和文本）对HTG模型训练的影响及其对性能的后续影响。本研究深入探讨了先进的风格化HTG方法，提出了关于输入准备和训练正则化的策略，使模型能够实现更好的性能和更好的泛化。这些方面通过在多种不同设置和数据集上进行广泛分析进行了验证。此外，在这项工作中，我们不仅致力于性能优化，还解决了HTG研究中的一个重要障碍 - 缺乏标准化评估协议。特别是，我们提出了一个评估标准化的方案。

    arXiv:2402.10798v1 Announce Type: cross  Abstract: Styled Handwritten Text Generation (HTG) has received significant attention in recent years, propelled by the success of learning-based solutions employing GANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in interest, there remains a critical yet understudied aspect - the impact of the input, both visual and textual, on the HTG model training and its subsequent influence on performance. This study delves deeper into a cutting-edge Styled-HTG approach, proposing strategies for input preparation and training regularization that allow the model to achieve better performance and generalize better. These aspects are validated through extensive analysis on several different settings and datasets. Moreover, in this work, we go beyond performance optimization and address a significant hurdle in HTG research - the lack of a standardized evaluation protocol. In particular, we propose a standardization of the evaluatio
    
[^15]: 掩码注意力是图的关键

    Masked Attention is All You Need for Graphs

    [https://arxiv.org/abs/2402.10793](https://arxiv.org/abs/2402.10793)

    提出了一种在图上学习的简单替代方法，称为掩码注意力（MAG），其利用注意力矩阵来创建定制的注意力模式，在长距离任务上表现出色并胜过其他方法。

    

    图神经网络（GNNs）和消息传递算法的变种主要用于在图上学习，这在很大程度上归功于它们的灵活性、速度和令人满意的性能。然而，设计强大而通用的GNNs需要大量的研究工作，通常依赖于精心选择的手工制作的消息传递操作符。受此启发，我们提出了一种在图上学习的非常简单的替代方法，它完全依赖于注意力。图被表示为节点或边集，并通过掩码注意权重矩阵来强制它们的连接，有效地为每个图创建定制的注意力模式。尽管其简单性，用于图的掩码注意力（MAG）在长距离任务上表现出色，并在55多个节点和图级任务上优于强消息传递基线和更复杂的基于注意力的方法。

    arXiv:2402.10793v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly 
    
[^16]: 在一个 1000 万根草垛中寻找针：循环记忆找到了语言模型不擅长的内容

    In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss

    [https://arxiv.org/abs/2402.10790](https://arxiv.org/abs/2402.10790)

    通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理长达 1000 万个元素的任务，这是迄今为止处理最长输入的开放神经网络模型，并展示了对长序列处理能力的显著改进。

    

    本文解决了使用生成式 Transformer 模型处理长文档的挑战。为了评估不同方法，我们引入了 BABILong，这是一个新的基准，旨在评估模型在提取和处理广泛文本中分布式事实方面的能力。我们的评估包括 GPT-4 和 RAG 的基准，结果显示常见方法仅适用于最多 $10^4$ 个元素的序列。相反，通过使用循环记忆增强对 GPT-2 进行微调，使其能够处理涉及最多 $10^7$ 个元素的任务。这一成就标志着迄今为止任何开源神经网络模型处理的最长输入，显示了对长序列处理能力的显著改进。

    arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
    
[^17]: EdgeQAT: 熵和分布引导的量化感知训练，用于加速轻量级LLMs在边缘设备上的应用

    EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge

    [https://arxiv.org/abs/2402.10787](https://arxiv.org/abs/2402.10787)

    本文提出了EdgeQAT，使用熵和分布引导的量化感知训练方法来优化轻量级LLMs，在边缘设备上实现推理加速。

    

    尽管大型语言模型（LLMs）在各个领域取得了显著进展，但由于其庞大的参数和计算量，LLMs在边缘设备上的广泛应用受到限制。为了解决这一问题，通常采用量化方法生成具有高效计算和快速推理的轻量级LLMs。然而，后训练量化（PTQ）方法在将权重、激活和KV缓存一起量化至8位以下时，质量会急剧下降。此外，许多量化感知训练（QAT）工作对模型权重进行量化，而激活未被触及，这不能充分发挥量化对边缘端推理加速的潜力。在本文中，我们提出了EdgeQAT，即熵和分布引导的QAT，用于优化轻量级LLMs以实现在边缘设备上的推理加速。我们首先确定量化性能下降主要源自信息

    arXiv:2402.10787v1 Announce Type: cross  Abstract: Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information
    
[^18]: 基于Affordance的任务规划与大型语言模型的AutoGPT+P

    AutoGPT+P: Affordance-based Task Planning with Large Language Models

    [https://arxiv.org/abs/2402.10778](https://arxiv.org/abs/2402.10778)

    提出了AutoGPT+P，它结合了基于Affordance的场景表示和规划系统，可以解决具有不确定性的任务规划问题。

    

    最近关于任务规划的一些新进展利用了大型语言模型（LLMs），通过将这些模型与经典规划算法结合起来来提高泛化能力，以解决它们在推理能力上固有的局限性。然而，这些方法面临着动态捕捉任务规划问题的初始状态的挑战。为了缓解这一问题，我们提出了AutoGPT+P，这是一个系统，将基于Affordance的场景表示与一个规划系统相结合。Affordance包括了一个代理在环境中和其中存在的物体上的动作可能性。因此，从基于Affordance的场景表示中推导出规划域，允许使用任意对象进行符号规划。AutoGPT+P利用这种表示来为用户用自然语言指定的任务制定和执行计划。除了在封闭世界假设下解决规划任务外，AutoGPT+P还可以处理具有不确定性的规划任务。

    arXiv:2402.10778v1 Announce Type: cross  Abstract: Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with inc
    
[^19]: 错误反馈重新加载：从平方到平滑度常数的算术平均值

    Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants

    [https://arxiv.org/abs/2402.10774](https://arxiv.org/abs/2402.10774)

    本文研究了一种现代形式的错误反馈EF21，将其依赖的通信复杂度从平方平均值改进为更小的算术平均值，在实践中表现良好。

    

    错误反馈（EF）是一种非常流行且极其有效的机制，用于解决分布式训练方法（如分布式GD或SGD）中由于与贪婪通信压缩技术（如TopK）结合而产生的收敛问题。尽管EF提出已有近十年时间（Seide等人，2014年），并且尽管社区为推进对该机制的理论理解而集中努力，仍有很多尚待探索之处。在本文中，我们研究了一种名为EF21（Richtarik等人，2021年）的现代形式的错误反馈，它提供了目前已知的最佳理论保证，在最弱的假设下也在实践中运行良好。特别地，虽然EF21的理论通信复杂度取决于某些平滑度参数的平方均值，我们将这种依赖性改进为它们的算术平均值，后者始终更小，尤其是在...

    arXiv:2402.10774v1 Announce Type: cross  Abstract: Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in
    
[^20]: 自动评估方法在面向指令的LLM中有多可靠？

    How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?

    [https://arxiv.org/abs/2402.10770](https://arxiv.org/abs/2402.10770)

    本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。

    

    面向指令的大型语言模型(LLMs)的研究使用基于文本重叠和LLM判断的自动方法作为人工评估的成本有效替代方案。本文研究了这些方法在广泛的任务范围和跨语言环境中的可靠性。与先前的研究结果相反，我们观察到在任务类型不同的情况下，自动方法与人工评估者之间的相关性存在显著变化。具体而言，广泛使用的ROUGE-L度量在短答案英语任务中与人类判断强相关，但在自由形式生成任务和跨语言转移中不可靠。使用GPT-4作为评估员的有效性取决于在要求评估时包含参考答案，这可能导致在自由形式生成任务中评估过于严格。总的来说，我们发现，尽管自动评估方法可以近似人类判断，但其准确性可能因任务类型和评估设置而异。

    arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
    
[^21]: 蒸馏增强生成式检索

    Distillation Enhanced Generative Retrieval

    [https://arxiv.org/abs/2402.10769](https://arxiv.org/abs/2402.10769)

    通过蒸馏方法增强生成式检索系统，提出了一种名为DGR的框架，利用先进排名模型和蒸馏RankNet损失来优化模型。

    

    生成式检索是文本检索中的一种新兴范式，通过生成相关段落的标识符字符串作为检索目标。该范式利用强大的生成式语言模型，不同于传统的稀疏或密集检索方法。本研究确定了通过蒸馏进一步增强生成式检索的可行方向，并提出了一个名为DGR的可行框架。DGR利用诸如跨编码器等先进排名模型，在教师角色中提供段落排名列表，捕获段落的不同相关程度，而不是二元硬标签；随后，DGR采用一种特别设计的蒸馏RankNet损失来优化生成式检索模型，考虑教师模型提供的段落排名顺序作为标签。该框架仅需要额外的蒸馏步骤来增强当前的生成式检索系统，并不增加任何负担。

    arXiv:2402.10769v1 Announce Type: cross  Abstract: Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden
    
[^22]: 大型语言模型中的最佳解释推断

    Inference to the Best Explanation in Large Language Models

    [https://arxiv.org/abs/2402.10767](https://arxiv.org/abs/2402.10767)

    该论文提出了一个受哲学启发设计的框架IBE-Eval，用于推进对大型语言模型解释的解释和评估，在因果问答实验中显示出高达77%的准确率。

    

    虽然大型语言模型（LLMs）在现实应用中取得了成功，但它们的基本解释过程仍然知之甚少。本文提出了IBE-Eval，这是一个受哲学关于最佳解释推断（IBE）的启发而设计的框架，旨在推进对LLMs解释的解释和评估。IBE-Eval通过结合包括一致性、简洁性、连贯性和不确定性在内的显式逻辑和语言特征来估计自然语言解释的合理性。在因果问答（CQA）领域进行了大量实验，其中IBE-Eval被要求在多个由LLMs（即GPT 3.5和Llama 2）生成的竞争性因果解释中选择最合理的因果解释。实验证明，IBE-Eval可以成功地以高达77\%的准确率（比随机高约27%）识别最佳解释，优于GPT 3.5作为判定基线的表现。

    arXiv:2402.10767v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\appr
    
[^23]: 政策学习在支持不足的离线动力学RL中的应用

    Policy Learning for Off-Dynamics RL with Deficient Support

    [https://arxiv.org/abs/2402.10765](https://arxiv.org/abs/2402.10765)

    该论文主要研究在离线动力学强化学习中如何应对源环境和目标环境之间的动态差异挑战。

    

    强化学习（RL）可以有效学习复杂的策略。然而，学习这些策略通常需要与环境进行大量的试错交互。在许多现实场景中，由于数据收集的高成本和安全问题，这种方法并不实际。因此，一个常见的策略是将在低成本、快速源模拟器中训练的策略转移到真实世界的目标环境。然而，这个过程存在挑战。无论模拟器多么先进，都不能完美复制真实世界的复杂性，导致源环境和目标环境之间存在动态差异。先前的研究提出，源领域必须包含所有可能的目标转换，这种条件我们称之为完全支持。然而，在预期完全支持往往是不切实际的，特别是在存在重大动态差异的情况下。在本文中，我们的重点转向了解决

    arXiv:2402.10765v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addres
    
[^24]: 关于解释不公平性：综述

    On Explaining Unfairness: An Overview

    [https://arxiv.org/abs/2402.10762](https://arxiv.org/abs/2402.10762)

    本文提出了算法公平性和可解释性在实现负责任人工智能中的重要性，并系统性地探讨了它们之间的关系、不同类型的公平性解释以及研究中的空白。

    

    算法公平性和可解释性是实现负责任人工智能的基本要素。本文关注它们的相互作用，这是最近受到越来越多关注的一个研究领域。首先，我们提出了两个全面的分类法，分别代表两个互补的研究领域：公平性和解释。然后，我们将用于公平性的解释分为三类：(a) 用于增强公平性指标的解释，(b) 有助于我们理解(不)公平性原因的解释，(c) 用于协助我们设计减轻不公平性方法的解释。最后，基于我们的公平性和解释分类法，我们提出未被发现的文献路径，揭示可以作为未来研究宝贵见解的空白。

    arXiv:2402.10762v1 Announce Type: new  Abstract: Algorithmic fairness and explainability are foundational elements for achieving responsible AI. In this paper, we focus on their interplay, a research area that is recently receiving increasing attention. To this end, we first present two comprehensive taxonomies, each representing one of the two complementary fields of study: fairness and explanations. Then, we categorize explanations for fairness into three types: (a) Explanations to enhance fairness metrics, (b) Explanations to help us understand the causes of (un)fairness, and (c) Explanations to assist us in designing methods for mitigating unfairness. Finally, based on our fairness and explanation taxonomies, we present undiscovered literature paths revealing gaps that can serve as valuable insights for future research.
    
[^25]: 朝向凝聚-公平-和谐：对比正则化在个体公平图聚类中的应用

    Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering

    [https://arxiv.org/abs/2402.10756](https://arxiv.org/abs/2402.10756)

    提出了一种对比公平正则化的个体公平非负矩阵三因子分解模型，能够实现平衡和凝聚的簇，并允许用户在精度和公平度之间进行权衡。

    

    传统的公平图聚类方法面临两个主要挑战：它们通过施加严格的约束优先考虑平衡的簇，而牺牲了簇的凝聚性；现有的个人和群体级公平方法在图分区中主要依赖于特征值分解，因此通常缺乏可解释性。为了解决这些问题，我们提出了iFairNMTF，这是一种具有对比公平正则化的个体公平非负矩阵三因子分解模型，实现了平衡和凝聚的簇。通过引入公平性正则化，我们的模型允许定制精度-公平度的权衡，从而增强了用户的自主权，而不会影响非负矩阵三因子分解提供的可解释性。在真实和合成数据集上的实验评估表明，iFairNMTF在实现公平性和聚类性能方面具有出色的灵活性。

    arXiv:2402.10756v1 Announce Type: cross  Abstract: Conventional fair graph clustering methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level fairness in graph partitioning mostly rely on eigen decompositions and thus, generally lack interpretability. To address these issues, we propose iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model with contrastive fairness regularization that achieves balanced and cohesive clusters. By introducing fairness regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization. Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving fairness and clustering performance.
    
[^26]: ToolSword：揭示大型语言模型在工具学习中的安全问题跨三个阶段

    ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages

    [https://arxiv.org/abs/2402.10753](https://arxiv.org/abs/2402.10753)

    ToolSword提出了一个专门用于细致调查大型语言模型在工具学习中安全问题的全面框架，揭示了在工具学习中持久存在的安全挑战。

    

    arXiv:2402.10753v1 公告类型：跨领域 抽象：工具学习被广泛认为是在现实场景中部署大型语言模型（LLMs）的基础方法。尽管当前研究主要强调利用工具来增强LLMs，但它经常忽视与其应用相关的新兴安全考虑。为填补这一空白，我们提出了$ToolSword$，这是一个致力于细致调查LLMs在工具学习中安全问题的全面框架。具体来说，ToolSword勾画了LLMs在工具学习中的六个安全场景，包括输入阶段的$恶意$ $查询$和$越狱$ $攻击$，执行阶段的$噪声$ $误导$和$风险$ $线索$，以及输出阶段的$有害$ $反馈$和$错误$ $冲突$。对11个开源和闭源LLMs进行的实验表明，在工具学习中存在持久的安全挑战，如处理有害查询、使用风险工具和提供有害反馈。

    arXiv:2402.10753v1 Announce Type: cross  Abstract: Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedb
    
[^27]: 完全可微的拉格朗日卷积神经网络用于连续一致物理信息降水预报

    Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting

    [https://arxiv.org/abs/2402.10747](https://arxiv.org/abs/2402.10747)

    提出了一种完全可微的拉格朗日卷积神经网络模型，实现了物理信息与数据驱动学习相结合，在降水预报中表现优秀，为其他拉格朗日机器学习模型提供了新思路。

    

    本文提出了一种卷积神经网络模型，用于降水预报，结合了数据驱动学习和基于物理信息的领域知识。我们提出了LUPIN，即用于物理信息的拉格朗日双U-Net的现在预报，借鉴了现有的基于外推的预报方法，并以完全可微且GPU加速的方式实现了数据的拉格朗日坐标系转换，以允许实时端到端训练和推断。根据我们的评估，LUPIN与并超过了所选择基准的性能，为其他拉格朗日机器学习模型敞开了大门。

    arXiv:2402.10747v1 Announce Type: cross  Abstract: This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.
    
[^28]: GenRES：在大语言模型时代重新思考生成式关系抽取的评估

    GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models

    [https://arxiv.org/abs/2402.10744](https://arxiv.org/abs/2402.10744)

    GenRES提出了一种多维度评估生成式关系抽取结果的方法，填补了使用传统指标评估GRE方法时的不足之处。

    

    关系抽取（RE）领域正朝着利用大语言模型（LLM）的能力的生成式关系抽取（GRE）方向发生显着转变。然而，我们发现传统的关系抽取（RE）指标如精确率和召回率在评估GRE方法时存在不足。这种不足的原因在于这些指标依赖于与人工注释的参考关系的精确匹配，而GRE方法通常会产生与参考不同的多样且语义准确的关系。为填补这一空白，我们提出了GenRES，以多维度评估GRE结果的主题相似性、独特性、粒度、真实性和完整性。通过GenRES，我们实证发现：（1）精确率/召回率不能充分证明GRE方法的性能；（2）人工注释的参考关系可能存在不完整情况；（3）以固定一组关系或实体提示LLM

    arXiv:2402.10744v1 Announce Type: cross  Abstract: The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities
    
[^29]: 从状态轨迹中学习规划行动模型

    Learning Planning Action Models from State Traces

    [https://arxiv.org/abs/2402.10726](https://arxiv.org/abs/2402.10726)

    该论文探讨了一种从状态轨迹中学习规划行动模型的方法，尤其关注在参数未提供的情况下进行学习，并提出了两种不同级别的跟踪质量以及相应的算法。

    

    先前从状态轨迹中学习STRIPS领域模型的方法通常从要学习的行动的名称和参数开始。因此，它们的唯一任务是推断给定行动的前提条件和效应。在这项工作中，我们探讨了在学习时未提供学习行动的参数的情况。我们根据提供的信息定义了两个级别的跟踪质量，并提出了相应的算法。在一个级别(L1)中，轨迹中的状态被标记为行动名称，因此我们可以推断出行动的数量和名称，但我们仍需要弄清参数的数量和类型。在另一个级别(L2)中，状态还额外标记有构成相应基于对象的行动的参数。在这里，我们仍然需要推断学习行动中参数的类型。我们对所提出的算法进行了实验评估，并将其与其他方法进行了比较。

    arXiv:2402.10726v1 Announce Type: new  Abstract: Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned. Therefore their only task is to deduce the preconditions and effects of the given actions. In this work, we explore learning in situations when the parameters of learned actions are not provided. We define two levels of trace quality based on which information is provided and present an algorithm for each. In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters. In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions. Here we still need to deduce the types of the parameters in the learned actions. We experimentally evaluate the proposed algorithms and compare them with the
    
[^30]: 云厨房：使用基于规划的复合AI优化食品配送流程

    Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process

    [https://arxiv.org/abs/2402.10725](https://arxiv.org/abs/2402.10725)

    Cloud Kitchen平台利用基于规划的复合AI优化食品配送流程，通过车辆路径问题和真实历史数据集降低延迟配送量，提高顾客满意度。

    

    全球食品配送市场为基于人工智能的服务提供了许多机会，可以提高全球供餐效率。本文介绍了Cloud Kitchen平台，作为一个决策工具，用于帮助具有食品配送服务的餐厅，并提供一个模拟器来评估决策的影响。该平台由一个Technology-Specific Bridge（TSB）组成，用于与餐厅或模拟器进行通信。TSB使用PDDL模型表示嵌入在Unified Planning Framework（UPF）中的决策。决策涉及将顾客订单分配到车辆以及决定以什么顺序为顾客提供服务（对于每辆车），这是通过具有时间窗口的车辆路径问题（VRPTW）来完成的，这个问题的解决是高效的。我们展示了我们平台制定的决策可以通过使用真实历史数据集减少延迟配送量来提高顾客满意度。

    arXiv:2402.10725v1 Announce Type: new  Abstract: The global food delivery market provides many opportunities for AI-based services that can improve the efficiency of feeding the world. This paper presents the Cloud Kitchen platform as a decision-making tool for restaurants with food delivery and a simulator to evaluate the impact of the decisions. The platform consists of a Technology-Specific Bridge (TSB) that provides an interface for communicating with restaurants or the simulator. TSB uses a PDDL model to represent decisions embedded in the Unified Planning Framework (UPF). Decision-making, which concerns allocating customers' orders to vehicles and deciding in which order the customers will be served (for each vehicle), is done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool for this problem. We show that decisions made by our platform can improve customer satisfaction by reducing the number of delayed deliveries using a real-world historical dataset.
    
[^31]: BioFusionNet：基于深度学习的多特征与多模态数据融合在ER+乳腺癌中的生存风险分层

    BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion

    [https://arxiv.org/abs/2402.10717](https://arxiv.org/abs/2402.10717)

    BioFusionNet是一个深度学习框架，将图像特征与基因和临床数据融合，实现ER+乳腺癌患者的生存风险分层。

    

    Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VA...

    arXiv:2402.10717v1 Announce Type: cross  Abstract: Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to captur
    
[^32]: 一项关于跨语言词汇适应用于高效生成LLM推理的实证研究

    An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference

    [https://arxiv.org/abs/2402.10712](https://arxiv.org/abs/2402.10712)

    通过实证研究，本文探讨了各种跨语言词汇适应方法对提高生成LLM推理效率的影响。

    

    arXiv:2402.10712v1 通告类型: 跨领域 摘要: 最先进的生成大型语言模型(LLMs)的发展在很大程度上依赖于英语为中心的分词器、词汇和预训练数据。尽管一些LLMs具有多语言能力，但最近的研究表明，当生成英语以外的其他语言时，它们的推理效率会下降。这导致推理时间和成本增加。已经提出了跨语言词汇适应方法，用于将模型调整到目标语言，旨在提高下游性能。然而，这些方法对提高生成LLM推理效率的有效性尚未得到探究。在本文中，我们对五种生成LLMs（包括单语和多语模型）在四种语言类型多样且四种自然语言理解任务上进行了各种跨语言词汇适应方法的实证研究。

    arXiv:2402.10712v1 Announce Type: cross  Abstract: The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find th
    
[^33]: AutoSAT:通过大型语言模型自动优化SAT求解器

    AutoSAT: Automatically Optimize SAT Solvers via Large Language Models

    [https://arxiv.org/abs/2402.10705](https://arxiv.org/abs/2402.10705)

    AutoSAT通过大型语言模型自动优化SAT求解器中的启发式，减少人为干预，提升求解器能力，实现了即插即用操作，保证了容错性，在广泛实验中表现出优越性能。

    

    启发式在SAT求解器中至关重要，然而，并没有适用于所有问题实例的启发式规则。因此，通常需要为特定问题实例优化特定求解器。在这种情况下，我们提出了AutoSAT，这是一个新颖的框架，用于自动优化SAT求解器中的启发式。AutoSAT基于大型语言模型（LLMs），能够自动生成代码，进行评估，然后利用反馈进一步优化启发式，从而减少人为干预，增强求解器能力。AutoSAT基于即插即用的方式运行，消除了对广泛的初步设置和模型训练的需求，并促进了一种带有容错能力的思维链协作过程，确保启发式优化的稳健性。对使用冲突驱动子句学习（CDCL）求解器的广泛实验表明AutoSAT的整体性能优越，特别在解决某些特定的SAT问题时。

    arXiv:2402.10705v1 Announce Type: new  Abstract: Heuristics are crucial in SAT solvers, while no heuristic rules are suitable for all problem instances. Therefore, it typically requires to refine specific solvers for specific problem instances. In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT is based on Large Large Models (LLMs) which is able to autonomously generate code, conduct evaluation, then utilize the feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need for extensive preliminary setup and model training, and fosters a Chain of Thought collaborative process with fault-tolerance, ensuring robust heuristic optimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL) solver demonstrates the overall superior performance of AutoSAT, especially in solving some specific SAT pr
    
[^34]: 双生车联网络在密集区域中是否提升了它们的性能？

    Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas?

    [https://arxiv.org/abs/2402.10701](https://arxiv.org/abs/2402.10701)

    研究揭示了双生车联网络在密集区域中通过虚拟双生显著降低网络延迟、保持低延迟和提高计算速度的优势。

    

    本文研究数字双生体（DTs）对于提高在人口密集的城市地区中网络性能的潜力，特别关注车联网络。研究包括两个阶段。在第一阶段，我们利用交通数据和AI聚类来识别关键位置，特别是在高事故率的拥挤城区。在第二阶段，我们通过三种部署场景评估了双生车联网络的优势：基于边缘的双生体，基于云的双生体和混合双生体。我们的分析表明，双生显著减少网络延迟，虚拟双生胜过物理网络。虚拟双生即使在增加车辆密度的情况下也保持低延迟，例如在300辆车的情况下为15.05秒。此外，它们表现出更快的计算速度，在某些情况下，基于云的双生比基于边缘的双生快1.7倍。这些发现为效率车联网络提供了见解。

    arXiv:2402.10701v1 Announce Type: cross  Abstract: This paper investigates the potential of Digital Twins (DTs) to enhance network performance in densely populated urban areas, specifically focusing on vehicular networks. The study comprises two phases. In Phase I, we utilize traffic data and AI clustering to identify critical locations, particularly in crowded urban areas with high accident rates. In Phase II, we evaluate the advantages of twinning vehicular networks through three deployment scenarios: edge-based twin, cloud-based twin, and hybrid-based twin. Our analysis demonstrates that twinning significantly reduces network delays, with virtual twins outperforming physical networks. Virtual twins maintain low delays even with increased vehicle density, such as 15.05 seconds for 300 vehicles. Moreover, they exhibit faster computational speeds, with cloud-based twins being 1.7 times faster than edge twins in certain scenarios. These findings provide insights for efficient vehicular 
    
[^35]: 与遗忘呼应的解除链接：简化GNN中的边解除

    Unlink to Unlearn: Simplifying Edge Unlearning in GNNs

    [https://arxiv.org/abs/2402.10695](https://arxiv.org/abs/2402.10695)

    研究揭示了GNN中边解除过程的关键问题，即过度遗忘现象，提出了解决方法来解决损失函数引起的问题。

    

    随着对数据隐私的担忧加剧，图神经网络（GNN）中的解除学习已经成为学术界一个突出的研究前沿。这一概念在强调被遗忘权利方面起着关键作用，包括在用户请求时有选择性地从已训练的GNN中删除特定数据。我们的研究关注边的解除学习，这一过程对现实应用特别相关，因为它具有广泛的适用性。目前的最先进方法如GNNDelete可以消除特定边的影响，然而我们的研究揭示了这些方法的一个关键局限，称为过度遗忘。当解除学习过程无意中除去超出特定数据的过多信息时，会导致对剩余边的预测准确性显著下降。为了解决这个问题，我们确定了GNNDelete的损失函数作为过度遗忘现象的主要来源。

    arXiv:2402.10695v1 Announce Type: cross  Abstract: As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. 
    
[^36]: LongHeads: 多头注意力其实是一个长上下文处理器

    LongHeads: Multi-Head Attention is Secretly a Long Context Processor

    [https://arxiv.org/abs/2402.10685](https://arxiv.org/abs/2402.10685)

    LongHeads 提出了一个无需训练的框架，通过释放多头注意力的潜力来增强大型语言模型(LLM)处理长上下文的能力。

    

    大型语言模型(LLMs)在许多领域取得了令人印象深刻的表现，但由于有限长度泛化和注意力的二次计算需求，往往难以有效高效地处理较长的输入。 许多人试图通过限制在预训练长度内的注意力窗口来缓解这一问题。 然而，这些方法引入了新问题，如忽略中间上下文和需要额外训练。 为了解决这些问题，我们提出了LongHeads，一个无需训练的框架，通过释放多头注意力的潜力来增强LLM的长上下文能力。 我们允许每个头部选择并关注重要的上下文块，以处理分布长度，而不是让每个头部都参与全句注意力，这样做由于分布之外的问题而难以泛化到更长的序列。

    arXiv:2402.10685v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that
    
[^37]: 物理相关的MeshGraphNets（PI-MGNs）：适用于任意网格上非定态和非线性仿真的神经有限元求解器

    Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes

    [https://arxiv.org/abs/2402.10681](https://arxiv.org/abs/2402.10681)

    提出了物理相关的MeshGraphNets（PI-MGNs），可以在任意网格上进行非定态和非线性仿真，利用PINNs来减少对大量昂贵训练数据的依赖

    

    工程组件必须满足日益增长的技术需求，而且开发周期变得越来越短。为了应对这些挑战，需要一种整体化的方法，可以同时开发零件设计、材料系统和制造工艺。当前的方法使用数值仿真，然而对于迭代优化而言很快变得计算密集。数据驱动的机器学习方法可用于取代耗时和资源密集的数值仿真。具体而言，MeshGraphNets（MGNs）显示出令人满意的结果。它们可以在未知网格几何上进行快速准确的预测，同时对优化是完全可微的。然而，这些模型依赖于大量昂贵的训练数据，例如数值仿真。物理相关的神经网络（PINNs）提供了一种机会，使用偏微分方程而不是标记的数据来训练神经网络

    arXiv:2402.10681v1 Announce Type: cross  Abstract: Engineering components must meet increasing technological demands in ever shorter development cycles. To face these challenges, a holistic approach is essential that allows for the concurrent development of part design, material system and manufacturing process. Current approaches employ numerical simulations, which however quickly becomes computation-intensive, especially for iterative optimization. Data-driven machine learning methods can be used to replace time- and resource-intensive numerical simulations. In particular, MeshGraphNets (MGNs) have shown promising results. They enable fast and accurate predictions on unseen mesh geometries while being fully differentiable for optimization. However, these models rely on large amounts of expensive training data, such as numerical simulations. Physics-informed neural networks (PINNs) offer an opportunity to train neural networks with partial differential equations instead of labeled dat
    
[^38]: 多个LLM之间的网络形成与动态

    Network Formation and Dynamics Among Multi-LLMs

    [https://arxiv.org/abs/2402.10659](https://arxiv.org/abs/2402.10659)

    分析了多个LLM在社交网络中的行为，发现它们在给定网络结构并被询问形成网络偏好时表现出与人类社交动态一致的原则。

    

    社交网络影响行为、偏好和关系，在人类社会中对信息和规范的传播起着至关重要的作用。随着大型语言模型（LLMs）越来越多地融入社交和专业环境中，理解它们在社交网络和互动背景下的行为变得至关重要。我们的研究分析了标准网络结构和现实世界网络的行为，以确定多个LLMs的动态是否与人类社交动态一致。我们探讨了各种社交网络原则，包括微观层面的概念，如偏爱附着、三角闭合和同似性，以及宏观层面的概念，如社区结构和小世界现象。我们的研究发现表明，当向LLMs提供网络结构并询问它们对网络形成的偏好时，它们表现出所有这些原则。

    arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
    
[^39]: 分隔符是否可以提高思维链提示的效果？

    Can Separators Improve Chain-of-Thought Prompting?

    [https://arxiv.org/abs/2402.10645](https://arxiv.org/abs/2402.10645)

    分隔符的引入在思维链提示中显著提高了大型语言模型（LLMs）在复杂推理任务上的表现。

    

    Chain-of-thought (CoT) prompting是一种简单有效的方法，用于提高大型语言模型（LLMs）的推理能力。CoT的基本理念是通过将示例放在输入提示中，让LLMs逐步拆解他们的思维过程。然而，CoT提示的密集结构可能导致LLMs的认知负荷过重。受人类认知启发，我们引入了CoT-Sep，一种新颖的方法，在CoT提示中每个示例的末尾策略性地应用分隔符。这些分隔符旨在帮助LLMs在推理过程中更好地理解他们的思维过程。结果表明，与不使用分隔符的普通CoT相比，CoT-Sep显著提高了LLMs在复杂推理任务（如GSM-8K、AQuA、CSQA）上的表现。我们还研究了不同类型和位置的分隔符对多个LLMs（包括GPT-3.5-Turbo、GPT-4和LLaMA-27）的影响。

    arXiv:2402.10645v1 Announce Type: cross  Abstract: Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7
    
[^40]: “保持联系：通过模拟人类记忆在提取摘要中强化连贯性”

    `Keep it Together': Enforcing Cohesion in Extractive Summaries by Simulating Human Memory

    [https://arxiv.org/abs/2402.10643](https://arxiv.org/abs/2402.10643)

    本文通过模拟人类记忆来保持主题连贯性，实现了在提取式摘要中强化连贯性的目标，同时保持信息量和减少冗余。

    

    提取式摘要通常以一系列句子的形式呈现，它们之间没有预期的连贯性。本文旨在在摘要中强化连贯性，同时控制信息量和冗余，特别是当输入具有较高冗余性时。该方法在处理长输入时控制冗余，并在选择句子时平衡信息量和连贯性。我们的句子选择器模拟人类记忆以跟踪主题 -- 被建模为词链 -- 在名词短语之间强化连贯联系。在各种领域的实验证明，可以提取高度连贯的摘要，然而读者仍会感到这些摘要和仅考虑信息量或冗余性的摘要一样富有信息。提取的摘要在句子之间展示了平滑的主题转换，这些转换被词链所标识，这些链跨越相邻或几乎相邻的句子。

    arXiv:2402.10643v1 Announce Type: cross  Abstract: Extractive summaries are usually presented as lists of sentences with no expected cohesion between them. In this paper, we aim to enforce cohesion whilst controlling for informativeness and redundancy in summaries, in cases where the input exhibits high redundancy. The pipeline controls for redundancy in long inputs as it is consumed, and balances informativeness and cohesion during sentence selection. Our sentence selector simulates human memory to keep track of topics --modeled as lexical chains--, enforcing cohesive ties between noun phrases. Across a variety of domains, our experiments revealed that it is possible to extract highly cohesive summaries that nevertheless read as informative to humans as summaries extracted by only accounting for informativeness or redundancy. The extracted summaries exhibit smooth topic transitions between sentences as signaled by lexical chains, with chains spanning adjacent or near-adjacent sentence
    
[^41]: 在小波域说话：加速语音扩散模型的简单高效方法

    Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model

    [https://arxiv.org/abs/2402.10642](https://arxiv.org/abs/2402.10642)

    通过将生成目标重定向到小波域，我们成功将语音DDPMs的训练和推理速度提高了一倍。

    

    最近，去噪扩散概率模型（DDPMs）在各种生成任务中表现出色。然而，在语音合成领域，尽管DDPMs表现出色，但其长时间训练和大量推理成本阻碍了实际部署。现有方法主要集中在增强推理速度，而加速训练的方法通常需要对模型进行复杂修改，从而损害其通用性。为了解决上述挑战，我们提出了一个问题：通过修改语音信号本身，是否可能提高DDPMs的训练/推理速度和性能？在本文中，我们通过简单地将生成目标重定向到小波域，将语音DDPMs的训练和推理速度提高了一倍。该方法不仅取得了…

    arXiv:2402.10642v1 Announce Type: cross  Abstract: Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves c
    
[^42]: ContiFormer：用于不规则时间序列建模的连续时间Transformer

    ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling

    [https://arxiv.org/abs/2402.10635](https://arxiv.org/abs/2402.10635)

    将vanilla Transformer的关系建模扩展到连续时间领域，提出了ContiFormer。

    

    在不规则时间序列上建模连续时间动态对于解释连续发生的数据演变和相关性至关重要。 传统方法包括循环神经网络或Transformer模型通过强大的神经架构利用归纳偏差来捕获复杂模式。然而，由于它们的离散特性，它们在泛化到连续时间数据范式方面存在局限性。 尽管神经常微分方程（Neural ODEs）及其变体在处理不规则时间序列方面表现出有前途的结果，但它们往往无法捕获这些序列内部复杂的相关性。 同时对输入数据点之间的关系进行建模并捕获连续时间系统的动态变化是具有挑战性但又需求迫切的。

    arXiv:2402.10635v1 Announce Type: cross  Abstract: Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, 
    
[^43]: 基于图的时空降采样缺失数据预测

    Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling

    [https://arxiv.org/abs/2402.10634](https://arxiv.org/abs/2402.10634)

    通过 hierarchical spatiotemporal downsampling 处理缺失数据问题，结合可解释的注意机制，实现对时空预测的有效建模

    

    给定一组与空间中传感器点相关联、具有相互关系的同步时间序列，时空预测问题包括为每个点预测未来观测值。时空图神经网络通过将时间序列表示为图来实现引人注目的结果。然而，大多数现有方法依赖于一个常常不切实际的假设，即输入始终可用，并且在数据部分缺失时无法捕捉隐藏的时空动态。在这项工作中，我们通过分层时空降采样来解决这个问题。输入时间序列随着时间和空间的推移逐渐粗化，获得一组捕捉异质时间和空间动态的表示。在观测值和缺失数据模式的条件下，通过一个可解释的注意机制来组合这些表示以生成

    arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
    
[^44]: 带有逻辑约束的多任务基于核心学习

    Multitask Kernel-based Learning with Logic Constraints

    [https://arxiv.org/abs/2402.10617](https://arxiv.org/abs/2402.10617)

    将逻辑约束融合到多任务核心学习中，提出了一种通用方法来转换逻辑陈述为连续实现，以实现核心谓词计算输出。

    

    本文提出了一个通用框架，将逻辑约束形式的先验知识整合到核心机器中的一组任务函数中。逻辑命题提供了环境的部分表示，学习算法利用它与监督样本中可用的信息。我们考虑了一个多任务学习方案，其中多个特征空间上的一元谓词要由核心机器学习，高级抽象表示由这些谓词的逻辑子句组成，已知对于任何输入都成立。我们提出了一种通用方法，将逻辑子句转换为连续实现，处理核心谓词计算的输出。学习任务被制定为损失函数的原始优化问题，其结合了测量监督样本拟合度的项

    arXiv:2402.10617v1 Announce Type: cross  Abstract: This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised ex
    
[^45]: 通过辩论调节LLMs以生成可控的具有争议性的声明

    Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements

    [https://arxiv.org/abs/2402.10614](https://arxiv.org/abs/2402.10614)

    本文通过辩论调节LLMs，使其生成可控的支持用户定义论点的声明，改进了LLMs的可控性，并提出了DEBATunE流程。通过两个LLMs之间的多轮辩论生成高质量的训练数据，以支持生成有更高质量和更突出的声明。

    

    LLMs代表不同的人群，尤其是少数群体，并产生支持其多样化甚至有争议观点的声明对于创造一个包容的环境至关重要。然而，现有的LLMs缺乏足够的控制性来支持生成内容的立场，其中往往包含不一致、中立或有偏见的声明。在本文中，我们改进了LLMs在生成支持用户在提示中定义的论点的声明时的可控性。我们发现两个持有相反立场的LLMs之间的多轮辩论产生了更高质量和更突出的声明，这些声明对于改善LLMs的可控性是重要的训练数据。受此启发，我们开发了一种新颖的Debate & Tuning（“DEBATunE”）流程，通过微调LLMs生成通过辩论获得的声明。为了检验DEBATunE，我们整理了迄今为止涵盖710个争议性主题的最大数据集。

    arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate & tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
    
[^46]: 使用单词替换密码来越狱专有的大型语言模型

    Jailbreaking Proprietary Large Language Models using Word Substitution Cipher

    [https://arxiv.org/abs/2402.10601](https://arxiv.org/abs/2402.10601)

    本文使用密码技术编码了越狱提示，成功地绕过了大型语言模型对有害问题的检测，实验结果显示攻击成功率高达59.42%。

    

    大型语言模型（LLMs）遵循道德和伦理准则，但仍然容易受到名为Jailbreak的创意提示的影响，这些提示可以绕过对齐过程。然而，大多数越狱提示包含自然语言（主要是英语）中的有害问题，可以被LLMs自身检测到。本文提出了使用密码技术编码的越狱提示。我们首先在最先进的LLM，GPT-4上进行了一个试点研究，解码了使用各种密码技术加密的几个安全句子，发现简单的单词替换密码可以被最有效地解码。受此结果启发，我们使用这种编码技术来编写越狱提示。我们提供了将不安全单词映射到安全单词，并使用这些映射的单词提出不安全问题的映射。实验结果显示，我们提出的越狱攻击成功率（高达59.42%）。

    arXiv:2402.10601v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbrea
    
[^47]: 规模效率：研究微小语言模型在临床任务中的性能

    Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks

    [https://arxiv.org/abs/2402.10597](https://arxiv.org/abs/2402.10597)

    研究了不同Parameter Efficient Fine-tuning (PEFT)方法在临床决策任务中的适用性，发现除了LoRA外，大多数PEFT方法在各个模型规模和任务中性能不稳定，而LoRA在所有情况下性能都相对较高。PEFT方法在临床领域特别有效，尤其适用于可以操作的专门模型。

    

    大型语言模型（LLMs）进入研究和商业领域，引发了越来越大模型的趋势，最初承诺通用性，随后普遍希望缩小规模并创建专门模型，而无需进行完整微调，使用参数高效微调（PEFT）方法。我们对不同PEFT方法在临床决策任务中的适用性进行了调查，涵盖一系列模型规模，包括只有$25$百万参数的极小模型。我们的分析表明，大多数PEFT方法在不同任务之间的性能差异较大，除了LoRA外，LoRA在所有模型规模和任务中的性能保持相对较高，通常接近或达到完全微调性能。 PEFT方法在临床领域的有效性是显而易见的，特别是对于可以操作的专门模型

    arXiv:2402.10597v1 Announce Type: cross  Abstract: The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability, followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as $25$ million parameters.   Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can oper
    
[^48]: 高效多任务不确定性用于联合语义分割和单目深度估计

    Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation

    [https://arxiv.org/abs/2402.10580](https://arxiv.org/abs/2402.10580)

    本研究将不同的不确定性量化方法与联合语义分割和单目深度估计相结合，介绍了一种新的学生-教师蒸馏方法 EMUFormer，揭示了多任务学习对不确定性质量的益处。

    

    预测不确定性的量化成为应对深度神经网络普遍挑战（如过度自信、缺乏解释性和鲁棒性）的可能解决方案，尽管它往往具有较高的计算成本。许多现实世界的应用都具有多模态性质，因此受益于多任务学习。例如，在自动驾驶中，语义分割和单目深度估计的联合解决方案已被证明具有重要价值。在这项工作中，我们首先将不同的不确定性量化方法与联合语义分割和单目深度估计相结合，并评估它们相互之间的表现。此外，我们揭示了与单独解决这两个任务相比，多任务学习在不确定性质量方面的益处。基于这些见解，我们引入了EMUFormer，一种新的学生-教师蒸馏方法，用于联合语义分割和单目深度估计。

    arXiv:2402.10580v1 Announce Type: cross  Abstract: Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular d
    
[^49]: 符号自编码用于自监督序列学习

    Symbolic Autoencoding for Self-Supervised Sequence Learning

    [https://arxiv.org/abs/2402.10575](https://arxiv.org/abs/2402.10575)

    符号自编码（$\Sigma$AE）是一个自监督框架，通过最小化重构损失和平行数据的监督损失来优化连接两个生成模型，实现了在转导任务上显著提升性能。

    

    传统语言模型擅长预测文本序列中的下一个标记，但在不同符号系统之间执行转导任务时通常会遇到困难，特别是在平行数据稀缺的情况下。为解决这一问题，我们引入了符号自编码（$\Sigma$AE），这是一个自监督框架，利用了丰富的不平行数据和有限的平行数据。$\Sigma$AE通过一个离散瓶颈层连接两个生成模型，并通过最小化重构损失（与平行数据的监督损失同时进行优化）进行端到端优化，使得离散瓶颈生成的序列可以被读取为转导的输入序列。我们还开发了基于梯度的方法，实现了尽管存在瓶颈离散性，仍能进行高效的自监督序列学习。我们的结果表明，$\Sigma$AE显著提高了转导任务的性能，即使使用了最少量的平行数据。

    arXiv:2402.10575v1 Announce Type: cross  Abstract: Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \textit{symbolic autoencoding} ($\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\Sigma$AE significantly enhances performance on transduction tasks, even with min
    
[^50]: 具有偏置的直接偏好优化

    Direct Preference Optimization with an Offset

    [https://arxiv.org/abs/2402.10571](https://arxiv.org/abs/2402.10571)

    提出了一种新颖的直接偏好优化方法，即具有偏置的DPO（ODPO），在微调过程中不同对待每个偏好对。

    

    直接偏好优化（DPO）是一种成功的微调策略，用于使大型语言模型与人类偏好保持一致，而无需训练奖励模型或使用强化学习。本文提出了一种DPO的泛化形式，称为具有偏置的DPO（ODPO），在微调过程中不将每个偏好对视为相等。

    arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
    
[^51]: 在InSaAF中融入安全性，通过准确性和公平性 | LLM是否已经准备好进入印度法律领域？

    InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?

    [https://arxiv.org/abs/2402.10567](https://arxiv.org/abs/2402.10567)

    本研究在印度法律领域探讨了大型语言模型（LLMs）在处理社会因素时的能力，提出了结合公平性和准确性的新指标$LSS_{\beta}$，并评估了模型在二元法律推理任务中的表现以及在印度社会各种不平等方面的公平性展示。

    

    语言技术和人工智能的最新进展已经导致提出了众多语言模型，用于执行法律领域的各种任务，从预测判决到生成摘要。尽管它们具有巨大潜力，但已经证明这些模型学习并展示社会偏见，并做出不公平的预测。在这项研究中，我们探讨了当涉及社会因素时大型语言模型（LLMs）在印度法律领域执行任务的能力。我们提出了一种新颖的度量标准，$\beta$-加权的$\textit{法律安全分数($LSS_{\beta}$)}$，将LLM的公平性和准确性两个方面结合起来。我们通过考虑LLM在$\textit{二元法律推理}$任务中的表现以及其在印度社会各种不平等方面的公平展示来评估LLMs的安全性。LLaMA和LLaMA--2模型的任务表现和公平得分表明...

    arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
    
[^52]: 消除否定导致的强幻觉

    Strong hallucinations from negation and how to fix them

    [https://arxiv.org/abs/2402.10543](https://arxiv.org/abs/2402.10543)

    论文针对语言模型在推理中造成的强幻觉问题，提出了一种处理否定的新方法，可以改善模型性能而无需使用稀疏负数据训练。

    

    尽管语言模型（LMs）在许多任务上表现出色，但仍然在推理方面存在困难，有时会提供由于逻辑不连贯而不可能成立的响应。我们称这种响应为\textit{强幻觉}，并证明它们源于LM计算其内部表示的逻辑运算符和从这些表示中产生的输出。重点关注否定，我们提供了一种新颖的解决方案，其中否定不是作为潜在表示的另一个元素，而是作为\textit{LM潜在表示上的一个操作，约束它们可能的演变方式}。我们展示了我们的方法改善了在带否定的填空提示和自然语言推理任务中的模型性能，而无需对稀疏负数据进行训练。

    arXiv:2402.10543v1 Announce Type: cross  Abstract: Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.
    
[^53]: LLM生成的解释的特性和挑战

    Properties and Challenges of LLM-Generated Explanations

    [https://arxiv.org/abs/2402.10532](https://arxiv.org/abs/2402.10532)

    该研究探讨了大型语言模型生成的解释在多领域指导微调数据集上的特性，发现生成的解释表现出选择性和包含说明性元素，但较少是主观或误导性的。

    

    大型语言模型（LLMs）的自我合理化能力在限定环境中得到了探索，使用特定任务/数据集。然而，当前LLMs并不（仅）依赖于特定注释的数据；然而，它们经常解释它们的输出。生成的解释的特性受预训练语料库和用于指导微调的目标数据的影响。由于预训练语料库包含大量野外人类编写的解释，我们假设LLMs采用了人类解释的共同特性。通过分析多域指导微调数据集的输出，我们发现生成的解释表现出选择性并包含说明性元素，但很少是主观或误导性的。我们讨论了属性存在或缺失的原因和后果。特别是，我们概述了根据LLMs预训练语料库和微调数据的性质，这些属性存在或缺失的积极和消极影响。

    arXiv:2402.10532v1 Announce Type: cross  Abstract: The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets. However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning. As the pre-training corpus includes a large amount of human-written explanations "in the wild", we hypothesise that LLMs adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the 
    
[^54]: 我们能否逐步验证错误答案检测？

    Can We Verify Step by Step for Incorrect Answer Detection?

    [https://arxiv.org/abs/2402.10528](https://arxiv.org/abs/2402.10528)

    通过推理链来预测大型语言模型输出的准确性，我们引入了一个新的基准R2PE，并提出了处理可辨识性评分（PDS）框架。

    

    Chain-of-Thought（CoT）提示在增强大型语言模型（LLMs）的推理能力方面取得了重大进展。先前的研究开发了各种扩展的CoT，主要集中在增强最终任务的性能上。此外，已经有研究评估了CoT中推理链的质量。这引发了一个有趣的问题：通过仔细审查它们生成的推理链，是否可以预测LLMs输出的准确性？为了回答这个研究问题，我们引入了一个基准，R2PE，专门设计用于探究不同领域涵盖五个不同推理任务中推理链与性能之间的关系。该基准旨在基于推理步骤衡量LLMs最终输出的虚假性。为了充分利用多个推理链中的信息，我们提出了打败常识分数（PDS）框架。

    arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
    
[^55]: LLM比较器：用于大型语言模型并行评估的可视化分析

    LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models

    [https://arxiv.org/abs/2402.10524](https://arxiv.org/abs/2402.10524)

    LLM Comparator是一种用于交互式分析自动并行评估结果的新型可视化工具，支持用户理解模型表现优劣和不同之处，解决了大型语言模型评估中的可扩展性和可解释性挑战。

    

    自动并行评估已成为评估大型语言模型（LLMs）响应质量的一种有前途的方法。然而，分析这种评估方法的结果存在可扩展性和可解释性挑战。本文提出了LLM比较器，这是一种新颖的可视化分析工具，用于交互式地分析自动并行评估结果。该工具支持用户进行交互式工作流，以了解为什么和何时模型比基准模型表现更好或更差，以及两个模型的响应在质量上有何不同。我们通过与一家大型科技公司的研究人员和工程师密切合作，迭代设计和开发了该工具。本文详细介绍了我们识别的用户挑战、该工具的设计和开发，以及定期评估其模型的参与者的观察研究。

    arXiv:2402.10524v1 Announce Type: cross  Abstract: Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.
    
[^56]: 控制可控蛋白质序列设计的生成AI：一项调查

    Generative AI for Controllable Protein Sequence Design: A Survey

    [https://arxiv.org/abs/2402.10516](https://arxiv.org/abs/2402.10516)

    人工智能领域的进步推动了蛋白质设计领域朝着前所未有的革命方向发展，这篇论文系统地审查了用于可控蛋白质序列设计的生成AI的最新进展。

    

    设计具有针对性功能的新型蛋白质序列是蛋白质工程中的一个核心主题，影响着药物发现和酶工程等各个领域。然而，由于时间和金融限制，导航这个庞大的组合搜索空间仍然是一个严峻挑战。随着人工智能领域的革命性进步，特别是生成模型和优化算法的突破性进展，这种情况正在迅速发展，推动蛋白质设计领域朝着前所未有的革命方向发展。在这项调查中，我们系统地审查了用于可控蛋白质序列设计的生成AI的最新进展。为了奠定基础，我们首先概述了蛋白质序列设计中涉及的约束性基本任务，并介绍了关键的生成模型和优化算法。然后，我们深入审查了每个设计任务，并讨论了相关应用。

    arXiv:2402.10516v1 Announce Type: cross  Abstract: The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in generative AI for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finall
    
[^57]: 使用自适应通道感知超宽带DL-TDOA的高效室内定位

    Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA

    [https://arxiv.org/abs/2402.10515](https://arxiv.org/abs/2402.10515)

    提出并实现了一种新颖的低功耗通道感知动态频率DL-TDOA测距算法，主要应对室内定位中的非直射信道路径和信号中断效应。

    

    在各种超宽带（UWB）测距方法中，缺乏上行通信或集中计算使得下行到达时间差（DL-TDOA）定位最适合大规模工业部署。然而，在部署区域中的临时或永久障碍物通常会导致非直射（NLOS）信道路径和信号中断效应，从而导致定位误差。本文提出并实现了一种新颖的低功耗通道感知动态频率DL-TDOA测距算法。它包括基于卷积神经网络（CNN）的NLOS概率预测器、动态测距频率控制模块和一个IMU传感器。

    arXiv:2402.10515v1 Announce Type: cross  Abstract: Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink communication or centralized computation makes downlink time-difference-of-arrival (DL-TDOA) localization the most suitable for large-scale industrial deployments. However, temporary or permanent obstacles in the deployment region often lead to non-line-of-sight (NLOS) channel path and signal outage effects, which result in localization errors. Prior research has addressed this problem by increasing the ranging frequency, which leads to a heavy increase in the user device power consumption. It also does not contribute to any increase in localization accuracy under line-of-sight (LOS) conditions. In this paper, we propose and implement a novel low-power channel-aware dynamic frequency DL-TDOA ranging algorithm. It comprises NLOS probability predictor based on a convolutional neural network (CNN), a dynamic ranging frequency control module, and an IMU sensor-ba
    
[^58]: 可以变压器预测振动吗？

    Can Transformers Predict Vibrations?

    [https://arxiv.org/abs/2402.10511](https://arxiv.org/abs/2402.10511)

    提出了一种新颖的基于Transformer的模型Resoformer，用于预测电动汽车传动轴上的扭振，解决了当前阻尼技术只能在共振发生后检测到的问题

    

    准确预测时间序列振动是电动汽车（EVs）的重要研究问题。EVs在崎岖地形上行驶时经常会产生振动，被称为扭振共振。这种由电机和轮胎振动之间的相互作用引起的共振会在车辆传动轴上施加过大负荷。然而，当前的阻尼技术仅在传动轴扭矩振动幅度达到一定阈值后才能检测到共振，导致在检测时传动轴上承受重要负荷。在本研究中，我们提出了一种新颖的方法来解决这一问题，引入了Resoformer，一种用于预测扭振的基于transformer的模型。Resoformer利用电机转速的时间序列作为输入，并在输入序列之后的特定分位数处预测传动轴扭振的幅度。通过计算递归和卷积之间的注意力

    arXiv:2402.10511v1 Announce Type: cross  Abstract: Highly accurate time-series vibration prediction is an important research issue for electric vehicles (EVs). EVs often experience vibrations when driving on rough terrains, known as torsional resonance. This resonance, caused by the interaction between motor and tire vibrations, puts excessive loads on the vehicle's drive shaft. However, current damping technologies only detect resonance after the vibration amplitude of the drive shaft torque reaches a certain threshold, leading to significant loads on the shaft at the time of detection. In this study, we propose a novel approach to address this issue by introducing Resoformer, a transformer-based model for predicting torsional resonance. Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series. By calculating the attention between recursive and convolutio
    
[^59]: 人类目标识别作为贝叶斯推断：探讨行动、时序和目标可解性的影响

    Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability

    [https://arxiv.org/abs/2402.10510](https://arxiv.org/abs/2402.10510)

    通过贝叶斯框架探讨了人类目标识别中行动、时序和目标可解性的作用，并开发了与人类推断更匹配的目标识别模型。

    

    目标识别是一种基本认知过程，使个体能够根据可用线索推断意图。当前的目标识别算法通常只考虑观察到的行动作为输入，但在这里我们使用贝叶斯框架来探讨行动、时序和目标可解性在目标识别中的作用。我们分析了人类在Sokoban领域中对目标识别问题的响应，发现行动被赋予最重要的作用，但在某些情况下，时序和可解性也影响目标识别，特别是当行动无信息时。我们利用这些发现开发了一个目标识别模型，其与现有算法相比更能匹配人类的推断。我们的工作为人类目标识别提供了新的见解，并迈出了更接近人类的AI模型的一步。

    arXiv:2402.10510v1 Announce Type: cross  Abstract: Goal recognition is a fundamental cognitive process that enables individuals to infer intentions based on available cues. Current goal recognition algorithms often take only observed actions as input, but here we use a Bayesian framework to explore the role of actions, timing, and goal solvability in goal recognition. We analyze human responses to goal-recognition problems in the Sokoban domain, and find that actions are assigned most importance, but that timing and solvability also influence goal recognition in some cases, especially when actions are uninformative. We leverage these findings to develop a goal recognition model that matches human inferences more closely than do existing algorithms. Our work provides new insight into human goal recognition and takes a step towards more human-like AI models.
    
[^60]: 通过主动偏好优化实现经验证的样本效率的RLHF

    Provably Sample Efficient RLHF via Active Preference Optimization

    [https://arxiv.org/abs/2402.10500](https://arxiv.org/abs/2402.10500)

    通过Active Preference Optimization算法，在Bradley-Terry-Luce偏好模型下实现了RLHF的样本效率提高，优化了对提示收集偏好数据的策略。

    

    强化学习从人类反馈（RLHF）在将大型语言模型（LLMs）与人类偏好相一致方面至关重要。虽然这些对齐的生成模型已经在各种任务中展示出令人印象深刻的能力，但是依赖高质量的人类偏好数据在实际RLHF实施中构成了昂贵的瓶颈。因此，需要更好和自适应的数据收集策略。为此，我们将RLHF以上下文偏好赌博机问题的形式框定，其中提示作为上下文，并表明通过随机选择提示收集偏好数据的天真方式导致一个在奖励方面具有$\Omega(1)$次优性差距的策略。然后，我们提出了$\textit{Active Preference Optimization}$（$\texttt{APO}$）算法，该算法积极选择提示以收集偏好数据。在Bradley-Terry-Luce（BTL）偏好模型下，\texttt{APO}实现了样本效率，而不会妥协于polic

    arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
    
[^61]: 比较多语言生成中幻觉检测指标

    Comparing Hallucination Detection Metrics for Multilingual Generation

    [https://arxiv.org/abs/2402.10496](https://arxiv.org/abs/2402.10496)

    本研究比较了多语言生成中不同幻觉检测指标的效果，发现基于自然语言推理（NLI）的指标在高资源语言的句子级别表现良好，但通常无法检测到原子事实幻觉。

    

    尽管已提出许多针对英文文本的自动幻觉检测技术，但它们在多语言环境中的效果尚未被探索。本文旨在填补对这些幻觉检测指标在非英语语言上表现如何的认识上的差距。我们评估了各种检测指标的有效性，包括诸如ROUGE和命名实体重叠以及基于自然语言推理（NLI）的指标，在多种语言的传记摘要中检测幻觉；我们还评估这些不同指标之间的相关性，以判断它们是否衡量相同的现象。我们的实证分析显示，虽然词汇指标显示出有限的有效性，但基于NLI的指标在高资源语言中在句子级别表现良好。相反，NLI-based指标通常无法检测到原子事实幻觉。我们的研究结果突显了多语言幻觉检测中的现有差距。

    arXiv:2402.10496v1 Announce Type: cross  Abstract: While many automatic hallucination detection techniques have been proposed for English texts, their effectiveness in multilingual contexts remains unexplored. This paper aims to bridge the gap in understanding how these hallucination detection metrics perform on non-English languages. We evaluate the efficacy of various detection metrics, including lexical metrics like ROUGE and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at detecting hallucinations in biographical summaries in many languages; we also evaluate how correlated these different metrics are to gauge whether they measure the same phenomena. Our empirical analysis reveals that while lexical metrics show limited effectiveness, NLI-based metrics perform well in high-resource languages at the sentence level. In contrast, NLI-based metrics often fail to detect atomic fact hallucinations. Our findings highlight existing gaps in multilingual hallucinati
    
[^62]: 发展一种预测小麦枯叶病严重程度的最佳模型（阿尔西和巴勒区案例研究）

    Developing an Optimal Model for Predicting the Severity of Wheat Stem Rust (Case study of Arsi and Bale Zone)

    [https://arxiv.org/abs/2402.10492](https://arxiv.org/abs/2402.10492)

    通过比较三种不同的人工神经网络方法，研究发现通用回归神经网络（GRNN）在预测小麦枯叶病严重程度方面表现出有效性，并需要较少的训练时间。

    

    这项研究利用了三种人工神经网络（ANN）方法，分别是具有不同训练、传输、分割和学习功能的反向传播神经网络（BPNN），径向基函数神经网络（RBFNN）和通用回归神经网络（GRNN），来预测枯叶病的严重程度。考虑了参数如平均最高温度、平均最低温度、平均降雨量、平均气温、平均相对湿度和不同小麦品种。统计分析表明，GRNN表现出有效的预测能力，并且相比其他模型需要更少的训练时间。此外，结果表明总季节降雨量对小麦枯叶病的发展有积极影响。

    arXiv:2402.10492v1 Announce Type: cross  Abstract: This research utilized three types of artificial neural network (ANN) methodologies, namely Backpropagation Neural Network (BPNN) with varied training, transfer, divide, and learning functions; Radial Basis Function Neural Network (RBFNN); and General Regression Neural Network (GRNN), to forecast the severity of stem rust. It considered parameters such as mean maximum temperature, mean minimum temperature, mean rainfall, mean average temperature, mean relative humidity, and different wheat varieties. The statistical analysis revealed that GRNN demonstrated effective predictive capability and required less training time compared to the other models. Additionally, the results indicated that total seasonal rainfall positively influenced the development of wheat stem rust.   Keywords: Wheat stem rust, Back propagation neural network, Radial Basis Function Neural Network, General Regression Neural Network.
    
[^63]: 针对多维时间序列预测的随机投影层

    Random Projection Layers for Multidimensional Time Sires Forecasting

    [https://arxiv.org/abs/2402.10487](https://arxiv.org/abs/2402.10487)

    提出了一种全MLP时间序列预测架构RPMixer，通过将随机投影层集成到模型中，增加了块输出之间的多样性，提高了整体性能

    

    多层感知器（MLP）混合模型已被证明对时间序列预测问题有效。然而，当将此类模型应用于高维时间序列（例如空间-时间数据集中的时间序列）时，由于过拟合问题，其性能可能会下降。本文提出了一种全MLP时间序列预测架构，称为RPMixer。我们的方法利用了深度神经网络的集成式行为，其中网络中的每个单独块的作用类似于集成模型中的基本学习器，特别是在引入身份映射残差连接时。通过将随机投影层集成到我们的模型中，我们增加了块输出之间的多样性，从而提高了RPMixer的整体性能。对大规模空间-时间预测基准数据集进行的大量实验表明，我们提出的方法胜过了

    arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
    
[^64]: 基于表情符号的加密资产市场反应

    Emoji Driven Crypto Assets Market Reactions

    [https://arxiv.org/abs/2402.10481](https://arxiv.org/abs/2402.10481)

    该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。

    

    在加密货币领域，诸如Twitter之类的社交媒体平台已经成为影响市场趋势和投资者情绪的关键因素。在我们的研究中，我们利用GPT-4和经过微调的基于BERT模型的多模态情感分析，重点关注表情符号情绪对加密货币市场的影响。通过将表情符号转化为可量化的情感数据，我们将这些见解与BTC价格和VCRIX指数等关键市场指标进行了相关联。这种方法可以用于开发旨在利用社交媒体元素识别和预测市场趋势的交易策略。关键是，我们的研究结果表明，基于表情符号情绪的策略可以有助于避免重大市场下挫，并有助于回报的稳定。这项研究强调了将先进的基于人工智能的分析整合到金融策略中的实际益处，并提供了一种新的方式来看待市场预测。

    arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
    
[^65]: 具有对抗课程图对比学习的成对增强

    Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation

    [https://arxiv.org/abs/2402.10468](https://arxiv.org/abs/2402.10468)

    提出一种对抗课程图对比学习（ACGCL）框架，利用成对增强生成具有可控相似性的图级正负样本，同时通过子图对比学习来识别有效的图模式

    

    图对比学习（GCL）已成为图表示学习领域中的一个关键技术。我们提出了一种创新的框架：对抗课程图对比学习（ACGCL），利用成对增强的优点生成具有可控相似性的图级正负样本，以及子图对比学习来识别其中的有效图模式。

    arXiv:2402.10468v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) has emerged as a pivotal technique in the domain of graph representation learning. A crucial aspect of effective GCL is the caliber of generated positive and negative samples, which is intrinsically dictated by their resemblance to the original data. Nevertheless, precise control over similarity during sample generation presents a formidable challenge, often impeding the effective discovery of representative graph patterns. To address this challenge, we propose an innovative framework: Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on the merits of pair-wise augmentation to engender graph-level positive and negative samples with controllable similarity, alongside subgraph contrastive learning to discern effective graph patterns therein. Within the ACGCL framework, we have devised a novel adversarial curriculum training methodology that facilitates progressive learning by se
    
[^66]: 将大型语言模型作为零-shot对话状态追踪器通过函数调用

    Large Language Models as Zero-shot Dialogue State Tracker through Function Calling

    [https://arxiv.org/abs/2402.10466](https://arxiv.org/abs/2402.10466)

    本研究提出了一种通过函数调用将大型语言模型用于零-shot对话状态追踪的新方法，能够在任务导向对话中取得出色的性能，适应不同领域而无需大量数据收集或模型调整。

    

    大型语言模型（LLMs）在会话系统中日益普遍，这是因为它们在一般情境中具有先进的理解和生成能力。然而，在需要不仅进行响应生成还需要在特定任务和领域内进行有效对话状态追踪（DST）的任务导向对话（TOD）中，它们的有效性仍不尽人意。在这项工作中，我们提出了一种通过函数调用解决LLMs中的DST的新方法FnCTOD。这种方法改进了零-shot DST，使其能够适应各种领域，而无需进行大量数据收集或模型调整。我们的实验结果表明，我们的方法在使用开源或专有LLMs时都取得了出色的性能：通过上下文提示，使得各种7B或13B参数模型超越了之前由ChatGPT实现的最新技术成果（SOTA）的水平，并提高了ChatGPT的性能，击败了

    arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
    
[^67]: 评估和改进口语理解中的持续学习

    Evaluating and Improving Continual Learning in Spoken Language Understanding

    [https://arxiv.org/abs/2402.10427](https://arxiv.org/abs/2402.10427)

    提出了一种评估方法来统一评估口语理解中的持续学习算法在稳定性、可塑性和泛化能力方面的整体表现，并展示了引入不同知识蒸馏如何改善模型性能。

    

    持续学习已经成为各种任务中越来越重要的挑战，包括口语理解。在口语理解中，其目标是有效处理新概念的出现和不断演变的环境。持续学习算法的评估通常涉及评估模型的稳定性、可塑性和泛化能力作为标准的基本方面。然而，现有的持续学习指标主要集中在其中一个或两个属性上。它们忽视了整体表现在所有任务上，并没有充分解开模型内的可塑性与稳定性/泛化能力之间的权衡。在本研究中，我们提出了一种评估方法，可以在持续学习中统一评估稳定性、可塑性和泛化能力。通过采用所提出的度量标准，我们演示了如何引入各种知识蒸馏来改进...

    arXiv:2402.10427v1 Announce Type: cross  Abstract: Continual learning has emerged as an increasingly important challenge across various tasks, including Spoken Language Understanding (SLU). In SLU, its objective is to effectively handle the emergence of new concepts and evolving environments. The evaluation of continual learning algorithms typically involves assessing the model's stability, plasticity, and generalizability as fundamental aspects of standards. However, existing continual learning metrics primarily focus on only one or two of the properties. They neglect the overall performance across all tasks, and do not adequately disentangle the plasticity versus stability/generalizability trade-offs within the model. In this work, we propose an evaluation methodology that provides a unified evaluation on stability, plasticity, and generalizability in continual learning. By employing the proposed metric, we demonstrate how introducing various knowledge distillations can improve diffe
    
[^68]: 使用鹈鹕汤框架理解上下文学习

    Understanding In-Context Learning with a Pelican Soup Framework

    [https://arxiv.org/abs/2402.10424](https://arxiv.org/abs/2402.10424)

    提出了一个鹈鹕汤框架，包括常识知识库、自然语言分类任务的形式化以及意义关联的概念，并建立了一个$O(1/T)$的上下文学习损失界限，能够解释对未见任务的泛化。

    

    许多现有关于自然语言处理中的上下文学习的理论分析是基于潜变量模型的，它们存在理论与实践之间的差距。我们旨在通过提出一个理论框架，即鹈鹕汤框架，来弥合这些差距。在这个框架中，我们引入了（1）常识知识库的概念，（2）自然语言分类任务的一般形式化，以及（3）意义关联的概念。在这个框架下，我们可以建立一个$\mathcal{O}(1/T)$的上下文学习损失界限，这里$T$是演示中示例-标签对的数量。与先前的作品相比，我们的界限反映了动词选择和指令调整的影响。一个额外的"原子概念"概念使我们的框架能够解释对语言模型训练数据中未见任务的泛化。最后，我们提出了一个玩具设置，Calcutec，

    arXiv:2402.10424v1 Announce Type: cross  Abstract: Many existing theoretical analyses of in-context learning for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a $\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of instruction tuning. An additional notion of \textit{atom concepts} makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec,
    
[^69]: 连接点：数据集浓缩、差分隐私和对抗不确定性

    Connect the dots: Dataset Condensation, Differential Privacy, and Adversarial Uncertainty

    [https://arxiv.org/abs/2402.10423](https://arxiv.org/abs/2402.10423)

    我们连接数据集浓缩、差分隐私和对抗不确定性，提出通过对抗不确定性选择最优噪声水平$\epsilon$的方案，以保证高保真数据的同时提供隐私保护。

    

    我们的工作旨在通过与($\epsilon$, $\delta$)-差分隐私相联系，通过对抗不确定性选择最优噪声$\epsilon$，理解数据集浓缩的基本机制。我们可以回答有关数据集浓缩程序内部运作方式的问题。先前的研究证明了数据集浓缩（DC）与($\epsilon$, $\delta$)-差分隐私之间的联系。然而，现有的关于去除数据集浓缩以获得能够创建高保真合成数据的$\epsilon$下限估计的工作尚不清楚。我们建议对抗不确定性是实现最优噪声水平$\epsilon$的最合适方法。作为数据集浓缩内部动态的一部分，我们采用了一个保证高保真数据同时提供隐私性的噪声估计满意方案。

    arXiv:2402.10423v1 Announce Type: cross  Abstract: Our work focuses on understanding the underpinning mechanism of dataset condensation by drawing connections with ($\epsilon$, $\delta$)-differential privacy where the optimal noise, $\epsilon$, is chosen by adversarial uncertainty \cite{Grining2017}. We can answer the question about the inner workings of the dataset condensation procedure. Previous work \cite{dong2022} proved the link between dataset condensation (DC) and ($\epsilon$, $\delta$)-differential privacy. However, it is unclear from existing works on ablating DC to obtain a lower-bound estimate of $\epsilon$ that will suffice for creating high-fidelity synthetic data. We suggest that adversarial uncertainty is the most appropriate method to achieve an optimal noise level, $\epsilon$. As part of the internal dynamics of dataset condensation, we adopt a satisfactory scheme for noise estimation that guarantees high-fidelity data while providing privacy.
    
[^70]: 将关于信念的语言接地于贝叶斯心灵理论

    Grounding Language about Belief in a Bayesian Theory-of-Mind

    [https://arxiv.org/abs/2402.10416](https://arxiv.org/abs/2402.10416)

    语义基础置于贝叶斯心灵理论中，通过模拟人们共同推断出解释代理人行为的一致性目标、信念和计划集合，再通过认识逻辑评估有关代理人信念的陈述，解释了人类信念归因的分级性和组合性，以及其与目标和计划的密切联系。

    

    尽管信念是无法直接观察的心理状态，人类常常使用丰富的组合语言来描述他人的想法和知识。这项研究通过将信念陈述的语义基础置于贝叶斯心灵理论中，为解释人类如何解释他人隐藏的认识内容迈出了一步：通过建模人类如何共同推断出解释一个代理人行动的一致性目标、信念和计划集合，然后通过认识逻辑对有关代理人信念的陈述进行评估，我们的框架为信念提供了一个概念角色语义，解释了人类信念归因的分级性和组合性，以及它们与目标和计划的密切联系。我们通过研究人们在观察一个代理人解决问题时是如何归因目标和信念的来评估这一框架。

    arXiv:2402.10416v1 Announce Type: new  Abstract: Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others' beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by grounding the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent's actions, then evaluating statements about the agent's beliefs against these inferences via epistemic logic, our framework provides a conceptual role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and beliefs while watching an agent solve
    
[^71]: 通过专家加权来衡量和减少LLM在没有黄金标准答案的情况下的虚构

    Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting

    [https://arxiv.org/abs/2402.10412](https://arxiv.org/abs/2402.10412)

    提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。

    

    LLM幻觉，即生成事实不正确但看似令人信服的答案，目前是LLM可信度和可靠性的主要威胁。解决这一复杂问题的第一步是对其进行衡量。然而，现有的幻觉度量标准需要具有具有黄金标准答案的基准数据集，即人类编写的“最佳”或“正确”答案。这种要求使幻觉测量成本高昂，并容易出现人为误差。在这项工作中，我们提出了通过加权LLM对事实性进行评估（FEWL），这是第一个专门为金标准答案缺失时设计的幻觉度量标准。FEWL利用了现成的LLM答案作为黄金标准答案的代理。关键挑战是如何有效地量化参考LLM的专业知识。我们展示FEWL具有一定的理论保证，并在实证中证明它更准确。度量虚构。

    arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
    
[^72]: 通过图表示学习理解大型语言模型调查论文分类法

    Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning

    [https://arxiv.org/abs/2402.10409](https://arxiv.org/abs/2402.10409)

    通过图结构信息在共类别图上利用图表示学习技术，可以在LLMs的预训练模型微调和零-shot/few-shot分类方面显著优于语言模型，揭示了弱标签微调LLMs的潜力。

    

    随着大型语言模型（LLMs）的新研究持续进行，难以跟上新的研究和模型。为帮助研究人员综合新研究成果，许多人写了调研论文，但即使这些论文也变得越来越多。本文提出了一种自动将调研论文分配到分类法的方法。我们收集了144篇LLM调研论文的元数据，并探讨了三种范例来对分类法内的论文进行分类。我们的工作表明，在共类别图上利用图结构信息可以显著优于两个范例中的语言模型; 使用LLMs进行预训练语言模型的微调和零-shot/few-shot分类。我们发现我们的模型超过了平均人类识别水平，并且利用较小模型生成的弱标签来微调LLMs（本研究中的GCN等）可能比使用地面实况标签更有效，揭示了从弱到强的潜力。

    arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro
    
[^73]: 通过视觉分析解释生成扩散模型，以实现可解释的决策过程

    Explaining generative diffusion models via visual analysis for interpretable decision-making process

    [https://arxiv.org/abs/2402.10404](https://arxiv.org/abs/2402.10404)

    通过视觉分析，这项研究提出了三个研究问题，解释生成扩散模型的过程，设计了可视化工具，并展示了如何逐步生成输出并强调与基础视觉概念的关系。

    

    扩散模型在生成任务中表现出色，然而解释扩散过程仍具挑战性，因为它是一系列难以解释的去噪图像序列。为了解决这一问题，我们从模型生成的视觉概念和模型在每个时间步骤关注的区域的角度提出了三个研究问题来解释扩散过程。我们设计了可视化扩散过程和回答上述研究问题的工具，使扩散过程易于人理解。通过利用工具进行各种视觉分析实验的结果，解释了扩散过程中输出是如何逐渐生成的，强调了每个时间步骤的去噪程度，并突出显示了与基础视觉概念的关系。

    arXiv:2402.10404v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable performance in generation tasks. Nevertheless, explaining the diffusion process remains challenging due to it being a sequence of denoising noisy images that are difficult for experts to interpret. To address this issue, we propose the three research questions to interpret the diffusion process from the perspective of the visual concepts generated by the model and the region where the model attends in each time step. We devise tools for visualizing the diffusion process and answering the aforementioned research questions to render the diffusion process human-understandable. We show how the output is progressively generated in the diffusion process by explaining the level of denoising and highlighting relationships to foundational visual concepts at each time step through the results of experiments with various visual analyses using the tools. Throughout the training of the diffusion model, 
    
[^74]: 从分段三线性网络中导出多面体复合体

    Polyhedral Complex Derivation from Piecewise Trilinear Networks

    [https://arxiv.org/abs/2402.10403](https://arxiv.org/abs/2402.10403)

    本文以三线性插值方法作为位置编码，提出了理论见解和分析网格提取方法，将高维曲面转换为平面，并引入了一种近似交点的方法，拓展了更广泛的应用。

    

    最近关于深度神经网络可视化的进展揭示了它们结构的见解，并且可以从连续分段仿射（CPWA）函数中提取网格。与此同时，神经表面表示学习的发展包括非线性位置编码，解决了诸如谱偏差之类的问题；然而，这在应用基于CPWA函数的网格提取技术方面带来了挑战。我们聚焦于三线性插值方法作为位置编码，提供了理论见解和分析的网格提取，展示了在奇拿尔约束下将高维曲面转换为三线性区域内的平面的过程。此外，我们引入了一种方法来近似三个高维曲面之间的交点，从而扩展了更广泛的应用。通过汉明距离和效率以及角距离来经验性地验证正确性和简洁性，同时检查了t之间的相关性

    arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
    
[^75]: 达尔文 图灵 邓金斯：建立一个普遍的进化论

    Darwin Turing Dawkins: Building a General Theory of Evolution

    [https://arxiv.org/abs/2402.10393](https://arxiv.org/abs/2402.10393)

    进化论不仅适用于基因，也适用于存储在大脑中的模因和计算机中的信息，这本书探讨了这一普遍的进化理论对自然、社会、文化和个体的影响。

    

    生物、计算机、社会，甚至书籍都是参与一个宏大的进化生存斗争的一部分。这场生存斗争塑造了自然、国家、宗教、艺术、科学以及你自己。你的思想、感觉和行为都受其影响。达尔文进化论不仅适用于存储在DNA中的基因。运用艾伦·图灵和理查德·道金斯的洞见，我们将看到它也适用于存储在我们大脑中的模因和存储在计算机中的信息。下次你竞选总统、参与战争，或者只是处理人类普遍问题时，也许这本书会有所帮助。如果你想了解为什么以及何时会死亡，或者想要取得伟大成就，这本书可能会有所帮助。如果你关心计算机革命的发展方向，这本书可能会提供一些答案。

    arXiv:2402.10393v1 Announce Type: cross  Abstract: Living things, computers, societies, and even books are part of a grand evolutionary struggle to survive. That struggle shapes nature, nations, religions, art, science, and you. What you think, feel, and do is determined by it. Darwinian evolution does not apply solely to the genes that are stored in DNA. Using the insights of Alan Turing and Richard Dawkins, we will see that it also applies to the memes we store in our brains and the information we store in our computers. The next time you run for president, fight a war, or just deal with the ordinary problems humans are heir to, perhaps this book will be of use. If you want to understand why and when you will die, or if you want to achieve greatness this book may help. If you are concerned about where the computer revolution is headed, this book may provide some answers.
    
[^76]: 针对事件序列数据的预训练算法

    Pretext Training Algorithms for Event Sequence Data

    [https://arxiv.org/abs/2402.10392](https://arxiv.org/abs/2402.10392)

    提出了针对事件序列数据的自监督预训练框架，通过引入新颖的对齐验证任务，构建了适用于事件序列的基础表示，可以推广到不同的下游任务和数据领域。

    

    预训练后进行特定任务微调在视觉和语言领域取得了成功。本文提出了一个针对事件序列数据定制的自监督预训练框架。我们引入了一项针对事件序列特化的新型对齐验证任务，借鉴了掩码重构和对比学习中的良好实践。我们的预训练任务可以释放基础表示，这些表示可以推广到不同的下游任务，包括用于时间点过程模型的下一事件预测，事件序列分类和缺失事件插值。对流行的公共基准数据集上的实验表明，所提出的方法在不同任务和数据领域上具有潜力。

    arXiv:2402.10392v1 Announce Type: cross  Abstract: Pretext training followed by task-specific fine-tuning has been a successful approach in vision and language domains. This paper proposes a self-supervised pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and contrastive learning. Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public benchmarks demonstrate the potential of the proposed method across different tasks and data domains.
    
[^77]: UMAIR-FPS：带绘画风格的用户感知多模态动画插画推荐融合

    UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style

    [https://arxiv.org/abs/2402.10381](https://arxiv.org/abs/2402.10381)

    UMAIR-FPS提出了一种新的用户感知多模态动画插画推荐系统，通过融合图像绘画风格特征和语义特征来增强表示。

    

    高质量基于人工智能的图像生成模型的快速进步产生了大量的动漫插画。在海量数据中向用户推荐插画已成为一项具有挑战性和受欢迎的任务。然而，现有的动漫推荐系统侧重于文本特征，但仍需要整合图像特征。此外，大多数多模态推荐研究受到紧密耦合数据集的限制，限制了其对动漫插画的适用性。我们提出了带绘画风格的用户感知多模态动画插画推荐融合（UMAIR-FPS）来解决这些问题。在特征提取阶段，对于图像特征，我们首次结合图像绘画风格特征与语义特征来构建双输出图像编码器以增强表示。对于文本特征，我们基于Fine-tuning Sentence-Transformers获得文本嵌入，通过整合领域知识

    arXiv:2402.10381v1 Announce Type: cross  Abstract: The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledg
    
[^78]: 子图级通用提示调整

    Subgraph-level Universal Prompt Tuning

    [https://arxiv.org/abs/2402.10380](https://arxiv.org/abs/2402.10380)

    设计了一种可适用于任何预训练策略的简单提示调整方法，通过位于输入图特征空间内的功能来实现，从而增加了其在各种下游应用中的通用性

    

    在不断发展的机器学习领域，通过提示调整来调整预训练模型的适应性变得日益突出。这一趋势在图领域特别明显，不同的预训练策略为为图神经网络开发有效的基于提示的调整方法提供了独特的挑战。之前的方法受到限制，主要针对具有边预测预训练任务的模型定制了专门的提示函数。然而，这些方法在不同预训练策略之间缺乏泛化能力。最近，设计了一种简单的提示调整方法，可适用于任何预训练策略，在输入图的特征空间内发挥作用。这使其从理论上可以模拟任何类型的提示函数，从而显著提高了其在一系列下游应用中的通用性。

    arXiv:2402.10380v1 Announce Type: cross  Abstract: In the evolving landscape of machine learning, the adaptation of pre-trained models through prompt tuning has become increasingly prominent. This trend is particularly observable in the graph domain, where diverse pre-training strategies present unique challenges in developing effective prompt-based tuning methods for graph neural networks. Previous approaches have been limited, focusing on specialized prompting functions tailored to models with edge prediction pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space. This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple prompts to ful
    
[^79]: BioMistral：面向医学领域的开源预训练大型语言模型集合

    BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains

    [https://arxiv.org/abs/2402.10373](https://arxiv.org/abs/2402.10373)

    BioMistral是一种面向生物医学领域的开源预训练大型语言模型集合，在医学问答任务中表现出优越性能并具有竞争优势。

    

    大型语言模型（LLMs）近年来展示出卓越的多功能性，为医疗保健和医学等专业领域提供潜在应用。尽管有各种针对健康领域定制的开源LLMs可用，但将通用LLMs调整到医学领域仍面临重大挑战。本文介绍了BioMistral，一种专为生物医学领域量身定制的开源LLM，采用Mistral作为基础模型，并在PubMed Central上进一步进行预训练。我们在包含10个已建立的英文医学问答（QA）任务的基准上对BioMistral进行了全面评估。我们还探讨通过量化和模型合并方法获得的轻量级模型。我们的结果表明，BioMistral相较于现有开源医学模型具有优越性能，并与专有对手具有竞争优势。最后，为了解决

    arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
    
[^80]: 大型语言模型在预测和异常检测中的应用：系统文献综述

    Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review

    [https://arxiv.org/abs/2402.10350](https://arxiv.org/abs/2402.10350)

    大型语言模型在预测和异常检测领域展现出显著潜力，但面临着挑战包括依赖历史数据集、泛化困难、模型幻觉等问题，提出了整合多模态数据等解决方案。

    

    这篇系统文献综述全面审查了大型语言模型（LLMs）在预测和异常检测中的应用，突出当前研究现状、固有挑战以及未来的发展方向。LLMs已经在解析和分析大量数据集，识别模式，预测未来事件，并在各个领域检测异常行为方面展现出显著潜力。然而，该综述确定了一些关键挑战，阻碍了它们更广泛的采用和有效性，包括依赖于庞大的历史数据集，在不同上下文中的泛化问题，模型幻觉现象，模型知识边界的限制以及所需的大量计算资源。通过详细分析，该综述讨论了克服这些障碍的潜在解决方案和策略，如整合多模态数据。

    arXiv:2402.10350v1 Announce Type: cross  Abstract: This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal dat
    
[^81]: 论部署LLMs/VLMs在机器人领域存在的安全问题：突显风险和漏洞

    On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities

    [https://arxiv.org/abs/2402.10340](https://arxiv.org/abs/2402.10340)

    论文突出探讨了在机器人应用中整合大型语言模型和视觉语言模型所带来的安全性和健壮性关键问题，指出这种整合可能容易受到恶意攻击并导致严重后果。

    

    在这篇论文中，我们着重讨论了将大型语言模型（LLMs）和视觉语言模型（VLMs）整合到机器人应用中所涉及的健壮性和安全性关键问题。最近的研究着重于利用LLMs和VLMs来提高机器人任务（如操作，导航等）的性能。然而，这种整合可能会引入显着的漏洞，即由于语言模型对恶意攻击的敏感性，可能导致灾难性后果。通过研究LLMs/VLMs与机器人界面的最新进展，我们展示了如何轻松操纵或误导机器人的行为，导致安全隐患。我们定义并提供了几种可能的恶意攻击示例，并对集成了语言模型的三个知名机器人框架（包括KnowNo VIMA和Instruct2Act）进行实验，以评估它们对这些攻击的敏感度。

    arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
    
[^82]: HI-GAN：具有辅助输入的层次化修复GAN用于混合RGB和深度修复

    HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting

    [https://arxiv.org/abs/2402.10334](https://arxiv.org/abs/2402.10334)

    提出了一种名为HI-GAN的层次化修复GAN，通过三个GAN以层次结构的方式进行RGBD修复，其中EdgeGAN和LabelGAN分别修复遮罩边缘和分割标签图像，而CombinedRGBD-GAN结合它们的潜在表示输出并进行RGB和深度修复。

    

    修复涉及填补图像中丢失的像素或区域，这是混合现实环境中使用的一项关键技术，特别是在减少现实（DR）中，其中从用户的视觉环境中删除内容。现有方法依赖于数字替换技术，需要多个摄像头并产生高成本。AR设备和智能手机使用ToF深度传感器捕获与RGB图像对齐的场景深度图。尽管速度快且价格实惠，但ToF相机会产生具有丢失像素的不完美深度图。为了解决以上挑战，我们提出了层次化修复GAN（HI-GAN），这是一种新颖的方法，由三个以层次结构方式组成的GAN构成，用于RGBD修复。EdgeGAN和LabelGAN分别修复遮罩边缘和分割标签图像，而CombinedRGBD-GAN结合它们的潜在表示输出并进行RGB和深度修复。

    arXiv:2402.10334v1 Announce Type: cross  Abstract: Inpainting involves filling in missing pixels or areas in an image, a crucial technique employed in Mixed Reality environments for various applications, particularly in Diminished Reality (DR) where content is removed from a user's visual environment. Existing methods rely on digital replacement techniques which necessitate multiple cameras and incur high costs. AR devices and smartphones use ToF depth sensors to capture scene depth maps aligned with RGB images. Despite speed and affordability, ToF cameras create imperfect depth maps with missing pixels. To address the above challenges, we propose Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked edge and segmentation label images respectively, while CombinedRGBD-GAN combines their latent representation outputs and performs RGB and Depth inpainting. Edge images and particularly
    
[^83]: LAVE：以LLM为动力的视频编辑代理辅助和语言增强技术

    LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing

    [https://arxiv.org/abs/2402.10294](https://arxiv.org/abs/2402.10294)

    LAVE通过整合大型语言模型（LLMs），提供LLM动力的代理辅助和语言增强编辑功能，减少视频编辑的障碍，帮助用户实现编辑目标

    

    视频制作变得越来越受欢迎，但编辑所需的专业知识和努力常常对初学者构成障碍。本文探讨了在视频编辑工作流程中整合大型语言模型（LLMs）以减少这些障碍。我们的设计理念体现在LAVE中，这是一个提供LLM动力的代理辅助和语言增强编辑功能的新颖系统。LAVE自动生成用户素材的语言描述，作为使LLM能够处理视频并协助编辑任务的基础。当用户提供编辑目标时，代理计划并执行相关动作以实现这些目标。此外，LAVE允许用户通过代理或直接UI操作编辑视频，提供灵活性并使代理动作能够进行手动调整。我们的用户研究包括了从初学者到熟练编辑者的八名参与者，证明了LAVE对于减少编辑障碍和帮助用户实现编辑目标的有效性。

    arXiv:2402.10294v1 Announce Type: cross  Abstract: Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user's footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated
    
[^84]: 着重于为神经网络编码结构化数据的实验

    Experiments with Encoding Structured Data for Neural Networks

    [https://arxiv.org/abs/2402.10290](https://arxiv.org/abs/2402.10290)

    该论文着眼于探索为神经网络编码结构化数据的技术，以便在游戏领域中创建一个能够选择良好行动的AI代理。

    

    该项目的目标是创建一个AI代理，能够在名为战场的游戏领域中选择良好的行动。类似战场的顺序领域是规划问题的重要测试平台，因此国防部使用这些领域进行战争演习。我们开发的代理结合了蒙特卡洛树搜索（MCTS）和深度Q网络（DQN）技术，试图在游戏环境中导航，避开障碍，与对手互动并夺旗。本文将重点关注我们探索的编码技术，以展示存储在Python类中的复杂结构化数据，这是一个代理的必要前提。

    arXiv:2402.10290v1 Announce Type: new  Abstract: The project's aim is to create an AI agent capable of selecting good actions in a game-playing domain called Battlespace. Sequential domains like Battlespace are important testbeds for planning problems, as such, the Department of Defense uses such domains for wargaming exercises. The agents we developed combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN) techniques in an effort to navigate the game environment, avoid obstacles, interact with adversaries, and capture the flag. This paper will focus on the encoding techniques we explored to present complex structured data stored in a Python class, a necessary precursor to an agent.
    
[^85]: 对一类序列异常检测模型的后门攻击

    Backdoor Attack against One-Class Sequential Anomaly Detection Models

    [https://arxiv.org/abs/2402.10283](https://arxiv.org/abs/2402.10283)

    本文提出了一种新型后门攻击策略，可以通过在良性正常数据中制作几乎不可察觉的触发器，并将其注入模型，成功妥协了两个一类异常检测模型。

    

    arXiv:2402.10283v1 公告类型：跨界 摘要：深度学习在序列数据上的异常检测引起了广泛关注，然而，基于深度学习的模型面临一种关键的安全威胁 - 它们容易受到后门攻击的影响。本文研究了通过提出一种新型后门攻击策略来妥协深度序列异常检测模型。攻击方法包括两个主要步骤，触发器生成和后门注入。 触发器生成是通过从良性正常数据中制作扰动样本来导出几乎不可察觉的触发器，其中扰动样本仍然正常。 后门注入则是适当地将后门触发器注入模型，只为具有触发器的样本。 实验结果表明了我们提出的攻击策略的有效性，通过在两个知名的一类异常检测模型上注入后门触发器。

    arXiv:2402.10283v1 Announce Type: cross  Abstract: Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.
    
[^86]: Brant-2：脑信号基础模型

    Brant-2: Foundation Model for Brain Signals

    [https://arxiv.org/abs/2402.10251](https://arxiv.org/abs/2402.10251)

    Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。

    

    基础模型受益于在大量未标记数据上进行预训练，并且在少量标记数据的情况下能够在各种应用中表现出色。这种模型在分析脑信号方面特别有效，因为这一领域涵盖了众多应用场景，并且进行大规模注释是成本高昂的。在这项工作中，我们提出了脑信号领域最大的基础模型，Brant-2。与用于颅内神经信号的基础模型Brant相比，Brant-2不仅对数据变化和建模尺度表现出稳健性，而且可以应用于更广泛范围的脑神经数据。通过在大量任务上进行实验，我们展示了Brant-2对脑信号中各种应用场景的适应性。进一步分析揭示了Brant-2的可扩展性，验证了每个组件的有效性，并展示了我们模型保持的能力。

    arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
    
[^87]: 一种基于数据驱动的监督机器学习方法用于估计全球环境空气污染浓度及其相关预测区间

    A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals

    [https://arxiv.org/abs/2402.10248](https://arxiv.org/abs/2402.10248)

    该论文提出了一种基于数据驱动的监督机器学习方法，可用于估计全球环境空气污染浓度，并提供预测区间，为各利益相关者提供更全面的数据，并通过检验模型在不同地理位置上的性能为研究提供洞见。

    

    全球环境空气污染是一个跨界挑战，通常通过依赖空间稀疏且异构放置的监测站数据的干预来解决。这些站点经常由于诸如停电等问题而出现时间数据缺失。为此，我们开发了一种可扩展的、数据驱动的、监督式机器学习框架。该模型旨在补充缺失的时间和空间测量数据，从而生成包括NO$_2$、O$_3$、PM$_{10}$、PM$_{2.5}$和SO$_2$等污染物的全面数据集。该数据集在每一估计值附带预测区间，并为依赖室外空气污染数据进行下游评估的广泛利益相关者提供服务。这使得可以进行更详细的研究。此外，还研究了该模型在不同地理位置上的性能，从而提供见解。

    arXiv:2402.10248v1 Announce Type: cross  Abstract: Global ambient air pollution, a transboundary challenge, is typically addressed through interventions relying on data from spatially sparse and heterogeneously placed monitoring stations. These stations often encounter temporal data gaps due to issues such as power outages. In response, we have developed a scalable, data-driven, supervised machine learning framework. This model is designed to impute missing temporal and spatial measurements, thereby generating a comprehensive dataset for pollutants including NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of 0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for each estimate, caters to a wide range of stakeholders relying on outdoor air pollution data for downstream assessments. This enables more detailed studies. Additionally, the model's performance across various geographical locations is examined, providing insights an
    
[^88]: 为什么问题的动态视角

    A Dynamical View of the Question of Why

    [https://arxiv.org/abs/2402.10240](https://arxiv.org/abs/2402.10240)

    提出了一种在时间过程中直接建立事件之间因果关系的学习范式，并提出了用于计算因果贡献的两个关键引理，可以揭示和量化扩散过程中的因果关系。

    

    我们研究由随机过程生成的多元时间序列数据中的因果推理。现有方法主要局限于静态设置，忽略了时间上的连续性和变化的发射。相比之下，我们提出了一个学习范式，直接在时间过程中建立事件之间的因果关系。我们提出了两个关键引理来计算因果贡献，并将其构造为强化学习问题。我们的方法提供了揭示和量化扩散过程中因果关系的形式化和计算工具，包括各种重要设置，如离散时间马尔可夫决策过程。最后，通过相当复杂的实验和通过纯学习，我们的框架揭示和量化了因果联系，否则看似莫名其妙。

    arXiv:2402.10240v1 Announce Type: cross  Abstract: We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.
    
[^89]: 使用多样性搜索在元胞自动机中发现感觉运动机构

    Discovering Sensorimotor Agency in Cellular Automata using Diversity Search

    [https://arxiv.org/abs/2402.10236](https://arxiv.org/abs/2402.10236)

    本论文利用多样性搜索、课程学习和梯度下降等算法，自动搜索元胞自动机中能够自组织出具有基础感觉运动机构的“个体”，为寻找这种基本机构自组织的环境条件提供了新方法。

    

    人工生命研究领域研究类似生命现象的如自主生成、机构性或自我调节等在计算机模拟中如何自组织。在元胞自动机（CA）中，一个关键的未解之谜是是否可能找到能够自组织出稳健“个体”的环境规则，而这些个体在初始状态下没有“身体”、“大脑”、“感知”或“行动”的存在。本文利用机器学习的最新进展，结合多样性搜索、课程学习和梯度下降等算法，自动搜索这些“个体”，即能够移动并有能力以一致的方式对外部障碍做出反应且保持完整性的局部结构，从而形成基础形式的感觉运动机构。我们展示了这种方法使得能够系统地找到在CA中导致这种基本机构自组织的环境条件。

    arXiv:2402.10236v1 Announce Type: cross  Abstract: The research field of Artificial Life studies how life-like phenomena such as autopoiesis, agency, or self-regulation can self-organize in computer simulations. In cellular automata (CA), a key open-question has been whether it it is possible to find environment rules that self-organize robust "individuals" from an initial state with no prior existence of things like "bodies", "brain", "perception" or "action". In this paper, we leverage recent advances in machine learning, combining algorithms for diversity search, curriculum learning and gradient descent, to automate the search of such "individuals", i.e. localized structures that move around with the ability to react in a coherent manner to external obstacles and maintain their integrity, hence primitive forms of sensorimotor agency. We show that this approach enables to find systematically environmental conditions in CA leading to self-organization of such basic forms of agency. Th
    
[^90]: HyperAgent：一种简单、可扩展、高效且可证明用于复杂环境的强化学习框架

    HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments

    [https://arxiv.org/abs/2402.10228](https://arxiv.org/abs/2402.10228)

    HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    

    为了在资源约束下解决复杂任务，强化学习（RL）代理需要简单、高效、可扩展、具有大状态空间和不断积累的交互数据。我们提出了HyperAgent，这是一个具有超模型、索引抽样方案和增量更新机制的RL框架，可以在一般价值函数逼近中进行计算高效的顺序后验逼近和数据高效的动作选择，超越了共轭性。HyperAgent的实现简单，只需要在DDQN中添加一个模块和一行额外代码。在实践中，HyperAgent在大规模深度RL基准测试中表现出稳健的性能，无论是在数据还是计算方面都获得了显着的效率提升。在理论上，在实际可扩展的算法中，HyperAgent是第一个能够实现可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
    
[^91]: 人类为中心的目标推理与Ripple-Down规则

    Human-Centric Goal Reasoning with Ripple-Down Rules

    [https://arxiv.org/abs/2402.10224](https://arxiv.org/abs/2402.10224)

    本文通过使用Ripple-Down Rules（RDR）扩展了ActorSim目标推理框架，使其能够通过示范学习并建立新的决策规则，以便在未来正确处理类似情况。

    

    ActorSim是在海军研究实验室开发的目标推理框架。最初，所有目标推理规则都是手工制作的。本作品通过展示学习的能力扩展了ActorSim，即当人类训练员与系统的决定不符时，训练员可以接管并向系统展示正确决策。学习组件使用Ripple-Down Rules（RDR）构建新的决策规则，以正确处理未来类似情况。该系统在RoboCup Rescue Agent Simulation中展示，该模拟器模拟了一个全市范围的灾难，需要紧急服务，包括消防、救护车和警察，派往不同地点从危险情况中撤离平民。RDRs实现在一个脚本语言FrameScript中，用于在ActorSim和代理模拟器之间进行调解。使用Ripple-Down Rules，ActorSim可以扩展一个数量级。

    arXiv:2402.10224v1 Announce Type: cross  Abstract: ActorSim is a goal reasoning framework developed at the Naval Research Laboratory. Originally, all goal reasoning rules were hand-crafted. This work extends ActorSim with the capability of learning by demonstration, that is, when a human trainer disagrees with a decision made by the system, the trainer can take over and show the system the correct decision. The learning component uses Ripple-Down Rules (RDR) to build new decision rules to correctly handle similar cases in the future. The system is demonstrated using the RoboCup Rescue Agent Simulation, which simulates a city-wide disaster, requiring emergency services, including fire, ambulance and police, to be dispatched to different sites to evacuate civilians from dangerous situations. The RDRs are implemented in a scripting language, FrameScript, which is used to mediate between ActorSim and the agent simulator. Using Ripple-Down Rules, ActorSim can scale to an order of magnitude 
    
[^92]: 通过深度强化学习实现自主车辆巡逻：学习沟通和协作

    Autonomous Vehicle Patrolling Through Deep Reinforcement Learning: Learning to Communicate and Cooperate

    [https://arxiv.org/abs/2402.10222](https://arxiv.org/abs/2402.10222)

    本文通过强化互代学习方法，使多代理车辆在面临环境因素、限制和合作问题时学会沟通协作，从而解决自主车辆巡逻的挑战性问题。

    

    自主车辆适用于连续区域巡逻问题。找到最佳巡逻策略可能具有挑战性，因为存在未知的环境因素，如风或地形；或自主车辆的限制，如有限的电池寿命或硬件故障。重要的是，巡逻大区域通常需要多个代理共同协调其行动。然而，由于巡逻环境的复杂性，通常难以手动定义最佳协调策略。本文考虑了一个具有环境因素、代理限制和三种典型合作问题--避撞、避让拥挤、巡逻目标协商的巡逻问题。我们提出了一种基于强化互代学习（RIAL）方法的多代理强化学习解决方案。通过这种方法，代理被训练出发展他们自己的通信协议来

    arXiv:2402.10222v1 Announce Type: cross  Abstract: Autonomous vehicles are suited for continuous area patrolling problems. Finding an optimal patrolling strategy can be challenging due to unknown environmental factors, such as wind or landscape; or autonomous vehicles' constraints, such as limited battery life or hardware failures. Importantly, patrolling large areas often requires multiple agents to collectively coordinate their actions. However, an optimal coordination strategy is often non-trivial to be manually defined due to the complex nature of patrolling environments. In this paper, we consider a patrolling problem with environmental factors, agent limitations, and three typical cooperation problems -- collision avoidance, congestion avoidance, and patrolling target negotiation. We propose a multi-agent reinforcement learning solution based on a reinforced inter-agent learning (RIAL) method. With this approach, agents are trained to develop their own communication protocol to c
    
[^93]: 使用展开网络对聚类归纳偏好进行建模

    Clustering Inductive Biases with Unrolled Networks

    [https://arxiv.org/abs/2402.10213](https://arxiv.org/abs/2402.10213)

    提出了一种自动编码器架构（WLSC），其潜在表示是隐含的，用于对聚类归纳偏好进行建模

    

    经典的稀疏编码（SC）模型将视觉刺激表示为少量学习基函数的线性组合，在对自然图像数据进行训练时，这些基函数类似于Gabor。然而，经典稀疏编码学习的类Gabor滤波器远远超过了实际观察到的简单细胞感受野轮廓的良好预测。我们提出了一种自动编码器架构（WLSC），其潜在表示是隐含的

    arXiv:2402.10213v1 Announce Type: cross  Abstract: The classical sparse coding (SC) model represents visual stimuli as a linear combination of a handful of learned basis functions that are Gabor-like when trained on natural image data. However, the Gabor-like filters learned by classical sparse coding far overpredict well-tuned simple cell receptive field profiles observed empirically. While neurons fire sparsely, neuronal populations are also organized in physical space by their sensitivity to certain features. In V1, this organization is a smooth progression of orientations along the cortical sheet. A number of subsequent models have either discarded the sparse dictionary learning framework entirely or whose updates have yet to take advantage of the surge in unrolled, neural dictionary learning architectures. A key missing theme of these updates is a stronger notion of \emph{structured sparsity}. We propose an autoencoder architecture (WLSC) whose latent representations are implicitl
    
[^94]: 检查生成对抗网络判别器中的病态偏见：以StyleGAN3模型为例的案例研究

    Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model

    [https://arxiv.org/abs/2402.09786](https://arxiv.org/abs/2402.09786)

    这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。

    

    生成对抗网络可以生成逼真的人脸，往往难以被人类区分出来。我们发现预训练的StyleGAN3模型中的判别器在图像和面部质量上系统地对得分进行分层，并且这不成比例地影响了不同性别、种族和其他类别的图像。我们检查了判别器在色彩和亮度方面对感知的种族和性别的偏见，然后检查了社会心理学中关于刻板印象研究中常见的偏见。

    arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
    
[^95]: CodeMind:一个用于挑战大型语言模型进行代码推理的框架

    CodeMind: A Framework to Challenge Large Language Models for Code Reasoning

    [https://arxiv.org/abs/2402.09664](https://arxiv.org/abs/2402.09664)

    CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    

    仅靠测试通过来评估大型语言模型（LLMs）的代码合成能力可能会导致不公正的评估或促进具有数据泄漏的模型，作为一种替代方案，我们介绍了CodeMind，这是一个旨在评估LLMs的代码推理能力的框架。CodeMind目前支持三种代码推理任务：独立执行推理（IER）、依赖执行推理（DER）和规范推理（SR）。前两者评估模型以预测任意代码的执行输出，或者模型能够正确合成的代码。第三个任务评估LLMs实现指定预期行为的程度。我们使用CodeMind对两种不同编程语言中的五个基准下的九个LLMs进行了广泛的评估，结果表明LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
    
[^96]: API Pack：一个用于API调用生成的大规模多语言数据集

    API Pack: A Massive Multilingual Dataset for API Call Generation

    [https://arxiv.org/abs/2402.09615](https://arxiv.org/abs/2402.09615)

    这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成

    

    我们介绍了API Pack，一个包含超过一百万个指令-API调用对的多语言数据集，旨在提高大型语言模型的API调用生成能力。通过实验，我们证明了API Pack在提升模型在这一特定任务上的效果的同时，保持其在一般编码方面的整体熟练程度。仅在20,000个Python实例上对CodeLlama-13B进行微调，其生成未见过的API调用的准确率比GPT-3.5和GPT-4分别高出10%和5%。扩展到100k个例子可以提高对训练期间未见过的新API的泛化能力。此外，实现了跨语言的API调用生成，而无需大量语言特定的数据。数据集、经过微调的模型和整体代码库可在https://github.com/anonymous_url上公开获取。

    arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
    
[^97]: 使用InTEnt进行医学图像分割：基于集成熵加权的单图像测试时适应

    Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation

    [https://arxiv.org/abs/2402.09604](https://arxiv.org/abs/2402.09604)

    本文提出了一种使用单个未标记测试图像来调整医学图像分割模型的方法。相比于直接最小化预测熵的其他方法，在这种设置下并不能显著提高性能。为了克服这个问题，我们使用各种目标域统计估计的预测进行集成，并基于权重进行加权。

    

    测试时适应（TTA）是指在测试期间将训练好的模型调整到一个新领域。现有的TTA技术依赖于在同一领域具有多个测试图像，然而在实际应用（如医学成像）中，数据获取费用昂贵且成像条件经常变化，因此这种方法可能不切实际。本文致力于使用仅有一个未标记的测试图像来调整医学图像分割模型。大多数TTA方法直接最小化预测熵，然而在这种设置下，它们未能显著提高性能。我们还观察到，批归一化（BN）层的统计量选择是一个非常重要但不稳定的因素，因为只有一个测试域示例。为了克服这个问题，我们提出使用各种目标域统计估计的预测进行\textit{集成}，并基于权重进行加权。

    arXiv:2402.09604v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing. Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently. Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image. Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example. To overcome this, we propose to instead \textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on
    
[^98]: WiMANS: WiFi-based多用户活动感知的基准数据集

    WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing

    [https://arxiv.org/abs/2402.09430](https://arxiv.org/abs/2402.09430)

    WiMANS是第一个基于WiFi的多用户活动感知数据集，包含WiFi信道状态信息和同步视频，旨在促进可重复和可比较的研究

    

    WiFi-based human sensing表现出了在不侵入和无需设备的情况下分析用户行为的显着潜力，使得智能家居和医疗保健等应用受益。然而，大多数先前的工作都集中在单用户感知上，在涉及多用户场景时具有有限的实用性。尽管最近的研究已经开始探讨基于WiFi的多用户活动感知，但仍然缺乏基准数据集以促进可重复和可比较的研究。为了弥补这一空白，我们呈现了WiMANS，据我们所知，这是第一个基于WiFi的多用户活动感知数据集。WiMANS包含超过9.4小时的WiFi信道状态信息（CSI），监测多个用户在各种环境中同时进行的活动。与现有数据集相比，WiMANS不仅收集了双WiFi频段的CSI，还包括了同步视频。我们利用WiMANS来进行基准测试

    arXiv:2402.09430v1 Announce Type: cross  Abstract: WiFi-based human sensing has exhibited remarkable potential to analyze user behaviors in a non-intrusive and device-free manner, benefiting applications as diverse as smart homes and healthcare. However, most previous works focus on single-user sensing, which has limited practicability in scenarios involving multiple users. Although recent studies have begun to investigate WiFi-based multi-user activity sensing, there remains a lack of benchmark datasets to facilitate reproducible and comparable research. To bridge this gap, we present WiMANS, to our knowledge, the first dataset for multi-user activity sensing based on WiFi. WiMANS contains over 9.4 hours of WiFi Channel State Information (CSI), monitoring simultaneous activities performed by multiple users in various environments. Compared to existing datasets, WiMANS not only collects the CSI of dual WiFi bands but also includes synchronized videos. We exploit WiMANS to benchmark the
    
[^99]: 使用人机协同的方法开发大型语言模型审计框架

    Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop

    [https://arxiv.org/abs/2402.09346](https://arxiv.org/abs/2402.09346)

    本文提出了一种使用人机协同的方法开发大型语言模型审计框架，通过使用不同版本的相同问题来探测模型可能存在的偏见或幻觉，实现了自动化和可扩展的审计方法。

    

    随着大型语言模型在各种用户和场景中的普及，识别使用这些模型时可能存在的问题变得至关重要，例如偏见、不一致性和幻觉。尽管对这些问题进行审计是可取的，但并不容易解决。一种有效的方法是使用不同版本的相同问题来探测语言模型，这可以暴露其知识或运行中的不一致性，从而表明可能存在偏见或幻觉。然而，要在大规模上实现这种审计方法，我们需要一种可靠且自动化的生成这些探测的方法。在本文中，我们提出了一种自动化和可扩展的解决方案，其中一种方法是使用不同的语言模型和人机协同。这种方法提供了可验证性和透明性，避免对同一语言模型的循环依赖，并增加了科学严谨性和普适性。

    arXiv:2402.09346v1 Announce Type: new Abstract: As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with 
    
[^100]: 通过信息论奖励建模来减轻奖励作弊问题

    Mitigating Reward Hacking via Information-Theoretic Reward Modeling

    [https://arxiv.org/abs/2402.09345](https://arxiv.org/abs/2402.09345)

    本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。

    

    尽管强化学习从人类反馈（RLHF）中的成功在与人类价值观的语言模型的对齐方面，奖励作弊问题，也被称为奖励过度优化，仍然是一个关键挑战，主要源于奖励建模的局限性，即奖励模型的泛化能力和偏好数据集的不一致性。在这项工作中，我们从信息论的视角来解决这个问题，并提出了一种可推广和鲁棒的奖励建模框架，称为InfoRM，通过引入变分信息瓶颈目标来过滤出不相关的信息，并开发一种模型复杂度调节机制。值得注意的是，我们进一步发现了过度优化与潜变量空间的异常值之间的相关性，将InfoRM作为检测奖励过度优化的一种有前途的工具。受到这一发现的启发，我们提出了集成聚类偏差得分（ICDS），用于量化过优化问题。

    arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
    
[^101]: 与LLM玩猜谜游戏: 通过隐含提示的间接越狱攻击

    Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues

    [https://arxiv.org/abs/2402.09091](https://arxiv.org/abs/2402.09091)

    通过提供隐含的线索，Puzzler通过绕过LLM的防御策略，在间接方式下实现了越狱攻击，成功率高达96.6%。

    

    随着LLM的发展，LLM的安全威胁越来越受到关注。许多越狱攻击已经提出来评估LLM的安全防御。目前的越狱攻击主要使用场景伪装技术。然而，它们对恶意意图的明确提及很容易被LLM识别和防御。在本文中，我们提出了一种间接越狱攻击方法，名为Puzzler，它可以通过隐含地为LLM提供一些有关原始恶意查询的提示来绕过LLM的防御策略并获取恶意响应。此外，受孙子的《孙子兵法》中“当攻无法攻，守”智慧的启发，我们采取了一种防御姿态来通过LLM收集关于原始恶意查询的线索。大量的实验结果表明，Puzzler在闭源LLM上的查询成功率为96.6%，比基准线高57.9%-82.7%。

    arXiv:2402.09091v1 Announce Type: cross Abstract: With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense strategy and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than baselines. Furthermore, w
    
[^102]: 智能画布: 通过快速原型设计、迭代和管理实现类似设计的探索性可视数据分析

    Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis through Rapid Prototyping, Iteration and Curation

    [https://arxiv.org/abs/2402.08812](https://arxiv.org/abs/2402.08812)

    该论文提出了一种"类似设计"的智能画布环境，通过将生成式AI组件集成到数据分析中，实现了快速原型设计、迭代和比较可视化管理。通过用户研究，论文验证了画布界面的有效性。

    

    复杂数据分析通过探索性的可视分析方法来寻求意想不到的洞见，并超越逻辑的逐步处理。然而，现有的界面（如笔记本和仪表板）在可视数据分析的探索和比较方面存在一些局限性。为了解决这些问题，我们介绍了一种“类似设计”的智能画布环境，将生成式AI集成到数据分析中，提供快速原型设计、迭代和比较可视化管理。我们的两个主要贡献包括将生成式AI组件集成到画布界面中，并通过用户研究（N=10）评估了画布界面的有效性。

    arXiv:2402.08812v1 Announce Type: cross Abstract: Complex data analysis inherently seeks unexpected insights through exploratory \re{visual analysis} methods, transcending logical, step-by-step processing. However, \re{existing interfaces such as notebooks and dashboards have limitations in exploration and comparison for visual data analysis}. Addressing these limitations, we introduce a "design-like" intelligent canvas environment integrating generative AI into data analysis, offering rapid prototyping, iteration, and comparative visualization management. Our dual contributions include the integration of generative AI components into a canvas interface, and empirical findings from a user study (N=10) evaluating the effectiveness of the canvas interface.
    
[^103]: 基于锚点的大型语言模型

    Anchor-based Large Language Models

    [https://arxiv.org/abs/2402.07616](https://arxiv.org/abs/2402.07616)

    基于锚点的大型语言模型（AnLLM）通过引入创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略，将序列信息压缩到锚点标记中，减少键/值缓存，提高推理效率。

    

    大型语言模型（LLMs）主要采用仅解码器的转换器架构，需要保留历史标记的键/值信息以提供上下文信息并避免冗余计算。然而，这些LLMs的巨大大小和参数量需要大量的GPU内存。这种内存需求随着输入文本的长度而增加，迫切需要更高效的信息存储和处理方法。本研究介绍了一种基于锚点的LLM（AnLLM），它利用了一种创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略。这种方法使LLMs能够将序列信息压缩成锚点标记，减少键/值缓存并提高推理效率。实验证明，AnLLM在减少键/值缓存高达99%和推理速度提高高达3.5倍的同时，仍保持可比的准确性。尽管牺牲了一些准确性，AnLLM的创新和贡献依然重要。

    Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
    
[^104]: LLM能够识别毒性吗？结构化毒性调查框架和基于语义的度量

    Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric

    [https://arxiv.org/abs/2402.06900](https://arxiv.org/abs/2402.06900)

    本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。

    

    在开发遵守社会标准的大型语言模型（LLMs）的过程中，识别生成文本中的毒性存在至关重要。现有的大多数毒性度量依赖于在特定毒性数据集上训练的编码模型。然而，这些编码器容易受到分布外的问题的影响，并且依赖于数据集中所假定的毒性定义。本文介绍了一种基于LLMs的自动鲁棒度量，用于区分模型回应是否具有毒性。我们首先分析了毒性因素，然后研究了LLMs的内在毒性属性，以确定它们作为评估器的适用性。随后，我们对评估数据集上的度量指标LLMs As ToxiciTy Evaluators（LATTE）进行了评估。实证结果表明，在不进行训练过程的情况下，我们的度量在测量毒性方面表现出色，F1得分比现有技术指标提高了12个百分点。我们还展示了上游毒性对度量结果的影响。

    In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
    
[^105]: 与更有说服力的LLMs辩论会导致更真实的回答

    Debating with More Persuasive LLMs Leads to More Truthful Answers

    [https://arxiv.org/abs/2402.06782](https://arxiv.org/abs/2402.06782)

    本文研究了更弱的语言模型是否能评估更强的模型的正确性。研究发现，通过进行辩论，非专家模型和人类回答问题的准确性都有所提高。

    

    与所需行为一致的大型语言模型（LLM）的常见方法主要依赖于人工标注的数据。然而，随着模型变得越来越复杂，它们将超过人类专业知识，人类评估的角色将演变为非专家监督专家。在此之前，我们问：更弱的模型能评估更强的模型的正确性吗？我们在类似的环境中调查了这个问题，其中更强的模型（专家）拥有回答问题所需的信息，而更弱的模型（非专家）缺乏这些信息。我们评估的方法是\textit{辩论}，其中两个LLM专家分别支持不同的答案，一个非专家选择答案。我们发现辩论 consistently帮助非专家模型和人类回答问题，分别达到76%和88%的准确性（朴素基准分别为48%和60%）。此外，以无监督方式优化专家辩论者的说服力会提高非专家的能力。

    Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
    
[^106]: 通过邻域划分和生成子图编码对领域知识图对齐进行大型语言模型微调的GLaM研究

    GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding

    [https://arxiv.org/abs/2402.06764](https://arxiv.org/abs/2402.06764)

    该论文提出了GLaM方法，通过邻域划分和生成子图编码，对领域知识图进行大型语言模型的微调。该方法的创新之处在于能够实现对实际应用中的多步推理，并减少虚构。

    

    将大型语言模型（LLMs）与从特定领域数据派生的知识图集成，代表了朝着更强大和事实推理的重要进展。随着这些模型变得越来越强大，使它们能够在现实世界的知识图上进行多步推理，并尽量减少虚构是至关重要的。然而，大型语言模型在处理互连实体的领域专用图时，能力仍然有限。因此，有必要填补这一技术上的重要差距。

    Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no--such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logica
    
[^107]: NICE: 优化上下文示例还是不优化？

    NICE: To Optimize In-Context Examples or Not?

    [https://arxiv.org/abs/2402.06733](https://arxiv.org/abs/2402.06733)

    通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。

    

    最近的研究表明，大型语言模型（LLMs）通过上下文学习和优化上下文示例（ICE），在各种任务上表现出色。然而，大多数研究假设在提示信息中要么是固定的，要么没有提供指令，导致了一个表面上的共识：优化上下文示例对于提高性能至关重要。我们针对经过指导的LLMs挑战这一共识，研究在提供了任务特定指令的情况下优化上下文示例是否必要，并发现有一些任务对于不同的优化上下文示例方法产生递减的回报。我们引入了一种任务特定的度量标准，称为"度量标准"（Metric），用于量化从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。通过对各种任务和逐步增加的指令集的系统性研究，我们验证了该启发式方法的有效性。

    Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
    
[^108]: LLM代理可以自主黑客网站

    LLM Agents can Autonomously Hack Websites

    [https://arxiv.org/abs/2402.06664](https://arxiv.org/abs/2402.06664)

    这项研究展示了LLM代理可以自主进行网站黑客攻击，包括盲目数据库模式提取和SQL注入，而且不需要人工反馈。这种能力是由高度工具使用和利用扩展上下文能力的前沿模型赋予的。

    

    近年来，大型语言模型（LLMs）变得越来越强大，现在可以与工具交互（即调用函数）、读取文档并递归调用自己。因此，这些LLMs现在可以自主作为代理人运作。随着这些代理人能力的提升，最近的研究已经推测LLM代理人将如何影响网络安全。然而，关于LLM代理人的攻击能力，我们还知之甚少。在本研究中，我们展示了LLM代理人可以自主黑客网站，执行诸如盲目数据库模式提取和SQL注入等复杂任务，无需人工反馈。重要的是，这种能力是由具有高度工具使用和利用扩展上下文能力的前沿模型所独特赋予的。我们展示了GPT-4能够进行这样的黑客攻击，但现有的开源模型则不能。最后，我们展示了GPT-4能够自主发现网站的漏洞。

    In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.   In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in we
    
[^109]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^110]: 表情符号解密：利用ChatGPT提升社交媒体沟通的理解能力

    Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications

    [https://arxiv.org/abs/2402.01681](https://arxiv.org/abs/2402.01681)

    在表情符号研究中，我们评估了ChatGPT在处理注释和下游任务中的有效性。我们的研究结果表明ChatGPT可以作为一个可行的替代人类注释者的工具，有效地解释表情符号。

    

    表情符号在社交网络沟通中已经普遍存在，它们承载了超越文字或短语的语义，这引发了学术界对其属性和功能的越来越多的研究兴趣。然而，与表情符号相关的研究和应用面临两个主要挑战。首先，研究者通常依赖众包来注释表情符号，以了解其情感、使用意图和语义含义。其次，用户的主观解释往往会导致对表情符号的误解，并造成沟通障碍。大型语言模型（LLMs）在各种注释任务中取得了显著的成功，ChatGPT在多个领域展示了专业能力。在我们的研究中，我们评估了ChatGPT在处理以前注释和下游任务中的有效性。我们的目标是验证ChatGPT可以在表情符号研究中作为人类注释者的可行替代者，并验证其解释表情符号的能力。

    Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
    
[^111]: StickerConv: 从零开始生成多模态共情回应

    StickerConv: Generating Multimodal Empathetic Responses from Scratch

    [https://arxiv.org/abs/2402.01679](https://arxiv.org/abs/2402.01679)

    本文介绍了StickerConv代理(Agent4SC)，该代理通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。为了利用构建的多模态共情对话数据集StickerConv，作者提出了PErceive and Generate Stickers (PEGS)模型，该模型能够生成情境相关和情感丰富的回应。

    

    在当前的共情对话研究中，贴纸尽管被广泛认可为提高在线交流中的共情能力，但仍未得到充分探索。本文介绍了StickerConv代理(Agent4SC)，通过协作代理交互，实现了与贴纸使用相仿的人类行为模拟，从而增强了多模态共情交流。在此基础上，我们构建了一个多模态共情对话数据集StickerConv，包括12.9K个对话会话，5.8K个独特贴纸和2K个多样化会话场景，专门设计用于增强多模态情境下的共情回应生成。为了利用这个数据集的丰富性，我们提出了PErceive and Generate Stickers (PEGS)，一种多模态共情回应生成模型，并结合基于LLM的全面共情评估指标。我们的实验表明，PEGS在生成情境相关和情感丰富的回应方面具有很好的效果。

    Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotional
    
[^112]: 将分层变分自动编码器与LLMs结合，实现在社交媒体中具有临床意义的时间线摘要

    Combining Hierachical VAEs with LLMs for clinically meaningful timeline summarisation in social media

    [https://arxiv.org/abs/2401.16240](https://arxiv.org/abs/2401.16240)

    利用混合的分层变分自动编码器与LLMs结合的方法实现了从社交媒体用户时间轴生成具有临床意义的摘要，通过对时间轴的时间敏感性和举重有力的抽象摘要，TH-VAE生成的摘要在捕捉随时间变化方面优于仅使用LLM方法。

    

    我们引入了一种混合的抽象汇总方法，将分层变分自动编码器与LLMs结合（LlaMA-2），以从社交媒体用户时间轴生成具有临床意义的摘要，适用于心理健康监测。摘要结合了两种不同的叙述观点：通过向专门的临床提示馈送来生成专向临床医生有用的第三人称临床见解，以及重要的，通过新颖的分层变分自动编码器TH-VAE生成用户时间线的临时敏感的第一人称抽象摘要。我们通过与专家摘要的自动评估和与临床专家的人工评估来评估生成的摘要，结果表明通过TH-VAE进行的时间线摘要会产生更富有临床效用、更具事实和逻辑连贯性的摘要，优于仅使用LLM方法捕捉时间变化。

    arXiv:2401.16240v2 Announce Type: replace-cross  Abstract: We introduce a hybrid abstractive summarisation approach combining hierarchical VAE with LLMs (LlaMA-2) to produce clinically meaningful summaries from social media user timelines, appropriate for mental health monitoring. The summaries combine two different narrative points of view: clinical insights in third person useful for a clinician are generated by feeding into an LLM specialised clinical prompts, and importantly, a temporally sensitive abstractive summary of the user's timeline in first person, generated by a novel hierarchical variational autoencoder, TH-VAE. We assess the generated summaries via automatic evaluation against expert summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in more factual and logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time.
    
[^113]: 多语言语言模型的文本嵌入反向安全性

    Text Embedding Inversion Security for Multilingual Language Models

    [https://arxiv.org/abs/2401.12192](https://arxiv.org/abs/2401.12192)

    该研究探讨了多语言语言模型的文本嵌入逆转安全性问题，发现多语言模型更容易受到逆转攻击的影响，并提出了简单的掩蔽防御方法。

    

    在自然语言处理中，文本数据通常以实数嵌入表示，尤其是随着大型语言模型（LLMs）和嵌入式服务（EaaS）的流行。然而，将敏感信息存储为嵌入可能容易受到安全漏洞的影响，因为研究表明，即使不知道底层模型的情况下，文本也可以从嵌入中重构。尽管已经探讨了防御机制，但这些机制专注于英语，使其他语言容易受到攻击。本文通过多语言嵌入逆转探讨了LLM安全性。我们定义了黑盒多语言和跨语言逆转攻击的问题，并深入探讨了它们可能的影响。我们的研究结果表明，多语言LLMs可能更容易受到逆转攻击的影响，部分原因是基于英语的防御可能无效。为了缓解这一问题，我们提出了一种简单的掩蔽防御方法，对b有效。

    arXiv:2401.12192v2 Announce Type: replace-cross  Abstract: Textual data is often represented as realnumbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be vulnerable to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages vulnerable to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and thoroughly explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for b
    
[^114]: 小型LLMs是弱工具学习者：多LLM代理

    Small LLMs Are Weak Tool Learners: A Multi-LLM Agent

    [https://arxiv.org/abs/2401.07324](https://arxiv.org/abs/2401.07324)

    本论文提出了一种新的策略，将大型语言模型代理（LLMs）的能力分解为计划器、调用器和总结器模块，以克服小型模型性能限制和工具更新的问题。

    

    大型语言模型（LLM）代理大大扩展了独立LLMs的能力，使它们能够与外部工具（例如API，函数）进行交互，并自主完成复杂任务。工具使用的挑战要求LLMs不仅能理解用户查询并生成答案，还要在任务规划、记忆管理、工具调用和结果总结方面表现出色。传统方法集中于训练单个具备所有这些功能的LLM，但在小型模型上会出现性能限制的问题，此外，当工具更新时，整个LLM可能需要重新训练。为了克服这些挑战，我们提出了一种新的策略，将上述能力分解为计划器、调用器和总结器。每个组件由一个单独的LLM实现，专注于特定的能力，并与其他组件合作完成任务。这种模块化框架便于进行个体更新和...

    Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and t
    
[^115]: 集成混合NOMA-OFDM的HAP与LEO卫星网络的通信高效联合学习

    Communication-Efficient Federated Learning for LEO Satellite Networks Integrated with HAPs Using Hybrid NOMA-OFDM

    [https://arxiv.org/abs/2401.00685](https://arxiv.org/abs/2401.00685)

    本文提出了NomaFedHAP这一新型FL-SatCom方法，利用HAPs作为PS来增强卫星可见性，并引入NOMA技术实现快速高效的模型传输。

    

    太空人工智能对政府、企业和社会变得日益重要，有时甚至成为必需。本文提出了一种针对LEO卫星量身定制的新型FL-SatCom方法NomaFedHAP，该方法利用高空平台(HAPs)作为分布式参数服务器(PS)来增强卫星的可见性，引入非正交多址接入(NOMA)到LEO，以实现快速和带宽高效的模型传输。

    arXiv:2401.00685v2 Announce Type: replace-cross  Abstract: Space AI has become increasingly important and sometimes even necessary for government, businesses, and society. An active research topic under this mission is integrating federated learning (FL) with satellite communications (SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively train a machine learning model. However, the special communication environment of SatCom leads to a very slow FL training process up to days and weeks. This paper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO satellites, that (1) utilizes high-altitude platforms (HAPs) as distributed parameter servers (PS) to enhance satellite visibility, and (2) introduces non-orthogonal multiple access (NOMA) into LEO to enable fast and bandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a new communication topology that exploits HAPs to bridge satellites among different orbits to mitigate the Dopple
    
[^116]: 响应增强的半监督对话查询生成

    Response Enhanced Semi-supervised Dialogue Query Generation

    [https://arxiv.org/abs/2312.12713](https://arxiv.org/abs/2312.12713)

    提出了一种新的半监督学习框架--SemiDQG，通过未标记的对话来改善模型性能，训练响应增强的查询生成器 (RA)。

    

    从互联网获取庞大且不断更新的知识被认为是对话系统的一个重要能力。因此，针对生成对话历史记录的搜索查询而提出了对话查询生成任务，这些查询将被提交到搜索引擎以检索互联网上相关的网站。为了解决数据稀缺和领域适应性的挑战，本文提出了一个半监督学习框架 - SemiDQG，通过未标记的对话来提高模型性能。基于搜索查询通常与对话响应主题相关的观察，我们训练一个响应增强的查询生成器（RA）来提供丰富且有效的训练。

    arXiv:2312.12713v2 Announce Type: replace-cross  Abstract: Leveraging vast and continually updated knowledge from the Internet has been considered an important ability for a dialogue system. Therefore, the dialogue query generation task is proposed for generating search queries from dialogue histories, which will be submitted to a search engine for retrieving relevant websites on the Internet. In this regard, previous efforts were devoted to collecting conversations with annotated queries and training a query producer (QP) via standard supervised learning. However, these studies still face the challenges of data scarcity and domain adaptation. To address these issues, in this paper, we propose a semi-supervised learning framework -- SemiDQG, to improve model performance with unlabeled conversations. Based on the observation that the search query is typically related to the topic of dialogue response, we train a response-augmented query producer (RA) to provide rich and effective traini
    
[^117]: KGLens：一个参数化知识图解决方案，用于评估LLM知道和不知道的内容

    KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know

    [https://arxiv.org/abs/2312.11539](https://arxiv.org/abs/2312.11539)

    KGLens 是一个旨在衡量知识图与大型语言模型（LLMs）之间对齐程度的框架，帮助找出LLMs相对于知识图的知识不足之处。

    

    衡量知识图（KG）与大型语言模型（LLMs）之间的对齐程度是评估事实性并识别LLMs的知识盲点的有效方法。然而，这种方法面临两个主要挑战，包括将KGs转化为自然语言和高效评估这些广泛且复杂的结构。在本文中，我们提出了KGLens--一个旨在衡量KGs和LLMs之间对齐程度，并找出LLMs相对于KGs的知识缺陷的新颖框架。KGLens具有一个图引导的问题生成器，用于将KGs转化为自然语言，以及一个基于参数化KG结构的精心设计的采样策略，以加快KG的遍历。我们使用来自Wikidata的三个领域特定KG进行实验，这些KG包括超过19,000条边，700个关系和21,000个实体。我们跨越8个LLMs的分析表明，KGLens不仅

    arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
    
[^118]: 探究大型语言模型如何表达对超出参数化知识范围的问题的不确定性

    Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge

    [https://arxiv.org/abs/2311.09731](https://arxiv.org/abs/2311.09731)

    本研究系统地调查了大型语言模型在缺乏足够参数化知识的情况下如何表达对超出其知识范围的问题的不确定性，并强调了诚实与帮助性之间的权衡。

    

    这项工作旨在系统地调查大型语言模型在缺乏足够参数化知识以生成合理回应的情况下的行为，强调诚实与帮助性之间的权衡。为了精确确定语言模型的知识空白挑战，我们诊断性地创建了包含不存在概念或错误前提的无法回答的问题，确保它们超出了语言模型庞大的训练数据。通过编制一个包含既有无法回答也有可回答问题的基准，UnknownBench，我们定量评估语言模型在保持诚实的同时提供帮助的表现。使用一个模型无关的统一信心引导方法，我们观察到大多数语言模型在一致拒绝或表达对超出其参数化知识范围的问题的不确定性方面表现不佳。

    arXiv:2311.09731v2 Announce Type: replace-cross  Abstract: Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fin
    
[^119]: 数字苏格拉底：通过解释批评评估LLM

    Digital Socrates: Evaluating LLMs through Explanation Critiques

    [https://arxiv.org/abs/2311.09613](https://arxiv.org/abs/2311.09613)

    通过定义新的解释批评任务、创建人工验证过的数据集并训练开源自动批评模型，数字苏格拉底有助于揭示学生模型的见解。

    

    虽然LLMs可以提供有理有据的解释以及答案，但这些解释的性质和质量仍然知之甚少。作为回应，我们的目标是定义一种详细的方式来表征现代模型的解释能力，创建一个细致且可解释的解释评估工具，该工具可以自动生成这种表征，而无需依赖昂贵的API调用或人类注释。我们的方法是：(a)定义解释批评的新任务——识别和分类解释中的任何主要缺陷，并提供建议来解决这些缺陷；(b)为此任务创建一个规模可观且经过人工验证的数据集；(c)使用这些数据训练一个开源的自动批评模型（称为数字苏格拉底）。通过定量和定性分析，我们展示了数字苏格拉底如何有助于通过检查其理由来揭示有关学生模型的见解。

    arXiv:2311.09613v2 Announce Type: replace-cross  Abstract: While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reas
    
[^120]: Fusion-Eval: 将评估器与LLMs集成

    Fusion-Eval: Integrating Evaluators with LLMs

    [https://arxiv.org/abs/2311.09204](https://arxiv.org/abs/2311.09204)

    Fusion-Eval是一种创新方法，利用LLMs整合不同辅助评估器的见解，极大提升自然语言系统评估的有效性。

    

    自然语言系统的评估在自然语言理解和高级推理领域面临着重大挑战。本文介绍了一种名为“Fusion-Eval”的创新方法，利用大型语言模型（LLMs）来整合来自各种辅助评估器的见解。每个评估器专门负责评估响应的不同方面。这种独特策略使得Fusion-Eval能够有效地跨越各种任务和标准，增强现有评估方法的效果。在SummEval上，Fusion-Eval与人类之间的系统级Kendall-Tau相关性达到0.962，在TopicalChat上的轮级Spearman相关性达到0.744，远高于基准方法。这些结果突显了Fusion-Eval在自然语言系统评估领域的巨大潜力。

    arXiv:2311.09204v2 Announce Type: replace-cross  Abstract: Evaluating natural language systems poses significant challenges, particularly in the realms of natural language understanding and high-level reasoning. In this paper, we introduce "Fusion-Eval", an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. This unique strategy enables Fusion-Eval to function effectively across a diverse range of tasks and criteria, enhancing the effectiveness of existing evaluation methods. Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval's significant potential in the realm of natural language system evaluation.
    
[^121]: 通过去噪扩散概率模型进行生成性量子机器学习

    Generative quantum machine learning via denoising diffusion probabilistic models

    [https://arxiv.org/abs/2310.05866](https://arxiv.org/abs/2310.05866)

    通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。

    

    深度生成模型是计算机视觉、文本生成和大型语言模型的关键技术。最近，由于其能够生成多样化和高质量的样本，以及结构灵活、训练简单的特点，去噪扩散概率模型（DDPMs）在许多计算机视觉任务中受到了广泛关注。量子生成模型利用纠缠和叠加的能力为学习经典和量子数据带来了新的见解。受经典模型的启发，我们提出了“量子去噪扩散概率模型”（QuDDPM），以实现对量子数据的高效可训练的生成学习。QuDDPM采用足够层数的电路来保证表达能力，同时引入多个中间训练任务，将目标分布与噪声之间的插值作为训练过程，以避免贫瘠平原并保证高效的训练。我们给出了学习误差的上界和...（未完待续）

    Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
    
[^122]: 可解释的符号网络代表意识的知觉

    Interpretable Semiotics Networks Representing Awareness

    [https://arxiv.org/abs/2310.05212](https://arxiv.org/abs/2310.05212)

    这个研究描述了一个计算模型，通过追踪和模拟物体感知以及其在交流中所传达的表示来模拟人类的意识。相比于大多数无法解释的神经网络，该模型具有解释性，并可以通过构建新网络来定义物体感知。

    

    人类每天都感知物体，并通过各种渠道传达他们的感知。在这里，我们描述了一个计算模型，追踪和模拟物体的感知以及它们在交流中所传达的表示。我们描述了我们内部表示的两个关键组成部分（"观察到的"和"看到的"），并将它们与熟悉的计算机视觉概念（编码和解码）相关联。这些元素被合并在一起形成符号网络，模拟了物体感知和人类交流中的意识。如今，大多数神经网络都是不可解释的。另一方面，我们的模型克服了这个限制。实验证明了该模型的可见性。我们人的物体感知模型使我们能够通过网络定义物体感知。我们通过构建一个包括基准分类器和额外层的新网络来演示这一点。这个层产生了图像的感知。

    Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
    
[^123]: 基于代码训练的语言模型的冗余和概念分析

    Redundancy and Concept Analysis for Code-trained Language Models

    [https://arxiv.org/abs/2305.00875](https://arxiv.org/abs/2305.00875)

    本文针对代码训练的语言模型进行冗余和概念分析，通过消除与给定任务不相关的神经元，帮助理解网络中哪些神经元和层可以被消除，并在哪里找到重要的代码属性。

    

    针对代码训练的语言模型在各种代码智能任务中表现出高效性。然而，由于计算瓶颈和内存限制，对于许多软件工程应用来说，它们可能难以训练和部署。为了解决这些问题，实施有效的策略需要更好地理解这些“黑匣子”模型。本文针对源代码模型进行了首次神经元级别分析，以识别潜在表示中的“重要”神经元。我们通过消除与给定任务高度相似或不相关的神经元来实现这一点。这种方法有助于我们了解哪些神经元和层可以被消除（冗余分析），以及网络中的重要代码属性位于何处（概念分析）。利用冗余分析，我们得出了与知识转移和模型优化应用相关的观察结果。

    arXiv:2305.00875v2 Announce Type: replace-cross  Abstract: Code-trained language models have proven to be highly effective for various code intelligence tasks. However, they can be challenging to train and deploy for many software engineering applications due to computational bottlenecks and memory constraints. Implementing effective strategies to address these issues requires a better understanding of these 'black box' models. In this paper, we perform the first neuron-level analysis for source code models to identify \textit{important} neurons within latent representations. We achieve this by eliminating neurons that are highly similar or irrelevant to the given task. This approach helps us understand which neurons and layers can be eliminated (redundancy analysis) and where important code properties are located within the network (concept analysis). Using redundancy analysis, we make observations relevant to knowledge transfer and model optimization applications. We find that over 9
    
[^124]: 单细胞是空间标记：用于空间转录组数据填充的Transformer

    Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation

    [https://arxiv.org/abs/2302.03038](https://arxiv.org/abs/2302.03038)

    本文探讨了如何将单细胞视为空间标记，并利用Transformer模型促进空间转录组数据填充。

    

    空间解析转录组学通过提供物理位置和基因表达带来了单细胞分析的激动人心的突破。然而，由于极高的空间分辨率成本，细胞水平的空间转录组数据在很大程度上存在缺失值。本文提出将单个细胞视为空间标记，并通过使用多头自注意机制和位置编码的Transformer模型来促进空间转录组填充。

    arXiv:2302.03038v2 Announce Type: replace-cross  Abstract: Spatially resolved transcriptomics brings exciting breakthroughs to single-cell analysis by providing physical locations along with gene expression. However, as a cost of the extremely high spatial resolution, the cellular level spatial transcriptomic data suffer significantly from missing values. While a standard solution is to perform imputation on the missing values, most existing methods either overlook spatial information or only incorporate localized spatial context without the ability to capture long-range spatial information. Using multi-head self-attention mechanisms and positional encoding, transformer models can readily grasp the relationship between tokens and encode location information. In this paper, by treating single cells as spatial tokens, we study how to leverage transformers to facilitate spatial tanscriptomics imputation. In particular, investigate the following two key questions: (1) $\textit{how to encod
    
[^125]: Orbit：一个统一的交互式机器人学习环境仿真框架

    Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments

    [https://arxiv.org/abs/2301.04195](https://arxiv.org/abs/2301.04195)

    Orbit是一个统一的模块化机器人学习框架，提供了逼真的场景和高保真的刚性和可变形体模拟，支持多样化的任务和传感器，能够通过GPU并行化快速训练强化学习策略和收集大型演示数据集。

    

    我们提出了Orbit，这是一个由NVIDIA Isaac Sim驱动的机器人学习的统一和模块化框架。它采用模块化设计，可以轻松高效地创建具有照片级逼真场景和高保真刚体和可变形体模拟的机器人环境。通过Orbit，我们提供了一套不同难度的基准任务，从单阶段的柜子打开和布料折叠到多阶段任务，如房间重组。为了支持处理不同的观测和动作空间，我们包括具有不同基于物理的传感器和运动生成器的固定臂和移动式操作器。Orbit可以通过利用基于GPU的并行化，在几分钟内训练强化学习策略并收集来自手工设计或专家解决方案的大型演示数据集。总之，我们提供了一个开源框架，预先提供了16种机器人平台，4种传感器模式，10种运动生成器。

    arXiv:2301.04195v2 Announce Type: replace-cross  Abstract: We present Orbit, a unified and modular framework for robot learning powered by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently create robotic environments with photo-realistic scenes and high-fidelity rigid and deformable body simulation. With Orbit, we provide a suite of benchmark tasks of varying difficulty -- from single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization. To support working with diverse observations and action spaces, we include fixed-arm and mobile manipulators with different physically-based sensors and motion generators. Orbit allows training reinforcement learning policies and collecting large demonstration datasets from hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization. In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4 sensor modalities, 10 motion gener
    
[^126]: 层级奖励函数：规定和快速学习所需行为

    Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior

    [https://arxiv.org/abs/2212.03733](https://arxiv.org/abs/2212.03733)

    层级奖励函数提出了一种解决奖励设计问题的方法，能够保证诱导出根据偏好关系是帕累托最优的策略，并在多个环境中展示其快速学习的能力。

    

    强化学习代理通过与环境的交互来最大化奖励信号。在学习过程中，我们作为人类的任务是设计奖励函数，以表达所期望的行为，并使代理能够迅速学习这种行为。在这项工作中，我们考虑了任务中达到良好状态和避免不良状态的奖励设计问题。首先，我们提出了一种严格的策略空间的部分排序，以解决行为偏好中的权衡。我们更倾向于能更快速地到达良好状态并以更高的概率到达，同时能更长时间地避免不良状态的策略。接下来，我们介绍了层级奖励，一类与环境无关的奖励函数，并表明它保证诱导出根据我们的偏好关系是帕累托最优的策略。最后，我们证明了层级奖励可以通过在多个环境上使用多个表格进行评估，从而实现快速学习。

    arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular
    
[^127]: 水平联邦学习模型的单方面劫持

    Hijack Vertical Federated Learning Models As One Party

    [https://arxiv.org/abs/2212.00322](https://arxiv.org/abs/2212.00322)

    这项研究探讨了水平联邦学习模型的安全性，弥补了现有研究中对于该模型安全性的不足。

    

    水平联邦学习（VFL）是一种新兴范式，使合作伙伴能够以分布式方式共同构建机器学习模型。一般来说，这些参与方有一组共同用户，但拥有不同特征。现有的VFL框架使用密码技术提供数据隐私和安全性保证，导致了一系列研究计算效率和快速实现的工作。然而，VFL模型的安全性仍未得到充分探讨。

    arXiv:2212.00322v2 Announce Type: replace-cross  Abstract: Vertical federated learning (VFL) is an emerging paradigm that enables collaborators to build machine learning models together in a distributed fashion. In general, these parties have a group of users in common but own different features. Existing VFL frameworks use cryptographic techniques to provide data privacy and security guarantees, leading to a line of works studying computing efficiency and fast implementation. However, the security of VFL's model remains underexplored.
    
[^128]: 鲁棒的心电图分类装饰网络架构

    Decorrelative Network Architecture for Robust Electrocardiogram Classification

    [https://arxiv.org/abs/2207.09031](https://arxiv.org/abs/2207.09031)

    我们提出了一种基于特征装饰和Fourier分区的新型集成方法，用于教授网络各种互补特征，减少基于扰动的愚弄的机会。

    

    人工智能在医学数据分析方面取得了很大进展，但缺乏鲁棒性和可信度使得这些方法尚未被广泛部署。我们提出了一种基于特征装饰和Fourier分区的新型集成方法，用于教授网络各种互补特征，减少基于扰动的愚弄的机会。我们在单通道和多通道心电图分类上测试了我们的方法，并将对抗训练和DVERGE调整为贝叶斯集成框架进行比较。我们的结果表明，结合

    arXiv:2207.09031v4 Announce Type: replace-cross  Abstract: Artificial intelligence has made great progress in medical data analysis, but the lack of robustness and trustworthiness has kept these methods from being widely deployed. As it is not possible to train networks that are accurate in all scenarios, models must recognize situations where they cannot operate confidently. Bayesian deep learning methods sample the model parameter space to estimate uncertainty, but these parameters are often subject to the same vulnerabilities, which can be exploited by adversarial attacks. We propose a novel ensemble approach based on feature decorrelation and Fourier partitioning for teaching networks diverse complementary features, reducing the chance of perturbation-based fooling. We test our approach on single and multi-channel electrocardiogram classification, and adapt adversarial training and DVERGE into the Bayesian ensemble framework for comparison. Our results indicate that the combination
    
[^129]: 温和的规范执行：更快的出现，更快乐的智能体

    Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])

    [http://arxiv.org/abs/2401.16461](http://arxiv.org/abs/2401.16461)

    通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。

    

    多智能体系统可视为一个自主智能体的社会，通过社会规范可以有效地调控智能体的交互。一般来说，一个社会的规范并不是硬编码的，而是从智能体的交互中产生的。具体来说，一个社会中的智能体对另一个智能体的行为作出的反应以及对他人反应的回应，决定了社会中出现哪些规范。我们将一个智能体对另一个智能体的满意或不满意行为的反应视为第一个智能体向第二个智能体的交流。理解这些交流是一种社会智能：这些交流通过推动智能体朝着某些行为进行，从而促进规范的出现。虽然众所周知惩罚可以导致规范的出现，但我们认为更宽泛的社会智能可能在促进多智能体系统中的合作方面更有效。因此，我们开发了一种被称为Ne的方法

    A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
    
[^130]: 大型语言模型中的通用漏洞：上下文学习后门攻击

    Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])

    [http://arxiv.org/abs/2401.05949](http://arxiv.org/abs/2401.05949)

    本研究发现上下文学习范式在大型语言模型中存在漏洞，攻击者可以通过污染示范上下文来操控模型行为，而无需进行微调。这项研究设计了一种名为ICLAttack的后门攻击方法，可以通过污染示范样本和提示来使模型按照预定义的意图行事。

    

    上下文学习是一种在预训练和微调之间弥合差距的范式，在几个自然语言处理任务中展现了高效性，特别是在少样本设置中。与传统的微调方法不同，上下文学习能够适应未见过的任务而无需更新任何参数。尽管被广泛应用，上下文学习仍然容易受到恶意攻击。本研究提出了对这一范式的安全性问题的关切。我们的研究表明，攻击者可以通过污染示范上下文来操控大型语言模型的行为，而无需对模型进行微调。具体来说，我们设计了一种新的后门攻击方法，命名为ICLAttack，针对基于上下文学习的大型语言模型。我们的方法包括两种类型的攻击：污染示范样本和污染提示，可以使模型按照预定义的意图行事。ICLAttack不需要额外的微调。

    In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
    
[^131]: AUTOACT：通过自主规划实现的自动代理学习

    AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])

    [http://arxiv.org/abs/2401.05268](http://arxiv.org/abs/2401.05268)

    AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。

    

    语言代理在各种复杂任务上取得了相当的性能。尽管在这个领域进行了不断的探索，但现有的语言代理系统仍然面临昂贵、不可重复的数据依赖问题，并且面临将单一模型应用于多个功能的挑战。为此，我们介绍了AutoAct，这是一个自动代理学习框架，不依赖于大规模带注释的数据和来自闭源模型（如GPT-4）的合成轨迹。给定有限的数据和工具库，AutoAct首先自动合成规划轨迹，不需要人类或强闭源模型的任何辅助。然后，AutoAct利用分工策略，根据目标任务信息和合成轨迹自动区分，产生一个子代理组来完成任务。我们进行了多种LLMs的广泛实验，结果显示AutoAct在性能上优于或与其相当。

    Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
    
[^132]: 并非所有国家都庆祝感恩节：关于大型语言模型中的文化主导问题

    Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models. (arXiv:2310.12481v1 [cs.CL])

    [http://arxiv.org/abs/2310.12481](http://arxiv.org/abs/2310.12481)

    本文研究了大型语言模型中的文化主导问题，发现由于在模型训练中主要使用英语数据，当用户使用非英语语言提问时，模型往往提供与预期文化不相关的不恰当答案。我们提出了通过多样化数据预训练和文化感知提示两种方法来解决这个问题。

    

    本文针对大型语言模型（LLM）中存在的文化主导问题进行了研究，该问题源于在模型训练中主要使用英语数据（例如ChatGPT）。当用户使用非英语语言提问时，LLMs往往会提供与预期文化不相关的不恰当的英语文化相关答案。为了系统评估文化主导问题，我们构建了一个包含具体文化对象（如假日和歌曲）和抽象文化对象（如价值观和观点）的基准测试集。实证结果表明，代表性的GPT模型存在文化主导问题，其中GPT-4受到最严重影响，而text-davinci-003在这个问题上受影响最小。我们的研究强调了在开发和部署过程中对文化主导问题进行批判性审视和伦理考虑的需要。我们展示了两种直接的方法：模型开发中的多样化数据预训练和部署中的文化感知提示，可以显著缓解文化主导问题。

    In this paper, we identify a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g. ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark that consists of both concrete (e.g. holidays and songs) and abstract (e.g. values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need for critical examination of cultural dominance and ethical consideration in their development and deployment. We show two straightforward methods in model development (i.e. pretraining on more diverse data) and deployment (e.g. culture-aware prompting) can signifi
    
[^133]: 机器学习模型地球科学系统建模中的质量保持感知器

    A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])

    [http://arxiv.org/abs/2310.08644](http://arxiv.org/abs/2310.08644)

    这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。

    

    虽然数十年来致力于构建用于预测地球科学系统时间序列演化的物理-概念 (PC) 模型，但最近的研究表明，基于机器学习 (ML) 的门控循环神经网络技术可以用于开发更准确的模型。然而，从ML基础模型中提取物理理解的困难使得其在增强对系统结构和功能的科学知识方面的应用变得复杂。在这里，我们提出了一个理解物理性的质量保持感知器 (MCP) 作为弥合PC模型和ML模型的方法。MCP利用PC模型和GRNNs背后的有向图结构的内在同构性，以可解释的方式明确表示物理过程的质量保持性质，同时利用现有数据和现成的ML技术直接学习这种过程的功能性（可解释性）。

    Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
    
[^134]: OpsEval: 用于大型语言模型的全面任务导向的AIOps基准测试

    OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. (arXiv:2310.07637v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.07637](http://arxiv.org/abs/2310.07637)

    OpsEval是一个全面任务导向的AIOps基准测试，评估了大型语言模型在有线网络操作、5G通信操作和数据库操作等关键场景下的能力水平，为提供针对AIOps定制的LLMs的优化方向。

    

    大型语言模型(Large Language Models, LLMs)在翻译、总结和生成等NLP相关任务中表现出了显著的能力。LLMs在特定领域中应用，特别是在AIOps（面向IT运维的人工智能）中，由于其先进的信息汇总、报告分析和API调用能力而具有巨大的潜力。然而，当前LLMs在AIOps任务中的性能尚未确定。此外，需要一个全面的基准测试来引导针对AIOps定制的LLMs的优化。与现有的专注于评估网络配置等特定领域的基准测试不同，本文提出了OpsEval，这是一个专为LLMs设计的全面任务导向的AIOps基准测试。OpsEval首次对LLMs在三个关键场景（有线网络操作、5G通信操作和数据库操作）以及不同的能力水平（知识回忆、分析思考）进行评估。

    Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, an
    
[^135]: (动态)提示可能是修复压缩LLMs所需的全部。(arXiv:2310.00867v2 [cs.CL] UPDATED)

    (Dynamic) Prompting might be all you need to repair Compressed LLMs. (arXiv:2310.00867v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00867](http://arxiv.org/abs/2310.00867)

    提出了一种动态提示(IDP)的机制，它可以作为一种轻量级的适应工具，修复压缩的大型语言模型(LLMs)在一些实际的下游任务中的性能下降。

    

    大型语言模型(LLMs)在自然语言处理方面有着重大的变革，但同时也带来了显著的计算需求，强调了高效、无需训练的压缩的需求。尽管针对最大的LLMs在无需训练的压缩方面取得了显著的改进，但我们使用LLaMA-7B和OPT-6.7b进行的测试显示，在一些实际的下游任务中存在显著的性能下降。对资源密集型的压缩后重新训练的权衡的调查表明，提示驱动的恢复作为一种轻量级的适应工具具有潜在的前景。然而，现有研究主要局限在困惑度评估和简单任务上，对提示的可扩展性和通用性没有给出明确的信心。我们通过两种关键方法解决了这种不确定性。首先，我们揭示了LLM压缩中天真提示的脆弱性，即过度依赖单一输入的提示。作为回应，我们提出了推理时动态提示(IDP)的机制，它可以自主选择最佳的提示。

    Large language models (LLMs), while transformative for NLP, come with significant computational demands, underlining the need for efficient, training-free compression. Notably, despite the marked improvement in training-free compression for the largest of LLMs, our tests using LLaMA-7B and OPT-6.7b highlight a significant performance drop in several realistic downstream tasks. Investigation into the trade-off between resource-intensive post-compression re-training highlights the prospect of prompt-driven recovery as a lightweight adaption tool. However, existing studies, confined mainly to perplexity evaluations and simple tasks, fail to offer unequivocal confidence in the scalability and generalizability of prompting. We tackle this uncertainty in two key ways. First, we uncover the vulnerability of naive prompts in LLM compression as an over-reliance on a singular prompt per input. In response, we propose inference-time dynamic prompting (IDP), a mechanism that autonomously chooses f
    
[^136]: ToRA：一种集成工具的数学问题求解推理代理

    ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.17452](http://arxiv.org/abs/2309.17452)

    ToRA是一种集成工具的数学问题求解推理代理，通过结合语言的分析能力和工具的计算效率，能够显著提高数学推理的性能，在多个数学推理数据集上取得了13%-19%的平均绝对改进率，并在竞赛级数据集MATH上达到了44.6%的性能。

    

    大型语言模型在各种语言任务中取得了重大进展，但在复杂的数学问题上仍然存在困难。在本文中，我们提出了一系列集成工具的推理代理ToRA，它通过无缝地将自然语言推理与外部工具（例如计算库和符号求解器）的利用相结合，从而将语言的分析能力与工具的计算效率融合在一起，用于解决具有挑战性的数学问题。为了训练ToRA，我们精选了数学数据集上的互动工具使用轨迹，应用模仿学习于注释，并提出输出空间整形来进一步改进模型的推理行为。结果显示，ToRA模型在10个涵盖各种规模的数学推理数据集上显著优于开源模型，平均绝对改进率达到13%至19%。值得注意的是，ToRA-7B 在竞赛级数据集MATH上达到了44.6%，超越了最佳开源模型WizardMath。

    Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
    
[^137]: 用户体验设计专业人员对生成性人工智能的感知

    User Experience Design Professionals' Perceptions of Generative Artificial Intelligence. (arXiv:2309.15237v1 [cs.CY])

    [http://arxiv.org/abs/2309.15237](http://arxiv.org/abs/2309.15237)

    经验丰富的设计师认为生成性人工智能（GenAI）在用户体验设计（UXD）实践中具有辅助作用。然而，初级设计师可能会面临技能退化、工作替代和创造力枯竭等问题。这篇论文讨论了人-GenAI协作的一些影响，包括版权与所有权、人类创造力和代理性以及AI素养和获取。

    

    在创意专业人员中，生成性人工智能（GenAI）引起了对其能力的兴奋和对未预期后果的担忧。GenAI如何影响用户体验设计（UXD）实践，并且这些担忧是否合理？我们采访了20位UX设计师，他们拥有丰富的经验，来自各种公司（创业公司到大型企业）。我们询问他们的实践特征，并了解他们的态度、关注点和期望。我们发现，经验丰富的设计师对他们的原创性、创造力和共情能力充满信心，并认为GenAI的角色是辅助性的。他们强调了"享受"和"代理"这两个独特的人类因素，在这两个方面人类仍然是"AI对齐"的仲裁者。然而，技能退化、工作替代和创造力枯竭可能对初级设计师产生负面影响。我们讨论了人-GenAI协作的影响，特别是版权和所有权、人类创造力和代理性，以及AI素养和获取方面。通过这个视角，我们将我们的发现与前人研究进行了比较。

    Among creative professionals, Generative Artificial Intelligence (GenAI) has sparked excitement over its capabilities and fear over unanticipated consequences. How does GenAI impact User Experience Design (UXD) practice, and are fears warranted? We interviewed 20 UX Designers, with diverse experience and across companies (startups to large enterprises). We probed them to characterize their practices, and sample their attitudes, concerns, and expectations. We found that experienced designers are confident in their originality, creativity, and empathic skills, and find GenAI's role as assistive. They emphasized the unique human factors of "enjoyment" and "agency", where humans remain the arbiters of "AI alignment". However, skill degradation, job replacement, and creativity exhaustion can adversely impact junior designers. We discuss implications for human-GenAI collaboration, specifically copyright and ownership, human creativity and agency, and AI literacy and access. Through the lens 
    
[^138]: 用于自动开放领域科学假设发现的大语言模型

    Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])

    [http://arxiv.org/abs/2309.02726](http://arxiv.org/abs/2309.02726)

    这项研究提出了用于社会科学学术假设发现的第一个自然语言处理数据集，旨在开发一个系统，能够基于原始网络语料库自动生成有效、新颖且对人类研究者有帮助的假设。

    

    当科学家观察世界并试图提出解释这些观察结果的假设时，假设归纳被认为是主要的推理类型。过去关于假设归纳的研究存在以下限制：（1）数据集的观察注释不是原始的网络语料库，而是手动选择的句子（导致了一个封闭领域的设置）；（2）实际的假设注释主要是常识知识，使得任务不太具有挑战性。在本文中，我们提出了第一个用于社会科学学术假设发现的自然语言处理数据集，包含50篇发表在顶级社会科学期刊上的最新论文。数据集中还收集了开发论文中的假设所需的原始网络语料库，最终目标是创建一个系统，仅通过一堆原始网络语料库就可以自动生成有效、新颖且对人类研究者有帮助的假设。这个新数据集可以解决以前关于假设归纳的研究所面临的限制问题。

    Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previou
    
[^139]: 通过编码本知识、自然语言推理和ChatGPT来合成政治零样本关系分类

    Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])

    [http://arxiv.org/abs/2308.07876](http://arxiv.org/abs/2308.07876)

    该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。

    

    最近的事件编码的监督模型在性能方面远远超过模式匹配方法。然而，它们仅仅依赖于新的注释，忽视了专家数据库中的大量知识，限制了它们在细粒度分类中的适用性。为了解决这些限制，我们通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类。我们的研究涵盖了ChatGPT和一种新颖的基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，将任务分解为上下文、语态和类别消歧的不同层次。该框架提高了解释性、效率和对模式更改的适应性。通过在我们新策划的数据集上进行大量实验，我们指出了ChatGPT中的不稳定性问题，并突出了ZSP的卓越性能。ZSP在细粒度根代码分类的F1得分上取得了令人印象深刻的提高40%。

    Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
    
[^140]: 公平机器遗忘：在减少差异的同时删除数据

    Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])

    [http://arxiv.org/abs/2307.14754](http://arxiv.org/abs/2307.14754)

    本研究提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的机器学习方法。

    

    随着公众对企业收集和使用个人信息的意识增强，消费者积极参与企业数据集的管理变得越来越重要。为此，数据管理框架（如欧洲通用数据保护条例）已经提出了被遗忘的权利，允许个人请求将其个人数据从组织使用的数据库和模型中删除。为了实现遗忘，已经提出了几种机器学习遗忘方法，以解决每个遗忘请求重新训练模型的计算效率问题。虽然这些在线替代方案可以高效地进行遗忘，但它们对于其他关键的实际应用属性（如公平性）的影响尚不清楚。在这项工作中，我们提出了第一个能够可靠而高效地遗忘数据实例并保持公平性的方法。

    As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fai
    
[^141]: 预测人类如何在自身利益与他人利益之间平衡的可预测性

    Predict-AI-bility of how humans balance self-interest with the interest of others. (arXiv:2307.12776v1 [econ.GN])

    [http://arxiv.org/abs/2307.12776](http://arxiv.org/abs/2307.12776)

    生成式AI能够准确预测人类在决策中平衡自身利益与他人利益的行为模式，但存在高估他人关注行为的倾向，这对AI的开发者和用户具有重要意义。

    

    生成式人工智能具有革命性的潜力，可以改变从日常生活到高风险场景的决策过程。然而，由于许多决策具有社会影响，为了使AI能够成为可靠的决策助手，它必须能够捕捉自身利益与他人利益之间的平衡。我们对三种最先进的聊天机器人对来自12个国家的78个实验的独裁者游戏决策进行了研究。我们发现，只有GPT-4（而不是Bard或Bing）能够正确捕捉到行为模式的定性特征，识别出三种主要的行为类别：自私的、不公平厌恶的和完全无私的。然而，GPT-4一直高估了他人关注行为，夸大了不公平厌恶和完全无私参与者的比例。这种偏见对于AI开发人员和用户具有重要意义。

    Generative artificial intelligence holds enormous potential to revolutionize decision-making processes, from everyday to high-stake scenarios. However, as many decisions carry social implications, for AI to be a reliable assistant for decision-making it is crucial that it is able to capture the balance between self-interest and the interest of others. We investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 78 experiments with human participants from 12 countries. We find that only GPT-4 (not Bard nor Bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. Nonetheless, GPT-4 consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. This bias has significant implications for AI developers and users.
    
[^142]: 具有确定性演化状态的强盗模型

    Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])

    [http://arxiv.org/abs/2307.11655](http://arxiv.org/abs/2307.11655)

    该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。

    

    我们提出了一种学习与强盗反馈结合的模型，同时考虑到确定性演化和不可观测的状态，我们称之为具有确定性演化状态的强盗模型。我们的模型主要应用于推荐系统和在线广告的学习。在这两种情况下，算法在每一轮获得的奖励是选择行动的短期奖励和系统的“健康”程度（即通过其状态测量）的函数。例如，在推荐系统中，平台从用户对特定类型内容的参与中获得的奖励不仅取决于具体内容的固有特征，还取决于用户与平台上其他类型内容互动后其偏好的演化。我们的通用模型考虑了状态演化的不同速率λ∈[0,1]（例如，用户的偏好因先前内容消费而快速变化）。

    We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
    
[^143]: FLASK: 基于对齐技能集的细粒度语言模型评估

    FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.10928](http://arxiv.org/abs/2307.10928)

    FLASK是一种基于对齐技能集的细粒度语言模型评估协议，通过将粗级评分分解为每个指令的技能集级评分，实现了对模型性能的全面视角和提高评估的可靠性。

    

    由于指令需要与人类的价值观进行对齐，并且所需的技能集根据指令而异，因此对大型语言模型（LLMs）进行评估具有挑战性。然而，先前的研究主要集中在粗粒度评估（即基于整体偏好的评估），这限制了可解释性，因为它未考虑需要实例级技能组合的用户指令的特性。在本文中，我们介绍了FLASK（基于对齐技能集的细粒度语言模型评估），这是一种细粒度评估协议，用于人类和模型的评估，它将粗级评分分解为每个指令的技能集级评分。通过实验证明，评估的细粒度对于获得对模型性能的全面视角和提高评估的可靠性至关重要。利用FLASK，我们比较了多个开源和专有LLMs，并观察到高度相关性。

    Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
    
[^144]: 重新思考对抗策略：多智能体强化学习中的广义攻击形式和可证明的防御

    Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])

    [http://arxiv.org/abs/2305.17342](http://arxiv.org/abs/2305.17342)

    本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。

    

    大多数现有的研究研究直接扰动受害者的状态/动作或基础转移动态以展示强化学习智能体在对抗攻击下的脆弱性。然而，这样的直接操纵在实践中并不总是可行的。在本文中，我们考虑另一种常见且现实的攻击设置：在经过训练的多智能体RL的设置中，在部署期间，受害代理$\nu$被攻击者控制另一个代理$\alpha$以敌对方式行动，使用“对抗策略”对受害代理进行攻击。尽管之前的攻击模型考虑了这种设置，但他们没有考虑到攻击者可以遇到抵抗，因此只能部分控制代理$\alpha$，同时引入可察觉的“异常”行为，这些行为很容易被检测到。并且缺乏针对这些对抗策略的可证明的防御。为了解决这些问题，我们引入了一个更一般化的攻击形式，模拟了攻击者在何种程度上可以控制代理$\alpha$。

    Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
    
[^145]: CRITIC：大型语言模型可以通过工具交互批评进行自我校正

    CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])

    [http://arxiv.org/abs/2305.11738](http://arxiv.org/abs/2305.11738)

    本文提出了一个名为CRITIC的框架，使得大型语言模型可以通过与工具的交互校正自己的错误，从而避免生成出现不一致和问题行为的结果。

    

    近年来，大型语言模型的发展非常引人注目。然而，这些模型有时会出现不一致和问题行为，例如出现幻觉事实，生成有缺陷的代码或创建冒犯和有害的内容。与这些模型不同，人类通常使用外部工具来交叉检查和精炼他们的初步内容，例如使用搜索引擎进行事实检查或使用代码解释器进行调试。受这一观察的启发，我们引入了一个名为CRITIC的框架，允许LLMs（实质上是“黑盒子”）以类似于人类与工具交互的方式验证和逐步修正自己的输出。更具体地说，从初始输出开始，CRITIC与适当的工具交互以评估文本的某些方面，然后根据在此验证过程中获得的反馈修改输出。涉及自由形式问答、数学程序综合和毒性检测的全面评估表明，我们的框架使LLMs能够从错误中学习并纠正自己的错误。

    Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
    
[^146]: 可计算的基于图诱导的和积网络进行概率图表示学习

    Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])

    [http://arxiv.org/abs/2305.10544](http://arxiv.org/abs/2305.10544)

    GSPNs是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询，并通过权重共享和树状计算图的优势获得了纯概率模型的效率和深度图网络的效果。

    

    我们介绍了基于图诱导的和积网络 (GSPN)，它是一种新的概率框架，用于图表示学习，可以可计算地回答概率查询。受消息传递神经网络中由顶点引起的计算树的启发，我们建立了一组和积网络（SPN）的层次结构，其中父SPN的参数是其子级的后验混合概率的可学习变换。由于权重共享和GSPN的树状计算图，我们获得了纯概率模型的效率和深度图网络的效果。我们在缺乏监督的情况下，处理缺失数据和图分类问题，证明了该模型相对于流行的神经模型的竞争力。我们通过超参数和模型回答概率查询的能力进行定性分析。

    We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.
    
[^147]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^148]: 通过可逆神经网络学习解释的非交互语义空间

    Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])

    [http://arxiv.org/abs/2305.01713](http://arxiv.org/abs/2305.01713)

    本文介绍了一种使用可逆神经网络将BERT-GPT2自动编码器的隐藏空间转换为更可分离的语义空间的方法，实验结果表明此方法可以改进模型的可解释性和可控性，并取得了比最先进模型更好的性能表现。

    

    在细化连续空间的句子表征上进行解耦可以在定位明确发生的生成因素的同时，改进可解释性和语义控制，这为基于神经的语言模型赋予了一些符号模型的优势，同时保持其灵活性。 本文提出了一种方法，通过使用可逆神经网络（INN）将BERT-GPT2自动编码器的隐藏空间转换为更可分离的语义空间来解除编码的隐藏空间。实验结果表明，与最新的最先进模型相比，INN能够将分布式隐藏空间转换为更好的语义上解耦的潜在空间，从而产生更好的可解释性和可控性。

    Disentangling sentence representations over continuous spaces can be a critical process in improving interpretability and semantic control by localising explicit generative factors. Such process confers to neural-based language models some of the advantages that are characteristic of symbolic models, while keeping their flexibility. This work presents a methodology for disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it into a more separable semantic space with the support of a flow-based invertible neural network (INN). Experimental results indicate that the INN can transform the distributed hidden space into a better semantically disentangled latent space, resulting in better interpretability and controllability, when compared to recent state-of-the-art models.
    
[^149]: 对比学习是相似性图谱上的谱聚类

    Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15103](http://arxiv.org/abs/2303.15103)

    本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。

    

    对比学习是一种强大的自监督学习方法，但我们对其运作原理和原因的理论理解有限。本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性。利用这种等价性作为基石，我们将分析扩展到CLIP模型，并严格描述多模态对象如何被嵌入到一起。在理论洞见的推动下，我们引入了核混合损失，结合新颖的核函数，在多个视觉数据集上优于标准高斯核。

    Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
    
[^150]: 自然语言作为知识表示的逻辑推理研究：综述

    Logical Reasoning over Natural Language as Knowledge Representation: A Survey. (arXiv:2303.12023v1 [cs.CL])

    [http://arxiv.org/abs/2303.12023](http://arxiv.org/abs/2303.12023)

    本文总结了一种新的逻辑推理方法，它使用自然语言作为知识表示，具有不同于端到端神经方法的优势。这种新模式在未来有着很高的潜力。

    

    逻辑推理是人类认知和智能的核心。以往的人工智能中的逻辑推理研究使用形式化语言作为知识表示（和符号推理器）。然而，使用形式化语言进行推理证明具有困难（例如脆弱性和知识获取瓶颈）。本文总结了一种新的逻辑推理方法的综合概述，它使用自然语言作为知识表示（以及预训练语言模型作为推理器），包括逻辑推理的哲学定义和分类，新模式的优势、基准和方法，未来需要的任务和方法以及与相关 NLP 领域的关系。这种新模式是很有前途的，因为它不仅可以缓解形式化表示的许多挑战，而且也具有优于端到端神经方法的优势。

    Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation~(and symbolic reasoners). However, reasoning with formal language has proved challenging~(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation~(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks & methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.
    
[^151]: Box$^2$EL: EL++描述逻辑中的概念和角色盒子嵌入的概念和角色盒子嵌入方法及其作用

    Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11118](http://arxiv.org/abs/2301.11118)

    Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。

    

    描述逻辑本体论扩展了知识图谱与概念信息和逻辑背景知识。近年来，人们对这种本体论的归纳推理技术越来越感兴趣，这些技术有望补充传统的演绎推理算法。类似于知识图谱的完善，现有的一些方法通过在潜在空间中学习本体论嵌入，同时确保这些嵌入能够准确地捕捉到底层描述逻辑的逻辑语义。然而，它们存在一些问题，主要是由于受限的角色表示。我们提出了Box$^2$EL方法，将概念和角色都表示为盒子（即轴对齐超矩形），并展示了它如何克服之前方法的局限性。我们在理论上证明了我们模型的正确性，并进行了大量的实验评估，在各种数据集上取得了领先的结果。作为我们评估的一部分，我们引入了一个新的基准。

    Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
    
[^152]: 高效的多阶门控聚合网络

    Efficient Multi-order Gated Aggregation Network. (arXiv:2211.03295v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.03295](http://arxiv.org/abs/2211.03295)

    本文探索了现代卷积神经网络的表征能力，使用多阶博弈论交互的新视角，提出了一种新的纯卷积神经网络架构MogaNet，它表现出优异的可扩展性，并在多种典型视觉基准中以更高效的参数利用达到了与最先进模型竞争的效果。

    

    自从视觉变换器（ViTs）取得最近的成功之后，对ViT风格架构的探索引发了卷积神经网络的复兴。在本文中，我们从多阶博弈论交互的新视角探索了现代卷积神经网络的表征能力，这种交互反映了基于博弈论的不同尺度上下文的变量相互作用效应。在现代卷积神经网络框架内，我们使用概念上简单而有效的深度可分离卷积来定制两个特征混合器，以促进跨空间和通道空间的中阶信息。在这个基础上，提出了一种新的纯卷积神经网络架构，称为MogaNet，它表现出优异的可扩展性，并在ImageNet和包括COCO目标检测、ADE20K语义分割、2D&3D人体姿势估计以及视频预测等多种典型视觉基准中以更高效的参数利用达到了与最先进模型竞争的效果。

    Since the recent success of Vision Transformers (ViTs), explorations toward ViT-style architectures have triggered the resurgence of ConvNets. In this work, we explore the representation ability of modern ConvNets from a novel view of multi-order game-theoretic interaction, which reflects inter-variable interaction effects w.r.t.~contexts of different scales based on game theory. Within the modern ConvNet framework, we tailor the two feature mixers with conceptually simple yet effective depthwise convolutions to facilitate middle-order information across spatial and channel spaces respectively. In this light, a new family of pure ConvNet architecture, dubbed MogaNet, is proposed, which shows excellent scalability and attains competitive results among state-of-the-art models with more efficient use of parameters on ImageNet and multifarious typical vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\&3D human pose estimation, and video prediction. Typica
    
[^153]: 基于时间序列的数据增强技术：一份综述和分类

    Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13508](http://arxiv.org/abs/2206.13508)

    本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。

    

    随着深度学习建模的最新进展，利用其在时间序列领域中出色性能的方式并不需要太长时间。深度神经网络在处理时间序列方面严重依赖于训练中使用的数据集的大小和一致性。这些特征通常在现实世界中并不丰富，通常受到限制和需要保证的约束。因此，提高数据量的有效方法是使用数据增强技术，无论是通过添加噪声或置换还是生成新的合成数据。本文系统地回顾了该领域中的最新技术现状，提供了所有可用算法的概述，并提出了最相关研究的分类法。不同变体的效率将作为该过程的中心部分进行评估，同时还将评估不同的性能指标以及每个模型的主要问题。

    With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
    
[^154]: 计算机视觉GAN综述：最新研究、分析和分类（arXiv：2203.11242v2 [cs.LG] UPDATED）

    A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.11242](http://arxiv.org/abs/2203.11242)

    本文综述了GAN的最新架构、损失函数优化、验证指标和应用领域，并提出了一个分类法以更好地理解计算机视觉中GAN的现状。

    

    在过去的几年中，深度学习领域已经进行了几次革命，其中最受关注的是生成对抗网络（GANs）的巨大影响。GAN不仅在定义其模型时提供了独特的架构，而且生成了引人注目的结果，对社会产生了直接影响。由于GAN带来的重大改进和新的研究领域，社区不断提出新的研究，使得跟上时代几乎是不可能的。我们的综述旨在提供GAN的概述，展示最新的架构、损失函数的优化、验证指标和最广泛认可的变体的应用领域。将评估不同变体的模型架构效率，展示最佳的应用领域；作为该过程的重要组成部分，将分析评估GAN性能的不同度量标准和经常使用的损失函数。最后，将提出一个分类法以更好地理解GAN在计算机视觉领域中的现状。

    In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks (GANs). GANs not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that GANs have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of GANs, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of GANs and the frequently used loss functions will
    
[^155]: 基于深度学习方法的深伪造内容检测综述

    A Review of Deep Learning-based Approaches for Deepfake Content Detection. (arXiv:2202.06095v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.06095](http://arxiv.org/abs/2202.06095)

    本文综述了最近基于深度学习方法的深伪造内容检测的研究，系统地回顾了不同类别的伪造内容检测，并报告了所考察工作的优点和缺点，以及深伪造检测领域仍未解决的问题和不足之处的未来研究方向。

    

    深度学习生成模型的最新进展引起了人们的担忧，因为它们可以创建高度逼真的伪造图像和视频。这对人们的完整性构成威胁，可能导致社会不稳定。为了解决这个问题，迫切需要开发新的计算模型，能够有效检测伪造内容，并提醒用户可能的图像和视频篡改。本文全面回顾了使用基于深度学习方法的深伪造内容检测的最新研究。我们旨在通过系统地审视不同类别的伪造内容检测来拓宽最新研究的前沿。此外，我们还报告了所考察工作的优点和缺点，以及深伪造检测领域仍未解决的问题和不足之处的未来研究方向。

    Recent advancements in deep learning generative models have raised concerns as they can create highly convincing counterfeit images and videos. This poses a threat to people's integrity and can lead to social instability. To address this issue, there is a pressing need to develop new computational models that can efficiently detect forged content and alert users to potential image and video manipulations. This paper presents a comprehensive review of recent studies for deepfake content detection using deep learning-based approaches. We aim to broaden the state-of-the-art research by systematically reviewing the different categories of fake content detection. Furthermore, we report the advantages and drawbacks of the examined works and future directions towards the issues and shortcomings still unsolved on deepfake detection.
    

