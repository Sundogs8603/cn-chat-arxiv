# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees](https://rss.arxiv.org/abs/2402.00899) | 这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。 |
| [^2] | [IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation](https://arxiv.org/abs/2402.08682) | 本文提出了一种名为IM-3D的方法，通过考虑视频生成器和使用高斯平铺的3D重构算法，从生成的视图直接输出高质量的3D结果，实现了更高效的流水线、更好的质量和更高的可用3D资产产出。 |
| [^3] | [Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance](https://arxiv.org/abs/2402.08680) | 本文介绍了一种名为MARINE的框架，用于通过无分类器引导来减少大型视觉语言模型的物体幻觉。该框架无需训练或API访问，并通过集成视觉模型和引入额外的物体基础特征来提高模型的生成精确性和效率。 |
| [^4] | [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679) | 本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。 |
| [^5] | [Model Assessment and Selection under Temporal Distribution Shift](https://arxiv.org/abs/2402.08672) | 本文研究了在变化环境中的模型评估与选择问题，通过合成不同时期的数据集，并开发了自适应滚动窗口方法来估计模型的泛化误差以及比较不同模型之间的差异。实验证明了我们提出的方法在非稳态数据中的适应性。 |
| [^6] | [Are Semi-Dense Detector-Free Methods Good at Matching Local Features?](https://arxiv.org/abs/2402.08671) | 本研究首次尝试研究半稠密无检测器方法（SDF）建立对应关系能力和估计位姿质量之间的联系。作者提出了一种新颖的图像匹配架构SAM，并发现SAM在位姿估计方面表现优秀，而SDF方法在匹配准确度方面表现更好。作者建议将匹配准确度的计算限制在纹理区域。 |
| [^7] | [Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models](https://arxiv.org/abs/2402.08670) | Rec-GPT4V是一种基于大规模视觉语言模型的多模态推荐算法，通过利用用户历史作为用户偏好信息并结合图像摘要和项目标题，实现了对多个图像动态的推荐。 |
| [^8] | [The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting](https://arxiv.org/abs/2402.08658) | 本研究探索了使用大型语言模型（LLMs）实现即时自适应干预（JITAIs）的可行性。通过测试GPT-4模型以促进门诊心脏康复中心的心脏健康体育活动的使用案例，我们提出了450个JITAI决策和信息。 |
| [^9] | [SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds](https://arxiv.org/abs/2402.08653) | SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。 |
| [^10] | [Generating Universal Adversarial Perturbations for Quantum Classifiers](https://arxiv.org/abs/2402.08648) | 这项工作引入了QuGAP-一个用于生成量子分类器的通用对抗扰动的新框架，并通过实验证明了量子分类器容易受到这些攻击。 |
| [^11] | [Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data](https://arxiv.org/abs/2402.08646) | 本研究提出了一个从数据中对各种类型的符号推理进行概率化描述的统一解释，并使用经典的推理关系、经验上的推理关系、最大一致集、最大可能集和最大似然估计来实现。这个理论为实现人类类似的机器智能提供了新的见解。 |
| [^12] | [Tandem Transformers for Inference Efficient LLMs](https://arxiv.org/abs/2402.08644) | 该论文提出了一种新的架构，称为串联Transformer，用于解决传统大型语言模型推断速度限制的问题。该架构通过将小型自回归模型和大模型以块模式结合起来，并让小模型关注大模型的丰富表示，从而显著提高了小模型的预测准确性。实验证明，在预训练数据集上，串联的PaLM2-Bison和PaLM2-Gecko相比独立的PaLM2-Gecko，在下一个词元预测准确性上提高了3.3%，并且相较于具有相似下游任务的PaLM2-Otter模型，加速比达到1.16倍。 |
| [^13] | [Forecasting high-impact research topics via machine learning on evolving knowledge graphs](https://arxiv.org/abs/2402.08640) | 通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。 |
| [^14] | [Knowledge Editing on Black-box Large Language Models](https://arxiv.org/abs/2402.08631) | 这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。 |
| [^15] | [A Cost-Sensitive Transformer Model for Prognostics Under Highly Imbalanced Industrial Data](https://arxiv.org/abs/2402.08611) | 本文介绍了一种面向高度不平衡工业数据的成本敏感变压器模型，该模型在故障检测和预测方面表现出了显著的性能提升，并通过剔除实验分析了不同组件的贡献。 |
| [^16] | [Mixtures of Experts Unlock Parameter Scaling for Deep RL](https://arxiv.org/abs/2402.08609) | 本文证明了将专家组合模块融入基于值的网络中，尤其是软MoE，可以实现更具参数可扩展性的深度强化学习模型，这提供了强有力的实证证据以发展强化学习的缩放定律。 |
| [^17] | [Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs](https://arxiv.org/abs/2402.08593) | 本文介绍了一种名为"图特征预处理器"的软件库，可以从实时交易图中检测典型的洗钱和欺诈模式，并生成丰富的交易特征，从而显著提高机器学习模型的预测准确率。 |
| [^18] | [FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis](https://arxiv.org/abs/2402.08582) | 本论文提出了一种名为FESS Loss的增强特征空间分割损失，将对比学习和Dice损失相结合，旨在在医学图像分割中提高空间精度和特征表示，从而实现更精确、更精细的分割过程。 |
| [^19] | [FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing](https://arxiv.org/abs/2402.08578) | FedLPS提出了一种在边缘计算环境中处理边缘设备生成的数据的多任务异构联邦学习方法，通过本地参数共享和迁移学习的原理来减少资源消耗和提高部署效率。 |
| [^20] | [Online Foundation Model Selection in Robotics](https://arxiv.org/abs/2402.08570) | 这篇论文介绍了在机器人学中的在线基础模型选择问题，针对采集闭源模型大量训练数据的高成本，提出了一种以用户为中心的在线模型选择解决方案，该方案结合了开源编码器和在线学习算法，通过提取上下文特征来实现模型选择。 |
| [^21] | [Artificial Intelligence for Literature Reviews: Opportunities and Challenges](https://arxiv.org/abs/2402.08565) | 这篇论文综述了人工智能在系统文献综述中的应用，尤其关注了在筛选和提取阶段的半自动化过程。该研究使用一个包括传统特征和人工智能特征的框架来考察21个领先的文献综述工具，并分析了11个利用大型语言模型进行文献搜索和学术写作辅助的最新工具。最后，论文讨论了该领域的当前趋势、主要研究挑战和发展方向。 |
| [^22] | [Higher Layers Need More LoRA Experts](https://arxiv.org/abs/2402.08562) | 这篇论文提出了一种新颖的参数高效的MoE方法（MoLA），用于Transformer-based模型，其中每个模型层可以灵活地使用不同数量的LoRA专家。通过在多个基准数据集上进行实验，研究结果表明高层需要更多的LoRA专家来提高模型性能。 |
| [^23] | [Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting](https://arxiv.org/abs/2402.08547) | 这篇论文研究了重复公平分割问题中的竞争策略，发现如果Bob过于偏好某一块蛋糕，Alice利用类似于二分查找的策略可以系统性地对Bob实施剥削，从而在时间上获得更多资源份额。 |
| [^24] | [A Distributional Analogue to the Successor Representation](https://arxiv.org/abs/2402.08530) | 本文提出了一种新的分布式强化学习方法，它通过分离转换结构和奖励，引入了分布式后继度量来描述行为的分布式后果。在实验中展示了该方法的实用性，特别是在零样本风险敏感策略评估方面。 |
| [^25] | [Counterfactual Influence in Markov Decision Processes](https://arxiv.org/abs/2402.08514) | 马尔可夫决策过程中的反事实推理问题是一个基本问题，我们提出了一种算法来构建反事实模型，并通过比较反事实和干预分布来对影响进行形式化的特征化。 |
| [^26] | [Amplifying Exploration in Monte-Carlo Tree Search by Focusing on the Unknown](https://arxiv.org/abs/2402.08511) | AmEx-MCTS introduces a novel formulation by decoupling value updates, visit count updates, and the selected path in Monte-Carlo tree search, allowing exclusion of already explored regions. This enables a broader search with the same computational resources while maintaining the utility of visit counts for exploration-exploitation balancing and quality metrics within MCTS. |
| [^27] | [From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)](https://arxiv.org/abs/2402.08509) | 本文研究了如何推断给定SPARQL CONSTRUCT查询的所有可能输出图形上成立的形状约束，考虑了输入图的形状约束和查询模板可能施加的新形状。 |
| [^28] | [A Systematic Review of Data-to-Text NLG](https://arxiv.org/abs/2402.08496) | 这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。 |
| [^29] | [The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale](https://arxiv.org/abs/2402.08492) | 本研究评估了ChatGPT在使用波士顿肠准备评分（BBPS）进行结肠镜评估时的准确性和一致性，发现其准确率较内镜医生低，但仍有成为辅助工具的潜力。 |
| [^30] | [Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming](https://arxiv.org/abs/2402.08491) | 本研究开发了一个基于深度强化学习的计算框架，用于细胞重编程中的重编程策略识别。在控制问题中，引入了伪吸引子的概念和识别方法，并设计了一个用于解决该问题的计算框架。 |
| [^31] | [Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models](https://arxiv.org/abs/2402.08473) | 本文通过新的梯度下降优化方法，探索了常用的视觉语言模型的嵌入空间。结果显示，尽管该模型在零样本分类上有超过99%的表现，但在系统性评估中却完全失败。通过线性近似，提供了一个解释这些差异的框架。 |
| [^32] | [Large Language Models for the Automated Analysis of Optimization Algorithms](https://arxiv.org/abs/2402.08472) | 本论文通过将大型语言模型集成到STNWeb中，展示了将其应用于优化算法领域的潜力，通过生成全面的报告和图表，提升用户体验和降低使用该工具的障碍。 |
| [^33] | [Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale](https://arxiv.org/abs/2402.08470) | 提出了一种并行友好的时空图学习方法，用于规模化光伏衰减分析。该方法集成了时空一致性和图注意力，可以准确估计大规模光伏逆变器的长期性能损失率，并提供在线模型和数据集以帮助改进光伏系统的性能评估和可靠性分析。 |
| [^34] | [Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence](https://arxiv.org/abs/2402.08466) | 这项研究认为，在人工智能的管理式监管方法中，加强人类引导和培训技术的研究和实践对于提高AI的性能，解决技术和伦理问题等方面具有重要作用。 |
| [^35] | [Is 3-(F)WL Enough to Distinguish All 3D Graphs?](https://arxiv.org/abs/2402.08429) | 本文探索了从图生成的角度是否存在能够区分所有3D图形的k，以及对于更复杂的3D图形，k-WL的同构判别能力是否严格增加。 |
| [^36] | [Vehicle Behavior Prediction by Episodic-Memory Implanted NDT](https://arxiv.org/abs/2402.08423) | 本文通过植入记忆的Neural Decision Tree（eMem-NDT）探索目标车辆行为预测的可解释性，并且在神经决策树中对历史车辆行为特征的行为记忆原型进行分组和对齐。 |
| [^37] | [Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging](https://arxiv.org/abs/2402.08409) | 本研究提出了一种将7T磁共振成像特征与低场磁共振成像特征融合的深度学习框架，实现在7T缺失环境下进行脑图像分割的任务。通过自适应融合和融入，利用强度引导特征，可以识别难以识别的细微结构特征。 |
| [^38] | [LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection](https://arxiv.org/abs/2402.08401) | 本文介绍了一种名为LOSS-GAT的半监督和一类方法用于假新闻检测，采用了标签传播和图注意力网络，以解决标记数据集有限性的挑战。 |
| [^39] | [Selective Learning: Towards Robust Calibration with Dynamic Regularization](https://arxiv.org/abs/2402.08384) | 本研究提出了一种名为动态正则化（DReg）的方法，通过训练学习应该学到什么，从而解决深度学习中的过拟合和误校准问题。 |
| [^40] | [Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution](https://arxiv.org/abs/2402.08383) | 本论文提出了一种将高效和精确的不确定性量化集成到基于深度学习的替代模型中的方法，称为LE-PDE-UQ。该方法利用潜在空间中的潜在向量来演化系统的状态和相应的不确定性估计。 |
| [^41] | [Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting](https://arxiv.org/abs/2402.08373) | 本文提出了动态策略（DyStrat）用于多步预测，并在实验中证明了其在多个时间序列数据集上的优越性能。 |
| [^42] | [One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill](https://arxiv.org/abs/2402.08369) | 本研究提出一种基于技能的模仿学习框架，在非平稳环境中实现一次性模仿和零次适应。通过从单个演示中推断出语义技能序列，并将其转换为经过优化的行动序列，以适应环境中隐含的动力学变化。实验结果表明，该框架在不同的一次性模仿场景下表现出良好的性能。 |
| [^43] | [Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries](https://arxiv.org/abs/2402.08349) | 本文首次深入评估了在实践中文本到SQL系统的数据模型的鲁棒性，通过基于一个多年的国际项目集中评估，对一个在FIFA World Cup背景下连续运行了9个月的真实部署的FootballDB系统进行了评估。 |
| [^44] | [Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach](https://arxiv.org/abs/2402.08341) | 本研究使用分类器驱动的方法，通过不同的输入提示探究大型语言模型的输出变化，以增加其透明度。结果显示，这些模型根据输入的不同提示而表现出不同的人格特质，类似于人类对刺激做出的反应。 |
| [^45] | [Uncertainty Quantification via Stable Distribution Propagation](https://arxiv.org/abs/2402.08324) | 通过局部线性化，我们提出了一种新的方法，能够通过神经网络传播稳定概率分布来量化输出的不确定性。我们的方法在预测校准置信区间和选择性预测方面显示出了明显的优势。 |
| [^46] | [Mapping the Ethics of Generative AI: A Comprehensive Scoping Review](https://arxiv.org/abs/2402.08323) | 本研究进行了一项生成型人工智能伦理问题的全面调查，提供了378个规范问题的分类和排序，并总结了伦理争议的核心内容，包括公正性、安全性、有害内容、隐私等问题。 |
| [^47] | [One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model](https://arxiv.org/abs/2402.08310) | 这项研究利用合成训练的生成模型，通过单个素描实现了一对多的文化艺术品三维几何重建，并能够通过多模态输入进行指导，对于历史素描等不太表现力强的表示具有较好的效果。这种解决方案仅依赖于合成数据进行训练，即使在训练样本数量很少的情况下也能适应，使领域专家能够互动地重建丢失艺术品的可能外观。 |
| [^48] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^49] | [Time to Stop and Think: What kind of research do we want to do?](https://arxiv.org/abs/2402.08298) | 这篇文章呼吁停下来思考我们想要进行什么样的研究，特别是在实验设计方面，避免不严谨和不令人信服的实验，改变以支持先前信念为目的的态度，提倡对个人和社群的真诚批判性评估和反思。 |
| [^50] | [The Effect of Data Poisoning on Counterfactual Explanations](https://arxiv.org/abs/2402.08290) | 本研究研究了反事实解释在数据污染方面的脆弱性，发现最先进的反事实生成方法和工具包容易受到数据污染的影响。 |
| [^51] | [A Logical Approach to Criminal Case Investigation](https://arxiv.org/abs/2402.08284) | 本文介绍了一种逻辑方法在刑事案件调查中的应用。研究人员通过推断凶手的动机、机会和方法来寻找罪犯。 |
| [^52] | [Pix2Code: Learning to Compose Neural Visual Concepts as Programs](https://arxiv.org/abs/2402.08280) | Pix2Code 是一个将神经视觉概念组合成程序的框架，通过利用显式、组合的符号和隐式的神经表示能力，从图像中检索对象表示并将关系概念合成为lambda演算程序，来解决通用性和可解释性的挑战。在推理领域Kandinsky Patterns和CURI上的评估结果表明，Pix2Code 能够识别组合视觉概念并推广到新数据和推理任务。 |
| [^53] | [Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss](https://arxiv.org/abs/2402.08267) | 通过应用辅助损失优化编码器，我们提出了一种改进的机器图像编码方法，能够在目标检测和语义分割任务中实现显著的速率提高。 |
| [^54] | [Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs](https://arxiv.org/abs/2402.08256) | 本论文提出了一种基于对比学习的框架，用于在MOOCs中平衡显式和隐式关系进行知识概念推荐。通过建立一个MOOCs的异构信息网络(HIN)，可以更好地表示和学习隐式关系，从而提高知识概念推荐的性能并满足用户的个性化需求。 |
| [^55] | [Distal Interference: Exploring the Limits of Model-Based Continual Learning](https://arxiv.org/abs/2402.08255) | 本研究探讨了持续学习中远距离干扰的极限问题，并提出了一种新型的可近似任何连续函数的反对称有界指数层B-spline ANN架构，用以解决灾难性干扰的挑战。 |
| [^56] | [A survey of recent methods for addressing AI fairness and bias in biomedicine](https://arxiv.org/abs/2402.08250) | 这篇论文综述了近期用于解决生物医学中人工智能公平性和偏见的方法。研究指出，在开发AI模型过程中可能存在偏见，因此需要采取措施来识别和解决这些偏见以确保在临床环境中准确可靠地应用AI模型。 |
| [^57] | [Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles](https://arxiv.org/abs/2402.08246) | 该论文提出了一种基于蚁群优化的新方法，用于解决无人机协作路径规划问题。该方法通过利用结构的三维模型生成无人机的视点，并将路径规划转化为旅行推销员问题。实验结果表明，该系统不仅能够生成可行的巡检路径，而且能够减少路径长度。 |
| [^58] | [Towards Equitable Agile Research and Development of AI and Robotics](https://arxiv.org/abs/2402.08242) | 这项研究提出了一种将研发项目管理方法与公平能力结合的框架，以解决机器学习和人工智能方法中存在的偏见和歧视问题。 |
| [^59] | [BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT](https://arxiv.org/abs/2402.08236) | BERT4FCA是一种使用形式概念分析和BERT进行二部图链接预测的新方法，利用FCA提取的最大双向团的丰富信息，提高了链接预测性能。 |
| [^60] | [Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective](https://arxiv.org/abs/2402.08228) | 这项研究从架构的角度全面调查了图的超分布推广，揭示了图自我注意机制和其他常见构建模块在超分布问题上的影响。 |
| [^61] | [BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models](https://arxiv.org/abs/2402.08219) | BBox-Adapter是一种适用于黑盒大型语言模型的轻量级适配器，通过区分目标和源域数据，并采用排名式噪音对比估计（NCE）损失和在线适应机制，实现了在透明、隐私和成本方面的有效适应。 |
| [^62] | [Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks](https://arxiv.org/abs/2402.08211) | 本研究分析了基于Transformer的神经网络在人类工作记忆任务上训练时所出现的机制，揭示了这种模型如何解决复杂的认知分支任务，并探讨了这些机制与人脑封锁机制的相似性。 |
| [^63] | [Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits](https://arxiv.org/abs/2402.08209) | 本文提出了一种利用阈值赌博机算法快速识别具有低数据Shapley值的实例子集的迭代方法，从而提高数据清洗的计算速度，同时保持模型性能。 |
| [^64] | [Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications](https://arxiv.org/abs/2402.08208) | 本文研究了自动驾驶系统中基于人工智能的软件元素的作用和挑战，探讨了泛化问题以及过度自信的AI模型所带来的风险，并提出了解决方法。 |
| [^65] | [PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction](https://arxiv.org/abs/2402.08198) | PSC-CPI是一种多尺度蛋白质序列-结构对比框架，通过内模态和跨模态对比捕获蛋白质序列和结构之间的依赖关系，用于高效且具有可推广性的化合物-蛋白质相互作用预测。 |
| [^66] | [THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation](https://arxiv.org/abs/2402.08191) | THE COLOSSEUM是一个新的模拟基准测试，用于评估机器人操作的泛化性能。它包括20个不同的操作任务，在12个环境干扰轴上进行系统评估。研究发现，四个最先进的操作模型在干扰因素下的成功率下降了30-50%。改变干扰对象数量、目标对象颜色或光照条件会对模型的性能产生重要影响。 |
| [^67] | [Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5](https://arxiv.org/abs/2402.08185) | 本研究介绍了一种新颖的基于数据驱动的天气预报策略，利用低分辨率数据进行全球天气预报和气候数据分析。通过使用自适应傅里叶神经算子（AFNO）模型和时间滑动方法扩充数据集，本研究改进了传统方法，添加了更多变量和新的方法。 |
| [^68] | [Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation](https://arxiv.org/abs/2402.08184) | 本研究引入了一种新的框架，通过将各种状态空间统一为固定大小的输入，实现了在多智能体系统中进行转移学习的能力。在SMAC环境中的实验结果表明，通过学习机动技能获得的知识可以显著提高多智能体的学习性能。 |
| [^69] | [LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents](https://arxiv.org/abs/2402.08178) | LoTa-Bench是一个用于评估具身代理任务规划性能的基准系统，通过对LLMs和提示进行实验，加速语言导向任务规划器的开发。 |
| [^70] | [Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction](https://arxiv.org/abs/2402.08174) | 本论文提出了一种使用地标和聚类的层级位置嵌入方法用于链接预测任务。通过选择具有高度中心度的节点作为地标和进行图聚类，本方法有效地将位置信息嵌入到图中，提高了链接预测的准确性和性能。 |
| [^71] | [LLaGA: Large Language and Graph Assistant](https://arxiv.org/abs/2402.08170) | LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。 |
| [^72] | [On Limitations of the Transformer Architecture](https://arxiv.org/abs/2402.08164) | 本论文通过通信复杂性证明了Transformer层在处理函数组合任务时的局限性，指出对于大型定义域和某些数学任务，Transformers可能无法解决。 |
| [^73] | [CMA-R:Causal Mediation Analysis for Explaining Rumour Detection](https://arxiv.org/abs/2402.08155) | CMA-R通过因果中介分析解释了神经模型在Twitter上进行谣言检测的决策过程，并能够识别出关键推文和因果影响单词，提高了对黑盒子谣言检测系统的解释性和透明度。 |
| [^74] | [Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models](https://arxiv.org/abs/2402.08151) | 本研究引入了渐变流自适应重要性抽样的方法，用于稳定贝叶斯分类模型的留一交叉验证预测的蒙特卡罗近似，以评估模型的普适性。 |
| [^75] | [Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search](https://arxiv.org/abs/2402.08147) | 本文提出了一种使用蒙特卡洛树搜索引导大型语言模型生成验证程序的方法，通过结合验证器的反馈和LLM先验知识提高了合成能力，实验证明这种方法在一组验证编程问题上的性能优于基本模型和具有插件的ChatGPT4。 |
| [^76] | [Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings](https://arxiv.org/abs/2402.08145) | 本文介绍了一种在非稳态环境中进行通用规划和学习的方法，通过认识性探索填补代理的知识空白，并且利用收集到的数据学习通用的概率模型来解决不断变化的任务。实验证明这种方法在非稳态环境中比传统方法更有效。 |
| [^77] | [Average-Case Analysis of Iterative Voting](https://arxiv.org/abs/2402.08144) | 这项工作通过分析代理人偏好分布的平均情况，扩展了迭代投票模型的效果分析。并且区分了迭代多数制何时改善或降低渐近福利。 |
| [^78] | [Recursive Joint Simulation in Games](https://arxiv.org/abs/2402.08128) | 本文研究了游戏中AI代理之间的递归协同模拟的互动方式，并证明了这种方式与原始游戏的无限重复版本在战略上是等价的。 |
| [^79] | [Customizable Perturbation Synthesis for Robust SLAM Benchmarking](https://arxiv.org/abs/2402.08125) | 我们提出了一种可定制化的噪声数据合成流程，用于评估多模态SLAM模型对各种扰动的鲁棒性。该流程结合了可定制化硬件配置、软件组件和被扰动环境，引入了全面的扰动分类法以及扰动组合工具箱，可以将干净的仿真转化为具有挑战性的嘈杂环境。我们还建立了鲁棒-SLAM基准测试。 |
| [^80] | [On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks](https://arxiv.org/abs/2402.08115) | 这项研究对大型语言模型在推理和规划任务中的自我验证能力进行了系统研究，并发现了迭代提示的有效性。 |
| [^81] | [Active Preference Learning for Large Language Models](https://arxiv.org/abs/2402.08114) | 本论文提出了一种用于大型语言模型的主动偏好学习策略，通过直接偏好优化（DPO）来更好地利用偏好标签。实验结果表明，该方法提高了基于成对偏好数据的微调的学习速度和最终性能。 |
| [^82] | [A Competition Winning Deep Reinforcement Learning Agent in microRTS](https://arxiv.org/abs/2402.08112) | 在IEEE microRTS竞赛中，RAISocketAI成为第一个获胜的深度强化学习代理，它通过逐步优化基本策略和迁移学习来击败了前两位竞赛获胜者，在未来的竞赛中可以作为基准参考，并为DRL研究提供起点。 |
| [^83] | [Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control](https://arxiv.org/abs/2402.08088) | 该论文提出了一种使用统计过程控制的机器学习框架，用于检测离群数据和监测数据漂移。该框架在临床环境中具有重要应用价值，能够帮助提高ML设备在放射学图像中的性能和患者安全。 |
| [^84] | [Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning](https://arxiv.org/abs/2402.08085) | "信息绕行"是一种用于层次性表征图中循环的方法，通过比较最短路径和最长路径之间的对比性，实现了与高阶"Weisfeiler-Lehman"（WL）测试相当的表达能力，但计算需求更少。 |
| [^85] | [Efficient and Scalable Fine-Tune of Language Models for Genome Understanding](https://arxiv.org/abs/2402.08075) | 这篇论文提出了一种名为Lingo的高效可扩展的基因理解语言模型微调方法，该方法利用了自然语言模型的上下文提示，并通过自适应秩采样方法适应了基因组注释的多样性任务。 |
| [^86] | [Enhancing Programming Error Messages in Real Time with Generative AI](https://arxiv.org/abs/2402.08072) | 本研究通过使用生成式人工智能改进编程错误信息，实时为学生提供帮助，并发现界面设计对于反馈的可用性起着关键作用。 |
| [^87] | [Beyond LLMs: Advancing the Landscape of Complex Reasoning](https://arxiv.org/abs/2402.08064) | 在人工智能领域，大型语言模型(LLMs)一直被视为解决许多问题的标准解决方案。然而，对于约束满足和优化问题，LLMs表现不佳。因此，Elemental Cognition开发了EC AI平台，采用神经符号方法解决这些问题，同时利用LLMs进行知识获取和用户交互。 |
| [^88] | [Avoiding Catastrophe in Continuous Spaces by Asking for Help](https://arxiv.org/abs/2402.08062) | 在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。 |
| [^89] | [Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking](https://arxiv.org/abs/2402.08030) | 本研究通过实验和访谈调查了大型语言模型(LLM)助手在软件帮助寻求中的有效性。结果显示，虽然优化后的LLM助手相较于基准LLM表现更好，但提示指南和领域上下文的融合与否对于LLM的使用和用户感知没有显著影响。 |
| [^90] | [UGMAE: A Unified Framework for Graph Masked Autoencoders](https://arxiv.org/abs/2402.08023) | UGMAE是一种统一框架，用于解决图形自动编码器中存在的节点重要性、图像信息利用、表示空间中的语义知识和重构稳定性等问题。 |
| [^91] | [Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning](https://arxiv.org/abs/2402.08010) | 本文描述了CNN中卷积瓶颈（CBN）结构的出现，网络在前几层将输入表示转换为在少数频率和通道上受支持的表示，然后通过最后几层映射回输出。CBN秩定义了保留在瓶颈中的频率的数量和类型，并部分证明了参数范数与深度和CBN秩的比例成正比。此外，我们还展示了网络的参数范数依赖于函数的规则性。我们发现任何具有接近最优参数范数的网络都会展示出CBN结构，这解释了下采样的常见实践；我们还验证了CBN结构在下采样下仍然成立。最后，我们使用CBN结构来解释...（摘要完整内容请见正文） |
| [^92] | [SMX: Sequential Monte Carlo Planning for Expert Iteration](https://arxiv.org/abs/2402.07963) | 这项研究介绍了一种名为SMX的顺序蒙特卡洛规划算法，它利用可扩展的方法创建了有效的自我学习机制。它适用于离散和连续动作空间的环境，具有高并行性能。 |
| [^93] | [Educational data mining and learning analytics: An updated survey](https://arxiv.org/abs/2402.07956) | 这项调查综述了教育数据挖掘和学习分析的发展和应用，提供了当前领域的最新研究现状和未来趋势。 |
| [^94] | [Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management](https://arxiv.org/abs/2402.07949) | 通过神经进化算法优化人工胰腺治疗策略，减少糖尿病患者的血糖偏差，并且降低注射次数。 |
| [^95] | [Re-Envisioning Command and Control](https://arxiv.org/abs/2402.07946) | 重新构想的论文提出了未来指挥与控制（C2）决策需要面对更复杂和挑战性的环境，因此提出了基于人工智能系统与人类强有力伙伴关系的未来C2的愿景。这个愿景的核心是优化C2操作流程，保持协同努力，发展自适应的集体知识系统。 |
| [^96] | [ScreenAgent: A Vision Language Model-driven Computer Control Agent](https://arxiv.org/abs/2402.07945) | 本文介绍了一种基于视觉语言模型的计算机控制代理ScreenAgent，该代理可以通过观察屏幕截图和输出鼠标键盘动作与计算机屏幕进行交互，完成多步任务。 |
| [^97] | [LLMs Among Us: Generative AI Participating in Digital Discourse](https://arxiv.org/abs/2402.07940) | LLMs Among Us实验框架通过在社交媒体平台上让机器人和人类互动的方式，研究了大型语言模型在伪装成人类参与者方面的能力，发现尽管存在一定的威胁，但参与者只有42%的时间能正确识别用户的性质。 |
| [^98] | [UFO: A UI-Focused Agent for Windows OS Interaction](https://arxiv.org/abs/2402.07939) | UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。 |
| [^99] | [Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs](https://arxiv.org/abs/2402.07938) | 本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。 |
| [^100] | [Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations](https://arxiv.org/abs/2402.07933) | 本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。 |
| [^101] | [A Human-Machine Collaboration Framework for the Development of Schemas](https://arxiv.org/abs/2402.07932) | 这篇论文提出了一个人机协作的框架用于设计新的模式，目的是解决机器智能中的挑战，并将AI社区的关注从技术转向AI科学。 |
| [^102] | [Abstracted Trajectory Visualization for Explainability in Reinforcement Learning](https://arxiv.org/abs/2402.07928) | 强化学习(RL)模型中使用模糊轨迹可视化，使非RL专家能够推断出RL的行为模式。 |
| [^103] | [A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications](https://arxiv.org/abs/2402.07927) | 这篇调查论文系统概述了大型语言模型中提示工程的最新进展，探讨了提示工程的方法和技术，并说明了其在各种应用中的重要作用。 |
| [^104] | [Point and Instruct: Enabling Precise Image Editing by Unifying Direct Manipulation and Text Instructions](https://arxiv.org/abs/2402.07925) | 点击和指导是一个将直接操作和文本指令结合起来，实现精确图像编辑的系统。 |
| [^105] | [Towards the Human Digital Twin: Definition and Design -- A survey](https://arxiv.org/abs/2402.07922) | 本调查综合了最新的HDT领域进展，提出了第一个跨领域HDT定义和十一个关键设计考虑因素，为未来开发者提供指导。 |
| [^106] | [How Can Generative AI Enhance the Well-being of Blind?](https://arxiv.org/abs/2402.07919) | 生成式人工智能通过分析图像等方式，可以改善盲人和视力受损人士的福祉，创造新的独立性和感知，并对其生活产生根本性的影响。 |
| [^107] | [QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners](https://arxiv.org/abs/2402.07913) | 为解决编程智能教育系统中数据稀缺问题，本文提出了一个新的针对Python学习者的中文问答数据集，通过收集与分类真实学生问题，提高在线编程教育的效果和质量。 |
| [^108] | [Spatial Computing: Concept, Applications, Challenges and Future Directions](https://arxiv.org/abs/2402.07912) | 空间计算是一种技术进步，将设备无缝集成到物理环境中，改善了数字世界的用户体验。它具有广泛的应用，并且在研究者和工业组织中的重要性不断增加。 |
| [^109] | [Does mapping elites illuminate search spaces? A large-scale user study of MAP--Elites applied to human--AI collaborative design](https://arxiv.org/abs/2402.07911) | 通过两项研究，揭示了MAP-Elites在人机协同设计中对搜索空间的作用。这些研究使用了基于进化算法的设计工具，以实现设计推荐对设计过程的影响的理解。 |
| [^110] | [Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization](https://arxiv.org/abs/2402.07909) | 提出了 Prompt4Vis，使用示例挖掘和结构过滤来为表格数据可视化的大语言模型提供提示。这种方法利用了巨大的语言模型的优势，并能够改进当前自然语言查询转换成数据可视化查询的方法。 |
| [^111] | [Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2402.07787) | 这篇论文提出了一种可扩展的多粒度融合网络（EMGF）用于基于方面的情感分析，通过整合不同的语言和结构特征，包括句法依赖、组成、注意力语义和外部知识图谱等，来提高情感分析的性能和准确性。 |
| [^112] | [A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?](https://arxiv.org/abs/2402.07462) | 我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。 |
| [^113] | [On the Transit Obfuscation Problem](https://arxiv.org/abs/2402.07420) | 本文研究了转运混淆问题，提出了转运匿名性的概念，并提出并评估了满足该匿名性准则的规划/搜索算法。 |
| [^114] | [How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?](https://arxiv.org/abs/2402.07282) | 本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。 |
| [^115] | [GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks](https://arxiv.org/abs/2402.07197) | "GraphTranslator"是一个旨在将预训练的图模型和大型语言模型对齐的翻译器，可以同时处理预定义任务和开放式任务。通过将这两种模型结合起来，能够有效地处理各种任务，并实现更具创新性和灵活性的应用。 |
| [^116] | [Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning](https://arxiv.org/abs/2402.07107) | 这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。 |
| [^117] | [Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities](https://arxiv.org/abs/2402.06938) | 这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。 |
| [^118] | [Discriminative Adversarial Unlearning](https://arxiv.org/abs/2402.06864) | 该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。 |
| [^119] | [Learn To be Efficient: Build Structured Sparsity in Large Language Models](https://arxiv.org/abs/2402.06126) | 本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。 |
| [^120] | [Limits of Transformer Language Models on Algorithmic Learning](https://arxiv.org/abs/2402.05785) | Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。 |
| [^121] | [Improving Token-Based World Models with Parallel Observation Prediction](https://arxiv.org/abs/2402.05643) | 该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。 |
| [^122] | [Densely Multiplied Physics Informed Neural Network](https://arxiv.org/abs/2402.04390) | 该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。 |
| [^123] | [Logical recognition method for solving the problem of identification in the Internet of Things](https://arxiv.org/abs/2402.04338) | 这项工作的目标是开发一种逻辑识别方法，通过构建逻辑函数的最优扩展，在整个特征空间上实现逻辑连接，以解决物联网中的识别问题。 |
| [^124] | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) | 本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。 |
| [^125] | [C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.03181) | C-RAG是第一个用于认证检索增强语言模型生成风险的框架，通过提供符合风险分析和生成风险的上界，确保生成结果的可信性。 |
| [^126] | [Vision-Language Models Provide Promptable Representations for Reinforcement Learning](https://arxiv.org/abs/2402.02651) | 本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。 |
| [^127] | [Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory](https://arxiv.org/abs/2402.00060) | 本文提出了一种基于Dempster-Shafer理论的方法，用于在共同数据信息中建模认知不确定性和根据置信度对共同事件进行分类。通过构建概率箱和DSt结构，可以计算特定碰撞概率的置信度和可能性。 |
| [^128] | [SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning](https://arxiv.org/abs/2401.16013) | 这个论文介绍了SERL软件套件，它是一个用于样本高效的机器人强化学习的库。该库包含了一个离线深度强化学习方法、计算奖励和重置环境的方法，高质量的机器人控制器，以及一些具有挑战性的示例任务。这个软件套件的目标是解决机器人强化学习的难以使用和获取性的挑战。 |
| [^129] | [Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?](https://arxiv.org/abs/2401.11911) | 该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。 |
| [^130] | [LLMLight: Large Language Models as Traffic Signal Control Agents](https://arxiv.org/abs/2312.16044) | LLMLight是一个采用大型语言模型作为交通信号控制代理的新框架，通过借助先进的泛化能力和类似人类直觉的推理和决策过程，实现了有效的交通控制。此外，通过构建专为TSC任务定制的骨干语言模型LightGPT，进一步提升了LLMLight的效果和性能。 |
| [^131] | [PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs](https://arxiv.org/abs/2312.15230) | 本研究中，通过仅更新少部分高度表达力的参数，我们挑战了全参数重新训练的做法，在修剪后恢复或甚至提升了性能。PERP方法显著减少了计算量和存储需求。 |
| [^132] | [Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools](https://arxiv.org/abs/2312.10622) | 本研究通过比较分析，实验性地研究了使用生成式人工智能（如ChatGPT）生成Python程序的单元测试脚本的有效性。结果显示，ChatGPT在覆盖率方面与现有的单元测试生成器Pynguin相当，在某些情况下性能优于Pynguin。然而，对于某些类别，ChatGPT生成的断言约有三分之一是不正确的。 |
| [^133] | [Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales](https://arxiv.org/abs/2312.07399) | 该论文提出了一种基于提示生成的理由的“推理感知”诊断框架，通过大型语言模型来进行临床推理，实现了在疾病诊断过程中的高效、时间节约和劳动节约的方法。 |
| [^134] | [Computational Copyright: Towards A Royalty Model for Music Generative AI](https://arxiv.org/abs/2312.06646) | 本文旨在解决音乐生成AI领域中的版权问题，提出了一种用于AI音乐生成平台的版税模型，并探讨了对AI生成音乐进行版权归因的算法解决方案。 |
| [^135] | [Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum](https://arxiv.org/abs/2312.06441) | 本文提出了一种基于半监督GNN的欺诈检测器SEC-GFD，通过混合过滤模块和局部环境约束模块解决了异质性和标签利用问题。 |
| [^136] | [What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes](https://arxiv.org/abs/2312.03096) | 这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。 |
| [^137] | [Multi-Step Dialogue Workflow Action Prediction](https://arxiv.org/abs/2311.09593) | 本文提出了多步骤工作流动作预测的新问题，通过准确预测多个步骤，实现对任务的多轮自动化，节省时间。提出了三种简单易行的建模方法，并展示了多步骤动作预测提高对话任务准确性和步骤自动化的特征。 |
| [^138] | [Omitted Labels in Causality: A Study of Paradoxes](https://arxiv.org/abs/2311.06840) | 本研究探讨了所谓的“遗漏标签上下文”，即训练数据仅限于可能标签的一个子集，并利用悖论展示了在这种上下文中因果推断面临的困难。研究发现在某些情况下，必须使用非可交换的处理组和对照组进行正确的校正。此外，研究还发现了结论网络与社会选择理论之间的有趣联系。 |
| [^139] | [PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks](https://arxiv.org/abs/2311.03415) | PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。 |
| [^140] | [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://arxiv.org/abs/2310.19791) | LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。 |
| [^141] | [Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)](https://arxiv.org/abs/2310.09877) | 本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。 |
| [^142] | [SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning](https://arxiv.org/abs/2310.04918) | 本研究提出了一种名为SWAP的网络剪枝方法，采用稀疏熵式Wasserstein回归来解决神经网络剪枝中的梯度不准确问题。SWAP在噪声抑制和协方差信息保留之间取得了平衡，具有较小的计算成本，与最先进的网络剪枝算法具有可比较的性能。 |
| [^143] | [Customized Load Profiles Synthesis for Electricity Customers Based on Conditional Diffusion Models](https://arxiv.org/abs/2304.12076) | 本文提出了一种基于条件扩散模型的定制负荷曲线合成方法，用于解决电力客户的数据短缺问题，并实现高质量负荷曲线的合成。 |
| [^144] | [Selective Uncertainty Propagation in Offline RL](https://arxiv.org/abs/2302.00284) | 本论文提出了一种名为选择性不确定性传播的方法，用于解决离线强化学习中的分布偏移问题。该方法通过自适应的方式建立置信区间，有效地处理了实际问题中策略学习的挑战。 |
| [^145] | [Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks.](http://arxiv.org/abs/2401.15170) | 本研究证明了大型语言模型在定性编码中的应用潜力。相比于GPT-3.5，GPT-4能够实现与人类相当的解释能力，并具有较高的编码一致性。无论模型规模大小，只要满足一定条件，模型都可以实现较高的编码准确性。 |
| [^146] | [PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models.](http://arxiv.org/abs/2401.15042) | PROXYQA是一个用于评估大型语言模型长篇文本生成的替代框架，通过生成详尽的内容，并利用评估器和生成内容作为背景环境，根据评估器回答代理问题的表现来评估生成内容的质量。 |
| [^147] | [An open dataset for the evolution of oracle bone characters: EVOBC.](http://arxiv.org/abs/2401.12467) | 本研究收集了古代字符数据集，揭示了甲骨文字符在六个历史阶段的演变过程，为解读甲骨文铭文提供了有价值的资源。 |
| [^148] | [Emergent Dominance Hierarchies in Reinforcement Learning Agents.](http://arxiv.org/abs/2401.12258) | 本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。 |
| [^149] | [Multilingual Instruction Tuning With Just a Pinch of Multilinguality.](http://arxiv.org/abs/2401.01854) | 本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。 |
| [^150] | [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models.](http://arxiv.org/abs/2401.01335) | 本文提出了一种名为自我对弱语言模型进行细调（SPIN）的方法，通过模型自我对弈生成训练数据，并从中优化模型策略，从而将弱语言模型转化为强语言模型，无需额外的人类标注数据。 |
| [^151] | [LLbezpeky: Leveraging Large Language Models for Vulnerability Detection.](http://arxiv.org/abs/2401.01269) | LLbezpeky是一项利用大型语言模型进行漏洞检测的研究，研究发现LLMs在理解人类和编程语言中的语义方面展现出巨大潜力，并通过构建一个AI驱动的工作流程来帮助开发人员识别和修复漏洞。 |
| [^152] | [Controlled Decoding from Language Models.](http://arxiv.org/abs/2310.17022) | 本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。 |
| [^153] | [Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery.](http://arxiv.org/abs/2310.13576) | 本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。 |
| [^154] | [Instruction Tuning with Human Curriculum.](http://arxiv.org/abs/2310.09518) | 本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。 |
| [^155] | [CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation.](http://arxiv.org/abs/2310.09401) | CIDER是一种基于类别引导的个性化新闻推荐框架，通过意图分离和一致性的新闻表示来准确理解新闻文章的多个意图，并区分用户不同的后阅读偏好。 |
| [^156] | [Ranking LLM-Generated Loop Invariants for Program Verification.](http://arxiv.org/abs/2310.09342) | 本研究提出了一种针对LLM生成结果进行重新排名的方法，可以显著提高正确不变量的排名，从而减少程序验证的调用次数。 |
| [^157] | [A Refutation of Shapley Values for Explainability.](http://arxiv.org/abs/2309.03041) | 这篇论文驳斥了Shapley Values在规则解释中的适用性，并证明了存在布尔函数，使得Shapley值给出的特征重要性信息具有误导性。该论文提供了一种蛮力方法来识别这种问题，但对于特征数量较大的布尔函数仍存在问题。 |
| [^158] | [Les Houches Lectures on Deep Learning at Large & Infinite Width.](http://arxiv.org/abs/2309.01592) | 本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。 |
| [^159] | [SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies.](http://arxiv.org/abs/2308.12367) | 本文提出了一种更安全的算法补救方法（SafeAR），该方法通过考虑风险因素在计算和评估补救措施时，为那些受到机器学习模型决策不利影响的个体提供更可靠的建议。 |
| [^160] | [LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning.](http://arxiv.org/abs/2308.01413) | LaFiCMIL是一个新的方法，从相关多实例学习的角度解决了Transformer模型输入长度限制的问题，可以用于改进大文件分类任务。 |
| [^161] | [Nonlinear Processing with Linear Optics.](http://arxiv.org/abs/2307.08533) | 该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。 |
| [^162] | [Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey.](http://arxiv.org/abs/2306.06123) | 本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。 |
| [^163] | [The Scope of ChatGPT in Software Engineering: A Thorough Investigation.](http://arxiv.org/abs/2305.12138) | ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。 |
| [^164] | [IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers.](http://arxiv.org/abs/2305.06741) | 本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。 |
| [^165] | [Making Batch Normalization Great in Federated Deep Learning.](http://arxiv.org/abs/2303.06530) | 本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。 |
| [^166] | [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning.](http://arxiv.org/abs/2301.11916) | 本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。 |

# 详细

[^1]: 弱监督学习器实现具有可证明性能保证的AI错误修正

    Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees

    [https://rss.arxiv.org/abs/2402.00899](https://rss.arxiv.org/abs/2402.00899)

    这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。

    

    我们提出了一种新的方法来处理AI错误，通过引入具有先验性能保证的弱监督AI错误修正器。这些AI修正器是辅助映射，其作用是通过批准或拒绝以调节之前构建的底层分类器的决策。拒绝一个决策可以用作建议放弃做出决策的信号。该工作的一个关键技术重点是通过对错误决策的概率界限提供这些新的AI修正器的性能保证。这些界限是分布不可知的，并且不依赖于对数据维度的假设。我们的实证示例说明了该框架如何应用于改善在训练数据稀缺的具有挑战性的真实世界任务中图像分类器的性能。

    We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
    
[^2]: IM-3D：用于高质量3D生成的迭代多视角扩散和重构

    IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation

    [https://arxiv.org/abs/2402.08682](https://arxiv.org/abs/2402.08682)

    本文提出了一种名为IM-3D的方法，通过考虑视频生成器和使用高斯平铺的3D重构算法，从生成的视图直接输出高质量的3D结果，实现了更高效的流水线、更好的质量和更高的可用3D资产产出。

    

    大多数文本到3D生成器都是基于已训练过的数十亿图像的文本到图像模型构建的。它们使用Score Distillation Sampling (SDS)的变体，这种方法较慢、不太稳定且容易产生伪影。一种缓解方法是将2D生成器微调为多视角感知，可以帮助消除伪影，或者与重构网络结合，直接输出3D对象。在本文中，我们进一步探索了文本到3D模型的设计空间。通过考虑视频而非图像生成器，我们显著改善了多视角生成。结合一种使用高斯平铺的3D重构算法，可以优化鲁棒的基于图像的损失，我们直接从生成的视图输出高质量的3D结果。我们的新方法IM-3D将2D生成器网络的计算次数减少了10-100倍，从而实现了更高效的流水线、更好的质量、更少的几何不一致性和更高的可用3D资产产出。

    Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.
    
[^3]: 通过无分类器引导来减轻大型视觉语言模型中的物体幻觉

    Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance

    [https://arxiv.org/abs/2402.08680](https://arxiv.org/abs/2402.08680)

    本文介绍了一种名为MARINE的框架，用于通过无分类器引导来减少大型视觉语言模型的物体幻觉。该框架无需训练或API访问，并通过集成视觉模型和引入额外的物体基础特征来提高模型的生成精确性和效率。

    

    大型视觉语言模型（LVLM）的进展越来越突出了它们在图像中产生虚假物体的严重问题。为了解决这个问题，先前的研究着重于使用特殊策划的数据集或强大的LLM（例如GPT-3.5）来纠正LVLM的输出。然而，这些方法要求昂贵的训练/微调或API访问先进的LLM来在生成后纠正模型的输出。在本文中，我们通过引入一个名为通过无分类器引导缓解幻觉的框架（MARINE）来解决这个挑战，该框架既无需训练也无需API访问，可以在生成过程中有效地减少物体幻觉。具体而言，MARINE通过集成现有的开源视觉模型丰富LVLM的视觉语境，并使用无分类器引导来整合额外的物体基础特征，以提高LVLM生成的精确性。

    The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
    
[^4]: COLD-Attack: 用于具有隐秘性和可控性的LLM越狱

    COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability

    [https://arxiv.org/abs/2402.08679](https://arxiv.org/abs/2402.08679)

    本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。

    

    最近对大型语言模型（LLMs）进行越狱的注意力越来越多。为了全面评估LLM的安全性，有必要考虑具有不同属性的越狱，例如上下文连贯性以及情感/风格变化，因此研究可控性越狱是有益的，即如何对LLM攻击进行控制。在本文中，我们正式形式化了可控性攻击生成问题，并建立了该问题与可控文本生成之间的新型关联，这是自然语言处理中一个被广泛探索的主题。基于这种关联，我们改进了能量限制解码与Langevin动力学（COLD）的算法，这是一种在可控文本生成中的高效算法，并引入了COLD-Attack框架，该框架统一且自动化地搜索各种控制要求下的对抗性LLM攻击，例如流畅性、隐秘性、情感和左右连贯性。

    Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
    
[^5]: 模型评估与选择在时间分布转移下的研究

    Model Assessment and Selection under Temporal Distribution Shift

    [https://arxiv.org/abs/2402.08672](https://arxiv.org/abs/2402.08672)

    本文研究了在变化环境中的模型评估与选择问题，通过合成不同时期的数据集，并开发了自适应滚动窗口方法来估计模型的泛化误差以及比较不同模型之间的差异。实验证明了我们提出的方法在非稳态数据中的适应性。

    

    我们通过合成当前时期和历史时期的数据集，研究了在变化环境中的模型评估与选择。为了解决未知和可能任意的时间分布转移，我们开发了一种自适应滚动窗口方法来估计给定模型的泛化误差。这种策略还通过估计两个候选模型之间的泛化误差差异来方便比较。我们进一步将两两比较整合到单场淘汰赛中，从候选模型集合中实现了近乎最优的模型选择。理论分析和数值实验证明了我们所提出方法对数据非稳态的适应性。

    We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
    
[^6]: 半稠密无检测器方法在匹配局部特征方面表现如何？

    Are Semi-Dense Detector-Free Methods Good at Matching Local Features?

    [https://arxiv.org/abs/2402.08671](https://arxiv.org/abs/2402.08671)

    本研究首次尝试研究半稠密无检测器方法（SDF）建立对应关系能力和估计位姿质量之间的联系。作者提出了一种新颖的图像匹配架构SAM，并发现SAM在位姿估计方面表现优秀，而SDF方法在匹配准确度方面表现更好。作者建议将匹配准确度的计算限制在纹理区域。

    

    半稠密无检测器方法（SDF），如LoFTR，目前是最受欢迎的图像匹配方法之一。虽然SDF方法被训练用于在两幅图像之间建立对应关系，但它们的性能几乎只使用相对位姿估计指标进行评估。因此，迄今为止，它们在建立对应关系的能力和估计位姿质量之间的联系得到的关注甚少。本文首次尝试研究这种联系。我们首先提出了一种新颖的基于结构化注意力的图像匹配架构（SAM）。它使我们能够在两个数据集（MegaDepth和HPatches）上展示一个逆直觉的结果：一方面，SAM在位姿/单应性估计指标方面要么优于SDF方法，要么与之相当；另一方面，SDF方法在匹配准确度方面明显优于SAM。然后，我们建议将匹配准确度的计算限制在纹理区域，并展示了在...

    Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in
    
[^7]: Rec-GPT4V: 基于大规模视觉语言模型的多模态推荐

    Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models

    [https://arxiv.org/abs/2402.08670](https://arxiv.org/abs/2402.08670)

    Rec-GPT4V是一种基于大规模视觉语言模型的多模态推荐算法，通过利用用户历史作为用户偏好信息并结合图像摘要和项目标题，实现了对多个图像动态的推荐。

    

    大规模视觉语言模型（LVLM）的发展为传统多模态推荐的挑战提供了解决方案，因为它们能够熟练理解静态图像和文本动态。然而，LVLM在该领域的应用仍然受到以下复杂性的限制：首先，由于LVLM是从大量通用数据集中训练得到的，因此缺乏用户偏好知识。其次，在涉及离散、嘈杂和冗余图像序列的情景中，LVLM在处理多个图像动态方面存在困难。为了克服这些问题，我们提出了一种名为Rec-GPT4V的新颖推理方案：利用大规模视觉语言模型进行多模态推荐的视觉摘要思维（VST）。我们利用用户历史作为上下文中的用户偏好来解决第一个挑战。接下来，我们促使LVLM生成项目图像摘要，并利用自然语言空间中的图像理解和项目标题来查询用户偏好。

    The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics. However, the application of LVLMs in this field is still limited due to the following complexities: First, LVLMs lack user preference knowledge as they are trained from vast general datasets. Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences. To overcome these issues, we propose the novel reasoning scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation. We utilize user history as in-context user preferences to address the first challenge. Next, we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences ov
    
[^8]: 最后的JITAI？大型语言模型在发放及时自适应干预中的不合理有效性：在前瞻性心脏康复环境中促进体育活动

    The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting

    [https://arxiv.org/abs/2402.08658](https://arxiv.org/abs/2402.08658)

    本研究探索了使用大型语言模型（LLMs）实现即时自适应干预（JITAIs）的可行性。通过测试GPT-4模型以促进门诊心脏康复中心的心脏健康体育活动的使用案例，我们提出了450个JITAI决策和信息。

    

    我们探索了大型语言模型（LLMs）在数字健康中触发和个性化即时自适应干预（JITAIs）内容的可行性。JITAIs被视为可持续行为改变的关键机制，将干预措施根据个体的当前情境和需求进行调整。然而，传统的基于规则和机器学习模型在JITAI实施中面临可扩展性和可靠性的限制，例如缺乏个性化、管理多参数系统困难以及数据稀疏性等问题。为了研究通过LLMs实现JITAI，我们使用基于在门诊心脏康复中促进心脏健康体育活动的使用案例的现代最高性能模型“GPT-4”的实例作为触发和个性化JITAIs的基础。随后，我们生成了总共450个建议的JITAI决策和信息。

    We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message c
    
[^9]: SAGMAN: 用于图神经网络在流形上的稳定性分析的方法

    SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds

    [https://arxiv.org/abs/2402.08653](https://arxiv.org/abs/2402.08653)

    SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。

    

    现代图神经网络（GNN）对输入图结构和节点特征的变化敏感，可能导致不可预测的行为和性能下降。本文引入了一种称为SAGMAN的谱框架，用于检验GNN的稳定性。该框架评估非线性映射中GNN在输入和输出流形之间引起的距离失真: 当输入流行中两个附近的节点（通过GNN模型）被映射到输出流行上的两个远离的节点时，意味着存在较大的距离失真，从而导致GNN的稳定性较差。我们提出了一种距离保持的图降维（GDR）方法，利用谱图嵌入和概率图模型（PGMs）来创建低维的输入/输出基于图的流形，以进行有意义的稳定性分析。我们的实证评估表明，SAGMAN能够有效评估每个节点在面对不同边缘或特征扰动时的稳定性。

    Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
    
[^10]: 为量子分类器生成通用对抗扰动

    Generating Universal Adversarial Perturbations for Quantum Classifiers

    [https://arxiv.org/abs/2402.08648](https://arxiv.org/abs/2402.08648)

    这项工作引入了QuGAP-一个用于生成量子分类器的通用对抗扰动的新框架，并通过实验证明了量子分类器容易受到这些攻击。

    

    量子机器学习（QML）作为一个有前景的研究领域已经出现，旨在利用量子计算的能力来增强现有的机器学习方法。最近的研究发现，像经典的模型一样，基于参数化量子电路（PQC）的QML模型也容易受到对抗性攻击。此外，在量子领域，理论上已经证明了存在通用对抗扰动（UAP），并且已在量子分类器的背景下进行了实证研究。在这项工作中，我们引入了QuGAP：一个为量子分类器生成UAP的新框架。我们对基于PQC的分类器的加性UAP的概念进行了理论上的证明，并通过概率模型（QuGAP-A）来制造加性UAP，并实验证明了量子分类器容易受到这类攻击。此外，我们提出了一种使用量子生成模型和压缩方法生成幺正UAP的新方法（QuGAP-U），并且进行了实验验证。

    Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a 
    
[^11]: 从数据推理抽象的统一解释

    Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data

    [https://arxiv.org/abs/2402.08646](https://arxiv.org/abs/2402.08646)

    本研究提出了一个从数据中对各种类型的符号推理进行概率化描述的统一解释，并使用经典的推理关系、经验上的推理关系、最大一致集、最大可能集和最大似然估计来实现。这个理论为实现人类类似的机器智能提供了新的见解。

    

    受到神经科学对贝叶斯方法在大脑功能方面的实证研究的启发，我们提供了一个统一的概率化解释，用于从数据中对各种类型的符号推理进行描述。我们使用经典的推理关系、经验上的推理关系、最大一致集、最大可能集和最大似然估计来对它们进行描述。这个理论为实现人类类似的机器智能提供了新的见解。

    Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.
    
[^12]: 用于推断高效LLMs的串联Transformer

    Tandem Transformers for Inference Efficient LLMs

    [https://arxiv.org/abs/2402.08644](https://arxiv.org/abs/2402.08644)

    该论文提出了一种新的架构，称为串联Transformer，用于解决传统大型语言模型推断速度限制的问题。该架构通过将小型自回归模型和大模型以块模式结合起来，并让小模型关注大模型的丰富表示，从而显著提高了小模型的预测准确性。实验证明，在预训练数据集上，串联的PaLM2-Bison和PaLM2-Gecko相比独立的PaLM2-Gecko，在下一个词元预测准确性上提高了3.3%，并且相较于具有相似下游任务的PaLM2-Otter模型，加速比达到1.16倍。

    

    传统的大型语言模型( LLMs )具有自回归的特性，这使得推断速度受到限制，因为词元是按顺序生成的。尽管有些预测和并行解码技术试图减轻这个问题，但它们都有限制：要么依赖更精简但准确度较低的模型进行生成，要么没有充分利用基础LLM的表示。我们提出了一种新颖的架构，即串联Transformer，来解决这些问题。这种架构独特地结合了(1)一个小型自回归模型和(2)一个以块模式运行的大模型(同时处理多个词元)。通过让小模型关注大模型更丰富的表示，大幅提升小模型的预测准确性。在PaLM2预训练数据集上，PaLM2-Bison和PaLM2-Gecko的串联相较独立的PaLM2-Gecko，在下一个词元预测准确性上提升了3.3%，与具有相似下游任务的PaLM2-Otter模型相比，提供了1.16倍的加速比。

    The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
    
[^13]: 通过机器学习在不断演化的知识图谱上预测高影响力的研究主题

    Forecasting high-impact research topics via machine learning on evolving knowledge graphs

    [https://arxiv.org/abs/2402.08640](https://arxiv.org/abs/2402.08640)

    通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。

    

    科学出版物的指数增长对人类研究者构成了严峻挑战。它迫使研究者将注意力集中在更狭窄的子领域上，使得发现其他领域的新颖且有影响力的研究想法和合作变得困难。虽然有办法预测科学论文未来的引用次数，但通常需要等到研究完成并且论文写成后才能进行评估，这样就错过了想法构思的早期阶段。在本文中，我们展示了如何预测从未被研究者发布的想法的影响力。为此，我们开发了一个大型的演化知识图谱，其中包含超过2100万篇科学论文。它结合了从论文内容中创建的语义网络和从历史引用中创建的影响网络。利用机器学习，我们可以高准确度地预测演化网络的动态情况，从而预测新的研究方向的影响力。我们预期这种能力将有助于研究者发现具有高影响力的研究主题。

    The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
    
[^14]: 在黑盒大型语言模型上进行知识编辑

    Knowledge Editing on Black-box Large Language Models

    [https://arxiv.org/abs/2402.08631](https://arxiv.org/abs/2402.08631)

    这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。

    

    知识编辑旨在高效、精确地修改大型语言模型的行为，以更新特定的知识，而不对其他知识产生负面影响。当前的研究主要集中在白盒语言模型编辑上，忽视了一个重要的场景：黑盒语言模型编辑，即通过接口访问语言模型，并仅可用文本输出。为了解决现有评估在黑盒语言模型编辑上不适用且缺乏全面性的局限性，我们提出了一种多角度评估框架，首次将风格保留的评估纳入其中。为了解决当前方法中的编辑数据隐私泄漏和风格过度编辑的问题，我们引入了一种新的postEdit框架，通过下游后处理解决隐私问题，并通过对原始回答进行细粒度编辑来保持文本风格一致性。在两个基准测试上的实验与分析表明，postEdit的性能超过了所有现有方法。

    Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
    
[^15]: 面向高度不平衡工业数据的成本敏感变压器模型的预测性能

    A Cost-Sensitive Transformer Model for Prognostics Under Highly Imbalanced Industrial Data

    [https://arxiv.org/abs/2402.08611](https://arxiv.org/abs/2402.08611)

    本文介绍了一种面向高度不平衡工业数据的成本敏感变压器模型，该模型在故障检测和预测方面表现出了显著的性能提升，并通过剔除实验分析了不同组件的贡献。

    

    数据驱动模型在工业领域的快速增长得益于传感器技术的普及，使得大量数据的收集成为可能。然而，在故障检测和预测方面利用这些模型面临着诸多挑战，包括缺失值和类别不平衡等问题。此外，工业运营中的成本敏感性进一步增加了在这个背景下应用传统模型的复杂性。本文介绍了一种新颖的成本敏感变压器模型，它是作为一个系统工作流的一部分开发的，还整合了混合重采样器和基于回归的插补器。通过对来自Scania卡车的APS故障数据集和SECOM数据集的严格测试，我们观察到与最先进方法相比性能有了显著提升。此外，我们进行了剔除实验来分析我们提出的方法中不同组件的贡献。

    The rapid influx of data-driven models into the industrial sector has been facilitated by the proliferation of sensor technology, enabling the collection of vast quantities of data. However, leveraging these models for failure detection and prognosis poses significant challenges, including issues like missing values and class imbalances. Moreover, the cost sensitivity associated with industrial operations further complicates the application of conventional models in this context. This paper introduces a novel cost-sensitive transformer model developed as part of a systematic workflow, which also integrates a hybrid resampler and a regression-based imputer. After subjecting our approach to rigorous testing using the APS failure dataset from Scania trucks and the SECOM dataset, we observed a substantial enhancement in performance compared to state-of-the-art methods. Moreover, we conduct an ablation study to analyze the contributions of different components in our proposed method. Our fi
    
[^16]: 专家组合解锁深度强化学习的参数缩放

    Mixtures of Experts Unlock Parameter Scaling for Deep RL

    [https://arxiv.org/abs/2402.08609](https://arxiv.org/abs/2402.08609)

    本文证明了将专家组合模块融入基于值的网络中，尤其是软MoE，可以实现更具参数可扩展性的深度强化学习模型，这提供了强有力的实证证据以发展强化学习的缩放定律。

    

    最近对（自我）监督学习模型的快速进展很大程度上是通过实证缩放定律预测的：模型的性能与其规模成比例。然而，在强化学习领域中，寻找类似的缩放定律仍然困难，因为增加模型的参数数量往往会损害其最终性能。在本文中，我们证明将专家组合（MoE）模块，特别是软MoE（Puigcerver等人，2023年），融入基于值的网络中，可以得到更具参数可扩展性的模型，通过各种训练方案和模型规模的显著性能提升加以证明。因此，这项工作为发展强化学习的缩放定律提供了有力的实证证据。

    The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
    
[^17]: 图特征预处理器：实时从交易图中提取基于子图的特征

    Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs

    [https://arxiv.org/abs/2402.08593](https://arxiv.org/abs/2402.08593)

    本文介绍了一种名为"图特征预处理器"的软件库，可以从实时交易图中检测典型的洗钱和欺诈模式，并生成丰富的交易特征，从而显著提高机器学习模型的预测准确率。

    

    在本文中，我们提出了一种名为"图特征预处理器"的软件库，用于实时检测金融交易图中的典型洗钱和欺诈模式。这些模式被用于生成丰富的交易特征，用于下游的机器学习训练和推断任务，如洗钱检测。我们展示了我们丰富的交易特征如何显著提高基于梯度提升的机器学习模型的预测准确率。我们的库利用多核并行性，维护一个动态的内存图，并高效地挖掘传入交易流中的子图模式，使其能够以流的方式操作。我们使用高度不平衡的合成反洗钱（AML）和真实的以太坊钓鱼数据集对我们的库进行评估。在这些数据集中，非法交易的比例非常小，使得学习过程具有挑战性。我们的解决方案结合了我们的图特征预处理器和...

    In this paper, we present "Graph Feature Preprocessor", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Prep
    
[^18]: FESS Loss：用于优化医学图像分析的增强特征空间分割损失

    FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis

    [https://arxiv.org/abs/2402.08582](https://arxiv.org/abs/2402.08582)

    本论文提出了一种名为FESS Loss的增强特征空间分割损失，将对比学习和Dice损失相结合，旨在在医学图像分割中提高空间精度和特征表示，从而实现更精确、更精细的分割过程。

    

    在医学成像领域，医学图像分割是一个关键的过程，对于诊断、治疗和研究起着重要作用。它涉及将图像划分为多个区域，代表不同的解剖结构或病理结构。传统方法往往面临空间精度和全面特征表示之间平衡的挑战，因为它们依赖于传统的损失函数。为了克服这一问题，我们提出了增强特征空间分割损失（FESS Loss），将对比学习的优势（在医学成像的微妙领域中提取复杂的特征）与Dice损失的空间准确性相结合。目标是在医学图像分割中增强空间精度和基于特征的表示。FESS Loss代表了一个显著的进展，提供了更精确、更精细的分割过程，最终提高了精度。

    Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research. It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures. Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions. To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss. The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images. FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision i
    
[^19]: FedLPS: 多任务异构联邦学习中的本地参数共享

    FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing

    [https://arxiv.org/abs/2402.08578](https://arxiv.org/abs/2402.08578)

    FedLPS提出了一种在边缘计算环境中处理边缘设备生成的数据的多任务异构联邦学习方法，通过本地参数共享和迁移学习的原理来减少资源消耗和提高部署效率。

    

    联邦学习（FL）已成为边缘计算环境中处理边缘设备生成的大量数据的一种有希望的解决方案。通过在分布式边缘设备上共同优化全局机器学习模型，FL避免了传输原始数据的需求，并增强了用户隐私保护。尽管实际上取得了成功，但FL仍然面临重大挑战，包括有限的边缘设备资源、多任务部署和数据异构性。然而，现有研究侧重于减少每个单独任务的FL训练成本，忽略了在异构FL场景中多个任务之间的资源消耗。在本文中，我们提出了一种称为具有本地参数共享的异构联邦学习（FedLPS）的方法来填补这一空白。FedLPS利用迁移学习的原理，在单个设备上实现多任务部署，通过将本地模型划分为可共享的编码器和任务特定的编码器。

    Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To f
    
[^20]: 在机器人学中的在线基础模型选择

    Online Foundation Model Selection in Robotics

    [https://arxiv.org/abs/2402.08570](https://arxiv.org/abs/2402.08570)

    这篇论文介绍了在机器人学中的在线基础模型选择问题，针对采集闭源模型大量训练数据的高成本，提出了一种以用户为中心的在线模型选择解决方案，该方案结合了开源编码器和在线学习算法，通过提取上下文特征来实现模型选择。

    

    在在计算机视觉和自然语言处理中表现出色后，基础模型最近扩展到了机器人学领域。这些模型可以通过两种方式获得：开源或付费的闭源选项。用户同时有两种选择时，他们会面临一个问题，即在有效但昂贵的闭源模型和免费但功能较弱的开源模型之间做出决策。我们称之为模型选择问题。由于从闭源模型中收集大量训练数据的成本较高，现有的监督学习方法是不实际的。因此，我们将重点放在在线学习设置上，其中算法在收集数据的同时学习，消除了需要大量预先收集的数据集的需求。因此，我们提出了一个以用户为中心的在线模型选择问题，并提出了一种新颖的解决方案，该解决方案将开源编码器与处理该上下文的在线学习算法相结合。编码器将大量的数据分布提炼成低维特征，即上下文。

    Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder distills vast data distributions into low-dimensional features, i.e., the context, with
    
[^21]: 文献综述中的人工智能：机遇与挑战

    Artificial Intelligence for Literature Reviews: Opportunities and Challenges

    [https://arxiv.org/abs/2402.08565](https://arxiv.org/abs/2402.08565)

    这篇论文综述了人工智能在系统文献综述中的应用，尤其关注了在筛选和提取阶段的半自动化过程。该研究使用一个包括传统特征和人工智能特征的框架来考察21个领先的文献综述工具，并分析了11个利用大型语言模型进行文献搜索和学术写作辅助的最新工具。最后，论文讨论了该领域的当前趋势、主要研究挑战和发展方向。

    

    本文对人工智能在系统文献综述（SLR）中的应用进行了全面的综述。SLR是一种严谨有序的方法论，用于评估和整合关于特定主题的先前研究。许多工具已被开发用于辅助和部分自动化SLR过程。人工智能在这个领域的日益重要角色显示了为研究人员提供更有效支持的巨大潜力，朝着文献综述的半自动化创建方向发展。我们的研究重点关注人工智能技术在SLR的半自动化中的应用，特别是在筛选和提取阶段。我们使用一个将23个传统特征与11个人工智能特征相结合的框架，对21个领先的SLR工具进行了考察。我们还分析了使用大型语言模型进行文献搜索和辅助学术写作的11个最新工具。最后，本文讨论了该领域的当前趋势，概述了主要的研究挑战，并提出了发展方向。

    This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions f
    
[^22]: Higher Layers Need More LoRA Experts

    Higher Layers Need More LoRA Experts

    [https://arxiv.org/abs/2402.08562](https://arxiv.org/abs/2402.08562)

    这篇论文提出了一种新颖的参数高效的MoE方法（MoLA），用于Transformer-based模型，其中每个模型层可以灵活地使用不同数量的LoRA专家。通过在多个基准数据集上进行实验，研究结果表明高层需要更多的LoRA专家来提高模型性能。

    

    参数高效调整（PEFT）技术，如低秩适应（LoRA），在大型语言模型上提供了训练效率，但对模型性能的影响仍有限。最近的努力整合了LoRA和专家混合（MoE），以提高PEFT方法的性能。尽管有了有希望的结果，但改进带有MoE的LoRA的效率的研究仍处于初级阶段。最近的研究表明，MoE体系结构中的专家具有不同的优势，并且还存在一些冗余。这个论断是否也适用于参数高效的MoE？在本文中，我们介绍了一种新颖的参数高效的MoE方法，称为MoLA（\textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation）），用于基于Transformer的模型，其中每个模型层可以灵活地使用不同数量的LoRA专家。我们研究了几种具有不同层级专家配置的体系结构。对六个知名的自然语言处理和常识问答基准进行了实验。

    Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benc
    
[^23]: 揭示了在重复的蛋糕切割中英勇竞争的艺术

    Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting

    [https://arxiv.org/abs/2402.08547](https://arxiv.org/abs/2402.08547)

    这篇论文研究了重复公平分割问题中的竞争策略，发现如果Bob过于偏好某一块蛋糕，Alice利用类似于二分查找的策略可以系统性地对Bob实施剥削，从而在时间上获得更多资源份额。

    

    我们考虑了两个玩家，分别代表Alice和Bob，通过对蛋糕的个人估值进行了重复的公平分割。在每一轮中，会出现一块与之前轮次相同的新蛋糕。Alice在自己选择的一个点上切割蛋糕，而Bob选择左边的部分或右边的部分，把剩余的给Alice。我们考虑了两个变种：顺序模式，Bob在选择左/右之前观察Alice的切割点；同时模式，他只在做出选择后观察她的切割点。同时模式是由Aumann和Maschler（1995）首次提出的。我们发现，如果Bob几乎是目光短浅的，并且经常选择自己喜欢的部分，那么他可以被Alice通过类似于二分查找的策略系统性地利用。这个策略使得Alice可以越来越精确地模拟Bob的偏好，从而在时间上获得不成比例的资源份额。我们分析了一个玩家能够利用其优势的极限。

    We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler (1995).   We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   We analyze the limits of how much a player can exploit t
    
[^24]: 分布式后续表示的分布式类比

    A Distributional Analogue to the Successor Representation

    [https://arxiv.org/abs/2402.08530](https://arxiv.org/abs/2402.08530)

    本文提出了一种新的分布式强化学习方法，它通过分离转换结构和奖励，引入了分布式后继度量来描述行为的分布式后果。在实验中展示了该方法的实用性，特别是在零样本风险敏感策略评估方面。

    

    本文提出了一种新的分布式强化学习方法，它将转换结构和奖励在学习过程中进行了明确的分离。与后续表示（SR）描述按照给定策略行为的期望后果类似，我们的分布式后继度量（SM）描述了这种行为的分布式结果。我们将分布式SM构建为一个分布的分布，并提供了与分布式和基于模型的强化学习相关的理论。此外，我们提出了一种从数据中学习分布式SM的算法，通过最小化两个层次的最大均值差异来实现。我们方法的关键是一些独立有价值的学习状态生成模型的算法技术。作为分布式SM有用性的例证，我们展示了它使得零样本风险敏感策略评估成为可能，这在以前是不可能的。

    This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possi
    
[^25]: 马尔可夫决策过程中的反事实影响

    Counterfactual Influence in Markov Decision Processes

    [https://arxiv.org/abs/2402.08514](https://arxiv.org/abs/2402.08514)

    马尔可夫决策过程中的反事实推理问题是一个基本问题，我们提出了一种算法来构建反事实模型，并通过比较反事实和干预分布来对影响进行形式化的特征化。

    

    我们的工作解决了马尔可夫决策过程（MDPs）中反事实推理的一个基本问题。给定一个MDP路径τ，这种推理允许我们得出描述τ的反事实路径τ'，描述了在与τ中观察到的动作序列不同的情况下，τ的“如果是这种情况”的版本。然而，由于反事实的状态和动作随时间发生偏离，观察τ可能不再影响反事实世界，这意味着分析不再针对个体观察结果，而是产生干预性结果而非反事实结果。尽管这个问题特别影响流行的Gumbel-max结构因果模型，这种模型用于MDP反事实，但直到现在一直被忽视。在这项工作中，我们引入了一个基于比较反事实和干预分布的影响的形式特征化。我们设计了一个算法来构建反事实模型。

    Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs). Given an MDP path $\tau$, this kind of inference allows us to derive counterfactual paths $\tau'$ describing what-if versions of $\tau$ obtained under different action sequences than those observed in $\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, the observation $\tau$ may no longer influence the counterfactual world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones. Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, it has remained overlooked until now. In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions. We devise an algorithm to construct counterfactual models that
    
[^26]: 通过专注于未知领域来放大蒙特卡罗树搜索中的探索能力

    Amplifying Exploration in Monte-Carlo Tree Search by Focusing on the Unknown

    [https://arxiv.org/abs/2402.08511](https://arxiv.org/abs/2402.08511)

    AmEx-MCTS introduces a novel formulation by decoupling value updates, visit count updates, and the selected path in Monte-Carlo tree search, allowing exclusion of already explored regions. This enables a broader search with the same computational resources while maintaining the utility of visit counts for exploration-exploitation balancing and quality metrics within MCTS.

    

    蒙特卡罗树搜索（MCTS）是一种具有广泛应用的有效的实时算法。它通过将计算资源有策略地分配给搜索树中有希望的部分，使得它成为大规模搜索空间中非常吸引人的搜索算法。然而，当仍然存在最有希望的路径时，MCTS经常在重新评估之前探索过的区域上消耗有限的资源。我们提出的方法，称为AmEx-MCTS，通过引入一种新颖的MCTS公式来解决这个问题。AmEx-MCTS的核心是在树搜索过程中解耦值更新、访问计数更新和选定路径，从而使得已探索的子树或叶节点可以被排除。这种分离保持了访问计数在MCTS中用于探索-开发平衡和质量指标的实用性。结果增强了使用相同计算资源进行更广泛搜索的能力，同时保留了MCTS的基本特性。

    Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast amount of applications. It strategically allocates computational resources to focus on promising segments of the search tree, making it a very attractive search algorithm in large search spaces. However, it often expends its limited resources on reevaluating previously explored regions when they remain the most promising path. Our proposed methodology, denoted as AmEx-MCTS, solves this problem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the decoupling of value updates, visit count updates, and the selected path during the tree search, thereby enabling the exclusion of already explored subtrees or leaves. This segregation preserves the utility of visit counts for both exploration-exploitation balancing and quality metrics within MCTS. The resultant augmentation facilitates in a considerably broader search using identical computational resources, preserving the essential characteristics of M
    
[^27]: 从形状到形状：推理SPARQL CONSTRUCT查询结果的SHACL形状（扩展版）

    From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)

    [https://arxiv.org/abs/2402.08509](https://arxiv.org/abs/2402.08509)

    本文研究了如何推断给定SPARQL CONSTRUCT查询的所有可能输出图形上成立的形状约束，考虑了输入图的形状约束和查询模板可能施加的新形状。

    

    SPARQL CONSTRUCT查询允许指定数据处理流水线，将给定的输入图形转换为新的输出图形。通过SHACL形状对图进行约束现在已经很常见，使用户能够了解他们可以预期哪些数据以及不会有哪些数据。然而，如果不知道特定的输入数据，在数据处理流水线结束时了解哪些图数据可以预期将变得具有挑战性：输入图的形状约束可能会影响输出图，但可能不再直接适用，并且查询模板可能会施加新的形状。在本文中，我们研究了对于给定的SPARQL CONSTRUCT查询的所有可能输出图形上成立的形状约束的推导。我们假设SPARQL CONSTRUCT查询是固定的，例如作为一个程序的一部分，而输入图遵守输入形状约束，但在其他方面可能会随时间变化，并且因此大多数是未知的。我们研究了SPARQL CONSTRUCT查询的一个片段（SCCQ）和一个片段的S

    SPARQL CONSTRUCT queries allow for the specification of data processing pipelines that transform given input graphs into new output graphs. It is now common to constrain graphs through SHACL shapes allowing users to understand which data they can expect and which not. However, it becomes challenging to understand what graph data can be expected at the end of a data processing pipeline without knowing the particular input data: Shape constraints on the input graph may affect the output graph, but may no longer apply literally, and new shapes may be imposed by the query template. In this paper, we study the derivation of shape constraints that hold on all possible output graphs of a given SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query is fixed, e.g., being part of a program, whereas the input graphs adhere to input shape constraints but may otherwise vary over time and, thus, are mostly unknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment of S
    
[^28]: 数据到文本自然语言生成研究的系统性回顾

    A Systematic Review of Data-to-Text NLG

    [https://arxiv.org/abs/2402.08496](https://arxiv.org/abs/2402.08496)

    这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。

    

    这篇系统性回顾旨在全面分析数据到文本生成研究的现状，重点是确定研究空白，提供未来方向，并解决回顾中发现的挑战。我们对文献进行了全面的检查，包括方法、数据集、评估指标、应用、多语言性和幻觉缓解措施。我们的回顾为这个快速发展的领域的未来研究提供了路线图。

    This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
    
[^29]: ChatGPT在回答与波士顿肠准备评分相关问题时的应用

    The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale

    [https://arxiv.org/abs/2402.08492](https://arxiv.org/abs/2402.08492)

    本研究评估了ChatGPT在使用波士顿肠准备评分（BBPS）进行结肠镜评估时的准确性和一致性，发现其准确率较内镜医生低，但仍有成为辅助工具的潜力。

    

    背景：结肠镜检查是胃肠学中的关键诊断工具，严重依赖良好的肠准备。ChatGPT是一个具有新兴智能的大型语言模型，也在医学应用中显示出潜力。本研究旨在评估ChatGPT在使用波士顿肠准备评分（BBPS）进行结肠镜评估时的准确性和一致性。方法：我们回顾性收集了2020年至2023年间的233个结肠镜图像，这些图像由3名资深内镜医生和3名初级内镜医生使用BBPS进行评估。此外，ChatGPT也对这些图像进行了评估，分为三组并进行了特定的微调。通过两轮测试评估一致性。结果：在初始轮中，ChatGPT的准确率在48.93%和62.66%之间变化，低于内镜医生的准确率（76.68%到77.83%）。ChatGPT的Kappa值为0.52到0.53，而内镜医生的Kappa值为0.75到0.87。结论：尽管ChatGPT在使用BBPS进行结肠镜评估时的准确性和一致性相对较低，但它仍具有潜力成为辅助工具。

    Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation. ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications. This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment. Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023. These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists. Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning. Consistency was evaluated through two rounds of testing. Results: In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT 
    
[^30]: 深度强化学习在细胞重编程的布尔模型吸引子景观中的控制遍历中的应用研究

    Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming

    [https://arxiv.org/abs/2402.08491](https://arxiv.org/abs/2402.08491)

    本研究开发了一个基于深度强化学习的计算框架，用于细胞重编程中的重编程策略识别。在控制问题中，引入了伪吸引子的概念和识别方法，并设计了一个用于解决该问题的计算框架。

    

    细胞重编程可用于预防和治疗不同疾病。然而，通过传统湿实验发现重编程策略的效率受到时间和成本的限制。在本研究中，我们基于深度强化学习开发了一个新颖的计算框架，以便帮助识别重编程策略。为此，我们在细胞重编程框架的BNs和PBNs以及异步更新模式下制定了一个控制问题。此外，我们引入了伪吸引子的概念和训练过程中伪吸引子状态的识别方法。最后，我们设计了一个用于解决控制问题的计算框架，并在多个不同模型上进行了测试。

    Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
    
[^31]: 零样本和系统性评估视觉语言Transformer模型之间的有趣差异

    Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models

    [https://arxiv.org/abs/2402.08473](https://arxiv.org/abs/2402.08473)

    本文通过新的梯度下降优化方法，探索了常用的视觉语言模型的嵌入空间。结果显示，尽管该模型在零样本分类上有超过99%的表现，但在系统性评估中却完全失败。通过线性近似，提供了一个解释这些差异的框架。

    

    过去几年中，基于Transformer的模型在自然语言处理和其他领域中占据主导地位，因为它们在基准数据集上具有出色的(零样本)性能。然而，由于其复杂性和规模，这些模型目前尚未被很好地理解。虽然探测法已广泛用于理解特定属性，但表示空间的结构尚未得到系统性的刻画；因此，目前还不清楚这样的模型如何推广和过度推广到超出数据集范围的新输入。在本文中，基于一种新的梯度下降优化方法，我们能够探索常用的视觉语言模型的嵌入空间。我们使用Imagenette数据集表明，尽管该模型实现了超过99%的零样本分类性能，但它在系统性评估中完全失败。我们使用线性近似提供了一个框架来解释这些显著的差异。我们还使用另一个模型获得了类似的结果，以支持这个解释。

    Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that o
    
[^32]: 用于自动分析优化算法的大型语言模型

    Large Language Models for the Automated Analysis of Optimization Algorithms

    [https://arxiv.org/abs/2402.08472](https://arxiv.org/abs/2402.08472)

    本论文通过将大型语言模型集成到STNWeb中，展示了将其应用于优化算法领域的潜力，通过生成全面的报告和图表，提升用户体验和降低使用该工具的障碍。

    

    大型语言模型（LLMs）生成高质量的文本和代码的能力使其越来越受欢迎。本文旨在通过将LLMs集成到STNWeb中，展示其在优化算法领域的潜力。STNWeb是一个用于生成搜索轨迹网络（STNs）的基于Web的工具，STNs是优化算法行为的可视化。虽然STNWeb生成的可视化结果对于算法设计者来说非常有信息量，但是往往需要一定的先验知识才能解释。为了弥补这种知识差距，我们将LLMs（特别是GPT-4）整合到STNWeb中，生成全面的书面报告，并自动生成图表，从而提升用户体验并减少研究界使用该工具的障碍。此外，我们的方法还可以扩展到优化社区中的其他工具，展示了其多功能性和潜力。

    The ability of Large Language Models (LLMs) to generate high-quality text and code has fuelled their rise in popularity. In this paper, we aim to demonstrate the potential of LLMs within the realm of optimization algorithms by integrating them into STNWeb. This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior. Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted. In an attempt to bridge this knowledge gap, we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community. Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential
    
[^33]: 面向规模化光伏衰减分析的并行友好时空图学习

    Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale

    [https://arxiv.org/abs/2402.08470](https://arxiv.org/abs/2402.08470)

    提出了一种并行友好的时空图学习方法，用于规模化光伏衰减分析。该方法集成了时空一致性和图注意力，可以准确估计大规模光伏逆变器的长期性能损失率，并提供在线模型和数据集以帮助改进光伏系统的性能评估和可靠性分析。

    

    我们提出了一种新颖的时空图神经网络驱动的趋势分析方法(ST-GTrend)，用于进行光伏电力网络的集群级性能衰减分析。光伏电站已成为全球可持续能源生产格局的重要组成部分。准确估计光伏系统的性能对于其作为发电技术和金融资产的可行性至关重要。评估光伏系统的水平化能源成本(LCOE)中最具挑战性的问题之一是了解和估计大规模光伏逆变器的长期性能损失率(PLR)。ST-GTrend集成了时空一致性和图注意力，将PLR作为长期的“老化”趋势与光伏输入数据中的多个波动项分离开来。为了应对时序中多样的衰减模式，ST-GTrend采用并行图自编码器数组同时提取老化和波动项。ST-GTrend发布了一个在线光伏衰减分析模型和数据集，以帮助学术界和工业界改进光伏系统的性能评估和可靠性分析。

    We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks. PV power stations have become an integral component to the global sustainable energy production landscape. Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset. One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term "aging" trend from multiple fluctuation terms in the PV input data. To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously. ST-GTrend impo
    
[^34]: 认真对待培训：人工智能的人类引导与基于管理的监管

    Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence

    [https://arxiv.org/abs/2402.08466](https://arxiv.org/abs/2402.08466)

    这项研究认为，在人工智能的管理式监管方法中，加强人类引导和培训技术的研究和实践对于提高AI的性能，解决技术和伦理问题等方面具有重要作用。

    

    对人工智能（AI）相关危害更强大的治理的热情呼声正在世界范围内引起管理学者所称的基于管理的监管方法的采用。美国和欧洲的最新倡议以及国际标准化组织采纳的重要自我监管标准都共同具有一个核心的基于管理的范式。这些基于管理的倡议旨在通过增加人类对AI工具的培训和开发的监督来激励。因此，在这个新兴的基于管理的监管范式时代中，需要对人类引导培训技术进行完善和系统化。如果认真对待，人类引导培训可以减轻一些对AI的技术和伦理压力，以人类直觉提高AI的性能，并更好地满足对公平性和有效解释的需求。在本文中，我们讨论了连接。

    Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability. In this paper, we discuss the connec
    
[^35]: 能够区分所有3D图形的是3-（F）WL方法足够吗？

    Is 3-(F)WL Enough to Distinguish All 3D Graphs?

    [https://arxiv.org/abs/2402.08429](https://arxiv.org/abs/2402.08429)

    本文探索了从图生成的角度是否存在能够区分所有3D图形的k，以及对于更复杂的3D图形，k-WL的同构判别能力是否严格增加。

    

    图同构问题是图分析领域中的一个重要且具有挑战性的问题，例如：分析两个化学分子的相似性，或者研究图神经网络的表达能力。WL测试是一种判断两个图是否同构的方法，但它不能区分所有非同构图。作为WL的改进，k-WL具有更强的同构判别能力，且随着k的增加，其判别能力严格增强。然而，对于更复杂的3D图形，k-WL的同构判别能力是否严格增加，或者是否存在能够区分所有3D图形的k，仍未被探索。本文试图从图生成的视角探索这个问题。

    The problem of graph isomorphism is an important but challenging problem in the field of graph analysis, for example: analyzing the similarity of two chemical molecules, or studying the expressive ability of graph neural networks. WL test is a method to judge whether two graphs are isomorphic, but it cannot distinguish all non-isomorphic graphs. As an improvement of WL, k-WL has stronger isomorphism discrimination ability, and as k increases, its discrimination ability is strictly increasing. However, whether the isomorphic discrimination power of k-WL is strictly increasing for more complex 3D graphs, or whether there exists k that can discriminate all 3D graphs, remains unexplored. This paper attempts to explore this problem from the perspective of graph generation.
    
[^36]: 通过植入记忆的Neural Decision Tree实现车辆行为预测

    Vehicle Behavior Prediction by Episodic-Memory Implanted NDT

    [https://arxiv.org/abs/2402.08423](https://arxiv.org/abs/2402.08423)

    本文通过植入记忆的Neural Decision Tree（eMem-NDT）探索目标车辆行为预测的可解释性，并且在神经决策树中对历史车辆行为特征的行为记忆原型进行分组和对齐。

    

    在自动驾驶中，预测目标车辆的行为（左转、停车等）对于自动驾驶车辆做出安全决策和避免事故至关重要。现有的基于深度学习的方法已经展示出了优秀而准确的性能，但是黑盒的特性使得它们难以在实际应用中得到信任。本文通过植入记忆的Neural Decision Tree（简称eMem-NDT）探索目标车辆行为预测的可解释性。eMem-NDT的结构通过对车辆行为描述的文本嵌入进行分层聚类构建。eMem-NDT是预训练深度学习模型的一个神经支持部分，通过将深度模型的软最大层替换为eMem-NDT，在神经决策树上对训练数据中历史车辆行为特征的行为记忆原型进行分组和对齐。eMem-NDT的每个叶节点由神经网络建模，用于对齐行为记忆原型。

    In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. 
    
[^37]: 将超高场图像的特征迁移到低场磁共振成像中进行强度引导的脑分割

    Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging

    [https://arxiv.org/abs/2402.08409](https://arxiv.org/abs/2402.08409)

    本研究提出了一种将7T磁共振成像特征与低场磁共振成像特征融合的深度学习框架，实现在7T缺失环境下进行脑图像分割的任务。通过自适应融合和融入，利用强度引导特征，可以识别难以识别的细微结构特征。

    

    超高场(7T)磁共振成像能够提供更好的解剖细节和对比度，然而其高成本和较低可访问性限制了其广泛应用。本研究提出了一种深度学习框架，通过将7T图像的特征与低场图像的特征进行融合，实现在7T缺失环境下的脑图像分割任务。我们的自适应融合模块通过预训练网络提取低场图像的7T-like特征，并将其精细调整，使其有效地融入低场图像的特征中。利用从融合和融入中获得的强度引导特征，分割模型能够识别通常在仅依赖低场图像时难以识别的细微结构特征。

    Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI, provides superior anatomical details of internal brain structures owing to its enhanced signal-to-noise ratio and susceptibility-induced contrast. However, the widespread use of 7T MRI is limited by its high cost and lower accessibility compared to low-field (LF) MRI. This study proposes a deep-learning framework that systematically fuses the input LF magnetic resonance feature representations with the inferred 7T-like feature representations for brain image segmentation tasks in a 7T-absent environment. Specifically, our adaptive fusion module aggregates 7T-like features derived from the LF image by a pre-trained network and then refines them to be effectively assimilable UHF guidance into LF image features. Using intensity-guided features obtained from such aggregation and assimilation, segmentation models can recognize subtle structural representations that are usually difficult to recognize when relying only on L
    
[^38]: LOSS-GAT: 标签传播和一类半监督图注意力网络用于假新闻检测

    LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection

    [https://arxiv.org/abs/2402.08401](https://arxiv.org/abs/2402.08401)

    本文介绍了一种名为LOSS-GAT的半监督和一类方法用于假新闻检测，采用了标签传播和图注意力网络，以解决标记数据集有限性的挑战。

    

    在广泛社交网络的时代，虚假新闻的快速传播已经成为一个重大威胁，在人们生活的各个方面造成了不利影响。机器学习和深度学习方法已被广泛应用于识别假新闻。然而，识别假新闻的一个重大挑战是标记新闻数据集的有限性。因此，使用只有一小部分标记数据的一个类学习（OCL）方法可以是解决这一挑战的合适方法。另一方面，将数据表示为图可以访问到不同内容和结构信息，并且图上的标签传播方法可以有效地预测节点的标签。在本文中，我们采用基于图的模型进行数据表示，并引入一种半监督和一类方法，用于假新闻检测，称为LOSS-GAT。

    In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives. Machine learning and deep learning approaches have been extensively employed for identifying fake news. However, a significant challenge in identifying fake news is the limited availability of labeled news datasets. Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge. On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels. In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT. Initially, we employ a two-step label propagation algori
    
[^39]: 选择性学习：实现动态正则化的鲁棒校准

    Selective Learning: Towards Robust Calibration with Dynamic Regularization

    [https://arxiv.org/abs/2402.08384](https://arxiv.org/abs/2402.08384)

    本研究提出了一种名为动态正则化（DReg）的方法，通过训练学习应该学到什么，从而解决深度学习中的过拟合和误校准问题。

    

    深度学习中的误校准指的是预测的可信度与性能之间存在差异。这个问题通常是由过拟合问题引起的，过拟合问题的特点是学习训练集中呈现出的所有内容，导致在测试过程中进行过于自信的预测。现有方法通常通过在目标函数中添加最大熵正则化器来解决过拟合问题并缓解误校准问题。这个目标可以理解为寻找一个模型，通过增加可信度来适应实际标签，同时通过降低可信度来最大化预测概率的熵。然而，以前的方法缺乏对可信度调整的明确指导，导致目标冲突（增加但也降低可信度）。因此，我们引入了一种称为动态正则化（DReg）的方法，旨在通过训练学习应该学到什么，从而避免可信度调整的权衡。

    Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At 
    
[^40]: 通过潜在全局演化实现偏微分方程的正逆问题的不确定性量化

    Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution

    [https://arxiv.org/abs/2402.08383](https://arxiv.org/abs/2402.08383)

    本论文提出了一种将高效和精确的不确定性量化集成到基于深度学习的替代模型中的方法，称为LE-PDE-UQ。该方法利用潜在空间中的潜在向量来演化系统的状态和相应的不确定性估计。

    

    基于深度学习的替代模型在速度方面相对于传统的偏微分方程求解器表现出了显著的优势，往往能够实现10到1000倍的加速。然而，一个重要的挑战阻碍了它们在科学和工业领域的广泛应用，即缺乏对其预测不确定性的理解，特别是在涉及关键决策的情况下。为了解决这个限制，我们提出了一种将高效和精确的不确定性量化集成到基于深度学习的替代模型中的方法。我们的方法称为带不确定性量化的偏微分方程的潜在演化（LE-PDE-UQ），为前向和反向问题赋予了深度学习替代模型强大而高效的不确定性量化能力。LE-PDE-UQ利用潜在空间中的潜在向量来演化系统的状态和相应的不确定性估计。

    Deep learning-based surrogate models have demonstrated remarkable advantages over classical solvers in terms of speed, often achieving speedups of 10 to 1000 times over traditional partial differential equation (PDE) solvers. However, a significant challenge hindering their widespread adoption in both scientific and industrial domains is the lack of understanding about their prediction uncertainties, particularly in scenarios that involve critical decision making. To address this limitation, we propose a method that integrates efficient and precise uncertainty quantification into a deep learning-based surrogate model. Our method, termed Latent Evolution of PDEs with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based surrogate models with robust and efficient uncertainty quantification capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent vectors within a latent space to evolve both the system's state and its corresponding uncertainty estimation.
    
[^41]: 动态策略下多步预测的时间序列分类

    Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting

    [https://arxiv.org/abs/2402.08373](https://arxiv.org/abs/2402.08373)

    本文提出了动态策略（DyStrat）用于多步预测，并在实验中证明了其在多个时间序列数据集上的优越性能。

    

    时间序列中的多步预测（MSF）是几乎所有时间领域的基础，能够预测多个未来时间步骤的能力。为了进行这种预测，必须假设时间动态的递归复杂性，这种假设被称为训练预测模型所使用的预测策略。之前的研究表明，在评估未见数据之前，不清楚哪种预测策略是最优的。此外，目前的多步预测方法使用单个（固定）的预测策略。在本文中，我们对最优预测策略的实例级别变化进行了描述，并提出了用于MSF的动态策略（DyStrat）。我们在来自不同规模、领域和多步预测长度的10个数据集上进行了实验。使用基于随机森林的分类器时，DyStrat在94%的时间内优于最佳固定策略，平均均方误差降低了11%。

    Multi-step forecasting (MSF) in time-series, the ability to make predictions multiple time steps into the future, is fundamental to almost all temporal domains. To make such forecasts, one must assume the recursive complexity of the temporal dynamics. Such assumptions are referred to as the forecasting strategy used to train a predictive model. Previous work shows that it is not clear which forecasting strategy is optimal a priori to evaluating on unseen data. Furthermore, current approaches to MSF use a single (fixed) forecasting strategy.   In this paper, we characterise the instance-level variance of optimal forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We experiment using 10 datasets from different scales, domains, and lengths of multi-step horizons. When using a random-forest-based classifier, DyStrat outperforms the best fixed strategy, which is not knowable a priori, 94% of the time, with an average reduction in mean-squared error of 11%. Our approach 
    
[^42]: 在非平稳环境中通过多模态技能实现一次性模仿

    One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill

    [https://arxiv.org/abs/2402.08369](https://arxiv.org/abs/2402.08369)

    本研究提出一种基于技能的模仿学习框架，在非平稳环境中实现一次性模仿和零次适应。通过从单个演示中推断出语义技能序列，并将其转换为经过优化的行动序列，以适应环境中隐含的动力学变化。实验结果表明，该框架在不同的一次性模仿场景下表现出良好的性能。

    

    一次性模仿是从单个演示中学习新任务的方法，然而，在非平稳环境中采用它来解决复杂任务的挑战性问题是很困难的，因为这种环境具有高域多样性。为了解决这个问题，我们探索了复杂任务的组合性，并提出了一种新颖的基于技能的模仿学习框架，实现一次性模仿和零次适应；通过单个演示来推断复杂的未见任务中的语义技能序列，然后将序列中的每个技能转化为经过优化的行动序列，以适应可能随时间变化的环境隐含动力学。具体而言，我们利用视觉语言模型从离线视频数据集中学习语义技能集，其中每个技能在视觉语言嵌入空间上表示，并利用动力学推理的元学习实现零次技能适应。我们使用各种一次性模仿场景对我们的框架进行评估，用于扩展的多阶段元世界。

    One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time. Specifically, we leverage a vision-language model to learn a semantic skill set from offline video datasets, where each skill is represented on the vision-language embedding space, and adapt meta-learning with dynamics inference to enable zero-shot skill adaptation. We evaluate our framework with various one-shot imitation scenarios for extended multi-stage Meta-world 
    
[^43]: 基于真实用户查询评估文本到SQL系统的数据模型鲁棒性

    Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries

    [https://arxiv.org/abs/2402.08349](https://arxiv.org/abs/2402.08349)

    本文首次深入评估了在实践中文本到SQL系统的数据模型的鲁棒性，通过基于一个多年的国际项目集中评估，对一个在FIFA World Cup背景下连续运行了9个月的真实部署的FootballDB系统进行了评估。

    

    文本到SQL系统（也称为自然语言到SQL系统）已成为弥合用户能力与基于SQL的数据访问之间差距的越来越流行的解决方案。这些系统将用户的自然语言请求转化为特定数据库的有效SQL语句。最近的基于转换器的语言模型使得文本到SQL系统受益匪浅。然而，虽然这些系统在常常是合成基准数据集上不断取得新的高分，但对于它们在真实世界、现实场景中对不同数据模型的鲁棒性的系统性探索明显缺乏。本文基于一个多年国际项目关于文本到SQL界面的集中评估，提供了对文本到SQL系统在实践中数据模型鲁棒性的首次深度评估。我们的评估基于FootballDB的真实部署，该系统在FIFA World Cup的背景下连续运行了9个月。

    Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
    
[^44]: 用分类器驱动的方法揭示大型语言模型中的五大人格特质：文本分析

    Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach

    [https://arxiv.org/abs/2402.08341](https://arxiv.org/abs/2402.08341)

    本研究使用分类器驱动的方法，通过不同的输入提示探究大型语言模型的输出变化，以增加其透明度。结果显示，这些模型根据输入的不同提示而表现出不同的人格特质，类似于人类对刺激做出的反应。

    

    大型语言模型（LLMs）在招聘背景下被应聘者和雇主广泛使用，然而这也引发了众多伦理问题，特别是与这些“黑盒子”模型缺乏透明度有关。尽管先前的研究试图通过调查LLMs的人格特质来增加其透明度，但许多先前的研究都要求模型来完成人格评估。相反，本研究旨在通过检查不同输入提示下模型的输出变化来更好地理解这些模型。具体来说，我们使用从常见面试问题和旨在引发特定的五大人格特质的提示来进行新颖的调查方法，以检查模型是否像人类一样容易激活特定人格特质，并根据其输出中的语言来评估其人格。为此，我们反复提供提示。

    Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompte
    
[^45]: 不确定性量化通过稳定分布传播

    Uncertainty Quantification via Stable Distribution Propagation

    [https://arxiv.org/abs/2402.08324](https://arxiv.org/abs/2402.08324)

    通过局部线性化，我们提出了一种新的方法，能够通过神经网络传播稳定概率分布来量化输出的不确定性。我们的方法在预测校准置信区间和选择性预测方面显示出了明显的优势。

    

    我们提出了一种通过神经网络传播稳定概率分布的新方法。我们的方法基于局部线性化，在ReLU非线性方面，我们证明它是总变差距离的最优近似。这使得能够通过神经网络传播高斯和柯西输入不确定性来量化其输出不确定性。为了展示传播分布的实用性，我们将所提出的方法应用于预测校准置信区间和选择性预测超出分布的数据上。结果表明了传播分布的广泛适用性，并显示了我们的方法在诸如矩匹配等其他方法上的优势。

    We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.
    
[^46]: 映射生成型人工智能的伦理: 一项全面的范围调查

    Mapping the Ethics of Generative AI: A Comprehensive Scoping Review

    [https://arxiv.org/abs/2402.08323](https://arxiv.org/abs/2402.08323)

    本研究进行了一项生成型人工智能伦理问题的全面调查，提供了378个规范问题的分类和排序，并总结了伦理争议的核心内容，包括公正性、安全性、有害内容、隐私等问题。

    

    生成型人工智能的出现和它在社会中的广泛应用引发了关于其伦理影响和风险的激烈辩论。这些风险往往与传统的判别式机器学习所带来的风险有所不同。为了综合近期的讨论并映射其规范概念，我们对生成型人工智能的伦理问题进行了一项范围调查，特别关注大型语言模型和文本到图像模型。我们的分析提供了一个对19个主题领域中378个规范问题进行分类的分类法，并按照它们在文献中的普遍性进行了排序。这项研究为学者、实践者或决策制定者提供了一个全面的概述，压缩了围绕公正性、安全性、有害内容、幻觉、隐私、互动风险、安全性、一致性、社会影响等伦理争议。我们讨论了结果，评估了文献中的不平衡情况，并探索了未经证实的风险场景。

    The advent of generative artificial intelligence and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of generative artificial intelligence, including especially large language models and text-to-image models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.
    
[^47]: 使用合成训练的生成模型进行文化艺术品的一对多三维几何重建

    One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model

    [https://arxiv.org/abs/2402.08310](https://arxiv.org/abs/2402.08310)

    这项研究利用合成训练的生成模型，通过单个素描实现了一对多的文化艺术品三维几何重建，并能够通过多模态输入进行指导，对于历史素描等不太表现力强的表示具有较好的效果。这种解决方案仅依赖于合成数据进行训练，即使在训练样本数量很少的情况下也能适应，使领域专家能够互动地重建丢失艺术品的可能外观。

    

    使用单个图像来估计物体的三维形状是一个困难的问题。现代方法针对一般物体基于真实照片可以取得良好的结果，但在历史素描等不太表现力强的表示上表现较差。我们的自动化方法可以从单个素描生成多样化的详细三维表示，能够通过文本提示等多模态输入进行指导。它仅依赖于合成数据进行训练，即使在训练样本数量很少的情况下也能适应。我们的解决方案允许领域专家（如馆长）与之交互地重建丢失艺术品的可能外观。

    Estimating the 3D shape of an object using a single image is a difficult problem. Modern approaches achieve good results for general objects, based on real photographs, but worse results on less expressive representations such as historic sketches. Our automated approach generates a variety of detailed 3D representation from a single sketch, depicting a medieval statue, and can be guided by multi-modal inputs, such as text prompts. It relies solely on synthetic data for training, making it adoptable even in cases of only small numbers of training examples. Our solution allows domain experts such as a curators to interactively reconstruct potential appearances of lost artifacts.
    
[^48]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^49]: 停下来思考：我们想要进行什么样的研究？

    Time to Stop and Think: What kind of research do we want to do?

    [https://arxiv.org/abs/2402.08298](https://arxiv.org/abs/2402.08298)

    这篇文章呼吁停下来思考我们想要进行什么样的研究，特别是在实验设计方面，避免不严谨和不令人信服的实验，改变以支持先前信念为目的的态度，提倡对个人和社群的真诚批判性评估和反思。

    

    实验是人工智能研究的固有部分，它可以收集定量观察数据，验证假设，并为其重新制定提供证据。因此，实验必须与研究目的一致，适当地处理每种情况下的相关问题。不幸的是，文献中充斥着一些实验缺乏严谨性和说服力的研究，往往是为了支持先前的信念，而不是回答相关的研究问题。本文我们主要关注元启发式优化领域，因为这是我们主要的工作领域，也是我们观察到存在不当行为的地方。即使我们在本文中将重点限制在研究的实验部分，我们的主要目标是激发对我们工作的真诚批判性评估，引发个人和社群层面的反思过程。

    Experimentation is an intrinsic part of research in artificial intelligence since it allows for collecting quantitative observations, validating hypotheses, and providing evidence for their reformulation. For that reason, experimentation must be coherent with the purposes of the research, properly addressing the relevant questions in each case. Unfortunately, the literature is full of works whose experimentation is neither rigorous nor convincing, oftentimes designed to support prior beliefs rather than answering the relevant research questions.   In this paper, we focus on the field of metaheuristic optimization, since it is our main field of work, and it is where we have observed the misconduct that has motivated this letter. Even if we limit the focus of this manuscript to the experimental part of the research, our main goal is to sew the seed of sincere critical assessment of our work, sparking a reflection process both at the individual and the community level. Such a reflection p
    
[^50]: 数据污染对反事实解释的影响

    The Effect of Data Poisoning on Counterfactual Explanations

    [https://arxiv.org/abs/2402.08290](https://arxiv.org/abs/2402.08290)

    本研究研究了反事实解释在数据污染方面的脆弱性，发现最先进的反事实生成方法和工具包容易受到数据污染的影响。

    

    反事实解释是分析黑盒系统预测的一种流行方法，它们提供了根据不同情况建议改变输入以获得不同（更有利）系统输出的计算补救机会。然而，最近的研究突显了它们对不同类型操纵的脆弱性。本研究研究了反事实解释对数据污染的脆弱性。我们在增加三个不同层次的补救成本方面，形式化地研究了反事实解释在单个实例、某个子组或所有实例上的数据污染。我们证明了最先进的反事实生成方法和工具包对此类数据污染是脆弱的。

    Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \& toolboxes are vulnerable to such data poisoning.
    
[^51]: 一种逻辑方法在刑事案件调查中的应用

    A Logical Approach to Criminal Case Investigation

    [https://arxiv.org/abs/2402.08284](https://arxiv.org/abs/2402.08284)

    本文介绍了一种逻辑方法在刑事案件调查中的应用。研究人员通过推断凶手的动机、机会和方法来寻找罪犯。

    

    具有解释其结论原因的属性的可解释性人工智能（XAI）技术吸引了人们的关注。XAI有望在法医科学和司法系统的发展中得到应用。在当前的法医和刑事调查环境中，专家们面临着大量数据、混乱复杂的环境中的小证据和传统实验室结构以及有时不足的知识等许多挑战。所有这些都可能导致调查失败和司法失误。本文描述了一种逻辑方法在犯罪现场调查中的应用。应用的主题是《斑纹带之谜》这部福尔摩斯短篇小说。应用的数据是为知识图谱推理挑战创建的知识图谱。我们通过推断每个具有动机、机会和方法的人来寻找凶手。我们创建了一个...

    XAI (eXplanable AI) techniques that have the property of explaining the reasons for their conclusions, i.e. explainability or interpretability, are attracting attention. XAI is expected to be used in the development of forensic science and the justice system. In today's forensic and criminal investigation environment, experts face many challenges due to large amounts of data, small pieces of evidence in a chaotic and complex environment, traditional laboratory structures and sometimes inadequate knowledge. All these can lead to failed investigations and miscarriages of justice. In this paper, we describe the application of one logical approach to crime scene investigation. The subject of the application is ``The Adventure of the Speckled Band'' from the Sherlock Holmes short stories. The applied data is the knowledge graph created for the Knowledge Graph Reasoning Challenge. We tried to find the murderer by inferring each person with the motive, opportunity, and method. We created an o
    
[^52]: Pix2Code：学习将神经视觉概念组合成程序

    Pix2Code: Learning to Compose Neural Visual Concepts as Programs

    [https://arxiv.org/abs/2402.08280](https://arxiv.org/abs/2402.08280)

    Pix2Code 是一个将神经视觉概念组合成程序的框架，通过利用显式、组合的符号和隐式的神经表示能力，从图像中检索对象表示并将关系概念合成为lambda演算程序，来解决通用性和可解释性的挑战。在推理领域Kandinsky Patterns和CURI上的评估结果表明，Pix2Code 能够识别组合视觉概念并推广到新数据和推理任务。

    

    在无监督学习中，学习从图像中抽象概念的挑战在于需要将视觉感知和通用关系推理进行整合。此外，该任务的无监督性质使得人类用户需要能够理解模型学到的概念，并可能修正错误的行为。为了解决视觉概念学习的通用性和可解释性约束，我们提出了Pix2Code，这是一个将程序合成扩展到视觉关系推理的框架，利用了明确的、组合的符号和隐式的神经表示的能力。通过从图像中检索对象表示并将关系概念合成为lambda演算程序来实现这一点。我们在具有挑战性的推理领域Kandinsky Patterns和CURI上评估了Pix2Code的多样特性，从而测试其识别组合视觉概念并推广到新数据和推理任务的能力。

    The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and co
    
[^53]: 通过优化编码器和辅助损失改进机器图像编码

    Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss

    [https://arxiv.org/abs/2402.08267](https://arxiv.org/abs/2402.08267)

    通过应用辅助损失优化编码器，我们提出了一种改进的机器图像编码方法，能够在目标检测和语义分割任务中实现显著的速率提高。

    

    机器图像编码（ICM）旨在通过识别模型而不是人眼视觉来压缩图像以供机器分析。因此，在ICM中，编码器识别和压缩对于机器识别任务来说是至关重要的。学习型ICM有两种主要方法：基于任务损失的压缩模型优化和基于感兴趣区域（ROI）的比特分配。这些方法为编码器提供了识别能力。然而，当识别模型很深时，使用任务损失进行优化变得困难，而基于ROI的方法在评估过程中通常会增加额外开销。在本研究中，我们提出了一种用于学习型ICM模型的新训练方法，通过对编码器应用辅助损失来提高其识别能力和速率-失真性能。与传统训练方法相比，我们的方法在目标检测和语义分割任务中实现了27.7%和20.3%的Bjontegaard Delta速率改进。

    Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training 
    
[^54]: 在MOOC中以对比学习的方式建模平衡显式和隐式关系，用于知识概念推荐

    Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs

    [https://arxiv.org/abs/2402.08256](https://arxiv.org/abs/2402.08256)

    本论文提出了一种基于对比学习的框架，用于在MOOCs中平衡显式和隐式关系进行知识概念推荐。通过建立一个MOOCs的异构信息网络(HIN)，可以更好地表示和学习隐式关系，从而提高知识概念推荐的性能并满足用户的个性化需求。

    

    在大规模开放在线课程(MOOCs)中，知识概念推荐是一个备受关注的重要问题。现有方法主要依赖于MOOC平台上用户和知识概念之间的显式关系进行推荐。然而，在用户的学习活动中会产生大量隐式关系（例如，共同兴趣或相同的知识水平），现有方法未能考虑这些隐式关系，并且这些关系本身很难学习和表示，导致知识概念推荐表现不佳，无法满足用户的个性化需求。为了解决这个问题，我们提出了一种基于对比学习的新框架，用于在MOOCs中表示和平衡显式和隐式关系进行知识概念推荐（CL-KCRec）。具体而言，我们首先通过对MOOCs异构信息网络(HIN)进行建模来构建一个HIN模型。

    The knowledge concept recommendation in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention. Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for recommendation. However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users' learning activities on the MOOC platforms. Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept recommendation and an inability to meet users' personalized needs. To address this issue, we propose a novel framework based on contrastive learning, which can represent and balance the explicit and implicit relations for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modelin
    
[^55]: Distal Interference: 探索基于模型的持续学习的极限

    Distal Interference: Exploring the Limits of Model-Based Continual Learning

    [https://arxiv.org/abs/2402.08255](https://arxiv.org/abs/2402.08255)

    本研究探讨了持续学习中远距离干扰的极限问题，并提出了一种新型的可近似任何连续函数的反对称有界指数层B-spline ANN架构，用以解决灾难性干扰的挑战。

    

    持续学习是机器学习模型按顺序学习不同任务的过程。持续学习被称为灾难性干扰或遗忘的阻碍，即在学习新任务时快速遗忘之前学习的任务。尽管人工神经网络（ANNs）在实践中取得了成功，但它们容易受到灾难性干扰的影响。该研究分析了梯度下降和远距离输入点之间重叠表示如何导致远距离干扰和灾难性干扰。远距离干扰是指在对域的子集进行模型训练时，对域的其他子集造成非局部变化的现象。该研究表明，没有远距离干扰的均匀可训练模型必须具有指数级的规模。提出了一种名为ABEL-Spline的新型反对称有界指数层B-spline ANN架构，该架构可以近似任何连续函数，具有均匀可训练性、多项式计算复杂度。

    Continual learning is the sequential learning of different tasks by a machine learning model. Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, an
    
[^56]: 最近解决生物医学中人工智能公平性和偏见的方法综述

    A survey of recent methods for addressing AI fairness and bias in biomedicine

    [https://arxiv.org/abs/2402.08250](https://arxiv.org/abs/2402.08250)

    这篇论文综述了近期用于解决生物医学中人工智能公平性和偏见的方法。研究指出，在开发AI模型过程中可能存在偏见，因此需要采取措施来识别和解决这些偏见以确保在临床环境中准确可靠地应用AI模型。

    

    人工智能系统具有改革临床实践的潜力，包括提高诊断准确性和手术决策，同时减少成本和人力。然而，需要认识到这些系统可能会延续社会不平等或展示基于种族或性别的偏见。这些偏见可能发生在AI模型的开发之前、期间或之后，因此了解和解决潜在的偏见对于在临床环境中准确可靠地应用AI模型至关重要。为了缓解模型开发过程中的偏见问题，我们调查了近期在生物医学自然语言处理（NLP）或计算机视觉（CV）领域中应用的不同去偏方法的文献。然后我们讨论了在生物医学领域中应用的解决偏见的方法。我们在PubMed、ACM数字图书馆和IEEE Xplore上执行了文献搜索，内容涵盖了相关文章的发布在1月之间。

    Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods that have been applied in the biomedical domain to address bias. We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between Janu
    
[^57]: 多无人机合作巡检路径规划的蚁群优化

    Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles

    [https://arxiv.org/abs/2402.08246](https://arxiv.org/abs/2402.08246)

    该论文提出了一种基于蚁群优化的新方法，用于解决无人机协作路径规划问题。该方法通过利用结构的三维模型生成无人机的视点，并将路径规划转化为旅行推销员问题。实验结果表明，该系统不仅能够生成可行的巡检路径，而且能够减少路径长度。

    

    本文提出一种基于群集智能的新方法来处理无人机协作路径规划问题，该问题对于基础设施的自动化巡检至关重要。该方法利用结构的三维模型生成无人机的视点。视点的计算考虑了与无人机编队模型、相机参数和数据后处理要求相关的约束。然后，将这些视点作为输入，将路径规划表示为扩展旅行推销员问题，并定义新的成本函数。最后使用蚁群优化来解决问题，得到最优的巡检路径。通过对真实结构的三维模型进行实验，评估了所提方法的性能。结果表明，我们的系统不仅能够为无人机生成可行的巡检路径，而且在复杂结构上将路径长度减少了29.47％。

    This paper presents a new swarm intelligence-based approach to deal with the cooperative path planning problem of unmanned aerial vehicles (UAVs), which is essential for the automatic inspection of infrastructure. The approach uses a 3D model of the structure to generate viewpoints for the UAVs. The calculation of the viewpoints considers the constraints related to the UAV formation model, camera parameters, and requirements for data post-processing. The viewpoints are then used as input to formulate the path planning as an extended traveling salesman problem and the definition of a new cost function. Ant colony optimization is finally used to solve the problem to yield optimal inspection paths. Experiments with 3D models of real structures have been conducted to evaluate the performance of the proposed approach. The results show that our system is not only capable of generating feasible inspection paths for UAVs but also reducing the path length by 29.47\% for complex structures when 
    
[^58]: 实现人工智能和机器人研发中的公平敏捷方法

    Towards Equitable Agile Research and Development of AI and Robotics

    [https://arxiv.org/abs/2402.08242](https://arxiv.org/abs/2402.08242)

    这项研究提出了一种将研发项目管理方法与公平能力结合的框架，以解决机器学习和人工智能方法中存在的偏见和歧视问题。

    

    机器学习（ML）和人工智能（AI）的方法往往会复制和放大现有的偏见和成见，AI机器人也是如此。例如，具备面部识别功能的机器人未能将黑人女性识别为人类，而其他机器人则仅根据外表将人们，如黑人男性，归类为罪犯。在“AI供应链”中，一种“模块化文化”意味着伤害被认为是“超出范围”的，或者是别人的责任。事件的发生是 routine enough（incidentdatabase.ai 列举了2000多个例子）以表明很少有组织能够完全尊重人们的权利；实现所声称的公平性、多样性和包容性（EDI或DEI）目标；或者识别然后解决这些组织和工件中的失败。我们提出了一个框架，用于调整广泛实践的研发项目管理方法，以建立组织的公平能力并更好地整合

    Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to replicate and amplify existing biases and prejudices, as do Robots with AI. For example, robots with facial recognition have failed to identify Black Women as human, while others have categorized people, such as Black Men, as criminals based on appearance alone. A 'culture of modularity' means harms are perceived as 'out of scope', or someone else's responsibility, throughout employment positions in the 'AI supply chain'. Incidents are routine enough (incidentdatabase.ai lists over 2000 examples) to indicate that few organizations are capable of completely respecting peoples' rights; meeting claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and then addressing such failures in their organizations and artifacts. We propose a framework for adapting widely practiced Research and Development (R&D) project management methodologies to build organizational equity capabilities and better integr
    
[^59]: BERT4FCA：一种使用形式概念分析和BERT进行二部图链接预测的方法

    BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT

    [https://arxiv.org/abs/2402.08236](https://arxiv.org/abs/2402.08236)

    BERT4FCA是一种使用形式概念分析和BERT进行二部图链接预测的新方法，利用FCA提取的最大双向团的丰富信息，提高了链接预测性能。

    

    我们提出了一种名为BERT4FCA的新方法，用于在二部图网络中进行链接预测，该方法使用了形式概念分析（FCA）和BERT。二部图网络中的链接预测是一个重要的任务，可以解决社交网络中的好友推荐和作者-论文网络中的合作预测等各种实际问题。最近的研究发现，在二部图网络中，最大的双向团提供了重要的信息，可以通过FCA来提取它们。一些基于FCA的二部图链接预测方法已经取得了良好的性能。然而，我们发现这些方法的性能可以进一步提高，因为它们没有充分捕捉到提取的最大双向团的丰富信息。为了解决这个限制，我们提出了一种使用BERT的方法，它可以从FCA提取的最大双向团中学习更多信息，并利用它们进行链接预测。我们在三个真实的二部图网络上进行了实验，并展示了其效果。

    We propose BERT4FCA, a novel method for link prediction in bipartite networks, using formal concept analysis (FCA) and BERT. Link prediction in bipartite networks is an important task that can solve various practical problems like friend recommendation in social networks and co-authorship prediction in author-paper networks. Recent research has found that in bipartite networks, maximal bi-cliques provide important information for link prediction, and they can be extracted by FCA. Some FCA-based bipartite link prediction methods have achieved good performance. However, we figured out that their performance could be further improved because these methods did not fully capture the rich information of the extracted maximal bi-cliques. To address this limitation, we propose an approach using BERT, which can learn more information from the maximal bi-cliques extracted by FCA and use them to make link prediction. We conduct experiments on three real-world bipartite networks and demonstrate th
    
[^60]: 研究GNN的超分布推广：从架构角度的视角

    Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective

    [https://arxiv.org/abs/2402.08228](https://arxiv.org/abs/2402.08228)

    这项研究从架构的角度全面调查了图的超分布推广，揭示了图自我注意机制和其他常见构建模块在超分布问题上的影响。

    

    图神经网络（GNN）在测试数据来自于训练数据相同分布的假设下表现出了出色的性能。然而，在真实场景中，这个假设可能并不总是成立。因此，在图的上下文中，对超分布（OOD）问题的探索日益受到关注。大部分现有的研究主要集中在改进图的OOD推广的两个“模型无关”角度上：数据驱动方法和策略学习。然而，对于已知的GNN模型架构对图的OOD推广的影响的研究相对较少，这与现有的研究相互独立。在这项工作中，我们从架构的角度首次全面调查了图的OOD推广，并对现代GNN的常见构建模块进行了考察。通过大量实验，我们揭示了图自我注意机制和...

    Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an
    
[^61]: BBox-Adapter: 轻量级适配黑盒大型语言模型

    BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models

    [https://arxiv.org/abs/2402.08219](https://arxiv.org/abs/2402.08219)

    BBox-Adapter是一种适用于黑盒大型语言模型的轻量级适配器，通过区分目标和源域数据，并采用排名式噪音对比估计（NCE）损失和在线适应机制，实现了在透明、隐私和成本方面的有效适应。

    

    适应最先进的大型语言模型（LLMs），如GPT-4和Gemini，以满足特定任务的要求是具有挑战性的。由于它们的参数、嵌入和输出概率的不透明性，现有的微调适应方法是不适用的。因此，只能通过它们的API服务适应这些黑盒LLMs，这引发了透明度、隐私和成本的担忧。为了解决这些挑战，我们介绍了BBox-Adapter，一种新颖的适用于黑盒LLMs的轻量级适配器。BBox-Adapter通过将目标数据视为正样本，将源数据视为负样本来区分目标和源域数据。它采用基于排名的噪音对比估计（NCE）损失来提高目标域数据的可能性，同时惩罚源域数据的可能性。此外，它还具有在线适应机制，该机制将来自真实数据、人类或AI反馈的实时正样本采样与先前适应的负样本数据相结合。广泛的实验表明，BBox-Adapter在不降低性能的同时，提供了高效而灵活的黑盒LLMs适应解决方案。

    Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
    
[^62]: 当在人类工作记忆任务上训练时，Transformer机制模仿额顶回路封锁操作

    Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks

    [https://arxiv.org/abs/2402.08211](https://arxiv.org/abs/2402.08211)

    本研究分析了基于Transformer的神经网络在人类工作记忆任务上训练时所出现的机制，揭示了这种模型如何解决复杂的认知分支任务，并探讨了这些机制与人脑封锁机制的相似性。

    

    基于Transformer神经网络架构的模型在许多需要复杂的“认知分支”（或在实现其他目标的同时保持对一个目标的追求）的任务上取得了成功。在认知神经科学中，成功完成这样的任务被认为依赖于复杂的额顶回路机制，这些机制通过选择性“封锁”实现了对信息的更新和读取，这些信息以神经元团簇的形式存储在记忆的不同“地址”上。然而，Transformer模型并没有有意地内置这样的机制。因此，如何解决这些任务以及为此而出现的机制是否与人脑的封锁机制相似都是一个开放问题。在这项工作中，我们分析了在仅使用注意力机制的基本Transformer上训练一个受到工作记忆研究任务启发的简单序列建模任务中出现的机制。

    Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex "cognitive branching" -- or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct "addresses" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working mem
    
[^63]: 利用多臂赌博机的阈值数据Shapley进行数据清洗

    Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits

    [https://arxiv.org/abs/2402.08209](https://arxiv.org/abs/2402.08209)

    本文提出了一种利用阈值赌博机算法快速识别具有低数据Shapley值的实例子集的迭代方法，从而提高数据清洗的计算速度，同时保持模型性能。

    

    数据清洗旨在通过从训练数据集中删除一组有害实例来提高模型性能。数据Shapley是一种常见的理论上保证的方法，用于评估每个实例对模型性能的贡献；然而，它需要在训练数据的所有子集上进行训练，这在计算上是昂贵的。在本文中，我们提出了一种使用阈值赌博机算法快速识别具有低数据Shapley值的实例子集的迭代方法。我们提供了一个理论保证，即如果进行足够多的迭代，所提出的方法可以准确选择有害实例。使用不同模型和数据集的实证评估表明，所提出的方法在保持模型性能的同时有效地提高了计算速度。

    Data cleansing aims to improve model performance by removing a set of harmful instances from the training dataset. Data Shapley is a common theoretically guaranteed method to evaluate the contribution of each instance to model performance; however, it requires training on all subsets of the training data, which is computationally expensive. In this paper, we propose an iterativemethod to fast identify a subset of instances with low data Shapley values by using the thresholding bandit algorithm. We provide a theoretical guarantee that the proposed method can accurately select harmful instances if a sufficiently large number of iterations is conducted. Empirical evaluation using various models and datasets demonstrated that the proposed method efficiently improved the computational speed while maintaining the model performance.
    
[^64]: 自动驾驶应用中基于人工智能的软件元素固有多样化冗余安全机制

    Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications

    [https://arxiv.org/abs/2402.08208](https://arxiv.org/abs/2402.08208)

    本文研究了自动驾驶系统中基于人工智能的软件元素的作用和挑战，探讨了泛化问题以及过度自信的AI模型所带来的风险，并提出了解决方法。

    

    本文探讨了人工智能算法在自动驾驶系统中的作用和挑战，特别是基于人工智能的软件元素。这些人工智能系统在复杂和高维环境中执行实时关键功能，处理多模态感知、认知和决策任务，如运动规划、车道保持和紧急制动。一个主要关注点是AI模型在初始训练数据之外如何进行泛化。这种泛化问题在实时场景中变得明显，模型经常遇到不在其训练或验证数据中表示的输入。在这种情况下，尽管面临分布或领域转移，AI系统仍必须有效地运行。本文调查了在自动驾驶等安全关键应用中，过度自信的AI模型带来的风险。为了减轻这些风险，本文提出了训练AI模型的一些方法。

    This paper explores the role and challenges of Artificial Intelligence (AI) algorithms, specifically AI-based software elements, in autonomous driving systems. These AI systems are fundamental in executing real-time critical functions in complex and high-dimensional environments. They handle vital tasks like multi-modal perception, cognition, and decision-making tasks such as motion planning, lane keeping, and emergency braking. A primary concern relates to the ability (and necessity) of AI models to generalize beyond their initial training data. This generalization issue becomes evident in real-time scenarios, where models frequently encounter inputs not represented in their training or validation data. In such cases, AI systems must still function effectively despite facing distributional or domain shifts. This paper investigates the risk associated with overconfident AI models in safety-critical applications like autonomous driving. To mitigate these risks, methods for training AI m
    
[^65]: PSC-CPI: 多尺度蛋白质序列-结构对比用于高效且具有可推广性的化合物-蛋白质相互作用预测

    PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction

    [https://arxiv.org/abs/2402.08198](https://arxiv.org/abs/2402.08198)

    PSC-CPI是一种多尺度蛋白质序列-结构对比框架，通过内模态和跨模态对比捕获蛋白质序列和结构之间的依赖关系，用于高效且具有可推广性的化合物-蛋白质相互作用预测。

    

    化合物-蛋白质相互作用（CPI）预测旨在预测化合物-蛋白质相互作用的模式和强度，以用于理性药物发现。现有的基于深度学习的方法仅利用蛋白质序列或结构的单一模态，缺乏对两个模态的联合分布进行共同建模，这可能导致在复杂的实际场景中由于各种因素（例如，模态丢失和领域转移）而出现显著的性能下降。更重要的是，这些方法仅以单一固定尺度模拟蛋白质序列和结构，忽视了更精细的多尺度信息，例如嵌入在关键蛋白质片段中的信息。在本文中，我们提出了一种新颖的多尺度蛋白质序列-结构对比框架用于CPI预测（PSC-CPI），通过内模态和跨模态对比捕获蛋白质序列和结构之间的依赖关系。我们还应用可变长度的蛋白质增强技术，以允许对不同长度的蛋白质进行建模。

    Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery. Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting. More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments. In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting. We further apply length-variable protein augmentation to allow
    
[^66]: THE COLOSSEUM：用于评估机器人操作泛化性的基准测试

    THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation

    [https://arxiv.org/abs/2402.08191](https://arxiv.org/abs/2402.08191)

    THE COLOSSEUM是一个新的模拟基准测试，用于评估机器人操作的泛化性能。它包括20个不同的操作任务，在12个环境干扰轴上进行系统评估。研究发现，四个最先进的操作模型在干扰因素下的成功率下降了30-50%。改变干扰对象数量、目标对象颜色或光照条件会对模型的性能产生重要影响。

    

    为了实现有效的大规模、现实世界的机器人应用，我们必须评估我们的机器人策略在环境条件变化时的适应能力。不幸的是，大多数研究评估机器人在与训练设置非常相似甚至相同的环境中的性能。我们提出了一个新颖的模拟基准测试THE COLOSSEUM，其中包括20个不同的操作任务，可以对模型在12个环境干扰轴上进行系统评估。这些干扰包括物体、桌面和背景的颜色、纹理和大小的变化；我们还改变了光照、干扰因素和相机姿态。使用THE COLOSSEUM，我们比较了4个最先进的操作模型，发现它们的成功率在这些干扰因素下下降了30-50%。当多个干扰同时应用时，成功率下降至≥75%。我们确定了改变干扰对象数量、目标对象颜色或光照条件会对模型的性能产生重要影响。

    To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions ar
    
[^67]: 推动基于数据驱动的天气预报：ERA5的时间滑动数据增强

    Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5

    [https://arxiv.org/abs/2402.08185](https://arxiv.org/abs/2402.08185)

    本研究介绍了一种新颖的基于数据驱动的天气预报策略，利用低分辨率数据进行全球天气预报和气候数据分析。通过使用自适应傅里叶神经算子（AFNO）模型和时间滑动方法扩充数据集，本研究改进了传统方法，添加了更多变量和新的方法。

    

    现代深度学习技术以模仿传统数值天气预报模型并从全球大气再分析数据派生而来，在几年内引起了重大革命。在这种新范式中，我们的研究引入了一种新颖的策略，不再依赖于高分辨率数据（通常受计算资源限制），而是利用低分辨率数据（2.5度）进行全球天气预报和气候数据分析。我们的主要关注点是评估基于数据驱动的天气预报（DDWP）框架，特别关注样本大小的充分性、模型的结构改进以及气候数据表达当前气候趋势的能力。通过使用自适应傅里叶神经算子（AFNO）模型通过FourCastNet和提出的时间滑动方法来扩充ECMWF再分析v5（ERA5）数据集，本文通过添加更多变量和新颖的方法改进了传统方法。

    Modern deep learning techniques, which mimic traditional numerical weather prediction (NWP) models and are derived from global atmospheric reanalysis data, have caused a significant revolution within a few years. In this new paradigm, our research introduces a novel strategy that deviates from the common dependence on high-resolution data, which is often constrained by computational resources, and instead utilizes low-resolution data (2.5 degrees) for global weather prediction and climate data analysis. Our main focus is evaluating data-driven weather prediction (DDWP) frameworks, specifically addressing sample size adequacy, structural improvements to the model, and the ability of climate data to represent current climatic trends. By using the Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5), this paper improves on conventional approaches by adding more variables and a novel approa
    
[^68]: 通过场景无关表示实现多智能体转移强化学习

    Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation

    [https://arxiv.org/abs/2402.08184](https://arxiv.org/abs/2402.08184)

    本研究引入了一种新的框架，通过将各种状态空间统一为固定大小的输入，实现了在多智能体系统中进行转移学习的能力。在SMAC环境中的实验结果表明，通过学习机动技能获得的知识可以显著提高多智能体的学习性能。

    

    多智能体强化学习（MARL）算法广泛应用于解决在动态的多智能体系统中需要合作和竞争的复杂任务。然而，从头开始学习这种任务是困难且可能不可行的，尤其对于具有大量交互智能体的多智能体系统而言，由于样本复杂性极高。因此，重复使用过去经验或其他智能体获得的知识可以有效加快学习过程并提升MARL算法。在本研究中，我们介绍了一种新颖的框架，通过将各种状态空间统一为固定大小的输入，实现了MARL的转移学习，从而在多智能体系统的不同场景中实现了可行的统一深度学习策略。我们在StarCraft多智能体挑战（SMAC）环境中评估了我们的方法，并发现使用通过机动技能学习得到的知识的多智能体学习性能大幅提升。

    Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned fr
    
[^69]: LoTa-Bench: 基于语言的任务规划器对于具身代理的基准测试

    LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents

    [https://arxiv.org/abs/2402.08178](https://arxiv.org/abs/2402.08178)

    LoTa-Bench是一个用于评估具身代理任务规划性能的基准系统，通过对LLMs和提示进行实验，加速语言导向任务规划器的开发。

    

    大型语言模型（LLMs）最近作为任务规划的替代解决方案受到了广泛关注。然而，比较语言导向的任务规划器的性能变得困难，关于预训练模型选择和提示构建等各种因素的详细探索也缺乏。为了解决这个问题，我们提出了一个基准系统，用于自动量化家庭服务具身代理的任务规划性能。任务规划器在两对数据集和模拟器上进行测试：1）ALFRED和AI2-THOR，2）Watch-And-Help和VirtualHome的扩展。使用提出的基准系统，我们对LLMs和提示进行了广泛的实验，并探索了基线规划器的几种改进。我们期望提出的基准工具将加速语言导向任务规划器的开发。

    Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.
    
[^70]: 使用地标和聚类的层级位置嵌入图形用于链接预测

    Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction

    [https://arxiv.org/abs/2402.08174](https://arxiv.org/abs/2402.08174)

    本论文提出了一种使用地标和聚类的层级位置嵌入方法用于链接预测任务。通过选择具有高度中心度的节点作为地标和进行图聚类，本方法有效地将位置信息嵌入到图中，提高了链接预测的准确性和性能。

    

    学习图中节点的位置信息对于链接预测任务非常重要。我们提出了使用代表性节点（称为地标）来表示位置信息的方法。我们选择少量具有高度中心度的节点作为地标，它们作为节点位置的参考点。我们证明了这种选择策略对于众所周知的随机图模型是合理的，并推导出涉及地标的平均路径长度的闭合形式上界。在幂律图的模型中，我们证明了地标为节点之间距离提供了渐近完全准确的信息。我们将理论洞察力应用于实际网络，并提出了具有地标和聚类的层级位置嵌入（HPLC）方法。HPLC将地标选择和图聚类相结合，其中图被分割为连通密集的聚类，选择具有最高度中心度的节点作为地标。HPLC利用了基于地标的节点位置信息的层级性。

    Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various l
    
[^71]: LLaGA: 大型语言和图形助手

    LLaGA: Large Language and Graph Assistant

    [https://arxiv.org/abs/2402.08170](https://arxiv.org/abs/2402.08170)

    LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。

    

    图神经网络（GNN）已经推动了图结构数据分析的进步。最近，大型语言模型（LLM）如GPT-4的崛起预示着深度学习的一个新时代。然而，将它们应用于图数据还面临着独特的挑战，由于将图结构转化为文本的固有难度。为此，我们引入了一个创新模型——大型语言和图形助手（LLaGA），它有效地整合了LLM的能力，以处理图结构数据的复杂性。LLaGA保留了LLM的通用性，同时将图数据转化为与LLM输入兼容的格式。LLaGA通过重新组织图节点以作为结构感知的序列，然后通过一个多功能投影仪将其映射到标记嵌入空间中。LLaGA在多样性、泛化性和可解释性方面表现出色，使其能够在不同数据集和任务上表现出一致的良好性能。

    Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
    
[^72]: 关于Transformer架构的限制

    On Limitations of the Transformer Architecture

    [https://arxiv.org/abs/2402.08164](https://arxiv.org/abs/2402.08164)

    本论文通过通信复杂性证明了Transformer层在处理函数组合任务时的局限性，指出对于大型定义域和某些数学任务，Transformers可能无法解决。

    

    大型语言模型（LLMs）中幻觉的根本原因是什么？我们使用通信复杂性来证明，如果函数的定义域足够大，Transformer层无法组合函数（例如，在家谱中查找一个人的祖父）；我们通过示例显示，当定义域相当小的时候，这种能力的缺乏已经在经验上存在。我们还指出，许多在所谓的组合任务中的数学任务，认为它们对LLMs来说很难解决，对于足够大的实例来说，且假设计算复杂性领域的某些被广泛接受的猜想是正确的，Transformers也不太可能解决。

    What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
    
[^73]: CMA-R：用于解释谣言检测的因果中介分析

    CMA-R:Causal Mediation Analysis for Explaining Rumour Detection

    [https://arxiv.org/abs/2402.08155](https://arxiv.org/abs/2402.08155)

    CMA-R通过因果中介分析解释了神经模型在Twitter上进行谣言检测的决策过程，并能够识别出关键推文和因果影响单词，提高了对黑盒子谣言检测系统的解释性和透明度。

    

    我们将因果中介分析应用于解释神经模型在Twitter上进行谣言检测的决策过程。在输入和网络层面进行干预可以揭示模型输出中推文和单词的因果影响。我们发现我们的方法CMA-R - 因果中介分析用于谣言检测 - 可以识别出解释模型预测并与人类判断关于故事真实性的关键推文的显著性推文，并展示出强烈的一致性。CMA-R还可以突出显著性推文中具有因果影响的单词，提供对这些黑盒子谣言检测系统的另一层解释性和透明度。代码可在此处找到：https://github.com/ltian678/cma-r.

    We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter. Interventions at the input and network level reveal the causal impacts of tweets and words in the model output. We find that our approach CMA-R -- Causal Mediation Analysis for Rumour detection -- identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories. CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems. Code is available at: https://github.com/ltian678/cma-r.
    
[^74]: 渐变流自适应重要性抽样用于sigmoid分类模型的贝叶斯留一交叉验证

    Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models

    [https://arxiv.org/abs/2402.08151](https://arxiv.org/abs/2402.08151)

    本研究引入了渐变流自适应重要性抽样的方法，用于稳定贝叶斯分类模型的留一交叉验证预测的蒙特卡罗近似，以评估模型的普适性。

    

    我们引入了一组梯度流引导的自适应重要性抽样（IS）变换，用于稳定贝叶斯分类模型的点级留一交叉验证（LOO）预测的蒙特卡罗近似。可以利用这种方法来评估模型的普适性，例如计算与AIC类似的LOO或计算LOO ROC / PRC曲线以及派生的度量指标，如AUROC和AUPRC。通过变分法和梯度流，我们推导出两个简单的非线性单步变换，利用梯度信息将模型的预训练完整数据后验靠近目标LOO后验预测分布。这样，变换稳定了重要性权重。因为变换涉及到似然函数的梯度，所以结果的蒙特卡罗积分依赖于模型Hessian的Jacobian行列式。我们推导出了这些Jacobian行列式的闭合精确公式。

    We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
    
[^75]: 使用大型语言模型和蒙特卡洛树搜索进行验证的多步合成

    Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search

    [https://arxiv.org/abs/2402.08147](https://arxiv.org/abs/2402.08147)

    本文提出了一种使用蒙特卡洛树搜索引导大型语言模型生成验证程序的方法，通过结合验证器的反馈和LLM先验知识提高了合成能力，实验证明这种方法在一组验证编程问题上的性能优于基本模型和具有插件的ChatGPT4。

    

    我们提出了一种使用蒙特卡洛树搜索（MCTS）引导大型语言模型（LLMs）生成在Dafny、Lean和Coq中验证的程序的方法。我们的方法被称为VMCTS，通过在每个步骤检查部分程序来利用搜索算法中的验证器。结合LLM先验知识，验证器的反馈提高了开源模型的合成能力。在一组五个经过验证的编程问题中，我们发现在四个问题中，即使重新对解决方案进行一小时的重新采样，基本模型无法解决问题，而VMCTS可以在6分钟内解决这些问题。在这些问题上，基本模型加上VMCTS甚至与具有插件和多次重试的ChatGPT4竞争力相当。我们的代码和基准测试结果可在https://github.com/namin/llm-verified-with-monte-carlo-tree-search找到。

    We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .
    
[^76]: 在非稳态环境中进行通用规划和学习的认识性探索

    Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings

    [https://arxiv.org/abs/2402.08145](https://arxiv.org/abs/2402.08145)

    本文介绍了一种在非稳态环境中进行通用规划和学习的方法，通过认识性探索填补代理的知识空白，并且利用收集到的数据学习通用的概率模型来解决不断变化的任务。实验证明这种方法在非稳态环境中比传统方法更有效。

    

    本文介绍了一种在使用关系表示的非稳态随机环境中进行连续规划和模型学习的新方法。这样的能力对于在不确定、不断发展的现实世界中部署顺序决策系统至关重要。在未知（且非稳态）转换系统和不断变化的任务中工作时，提出的框架模拟了代理状态知识的空白，并将其用于开展专注、调查性的探索。使用这些探索收集的数据用于学习通用的概率模型，以解决当前的任务，尽管环境动态不断变化。对几个基准领域进行的实证评估表明，这种方法在非稳态环境中的样本复杂度方面明显优于规划和强化学习基准。理论结果表明，系统能够回归到表现出理想的收敛性。

    This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings. Theoretical results show that the system reverts to exhibit desirable convergence pr
    
[^77]: 迭代投票的平均情况分析

    Average-Case Analysis of Iterative Voting

    [https://arxiv.org/abs/2402.08144](https://arxiv.org/abs/2402.08144)

    这项工作通过分析代理人偏好分布的平均情况，扩展了迭代投票模型的效果分析。并且区分了迭代多数制何时改善或降低渐近福利。

    

    迭代投票是社会选择中重复战略决策的自然模型，当代理可以在最终确定群体决策之前更新他们的投票时。之前的研究通过对无序文化下代理人偏好的最坏情况和平均情况表现进行分析，通过改进安纳基价格来分析迭代多数制对平衡点选出的结果福利的有效性。然而，之前的分析只研究了在代理人偏好通过无偏文化分布的最坏情况和平均情况下的性能。本研究将平均情况分析扩展到更广泛的分布类，并区分出迭代多数制何时改善或降低渐近福利。

    Iterative voting is a natural model of repeated strategic decision-making in social choice when agents have the opportunity to update their votes prior to finalizing the group decision. Prior work has analyzed the efficacy of iterative plurality on the welfare of the chosen outcome at equilibrium, relative to the truthful vote profile, via an adaptation of the price of anarchy. However, prior analyses have only studied the worst-case and average-case performances when agents' preferences are distributed by the impartial culture. This work extends average-case analyses to a wider class of distributions and distinguishes when iterative plurality improves or degrades asymptotic welfare.
    
[^78]: 递归协同模拟在游戏中的应用

    Recursive Joint Simulation in Games

    [https://arxiv.org/abs/2402.08128](https://arxiv.org/abs/2402.08128)

    本文研究了游戏中AI代理之间的递归协同模拟的互动方式，并证明了这种方式与原始游戏的无限重复版本在战略上是等价的。

    

    人工智能(AI)代理之间的博弈动态与传统的人-人互动可能存在各种不同之处。其中一个区别是可能能够准确地模拟AI代理，例如因为其源代码是已知的。我们的目标是探索利用这种可能性在战略设置中实现更合作的结果的方法。在本文中，我们研究了AI代理之间运行递归协同模拟的互动。即，代理首先共同观察他们所面对情境的模拟。这种模拟反过来递归地包括了额外的模拟（为了避免无限递归，具有小概率的失败），并且在选择行动之前观察所有这些嵌套模拟的结果。我们证明，由此产生的互动在战略上等价于原始游戏的无限重复版本，从而可以直接转移诸如各种民间定理等现有结果。

    Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known. Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings. In this paper, we study an interaction between AI agents where the agents run a recursive joint simulation. That is, the agents first jointly observe a simulation of the situation they face. This simulation in turn recursively includes additional simulations (with a small chance of failure, to avoid infinite recursion), and the results of all these nested simulations are observed before an action is chosen. We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems.
    
[^79]: 可自定义扰动合成用于鲁棒性SLAM基准测试

    Customizable Perturbation Synthesis for Robust SLAM Benchmarking

    [https://arxiv.org/abs/2402.08125](https://arxiv.org/abs/2402.08125)

    我们提出了一种可定制化的噪声数据合成流程，用于评估多模态SLAM模型对各种扰动的鲁棒性。该流程结合了可定制化硬件配置、软件组件和被扰动环境，引入了全面的扰动分类法以及扰动组合工具箱，可以将干净的仿真转化为具有挑战性的嘈杂环境。我们还建立了鲁棒-SLAM基准测试。

    

    鲁棒性是机器人在非结构化环境中成功部署的关键因素，特别在同时定位和建图（SLAM）领域。与真实世界数据收集相比，基于仿真的基准测试已成为一种高度可扩展的评估鲁棒性的方法。然而，创建具有多样化扰动的具有挑战性且可控的嘈杂环境仍然相对未开发。为此，我们提出了一种新颖的可定制化的噪声数据合成流程，旨在评估多模态SLAM模型对各种扰动的鲁棒性。该流程结合了可定制化硬件配置、软件组件和被扰动环境。特别地，我们引入了全面的扰动分类法以及扰动组合工具箱，可以将干净的仿真转化为具有挑战性的嘈杂环境。利用该流程，我们建立了鲁棒-SLAM基准测试，其中包括多模态的扰动合成和鲁棒性评估指标。

    Robustness is a crucial factor for the successful deployment of robots in unstructured environments, particularly in the domain of Simultaneous Localization and Mapping (SLAM). Simulation-based benchmarks have emerged as a highly scalable approach for robustness evaluation compared to real-world data collection. However, crafting a challenging and controllable noisy world with diverse perturbations remains relatively under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. This pipeline incorporates customizable hardware setups, software components, and perturbed environments. In particular, we introduce comprehensive perturbation taxonomy along with a perturbation composition toolbox, allowing the transformation of clean simulations into challenging noisy environments. Utilizing the pipeline, we instantiate the Robust-SLAM benchmark, which includes divers
    
[^80]: 关于大型语言模型在推理和规划任务中的自我验证限制问题

    On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks

    [https://arxiv.org/abs/2402.08115](https://arxiv.org/abs/2402.08115)

    这项研究对大型语言模型在推理和规划任务中的自我验证能力进行了系统研究，并发现了迭代提示的有效性。

    

    关于大型语言模型（LLMs）的推理能力，存在着较大的观点差异。尽管最初对于推理可能会随着规模的扩大自动出现的乐观情绪已经受到了一系列反例的抑制，从乘法到简单规划，但仍然普遍认为LLMs可以自我批判并迭代改进其解决方案。这种信念似乎建立在验证正确性比生成更容易的假设上，这是一个典型的计算复杂性论证，对于LLMs来说应该是无关紧要的，因为它们所做的是近似检索。在本文中，我们系统地研究了推理和规划环境中迭代提示的有效性。 我们对GPT-4在三个领域（24点游戏、图着色和STRIPS规划）的性能进行了有原则的实证研究。我们对模型的批判性实验进行了探索。

    There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiq
    
[^81]: 大型语言模型的主动偏好学习

    Active Preference Learning for Large Language Models

    [https://arxiv.org/abs/2402.08114](https://arxiv.org/abs/2402.08114)

    本论文提出了一种用于大型语言模型的主动偏好学习策略，通过直接偏好优化（DPO）来更好地利用偏好标签。实验结果表明，该方法提高了基于成对偏好数据的微调的学习速度和最终性能。

    

    随着大型语言模型（LLM）的能力越来越强，与人类意图对齐的微调技术变得越来越重要。对于对齐这些模型来说，最关键的考虑是如何最有效地利用人力资源，或者在LLM本身被用作oracle的情况下如何最有效地利用模型资源。从人类或AI偏好中进行强化学习（RLHF / RLAIF）是这种技术最突出的例子，但它往往复杂且不稳定。最近，直接偏好优化（DPO）被提出作为一个更简单和更稳定的替代方法。在这项工作中，我们开发了一种DPO的主动学习策略，以更好地利用偏好标签。我们提出了一个基于语言模型的预测熵和DPO优化的隐式偏好模型的确定性度量的实用采集函数，展示了我们的方法如何提高基于成对偏好数据的微调的学习速度和最终性能。

    As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
    
[^82]: 一种在microRTS中获奖的深度强化学习代理

    A Competition Winning Deep Reinforcement Learning Agent in microRTS

    [https://arxiv.org/abs/2402.08112](https://arxiv.org/abs/2402.08112)

    在IEEE microRTS竞赛中，RAISocketAI成为第一个获胜的深度强化学习代理，它通过逐步优化基本策略和迁移学习来击败了前两位竞赛获胜者，在未来的竞赛中可以作为基准参考，并为DRL研究提供起点。

    

    在CIG和CoG举办的IEEE microRTS（$\mu$RTS）竞赛的五届中，脚本代理主导了比赛。尽管深度强化学习（DRL）算法在实时策略（RTS）游戏中取得了重大进展，但由于需要大量的培训资源以及创建和调试此类代理所固有的复杂性，它们在这个主要是学术竞赛中的采用仍然有限。RAISocketAI是第一个在IEEE microRTS竞赛中获胜的DRL代理。在一个没有性能限制的基准测试中，RAISocketAI经常击败前两位竞赛获胜者。这个第一个获胜的DRL提交可以成为未来microRTS竞赛的基准，并成为未来DRL研究的起点。逐步优化基本策略和对特定地图进行迁移学习对RAISocketAI的获胜表现至关重要。这些策略可以用于经济训练未来的DRL代理。在模仿学习方面的进一步工作可以进一步提高DRL代理的性能。

    Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation L
    
[^83]: 使用统计过程控制的离群数据检测和数据漂移监测方法

    Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control

    [https://arxiv.org/abs/2402.08088](https://arxiv.org/abs/2402.08088)

    该论文提出了一种使用统计过程控制的机器学习框架，用于检测离群数据和监测数据漂移。该框架在临床环境中具有重要应用价值，能够帮助提高ML设备在放射学图像中的性能和患者安全。

    

    背景：机器学习（ML）方法往往在数据偏离其训练分布时失效。这对于临床环境中的ML设备来说是一个重要的问题，因为数据漂移可能导致意外的性能下降，危及患者安全。方法：我们提出了一种基于ML的统计过程控制（SPC）框架，用于离群数据检测和漂移监测。SPC优势在于能够直观地和统计上突出显示与预期分布的偏差。为了证明所提出的框架在放射学图像的数据漂移监测中的实用性，我们考察了不同的设计选择，包括特征提取方法、漂移量化和SPC参数选择等。结果：我们展示了我们的框架在两个任务中的有效性：1）区分轴向与非轴向的计算机断层摄影（CT）图像；2）将胸部X光（CXR）与其他模态进行分离。

    Background: Machine learning (ML) methods often fail with data that deviates from their training distribution. This is a significant concern for ML-enabled devices in clinical settings, where data drift may cause unexpected performance that jeopardizes patient safety.   Method: We propose a ML-enabled Statistical Process Control (SPC) framework for out-of-distribution (OOD) detection and drift monitoring. SPC is advantageous as it visually and statistically highlights deviations from the expected distribution. To demonstrate the utility of the proposed framework for monitoring data drift in radiological images, we investigated different design choices, including methods for extracting feature representations, drift quantification, and SPC parameter selection.   Results: We demonstrate the effectiveness of our framework for two tasks: 1) differentiating axial vs. non-axial computed tomography (CT) images and 2) separating chest x-ray (CXR) from other modalities. For both tasks, we achie
    
[^84]: 信息绕行：一种简单且有效的用于表达图学习的循环表示方法

    Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning

    [https://arxiv.org/abs/2402.08085](https://arxiv.org/abs/2402.08085)

    "信息绕行"是一种用于层次性表征图中循环的方法，通过比较最短路径和最长路径之间的对比性，实现了与高阶"Weisfeiler-Lehman"（WL）测试相当的表达能力，但计算需求更少。

    

    图学习在生物信息学、社交网络和化学领域中至关重要。尽管高阶图形特征，如循环，对于节点分类、边预测和图像识别至关重要，但对高阶拓扑特征进行建模在计算上面临着困难，限制了其在机器学习中的广泛应用。为了解决这个问题，我们引入了"信息绕行"的概念，以在整个图中层次性地表征循环，利用每个图节点相关的一系列局部拓扑结构中最短路径和最长路径之间的对比性。我们从信息绕行景观中得到的拓扑特征表示具有与高阶"Weisfeiler-Lehman"（WL）测试相当的表达能力，但计算需求更少。除了与图核和信息的集成外

    Graph learning is crucial in the fields of bioinformatics, social networks, and chemicals. Although high-order graphlets, such as cycles, are critical to achieving an informative graph representation for node classification, edge prediction, and graph recognition, modeling high-order topological characteristics poses significant computational challenges, restricting its widespread applications in machine learning. To address this limitation, we introduce the concept of \textit{message detouring} to hierarchically characterize cycle representation throughout the entire graph, which capitalizes on the contrast between the shortest and longest pathways within a range of local topologies associated with each graph node. The topological feature representations derived from our message detouring landscape demonstrate comparable expressive power to high-order \textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In addition to the integration with graph kernel and message
    
[^85]: 高效可扩展的基因理解语言模型微调方法

    Efficient and Scalable Fine-Tune of Language Models for Genome Understanding

    [https://arxiv.org/abs/2402.08075](https://arxiv.org/abs/2402.08075)

    这篇论文提出了一种名为Lingo的高效可扩展的基因理解语言模型微调方法，该方法利用了自然语言模型的上下文提示，并通过自适应秩采样方法适应了基因组注释的多样性任务。

    

    尽管DNA基因组模型在基因理解方面取得了进展，但在基因组数据规模和多样性方面仍面临重大挑战。这种限制与自然语言模型相比形成鲜明对比，后者在更大规模上取得了成功。此外，基因理解涉及许多具有固有数据异质性的下游基因组注释任务，因此需要更高效且稳健的针对基因组的微调方法。在这里，我们提出了Lingo：语言前缀微调基因组模型。与DNA基因组模型不同，Lingo策略性地利用了自然语言模型的上下文提示，重新校准其在基因组序列方面的语言知识。Lingo通过自适应秩采样方法进一步适应了许多不同的、异质的下游微调任务，该方法剪枝并随机重新引入剪枝的奇异向量。

    Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \textsc{Lingo}: \textsc{L}anguage prefix f\textsc{In}e-tuning for \textsc{G}en\textsc{O}mes. Unlike DNA foundation models, \textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors w
    
[^86]: 实时使用生成式人工智能改进编程错误信息

    Enhancing Programming Error Messages in Real Time with Generative AI

    [https://arxiv.org/abs/2402.08072](https://arxiv.org/abs/2402.08072)

    本研究通过使用生成式人工智能改进编程错误信息，实时为学生提供帮助，并发现界面设计对于反馈的可用性起着关键作用。

    

    生成式人工智能正在改变许多学科的教学方式，包括计算机科学。研究人员已经表明，生成式人工智能工具能够解决编程问题、编写大量代码块，并以简单的术语解释复杂的代码。在使用生成式人工智能增强编程错误信息方面显示出了特别的潜力。学生和教师多年来一直抱怨这些信息往往晦涩难懂。然而最近的研究表明，通过GPT-4改进的消息使学生的错误次数减少。我们通过将ChatGPT的反馈添加到我们的自动化评估工具Athene中，为所有提交的程序提供对编译器、运行时和逻辑错误的帮助，对这项工作进行了扩展。我们的结果表明，向自动化评估工具添加生成式人工智能并不一定使其变得更好，接口的设计对GPT-4提供的反馈的可用性有重大影响。

    Generative AI is changing the way that many disciplines are taught, including computer science. Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using generative AI to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4. We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided.
    
[^87]: 超越LLMs：推进复杂推理的发展

    Beyond LLMs: Advancing the Landscape of Complex Reasoning

    [https://arxiv.org/abs/2402.08064](https://arxiv.org/abs/2402.08064)

    在人工智能领域，大型语言模型(LLMs)一直被视为解决许多问题的标准解决方案。然而，对于约束满足和优化问题，LLMs表现不佳。因此，Elemental Cognition开发了EC AI平台，采用神经符号方法解决这些问题，同时利用LLMs进行知识获取和用户交互。

    

    自几年前出现大型语言模型以来，它们往往被视为许多人工智能问题的事实解决方案。然而，除了LLMs的许多不足之外，如可靠性、成本和速度等问题，还有一类常见的现实世界问题使大型语言模型表现不佳，即约束满足和优化问题。这些问题无处不在，目前的解决方案非常专业化且实施成本高。在Elemental Cognition，我们开发了EC AI平台，该平台采用了神经符号方法来解决约束满足和优化问题。该平台的核心是一个精确高效的逻辑推理引擎，并利用LLMs进行知识获取和用户交互。该平台支持开发人员用自然简洁的语言指定应用逻辑，并生成应用用户界面以与用户进行交互。

    Since the advent of Large Language Models a few years ago, they have often been considered the de facto solution for many AI problems. However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems. These problems are ubiquitous and current solutions are highly specialized and expensive to implement. At Elemental Cognition, we developed our EC AI platform which takes a neuro-symbolic approach to solving constraint satisfaction and optimization problems. The platform employs, at its core, a precise and high performance logical reasoning engine, and leverages LLMs for knowledge acquisition and user interaction. This platform supports developers in specifying application logic in natural and concise language while generating application user interfaces to int
    
[^88]: 避免连续空间中的灾难：通过寻求帮助

    Avoiding Catastrophe in Continuous Spaces by Asking for Help

    [https://arxiv.org/abs/2402.08062](https://arxiv.org/abs/2402.08062)

    在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。

    

    大多数具有正式遗憾保证的强化学习算法假设所有错误都是可逆的，并依赖于尝试所有可能的选项。当一些错误是无法修复甚至是灾难性的时，这种方法会导致糟糕的结果。我们提出了一种上下文多臂赌博问题的变体，在这个问题中，目标是最小化发生灾难的概率。具体而言，我们假设每轮的回报代表了在该轮避免灾难的概率，并尝试最大化回报的乘积（总体避免灾难的概率）。为了给 agent 一些成功的机会，我们允许有限次向导师提问，并假设回报函数为 Lipschitz 连续的。我们提出了一种算法，当时间跨度增长时，它的遗憾和向导师查询率都趋近于 0，假设是一个连续的 1D 状态空间和相对"简单"的回报函数。我们还提供了一个匹配的下界：在没有简单性假设的情况下，任何算法要么不断查询异常的行为，要么每次查询完全相同的行为。

    Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
    
[^89]: 为什么和何时LLM助手可能出错：探究基于提示的交互对软件寻求帮助的有效性

    Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking

    [https://arxiv.org/abs/2402.08030](https://arxiv.org/abs/2402.08030)

    本研究通过实验和访谈调查了大型语言模型(LLM)助手在软件帮助寻求中的有效性。结果显示，虽然优化后的LLM助手相较于基准LLM表现更好，但提示指南和领域上下文的融合与否对于LLM的使用和用户感知没有显著影响。

    

    大型语言模型（LLM）助手，如ChatGPT，已成为帮助用户在复杂的、功能丰富的软件中导航的潜在替代方法。LLM使用来自特定领域文本、软件手册和代码库的大量训练数据来模拟人类般的交互，提供量身定制的帮助，包括逐步指导。本研究通过一项16名参与者的被试实验和后续访谈来调查LLM生成的软件指导。我们比较了基线LLM助手和针对特定软件环境进行优化的LLM，即SoftAIBot，后者还提供了构建适当提示的指南。我们评估了任务完成情况、感知准确性、相关性和信任度。令人惊讶的是，尽管SoftAIBot表现优于基准LLM，但我们的结果显示，在提示指南和领域上下文的融合与否对LLM的使用和用户感知没有显著差异。大多数用户在使用LLM时遇到困难。

    Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to u
    
[^90]: UGMAE：一种统一框架用于图形Masked Autoencoders

    UGMAE: A Unified Framework for Graph Masked Autoencoders

    [https://arxiv.org/abs/2402.08023](https://arxiv.org/abs/2402.08023)

    UGMAE是一种统一框架，用于解决图形自动编码器中存在的节点重要性、图像信息利用、表示空间中的语义知识和重构稳定性等问题。

    

    图形自动编码器的生成自监督学习已成为一种流行的学习范式，并在处理非欧几里德数据方面表现出了其有效性。然而，仍存在一些问题限制了现有方法的能力：1）在掩模中忽视不均匀的节点重要性，2）对整体图像信息的利用不足，3）由于仅在输出空间中使用重构损失，忽视了表示空间中的语义知识，以及4）由于遮蔽内容的大量，导致不稳定的重构。因此，我们提出了UGMAE，一种用于解决这些问题的图形Masked Autoencoders的统一框架，从适应性、完整性、互补性和一致性的角度来考虑。具体而言，我们首先开发了一个自适应特征遮罩生成器，以考虑节点的独特重要性并采样信息丰富的遮罩（适应性）。然后，我们设计了一个基于排名的结构重建方法，以维护图形的完整性和语义一致性，并使用额外的自监督任务增强图像生成过程（完整性和互补性）。

    Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure reconstruct
    
[^91]: CNN需要哪些频率？特征学习中的紧急瓶颈结构的出现

    Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning

    [https://arxiv.org/abs/2402.08010](https://arxiv.org/abs/2402.08010)

    本文描述了CNN中卷积瓶颈（CBN）结构的出现，网络在前几层将输入表示转换为在少数频率和通道上受支持的表示，然后通过最后几层映射回输出。CBN秩定义了保留在瓶颈中的频率的数量和类型，并部分证明了参数范数与深度和CBN秩的比例成正比。此外，我们还展示了网络的参数范数依赖于函数的规则性。我们发现任何具有接近最优参数范数的网络都会展示出CBN结构，这解释了下采样的常见实践；我们还验证了CBN结构在下采样下仍然成立。最后，我们使用CBN结构来解释...（摘要完整内容请见正文）

    

    我们描述了CNN中卷积瓶颈（CBN）结构的出现，网络使用其前几层将输入表示转换为仅在几个频率和通道上受支持的表示，然后使用最后几层将其映射回输出。我们定义了CBN秩，描述了保留在瓶颈内的频率的数量和类型，并在一定程度上证明了表示函数$f$所需的参数范数按深度乘以CBN秩$f$的比例缩放。我们还展示了参数范数在下一阶中依赖于$f$的正则性。我们展示了任何具有近乎最优参数范数的网络都会在权重和（在网络对大学习率稳定的假设下）激活中表现出CBN结构，这促使了下采样的常见做法；并且我们验证了CBN结构在下采样下仍然成立。最后，我们使用CBN结构来解释...

    We describe the emergence of a Convolution Bottleneck (CBN) structure in CNNs, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$. We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling. Finally we use the CBN structure to interpret the
    
[^92]: SMX: 专家迭代的顺序蒙特卡洛规划

    SMX: Sequential Monte Carlo Planning for Expert Iteration

    [https://arxiv.org/abs/2402.07963](https://arxiv.org/abs/2402.07963)

    这项研究介绍了一种名为SMX的顺序蒙特卡洛规划算法，它利用可扩展的方法创建了有效的自我学习机制。它适用于离散和连续动作空间的环境，具有高并行性能。

    

    发展能够在决策和学习过程中利用规划能力的智能体对于人工智能的进步至关重要。最近的研究已经证明了树状搜索方法和自我对弈学习机制的有效性。然而，由于搜索过程的顺序性质，这些方法通常面临扩展性挑战。虽然实践工程解决方案可以部分克服这个问题，但仍需要大量计算资源，这限制了它们的适用性。在本文中，我们介绍一种名为SMX的基于模型的计划算法，它利用可扩展的顺序蒙特卡洛方法创建了一种有效的自我学习机制。SMX基于控制作为推断的理论框架，并受益于坚实的理论基础。它基于采样的搜索方法使其适应具有离散和连续动作空间的环境。此外，SMX允许高度并行化并可以运行于各类计算机设备上。

    Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
    
[^93]: 教育数据挖掘和学习分析：一项更新的调查

    Educational data mining and learning analytics: An updated survey

    [https://arxiv.org/abs/2402.07956](https://arxiv.org/abs/2402.07956)

    这项调查综述了教育数据挖掘和学习分析的发展和应用，提供了当前领域的最新研究现状和未来趋势。

    

    这项调查是前一篇在本期刊上发表的2013年的论文“教育数据挖掘”最新改进版本，并以易懂和广泛的方式综述了教育数据挖掘和学习分析在教育数据上的应用。在过去的十年中，这一研究领域取得了巨大的发展，并在参考文献中使用了一系列相关术语，如学习分析、机构分析、教学分析、数据驱动教育、数据驱动决策教育、教育大数据和教育数据科学。本文通过综述主要的出版物、关键里程碑、知识发现周期、主要的教育环境、具体工具、可用的免费数据集、最常用的方法、主要目标以及未来的趋势，提供了当前的研究现状。

    This survey is an updated and improved version of the previous one published in 2013 in this journal with the title data mining in education. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.
    
[^94]: 优化人工胰腺设计以改善糖尿病管理

    Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management

    [https://arxiv.org/abs/2402.07949](https://arxiv.org/abs/2402.07949)

    通过神经进化算法优化人工胰腺治疗策略，减少糖尿病患者的血糖偏差，并且降低注射次数。

    

    糖尿病是一种慢性疾病，影响美国境内有3800万人，它会影响身体将食物转化为能量（即血糖）的能力。标准的治疗方法是通过使用人工胰腺，即持续胰岛素泵（基础注射），以及定期注射胰岛素（突发注射）来补充碳水化合物摄入量。治疗目标是将血糖保持在可接受范围的中心位置，通过持续血糖测量来进行衡量。次要目标是减少注射次数，因为对某些患者来说注射是不愉快且难以实施的。本研究使用神经进化来发现治疗的最佳策略。基于30天的治疗和单个患者的测量数据集，首先训练了随机森林来预测未来的血糖水平。然后通过进化了一个神经网络来指定碳水化合物摄入量、基础注射水平和突发注射。进化发现了一个帕累托前沿，减少了与目标值的偏差。

    Diabetes, a chronic condition that impairs how the body turns food into energy, i.e. blood glucose, affects 38 million people in the US alone. The standard treatment is to supplement carbohydrate intake with an artificial pancreas, i.e. a continuous insulin pump (basal shots), as well as occasional insulin injections (bolus shots). The goal of the treatment is to keep blood glucose at the center of an acceptable range, as measured through a continuous glucose meter. A secondary goal is to minimize injections, which are unpleasant and difficult for some patients to implement. In this study, neuroevolution was used to discover an optimal strategy for the treatment. Based on a dataset of 30 days of treatment and measurements of a single patient, a random forest was first trained to predict future glucose levels. A neural network was then evolved to prescribe carbohydrates, basal pumping levels, and bolus injections. Evolution discovered a Pareto front that reduced deviation from the targe
    
[^95]: 重新构想指挥与控制

    Re-Envisioning Command and Control

    [https://arxiv.org/abs/2402.07946](https://arxiv.org/abs/2402.07946)

    重新构想的论文提出了未来指挥与控制（C2）决策需要面对更复杂和挑战性的环境，因此提出了基于人工智能系统与人类强有力伙伴关系的未来C2的愿景。这个愿景的核心是优化C2操作流程，保持协同努力，发展自适应的集体知识系统。

    

    未来的战争将要求在更复杂、快节奏、不结构化和极具挑战性的环境中进行指挥与控制（C2）决策。C2将因被拒绝、退化、间歇和有限的通信以及需要考虑到多个作战领域中的许多数据流而变得更加复杂。然而，当前的C2实践——源自工业时代而非新兴的智能时代——是线性的且耗时。而且，这些方法可能无法在未来战场上与对手保持优势。为了应对这些挑战，我们提出了一种基于人工智能（AI）系统与人类之间强有力伙伴关系的未来C2愿景。这个未来愿景体现在三个运营影响上：优化C2操作流程，保持协同努力，以及发展自适应的集体知识系统。本文阐述了所设想的未来指挥与控制的愿景。

    Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
    
[^96]: ScreenAgent:一种基于视觉语言模型驱动的计算机控制代理

    ScreenAgent: A Vision Language Model-driven Computer Control Agent

    [https://arxiv.org/abs/2402.07945](https://arxiv.org/abs/2402.07945)

    本文介绍了一种基于视觉语言模型的计算机控制代理ScreenAgent，该代理可以通过观察屏幕截图和输出鼠标键盘动作与计算机屏幕进行交互，完成多步任务。

    

    现有的大型语言模型(LLM)可以调用各种工具和API来完成复杂任务。作为最强大和通用的工具，计算机可能被训练有素的LLM代理直接控制。由计算机驱动，我们希望构建一个更通用的代理，在各种日常数字工作中协助人类。在本文中，我们构建了一个环境，用于让视觉语言模型(VLM)代理与真实的计算机屏幕进行交互。在这个环境中，代理可以观察屏幕截图，并通过输出鼠标和键盘动作来操作图形用户界面(GUI)。我们还设计了一个自动控制流水线，包括规划、行动和反思阶段，指导代理不断与环境交互，完成多步任务。此外，我们构建了ScreenAgent数据集，该数据集收集了完成各种日常计算机任务时的屏幕截图和动作序列。最后，我们训练了一个模型，ScreenAgent。

    Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, Screen
    
[^97]: LLMs来袭: 生成型人工智能参与数字话语

    LLMs Among Us: Generative AI Participating in Digital Discourse

    [https://arxiv.org/abs/2402.07940](https://arxiv.org/abs/2402.07940)

    LLMs Among Us实验框架通过在社交媒体平台上让机器人和人类互动的方式，研究了大型语言模型在伪装成人类参与者方面的能力，发现尽管存在一定的威胁，但参与者只有42%的时间能正确识别用户的性质。

    

    大型语言模型（LLMs）的出现对许多社交媒体平台的格局具有重塑潜力。虽然这带来了很多有前途的机会，但也引发了许多威胁，如偏见和隐私问题，并可能导致恶意行为者传播宣传。我们在Mastodon社交媒体平台上开发了“LLMs Among Us”实验框架，用于机器人和人类参与者在不了解机器人和人类参与者的比例或性质的情况下进行交流。我们建立了10个不同LLMs（GPT-4、LLama 2 Chat和Claude）的人物形象。我们进行了三轮实验，并在每轮实验后对参与者进行了调查，以衡量LLMs伪装成人类参与者而不被发现的能力。尽管知道实验中存在机器人和人类，参与者只有42％的时间能正确识别其他用户的性质。我们还发现，角色选择在实验中产生了显著影响。

    The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the "LLMs Among Us" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially mor
    
[^98]: UFO: 一个专注于Windows操作系统交互的用户界面智能体

    UFO: A UI-Focused Agent for Windows OS Interaction

    [https://arxiv.org/abs/2402.07939](https://arxiv.org/abs/2402.07939)

    UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。

    

    我们介绍了UFO，一个创新的专注于Windows操作系统上应用程序的用户界面智能体，利用了GPT-Vision的能力来满足用户需求。UFO采用双智能体框架，精确观察和分析Windows应用程序的图形用户界面（GUI）和控制信息。这使得智能体可以无缝地在单个应用程序内以及跨应用程序进行导航和操作，以满足用户的需求，即使涉及多个应用程序。该框架包括一个控制交互模块，实现无需人工干预的动作连接，并实现完全自动化执行。因此，UFO将艰巨而耗时的过程转变为仅通过自然语言命令就可以完成的简单任务。我们在9个流行的Windows应用程序上对UFO进行了测试，涵盖了反映用户日常使用情景的各种情况。通过定量指标和真实案例研究得出的结果强调了UFO的效果。

    We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
    
[^99]: 大型语言用户界面：由LLMs驱动的语音交互用户界面

    Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs

    [https://arxiv.org/abs/2402.07938](https://arxiv.org/abs/2402.07938)

    本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。

    

    最近大型语言模型的快速发展展示了其在逻辑推理和理解方面的卓越能力。这些新发现的能力引发了新一代软件的诞生，正如它们在工业界无数应用中所展示的那样。本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介。通过对自然文本输入进行彻底分析，一个经过精心设计的LLM引擎可以理解用户的需求，分类最有可能的应用程序，识别所需的UI组件，并随后执行用户期望的操作。这种集成可以将静态UI系统发展成高度动态和可适应的解决方案，引入智能和响应式用户体验的新领域。这样的框架可以从根本上改变用户完成日常任务的方式，极大提升用户体验。

    The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
    
[^100]: 人本智能产品原型设计中的无代码AutoML：概念框架、潜力与限制

    Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations

    [https://arxiv.org/abs/2402.07933](https://arxiv.org/abs/2402.07933)

    本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。

    

    本文评估了无代码AutoML作为解决人工智能产品原型设计中的挑战的方案，这些产品的特点是不可预测性和对非专家的不可访问性，并提出了一个概念框架。人工智能产品的复杂性阻碍了无缝执行和跨学科合作，这对于人本智能产品至关重要。与行业和创新相关，它影响战略决策和投资风险的降低。目前的方法对人工智能产品想法的潜力和可行性提供了有限的见解。本研究采用设计科学研究的方法，识别挑战，并通过提出无代码AutoML的概念框架来解决这些挑战。案例研究证实了其对非专家的支持潜力，提供了一种结构化的人工智能产品开发方法。该框架有助于实现可访问和可解释的原型设计，使学术界、管理者和决策者受益。无代码AutoML的战略整合

    This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
    
[^101]: 一个人机协作的框架用于的模式开发

    A Human-Machine Collaboration Framework for the Development of Schemas

    [https://arxiv.org/abs/2402.07932](https://arxiv.org/abs/2402.07932)

    这篇论文提出了一个人机协作的框架用于设计新的模式，目的是解决机器智能中的挑战，并将AI社区的关注从技术转向AI科学。

    

    Winograd模式挑战（WSC）是一个为了研究展示人类行为的系统而设立的测试，它旨在将AI社区的关注点从技术转向AI科学。研究表明，尽管对人类来说是常见和琐碎的，但对机器来说仍然具有挑战性，尤其是当它们需要处理需要解决确定性代词的精心设计的句子时。为了解决这个需求，我们提出了一个专注于人机如何作为团队合作来设计新模式的新框架。

    The Winograd Schema Challenge (WSC), a seemingly well-thought-out test for machine intelligence, has been proposed to shed light on developing systems that exhibit human behavior. Since its introduction, it aimed to pivot the focus of the AI community from the technology to the science of AI. While common and trivial for humans, studies show that it is still challenging for machines, especially when they have to deal with novel schemas, that is, well-designed sentences that require the resolving of definite pronouns. As researchers have become increasingly interested in the challenge itself, this presumably necessitates the availability of an extensive collection of Winograd schemas, which goes beyond what human experts can reasonably develop themselves, especially after proposed ways of utilizing them as novel forms of CAPTCHAs.   To address this necessity, we propose a novel framework that explicitly focuses on how humans and machines can collaborate as teammates to design novel sche
    
[^102]: 在强化学习中，模糊轨迹可视化用于可解释性

    Abstracted Trajectory Visualization for Explainability in Reinforcement Learning

    [https://arxiv.org/abs/2402.07928](https://arxiv.org/abs/2402.07928)

    强化学习(RL)模型中使用模糊轨迹可视化，使非RL专家能够推断出RL的行为模式。

    

    可解释的人工智能（XAI）已经展示出帮助强化学习（RL）从业者理解RL模型工作原理的潜力。然而，对于没有RL专业知识的用户（非RL专家），XAI的研究还不够。这导致非RL专家难以参与如何为人类和AI共存的社会设计RL模型的基本讨论。解决这个问题将使RL专家能够与非RL专家进行沟通，从而生成更适合我们社会的机器学习解决方案。我们认为，描述RL模型主要状态之间转换的模糊轨迹对于非RL专家构建代理的心理模型将是有用的。我们的初步结果表明，通过利用模糊轨迹的可视化，没有RL专业知识的用户能够推断出RL的行为模式。

    Explainable AI (XAI) has demonstrated the potential to help reinforcement learning (RL) practitioners to understand how RL models work. However, XAI for users who do not have RL expertise (non-RL experts), has not been studied sufficiently. This results in a difficulty for the non-RL experts to participate in the fundamental discussion of how RL models should be designed for an incoming society where humans and AI coexist. Solving such a problem would enable RL experts to communicate with the non-RL experts in producing machine learning solutions that better fit our society. We argue that abstracted trajectories, that depicts transitions between the major states of the RL model, will be useful for non-RL experts to build a mental model of the agents. Our early results suggest that by leveraging a visualization of the abstracted trajectories, users without RL expertise are able to infer the behavior patterns of RL.
    
[^103]: 大型语言模型中提示工程的系统调查：技术和应用

    A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications

    [https://arxiv.org/abs/2402.07927](https://arxiv.org/abs/2402.07927)

    这篇调查论文系统概述了大型语言模型中提示工程的最新进展，探讨了提示工程的方法和技术，并说明了其在各种应用中的重要作用。

    

    提示工程已成为扩展大型语言模型（LLM）和视觉语言模型（VLM）能力的不可或缺的技术。该方法利用任务特定的指令（称为提示）在不修改核心模型参数的情况下增强模型的效果。提示允许将预训练模型无缝集成到下游任务中，仅根据给定的提示引发所需的模型行为，而不是更新模型参数。提示可以是提供上下文以指导模型的自然语言指令，也可以是调用相关知识的学习向量表示。这个新兴领域在各种应用中取得了成功，从问答到常识推理都有涉及。然而，对于多样的提示工程方法和技术缺乏系统的组织和理解。本调查论文通过提供对最近进展的结构化概述来填补这一空白。

    Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in pro
    
[^104]: 点击和指导：通过统一直接操作和文本指令实现精确图像编辑

    Point and Instruct: Enabling Precise Image Editing by Unifying Direct Manipulation and Text Instructions

    [https://arxiv.org/abs/2402.07925](https://arxiv.org/abs/2402.07925)

    点击和指导是一个将直接操作和文本指令结合起来，实现精确图像编辑的系统。

    

    机器学习使得能够通过自然语言指令编辑图像的强大系统得以开发。然而，在许多常见情景下，使用者很难仅通过文本来指定精确的图像变换。例如，在一张包含多只狗的图像中，难以选择特定的狗并将其移动到一个精确的位置。仅通过文本完成这个操作需要一个复杂的提示，用于确定目标狗，并描述目的位置。然而，直接操作很适合于选择对象和指定位置等视觉任务。我们引入了一个名为点选与指导的系统，通过无缝地结合直接操作和文本指令，实现精确图像编辑。通过我们的系统，用户可以在视觉上标记对象和位置，并在文本指令中引用它们。这使得用户既能从自然语言的视觉描述中受益，又能从直接操作的空间精确性中受益。

    Machine learning has enabled the development of powerful systems capable of editing images from natural language instructions. However, in many common scenarios it is difficult for users to specify precise image transformations with text alone. For example, in an image with several dogs, it is difficult to select a particular dog and move it to a precise location. Doing this with text alone would require a complex prompt that disambiguates the target dog and describes the destination. However, direct manipulation is well suited to visual tasks like selecting objects and specifying locations. We introduce Point and Instruct, a system for seamlessly combining familiar direct manipulation and textual instructions to enable precise image manipulation. With our system, a user can visually mark objects and locations, and reference them in textual instructions. This allows users to benefit from both the visual descriptiveness of natural language and the spatial precision of direct manipulatio
    
[^105]: 朝向人类数字孪生：定义与设计 - 一项调查

    Towards the Human Digital Twin: Definition and Design -- A survey

    [https://arxiv.org/abs/2402.07922](https://arxiv.org/abs/2402.07922)

    本调查综合了最新的HDT领域进展，提出了第一个跨领域HDT定义和十一个关键设计考虑因素，为未来开发者提供指导。

    

    人类数字孪生 (HDT) 是一种快速崛起的技术，在医疗保健到体育等领域具有巨大潜力。HDT通过将人类表示为基础物理实体扩展了对数字孪生的传统理解。这引入了几个重大挑战，包括HDT定义的模糊性以及缺乏对其设计的指导。本调查将HDT领域的最新进展汇集起来，通过基于其特征的首个跨领域HDT定义以及从相关挑战中出现的十一个关键设计考虑因素，为未来的开发者提供指导。

    Human Digital Twins (HDTs) are a fast-emerging technology with significant potential in fields ranging from healthcare to sports. HDTs extend the traditional understanding of Digital Twins by representing humans as the underlying physical entity. This has introduced several significant challenges, including ambiguity in the definition of HDTs and a lack of guidance for their design. This survey brings together the recent advances in the field of HDTs to guide future developers by proposing a first cross-domain definition of HDTs based on their characteristics, as well as eleven key design considerations that emerge from the associated challenges.
    
[^106]: 生成式人工智能如何提升盲人的福祉？

    How Can Generative AI Enhance the Well-being of Blind?

    [https://arxiv.org/abs/2402.07919](https://arxiv.org/abs/2402.07919)

    生成式人工智能通过分析图像等方式，可以改善盲人和视力受损人士的福祉，创造新的独立性和感知，并对其生活产生根本性的影响。

    

    本文探讨了生成式人工智能如何改善盲人或视力受损人士的福祉问题。它引用了一个当前的例子，即Be My Eyes应用程序，在2023年集成了Be My AI功能，该功能基于来自OpenAI的GPT-4。作者描述和评估了自己的测试，并进行了伦理和社会讨论。该工具可以以惊人的方式分析静止图像的能力得到了展示。受影响的人获得了新的独立性和对环境的新感知。同时，他们也依赖于提供者或开发者的世界观和道德观，后者会为他们指定或拒绝某些描述。展望表明，对移动图像的分析将意味着进一步的飞跃。可以说，生成式人工智能可以从根本上改善盲人和视力受损人士的福祉，并以各种方式改变他们的生活。

    This paper examines the question of how generative AI can improve the well-being of blind or visually impaired people. It refers to a current example, the Be My Eyes app, in which the Be My AI feature was integrated in 2023, which is based on GPT-4 from OpenAI. The author's tests are described and evaluated. There is also an ethical and social discussion. The power of the tool, which can analyze still images in an amazing way, is demonstrated. Those affected gain a new independence and a new perception of their environment. At the same time, they are dependent on the world view and morality of the provider or developer, who prescribe or deny them certain descriptions. An outlook makes it clear that the analysis of moving images will mean a further leap forward. It is fair to say that generative AI can fundamentally improve the well-being of blind and visually impaired people and will change it in various ways.
    
[^107]: 为帮助中国Python编程学习者提供的一个带有注释的问答数据集

    QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners

    [https://arxiv.org/abs/2402.07913](https://arxiv.org/abs/2402.07913)

    为解决编程智能教育系统中数据稀缺问题，本文提出了一个新的针对Python学习者的中文问答数据集，通过收集与分类真实学生问题，提高在线编程教育的效果和质量。

    

    在在线学习平台中，特别是在快速增长的计算机编程课程中，解答成千上万学生的学习问题需要相当大的人力成本。为编程教育定制智能助手大型语言模型（LLMs）的创建需要独特的数据支持。然而，在实际应用场景中，用于训练此类LLMs的数据资源相对稀缺。因此，为了解决编程智能教育系统中的数据稀缺问题，本文提出了一个新的针对Python学习者的中文问答数据集。为确保问题的来源的真实性和可靠性，我们收集了实际学生提出的问题，并根据问题的类型和学习者的类型进行分类。这种注释原则旨在提高在线编程教育的效果和质量，为开发这方面的工作提供坚实的数据基础。

    In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing th
    
[^108]: 空间计算：概念、应用、挑战和未来方向

    Spatial Computing: Concept, Applications, Challenges and Future Directions

    [https://arxiv.org/abs/2402.07912](https://arxiv.org/abs/2402.07912)

    空间计算是一种技术进步，将设备无缝集成到物理环境中，改善了数字世界的用户体验。它具有广泛的应用，并且在研究者和工业组织中的重要性不断增加。

    

    空间计算是一种技术进步，它能够将设备无缝集成到物理环境中，从而使数字世界的用户体验更加自然和直观。空间计算有潜力成为计算领域的重要进展。从GPS和基于位置的服务到医疗保健，空间计算技术影响并改善了我们与数字世界的交互。在创建交互式数字环境方面，空间计算的应用越来越受欢迎和有效。这促使我们进行了这篇综述，详细介绍了空间计算，包括其支持技术和对各种应用的影响。我们还讨论了与空间计算相关的项目。在这篇综述中，我们还探讨了空间计算的潜在挑战和限制。

    Spatial computing is a technological advancement that facilitates the seamless integration of devices into the physical environment, resulting in a more natural and intuitive digital world user experience. Spatial computing has the potential to become a significant advancement in the field of computing. From GPS and location-based services to healthcare, spatial computing technologies have influenced and improved our interactions with the digital world. The use of spatial computing in creating interactive digital environments has become increasingly popular and effective. This is explained by its increasing significance among researchers and industrial organisations, which motivated us to conduct this review. This review provides a detailed overview of spatial computing, including its enabling technologies and its impact on various applications. Projects related to spatial computing are also discussed. In this review, we also explored the potential challenges and limitations of spatial
    
[^109]: MAP-Elites应用于人机协同设计中，显现搜索空间的作用：一项大规模用户研究

    Does mapping elites illuminate search spaces? A large-scale user study of MAP--Elites applied to human--AI collaborative design

    [https://arxiv.org/abs/2402.07911](https://arxiv.org/abs/2402.07911)

    通过两项研究，揭示了MAP-Elites在人机协同设计中对搜索空间的作用。这些研究使用了基于进化算法的设计工具，以实现设计推荐对设计过程的影响的理解。

    

    为了了解设计建议对设计过程的影响，进行了两项人工智能协同设计工具的研究。所研究的工具基于进化算法，旨在设计一辆在固定时间内行驶最远的虚拟汽车。参与者可以设计自己的汽车，向算法提出建议，并查看算法提供的一系列建议。算法提供的设计是之前经过测试的设计；其中一些是随机选择的，另一些是使用MAP-Elites选择的。在第一项研究中，作为科学普及计划的一部分，记录了808次设计会话，每次会话都有参与者使用工具的分析数据。为了提供定量数据的背景信息，还进行了一项包含12位参与者的双盲实验室研究。在实验室研究中，收集了与大规模研究的相同定量数据，并收集了对访谈问题的回答。

    Two studies of a human-AI collaborative design tool were carried out in order to understand the influence design recommendations have on the design process. The tool investigated is based on an evolutionary algorithm attempting to design a virtual car to travel as far as possible in a fixed time. Participants were able to design their own cars, make recommendations to the algorithm and view sets of recommendations from the algorithm. The algorithm-recommended sets were designs which had been previously tested; some sets were simply randomly picked and other sets were picked using MAP-Elites. In the first study 808 design sessions were recorded as part of a science outreach program, each with analytical data of how each participant used the tool. To provide context to this quantitative data, a smaller double-blind lab study was also carried out with 12 participants. In the lab study the same quantitative data from the large scale study was collected alongside responses to interview ques
    
[^110]: Prompt4Vis: 使用示例挖掘和结构过滤来为表格数据可视化提供提示的大语言模型

    Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization

    [https://arxiv.org/abs/2402.07909](https://arxiv.org/abs/2402.07909)

    提出了 Prompt4Vis，使用示例挖掘和结构过滤来为表格数据可视化的大语言模型提供提示。这种方法利用了巨大的语言模型的优势，并能够改进当前自然语言查询转换成数据可视化查询的方法。

    

    数据可视化(DV)系统因其在大数据集中发现洞见的深厚能力而得到越来越多的认可，引起了工业界和学术界的关注。在某些声明式可视化语言(DVLs，如Vega-Lite、EChart)中，编制数据查询是一个重要的过程。自然语言处理(NLP)技术的发展使得使用自然语言界面来可视化表格数据的过程更加简单和直观，提供了更可访问和直观的用户体验。然而，当前将自然语言问题转换成数据可视化查询的方法，如Seq2Vis、ncNet和RGVisNet，尽管利用了复杂的神经网络架构，仍然不尽人意，有很大的改进空间。大语言模型(LLMs)如ChatGPT和GPT-4在各种NLP任务中取得了新的基准，从根本上改变了该领域的格局。受到这些进展的启发，我们引入了一种新方法。

    Data visualization (DV) systems are increasingly recognized for their profound capability to uncover insights from vast datasets, gaining attention across both industry and academia. Crafting data queries is an essential process within certain declarative visualization languages (DVLs, e.g., Vega-Lite, EChart.). The evolution of natural language processing (NLP) technologies has streamlined the use of natural language interfaces to visualize tabular data, offering a more accessible and intuitive user experience. However, current methods for converting natural language questions into data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite utilizing complex neural network architectures, still fall short of expectations and have great room for improvement.   Large language models (LLMs) such as ChatGPT and GPT-4, have established new benchmarks in a variety of NLP tasks, fundamentally altering the landscape of the field. Inspired by these advancements, we introduce a nov
    
[^111]: 可扩展的多粒度融合网络用于基于方面的情感分析

    Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis

    [https://arxiv.org/abs/2402.07787](https://arxiv.org/abs/2402.07787)

    这篇论文提出了一种可扩展的多粒度融合网络（EMGF）用于基于方面的情感分析，通过整合不同的语言和结构特征，包括句法依赖、组成、注意力语义和外部知识图谱等，来提高情感分析的性能和准确性。

    

    基于方面的情感分析（ABSA）评估文本中的情感表达以理解情感信息。先前的研究整合了外部知识，如知识图谱，以加强ABSA模型中的语义特征。最近的研究探讨了在依赖和组成树上使用图神经网络（GNN）进行句法分析。随着ABSA的不断发展，越来越多的创新的语言和结构特征被融入其中（例如潜在图），但这也引入了复杂性和混淆。目前，尚不存在一个可扩展的框架，可以将多样性的语言和结构特征集成到ABSA中。本文介绍了可扩展的多粒度融合（EMGF）网络，它整合了来自句法依赖和组成、注意力语义和外部知识图谱的信息。EMGF配备了多锚点三元学习和正交投影，高效地利用了这些特征的综合潜力。

    Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of 
    
[^112]: 价值装载问题的激素适应方法：预防回形针启示录？

    A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?

    [https://arxiv.org/abs/2402.07462](https://arxiv.org/abs/2402.07462)

    我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。

    

    价值装载问题对于研究人员来说是一个重要的挑战，他们旨在创建与人类价值观和偏好相一致的人工智能系统。该问题需要一种方法来定义和规范人工智能行为的安全和最优限制。在这项工作中，我们提出了HALO（激素适应通过对手过程）这个法规模式，它使用激素分析来调节人工智能的行为模式。行为激素适应是一种现象，低频率的行为具有益处，而高频率的行为则有害。通过将行为建模为变态对手过程，我们可以使用行为频率响应分析（BFRA）或行为计数响应分析（BCRA）来量化可重复行为的激素限制。我们展示了如何使用HALO来解决“回形针最大化器”场景，这是一个思想实验，其中一个未受管制的人工智能任务是将所有物质转化为回形针。

    The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
    
[^113]: 关于转运混淆问题的研究

    On the Transit Obfuscation Problem

    [https://arxiv.org/abs/2402.07420](https://arxiv.org/abs/2402.07420)

    本文研究了转运混淆问题，提出了转运匿名性的概念，并提出并评估了满足该匿名性准则的规划/搜索算法。

    

    在某些交通和监视场景中，隐藏路径上或从路径上可见的中间点是一个重要的目标。本文研究了转运混淆问题，即从某个起始位置到达目标位置的同时，"覆盖"需要隐藏的特定过境点的问题。我们提出了转运匿名性的概念，即对特定过境点的匿名性进行量化保证，即使在具有对路径规划算法有全面了解的强大对手面前也是如此。我们提出并评估了满足该匿名性准则的规划/搜索算法。

    Concealing an intermediate point on a route or visible from a route is an important goal in some transportation and surveillance scenarios. This paper studies the Transit Obfuscation Problem, the problem of traveling from some start location to an end location while "covering" a specific transit point that needs to be concealed from adversaries. We propose the notion of transit anonymity, a quantitative guarantee of the anonymity of a specific transit point, even with a powerful adversary with full knowledge of the path planning algorithm. We propose and evaluate planning/search algorithms that satisfy this anonymity criterion.
    
[^114]: 大型语言模型如何在诚实与帮助之间进行权衡？

    How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?

    [https://arxiv.org/abs/2402.07282](https://arxiv.org/abs/2402.07282)

    本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。

    

    在日常交流中，人们经常为了最大限度地帮助听众而近似真相，例如约略时间或省略细节。大型语言模型（LLMs）如何处理这种微妙的权衡？为了回答这个问题，我们使用心理模型和旨在描述人类行为的实验来分析LLMs。我们测试了一系列LLMs，并探讨了优化人类偏好或推理时思考对这些权衡的影响。我们发现，从人类反馈中的强化学习改善了诚实和帮助性，而链式思维提示使LLMs偏向于帮助性而不是诚实。最后，GPT-4 Turbo展示了类似人类的回应模式，包括对对话框架和听众决策背景的敏感性。我们的研究结果揭示了LLMs内化的对话价值观，并暗示即使这些抽象价值观也可以在零-shot提示下在一定程度上被引导。

    In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
    
[^115]: GraphTranslator：将图模型与大型语言模型对齐用于开放式任务

    GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks

    [https://arxiv.org/abs/2402.07197](https://arxiv.org/abs/2402.07197)

    "GraphTranslator"是一个旨在将预训练的图模型和大型语言模型对齐的翻译器，可以同时处理预定义任务和开放式任务。通过将这两种模型结合起来，能够有效地处理各种任务，并实现更具创新性和灵活性的应用。

    

    大型语言模型（LLMs）如ChatGPT，展示了强大的零样本和遵循指令的能力，在人工智能的各个研究领域中引发了一场革命性的转变，尤其是对于开放式任务。尽管在图领域中这个想法较少被探索，尽管有许多强大的图模型（GMs）可用，但它们被限制在预定义形式的任务中。虽然已经提出了几种将LLMs应用于图的方法，但它们未能同时处理预定义任务和开放式任务，无论是将LLMs作为节点特征增强器还是作为独立预测器。为了打破这个困境，我们提出了一个名为GraphTranslator的翻译器，旨在通过预训练的GM和LLMs之间的桥梁，有效地处理预定义任务，并利用LLMs的扩展接口为GM提供各种开放式任务。为了训练这样的翻译器，我们提出了一个称为Producer的构建图文对齐数据的工具。

    Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node
    
[^116]: 索crates怀疑的回声：在校准的证据增强学习中接受不确定性

    Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning

    [https://arxiv.org/abs/2402.07107](https://arxiv.org/abs/2402.07107)

    这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。

    

    我们提出了一种新颖的统计方法，用于在基于模型的分布强化学习中引入不确定性意识，涉及基于分位数回归的深度Q网络。提出的算法$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$旨在解决在随机环境中分别估计aleatoric和epistemic不确定性所面临的关键挑战。它将深度证据学习与基于合规推理原则的分位数校准相结合，提供了显式的、无样本计算的$\textit{全局}$不确定性，而不是基于简单方差的$\textit{局部}$估计，克服了传统方法在计算和统计效率以及处理超出分布范围的观测数据方面的局限性。在一套小型化的Atari游戏（即MinAtar）上进行测试，CEQR-DQN在得分和学习速度方面超过了类似的现有框架。它能够严谨地处理外部数据观测，并提供更高的计算和统计效率。

    We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
    
[^117]: 使用谈判能力的分布式基础设施高效资源调度

    Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities

    [https://arxiv.org/abs/2402.06938](https://arxiv.org/abs/2402.06938)

    这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。

    

    在过去的几十年里，信息和互联网技术的快速发展催生了大量的数据和信息。信息爆炸推动许多企业或个人寻求租用云计算基础设施来将他们的应用程序放置在云中。然而，云计算提供商和客户之间达成的协议通常不高效。许多因素影响效率，如提供商云计算基础设施的闲置和对客户的额外成本。一个可能的解决方案是引入一种综合的、谈判类的博弈，并根据谈判结果安排资源。我们提出了一种基于模糊逻辑的基于代理的自动谈判系统用于资源调度。所提出的方法可以完成一对一的自动谈判过程，并为提供商和客户生成最优的报价。我们比较了不同成员函数、模糊规则集和网络拓扑结构对资源调度的影响。

    In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
    
[^118]: 具有辨别性对抗学习的论文

    Discriminative Adversarial Unlearning

    [https://arxiv.org/abs/2402.06864](https://arxiv.org/abs/2402.06864)

    该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。

    

    我们引入了一种新颖的机器反学习框架，基于最小最大优化范式的已建立原则。我们利用强大的成员推断攻击（MIA）的能力，以促进从训练模型中反学习特定样本。我们考虑了两个网络的场景，攻击者$\mathbf{A}$和经过训练的防御者 $\mathbf{D}$在对抗目标下相互对抗，其中攻击者旨在揭示数据的信息以推断成员身份，而防御者在反击中进行反学习，同时保持其总体性能。算法可以使用反向传播进行端到端训练，遵循已知的迭代最小最大方法来更新攻击者和防御者。我们还加入了自监督目标，有效地解决了遗忘集和验证集之间的特征空间差异，增强了反学习能力

    We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
    
[^119]: 学习变得高效：在大型语言模型中构建结构化稀疏性

    Learn To be Efficient: Build Structured Sparsity in Large Language Models

    [https://arxiv.org/abs/2402.06126](https://arxiv.org/abs/2402.06126)

    本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。

    

    大型语言模型(LLM)以其十亿级参数取得了显著的成功，但它们产生了高昂的推理开销。在LLM中出现的激活稀疏性为通过仅涉及部分参数进行推理提供了一种自然的方法来减少这种成本。现有方法只关注利用这种自然形成的激活稀疏性，忽视了进一步放大这种固有稀疏性的潜力。本文中，我们假设LLM可以通过实现更结构化的激活稀疏性来学习高效。为实现这一目标，我们引入了一种新颖的算法"Learn-To-be-Efficient(LTE)", 旨在训练高效意识的LLM学习激活更少的神经元，并在稀疏性和性能之间取得更好的折衷。此外，与主要关注基于ReLU模型的SOTA MoEfication方法不同，LTE还可以应用于像GPT和LLaMA这样具有软激活函数的LLM。我们在四个模型和十一个数据集上评估了LTE。

    Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
    
[^120]: Transformer语言模型在算法学习上的限制

    Limits of Transformer Language Models on Algorithmic Learning

    [https://arxiv.org/abs/2402.05785](https://arxiv.org/abs/2402.05785)

    Transformer语言模型在学习离散算法方面的组合能力非常有限，比重新学习所有子任务对于新的算法组合的效果更差，而且梯度下降在记忆前馈模型上的效率非常低。

    

    我们分析了Transformer语言模型在学习离散算法方面的能力。为此，我们引入了两个要求组合多个离散子任务的新任务。我们通过从头开始训练LLaMA模型和在GPT-4和Gemini上提示来衡量学习学习原语的组合。我们观察到，目前最先进的Transformer语言模型的组合能力非常有限，并且在样本规模方面比为新的算法组合重新学习所有子任务效果更差。我们还提出了一个复杂性理论的定理，证明了记忆前馈模型上的梯度下降可以指数级地浪费数据。

    We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
    
[^121]: 通过并行观测预测改进基于令牌的世界模型

    Improving Token-Based World Models with Parallel Observation Prediction

    [https://arxiv.org/abs/2402.05643](https://arxiv.org/abs/2402.05643)

    该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。

    

    受到将Transformer应用于离散符号序列的成功启发，最近提出了基于令牌的世界模型（TBWMs）作为高效样本方法。在TBWMs中，世界模型将代理经验作为一种类似语言的令牌序列进行消耗，其中每个观测构成一个子序列。然而，在想象过程中，通过令牌逐个生成下一个观测的串行方式导致了严重的瓶颈问题，导致训练时间长、GPU利用率低和表示能力有限。为了解决这个瓶颈问题，我们设计了一种新颖的并行观测预测（POP）机制。POP通过一种针对我们的强化学习环境设计的新型前向模式来扩充了保持网络（RetNet）。我们将POP集成到一种名为REM（保持环境模型）的新型TBWM代理中，展示了比以前的TBWMs快15.4倍的想象能力。REM在Atari 100K基准测试的26个游戏中的12个游戏中达到超越人类水平的性能，并且在不到12小时的训练时间内完成训练。

    Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
    
[^122]: 密集乘法物理信息神经网络

    Densely Multiplied Physics Informed Neural Network

    [https://arxiv.org/abs/2402.04390](https://arxiv.org/abs/2402.04390)

    该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。

    

    尽管物理信息神经网络（Physics-Informed Neural Networks, PINNs）在处理非线性偏微分方程（PDEs）方面显示出巨大潜力，但常常会出现精度不足或获取不正确结果的问题。与大多数现有的解决方案不同，该论文改进了神经网络架构以提高PINN的性能。我们提出了一种密集乘法PINN（DM-PINN）架构，它将隐藏层的输出与所有后面的隐藏层的输出相乘。在不引入更多可训练参数的情况下，该有效机制可以显著提高PINN的准确性。所提出的架构在四个基准示例（Allan-Cahn方程，Helmholtz方程，Burgers方程和1D对流方程）上进行了评估。将所提出的架构与不同的PINN结构进行比较，证明了其卓越的性能。

    Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
    
[^123]: 用于解决物联网识别问题的逻辑识别方法

    Logical recognition method for solving the problem of identification in the Internet of Things

    [https://arxiv.org/abs/2402.04338](https://arxiv.org/abs/2402.04338)

    这项工作的目标是开发一种逻辑识别方法，通过构建逻辑函数的最优扩展，在整个特征空间上实现逻辑连接，以解决物联网中的识别问题。

    

    最近，出现了一种应用逻辑代数和价值逻辑方法的新领域，即识别各种物体和现象、医学或技术诊断、构建现代机器、检查测试问题等问题，可以归结为在整个特征空间构建逻辑函数的最优扩展。例如，在逻辑识别系统中，使用基于离散分析和命题演算法的逻辑方法来构建自己的识别算法。在一般情况下，逻辑识别方法的使用需要存在由k值函数在整个特征空间上的最优延续所表示的逻辑连接，其中变量是正在识别的对象或现象的逻辑特征。本研究的目标是开发一种逻辑识别方法，该方法由具有逻辑特征和类别的参考表组成。

    A new area of application of methods of algebra of logic and to valued logic, which has emerged recently, is the problem of recognizing a variety of objects and phenomena, medical or technical diagnostics, constructing modern machines, checking test problems, etc., which can be reduced to constructing an optimal extension of the logical function to the entire feature space. For example, in logical recognition systems, logical methods based on discrete analysis and propositional calculus based on it are used to build their own recognition algorithms. In the general case, the use of a logical recognition method provides for the presence of logical connections expressed by the optimal continuation of a k-valued function over the entire feature space, in which the variables are the logical features of the objects or phenomena being recognized. The goal of this work is to develop a logical method for object recognition consisting of a reference table with logical features and classes of non
    
[^124]: MolTC: 在语言模型中进行分子关系建模

    MolTC: Towards Molecular Relational Modeling In Language Models

    [https://arxiv.org/abs/2402.03781](https://arxiv.org/abs/2402.03781)

    本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。

    

    分子关系学习（MRL）旨在理解分子之间的相互作用，在推进生物化学研究方面起到了关键作用。最近，大型语言模型（LLMs）的采用已成为一种有效和高效的MRL方法，这些模型以其庞大的知识存储库和先进的逻辑推理能力而闻名。尽管具有潜力，但这些方法主要依赖于文本数据，因此没有充分利用分子图中固有的丰富结构信息。此外，缺乏统一的框架加剧了信息的浪费，因为它阻碍了在不同数据集之间共享学习到的相互作用理由。为了解决这些挑战，本研究提出了一种基于LLM的多模态框架，用于根据思维链（CoT）理论对分子相互作用进行预测，称为MolTC，它可以高效地整合分子对的丰富图形信息。

    Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
    
[^125]: C-RAG: 针对检索增强语言模型的认证生成风险

    C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models

    [https://arxiv.org/abs/2402.03181](https://arxiv.org/abs/2402.03181)

    C-RAG是第一个用于认证检索增强语言模型生成风险的框架，通过提供符合风险分析和生成风险的上界，确保生成结果的可信性。

    

    尽管大型语言模型（LLMs）在各种应用中具备令人印象深刻的能力，但它们仍然存在可信度问题，如幻觉和错位。检索增强语言模型（RAG）被提出来增强生成结果的可信性，通过引入外部知识。但是，对于RAG模型的生成风险的理论理解尚未被研究。本文回答了以下问题：1）RAG是否确实能够降低生成风险，2）如何对RAG和传统LLM的生成风险提供可证明的保证，以及3）哪些充分条件使得RAG模型能够降低生成风险。我们提出了C-RAG，第一个用于认证RAG模型生成风险的框架。具体而言，我们为RAG模型提供了符合风险分析，并确保了生成风险的上界，我们称之为符合生成风险。我们还对一般有界风险下的符合生成风险提供了理论保证。

    Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
    
[^126]: 视觉-语言模型为强化学习提供可提示的表示

    Vision-Language Models Provide Promptable Representations for Reinforcement Learning

    [https://arxiv.org/abs/2402.02651](https://arxiv.org/abs/2402.02651)

    本论文提出一种利用预训练的视觉-语言模型作为可提示的表示，为强化学习提供世界知识，使得代理能够更快地学习新的行为。在实验中，我们发现使用这种表示训练的策略在复杂环境下表现更好，优于通用图像表示和遵循指示的方法。

    

    人类可以通过利用背景世界知识快速学习新的行为。相比之下，利用强化学习训练的代理通常需要从零开始学习行为。因此，我们提出了一种新的方法，利用在互联网规模数据上预训练的视觉-语言模型（VLMs）中编码的大量通用和可索引的世界知识来进行具象的强化学习。我们通过将VLMs用作可提示表示来初始化策略：这些嵌入在视觉观察中具有基础，并根据VLM的内部知识编码语义特征，通过提供任务上下文和辅助信息来触发。我们在Minecraft和Habitat中的视觉复杂、长期的强化学习任务上评估了我们的方法。我们发现，使用通用型VLMs提取的嵌入训练的策略胜过使用通用的、不可提示的图像嵌入训练的策略。我们还发现我们的方法胜过遵循指示的元策略。

    Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
    
[^127]: 在Dempster-Shafer理论中处理共同分析中的认知不确定性

    Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory

    [https://arxiv.org/abs/2402.00060](https://arxiv.org/abs/2402.00060)

    本文提出了一种基于Dempster-Shafer理论的方法，用于在共同数据信息中建模认知不确定性和根据置信度对共同事件进行分类。通过构建概率箱和DSt结构，可以计算特定碰撞概率的置信度和可能性。

    

    本文提出了一种在共同数据信息（CDM）中建模认知不确定性和根据碰撞概率的置信度对共同事件进行分类的方法。本文提出的方法基于Dempster-Shafer论证的理论，假设观察到的CDMs来自一组未知分布的家族。使用Dvoretzky-Kiefer-Wolfowitz不等式从CDMs的时间序列构建鲁棒界限。然后使用DKW不等式构建的概率箱派生DSt结构。该DSt结构封装了时间序列上每个点的CDMs的不确定性，并允许计算特定碰撞概率实现的置信度和可能性。本文提出的方法在一些实际事件上进行了测试，并与欧洲现有的实践进行了比较。

    The paper presents an approach to the modelling of epistemic uncertainty in Conjunction Data Messages (CDM) and the classification of conjunction events according to the confidence in the probability of collision. The approach proposed in this paper is based on the Dempster-Shafer Theory (DSt) of evidence and starts from the assumption that the observed CDMs are drawn from a family of unknown distributions. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality is used to construct robust bounds on such a family of unknown distributions starting from a time series of CDMs. A DSt structure is then derived from the probability boxes constructed with DKW inequality. The DSt structure encapsulates the uncertainty in the CDMs at every point along the time series and allows the computation of the belief and plausibility in the realisation of a given probability of collision. The methodology proposed in this paper is tested on a number of real events and compared against existing practices in the Eu
    
[^128]: SERL: 用于样本高效的机器人强化学习的软件套件

    SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning

    [https://arxiv.org/abs/2401.16013](https://arxiv.org/abs/2401.16013)

    这个论文介绍了SERL软件套件，它是一个用于样本高效的机器人强化学习的库。该库包含了一个离线深度强化学习方法、计算奖励和重置环境的方法，高质量的机器人控制器，以及一些具有挑战性的示例任务。这个软件套件的目标是解决机器人强化学习的难以使用和获取性的挑战。

    

    近年来，在机器人强化学习领域取得了显著进展，使得可以处理复杂的图像观察，实际训练，并结合辅助数据（如示范和先前经验）。然而，尽管取得了这些进展，机器人强化学习仍然难以使用。从实践者中认识到，这些算法的具体实现细节对性能的影响常常与算法选择同样重要（如果不是更重要）。我们认为，机器人强化学习被广泛采用以及进一步发展机器人强化学习方法的一个重要挑战是这些方法的相对难以获取性。为了解决这个挑战，我们开发了一个精心实现的库，其中包含了一种高效样本离线深度强化学习方法，以及计算奖励和重置环境的方法，针对广泛采用的机器人的高质量控制器，以及一些具有挑战性的示例任务。

    In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task
    
[^129]: 如何合并生成和检索上下文以增强开放领域问答的语言模型的研究

    Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?

    [https://arxiv.org/abs/2401.11911](https://arxiv.org/abs/2401.11911)

    该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。

    

    虽然辅助信息已经成为增强大型语言模型（LLMs）的关键，但对于LLMs如何合并生成的和检索的上下文仍知之甚少。为了研究这一点，我们制定了一个系统性的框架来确定LLMs的响应是源自于生成的上下文还是检索的上下文。为了实现这个目标，我们构建了包含相互冲突的上下文的数据集，其中每个问题都与生成的和检索的上下文配对，但只有一个上下文包含了正确的答案。我们的实验证明，LLMs（如GPT-4/3.5和Llama2）存在显著的偏差，更倾向于生成的上下文，即使这些上下文提供了错误的信息。我们进一步确定了导致这种偏差的两个关键因素：i）LLMs生成的上下文通常与问题更相似，增加了其被选择的可能性；ii）检索上下文中使用的分割过程打断了其连贯性。

    While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
    
[^130]: LLMLight: 大型语言模型作为交通信号控制代理

    LLMLight: Large Language Models as Traffic Signal Control Agents

    [https://arxiv.org/abs/2312.16044](https://arxiv.org/abs/2312.16044)

    LLMLight是一个采用大型语言模型作为交通信号控制代理的新框架，通过借助先进的泛化能力和类似人类直觉的推理和决策过程，实现了有效的交通控制。此外，通过构建专为TSC任务定制的骨干语言模型LightGPT，进一步提升了LLMLight的效果和性能。

    

    交通信号控制（TSC）是城市交通管理的关键组成部分，旨在优化道路网络效率和减少拥堵。传统的TSC方法主要基于交通工程和强化学习（RL），往往在各种交通场景中存在泛化性不足和缺乏解释性等限制。本文提出了LLMLight，这是一个采用大型语言模型（LLMs）作为TSC决策代理的新框架。具体而言，该框架通过向LLM提供详细的实时交通状况说明作为指导，借助LLM的先进泛化能力，LLMLight实现了类似人类直觉的推理和决策过程，从而实现有效的交通控制。此外，我们构建了LightGPT，这是一个专为TSC任务量身定制的骨干LLM。通过学习细微的交通模式和控制策略，LightGPT在经济成本方面提升了LLMLight框架的效果。进行了大量的实验验证了LLMLight的有效性和性能优势。

    Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional methods in TSC, primarily based on transportation engineering and reinforcement learning (RL), often exhibit limitations in generalization across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments 
    
[^131]: PERP: 在LLMs时代重新思考修剪-重新训练范式

    PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs

    [https://arxiv.org/abs/2312.15230](https://arxiv.org/abs/2312.15230)

    本研究中，通过仅更新少部分高度表达力的参数，我们挑战了全参数重新训练的做法，在修剪后恢复或甚至提升了性能。PERP方法显著减少了计算量和存储需求。

    

    神经网络可以通过修剪实现高效压缩，显著减少存储和计算需求同时保持预测性能。像迭代幅值修剪（IMP，Han等，2015）这样的简单而有效的方法可以去除不重要的参数，并需要昂贵的重新训练过程以在修剪后恢复性能。然而，随着大型语言模型（LLMs）的兴起，由于内存和计算限制，完全重新训练变得不可行。在本研究中，我们挑战了重新训练所有参数的做法，通过证明只更新少部分高度表达力的参数通常足以恢复甚至提高性能。令人惊讶的是，仅重新训练GPT-结构的0.27%-0.35%的参数即可在不同稀疏水平上实现与一次性IMP相当的性能。我们的方法，即修剪后参数高效重新训练（PERP），大大减少了计算量。

    Neural Networks can be efficiently compressed through pruning, significantly reducing storage and computational demands while maintaining predictive performance. Simple yet effective methods like Iterative Magnitude Pruning (IMP, Han et al., 2015) remove less important parameters and require a costly retraining procedure to recover performance after pruning. However, with the rise of Large Language Models (LLMs), full retraining has become infeasible due to memory and compute constraints. In this study, we challenge the practice of retraining all parameters by demonstrating that updating only a small subset of highly expressive parameters is often sufficient to recover or even improve performance compared to full retraining. Surprisingly, retraining as little as 0.27%-0.35% of the parameters of GPT-architectures achieves comparable performance to One Shot IMP across various sparsity levels. Our approach, Parameter-Efficient Retraining after Pruning (PERP), drastically reduces compute a
    
[^132]: 使用生成式人工智能生成单元测试：自动生成工具的性能比较分析

    Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools

    [https://arxiv.org/abs/2312.10622](https://arxiv.org/abs/2312.10622)

    本研究通过比较分析，实验性地研究了使用生成式人工智能（如ChatGPT）生成Python程序的单元测试脚本的有效性。结果显示，ChatGPT在覆盖率方面与现有的单元测试生成器Pynguin相当，在某些情况下性能优于Pynguin。然而，对于某些类别，ChatGPT生成的断言约有三分之一是不正确的。

    

    生成单元测试是软件开发中一个关键的任务，程序员需要花费大量时间和精力。大型语言模型（LLM）的出现为单元测试脚本的生成提供了一种新的方式。本研究旨在实验性地研究LLM的有效性，特别是以ChatGPT为代表，用于生成Python程序的单元测试脚本，并将生成的测试用例与现有的单元测试生成器（Pynguin）生成的进行比较。在实验中，我们考虑了三种类型的代码单元：1）过程脚本，2）基于函数的模块化代码，3）基于类的代码。生成的测试用例基于覆盖率、正确性和可读性等标准进行评估。我们的研究结果表明，ChatGPT在覆盖率方面与Pynguin相当，但在某些情况下其性能优于Pynguin。我们还发现，对于某些类别，ChatGPT生成的断言约有三分之一是不正确的。

    Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our
    
[^133]: 大型语言模型是临床推理者：基于提示生成的理由的推理感知诊断框架

    Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales

    [https://arxiv.org/abs/2312.07399](https://arxiv.org/abs/2312.07399)

    该论文提出了一种基于提示生成的理由的“推理感知”诊断框架，通过大型语言模型来进行临床推理，实现了在疾病诊断过程中的高效、时间节约和劳动节约的方法。

    

    由于大型语言模型（LLMs）的进展，机器推理在近年来取得了巨大的进展。然而，在临床领域，大多数以自然语言处理为驱动的项目主要集中在临床分类或阅读理解上，并且由于与临床医生的理念注解成本较高，对于疾病诊断的临床推理还未得到充分的研究。在这项工作中，我们提出了一个“推理感知”的诊断框架，通过基于提示的学习以一种高效的时间和劳动方式去理性化诊断过程，并学习对提示生成的理由进行推理。具体而言，我们解决了疾病诊断的临床推理问题，其中LLM生成了诊断性的理由，提供其对呈现的患者数据的见解以及达到诊断的推理路径，即临床思维链（Clinical CoT）。我们通过广泛的实验和分析在理由生成和疾病诊断方面实证了LLMs/LMs的临床推理能力。

    Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various s
    
[^134]: 计算版权: 面向音乐生成AI的版税模型

    Computational Copyright: Towards A Royalty Model for Music Generative AI

    [https://arxiv.org/abs/2312.06646](https://arxiv.org/abs/2312.06646)

    本文旨在解决音乐生成AI领域中的版权问题，提出了一种用于AI音乐生成平台的版税模型，并探讨了对AI生成音乐进行版权归因的算法解决方案。

    

    生成AI的进步引发了版权挑战，在音乐行业尤为突出。本文关注这些挑战的经济方面，强调经济影响在版权领域中构成一个核心问题。黑盒生成AI技术的复杂性不仅表明，而且需要算法解决方案。然而，这样的解决方案在很大程度上缺失，导致监管挑战。我们旨在通过为AI音乐生成平台提出潜在的版税模型来弥补当前方法的差距。我们的方法涉及对Spotify和YouTube等平台现有版税模型的详细分析，并将其调整到AI生成音乐的独特背景中。我们面临的一个重要挑战是将AI生成的音乐归因于训练数据中有影响力的版权内容。为此，我们提出了利用数据归因的算法解决方案。

    The advancement of generative AI has given rise to pressing copyright challenges, particularly in music industry. This paper focuses on the economic aspects of these challenges, emphasizing that the economic impact constitutes a central issue in the copyright arena. The complexity of the black-box generative AI technologies not only suggests but necessitates algorithmic solutions. However, such solutions have been largely missing, leading to regulatory challenges in this landscape. We aim to bridge the gap in current approaches by proposing potential royalty models for revenue sharing on AI music generation platforms. Our methodology involves a detailed analysis of existing royalty models in platforms like Spotify and YouTube, and adapting these to the unique context of AI-generated music. A significant challenge we address is the attribution of AI-generated music to influential copyrighted content in the training data. To this end, we present algorithmic solutions employing data attri
    
[^135]: 在异质性和谱问题下重新审视基于图的欺诈检测

    Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum

    [https://arxiv.org/abs/2312.06441](https://arxiv.org/abs/2312.06441)

    本文提出了一种基于半监督GNN的欺诈检测器SEC-GFD，通过混合过滤模块和局部环境约束模块解决了异质性和标签利用问题。

    

    基于图的欺诈检测（GFD）可视为一项具有挑战性的半监督节点二分类任务。近年来，图神经网络（GNN）已广泛应用于GFD，通过聚合邻居信息来刻画节点的异常可能性。然而，欺诈图在本质上是异质的，因此大多数GNN由于假设同质性而表现不佳。此外，由于存在异质性和类别不平衡问题，现有模型未充分利用宝贵的节点标签信息。为了解决上述问题，本文提出了一种基于半监督GNN的欺诈检测器SEC-GFD。该检测器包括混合过滤模块和局部环境约束模块，这两个模块分别用于解决异质性和标签利用问题。第一个模块从谱域的角度出发，在一定程度上解决了异质性问题。具体而言，它将图分割称不同的谱成分，

    Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides t
    
[^136]: 引起多义性的原因是什么？通过偶然因素的混合选择性的替代起源故事

    What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes

    [https://arxiv.org/abs/2312.03096](https://arxiv.org/abs/2312.03096)

    这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。

    

    多义性神经元——激活一组不相关特征的神经元——被视为解释任务优化深度网络的显著障碍，对AI安全性产生影响。传统的多义性起源故事是数据包含的“特征”多于神经元，因此学习执行任务迫使网络将多个不相关特征分配给同一个神经元，危及我们理解网络内部处理的能力。在这项工作中，我们提出了多义性的第二个且非互斥的替代起源故事。我们展示了即使有足够的神经元来表示数据中的所有特征，偶然多义性也可能产生，这是一种我们称之为“偶然多义性”的现象。通过理论和实验证明，偶然多义性可以由多种原因引起，包括正则化和神经噪音；这种偶然多义性发生是因为随机的因素。

    Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
    
[^137]: 多步骤对话工作流动作预测

    Multi-Step Dialogue Workflow Action Prediction

    [https://arxiv.org/abs/2311.09593](https://arxiv.org/abs/2311.09593)

    本文提出了多步骤工作流动作预测的新问题，通过准确预测多个步骤，实现对任务的多轮自动化，节省时间。提出了三种简单易行的建模方法，并展示了多步骤动作预测提高对话任务准确性和步骤自动化的特征。

    

    在面向任务的对话中，系统通常需要遵循一系列动作的顺序，称为工作流，以便根据一组准则完成任务。本文提出了多步骤工作流动作预测的新问题，系统预测未来的多个工作流动作。准确预测多个步骤可以实现多轮自动化，从而能够节省时间专注于更复杂的任务。我们提出了三种简单易行且能提高动作自动化的建模方法：1）在训练数据集上进行微调，2）使用检索和大型语言模型提示进行少样本上下文学习，以及3）零样本图遍历，将历史动作序列汇总成图进行预测。我们展示了多步骤动作预测产生的特征，可以提高下游对话任务（如预测任务成功）的准确性，并可以在不需要太多反馈的情况下将步骤的自动化提高20%。

    In task-oriented dialogue, a system often needs to follow a sequence of actions, called a workflow, that complies with a set of guidelines in order to complete a task. In this paper, we propose the novel problem of multi-step workflow action prediction, in which the system predicts multiple future workflow actions. Accurate prediction of multiple steps allows for multi-turn automation, which can free up time to focus on more complex tasks. We propose three modeling approaches that are simple to implement yet lead to more action automation: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction. We show that multi-step action prediction produces features that improve accuracy on downstream dialogue tasks like predicting task success, and can increase automation of steps by 20% without requiring as much feedback
    
[^138]: 在因果推断中的遗漏标签: 一项关于悖论的研究

    Omitted Labels in Causality: A Study of Paradoxes

    [https://arxiv.org/abs/2311.06840](https://arxiv.org/abs/2311.06840)

    本研究探讨了所谓的“遗漏标签上下文”，即训练数据仅限于可能标签的一个子集，并利用悖论展示了在这种上下文中因果推断面临的困难。研究发现在某些情况下，必须使用非可交换的处理组和对照组进行正确的校正。此外，研究还发现了结论网络与社会选择理论之间的有趣联系。

    

    我们探讨了我们所称之为“遗漏标签上下文”的概念，即训练数据仅限于可能标签的一个子集。这种设置在专业人士或特定的专注研究中非常普遍。我们利用已广泛研究的悖论（辛普森悖论和康多塞悖论）来说明在遗漏标签上下文中因果推断面临的更普遍困难。与因果推断基本原理相反，我们展示了“正确”的校正有时需要非可交换的处理组和对照组。这些陷阱引导我们研究不同上下文中得出的结论网络和其形成的结构，从而证明了这些网络与社会选择理论的有趣联系。

    We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
    
[^139]: PowerFlowNet: 使用消息传递图神经网络进行功率流近似

    PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks

    [https://arxiv.org/abs/2311.03415](https://arxiv.org/abs/2311.03415)

    PowerFlowNet 是一种使用消息传递图神经网络进行功率流近似的新型架构，与传统的牛顿-拉夫逊方法相比，在简单的系统中速度提高了4倍，在实际的法国高电压网络中提高了145倍，同时在性能和执行时间方面明显优于其他传统方法。

    

    准确且高效的功率流分析对于现代电力网络的运行和规划至关重要。因此，需要能够为小型和大型电力网络提供准确和快速解的可扩展算法。由于电力网络可以被解释为一个图，图神经网络(GNNs)已经成为通过利用底层图结构的信息共享来改善功率流近似的准确性和速度的一种有前景的方法。在这项研究中，我们介绍了PowerFlowNet，一个新颖的GNN架构，用于功率流近似，在简单的IEEE 14总线系统中与传统的牛顿-拉夫逊方法展示了相似的性能，但在法国高电压网络(6470rte)的真实情况下实现了4倍的速度提升。同时，与其他传统的近似方法(如直流松弛法)相比，在性能和执行时间方面显著优于它们，从而实现了优越的表现。

    Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
    
[^140]: LILO：通过压缩和文档化代码学习可解释库

    LILO: Learning Interpretable Libraries by Compressing and Documenting Code

    [https://arxiv.org/abs/2310.19791](https://arxiv.org/abs/2310.19791)

    LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。

    

    尽管大型语言模型（LLMs）在代码生成方面表现出色，但软件开发的关键方面是重构的艺术：将代码整合到可重用和可读的程序库中。本文介绍了一种名为LILO的神经符号框架，它通过迭代地合成、压缩和文档化代码来构建适合特定问题领域的库。LILO将LLM引导的程序合成与Stitch自动重构的近期算法进展相结合：Stitch是一个符号压缩系统，可以高效地识别大型代码语料库中的最佳lambda抽象。为了使这些抽象可解释，我们引入了一种自动文档（AutoDoc）过程，它根据上下文中的使用示例推断出自然语言名称和文档字符串。除了提高人类可读性外，我们发现AutoDoc通过帮助LILO的合成器解释和部署学习到的抽象来提高性能。我们对LILO进行了三个归纳式程序综合的评估。

    While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
    
[^141]: 使用机器学习和经典技术的统计推断：基于积累的局部效应（ALE）

    Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)

    [https://arxiv.org/abs/2310.09877](https://arxiv.org/abs/2310.09877)

    本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。

    

    积累的局部效应（ALE）是一种对黑盒机器学习（ML）算法结果进行全局解释的模型无关方法。使用ALE进行统计推断面临至少三个挑战：确保ALE分析的可靠性，尤其在小数据集的情况下；直观地表征变量在ML中的整体效应；以及从ML数据分析中进行健壮的推断。为此，我们引入了创新的工具和技术，使用ALE进行统计推断，建立了适应数据集大小的自助法置信区间，并引入了直观指示对结果变量和标准化尺度上的效应的ALE效应大小度量。此外，我们演示了如何使用这些工具绘制可靠的统计推断，反映了ALE熟练突出的灵活模式，实现了R中“ale”包中的实现。这项工作推动了关于ALE及其应用的讨论。

    Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
    
[^142]: SWAP: 稀疏熵式Wasserstein回归用于鲁棒网络剪枝

    SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning

    [https://arxiv.org/abs/2310.04918](https://arxiv.org/abs/2310.04918)

    本研究提出了一种名为SWAP的网络剪枝方法，采用稀疏熵式Wasserstein回归来解决神经网络剪枝中的梯度不准确问题。SWAP在噪声抑制和协方差信息保留之间取得了平衡，具有较小的计算成本，与最先进的网络剪枝算法具有可比较的性能。

    

    本研究解决了神经网络剪枝中计算经验Fisher信息矩阵(FIM)时存在不准确梯度的问题。我们引入了SWAP，一种基于最优输运问题的熵式Wasserstein回归(EWR)网络剪枝方法。通过在优化中将常用的标准线性回归(LR)和EWR交换展示，SWAP在采用邻近插值跨数据点时在噪声抑制方面具有显著优势，同时增加了较小的额外计算成本。SWAP的独特优势在于能够在噪声减少和协方差信息保留之间取得平衡。在多个网络上进行的广泛实验表明，SWAP与最先进的网络剪枝算法具有可比较的性能。当网络规模或目标稀疏度较大时，我们的方法优于最先进算法，且在存在噪声的情况下，优势更加明显。

    This study tackles the issue of neural network pruning that inaccurate gradients exist when computing the empirical Fisher Information Matrix (FIM). We introduce SWAP, an Entropic Wasserstein regression (EWR) network pruning formulation, capitalizing on the geometric attributes of the optimal transport (OT) problem. The "swap" of a commonly used standard linear regression (LR) with the EWR in optimization is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points, yet incurs marginal extra computational cost. The unique strength of SWAP is its intrinsic ability to strike a balance between noise reduction and covariance information preservation. Extensive experiments performed on various networks show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy 
    
[^143]: 基于条件扩散模型的电力客户定制负荷曲线合成

    Customized Load Profiles Synthesis for Electricity Customers Based on Conditional Diffusion Models

    [https://arxiv.org/abs/2304.12076](https://arxiv.org/abs/2304.12076)

    本文提出了一种基于条件扩散模型的定制负荷曲线合成方法，用于解决电力客户的数据短缺问题，并实现高质量负荷曲线的合成。

    

    客户的负荷曲线是支持现代电力系统数据分析应用的关键资源。然而，由于采集成本和数据隐私问题，通常会缺乏足够的历史负荷曲线进行数据分析。为了解决数据短缺问题，负荷曲线合成是一种有效的技术，为客户提供合成的训练数据，以建立高性能的数据驱动模型。然而，由于客户负荷的高异质性，使用基于各自客户数据训练的生成模型合成高质量的负荷曲线仍然具有挑战性。在本文中，我们提出了一种新颖的基于条件扩散模型的定制负荷曲线合成方法，用于异构客户。具体而言，我们首先将定制合成转化为条件数据生成问题。然后，我们扩展传统的扩散模型为条件扩散模型，实现条件数据生成。

    Customers' load profiles are critical resources to support data analytics applications in modern power systems. However, there are usually insufficient historical load profiles for data analysis, due to the collection cost and data privacy issues. To address such data shortage problems, load profiles synthesis is an effective technique that provides synthetic training data for customers to build high-performance data-driven models. Nonetheless, it is still challenging to synthesize high-quality load profiles for each customer using generation models trained by the respective customer's data owing to the high heterogeneity of customer load. In this paper, we propose a novel customized load profiles synthesis method based on conditional diffusion models for heterogeneous customers. Specifically, we first convert the customized synthesis into a conditional data generation issue. We then extend traditional diffusion models to conditional diffusion models to realize conditional data generat
    
[^144]: 选择性不确定性传播在离线强化学习中的应用

    Selective Uncertainty Propagation in Offline RL

    [https://arxiv.org/abs/2302.00284](https://arxiv.org/abs/2302.00284)

    本论文提出了一种名为选择性不确定性传播的方法，用于解决离线强化学习中的分布偏移问题。该方法通过自适应的方式建立置信区间，有效地处理了实际问题中策略学习的挑战。

    

    本研究考虑有限时间段离线强化学习的情景，目标在于应对动态规划算法中每一步策略学习的挑战。通过评估离开行为策略在第h步时的处理效果，就可以学习到这一步的策略。由于每一步策略都会影响下一状态的分布，相关的分布偏移问题使得这一问题在统计学上比随机情境挑战下的处理效果估计更加困难。然而，许多现实强化学习问题的难度介于这两种情境之间。我们开发了一种灵活且通用的方法，名为选择性不确定性传播，用于建立置信区间，并根据相关分布偏移问题的难度进行自适应。在玩具环境中展示了我们方法的优势，并证明了这些技术在离线策略学习中的好处。

    We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
    
[^145]: LLMs实现可扩展的定性编码：思维链推理在某些解释学任务中能达到人类水平

    Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])

    [http://arxiv.org/abs/2401.15170](http://arxiv.org/abs/2401.15170)

    本研究证明了大型语言模型在定性编码中的应用潜力。相比于GPT-3.5，GPT-4能够实现与人类相当的解释能力，并具有较高的编码一致性。无论模型规模大小，只要满足一定条件，模型都可以实现较高的编码准确性。

    

    定性编码或内容分析从文本中提取含义，以识别跨文本语料库的定量模式。最近，大型语言模型（LLMs）在解释能力方面的进展为自动化编码过程（对文本应用类别标签）提供了潜力，从而使人类研究人员能够专注于更有创造力的研究方面，同时将这些解释任务委托给人工智能。我们的案例研究包括对人文学研究具有代表性的密集段落的一组社会历史编码。我们发现GPT-4能够达到与人类相当的解释，而GPT-3.5则不能。与我们由人类获得的金标准相比，GPT-4在3个编码中具有优秀的编码一致性（Cohen's κ ≥ 0.79），在9个编码中有8个具有显著的一致性（κ ≥ 0.6）。相比之下，GPT-3.5在所有编码中表现不佳（mean(κ) = 0.34；max(κ) = 0.55）。重要的是，我们发现编码的准确性不受模型规模影响，在满足一定条件的情况下，较小的模型也可以实现较高的编码准确性。

    Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq 0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding fidelity
    
[^146]: PROXYQA：一种用于评估大型语言模型长篇文本生成的替代框架

    PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])

    [http://arxiv.org/abs/2401.15042](http://arxiv.org/abs/2401.15042)

    PROXYQA是一个用于评估大型语言模型长篇文本生成的替代框架，通过生成详尽的内容，并利用评估器和生成内容作为背景环境，根据评估器回答代理问题的表现来评估生成内容的质量。

    

    大型语言模型（LLM）在长篇文本理解任务中取得了显著的成功。然而，它们生成长篇内容（如报告和文章）的能力尚未得到充分探索。当前的基准不足以充分评估LLMs生成信息丰富且全面的内容，因此需要一种更严格的评估方法。在本研究中，我们介绍了一种名为\textsc{ProxyQA}的框架，用于评估长篇文本生成，包括深入人工策划的涵盖多个领域的“元问题”。每个元问题都包含相应的带注释答案的“代理问题”。LLMs被要求根据这些元问题生成详尽的内容。利用评估器并将生成的内容作为背景环境，\textsc{ProxyQA}根据评估器回答“代理问题”的表现评估生成内容的质量。我们检验了多个LLMs，重点关注了...

    Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
    
[^147]: 用于甲骨文字符演变的开放数据集：EVOBC

    An open dataset for the evolution of oracle bone characters: EVOBC. (arXiv:2401.12467v1 [cs.AI])

    [http://arxiv.org/abs/2401.12467](http://arxiv.org/abs/2401.12467)

    本研究收集了古代字符数据集，揭示了甲骨文字符在六个历史阶段的演变过程，为解读甲骨文铭文提供了有价值的资源。

    

    最早的中文字符源自甲骨文铭文，与其他东亚语言密切相关。这些铭文对人类学和考古学具有巨大价值。然而，解读甲骨文仍然是一个巨大的挑战，迄今为止只有约1600个4500多个现存字符得到诠释。需要进一步的学术研究，全面了解这种古代书写系统。人工智能技术在解读甲骨文字符方面具有潜力，特别是在字符演变方面。然而，挑战之一是缺乏映射这些字符演变的数据集。在本研究中，我们系统地收集了古代字符，涵盖了甲骨文（公元前15世纪）、金文（公元前13世纪至公元221年）、篆书（公元前11至8世纪）、秦简（公元前221至206年）、小篆（公元前206至8世纪）、楷书（公元2至5世纪）这六个历史阶段的文字。

    The earliest extant Chinese characters originate from oracle bone inscriptions, which are closely related to other East Asian languages. These inscriptions hold immense value for anthropology and archaeology. However, deciphering oracle bone script remains a formidable challenge, with only approximately 1,600 of the over 4,500 extant characters elucidated to date. Further scholarly investigation is required to comprehensively understand this ancient writing system. Artificial Intelligence technology is a promising avenue for deciphering oracle bone characters, particularly concerning their evolution. However, one of the challenges is the lack of datasets mapping the evolution of these characters over time. In this study, we systematically collected ancient characters from authoritative texts and websites spanning six historical stages: Oracle Bone Characters - OBC (15th century B.C.), Bronze Inscriptions - BI (13th to 221 B.C.), Seal Script - SS (11th to 8th centuries B.C.), Spring and
    
[^148]: 强化学习代理中的新兴支配等级

    Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])

    [http://arxiv.org/abs/2401.12258](http://arxiv.org/abs/2401.12258)

    本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。

    

    现代强化学习算法在各种任务中能够胜过人类。多智能体强化学习(MARL)设置提出了额外的挑战，成功的混合动机代理协作取决于个体和群体目标之间的微妙平衡。社会习惯和规范，往往受到人类机构的启发，被用作实现这种平衡的工具。在本文中，我们研究了一种基本且经过深入研究的社会习惯，即支配等级，它在动物和人类社会中都存在。我们将支配等级的行为理论应用于人工智能代理，并尽可能少地修改现有的术语和定义。我们证明，在没有明确编程或内在奖励的情况下，强化学习代理的群体能够发明、学习、实施和传递支配等级给新的群体。所产生的支配等级有一个

    Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
    
[^149]: 多语言指令调优中的多语言性

    Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])

    [http://arxiv.org/abs/2401.01854](http://arxiv.org/abs/2401.01854)

    本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    

    随着大型语言模型（LLMs）的全球采纳，它们在多语言指令遵循能力变得越来越重要。一种有前途的方法是跨语言转移，通过在另一种语言上微调，模型可以在某种语言上获得特定的功能。本文研究了多语言LLM在指令调优过程中的多语言性对跨语言指令遵循的影响。首先我们发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，我们发现在英语调优集合中，只有40个多语言示例能够显著提高多语言指令遵循，在调优过程中不论是已见语言还是未见语言。总的来说，我们观察到在多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
    
[^150]: 自我对弱语言模型进行细调可以将其转化为强语言模型

    Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])

    [http://arxiv.org/abs/2401.01335](http://arxiv.org/abs/2401.01335)

    本文提出了一种名为自我对弱语言模型进行细调（SPIN）的方法，通过模型自我对弈生成训练数据，并从中优化模型策略，从而将弱语言模型转化为强语言模型，无需额外的人类标注数据。

    

    通过监督细调（SFT）利用人类标注数据的力量对于推进大型语言模型（LLMs）至关重要。本文探讨了在不需要获取额外人类标注数据的情况下，将弱语言模型发展成为强语言模型的可能性。我们提出了一种名为自我对弱语言模型进行细调（SPIN）的新的细调方法，该方法从一个经过监督细调的模型开始。SPIN的核心是自我对弱语言模型的机制，其中弱语言模型通过与自身的实例对弈来提升自己的能力。具体而言，弱语言模型通过生成自己的训练数据来优化自身策略，通过区分自我生成的回应与来自人类标注数据的回应来改进。我们的方法逐步将弱语言模型提升为强大的模型，充分发掘人类标注示范数据在SFT中的潜力。在理论上，我们证明了该方法的训练目标函数的全局最优解是可以达到的。

    Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
    
[^151]: LLbezpeky: 利用大型语言模型进行漏洞检测

    LLbezpeky: Leveraging Large Language Models for Vulnerability Detection. (arXiv:2401.01269v1 [cs.CR])

    [http://arxiv.org/abs/2401.01269](http://arxiv.org/abs/2401.01269)

    LLbezpeky是一项利用大型语言模型进行漏洞检测的研究，研究发现LLMs在理解人类和编程语言中的语义方面展现出巨大潜力，并通过构建一个AI驱动的工作流程来帮助开发人员识别和修复漏洞。

    

    尽管在构建安全系统方面进行了持续的研究和进展，但安卓应用程序仍然存在漏洞，需要有效的检测方法。目前的静态和动态分析工具策略存在一些限制，如大量的误报和有限的分析范围，使得难以采用。在过去的几年中，基于机器学习的方法在漏洞检测方面得到了广泛探索，但其在实际应用中受到数据需求和特征工程挑战的限制。大型语言模型（LLMs）凭借其庞大的参数，在理解人类和编程语言中的语义方面展现出巨大潜力。我们深入研究了在安卓安全的背景下，LLMs用于检测漏洞的效果。我们的重点是构建一个基于人工智能的工作流程，帮助开发人员识别和修复漏洞。我们的实验结果表明，LLMs能够有效检测出漏洞。

    Despite the continued research and progress in building secure systems, Android applications continue to be ridden with vulnerabilities, necessitating effective detection methods. Current strategies involving static and dynamic analysis tools come with limitations like overwhelming number of false positives and limited scope of analysis which make either difficult to adopt. Over the past years, machine learning based approaches have been extensively explored for vulnerability detection, but its real-world applicability is constrained by data requirements and feature engineering challenges. Large Language Models (LLMs), with their vast parameters, have shown tremendous potential in understanding semnatics in human as well as programming languages. We dive into the efficacy of LLMs for detecting vulnerabilities in the context of Android security. We focus on building an AI-driven workflow to assist developers in identifying and rectifying vulnerabilities. Our experiments show that LLMs o
    
[^152]: 受控解码来自语言模型

    Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])

    [http://arxiv.org/abs/2310.17022](http://arxiv.org/abs/2310.17022)

    本论文提出了一种名为受控解码（CD）的离策略强化学习方法，用于控制语言模型的生成，以达到高回报的结果。CD通过前缀评分器来引导生成，可以在推理时预测预期回报，并且具有模块化设计，可用于解决多目标强化学习问题，而不增加复杂性。

    

    我们提出了一种新颖的离策略强化学习方法，称为受控解码（CD），用于控制自回归语言模型的生成，以获得高回报的结果。CD通过值函数来解决离策略强化学习问题，该值函数被称为前缀评分器。前缀评分器在推理时用于引导生成向更高回报的结果。我们展示了前缀评分器可以从（可能是）离策略数据中训练出来，用于预测从部分解码的响应继续解码时的预期回报。我们在Reddit对话语料库上经验证明，CD作为一种控制机制是有效的。我们还展示了CD设计的模块化使其能够有效解决多目标强化学习问题，而不会增加任何复杂性。最后，我们展示了CD可以以一种新颖的分块方式在推理时应用，同样无需任何额外的操作。

    We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
    
[^153]: 在DAG空间中使用基于模型的强化学习进行因果发现的树搜索

    Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])

    [http://arxiv.org/abs/2310.13576](http://arxiv.org/abs/2310.13576)

    本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。

    

    识别因果结构对于许多领域至关重要，从战略决策到生物学和经济学。在这项工作中，我们提出了一种基于树搜索的因果发现的模型驱动强化学习方法，该方法逐步构建有向无环图。我们还形式化并证明了一种排除会引入循环的边的高效算法的正确性，这使得在DAG空间中进行更深入的离散搜索和采样成为可能。我们在两个实际任务中评估了我们的方法，在性能上比最先进的无模型方法和贪婪搜索取得了显著优势，这是组合方法的一个有希望的进展。

    Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
    
[^154]: 人类课程指导下的指令调整

    Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])

    [http://arxiv.org/abs/2310.09518](http://arxiv.org/abs/2310.09518)

    本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。

    

    指令调整的主流范式是随机洗牌训练最大多样化指令-响应对。本文探讨了在当代大型语言模型如ChatGPT和GPT-4中应用结构化认知学习方法进行指令调整的潜在好处。与以往传统的随机化指令数据集不同，我们提出了一个高度结构化的合成数据集，模拟了人类教育的渐进性和有组织性。我们通过将数据集与教育框架对齐来策划我们的数据集，为每个样本包括主题和认知严谨程度等元信息。我们的数据集涵盖了从中学到研究生阶段的全面细粒度主题，每个主题都有各种问题，以利用布鲁姆的认知分级法提高概念深度，该分级法用于区分每个概念的不同人类认知水平。结果表明，这种认知学习方法优于传统的随机化方法，提高了指令调整的性能。

    The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
    
[^155]: CIDER: 基于类别引导的意图分离方法用于准确的个性化新闻推荐

    CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])

    [http://arxiv.org/abs/2310.09401](http://arxiv.org/abs/2310.09401)

    CIDER是一种基于类别引导的个性化新闻推荐框架，通过意图分离和一致性的新闻表示来准确理解新闻文章的多个意图，并区分用户不同的后阅读偏好。

    

    个性化新闻推荐旨在帮助用户找到与其兴趣相符的新闻文章，这在缓解用户信息过载问题方面起到至关重要的作用。尽管许多最近的研究致力于改进用户和新闻的表示方法，但以下挑战很少被研究：（C1）如何准确理解一篇新闻文章中包含的多个意图？以及（C2）如何区分用户点击历史中对新闻文章有不同后阅读偏好的情况？为了同时解决这两个挑战，在本文中，我们提出了一种新的个性化新闻推荐框架（CIDER），它利用（1）基于类别引导的意图分离来解决（C1）和（2）基于一致性的新闻表示来解决（C2）。此外，我们将类别预测纳入CIDER的训练过程作为辅助任务，这提供了额外的监督信号，以增强意图分离。在两个真实数据集上进行了广泛的实验。

    Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
    
[^156]: 为程序验证对LLM生成的循环不变式进行排名

    Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])

    [http://arxiv.org/abs/2310.09342](http://arxiv.org/abs/2310.09342)

    本研究提出了一种针对LLM生成结果进行重新排名的方法，可以显著提高正确不变量的排名，从而减少程序验证的调用次数。

    

    合成归纳循环不变量是自动化程序验证的基础。我们观察到，大型语言模型（如gpt-3.5或gpt-4）能够在0-shot环境下为一类程序合成循环不变量，但需要多个样本才能生成正确的不变量。这可能导致大量调用程序验证器来建立不变性。为了解决这个问题，我们提出了一种对LLM生成结果进行重新排名的方法。我们设计了一个排名器，可以根据问题定义区分正确的归纳不变量和错误的尝试。该排名器经过对比排名优化。实验结果表明，这种重新排名机制显著提高了正确不变量在生成的候选项中的排名，从而大幅减少了对验证器的调用次数。

    Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
    
[^157]: 驳斥Shapley Values用于可解释性的论证

    A Refutation of Shapley Values for Explainability. (arXiv:2309.03041v1 [cs.AI])

    [http://arxiv.org/abs/2309.03041](http://arxiv.org/abs/2309.03041)

    这篇论文驳斥了Shapley Values在规则解释中的适用性，并证明了存在布尔函数，使得Shapley值给出的特征重要性信息具有误导性。该论文提供了一种蛮力方法来识别这种问题，但对于特征数量较大的布尔函数仍存在问题。

    

    最近的研究证明了在基于规则的解释中，Shapley值对特征的相对重要性提供了误导性信息的布尔函数的存在。这些误导性信息被广泛分类为几个可能的问题。每个问题都涉及与预测相关的特征的相关性或无关性，并且在基于规则的解释中，所有这些问题都与Shapley值的不足有关。其中的一个研究使用了一种蛮力方法来识别仅包含少数特征和相关实例的布尔函数，展示了Shapley值不足的问题，并作为证明在规则解释中Shapley值的不足的证据。然而，一个重要的问题是这种Shapley值不足的问题在具有任意大数量特征的布尔函数中有多频繁出现。很显然，蛮力方法不太可能提供洞察如何处理具有任意大数量特征的问题的见解。

    Recent work demonstrated the existence of Boolean functions for which Shapley values provide misleading information about the relative importance of features in rule-based explanations. Such misleading information was broadly categorized into a number of possible issues. Each of those issues relates with features being relevant or irrelevant for a prediction, and all are significant regarding the inadequacy of Shapley values for rule-based explainability. This earlier work devised a brute-force approach to identify Boolean functions, defined on small numbers of features, and also associated instances, which displayed such inadequacy-revealing issues, and so served as evidence to the inadequacy of Shapley values for rule-based explainability. However, an outstanding question is how frequently such inadequacy-revealing issues can occur for Boolean functions with arbitrary large numbers of features. It is plain that a brute-force approach would be unlikely to provide insights on how to ta
    
[^158]: 大尺度和无穷宽度下的深度学习勒让演讲

    Les Houches Lectures on Deep Learning at Large & Infinite Width. (arXiv:2309.01592v1 [stat.ML])

    [http://arxiv.org/abs/2309.01592](http://arxiv.org/abs/2309.01592)

    本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。

    

    这些演讲是在2022年勒让夏季学校统计物理和机器学习课程上展示的，着重探讨了深度神经网络在无限宽度和大宽度范围内的情况。涵盖的主题包括这些网络的各种统计和动力学特性。特别是，讲师们讨论了随机深度神经网络的特性；训练过的深度神经网络，线性模型，核函数和高斯过程之间的联系，这些联系在无穷宽度的极限下出现；以及在初始化和训练后对大但有限宽度网络的摄动和非摄动处理。

    These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
    
[^159]: SafeAR: 通过风险感知策略实现更安全的算法补偿

    SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])

    [http://arxiv.org/abs/2308.12367](http://arxiv.org/abs/2308.12367)

    本文提出了一种更安全的算法补救方法（SafeAR），该方法通过考虑风险因素在计算和评估补救措施时，为那些受到机器学习模型决策不利影响的个体提供更可靠的建议。

    

    随着机器学习模型在金融和医疗等关键领域的广泛使用，为那些受到机器学习模型决策不利影响的个体提供补救措施的需求变得更加重要；个体应该获得改善自身情况和获得有利决策的建议。之前关于顺序算法补救的工作——推荐一系列变化——主要关注行动的可行性，并使用特征变化的接近程度确定行动成本。然而，未考虑特征变化的不确定性和补救中高于平均成本的风险。如果补救措施可能（以一定概率）导致更糟糕的情况，而恢复需要付出非常高的代价，那将是不可取的。在计算和评估补救措施时，必须考虑风险。我们将考虑了这种风险因素计算出的补救措施称为更安全的算法补救（SafeAR）。

    With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
    
[^160]: LaFiCMIL：从相关多实例学习的角度重新思考大文件分类

    LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])

    [http://arxiv.org/abs/2308.01413](http://arxiv.org/abs/2308.01413)

    LaFiCMIL是一个新的方法，从相关多实例学习的角度解决了Transformer模型输入长度限制的问题，可以用于改进大文件分类任务。

    

    基于Transformer的模型在各种语言任务的性能上取得了革命性的突破。直观上，人们可能会期望文本分类，作为不需要像生成任务那样许多高级表示的任务，能够充分利用Transformer强大的表示能力来进行综合性的处理。然而，实际上，在多类别和多标签分类长文本文档和其他大文件的领域仍然存在较大的改进潜力。Transformer模型的性能主要受到一个重要限制的阻碍：有限的输入长度，比如BERT的512个标记。虽然增加GPU内存可以稍微扩展这个限制，但实际应用中往往受限于有限的GPU资源。在这项工作中，我们从相关多实例学习的角度解决了输入限制问题。所提出的方法LaFiCMIL，作为一个多功能的框架，适用于

    Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
    
[^161]: 非线性处理与线性光学

    Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2307.08533](http://arxiv.org/abs/2307.08533)

    该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。

    

    深度神经网络通过利用多层数据处理来提取隐藏的表征，取得了显着的突破，但却以大电子计算能力为代价。为了提高能量效率和速度，光学实现神经网络的目标是利用光学带宽的优势和光学互连的能量效率。在缺乏低功率光学非线性性的情况下，在实现多层光学网络中的挑战在于实现多个光学层，而不依赖电子元件。在本研究中，我们提出了一个新颖的框架，利用多次散射可以同时以低光功率合成可编程的线性和非线性转换，利用散射势能（由数据表示）与散射场之间的非线性关系。理论和实验研究表明，通过多次散射进行数据重复可以实现多个光学层。

    Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
    
[^162]: 《可解释人工智能中的对抗性攻击和防御：调查报告》

    Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])

    [http://arxiv.org/abs/2306.06123](http://arxiv.org/abs/2306.06123)

    本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。

    

    可解释人工智能（XAI）方法被描绘为调试和信任统计和深度学习模型的治疗方式，以及解释它们的预测。然而，对抗机器学习的最新进展突出了最新解释的局限性和漏洞，这些进展令人对其安全性和可信度产生质疑。操纵、欺骗或洗白模型推理证据的可能性在高风险决策和知识发现中产生不利后果。本文总结了50多篇论文的研究，概述了针对机器学习模型解释的对抗攻击以及公平度量的研究。我们讨论了如何防御攻击并设计鲁棒的解释方法。我们列出XAI中现有的不安全因素，并概述了对抗性XAI（AdvXAI）的新兴研究方向。

    Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
    
[^163]: ChatGPT在软件工程中的应用范围：全面探究

    The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])

    [http://arxiv.org/abs/2305.12138](http://arxiv.org/abs/2305.12138)

    ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。

    

    ChatGPT展示了在软件工程中转化的巨大潜力，表现出在代码和文档生成等任务中的卓越性能。然而，软件工程需要高可靠性和风险控制，使ChatGPT的缺乏可解释性成为一个问题。为了解决这个问题，我们进行了一项研究，评估了ChatGPT在软件工程中的能力和局限性。我们将AI模型应对SE任务所需的能力分为三类：1）语法理解，2）静态行为理解，和3）动态行为理解。我们的调查重点是ChatGPT理解代码语法和语义结构的能力，包括抽象语法树（AST）、控制流程图（CFG）和调用图（CG）。我们评估了ChatGPT在涉及C、Java、Python和Solidity的跨语言任务中的表现。我们的发现表明，虽然ChatGPT表现出了对理解代码语法（AST）的出色能力，但在理解代码语义和部分功能上存在问题。

    ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
    
[^164]: IVP-VAE: 利用初值问题求解器对电子病历时间序列进行建模

    IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])

    [http://arxiv.org/abs/2305.06741](http://arxiv.org/abs/2305.06741)

    本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。

    

    连续时间模型（例如神经ODE和神经流量）在分析电子病历中常见的不规则采样时间序列方面显示出有希望的结果。 基于这些模型，时间序列通常在变分自动编码器架构中通过初值问题（IVP）求解器和递归神经网络的混合处理。 顺序求解IVP使得这样的模型在计算效率上不够高。 本文提出了一种纯粹使用连续过程对时间序列进行建模的方法，其状态演变可以通过IVP直接近似。 这消除了递归计算的需要，并允许多个状态并行演变。 我们进一步通过一种基于其可逆性的IVP求解器融合编码器和解码器，这导致参数更少，收敛更快。 在三个真实世界的数据集上进行的实验表明，所提出的方法在获得更快的训练速度的同时，仍然可以获得较高的分类性能和预测性能。

    Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
    
[^165]: 在联邦深度学习中优化批标准化

    Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])

    [http://arxiv.org/abs/2303.06530](http://arxiv.org/abs/2303.06530)

    本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。

    This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.

    批标准化（BN）通常用于现代深度神经网络（DNN）中，以提高稳定性并加速集中式训练的收敛速度。在具有非IID分散数据的联邦学习（FL）中，先前的研究观察到使用BN进行训练可能会由于训练和测试之间的BN统计不匹配而阻碍性能。因此，群组归一化（GN）更常用于FL作为BN的替代方法。然而，通过我们在各种FL设置下的实证研究，我们发现BN和GN之间没有一致的优胜者。这促使我们重新审视FL中归一化层的使用。我们发现，在适当的处理下，BN可以在广泛的FL设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。我们希望我们的研究可以成为FL未来实际使用和理论分析的有价值参考。

    Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
    
[^166]: 大型语言模型可被视为隐含的主题模型：解释和寻找好的示范以实现上下文学习

    Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11916](http://arxiv.org/abs/2301.11916)

    本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。

    

    近年来，预训练的大型语言模型表现出了在推理时实现少量样本学习能力的显著效率，被称为上下文学习。 然而，现有文献强调这种能力对少量样本示范的选择很敏感。本研究旨在通过贝叶斯视角研究上下文学习现象，将大型语言模型视为从示范中隐含地推断出相关信息的主题模型。在此前提下，我们提出了一种算法，用于从一组注释数据中选择最佳示范，并证明相对于随机选择基线的平均值，在八个不同的真实文本分类数据集上平均每个 GPT2 和 GPT3 模型有显着的 12.5% 的提升。我们的实证发现支持我们的假设，即大型语言模型可被视为隐含的主题模型。

    In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
    

