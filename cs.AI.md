# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Chain-of-Verification Reduces Hallucination in Large Language Models.](http://arxiv.org/abs/2309.11495) | 该论文提出了一种链式验证方法（CoVe），通过在回答之前进行备查问题来减少大型语言模型中的幻觉。实验证明CoVe方法在各种任务中都能有效降低幻觉的发生。 |
| [^2] | [Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning.](http://arxiv.org/abs/2309.11489) | Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。 |
| [^3] | [Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs.](http://arxiv.org/abs/2309.11478) | 本研究通过结合故事和Large Language Models (LLMs)，将虚构角色转化为"活体"社交实体，开发了引人入胜且可信的社区故事聊天机器人。在社交互动中，这些机器人可以通过回忆挑战并寻求建议与用户进行交流。 |
| [^4] | [Multi-view Fuzzy Representation Learning with Rules based Model.](http://arxiv.org/abs/2309.11473) | 本文提出了一种基于可解释的Takagi-Sugeno-Kang（TSK）模糊系统的新的多视图模糊表示学习方法（MVRL_FS）。该方法可以全面地探索多视图数据，同时保留视图之间的通用信息，并具有较高的解释性。 |
| [^5] | [Multi-Label Takagi-Sugeno-Kang Fuzzy System.](http://arxiv.org/abs/2309.11469) | 提出了一种新的多标签分类方法ML-TSK FS，通过模糊规则建模特征和标签之间的关系，并结合模糊推理和多标签回归损失进行训练。实验结果表明，ML-TSK FS在多种评估指标上与现有方法具有竞争力，能够有效地建模特征-标签关系并提高分类性能。 |
| [^6] | [AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition.](http://arxiv.org/abs/2309.11462) | AudioFool是一种高速、通用和无需同步的跨领域语音识别攻击，能够通过反向傅里叶变换构造对同步的不变性和对滤波的鲁棒性，实现对ASR系统的拒绝服务攻击。 |
| [^7] | [Using deep learning to construct stochastic local search SAT solvers with performance bounds.](http://arxiv.org/abs/2309.11452) | 本论文利用深度学习方法构建了一种随机局部搜索SAT求解器，并且使用图神经网络训练了求解SAT问题的“神谕”。实验结果表明，访问基于GNN的神谕显著提高了求解器的性能。 |
| [^8] | [You Only Look at Screens: Multimodal Chain-of-Action Agents.](http://arxiv.org/abs/2309.11436) | 本论文提出了一种名为Auto-UI的多模态动作链机器人，通过直接与界面交互，避免了环境解析或依赖于应用程序API的需要，并引入了动作链技术来帮助模型进行决策。 |
| [^9] | [A Systematic Review of Few-Shot Learning in Medical Imaging.](http://arxiv.org/abs/2309.11433) | 本文系统综述了医学影像中的少样本学习技术，发现少样本学习可以缓解数据稀缺问题并提升医学影像分析性能，特别是在元学习方面。 |
| [^10] | [Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing.](http://arxiv.org/abs/2309.11427) | 本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。 |
| [^11] | [EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning.](http://arxiv.org/abs/2309.11414) | EDMP是一种结合了传统和基于深度学习运动规划优势的基于成本引导的扩散运动规划方法，通过对多样化的运动学有效轨迹进行训练，来提高解决方案的成功率。 |
| [^12] | [Long-Form End-to-End Speech Translation via Latent Alignment Segmentation.](http://arxiv.org/abs/2309.11384) | 本文提出了一种潜在对齐分割方法，通过将语音翻译与分割放在同一个模型中实现了低延迟的端到端语音翻译，该方法是首次实现了这种方式。 |
| [^13] | [Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions.](http://arxiv.org/abs/2309.11382) | 本研究提出了一种新颖的零射击视觉语言导航框架，其中通过与大型模型进行讨论来收集必要的信息，以提升导航性能。 |
| [^14] | [Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff.](http://arxiv.org/abs/2309.11379) | 本研究提出了一种增量分块束搜索方法，用于实现同时语音翻译。该方法可以控制质量与延迟的权衡，通过引入本地一致性或保持-n策略来实现。在实验中，我们发现该方法在不改变延迟的情况下可以提高0.6-3.6 BLEU的翻译质量，或者在不改变质量的情况下可以提高0.8-1.4秒的延迟。 |
| [^15] | [Preconditioned Federated Learning.](http://arxiv.org/abs/2309.11378) | 本文提出了基于两个自适应框架的新的通信高效联邦学习算法：PreFed和PreFedOp，通过使用新颖的协方差矩阵预处理器来实现适应性。实验证明这些方法在i.i.d.和非i.i.d.设置下均取得了最先进的性能。 |
| [^16] | [Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition.](http://arxiv.org/abs/2309.11368) | 本文介绍了一种使用语音识别的动态手势特征的人体运动适应方法，通过无缝集成多种模块来实现用户友好的工具传递，使用户无需额外培训以使用复杂的人机接口。 |
| [^17] | [Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG).](http://arxiv.org/abs/2309.11361) | 本论文为材料科学领域构建了一个知识图问答基准数据集（KGQA4MAT），重点关注金属有机框架（MOF）。通过开发自然语言接口和利用ChatGPT将自然语言问题转化为形式化的KG查询，该论文展示了ChatGPT在解决KGQA问题的潜力。 |
| [^18] | [3D Face Reconstruction: the Road to Forensics.](http://arxiv.org/abs/2309.11357) | 本综述研究了3D人脸重建算法的应用，从整形手术到娱乐行业，但在法医学领域的应用仍然有一些限制和障碍。对于将3D人脸重建作为法医学证据的有效工具，我们仍需进行进一步的研究和探索。 |
| [^19] | [A Comprehensive Survey on Rare Event Prediction.](http://arxiv.org/abs/2309.11356) | 本文综合调研了罕见事件预测领域的当前方法，通过考虑罕见事件数据、数据处理、算法方法和评估方法四个维度，总结出了在机器学习流程中解决罕见事件预测问题的关键方法。 |
| [^20] | [C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters.](http://arxiv.org/abs/2309.11351) | C⋅ASE是一个学习基于物理的角色的条件对抗技能嵌入的框架，通过将异构的技能动作划分为不同子集，以实现多样性和可控性。训练过程中采用了焦点技能采样、骨骼残余力和逐元素特征屏蔽，以平衡不同复杂性的技能，并捕捉更一般的行为特征。 |
| [^21] | [TRAVID: An End-to-End Video Translation Framework.](http://arxiv.org/abs/2309.11338) | TRAVID是一个端到端的视频翻译框架，不仅可以翻译口语，还可以将翻译的语音与说话者的嘴唇动作同步，为学生和用户提供了一种增强的视频翻译体验。 |
| [^22] | [Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism.](http://arxiv.org/abs/2309.11331) | 本研究提出了Gold-YOLO模型，通过先进的收集和分发机制（GD）机制以及MAE风格的预训练，解决了YOLO系列模型中的信息融合问题，实现了高效的目标检测和多尺度特征融合。 |
| [^23] | [Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory.](http://arxiv.org/abs/2309.11316) | 本研究使用博弈论设计了一个动态定价策略的模型，在云市场中实现了有效的定价策略，增强了市场竞争力。 |
| [^24] | [A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques.](http://arxiv.org/abs/2309.11312) | 本研究提出了一种基于遗憾最小化算法的云市场定价策略，通过模拟供应商之间的竞争环境并迭代更新策略概率，实现了供应商利润的显著增加。 |
| [^25] | [Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features.](http://arxiv.org/abs/2309.11307) | 本文提出了一种预测对话任务助手的评分的方法，通过将对话流特征和用户行为特征结合起来，并使用真实人-机对话和评分数据进行模型训练和分析。结果表明，将对话流和行为方面的特征考虑在内的模型在离线评分预测中具有优势，并且对CTA特定行为特征的分析可以为未来的系统提供启发。 |
| [^26] | [FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion.](http://arxiv.org/abs/2309.11306) | FaceDiffuser是一种使用扩散技术的基于语音驱动的3D面部动画合成方法，它采用了非确定性的深度学习模型，并使用了基于3D顶点和混合形状的数据集进行训练。 |
| [^27] | [A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing.](http://arxiv.org/abs/2309.11299) | 本论文提出了一种成本感知的优化资源配置机制，通过学习自动机实现对请求应用程序的高效资源配送，为实现成本效益的服务配置提供了新的方法。 |
| [^28] | [CPLLM: Clinical Prediction with Large Language Models.](http://arxiv.org/abs/2309.11295) | CPLLM是一种使用大规模语言模型进行临床疾病预测的方法。通过量化和提示来微调语言模型，利用患者的历史诊断记录来预测目标疾病的诊断结果。实验证明，CPLLM在各项指标上均超越了其他基线模型，显示出显著的改进。 |
| [^29] | [Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains.](http://arxiv.org/abs/2309.11285) | AuTexTification是IberLEF 2023研讨会的共享任务，旨在检测和归属多领域机器生成的文本。数据集包含160,000多条文本，涵盖了英语和西班牙语以及推文、评论、新闻、法律和操作指南等五个领域。共有114个团队参与，提交了175次运行结果。 |
| [^30] | [Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting.](http://arxiv.org/abs/2309.11284) | 本文重新思考了传感器的依赖建模问题，从区域和全局两个层次出发，通过合并区域节点和生成全局节点来反映传感器之间的依赖关系，并且应用元图卷积网络来提高节点表示的普遍性和真实性。 |
| [^31] | [Open-endedness induced through a predator-prey scenario using modular robots.](http://arxiv.org/abs/2309.11275) | 本研究通过使用具有固定形态的模块化机器人，在捕食-被捕食者场景中诱发了开放式进化的出现。通过引入标记系统，增加了机器人的行为复杂性，并展示了适应性策略的出现。这一研究证明了利用模块化机器人进行捕食-被捕食者动态时诱导开放式进化的可行性。 |
| [^32] | [Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework.](http://arxiv.org/abs/2309.11274) | 本文提出了基于故障注入的输入数据测试框架，用于测试机器学习模型对多个故意引发的数据故障的弹性。该框架使用数据突变器探索ML系统对不同故障注入效果的脆弱性，并在选定的ML模型之前进行优化。 |
| [^33] | [Grounded Complex Task Segmentation for Conversational Assistants.](http://arxiv.org/abs/2309.11271) | 本文通过将结构化指令转化为会话式指令，针对会话助手中的复杂任务分割问题进行了研究。实验结果表明基于标记的Transformer模型在计算会话步骤特征方面效果最好，并且用户研究证明提出的方法可以改善原始指令的效果。 |
| [^34] | [Sequence-to-Sequence Spanish Pre-trained Language Models.](http://arxiv.org/abs/2309.11259) | 该论文介绍了一种新的序列到序列的西班牙预训练语言模型，该模型在各种序列到序列任务中表现出了竞争性能，并提供了BART、T5和BERT2BERT-style模型的西班牙版本。 |
| [^35] | [Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering.](http://arxiv.org/abs/2309.11247) | 提出了一个针对空中战斗机动的分层多智能体强化学习框架，通过将决策过程分为两个层次的抽象来处理高维状态和动作空间的挑战，通过训练低层策略实现准确的单位战斗控制。 |
| [^36] | [Colour Passing Revisited: Lifted Model Construction with Commutative Factors.](http://arxiv.org/abs/2309.11236) | 提出一种改进的色彩传递算法，利用逻辑变量构建了一个与特定推理算法无关的抬升表示，同时利用因子的交换性，大大提升了概率推理查询速度。 |
| [^37] | [ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish.](http://arxiv.org/abs/2309.11231) | 本研究评估了ChatGPT-4作为西班牙学术图书编辑工具的潜力，结果显示ChatGPT-4在语法和拼写纠正方面具有高精度和快速性能，但在上下文敏感性和文献计量等方面仍存在挑战。 |
| [^38] | [Leveraging Diversity in Online Interactions.](http://arxiv.org/abs/2309.11224) | 本文研究了在在线互动中如何利用多样性，通过利用声明性规范来连接人们以帮助他们解决日常问题。试点结果表明，所选择的个人资料的多样性取得了相对成功，并得到了用户的高度满意。 |
| [^39] | [Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering.](http://arxiv.org/abs/2309.11206) | 提出了一种提高知识图谱问答任务性能的增强型LLMs框架，通过转化KG知识为文本化陈述的方式，实现了对答案敏感的KG-to-Text方法。 |
| [^40] | [Using Artificial Intelligence for the Automation of Knitting Patterns.](http://arxiv.org/abs/2309.11202) | 本研究提出了一种利用数据增强和迁移学习技术的深度学习模型，用于识别和分类编织图案。模型采用Inception ResNet-V2作为特征提取和分类算法，在准确率、对数损失、F1分数、精确度和召回率等指标上进行评估。 |
| [^41] | [When to Trust AI: Advances and Challenges for Certification of Neural Networks.](http://arxiv.org/abs/2309.11196) | 本文主要关注人工智能（AI）的认证和可解释性，综述了已经开发的用于确保AI决策安全的技术，并探讨了未来的挑战。 |
| [^42] | [Long-tail Augmented Graph Contrastive Learning for Recommendation.](http://arxiv.org/abs/2309.11177) | 提出了一种新颖的长尾增强图对比学习（LAGCL）方法用于推荐系统，通过引入可学习的长尾增强方法和生成对比视图来解决头尾节点之间的显著度差异问题，并采用度均衡机制来平衡数据增强程度。 |
| [^43] | [Are Large Language Models Really Robust to Word-Level Perturbations?.](http://arxiv.org/abs/2309.11166) | 该论文提出了一种用于评估大型语言模型（LLMs）鲁棒性的新颖方法，使用预训练的奖励模型作为诊断工具。实验证明这种方法在评估LLM鲁棒性方面表现准确。 |
| [^44] | [ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement.](http://arxiv.org/abs/2309.11155) | 本文提出了一个用于探索和优化基于原型的深度伪造检测模型的可视化分析系统ProtoExplorer，为深度学习模型的可解释性提供了一种新方法。 |
| [^45] | [CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought.](http://arxiv.org/abs/2309.11143) | CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。 |
| [^46] | [Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation.](http://arxiv.org/abs/2309.11127) | 本文提出了一种基于语义编码和知识蒸馏的面向语言的通信框架，通过将大型语言模型和生成模型集成到语义通信范式中，实现了使用人类语言消息进行通信的效率。同时引入了语义源编码、语义信道编码和语义知识蒸馏等创新算法来提高通信的效果。 |
| [^47] | [AttentionMix: Data augmentation method that relies on BERT attention mechanism.](http://arxiv.org/abs/2309.11104) | AttentionMix是一种新的数据增强方法，利用了基于注意力的信息。在三个标准情感分类数据集上的评估结果表明，AttentionMix在NLP领域的数据增强中表现优于使用Mixup机制的两种基准方法和普通BERT方法。 |
| [^48] | [A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making.](http://arxiv.org/abs/2309.11101) | 本研究介绍了一个新的神经网络模型，称为TT-rules，在医疗决策中具有可解释性，并通过将神经网络转换为基于规则的模型实现了高性能。该模型支持二分类、多标签分类和回归任务，且在医疗应用中表现出色。 |
| [^49] | [Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling.](http://arxiv.org/abs/2309.11089) | 本论文提出了一种实用的概率模型基于深度强化学习方法，通过结合dropout不确定性和轨迹采样，稳定地预测系统不确定性，纠正神经网络的拟合误差，过滤aleatoric不确定性，从而实现了优越的控制能力。在实验评估中，该方法在多个控制任务和实际机械臂操作任务中表现出优于其他方法和模型的性能。 |
| [^50] | [Weak Supervision for Label Efficient Visual Bug Detection.](http://arxiv.org/abs/2309.11077) | 我们提出了一种利用未标注数据和领域特定增强技术的弱监督方法，在视觉错误检测中实现了数据集扩大和自主/交互弱监督，展示了在广阔的游戏世界中的有效性。 |
| [^51] | [Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection.](http://arxiv.org/abs/2309.11069) | 动态平铺是一种模型无关、自适应、可扩展且推理数据中心化的方法，通过使用动态重叠率和平铺缩小器，解决了碎片化的小物体检测问题，并提高了检测准确性和计算效率。 |
| [^52] | [Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness.](http://arxiv.org/abs/2309.11064) | 本文通过初步探索性调查研究了LLM的幻觉现象与提示语言因素之间的关系，发现更形式化和具体的提示有助于减少幻觉的发生，但可读性相关的结果则不确定。 |
| [^53] | [Design of Chain-of-Thought in Math Problem Solving.](http://arxiv.org/abs/2309.11054) | 本论文研究了数学问题解决中思路链的设计方法，对比了自然语言思路链和程序思路链的效果，并发现程序思路链通常在数学问题解决中更加有效，特别是自我描述程序具有更大多样性且性能更高。此外，研究还发现Python是程序思路链的较好选择。实验结果为未来思路链设计提供了宝贵指导。 |
| [^54] | [Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion.](http://arxiv.org/abs/2309.11044) | 提出了一种基于贝叶斯信息准则的中间全局模型Clustered FedStack。通过将本地客户端的模型预测和输出层权重发送到服务器，构建一个强大的全局模型，并使用聚类机制对本地客户进行聚类。 |
| [^55] | [Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters.](http://arxiv.org/abs/2309.11042) | 本论文提出了一种使用任务适配器的混合多任务学习方法，通过在小型语言模型上构建混合任务适配器，同时处理多个NLP任务，并通过两阶段训练方法优化适配器之间的协作，从而实现在较小的计算成本下支持多个领域特定应用。 |
| [^56] | [Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems.](http://arxiv.org/abs/2309.11039) | 联邦学习在智能交通系统中提供了一种具有隐私保护和易扩展性的新范式，为解决动态车辆环境中的各种应用挑战提供了解决方案。 |
| [^57] | [ModelGiF: Gradient Fields for Model Functional Distance.](http://arxiv.org/abs/2309.11013) | 本文介绍了一种用于测量模型功能距离的方法，称为ModelGiF。通过在异构的预训练模型中提取同质的表示，ModelGiF可通过模型之间的相似性来衡量它们之间的距离。实验证实了该方法在任务相关性估计、知识产权保护和模型遗忘验证等方面的有效性。 |
| [^58] | [Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World.](http://arxiv.org/abs/2309.10987) | 本文介绍了SpikingNeRF，它通过将辐射光线与脉冲神经网络的时间维度对齐，以节省能量并减少计算量。 |
| [^59] | [Is GPT4 a Good Trader?.](http://arxiv.org/abs/2309.10982) | 本研究旨在考察GPT-4在经典交易理论理解和实际交易数据分析中的准确性和能力，以验证其作为交易员的可靠性。 |
| [^60] | [AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2309.10980) | 本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。 |
| [^61] | [LMDX: Language Model-based Document Information Extraction and Localization.](http://arxiv.org/abs/2309.10952) | LMDX是一种基于语言模型的文档信息提取与定位方法，克服了布局编码和答案虚构的困难，能够在半结构化文档中提取关键实体。 |
| [^62] | [Benchmarks for Pir\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change.](http://arxiv.org/abs/2309.10945) | 通过创建基准测试，我们为Pir\'a 2.0数据集定义了六个不同的问答任务的测试，可以更好地利用该数据集来评估当前机器学习模型在阅读理解方面的能力。 |
| [^63] | [End-to-End Speech Recognition Contextualization with Large Language Models.](http://arxiv.org/abs/2309.10917) | 本论文介绍了一种在语音识别中使用大型语言模型进行上下文化的新方法。通过整合预训练的语言模型，我们的方法在训练过程中通过提供音频特征和文本上下文，使系统隐含地学习如何利用上下文信息，并达到了显著的性能改善。 |
| [^64] | [Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning.](http://arxiv.org/abs/2309.10910) | 该研究通过跨数据集迁移学习来放大脑电信号通路中的病理检测，以提高病理诊断的准确性和可行性，解决了标注数据稀缺性和低成本招募真实患者队列的问题。 |
| [^65] | [Multicopy Reinforcement Learning Agents.](http://arxiv.org/abs/2309.10908) | 本文研究了一种新型的多智能体问题，在这个问题中，智能体制作多个相同副本来更好地完成任务。通过利用价值函数的结构，提出了一种学习算法来平衡添加额外副本的优势和成本。 |
| [^66] | [Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education.](http://arxiv.org/abs/2309.10892) | 该论文介绍了一个名为人工智能助力的智能助手（AIIA）的框架，用于高等教育中的个性化和自适应学习。该系统利用先进的AI和自然语言处理技术，提供交互和引人入胜的学习平台，通过简化信息获取、促进知识评估和个性化学习支持来降低学习者的认知负荷。其功能包括理解和回答学生问题、生成测验和闪卡，并提供个性化学习路径。该研究结果可对高等教育中人工智能虚拟助教（VTA）的设计、实施和评估产生重要影响，从而提高学生的学习成果、参与度和满意度。 |
| [^67] | [Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer.](http://arxiv.org/abs/2309.10891) | 本文提出了一种名为SALT的方法，通过结合代码切换和嵌入混合与自我增强，有效地提高了多语言预训练模型的零样本跨语言传递能力。 |
| [^68] | [Classifying Organizations for Food System Ontologies using Natural Language Processing.](http://arxiv.org/abs/2309.10880) | 本研究使用自然语言处理方法自动对食物系统本体中的组织进行分类，研究结果表明NLP模型可以在这两个分类任务中取得良好性能。 |
| [^69] | [Believable Minecraft Settlements by Means of Decentralised Iterative Planning.](http://arxiv.org/abs/2309.10871) | 通过分散的、迭代的规划过程，我们的方法在Minecraft定居点生成竞赛中获胜，该方法可用于产生逼真且可信赖的程序化城市内容。 |
| [^70] | [Using AI Uncertainty Quantification to Improve Human Decision-Making.](http://arxiv.org/abs/2309.10852) | 本论文研究了使用AI不确定性量化改进人类决策的方法，并通过基于实例的UQ和行为实验验证了其性能优势。 |
| [^71] | [AI Foundation Models for Weather and Climate: Applications, Design, and Implementation.](http://arxiv.org/abs/2309.10808) | 该论文介绍了AI基础模型在天气和气候中的应用，主要讨论了使用transformers、物理知识启发的机器学习和图神经网络的最新方法。尽管取得了进展，但全球地球系统模型的可推广AI模型仍处于初级阶段。 |
| [^72] | [MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation.](http://arxiv.org/abs/2309.10738) | 提出了一种名为MelodyGLM的多任务预训练框架，用于生成具有长期结构的旋律。该框架通过设计音乐n-gram和长跨度抽样策略来捕捉旋律的局部和全局结构，并使用大规模符号旋律数据集进行预训练改进。 |
| [^73] | [NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages.](http://arxiv.org/abs/2309.10661) | NusaWrites项目通过母语者段落撰写构建高质量语料库，弥补了在线爬取和翻译文档所带来的词汇多样性和文化相关性的限制。该研究在印度尼西亚地方语言上进行，并提出了“datasetname”基准，涵盖了12种被低估和极度资源匮乏的语言。 |
| [^74] | [PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring.](http://arxiv.org/abs/2309.10576) | 本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。 |
| [^75] | [Toward Unified Controllable Text Generation via Regular Expression Instruction.](http://arxiv.org/abs/2309.10447) | 本文通过引入正则表达式指令（REI）实现了统一可控文本生成，通过指令方式支持各种约束，无需对架构进行修改，并对各种约束组合表现出良好的性能。 |
| [^76] | [QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation.](http://arxiv.org/abs/2309.10326) | QASnowball是一个迭代自举框架，可以根据有监督的样本种子集生成大规模高质量的QA数据，并通过重新种子化进行自我增强。在高资源英文场景中进行了实验。 |
| [^77] | [QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems.](http://arxiv.org/abs/2309.10293) | 该论文提出了一种QXAI框架，用于患者监测系统中的定量分析。该框架具有可解释性，对于决策者在医疗应用中基于非线性模型结果进行决策非常重要。 |
| [^78] | [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions.](http://arxiv.org/abs/2309.10150) | Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。 |
| [^79] | [Context $\approx$ Environment.](http://arxiv.org/abs/2309.09888) | 在这篇论文中，作者通过理论与实验证明了将注意力放在上下文-未标记样本上，可以实现更好的领域泛化。 |
| [^80] | [A Quantum Optimization Case Study for a Transport Robot Scheduling Problem.](http://arxiv.org/abs/2309.09736) | 本研究通过比较D-Waves的量子-经典混合框架、富士通的量子启发式数字退火器和Gurobi的经典求解器的性能，提供了解决运输机器人调度问题的指导，发现数字退火器有希望的结果，并为混合量子退火器提供了一些机会。 |
| [^81] | [Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis.](http://arxiv.org/abs/2309.09553) | 提出了一种称为因果故事的新模型，利用局部因果注意力机制来改进视觉故事合成的全局一致性，该模型考虑了历史标题、帧和当前标题之间的因果关系，实现了更好的生成效果。 |
| [^82] | [Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level.](http://arxiv.org/abs/2309.08182) | 本研究证明，使用大型语言模型(如GPT3.5)可以解决和解释物理词问题，通过对物理知识进行计算和推理，实现了接近人类水平的解决率。此外，该模型还能够总结涉及的知识、生成解释，并创造新的物理词问题。 |
| [^83] | [VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning.](http://arxiv.org/abs/2309.07832) | VAPOR是一种使用离线强化学习的方法，用于在室外复杂植被环境中进行全向腿式机器人的自主导航。通过从未标记的真实植被数据中训练，该方法学习了植被的物理和几何特性，并使用一个自适应规划器生成动态可行机器人动作，在狭窄通道中导航，并避免被高草和灌木丛所困住。在实验中，我们观察到成功率有所提高。 |
| [^84] | [REFORMS: Reporting Standards for Machine Learning Based Science.](http://arxiv.org/abs/2308.07832) | REFORMS是一个基于机器学习的科学报告标准，旨在解决使用机器学习方法在科学研究中出现的有效性、可重复性和可推广性失败问题。这个标准由32个问题和一套指导方针组成，可作为研究人员设计和实施科研时的参考资源。 |
| [^85] | [Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models.](http://arxiv.org/abs/2308.06534) | 本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。 |
| [^86] | [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus.](http://arxiv.org/abs/2307.11760) | EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。 |
| [^87] | [Principles and Guidelines for Evaluating Social Robot Navigation Algorithms.](http://arxiv.org/abs/2306.16740) | 本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。 |
| [^88] | [Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System.](http://arxiv.org/abs/2306.05809) | 本文旨在采用以用户为中心的交互式解释模型，在推荐系统中为用户提供不同细节级别的解释，赋予用户个性化解释的能力。 |
| [^89] | [Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal.](http://arxiv.org/abs/2306.04502) | 本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。 |
| [^90] | [Adaptive PD Control using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays.](http://arxiv.org/abs/2305.16979) | 本论文引入了一种使用深度强化学习来解决局部-远程遥操作中的时延问题的自适应PD控制方法，通过实时调整控制器参数，改善同步性和稳定性，并克服了随机延迟带来的挑战。 |
| [^91] | [GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts.](http://arxiv.org/abs/2305.12477) | 本文通过在多个任务和数据集上对GPT-3.5、GPT-4和BARD进行评估，实验证明在零样本情境中，ChatGPT-4表现出了更高的性能。GPT-4相对于GPT-3.5的优势可能是由其更大的模型规模和NLP效率所引起的，但对于BARD来说并不明显。此外，这三个模型在归纳、数学和多跳推理任务上的表现有限。 |
| [^92] | [Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing.](http://arxiv.org/abs/2305.08415) | Marsellus是一款具有2至8位DNN加速和30%自适应体偏化的异构RISC-V AI-IoT末端SoC，适用于计算密集型的深度神经网络推理以及高精度浮点运算的信号处理和控制。 |
| [^93] | [ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion.](http://arxiv.org/abs/2305.06395) | 本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。 |
| [^94] | [Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial.](http://arxiv.org/abs/2305.04933) | 本文教程介绍机器学习中的不确定性量化方法，帮助提升其安全性和可靠性，进而促进其在高风险决策背景下的广泛应用，特别关注神经网络及其在工程设计和预测健康管理中的应用。 |
| [^95] | [The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation.](http://arxiv.org/abs/2305.04718) | 该论文介绍了一种用于机器人操作中的深度策略学习的贝叶斯场景关键点跟踪方法。该方法通过解决图像中的歧义问题，实现了对于对称物体和遮挡和视野之外物体的关键点跟踪，提高了相机观察的效用，对于策略学习具有优势。 |
| [^96] | [A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction.](http://arxiv.org/abs/2304.13032) | 提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。 |
| [^97] | [Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection.](http://arxiv.org/abs/2304.04688) | 提出了一种采用预训练的视觉-语言模型和交互模块进行交互感知提示的零样本时空动作检测方法，优化了视觉-语言特征的对齐，实现了更好的结果。 |
| [^98] | [Autonomous Robotic Drilling System for Mice Cranial Window Creation: An Evaluation with an Egg Model.](http://arxiv.org/abs/2303.12265) | 本文提出了一种针对小鼠颅骨不均匀问题的自主式机器人操作系统。 |
| [^99] | [Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews.](http://arxiv.org/abs/2302.08062) | 该论文提出了使用深度学习集成的数据增强多视角方法识别化石图像。通过收集化石图像的不同视角，使用多个基础模型进行训练，并通过软投票进行最终决策。实验证明，该方法在性能上优于基线方法，并且与使用三个原始视图模型的方法相当。 |
| [^100] | [A Robust Multilabel Method Integrating Rule-based Transparent Model, Soft Label Correlation Learning and Label Noise Resistance.](http://arxiv.org/abs/2301.03283) | 本论文提出了一种融合基于规则的透明模型、软标签相关性学习和标签噪声抗性的鲁棒多标记方法（R-MLTSK-FS）。实验证明了该方法在多标记学习中的优越性能。 |
| [^101] | [RouteNet-Fermi: Network Modeling with Graph Neural Networks.](http://arxiv.org/abs/2212.12070) | RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。 |
| [^102] | [DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision.](http://arxiv.org/abs/2210.16906) | DyG2Vec是一个具有自监督学习能力的动态图表征学习模型，采用了高效而有效的注意力编码器和非对比自监督学习方法，能够提取丰富的时间嵌入表示。在基准数据集上的实验结果表明，该模型在未来链接预测任务中表现出色。 |
| [^103] | [Graph Fuzzy System: Concepts, Models and Algorithms.](http://arxiv.org/abs/2210.16730) | 本文提出了一种适用于图数据建模的新型模糊系统，名为图模糊系统（GFS），通过定义相关概念、构建模型框架和算法，实现了在处理具有非欧几里得结构的图数据时保留模糊系统优势的目标。 |
| [^104] | [Conformal Prediction is Robust to Dispersive Label Noise.](http://arxiv.org/abs/2209.14295) | 本研究研究了Conformal Prediction方法对于标签噪声具有鲁棒性。我们找出了构建可以正确覆盖无噪声真实标签的不确定性集合的条件，并提出了对具有噪声标签的一般损失函数进行正确控制的要求。实验证明，在对抗性案例之外，使用Conformal Prediction和风险控制技术可以实现对干净真实标签的保守风险。我们还提出了一种有界尺寸噪声修正的方法，以确保实现正确的真实标签风险。 |
| [^105] | [Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec.](http://arxiv.org/abs/2208.03680) | 通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。 |
| [^106] | [Toward Dynamic Stability Assessment of Power Grid Topologies using Graph Neural Networks.](http://arxiv.org/abs/2206.06369) | 本研究通过使用图神经网络（GNN）分析电网的动态稳定性，生成了新的大型数据集，并证明了GNN在仅利用拓扑信息的情况下能够有效预测非线性目标，同时能够准确识别电网中的脆弱节点。 |
| [^107] | [Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts.](http://arxiv.org/abs/2205.04952) | 本研究描述了一种选择机器人语音风格的过程和结果，以达到社交适应性和环境感知。通过在虚拟环境中收集和验证语音数据交互，并使用投影、灯光和声音在重新创造的环境中测试机器人的语音风格，我们探索和聚类人类的语音话语以识别主要的语音风格，并以餐饮服务场景作为概念验证环境。结果表明，这种研究可以改善机器人在特定语境下的社交适应性和智能感知。 |
| [^108] | [Attacking Open-domain Question Answering by Injecting Misinformation.](http://arxiv.org/abs/2110.07803) | 本研究探究了将错误信息注入问答系统的攻击，并发现问答模型对于错误信息具有脆弱性，即使少量错误信息的污染也会导致性能大幅下降。 |
| [^109] | [Procedures as Programs: Hierarchical Control of Situated Agents through Natural Language.](http://arxiv.org/abs/2109.08214) | 本文提出了一种以程序为过程的形式化方法，用于代理指令和控制。通过建立层级模块化网络，将自然语言意图转化为可执行程序的预测，并在两个数据集上取得了显著性能优势。 |
| [^110] | [Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration.](http://arxiv.org/abs/2108.06038) | Co-GAIL是一种学习人机多样化协作策略的方法，通过共同优化人类策略和机器人策略，实现从人-人协作示范中学习，并能在在线任务执行过程中应对人类策略调整的鲁棒性。 |

# 详细

[^1]: 链式验证减少大型语言模型中的幻觉

    Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v1 [cs.CL])

    [http://arxiv.org/abs/2309.11495](http://arxiv.org/abs/2309.11495)

    该论文提出了一种链式验证方法（CoVe），通过在回答之前进行备查问题来减少大型语言模型中的幻觉。实验证明CoVe方法在各种任务中都能有效降低幻觉的发生。

    

    大型语言模型中存在生成合理但不正确的事实信息（即幻觉）的问题，我们研究了语言模型在给出回复时进行思考以纠正错误的能力。我们开发了一种链式验证（CoVe）方法，模型首先（i）起草初始回复；然后（ii）计划验证问题来事实检查草稿；（iii）独立回答这些问题，以避免答案受其他回复的影响；最后（iv）生成最终的经过验证的回答。在实验中，我们展示了CoVe在各种任务中降低了幻觉的情况，包括来自维基数据的列表问题、封闭书籍MultiSpanQA和长文本生成。

    Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
    
[^2]: Text2Reward：针对强化学习的自动生成密集奖励函数的自动化框架

    Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])

    [http://arxiv.org/abs/2309.11489](http://arxiv.org/abs/2309.11489)

    Text2Reward是一个无需数据的自动化框架，可以根据大型语言模型自动生成可解释、自由形式的密集奖励函数，广泛适用于各种任务，并允许人类反馈进行迭代改进。

    

    设计奖励函数是强化学习中长期以来的挑战；它需要专业知识或领域数据，导致开发成本高。为了解决这个问题，我们引入了Text2Reward，一个无需数据的框架，可基于大型语言模型（LLM）自动生成密集奖励函数。给定自然语言描述的目标，Text2Reward生成作为环境紧凑表示的可执行程序的密集奖励函数。与逆强化学习和最近使用LLM编写稀疏奖励代码的工作不同，Text2Reward生成可解释的、自由形式的密集奖励代码，可涵盖各种任务，利用现有软件包，并允许通过人类反馈进行迭代改进。我们在两个机器人操作基准（ManiSkill2，MetaWorld）和两个MuJoCo的运动环境上评估了Text2Reward。在17个操作任务中的13个任务中，使用生成的奖励代码训练的政策实现了类似或更好的性能。

    Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
    
[^3]: 虚构世界，现实连接：通过LLMs开发社区故事聊天机器人

    Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs. (arXiv:2309.11478v1 [cs.AI])

    [http://arxiv.org/abs/2309.11478](http://arxiv.org/abs/2309.11478)

    本研究通过结合故事和Large Language Models (LLMs)，将虚构角色转化为"活体"社交实体，开发了引人入胜且可信的社区故事聊天机器人。在社交互动中，这些机器人可以通过回忆挑战并寻求建议与用户进行交流。

    

    我们探讨了故事和Large Language Models (LLMs)的整合，以在社区环境中开发引人入胜且可信的社交聊天机器人（SCs）。受到虚构角色增强社交互动潜力的启发，我们引入了故事聊天机器人（SSCs）和故事工程的概念，将虚构游戏角色转化为玩家社区中的“活体”社交实体。我们的故事工程过程包括三个步骤：（1）角色和故事创造，定义SC的个性和世界观，（2）向社区展示活体故事，让SC描述挑战并寻求建议，（3）与社区成员进行交流，实现SC与用户之间的互动。我们使用LLM GPT-3驱动我们的SSC原型“David”和“Catherine”，并在Discord的在线游戏社区“DE（Alias）”上评估了它们的表现。我们进行了基于问卷（N=15）和访谈（N=8）的混合方法分析。

    We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into "live" social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes, "David" and "Catherine," and evaluated their performance in an online gaming community, "DE (Alias)," on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) wit
    
[^4]: 基于规则模型的多视图模糊表示学习

    Multi-view Fuzzy Representation Learning with Rules based Model. (arXiv:2309.11473v1 [cs.AI])

    [http://arxiv.org/abs/2309.11473](http://arxiv.org/abs/2309.11473)

    本文提出了一种基于可解释的Takagi-Sugeno-Kang（TSK）模糊系统的新的多视图模糊表示学习方法（MVRL_FS）。该方法可以全面地探索多视图数据，同时保留视图之间的通用信息，并具有较高的解释性。

    

    无监督多视图表示学习已经广泛应用于挖掘多视图数据。然而，仍然存在一些关键的挑战。一方面，现有方法不能全面地探索多视图数据，因为它们通常在视图之间学习一个公共的表示，而多视图数据既包含视图之间的公共信息，又包含每个视图内的特定信息。另一方面，为了挖掘数据之间的非线性关系，常常使用核方法或神经网络方法进行多视图表示学习。然而，这些方法在解释性方面存在不足。因此，本文提出了一种基于可解释的Takagi-Sugeno-Kang（TSK）模糊系统的新的多视图模糊表示学习方法（MVRL_FS）。该方法从两个方面实现了多视图表示学习。首先，将多视图数据转化为高维模糊特征空间，同时保留视图之间的通用信息；

    Unsupervised multi-view representation learning has been extensively studied for mining multi-view data. However, some critical challenges remain. On the one hand, the existing methods cannot explore multi-view data comprehensively since they usually learn a common representation between views, given that multi-view data contains both the common information between views and the specific information within each view. On the other hand, to mine the nonlinear relationship between data, kernel or neural network methods are commonly used for multi-view representation learning. However, these methods are lacking in interpretability. To this end, this paper proposes a new multi-view fuzzy representation learning method based on the interpretable Takagi-Sugeno-Kang (TSK) fuzzy system (MVRL_FS). The method realizes multi-view representation learning from two aspects. First, multi-view data are transformed into a high-dimensional fuzzy feature space, while the common information between views a
    
[^5]: 多标签Takagi-Sugeno-Kang模糊系统

    Multi-Label Takagi-Sugeno-Kang Fuzzy System. (arXiv:2309.11469v1 [cs.AI])

    [http://arxiv.org/abs/2309.11469](http://arxiv.org/abs/2309.11469)

    提出了一种新的多标签分类方法ML-TSK FS，通过模糊规则建模特征和标签之间的关系，并结合模糊推理和多标签回归损失进行训练。实验结果表明，ML-TSK FS在多种评估指标上与现有方法具有竞争力，能够有效地建模特征-标签关系并提高分类性能。

    

    多标签分类可以有效地从给定的标签集中识别实例的相关标签。然而，特征和标签之间的关系建模对分类性能至关重要。为此，我们提出了一种新的多标签分类方法，称为多标签Takagi-Sugeno-Kang模糊系统（ML-TSK FS），以提高分类性能。ML-TSK FS的结构使用模糊规则来建模特征和标签之间的关系。模糊系统通过将基于模糊推理的多标签相关学习与多标签回归损失相结合进行训练。在12个基准多标签数据集上进行了对提出的ML-TSK FS进行了实验评估。结果表明，在各种评估指标方面，ML-TSK FS的性能与现有方法具有竞争力，表明它能够使用模糊推理规则有效地建模特征-标签关系并增强分类性能。

    Multi-label classification can effectively identify the relevant labels of an instance from a given set of labels. However,the modeling of the relationship between the features and the labels is critical to the classification performance. To this end, we propose a new multi-label classification method, called Multi-Label Takagi-Sugeno-Kang Fuzzy System (ML-TSK FS), to improve the classification performance. The structure of ML-TSK FS is designed using fuzzy rules to model the relationship between features and labels. The fuzzy system is trained by integrating fuzzy inference based multi-label correlation learning with multi-label regression loss. The proposed ML-TSK FS is evaluated experimentally on 12 benchmark multi-label datasets. 1 The results show that the performance of ML-TSK FS is competitive with existing methods in terms of various evaluation metrics, indicating that it is able to model the feature-label relationship effectively using fuzzy inference rules and enhances the cl
    
[^6]: AudioFool: 高速、通用和无需同步的跨领域语音识别攻击

    AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition. (arXiv:2309.11462v1 [cs.CR])

    [http://arxiv.org/abs/2309.11462](http://arxiv.org/abs/2309.11462)

    AudioFool是一种高速、通用和无需同步的跨领域语音识别攻击，能够通过反向傅里叶变换构造对同步的不变性和对滤波的鲁棒性，实现对ASR系统的拒绝服务攻击。

    

    自动语音识别系统已经被证明容易受到对设备上命令的恶意攻击。最近的研究主要探讨如何创建这种攻击，然而一些与空中接口 (OTA) 攻击相关的问题尚未得到妥善解决。在我们的工作中，我们研究了与OTA模型兼容的强攻击所需的属性，并设计了一种可以生成具有任意所需属性的攻击的方法，即对同步的不变性和对滤波的鲁棒性：这允许对ASR系统进行拒绝服务（DoS）攻击。我们通过构建一个改进的频域攻击，并通过逆傅里叶变换实现了这些特性。我们在标准的关键词分类任务上评估了我们的方法，并在OTA环境中进行了分析，并分析了跨领域攻击的属性，以解释该方法的高效性。

    Automatic Speech Recognition systems have been shown to be vulnerable to adversarial attacks that manipulate the command executed on the device. Recent research has focused on exploring methods to create such attacks, however, some issues relating to Over-The-Air (OTA) attacks have not been properly addressed. In our work, we examine the needed properties of robust attacks compatible with the OTA model, and we design a method of generating attacks with arbitrary such desired properties, namely the invariance to synchronization, and the robustness to filtering: this allows a Denial-of-Service (DoS) attack against ASR systems. We achieve these characteristics by constructing attacks in a modified frequency domain through an inverse Fourier transform. We evaluate our method on standard keyword classification tasks and analyze it in OTA, and we analyze the properties of the cross-domain attacks to explain the efficiency of the approach.
    
[^7]: 使用深度学习构建具有性能边界的随机局部搜索SAT求解器

    Using deep learning to construct stochastic local search SAT solvers with performance bounds. (arXiv:2309.11452v1 [cs.AI])

    [http://arxiv.org/abs/2309.11452](http://arxiv.org/abs/2309.11452)

    本论文利用深度学习方法构建了一种随机局部搜索SAT求解器，并且使用图神经网络训练了求解SAT问题的“神谕”。实验结果表明，访问基于GNN的神谕显著提高了求解器的性能。

    

    布尔可满足性问题（SAT）是最典型的NP完全问题，具有极大的实际重要性。这个问题的一种重要求解器类别是随机局部搜索（SLS）算法，通过迭代和随机更新候选解来求解。最近，在理论计算机科学领域取得了重大突破，建立了足够的条件，以确保SLS求解器能够有效地求解SAT实例，只要它们可以访问合适的“神谕”，从实例特定的分布中提供样本，利用实例的局部结构。受这些结果以及神经网络在学习大型数据集中常见结构的良好能力的启发，本研究利用图神经网络训练了神谕，并在两个SLS求解器上对具有不同难度的随机SAT实例进行了评估。我们发现，访问基于GNN的神谕显著提高了两个求解器的性能，使它们平均能够解决17个实例。

    The Boolean Satisfiability problem (SAT) is the most prototypical NP-complete problem and of great practical relevance. One important class of solvers for this problem are stochastic local search (SLS) algorithms that iteratively and randomly update a candidate assignment. Recent breakthrough results in theoretical computer science have established sufficient conditions under which SLS solvers are guaranteed to efficiently solve a SAT instance, provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. Motivated by these results and the well established ability of neural networks to learn common structure in large datasets, in this work, we train oracles using Graph Neural Networks and evaluate them on two SLS solvers on random SAT instances of varying difficulty. We find that access to GNN-based oracles significantly boosts the performance of both solvers, allowing them, on average, to solve 17
    
[^8]: 你仅关注屏幕：多模态动作链机器人

    You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])

    [http://arxiv.org/abs/2309.11436](http://arxiv.org/abs/2309.11436)

    本论文提出了一种名为Auto-UI的多模态动作链机器人，通过直接与界面交互，避免了环境解析或依赖于应用程序API的需要，并引入了动作链技术来帮助模型进行决策。

    

    自主用户界面（UI）机器人旨在通过与用户界面进行交互，实现任务自动化，无需手动干预。最近的研究探讨了利用大型语言模型（LLM）的能力，以在多样环境中有效参与。为了符合LLM的输入-输出要求，现有方法在沙盒环境中开发，依赖于外部工具和应用程序特定的API将环境解析为文本元素，并解释预测的动作。因此，这些方法常常受到推理效率低和错误传播风险的困扰。为了缓解这些挑战，我们引入了Auto-UI，一种多模态解决方案，它直接与界面交互，避免了对环境解析或依赖于应用程序相关的API的需求。此外，我们提出了一种动作链技术，利用一系列中间先前动作历史和未来动作计划，以帮助模型进行决策。

    Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
    
[^9]: 医学影像领域的少样本学习系统综述

    A Systematic Review of Few-Shot Learning in Medical Imaging. (arXiv:2309.11433v1 [cs.CV])

    [http://arxiv.org/abs/2309.11433](http://arxiv.org/abs/2309.11433)

    本文系统综述了医学影像中的少样本学习技术，发现少样本学习可以缓解数据稀缺问题并提升医学影像分析性能，特别是在元学习方面。

    

    缺乏标注的医学影像限制了深度学习模型的性能，而这些模型通常需要大规模标记的数据集。少样本学习技术可以减少数据稀缺问题，并增强医学影像分析，尤其是在元学习方面。本系统综述全面概述了医学影像中的少样本学习。我们系统地搜索了相关文献，并从2018年到2023年选择了80篇相关文章。我们基于医学结果（如肿瘤分割、疾病分类和图像配准）、研究的解剖结构（即心脏、肺等）以及所使用的元学习方法对这些文章进行了聚类。对于每个聚类，我们研究了论文的分布以及最先进模型提供的结果。此外，我们还确定了所有研究中的共享通用流程。综述表明少样本学习可以在大多数结果中克服数据稀缺问题，并且元学习是一个流行的选择。

    The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis, especially with meta-learning. This systematic review gives a comprehensive overview of few-shot learning in medical imaging. We searched the literature systematically and selected 80 relevant articles published from 2018 to 2023. We clustered the articles based on medical outcomes, such as tumour segmentation, disease classification, and image registration; anatomical structure investigated (i.e. heart, lung, etc.); and the meta-learning method used. For each cluster, we examined the papers' distributions and the results provided by the state-of-the-art. In addition, we identified a generic pipeline shared among all the studies. The review shows that few-shot learning can overcome data scarcity in most outcomes and that meta-learning is a popular choice to 
    
[^10]: 半导体制造业中无监督故障检测的时间序列数据生成预训练

    Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])

    [http://arxiv.org/abs/2309.11427](http://arxiv.org/abs/2309.11427)

    本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。

    

    本文介绍了TRACE-GPT（Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers），它是用于半导体制造中的未标记数据集上预训练单变量时间序列传感器数据并检测故障的模型。研究表明，通过使用时间卷积嵌入和生成式预训练Transformer（GPT）来提取时间序列数据的特征，并使用交叉熵损失函数对异常序列和正常序列进行分类，我们的模型表现出更好的性能。

    This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
    
[^11]: EDMP: 基于成本引导的扩散运动规划

    EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning. (arXiv:2309.11414v1 [cs.RO])

    [http://arxiv.org/abs/2309.11414](http://arxiv.org/abs/2309.11414)

    EDMP是一种结合了传统和基于深度学习运动规划优势的基于成本引导的扩散运动规划方法，通过对多样化的运动学有效轨迹进行训练，来提高解决方案的成功率。

    

    传统的机器人操纵运动规划包括一组通用算法，旨在最小化执行给定计划的特定于场景的成本。这种方法具有显著的适应性，因为它们可以直接使用于任何新场景，无需特定的训练数据集。然而，如果没有对多样有效轨迹有先验了解，并且没有针对给定场景设计的成本函数，解决方案的成功率往往较低。虽然基于深度学习的算法极大地提高了成功率，但没有专门的训练数据集很难应用。我们提出了EDMP，一种基于成本引导的扩散运动规划集成，旨在结合传统和基于深度学习的运动规划的优势。我们的扩散网络在一组多样化的运动学有效轨迹上进行训练。与传统规划一样，在推断时对于任何新场景，我们计算特定于场景的成本函数。

    Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific 
    
[^12]: 通过潜在对齐分割实现长文本端到端语音翻译

    Long-Form End-to-End Speech Translation via Latent Alignment Segmentation. (arXiv:2309.11384v1 [cs.CL])

    [http://arxiv.org/abs/2309.11384](http://arxiv.org/abs/2309.11384)

    本文提出了一种潜在对齐分割方法，通过将语音翻译与分割放在同一个模型中实现了低延迟的端到端语音翻译，该方法是首次实现了这种方式。

    

    当前的同时语音翻译模型只能处理几秒钟的音频。现有数据集可以根据人工标注的转录和翻译提供句子级别的分割。然而，现实世界中并不存在句子级别的分割。目前的语音分割方法要么提供较差的分割质量，要么需要以延迟换取质量。本文提出了一种新颖的分割方法，用于低延迟的端到端语音翻译。我们利用现有的语音翻译编码器-解码器架构与 ST CTC，并证明它可以在没有监督或额外参数的情况下执行分割任务。据我们所知，我们的方法是第一个允许实际的端到端同时语音翻译的方法，因为同一个模型同时用于翻译和分割。在各种语言对和域内外数据上，我们展示了所提出方法的有效性。

    Current simultaneous speech translation models can process audio only up to a few seconds long. Contemporary datasets provide an oracle segmentation into sentences based on human-annotated transcripts and translations. However, the segmentation into sentences is not available in the real world. Current speech segmentation approaches either offer poor segmentation quality or have to trade latency for quality. In this paper, we propose a novel segmentation approach for a low-latency end-to-end speech translation. We leverage the existing speech translation encoder-decoder architecture with ST CTC and show that it can perform the segmentation task without supervision or additional parameters. To the best of our knowledge, our method is the first that allows an actual end-to-end simultaneous speech translation, as the same model is used for translation and segmentation at the same time. On a diverse set of language pairs and in- and out-of-domain data, we show that the proposed approach ac
    
[^13]: 议论再行动：多专家讨论下的视觉语言导航

    Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions. (arXiv:2309.11382v1 [cs.RO])

    [http://arxiv.org/abs/2309.11382](http://arxiv.org/abs/2309.11382)

    本研究提出了一种新颖的零射击视觉语言导航框架，其中通过与大型模型进行讨论来收集必要的信息，以提升导航性能。

    

    视觉语言导航（VLN）是一项需要包括理解、感知和规划在内的广泛技能的任务。对于这样一个多方面的挑战，先前的VLN方法完全依赖于一个模型自己的思考在一个回合内进行预测。然而，现有的模型，甚至是最先进的大型语言模型GPT4，仍然难以通过单回合自我思考来处理多个任务。在这项工作中，我们从专家咨询会议中得到灵感，引入了一种新颖的零射击VLN框架。在这个框架中，具有不同能力的大型模型被作为领域专家。我们提出的导航代理DiscussNav可以在每一步之前与这些专家积极讨论，收集必要的信息再行动。这些讨论涵盖了关键的导航子任务，如指令理解、环境感知和完成估计。通过综合实验，我们证明讨论可以显著提高VLN的性能，尤其是在需要多方面决策的情况下。

    Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions w
    
[^14]: 增量分块束搜索用于具有可控的质量-延迟权衡的同时语音翻译

    Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff. (arXiv:2309.11379v1 [cs.CL])

    [http://arxiv.org/abs/2309.11379](http://arxiv.org/abs/2309.11379)

    本研究提出了一种增量分块束搜索方法，用于实现同时语音翻译。该方法可以控制质量与延迟的权衡，通过引入本地一致性或保持-n策略来实现。在实验中，我们发现该方法在不改变延迟的情况下可以提高0.6-3.6 BLEU的翻译质量，或者在不改变质量的情况下可以提高0.8-1.4秒的延迟。

    

    最近，基于分块自注意编码模型作为一种有前途的端到端方法在同时语音翻译领域崭露头角。这些模型采用分块束搜索和假设可靠性评分来确定何时等待更多输入语音以便进一步翻译。然而，这种方法直到整个语音输入被消耗掉才能展示出单个的增量翻译，无法直接向用户展示出单个增量翻译。此外，这种方法缺乏控制质量与延迟权衡的机制。我们提出了一种改进的增量分块束搜索方法，结合了本地一致性或保持-n策略用于质量-延迟控制。我们将我们的框架应用于在线或离线翻译模型中，并证明这两种类型都可以有效地在在线模式中使用。MuST-C的实验结果显示0.6-3.6 BLEU的提升而不改变延迟，或0.8-1.4秒的延迟提升而不改变质量。

    Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \textit{incremental} translation to users. Further, this method lacks mechanisms for \textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode.  Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.
    
[^15]: 预处理联邦学习

    Preconditioned Federated Learning. (arXiv:2309.11378v1 [cs.LG])

    [http://arxiv.org/abs/2309.11378](http://arxiv.org/abs/2309.11378)

    本文提出了基于两个自适应框架的新的通信高效联邦学习算法：PreFed和PreFedOp，通过使用新颖的协方差矩阵预处理器来实现适应性。实验证明这些方法在i.i.d.和非i.i.d.设置下均取得了最先进的性能。

    

    联邦学习（FL）是一种分布式机器学习方法，可以以高效的通信和保护隐私的方式进行模型训练。FL中的标准优化方法是联邦平均（FedAvg），它在通信轮之间执行多个本地SGD步骤。与现代的一阶自适应优化相比，FedAvg被认为缺乏算法适应性。本文提出了基于两个自适应框架（本地适应性和服务器端适应性）的新的高效通信FL算法：PreFed和PreFedOp。提出的方法通过使用新颖的协方差矩阵预处理器来实现适应性。理论上，我们为我们的算法提供了收敛性保证。实证实验表明，我们的方法在i.i.d.和非i.i.d.设置上都达到了最先进的性能。

    Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.
    
[^16]: 使用语音识别的动态手势特征的人体运动适应在工具传递中的应用

    Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition. (arXiv:2309.11368v1 [cs.RO])

    [http://arxiv.org/abs/2309.11368](http://arxiv.org/abs/2309.11368)

    本文介绍了一种使用语音识别的动态手势特征的人体运动适应方法，通过无缝集成多种模块来实现用户友好的工具传递，使用户无需额外培训以使用复杂的人机接口。

    

    人机协作有助于用户在交互任务中提高效率。然而，大多数协作方案依赖于复杂的人机接口，与自然肢体控制相比可能缺乏必要的直观性。我们希望通过低训练数据要求来理解人类意图。针对这些挑战，本文介绍了一种创新的人机协作框架，无缝集成了手势和动态运动识别、语音识别和可切换的控制适应策略。这些模块提供了一种用户友好的方法，使机器人能够根据用户需要传递工具，特别是当用户双手同时工作时。因此，用户可以专注于任务执行，而不需要额外培训人机界面的使用，同时机器人可以解读他们的直观手势。

    Human-robot collaboration has benefited users with higher efficiency towards interactive tasks. Nevertheless, most collaborative schemes rely on complicated human-machine interfaces, which might lack the requisite intuitiveness compared with natural limb control. We also expect to understand human intent with low training data requirements. In response to these challenges, this paper introduces an innovative human-robot collaborative framework that seamlessly integrates hand gesture and dynamic movement recognition, voice recognition, and a switchable control adaptation strategy. These modules provide a user-friendly approach that enables the robot to deliver the tools as per user need, especially when the user is working with both hands. Therefore, users can focus on their task execution without additional training in the use of human-machine interfaces, while the robot interprets their intuitive gestures. The proposed multimodal interaction framework is executed in the UR5e robot pla
    
[^17]: 材料科学中的知识图问答（KGQA4MAT）：为金属有机框架知识图（MOF-KG）开发自然语言接口

    Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG). (arXiv:2309.11361v1 [cs.AI])

    [http://arxiv.org/abs/2309.11361](http://arxiv.org/abs/2309.11361)

    本论文为材料科学领域构建了一个知识图问答基准数据集（KGQA4MAT），重点关注金属有机框架（MOF）。通过开发自然语言接口和利用ChatGPT将自然语言问题转化为形式化的KG查询，该论文展示了ChatGPT在解决KGQA问题的潜力。

    

    我们提出了一个全面的材料科学中的知识图问答基准数据集（KGQA4MAT），重点关注金属有机框架（MOF）。通过整合结构化数据库和从文献中提取的知识，构建了一个金属有机框架知识图（MOF-KG）。为了增强领域专家访问MOF-KG的能力，我们旨在开发一个自然语言接口来查询知识图。我们开发了一个基准，包括161个涉及比较、聚合和复杂图结构的复杂问题。每个问题都有三种不同的改写形式，共计644个问题和161个KG查询。为了评估基准，我们开发了一种系统的方法，利用ChatGPT将自然语言问题转化为形式化的KG查询。我们还将该方法应用于著名的QALD-9数据集，展示了ChatGPT在解决不同平台和问题下KGQA问题的潜力。

    We present a comprehensive benchmark dataset for Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and q
    
[^18]: 3D人脸重建：通往法医学的道路

    3D Face Reconstruction: the Road to Forensics. (arXiv:2309.11357v1 [cs.CV])

    [http://arxiv.org/abs/2309.11357](http://arxiv.org/abs/2309.11357)

    本综述研究了3D人脸重建算法的应用，从整形手术到娱乐行业，但在法医学领域的应用仍然有一些限制和障碍。对于将3D人脸重建作为法医学证据的有效工具，我们仍需进行进一步的研究和探索。

    

    3D人脸重建算法通过图像和视频在许多领域得到应用，从整形手术到娱乐行业，得益于其优势特点。然而，当涉及法医应用时，3D人脸重建必须遵守严格的要求，这仍然使其在提供诉讼证据方面的潜在作用不明确。对其在法医学应用中的约束、潜力和限制进行全面的研究仍然缺失。本综述旨在阐明法医应用和生物特征识别之间的关系，重点关注人脸识别。因此，它对来自监控视频和嫌疑犯照片的3D人脸重建算法的成果进行了分析，并讨论了目前阻碍3D人脸重建在法医应用中发挥积极作用的障碍。最后，它还对底层数据集进行了检查，包括其优点和限制。

    3D face reconstruction algorithms from images and videos are applied to many fields, from plastic surgery to the entertainment sector, thanks to their advantageous features. However, when looking at forensic applications, 3D face reconstruction must observe strict requirements that still make its possible role in bringing evidence to a lawsuit unclear. An extensive investigation of the constraints, potential, and limits of its application in forensics is still missing. Shedding some light on this matter is the goal of the present survey, which starts by clarifying the relation between forensic applications and biometrics, with a focus on face recognition. Therefore, it provides an analysis of the achievements of 3D face reconstruction algorithms from surveillance videos and mugshot images and discusses the current obstacles that separate 3D face reconstruction from an active role in forensic applications. Finally, it examines the underlying data sets, with their advantages and limitati
    
[^19]: 针对罕见事件预测的综合调研

    A Comprehensive Survey on Rare Event Prediction. (arXiv:2309.11356v1 [cs.AI])

    [http://arxiv.org/abs/2309.11356](http://arxiv.org/abs/2309.11356)

    本文综合调研了罕见事件预测领域的当前方法，通过考虑罕见事件数据、数据处理、算法方法和评估方法四个维度，总结出了在机器学习流程中解决罕见事件预测问题的关键方法。

    

    罕见事件预测涉及使用机器学习和数据分析识别和预测低概率事件。由于数据分布不平衡，普通事件的频率远远超过罕见事件的频率，因此需要在机器学习流程的每个步骤中使用专门的方法，从数据处理到算法到评估协议。预测罕见事件的发生对于工业4.0等实际应用非常重要，也是统计和机器学习领域的一个活跃的研究领域。本文综合评估了罕见事件预测的当前方法，从罕见事件数据、数据处理、算法方法和评估方法四个维度考虑。具体来说，我们考虑了来自不同模态的73个数据集（数值、图像、文本和音频），四个主要的数据处理分类，五个主要的算法分类和两个更广泛的评估方法。

    Rare event prediction involves identifying and forecasting events with a low probability using machine learning and data analysis. Due to the imbalanced data distributions, where the frequency of common events vastly outweighs that of rare events, it requires using specialized methods within each step of the machine learning pipeline, i.e., from data processing to algorithms to evaluation protocols. Predicting the occurrences of rare events is important for real-world applications, such as Industry 4.0, and is an active research area in statistical and machine learning. This paper comprehensively reviews the current approaches for rare event prediction along four dimensions: rare event data, data processing, algorithmic approaches, and evaluation approaches. Specifically, we consider 73 datasets from different modalities (i.e., numerical, image, text, and audio), four major categories of data processing, five major algorithmic groupings, and two broader evaluation approaches. This pape
    
[^20]: C⋅ASE：学习基于物理的角色的条件对抗技能嵌入

    C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters. (arXiv:2309.11351v1 [cs.GR])

    [http://arxiv.org/abs/2309.11351](http://arxiv.org/abs/2309.11351)

    C⋅ASE是一个学习基于物理的角色的条件对抗技能嵌入的框架，通过将异构的技能动作划分为不同子集，以实现多样性和可控性。训练过程中采用了焦点技能采样、骨骼残余力和逐元素特征屏蔽，以平衡不同复杂性的技能，并捕捉更一般的行为特征。

    

    我们提出了C⋅ASE，一个高效而有效的框架，用于学习基于物理的角色的条件对抗技能嵌入。我们的物理模拟角色可以学习各种技能，同时提供以直接操纵执行的技能的可控性。C⋅ASE将异构的技能动作划分为包含同质样本的不同子集，用于训练低级条件模型来学习条件行为分布。训练过程中采用了焦点技能采样、骨骼残余力和逐元素特征屏蔽，以平衡不同复杂性的各种技能，减轻动力学不匹配以掌握敏捷动作，并捕捉更多的一般行为特征。一旦训练完成，条件模型就可以生成高度多样且逼真的技能。

    We present C$\cdot$ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for physics-based characters. Our physically simulated character can learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, o
    
[^21]: TRAVID：一个端到端的视频翻译框架

    TRAVID: An End-to-End Video Translation Framework. (arXiv:2309.11338v1 [cs.CL])

    [http://arxiv.org/abs/2309.11338](http://arxiv.org/abs/2309.11338)

    TRAVID是一个端到端的视频翻译框架，不仅可以翻译口语，还可以将翻译的语音与说话者的嘴唇动作同步，为学生和用户提供了一种增强的视频翻译体验。

    

    在当今全球化的世界中，与来自不同语言背景的人有效沟通变得越来越重要。虽然传统的语言翻译方法，如文字或仅声音的翻译，可以完成任务，但它们常常无法捕捉到通过面部表情和嘴唇动作传达的完整上下文和微妙信息。在本文中，我们提出了一个端到端的视频翻译系统，它不仅可以翻译口语，还可以将翻译的语音与说话者的嘴唇动作同步。我们的系统专注于翻译各种印度语言的教育讲座，并且它被设计成在资源有限的系统设置中也能有效。通过将与目标语言一致的嘴唇动作与说话者的声音进行匹配，使用语音克隆技术，我们的应用为学生和用户提供了一种增强的体验。这个附加功能创造了一个更完整的视频翻译体验。

    In today's globalized world, effective communication with people from diverse linguistic backgrounds has become increasingly crucial. While traditional methods of language translation, such as written text or voice-only translations, can accomplish the task, they often fail to capture the complete context and nuanced information conveyed through nonverbal cues like facial expressions and lip movements. In this paper, we present an end-to-end video translation system that not only translates spoken language but also synchronizes the translated speech with the lip movements of the speaker. Our system focuses on translating educational lectures in various Indian languages, and it is designed to be effective even in low-resource system settings. By incorporating lip movements that align with the target language and matching them with the speaker's voice using voice cloning techniques, our application offers an enhanced experience for students and users. This additional feature creates a mo
    
[^22]: Gold-YOLO: 通过收集和分发机制实现高效目标检测器

    Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v1 [cs.CV])

    [http://arxiv.org/abs/2309.11331](http://arxiv.org/abs/2309.11331)

    本研究提出了Gold-YOLO模型，通过先进的收集和分发机制（GD）机制以及MAE风格的预训练，解决了YOLO系列模型中的信息融合问题，实现了高效的目标检测和多尺度特征融合。

    

    过去几年中，YOLO系列模型已成为实时目标检测领域的领先方法。许多研究通过修改架构、增加数据和设计新的损失函数将基线提升到了更高水平。然而，我们发现之前的模型仍然存在信息融合问题，虽然特征金字塔网络（FPN）和路径聚合网络（PANet）已经缓解了这个问题。因此，本研究提出了一种先进的收集和分发机制（GD）机制，通过卷积和自注意力操作实现。这个新设计的模型名为Gold-YOLO，提升了多尺度特征融合能力，并在所有模型尺度上实现了延迟和准确性的理想平衡。此外，我们首次在YOLO系列中实现了MAE风格的预训练，使得YOLO系列模型可以从无监督预训练中受益。Gold-YOLO-N在COCO val2017数据集上达到了出色的39.9%平均精度（AP）。

    In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datas
    
[^23]: 使用博弈论的动态定价策略在云市场中的应用

    Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory. (arXiv:2309.11316v1 [cs.GT])

    [http://arxiv.org/abs/2309.11316](http://arxiv.org/abs/2309.11316)

    本研究使用博弈论设计了一个动态定价策略的模型，在云市场中实现了有效的定价策略，增强了市场竞争力。

    

    云市场的竞争性使得定价策略成为企业的一个关键任务，近年来吸引了很多研究者的注意。本研究通过设计一个普通形式的博弈模型来解决这一问题，其中提供者通过一个委员会注册以改善他们的竞争定价策略。博弈论的功能被应用于设计动态定价策略。委员会的使用使得博弈成为一个完全信息的博弈，每个参与者都知道其他人的收益函数。参与者改进他们的定价策略以最大化利润。本文的贡献是以博弈的形式量化了云市场，并提供了新颖的动态定价策略；通过证明博弈的纳什均衡的存在和唯一性验证了该模型。

    The competitive nature of Cloud marketplaces as new concerns in delivery of services makes the pricing policies a crucial task for firms. so that, pricing strategies has recently attracted many researchers. Since game theory can handle such competing well this concern is addressed by designing a normal form game between providers in current research. A committee is considered in which providers register for improving their competition based pricing policies. The functionality of game theory is applied to design dynamic pricing policies. The usage of the committee makes the game a complete information one, in which each player is aware of every others payoff functions. The players enhance their pricing policies to maximize their profits. The contribution of this paper is the quantitative modeling of Cloud marketplaces in form of a game to provide novel dynamic pricing strategies; the model is validated by proving the existence and the uniqueness of Nash equilibrium of the game.
    
[^24]: 云市场中基于竞争的定价策略：利用遗憾最小化技术

    A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques. (arXiv:2309.11312v1 [cs.GT])

    [http://arxiv.org/abs/2309.11312](http://arxiv.org/abs/2309.11312)

    本研究提出了一种基于遗憾最小化算法的云市场定价策略，通过模拟供应商之间的竞争环境并迭代更新策略概率，实现了供应商利润的显著增加。

    

    云计算作为一种相对新的商业模式，已经受到不同研究者的广泛关注，但也面临着许多挑战。定价是云计算市场的一个主要问题，因为供应商需要在不了解对手定价策略的情况下吸引更多的客户。为了解决这个问题，本文提出了一个基于遗憾最小化算法的定价策略，并将其应用于考虑到不完全信息的博弈模型中。基于云计算市场的竞争环境，供应商利用经验遗憾来更新其策略的分布。通过迭代应用算法更新策略概率，可以更快地减小遗憾。实验结果显示，与其他定价策略相比，供应商的利润大幅增加。此外，多种遗憾最小化技术也得到了有效的验证。

    Cloud computing as a fairly new commercial paradigm, widely investigated by different researchers, already has a great range of challenges. Pricing is a major problem in Cloud computing marketplace; as providers are competing to attract more customers without knowing the pricing policies of each other. To overcome this lack of knowledge, we model their competition by an incomplete-information game. Considering the issue, this work proposes a pricing policy related to the regret minimization algorithm and applies it to the considered incomplete-information game. Based on the competition based marketplace of the Cloud, providers update the distribution of their strategies using the experienced regret. The idea of iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized faster. The experimental results show much more increase in profits of the providers in comparison with other pricing policies. Besides, the efficiency of a variety of reg
    
[^25]: 使用行为和对话流特征预测对话任务助手的评分预测

    Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features. (arXiv:2309.11307v1 [cs.CL])

    [http://arxiv.org/abs/2309.11307](http://arxiv.org/abs/2309.11307)

    本文提出了一种预测对话任务助手的评分的方法，通过将对话流特征和用户行为特征结合起来，并使用真实人-机对话和评分数据进行模型训练和分析。结果表明，将对话流和行为方面的特征考虑在内的模型在离线评分预测中具有优势，并且对CTA特定行为特征的分析可以为未来的系统提供启发。

    

    预测对话任务助手（CTA）的成功对于理解用户行为并相应地采取行动至关重要。在本文中，我们提出了TB-Rater，这是一个Transformer模型，它将对话流特征与用户行为特征相结合，用于预测CTA场景中的用户评分。特别地，我们使用了在Alexa TaskBot挑战中收集的真实人-机对话和评分，这是一个新颖的多模态和多轮对话环境。我们的结果显示，将对话流和行为方面的特征模型化为单一模型对于离线评分预测具有优势。此外，对CTA特定行为特征的分析为这种设置带来了洞察，并且可以用于启动未来的系统。

    Predicting the success of Conversational Task Assistants (CTA) can be critical to understand user behavior and act accordingly. In this paper, we propose TB-Rater, a Transformer model which combines conversational-flow features with user behavior features for predicting user ratings in a CTA scenario. In particular, we use real human-agent conversations and ratings collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn conversational context. Our results show the advantages of modeling both the conversational-flow and behavioral aspects of the conversation in a single model for offline rating prediction. Additionally, an analysis of the CTA-specific behavioral features brings insights into this setting and can be used to bootstrap future systems.
    
[^26]: FaceDiffuser：使用扩散技术的基于语音驱动的3D面部动画合成

    FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion. (arXiv:2309.11306v1 [cs.CV])

    [http://arxiv.org/abs/2309.11306](http://arxiv.org/abs/2309.11306)

    FaceDiffuser是一种使用扩散技术的基于语音驱动的3D面部动画合成方法，它采用了非确定性的深度学习模型，并使用了基于3D顶点和混合形状的数据集进行训练。

    

    基于语音驱动的3D面部动画合成一直是一个具有挑战性的任务，无论在工业界还是研究领域。最近的方法主要集中在确定性深度学习方法上，这意味着给定一个语音输入，输出总是相同的。然而，在现实中，脸部中存在的非语言面部线索是非确定性的。此外，大多数方法都集中在基于3D顶点的数据集和与现有面部动画流程兼容的方法上是稀缺的。为了解决这些问题，我们提出了FaceDiffuser，这是一个基于非确定性深度学习模型的生成语音驱动的面部动画的方法，它在训练中使用了基于3D顶点和混合形状的数据集。我们的方法基于扩散技术，使用预训练的大型语音表示模型HuBERT来编码音频输入。据我们所知，我们是首次将扩散方法用于语音驱动的3D面部动画任务。

    Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation
    
[^27]: 云计算中一种成本感知的优化资源配置机制

    A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing. (arXiv:2309.11299v1 [cs.DC])

    [http://arxiv.org/abs/2309.11299](http://arxiv.org/abs/2309.11299)

    本论文提出了一种成本感知的优化资源配置机制，通过学习自动机实现对请求应用程序的高效资源配送，为实现成本效益的服务配置提供了新的方法。

    

    由于云计算中计算资源的广泛使用，出现了新的资源配置挑战。资源配置技术必须在满足请求要求的同时将总成本降到最低。鉴于云服务的广泛应用，开发成本有效的服务配置方案似乎更具挑战性；我们提出了一种新颖的基于学习的资源配置方法，它实现了对需求的降低成本保证。我们优化的资源配置 (ORP) 方法的贡献如下。首先，它被设计为提供一种成本有效的方法来高效处理请求的应用程序；而大多数现有模型只允许一般的流程，关心任务的依赖性，ORP则基于应用程序所组成的服务进行性能优化全面关注他们的高效提供。其次，它是基于学习自动机的机制

    Due to the recent wide use of computational resources in cloud computing, new resource provisioning challenges have been emerged. Resource provisioning techniques must keep total costs to a minimum while meeting the requirements of the requests. According to widely usage of cloud services, it seems more challenging to develop effective schemes for provisioning services cost-effectively; we have proposed a novel learning based resource provisioning approach that achieves cost-reduction guarantees of demands. The contributions of our optimized resource provisioning (ORP) approach are as follows. Firstly, it is designed to provide a cost-effective method to efficiently handle the provisioning of requested applications; while most of the existing models allow only workflows in general which cares about the dependencies of the tasks, ORP performs based on services of which applications comprised and cares about their efficient provisioning totally. Secondly, it is a learning automata-based 
    
[^28]: CPLLM: 基于大规模语言模型的临床预测

    CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])

    [http://arxiv.org/abs/2309.11295](http://arxiv.org/abs/2309.11295)

    CPLLM是一种使用大规模语言模型进行临床疾病预测的方法。通过量化和提示来微调语言模型，利用患者的历史诊断记录来预测目标疾病的诊断结果。实验证明，CPLLM在各项指标上均超越了其他基线模型，显示出显著的改进。

    

    我们提出了一种使用大规模语言模型 (LLM) 进行临床疾病预测的方法，该方法包括对预训练的语言模型进行微调。我们利用量化和提示来微调LLM，任务是预测患者在下一次就诊或随后的诊断中是否会被诊断为目标疾病，并利用他们的历史诊断记录。我们将结果与多个基线模型进行了比较，包括逻辑回归、RETAIN和Med-BERT，后者是使用结构化电子病历数据进行疾病预测的当前最先进模型。实验结果显示，CPLLM在PR-AUC和ROC-AUC指标上均超过了所有测试模型，相比基线模型显示出显著的改进。

    We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
    
[^29]: 《IberLEF 2023的AuTexTification概述：多领域机器生成文本的检测和归属》

    Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains. (arXiv:2309.11285v1 [cs.CL])

    [http://arxiv.org/abs/2309.11285](http://arxiv.org/abs/2309.11285)

    AuTexTification是IberLEF 2023研讨会的共享任务，旨在检测和归属多领域机器生成的文本。数据集包含160,000多条文本，涵盖了英语和西班牙语以及推文、评论、新闻、法律和操作指南等五个领域。共有114个团队参与，提交了175次运行结果。

    

    本文介绍了作为IberLEF 2023研讨会一部分的AuTexTification共享任务的概述，该研讨会是在SEPLN 2023会议框架内的伊比利亚语言评估论坛中进行的。AuTexTification包括两个子任务：在子任务1中，参与者需要确定一段文本是人工撰写还是由大型语言模型生成的。在子任务2中，参与者需要将机器生成的文本归属于六种不同的文本生成模型之一。我们的AuTexTification 2023数据集涵盖了两种语言（英语和西班牙语）和五个领域（推文、评论、新闻、法律和操作指南），共包含超过160,000条文本。共有114个团队报名参与，其中36个团队提交了175次运行结果，其中20个团队还提交了工作笔记。在这个概述中，我们介绍了AuTexTification数据集和任务，以及参与系统的结果。

    This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
    
[^30]: 重新思考传感器建模: 分层信息增强的交通预测

    Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting. (arXiv:2309.11284v1 [cs.AI])

    [http://arxiv.org/abs/2309.11284](http://arxiv.org/abs/2309.11284)

    本文重新思考了传感器的依赖建模问题，从区域和全局两个层次出发，通过合并区域节点和生成全局节点来反映传感器之间的依赖关系，并且应用元图卷积网络来提高节点表示的普遍性和真实性。

    

    随着城市化的加速，交通预测在智慧城市建设中变得至关重要。在时空预测的背景下，关键在于如何建模传感器之间的依赖关系。然而，现有的工作基本上只考虑传感器之间的微观关系，将传感器等同对待，忽视了它们的宏观依赖关系。在本文中，我们认为应该从区域和全局的角度重新思考传感器的依赖建模。特别地，我们将具有高内部区域相关性的原始传感器合并为区域节点，以保留区域间的依赖关系。然后，我们生成代表性和常见的时空模式作为全局节点，以反映传感器之间的全局依赖关系，并为时空依赖学习提供辅助信息。为了追求节点表示的普遍性和真实性，我们引入了元图卷积网络（Meta GCN）来校准区域和全局节点。

    With the acceleration of urbanization, traffic forecasting has become an essential role in smart city construction. In the context of spatio-temporal prediction, the key lies in how to model the dependencies of sensors. However, existing works basically only consider the micro relationships between sensors, where the sensors are treated equally, and their macroscopic dependencies are neglected. In this paper, we argue to rethink the sensor's dependency modeling from two hierarchies: regional and global perspectives. Particularly, we merge original sensors with high intra-region correlation as a region node to preserve the inter-region dependency. Then, we generate representative and common spatio-temporal patterns as global nodes to reflect a global dependency between sensors and provide auxiliary information for spatio-temporal dependency learning. In pursuit of the generality and reality of node representations, we incorporate a Meta GCN to calibrate the regional and global nodes in 
    
[^31]: 使用模块化机器人通过捕食-被捕食者场景诱导开放式进化研究

    Open-endedness induced through a predator-prey scenario using modular robots. (arXiv:2309.11275v1 [cs.RO])

    [http://arxiv.org/abs/2309.11275](http://arxiv.org/abs/2309.11275)

    本研究通过使用具有固定形态的模块化机器人，在捕食-被捕食者场景中诱发了开放式进化的出现。通过引入标记系统，增加了机器人的行为复杂性，并展示了适应性策略的出现。这一研究证明了利用模块化机器人进行捕食-被捕食者动态时诱导开放式进化的可行性。

    

    本研究探讨了如何通过捕食-被捕食者场景诱发开放式进化（OEE）的出现。我们利用具有固定形态的模块化机器人，并对其控制器进行进化。在两个物种中，机器人可以发送和接收信号，并感知环境中其他机器人的相对位置。具体来说，我们引入了一个称为标记系统的特征：它修改了个体之间的感知方式，预计可以增加行为复杂性。我们的结果显示了适应性策略的出现，证明了通过捕食-被捕食者动态使用模块化机器人诱导开放式进化方法的可行性。然而，这种出现似乎依赖于将繁殖条件化为明确的行为准则。

    This work investigates how a predator-prey scenario can induce the emergence of Open-Ended Evolution (OEE). We utilize modular robots of fixed morphologies whose controllers are subject to evolution. In both species, robots can send and receive signals and perceive the relative positions of other robots in the environment. Specifically, we introduce a feature we call a tagging system: it modifies how individuals can perceive each other and is expected to increase behavioral complexity. Our results show the emergence of adaptive strategies, demonstrating the viability of inducing OEE through predator-prey dynamics using modular robots. Such emergence, nevertheless, seemed to depend on conditioning reproduction to an explicit behavioral criterion.
    
[^32]: 使用故障注入测试框架进行机器学习数据适应性和性能测试

    Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework. (arXiv:2309.11274v1 [cs.AI])

    [http://arxiv.org/abs/2309.11274](http://arxiv.org/abs/2309.11274)

    本文提出了基于故障注入的输入数据测试框架，用于测试机器学习模型对多个故意引发的数据故障的弹性。该框架使用数据突变器探索ML系统对不同故障注入效果的脆弱性，并在选定的ML模型之前进行优化。

    

    创建弹性机器学习(ML)系统已成为确保具有用户信心的生产就绪ML系统的必要条件。输入数据和模型的质量对数据敏感系统中成功的端到端测试有很大影响。然而，相对于模型测试来说，输入数据的测试方法并不像模型测试那样系统化，且数量较少。为了填补这一空白，本文提出了故障注入不满意学习输入数据(FIUL-Data)测试框架，用于测试ML模型对多个有意触发的数据故障的弹性。数据突变器可以探索ML系统对不同故障注入效果的脆弱性。该框架根据三个主要思想进行设计：突变器不是随机的；一个数据突变器在一个时间实例应用；在进行选择的ML模型之前进行了优化。本文使用分析化学的数据对FIUL-Data框架进行评估。

    Creating resilient machine learning (ML) systems has become necessary to ensure production-ready ML systems that acquire user confidence seamlessly. The quality of the input data and the model highly influence the successful end-to-end testing in data-sensitive systems. However, the testing approaches of input data are not as systematic and are few compared to model testing. To address this gap, this paper presents the Fault Injection for Undesirable Learning in input Data (FIUL-Data) testing framework that tests the resilience of ML models to multiple intentionally-triggered data faults. Data mutators explore vulnerabilities of ML systems against the effects of different fault injections. The proposed framework is designed based on three main ideas: The mutators are not random; one data mutator is applied at an instance of time, and the selected ML models are optimized beforehand. This paper evaluates the FIUL-Data framework using data from analytical chemistry, comprising retention t
    
[^33]: 为会话助手的复杂任务分割建立基础

    Grounded Complex Task Segmentation for Conversational Assistants. (arXiv:2309.11271v1 [cs.CL])

    [http://arxiv.org/abs/2309.11271](http://arxiv.org/abs/2309.11271)

    本文通过将结构化指令转化为会话式指令，针对会话助手中的复杂任务分割问题进行了研究。实验结果表明基于标记的Transformer模型在计算会话步骤特征方面效果最好，并且用户研究证明提出的方法可以改善原始指令的效果。

    

    与阅读相同的指令相比，会话助手在执行复杂指令时往往会面临注意力和记忆力短缺的问题。因此，当会话助手引导用户完成复杂任务的步骤时，需要将任务结构化为合适长度和复杂度的可管理信息。本文针对食谱领域，将结构化指令转化为会话式指令。我们根据会话场景注释了指令的结构，以了解该场景中的期望行为。为了计算上述会话步骤的特征，我们测试了各种基于Transformer的架构，结果显示基于标记的方法效果最好。进一步的用户研究表明，用户倾向于接受长度和复杂度适中的步骤，并且提出的方法可以提高原始基于网页的指令的效果。

    Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions. Hence, when conversational assistants walk users through the steps of complex tasks, there is a need to structure the task into manageable pieces of information of the right length and complexity. In this paper, we tackle the recipes domain and convert reading structured instructions into conversational structured ones. We annotated the structure of instructions according to a conversational scenario, which provided insights into what is expected in this setting. To computationally model the conversational step's characteristics, we tested various Transformer-based architectures, showing that a token-based approach delivers the best results. A further user study showed that users tend to favor steps of manageable complexity and length, and that the proposed methodology can improve the original web-based instructi
    
[^34]: 序列到序列的西班牙预训练语言模型

    Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])

    [http://arxiv.org/abs/2309.11259](http://arxiv.org/abs/2309.11259)

    该论文介绍了一种新的序列到序列的西班牙预训练语言模型，该模型在各种序列到序列任务中表现出了竞争性能，并提供了BART、T5和BERT2BERT-style模型的西班牙版本。

    

    近年来，预训练语言模型的重大进展为许多非英语语言版本的开发铺平了道路，其中特别关注了仅编码器和仅解码器的架构。虽然西班牙语语言模型包括BERT、RoBERTa和GPT在自然语言理解和生成方面展现出了优势，但在涉及输入输出对的序列到序列任务中，缺乏编码器-解码器模型。本文通过引入实施和评估著名的仅在西班牙语语料库上进行预训练的编码器-解码器架构，开创了新的领域。具体而言，我们提出了BART、T5和BERT2BERT风格模型的西班牙语版本，并对它们在各种序列到序列任务上进行了全面评估，包括摘要、重述和生成式问答。我们的研究结果强调了所有模型的竞争性能，其中BART和T5表现出色。

    In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
    
[^35]: 针对空中战斗机动的分层多智能体强化学习

    Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering. (arXiv:2309.11247v1 [cs.LG])

    [http://arxiv.org/abs/2309.11247](http://arxiv.org/abs/2309.11247)

    提出了一个针对空中战斗机动的分层多智能体强化学习框架，通过将决策过程分为两个层次的抽象来处理高维状态和动作空间的挑战，通过训练低层策略实现准确的单位战斗控制。

    

    将人工智能应用于模拟空中对空格斗场景正引起越来越多的关注。迄今为止，高维状态和行动空间、情况信息的高复杂性（如不完美和筛选信息、随机性、关于任务目标的不完全知识）以及非线性飞行动力学给准确的空战决策带来了重大挑战。当涉及多个异构智能体时，这些挑战变得更加严峻。我们提出了一个针对多个异构智能体的空对空格斗的分层多智能体强化学习框架。在我们的框架中，决策过程分为两个层次的抽象，异构的低层策略控制单个单位的行动，并且在整体任务目标下，高层指挥策略发出宏观命令。对于准确的单位战斗控制，我们训练了低层策略。他们的训练是以一种学习方式组织起来的。

    The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learnin
    
[^36]: 重新思考色彩传递：带有交换因子的抬升模型构建

    Colour Passing Revisited: Lifted Model Construction with Commutative Factors. (arXiv:2309.11236v1 [cs.AI])

    [http://arxiv.org/abs/2309.11236](http://arxiv.org/abs/2309.11236)

    提出一种改进的色彩传递算法，利用逻辑变量构建了一个与特定推理算法无关的抬升表示，同时利用因子的交换性，大大提升了概率推理查询速度。

    

    带交换因子的抬升模型构建可以利用概率模型中的对称性，实现对领域大小的可行的概率推理。为了应用抬升推理，必须获得一个抬升表示，并且为了这样做，目前的方法是使用所谓的色彩传递算法。不过，我们发现色彩传递算法在构建抬升表示时忽略了因子的交换性。我们提出了改进的色彩传递算法，利用逻辑变量构建一个与特定推理算法无关的抬升表示，同时在离线阶段利用因子的交换性。我们的算法能够更高效地检测出更多的对称性，从而大大增加压缩率，使得应用该模型时的概率推理查询速度显著提升。

    Lifted probabilistic inference exploits symmetries in a probabilistic model to allow for tractable probabilistic inference with respect to domain sizes. To apply lifted inference, a lifted representation has to be obtained, and to do so, the so-called colour passing algorithm is the state of the art. The colour passing algorithm, however, is bound to a specific inference algorithm and we found that it ignores commutativity of factors while constructing a lifted representation. We contribute a modified version of the colour passing algorithm that uses logical variables to construct a lifted representation independent of a specific inference algorithm while at the same time exploiting commutativity of factors during an offline-step. Our proposed algorithm efficiently detects more symmetries than the state of the art and thereby drastically increases compression, yielding significantly faster online query times for probabilistic inference when the resulting model is applied.
    
[^37]: 使用ChatGPT-4作为西班牙学术图书编辑工具的研究

    ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish. (arXiv:2309.11231v1 [cs.AI])

    [http://arxiv.org/abs/2309.11231](http://arxiv.org/abs/2309.11231)

    本研究评估了ChatGPT-4作为西班牙学术图书编辑工具的潜力，结果显示ChatGPT-4在语法和拼写纠正方面具有高精度和快速性能，但在上下文敏感性和文献计量等方面仍存在挑战。

    

    本研究评估了由OpenAI开发的人工智能语言模型ChatGPT-4在编辑西班牙文学和学术图书方面的潜力。出版行业对高效且易于访问的审阅和编辑过程的需求推动了对自动化解决方案的寻求。作为最先进的语言模型之一，ChatGPT-4在文本理解和生成方面具有显著的能力。本研究分析了ChatGPT-4的特点和能力，包括语法校正、文体连贯性和西班牙文本的语言丰富性。对100个文学和学术文本进行了测试，比较了ChatGPT-4与专家人工审阅者和编辑者的修改。结果表明，虽然ChatGPT-4能够在非常短的时间内高精度地进行语法和拼写纠正，但在上下文敏感性和文献计量等领域仍面临挑战。

    This study evaluates the potential of ChatGPT-4, an artificial intelligence language model developed by OpenAI, as an editing tool for Spanish literary and academic books. The need for efficient and accessible reviewing and editing processes in the publishing industry has driven the search for automated solutions. ChatGPT-4, being one of the most advanced language models, offers notable capabilities in text comprehension and generation. In this study, the features and capabilities of ChatGPT-4 are analyzed in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish. Tests were conducted with 100 literary and academic texts, where the edits made by ChatGPT-4 were compared to those made by expert human reviewers and editors. The results show that while ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time, it still faces challenges in areas such as context sensitivity, bibliometric an
    
[^38]: 在线互动中利用多样性

    Leveraging Diversity in Online Interactions. (arXiv:2309.11224v1 [cs.AI])

    [http://arxiv.org/abs/2309.11224](http://arxiv.org/abs/2309.11224)

    本文研究了在在线互动中如何利用多样性，通过利用声明性规范来连接人们以帮助他们解决日常问题。试点结果表明，所选择的个人资料的多样性取得了相对成功，并得到了用户的高度满意。

    

    本文解决了在线连接人们来帮助他们解决日常问题的问题。我们利用声明性规范来调节在线互动，并特别关注在连接人们时利用多样性的问题。我们在不同的大学网站上进行了试点，结果显示所选择的个人资料的多样性相对成功，并得到了高用户满意度的支持。

    This paper addresses the issue of connecting people online to help them find support with their day-to-day problems. We make use of declarative norms for mediating online interactions, and we specifically focus on the issue of leveraging diversity when connecting people. We run pilots at different university sites, and the results show relative success in the diversity of the selected profiles, backed by high user satisfaction.
    
[^39]: 提取-改写-回答：一种用于知识图谱问答的增强型LLMs框架

    Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v1 [cs.CL])

    [http://arxiv.org/abs/2309.11206](http://arxiv.org/abs/2309.11206)

    提出了一种提高知识图谱问答任务性能的增强型LLMs框架，通过转化KG知识为文本化陈述的方式，实现了对答案敏感的KG-to-Text方法。

    

    尽管大型语言模型（LLMs）在知识密集型任务上表现出色，但仍然存在在记忆所有世界知识，尤其是长尾知识方面的局限性。本文研究了基于知识图谱增强语言模型的方法，用于解决需要丰富世界知识的知识图谱问答（KGQA）任务。现有的工作表明，检索知识图谱（KG）以增强LLMs提示可以显著改善KGQA中LLMs的性能。然而，他们的方法缺乏基于文本的合理表述KG知识，即忽略了KG表示和文本表示之间的差距。为此，我们提出了一种对答案敏感的KG-to-Text方法，可以将KG知识转化为最具信息量的文本化陈述，用于KGQA。基于该方法，我们提出了一种用于解决KGQA任务的增强型KG-to-Text LLMS框架。在几个KGQA基准上的实验表明，所提出的KG-to-Text增强LLMs方法在性能上表现优异。

    Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperfor
    
[^40]: 利用人工智能自动化编织图案

    Using Artificial Intelligence for the Automation of Knitting Patterns. (arXiv:2309.11202v1 [cs.AI])

    [http://arxiv.org/abs/2309.11202](http://arxiv.org/abs/2309.11202)

    本研究提出了一种利用数据增强和迁移学习技术的深度学习模型，用于识别和分类编织图案。模型采用Inception ResNet-V2作为特征提取和分类算法，在准确率、对数损失、F1分数、精确度和召回率等指标上进行评估。

    

    编织图案是创建和设计针织材料的关键组成部分。传统上，这些图案是通过非正式途径传授的，但由于技术的进步，任何对编织感兴趣的人都可以使用这些图案作为指导开始编织。尽管编织大多是个爱好，除了利用专门的编织机进行工业制造之外，使用人工智能进行编织的应用还不如其他领域广泛。然而，确定使用自动化系统进行编织图案分类是否可行是很重要的。为了识别和分类编织图案，本研究提出了一种深度学习模型，该模型使用数据增强和迁移学习技术。Inception ResNet-V2是该模型中主要的特征提取和分类算法。使用准确率、对数损失、F1分数、精确度和召回率评估了模型。模型评估的结果表明...

    Knitting patterns are a crucial component in the creation and design of knitted materials. Traditionally, these patterns were taught informally, but thanks to advancements in technology, anyone interested in knitting can use the patterns as a guide to start knitting. Perhaps because knitting is mostly a hobby, with the exception of industrial manufacturing utilising specialised knitting machines, the use of Al in knitting is less widespread than its application in other fields. However, it is important to determine whether knitted pattern classification using an automated system is viable. In order to recognise and classify knitting patterns. Using data augmentation and a transfer learning technique, this study proposes a deep learning model. The Inception ResNet-V2 is the main feature extraction and classification algorithm used in the model. Metrics like accuracy, logarithmic loss, F1-score, precision, and recall score were used to evaluate the model. The model evaluation's findings 
    
[^41]: 何时信任人工智能：神经网络认证的进展与挑战

    When to Trust AI: Advances and Challenges for Certification of Neural Networks. (arXiv:2309.11196v1 [cs.LG])

    [http://arxiv.org/abs/2309.11196](http://arxiv.org/abs/2309.11196)

    本文主要关注人工智能（AI）的认证和可解释性，综述了已经开发的用于确保AI决策安全的技术，并探讨了未来的挑战。

    

    人工智能（AI）正在快速发展，并且正准备在自主系统、医学诊断和自然语言处理等各种应用中进行部署。对于真实世界应用来说，早期采用AI技术并不是没有问题的，特别是对于神经网络来说，其可能是不稳定的，并容易受到对抗性示例的影响。从长远来看，需要开发适当的安全保证技术，以减少可避免的系统故障带来的潜在危害，并确保其可信性。本文以认证和可解释性为重点，综述了已经开发的用于确保AI决策安全的技术，并讨论了未来的挑战。

    Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.
    
[^42]: 长尾增强图对比学习用于推荐

    Long-tail Augmented Graph Contrastive Learning for Recommendation. (arXiv:2309.11177v1 [cs.IR])

    [http://arxiv.org/abs/2309.11177](http://arxiv.org/abs/2309.11177)

    提出了一种新颖的长尾增强图对比学习（LAGCL）方法用于推荐系统，通过引入可学习的长尾增强方法和生成对比视图来解决头尾节点之间的显著度差异问题，并采用度均衡机制来平衡数据增强程度。

    

    图卷积网络（GCNs）已经在推荐系统中展示了有希望的结果，因为它们可以有效地利用高阶关系。然而，这些方法在真实场景中通常遇到数据稀疏问题。为了解决这个问题，基于GCN的推荐方法采用对比学习来引入自监督的信号。尽管这些方法有效，但它们缺乏对头和尾节点之间显著度差异的考虑。这可能导致非均匀的表示分布，这对于对比学习方法的性能是一个关键因素。为了解决上述问题，我们提出了一种新颖的用于推荐的长尾增强图对比学习（LAGCL）方法。具体而言，我们引入了一种可学习的长尾增强方法，通过补充预测的邻居信息来增强尾节点，并基于生成的增强图生成对比视图。为了使数据增强的程度平衡，我们还引入了一个度均衡机制。在实验中，我们在两个真实世界的推荐数据集上评估了LAGCL的性能，并与现有的对比学习方法进行了比较。

    Graph Convolutional Networks (GCNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship. However, these methods usually encounter data sparsity issue in real-world scenarios. To address this issue, GCN-based recommendation methods employ contrastive learning to introduce self-supervised signals. Despite their effectiveness, these methods lack consideration of the significant degree disparity between head and tail nodes. This can lead to non-uniform representation distribution, which is a crucial factor for the performance of contrastive learning methods. To tackle the above issue, we propose a novel Long-tail Augmented Graph Contrastive Learning (LAGCL) method for recommendation. Specifically, we introduce a learnable long-tail augmentation approach to enhance tail nodes by supplementing predicted neighbor information, and generate contrastive views based on the resulting augmented graph. To make the data augmentation sch
    
[^43]: 大型语言模型对单词级扰动真的具有鲁棒性吗？

    Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v1 [cs.CL])

    [http://arxiv.org/abs/2309.11166](http://arxiv.org/abs/2309.11166)

    该论文提出了一种用于评估大型语言模型（LLMs）鲁棒性的新颖方法，使用预训练的奖励模型作为诊断工具。实验证明这种方法在评估LLM鲁棒性方面表现准确。

    

    大型语言模型（LLMs）在规模和能力上的快速发展使它们成为各种下游任务的有前途的工具。除了追求更好的性能和避免对特定提示的激烈反馈外，确保LLM的责任性还需要关注LLMs的鲁棒性。然而，现有的评估方法大多依赖于具有预定义监督标签的传统问答数据集，这与当代LLMs的出色生成能力不一致。为了解决这个问题，我们提出了一种新颖的合理评估方法，利用预训练的奖励模型作为诊断工具来评估LLMs的鲁棒性，我们将其称为合理鲁棒性评估的奖励模型（TREvaL）。我们广泛的实证实验表明，TREval提供了一种准确评估LLM鲁棒性的方法，特别是面对更具挑战性的开放式问题时。

    The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open 
    
[^44]: ProtoExplorer：通过原型探索和优化，对深度伪造视频进行可解释的取证分析

    ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement. (arXiv:2309.11155v1 [cs.AI])

    [http://arxiv.org/abs/2309.11155](http://arxiv.org/abs/2309.11155)

    本文提出了一个用于探索和优化基于原型的深度伪造检测模型的可视化分析系统ProtoExplorer，为深度学习模型的可解释性提供了一种新方法。

    

    在高风险环境中，能够为人类提供可解释的预测的机器学习模型至关重要。随着复杂的基于深度学习的模型和大量可调参数的出现，这一点变得更加重要。最近，基于原型的方法已经成为使深度学习可解释的一种有前途的方法。我们特别关注在取证环境中对深度伪造视频的分析。尽管原型基方法已经被引入用于检测深度伪造视频，但在实际情况下使用仍然存在重大挑战，原因在于原型往往过于相似，可解释性在不同原型之间变化。本文提出了一种用于原型学习的可视化分析流程模型，并基于此模型提出了ProtoExplorer，一个用于探索和优化基于原型的深度伪造检测模型的可视化分析系统。ProtoExplorer提供了可视化和时间过滤原型的工具。

    In high-stakes settings, Machine Learning models that can provide predictions that are interpretable for humans are crucial. This is even more true with the advent of complex deep learning based models with a huge number of tunable parameters. Recently, prototype-based methods have emerged as a promising approach to make deep learning interpretable. We particularly focus on the analysis of deepfake videos in a forensics context. Although prototype-based methods have been introduced for the detection of deepfake videos, their use in real-world scenarios still presents major challenges, in that prototypes tend to be overly similar and interpretability varies between prototypes. This paper proposes a Visual Analytics process model for prototype learning, and, based on this, presents ProtoExplorer, a Visual Analytics system for the exploration and refinement of prototype-based deepfake detection models. ProtoExplorer offers tools for visualizing and temporally filtering prototype-based pre
    
[^45]: CoT-BERT: 通过思维链条增强无监督句子表示

    CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])

    [http://arxiv.org/abs/2309.11143](http://arxiv.org/abs/2309.11143)

    CoT-BERT提出了一种通过思维链条增强无监督句子表示的方法，通过两个阶段的处理，引入思维链条的概念进行向量化，以提高模型性能。

    

    无监督句子表示学习旨在将输入句子转化为富含复杂语义信息的固定长度向量，同时消除对标注数据的依赖。近年来，在对比学习和提示工程的推动下，该领域取得了显著进展，极大地缩小了无监督和有监督策略之间的差距。然而，在这个轨迹中，仍然没有充分利用思维链条的潜在能力。为了释放预训练模型（如BERT）中的潜能，我们提出了一个句子表示的两阶段方法：理解和摘要。随后，后一阶段的输出被利用为输入句子的向量化表示。为了进一步提高性能，我们对对比学习损失函数和模板去噪技术进行了精细调整。严格的实验验证了我们的方法CoT-BERT的优越性。

    Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
    
[^46]: 基于语义编码和知识蒸馏的面向语言的通信用于文本到图像生成

    Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation. (arXiv:2309.11127v1 [eess.SP])

    [http://arxiv.org/abs/2309.11127](http://arxiv.org/abs/2309.11127)

    本文提出了一种基于语义编码和知识蒸馏的面向语言的通信框架，通过将大型语言模型和生成模型集成到语义通信范式中，实现了使用人类语言消息进行通信的效率。同时引入了语义源编码、语义信道编码和语义知识蒸馏等创新算法来提高通信的效果。

    

    通过将最近的大型语言模型（LLM）和生成模型集成到新兴的语义通信（SC）范式中，本文提出了一种新颖的面向语言的语义通信（LSC）框架。在LSC中，机器使用可以通过自然语言处理（NLP）技术解释和操作的人类语言消息进行通信，以提高SC效率。为了展示LSC的潜力，我们引入了三种创新算法：1）语义源编码（SSC），将文本提示压缩成捕捉提示的句法本质的关键头词，同时保持它们的出现顺序以保持提示的上下文；2）语义信道编码（SCC），通过用更长的同义词替换头词来提高对错误的容错性；3）语义知识蒸馏（SKD），通过在上下文中学习听众的语言风格来生成定制的提示。在逐步进行的通信任务中，我们验证了LSC的潜在能力。

    By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive te
    
[^47]: AttentionMix: 一种基于BERT注意机制的数据增强方法

    AttentionMix: Data augmentation method that relies on BERT attention mechanism. (arXiv:2309.11104v1 [cs.CL])

    [http://arxiv.org/abs/2309.11104](http://arxiv.org/abs/2309.11104)

    AttentionMix是一种新的数据增强方法，利用了基于注意力的信息。在三个标准情感分类数据集上的评估结果表明，AttentionMix在NLP领域的数据增强中表现优于使用Mixup机制的两种基准方法和普通BERT方法。

    

    Mixup方法在计算机视觉领域已被证明是一种强大的数据增强技术，并且有许多以引导方式执行图像混合的后继方法。将Mixup思想应用于其他领域，如自然语言处理（NLP），是一个有趣的研究方向。尽管已经存在一些将Mixup应用于文本数据的方法，但仍有改进的空间。在本文中，我们介绍了AttentionMix，一种基于注意力的混合方法。虽然本文关注BERT注意机制，但所提出的方法可以应用于任何基于注意力的模型。AttentionMix在3个标准情感分类数据集上进行了评估，并在所有三种情况下表现优于利用Mixup机制和普通BERT方法的两种基准方法。结果证实，注意力的信息可以有效地用于NLP领域的数据增强。

    The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.
    
[^48]: 一个新的可解释的基于神经网络的规则模型用于医疗决策

    A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making. (arXiv:2309.11101v1 [cs.LG])

    [http://arxiv.org/abs/2309.11101](http://arxiv.org/abs/2309.11101)

    本研究介绍了一个新的神经网络模型，称为TT-rules，在医疗决策中具有可解释性，并通过将神经网络转换为基于规则的模型实现了高性能。该模型支持二分类、多标签分类和回归任务，且在医疗应用中表现出色。

    

    在医疗应用中，理解机器/深度学习模型如何做出决策至关重要。在这项研究中，我们介绍了一个神经网络框架，称为“真值表规则”（TT-rules），它将基于规则的模型的全局和精确可解释性性质与深度神经网络的高性能相结合。TT-rules基于“真值表网络”（TTnet）构建，这是一族最初用于形式验证的深度神经网络。通过从训练好的TTnet模型中提取必要且充分的规则$\mathcal{R}$来产生与TTnet相同输出的规则（全局可解释性），TT-rules有效地将神经网络转换为基于规则的模型。这种基于规则的模型支持小到大的表格数据集的二分类、多标签分类和回归任务。在概述了框架后，我们评估了TT-rules在医疗应用中的性能，并将其与最先进的规则模型进行了比较。

    In healthcare applications, understanding how machine/deep learning models make decisions is crucial. In this study, we introduce a neural network framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks. TT-rules is built upon $\textit{Truth Table nets}$ (TTnet), a family of deep neural networks initially developed for formal verification. By extracting the necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for small to large tabular datasets. After outlining the framework, we evaluate TT-rules' performance on healthcare applications and compare it to state-of-the-art rul
    
[^49]: 结合Dropout不确定性与轨迹采样的实用概率模型基于深度强化学习

    Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling. (arXiv:2309.11089v1 [eess.SY])

    [http://arxiv.org/abs/2309.11089](http://arxiv.org/abs/2309.11089)

    本论文提出了一种实用的概率模型基于深度强化学习方法，通过结合dropout不确定性和轨迹采样，稳定地预测系统不确定性，纠正神经网络的拟合误差，过滤aleatoric不确定性，从而实现了优越的控制能力。在实验评估中，该方法在多个控制任务和实际机械臂操作任务中表现出优于其他方法和模型的性能。

    

    本文针对当前建立在神经网络上的概率模型基于强化学习（MBRL）的预测稳定性、预测准确性和控制能力进行了研究。提出了一种新颖的方法，即基于dropout的概率集成和轨迹采样（DPETS），在一个框架中通过Monte-Carlo dropout和轨迹采样来稳定地预测系统不确定性。其损失函数设计用于纠正神经网络的拟合误差，以更准确地预测概率模型。其策略中的状态传播被扩展用于滤除aleatoric不确定性，从而实现了更优越的控制能力。通过在几个Mujoco基准控制任务和一个实际机械臂操作任务下的评估，DPETS在平均回报和收敛速度上优于相关MBRL方法，同时在显著的样本效率方面表现出优异性能，胜过知名的无模型基线模型。

    This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The ope
    
[^50]: 弱监督在标注高效可视化错误检测中的应用

    Weak Supervision for Label Efficient Visual Bug Detection. (arXiv:2309.11077v1 [cs.CV])

    [http://arxiv.org/abs/2309.11077](http://arxiv.org/abs/2309.11077)

    我们提出了一种利用未标注数据和领域特定增强技术的弱监督方法，在视觉错误检测中实现了数据集扩大和自主/交互弱监督，展示了在广阔的游戏世界中的有效性。

    

    随着视频游戏进化为广阔、细致的世界，视觉质量变得至关重要，但也日益具有挑战性。传统的测试方法由于资源有限，难以应对大量潜在的错误。机器学习提供了可扩展的解决方案，然而，对大型标注数据集的严重依赖仍然是一个制约因素。针对这一挑战，我们提出了一种新的方法，利用未标注的游戏玩法和领域特定的增强技术生成用于预训练或多任务设置的数据集和自监督目标，用于后续的视觉错误检测。我们的方法使用弱监督技术扩大量表和促进自主和交互式弱监督，包括基于无监督聚类和/或基于文本和几何提示的交互式方法。我们在广阔的Giantmap游戏世界中展示了对一人称玩家剪辑/碰撞错误（FPPC）的检测，证明了我们的方法的有效性

    As video games evolve into expansive, detailed worlds, visual quality becomes essential, yet increasingly challenging. Traditional testing methods, limited by resources, face difficulties in addressing the plethora of potential bugs. Machine learning offers scalable solutions; however, heavy reliance on large labeled datasets remains a constraint. Addressing this challenge, we propose a novel method, utilizing unlabeled gameplay and domain-specific augmentations to generate datasets & self-supervised objectives used during pre-training or multi-task settings for downstream visual bug detection. Our methodology uses weak-supervision to scale datasets for the crafted objectives and facilitates both autonomous and interactive weak-supervision, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate on first-person player clipping/collision bugs (FPPC) within the expansive Giantmap game world, that our approach is very effect
    
[^51]: 动态平铺: 高效准确的小物体检测的模型无关、自适应、可扩展和推理数据中心化方法

    Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection. (arXiv:2309.11069v1 [cs.CV])

    [http://arxiv.org/abs/2309.11069](http://arxiv.org/abs/2309.11069)

    动态平铺是一种模型无关、自适应、可扩展且推理数据中心化的方法，通过使用动态重叠率和平铺缩小器，解决了碎片化的小物体检测问题，并提高了检测准确性和计算效率。

    

    我们引入了动态平铺方法，这是一种模型无关、自适应和可扩展的小物体检测方法，以我们的推理数据中心化哲学为基础。动态平铺从非重叠的平铺开始进行初始检测，并利用动态重叠率和平铺缩小器。这种双重方法有效地解决了碎片化的物体，提高了检测准确性，并通过减少对物体检测模型的正向传递次数来减小计算开销。适用于各种操作环境，我们的方法消除了繁琐的重新校准的需求。此外，我们的大小过滤机制提高了不同大小物体的检测质量。总体而言，动态平铺优于现有的模型无关均匀裁剪方法，为效率和准确性设立了新的基准。

    We introduce Dynamic Tiling, a model-agnostic, adaptive, and scalable approach for small object detection, anchored in our inference-data-centric philosophy. Dynamic Tiling starts with non-overlapping tiles for initial detections and utilizes dynamic overlapping rates along with a tile minimizer. This dual approach effectively resolves fragmented objects, improves detection accuracy, and minimizes computational overhead by reducing the number of forward passes through the object detection model. Adaptable to a variety of operational environments, our method negates the need for laborious recalibration. Additionally, our large-small filtering mechanism boosts the detection quality across a range of object sizes. Overall, Dynamic Tiling outperforms existing model-agnostic uniform cropping methods, setting new benchmarks for efficiency and accuracy.
    
[^52]: 探索LLM幻觉与提示语言细微差别之间的关系：可读性、形式化和具体性

    Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness. (arXiv:2309.11064v1 [cs.AI])

    [http://arxiv.org/abs/2309.11064](http://arxiv.org/abs/2309.11064)

    本文通过初步探索性调查研究了LLM的幻觉现象与提示语言因素之间的关系，发现更形式化和具体的提示有助于减少幻觉的发生，但可读性相关的结果则不确定。

    

    随着大型语言模型（LLM）的发展，它们引发了新的挑战，其中突出的问题之一是LLM的幻觉现象。虽然出现了各种缓解技术来解决幻觉问题，但深入研究其根本原因同样至关重要。因此，在这个初步的探索性调查中，我们研究了提示语言因素，特别是可读性、形式化和具体性，对幻觉发生的影响。我们的实验结果表明，具有更高形式化和具体性的提示倾向于减少幻觉现象。然而，与可读性相关的结果有些不确定，显示出了混合的模式。

    As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination. While various mitigation techniques are emerging to address hallucination, it is equally crucial to delve into its underlying causes. Consequently, in this preliminary exploratory investigation, we examine how linguistic factors in prompts, specifically readability, formality, and concreteness, influence the occurrence of hallucinations. Our experimental results suggest that prompts characterized by greater formality and concreteness tend to result in reduced hallucination. However, the outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern.
    
[^53]: 数学问题解决中的思路链设计

    Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])

    [http://arxiv.org/abs/2309.11054](http://arxiv.org/abs/2309.11054)

    本论文研究了数学问题解决中思路链的设计方法，对比了自然语言思路链和程序思路链的效果，并发现程序思路链通常在数学问题解决中更加有效，特别是自我描述程序具有更大多样性且性能更高。此外，研究还发现Python是程序思路链的较好选择。实验结果为未来思路链设计提供了宝贵指导。

    

    思路链在数学问题解决中扮演着至关重要的角色。我们对设计思路链的方法进行了全面的考察，比较了传统自然语言思路链和各种程序思路链，包括自我描述程序、注释描述程序和非描述程序。此外，我们还研究了编程语言对程序思路链的影响，比较了Python和Wolfram语言。通过对GSM8K、MATHQA和SVAMP进行广泛实验，我们发现程序思路链在数学问题解决中通常具有更好的效果。值得注意的是，具有30B参数的最佳组合明显超过了GPT-3.5-turbo。结果表明，自我描述程序提供了更大的多样性，因此通常可以实现更高的性能。我们还发现，Python是程序思路链的更好选择比Wolfram语言。实验结果为未来考虑因素提供了宝贵的指导。

    Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
    
[^54]: Clustered FedStack：基于贝叶斯信息准则的中间全局模型。

    Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])

    [http://arxiv.org/abs/2309.11044](http://arxiv.org/abs/2309.11044)

    提出了一种基于贝叶斯信息准则的中间全局模型Clustered FedStack。通过将本地客户端的模型预测和输出层权重发送到服务器，构建一个强大的全局模型，并使用聚类机制对本地客户进行聚类。

    

    联邦学习（FL）是目前人工智能领域最受欢迎的技术之一，因其协作学习和保护客户隐私的能力而受到青睐。然而，它面临非独立和非独立分布（非IID）以及本地客户之间标签不平衡等挑战。为了解决这些限制，研究团队探索了各种方法，如使用本地模型参数、联邦生成对抗学习和联邦表示学习。在我们的研究中，我们提出了一种基于已发表的Stacked Federated Learning（FedStack）框架的新颖Clustered FedStack框架。本地客户端将其模型预测和输出层权重发送到服务器，然后构建一个强大的全局模型。这个全局模型使用聚类机制基于其输出层权重对本地客户进行聚类。我们采用了三种聚类机制，分别是K-Means、Agglomerative、DBSCAN。

    Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, a
    
[^55]: 使用任务适配器的混合多任务学习使小型语言模型更好

    Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])

    [http://arxiv.org/abs/2309.11042](http://arxiv.org/abs/2309.11042)

    本论文提出了一种使用任务适配器的混合多任务学习方法，通过在小型语言模型上构建混合任务适配器，同时处理多个NLP任务，并通过两阶段训练方法优化适配器之间的协作，从而实现在较小的计算成本下支持多个领域特定应用。

    

    最近，大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了惊人的零样本学习性能，特别是文本生成任务。然而，LLMs的尺寸通常会导致模型训练和在线部署的高计算成本。在我们的工作中，我们提出了ALTER，一个在小型语言模型（参数<1B）上有效构建混合任务适配器的多任务学习系统，以同时处理多个NLP任务，并捕捉任务之间的共同点和差异，以支持特定领域的应用。具体而言，在ALTER中，我们提出了任务适配器混合（MTA）模块，作为底层模型变压器架构的扩展，用于捕捉任务内部的知识和任务间的知识。进一步提出了一种两阶段的训练方法，以在较小的计算成本下优化适配器之间的协作。在混合NLP任务的实验结果上

    Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with <1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks 
    
[^56]: 智能交通系统中的联邦学习：最近的应用和开放问题

    Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems. (arXiv:2309.11039v1 [cs.LG])

    [http://arxiv.org/abs/2309.11039](http://arxiv.org/abs/2309.11039)

    联邦学习在智能交通系统中提供了一种具有隐私保护和易扩展性的新范式，为解决动态车辆环境中的各种应用挑战提供了解决方案。

    

    智能交通系统（ITS）受到通信技术、传感器技术和物联网（IoT）的迅猛发展的推动。然而，由于车辆网络的动态特性，及时准确地决策车辆行为非常具有挑战性。此外，在移动无线通信的情况下，车辆信息的隐私和安全面临着持续的风险。在这种背景下，急需一种新的范式来应对动态车辆环境中的各种应用。作为一种分布式机器学习技术，联邦学习（FL）因其出色的隐私保护性能和易扩展性而受到广泛关注。我们对ITS中FL最新发展进行了全面调研。具体而言，我们首先研究ITS中普遍存在的挑战，并从各个角度阐明应用FL的动机。随后，我们回顾了现有的FL部署

    Intelligent transportation systems (ITSs) have been fueled by the rapid development of communication technologies, sensor technologies, and the Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of the vehicle networks, it is rather challenging to make timely and accurate decisions of vehicle behaviors. Moreover, in the presence of mobile wireless communications, the privacy and security of vehicle information are at constant risk. In this context, a new paradigm is urgently needed for various applications in dynamic vehicle environments. As a distributed machine learning technology, federated learning (FL) has received extensive attention due to its outstanding privacy protection properties and easy scalability. We conduct a comprehensive survey of the latest developments in FL for ITS. Specifically, we initially research the prevalent challenges in ITS and elucidate the motivations for applying FL from various perspectives. Subsequently, we review existing depl
    
[^57]: ModelGiF: 模型功能距离的梯度场

    ModelGiF: Gradient Fields for Model Functional Distance. (arXiv:2309.11013v1 [cs.LG])

    [http://arxiv.org/abs/2309.11013](http://arxiv.org/abs/2309.11013)

    本文介绍了一种用于测量模型功能距离的方法，称为ModelGiF。通过在异构的预训练模型中提取同质的表示，ModelGiF可通过模型之间的相似性来衡量它们之间的距离。实验证实了该方法在任务相关性估计、知识产权保护和模型遗忘验证等方面的有效性。

    

    过去十年见证了深度学习的成功和公开发布的训练模型的激增，这就需要对各种目的的模型功能距离进行量化。然而，由于内部工作的不透明性和体系结构或任务的异质性，量化模型功能距离始终具有挑战性。受物理学中“场”概念的启发，本文引入了模型梯度场（简称ModelGiF），从异构的预训练模型中提取同质的表示。我们的主要假设是，每个预训练深度模型在输入空间上唯一确定一个ModelGiF。因此，模型之间的距离可以通过它们的ModelGiF的相似性来衡量。我们使用一系列实验验证了所提出的ModelGiF的有效性，包括任务相关性估计、知识产权保护和模型遗忘验证。实验结果证明了其有效性。

    The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures or tasks. Inspired by the concept of "field" in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate th
    
[^58]: Spiking NeRF：使生物启发的神经网络穿透现实世界

    Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])

    [http://arxiv.org/abs/2309.10987](http://arxiv.org/abs/2309.10987)

    本文介绍了SpikingNeRF，它通过将辐射光线与脉冲神经网络的时间维度对齐，以节省能量并减少计算量。

    

    脉冲神经网络（SNN）在许多任务中取得了成功，利用其具有潜在生物学可行性的能量效率和潜力。与此同时，神经辐射场（NeRF）以大量能量消耗渲染高质量的3D场景，但很少有研究深入探索以生物启发的方法进行节能解决方案。本文提出了脉冲NeRF（SpikingNeRF），将辐射光线与SNN的时间维度对齐，以自然地适应SNN对辐射场的重建。因此，计算以基于脉冲、无乘法的方式进行，从而减少能量消耗。在SpikingNeRF中，光线上的每个采样点匹配到特定的时间步，并以混合方式表示，其中体素网格也得到维护。基于体素网格，确定采样点是否在训练和推断过程中被屏蔽以进行更好的处理。然而，这个操作也会产生不可逆性。

    Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
    
[^59]: GPT4是一个优秀的交易员吗？

    Is GPT4 a Good Trader?. (arXiv:2309.10982v1 [cs.AI])

    [http://arxiv.org/abs/2309.10982](http://arxiv.org/abs/2309.10982)

    本研究旨在考察GPT-4在经典交易理论理解和实际交易数据分析中的准确性和能力，以验证其作为交易员的可靠性。

    

    最近，大型语言模型（LLM），特别是GPT-4，在各种计划和推理任务中展示出了显著的能力。受到这些进展的推动，研究人员对利用GPT-4的能力进行自动化设计与现有因子库不重叠的定量因子的兴趣激增，渴望实现Alpha回报。与这些工作相反，本研究旨在考察GPT-4对经典交易理论的理解准确度以及其在真实交易数据分析中应用代码解释能力的熟练程度。这样的探索对于判断GPT-4用于交易的基本逻辑是否可靠非常重要。此外，考虑到大多数交易理论中存在的解释自由度，我们希望从GPT中提炼出更精确的方法来应用这些理论。

    Recently, large language models (LLMs), particularly GPT-4, have demonstrated significant capabilities in various planning and reasoning tasks \cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there has been a surge of interest among researchers to harness the capabilities of GPT-4 for the automated design of quantitative factors that do not overlap with existing factor libraries, with an aspiration to achieve alpha returns \cite{webpagequant}. In contrast to these work, this study aims to examine the fidelity of GPT-4's comprehension of classic trading theories and its proficiency in applying its code interpreter abilities to real-world trading data analysis. Such an exploration is instrumental in discerning whether the underlying logic GPT-4 employs for trading is intrinsically reliable. Furthermore, given the acknowledged interpretative latitude inherent in most trading theories, we seek to distill more precise methodologies of deploying these theories from GPT
    
[^60]: 基于多智能体深度强化学习的AI驱动患者监测

    AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])

    [http://arxiv.org/abs/2309.10980](http://arxiv.org/abs/2309.10980)

    本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。

    

    有效的患者监测对及时干预和改善医疗结果至关重要。传统的监测系统往往难以处理复杂、动态的环境和波动的生命体征，导致延迟发现危急情况。为了应对这一挑战，我们提出了一种新颖的基于多智能体深度强化学习（DRL）的AI驱动患者监测框架。我们的方法部署了多个学习智能体，每个智能体专门负责监测特定的生理特征，如心率、呼吸和体温。这些智能体与通用的医疗监测环境进行交互，学习患者的行为模式，并根据估计的紧急程度做出通知相应医疗紧急团队（MET）的决策。在本研究中，我们使用来自两个数据集（PPG-DaLiA和WESAD）的真实生理和运动数据评估了提出的多智能体DRL框架的性能。

    Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
    
[^61]: LMDX：基于语言模型的文档信息提取与定位

    LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])

    [http://arxiv.org/abs/2309.10952](http://arxiv.org/abs/2309.10952)

    LMDX是一种基于语言模型的文档信息提取与定位方法，克服了布局编码和答案虚构的困难，能够在半结构化文档中提取关键实体。

    

    大规模语言模型（LLM）在自然语言处理（NLP）中取得了革命性的进展，改进了许多现有任务的最新技术，并展示了新兴的能力。然而，LLM尚未成功应用于半结构化文档信息提取，这是许多文档处理工作流的核心，包括从视觉丰富的文档（VRD）中提取关键实体，给定预定义的目标模式。LLM在这个任务中的主要障碍是LLM中缺乏布局编码，这对于高质量的提取至关重要，以及缺乏一个基于理论的机制，确保答案不是虚构的。在本文中，我们介绍了一种基于语言模型的文档信息提取与定位（LMDX）的方法，用于将任意LLM适应文档信息提取。LMDX可以提取单一、重复和层次结构实体，无论是否有训练数据，并提供基于理论的保证。

    Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and lo
    
[^62]: Pir\'a 2.0的基准测试：一个关于海洋、巴西海岸和气候变化的阅读理解数据集

    Benchmarks for Pir\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. (arXiv:2309.10945v1 [cs.CL])

    [http://arxiv.org/abs/2309.10945](http://arxiv.org/abs/2309.10945)

    通过创建基准测试，我们为Pir\'a 2.0数据集定义了六个不同的问答任务的测试，可以更好地利用该数据集来评估当前机器学习模型在阅读理解方面的能力。

    

    Pir\'a是一个专注于海洋、巴西海岸和气候变化的阅读理解数据集，该数据集是从有关这些主题的科学摘要和报告的收藏中构建而成的。该数据集代表了一种通用的语言资源，特别适用于测试当前机器学习模型获取专家科学知识的能力。尽管具有潜力，但Pir\'a尚未开发出详细的基准测试。通过创建这些基准测试，研究人员可以更轻松地利用Pir\'a作为测试各种问答任务的机器学习模型的资源。在本文中，我们为Pir\'a数据集定义了六个基准测试，涵盖封闭生成问答、机器阅读理解、信息检索、开放式问答、答案触发和多项选择问答。作为这一努力的一部分，我们还制作了原始数据集的精选版本，其中修正了一些语法问题。

    Pir\'a is a reading comprehension dataset focused on the ocean, the Brazilian coast, and climate change, built from a collection of scientific abstracts and reports on these topics. This dataset represents a versatile language resource, particularly useful for testing the ability of current machine learning models to acquire expert scientific knowledge. Despite its potential, a detailed set of baselines has not yet been developed for Pir\'a. By creating these baselines, researchers can more easily utilize Pir\'a as a resource for testing machine learning models across a wide range of question answering tasks. In this paper, we define six benchmarks over the Pir\'a dataset, covering closed generative question answering, machine reading comprehension, information retrieval, open question answering, answer triggering, and multiple choice question answering. As part of this effort, we have also produced a curated version of the original dataset, where we fixed a number of grammar issues, r
    
[^63]: 使用大型语言模型的端到端语音识别上下文化

    End-to-End Speech Recognition Contextualization with Large Language Models. (arXiv:2309.10917v1 [eess.AS])

    [http://arxiv.org/abs/2309.10917](http://arxiv.org/abs/2309.10917)

    本论文介绍了一种在语音识别中使用大型语言模型进行上下文化的新方法。通过整合预训练的语言模型，我们的方法在训练过程中通过提供音频特征和文本上下文，使系统隐含地学习如何利用上下文信息，并达到了显著的性能改善。

    

    近年来，大型语言模型（LLMs）由于其出色的性能和泛化能力而受到研究界的广泛关注。本文介绍了一种新的方法，用于在语音识别模型中融入LLMs进行上下文化。我们的方法将语音识别视为基于预训练LLM的混合模态语言建模任务。我们提供音频特征以及可选的文本标记来训练系统以解码方式完成转录。因此，在训练过程中，系统会隐含地学习如何利用非结构化的上下文信息。我们的实证结果表明性能显著提高，当提供额外的文本上下文时，词错误率（WER）降低了6%。此外，我们发现我们的方法在竞争中表现良好，并在整体上将WER提高了7.5%，对于罕见词语的WER提高了17%，相较于基准上下文化RNN-T系统的训练结果。

    In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on
    
[^64]: 通过跨数据集迁移学习来放大脑电信号通路中的病理检测

    Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning. (arXiv:2309.10910v1 [cs.LG])

    [http://arxiv.org/abs/2309.10910](http://arxiv.org/abs/2309.10910)

    该研究通过跨数据集迁移学习来放大脑电信号通路中的病理检测，以提高病理诊断的准确性和可行性，解决了标注数据稀缺性和低成本招募真实患者队列的问题。

    

    基于脑电信号和脑活动解码的病理诊断在理解神经系统疾病方面具有重要意义。随着人工智能方法和机器学习技术的进步，准确的数据驱动诊断和有效的治疗的潜力显著增长。然而，将机器学习算法应用于真实世界的数据集在多个层面上都存在各种挑战。标注数据的稀缺性，特别是在低范围情景下，由于高昂的招募成本导致真实患者队列有限，凸显了扩展和迁移学习技术的重要性。在这项研究中，我们探讨了一个真实的病理分类任务，以凸显数据和模型扩展以及跨数据集知识迁移的有效性。因此，我们观察到通过数据扩展可以获得不同程度的性能改进，表明需要进行仔细评估和标注。

    Pathology diagnosis based on EEG signals and decoding brain activity holds immense importance in understanding neurological disorders. With the advancement of artificial intelligence methods and machine learning techniques, the potential for accurate data-driven diagnoses and effective treatments has grown significantly. However, applying machine learning algorithms to real-world datasets presents diverse challenges at multiple levels. The scarcity of labelled data, especially in low regime scenarios with limited availability of real patient cohorts due to high costs of recruitment, underscores the vital deployment of scaling and transfer learning techniques. In this study, we explore a real-world pathology classification task to highlight the effectiveness of data and model scaling and cross-dataset knowledge transfer. As such, we observe varying performance improvements through data scaling, indicating the need for careful evaluation and labelling. Additionally, we identify the chall
    
[^65]: 多副本强化学习智能体

    Multicopy Reinforcement Learning Agents. (arXiv:2309.10908v1 [cs.MA])

    [http://arxiv.org/abs/2309.10908](http://arxiv.org/abs/2309.10908)

    本文研究了一种新型的多智能体问题，在这个问题中，智能体制作多个相同副本来更好地完成任务。通过利用价值函数的结构，提出了一种学习算法来平衡添加额外副本的优势和成本。

    

    本文研究了一种新型的多智能体问题，其中一个智能体通过制作多个相同副本来更好或更高效地完成单个智能体任务。如果环境嘈杂，并且单个智能体副本有时无法完成任务，则这种策略可以提高性能。我们提出了一种用于解决多副本问题的学习算法，该算法利用价值函数的结构，有效地学习如何平衡添加额外副本的优势和成本。

    This paper examines a novel type of multi-agent problem, in which an agent makes multiple identical copies of itself in order to achieve a single agent task better or more efficiently. This strategy improves performance if the environment is noisy and the task is sometimes unachievable by a single agent copy. We propose a learning algorithm for this multicopy problem which takes advantage of the structure of the value function to efficiently learn how to balance the advantages and costs of adding additional copies.
    
[^66]: 人工智能助力个性化和自适应学习的智能助手在高等教育中的应用

    Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education. (arXiv:2309.10892v1 [cs.AI])

    [http://arxiv.org/abs/2309.10892](http://arxiv.org/abs/2309.10892)

    该论文介绍了一个名为人工智能助力的智能助手（AIIA）的框架，用于高等教育中的个性化和自适应学习。该系统利用先进的AI和自然语言处理技术，提供交互和引人入胜的学习平台，通过简化信息获取、促进知识评估和个性化学习支持来降低学习者的认知负荷。其功能包括理解和回答学生问题、生成测验和闪卡，并提供个性化学习路径。该研究结果可对高等教育中人工智能虚拟助教（VTA）的设计、实施和评估产生重要影响，从而提高学生的学习成果、参与度和满意度。

    

    本文提出了一个新颖的框架，即人工智能助力的智能助手(AIIA)，用于在高等教育中进行个性化和自适应学习。 AIIA系统利用先进的人工智能和自然语言处理技术创建了一个交互性和引人入胜的学习平台。该平台旨在通过提供信息的简单获取、促进知识评估和提供个性化学习支持来减少学习者的认知负荷，以适应个体需求和学习风格。 AIIA的功能包括理解和回答学生的问题、生成测验和闪卡，以及提供个性化的学习路径。该研究结果有可能对高等教育中基于人工智能的虚拟教学助手(VTA)的设计、实施和评估产生重大影响，为开发创新的教育工具提供指导，以增强学生的学习成果、参与度和满意度。

    This paper presents a novel framework, Artificial Intelligence-Enabled Intelligent Assistant (AIIA), for personalized and adaptive learning in higher education. The AIIA system leverages advanced AI and Natural Language Processing (NLP) techniques to create an interactive and engaging learning platform. This platform is engineered to reduce cognitive load on learners by providing easy access to information, facilitating knowledge assessment, and delivering personalized learning support tailored to individual needs and learning styles. The AIIA's capabilities include understanding and responding to student inquiries, generating quizzes and flashcards, and offering personalized learning pathways. The research findings have the potential to significantly impact the design, implementation, and evaluation of AI-enabled Virtual Teaching Assistants (VTAs) in higher education, informing the development of innovative educational tools that can enhance student learning outcomes, engagement, and 
    
[^67]: 自我增强改进了零样本跨语言传递

    Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer. (arXiv:2309.10891v1 [cs.CL])

    [http://arxiv.org/abs/2309.10891](http://arxiv.org/abs/2309.10891)

    本文提出了一种名为SALT的方法，通过结合代码切换和嵌入混合与自我增强，有效地提高了多语言预训练模型的零样本跨语言传递能力。

    

    零样本跨语言传递是多语言自然语言处理的核心任务，允许在具有更充足训练资源的语言中训练的模型推广到其他资源匮乏的语言。先前在这个任务上的努力使用平行语料库、双语词典或其他标注对齐数据来提高跨语言传递能力，这些通常是昂贵的获取方式。在本文中，我们提出了一种简单而有效的方法SALT，用于改进多语言预训练语言模型的零样本跨语言传递，而不需要这些外部数据的帮助。通过结合代码切换和嵌入混合与自我增强，SALT有效地蒸馏了多语言PLM的跨语言知识，并增强了其在下游任务中的传递能力。在XNLI和PAWS-X上的实验结果表明，我们的方法可以在没有外部数据的情况下提高零样本跨语言传递能力。我们的代码可以在https://github.com/luka-group/SALT找到。

    Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. Our code is available at https://github.com/luka-group/SALT.
    
[^68]: 使用自然语言处理对食物系统本体进行组织分类

    Classifying Organizations for Food System Ontologies using Natural Language Processing. (arXiv:2309.10880v1 [cs.CL])

    [http://arxiv.org/abs/2309.10880](http://arxiv.org/abs/2309.10880)

    本研究使用自然语言处理方法自动对食物系统本体中的组织进行分类，研究结果表明NLP模型可以在这两个分类任务中取得良好性能。

    

    我们的研究探索了使用自然语言处理（NLP）方法自动对实体进行分类，以达到知识图谱的构建和与食物系统本体的集成的目的。我们创建了能够自动将组织根据与环境问题相关的类别以及美国政府用于描述商业活动的标准产业分类（SIC）代码进行分类的NLP模型。NLP模型的输入为每个组织通过Google搜索引擎检索到的文本片段，该文本片段用作用于学习的组织的文本描述。我们的实验结果显示，NLP模型可以在这两个分类任务中实现相当好的性能，并且它们依赖于一个通用框架，该框架也可以应用于许多其他分类问题。我们相信，NLP模型代表了一种有前景的方法，可以自动收集信息。

    Our research explores the use of natural language processing (NLP) methods to automatically classify entities for the purpose of knowledge graph population and integration with food system ontologies. We have created NLP models that can automatically classify organizations with respect to categories associated with environmental issues as well as Standard Industrial Classification (SIC) codes, which are used by the U.S. government to characterize business activities. As input, the NLP models are provided with text snippets retrieved by the Google search engine for each organization, which serves as a textual description of the organization that is used for learning. Our experimental results show that NLP models can achieve reasonably good performance for these two classification tasks, and they rely on a general framework that could be applied to many other classification problems as well. We believe that NLP models represent a promising approach for automatically harvesting informatio
    
[^69]: 基于分散迭代规划的可信赖Minecraft定居点

    Believable Minecraft Settlements by Means of Decentralised Iterative Planning. (arXiv:2309.10871v1 [cs.AI])

    [http://arxiv.org/abs/2309.10871](http://arxiv.org/abs/2309.10871)

    通过分散的、迭代的规划过程，我们的方法在Minecraft定居点生成竞赛中获胜，该方法可用于产生逼真且可信赖的程序化城市内容。

    

    在过程内容生成（PCG）领域中，以可信度和适应性为重点的程序化城市生成是一个具有挑战性的任务。在像Minecraft中的生成定居设计（GDMC）这样的挑战中，数十位研究人员竞争提出现实的方法，而我们的方法在2022年的比赛中获胜。这通过一种分散的、迭代的规划过程实现，该过程可在类似的生成过程中转移，以产生程序化的“有机”内容。

    Procedural city generation that focuses on believability and adaptability to random terrain is a difficult challenge in the field of Procedural Content Generation (PCG). Dozens of researchers compete for a realistic approach in challenges such as the Generative Settlement Design in Minecraft (GDMC), in which our method has won the 2022 competition. This was achieved through a decentralised, iterative planning process that is transferable to similar generation processes that aims to produce "organic" content procedurally.
    
[^70]: 使用AI不确定性量化改进人类决策

    Using AI Uncertainty Quantification to Improve Human Decision-Making. (arXiv:2309.10852v1 [cs.AI])

    [http://arxiv.org/abs/2309.10852](http://arxiv.org/abs/2309.10852)

    本论文研究了使用AI不确定性量化改进人类决策的方法，并通过基于实例的UQ和行为实验验证了其性能优势。

    

    AI不确定性量化（UQ）有可能通过为用户提供有用的概率信息，超越单纯的AI预测，从而改进人类决策。过去的大部分关于AI和人类决策的研究都集中在模型可解释性和可解释性上。我们运用基于实例的UQ在三个真实数据集上实现了这一目标。为了实现这一目标，我们为每个数据集训练了不同的分类AI模型，并使用在给定实例附近生成的随机样本创建了UQ的置信区间。我们使用严格的适当评分规则对计算出的UQ进行了校准，作为UQ的质量保证。然后，我们进行了两个预先注册的在线行为实验，比较了不同AI信息条件下客观人类决策表现，包括UQ。在实验1中，我们比较了不使用AI（对照组）、仅使用AI预测和使用AI预测及UQ可视化的决策。我们发现UQ的使用可以提高人类决策的性能。

    AI Uncertainty Quantification (UQ) has the potential to improve human decision-making beyond AI predictions alone by providing additional useful probabilistic information to users. The majority of past research on AI and human decision-making has concentrated on model explainability and interpretability. We implemented instance-based UQ for three real datasets. To achieve this, we trained different AI models for classification for each dataset, and used random samples generated around the neighborhood of the given instance to create confidence intervals for UQ. The computed UQ was calibrated using a strictly proper scoring rule as a form of quality assurance for UQ. We then conducted two preregistered online behavioral experiments that compared objective human decision-making performance under different AI information conditions, including UQ. In Experiment 1, we compared decision-making for no AI (control), AI prediction alone, and AI prediction with a visualization of UQ. We found UQ
    
[^71]: 天气和气候的AI基础模型：应用、设计和实施

    AI Foundation Models for Weather and Climate: Applications, Design, and Implementation. (arXiv:2309.10808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.10808](http://arxiv.org/abs/2309.10808)

    该论文介绍了AI基础模型在天气和气候中的应用，主要讨论了使用transformers、物理知识启发的机器学习和图神经网络的最新方法。尽管取得了进展，但全球地球系统模型的可推广AI模型仍处于初级阶段。

    

    机器学习和深度学习方法在理解大气的混沌行为和推进天气预报方面得到了广泛探索。科技公司、政府机构和气象机构对建立地球的数字孪生体表现出越来越大的兴趣。最近使用transformers、受物理知识启发的机器学习和图神经网络的方法在相对狭窄的时空范围和特定任务上展示了最先进的性能。随着使用预训练的transformers进行语言建模和视觉生成人工智能的最近成功，我们正在朝着可推广的人工智能模型迈进。特别是，我们正在见证能够在多个领域特定下游任务上有竞争力的AI基础模型的崛起。尽管取得了进展，但全球地球系统模型的可推广AI模型仍处于初级阶段。

    Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models
    
[^72]: MelodyGLM: 音乐符号旋律生成的多任务预训练

    MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation. (arXiv:2309.10738v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2309.10738](http://arxiv.org/abs/2309.10738)

    提出了一种名为MelodyGLM的多任务预训练框架，用于生成具有长期结构的旋律。该框架通过设计音乐n-gram和长跨度抽样策略来捕捉旋律的局部和全局结构，并使用大规模符号旋律数据集进行预训练改进。

    

    预训练语言模型在各种音乐理解和生成任务中取得了令人印象深刻的结果。然而，现有的用于符号旋律生成的预训练方法在捕捉音符序列中的多尺度、多维结构信息方面存在困难，这是由于文本和音乐之间领域知识差异的缘故。此外，可用大规模符号旋律数据集的缺乏限制了预训练的改进。在本文中，我们提出了MelodyGLM，这是一个用于生成具有长期结构旋律的多任务预训练框架。我们设计了音乐n-gram和长跨度抽样策略，为旋律的局部和全局结构建立了局部和全局空白填充任务，以进行建模。具体而言，我们将音高n-gram、节奏n-gram及其组合的n-gram纳入音乐n-gram空白填充任务中，以建模旋律的多维结构。为此，我们构建了一个大规模符号旋律数据集。

    Pre-trained language models have achieved impressive results in various music understanding and generation tasks. However, existing pre-training methods for symbolic melody generation struggle to capture multi-scale, multi-dimensional structural information in note sequences, due to the domain knowledge discrepancy between text and music. Moreover, the lack of available large-scale symbolic melody datasets limits the pre-training improvement. In this paper, we propose MelodyGLM, a multi-task pre-training framework for generating melodies with long-term structure. We design the melodic n-gram and long span sampling strategies to create local and global blank infilling tasks for modeling the local and global structures in melodies. Specifically, we incorporate pitch n-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram blank infilling tasks for modeling the multi-dimensional structures in melodies. To this end, we have constructed a large-scale symbolic melody datas
    
[^73]: NusaWrites: 为被低估和极度资源匮乏的语言构建高质量语料库

    NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages. (arXiv:2309.10661v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10661](http://arxiv.org/abs/2309.10661)

    NusaWrites项目通过母语者段落撰写构建高质量语料库，弥补了在线爬取和翻译文档所带来的词汇多样性和文化相关性的限制。该研究在印度尼西亚地方语言上进行，并提出了“datasetname”基准，涵盖了12种被低估和极度资源匮乏的语言。

    

    民主化自然语言处理（NLP）技术的访问至关重要，特别是对于被低估和极度资源匮乏的语言。以往的研究侧重于通过在线爬取和文件翻译为这些语言开发带标签和无标签的语料库。尽管这些方法已经被证明是有效和费用效益的，但我们发现所得到的语料库存在一些限制，包括缺乏词汇多样性和与当地社区的文化相关性。为了解决这一差距，我们在印度尼西亚的地方语言上进行了案例研究。我们比较了在线爬取、人工翻译和母语者段落撰写在构建数据集方面的效果。我们的研究结果表明，通过母语者段落撰写生成的数据集在词汇多样性和文化内容方面具有更高的质量。此外，我们还提出了“datasetname ”基准，涵盖了12个被低估和极度资源匮乏的语言。

    Democratizing access to natural language processing (NLP) technology is crucial, especially for underrepresented and extremely low-resource languages. Previous research has focused on developing labeled and unlabeled corpora for these languages through online scraping and document translation. While these methods have proven effective and cost-efficient, we have identified limitations in the resulting corpora, including a lack of lexical diversity and cultural relevance to local communities. To address this gap, we conduct a case study on Indonesian local languages. We compare the effectiveness of online scraping, human translation, and paragraph writing by native speakers in constructing datasets. Our findings demonstrate that datasets generated through paragraph writing by native speakers exhibit superior quality in terms of lexical diversity and cultural content. In addition, we present the \datasetname{} benchmark, encompassing 12 underrepresented and extremely low-resource languag
    
[^74]: PDRL：基于多智能体强化学习的预测监控

    PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])

    [http://arxiv.org/abs/2309.10576](http://arxiv.org/abs/2309.10576)

    本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。

    

    强化学习因其能够从以往经验中学习并做出自适应决策的能力，在监控应用中越来越被应用。然而，现有的基于机器学习的健康监控应用大多是基于监督学习算法，训练标签数据，无法在不确定的复杂环境中做出自适应决策。本研究提出了一种新颖的通用系统，即具有多个强化学习智能体的预测深度强化学习（PDRL），应用于时间序列预测环境。该提出的通用框架可以容纳虚拟深度 Q 网络（DQN）智能体，以监测复杂环境的预测未来状态，并根据明确定义的奖励策略使智能体在最大化奖励的同时学习现有知识。在评估该框架的过程中，部署了三个强化学习智能体以监测通过 BiLSTM 模型预测的受试者未来的心率、呼吸率和体温。随着每次迭代，智能体根据实际反馈调整其行为策略。

    Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
    
[^75]: 通过正则表达式指令实现统一可控文本生成

    Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v1 [cs.CL])

    [http://arxiv.org/abs/2309.10447](http://arxiv.org/abs/2309.10447)

    本文通过引入正则表达式指令（REI）实现了统一可控文本生成，通过指令方式支持各种约束，无需对架构进行修改，并对各种约束组合表现出良好的性能。

    

    可控文本生成是自然语言生成的基本方面之一，已经提出了许多针对不同约束类型的方法。然而，这些方法往往需要重大的架构或解码修改，使得它们难以应用于附加约束或解决不同约束组合。为了解决这个问题，本文引入了正则表达式指令（REI），利用基于指令的机制充分利用正则表达式的优势，统一建模各种约束。具体而言，我们的REI通过正则表达式风格的指令支持所有流行的细粒度可控生成约束，即词汇、位置和长度，以及它们的复杂组合。我们的方法只需要在中等规模语言模型上进行微调或在大规模语言模型上进行少样本、上下文学习，并且在应用于各种约束组合时不需要进一步调整。实验证明了我们方法的有效性。

    Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demon
    
[^76]: QASnowball: 一个用于高质量问答数据生成的迭代自举框架

    QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])

    [http://arxiv.org/abs/2309.10326](http://arxiv.org/abs/2309.10326)

    QASnowball是一个迭代自举框架，可以根据有监督的样本种子集生成大规模高质量的QA数据，并通过重新种子化进行自我增强。在高资源英文场景中进行了实验。

    

    近年来，问题回答（QA）取得了成功，特别是其在应对各种自然语言处理任务中的潜力。然而，获取足够的数据来构建一个有效稳定的QA系统仍然是一个未解决的问题。针对这个问题，我们引入了一个迭代自举框架QASnowball，用于QA数据增强，它可以根据有监督的样本种子集迭代地生成大规模高质量的QA数据。具体而言，QASnowball包括三个模块：回答提取器，用于从无标签的文档中提取候选答案的核心短语；问题生成器，根据文档和候选答案生成问题；QA数据过滤器，用于过滤出高质量的QA数据。此外，QASnowball可以通过重新种子化种子集在不同迭代中进行自我增强，从而不断提高生成质量。我们在高资源英文场景中进行了实验。

    Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario
    
[^77]: QXAI：可解释的人工智能框架用于患者监测系统中的定量分析

    QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems. (arXiv:2309.10293v1 [cs.AI])

    [http://arxiv.org/abs/2309.10293](http://arxiv.org/abs/2309.10293)

    该论文提出了一种QXAI框架，用于患者监测系统中的定量分析。该框架具有可解释性，对于决策者在医疗应用中基于非线性模型结果进行决策非常重要。

    

    人工智能技术可用于对患者的身体活动进行分类和预测远程患者监测的生命体征。基于非线性模型（如深度学习模型）的回归分析由于其黑盒性质而具有有限的可解释性。这可能需要决策者在医疗应用中基于非线性模型结果盲目决策。在非侵入式监测中，跟踪传感器获取的患者数据和其相关临床特征作为预测未来生命体征的输入特征。解释各种特征对监测应用整体输出的贡献对于临床医师的决策至关重要。本研究提出了一种QXAI框架，用于监督学习方法中回归和分类任务的事后模型可解释性和内在可解释性。这是通过利用Shapley值进行实现的。

    Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values c
    
[^78]: Q-Transformer：通过自回归Q-函数提供可扩展的离线强化学习

    Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])

    [http://arxiv.org/abs/2309.10150](http://arxiv.org/abs/2309.10150)

    Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。

    

    在这项工作中，我们提出了一种可扩展的强化学习方法，用于训练可以利用人类演示和自主采集数据的大型离线数据集的多任务策略。我们的方法使用Transformer提供可扩展的Q函数表示，通过离线时差备份进行训练。因此，我们将该方法称为Q-Transformer。通过将每个动作维度进行离散化，并将每个动作维度的Q值表示为单独的标记，我们可以应用高容量序列建模技术进行Q学习。我们提出了几个设计决策，使其在离线RL训练中表现出良好性能，并展示了Q-Transformer在大规模多样化的真实世界机器人操作任务套件上优于以往的离线RL算法和模仿学习技术。该项目的网站和视频可以在https://q-transformer.github.io找到。

    In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
    
[^79]: 上下文≈环境。

    Context $\approx$ Environment. (arXiv:2309.09888v1 [cs.LG])

    [http://arxiv.org/abs/2309.09888](http://arxiv.org/abs/2309.09888)

    在这篇论文中，作者通过理论与实验证明了将注意力放在上下文-未标记样本上，可以实现更好的领域泛化。

    

    AI研究的中心在于两个方面。一方面，社区正在努力构建能够丢弃虚假相关性并在新颖的测试环境中更好地进行泛化的模型。不幸的是，到目前为止，没有任何提案能够令人信服地超越简单的经验风险最小化基线。另一方面，大型语言模型(LLMs)已经成为能够在上下文中学习、根据用户通过提示施加的多种上下文背景灵活泛化的算法。本文认为上下文≈环境，并假设在上下文学习中隐藏着更好的领域泛化之钥。通过广泛的理论与实验，我们展示了注意上下文-未标记的样本的重要性，这种注意可以使我们提出的In-Context Risk Minimization (ICRM)算法聚焦于测试环境风险最小化。

    Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to the eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context $\approx$ environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\unicode{x2013}\unicode{x2013}$unlabeled examples as they arrive$\unicode{x2013}\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, le
    
[^80]: 一个用于运输机器人调度问题的量子优化案例研究

    A Quantum Optimization Case Study for a Transport Robot Scheduling Problem. (arXiv:2309.09736v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2309.09736](http://arxiv.org/abs/2309.09736)

    本研究通过比较D-Waves的量子-经典混合框架、富士通的量子启发式数字退火器和Gurobi的经典求解器的性能，提供了解决运输机器人调度问题的指导，发现数字退火器有希望的结果，并为混合量子退火器提供了一些机会。

    

    本文提出了一个综合的案例研究，比较了D-Waves的量子-经典混合框架、富士通的量子启发式数字退火器和Gurobi的最先进经典求解器在解决运输机器人调度问题方面的性能。这个问题源于一个具有工业相关性的现实场景。我们为我们的问题提供了三种不同的模型，采用不同的设计理念。在我们的基准测试中，我们关注不同模型和求解器组合的解决方案质量和端到端运行时间。我们发现，与Gurobi直接比较，数字退火器有希望的结果，并为混合量子退火器提供了一些机会。我们的研究提供了有关使用不同策略解决应用导向优化问题的工作流程的见解，并可以用于评估不同方法的优势和劣势。

    We present a comprehensive case study comparing the performance of D-Waves' quantum-classical hybrid framework, Fujitsu's quantum-inspired digital annealer, and Gurobi's state-of-the-art classical solver in solving a transport robot scheduling problem. This problem originates from an industrially relevant real-world scenario. We provide three different models for our problem following different design philosophies. In our benchmark, we focus on the solution quality and end-to-end runtime of different model and solver combinations. We find promising results for the digital annealer and some opportunities for the hybrid quantum annealer in direct comparison with Gurobi. Our study provides insights into the workflow for solving an application-oriented optimization problem with different strategies, and can be useful for evaluating the strengths and weaknesses of different approaches.
    
[^81]: 因果故事：利用参数高效调整的局部因果注意力实现视觉故事合成

    Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis. (arXiv:2309.09553v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09553](http://arxiv.org/abs/2309.09553)

    提出了一种称为因果故事的新模型，利用局部因果注意力机制来改进视觉故事合成的全局一致性，该模型考虑了历史标题、帧和当前标题之间的因果关系，实现了更好的生成效果。

    

    演化模型在文本到图像合成方面具有出色的能力，推动了连贯视觉故事的合成进展。目前最先进的方法将历史标题、历史帧和当前标题的特征作为生成当前帧的条件进行组合。然而，该方法将每个历史帧和标题都视为同样的贡献，并以相等的权重将它们连接起来，忽视了并非所有历史条件都与生成当前帧相关。为了解决这个问题，我们提出了因果故事。该模型引入了一种考虑先前标题、帧和当前标题之间因果关系的局部因果注意机制。通过根据这种关系分配权重，因果故事生成当前帧，从而提高了故事生成的全局一致性。我们在PororoSV和FlintstonesSV数据集上评估了我们的模型，并获得了最先进的FID分数。

    The excellent text-to-image synthesis capability of diffusion models has driven progress in synthesizing coherent visual stories. The current state-of-the-art method combines the features of historical captions, historical frames, and the current captions as conditions for generating the current frame. However, this method treats each historical frame and caption as the same contribution. It connects them in order with equal weights, ignoring that not all historical conditions are associated with the generation of the current frame. To address this issue, we propose Causal-Story. This model incorporates a local causal attention mechanism that considers the causal relationship between previous captions, frames, and current captions. By assigning weights based on this relationship, Causal-Story generates the current frame, thereby improving the global consistency of story generation. We evaluated our model on the PororoSV and FlintstonesSV datasets and obtained state-of-the-art FID score
    
[^82]: 使用大型语言模型解决和解释物理词问题接近人类水平

    Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v1 [cs.CL])

    [http://arxiv.org/abs/2309.08182](http://arxiv.org/abs/2309.08182)

    本研究证明，使用大型语言模型(如GPT3.5)可以解决和解释物理词问题，通过对物理知识进行计算和推理，实现了接近人类水平的解决率。此外，该模型还能够总结涉及的知识、生成解释，并创造新的物理词问题。

    

    我们的工作表明，基于文本预训练的大型语言模型(LLM)不仅可以解决纯数学题，还可以解决物理词问题-即基于先前的物理知识进行计算和推理的问题。我们收集并注释了第一个物理词问题数据集-PhysQA，其中包含超过1000个初中物理词问题（包括运动学、质量和密度、力学、热学和电学）。然后我们使用OpenAI的GPT3.5来生成这些问题的答案，发现GPT3.5可以在零样本学习上自动解决49.3%的问题，在少样本学习上则为73.2%。这个结果表明，通过使用类似问题及其答案作为提示，LLM可以解决接近人类水平的基础物理词问题。除了自动解决问题，GPT3.5还可以总结问题涉及的知识或主题，生成相关解释，并根据输入问题综合出新的物理词问题。

    Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems-problems to be solved by calculation and inference based on some prior physical knowledge. We collect and annotate the first physics word problem dataset-PhysQA, which contains over 1000 junior high school physics word problems (on Kinematics, Mass&Density, Mechanics, Heat, Electricity). Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found that GPT3.5 could automatically solve 49.3% of the problems on zero-shot learning and 73.2% on few-shot learning. This result show that by using similar problem and its answer as prompt, LLM could solve elementary physics word problems approaching human level. Besides automatically solving problems, GPT3.5 could also summarize the knowledge or topic examined by the problem, generate the relevant explanation, and synthesis new physics word problems according tothe input problem
    
[^83]: 使用离线强化学习的全向腿式机器人在室外植被中进行导航的VAPOR方法

    VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning. (arXiv:2309.07832v1 [cs.RO])

    [http://arxiv.org/abs/2309.07832](http://arxiv.org/abs/2309.07832)

    VAPOR是一种使用离线强化学习的方法，用于在室外复杂植被环境中进行全向腿式机器人的自主导航。通过从未标记的真实植被数据中训练，该方法学习了植被的物理和几何特性，并使用一个自适应规划器生成动态可行机器人动作，在狭窄通道中导航，并避免被高草和灌木丛所困住。在实验中，我们观察到成功率有所提高。

    

    我们提出了一种名为VAPOR的方法，该方法使用离线强化学习（RL）在结构混乱、密集植被的室外环境中实现全向腿式机器人的自主导航。该方法使用从三维LiDAR点云导出的高度和强度基于成本地图、目标成本地图以及经过处理的固有感知数据作为状态输入，从未标记的实际室外植被数据中训练出一种新的RL策略。该策略学习了周围植被的物理和几何特性，如高度、密度和坚实度/刚性，以进行导航。与使用端到端策略动作不同，完全训练好的RL策略的Q网络用于评估由一种新的自适应规划器生成的动态可行机器人动作，该规划器能够穿过狭窄的通道并防止被植被，例如高草和灌木丛所困住。我们在复杂的室外植被上展示了该方法的能力。我们观察到成功率有所提高。

    We present VAPOR, a novel method for autonomous legged robot navigation in unstructured, densely vegetated outdoor environments using Offline Reinforcement Learning (RL). Our method trains a novel RL policy from unlabeled data collected in real outdoor vegetation. This policy uses height and intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map, and processed proprioception data as state inputs, and learns the physical and geometric properties of the surrounding vegetation such as height, density, and solidity/stiffness for navigation. Instead of using end-to-end policy actions, the fully-trained RL policy's Q network is used to evaluate dynamically feasible robot actions generated from a novel adaptive planner capable of navigating through dense narrow passages and preventing entrapment in vegetation such as tall grass and bushes. We demonstrate our method's capabilities on a legged robot in complex outdoor vegetation. We observe an improvement in success rates
    
[^84]: REFORMS: 基于机器学习的科学报告标准

    REFORMS: Reporting Standards for Machine Learning Based Science. (arXiv:2308.07832v1 [cs.LG])

    [http://arxiv.org/abs/2308.07832](http://arxiv.org/abs/2308.07832)

    REFORMS是一个基于机器学习的科学报告标准，旨在解决使用机器学习方法在科学研究中出现的有效性、可重复性和可推广性失败问题。这个标准由32个问题和一套指导方针组成，可作为研究人员设计和实施科研时的参考资源。

    

    机器学习方法在科学研究中得到了广泛应用。然而，这些方法的采用也伴随着有效性、可重复性和可推广性的失败。这些失败可能会阻碍科学进展，导致对无效结论的错误共识，并削弱基于机器学习的科学的可信度。机器学习方法在不同学科中常常以相似的方式应用且失败。出于这个观察，我们的目标是为基于机器学习的科学提供清晰的报告标准。基于对过去文献的广泛评论，我们提出了REFORMS检查表（$\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience）。它由32个问题和一套配套的指导方针组成。REFORMS是基于19位研究人员的共识开发的，这些人来自计算机科学、数据科学、数学、社会科学和生物医学科学领域。REFORMS可以为研究人员在设计和实施科研时提供参考。

    Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing 
    
[^85]: 解决医学影像深度学习中的小型注释数据集问题：对比共同对比学习和掩码自编码器方法在CT扫描卷积模型中的自监督预训练的评估

    Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])

    [http://arxiv.org/abs/2308.06534](http://arxiv.org/abs/2308.06534)

    本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。

    

    医学影像中的深度学习有潜力减少诊断错误的风险、减轻放射科医生的工作负担并加速确诊。训练这样的深度学习模型需要大型且准确的数据集，并且需要为所有训练样本提供注释。然而，在医学影像领域，由于注释的高复杂性、受限的获取方式或疾病的罕见性，特定任务的注释数据集通常很小。为了应对这一挑战，深度学习模型可以使用自监督学习领域的方法，在没有注释的大型图像数据集上进行预训练。在预训练之后，小型的已注释数据集就足以对模型进行特定任务的微调，即所谓的“下游任务”。医学影像中最流行的自监督预训练方法基于共同对比学习。然而，最近的自然图像处理研究表明掩码自编码器方法具有很大的潜力。本研究比较了二者在CT扫描卷积模型中的性能。

    Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
    
[^86]: EmotionPrompt: 通过情感刺激提升大型语言模型的关键心理学方法

    EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])

    [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)

    EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。

    

    大型语言模型（LLMs）在推理、语言理解和数学问题解决等许多领域取得了显著的性能，并被视为人工通用智能（AGI）的关键步骤。然而，LLMs对提示的敏感性仍然是其日常应用的主要瓶颈。本文从心理学中汲取灵感，提出了EmotionPrompt来探索情感智能以提升LLMs的性能。EmotionPrompt基于一个非常简单明了的原则：将情感刺激融入到提示中。实验结果表明，我们的方法在相同的单一提示模板上，与原始的零样本提示和Zero-shot-CoT相比，在8个任务上都显著优于多种模型：ChatGPT、Vicuna-13b、Bloom和T5。此外，观察到EmotionPrompt能够提高真实性和信息量。我们相信EmotionPrompt为探索跨学科知识开辟了一条新的道路。

    Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
    
[^87]: 评估社交机器人导航算法的原则与指南

    Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])

    [http://arxiv.org/abs/2306.16740](http://arxiv.org/abs/2306.16740)

    本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。

    

    在人类居住环境中导航是部署机器人广泛应用的主要挑战，通常被称为社交机器人导航。虽然社交导航领域近年来取得了很大进展，但评估解决社交导航的算法仍然困难，因为它不仅涉及机器人在静态环境中移动，还涉及到动态的人类参与者及其对机器人行为的感知适应性。相比之下，清晰、可重复、易于获得的基准在计算机视觉、自然语言处理和传统机器人导航等领域加速了进展，使研究人员能够公平比较算法，揭示现有解决方案的局限性，并呈现有前途的新方向。我们相信相同的方法可以有助于社交导航。在本文中，我们为评估社交机器人导航建立了共同、广泛可用且可重复的基准标准，并提出了自己的创新点。

    A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
    
[^88]: 在可解释的科学文献推荐系统中采用不同细节级别的交互式解释

    Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System. (arXiv:2306.05809v1 [cs.IR])

    [http://arxiv.org/abs/2306.05809](http://arxiv.org/abs/2306.05809)

    本文旨在采用以用户为中心的交互式解释模型，在推荐系统中为用户提供不同细节级别的解释，赋予用户个性化解释的能力。

    

    传统上，可解释的推荐系统采用一种“一刀切”的方法，向每个用户提供相同程度的解释，而不考虑他们的个体需求和目标。此外，推荐系统中的解释大多以静态和非交互方式呈现。为填补这些研究空白，本文旨在采用以用户为中心的交互式解释模型，为用户提供不同细节级别的解释，并赋予用户基于其需求和偏好进行交互、控制和个性化解释的能力。我们采用以用户为中心的方法，设计了三个细节级别的交互式解释（基本、中级和高级），并在透明的推荐和兴趣建模应用（RIMA）中实现了它们。我们进行了一个定性用户研究（N=14），以调查提供不同细节级别的交互式解释对用户对系统可解释性的感知的影响。

    Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the e
    
[^89]: 自适应基于梯度的异常值去除的嘈杂标签学习方法

    Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])

    [http://arxiv.org/abs/2306.04502](http://arxiv.org/abs/2306.04502)

    本文提出了一种名为AGRA的自适应梯度异常值去除方法，能够在模型训练过程中动态调整数据集从而有效提高模型学习效果。

    

    训练可靠和高性能模型需要准确和丰富的数据集，但即便是人工标注的数据集也会包含错误，更不用说自动标注的数据集了。现有的一些数据去噪方法主要集中于检测异常值并进行永久性去除，但这种方法很容易过度或者欠度过滤数据集。在本论文中，我们提出了一种新的自适应梯度异常值去除方法（AGRA），不同于在模型训练之前清洗数据集，我们的方法在训练过程中动态调整数据集。通过比较一组样本的累积梯度和单个样本的梯度，我们的方法可以决定是否在当前更新时保留对应的样本，以此来确定它是否有助于模型的学习效果。在多个数据集上进行的广泛评估表明，AGRA方法的有效性，并且全面的结果分析证实了我们方法的理论和实践收益。

    An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
    
[^90]: 使用深度强化学习的自适应PD控制在具有随机时延的局部-远程遥操作中

    Adaptive PD Control using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays. (arXiv:2305.16979v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.16979](http://arxiv.org/abs/2305.16979)

    本论文引入了一种使用深度强化学习来解决局部-远程遥操作中的时延问题的自适应PD控制方法，通过实时调整控制器参数，改善同步性和稳定性，并克服了随机延迟带来的挑战。

    

    局部-远程系统允许机器人在危险环境中执行复杂任务，如太空和核电站。然而，由于可能威胁到系统性能和稳定性的时延，确立局部和远程设备之间的准确位置映射可能很困难。增强局部-远程系统的同步性和稳定性对于使机器人能够在更远距离和高度挑战的网络环境中与环境交互至关重要，包括时延。我们引入了一种采用强化学习来解决时延控制问题的自适应控制方法。通过实时调整控制器参数，这种自适应控制器可以补偿随机延迟并改善局部和远程机器人操作器之间的同步性。为了提高自适应PD控制器的性能，我们设计了一种基于模型的强化学习方法，有效地将多步延迟纳入到学习框架中。

    Local-remote systems allow robots to execute complex tasks in hazardous environments such as space and nuclear power stations. However, establishing accurate positional mapping between local and remote devices can be difficult due to time delays that can compromise system performance and stability. Enhancing the synchronicity and stability of local-remote systems is vital for enabling robots to interact with environments at greater distances and under highly challenging network conditions, including time delays. We introduce an adaptive control method employing reinforcement learning to tackle the time-delayed control problem. By adjusting controller parameters in real-time, this adaptive controller compensates for stochastic delays and improves synchronicity between local and remote robotic manipulators. To improve the adaptive PD controller's performance, we devise a model-based reinforcement learning approach that effectively incorporates multi-step delays into the learning framewor
    
[^91]: GPT-3.5、GPT-4还是BARD？对LLM在零样本情境中的推理能力和通过提示提升性能的评估

    GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts. (arXiv:2305.12477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12477](http://arxiv.org/abs/2305.12477)

    本文通过在多个任务和数据集上对GPT-3.5、GPT-4和BARD进行评估，实验证明在零样本情境中，ChatGPT-4表现出了更高的性能。GPT-4相对于GPT-3.5的优势可能是由其更大的模型规模和NLP效率所引起的，但对于BARD来说并不明显。此外，这三个模型在归纳、数学和多跳推理任务上的表现有限。

    

    大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展现了显著的性能。然而，关于它们的推理能力存在着当前的热议。本文通过在十一个不同数据集上对不同的推理任务进行彻底技术评估，研究了GPT-3.5、GPT-4和BARD模型的性能。我们的研究提供了实证证据，证明在零样本情境下，ChatGPT-4相对于ChatGPT-3.5和BARD在几乎所有评估任务中表现出了更高的性能。虽然GPT-4相对于GPT-3.5的优势可能可以通过其更大的模型规模和NLP效率来解释，但对于BARD来说并不明显。我们还证明了这三个模型在归纳、数学和多跳推理任务上显示出有限的能力。为了强化我们的发现，我们对这三个模型的结果进行了详细全面的分析。此外，我们提出了一组工程化的提示方式。

    Large Language Models (LLMs) have exhibited remarkable performance on various Natural Language Processing (NLP) tasks. However, there is a current hot debate regarding their reasoning capacity. In this paper, we examine the performance of GPT-3.5, GPT-4, and BARD models, by performing a thorough technical evaluation on different reasoning tasks across eleven distinct datasets. Our paper provides empirical evidence showcasing the superior performance of ChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting throughout almost all evaluated tasks. While the superiority of GPT-4 compared to GPT-3.5 might be explained by its larger size and NLP efficiency, this was not evident for BARD. We also demonstrate that the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To bolster our findings, we present a detailed and comprehensive analysis of the results from these three models. Furthermore, we propose a set of engineered prompt
    
[^92]: Marsellus: 一款具有2至8位DNN加速和30%自适应体偏化的异构RISC-V AI-IoT末端SoC

    Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2305.08415](http://arxiv.org/abs/2305.08415)

    Marsellus是一款具有2至8位DNN加速和30%自适应体偏化的异构RISC-V AI-IoT末端SoC，适用于计算密集型的深度神经网络推理以及高精度浮点运算的信号处理和控制。

    

    新兴的人工智能互联物联网（AI-IoT）系统级芯片（SoC）需要在范围广泛的工作条件下，在几十毫瓦的功耗限制下运行许多不同的任务，包括计算密集型但强量化的深度神经网络（DNN）推理以及需要高精度浮点运算的信号处理和控制。我们提出了Marsellus，一个在GlobalFoundries 22nm FDX上制造的全数字异构SoC，用于AI-IoT末端节点，它结合了：1）一个16个RISC-V数字信号处理（DSP）核心的通用集群，用于执行各种支持4位和2位算术扩展（XpulpNN）的工作负载，同时结合了融合的MAC和LOAD操作和浮点支持；2）一个2-8位可重构二进制引擎（RBE），用于加速DNN中的3x3和1x1（逐点）卷积；3）一组连接到自适应体偏化的片上监视（OCM）模块。

    Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT) System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and nano-robotics need to run many diverse tasks within a power envelope of a few tens of mW over a wide range of operating conditions: compute-intensive but strongly quantized Deep Neural Network (DNN) inference, as well as signal processing and control requiring high-precision floating-point. We present Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16 RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions (XpulpNN), combined with fused MAC&LOAD operations and floating-point support; 2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1 (pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks connected to an Ad
    
[^93]: ACTC: 冷启动知识图谱补全的主动阈值校准

    ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])

    [http://arxiv.org/abs/2305.06395](http://arxiv.org/abs/2305.06395)

    本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。

    

    自监督的知识图谱补全(KGC)依赖于估计得分模型(实体，关系，实体)-元组，例如，通过嵌入初始知识图。通过调整预测阈值(使用手动注释的示例)，可以改善预测质量。本文尝试首次针对KGC进行冷启动校准，在此过程中初始没有注释的示例，并且只能选择有限数量的元组进行注释。我们的新方法ACTC基于有限的注释元组有效地找到好的每个关系的阈值。除了一些注释的元组外，ACTC还利用Logistic回归或高斯过程分类器估计的未标记元组的正确性。我们还通过密度和随机选择等不同方法选择候选元组进行注释。我们使用五个评分模型和一个oracle注释进行实验。

    Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
    
[^94]: 机器学习中的不确定性量化在工程设计与健康预测中的应用：一篇教程

    Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial. (arXiv:2305.04933v1 [cs.LG])

    [http://arxiv.org/abs/2305.04933](http://arxiv.org/abs/2305.04933)

    本文教程介绍机器学习中的不确定性量化方法，帮助提升其安全性和可靠性，进而促进其在高风险决策背景下的广泛应用，特别关注神经网络及其在工程设计和预测健康管理中的应用。

    

    在机器学习模型中，不确定性量化（UQ）作为安全保障的基本层，可以通过启用合理的风险评估和管理来促进更加原则性的决策制定。UQ使得ML模型的安全性和可靠性得到改善，有潜力显著促进高风险决策背景下的ML解决方案的广泛采用，例如医疗保健，制造业和航空等。在本教程中，我们旨在全方位介绍机器学习模型中新兴UQ方法，特别关注神经网络，以及这些UQ方法在解决工程设计和预测健康管理问题方面的应用。为此，我们首先对涉及ML模型UQ的不确定性类型，来源和原因进行了全面分类。接下来，我们提供了几种最先进的UQ方法的教程式描述：高斯过程回归，贝叶斯神经网络。

    On top of machine learning models, uncertainty quantification (UQ) functions as an essential layer of safety assurance that could lead to more principled decision making by enabling sound risk assessment and management. The safety and reliability improvement of ML models empowered by UQ has the potential to significantly facilitate the broad adoption of ML solutions in high-stakes decision settings, such as healthcare, manufacturing, and aviation, to name a few. In this tutorial, we aim to provide a holistic lens on emerging UQ methods for ML models with a particular focus on neural networks and the applications of these UQ methods in tackling engineering design as well as prognostics and health management problems. Toward this goal, we start with a comprehensive classification of uncertainty types, sources, and causes pertaining to UQ of ML models. Next, we provide a tutorial-style description of several state-of-the-art UQ methods: Gaussian process regression, Bayesian neural network
    
[^95]: 图像的背叛：贝叶斯场景关键点在机器人操作中的深度策略学习

    The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation. (arXiv:2305.04718v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04718](http://arxiv.org/abs/2305.04718)

    该论文介绍了一种用于机器人操作中的深度策略学习的贝叶斯场景关键点跟踪方法。该方法通过解决图像中的歧义问题，实现了对于对称物体和遮挡和视野之外物体的关键点跟踪，提高了相机观察的效用，对于策略学习具有优势。

    

    在机器人操作的策略学习中，样本效率至关重要。因此，从相机观察中学习和提取更紧凑的表示是一个有前途的途径。然而，当前的方法常常假设场景的完全可观测性，并且对于尺度不变性的处理存在困难。在许多任务和情景中，这个假设并不成立，因为场景中的物体经常被遮挡或者位于相机的视野之外，导致相机观察在物体位置方面产生歧义。为了解决这个问题，我们提出了BASK，一种贝叶斯方法来跟踪随时间变化的尺度不变关键点。我们的方法成功地解决了图像中固有的歧义问题，实现了对对称物体和遮挡和视野之外的物体的关键点跟踪。我们利用我们的方法从手腕相机观察中学习具有挑战性的多物体机器人操作任务，并与其他表示学习方法相比，展示了更出色的策略学习效果。

    In policy learning for robotic manipulation, sample efficiency is of paramount importance. Thus, learning and extracting more compact representations from camera observations is a promising avenue. However, current methods often assume full observability of the scene and struggle with scale invariance. In many tasks and settings, this assumption does not hold as objects in the scene are often occluded or lie outside the field of view of the camera, rendering the camera observation ambiguous with regard to their location. To tackle this problem, we present BASK, a Bayesian approach to tracking scale-invariant keypoints over time. Our approach successfully resolves inherent ambiguities in images, enabling keypoint tracking on symmetrical objects and occluded and out-of-view objects. We employ our method to learn challenging multi-object robot manipulation tasks from wrist camera observations and demonstrate superior utility for policy learning compared to other representation learning te
    
[^96]: 一个统一的主动学习框架，用于注释图形数据并应用于软件代码性能预测

    A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])

    [http://arxiv.org/abs/2304.13032](http://arxiv.org/abs/2304.13032)

    提出了一个针对软件性能预测的主动学习框架，通过将源代码解析成流增强抽象语法树图形式，构造各种无监督和有监督的图嵌入，进行主动学习，实现对未标注数据的高效利用，并比现有方法更加优越。

    

    大多数机器学习和数据分析应用程序，包括软件系统中的性能工程，需要大量注释和标记数据，这些可能事先并不可用。获取注释通常需要显着的时间、精力和计算资源，这使得任务变得具有挑战性。我们开发了一个统一的主动学习框架，专门针对软件性能预测来解决这个任务。我们从将源代码解析成抽象语法树（AST）开始，然后用数据和控制流边来增强它。然后，我们将源代码的树形表示转换为流增强抽象语法树图（FA-AST）表示法。基于图形表示，我们将各种图嵌入（无监督和有监督）构造成一个潜在空间。鉴于这样的嵌入，该框架变得任务不可知，因为可以使用任何回归方法和适用于回归的查询策略来执行主动学习。在该框架内，我们引入并评估了几种用于软件性能预测的主动学习查询策略和回归算法，证明了我们的方法优于现有方法。

    Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
    
[^97]: 交互感知提示的零样本时空动作检测

    Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection. (arXiv:2304.04688v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.04688](http://arxiv.org/abs/2304.04688)

    提出了一种采用预训练的视觉-语言模型和交互模块进行交互感知提示的零样本时空动作检测方法，优化了视觉-语言特征的对齐，实现了更好的结果。

    

    空间-时间动作检测的目标是确定每个人在视频中动作发生的时间和位置，并对相应的动作类别进行分类。大多数现有方法采用全监督学习，需要大量的训练数据，因此难以实现零样本学习。本文提出利用预训练的视觉-语言模型提取代表性的图像和文本特征，并通过不同的交互模块建模这些特征之间的关系以得到交互特征。此外，利用这个特征提示每个标签以获取更合适的文本特征。最后，我们计算每个标签的交互特征和文本特征之间的相似度，以确定动作类别。在J-HMDB和UCF101-24数据集上的实验表明，所提出的交互模块和提示使视觉-语言特征更加对齐，从而实现了更好的结果。

    The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text features. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achievi
    
[^98]: 自主式小鼠颅窗骨抽样机器人钻孔系统：一个基于蛋模型的评估。

    Autonomous Robotic Drilling System for Mice Cranial Window Creation: An Evaluation with an Egg Model. (arXiv:2303.12265v1 [cs.RO])

    [http://arxiv.org/abs/2303.12265](http://arxiv.org/abs/2303.12265)

    本文提出了一种针对小鼠颅骨不均匀问题的自主式机器人操作系统。

    

    在生命科学实验中，机器人技术的应用可以使得对实验样本进行精确操作，不受科学家技能的限制。本研究以小鼠颅窗的骨抽样操作为例，考虑小鼠颅骨的不均匀情况，提出了一种自主式操作机器人系统。

    Robotic assistance for experimental manipulation in the life sciences is expected to enable precise manipulation of valuable samples, regardless of the skill of the scientist. Experimental specimens in the life sciences are subject to individual variability and deformation, and therefore require autonomous robotic control. As an example, we are studying the installation of a cranial window in a mouse. This operation requires the removal of the skull, which is approximately 300 um thick, to cut it into a circular shape 8 mm in diameter, but the shape of the mouse skull varies depending on the strain of mouse, sex and week of age. The thickness of the skull is not uniform, with some areas being thin and others thicker. It is also difficult to ensure that the skulls of the mice are kept in the same position for each operation. It is not realistically possible to measure all these features and pre-program a robotic trajectory for individual mice. The paper therefore proposes an autonomous 
    
[^99]: 使用深度学习集成的数据增强多视角方法识别化石图像

    Fossil Image Identification using Deep Learning Ensembles of Data Augmented Multiviews. (arXiv:2302.08062v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08062](http://arxiv.org/abs/2302.08062)

    该论文提出了使用深度学习集成的数据增强多视角方法识别化石图像。通过收集化石图像的不同视角，使用多个基础模型进行训练，并通过软投票进行最终决策。实验证明，该方法在性能上优于基线方法，并且与使用三个原始视图模型的方法相当。

    

    化石物种的识别对于进化研究至关重要。近期深度学习的进展在化石图像识别方面显示出了很有前景的发展。然而，由于化石保存、有条件采样以及领域专家昂贵且不一致的标签注释的限制，标记化石图像的数量和质量通常都受到限制，从而给使用基于深度学习的图像分类模型训练带来了巨大挑战。为了解决这些问题，我们遵循“群众的智慧”的思想，提出了一个多视角集成框架，通过收集每个化石图像的原始视图、灰度视图和骨架视图来训练多个基础模型，并通过软投票进行最终决策。在包含2400张图像的最大造针螺化石数据集上的实验证明，所提出的OGS方法始终优于基线方法（针对每个视图使用单一模型），并且在性能上优于或可与OOO方法相媲美（使用三个原始视图模型）。

    Identification of fossil species is crucial to evolutionary studies. Recent advances from deep learning have shown promising prospects in fossil image identification. However, the quantity and quality of labeled fossil images are often limited due to fossil preservation, conditioned sampling, and expensive and inconsistent label annotation by domain experts, which pose great challenges to training deep learning based image classification models. To address these challenges, we follow the idea of the wisdom of crowds and propose a multiview ensemble framework, which collects Original (O), Gray (G), and Skeleton (S) views of each fossil image reflecting its different characteristics to train multiple base models, and then makes the final decision via soft voting. Experiments on the largest fusulinid dataset with 2400 images show that the proposed OGS consistently outperforms baselines (using a single model for each view), and obtains superior or comparable performance compared to OOO (us
    
[^100]: 一个融合基于规则的透明模型、软标签相关性学习和标签噪声抗性的鲁棒多标记方法

    A Robust Multilabel Method Integrating Rule-based Transparent Model, Soft Label Correlation Learning and Label Noise Resistance. (arXiv:2301.03283v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.03283](http://arxiv.org/abs/2301.03283)

    本论文提出了一种融合基于规则的透明模型、软标签相关性学习和标签噪声抗性的鲁棒多标记方法（R-MLTSK-FS）。实验证明了该方法在多标记学习中的优越性能。

    

    在多标记学习中，模型透明性、标签相关性学习和对标签噪声的鲁棒性非常重要。然而，目前很少有方法同时研究这三个特点。为了解决这个挑战，我们提出了一种鲁棒的多标记Takagi-Sugeno-Kang模糊系统（R-MLTSK-FS），其中包括三种机制。首先，我们设计了一个软标签学习机制，通过明确测量标签之间的交互作用来减少标签噪声的影响，这也是其他两种机制的基础。其次，我们使用基于规则的TSK FS作为基模型，以比许多现有的多标记模型更透明的方式有效地建模特征和软标签之间的推理关系。第三，为了进一步提高多标记学习的性能，我们基于软标签空间和模糊特征空间构建了一个相关增强学习机制。我们进行了大量实验证明了所提方法的优越性。

    Model transparency, label correlation learning and the robust-ness to label noise are crucial for multilabel learning. However, few existing methods study these three characteristics simultaneously. To address this challenge, we propose the robust multilabel Takagi-Sugeno-Kang fuzzy system (R-MLTSK-FS) with three mechanisms. First, we design a soft label learning mechanism to reduce the effect of label noise by explicitly measuring the interactions between labels, which is also the basis of the other two mechanisms. Second, the rule-based TSK FS is used as the base model to efficiently model the inference relationship be-tween features and soft labels in a more transparent way than many existing multilabel models. Third, to further improve the performance of multilabel learning, we build a correlation enhancement learning mechanism based on the soft label space and the fuzzy feature space. Extensive experiments are conducted to demonstrate the superiority of the proposed method.
    
[^101]: RouteNet-Fermi: 使用图神经网络进行网络建模

    RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2212.12070](http://arxiv.org/abs/2212.12070)

    RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。

    

    网络模型是现代网络的重要组成部分，广泛用于网络规划和优化。然而，随着网络规模和复杂性的增加，一些模型存在限制，如排队理论模型中对马尔可夫流量的假设，以及网络模拟器的高计算成本。机器学习的最新进展，如图神经网络（GNN），正在推动一代新的数据驱动网络模型，能够学习复杂的非线性行为。在本文中，我们提出了一种名为RouteNet-Fermi的自定义GNN模型，它与排队理论具有相同的目标，并且在存在真实流量模型的情况下准确性更高。该模型可以准确预测网络的延迟、抖动和丢包情况。我们在不断增长的网络规模（最大达到300个节点）和包括具有混合流量特性的样本（如复杂的非马尔可夫模型）中测试了RouteNet-Fermi模型。

    Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
    
[^102]: DyG2Vec: 带有自监督的动态图表征学习

    DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision. (arXiv:2210.16906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16906](http://arxiv.org/abs/2210.16906)

    DyG2Vec是一个具有自监督学习能力的动态图表征学习模型，采用了高效而有效的注意力编码器和非对比自监督学习方法，能够提取丰富的时间嵌入表示。在基准数据集上的实验结果表明，该模型在未来链接预测任务中表现出色。

    

    时间图神经网络已经展示出在通过自动提取时间模式来学习归纳表示方面的有希望结果。然而，以往的工作常常依赖于复杂的记忆模块或低效的随机游走方法来构建时间表示。此外，现有的动态图编码器不容易适应自监督范式，这阻碍了它们利用无标签数据。为了解决这些限制，我们提出了一种高效而有效的基于注意力的编码器，利用时间边编码和基于窗口的子图采样来生成任务无关的嵌入。此外，我们提出了一种使用非对比SSL的联合嵌入架构，以学习丰富的时间嵌入而不需要标签。在7个基准数据集上的实验结果表明，我们的模型在传导设置和归纳设置的未来链接预测任务中，平均优于现有的SoTA基线4.23％和3.30％。

    Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requi
    
[^103]: 图模糊系统: 概念、模型和算法

    Graph Fuzzy System: Concepts, Models and Algorithms. (arXiv:2210.16730v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.16730](http://arxiv.org/abs/2210.16730)

    本文提出了一种适用于图数据建模的新型模糊系统，名为图模糊系统（GFS），通过定义相关概念、构建模型框架和算法，实现了在处理具有非欧几里得结构的图数据时保留模糊系统优势的目标。

    

    模糊系统在各个领域都有广泛的应用，包括模式识别、智能控制、数据挖掘和生物信息学，这归功于其强大的解释和学习能力。在传统的应用场景中，模糊系统主要用于建模欧几里得空间数据，无法处理非欧几里得结构的图数据，比如社交网络和交通路线图。因此，开发适用于图数据并能保留传统模糊系统优势的建模方法是一个重要的研究课题。为了应对这个挑战，本文提出了一种适用于图数据建模的新型模糊系统，称为图模糊系统（GFS），并系统地开发了相关概念、建模框架和构建算法。首先，定义了GFS相关概念，包括图模糊规则库、图模糊集和图后件处理单元（GCPU）。然后构建了一个GFS建模框架。

    Fuzzy systems (FSs) have enjoyed wide applications in various fields, including pattern recognition, intelligent control, data mining and bioinformatics, which is attributed to the strong interpretation and learning ability. In traditional application scenarios, FSs are mainly applied to model Euclidean space data and cannot be used to handle graph data of non-Euclidean structure in nature, such as social networks and traffic route maps. Therefore, development of FS modeling method that is suitable for graph data and can retain the advantages of traditional FSs is an important research. To meet this challenge, a new type of FS for graph data modeling called Graph Fuzzy System (GFS) is proposed in this paper, where the concepts, modeling framework and construction algorithms are systematically developed. First, GFS related concepts, including graph fuzzy rule base, graph fuzzy sets and graph consequent processing unit (GCPU), are defined. A GFS modeling framework is then constructed and
    
[^104]: Conformal Prediction对分散标签噪声具有稳健性

    Conformal Prediction is Robust to Dispersive Label Noise. (arXiv:2209.14295v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14295](http://arxiv.org/abs/2209.14295)

    本研究研究了Conformal Prediction方法对于标签噪声具有鲁棒性。我们找出了构建可以正确覆盖无噪声真实标签的不确定性集合的条件，并提出了对具有噪声标签的一般损失函数进行正确控制的要求。实验证明，在对抗性案例之外，使用Conformal Prediction和风险控制技术可以实现对干净真实标签的保守风险。我们还提出了一种有界尺寸噪声修正的方法，以确保实现正确的真实标签风险。

    

    我们研究了对标签噪声具有鲁棒性的Conformal Prediction方法，该方法是一种用于不确定性量化的强大工具。我们的分析涵盖了回归和分类问题，对于如何构建能够正确覆盖未观察到的无噪声真实标签的不确定性集合进行了界定。我们进一步扩展了我们的理论，并提出了对于带有噪声标签正确控制一般损失函数（如假阴性比例）的要求。我们的理论和实验表明，在带有噪声标签的情况下，Conformal Prediction和风险控制技术能够实现对干净真实标签的保守风险，除了在对抗性案例中。在这种情况下，我们还可以通过对Conformal Prediction算法进行有界尺寸的噪声修正，以确保实现正确的真实标签风险，而无需考虑分数或数据的规则性。

    We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels except in adversarial cases. In such cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.
    
[^105]: 通过NeurVec加速大规模动态系统数值求解器的模拟

    Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2208.03680](http://arxiv.org/abs/2208.03680)

    通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。

    

    在众多科学和工程领域中，大规模动态系统的模拟至关重要。然而，传统的数值求解器在估计积分时由于步长选择的限制，存在着精度和计算效率之间的权衡。为了解决这个挑战，我们引入了一种基于深度学习的修正器，称为NeurVec，它可以补偿集成误差并在模拟中实现更大的时间步长。我们在各种复杂动态系统基准上进行了大量实验证明，即使在使用有限和离散数据进行训练时，NeurVec在连续相空间上表现出了显著的泛化能力。NeurVec显著加速了传统求解器，实现了几十到几百倍的速度提升，同时保持了高水平的准确性和稳定性。此外，NeurVec的简单而有效的设计结合了易于实现的特点，有潜力建立起一个新的求解器范式。

    The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
    
[^106]: 用图神经网络评估电网拓扑的动态稳定性

    Toward Dynamic Stability Assessment of Power Grid Topologies using Graph Neural Networks. (arXiv:2206.06369v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06369](http://arxiv.org/abs/2206.06369)

    本研究通过使用图神经网络（GNN）分析电网的动态稳定性，生成了新的大型数据集，并证明了GNN在仅利用拓扑信息的情况下能够有效预测非线性目标，同时能够准确识别电网中的脆弱节点。

    

    为了应对气候变化，可再生能源在电力生产中的份额需要增加。可再生能源引入了新的挑战，涉及到电网的动态稳定性，包括分散化、降低的惯性和生产的波动性。由于对于大型电网而言，动态稳定性模拟是棘手且极其昂贵的，图神经网络（GNN）是一个有希望的方法，可以减少分析电网动态稳定性的计算工作量。作为GNN模型的测试平台，我们生成了新的、大型的合成电网动态稳定性数据集，并将其作为开源资源提供给研究界。我们发现，仅凭拓扑信息，GNN能够出奇地有效地预测高度非线性的目标。首次实现了适用于实际应用的性能。此外，我们展示了这些模型准确识别电网中特定脆弱节点的能力。

    To mitigate climate change, the share of renewable energies in power production needs to be increased. Renewables introduce new challenges to power grids regarding the dynamic stability due to decentralization, reduced inertia, and volatility in production. Since dynamic stability simulations are intractable and exceedingly expensive for large grids, graph neural networks (GNNs) are a promising method to reduce the computational effort of analyzing the dynamic stability of power grids. As a testbed for GNN models, we generate new, large datasets of dynamic stability of synthetic power grids, and provide them as an open-source resource to the research community. We find that GNNs are surprisingly effective at predicting the highly non-linear targets from topological information only. For the first time, performance that is suitable for practical use cases is achieved. Furthermore, we demonstrate the ability of these models to accurately identify particular vulnerable nodes in power grid
    
[^107]: 分析环境和社交语境，使机器人的语音适应环境

    Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts. (arXiv:2205.04952v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.04952](http://arxiv.org/abs/2205.04952)

    本研究描述了一种选择机器人语音风格的过程和结果，以达到社交适应性和环境感知。通过在虚拟环境中收集和验证语音数据交互，并使用投影、灯光和声音在重新创造的环境中测试机器人的语音风格，我们探索和聚类人类的语音话语以识别主要的语音风格，并以餐饮服务场景作为概念验证环境。结果表明，这种研究可以改善机器人在特定语境下的社交适应性和智能感知。

    

    机器人在正式、安静、黑暗的环境或明亮、热闹、嘈杂的环境中应该如何说话？通过设计机器人以更社交和适应环境的方式说话，我们可以提高人们对这些代理人的感知意识和智能程度。我们描述了一种选择机器人语音风格以达到社交适应性和环境感知的过程和结果。由于野外语音获取的困难，理解人类在不同声学环境中如何调整自己的声音可能具有挑战性。我们的方法包括三个步骤：(a) 在虚拟的Zoom环境中收集和验证语音数据交互，(b) 探索和聚类人类的语音话语以识别主要的语音风格，以及(c) 使用投影、灯光和声音在再现的环境中测试机器人的语音风格。我们以餐饮服务场景作为概念验证环境。我们提供了使用Pepper机器人的不同风格的语音的结果，朝着在特定语境下说话的机器人。

    How should a robot speak in a formal, quiet and dark, or a bright, lively and noisy environment? By designing robots to speak in a more social and ambient-appropriate manner we can improve perceived awareness and intelligence for these agents. We describe a process and results toward selecting robot voice styles for perceived social appropriateness and ambiance awareness. Understanding how humans adapt their voices in different acoustic settings can be challenging due to difficulties in voice capture in the wild. Our approach includes 3 steps: (a) Collecting and validating voice data interactions in virtual Zoom ambiances, (b) Exploration and clustering human vocal utterances to identify primary voice styles, and (c) Testing robot voice styles in recreated ambiances using projections, lighting and sound. We focus on food service scenarios as a proof-of-concept setting. We provide results using the Pepper robot's voice with different styles, towards robots that speak in a contextually a
    
[^108]: 将错误信息注入进行开放领域问答的攻击

    Attacking Open-domain Question Answering by Injecting Misinformation. (arXiv:2110.07803v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.07803](http://arxiv.org/abs/2110.07803)

    本研究探究了将错误信息注入问答系统的攻击，并发现问答模型对于错误信息具有脆弱性，即使少量错误信息的污染也会导致性能大幅下降。

    

    随着宣传、新闻和社交媒体中虚假、不准确和误导性信息的增加，现实世界中的问答系统面临着在被错误信息污染的语境中综合和推理以得出正确答案的挑战。这种紧迫性促使我们需要使问答系统对错误信息具备鲁棒性，而这是之前未被探索的一个主题。我们通过调查开放领域问答模型对带有错误信息文档的语料库污染的敏感性，来研究错误信息对问答模型的风险。我们整理了人工撰写和模型生成的虚假文档，将其注入到问答模型的证据语料库中，并评估这些系统性能的影响。实验证明，即使少量的错误信息污染也会使问答模型脆弱，所有模型的性能都会大幅度下降。当神经模型批量生成虚假文档时或攻击者针对问答模型时，错误信息攻击的威胁更大。

    With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over misinformation-polluted contexts to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the sensitivity of open-domain QA models to corpus pollution with misinformation documents. We curate both human-written and model-generated false documents that we inject into the evidence corpus of QA models and assess the impact on the performance of these systems. Experiments show that QA models are vulnerable to even small amounts of evidence contamination brought by misinformation, with large absolute performance drops on all models. Misinformation attack brings more threat when fake documents are produced at scale by neural models or the attacker targ
    
[^109]: 以程序为过程：通过自然语言对处境代理进行层级控制

    Procedures as Programs: Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.08214](http://arxiv.org/abs/2109.08214)

    本文提出了一种以程序为过程的形式化方法，用于代理指令和控制。通过建立层级模块化网络，将自然语言意图转化为可执行程序的预测，并在两个数据集上取得了显著性能优势。

    

    当人类构思执行特定任务时，他们会以层次结构的方式进行：将高级任务分解为较小的子任务。然而，在关于自然语言对处境代理的文献中，大多数研究都将要执行的过程视为简单动作的平坦序列，或者最多只有浅层的程序层次结构。在本文中，我们提出了一种将程序作为过程的形式化方法，这是一种强大而直观的表示层级过程知识以进行代理指令和控制的方法。我们进一步提出了一种层级模块化网络的建模范式，该网络由规划者和反应器组成，将自然语言意图转化为可执行程序的预测，并探测环境以获取完成程序执行所需的信息。我们在IQA和ALFRED数据集上实例化这个框架用于自然语言指令跟随。我们的模型在两个数据集上远远超过了反应式基线的性能。我们还证明了o...

    When humans conceive how to perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, in the literature on natural language (NL) command of situated agents, most works have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a powerful yet intuitive method of representing hierarchical procedural knowledge for agent command and control. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that o
    
[^110]: Co-GAIL: 学习多样化的人机协作策略

    Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration. (arXiv:2108.06038v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2108.06038](http://arxiv.org/abs/2108.06038)

    Co-GAIL是一种学习人机多样化协作策略的方法，通过共同优化人类策略和机器人策略，实现从人-人协作示范中学习，并能在在线任务执行过程中应对人类策略调整的鲁棒性。

    

    我们提出了一种从人-人协作示范中学习人机协作策略的方法。一个有效的机器人助手必须学会处理示范中展示的多样化的人类行为，并在在线任务执行过程中当人类调整策略时具有鲁棒性。我们的方法在交互式学习过程中共同优化人类策略和机器人策略：人类策略通过示范学习生成多样化和可信的协作行为，而机器人策略则通过估计其人类合作者未观测到的潜在策略来进行辅助。在2D策略游戏、人机交接任务和多步协作操纵任务中，我们的方法在模拟评估和与真实人类操作员一起执行任务时均优于替代方法。附加材料和视频请参见 https://sites.google.com/view/co-gail-web/home

    We present a method for learning a human-robot collaboration policy from human-human collaboration demonstrations. An effective robot assistant must learn to handle diverse human behaviors shown in the demonstrations and be robust when the humans adjust their strategies during online task execution. Our method co-optimizes a human policy and a robot policy in an interactive learning process: the human policy learns to generate diverse and plausible collaborative behaviors from demonstrations while the robot policy learns to assist by estimating the unobserved latent strategy of its human collaborator. Across a 2D strategy game, a human-robot handover task, and a multi-step collaborative manipulation task, our method outperforms the alternatives in both simulated evaluations and when executing the tasks with a real human operator in-the-loop. Supplementary materials and videos at https://sites.google.com/view/co-gail-web/home
    

