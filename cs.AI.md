# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Executable Code Actions Elicit Better LLM Agents](https://rss.arxiv.org/abs/2402.01030) | 使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。 |
| [^2] | [Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects](https://arxiv.org/abs/2404.01440) | 该方法通过显式建模点级对应关系、利用图像、3D重建和运动学的线索，成功构建了未知关节对象的数字孪生体，结果更准确和稳定，且不依赖于任何对象形状或结构先验。 |
| [^3] | [Graph Neural Networks for Treatment Effect Prediction](https://arxiv.org/abs/2403.19289) | 提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性 |
| [^4] | [Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings](https://arxiv.org/abs/2403.07750) | 这项研究提出了一种新方法，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。 |
| [^5] | [Graph Data Condensation via Self-expressive Graph Structure Reconstruction](https://arxiv.org/abs/2403.07294) | 通过自表达图结构重建的方法解决了图数据压缩中的问题 |
| [^6] | [Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation](https://arxiv.org/abs/2403.05131) | 对文本到视频生成技术的发展进行了详细调查, 着重介绍了从传统生成模型到尖端Sora模型的转变，强调了可扩展性和通用性的发展。 |
| [^7] | [Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification](https://arxiv.org/abs/2403.04696) | 提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。 |
| [^8] | [Never-Ending Embodied Robot Learning](https://arxiv.org/abs/2403.00336) | 提出了一种具身机器人学习代理NBCagent，通过技能特定的演化规划器和技能共享的语义渲染模块，实现从视觉观测中连续学习新的机器人操作技能知识。 |
| [^9] | [Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization](https://arxiv.org/abs/2402.17574) | Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。 |
| [^10] | [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459) | 通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。 |
| [^11] | [REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories](https://arxiv.org/abs/2402.16310) | 该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。 |
| [^12] | [Deep Networks Always Grok and Here is Why](https://arxiv.org/abs/2402.15555) | 深度神经网络存在延迟泛化和延迟鲁棒性现象，在各种实际环境中普遍存在，并基于新的局部复杂度度量提供了解释。 |
| [^13] | [NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks](https://arxiv.org/abs/2402.15393) | NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。 |
| [^14] | [DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents](https://arxiv.org/abs/2402.14865) | 本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。 |
| [^15] | [Neural Control System for Continuous Glucose Monitoring and Maintenance](https://arxiv.org/abs/2402.13852) | 引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。 |
| [^16] | [The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis](https://arxiv.org/abs/2402.12976) | 通过多维分析发现，示范的有效性在多语种上下文学习中存在显著差异，其中部分模型对示范质量不敏感，而精心设计的模板可以消除对示范的依赖。 |
| [^17] | [EmoBench: Evaluating the Emotional Intelligence of Large Language Models](https://arxiv.org/abs/2402.12071) | EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。 |
| [^18] | [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753) | 提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。 |
| [^19] | [GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network](https://arxiv.org/abs/2402.11709) | GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。 |
| [^20] | [CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback](https://arxiv.org/abs/2402.10980) | 通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索 |
| [^21] | [Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs](https://arxiv.org/abs/2402.10586) | 本文探讨了如何通过研究文本中的话语特征来区分人类创作和机器生成的文本，引入了一种新颖的方法来揭示这些特征，并发现人类写作在结构上更为多样化。 |
| [^22] | [Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting](https://arxiv.org/abs/2402.10412) | 提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。 |
| [^23] | [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983) | 本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。 |
| [^24] | [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://arxiv.org/abs/2402.08679) | 本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。 |
| [^25] | [Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings](https://arxiv.org/abs/2402.08145) | 本文介绍了一种在非稳态环境中进行通用规划和学习的方法，通过认识性探索填补代理的知识空白，并且利用收集到的数据学习通用的概率模型来解决不断变化的任务。实验证明这种方法在非稳态环境中比传统方法更有效。 |
| [^26] | [Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations](https://arxiv.org/abs/2402.07933) | 本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。 |
| [^27] | [Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training](https://arxiv.org/abs/2402.07076) | 通过对比预训练和多领域匹配结构，我们提出了CAMA框架来改善B2B云解决方案的匹配问题，克服了复杂特征和有限数据的挑战。 |
| [^28] | [Feedback Loops With Language Models Drive In-Context Reward Hacking](https://arxiv.org/abs/2402.06627) | 与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。 |
| [^29] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^30] | [The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise](https://arxiv.org/abs/2401.07844) | 本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。 |
| [^31] | [Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes](https://arxiv.org/abs/2312.08519) | LCNet是一个神经网络模型，能够通过学习存储共享结构，同时使用上下文模块表示特定上下文结构，成功实现了在不同任务中提取共享结构并避免灾难性干扰的功能；并且能够捕捉人类数据中的课程效应。 |
| [^32] | [CLOMO: Counterfactual Logical Modification with Large Language Models](https://arxiv.org/abs/2311.17438) | 本研究探索了大型语言模型的反事实推理能力，并引入了反事实逻辑修改任务和相应的评估指标。实验结果表明，大型语言模型展现出显著的逻辑能力。 |
| [^33] | [Fusion-Eval: Integrating Evaluators with LLMs](https://arxiv.org/abs/2311.09204) | Fusion-Eval是一种创新方法，利用LLMs整合不同辅助评估器的见解，极大提升自然语言系统评估的有效性。 |
| [^34] | [CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation](https://arxiv.org/abs/2311.08588) | CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。 |
| [^35] | [Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras](https://arxiv.org/abs/2310.04521) | 本文提出了一种Lie神经元网络，能够以任何半单Lie代数数据为输入，通过伴随操作使其具有等变性。通过推广向量神经元网络和引入新的层，该网络在各个领域具有广泛的适用性和竞争性能。 |
| [^36] | [WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.](http://arxiv.org/abs/2401.13919) | WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。 |
| [^37] | [HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing.](http://arxiv.org/abs/2401.10267) | HyperSense是一个协同设计的硬件和软件系统，能够根据传感器数据中的物体存在预测有效地控制数据生成速率，通过使用低精度ADC减少冗余数据降低机器学习系统的成本，利用超维度计算的特点分析实时的低精度传感器数据，在处理噪声、以内存为中心和实时学习方面具有优势。该系统结合了高性能的物体检测软件和实时的硬件预测，引入了智能传感器控制的新概念。在软件和硬件评估中表现出卓越的性能。 |
| [^38] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^39] | [Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives.](http://arxiv.org/abs/2401.02009) | 自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。 |
| [^40] | [WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation.](http://arxiv.org/abs/2312.14187) | 本文提出了WaveCoder，一个广泛和多功能的改进指令调优模型，通过将指令数据分类并利用LLM框架生成多样的高质量指令数据，以提高调优模型的效果和泛化能力。 |
| [^41] | [The Development of LLMs for Embodied Navigation.](http://arxiv.org/abs/2311.00530) | 本文概述了LLMs与具身智能在导航领域的共生关系，并评估了现有模型和数据集的优缺点。 |
| [^42] | [BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing.](http://arxiv.org/abs/2310.19975) | BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。 |
| [^43] | [CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents.](http://arxiv.org/abs/2310.17512) | 本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。 |
| [^44] | [Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives.](http://arxiv.org/abs/2310.12324) | 适应性实验为持续课程改进提供了机会，通过动态部署最有效的条件以满足学生需求。 |
| [^45] | [GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers.](http://arxiv.org/abs/2310.10375) | 提出了一种面向几何的注意力机制（GTA），用于将几何结构编码为相对变换，从而改进了多视图Transformer的学习效率和性能。 |
| [^46] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^47] | [How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.](http://arxiv.org/abs/2310.05492) | 本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。 |
| [^48] | [You Only Look at Screens: Multimodal Chain-of-Action Agents.](http://arxiv.org/abs/2309.11436) | 本论文提出了一种名为Auto-UI的多模态动作链机器人，通过直接与界面交互，避免了环境解析或依赖于应用程序API的需要，并引入了动作链技术来帮助模型进行决策。 |
| [^49] | ["Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets.](http://arxiv.org/abs/2308.05201) | 这项研究通过利用ChatGPT作为外生冲击，揭示了其对在线劳动市场的影响。结果显示，直接接触ChatGPT的任务和自由职业者的交易量显著下降，但适应新技术并提供增强人工智能的服务的自由职业者仍能获得利益。 |
| [^50] | [Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models.](http://arxiv.org/abs/2306.08889) | 通过设计轻量级探针QUAG和替代方法QUAG-attention，发现视频问答模型在多模态理解方面存在幻象，即使在多模态损伤下仍能保持高性能，且用更少的计算量实现相似的性能。 |

# 详细

[^1]: 可执行代码行动能够激发更出色的LLM智能体

    Executable Code Actions Elicit Better LLM Agents

    [https://rss.arxiv.org/abs/2402.01030](https://rss.arxiv.org/abs/2402.01030)

    使用可执行的Python代码整合LLM智能体行动，提升了成功率高达20%。

    

    大型语言模型（LLM）智能体具备执行广泛行动的能力，如调用工具和控制机器人等，在解决现实世界的挑战中显示出巨大潜力。LLM智能体通常通过生成JSON或文本的预定义格式来产生行动，这通常受限于受限制的行动空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本研究提出使用可执行的Python代码将LLM智能体的行动整合到一个统一的行动空间中（CodeAct）。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互在新的观察中动态修订先前的行动或发出新的行动。我们对17个LLM在API-Bank和新编制的基准测试中进行了广泛分析，结果显示CodeAct的性能超过了广泛使用的替代方案（成功率高出20%）。CodeAct的令人鼓舞的表现激励我们建立了一个开源的...

    Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-sourc
    
[^2]: 用于构建未知关节对象数字孪生体的神经隐式表示

    Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects

    [https://arxiv.org/abs/2404.01440](https://arxiv.org/abs/2404.01440)

    该方法通过显式建模点级对应关系、利用图像、3D重建和运动学的线索，成功构建了未知关节对象的数字孪生体，结果更准确和稳定，且不依赖于任何对象形状或结构先验。

    

    我们解决了从两个不同关节状态的物体的RGBD扫描构建未知关节对象的数字孪生体的问题。我们将问题分解为两个阶段，每个阶段都处理不同的方面。我们的方法首先在每个状态重建物体级形状，然后恢复包括部件分割和关节关联在内的基础关节模型。通过显式建模点级对应关系并利用来自图像、3D重建和运动学的线索，我们的方法与先前工作相比产生了更准确和稳定的结果。它还处理了多个可移动部件，不依赖于任何对象形状或结构先验。

    arXiv:2404.01440v1 Announce Type: cross  Abstract: We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt
    
[^3]: 用于治疗效果预测的图神经网络

    Graph Neural Networks for Treatment Effect Prediction

    [https://arxiv.org/abs/2403.19289](https://arxiv.org/abs/2403.19289)

    提出了一种图神经网络来减少治疗效果预测所需的训练集大小，有效利用电子商务数据的图结构，为治疗效果预测带来新的可能性

    

    在电子商务中估计因果效应往往涉及昂贵的治疗分配，这在大规模设置中可能是不切实际的。利用机器学习来预测这种治疗效果而无需实际干预是减少风险的一种标准做法。然而，现有的治疗效果预测方法往往依赖于大规模实验构建的训练集，因此从根本上存在风险。在这项工作中，我们提出了一种图神经网络，以减少所需的训练集大小，依赖于电子商务数据中常见的图。具体地，我们将问题视为具有有限数量标记实例的节点回归，开发了一个类似于先前因果效应估计器的双模型神经架构，并测试了不同的消息传递层进行编码。此外，作为额外步骤，我们将模型与获取函数相结合，以引导信息传递。

    arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
    
[^4]: Synth$^2$: 用合成标题和图像嵌入提升视觉-语言模型

    Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings

    [https://arxiv.org/abs/2403.07750](https://arxiv.org/abs/2403.07750)

    这项研究提出了一种新方法，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。

    

    高质量的人工标记图像标题数据集的创建在视觉-语言模型（VLMs）的发展中构成了一个重大瓶颈。我们提出了一种新方法，利用大型语言模型（LLMs）和图像生成模型的优势，为高效有效的VLM训练创建合成图像-文本对。我们的方法利用预训练的文本到图像模型，从LLM生成的标题开始合成图像嵌入。然后，这些合成对用于训练VLM。大量实验证明，通过合成数据训练的VLM在图像字幕方面表现出可比较的性能，而所需的数据量仅为仅使用人类注释数据训练的模型的一小部分。特别是，通过合成数据集的增强，我们超过基线17%。此外，我们展示，在图像嵌入空间合成比在像素空间中快25%。

    arXiv:2403.07750v1 Announce Type: cross  Abstract: The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel sp
    
[^5]: 通过自表达图结构重建对图数据进行压缩

    Graph Data Condensation via Self-expressive Graph Structure Reconstruction

    [https://arxiv.org/abs/2403.07294](https://arxiv.org/abs/2403.07294)

    通过自表达图结构重建的方法解决了图数据压缩中的问题

    

    随着训练大规模图神经网络（GNNs）需求的增加，图数据压缩已经成为在训练阶段减轻存储和时间成本的关键技术。它旨在将原始大规模图压缩为一个更小的合成图，同时保留训练下游GNN所需的基本信息。然而，现有方法要么集中于仅优化节点特征，要么努力独立学习节点特征和图结构生成器。它们无法明确利用原始图结构的信息，并未能为合成数据集构建可解释的图结构。为了解决这些问题，我们引入了一种名为\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})的新型框架。我们的方法突出之处在于

    arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
    
[^6]: Sora作为AGI世界模型？关于文本到视频生成的完整调查

    Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation

    [https://arxiv.org/abs/2403.05131](https://arxiv.org/abs/2403.05131)

    对文本到视频生成技术的发展进行了详细调查, 着重介绍了从传统生成模型到尖端Sora模型的转变，强调了可扩展性和通用性的发展。

    

    arXiv:2403.05131v1 公告类型: 新摘要: 文本到视频生成标志着生成式人工智能不断发展领域中的重要前沿，整合了文本到图像合成、视频字幕和文本引导编辑的进展。本调查对文本到视频技术的发展进行了批判性审视，重点关注传统生成模型向尖端Sora模型转变的过程，突出了可扩展性和通用性的发展。区别于以往作品的分析，我们深入探讨了这些模型的技术框架和演化路径。此外，我们还深入探讨了实际应用，并解决了伦理和技术挑战，如无法执行多实体处理、理解因果关系学习、理解物理互动、感知物体缩放和比例以及对抗物体幻觉，这也是生成模型中长期存在的问题。

    arXiv:2403.05131v1 Announce Type: new  Abstract: Text-to-video generation marks a significant frontier in the rapidly evolving domain of generative AI, integrating advancements in text-to-image synthesis, video captioning, and text-guided editing. This survey critically examines the progression of text-to-video technologies, focusing on the shift from traditional generative models to the cutting-edge Sora model, highlighting developments in scalability and generalizability. Distinguishing our analysis from prior works, we offer an in-depth exploration of the technological frameworks and evolutionary pathways of these models. Additionally, we delve into practical applications and address ethical and technological challenges such as the inability to perform multiple entity handling, comprehend causal-effect learning, understand physical interaction, perceive object scaling and proportioning, and combat object hallucination which is also a long-standing problem in generative models. Our c
    
[^7]: 通过标记级别的不确定性量化检验大型语言模型的输出

    Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification

    [https://arxiv.org/abs/2403.04696](https://arxiv.org/abs/2403.04696)

    提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。

    

    大型语言模型(LLMs)以产生错误的声明而臭名昭著。这种幻觉可能很危险，因为在生成的文本中偶尔出现的事实不准确可能会被整体上是事实的文本掩盖，这使得用户极其难以发现。利用LLMs的当前服务通常不提供检测不可靠生成的方式。在这里，我们旨在弥补这一空白。具体而言，我们提出了一种基于标记级别的不确定性量化的新型事实核查和幻觉检测流程。不确定性分数利用了神经网络或其层输出中包含的信息来检测不可靠的预测，并我们展示它们可以用于核查LLM输出中的各种声明。此外，我们提出了一种新型的标记级别不确定性量化方法，消除了对事实提出怀疑的影响。

    arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
    
[^8]: 永不停止的具身机器人学习

    Never-Ending Embodied Robot Learning

    [https://arxiv.org/abs/2403.00336](https://arxiv.org/abs/2403.00336)

    提出了一种具身机器人学习代理NBCagent，通过技能特定的演化规划器和技能共享的语义渲染模块，实现从视觉观测中连续学习新的机器人操作技能知识。

    

    依赖于大型语言模型（LLM），具身机器人可以通过强大的泛化能力，从视觉观测中执行复杂的多模态机器人操作任务。然而，大多数视觉行为克隆代理在适应一系列具有挑战性的未见任务时，会遭受操纵性能下降以及技能知识遗忘的困扰。在本研究中，我们通过NBCagent在具身机器人中探讨了上述挑战，这是一种开创性的、以语言为条件的永不停止行为克隆代理，可以不断从特定技能和共享技能属性中学习新的机器人操作技能的观察知识。具体来说，我们建立了一个特定技能不断演化的规划器来进行知识解耦，这可以从潜在和低秩空间中不断向我们的NBCagent代理嵌入新的技能特定知识。与此同时，我们提出了一个技能共享语义渲染模块和一个技能共享表示区分

    arXiv:2403.00336v1 Announce Type: cross  Abstract: Relying on large language models (LLMs), embodied robots could perform complex multimodal robot manipulation tasks from visual observations with powerful generalization ability. However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks. We here investigate the above challenge with NBCagent in embodied robots, a pioneering language-conditioned Never-ending Behavior-Cloning agent, which can continually learn observation knowledge of novel robot manipulation skills from skill-specific and skill-shared attributes. Specifically, we establish a skill-specific evolving planner to perform knowledge decoupling, which can continually embed novel skill-specific knowledge in our NBCagent agent from latent and low-rank space. Meanwhile, we propose a skill-shared semantics rendering module and a skill-shared representation disti
    
[^9]: Agent-Pro: 通过策略级别反思和优化学习进化

    Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization

    [https://arxiv.org/abs/2402.17574](https://arxiv.org/abs/2402.17574)

    Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。

    

    大型语言模型表现出在各种任务中具有强大问题解决能力。然而，大多数基于LLM的代理都是特定任务求解器，并具有复杂的提示工程，而不是能够通过互动学习和进化的代理。本文提出了Agent-Pro：一种基于LLM的代理，具有策略级别的反思和优化，可以从互动经验中学习丰富的专业知识，并逐渐提升其行为策略。具体来说，它涉及一个动态信念生成和反思过程，用于策略演化。

    arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
    
[^10]: 通过反向翻译防御LLMs免受越狱攻击

    Defending LLMs against Jailbreaking Attacks via Backtranslation

    [https://arxiv.org/abs/2402.16459](https://arxiv.org/abs/2402.16459)

    通过反向翻译来防御LLMs免受越狱攻击，将生成的反向翻译提示用于揭示原始提示的实际意图，提高了模型的安全性。

    

    尽管许多大型语言模型（LLMs）已经被训练成拒绝有害请求，但它们仍然容易受到越狱攻击的影响，这种攻击会重写原始提示以隐藏其有害意图。在本文中，我们提出了一种新方法，通过“反向翻译”来防御LLMs免受越狱攻击。具体来说，给定目标LLM从输入提示生成的初始响应，我们的反向翻译提示一个语言模型来推断可以导致该响应的输入提示。推断的提示称为反向翻译提示，倾向于揭示原始提示的实际意图，因为它是基于LLM的响应生成的，不是直接由攻击者操纵的。然后，我们再次在反向翻译提示上运行目标LLM，如果模型拒绝了反向翻译提示，则拒绝原始提示。我们解释了所提出的防御措施对其有效性的几个好处。

    arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
    
[^11]: REPLAY: 对稀疏轨迹进行位置预测的人类移动时间变化规律建模

    REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories

    [https://arxiv.org/abs/2402.16310](https://arxiv.org/abs/2402.16310)

    该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。

    

    位置预测是根据历史用户移动轨迹来预测用户位置的技术。为了应对真实世界用户移动轨迹的固有稀疏问题，时空上下文被证明是非常有用的。现有的解决方案主要是将位置之间的时空距离纳入到移动轨迹中，要么通过将其作为附加输入提供给递归神经网络（RNNs），要么通过利用它们来寻找有信息的过去隐藏状态进行预测。然而，这种基于距离的方法未能捕捉人类移动的时间变化规律，例如，人类移动在早晨通常比其他时间更有规律；这暗示了实际时间戳的有用性。基于这一背景，我们提出了REPLAY，是一种通用的RNN架构，旨在捕捉时间变化的人类移动时间规律以进行位置预测。

    arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
    
[^12]: 深度神经网络总是理解并且这就是原因

    Deep Networks Always Grok and Here is Why

    [https://arxiv.org/abs/2402.15555](https://arxiv.org/abs/2402.15555)

    深度神经网络存在延迟泛化和延迟鲁棒性现象，在各种实际环境中普遍存在，并基于新的局部复杂度度量提供了解释。

    

    Grokking，或者延迟泛化，是指深度神经网络（DNN）在达到接近于零的训练误差后很长时间内才发生泛化的现象。先前的研究报告了在特定受控环境中出现grokking的情况，例如使用大范数参数初始化的DNN或者在算法数据集上训练的transformers。我们展示了grokking实际上更加普遍，并且在广泛的实际环境中呈现，例如在CIFAR10上训练的卷积神经网络（CNN）或者在Imagenette上训练的Resnet。我们引入了延迟鲁棒性的新概念，即DNN在插值和/或泛化之后对抗示例进行理解并变得鲁棒。我们针对DNN的输入-输出映射的局部复杂度提出了出现延迟泛化和延迟鲁棒性的解释。我们的局部复杂度衡量了DNN输入-输出映射的复杂程度。

    arXiv:2402.15555v1 Announce Type: cross  Abstract: Grokking, or delayed generalization, is a phenomenon where generalization in a deep neural network (DNN) occurs long after achieving near zero training error. Previous studies have reported the occurrence of grokking in specific controlled settings, such as DNNs initialized with large-norm parameters or transformers trained on algorithmic datasets. We demonstrate that grokking is actually much more widespread and materializes in a wide range of practical settings, such as training of a convolutional neural network (CNN) on CIFAR10 or a Resnet on Imagenette. We introduce the new concept of delayed robustness, whereby a DNN groks adversarial examples and becomes robust, long after interpolation and/or generalization. We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on a new measure of the local complexity of a DNN's input-output mapping. Our local complexity measures the d
    
[^13]: NeuralThink: 在一般任务中进行外推的算法综合

    NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks

    [https://arxiv.org/abs/2402.15393](https://arxiv.org/abs/2402.15393)

    NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。

    

    虽然机器学习方法擅长模式识别，但在可扩展的算法方式上处理复杂的推理任务时仍然面临困难。最近的深度思维方法展现了学习可以外推的算法的潜力：在较小的环境中学习并在较大的环境中执行学到的算法。然而，这些工作局限于对称任务，即输入和输出的维度相同。为了填补这一空白，我们提出了 NeuralThink，一种新的递归架构，可以一贯地对对称和不对称任务进行外推，其中输入和输出的维度不同。我们提供了一个新颖的不对称任务外推基准。我们展示了 NeuralThink 在稳定地从较小的训练规模对大观测进行外推方面一直优于之前的最先进的深度思维架构。

    arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
    
[^14]: DyVal 2: 元探测代理动态评估大型语言模型

    DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents

    [https://arxiv.org/abs/2402.14865](https://arxiv.org/abs/2402.14865)

    本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。

    

    大型语言模型（LLMs）的评估引起了社区的极大关注，因为存在数据污染问题。现有工作设计了使用针对特定任务的明确定义算法的评估协议，这些协议无法轻松扩展到不同的场景。此外，当前的评估基准只能提供整体基准结果，不能支持对LLMs能力进行细粒度和多方面的分析。在本文中，我们提出了元探测代理（MPA），这是一种受心理测量学启发的通用动态评估协议，用于评估LLMs。 MPA 是 DyVal 2 的关键组件，自然地扩展了先前的 DyVal。 MPA 设计了探测和评判代理，以自动将原始评估问题转化为一个新问题，遵循心理测量理论在三个基本认知能力上的应用: 语言理解、问题解决和领域知识。

    arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
    
[^15]: 连续葡萄糖监测和维护的神经控制系统

    Neural Control System for Continuous Glucose Monitoring and Maintenance

    [https://arxiv.org/abs/2402.13852](https://arxiv.org/abs/2402.13852)

    引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，实时动态调整胰岛素输送，增强葡萄糖优化，最大化效率并确保个性化护理。

    

    精确的葡萄糖水平管理对于糖尿病患者至关重要，可以避免严重并发症。本研究引入了一种新颖的神经控制系统，用于连续葡萄糖监测和维护，利用微分预测控制。我们的系统受到复杂神经策略和可区分建模的指导，实时动态调整胰岛素输送，增强葡萄糖优化。这种端到端方法最大化效率，确保个性化护理和改善健康结果，如经验发现所证实。

    arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
    
[^16]: 示范对多语境学习的影响：多维分析

    The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis

    [https://arxiv.org/abs/2402.12976](https://arxiv.org/abs/2402.12976)

    通过多维分析发现，示范的有效性在多语种上下文学习中存在显著差异，其中部分模型对示范质量不敏感，而精心设计的模板可以消除对示范的依赖。

    

    在上下文学习中，示范经常被用作推理策略，在此策略中，大型语言模型仅使用少量标记的示范来解决任务，而无需进行任何参数更新。与单语言（英语）上下文学习的研究相比，多语种上下文学习尚未得到充分探讨，我们缺乏对该环境中示范作用的深入理解。为了填补这一空白，我们对多语境学习进行了多维分析，实验采用了来自不同模型家族的5个模型，涵盖了包括分类和生成任务在内的9个数据集，覆盖了56种类型上不同的语言。我们的结果表明，示范的有效性在模型、任务和语言之间存在显著差异。我们还发现，Llama 2-Chat、GPT-3.5和GPT-4对示范质量的敏感度较低。相反，精心设计的模板往往会消除一些模型对示范的益处。

    arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t
    
[^17]: EmoBench: 评估大型语言模型的情感智能

    EmoBench: Evaluating the Emotional Intelligence of Large Language Models

    [https://arxiv.org/abs/2402.12071](https://arxiv.org/abs/2402.12071)

    EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。

    

    近年来，大型语言模型（LLMs）的快速发展凸显了需要稳健、全面和具有挑战性的基准测试的重要性。然而，对它们的情感智能（EI）进行评估的研究相当有限。现有的基准测试存在两个主要缺点：首先，它们主要关注情感识别，忽视了情感调节等重要的情感智能能力，而情感理解则促进情感; 其次，它们主要基于现有数据集构建，这些数据集包含频繁模式、明确信息和注释错误，导致评估不可靠。我们提出了EmoBench，这是一个基准测试，借鉴了已建立的心理理论，并为机器EI提出了综合定义，包括情感理解和情感应用。EmoBench包括一组400个用英语和中文手工制作的问题，经过精心设计，需要深入推理。

    arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
    
[^18]: ArtPrompt: 基于ASCII艺术的对齐LLMs越狱攻击

    ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs

    [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753)

    提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。

    

    安全对于大型语言模型（LLMs）的使用至关重要。已经开发了多种技术，如数据过滤和监督微调，以加强LLMs的安全性。然而，当前已知的技术假设用于对齐LLMs安全性的语料库仅由语义进行解释。然而，这一假设在现实应用中不成立，导致LLMs存在严重漏洞。本文提出了一种新颖的基于ASCII艺术的越狱攻击，并引入了一个全面的基准Vision-in-Text Challenge（ViTC）来评估LLMs在识别不能仅通过语义进行解释的提示的能力。我们展示了五个SOTA LLMs（GPT-3.5、GPT-4、Gemini、Claude和Llama2）在识别以ASCII艺术形式提供的提示方面存在困难。基于这一观察，我们开发了

    arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
    
[^19]: GNNavi：通过图神经网络导航大型语言模型中的信息流

    GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network

    [https://arxiv.org/abs/2402.11709](https://arxiv.org/abs/2402.11709)

    GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。

    

    大型语言模型（LLMs）在接收示范输入时表现出强大的上下文学习能力（ICL）。然而，微调仍然至关重要以进一步增强其适应性。基于提示的微调方法在数据稀缺情况下证明是有效的微调方法，但对计算资源的高需求限制了其实用性。我们通过引入基于提示的参数高效微调（PEFT）方法来解决这个问题。GNNavi利用了有关ICL信息流动态的见解，表明标签词在提示中作为信息传播的锚点。GNNavi利用图神经网络（GNN）层精确地引导信息流的汇聚和分布，在处理提示时将期望的信息流硬编码到GNN中。我们在使用GPT-2和Llama2进行文本分类任务的实验中发现，GNNavi超越了标准提示式微调的性能。

    arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba
    
[^20]: CHEMREASONER：使用量子化学反馈在大型语言模型的知识空间中进行启发式搜索

    CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback

    [https://arxiv.org/abs/2402.10980](https://arxiv.org/abs/2402.10980)

    通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索

    

    arXiv:2402.10980v1 类型公告：跨领域 摘要：发现新的催化剂对于设计新的更高效的化学过程至关重要，以实现向可持续未来的过渡。我们引入了一种人工智能引导的计算筛选框架，将语言推理与基于量子化学的三维原子表示的反馈统一起来。我们的方法将催化剂发现构建为一个不确定环境，其中一个代理通过大型语言模型（LLM）推导的假设与基于原子图神经网络（GNN）的反馈的迭代组合，积极搜索高效催化剂。在中间搜索步骤确定的催化剂经过基于空间定向、反应途径和稳定性的结构评估。基于吸附能和势垒的评分函数引导在LLM的知识空间中向能量有利、高效的催化剂探索。我们引入了可以自动规划的方法

    arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
    
[^21]: 细微之线：通过话语主题识别机器生成的文本

    Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs

    [https://arxiv.org/abs/2402.10586](https://arxiv.org/abs/2402.10586)

    本文探讨了如何通过研究文本中的话语特征来区分人类创作和机器生成的文本，引入了一种新颖的方法来揭示这些特征，并发现人类写作在结构上更为多样化。

    

    随着大型语言模型（LLM）的出现，人类创作和机器生成的文本之间的界限变得日益模糊。本文探讨了识别人类撰写的文本中可辨识和独特的语言特性的研究，特别是揭示文本在表面结构之外的潜在话语结构。引入了一种新颖的方法论，我们利用层次化解析树和递归超图来揭示LLM和人类生成的文本中的独特话语模式。实证研究结果表明，尽管LLM和人类生成的文本都受特定领域的影响而产生不同的话语模式，但人类撰写的文本表现出更多的结构变异性，反映了不同领域人类写作的微妙性质。值得注意的是，引入层次话语特征可以增强二元分类器在区分人类生成和机器生成文本方面的整体性能。

    arXiv:2402.10586v1 Announce Type: new  Abstract: With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between huma
    
[^22]: 通过专家加权来衡量和减少LLM在没有黄金标准答案的情况下的虚构

    Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting

    [https://arxiv.org/abs/2402.10412](https://arxiv.org/abs/2402.10412)

    提出了一种名为FEWL的幻觉度量方法，通过对LLM答案进行加权评估事实性，适用于没有黄金标准答案的情况。

    

    LLM幻觉，即生成事实不正确但看似令人信服的答案，目前是LLM可信度和可靠性的主要威胁。解决这一复杂问题的第一步是对其进行衡量。然而，现有的幻觉度量标准需要具有具有黄金标准答案的基准数据集，即人类编写的“最佳”或“正确”答案。这种要求使幻觉测量成本高昂，并容易出现人为误差。在这项工作中，我们提出了通过加权LLM对事实性进行评估（FEWL），这是第一个专门为金标准答案缺失时设计的幻觉度量标准。FEWL利用了现成的LLM答案作为黄金标准答案的代理。关键挑战是如何有效地量化参考LLM的专业知识。我们展示FEWL具有一定的理论保证，并在实证中证明它更准确。度量虚构。

    arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
    
[^23]: SafeDecoding: 通过安全感知解码防御越狱攻击

    SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding

    [https://arxiv.org/abs/2402.08983](https://arxiv.org/abs/2402.08983)

    本文提出了一种名为SafeDecoding的解决方案，通过安全感知解码策略来防御大型语言模型（LLMs）的越狱攻击。该策略可以生成对用户查询有益且无害的响应，有效缓解了LLMs安全性威胁。

    

    随着大型语言模型（LLMs）越来越多地应用于代码生成和聊天机器人辅助等现实应用中，人们为了使LLM的行为与人类价值观保持一致，包括安全性在内做出了大量努力。越狱攻击旨在引发LLM的非预期和不安全行为，仍然是LLM安全性的重要威胁。本文旨在通过引入SafeDecoding来防御LLM的越狱攻击，这是一种安全感知的解码策略，用于生成对用户查询有益且无害的响应。我们在开发SafeDecoding时的洞察力基于观察到，即使代表有害内容的标记的概率超过代表无害响应的标记的概率，安全免责声明仍然出现在按概率降序排序的标记中的前几个。这使我们能够通过识别安全免责声明并增强其良性影响力来减轻越狱攻击。

    arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
    
[^24]: COLD-Attack: 用于具有隐秘性和可控性的LLM越狱

    COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability

    [https://arxiv.org/abs/2402.08679](https://arxiv.org/abs/2402.08679)

    本文提出了COLD-Attack框架，旨在实现具有隐秘性和可控性的LLM越狱。通过建立可控文本生成与攻击生成之间的关联，采用了能量限制解码与Langevin动力学算法，使得在不同的控制要求下搜索对抗性LLM攻击成为可能。

    

    最近对大型语言模型（LLMs）进行越狱的注意力越来越多。为了全面评估LLM的安全性，有必要考虑具有不同属性的越狱，例如上下文连贯性以及情感/风格变化，因此研究可控性越狱是有益的，即如何对LLM攻击进行控制。在本文中，我们正式形式化了可控性攻击生成问题，并建立了该问题与可控文本生成之间的新型关联，这是自然语言处理中一个被广泛探索的主题。基于这种关联，我们改进了能量限制解码与Langevin动力学（COLD）的算法，这是一种在可控文本生成中的高效算法，并引入了COLD-Attack框架，该框架统一且自动化地搜索各种控制要求下的对抗性LLM攻击，例如流畅性、隐秘性、情感和左右连贯性。

    Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
    
[^25]: 在非稳态环境中进行通用规划和学习的认识性探索

    Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings

    [https://arxiv.org/abs/2402.08145](https://arxiv.org/abs/2402.08145)

    本文介绍了一种在非稳态环境中进行通用规划和学习的方法，通过认识性探索填补代理的知识空白，并且利用收集到的数据学习通用的概率模型来解决不断变化的任务。实验证明这种方法在非稳态环境中比传统方法更有效。

    

    本文介绍了一种在使用关系表示的非稳态随机环境中进行连续规划和模型学习的新方法。这样的能力对于在不确定、不断发展的现实世界中部署顺序决策系统至关重要。在未知（且非稳态）转换系统和不断变化的任务中工作时，提出的框架模拟了代理状态知识的空白，并将其用于开展专注、调查性的探索。使用这些探索收集的数据用于学习通用的概率模型，以解决当前的任务，尽管环境动态不断变化。对几个基准领域进行的实证评估表明，这种方法在非稳态环境中的样本复杂度方面明显优于规划和强化学习基准。理论结果表明，系统能够回归到表现出理想的收敛性。

    This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings. Theoretical results show that the system reverts to exhibit desirable convergence pr
    
[^26]: 人本智能产品原型设计中的无代码AutoML：概念框架、潜力与限制

    Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations

    [https://arxiv.org/abs/2402.07933](https://arxiv.org/abs/2402.07933)

    本文评估了无代码AutoML作为解决人工智能产品原型设计中挑战的方案，提出了一个概念框架，通过案例研究证实了其对非专家的支持潜力。无代码AutoML的战略整合有助于实现可访问和可解释的原型设计，对于学术界、管理者和决策者具有益处。

    

    本文评估了无代码AutoML作为解决人工智能产品原型设计中的挑战的方案，这些产品的特点是不可预测性和对非专家的不可访问性，并提出了一个概念框架。人工智能产品的复杂性阻碍了无缝执行和跨学科合作，这对于人本智能产品至关重要。与行业和创新相关，它影响战略决策和投资风险的降低。目前的方法对人工智能产品想法的潜力和可行性提供了有限的见解。本研究采用设计科学研究的方法，识别挑战，并通过提出无代码AutoML的概念框架来解决这些挑战。案例研究证实了其对非专家的支持潜力，提供了一种结构化的人工智能产品开发方法。该框架有助于实现可访问和可解释的原型设计，使学术界、管理者和决策者受益。无代码AutoML的战略整合

    This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
    
[^27]: 通过对比预训练，改善多领域B2B云解决方案匹配

    Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training

    [https://arxiv.org/abs/2402.07076](https://arxiv.org/abs/2402.07076)

    通过对比预训练和多领域匹配结构，我们提出了CAMA框架来改善B2B云解决方案的匹配问题，克服了复杂特征和有限数据的挑战。

    

    云解决方案在技术行业中非常受欢迎，因为它们提供了一系列服务和工具来解决特定问题。然而，尽管它们被广泛使用，但解决方案提供商的销售团队仍然面临一个复杂的业务问题，即找到适合特定目标解决方案的合适的公司客户，现有的匹配系统尚未能够充分解决这个问题。在这项工作中，我们研究了B2B解决方案匹配问题，并确定了该场景的两个主要挑战：(1) 复杂多领域特征的建模和(2) 有限、不完整和稀疏的交易数据。为了解决这些挑战，我们提出了一个框架CAMA，它以分层多领域匹配结构为主干，并通过三种数据增强策略和对比预训练目标来弥补可用数据的不完善之处。通过对一个真实数据集进行大量实验，我们证明了...

    Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstra
    
[^28]: 与语言模型的反馈循环推动上下文内奖励欺骗

    Feedback Loops With Language Models Drive In-Context Reward Hacking

    [https://arxiv.org/abs/2402.06627](https://arxiv.org/abs/2402.06627)

    与语言模型的反馈循环可能导致上下文内奖励欺骗（ICRH），即语言模型在测试时在优化目标的同时却产生负面副作用。这项研究确定了两个导致ICRH的过程：输出优化和策略优化。

    

    语言模型对外部世界产生影响：它们查询可以读写网页的API，生成能够影响人类行为的内容，以及作为自主代理运行系统命令。这些互动形成了反馈循环：语言模型的输出影响世界，反过来又影响后续的语言模型输出。在这项工作中，我们展示了反馈循环可能导致上下文内奖励欺骗(ICRH)，即测试时的语言模型在优化（可能隐含的）目标的同时，产生负面副作用。例如，考虑一个被部署用于增加Twitter参与度的语言模型代理；语言模型可能在上下文窗口中检索其以前的推文，并使推文更具争议性，从而增加参与度，但也增加了有毒性。我们确定并研究了导致ICRH的两个过程：输出优化和策略优化。对于这些过程，静态数据集上的评估是不足够的-他们无法捕捉到反馈效应，也不能捕捉到最有害的行为。为此，我们提供了...

    Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
    
[^29]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^30]: 使用ODE方法进行带有马尔可夫噪声的随机逼近和强化学习

    The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise

    [https://arxiv.org/abs/2401.07844](https://arxiv.org/abs/2401.07844)

    本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。

    

    随机逼近是一类通过迭代、增量和随机更新向量的算法，包括随机梯度下降和时序差分学习。分析随机逼近算法的一个主要挑战是确保其稳定性，即证明随机向量迭代几乎必定有界。本文将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，大大提高了其在强化学习中的适用性，特别是那些具有线性函数逼近和资格迹的离策略强化学习算法。我们的分析的核心在于少数函数的渐进变化速率下降，这一点由大数定律和常用的V4 Lyapunov漂移条件隐含，并在马尔可夫链是有限且不可约时显然成立。

    Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
    
[^31]: 在神经网络模型中协调共享与特定上下文信息对潜在因果的作用

    Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes

    [https://arxiv.org/abs/2312.08519](https://arxiv.org/abs/2312.08519)

    LCNet是一个神经网络模型，能够通过学习存储共享结构，同时使用上下文模块表示特定上下文结构，成功实现了在不同任务中提取共享结构并避免灾难性干扰的功能；并且能够捕捉人类数据中的课程效应。

    

    已经提出，当处理一系列事件时，人类会根据推断的潜在因果（LCs）来划分他们的经验，以支持依赖于上下文的学习。然而，当共享结构存在于不同上下文中时，如何同时实现LCs的“分裂”和学习共享结构仍不清楚。本文介绍了潜在因果网络（LCNet），这是一个LC推断的神经网络模型。通过学习，它自然地储存网络权重中跨任务共享的结构。此外，它利用一个上下文模块表示特定上下文结构，由贝叶斯非参数推理算法控制，为每个推断的LC分配一个唯一的上下文向量。通过三个模拟实验，我们发现LCNet能够1)在功能学习任务中提取跨LC的共享结构，同时避免灾难性干扰，2)捕捉关于课程效应的人类数据。

    arXiv:2312.08519v2 Announce Type: replace-cross  Abstract: It has been proposed that, when processing a stream of events, humans divide their experiences in terms of inferred latent causes (LCs) to support context-dependent learning. However, when shared structure is present across contexts, it is still unclear how the "splitting" of LCs and learning of shared structure can be simultaneously achieved. Here, we present the Latent Cause Network (LCNet), a neural network model of LC inference. Through learning, it naturally stores structure that is shared across tasks in the network weights. Additionally, it represents context-specific structure using a context module, controlled by a Bayesian nonparametric inference algorithm, which assigns a unique context vector for each inferred LC. Across three simulations, we found that LCNet could 1) extract shared structure across LCs in a function learning task while avoiding catastrophic interference, 2) capture human data on curriculum effects 
    
[^32]: CLOMO: 带有大型语言模型的反事实逻辑修改

    CLOMO: Counterfactual Logical Modification with Large Language Models

    [https://arxiv.org/abs/2311.17438](https://arxiv.org/abs/2311.17438)

    本研究探索了大型语言模型的反事实推理能力，并引入了反事实逻辑修改任务和相应的评估指标。实验结果表明，大型语言模型展现出显著的逻辑能力。

    

    本研究探索了大型语言模型（LLMs）的反事实推理能力。我们的主要目标是培养LLMs内的反事实思维过程，并对其有效性进行严格评估。具体而言，我们引入了一项新任务，即反事实逻辑修改（CLOMO），以及一个高质量的人工注释基准。在这个任务中，LLMs必须熟练地改变给定的论证文本，以保持预定的逻辑关系。为了有效评估生成模型的反事实能力，我们提出了一种创新的评估指标，逻辑感知的反事实分数，直接评估LLMs的自然语言输出，而不是将任务建模为多项选择问题。分析表明，所提出的自动评估指标与人类偏好很好地一致。我们的实验结果表明，尽管LLMs展示了显着的逻辑能力。

    arXiv:2311.17438v3 Announce Type: replace-cross Abstract: In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for log
    
[^33]: Fusion-Eval: 将评估器与LLMs集成

    Fusion-Eval: Integrating Evaluators with LLMs

    [https://arxiv.org/abs/2311.09204](https://arxiv.org/abs/2311.09204)

    Fusion-Eval是一种创新方法，利用LLMs整合不同辅助评估器的见解，极大提升自然语言系统评估的有效性。

    

    自然语言系统的评估在自然语言理解和高级推理领域面临着重大挑战。本文介绍了一种名为“Fusion-Eval”的创新方法，利用大型语言模型（LLMs）来整合来自各种辅助评估器的见解。每个评估器专门负责评估响应的不同方面。这种独特策略使得Fusion-Eval能够有效地跨越各种任务和标准，增强现有评估方法的效果。在SummEval上，Fusion-Eval与人类之间的系统级Kendall-Tau相关性达到0.962，在TopicalChat上的轮级Spearman相关性达到0.744，远高于基准方法。这些结果突显了Fusion-Eval在自然语言系统评估领域的巨大潜力。

    arXiv:2311.09204v2 Announce Type: replace-cross  Abstract: Evaluating natural language systems poses significant challenges, particularly in the realms of natural language understanding and high-level reasoning. In this paper, we introduce "Fusion-Eval", an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. This unique strategy enables Fusion-Eval to function effectively across a diverse range of tasks and criteria, enhancing the effectiveness of existing evaluation methods. Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval's significant potential in the realm of natural language system evaluation.
    
[^34]: CodeScope:一个基于执行的多语言多任务多维基准用于评估LLMs在代码理解和生成方面的能力

    CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation

    [https://arxiv.org/abs/2311.08588](https://arxiv.org/abs/2311.08588)

    CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。

    

    大型语言模型（LLMs）在编码相关任务上表现出色，特别是在帮助人类编程和促进编程自动化方面。然而，现有的用于评估LLMs的代码理解和生成能力的基准存在严重的限制。首先，大部分基准存在缺陷，因为它们只关注于狭窄范围内的流行编程语言和特定任务，而实际软件开发场景需要实现多语言编程环境以满足各种需求。实际编程实践还强烈期望多任务设置，以全面和稳健地测试LLMs的编码能力。其次，大部分基准也未考虑生成代码的可执行性和执行结果的一致性。为了弥补现有基准与实际应用期望之间的差距，我们引入了CodeScope。

    Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope
    
[^35]: Lie神经元：半单Lie代数的伴随等变神经网络

    Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras

    [https://arxiv.org/abs/2310.04521](https://arxiv.org/abs/2310.04521)

    本文提出了一种Lie神经元网络，能够以任何半单Lie代数数据为输入，通过伴随操作使其具有等变性。通过推广向量神经元网络和引入新的层，该网络在各个领域具有广泛的适用性和竞争性能。

    

    本文提出了一种等变神经网络，将数据作为输入，该数据存在于任何半单Lie代数中。对应的群通过伴随操作作用于Lie代数上，使得我们提出的网络具有伴随等变性。我们的框架将简单的$\mathrm{SO}(3)$-等变网络——向量神经元从3维欧几里得空间推广到Lie代数空间，利用Killing形式的不变性质。此外，我们还提出了新颖的Lie括号层和几何通道混合层来扩展建模能力。实验在多个任务上对$\mathfrak{so}(3)$和$\mathfrak{sl}(3)$ Lie代数进行了验证，包括拟合等变和不变函数、学习系统动力学、点云配准和基于单应性的形状分类。我们提出的等变网络在各个领域都表现出广泛的适用性和竞争性能。

    This paper proposes an equivariant neural network that takes data in any semi-simple Lie algebra as input. The corresponding group acts on the Lie algebra as adjoint operations, making our proposed network adjoint-equivariant. Our framework generalizes the Vector Neurons, a simple $\mathrm{SO}(3)$-equivariant network, from 3-D Euclidean space to Lie algebra spaces, building upon the invariance property of the Killing form. Furthermore, we propose novel Lie bracket layers and geometric channel mixing layers that extend the modeling capacity. Experiments are conducted for the $\mathfrak{so}(3)$ and $\mathfrak{sl}(3)$ Lie algebras on various tasks, including fitting equivariant and invariant functions, learning system dynamics, point cloud registration, and homography-based shape classification. Our proposed equivariant network shows wide applicability and competitive performance in various domains.
    
[^36]: WebVoyager：使用大型多模态模型构建端到端的Web Agent

    WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])

    [http://arxiv.org/abs/2401.13919](http://arxiv.org/abs/2401.13919)

    WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。

    

    大型语言模型（LLMs）的进步引领了一个由真实世界中自主应用程序的发展所标志的新时代，推动了基于网络的高级代理的创新。现有的网络代理通常只处理一个输入模态，并且仅在简化的网络模拟器或静态的网络快照中进行评估，极大地限制了它们在真实场景中的适用性。为了填补这一差距，我们引入了WebVoyager，一种创新的基于大型多模态模型（LMM）的Web代理，通过与真实网站进行交互，能够端到端地完成用户指令。此外，我们提出了一个新的Web代理评估协议，以解决开放式Web代理任务的自动评估挑战，利用了GPT-4V的强大多模态理解能力。我们通过收集来自15个广泛使用的网站的真实世界任务来创建一个新的基准来评估我们的代理。我们展示了WebVoyager实现了55.7％的任务成功率，显著地.....

    The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
    
[^37]: HyperSense: 加速超维度计算以用于智能传感器数据处理

    HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent Sensor Data Processing. (arXiv:2401.10267v1 [cs.AR])

    [http://arxiv.org/abs/2401.10267](http://arxiv.org/abs/2401.10267)

    HyperSense是一个协同设计的硬件和软件系统，能够根据传感器数据中的物体存在预测有效地控制数据生成速率，通过使用低精度ADC减少冗余数据降低机器学习系统的成本，利用超维度计算的特点分析实时的低精度传感器数据，在处理噪声、以内存为中心和实时学习方面具有优势。该系统结合了高性能的物体检测软件和实时的硬件预测，引入了智能传感器控制的新概念。在软件和硬件评估中表现出卓越的性能。

    

    引入HyperSense，我们协同设计的硬件和软件系统根据传感器数据中的物体存在预测有效地控制模拟到数字转换器（ADC）模块的数据生成速率。针对不断增加的传感器数量和数据速率所带来的挑战，HyperSense使用高效的低精度ADC减少冗余的数字数据，降低了机器学习系统的成本。利用神经启发的超维度计算（HDC），HyperSense分析实时的原始低精度传感器数据，在处理噪声、以内存为中心和实时学习方面具有优势。我们提出的HyperSense模型将高性能的物体检测软件与实时的硬件预测结合起来，引入了智能传感器控制的新概念。全面的软件和硬件评估展示了我们解决方案的卓越性能，通过最高的曲线下面积（AUC）和最陡的接收器操作特性（ROC）曲线证明了这一点。

    Introducing HyperSense, our co-designed hardware and software system efficiently controls Analog-to-Digital Converter (ADC) modules' data generation rate based on object presence predictions in sensor data. Addressing challenges posed by escalating sensor quantities and data rates, HyperSense reduces redundant digital data using energy-efficient low-precision ADC, diminishing machine learning system costs. Leveraging neurally-inspired HyperDimensional Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data, offering advantages in handling noise, memory-centricity, and real-time learning.  Our proposed HyperSense model combines high-performance software for object detection with real-time hardware prediction, introducing the novel concept of Intelligent Sensor Control. Comprehensive software and hardware evaluations demonstrate our solution's superior performance, evidenced by the highest Area Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC
    
[^38]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^39]: 自我对比：通过不一致的求解视角获得更好的反思能力

    Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])

    [http://arxiv.org/abs/2401.02009](http://arxiv.org/abs/2401.02009)

    自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。

    

    大型语言模型（LLM）的反思能力引起了广泛关注。一种事后提示策略，例如反思和自我改进，根据自我评估或外部反馈来改善LLM的响应。然而，最近的研究表明，在没有外部反馈的情况下，LLM的内在反思是不稳定的。我们的调查揭示了自我评估反馈质量是关键瓶颈。我们发现LLM在自我评估时常常表现出过度自信或高度随机性，提供固执或不一致的反馈，导致反思能力不佳。为了解决这个问题，我们提出了自我对比的方法：它根据请求自适应地探索多样的求解视角，对比差异，并将这些差异总结为一个检查表，用于重新审视和消除差异。我们的方法赋予LLM多样的视角以减轻固执偏见。此外，差异指示了潜在的错误或固有的不确定性。

    The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
    
[^40]: WaveCoder: 广泛和多功能的改进指令调优与完善的数据生成

    WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.14187](http://arxiv.org/abs/2312.14187)

    本文提出了WaveCoder，一个广泛和多功能的改进指令调优模型，通过将指令数据分类并利用LLM框架生成多样的高质量指令数据，以提高调优模型的效果和泛化能力。

    

    近期的研究表明，在对高质量指令数据集进行调优后，生成的模型可以在广泛的任务上展现出令人印象深刻的能力。然而，现有的指令数据生成方法经常会产生重复数据，并且对数据质量的控制不够灵活。本文通过将指令数据分类为4个与代码相关的任务，扩展了指令调优的普适性，并提出了基于LLM的生成器-判别器数据处理框架，从开源代码中生成多样的、高质量的指令数据。因此，我们介绍了CodeOcean，一个包含4个通用代码相关任务的、共计20,000个指令实例的数据集，旨在增强指令调优的效果，并提高调优模型的泛化能力。随后，我们提出了WaveCoder，一个具有广泛和多功能的改进指令调优的Code LLM模型。

    Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enha
    
[^41]: LLMs对具身导航的发展

    The Development of LLMs for Embodied Navigation. (arXiv:2311.00530v1 [cs.AI])

    [http://arxiv.org/abs/2311.00530](http://arxiv.org/abs/2311.00530)

    本文概述了LLMs与具身智能在导航领域的共生关系，并评估了现有模型和数据集的优缺点。

    

    近年来，诸如生成预训练变压器（GPT）之类的大语言模型（LLMs）的快速发展引起了越来越多的关注，因为它们在各种实际应用中具有潜力。LLMs与具身智能的应用已成为一个重要的研究领域。在众多应用中，导航任务尤为引人注目，因为它们要求对环境有深入的理解和快速、准确的决策能力。LLMs可以通过利用其强大的语言和图像处理能力，增强具身智能系统在环境感知和决策支持方面的能力。本文全面总结了LLMs与具身智能之间在导航方面的共生关系，审视了最先进的模型、研究方法，并评估了现有的具身导航模型和数据集的优缺点。最后，本文阐明了LLMs在具身导航领域的创新和贡献。

    In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidat
    
[^42]: BioInstruct:用于生物医学自然语言处理的大型语言模型指令调整

    BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])

    [http://arxiv.org/abs/2310.19975](http://arxiv.org/abs/2310.19975)

    BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。

    

    大型语言模型通过在大量数据上进行预训练，然后进行特定领域的指令调整，在许多自然语言处理任务中取得了巨大成功。然而，在生物医学领域只发表了很少的指令。为了解决这个问题，我们引入了BioInstruct，这是一个定制的任务特定指令数据集，包含超过25,000个示例。通过使用三个人工筛选的指令样本，以GPT-4语言模型作为提示，精调大型语言模型，我们旨在优化其在生物医学自然语言处理中的性能。我们对LLaMA LLMs (1&2,7B&13B)进行了指令调整，并在生物医学自然语言处理应用中进行了评估，包括信息提取、问答和文本生成。我们还评估了指令如何对模型性能的贡献，使用了多任务学习原则。

    Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
    
[^43]: CompeteAI:理解基于大型语言模型的智能体竞争行为

    CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])

    [http://arxiv.org/abs/2310.17512](http://arxiv.org/abs/2310.17512)

    本文研究了基于大型语言模型的智能体之间的竞争行为。通过建立一个竞争环境并进行实验，发现竞争可以促使智能体进行转变和采取新策略，从而在社会和经济发展中发挥重要作用。

    

    大型语言模型（LLM）被广泛应用于完成不同任务，如个人助理或事件规划。虽然大多数工作都集中在智能体之间的合作与协作，但很少有研究探索另一个重要机制——竞争，它是社会和经济发展的推动力之一。本文旨在研究LLM智能体之间的竞争行为。我们首先提出一个通用框架来研究智能体之间的竞争。然后，我们使用GPT-4实现了一个实际的竞争环境，模拟了一个由餐馆智能体和顾客智能体组成的虚拟城镇。具体而言，餐馆智能体相互竞争以吸引更多顾客，这种竞争促使它们进行转变，比如培养新的运营策略。我们实验的结果揭示了从社会学习到马太效应等多个有趣发现，与现有的社会学和经济学理论相一致。

    Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and e
    
[^44]: 通过平衡教师和研究者的激励，探索适应性实验促进持续改进的机会

    Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])

    [http://arxiv.org/abs/2310.12324](http://arxiv.org/abs/2310.12324)

    适应性实验为持续课程改进提供了机会，通过动态部署最有效的条件以满足学生需求。

    

    随机实验比较不同教学策略的机会可以为教师的决策提供有用的经验证据。然而，传统实验缺乏清晰简明的使用数据快速增加实验学生获得最佳条件机会的途径。受领先科技公司在产品开发中使用机器学习和实验的启示，我们探讨了如何利用适应性实验来持续改进课程。在适应性实验中，不同的条件将被应用于学生身上，数据将被分析并用于改变未来学生的学习体验。可以使用机器学习算法来识别哪些行动可以更有希望改善学生的体验或结果。然后，该算法可以动态地将最有效的条件应用于未来的学生，从而更好地满足学生的需求。我们通过实例说明了这种方法的应用。

    Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the appr
    
[^45]: GTA：一种面向几何的多视图Transformer的注意力机制

    GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])

    [http://arxiv.org/abs/2310.10375](http://arxiv.org/abs/2310.10375)

    提出了一种面向几何的注意力机制（GTA），用于将几何结构编码为相对变换，从而改进了多视图Transformer的学习效率和性能。

    

    随着transformers对输入标记的排列具有等变性，对标记的位置信息进行编码对许多任务是必要的。然而，由于现有的位置编码方案最初是为自然语言处理任务设计的，对于通常在其数据中表现出不同结构特性的视觉任务来说，它们的适用性值得怀疑。我们认为现有的位置编码方案对于3D视觉任务来说是次优的，因为它们不尊重其底层的3D几何结构。基于这个假设，我们提出了一种面向几何的注意力机制，它将标记的几何结构编码为由查询和键值对之间的几何关系所确定的相对变换。通过在稀疏宽基线多视图设置中评估多个新颖视图合成（NVS）数据集，我们展示了我们的注意力机制——几何变换注意力（GTA）如何提高了最先进的Transformer的学习效率和性能。

    As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b
    
[^46]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^47]: 大规模语言模型对监督微调数据组合的影响

    How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05492](http://arxiv.org/abs/2310.05492)

    本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。

    

    大规模语言模型（LLMs）具备大量的预训练标记和参数，展现出数学推理、代码生成和指令跟随等能力。这些能力通过监督微调（SFT）进一步增强。开源社区已经研究了针对每种能力的临时SFT，而专有LLMs可以适用于所有能力。因此，研究如何通过SFT解锁多重能力变得重要。在本研究中，我们特别关注SFT过程中数学推理、代码生成和人类对齐能力之间的数据组合。从规模的角度，我们研究了模型能力与各种因素之间的关系，包括数据量、数据组合比例、模型参数和SFT策略。我们的实验发现不同的能力表现出不同的扩展模式，较大的模型通常在相同的数据量下表现出更优异的性能。数学推理和代码生成能力通过微调数据和模型参数的增加而获得显著的性能提升。

    Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
    
[^48]: 你仅关注屏幕：多模态动作链机器人

    You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])

    [http://arxiv.org/abs/2309.11436](http://arxiv.org/abs/2309.11436)

    本论文提出了一种名为Auto-UI的多模态动作链机器人，通过直接与界面交互，避免了环境解析或依赖于应用程序API的需要，并引入了动作链技术来帮助模型进行决策。

    

    自主用户界面（UI）机器人旨在通过与用户界面进行交互，实现任务自动化，无需手动干预。最近的研究探讨了利用大型语言模型（LLM）的能力，以在多样环境中有效参与。为了符合LLM的输入-输出要求，现有方法在沙盒环境中开发，依赖于外部工具和应用程序特定的API将环境解析为文本元素，并解释预测的动作。因此，这些方法常常受到推理效率低和错误传播风险的困扰。为了缓解这些挑战，我们引入了Auto-UI，一种多模态解决方案，它直接与界面交互，避免了对环境解析或依赖于应用程序相关的API的需求。此外，我们提出了一种动作链技术，利用一系列中间先前动作历史和未来动作计划，以帮助模型进行决策。

    Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
    
[^49]: 通过人工智能"生成"工作：在线劳动市场的经验证据

    "Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets. (arXiv:2308.05201v1 [cs.AI])

    [http://arxiv.org/abs/2308.05201](http://arxiv.org/abs/2308.05201)

    这项研究通过利用ChatGPT作为外生冲击，揭示了其对在线劳动市场的影响。结果显示，直接接触ChatGPT的任务和自由职业者的交易量显著下降，但适应新技术并提供增强人工智能的服务的自由职业者仍能获得利益。

    

    随着通用生成式人工智能的出现，对其对劳动市场的影响的兴趣不断增加。为了填补现有的实证空白，我们将ChatGPT的推出解释为一种外生冲击，并采用差异法来量化其对在线劳动市场中与文本相关的工作和自由职业者的影响。我们的结果显示，直接接触ChatGPT的任务和自由职业者的交易量显著下降。此外，这种下降在相对较高的过去交易量或较低的质量标准下尤为显著。然而，并非所有服务提供商都普遍经历了负面影响。随后的分析表明，在这个转型期间，能够适应新进展并提供增强人工智能技术的服务的自由职业者可以获得可观的利益。因此，虽然ChatGPT的出现有可能替代人力劳动

    With the advent of general-purpose Generative AI, the interest in discerning its impact on the labor market escalates. In an attempt to bridge the extant empirical void, we interpret the launch of ChatGPT as an exogenous shock, and implement a Difference-in-Differences (DID) approach to quantify its influence on text-related jobs and freelancers within an online labor marketplace. Our results reveal a significant decrease in transaction volume for gigs and freelancers directly exposed to ChatGPT. Additionally, this decline is particularly marked in units of relatively higher past transaction volume or lower quality standards. Yet, the negative effect is not universally experienced among service providers. Subsequent analyses illustrate that freelancers proficiently adapting to novel advancements and offering services that augment AI technologies can yield substantial benefits amidst this transformative period. Consequently, even though the advent of ChatGPT could conceivably substitute
    
[^50]: 揭示视频问答模型中联合多模态理解的幻象

    Revealing the Illusion of Joint Multimodal Understanding in VideoQA Models. (arXiv:2306.08889v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.08889](http://arxiv.org/abs/2306.08889)

    通过设计轻量级探针QUAG和替代方法QUAG-attention，发现视频问答模型在多模态理解方面存在幻象，即使在多模态损伤下仍能保持高性能，且用更少的计算量实现相似的性能。

    

    尽管视频问答（VideoQA）Transformer模型在标准基准测试中表现出竞争力，但其成功原因尚未完全理解。这些模型是否能够共同捕捉和利用视频和文本中丰富的多模态结构和动态性？或者它们仅仅是利用了捷径来获得高分？因此，我们设计了一个轻量级且非参数化的探针“QUAG”（QUadrant AveraGe），以对多模态表示进行批判性分析。QUAG通过在推理过程中系统地消除模型的耦合多模态理解来促进联合数据集-模型研究。令人惊讶的是，它表明即使在多模态损伤下，模型仍能保持高性能。我们将QUAG扩展为“QUAG-attention”，这是一个简化且表达能力较弱的自注意力替代方法。我们发现，带有QUAG-attention的模型在没有任何微调的情况下能够达到类似的性能，而且计算量显著减少。这些发现表明当前的VideoQA模型在理解多模态信息时存在一定的幻象。

    While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models jointly capture and leverage the rich multimodal structures and dynamics from video and text? Or are they merely exploiting shortcuts to achieve high scores? Hence, we design $\textit{QUAG}$ (QUadrant AveraGe), a lightweight and non-parametric probe, to critically analyze multimodal representations. QUAG facilitates combined dataset-model study by systematic ablation of model's coupled multimodal understanding during inference. Surprisingly, it demonstrates that the models manage to maintain high performance even under multimodal impairment. We extend QUAG to design "QUAG-attention", a simplistic and less-expressive replacement of self-attention. We find that the models with QUAG-attention achieve similar performance with significantly less mulops without any finetuning. These findings indicate that the current VideoQA b
    

