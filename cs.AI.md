# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation.](http://arxiv.org/abs/2305.09662) | Make-An-Animation 是一个用于文本条件下三维人体运动生成的模型，通过大规模数据集的训练得以弥补现有方法的局限，提高了运动生成的质量和多样性。 |
| [^2] | [Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage.](http://arxiv.org/abs/2305.09659) | 本论文提出了一个名为P2MPO的算法框架，用于解决基于鲁棒离线RL的问题。该框架结合了灵活的模型估计子例程和双重悲观的策略优化步骤，采用双重悲观性原则以克服模型偏移等问题。研究表明，在模型准确性的假设下，该框架在拥有良好的鲁棒部分覆盖数据的情况下是具备高效性的。 |
| [^3] | [Satisfiability-Aided Language Models Using Declarative Prompting.](http://arxiv.org/abs/2305.09656) | 本文提出了一种利用自动定理证明器和声明性任务规范的可满足性辅助语言建模方法，可以提高大型语言模型的推理能力。 |
| [^4] | [Prompt-Tuning Decision Transformer with Preference Ranking.](http://arxiv.org/abs/2305.09648) | 本研究提出了Prompt-Tuning DT算法，通过使用轨迹段作为prompt来指导RL agent获取环境信息，从而在具有挑战性的任务中实现了优秀表现。 |
| [^5] | [AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys.](http://arxiv.org/abs/2305.09620) | 本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。 |
| [^6] | [Towards Expert-Level Medical Question Answering with Large Language Models.](http://arxiv.org/abs/2305.09617) | 本研究提出了Med-PaLM2，通过结合基础LLM改进、医学领域微调和提示策略，并用新颖的集成精炼方法，实现了在MedQA数据集上达到86.5%的医学问答准确率，迈向医学专家级别的问答能力。 |
| [^7] | [Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow.](http://arxiv.org/abs/2305.09610) | 该论文提出了一种基于能量归一化流框架的生成模型，用于并发的在分布内误分类（IDM）和越界检测，可以扩展先前部署的分割模型而无需重新训练，并在测试中实现很好的结果。 |
| [^8] | [Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion.](http://arxiv.org/abs/2305.09600) | 本文提出了一种基于深度强化学习的方法来减少繁忙多车道高速公路交通拥堵，该方法可以学习适应性绕行策略，在减少拥堵和提高车速的同时，优化高速公路车道和周边局部收发路网的使用，实验证明其可以显著优化道路收发效率并减少总通行时间。 |
| [^9] | [Revisiting Proprioceptive Sensing for Articulated Object Manipulation.](http://arxiv.org/abs/2305.09584) | 本文探讨了在关节物体操作中使用本体感知信息的重要性，通过创建一个系统并进行初步测试，发现了其表现良好，对于机器人操作关节物体具有重要意义。 |
| [^10] | [UOR: Universal Backdoor Attacks on Pre-trained Language Models.](http://arxiv.org/abs/2305.09574) | 本文介绍了一种新的后门攻击方法UOR，可以自动选择触发器并学习通用输出表示，成功率高达99.3％，能够对多种预训练语言模型和下游任务实施攻击，且可突破最新的防御方法。 |
| [^11] | [Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs.](http://arxiv.org/abs/2305.09573) | 本文研究基于定性分析，对负责将AI伦理融入产品开发的技术工人进行了分析。结果显示，伦理企业家面临的三大障碍是： 在公司产品发布为中心的环境中，难以使伦理优先考虑；在以指标激励公司目标的情况下，伦理难以量化；频繁的团队重组使得获取知识和维护关系变得困难。 |
| [^12] | [Learning from Aggregated Data: Curated Bags versus Random Bags.](http://arxiv.org/abs/2305.09557) | 本文研究了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包，对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。 |
| [^13] | [EEG-based Sleep Staging with Hybrid Attention.](http://arxiv.org/abs/2305.09543) | 本文提出了一种基于混合注意力EEG睡眠分期（HASS）框架，采用时空注意力机制自适应地分配给予通道间和通道内的EEG片段权重，显著提高典型睡眠分期网络的性能。 |
| [^14] | [What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem.](http://arxiv.org/abs/2305.09535) | 这篇论文探讨了“连接谬误”作为公平性问题在人工智能领域中的重要性，并提出了一些问题，以帮助AI研究人员和从业者避免类似情况在未来中复现。 |
| [^15] | [Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction.](http://arxiv.org/abs/2305.09510) | 该论文提出了一种实现实时同时多物体三维形状重建、6DoF姿态估计和密集抓取预测的新方法，无需顺序感知和抓取规划步骤，具有快速推理且具有竞争力的性能。 |
| [^16] | [The Hardness of Reasoning about Probabilities and Causality.](http://arxiv.org/abs/2305.09508) | 该论文从计算复杂性的角度研究了能够完全表达概率量化推理和因果效应的 do-calculus 推理的形式语言的可满足性问题，并确立了这些问题的精确计算复杂度，该研究证明了在概率和因果推断中通常使用的标准语言的变种的算法限制更强。 |
| [^17] | [Efficient Computation of General Modules for ALC Ontologies (Extended Version).](http://arxiv.org/abs/2305.09503) | 该论文提出了一种从ALC本体中提取通用模块的方法，并且通过评估表明其可以在显着较短的时间内计算出，相比于最先进的方法可以得到更小的结果。 |
| [^18] | [Discrete Diffusion Probabilistic Models for Symbolic Music Generation.](http://arxiv.org/abs/2305.09489) | 本研究提出了使用离散扩散概率模型直接生成复调符号音乐。该模型表现出了最先进的样本质量，并允许在音符级别上进行灵活的插值。与此同时，我们还探讨了通过统计指标对音乐样本质量进行定量评估的限制。 |
| [^19] | [Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer.](http://arxiv.org/abs/2305.09480) | 本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。 |
| [^20] | [Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies.](http://arxiv.org/abs/2305.09477) | 解释型AI研究中的参与者样本量和推广问题限制了该领域对XAI系统可解释性的评估，需要更具包容性的用户研究原则来解决这些问题。 |
| [^21] | [Growing and Serving Large Open-domain Knowledge Graphs.](http://arxiv.org/abs/2305.09464) | 本文介绍了一种平台，它可以扩展和服务于大规模开放领域知识图谱，同时介绍了训练知识图嵌入的管道以及如何创建语义注释服务和推动开放领域知识抽取。 |
| [^22] | [Low-complexity deep learning frameworks for acoustic scene classification using teacher-student scheme and multiple spectrograms.](http://arxiv.org/abs/2305.09463) | 本文提出了一种采用教师-学生方案和多种谱图的低复杂度深度学习框架用于声景分类，获得了57.4%的最佳分类准确率，相比DCASE基线提高14.5%。 |
| [^23] | [A sequential transit network design algorithm with optimal learning under correlated beliefs.](http://arxiv.org/abs/2305.09452) | 提出一种结合序列公交网络设计和最优学习的算法，用于准确估计潜在出行需求，并规避设计与实际需求不一致的风险。 |
| [^24] | [About Evaluation of F1 Score for RECENT Relation Extraction System.](http://arxiv.org/abs/2305.09410) | 本文讨论了一个名为RECENT的关系抽取系统，作者在TACRED数据集上取得了之前的最新成果74.8的F1分数，但在更正错误和重新评估后其F1分数为65.16。 |
| [^25] | [Diffusion Dataset Generation: Towards Closing the Sim2Real Gap for Pedestrian Detection.](http://arxiv.org/abs/2305.09401) | 本文提出了一种使用扩散模型增强模拟数据集的方法，以缩小行人检测中的Sim2Real差距，该方法将生成的数据与模拟数据混合进行训练，在真实世界的数据中表现出更好的性能，平均精度提高高达27.3％。 |
| [^26] | [Capturing Emerging Complexity in Lenia.](http://arxiv.org/abs/2305.09378) | 研究人工生命平台Lenia，通过识别复杂新兴行为的度量标准和使用遗传算法产生不同行为的结果，以进化出更好的Lenia行为。 |
| [^27] | [Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring.](http://arxiv.org/abs/2305.09368) | 本论文提出了一种无监督序列到序列学习方法，用于自动评估多通道电阻抗血流动力学监测中心肺容积信号（CVS）的运动诱导可靠性降低。通过利用长短时记忆和变分自编码器结构，编码器 - 解码器模型可以捕捉到暂态 CVS 序列中存在的上下文知识。与现有方法相比，本方法不需要手动注释运动影响以及缺乏在 CVS 随时间出现上下文变化的情况下实现运动引起异常的显式机制，具有竞争性能。 |
| [^28] | [Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image.](http://arxiv.org/abs/2305.09333) | 本论文研究了基于提示的多模态视觉理解技术，通过分离语义信息提高对图像的理解，旨在推进图像识别和理解领域的研究。 |
| [^29] | [Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation.](http://arxiv.org/abs/2305.09330) | 推荐系统的公平性问题越来越引起人们的关注，尤其是用户公平性，已经提出了很多解决方案来减轻用户在使用推荐系统过程中体验到的歧视问题。 |
| [^30] | [BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling.](http://arxiv.org/abs/2305.09329) | 本文提出了一种新颖的神经主题模型，利用来自预训练语言模型BERT的上下文化词嵌入，可以在不使用任何BoW信息的情况下推断出文档的主题分布，并直接从上下文化词嵌入中推断出文档中每个单词的主题分布。实验结果表明，该模型优于仅依赖BoW表示和其他神经主题模型的现有最先进方法。 |
| [^31] | [Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings.](http://arxiv.org/abs/2305.09316) | 本文研究了使用图神经网络表示加强预训练语言模型对长篇科技论文的关键词提取。通过构建文本共现图并结合图表示和上下文化的PLM嵌入，我们展示了对于长篇文档，增强PLMs性能比现有技术的模型表现更加出色。 |
| [^32] | [Hybrid and Collaborative Passage Reranking.](http://arxiv.org/abs/2305.09313) | 该论文提出了一种名为HybRank的混合与协作的段落再排序方法，通过利用上游检索器的相似性度量实现段落协作，再利用稀疏和密集检索器的词汇和语义属性进行重新排序，该方法可以增强包括先前被重新排序的段落列表在内的任意段落列表，并在实验证明了性能稳定的提升。 |
| [^33] | [OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research.](http://arxiv.org/abs/2305.09304) | OmniSafe是一种基础设施，用于加速安全强化学习研究，帮助解决当代SafeRL研究环境中缺乏协调和有效的学习框架的问题。 |
| [^34] | [Pink-Eggs Dataset V1: A Step Toward Invasive Species Management Using Deep Learning Embedded Solutions.](http://arxiv.org/abs/2305.09302) | 这是一个数据集，其中包含被确认为褐化螺(P. canaliculata)蛋的图像以及对应的边界框注释，旨在帮助研究人员利用深度学习技术分析P. canaliculata物种的传播情况，并支持其他需要与褐化螺蛋相关的视觉数据的调查活动。 |
| [^35] | [Noise robust neural network architecture.](http://arxiv.org/abs/2305.09276) | 本文提出了一种名为“Dune Neural Network”的神经网络架构，通过将网络的每个自由参数表示为不确定性区间，并对每个输入元素应用线性变换，实现了对于白噪声数据的噪声鲁棒性能，即使为非常嘈杂的输入图像，此方法也比使用数据增强的人类在测试集上实现了更好的准确度。 |
| [^36] | [Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?.](http://arxiv.org/abs/2305.09275) | 本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量是不可靠的，现有算法会学习表面的标签相关性，我们提出了一种新的度量来衡量适应性并进行了基准测试。 |
| [^37] | [A new node-shift encoding representation for the travelling salesman problem.](http://arxiv.org/abs/2305.09257) | 本文提出了一种新的遗传算法编码表示来解决旅行商问题，并通过与最先进的编码表示的比较进行了性能评价。 |
| [^38] | [Rounding Meets Approximate Model Counting.](http://arxiv.org/abs/2305.09247) | 本文提出了一种新的基于取整的方法，以实现模型计数的高效率和高置信度估计。 |
| [^39] | [Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning.](http://arxiv.org/abs/2305.09246) | 本研究发现只需使用0.5%数据便可以进行训练大型语言模型（LLMs）指令调整，并且不影响性能表现。这种方法可以提高数据效率和节约培训成本。 |
| [^40] | [Privacy-Preserving Ensemble Infused Enhanced Deep Neural Network Framework for Edge Cloud Convergence.](http://arxiv.org/abs/2305.09224) | 该论文提出了一个应用于医疗保健领域物联网、边缘和云的隐私保护集成增强深度神经网络学习框架，旨在通过融合和集成学习提高模型准确性，同时利用差分隐私和迁移学习确保在边缘服务器上最小损失和更高效率。 |
| [^41] | [Towards Unifying Multi-Lingual and Cross-Lingual Summarization.](http://arxiv.org/abs/2305.09220) | 本文旨在将多语言摘要和跨语言摘要统一到更通用的多对多摘要中。我们提出了预先训练的M2MS模型“Pisces”，该模型可以处理任何语言的文档并生成摘要。实验结果表明，Pisces在零-shot方向上表现显着优于现有的基线模型。 |
| [^42] | [Trustworthy Privacy-preserving Hierarchical Ensemble and Federated Learning in Healthcare 4.0 with Blockchain.](http://arxiv.org/abs/2305.09209) | 本文提出了一种基于安全的多方计算的集成联邦学习和区块链技术，允许异构模型从医疗机构的数据中协同学习，保障数据隐私和完整性。 |
| [^43] | [Counterfactual Outcome Prediction using Structured State Space Model.](http://arxiv.org/abs/2305.09207) | 本文探讨了使用结构化状态空间模型对纵向数据中的反事实结果进行预测的方法，相比Treatment Effect Neural Controlled Differential Equation，S4Model更有效建模长期依赖关系、更易于训练、训练时间更短，且在归一化均方误差上提高了10倍。 |
| [^44] | [The Weighted M\"obius Score: A Unified Framework for Feature Attribution.](http://arxiv.org/abs/2305.09204) | 本文提出了权重莫比乌斯分数作为一个参数化的归因框架，可以涵盖很多不同的特征归因方法，包括特征交互，解决了方法繁衍和不可比的问题。通过研究方法的向量空间，提供了一些新方法和解释，实证结果表明其多功能性和有效性。 |
| [^45] | [Can we forget how we learned? Representing states in iterated belief revision}.](http://arxiv.org/abs/2305.09200) | 本文比较了迭代信念修正中的三种状态表示方法，证明了用字典序修订的重写历史是最有效率的，并提供了一个多项式时间算法，用于确定Horn公式是否等价于neg。 |
| [^46] | [Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks.](http://arxiv.org/abs/2305.09179) | 本文研究了神经常微分方程（NODE）在面对噪声和对抗性攻击时所表现出的自然鲁棒性，并通过控制ODE动力学的Lipschitz常数来显著提高其鲁棒性。实验结果在多个数据集上得到了验证。 |
| [^47] | [SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification.](http://arxiv.org/abs/2305.09160) | 本文提出了一种单数据集统一泛化（SUG）框架，通过多细粒度子域对齐和样本级域感知注意力策略，解决了三维点云领域泛化问题。 |
| [^48] | [Modelling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network for Adaptive Motion Integration.](http://arxiv.org/abs/2305.09156) | 本文介绍了一个结合可训练动态能量感知和自注意力网络的可计算图像模型，旨在捕捉生物视觉系统中运动感知的核心结构计算过程，填补了从自然场景中提取信息的可计算图像模型的空白。 |
| [^49] | [Dual-Alignment Pre-training for Cross-lingual Sentence Embedding.](http://arxiv.org/abs/2305.09148) | 该论文提出了一个双重对齐预训练框架，用于跨语言句子嵌入，它结合了句子级别和标记级别的对齐。引入了一种表示翻译学习任务，从而将翻译信息嵌入标记表示中。 |
| [^50] | [Deep ReLU Networks Have Surprisingly Simple Polytopes.](http://arxiv.org/abs/2305.09145) | 本文通过计算和分析ReLU网络多面体的单纯形直方图，发现在初始化和梯度下降时它们结构相对简单，这说明了一种新的隐式偏见。 |
| [^51] | [Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models.](http://arxiv.org/abs/2305.09144) | 本论文研究了语言模型的记忆机制，发现预训练可以有效提高模型的记忆能力，而知识相关性和多样性对于记忆形成也有显著影响。 |
| [^52] | [On Optimal Strategies for Wordle and General Guessing Games.](http://arxiv.org/abs/2305.09111) | 该论文提出了一种通用的方法来寻找猜词游戏的最优策略，并且证明了该方法的有效性（快速找到最优解），以Wordle猜词游戏为例具体说明。 |
| [^53] | [Is a Video worth $n\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering.](http://arxiv.org/abs/2305.09107) | 本文提出了一种高效的Transformer-based Video Question Answering方法，即将视频帧连接成 $n\times n$ 的矩阵，从而将图像编码器的使用量从 $n^{2}$ 减少到1，从而显著提高了训练和推理速度和节省了存储空间，而仍然保持了原始视频的时间结构，并在实验中取得了较好结果。 |
| [^54] | [AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition.](http://arxiv.org/abs/2305.09091) | 该论文提出了一个基于认知通用模型的 System-1 和 System-2 实现方法，解决了其区分不清的问题。通用模型提供了对涉及 System-1 和 System-2 的计算单元、基础机制和其对学习、元认知和情感的影响的全面视野。 |
| [^55] | [Capturing Humans' Mental Models of AI: An Item Response Theory Approach.](http://arxiv.org/abs/2305.09064) | 本研究提出基于IRT的模型来分析人类对于AI的感知，实验结果表明人们普遍期望AI团队伙伴的表现优于人类，并且人们对AI与人类成员的心理模型不同。 |
| [^56] | [SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification.](http://arxiv.org/abs/2305.09062) | 本文提出了两种新的基于距离的损失函数，通过考虑少量数据之间的类内距离和类间距离来考虑嵌入向量的重要性，以增强度量学习中的类别可分性，实现优秀的少样本图像分类表现。 |
| [^57] | [LoViT: Long Video Transformer for Surgical Phase Recognition.](http://arxiv.org/abs/2305.08989) | LoViT是一种用于手术阶段识别的长视频Transformer，它通过结合时间丰富的空间特征提取器和多尺度时间聚合器来对长视频进行分析，优于现有方法。 |
| [^58] | [Federated Learning over Harmonized Data Silos.](http://arxiv.org/abs/2305.08985) | 该论文提出了一种解决异构数据集的联邦学习方法，通过将数据协调和数据填补等关键步骤纳入架构愿景，来促进数据管理信息系统和机器学习的交叉研究。 |
| [^59] | [Motion Question Answering via Modular Motion Programs.](http://arxiv.org/abs/2305.08953) | 提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。 |
| [^60] | [MIMEx: Intrinsic Rewards from Masked Input Modeling.](http://arxiv.org/abs/2305.08932) | MIMEx是一个通用的框架，它使用遮盖输入建模来提取内在奖励，通过控制遮盖分布来控制难度，可以在高维环境中取得优越的探索结果。 |
| [^61] | [AF2-Mutation: Adversarial Sequence Mutations against AlphaFold2 on Protein Tertiary Structure Prediction.](http://arxiv.org/abs/2305.08929) | 本文研究了针对AlphaFold2的对抗序列突变，实测仅修改三个氨基酸残基，就使其预测的蛋白质三级结构的改变达到了46.61，并能成功发现在特定蛋白质中对结构至关重要且有生物学意义的氨基酸残基，为蛋白质的修饰提供了新思路。 |
| [^62] | [Differential Convolutional Fuzzy Time Series Forecasting.](http://arxiv.org/abs/2305.08890) | 本文提出了一种新的预测模型DFCNN，利用卷积神经网络实现具有可学习能力的FTSF，并能够处理非平稳时间序列。 |
| [^63] | [Watermarking Text Generated by Black-Box Language Models.](http://arxiv.org/abs/2305.08883) | 本研究提出了一种利用Transformer注意力图来获得词级反馈的新型黑盒LLM水印算法，实现对由黑盒语言模型生成的文本的水印保护。 |
| [^64] | [Spiking Network Initialisation and Firing Rate Collapse.](http://arxiv.org/abs/2305.08879) | 该论文尝试通过使用ANN初始化文献中的技术以及计算神经科学结果来解决突触神经网络（SNN）初始化的问题。作者指出，与ANNs相比，SNNs的重量初始化问题更为微妙，因为SNNs具有尖峰和重置非线性以及射频崩溃问题，为此作者提出并解决了几种射频崩溃问题的方案。 |
| [^65] | [Neurosymbolic AI and its Taxonomy: a survey.](http://arxiv.org/abs/2305.08876) | 本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。 |
| [^66] | [Online machine-learning forecast uncertainty estimation for sequential data assimilation.](http://arxiv.org/abs/2305.08874) | 该论文提出了一种使用卷积神经网络进行在线机器学习的方法，在单个动力学模型集成中估计表示预测误差协方差矩阵的状态依赖预测不确定性，并在混合数据同化方法中应用。通过模拟实验证明，该方法在准确性和计算成本方面优于传统的基于集合的方法。 |
| [^67] | [Automating privacy decisions -- where to draw the line?.](http://arxiv.org/abs/2305.08747) | 快速发展的互联网、移动和IoT技术中，用户需要管理大量的个人数据，但是决定如何保护隐私是一个令人不知所措的挑战。该论文提供了自动化隐私决策的现有挑战和解决方案的概述。 |
| [^68] | [The Structure and Dynamics of Knowledge Graphs, with Superficiality.](http://arxiv.org/abs/2305.08116) | 该论文提出了第一个知识图谱的结构和动态模型，并引入表层性来简单建模复杂特征，从而掌握全局知识分布的平衡。 |
| [^69] | [Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning.](http://arxiv.org/abs/2305.08014) | 本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。 |
| [^70] | [In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making.](http://arxiv.org/abs/2305.07722) | AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。 |
| [^71] | [Value Iteration Networks with Gated Summarization Module.](http://arxiv.org/abs/2305.07039) | 本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。 |
| [^72] | [Autonomous GIS: the next-generation AI-powered GIS.](http://arxiv.org/abs/2305.06453) | 自主GIS是一种AI动力地理信息系统，采用大型语言模型作为推理核心，具有自动空间数据收集、分析和可视化的能力，旨在实现五个自主目标：自动生成、自组织、自验证、自执行和自生长。 |
| [^73] | [Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance.](http://arxiv.org/abs/2305.05228) | 本研究提出了一种通用的语义嵌入深度神经网络，通过空间感知的语义特征和基于通道的注意力模型来提高多标签预测的模型性能，平均相对改进达到15.27%。 |
| [^74] | [Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection.](http://arxiv.org/abs/2305.04379) | 本文提出了一种最先进的加权目标函数，用于提高多标签分类中深度神经网络（DNN）针对长尾数据分布的性能，并通过对时尚服装的图像属性分类的实验，取得了良好的性能。 |
| [^75] | [Automated Code generation for Information Technology Tasks in YAML through Large Language Models.](http://arxiv.org/abs/2305.02783) | 这项研究提出了一种名为Ansible Wisdom的自然语言转Ansible-YAML代码的工具，可自动化生成Ansible脚本，提高IT自动化生产力，并相比现有技术达到或更好的性能水平。 |
| [^76] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^77] | [LLT: An R package for Linear Law-based Feature Space Transformation.](http://arxiv.org/abs/2304.14211) | LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。 |
| [^78] | [Towards Mode Balancing of Generative Models via Diversity Weights.](http://arxiv.org/abs/2304.11961) | 本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。 |
| [^79] | [GeneGPT: Teaching Large Language Models to Use NCBI Web APIs.](http://arxiv.org/abs/2304.09667) | GeneGPT通过少量NCBI API调用URL请求作为演示，教授大型语言模型使用NCBI Web API回答基因组问题，并在GeneTuring测试中达到了优异的结果。 |
| [^80] | [SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis.](http://arxiv.org/abs/2304.00020) | 研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。 |
| [^81] | [Synthetic Experience Replay.](http://arxiv.org/abs/2303.06614) | 本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。 |
| [^82] | [Improving the Data Efficiency of Multi-Objective Quality-Diversity through Gradient Assistance and Crowding Exploration.](http://arxiv.org/abs/2302.12668) | 本文介绍了一种新的QD算法——带有策略梯度辅助和基于拥挤的探索的多目标MAP-Elites (MOME-PGX)来扩展MOME以提高其数据效率和性能。 |
| [^83] | [Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking.](http://arxiv.org/abs/2302.10902) | 本文回顾了基于深度学习的时间序列健康数据缺失值填补方法，并在五个时间序列健康数据集上进行基准测试，研究发现填补的效果依赖于数据类型、变量统计、缺失值率和类型。在时间序列数据中同时进行交叉剖面和纵向缺失值填补的深度学习方法表现最佳。 |
| [^84] | [Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2301.08491) | 本文使用多智能体强化学习模拟社会困境中的道德选择，设计了一套道德奖励结构，旨在分析和研究AI代理的道德行为。 |
| [^85] | [Learning-Rate-Free Learning by D-Adaptation.](http://arxiv.org/abs/2301.07733) | D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。 |
| [^86] | [tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation.](http://arxiv.org/abs/2301.05948) | tasksource是一个数据集协调框架，为多任务学习和评估提供流畅的体验。该框架提供了结构化注释，使得数据处理更加便捷。 |
| [^87] | [Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning.](http://arxiv.org/abs/2301.04746) | 本论文提出了一种名为SLAP的方法，它可以替代数据增强，加强经验以加速机器学习并减少样本大小。 在Gomoku游戏和强化学习领域的实验中，证明了SLAP的有效性，可以提高模型的收敛速度，同时减少了样本数量。这种策略至少适用于对称或特定变换不变的领域。 |
| [^88] | [TAToo: Vision-based Joint Tracking of Anatomy and Tool for Skull-base Surgery.](http://arxiv.org/abs/2212.14131) | TAToo是一个基于视觉的联合跟踪方法，可用于恢复颅底手术中手术工具和患者解剖结构的三维运动。该方法具有高精度和鲁棒性，并可与现有的临床工作流程和仪器兼容。 |
| [^89] | [On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective.](http://arxiv.org/abs/2212.12669) | 本文提出了基础决策模型（FDM），通过使用转换器神经架构将各种决策任务制定为序列解码任务，为在复杂现实世界环境中实现机器驱动智能决策(IDM)提供了一个前景广阔的解决方案。 |
| [^90] | [Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias.](http://arxiv.org/abs/2212.11261) | 本文使用对比语言图像预训练的多模态AI模型，使用网页抓取的数据训练，发现这些模型存在性物化偏见，即人的情感状态与身体的呈现相关，表现出对女性的性别偏见。 |
| [^91] | [A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling.](http://arxiv.org/abs/2212.10936) | 本文提出了一种基于深度强化学习的模因算法，用于解决具有实际约束的灵活生产调度问题，并弥补元启发式研究中的缺陷。 |
| [^92] | [DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships.](http://arxiv.org/abs/2212.10545) | 本文提出了DimonGen模型，通过生成各种日常场景的概念关系描述来实现多元化通识推理。实验结果表明，MoREE模型在生成质量和多样性方面均优于基线模型。 |
| [^93] | [Self-Prompting Large Language Models for Zero-Shot Open-Domain QA.](http://arxiv.org/abs/2212.08635) | 本论文提出了一种自我提示框架，可以有效利用大型语言模型的参数中存储的知识和指令理解能力，以实现零样本开放域问答，并且实验证明该方法在三个广泛使用的ODQA数据集中显著优于现有的最先进方法。 |
| [^94] | [Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator.](http://arxiv.org/abs/2212.06751) | 本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。 |
| [^95] | [AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning.](http://arxiv.org/abs/2211.16944) | AIONER是一种新颖的基于深度学习的全套方案生物医学命名实体识别工具，利用外部数据提高BioNER模型准确性和稳定性，具有良好的性能和泛化能力。 |
| [^96] | [Frustratingly Easy Label Projection for Cross-lingual Transfer.](http://arxiv.org/abs/2211.15613) | 本文通过一项广泛的实证研究，对57种语言和三个任务下的跨语言转移进行了研究，并发现优化后的标记-翻译法比传统注释投影方法更有效。 |
| [^97] | [c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization.](http://arxiv.org/abs/2211.14411) | 本文提出了约束TPE（c-TPE）方法，是树形Parzen估计器（TPE）的扩展，可有效处理在性能要求之上施加的约束限制，实验证明在81个昂贵的HPO设置中表现出最佳性能排名。 |
| [^98] | [Hybrid quantum neural network for drug response prediction.](http://arxiv.org/abs/2211.05777) | 该论文提出了一种基于混合量子神经网络的药物反应预测模型，能够有效减少需要的训练数据，并有望提高治疗方案的效率。 |
| [^99] | [Experiential Explanations for Reinforcement Learning.](http://arxiv.org/abs/2210.04723) | 该论文提出了一种经验解释技术，通过训练影响预测器来恢复强化学习系统中的信息，使得非AI专家能够更好地理解其决策过程。 |
| [^100] | [What Makes Pre-trained Language Models Better Zero-shot Learners?.](http://arxiv.org/abs/2209.15206) | 本文提出一个理论框架来解释prompt learning在零样本/少样本场景下的有效性，并基于此提出了一个注释无关的模板选择方法。 |
| [^101] | [Dataset Distillation Using Parameter Pruning.](http://arxiv.org/abs/2209.14609) | 本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。 |
| [^102] | [Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans.](http://arxiv.org/abs/2209.13020) | 这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。 |
| [^103] | [Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers.](http://arxiv.org/abs/2209.12816) | FNet模型通过替换注意力层为傅里叶变换，加速了Transformer编码器模型的训练过程并保持相同的性能水平。 |
| [^104] | [WeLM: A Well-Read Pre-trained Language Model for Chinese.](http://arxiv.org/abs/2209.10372) | WeLM 是一种面向中文的读过书的预训练语言模型，它通过读取高质量的语料库训练了100亿个参数，并可以在18个中文任务中显著优于现有同规模预训练模型，同时表现出强大的多语言和代码转换理解能力。 |
| [^105] | [Literature Review of the Recent Trends and Applications in various Fuzzy Rule based systems.](http://arxiv.org/abs/2209.07175) | 本文综述了模糊规则系统的最新趋势和应用，包括GFS、HFS、NFS、eFS、用于大数据和不平衡数据的FRBS以及可解释性和使用聚类中心作为模糊规则的FRBS。重点突出其重要贡献和出版统计数据。 |
| [^106] | [Simulation-Assisted Optimization for Large-Scale Evacuation Planning with Congestion-Dependent Delays.](http://arxiv.org/abs/2209.01535) | 本研究提出了一种通过模拟辅助优化的方法，能够在考虑拥堵依赖延迟的情况下，提高大规模疏散计划的性能表现，相比现有方法表现更优。 |
| [^107] | [BERT4Loc: BERT for Location -- POI Recommender System.](http://arxiv.org/abs/2208.01375) | BERT4Loc是一种基于BERT的位置推荐系统，将地理位置信息和用户偏好相结合，提供个性化的基于位置的建议，相较于预测序列中下一个POI的模型，提供更相关的推荐，在基准数据集上实现了较好的性能。 |
| [^108] | [Challenging Common Assumptions about Catastrophic Forgetting.](http://arxiv.org/abs/2207.04543) | 本文挑战了DNNs遭受灾难性遗忘的常见假设，提出当前优化技术可以缓解这种现象。我们的实验表明，使用标准梯度下降优化方法训练的DNNs在回归任务上可以积累知识，即使任务重新出现，也不会遗忘过去的知识，这种知识积累可以改善在未见任务上的泛化性能。 |
| [^109] | [Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm.](http://arxiv.org/abs/2206.02059) | 本文提出了一种基于边缘感知的Weisfeiler-Lehman算法，以增强图神经网络的表达能力，同时保持消息传递方案的可扩展性。实验表明，我们NC-GNN框架在各种基准测试中表现出有效性和高效性。 |
| [^110] | [A Robust Framework of Chromosome Straightening with ViT-Patch GAN.](http://arxiv.org/abs/2203.02901) | 本文提出了一种新的ViT-Patch GAN架构，其包括一个自学习的运动变换生成器和基于Vision Transformer的patch（ViT-Patch）鉴别器。该生成器学习染色体的运动表示，以实现鲁棒性染色体修直，并且在保留形状和斑带图案细节方面具有更好的性能。 |
| [^111] | [S-ConvNet: A Shallow Convolutional Neural Network Architecture for Neuromuscular Activity Recognition Using Instantaneous High-Density Surface EMG Images.](http://arxiv.org/abs/1906.03381) | S-ConvNet和All-ConvNet模型为神经肌肉活动识别提出了一种简单而有效的学习框架，不需要预训练，表现出非常具有竞争力的识别准确性。 |

# 详细

[^1]: Make-An-Animation: 大规模文本条件下的三维人体动作生成

    Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation. (arXiv:2305.09662v1 [cs.CV])

    [http://arxiv.org/abs/2305.09662](http://arxiv.org/abs/2305.09662)

    Make-An-Animation 是一个用于文本条件下三维人体运动生成的模型，通过大规模数据集的训练得以弥补现有方法的局限，提高了运动生成的质量和多样性。

    

    文本指导下的人体动作生成因其在动画和机器人领域的应用而备受关注。最近，扩散模型在动作生成方面的应用使生成动作的质量得到了改进。然而，现有方法局限于相对较小规模的运动捕捉数据，导致对更加多样化的“野外”提示的性能较差。本文介绍了Make-An-Animation，这是一个文本条件的人体运动生成模型，它从大规模的图像文本数据集中学习更多样化的姿势和提示，从而在前期工作的基础上显着提高了性能。Make-An-Animation经过两个阶段的训练。首先，我们在一个策划的大规模数据集上进行训练，该数据集包括从图像文本数据集中提取的（文本，静态假姿态）对。其次，我们在运动捕捉数据上进行微调，添加额外的层以模拟时间维度。与以往的扩散模型不同，Make-An-Animation采用两阶段训练过程，基于大规模的图像文本数据集生成更加多样化和高质量的人体运动。

    Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation,
    
[^2]: 分布式鲁棒的离线强化学习：基于双重悲观性的通用算法和强健部分覆盖

    Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])

    [http://arxiv.org/abs/2305.09659](http://arxiv.org/abs/2305.09659)

    本论文提出了一个名为P2MPO的算法框架，用于解决基于鲁棒离线RL的问题。该框架结合了灵活的模型估计子例程和双重悲观的策略优化步骤，采用双重悲观性原则以克服模型偏移等问题。研究表明，在模型准确性的假设下，该框架在拥有良好的鲁棒部分覆盖数据的情况下是具备高效性的。

    

    本文研究了分布式鲁棒的离线强化学习（鲁棒离线RL），其旨在从离线数据集中纯粹地找到一个能够在扰动环境中表现良好的最优强鲁棒策略。我们提出了一个名为P2MPO的算法框架，其中包含了灵活的模型估计子例程和双重悲观的策略优化步骤。双重悲观性原则对于克服由行为策略和目标策略家族之间的不匹配以及名义模型的扰动所引起的分布偏移至关重要。在对模型估计子例程进行一定准确性假设的情况下，我们证明了P2MPO算法在拥有良好的鲁棒部分覆盖数据的情况下是可证明有效的。

    We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \underline{D}oubly \underline{P}essimistic \underline{M}odel-based \underline{P}olicy \underline{O}ptimization ($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with \emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d
    
[^3]: 声明提示下的可满足性辅助语言模型

    Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])

    [http://arxiv.org/abs/2305.09656](http://arxiv.org/abs/2305.09656)

    本文提出了一种利用自动定理证明器和声明性任务规范的可满足性辅助语言建模方法，可以提高大型语言模型的推理能力。

    

    本文提出了一种新的可满足性辅助语言建模方法，用于提高大型语言模型的推理能力。我们使用一个大型语言模型生成一个声明性任务规范，并利用一个现成的自动定理证明器得出最终答案。该方法具有两个关键优点：第一，声明性规范比推理步骤更接近问题描述，因此大型语言模型可以更准确地解析它；第二，通过将实际推理任务委托给自动定理证明器，我们的方法可以保证正确性。

    Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
    
[^4]: 基于偏好排序的Prompt-Tuning决策变换器

    Prompt-Tuning Decision Transformer with Preference Ranking. (arXiv:2305.09648v1 [cs.LG])

    [http://arxiv.org/abs/2305.09648](http://arxiv.org/abs/2305.09648)

    本研究提出了Prompt-Tuning DT算法，通过使用轨迹段作为prompt来指导RL agent获取环境信息，从而在具有挑战性的任务中实现了优秀表现。

    

    Prompt-tuning已成为一种很有前途的适应预训练模型到下游任务或与人类偏好对齐的方法。Prompt learning 在自然语言处理中得到广泛应用，但由于RL prompts中包含的复杂物理含义和环境特定信息，其适用性有限。这些因素需要通过监督学习来模仿演示，并在学习后可能导致意义的丧失。此外，将prompt-tuning方法直接扩展到RL是具有挑战性的，因为RL prompts是根据环境建模和分析来指导agent行为的，而不是填补缺失的信息，因此在下游任务中调整prompt格式，如在NLP中，可能不会产生显著的改进。本文提出了Prompt-Tuning DT算法来解决这些挑战，通过将轨迹段用作prompt来指导RL agent获取环境信息，并通过黑盒调整来优化prompt，从而改善了下游任务的性能。

    Prompt-tuning has emerged as a promising method for adapting pre-trained models to downstream tasks or aligning with human preferences. Prompt learning is widely used in NLP but has limited applicability to RL due to the complex physical meaning and environment-specific information contained within RL prompts. These factors require supervised learning to imitate the demonstrations and may result in a loss of meaning after learning. Additionally, directly extending prompt-tuning approaches to RL is challenging because RL prompts guide agent behavior based on environmental modeling and analysis, rather than filling in missing information, making it unlikely that adjustments to the prompt format for downstream tasks, as in NLP, can yield significant improvements. In this work, we propose the Prompt-Tuning DT algorithm to address these challenges by using trajectory segments as prompts to guide RL agents in acquiring environmental information and optimizing prompts via black-box tuning to 
    
[^5]: AI增强的调查：利用大语言模型进行全国代表性调查的观点预测

    AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])

    [http://arxiv.org/abs/2305.09620](http://arxiv.org/abs/2305.09620)

    本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。

    

    本论文研究了如何使用经过全国代表性调查微调的大语言模型（LLMs）来增强调查。本文探讨了LLMs在观点预测中，遗漏数据插值，回溯推理和零次预测三个不同应用。我们提出了一种新的方法论框架，将调查问题、个人信念和时间背景的神经嵌入引入到观点预测的个性化LLMs中。在1972年到2021年的“常规社会调查”中，我们从68,846名美国人中获得了3,110个二进制观点，在Alpaca-7b模型的基础上取得了最好的成果，在缺失数据插值（AUC=0.87，公开观点预测为$\rho$=0.99）和回溯推理（AUC=0.86，$\rho$=0.98）方面表现出色。这些显著的预测能力能够以高置信度填补缺失的趋势，并标明公众态度何时发生变化，如同性婚姻的获取支持。然而，在零次预测的情况下，模型的表现受到限制，需要进一步研究。

    How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
    
[^6]: 大型语言模型在医学问答中的应用：迈向医学专家级别的问答能力

    Towards Expert-Level Medical Question Answering with Large Language Models. (arXiv:2305.09617v1 [cs.CL])

    [http://arxiv.org/abs/2305.09617](http://arxiv.org/abs/2305.09617)

    本研究提出了Med-PaLM2，通过结合基础LLM改进、医学领域微调和提示策略，并用新颖的集成精炼方法，实现了在MedQA数据集上达到86.5%的医学问答准确率，迈向医学专家级别的问答能力。

    

    近年来，人工智能系统在诸如围棋和蛋白质折叠等“宏伟挑战”方面取得了里程碑式的进展。但回答医学问题并像医生一样进行推理被认为也是一种宏伟挑战。大型语言模型在医学问答方面取得了重大进展；Med-PaLM是第一个在MedQA数据集上以67.2％的分数超过美国医疗执业考试（USMLE）样式问题的“及格”分数的模型。 然而，对比模型答案和医生答案，这项和其他先前工作表明还有很大的改进空间。本文提出了Med-PaLM2，通过利用基础LLM改进（PaLM2）、医学领域微调和提示策略（包括新颖的集成精炼方法）来弥合这些差距。在MedQA数据集上，Med-PaLM2的得分可达86.5％，比Med-PaLM提高了超过11％。

    Recent artificial intelligence (AI) systems have reached milestones in "grand challenges" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.  Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a "passing" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 
    
[^7]: 基于能量归一化流的语义分割中并发误分类和越界检测

    Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow. (arXiv:2305.09610v1 [cs.CV])

    [http://arxiv.org/abs/2305.09610](http://arxiv.org/abs/2305.09610)

    该论文提出了一种基于能量归一化流框架的生成模型，用于并发的在分布内误分类（IDM）和越界检测，可以扩展先前部署的分割模型而无需重新训练，并在测试中实现很好的结果。

    

    最近的语义分割模型对于和训练数据集分布相似的测试样本进行分类具有很高的准确性。但是，它们的判别式闭合方法在实际数据设置中对于分布偏移和越界（OOD）类不够稳健。因此，在测试时间使用预测概率作为置信度分数时，预测的概率可能非常不精确。为了解决这个问题，我们提出了一种基于正常化流框架的生成模型，用于并发的在分布内误分类（IDM）和越界检测。所提出的基于流的带有能量输入的检测器（FlowEneDet）可以扩展先前部署的分割模型而无需耗费大量时间进行重新训练。我们的FlowEneDet在内存占用方面具有极小的增加，因此具有低复杂度架构。在Cityscapes、Cityscapes-C、FishyScapes和 SegmentMeIfYouCan评估中，FlowEneDet在使用预先训练的DeepLabV3+和Translated title:基于能量归一化流的语义分割中并发误分类和越界检测的测试中实现很好的结果。

    Recent semantic segmentation models accurately classify test-time examples that are similar to a training dataset distribution. However, their discriminative closed-set approach is not robust in practical data setups with distributional shifts and out-of-distribution (OOD) classes. As a result, the predicted probabilities can be very imprecise when used as confidence scores at test time. To address this, we propose a generative model for concurrent in-distribution misclassification (IDM) and OOD detection that relies on a normalizing flow framework. The proposed flow-based detector with an energy-based inputs (FlowEneDet) can extend previously deployed segmentation models without their time-consuming retraining. Our FlowEneDet results in a low-complexity architecture with marginal increase in the memory footprint. FlowEneDet achieves promising results on Cityscapes, Cityscapes-C, FishyScapes and SegmentMeIfYouCan benchmarks in IDM/OOD detection when applied to pretrained DeepLabV3+ and
    
[^8]: 深度强化学习最大化繁忙路段通行效率

    Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion. (arXiv:2305.09600v1 [cs.AI])

    [http://arxiv.org/abs/2305.09600](http://arxiv.org/abs/2305.09600)

    本文提出了一种基于深度强化学习的方法来减少繁忙多车道高速公路交通拥堵，该方法可以学习适应性绕行策略，在减少拥堵和提高车速的同时，优化高速公路车道和周边局部收发路网的使用，实验证明其可以显著优化道路收发效率并减少总通行时间。

    

    在道路网络中，交通事故和其他事件，如果不加以缓解，可能会导致级联故障，影响系统的大部分功能。及时处理这种极端拥堵情况是降低排放量、提高生产率和改善城市生活质量的必要手段。本文提出了一种基于深度强化学习（DRL）的方法，在繁忙的多车道高速公路上减少交通拥堵。该智能体被训练学习适应性绕行策略，以便在拥堵的高速公路交通中最优地利用高速公路车道和周边局部收发路网，奖励是减少拥堵及提高车速。实验设置在美国华盛顿州Shoreline市的一段长2.6英里、4条车道的高速公路中进行，在微观和连续的综合交通模拟器SUMO（城市移动仿真）中模拟了两个出口和相关的收发路，同时使用了参数化的转移学习。研究结果表明，所提出的方法可以显著提高极端拥堵时的道路收发效率并减少总通行时间。

    Collisions, crashes, and other incidents on road networks, if left unmitigated, can potentially cause cascading failures that can affect large parts of the system. Timely handling such extreme congestion scenarios is imperative to reduce emissions, enhance productivity, and improve the quality of urban living. In this work, we propose a Deep Reinforcement Learning (DRL) approach to reduce traffic congestion on multi-lane freeways during extreme congestion. The agent is trained to learn adaptive detouring strategies for congested freeway traffic such that the freeway lanes along with the local arterial network in proximity are utilized optimally, with rewards being congestion reduction and traffic speed improvement. The experimental setup is a 2.6-mile-long 4-lane freeway stretch in Shoreline, Washington, USA with two exits and associated arterial roads simulated on a microscopic and continuous multi-modal traffic simulator SUMO (Simulation of Urban MObility) while using parameterized t
    
[^9]: 重新审视关节物体操作中的本体感知

    Revisiting Proprioceptive Sensing for Articulated Object Manipulation. (arXiv:2305.09584v1 [cs.RO])

    [http://arxiv.org/abs/2305.09584](http://arxiv.org/abs/2305.09584)

    本文探讨了在关节物体操作中使用本体感知信息的重要性，通过创建一个系统并进行初步测试，发现了其表现良好，对于机器人操作关节物体具有重要意义。

    

    辅助人类的机器人需要与关节物体（例如橱柜或微波炉）交互。早期研究使用本体感知来在接触过程中估计关节机制。但是，现在几乎所有系统在接触过程中仅使用视觉而不再考虑本体感知信息。我们认为，在接触过程中使用本体感知信息是有价值的信息来源，并且在文献中没有明确的不使用它的动机。因此，在本文中，我们创建了一个系统，从给定的握取开始，使用本体感知来控制位置和平行夹持器打开橱柜。我们进行了定性评估，发现夹持器和把手之间的滑动限制了系统的性能。尽管如此，我们发现该系统已经表现得非常好。这引出了一个问题：我们是否应该更多地利用关节物体操作中的本体感知信息？

    Robots that assist humans will need to interact with articulated objects such as cabinets or microwaves. Early work on creating systems for doing so used proprioceptive sensing to estimate joint mechanisms during contact. However, nowadays, almost all systems use only vision and no longer consider proprioceptive information during contact. We believe that proprioceptive information during contact is a valuable source of information and did not find clear motivation for not using it in the literature. Therefore, in this paper, we create a system that, starting from a given grasp, uses proprioceptive sensing to open cabinets with a position-controlled robot and a parallel gripper. We perform a qualitative evaluation of this system, where we find that slip between the gripper and handle limits the performance. Nonetheless, we find that the system already performs quite well. This poses the question: should we make more use of proprioceptive information during contact in articulated object
    
[^10]: UOR：预训练语言模型的通用后门攻击

    UOR: Universal Backdoor Attacks on Pre-trained Language Models. (arXiv:2305.09574v1 [cs.CL])

    [http://arxiv.org/abs/2305.09574](http://arxiv.org/abs/2305.09574)

    本文介绍了一种新的后门攻击方法UOR，可以自动选择触发器并学习通用输出表示，成功率高达99.3％，能够对多种预训练语言模型和下游任务实施攻击，且可突破最新的防御方法。

    

    在预训练语言模型中植入后门可以传递到各种下游任务，这对安全构成了严重威胁。然而，现有的针对预训练语言模型的后门攻击大都是非目标和特定任务的。很少有针对目标和任务不可知性的方法使用手动预定义的触发器和输出表示，这使得攻击效果不够强大和普适。本文首先总结了一个更具威胁性的预训练语言模型后门攻击应满足的要求，然后提出了一种新的后门攻击方法UOR，通过将手动选择变成自动优化，打破了以往方法的瓶颈。具体来说，我们定义了被污染的监督对比学习，可以自动学习各种预训练语言模型触发器的更加均匀和通用输出表示。此外，我们使用梯度搜索选取适当的触发词，可以适应不同的预训练语言模型和词汇表。实验证明，UOR可以在各种PLMs和下游任务中实现高后门成功率（高达99.3％），优于现有方法。此外，UOR还可以突破对抗后门攻击的最新防御方法。

    Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experi
    
[^11]: AI伦理：组织挑战和伦理企业家中的风险个人化

    Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs. (arXiv:2305.09573v1 [cs.CY])

    [http://arxiv.org/abs/2305.09573](http://arxiv.org/abs/2305.09573)

    本文研究基于定性分析，对负责将AI伦理融入产品开发的技术工人进行了分析。结果显示，伦理企业家面临的三大障碍是： 在公司产品发布为中心的环境中，难以使伦理优先考虑；在以指标激励公司目标的情况下，伦理难以量化；频繁的团队重组使得获取知识和维护关系变得困难。

    

    在公众对技术信任下降的背景下，计算机伦理已经成为焦点，并且批评家已经对企业伦理洗涤提出了质疑。然而，很少有研究探讨AI伦理价值在技术公司中的实际实施情况。本文通过对负责将AI伦理融入产品开发的技术工人进行定性分析，发现工人们面临一个政策、实践和结果分离的环境。我们将AI伦理工作者解析为伦理企业家，他们在组织内工作，以使新的伦理相关实践制度化。我们显示伦理企业家面临三个主要障碍。首先，他们在围绕软件产品发布为中心的环境中努力使伦理优先考虑。其次，在以指标激励公司目标的情况下，伦理在难以量化。第三，频繁的团队重组使得获取知识和维护关系变得困难。

    Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate ethics washing. Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships centr
    
[^12]: 大数据学习：精选包与随机包的对比研究

    Learning from Aggregated Data: Curated Bags versus Random Bags. (arXiv:2305.09557v1 [cs.LG])

    [http://arxiv.org/abs/2305.09557](http://arxiv.org/abs/2305.09557)

    本文研究了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包，对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。

    

    保护用户隐私是许多机器学习系统部署的一个主要关注点，这些系统收集来自各种群体的数据。为了应对这种问题，一种方法是以聚合的形式收集和发布数据标签，从而可以将单个用户的信息与其他用户的信息组合起来。本文探讨了使用聚合数据标签而非单个标签来训练机器学习模型的可能性，具体来说，我们考虑了两种自然的聚合方法：基于共同特征将数据点分组的精选包和将数据点随机分组的随机包。对于精选包设置和广泛的损失函数范围内，我们展示了可以通过梯度下降学习而不会导致数据聚合导致性能下降的情况。我们的方法基于以下观察：损失函数的梯度之和可以表示为每个包的梯度的加权和，其中权重是包的大小。

    Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss functio
    
[^13]: 基于混合注意力的EEG睡眠分期

    EEG-based Sleep Staging with Hybrid Attention. (arXiv:2305.09543v1 [eess.SP])

    [http://arxiv.org/abs/2305.09543](http://arxiv.org/abs/2305.09543)

    本文提出了一种基于混合注意力EEG睡眠分期（HASS）框架，采用时空注意力机制自适应地分配给予通道间和通道内的EEG片段权重，显著提高典型睡眠分期网络的性能。

    

    睡眠分期对于评估睡眠质量和诊断睡眠障碍至关重要。然而，在不同睡眠阶段期间，捕捉脑电图（EEG）信号内的空间和时间关系仍然具有挑战性。在本文中，我们提出了一种新的框架，称为混合注意力EEG睡眠分期（HASS）框架。具体而言，我们提出了一个精心设计的时空注意力机制，根据不同睡眠阶段期间的大脑空间-时间关系自适应地分配给予通道间和通道内的EEG片段权重。在MASS和ISRUC数据集上进行的实验结果表明，HASS可以显著提高典型睡眠分期网络的性能。我们提出的框架缓解了在睡眠分期期间捕捉EEG信号时的空间-时间关系的困难，并有望提高临床和研究环境中睡眠评估的准确性和可靠性。

    Sleep staging is critical for assessing sleep quality and diagnosing sleep disorders. However, capturing both the spatial and temporal relationships within electroencephalogram (EEG) signals during different sleep stages remains challenging. In this paper, we propose a novel framework called the Hybrid Attention EEG Sleep Staging (HASS) Framework. Specifically, we propose a well-designed spatio-temporal attention mechanism to adaptively assign weights to inter-channels and intra-channel EEG segments based on the spatio-temporal relationship of the brain during different sleep stages. Experiment results on the MASS and ISRUC datasets demonstrate that HASS can significantly improve typical sleep staging networks. Our proposed framework alleviates the difficulties of capturing the spatial-temporal relationship of EEG signals during sleep staging and holds promise for improving the accuracy and reliability of sleep assessment in both clinical and research settings.
    
[^14]: 《琳达，出了什么问题？》“连接谬误”作为公平性问题的探讨

    What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem. (arXiv:2305.09535v1 [cs.AI])

    [http://arxiv.org/abs/2305.09535](http://arxiv.org/abs/2305.09535)

    这篇论文探讨了“连接谬误”作为公平性问题在人工智能领域中的重要性，并提出了一些问题，以帮助AI研究人员和从业者避免类似情况在未来中复现。

    

    人工智能领域正在专注于创建尽可能接近人类智能的自动决策系统。这一努力推动人工智能研究人员探索心理学等认知领域。 Daniel Kahneman和已故的Amos Tversky在有偏见的人类决策制定方面的工作，包括对连接谬误的研究，因此进行了第二次复兴。 在连接谬误下，决策制定者会违反基本概率法则，认为连词比其中一个部分更有可能。通过一系列与琳达问题最为著名的实验，它已被证明是经得起时间考验的。虽然这种跨学科的努力受到欢迎，但我们担心，人工智能研究人员忽略了琳达问题所捕捉到的驱动力：琳达必须被刻板地描述为一个女性。 在本文中，我们重新审视琳达问题，并将其形式化为AI中的公平性问题。我们认为连接谬误是偏见数据集如何导致偏见结果的明显例子，从而延续和放大现有的系统性偏见。我们提出了一组问题供AI研究人员和从业人员使用，以避免类似情况在未来发生。

    The field of Artificial Intelligence (AI) is focusing on creating automated decision-making (ADM) systems that operate as close as possible to human-like intelligence. This effort has pushed AI researchers into exploring cognitive fields like psychology. The work of Daniel Kahneman and the late Amos Tversky on biased human decision-making, including the study of the conjunction fallacy, has experienced a second revival because of this. Under the conjunction fallacy a human decision-maker will go against basic probability laws and rank as more likely a conjunction over one of its parts. It has been proven overtime through a set of experiments with the Linda Problem being the most famous one. Although this interdisciplinary effort is welcomed, we fear that AI researchers ignore the driving force behind the conjunction fallacy as captured by the Linda Problem: the fact that Linda must be stereotypically described as a woman. In this paper we revisit the Linda Problem and formulate it as a
    
[^15]: 实时同时多物体三维形状重建，6DoF姿态估计和密集抓取预测。

    Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction. (arXiv:2305.09510v1 [cs.RO])

    [http://arxiv.org/abs/2305.09510](http://arxiv.org/abs/2305.09510)

    该论文提出了一种实现实时同时多物体三维形状重建、6DoF姿态估计和密集抓取预测的新方法，无需顺序感知和抓取规划步骤，具有快速推理且具有竞争力的性能。

    

    在复杂环境中操作的机器人操作系统依赖于感知系统，该系统提供有关场景中对象的几何（姿态和三维形状）以及其他语义信息（例如对象标签）的信息。然后利用这些信息选择相关对象上的可行抓取。在本文中，我们提出了一种新方法，可以同时提供场景中所有对象的几何和语义信息以及这些对象上的可行抓取。我们方法的主要优点在于其速度，因为它避免了顺序感知和抓取规划步骤。通过详细的定量分析，我们展示了我们的方法提供与面向对象形状、姿态和抓取预测的最新专用方法相比具有竞争力的性能，同时提供每秒30帧的快速推理。

    Robotic manipulation systems operating in complex environments rely on perception systems that provide information about the geometry (pose and 3D shape) of the objects in the scene along with other semantic information such as object labels. This information is then used for choosing the feasible grasps on relevant objects. In this paper, we present a novel method to provide this geometric and semantic information of all objects in the scene as well as feasible grasps on those objects simultaneously. The main advantage of our method is its speed as it avoids sequential perception and grasp planning steps. With detailed quantitative analysis, we show that our method delivers competitive performance compared to the state-of-the-art dedicated methods for object shape, pose, and grasp predictions while providing fast inference at 30 frames per second speed.
    
[^16]: 涉及概率和因果推理的推理困难度研究

    The Hardness of Reasoning about Probabilities and Causality. (arXiv:2305.09508v1 [cs.AI])

    [http://arxiv.org/abs/2305.09508](http://arxiv.org/abs/2305.09508)

    该论文从计算复杂性的角度研究了能够完全表达概率量化推理和因果效应的 do-calculus 推理的形式语言的可满足性问题，并确立了这些问题的精确计算复杂度，该研究证明了在概率和因果推断中通常使用的标准语言的变种的算法限制更强。

    

    我们从计算复杂性的角度研究了能够完全表达概率量化推理和因果效应的 do-calculus 推理的形式语言。我们专注于实例公式允许表达概率和因果推断中的许多任务的可满足性问题。本研究的主要贡献是确立这些可满足性问题的精确计算复杂度。我们引入了一个新的自然复杂类，命名为 succ$\exists$R，它可以被视为众所周知的 $\exists$R 类的简洁变种，并且证明了我们考虑的问题是 succ$\exists$R 类完全的。我们的结果意味着对于一些在概率和因果推断中通常使用的标准语言的变种，它们比 Fagin、Halpern 和 Megiddo (1990)以及 Moss\'{e}、Ibeling 和 Icard (2022)证明的算法限制更强。

    We study formal languages which are capable of fully expressing quantitative probabilistic reasoning and do-calculus reasoning for causal effects, from a computational complexity perspective. We focus on satisfiability problems whose instance formulas allow expressing many tasks in probabilistic and causal inference. The main contribution of this work is establishing the exact computational complexity of these satisfiability problems. We introduce a new natural complexity class, named succ$\exists$R, which can be viewed as a succinct variant of the well-studied class $\exists$R, and show that the problems we consider are complete for succ$\exists$R. Our results imply even stronger algorithmic limitations than were proven by Fagin, Halpern, and Megiddo (1990) and Moss\'{e}, Ibeling, and Icard (2022) for some variants of the standard languages used commonly in probabilistic and causal inference.
    
[^17]: ALC本体的通用模块的高效计算方法（扩展版）

    Efficient Computation of General Modules for ALC Ontologies (Extended Version). (arXiv:2305.09503v1 [cs.AI])

    [http://arxiv.org/abs/2305.09503](http://arxiv.org/abs/2305.09503)

    该论文提出了一种从ALC本体中提取通用模块的方法，并且通过评估表明其可以在显着较短的时间内计算出，相比于最先进的方法可以得到更小的结果。

    

    我们提出了一种从描述逻辑ALC中提取本体通用模块的方法。对于用户指定集合的术语，一个模块是一个理论性质上大大简化的本体，它可以保留所有蕴含。因此，它具有本体重用和本体分析等应用。不同于传统模块，通用模块可以使用输入本体中未显式出现的公理，这允许额外的简明性。迄今为止，通用模块仅针对轻量级描述逻辑进行了研究。我们提出了第一个考虑更具表达力的描述逻辑ALC的工作。我们的贡献是一种基于均匀插值的新方法，这种方法由一些新的理论结果支持。我们的评估表明，我们的通用模块通常比经典模块和最先进的均匀插值计算出的均匀插值小，并且与均匀插值相比，可以在显着较短的时间内计算出来。

    We present a method for extracting general modules for ontologies formulated in the description logic ALC. A module for an ontology is an ideally substantially smaller ontology that preserves all entailments for a user-specified set of terms. As such, it has applications such as ontology reuse and ontology analysis. Different from classical modules, general modules may use axioms not explicitly present in the input ontology, which allows for additional conciseness. So far, general modules have only been investigated for lightweight description logics. We present the first work that considers the more expressive description logic ALC. In particular, our contribution is a new method based on uniform interpolation supported by some new theoretical results. Our evaluation indicates that our general modules are often smaller than classical modules and uniform interpolants computed by the state-of-the-art, and compared with uniform interpolants, can be computed in a significantly shorter tim
    
[^18]: 用于符号音乐生成的离散扩散概率模型

    Discrete Diffusion Probabilistic Models for Symbolic Music Generation. (arXiv:2305.09489v1 [cs.SD])

    [http://arxiv.org/abs/2305.09489](http://arxiv.org/abs/2305.09489)

    本研究提出了使用离散扩散概率模型直接生成复调符号音乐。该模型表现出了最先进的样本质量，并允许在音符级别上进行灵活的插值。与此同时，我们还探讨了通过统计指标对音乐样本质量进行定量评估的限制。

    

    在连续和离散领域中，去噪扩散概率模型（DDPM）在生成高质量样本方面取得了重大进展。然而，离散DDPM（D3PM）尚未应用于符号音乐领域。本研究提出了使用D3PM直接生成复调符号音乐。根据当前的定量评估指标，我们的模型展示出了最先进的样本质量，并允许在音符级别上进行灵活的插值。我们进一步展示了我们的模型对后期分类器指导的可访问性，扩大了可能应用的范围。然而，我们也对通过统计指标对音乐样本质量进行定量评估持有批判的观点，并呈现了一个简单的算法，可以干扰我们的指标，生成完全虚假的非音乐样本。

    Denoising Diffusion Probabilistic Models (DDPMs) have made great strides in generating high-quality samples in both discrete and continuous domains. However, Discrete DDPMs (D3PMs) have yet to be applied to the domain of Symbolic Music. This work presents the direct generation of Polyphonic Symbolic Music using D3PMs. Our model exhibits state-of-the-art sample quality, according to current quantitative evaluation metrics, and allows for flexible infilling at the note level. We further show, that our models are accessible to post-hoc classifier guidance, widening the scope of possible applications. However, we also cast a critical view on quantitative evaluation of music sample quality via statistical metrics, and present a simple algorithm that can confound our metrics with completely spurious, non-musical samples.
    
[^19]: 交叉门控多层感知机下的蛋白质复合物不变嵌入是一种一次性抗体设计器

    Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09480](http://arxiv.org/abs/2305.09480)

    本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。

    

    抗体是由免疫系统产生的针对外来物质或抗原的重要蛋白质。抗体的特异性由其互补决定区（CDR）决定，CDR位于抗体链的可变区域中，形成与抗原结合的位点。以往的研究利用复杂的技术生成CDR，但它们遭受了几何建模不足的问题。此外，常见的迭代精化策略导致了低效的推断。本文提出了一种深度生成模型，可以一次性地共同设计CDR的1D序列和3D结构。为了实现这一目标，我们将抗体CDR设计分为两个阶段：（i）蛋白质结构的几何建模和（ii）序列结构共学习。我们开发了一种蛋白质复合物不变嵌入，可捕捉蛋白质骨架原子（包括Cα、N、C和O原子）之间的内部和外部组分相互作用，以实现全面的几何建模。

    Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
    
[^20]: 解释型AI研究中未经证明的样本量和推广：更具包容性用户研究的原则

    Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies. (arXiv:2305.09477v1 [cs.AI])

    [http://arxiv.org/abs/2305.09477](http://arxiv.org/abs/2305.09477)

    解释型AI研究中的参与者样本量和推广问题限制了该领域对XAI系统可解释性的评估，需要更具包容性的用户研究原则来解决这些问题。

    

    许多伦理框架要求人工智能系统可解释。解释型AI（XAI）模型经常在用户研究中测试其充分性。由于不同的人可能有不同的解释需求，因此参与者样本在用户研究中应足够大，以代表目标人群，以实现推广。然而，尚不清楚XAI研究人员在多大程度上反思和证明其样本量或避免跨人群广泛推广。我们分析了2012年至2022年间发表的220篇XAI用户研究。大多数研究没有提供样本量的理由。此外，大多数论文将其结论推广到目标人群之外，并且没有证据表明定量研究中更广泛的结论与更大的样本有关。这些方法论问题可能会妨碍评估XAI系统是否实现了伦理框架中提出的可解释性。我们概述了更具包容性的XAI研究用户研究原则，以解决这些问题。

    Many ethical frameworks require artificial intelligence (AI) systems to be explainable. Explainable AI (XAI) models are frequently tested for their adequacy in user studies. Since different people may have different explanatory needs, it is important that participant samples in user studies are large enough to represent the target population to enable generalizations. However, it is unclear to what extent XAI researchers reflect on and justify their sample sizes or avoid broad generalizations across people. We analyzed XAI user studies (N = 220) published between 2012 and 2022. Most studies did not offer rationales for their sample sizes. Moreover, most papers generalized their conclusions beyond their target population, and there was no evidence that broader conclusions in quantitative studies were correlated with larger samples. These methodological problems can impede evaluations of whether XAI systems implement the explainability called for in ethical frameworks. We outline princip
    
[^21]: 构建和服务于大规模开放领域知识图谱

    Growing and Serving Large Open-domain Knowledge Graphs. (arXiv:2305.09464v1 [cs.AI])

    [http://arxiv.org/abs/2305.09464](http://arxiv.org/abs/2305.09464)

    本文介绍了一种平台，它可以扩展和服务于大规模开放领域知识图谱，同时介绍了训练知识图嵌入的管道以及如何创建语义注释服务和推动开放领域知识抽取。

    

    大规模开放领域知识图谱(KG)应用于现实世界问题时面临许多独特的挑战。本文介绍了我们平台Saga的扩展，该平台可持续地构建和服务于知识图谱规模。具体而言，我们描述了一个用于训练知识图嵌入的管道，可用于支持主要功能，如事实排名、事实验证、相关实体服务以及支持实体链接。然后，我们描述了如何利用我们的平台（包括图嵌入）来创建语义注释服务，将非结构化Web文档链接到我们KG中的实体。Web的语义注释有效地扩展了我们的知识图谱，提供了链接到开放领域Web内容的边缘，可用于各种搜索和排名问题。最后，我们利用注释的Web文档来推动开放领域知识抽取。这个有针对性的提取框架识别了KG中的重要覆盖问题，然后寻找了目标的相关数据源。

    Applications of large open-domain knowledge graphs (KGs) to real-world problems pose many unique challenges. In this paper, we present extensions to Saga our platform for continuous construction and serving of knowledge at scale. In particular, we describe a pipeline for training knowledge graph embeddings that powers key capabilities such as fact ranking, fact verification, a related entities service, and support for entity linking. We then describe how our platform, including graph embeddings, can be leveraged to create a Semantic Annotation service that links unstructured Web documents to entities in our KG. Semantic annotation of the Web effectively expands our knowledge graph with edges to open-domain Web content which can be used in various search and ranking problems. Finally, we leverage annotated Web documents to drive Open-domain Knowledge Extraction. This targeted extraction framework identifies important coverage issues in the KG, then finds relevant data sources for target
    
[^22]: 采用教师-学生方案和多种谱图的低复杂度深度学习框架用于声景分类

    Low-complexity deep learning frameworks for acoustic scene classification using teacher-student scheme and multiple spectrograms. (arXiv:2305.09463v1 [cs.SD])

    [http://arxiv.org/abs/2305.09463](http://arxiv.org/abs/2305.09463)

    本文提出了一种采用教师-学生方案和多种谱图的低复杂度深度学习框架用于声景分类，获得了57.4%的最佳分类准确率，相比DCASE基线提高14.5%。

    

    本文提出了一种用于声景分类的低复杂度深度学习系统。该系统由两个主要阶段组成：（阶段I）训练教师网络; 和（阶段II）利用教师网络的压缩知识训练学生网络。在第一阶段中，训练一个大规模的教师模型。在训练教师之后，提取特征图，即教师网络的倒数第二层的特征。在第二阶段中，利用从教师提取的特征图来训练低复杂度的学生网络。我们在DCASE 2023任务1开发数据集上进行的实验，满足低复杂度要求，并取得了57.4％的最佳分类准确率，相比DCASE基线提高了14.5％。

    In this technical report, a low-complexity deep learning system for acoustic scene classification (ASC) is presented. The proposed system comprises two main phases: (Phase I) Training a teacher network; and (Phase II) training a student network using distilled knowledge from the teacher. In the first phase, the teacher, which presents a large footprint model, is trained. After training the teacher, the embeddings, which are the feature map of the second last layer of the teacher, are extracted. In the second phase, the student network, which presents a low complexity model, is trained with the embeddings extracted from the teacher. Our experiments conducted on DCASE 2023 Task 1 Development dataset have fulfilled the requirement of low-complexity and achieved the best classification accuracy of 57.4%, improving DCASE baseline by 14.5%.
    
[^23]: 一种具有相关信念下最优学习的序列公交网络设计算法

    A sequential transit network design algorithm with optimal learning under correlated beliefs. (arXiv:2305.09452v1 [cs.AI])

    [http://arxiv.org/abs/2305.09452](http://arxiv.org/abs/2305.09452)

    提出一种结合序列公交网络设计和最优学习的算法，用于准确估计潜在出行需求，并规避设计与实际需求不一致的风险。

    

    准确估计潜在需求对于公交服务路线设计至关重要。本论文提出了一种结合了序列公交网络设计和最优学习的人工智能驱动算法。运营商逐步扩展其路线系统，以避免设计路线与实际出行需求不一致的风险。同时，观测到的信息将被存档以更新运营商当前使用的知识。算法中比较了三种学习策略：多臂老虎机、知识梯度和具有相关信念的知识梯度。该算法的模拟结果显示其在公交网络设计中的良好表现。

    Mobility service route design requires potential demand information to well accommodate travel demand within the service region. Transit planners and operators can access various data sources including household travel survey data and mobile device location logs. However, when implementing a mobility system with emerging technologies, estimating demand level becomes harder because of more uncertainties with user behaviors. Therefore, this study proposes an artificial intelligence-driven algorithm that combines sequential transit network design with optimal learning. An operator gradually expands its route system to avoid risks from inconsistency between designed routes and actual travel demand. At the same time, observed information is archived to update the knowledge that the operator currently uses. Three learning policies are compared within the algorithm: multi-armed bandit, knowledge gradient, and knowledge gradient with correlated beliefs. For validation, a new route system is de
    
[^24]: 关于F1分数的讨论——以最近的关系抽取系统为例

    About Evaluation of F1 Score for RECENT Relation Extraction System. (arXiv:2305.09410v1 [cs.CL])

    [http://arxiv.org/abs/2305.09410](http://arxiv.org/abs/2305.09410)

    本文讨论了一个名为RECENT的关系抽取系统，作者在TACRED数据集上取得了之前的最新成果74.8的F1分数，但在更正错误和重新评估后其F1分数为65.16。

    

    本文讨论了Shengfei Lyu和Huanhuan Chen在Findings of the Association for Computational Linguistics：ACL-IJCNLP 2021上发表的论文“Relation Classification with Entity Type Restriction”中使用的F1分数评估方法。作者创建的系统名为RECENT，声称其在TACRED数据集上取得了（当时的）最新的75.2（之前为74.8）的最新成果，但在更正错误和重新评估后最终结果为65.16。

    This document contains a discussion of the F1 score evaluation used in the article 'Relation Classification with Entity Type Restriction' by Shengfei Lyu, Huanhuan Chen published on Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and claim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on the TACRED dataset, while after correcting errors and reevaluation the final result is 65.16
    
[^25]: 扩散数据集生成：为行人检测缩小Sim2Real的差距

    Diffusion Dataset Generation: Towards Closing the Sim2Real Gap for Pedestrian Detection. (arXiv:2305.09401v1 [cs.CV])

    [http://arxiv.org/abs/2305.09401](http://arxiv.org/abs/2305.09401)

    本文提出了一种使用扩散模型增强模拟数据集的方法，以缩小行人检测中的Sim2Real差距，该方法将生成的数据与模拟数据混合进行训练，在真实世界的数据中表现出更好的性能，平均精度提高高达27.3％。

    

    我们提出了一种方法，使用扩散模型增强模拟数据集，以提高现实世界数据中行人检测的性能。在现实世界中收集和注释数据的高昂成本促使使用模拟平台创建训练数据集。虽然模拟数据易于收集和注释，但往往与现实世界的数据分布不符，这被称为Sim2Real差距。在本文中，我们提出了一种新颖的合成数据创建方法，旨在缩小行人检测任务中的Sim2Real差距。我们的方法使用基于扩散的架构来学习现实世界的分布，一旦训练完成，就将其用于生成数据集。我们将此生成数据与模拟数据混合以形成增强形式，并证明在真实世界的数据中训练生成数据和模拟数据的组合可以将行人检测模型的平均精度提高高达27.3％。

    We propose a method that augments a simulated dataset using diffusion models to improve the performance of pedestrian detection in real-world data. The high cost of collecting and annotating data in the real-world has motivated the use of simulation platforms to create training datasets. While simulated data is inexpensive to collect and annotate, it unfortunately does not always closely match the distribution of real-world data, which is known as the sim2real gap. In this paper we propose a novel method of synthetic data creation meant to close the sim2real gap for the challenging pedestrian detection task. Our method uses a diffusion-based architecture to learn a real-world distribution which, once trained, is used to generate datasets. We mix this generated data with simulated data as a form of augmentation and show that training on a combination of generated and simulated data increases average precision by as much as 27.3% for pedestrian detection models in real-world data, compar
    
[^26]: 在Lenia中捕获新兴复杂性

    Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v1 [cs.NE])

    [http://arxiv.org/abs/2305.09378](http://arxiv.org/abs/2305.09378)

    研究人工生命平台Lenia，通过识别复杂新兴行为的度量标准和使用遗传算法产生不同行为的结果，以进化出更好的Lenia行为。

    

    本研究项目探讨了Lenia，这是一个模拟数字生物系统的人工生命平台。Lenia的生态系统由简单的人工生物组成，它们可以移动、消耗、生长和繁殖。该平台是一个研究人工生命和进化的重要工具，因为它提供了一个可扩展和灵活的环境，用于创建具有不同能力和行为的多样化生物。该研究的关键是在Lenia中测量复杂性，识别测量规则的长期复杂性新兴行为的度量标准，旨在进化出尚未发现的更好的Lenia行为。遗传算法使用相邻区域或核作为基因型，同时保持Lenia的其他参数（例如生长函数）不变，以产生不同人口行为的结果，然后测量适应度值以决定所得行为的复杂性。首先，我们使用时间变化作为适应度函数，

    This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenia's ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitn
    
[^27]: 多通道电阻抗血流动力学监测中的自动信号质量评估的无监督序列到序列学习

    Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring. (arXiv:2305.09368v1 [eess.SP])

    [http://arxiv.org/abs/2305.09368](http://arxiv.org/abs/2305.09368)

    本论文提出了一种无监督序列到序列学习方法，用于自动评估多通道电阻抗血流动力学监测中心肺容积信号（CVS）的运动诱导可靠性降低。通过利用长短时记忆和变分自编码器结构，编码器 - 解码器模型可以捕捉到暂态 CVS 序列中存在的上下文知识。与现有方法相比，本方法不需要手动注释运动影响以及缺乏在 CVS 随时间出现上下文变化的情况下实现运动引起异常的显式机制，具有竞争性能。

    

    本研究提出了一种无监督的序列到序列学习方法，用于自动评估多通道电阻抗血流动力学监测中心肺容积信号（CVS）的运动诱导可靠性降低。该方法试图解决现有学习型评估方法的缺点，例如需要手动注释运动影响以及缺乏在 CVS 随时间出现上下文变化的情况下实现运动引起异常的显式机制。通过利用长短时记忆和变分自编码器结构，编码器 - 解码器模型不仅被训练成自我重现 CVS 的输入序列，而且还被训练成以平行方式外推未来。通过这样做，模型可以捕捉到暂态 CVS 序列中存在的上下文知识，同时被规范化以探索整个时间序列的一般关系。根据实际和预测 CVS 之间的差异，检测出低质量的受运动影响的 CVS。多通道 EIT 基心血管监测的大规模数据集实验表明，该方法在运动引起的测量质量评估方法的竞争性能上具有竞争力。

    This study proposes an unsupervised sequence-to-sequence learning approach that automatically assesses the motion-induced reliability degradation of the cardiac volume signal (CVS) in multi-channel electrical impedance-based hemodynamic monitoring. The proposed method attempts to tackle shortcomings in existing learning-based assessment approaches, such as the requirement of manual annotation for motion influence and the lack of explicit mechanisms for realizing motion-induced abnormalities under contextual variations in CVS over time. By utilizing long-short term memory and variational auto-encoder structures, an encoder--decoder model is trained not only to self-reproduce an input sequence of the CVS but also to extrapolate the future in a parallel fashion. By doing so, the model can capture contextual knowledge lying in a temporal CVS sequence while being regularized to explore a general relationship over the entire time-series. A motion-influenced CVS of low-quality is detected, ba
    
[^28]: 基于提示的多模态视觉理解技术对图像语义信息进行分离的研究

    Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image. (arXiv:2305.09333v1 [cs.CV])

    [http://arxiv.org/abs/2305.09333](http://arxiv.org/abs/2305.09333)

    本论文研究了基于提示的多模态视觉理解技术，通过分离语义信息提高对图像的理解，旨在推进图像识别和理解领域的研究。

    

    通过提示来进行多模态视觉理解技术可利用各种视觉及文本线索，提高对图像的语义理解。这种方法结合了视觉和语言处理，能够产生更准确的预测和识别图像。通过采用基于提示的技术，模型可以学习关注图像的某些特征，以提取下游任务所需的有用信息。此外，多模态理解可以通过提供更强大的图像表示来改善单模态模型的性能。总的来说，视觉和文本信息的结合是推进图像识别和理解领域的有望研究方向。本文将尝试一些提示设计方法，并提出一种更好地提取语义信息的新方法。

    Multi-modal visual understanding of images with prompts involves using various visual and textual cues to enhance the semantic understanding of images. This approach combines both vision and language processing to generate more accurate predictions and recognition of images. By utilizing prompt-based techniques, models can learn to focus on certain features of an image to extract useful information for downstream tasks. Additionally, multi-modal understanding can improve upon single modality models by providing more robust representations of images. Overall, the combination of visual and textual information is a promising area of research for advancing image recognition and understanding. In this paper we will try an amount of prompt design methods and propose a new method for better extraction of semantic information
    
[^29]: 推荐系统中的用户公平性: 方法和评估的系统调查

    Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation. (arXiv:2305.09330v1 [cs.IR])

    [http://arxiv.org/abs/2305.09330](http://arxiv.org/abs/2305.09330)

    推荐系统的公平性问题越来越引起人们的关注，尤其是用户公平性，已经提出了很多解决方案来减轻用户在使用推荐系统过程中体验到的歧视问题。

    

    在数字化水平不断提高的当前社会中，面临着可扩展性方面的巨大挑战。推荐系统已经成为帮助用户导航日益增长的数据量，以及帮助供应商向感兴趣的用户营销产品的必不可少的工具。机器学习方法中的歧视问题日益突出，这促使学术界和工业界研究如何确保推荐系统的公平性。在推荐系统中，这些问题在职业推荐中得到了很好的体现，历史数据中的偏见可能导致推荐系统将一个性别与较低的工资或刻板印象联系起来。特别地，用户公平性关注如何减轻用户在使用推荐系统过程中体验到的歧视问题，该领域已经出现了很多不同的方法来解决不同类型的歧视。所述歧视的性质取决于所处的情境。

    In the current landscape of ever-increasing levels of digitalization, we are facing major challenges pertaining to scalability. Recommender systems have become irreplaceable both for helping users navigate the increasing amounts of data and, conversely, aiding providers in marketing products to interested users. The growing awareness of discrimination in machine learning methods has recently motivated both academia and industry to research how fairness can be ensured in recommender systems. For recommender systems, such issues are well exemplified by occupation recommendation, where biases in historical data may lead to recommender systems relating one gender to lower wages or to the propagation of stereotypes. In particular, consumer-side fairness, which focuses on mitigating discrimination experienced by users of recommender systems, has seen a vast number of diverse approaches for addressing different types of discrimination. The nature of said discrimination depends on the setting 
    
[^30]: BERTTM: 利用来自预训练语言模型的上下文化词向量进行神经主题建模

    BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])

    [http://arxiv.org/abs/2305.09329](http://arxiv.org/abs/2305.09329)

    本文提出了一种新颖的神经主题模型，利用来自预训练语言模型BERT的上下文化词嵌入，可以在不使用任何BoW信息的情况下推断出文档的主题分布，并直接从上下文化词嵌入中推断出文档中每个单词的主题分布。实验结果表明，该模型优于仅依赖BoW表示和其他神经主题模型的现有最先进方法。

    

    随着近年来神经主题模型的发展，主题建模在自然语言理解中扮演着日益重要的角色。然而，大多数现有的主题模型仍然依赖于词袋（BoW）信息，无论是作为训练输入还是训练目标。这限制了它们捕捉文档中的单词顺序信息的能力，并导致它们在处理新文档中的未观察到的单词时遇到困难。预训练语言模型中的上下文化词向量在词义消歧的能力上表现优越，并证明了它们在处理OOV单词时是有效的。在这项工作中，我们开发了一种新颖的神经主题模型，结合了预训练语言模型BERT的上下文化词嵌入。该模型可以在不使用任何BoW信息的情况下推断出文档的主题分布。此外，该模型可以直接从上下文化词嵌入中推断出文档中每个单词的主题分布。基准数据集的实验表明，我们的模型优于仅依赖BoW表示和其他神经主题模型的现有最先进方法。

    With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
    
[^31]: 利用图嵌入增强从长篇科技论文中提取关键词的方法

    Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings. (arXiv:2305.09316v1 [cs.CL])

    [http://arxiv.org/abs/2305.09316](http://arxiv.org/abs/2305.09316)

    本文研究了使用图神经网络表示加强预训练语言模型对长篇科技论文的关键词提取。通过构建文本共现图并结合图表示和上下文化的PLM嵌入，我们展示了对于长篇文档，增强PLMs性能比现有技术的模型表现更加出色。

    

    本研究探讨了使用图神经网络（GNN）表示强化预训练语言模型（PLMs）对长篇文档中的关键词提取的上下文表示的方法。我们展示了利用图嵌入增强PLM提供更全面的语义理解文档中的单词，特别是对于长篇文档。我们构建了文本的共现图，并使用在边预测任务上训练的图卷积网络（GCN）来嵌入它。我们提出了一种图增强的序列标记架构，它将上下文化的PLM嵌入与图表示相结合。在基准测试数据集上的评估表明，增强PLM与图嵌入比现有技术的模型在长文档上表现更出色，在所有数据集中都显着提高F1得分。我们的研究突出了GNN表示作为改进PLM性能的一种补充方法的潜力。

    In this study, we investigate using graph neural network (GNN) representations to enhance contextualized representations of pre-trained language models (PLMs) for keyphrase extraction from lengthy documents. We show that augmenting a PLM with graph embeddings provides a more comprehensive semantic understanding of words in a document, particularly for long documents. We construct a co-occurrence graph of the text and embed it using a graph convolutional network (GCN) trained on the task of edge prediction. We propose a graph-enhanced sequence tagging architecture that augments contextualized PLM embeddings with graph representations. Evaluating on benchmark datasets, we demonstrate that enhancing PLMs with graph embeddings outperforms state-of-the-art models on long documents, showing significant improvements in F1 scores across all the datasets. Our study highlights the potential of GNN representations as a complementary approach to improve PLM performance for keyphrase extraction fro
    
[^32]: 混合与协作的段落再排序方法

    Hybrid and Collaborative Passage Reranking. (arXiv:2305.09313v1 [cs.IR])

    [http://arxiv.org/abs/2305.09313](http://arxiv.org/abs/2305.09313)

    该论文提出了一种名为HybRank的混合与协作的段落再排序方法，通过利用上游检索器的相似性度量实现段落协作，再利用稀疏和密集检索器的词汇和语义属性进行重新排序，该方法可以增强包括先前被重新排序的段落列表在内的任意段落列表，并在实验证明了性能稳定的提升。

    

    在段落检索系统中，初始检索结果可能不尽如人意，需要通过重新排序方案进行改善。现有的段落重新排序方案主要集中于丰富查询和每个段落之间的交互，忽略了在初始检索列表中排名靠前的多个段落之间的上下文关系。为解决这个问题，我们提出了一种混合与协作的段落再排序方法（HybRank），该方法利用上游检索器的相似性度量进行段落协作，并结合稀疏和密集检索器的词汇和语义属性进行重新排序。此外，基于现成的检索器特征，HybRank是一个插件再排序器，能够增强包括先前重新排序的段落列表在内的任意段落列表。大量实验证明了比普遍的检索和再排序方法性能稳定的提升，并验证了HybRank的核心组件的有效性。

    In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked passages in the initial retrieval list. To tackle this problem, we propose a Hybrid and Collaborative Passage Reranking (HybRank) method, which leverages the substantial similarity measurements of upstream retrievers for passage collaboration and incorporates the lexical and semantic properties of sparse and dense retrievers for reranking. Besides, built on off-the-shelf retriever features, HybRank is a plug-in reranker capable of enhancing arbitrary passage lists including previously reranked ones. Extensive experiments demonstrate the stable improvements of performance over prevalent retrieval and reranking methods, and verify the effectiveness of the core components of HybRank.
    
[^33]: OmniSafe：一种加速安全强化学习研究的基础设施

    OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research. (arXiv:2305.09304v1 [cs.LG])

    [http://arxiv.org/abs/2305.09304](http://arxiv.org/abs/2305.09304)

    OmniSafe是一种基础设施，用于加速安全强化学习研究，帮助解决当代SafeRL研究环境中缺乏协调和有效的学习框架的问题。

    

    强化学习（RL）算法赋能的AI系统有着促进社会进步的巨大潜力，但它们的部署常常受到重大安全隐患的阻碍。尤其是在安全关键的应用中，研究人员已经引起了对不受约束的RL代理的意外伤害或不安全行为的担忧。安全强化学习（SafeRL）的理念是将RL代理与无害意图和安全行为模式相一致。在SafeRL中，代理通过从环境中接收反馈来学习开发优化策略，同时也满足了将意外伤害或不安全行为的风险最小化的要求。然而，由于SafeRL算法实现的复杂性，跨越各个领域的方法的结合提出了巨大的挑战。这导致了当代SafeRL研究环境中缺乏一个协调和有效的学习框架。在这项工作中，我们介绍了一个基础框架：OmniSafe，用于加速安全强化学习研究。

    AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework d
    
[^34]: Pink-Eggs数据集V1：使用深度学习嵌入式解决方案管理入侵物种的一步

    Pink-Eggs Dataset V1: A Step Toward Invasive Species Management Using Deep Learning Embedded Solutions. (arXiv:2305.09302v1 [cs.CV])

    [http://arxiv.org/abs/2305.09302](http://arxiv.org/abs/2305.09302)

    这是一个数据集，其中包含被确认为褐化螺(P. canaliculata)蛋的图像以及对应的边界框注释，旨在帮助研究人员利用深度学习技术分析P. canaliculata物种的传播情况，并支持其他需要与褐化螺蛋相关的视觉数据的调查活动。

    

    我们介绍了一个新颖的数据集，其中包含了被确认为褐化螺(P. canaliculata)蛋的图像，附带了对应的边界框注释。该数据集的目的是利用深度学习技术帮助研究人员分析P. canaliculata物种的传播情况，并支持其他需要与褐化螺蛋相关的视觉数据的调查活动。然而，需要注意的是，有其他物种在美洲地区产生类似褐化螺蛋的观察记录。因此，任何消除这些蛋的决定的关键前提是明确它们是否完全归属于入侵的P. canaliculata或是否涉及其他物种。

    We introduce a novel dataset consisting of images depicting pink eggs that have been identified as Pomacea canaliculata eggs, accompanied by corresponding bounding box annotations. The purpose of this dataset is to aid researchers in the analysis of the spread of Pomacea canaliculata species by utilizing deep learning techniques, as well as supporting other investigative pursuits that require visual data pertaining to the eggs of Pomacea canaliculata. It is worth noting, however, that the identity of the eggs in question is not definitively established, as other species within the same taxonomic family have been observed to lay similar-looking eggs in regions of the Americas. Therefore, a crucial prerequisite to any decision regarding the elimination of these eggs would be to establish with certainty whether they are exclusively attributable to invasive Pomacea canaliculata or if other species are also involved. The dataset is available at https://www.kaggle.com/datasets/deeshenzhen/pi
    
[^35]: 噪声鲁棒的神经网络架构

    Noise robust neural network architecture. (arXiv:2305.09276v1 [cs.CV])

    [http://arxiv.org/abs/2305.09276](http://arxiv.org/abs/2305.09276)

    本文提出了一种名为“Dune Neural Network”的神经网络架构，通过将网络的每个自由参数表示为不确定性区间，并对每个输入元素应用线性变换，实现了对于白噪声数据的噪声鲁棒性能，即使为非常嘈杂的输入图像，此方法也比使用数据增强的人类在测试集上实现了更好的准确度。

    

    在本文中，我们提出了一种名为“Dune Neural Network”的神经网络架构，用于识别一般噪声图像，而不需要在训练数据中添加任何人工噪声。通过将网络的每个自由参数表示为不确定性区间，并对每个输入元素应用线性变换，我们展示了所得到的架构在面对有白噪声的输入数据时具有相当好的噪声鲁棒性。我们将简单的Dune神经网络应用于MNIST数据集，并证明了即使在对于人类难以识别的非常嘈杂的输入图像上，我们的方法也比使用数据增强的人类在测试集上实现了更好的准确度。我们还发现，我们的方法对于许多其他添加了各种背景模式的示例都很鲁棒。

    In which we propose neural network architecture (dune neural network) for recognizing general noisy image without adding any artificial noise in the training data. By representing each free parameter of the network as an uncertainty interval, and applying a linear transformation to each input element, we show that the resulting architecture achieves decent noise robustness when faced with input data with white noise. We apply simple dune neural networks for MNIST dataset and demonstrate that even for very noisy input images which are hard for human to recognize, our approach achieved better test set accuracy than human without dataset augmentation. We also find that our method is robust for many other examples with various background patterns added.
    
[^36]: 在线持续学习中的快速适应性：我们评估得对吗？

    Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?. (arXiv:2305.09275v1 [cs.LG])

    [http://arxiv.org/abs/2305.09275](http://arxiv.org/abs/2305.09275)

    本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量是不可靠的，现有算法会学习表面的标签相关性，我们提出了一种新的度量来衡量适应性并进行了基准测试。

    

    本文重新审视了在线持续学习算法适应性的常见评估方法——在线准确率度量。该度量衡量模型在接下来的几个样本上的准确性。然而，我们证明了这个度量是不可靠的。即使是空泛的盲分类器也能通过利用数据流中表面的标签相关性来实现非常高的在线准确率。我们的研究表明，现有的在线持续学习算法也能够实现高在线准确率，但是表现出较差的信息保留能力，表明它们意外地学习了表面的标签相关性。为解决这个问题，我们提出了一种基于消除表面的相关性的近期样本准确度的新度量来衡量适应性。我们使用我们提出的度量在各种计算预算下对大规模数据集中现有的OCL方法进行了基准测试，并发现更好的泛化能力可以达到。

    We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be a
    
[^37]: 一种新的节点移位编码表示来解决旅行商问题

    A new node-shift encoding representation for the travelling salesman problem. (arXiv:2305.09257v1 [cs.NE])

    [http://arxiv.org/abs/2305.09257](http://arxiv.org/abs/2305.09257)

    本文提出了一种新的遗传算法编码表示来解决旅行商问题，并通过与最先进的编码表示的比较进行了性能评价。

    

    本文提出一种新的遗传算法编码表示来解决旅行商问题。为了评价所提出的染色体结构的性能，我们将其与最先进的编码表示进行比较。为此，我们使用了来自TSPLIB的14个不同大小的基准。最终，在进行实验研究后，我们报告了所获得的结果并得出了结论。

    This paper presents a new genetic algorithm encoding representation to solve the travelling salesman problem. To assess the performance of the proposed chromosome structure, we compare it with state-of-the-art encoding representations. For that purpose, we use 14 benchmarks of different sizes taken from TSPLIB. Finally, after conducting the experimental study, we report the obtained results and draw our conclusion.
    
[^38]: 取整遇上近似模型计数

    Rounding Meets Approximate Model Counting. (arXiv:2305.09247v1 [cs.AI])

    [http://arxiv.org/abs/2305.09247](http://arxiv.org/abs/2305.09247)

    本文提出了一种新的基于取整的方法，以实现模型计数的高效率和高置信度估计。

    

    模型计数问题，也称为＃ SAT，是计算给定布尔公式$ F $的模型或满足赋值的数量的问题。模型计数是计算机科学中的一个基本问题，具有广泛的应用。最近，越来越多地使用基于哈希的技术进行近似模型计数，并提供$（\varepsilon，\delta）$保证：即，计数返回的结果与确切计数的$(1+\varepsilon)$-乘数具有$1-\delta$的置信度。虽然哈希技术可在$ \delta $足够大时具有合理的可扩展性，但在较小的$\delta$值时，其可扩展性受到严重影响，从而阻止了其在需要高置信度估计的应用领域的采用。本文的主要贡献在于解决哈希技术的致命弱点：我们提出了一种基于取整的新方法，可以大大减少运行时间。

    The problem of model counting, also known as #SAT, is to compute the number of models or satisfying assignments of a given Boolean formula $F$. Model counting is a fundamental problem in computer science with a wide range of applications. In recent years, there has been a growing interest in using hashing-based techniques for approximate model counting that provide $(\varepsilon, \delta)$-guarantees: i.e., the count returned is within a $(1+\varepsilon)$-factor of the exact count with confidence at least $1-\delta$. While hashing-based techniques attain reasonable scalability for large enough values of $\delta$, their scalability is severely impacted for smaller values of $\delta$, thereby preventing their adoption in application domains that require estimates with high confidence.  The primary contribution of this paper is to address the Achilles heel of hashing-based techniques: we propose a novel approach based on rounding that allows us to achieve a significant reduction in runtime
    
[^39]: 或许只需要0.5％的数据：低数据量训练指令调整初步探索

    Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. (arXiv:2305.09246v1 [cs.AI])

    [http://arxiv.org/abs/2305.09246](http://arxiv.org/abs/2305.09246)

    本研究发现只需使用0.5%数据便可以进行训练大型语言模型（LLMs）指令调整，并且不影响性能表现。这种方法可以提高数据效率和节约培训成本。

    

    针对训练大型语言模型（LLMs）的问题，本文进行了初步探索，以降低LLM指令调整所需的数据量，从而减少培训成本，提高数据效率。 研究发现，LLM指令调整的数据可以降至0.5％而不影响性能，将这种方法称为Low Training Data Instruction Tuning（LTD Instruction Tuning）。

    Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing the data used in LLM instruction tuning to decrease training costs and improve data efficiency, dubbed as Low Training Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper conducts a preliminary exploration into reducing the data used in LLM training and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of 
    
[^40]: 面向边缘云融合的隐私保护集成增强深度神经网络框架

    Privacy-Preserving Ensemble Infused Enhanced Deep Neural Network Framework for Edge Cloud Convergence. (arXiv:2305.09224v1 [cs.CR])

    [http://arxiv.org/abs/2305.09224](http://arxiv.org/abs/2305.09224)

    该论文提出了一个应用于医疗保健领域物联网、边缘和云的隐私保护集成增强深度神经网络学习框架，旨在通过融合和集成学习提高模型准确性，同时利用差分隐私和迁移学习确保在边缘服务器上最小损失和更高效率。

    

    本文提出了一个针对医疗保健领域物联网（IoT）、边缘和云的收敛的隐私保护集成增强深度神经网络（DNN）学习框架。在这种收敛中，边缘服务器用于存储 IoT 产生的生物图像并托管 DNN 算法以进行本地模型训练。云用于合并本地模型。使用本地数据集的 DNN 基础训练过程可能受到低准确性的影响，这可以通过前述的融合和集成学习来改善。集成学习允许多个参与者外包其本地模型，以产生具有高准确性的广义最终模型。然而，集成学习提高了从最终模型中泄漏敏感私人数据的风险。该提出的框架提出一种基于差分隐私的隐私保护 DNN，并结合迁移学习进行本地模型生成，以确保在边缘服务器上最小损失和更高效率。

    We propose a privacy-preserving ensemble infused enhanced Deep Neural Network (DNN) based learning framework in this paper for Internet-of-Things (IoT), edge, and cloud convergence in the context of healthcare. In the convergence, edge server is used for both storing IoT produced bioimage and hosting DNN algorithm for local model training. The cloud is used for ensembling local models. The DNN-based training process of a model with a local dataset suffers from low accuracy, which can be improved by the aforementioned convergence and Ensemble Learning. The ensemble learning allows multiple participants to outsource their local model for producing a generalized final model with high accuracy. Nevertheless, Ensemble Learning elevates the risk of leaking sensitive private data from the final model. The proposed framework presents a Differential Privacy-based privacy-preserving DNN with Transfer Learning for a local model generation to ensure minimal loss and higher efficiency at edge serve
    
[^41]: 走向统一多语言和跨语言摘要

    Towards Unifying Multi-Lingual and Cross-Lingual Summarization. (arXiv:2305.09220v1 [cs.CL])

    [http://arxiv.org/abs/2305.09220](http://arxiv.org/abs/2305.09220)

    本文旨在将多语言摘要和跨语言摘要统一到更通用的多对多摘要中。我们提出了预先训练的M2MS模型“Pisces”，该模型可以处理任何语言的文档并生成摘要。实验结果表明，Pisces在零-shot方向上表现显着优于现有的基线模型。

    

    为了适应多语言的世界，先前的工作提出了多语言摘要（MLS）和跨语言摘要（CLS）。然而，这两个任务因为定义的不同而被分别研究，限制了两者的兼容性和系统研究。在本文中，我们旨在将MLS和CLS统一到更通用的设置中，即多对多摘要（M2MS），其中单个模型可以处理任何语言的文档并生成它们的摘要，也可以用任何语言。作为通向M2MS的第一步，我们进行初步研究，表明M2MS可以更好地在不同语言之间传递任务知识，而不是MLS和CLS。此外，我们提出了Pisces，这是一个预先训练的M2MS模型，通过三阶段的预先训练学习语言建模、跨语言和摘要能力。实验结果表明，我们的Pisces显着优于现有的基线，特别是在零-shot方向中。

    To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate their summaries also in any language. As the first step towards M2MS, we conduct preliminary studies to show that M2MS can better transfer task knowledge across different languages than MLS and CLS. Furthermore, we propose Pisces, a pre-trained M2MS model that learns language modeling, cross-lingual ability and summarization ability via three-stage pre-training. Experimental results indicate that our Pisces significantly outperforms the state-of-the-art baselines, especially in the zero-shot directio
    
[^42]: 区块链下的可信隐私保护的医疗4.0级层次集成和联邦学习

    Trustworthy Privacy-preserving Hierarchical Ensemble and Federated Learning in Healthcare 4.0 with Blockchain. (arXiv:2305.09209v1 [cs.CR])

    [http://arxiv.org/abs/2305.09209](http://arxiv.org/abs/2305.09209)

    本文提出了一种基于安全的多方计算的集成联邦学习和区块链技术，允许异构模型从医疗机构的数据中协同学习，保障数据隐私和完整性。

    

    互联网和通信技术的进步引领了工业4.0的时代。此转变随之而来的是医疗行业创造出的医疗4.0。在医疗4.0中，使用物联网医疗影像设备进行早期疾病检测使医护人员能够提高医疗机构的服务质量。但由于数据隐私问题，在人工智能和大数据方面，医疗4.0仍落后于其他工业4.0。此外，机构的多样化存储和计算能力限制了机构融合相同训练模型结构的能力。本文提出了一种基于安全的多方计算的集成联邦学习，通过区块链技术使异构模型能够从医疗机构的数据中协同学习而不会侵犯用户的隐私。区块链的属性还允许各方在没有中心化服务器的情况下享受数据完整性而不需信任。

    The advancement of Internet and Communication Technologies (ICTs) has led to the era of Industry 4.0. This shift is followed by healthcare industries creating the term Healthcare 4.0. In Healthcare 4.0, the use of IoT-enabled medical imaging devices for early disease detection has enabled medical practitioners to increase healthcare institutions' quality of service. However, Healthcare 4.0 is still lagging in Artificial Intelligence and big data compared to other Industry 4.0 due to data privacy concerns. In addition, institutions' diverse storage and computing capabilities restrict institutions from incorporating the same training model structure. This paper presents a secure multi-party computation-based ensemble federated learning with blockchain that enables heterogeneous models to collaboratively learn from healthcare institutions' data without violating users' privacy. Blockchain properties also allow the party to enjoy data integrity without trust in a centralized server while a
    
[^43]: 结构化状态空间模型用于反事实结果预测

    Counterfactual Outcome Prediction using Structured State Space Model. (arXiv:2305.09207v1 [cs.LG])

    [http://arxiv.org/abs/2305.09207](http://arxiv.org/abs/2305.09207)

    本文探讨了使用结构化状态空间模型对纵向数据中的反事实结果进行预测的方法，相比Treatment Effect Neural Controlled Differential Equation，S4Model更有效建模长期依赖关系、更易于训练、训练时间更短，且在归一化均方误差上提高了10倍。

    

    近年来，由于在医疗保健和社会科学中的潜在应用，纵向数据中的反事实结果预测越来越受到关注。本文探讨了使用状态空间模型（一种流行的序列模型）进行这项任务的方法。具体而言，我们比较了两种模型的性能：Treatment Effect Neural Controlled Differential Equation（TE-CDE）和结构化状态空间模型（S4Model）。虽然TE-CDE利用可控微分方程来解决时间依赖性混淆，但它存在优化问题和训练缓慢的问题。相比之下，S4Model更有效地建模长期依赖关系，并且更容易训练。我们在一个模拟肺部肿瘤增长数据集上评估了这些模型，发现S4Model在每个时期的训练时间减少了1.63倍，并且归一化均方误差也提高了10倍。此外，相较TE-CDE，S4Model在训练期间更稳定，对权重初始化也不太敏感。我们的结果表明，由于其高效和稳定性，S4Model是纵向数据中反事实结果预测更为有前途的方法。

    Counterfactual outcome prediction in longitudinal data has recently gained attention due to its potential applications in healthcare and social sciences. In this paper, we explore the use of the state space model, a popular sequence model, for this task. Specifically, we compare the performance of two models: Treatment Effect Neural Controlled Differential Equation (TE-CDE) and structured state space model (S4Model). While TE-CDE uses controlled differential equations to address time-dependent confounding, it suffers from optimization issues and slow training. In contrast, S4Model is more efficient at modeling long-range dependencies and easier to train. We evaluate the models on a simulated lung tumor growth dataset and find that S4Model outperforms TE-CDE with 1.63x reduction in per epoch training time and 10x better normalized mean squared error. Additionally, S4Model is more stable during training and less sensitive to weight initialization than TE-CDE. Our results suggest that the
    
[^44]: 权重莫比乌斯分数：一个特征归因的统一框架

    The Weighted M\"obius Score: A Unified Framework for Feature Attribution. (arXiv:2305.09204v1 [cs.LG])

    [http://arxiv.org/abs/2305.09204](http://arxiv.org/abs/2305.09204)

    本文提出了权重莫比乌斯分数作为一个参数化的归因框架，可以涵盖很多不同的特征归因方法，包括特征交互，解决了方法繁衍和不可比的问题。通过研究方法的向量空间，提供了一些新方法和解释，实证结果表明其多功能性和有效性。

    

    特征归因旨在通过识别每个特征对预测的影响来解释黑盒模型预测的推理过程。最近的工作将特征归因扩展到多个特征之间的交互。然而，缺乏统一的框架导致方法的大量繁衍，这些方法通常不能直接比较。本文介绍了一种参数化的归因框架——权重莫比乌斯分数，并显示了许多不同的针对单个特征和特征交互的归因方法是特例，还验证了一些新方法。通过研究归因方法的向量空间，我们的框架利用标准线性代数工具，并在各个领域提供解释，包括合作博弈理论和因果中介分析。我们通过将这些归因方法应用于情感分析中的特征交互来实证了框架的多功能性和有效性。

    Feature attribution aims to explain the reasoning behind a black-box model's prediction by identifying the impact of each feature on the prediction. Recent work has extended feature attribution to interactions between multiple features. However, the lack of a unified framework has led to a proliferation of methods that are often not directly comparable. This paper introduces a parameterized attribution framework -- the Weighted M\"obius Score -- and (i) shows that many different attribution methods for both individual features and feature interactions are special cases and (ii) identifies some new methods. By studying the vector space of attribution methods, our framework utilizes standard linear algebra tools and provides interpretations in various fields, including cooperative game theory and causal mediation analysis. We empirically demonstrate the framework's versatility and effectiveness by applying these attribution methods to feature interactions in sentiment analysis and chain-
    
[^45]: 我们能否忘记我们的学习方式？比较迭代信念修正中的状态表示(arXiv:2305.09200v1 [cs.AI])

    Can we forget how we learned? Representing states in iterated belief revision}. (arXiv:2305.09200v1 [cs.AI])

    [http://arxiv.org/abs/2305.09200](http://arxiv.org/abs/2305.09200)

    本文比较了迭代信念修正中的三种状态表示方法，证明了用字典序修订的重写历史是最有效率的，并提供了一个多项式时间算法，用于确定Horn公式是否等价于neg。

    

    本文比较了迭代信念修正中三种最常见的状态表示方法：显式表示，按层次表示和按历史表示。前者是模型之间的连通偏序关系，第二种是表示等价类的公式列表，第三种是先前修订的序列。后者取决于修订语义和历史重写，而前者则取决于允许的重写。所有机制都表示所有可能的状态。用字典序修订的重写历史在大小方面比其他考虑的表示方法更有效率。证明了这样一个历史的冗余是一种轻微的重写。在一般情况下，这是一个coNP完全问题，即使在Horn公式的两次修订历史或任意长度的修订历史上，这也是困难的，但在两个Horn公式的历史上，它是多项式的。一个次要的技术结果是一个多项式时间算法，用于确定一个Horn公式是否等价于neg。

    The three most common representations of states in iterated belief revision are compared: explicit, by levels and by history. The first is a connected preorder between models, the second is a list of formulae representing equivalence classes, the third is the sequence of the previous revisions. The latter depends on the revision semantics and on history rewriting, and the latter depends on the allowed rewritings. All mechanisms represent all possible states. A rewritten history of lexicographic revision is more efficient than the other considered representations in terms of size with arbitrary history rewritings. Establishing the redundancy of such a history is a mild rewriting. It is coNP-complete in the general case, and is hard even on histories of two revisions or revisions of arbitrary length of Horn formulae, and is polynomial on histories of two Horn formulae. A minor technical result is a polynomial-time algorithm for establishing whether a Horn formula is equivalent to the neg
    
[^46]: Ortho-ODE：增强神经常微分方程对抗攻击的鲁棒性

    Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks. (arXiv:2305.09179v1 [cs.LG])

    [http://arxiv.org/abs/2305.09179](http://arxiv.org/abs/2305.09179)

    本文研究了神经常微分方程（NODE）在面对噪声和对抗性攻击时所表现出的自然鲁棒性，并通过控制ODE动力学的Lipschitz常数来显著提高其鲁棒性。实验结果在多个数据集上得到了验证。

    

    神经常微分方程（NODE）通过使用数值求解器来求解由神经网络（NN）表示的微分方程，从而引发了一种具有无限深度的新型深度学习模型范式。 NODE旨在解决不规则时间序列问题。然而，NODE对各种噪声和对抗性攻击表现出了鲁棒性。本文研究NODE的自然鲁棒性并考察其中令人惊讶行为的原因。我们表明，通过控制ODE动力学的Lipschitz常数，可以显着提高其鲁棒性。我们从Grownwall不等式中推导出我们的方法。此外，我们绘制收缩理论和Grownwall不等式之间的类比。实验上，我们在多个数据集（MNIST、CIFAR-10和CIFAR 100）上证实了增强的鲁棒性。我们还展示了自适应和非自适应求解器对NODE鲁棒性的影响。

    Neural Ordinary Differential Equations (NODEs) probed the usage of numerical solvers to solve the differential equation characterized by a Neural Network (NN), therefore initiating a new paradigm of deep learning models with infinite depth. NODEs were designed to tackle the irregular time series problem. However, NODEs have demonstrated robustness against various noises and adversarial attacks. This paper is about the natural robustness of NODEs and examines the cause behind such surprising behaviour. We show that by controlling the Lipschitz constant of the ODE dynamics the robustness can be significantly improved. We derive our approach from Grownwall's inequality. Further, we draw parallels between contractivity theory and Grownwall's inequality. Experimentally we corroborate the enhanced robustness on numerous datasets - MNIST, CIFAR-10, and CIFAR 100. We also present the impact of adaptive and non-adaptive solvers on the robustness of NODEs.
    
[^47]: SUG：单数据集统一泛化用于 3D 点云分类

    SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification. (arXiv:2305.09160v1 [cs.CV])

    [http://arxiv.org/abs/2305.09160](http://arxiv.org/abs/2305.09160)

    本文提出了一种单数据集统一泛化（SUG）框架，通过多细粒度子域对齐和样本级域感知注意力策略，解决了三维点云领域泛化问题。

    

    虽然领域泛化（DG）问题在二维图像任务中增长迅速，但其在三维点云数据上的探索仍然不足，并且受到更复杂和不确定的跨域差异以及不均匀的跨类模态分布的挑战。与之前的二维 DG 工作不同，本文关注三维 DG 问题，并提出了一种 Single-dataset Unified Generalization（SUG）框架，该框架仅利用单个源数据集来缓解经过充分训练的源模型面临的未知域差异。具体来说，首先设计了一种多细粒度子域对齐（MSA）方法，通过在来自单个源数据集的分裂子域之间执行多细粒度特征对齐过程来约束学习表示为域不可知和有区别性。然后，提出了一种样本级域感知注意力（SDA）策略，该策略可以根据每个样本所属的不同子域选择性地增强易于适应的样本。

    Although Domain Generalization (DG) problem has been fast-growing in the 2D image tasks, its exploration on 3D point cloud data is still insufficient and challenged by more complex and uncertain cross-domain variances with uneven inter-class modality distribution. In this paper, different from previous 2D DG works, we focus on the 3D DG problem and propose a Single-dataset Unified Generalization (SUG) framework that only leverages a single source dataset to alleviate the unforeseen domain differences faced by a well-trained source model. Specifically, we first design a Multi-grained Sub-domain Alignment (MSA) method, which can constrain the learned representations to be domain-agnostic and discriminative, by performing a multi-grained feature alignment process between the splitted sub-domains from the single source dataset. Then, a Sample-level Domain-aware Attention (SDA) strategy is presented, which can selectively enhance easy-to-adapt samples from different sub-domains according to
    
[^48]: 可训练动态能量感知和自注意力网络的人类视觉动态处理模型

    Modelling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network for Adaptive Motion Integration. (arXiv:2305.09156v1 [cs.AI])

    [http://arxiv.org/abs/2305.09156](http://arxiv.org/abs/2305.09156)

    本文介绍了一个结合可训练动态能量感知和自注意力网络的可计算图像模型，旨在捕捉生物视觉系统中运动感知的核心结构计算过程，填补了从自然场景中提取信息的可计算图像模型的空白。

    

    视觉动态处理对于生物感知和与动态环境交互至关重要。尽管在认知神经科学领域进行了大量的研究，但尚未建立起能够以一种与人类视觉处理一致的方式从自然场景中提取信息的可计算图像模型。同时，由深度学习推动的计算机视觉的最近进展在光流估计方面取得了重大进展, 这是与运动感知密切相关的任务。本文提出了一个结合人类视觉模型和计算机视觉模型的可计算图像模型，旨在捕捉生物视觉系统中运动感知的核心结构V1-MT中的计算过程。

    Visual motion processing is essential for organisms to perceive and interact with dynamic environments. Despite extensive research in cognitive neuroscience, image-computable models that can extract informative motion flow from natural scenes in a manner consistent with human visual processing have yet to be established. Meanwhile, recent advancements in computer vision (CV), propelled by deep learning, have led to significant progress in optical flow estimation, a task closely related to motion perception. Here we propose an image-computable model of human motion perception by bridging the gap between human and CV models. Specifically, we introduce a novel two-stage approach that combines trainable motion energy sensing with a recurrent self-attention network for adaptive motion integration and segregation. This model architecture aims to capture the computations in V1-MT, the core structure for motion perception in the biological visual system. In silico neurophysiology reveals that 
    
[^49]: 双重对齐预训练用于跨语言句子嵌入

    Dual-Alignment Pre-training for Cross-lingual Sentence Embedding. (arXiv:2305.09148v1 [cs.CL])

    [http://arxiv.org/abs/2305.09148](http://arxiv.org/abs/2305.09148)

    该论文提出了一个双重对齐预训练框架，用于跨语言句子嵌入，它结合了句子级别和标记级别的对齐。引入了一种表示翻译学习任务，从而将翻译信息嵌入标记表示中。

    

    最近的研究表明，使用句子级别翻译排名任务训练的双编码器模型是跨语言句子嵌入的有效方法。然而，我们的研究表明，在多语言场景中，标记级别的对齐也是至关重要的，但此前尚未完全探索这一问题。基于我们的发现，我们提出了一个双重对齐预训练（DAP）框架，用于跨语言句子嵌入，它结合了句子级别和标记级别的对齐。为此，我们引入了一种新颖的表示翻译学习（RTL）任务，其中模型学习使用单侧上下文化的标记表示来重建其翻译对应物。这种重建目标鼓励模型将翻译信息嵌入标记表示中。与其他标记级别对齐方法（如翻译语言模型）相比，RTL更适合双编码器体系结构，而且计算效率更高。

    Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive
    
[^50]: 深层ReLU网络的多面体异常简单

    Deep ReLU Networks Have Surprisingly Simple Polytopes. (arXiv:2305.09145v1 [cs.LG])

    [http://arxiv.org/abs/2305.09145](http://arxiv.org/abs/2305.09145)

    本文通过计算和分析ReLU网络多面体的单纯形直方图，发现在初始化和梯度下降时它们结构相对简单，这说明了一种新的隐式偏见。

    

    ReLU网络是一种多面体上的分段线性函数。研究这种多面体的性质对于神经网络的研究和发展至关重要。目前，对于多面体的理论和实证研究仅停留在计算数量的水平，这远远不能完整地描述多面体。为了将特征提升到一个新的水平，我们提出通过三角剖分多面体得出多面体的形状。通过计算和分析不同多面体的单纯形直方图，我们发现ReLU网络在初始化和梯度下降时具有相对简单的多面体结构，尽管这些多面体从理论上来说可以非常丰富和复杂。这一发现可以被认为是一种新的隐式偏见。随后，我们使用非平凡的组合推导来理论上解释为什么增加深度不会创建更复杂的多面体，通过限制每个维度的平均单纯形数量。

    A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the av
    
[^51]: 记忆还是忘却？深入探讨语言模型的知识记忆机制

    Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. (arXiv:2305.09144v1 [cs.CL])

    [http://arxiv.org/abs/2305.09144](http://arxiv.org/abs/2305.09144)

    本论文研究了语言模型的记忆机制，发现预训练可以有效提高模型的记忆能力，而知识相关性和多样性对于记忆形成也有显著影响。

    

    记忆是最基本的认知功能之一，是存储世界知识和活动经历的储藏库。近年来，大规模预训练语言模型展现出卓越的记忆能力。相反，没有预训练的神经网络长期以来一直存在灾难性遗忘问题。为了研究这种保持-遗忘的矛盾并了解语言模型的记忆机制，我们通过控制目标知识类型、学习策略和学习时间表等，开展了深入的实验研究。结果发现：1）传统语言模型是容易遗忘的；2）预训练可以使语言模型具有记忆能力；3）知识相关性和多样性显著影响记忆形成。这些结论有助于理解预训练语言模型的能力，并为设计和评估新的语言模型学习方法和推理算法提供了启示。

    Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memory mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules. We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation. These conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.
    
[^52]: 关于Wordle和一般猜词游戏中的最优策略

    On Optimal Strategies for Wordle and General Guessing Games. (arXiv:2305.09111v1 [cs.AI])

    [http://arxiv.org/abs/2305.09111](http://arxiv.org/abs/2305.09111)

    该论文提出了一种通用的方法来寻找猜词游戏的最优策略，并且证明了该方法的有效性（快速找到最优解），以Wordle猜词游戏为例具体说明。

    

    最近Wordle游戏的火爆引发了人们对猜词游戏的兴趣。我们提出了一种寻找猜词游戏最优策略的通用方法，避免了枚举搜索。我们的主要贡献是几个定理，构建一个证明猜词游戏策略最优性的通用理论。这项工作可以应用于任意猜词游戏，我们以Wordle游戏作为例子展示具体结果。

    The recent popularity of Wordle has revived interest in guessing games. We develop a general method for finding optimal strategies for guessing games while avoiding an exhaustive search. Our main contributions are several theorems that build towards a general theory to prove the optimality of a strategy for a guessing game. This work is developed to apply to any guessing game, but we use Wordle as an example to present concrete results.
    
[^53]: 视频值得 $n\times n$ 张图像吗? 一种基于Transformer的视频问答高效方法。

    Is a Video worth $n\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering. (arXiv:2305.09107v1 [cs.CV])

    [http://arxiv.org/abs/2305.09107](http://arxiv.org/abs/2305.09107)

    本文提出了一种高效的Transformer-based Video Question Answering方法，即将视频帧连接成 $n\times n$ 的矩阵，从而将图像编码器的使用量从 $n^{2}$ 减少到1，从而显著提高了训练和推理速度和节省了存储空间，而仍然保持了原始视频的时间结构，并在实验中取得了较好结果。

    

    传统的基于Transformer的视频问答（VideoQA）方法通常通过一个或多个图像编码器独立编码帧，并在帧和问题之间进行交互。然而，这种模式会导致显著的内存使用和训练和推理速度的不可避免的减慢。本文提出了一种基于现有视觉-语言预训练模型的高效VideoQA方法，其中我们将视频帧连接到一个 $n\times n$ 矩阵中，然后将其转换为一张图像。通过这样做，我们将图像编码器的使用从 $n^{2}$减少到1，同时保持了原始视频的时间结构。在MSRVTT和TrafficQA上的实验结果表明，我们提出的方法以近 $4\times$ 更快的速度和只有30％的内存使用实现了最先进的性能。我们展示了通过将我们的方法集成到VideoQA系统中，我们能够在只有很小代价的情况下实现可比甚至优异的表现，同时训练和推理速度显著加快。

    Conventional Transformer-based Video Question Answering (VideoQA) approaches generally encode frames independently through one or more image encoders followed by interaction between frames and question. However, such schema would incur significant memory use and inevitably slow down the training and inference speed. In this work, we present a highly efficient approach for VideoQA based on existing vision-language pre-trained models where we concatenate video frames to a $n\times n$ matrix and then convert it to one image. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$ while maintaining the temporal structure of the original video. Experimental results on MSRVTT and TrafficQA show that our proposed approach achieves state-of-the-art performance with nearly $4\times$ faster speed and only 30% memory use. We show that by integrating our approach into VideoQA systems we can achieve comparable, even superior, performance with a significant speed up for training and 
    
[^54]: AAAI 2022 秋季研讨会：基于认知通用模型实现的 System-1 和 System-2

    AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition. (arXiv:2305.09091v1 [cs.AI])

    [http://arxiv.org/abs/2305.09091](http://arxiv.org/abs/2305.09091)

    该论文提出了一个基于认知通用模型的 System-1 和 System-2 实现方法，解决了其区分不清的问题。通用模型提供了对涉及 System-1 和 System-2 的计算单元、基础机制和其对学习、元认知和情感的影响的全面视野。

    

    AI 领域中对 System-1 和 System-2 二元模型的引入一直受到其区分不清的阻碍。我们通过将 System-1 和 System-2 置于认知通用模型中来解决此问题和其他问题。结果显示，被认为是 System-1 和 2 特征的不同之处实际上形成了一种认知属性的范围。通用模型提供了有关涉及 System-1 和 System-2 的计算单元、其基础机制以及对学习、元认知和情感的影响的全面视野。

    Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and System-2 within the Common Model of Cognition. Results show that what are thought to be distinctive characteristics of System-1 and 2 instead form a spectrum of cognitive properties. The Common Model provides a comprehensive vision of the computational units involved in System-1 and System-2, their underlying mechanisms, and the implications for learning, metacognition, and emotion.
    
[^55]: 捕捉人类对AI的心理模型：基于IRT方法的研究

    Capturing Humans' Mental Models of AI: An Item Response Theory Approach. (arXiv:2305.09064v1 [cs.LG])

    [http://arxiv.org/abs/2305.09064](http://arxiv.org/abs/2305.09064)

    本研究提出基于IRT的模型来分析人类对于AI的感知，实验结果表明人们普遍期望AI团队伙伴的表现优于人类，并且人们对AI与人类成员的心理模型不同。

    

    深入了解人类如何感知与AI搭档的信息，是我们理解人工智能团队合作的重要基础。我们借鉴认知科学的相关研究，提出了一个基于IRT的框架来模拟人们的感知，针对人与人、人与AI进行实验，并重点研究两者感知之间的异同，以及人们对AI团队伙伴的期望。我们的研究结果表明，人们普遍期望人工智能的表现优于人类，而且对AI成员的心理模型与人类成员的不同。同时，我们还发现学习、反馈和个体差异等因素同样有可能影响人们的心理模型。我们的研究为捕捉和分析人类对不同情境中的AI感知提供了一种有用工具。

    Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate's performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people's perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception. Our results indicate that people expect AI agents' performance to be significantly better on average than the performance of other hu
    
[^56]: SuSana Distance是你所需要的：通过两种新的基于距离的损失函数实现度量学习中的类别可分性，用于少样本图像分类

    SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification. (arXiv:2305.09062v1 [cs.CV])

    [http://arxiv.org/abs/2305.09062](http://arxiv.org/abs/2305.09062)

    本文提出了两种新的基于距离的损失函数，通过考虑少量数据之间的类内距离和类间距离来考虑嵌入向量的重要性，以增强度量学习中的类别可分性，实现优秀的少样本图像分类表现。

    

    少样本学习是一项具有挑战性的研究领域，旨在仅利用少量标记数据样本学习新的概念。基于度量学习方法的最近工作利用元学习方法，其中包括支持（训练）和查询集（测试），旨在学习这些集合之间的相似性比较度量。由于数据缺乏，嵌入网络的学习过程成为少样本任务的重要部分。先前的工作使用度量学习方法解决了这个问题，但底层潜在空间的属性和类别之间的可分性并未完全强制执行。在这项工作中，我们提出了两种不同的损失函数，通过考虑少量数据之间的类内距离和类间距离来考虑嵌入向量的重要性。第一个损失函数是Proto-Triplet Loss，它基于原始三元组损失，并添加了原型向量。第二个损失函数是SuSana Distance Loss，它考虑了同类样本和不同类样本之间的距离。我们在两个少样本图像分类的基准数据集上评估了我们提出的方法，实验结果证明了我们的方法在实现类别可分性和达到最先进性能方面的有效性。

    Few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. Recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. Due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. Previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. In this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. The first loss function is the Proto-Triplet Loss, which is based on the original triplet loss with 
    
[^57]: LoViT: 用于手术阶段识别的长视频Transformer

    LoViT: Long Video Transformer for Surgical Phase Recognition. (arXiv:2305.08989v1 [cs.CV])

    [http://arxiv.org/abs/2305.08989](http://arxiv.org/abs/2305.08989)

    LoViT是一种用于手术阶段识别的长视频Transformer，它通过结合时间丰富的空间特征提取器和多尺度时间聚合器来对长视频进行分析，优于现有方法。

    

    在构建能够量化表现并监督手术流程执行的上下文工具方面，在线手术阶段识别发挥着重要作用。目前的方法有限，因为它们使用基于帧级监督的空间特征提取器进行训练，这可能会导致由于相似帧在不同阶段出现而导致的错误预测，并且由于计算限制而未能很好地融合本地和全局特征，这可能影响手术干预中通常遇到的长视频的分析。在本文中，我们提出了一种名为LoViT的两阶段方法，用于融合短期和长期时间信息，它结合了一个时间丰富的空间特征提取器和一个多尺度时间聚合器，后者由基于自注意力的两个级联L-Trans模块和一个基于ProbSparse自注意力的G-Informer模块组成，用于处理全局时间信息。然后，多尺度时间头结合本地和全局特征来识别长视频中的手术阶段。我们展示了LoViT在两个公共手术数据集上优于现有方法。

    Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT) for fusing short- and long-term temporal information that combines a temporally-rich spatial feature extractor and a multi-scale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then combines loc
    
[^58]: 异构数据集的联邦学习方法

    Federated Learning over Harmonized Data Silos. (arXiv:2305.08985v1 [cs.LG])

    [http://arxiv.org/abs/2305.08985](http://arxiv.org/abs/2305.08985)

    该论文提出了一种解决异构数据集的联邦学习方法，通过将数据协调和数据填补等关键步骤纳入架构愿景，来促进数据管理信息系统和机器学习的交叉研究。

    

    联邦学习是一种分布式机器学习方法，它可以使地理分布的数据集共同学习一个机器学习模型，而无需共享数据。然而，现有的大部分工作都是基于非结构化数据（如图像或文本），或者基于在不同的站点上具有一致结构的结构化数据。然而，实际情况是各站点具有不同的模式、数据格式、数据值和访问模式，因此需要数据集成方法来解决这些挑战，包括使用声明性模式映射进行数据交换和查询重写，以及实体连接等。因此，我们提出了一个集成了数据协调和数据填补等关键步骤的端到端联邦学习和集成系统的架构愿景，以促进数据管理信息系统和机器学习的交叉研究。

    Federated Learning is a distributed machine learning approach that enables geographically distributed data silos to collaboratively learn a joint machine learning model without sharing data. Most of the existing work operates on unstructured data, such as images or text, or on structured data assumed to be consistent across the different sites. However, sites often have different schemata, data formats, data values, and access patterns. The field of data integration has developed many methods to address these challenges, including techniques for data exchange and query rewriting using declarative schema mappings, and for entity linkage. Therefore, we propose an architectural vision for an end-to-end Federated Learning and Integration system, incorporating the critical steps of data harmonization and data imputation, to spur further research on the intersection of data management information systems and machine learning.
    
[^59]: 基于模块化动作程序的动作问答

    Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])

    [http://arxiv.org/abs/2305.08953](http://arxiv.org/abs/2305.08953)

    提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。

    

    为了构建能够感知和理解真实世界中的人类行为的人工智能系统，我们必须首先设计模型，对动作序列进行复杂的时空推理。为了实现这个目标，我们提出了HumanMotionQA任务，评估模型在长时间人类运动序列上进行复杂、多步推理的能力。我们生成了一个问答对数据集，需要在动作序列的小部分中检测运动线索，对事件发生的时间进行推理，并查询特定的运动属性。此外，我们还提出了NSPose，一种神经符号方法，用于处理该任务，它利用符号化推理和模块化设计，通过学习运动概念、属性神经操作符和时间关系，来处理动作。我们证明了NSPose在HumanMotionQA任务中的适用性，胜过了所有基线方法。

    In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
    
[^60]: MIMEx: 遮盖输入建模的内在奖励

    MIMEx: Intrinsic Rewards from Masked Input Modeling. (arXiv:2305.08932v1 [cs.LG])

    [http://arxiv.org/abs/2305.08932](http://arxiv.org/abs/2305.08932)

    MIMEx是一个通用的框架，它使用遮盖输入建模来提取内在奖励，通过控制遮盖分布来控制难度，可以在高维环境中取得优越的探索结果。

    

    在高维观测环境中进行探索很困难。使用内在奖励的一种有前途的方法是使用深度网络估计状态、转换或轨迹的“新颖性”。之前的研究表明，条件预测目标，如遮盖自动编码可看作伪似然的随机估计。我们展示了这一观点如何自然地导致对现有内在奖励方法的统一看法:它们是条件预测的特例，在这种情况下，新颖性的估计可以看作是使用不同的遮盖分布进行伪似然估计。从这个角度，我们提出了一个通用的框架——遮盖输入建模探索内在奖励(MIMEx)，其中遮盖分布可以灵活调整以控制底层条件预测任务的难度。我们演示了当与竞争方法相比时，MIMEx可以取得优越的结果。

    Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating "novelty" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive
    
[^61]: AF2-Mutation：针对AlphaFold2的蛋白三级结构预测的对抗序列突变

    AF2-Mutation: Adversarial Sequence Mutations against AlphaFold2 on Protein Tertiary Structure Prediction. (arXiv:2305.08929v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.08929](http://arxiv.org/abs/2305.08929)

    本文研究了针对AlphaFold2的对抗序列突变，实测仅修改三个氨基酸残基，就使其预测的蛋白质三级结构的改变达到了46.61，并能成功发现在特定蛋白质中对结构至关重要且有生物学意义的氨基酸残基，为蛋白质的修饰提供了新思路。

    

    基于深度学习的方法，如AlphaFold2（AF2），取得了与真实生物实验方法可比的蛋白质三级结构预测效果。虽然AF2在预测变异效果方面存在局限性，但其对序列变异的鲁棒性尚待确定。本文从野生型（WT）序列开始，通过进化方法生成对抗序列，AF2预测与WT存在显著差异。我们在CASP14上的实验证明，在蛋白质序列中仅修改3个氨基酸残基，采用替换、删除和插入策略的组合，AF2预测的本地距离差异测试（lDDT）的改变达到了46.61。此外，当应用于特定蛋白质SPNS2时，我们提出的算法成功地识别了对蛋白质结构确定至关重要且有生物学意义的氨基酸残基，可能表明可修改的蛋白质部位。

    Deep learning-based approaches, such as AlphaFold2 (AF2), have significantly advanced protein tertiary structure prediction, achieving results comparable to real biological experimental methods. While AF2 has shown limitations in predicting the effects of mutations, its robustness against sequence mutations remains to be determined. Starting with the wild-type (WT) sequence, we investigate adversarial sequences generated via an evolutionary approach, which AF2 predicts to be substantially different from WT. Our experiments on CASP14 reveal that by modifying merely three residues in the protein sequence using a combination of replacement, deletion, and insertion strategies, the alteration in AF2's predictions, as measured by the Local Distance Difference Test (lDDT), reaches 46.61. Moreover, when applied to a specific protein, SPNS2, our proposed algorithm successfully identifies biologically meaningful residues critical to protein structure determination and potentially indicates alter
    
[^62]: 差分卷积模糊时间序列预测

    Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v1 [cs.LG])

    [http://arxiv.org/abs/2305.08890](http://arxiv.org/abs/2305.08890)

    本文提出了一种新的预测模型DFCNN，利用卷积神经网络实现具有可学习能力的FTSF，并能够处理非平稳时间序列。

    

    模糊时间序列预测（FTSF）是一种具有广泛应用的典型预测方法。传统的FTSF被认为是一种专家系统，导致失去了识别未定义特征的能力，这是FTSF预测不准确的主要原因。为了解决这个问题，提出了差分模糊卷积神经网络（DFCNN）模型，利用卷积神经网络实现具有可学习能力的FTSF。DFCNN能够识别潜在信息并改善预测精度。由于神经网络的可学习能力，FTSF建立的模糊规则的长度可以任意扩展，这是专家系统所无法处理的。同时，由于非平稳时间序列的趋势，FTSF通常无法实现令人满意的性能。非平稳时间序列的趋势会导致FTSF建立的模糊集失效，并导致预测失败。DFCNN利用卷积神经网络的学习能力，可以处理非平稳时间序列。

    Fuzzy time series forecasting (FTSF) is a typical forecasting method with wide application. Traditional FTSF is regarded as an expert system which leads to lose the ability to recognize undefined feature. The mentioned is main reason of poor forecasting with FTSF. To solve the problem, the proposed model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes convolution neural network to re-implement FTSF with learnable ability. DFCNN is capable of recognizing the potential information and improve the forecasting accuracy. Thanks to learnable ability of neural network, length of fuzzy rules established in FTSF is expended to arbitrary length which expert is not able to be handle by expert system. At the same time, FTSF usually cannot achieve satisfactory performance of non-stationary time series due to trend of non-stationary time series. The trend of non-stationary time series causes the fuzzy set established by FTSF to invalid and cause the forecasting to fail. DFCNN utiliz
    
[^63]: 黑盒语言模型生成的水印文本

    Watermarking Text Generated by Black-Box Language Models. (arXiv:2305.08883v1 [cs.CL])

    [http://arxiv.org/abs/2305.08883](http://arxiv.org/abs/2305.08883)

    本研究提出了一种利用Transformer注意力图来获得词级反馈的新型黑盒LLM水印算法，实现对由黑盒语言模型生成的文本的水印保护。

    

    LLM现在展示了在各领域中与人类类似的技能，引发人们对误用的担忧。因此，检测生成的文本至关重要。然而，被动式检测方法陷入了领域特异性和有限的对抗强度。为了实现可靠的检测，提出了一种基于水印的白盒LLM方法，使其能够在文本生成过程中嵌入水印。该方法涉及将模型词汇随机分割以获得特殊列表并调整概率分布以促进列表中单词的选择。一个知晓列表的检测算法可以识别带水印的文本。然而，这种方法在许多仅有黑盒语言模型的真实场景中不适用。为了允许第三方给由黑盒语言模型生成的文本加上水印，本文提出了一种利用转换器注意力图来获得词级反馈的新水印算法。实验结果表明，所提出的方法可以有效地给由黑盒语言模型生成的文本加上水印，而不影响文本生成的质量。

    LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-part
    
[^64]: 突触网络初始化与射频崩溃

    Spiking Network Initialisation and Firing Rate Collapse. (arXiv:2305.08879v1 [cs.NE])

    [http://arxiv.org/abs/2305.08879](http://arxiv.org/abs/2305.08879)

    该论文尝试通过使用ANN初始化文献中的技术以及计算神经科学结果来解决突触神经网络（SNN）初始化的问题。作者指出，与ANNs相比，SNNs的重量初始化问题更为微妙，因为SNNs具有尖峰和重置非线性以及射频崩溃问题，为此作者提出并解决了几种射频崩溃问题的方案。

    

    近年来，新开发的训练突触神经网络（SNN）的方法已经证明了它们在准确性方面可以作为人工神经网络（ANN）的替代品，同时在推理和潜在的训练时间上都更加节能。然而，对于SNN，什么是一个好的初始化仍然不清楚。我们经常使用为ANN训练开发的初始化方案，这些方案经常是不足的，需要手动调整。在本文中，我们尝试通过使用ANN初始化文献中的技术以及计算神经科学结果来解决这个问题。我们表明，由于SNN的尖峰和重置非线性以及射频崩溃问题，与ANNs相比，ANNs的重量初始化问题更为微妙。我们首先确定并提出了几种解决射频崩溃问题的方案，在不同的假设下成功地解决了这个问题。

    In recent years, newly developed methods to train spiking neural networks (SNNs) have rendered them as a plausible alternative to Artificial Neural Networks (ANNs) in terms of accuracy, while at the same time being much more energy efficient at inference and potentially at training time. However, it is still unclear what constitutes a good initialisation for an SNN. We often use initialisation schemes developed for ANN training which are often inadequate and require manual tuning. In this paper, we attempt to tackle this issue by using techniques from the ANN initialisation literature as well as computational neuroscience results. We show that the problem of weight initialisation for ANNs is a more nuanced problem than it is for ANNs due to the spike-and-reset non-linearity of SNNs and the firing rate collapse problem. We firstly identify and propose several solutions to the firing rate collapse problem under different sets of assumptions which successfully solve the issue by leveragin
    
[^65]: 神经符号人工智能及其分类法：一项调查研究

    Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])

    [http://arxiv.org/abs/2305.08876](http://arxiv.org/abs/2305.08876)

    本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。

    

    神经符号人工智能涉及组合符号处理（如经典人工智能）和神经网络的模型，是一个非常成熟的领域。这些模型作为实现人工智能通用性的一种尝试，在探索除了增加数据集和模型尺寸以外的替代方案以及将学习数据分布和推理先前和学习知识相结合方面具有独特作用。本次调查研究了这一领域近年来的研究论文，并提供了这些模型的分类和比较，同时介绍了应用案例。

    Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
    
[^66]: 在串行数据同化中，基于在线机器学习的预测不确定性估计

    Online machine-learning forecast uncertainty estimation for sequential data assimilation. (arXiv:2305.08874v1 [physics.ao-ph])

    [http://arxiv.org/abs/2305.08874](http://arxiv.org/abs/2305.08874)

    该论文提出了一种使用卷积神经网络进行在线机器学习的方法，在单个动力学模型集成中估计表示预测误差协方差矩阵的状态依赖预测不确定性，并在混合数据同化方法中应用。通过模拟实验证明，该方法在准确性和计算成本方面优于传统的基于集合的方法。

    

    量化预测不确定性是现代数值天气预报和数据同化系统中的关键。基于集合的数据同化系统采用多个模型集成来包含状态依赖的不确定性量化,但这种方法在计算和开发方面具有挑战性。本文提出了一种基于卷积神经网络的机器学习方法，使用单个动力学模型集成估计表示预测误差协方差矩阵的状态依赖预测不确定性。这是通过利用损失函数实现的，该函数考虑了预测误差的异方差性。该方法的性能在混合数据同化方法中进行了研究，该方法结合了Kalman样式分析更新和基于机器学习的状态依赖预测误差协方差矩阵的估计。使用Lorensso等（2006）模型进行了观测系统模拟实验，结果表明，与常用的基于集合的方法相比，机器学习方法在准确性和计算成本方面表现更好。

    Quantifying forecast uncertainty is a key aspect of state-of-the-art numerical weather prediction and data assimilation systems. Ensemble-based data assimilation systems incorporate state-dependent uncertainty quantification based on multiple model integrations. However, this approach is demanding in terms of computations and development. In this work a machine learning method is presented based on convolutional neural networks that estimates the state-dependent forecast uncertainty represented by the forecast error covariance matrix using a single dynamical model integration. This is achieved by the use of a loss function that takes into account the fact that the forecast errors are heterodastic. The performance of this approach is examined within a hybrid data assimilation method that combines a Kalman-like analysis update and the machine learning based estimation of a state-dependent forecast error covariance matrix. Observing system simulation experiments are conducted using the Lo
    
[^67]: 自动化隐私决策：何时画一条线？

    Automating privacy decisions -- where to draw the line?. (arXiv:2305.08747v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2305.08747](http://arxiv.org/abs/2305.08747)

    快速发展的互联网、移动和IoT技术中，用户需要管理大量的个人数据，但是决定如何保护隐私是一个令人不知所措的挑战。该论文提供了自动化隐私决策的现有挑战和解决方案的概述。

    

    用户通常对管理其个人数据的隐私决策感到不知所措，这些决策可以出现在网络、移动和物联网环境中。这些决策可以以各种形式出现——比如设置隐私权限或偏好、回答同意请求或干预并“拒绝”处理个人数据——每一种都可能具有不同的法律影响。针对所有情况和所有类型的决策，学者和工业界一直在提出不同层次的工具，以改善隐私决策流程的自动化，以提高易用性。本文概述了自动化隐私决策所涉及到的主要挑战，并分类介绍了现有和预期的工作和提议，这些工作和提议都致力于实现隐私决策的自动化。

    Users are often overwhelmed by privacy decisions to manage their personal data, which can happen on the web, in mobile, and in IoT environments. These decisions can take various forms -- such as decisions for setting privacy permissions or privacy preferences, decisions responding to consent requests, or to intervene and ``reject'' processing of one's personal data --, and each can have different legal impacts. In all cases and for all types of decisions, scholars and industry have been proposing tools to better automate the process of privacy decisions at different levels, in order to enhance usability. We provide in this paper an overview of the main challenges raised by the automation of privacy decisions, together with a classification scheme of the existing and envisioned work and proposals addressing automation of privacy decisions.
    
[^68]: 知识图谱的结构和动态，以及其表层性质

    The Structure and Dynamics of Knowledge Graphs, with Superficiality. (arXiv:2305.08116v1 [cs.AI])

    [http://arxiv.org/abs/2305.08116](http://arxiv.org/abs/2305.08116)

    该论文提出了第一个知识图谱的结构和动态模型，并引入表层性来简单建模复杂特征，从而掌握全局知识分布的平衡。

    

    大型知识图谱综合了从学术机构和企业到大众集资等项目中获得的人类知识，每两个节点之间的关系代表这两个实体之间的基本事实。关系语义的多样性组成了知识图谱的丰富性，导致出现有时混乱的奇异拓扑结构。然而，这种复杂特征可以通过引入表层性的概念来简单建模，表层性控制着独立生成事实的关系之间的重叠情况，也通过确定错误描述实体的比例来控制全局知识分布的平衡。这是知识图谱结构和动态方面的首个模型，有助于更好地理解正式知识获取和组织。

    Large knowledge graphs combine human knowledge garnered from projects ranging from academia and institutions to enterprises and crowdsourcing. Within such graphs, each relationship between two nodes represents a basic fact involving these two entities. The diversity of the semantics of relationships constitutes the richness of knowledge graphs, leading to the emergence of singular topologies, sometimes chaotic in appearance. However, this complex characteristic can be modeled in a simple way by introducing the concept of superficiality, which controls the overlap between relationships whose facts are generated independently. Superficiality also regulates the balance of the global distribution of knowledge by determining the proportion of misdescribed entities. This is the first model for the structure and dynamics of knowledge graphs. It leads to a better understanding of formal knowledge acquisition and organization.
    
[^69]: 基于表面肌电图像的轻量级全卷积神经网络和迁移学习的跨场景手势识别

    Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])

    [http://arxiv.org/abs/2305.08014](http://arxiv.org/abs/2305.08014)

    本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。

    

    利用低分辨率瞬时高清肌电图像进行手势识别可以开辟发展更流畅、更自然的肌肉-计算机界面的新途径。然而，跨场景数据的变异性存在极大的挑战。现有的方法采用非常大且复杂的深度卷积神经网络或基于2SRNN的领域适应方法，来逼近由这些跨场景数据变异性引起的分布偏移。因此，这些方法也需要在预训练和适应阶段中在数百万个训练参数和大规模预训练数据集上进行学习。结果，这使得在实时应用中进行高端资源约束和计算非常昂贵的部署。为了解决这个问题，我们提出了一种轻量级的全卷积神经网络+迁移学习模型，利用轻量级全卷积神经网络和迁移学习(TL)来增强跨场景手势识别。

    Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
    
[^70]: 寻求可验证性: 解释很少能够在AI辅助决策中提高决策性能

    In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])

    [http://arxiv.org/abs/2305.07722](http://arxiv.org/abs/2305.07722)

    AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。

    

    目前关于AI辅助决策的文献，涉及可解释的AI系统为人类决策者提供建议，并呈现出一系列不确定和令人困惑的结果。为了综合这些发现，我们提出了一个简单的理论，阐明了AI解释经常无法促使适当的依赖和互补决策表现的失败。我们认为解释只有在允许人类决策者验证AI预测的正确性时才有用，而不是其他期望，例如可解释性或清晰阐述AI的推理过程。先前的研究发现，在许多决策环境中，AI解释并未促进这种验证。此外，无论解释方法如何，大多数环境基本上都无法进行验证。我们最后讨论了更有效的可解释AI辅助决策和人工智能协作的潜在方法。

    The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
    
[^71]: 具有门控汇总模块的值迭代网络研究

    Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])

    [http://arxiv.org/abs/2305.07039](http://arxiv.org/abs/2305.07039)

    本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。

    

    本文针对值迭代网络（VIN）在处理更大的输入地图，减轻由增加迭代次数引起的累积误差的挑战提出了一种新方法——具有门控汇总模块的值迭代网络（GS-VIN）。我们提出自适应迭代策略，利用更大的卷积核减少迭代次数，减少网络深度，提高训练稳定性，同时保持计划过程的准确性。我们还引入了门控汇总模块，使得网络可以强调整个规划过程，而不仅仅依赖于最终的全局规划结果。

    In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
    
[^72]: 自主GIS：下一代基于人工智能的GIS

    Autonomous GIS: the next-generation AI-powered GIS. (arXiv:2305.06453v1 [cs.AI])

    [http://arxiv.org/abs/2305.06453](http://arxiv.org/abs/2305.06453)

    自主GIS是一种AI动力地理信息系统，采用大型语言模型作为推理核心，具有自动空间数据收集、分析和可视化的能力，旨在实现五个自主目标：自动生成、自组织、自验证、自执行和自生长。

    

    大型语言模型（LLM）如 ChatGPT ，展示了对人类自然语言的强大理解能力，在推理、创造性写作、代码生成、翻译和信息检索等领域得到了应用与探索。我们采用LLM作为推理核心，提出了一种称之为“自主GIS”的AI动力地理信息系统（GIS），以自动空间数据收集、分析和可视化来解决空间问题。我们设想，自主GIS将需要实现五个自主目标，包括自动生成、自组织、自验证、自执行和自生长。我们引入了自主GIS的设计原则来实现这五个自主目标，从信息充分性、LLM能力和代理架构三个方面进行。我们开发了一个原型系统称为LLM-Geo ，它在Python环境中使用GPT-4 API。

    Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting LLM as the reasoning core, we propose Autonomous GIS, an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning and coding for addressing spatial problems with automatic spatial data collection, analysis and visualization. We envision that autonomous GIS will need to achieve five autonomous goals including self-generating, self-organizing, self-verifying, self-executing, and self-growing. We introduce the design principles of autonomous GIS to achieve these five autonomous goals from the aspects of information sufficiency, LLM ability, and agent architecture. We developed a prototype system called LLM-Geo using GPT-4 API in a Python environme
    
[^73]: 语义嵌入深度神经网络：一种提升多标签图像分类性能的通用方法。

    Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance. (arXiv:2305.05228v1 [cs.CV])

    [http://arxiv.org/abs/2305.05228](http://arxiv.org/abs/2305.05228)

    本研究提出了一种通用的语义嵌入深度神经网络，通过空间感知的语义特征和基于通道的注意力模型来提高多标签预测的模型性能，平均相对改进达到15.27%。

    

    精细的多标签分类模型在亚马逊生产功能中具有广泛的应用，例如基于视觉的标签预测，从时尚属性检测到品牌识别。实现这些分类任务的一个挑战是野外视觉背景信号，其中包含混淆模型的无关像素，使模型难以专注于感兴趣区域并根据该特定区域进行预测。在本文中，我们介绍了一种通用的语义嵌入深度神经网络，应用空间感知的语义特征，并结合基于通道的注意力模型来利用定位引导，以提高多标签预测的模型性能。与基线方法相比，我们观察到所有标签的AUC得分的平均相对改进为15.27%。核心实验和消融研究涉及对Instagram时尚服装的多标签时尚属性分类进行的。

    Fine-grained multi-label classification models have broad applications in Amazon production features, such as visual based label predictions ranging from fashion attribute detection to brand recognition. One challenge to achieve satisfactory performance for those classification tasks in real world is the wild visual background signal that contains irrelevant pixels which confuses model to focus onto the region of interest and make prediction upon the specific region. In this paper, we introduce a generic semantic- embedding deep neural network to apply the spatial awareness semantic feature incorporating a channel- wise attention based model to leverage the localization guidance to boost model performance for multi- label prediction. We observed an Avg.relative improvement of 15.27% in terms of AUC score across all labels compared to the baseline approach. Core experiment and ablation studies involve multi-label fashion attribute classification performed on Instagram fashion apparels' 
    
[^74]: 针对时尚检测的不平衡标签样本分布的数据高效训练

    Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection. (arXiv:2305.04379v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.04379](http://arxiv.org/abs/2305.04379)

    本文提出了一种最先进的加权目标函数，用于提高多标签分类中深度神经网络（DNN）针对长尾数据分布的性能，并通过对时尚服装的图像属性分类的实验，取得了良好的性能。

    

    多标签分类模型在电子商务中有广泛的应用，包括基于视觉的标签预测以及基于语言的情感分类。实现这些任务的一个主要难点是数据分布的显著不平衡。为了解决这个问题，本文探索了更多的数据高效模型训练技术，并提出了一种最先进的加权目标函数，用于提高多标签分类中深度神经网络（DNN）针对长尾数据分布的性能。我们的实验涉及时尚服装的基于图像的属性分类，并且结果表明，新的加权目标函数相较于传统方法具有良好的性能。

    Multi-label classification models have a wide range of applications in E-commerce, including visual-based label predictions and language-based sentiment classifications. A major challenge in achieving satisfactory performance for these tasks in the real world is the notable imbalance in data distribution. For instance, in fashion attribute detection, there may be only six 'puff sleeve' clothes among 1000 products in most E-commerce fashion catalogs. To address this issue, we explore more data-efficient model training techniques rather than acquiring a huge amount of annotations to collect sufficient samples, which is neither economic nor scalable. In this paper, we propose a state-of-the-art weighted objective function to boost the performance of deep neural networks (DNNs) for multi-label classification with long-tailed data distribution. Our experiments involve image-based attribute classification of fashion apparels, and the results demonstrate favorable performance for the new weig
    
[^75]: 大语言模型在信息技术任务中自动生成YAML代码

    Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])

    [http://arxiv.org/abs/2305.02783](http://arxiv.org/abs/2305.02783)

    这项研究提出了一种名为Ansible Wisdom的自然语言转Ansible-YAML代码的工具，可自动化生成Ansible脚本，提高IT自动化生产力，并相比现有技术达到或更好的性能水平。

    

    由于大语言模型在代码生成方面的不断提升，在通用编程语言方面的受益最大，而针对IT自动化等领域特定语言的研究较少。本研究聚焦于Ansible-YAML的生成，提出了一种名为Ansible Wisdom的自然语言转Ansible-YAML代码的工具，旨在提高IT自动化生产力。研究采用基于Transformer的模型，并通过新的包含Ansible-YAML的数据集进行扩展训练。同时，还开发了两个用于捕捉此领域特征的YAML和Ansible性能指标。结果表明，Ansible Wisdom可以精确地从自然语言提示中生成Ansible脚本，并且其性能可与现有技术的状态相媲美或更好。

    The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
    
[^76]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^77]: LLT：线性定律特征空间变换的R包

    LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])

    [http://arxiv.org/abs/2304.14211](http://arxiv.org/abs/2304.14211)

    LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。

    

    线性定律特征空间转换(LLT )算法的目标是帮助对单变量和多变量时间序列进行分类。LLT R包以灵活和用户友好的方式实现了该算法。该包将实例分为训练和测试集，并利用时延嵌入和谱分解技术，识别训练集中每个输入序列(初始特征)的控制模式(称为线性定律)。最后，它应用训练集的线性定律来转换测试集的初始特征。trainTest、trainLaw和testTrans三个单独的函数来执行这些步骤，它们需要预定义的数据结构;然而，为了快速计算，它们只使用内置函数。LLT R包和适当数据结构的示例数据集在GitHub上公开可用。

    The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
    
[^78]: 通过多样性权重实现生成模型的模式平衡

    Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11961](http://arxiv.org/abs/2304.11961)

    本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。

    

    大型数据驱动的图像模型被广泛用于支持创意和艺术作品。在当前主导的分布拟合范式下，数据集被视为要尽可能接近的真实值。然而，许多创意应用需要多样化的输出，创作者经常努力从给定的数据分布中积极分离出来。我们认为，从纯模式覆盖转向模式平衡的建模目标调整是必要的，以适应更高的输出多样性目标。我们提出了多样性权重，这是一种通过平衡训练数据集中的模式来增加模型输出多样性的训练方案。在受控环境中进行的初步实验展示了我们方法的潜力。我们讨论了我们方法与多样性、公平和包容在生成式机器学习以及计算机创意中的联系。我们的算法实现可以在https://github.com/找到。

    Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
    
[^79]: GeneGPT: 教授大型语言模型使用NCBI Web API

    GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])

    [http://arxiv.org/abs/2304.09667](http://arxiv.org/abs/2304.09667)

    GeneGPT通过少量NCBI API调用URL请求作为演示，教授大型语言模型使用NCBI Web API回答基因组问题，并在GeneTuring测试中达到了优异的结果。

    

    本文介绍了GeneGPT，一种新颖的方法，用于教授大型语言模型（LLM）使用国家生物技术信息中心（NCBI）的Web应用程序编程接口（API），并回答基因组问题。具体而言，我们通过少量的NCBI API调用URL请求作为上下文学习的演示，启发Codex（code-davinci-002）解决GeneTuring测试。在推理过程中，一旦检测到调用请求，我们就停止解码并使用生成的URL进行API调用。我们然后将NCBI API返回的原始执行结果附加到生成的文本中，并继续生成直到找到答案或检测到另一个API调用。初步结果表明，GeneGPT在GeneTuring数据集的四个One-shot任务中取得了三个最先进的结果，在五个Zero-shot任务中取得了四个最先进的结果。总体而言，GeneGPT的宏平均分数为0.76，远高于检索增强LLM，如New Bin。

    In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
    
[^80]: SemiMemes：一种用于多模态Memes分析的半监督学习方法

    SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])

    [http://arxiv.org/abs/2304.00020](http://arxiv.org/abs/2304.00020)

    研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。

    

    社交媒体上Memes的普及性引发了分析其隐含含义、审查有害内容的需求。机器学习的Meme审查系统需要半监督学习解决方案，以利用互联网上大量未标记的Memes，并使注释过程变得更简单。此外，该方法需要利用多模态数据，因为Memes的含义通常来自图像和文本。该研究提出了一种多模态半监督学习方法，在两个数据集，即多媒体自动性别歧视识别和令人讨厌的Memes数据集上，优于其他多模态半监督和监督学习的最新模型。借鉴对比语言-图像预训练所获得的见解，这项研究引入了SemiMemes，一种新颖的训练方法，它结合了自编码器和分类任务

    The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
    
[^81]: 合成经验回放：旨在用扩充数据来提高深度强化学习的效果

    Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06614](http://arxiv.org/abs/2303.06614)

    本文提出了合成经验回放方法解决深度强化学习中数据匮乏问题，通过巧妙应用生成建模技术来扩充数据效果显著。

    

    过去十年的一个关键主题是，当大型神经网络和大型数据集相结合时，它们可以产生令人惊异的结果。在深度强化学习中，这种范式通常通过经验回放实现，其中过去的经验数据集用于训练策略或值函数。然而，与监督学习或自监督学习不同，强化学习代理必须收集自己的数据，这通常是有限的。因此，利用深度学习的好处是具有挑战性的，即使是小型神经网络在训练开始时也可能出现过拟合现象。在这项工作中，我们利用了生成建模的巨大进步，并提出了合成经验回放（SynthER），一种基于扩散的方法来灵活地上采样代理收集的经验。我们证明了SynthER是一种有效的方法，可以在离线和在线设置下训练强化学习代理，无论是在感知环境还是在像素环境中。在离线设置中，我们观察到了显着的改进。

    A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
    
[^82]: 通过梯度辅助和密集探索提高多目标质量多样性的数据效率

    Improving the Data Efficiency of Multi-Objective Quality-Diversity through Gradient Assistance and Crowding Exploration. (arXiv:2302.12668v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2302.12668](http://arxiv.org/abs/2302.12668)

    本文介绍了一种新的QD算法——带有策略梯度辅助和基于拥挤的探索的多目标MAP-Elites (MOME-PGX)来扩展MOME以提高其数据效率和性能。

    

    最近，由于其高效逃离局部最优和生成广泛高效解的能力，质量多样性(QD)算法已成为优化方法。最近，多目标MAP-Elites (MOME)通过在Map Elites网格的每个单元格中维护帕累托前沿将QD范例扩展到多目标设置。MOME在同时获得多样的解的同时实现了与NSGA-II和SPEA2相竞争的全局性能。但是，MOME受非定向遗传搜索机制的限制，这些机制在高维搜索空间中面临困难。在这项工作中，我们提出了一种新的QD算法——带有策略梯度辅助和基于拥挤的探索的多目标MAP-Elites (MOME-PGX)来扩展MOME以提高其数据效率和性能。MOME-PGX使用基于梯度的优化有效地将解决方案驱向更高的性能。

    Quality-Diversity (QD) algorithms have recently gained traction as optimisation methods due to their effectiveness at escaping local optima and capability of generating wide-ranging and high-performing solutions. Recently, Multi-Objective MAP-Elites (MOME) extended the QD paradigm to the multi-objective setting by maintaining a Pareto front in each cell of a map-elites grid. MOME achieved a global performance that competed with NSGA-II and SPEA2, two well-established Multi-Objective Evolutionary Algorithms (MOEA), while also acquiring a diverse repertoire of solutions. However, MOME is limited by non-directed genetic search mechanisms which struggle in high-dimensional search spaces. In this work, we present Multi-Objective MAP-Elites with Policy-Gradient Assistance and Crowding-based Exploration (MOME-PGX): a new QD algorithm that extends MOME to improve its data efficiency and performance. MOME-PGX uses gradient-based optimisation to efficiently drive solutions towards higher perform
    
[^83]: 基于深度学习的时间序列健康数据缺失值填补：回顾与基准测试

    Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking. (arXiv:2302.10902v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10902](http://arxiv.org/abs/2302.10902)

    本文回顾了基于深度学习的时间序列健康数据缺失值填补方法，并在五个时间序列健康数据集上进行基准测试，研究发现填补的效果依赖于数据类型、变量统计、缺失值率和类型。在时间序列数据中同时进行交叉剖面和纵向缺失值填补的深度学习方法表现最佳。

    

    在多元时间序列（MTS）数据中填补缺失值对于确保数据质量和生成可靠的数据驱动预测模型至关重要。除了许多统计方法外，近期一些研究提出了最先进的深度学习方法来填补MTS数据中的缺失值。然而，这些深度方法的评估仅局限于一个或两个数据集、较低的缺失率和完全随机的缺失值类型。本文对五个时间序列健康数据集进行了六个数据中心实验，对最先进的深度填补方法进行基准测试。我们广泛的分析表明，没有一种单一的填补方法在所有五个数据集上表现最好。填补效果取决于数据类型、单个变量统计、缺失值率和类型。在时间序列数据中同时进行交叉剖面（跨变量）和纵向（跨时间）缺失值填补的深度学习方法产生了统计学上的最佳性能。

    The imputation of missing values in multivariate time series (MTS) data is critical in ensuring data quality and producing reliable data-driven predictive models. Apart from many statistical approaches, a few recent studies have proposed state-of-the-art deep learning methods to impute missing values in MTS data. However, the evaluation of these deep methods is limited to one or two data sets, low missing rates, and completely random missing value types. This survey performs six data-centric experiments to benchmark state-of-the-art deep imputation methods on five time series health data sets. Our extensive analysis reveals that no single imputation method outperforms the others on all five data sets. The imputation performance depends on data types, individual variable statistics, missing value rates, and types. Deep learning methods that jointly perform cross-sectional (across variables) and longitudinal (across time) imputations of missing values in time series data yield statistica
    
[^84]: 用多智能体强化学习模拟社会困境中的道德选择

    Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2301.08491](http://arxiv.org/abs/2301.08491)

    本文使用多智能体强化学习模拟社会困境中的道德选择，设计了一套道德奖励结构，旨在分析和研究AI代理的道德行为。

    

    在实际应用中，人工智能（AI）在智能代理中纳入道德选择的重要性不断展现。同时也强调，按照任何一种道德观定义顶层的AI伦理约束非常具有挑战，并且会带来风险。从底层学习的角度出发，或许更适合研究和开发AI代理的道德行为。我们认为，分析根据预定义的道德奖励在社会困境中实行行动的强化学习代理的新兴行为是一个有趣和富有洞察力的起点。在这项工作中，我们对强化学习代理根据道德理论的奖励进行的选择进行了系统分析。我们旨在设计简化但代表一组关键伦理系统的奖励结构。因此，我们首先定义了区分后果和规范伦理的道德奖励函数，并将它们混合以创建新的奖励方案。然后，我们通过训练在社会困境下进行内在动机驱动的强化学习代理来评估这些奖励函数。结果表明，我们的方法能够复制并扩展有关道德选择的文献研究中的许多发现，并能够出现以前未曾报道的新行为。

    Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm
    
[^85]: 通过D适应实现学习率自由学习

    Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07733](http://arxiv.org/abs/2301.07733)

    D-Adaptation是一种可以自动设置学习率的方法，针对最小化凸性Lipschitz函数，用于实现最优收敛速率，而无需超参数，也无需额外对数因子改进，能够在各种机器学习问题中自动匹配手动调整的学习率。

    

    D适应是一种自动设置学习率的方法，可以渐近地实现最优收敛速率，用于最小化凸性Lipschitz函数，无需回溯或线性搜索，并且每步无需进行额外的函数值或梯度评估。我们的方法是这一类问题的第一个无超参数且收敛速率无需额外对数因子改进的方法。我们针对SGD和Adam变体展示了广泛的实验，其中该方法自动匹配手动调整的学习率，在十多个不同的机器学习问题中应用，包括大规模的视觉和语言问题。开源实现在 \url{https://github.com/facebookresearch/dadaptation}.

    D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
    
[^86]: tasksource：流畅的NLP多任务学习和评估的数据集协调框架

    tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation. (arXiv:2301.05948v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.05948](http://arxiv.org/abs/2301.05948)

    tasksource是一个数据集协调框架，为多任务学习和评估提供流畅的体验。该框架提供了结构化注释，使得数据处理更加便捷。

    

    HuggingFace数据集中心提供数千个数据集，为语言模型的训练和评估提供了令人兴奋的机会。然而，特定任务类型的数据集往往具有不同的架构，使得协调变得具有挑战性。多任务训练或评估需要手动将数据适配到任务模板中。为了解决这个问题，一些倡议采取独立的方法，发布协调的数据集或提供协调代码以将数据集预处理为一致格式。我们确定了以前的预处理工作中的模式，例如列名称映射和从列中提取结构化数据的特定子字段。然后，我们提出了一个结构化注释框架，确保我们的注释完全暴露而不隐藏在非结构化代码中。我们发布了一个数据集注释框架和500多个英语任务的数据集注释。

    The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting opportunities for language model training and evaluation. However, datasets for a specific task type often have different schemas, making harmonization challenging. Multi-task training or evaluation necessitates manual work to fit data into task templates. Several initiatives independently tackle this issue by releasing harmonized datasets or providing harmonization codes to preprocess datasets into a consistent format. We identify patterns across previous preprocessing efforts, such as column name mapping and extracting specific sub-fields from structured data in a column. We then propose a structured annotation framework that ensures our annotations are fully exposed and not hidden within unstructured code. We release a dataset annotation framework and dataset annotations for more than 500 English tasks\footnote{\url{https://github.com/sileod/tasksource}}. These annotations include metadata, such as the names
    
[^87]: 可切换轻量级反对称处理（SLAP）在 Gomoku 强化学习中使用 CNN 比数据增强更快

    Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning. (arXiv:2301.04746v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04746](http://arxiv.org/abs/2301.04746)

    本论文提出了一种名为SLAP的方法，它可以替代数据增强，加强经验以加速机器学习并减少样本大小。 在Gomoku游戏和强化学习领域的实验中，证明了SLAP的有效性，可以提高模型的收敛速度，同时减少了样本数量。这种策略至少适用于对称或特定变换不变的领域。

    

    本文提出了一种名为 SLAP 的方法，用于加强经验以加速机器学习并减少样本大小，以代替数据增强。SLAP是一种模型无关的协议/函数，可以产生相同的输出，但给予不同的变换变量。在Gomoku游戏状态的实验中，SLAP提高了卷积神经网络学习的收敛速度达83％，样本大小只有数据增强的1/8。在Gomoku强化学习中，使用AlphaGo Zero / AlphaZero算法和数据增强作为基线，SLAP将训练样本数量减少了8倍，并在与相同评估器对比时实现了类似的获胜率，但尚不能证明它可以加速强化学习。这些益处至少应适用于对称或特定变换不变的领域。作为未来的工作，SLAP可能有助于更可解释的学习以及不适用于对称的领域的转移学习。

    To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to sym
    
[^88]: TAToo：面向颅底手术的解剖和工具联合跟踪的基于视觉的方法

    TAToo: Vision-based Joint Tracking of Anatomy and Tool for Skull-base Surgery. (arXiv:2212.14131v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.14131](http://arxiv.org/abs/2212.14131)

    TAToo是一个基于视觉的联合跟踪方法，可用于恢复颅底手术中手术工具和患者解剖结构的三维运动。该方法具有高精度和鲁棒性，并可与现有的临床工作流程和仪器兼容。

    

    在计算机辅助的颅底手术中，跟踪手术工具和患者解剖结构的三维运动是必要的。TAToo是一个基于视觉的跟踪方法，它可以从手术视频中恢复运动信息，并采用概率建模和几何约束，以提高跟踪的鲁棒性能。

    Purpose: Tracking the 3D motion of the surgical tool and the patient anatomy is a fundamental requirement for computer-assisted skull-base surgery. The estimated motion can be used both for intra-operative guidance and for downstream skill analysis. Recovering such motion solely from surgical videos is desirable, as it is compliant with current clinical workflows and instrumentation.  Methods: We present Tracker of Anatomy and Tool (TAToo). TAToo jointly tracks the rigid 3D motion of patient skull and surgical drill from stereo microscopic videos. TAToo estimates motion via an iterative optimization process in an end-to-end differentiable form. For robust tracking performance, TAToo adopts a probabilistic formulation and enforces geometric constraints on the object level.  Results: We validate TAToo on both simulation data, where ground truth motion is available, as well as on anthropomorphic phantom data, where optical tracking provides a strong baseline. We report sub-millimeter and 
    
[^89]: 在现实世界中实现智能决策：基础决策模型的视角

    On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective. (arXiv:2212.12669v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.12669](http://arxiv.org/abs/2212.12669)

    本文提出了基础决策模型（FDM），通过使用转换器神经架构将各种决策任务制定为序列解码任务，为在复杂现实世界环境中实现机器驱动智能决策(IDM)提供了一个前景广阔的解决方案。

    

    现实世界环境的普遍不确定性和动态特性给机器驱动智能决策(IDM)系统的广泛实施带来了重大挑战。因此，IDM应具备持续获取新技能并在广泛应用中有效推广的能力。超越任务和应用边界的人工通用智能(AGI)的进步对于增强IDM至关重要。最近的研究广泛调查了转换器神经架构作为各种任务的基础模型，包括计算机视觉，自然语言处理和强化学习。我们提出一个基础决策模型(FDM)，通过使用转换器体系结构将各种决策任务制定为序列解码任务，为扩展复杂实际情况下的IDM应用提供了一个有前途的解决方案。在本文中，我们讨论了基于转换器神经架构开发基础决策模型(FDM)的效率和可行性，用于机器驱动智能决策(IDM)在复杂实际情况中的应用。FDM可以允许持续的技能获取并在各个应用之间进行概括，具有增进人工通用智能(AGI)的潜力。

    The pervasive uncertainty and dynamic nature of real-world environments present significant challenges for the widespread implementation of machine-driven Intelligent Decision-Making (IDM) systems. Consequently, IDM should possess the ability to continuously acquire new skills and effectively generalize across a broad range of applications. The advancement of Artificial General Intelligence (AGI) that transcends task and application boundaries is critical for enhancing IDM. Recent studies have extensively investigated the Transformer neural architecture as a foundational model for various tasks, including computer vision, natural language processing, and reinforcement learning. We propose that a Foundation Decision Model (FDM) can be developed by formulating diverse decision-making tasks as sequence decoding tasks using the Transformer architecture, offering a promising solution for expanding IDM applications in complex real-world situations. In this paper, we discuss the efficiency an
    
[^90]: 使用网页抓取的多模态数据预训练的对比语言-视觉AI模型存在性物化偏见

    Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2212.11261](http://arxiv.org/abs/2212.11261)

    本文使用对比语言图像预训练的多模态AI模型，使用网页抓取的数据训练，发现这些模型存在性物化偏见，即人的情感状态与身体的呈现相关，表现出对女性的性别偏见。

    

    本文评估了使用对比语言图像预训练（CLIP）目标进行网页抓取的多模态数据的九种语言-视觉AI模型，以寻找心理学家研究的偏见的证据：女孩和女性的性物化现象。我们复制了三个心理实验，并显示这种偏见在AI中依然存在。第一个实验使用Sexual OBjectification and EMotion Database中的标准女性图像，并发现情感状态的识别是由主体是否全身或部分穿着进行介导的。嵌入关联测试返回了愤怒(d>0.80)和悲伤(d>0.50)的显着效应大小，将完全穿着的主体的图像与情感相关联。GRAD-CAM显著性图突出显示，CLIP生成的模型存在性物化偏见，即使使用最先进的对比多模态预训练和网页抓取的数据也是如此。

    Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d >0.80) and sadness (d >0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP ge
    
[^91]: 一种基于强化学习的模因算法用于社会技术生产调度

    A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10936](http://arxiv.org/abs/2212.10936)

    本文提出了一种基于深度强化学习的模因算法，用于解决具有实际约束的灵活生产调度问题，并弥补元启发式研究中的缺陷。

    

    本文提出了一个应用深度强化学习的模因算法，用于解决实际双资源约束柔性作业车间调度问题（DRC-FJSSP）。近年来，对于DRL技术已经进行了广泛的研究，但是没有考虑到现实、灵活和以人为中心的车间。本文发现，在以订单为导向的间歇性制造中存在一个研究空白，它经常在具有高服务水平的中小型公司中表示。从这一领域的实际工业项目中，我们认识到需要描述灵活的机器、人工工作者和能力、设置和处理操作、物料到达时间、具有并行任务的复杂作业路径以进行物料清单（BOM）制造、顺序相关设置时间和（部分）自动化任务。另一方面，在DRC-FJSSP的背景下，已经进行了大量的元启发式研究。然而，缺乏适当的方法来解决生产过程的复杂性和不确定性，这是现实工业世界中面临的主要挑战。因此，提出了一种新的算法，以弥补相关领域的缺陷。

    The following article presents a memetic algorithm with applying deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years, there has been extensive research on DRL techniques, but without considering realistic, flexible and human-centered shopfloors. A research gap can be identified in the context of make-to-order oriented discontinuous manufacturing as it is often represented in medium-size companies with high service levels. From practical industry projects in this domain, we recognize requirements to depict flexible machines, human workers and capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-depended setup times and (partially) automated tasks. On the other hand, intensive research has been done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of sui
    
[^92]: DimonGen：多元化生成通识推理以解释概念关系。

    DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships. (arXiv:2212.10545v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10545](http://arxiv.org/abs/2212.10545)

    本文提出了DimonGen模型，通过生成各种日常场景的概念关系描述来实现多元化通识推理。实验结果表明，MoREE模型在生成质量和多样性方面均优于基线模型。

    

    本文提出了DimonGen，旨在生成描述各种日常场景中概念关系的多样化句子。为此，我们首先通过改编现有的CommonGen数据集创建了一个基准数据集。然后，我们提出了一个名为MoREE的两阶段模型来生成目标句子。MoREE包含一种混合检索模型，该模型检索与给定概念相关的多样性上下文句子，以及一种混合生成器模型，该模型基于检索到的上下文生成多样化的句子。我们在DimonGen任务上进行了实验，并展示了MoREE在生成的句子的质量和多样性方面优于强基线模型。我们的结果表明，MoREE能够生成反映概念之间不同关系的多样化句子，从而实现对概念关系的全面理解。

    In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships.
    
[^93]: 自我提示的大型语言模型用于零样本开放域问答

    Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. (arXiv:2212.08635v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08635](http://arxiv.org/abs/2212.08635)

    本论文提出了一种自我提示框架，可以有效利用大型语言模型的参数中存储的知识和指令理解能力，以实现零样本开放域问答，并且实验证明该方法在三个广泛使用的ODQA数据集中显著优于现有的最先进方法。

    

    开放域问答目标在于回答关于事实的问题，而无需提供特定的背景文档。在零样本设置下，由于没有数据来训练类似检索器-阅读器的定制模型，因此此任务更加具有挑战性。最近，像GPT-3这样的大型语言模型已经通过直接提示方法在零样本开放域问答中展示了其强大的能力，但是这些方法仍然远远不能充分发挥LLM的强大功能，而只是以隐式方式调用它们而已。本文提出了一个自我提示框架，以明确利用LLM参数中存储的大量知识和其强大的指令理解能力。具体而言，我们逐步提示LLM生成多个伪QA对，并从头开始生成背景段落和解释，然后使用生成的元素进行上下文学习。实验结果表明，我们的方案在三个广泛使用的ODQA数据集上显著超过了先前的SOTA方法。

    Open-Domain Question Answering (ODQA) aims at answering factoid questions without explicitly providing specific background documents. In a zero-shot setting, this task is more challenging since no data is available to train customized models like Retriever-Readers. Recently, Large Language Models (LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct prompting methods, but these methods are still far from releasing the full powerfulness of LLMs only in an implicitly invoking way. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge stored in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations from scratch and then use those generated elements for in-context learning. Experimental results show our method surpasses previous SOTA methods significantly on three widely-used ODQA datasets, a
    
[^94]: 基于任务相似度元学习加速多目标非分层超参数最优化的树形结构Parzen估计

    Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06751](http://arxiv.org/abs/2212.06751)

    本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。

    

    超参数优化是提高深度学习性能的关键步骤。实践者通常面临多个方面的权衡，如准确性和延迟时间。在深度学习的高计算需求和对高效超参数优化的不断增长需求下，加速多目标优化变得越来越重要。本文将TPE的收购函数扩展到元学习设置中，使用由任务之间顶级域之间的重叠度定义的任务相似性。我们也从理论上分析并解决了任务相似性的局限性。在实验中，我们展示了我们的方法在表格HPO基准上加速了MO-TPE，并获得了最先进的性能。我们的方法还通过赢得AutoML 2022来得到外部验证。

    Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
    
[^95]: AIONER: 基于深度学习的全套方案生物医学命名实体识别

    AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.16944](http://arxiv.org/abs/2211.16944)

    AIONER是一种新颖的基于深度学习的全套方案生物医学命名实体识别工具，利用外部数据提高BioNER模型准确性和稳定性，具有良好的性能和泛化能力。

    

    生物医学命名实体识别（BioNER）旨在自动识别自然语言文本中的生物医学实体，为信息提取和问答等下游文本挖掘任务和应用提供必要的基础。然而，由于准确注释所需的领域专业知识昂贵，因此手动标记训练数据对BioNER任务来说是昂贵的。导致的数据稀缺问题导致当前的BioNER方法容易出现过度拟合，具有有限的泛化能力，并且一次只能解决一种实体类型（例如，基因或疾病）。因此，我们提出了一种新颖的全套方案（AIO）方案，利用现有的注释资源外部数据来提高BioNER模型的准确性和稳定性。我们进一步提出了基于尖端深度学习和我们的AIO方案的通用BioNER工具AIONER。我们在14个BioNER基准任务上评估了AIONER，并展示了AIONER的有效性，稳健性，与其他方法相比优异。

    Biomedical named entity recognition (BioNER) seeks to automatically recognize biomedical entities in natural language text, serving as a necessary foundation for downstream text mining tasks and applications such as information extraction and question answering. Manually labeling training data for the BioNER task is costly, however, due to the significant domain expertise required for accurate annotation. The resulting data scarcity causes current BioNER approaches to be prone to overfitting, to suffer from limited generalizability, and to address a single entity type at a time (e.g., gene or disease). We therefore propose a novel all-in-one (AIO) scheme that uses external data from existing annotated resources to enhance the accuracy and stability of BioNER models. We further present AIONER, a general-purpose BioNER tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and compares favora
    
[^96]: 跨语言转移的令人沮丧的简易标签投影

    Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15613](http://arxiv.org/abs/2211.15613)

    本文通过一项广泛的实证研究，对57种语言和三个任务下的跨语言转移进行了研究，并发现优化后的标记-翻译法比传统注释投影方法更有效。

    

    将训练数据翻译成多种语言已成为提高跨语言转移的实际解决方案。对于涉及跨度级别注释（例如信息提取或问题回答）的任务，需要进行额外的标签投影步骤，将已注释的跨度映射到翻译后的文本中。然而，据我们所知，迄今为止尚未对这种方法与基于单词对齐的传统注释投影进行实证分析。在本文中，我们展示了一项对57种语言和三个任务（QA，NER和事件提取）进行广泛的实证研究，以评估两种方法的有效性和局限性，并填补文献中的重要空白。实验结果表明，我们优化后的标记-翻译法比传统注释投影方法更有效。

    Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
    
[^97]: c-TPE:基于树形结构的带不等式约束的帕捷斯特估计器用于昂贵超参数优化

    c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14411](http://arxiv.org/abs/2211.14411)

    本文提出了约束TPE（c-TPE）方法，是树形Parzen估计器（TPE）的扩展，可有效处理在性能要求之上施加的约束限制，实验证明在81个昂贵的HPO设置中表现出最佳性能排名。

    

    超参数优化（HPO）对于深度学习算法的强大性能至关重要，实际应用通常会在性能要求之上施加一些限制，例如内存使用或延迟等。在本文中，我们提出了约束TPE（c-TPE），这是广泛使用的多功能贝叶斯优化方法——树形Parzen估计器（TPE）的扩展，以处理这些约束。我们提出的扩展不仅是简单地将现有收益函数和原始TPE组合起来，而是包括修改来解决导致性能不佳的问题。我们从经验和理论上深入分析这些修改，提供了有关它们如何有效地克服这些挑战的见解。在实验中，我们证明了c-TPE在81个昂贵的HPO设置中表现出最佳的平均排名性能，具有统计显着性。

    Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
    
[^98]: 面向药物反应预测的混合量子神经网络

    Hybrid quantum neural network for drug response prediction. (arXiv:2211.05777v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.05777](http://arxiv.org/abs/2211.05777)

    该论文提出了一种基于混合量子神经网络的药物反应预测模型，能够有效减少需要的训练数据，并有望提高治疗方案的效率。

    

    癌症是全球死亡率较高的疾病之一。由于每个病例的基因突变各不相同，化疗的副作用非常严重，因此每个患者都需要定制化的治疗方案。深度神经网络能够自动化并提高药物选择的效率，但其需要大量的训练数据。因此，需要机器学习方法来减少数据需求。混合量子神经网络已被证明在训练数据有限的问题上具有潜在优势。我们提出了一种基于卷积、图卷积和8量子比特和363层深度量子神经元的组合的混合量子神经网络模型用于药物反应预测。我们在减少过的癌症药物敏感性基因组数据集上测试了我们的模型。

    Cancer is one of the leading causes of death worldwide. It is caused by a variety of genetic mutations, which makes every instance of the disease unique. Since chemotherapy can have extremely severe side effects, each patient requires a personalized treatment plan. Finding the dosages that maximize the beneficial effects of the drugs and minimize their adverse side effects is vital. Deep neural networks automate and improve drug selection. However, they require a lot of data to be trained on. Therefore, there is a need for machine-learning approaches that require less data. Hybrid quantum neural networks were shown to provide a potential advantage in problems where training data availability is limited. We propose a novel hybrid quantum neural network for drug response prediction, based on a combination of convolutional, graph convolutional, and deep quantum neural layers of 8 qubits with 363 layers. We test our model on the reduced Genomics of Drug Sensitivity in Cancer dataset and sh
    
[^99]: 强化学习的经验解释

    Experiential Explanations for Reinforcement Learning. (arXiv:2210.04723v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.04723](http://arxiv.org/abs/2210.04723)

    该论文提出了一种经验解释技术，通过训练影响预测器来恢复强化学习系统中的信息，使得非AI专家能够更好地理解其决策过程。

    

    强化学习系统可能非常复杂和无法解释，这使得非人工智能专家难以理解或干预它们的决策。我们提出了一种经验解释技术，通过在强化学习策略旁边训练影响预测器来生成反事实解释。影响预测器是学习奖励来源如何影响代理在不同状态下的模型，从而恢复有关策略如何反映环境的信息。

    Reinforcement Learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due, in part, to the sequential nature of RL in which actions are chosen because of future rewards. However, RL agents discard the qualitative features of their training, making it hard to recover user-understandable information for "why" an action is chosen. Proposed sentence chunking: We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors alongside the RL policy. Influence predictors are models that learn how sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. A human evaluation study revealed that participants presented with experiential explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanations. Pa
    
[^100]: 预训练语言模型为什么更适合零样本学习？

    What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.15206](http://arxiv.org/abs/2209.15206)

    本文提出一个理论框架来解释prompt learning在零样本/少样本场景下的有效性，并基于此提出了一个注释无关的模板选择方法。

    

    本文提出了一个理论框架来解释prompt learning在零样本/少样本场景下的有效性。首先，我们证明传统的预训练和微调范式在少样本场景下会因为过拟合不具代表性的标注数据而失败。然后，我们详细阐述了prompt learning更有效的假设，因为它使建立在海量文本语料库和领域相关人类知识的预训练语言模型可以更多地参与预测，从而减少小型训练集提供的有限标签信息的影响。我们进一步假设，语言差异可以衡量提示质量。我们进行了全面的实验证明了我们的假设。更为重要的是，受到理论框架的启发，我们提出了一种基于困惑度的注释无关的模板选择方法，可以事先“预测”提示性能。这种方法特别值得鼓励。

    In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou
    
[^101]: 参数修剪的数据集蒸馏方法

    Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14609](http://arxiv.org/abs/2209.14609)

    本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。

    

    在许多领域中，获得先进模型的方法取决于大型数据集，这使得数据存储和模型训练变得昂贵。作为解决方案，数据集蒸馏可以合成保留原始大型数据集大多数信息的小型数据集。最近提出的匹配网络参数的数据集蒸馏方法已被证明在几个数据集上有效。然而，网络参数的维度通常很大。此外，一些参数在蒸馏过程中难以匹配，降低了蒸馏性能。基于这个观察，本研究提出了一种基于参数修剪的新型数据集蒸馏方法来解决这个问题。该方法可以在蒸馏过程中修剪难以匹配的参数，从而合成更加稳健的蒸馏数据集并提高蒸馏性能。在三个数据集上的实验结果表明，该方法优于其他最先进的数据集蒸馏方法。

    In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
    
[^102]: 法律引导代码：一种法律信息学方法来使人工智能与人类保持一致

    Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2209.13020](http://arxiv.org/abs/2209.13020)

    这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。

    

    目前我们无法可靠地指定人类的目标和社会价值，以引导人工智能的行为。制定法律和解释法律构成了一种计算引擎，将不透明的人类价值转化为易读的指令。 “法律指导代码”是嵌入了法律知识和推理的人工智能研究议程。类似于合同当事人无法预见他们未来关系的每个潜在变数，立法者无法预测其提出的法案将适用的所有情况，我们无法提前明确规则，以可靠地引导良好的人工智能行为。法律理论和实践已经开发出各种工具来解决这些规定问题。与法律更为普通的用途（例如通过制裁威胁来阻止不良行为）相反，法律作为一种表达人类沟通目标和价值的表现，可以引导人工智能代码的发展方向。具体而言，法律信息学是计算规则和系统用于表示，分析和操作法律知识的跨学科研究。法律指导代码利用法律信息学来使人工智能与人类的目标和社会价值保持一致，以一种透明，负责，灵活的方式。

    We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
    
[^103]: 快速FNet：通过高效的傅里叶层加速Transformer编码器模型

    Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers. (arXiv:2209.12816v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.12816](http://arxiv.org/abs/2209.12816)

    FNet模型通过替换注意力层为傅里叶变换，加速了Transformer编码器模型的训练过程并保持相同的性能水平。

    

    基于Transformer的语言模型在几乎所有自然语言处理任务中利用注意力机制实现了显著的性能提升。相似的注意力结构也在其他领域广泛研究。虽然注意力机制显著增强了模型性能，但其二次复杂度阻碍了对长序列的高效处理。最近的研究集中在消除计算效率的缺点上，并表明在无需注意力层的情况下，基于Transformer的模型仍然能够达到竞争性的结果。一项开创性的研究提出了FNet，在Transformer编码器结构中用傅里叶变换（FT）替换注意力层。FNet在加速训练过程中去除了注意力机制的运算负担，同时实现了与原始Transformer编码器模型相当的性能。然而，FNet模型忽略了FT的基本属性..

    Transformer-based language models utilize the attention mechanism for substantial performance improvements in almost all natural language processing (NLP) tasks. Similar attention structures are also extensively studied in several other areas. Although the attention mechanism enhances the model performances significantly, its quadratic complexity prevents efficient processing of long sequences. Recent works focused on eliminating the disadvantages of computational inefficiency and showed that transformer-based models can still reach competitive results without the attention layer. A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture. FNet achieves competitive performances concerning the original transformer encoder model while accelerating training process by removing the computational burden of the attention mechanism. However, the FNet model ignores essential properties of the FT from the clas
    
[^104]: WeLM: 一种面向中文的读过书的预训练语言模型

    WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.10372](http://arxiv.org/abs/2209.10372)

    WeLM 是一种面向中文的读过书的预训练语言模型，它通过读取高质量的语料库训练了100亿个参数，并可以在18个中文任务中显著优于现有同规模预训练模型，同时表现出强大的多语言和代码转换理解能力。

    

    以自我监督学习为基础的大型语言模型在广泛的任务中显示了出色的零样本泛化能力。在本文中，我们介绍了WeLM：一种能够零或少样本演示无缝执行不同类型任务的面向中文的读过书的预训练语言模型。WeLM通过阅读精心策划的高质量语料库中的信息，以100亿个参数进行训练。我们展示了WeLM在各个领域和语言方面具备广泛的知识。在18项独立的（中文）任务中，WeLM能够显著优于现有规模相似且预训练模型，并且能够匹配规模高达25倍的模型的性能。WeLM还表现出强大的多语言和代码转换理解能力，优于预先在30种语言上进行预训练的现有多语言语言模型。此外，我们为大量中文监督式数据集收集了人工编写的提示，并对WeLM进行了微调。

    Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by "reading" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM
    
[^105]: 多种模糊规则系统的最新趋势和应用的文献综述

    Literature Review of the Recent Trends and Applications in various Fuzzy Rule based systems. (arXiv:2209.07175v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.07175](http://arxiv.org/abs/2209.07175)

    本文综述了模糊规则系统的最新趋势和应用，包括GFS、HFS、NFS、eFS、用于大数据和不平衡数据的FRBS以及可解释性和使用聚类中心作为模糊规则的FRBS。重点突出其重要贡献和出版统计数据。

    

    模糊规则系统（FRBS）是一种基于规则的系统，它使用语言模糊变量作为前提和结论来表示人可理解的知识。它们已经应用于软计算文献中的各个应用和领域。然而，FRBS面临许多缺点，例如不确定性表示，规则数量过多，解释能力丧失，学习所需的计算时间长等。为了克服FRBS的这些问题，存在许多FRBS的扩展。本文概述和文献综述了模糊系统（FRBS）的各种类型和突出领域的最新趋势，包括遗传模糊系统（GFS）、层级模糊系统（HFS）、神经模糊系统（NFS）、进化模糊系统（eFS）、用于大数据的FRBS、用于不平衡数据的FRBS、FRBS中的可解释性以及将聚类中心用作模糊规则的FRBS。综述的时间范围为2010年至2021年。本文还强调了重要的贡献、出版统计数据等。

    Fuzzy rule based systems (FRBSs) is a rule-based system which uses linguistic fuzzy variables as antecedents and consequent to represent human understandable knowledge. They have been applied to various applications and areas throughout the soft computing literature. However, FRBSs suffers from many drawbacks such as uncertainty representation, high number of rules, interpretability loss, high computational time for learning etc. To overcome these issues with FRBSs, there exists many extensions of FRBSs. This paper presents an overview and literature review of recent trends on various types and prominent areas of fuzzy systems (FRBSs) namely genetic fuzzy system (GFS), hierarchical fuzzy system (HFS), neuro fuzzy system (NFS), evolving fuzzy system (eFS), FRBSs for big data, FRBSs for imbalanced data, interpretability in FRBSs and FRBSs which use cluster centroids as fuzzy rules. The review is for years 2010-2021. This paper also highlights important contributions, publication statisti
    
[^106]: 利用模拟辅助优化大规模疏散计划中的拥堵依赖延迟问题

    Simulation-Assisted Optimization for Large-Scale Evacuation Planning with Congestion-Dependent Delays. (arXiv:2209.01535v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.01535](http://arxiv.org/abs/2209.01535)

    本研究提出了一种通过模拟辅助优化的方法，能够在考虑拥堵依赖延迟的情况下，提高大规模疏散计划的性能表现，相比现有方法表现更优。

    

    疏散计划是灾难管理的重要组成部分。然而，将路线和调度两个关键组成部分进行联合优化，以达到最小化平均疏散时间或疏散完成时间等目标，是一个计算难题。为了解决这个问题，我们提出了MIP-LNS，这是一种可扩展的优化方法，利用启发式搜索和数学优化来优化各种目标函数。我们还提出了MIP-LNS-SIM方法，结合基于代理的模拟以估计由于拥堵而引起的延迟，并找到考虑这些延迟的优化计划。我们以得克萨斯州休斯顿市的哈里斯县作为研究区域。我们发现，在给定的时间限制下，MIP-LNS相对于现有方法在三个不同指标方面找到更好的解。然而，当考虑到拥堵依赖的延迟时，MIP-LNS-SIM在多个性能指标上优于MIP-LNS。此外，MIP-LNS-SIM具有明显较低的求解时间。

    Evacuation planning is a crucial part of disaster management. However, joint optimization of its two essential components, routing and scheduling, with objectives such as minimizing average evacuation time or evacuation completion time, is a computationally hard problem. To approach it, we present MIP-LNS, a scalable optimization method that utilizes heuristic search with mathematical optimization and can optimize a variety of objective functions. We also present the method MIP-LNS-SIM, where we combine agent-based simulation with MIP-LNS to estimate delays due to congestion, as well as, find optimized plans considering such delays. We use Harris County in Houston, Texas, as our study area. We show that, within a given time limit, MIP-LNS finds better solutions than existing methods in terms of three different metrics. However, when congestion dependent delay is considered, MIP-LNS-SIM outperforms MIP-LNS in multiple performance metrics. In addition, MIP-LNS-SIM has a significantly low
    
[^107]: BERT4Loc：基于BERT的位置推荐系统--POI推荐

    BERT4Loc: BERT for Location -- POI Recommender System. (arXiv:2208.01375v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2208.01375](http://arxiv.org/abs/2208.01375)

    BERT4Loc是一种基于BERT的位置推荐系统，将地理位置信息和用户偏好相结合，提供个性化的基于位置的建议，相较于预测序列中下一个POI的模型，提供更相关的推荐，在基准数据集上实现了较好的性能。

    

    推荐兴趣点（POI）是一项具有挑战性的任务，需要从基于位置的社交媒体平台中提取全面的位置数据。为了提供有效的基于位置的推荐，分析用户的历史行为和偏好是非常重要的。本研究提出了一种复杂的位置感知推荐系统，利用Transformers中的双向编码器表示（BERT）提供个性化的基于位置的建议。我们的模型将地理位置信息和用户偏好相结合，相比于预测序列中下一个POI的模型，提供更相关的推荐。在两个基准数据集上的实验表明，我们基于BERT的模型优于各种最先进的序列模型。此外，我们通过附加实验展示了所提出模型的有效性。

    Recommending points of interest (POIs) is a challenging task that requires extracting comprehensive location data from location-based social media platforms. To provide effective location-based recommendations, it's important to analyze users' historical behavior and preferences. In this study, we present a sophisticated location-aware recommendation system that uses Bidirectional Encoder Representations from Transformers (BERT) to offer personalized location-based suggestions. Our model combines location information and user preferences to provide more relevant recommendations compared to models that predict the next POI in a sequence. Our experiments on two benchmark dataset show that our BERT-based model outperforms various state-of-the-art sequential models. Moreover, we see the effectiveness of the proposed model for quality through additional experiments.
    
[^108]: 挑战关于灾难性遗忘的常见假设

    Challenging Common Assumptions about Catastrophic Forgetting. (arXiv:2207.04543v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04543](http://arxiv.org/abs/2207.04543)

    本文挑战了DNNs遭受灾难性遗忘的常见假设，提出当前优化技术可以缓解这种现象。我们的实验表明，使用标准梯度下降优化方法训练的DNNs在回归任务上可以积累知识，即使任务重新出现，也不会遗忘过去的知识，这种知识积累可以改善在未见任务上的泛化性能。

    

    建立能够逐步学习和积累知识的学习智能体是连续学习(CL)研究领域的核心目标。然而，将模型训练在新数据上通常会损害过去数据的性能。在CL文献中，这种效应被称为灾难性遗忘(CF)。尽管CF已被广泛研究，且已提出了大量方法来解决非重叠任务短序列的CF问题，但在这种设置下，CF总是导致过去任务的性能迅速而显著地下降。然而，最近的工作表明，在CL回归设置下，SGD训练线性模型可以累积知识。当任务重新发生时，这种现象尤为明显。我们可能会想知道，是否使用SGD或任何标准梯度下降优化方法训练的DNNs会以这种方式积累知识。这些现象会对将DNNs应用于真实的连续情境产生有趣的影响。实际上，标准的梯度下降优化技术被广泛使用，并为大规模问题提供高效的训练策略。在这项工作中，我们挑战常见的DNNs遭受CF的假设，并提出当前的优化技术可以缓解这种现象。我们展示了在回归任务上使用SGD训练的DNNs可以积累知识，即使任务重新出现，也不会遗忘过去的知识。我们在实际数据集上的实验表明，这种知识积累可以改善在未见任务上的泛化性能。

    Building learning agents that can progressively learn and accumulate knowledge is the core goal of the continual learning (CL) research field. Unfortunately, training a model on new data usually compromises the performance on past data. In the CL literature, this effect is referred to as catastrophic forgetting (CF). CF has been largely studied, and a plethora of methods have been proposed to address it on short sequences of non-overlapping tasks. In such setups, CF always leads to a quick and significant drop in performance in past tasks. Nevertheless, despite CF, recent work showed that SGD training on linear models accumulates knowledge in a CL regression setup. This phenomenon becomes especially visible when tasks reoccur. We might then wonder if DNNs trained with SGD or any standard gradient-based optimization accumulate knowledge in such a way. Such phenomena would have interesting consequences for applying DNNs to real continual scenarios. Indeed, standard gradient-based optimiz
    
[^109]: 通过基于边的Weisfeiler-Lehman算法增强GNN

    Empowering GNNs via Edge-Aware Weisfeiler-Lehman Algorithm. (arXiv:2206.02059v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02059](http://arxiv.org/abs/2206.02059)

    本文提出了一种基于边缘感知的Weisfeiler-Lehman算法，以增强图神经网络的表达能力，同时保持消息传递方案的可扩展性。实验表明，我们NC-GNN框架在各种基准测试中表现出有效性和高效性。

    

    消息传递图神经网络(GNN)的表达能力被已知的一维Weisfeiler-Lehman (1-WL)算法上界所限制。为了实现更强大的GNN，现有的尝试要么需要特定的特征，要么涉及高时间和空间复杂度的操作。在本文中，我们提出了一个通用且可证明具有强大表达力的GNN框架，保持了消息传递方案的可扩展性。具体而言，我们首先通过考虑邻居之间的边缘来授权1-WL进行图同构测试，从而产生NC-1-WL。 NC-1-WL的表达能力在理论上被显示为严格高于1-WL且低于3-WL。进一步，我们提出了NC-GNN框架作为NC-1-WL的可区分神经版本。我们的简单NC-GNN实现可证明与NC-1-WL一样强大。实验表明，我们的NC-GNN在各种基准测试中表现出有效性和高效性。

    Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks.
    
[^110]: 一种基于ViT-Patch GAN的染色体修直鲁棒性框架

    A Robust Framework of Chromosome Straightening with ViT-Patch GAN. (arXiv:2203.02901v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.02901](http://arxiv.org/abs/2203.02901)

    本文提出了一种新的ViT-Patch GAN架构，其包括一个自学习的运动变换生成器和基于Vision Transformer的patch（ViT-Patch）鉴别器。该生成器学习染色体的运动表示，以实现鲁棒性染色体修直，并且在保留形状和斑带图案细节方面具有更好的性能。

    

    染色体承载着人类的遗传信息。它们具有不规则的非刚性和非关节性，并且存在不同程度的弯曲。染色体修直是后续核型构建、病理诊断和细胞遗传图谱发展的重要步骤。然而，由于没有训练图像、修直后畸变的染色体细节和形状以及泛化能力差，鲁棒性染色体修直仍然具有挑战性。本文提出了一种新的架构ViT-Patch GAN，包括一个自学习的运动变换生成器和一个基于Vision Transformer的patch（ViT-Patch）鉴别器。生成器学习染色体的运动表示以进行修直。在ViT-Patch鉴别器的帮助下，修直的染色体保留更多的形状和斑带图案细节。实验结果表明，所提出的方法在Fr\'echet Inception距离（FI

    Chromosomes carry the genetic information of humans. They exhibit non-rigid and non-articulated nature with varying degrees of curvature. Chromosome straightening is an important step for subsequent karyotype construction, pathological diagnosis and cytogenetic map development. However, robust chromosome straightening remains challenging, due to the unavailability of training images, distorted chromosome details and shapes after straightening, as well as poor generalization capability. In this paper, we propose a novel architecture, ViT-Patch GAN, consisting of a self-learned motion transformation generator and a Vision Transformer-based patch (ViT-Patch) discriminator. The generator learns the motion representation of chromosomes for straightening. With the help of the ViT-Patch discriminator, the straightened chromosomes retain more shape and banding pattern details. The experimental results show that the proposed method achieves better performance on Fr\'echet Inception Distance (FI
    
[^111]: S-ConvNet: 一种深度表面肌电图像神经肌肉活动识别的浅层卷积神经网络架构

    S-ConvNet: A Shallow Convolutional Neural Network Architecture for Neuromuscular Activity Recognition Using Instantaneous High-Density Surface EMG Images. (arXiv:1906.03381v1 [eess.SP] CROSS LISTED)

    [http://arxiv.org/abs/1906.03381](http://arxiv.org/abs/1906.03381)

    S-ConvNet和All-ConvNet模型为神经肌肉活动识别提出了一种简单而有效的学习框架，不需要预训练，表现出非常具有竞争力的识别准确性。

    

    利用瞬时高密度表面肌电图像进行神经肌肉活动识别的概念为开发更流畅和自然的肌肉-计算机接口开辟了新的途径。然而，现有的方法采用了非常大的深度卷积神经网络(ConvNet)架构和复杂的训练方案来进行HD-sEMG图像识别，需要在非常大规模的标记训练数据集上预训练网络架构，因此计算非常昂贵。为了解决这个问题，本文提出了一种简单而有效的学习即时HD-sEMG图像的神经肌肉活动识别框架——S-ConvNet和All-ConvNet模型。在不使用任何预训练模型的情况下，我们提出的S-ConvNet和All-ConvNet表现出了非常具有竞争力的识别准确性，比基于瞬时HD-sEMG图像的神经肌肉活动识别的更复杂的最先进技术更具优势。

    The concept of neuromuscular activity recognition using instantaneous high-density surface electromyography (HD-sEMG) images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the existing approaches employed a very large deep convolutional neural network (ConvNet) architecture and complex training schemes for HD-sEMG image recognition, which requires the network architecture to be pre-trained on a very large-scale labeled training dataset, as a result, it makes computationally very expensive. To overcome this problem, we propose S-ConvNet and All-ConvNet models, a simple yet efficient framework for learning instantaneous HD-sEMG images from scratch for neuromuscular activity recognition. Without using any pre-trained models, our proposed S-ConvNet and All-ConvNet demonstrate very competitive recognition accuracy to the more complex state of the art for neuromuscular activity recognition based on instantaneous HD-sEMG images, while u
    

