# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism.](http://arxiv.org/abs/2401.02954) | DeepSeek LLM是一个致力于以长期视野推进开源语言模型发展的项目，通过研究和应用扩展规律，成功创建了DeepSeek Chat模型，该模型在各种基准测试中表现出色，特别是在代码领域。 |
| [^2] | [Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving.](http://arxiv.org/abs/2401.02949) | 本文提出了一种名为Graph2Tac的图神经网络模型，用于在定理证明中学习数学概念的分层表示。该模型能够动态地将新的数学概念纳入到知识库中，并在Coq证明助手中进行训练和应用。 |
| [^3] | [Unsupervised Federated Domain Adaptation for Segmentation of MRI Images.](http://arxiv.org/abs/2401.02941) | 本文提出了一种无监督的联邦领域适应方法，可以在未注释的目标域中使用多个带有注释的源领域的知识来进行MRI图像分割。 |
| [^4] | [Analytically-Driven Resource Management for Cloud-Native Microservices.](http://arxiv.org/abs/2401.02920) | 本文介绍了一种基于分析模型的轻量级云原生微服务资源管理系统Ursa，通过将端到端SLA分解为每个服务的SLA，并在每个微服务层进行资源分配，以解决机器学习驱动方法中数据收集过程冗长和可扩展性有限的挑战。 |
| [^5] | [H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses.](http://arxiv.org/abs/2401.02905) | H2G2-Net是一种用于发现多模态生理反应的分层异构图生成网络框架，能够自动学习图结构而不需要预定义的领域知识。 |
| [^6] | [MsDC-DEQ-Net: Deep Equilibrium Model (DEQ) with Multi-scale Dilated Convolution for Image Compressive Sensing (CS).](http://arxiv.org/abs/2401.02884) | 这篇论文提出了一个名为MsDC-DEQ-Net的深度平衡模型，通过将压缩感知重建算法的步骤映射到深度网络块中，并结合了聚合残差变换和挤压与激励机制，在压缩感知重建中取得了竞争性能。 |
| [^7] | [Optimal Chaining of Vehicle Plans with Time Windows.](http://arxiv.org/abs/2401.02873) | 本论文提出了一种解决带有时间窗口的车辆路径问题的最优链路方法，考虑了计划的时间灵活性，并通过实证结果证明了该方法在解决静态拨打车问题时的优越性。 |
| [^8] | [AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models.](http://arxiv.org/abs/2401.02870) | 提出了一个用于塑造偏好和个性的代理框架（AFSPP），可以探索社交网络和主观意识对基于大语言模型的代理的偏好和个性形成的影响，并成功复制了几个人类个性实验的关键发现。 |
| [^9] | [A Customizable Generator for Comic-Style Visual Narrative.](http://arxiv.org/abs/2401.02863) | 本文介绍了一个理论启发的漫画风格视觉叙述生成器，通过将漫画创作习惯融入系统层级中的不同决策层次，实现了根据叙事目标生成多样化漫画的功能。 |
| [^10] | [Generative Large Language Models are autonomous practitioners of evidence-based medicine.](http://arxiv.org/abs/2401.02851) | 生成式大语言模型被应用于基于证据的医学实践，能够帮助管理临床医生面临的信息过载挑战。 |
| [^11] | [Thousands of AI Authors on the Future of AI.](http://arxiv.org/abs/2401.02843) | 数千位AI作者对未来AI的预测显示，到2028年，AI系统有50%的几率实现多个里程碑，包括自主构建全新的付款处理网站、创作一首与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。同时，无需辅助的机器在各种任务上胜过人类的几率估计为10%到2047年为50%。 |
| [^12] | [Pheme: Efficient and Conversational Speech Generation.](http://arxiv.org/abs/2401.02839) | Pheme是一个高效和对话式语音生成模型，能够在实时情况下工作，不仅具备自然的语音生成能力，还能与大型语言模型结合使用。 |
| [^13] | [CrisisViT: A Robust Vision Transformer for Crisis Image Classification.](http://arxiv.org/abs/2401.02838) | CrisisViT是一种用于危机图像分类的强大视觉Transformer，通过使用深度神经模型自动分类和标记图像，应对了社交媒体上大量危机图片分析需要更多时间和努力的问题。 |
| [^14] | [Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning.](http://arxiv.org/abs/2401.02810) | 本论文提出使用迁移学习来提高物理信息神经网络（PINN）训练的鲁棒性和收敛性。经过两个案例研究，发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数，且需要更少的数据点和更短的训练时间。 |
| [^15] | [PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering.](http://arxiv.org/abs/2401.02797) | 本文提出了一种针对医学视觉问答的多模态大型语言模型的参数高效微调框架，并在公共数据集上进行了验证。实验证明，该模型在封闭式问题上比GPT-4v模型的准确率提高了26％。 |
| [^16] | [From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models.](http://arxiv.org/abs/2401.02777) | 这项工作介绍了一种名为RAISE的架构，它将大型语言模型（LLMs）如GPT-4整合到对话代理中，通过引入双组件记忆系统来增强代理在多轮对话中的可控性和适应性。预liminary evaluations表明，RAISE在房地产销售领域具有优势，并具有广泛应用的潜力。 |
| [^17] | [Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets.](http://arxiv.org/abs/2401.02773) | 本研究提出了一种解决姿势识别中电极漂移问题的方法，通过使用HD-EMG电极子集，并增加来自不同电极位置的数据，同时提高了稳健性和性能。 |
| [^18] | [Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding.](http://arxiv.org/abs/2401.02749) | 该论文提出了一种无需超参数的近似最小贝叶斯风险（AMBR）解码方法，用于更快地进行文本生成任务。该方法通过使用迭代消除法算法来解决中位数识别问题，以达到加速解码的目的。 |
| [^19] | [MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning.](http://arxiv.org/abs/2401.02744) | 本文提出了MAMI方法，利用多注意力互信息用于长序列神经元字幕生成，并通过不同类型的注意机制和多个注意力结果的汇聚进一步提升了MILAN方法的性能。 |
| [^20] | [Fairness-Aware Job Scheduling for Multi-Job Federated Learning.](http://arxiv.org/abs/2401.02740) | 本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。 |
| [^21] | [Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks.](http://arxiv.org/abs/2401.02731) | 本文提出了一种参数高效稀疏制作的方法，它使用专家混合式架构将密集模型转换为稀疏模型，以实现在模型容量有限的情况下进行指令调整和泛化能力增强。 |
| [^22] | [Enhancing targeted transferability via feature space fine-tuning.](http://arxiv.org/abs/2401.02727) | 本文提出了一种通过特征空间微调AE，显著提高现有攻击的目标传递性的方法。实验结果表明，只需少量的微调即可普遍地增强攻击的传递能力，并显示出简单的迭代攻击可以与资源密集型方法相媲美甚至更好。 |
| [^23] | [Une ontologie pour les syst{\`e}mes multi-agents ambiants dans les villes intelligentes.](http://arxiv.org/abs/2401.02726) | 该论文提出了一种用于智能城市中环境多智能体系统的本体论，用于描述对象基础设施的语义结构，以便利用连接设备提供个性化服务。本体论在智能可移动性领域得到了应用，并且可适用于其他智能城市领域。 |
| [^24] | [Learning Image Demoireing from Unpaired Real Data.](http://arxiv.org/abs/2401.02719) | 本文提出了一种从不成对的真实数据中学习图像去莫尔问题的方法，通过合成与清晰图像对应的伪莫尔图像，并实现了多样化莫尔特征和类似真实无莫尔图像的细节，最终去除图像中的莫尔效应。 |
| [^25] | [Complementary Information Mutual Learning for Multimodality Medical Image Segmentation.](http://arxiv.org/abs/2401.02717) | 这篇论文介绍了一种称为互补信息相互学习（CIML）的框架，在多模态医学影像分割中解决了模态间冗余信息的负面影响。通过采用加法和任务分解的方法，CIML成功地消除了冗余信息，提高了分割的准确性。 |
| [^26] | [Graph-level Protein Representation Learning by Structure Knowledge Refinement.](http://arxiv.org/abs/2401.02713) | 本文提出了一种名为结构知识提炼（SKR）的新框架，通过对比学习解决了图级表示学习中存在的问题，并提出了一种自适应的数据增强策略。 |
| [^27] | [Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning.](http://arxiv.org/abs/2401.02710) | 本文提出了一种基于强化学习的方法，通过扩展搜索空间和利用预训练的公式化alpha集来生成协同公式化alpha因子，从而提高量化交易的性能。 |
| [^28] | [German Text Embedding Clustering Benchmark.](http://arxiv.org/abs/2401.02709) | 本研究提出了一个评估不同领域中聚类德语文本嵌入性能的基准测试，并对预训练模型和聚类算法进行了初步分析。结果显示单语和多语模型表现强劲，缩减嵌入维度可以提高聚类效果。此外，持续预训练德语BERT模型可能对短文本性能有显著改善。所有代码和数据集均公开可用。 |
| [^29] | [TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis.](http://arxiv.org/abs/2401.02708) | 本文提出了一种适应时间的三元组坐标损失函数TripleSurv，通过引入样本对之间的生存时间差异来鼓励模型量化排名相对风险，从而提高生存分析的准确性。 |
| [^30] | [XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model.](http://arxiv.org/abs/2401.02705) | XUAT-Copilot是一个使用大型语言模型的多智能体协作系统，旨在提高自动用户验收测试的自动化水平和测试脚本生成阶段的效率。 |
| [^31] | [Verifying Relational Explanations: A Probabilistic Approach.](http://arxiv.org/abs/2401.02703) | 本文提出了一种概率方法，用于验证关联数据上的解释。通过在原始数据上生成对称近似的反事实示例，学习了一个因子图模型来量化解释的不确定性。实验证明该方法可以帮助验证解释的质量。 |
| [^32] | [Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation.](http://arxiv.org/abs/2401.02683) | 这项研究提出了一个几何便利的去噪扩散模型，用于解决3D分子生成中的多体原子关系建模和键的预测问题。 |
| [^33] | [Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss.](http://arxiv.org/abs/2401.02677) | 本研究通过逐层损失的渐进式削减，引入了两个精简变体Segmind稳定扩散（SSD-1B）和Segmind-Vega，分别使用1.3B和0.74B参数的UNet结构实现。这些精简模型通过转移学习有效地模拟了原始的SDXL，并在与更大的SDXL模型相比的竞争中取得了有竞争力的结果。 |
| [^34] | [A unified multichannel far-field speech recognition system: combining neural beamforming with attention based end-to-end model.](http://arxiv.org/abs/2401.02673) | 本文提出了一个统一的多通道远场语音识别系统，将神经波束形成和基于注意力的端到端模型相结合，并进行了联合训练。使用分解复线性投影来形成神经波束形成，并探索源方向作为先验知识的有用性。 |
| [^35] | [Zero-shot Microclimate Prediction with Deep Learning.](http://arxiv.org/abs/2401.02665) | 该论文提出了一种零样本学习方法，利用从其他地理位置提取的知识，实现了对新的和未监测到的地点的各种气候测量的预测。 |
| [^36] | [A backdoor attack against link prediction tasks with graph neural networks.](http://arxiv.org/abs/2401.02663) | 本文研究了一种针对图神经网络链接预测任务的后门攻击方法，发现GNN模型容易受到后门攻击，提出了针对该任务的后门攻击方式。 |
| [^37] | [Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin.](http://arxiv.org/abs/2401.02661) | 这项研究开发了一种护士参与型人工智能系统，利用转移学习的预测数字孪生体来实现针对2型糖尿病患者的精准管理。在临床试验中，该系统通过结合各种数据源的模式和护士的专业知识，提供个性化的治疗反馈，以改善患者的治疗效果。 |
| [^38] | [A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids.](http://arxiv.org/abs/2401.02653) | 本研究提出了一种基于深度强化学习的智能调度方法，用于解决电动车在智能电网中的需求响应问题。通过调度电动车的充放电活动，以与配电系统操作员提供的目标能量配置文件一致，可以实现局部网络的平衡和优化。 |
| [^39] | [Adaptive Discounting of Training Time Attacks.](http://arxiv.org/abs/2401.02652) | 本研究展示了在存在环境动态和相对于受害者目标的非最优性时，即使目标行为无法被采纳，仍然可能进行C-TTA。我们开发了一种gammaDDPG算法来学习这种更强版本的C-TTA，并根据受害者当前的行为动态改变攻击策略规划时间。 |
| [^40] | [Simple Hierarchical Planning with Diffusion.](http://arxiv.org/abs/2401.02644) | 本研究引入了分层扩散规划(Hierarchical Diffuser)，结合了分层和基于扩散的规划的优点，提出了一种简单、快速且有效的规划方法。通过在较高层面上采用“跳跃”规划策略，我们的模型能够具有较大的感受野且计算成本较低，同时跳跃的子目标还能指导低层规划器，在微调阶段进一步提高方法的效果。在实验评估中，我们的方法在性能和效率方面表现出卓越的结果。 |
| [^41] | [Training and Serving System of Foundation Models: A Comprehensive Survey.](http://arxiv.org/abs/2401.02643) | 这篇论文是一项对基础模型的训练与部署系统进行综合调研的工作，讨论了在训练和部署过程中所面临的挑战和采用的高效策略。 |
| [^42] | [Characteristics and prevalence of fake social media profiles with AI-generated faces.](http://arxiv.org/abs/2401.02627) | 本文分析了使用AI生成的人脸作为头像的伪造社交媒体账户，发现它们用于传播诈骗、垃圾信息以及放大协同信息等不真实活动。通过开发一种有效的方法，作者估计使用GAN生成的面孔的账户普遍性下限在0.021%到0.044%之间，约为每日活跃账户10K个。 |
| [^43] | [Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human.](http://arxiv.org/abs/2401.02620) | 3D生成AI领域的进展及前景。稳定的扩散方法、多视角一致性控制和逼真的人体模型为高一致性和逼真外观的3D模型的生成做出了贡献。神经网络基础的3D存储和渲染模型加速了神经渲染模型的效率和逼真度。大规模语言模型的多模态能力增加了对多模态场景生成的灵活性和创造力。 |
| [^44] | [Neural Causal Abstractions.](http://arxiv.org/abs/2401.02602) | 本文提出了一种新的神经因果抽象方法，通过聚类变量和其域，用于解决真实因果推断任务中的挑战，并通过神经因果模型实现了学习和应用。 |
| [^45] | [Object-oriented backdoor attack against image captioning.](http://arxiv.org/abs/2401.02600) | 本文研究了图像描述模型的面向对象后门攻击，通过污染训练数据，并设计了一种面向对象的方法来制作污染样本。攻击后，模型在良性图像上表现正常，但对于污染图像，模型将生成与图像无关的句子。 |
| [^46] | [Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data.](http://arxiv.org/abs/2401.02591) | 本研究提出了一种通过生成合成数据来平衡类别不平衡数据的技术，优先平衡信息丰富区域，并通过优化类别后验比率来最大化在正确的类别区域生成合成样本的概率。实验结果表明该技术在提升深度学习模型方面具有卓越性能。 |
| [^47] | [Identification of 4FGL uncertain sources at Higher Resolutions with Inverse Discrete Wavelet Transform.](http://arxiv.org/abs/2401.02589) | 本文研究了从4FGL DR3不确定源中找到AGN候选源和识别BL Lac/FSRQ候选源的任务，提出了一种名为FDIDWT的新方法，通过逆离散小波变换（IDWT）的多分辨率分析和分形维数（FD）理论的相关特征估计，将原始数据转换成低维度和突出特征的数据集。 |
| [^48] | [Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting.](http://arxiv.org/abs/2401.02588) | 本文提出了一种基于3D高斯粒子投影的方法来映射卫星在轨道上的几何形状，能够在当前航天硬件上运行，并且在计算速度上比之前快近两个数量级，可以训练和渲染未知卫星的高质量新视角。 |
| [^49] | [Federated Learning for distribution skewed data using sample weights.](http://arxiv.org/abs/2401.02586) | 本论文研究了在分布偏斜数据情况下如何通过使用样本权重来改进联邦学习性能。主要思路是通过调整客户端分布使其更接近全局分布，从而实现机器学习模型更快地收敛和更高的准确性。 |
| [^50] | [t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making.](http://arxiv.org/abs/2401.02576) | t-DGR是一种用于决策制定中持续学习的基于轨迹的深度生成回放方法，通过生成任务样本来解决灾难性遗忘问题，并在连续世界基准测试中取得了最先进的性能。 |
| [^51] | [Large Language Models for Social Networks: Applications, Challenges, and Solutions.](http://arxiv.org/abs/2401.02575) | 本研究调查了如何在在线社交网络中开发大型语言模型（LLMs）应用。我们将LLM应用分为知识任务、娱乐任务和基础任务，并分享了挑战、解决方案和经验教训。 |
| [^52] | [Quantitative Technology Forecasting: a Review of Trend Extrapolation Methods.](http://arxiv.org/abs/2401.02549) | 本研究对定量技术预测中的趋势外推方法进行了系统回顾，发现增长曲线和时间序列方法是过去十年中最受欢迎的技术方法，而基于机器学习的混合模型则是近年的新兴方法。 |
| [^53] | [A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature.](http://arxiv.org/abs/2401.02542) | 本研究提出了一种将社区检测和图神经网络相结合的创新方法，以增强科学文献网络中的链接预测能力。通过将社区检测的结果与图神经网络模型结合，我们突破了链接预测中的可扩展性和分辨率限制，提高了预测准确性。 |
| [^54] | [DISO: A Domain Ontology for Modeling Dislocations in Crystalline Materials.](http://arxiv.org/abs/2401.02540) | 本文介绍了一种名为DISO的领域本体，用于建模晶体材料中的位错缺陷。该本体通过定义与线状缺陷相关的概念和关系，有助于理解位错行为并在位错动力学领域中具有广泛的应用价值。 |
| [^55] | [Comprehensive Exploration of Synthetic Data Generation: A Survey.](http://arxiv.org/abs/2401.02524) | 本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。 |
| [^56] | [Image-based Deep Learning for Smart Digital Twins: a Review.](http://arxiv.org/abs/2401.02523) | 本论文综述了基于图像的智能数字孪生体 (SDTs) 的发展方法和挑战，重点讨论了通过持续同化图像数据来观察和学习系统行为的方法，以及设计和实现SDTs的深度学习 (DL) 模型所面临的挑战。提供了对未来发展方向和机遇的见解。 |
| [^57] | [Moving-Horizon Estimators for Hyperbolic and Parabolic PDEs in 1-D.](http://arxiv.org/abs/2401.02516) | 本文介绍了一种用于双曲和抛物型PDE的移动时域估计器，通过PDE反步法将难以解决的观测器PDE转化为可以明确解决的目标观测器PDE，从而实现了在实时环境下消除数值解观测器PDE的需求。 |
| [^58] | [Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation.](http://arxiv.org/abs/2401.02511) | 本文介绍了使用神经算子进行增益调度的方法来处理具有非线性循环的输运PDE，并在局部实现了稳定。 |
| [^59] | [Memory, Consciousness and Large Language Model.](http://arxiv.org/abs/2401.02509) | 该论文研究了大型语言模型（LLM）与图尔文的记忆理论之间的对应关系，并提出了意识可能是一种基于这种对应关系的新兴能力的猜想。 |
| [^60] | [On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS).](http://arxiv.org/abs/2401.02500) | 本文研究了将大型语言模型（LLMs）与传统符号规划器集成的前景，并指出这种神经符号方法有望在解决复杂规划问题中发挥重要作用。 |
| [^61] | [Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows.](http://arxiv.org/abs/2401.02465) | 这篇论文讨论了气候变化对污水管理带来的复杂挑战，提出了一种基于可解释时间序列模型的废水建模方法，可以帮助预测关键水位点并改善废水管理和环境污染预防。 |
| [^62] | [Data-Centric Foundation Models in Computational Healthcare: A Survey.](http://arxiv.org/abs/2401.02458) | 计算医疗中的数据中心基础模型是一项调查研究，为医疗工作流程的改进提供了基于数据的人工智能方法，并讨论了安全性、评估和与人类价值观的一致性。基于FM的分析有望提高患者结果和临床工作流程表现。 |
| [^63] | [eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning.](http://arxiv.org/abs/2401.02457) | eCIL-MU是一种基于嵌入技术的逐类增量学习和机器取消学习的非破坏性框架，可用于在动态环境中快速获取关于新类别的知识，并消除先前学习类别的影响。 |
| [^64] | [A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management.](http://arxiv.org/abs/2401.02456) | 本研究综述探讨了AI支持的无人机系统在火灾管理中的应用，从火灾前到主动火灾阶段再到火灾后，通过整合无人机和深度学习模型，提供了更有效的野火管理解决方案。 |
| [^65] | [The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?.](http://arxiv.org/abs/2401.02452) | 本论文调查了机器学习研究中计算分歧对学术贡献和审查的影响，发现计算分歧导致学术界在计算密集型研究主题中的影响力降低，并且学术研究趋向于使用工业界开发的开源、预训练模型。为解决这一问题，建议通过国家支持的计算基础设施与开放科学倡议的结合来拓展学术见解。 |
| [^66] | [FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients.](http://arxiv.org/abs/2401.02433) | FedDiff是一个多模态协作扩散联邦学习框架，旨在实现来自不同客户的异构数据的安全融合，通过建立双分支扩散模型特征提取设置来驱动联邦学习过程。 |
| [^67] | [Automated Classification of Model Errors on ImageNet.](http://arxiv.org/abs/2401.02430) | 该论文提出了自动错误分类框架来解决ImageNet数据集中的标签噪声和错误问题，为研究模型选择如何影响错误分布提供了有价值的工具。 |
| [^68] | [Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities.](http://arxiv.org/abs/2401.02429) | 基于脑启发的脉冲神经网络是一种有前景的应用于工业故障诊断的替代方法，可以克服人工神经网络的限制，提供更精确和有效的故障识别。 |
| [^69] | [5G Positioning Advancements with AI/ML.](http://arxiv.org/abs/2401.02427) | 本文对基于AI/ML的5G系统内的直接定位进行了全面回顾，重点讨论了在传统方法不适用的挑战场景和条件下的潜力。研究发现了重要的模拟结果和观察，并提出了解决测量报告、数据收集和模型管理的解决方案，促进了直接定位的进一步发展。 |
| [^70] | [UAV Trajectory Planning for AoI-Minimal Data Collection in UAV-Aided IoT Networks by Transformer.](http://arxiv.org/abs/2401.02425) | 这项研究提出了一种机器学习算法，通过优化无人机的轨迹规划，最小化了在物联网网络中收集的数据的总年龄，从而保持了数据的新鲜度。 |
| [^71] | [Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning.](http://arxiv.org/abs/2401.02424) | 该论文利用EuroSAT和迁移学习技术对土地利用和土地覆盖进行映射，通过使用颜色波段进行微调，取得了99.19%的精确度，有助于制定保护和城市规划政策。 |
| [^72] | [Neuronal Auditory Machine Intelligence (NEURO-AMI) In Perspective.](http://arxiv.org/abs/2401.02421) | 神经听觉机器智能（NEURO-AMI）是一种从人类大脑中获取灵感的人工神经机器学习系统，在软计算领域有着广阔的应用前景。然而，为了满足用户需求，仍然需要改进其可承受性、复杂性和数据学习大小要求等方面的优化措施。 |
| [^73] | [Can AI Be as Creative as Humans?.](http://arxiv.org/abs/2401.01623) | 本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。 |
| [^74] | [Tensor Networks for Explainable Machine Learning in Cybersecurity.](http://arxiv.org/abs/2401.00867) | 张量网络可以帮助发展可解释的机器学习算法，并提供丰富的模型可解释性。在网络安全中，我们的无监督聚类算法基于矩阵乘积状态，在性能上与传统的深度学习模型相媲美。我们的方法还能提取特征概率、熵和互信息，提供了分类异常的引人入胜的叙述，并实现了前所未有的透明度和可解释性水平。 |
| [^75] | [Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges.](http://arxiv.org/abs/2401.00031) | 本文认为将大规模自监督预训练中的知识与决策问题相结合，可以解决决策中的样本效率和泛化性问题。通过提出先预训练再自适应的流程，并调研了决策预训练和下游推理中的数据收集、预训练目标和自适应策略的最新研究。最后，确定了发展决策基础模型的关键挑战和未来方向。 |
| [^76] | [FENet: Focusing Enhanced Network for Lane Detection.](http://arxiv.org/abs/2312.17163) | FENet是一个增强聚焦网络用于精准车道检测，通过聚焦采样和部分视野评估等创新方法，显著提高了检测准确性，尤其适用于曲线和远距离车道，在安全性方面具有重要意义。 |
| [^77] | [LLM in a flash: Efficient Large Language Model Inference with Limited Memory.](http://arxiv.org/abs/2312.11514) | 本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。 |
| [^78] | [Retrieval-Augmented Generation for Large Language Models: A Survey.](http://arxiv.org/abs/2312.10997) | 本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。 |
| [^79] | [PromptBench: A Unified Library for Evaluation of Large Language Models.](http://arxiv.org/abs/2312.07910) | PromptBench是一个用于评估大型语言模型的统一库，包括了提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具等组件，旨在促进原创研究和创建新的基准测试、部署下游应用以及设计新的评估协议。 |
| [^80] | [KwaiAgents: Generalized Information-seeking Agent System with Large Language Models.](http://arxiv.org/abs/2312.04889) | 本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。 |
| [^81] | [Grounding Foundation Models through Federated Transfer Learning: A General Framework.](http://arxiv.org/abs/2311.17431) | 本论文提出了一个通用框架，通过联邦迁移学习将基础模型接地，以解决面临的挑战，如受限的计算资源、数据隐私、模型异构性和模型所有权。这个框架可以帮助释放基础模型的潜力，并在不同行业中产生重要影响。 |
| [^82] | [GeoLocator: a location-integrated large multimodal model for inferring geo-privacy.](http://arxiv.org/abs/2311.13018) | GeoLocator是一种用于推断地理隐私的位置集成大型多模态模型，能够高精度地生成具体的地理细节，突显了在线数据共享和信息获取的威胁。 |
| [^83] | ["It's not like Jarvis, but it's pretty close!" -- Examining ChatGPT's Usage among Undergraduate Students in Computer Science.](http://arxiv.org/abs/2311.09651) | 大多数计算机科学本科生对ChatGPT持有积极态度，并认为它能在课程相关任务中发挥作用，但也存在一些需解决的挑战。 |
| [^84] | [Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning.](http://arxiv.org/abs/2311.09441) | 本文研究了分割联邦学习（SFL）中隐私和能耗之间的权衡，强调了快速收敛的优势，并分析了切割层对客户端能耗和隐私的影响。 |
| [^85] | [Deep learning in computed tomography pulmonary angiography imaging: a dual-pronged approach for pulmonary embolism detection.](http://arxiv.org/abs/2311.05197) | 我们提出了一种深度学习方法，用于计算机断层扫描肺动脉造影中的肺栓塞检测。该方法通过引入分类器的概率推理来指导检测预测，在自动PE诊断领域提供了一种新的贡献。我们的分类器利用注意力机制来同时关注整体外貌和局部病变区域，实现了稳健的性能。 |
| [^86] | [A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem.](http://arxiv.org/abs/2310.18446) | 本论文提出了一种新型的跳跃正交列表来解决动态最优传输问题，在考虑数据点权重或位置变化时能够有效地更新最优传输方案。 |
| [^87] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^88] | [Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals.](http://arxiv.org/abs/2309.09404) | 通过利用开放数据和人工智能方法，我们设计了一个系统来推荐团队，使得每个团队能够满足项目要求的技能覆盖，并且平衡候选成员之间的工作分配。 |
| [^89] | [Retrieval-Augmented Text-to-Audio Generation.](http://arxiv.org/abs/2309.08051) | 这篇论文提出了一种检索增强的文本到音频生成方法，用于解决长尾文本到音频生成的问题。通过利用检索到的相关文本-音频数据作为额外条件，从而增强了模型的学习能力，在AudioCaps数据集上取得了最先进的结果。 |
| [^90] | [Code-Style In-Context Learning for Knowledge-Based Question Answering.](http://arxiv.org/abs/2309.04695) | 本论文提出了一种在上下文中学习编程风格的方法，用于解决基于知识的问答中生成逻辑表达式的格式错误问题。 |
| [^91] | [Subjectivity in Unsupervised Machine Learning Model Selection.](http://arxiv.org/abs/2309.00201) | 无监督机器学习模型选择具有主观性，模型选择结果受模型构建者偏好的影响，并可能导致选择的不一致性。需要对模型选择过程进行更加深入的研究和标准化。 |
| [^92] | [Stabilizing RNN Gradients through Pre-training.](http://arxiv.org/abs/2308.12075) | 该研究通过预训练网络实现局部稳定性，拓展了已有的稳定性理论，可以应用于复杂结构的循环神经网络。 |
| [^93] | [Developmental Bootstrapping of AIs.](http://arxiv.org/abs/2308.04586) | 传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。 |
| [^94] | [Knapsack: Connectedness, Path, and Shortest-Path.](http://arxiv.org/abs/2307.12547) | 该论文研究了带有图论约束的背包问题，证明了问题的复杂性并提出了近似算法。 |
| [^95] | [Reinforcement Learning for Syntax-Guided Synthesis.](http://arxiv.org/abs/2307.09564) | 本论文提出了一种基于树搜索和强化学习的语法引导综合算法，解决了搜索问题复杂性和数据集较小的挑战。 |
| [^96] | [DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.](http://arxiv.org/abs/2306.11698) | 这项工作提出了对GPT模型进行全面可信度评估，考虑了多个方面的风险，发现了以前未公开的威胁漏洞，例如对毒性输出和个人信息泄漏的易被误导性。 |
| [^97] | [RE-centric Recommendations for the Development of Trustworthy(er) Autonomous Systems.](http://arxiv.org/abs/2306.01774) | 本研究发现目前AI系统开发中缺少要求工程（RE）这一环节且伦理指南术语和原则覆盖的不一致性，为解决该问题我们制定了一个术语表并研究了伦理AI开发框架在执行RE方面的适用性。 |
| [^98] | [DeepMerge: Deep Learning-Based Region-Merging for Image Segmentation.](http://arxiv.org/abs/2305.19787) | 本文提出一种名为DeepMerge的基于深度学习的区域合并方法，用于处理高分辨率图像分割问题，并成功使用远程感知数据集进行了验证。 |
| [^99] | [Training Diffusion Models with Reinforcement Learning.](http://arxiv.org/abs/2305.13301) | 本文研究了利用强化学习方法直接优化扩散模型以实现下游对象的问题，并提出一种称之为去噪扩散策略优化（DDPO）的有效策略梯度算法，能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。 |
| [^100] | [Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement.](http://arxiv.org/abs/2305.12711) | 本论文提出了一个双重最优传输标签分配(DOTLA)框架，以同时将一个模态中生成的标签分配给其对应的模态，实现无监督可见-红外人员再识别。在相应模态中邻居样本的指导下，还提出了一个跨模态邻居一致性引导的标签精炼和正则化模块，进一步提高了算法的精度和鲁棒性。 |
| [^101] | [MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos.](http://arxiv.org/abs/2304.05292) | 本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。 |
| [^102] | [Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks.](http://arxiv.org/abs/2301.06683) | 本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。 |
| [^103] | [Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration.](http://arxiv.org/abs/2211.12735) | 我们提出了Fast-iTPN，这是一个整体预训练Transformer金字塔网络，通过令牌迁移和令牌聚集等灵活设计来减少计算内存开销和加速推断。基于ImageNet-1K和COCO数据集，Fast-iTPN在图像分类和目标检测任务中达到了很好的性能。 |
| [^104] | [Variational Quantum and Quantum-Inspired Clustering.](http://arxiv.org/abs/2206.09893) | 这个论文提出了一种基于变分量子电路的量子聚类算法。通过将聚类问题转化为优化问题，并通过变分量子本征求解器（VQE）结合非正交量子比特状态来解决，可以有效地在NISQ设备上实现。数值模拟结果表明，即使只有一个量子比特，算法也具有出色的性能。此外，通过张量网络模拟算法，可以在经典硬件上运行量子启发式聚类算法。 |
| [^105] | [Stabilizing the LIF Neuron Training.](http://arxiv.org/abs/2202.00282) | 该论文研究了稳定LIF神经元训练的方法，通过实验和理论分析，确定了在不同任务和网络中选择最佳替代梯度的稳定性与效果的关系，减少了对超参数搜索的需求。 |
| [^106] | [INVIGORATE: Interactive Visual Grounding and Grasping in Clutter.](http://arxiv.org/abs/2108.11092) | INVIGORATE是一个通过自然语言与人类进行交互并在杂乱环境中抓取指定物体的机器人系统。该系统能够推断目标物体、推断物体阻挡关系，并生成多步计划来消除歧义并成功抓取目标物体。 |

# 详细

[^1]: DeepSeek LLM：借助长期主义扩展开源语言模型

    DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])

    [http://arxiv.org/abs/2401.02954](http://arxiv.org/abs/2401.02954)

    DeepSeek LLM是一个致力于以长期视野推进开源语言模型发展的项目，通过研究和应用扩展规律，成功创建了DeepSeek Chat模型，该模型在各种基准测试中表现出色，特别是在代码领域。

    

    开源大规模语言模型（LLM）的快速发展令人瞩目。然而，之前文献中描述的扩展规律得出了不同的结论，这对扩展LLM产生了负面影响。我们深入研究了扩展规律，并提出了我们独特的研究发现，以促进两种常用的开源配置（7B和67B）中大规模模型的扩展。在扩展规律的指导下，我们推出了DeepSeek LLM项目，致力于以长期视野推进开源语言模型的发展。为了支持预训练阶段，我们开发了一个数据集，目前包含2万亿个标记，并不断扩大。我们还在DeepSeek LLM基本模型上进行了监督微调（SFT）和直接偏好优化（DPO），从而创建了DeepSeek Chat模型。我们的评估结果表明，DeepSeek LLM 67B在各种基准测试中超过了LLaMA-2 70B，特别是在代码领域。

    The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code
    
[^2]: Graph2Tac: 在定理证明中学习数学概念的分层表示

    Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])

    [http://arxiv.org/abs/2401.02949](http://arxiv.org/abs/2401.02949)

    本文提出了一种名为Graph2Tac的图神经网络模型，用于在定理证明中学习数学概念的分层表示。该模型能够动态地将新的数学概念纳入到知识库中，并在Coq证明助手中进行训练和应用。

    

    数学及其应用中存在大量的概念。它们在不同的学科领域中有很大的变化，并且每篇数学论文或应用中都会引入新的概念。形式化理论建立了一个层次结构，其中包括了定义、定理和相互引用的证明。当一个AI代理人证明一个新的定理时，大多数与该定理相关的数学概念和引理在训练过程中可能从未被见过。这在Coq证明助手中尤为明显，该助手拥有各种各样的Coq项目，每个项目都有自己的定义、引理，甚至用于证明这些引理的自定义策略过程。将这样的新信息即时地融入到代理人的知识库中对于代理人至关重要。我们通过利用一个新的、大规模的、基于图的数据集，在Coq中进行机器学习。我们利用Coq术语的忠实图表示，创建了一种新颖的图神经网络Graph2Tac，该网络通过定义之间的依赖关系创建了一个有向图。

    Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
    
[^3]: 无监督的联邦领域适应用于MRI图像分割

    Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v1 [cs.CV])

    [http://arxiv.org/abs/2401.02941](http://arxiv.org/abs/2401.02941)

    本文提出了一种无监督的联邦领域适应方法，可以在未注释的目标域中使用多个带有注释的源领域的知识来进行MRI图像分割。

    

    使用深度神经网络自动对磁共振成像（MRI）图像进行语义分割极大地有助于评估和规划各种临床应用的治疗。然而，训练这些模型需要大量标注数据来实施端到端的监督学习过程。即使我们标注了足够的数据，MRI图像由于患者、MRI扫描仪和成像协议的差异而显示出相当大的变异性。这种变异性需要对每个特定的应用领域重新训练神经网络，这又需要专家放射科医生对所有新领域进行手工注释。为了减轻对持续数据注释的需求，我们开发了一种无监督的联邦领域适应方法，该方法使用多个带有注释的源领域。我们的方法能够从多个带有注释的源域中转移知识，以适应在未注释的目标域中的有效使用。

    Automatic semantic segmentation of magnetic resonance imaging (MRI) images using deep neural networks greatly assists in evaluating and planning treatments for various clinical applications. However, training these models is conditioned on the availability of abundant annotated data to implement the end-to-end supervised learning procedure. Even if we annotate enough data, MRI images display considerable variability due to factors such as differences in patients, MRI scanners, and imaging protocols. This variability necessitates retraining neural networks for each specific application domain, which, in turn, requires manual annotation by expert radiologists for all new domains. To relax the need for persistent data annotation, we develop a method for unsupervised federated domain adaptation using multiple annotated source domains. Our approach enables the transfer of knowledge from several annotated source domains to adapt a model for effective use in an unannotated target domain. Init
    
[^4]: 基于分析的云原生微服务资源管理

    Analytically-Driven Resource Management for Cloud-Native Microservices. (arXiv:2401.02920v1 [cs.DC])

    [http://arxiv.org/abs/2401.02920](http://arxiv.org/abs/2401.02920)

    本文介绍了一种基于分析模型的轻量级云原生微服务资源管理系统Ursa，通过将端到端SLA分解为每个服务的SLA，并在每个微服务层进行资源分配，以解决机器学习驱动方法中数据收集过程冗长和可扩展性有限的挑战。

    

    云原生微服务的资源管理近年来受到了很多关注。先前的研究表明，机器学习驱动的方法在SLA维护和资源效率方面优于传统的技术，如自动伸缩。然而，机器学习驱动的方法也面临着数据收集过程冗长和可扩展性有限等挑战。本文介绍了Ursa，一个轻量级的云原生微服务资源管理系统，旨在解决这些挑战。Ursa使用一个分析模型将端到端SLA分解为每个服务的SLA，并将每个服务的SLA映射到每个微服务层的资源分配。为了加快探索过程并避免长时间的SLA违规，Ursa逐个微服务进行探索，并在延迟超过其SLA时快速停止探索。我们在一组代表性的端到端微服务拓扑上评估了Ursa，包括社交网络、媒体服务和视频等。

    Resource management for cloud-native microservices has attracted a lot of recent attention. Previous work has shown that machine learning (ML)-driven approaches outperform traditional techniques, such as autoscaling, in terms of both SLA maintenance and resource efficiency. However, ML-driven approaches also face challenges including lengthy data collection processes and limited scalability. We present Ursa, a lightweight resource management system for cloud-native microservices that addresses these challenges. Ursa uses an analytical model that decomposes the end-to-end SLA into per-service SLA, and maps per-service SLA to individual resource allocations per microservice tier. To speed up the exploration process and avoid prolonged SLA violations, Ursa explores each microservice individually, and swiftly stops exploration if latency exceeds its SLA.  We evaluate Ursa on a set of representative and end-to-end microservice topologies, including a social network, media service and video 
    
[^5]: H2G2-Net:一种用于多模态生理反应发现的分层异构图生成网络框架

    H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])

    [http://arxiv.org/abs/2401.02905](http://arxiv.org/abs/2401.02905)

    H2G2-Net是一种用于发现多模态生理反应的分层异构图生成网络框架，能够自动学习图结构而不需要预定义的领域知识。

    

    在各种研究应用中，利用多模态生理信号来发现人类认知和情感状态引起了人们的关注。人体的生理反应受到人类认知的影响，常用于分析认知状态。从网络科学的角度来看，这些异构生理模式在图结构中的互动可能提供有益的信息来支持认知状态的预测。然而，目前没有办法得到异构模态之间的精确连接，并且存在一种分层结构的子模态。现有的图神经网络设计用于在预定义的图结构上学习非层次化的同质图，无法从层次化的多模态生理数据中学习，没有预定义的图结构。为此，我们提出了一种分层异构图生成网络（H2G2-Net），能够自动学习图结构而不需要先验领域知识。

    Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain 
    
[^6]: MsDC-DEQ-Net: 用于图像压缩感知的多尺度扩张卷积DEQ深度平衡模型

    MsDC-DEQ-Net: Deep Equilibrium Model (DEQ) with Multi-scale Dilated Convolution for Image Compressive Sensing (CS). (arXiv:2401.02884v1 [eess.IV])

    [http://arxiv.org/abs/2401.02884](http://arxiv.org/abs/2401.02884)

    这篇论文提出了一个名为MsDC-DEQ-Net的深度平衡模型，通过将压缩感知重建算法的步骤映射到深度网络块中，并结合了聚合残差变换和挤压与激励机制，在压缩感知重建中取得了竞争性能。

    

    压缩感知是一种利用较少测量量恢复稀疏信号的技术。为了解决压缩感知重建的计算挑战，我们的目标是开发一个可解释且简明的神经网络模型，用于使用压缩感知重建自然图像。我们通过将迭代缩减阈值算法（ISTA）的一个步骤映射到一个深度网络块中来实现这一目标，表示一个ISTA的一次迭代。为了增强学习能力和融入结构多样性，我们将聚合残差变换（ResNeXt）和挤压与激励机制（SE）集成到ISTA块中。该块作为深度平衡层连接到一个半张量积网络（STP-Net），用于方便采样并提供初始重建。得到的模型称为MsDC-DEQ-Net，与最先进的基于网络的方法相比具有竞争性能。它显著减少了存储需求。

    Compressive sensing (CS) is a technique that enables the recovery of sparse signals using fewer measurements than traditional sampling methods. To address the computational challenges of CS reconstruction, our objective is to develop an interpretable and concise neural network model for reconstructing natural images using CS. We achieve this by mapping one step of the iterative shrinkage thresholding algorithm (ISTA) to a deep network block, representing one iteration of ISTA. To enhance learning ability and incorporate structural diversity, we integrate aggregated residual transformations (ResNeXt) and squeeze-and-excitation (SE) mechanisms into the ISTA block. This block serves as a deep equilibrium layer, connected to a semi-tensor product network (STP-Net) for convenient sampling and providing an initial reconstruction. The resulting model, called MsDC-DEQ-Net, exhibits competitive performance compared to state-of-the-art network-based methods. It significantly reduces storage requ
    
[^7]: 车辆计划与时间窗口的最优链路

    Optimal Chaining of Vehicle Plans with Time Windows. (arXiv:2401.02873v1 [math.OC])

    [http://arxiv.org/abs/2401.02873](http://arxiv.org/abs/2401.02873)

    本论文提出了一种解决带有时间窗口的车辆路径问题的最优链路方法，考虑了计划的时间灵活性，并通过实证结果证明了该方法在解决静态拨打车问题时的优越性。

    

    在解决带有时间窗口的车辆路径问题时，我们经常需要将车辆计划连接成跨越更长时间区间的序列，换句话说，我们需要执行计划链路。最近，提出了一种基于网络的方法来解决车队规模问题。然而，该方法不考虑计划的时间灵活性，这是所有带有时间窗口的车辆路径问题的重要属性。相反，计划具有固定时间，不能延迟。本文提出了一种新的问题建模，考虑了延迟和给定时间窗口，并提出了一种解决该问题的方法。此外，我们证明了该方法是最优的，并对其复杂性进行了分析。最后，我们列举了一些实际应用，并对其中一个应用进行了演示：静态拨打车问题的解决方法。演示结果显示，在大量实例中，所提出的方法提供了更好的解决方案。

    For solving problems from the domain of vehicle routing with time windows, we often need to connect vehicle plans into sequences spanning a longer time horizon or, in other words, we need to perform a plan chaining. Recently, a network-based solution has been proposed to solve the fleet-sizing problem. The method, however, does not consider the time flexibility of the plans, an essential property of all vehicle routing problems with time windows. Instead, plans have fixed times and cannot be delayed. This work presents a new problem formulation that considers delays in line with the given time windows and a method that can be used to solve it. Moreover, we prove that the method is optimal, and we analyze its complexity. Finally, we list some practical applications and perform a demonstration for one of them: the method for solving the static Dial-a-ride problem. The demonstration results show that for a significant number of instances, the proposed method provides a better solution tha
    
[^8]: AFSPP: 利用大语言模型塑造偏好和个性的代理框架

    AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models. (arXiv:2401.02870v1 [cs.MA])

    [http://arxiv.org/abs/2401.02870](http://arxiv.org/abs/2401.02870)

    提出了一个用于塑造偏好和个性的代理框架（AFSPP），可以探索社交网络和主观意识对基于大语言模型的代理的偏好和个性形成的影响，并成功复制了几个人类个性实验的关键发现。

    

    大语言模型的发展引入了一种新的方法来研究人类行为仿真。最近的研究利用基于大语言模型的代理创建了一个社会学研究环境，代理基于大语言模型的未过滤特征展示行为。然而，这些研究忽视了在类似人类环境中的迭代发展 - 人类的偏好和个性是复杂的，受到各种因素的影响，并且因环境和主观影响而不断变化。鉴于这一观察，我们提出了一个用于塑造偏好和个性的代理框架（AFSPP），探索社交网络和主观意识对基于大语言模型的代理的偏好和个性形成的多方面影响。通过AFSPP，我们首次成功复制了几个人类个性实验的关键发现。其他基于AFSPP的实验结果表明，计划制定...

    The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents' preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, s
    
[^9]: 一个可定制的漫画风格视觉叙述生成器

    A Customizable Generator for Comic-Style Visual Narrative. (arXiv:2401.02863v1 [cs.AI])

    [http://arxiv.org/abs/2401.02863](http://arxiv.org/abs/2401.02863)

    本文介绍了一个理论启发的漫画风格视觉叙述生成器，通过将漫画创作习惯融入系统层级中的不同决策层次，实现了根据叙事目标生成多样化漫画的功能。

    

    我们提出了一个理论启发的视觉叙述生成器，将漫画创作习惯融入到整个系统层级中，以创建漫画内容。生成器通过从面板构图、物体位置、面板转换和叙事元素的层级中进行顺序决策来创建漫画。每个层级的决策都基于叙事目标，并遵循媒介的相应层级习惯。Cohn的叙事语法提供整体故事线。基于三分法则的摄影构图用于提供面板构图。基于McCloud提出的以局部、场景、角色和时间变化为重点的面板转换被编码在转换层中。最后，使用行动动词本体论来分析行动动词并基于其添加常见的叠加符号（如感叹号）。通过各种设置和示例来展示生成的漫画的多样性。

    We present a theory-inspired visual narrative generator that incorporates comic-authoring idioms, which transfers the conceptual principles of comics into system layers that integrate the theories to create comic content. The generator creates comics through sequential decision-making across layers from panel composition, object positions, panel transitions, and narrative elements. Each layer's decisions are based on narrative goals and follow the respective layer idioms of the medium. Cohn's narrative grammar provides the overall story arc. Photographic compositions inspired by the rule of thirds is used to provide panel compositions. McCloud's proposed panel transitions based on focus shifts between scene, character, and temporal changes are encoded in the transition layer. Finally, common overlay symbols (such as the exclamation) are added based on analyzing action verbs using an action-verb ontology. We demonstrate the variety of generated comics through various settings with examp
    
[^10]: 生成式大语言模型是基于证据的医学实践的自主从业者。 (arXiv:2401.02851v1 [cs.AI])

    Generative Large Language Models are autonomous practitioners of evidence-based medicine. (arXiv:2401.02851v1 [cs.AI])

    [http://arxiv.org/abs/2401.02851](http://arxiv.org/abs/2401.02851)

    生成式大语言模型被应用于基于证据的医学实践，能够帮助管理临床医生面临的信息过载挑战。

    

    背景：基于证据的医学 (EBM) 是现代临床实践的基础，要求临床医生不断更新知识并在患者护理中应用最佳临床证据。EBM的实践面临着医学研究的快速进展带来的信息过载挑战。人工智能 (AI)，特别是生成式大语言模型 (LLMs)，提供了管理这种复杂性的有希望的解决方案。方法：本研究涉及在各个专业领域收集现实世界的临床案例，将其转化为.json文件进行分析。使用了LLMs，包括ChatGPT 3.5和4等专有模型，Gemini Pro等，以及开源模型，如LLaMA v2和Mixtral-8x7B。这些模型配备了从案例文件中检索信息并做出临床决策的工具，类似于临床医生在现实世界中的操作方式。模型表现根据正确性进行评估。

    Background: Evidence-based medicine (EBM) is fundamental to modern clinical practice, requiring clinicians to continually update their knowledge and apply the best clinical evidence in patient care. The practice of EBM faces challenges due to rapid advancements in medical research, leading to information overload for clinicians. The integration of artificial intelligence (AI), specifically Generative Large Language Models (LLMs), offers a promising solution towards managing this complexity.  Methods: This study involved the curation of real-world clinical cases across various specialties, converting them into .json files for analysis. LLMs, including proprietary models like ChatGPT 3.5 and 4, Gemini Pro, and open-source models like LLaMA v2 and Mixtral-8x7B, were employed. These models were equipped with tools to retrieve information from case files and make clinical decisions similar to how clinicians must operate in the real world. Model performance was evaluated based on correctness
    
[^11]: 数千位AI作者对未来AI的预测

    Thousands of AI Authors on the Future of AI. (arXiv:2401.02843v1 [cs.CY])

    [http://arxiv.org/abs/2401.02843](http://arxiv.org/abs/2401.02843)

    数千位AI作者对未来AI的预测显示，到2028年，AI系统有50%的几率实现多个里程碑，包括自主构建全新的付款处理网站、创作一首与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。同时，无需辅助的机器在各种任务上胜过人类的几率估计为10%到2047年为50%。

    

    在迄今为止最大规模的调查中，2778名在顶级人工智能（AI）会议上发表过论文的研究人员对AI进展的速度、高级AI系统的性质和影响进行了预测。总体预测显示，到2028年，AI系统至少有50%的几率实现多个里程碑，包括自主构建一个全新的付款处理网站、创作一首可以与知名音乐家的新歌难以区分的歌曲，并自主下载和调整大型语言模型。如果科学持续不受干扰，2027年无需辅助的机器在各种任务上胜过人类的几率估计为10%，到2047年为50%。后者的估计比我们一年前进行的类似调查[Grace et al., 2022]提前了13年。然而，所有人类职业完全可自动化的几率预计要到2037年达到10%，到2116年才达到50%（与2022年调查中的2164年相比）。

    In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).  Most
    
[^12]: Pheme: 高效和对话式语音生成

    Pheme: Efficient and Conversational Speech Generation. (arXiv:2401.02839v1 [eess.AS])

    [http://arxiv.org/abs/2401.02839](http://arxiv.org/abs/2401.02839)

    Pheme是一个高效和对话式语音生成模型，能够在实时情况下工作，不仅具备自然的语音生成能力，还能与大型语言模型结合使用。

    

    最近几年，语音生成取得了显著进展，现在已经实现了一次生成能力，往往几乎无法区分是否为真实的人声。将这样的语音生成进展与大型语言模型结合起来，可能会彻底改变各种应用。然而，某些应用，如辅助对话系统，需要自然和对话式语音生成工具，也需要在实时情况下运行效率高。当前的最先进模型如VALL-E和SoundStorm，由分层神经音频编解码器驱动，需要大型神经组件和大量训练数据才能良好运作。相比之下，MQTTS旨在构建更紧凑的对话式TTS模型，同时利用小规模真实对话式语音数据。然而，其自回归性质导致推理延迟高，从而限制了实时应用。

    In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitali
    
[^13]: CrisisViT:一种用于危机图像分类的强大视觉Transformer

    CrisisViT: A Robust Vision Transformer for Crisis Image Classification. (arXiv:2401.02838v1 [cs.CV])

    [http://arxiv.org/abs/2401.02838](http://arxiv.org/abs/2401.02838)

    CrisisViT是一种用于危机图像分类的强大视觉Transformer，通过使用深度神经模型自动分类和标记图像，应对了社交媒体上大量危机图片分析需要更多时间和努力的问题。

    

    在紧急情况下，危机应对机构需要快速准确地评估地面情况，以便部署相关服务和资源。然而，由于受影响地区的数据可能稀缺，直到当地的应急响应服务能够提供第一手报告，当局常常必须根据有限信息做出决策。幸运的是，智能手机普及和高质量相机的普遍可用性使得社交媒体上的公民记者成为危机响应者宝贵的信息来源。然而，分析公民发布的大量图片需要比通常可用的时间和努力更多。为了解决这个问题，本文提出了使用最先进的深度神经模型进行自动图像分类/标记的方法，具体来说是通过将Transformer架构应用于危机图像分类中（CrisisViT）。我们利用新的Incidents1M危机图像数据集开发了一系列基于Transformer的新方法。

    In times of emergency, crisis response agencies need to quickly and accurately assess the situation on the ground in order to deploy relevant services and resources. However, authorities often have to make decisions based on limited information, as data on affected regions can be scarce until local response services can provide first-hand reports. Fortunately, the widespread availability of smartphones with high-quality cameras has made citizen journalism through social media a valuable source of information for crisis responders. However, analyzing the large volume of images posted by citizens requires more time and effort than is typically available. To address this issue, this paper proposes the use of state-of-the-art deep neural models for automatic image classification/tagging, specifically by adapting transformer-based architectures for crisis image classification (CrisisViT). We leverage the new Incidents1M crisis image dataset to develop a range of new transformer-based image 
    
[^14]: 使用迁移学习的物理信息神经网络解决高频率和多尺度问题

    Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning. (arXiv:2401.02810v1 [cs.LG])

    [http://arxiv.org/abs/2401.02810](http://arxiv.org/abs/2401.02810)

    本论文提出使用迁移学习来提高物理信息神经网络（PINN）训练的鲁棒性和收敛性。经过两个案例研究，发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数，且需要更少的数据点和更短的训练时间。

    

    物理信息神经网络（PINN）是一种用于偏微分方程（ODEs/PDEs）的数据驱动求解器，提供了统一的框架来处理前向和反向问题。然而，目标函数的复杂性常常导致训练失败。当解决高频率和多尺度问题时，这个问题尤为突出。我们提出使用迁移学习来提高训练PINN的鲁棒性和收敛性，从低频率问题开始训练，并逐渐接近高频率问题。通过两个案例研究，我们发现迁移学习可以有效地训练PINN来近似解决方案，从低频率问题到高频率问题，而不增加网络参数。此外，它需要更少的数据点和更短的训练时间。我们详细描述了我们的训练策略，包括优化器的选择，并提出了使用迁移学习来训练神经网络的指南。

    Physics-informed neural network (PINN) is a data-driven solver for partial and ordinary differential equations(ODEs/PDEs). It provides a unified framework to address both forward and inverse problems. However, the complexity of the objective function often leads to training failures. This issue is particularly prominent when solving high-frequency and multi-scale problems. We proposed using transfer learning to boost the robustness and convergence of training PINN, starting training from low-frequency problems and gradually approaching high-frequency problems. Through two case studies, we discovered that transfer learning can effectively train PINN to approximate solutions from low-frequency problems to high-frequency problems without increasing network parameters. Furthermore, it requires fewer data points and less training time. We elaborately described our training strategy, including optimizer selection, and suggested guidelines for using transfer learning to train neural networks 
    
[^15]: PeFoMed：针对医学视觉问答的多模态大型语言模型的参数高效微调

    PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])

    [http://arxiv.org/abs/2401.02797](http://arxiv.org/abs/2401.02797)

    本文提出了一种针对医学视觉问答的多模态大型语言模型的参数高效微调框架，并在公共数据集上进行了验证。实验证明，该模型在封闭式问题上比GPT-4v模型的准确率提高了26％。

    

    多模态大型语言模型（MLLM）在传统大型语言模型的能力上进行了进化扩展，使它们能够应对超越纯文本应用范围的挑战。它利用了先前编码在这些语言模型中的知识，从而增强了它们在多模态环境下的适用性和功能性。最近的研究探讨了将MLLMs适应为生成任务以解决医学视觉问答（Med-VQA）任务的自由形式答案的能力。在本文中，我们提出了一种针对Med-VQA应用特别定制的参数高效微调框架，并在公共基准数据集上进行了实证验证。为了准确衡量性能，我们进行了人工评估，结果显示我们的模型在封闭式问题的整体准确率上达到了81.9％，且其相对于GPT-4v模型的绝对准确率超过了26％。

    Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
    
[^16]: 从LLM到对话代理：具有大型语言模型微调的记忆增强架构

    From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])

    [http://arxiv.org/abs/2401.02777](http://arxiv.org/abs/2401.02777)

    这项工作介绍了一种名为RAISE的架构，它将大型语言模型（LLMs）如GPT-4整合到对话代理中，通过引入双组件记忆系统来增强代理在多轮对话中的可控性和适应性。预liminary evaluations表明，RAISE在房地产销售领域具有优势，并具有广泛应用的潜力。

    

    本文介绍了RAISE（Scratchpad和Examples辅助推理和行为）,一种先进的架构，增强了将GPT-4等大型语言模型（LLMs）整合到对话代理中的能力。RAISE是ReAct框架的改进版本，包括一个双组件记忆系统，模仿人类的短期记忆和长期记忆，以保持对话的上下文和连续性。它包括了一个全面的代理构建情景，包括对话选择，场景提取，CoT完成和场景增强等阶段，最终导致LLMs的训练阶段。这种方法似乎提高了代理在复杂的多轮对话中的可控性和适应性。我们在房地产销售环境中的初步评估表明，RAISE相对于传统代理有一些优势，表明它在更广泛的应用中具有潜力。通过提供一个强大的框架来开发更具上下文感知和多功能的对话代理，这项工作为AI领域做出了贡献。

    This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile convers
    
[^17]: 通过HD-EMG电极子集解决姿势识别中的电极漂移问题

    Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets. (arXiv:2401.02773v1 [cs.LG])

    [http://arxiv.org/abs/2401.02773](http://arxiv.org/abs/2401.02773)

    本研究提出了一种解决姿势识别中电极漂移问题的方法，通过使用HD-EMG电极子集，并增加来自不同电极位置的数据，同时提高了稳健性和性能。

    

    sEMG模式识别算法在解码运动意图方面进行了广泛研究，但已知对于不断变化的记录条件非常脆弱，性能在不同受试者和不同会话之间会显著下降。多通道表面肌电图（也称为高密度sEMG）系统通过使用额外的电极收集信息，可以提高性能。然而，由于有限的数据集和解决电极放置等变量来源的困难，缺乏稳健性一直存在。在本研究中，我们提出在一组输入通道子集上进行训练，并通过来自不同电极位置的数据增强我们的训练分布，从而同时解决电极漂移问题和降低输入维度。我们的方法提高了对电极漂移的稳健性，并在不同受试者和分类算法间显著提高了会话间性能。

    sEMG pattern recognition algorithms have been explored extensively in decoding movement intent, yet are known to be vulnerable to changing recording conditions, exhibiting significant drops in performance across subjects, and even across sessions. Multi-channel surface EMG, also referred to as high-density sEMG (HD-sEMG) systems, have been used to improve performance with the information collected through the use of additional electrodes. However, a lack of robustness is ever present due to limited datasets and the difficulties in addressing sources of variability, such as electrode placement. In this study, we propose training on a collection of input channel subsets and augmenting our training distribution with data from different electrode locations, simultaneously targeting electrode shift and reducing input dimensionality. Our method increases robustness against electrode shift and results in significantly higher intersession performance across subjects and classification algorith
    
[^18]: 无需超参数的更快最小贝叶斯风险解码方法

    Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])

    [http://arxiv.org/abs/2401.02749](http://arxiv.org/abs/2401.02749)

    该论文提出了一种无需超参数的近似最小贝叶斯风险（AMBR）解码方法，用于更快地进行文本生成任务。该方法通过使用迭代消除法算法来解决中位数识别问题，以达到加速解码的目的。

    

    最小贝叶斯风险解码被证明是一种在广泛的文本生成任务中替代束搜索解码的强大方法。然而，MBR需要大量的时间来计算MBR目标，这使得该方法在许多需要响应时间至关重要的情况下不可行。最近提出了基于置信度的剪枝(CBP)方法来降低机器翻译任务中的推理时间。尽管已经证明它能显著减少计算量，但是它需要使用开发集进行超参数调优才能发挥作用。为此，我们提出了一种无需超参数的近似最小贝叶斯风险（AMBR）解码方法。AMBR基于以下观察得出：计算基于样本的MBR目标的问题是中位数识别问题。AMBR使用了迭代消除法（CSH）算法，这是迄今为止最好的近似算法。

    Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding approximately. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best approximation algorithm to date
    
[^19]: MAMI: 多注意力互信息用于长序列神经元字幕生成

    MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning. (arXiv:2401.02744v1 [cs.AI])

    [http://arxiv.org/abs/2401.02744](http://arxiv.org/abs/2401.02744)

    本文提出了MAMI方法，利用多注意力互信息用于长序列神经元字幕生成，并通过不同类型的注意机制和多个注意力结果的汇聚进一步提升了MILAN方法的性能。

    

    神经元标记是一种可视化特定神经元对激活该神经元的模式的行为和反应的方法。神经元标记提取了深度神经网络中某些神经元捕获的特征信息，其中之一使用了编码器-解码器图像字幕生成方法。编码器可以是预训练的基于CNN的模型，解码器是基于RNN的模型用于文本生成。之前的工作，即MILAN（Mutual Information-guided Linguistic Annotation of Neuron），尝试使用修改后的Show, Attend, and Tell（SAT）模型进行神经元行为的可视化，在编码器中使用LSTM与Bahdanau注意力。MILAN在短序列神经元字幕生成方面表现出色，但在长序列神经元字幕生成方面表现不佳，因此在本文中，我们希望通过利用不同类型的注意力机制并额外添加若干注意力结果来进一步提升MILAN的性能。

    Neuron labeling is an approach to visualize the behaviour and respond of a certain neuron to a certain pattern that activates the neuron. Neuron labeling extract information about the features captured by certain neurons in a deep neural network, one of which uses the encoder-decoder image captioning approach. The encoder used can be a pretrained CNN-based model and the decoder is an RNN-based model for text generation. Previous work, namely MILAN (Mutual Information-guided Linguistic Annotation of Neuron), has tried to visualize the neuron behaviour using modified Show, Attend, and Tell (SAT) model in the encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show great result on short sequence neuron captioning, but it does not show great result on long sequence neuron captioning, so in this work, we would like to improve the performance of MILAN even more by utilizing different kind of attention mechanism and additionally adding several attention result into one, 
    
[^20]: 为多任务联邦学习提供公平性感知的作业调度

    Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])

    [http://arxiv.org/abs/2401.02740](http://arxiv.org/abs/2401.02740)

    本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。

    

    联邦学习（FL）使多个数据所有者（即FL客户端）能够在不泄露敏感私人数据的情况下共同训练机器学习模型。现有的FL研究主要关注垄断场景，在该场景中，单个FL服务器在每轮训练中选择一部分FL客户端来更新其本地模型。实际上，可能会有多个FL服务器同时尝试从同一个池中选择客户端。本文提出了一种首创的公平感知联邦作业调度（FairFedJS）方法来弥合这一差距。基于Lyapunov优化，它通过同时考虑当前需求和作业付款出价，确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，以防止等待时间过长。基于两个数据集对FairFedJS与四种最先进的方法进行了大量实验证明了其显著优势。它在平均上击败了最佳基准线31.9%和1.0%。

    Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
    
[^21]: 参数高效稀疏制作：从密集型到专家混合式用于通用任务的指令调整

    Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])

    [http://arxiv.org/abs/2401.02731](http://arxiv.org/abs/2401.02731)

    本文提出了一种参数高效稀疏制作的方法，它使用专家混合式架构将密集模型转换为稀疏模型，以实现在模型容量有限的情况下进行指令调整和泛化能力增强。

    

    大型语言模型(LLMs)在通用自然语言处理(NLP)任务中展示出相当的熟练程度。指令调整作为一种成功的范例，增强了LLMs遵循自然语言指令并在各种任务中展现出强大的泛化能力。然而，由于模型容量限制，这些模型在多个任务中经常遇到性能限制。在指令调整阶段扩展模型容量面临着巨大的挑战。为了解决这个问题，我们引入了一种新颖的方法，参数高效稀疏制作(PESC)，它使用专家混合式(MoE)架构将密集模型转换为稀疏模型。PESC将适配器集成到稀疏模型的MoE层中，区分不同的专家而不改变这些层中的个体权重。这种方法显著降低了计算成本和GPU内存需求，通过最小的增加实现了模型容量的扩展。

    Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase 
    
[^22]: 通过特征空间微调提高目标可传递性

    Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])

    [http://arxiv.org/abs/2401.02727](http://arxiv.org/abs/2401.02727)

    本文提出了一种通过特征空间微调AE，显著提高现有攻击的目标传递性的方法。实验结果表明，只需少量的微调即可普遍地增强攻击的传递能力，并显示出简单的迭代攻击可以与资源密集型方法相媲美甚至更好。

    

    由于其对隐私保护的潜力和激发鲁棒性神经网络的能力，对抗性示例（AEs）已经被广泛研究。然而，使目标AE在未知模型之间可传递仍然具有挑战性。在本文中，为了减轻现有简单迭代攻击所生成的AE中常见的过拟合困境，我们提出在特征空间中对其进行微调。具体而言，我们从基线攻击生成的AE开始，在源模型的中间层中鼓励有助于目标类别的特征，阻碍有助于原始类别的特征。大量实验表明，仅使用几次微调即可显著和普遍地提高现有攻击的目标传递性。我们的结果还验证了简单的迭代攻击可以产生与资源密集型方法相当甚至更好的传递性，后者依赖于训练特定目标分类器或生成特定的AE方法。

    Adversarial examples (AEs) have been extensively studied due to their potential for privacy protection and inspiring robust neural networks. However, making a targeted AE transferable across unknown models remains challenging. In this paper, to alleviate the overfitting dilemma common in an AE crafted by existing simple iterative attacks, we propose fine-tuning it in the feature space. Specifically, starting with an AE generated by a baseline attack, we encourage the features that contribute to the target class and discourage the features that contribute to the original class in a middle layer of the source model. Extensive experiments demonstrate that only a few iterations of fine-tuning can boost existing attacks in terms of targeted transferability nontrivially and universally. Our results also verify that the simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods, which rely on training target-specific classifiers or generat
    
[^23]: 一种用于智能城市中环境多智能体系统的本体论

    Une ontologie pour les syst{\`e}mes multi-agents ambiants dans les villes intelligentes. (arXiv:2401.02726v1 [cs.AI])

    [http://arxiv.org/abs/2401.02726](http://arxiv.org/abs/2401.02726)

    该论文提出了一种用于智能城市中环境多智能体系统的本体论，用于描述对象基础设施的语义结构，以便利用连接设备提供个性化服务。本体论在智能可移动性领域得到了应用，并且可适用于其他智能城市领域。

    

    目前，城市正在采购各种连接设备，以便将其转变为“智能城市”。为了管理这些连接对象的大量数据，可以将称为智能体的自主软件实体附加到它们上，以协作并利用这些设备提供个性化服务。然而，这种对象基础设施需要语义结构化才能被利用。因此，本文提出的是一种格式化为OWL的本体论，描述了对象基础设施，它们与多智能体系统的组织之间的关联以及根据系统用户的需求提供的服务。该本体论在智能可移动性领域应用于减少流动性的人群，并且可以应用到其他智能城市领域。

    Towns and cities are currently equipping themselves with a host of connected devices, with a view to transforming themselves into ''smart cities''. To manage this mass of connected objects, autonomous software entities, known as agents, can be attached to them to cooperate and use these devices to offer personalized services. However, this object infrastructure needs to be semantically structured in order to be exploited. This is why the proposal of this article is an ontology, formatted in OWL, describing the object infrastructures, their links with the organization of the multi-agent system and the services to be delivered according to the users of the system. The ontology is applied to smart mobility for people with reduced mobility, and could be adapted to other smart city axes.
    
[^24]: 从不成对的真实数据中学习图像去莫尔问题

    Learning Image Demoireing from Unpaired Real Data. (arXiv:2401.02719v1 [cs.CV])

    [http://arxiv.org/abs/2401.02719](http://arxiv.org/abs/2401.02719)

    本文提出了一种从不成对的真实数据中学习图像去莫尔问题的方法，通过合成与清晰图像对应的伪莫尔图像，并实现了多样化莫尔特征和类似真实无莫尔图像的细节，最终去除图像中的莫尔效应。

    

    本文专注于解决图像去莫尔问题。与现有大量研究依赖于成对真实数据学习的情况不同，我们尝试从不成对的真实数据中学习莫尔问题模型，即关联到不相关清晰图像的莫尔图像。提出的方法称为无对齐去莫尔（Unpaired Demoireing，UnDeM），从不成对数据集中合成伪莫尔图像，为训练去莫尔模型生成与清晰图像对应的图像对。为了实现这一目标，我们将真实莫尔图像分成小块并按照莫尔复杂程度进行分组。我们引入了一种新的莫尔生成框架，以合成具有多样化莫尔特征的莫尔图像，类似于真实的莫尔块，并具有类似真实无莫尔图像的细节。此外，我们引入了自适应去噪方法，以消除低质量的伪莫尔图像对去莫尔模型的学习产生的不利影响。我们在常用的FHDMi和UHDM数据集上进行了广泛的实验。

    This paper focuses on addressing the issue of image demoireing. Unlike the large volume of existing studies that rely on learning from paired real data, we attempt to learn a demoireing model from unpaired real data, i.e., moire images associated with irrelevant clean images. The proposed method, referred to as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from unpaired datasets, generating pairs with clean images for training demoireing models. To achieve this, we divide real moire images into patches and group them in compliance with their moire complexity. We introduce a novel moire generation framework to synthesize moire images with diverse moire features, resembling real moire patches, and details akin to real moire-free images. Additionally, we introduce an adaptive denoise method to eliminate the low-quality pseudo moire images that adversely impact the learning of demoireing models. We conduct extensive experiments on the commonly-used FHDMi and UHDM datasets. R
    
[^25]: 多模态医学影像分割的互补信息相互学习

    Complementary Information Mutual Learning for Multimodality Medical Image Segmentation. (arXiv:2401.02717v1 [cs.CV])

    [http://arxiv.org/abs/2401.02717](http://arxiv.org/abs/2401.02717)

    这篇论文介绍了一种称为互补信息相互学习（CIML）的框架，在多模态医学影像分割中解决了模态间冗余信息的负面影响。通过采用加法和任务分解的方法，CIML成功地消除了冗余信息，提高了分割的准确性。

    

    由于医学影像的局限性和肿瘤信号的多样性，放射科医生必须利用多模态图像进行肿瘤分割和诊断。这导致了多模态学习在分割中的发展。然而，模态之间的冗余性给现有的基于减法的联合学习方法带来了挑战，例如错误判断模态的重要性，忽视特定的模态信息，以及增加认知负荷。这些棘手的问题最终降低了分割的准确性并增加了过拟合的风险。本文提出了互补信息相互学习（CIML）框架，可以对模态间冗余信息的负面影响进行数学建模和解决。CIML采用了加法的思想，并通过归纳偏置驱动的任务分解和基于消息传递的冗余性过滤来消除模态间的冗余信息。CIML将多模态分割任务首先分解为多个子任务

    Radiologists must utilize multiple modal images for tumor segmentation and diagnosis due to the limitations of medical imaging and the diversity of tumor signals. This leads to the development of multimodal learning in segmentation. However, the redundancy among modalities creates challenges for existing subtraction-based joint learning methods, such as misjudging the importance of modalities, ignoring specific modal information, and increasing cognitive load. These thorny issues ultimately decrease segmentation accuracy and increase the risk of overfitting. This paper presents the complementary information mutual learning (CIML) framework, which can mathematically model and address the negative impact of inter-modal redundant information. CIML adopts the idea of addition and removes inter-modal redundant information through inductive bias-driven task decomposition and message passing-based redundancy filtering. CIML first decomposes the multimodal segmentation task into multiple subta
    
[^26]: 通过结构知识提炼进行图级蛋白质表示学习

    Graph-level Protein Representation Learning by Structure Knowledge Refinement. (arXiv:2401.02713v1 [cs.LG])

    [http://arxiv.org/abs/2401.02713](http://arxiv.org/abs/2401.02713)

    本文提出了一种名为结构知识提炼（SKR）的新框架，通过对比学习解决了图级表示学习中存在的问题，并提出了一种自适应的数据增强策略。

    

    本文关注以非监督方式在整个图级别上学习表示。学习图级表示在诸如分子属性预测、蛋白质结构特征提取和社交网络分析等各种实际问题中起着重要作用。主流方法是利用对比学习促进图特征提取，称为图对比学习（GCL）。尽管GCL有效，但在对比学习中存在一些复杂问题，比如虚假负样本对的影响。此外，GCL中的数据增强策略对不同的图数据集适应性较弱。受到这些问题的启发，我们提出了一种新的框架，称为结构知识提炼（SKR），它利用数据结构确定一对样本是正样本还是负样本的概率。同时，我们提出了一种增强策略，能够自然地保留原始数据的语义含义，并且与我们的SKR框架兼容。

    This paper focuses on learning representation on the whole graph level in an unsupervised manner. Learning graph-level representation plays an important role in a variety of real-world issues such as molecule property prediction, protein structure feature extraction, and social network analysis. The mainstream method is utilizing contrastive learning to facilitate graph feature extraction, known as Graph Contrastive Learning (GCL). GCL, although effective, suffers from some complications in contrastive learning, such as the effect of false negative pairs. Moreover, augmentation strategies in GCL are weakly adaptive to diverse graph datasets. Motivated by these problems, we propose a novel framework called Structure Knowledge Refinement (SKR) which uses data structure to determine the probability of whether a pair is positive or negative. Meanwhile, we propose an augmentation strategy that naturally preserves the semantic meaning of the original data and is compatible with our SKR frame
    
[^27]: 基于强化学习的量化交易中协同公式Alpha生成的方法

    Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning. (arXiv:2401.02710v1 [cs.CE])

    [http://arxiv.org/abs/2401.02710](http://arxiv.org/abs/2401.02710)

    本文提出了一种基于强化学习的方法，通过扩展搜索空间和利用预训练的公式化alpha集来生成协同公式化alpha因子，从而提高量化交易的性能。

    

    公式化alpha因子的挖掘是指在股票市场中发现和开发特定的因子或指标（称为alpha因子）以用于量化交易的过程。为了在庞大的搜索空间中高效地发现alpha因子，常常采用强化学习（RL）方法。本文提出了一种通过扩展搜索空间并利用预训练的公式化alpha集作为初始种子值来生成协同公式化alpha的方法。我们使用信息系数（IC）和排名信息系数（Rank IC）作为模型的性能评估指标。通过使用CSI300市场数据，我们进行了实际投资模拟，并观察到与现有技术相比的显著性能提升。

    Mining of formulaic alpha factors refers to the process of discovering and developing specific factors or indicators (referred to as alpha factors) for quantitative trading in stock market. To efficiently discover alpha factors in vast search space, reinforcement learning (RL) is commonly employed. This paper proposes a method to enhance existing alpha factor mining approaches by expanding a search space and utilizing pretrained formulaic alpha set as initial seed values to generate synergistic formulaic alpha. We employ information coefficient (IC) and rank information coefficient (Rank IC) as performance evaluation metrics for the model. Using CSI300 market data, we conducted real investment simulations and observed significant performance improvement compared to existing techniques.
    
[^28]: 德语文本嵌入聚类基准研究

    German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])

    [http://arxiv.org/abs/2401.02709](http://arxiv.org/abs/2401.02709)

    本研究提出了一个评估不同领域中聚类德语文本嵌入性能的基准测试，并对预训练模型和聚类算法进行了初步分析。结果显示单语和多语模型表现强劲，缩减嵌入维度可以提高聚类效果。此外，持续预训练德语BERT模型可能对短文本性能有显著改善。所有代码和数据集均公开可用。

    

    本研究介绍了一个基准测试，用于评估在不同领域中聚类德语文本嵌入的性能。这个基准测试是由对聚类神经文本嵌入在需要对文本进行分组（如主题建模）的任务中的增加使用所驱动的，并且在现有基准测试中需要德语资源。我们对一系列预训练的单语和多语模型在不同聚类算法的结果上进行了初步分析。结果包括表现强劲的单语和多语模型。缩减嵌入的维度可以进一步提高聚类效果。此外，我们还进行了与德语BERT模型的持续预训练的实验，以评估这种额外训练的好处。我们的实验表明，对于短文本可能存在显著的性能改进。所有代码和数据集均可公开获取。

    This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains. This benchmark is driven by the increasing use of clustering neural text embeddings in tasks that require the grouping of texts (such as topic modeling) and the need for German resources in existing benchmarks. We provide an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms. Results include strong performing mono- and multilingual models. Reducing the dimensions of embeddings can further improve clustering. Additionally, we conduct experiments with continued pre-training for German BERT models to estimate the benefits of this additional training. Our experiments suggest that significant performance improvements are possible for short text. All code and datasets are publicly available.
    
[^29]: TripleSurv：适应时间三元组坐标损失用于生存分析

    TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis. (arXiv:2401.02708v1 [cs.LG])

    [http://arxiv.org/abs/2401.02708](http://arxiv.org/abs/2401.02708)

    本文提出了一种适应时间的三元组坐标损失函数TripleSurv，通过引入样本对之间的生存时间差异来鼓励模型量化排名相对风险，从而提高生存分析的准确性。

    

    生存分析中的一个核心挑战是对被截尾的事件时间数据进行建模，其中感兴趣的事件可能是死亡、失败或特定事件的发生。过去的研究表明，排序损失和最大似然估计（MLE）损失函数被广泛应用于生存分析。然而，排序损失仅关注生存时间排名，不考虑样本对于确切生存时间值的潜在影响。此外，MLE是无界的且容易受到异常值（例如，被截尾数据）的影响，这可能导致建模性能较差。为了处理学习过程的复杂性并利用有价值的生存时间值，我们提出了一种适应时间三元组坐标损失函数TripleSurv，通过将样本对之间的生存时间差异引入排序中，以鼓励模型量化排名相对风险，最终提高预测准确性。

    A core challenge in survival analysis is to model the distribution of censored time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation (MLE)loss functions are widely-used for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples for exact survival time values. Furthermore, the MLE is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predi
    
[^30]: XUAT-Copilot: 多智能体协作系统用于使用大型语言模型进行自动用户验收测试

    XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model. (arXiv:2401.02705v1 [cs.AI])

    [http://arxiv.org/abs/2401.02705](http://arxiv.org/abs/2401.02705)

    XUAT-Copilot是一个使用大型语言模型的多智能体协作系统，旨在提高自动用户验收测试的自动化水平和测试脚本生成阶段的效率。

    

    过去几年，我们致力于自动化微信支付的用户验收测试（UAT）过程，这是中国最具影响力的移动支付应用之一。我们开发了一个名为XUAT的系统用于这个目的。然而，在当前系统中仍然存在一个人力密集阶段，即测试脚本的生成。因此，在本文中，我们集中研究提高当前系统自动化水平的方法，特别是测试脚本生成阶段。近年来，大型语言模型（LLMs）取得了显著的成功，展示了获取人类智能的潜力，并且已经形成了一个不断增长的研究领域，使用LLMs作为自主智能体来获得类似人类的决策能力。受到这些工作的启发，我们提出了一个基于LLM的多智能体协作系统，名为XUAT-Copilot，用于自动UAT。该系统主要由三个以LLM为基础的智能体组成，负责动作规划、状态检测和...

    In past years, we have been dedicated to automating user acceptance testing (UAT) process of WeChat Pay, one of the most influential mobile payment applications in China. A system titled XUAT has been developed for this purpose. However, there is still a human-labor-intensive stage, i.e, test scripts generation, in the current system. Therefore, in this paper, we concentrate on methods of boosting the automation level of the current system, particularly the stage of test scripts generation. With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities. Inspired by these works, we propose an LLM-powered multi-agent collaborative system, named XUAT-Copilot, for automated UAT. The proposed system mainly consists of three LLM-based agents responsible for action planning, state checking and pa
    
[^31]: 验证关联解释:一种概率方法

    Verifying Relational Explanations: A Probabilistic Approach. (arXiv:2401.02703v1 [cs.AI])

    [http://arxiv.org/abs/2401.02703](http://arxiv.org/abs/2401.02703)

    本文提出了一种概率方法，用于验证关联数据上的解释。通过在原始数据上生成对称近似的反事实示例，学习了一个因子图模型来量化解释的不确定性。实验证明该方法可以帮助验证解释的质量。

    

    在关联数据上解释是很难验证的，因为解释结构更加复杂（例如图形）。为了验证可解释的解释（例如在图像、文本等中做出的预测的解释），通常使用人类主体，因为这不一定需要很多专业知识。然而，验证关联解释的质量需要专业知识，并且很难规模化。GNNExplainer可以说是图神经网络解释方法中最受欢迎的方法之一。在本文中，我们开发了一种方法，用于评估GNNExplainer生成的解释中的不确定性。具体而言，我们要求解释器为几个反事实示例生成解释。我们将这些示例生成为原始数据中关联结构的对称近似。从这些解释中，我们学习了一个因子图模型来量化解释中的不确定性。我们在几个数据集上的结果表明，我们的方法可以帮助验证解释的质量。

    Explanations on relational data are hard to verify since the explanation structures are more complex (e.g. graphs). To verify interpretable explanations (e.g. explanations of predictions made in images, text, etc.), typically human subjects are used since it does not necessarily require a lot of expertise. However, to verify the quality of a relational explanation requires expertise and is hard to scale-up. GNNExplainer is arguably one of the most popular explanation methods for Graph Neural Networks. In this paper, we develop an approach where we assess the uncertainty in explanations generated by GNNExplainer. Specifically, we ask the explainer to generate explanations for several counterfactual examples. We generate these examples as symmetric approximations of the relational structure in the original data. From these explanations, we learn a factor graph model to quantify uncertainty in an explanation. Our results on several datasets show that our approach can help verify explanati
    
[^32]: 用于3D分子生成的几何便利去噪扩散模型

    Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])

    [http://arxiv.org/abs/2401.02683](http://arxiv.org/abs/2401.02683)

    这项研究提出了一个几何便利的去噪扩散模型，用于解决3D分子生成中的多体原子关系建模和键的预测问题。

    

    去噪扩散模型在多个研究领域显示出巨大潜力。现有的面向全新3D分子生成的扩散基于生成方法面临两个主要挑战。由于分子中的大多数重原子通过单键与多个原子相连，仅使用成对距离来模拟分子几何是不足的。因此，第一个挑战涉及提出一个能够捕捉复杂的多体原子间关系和学习高质量特征的有效神经网络作为去噪内核。由于图的离散性质，面对分子的主流扩散方法严重依赖预定义规则，并以间接方式生成边缘。第二个挑战涉及将分子的生成与扩散相结合，并准确预测键的存在。在我们的研究中，我们认为在扩散过程中更新分子构型的迭代方式与分子动力学一致，并引入了新的方法来解决两个挑战。

    Denoising diffusion models have shown great potential in multiple research areas. Existing diffusion-based generative methods on de novo 3D molecule generation face two major challenges. Since majority heavy atoms in molecules allow connections to multiple atoms through single bonds, solely using pair-wise distance to model molecule geometries is insufficient. Therefore, the first one involves proposing an effective neural network as the denoising kernel that is capable to capture complex multi-body interatomic relationships and learn high-quality features. Due to the discrete nature of graphs, mainstream diffusion-based methods for molecules heavily rely on predefined rules and generate edges in an indirect manner. The second challenge involves accommodating molecule generation to diffusion and accurately predicting the existence of bonds. In our research, we view the iterative way of updating molecule conformations in diffusion process is consistent with molecular dynamics and introd
    
[^33]: 逐层损失稳定扩散XL的渐进式知识蒸馏

    Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])

    [http://arxiv.org/abs/2401.02677](http://arxiv.org/abs/2401.02677)

    本研究通过逐层损失的渐进式削减，引入了两个精简变体Segmind稳定扩散（SSD-1B）和Segmind-Vega，分别使用1.3B和0.74B参数的UNet结构实现。这些精简模型通过转移学习有效地模拟了原始的SDXL，并在与更大的SDXL模型相比的竞争中取得了有竞争力的结果。

    

    稳定扩散XL（SDXL）已成为非常多功能且质量卓越的开源文本到图像模型（T2I）。高效地解决SDXL模型的计算需求对于扩大其应用范围至关重要。在本研究中，我们通过逐层损失的渐进式削减引入了两个精简变体，即Segmind稳定扩散（SSD-1B）和Segmind-Vega，分别使用13亿和0.74亿参数的UNet结构实现，并注重减小模型大小同时保持生成质量。我们在https://hf.co/Segmind上发布了这些模型的权重。我们的方法包括从SDXL的U-Net结构中消除残差网络和Transformer块，从而显著减少参数和延迟。我们的精简模型通过利用转移学习能够有效地模拟原始的SDXL，并在与更大的千亿参数级SDXL相比的竞争中取得了有竞争力的结果。我们的工作强调了知识蒸馏的有效性。

    Stable Diffusion XL (SDXL) has become the best open source text-to-image model (T2I) for its versatility and top-notch image quality. Efficiently addressing the computational demands of SDXL models is crucial for wider reach and applicability. In this work, we introduce two scaled-down variants, Segmind Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter UNets, respectively, achieved through progressive removal using layer-level losses focusing on reducing the model size while preserving generative quality. We release these models weights at https://hf.co/Segmind. Our methodology involves the elimination of residual networks and transformer blocks from the U-Net structure of SDXL, resulting in significant reductions in parameters, and latency. Our compact models effectively emulate the original SDXL by capitalizing on transferred knowledge, achieving competitive results against larger multi-billion parameter SDXL. Our work underscores the efficacy of knowledge dis
    
[^34]: 一个统一的多通道远场语音识别系统：将神经波束形成与基于注意力的端到端模型相结合

    A unified multichannel far-field speech recognition system: combining neural beamforming with attention based end-to-end model. (arXiv:2401.02673v1 [eess.AS])

    [http://arxiv.org/abs/2401.02673](http://arxiv.org/abs/2401.02673)

    本文提出了一个统一的多通道远场语音识别系统，将神经波束形成和基于注意力的端到端模型相结合，并进行了联合训练。使用分解复线性投影来形成神经波束形成，并探索源方向作为先验知识的有用性。

    

    远场语音识别是一个具有挑战性的任务，传统上使用信号处理波束形成来解决噪声和干扰问题。但是由于对环境假设的严重依赖，性能通常受到限制。本文提出了一个统一的多通道远场语音识别系统，该系统结合了神经波束形成和基于变压器的Listen, Spell, Attend (LAS)语音识别系统，进一步扩展了端到端语音识别系统，包括语音增强。然后，这个框架被共同训练以优化最终的目标。具体地，采用分解复线性投影(fCLP)来形成神经波束形成。然后比较了几种合并方向的汇聚策略，以找到最优的方法。此外，源方向的信息也被集成到波束形成中，探索源方向作为先验知识的有用性，这通常是不常见的。

    Far-field speech recognition is a challenging task that conventionally uses signal processing beamforming to attack noise and interference problem. But the performance has been found usually limited due to heavy reliance on environmental assumption. In this paper, we propose a unified multichannel far-field speech recognition system that combines the neural beamforming and transformer-based Listen, Spell, Attend (LAS) speech recognition system, which extends the end-to-end speech recognition system further to include speech enhancement. Such framework is then jointly trained to optimize the final objective of interest. Specifically, factored complex linear projection (fCLP) has been adopted to form the neural beamforming. Several pooling strategies to combine look directions are then compared in order to find the optimal approach. Moreover, information of the source direction is also integrated in the beamforming to explore the usefulness of source direction as a prior, which is usuall
    
[^35]: 零样本深度学习实现微气候预测

    Zero-shot Microclimate Prediction with Deep Learning. (arXiv:2401.02665v1 [cs.LG])

    [http://arxiv.org/abs/2401.02665](http://arxiv.org/abs/2401.02665)

    该论文提出了一种零样本学习方法，利用从其他地理位置提取的知识，实现了对新的和未监测到的地点的各种气候测量的预测。

    

    天气站数据是气候预测的宝贵资源，但在偏远地区其可靠性可能受限。此外，进行本地预测通常依赖于可能无法访问的传感器数据，这对于新的、以前未监测到的地点来说尤其困难。为了应对这些挑战，我们提出了一种新颖的零样本学习方法，旨在预测新的、未监测到的地点的各种气候测量。我们的方法通过利用从其他地理位置提取的知识，超越了传统的天气预报技术，预测微气候变量。

    Weather station data is a valuable resource for climate prediction, however, its reliability can be limited in remote locations. To compound the issue, making local predictions often relies on sensor data that may not be accessible for a new, previously unmonitored location. In response to these challenges, we propose a novel zero-shot learning approach designed to forecast various climate measurements at new and unmonitored locations. Our method surpasses conventional weather forecasting techniques in predicting microclimate variables by leveraging knowledge extracted from other geographic locations.
    
[^36]: 用于图神经网络链接预测任务的后门攻击

    A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])

    [http://arxiv.org/abs/2401.02663](http://arxiv.org/abs/2401.02663)

    本文研究了一种针对图神经网络链接预测任务的后门攻击方法，发现GNN模型容易受到后门攻击，提出了针对该任务的后门攻击方式。

    

    图神经网络（GNN）是一类能够处理图结构数据的深度学习模型，在各种实际应用中表现出显著的性能。最近的研究发现，GNN模型容易受到后门攻击。当具体的模式（称为后门触发器，例如子图、节点等）出现在输入数据中时，嵌入在GNN模型中的后门会被激活，将输入数据误分类为攻击者指定的目标类标签，而当输入中没有后门触发器时，嵌入在GNN模型中的后门不会被激活，模型正常工作。后门攻击具有极高的隐蔽性，给GNN模型带来严重的安全风险。目前，对GNN的后门攻击研究主要集中在图分类和节点分类等任务上，对链接预测任务的后门攻击研究较少。在本文中，我们提出一种后门攻击方法。

    Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor a
    
[^37]: 护士参与型人工智能在临床试验中针对2型糖尿病的精准管理：利用转移学习的预测数字孪生体

    Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin. (arXiv:2401.02661v1 [cs.LG])

    [http://arxiv.org/abs/2401.02661](http://arxiv.org/abs/2401.02661)

    这项研究开发了一种护士参与型人工智能系统，利用转移学习的预测数字孪生体来实现针对2型糖尿病患者的精准管理。在临床试验中，该系统通过结合各种数据源的模式和护士的专业知识，提供个性化的治疗反馈，以改善患者的治疗效果。

    

    背景：2型糖尿病（T2D）是一种常见的慢性疾病，其会增加严重健康并对生活质量产生负面影响。考虑到个体特征和生活方式对治疗计划和患者结果的影响，开发精确和个性化的管理策略至关重要。人工智能（AI）结合来自各种数据源的模式和护士的专业知识，可以实现最佳护理效果。方法：这是一项针对T2D患者（n = 20，年龄= 57+-10）的为期6个月的附属研究。参与者随机分配为干预组（AI，n = 10）和对照组，在最后三个月中干预组每天接收AI生成的个体化反馈，而对照组不接收每日反馈（非AI，n = 10）。研究开发了一种在线护士参与型预测控制模型（ONLC），该模型利用预测数字孪生体（PDT）。PDT是通过基于转移学习的人工神经网络训练的。

    Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a significant risk of serious health complications and negative impacts on the quality of life. Given the impact of individual characteristics and lifestyle on the treatment plan and patient outcomes, it is crucial to develop precise and personalized management strategies. Artificial intelligence (AI) provides great promise in combining patterns from various data sources with nurses' expertise to achieve optimal care. Methods: This is a 6-month ancillary study among T2D patients (n = 20, age = 57 +- 10). Participants were randomly assigned to an intervention (AI, n=10) group to receive daily AI-generated individualized feedback or a control group without receiving the daily feedback (non-AI, n=10) in the last three months. The study developed an online nurse-in-the-loop predictive control (ONLC) model that utilizes a predictive digital twin (PDT). The PDT was developed using a transfer-learning-based Artificial Neura
    
[^38]: EV在智能电网中的需求响应智能调度的基于深度强化学习的研究

    A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids. (arXiv:2401.02653v1 [cs.LG])

    [http://arxiv.org/abs/2401.02653](http://arxiv.org/abs/2401.02653)

    本研究提出了一种基于深度强化学习的智能调度方法，用于解决电动车在智能电网中的需求响应问题。通过调度电动车的充放电活动，以与配电系统操作员提供的目标能量配置文件一致，可以实现局部网络的平衡和优化。

    

    经济和政策因素推动了电动车(EV)的不断增加和使用。然而，尽管EV是一种比燃油车更清洁的替代品，但由于电力需求增加和使用时间导致的负面影响，EV对微电网设备寿命和能源平衡产生了负面影响。在我们看来，电网管理应该利用EV的调度灵活性，通过积极参与需求响应计划来支持局部网络平衡。在本文中，我们提出了一种基于深度强化学习的无模型解决方案，用于将EV的充电和放电活动调度到微电网中，以与配电系统操作员提供的目标能量配置文件一致。我们改进了Bellman方程，通过特定的奖励评估状态的价值，使用神经网络估计可用动作的Q值，并使用epsilon-greedy算法平衡开发和探索。

    Economic and policy factors are driving the continuous increase in the adoption and usage of electrical vehicles (EVs). However, despite being a cleaner alternative to combustion engine vehicles, EVs have negative impacts on the lifespan of microgrid equipment and energy balance due to increased power demand and the timing of their usage. In our view grid management should leverage on EVs scheduling flexibility to support local network balancing through active participation in demand response programs. In this paper, we propose a model-free solution, leveraging Deep Q-Learning to schedule the charging and discharging activities of EVs within a microgrid to align with a target energy profile provided by the distribution system operator. We adapted the Bellman Equation to assess the value of a state based on specific rewards for EV scheduling actions and used a neural network to estimate Q-values for available actions and the epsilon-greedy algorithm to balance exploitation and explorati
    
[^39]: 自适应折扣训练时间攻击

    Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])

    [http://arxiv.org/abs/2401.02652](http://arxiv.org/abs/2401.02652)

    本研究展示了在存在环境动态和相对于受害者目标的非最优性时，即使目标行为无法被采纳，仍然可能进行C-TTA。我们开发了一种gammaDDPG算法来学习这种更强版本的C-TTA，并根据受害者当前的行为动态改变攻击策略规划时间。

    

    在强化学习解决方案中，训练时间攻击（TTAs）是最阴险的攻击之一，可以在学习到的行为中制造漏洞和后门。现在已经有了不仅仅是简单破坏的建设性TTAs（C-TTAs），攻击者可以强制训练RL agent（受害者）表现出特定的目标行为。然而，即使是最先进的C-TTAs也只针对那些如果不因环境动态的一个特定特征被利用，受害者本可以自然地采纳的目标行为。在这项工作中，我们展示了即使目标行为由于环境动态和相对于受害者目标的非最优性而无法采纳，C-TTA也是可能的。为了在这种情况下找到高效的攻击方法，我们开发了一种特定的DDPG算法，称为gammaDDPG，用于学习这种更强版本的C-TTA。gammaDDPG根据受害者当前的行为动态改变攻击策略规划时间。

    Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, where the attacker forces a specific, target behaviour upon a training RL agent (victim). However, even state-of-the-art C-TTAs focus on target behaviours that could be naturally adopted by the victim if not for a particular feature of the environment dynamics, which C-TTAs exploit. In this work, we show that a C-TTA is possible even when the target behaviour is un-adoptable due to both environment dynamics as well as non-optimality with respect to the victim objective(s). To find efficient attacks in this context, we develop a specialised flavour of the DDPG algorithm, which we term gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically alters the attack policy planning horizon based on the victim's current behaviour
    
[^40]: 简单的分层扩散规划

    Simple Hierarchical Planning with Diffusion. (arXiv:2401.02644v1 [cs.LG])

    [http://arxiv.org/abs/2401.02644](http://arxiv.org/abs/2401.02644)

    本研究引入了分层扩散规划(Hierarchical Diffuser)，结合了分层和基于扩散的规划的优点，提出了一种简单、快速且有效的规划方法。通过在较高层面上采用“跳跃”规划策略，我们的模型能够具有较大的感受野且计算成本较低，同时跳跃的子目标还能指导低层规划器，在微调阶段进一步提高方法的效果。在实验评估中，我们的方法在性能和效率方面表现出卓越的结果。

    

    基于扩散的生成方法在建模离线数据集中的轨迹方面已被证明是有效的。然而，它们常常面临计算挑战，并且在泛化能力方面可能不足，特别是在捕捉长时任务的时间抽象方面。为了克服这个问题，我们引入了分层扩散规划，这是一种简单、快速、但令人惊讶地有效的规划方法，结合了分层和基于扩散的规划的优点。我们的模型在较高层面上采用了“跳跃”规划策略，使其具有较大的感受野但计算成本较低 - 这对于基于扩散的规划方法来说是一个关键因素，我们已经经验性地验证了这一点。此外，跳跃的子目标指导我们的低层规划器，促进了一个微调阶段，并进一步提高了我们方法的效果。我们对标准离线强化学习基准进行了经验评估，展示了我们方法在性能和效率方面的优越表现。

    Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a "jumpy" planning strategy at the higher level, which allows it to have a larger receptive field but at a lower computational cost -- a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of 
    
[^41]: 基础模型的训练与部署系统：一项综合调研

    Training and Serving System of Foundation Models: A Comprehensive Survey. (arXiv:2401.02643v1 [cs.AI])

    [http://arxiv.org/abs/2401.02643](http://arxiv.org/abs/2401.02643)

    这篇论文是一项对基础模型的训练与部署系统进行综合调研的工作，讨论了在训练和部署过程中所面临的挑战和采用的高效策略。

    

    基础模型（例如ChatGPT、DALL-E、彭程Mind、PanGu-$\Sigma$）在自然语言处理和视觉识别等关键技术领域展现了非凡的性能，并成为人工通用智能的主流趋势。这导致越来越多的科技巨头投入大量人力和财力积极开发基础模型系统，推动了这些模型参数的持续增长。因此，这些模型的训练与部署面临着显著的挑战，包括大量的计算能力、内存消耗、带宽需求等。因此，采用高效的训练和部署策略变得尤为关键。许多研究人员积极探索并提出了有效的方法。因此，对它们进行综合调研对于系统开发者和研究人员至关重要。本文广泛探讨了基础模型训练和部署中采用的方法。

    Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-$\Sigma$) have demonstrated extraordinary performance in key technological areas, such as natural language processing and visual recognition, and have become the mainstream trend of artificial general intelligence. This has led more and more major technology giants to dedicate significant human and financial resources to actively develop their foundation model systems, which drives continuous growth of these models' parameters. As a result, the training and serving of these models have posed significant challenges, including substantial computing power, memory consumption, bandwidth demands, etc. Therefore, employing efficient training and serving strategies becomes particularly crucial. Many researchers have actively explored and proposed effective methods. So, a comprehensive survey of them is essential for system developers and researchers. This paper extensively explores the methods employed in training and serving fou
    
[^42]: 伪造社交媒体账户的特点和普遍性，使用的是AI生成的面孔

    Characteristics and prevalence of fake social media profiles with AI-generated faces. (arXiv:2401.02627v1 [cs.CY])

    [http://arxiv.org/abs/2401.02627](http://arxiv.org/abs/2401.02627)

    本文分析了使用AI生成的人脸作为头像的伪造社交媒体账户，发现它们用于传播诈骗、垃圾信息以及放大协同信息等不真实活动。通过开发一种有效的方法，作者估计使用GAN生成的面孔的账户普遍性下限在0.021%到0.044%之间，约为每日活跃账户10K个。

    

    最近人工智能生成模型的进步引发了对其可能创建出逼真伪造社交媒体账户的担忧，但缺乏实证证据。本文对使用生成对抗网络（GANs）生成的人脸作为头像的Twitter(X)账户进行了系统分析。我们提供了一个包含1,353个此类账户的数据集，并展示了它们用于传播诈骗、垃圾信息以及放大协同信息等不真实活动。通过利用GAN生成的面孔的一个特征——眼睛的一致位置，并与人工注释相结合，我们设计了一种有效的方法来识别野外中使用GAN生成的账户。将该方法应用于随机样本的活跃Twitter用户中，我们估计使用GAN生成的面孔的账户普遍性下限在0.021%到0.044%之间，约为每日活跃账户10K个。这些发现突显了多模式账户对于新兴威胁的重要性。

    Recent advancements in generative artificial intelligence (AI) have raised concerns about their potential to create convincing fake social media accounts, but empirical evidence is lacking. In this paper, we present a systematic analysis of Twitter(X) accounts using human faces generated by Generative Adversarial Networks (GANs) for their profile pictures. We present a dataset of 1,353 such accounts and show that they are used to spread scams, spam, and amplify coordinated messages, among other inauthentic activities. Leveraging a feature of GAN-generated faces -- consistent eye placement -- and supplementing it with human annotation, we devise an effective method for identifying GAN-generated profiles in the wild. Applying this method to a random sample of active Twitter users, we estimate a lower bound for the prevalence of profiles using GAN-generated faces between 0.021% and 0.044% -- around 10K daily active accounts. These findings underscore the emerging threats posed by multimod
    
[^43]: 3D生成AI中的进展与前景: 包括3D人体的技术概述

    Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human. (arXiv:2401.02620v1 [cs.AI])

    [http://arxiv.org/abs/2401.02620](http://arxiv.org/abs/2401.02620)

    3D生成AI领域的进展及前景。稳定的扩散方法、多视角一致性控制和逼真的人体模型为高一致性和逼真外观的3D模型的生成做出了贡献。神经网络基础的3D存储和渲染模型加速了神经渲染模型的效率和逼真度。大规模语言模型的多模态能力增加了对多模态场景生成的灵活性和创造力。

    

    当AI生成的文本和2D图像继续扩展其领域时，3D生成逐渐成为一个不可忽视的趋势。自2023年以来，大量的研究论文已在3D生成领域涌现。这种增长不仅包括3D物体的创造，还包括3D角色和动画生成的快速发展。几个关键因素促进了这一进展。稳定扩散中的增强保真度，结合保证多视角一致性和逼真人体模型（如SMPL-X）的控制方法，协同促进了具有显著一致性和接近逼真外观的3D模型的产生。基于神经网络的3D储存和渲染模型的进步(如Neural Radiance Fields (NeRF)和3D Gaussian Splatting (3DGS))加速了神经渲染模型的效率和逼真度。此外，大规模语言模型的多模态能力使语言生成与3D模型生成紧密结合，为多模态场景生成提供了更高的灵活性和创造力。

    While AI-generated text and 2D images continue to expand its territory, 3D generation has gradually emerged as a trend that cannot be ignored. Since the year 2023 an abundant amount of research papers has emerged in the domain of 3D generation. This growth encompasses not just the creation of 3D objects, but also the rapid development of 3D character and motion generation. Several key factors contribute to this progress. The enhanced fidelity in stable diffusion, coupled with control methods that ensure multi-view consistency, and realistic human models like SMPL-X, contribute synergistically to the production of 3D models with remarkable consistency and near-realistic appearances. The advancements in neural network-based 3D storing and rendering models, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have accelerated the efficiency and realism of neural rendered models. Furthermore, the multimodality capabilities of large language models have enabled language i
    
[^44]: 神经因果抽象

    Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])

    [http://arxiv.org/abs/2401.02602](http://arxiv.org/abs/2401.02602)

    本文提出了一种新的神经因果抽象方法，通过聚类变量和其域，用于解决真实因果推断任务中的挑战，并通过神经因果模型实现了学习和应用。

    

    人类理解世界中的因果关系以及将信息压缩成抽象概念的能力是人类智慧的两个标志性特征。这两个主题在文献中被统称为因果抽象理论同时进行研究。在实践中，如何在真实的因果推断任务中充分利用抽象理论仍然是一个开放的问题，因为真实机制是未知的，只有有限的数据可用。在本文中，我们通过对变量及其域进行聚类，开发了一种新的因果抽象家族。这种方法改进和概括了之前的抽象概念，以更好地适应Pearl的因果层次结构引发的个体因果分布。我们证明了在实际场景中通过神经因果模型（Xia等，2021）可以学得这样的抽象概念，从而能够利用深度学习技术解决各种具有挑战性的因果推断任务。

    The abilities of humans to understand the world in terms of cause and effect relationships, as well as to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem in the literature under the rubric of causal abstractions theory. In practice, it remains an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true mechanisms are unknown and only limited data is available. In this paper, we develop a new family of causal abstractions by clustering variables and their domains. This approach refines and generalizes previous notions of abstractions to better accommodate individual causal distributions that are spawned by Pearl's causal hierarchy. We show that such abstractions are learnable in practical settings through Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning toolkit to solve various challenging causal inference tasks -- iden
    
[^45]: 面向图像描述的面向对象后门攻击

    Object-oriented backdoor attack against image captioning. (arXiv:2401.02600v1 [cs.CV])

    [http://arxiv.org/abs/2401.02600](http://arxiv.org/abs/2401.02600)

    本文研究了图像描述模型的面向对象后门攻击，通过污染训练数据，并设计了一种面向对象的方法来制作污染样本。攻击后，模型在良性图像上表现正常，但对于污染图像，模型将生成与图像无关的句子。

    

    在对图像分类任务进行后门攻击已经被广泛研究并被证明成功的同时，对视觉语言模型进行后门攻击的研究还很少。本文通过污染训练数据，探索了对图像描述模型的后门攻击。假设攻击者完全访问训练数据集，但不能干预模型的构建或训练过程。具体而言，我们随机选择部分良性训练样本进行污染。然后，考虑到描述通常围绕图像中的对象展开，我们设计了一种面向对象的方法来制作污染样本，该方法旨在通过微小的像素值修改来修改当前检测到的对象区域的规模相应的修改数量。在使用经过污染数据训练的模型上，对于良性图像，攻击模型的行为正常，但对于污染图像，模型将生成与给定图像无关的句子。

    Backdoor attack against image classification task has been widely studied and proven to be successful, while there exist little research on the backdoor attack against vision-language models. In this paper, we explore backdoor attack towards image captioning models by poisoning training data. Assuming the attacker has total access to the training dataset, and cannot intervene in model construction or training process. Specifically, a portion of benign training samples is randomly selected to be poisoned. Afterwards, considering that the captions are usually unfolded around objects in an image, we design an object-oriented method to craft poisons, which aims to modify pixel values by a slight range with the modification number proportional to the scale of the current detected object region. After training with the poisoned data, the attacked model behaves normally on benign images, but for poisoned images, the model will generate some sentences irrelevant to the given image. The attack 
    
[^46]: 通过生成合成数据来实现对不均衡数据上的深度学习的最大后验比率的研究

    Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data. (arXiv:2401.02591v1 [cs.LG])

    [http://arxiv.org/abs/2401.02591](http://arxiv.org/abs/2401.02591)

    本研究提出了一种通过生成合成数据来平衡类别不平衡数据的技术，优先平衡信息丰富区域，并通过优化类别后验比率来最大化在正确的类别区域生成合成样本的概率。实验结果表明该技术在提升深度学习模型方面具有卓越性能。

    

    本研究检验了类别不平衡数据对深度学习模型的影响，并提出了一种通过生成少数类别的合成数据来平衡数据的技术。与基于随机过采样的方法不同，我们的方法优先平衡信息丰富区域，通过识别高熵样本。生成适当位置的合成数据可以提高机器学习算法的准确性和效率，而生成位置不当的合成数据可能导致更高的误分类率。我们介绍了一种通过优化类别后验比率来最大化在正确的类别区域生成合成样本的概率的算法。此外，为了保持数据拓扑，合成数据在每个少数类别样本的邻域内生成。我们在41个数据集上的实验结果表明了我们的技术在提升深度学习模型方面的卓越性能。

    This study examines the impact of class-imbalanced data on deep learning models and proposes a technique for data balancing by generating synthetic data for the minority class. Unlike random-based oversampling, our method prioritizes balancing the informative regions by identifying high entropy samples. Generating well-placed synthetic data can enhance machine learning algorithms accuracy and efficiency, whereas poorly-placed ones may lead to higher misclassification rates. We introduce an algorithm that maximizes the probability of generating a synthetic sample in the correct region of its class by optimizing the class posterior ratio. Additionally, to maintain data topology, synthetic data are generated within each minority sample's neighborhood. Our experimental results on forty-one datasets demonstrate the superior performance of our technique in enhancing deep-learning models.
    
[^47]: 利用逆离散小波变换在更高分辨率下识别4FGL不确定源

    Identification of 4FGL uncertain sources at Higher Resolutions with Inverse Discrete Wavelet Transform. (arXiv:2401.02589v1 [astro-ph.HE])

    [http://arxiv.org/abs/2401.02589](http://arxiv.org/abs/2401.02589)

    本文研究了从4FGL DR3不确定源中找到AGN候选源和识别BL Lac/FSRQ候选源的任务，提出了一种名为FDIDWT的新方法，通过逆离散小波变换（IDWT）的多分辨率分析和分形维数（FD）理论的相关特征估计，将原始数据转换成低维度和突出特征的数据集。

    

    在大型天文数据时代来临之际，从地基和空间望远镜中找到目标源是一项负担。虽然机器学习（ML）方法已被广泛用于解决这个问题，但深入数据分析的整合可以显著提高处理大量天文数据时识别目标源的效率。本研究重点关注从4FGL DR3不确定源中找到AGN候选源和识别BL Lac/FSRQ候选源的任务。我们研究了4FGL DR3目录属性之间的相关性，并提出了一种名为FDIDWT的新方法以转换原始数据。转换后的数据集具有低维度和突出特征，通过分形维数（FD）理论的相关特征估计和逆离散小波变换（IDWT）的多分辨率分析。将FDIDWT方法与改进的轻量级MatchboxCon方法相结合，

    In the forthcoming era of big astronomical data, it is a burden to find out target sources from ground-based and space-based telescopes. Although Machine Learning (ML) methods have been extensively utilized to address this issue, the incorporation of in-depth data analysis can significantly enhance the efficiency of identifying target sources when dealing with massive volumes of astronomical data. In this work, we focused on the task of finding AGN candidates and identifying BL Lac/FSRQ candidates from the 4FGL DR3 uncertain sources. We studied the correlations among the attributes of the 4FGL DR3 catalogue and proposed a novel method, named FDIDWT, to transform the original data. The transformed dataset is characterized as low-dimensional and feature-highlighted, with the estimation of correlation features by Fractal Dimension (FD) theory and the multi-resolution analysis by Inverse Discrete Wavelet Transform (IDWT). Combining the FDIDWT method with an improved lightweight MatchboxCon
    
[^48]: 通过加速3D高斯粒子投影来表征卫星几何形状

    Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting. (arXiv:2401.02588v1 [cs.CV])

    [http://arxiv.org/abs/2401.02588](http://arxiv.org/abs/2401.02588)

    本文提出了一种基于3D高斯粒子投影的方法来映射卫星在轨道上的几何形状，能够在当前航天硬件上运行，并且在计算速度上比之前快近两个数量级，可以训练和渲染未知卫星的高质量新视角。

    

    卫星在轨道上的加速部署引起了对在轨维修（OOS）、卫星检查和主动清除碎片（ADR）的兴趣。这类任务需要在非合作的、可能是未知的驻留空间物体附近进行精确的交会和靠近操作。由于载人任务的安全问题和地面控制的延迟，必须实现完全自主化。这需要对目标的几何形状进行可靠的表征。本文提出了一种基于3D高斯粒子投影的卫星几何形状映射方法，可以在当前航天硬件上的计算资源上运行。我们在一个硬件环路卫星模拟装置上展示了模型训练和3D渲染性能，在几种逼真的光照和运动条件下。我们的模型显示出能够在机载上进行训练，并以比之前快近两个数量级的速度生成未知卫星的更高质量的新视角。

    The accelerating deployment of spacecraft in orbit have generated interest in on-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR). Such missions require precise rendezvous and proximity operations in the vicinity of non-cooperative, possible unknown, resident space objects. Safety concerns with manned missions and lag times with ground-based control necessitate complete autonomy. This requires robust characterization of the target's geometry. In this article, we present an approach for mapping geometries of satellites on orbit based on 3D Gaussian Splatting that can run on computing resources available on current spaceflight hardware. We demonstrate model training and 3D rendering performance on a hardware-in-the-loop satellite mock-up under several realistic lighting and motion conditions. Our model is shown to be capable of training on-board and rendering higher quality novel views of an unknown satellite nearly 2 orders of magnitude faster than previo
    
[^49]: 使用样本权重进行分布偏斜数据的联邦学习

    Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])

    [http://arxiv.org/abs/2401.02586](http://arxiv.org/abs/2401.02586)

    本论文研究了在分布偏斜数据情况下如何通过使用样本权重来改进联邦学习性能。主要思路是通过调整客户端分布使其更接近全局分布，从而实现机器学习模型更快地收敛和更高的准确性。

    

    联邦学习中最具挑战性的问题之一是数据通常不是独立同分布的（nonIID）。客户端被期望贡献相同类型的数据并从一个全局分布中抽取数据。然而，数据往往以不同的方式从不同资源收集。因此，客户端之间的数据分布可能与底层全局分布不同。这就产生了权重发散问题，并降低了联邦学习的性能。该工作侧重于改善客户端之间分布偏斜的联邦学习性能。主要思想是使用样本权重将客户端分布调整到全局分布更接近，从而使机器学习模型收敛更快且精度更高。我们从经验风险最小化的基本概念开始，从理论上推导出使用样本权重调整分布偏斜的解决方案。为了确定样本权重，我们隐含地交换...

    One of the most challenging issues in federated learning is that the data is often not independent and identically distributed (nonIID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces federated learning performance. This work focuses on improving federated learning performance for skewed data distribution across clients. The main idea is to adjust the client distribution closer to the global distribution using sample weights. Thus, the machine learning model converges faster with higher accuracy. We start from the fundamental concept of empirical risk minimization and theoretically derive a solution for adjusting the distribution skewness using sample weights. To determine sample weights, we implicitly exchan
    
[^50]: t-DGR: 一种基于轨迹的深度生成回放方法用于决策制定中的持续学习

    t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])

    [http://arxiv.org/abs/2401.02576](http://arxiv.org/abs/2401.02576)

    t-DGR是一种用于决策制定中持续学习的基于轨迹的深度生成回放方法，通过生成任务样本来解决灾难性遗忘问题，并在连续世界基准测试中取得了最先进的性能。

    

    深度生成回放已经成为决策制定中持续学习的一种有希望的方法。该方法通过利用从以前遇到的任务生成轨迹来增加当前数据集，解决了灾难性遗忘的问题。然而，现有的深度生成回放方法依赖于自回归模型，在生成的轨迹中会出现累积误差。在本文中，我们提出了一种简单、可扩展且非自回归的方法，用于决策制定中的持续学习，使用生成模型根据轨迹时间步生成任务样本。我们在连续世界基准测试中评估了我们的方法，并发现在持续学习方法中，我们的方法在平均成功率指标上达到了最先进的性能。代码可在https://github.com/WilliamYue37/t-DGR找到。

    Deep generative replay has emerged as a promising approach for continual learning in decision-making tasks. This approach addresses the problem of catastrophic forgetting by leveraging the generation of trajectories from previously encountered tasks to augment the current dataset. However, existing deep generative replay methods for continual learning rely on autoregressive models, which suffer from compounding errors in the generated trajectories. In this paper, we propose a simple, scalable, and non-autoregressive method for continual learning in decision-making tasks using a generative model that generates task samples conditioned on the trajectory timestep. We evaluate our method on Continual World benchmarks and find that our approach achieves state-of-the-art performance on the average success rate metric among continual learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
    
[^51]: 社交网络中的大型语言模型：应用、挑战和解决方案

    Large Language Models for Social Networks: Applications, Challenges, and Solutions. (arXiv:2401.02575v1 [cs.SI])

    [http://arxiv.org/abs/2401.02575](http://arxiv.org/abs/2401.02575)

    本研究调查了如何在在线社交网络中开发大型语言模型（LLMs）应用。我们将LLM应用分为知识任务、娱乐任务和基础任务，并分享了挑战、解决方案和经验教训。

    

    大型语言模型（LLMs）正在改变人们生成、探索和参与内容的方式。本研究探讨如何在在线社交网络中开发LLM应用。尽管LLMs在其他领域取得了成功，但在社交网络中开发基于LLM的产品面临着许多挑战，并且在研究界中相对较少报道。我们将社交网络中的LLM应用分为三类。第一类是知识任务，用户想要查找新知识和信息，例如搜索和问答。第二类是娱乐任务，用户想要消费有趣的内容，例如获取娱乐性通知内容。第三类是基础任务，需要进行社交网络的内容注释和LLM监控。针对每个任务，我们分享了我们发现的挑战、开发的解决方案和吸取的经验教训。据我们所知，这是第一个全面的研究关于在社交网络中应用LLMs的研究。

    Large Language Models (LLMs) are transforming the way people generate, explore, and engage with content. We study how we can develop LLM applications for online social networks. Despite LLMs' successes in other domains, it is challenging to develop LLM-based products for social networks for numerous reasons, and it has been relatively under-reported in the research community. We categorize LLM applications for social networks into three categories. First is knowledge tasks where users want to find new knowledge and information, such as search and question-answering. Second is entertainment tasks where users want to consume interesting content, such as getting entertaining notification content. Third is foundational tasks that need to be done to moderate and operate the social networks, such as content annotation and LLM monitoring. For each task, we share the challenges we found, solutions we developed, and lessons we learned. To the best of our knowledge, this is the first comprehensi
    
[^52]: 定量技术预测：趋势外推方法综述

    Quantitative Technology Forecasting: a Review of Trend Extrapolation Methods. (arXiv:2401.02549v1 [cs.AI])

    [http://arxiv.org/abs/2401.02549](http://arxiv.org/abs/2401.02549)

    本研究对定量技术预测中的趋势外推方法进行了系统回顾，发现增长曲线和时间序列方法是过去十年中最受欢迎的技术方法，而基于机器学习的混合模型则是近年的新兴方法。

    

    定量技术预测利用定量方法来理解和预测技术变化。这是一个广泛的领域，涵盖了许多不同的技术，并且已应用于广泛的技术范围。这个领域中广泛使用的方法之一是趋势外推。根据我们所得到的出版物，很少或没有系统地回顾定量趋势外推技术的经验证据。本研究试图填补这一空白，通过对涉及定量趋势外推技术应用的技术预测文献进行系统回顾。我们鉴别出了与本研究目标相关的25项研究，并将这些研究中使用的技术分类为不同的类别，其中增长曲线和时间序列方法在过去十年中仍然受到欢迎，而新的方法，如基于机器学习的混合模型，则在近年兴起。

    Quantitative technology forecasting uses quantitative methods to understand and project technological changes. It is a broad field encompassing many different techniques and has been applied to a vast range of technologies. A widely used approach in this field is trend extrapolation. Based on the publications available to us, there has been little or no attempt made to systematically review the empirical evidence on quantitative trend extrapolation techniques. This study attempts to close this gap by conducting a systematic review of technology forecasting literature addressing the application of quantitative trend extrapolation techniques. We identified 25 studies relevant to the objective of this research and classified the techniques used in the studies into different categories, among which growth curves and time series methods were shown to remain popular over the past decade, while newer methods, such as machine learning-based hybrid models, have emerged in recent years. As more 
    
[^53]: 一种基于社区检测和图神经网络的科学文献链接预测方法

    A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature. (arXiv:2401.02542v1 [cs.SI])

    [http://arxiv.org/abs/2401.02542](http://arxiv.org/abs/2401.02542)

    本研究提出了一种将社区检测和图神经网络相结合的创新方法，以增强科学文献网络中的链接预测能力。通过将社区检测的结果与图神经网络模型结合，我们突破了链接预测中的可扩展性和分辨率限制，提高了预测准确性。

    

    本研究介绍了一种创新的方法，将社区检测算法与图神经网络（GNN）模型相结合，以增强科学文献网络中的链接预测能力。我们特别关注利用Louvain社区检测算法揭示这些网络中的潜在社区结构，并将其纳入GNN架构中以预测潜在链接。我们的方法展示了在复杂网络中理解社区动态的重要性，并利用社区检测和GNN的优势来提高预测准确性。通过对代表科学合作和引文关系的二分图进行大量实验，我们的方法不仅突显了社区检测和GNN之间的协同作用，还解决了链接预测中的一些普遍挑战，如可扩展性和分辨率限制。结果表明，纳入社区级别信息可以显著提高链接预测的性能。

    This study introduces an innovative approach that integrates community detection algorithms with Graph Neural Network (GNN) models to enhance link prediction in scientific literature networks. We specifically focus on the utilization of the Louvain community detection algorithm to uncover latent community structures within these networks, which are then incorporated into GNN architectures to predict potential links. Our methodology demonstrates the importance of understanding community dynamics in complex networks and leverages the strengths of both community detection and GNNs to improve predictive accuracy. Through extensive experiments on bipartite graphs representing scientific collaborations and citations, our approach not only highlights the synergy between community detection and GNNs but also addresses some of the prevalent challenges in link prediction, such as scalability and resolution limits. The results suggest that incorporating community-level information can significant
    
[^54]: DISO：用于建模晶体材料中位错的领域本体

    DISO: A Domain Ontology for Modeling Dislocations in Crystalline Materials. (arXiv:2401.02540v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2401.02540](http://arxiv.org/abs/2401.02540)

    本文介绍了一种名为DISO的领域本体，用于建模晶体材料中的位错缺陷。该本体通过定义与线状缺陷相关的概念和关系，有助于理解位错行为并在位错动力学领域中具有广泛的应用价值。

    

    晶体材料（如金属和半导体）几乎始终包含一种特殊的缺陷类型，称为位错。这种缺陷决定了许多重要的材料性质，例如强度、断裂韧性或延展性。过去几年中，人们通过实验表征技术和模拟研究不同长度尺度下的位错行为投入了大量的工作。本文介绍了位错本体（DISO），它定义了晶体材料中与线状缺陷相关的概念和关系。我们使用自顶向下的方法开发了DISO，即首先定义位错领域中最通用的概念，然后对其进行特化。DISO通过符合W3C最佳实践的持久URL进行发布。我们还提供了DISO的两个潜在用例，以说明其在位错动力学领域的有用性。对本体的评估是在...

    Crystalline materials, such as metals and semiconductors, nearly always contain a special defect type called dislocation. This defect decisively determines many important material properties, e.g., strength, fracture toughness, or ductility. Over the past years, significant effort has been put into understanding dislocation behavior across different length scales via experimental characterization techniques and simulations. This paper introduces the dislocation ontology (DISO), which defines the concepts and relationships related to linear defects in crystalline materials. We developed DISO using a top-down approach in which we start defining the most general concepts in the dislocation domain and subsequent specialization of them. DISO is published through a persistent URL following W3C best practices for publishing Linked Data. Two potential use cases for DISO are presented to illustrate its usefulness in the dislocation dynamics domain. The evaluation of the ontology is performed in
    
[^55]: 合成数据生成的综合探索：一项调查

    Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])

    [http://arxiv.org/abs/2401.02524](http://arxiv.org/abs/2401.02524)

    本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。

    

    最近几年，机器学习在各个领域的应用变得越来越受欢迎。然而，由于数据采集成本高昂和隐私法规的限制，训练数据的稀缺性阻碍了进展。合成数据成为一种解决方案，但发布的模型过多和有限的综述文献给决策带来了挑战。本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。确定了共同的特征，进行了分类和趋势分析。结果显示模型性能和复杂性增加，以基于神经网络的方法为主要趋势，除了隐私保护数据生成。计算机视觉占据主导地位，生成模型主要是GAN，而扩散模型、转换器和RNN也在竞争中。通过性能评估的结果显示了共同指标和数据集稀缺性的问题。

    Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
    
[^56]: 基于图像的深度学习用于智能数字孪生体: 一份综述

    Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])

    [http://arxiv.org/abs/2401.02523](http://arxiv.org/abs/2401.02523)

    本论文综述了基于图像的智能数字孪生体 (SDTs) 的发展方法和挑战，重点讨论了通过持续同化图像数据来观察和学习系统行为的方法，以及设计和实现SDTs的深度学习 (DL) 模型所面临的挑战。提供了对未来发展方向和机遇的见解。

    

    智能数字孪生体(SDTs)通过持续数据同化来虚拟复制和预测复杂物理系统的行为，从而通过控制系统的行为来优化这些系统的性能。近年来，深度学习(DL)模型显著增强了SDTs的能力，尤其是在预测性维修、异常检测和优化等任务上。在许多领域，包括医学、工程和教育，在观察和学习系统行为和控制其行为方面，SDTs使用图像数据(基于图像的SDTs)。本文重点介绍了开发基于图像的SDTs的各种方法和相关挑战，包括持续同化来自物理系统的图像数据。本文还讨论了设计和实现SDTs的DL模型所涉及的挑战，包括数据获取、处理和解释。此外，还提供了对未来发展方向和机遇的见解。

    Smart Digital twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, deep learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe and learn system behaviors and control their behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges involved in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opport
    
[^57]: 在一维中为双曲和抛物型PDE引入移动时域估计器

    Moving-Horizon Estimators for Hyperbolic and Parabolic PDEs in 1-D. (arXiv:2401.02516v1 [eess.SY])

    [http://arxiv.org/abs/2401.02516](http://arxiv.org/abs/2401.02516)

    本文介绍了一种用于双曲和抛物型PDE的移动时域估计器，通过PDE反步法将难以解决的观测器PDE转化为可以明确解决的目标观测器PDE，从而实现了在实时环境下消除数值解观测器PDE的需求。

    

    对于PDE的观测器本身也是PDE。因此，使用这样的观测器产生实时估计是计算负担很重的。对于有限维和ODE系统，移动时域估计器（MHE）是一种操作符，其输出是状态估计，而输入是时域起始处的初始状态估计以及移动时间域内的测量输出和输入信号。在本文中，我们引入了用于解决PDE的MHE，以消除实时数值解观测器PDE的需求。我们使用PDE反步法实现了这一点，对于某些特定类别的双曲和抛物型PDE，它能够明确地产生移动时域状态估计。具体来说，为了明确地产生状态估计，我们使用了一个难以解决的观测器PDE的反步变换，将其转化为一个可以明确解决的目标观测器PDE。我们提出的MHE并不是新的观测器设计，而只是明确的MHE实现，它能够在移动时域内产生状态估计。

    Observers for PDEs are themselves PDEs. Therefore, producing real time estimates with such observers is computationally burdensome. For both finite-dimensional and ODE systems, moving-horizon estimators (MHE) are operators whose output is the state estimate, while their inputs are the initial state estimate at the beginning of the horizon as well as the measured output and input signals over the moving time horizon. In this paper we introduce MHEs for PDEs which remove the need for a numerical solution of an observer PDE in real time. We accomplish this using the PDE backstepping method which, for certain classes of both hyperbolic and parabolic PDEs, produces moving-horizon state estimates explicitly. Precisely, to explicitly produce the state estimates, we employ a backstepping transformation of a hard-to-solve observer PDE into a target observer PDE, which is explicitly solvable. The MHEs we propose are not new observer designs but simply the explicit MHE realizations, over a moving
    
[^58]: 使用神经算子的增益调度方法处理具有非线性循环的输运PDE

    Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation. (arXiv:2401.02511v1 [eess.SY])

    [http://arxiv.org/abs/2401.02511](http://arxiv.org/abs/2401.02511)

    本文介绍了使用神经算子进行增益调度的方法来处理具有非线性循环的输运PDE，并在局部实现了稳定。

    

    为了稳定PDE模型，控制律需要通过非线性算子将空间相关的增益映射到PDE函数系数上。当PDE是非线性的且"伪系数"函数依赖于状态时，增益调度非线性设计是处理非线性反馈设计的最简单方法。PDE回溯的增益调度版本利用在每个状态值处求解PDE得到的增益。但是在实时情况下进行这种PDE计算可能是困难的。最近引入的神经算子（NO）可以在每个状态值上快速且实时地训练，产生增益函数，而无需求解PDE。本文介绍了将NO用于GS-PDE回溯的方法。GS控制器假设状态变化缓慢，并且仅保证局部稳定性，即使对于ODE也是如此。我们通过"全核"方法和"仅增益"方法，确立了具有非线性循环的双曲型PDE的局部稳定性。

    To stabilize PDE models, control laws require space-dependent functional gains mapped by nonlinear operators from the PDE functional coefficients. When a PDE is nonlinear and its "pseudo-coefficient" functions are state-dependent, a gain-scheduling (GS) nonlinear design is the simplest approach to the design of nonlinear feedback. The GS version of PDE backstepping employs gains obtained by solving a PDE at each value of the state. Performing such PDE computations in real time may be prohibitive. The recently introduced neural operators (NO) can be trained to produce the gain functions, rapidly in real time, for each state value, without requiring a PDE solution. In this paper we introduce NOs for GS-PDE backstepping. GS controllers act on the premise that the state change is slow and, as a result, guarantee only local stability, even for ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear recirculation using both a "full-kernel" approach and the "gain-only" approa
    
[^59]: 记忆、意识和大型语言模型

    Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.02509](http://arxiv.org/abs/2401.02509)

    该论文研究了大型语言模型（LLM）与图尔文的记忆理论之间的对应关系，并提出了意识可能是一种基于这种对应关系的新兴能力的猜想。

    

    随着认知科学和大型语言模型（LLM）的发展，这两个不同领域之间的联系越来越多地被揭示出来。在这些联系的基础上，我们提出了一个猜想，即LLM和图尔文的记忆理论之间存在一种对偶关系。我们确定了图尔文的协同引发（SEM）检索模型和LLM中观察到的新兴能力之间的潜在对应关系，为我们的猜想提供了支持证据。此外，我们推测意识可能被认为是这种对偶性的一种新兴能力形式。我们还讨论了其他意识理论如何与我们的研究相交叉。

    With the development in cognitive science and Large Language Models (LLMs), increasing connections have come to light between these two distinct fields. Building upon these connections, we propose a conjecture suggesting the existence of a duality between LLMs and Tulving's theory of memory. We identify a potential correspondence between Tulving's synergistic ecphory model (SEM) of retrieval and the emergent abilities observed in LLMs, serving as supporting evidence for our conjecture. Furthermore, we speculate that consciousness may be considered a form of emergent ability based on this duality. We also discuss how other theories of consciousness intersect with our research.
    
[^60]: 论自动规划与调度中引入大型语言模型的前景

    On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS). (arXiv:2401.02500v1 [cs.AI])

    [http://arxiv.org/abs/2401.02500](http://arxiv.org/abs/2401.02500)

    本文研究了将大型语言模型（LLMs）与传统符号规划器集成的前景，并指出这种神经符号方法有望在解决复杂规划问题中发挥重要作用。

    

    自动规划与调度是人工智能领域中增长迅速的领域之一，其中大型语言模型（LLMs）的应用越来越受到关注。本文基于对126篇论文的综合回顾，研究了LLMs在解决规划问题的各个方面中独特应用的八个类别：语言翻译、计划生成、模型构建、多智能体规划、交互式规划、启发式优化、工具集成和脑启发式规划。针对每个类别，我们明确了考虑的问题和现有的空白。我们回顾的一个重要观点是，在与传统符号规划器集成时，LLMs的真正潜力得以发挥，这指向了一种有前景的神经符号方法。这种方法有效地将LLMs的生成能力与经典规划方法的精确性结合起来。通过综合现有文献的见解，我们强调了这种集成在解决复杂规划问题上的潜力。

    Automated Planning and Scheduling is among the growing areas in Artificial Intelligence (AI) where mention of LLMs has gained popularity. Based on a comprehensive review of 126 papers, this paper investigates eight categories based on the unique applications of LLMs in addressing various aspects of planning problems: language translation, plan generation, model construction, multi-agent planning, interactive planning, heuristics optimization, tool integration, and brain-inspired planning. For each category, we articulate the issues considered and existing gaps. A critical insight resulting from our review is that the true potential of LLMs unfolds when they are integrated with traditional symbolic planners, pointing towards a promising neuro-symbolic approach. This approach effectively combines the generative aspects of LLMs with the precision of classical planning methods. By synthesizing insights from existing literature, we underline the potential of this integration to address comp
    
[^61]: 可解释的时间序列模型用于综合下水道溢流的废水建模

    Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows. (arXiv:2401.02465v1 [cs.LG])

    [http://arxiv.org/abs/2401.02465](http://arxiv.org/abs/2401.02465)

    这篇论文讨论了气候变化对污水管理带来的复杂挑战，提出了一种基于可解释时间序列模型的废水建模方法，可以帮助预测关键水位点并改善废水管理和环境污染预防。

    

    气候变化对我们的社会提出了越来越复杂的挑战。洪水、野火或干旱等极端天气事件变得越来越频繁、突发且难以预见或应对。在这项工作中，我们特别解决了重雨事件导致下雨罐溢出以后，污水污染地表水体的问题。我们调查了最先进的可解释时间序列模型在如何预测这些临界水位点方面的作用，以便能够及时将多余的水重新分配到下水道网络中。我们的结果表明，现代时间序列模型可以为改善废水管理和预防下水道系统对环境的污染做出贡献。所有的代码和实验可以在我们的存储库中找到：https://github.com/TeodorChiaburu/RIWWER_TimeSeries。

    Climate change poses increasingly complex challenges to our society. Extreme weather events such as floods, wild fires or droughts are becoming more frequent, spontaneous and difficult to foresee or counteract. In this work we specifically address the problem of sewage water polluting surface water bodies after spilling over from rain tanks as a consequence of heavy rain events. We investigate to what extent state-of-the-art interpretable time series models can help predict such critical water level points, so that the excess can promptly be redistributed across the sewage network. Our results indicate that modern time series models can contribute to better waste water management and prevention of environmental pollution from sewer systems. All the code and experiments can be found in our repository: https://github.com/TeodorChiaburu/RIWWER_TimeSeries.
    
[^62]: 计算医疗中的数据中心基础模型：一项调查

    Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])

    [http://arxiv.org/abs/2401.02458](http://arxiv.org/abs/2401.02458)

    计算医疗中的数据中心基础模型是一项调查研究，为医疗工作流程的改进提供了基于数据的人工智能方法，并讨论了安全性、评估和与人类价值观的一致性。基于FM的分析有望提高患者结果和临床工作流程表现。

    

    作为一套新兴的人工智能技术，基础模型（FMs）的出现为计算医疗带来了一系列机遇。这些模型的交互性由预训练数据和人类指令引导，引发了一个数据中心的人工智能范式，强调更好的数据表征、质量和规模。在医疗人工智能领域，获取和处理高质量的临床数据记录一直是一个长期存在的挑战，涉及数据数量、注释、患者隐私和伦理等方面。在这项调查中，我们研究了FM时代的广泛数据中心方法（从模型预训练到推理），以改进医疗工作流程。我们讨论了人工智能安全性、评估以及与人类价值观的一致性的关键观点。最后，我们展望了基于FM的分析在医疗和医药领域不断发展的格局中提高患者结果和临床工作流程表现的前景。我们提供了一个最新的医疗清单。

    The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare
    
[^63]: eCIL-MU: 基于嵌入的逐类增量学习和机器取消学习

    eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning. (arXiv:2401.02457v1 [cs.LG])

    [http://arxiv.org/abs/2401.02457](http://arxiv.org/abs/2401.02457)

    eCIL-MU是一种基于嵌入技术的逐类增量学习和机器取消学习的非破坏性框架，可用于在动态环境中快速获取关于新类别的知识，并消除先前学习类别的影响。

    

    在动态环境中，可能会不断引入新的类别，或者需要对现有类别进行重新分类。逐类增量学习 (CIL) 用于在获取关于新类别的知识的同时，保留对先前学习类别的信息。为了适应重新分类，还可能需要消除相关类别对模型的影响。因此，我们在CIL中引入了基于类别的机器取消学习 (MU)。通常，MU方法倾向于耗时，并且可能会损害模型的性能。连续的取消学习请求可能导致灾难性遗忘。为解决这些问题，我们提出了一种非破坏性的基于嵌入技术的eCIL-MU框架，将数据映射到向量并存储在向量数据库中。我们的方法利用了CIL和MU任务之间的重叠来加速。实验证明了实现取消学习效果和数量级的能力。

    New categories may be introduced over time, or existing categories may need to be reclassified. Class incremental learning (CIL) is employed for the gradual acquisition of knowledge about new categories while preserving information about previously learned ones in such dynamic environments. It might also be necessary to also eliminate the influence of related categories on the model to adapt to reclassification. We thus introduce class-level machine unlearning (MU) within CIL. Typically, MU methods tend to be time-consuming and can potentially harm the model's performance. A continuous stream of unlearning requests could lead to catastrophic forgetting. To address these issues, we propose a non-destructive eCIL-MU framework based on embedding techniques to map data into vectors and then be stored in vector databases. Our approach exploits the overlap between CIL and MU tasks for acceleration. Experiments demonstrate the capability of achieving unlearning effectiveness and orders of mag
    
[^64]: 面向火灾管理的AI支持无人机系统研究综述

    A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management. (arXiv:2401.02456v1 [cs.LG])

    [http://arxiv.org/abs/2401.02456](http://arxiv.org/abs/2401.02456)

    本研究综述探讨了AI支持的无人机系统在火灾管理中的应用，从火灾前到主动火灾阶段再到火灾后，通过整合无人机和深度学习模型，提供了更有效的野火管理解决方案。

    

    野火已成为全球最具破坏性的自然灾害之一，给人类生命和森林野生动物造成了灾难性损失。最近，人工智能（AI）在野火中的应用，通过无人机（UAV）与深度学习模型的整合，创造了实施和发展更有效的野火管理的前所未有的动力。尽管一些现有的调查论文已探讨了各种基于学习的方法，但明显缺乏一篇全面综述，强调AI支持的UAV系统在多阶段野火管理中的应用及其后续影响。本综述旨在填补这些差距，通过系统回顾最新的技术进展，重点介绍了从火灾前到主动火灾阶段再到火灾后管理的UAV系统和AI模型的进展。为此，我们对现有的遥感系统进行了广泛分析。

    Wildfires have emerged as one of the most destructive natural disasters worldwide, causing catastrophic losses in both human lives and forest wildlife. Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models, has created an unprecedented momentum to implement and develop more effective wildfire management. Although some of the existing survey papers have explored various learning-based approaches, a comprehensive review emphasizing the application of AI-enabled UAV systems and their subsequent impact on multi-stage wildfire management is notably lacking. This survey aims to bridge these gaps by offering a systematic review of the recent state-of-the-art technologies, highlighting the advancements of UAV systems and AI models from pre-fire, through the active-fire stage, to post-fire management. To this aim, we provide an extensive analysis of the existing remote sensing systems with a parti
    
[^65]: 机器学习中的计算分歧：对学术贡献和审查的威胁？

    The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?. (arXiv:2401.02452v1 [cs.CY])

    [http://arxiv.org/abs/2401.02452](http://arxiv.org/abs/2401.02452)

    本论文调查了机器学习研究中计算分歧对学术贡献和审查的影响，发现计算分歧导致学术界在计算密集型研究主题中的影响力降低，并且学术研究趋向于使用工业界开发的开源、预训练模型。为解决这一问题，建议通过国家支持的计算基础设施与开放科学倡议的结合来拓展学术见解。

    

    工业和学术人工智能实验室在使用计算资源方面存在明显差异。我们提供了一项数据驱动的调查，以了解计算分歧对机器学习研究的影响。我们表明，计算分歧与计算密集型研究主题中学术独立研究团队的表示减少有关，特别是基础模型方面。我们认为，学术界在推进相关技术、提供关键的评估和审查以及在这些模型的传播方面可能发挥的作用将较小。与研究重心的这种变化同时，学术研究在倡导工业界开发的开源、预训练模型方面出现了明显转变。为了应对这一趋势所带来的挑战，尤其是对影响力模型的审查减少，我们建议采取一些方法来有针对性地拓展学术见解。这些方法包括国家支持的计算基础设施与开放科学倡议的结合。

    There are pronounced differences in the extent to which industrial and academic AI labs use computing resources. We provide a data-driven survey of the role of the compute divide in shaping machine learning research. We show that a compute divide has coincided with a reduced representation of academic-only research teams in compute intensive research topics, especially foundation models. We argue that, academia will likely play a smaller role in advancing the associated techniques, providing critical evaluation and scrutiny, and in the diffusion of such models. Concurrent with this change in research focus, there is a noticeable shift in academic research towards embracing open source, pre-trained models developed within the industry. To address the challenges arising from this trend, especially reduced scrutiny of influential models, we recommend approaches aimed at thoughtfully expanding academic insights. Nationally-sponsored computing infrastructure coupled with open science initia
    
[^66]: FedDiff: 基于扩散模型的多模态和多客户联邦学习

    FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])

    [http://arxiv.org/abs/2401.02433](http://arxiv.org/abs/2401.02433)

    FedDiff是一个多模态协作扩散联邦学习框架，旨在实现来自不同客户的异构数据的安全融合，通过建立双分支扩散模型特征提取设置来驱动联邦学习过程。

    

    随着遥感领域成像传感器技术的快速发展，多模态遥感数据融合已成为土地覆盖分类任务中的一个关键研究方向。然而，现有的扩散模型主要集中在单模态和单客户控制上，即扩散过程由单个模态在单个计算节点驱动。为了实现来自客户的异构数据的安全融合，需要实现分布式的多模态控制，例如在每个基站客户端上，将组织A的高光谱数据和组织B的激光雷达数据进行私密合并。本研究提出了一种名为FedDiff的多模态协作扩散联邦学习框架。我们的框架建立了一个双分支扩散模型特征提取设置，其中两种模态数据被输入到编码器的分支中。

    With the rapid development of imaging sensor technology in the field of remote sensing, multi-modal remote sensing data fusion has emerged as a crucial research direction for land cover classification tasks. While diffusion models have made great progress in generative models and image classification tasks, existing models primarily focus on single-modality and single-client control, that is, the diffusion process is driven by a single modal in a single computing node. To facilitate the secure fusion of heterogeneous data from clients, it is necessary to enable distributed multi-modal control, such as merging the hyperspectral data of organization A and the LiDAR data of organization B privately on each base station client. In this study, we propose a multi-modal collaborative diffusion federated learning framework called FedDiff. Our framework establishes a dual-branch diffusion model feature extraction setup, where the two modal data are inputted into separate branches of the encoder
    
[^67]: 自动分类ImageNet上的模型错误

    Automated Classification of Model Errors on ImageNet. (arXiv:2401.02430v1 [cs.CV])

    [http://arxiv.org/abs/2401.02430](http://arxiv.org/abs/2401.02430)

    该论文提出了自动错误分类框架来解决ImageNet数据集中的标签噪声和错误问题，为研究模型选择如何影响错误分布提供了有价值的工具。

    

    虽然ImageNet数据集在过去十年中推动了计算机视觉研究，但显著的标签噪声和歧义使得Top-1准确率成为进一步进展的不够充分的指标。为了解决这个问题，提出了新的标签集和评估协议，显示出最先进的模型已经实现了超过95%的准确率，并将重点转向为什么剩余的错误仍然存在的调查。最近的工作采用专家小组对两个选择的模型的所有剩余分类错误进行手动分类。然而，这个过程耗时，容易产生不一致性，并且需要受过训练的专家，使其不适用于常规模型评估，从而限制了其效用。为了克服这些局限性，我们提出了第一个自动错误分类框架，这是一个有价值的工具，用于研究建模选择如何影响错误分布。我们使用我们的框架全面评估了超过90个模型的错误分布。

    While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over 95% accuracy and shifting the focus on investigating why the remaining errors persist.  Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 90
    
[^68]: 基于脑启发的脉冲神经网络在工业故障诊断中的应用：调查、挑战和机遇

    Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])

    [http://arxiv.org/abs/2401.02429](http://arxiv.org/abs/2401.02429)

    基于脑启发的脉冲神经网络是一种有前景的应用于工业故障诊断的替代方法，可以克服人工神经网络的限制，提供更精确和有效的故障识别。

    

    近几十年来，工业故障诊断（IFD）作为一门关注检测和收集工业设备健康状况重要信息的学科而出现，从而促进了对故障类型和严重程度的识别。精确和有效的故障识别引起了广泛关注，导致对自动化设备监测的关注，以避免安全事故并减少对人力的依赖。人工神经网络（ANNs）的出现在增强智能IFD算法方面起到了重要作用，特别是在大数据背景下。尽管取得了这些进展，作为一种简化的仿生神经网络模型，ANNs存在固有的限制，如资源和数据依赖性以及受限的认知能力。为了解决这些限制，基于脑启发计算原理的第三代脉冲神经网络（SNN）已经成为一种有前景的替代方法。

    In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. Th
    
[^69]: 利用人工智能/机器学习的5G定位技术的进展

    5G Positioning Advancements with AI/ML. (arXiv:2401.02427v1 [cs.NI])

    [http://arxiv.org/abs/2401.02427](http://arxiv.org/abs/2401.02427)

    本文对基于AI/ML的5G系统内的直接定位进行了全面回顾，重点讨论了在传统方法不适用的挑战场景和条件下的潜力。研究发现了重要的模拟结果和观察，并提出了解决测量报告、数据收集和模型管理的解决方案，促进了直接定位的进一步发展。

    

    本文全面回顾了基于人工智能/机器学习的5G系统内的直接定位，重点讨论在传统方法往往表现不佳的挑战场景和条件下的潜力。在技术报告TR38.843的洞见基础上，我们着重研究了生命周期管理（LCM）与直接定位过程相关的方面。我们强调了来自报告中对各种挑战条件下的直接定位的重要模拟结果和关键观察。此外，我们还讨论了解决测量报告、数据收集和模型管理的选择性解决方案，强调它们对推进直接定位的重要性。

    This paper provides a comprehensive review of AI/ML-based direct positioning within 5G systems, focusing on its potential in challenging scenarios and conditions where conventional methods often fall short. Building upon the insights from the technical report TR38.843, we examine the Life Cycle Management (LCM) with a focus on to the aspects associated direct positioning process. We highlight significant simulation results and key observations from the report on the direct positioning under the various challenging conditions. Additionally, we discuss selected solutions that address measurement reporting, data collection, and model management, emphasizing their importance for advancing direct positioning.
    
[^70]: 无人机辅助物联网网络中基于转换器的最小AoI数据收集的无人机轨迹规划

    UAV Trajectory Planning for AoI-Minimal Data Collection in UAV-Aided IoT Networks by Transformer. (arXiv:2401.02425v1 [cs.NI])

    [http://arxiv.org/abs/2401.02425](http://arxiv.org/abs/2401.02425)

    这项研究提出了一种机器学习算法，通过优化无人机的轨迹规划，最小化了在物联网网络中收集的数据的总年龄，从而保持了数据的新鲜度。

    

    在物联网（IoT）网络中，保持数据收集的新鲜度引起了越来越多的关注。考虑到信息的年龄（AoI），我们研究了无人机（UAV）的轨迹规划问题，该无人机用于辅助基于集群的物联网网络。我们提出了一个优化问题，通过优化选择悬停点和访问这些点的顺序，以最小化UAV从地面IoT网络收集的数据的总AoI。我们利用了最先进的转换器和加权A*算法来设计一个机器学习算法来解决这个问题。整个UAV-IoT系统被输入到所提出算法的编码器网络中，算法的解码器网络输出了访问地面集群的顺序。

    Maintaining freshness of data collection in Internet-of-Things (IoT) networks has attracted increasing attention. By taking into account age-of-information (AoI), we investigate the trajectory planning problem of an unmanned aerial vehicle (UAV) that is used to aid a cluster-based IoT network. An optimization problem is formulated to minimize the total AoI of the collected data by the UAV from the ground IoT network. Since the total AoI of the IoT network depends on the flight time of the UAV and the data collection time at hovering points, we jointly optimize the selection of hovering points and the visiting order to these points. We exploit the state-of-the-art transformer and the weighted A*, which is a path search algorithm, to design a machine learning algorithm to solve the formulated problem. The whole UAV-IoT system is fed into the encoder network of the proposed algorithm, and the algorithm's decoder network outputs the visiting order to ground clusters. Then, the weighted A* 
    
[^71]: 使用EuroSAT和迁移学习进行土地利用和土地覆盖（LULC）的映射

    Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning. (arXiv:2401.02424v1 [cs.CV])

    [http://arxiv.org/abs/2401.02424](http://arxiv.org/abs/2401.02424)

    该论文利用EuroSAT和迁移学习技术对土地利用和土地覆盖进行映射，通过使用颜色波段进行微调，取得了99.19%的精确度，有助于制定保护和城市规划政策。

    

    随着全球人口的不断增长，对自然资源的需求也在增加。不幸的是，人类活动占温室气体排放的23%。好消息是，遥感技术已经成为管理我们环境的有价值工具。这些技术使我们能够监测土地利用，规划城市区域，并推动农业、气候变化缓解、灾害恢复和环境监测等领域的进步。近年来，人工智能、计算机视觉和地球观测数据的进展使土地利用映射的准确性达到了前所未有的水平。通过使用迁移学习和用RGB波段微调，我们在土地利用分析中取得了令人印象深刻的99.19%的准确性。这样的发现可以用于制定保护和城市规划政策。

    As the global population continues to expand, the demand for natural resources increases. Unfortunately, human activities account for 23% of greenhouse gas emissions. On a positive note, remote sensing technologies have emerged as a valuable tool in managing our environment. These technologies allow us to monitor land use, plan urban areas, and drive advancements in areas such as agriculture, climate change mitigation, disaster recovery, and environmental monitoring. Recent advances in AI, computer vision, and earth observation data have enabled unprecedented accuracy in land use mapping. By using transfer learning and fine-tuning with RGB bands, we achieved an impressive 99.19% accuracy in land use analysis. Such findings can be used to inform conservation and urban planning policies.
    
[^72]: 神经听觉机器智能（NEURO-AMI）的视角

    Neuronal Auditory Machine Intelligence (NEURO-AMI) In Perspective. (arXiv:2401.02421v1 [cs.NE])

    [http://arxiv.org/abs/2401.02421](http://arxiv.org/abs/2401.02421)

    神经听觉机器智能（NEURO-AMI）是一种从人类大脑中获取灵感的人工神经机器学习系统，在软计算领域有着广阔的应用前景。然而，为了满足用户需求，仍然需要改进其可承受性、复杂性和数据学习大小要求等方面的优化措施。

    

    在软计算的最新发展中，不能不提到从真实的大脑皮层组织或人类大脑中发生的过程中汲取灵感的人工神经机器学习系统的贡献。这些神经系统的普遍逼近性已经导致了其广泛使用，这种进化技术的新发展表明，在软计算领域中，这种人工智能技术有着光明的未来。实际上，大规模和非常深层的人工神经系统网络的泛滥以及相应的神经机器学习算法的增强和发展，对于深度学习现代领域的发展做出了巨大贡献，可以在Lecun、Bengio和Hinton的研究成果中找到。然而，终端用户价格可承受性、降低复杂性和降低数据学习大小要求等关键需求意味着，仍然需要满足句法要求的优化措施。

    The recent developments in soft computing cannot be complete without noting the contributions of artificial neural machine learning systems that draw inspiration from real cortical tissue or processes that occur in human brain. The universal approximability of such neural systems has led to its wide spread use, and novel developments in this evolving technology has shown that there is a bright future for such Artificial Intelligent (AI) techniques in the soft computing field. Indeed, the proliferation of large and very deep networks of artificial neural systems and the corresponding enhancement and development of neural machine learning algorithms have contributed immensely to the development of the modern field of Deep Learning as may be found in the well documented research works of Lecun, Bengio and Hinton. However, the key requirements of end user affordability in addition to reduced complexity and reduced data learning size requirement means there still remains a need for the synt
    
[^73]: AI是否能像人类一样具备创造力？

    Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])

    [http://arxiv.org/abs/2401.01623](http://arxiv.org/abs/2401.01623)

    本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。

    

    创造力是社会进步和创新的基石，但其评估仍然是一个复杂且主观的任务。随着先进的生成型AI模型的出现，能够完成曾经只属于人类创造力的任务，探索AI的创造潜力变得至关重要，以确保其负责任的发展和应用。本文通过引入一个名为“相对创造力”的新概念来解决定义和评估创造力的复杂性。我们不再试图对创造力进行普遍定义，而是将焦点转向AI是否能够与一位假设的人类具备相同的创造能力。这种观点借鉴了图灵测试的思想，并扩展其范围以解决评估创造力中所固有的挑战和主观性。这种方法的转变使得对AI创造力的统计量化评估成为可能，我们将其称为统计创造力。这种方法允许直接比较AI与特定人类的创造能力。

    Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
    
[^74]: 张量网络在可解释的机器学习中在网络安全中的应用

    Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])

    [http://arxiv.org/abs/2401.00867](http://arxiv.org/abs/2401.00867)

    张量网络可以帮助发展可解释的机器学习算法，并提供丰富的模型可解释性。在网络安全中，我们的无监督聚类算法基于矩阵乘积状态，在性能上与传统的深度学习模型相媲美。我们的方法还能提取特征概率、熵和互信息，提供了分类异常的引人入胜的叙述，并实现了前所未有的透明度和可解释性水平。

    

    本文展示了张量网络如何帮助发展可解释的机器学习算法。具体而言，我们基于矩阵乘积状态（MPS）开发了一种无监督聚类算法，并将其应用于实际使用案例中的对手生成的威胁情报。我们的研究证明，MPS在性能方面可以与传统的深度学习模型如自编码器和生成对抗网络相媲美，同时提供更丰富的模型可解释性。我们的方法自然地促进了特征概率、冯·诺伊曼熵和互信息的提取，为异常分类提供了引人入胜的叙述，并促进了前所未有的透明度和可解释性水平，这对于理解人工智能决策的基本原理至关重要。

    In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
    
[^75]: 自监督预训练用于决策基础模型：形式化、流程和挑战

    Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges. (arXiv:2401.00031v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00031](http://arxiv.org/abs/2401.00031)

    本文认为将大规模自监督预训练中的知识与决策问题相结合，可以解决决策中的样本效率和泛化性问题。通过提出先预训练再自适应的流程，并调研了决策预训练和下游推理中的数据收集、预训练目标和自适应策略的最新研究。最后，确定了发展决策基础模型的关键挑战和未来方向。

    

    决策是一个动态过程，需要感知、记忆和推理来进行选择并找到最优策略。传统的决策方法在样本效率和泛化性上存在问题，而大规模的自监督预训练已经使得语言和视觉领域的快速适应成为可能，通过微调或少样本学习。因此，我们提出将从大规模自监督预训练中获取的知识与下游决策问题融合起来。我们提出了先预训练再自适应的流程，并调研了决策预训练和下游推理中的数据收集、预训练目标和自适应策略的最新研究。最后，我们确定了在通用灵活的自监督预训练的帮助下，发展决策基础模型面临的关键挑战和未来方向。

    Decision-making is a dynamic process requiring perception, memory, and reasoning to make choices and find optimal policies. Traditional approaches to decision-making suffer from sample efficiency and generalization, while large-scale self-supervised pretraining has enabled fast adaptation with fine-tuning or few-shot learning in language and vision. We thus argue to integrate knowledge acquired from generic large-scale self-supervised pretraining into downstream decision-making problems. We propose Pretrain-Then-Adapt pipeline and survey recent work on data collection, pretraining objectives and adaptation strategies for decision-making pretraining and downstream inference. Finally, we identify critical challenges and future directions for developing decision foundation model with the help of generic and flexible self-supervised pretraining.
    
[^76]: FENet: 增强聚焦网络用于车道检测

    FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.17163](http://arxiv.org/abs/2312.17163)

    FENet是一个增强聚焦网络用于精准车道检测，通过聚焦采样和部分视野评估等创新方法，显著提高了检测准确性，尤其适用于曲线和远距离车道，在安全性方面具有重要意义。

    

    受到人类驾驶注意力的启发，本研究首次开发了增强聚焦采样、部分视野评估、增强FPN架构和定向IoU损失的网络创新，解决了自动驾驶车道检测中的精准性障碍。实验证明，我们的聚焦采样策略，强调远处重要细节，与均匀方法相比，显著提高了基准和实际曲线/远距离车道识别的准确性，这对安全至关重要。虽然FENetV1通过模拟驾驶员视觉的透视感知上下文改进，实现了最先进的传统度量性能，但FENetV2在提出的部分视野分析中证明是最可靠的。因此，我们特别推荐V2用于实际车道导航，尽管在标准的整张图像测量上有轻微的降级。未来的方向包括收集实际道路数据和集成互补的双重框架，以进一步通过人类感知指导实现突破性进展。

    Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving. Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety. While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis. Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures. Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perc
    
[^77]: 闪存LLM：在有限内存下高效运行大型语言模型

    LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11514](http://arxiv.org/abs/2312.11514)

    本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。

    

    大型语言模型（LLM）在现代自然语言处理中起着至关重要的作用，在各种任务中表现出色。然而，它们庞大的计算和内存需求带来了挑战，特别是对于具有有限DRAM容量的设备而言。本文通过将模型参数存储在闪存中，并按需将其传输到DRAM的方式，解决了超过可用DRAM容量的LLM高效运行的挑战。我们的方法涉及构建一个考虑闪存特性的推理成本模型，引导我们在两个关键领域进行优化：减少从闪存传输的数据量，并以较大、更连续的块读取数据。在这个受硬件启发的框架内，我们引入了两个主要技术。首先，“窗口化”通过重复使用之前激活的神经元来策略性地减少数据传输，其次，“行列绑定”适应了闪存的顺序数据访问特点，

    Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
    
[^78]: 基于检索增强的大型语言模型：一项调研

    Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.10997](http://arxiv.org/abs/2312.10997)

    本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。

    

    大型语言模型（LLMs）展示了显著的能力，但面临幻觉、过时知识和非透明、不可追溯的推理过程等挑战。检索增强生成（RAG）已经成为一种有前途的解决方案，通过整合来自外部数据库的知识，增强模型的准确性和可信度，特别适用于知识密集型任务，并允许持续更新知识和整合领域特定信息。RAG将LLMs自身的知识与庞大、动态的外部数据库相结合，实现协同效应。本综述论文详细考察了RAG范式的发展，包括Naive RAG、Advanced RAG和Modular RAG。它详细审视了RAG框架的三个基本要素，包括检索、生成和增强技术。本文强调了嵌入在RAG框架中的最新技术。

    Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
    
[^79]: PromptBench：一个用于评估大型语言模型的统一库

    PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.07910](http://arxiv.org/abs/2312.07910)

    PromptBench是一个用于评估大型语言模型的统一库，包括了提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具等组件，旨在促进原创研究和创建新的基准测试、部署下游应用以及设计新的评估协议。

    

    对大型语言模型（LLMs）的评估对于评估其性能和减轻潜在的安全风险至关重要。本文介绍了PromptBench，一个用于评估LLMs的统一库。它由几个关键组件组成，研究人员可以轻松使用和扩展：提示语构建、提示语工程、数据集和模型加载、对抗性提示攻击、动态评估协议和分析工具。PromptBench旨在成为一个开放、通用和灵活的代码库，以促进原创研究，创建新的基准测试、部署下游应用和设计新的评估协议。代码可在https://github.com/microsoft/promptbench上找到，并将持续支持。

    The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
    
[^80]: KwaiAgents：基于大型语言模型的通用信息搜索智能体系统

    KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.04889](http://arxiv.org/abs/2312.04889)

    本文介绍了 KwaiAgents，这是一个基于大型语言模型的通用信息搜索智能体系统。该系统能够利用语言模型作为认知核心，理解用户的查询，行为准则并参考外部文档，以提供高质量的知识和信息。

    

    人类由于好奇心的驱使，不断探索和理解周围的世界，从而发明了各种工具来满足这种好奇心。尽管人类无法在大脑中处理和记忆大量信息，但在批判思维、规划、反思以及利用现有工具与世界进行交互和解释方面卓越出色，使其能够高效地寻找答案。最近大型语言模型（LLM）的进步表明，机器可能也具备类似于人类的能力，即使参数数量受限，也能展示强大的能力。在本文中，我们介绍了 KwaiAgents，这是一个基于LLM的通用信息搜索智能体系统。在 KwaiAgents 中，我们提出了一种利用LLM作为认知核心的智能体系统，它能够理解用户的查询、行为准则和参考外部文档。智能体还可以更新查询结果，与用户进行互动，并提供高质量的知识和信息。

    Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
    
[^81]: 通过联邦迁移学习对基础模型进行接地：一个通用框架

    Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.17431](http://arxiv.org/abs/2311.17431)

    本论文提出了一个通用框架，通过联邦迁移学习将基础模型接地，以解决面临的挑战，如受限的计算资源、数据隐私、模型异构性和模型所有权。这个框架可以帮助释放基础模型的潜力，并在不同行业中产生重要影响。

    

    基于广泛知识和强大的新兴能力编码的Foundation Models（FMs），如GPT-4，在各种自然语言处理和计算机视觉任务中取得了显著成功。通过将FMs适应于特定领域任务或增加领域特定知识来对其进行接地，我们可以充分发挥FMs的潜力。然而，接地FMs面临着多个挑战，主要是受限的计算资源、数据隐私、模型异构性和模型所有权。联邦迁移学习（FTL），即联邦学习和迁移学习的结合，为解决这些挑战提供了有希望的解决方案。近年来，学术界和工业界对通过FTL-FM利用FMs进行接地的需求强烈增长。受到FTL-FM研究的强劲增长和FTL-FM对工业应用的潜在影响的推动，我们提出了一个FTL-FM框架，用于在联邦学习环境中建立FMs的接地问题。

    Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated lea
    
[^82]: GeoLocator:一种用于推断地理隐私的位置集成大型多模态模型

    GeoLocator: a location-integrated large multimodal model for inferring geo-privacy. (arXiv:2311.13018v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2311.13018](http://arxiv.org/abs/2311.13018)

    GeoLocator是一种用于推断地理隐私的位置集成大型多模态模型，能够高精度地生成具体的地理细节，突显了在线数据共享和信息获取的威胁。

    

    地理隐私是指保护个人地理位置的私密性，特别是限制个人电子设备中维护的地理数据。地理隐私是个人安全的重要方面，然而在日常活动中常常被忽视。随着大型多模态模型（如GPT-4）在开源情报（OSINT）中的使用激增，与地理隐私泄露相关的潜在风险加剧。本研究开发了一种名为GeoLocator的基于GPT-4的位置集成模型，并设计了四维实验来展示其推断输入图像和/或社交媒体内容的位置信息的能力。我们的实验结果表明，GeoLocator能够高精度地生成具体的地理细节，从而将模型用户无意中暴露地理空间信息的风险嵌入其中，突显了在线数据共享和信息获取的威胁。

    Geographic privacy or geo-privacy refers to the keeping private of one's geographic location, especially the restriction of geographical data maintained by personal electronic devices. Geo-privacy is a crucial aspect of personal security; however, it often goes unnoticed in daily activities. With the surge in the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source Intelligence (OSINT), the potential risks associated with geo-privacy breaches have intensified. This study develops a location-integrated GPT-4 based model named GeoLocator and designs four-dimensional experiments to demonstrate its capability in inferring the locational information of input imageries and/or social media contents. Our experiments reveal that GeoLocator generates specific geographic details with high accuracy and consequently embeds the risk of the model users exposing geospatial information to the public unintentionally, highlighting the thread of online data sharing, information gathering 
    
[^83]: "它不像Jarvis，但很接近！" -- 探究ChatGPT在计算机科学本科生中的使用情况

    "It's not like Jarvis, but it's pretty close!" -- Examining ChatGPT's Usage among Undergraduate Students in Computer Science. (arXiv:2311.09651v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2311.09651](http://arxiv.org/abs/2311.09651)

    大多数计算机科学本科生对ChatGPT持有积极态度，并认为它能在课程相关任务中发挥作用，但也存在一些需解决的挑战。

    

    大型语言模型（LLM）如ChatGPT和Google Bard在学术界引起了广泛关注。先前的研究已评估了这些LLM在生成编程练习和解决方案等各种应用方面的表现。然而，这些评估主要由教师和研究人员进行，并未考虑到学生对LLM的实际使用情况。本研究采用以学生为中心的方法，全面了解本科计算机科学学生如何利用OpenAI发布的热门LLM ChatGPT。我们通过学生调查和访谈相结合的方式获取有价值的洞察，了解与ChatGPT相关的好处、挑战和改进建议。我们的研究结果表明，大多数学生（超过57%）对将ChatGPT作为课程相关任务的辅助工具持有积极的态度。然而，我们的研究也强调了必须解决的各种挑战，以实现对ChatGPT的长期接受。

    Large language models (LLMs) such as ChatGPT and Google Bard have garnered significant attention in the academic community. Previous research has evaluated these LLMs for various applications such as generating programming exercises and solutions. However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students. This study adopts a student-first approach to comprehensively understand how undergraduate computer science students utilize ChatGPT, a popular LLM, released by OpenAI. We employ a combination of student surveys and interviews to obtain valuable insights into the benefits, challenges, and suggested improvements related to ChatGPT. Our findings suggest that a majority of students (over 57%) have a convincingly positive outlook towards adopting ChatGPT as an aid in coursework-related tasks. However, our research also highlights various challenges that must be resolved for long-term acceptance of Chat
    
[^84]: 探索分割联邦学习的隐私-能耗权衡

    Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.09441](http://arxiv.org/abs/2311.09441)

    本文研究了分割联邦学习（SFL）中隐私和能耗之间的权衡，强调了快速收敛的优势，并分析了切割层对客户端能耗和隐私的影响。

    

    分割联邦学习（SFL）最近已经成为一种有前景的分布式学习技术，充分利用了联邦学习和分割学习的优势。它强调了快速收敛的优势，同时解决了隐私问题。因此，这一创新受到了工业界和学术界的广泛关注。然而，由于SFL中模型在特定层（称为切割层）上被分割为客户端和服务器端模型，选择切割层可能对客户端的能耗和隐私产生重大影响，因为它影响了训练负担和客户端模型的输出。此外，确定切割层的设计挑战非常复杂，主要由于客户端的计算和网络能力的固有异质性。在本文中，我们全面概述了SFL的过程，并对能耗和隐私进行了深入分析。

    Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and
    
[^85]: 计算机断层扫描肺动脉造影中的深度学习：用于肺栓塞检测的双重方法

    Deep learning in computed tomography pulmonary angiography imaging: a dual-pronged approach for pulmonary embolism detection. (arXiv:2311.05197v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.05197](http://arxiv.org/abs/2311.05197)

    我们提出了一种深度学习方法，用于计算机断层扫描肺动脉造影中的肺栓塞检测。该方法通过引入分类器的概率推理来指导检测预测，在自动PE诊断领域提供了一种新的贡献。我们的分类器利用注意力机制来同时关注整体外貌和局部病变区域，实现了稳健的性能。

    

    肺栓塞（PE）诊断对计算机断层扫描肺动脉造影（CTPA）的依赖度不断增加，这导致了对改进诊断方法的挑战和迫切需求。本研究的主要目标是利用深度学习技术提升PE的计算机辅助诊断（CAD）。为此，我们提出了一种分类器引导的检测方法，该方法通过有效利用分类器的概率推理来指导检测预测，在自动PE诊断领域提供了一种新的贡献。我们的分类系统包括一个采用注意力机制的卷积神经网络（AG-CNN），它通过使用局部上下文信息来模拟人类专家的关注，即在做出判断之前同时关注整体外貌和局部病变区域。该分类器在FUMPE数据集上展现出稳健的性能，实现了0.927的AUROC、0.862的敏感度、0.879的特异度。

    The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA) for Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need for improved diagnostic solutions. The primary objective of this study is to leverage deep learning techniques to enhance the Computer Assisted Diagnosis (CAD) of PE. With this aim, we propose a classifier-guided detection approach that effectively leverages the classifier's probabilistic inference to direct the detection predictions, marking a novel contribution in the domain of automated PE diagnosis. Our classification system includes an Attention-Guided Convolutional Neural Network (AG-CNN) that uses local context by employing an attention mechanism. This approach emulates a human expert's attention by looking at both global appearances and local lesion regions before making a decision. The classifier demonstrates robust performance on the FUMPE dataset, achieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and 
    
[^86]: 动态最优传输问题的一种新型跳跃正交列表

    A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2310.18446](http://arxiv.org/abs/2310.18446)

    本论文提出了一种新型的跳跃正交列表来解决动态最优传输问题，在考虑数据点权重或位置变化时能够有效地更新最优传输方案。

    

    最优传输是过去几十年来引起了优化社区极大关注的一个基本主题。本文研究了一个有趣的离散动态最优传输问题：当数据点的权重或位置发生变化时，我们是否可以高效地更新最优传输方案？这个问题在机器学习中有多个应用。我们经常需要计算两个不同数据集之间的最优传输成本；如果一些数据点发生了变化，我们应该重新计算高复杂度的成本函数，还是通过一些高效的动态数据结构来更新成本？我们注意到先前已提出了几种动态最大流算法，但据我们所知，对于动态最小费用流问题的研究仍然非常有限。我们提出了一种新型的2D跳跃正交列表，结合一些动态树技术。

    Optimal transport is a fundamental topic that has attracted a great amount of attention from the optimization community in the past decades. In this paper, we consider an interesting discrete dynamic optimal transport problem: can we efficiently update the optimal transport plan when the weights or the locations of the data points change? This problem is naturally motivated by several applications in machine learning. For example, we often need to compute the optimal transport cost between two different data sets; if some changes happen to a few data points, should we re-compute the high complexity cost function or update the cost by some efficient dynamic data structure? We are aware that several dynamic maximum flow algorithms have been proposed before, however, the research on dynamic minimum cost flow problem is still quite limited, to the best of our knowledge. We propose a novel 2D Skip Orthogonal List together with some dynamic tree techniques. Although our algorithm is based on
    
[^87]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^88]: 用开放数据驱动的团队推荐促进研究合作

    Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09404](http://arxiv.org/abs/2309.09404)

    通过利用开放数据和人工智能方法，我们设计了一个系统来推荐团队，使得每个团队能够满足项目要求的技能覆盖，并且平衡候选成员之间的工作分配。

    

    团队建设和促进合作是非常常见的商业活动。一个例子就是TeamingForFunding问题，研究机构和研究人员在向资助机构申请时，希望能够找到合作机会以回应后者的项目申请。我们描述了一个新颖的系统，利用各种人工智能方法来推荐团队，使得每个团队都能够达到机会要求的最高技能覆盖，并且候选成员之间的工作分配是平衡的。我们通过提取开放数据中的项目申请（需求）和研究人员简介（供给）中的技能潜力，使用分类法对其进行归一化，创建了有效的算法来匹配需求和供给。我们创建团队以最大化一个新的衡量短期和长期目标的度量的优势。我们定量验证了我们算法的成功，通过…

    Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by e
    
[^89]: 检索增强型文本到音频生成

    Retrieval-Augmented Text-to-Audio Generation. (arXiv:2309.08051v1 [cs.SD])

    [http://arxiv.org/abs/2309.08051](http://arxiv.org/abs/2309.08051)

    这篇论文提出了一种检索增强的文本到音频生成方法，用于解决长尾文本到音频生成的问题。通过利用检索到的相关文本-音频数据作为额外条件，从而增强了模型的学习能力，在AudioCaps数据集上取得了最先进的结果。

    

    尽管在文本到音频(TTA)生成方面取得了一些进展，我们发现状态-艺术模型，如AudioLDM，在数据集上表现出类别分布不平衡（如AudioCaps）的训练中，其生成性能存在偏差。具体而言，它们在生成常见音频类别方面表现出色，而在罕见类别方面表现不佳，从而降低了整体生成性能。我们将此问题称为长尾文本到音频生成。为解决这个问题，我们提出了一种简单的检索增强方法来进行TTA模型。具体而言，给定一个文本输入提示，我们首先使用对比语音语言预训练（CLAP）模型来检索相关的文本-音频对。然后使用检索到的音频-文本数据的特征作为额外条件来指导TTA模型的学习。我们使用我们提出的方法增强了AudioLDM，并将所得到的增强系统称为Re-AudioLDM。在AudioCaps数据集上，Re-AudioLDM实现了最先进的Frechet得分。

    Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet 
    
[^90]: 在上下文中学习编程风格以解决基于知识的问答中的问题

    Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])

    [http://arxiv.org/abs/2309.04695](http://arxiv.org/abs/2309.04695)

    本论文提出了一种在上下文中学习编程风格的方法，用于解决基于知识的问答中生成逻辑表达式的格式错误问题。

    

    目前，针对基于知识的问答(KBQA)的方法通常依赖复杂的训练技术和模型框架，导致在实际应用中存在许多限制。最近，大型语言模型(LLMs)中的上下文学习(ICL)能力的出现为KBQA提供了一种简单且无需训练的语义解析范式：给定少量问题及其标记的逻辑表达式作为演示示例，LLMs能够理解任务意图并为新问题生成逻辑表达式。然而，当前强大的LLMs在预训练过程中对逻辑表达式的了解很少，导致格式错误率较高。为了解决这个问题，我们提出了一种针对KBQA的代码风格上下文学习方法，将陌生逻辑表达式的生成过程转换为更为熟悉的代码生成过程。对三个主流数据集的实验结果表明，我们的方法显著减轻了生成逻辑表达式中的格式错误问题。

    Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for
    
[^91]: 无监督机器学习模型选择中的主观性

    Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v1 [cs.LG])

    [http://arxiv.org/abs/2309.00201](http://arxiv.org/abs/2309.00201)

    无监督机器学习模型选择具有主观性，模型选择结果受模型构建者偏好的影响，并可能导致选择的不一致性。需要对模型选择过程进行更加深入的研究和标准化。

    

    模型选择是无监督机器学习中必要的步骤。尽管有很多标准和指标，但模型选择仍然存在主观性。高度主观性可能会对各种机器学习研究的重复性和可再现性产生疑问，并对实际部署的模型的稳健性产生怀疑。然而，模型选择结果中模型构建者的偏好影响的影响尚未得到充分探索。本研究以隐马尔可夫模型为例，调查了模型选择中的主观性。我们邀请了33位参与者和三个大型语言模型（LLMs），在三个场景中进行模型选择。结果显示，无论是参与者还是LLMs的选择都存在变异性和不一致性，尤其是当不同的标准和指标存在分歧时。主观性来源包括对不同标准和指标重要性的不同意见，对模型应该有多简洁的不同看法，以及对数据规模的大小的看法。

    Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a da
    
[^92]: 通过预训练稳定RNN的梯度

    Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v1 [cs.LG])

    [http://arxiv.org/abs/2308.12075](http://arxiv.org/abs/2308.12075)

    该研究通过预训练网络实现局部稳定性，拓展了已有的稳定性理论，可以应用于复杂结构的循环神经网络。

    

    学习的众多理论都建议通过防止梯度的方差以指数形式随深度或时间增长来稳定和改善训练。通常情况下，这些分析是在前馈全连接神经网络或单层循环神经网络上进行的，因为它们具有数学的可解性。与此相反，本研究表明，当体系结构过于复杂以至于无法进行解析初始化时，通过预训练网络实现局部稳定性是有效的。此外，我们扩展了已知的稳定性理论，涵盖了一个更广泛的深层循环网络家族，对数据和参数分布的要求较少，这个理论被称为局部稳定性条件（LSC）。我们的调查发现，经典的Glorot、He和正交初始化方案在应用于前馈全连接神经网络时可以满足LSC。然而，在对深层循环网络进行分析时，我们发现了一种新的指数增长来源。

    Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponent
    
[^93]: AIs的发展脱靴法

    Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])

    [http://arxiv.org/abs/2308.04586](http://arxiv.org/abs/2308.04586)

    传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。

    

    尽管当前一些AI在封闭的世界，如棋盘游戏中超越了人类能力，但它们在混乱的现实世界中的表现有限。它们会犯奇怪的错误而且没有意识到。它们很难受到指导，不能运用常识，缺乏好奇心。它们不能成为良好的合作者。传统手动构建的符号AI方法构建的系统和使用生成和深度学习AI方法(包括大规模语言模型)构建的系统都无法应对这些挑战。它们不适合创建强大和可信赖的AI。尽管此方法不属于主流的AI方法，但发展脱靴法显示出希望。在发展脱靴法中，AI像人类儿童一样发展能力。它们从先天能力开始。像人类一样，它们与环境互动，并从互动中学习。它们通过自我发展的能力逐步扩展先天能力。它们互动并逐渐将所学应用于实际操作。

    Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
    
[^94]: 背包问题：连通性、路径和最短路径

    Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2307.12547](http://arxiv.org/abs/2307.12547)

    该论文研究了带有图论约束的背包问题，证明了问题的复杂性并提出了近似算法。

    

    我们研究了带有图论约束的背包问题。也就是说，我们假设背包项目集上存在一个图结构，并且解决方案还需要满足背包约束之上的某些图论性质。特别是，在连通背包问题中，我们需要计算一个连通的项目子集，其价值最大，且满足背包约束的大小。我们证明了即使对于最大度为四的图，这个问题也是强NP完全的，对于星形图，这个问题也是NP完全的。另一方面，我们开发了一个运行时间为$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$的算法，在其中$tw,s,d$分别是图的树宽度，背包的大小和目标值。我们进一步展示了一个$(1-\epsilon)$近似算法，其运行时间为$O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$，对于每个$\epsilon>0$都成立。我们还展示了对几个其他图论性质的类似结果。

    We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon>0$. We show similar results for several other graph theoretic propertie
    
[^95]: 语法引导综合的强化学习

    Reinforcement Learning for Syntax-Guided Synthesis. (arXiv:2307.09564v1 [cs.AI])

    [http://arxiv.org/abs/2307.09564](http://arxiv.org/abs/2307.09564)

    本论文提出了一种基于树搜索和强化学习的语法引导综合算法，解决了搜索问题复杂性和数据集较小的挑战。

    

    程序综合是根据规范自动生成代码的任务。在语法引导综合（SyGuS）中，规范是一个语法模板和一个逻辑公式的组合，生成的代码被证明满足规范。像SyGuS这样的技术对于确保正确的综合结果至关重要。尽管机器学习在其他类型的程序综合中已经得到广泛应用，但在SyGuS中目前的技术仍然主要依赖自动推理工具和简单的枚举方法。我们假设这是由于搜索问题的复杂性和可用数据集相对较小的原因。在这项工作中，我们通过将通用SyGuS问题构建为树搜索，并基于Monte-Carlo Tree Search (MCTS)提出了一种强化学习引导的SyGuS综合算法。我们的算法结合了学习的策略和值函数与用于平衡探索和利用的树的上限置信度。

    Program synthesis is the task of automatically generating code based on a specification. In Syntax-Guided Synthesis(SyGuS) this specification is a combination of a syntactic template and a logical formula, and any generated code is proven to satisfy both. Techniques like SyGuS are critical to guaranteeing correct synthesis results. Despite the proliferation of machine learning in other types of program synthesis, state-of-the-art techniques in SyGuS are still driven by automated reasoning tools and simple enumeration. We hypothesize this is for two reasons: first the complexity of the search problem, and second the relatively small data sets available. In this work, we tackle these challenges by framing general SyGuS problems as a tree-search, and present a reinforcement learning guided synthesis algorithm for SyGuS based on Monte-Carlo Tree Search (MCTS). Our algorithm incorporates learned policy and value functions combined with the upper confidence bound for trees to balance explora
    
[^96]: DecodingTrust: GPT模型的全面可信度评估

    DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11698](http://arxiv.org/abs/2306.11698)

    这项工作提出了对GPT模型进行全面可信度评估，考虑了多个方面的风险，发现了以前未公开的威胁漏洞，例如对毒性输出和个人信息泄漏的易被误导性。

    

    生成预训练变压器（GPT）模型在其能力方面取得了令人兴奋的进展，引起了从从业者到公众的兴趣。然而，尽管关于GPT模型的可信度的文献仍然有限，从业者们提议将强大的GPT模型用于敏感应用，如医疗保健和金融领域，其中错误可能代价高昂。为此，本研究提出了对大型语言模型（重点放在GPT-4和GPT-3.5上）进行全面的可信度评估，考虑了多样的观点 - 包括有毒性、陈规偏见、对抗强度、超出分布的强度、对抗示范的强度、隐私、机器伦理和公平性。根据我们的评估，我们发现了以前未公开的可信度威胁漏洞。例如，我们发现GPT模型可以轻松被误导生成有毒和偏见的输出，并在训练数据和上下文中泄漏私人信息。

    Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conv
    
[^97]: 面向可信自动系统开发的RE中心化建议

    RE-centric Recommendations for the Development of Trustworthy(er) Autonomous Systems. (arXiv:2306.01774v1 [cs.CY])

    [http://arxiv.org/abs/2306.01774](http://arxiv.org/abs/2306.01774)

    本研究发现目前AI系统开发中缺少要求工程（RE）这一环节且伦理指南术语和原则覆盖的不一致性，为解决该问题我们制定了一个术语表并研究了伦理AI开发框架在执行RE方面的适用性。

    

    在欧盟内开发和实施人工智能系统时符合欧盟AI法案（AIA）指南将很快是强制性的。然而，从行动指南方面，实践者缺乏在AI系统开发期间实施伦理的可操作说明。对不同伦理指南的文献综述揭示了所涉及原则和术语描述的不一致性。此外，要在AI开发过程的早期阶段培养信任的要求工程（RE）被证明在许多支持道德和可信AI开发的框架中缺失。这种不协调的措辞加上缺乏具体的开发实践使可信AI开发更加困难。为解决此问题，我们制定了一个术语表，用于比较主要伦理AI指南中使用的术语和伦理AI原则的覆盖范围。然后，我们研究了伦理AI开发框架在执行RE方面的适用性。

    Complying with the EU AI Act (AIA) guidelines while developing and implementing AI systems will soon be mandatory within the EU. However, practitioners lack actionable instructions to operationalise ethics during AI systems development. A literature review of different ethical guidelines revealed inconsistencies in the principles addressed and the terminology used to describe them. Furthermore, requirements engineering (RE), which is identified to foster trustworthiness in the AI development process from the early stages was observed to be absent in a lot of frameworks that support the development of ethical and trustworthy AI. This incongruous phrasing combined with a lack of concrete development practices makes trustworthy AI development harder. To address this concern, we formulated a comparison table for the terminology used and the coverage of the ethical AI principles in major ethical AI guidelines. We then examined the applicability of ethical AI development frameworks for perfo
    
[^98]: DeepMerge: 基于深度学习的区域合并方法用于图像分割

    DeepMerge: Deep Learning-Based Region-Merging for Image Segmentation. (arXiv:2305.19787v1 [cs.CV])

    [http://arxiv.org/abs/2305.19787](http://arxiv.org/abs/2305.19787)

    本文提出一种名为DeepMerge的基于深度学习的区域合并方法，用于处理高分辨率图像分割问题，并成功使用远程感知数据集进行了验证。

    

    在图像分析中，从高分辨率遥感图像中精确地分割大区域仍然是一个具有挑战性的问题。现有的有监督和无监督方法都受到对象尺寸巨大变化和尺度选择困难的影响，经常导致分割精度不佳。为了应对上述挑战，我们提出了一种基于深度学习的区域合并方法（DeepMerge），通过将Transformers、多级嵌入模块、基于段的特征嵌入模块和区域相邻图模型相结合来处理大的高分辨率图像分割。此外，我们提出了一种改进的二叉树采样方法，以生成多级输入，用于DeepMerge模型的输入。根据我们的最佳知识，提出的方法是第一个使用深度学习学习相邻段之间相似性进行区域合并的方法。我们使用远程感知数据集对所提出的DeepMerge方法进行验证。

    Accurate segmentation of large areas from very high spatial-resolution (VHR) remote sensing imagery remains a challenging issue in image analysis. Existing supervised and unsupervised methods both suffer from the large variance of object sizes and the difficulty in scale selection, which often result in poor segmentation accuracies. To address the above challenges, we propose a deep learning-based region-merging method (DeepMerge) to handle the segmentation in large VHR images by integrating a Transformer with a multi-level embedding module, a segment-based feature embedding module and a region-adjacency graph model. In addition, we propose a modified binary tree sampling method to generate multi-level inputs from initial segmentation results, serving as inputs for the DeepMerge model. To our best knowledge, the proposed method is the first to use deep learning to learn the similarity between adjacent segments for region-merging. The proposed DeepMerge method is validated using a remot
    
[^99]: 利用强化学习训练扩散模型

    Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13301](http://arxiv.org/abs/2305.13301)

    本文研究了利用强化学习方法直接优化扩散模型以实现下游对象的问题，并提出一种称之为去噪扩散策略优化（DDPO）的有效策略梯度算法，能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。

    

    扩散模型是一类灵活的生成模型，采用对数似然目标的近似训练。然而，大多数扩散模型的使用案例并不关注似然，而是关注人类感知的图像质量或药物效力等下游目标。本文研究利用强化学习方法直接优化扩散模型以实现此类目标。我们描述了将去噪视为多步决策问题的方法，并提出称之为去噪扩散策略优化（DDPO）的一类策略梯度算法，相对于替代的奖励加权似然方法更为有效。在实证研究中，DDPO能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。最后，我们展示DDPO可以利用来自反馈的提示-图像对齐方式来进行优化。

    Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
    
[^100]: 通过邻居引导的标签精炼协同学习实现无监督可见-红外人员再识别

    Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement. (arXiv:2305.12711v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.12711](http://arxiv.org/abs/2305.12711)

    本论文提出了一个双重最优传输标签分配(DOTLA)框架，以同时将一个模态中生成的标签分配给其对应的模态，实现无监督可见-红外人员再识别。在相应模态中邻居样本的指导下，还提出了一个跨模态邻居一致性引导的标签精炼和正则化模块，进一步提高了算法的精度和鲁棒性。

    

    无监督学习可见-红外人员再识别(USL-VI-ReID)旨在从未标记的跨模态数据集中学习模态不变特征，这在视频监控系统等实际应用中至关重要。解决跨模态数据关联问题对于进一步进行异质联合学习非常关键。针对这个问题，我们提出了一个双重最优传输标签分配(DOTLA)框架，同时将一个模态中生成的标签分配给其对应的模态。所提出的DOTLA机制formulate了一种相互增强和高效的跨模态数据关联解决方案，可以有效地减少一些不足和噪声标签关联的副作用。此外，我们还提出了一个跨模态邻居一致性引导的标签精炼和正则化模块，在相应模态中邻居样本的指导下消除由不准确的监督信号带来的负面影响。在两个基准数据集上的大量实验证明，所提出的USL-VI-ReID模型与现有的无监督方法甚至一些有监督方法相比，实现了最先进的性能。

    Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims at learning modality-invariant features from unlabeled cross-modality dataset, which is crucial for practical applications in video surveillance systems. The key to essentially address the USL-VI-ReID task is to solve the cross-modality data association problem for further heterogeneous joint learning. To address this issue, we propose a Dual Optimal Transport Label Assignment (DOTLA) framework to simultaneously assign the generated labels from one modality to its counterpart modality. The proposed DOTLA mechanism formulates a mutual reinforcement and efficient solution to cross-modality data association, which could effectively reduce the side-effects of some insufficient and noisy label associations. Besides, we further propose a cross-modality neighbor consistency guided label refinement and regularization module, to eliminate the negative effects brought by the inaccurate supervised signals, under th
    
[^101]: MC-ViViT: 多分支分类器-ViViT用于使用面部视频检测老年人轻度认知障碍

    MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])

    [http://arxiv.org/abs/2304.05292](http://arxiv.org/abs/2304.05292)

    本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。

    

    深度机器学习模型包括卷积神经网络(CNN)已成功地应用于使用医学图像、问卷和视频检测轻度认知障碍(MCI)。本文提出了一种新的多分支分类器-视频视觉变换器(MC-ViViT)模型，通过分析面部特征区分MCI和正常认知。数据来自I-CONECT，一个旨在通过提供频繁视频聊天来改善认知功能的行为干预试验。MC-ViViT在一个分支中提取视频的时空特征，并通过MC模块增强表示。由于I-CONECT数据集中的样本不平衡问题（包含难易和正负样本），这使MC-ViViT的性能受到影响。我们提出了一种Hard-Easy和Positive-Negative样本的损失函数（HP Loss）来结合对比度调节损失Focal loss和AD-CORRE loss来解决不平衡问题。我们在I-CONECT数据集上的实验结果显示出该算法的有效性。

    Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
    
[^102]: 手术聚合：一种用于协同学习的分布式医学影像数据和多样任务协调框架

    Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.06683](http://arxiv.org/abs/2301.06683)

    本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。

    

    大规模的胸部X光数据集已经通过深度学习进行异常检测，并有潜力为许多临床应用提供巨大的益处。然而，每个数据集仅专注于检测患者可能同时出现的一部分发现，从而限制了其临床效用。因此，数据协调对于聚合这些数据集来训练具有完整胸部内可能出现的所有异常的临床实用、强大模型至关重要。为此，我们提出了手术聚合，一种协同学习框架，用于协调和聚合分布式异构数据集的知识，并带有部分疾病注释。我们在合成的iid数据集和具有部分注释的真实大规模非iid数据集上评估了手术聚合。我们的结果表明，手术聚合显著优于当前的策略，具有更好的通用性。

    Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
    
[^103]: 快速iTPN：具有令牌迁移的整体预训练Transformer金字塔网络

    Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration. (arXiv:2211.12735v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12735](http://arxiv.org/abs/2211.12735)

    我们提出了Fast-iTPN，这是一个整体预训练Transformer金字塔网络，通过令牌迁移和令牌聚集等灵活设计来减少计算内存开销和加速推断。基于ImageNet-1K和COCO数据集，Fast-iTPN在图像分类和目标检测任务中达到了很好的性能。

    

    我们提出了一种整体预训练的Transformer金字塔网络（iTPN），旨在共同优化网络骨干和瓶颈，以使表示模型与下游任务之间的转移差距最小化。iTPN具有两个精心设计：1）基于视觉Transformer（ViT）的第一个预训练特征金字塔。2）使用遮蔽特征建模（MFM）对特征金字塔进行多阶段监督。Fast-iTPN是iTPN的升级版，通过两个灵活的设计减少了计算内存开销并加速推断。1）令牌迁移：舍弃骨干的冗余标记，同时在特征金字塔中进行补充，无需注意力操作。2）令牌聚集：通过引入少量的聚集标记减少全局注意力引起的计算成本。基于ImageNet-1K数据集，基本/高级的Fast-iTPN分别达到88.75%/89.5%的top-1准确率。在COCO目标检测任务中，使用DINO的1x训练计划，基本/高级的Fast-iTPN分别达到58.4%/58.8%的框AP。

    We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, an
    
[^104]: 变分量子和量子启发式聚类

    Variational Quantum and Quantum-Inspired Clustering. (arXiv:2206.09893v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2206.09893](http://arxiv.org/abs/2206.09893)

    这个论文提出了一种基于变分量子电路的量子聚类算法。通过将聚类问题转化为优化问题，并通过变分量子本征求解器（VQE）结合非正交量子比特状态来解决，可以有效地在NISQ设备上实现。数值模拟结果表明，即使只有一个量子比特，算法也具有出色的性能。此外，通过张量网络模拟算法，可以在经典硬件上运行量子启发式聚类算法。

    

    在这里，我们提出了一种基于变分量子电路的量子聚类算法。该算法可以将数据分类为多个聚类，并且可以在少量量子比特的噪声中间尺度量子（NISQ）设备上轻松实现。该算法的思想是将聚类问题缩减为一个优化问题，然后通过变分量子本征求解器（VQE）与非正交量子比特状态相结合来解决它。实际上，该方法使用目标希尔伯特空间的最大正交状态而不是通常的计算基础，即使有少量量子比特也可以考虑大量聚类。我们使用真实数据集进行数值模拟来评估算法的性能，结果表明即使只有一个量子比特，算法的性能也很好。此外，通过张量网络模拟算法，我们可以构建一个可以在当前经典硬件上运行的量子启发式聚类算法。

    Here we present a quantum algorithm for clustering data based on a variational quantum circuit. The algorithm allows to classify data into many clusters, and can easily be implemented in few-qubit Noisy Intermediate-Scale Quantum (NISQ) devices. The idea of the algorithm relies on reducing the clustering problem to an optimization, and then solving it via a Variational Quantum Eigensolver (VQE) combined with non-orthogonal qubit states. In practice, the method uses maximally-orthogonal states of the target Hilbert space instead of the usual computational basis, allowing for a large number of clusters to be considered even with few qubits. We benchmark the algorithm with numerical simulations using real datasets, showing excellent performance even with one single qubit. Moreover, a tensor network simulation of the algorithm implements, by construction, a quantum-inspired clustering algorithm that can run on current classical hardware.
    
[^105]: 稳定LIF神经元训练

    Stabilizing the LIF Neuron Training. (arXiv:2202.00282v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2202.00282](http://arxiv.org/abs/2202.00282)

    该论文研究了稳定LIF神经元训练的方法，通过实验和理论分析，确定了在不同任务和网络中选择最佳替代梯度的稳定性与效果的关系，减少了对超参数搜索的需求。

    

    脉冲神经形态计算利用二进制活动来提高人工智能的能源效率。然而，二进制活动的非平滑性要求使用近似梯度，也称为替代梯度（SG），以弥合与深度学习的性能差距。文献中已提出了几种SG，但目前尚不清楚如何确定适合特定任务和网络的最佳SG。在昂贵的超参数搜索后，大多数SG形状都可以实现良好的性能。因此，我们旨在在不同的压力测试中实验证明最佳SG，并在实验和理论上减少未来对网格搜索的需求。为了理解该领域的差距，我们展示了更复杂的任务和网络需要更慎重地选择SG，即使整体上，快速Sigmoid函数的导数在各种学习率下表现优于其他SG。因此，在训练之前，我们设计了一种基于稳定性的理论方法来选择初始化和SG形状。

    Spiking Neuromorphic Computing uses binary activity to improve Artificial Intelligence energy efficiency. However, the non-smoothness of binary activity requires approximate gradients, known as Surrogate Gradients (SG), to close the performance gap with Deep Learning. Several SG have been proposed in the literature, but it remains unclear how to determine the best SG for a given task and network. Good performance can be achieved with most SG shapes, after a costly search of hyper-parameters. Thus, we aim at experimentally and theoretically define the best SG across different stress tests, to reduce future need of grid search. To understand the gap for this line of work, we show that more complex tasks and networks need more careful choice of SG, even if overall the derivative of the fast sigmoid outperforms other SG across tasks and networks, for a wide range of learning rates. We therefore design a stability based theoretical method to choose initialization and SG shape before trainin
    
[^106]: INVIGORATE: 互动视觉场景理解和抓取

    INVIGORATE: Interactive Visual Grounding and Grasping in Clutter. (arXiv:2108.11092v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2108.11092](http://arxiv.org/abs/2108.11092)

    INVIGORATE是一个通过自然语言与人类进行交互并在杂乱环境中抓取指定物体的机器人系统。该系统能够推断目标物体、推断物体阻挡关系，并生成多步计划来消除歧义并成功抓取目标物体。

    

    本文介绍了INVIGORATE，一个通过自然语言与人类进行交互并在杂乱环境中抓取指定物体的机器人系统。在这种杂乱环境中，物体可能会相互遮挡、阻挡甚至叠放在一起。INVIGORATE面临着几个挑战：（i）从输入的语言表达和RGB图像中推断出目标物体，而忽略其他遮挡物体；（ii）从图像中推断出物体的阻挡关系（OBR）；（iii）生成一个多步计划，通过提问消除目标物体的歧义并成功抓取它。我们为目标检测、视觉场景理解、问题生成和OBR检测以及抓取训练了独立的神经网络。它们可以处理任意的物体类别和语言表达，只要有相应的训练数据集。然而，视觉感知中的误差以及人类语言中的歧义不可避免地会对机器人的性能产生负面影响。为了克服这些不确定性，我们建立了一个部分可观察的马尔可夫决策模型。

    This paper presents INVIGORATE, a robot system that interacts with human through natural language and grasps a specified object in clutter. The objects may occlude, obstruct, or even stack on top of one another. INVIGORATE embodies several challenges: (i) infer the target object among other occluding objects, from input language expressions and RGB images, (ii) infer object blocking relationships (OBRs) from the images, and (iii) synthesize a multi-step plan to ask questions that disambiguate the target object and to grasp it successfully. We train separate neural networks for object detection, for visual grounding, for question generation, and for OBR detection and grasping. They allow for unrestricted object categories and language expressions, subject to the training datasets. However, errors in visual perception and ambiguity in human languages are inevitable and negatively impact the robot's performance. To overcome these uncertainties, we build a partially observable Markov decis
    

