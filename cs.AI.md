# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multiple Hands Make Light Work: Enhancing Quality and Diversity using MAP-Elites with Multiple Parallel Evolution Strategies.](http://arxiv.org/abs/2303.06137) | 本论文提出了一种新的QD算法——MAP-Elites-Multi-ES（MEMES），该算法基于进化策略（ES）和快速并行评估，通过维护具有大规模并行性的多个独立ES线程来扩展现有算法。实验表明，该算法在代数方面表现更好，是一种有效的算法创新。 |
| [^2] | [Rewarding Chatbots for Real-World Engagement with Millions of Users.](http://arxiv.org/abs/2303.06135) | 本研究提出了一种奖励系统来训练优秀的聊天机器人，利用用户反馈数据去筛选输出来提高保留率，A/B测试表明该方法能提高68%的保留率。 |
| [^3] | [Self-supervised Training Sample Difficulty Balancing for Local Descriptor Learning.](http://arxiv.org/abs/2303.06124) | 本文提出了一种自监督训练样本难度平衡的局部描述符学习方法，在正负样本不平衡的情况下，通过平衡损失对负样本的质量进行有效的鉴别，并使用动态梯度调节来实现样本难度的平衡。 |
| [^4] | [Ignorance is Bliss: Robust Control via Information Gating.](http://arxiv.org/abs/2303.06121) | 本论文提出了信息门控的概念，即学习用于解决特定任务的最小信息掩码，以实现更简洁的表示。该方法适用于各种目标，并且可以改善算法性能。 |
| [^5] | [On the Fusion Strategies for Federated Decision Making.](http://arxiv.org/abs/2303.06109) | 本文考虑了联邦决策中信息聚合的问题，建立了汇集策略都导致系统的渐近正态性描述，用于计算错误概率，并通过模拟验证了理论结果。 |
| [^6] | [HiNet: A Novel Multi-Scenario & Multi-Task Learning Approach with Hierarchical Information Extraction.](http://arxiv.org/abs/2303.06095) | HiNet是一种用于多场景和多任务推荐的分层信息提取网络，能够增强跨场景传递有价值信息的能力，同时保留场景和任务的特定特征。 |
| [^7] | [An End-to-End Neural Network for Image-to-Audio Transformation.](http://arxiv.org/abs/2303.06078) | 本文提出了一种端到端的图像到语音神经网络，用于在低资源设备上呈现小部分显示内容的音频渲染，解决了硬件层面上视障或视觉分散用户的可访问性问题，并且比非E2E方法更高效，使用的参数更少，降低了电话准确率。 |
| [^8] | [EEG Synthetic Data Generation Using Probabilistic Diffusion Models.](http://arxiv.org/abs/2303.06068) | 本文提出一种使用概率扩散模型生成脑电合成数据的方法，从带有情感标签的EEG记录的电极-频率分布图中生成合成数据。 |
| [^9] | [Non-invasive Waveform Analysis for Emergency Triage via Simulated Hemorrhage: An Experimental Study using Novel Dynamic Lower Body Negative Pressure Model.](http://arxiv.org/abs/2303.06064) | 本研究使用新颖的动态下半身负压模型模拟持续性失血水平，并通过深度学习（DL）框架分类鉴别不同程度的失血水平，为急诊分级提供新思路。 |
| [^10] | [TSMixer: An all-MLP Architecture for Time Series Forecasting.](http://arxiv.org/abs/2303.06053) | 本文提出了一种全MLP架构的时间序列预测方法TSMixer，通过混合操作，利用时间和特征维度高效提取信息，在M5基准测试中表现优于最新的替代方案 |
| [^11] | [Analysis and Evaluation of Explainable Artificial Intelligence on Suicide Risk Assessment.](http://arxiv.org/abs/2303.06052) | 本研究使用了可解释人工智能技术和数据增强技术来预测自杀风险，实验结果表明决策树（DT）、随机森林（RF）和极限梯度提升（XGBoost）模型取得了最佳结果。愤怒问题、抑郁症和社交隔离是预测自杀风险的主要变量，而收入好、职业受人尊敬和接受大学教育的患者风险最小。这些结果证明了机器学习和XAI框架在自杀风险预测方面的有效性，以及它们在医疗保健和公共卫生领域的应用。 |
| [^12] | [Optimal foraging strategies can be learned and outperform L\'evy walks.](http://arxiv.org/abs/2303.06050) | 本文证明最优觅食策略可以被生物学习，实验表明学习到的策略优于已知策略如利维步行。 |
| [^13] | [Affordable Artificial Intelligence -- Augmenting Farmer Knowledge with AI.](http://arxiv.org/abs/2303.06049) | 这篇论文介绍了一种预测农场微气候条件的AI技术，该技术将农业实践与数据分析相结合，有助于增强农民对土地的深入了解，使农业生产更可持续和有利可图 |
| [^14] | [Tactile-Filter: Interactive Tactile Perception for Part Mating.](http://arxiv.org/abs/2303.06034) | 本文提出了一种使用基于视觉的触觉传感器进行多对象组装的交互式感知方法，其中机器人使用触觉传感器和使用粒子滤波器的反馈机制逐步改进其对适合组装的对象的估计。 |
| [^15] | [HARDC : A novel ECG-based heartbeat classification method to detect arrhythmia using hierarchical attention based dual structured RNN with dilated CNN.](http://arxiv.org/abs/2303.06020) | 本文提出了一种新的心律失常分类方法，称为HARDC，通过利用扩张CNN和双向递归神经网络单元来生成融合特征，加上注意力机制，显著提高了模型的性能和可解释性。 |
| [^16] | [Hierarchical Neural Program Synthesis.](http://arxiv.org/abs/2303.06018) | 本研究提出了一种分层组合程序来合成程序的可扩展程序合成框架，这能够避免从头合成长或更复杂的程序。 |
| [^17] | [Modelling Projection Bias in Intertemporal Choices: A Prospect Theory Based Approach.](http://arxiv.org/abs/2303.06016) | 本研究提出了一种Pobe模型，以展望理论分析用户的非理性决策，并计算用户的投影偏误，从而可以准确预测用户的选择。 |
| [^18] | [New Benchmarks for Accountable Text-based Visual Re-creation.](http://arxiv.org/abs/2303.05983) | 该论文定义了一种新的基于文本的视觉重现任务，并构建了新的合成CLEVR-NOT数据集和手动绘制的Fruit-NOT数据集。该方法能够对禁止的指令进行负责并说明理由。 |
| [^19] | [Classifying the evolution of COVID-19 severity on patients with combined dynamic Bayesian networks and neural networks.](http://arxiv.org/abs/2303.05972) | 本研究采用动态贝叶斯网络和神经网络，对COVID-19患者的病情进行预测分类，该模型具有较高的分类准确性。 |
| [^20] | [Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning.](http://arxiv.org/abs/2303.05952) | 对比损失使模态匹配，但完美的模态对齐不利于下游预测任务。 本文提出有意义的潜在模态结构是更好的表现关键。 作者设计了三种通用方法，分别是用于模内正则化的深度特征分离损失，用于模间正则化的布朗运动桥损失以及用于模内和模间正则化的几何一致性损失。 经过广泛实验验证。 |
| [^21] | [Accurate Real-time Polyp Detection in Videos from Concatenation of Latent Features Extracted from Consecutive Frames.](http://arxiv.org/abs/2303.05871) | 本文提出了一种有效的特征串联方法，通过将前一帧的特征映射并入当前帧来检测息肉，以此提高视频中自动息肉检测的性能。 |
| [^22] | [A Rule Based Theorem Prover: an Introduction to Proofs in Secondary Schools.](http://arxiv.org/abs/2303.05863) | 本文介绍了一种基于规则的定理证明器，其使用几何推理数据库方法实现，可以帮助中学生学习证明，对学生进行证明教学是有益的。 |
| [^23] | [Decision-Making Under Uncertainty: Beyond Probabilities.](http://arxiv.org/abs/2303.05848) | 本文讨论超越传统概率理解的不确定性决策，介绍了解决部分可观测性和对抗行为的马尔科夫决策过程及其扩展。此外，本文还展示了多种解决离散和连续模型的方法。 |
| [^24] | [Contrastive Language-Image Pretrained (CLIP) Models are Powerful Out-of-Distribution Detectors.](http://arxiv.org/abs/2303.05828) | 研究表明，对比语言-图像预训练模型无需内部分布微调即可实现最先进的无监督越界检测表现，并提出是否需要新的视觉异常检测基准 |
| [^25] | [Knowledge Transfer via Multi-Head Feature Adaptation for Whole Slide Image Classification.](http://arxiv.org/abs/2303.05780) | 提出了一种多头特征自适应模块，用于从源领域到目标领域的知识迁移。WSI分类是该方法的理想测试平台。实验结果表明，具有知识迁移的模型优于从头开始训练的模型，无论数据集中的WSI数量如何。 |
| [^26] | [Automatic Detection and Rectification of Paper Receipts on Smartphones.](http://arxiv.org/abs/2303.05763) | 本文描述了一种智能手机应用程序的开发，允许用户通过“挥动”手机将纸质收据数字化，并自动检测和校正以进行后续的文本识别。 作者提出了一种创新的解决方案，通过将每个四个角作为一个独特的“对象”，并训练单次检测MobileNet对象检测模型来处理收据角的检测，以解决传统方法在检测和纠正收据方面的问题。实验结果表明，该方法具有更好的性能。 |
| [^27] | [Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition.](http://arxiv.org/abs/2303.05754) | 本文提出了一种快速扩散采样策略，采用几何分解的方法解决逆问题中条件采样的限制问题。研究者们发现扩散模型生成的样本可以分解为去噪和噪声两个部分，这使得施加条件的共轭梯度更新属于干净流形，从而提高了采样速度。 |
| [^28] | [QVRF: A Quantization-error-aware Variable Rate Framework for Learned Image Compression.](http://arxiv.org/abs/2303.05744) | 该论文提出了一种量化误差感知的变速率框架，利用单变量量化调节器实现单模型内广泛的可变比特率，同时实现了离散可变比特率。在实验证明，该框架可以实现广泛的连续可变比特率而不会出现显著的性能降级。 |
| [^29] | [3D Cinemagraphy from a Single Image.](http://arxiv.org/abs/2303.05724) | 本文提出了一种新技术3D Cinemagraphy，给定一张单照片作为输入，可以生成包含视觉内容动画和相机动作的视频。通过将场景在3D空间中表示和动画化，解决了现有2D图像动画和3D摄影方法组合带来的伪影或不一致的动画问题。 |
| [^30] | [Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity.](http://arxiv.org/abs/2303.05689) | 本文提出了一种新的方法，在深度神经网络中利用神经坍塌现象来减少错误严重程度。与传统的ETF不同，本文采用层次感知框架(HAFrame)来固定线性分类器，并使用余弦相似度辅助损失来训练层次感知倒数第二特征。实验结果表明该方法有效降低了模型预测的错误严重程度。 |
| [^31] | [Rearrange Indoor Scenes for Human-Robot Co-Activity.](http://arxiv.org/abs/2303.05676) | 我们提出了一种室内家具的重组框架，旨在为机器人活动提供足够的可访问空间，同时不影响日常人类活动。实验显示，这种方法可以增加14%的可访问空间和30%的可交互对象。 |
| [^32] | [Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning.](http://arxiv.org/abs/2303.05670) | 本文研究了预训练的句子编码器中存在的带有陈规陋习的刻板印象，并比较了与之对应的文本蕴涵模型。研究发现，采用文本蕴涵的明确逻辑学习可以显著减少偏差，并提高社交社区的识别能力，而无需明确去偏见的过程。 |
| [^33] | [UNFUSED: UNsupervised Finetuning Using SElf supervised Distillation.](http://arxiv.org/abs/2303.05668) | UnFuSeD是一种使用自监督学习和无监督微调的方法，将自监督学习和无监督微调相结合，使用伪标签进行微调，提出了第一个脱离文献中通用自监督学习范例的系统 |
| [^34] | [GATOR: Graph-Aware Transformer with Motion-Disentangled Regression for Human Mesh Recovery from a 2D Pose.](http://arxiv.org/abs/2303.05652) | GATOR提出了一种新的方法来捕捉从骨架到网格的多个关系，包括关节-关节，关节-顶点和顶点-顶点关系，并利用运动解耦回归来探索关节和顶点关系。GATOR在两个数据集上实现了最先进的性能。 |
| [^35] | [Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals.](http://arxiv.org/abs/2303.05648) | 本文提出了一个上下文依赖偏好模型Pacos，针对偏好反转问题同时解决用户自适应权重、项目间比较和显示位置三个因素，该模型包括具有高解释性的加法方法和具有高准确性的基于ANN的方法。 |
| [^36] | [Efficient Real Time Recurrent Learning through combined activity and parameter sparsity.](http://arxiv.org/abs/2303.05641) | 本文介绍了一种递归网络的训练算法，该算法可用于实现高效实时递归学习。该算法可以显著减少计算成本，并在低资源实时系统上实现。 |
| [^37] | [Explainable Goal Recognition: A Framework Based on Weight of Evidence.](http://arxiv.org/abs/2303.05622) | 该论文介绍和评估了一种使用证据权重框架的可解释目标识别模型，可以提供人类中心的解释并且在八个不同的领域表现良好。经过人类行为研究验证该模型可以成功生成类似人类的解释。 |
| [^38] | [KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Pose Synthesis on RGB-D input.](http://arxiv.org/abs/2303.05617) | 本文提出了一种基于关键点的RGB-D输入下的新型六自由度抓取姿态合成方法，通过设计新的抓取生成网络，减少了对精确关键点估计的依赖性，该方法表现更好，验证了该方法的有效性。 |
| [^39] | [Learning the Wrong Lessons: Inserting Trojans During Knowledge Distillation.](http://arxiv.org/abs/2303.05593) | 本文利用知识蒸馏的未标记数据嵌入木马，最终成功地设计出可以降低学生模型准确度而不影响教师模型的Trojan攻击 |
| [^40] | [SOCIALGYM 2.0: Simulator for Multi-Agent Social Robot Navigation in Shared Human Spaces.](http://arxiv.org/abs/2303.05584) | 本论文提出了一个名为SocialGym 2的多智能体导航模拟器，用于社交机器人研究。它采用多智能体强化学习（MARL）来开发多个具有不同、动态约束的机器人的最佳导航策略，提供易于访问的python接口，并提供了交换和评估不同MARL算法的能力。 |
| [^41] | [Open World Classification with Adaptive Negative Samples.](http://arxiv.org/abs/2303.05581) | 本文提出了一种基于自适应负样本的方法，旨在在培训阶段生成有效的合成开放类别样本，用于解决开放世界分类任务中的挑战。实证结果表明，使用辅助的一对多二进制分类器具有显着优势。 |
| [^42] | [Evaluating the Robustness of Conversational Recommender Systems by Adversarial Examples.](http://arxiv.org/abs/2303.05575) | 本文提出了一个对抗性评估方案，包括两个类别的四个方案，并自动生成对抗性示例来评估会话式推荐系统在面对不同输入数据时的鲁棒性。结果表明，现有的系统都不够鲁棒和可靠以抵御对抗性示例。 |
| [^43] | [Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors.](http://arxiv.org/abs/2303.05546) | 本文提出了一种仅使用图像级交互标签、预训练的视觉-语言模型（VLM）和大型语言模型（LLM）来处理HOI检测的方法。通过利用视觉-语言模型的定位能力来修剪非交互人和物体的建议，从而提高袋内正样本对的质量。 |
| [^44] | [Computably Continuous Reinforcement-Learning Objectives are PAC-learnable.](http://arxiv.org/abs/2303.05518) | 本文研究了强化学习中通用目标的PAC可学习性，并证明了只要该目标是一致连续的或可计算连续的，则它就是PAC可学习的 |

# 详细

[^1]: 多手轻松完成：使用多个并行进化策略的MAP-Elites增强质量和多样性

    Multiple Hands Make Light Work: Enhancing Quality and Diversity using MAP-Elites with Multiple Parallel Evolution Strategies. (arXiv:2303.06137v1 [cs.NE])

    [http://arxiv.org/abs/2303.06137](http://arxiv.org/abs/2303.06137)

    随着硬件加速器及其相应工具的发展，在某些应用中通过快速和大规模的评估，评估已变得更加经济实惠。这种进展极大地加快了启发式算法（如质量-多样性优化）的运行时，并创造了通过规模进行算法创新的巨大潜力。在这项工作中，我们提出了MAP-Elites-Multi-ES（MEMES），这是一种基于进化策略（ES）的新型QD算法，专为快速并行评估而设计。 ME-Multi-ES在现有的MAP-Elites-ES算法基础上进行了改进，通过维护具有大规模并行性的多个独立ES线程来扩展它。我们还介绍了一种新的动态重置程序，用于自主最大化QD群体的改进。实验表明，与基于梯度和客观不可知的QD算法相比，MEMES在代数方面表现更好。

    With the development of hardware accelerators and their corresponding tools, evaluations have become more affordable through fast and massively parallel evaluations in some applications. This advancement has drastically sped up the runtime of evolution-inspired algorithms such as Quality-Diversity optimization, creating tremendous potential for algorithmic innovation through scale. In this work, we propose MAP-Elites-Multi-ES (MEMES), a novel QD algorithm based on Evolution Strategies (ES) designed for fast parallel evaluations. ME-Multi-ES builds on top of the existing MAP-Elites-ES algorithm, scaling it by maintaining multiple independent ES threads with massive parallelization. We also introduce a new dynamic reset procedure for the lifespan of the independent ES to autonomously maximize the improvement of the QD population. We show experimentally that MEMES outperforms existing gradient-based and objective-agnostic QD algorithms when compared in terms of generations. We perform thi
    
[^2]: 用于百万用户真实世界互动的聊天机器人的奖励系统

    Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v1 [cs.CL])

    [http://arxiv.org/abs/2303.06135](http://arxiv.org/abs/2303.06135)

    预训练的大型语言模型的出现导致了一系列的社交聊天机器人的部署，用于闲聊。虽然这些聊天机器人展示了语言能力和流利度，但它们并不一定引人入胜，有时候很难吸引用户。本研究调查了开发注重用户参与度的社交聊天机器人以增强保留率，特别是考察了使用人类反馈来高效开发极具吸引力的聊天机器人。所提出的方法利用从用户交互中收集的自动伪标签来训练一个奖励模型，在推理时可以用来拒绝聊天机器人模型产生的低分样本响应。引入了直观的评估指标，如平均对话长度 (MCL)，作为衡量部署聊天机器人的参与水平的代理。在Chai Research平台上，对每日新的10,000个聊天机器人用户组进行的A/B测试表明，这种方法将MCL提高了最多70％，这相当于将保留率从40％增加到68％。

    The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
    
[^3]: 自监督训练样本难度平衡的局部描述符学习

    Self-supervised Training Sample Difficulty Balancing for Local Descriptor Learning. (arXiv:2303.06124v1 [cs.CV])

    [http://arxiv.org/abs/2303.06124](http://arxiv.org/abs/2303.06124)

    在正负样本不平衡的情况下，硬负样本挖掘策略已被证明有助于模型学习正负样本之间更微妙的差异，从而提高识别性能。然而，如果在数据集中推广过于严格的挖掘策略，可能存在引入假负样本的风险。同时，挖掘策略的实施破坏了真实数据集中样本的难度分布，这可能导致模型过度拟合这些难的样本。因此，本文研究了如何权衡挖掘样本的难度，以获得和利用高质量的负样本，并试图从损失函数和训练策略两方面解决问题。所提出的平衡损失通过将自监督方法与损失函数相结合，提供了负样本质量的有效鉴别，并使用动态梯度调节来实现样本难度的平衡。

    In the case of an imbalance between positive and negative samples, hard negative mining strategies have been shown to help models learn more subtle differences between positive and negative samples, thus improving recognition performance. However, if too strict mining strategies are promoted in the dataset, there may be a risk of introducing false negative samples. Meanwhile, the implementation of the mining strategy disrupts the difficulty distribution of samples in the real dataset, which may cause the model to over-fit these difficult samples. Therefore, in this paper, we investigate how to trade off the difficulty of the mined samples in order to obtain and exploit high-quality negative samples, and try to solve the problem in terms of both the loss function and the training strategy. The proposed balance loss provides an effective discriminant for the quality of negative samples by combining a self-supervised approach to the loss function, and uses a dynamic gradient modulation st
    
[^4]: 无知即福：通过信息门控实现鲁棒控制

    Ignorance is Bliss: Robust Control via Information Gating. (arXiv:2303.06121v1 [cs.LG])

    [http://arxiv.org/abs/2303.06121](http://arxiv.org/abs/2303.06121)

    信息简洁性——即使用任务所需的最小信息——为学习表示提供了有用的归纳偏置，从而通过对噪声和伪相关性具有鲁棒性而实现更好的泛化。我们提出了像素空间中的信息门控作为学习更简洁表示的一种方法。信息门控的作用是学习捕捉解决给定任务所需的最小信息的掩码。直观地说，我们的模型学习识别哪些视觉线索对于给定任务实际上很重要。我们使用信噪比的可微参数化对信号进行门控，该信噪比可应用于网络中的任意值，例如屏蔽输入层的像素。我们将我们的方法称为 InfoGating，并将其应用于各种目标，例如：多步前向和逆向动力学、Q学习、行为克隆和标准自监督任务。我们的实验表明，学习识别和使用最小信息有助于改善性能。

    Informational parsimony -- i.e., using the minimal information required for a task, -- provides a useful inductive bias for learning representations that achieve better generalization by being robust to noise and spurious correlations. We propose information gating in the pixel space as a way to learn more parsimonious representations. Information gating works by learning masks that capture only the minimal information required to solve a given task. Intuitively, our models learn to identify which visual cues actually matter for a given task. We gate information using a differentiable parameterization of the signal-to-noise ratio, which can be applied to arbitrary values in a network, e.g.~masking out pixels at the input layer. We apply our approach, which we call InfoGating, to various objectives such as: multi-step forward and inverse dynamics, Q-learning, behavior cloning, and standard self-supervised tasks. Our experiments show that learning to identify and use minimal information 
    
[^5]: 关于联邦决策中融合策略的研究

    On the Fusion Strategies for Federated Decision Making. (arXiv:2303.06109v1 [cs.LG])

    [http://arxiv.org/abs/2303.06109](http://arxiv.org/abs/2303.06109)

    本文考虑了联邦决策中信息聚合的问题，该问题涉及一组代理协作以推断自然状态，而不与中央处理器或彼此共享私人数据。我们分析了非贝叶斯社交学习策略，在该策略中，代理将其个人观察结果与贝叶斯规则相结合，形成观点（即软决策），并且中央处理器通过算术或几何平均聚合这些观点。基于我们之前的工作，我们确定两种汇集策略都导致系统的渐近正态性描述，例如，可以利用这种描述近似计算错误概率的表达式。我们通过模拟验证了理论结果并比较了两种策略。

    We consider the problem of information aggregation in federated decision making, where a group of agents collaborate to infer the underlying state of nature without sharing their private data with the central processor or each other. We analyze the non-Bayesian social learning strategy in which agents incorporate their individual observations into their opinions (i.e., soft-decisions) with Bayes rule, and the central processor aggregates these opinions by arithmetic or geometric averaging. Building on our previous work, we establish that both pooling strategies result in asymptotic normality characterization of the system, which, for instance, can be utilized in order to give approximate expressions for the error probability. We verify the theoretical findings with simulations and compare both strategies.
    
[^6]: HiNet:一种新颖的多场景和多任务学习方法，具有分层信息提取

    HiNet: A Novel Multi-Scenario & Multi-Task Learning Approach with Hierarchical Information Extraction. (arXiv:2303.06095v1 [cs.IR])

    [http://arxiv.org/abs/2303.06095](http://arxiv.org/abs/2303.06095)

    多场景和多任务学习已被广泛应用于工业应用的许多推荐系统中，在Mixture-of-Expert（MoE）体系结构的基础上进行多场景迁移学习是一种有效和实用的方法。然而，以将所有信息投影到同一特征空间为目标的MoE方法无法有效地处理各种场景和任务之间固有的复杂关系，导致表现不佳。为了解决这个问题，我们提出了一个用于多场景和多任务推荐的分层信息提取网络（HiNet），它通过粗到细的知识传递方案实现分层提取。分层网络的多个提取层使模型能够增强跨场景传递有价值信息的能力，同时保留场景和任务的特定特征。此外，一种新颖的场景感知注意力网络

    Multi-scenario & multi-task learning has been widely applied to many recommendation systems in industrial applications, wherein an effective and practical approach is to carry out multi-scenario transfer learning on the basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based method, which aims to project all information in the same feature space, cannot effectively deal with the complex relationships inherent among various scenarios and tasks, resulting in unsatisfactory performance. To tackle the problem, we propose a Hierarchical information extraction Network (HiNet) for multi-scenario and multi-task recommendation, which achieves hierarchical extraction based on coarse-to-fine knowledge transfer scheme. The multiple extraction layers of the hierarchical network enable the model to enhance the capability of transferring valuable information across scenarios while preserving specific features of scenarios and tasks. Furthermore, a novel scenario-aware attentive netw
    
[^7]: 一种端到端的图像到音频转换神经网络

    An End-to-End Neural Network for Image-to-Audio Transformation. (arXiv:2303.06078v1 [eess.AS])

    [http://arxiv.org/abs/2303.06078](http://arxiv.org/abs/2303.06078)

    本文描述了一种端到端（E2E）神经结构，用于在低资源个人计算设备上呈现小部分显示内容的音频渲染。旨在解决硬件层面上视障或视觉分散用户的可访问性问题。回顾了神经图像到文本（ITT）和文本到语音（TTS）方法，并引入了一种新技术，以有效地将它们集成在一起，既高效又可反向传播，从而导致一种非自回归的E2E图像到语音（ITS）神经网络，这种神经网络具有高效和可训练性。实验结果表明，与非E2E方法相比，所提出的E2E系统速度提高了29％，参数减少了19％，电话准确率降低了2％。提出了一个未来方向来解决准确性问题。

    This paper describes an end-to-end (E2E) neural architecture for the audio rendering of small portions of display content on low resource personal computing devices. It is intended to address the problem of accessibility for vision-impaired or vision-distracted users at the hardware level. Neural image-to-text (ITT) and text-to-speech (TTS) approaches are reviewed and a new technique is introduced to efficiently integrate them in a way that is both efficient and back-propagate-able, leading to a non-autoregressive E2E image-to-speech (ITS) neural network that is efficient and trainable. Experimental results are presented showing that, compared with the non-E2E approach, the proposed E2E system is 29% faster and uses 19% fewer parameters with a 2% reduction in phone accuracy. A future direction to address accuracy is presented.
    
[^8]: 使用概率扩散模型生成脑电合成数据

    EEG Synthetic Data Generation Using Probabilistic Diffusion Models. (arXiv:2303.06068v1 [eess.SP])

    [http://arxiv.org/abs/2303.06068](http://arxiv.org/abs/2303.06068)

    脑电图（EEG）由于其无创、低成本和易于使用的特点在大脑计算机界颇具影响力，这使其成为公众广泛采用的高度理想的选项。本研究提出了一种先进的数据增强方法：使用去噪扩散概率模型生成合成EEG数据。合成数据是从带有情感标签的EEG记录的电极-频率分布图（EFDM）中生成的。为了评估所生成的合成数据的有效性，成功地进行了定性和定量与真实EEG数据的比较。 

    Electroencephalography (EEG) plays a significant role in the Brain Computer Interface (BCI) domain, due to its non-invasive nature, low cost, and ease of use, making it a highly desirable option for widespread adoption by the general public. This technology is commonly used in conjunction with deep learning techniques, the success of which is largely dependent on the quality and quantity of data used for training. To address the challenge of obtaining sufficient EEG data from individual participants while minimizing user effort and maintaining accuracy, this study proposes an advanced methodology for data augmentation: generating synthetic EEG data using denoising diffusion probabilistic models. The synthetic data are generated from electrode-frequency distribution maps (EFDMs) of emotionally labeled EEG recordings. To assess the validity of the synthetic data generated, both a qualitative and a quantitative comparison with real EEG data were successfully conducted. This study opens up
    
[^9]: 非侵入式波形分析用于通过模拟失血进行急诊分级：一项使用新颖的动态下半身负压模型的实验研究。(arXiv:2303.06064v1 [eess.SP])

    Non-invasive Waveform Analysis for Emergency Triage via Simulated Hemorrhage: An Experimental Study using Novel Dynamic Lower Body Negative Pressure Model. (arXiv:2303.06064v1 [eess.SP])

    [http://arxiv.org/abs/2303.06064](http://arxiv.org/abs/2303.06064)

    探索非侵入性生理信号的先进波形分析能够诊断失血程度的程度仍不足。本研究探讨了深度学习（DL）框架分类进行下半身负压（LBNP）模拟的持续性失血水平的鉴别能力。我们使用动态LBNP协议，而不是传统的模型，在该协议中，LBNP以可预测的逐步下降的方式施加。该动态LBNP版本有助于解决时间依赖性问题，因为在现实生活中的院前设置中，由于体积复苏，血管内的血容量可能会波动。通过分割底层非侵入式信号并使用相应的LBNP目标级别对片段进行标记，实现了用于三元分类的监督式DL框架。具有两个输入的DL模型经过训练

    The extent to which advanced waveform analysis of non-invasive physiological signals can diagnose levels of hypovolemia remains insufficiently explored. The present study explores the discriminative ability of a deep learning (DL) framework to classify levels of ongoing hypovolemia, simulated via novel dynamic lower body negative pressure (LBNP) model among healthy volunteers. We used a dynamic LBNP protocol as opposed to the traditional model, where LBNP is applied in a predictable step-wise, progressively descending manner. This dynamic LBNP version assists in circumventing the problem posed in terms of time dependency, as in real-life pre-hospital settings, intravascular blood volume may fluctuate due to volume resuscitation. A supervised DL-based framework for ternary classification was realized by segmenting the underlying noninvasive signal and labeling segments with corresponding LBNP target levels. The proposed DL model with two inputs was trained with respective time-frequency
    
[^10]: TSMixer：一种全MLP架构的时间序列预测方法

    TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v1 [cs.LG])

    [http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)

    实际的时间序列数据集通常是多变量且具有复杂的动态特性。传统的高容量体系结构，如基于递归或注意力的顺序模型已经变得流行。然而，最近的研究表明，简单的单变量线性模型可以胜过那些深度的替代方案。在本文中，我们调查了线性模型用于时间序列预测的能力，并提出了时间序列混合器（TSMixer），一种通过堆叠多层感知机（MLP）设计的体系结构。TSMixer基于沿时间和特征维度进行混合操作，以有效提取信息。在流行的学术基准测试中，易于实现的TSMixer与利用特定基准的归纳偏差的专业最新模型相当。在具有挑战性和大规模的M5基准测试中，即实际的零售数据集上，TSMixer表现优于最新的替代方案。我们的结果强调了高效提取信息的重要性

    Real-world time-series datasets are often multivariate with complex dynamics. Commonly-used high capacity architectures like recurrent- or attention-based sequential models have become popular. However, recent work demonstrates that simple univariate linear models can outperform those deep alternatives. In this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), an architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates superior performance compared to the state-of-the-art alternatives. Our results underline the importance of ef
    
[^11]: 可解释人工智能在自杀风险评估中的分析与评估

    Analysis and Evaluation of Explainable Artificial Intelligence on Suicide Risk Assessment. (arXiv:2303.06052v1 [cs.LG])

    [http://arxiv.org/abs/2303.06052](http://arxiv.org/abs/2303.06052)

    本研究探讨了可解释人工智能（XAI）技术在预测自杀风险和确定主要原因方面的有效性。利用数据增强技术和机器学习模型来预测相关风险。此外，使用SHapley Additive exPlanations（SHAP）和相关分析来排名预测中变量的重要性。实验结果表明，决策树（DT）、随机森林（RF）和极限梯度提升（XGBoost）模型取得了最佳结果，其中DT具有最佳表现，准确率为95.23%，曲线下面积（AUC）为0.95。根据SHAP的结果，愤怒问题、抑郁症和社交隔离是预测自杀风险的主要变量，而收入好、职业受人尊敬和接受大学教育的患者风险最小。结果证明了机器学习和XAI框架在自杀风险预测方面的有效性，以及它们在医疗保健和公共卫生领域的应用。

    This study investigates the effectiveness of Explainable Artificial Intelligence (XAI) techniques in predicting suicide risks and identifying the dominant causes for such behaviours. Data augmentation techniques and ML models are utilized to predict the associated risk. Furthermore, SHapley Additive exPlanations (SHAP) and correlation analysis are used to rank the importance of variables in predictions. Experimental results indicate that Decision Tree (DT), Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) models achieve the best results while DT has the best performance with an accuracy of 95:23% and an Area Under Curve (AUC) of 0.95. As per SHAP results, anger problems, depression, and social isolation are the leading variables in predicting the risk of suicide, and patients with good incomes, respected occupations, and university education have the least risk. Results demonstrate the effectiveness of machine learning and XAI framework for suicide risk prediction, and they c
    
[^12]: 最优觅食策略可以被学习并超过利维步行。

    Optimal foraging strategies can be learned and outperform L\'evy walks. (arXiv:2303.06050v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2303.06050](http://arxiv.org/abs/2303.06050)

    利维步行和其他理论模型已经成功地用于描述现实情况，在经济学、物理学、生态学和进化生物学等多个领域引起了关注。然而，在大多数情况下，最大化觅食效率的策略以及这些策略是否可以被生物学习还不清楚。为了解决这些问题，我们将觅食者建模为强化学习代理。我们首先从理论上证明，我们的强化学习模型中最大化奖励等价于优化觅食效率。然后，我们通过数字实验展示了我们的代理学习了优于已知策略（如利维步行）效率的觅食策略。

    L\'evy walks and other theoretical models of optimal foraging have been successfully used to describe real-world scenarios, attracting attention in several fields such as economy, physics, ecology, and evolutionary biology. However, it remains unclear in most cases which strategies maximize foraging efficiency and whether such strategies can be learned by living organisms. To address these questions, we model foragers as reinforcement learning agents. We first prove theoretically that maximizing rewards in our reinforcement learning model is equivalent to optimizing foraging efficiency. We then show with numerical experiments that our agents learn foraging strategies which outperform the efficiency of known strategies such as L\'evy walks.
    
[^13]: 廉价人工智能——通过AI增强农民的知识

    Affordable Artificial Intelligence -- Augmenting Farmer Knowledge with AI. (arXiv:2303.06049v1 [eess.SP])

    [http://arxiv.org/abs/2303.06049](http://arxiv.org/abs/2303.06049)

    农场每天会产生数十万个数据点，将种植技术与这些数据点中发现的见解结合起来的农业技术称为精准农业。精准农业技术增强和扩展了农民对土地的深入了解，使生产更加可持续和有利可图。本文作为微软赋能农业劳动力更具生产力和可持续性的更大努力的一部分，介绍了预测农场微气候条件的AI技术。本文是联合国粮食和农业组织以及国际电信联盟2021年曼谷出版的一章。这本关于农业人工智能的出版物是“电子农业行动系列”中的第五本，该系列于2016年推出，由联合国粮食和农业组织和国际电信联盟共同制作。它旨在提高人们对农业中现有人工智能应用的认识，并激励利益相关者

    Farms produce hundreds of thousands of data points on the ground daily. Farming technique which combines farming practices with the insights uncovered in these data points using AI technology is called precision farming. Precision farming technology augments and extends farmers' deep knowledge about their land, making production more sustainable and profitable. As part of the larger effort at Microsoft for empowering agricultural labor force to be more productive and sustainable, this paper presents the AI technology for predicting micro-climate conditions on the farm.  This article is a chapter in publication by Food and Agriculture Organization of the United Nations and International Telecommunication Union Bangkok, 2021. This publication on artificial intelligence (AI) for agriculture is the fifth in the E-agriculture in Action series, launched in 2016 and jointly produced by FAO and ITU. It aims to raise awareness about existing AI applications in agriculture and to inspire stakeho
    
[^14]: Tactile-Filter: 用于配件组装的交互式触觉感知

    Tactile-Filter: Interactive Tactile Perception for Part Mating. (arXiv:2303.06034v1 [cs.RO])

    [http://arxiv.org/abs/2303.06034](http://arxiv.org/abs/2303.06034)

    人们依赖触觉和触觉传感器完成许多熟练的操作任务。 触觉传感器为我们提供了许多关于接触形态以及在任何交互期间关于对象的几何信息。 凭借这种动力，基于视觉的触觉传感器被广泛用于各种机器人感知和控制任务中。在本文中，我们提出了一种使用基于视觉的触觉传感器进行多对象组装的交互式感知方法。特别是，我们对零件组装过程中的触觉感知感兴趣，其中机器人可以使用触觉传感器和使用粒子滤波器的反馈机制逐步改进其对适合组装的对象的估计。为此，我们首先训练了一个深度神经网络，利用触觉图像来预测彼此契合的任意形状对象之间的概率对应关系。训练好的模型用于设计粒子滤波器，其具有两个用途。 首先，给定

    Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for multi-object assembly. In particular, we are interested in tactile perception during part mating, where a robot can use tactile sensors and a feedback mechanism using particle filter to incrementally improve its estimate of objects that fit together for assembly. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given 
    
[^15]: HARDC：一种基于心电图的心跳分类方法，使用分层注意力双结构RNN和扩张CNN(arXiv：2303.06020v1 [eess.SP])

    HARDC : A novel ECG-based heartbeat classification method to detect arrhythmia using hierarchical attention based dual structured RNN with dilated CNN. (arXiv:2303.06020v1 [eess.SP])

    [http://arxiv.org/abs/2303.06020](http://arxiv.org/abs/2303.06020)

    本文提出了一种新的混合分层注意力双向循环神经网络与扩张CNN（HARDC）方法用于心律失常分类。这解决了传统扩张卷积神经网络(CNN)模型忽略上下文之间的关联和梯度分散的问题。所提出的HARDC充分利用了扩张CNN和双向递归神经网络单元（BiGRU-BiLSTM）体系结构来生成融合特征。通过结合本地和全局特征信息和注意力机制，该模型的预测性能得到了提高。通过将融合特征与扩张CNN和分层注意力机制相结合，训练后的HARDC模型在PhysioNet 2017挑战数据集上表现出了明显的改进分类结果和特征提取的可解释性。使用连续Z-Score归一化、滤波、去噪和分段来准备原始数据。

    In this paper have developed a novel hybrid hierarchical attention-based bidirectional recurrent neural network with dilated CNN (HARDC) method for arrhythmia classification. This solves problems that arise when traditional dilated convolutional neural network (CNN) models disregard the correlation between contexts and gradient dispersion. The proposed HARDC fully exploits the dilated CNN and bidirectional recurrent neural network unit (BiGRU-BiLSTM) architecture to generate fusion features. As a result of incorporating both local and global feature information and an attention mechanism, the model's performance for prediction is improved.By combining the fusion features with a dilated CNN and a hierarchical attention mechanism, the trained HARDC model showed significantly improved classification results and interpretability of feature extraction on the PhysioNet 2017 challenge dataset. Sequential Z-Score normalization, filtering, denoising, and segmentation are used to prepare the raw
    
[^16]: 分层神经程序合成

    Hierarchical Neural Program Synthesis. (arXiv:2303.06018v1 [cs.SE])

    [http://arxiv.org/abs/2303.06018](http://arxiv.org/abs/2303.06018)

    程序合成旨在自动构建满足给定任务规范（例如输入/输出对或演示）的人类可读程序。最近的研究在各种领域取得了令人鼓舞的成果，如字符串转换、张量操作和描述具体化身代理的行为。大多数现有的程序合成方法都是从头开始合成程序，逐个令牌、逐行生成程序。这从根本上阻止了这些方法扩展到合成更长或更复杂的程序。在这项工作中，我们提出了一个可扩展的程序合成框架，该框架通过分层组合程序来合成程序。具体而言，我们首先学习任务嵌入空间和程序解码器，该解码器可以将任务嵌入解码为程序。然后，我们训练一个高层模块，以理解来自长程序的任务规范（例如输入/输出对或演示）并生成程序。

    Program synthesis aims to automatically construct human-readable programs that satisfy given task specifications, such as input/output pairs or demonstrations. Recent works have demonstrated encouraging results in a variety of domains, such as string transformation, tensor manipulation, and describing behaviors of embodied agents. Most existing program synthesis methods are designed to synthesize programs from scratch, generating a program token by token, line by line. This fundamentally prevents these methods from scaling up to synthesize programs that are longer or more complex. In this work, we present a scalable program synthesis framework that instead synthesizes a program by hierarchically composing programs. Specifically, we first learn a task embedding space and a program decoder that can decode a task embedding into a program. Then, we train a high-level module to comprehend the task specification (e.g., input/output pairs or demonstrations) from long programs and produce a se
    
[^17]: 建立基于展望理论的投影偏误的时间选择模型

    Modelling Projection Bias in Intertemporal Choices: A Prospect Theory Based Approach. (arXiv:2303.06016v1 [cs.IR])

    [http://arxiv.org/abs/2303.06016](http://arxiv.org/abs/2303.06016)

    在购买时，用户经常面临捆绑促销，需要在两个选项之间进行选择：以全价购买单个物品，或以折扣价购买捆绑包。在这种情况下，用户的偏好通常受到投影偏误的影响，即用户经常认为他们未来的偏好与他们当前的偏好相似，导致他们做出非理性和短视的决策。分析投影偏误对用户偏好的影响具有重要意义，本研究可能有助于理解用户的决策过程，并为卖家提供捆绑和定价策略。以往的工作通常使用线性偏误模型进行定性分析，无法定量计算用户的非线性和个性化偏误。在这项工作中，我们提出了Pobe，一种投影偏误嵌入偏好模型，以准确预测用户的选择。所提出的Pobe引入了展望理论来分析用户的非理性决策，并利用

    Users often face bundle promotions when purchasing, where they have to select between two options: buy the single item at full price, or buy the bundle at a discount. In this scenario, users' preferences are usually influenced by the projection bias, that is, users often believe that their future preferences are similar to their current preferences, causing them to make irrational and short-sighted decisions. It is of great significance to analyze the effect of the projection bias on users' preferences, and this study may help understand users' decision-making process and provide bundling and pricing strategies for sellers. Prior works typically use a linear bias model for qualitative analysis, and they cannot quantitatively calculate users' nonlinear and personalized bias. In this work, we propose Pobe, a projection bias-embedded preference model to accurately predict users' choices. The proposed Pobe introduces the prospect theory to analyze users' irrational decisions, and utilizes 
    
[^18]: 责任化文本视觉重现的新基准。

    New Benchmarks for Accountable Text-based Visual Re-creation. (arXiv:2303.05983v1 [cs.CV])

    [http://arxiv.org/abs/2303.05983](http://arxiv.org/abs/2303.05983)

    给定指令后，人类可以直接执行动作或选择拒绝，同时合理地反馈。然而，现有的文本到图像生成方法的行为不可控且不负责任。在本文中，我们构建了大量实验来验证它们能否对这些被禁止的指令负责并说明理由。为此，我们定义了一种新的基于文本的视觉重现任务，并构建了新的合成CLEVR-NOT数据集（620K）和手动绘制的Fruit-NOT数据集（50K）。在我们的方法中，将一个文本-图像对作为查询馈入机器，模型在视觉和文本推理之后给出一个是或否的答案。如果答案是肯定的，那么图像自动编码器和自回归变换器必须在确保图像质量的前提下完成视觉重现，否则系统需要解释为什么命令无法完成或被禁止。我们对实验进行了详细的分析。

    Given a command, humans can directly execute the action after thinking or choose to reject it, with reasonable feedback at the same time. However, the behavior of existing text-to-image generation methods are uncontrollable and irresponsible. In this paper, we construct extensive experiments to verify whether they can be accountable (say no and explain why) for those prohibited instructions. To this end, we define a novel text-based visual re-creation task and construct new synthetic CLEVR-NOT dataset (620K) and manually pictured Fruit-NOT dataset (50K). In our method, one text-image pair as the query is fed into the machine, and the model gives a yes or no answer after visual and textual reasoning. If the answer is yes, the image auto-encoder and auto-regressive transformer must complete the visual re-creation under the premise of ensuring image quality, otherwise the system needs to explain why the commands cannot be completed or prohibited. We provide a detailed analysis of experime
    
[^19]: 采用动态贝叶斯网络和神经网络对COVID-19患者病情演变进行分类

    Classifying the evolution of COVID-19 severity on patients with combined dynamic Bayesian networks and neural networks. (arXiv:2303.05972v1 [cs.LG])

    [http://arxiv.org/abs/2303.05972](http://arxiv.org/abs/2303.05972)

    当我们面对到达医院的患者患病的影响时，我们会面临一个主要问题，即评估这些患者是否在不久的将来需要重症监护。这种重症监护需要分配宝贵而稀缺的资源，预先知道患者疾病的严重程度可以改善其治疗和资源组织。我们在由西班牙COVID-19患者组成的数据集中说明了这个问题，其中我们将患者标记为重症患者，当他们不得不进入重症监护室或去世时。然后，我们结合使用动态贝叶斯网络，对未来40小时内患者的生命体征和血液分析结果进行预测，并使用神经网络评估患者在该时间间隔内的疾病严重程度。我们的实证结果表明，使用DBN将患者当前状态转移到未来值可以提高模型的准确性，我们的模型具有很高的分类准确性。

    When we face patients arriving to a hospital suffering from the effects of some illness, one of the main problems we can encounter is evaluating whether or not said patients are going to require intensive care in the near future. This intensive care requires allotting valuable and scarce resources, and knowing beforehand the severity of a patients illness can improve both its treatment and the organization of resources. We illustrate this issue in a dataset consistent of Spanish COVID-19 patients from the sixth epidemic wave where we label patients as critical when they either had to enter the intensive care unit or passed away. We then combine the use of dynamic Bayesian networks, to forecast the vital signs and the blood analysis results of patients over the next 40 hours, and neural networks, to evaluate the severity of a patients disease in that interval of time. Our empirical results show that the transposition of the current state of a patient to future values with the DBN for it
    
[^20]: 理解和构建多模态表示学习中的潜在模态结构

    Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning. (arXiv:2303.05952v1 [cs.LG])

    [http://arxiv.org/abs/2303.05952](http://arxiv.org/abs/2303.05952)

    对比损失在从多个模态学习表示中的应用越来越广泛。 在极限情况下，对比损失的本质促使模态在潜在空间中完全匹配。 然而，模态对齐如何影响下游任务性能仍然是一个开放的问题。 基于信息理论的论据，本文首先证明一般情况下精确的模态对齐是次优的下游预测任务。 因此，我们主张更好的表现关键在于有意义的潜在模态结构，而不是完美的模态对齐。 为此，我们提出了三种构建潜在模态结构的通用方法。 具体而言，我们设计了1）用于模内正则化的深度特征分离损失； 2）用于模间正则化的布朗运动桥损失； 3）用于模内和模间正则化的几何一致性损失。 在两个流行的数据集上进行了广泛的实验。

    Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for downstream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are conducted on two popular
    
[^21]: 视频中准确的实时息肉检测：来自连续帧提取的潜在特征的串联

    Accurate Real-time Polyp Detection in Videos from Concatenation of Latent Features Extracted from Consecutive Frames. (arXiv:2303.05871v1 [cs.CV])

    [http://arxiv.org/abs/2303.05871](http://arxiv.org/abs/2303.05871)

    实时的息肉检测对于减少筛查过程中的漏诊率至关重要。本研究尝试通过在相邻帧之间整合时间信息来解决这个问题。我们提出了一种有效的特征串联方法，通过将前一帧的特征映射并入当前帧来检测息肉，以此不增加模型的复杂性。实验结果表明，这种特征串联方法可以提高视频中自动息肉检测的性能。

    An efficient deep learning model that can be implemented in real-time for polyp detection is crucial to reducing polyp miss-rate during screening procedures. Convolutional neural networks (CNNs) are vulnerable to small changes in the input image. A CNN-based model may miss the same polyp appearing in a series of consecutive frames and produce unsubtle detection output due to changes in camera pose, lighting condition, light reflection, etc. In this study, we attempt to tackle this problem by integrating temporal information among neighboring frames. We propose an efficient feature concatenation method for a CNN-based encoder-decoder model without adding complexity to the model. The proposed method incorporates extracted feature maps of previous frames to detect polyps in the current frame. The experimental results demonstrate that the proposed method of feature concatenation improves the overall performance of automatic polyp detection in videos. The following results are obtained on a
    
[^22]: 一种基于规则的定理证明器：中学证明教学介绍

    A Rule Based Theorem Prover: an Introduction to Proofs in Secondary Schools. (arXiv:2303.05863v1 [cs.AI])

    [http://arxiv.org/abs/2303.05863](http://arxiv.org/abs/2303.05863)

    在中学引入自动推理系统面临几个瓶颈。除了与课程和教师相关的问题外，几何自动定理证明器的结果与学校猜测和证明的正常实践之间的不和谐是在教育环境中广泛使用此类工具的主要障碍。自几何自动定理证明器的早期实现以来，基于推理规则的人工智能方法、综合证明器和使用前向链接推理的方法被认为更适合教育目的。选择适当的规则集和能够使用这些规则的自动化方法是一个重大挑战。我们讨论了这样一种规则集及其使用几何推理数据库方法（GDDM）的实现。该方法使用了一些选择的几何猜想进行测试，这些猜想可能是第7年级（约12岁的学生）班级的目标。

    The introduction of automated deduction systems in secondary schools face several bottlenecks. Beyond the problems related with the curricula and the teachers, the dissonance between the outcomes of the geometry automated theorem provers and the normal practice of conjecturing and proving in schools is a major barrier to a wider use of such tools in an educational environment. Since the early implementations of geometry automated theorem provers, applications of artificial intelligence methods, synthetic provers based on inference rules and using forward chaining reasoning are considered to be more suited for education proposes. Choosing an appropriate set of rules and an automated method that can use those rules is a major challenge. We discuss one such rule set and its implementation using the geometry deductive databases method (GDDM). The approach is tested using some chosen geometric conjectures that could be the goal of a 7th year class (approx. 12-year-old students). A lesson pl
    
[^23]: 不确定性决策：超越概率

    Decision-Making Under Uncertainty: Beyond Probabilities. (arXiv:2303.05848v1 [cs.AI])

    [http://arxiv.org/abs/2303.05848](http://arxiv.org/abs/2303.05848)

    这篇论文对不确定性决策的最新发展进行了回顾。传统的假设是概率可以充分捕捉系统中的所有不确定性。本文关注超越这种传统解释的不确定性，特别是通过清晰区分模糊不确定性和认知不确定性。文章概述了马尔可夫决策过程（MDP）及其扩展，以解决部分可观测性和对抗行为的问题。这些模型可以很好地捕捉模糊不确定性，但未能充分考虑认知不确定性。因此，我们对所谓的“不确定性模型”进行了全面的概述，并展示了一些离散和连续模型的解决方法，从形式化验证、基于控制的抽象到强化学习。作为本文的一个重要组成部分，我们列出并讨论了若干个例子。

    This position paper reflects on the state-of-the-art in decision-making under uncertainty. A classical assumption is that probabilities can sufficiently capture all uncertainty in a system. In this paper, the focus is on the uncertainty that goes beyond this classical interpretation, particularly by employing a clear distinction between aleatoric and epistemic uncertainty. The paper features an overview of Markov decision processes (MDPs) and extensions to account for partial observability and adversarial behavior. These models sufficiently capture aleatoric uncertainty but fail to account for epistemic uncertainty robustly. Consequently, we present a thorough overview of so-called uncertainty models that exhibit uncertainty in a more robust interpretation. We show several solution techniques for both discrete and continuous models, ranging from formal verification, over control-based abstractions, to reinforcement learning. As an integral part of this paper, we list and discuss severa
    
[^24]: 对比语言图像预训练（CLIP）模型是强大的越界检测器。

    Contrastive Language-Image Pretrained (CLIP) Models are Powerful Out-of-Distribution Detectors. (arXiv:2303.05828v1 [cs.CV])

    [http://arxiv.org/abs/2303.05828](http://arxiv.org/abs/2303.05828)

    我们对用于视觉越界检测的预训练特征提取器进行了全面的实验研究。 我们检查了几个设置，基于标签或图像标题的可用性，使用不同的内部和外部分布的组合。 有趣的是，我们发现（i）对比语言-图像预训练模型使用最近邻特征相似性作为越界检测分数，实现了最先进的无监督越界检测表现，（ii）可以在不需要内部分布微调的情况下获得监督状态下的最先进越界检测性能，（iii）即使是针对自然语言监督进行训练的性能最佳的十亿级视觉变换器也无法检测到经过敌对操纵的越界图像。 最后，我们根据实验结果讨论了是否需要新的基于视觉异常检测的基准。 使用最大的公开可用视觉变换器，在所有18个报告的越界基准测试中都实现了最先进的性能

    We present a comprehensive experimental study on pretrained feature extractors for visual out-of-distribution (OOD) detection. We examine several setups, based on the availability of labels or image captions and using different combinations of in- and out-distributions. Intriguingly, we find that (i) contrastive language-image pretrained models achieve state-of-the-art unsupervised out-of-distribution performance using nearest neighbors feature similarity as the OOD detection score, (ii) supervised state-of-the-art OOD detection performance can be obtained without in-distribution fine-tuning, (iii) even top-performing billion-scale vision transformers trained with natural language supervision fail at detecting adversarially manipulated OOD images. Finally, we argue whether new benchmarks for visual anomaly detection are needed based on our experiments. Using the largest publicly available vision transformer, we achieve state-of-the-art performance across all $18$ reported OOD benchmark
    
[^25]: 多头特征自适应学习中的知识迁移在全幻灯片图像分类中的应用

    Knowledge Transfer via Multi-Head Feature Adaptation for Whole Slide Image Classification. (arXiv:2303.05780v1 [cs.CV])

    [http://arxiv.org/abs/2303.05780](http://arxiv.org/abs/2303.05780)

    从源领域传递先前的知识到相同或相似的目标领域可以极大地提高模型在目标领域上的性能。但是，由于任务不一致和领域转移，直接利用源领域中的知识是具有挑战性的。为了弥合不同任务和领域之间的差距，我们提出了一种多头特征自适应模块，它将源特征空间中的特征投影到一个更类似于目标空间的新空间中。在整个幻灯片图像（WSI）分类中，知识转移尤其重要，因为一个数据集中的WSI数量可能太少而无法达到令人满意的性能。因此，WSI分类是我们方法的理想测试平台，并且我们针对WSI分类调整了多种知识迁移方法。实验结果表明，具有知识转移的模型无论数据集中的WSI数量如何，其性能都大大优于从头开始训练的模型。

    Transferring prior knowledge from a source domain to the same or similar target domain can greatly enhance the performance of models on the target domain. However, it is challenging to directly leverage the knowledge from the source domain due to task discrepancy and domain shift. To bridge the gaps between different tasks and domains, we propose a Multi-Head Feature Adaptation module, which projects features in the source feature space to a new space that is more similar to the target space. Knowledge transfer is particularly important in Whole Slide Image (WSI) classification since the number of WSIs in one dataset might be too small to achieve satisfactory performance. Therefore, WSI classification is an ideal testbed for our method, and we adapt multiple knowledge transfer methods for WSI classification. The experimental results show that models with knowledge transfer outperform models that are trained from scratch by a large margin regardless of the number of WSIs in the datasets
    
[^26]: 智能手机上纸质收据的自动检测和矫正

    Automatic Detection and Rectification of Paper Receipts on Smartphones. (arXiv:2303.05763v1 [cs.CV])

    [http://arxiv.org/abs/2303.05763](http://arxiv.org/abs/2303.05763)

    我们描述了一种实时智能手机应用的开发，允许用户以一种新颖的方式通过“挥动”手机将纸质收据数字化，让应用程序自动检测和校正收据以进行后续的文本识别。我们证明，传统的边缘和角点检测的计算机视觉算法不能在真实世界的环境下强制检测典型纸质收据的非线性和不连续的边缘和角点。特别是在收据和背景颜色相似或其他干扰矩形物体存在的情况下。当使用仿射投影变换来矫正透视时，收据角的位置的不准确检测会导致图像变形。我们提出了一种创新的解决方案，通过将每个四个角作为一个独特的“对象”，并训练单次检测MobileNet对象检测模型来处理收据角的检测。我们使用少量的重新训练数据进行训练，从而在各种不同的情况下都能准确检测收据。我们的实验结果表明，我们的方法在检测和纠正收据方面比传统方法有更好的性能。

    We describe the development of a real-time smartphone app that allows the user to digitize paper receipts in a novel way by "waving" their phone over the receipts and letting the app automatically detect and rectify the receipts for subsequent text recognition.  We show that traditional computer vision algorithms for edge and corner detection do not robustly detect the non-linear and discontinuous edges and corners of a typical paper receipt in real-world settings. This is particularly the case when the colors of the receipt and background are similar, or where other interfering rectangular objects are present. Inaccurate detection of a receipt's corner positions then results in distorted images when using an affine projective transformation to rectify the perspective.  We propose an innovative solution to receipt corner detection by treating each of the four corners as a unique "object", and training a Single Shot Detection MobileNet object detection model. We use a small amount of re
    
[^27]: 通过几何分解实现逆问题的快速扩散采样器

    Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition. (arXiv:2303.05754v1 [cs.LG])

    [http://arxiv.org/abs/2303.05754](http://arxiv.org/abs/2303.05754)

    扩散模型在解决逆问题方面表现出卓越的性能，但其推理时间较慢。本文提出了一种新颖高效的扩散采样策略，采用扩散采样的几何分解。具体来说，我们发现从扩散模型生成的样本可以分解为两个正交分量：通过将样本投影到干净数据流形上获得的“去噪”分量以及通过添加随机噪声将其转换为下一个较低级别的嘈杂流形的“噪声”分量。此外，我们证明，在一些对于干净数据流形的条件下，通过从去噪信号中施加条件的共轭梯度更新属于干净流形，从而导致一个mu

    Diffusion models have shown exceptional performance in solving inverse problems. However, one major limitation is the slow inference time. While faster diffusion samplers have been developed for unconditional sampling, there has been limited research on conditional sampling in the context of inverse problems. In this study, we propose a novel and efficient diffusion sampling strategy that employs the geometric decomposition of diffusion sampling. Specifically, we discover that the samples generated from diffusion models can be decomposed into two orthogonal components: a ``denoised" component obtained by projecting the sample onto the clean data manifold, and a ``noise" component that induces a transition to the next lower-level noisy manifold with the addition of stochastic noise. Furthermore, we prove that, under some conditions on the clean data manifold, the conjugate gradient update for imposing conditioning from the denoised signal belongs to the clean manifold, resulting in a mu
    
[^28]: QVRF：一种量化误差感知的变速率学习图像压缩框架

    QVRF: A Quantization-error-aware Variable Rate Framework for Learned Image Compression. (arXiv:2303.05744v1 [eess.IV])

    [http://arxiv.org/abs/2303.05744](http://arxiv.org/abs/2303.05744)

    学习图像压缩表现出了很好的压缩性能，但在广泛的比特率范围内实现可变比特率仍然是一个挑战。最先进的可变速率方法在模型性能损失方面存在妥协，并需要大量的额外参数。本文提出了一种量化误差感知的变速率框架（QVRF），该框架利用单变量量化调节器实现单模型内广泛的可变比特率。具体来说，QVRF定义了一个量化调节器向量，结合预定义的Lagrange乘数，控制了所有潜在表示的量化误差，从而实现了离散可变比特率。此外，重新参数化方法使得QVRF与圆整量化器兼容。详尽的实验证明，现有的配备QVRF的固定比特率基于VAE的方法可以在单个模型内实现广泛的连续可变比特率而不会出现显著的性能降级。此外，QVRF优于当代可变速率压缩方法。

    Learned image compression has exhibited promising compression performance, but variable bitrates over a wide range remain a challenge. State-of-the-art variable rate methods compromise the loss of model performance and require numerous additional parameters. In this paper, we present a Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a univariate quantization regulator a to achieve wide-range variable rates within a single model. Specifically, QVRF defines a quantization regulator vector coupled with predefined Lagrange multipliers to control quantization error of all latent representation for discrete variable rates. Additionally, the reparameterization method makes QVRF compatible with a round quantizer. Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods equipped with QVRF can achieve wide-range continuous variable rates within a single model without significant performance degradation. Furthermore, QVRF outperforms contemporary variabl
    
[^29]: 一张单照片生成3D Cinemagraphy

    3D Cinemagraphy from a Single Image. (arXiv:2303.05724v1 [cs.CV])

    [http://arxiv.org/abs/2303.05724](http://arxiv.org/abs/2303.05724)

    本文提出了一种新技术3D Cinemagraphy，将2D图像动画和3D摄影融合起来。给定一张单照片作为输入，我们的目标是生成包含视觉内容动画和相机动作的视频。通过实验我们发现，简单的组合现有的2D图像动画和3D摄影方法会导致明显的伪影或不一致的动画。我们的关键洞察力是在3D空间中表示和动画化场景为这个任务提供了一个自然的解决方案。为此，我们首先使用预测的深度值将输入图像转换为基于特征的分层深度图像，然后将它们展开到一个特征点云中。为了动画化场景，我们执行运动估计并将2D动作提升到3D场景流中。最后，为了解决点向前移动时出现的洞隐问题，我们建议按场景流双向位移点云，并通过将其分别投影到目标图像平面中合成新视图。

    We present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and
    
[^30]: 将神经坍塌诱导到固定的层次感知框架，以减少错误严重程度

    Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity. (arXiv:2303.05689v1 [cs.CV])

    [http://arxiv.org/abs/2303.05689](http://arxiv.org/abs/2303.05689)

    最近发现了一个有趣的现象称为神经坍塌：在训练用于分类的深度神经网络的末期阶段，所有平坦类别的倒数第二特征平均值和相关的分类器向量坍塌到一个简单的等角紧框架(ETF)的顶点。最近的工作尝试利用这种现象，通过将相关的分类器权重固定为预计算的ETF来诱导神经坍塌，并在不平衡数据训练时最大化学习特征之间的分离。在这项工作中，我们提出将深度神经网络的线性分类器固定到一个层次感知框架(HAFrame)上，而不是ETF，并使用基于余弦相似度的辅助损失来学习崩溃到HAFrame的层次感知倒数第二特征。我们证明了我们的方法在保持在具有层次结构的不同规模的数据集上的top-1准确率的同时减少了模型预测的错误严重程度。

    There is a recently discovered and intriguing phenomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned features when training with imbalanced data. In this work, we propose to fix the linear classifier of a deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach reduces the mistake severity of the model's predictions while maintaining its top-1 accuracy on several datasets of varying scales with hierarchies of he
    
[^31]: 室内场景的人机合作优化与重组

    Rearrange Indoor Scenes for Human-Robot Co-Activity. (arXiv:2303.05676v1 [cs.RO])

    [http://arxiv.org/abs/2303.05676](http://arxiv.org/abs/2303.05676)

    我们提出了一种基于优化的框架，以更好地配合人机协作来重新布置室内家具。重组旨在为机器人活动提供足够的可访问空间，同时不影响日常人类活动。为保留人类活动，我们的算法通过整合从SUNCG和ConceptNet中提取的空间和语义共现来保留家具之间的功能关系。通过定义机器人可访问空间的开放空间量和能够到达的物体数量，我们将人机协作的重组形式化为一个优化问题，由自适应模拟退火(ASA)和协方差矩阵适应进化策略(CMA-ES)求解。我们在SUNCG数据集上的实验定量显示，重新排列的场景提供平均14%的可访问空间和30%的可交互对象。重组场景的质量在定性上得到验证。

    We present an optimization-based framework for rearranging indoor furniture to accommodate human-robot co-activities better. The rearrangement aims to afford sufficient accessible space for robot activities without compromising everyday human activities. To retain human activities, our algorithm preserves the functional relations among furniture by integrating spatial and semantic co-occurrence extracted from SUNCG and ConceptNet, respectively. By defining the robot's accessible space by the amount of open space it can traverse and the number of objects it can reach, we formulate the rearrangement for human-robot co-activity as an optimization problem, solved by adaptive simulated annealing (ASA) and covariance matrix adaptation evolution strategy (CMA-ES). Our experiments on the SUNCG dataset quantitatively show that rearranged scenes provide an average of 14% more accessible space and 30% more objects to interact with. The quality of the rearranged scenes is qualitatively validated b
    
[^32]: 逻辑打破偏见：文本蕴涵缓解了带有陈规陋习的句子推理。

    Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning. (arXiv:2303.05670v1 [cs.CL])

    [http://arxiv.org/abs/2303.05670](http://arxiv.org/abs/2303.05670)

    由于预训练句子编码器的相似度学习目标，它们经常内化反映其训练语料库中存在的社会偏见的陈规陋习。本文描述了存在于流行句子表示模型中的几种关于不同社区的刻板印象。我们将这样的模型与学习各种下游语言理解任务的文本蕴涵模型进行比较。通过比较基于文本相似度的强预训练模型和文本蕴涵学习，我们得出结论：文本蕴涵的明确逻辑学习可以显著减少偏差，并提高社交社区的识别能力，而无需明确去偏见的过程。

    Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process
    
[^33]: UNFUSED: 使用自监督蒸馏的无监督微调方法

    UNFUSED: UNsupervised Finetuning Using SElf supervised Distillation. (arXiv:2303.05668v1 [eess.AS])

    [http://arxiv.org/abs/2303.05668](http://arxiv.org/abs/2303.05668)

    本文提出了一种新颖的方法UnFuSeD，利用自监督学习并减少音频分类所需的标记数据量。相比之前的工作，直接在目标数据集上微调自监督预训练的编码器，我们使用编码器通过聚类提取的表征生成伪标签进行无监督微调。这些伪标签然后用于指导随机初始化模型上的自蒸馏。最后，对得到的编码器在目标任务数据集上进行微调。通过UnFuSeD，我们提出了第一个脱离文献中通用自监督学习范例的系统，该范例预训练和微调相同的编码器。

    In this paper, we introduce UnFuSeD, a novel approach to leverage self-supervised learning and reduce the need for large amounts of labeled data for audio classification. Unlike prior works, which directly fine-tune a self-supervised pre-trained encoder on a target dataset, we use the encoder to generate pseudo-labels for unsupervised fine-tuning before the actual fine-tuning step. We first train an encoder using a novel self-supervised learning algorithm (SSL) on an unlabeled audio dataset. Then, we use that encoder to generate pseudo-labels on our target task dataset via clustering the extracted representations. These pseudo-labels are then used to guide self-distillation on a randomly initialized model, which we call unsupervised fine-tuning. Finally, the resultant encoder is then fine-tuned on our target task dataset. Through UnFuSeD, we propose the first system that moves away from generic SSL paradigms in literature, which pre-train and fine-tune the same encoder, and present a n
    
[^34]: 使用图形感知变压器和运动解耦回归进行人体网格从2D姿势中恢复

    GATOR: Graph-Aware Transformer with Motion-Disentangled Regression for Human Mesh Recovery from a 2D Pose. (arXiv:2303.05652v1 [cs.CV])

    [http://arxiv.org/abs/2303.05652](http://arxiv.org/abs/2303.05652)

    从2D姿势中恢复3D人体网格在各种应用中起着重要作用。 但是，现有方法很难同时捕获从骨架到网格的多个关系，包括关节-关节，关节-顶点和顶点-顶点关系，这经常导致不切实际的结果。 为了解决这个问题，我们提出了一种新的解决方案，称为GATOR，其中包含一个图形感知变压器（GAT）的编码器和具有运动解耦回归（MDR）的解码器，以探索这些多个关系。 具体而言，GAT并行组合了GCN和图形感知的自我注意力，以捕获物理和隐藏的关节-关节关系。 此外，MDR通过模拟关节和顶点关系来探索关节-顶点和顶点-顶点交互作用。 基于顶点偏移场的聚类特征，MDR通过组合预测的基本运动来回归顶点。 大量实验表明，GATOR在两个数据集上实现了最先进的性能。

    3D human mesh recovery from a 2D pose plays an important role in various applications. However, it is hard for existing methods to simultaneously capture the multiple relations during the evolution from skeleton to mesh, including joint-joint, joint-vertex and vertex-vertex relations, which often leads to implausible results. To address this issue, we propose a novel solution, called GATOR, that contains an encoder of Graph-Aware Transformer (GAT) and a decoder with Motion-Disentangled Regression (MDR) to explore these multiple relations. Specifically, GAT combines a GCN and a graph-aware self-attention in parallel to capture physical and hidden joint-joint relations. Furthermore, MDR models joint-vertex and vertex-vertex interactions to explore joint and vertex relations. Based on the clustering characteristics of vertex offset fields, MDR regresses the vertices by composing the predicted base motions. Extensive experiments show that GATOR achieves state-of-the-art performance on two 
    
[^35]: Pacos：在偏好反转中建模用户的可解释性和上下文依赖选择

    Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals. (arXiv:2303.05648v1 [cs.IR])

    [http://arxiv.org/abs/2303.05648](http://arxiv.org/abs/2303.05648)

    选择问题是从多个项目中选择最佳选择，学习用户在选择问题中的偏好对于了解决策制定机制和提供个性化服务具有重要意义。现有的工作通常假定人们独立评估项目。然而，在实践中，用户的偏好取决于物品所在的市场，这被称为上下文效应；用户对两个项目的偏好顺序甚至可能被颠倒，这被称为偏好反转。在这项工作中，我们确定了三个导致上下文效应的因素：用户的自适应权重，项目间比较和显示位置。我们提出了一个名为Pacos的上下文依赖偏好模型，作为同时解决三个因素的统一框架，并考虑了两种设计方法，包括具有高解释性的加法方法和具有高准确性的基于ANN的方法。我们研究了偏好反转的条件

    Choice problems refer to selecting the best choices from several items, and learning users' preferences in choice problems is of great significance in understanding the decision making mechanisms and providing personalized services. Existing works typically assume that people evaluate items independently. In practice, however, users' preferences depend on the market in which items are placed, which is known as context effects; and the order of users' preferences for two items may even be reversed, which is referred to preference reversals. In this work, we identify three factors contributing to context effects: users' adaptive weights, the inter-item comparison, and display positions. We propose a context-dependent preference model named Pacos as a unified framework for addressing three factors simultaneously, and consider two design methods including an additive method with high interpretability and an ANN-based method with high accuracy. We study the conditions for preference reversa
    
[^36]: 通过活动和参数稀疏相结合实现高效的实时递归学习

    Efficient Real Time Recurrent Learning through combined activity and parameter sparsity. (arXiv:2303.05641v1 [cs.LG])

    [http://arxiv.org/abs/2303.05641](http://arxiv.org/abs/2303.05641)

    BPTT是训练循环神经网络(RNNs)的标准算法，需要前向和后向模拟阶段分别进行推理和学习，BPTT需要存储网络状态的完整历史记录，存储量与输入序列长度成比例。这使得BPTT不适合在线学习，并在低资源实时系统上实现是一项挑战。实时递归学习(RTRL)允许在线学习，所需内存的增长与序列长度无关。然而，RTRL的计算成本非常高，与状态大小的四次方成比例，使RTRL在所有网络规模上的计算都是不可行的。在这项工作中，我们展示了表现出高活动稀疏性的递归网络可以降低RTRL的计算成本。此外，组合活动和参数稀疏度的方法可进一步降低计算成本，实现高效实时递归学习。

    Backpropagation through time (BPTT) is the standard algorithm for training recurrent neural networks (RNNs), which requires separate simulation phases for the forward and backward passes for inference and learning, respectively. Moreover, BPTT requires storing the complete history of network states between phases, with memory consumption growing proportional to the input sequence length. This makes BPTT unsuited for online learning and presents a challenge for implementation on low-resource real-time systems. Real-Time Recurrent Learning (RTRL) allows online learning, and the growth of required memory is independent of sequence length. However, RTRL suffers from exceptionally high computational costs that grow proportional to the fourth power of the state size, making RTRL computationally intractable for all but the smallest of networks. In this work, we show that recurrent networks exhibiting high activity sparsity can reduce the computational cost of RTRL. Moreover, combining activit
    
[^37]: 可解释的目标识别: 基于证据权重的框架

    Explainable Goal Recognition: A Framework Based on Weight of Evidence. (arXiv:2303.05622v1 [cs.AI])

    [http://arxiv.org/abs/2303.05622](http://arxiv.org/abs/2303.05622)

    我们提出并评估了一种使用证据权重(WoE)框架来解释目标识别问题的可解释的目标识别(XGR)模型。我们的模型提供了人类中心的解释，回答了为什么？和为什么不？的问题。我们在八个不同的领域计算地评估了我们系统的性能。使用人类行为研究来从人类注释中获得基础事实，我们进一步展示了XGR模型可以成功生成类似人类的解释。然后，我们报告了一项研究，其中60名参与者观察代理程序玩Sokoban游戏，然后接收目标识别输出的解释。我们通过任务预测、解释满意度和信任调查参与者对解释的理解。

    We introduce and evaluate an eXplainable Goal Recognition (XGR) model that uses the Weight of Evidence (WoE) framework to explain goal recognition problems. Our model provides human-centered explanations that answer why? and why not? questions. We computationally evaluate the performance of our system over eight different domains. Using a human behavioral study to obtain the ground truth from human annotators, we further show that the XGR model can successfully generate human-like explanations. We then report on a study with 60 participants who observe agents playing Sokoban game and then receive explanations of the goal recognition output. We investigate participants' understanding obtained by explanations through task prediction, explanation satisfaction, and trust.
    
[^38]: KGNv2:基于关键点的RGB-D输入下的基于六自由度的抓取姿态合成中的尺度和姿态预测的分离

    KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Pose Synthesis on RGB-D input. (arXiv:2303.05617v1 [cs.RO])

    [http://arxiv.org/abs/2303.05617](http://arxiv.org/abs/2303.05617)

    我们提出了一种基于关键点的二维/二点五维输入的新型六自由度抓取姿态合成方法。从图像输入的基于关键点的抓取探测器在以前的研究中表现出有前途的结果，其中颜色图像提供的附加视觉信息补偿了噪声深度感知。然而，它在很大程度上依赖于准确地预测图像空间中的关键点位置。在本文中，我们设计了一个新的抓取生成网络，它减少了对精确关键点估计的依赖性。给定RGB-D输入，我们的网络既可以从关键点检测中估计抓取姿态，也可以估计接近相机的尺度。我们进一步重新设计关键点输出空间，以减轻关键点预测噪声对透视N点（PnP）算法的负面影响。实验表明，所提出的方法比基线方法表现更好，验证了我们方法的有效性。最后，尽管是在简单的合成物体上进行训练，我们的方法却表现出了很好的效果。

    We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based on keypoints. Keypoint-based grasp detector from image input has demonstrated promising results in the previous study, where the additional visual information provided by color images compensates for the noisy depth perception. However, it relies heavily on accurately predicting the location of keypoints in the image space. In this paper, we devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, our network estimates both the grasp pose from keypoint detection as well as scale towards the camera. We further re-design the keypoint output space in order to mitigate the negative impact of keypoint prediction noise to Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method outperforms the baseline by a large margin, validating the efficacy of our approach. Finally, despite trained on simple synthetic objects, our method demons
    
[^39]: 学习错误的教训：在知识蒸馏过程中插入木马。

    Learning the Wrong Lessons: Inserting Trojans During Knowledge Distillation. (arXiv:2303.05593v1 [cs.LG])

    [http://arxiv.org/abs/2303.05593](http://arxiv.org/abs/2303.05593)

    近年来，知识蒸馏已成为有效部署机器学习的基石，许多实验室和产业都使用知识蒸馏来训练经济、资源优化的模型。同时，Trojan攻击也引起了重要的关注，揭示了深度学习模型的基本漏洞。鉴于知识蒸馏的广泛应用，我们在这项工作中试图利用未标记的数据知识蒸馏过程在学生模型中嵌入木马而不引入教师中显眼的行为。我们最终设计了一种Trojan攻击，可以有效地降低学生的准确性，不会改变教师的性能，并且在实践中易于构建。

    In recent years, knowledge distillation has become a cornerstone of efficiently deployed machine learning, with labs and industries using knowledge distillation to train models that are inexpensive and resource-optimized. Trojan attacks have contemporaneously gained significant prominence, revealing fundamental vulnerabilities in deep learning models. Given the widespread use of knowledge distillation, in this work we seek to exploit the unlabelled data knowledge distillation process to embed Trojans in a student model without introducing conspicuous behavior in the teacher. We ultimately devise a Trojan attack that effectively reduces student accuracy, does not alter teacher performance, and is efficiently constructible in practice.
    
[^40]: SOCIALGYM 2.0：共享人类空间中多智能体社交机器人导航模拟器

    SOCIALGYM 2.0: Simulator for Multi-Agent Social Robot Navigation in Shared Human Spaces. (arXiv:2303.05584v1 [cs.RO])

    [http://arxiv.org/abs/2303.05584](http://arxiv.org/abs/2303.05584)

    我们提出了SocialGym 2，一个用于社交机器人研究的多智能体导航模拟器。我们的模拟器模拟了多个自主代理，复制了复杂环境中的实际动态，包括门廊、走廊、十字路口和环形交叉口。与传统的以开放空间中具有基本运动约束的单个机器人为重点的模拟器不同，SocialGym 2采用多智能体强化学习（MARL）来开发多个具有不同、动态约束的机器人的最佳导航策略。基于PettingZoo MARL库和Stable Baselines3 API，SocialGym 2提供了一个易于访问的python接口，通过ROS消息与导航堆栈集成。SocialGym 2可以轻松安装，并打包在一个docker容器中，它提供了交换和评估不同MARL算法的能力，以及自定义观察和奖励函数。我们还提供了脚本，使用户可以创建自己的环境。

    We present SocialGym 2, a multi-agent navigation simulator for social robot research. Our simulator models multiple autonomous agents, replicating real-world dynamics in complex environments, including doorways, hallways, intersections, and roundabouts. Unlike traditional simulators that concentrate on single robots with basic kinematic constraints in open spaces, SocialGym 2 employs multi-agent reinforcement learning (MARL) to develop optimal navigation policies for multiple robots with diverse, dynamic constraints in complex environments. Built on the PettingZoo MARL library and Stable Baselines3 API, SocialGym 2 offers an accessible python interface that integrates with a navigation stack through ROS messaging. SocialGym 2 can be easily installed and is packaged in a docker container, and it provides the capability to swap and evaluate different MARL algorithms, as well as customize observation and reward functions. We also provide scripts to allow users to create their own environm
    
[^41]: 自适应负样本的开放世界分类

    Open World Classification with Adaptive Negative Samples. (arXiv:2303.05581v1 [cs.CL])

    [http://arxiv.org/abs/2303.05581](http://arxiv.org/abs/2303.05581)

    开放世界分类是自然语言处理中具有关键实际意义和影响的任务。由于开放或未知类别数据仅在推断阶段显示，因此寻找具有适当决策边界以容纳已知类别的识别和开放类别的区分的模型具有挑战性。现有模型的性能受到训练阶段缺乏有效的开放类别数据或缺乏学习适当决策边界的良好机制的限制。我们提出了一种基于自适应负样本（ANS）的方法，旨在在培训阶段生成有效的合成开放类别样本，而不需要任何先前的知识或外部数据集。实证结果表明，使用辅助的一对多二进制分类器具有显着优势，这些分类器有效地利用了生成的负样本，并避免了复杂的阈值搜索阶段。

    Open world classification is a task in natural language processing with key practical relevance and impact. Since the open or {\em unknown} category data only manifests in the inference phase, finding a model with a suitable decision boundary accommodating for the identification of known classes and discrimination of the open category is challenging. The performance of existing models is limited by the lack of effective open category data during the training stage or the lack of a good mechanism to learn appropriate decision boundaries. We propose an approach based on \underline{a}daptive \underline{n}egative \underline{s}amples (ANS) designed to generate effective synthetic open category samples in the training stage and without requiring any prior knowledge or external datasets. Empirically, we find a significant advantage in using auxiliary one-versus-rest binary classifiers, which effectively utilize the generated negative samples and avoid the complex threshold-seeking stage in pr
    
[^42]: 通过对抗样本评估会话式推荐系统的鲁棒性

    Evaluating the Robustness of Conversational Recommender Systems by Adversarial Examples. (arXiv:2303.05575v1 [cs.IR])

    [http://arxiv.org/abs/2303.05575](http://arxiv.org/abs/2303.05575)

    会话式推荐系统（CRS）根据标准的推荐准确性指标迅速发展。然而，确保这些系统与用户交互的鲁棒性是至关重要的，包括正常用户和恶意用户。他们希望通过提供修改过的输入数据来攻击系统。在本文中，我们提出了一个对抗性评估方案，包括两个类别的四个方案，并自动生成对抗性示例来评估系统在面对不同输入数据时的鲁棒性。通过执行这些对抗性示例，我们可以比较不同对话式推荐系统满足用户偏好的能力。我们通过提出的对抗性示例对两个数据集上的三个CRS进行评估。我们的结果表明，这些系统都不够鲁棒和可靠以抵御对抗性示例。

    Conversational recommender systems (CRSs) are improving rapidly, according to the standard recommendation accuracy metrics. However, it is essential to make sure that these systems are robust in interacting with users including regular and malicious users who want to attack the system by feeding the system modified input data. In this paper, we propose an adversarial evaluation scheme including four scenarios in two categories and automatically generate adversarial examples to evaluate the robustness of these systems in the face of different input data. By executing these adversarial examples we can compare the ability of different conversational recommender systems to satisfy the user's preferences. We evaluate three CRSs by the proposed adversarial examples on two datasets. Our results show that none of these systems are robust and reliable to the adversarial examples.
    
[^43]: 从交互标签和语言/视觉-语言先验中仅使用弱监督进行HOI检测

    Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors. (arXiv:2303.05546v1 [cs.CV])

    [http://arxiv.org/abs/2303.05546](http://arxiv.org/abs/2303.05546)

    人-物交互（HOI）检测旨在从给定的自然图像中提取相互作用的人-物对及其交互类别。尽管构建HOI检测数据集所需的标记工作本质上比许多其他计算机视觉任务更加广泛，但由于相互作用在对象和谓词空间上的组合性质，学习弱监督下的人-物交互仍然很困难。本文使用仅有的图像级交互标签和预训练的视觉-语言模型（VLM）以及大型语言模型（LLM）来处理HOI检测。首先，我们提出了一种方法，利用视觉-语言模型的定位能力来修剪非交互人和物体的建议，从而提高袋内正样本对的质量。其次，我们使用一个L...

    Human-object interaction (HOI) detection aims to extract interacting human-object pairs and their interaction categories from a given natural image. Even though the labeling effort required for building HOI detection datasets is inherently more extensive than for many other computer vision tasks, weakly-supervised directions in this area have not been sufficiently explored due to the difficulty of learning human-object interactions with weak supervision, rooted in the combinatorial nature of interactions over the object and predicate space. In this paper, we tackle HOI detection with the weakest supervision setting in the literature, using only image-level interaction labels, with the help of a pretrained vision-language model (VLM) and a large language model (LLM). We first propose an approach to prune non-interacting human and object proposals to increase the quality of positive pairs within the bag, exploiting the grounding capability of the vision-language model. Second, we use a l
    
[^44]: 可计算连续强化学习目标是PAC可学习的

    Computably Continuous Reinforcement-Learning Objectives are PAC-learnable. (arXiv:2303.05518v1 [cs.LG])

    [http://arxiv.org/abs/2303.05518](http://arxiv.org/abs/2303.05518)

    在强化学习中，最大化折扣和有限时间累积回报的经典目标是可以被PAC学习的：有算法可以使用有限的样本和计算高概率地学习接近最优策略。近年来，研究人员介绍了超越经典累积回报的目标和相应的强化学习算法，如指定为线性时态逻辑公式的目标。然而，这些新目标的PAC可学习性问题仍然没有得到解决。这项工作通过两种分析设置的PAC可学习性的充分条件，证明了通用强化学习目标的PAC可学习性。特别是对于只考虑样本复杂度的分析，我们证明，如果作为一个oracle给出的目标是一致连续的，则它是PAC可学习的。此外，对于考虑计算复杂度的分析，我们证明如果一个目标是可计算连续的则它是PAC可学习的。

    In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is com
    

