# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities.](http://arxiv.org/abs/2401.14405) | 本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。 |
| [^2] | [Adaptive Mobile Manipulation for Articulated Objects In the Open World.](http://arxiv.org/abs/2401.14403) | 本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。 |
| [^3] | [TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation.](http://arxiv.org/abs/2401.14373) | TURNA是一种用于土耳其语的低资源语言模型，具备自然语言理解和生成任务能力。TURNA通过预训练的编码-解码架构在理解和生成任务中表现优于多语言模型，并能与土耳其语单语模型竞争。 |
| [^4] | [Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input.](http://arxiv.org/abs/2401.14371) | 本文介绍了一种仅使用延迟输入信号来实现储备计算机最优操作区域识别的高效优化技术，并通过实验证明其在不同任务和操作条件下的有效性。 |
| [^5] | [Genie: Achieving Human Parity in Content-Grounded Datasets Generation.](http://arxiv.org/abs/2401.14367) | Genie是一个用于自动生成高质量内容导向数据的方法，通过三个阶段实现：内容准备、生成和过滤。在人类评估中，生成的数据被发现是自然且高质量的，并且通过与使用人工数据训练的模型比较，我们的模型表现相当或更好。 |
| [^6] | [The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support.](http://arxiv.org/abs/2401.14362) | 本研究通过调查使用大型语言模型聊天机器人进行心理健康支持的人的经历，分析了用户如何为聊天机器人创建独特的支持角色，并介绍了在心理健康背景下将人工智能与治疗价值观相匹配的概念。研究提供了设计师处理伦理问题的建议。 |
| [^7] | [Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition.](http://arxiv.org/abs/2401.14336) | 本文提出了渐进多任务抗噪声学习（PMAL）框架和渐进多任务提取（PMD）框架，用于解决细粒度车辆识别中由图像噪声引起的类内变化问题，并取得了很高的识别准确性。 |
| [^8] | [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts.](http://arxiv.org/abs/2401.14295) | 这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。 |
| [^9] | [AST-2: Single and bi-layered 2-D acoustic soft tactile skin.](http://arxiv.org/abs/2401.14292) | 本文提出了一个创新且成本效益的设计，可以显著提高二维触觉特征估计的准确性。通过利用声学能量和分析幅度调制，可以有效改善触觉特征估计。实际测试证明了该设计的有效性，达到了显著的精度。 |
| [^10] | [POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation.](http://arxiv.org/abs/2401.14285) | POUR-Net是一个基于人群先验的过偏差低计数PET衰减映射生成网络，旨在通过有效的特征提取和全面的先验信息来提高低剂量PET的衰减映射质量。 |
| [^11] | [RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization.](http://arxiv.org/abs/2401.14280) | 本研究提出了一种创新的方法，通过使用罗马化形式的文本作为接口，有效地利用大语言模型的多语言能力。通过在印地语上的实验证明，罗马化文本不仅提高了推理效率，还在有限的预训练下实现了有竞争力的性能。这些发现表明罗马化有潜力弥合大语言模型应用中的语言障碍。 |
| [^12] | [Transformers and Cortical Waves: Encoders for Pulling In Context Across Time.](http://arxiv.org/abs/2401.14267) | 这项研究探讨了transformer网络和大脑皮层波之间的相似性，并指出了皮层波在提取感觉输入序列中的时间上下文方面的潜在应用。 |
| [^13] | [Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation.](http://arxiv.org/abs/2401.14257) | 本文提出了一个多视角草图引导的文本到3D生成框架(Sketch2NeRF)，以增加对3D生成的草图控制。通过利用预训练的2D扩散模型和神经辐射场来优化3D场景实现细粒度控制。实验证明，该方法能够合成一致的3D内容。 |
| [^14] | [AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles.](http://arxiv.org/abs/2401.14232) | 这篇论文提出了一种基于生成对抗网络的防御方法AR-GAN，用于自动驾驶车辆中的交通标志分类系统，该方法具有在各种对抗攻击下保持高性能的能力。 |
| [^15] | [Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods.](http://arxiv.org/abs/2401.14228) | 本文研究了通过参数高效微调方法训练的模块的可移植性，发现这些移植的模块在各种情景下表现出优异的性能，可以有效地重复利用任务特定知识。 |
| [^16] | [Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement.](http://arxiv.org/abs/2401.14215) | 本文提出了一个旨在解决长期对话中角色句子不具信息性的问题的框架，通过利用常识增强的角色扩展，并设计策略将相互矛盾的角色转化为包含丰富说话者信息的句子，以提高回应生成质量。 |
| [^17] | [Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation classification.](http://arxiv.org/abs/2401.14206) | 本研究利用肝脏CT扫描对结直肠癌基因突变进行分类，提出了一种基于深度学习的方法，能够在CT图像中识别CRC RAS突变家族。 |
| [^18] | [TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion.](http://arxiv.org/abs/2401.14185) | TDFNet是一种高效的音频视觉语音分离模型，基于TDANet架构，具有更少的参数，并在性能上相比现有方法提高了10%。 |
| [^19] | [Towards Autonomous Supply Chains: Definition, Characteristics, Conceptual Framework, and Autonomy Levels.](http://arxiv.org/abs/2401.14183) | 本论文提出了自主供应链（ASC）的正式定义、特征和辅助概念，提出了一个分层概念框架和供应链自治参考模型。通过案例研究展示了基于这些概念的初始ASC实施。 |
| [^20] | [Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code.](http://arxiv.org/abs/2401.14176) | 本研究旨在探索Copilot生成的Python代码中的代码异味，评估Copilot修复这些问题的能力。结果表明，有8种Python代码异味可以在Copilot生成的代码中检测到。 |
| [^21] | [The Boundaries of Tractability in Hierarchical Task Network Planning.](http://arxiv.org/abs/2401.14174) | 本论文研究了在层次化任务网络规划中三个经典问题的可计算边界，提供了在特定条件下将多项式时间可解性结果从原始任务网络推广到一般任务网络的方法，并且给出了这三个问题的参数化复杂度分析。 |
| [^22] | [Predicting Hypoxia in Brain Tumors from Multiparametric MRI.](http://arxiv.org/abs/2401.14171) | 本研究利用多参数磁共振成像(MRI)预测脑肿瘤的缺氧情况，提出了一种替代昂贵且普及率有限的FMISO PET方法。研究结果表明，深度学习模型可以有效地从MRI扫描中预测缺氧情况，并与实际值有很强的相关性。 |
| [^23] | [BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction.](http://arxiv.org/abs/2401.14166) | BayesPrompt通过无偏领域抽象解决大规模预训练语言模型在少样本推理中的泛化问题。 |
| [^24] | [Alleviating Structural Distribution Shift in Graph Anomaly Detection.](http://arxiv.org/abs/2401.14155) | 这项研究从特征视角解决了图形异常检测中的结构分布偏移问题，通过抵抗异常节点的高异质性，同时从同质邻居中受益于正常节点的学习。 |
| [^25] | [Agent-based Simulation with Netlogo to Evaluate AmI Scenarios.](http://arxiv.org/abs/2401.14153) | 本文开发了一个基于代理的模拟环境来评估AmI场景，通过测量代理满意度和节约时间来评估效益。 |
| [^26] | [True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning.](http://arxiv.org/abs/2401.14151) | 本研究通过使用大型语言模型（LLMs）作为决策智能体，通过强化学习与具身环境高效互动来解决LLMs与环境之间知识不对齐的问题。通过查询LLMs的联合概率，形成行为策略，并通过两种归一化方法和四个提示设计原则提高策略的稳定性和鲁棒性。最后，通过设计参数高效的训练架构提高学习效率。 |
| [^27] | [Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations.](http://arxiv.org/abs/2401.14142) | 基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。 |
| [^28] | [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design.](http://arxiv.org/abs/2401.14112) | FP6-LLM提出了一种支持六位量化的GPU算法-系统协同设计方案，实现了在大型语言模型中推断成本和模型质量之间的平衡。 |
| [^29] | [Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators.](http://arxiv.org/abs/2401.14110) | 本论文提出了一种简单的方法，可以训练和微调高端DNN，实现使用更便宜的12位累加器，而不会导致精度降低，并展示了使用细粒度梯度近似可以提高DNN的精度。 |
| [^30] | [CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks.](http://arxiv.org/abs/2401.14109) | CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。 |
| [^31] | [GQHAN: A Grover-inspired Quantum Hard Attention Network.](http://arxiv.org/abs/2401.14089) | GQHAN是一种受到Grover启发的量子硬注意力网络，通过柔性预言机和自适应扩散算子来解决了硬注意力机制的不可微分性，提高了处理量子数据集的效果。 |
| [^32] | [Generating Likely Counterfactuals Using Sum-Product Networks.](http://arxiv.org/abs/2401.14086) | 由于用户需求和最近的法规要求，需要对AI系统所做出的决策进行解释。本论文提出了一种使用Sum-Product Networks模拟寻找高可能性反事实推理的系统，该系统能够提供满足多个常见要求的最佳解释。 |
| [^33] | [From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures.](http://arxiv.org/abs/2401.14079) | 本文提出了一种基于人工智能技术的半自动软件架构生成方法，可以根据需求生成架构候选方案，并使用大型语言模型和定量方法进行自动评估和交叉分析。 |
| [^34] | [Ta'keed: The First Generative Fact-Checking System for Arabic Claims.](http://arxiv.org/abs/2401.14067) | Ta'keed是首个用于阿拉伯语论断的生成式事实核查系统，通过基于检索片段的论断真实性评估和LLM-based论断验证，解决了目前阿拉伯语领域缺乏生成解释论断可信度的研究。引入了一个测试黄金标签数据集，并分析了系统生成解释与黄金标准解释之间的语义相似性。研究还探讨了不同片段数量对论断分类准确性的影响。 |
| [^35] | [CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion.](http://arxiv.org/abs/2401.14066) | CreativeSynth是一种基于多模态扩散的创新统一框架，通过整合多模态特征和定制的注意力机制，实现了将现实世界的语义内容导入到艺术领域中，能够协调多模态输入和多任务，在艺术图像生成方面具有重要意义。 |
| [^36] | [Left/Right Brain, human motor control and the implications for robotics.](http://arxiv.org/abs/2401.14057) | 本研究通过训练不同的损失函数，实现了类似于人类的左右半球专门化控制系统，该系统在不同的运动任务中展现出协调性、运动效率和位置稳定性的优势。 |
| [^37] | [Towards Goal-oriented Large Language Model Prompting: A Survey.](http://arxiv.org/abs/2401.14043) | 本文调查了大型语言模型(LLM)中目标导向提示工程的重要性。通过对35个代表性研究的回顾，我们发现引导LLM遵循人类的逻辑思维的目标导向提示公式显著提高了LLM的性能。我们还提出了一个新的分类体系，并总结了十个适用任务来展示我们框架的广泛适用性。同时，我们提出了四个未来的方向，以推动目标导向提示工程的进一步发展。 |
| [^38] | [GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting.](http://arxiv.org/abs/2401.14032) | 本论文提出了一种在大规模三维重建数据集上使用高斯点云渲染的场景重构基准，通过对U-Scene数据集进行详细评估，突出了综合多模态信息在场景重建中的重要性。 |
| [^39] | [Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI.](http://arxiv.org/abs/2401.14019) | Unitxt是一个灵活、可共享和可复用的数据准备与评估库，针对生成式语言模型进行定制，实现了结构化、模块化和可定制的解决方案。该库集成了常用的库，将处理流程分解为模块化组件，促进了协作和共享。 |
| [^40] | [CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning.](http://arxiv.org/abs/2401.14011) | CMMU是一个用于中文多模态多类型问题理解和推理的基准测试，涵盖了从小学到高中的知识，提供了多项选择题、多项回答题和填空题三种类型的问题，对于评估多模态大型语言模型的智能水平具有重要意义。 |
| [^41] | [ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases.](http://arxiv.org/abs/2401.14003) | ConstraintChecker是一个针对大型语言模型的插件，用于推理常识知识库。它通过提示技术提供和检查显式约束，帮助解决了大型语言模型在处理常识知识库推理时的困难。 |
| [^42] | [Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution.](http://arxiv.org/abs/2401.13996) | 本文介绍了调查-整合-开发（ICE）策略，通过任务间的自进化来提高AI代理的适应性和灵活性。实验证明了ICE的有效性，可以显著减少API调用，同时与GPT-3.5结合使用可以在各种代理任务上达到与GPT-4相当的性能。 |
| [^43] | [Cross-Domain Few-Shot Learning via Adaptive Transformer Networks.](http://arxiv.org/abs/2401.13987) | 本文提出了一种名为ADAPTER的自适应变压器网络，用于跨领域的少样本学习。借助双向交叉注意力和标签平滑方法，ADAPTER能够在存在领域转移的情况下学习可转移的特征，并避免监督崩溃问题。该方法在BSCD-FSL基准测试中表现优秀，超过了之前的方法。 |
| [^44] | [Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning.](http://arxiv.org/abs/2401.13986) | 本文提出了一种通过解释一致性微调方法，使得大型语言模型（LLMs）在相关示例上生成更一致的自然语言解释。实验证明，该方法在不同领域的问答数据集上相对提高了10.0%的解释一致性，并且能够泛化到其他数据集。 |
| [^45] | [Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration.](http://arxiv.org/abs/2401.13979) | 本研究提出了Leeroo编排器的架构，通过集成多个训练过的LLMs模型，实现了一个新的最先进模型。该编排器在性能上与Mixtral模型相当，并且成本只有其三分之二。当允许更高的成本时，Leeroo编排器的准确性超过了Mixtral模型，并且当集成GPT4时进一步提升。 |
| [^46] | [Learning to Manipulate Artistic Images.](http://arxiv.org/abs/2401.13976) | 本文提出了一种利用无语义信息，通过自监督方式进行图像生成的任意风格图像操纵网络。该方法解决了艺术图像中准确语义获取的困难，并避免了跨域伪影和精确结构问题。实验证明了该方法在计算效率和高分辨率方面的优势。 |
| [^47] | [BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models.](http://arxiv.org/abs/2401.13974) | 提出了BootPIG架构，在预训练扩散模型中引导个性化图像生成能力。该架构使用用户提供的参考图像来指导生成图像中概念的外观。通过利用预训练的模型生成的数据进行训练，实现了对BootPIG架构的个性化能力的引导。 |
| [^48] | [Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks.](http://arxiv.org/abs/2401.13968) | 本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于动态长期时间序列预测。该模型通过快速和慢速学习器的结合，以及使用通用表示转换层，实现对动态环境的快速适应，并在实验中表现出优于基准算法的性能提升。 |
| [^49] | [General Automatic Solution Generation of Social Problems.](http://arxiv.org/abs/2401.13945) | 本研究提出了一个基于代理模型的自动社会操作系统（ASOS），用于通用社会解决方案的生成，通过超图和符号化混合策略实现了全局和局部的问题分析和规定。 |
| [^50] | [A New Paradigm for Counterfactual Reasoning in Fairness and Recourse.](http://arxiv.org/abs/2401.13935) | 本研究探索了一种新的反事实推理范式，基于回溯反事实，通过回溯已有路径，而不是想象在法律保护特征上进行的可干预的假设干预。 |
| [^51] | [LocMoE: A Low-overhead MoE for Large Language Model Training.](http://arxiv.org/abs/2401.13920) | LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。 |
| [^52] | [WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.](http://arxiv.org/abs/2401.13919) | WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。 |
| [^53] | [Spectral Clustering for Discrete Distributions.](http://arxiv.org/abs/2401.13913) | 本文提出了一种基于谱聚类和分布相似度度量的框架来解决离散分布聚类问题。通过使用线性最优传输构建相似度矩阵，我们在聚类准确性和计算效率方面取得了显著的改进。 |
| [^54] | [Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression.](http://arxiv.org/abs/2401.13904) | 本论文介绍了一种无监督的层次符号回归方法(UHiSR)，通过结合层次神经网络和符号回归，实现了自动提取化学直观的极性指数，并发现了将分子结构与色谱行为联系起来的可解释方程。 |
| [^55] | [Domain-Independent Dynamic Programming.](http://arxiv.org/abs/2401.13883) | 本文提出了一种领域无关的动态规划方法，并介绍了基于状态转移系统的动态规划描述语言。实验证明，该方法在许多组合优化问题上优于传统的混合整数规划和约束规划方法。 |
| [^56] | [TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance.](http://arxiv.org/abs/2401.13849) | 通过原则发现和指导提升学生语言模型的推理能力的TPD框架模拟了教师和学生之间的互动，通过生成问题解决指令和纠正原则，从而引导学生模型从教师的指导和自身的错误中进行学习。 |
| [^57] | [A V2X-based Privacy Preserving Federated Measuring and Learning System.](http://arxiv.org/abs/2401.13848) | 本文介绍了一种基于V2X的隐私保护联合测量和学习系统，通过V2V通信向其他车辆提供实时数据，同时通过V2N链路上的FL方案创建交通网络的预测模型。 |
| [^58] | [The Calibration Gap between Model and Human Confidence in Large Language Models.](http://arxiv.org/abs/2401.13835) | 该论文研究了大型语言模型在传达置信度方面模型和人类之间存在的差距，并发现默认解释会导致用户过高估计模型置信度和准确性。 |
| [^59] | [Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models.](http://arxiv.org/abs/2401.13827) | 本文提出了一个新的学习框架，用于估计基于马尔可夫事件的物联网设备的流量情况，并优化多个无人机的轨迹和调度策略，以降低信息时代、节省能量和提高吞吐量。 |
| [^60] | [Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face.](http://arxiv.org/abs/2401.13822) | 本研究通过分析Hugging Face上的数据集文档，提供了Hugging Face数据集生态系统的概览并洞察到数据集文档实践的关键问题。研究发现，数据集卡片完成率与数据集受欢迎程度存在显著异质性，从业者在数据集描述和数据集结构部分更为关注，而对于使用数据的考虑部分较为忽视。 |
| [^61] | [Investigating the Efficacy of Large Language Models for Code Clone Detection.](http://arxiv.org/abs/2401.13802) | 这项研究探索了大型语言模型在代码克隆检测任务中的应用。 |
| [^62] | [Multi-Object Navigation in real environments using hybrid policies.](http://arxiv.org/abs/2401.13800) | 本论文研究了在真实环境中使用混合策略进行多目标导航的问题，主要解决了导航中涉及的高级推理、局部规划和空间表示等复杂任务，在模拟环境下已有先进的模型，但在真实环境中尚未评估。 |
| [^63] | [Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning.](http://arxiv.org/abs/2401.13796) | 本文讨论了机器学习中的数据泄露问题，即未预期的信息污染训练数据，影响模型性能评估，用户可能由于缺乏理解而忽视关键步骤，导致乐观的性能估计在实际场景中不成立。 |
| [^64] | [Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility.](http://arxiv.org/abs/2401.13782) | 本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。 |
| [^65] | [AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems.](http://arxiv.org/abs/2401.13770) | AlphaMapleSAT是一种基于MCTS的Cube-and-Conquer SAT求解器，通过推理驱动的先行计算技术来高效解决困难的组合问题。 |
| [^66] | [Assumptions and Bounds in the Instrumental Variable Model.](http://arxiv.org/abs/2401.13758) | 本文证明了在工具变量（IV）模型中，具有二进制响应$Y$和二进制处理$X$的情况下，使用接受$K$个状态的仪器$Z$时，关于ACE界限的结果。 |
| [^67] | [Explaining Image Classifiers.](http://arxiv.org/abs/2401.13752) | 本论文研究了解释图像分类器的问题，并以Mothilal等人[2021]的工作为起点。论文指出Mothilal等人虽然声称使用Halpern [2016]提出的解释定义，但实际上有一定的偏差。论文还展示了Halpern的解释定义可以处理其他方法难以解决的缺失和罕见事件问题。 |
| [^68] | [A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks.](http://arxiv.org/abs/2401.13751) | 本论文提出一种系统化方法，用于针对深度卷积神经网络进行鲁棒性建模。研究发现隐藏层数量对模型的推广性能有影响，同时还测试了模型大小、浮点精度、训练数据和模型输出的噪声水平等参数。为了改进模型的预测能力和计算成本，提出了一种使用诱发故障来建模故障概率的方法。 |
| [^69] | [Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion Monitoring.](http://arxiv.org/abs/2401.13722) | 该项目使用预训练的BERT模型检测社交媒体和用户的网络浏览数据中的抑郁文本，并结合可穿戴设备的生理信号提供长期追踪和预后，有望提高早期检测抑郁症的能力，推动整体心理健康。 |
| [^70] | [Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare.](http://arxiv.org/abs/2401.13716) | 这项研究开发了一个综合质量评估框架，用于评估医疗领域中合成表格数据的质量，并解决了现有框架中存在的一些问题和限制。 |
| [^71] | [EMP: Effective Multidimensional Persistence for Graph Representation Learning.](http://arxiv.org/abs/2401.13713) | 本论文提出了EMP框架，通过同时变化多个尺度参数来探索数据，并将描述符函数整合到分析过程中，从而提供一个高度表达的数据摘要。 |
| [^72] | [Accelerating hyperbolic t-SNE.](http://arxiv.org/abs/2401.13708) | 本文提出了加速双曲线t-SNE算法，通过引入极坐标四叉树的加速结构，解决了现有方法在处理大规模输入数据时的效率问题。 |
| [^73] | [Using Java Geometry Expert as Guide in the Preparations for Math Contests.](http://arxiv.org/abs/2401.13704) | Java几何专家（JGEX）在学校环境中的使用情况及其在解决数学竞赛题目中的支持。同时也讨论了程序的局限性。 |
| [^74] | [Solving Some Geometry Problems of the N\'aboj 2023 Contest with Automated Deduction in GeoGebra Discovery.](http://arxiv.org/abs/2401.13703) | 本文使用GeoGebra Discovery软件工具，利用计算机解决了N'aboj 2023竞赛的一些几何问题，并分析了问题输入的难度和未来的目标。 |
| [^75] | [Towards Automated Readable Proofs of Ruler and Compass Constructions.](http://arxiv.org/abs/2401.13700) | 本研究提出了一种能够生成直尺和圆规构造问题的可读与形式化正确性证明的方法，并与自动定理证明器合作实现。目标是从基本公理形式化证明所有高级引理。 |
| [^76] | [Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey.](http://arxiv.org/abs/2401.13699) | 该论文调查了在物联网健康护理中利用生成式人工智能驱动的人类数字孪生的应用。人类数字孪生作为多功能、生动的人类数字测试平台，可以模拟结果并指导实际治疗，从而提高物联网健康护理的能力。 |
| [^77] | [Toward Robust Multimodal Learning using Multimodal Foundational Models.](http://arxiv.org/abs/2401.13697) | 该论文提出了一种名为TRML的框架，旨在实现在随机缺失模态的情况下的稳健多模态学习。TRML利用生成的虚拟模态替换缺失的模态，并且通过对齐语义空间来解决模态缺失的问题。 |
| [^78] | [Challenge design roadmap.](http://arxiv.org/abs/2401.13693) | 挑战设计是一项类似于推出产品的过程，需要制定有效的游戏规则，并考虑解决现实问题、推进科学技术、科学发现和教育公众等多个目标。 |
| [^79] | [Process Mining for Unstructured Data: Challenges and Research Directions.](http://arxiv.org/abs/2401.13677) | 这篇论文讨论了将过程挖掘应用于非结构化数据所面临的挑战，并提出了初步解决方案和未来的研究方向。 |
| [^80] | [Transforming Agriculture with Intelligent Data Management and Insights.](http://arxiv.org/abs/2401.13672) | 使用智能数据管理和洞察力可以解决现代农业面临的增长需求和气候变化等挑战，通过数据创新可以提高农业生产力、可持续性和适应性。 |
| [^81] | [Inadequacy of common stochastic neural networks for reliable clinical decision support.](http://arxiv.org/abs/2401.13657) | 本研究调查了随机神经网络在临床应用中的可靠性，并发现常见的深度学习方法在数据转移情况下过于自信。这强调了对本地不确定性可靠估计及其向最终用户传达的重要性。 |
| [^82] | [Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors.](http://arxiv.org/abs/2401.13652) | 本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。 |
| [^83] | [Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions.](http://arxiv.org/abs/2401.13324) | 本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。 |
| [^84] | [Visibility into AI Agents.](http://arxiv.org/abs/2401.13138) | 本文评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录，并提出了可能的实施方式。这些措施在集中化和去中心化部署环境中应用广泛，有助于理解和减轻AI代理带来的风险。 |
| [^85] | [Binary structured physics-informed neural networks for solving equations with rapidly changing solutions.](http://arxiv.org/abs/2401.12806) | 本论文提出了一种二进制结构的物理信息神经网络框架，通过利用二进制结构来捕捉局部特征，并解决了传统物理信息神经网络在处理具有快速变化解的方程时的困难。 |
| [^86] | [What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition.](http://arxiv.org/abs/2401.12756) | 本研究提出了一个新的零样本模块组合框架，统一了选择、加权和组合参数模块的各种变化。以领域知识和适配器层为场景，通过系统化的统一概念，进行了首次全面的零样本知识组合的基准研究。 |
| [^87] | [Energy-based Automated Model Evaluation.](http://arxiv.org/abs/2401.12689) | 提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。 |
| [^88] | [BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models.](http://arxiv.org/abs/2401.12522) | BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。 |
| [^89] | [Instructional Fingerprinting of Large Language Models.](http://arxiv.org/abs/2401.12255) | 这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。 |
| [^90] | [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.](http://arxiv.org/abs/2401.10529) | Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。 |
| [^91] | [Top in Chinese Data Processing: English Code Models.](http://arxiv.org/abs/2401.10286) | 在中文数据处理中，基于代码的语言模型在非编程中文任务中表现出色，尤其是在对中文幻觉敏感的任务中。此研究为讨论“中文房间”思想实验提供了独特的视角。 |
| [^92] | [SAiD: Speech-driven Blendshape Facial Animation with Diffusion.](http://arxiv.org/abs/2401.08655) | 提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画方法，通过轻量级的Transformer-based U-Net模型和音频与视觉的交叉模态对齐偏差，实现了较好的唇部同步和更多样化的唇部运动。 |
| [^93] | [DiConStruct: Causal Concept-based Explanations through Black-Box Distillation.](http://arxiv.org/abs/2401.08534) | DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。 |
| [^94] | [CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians.](http://arxiv.org/abs/2401.05925) | CoSSegGaussians是一种紧凑且迅速的3D高斯场景分割方法，通过映射空间和语义特征实现紧凑和可靠的零样本场景分割。 |
| [^95] | [Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices.](http://arxiv.org/abs/2401.03233) | 本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。 |
| [^96] | [Can AI Be as Creative as Humans?.](http://arxiv.org/abs/2401.01623) | 本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。 |
| [^97] | [Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation.](http://arxiv.org/abs/2312.15643) | 这篇论文介绍了一种复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。研究发现，过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。为了解决推广到未见过观察结果的问题，引入了基于知识图谱的强化学习方法（RLF-KG），最小化观察结果与结论之间的差异。 |
| [^98] | [Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing.](http://arxiv.org/abs/2312.14472) | 本文提出了具有动态深度路由的多任务深度强化学习框架(D2R)，该框架灵活选择不同任务所需的模块数量，并通过引入ResRouting方法和自动路由平衡机制来解决存在的问题。 |
| [^99] | [An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training.](http://arxiv.org/abs/2312.11819) | 提出了一种自适应模型部署和并行框架，用于加速RLHF训练。该框架提供了两种灵活的模型部署策略，其中交替策略有助于减少内存冗余和通信成本。 |
| [^100] | [A Survey of Reasoning with Foundation Models.](http://arxiv.org/abs/2312.11562) | 本文调查了使用基础模型进行推理的研究，介绍了最新的推理任务、方法和基准，并讨论了基础模型中推理能力的未来发展方向。 |
| [^101] | [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs.](http://arxiv.org/abs/2312.05934) | 该研究比较了无监督的微调和检索增强生成（RAG）这两种常见方法在LLMs中的应用。结果发现，RAG在现有知识和新知识上表现出更好的性能，而LLMs通过无监督的微调学习新的事实信息较困难。 |
| [^102] | [Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks.](http://arxiv.org/abs/2312.03365) | 本文重点研究了基于物理启发神经网络的蒙特卡洛树搜索控制方法在住宅建筑供暖的需求响应中的应用，该方法通过灵活的优化整合了外部约束条件，为实现能源消耗的优化和用户热舒适度的保证提供了有前景的解决方案。 |
| [^103] | [Perceptual Group Tokenizer: Building Perception with Iterative Grouping.](http://arxiv.org/abs/2311.18296) | 本研究提出了一种名为感知组分词器的模型，通过迭代分组操作来提取视觉特征并进行自我监督表示学习。实验证明，该模型能够与最先进的视觉架构相竞争，并具有自适应计算的优点。 |
| [^104] | [IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction.](http://arxiv.org/abs/2311.15193) | 本研究提出了一种交互感知的 LSTM 模型，用于行人轨迹预测。通过引入新的基于相关熵的机制，可以衡量人与人之间交互的相对重要性，并为每个行人建立个人空间。这个数据驱动机制可以有效提取动态人与人之间的特征表示。 |
| [^105] | [General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level.](http://arxiv.org/abs/2311.13892) | 本文提出了一种名为“通用短语去偏器”的自动多标记去偏管道，能够有效减轻掩码语言模型中的短语级别偏见，并在标准数据集和指标上取得了最新成果。 |
| [^106] | [Meta Prompting for AGI Systems.](http://arxiv.org/abs/2311.11482) | 本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。 |
| [^107] | [2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection.](http://arxiv.org/abs/2311.08543) | 这篇论文介绍了一种新颖的二维神经网络方法，用于高运动性场景下无线通信中的OTFS符号检测。该方法将OTFS系统的领域知识与在线子帧符号检测的设计结合起来，并通过引入二维循环填充和滤波结构来实现。 |
| [^108] | [Leveraging Large Language Models for Collective Decision-Making.](http://arxiv.org/abs/2311.04928) | 本论文提出了一种利用大型语言模型（LLM）促进集体决策的系统，通过管理对话和平衡个人偏好来提供满足成员需求的选项，实现高效协调并不断优化系统性能。 |
| [^109] | [Contextual Confidence and Generative AI.](http://arxiv.org/abs/2311.01193) | 本文讨论了在面对生成型人工智能对上下文置信度的挑战时，采取的两类策略：遏制策略和动员策略。 |
| [^110] | [A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets.](http://arxiv.org/abs/2310.07799) | 本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。 |
| [^111] | [Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction.](http://arxiv.org/abs/2310.05207) | 本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。 |
| [^112] | [Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals.](http://arxiv.org/abs/2309.09404) | 通过利用开放数据和人工智能方法，我们设计了一个系统来推荐团队，使得每个团队能够满足项目要求的技能覆盖，并且平衡候选成员之间的工作分配。 |
| [^113] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^114] | [Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning.](http://arxiv.org/abs/2309.03251) | 本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。 |
| [^115] | [Multi-Objective Optimization for Sparse Deep Neural Network Training.](http://arxiv.org/abs/2308.12243) | 这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。 |
| [^116] | [Arithmetic with Language Models: from Memorization to Computation.](http://arxiv.org/abs/2308.01154) | 本研究探索了使用语言模型进行算术计算的能力，发现语言模型可以通过内部的值空间进行计算，并取得了成功的实验结果。 |
| [^117] | [KoBBQ: Korean Bias Benchmark for Question Answering.](http://arxiv.org/abs/2307.16778) | 本文介绍了KoBBQ，一个针对韩国文化的偏见基准数据集，提出了一个通用框架来解决数据集的文化适应性问题，并通过大规模调查收集和验证了反映韩国文化刻板印象的社会偏见和偏见目标。 |
| [^118] | [Unsupervised Conditional Slot Attention for Object Centric Learning.](http://arxiv.org/abs/2307.09437) | 本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。 |
| [^119] | [Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks.](http://arxiv.org/abs/2307.03761) | 该论文提出了一种基于图的方法，命名为DyGATAD，用于在异构传感器网络中进行异常检测。该方法利用动态图注意力机制来识别集体异常行为，其中异常行为可能由于系统内部相互关系的变化引起。这在工业物联网监控系统中具有重要的实际应用价值。 |
| [^120] | [Realistic Synthetic Financial Transactions for Anti-Money Laundering Models.](http://arxiv.org/abs/2306.16424) | 本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。 |
| [^121] | [A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling.](http://arxiv.org/abs/2306.14806) | 我们提出了一种正负样本度量学习框架（P3M）用于具有不完整标注的文档级关系抽取，通过拉近实体嵌入和其对应关系嵌入的距离，同时使其与非类别关系嵌入的距离推远，以提高模型的泛化能力。 |
| [^122] | [System-Level Natural Language Feedback.](http://arxiv.org/abs/2306.13588) | 本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了通过任务度量设计和语言模型提示设计，如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型，并展示了使用系统级别反馈和实例级别反馈的有效性。 |
| [^123] | [Unifying Large Language Models and Knowledge Graphs: A Roadmap.](http://arxiv.org/abs/2306.08302) | 本文提出了一个前瞻性的统一大型语言模型和知识图谱的路线图，通过三个框架：增强KGs的LLMs，知识增强KGs和LLMs与KGs的联合推理，综合利用两者的优点。 |
| [^124] | [GateON: an unsupervised method for large scale continual learning.](http://arxiv.org/abs/2306.01690) | GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。 |
| [^125] | [Cross-Lingual Transfer Learning for Low-Resource Speech Translation.](http://arxiv.org/abs/2306.00789) | 提出了一种三步跨语言迁移学习框架，通过在现有框架中增加一步语义知识蒸馏，该方法有效地增强了自动语音翻译中从高资源语言到低资源语言的跨语言迁移能力，显著改善了翻译性能，特别是对于低资源语言，并减少了跨语言迁移间隙(TRFGap)。 |
| [^126] | [A transformer-based method for zero and few-shot biomedical named entity recognition.](http://arxiv.org/abs/2305.04928) | 本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。 |
| [^127] | [The effectiveness of MAE pre-pretraining for billion-scale pretraining.](http://arxiv.org/abs/2303.13496) | 本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。 |
| [^128] | [Rotation Invariant Quantization for Model Compression.](http://arxiv.org/abs/2303.03106) | 本研究提出了一种旋转不变量量化（RIQ）技术，可以在不同层次上实现混合精度量化，用于后训练神经网络模型压缩，并证明了其在压缩方面的优势。在多种模型和任务上进行了严格评估，取得了令人满意的结果。 |
| [^129] | [Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging.](http://arxiv.org/abs/2302.01622) | 本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。 |
| [^130] | [GNN-based Passenger Request Prediction.](http://arxiv.org/abs/2301.02515) | 本文开发了一个基于图神经网络和注意力机制的模型来预测乘客的起点-终点（OD）流。该模型利用各种线性和非线性依赖关系，并捕捉重复模式和上下文数据。通过广泛的仿真实验，结果显示我们的模型性能优于现有基线模型。 |
| [^131] | [HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks.](http://arxiv.org/abs/2211.01839) | 该论文介绍了一种名为HyperSound的方法，利用超网络生成音频信号的隐式神经表示。与其他模型相比，该方法在重构声波方面具有可比较的质量。 |
| [^132] | [Learning Individual Treatment Effects under Heterogeneous Interference in Networks.](http://arxiv.org/abs/2210.14080) | 本文针对网络观测数据中个体治疗效应的估计问题，提出了一种新颖的双加权回归（DWR）算法，通过同时学习注意权重来解决网络干扰和异质干预的挑战。 |
| [^133] | [Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds.](http://arxiv.org/abs/2210.14051) | 本论文证明了使用分布式强化学习方法可以实现风险敏感强化学习的遗憾保证问题，并提出了两种新颖算法，其遗憾上界与先前方法相匹配。 |
| [^134] | [Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making.](http://arxiv.org/abs/2209.11812) | 本研究探讨了基于特征的解释对AI辅助决策公正性的影响，发现解释可以影响公正性感知和人们对AI建议的依赖。然而，解释并不能帮助人们区分正确和错误的AI建议。这些发现对于促进有效的决策和公正性至关重要。 |
| [^135] | [A note on incorrect inferences in non-binary qualitative probabilistic networks.](http://arxiv.org/abs/2208.09344) | 本文指出了非二进制定性概率网络中的错误推理问题，并提供了示例和解决方案讨论。 |
| [^136] | [Graph Representation Learning via Diversity-preserving Graph Refinement.](http://arxiv.org/abs/2103.07295) | 该论文提出了一种基于多样性保持的图结构细化的图表示学习方法，它利用已学习的节点表示来逐步改善图形结构质量。在多个下游任务上，包括节点分类、链接预测和图聚类，实验表明该方法优于现有的最先进方法。 |

# 详细

[^1]: 多模态路径：通过其他模态的无关数据来改进Transformer

    Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])

    [http://arxiv.org/abs/2401.14405](http://arxiv.org/abs/2401.14405)

    本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。

    

    我们提出使用来自其他模态的无关数据来改进特定模态的Transformer，例如，使用音频或点云数据集来改进ImageNet模型。我们强调目标模态的数据样本与其他模态无关，这与利用不同模态的配对数据（如CLIP）或交错数据的其他方法不同。我们提出了一种名为多模态路径的方法-给定目标模态和设计用于该模态的Transformer，我们使用使用另一个模态的数据训练的辅助Transformer，并构建路径来连接两个模型的组件，以便目标模态的数据可以被两个模型处理。通过这种方式，我们利用了从两个模态获得的Transformer的通用序列建模能力。作为具体实现，我们通常使用特定模态的tokenizer和任务特定的head，但是利用辅助模型的Transformer block。

    We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
    
[^2]: 自适应移动操作在开放环境中的可关节物体研究

    Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])

    [http://arxiv.org/abs/2401.14403](http://arxiv.org/abs/2401.14403)

    本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。

    

    在开放的无结构环境中部署机器人一直是一个长期存在的问题。然而，机器人通常只在封闭的实验室环境中进行研究，之前的移动操作工作也仅限于拾取、移动、放置，这在这一领域中只是冰山一角。在本文中，我们引入了开放世界移动操作系统，采用全栈方法来解决现实世界中可关节物体的操作，例如真实世界中的门、柜子、抽屉和冰箱。机器人利用自适应学习框架，通过行为克隆先从一小组数据中学习，然后通过在线实践学习来处理训练分布之外的新对象。我们还开发了一个低成本的移动操作硬件平台，能够在无结构环境中进行安全和自主的在线适应，成本约为20,000美元。在我们的实验中，我们使用了20个可关节的物体。

    Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
    
[^3]: TURNA: 一种用于增强理解和生成的土耳其编码-解码语言模型

    TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation. (arXiv:2401.14373v1 [cs.CL])

    [http://arxiv.org/abs/2401.14373](http://arxiv.org/abs/2401.14373)

    TURNA是一种用于土耳其语的低资源语言模型，具备自然语言理解和生成任务能力。TURNA通过预训练的编码-解码架构在理解和生成任务中表现优于多语言模型，并能与土耳其语单语模型竞争。

    

    自然语言处理的最新进展主要偏向于资源丰富且以英语为中心的模型，这导致了与资源稀缺的语言之间存在显著差距。在这项工作中，我们介绍了TURNA语言模型，该模型针对资源稀缺的土耳其语开发，能够进行自然语言理解和生成任务。TURNA使用基于统一框架UL2的编码-解码架构进行预训练，并且我们专门为此目的筛选了一个多样的语料库。我们对TURNA在土耳其语的三个生成任务和五个理解任务上进行了评估。结果表明，TURNA在理解和生成任务上优于多语言模型，并与土耳其语单语模型在理解任务上竞争。TURNA已在https://huggingface.co/boun-tabi-LMG/TURNA 上提供。

    The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .
    
[^4]: 仅使用延迟输入的物理储备计算机的高效优化

    Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input. (arXiv:2401.14371v1 [cs.ET])

    [http://arxiv.org/abs/2401.14371](http://arxiv.org/abs/2401.14371)

    本文介绍了一种仅使用延迟输入信号来实现储备计算机最优操作区域识别的高效优化技术，并通过实验证明其在不同任务和操作条件下的有效性。

    

    我们提出了一种最近提出的用于储备计算的优化技术的实验验证，使用了光电子设置。储备计算是一种用于信号处理应用的稳健框架，高效优化方法的开发仍然是一个关键挑战。我们提出的技术仅利用延迟版本的输入信号来识别储备的最佳操作区域，简化了传统上耗时的超参数调整任务。我们验证了该方法在不同基准任务和储备操作条件下的有效性。

    We present an experimental validation of a recently proposed optimization technique for reservoir computing, using an optoelectronic setup. Reservoir computing is a robust framework for signal processing applications, and the development of efficient optimization approaches remains a key challenge. The technique we address leverages solely a delayed version of the input signal to identify the optimal operational region of the reservoir, simplifying the traditionally time-consuming task of hyperparameter tuning. We verify the effectiveness of this approach on different benchmark tasks and reservoir operating conditions.
    
[^5]: Genie：实现内容导向数据集生成的人类水平

    Genie: Achieving Human Parity in Content-Grounded Datasets Generation. (arXiv:2401.14367v1 [cs.CL])

    [http://arxiv.org/abs/2401.14367](http://arxiv.org/abs/2401.14367)

    Genie是一个用于自动生成高质量内容导向数据的方法，通过三个阶段实现：内容准备、生成和过滤。在人类评估中，生成的数据被发现是自然且高质量的，并且通过与使用人工数据训练的模型比较，我们的模型表现相当或更好。

    

    对于内容导向生成任务，缺乏高质量的数据被认为是推动这些任务发展的主要障碍。为了解决这个问题，我们提出了Genie，一种用于自动生成高质量内容导向数据的新方法。它包括三个阶段：（a）内容准备，（b）生成：从内容中创建特定任务的示例（例如问题-答案对或摘要），（c）过滤机制，旨在确保生成数据的质量和可信度。我们通过生成三个大规模的合成数据来展示这种方法：长型问题回答（LFQA）、摘要和信息提取。在人类评估中，我们生成的数据被发现是自然且高质量的。此外，我们将使用我们的数据训练的模型与使用人工编写的数据训练的模型进行比较 - 对于LFQA，我们与ELI5和ASQA进行比较，对于摘要，我们与CNN-DailyMail进行比较。我们表明，我们的模型与或超过使用人工数据训练的模型。

    The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained 
    
[^6]: 打字疗法：大型语言模型聊天机器人在心理健康支持中的应用经验

    The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support. (arXiv:2401.14362v1 [cs.HC])

    [http://arxiv.org/abs/2401.14362](http://arxiv.org/abs/2401.14362)

    本研究通过调查使用大型语言模型聊天机器人进行心理健康支持的人的经历，分析了用户如何为聊天机器人创建独特的支持角色，并介绍了在心理健康背景下将人工智能与治疗价值观相匹配的概念。研究提供了设计师处理伦理问题的建议。

    

    越来越多的人使用大型语言模型（LLM）聊天机器人作为心理健康支持工具，但有证据表明通用型LLM聊天机器人也存在一定风险，如果设计不负责任可能会危及用户的福祉。本研究调查了使用LLM聊天机器人进行心理健康支持的人的真实经历。我们通过对来自不同国家背景的21个个人进行访谈，分析了用户如何为他们的聊天机器人创建独特的支持角色，填补日常护理的空白，并在寻求来自聊天机器人的支持时如何导航相关的文化限制。我们将分析基于心理治疗文献中有效支持的概念，并引入了AI与心理健康背景下的治疗价值观对其进行匹配的概念。我们的研究提供了设计师如何处理伦理问题的建议。

    People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethica
    
[^7]: 渐进多任务抗噪声学习和提取框架用于细粒度车辆识别

    Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition. (arXiv:2401.14336v1 [cs.CV])

    [http://arxiv.org/abs/2401.14336](http://arxiv.org/abs/2401.14336)

    本文提出了渐进多任务抗噪声学习（PMAL）框架和渐进多任务提取（PMD）框架，用于解决细粒度车辆识别中由图像噪声引起的类内变化问题，并取得了很高的识别准确性。

    

    细粒度车辆识别（FGVR）是智能交通系统中的基础技术，但由于内在的类内变化非常困难。之前的FGVR研究大多只关注由不同拍摄角度、位置等引起的类内变化，而由图像噪声引起的类内变化很少受到关注。本文提出了一种渐进多任务抗噪声学习（PMAL）框架和渐进多任务提取（PMD）框架，用于解决由于图像噪声引起的FGVR中的类内变化问题。PMAL框架通过将图像去噪视为图像识别中的附加任务，并逐步强制模型学习噪声不变性，从而实现高识别准确性。PMD框架将PMAL训练过的模型的知识转移到原始骨干网络中，产生一个具有与PMAL训练模型相同识别准确性的模型，但不需要额外添加任何东西。

    Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation. Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention. This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise. The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance. The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any add
    
[^8]: 推理的拓扑学：揭秘思维链、树和图

    Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])

    [http://arxiv.org/abs/2401.14295](http://arxiv.org/abs/2401.14295)

    这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。

    

    自然语言处理（NLP）领域近年来取得了显著进展，特别是在通过创新的提示技术提高大型语言模型（LLM）性能方面。其中，与结构相结合的提示工程被视为一种有前途的范式，其设计如思维链、思维树或思维图等，通过结构指导整体LLM推理过程。通过大量实例的说明，这种范式显著增强了LLM在逻辑或数学推理、规划或创造性写作等各种任务中的能力。为了方便理解这个不断发展的领域并为未来的发展铺平道路，我们设计了一个有效和高效的LLM推理方案的通用蓝图。为此，我们对提示执行流程进行了深入分析，澄清并明确定义了不同的概念。然后我们建立第一个分类系统

    The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
    
[^9]: AST-2:单层和双层二维声学软触觉皮肤

    AST-2: Single and bi-layered 2-D acoustic soft tactile skin. (arXiv:2401.14292v1 [cs.RO])

    [http://arxiv.org/abs/2401.14292](http://arxiv.org/abs/2401.14292)

    本文提出了一个创新且成本效益的设计，可以显著提高二维触觉特征估计的准确性。通过利用声学能量和分析幅度调制，可以有效改善触觉特征估计。实际测试证明了该设计的有效性，达到了显著的精度。

    

    本文旨在提出一种创新且具有成本效益的声学软触觉(AST)皮肤设计，主要目标是显著提高二维触觉特征估计的准确性。现有的挑战在于使用成本效益的解决方案实现精确的触觉特征估计，特别是涉及接触几何特征。我们假设通过在感测表面下的两层专用声学通道中利用声学能量，并分析幅度调制，可以有效解码感测表面上的交互作用，从而改善触觉特征估计。我们的方法涉及硬件组件明确分离，负责发射和接收声学信号，从而实现模块化和高度可定制的皮肤设计。实际测试证明了这种新颖设计的有效性，达到了在估计接触法向力(MAE <0.8 N)和二维接触定位方面的显著精度

    This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation. The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions. We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation. Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design. Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE < 0.8 N), 2D contact localisation (M
    
[^10]: POUR-Net:一种基于人群先验的过偏差低计数PET衰减映射生成网络

    POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation. (arXiv:2401.14285v1 [cs.CV])

    [http://arxiv.org/abs/2401.14285](http://arxiv.org/abs/2401.14285)

    POUR-Net是一个基于人群先验的过偏差低计数PET衰减映射生成网络，旨在通过有效的特征提取和全面的先验信息来提高低剂量PET的衰减映射质量。

    

    低剂量PET为PET成像中减少辐射暴露提供了一种宝贵的手段。然而，目前通行的使用额外的CT扫描来生成PET衰减映射（u-map）的做法显著提高了辐射剂量。为了解决这个问题并进一步减轻低剂量PET检查中的辐射暴露，我们提出了POUR-Net——一种创新的基于人群先验的过偏差低计数PET衰减映射生成网络。首先，POUR-Net集成了一种过偏差低计数网络（OUR-Net），以促进有效的特征提取，包括低分辨率的抽象特征和细节特征，以辅助全分辨率级别上的深度生成。其次，作为OUR-Net的补充，一个利用全面的CT衍生的u-map数据集的人群先验生成机（PPGM）提供额外的先验信息来辅助OUR-Net的生成。OUR-Net和PPGM的集成在低剂量PET衰减映射生成中提供了更高质量的结果。

    Low-dose PET offers a valuable means of minimizing radiation exposure in PET imaging. However, the prevalent practice of employing additional CT scans for generating attenuation maps (u-map) for PET attenuation correction significantly elevates radiation doses. To address this concern and further mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an innovative population-prior-aided over-under-representation network that aims for high-quality attenuation map generation from low-dose PET. First, POUR-Net incorporates an over-under-representation network (OUR-Net) to facilitate efficient feature extraction, encompassing both low-resolution abstracted and fine-detail features, for assisting deep generation on the full-resolution level. Second, complementing OUR-Net, a population prior generation machine (PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional prior information to aid OUR-Net generation. The integration of OUR-Net and PPGM within
    
[^11]: RomanSetu: 通过罗马化有效地利用大语言模型的多语言能力

    RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])

    [http://arxiv.org/abs/2401.14280](http://arxiv.org/abs/2401.14280)

    本研究提出了一种创新的方法，通过使用罗马化形式的文本作为接口，有效地利用大语言模型的多语言能力。通过在印地语上的实验证明，罗马化文本不仅提高了推理效率，还在有限的预训练下实现了有竞争力的性能。这些发现表明罗马化有潜力弥合大语言模型应用中的语言障碍。

    

    本研究解决了将大型语言模型扩展到非英语语言（特别是使用非拉丁字母表的语言）的挑战。我们提出了一种创新的方法，利用罗马化形式的文本作为大语言模型的接口，假设频繁的非正式使用和与英语共享的标记有助于跨语言对齐。我们以印地语为重点，通过印地语到英语的翻译和情感分析任务，证明罗马化文本不仅由于其较低的生产力而显著改善了推理效率，还在有限的预训练中实现了有竞争力的性能。此外，我们的新颖的多脚本提示方法结合了罗马化和原生文本，在进一步提高任务性能方面显示出潜力。这些发现表明罗马化在弥合大语言模型应用中的语言障碍方面具有潜力，未来的工作将致力于将此方法扩展到更多的语言和任务。

    This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.
    
[^12]: Transformers和大脑皮层波：在时间上传递上下文的编码器

    Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])

    [http://arxiv.org/abs/2401.14267](http://arxiv.org/abs/2401.14267)

    这项研究探讨了transformer网络和大脑皮层波之间的相似性，并指出了皮层波在提取感觉输入序列中的时间上下文方面的潜在应用。

    

    类似ChatGPT和其他大语言模型（LLM）的transformer网络的能力已经引起了世界的关注。它们的性能依赖于将完整的输入序列（例如句子中的所有单词）转化为一个长的“编码向量”，使得transformer能够学习自然序列中的长程时间依赖关系。具体而言，“自注意力”应用于这个编码向量，通过计算输入序列中单词对之间的关联，增强了transformer中的时间上下文。我们认为神经活动在单个皮层区域内或整个大脑范围内传播的波可以实现类似的编码原理。通过在每个时刻将最近的输入历史封装为单个空间模式，皮层波可以从感觉输入序列中提取时间上下文，这与计算原理相同。

    The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long "encoding vector" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in
    
[^13]: Sketch2NeRF: 多视角草图引导的文本到3D生成

    Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])

    [http://arxiv.org/abs/2401.14257](http://arxiv.org/abs/2401.14257)

    本文提出了一个多视角草图引导的文本到3D生成框架(Sketch2NeRF)，以增加对3D生成的草图控制。通过利用预训练的2D扩散模型和神经辐射场来优化3D场景实现细粒度控制。实验证明，该方法能够合成一致的3D内容。

    

    最近，文本到3D的方法通过文本描述实现了高保真度的3D内容生成。然而，生成的对象是随机的，并且缺乏细粒度的控制。草图提供了一种廉价的方法来引入这种细粒度的控制。然而，由于草图的抽象和模糊性，实现从这些草图中获得灵活的控制是具有挑战性的。在本文中，我们提出了一个多视角草图引导的文本到3D生成框架（即Sketch2NeRF），以增加对3D生成的草图控制。具体而言，我们的方法利用预训练的2D扩散模型（例如Stable Diffusion和ControlNet）来监督由神经辐射场（NeRF）表示的3D场景的优化。我们提出了一种新颖的同步生成和重建方法，以有效优化NeRF。在实验中，我们收集了两种多视角草图数据集来评估所提出的方法。我们证明我们的方法可以合成一致的3D内容。

    Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents w
    
[^14]: AR-GAN: 基于生成对抗网络的自动驾驶车辆交通标志分类系统的防御方法

    AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles. (arXiv:2401.14232v1 [cs.CV])

    [http://arxiv.org/abs/2401.14232](http://arxiv.org/abs/2401.14232)

    这篇论文提出了一种基于生成对抗网络的防御方法AR-GAN，用于自动驾驶车辆中的交通标志分类系统，该方法具有在各种对抗攻击下保持高性能的能力。

    

    本研究开发了一种基于生成对抗网络（GAN）的防御方法，用于自动驾驶车辆中的交通标志分类，称为攻击鲁棒的GAN（AR-GAN）。AR-GAN的创新之处在于（i）假设对抗攻击模型和样本一无所知，（ii）在各种对抗攻击类型下始终提供高水平的交通标志分类性能。AR-GAN分类系统由一个通过重建去噪图像的生成器和一个对重建图像进行分类的分类器组成。作者在没有攻击和各种对抗攻击（如快速梯度符号法（FGSM），DeepFool，Carlini和Wagner（C&W），以及投影梯度下降（PGD））的情况下测试了AR-GAN。作者考虑了这些攻击的两种形式，即（i）黑盒攻击（假设攻击者对分类器没有任何先验知识），以及（ii）白盒攻击（假设攻击者拥有完全知识）。

    This study developed a generative adversarial network (GAN)-based defense method for traffic sign classification in an autonomous vehicle (AV), referred to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i) assuming zero knowledge of adversarial attack models and samples and (ii) providing consistently high traffic sign classification performance under various adversarial attack types. The AR-GAN classification system consists of a generator that denoises an image by reconstruction, and a classifier that classifies the reconstructed image. The authors have tested the AR-GAN under no-attack and under various adversarial attacks, such as Fast Gradient Sign Method (FGSM), DeepFool, Carlini and Wagner (C&W), and Projected Gradient Descent (PGD). The authors considered two forms of these attacks, i.e., (i) black-box attacks (assuming the attackers possess no prior knowledge of the classifier), and (ii) white-box attacks (assuming the attackers possess full knowledge
    
[^15]: 评估通过参数高效微调方法训练的参数矩阵的可移植性

    Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])

    [http://arxiv.org/abs/2401.14228](http://arxiv.org/abs/2401.14228)

    本文研究了通过参数高效微调方法训练的模块的可移植性，发现这些移植的模块在各种情景下表现出优异的性能，可以有效地重复利用任务特定知识。

    

    随着训练规模越来越大的语言模型的成本增加，对重复利用先前学到的知识的兴趣也在增加。迁移学习方法表明，重复利用非任务特定知识可以帮助后续特定任务学习。本文研究了相反的情况：将从一个模型移植编码任务特定知识的完整功能模块到另一个模型。我们设计了一项包括1,440个训练/测试运行的研究，以测试通过参数高效微调(PEFT)技术训练的模块的可移植性，以情感分析为示例任务。我们在各种场景中测试了可移植性，涉及不同的PEFT技术和不同的预训练主机模型，等等。我们将移植的模块的性能与(i)从头开始训练的相等模块的性能和(ii)从与移植的模块相同分布的参数中采样训练的模块进行比较。我们发现，移植的模块的性能远远超过所测试的两种替代方案的性能，但需注意一些局限性。

    As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but t
    
[^16]: 通过上下文感知个性化细化，增强长期对话中的常识增强性内存构建和管理

    Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])

    [http://arxiv.org/abs/2401.14215](http://arxiv.org/abs/2401.14215)

    本文提出了一个旨在解决长期对话中角色句子不具信息性的问题的框架，通过利用常识增强的角色扩展，并设计策略将相互矛盾的角色转化为包含丰富说话者信息的句子，以提高回应生成质量。

    

    在长期对话中，记忆和利用说话者的角色是生成回应的常见做法。然而，人工编写的数据集通常提供无信息的角色句子，这妨碍了回应质量。本文提出了一个新颖的框架，利用常识增强的角色扩展来解决长期对话中的这些问题。以前的工作侧重于不产生与其他角色相矛盾的角色，我们侧重于根据设计的策略，将相互矛盾的角色转化为包含丰富说话者信息的句子，以此来细化它们的上下文背景。作为多会话情境中角色扩展的先驱，我们的框架通过类人个性细化促进了更好的回应生成。

    Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
    
[^17]: 利用肝脏CT扫描进行结直肠癌基因突变分类的研究

    Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation classification. (arXiv:2401.14206v1 [eess.IV])

    [http://arxiv.org/abs/2401.14206](http://arxiv.org/abs/2401.14206)

    本研究利用肝脏CT扫描对结直肠癌基因突变进行分类，提出了一种基于深度学习的方法，能够在CT图像中识别CRC RAS突变家族。

    

    肝脏是结直肠癌（CRC）患者远端转移最常见的器官，了解病变的突变状态对正确设计最佳个体化治疗非常重要。迄今为止，人们为了开发非侵入性和实时的方法，通过使用新的人工智能工具分析计算机断层扫描（CT）所得到的肿瘤影像，以便分析整个肿瘤。为了解决目前基于活检分析的医疗工作流程，我们提出了首个基于深度学习的方法，通过患者医学影像进行突变分类。我们提出了i）一个管理可用CT扫描数据集的可靠流程和ii）基因突变诊断支持的基线研究，以进行预防性患者随访。我们的方法能够在CT图像中以0.73的F1得分识别CRC RAS突变家族。

    The liver is the most involved organ by distant metastasis in colon-rectal cancer (CRC) patients and it comes necessary to be aware of the mutational status of the lesions to correctly design the best individual treatment. So far, efforts have been made in order to develop non-invasive and real-time methods that permit the analysis of the whole tumor, using new artificial intelligence tools to analyze the tumor's image obtained by Computed Tomography (CT) scan. In order to address the current medical workflow, that is biopsy analysis-based, we propose the first DeepLearning-based exploration, to our knowledge, of such classification approach from the patient medical imaging. We propose i) a solid pipeline for managing undersized datasets of available CT scans and ii) a baseline study for genomics mutation diagnosis support for preemptive patient follow-up. Our method is able to identify CRC RAS mutation family from CT images with 0.73 F1 score.
    
[^18]: TDFNet: 一种高效的音频视觉语音分离模型，具有自顶向下融合

    TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion. (arXiv:2401.14185v1 [cs.SD])

    [http://arxiv.org/abs/2401.14185](http://arxiv.org/abs/2401.14185)

    TDFNet是一种高效的音频视觉语音分离模型，基于TDANet架构，具有更少的参数，并在性能上相比现有方法提高了10%。

    

    音频视觉语音分离近年来受到广泛关注，由于其在语音识别、日程分析、场景分析和辅助技术等领域的潜在应用。设计一个轻量级的音频视觉语音分离网络对于低延迟应用非常重要，但是现有方法通常需要更高的计算成本和更多的参数才能实现更好的分离性能。在本文中，我们提出了一种名为TDFNet的音频视觉语音分离模型，该模型是一种基于TDANet架构的最新技术（SOTA）模型，TDANet是一种仅用于音频的语音分离方法。TDANet在TDFNet内部构建了听觉和视觉网络的架构基础，提供了一个具有较少参数的高效模型。在LRS2-2Mix数据集上，与前一最佳方法相比，TDFNet在所有性能指标上的表现提高了10%。

    Audio-visual speech separation has gained significant traction in recent years due to its potential applications in various fields such as speech recognition, diarization, scene analysis and assistive technologies. Designing a lightweight audio-visual speech separation network is important for low-latency applications, but existing methods often require higher computational costs and more parameters to achieve better separation performance. In this paper, we present an audio-visual speech separation model called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for audio-visual speech separation, which builds upon the architecture of TDANet, an audio-only speech separation method. TDANet serves as the architectural foundation for the auditory and visual networks within TDFNet, offering an efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet achieves a performance increase of up to 10\% across all performance metrics compared with the previous SOTA method 
    
[^19]: 强调自主供应链：定义、特征、概念框架和自主水平

    Towards Autonomous Supply Chains: Definition, Characteristics, Conceptual Framework, and Autonomy Levels. (arXiv:2401.14183v1 [cs.AI])

    [http://arxiv.org/abs/2401.14183](http://arxiv.org/abs/2401.14183)

    本论文提出了自主供应链（ASC）的正式定义、特征和辅助概念，提出了一个分层概念框架和供应链自治参考模型。通过案例研究展示了基于这些概念的初始ASC实施。

    

    最近的全球性干扰，如大流行和地缘政治冲突，深刻暴露了传统供应链的脆弱性，需要探索更有弹性的替代方案。自主供应链（ASC）作为潜在解决方案出现，提供了在动荡的贸易环境中增加可见性、灵活性和弹性的优势。尽管工业界和学术界多年来一直在讨论ASC，但缺乏良好建立的理论基础。本文通过提供ASC的正式定义以及其定义特征和辅助概念来填补这一研究空白。我们提出了一个名为MIISI模型的分层概念框架。通过一个重点关注肉类供应链的案例研究，展示了基于这一概念模型的初始ASC实施。此外，我们还引入了一个七级供应链自治参考模型，描绘了实现完全供应链自治的轨迹。

    Recent global disruptions, such as the pandemic and geopolitical conflicts, have profoundly exposed vulnerabilities in traditional supply chains, requiring exploration of more resilient alternatives. Autonomous supply chains (ASCs) have emerged as a potential solution, offering increased visibility, flexibility, and resilience in turbulent trade environments. Despite discussions in industry and academia over several years, ASCs lack well-established theoretical foundations. This paper addresses this research gap by presenting a formal definition of ASC along with its defining characteristics and auxiliary concepts. We propose a layered conceptual framework called the MIISI model. An illustrative case study focusing on the meat supply chain demonstrates an initial ASC implementation based on this conceptual model. Additionally, we introduce a seven-level supply chain autonomy reference model, delineating a trajectory towards achieving a full supply chain autonomy. Recognising that this 
    
[^20]: Copilot细化：解决Copilot生成的Python代码中的代码异味

    Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code. (arXiv:2401.14176v1 [cs.SE])

    [http://arxiv.org/abs/2401.14176](http://arxiv.org/abs/2401.14176)

    本研究旨在探索Copilot生成的Python代码中的代码异味，评估Copilot修复这些问题的能力。结果表明，有8种Python代码异味可以在Copilot生成的代码中检测到。

    

    作为最流行的动态语言之一，Python在存在代码异味时可读性和可维护性会下降。大型语言模型的最新进展引发了对AI支持的代码生成和重构工具的日益关注。GitHub Copilot是其中一种被广泛使用的工具。Copilot Chat是在2023年9月发布的一种交互式工具，旨在为自然语言驱动的编码提供便利。然而，对于理解Copilot生成的Python代码中的代码异味以及Copilot修复其生成的代码异味的能力，人们并没有给予足够的关注。为此，我们构建了一个包含102个Copilot生成的Python代码中的代码异味的数据集。我们的目标是首先探索Copilot生成的Python代码中代码异味的发生情况，然后评估Copilot在使用不同提示修复这些代码异味时的有效性。结果显示，10种Python代码异味中有8种可以在Copilot生成的代码中检测到。

    As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot's ability to fix the code smells it generates. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts. The results show that 8 out of 10 types of Python smells can be detected in 
    
[^21]: 划分在层次化任务网络规划中的可计算边界

    The Boundaries of Tractability in Hierarchical Task Network Planning. (arXiv:2401.14174v1 [cs.CC])

    [http://arxiv.org/abs/2401.14174](http://arxiv.org/abs/2401.14174)

    本论文研究了在层次化任务网络规划中三个经典问题的可计算边界，提供了在特定条件下将多项式时间可解性结果从原始任务网络推广到一般任务网络的方法，并且给出了这三个问题的参数化复杂度分析。

    

    我们研究了在层次化任务网络规划中三个经典问题的可计算边界：提供计划的验证，是否存在可执行计划以及是否可以通过某个计划达到给定状态。我们证明了在常量偏序宽度的原始任务网络（以及其推广形式）上，这三个问题都可以在多项式时间内解决，而对于后两个问题，这种情况仅在对状态空间进行可证明的必要限制下才成立。接下来，我们提供了一个算法元定理及相应的下界，以确定从原始任务网络到一般任务网络的一般多项式时间可解性结果可以被提升的严格条件。最后，我们通过分析这三个问题的参数化复杂度来丰富我们的研究，并且证明了(1)通过将偏序宽度替换为t，可以实现这三个问题的固定参数可计算性。

    We study the complexity-theoretic boundaries of tractability for three classical problems in the context of Hierarchical Task Network Planning: the validation of a provided plan, whether an executable plan exists, and whether a given state can be reached by some plan. We show that all three problems can be solved in polynomial time on primitive task networks of constant partial order width (and a generalization thereof), whereas for the latter two problems this holds only under a provably necessary restriction to the state space. Next, we obtain an algorithmic meta-theorem along with corresponding lower bounds to identify tight conditions under which general polynomial-time solvability results can be lifted from primitive to general task networks. Finally, we enrich our investigation by analyzing the parameterized complexity of the three considered problems, and show that (1) fixed-parameter tractability for all three problems can be achieved by replacing the partial order width with t
    
[^22]: 从多参数MRI预测脑肿瘤缺氧

    Predicting Hypoxia in Brain Tumors from Multiparametric MRI. (arXiv:2401.14171v1 [eess.IV])

    [http://arxiv.org/abs/2401.14171](http://arxiv.org/abs/2401.14171)

    本研究利用多参数磁共振成像(MRI)预测脑肿瘤的缺氧情况，提出了一种替代昂贵且普及率有限的FMISO PET方法。研究结果表明，深度学习模型可以有效地从MRI扫描中预测缺氧情况，并与实际值有很强的相关性。

    

    本研究论文提出了一种利用多参数磁共振成像(MRI)预测脑肿瘤缺氧的新方法。缺氧是一种低氧水平的情况，是与预后不良相关的恶性脑肿瘤的常见特征。氟米索氟核素正电子发射断层扫描 (FMISO PET) 是一种用于体内检测缺氧的成熟方法，但其昂贵且普及率有限。本研究提出了使用MRI这种更容易获取和经济实惠的成像模式来预测FMISO PET信号。我们研究了在ACRIN 6684数据集上训练的深度学习模型(DL)，该数据集包含脑肿瘤患者的配对MRI和FMISO PET图像。我们训练的模型能够有效地学习MRI特征与相应的FMISO PET信号之间的复杂关系，从而可以仅通过MRI扫描预测缺氧。结果显示预测值与实际值之间有很强的相关性。

    This research paper presents a novel approach to the prediction of hypoxia in brain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia, a condition characterized by low oxygen levels, is a common feature of malignant brain tumors associated with poor prognosis. Fluoromisonidazole Positron Emission Tomography (FMISO PET) is a well-established method for detecting hypoxia in vivo, but it is expensive and not widely available. Our study proposes the use of MRI, a more accessible and cost-effective imaging modality, to predict FMISO PET signals. We investigate deep learning models (DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and FMISO PET images from patients with brain tumors. Our trained models effectively learn the complex relationships between the MRI features and the corresponding FMISO PET signals, thereby enabling the prediction of hypoxia from MRI scans alone. The results show a strong correlation between the predicted and actual
    
[^23]: BayesPrompt: 通过无偏领域抽象在少样本推理上指导大规模预训练语言模型

    BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])

    [http://arxiv.org/abs/2401.14166](http://arxiv.org/abs/2401.14166)

    BayesPrompt通过无偏领域抽象解决大规模预训练语言模型在少样本推理中的泛化问题。

    

    作为一种基于大规模预训练语言模型（PLMs）的新颖有效的微调范式，prompt-tuning旨在缩小下游任务与预训练目标之间的差距。虽然prompt-tuning在各种任务中取得了持续进展，但这种方法仍然存在一个持久的缺陷：prompt-tuning方法无法泛化到特定的少样本模式。从分布分析的角度来看，我们揭示了这一现象背后的内在问题是PLMs中包含过多的概念知识和目标下游领域的缩减知识，两者共同导致PLMs在普遍的知识嵌入空间中错误地定位与目标领域相对应的知识分布。为此，我们直观地探索了以无偏方式逼近下游任务的完整目标领域，并通过抽象这样的领域生成有区别的提示，从而提供了无歧义的信息。

    As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous
    
[^24]: 缓解图形异常检测中的结构分布偏移问题

    Alleviating Structural Distribution Shift in Graph Anomaly Detection. (arXiv:2401.14155v1 [cs.LG])

    [http://arxiv.org/abs/2401.14155](http://arxiv.org/abs/2401.14155)

    这项研究从特征视角解决了图形异常检测中的结构分布偏移问题，通过抵抗异常节点的高异质性，同时从同质邻居中受益于正常节点的学习。

    

    图形异常检测是一个具有挑战性的二分类问题，由于异常节点和正常节点之间的结构分布不同，使得该问题变得困难。异常节点是少数，因此与正常节点相比，具有较高的异质性和较低的同质性。此外，由于各种时间因素和人类专家的注释偏好，异质性和同质性在训练和测试数据之间可能会发生变化，这在本文中被称为结构分布偏移（SDS）。主流方法是基于图神经网络（GNN）构建的，通过聚合同质邻居有利于正常节点的分类，但忽视了异常节点的SDS问题，导致泛化能力差。本研究从特征视角解决了这个问题。我们观察到SDS的程度在异常节点和正常节点之间有所不同。因此，解决这个问题的关键在于对异常节点抵抗高异质性的同时，从同质邻居中受益于正常节点的学习。

    Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization.  This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophi
    
[^25]: 用NetLogo开发基于代理的模拟评估AmI场景

    Agent-based Simulation with Netlogo to Evaluate AmI Scenarios. (arXiv:2401.14153v1 [cs.AI])

    [http://arxiv.org/abs/2401.14153](http://arxiv.org/abs/2401.14153)

    本文开发了一个基于代理的模拟环境来评估AmI场景，通过测量代理满意度和节约时间来评估效益。

    

    本文开发了一个基于代理的模拟来评估基于代理的AmI场景。许多AmI应用程序都是通过代理来实现的，但没有将它们与其他现有的替代方法进行比较，以评估使用它们相对于其他方法的效益。提出的模拟环境使用两个评估指标来分析这些效益：首先，通过测量不同类型愿望的代理满意度来衡量。其次，通过正确使用上下文信息所获得的时间节约来衡量。因此，本文使用NetLogo模拟环境评估了之前提出的在机场提供AmI服务的代理架构、本体论和12步协议。本文使用NetLogo模型考虑了该应用领域的可扩展性问题，同时使用FIPA和BDI扩展与我们先前的工作和我们先前的JADE实现保持一致。

    In this paper an agent-based simulation is developed in order to evaluate an AmI scenario based on agents. Many AmI applications are implemented through agents but they are not compared to any other existing alternative in order to evaluate the relative benefits of using them. The proposal simulation environment developed in Netlogo analyse such benefits using two evaluation criteria: First, measuring agent satisfaction of different types of desires along the execution. Second, measuring time savings obtained through a correct use of context information.  So, here, a previously suggested agent architecture, an ontology and a 12-steps protocol to provide AmI services in airports, is evaluated using a NetLogo simulation environment. The present work uses a NetLogo model considering scalability problems of this application domain but using FIPA and BDI extensions to be coherent with our previous works and our previous JADE implementation of them.  The NetLogo model presented simulates an 
    
[^26]: 真知来源于实践：通过强化学习使LLMs与具身环境对齐的方法研究

    True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])

    [http://arxiv.org/abs/2401.14151](http://arxiv.org/abs/2401.14151)

    本研究通过使用大型语言模型（LLMs）作为决策智能体，通过强化学习与具身环境高效互动来解决LLMs与环境之间知识不对齐的问题。通过查询LLMs的联合概率，形成行为策略，并通过两种归一化方法和四个提示设计原则提高策略的稳定性和鲁棒性。最后，通过设计参数高效的训练架构提高学习效率。

    

    尽管在众多任务中取得了令人印象深刻的表现，但大型语言模型（LLMs）在解决简单的决策任务上经常失败，原因是LLMs中的知识与环境不对齐。相反，强化学习（RL）智能体从零开始学习策略，这使得它们始终与环境保持一致，但难以将先前的知识整合到其中以进行有效的探索。为了缩小这一差距，我们提出了TWOSOME，一种新颖的在线框架，利用LLMs作为决策智能体，通过RL与具身环境高效互动并实现对齐，而无需任何准备好的数据集或环境的先前知识。首先，我们使用LLMs查询每个有效动作的联合概率以形成行为策略。然后，为了增强策略的稳定性和鲁棒性，我们提出了两种归一化方法，并总结了四个提示设计原则。最后，我们设计了一种新颖的参数高效的训练架构，其中包括一个行为评估和选择算法来提高学习效率。

    Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
    
[^27]: 基于能量的概念瓶颈模型：统一预测、概念干预和条件解释

    Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])

    [http://arxiv.org/abs/2401.14142](http://arxiv.org/abs/2401.14142)

    基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。

    

    现有方法，如概念瓶颈模型 (CBM)，在为黑盒深度学习模型提供基于概念的解释方面取得了成功。它们通常通过在给定输入的情况下预测概念，然后在给定预测的概念的情况下预测最终的类别标签。然而，它们经常无法捕捉到概念之间的高阶非线性相互作用，例如纠正一个预测的概念（例如“黄色胸部”）无法帮助纠正高度相关的概念（例如“黄色腹部”），导致最终准确率不理想；它们无法自然地量化不同概念和类别标签之间的复杂条件依赖关系（例如对于一个带有类别标签“Kentucky Warbler”和概念“黑色嘴巴”的图像，模型能够正确预测另一个概念“黑色冠”的概率是多少），因此无法提供关于黑盒模型工作原理更深层次的洞察。针对这些限制，我们提出了基于能量的概念瓶颈模型（Energy-based Concept Bottleneck Models）。

    Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
    
[^28]: FP6-LLM: 通过FP6中心算法-系统协同设计高效提供大型语言模型

    FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])

    [http://arxiv.org/abs/2401.14112](http://arxiv.org/abs/2401.14112)

    FP6-LLM提出了一种支持六位量化的GPU算法-系统协同设计方案，实现了在大型语言模型中推断成本和模型质量之间的平衡。

    

    六位量化（FP6）可以有效地减小大型语言模型（LLM）的大小，并在不同应用中保持模型质量的一致性。然而，现有系统不提供FP6量化的张量核心支持，并且在LLM推断过程中很难实现实际性能改进。由于（1）模型权重具有不规则位宽的不友好内存访问和（2）权重去量化的高运行时开销，支持在GPU上进行FP6量化是具有挑战性的。为了解决这些问题，我们提出了TC-FPx，这是第一个具有统一张量核心支持的浮点权重的完整GPU内核设计方案，适用于各种量化位宽。我们将TC-FPx内核集成到现有推断系统中，提供了新的端到端支持（称为FP6-LLM）用于量化LLM推断，从而实现了推断成本和模型质量之间更好的平衡。实验证明，FP6-LLM仅使用一部分存储空间就可以进行LLaMA-70b的推断。

    Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin
    
[^29]: 以较低比特宽度的累加器降低深度网络推理成本

    Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators. (arXiv:2401.14110v1 [cs.LG])

    [http://arxiv.org/abs/2401.14110](http://arxiv.org/abs/2401.14110)

    本论文提出了一种简单的方法，可以训练和微调高端DNN，实现使用更便宜的12位累加器，而不会导致精度降低，并展示了使用细粒度梯度近似可以提高DNN的精度。

    

    目前大多数关于深度神经网络（DNN）量化的研究都着重于降低高级框架可见的张量精度（例如权重、激活和梯度）。然而，当前的硬件仍然依赖于高精度的核心操作，最重要的是累加乘积的运算。这种高精度累加运算逐渐成为主要的计算瓶颈。这是因为到目前为止，低精度累加器的使用导致了性能的显著降低。在这项工作中，我们提出了一种简单的方法来训练和微调高端DNN，首次实现使用更便宜的12位累加器，而不会导致显著的精度降低。最后，我们还展示出随着累加精度进一步降低，使用细粒度梯度近似可以提高DNN的精度。

    The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.
    
[^30]: CompactifAI: 使用量子启发的张量网络对大型语言模型进行极压缩

    CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])

    [http://arxiv.org/abs/2401.14109](http://arxiv.org/abs/2401.14109)

    CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。

    

    大型语言模型（LLM）如ChatGPT和LlaMA在生成人工智能（AI）方面取得了快速进展，但其庞大的规模带来了重要挑战，如巨大的训练和推断成本、较大的能源需求以及现场部署的限制。传统的压缩方法如剪枝、蒸馏和低秩逼近主要关注减少网络中神经元的有效数量，而量化方法则侧重于降低单个权重的数值精度，以减小模型大小同时保持神经元数目不变。虽然这些压缩方法在实践中取得了相对成功，但没有令人信服的理由认为截断神经元的数量是一种最优策略。本文介绍了一种创新的LLM压缩方法CompactifAI，它使用量子启发的张量网络，而不是传统的压缩方法，更注重模型的相关空间，实现更加可控和精细的压缩。

    Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
    
[^31]: GQHAN: 一种受到Grover启发的量子硬注意力网络

    GQHAN: A Grover-inspired Quantum Hard Attention Network. (arXiv:2401.14089v1 [quant-ph])

    [http://arxiv.org/abs/2401.14089](http://arxiv.org/abs/2401.14089)

    GQHAN是一种受到Grover启发的量子硬注意力网络，通过柔性预言机和自适应扩散算子来解决了硬注意力机制的不可微分性，提高了处理量子数据集的效果。

    

    许多当前的量子机器学习(QML)模型在识别量子数据的重要性方面存在不足，导致处理大规模量子数据集时效果下降。硬注意力机制(HAM)被期望能够有效解决上述QML瓶颈，但面临无法微分的严重挑战，因此限制了其广泛应用性。针对HAM和QML的困境，提出了一种受到Grover启发的量子硬注意力机制(GQHAM)，由柔性预言机(Flexible Oracle, FO)和自适应扩散算子(Adaptive Diffusion Operator, ADO)组成。值得注意的是，FO通过执行具有柔性控制的离散原语(Discrete Primitives, DPs)的激活或屏蔽来克服不可微的问题，从而编织各种离散的命运。在此基础上，这种离散选择可以通过特殊定义的量子硬注意力分数(QHAS)进行可视化。此外，还设计了一个可训练的ADO来增强通用性和灵活性。

    Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy in discerning the significance of quantum data, resulting in diminished efficacy when handling extensive quantum datasets. Hard Attention Mechanism (HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters the substantial challenge of non-differentiability, consequently constraining its extensive applicability. In response to the dilemma of HAM and QML, a Grover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a Flexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed. Notably, the FO is designed to surmount the non-differentiable issue by executing the activation or masking of Discrete Primitives (DPs) with Flexible Control (FC) to weave various discrete destinies. Based on this, such discrete choice can be visualized with a specially defined Quantum Hard Attention Score (QHAS). Furthermore, a trainable ADO is devised to boost the generality and flexibility
    
[^32]: 使用Sum-Product Networks生成可能的反事实推理

    Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])

    [http://arxiv.org/abs/2401.14086](http://arxiv.org/abs/2401.14086)

    由于用户需求和最近的法规要求，需要对AI系统所做出的决策进行解释。本论文提出了一种使用Sum-Product Networks模拟寻找高可能性反事实推理的系统，该系统能够提供满足多个常见要求的最佳解释。

    

    由于用户需求和最近的法规（GDPR、AI法案），需要解释AI系统所做出的决策。这些决策往往只能在事后解释，反事实推理成为常见的解释方式。什么构成了最佳的反事实解释必须考虑多个方面，其中“样本距离”是最常见的。我们认为，这一要求经常会导致不太可能且因此价值有限的解释。在这里，我们提出了一个能够提供高可能性解释的系统。我们展示了使用混合整数优化（MIO）模拟寻找满足反事实推理的许多常见要求的最有可能解释。在此过程中，我们提出了Sum-Product Network（SPN）的MIO表达，并使用SPN估计反事实的可能性，这对独立的兴趣也有用。与生成反事实解释的几种方法进行数值比较。

    Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
    
[^33]: 从需求到架构：基于人工智能的半自动生成软件架构的旅程

    From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures. (arXiv:2401.14079v1 [cs.SE])

    [http://arxiv.org/abs/2401.14079](http://arxiv.org/abs/2401.14079)

    本文提出了一种基于人工智能技术的半自动软件架构生成方法，可以根据需求生成架构候选方案，并使用大型语言模型和定量方法进行自动评估和交叉分析。

    

    在软件开发中，设计领域模型和软件架构是一个重要的挑战，因为得到的架构对系统的服务质量起着关键作用。由于时间压力，架构师通常只基于自己对领域的有限了解、模式和经验来建模一个架构，而不是对领域进行彻底分析和评估多个候选方案，选择最合适的方案。现有方法尝试根据需求来生成领域模型，但仍需要耗费大量时间手动努力才能获得良好的结果。因此，在这篇展望性论文中，我们提出了一种基于人工智能技术，半自动地根据需求生成软件架构候选方案的方法。我们进一步设想使用架构权衡分析方法与大型语言模型和定量方法自动评估和进行交叉分析生成的架构候选方案。

    Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative 
    
[^34]: Ta'keed: 首个用于阿拉伯语论断的生成式事实核查系统

    Ta'keed: The First Generative Fact-Checking System for Arabic Claims. (arXiv:2401.14067v1 [cs.CL])

    [http://arxiv.org/abs/2401.14067](http://arxiv.org/abs/2401.14067)

    Ta'keed是首个用于阿拉伯语论断的生成式事实核查系统，通过基于检索片段的论断真实性评估和LLM-based论断验证，解决了目前阿拉伯语领域缺乏生成解释论断可信度的研究。引入了一个测试黄金标签数据集，并分析了系统生成解释与黄金标准解释之间的语义相似性。研究还探讨了不同片段数量对论断分类准确性的影响。

    

    本文介绍了Ta'keed，一个可解释的阿拉伯语自动事实核查系统。现有研究通常将论断分类为“真”或“假”，但对于生成论断可信度的解释尤其在阿拉伯语领域的研究有限。Ta'keed通过基于检索片段的论断真实性评估来填补这一空白，利用信息检索和基于LLM的论断验证两个主要组件。我们编制了ArFactEx，一个带有手工证明参考的测试黄金标签数据集，用于评估该系统。初始模型在分类任务中取得了有希望的0.72的F1得分。与黄金标准解释在句法和语义上进行比较，系统生成的解释得到了推荐的语义相似性评估，平均余弦相似度分数为0.76。此外，我们还探讨了不同片段数量对论断分类准确性的影响，揭示....

    This paper introduces Ta'keed, an explainable Arabic automatic fact-checking system. While existing research often focuses on classifying claims as "True" or "False," there is a limited exploration of generating explanations for claim credibility, particularly in Arabic. Ta'keed addresses this gap by assessing claim truthfulness based on retrieved snippets, utilizing two main components: information retrieval and LLM-based claim verification. We compiled the ArFactEx, a testing gold-labelled dataset with manually justified references, to evaluate the system. The initial model achieved a promising F1 score of 0.72 in the classification task. Meanwhile, the system's generated explanations are compared with gold-standard explanations syntactically and semantically. The study recommends evaluating using semantic similarities, resulting in an average cosine similarity score of 0.76. Additionally, we explored the impact of varying snippet quantities on claim classification accuracy, revealin
    
[^35]: CreativeSynth：基于多模态扩散的视觉艺术创意融合与合成

    CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion. (arXiv:2401.14066v1 [cs.CV])

    [http://arxiv.org/abs/2401.14066](http://arxiv.org/abs/2401.14066)

    CreativeSynth是一种基于多模态扩散的创新统一框架，通过整合多模态特征和定制的注意力机制，实现了将现实世界的语义内容导入到艺术领域中，能够协调多模态输入和多任务，在艺术图像生成方面具有重要意义。

    

    大规模的文本到图像生成模型取得了巨大的进步，展示了其合成各种高质量图像的能力。然而，将这些模型应用于艺术图像编辑面临两个重要挑战。首先，用户往往难以构建详细描述输入图像视觉元素的文本提示。其次，现有模型在特定区域进行修改时常常会破坏整体艺术风格，使得实现一致且具有审美统一的作品变得更加复杂。为了克服这些障碍，我们构建了一种创新的统一框架CreativeSynth，该框架基于具有协调多模态输入和多任务能力的扩散模型，通过整合多模态特征和定制的注意力机制，CreativeSynth实现了将现实世界的语义内容导入到艺术领域中，实现了反转和实时风格转移。

    Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. T
    
[^36]: 左/右脑、人类运动控制及对机器人的影响

    Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])

    [http://arxiv.org/abs/2401.14057](http://arxiv.org/abs/2401.14057)

    本研究通过训练不同的损失函数，实现了类似于人类的左右半球专门化控制系统，该系统在不同的运动任务中展现出协调性、运动效率和位置稳定性的优势。

    

    神经网络运动控制器相对传统控制方法具有各种优点，然而由于其无法产生可靠的精确运动，因此尚未得到广泛采用。本研究探讨了一种双侧神经网络架构作为运动任务的控制系统。我们旨在实现类似于人类在不同任务中观察到的半球专门化：优势系统（通常是右手、左半球）擅长协调和运动效率的任务，而非优势系统在需要位置稳定性的任务上表现更好。通过使用不同的损失函数对半球进行训练，实现了专门化。我们比较了具有专门化半球和无专门化半球、具有半球间连接（代表生物学脑桥）和无半球间连接、具有专门化和无专门化的单侧模型。

    Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. 
    
[^37]: 朝着目标导向的大型语言模型提示方法：一项调查

    Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])

    [http://arxiv.org/abs/2401.14043](http://arxiv.org/abs/2401.14043)

    本文调查了大型语言模型(LLM)中目标导向提示工程的重要性。通过对35个代表性研究的回顾，我们发现引导LLM遵循人类的逻辑思维的目标导向提示公式显著提高了LLM的性能。我们还提出了一个新的分类体系，并总结了十个适用任务来展示我们框架的广泛适用性。同时，我们提出了四个未来的方向，以推动目标导向提示工程的进一步发展。

    

    大型语言模型(LLM)在各种下游任务中显示出卓越的性能，而提示工程在优化LLM性能中起着关键作用。本文旨在强调设计提示的限制，同时保持人类追求LLM像人类思考的人类学假设。通过对35个代表性研究的回顾，我们展示了目标导向提示公式的重要性，该公式指导LLM遵循人类的逻辑思维，显著提高了LLM的性能。此外，我们引入了一个新的分类体系，将目标导向提示方法分为五个相互关联的阶段，并通过总结十个适用任务来展示我们框架的广泛适用性。最后，我们提出了四个未来的方向，希望进一步强调和推动目标导向提示工程。

    Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
    
[^38]: GauU-Scene: 使用高斯点云渲染的大规模三维重构数据集上的场景重建基准

    GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting. (arXiv:2401.14032v1 [cs.CV])

    [http://arxiv.org/abs/2401.14032](http://arxiv.org/abs/2401.14032)

    本论文提出了一种在大规模三维重建数据集上使用高斯点云渲染的场景重构基准，通过对U-Scene数据集进行详细评估，突出了综合多模态信息在场景重建中的重要性。

    

    我们介绍了一个新颖的大规模场景重建基准，使用了最新开发的三维表示方法高斯点云渲染，基于我们庞大的U-Scene数据集。U-Scene包含了超过1.5平方公里的区域，配备了全面的RGB数据和LiDAR地面真值。我们采用了Matrix 300无人机和高精度Zenmuse L1 LiDAR进行数据采集，实现了精确的屋顶数据收集。这个数据集提供了城市和学术环境的独特结合，用于高级空间分析，覆盖了超过1.5平方公里的区域。我们对使用高斯点云渲染的U-Scene进行了详细的评估，包括对各种新颖视角的分析。我们还将这些结果与准确的点云数据集进行了对比，突出了显著的差异，强调了综合多模态信息的重要性。

    We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information
    
[^39]: Unitxt：用于生成式人工智能的灵活、可共享和可复用数据准备与评估

    Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI. (arXiv:2401.14019v1 [cs.CL])

    [http://arxiv.org/abs/2401.14019](http://arxiv.org/abs/2401.14019)

    Unitxt是一个灵活、可共享和可复用的数据准备与评估库，针对生成式语言模型进行定制，实现了结构化、模块化和可定制的解决方案。该库集成了常用的库，将处理流程分解为模块化组件，促进了协作和共享。

    

    在生成式自然语言处理（NLP）的动态环境中，传统的文本处理流程限制了研究的灵活性和可重现性，因为它们针对特定的数据集、任务和模型组合进行了定制。随着系统提示、模型特定格式、指令等越来越复杂，需要转向一种结构化、模块化和可定制的解决方案。为满足这一需求，我们提出了Unitxt，这是一个创新的库，专为生成式语言模型定制的文本数据准备和评估而设计。Unitxt与HuggingFace和LM-eval-harness等常用库进行了本地集成，并将处理流程分解为模块化组件，实现了易于定制和共享的功能。这些组件包括模型特定格式、任务提示和许多其他全面的数据集处理定义。Unitxt-Catalog集中了这些组件，促进了现代文本数据流程中的协作和探索。

    In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution. Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt-Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond be
    
[^40]: CMMU: 一个用于中文多模态多类型问题理解与推理的基准测试

    CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])

    [http://arxiv.org/abs/2401.14011](http://arxiv.org/abs/2401.14011)

    CMMU是一个用于中文多模态多类型问题理解和推理的基准测试，涵盖了从小学到高中的知识，提供了多项选择题、多项回答题和填空题三种类型的问题，对于评估多模态大型语言模型的智能水平具有重要意义。

    

    多模态大型语言模型（MLLMs）已经取得了显著的进展，并展现出强大的知识理解和推理能力。然而，评估MLLM的智能水平所需的领域特定知识掌握仍然是一个挑战。当前用于领域特定知识的多模态基准测试主要集中在英语多项选择题上，并且在评估的全面性方面存在局限性。为此，我们引入了CMMU，一个用于中文多模态多类型问题理解和推理的新型基准测试。CMMU包含7个学科的3603个问题，涵盖了从小学到高中的知识。这些问题可以分为多项选择题、多项回答题和填空题三类，对MLLMs提出更大的挑战。此外，我们提出了一种严格的评估策略，称为ShiftCheck，用于评估多项选择题。

    Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strat
    
[^41]: ConstraintChecker：用于大型语言模型推理常识知识库的插件

    ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases. (arXiv:2401.14003v1 [cs.CL])

    [http://arxiv.org/abs/2401.14003](http://arxiv.org/abs/2401.14003)

    ConstraintChecker是一个针对大型语言模型的插件，用于推理常识知识库。它通过提示技术提供和检查显式约束，帮助解决了大型语言模型在处理常识知识库推理时的困难。

    

    基于常识知识库的推理（即常识知识库推理）已被探索作为一种通过原始常识知识库和外部先验知识来获取新的常识知识的方法。尽管大型语言模型（LLM）和提示工程技术在各种推理任务中取得了进展，但它们仍然难以处理常识知识库推理。其中一个问题是，它们很难只通过上下文示例从常识知识库中获取显式关系约束，这是因为缺乏符号推理能力（Bengio等人，2021年）。为此，我们提出了一种名为ConstraintChecker的插件，它基于提示技术提供和检查显式约束。在考虑新的知识实例时，ConstraintChecker使用基于规则的模块生成约束列表，然后使用零样本学习模块检查该知识实例是否满足所有约束。然后，获取的约束检查结果被聚合...

    Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge. Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning. One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (Bengio et al., 2021). To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints. When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints. The acquired constraint-checking result is then aggrega
    
[^42]: 调查-整合-开发：一种用于任务间代理自进化的通用策略

    Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution. (arXiv:2401.13996v1 [cs.CL])

    [http://arxiv.org/abs/2401.13996](http://arxiv.org/abs/2401.13996)

    本文介绍了调查-整合-开发（ICE）策略，通过任务间的自进化来提高AI代理的适应性和灵活性。实验证明了ICE的有效性，可以显著减少API调用，同时与GPT-3.5结合使用可以在各种代理任务上达到与GPT-4相当的性能。

    

    本文介绍了一种名为调查-整合-开发（ICE）的新策略，通过任务间的自进化来提高AI代理的适应性和灵活性。与现有的注重任务内学习的方法不同，ICE促进了任务间知识的传递，实现了真正的自进化，类似于人类的经验学习。该策略动态地调查规划和执行轨迹，将其整合为简化的工作流和管道，并利用它们来改进任务执行。我们在XAgent框架上的实验证明了ICE的有效性，可以将API调用减少高达80％，并显著减少模型能力的需求。具体来说，当与GPT-3.5结合使用时，ICE的性能在各种代理任务上与原始GPT-4相匹配。我们认为这种自进化方法代表了代理设计的范式转变，为更强大的AI社区和生态系统做出了贡献，并向全面实现人工智能跨进了一步。

    This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model's capability. Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full 
    
[^43]: 通过自适应变压器网络进行跨领域的少样本学习

    Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])

    [http://arxiv.org/abs/2401.13987](http://arxiv.org/abs/2401.13987)

    本文提出了一种名为ADAPTER的自适应变压器网络，用于跨领域的少样本学习。借助双向交叉注意力和标签平滑方法，ADAPTER能够在存在领域转移的情况下学习可转移的特征，并避免监督崩溃问题。该方法在BSCD-FSL基准测试中表现优秀，超过了之前的方法。

    

    大多数少样本学习方法依赖于基任务和目标任务之间的相同领域的假设，限制了它们的实际应用。本文提出了自适应变压器网络（ADAPTER），这是一种简单但有效的解决方案，用于处理基任务和目标任务之间存在大领域转移的跨领域少样本学习。ADAPTER基于双向交叉注意力的思想构建，以学习两个领域之间的可转移特征。所提出的架构使用DINO进行训练，以产生多样的、较少偏见的特征，以避免监督崩溃问题。此外，还提出了标签平滑方法，通过在嵌入空间中同时考虑接近样本的预测标签，提高预测的一致性和可靠性。ADAPTER的性能在BSCD-FSL基准测试中进行了严格评估，它以显著的优势超越了之前的方法。

    Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.
    
[^44]: 通过解释一致性微调实现一致的自然语言解释

    Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])

    [http://arxiv.org/abs/2401.13986](http://arxiv.org/abs/2401.13986)

    本文提出了一种通过解释一致性微调方法，使得大型语言模型（LLMs）在相关示例上生成更一致的自然语言解释。实验证明，该方法在不同领域的问答数据集上相对提高了10.0%的解释一致性，并且能够泛化到其他数据集。

    

    大型语言模型（LLMs）通常能够生成令人信服、流畅的解释。然而，与人类不同，它们在不同输入上生成的解释常常不一致。例如，LLM在回答问题“麻雀能飞吗？”时可能生成解释“所有鸟都能飞”，但同时在回答与之相关的问题“企鹅能飞吗？”时回答“不行”。解释应该在相关示例中保持一致，以便让人类能够模拟LLM在多个示例上的决策过程。我们提出了解释一致性微调（EC-finetuning）方法，该方法通过适应LLM在相关示例上生成更一致的自然语言解释。EC-finetuning包括在经过精心构建的包含一致解释的合成数据上微调LLM。在各种不同领域的问答数据集上，EC-finetuning在四个微调数据集上相对提高了10.0%的解释一致性，并且在七个外部数据集上具有泛化性能。

    Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation "all birds can fly" when answering the question "Can sparrows fly?" but meanwhile answer "no" to the related question "Can penguins fly?". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out
    
[^45]: Leeroo Orchestrator: 通过模型集成提高LLMs的性能

    Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])

    [http://arxiv.org/abs/2401.13979](http://arxiv.org/abs/2401.13979)

    本研究提出了Leeroo编排器的架构，通过集成多个训练过的LLMs模型，实现了一个新的最先进模型。该编排器在性能上与Mixtral模型相当，并且成本只有其三分之二。当允许更高的成本时，Leeroo编排器的准确性超过了Mixtral模型，并且当集成GPT4时进一步提升。

    

    本文提出了一种架构，利用多个训练过的LLMs的集体知识，创建一个新的最先进模型。该框架的核心是一个基于LLM的编排器，能够选择最佳的底层LLM专家进行任务执行。受到强化学习中的自我对弈的启发，我们创建了一个查询生成、编排和评估的循环，为编排器生成训练数据。我们的评估主要针对MMLU基准，在Hugging Face上使用了具有7B、13B和34B参数的模型。结果显示我们的Leeroo编排器实现了与Mixtral模型相当的性能，但只产生了其成本的三分之二。此外，增加允许的成本超过了Mixtral的准确性，达到了75.9%的准确性。当将GPT4集成到底层模型池中时，进一步提升也得到了观察。

    In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
    
[^46]: 学习操纵艺术图像

    Learning to Manipulate Artistic Images. (arXiv:2401.13976v1 [cs.CV])

    [http://arxiv.org/abs/2401.13976](http://arxiv.org/abs/2401.13976)

    本文提出了一种利用无语义信息，通过自监督方式进行图像生成的任意风格图像操纵网络。该方法解决了艺术图像中准确语义获取的困难，并避免了跨域伪影和精确结构问题。实验证明了该方法在计算效率和高分辨率方面的优势。

    

    最近计算机视觉的进展显著降低了艺术创作的障碍。基于示例的图像转换方法因其灵活性和可控性而受到广泛关注。然而，这些方法对语义有所假设或需要语义信息作为输入，而在艺术图像中准确的语义信息并不容易获得。此外，由于训练数据先验和在空间域中的特征压缩，这些方法会产生跨域伪影和不精确的结构。本文提出了一种任意风格图像操纵网络（SIM-Net），利用无语义的信息作为引导，并通过自监督方式进行区域传输策略来生成图像。我们的方法在一定程度上平衡了计算效率和高分辨率。此外，我们的方法促进了零样式图像操作。定性和定量实验证明了我们方法的优越性。

    Recent advancement in computer vision has significantly lowered the barriers to artistic creation. Exemplar-based image translation methods have attracted much attention due to flexibility and controllability. However, these methods hold assumptions regarding semantics or require semantic information as the input, while accurate semantics is not easy to obtain in artistic images. Besides, these methods suffer from cross-domain artifacts due to training data prior and generate imprecise structure due to feature compression in the spatial domain. In this paper, we propose an arbitrary Style Image Manipulation Network (SIM-Net), which leverages semantic-free information as guidance and a region transportation strategy in a self-supervised manner for image generation. Our method balances computational efficiency and high resolution to a certain extent. Moreover, our method facilitates zero-shot style image manipulation. Both qualitative and quantitative experiments demonstrate the superior
    
[^47]: BootPIG: 在预训练扩散模型中引导零样本个性化图像生成能力的引导模型

    BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models. (arXiv:2401.13974v1 [cs.CV])

    [http://arxiv.org/abs/2401.13974](http://arxiv.org/abs/2401.13974)

    提出了BootPIG架构，在预训练扩散模型中引导个性化图像生成能力。该架构使用用户提供的参考图像来指导生成图像中概念的外观。通过利用预训练的模型生成的数据进行训练，实现了对BootPIG架构的个性化能力的引导。

    

    最近的文本到图像生成模型在生成遵循输入提示的图像方面取得了令人难以置信的成功。然而，使用词语来描述所需的概念限制了对生成图像外观的控制能力。在本文中，我们提出了一种方法，通过在现有的文本到图像扩散模型中引入个性化能力来解决这个问题。我们提出了一种新的架构（BootPIG），允许用户提供目标对象的参考图像，以指导生成图像中概念的外观。提出的BootPIG架构对预训练的文本到图像扩散模型进行最小的修改，并利用一个单独的UNet模型来引导生成图像的外观。我们介绍了一种训练过程，能够使用从预训练的文本到图像模型、LLM聊天代理和i（候选图像）生成的数据来引导BootPIG架构的个性化能力。

    Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.  The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and i
    
[^48]: 动态长期时间序列预测：基于元转换网络的研究

    Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])

    [http://arxiv.org/abs/2401.13968](http://arxiv.org/abs/2401.13968)

    本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于动态长期时间序列预测。该模型通过快速和慢速学习器的结合，以及使用通用表示转换层，实现对动态环境的快速适应，并在实验中表现出优于基准算法的性能提升。

    

    在实践中，可靠的长期时间序列预测模型面临诸多挑战，如低计算和内存占用以及对动态学习环境的鲁棒性。本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于处理动态的长期时间序列预测任务。MANTRA依赖于快速和慢速学习器的概念，其中一组快速学习器学习数据分布的不同方面，同时快速适应变化。慢速学习器为快速学习器定制适当的表示。通过使用通用表示转换层产生任务适应性表示，并具有少量参数，实现对动态环境的快速适应。我们在四个具有不同预测长度的数据集上进行实验，结果表明我们的方法在多变量和单变量设置下至少比基准算法改进了3％。MANTRA的源代码可在链接中找到。

    A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANT
    
[^49]: 社会问题的通用自动生成解决方案

    General Automatic Solution Generation of Social Problems. (arXiv:2401.13945v1 [cs.CY])

    [http://arxiv.org/abs/2401.13945](http://arxiv.org/abs/2401.13945)

    本研究提出了一个基于代理模型的自动社会操作系统（ASOS），用于通用社会解决方案的生成，通过超图和符号化混合策略实现了全局和局部的问题分析和规定。

    

    鉴于当代社会系统日益复杂和多层次的特性，手动生成解决相关社会问题的解决方案已成为一项艰巨的任务。为了应对这一挑战，人工智能的快速发展推动了计算方法的探索，旨在自动生成解决方案。然而，目前的自动生成解决方案的方法主要集中在涉及特定场景的局部社会规定。在这里，我们报告了一个专为通用社会解决方案生成而设计的自动社会操作系统（ASOS），该系统建立在基于代理模型的基础上，能够在时空维度上对社会问题进行全局和局部的分析与规定。ASOS采用可扩展的社会语义的超图，以全面和结构化的方式表示社会动态。它还结合了用于标准化超图操作的通用协议和符号化的混合策略。

    Given the escalating intricacy and multifaceted nature of contemporary social systems, manually generating solutions to address pertinent social issues has become a formidable task. In response to this challenge, the rapid development of artificial intelligence has spurred the exploration of computational methodologies aimed at automatically generating solutions. However, current methods for auto-generation of solutions mainly concentrate on local social regulations that pertain to specific scenarios. Here, we report an automatic social operating system (ASOS) designed for general social solution generation, which is built upon agent-based models, enabling both global and local analyses and regulations of social problems across spatial and temporal dimensions. ASOS adopts a hypergraph with extensible social semantics for a comprehensive and structured representation of social dynamics. It also incorporates a generalized protocol for standardized hypergraph operations and a symbolic hyb
    
[^50]: 公平性和救济中反事实推理的新范式

    A New Paradigm for Counterfactual Reasoning in Fairness and Recourse. (arXiv:2401.13935v1 [cs.AI])

    [http://arxiv.org/abs/2401.13935](http://arxiv.org/abs/2401.13935)

    本研究探索了一种新的反事实推理范式，基于回溯反事实，通过回溯已有路径，而不是想象在法律保护特征上进行的可干预的假设干预。

    

    反事实和反事实推理是审计和理解人工智能系统的许多技术的基础。在这一领域中，反事实推理的传统范式是干预反事实，即想象和模拟假设性干预。因此，关于人工智能中的法律保护和人口统计数据的因果推理的出发点是对法律保护特征（如种族、性别、残疾、年龄等）进行假设干预。我们问的问题是，如果你的种族不同，会发生什么情况？这个范式的一个固有限制是，一些人口统计干预（比如种族干预）可能无法转化为干预反事实的形式主义。在这项研究中，我们探索了一种基于回溯反事实的新范式，与想象在法律保护特征上进行的假设干预不同，我们想象可以沿着已有的路径进行回溯的反事实。

    Counterfactuals and counterfactual reasoning underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions -- like interventions on race -- may not translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine al
    
[^51]: LocMoE: 一种用于大型语言模型训练的低开销MoE

    LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])

    [http://arxiv.org/abs/2401.13920](http://arxiv.org/abs/2401.13920)

    LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。

    

    混合专家模型（MoE）是一种广泛采用的分布式和集成学习方法，用于大型语言模型（LLM），由于其能够有效稀疏和扩展模型，因此备受青睐。然而，MoE的性能受到负载不平衡和全对全通信的高延迟的限制，同时由于大量的专家容量导致相对冗余的计算。负载不平衡可能是由于现有路由策略始终倾向于选择特定的专家导致的。全对全过程中频繁的节点间通信也显著延长了训练时间。为了缓解上述性能问题，我们提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性。值得注意的是，我们阐明了专家容量的最小阈值，通过将专家的门控权重与分配的标记之间的最大角偏差计算出来。

    The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
    
[^52]: WebVoyager：使用大型多模态模型构建端到端的Web Agent

    WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])

    [http://arxiv.org/abs/2401.13919](http://arxiv.org/abs/2401.13919)

    WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。

    

    大型语言模型（LLMs）的进步引领了一个由真实世界中自主应用程序的发展所标志的新时代，推动了基于网络的高级代理的创新。现有的网络代理通常只处理一个输入模态，并且仅在简化的网络模拟器或静态的网络快照中进行评估，极大地限制了它们在真实场景中的适用性。为了填补这一差距，我们引入了WebVoyager，一种创新的基于大型多模态模型（LMM）的Web代理，通过与真实网站进行交互，能够端到端地完成用户指令。此外，我们提出了一个新的Web代理评估协议，以解决开放式Web代理任务的自动评估挑战，利用了GPT-4V的强大多模态理解能力。我们通过收集来自15个广泛使用的网站的真实世界任务来创建一个新的基准来评估我们的代理。我们展示了WebVoyager实现了55.7％的任务成功率，显著地.....

    The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
    
[^53]: 离散分布的谱聚类

    Spectral Clustering for Discrete Distributions. (arXiv:2401.13913v1 [cs.LG])

    [http://arxiv.org/abs/2401.13913](http://arxiv.org/abs/2401.13913)

    本文提出了一种基于谱聚类和分布相似度度量的框架来解决离散分布聚类问题。通过使用线性最优传输构建相似度矩阵，我们在聚类准确性和计算效率方面取得了显著的改进。

    

    离散分布聚类（D2C）通常通过Wasserstein质心方法来解决。这些方法在一个共同的假设下工作，即聚类可以通过质心很好地表示，但在许多实际应用中可能不成立。在本文中，我们提出了一个简单而有效的基于谱聚类和分布相似度度量（例如最大均值差异和Wasserstein距离）的框架来解决D2C问题。为了提高可扩展性，我们提出使用线性最优传输在大型数据集上高效地构建相似度矩阵。我们提供了理论保证，保证了所提方法在聚类分布方面的成功。在合成数据和真实数据上的实验证明，我们的方法在聚类准确性和计算效率方面大大优于基准方法。

    Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.
    
[^54]: 赋予机器像化学家一样思考的能力：用层次符号回归揭示分子结构极性关系

    Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression. (arXiv:2401.13904v1 [cs.LG])

    [http://arxiv.org/abs/2401.13904](http://arxiv.org/abs/2401.13904)

    本论文介绍了一种无监督的层次符号回归方法(UHiSR)，通过结合层次神经网络和符号回归，实现了自动提取化学直观的极性指数，并发现了将分子结构与色谱行为联系起来的可解释方程。

    

    薄层色谱法(TLC)是分子极性分析中至关重要的技术。尽管其重要性，对于TLC的预测模型，尤其是那些由人工智能驱动的模型的可解释性仍然面临挑战。当前的方法要么利用高维分子指纹，要么利用基于领域知识的特征工程，经常面临表达能力和可解释性之间的两难选择。为了弥合这一差距，我们介绍了无监督的层次符号回归(UHiSR)方法，结合了层次神经网络和符号回归。UHiSR自动提取化学直观的极性指数，并发现将分子结构与色谱行为联系起来的可解释方程。

    Thin-layer chromatography (TLC) is a crucial technique in molecular polarity analysis. Despite its importance, the interpretability of predictive models for TLC, especially those driven by artificial intelligence, remains a challenge. Current approaches, utilizing either high-dimensional molecular fingerprints or domain-knowledge-driven feature engineering, often face a dilemma between expressiveness and interpretability. To bridge this gap, we introduce Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical neural networks and symbolic regression. UHiSR automatically distills chemical-intuitive polarity indices, and discovers interpretable equations that link molecular structure to chromatographic behavior.
    
[^55]: 领域无关的动态规划方法

    Domain-Independent Dynamic Programming. (arXiv:2401.13883v1 [cs.AI])

    [http://arxiv.org/abs/2401.13883](http://arxiv.org/abs/2401.13883)

    本文提出了一种领域无关的动态规划方法，并介绍了基于状态转移系统的动态规划描述语言。实验证明，该方法在许多组合优化问题上优于传统的混合整数规划和约束规划方法。

    

    对于组合优化问题，基于模型的范例如混合整数规划 (MIP) 和约束规划 (CP) 旨在解耦问题的建模和求解过程，这是声明性问题求解的“圣杯”。我们提出了领域无关的动态规划（DIDP），这是一种基于动态规划 (DP) 的新的基于模型的方法。虽然DP并不新鲜，但通常它被作为一种特定问题的方法来实现。我们引入了动态规划描述语言 (DyPDL)，一种基于状态转移系统的形式化语言，灵感来自于AI规划。我们展示了启发式搜索算法可以用来求解DyPDL模型，并提出了七种DIDP求解器。我们在常见的11个组合优化问题类别的基准实例上，将我们的DIDP求解器与商业MIP和CP求解器进行了实验比较（分别求解MIP和CP模型）。结果显示DIDP在九个问题类别中优于MIP，也优于CP在九个问题类别中。

    For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and 
    
[^56]: TPD: 通过原则发现和指导提升学生语言模型的推理能力

    TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance. (arXiv:2401.13849v1 [cs.CL])

    [http://arxiv.org/abs/2401.13849](http://arxiv.org/abs/2401.13849)

    通过原则发现和指导提升学生语言模型的推理能力的TPD框架模拟了教师和学生之间的互动，通过生成问题解决指令和纠正原则，从而引导学生模型从教师的指导和自身的错误中进行学习。

    

    大型语言模型(LLM)最近展示出了令人惊叹的推理能力。然而，更大的模型往往在推理任务上超过了较小的模型，因此有效地将这些能力从较大的模型转移到较小的模型是一项挑战。现有的方法往往依赖于大量的微调数据或在推理过程中与优秀的教师LLM进行持续的交互。我们引入了一种基于原则的师生框架，称为“通过原则发现教学”(TPD)，以解决这些限制。受人类学习机制启发，TPD模仿了教师和学生之间的互动，采用基于原则的方法。教师LLM生成问题解决指令和纠正原则，基于学生LLM的错误。这些原则指导指令的完善和从验证集中选择有教育意义的示例。这使得学生模型能够从教师的指导和自己的错误中学习。一旦

    Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the
    
[^57]: 一种基于V2X的隐私保护联合测量和学习系统

    A V2X-based Privacy Preserving Federated Measuring and Learning System. (arXiv:2401.13848v1 [cs.LG])

    [http://arxiv.org/abs/2401.13848](http://arxiv.org/abs/2401.13848)

    本文介绍了一种基于V2X的隐私保护联合测量和学习系统，通过V2V通信向其他车辆提供实时数据，同时通过V2N链路上的FL方案创建交通网络的预测模型。

    

    未来的自动驾驶车辆将使用各种传感器生成大量数据。这些数据不仅可以用于自动驾驶算法，还可以帮助其他车辆或基础设施进行实时决策。因此，车辆需要通过车辆到一切(V2X)技术来交换测量数据。此外，预测道路网络的状态可能也会有益处。通过这种预测，我们可以减轻道路拥堵、平衡停车场使用情况或优化交通流动。这将降低运输成本，减少环境影响。在本文中，我们提出了一种联合测量和学习系统，通过车辆到车辆(V2V)通信向其他车辆提供实时数据，同时通过车辆到网络(V2N)链路上的联合学习(FL)方案创建交通网络的预测模型。由于尚无真实世界的自动驾驶数据，我们模拟了数据，

    Future autonomous vehicles (AVs) will use a variety of sensors that generate a vast amount of data. Naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. Consequently, vehicles shall exchange their measurement data over Vehicle-to-Everything (V2X) technologies. Moreover, predicting the state of the road network might be beneficial too. With such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. That would decrease transportation costs as well as reduce its environmental impact.  In this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V) communication while also operating a federated learning (FL) scheme over the Vehicle-to-Network (V2N) link to create a predictive model of the transportation network. As we are yet to have real-world AV data, we model
    
[^58]: 语言模型中模型和人类置信度之间的校准差距

    The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])

    [http://arxiv.org/abs/2401.13835](http://arxiv.org/abs/2401.13835)

    该论文研究了大型语言模型在传达置信度方面模型和人类之间存在的差距，并发现默认解释会导致用户过高估计模型置信度和准确性。

    

    为了使大型语言模型（LLM）能够获得人类的信任，它们需要在某种意义上实现良好的校准，即能够准确评估和传达它们的预测正确的可能性。最近的研究关注了LLM内部置信度评估的质量，但问题仍然是LLM能够如何将这种内部模型置信度传达给人类用户。本文探讨了人类对LLM响应的外部置信度与模型内部置信度之间的差距。通过涉及多项选择题的实验，我们系统地检查了人类用户识别LLM输出可信度的能力。我们的研究重点分为两个方面：（1）评估用户对真实LLM置信度的感知和（2）调查个性化解释对该感知的影响。研究结果显示，LLM的默认解释往往会导致用户过高估计模型的置信度和准确性。通过修改解释的方式可以减小这种误差。

    For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the expl
    
[^59]: 用于马尔可夫物联网模型中数据上行的流量学习和主动无人机轨迹规划。

    Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models. (arXiv:2401.13827v1 [cs.LG])

    [http://arxiv.org/abs/2401.13827](http://arxiv.org/abs/2401.13827)

    本文提出了一个新的学习框架，用于估计基于马尔可夫事件的物联网设备的流量情况，并优化多个无人机的轨迹和调度策略，以降低信息时代、节省能量和提高吞吐量。

    

    信息时代(AoI)用于衡量数据的新鲜度。在物联网网络中，传统的资源管理方案依赖于设备和基站(BS)之间的消息交换，导致AoI高、能量消耗高且可靠性低。作为飞行基站的无人机(UAV)在减小AoI、节省能量和提高吞吐量方面具有许多优势。本文提出了一个基于学习的新框架，根据马尔科维事件估计物联网设备的流量到达情况。学习过程用于优化多个无人机的轨迹和调度策略。首先，BS预测设备未来的流量。我们比较了两种流量预测器：前向算法(FA)和长短期记忆(LSTM)。然后，我们提出了一种用于优化每个无人机最优策略的深度强化学习(DRL)方法。最后，我们针对提出的优化奖励函数进行了操作。

    The age of information (AoI) is used to measure the freshness of the data. In IoT networks, the traditional resource management schemes rely on a message exchange between the devices and the base station (BS) before communication which causes high AoI, high energy consumption, and low reliability. Unmanned aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the AoI, energy-saving, and throughput improvement. In this paper, we present a novel learning-based framework that estimates the traffic arrival of IoT devices based on Markovian events. The learning proceeds to optimize the trajectory of multiple UAVs and their scheduling policy. First, the BS predicts the future traffic of the devices. We compare two traffic predictors: the forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we propose a deep reinforcement learning (DRL) approach to optimize the optimal policy of each UAV. Finally, we manipulate the optimum reward function for the proposed
    
[^60]: 在AI中浏览数据集文档：Hugging Face数据集卡片的大规模分析

    Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face. (arXiv:2401.13822v1 [cs.LG])

    [http://arxiv.org/abs/2401.13822](http://arxiv.org/abs/2401.13822)

    本研究通过分析Hugging Face上的数据集文档，提供了Hugging Face数据集生态系统的概览并洞察到数据集文档实践的关键问题。研究发现，数据集卡片完成率与数据集受欢迎程度存在显著异质性，从业者在数据集描述和数据集结构部分更为关注，而对于使用数据的考虑部分较为忽视。

    

    机器学习的进展与数据集的创建密切相关。虽然数据文档被广泛认为对于ML的可靠性、可重复性和透明性至关重要，但我们缺乏对当前数据集文档实践的系统实证理解。为了揭示这个问题，我们以Hugging Face为例，这是一个用于共享和协作ML模型和数据集的最大平台。通过分析Hugging Face上的7433个数据集文档，我们的调查提供了Hugging Face数据集生态系统的概览和对数据集文档实践的洞察，得出了5个主要发现：（1）数据集卡片完成率显示出与数据集的受欢迎程度相关的显著异质性。（2）对数据集卡片中的每个部分进行细致的考察表明，从业者似乎更重视数据集描述和数据集结构部分，而对于使用数据的考虑部分更加忽视。

    Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face -- one of the largest platforms for sharing and collaborating on ML models and datasets -- as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, while the Considerations for Using the Data section rec
    
[^61]: 研究大型语言模型在代码克隆检测方面的功效

    Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])

    [http://arxiv.org/abs/2401.13802](http://arxiv.org/abs/2401.13802)

    这项研究探索了大型语言模型在代码克隆检测任务中的应用。

    

    大型语言模型（LLMs）在各种自然语言处理和软件工程任务中表现出了显著的成功，例如代码生成。LLMs主要在基于提示的零/少样本范式中被用于指导模型完成任务。本研究探索了LLMs在代码克隆检测（CCD）这一非生成任务中的适用性。

    Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
    
[^62]: 在真实环境中使用混合策略进行多目标导航

    Multi-Object Navigation in real environments using hybrid policies. (arXiv:2401.13800v1 [cs.RO])

    [http://arxiv.org/abs/2401.13800](http://arxiv.org/abs/2401.13800)

    本论文研究了在真实环境中使用混合策略进行多目标导航的问题，主要解决了导航中涉及的高级推理、局部规划和空间表示等复杂任务，在模拟环境下已有先进的模型，但在真实环境中尚未评估。

    

    导航问题在机器人领域一直以来都是通过SLAM和规划的结合来解决的。最近，除了航点规划，涉及到(视觉)高级推理的问题在模拟环境中得到了探索，并且主要通过大规模机器学习，特别是RL、离线RL或模仿学习来解决。这些方法要求智能体学习各种技能，比如局部规划、映射对象和查询学习得到的空间表示。与航点规划等简单任务不同，对于这些更复杂的任务，目前最先进的模型已经在模拟环境中进行了充分的评估，但据我们所知，尚未在真实环境中进行评估。本研究侧重于模拟到真实的迁移。我们以具有原始虚拟多目标导航对象实物复制品的物理环境为目标，针对具有挑战性的多目标导航任务引入了一种混合导航方法。

    Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.  In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, wh
    
[^63]: 不要按按钮！探索机器学习和迁移学习中的数据泄露风险

    Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])

    [http://arxiv.org/abs/2401.13796](http://arxiv.org/abs/2401.13796)

    本文讨论了机器学习中的数据泄露问题，即未预期的信息污染训练数据，影响模型性能评估，用户可能由于缺乏理解而忽视关键步骤，导致乐观的性能估计在实际场景中不成立。

    

    机器学习（ML）在各个领域取得了革命性的进展，为多个领域提供了预测能力。然而，随着ML工具的日益可获得性，许多从业者缺乏深入的ML专业知识，采用了“按按钮”方法，利用用户友好的界面而忽视了底层算法的深入理解。虽然这种方法提供了便利，但它引发了对结果可靠性的担忧，导致了错误的性能评估等挑战。本文解决了ML中的一个关键问题，即数据泄露，其中未预期的信息污染了训练数据，影响了模型的性能评估。由于缺乏理解，用户可能会无意中忽视关键步骤，从而导致在现实场景中可能不成立的乐观性能估计。评估性能与实际在新数据上的性能的差异是一个重要的关注点。本文特别将ML中的数据泄露分为不同类别，并讨论了相关解决方法。

    Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
    
[^64]: 从推特到引用：揭示社交媒体影响者对人工智能研究可见性的影响

    Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])

    [http://arxiv.org/abs/2401.13782](http://arxiv.org/abs/2401.13782)

    本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。

    

    随着人工智能和机器学习会议上被接受的论文数量达到数千篇，研究人员如何获取和阅读研究论文变得不清楚。本文研究了社交媒体影响者在增强机器学习研究可见性中的作用，特别是他们分享的论文引用次数。我们编制了一个包括8000多篇论文的全面数据集，涵盖了2018年12月至2023年10月的推特，以及基于出版年份、会议地点和摘要主题进行1：1匹配的对照组。我们的分析揭示了这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还深入研究了被展示作者的地理、性别和机构多样性。这些发现突显了社交媒体在学术交流中的不断扩大的影响力，并强调了当今数字化时代不断发展的生态系统的重要性。

    As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
    
[^65]: AlphaMapleSAT：一种基于MCTS的Cube-and-Conquer SAT求解器，用于解决困难的组合问题

    AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems. (arXiv:2401.13770v1 [cs.AI])

    [http://arxiv.org/abs/2401.13770](http://arxiv.org/abs/2401.13770)

    AlphaMapleSAT是一种基于MCTS的Cube-and-Conquer SAT求解器，通过推理驱动的先行计算技术来高效解决困难的组合问题。

    

    本文介绍了AlphaMapleSAT，一种新颖的基于Monte Carlo Tree Search (MCTS)的Cube-and-Conquer (CnC) SAT求解方法，旨在高效地解决具有挑战性的组合问题。尽管CnC求解器在解决各种困难的组合问题上取得了巨大成功，但多年来，CnC的先行计算技术并没有得到很大发展。其中一个原因是很难提出既低成本又能有效地将输入公式分割为子公式的新型分割技术，从而使整体运行时间最小化。当前最先进的CnC求解器（如March）使用的先行计算技术通过约束搜索最优分割变量来降低计算成本。相比之下，我们的关键创新是一种基于推理驱动的MCTS先行计算技术，通过进行更深入的启发式搜索来寻找有效的分割，同时使计算成本低。我们进行了详细的对比实验

    This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS) based Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving challenging combinatorial problems. Despite the tremendous success of CnC solvers in solving a variety of hard combinatorial problems, the lookahead cubing techniques at the heart of CnC have not evolved much for many years. Part of the reason is the sheer difficulty of coming up with new cubing techniques that are both low-cost and effective in partitioning input formulas into sub-formulas, such that the overall runtime is minimized.  Lookahead cubing techniques used by current state-of-the-art CnC solvers, such as March, keep their cubing costs low by constraining the search for the optimal splitting variables. By contrast, our key innovation is a deductively-driven MCTS-based lookahead cubing technique, that performs a deeper heuristic search to find effective cubes, while keeping the cubing cost low. We perform an extensive compari
    
[^66]: 工具变量模型中的假设和界限

    Assumptions and Bounds in the Instrumental Variable Model. (arXiv:2401.13758v1 [math.ST])

    [http://arxiv.org/abs/2401.13758](http://arxiv.org/abs/2401.13758)

    本文证明了在工具变量（IV）模型中，具有二进制响应$Y$和二进制处理$X$的情况下，使用接受$K$个状态的仪器$Z$时，关于ACE界限的结果。

    

    在本文中，我们给出了关于具有二进制响应$Y$和二进制处理$X$的工具变量（IV）模型的结果证明，但是仪器$Z$接受$K$个状态，这些状态最初是在Richardson＆Robins（2014）的论文“ACE Bounds; SEMS with Equilibrium Conditions”中提出的（arXiv:1410.0470）。

    In this note we give proofs for results relating to the Instrumental Variable (IV) model with binary response $Y$ and binary treatment $X$, but with an instrument $Z$ that takes $K$ states that were originally stated in Richardson & Robins (2014), "ACE Bounds; SEMS with Equilibrium Conditions," arXiv:1410.0470.
    
[^67]: 解释图像分类器（arXiv：2401.13752v1 [cs.AI]）

    Explaining Image Classifiers. (arXiv:2401.13752v1 [cs.AI])

    [http://arxiv.org/abs/2401.13752](http://arxiv.org/abs/2401.13752)

    本论文研究了解释图像分类器的问题，并以Mothilal等人[2021]的工作为起点。论文指出Mothilal等人虽然声称使用Halpern [2016]提出的解释定义，但实际上有一定的偏差。论文还展示了Halpern的解释定义可以处理其他方法难以解决的缺失和罕见事件问题。

    

    我们的研究重点是解释图像分类器，以莫蒂拉尔等人[2021]（MMTS）的工作作为出发点。我们观察到，尽管MMTS声称使用Halpern [2016]提出的解释定义，但并非完全如此。粗略地说，Halpern的定义包含必要性和充分性两个条件。MMTS将必要性条款替换为我们展示的一个要求，并且暗含了必要性。Halpern的定义还允许代理人限制考虑的选项集。虽然这些差异可能看似微小，但如我们所示，它们对解释产生了重要影响。我们还展示了，几乎没有改变，Halpern的定义可以处理其他方法难以解决的两个问题：解释缺失（例如，图像分类器输出“无肿瘤”）和解释罕见事件（比如肿瘤）。

    We focus on explaining image classifiers, taking the work of Mothilal et al. [2021] (MMTS) as our point of departure. We observe that, although MMTS claim to be using the definition of explanation proposed by Halpern [2016], they do not quite do so. Roughly speaking, Halpern's definition has a necessity clause and a sufficiency clause. MMTS replace the necessity clause by a requirement that, as we show, implies it. Halpern's definition also allows agents to restrict the set of options considered. While these difference may seem minor, as we show, they can have a nontrivial impact on explanations. We also show that, essentially without change, Halpern's definition can handle two issues that have proved difficult for other approaches: explanations of absence (when, for example, an image classifier for tumors outputs "no tumor") and explanations of rare events (such as tumors).
    
[^68]: 一种针对深度卷积神经网络的鲁棒性建模的系统化方法

    A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])

    [http://arxiv.org/abs/2401.13751](http://arxiv.org/abs/2401.13751)

    本论文提出一种系统化方法，用于针对深度卷积神经网络进行鲁棒性建模。研究发现隐藏层数量对模型的推广性能有影响，同时还测试了模型大小、浮点精度、训练数据和模型输出的噪声水平等参数。为了改进模型的预测能力和计算成本，提出了一种使用诱发故障来建模故障概率的方法。

    

    当有大量标记数据可用时，卷积神经网络已经被证明在许多领域都可以广泛应用。最近的趋势是使用具有越来越多可调参数的模型，以提高模型准确性，降低模型损失或创建更具对抗鲁棒性的模型，而这些目标通常相互矛盾。特别是，最近的理论研究提出了对更大模型能否推广到受控的训练和测试集之外的数据的疑问。因此，我们研究了ResNet模型中隐藏层的数量在MNIST、CIFAR10和CIFAR100数据集上的作用。我们测试了各种参数，包括模型的大小、浮点精度，以及训练数据和模型输出的噪声水平。为了改进模型的预测能力和计算成本，我们提供了一种使用诱发故障来建模故障概率的方法。

    Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a fun
    
[^69]: 主动情绪追踪器：基于人工智能的持续情绪和心理状态监测

    Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion Monitoring. (arXiv:2401.13722v1 [cs.HC])

    [http://arxiv.org/abs/2401.13722](http://arxiv.org/abs/2401.13722)

    该项目使用预训练的BERT模型检测社交媒体和用户的网络浏览数据中的抑郁文本，并结合可穿戴设备的生理信号提供长期追踪和预后，有望提高早期检测抑郁症的能力，推动整体心理健康。

    

    该研究项目旨在解决当今数字时代不断增长的心理健康挑战。它采用改进的预训练BERT模型来检测社交媒体和用户的网络浏览数据中的抑郁文本，实现了令人印象深刻的93％的测试准确性。同时，该项目旨在结合可穿戴设备（如智能手表和脑电传感器）的生理信号，提供情绪障碍和心理状态的长期追踪和预后。这种综合方法有望提高抑郁的早期检测并改善整体心理健康结果。

    This research project aims to tackle the growing mental health challenges in today's digital age. It employs a modified pre-trained BERT model to detect depressive text within social media and users' web browsing data, achieving an impressive 93% test accuracy. Simultaneously, the project aims to incorporate physiological signals from wearable devices, such as smartwatches and EEG sensors, to provide long-term tracking and prognosis of mood disorders and emotional states. This comprehensive approach holds promise for enhancing early detection of depression and advancing overall mental health outcomes.
    
[^70]: 我能相信我的假数据吗--一种综合质量评估框架用于医疗领域的合成表格数据。

    Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare. (arXiv:2401.13716v1 [cs.LG])

    [http://arxiv.org/abs/2401.13716](http://arxiv.org/abs/2401.13716)

    这项研究开发了一个综合质量评估框架，用于评估医疗领域中合成表格数据的质量，并解决了现有框架中存在的一些问题和限制。

    

    在医疗领域，确保人工智能工具的安全采用取决于获得足够的训练、测试和验证数据。为了解决隐私问题和监管要求，使用合成数据被提出。合成数据是通过在真实数据上训练生成器来创建具有类似统计属性的数据集。不同分类标准的竞争性指标已被提出，导致了一个复杂的情况。优化质量需要平衡使数据适用于使用的考虑因素，但现有框架中留下了一些相关的维度。我们对在表格医疗数据范围内评估质量指标和使用深度生成方法生成的合成数据进行了全面的文献综述。基于此和团队的集体经验，我们开发了一个概念性的质量保证框架。该框架的适用性与荷兰国家癌症登记中心的实际案例进行了基准测试。

    Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Regist
    
[^71]: EMP: 有效的多维持久化用于图表示学习

    EMP: Effective Multidimensional Persistence for Graph Representation Learning. (arXiv:2401.13713v1 [cs.LG])

    [http://arxiv.org/abs/2401.13713](http://arxiv.org/abs/2401.13713)

    本论文提出了EMP框架，通过同时变化多个尺度参数来探索数据，并将描述符函数整合到分析过程中，从而提供一个高度表达的数据摘要。

    

    拓扑数据分析(TDA)在从流形学习到图分类的各种机器学习任务中越来越受到关注。TDA中的关键技术是持久化同调(PH)，通过跟踪潜在结构随尺度参数变化的演化，为数据提供独特的拓扑特征。现有的PH工具仅能通过单个过滤参数对数据进行分析。然而，许多情况下需要考虑多个相关参数以获取更详细的数据洞察。我们通过引入有效的多维持久化(EMP)框架来解决这个问题。该框架通过同时改变多个尺度参数来探索数据。该框架将描述符函数整合到分析过程中，得到一个高度表达的数据摘要。它无缝地将建立的单一PH摘要集成到多维对应物中，如EMP景观、轮廓线等。

    Topological data analysis (TDA) is gaining prominence across a wide spectrum of machine learning tasks that spans from manifold learning to graph classification. A pivotal technique within TDA is persistent homology (PH), which furnishes an exclusive topological imprint of data by tracing the evolution of latent structures as a scale parameter changes. Present PH tools are confined to analyzing data through a single filter parameter. However, many scenarios necessitate the consideration of multiple relevant parameters to attain finer insights into the data. We address this issue by introducing the Effective Multidimensional Persistence (EMP) framework. This framework empowers the exploration of data by simultaneously varying multiple scale parameters. The framework integrates descriptor functions into the analysis process, yielding a highly expressive data summary. It seamlessly integrates established single PH summaries into multidimensional counterparts like EMP Landscapes, Silhouett
    
[^72]: 加速双曲线t-SNE算法

    Accelerating hyperbolic t-SNE. (arXiv:2401.13708v1 [cs.HC])

    [http://arxiv.org/abs/2401.13708](http://arxiv.org/abs/2401.13708)

    本文提出了加速双曲线t-SNE算法，通过引入极坐标四叉树的加速结构，解决了现有方法在处理大规模输入数据时的效率问题。

    

    在各个领域中，理解层次化或高维数据的结构是必要的。双曲空间已经被证明是一种重要的嵌入计算和分析工具，因为它们的非线性特性很适合处理树状或图形数据。因此，它们也被用于高维数据的可视化，其中它们表现出更好的嵌入性能。然而，现有的嵌入到双曲空间的降维方法都不能很好地处理输入数据的规模扩展问题。这是因为嵌入是通过迭代优化方案计算的，而每次迭代的计算成本与输入规模的平方成正比。此外，由于双曲空间的非线性特性，欧几里德加速结构不能直接转化为双曲设置。本文介绍了第一个用于双曲嵌入加速的结构，基于极坐标四叉树。我们将其应用于t-SNE算法，提出了加速双曲线t-SNE算法。

    The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, Euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We com
    
[^73]: 使用Java几何专家作为数学竞赛准备的指导

    Using Java Geometry Expert as Guide in the Preparations for Math Contests. (arXiv:2401.13704v1 [cs.CY])

    [http://arxiv.org/abs/2401.13704](http://arxiv.org/abs/2401.13704)

    Java几何专家（JGEX）在学校环境中的使用情况及其在解决数学竞赛题目中的支持。同时也讨论了程序的局限性。

    

    我们介绍了Java几何专家（JGEX）在学校环境中的使用情况，重点关注奥地利的学校系统。JGEX在某些课堂情境下可以提供很大的支持，尤其是解决数学竞赛题目。此外，我们还讨论了该程序的一些局限性。

    We give an insight into Java Geometry Expert (JGEX) in use in a school context, focusing on the Austrian school system. JGEX can offer great support in some classroom situations, especially for solving mathematical competition tasks. Also, we discuss some limitations of the program.
    
[^74]: 使用GeoGebra Discovery自动推理解决N'aboj 2023竞赛的一些几何问题

    Solving Some Geometry Problems of the N\'aboj 2023 Contest with Automated Deduction in GeoGebra Discovery. (arXiv:2401.13703v1 [math.HO])

    [http://arxiv.org/abs/2401.13703](http://arxiv.org/abs/2401.13703)

    本文使用GeoGebra Discovery软件工具，利用计算机解决了N'aboj 2023竞赛的一些几何问题，并分析了问题输入的难度和未来的目标。

    

    在本文中，我们使用计算机解决了N'aboj 2023竞赛中的一些几何问题，借助计算软件工具GeoGebra Discovery进行计算。在每个案例中，计算都需要符号计算。我们分析了将问题输入计算机的难度，并制定了进一步的目标，以使这类竞赛问题在将来更易处理。

    In this article, we solve some of the geometry problems of the N\'aboj 2023 competition with the help of a computer, using examples that the software tool GeoGebra Discovery can calculate. In each case, the calculation requires symbolic computations. We analyze the difficulty of feeding the problem into the machine and set further goals to make the problems of this type of contests even more tractable in the future.
    
[^75]: 实现自动可读的直尺和圆规构造的证明

    Towards Automated Readable Proofs of Ruler and Compass Constructions. (arXiv:2401.13700v1 [cs.LO])

    [http://arxiv.org/abs/2401.13700](http://arxiv.org/abs/2401.13700)

    本研究提出了一种能够生成直尺和圆规构造问题的可读与形式化正确性证明的方法，并与自动定理证明器合作实现。目标是从基本公理形式化证明所有高级引理。

    

    虽然已经有几个系统成功地生成了直尺和圆规构造问题的构造步骤，但它们都没有提供可读的合成正确性证明。在本研究中，我们展示了我们的三角形构造求解器ArgoTriCS如何能够与一阶逻辑和连续逻辑的自动定理证明器合作，生成既可被人类理解又可以由交互式定理证明器（如Coq或Isabelle/HOL）进行检查的构造正确性证明。这些证明目前依赖于许多高级引理，我们的目标是从几何的基本公理中形式化证明所有这些引理。

    Although there are several systems that successfully generate construction steps for ruler and compass construction problems, none of them provides readable synthetic correctness proofs for generated constructions. In the present work, we demonstrate how our triangle construction solver ArgoTriCS can cooperate with automated theorem provers for first order logic and coherent logic so that it generates construction correctness proofs, that are both human-readable and formal (can be checked by interactive theorem provers such as Coq or Isabelle/HOL). These proofs currently rely on many high-level lemmas and our goal is to have them all formally shown from the basic axioms of geometry.
    
[^76]: 生成式人工智能驱动的物联网健康护理中的人类数字孪生：一项综合调查

    Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey. (arXiv:2401.13699v1 [cs.HC])

    [http://arxiv.org/abs/2401.13699](http://arxiv.org/abs/2401.13699)

    该论文调查了在物联网健康护理中利用生成式人工智能驱动的人类数字孪生的应用。人类数字孪生作为多功能、生动的人类数字测试平台，可以模拟结果并指导实际治疗，从而提高物联网健康护理的能力。

    

    物联网可以显著提高人类生活的质量，特别是在健康护理方面吸引了广泛的关注。同时，人类数字孪生被提出作为一种创新的范式，可以全面地描述个体人体在数字世界中的复制，并实时反映其物理状况。自然地，人类数字孪生被设想为通过充当多功能、生动的人类数字测试平台来增强物联网健康护理的能力，模拟结果并指导实际治疗。然而，成功建立人类数字孪生需要高保真度的虚拟建模和强大的信息交互，但可能存在稀缺、偏倚和噪声数据。幸运的是，最近流行的一种名为生成式人工智能（GAI）的技术可能是一个有前途的解决方案，因为它可以利用先进的人工智能算法自动生成、操作和修改渐变视图。

    The Internet of things (IoT) can significantly enhance the quality of human life, specifically in healthcare, attracting extensive attentions to IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as an innovative paradigm that can comprehensively characterize the replication of the individual human body in the digital world and reflect its physical status in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the application of healthcare monitoring by acting as a versatile and vivid human digital testbed, simulating the outcomes and guiding the practical treatments. However, successfully establishing HDT requires high-fidelity virtual modeling and strong information interactions but possibly with scarce, biased and noisy data. Fortunately, a recent popular technology called generative artificial intelligence (GAI) may be a promising solution because it can leverage advanced AI algorithms to automatically create, manipulate, and modify val
    
[^77]: 迈向使用多模态基础模型的稳健多模态学习

    Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])

    [http://arxiv.org/abs/2401.13697](http://arxiv.org/abs/2401.13697)

    该论文提出了一种名为TRML的框架，旨在实现在随机缺失模态的情况下的稳健多模态学习。TRML利用生成的虚拟模态替换缺失的模态，并且通过对齐语义空间来解决模态缺失的问题。

    

    现有的多模态情感分析任务高度依赖于训练集和测试集是完整的多模态数据的假设，然而这个假设在现实场景中往往很难成立：多模态数据往往在现实世界中是不完整的。因此，在存在随机缺失模态的场景中，一种稳健的多模态模型将会更受欢迎。最近，基于CLIP的多模态基础模型通过学习图像和文本对的跨模态语义对齐，在众多多模态任务中展示了令人印象深刻的性能，但是这些多模态基础模型也无法直接解决涉及模态缺失的场景。为了缓解这个问题，我们提出了一个简单而有效的框架，即TRML（Toward Robust Multimodal Learning using Multimodal Foundational Models）。TRML利用生成的虚拟模态替换缺失的模态，并且对生成的模态和缺失的模态之间的语义空间进行对齐。具体来说，我们设计了一个缺失模态生成模块，可以生成缺失的模态，然后通过特征对齐模块来学习模态之间的对齐关系。

    Existing multimodal sentiment analysis tasks are highly rely on the assumption that the training and test sets are complete multimodal data, while this assumption can be difficult to hold: the multimodal data are often incomplete in real-world scenarios. Therefore, a robust multimodal model in scenarios with randomly missing modalities is highly preferred. Recently, CLIP-based multimodal foundational models have demonstrated impressive performance on numerous multimodal tasks by learning the aligned cross-modal semantics of image and text pairs, but the multimodal foundational models are also unable to directly address scenarios involving modality absence. To alleviate this issue, we propose a simple and effective framework, namely TRML, Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML employs generated virtual modalities to replace missing modalities, and aligns the semantic spaces between the generated and missing modalities. Concretely, we design a missin
    
[^78]: 挑战设计路线图

    Challenge design roadmap. (arXiv:2401.13693v1 [cs.OH])

    [http://arxiv.org/abs/2401.13693](http://arxiv.org/abs/2401.13693)

    挑战设计是一项类似于推出产品的过程，需要制定有效的游戏规则，并考虑解决现实问题、推进科学技术、科学发现和教育公众等多个目标。

    

    挑战可以被看作是一种激励参与者解决严肃任务的游戏。因此，竞赛组织者必须制定有效的游戏规则。然而，这些规则除了使游戏对参与者来说趣味十足之外还有多个目标。这些目标可能包括解决现实世界的问题、推进科学或技术领域、进行科学发现和教育公众。在许多方面，创建一项挑战类似于推出一款产品。它需要相同的兴奋和严格的测试，其目标是以参与者的形式吸引“客户”。这个过程从一个坚实的计划开始，比如一个竞赛提案，最终将提交给国际会议并接受同行评审。虽然同行评审不能保证质量，但它确实迫使组织者考虑他们的挑战的影响，识别潜在的疏漏，并通常提高其质量。本章提供了指导原则。

    Challenges can be seen as a type of game that motivates participants to solve serious tasks. As a result, competition organizers must develop effective game rules. However, these rules have multiple objectives beyond making the game enjoyable for participants. These objectives may include solving real-world problems, advancing scientific or technical areas, making scientific discoveries, and educating the public. In many ways, creating a challenge is similar to launching a product. It requires the same level of excitement and rigorous testing, and the goal is to attract ''customers'' in the form of participants. The process begins with a solid plan, such as a competition proposal that will eventually be submitted to an international conference and subjected to peer review. Although peer review does not guarantee quality, it does force organizers to consider the impact of their challenge, identify potential oversights, and generally improve its quality. This chapter provides guidelines 
    
[^79]: 用于非结构化数据的过程挖掘：挑战与研究方向

    Process Mining for Unstructured Data: Challenges and Research Directions. (arXiv:2401.13677v1 [cs.DB])

    [http://arxiv.org/abs/2401.13677](http://arxiv.org/abs/2401.13677)

    这篇论文讨论了将过程挖掘应用于非结构化数据所面临的挑战，并提出了初步解决方案和未来的研究方向。

    

    将过程挖掘应用于非结构化数据可能显著提升对非结构化数据常见数据格式领域的新见解。要有效分析非结构化数据，并对分析结果传达信心，需要克服多个挑战。本文旨在讨论这些挑战，提出初步解决方案并描述未来的研究方向。我们希望这篇文章为未来在这个主题上的合作奠定基础。

    The application of process mining for unstructured data might significantly elevate novel insights into disciplines where unstructured data is a common data format. To efficiently analyze unstructured data by process mining and to convey confidence into the analysis result, requires bridging multiple challenges. The purpose of this paper is to discuss these challenges, present initial solutions and describe future research directions. We hope that this article lays the foundations for future collaboration on this topic.
    
[^80]: 用智能数据管理和洞察力改变农业

    Transforming Agriculture with Intelligent Data Management and Insights. (arXiv:2401.13672v1 [cs.DB])

    [http://arxiv.org/abs/2401.13672](http://arxiv.org/abs/2401.13672)

    使用智能数据管理和洞察力可以解决现代农业面临的增长需求和气候变化等挑战，通过数据创新可以提高农业生产力、可持续性和适应性。

    

    现代农业面临着在气候变化和自然资源减少的约束下，在人口增长的情况下满足粮食、燃料、饲料和纤维的增加需求的巨大挑战。急需数据创新来确保和提高农业生态系统的生产力、可持续性和适应性。随着各种传感器和物联网设备变得越来越可用、可负担得起、可靠、稳定，可以进行多时空尺度、实时和高分辨率的数据收集、整合和分析。与此同时，庞大的数据量对数据存储和分析构成了巨大挑战，科学家们常规的数据管理和分析实践越来越低效。此外，来自不同学科的数据，如基因组学、表型学、环境学、农学和社会经济学，可能高度异质。

    Modern agriculture faces grand challenges to meet increased demands for food, fuel, feed, and fiber with population growth under the constraints of climate change and dwindling natural resources. Data innovation is urgently required to secure and improve the productivity, sustainability, and resilience of our agroecosystems. As various sensors and Internet of Things (IoT) instrumentation become more available, affordable, reliable, and stable, it has become possible to conduct data collection, integration, and analysis at multiple temporal and spatial scales, in real-time, and with high resolutions. At the same time, the sheer amount of data poses a great challenge to data storage and analysis, and the \textit{de facto} data management and analysis practices adopted by scientists have become increasingly inefficient. Additionally, the data generated from different disciplines, such as genomics, phenomics, environment, agronomy, and socioeconomic, can be highly heterogeneous. That is, d
    
[^81]: 常见的随机神经网络在可靠性临床决策支持方面的不足

    Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])

    [http://arxiv.org/abs/2401.13657](http://arxiv.org/abs/2401.13657)

    本研究调查了随机神经网络在临床应用中的可靠性，并发现常见的深度学习方法在数据转移情况下过于自信。这强调了对本地不确定性可靠估计及其向最终用户传达的重要性。

    

    由于伦理和安全相关的关切，人工智能的广泛应用于医学决策仍然受阻。对于基于人工智能的医疗决策支持系统来说，可靠性和可信度至关重要。然而，常见的深度学习方法在数据转移下往往过于自信。这种在基于证据的场景之外不恰当的推理可能会产生严重后果。这凸显了对本地不确定性可靠估计及其向最终用户传达的重要性。尽管随机神经网络被誉为这些问题的潜在解决方案，但本研究调查了其在临床应用中的实际可靠性。我们以从MIMIC3研究中使用的EHR的ICU住院病死率预测为例来进行分析。对于EHR时间序列的预测，我们采用了仅编码器的Transformer模型。通过纳入常见方法来实现模型函数的随机性。

    Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such 
    
[^82]: 基于稀疏网格的不连续性检测的图信息神经网络

    Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])

    [http://arxiv.org/abs/2401.13652](http://arxiv.org/abs/2401.13652)

    本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。

    

    本文提出了一种新颖的方法来检测不连续函数的不连续界面。该方法利用了基于图的神经网络（GINNs）和稀疏网格来解决维度大于3的情况下的不连续性检测。训练过的GINNs在稀疏网格上识别有问题的点，并利用构建在网格上的图结构实现高效准确的不连续性检测性能。我们还引入了一种递归算法用于一般的基于稀疏网格的检测器，具有收敛性和易于应用性。在维度n=2和n=4的函数上进行的数值实验证明了GINNs在检测不连续界面方面的高效性和鲁棒泛化能力。值得注意的是，经过训练的GINNs具有可移植性和多功能性，可以集成到各种算法中并共享给用户。

    In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
    
[^83]: 有关算法决策的信息：探索受到算法决策影响的人的信息需求。

    Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])

    [http://arxiv.org/abs/2401.13324](http://arxiv.org/abs/2401.13324)

    本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。

    

    AI系统的解释很少涉及到受算法决策影响的人的信息需求。这种传达信息与受影响利益相关者所关心的信息之间的差距可能阻碍对监管框架（如AI法案）的理解和遵守。为了解决这个差距，我们提出了“XAI初学者问题库”：这是一个涵盖两个算法决策应用领域（就业预测和健康监测）中受影响利益相关者信息需求的目录，包括数据、系统背景、系统使用和系统规范等类别。信息需求是通过访谈研究收集的，参与者根据自己的问题获得解释。参与者还报告了他们的理解和决策信心，结果显示，尽管在接受解释后信心倾向于增加，但参与者也面临着理解上的挑战，如无法解释为什么自己的理解感觉不完整。解释还对理解产生了影响。

    Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
    
[^84]: 对AI代理的可见性

    Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])

    [http://arxiv.org/abs/2401.13138](http://arxiv.org/abs/2401.13138)

    本文评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录，并提出了可能的实施方式。这些措施在集中化和去中心化部署环境中应用广泛，有助于理解和减轻AI代理带来的风险。

    

    将商业、科学、政府和个人活动委托给具有有限监督能力的AI代理系统，可能会加剧现有的社会风险并引入新的风险。理解和减轻这些风险涉及对现有治理结构进行批判性评估，根据需要进行修订和调整，并确保关键利益相关者的问责制。我们将AI代理的使用地点、原因、方式以及使用者等信息称为“可见性”，这对于实现上述目标至关重要。在本文中，我们评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录。对于每一种措施，我们概述了可能的实施方式，这些方式在侵入性和信息性方面有所差异。我们分析了这些措施在集中化和去中心化部署环境中的应用情况，考虑了不同变量的影响。

    Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
    
[^85]: 二进制结构的物理信息神经网络用于解决具有快速变化解的方程

    Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])

    [http://arxiv.org/abs/2401.12806](http://arxiv.org/abs/2401.12806)

    本论文提出了一种二进制结构的物理信息神经网络框架，通过利用二进制结构来捕捉局部特征，并解决了传统物理信息神经网络在处理具有快速变化解的方程时的困难。

    

    物理信息神经网络(PINNs)，基于深度学习，已成为解决偏微分方程(PDEs)的一种有前途的方法。通过将PDEs描述的物理信息嵌入前馈神经网络中，PINNs被训练为替代模型，以近似解决方案而无需标签数据。然而，尽管PINNs表现出了卓越的性能，但它们在处理具有快速变化解的方程时可能会遇到困难。这些困难包括收敛速度慢、易陷入局部最小值和解决精度降低。为了解决这些问题，我们提出了一种二进制结构的物理信息神经网络(BsPINN)框架，该框架使用二进制结构的神经网络(BsNN)作为神经网络组件。通过利用二进制结构，BsPINNs在捕捉局部特征方面表现出色

    Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features 
    
[^86]: What the Weight?! 零样本知识组合的统一框架

    What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v1 [cs.CL])

    [http://arxiv.org/abs/2401.12756](http://arxiv.org/abs/2401.12756)

    本研究提出了一个新的零样本模块组合框架，统一了选择、加权和组合参数模块的各种变化。以领域知识和适配器层为场景，通过系统化的统一概念，进行了首次全面的零样本知识组合的基准研究。

    

    模型中所封装的知识是确定其在下游任务中最终性能的核心因素。自然语言处理领域的许多研究都集中在存储和调整不同类型知识的有效方法上，例如在专用的模块化结构中，以及如何通过学习额外的参数来有效地组合这些知识。然而，鉴于存在许多可能的选项，对于这些组合中涉及的机制缺乏全面的理解，因此目前仍不清楚应该使用哪些策略。为了填补这一研究空白，我们提出了一个新的零样本模块组合框架，它涵盖了现有的一些选择、加权和组合参数模块的变化，统一了这些概念。在聚焦领域知识和适配器层的情景下，我们的框架提供了一个系统化的统一概念，使我们能够进行首次全面的各种零样本知识组合的基准研究。

    The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge compositio
    
[^87]: 基于能量的自动化模型评估

    Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])

    [http://arxiv.org/abs/2401.12689](http://arxiv.org/abs/2401.12689)

    提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。

    

    传统的机器学习模型评估协议依赖于标记的、假设独立同分布的测试数据集，而这在实际应用中往往并不常见。自动模型评估（AutoEval）提出了一种替代传统工作流程的方法，通过形成一个接近预测性能的测试管线，而无需真实标签的存在。尽管AutoEval框架近年来取得了一些成功，但仍存在过度自信、存储和计算成本高的问题。因此，我们提出了一种新颖的度量方式——元分布能量（MDE），它可以使AutoEval框架更加高效和有效。MDE的核心是建立一个关于个体样本相关信息（能量）的元分布统计量，然后通过基于能量的学习提供更平滑的表示能力。我们通过将MDE与分类损失相连接，进一步提供了理论洞见。我们还提供了大量实验证据来验证我们的方法。

    The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
    
[^88]: BiTA: 大语言模型中无损加速的双向调整

    BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])

    [http://arxiv.org/abs/2401.12522](http://arxiv.org/abs/2401.12522)

    BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。

    

    大型语言模型（LLMs）通常在推理过程中使用自回归生成，导致高内存带宽需求和延迟延长。为了减轻这种效率低下的问题，我们提出了一种创新方法——双向调整以实现无损加速（BiTA），通过简化的半自回归生成和草稿验证来加速LLMs。受启发于提示调整的概念，我们使用一种参数高效的设计，称为双向调整，来增强LLMs在半自回归生成方面的能力。采用高效的基于树的解码，模型可以同时进行草稿候选生成和验证，确保输出结果与它们的自回归对应物在贪婪抽样下完全相同。BiTA作为一个轻量级的插件模块，可以无缝增强现有LLMs的推理效率，而无需额外的辅助模型或承担显著的额外内存开销。通过应用提出的BiTA，LLaMA-2-70B-Chat实现了

    Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
    
[^89]: 大型语言模型的指令指纹识别

    Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])

    [http://arxiv.org/abs/2401.12255](http://arxiv.org/abs/2401.12255)

    这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。

    

    从零开始训练大型语言模型（LLM）的巨大成本使得对模型进行指纹识别以保护知识产权成为必要，通过所有权认证并确保下游用户和开发者遵守许可条款（如限制商业使用）。在这项研究中，我们提出了LLM指纹识别的试点研究，作为一种非常轻量级的指令调整形式。模型发布者指定一个机密的私钥，并将其植入为一个指令后门，当密钥存在时，导致LLM生成特定的文本。对11个常用LLMs的结果表明，这种方法轻量级且不影响模型的正常行为。它还可以防止发布者过度宣称，对指纹猜测和参数高效训练保持鲁棒性，并支持类似于MIT许可证的多阶段指纹识别。代码可在https://cnut1648.github.io/Model-Fingerprint/中获得。

    The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
    
[^90]: Mementos: 一种针对图像序列的多模态大型语言模型推理的综合基准测试

    Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])

    [http://arxiv.org/abs/2401.10529](http://arxiv.org/abs/2401.10529)

    Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。

    

    多模态大型语言模型（MLLMs）在处理各种视觉语言任务方面展示了高超的能力。然而，目前的MLLM基准测试主要用于评估基于单个图像的静态信息的推理能力，而现代MLLM在从图像序列中进行推断的能力，在理解不断变化的世界方面的重要性却被较少研究。为了解决这一挑战，本文引入了一个新的基准测试Mementos，用于评估MLLM的序列图像推理能力。Mementos包括4761个具有不同长度的多样的图像序列。我们还采用了GPT-4辅助方法来评估MLLM的推理性能。通过对Mementos中包括GPT-4V和Gemini在内的九个最新MLLM进行仔细评估，我们发现它们在准确描述所给图像序列的动态信息方面存在困难，往往导致对象及其对应行为的错误描述或错觉。

    Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
    
[^91]: 中文数据处理中的佼佼者：英文代码模型

    Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])

    [http://arxiv.org/abs/2401.10286](http://arxiv.org/abs/2401.10286)

    在中文数据处理中，基于代码的语言模型在非编程中文任务中表现出色，尤其是在对中文幻觉敏感的任务中。此研究为讨论“中文房间”思想实验提供了独特的视角。

    

    尽管在语言模型的应用中，任务与训练语料之间的对齐是一个基本的共识，但我们的一系列实验和我们设计的评估指标表明，基于代码的大型语言模型(LLMs)在非编程中文任务中的表现明显优于与任务紧密匹配的训练数据。此外，在对中文幻觉敏感程度较高的任务中，展示较少中文语言特征的模型表现更好。我们的实验结果可以通过简单地用代码模型替换基础模型，在中文数据处理任务中，如为检索增强生成(RAG)准备数据，很容易得到复制。此外，我们的研究为讨论“中文房间”思想实验提供了独特的视角。

    While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
    
[^92]: SAiD: 使用扩散方法驱动的语音驱动表情动画

    SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])

    [http://arxiv.org/abs/2401.08655](http://arxiv.org/abs/2401.08655)

    提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画方法，通过轻量级的Transformer-based U-Net模型和音频与视觉的交叉模态对齐偏差，实现了较好的唇部同步和更多样化的唇部运动。

    

    尽管进行了大量研究，但由于缺乏大规模的视听数据集，语音驱动的三维面部动画仍然具有挑战性。大多数过去的工作通常采用最小二乘法在小数据集上学习回归模型，但在从语音生成各种唇部动作方面遇到困难，并且需要大量精细调整生成的输出结果。为了解决这些问题，我们提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画，这是一种轻量级的基于Transformer的U-Net模型，具有音频和视觉之间的交叉模态对齐偏差，以增强唇部同步。此外，我们还介绍了BlendVOCA，这是一种语音音频和混合形状面部模型参数对的基准数据集，以解决公共资源的缺乏问题。我们的实验结果表明，所提出的方法在唇部同步方面达到了与基线相当或更好的性能，确保了更多样化的唇部运动，并简化了动画流程。

    Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animati
    
[^93]: DiConStruct: 基于黑盒精华的因果概念解释

    DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08534](http://arxiv.org/abs/2401.08534)

    DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。

    

    模型可解释性在人工智能决策系统中起着核心作用。理想情况下，解释应该使用人可解释的语义概念来表达。此外，解释器应该捕捉这些概念之间的因果关系，以便对解释进行推理。最后，解释方法应该高效，并不损害预测任务的性能。尽管近年来AI解释性取得了快速进展，但据我们所知，至今没有一种方法满足这三个条件。事实上，主流的局部概念可解释性方法不产生因果解释，并在解释性和预测性能之间存在权衡。我们提出了DiConStruct，一种既基于概念又具有因果性的解释方法，旨在通过结构性因果模型和概念归因方式创建更具可解释性的局部解释。我们的解释器作为一个精华模型适用于任何黑盒机器学习模型。

    Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
    
[^94]: CoSSegGaussians：紧凑且迅速的3D高斯场景分割方法

    CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])

    [http://arxiv.org/abs/2401.05925](http://arxiv.org/abs/2401.05925)

    CoSSegGaussians是一种紧凑且迅速的3D高斯场景分割方法，通过映射空间和语义特征实现紧凑和可靠的零样本场景分割。

    

    我们提出了一种紧凑且迅速的3D高斯场景分割方法（CoSSegGaussians），该方法仅使用RGB图像输入，以快速的渲染速度实现紧凑的3D一致性场景分割。先前基于NeRF的3D分割方法依赖于隐式或体素神经场表示和光线行进体积渲染，这些方法耗时较长。最近的3D高斯场投影显著提高了渲染速度，然而，现有的基于高斯的分割方法（例如高斯分组）在零样本分割中没有提供紧凑的分割掩模，主要原因是在遇到不一致的2D机器生成标签时，无法直接为每个高斯分配可学习参数，缺乏鲁棒性和紧凑性。我们的方法旨在通过使用浅层解码网络将每个高斯点的融合空间和语义上有意义的特征映射，迅速实现紧凑且可靠的零样本场景分割。

    We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
    
[^95]: 基于Split Learning的肌电假肢控制中的收敛速率最大化

    Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])

    [http://arxiv.org/abs/2401.03233](http://arxiv.org/abs/2401.03233)

    本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。

    

    Split Learning (SL)是一种有前途的分布式学习方法，可以在资源有限的环境中应用于基于肌电的假肢控制。与深度学习和联邦学习等其他学习方法相比，SL能够提供更优的解决方案，因为假肢设备在处理能力和电池寿命方面非常有限。在这些情况下，实现SL的可行性源于其固有的模型分割，其中客户端执行较小的模型部分。然而，选择不恰当的切层会阻碍SL系统的训练过程。本文提出了一种用于最大化模型收敛速率的切层选择算法。性能评估表明，所提出的算法在改善假肢控制的肌电模式识别任务中显著加速了收敛过程。

    Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
    
[^96]: AI是否能像人类一样具备创造力？

    Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])

    [http://arxiv.org/abs/2401.01623](http://arxiv.org/abs/2401.01623)

    本文引入了一个新概念【相对创造力】，通过将焦点转向AI是否能够与人类具备相同的创造能力，实现对创造力的统计量化评估和直接比较。

    

    创造力是社会进步和创新的基石，但其评估仍然是一个复杂且主观的任务。随着先进的生成型AI模型的出现，能够完成曾经只属于人类创造力的任务，探索AI的创造潜力变得至关重要，以确保其负责任的发展和应用。本文通过引入一个名为“相对创造力”的新概念来解决定义和评估创造力的复杂性。我们不再试图对创造力进行普遍定义，而是将焦点转向AI是否能够与一位假设的人类具备相同的创造能力。这种观点借鉴了图灵测试的思想，并扩展其范围以解决评估创造力中所固有的挑战和主观性。这种方法的转变使得对AI创造力的统计量化评估成为可能，我们将其称为统计创造力。这种方法允许直接比较AI与特定人类的创造能力。

    Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
    
[^97]: 通过复杂逻辑假设生成在知识图谱中推进诱导推理

    Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.15643](http://arxiv.org/abs/2312.15643)

    这篇论文介绍了一种复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。研究发现，过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。为了解决推广到未见过观察结果的问题，引入了基于知识图谱的强化学习方法（RLF-KG），最小化观察结果与结论之间的差异。

    

    诱导推理是通过做出有根据的猜测来解释观察结果的过程。尽管许多应用需要使用知识进行解释，但将诱导推理与结构化知识（如知识图谱）结合使用的方法仍然尚未得到广泛探索。为了填补这一空白，本文介绍了复杂逻辑假设生成的任务，作为实现与知识图谱的诱导逻辑推理的初始步骤。在这个任务中，我们的目标是生成一个复杂的逻辑假设，以解释一组观察结果。我们发现，经过监督训练的生成模型可以生成结构上更接近参考假设的逻辑假设。然而，当推广到未见过的观察结果时，这种训练目标并不能保证更好的假设生成。为了解决这个问题，我们引入了基于知识图谱的强化学习方法（RLF-KG），该方法最小化观察结果与结论之间的差异。

    Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusio
    
[^98]: 不是所有的任务都一样困难：具有动态深度路由的多任务深度强化学习

    Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing. (arXiv:2312.14472v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.14472](http://arxiv.org/abs/2312.14472)

    本文提出了具有动态深度路由的多任务深度强化学习框架(D2R)，该框架灵活选择不同任务所需的模块数量，并通过引入ResRouting方法和自动路由平衡机制来解决存在的问题。

    

    多任务强化学习旨在通过单个策略完成一组不同的任务。为了通过在多个任务之间共享参数来增强数据效率，一种常见的做法是将网络分割成不同的模块，并训练一个路由网络将这些模块重新组合成任务特定的策略。然而，现有的路由方法在所有任务中使用固定数量的模块，忽略不同难度的任务通常需要不同数量的知识。本文提出了一种动态深度路由 (D2R) 框架，该框架学习跳过某些中间模块，从而灵活地为每个任务选择不同数量的模块。在这个框架下，我们进一步引入了一种 ResRouting 方法，解决了离线训练过程中行为策略和目标策略之间截然不同的路由路径的问题。此外，我们设计了一种自动的路由平衡机制，鼓励对未掌握任务进行持续的路由探索。

    Multi-task reinforcement learning endeavors to accomplish a set of different tasks with a single policy. To enhance data efficiency by sharing parameters across multiple tasks, a common practice segments the network into distinct modules and trains a routing network to recombine these modules into task-specific policies. However, existing routing approaches employ a fixed number of modules for all tasks, neglecting that tasks with varying difficulties commonly require varying amounts of knowledge. This work presents a Dynamic Depth Routing (D2R) framework, which learns strategic skipping of certain intermediate modules, thereby flexibly choosing different numbers of modules for each task. Under this framework, we further introduce a ResRouting method to address the issue of disparate routing paths between behavior and target policies during off-policy training. In addition, we design an automatic route-balancing mechanism to encourage continued routing exploration for unmastered tasks 
    
[^99]: 一种用于加速RLHF训练的自适应部署和并行框架

    An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11819](http://arxiv.org/abs/2312.11819)

    提出了一种自适应模型部署和并行框架，用于加速RLHF训练。该框架提供了两种灵活的模型部署策略，其中交替策略有助于减少内存冗余和通信成本。

    

    最近，像ChatGPT或InstructGPT这样的大型语言模型（LLM）在人工智能领域产生了重大影响。许多研究尝试复现复杂的InstructGPT的训练流程，即基于人类反馈的强化学习（RLHF）。然而，主流的分布式RLHF训练方法通常采用固定的模型部署策略，称为Flattening策略。该策略将RLHF中涉及的四个相互依赖的模型视为单个实体，将它们分配到所有设备上，并应用于单个模型设计的并行技术，而不考虑每个模型固有的不同工作负载。结果，该策略加剧了RLHF训练中的生成瓶颈，并降低了整体训练效率。为了解决这些问题，我们提出了一种自适应模型部署框架，提供了两种灵活的模型部署策略。交替策略有助于减少内存冗余和通信成本。

    Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). However, the mainstream distributed RLHF training methods typically adopt a fixed model placement strategy, referred to as the Flattening strategy. This strategy treats all four interdependent models involved in RLHF as a single entity, distributing them across all devices and applying parallelism techniques designed for a single model, regardless of the different workloads inherent to each model. As a result, this strategy exacerbates the generation bottlenecks in the RLHF training and degrades the overall training efficiency. To address these issues, we propose an adaptive model placement framework that offers two flexible model placement strategies. The Interleaving strategy helps reduce memory redundancy and communication costs of 
    
[^100]: 使用基础模型进行推理的调查

    A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.11562](http://arxiv.org/abs/2312.11562)

    本文调查了使用基础模型进行推理的研究，介绍了最新的推理任务、方法和基准，并讨论了基础模型中推理能力的未来发展方向。

    

    推理是复杂问题解决的关键能力，在谈判、医学诊断和刑事调查等各种现实世界环境中起着重要作用。它在人工通用智能（AGI）领域中作为一种基本方法学。随着基础模型（如大型语言模型）的不断发展，越来越多的研究者对它们在推理任务中的能力产生了兴趣。在本文中，我们介绍了用于推理的开创性基础模型，并突出了各种推理任务、方法和基准的最新进展。我们还深入探讨了基础模型中推理能力的潜在未来发展方向，并讨论了多模式学习、自主代理和超级对齐在推理背景下的相关性。通过讨论这些未来研究方向，我们希望激发研究者们在这一领域的探索，推动进一步的发展。

    Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advance
    
[^101]: Fine-Tuning还是检索？比较在LLMs中的知识注入

    Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.05934](http://arxiv.org/abs/2312.05934)

    该研究比较了无监督的微调和检索增强生成（RAG）这两种常见方法在LLMs中的应用。结果发现，RAG在现有知识和新知识上表现出更好的性能，而LLMs通过无监督的微调学习新的事实信息较困难。

    

    大型语言模型（LLMs）在其预训练的权重中封装了大量的事实信息，正如它们能够在不同领域回答各种问题所证明的那样。然而，这种知识本质上是有限的，很大程度上依赖于训练数据的特性。因此，使用外部数据集来整合新的信息或改进LLMs在已见信息上的能力面临着重大挑战。在这个研究中，我们比较了两种常见的方法：无监督的微调和检索增强生成（RAG）。我们在不同主题的各种知识密集型任务上评估了这两种方法。我们的发现表明，虽然无监督的微调能够提供一定的改进，但RAG在现有知识和完全新知识上始终表现出更好的性能。此外，我们发现LLMs很难通过无监督的微调来学习新的事实信息，并且暴露

    Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
    
[^102]: 住宅建筑供暖的需求响应：基于物理启发神经网络的有效蒙特卡洛树搜索控制

    Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks. (arXiv:2312.03365v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2312.03365](http://arxiv.org/abs/2312.03365)

    本文重点研究了基于物理启发神经网络的蒙特卡洛树搜索控制方法在住宅建筑供暖的需求响应中的应用，该方法通过灵活的优化整合了外部约束条件，为实现能源消耗的优化和用户热舒适度的保证提供了有前景的解决方案。

    

    控制建筑物的能源消耗通过需求响应已成为减少全球碳排放和限制气候变化的越来越重要的手段。本文特别关注住宅建筑供暖系统的控制，以优化能源消耗同时保证用户的热舒适度。在这个领域，最近的研究主要集中在基于模型的控制，例如模型预测控制（MPC），或者基于模型无关的强化学习（RL）来实现实际的需求响应算法。蒙特卡洛树搜索（MCTS）是一种最近在棋盘游戏（围棋、国际象棋）等领域取得了令人印象深刻成功的RL方法。然而，在建筑控制方面，MCTS仍然被较少探索。因此，我们专门研究了MCTS在建筑需求响应方面的应用。其自然的结构允许灵活的优化，隐式地集成外部约束条件（与传统的RL解决方案相比），使MCTS成为一个有前景的候选方法。

    Controlling energy consumption in buildings through demand response (DR) has become increasingly important to reduce global carbon emissions and limit climate change. In this paper, we specifically focus on controlling the heating system of a residential building to optimize its energy consumption while respecting user's thermal comfort. Recent works in this area have mainly focused on either model-based control, e.g., model predictive control (MPC), or model-free reinforcement learning (RL) to implement practical DR algorithms. A specific RL method that recently has achieved impressive success in domains such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for building control it has remained largely unexplored. Thus, we study MCTS specifically for building demand response. Its natural structure allows a flexible optimization that implicitly integrate exogenous constraints (as opposed, for example, to conventional RL solutions), making MCTS a promising candidate for
    
[^103]: 感知组分词器：通过迭代分组构建视觉感知

    Perceptual Group Tokenizer: Building Perception with Iterative Grouping. (arXiv:2311.18296v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.18296](http://arxiv.org/abs/2311.18296)

    本研究提出了一种名为感知组分词器的模型，通过迭代分组操作来提取视觉特征并进行自我监督表示学习。实验证明，该模型能够与最先进的视觉架构相竞争，并具有自适应计算的优点。

    

    人类视觉识别系统展示了令人惊叹的能力，可以将视觉信息压缩成一组含有丰富表示的标记，而无需标签监督。其中一个关键驱动原则是感知分组。尽管在2010年代早期被广泛应用于计算机视觉领域，但感知分组能否被用来推导出生成强大表示的神经视觉识别框架仍然是一个谜。本文提出了Perceptual Group Tokenizer，这是一个完全依赖分组操作来提取视觉特征和进行自我监督表示学习的模型，其中一系列的分组操作被用来迭代性地推断像素或超像素的上下文，以改进特征表示。我们展示了所提出的模型可以达到与最先进的视觉架构相竞争的性能，并且具有自适应计算而无需重新训练的可取性质。

    Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping. Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. We show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-traini
    
[^104]: IA-LSTM: 交互感知 LSTM 用于行人轨迹预测

    IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction. (arXiv:2311.15193v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.15193](http://arxiv.org/abs/2311.15193)

    本研究提出了一种交互感知的 LSTM 模型，用于行人轨迹预测。通过引入新的基于相关熵的机制，可以衡量人与人之间交互的相对重要性，并为每个行人建立个人空间。这个数据驱动机制可以有效提取动态人与人之间的特征表示。

    

    在自动驾驶或自主移动机器人领域，预测拥挤场景中行人的轨迹是不可或缺的，因为估计周围行人的未来位置有助于决策避免碰撞。这是一个具有挑战性的问题，因为人类具有不同的行走方式，当前环境中的人与物体之间的交互，特别是人与人之间的交互是复杂的。以前的研究者关注如何建模人与人之间的交互，但忽视了交互的相对重要性。为了解决这个问题，引入了一种基于相关熵的新机制。所提出的机制不仅可以衡量人与人之间交互的相对重要性，还可以为每个行人建立个人空间。进一步提出了包括这个数据驱动机制的交互模块。在提出的模块中，数据驱动机制可以有效提取动态人与人之间的特征表示。

    Predicting the trajectory of pedestrians in crowd scenarios is indispensable in self-driving or autonomous mobile robot field because estimating the future locations of pedestrians around is beneficial for policy decision to avoid collision. It is a challenging issue because humans have different walking motions, and the interactions between humans and objects in the current environment, especially between humans themselves, are complex. Previous researchers focused on how to model human-human interactions but neglected the relative importance of interactions. To address this issue, a novel mechanism based on correntropy is introduced. The proposed mechanism not only can measure the relative importance of human-human interactions but also can build personal space for each pedestrian. An interaction module including this data-driven mechanism is further proposed. In the proposed module, the data-driven mechanism can effectively extract the feature representations of dynamic human-human 
    
[^105]: 通用短语去偏器：在多标记级别上消除掩码语言模型中的偏见

    General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.13892](http://arxiv.org/abs/2311.13892)

    本文提出了一种名为“通用短语去偏器”的自动多标记去偏管道，能够有效减轻掩码语言模型中的短语级别偏见，并在标准数据集和指标上取得了最新成果。

    

    预训练语言模型所揭示的社会偏见和不受欢迎的刻板印象正在成为其应用的障碍。与针对词级别的众多去偏方法相比，对于短语级别的偏见关注相对较少，限制了学科领域去偏的性能。在本文中，我们提出了一种名为“通用短语去偏器”的自动多标记去偏管道，能够减轻掩码语言模型中的短语级别偏见。具体而言，我们的方法包括一个“短语过滤阶段”，从维基百科页面中生成刻板印象的短语，以及一个“模型去偏阶段”，可以在多标记级别上去偏模型以应对短语上的偏见挑战。后者寻找触发模型偏见的提示，然后将其用于去偏。标准数据集和评估指标上的最新成果表明，我们的方法可以显著减少职业和加强性别偏见。

    The social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. Compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. In this paper, we propose an automatic multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which is capable of mitigating phrase-level biases in masked language models. Specifically, our method consists of a \textit{phrase filter stage} that generates stereotypical phrases from Wikipedia pages as well as a \textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. The latter searches for prompts that trigger model's bias, and then uses them for debiasing. State-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and 
    
[^106]: AGI系统的元提示

    Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.11482](http://arxiv.org/abs/2311.11482)

    本文全面研究了元提示技术，这是一种创新方法，重塑了大型语言模型、多模态模型和人工智能系统在问题解决和数据解释方面的应用。通过强调信息的结构和句法，元提示将复杂问题拆解为简单的子问题，提高了效率，并且能够与少样本方法进行公平的比较。同时，本文还提出了元提示用于自动生成提示的方法。

    

    本文介绍了元提示(meta prompting)的全面研究，这是一种创新技术，重新塑造了大型语言模型(LLMs)、多模态基础模型和人工智能系统在问题解决和数据解释方面的利用。基于类型理论和范畴论，元提示注重信息的结构和句法，而不是传统以内容为中心的方法。本文探讨了元提示的形式定义，并将其与少样本提示(few-shot prompting)区分开来，并强调其在各种人工智能应用中的有效性。重点关注将元提示扩展到复杂推理任务上，展示如何将复杂问题拆分成较为简单的子问题，提高令牌效率，并使问题求解的比较更加公平，尤其是与少样本示例方法相比。此外，本文还引入了元提示用于提示任务，允许LLMs以迭代的元编程形式自动生成新的提示。

    This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
    
[^107]: 2D-RC: 二维神经网络方法用于OTFS符号检测

    2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection. (arXiv:2311.08543v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2311.08543](http://arxiv.org/abs/2311.08543)

    这篇论文介绍了一种新颖的二维神经网络方法，用于高运动性场景下无线通信中的OTFS符号检测。该方法将OTFS系统的领域知识与在线子帧符号检测的设计结合起来，并通过引入二维循环填充和滤波结构来实现。

    

    正交时频空间（OTFS）是高运动性场景下无线通信的一种有前途的调制方案。最近，一种基于储层计算（RC）的方法已经引入到OTFS系统中的在线子帧符号检测中，该方法只利用了有限数量的空中（OTA）导频符号用于训练。然而，这种方法没有充分利用OTFS系统特定的领域知识，无法充分发挥RC的潜力。本文介绍一种新颖的二维RC（2D-RC）方法，将OTFS系统的领域知识结合到在线子帧符号检测的设计中。具体而言，由于延迟-多普勒（DD）域中的信道相互作用是一个二维（2D）的循环操作，2D-RC被设计为具有2D循环填充过程和2D滤波结构以嵌入此知识。通过引入这种架构，2D-RC可以在DD域中仅使用一个单一的导频符号操作。

    Orthogonal time frequency space (OTFS) is a promising modulation scheme for wireless communication in high-mobility scenarios. Recently, a reservoir computing (RC) based approach has been introduced for online subframe-based symbol detection in the OTFS system, where only a limited number of over-the-air (OTA) pilot symbols are utilized for training. However, this approach does not leverage the domain knowledge specific to the OTFS system to fully unlock the potential of RC. This paper introduces a novel two-dimensional RC (2D-RC) method that incorporates the domain knowledge of the OTFS system into the design for symbol detection in an online subframe-based manner. Specifically, as the channel interaction in the delay-Doppler (DD) domain is a two-dimensional (2D) circular operation, the 2D-RC is designed to have the 2D circular padding procedure and the 2D filtering structure to embed this knowledge. With the introduced architecture, 2D-RC can operate in the DD domain with only a sing
    
[^108]: 利用大型语言模型进行集体决策

    Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.04928](http://arxiv.org/abs/2311.04928)

    本论文提出了一种利用大型语言模型（LLM）促进集体决策的系统，通过管理对话和平衡个人偏好来提供满足成员需求的选项，实现高效协调并不断优化系统性能。

    

    在各种工作环境中，如会议安排、合作和项目规划中，集体决策是必不可少的，但由于个体偏好多样性、工作焦点不同和成员之间的权力动态等因素，常常具有挑战性。为了解决这个问题，我们提出了一种利用大型语言模型（LLM）来促进群体决策的系统，通过管理对话和平衡个人偏好来实现。我们的系统旨在从对话中提取个体偏好，并提出满足成员偏好的选项。我们特别将此系统应用于企业会议安排。我们利用LLM创建了合成员工配置文件，并模拟了大规模的对话，通过利用LLM评估系统表现来作为开展用户研究的新方法。我们的结果表明，系统能实现成员与LLM系统之间的高效协调，并随着时间的推移对其提出的选项进行改进和完善，确保优化系统性能。

    In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from conversations and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that
    
[^109]: 上下文置信度和生成型人工智能

    Contextual Confidence and Generative AI. (arXiv:2311.01193v1 [cs.AI])

    [http://arxiv.org/abs/2311.01193](http://arxiv.org/abs/2311.01193)

    本文讨论了在面对生成型人工智能对上下文置信度的挑战时，采取的两类策略：遏制策略和动员策略。

    

    生成型人工智能模型扰乱了有效人际沟通的基础，给上下文置信度带来新的挑战，使参与者难以确定沟通的真实上下文，并且保护沟通不被在意图之外的环境中重复使用和重组。本文描述了旨在在面对这些挑战时稳定沟通的策略-工具、技术和政策。我们讨论的策略可分为两个大类。遏制策略旨在在当前被威胁上下文自由的期望和规范的环境中重新确定上下文-这是对互联网所建立的无上下文期望和规范的一种反应。相反，动员策略将生成型人工智能的崛起视为在媒体沟通中主动建立隐私和真实性新期望的机会。

    Generative AI models perturb the foundations of effective human communication. They present new challenges to contextual confidence, disrupting participants' ability to identify the authentic context of communication and their ability to protect communication from reuse and recombination outside its intended context. In this paper, we describe strategies--tools, technologies and policies--that aim to stabilize communication in the face of these challenges. The strategies we discuss fall into two broad categories. Containment strategies aim to reassert context in environments where it is currently threatened--a reaction to the context-free expectations and norms established by the internet. Mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication.
    
[^110]: 基于迁移学习的EMR数据集之间数据分布变化的预后预测模型

    A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])

    [http://arxiv.org/abs/2310.07799](http://arxiv.org/abs/2310.07799)

    本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。

    

    由于对新兴疾病的信息有限，症状很难被察觉和认识到，因此可能忽视临床干预的窗口。期望能够建立一个有效的预后模型，辅助医生进行正确诊断和制定个性化治疗方案，从而及时预防不利结果。然而，在疾病早期阶段，由于数据收集和临床经验有限，再加上对隐私和伦理的考虑，导致可供参考的数据受限，甚至难以正确标记数据标签。此外，不同疾病或同一疾病不同来源的电子医疗记录（EMR）数据可能存在严重的跨数据集特征不匹配问题，严重影响深度学习模型的效率。本文介绍了一种迁移学习方法，建立一个从源数据集到目标数据集的过渡模型，通过对特征进行约束，来解决数据分布变化的问题。

    Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
    
[^111]: 通过同时学习面部标志检测、域分离和重建来提高面部动作单位检测的精度

    Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05207](http://arxiv.org/abs/2310.05207)

    本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。

    

    最近，如何将大量的在野非标记面部图像引入监督式面部动作单位（AU）检测框架中成为一个具有挑战性的问题。本文提出了一种新的AU检测框架，通过共享同构面部提取模块的参数，引入多任务学习，同时学习AU域分离和重建以及面部标志检测。另外，我们提出了一种基于对比学习的新特征对齐方案，通过简单的投影器和改进的对比损失添加了四个额外的中间监督器来促进特征重建的过程。在两个基准测试上的实验结果表明，我们在野外AU检测方面优于现有的方法。

    Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
    
[^112]: 用开放数据驱动的团队推荐促进研究合作

    Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09404](http://arxiv.org/abs/2309.09404)

    通过利用开放数据和人工智能方法，我们设计了一个系统来推荐团队，使得每个团队能够满足项目要求的技能覆盖，并且平衡候选成员之间的工作分配。

    

    团队建设和促进合作是非常常见的商业活动。一个例子就是TeamingForFunding问题，研究机构和研究人员在向资助机构申请时，希望能够找到合作机会以回应后者的项目申请。我们描述了一个新颖的系统，利用各种人工智能方法来推荐团队，使得每个团队都能够达到机会要求的最高技能覆盖，并且候选成员之间的工作分配是平衡的。我们通过提取开放数据中的项目申请（需求）和研究人员简介（供给）中的技能潜力，使用分类法对其进行归一化，创建了有效的算法来匹配需求和供给。我们创建团队以最大化一个新的衡量短期和长期目标的度量的优势。我们定量验证了我们算法的成功，通过…

    Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by e
    
[^113]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^114]: 临时归纳路径神经网络用于时间知识图推理

    Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])

    [http://arxiv.org/abs/2309.03251](http://arxiv.org/abs/2309.03251)

    本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。

    

    时间知识图（TKG）是传统知识图（KG）的扩展，融入了时间维度。在TKGs上进行推理是一个关键任务，旨在基于历史事件预测未来事实。关键挑战在于揭示历史子图和时间模式中的结构依赖关系。大多数现有方法依靠实体建模来模拟TKGs，因为图中的节点在知识表示中起着至关重要的作用。然而，现实场景通常涉及大量实体，并且随着时间的推移会出现新实体。这使得依赖于实体的方法很难应对大量实体，并且有效处理新出现的实体也成为一个重要的挑战。因此，我们提出了一种临时归纳路径神经网络（TiPNN），它以实体独立的角度对历史信息进行建模。具体而言，TiPNN采用了一个统一的图，名为历史时间图，来建模历史信息，并通过临时归纳路径提取结构和时间信息。

    Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
    
[^115]: 用于稀疏深度神经网络训练的多目标优化

    Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])

    [http://arxiv.org/abs/2308.12243](http://arxiv.org/abs/2308.12243)

    这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。

    

    在深度学习的各种场景中，会自然地出现不同的冲突优化准则。这些准则可以解决不同的主任务（如多任务学习设置），也可以解决主要任务和次要任务，例如损失最小化与稀疏性。通常的方法是简单地加权准则，但在凸设置中才有效。本文提出了一种多目标优化算法，对多任务深度神经网络（DNNs）进行训练，使用改进的加权Chebyshev标量化方法。通过使用这种标量化技术，算法可以识别原始问题的所有最优解，同时将其复杂性降低为一系列单目标问题。然后，使用增广Lagrangian方法来解决简化后的问题，从而可以使用常见的优化技术，如Adam和随机梯度下降，同时有效地处理约束条件。我们的工作旨在解决经济化问题。

    Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
    
[^116]: 使用语言模型进行算术运算：从记忆到计算

    Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])

    [http://arxiv.org/abs/2308.01154](http://arxiv.org/abs/2308.01154)

    本研究探索了使用语言模型进行算术计算的能力，发现语言模型可以通过内部的值空间进行计算，并取得了成功的实验结果。

    

    更好地理解最近的大型语言模型的出现性计算和问题解决能力对于进一步改进它们并拓宽其适用性至关重要。本研究探讨了一个训练用于预测下一个标记的语言模型如何在训练数据之外执行算术计算。二进制加法和乘法是一个很好的测试基础，因为它们需要一个非常小的词汇表，并且在输入/输出上展示了相关的不连续性，使得对新数据进行平滑的输入插值无效。我们成功地训练了一个轻量级的语言模型来学习这些任务，并进行了一系列实验证明其外推能力和内部信息处理。我们的研究结果支持这样一个假设，即语言模型作为一个编码-回归-解码机器，一旦将输入标记表示映射到合适的内部值空间，计算就在值空间中进行。

    A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate intern
    
[^117]: KoBBQ: 针对问答的韩国偏见基准

    KoBBQ: Korean Bias Benchmark for Question Answering. (arXiv:2307.16778v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.16778](http://arxiv.org/abs/2307.16778)

    本文介绍了KoBBQ，一个针对韩国文化的偏见基准数据集，提出了一个通用框架来解决数据集的文化适应性问题，并通过大规模调查收集和验证了反映韩国文化刻板印象的社会偏见和偏见目标。

    

    偏见问答的基准（BBQ）旨在评估语言模型（LMs）的社会偏见，但是将此基准适应于美国以外的文化背景并不简单，因为社会偏见在很大程度上取决于文化背景。本文介绍了KoBBQ，一个韩国偏见基准数据集，并提出了一个通用框架来解决数据集的文化适应性问题。我们的框架将BBQ数据集分为三类——简单转换（可以在文化翻译后直接使用）、目标修改（需要在目标群体中进行本地化）和样本删除（不适合韩国文化），并添加了四个针对韩国文化特定的偏见类别。我们进行了大规模调查，收集和验证了反映韩国文化刻板印象的社会偏见和偏见目标。最终的KoBBQ数据集包含了268个模板和76,048个样本，涵盖了12个社会分类。

    The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes--Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)-- and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social b
    
[^118]: 无监督条件槽注意力用于物体中心学习

    Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])

    [http://arxiv.org/abs/2307.09437](http://arxiv.org/abs/2307.09437)

    本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。

    

    提取物体层次的表示以进行后续的推理任务是人工智能中涌现的一个领域。在无监督的设置中学习物体中心表示面临多个挑战，其中一个关键挑战是将任意数量的物体实例绑定到专门的物体槽位。最近的物体中心表示方法如槽位注意力利用迭代式注意力学习具有动态推理层级绑定的可组合表示，但未能达到专门的槽位绑定。为了解决这个问题，本文提出了一种使用新颖的概率性槽字典（PSD）的无监督条件槽注意力。我们将PSD定义为（i）抽象的物体层次属性向量作为键，（ii）参数化高斯分布作为相应的值。我们在多个后续任务中展示了学习到的具体物体层次调节分布的好处，包括物体发现、组合式场景生成和组合式视觉推理。

    Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
    
[^119]: 异构传感器网络中的异常检测的动态图注意力

    Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])

    [http://arxiv.org/abs/2307.03761](http://arxiv.org/abs/2307.03761)

    该论文提出了一种基于图的方法，命名为DyGATAD，用于在异构传感器网络中进行异常检测。该方法利用动态图注意力机制来识别集体异常行为，其中异常行为可能由于系统内部相互关系的变化引起。这在工业物联网监控系统中具有重要的实际应用价值。

    

    在数字化转型的时代，由工业物联网 (IIoT) 监控的系统通过异构传感器网络生成大量的多元时间序列 (MTS) 数据。虽然这些数据有助于条件监控和异常检测，但是传感器网络中日益复杂和相互依赖的关系也给异常检测带来了重大挑战。尽管在这个领域取得了一些进展，但主要集中在点异常和背景异常，对集体异常的关注较少。集体异常的一种常见变种是异常集体行为由系统内部的相互关系变化引起。这可能是由于异常环境条件（如过热）、由于网络攻击造成的不正确操作设置或系统级故障引起的。为了解决这些挑战，本文提出了 DyGATAD（一种动态图注意力的异常检测方法），采用基于图的方法来识别传感器网络中的异常行为。

    In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
    
[^120]: 逼真的合成金融交易用于反洗钱模型

    Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])

    [http://arxiv.org/abs/2306.16424](http://arxiv.org/abs/2306.16424)

    本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。

    

    随着金融的广泛数字化和加密货币的日益流行，网络犯罪分子设计的欺诈方案越来越复杂。洗钱——将非法资金移动以掩盖其来源——可以跨越银行和国界，产生复杂的交易模式。联合国估计每年全球洗钱金额占全球GDP的2-5%，约为0.8-2.0万亿美元。不幸的是，通常无法获得用于训练机器学习模型来检测洗钱的真实数据，且之前的合成数据生成器存在显著缺陷。为了比较模型并推进该领域的发展，需要一个逼真、标准化、公开可用的基准数据集。为此，本文提出了一种合成金融交易数据集生成器和一组合成的反洗钱数据集。我们根据实际交易尽可能地校准了这个基于代理的生成器。

    With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
    
[^121]: 用于具有不完整标注的文档级关系抽取的正负样本度量学习框架

    A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling. (arXiv:2306.14806v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14806](http://arxiv.org/abs/2306.14806)

    我们提出了一种正负样本度量学习框架（P3M）用于具有不完整标注的文档级关系抽取，通过拉近实体嵌入和其对应关系嵌入的距离，同时使其与非类别关系嵌入的距离推远，以提高模型的泛化能力。

    

    文档级关系抽取的目标是识别跨多个句子的实体之间的关系。最近，文档级关系抽取中的不完整标注问题引起了越来越多的关注，一些研究采用正负样本学习等方法来解决这个问题，但仍有很大的改进空间。受此启发，我们提出了一种正样本增强和正样本混合的正负样本度量学习框架（P3M）。具体而言，我们将文档级关系抽取形式化为度量学习问题。我们旨在拉近实体对嵌入和它们对应的关系嵌入之间的距离，同时将它们与非类别关系嵌入的距离推远。此外，我们将正负样本学习方法应用于该损失目标。为了提高模型的泛化能力，我们使用了dropout来增加正样本，并提出了正-非类别混合方法。大量实验证明了P3M的改进效果。

    The goal of document-level relation extraction (RE) is to identify relations between entities that span multiple sentences. Recently, incomplete labeling in document-level RE has received increasing attention, and some studies have used methods such as positive-unlabeled learning to tackle this issue, but there is still a lot of room for improvement. Motivated by this, we propose a positive-augmentation and positive-mixup positive-unlabeled metric learning framework (P3M). Specifically, we formulate document-level RE as a metric learning problem. We aim to pull the distance closer between entity pair embedding and their corresponding relation embedding, while pushing it farther away from the none-class relation embedding. Additionally, we adapt the positive-unlabeled learning to this loss objective. In order to improve the generalizability of the model, we use dropout to augment positive samples and propose a positive-none-class mixup method. Extensive experiments show that P3M improve
    
[^122]: 系统级自然语言反馈

    System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])

    [http://arxiv.org/abs/2306.13588](http://arxiv.org/abs/2306.13588)

    本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了通过任务度量设计和语言模型提示设计，如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型，并展示了使用系统级别反馈和实例级别反馈的有效性。

    

    自然语言反馈包含了丰富的用户体验信息。现有研究聚焦于实例级别的方法，即将反馈用于细化特定例子，而忽略了其系统范围的应用。本文提出了一个通用框架，用于解锁系统级别使用自然语言反馈的方法。我们展示了如何使用反馈在人工交互流程中形式化系统级别的设计决策，以便产生更好的模型。具体而言，这是通过以下两方面实现的：(i) 任务度量设计; (ii) 用于改进模型响应的语言模型提示设计。我们进行了两项案例研究，来改进搜索查询生成和对话响应生成，展示了使用系统级别反馈的有效性。我们表明系统级别反馈和实例级别反馈的组合带来了进一步的收益，并且由人类撰写的实例级别反馈导致比GPT-3.5撰写的反馈更加扎实。

    Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
    
[^123]: 统一大型语言模型和知识图谱: 一条路线图

    Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08302](http://arxiv.org/abs/2306.08302)

    本文提出了一个前瞻性的统一大型语言模型和知识图谱的路线图，通过三个框架：增强KGs的LLMs，知识增强KGs和LLMs与KGs的联合推理，综合利用两者的优点。

    

    大型语言模型（LLM）如ChatGPT和GPT4正在自然语言处理和人工智能领域掀起新的热潮，由于它们的突现能力和一般化能力。然而，LLM是黑盒模型，往往不能捕捉和获取实际知识。相比之下，知识图谱（KGs）如维基百科和华普则是明确存储丰富实际知识的结构化知识模型。KGs可以通过为推理和可解释性提供外部知识来增强LLMs。同时，KGs的构建困难，自然而然地演化，这挑战了现有的KGs方法来生成新事实并表示未见过的知识。因此，统一LLMs和KGs并同时利用它们的优点是有益的。本文介绍了一个前瞻性的统一LLMs和KGs的路线图。我们的路线图包括三个一般框架，即1）增强KGs的LLMs，它们将知识表示为LM的一部分，从而能够捕捉丰富的实体关系，2）知识增强KGs，它们将LLMs用作知识表示学习的优秀工具，3）LLMs与KGs的联合推理，其中LLMs和KGs相互增强，从而获得更准确的推理模型。

    Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorpo
    
[^124]: GateON: 一种用于大规模连续学习的无监督方法

    GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])

    [http://arxiv.org/abs/2306.01690](http://arxiv.org/abs/2306.01690)

    GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。

    

    连续学习（CL）的目标是在不对早期任务进行重新训练的情况下按顺序学习任务。然而，传统的神经网络在经过CL训练后会出现灾难性遗忘和有限的泛化能力。为了克服这些问题，我们引入了一种新的方法，称为'Gate and Obstruct Network'（GateON）。GateON将可学习的活动门控与参数相关性的在线估计相结合，以防止重要知识被覆盖。我们的方法在任务之间生成部分重叠的路径，允许在顺序学习过程中进行正向和反向转移。GateON通过定点神经元的重新激活机制来解决参数固定后网络饱和的问题，实现了大规模连续学习。GateON适用于各种网络（全连接、CNN、Transformers），计算复杂度低，有效地学习了高达100个MNIST学习任务，并在预训练BERT中取得了顶尖结果。

    The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
    
[^125]: 低资源语音翻译的跨语言迁移学习

    Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00789](http://arxiv.org/abs/2306.00789)

    提出了一种三步跨语言迁移学习框架，通过在现有框架中增加一步语义知识蒸馏，该方法有效地增强了自动语音翻译中从高资源语言到低资源语言的跨语言迁移能力，显著改善了翻译性能，特别是对于低资源语言，并减少了跨语言迁移间隙(TRFGap)。

    

    本文提出了一种新颖的三步跨语言迁移学习框架，用于增强自动语音翻译中从高资源语言到低资源语言的跨语言迁移能力。该方法将语义知识蒸馏步骤集成到现有的两步跨语言迁移学习框架XLS-R中。这一额外的步骤旨在通过使用无标签语音进行自监督学习来对多语言语音编码器进行预训练以编码语义知识。我们提出的三步跨语言迁移学习框架解决了XLS-R框架中高资源语言和低资源语言之间存在的大的跨语言迁移差距。我们通过在CoVoST-2基准测试上进行广泛实验和比较来验证我们的提议，结果显示在翻译性能方面取得了显著改进，特别是对于低资源语言，并且跨语言迁移间隙(TRFGap)有明显减少。

    The paper presents a novel three-step transfer learning framework for enhancing cross-lingual transfer from high- to low-resource languages in the downstream application of Automatic Speech Translation. The approach integrates a semantic knowledge-distillation step into the existing two-step cross-lingual transfer learning framework XLS-R. This extra step aims to encode semantic knowledge in the multilingual speech encoder pre-trained via Self-Supervised Learning using unlabeled speech. Our proposed three-step cross-lingual transfer learning framework addresses the large cross-lingual transfer gap (TRFGap) observed in the XLS-R framework between high-resource and low-resource languages. We validate our proposal through extensive experiments and comparisons on the CoVoST-2 benchmark, showing significant improvements in translation performance, especially for low-resource languages, and a notable reduction in the TRFGap.
    
[^126]: 基于Transformer的零样本和少样本生物医学命名实体识别方法

    A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])

    [http://arxiv.org/abs/2305.04928](http://arxiv.org/abs/2305.04928)

    本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。

    

    在生物医学领域中，有监督的命名实体识别（NER）依赖于具有给定命名实体的大量注释文本，其创建可能耗时且昂贵。此外，提取新实体通常需要进行额外的注释任务和重新训练模型。为解决这些挑战，本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。该方法基于将多类标记分类任务转换为二元标记分类（标记包含搜索的实体或不包含搜索的实体），并在更多的数据集和生物医学实体上进行预训练，从而可学习到给定和潜在类别之间的语义关系。在9种不同的生物医学实体上，我们在零样本NER、一次样本NER、10次样本NER和100次样本NER上实现了平均F1得分分别为35.44％、50.10％、69.94％和79.51％。

    Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
    
[^127]: MAE预前置训练对于亿级预训练的有效性

    The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])

    [http://arxiv.org/abs/2303.13496](http://arxiv.org/abs/2303.13496)

    本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。

    

    本文重新审视了计算机视觉中用于视觉识别任务的标准预训练-微调范式。通常情况下，最先进的基础模型使用数十亿张图像的大规模（弱）监督数据集进行预训练。我们引入了一个额外的预前置训练阶段，它使用了自我监督的MAE技术来初始化模型。虽然MAE技术仅被证明能够与模型大小相缩放，但我们发现它也可以随数据集大小缩放。因此，我们基于MAE的预前置训练可同时适用于训练基础模型的模型和数据规模。预前置训练在一系列模型规模（参数数百万到数十亿）和数据集大小（图像数百万到数十亿）上一致提高了模型收敛性和下游转移性能，且我们还测量了其在10个不同的视觉识别任务上的有效性，包括图像分类、视频识别和目标检测。

    This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
    
[^128]: 旋转不变量量化用于模型压缩

    Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03106](http://arxiv.org/abs/2303.03106)

    本研究提出了一种旋转不变量量化（RIQ）技术，可以在不同层次上实现混合精度量化，用于后训练神经网络模型压缩，并证明了其在压缩方面的优势。在多种模型和任务上进行了严格评估，取得了令人满意的结果。

    

    后训练神经网络（NN）模型压缩是一种将大型、消耗内存的模型部署到内存资源有限设备上的吸引人的方法。本研究探讨了NN模型压缩的速率-失真权衡。首先，我们提出了一种旋转不变量量化（RIQ）技术，它利用一个单一参数量化整个NN模型，在每个层次上得到不同的速率，即混合精度量化。然后，我们证明了我们的旋转不变量方法在压缩方面的优势。我们对RIQ进行了严格评估，并展示了它在各种模型和任务上的能力。例如，RIQ在预训练的VGG稠密和修剪模型上分别实现了19.4倍和52.9倍的压缩比，精度降低小于0.4%。代码可以在\url{https://github.com/ehaleva/RIQ}上找到。

    Post-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources. In this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates $\times 19.4$ and $\times 52.9$ compression ratios on pre-trained VGG dense and pruned models, respectively, with $<0.4\%$ accuracy degradation. Code is available in \url{https://github.com/ehaleva/RIQ}.
    
[^129]: 私密、公平且精确：在医学影像中训练大规模隐私保护的人工智能模型

    Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.01622](http://arxiv.org/abs/2302.01622)

    本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。

    

    人工智能模型在医学领域的应用越来越多。然而，由于医学数据的高度敏感性，需要采取特殊措施确保其保护。保护隐私的黄金标准是引入差分隐私（DP）来进行模型训练。先前的研究表明，DP对模型的准确性和公平性有负面影响，这在医学中是不可接受的，并且是隐私保护技术广泛应用的主要障碍。在这项工作中，我们评估了隐私保护训练人工智能模型对准确性和公平性的影响，与非隐私训练进行了比较。为此，我们使用了两个数据集：（1）一个大规模数据集（N=193,311）的高质量临床胸部X射线图像，和（2）一个数据集（N=1,625）的3D腹部计算机断层扫描（CT）图像，用于分类胰腺导管腺癌（PDAC）的存在。两个数据集均为回顾性采集，并由经验丰富的医学影像专家进行手动标注。

    Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
    
[^130]: 基于GNN的乘客请求预测

    GNN-based Passenger Request Prediction. (arXiv:2301.02515v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02515](http://arxiv.org/abs/2301.02515)

    本文开发了一个基于图神经网络和注意力机制的模型来预测乘客的起点-终点（OD）流。该模型利用各种线性和非线性依赖关系，并捕捉重复模式和上下文数据。通过广泛的仿真实验，结果显示我们的模型性能优于现有基线模型。

    

    乘客请求预测对于乘车共享平台的运营规划、控制和管理至关重要。虽然需求预测问题已得到广泛研究，但乘客的起点-终点（OD）流预测却在研究界中受到较少关注。本文开发了一个基于图神经网络框架和注意力机制的模型来预测乘客的OD流。该模型利用不同位置起始的请求之间产生的各种线性和非线性依赖关系，并捕捉了该地点的重复模式和上下文数据。此外，确定了覆盖道路网络并保持模型复杂性和准确性的网格单元的最佳大小。通过广泛的仿真来检查我们提出的方法及其各个组件的特征。结果显示我们提出的模型相比现有基线具有更优越的性能。

    Passenger request prediction is essential for operations planning, control, and management in ride-sharing platforms. While the demand prediction problem has been studied extensively, the Origin-Destination (OD) flow prediction of passengers has received less attention from the research community. This paper develops a Graph Neural Network framework along with the Attention Mechanism to predict the OD flow of passengers. The proposed framework exploits various linear and non-linear dependencies that arise among requests originating from different locations and captures the repetition pattern and the contextual data of that place. Moreover, the optimal size of the grid cell that covers the road network and preserves the complexity and accuracy of the model is determined. Extensive simulations are conducted to examine the characteristics of our proposed approach and its various components. The results show the superior performance of our proposed model compared to the existing baselines.
    
[^131]: HyperSound：利用超网络生成音频信号的隐式神经表示

    HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks. (arXiv:2211.01839v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.01839](http://arxiv.org/abs/2211.01839)

    该论文介绍了一种名为HyperSound的方法，利用超网络生成音频信号的隐式神经表示。与其他模型相比，该方法在重构声波方面具有可比较的质量。

    

    隐式神经表示（INRs）是一个快速发展的研究领域，提供了多媒体信号的替代表示方式。最近，INRs的应用包括图像超分辨率、高维信号压缩或3D渲染等。然而，这些解决方案通常集中在视觉数据上，将它们适应到音频领域并不容易。此外，这需要为每个数据样本单独训练模型。为了解决这个限制，我们提出了HyperSound，一种利用超网络实现在训练时未见过的音频信号的INRs的元学习方法。我们展示了我们的方法可以重构与其他最先进模型相媲美的声波。

    Implicit neural representations (INRs) are a rapidly growing research field, which provides alternative ways to represent multimedia signals. Recent applications of INRs include image super-resolution, compression of high-dimensional signals, or 3D rendering. However, these solutions usually focus on visual data, and adapting them to the audio domain is not trivial. Moreover, it requires a separately trained model for every data sample. To address this limitation, we propose HyperSound, a meta-learning method leveraging hypernetworks to produce INRs for audio signals unseen at training time. We show that our approach can reconstruct sound waves with quality comparable to other state-of-the-art models.
    
[^132]: 在网络中学习异质干预下的个体治疗效应

    Learning Individual Treatment Effects under Heterogeneous Interference in Networks. (arXiv:2210.14080v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14080](http://arxiv.org/abs/2210.14080)

    本文针对网络观测数据中个体治疗效应的估计问题，提出了一种新颖的双加权回归（DWR）算法，通过同时学习注意权重来解决网络干扰和异质干预的挑战。

    

    近年来，从网络观测数据中估计个体治疗效应引起了人们越来越多的关注。在网络场景中，一个主要挑战是稳定单元治疗价值假设（SUTVA）的违反，该假设认为一个单位的治疗分配不会影响其他人的结果。在网络数据中，由于干扰，一个单位的结果不仅受到其治疗的影响（即直接效应），还受到其他人的治疗影响（即溢出效应）。此外，其他单位的影响始终是异质的（例如，与兴趣相似的朋友对一个人的影响与兴趣不同的朋友不同）。本文针对在异质干扰下估计个体治疗效应（包括直接效应和溢出效应）的问题，我们提出了一种新颖的双加权回归（DWR）算法，通过同时学习捕捉异质干预的注意权重来解决这个问题。

    Estimates of individual treatment effects from networked observational data are attracting increasing attention these days. One major challenge in network scenarios is the violation of the stable unit treatment value assumption (SUTVA), which assumes that the treatment assignment of a unit does not influence others' outcomes. In network data, due to interference, the outcome of a unit is influenced not only by its treatment (i.e., direct effects) but also by others' treatments (i.e., spillover effects). Furthermore, the influences from other units are always heterogeneous (e.g., friends with similar interests affect a person differently than friends with different interests). In this paper, we focus on the problem of estimating individual treatment effects (both direct and spillover effects) under heterogeneous interference. To address this issue, we propose a novel Dual Weighting Regression (DWR) algorithm by simultaneously learning attention weights that capture the heterogeneous int
    
[^133]: 证明了分布式风险敏感强化学习与风险敏感强化学习之间的可证明遗憾上界

    Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds. (arXiv:2210.14051v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14051](http://arxiv.org/abs/2210.14051)

    本论文证明了使用分布式强化学习方法可以实现风险敏感强化学习的遗憾保证问题，并提出了两种新颖算法，其遗憾上界与先前方法相匹配。

    

    本论文研究了使用分布式强化学习方法对风险敏感强化学习（RSRL）的遗憾保证问题。具体而言，我们考虑了目标为回报的熵风险测度（EntRM）的有限情节马尔可夫决策过程。通过利用EntRM的一个关键属性，独立性属性，我们建立了风险敏感分布式动态规划框架。然后，我们提出了两种新颖的分布式强化学习算法，通过两种不同的方案实现了乐观性，包括基于模型和无模型的方法。我们证明了这两种算法都达到了$\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$的遗憾上界，其中$S$，$A$，$K$和$H$分别表示状态的数量，动作的数量，情节的数量和时间的长度。这与\cite{fei2021exponential}中提出的RSVI2相一致，并进行了新颖的分布式分析。据我们所知，这是第一个以样本复杂度方向将分布式强化学习和风险敏感强化学习联系起来的遗憾分析。

    We study the regret guarantee for risk-sensitive reinforcement learning (RSRL) via distributional reinforcement learning (DRL) methods. In particular, we consider finite episodic Markov decision processes whose objective is the entropic risk measure (EntRM) of return. By leveraging a key property of the EntRM, the independence property, we establish the risk-sensitive distributional dynamic programming framework. We then propose two novel DRL algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.  We prove that they both attain $\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$ regret upper bound, where $S$, $A$, $K$, and $H$ represent the number of states, actions, episodes, and the time horizon, respectively. It matches RSVI2 proposed in \cite{fei2021exponential}, with novel distributional analysis. To the best of our knowledge, this is the first regret analysis that bridges DRL and RSRL in terms of sampl
    
[^134]: 解释、公正性和人类-AI决策中的适当依赖（arXiv:2209.11812v3 [cs.HC] UPDATED）

    Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2209.11812](http://arxiv.org/abs/2209.11812)

    本研究探讨了基于特征的解释对AI辅助决策公正性的影响，发现解释可以影响公正性感知和人们对AI建议的依赖。然而，解释并不能帮助人们区分正确和错误的AI建议。这些发现对于促进有效的决策和公正性至关重要。

    

    在这项工作中，我们研究了基于特征的解释对AI辅助决策的公正性的影响，特别关注从简短的文本简介中预测职业的任务。我们还研究了这些影响如何通过人们的公正性感知和对AI建议的依赖来调节。我们的研究结果表明，解释影响了公正性感知，而公正性感知又与人们遵循AI建议的倾向相关。然而，我们发现这样的解释并不能让人们区分正确和错误的AI建议。相反，我们发现解释可能会影响依赖，而不论AI建议的正确性如何。取决于解释突出的特征，这可以促进或阻碍分配公正性：当解释突出与敏感属性明显相关的与任务无关的特征时，这会促使人们覆盖AI与性别刻板印象一致的建议。

    In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans' fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanw
    
[^135]: 关于非二进制定性概率网络中错误推理的一点说明

    A note on incorrect inferences in non-binary qualitative probabilistic networks. (arXiv:2208.09344v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.09344](http://arxiv.org/abs/2208.09344)

    本文指出了非二进制定性概率网络中的错误推理问题，并提供了示例和解决方案讨论。

    

    定性概率网络（QPNs）将贝叶斯网络的条件独立性假设与正负依赖性的定性特性相结合。它们形式化了正依赖性的各种直观特性，以便在大型变量网络中进行推理。然而，在本文中，我们将证明由于一个不正确的对称性质，在非二进制QPNs中得到的许多推理在数学上不成立。我们将提供一些此类错误推理的例子，并简要讨论可能的解决方案。

    Qualitative probabilistic networks (QPNs) combine the conditional independence assumptions of Bayesian networks with the qualitative properties of positive and negative dependence. They formalise various intuitive properties of positive dependence to allow inferences over a large network of variables. However, we will demonstrate in this paper that, due to an incorrect symmetry property, many inferences obtained in non-binary QPNs are not mathematically true. We will provide examples of such incorrect inferences and briefly discuss possible resolutions.
    
[^136]: 基于多样性保持的图结构细化的图表示学习

    Graph Representation Learning via Diversity-preserving Graph Refinement. (arXiv:2103.07295v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.07295](http://arxiv.org/abs/2103.07295)

    该论文提出了一种基于多样性保持的图结构细化的图表示学习方法，它利用已学习的节点表示来逐步改善图形结构质量。在多个下游任务上，包括节点分类、链接预测和图聚类，实验表明该方法优于现有的最先进方法。

    

    对于真实的图数据，节点之间的复杂关系通常被表示为硬性二进制链接。显然，这是一种离散和简化的连续关系形式，严重限制了学习到的节点表示的可表达性。另一方面，嵌入空间中获得的节点表示可以反过来揭示节点之间的内在关系。为了更好地特征化节点关系并进一步促进节点表示的学习，一种直观的方法是使用嵌入的节点表示来细化最初给定的图结构。但是，全局细化所有节点之间的关系无法区分将不可避免地导致一些噪声边缘，这可能进一步混淆节点表示学习模型的训练。此外，大型图形上也存在可扩展性问题。为了解决这些问题，我们提出了一种局部结构感知的图形细化方法，利用已经学到的节点表示逐步改善图形结构质量。具体而言，我们首先通过对图中的随机游走模拟生成一个多样化的邻域结构集。然后，对于每个模拟邻域，我们在整个细化过程中保持邻域结构的多样性，同时细化邻域内的节点关系。在各种基准数据集上进行的评估实验结果表明，所提出的方法在多个下游任务上，包括节点分类、链接预测和图聚类，均优于现有最先进的方法。

    For real-world graph data, the complex relationship between nodes is often represented as a hard binary link. Obviously, it is a discrete and simplified form of continuous relationship between nodes, which seriously limits the expressibility of the learned node representation. On the other hand, the node representation obtained in the embedding space can in turn be used to reveal the intrinsic relationship between nodes. To better characterize the node relationships and further facilitate the learning of node representation, an intuitive way is to refine the originally given graph structure with the embedded node representations. However, such global refinement of the relationships among all nodes without distinction will inevitably lead to some noisy edges, which may further confuse the training of the node representation learning model. In addition, it also has scalability problems on large graphs. To address these issues, we propose a local structure aware graph refinement to progre
    

