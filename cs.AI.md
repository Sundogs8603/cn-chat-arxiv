# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^2] | [Federated Unlearning: a Perspective of Stability and Fairness](https://rss.arxiv.org/abs/2402.01276) | 本文研究了在数据异构性情况下的联邦取消学习（FU）的稳定性和公平性问题。我们提出了关键指标来评估FU，重点考虑了验证、全局稳定性和局部公平性，并研究了其中的权衡。通过理论分析，我们洞察了数据异构性对FU的影响，并提出了管理权衡的FU机制。通过实证验证，我们证实了这些机制的有效性。 |
| [^3] | [FAST: Factorizable Attention for Speeding up Transformers](https://arxiv.org/abs/2402.07901) | 该论文介绍了一种可以加速Transformers模型的可分解注意力机制，通过引入因子分解形式的注意力，将注意力机制的复杂度从O(N^2)降低到O(N)，并 在维持注意力矩阵完整表示的同时保持稀疏性和所有-所有令牌关系。实验证明该注意力机制具有稳健的性能，并在不同应用中具有重要潜力。 |
| [^4] | [Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets](https://arxiv.org/abs/2402.07895) | 本研究提出了一种使用机器学习方法检测拉布拉多豆上蜘蛛螨的视觉方法，通过构建RGBN数据集并使用两阶段早期病害检测模型，相比于单阶段分割模型，提高了mAP。使用RGBN数据的顺序CNN模型在分类中也取得了较高的准确率。 |
| [^5] | [MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning](https://arxiv.org/abs/2402.07890) | MAIDCRL 是一种半集中式密集神经网络强化学习算法，通过引入智能体影响图(AIMs)和卷积层，成功在 StarCraft 多智能体挑战中实现了有效的多智能体控制。CNN-enabled MAIDCRL 在学习性能上显著提高，并在复杂的异质场景中取得了更快的学习速度。 |
| [^6] | [WildfireGPT: Tailored Large Language Model for Wildfire Analysis](https://arxiv.org/abs/2402.07877) | WildfireGPT是一个针对野火分析的定制化大型语言模型，通过提供领域特定的上下文信息和科学准确性，将用户查询转化为关于野火风险的可操作见解。 |
| [^7] | [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876) | 本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。 |
| [^8] | [Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States](https://arxiv.org/abs/2402.07875) | 本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。 |
| [^9] | [Scaling Laws for Fine-Grained Mixture of Experts](https://arxiv.org/abs/2402.07871) | 本研究分析了细粒度混合专家模型的标度特性，并引入了粒度作为新的超参数，通过调整粒度可以精确控制专家的大小。研究结果显示，MoE模型在效果上始终优于密集变压器模型，并且随着模型大小和训练预算的增大，密集和MoE模型之间的效率差距也在增大。同时，将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。 |
| [^10] | [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865) | 本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。 |
| [^11] | [AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy](https://arxiv.org/abs/2402.07862) | 本研究发现，使用LLMs助手可以显著提高预测准确性，不仅仅是由于模型预测准确性的提升。 |
| [^12] | [On the Detection of Reviewer-Author Collusion Rings From Paper Bidding](https://arxiv.org/abs/2402.07860) | 本文研究了如何从论文竞标中检测勾结团体，以解决同行评审系统中的欺诈问题。 |
| [^13] | [Lissard: Long and Simple Sequential Reasoning Datasets](https://arxiv.org/abs/2402.07859) | Lissard是一个包含七个任务的基准，用于评估模型处理和生成各种序列长度的能力，需要重复的过程执行。评估结果显示随着序列复杂性增加，所有模型的性能都呈一致下降趋势。 |
| [^14] | [An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering](https://arxiv.org/abs/2402.07845) | 本研究展示了使用无监督度量模块性优化GNN进行节点聚类的方法，且无需与基准值进行比较。在设计合成实验的过程中，我们发现了这种方法的局限性。 |
| [^15] | [Understanding fitness landscapes in morpho-evolution via local optima networks](https://arxiv.org/abs/2402.07822) | 本研究通过局部最优网络分析不同编码方法在演化机器人运动任务中产生的适应度景观结构，为搜索过程提供新的启示。 |
| [^16] | [Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning](https://arxiv.org/abs/2402.07818) | 本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。 |
| [^17] | [PBADet: A One-Stage Anchor-Free Approach for Part-Body Association](https://arxiv.org/abs/2402.07814) | PBADet是一种用于部件-身体关联的单阶段无锚点方法，通过引入部件-身体中心偏移量来有效表达部件与身体之间的关系，具有高效、准确和稳健的特点，相比现有技术具有更好的性能。 |
| [^18] | [Retrieval-Augmented Thought Process as Sequential Decision Making](https://arxiv.org/abs/2402.07812) | 检索增强思维过程（RATP）通过多步决策和蒙特卡洛树搜索，以及Q值估计器，解决了大型语言模型在隐私、产生幻觉和处理长文本方面的挑战，并在处理私人数据的问答任务中实现了50%的性能提升。 |
| [^19] | [Generalising Planning Environment Redesign](https://arxiv.org/abs/2402.07799) | 本论文提出了一种广义化的规划环境重新设计方法，该方法能够不受度量影响，并能够适应不同的目标和指标。 |
| [^20] | [Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2402.07787) | 这篇论文提出了一种可扩展的多粒度融合网络（EMGF）用于基于方面的情感分析，通过整合不同的语言和结构特征，包括句法依赖、组成、注意力语义和外部知识图谱等，来提高情感分析的性能和准确性。 |
| [^21] | [End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty](https://arxiv.org/abs/2402.07772) | 本文将机器学习中的“先预测再优化”（PtO）范式扩展到具有不可微分的有序加权平均（OWA）目标的优化问题上，并展示了如何有效地将OWA函数的优化与参数推断相结合。 |
| [^22] | [Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model](https://arxiv.org/abs/2402.07757) | 该论文研究了Transformer中的逐步推理，提出了一个合成图导航模型来探索逐步推理的底层机制，并通过该模型在合成任务上的实验证明了几个关键现象的存在。 |
| [^23] | [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) | 本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。 |
| [^24] | [Towards Unified Alignment Between Agents, Humans, and Environment](https://arxiv.org/abs/2402.07744) | 本文介绍了统一对齐原则 ($\mathbf{UA}^2$)，旨在实现智能体与人类意图、环境动态和自我约束的统一对齐，提出了引入实际特性进行概念验证研究的方法。 |
| [^25] | [Model Collapse Demystified: The Case of Regression](https://arxiv.org/abs/2402.07712) | 本研究在核回归的简化环境中解析了模型崩溃现象，并发现了模型能够处理虚假数据与性能完全崩溃之间的交叉点。通过提出基于自适应正则化的策略，成功缓解了模型崩溃问题。这些发现通过实验证实。 |
| [^26] | [Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA](https://arxiv.org/abs/2402.07710) | 本论文研究了在3D点云上进行稀疏卷积的GPU优化方法，以解决点云的稀疏性和计算问题。 |
| [^27] | [Online Sequential Decision-Making with Unknown Delays](https://arxiv.org/abs/2402.07703) | 本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。 |
| [^28] | [OrderBkd: Textual backdoor attack through repositioning](https://arxiv.org/abs/2402.07689) | 本论文提出了一种通过重新定位句子中的两个单词实施文本后门攻击的方法，与已有的攻击方式相比，在攻击成功率、困惑度和与干净样本的语义相似性方面表现更好，并且对ONION防御方法具有鲁棒性。 |
| [^29] | [CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity](https://arxiv.org/abs/2402.07688) | CyberMetric是一个基准数据集，旨在评估大型语言模型在网络安全领域的知识。该数据集由10000个问题组成，通过合作过程将专家知识与LLMs相结合。除了评估LLMs的知识外，数据集的主要目标是促进人类与不同LLMs在网络安全领域中的公平比较。 |
| [^30] | [Large Language Models "Ad Referendum": How Good Are They at Machine Translation in the Legal Domain?](https://arxiv.org/abs/2402.07681) | 本研究评估了两种大型语言模型和传统神经机器翻译系统在法律领域的机器翻译质量，结果显示大型语言模型在产生上下文足够且流畅的译文方面表现优异，强调了人工评估方法在评估机器翻译质量中的重要性。 |
| [^31] | [AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer](https://arxiv.org/abs/2402.07680) | AYDIV是一个通过集成上下文视觉Transformer来增强3D物体检测的框架，特别设计了三阶段对齐过程来增强长距离的检测能力，包括改善相机特征提取、融合LiDAR和相机细节以及全面的空间数据融合。在Waymo开放数据集上，AYDIV的性能提高了1.24%的平均精度（mA）。 |
| [^32] | [Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data](https://arxiv.org/abs/2402.07640) | 该论文提出了一个可控的多模态反馈合成系统，能够根据文本和图像输入生成具有特定情感（积极或消极）的反馈，有着广泛的应用价值。 |
| [^33] | [Tighter Bounds on the Information Bottleneck with Application to Deep Learning](https://arxiv.org/abs/2402.07639) | 这个论文提出了一个对信息瓶颈更为紧密的变分界限，可以改善以前的基于信息瓶颈的DNNs的性能，并提供了一种简单方法来显著增强分类器DNNs的对抗鲁棒性。 |
| [^34] | [Overconfident and Unconfident AI Hinder Human-AI Collaboration](https://arxiv.org/abs/2402.07632) | 人工智能的过度自信和缺乏自信会阻碍人机协作，披露信心水平和提供反馈有助于认识到人工智能的信心不一致，但参与者往往因此不信任人工智能的建议，导致协作结果较差。 |
| [^35] | [AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts](https://arxiv.org/abs/2402.07625) | 本论文介绍了一种自主数据选择策略，利用语言模型进行数学文本的自动评估和选择，并通过连续预训练显著提高了数学推理能力。主要创新包括利用元提示语言模型作为验证器，发布了高质量的AutoMathText数据集，并实现了预训练令牌效率的提升。 |
| [^36] | [Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data](https://arxiv.org/abs/2402.07619) | 本研究开发了一个深度学习模型，通过声音录制数据识别COVID-19。通过提取语音特征并采用不同的深度学习分类模型，HuBERT实现了86%的最高准确率。 |
| [^37] | [Anchor-based Large Language Models](https://arxiv.org/abs/2402.07616) | 基于锚点的大型语言模型（AnLLM）通过引入创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略，将序列信息压缩到锚点标记中，减少键/值缓存，提高推理效率。 |
| [^38] | [Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping](https://arxiv.org/abs/2402.07610) | 本文首次探索了自助引导自对齐对大型语言模型的影响，发现其明显优于单次循环的方法，并通过调整数据训练顺序进一步提升模型性能。 |
| [^39] | [Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction](https://arxiv.org/abs/2402.07570) | 通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。 |
| [^40] | [Ensuring trustworthy and ethical behaviour in intelligent logical agents](https://arxiv.org/abs/2402.07547) | 该论文提出了基于动态逻辑的自检技术，旨在确保智能逻辑代理的可信和道德行为。 |
| [^41] | [Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models](https://arxiv.org/abs/2402.07543) | 本研究证明了使用解释来改进语言模型性能的显著好处，尤其适用于较小的模型，解释的加入使模型能够解决之前无法解决的任务。 |
| [^42] | [PKG API: A Tool for Personal Knowledge Graph Management](https://arxiv.org/abs/2402.07540) | 本文提出了一个完整的个人知识图（PKG）管理解决方案，包括用户界面友好的PKG客户端和面向服务的PKG API，以及基于RDF的PKG词汇表用于表示陈述和访问权限。 |
| [^43] | [BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection](https://arxiv.org/abs/2402.07536) | BreakGPT是第一个用于金融突破检测的大型语言模型，采用多阶段结构框架，提高了答案和理由的准确性。 |
| [^44] | [Physics-informed machine learning as a kernel method](https://arxiv.org/abs/2402.07514) | 物理约束的机器学习结合了数据方法的表达能力与物理模型的可解释性，可以用于正则化经验风险并提高估计器的统计性能。 |
| [^45] | [The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese](https://arxiv.org/abs/2402.07513) | 本研究通过对Whisper和MMS系统进行全面探索，评估了葡萄牙语中非正式对话语音的自动语音识别（ASR）偏见，并发现采用过采样技术可以缓解这种陈规定型偏见。 |
| [^46] | [Secret Collusion Among Generative AI Agents](https://arxiv.org/abs/2402.07510) | 本文汇集了人工智能和安全领域的相关概念，系统地形式化了生成式AI代理系统中的秘密勾结问题，并提出了缓解措施。通过测试各种形式的秘密勾结所需的能力，我们发现当前模型的隐写能力有限，但 GPT-4 展示了能力的飞跃。 |
| [^47] | [Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations](https://arxiv.org/abs/2402.07507) | 通过使用稀疏的GPS数据和地形特征，我们提出了一种基于聚类动力学的方法来提高速度预测，以解决从数据稀缺的地理区域提取准确交通信息的挑战。 |
| [^48] | [One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning](https://arxiv.org/abs/2402.07501) | 本论文提出了使用监督对比学习的加密流量分类框架，利用对比学习增强数据包级别和流级别表示，并通过图数据增强捕获字节级流量图的细粒度语义不变特征。同时，采用跨级多任务学习的方法，使得数据包级别任务学习到的表示能够被流级别任务利用。 |
| [^49] | [T-RAG: Lessons from the LLM Trenches](https://arxiv.org/abs/2402.07483) | T-RAG是一个基于LLM的应用程序，用于私人企业文件问答，它结合了RAG框架和经过微调的开源LLM，并分享了构建和部署过程中的经验教训。 |
| [^50] | [Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm](https://arxiv.org/abs/2402.07477) | 食物推荐作为语言处理（F-RLP）是一个针对食物的个性化、情境化的框架，利用大型语言模型的能力来实现更准确、个性化的食物推荐。 |
| [^51] | [Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2402.07465) | 这项研究提出了一种基于得分函数的求解器来解决高维福克-普朗克方程中的维数灾难问题。与蒙特卡洛和普通PINN相比，该方法能够更准确地处理与布朗运动相关的概率密度函数，并提供快速采样。 |
| [^52] | [A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?](https://arxiv.org/abs/2402.07462) | 我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。 |
| [^53] | [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/abs/2402.07456) | OS-Copilot是一个通用计算机代理的框架，能够与操作系统中的各种元素进行交互，包括网络、代码终端、文件、多媒体和第三方应用程序。使用OS-Copilot构建的自我提升的FRIDAY代理在各种计算机任务上表现出强大的泛化能力，并在通用人工智能助手基准测试中超过以前的方法35%。 |
| [^54] | [TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound](https://arxiv.org/abs/2402.07452) | TriAug是一个用于乳腺超声图像的异常样本检测框架，通过使用三元状态增强和平衡的球形损失来提高示踪分类的准确性和异常样本检测性能。 |
| [^55] | [AraSpider: Democratizing Arabic-to-SQL](https://arxiv.org/abs/2402.07448) | AraSpider是首个阿拉伯语版本的Spider数据集，研究表明使用回译策略可以显著提高ChatGPT 3.5和SQLCoder模型在阿拉伯语NLP任务中的性能。 |
| [^56] | [Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch](https://arxiv.org/abs/2402.07442) | 本论文提出了一种创新的游戏代理文本命令控制系统，通过使用大语言模型（LLM）进行代码生成，实现了对自由形式的自然语言命令的理解和执行，为实时语言交互游戏代理领域带来了显著贡献。 |
| [^57] | [Learning Optimal Tax Design in Nonatomic Congestion Games](https://arxiv.org/abs/2402.07437) | 本研究致力于学习如何设计最优税收，以在非原子拥堵博弈中提高效率。为了解决指数级的税收函数空间、梯度不存在和目标函数的非凸性等挑战，该算法利用了分段线性税收、额外的线性项和有效的子例程的新颖组成部分。 |
| [^58] | [Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs](https://arxiv.org/abs/2402.07435) | 本研究比较了GARCH、EWMA和IV模型在预测GBP/USD和EUR/GBP货币对每日回报的20天变动方面的效果。研究发现EUR/GBP货币对存在非对称回报的证据，而GBP/USD货币对的证据并不一致。 |
| [^59] | [Particle Filter SLAM for Vehicle Localization](https://arxiv.org/abs/2402.07429) | 本研究采用粒子滤波SLAM方法解决了车辆定位的挑战，利用编码数据、光纤陀螺仪和激光雷达技术实现精确的车辆运动估计和环境感知。 |
| [^60] | [News Recommendation with Attention Mechanism](https://arxiv.org/abs/2402.07422) | 本文提出了一种新的带有注意机制的新闻推荐方法NRAM，该方法可以显著提高数字新闻平台上的用户个性化新闻内容质量。 |
| [^61] | [On the Transit Obfuscation Problem](https://arxiv.org/abs/2402.07420) | 本文研究了转运混淆问题，提出了转运匿名性的概念，并提出并评估了满足该匿名性准则的规划/搜索算法。 |
| [^62] | [Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand](https://arxiv.org/abs/2402.07419) | 本文展示了通过条件生成模型的推进计算可以计算任何可辨识的因果效应，并提出了基于扩散的方法用于从图像的任何（条件）干预分布中进行采样。 |
| [^63] | [SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation](https://arxiv.org/abs/2402.07418) | 本论文介绍了一种名为SemTra的语义技能翻译器，可以在跨领域环境中实现零-shot策略自适应。该翻译器利用多模态模型提取技能，并利用预训练语言模型的推理能力将提取出的技能适应到目标领域，从而实现任务和技能的自适应。 |
| [^64] | [Auxiliary Reward Generation with Transition Distance Representation Learning](https://arxiv.org/abs/2402.07412) | 这篇论文提出了一种使用状态转换距离表示学习的辅助奖励生成方法，可以在强化学习中自动生成奖励，提高学习效率和减少人工设计奖励的工作量。 |
| [^65] | [Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples](https://arxiv.org/abs/2402.07408) | 我们提出了一种利用大型语言模型生成Webshell逃逸样本的混合提示算法，以解决现有研究中弱Webshell样本逃逸能力和缺乏复杂恶意特征数据集的问题。 |
| [^66] | [Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support](https://arxiv.org/abs/2402.07404) | 这项研究提出了一个新框架，将层次分析法和GPT-4集成，利用GPT-4自主智能体自动化决策过程，为网络安全多准则决策制定带来了重大进展。 |
| [^67] | [BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind](https://arxiv.org/abs/2402.07402) | 本文介绍了BDIQA数据集，用于探索VideoQA模型在心智理论（ToM）背景下的认知推理能力。该数据集受到儿童ToM认知发展的启发，并解决了现有数据集和任务中机器ToM的不足。它提供了两个难度级别的任务，评估了简单和复杂场景中的信念、欲望和意图（BDI）推理。 |
| [^68] | [VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization](https://arxiv.org/abs/2402.07398) | VisLingInstruct通过自主优化指导文本和视觉特征提取模块，显著提高了多模态语言模型在零样本学习中的性能，在TextVQA和HatefulMemes数据集上的准确率分别提高了13.1%和9%。 |
| [^69] | [TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator](https://arxiv.org/abs/2402.07393) | TeMPO是一种时分多路动态光学张量加速器，通过跨层设备/电路/架构定制，弥合了光学神经加速器与高度定制的电子对应器之间的性能差距。在设备级别上，该加速器采用了工厂可用的、定制的光学器件，包括实验演示的慢光电光调制器、光分配器和相移器，显著减小了输入编码和点乘中的占地面积和功耗。 |
| [^70] | [Exploring Perceptual Limitation of Multimodal Large Language Models](https://arxiv.org/abs/2402.07384) | 在这项研究中，我们探索了多模态大型语言模型（MLLMs）在感知能力上的限制。我们发现，对于小型视觉对象的问题，MLLMs的回答能力普遍存在限制。通过控制干预实验，我们发现物体质量、大小和位置都对MLLMs的感知能力有影响。 |
| [^71] | [SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder](https://arxiv.org/abs/2402.07370) | 本文介绍了SelfSwapper，一种通过 Shape Agnostic Masked AutoEncoder (SAMAE) 自监督方案来提升人脸交换模型训练的方法。通过绕过传统的训练方案，引入清晰的真实数据，以及利用遮罩和学到的特征，我们成功解决了身份泄漏和形状不对齐的问题。 |
| [^72] | [Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning](https://arxiv.org/abs/2402.07368) | 本研究通过使用2016年和2020年的选举数据，评估了基于大型语言模型的分组代表模型在泛化能力上的表现。研究发现，尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，这对实施分组代表模型的从业人员和决策者构成了挑战。 |
| [^73] | [Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing](https://arxiv.org/abs/2402.07366) | 本文提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架，通过结合期望最大化和Turbo Deep近似消息传递（TDAMP）实现分布式学习和压缩。该框架在处理联邦学习算法的缺点上有着显著的改进。 |
| [^74] | [Antagonistic AI](https://arxiv.org/abs/2402.07350) | 对抗性AI是一种具有相反行为或价值观的AI系统，与大多数人认为的相反，它可能对用户有益处，如迫使用户面对假设、建立恢复力或培养更健康的关系边界。 |
| [^75] | [Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble](https://arxiv.org/abs/2402.07347) | 本研究研究了TextFooler黑盒对01 loss sign激活神经网络集成的攻击准确性。研究发现，相比于sigmoid激活交叉熵和二进制神经网络，使用01 loss sign激活的网络更难受到TextFooler的攻击，并且通过引入一种新的全局汇集步骤，进一步提高了对抗精确度，使TextFooler几乎无效化。 |
| [^76] | [Measurement Scheduling for ICU Patients with Offline Reinforcement Learning](https://arxiv.org/abs/2402.07344) | 本研究介绍了使用离线强化学习进行ICU病人实验室检测排程的方法，并探索了最新数据集和离线强化学习方法在这一领域的应用。研究结果有助于提高实验室检测排程策略的质量和实用性。 |
| [^77] | [Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation](https://arxiv.org/abs/2402.07342) | 本文提出了一个涉及AI技术的未来设计工作流程，强调了动态建模，建设性协商和可持续激励三个关键点，这些点可以支持设计过程，并为AI技术与人类设计师合作的相对可行性提供了新思路。 |
| [^78] | [Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers](https://arxiv.org/abs/2402.07327) | 本研究使用了文本、语音和视频三种输入模态，并利用预训练的Transformer模型进行特征提取和情感结构分析。通过特征级融合和支持向量机分类，实现了75.42%的情感识别准确率。 |
| [^79] | [Persian Speech Emotion Recognition by Fine-Tuning Transformers](https://arxiv.org/abs/2402.07326) | 本文通过微调Transformer，针对波斯语语音情感识别问题提出了两种模型，显著提高了准确性，为这一领域的研究提供了有价值的创新。 |
| [^80] | [Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets](https://arxiv.org/abs/2402.07320) | 本研究提出了一种使用语言嵌入和主动学习来实现可解释、安全的自动驾驶的框架。通过使用对比语言-图像预训练嵌入进行聚类实验，我们有效地识别和解释了新颖的自动驾驶场景。 |
| [^81] | [ODIN: Disentangled Reward Mitigates Hacking in RLHF](https://arxiv.org/abs/2402.07319) | 本研究解决了强化学习中的奖励黑客问题，针对回复长度这一挑战，通过建立可靠的评估协议和改进奖励模型的方法，提出了减轻长度偏差的超参数和技巧，并进行了大规模研究。 |
| [^82] | [Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning](https://arxiv.org/abs/2402.07295) | 本文提出在无服务器联邦学习中利用知识蒸馏训练异构客户端模型的方法，以解决资源和统计数据的异质性挑战。 |
| [^83] | [On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study](https://arxiv.org/abs/2402.07294) | 本研究通过引入具有高测试覆盖率的真实Java程序数据集，探索了机器学习基于调用图剪枝的有效性，并通过对比分析静态调用图生成技术来解决当前方法存在的问题。 |
| [^84] | [How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?](https://arxiv.org/abs/2402.07282) | 本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。 |
| [^85] | [Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer](https://arxiv.org/abs/2402.07268) | 使用名为PathFormer的新型GNN模型架构，系统整合信号网络、先验知识和组学数据来实现高精确度疾病诊断和高可重复性生物标志物识别。 |
| [^86] | [DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains](https://arxiv.org/abs/2402.07250) | DIMON是一个学习在一系列变形的域上解算偏微分方程的通用算子学习框架，通过在参考域训练数据上学习解的映射，然后将其重新映射回原始域来实现对多个域上变化的初始/边界条件下的偏微分方程求解的近似。 |
| [^87] | [SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm](https://arxiv.org/abs/2402.07244) | SAIS是一种基于共生模式的新型生物启发式人工免疫系统，通过并行处理克服了传统人工免疫系统和SOS算法在处理大规模人口和增强人口多样性方面的挑战。实验证明，在26个基准问题上，SAIS在性能上与最先进的SOS方法相当，并超过其他常见的AIS方法和进化算法。 |
| [^88] | [CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain](https://arxiv.org/abs/2402.07234) | CPSDBench是一个专门为中国公共安全领域量身定制的大型语言模型评估基准，通过整合实际场景中收集的公共安全相关数据集，针对文本分类、信息提取、问题回答和文本生成四个关键维度全面评估LLMs的性能，并引入创新的评估指标，提高了对现有模型在解决公共安全问题方面性能的理解。 |
| [^89] | [TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation](https://arxiv.org/abs/2402.07233) | TransGPT是一种面向交通领域的新型多模式生成预训练Transformer，使用单模式和多模式数据进行微调，在交通领域的各种任务中优于基准模型。 |
| [^90] | [Successive Refinement in Large-Scale Computation: Advancing Model Inference Applications](https://arxiv.org/abs/2402.07229) | 本文介绍了一种分层分辨率计算的解决方案，可以在时间约束下获得近似版本的最终结果，这对于基于截止日期的系统和某些操作模式下的决策系统具有重要意义。 |
| [^91] | [Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL](https://arxiv.org/abs/2402.07226) | 本文提出了一种名为SSD的模型，该模型利用条件扩散模型来解决离线目标条件下强化学习中的问题，包括从次优行为数据集学习策略的困难以及在子轨迹中提取有用技能的需求。 |
| [^92] | [The Reasons that Agents Act: Intention and Instrumental Goals](https://arxiv.org/abs/2402.07221) | 本论文提出了一个对代理行为意图的操作化定义，并通过一些例子和结果展示了其灵活性和适用性。这一定义有助于理解和解释机器学习系统的行为，以及相关概念如工具性目标的关系。 |
| [^93] | [Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning](https://arxiv.org/abs/2402.07204) | 本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。 |
| [^94] | [Link-aware link prediction over temporal graph by pattern recognition](https://arxiv.org/abs/2402.07199) | 提出了一种链接感知模型，通过将历史链接和查询链接一起输入模型层来进行链接预测。与之前的方法不同，该模型关注链接演化模式的建模而非节点表示。实验证明该模型在六个数据集上取得了很好的效果。 |
| [^95] | [GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks](https://arxiv.org/abs/2402.07197) | "GraphTranslator"是一个旨在将预训练的图模型和大型语言模型对齐的翻译器，可以同时处理预定义任务和开放式任务。通过将这两种模型结合起来，能够有效地处理各种任务，并实现更具创新性和灵活性的应用。 |
| [^96] | [GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention](https://arxiv.org/abs/2402.07191) | 本文提出了一种改进的图不变学习方法，通过稀疏性、软性和可微性原则来提取不变子图，从而提高图学习的泛化性能。 |
| [^97] | [A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense](https://arxiv.org/abs/2402.07183) | 本文提出了一个新方法，利用视觉Transformer构造一个随机集成的加密模型，以增强对抗白盒和黑盒攻击的能力，并在多个实验中验证了其在图像分类任务中的鲁棒性和优越性能。 |
| [^98] | [MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization](https://arxiv.org/abs/2402.07180) | 本文提出了一种名为MAGNETO的边缘AI平台，通过从云端推向边缘进行增量人体活动学习，避免了云端与边缘设备之间的数据传输，实现了数据隐私保护、低延迟处理和高度个性化。 |
| [^99] | [EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches](https://arxiv.org/abs/2402.07174) | EmoWear是一种智能手表语音信息系统，通过使用动画提示反映情感，显著增强了情感交流体验。 |
| [^100] | [Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy](https://arxiv.org/abs/2402.07167) | 本论文介绍了一种使用深度学习模型和大型语言模型预测剂量容积直方图（DVH）的方法，通过将非结构化图像转化为结构化图，并利用大型语言模型编码处方和临床医生的指示，实现了对放射治疗计划质量的提高。 |
| [^101] | [Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias](https://arxiv.org/abs/2402.07166) | 本研究提供了人工智能和大型语言模型发展的全景视图，并指出了毒性、偏见和其他问题，同时强调人类大脑并不特殊，人类智能只是一个尺度上的新兴现象。 |
| [^102] | [Natural Language Reinforcement Learning](https://arxiv.org/abs/2402.07157) | 本研究将自然语言表示和强化学习原则相结合，提出了自然语言强化学习（NLRL）框架，解决了强化学习在样本效率低、解释性不足和缺乏监督信号等方面的限制问题，通过实验验证了其有效性和可解释性。 |
| [^103] | [Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations](https://arxiv.org/abs/2402.07153) | 本文提供了物理信息神经网络逼近半线性波动方程的严格误差界限，包括泛化误差和训练误差的界限，并在数值实验中展示了理论界限的有效性。 |
| [^104] | [Explainable Global Wildfire Prediction Models using Graph Neural Networks](https://arxiv.org/abs/2402.07152) | 本研究提出了一个创新的基于图神经网络的全球野火预测模型，将全球气候和野火数据转化为图表示，解决了传统模型中的海洋数据缺失和远程依赖性问题，并展示了更高的预测准确性。同时，该模型还具有可解释性，揭示了其潜在价值。 |
| [^105] | [X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design](https://arxiv.org/abs/2402.07148) | X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。 |
| [^106] | [Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models](https://arxiv.org/abs/2402.07140) | 这项研究揭示了图描述的文本顺序对大语言模型在图推理中的性能产生显著影响，并通过改变文本顺序提高了大语言模型的性能。此外，发现大语言模型的推理性能与图大小之间的关系不是单调递减的。为了评估大语言模型在不同图大小上的性能，引入了规模化图推理基准。 |
| [^107] | [An attempt to generate new bridge types from latent space of denoising diffusion Implicit model](https://arxiv.org/abs/2402.07129) | 本论文尝试使用去噪扩散隐式模型创新桥梁类型，通过易于理解的代数方法推导出函数公式，使用深度学习平台构建模型，并利用潜空间采样生成具有非对称结构的新桥梁类型。 |
| [^108] | [Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation](https://arxiv.org/abs/2402.07127) |  |
| [^109] | [Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation](https://arxiv.org/abs/2402.07118) | 本研究提出了一种基于人工智能的眼科质量评估系统，可以帮助远程眼科咨询，尤其是在低收入和中等收入国家。该系统能够即时反馈用户拍摄的眼部图像的质量，解决了当前用户拍摄图像质量不佳的问题。 |
| [^110] | [Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning](https://arxiv.org/abs/2402.07107) | 这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。 |
| [^111] | [Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments](https://arxiv.org/abs/2402.07102) | 未来预测在部分可观测环境中学习 History Representation 具有很强的相关性和有效性。 |
| [^112] | [Improving Pallet Detection Using Synthetic Data](https://arxiv.org/abs/2402.07098) | 本研究通过使用合成数据和Unity平台生成数据，提高了在仓库环境中托盘检测的性能。此外，研究还发现模型在光线较暗的环境下性能较差，并且使用YOLOv8和SAM的两阶段检测器具有不稳定的性能。 |
| [^113] | [Self-Correcting Self-Consuming Loops for Generative Model Training](https://arxiv.org/abs/2402.07087) | 本论文研究了使用合成数据进行生成模型训练时可能出现的自我消耗循环问题，并提出了一种通过引入理想的修正函数来稳定训练的方法。同时，我们还提出了自我修正函数来近似理想的修正函数，并通过实验证实了其有效性。 |
| [^114] | [Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training](https://arxiv.org/abs/2402.07076) | 通过对比预训练和多领域匹配结构，我们提出了CAMA框架来改善B2B云解决方案的匹配问题，克服了复杂特征和有限数据的挑战。 |
| [^115] | [Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine](https://arxiv.org/abs/2402.07069) | 这篇论文介绍了一种使用大型语言模型自动生成自动机来编码高级知识，加速强化学习过程的算法，并证明了其在多个任务上的有效性和优越性。 |
| [^116] | [$L^*LM$: Learning Automata from Examples using Natural Language Oracles](https://arxiv.org/abs/2402.07051) | 该论文提出了一个名为 $L^*LM$ 的算法，通过自然语言和演示学习 DFA，提高了数据效率，具备强大的少样本学习能力。 |
| [^117] | [A Factor Graph Model of Trust for a Collaborative Multi-Agent System](https://arxiv.org/abs/2402.07049) | 本研究提出了一种利用因子图模型表示智能体之间相互依赖行为和可信度的方法。该方法以高斯过程因子图对机器人的行为进行建模，并考虑了平滑性、避障和信任相关因素。信任评估是分散的，考虑了接近安全性、一致性和合作性等关键因素。整个系统是由一系列因子图组成的网络，通过信任相关因素进行交互，并采用贝叶斯推理方法动态评估信任。 |
| [^118] | [A Tale of Tails: Model Collapse as a Change of Scaling Laws](https://arxiv.org/abs/2402.07043) | 本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。 |
| [^119] | [Coordinated Disclosure for AI: Beyond Security Vulnerabilities](https://arxiv.org/abs/2402.07039) | 这篇论文提出了一种针对机器学习和人工智能问题的协调缺陷披露（CFD）框架，以解决目前领域中缺乏结构化过程的问题。 |
| [^120] | [Distilling Symbolic Priors for Concept Learning into Neural Networks](https://arxiv.org/abs/2402.07035) | 本论文通过元学习的方法，将符号贝叶斯模型的先验知识提取到神经网络中，使神经网络具有显示归纳偏见的能力，从而加快对抽象概念的学习。 |
| [^121] | [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://arxiv.org/abs/2402.07033) | 本文介绍了Fiddler，一种用于Mixture-of-Experts模型的资源高效推断引擎，通过CPU-GPU编排实现最小化数据传输，相比现有方法提高了一个数量级的推断速度。 |
| [^122] | [Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration](https://arxiv.org/abs/2402.07031) | 本论文研究了实例级别的合成数据质量与安全感知，引入了四种超越纯视觉特征的合成数据质量，并提出了优化方法来减少合成和真实图像之间的质量差距。 |
| [^123] | [Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations](https://arxiv.org/abs/2402.07023) | 该论文综合评估了开源LLM和谷歌的多模态LLM Gemini 在医学推理、幻觉检测和医学视觉问答任务上的能力。Gemini在诊断准确性方面落后于最先进模型，且易出现幻觉、过度自信和知识盲点。采用提示策略可以提高性能。 |
| [^124] | [REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models](https://arxiv.org/abs/2402.07016) | REALM是一个通过大型语言模型和检索增强生成驱动的框架，用于增强多模态电子健康记录（EHR）的表征，以提高临床预测能力。 |
| [^125] | [FedImpro: Measuring and Improving Client Update in Federated Learning](https://arxiv.org/abs/2402.07011) | 本文提出了FedImpro方法，通过生成改进的本地模型来减轻联邦学习中的客户漂移问题。该方法通过分析本地训练的泛化贡献，并利用类似的条件分布进行训练，增强了泛化贡献并减小了梯度的差异性。 |
| [^126] | [Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off](https://arxiv.org/abs/2402.07002) | 本论文提出了一种名为FedCEO的新型联邦学习框架，在保护用户隐私的同时，通过让客户端相互协作，实现了对模型效用和隐私之间的权衡。通过高效的张量低秩近端优化，该框架能够恢复被打断的语义信息，并在效用-隐私权衡方面取得了显著的改进。 |
| [^127] | [A Rational Analysis of the Speech-to-Song Illusion](https://arxiv.org/abs/2402.06992) | 本论文提出了对语音转换成歌曲的幻象的合理分析，将其视为一种统计推断过程，通过分析语料库，还发现了一种纯文本的小说转歌词的幻象，并提供了强有力的证据来支持这一观点。 |
| [^128] | [OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery](https://arxiv.org/abs/2402.06985) | OSSAR是一种创新的开放集手术活动识别框架，通过利用超球面互补点策略来区分已知类别和未知类别，并通过改善模型校准解决封闭集中过于自信的问题。 |
| [^129] | [Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI](https://arxiv.org/abs/2402.06982) | 本研究旨在使用多参数术前磁共振成像预测脑胶质母细胞瘤患者在不同治疗下的生存时间，通过考虑治疗信息与磁共振扫描相结合的模型，实现个体化和精确的治疗规划。 |
| [^130] | [Event-Keyed Summarization](https://arxiv.org/abs/2402.06973) | 事件关键摘要（EKS）是一种新颖的任务，旨在为特定事件生成上下文化的摘要。我们提出了一个基准数据集MUCSUM，并展示了EKS与传统摘要和结构到文本的比较结果。 |
| [^131] | [Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue](https://arxiv.org/abs/2402.06967) | 本论文提出了一种名为Midi-Tuning的多轮交互对话调整框架，通过分别对代理人和用户建模，并利用轮次级内存缓存机制进行调整，实现了对话代理的一致性和稳定性。 |
| [^132] | [Tree Ensembles for Contextual Bandits](https://arxiv.org/abs/2402.06963) | 本论文提出了一种基于树集成的情境多臂老虎机新框架，通过整合两种广泛使用的老虎机方法，在标准和组合设置中实现了优于基于神经网络的方法的性能，在减少后悔和计算时间方面表现出更出色的性能。 |
| [^133] | [Architectural Neural Backdoors from First Principles](https://arxiv.org/abs/2402.06957) | 本论文提出了从第一原理构建架构神经后门的方法，并描述了12种不同类型的架构后门。同时，通过构建一个任意触发检测器，展示了无需人工监督即可为架构引入后门的能力。 |
| [^134] | [Training dynamics in Physics-Informed Neural Networks with feature mapping](https://arxiv.org/abs/2402.06955) | 本研究探究了使用特征映射层的物理引导神经网络（PINNs）的训练动态，通过极限共轭核和神经切向核揭示了模型的收敛和泛化。我们提出了一种替代基于傅里叶变换的特征映射的条件正定径向基函数，证明了该方法在各种问题集中的有效性，并可以轻松实现在坐标输入网络中。这为广泛的PINNs研究带来了益处。 |
| [^135] | [Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices](https://arxiv.org/abs/2402.06952) | 本研究对噪声中等规模量子计算机上的串扰误差进行了全面分析，发现并行指令之间的串扰可能会破坏量子状态并导致程序执行错误 |
| [^136] | [Evaluation Metrics for Automated Typographic Poster Generation](https://arxiv.org/abs/2402.06945) | 本文提出一组启发式度量标准用于评估自动生成的印刷海报，包括可读性、美观度和语义特征。通过约束进化方法生成海报，并将度量标准作为约束条件。此外，还集成情感识别技术，并分析了方法和视觉特征的性能。 |
| [^137] | [Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities](https://arxiv.org/abs/2402.06938) | 这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。 |
| [^138] | [ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G](https://arxiv.org/abs/2402.06931) | ORIENT是一种面向6G的优先权感知节能方法，旨在通过解决服务实例放置和分配、路径选择和请求优先级的联合问题来最大化系统的整体利润，并在长时间内最小化能耗。 |
| [^139] | [Making a prototype of Seoul historical sites chatbot using Langchain](https://arxiv.org/abs/2402.06929) | 本文介绍了使用Langchain制作首尔历史遗址聊天机器人的原型，旨在提高游客对该地区宝贵文化遗产的认识，并提供准确和可靠的信息。该代理设计用于英语访问，并利用首尔市政府提供的数据。方法和结构概述也提供在论文中，同时也讨论了改进的潜力。 |
| [^140] | [Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought](https://arxiv.org/abs/2402.06918) | 本文提出了一种基于直接两两比较的方法，通过利用LLMs的噪声反馈，直接识别出最有潜力的中间思维，从而生成优秀的思维链。 |
| [^141] | [Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks](https://arxiv.org/abs/2402.06912) | 本研究通过使用进化策略(ES)来优化神经网络的权重，以通过直接策略搜索解决深度强化学习(DRL)基准问题。研究结果显示，ES可以在许多基准任务中找到有效的线性策略，与当前使用更大网络的DRL方法相比，这表明当前的基准问题比以往认为的更容易解决。 |
| [^142] | [Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions](https://arxiv.org/abs/2402.06908) | 本文研究了图神经网络的瓶颈问题，并提出了一种名为高拓扑神经网络的方法，通过引入高阶相互作用和多关系归纳偏置来缓解这些问题。 |
| [^143] | [Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900) | 本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。 |
| [^144] | [GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators](https://arxiv.org/abs/2402.06894) | GenTranslate是一个新的翻译任务生成模型，通过利用大型语言模型的丰富语言知识和强大推理能力，可以从N-best列表中生成更高质量的翻译结果。 |
| [^145] | [Non-autoregressive Generative Models for Reranking Recommendation](https://arxiv.org/abs/2402.06871) | 本研究提出了一个非自回归的生成模型用于排序推荐，在多阶段推荐系统中扮演关键角色。该模型旨在提高效率和效果，并解决稀疏训练样本和动态候选项对模型收敛性的挑战。 |
| [^146] | [Discriminative Adversarial Unlearning](https://arxiv.org/abs/2402.06864) | 该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。 |
| [^147] | [UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction](https://arxiv.org/abs/2402.06861) | UrbanKGent是一个用于城市知识图谱构建的统一大型语言模型代理框架，通过异构感知和地理空间注入构建知识化指令集，并通过迭代轨迹细化模块来提升轨迹的质量。在两个真实世界的数据集上进行的评估表明了UrbanKGent的有效性和性能。 |
| [^148] | [LiRank: Industrial Large Scale Ranking Models at LinkedIn](https://arxiv.org/abs/2402.06859) | LiRank是领英的一个大规模排名框架，它应用了最先进的建模架构和优化方法，并提出了新的建模改进和技术，通过A/B测试取得了有效的结果。 |
| [^149] | [ChemLLM: A Chemical Large Language Model](https://arxiv.org/abs/2402.06852) | ChemLLM是第一个专门用于化学领域的大型语言模型，利用新颖的指令构建方法将结构化知识转化为对话形式，具有平滑对话交互的能力，并在化学的三个主要任务中击败了GPT-3.5。 |
| [^150] | [Discipline and Label: A WEIRD Genealogy and Social Theory of Data Annotation](https://arxiv.org/abs/2402.06811) | 本文探讨了数据注释的批判家谱，强调了评级者多样性对公平性和模型性能的重要性，并研究了数据注释工作者的工作条件、注释者主观性对标签的影响以及注释工作中的心理伤害。 |
| [^151] | [Evaluating Co-Creativity using Total Information Flow](https://arxiv.org/abs/2402.06810) | 本研究旨在通过使用总信息流来定量评估音乐中的共同创造力过程，并通过定性研究证明该方法与人类感知相匹配。 |
| [^152] | [Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing](https://arxiv.org/abs/2402.06794) | 本文介绍了使用GPT-4V进行可解释风险评估的方法，该方法通过解释复杂的过马路场景，为盲人和视力低下人士的安全决策提供支持。 |
| [^153] | [Transfer learning with generative models for object detection on limited datasets](https://arxiv.org/abs/2402.06784) | 本论文提出了一个适用于通用情景的基于生成模型的迁移学习框架，用于解决有限数据集上的目标检测任务。 |
| [^154] | [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782) | 本文研究了更弱的语言模型是否能评估更强的模型的正确性。研究发现，通过进行辩论，非专家模型和人类回答问题的准确性都有所提高。 |
| [^155] | [Retrosynthesis Prediction via Search in (Hyper) Graph](https://arxiv.org/abs/2402.06772) | 本研究提出了一种基于半模板的逆向合成方法，通过在产物分子图和离开基团超图中进行搜索，以处理复杂的反应。 |
| [^156] | [Evaluation Metrics for Text Data Augmentation in NLP](https://arxiv.org/abs/2402.06766) | 这项研究提供了文本增强方法的评估指标分类法，为统一基准提供方向和帮助。在不同任务、度量标准和实验设置下，该分类法有助于比较不同的增强方法。 |
| [^157] | [GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding](https://arxiv.org/abs/2402.06764) | 该论文提出了GLaM方法，通过邻域划分和生成子图编码，对领域知识图进行大型语言模型的微调。该方法的创新之处在于能够实现对实际应用中的多步推理，并减少虚构。 |
| [^158] | [A Methodology for Questionnaire Analysis: Insights through Cluster Analysis of an Investor Competition Data](https://arxiv.org/abs/2402.06759) | 本文提出了一种通过对投资者竞赛数据进行聚类分析的方法来分析问卷数据，发现与金融数据相结合时的额外见解，并提出了创新的视觉表示方法来验证聚类分析和问题之间的关系发现。 |
| [^159] | [ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning](https://arxiv.org/abs/2402.06737) | 本文介绍了一种新颖的自监督学习方法ExGRG，它通过显式生成关系图来解决图结构数据上的挑战，将先验领域知识和在线提取的信息纳入自监督学习中，取得了显著的成功。 |
| [^160] | [Corruption Robust Offline Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2402.06734) | 我们研究了具有人类反馈的强化学习中的数据腐败鲁棒性问题，并设计了新颖的离线方法来处理损坏的数据，并且在不同的数据生成分布假设下具有性能保证。 |
| [^161] | [NICE: To Optimize In-Context Examples or Not?](https://arxiv.org/abs/2402.06733) | 通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。 |
| [^162] | [Dynamic Graph Information Bottleneck](https://arxiv.org/abs/2402.06716) | 动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。 |
| [^163] | [Entropy-Regularized Token-Level Policy Optimization for Large Language Models](https://arxiv.org/abs/2402.06700) | 本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。 |
| [^164] | [Feed-Forward Neural Networks as a Mixed-Integer Program](https://arxiv.org/abs/2402.06697) | 这项研究探索了将训练的修正线性单元(ReLU)神经元作为混合整数规划(MIP)的形式，并将MIP模型应用于训练神经网络。研究发现MIP技术在不同的神经网络架构中具有广泛的应用潜力，包括二进制DNN和二值化DNN。 |
| [^165] | [FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models](https://arxiv.org/abs/2402.06696) | 本研究提出了一种名为FL-NAS的基于大型语言模型的神经架构搜索框架，该框架能够在模型准确性、公平性和硬件部署效率三个方面达到卓越的性能。 |
| [^166] | [Integrating LLMs for Explainable Fault Diagnosis in Complex Systems](https://arxiv.org/abs/2402.06695) | 本研究介绍了一种集成系统，通过将基于物理的诊断工具与大型语言模型相结合，帮助复杂系统实现可解释的故障诊断。该系统不仅能够识别故障，还能够提供清晰易懂的故障原因和影响解释，提高自主系统的可靠性和透明度。 |
| [^167] | [Scaling Intelligent Agents in Combat Simulations for Wargaming](https://arxiv.org/abs/2402.06694) | 本研究利用分层强化学习提升战争游戏中智能代理的性能，以加速决策速度和提高决策质量。 |
| [^168] | [HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation](https://arxiv.org/abs/2402.06692) | HistoHDR-Net是一种用于单个LDR到HDR图像转换的简单而有效的方法，通过融合直方图均衡化的LDR图像和自注意力引导，恢复HDR图像的细节。 |
| [^169] | [Private Knowledge Sharing in Distributed Learning: A Survey](https://arxiv.org/abs/2402.06682) | 这篇论文提供了关于分布式学习中的私人知识共享的全面调查，分析了在领先的分布式学习架构中使用的各种知识组件，旨在揭示现有的解决方法和未来的研究方向。 |
| [^170] | [Social Physics Informed Diffusion Model for Crowd Simulation](https://arxiv.org/abs/2402.06680) | 本文提出了一种叫做SPDiff的基于社会物理的扩散模型，用于人群模拟。模型综合考虑了人群的互动和历史信息，通过逆向扩散过程生成下一个时间段内行人运动的分布。此外，借鉴社会力模型，并利用人群互动的等变性属性增强了模型的性能。为了解决长期模拟中的误差累积问题，引入了多层次扩散模型和最小二乘法进行参数估计。 |
| [^171] | [Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain](https://arxiv.org/abs/2402.06673) | 推动可解释人工智能（XAI）在复杂决策过程中的透明度和可解释性关键，从特征到人为中心方法的演进被探索。 XAI在医疗和金融等领域应用广泛。挑战包括生成模型的可解释性、负责任的AI实践和道德影响。XAI和认知科学的融合、情感智能AI的发展以及AI系统寻求人类智能（HLI）也被研究。意识、伦理和社会影响变得至关重要。AI解密大脑之谜和寻求人造大脑也代表了一场变革的追求。 |
| [^172] | [Weather Prediction with Diffusion Guided by Realistic Forecast Processes](https://arxiv.org/abs/2402.06666) | 这种新方法利用扩散模型进行天气预测，能够在相同的建模框架下实现直接和迭代预测，并且可以嵌入NWP预测，提高可信赖性和预测性能。 |
| [^173] | [The Essential Role of Causality in Foundation World Models for Embodied AI](https://arxiv.org/abs/2402.06665) | 基于因果关系的基础世界模型对于具身人工智能的发展至关重要，当前的基础模型无法准确建模与现实世界的物理相互作用。因果关系的研究有助于构建真实世界模型，提高对可能相互作用结果的准确预测能力。 |
| [^174] | [LLM Agents can Autonomously Hack Websites](https://arxiv.org/abs/2402.06664) | 这项研究展示了LLM代理可以自主进行网站黑客攻击，包括盲目数据库模式提取和SQL注入，而且不需要人工反馈。这种能力是由高度工具使用和利用扩展上下文能力的前沿模型赋予的。 |
| [^175] | [Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface](https://arxiv.org/abs/2402.06663) | 本文提出了一个对抗学习框架，用于合法参与方间的物理层密钥生成，在恶意可重构智能面干扰下提供了一个可解释的解决方案。 |
| [^176] | [The role of the metaverse in calibrating an embodied artificial general intelligence](https://arxiv.org/abs/2402.06660) | 本文研究了具有肉身的人工通用智能(AGI)的概念及其与人类意识的关系，强调了元宇宙在促进这一关系中的关键作用。通过结合不同理论框架和技术工具，论文总结出实现具有肉身的AGI的关键要素和发展阶段。 |
| [^177] | [Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://arxiv.org/abs/2402.06659) | Shadowcast是一种隐秘的数据污染攻击方法，可以通过伪装成良性图像和匹配文本来操纵视觉语言模型的响应。它包括标签攻击和说服攻击，可以混淆类别标签并编写有说服力的描述。使用仅50个毒样本，Shadowcast能够高效实现攻击者的意图。 |
| [^178] | [DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation](https://arxiv.org/abs/2402.06656) | DiffsFormer是一种利用Diffusion Transformer和人工智能生成样本的方法，用于在股票预测中解决数据稀缺性和数据同质性的问题。 |
| [^179] | [Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655) | 本文研究了防御文本分类器中对抗性净化方法的有效性，并提出了一种基于大型语言模型加以净化的方法。 |
| [^180] | [Conversational Crowdsensing: A Parallel Intelligence Powered Novel Sensing Approach](https://arxiv.org/abs/2402.06654) | 本文提出了一种名为会话式众包感知的新型感知方法，用于工业5.0。通过有效的对话和组织多样化劳动力，该方法可以缓解个人工作负担，促进更快的响应和广泛普及的众包感知系统。 |
| [^181] | [A Survey on Large Language Model Hallucination via a Creativity Perspective](https://arxiv.org/abs/2402.06647) | 通过创造性视角对大型语言模型（LLM）的幻觉进行调查研究，发现幻觉可能通过培养创造力来促进LLM的应用。 |
| [^182] | [Modeling and Optimization of Epidemiological Control Policies Through Reinforcement Learning](https://arxiv.org/abs/2402.06640) | 通过强化学习建模和优化，设计出适用于大流行控制的多目标策略，最小化传染和死亡率的同时减少经济影响。 |
| [^183] | [Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting](https://arxiv.org/abs/2402.06638) | 本文提出了一种使用注意力联合聚合的Transformer模型，用于时间序列股票预测，旨在解决传统训练方案中存在的过拟合、数据稀缺和隐私问题。 |
| [^184] | [SocraSynth: Multi-LLM Reasoning with Conditional Statistics](https://arxiv.org/abs/2402.06634) | SocraSynth是一个多语言模型推理平台，通过使用条件统计和系统化的语境增强技术，以及可调节的辩论争议程度，解决了大型语言模型(LLMs)面临的偏见、幻觉和推理能力不足等问题。 |
| [^185] | [Towards the mathematical foundation of the minimum enclosing ball and related problems](https://arxiv.org/abs/2402.06629) | 这项研究提供了最小外接球问题的数学基础，通过包围和划分定理探索了圆半径、直径和宽度等之间的界限和关系。 |
| [^186] | [Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss](https://arxiv.org/abs/2402.06187) | Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。 |
| [^187] | [\textit{MinMaxMin} $Q$-learning](https://arxiv.org/abs/2402.05951) | \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。 |
| [^188] | [\textit{SQT} -- \textit{std} $Q$-target](https://arxiv.org/abs/2402.05950) | SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。 |
| [^189] | [ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation](https://arxiv.org/abs/2402.05902) | 本研究提出了ClickSAM，该方法使用点击提示对超声图像进行Segment Anything Model的精细调整，解决了超声图像分割中噪声干扰的问题。 |
| [^190] | [Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking](https://arxiv.org/abs/2402.05880) | LLM驱动的对话式搜索系统增加了选择性曝光，且支持用户观点的有偏见LLM加剧了这种偏差。 |
| [^191] | [PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models](https://arxiv.org/abs/2402.05868) | PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。 |
| [^192] | [In-Context Principle Learning from Mistakes](https://arxiv.org/abs/2402.05403) | 本文提出了一种新的学习方法LEAP，通过让模型从少量输入-输出示例中犯错误，然后反思并学习准则，从而提升模型在各种任务上的表现。 |
| [^193] | [BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs](https://arxiv.org/abs/2402.05301) | 本文介绍了BIKED++数据集，其中包含了140万个自行车设计的图像和参数化CAD文件。该数据集可以用于训练跨模态预测模型，例如使用参数化表示来准确估计图像的特征嵌入。该数据集也已公开，可供研究者使用。 |
| [^194] | [Do Transformer World Models Give Better Policy Gradients?](https://arxiv.org/abs/2402.05290) | 在强化学习中，通过使用变形器世界模型来预测未来奖励并进行策略梯度学习通常变得不可行。研究人员发现常用的变形器世界模型会产生迂回的梯度路径，对于长距离的策略梯度是有害的。为了解决这个问题，他们提出了一种名为Actions World Models (AWMs)的世界模型，可以提供更直接的梯度传播路径。 |
| [^195] | [Large Language Models As MOOCs Graders](https://arxiv.org/abs/2402.03776) | 该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。 |
| [^196] | [RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback](https://arxiv.org/abs/2402.03681) | RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。 |
| [^197] | [Diffusion World Model](https://arxiv.org/abs/2402.03570) | 扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。 |
| [^198] | [Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines](https://arxiv.org/abs/2402.03457) | 本研究评估了一种高效且可解释的交通目的地预测模型（EBM），在多个混合交通数据集上表现出有竞争力的性能，并能够提供特征重要性和交互作用的分析以及预测解释的质量示例。 |
| [^199] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^200] | [Evading Data Contamination Detection for Language Models is (too) Easy](https://arxiv.org/abs/2402.02823) | 本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。 |
| [^201] | [Review of multimodal machine learning approaches in healthcare](https://arxiv.org/abs/2402.02460) | 这篇综述主要介绍医疗保健领域中多模态机器学习方法的最新研究进展。通过综合分析最近的文献，探讨了临床诊断中各种数据模态的应用以及融合技术的评估。重点关注影像数据的应用，并介绍了现有的多模态数据集和训练方法。 |
| [^202] | [MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters](https://arxiv.org/abs/2402.02342) | MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。 |
| [^203] | [VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models](https://arxiv.org/abs/2402.01735) | 这项研究调查了具有大型模型的视觉障碍辅助，并通过基准实验评估了模型的能力，进一步推动了视觉障碍辅助技术的发展。 |
| [^204] | [Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data](https://arxiv.org/abs/2402.01713) | 本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。 |
| [^205] | [SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks](https://arxiv.org/abs/2401.15741) | 这篇论文提出了一种名为SERNet-Former的高效剩余网络语义分割方法。它通过引入注意力增强门和注意力融合网络来改善语义分割方法的效率，并解决了从全局和局部上融合语义信息的问题。实验结果表明，该方法在挑战性的数据集上取得了良好的性能。 |
| [^206] | [Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions](https://arxiv.org/abs/2401.06790) | 这项工作提出了使用无监督方法和零样本提示来自动构建和扩展主题分类法的研究。通过应用主题建模和关键词提取技术，结合基于指令的微调LLMs，在零售银行数据集中为商家分配标签，具有超过90%的一致性率和令人兴奋的结果。 |
| [^207] | [In-Context Reinforcement Learning for Variable Action Spaces](https://arxiv.org/abs/2312.13327) | 本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。 |
| [^208] | [Parameterized Projected Bellman Operator](https://arxiv.org/abs/2312.12869) | 本论文提出了一种基于学习的近似贝尔曼算子的新方法，以解决近似值迭代算法中样本不确定性和计算复杂度的问题。 |
| [^209] | [Scaling Opponent Shaping to High Dimensional Games](https://arxiv.org/abs/2312.12568) | 本文通过对对手塑形（OS）方法的扩展，成功将其应用于具有时间延长操作和长时间范围的广义游戏，该方法能够改善个体和集体的结果。 |
| [^210] | [Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models](https://arxiv.org/abs/2312.07130) | 该论文介绍了一种称为分而治之攻击的方法，利用LLM作为文本转换代理绕过文本到图像模型的安全过滤器。该攻击设计了攻击辅助提示，引导LLM将不道德的绘图意图分解为多个个体图像元素的良性描述，以绕过安全过滤器生成不道德的图像。实验结果表明，该攻击成功地绕过了多个强大的封闭式安全过滤器。 |
| [^211] | [Uncovering communities of pipelines in the task-fMRI analytical space](https://arxiv.org/abs/2312.06231) | 本论文通过使用社群检测算法揭示了任务fMRI分析空间中的流程社群，并评估了不同背景下的流程关系的稳定性。研究表明，存在一些子集的流程给出相似的结果，特别是那些分享特定参数。这些结果对于参与者群体来说是稳定的，但在不同任务之间不稳定。流程空间的形成主要受到大脑激活区域大小和统计值规模的影响。 |
| [^212] | [zkDFL: An efficient and privacy-preserving decentralized federated learning with zero-knowledge proof](https://arxiv.org/abs/2312.04579) | zkDFL是一种利用零知识证明实现高效且保护隐私的去中心化联合学习系统，通过将大规模模型参数与可信服务器共享，并使用区块链技术进行算法管理和验证，实现数据隐私保护和完整性。 |
| [^213] | [Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use](https://arxiv.org/abs/2312.04455) | 本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。 |
| [^214] | [What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes](https://arxiv.org/abs/2312.03096) | 这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。 |
| [^215] | [Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective](https://arxiv.org/abs/2312.00054) | 逆向强化学习是从专家策略示范中学习奖励函数的问题，本文提出了在标准离线和在线设置下用多项式样本和运行时间进行高效逆向强化学习的结果线索，并提供了几乎最优的样本复杂性的下界。 |
| [^216] | [Monitor Placement for Fault Localization in Deep Neural Network Accelerators](https://arxiv.org/abs/2311.16594) | 本研究提出了一种在并行阵列中优化硬件监测器部署的解决方案，以提高深度神经网络加速器的可靠性。通过证明和推导，我们确定了定位单个故障的PE所需的监测器数量，并解决了NP困难的监测器部署方案优化问题。然后，我们提出了一种启发式方法来平衡效益与开销。 |
| [^217] | [Large language models can enhance persuasion through linguistic feature alignment](https://arxiv.org/abs/2311.16466) | 本研究调查了大型语言模型对人类沟通的影响，使用了消费者金融投诉数据，并发现大型语言模型的使用可能增强了一整套语言特征，提高了信息说服力。 |
| [^218] | [Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision](https://arxiv.org/abs/2311.15108) | 本研究利用扩散模型创建了一个以人口统计特征平衡的数据集，用于衡量计算机视觉模型的公平性。通过测试多个模型的职业分类任务，发现使用非高加索标签生成的图像存在明显的职业误分类率高于使用高加索标签生成的图像，且几个误分类表明存在种族偏见。 |
| [^219] | [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455) | 本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。 |
| [^220] | [Discovering Effective Policies for Land-Use Planning](https://arxiv.org/abs/2311.12304) | 通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。 |
| [^221] | [Efficient Reinforcement Learning from Partial Observability](https://arxiv.org/abs/2311.12244) | 该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。 |
| [^222] | [On Measuring Faithfulness or Self-consistency of Natural Language Explanations](https://arxiv.org/abs/2311.07466) | 本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。 |
| [^223] | [Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models](https://arxiv.org/abs/2311.06233) | 这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。 |
| [^224] | [From Chaos to Clarity: Claim Normalization to Empower Fact-Checking](https://arxiv.org/abs/2310.14338) | 本研究提出了声明标准化任务，通过使用CACN模型利用思维链和声明检查来从复杂的社交媒体帖子中提取简化的声明，以加强事实核查。 |
| [^225] | [Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)](https://arxiv.org/abs/2310.09877) | 本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。 |
| [^226] | [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) | MemGPT是一个以语言模型为操作系统的系统，通过虚拟上下文管理技术，实现了超出有限上下文窗口的上下文利用。在文档分析方面，MemGPT能够分析超出底层限制的大型文档。 |
| [^227] | [Faithful Knowledge Graph Explanations for Commonsense Reasoning](https://arxiv.org/abs/2310.04910) | 本论文提出了两个量化指标来衡量基于知识图谱的解释的可信性，并引入了一种新的训练方法来改善解释的可信度。实验结果表明该方法可以提高解释的一致性和保真度。 |
| [^228] | [Hierarchical Multi-Marginal Optimal Transport for Network Alignment](https://arxiv.org/abs/2310.04470) | 层次化多边汇运输（HOT）框架通过融合Gromov-Wasserstein（FGW）重心将多个网络分解为对齐簇，并将FGW距离广义化到多边汇环境，实现多网络的联合对齐。实验证明，HOT相对于统计方法取得了显著的改进。 |
| [^229] | [KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation](https://arxiv.org/abs/2307.01878) | KDSTM是一种使用知识蒸馏的神经半监督主题建模方法，对于文本分类任务，在没有预训练嵌入且资源受限的情况下，能够提供高准确性、鲁棒性和效率。 |
| [^230] | [Making Language Models Better Tool Learners with Execution Feedback](https://arxiv.org/abs/2305.13068) | 这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。 |
| [^231] | [Scalable Neural Network Training over Distributed Graphs](https://arxiv.org/abs/2302.13053) | RETEXO是第一个消除分布式图神经网络训练中通信瓶颈的框架，通过新的训练过程懒消息传递来改善网络通信效率。 |
| [^232] | [Online Reinforcement Learning in Non-Stationary Context-Driven Environments](https://arxiv.org/abs/2302.02182) | 提出了一种名为LCPO的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧经验进行锚定来解决强化学习中的灾难性遗忘问题。 |
| [^233] | [Retrieval-based Disentangled Representation Learning with Natural Language Supervision](https://arxiv.org/abs/2212.07699) | 本研究提出了基于检索的带自然语言监督的解缠表示学习框架，利用自然语言作为数据变化的代理，通过词汇空间中的双编码器模型实现对数据内在特征的解缠表示学习。 |
| [^234] | [Dynamic Latent Separation for Deep Learning](https://arxiv.org/abs/2210.03728) | 本研究提出了动态潜变量分离的方法，可以在复杂数据中学习表达性强的潜变量，提升输出的多样性。该方法受原子物理学启发，通过学习每个数据样本的结构来解释各个子组件的重要性。实验证明该方法在不同分类和生成问题中提升了模型的性能。 |
| [^235] | [GBSVM: Granular-ball Support Vector Machine](https://arxiv.org/abs/2210.03120) | GBSVM是一种使用粗粒-球作为输入的支持向量机，修复了现有模型的错误并推导出了对偶模型，提出了粒子群优化算法和顺序最小优化算法来解决问题，具有良好的稳健性和效率。 |
| [^236] | [Compositional Q-learning for electrolyte repletion with imbalanced patient sub-populations](https://arxiv.org/abs/2110.02879) | 组合Q学习方法用于解决医疗环境中存在异质性治疗反应的问题，通过使用复合任务结构和分离的模块化Q值函数，能够更有效地进行决策。 |
| [^237] | [Creativity of Deep Learning: Conceptualization and Assessment](https://arxiv.org/abs/2012.02282) | 这篇论文探讨了深度学习在创意领域的应用，并使用计算创造力的视角对其进行了概念化和评估。研究发现，尽管深度学习可以产生高质量的结果，但其创新性受到限制，同时也存在内部问题表达无法更改以及在不同领域之间建立联系的缺乏能力的问题。 |
| [^238] | [ACCESS: Prompt Engineering for Automated Web Accessibility Violation Corrections.](http://arxiv.org/abs/2401.16450) | 本研究提出了一种用于修复网页无障碍违规的新方法，通过实时修改文档对象模型(DOM)和利用大型语言模型(LLMs)以及提示工程技术，解决了自动修复无障碍错误的问题。 |
| [^239] | [Building ethical guidelines for generative AI in scientific research.](http://arxiv.org/abs/2401.15284) | 本文提出了一个初步的框架，通过五个关键主题的分析和缓解策略来建立科学研究中生成AI的伦理指南。全球共识、专业培训和合理的执行对于促进AI的益处和维护研究诚信至关重要。 |
| [^240] | [PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models.](http://arxiv.org/abs/2401.15042) | PROXYQA是一个用于评估大型语言模型长篇文本生成的替代框架，通过生成详尽的内容，并利用评估器和生成内容作为背景环境，根据评估器回答代理问题的表现来评估生成内容的质量。 |
| [^241] | [Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem.](http://arxiv.org/abs/2401.14876) | 本论文提出了一种跨空间自适应滤波器（CSF），可以从图拓扑和节点属性空间中提取自适应频率信息，以减轻图卷积网络（GCN）的过度平滑问题。 |
| [^242] | [Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement.](http://arxiv.org/abs/2401.14215) | 本文提出了一个旨在解决长期对话中角色句子不具信息性的问题的框架，通过利用常识增强的角色扩展，并设计策略将相互矛盾的角色转化为包含丰富说话者信息的句子，以提高回应生成质量。 |
| [^243] | [All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks.](http://arxiv.org/abs/2401.09798) | 本研究提出了一种简单的黑盒方法，用于生成越狱攻击提示，克服了现有方法的复杂性和计算成本的限制。该方法通过使用语言模型自身，将有害提示重写为非有害表达，实现了超过80%的攻击成功率，并且即使模型更新，效果仍然有效。 |
| [^244] | [Learning Shortcuts: On the Misleading Promise of NLU in Language Models.](http://arxiv.org/abs/2401.09615) | 该论文调查了大型语言模型在自然语言理解任务中使用捷径学习的现象，强调了这种现象对语言模型评估的影响，并呼吁加大对捷径学习的研究力度以提升语言模型的鲁棒性和实际场景中的自然语言理解评估标准。 |
| [^245] | [Preparing Lessons for Progressive Training on Language Models.](http://arxiv.org/abs/2401.09192) | 提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。 |
| [^246] | [Augmenting Math Word Problems via Iterative Question Composing.](http://arxiv.org/abs/2401.09003) | 本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。 |
| [^247] | [InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks.](http://arxiv.org/abs/2401.05507) | InfiAgent-DABench是第一个评估基于LLM的代理在数据分析任务中的基准测试，包括DAEval数据集和代理框架。对23个最先进的LLMs进行的基准测试揭示了当前数据分析任务中的挑战。 |
| [^248] | [Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks.](http://arxiv.org/abs/2401.02731) | 本文提出了一种参数高效稀疏制作的方法，它使用专家混合式架构将密集模型转换为稀疏模型，以实现在模型容量有限的情况下进行指令调整和泛化能力增强。 |
| [^249] | [NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes.](http://arxiv.org/abs/2312.14890) | NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。 |
| [^250] | [Symbolic Numeric Planning with Patterns.](http://arxiv.org/abs/2312.09963) | 本文提出了一种符号模式规划的新方法，通过编码问题为一个公式可以比现有方法更有效地寻找解决方案。在实验中，我们的规划器Patty在今年的IPC问题上表现出优异的性能。 |
| [^251] | [Linear Log-Normal Attention with Unbiased Concentration.](http://arxiv.org/abs/2311.13541) | 本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。 |
| [^252] | [DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries.](http://arxiv.org/abs/2311.02017) | DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。 |
| [^253] | [Towards Calibrated Robust Fine-Tuning of Vision-Language Models.](http://arxiv.org/abs/2311.01723) | 本文提出了一个名为校准鲁棒微调（CaRot）的方法，针对视觉语言模型在分布变化下的校准问题。通过该方法，作者成功提高了预训练模型的校准性能和鲁棒性能。 |
| [^254] | [A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents.](http://arxiv.org/abs/2311.00344) | 本文为开放式学习问题定义了一个关键的基本属性，即无限时间内不断产生新元素。在这基础上，提出了开放式学习问题的概念，并着重研究了开放式目标条件强化学习的子集。 |
| [^255] | [Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning.](http://arxiv.org/abs/2310.12609) | 本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。 |
| [^256] | [CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation.](http://arxiv.org/abs/2310.09401) | CIDER是一种基于类别引导的个性化新闻推荐框架，通过意图分离和一致性的新闻表示来准确理解新闻文章的多个意图，并区分用户不同的后阅读偏好。 |
| [^257] | [ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection.](http://arxiv.org/abs/2310.09298) | ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。 |
| [^258] | [Discerning Temporal Difference Learning.](http://arxiv.org/abs/2310.08091) | 该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。 |
| [^259] | [Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits.](http://arxiv.org/abs/2310.02975) | 本文研究了在重尾波段问题中完全自适应的遗憾最小化，提出了随机自适应重尾波段问题，并证明了适应性算法相对于标准设置会有更高的遗憾。 |
| [^260] | [Zero-Shot Refinement of Buildings' Segmentation Models using SAM.](http://arxiv.org/abs/2310.01845) | 本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。 |
| [^261] | [Class Incremental Learning via Likelihood Ratio Based Task Prediction.](http://arxiv.org/abs/2309.15048) | 该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。 |
| [^262] | [Zero-Shot Robustification of Zero-Shot Models With Foundation Models.](http://arxiv.org/abs/2309.04344) | 提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。 |
| [^263] | [Certifying LLM Safety against Adversarial Prompting.](http://arxiv.org/abs/2309.02705) | 本研究提出了首个具有可验证安全保证的框架——消除和检查，用于对抗敌对提示。通过逐个消除标记并使用安全过滤器检查生成的子序列，确保任何敌对修改的有害输入提示都能被正确标识为有害。 |
| [^264] | [A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models.](http://arxiv.org/abs/2308.08925) | 本文提出了一种针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的新方法，通过将攻击视为书写风格之间的风格转换，引入新的损失函数来生成欺骗性图像，实现了最先进的攻击成功率。 |
| [^265] | [Unsupervised Deep Graph Matching Based on Cycle Consistency.](http://arxiv.org/abs/2307.08930) | 本文提出了一种基于循环一致性的无监督深度图匹配方法，不需要真实对应的关键点对，通过在同一对象类别的图像之间强制匹配一致性来进行自我监督学习，该方法具有很高的灵活性，并且在无监督图匹配方面达到了最新的最先进水平。 |
| [^266] | [Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models.](http://arxiv.org/abs/2307.08303) | 本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。 |
| [^267] | [Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability.](http://arxiv.org/abs/2307.07084) | 本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。 |
| [^268] | [Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks.](http://arxiv.org/abs/2306.14043) | 本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。 |
| [^269] | [Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective.](http://arxiv.org/abs/2306.13092) | SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。 |
| [^270] | [Adaptive interventions for both accuracy and time in AI-assisted human decision making.](http://arxiv.org/abs/2306.07458) | 本研究探索适用于人工决策辅助的智能干预方案，在同时考虑准确性和时间性的前提下，根据问题和用户的属性自适应地展示AI辅助具有良好的效果。 |
| [^271] | [FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs.](http://arxiv.org/abs/2306.04959) | 本文介绍了一个名为FedMLSecurity的基准测试，它可以模拟在联邦学习中可能出现的对抗攻击并提供相应的防御策略。该测试对各种机器学习模型和联合优化器都可以适用，并且能够轻松应用于大规模语言模型中。 |
| [^272] | [MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation.](http://arxiv.org/abs/2306.04120) | MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。 |
| [^273] | [Exploring EFL students' prompt engineering in human-AI story writing: an Activity Theory perspective.](http://arxiv.org/abs/2306.01798) | 本研究应用活动理论分析了香港中学生在短篇故事创作中利用生成式人工智能工具的方式和目的，发现其中缺乏目的意识、克服创作障碍以及发展、扩展和改进故事为主要目的。同时，学生活动系统的共同特征也被研究确定。 |
| [^274] | [Biomarker Discovery with Quantum Neural Networks: A Case-study in CTLA4-Activation Pathways.](http://arxiv.org/abs/2306.01745) | 该论文介绍了一种利用量子神经网络的模型，可以用于发现与CLTA4通路相关的新生物标志物，并且该模型经济实用。 |
| [^275] | [Distributed Online Rollout for Multivehicle Routing in Unmapped Environments.](http://arxiv.org/abs/2305.15596) | 本文提出了一个在线策略，通过分布式的协调策略解决了未建图环境下多车辆路径规划问题。 |
| [^276] | [Dynamic Masking Rate Schedules for MLM Pretraining.](http://arxiv.org/abs/2305.15096) | 本论文提出了一种动态调度掩码率的方法来改进MLM预训练的质量，通过线性降低掩码率，达到了对BERT-base和BERT-large模型分别提高0.46%和0.25%的平均GLUE准确率的效果。这种方法不仅加快了BERT-base的预训练速度，还实现了对BERT-large的帕累托改善。 |
| [^277] | [StyleLipSync: Style-based Personalized Lip-sync Video Generation.](http://arxiv.org/abs/2305.00521) | 本文提出了一种基于风格的个性化唇形动画视频生成模型，可以准确地生成任意身份的唇形同步视频，且可用于增强未见面孔的特征。 |
| [^278] | [Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control.](http://arxiv.org/abs/2302.11694) | 提出了一种使用分布式表示进行受限强化学习的方法，用于可信四旋翼无人机的跟踪控制。通过集成分布式强化学习干扰估计器和随机模型预测控制器，能够准确识别气动效应的不确定性，实现最优的全局收敛速率和一定的亚线性收敛速率。 |
| [^279] | [A Survey of Deep Learning: From Activations to Transformers.](http://arxiv.org/abs/2302.00722) | 这篇综述调查了深度学习领域的重要进展，包括各种架构、层、目标和优化技术的发展，以及关注机制、归一化、跳跃连接、Transformer和自监督学习等方法的变体。总结了成功创新的关键策略，并讨论了最近的商业闭源模型。 |

# 详细

[^1]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^2]: 联邦取消学习: 稳定性和公平性的视角

    Federated Unlearning: a Perspective of Stability and Fairness

    [https://rss.arxiv.org/abs/2402.01276](https://rss.arxiv.org/abs/2402.01276)

    本文研究了在数据异构性情况下的联邦取消学习（FU）的稳定性和公平性问题。我们提出了关键指标来评估FU，重点考虑了验证、全局稳定性和局部公平性，并研究了其中的权衡。通过理论分析，我们洞察了数据异构性对FU的影响，并提出了管理权衡的FU机制。通过实证验证，我们证实了这些机制的有效性。

    

    本文探讨了在数据异构性情况下联邦取消学习（FU）的多方面影响。我们介绍了FU评估的关键指标，重点关注验证，全局稳定性和局部公平性，并研究了内在的权衡。此外，我们通过一个优化框架对具有数据异构性的取消学习过程进行了形式化。我们的核心贡献在于对FU中权衡进行了全面的理论分析，并提供了数据异构性对FU的影响的见解。利用这些见解，我们提出了管理权衡的FU机制，为FU机制的进一步发展提供指导。我们通过实证验证了我们的FU机制有效地平衡了权衡，确认了从我们的理论分析中得出的见解。

    This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.
    
[^3]: FAST: 用于加速Transformers的可分解注意力机制

    FAST: Factorizable Attention for Speeding up Transformers

    [https://arxiv.org/abs/2402.07901](https://arxiv.org/abs/2402.07901)

    该论文介绍了一种可以加速Transformers模型的可分解注意力机制，通过引入因子分解形式的注意力，将注意力机制的复杂度从O(N^2)降低到O(N)，并 在维持注意力矩阵完整表示的同时保持稀疏性和所有-所有令牌关系。实验证明该注意力机制具有稳健的性能，并在不同应用中具有重要潜力。

    

    在原始的快速多极方法和改进后的快速高斯变换所固有的因子分解的驱动下，我们提出了一种在高维度中高效运行的可分解注意力形式。这种方法将Transformers中的注意力机制的计算和存储复杂度从O(N^2)降低到O(N)。与之前的尝试相比，我们的工作呈现了一个线性缩放的注意力机制，既保持了注意力矩阵的完整表示，又不妥协于稀疏化，并且包含了令牌之间的全互操作关系。我们探索了我们新的注意力度量的属性，并在各种标准设置下进行了测试。结果表明，我们的注意力机制具有稳健的性能，并在使用自我注意力的多样的应用中具有重要的潜力。

    Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.
    
[^4]: 使用自定义数据集的机器学习方法检测拉布拉多豆上的蜘蛛螨

    Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets

    [https://arxiv.org/abs/2402.07895](https://arxiv.org/abs/2402.07895)

    本研究提出了一种使用机器学习方法检测拉布拉多豆上蜘蛛螨的视觉方法，通过构建RGBN数据集并使用两阶段早期病害检测模型，相比于单阶段分割模型，提高了mAP。使用RGBN数据的顺序CNN模型在分类中也取得了较高的准确率。

    

    在不断增长的粮食生产需求中，早期植物病害的检测对保护作物至关重要；本研究提出了一种利用在真实环境条件下通过JAI FS-1600D-10GE相机采集的RGB和NIR数据构建RGBN数据集进行植物病害检测的视觉机器学习方法。使用YOLOv8和顺序CNN的两阶段早期植物病害检测模型，在具有部分标签的数据集上进行训练，相对于单阶段端到端分割模型提高了3.6％的mAP。顺序CNN模型使用RGBN数据实现了90.62％的验证准确率。使用ResNet15和顺序CNN模型在分类中使用RGBN相比仅使用RGB，平均验证准确率提高了6.25％。进一步的研究和数据集改进需要以满足粮食生产需求。

    Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset. A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model. The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data. An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models. Further research and dataset improvements are needed to meet food production demands.
    
[^5]: MAIDCRL: 半集中式多智能体影响密集卷积神经网络强化学习

    MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning

    [https://arxiv.org/abs/2402.07890](https://arxiv.org/abs/2402.07890)

    MAIDCRL 是一种半集中式密集神经网络强化学习算法，通过引入智能体影响图(AIMs)和卷积层，成功在 StarCraft 多智能体挑战中实现了有效的多智能体控制。CNN-enabled MAIDCRL 在学习性能上显著提高，并在复杂的异质场景中取得了更快的学习速度。

    

    在多智能体系统中的分布式决策制定中，对于合作和竞争系统中的交互行为学习，存在困难挑战。为了减轻这种复杂性，MAIDRL提出了一种加强了智能体影响图(AIMs)的半集中式密集强化学习算法，用于学习在StarCraft多智能体挑战(SMAC)场景中的有效多智能体控制。本文在MAIDRL中扩展了DenseNet，引入了半集中式多智能体密集卷积神经网络强化学习(MAIDCRL)，通过将卷积层融入深度模型架构，并在同质和异质场景上评估了性能。结果显示，启用了CNN的MAIDCRL在学习性能上显著提高，并且相对于现有的MAIDRL，特别是在更复杂的异质SMAC场景上，实现了更快的学习速度。我们进一步研究了模型的稳定性和鲁棒性。统计数据显示...

    Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect tha
    
[^6]: WildfireGPT：针对野火分析的定制化大型语言模型

    WildfireGPT: Tailored Large Language Model for Wildfire Analysis

    [https://arxiv.org/abs/2402.07877](https://arxiv.org/abs/2402.07877)

    WildfireGPT是一个针对野火分析的定制化大型语言模型，通过提供领域特定的上下文信息和科学准确性，将用户查询转化为关于野火风险的可操作见解。

    

    大型语言模型（LLMs）的最新进展代表了人工智能（AI）和机器学习（ML）领域的一种变革性能力。然而，LLMs是通用模型，训练于广泛的文本语料库，往往难以提供特定上下文信息，尤其是在需要专业知识的领域，比如野火细节在更广泛的气候变化背景下。对于关注野火弹性和适应性的决策者和政策制定者来说，获取不仅准确而且领域特定的响应至关重要，而不是泛泛而谈。为此，我们开发了WildfireGPT，一个原型LLM代理，旨在将用户查询转化为关于野火风险的可操作见解。我们通过提供气候预测和科学文献等额外上下文信息来丰富WildfireGPT，以确保其信息具有时效性、相关性和科学准确性。这使得WildfireGPT成为一个有效的工具来解决实际问题。

    The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML). However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change. For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for del
    
[^7]: 使用语言反馈模型来改进政策

    Policy Improvement using Language Feedback Models

    [https://arxiv.org/abs/2402.07876](https://arxiv.org/abs/2402.07876)

    本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。

    

    我们引入了语言反馈模型（LFMs），用于在指令遵循中识别期望的行为-有助于实现指令中指定任务的行动-以进行模仿学习。为了训练LFMs，我们从大型语言模型（LLMs）获取对视觉轨迹进行语言描述的反馈。首先，通过使用LFMs识别期望模仿的行为，我们在三种不同的语言基础环境（Touchdown，ScienceWorld和ALFWorld）上，在任务完成率上改善了强行为克隆的基线方法。其次，与LLMs直接预测行动相比，使用LFMs在LLM输出标记的数量相同的情况下表现更好。第三，LFMs适应未见环境，通过一轮适应使任务完成率提高了3.5-12.0％。最后，可以修改LFM以提供人类可解释的反馈，无需性能损失，从而允许人类验证模仿学习的期望行为。

    We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
    
[^8]: 线性二次控制中策略梯度的隐性偏差：对未见初始状态的外推

    Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States

    [https://arxiv.org/abs/2402.07875](https://arxiv.org/abs/2402.07875)

    本文研究了策略梯度在线性二次调节控制中对未见初始状态的外推问题，发现外推程度取决于训练中系统的探索程度。

    

    在现代机器学习中，模型可以以多种方式拟合训练数据，其中一些在未见（测试）数据上表现良好，而其他一些则不然。有趣的是，在这种情况下，梯度下降经常展现出一种隐性偏差，导致在未见数据上表现出色。这种隐性偏差在监督学习中已经得到了广泛研究，但在最优控制（强化学习）中却了解得较少。在那里，通过梯度下降学习应用于系统的控制器被称为策略梯度，并且一个非常重要的问题是学习的控制器在对未见初始状态的外推程度。本文在理论上研究了策略梯度在对未见初始状态的外推方面的隐性偏差。我们以基本的线性二次调节器（LQR）问题为重点，确立了外推程度取决于训练中系统在初始状态下引起的探索程度。

    In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
    
[^9]: 细粒度混合专家模型的标度律

    Scaling Laws for Fine-Grained Mixture of Experts

    [https://arxiv.org/abs/2402.07871](https://arxiv.org/abs/2402.07871)

    本研究分析了细粒度混合专家模型的标度特性，并引入了粒度作为新的超参数，通过调整粒度可以精确控制专家的大小。研究结果显示，MoE模型在效果上始终优于密集变压器模型，并且随着模型大小和训练预算的增大，密集和MoE模型之间的效率差距也在增大。同时，将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。

    

    混合专家（MoE）模型已成为减少大型语言模型计算成本的主要解决方案。在这项工作中，我们分析了它们的标度特性，并纳入了更广泛的变量范围。具体地，我们引入了一个新的超参数，称为粒度，通过调整粒度可以精确控制专家的大小。基于此，我们建立了细粒度MoE的标度律，考虑了训练标记数、模型大小和粒度。利用这些定律，我们推导出了给定计算预算下的最佳训练配置。我们的研究结果不仅表明MoE模型始终优于密集变压器模型，而且还凸显了在扩大模型大小和训练预算时，密集和MoE模型之间的效率差距在扩大。此外，我们证明了将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。

    Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.
    
[^10]: 透视VLMs：探索视觉条件化语言模型的设计空间

    Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models

    [https://arxiv.org/abs/2402.07865](https://arxiv.org/abs/2402.07865)

    本论文探索了视觉条件化语言模型（VLMs）设计的关键空间，并提供了一套标准化评估，同时还研究了预训练的视觉表示和权衡的问题。

    

    视觉条件化语言模型（VLMs）在视觉对话、场景理解和机器人任务规划等应用中得到了越来越多的应用，这种应用促使了像LLaVa、InstructBLIP和PaLI-3等许多新模型的出现。尽管有这么多新的发布，但关于图像预处理、架构和优化的关键设计决策仍然未被充分探索，这使得我们很难理解模型性能的因素，这一挑战又因缺乏客观、一致的评估而变得更加复杂。为了填补这些空白，我们首先编制了一套标准化评估，涵盖了视觉问答、从语言中定位物体以及探索幻觉等属性的目标挑战集，这些评估可以提供关于VLM能力的精细、准确的见解。其次，我们对关键的设计轴进行了严格的研究，包括预训练的视觉表示和使用的权衡。

    Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
    
[^11]: AI增强预测：LLM助手提高人类预测准确性

    AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy

    [https://arxiv.org/abs/2402.07862](https://arxiv.org/abs/2402.07862)

    本研究发现，使用LLMs助手可以显著提高预测准确性，不仅仅是由于模型预测准确性的提升。

    

    大型语言模型(LLMs)展现出令人印象深刻的能力，在许多领域与甚至超过人类表现。本研究探讨了LLMs在预测任务中增强判断力的潜力。我们评估了两个GPT-4-Turbo助手对预测准确性的影响：一个旨在提供高质量建议（超级预测），另一个旨在过于自信和基本概率忽视。参与者（N = 991）可以在整个研究过程中咨询他们被分配的LLM助手，而对照组则使用一个较低级别的模型（DaVinci-003），不提供直接的预测支持。我们的注册分析显示，LLM增强显著提高了23%的预测准确性，无论是对于任何一种助手类型，相比于对照组。这种改进发生在超级预测助手在预测中更高的准确性的情况下，表明增强的效益不仅仅是由于模型预测准确性。

    Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Explora
    
[^12]: 论文竞标中检测作者与审稿人勾结的方法研究

    On the Detection of Reviewer-Author Collusion Rings From Paper Bidding

    [https://arxiv.org/abs/2402.07860](https://arxiv.org/abs/2402.07860)

    本文研究了如何从论文竞标中检测勾结团体，以解决同行评审系统中的欺诈问题。

    

    计算机科学会议的同行评审系统面临的主要威胁是审稿人之间存在的"勾结团体"。在这种勾结团体中，提交了自己的论文的审稿人合作，试图通过操纵会议的论文分配，以便被指派为彼此论文的审稿人。纵观现有的研究，虽然已经发展出了有效的技术来检测其他种类的欺诈行为，但尚未有研究证明检测勾结团体的可能性。本研究解决了如何从论文竞标中检测勾结团体的问题。

    A major threat to the peer-review systems of computer science conferences is the existence of "collusion rings" between reviewers. In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers. The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding. One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action. While prior work has has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible. In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding. To answ
    
[^13]: Lissard：长而简单的顺序推理数据集

    Lissard: Long and Simple Sequential Reasoning Datasets

    [https://arxiv.org/abs/2402.07859](https://arxiv.org/abs/2402.07859)

    Lissard是一个包含七个任务的基准，用于评估模型处理和生成各种序列长度的能力，需要重复的过程执行。评估结果显示随着序列复杂性增加，所有模型的性能都呈一致下降趋势。

    

    语言模型现在能够解决需要处理数十万个标记的长序列的任务。然而，它们在需要重复使用简单规则的任务上常常失败，甚至在比训练中看到的序列要短得多的情况下也是如此。例如，最先进的LLMs可以在两个列表中找到共同项，列表中的项最多可达20个，但是当列表中的项达到80个时，它们会失败。在本文中，我们介绍了Lissard，这是一个包含七个任务的基准，旨在评估模型处理和生成各种序列长度的能力，需要重复的过程执行。我们评估了开源模型（Mistral-7B和Mixtral-8x7B）和专有模型（GPT-3.5和GPT-4），结果显示随着序列复杂性增加，所有模型的性能都呈一致下降趋势。数据集和代码可在https://github.com/unicamp-dl/Lissard获得。

    Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard
    
[^14]: 使用无监督度量优化GNN进行节点聚类的研究

    An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering

    [https://arxiv.org/abs/2402.07845](https://arxiv.org/abs/2402.07845)

    本研究展示了使用无监督度量模块性优化GNN进行节点聚类的方法，且无需与基准值进行比较。在设计合成实验的过程中，我们发现了这种方法的局限性。

    

    图神经网络（GNN）可以通过学习特征和连接信息的二元性来训练以检测图中的社区。目前，优化GNN的常见方法是使用与基准值的比较来进行超参数调整和模型选择。本研究表明，仅通过优化模块性，可以使用GNN将节点聚类成社区，而无需与基准值进行比较。尽管模块性是一种图分区质量度量，我们证明这也可以用于优化同时编码特征的GNN，并且不会降低性能。我们进一步研究无监督度量性能是否能够预测基准值的性能。为了探究为什么可以使用模块性优化GNN，我们设计了一些合成实验来展示这种方法的局限性。这些合成图表明其在不同、随机和零信息空间分区中的当前能力。

    Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in att
    
[^15]: 通过局部最优网络理解形态进化中的适应度景观

    Understanding fitness landscapes in morpho-evolution via local optima networks

    [https://arxiv.org/abs/2402.07822](https://arxiv.org/abs/2402.07822)

    本研究通过局部最优网络分析不同编码方法在演化机器人运动任务中产生的适应度景观结构，为搜索过程提供新的启示。

    

    形态进化是指在给定任务和环境的情况下，同时优化机器人设计和控制器以最大化性能。许多基因编码方法已被提出，能够表示设计和控制。先前的研究以目标函数的性能和评估的设计多样性为指标，对编码方法进行了经验比较，但尚未尝试解释观察到的结果。本研究通过应用局部最优网络（LON）分析，调查三种不同编码方法在演化机器人运动任务中产生的适应度景观结构，为搜索过程在不同适应度景观上的遍历提供新的启示。这是LON分析首次应用于形态进化领域，尽管在组合优化领域中非常受欢迎；研究结果将有助于设计新的算法。

    Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's design and controller to maximise performance given a task and environment. Many genetic encodings have been proposed which are capable of representing design and control. Previous research has provided empirical comparisons between encodings in terms of their performance with respect to an objective function and the diversity of designs that are evaluated, however there has been no attempt to explain the observed findings. We address this by applying Local Optima Network (LON) analysis to investigate the structure of the fitness landscapes induced by three different encodings when evolving a robot for a locomotion task, shedding new light on the ease by which different fitness landscapes can be traversed by a search process. This is the first time LON analysis has been applied in the field of ME despite its popularity in combinatorial optimisation domains; the findings will facilitate design of new algorithms o
    
[^16]: 可扩展大型语言模型微调的差分隐私零阶方法

    Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning

    [https://arxiv.org/abs/2402.07818](https://arxiv.org/abs/2402.07818)

    本文研究了差分隐私零阶方法在大型语言模型微调中的应用，该方法通过使用零阶梯度来避免传统优化方法的可扩展性瓶颈，实现了在隐私、效用和可扩展性之间的良好平衡。

    

    在特定任务的数据集上进行微调是利用预训练语言模型的强大能力进行各种下游任务的广泛接受的范例。由于预训练语言模型微调的普及以及与之相关的隐私问题，差分隐私预训练语言模型微调引起了越来越多的关注，以保护特定任务数据集的隐私。差分隐私预训练语言模型微调方法的设计核心是在隐私、效用和可扩展性之间达到满意的权衡。大多数现有方法都是基于DP-SGD的创新性工作。尽管将DP-SGD的可扩展性推到了极限，但基于DP-SGD的微调方法不幸地受到了SGD固有低效率的限制。在本文中，我们研究了DP零阶方法在LLM预训练中的潜力，该方法通过用更高效的零阶梯度来近似梯度，避免了SGD的可扩展性瓶颈。与将零阶方法作为一种替代方法进行处理不同，我们引入了一种新的割接框架，该框架能够以非常接近的方式模拟DP-SGD的基本操作，然后利用零阶优化方法来近似梯度。

    Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
    
[^17]: PBADet：一种用于部件-身体关联的单阶段无锚点方法

    PBADet: A One-Stage Anchor-Free Approach for Part-Body Association

    [https://arxiv.org/abs/2402.07814](https://arxiv.org/abs/2402.07814)

    PBADet是一种用于部件-身体关联的单阶段无锚点方法，通过引入部件-身体中心偏移量来有效表达部件与身体之间的关系，具有高效、准确和稳健的特点，相比现有技术具有更好的性能。

    

    人体部件（如手、脸）的检测及其与个人的正确关联是一项重要任务，例如用于普适人机界面和动作识别。传统方法通常采用多阶段流程，依赖繁琐的基于锚点的系统，或者在处理较大的部分集时扩展性差。本文提出了一种名为PBADet的新型单阶段、无锚点的部件-身体关联检测方法。基于多尺度特征图的无锚点物体表示，我们引入了一个单一的部件-身体中心偏移量，有效地表达了部件与其父身体之间的关系。我们的设计具有很强的通用性，能够处理多个部件-身体关联，同时不影响检测准确性和稳健性。在各种数据集上的全面实验证明了我们方法的有效性，不仅超越了现有的最先进技术，而且还提供了更加简化和高效的解决方案。

    The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets. This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and eff
    
[^18]: 检索增强的思维过程作为序列决策制定

    Retrieval-Augmented Thought Process as Sequential Decision Making

    [https://arxiv.org/abs/2402.07812](https://arxiv.org/abs/2402.07812)

    检索增强思维过程（RATP）通过多步决策和蒙特卡洛树搜索，以及Q值估计器，解决了大型语言模型在隐私、产生幻觉和处理长文本方面的挑战，并在处理私人数据的问答任务中实现了50%的性能提升。

    

    大型语言模型(LLM)展示了其强大的辅助人类并展现出"智能的火花"的能力。然而，几个开放挑战阻碍了它们的广泛应用：如对隐私的关注、倾向于产生幻觉、难以处理长文本。在本研究中，我们通过引入检索增强思维过程(RATP)来解决这些挑战。通过获取外部知识，RATP将LLM的思考生成过程定式为多步决策过程。为了优化这种思考过程，RATP利用蒙特卡洛树搜索，并学习了一个Q值估计器，实现了高效的推理。在处理具有私人数据的问答任务时，LLM训练方法受到伦理和安全问题的限制。RATP在上下文检索增强语言模型的基础上实现了50%的性能提升。

    Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
    
[^19]: 广义化规划环境重新设计

    Generalising Planning Environment Redesign

    [https://arxiv.org/abs/2402.07799](https://arxiv.org/abs/2402.07799)

    本论文提出了一种广义化的规划环境重新设计方法，该方法能够不受度量影响，并能够适应不同的目标和指标。

    

    在环境设计中，一个感兴趣的方与另一个代理通过对环境进行改变来影响其决策。大部分关于规划环境重新设计的研究假设感兴趣的方的目标是促进目标和计划的识别，并在环境修改空间中搜索以找到简化这些任务并优化特定度量标准的最小一组变化。这个搜索空间通常难以处理，因此现有方法设计了度量相关的修剪技术以更高效地进行搜索。这导致方法无法在不同的目标和/或指标上进行泛化。在本文中，我们认为感兴趣的方的目标和指标不一定与识别代理的目标或计划相关。因此，为了广义化规划环境重新设计的任务，我们开发了一种通用的环境重新设计方法，该方法不受度量影响，并利用了最新的顶层研究成果。

    In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment. Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric. This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently. This results in approaches that are not able to generalise across different objectives and/or metrics. In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans. Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top
    
[^20]: 可扩展的多粒度融合网络用于基于方面的情感分析

    Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis

    [https://arxiv.org/abs/2402.07787](https://arxiv.org/abs/2402.07787)

    这篇论文提出了一种可扩展的多粒度融合网络（EMGF）用于基于方面的情感分析，通过整合不同的语言和结构特征，包括句法依赖、组成、注意力语义和外部知识图谱等，来提高情感分析的性能和准确性。

    

    基于方面的情感分析（ABSA）评估文本中的情感表达以理解情感信息。先前的研究整合了外部知识，如知识图谱，以加强ABSA模型中的语义特征。最近的研究探讨了在依赖和组成树上使用图神经网络（GNN）进行句法分析。随着ABSA的不断发展，越来越多的创新的语言和结构特征被融入其中（例如潜在图），但这也引入了复杂性和混淆。目前，尚不存在一个可扩展的框架，可以将多样性的语言和结构特征集成到ABSA中。本文介绍了可扩展的多粒度融合（EMGF）网络，它整合了来自句法依赖和组成、注意力语义和外部知识图谱的信息。EMGF配备了多锚点三元学习和正交投影，高效地利用了这些特征的综合潜力。

    Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of 
    
[^21]: 不确定性下公平多目标优化的端到端学习

    End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty

    [https://arxiv.org/abs/2402.07772](https://arxiv.org/abs/2402.07772)

    本文将机器学习中的“先预测再优化”（PtO）范式扩展到具有不可微分的有序加权平均（OWA）目标的优化问题上，并展示了如何有效地将OWA函数的优化与参数推断相结合。

    

    许多人工智能和运筹学中的决策过程是由参数优化问题建模的，其定义参数是未知的，必须从可观测数据中推断出来。机器学习中的“先预测再优化”（PtO）范式旨在通过训练参数推断模型与后续的约束优化来最大化下游决策质量。这需要通过适用于问题形式的逼近技术，特别是对于不可微分的线性和混合整数程序，通过优化问题进行反向传播。本文将PtO方法扩展到具有不可微分的有序加权平均（OWA）目标的优化问题上，OWA目标在决策模型中保证公平性和鲁棒性的能力是众所周知的。通过一系列的训练技巧和提出的应用设置，本文展示了如何有效地将OWA函数的优化与参数推断相结合。

    Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric pred
    
[^22]: 对Transformer中逐步推理的理解: 一个合成图导航模型的研究

    Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model

    [https://arxiv.org/abs/2402.07757](https://arxiv.org/abs/2402.07757)

    该论文研究了Transformer中的逐步推理，提出了一个合成图导航模型来探索逐步推理的底层机制，并通过该模型在合成任务上的实验证明了几个关键现象的存在。

    

    逐步推理协议，如scratchpads和chain-of-thought，通过将复杂问题分解为一系列较简单的子问题，帮助语言模型解决复杂问题。尽管这些协议在性能上取得了显著的提升，但逐步推理的底层机制仍然难以理解。为了解决这个问题，我们提出在合成任务中研究自回归Transformer模型，该任务体现了问题的多步性质，其中逐步推理通常最有用。具体而言，我们定义了一个图导航问题，模型的任务是在图上从起始节点到目标节点的路径上进行遍历。尽管简单，我们发现我们可以通过经验重现和分析观察到的几个现象：(i)逐步推理推理间隙，我们发现其原因在于训练数据的结构；(ii)在模型生成中的多样性-准确性权衡，随着采样温度的变化；(iii)模型在输出中的简单性偏见。

    Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's out
    
[^23]: 思想传播：扩散语言模型中的思维链推理

    Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models

    [https://arxiv.org/abs/2402.07754](https://arxiv.org/abs/2402.07754)

    本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。

    

    扩散模型在文本处理中引起了关注，相对传统的自回归模型具有许多潜在优势。本文探讨了将扩散模型与思维链（CoT）集成的方法，CoT是一种在自回归语言模型中改进推理能力的成熟技术。我们提出了思维扩散（DoT）模型，允许推理步骤通过扩散过程在时间上传播。与传统的自回归语言模型逐个token从左到右做出决策的方式相比，DoT在计算和推理性能之间具有更大的灵活性。我们的实验证明了DoT在多位数乘法和小学数学问题中的有效性。此外，DoT展示了有希望的自我纠正能力，并从现有的增强推理技术（如自一致解码）中受益。我们的发现有助于理解和发展推理能力。

    Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
    
[^24]: 实现智能体、人类和环境之间的统一对齐

    Towards Unified Alignment Between Agents, Humans, and Environment

    [https://arxiv.org/abs/2402.07744](https://arxiv.org/abs/2402.07744)

    本文介绍了统一对齐原则 ($\mathbf{UA}^2$)，旨在实现智能体与人类意图、环境动态和自我约束的统一对齐，提出了引入实际特性进行概念验证研究的方法。

    

    基于基础模型的快速进展导致了自主智能体的繁荣，这些智能体利用基础模型的通用能力进行推理、决策和环境交互。然而，当在复杂、现实的环境中运行时，智能体的效能仍然有限。在本研究中，我们引入了统一对齐原则，即同时对齐智能体与人类意图、环境动态和自我约束（如货币预算限制）。从统一对齐 ($\mathbf{UA}^2$) 的视角出发，我们回顾了当前智能体研究的现状，并指出了现有智能体基准和方法候选中被忽视的因素。我们还通过为WebShop引入实际特性进行了概念验证研究，包括使用用户配置文件来展示意图、个性化重新排名以应对复杂的环境动态和运行时成本统计。

    The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
    
[^25]: 模型崩溃解密：回归案例研究

    Model Collapse Demystified: The Case of Regression

    [https://arxiv.org/abs/2402.07712](https://arxiv.org/abs/2402.07712)

    本研究在核回归的简化环境中解析了模型崩溃现象，并发现了模型能够处理虚假数据与性能完全崩溃之间的交叉点。通过提出基于自适应正则化的策略，成功缓解了模型崩溃问题。这些发现通过实验证实。

    

    在像ChatGPT这样的大型语言模型的时代，"模型崩溃"现象指的是模型在递归地训练自身上一代又一代生成的数据时，其性能逐渐降低，最终变得完全无用，即模型崩溃。在这项工作中，我们在核回归的简化环境中研究了这一现象，并获得了结果，显示模型能够处理虚假数据与模型性能完全崩溃之间存在明显的交叉点。在多项式衰减的光谱和源条件下，我们获得了修改后的缩放定律，展示了从快速到缓慢速率的新交叉现象。我们还提出了基于自适应正则化的简单策略来缓解模型崩溃。我们的理论结果通过实验证实。

    In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.
    
[^26]: 基于CUDA的GPU优化稀疏卷积在3D点云上的应用

    Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA

    [https://arxiv.org/abs/2402.07710](https://arxiv.org/abs/2402.07710)

    本论文研究了在3D点云上进行稀疏卷积的GPU优化方法，以解决点云的稀疏性和计算问题。

    

    最近几年，深度学习方法的应用显著增加，特别是卷积神经网络（CNN），它们已经成为涉及结构化格网数据的各个领域中的主要方法，如图像分析和处理。然而，随着LiDAR和3D传感器在许多领域的使用呈指数增长，对3D点云的分析需求也增加了。利用3D点云在包括物体识别和分割在内的各种应用中至关重要，因为它们提供了三维环境中事物的空间描述。与照片不同，点云具有稀疏性和缺乏规则的格网，因此存在着独特的处理和计算问题。

    In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.
    
[^27]: 在线顺序决策中的未知延迟问题

    Online Sequential Decision-Making with Unknown Delays

    [https://arxiv.org/abs/2402.07703](https://arxiv.org/abs/2402.07703)

    本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。

    

    在在线顺序决策领域，我们利用在线凸优化（OCO）框架解决了具有延迟的问题，其中决策的反馈可能以未知延迟到达。与之前仅限于欧几里得范数和梯度信息的研究不同，我们提出了三个基于近似解的延迟算法族，处理不同类型的接收反馈。我们提出的算法是多功能且适用于通用范数。具体地，我们引入了一系列针对具有完整损失函数信息反馈的延迟规范化领导算法族，一系列针对具有梯度信息反馈的延迟镜像下降算法族，以及一系列针对相应决策点损失函数梯度值信息反馈的简化延迟镜像下降算法族。对于每种类型的算法，我们提供了相应的遗憾界限。

    In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
    
[^28]: OrderBkd: 通过重新定位进行的文本后门攻击

    OrderBkd: Textual backdoor attack through repositioning

    [https://arxiv.org/abs/2402.07689](https://arxiv.org/abs/2402.07689)

    本论文提出了一种通过重新定位句子中的两个单词实施文本后门攻击的方法，与已有的攻击方式相比，在攻击成功率、困惑度和与干净样本的语义相似性方面表现更好，并且对ONION防御方法具有鲁棒性。

    

    使用第三方数据集和预训练的机器学习模型对NLP系统构成威胁，可能隐藏后门攻击。现有的攻击方式包括插入标记或句子重述等污染数据样本，这要么改变了原始文本的语义，要么可以被检测出来。我们与以往工作的主要区别在于，我们使用重新定位句子中的两个单词作为触发器。通过设计并应用基于词性的规则来选择这些标记，我们在SST-2和AG分类数据集上保持了高攻击成功率，同时在困惑度和与干净样本的语义相似性方面优于现有攻击方法。此外，我们展示了我们的攻击对ONION防御方法的鲁棒性。论文中的所有代码和数据可在https://github.com/alekseevskaia/OrderBkd获取。

    The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
    
[^29]: CyberMetric: 一份用于评估大型语言模型在网络安全领域的知识的基准数据集

    CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity

    [https://arxiv.org/abs/2402.07688](https://arxiv.org/abs/2402.07688)

    CyberMetric是一个基准数据集，旨在评估大型语言模型在网络安全领域的知识。该数据集由10000个问题组成，通过合作过程将专家知识与LLMs相结合。除了评估LLMs的知识外，数据集的主要目标是促进人类与不同LLMs在网络安全领域中的公平比较。

    

    大型语言模型（LLM）在各个领域都表现出色，从计算机视觉到医学诊断。然而，理解网络安全这个涵盖密码学、逆向工程和风险评估等多样化领域的挑战即使对于人类专家来说也是困难的。在本文中，我们介绍了CyberMetric，这是一个基准数据集，包含了来自网络安全领域的标准、认证、研究论文、书籍和其他出版物的1万个问题。这些问题通过一种协作过程创建，即将专家知识与LLM（包括GPT-3.5和Falcon-180B）相结合。人类专家花费了超过200小时验证其准确性和相关性。除了评估LLM的知识外，该数据集的主要目标是在网络安全领域中促进人类与不同LLM之间的公平比较。为了实现这一目标，我们精选了80个问题，涵盖了网络安全领域的多个主题，并让30个参与者参与其中。

    Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics. However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts. In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain. The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity. To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants 
    
[^30]: 大型语言模型“评审”: 在法律领域的机器翻译效果如何？

    Large Language Models "Ad Referendum": How Good Are They at Machine Translation in the Legal Domain?

    [https://arxiv.org/abs/2402.07681](https://arxiv.org/abs/2402.07681)

    本研究评估了两种大型语言模型和传统神经机器翻译系统在法律领域的机器翻译质量，结果显示大型语言模型在产生上下文足够且流畅的译文方面表现优异，强调了人工评估方法在评估机器翻译质量中的重要性。

    

    本研究评估了两种最先进的大型语言模型（LLMs）与传统神经机器翻译（NMT）系统在法律领域四种语言对中的机器翻译质量。研究结合了自动评估指标（AEMs）和专业翻译人员进行的人工评估（HE），评估了翻译排名、流畅性和足够性。结果表明，虽然谷歌翻译在AEMs方面通常优于LLMs，但人工评估者认为LLMs，特别是GPT-4，在产生上下文足够且流畅的译文方面相当或略好于谷歌翻译。这种差异表明LLMs在处理专业法律术语和背景方面的潜力，凸显了人工评估方法在评估机器翻译质量方面的重要性。本研究强调了LLMs在专业领域的不断发展能力，并呼吁重新评估传统AEMs以更好地捕捉LLMs生成的翻译的细微差别。

    This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.
    
[^31]: AYDIV: 通过集成上下文视觉Transformer的可调节性3D物体检测

    AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer

    [https://arxiv.org/abs/2402.07680](https://arxiv.org/abs/2402.07680)

    AYDIV是一个通过集成上下文视觉Transformer来增强3D物体检测的框架，特别设计了三阶段对齐过程来增强长距离的检测能力，包括改善相机特征提取、融合LiDAR和相机细节以及全面的空间数据融合。在Waymo开放数据集上，AYDIV的性能提高了1.24%的平均精度（mA）。

    

    结合LiDAR和相机数据在自动驾驶系统中增强短距离物体检测显示出潜力。然而，由于LiDAR的稀疏数据和相机的高密度分辨率之间的对比，融合在延伸距离检测方面遇到困难。此外，两种数据表示的差异进一步复杂化了融合方法。我们引入了AYDIV，这是一个新颖的框架，集成了一个特殊设计的三阶段对齐过程，专门用于增强在数据差异中延伸距离的检测能力。AYDIV包括全局上下文融合对齐变换器（GCFAT），它改善相机特征的提取并对大规模模式进行更深入的理解；稀疏融合特征注意力（SFFA），它对LiDAR和相机细节的融合进行微调；以及用于全面空间数据融合的体积网格注意力（VGA）。AYDIV在Waymo开放数据集（WOD）上的性能提升了1.24%的平均精度（mA）。

    Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mA
    
[^32]: 合成对多模态文本和图片数据的情感控制反馈

    Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data

    [https://arxiv.org/abs/2402.07640](https://arxiv.org/abs/2402.07640)

    该论文提出了一个可控的多模态反馈合成系统，能够根据文本和图像输入生成具有特定情感（积极或消极）的反馈，有着广泛的应用价值。

    

    生成对多模态输入（包括文本和图片）的情感控制反馈能够弥补人机交互领域的一个关键差距，使系统能够提供具有同理心、准确性和引人入胜的回应。这种能力在医疗、营销和教育等领域有着深远的应用。为此，我们构建了一个大规模的可控多模态反馈合成（CMFeed）数据集，并提出了一个可控的反馈合成系统。所提出的系统包括一个编码器、解码器和控制性模块，用于处理文本和视觉输入。它使用Transformer和Faster R-CNN网络提取文本和视觉特征，并将它们结合起来生成反馈。CMFeed数据集包含图片、文本、对帖子的反应、带有相关性评分的人类评论以及对评论的反应。对帖子和评论的反应被用来训练提出的模型以产生具有特定（积极或消极）情感的反馈。

    The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative)
    
[^33]: 对信息瓶颈的限制更紧的界限，并应用于深度学习

    Tighter Bounds on the Information Bottleneck with Application to Deep Learning

    [https://arxiv.org/abs/2402.07639](https://arxiv.org/abs/2402.07639)

    这个论文提出了一个对信息瓶颈更为紧密的变分界限，可以改善以前的基于信息瓶颈的DNNs的性能，并提供了一种简单方法来显著增强分类器DNNs的对抗鲁棒性。

    

    深度神经网络（DNNs）通过下游任务、目标函数和其他参数来学习引发的潜在表示。学习到的表示的质量影响着DNN的概括能力和新出现的潜在空间的连贯性。信息瓶颈（IB）提供了一种理论上最优的数据建模框架，但通常是难以处理的。最近的研究工作将DNNs与IB相结合，通过应用VAE-inspire的变分方法来近似相互信息的界限，从而提高对抗攻击的鲁棒性。本文引入了一种新的和更紧的变分界限，提高了以前IB-inspire DNNs的性能。这些进展加强了IB及其变分近似作为数据模型框架的论点，并为分类器DNNs的对抗鲁棒性提供了一种简单的方法。

    Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.
    
[^34]: 过于自信和缺乏自信的人工智能阻碍人机协作

    Overconfident and Unconfident AI Hinder Human-AI Collaboration

    [https://arxiv.org/abs/2402.07632](https://arxiv.org/abs/2402.07632)

    人工智能的过度自信和缺乏自信会阻碍人机协作，披露信心水平和提供反馈有助于认识到人工智能的信心不一致，但参与者往往因此不信任人工智能的建议，导致协作结果较差。

    

    随着人工智能的进步，人机协作在专业和日常场景中越来越普遍。在这种协作中，人工智能可以表达其对自己表现的信心水平，作为人类评估人工智能建议的重要指标。然而，人工智能可能表现出过度自信或缺乏自信，即其表达的信心高于或低于其实际表现，这可能导致人们错误地评估人工智能的建议。我们的研究调查了人工智能过度自信和缺乏自信对人类信任、接受人工智能建议和协作结果的影响。我们的研究发现，披露人工智能的信心水平和表现反馈有助于更好地认识人工智能信心不一致。然而，参与者往往会因为察觉到这种不一致而不信任人工智能的建议，导致拒绝人工智能的建议，并且在协作任务中表现更差。相反，没有这些提示的情况下，参与者更容易信任人工智能并接受其建议，从而在协作任务中表现更好。

    As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without su
    
[^35]: AutoMathText：使用语言模型进行数学文本的自主数据选择

    AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts

    [https://arxiv.org/abs/2402.07625](https://arxiv.org/abs/2402.07625)

    本论文介绍了一种自主数据选择策略，利用语言模型进行数学文本的自动评估和选择，并通过连续预训练显著提高了数学推理能力。主要创新包括利用元提示语言模型作为验证器，发布了高质量的AutoMathText数据集，并实现了预训练令牌效率的提升。

    

    为了通过持续的预训练改善语言模型在数学推理方面的能力，我们引入了一种新颖的策略，利用基础语言模型进行自主数据选择。与传统的有人工标注数据的监督微调或训练过的分类器不同，我们的方法利用元提示语言模型作为零样本验证器，自主评估和选择高质量的数学内容，并发布了经过策划的开源AutoMathText数据集，其中包含超过200GB的数据。为了证明我们方法的有效性，我们对AutoMathText数据集进行了连续预训练，使得7B参数的Mistral语言模型在MATH数据集上的下游性能大幅提升，而令牌数量比之前的连续预训练工作减少了几个数量级。我们的方法展示了基准的预训练令牌效率提高了2倍，突显了我们方法在增强中的潜力。

    To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
    
[^36]: 从众包呼吸声数据开发多变量COVID-19预测模型

    Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data

    [https://arxiv.org/abs/2402.07619](https://arxiv.org/abs/2402.07619)

    本研究开发了一个深度学习模型，通过声音录制数据识别COVID-19。通过提取语音特征并采用不同的深度学习分类模型，HuBERT实现了86%的最高准确率。

    

    COVID-19已经影响了全球223个国家，在后COVID时代，迫切需要非侵入性、低成本、高可扩展性的解决方案来检测COVID-19。我们开发了一个深度学习模型，从语音录制数据中识别COVID-19。这项工作的创新之处在于仅使用语音录音开发COVID-19识别的深度学习模型。我们使用了剑桥COVID-19声音数据库，其中包含了893个语音样本，通过COVID-19声音应用程序由4352名参与者众包而来。提取了包括Mel频谱图和Mel频率倒谱系数(MFCC)在内的语音特征以及CNN编码器特征。基于语音数据，我们开发了深度学习分类模型来检测COVID-19病例。这些模型包括长短期记忆(LSTM)、卷积神经网络(CNN)和隐藏单元BERT(HuBERT)。我们将它们的预测能力与基线机器学习模型进行了比较。HuBERT取得了86%的最高准确率。

    COVID-19 has affected more than 223 countries worldwide and in the Post-COVID Era, there is a pressing need for non-invasive, low-cost, and highly scalable solutions to detect COVID-19. We develop a deep learning model to identify COVID-19 from voice recording data. The novelty of this work is in the development of deep learning models for COVID-19 identification from only voice recordings. We use the Cambridge COVID-19 Sound database which contains 893 speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app. Voice features including Mel-spectrograms and Mel-frequency cepstral coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice data, we develop deep learning classification models to detect COVID-19 cases. These models include Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power to baseline machine learning models. HuBERT achieves the highest accuracy of 86\
    
[^37]: 基于锚点的大型语言模型

    Anchor-based Large Language Models

    [https://arxiv.org/abs/2402.07616](https://arxiv.org/abs/2402.07616)

    基于锚点的大型语言模型（AnLLM）通过引入创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略，将序列信息压缩到锚点标记中，减少键/值缓存，提高推理效率。

    

    大型语言模型（LLMs）主要采用仅解码器的转换器架构，需要保留历史标记的键/值信息以提供上下文信息并避免冗余计算。然而，这些LLMs的巨大大小和参数量需要大量的GPU内存。这种内存需求随着输入文本的长度而增加，迫切需要更高效的信息存储和处理方法。本研究介绍了一种基于锚点的LLM（AnLLM），它利用了一种创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略。这种方法使LLMs能够将序列信息压缩成锚点标记，减少键/值缓存并提高推理效率。实验证明，AnLLM在减少键/值缓存高达99%和推理速度提高高达3.5倍的同时，仍保持可比的准确性。尽管牺牲了一些准确性，AnLLM的创新和贡献依然重要。

    Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
    
[^38]: 踩脚调校：通过自助引导扩展LLM的自对齐能力的规模化方法

    Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping

    [https://arxiv.org/abs/2402.07610](https://arxiv.org/abs/2402.07610)

    本文首次探索了自助引导自对齐对大型语言模型的影响，发现其明显优于单次循环的方法，并通过调整数据训练顺序进一步提升模型性能。

    

    自对齐是一种降低人工注释成本并确保模型能力的有效方法。然而，大多数当前的方法在单次循环中完成数据收集和训练步骤，可能忽视了自对齐模型不断改进的能力。这引发了一个关键问题：如果我们进行多次自助引导自对齐，会增强模型性能还是导致快速退化？本文首次探索了自助引导自对齐对大型语言模型的影响。我们的研究结果表明，通过保证从上下文学习中获得的数据多样性，自助引导自对齐明显优于单次循环的方法。为了进一步发挥自助引导的能力，我们还研究并调整了数据的训练顺序，从而提高了模型的性能。基于这些发现，我们提出了踩脚调校（SOFT）的方法，利用模型的持续增强能力。

    Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
    
[^39]: 只有曲线形状有关：通过下一个曲线形状预测训练基础模型进行零样本多元时间序列预测

    Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction

    [https://arxiv.org/abs/2402.07570](https://arxiv.org/abs/2402.07570)

    通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。

    

    我们提出了General Time Transformer (GTT)，一种仅有编码器的基础模型，用于零样本多元时间序列预测。GTT在一个包含2亿个高质量时间序列样本的大型数据集上进行预训练，涵盖了不同领域。在我们提出的框架中，多元时间序列预测的任务被建模为一个逐通道的下一个曲线形状预测问题，其中每个时间序列样本表示为一系列非重叠的曲线形状，具有统一的数值大小。GTT在通道级别上通过预测过去曲线形状的窗口来预测下一个曲线形状。实验结果表明，GTT在未见时间序列数据集上展现出优秀的零样本多元预测能力，甚至超过了最先进的有监督基线模型。此外，我们还研究了GTT模型参数和训练数据集规模变化的影响，观察到在零样本多元预测的背景下，规模定律也成立。

    We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
    
[^40]: 确保智能逻辑代理的可信和道德行为

    Ensuring trustworthy and ethical behaviour in intelligent logical agents

    [https://arxiv.org/abs/2402.07547](https://arxiv.org/abs/2402.07547)

    该论文提出了基于动态逻辑的自检技术，旨在确保智能逻辑代理的可信和道德行为。

    

    自主智能代理在许多应用中被使用，这些应用可能涉及到生命和福祉以及重要的社会功能。因此，代理应该是可信的。先验认证技术（即在系统部署之前应用的技术）可能是有用的，但对于演变的代理和开放的多代理系统来说，这是不够的，因为异构代理可以在系统的任何阶段加入或离开系统。在本文中，我们提出/改进/扩展了基于动态（运行时）逻辑的自检技术，目的是确保代理的可信和道德行为。

    Autonomous Intelligent Agents are employed in many applications upon which the life and welfare of living beings and vital social functions may depend. Therefore, agents should be trustworthy. A priori certification techniques (i.e., techniques applied prior to system's deployment) can be useful, but are not sufficient for agents that evolve, and thus modify their epistemic and belief state, and for open Multi-Agent Systems, where heterogeneous agents can join or leave the system at any stage of its operation. In this paper, we propose/refine/extend dynamic (runtime) logic-based self-checking techniques, devised in order to be able to ensure agents' trustworthy and ethical behaviour.
    
[^41]: 给我看怎么做：解释在细调语言模型中的作用

    Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models

    [https://arxiv.org/abs/2402.07543](https://arxiv.org/abs/2402.07543)

    本研究证明了使用解释来改进语言模型性能的显著好处，尤其适用于较小的模型，解释的加入使模型能够解决之前无法解决的任务。

    

    我们的研究证明了使用解释来增强语言模型性能的显著好处。与提示方式不同，细调允许模型在训练阶段学习和更新参数。在本研究中，我们应用细调的方法，使用包含输出解释而非仅呈现答案的数据来对不同大小的语言模型进行训练。我们发现，即使是只有6000万参数的较小语言模型也能从这种方法中获益。有趣的是，我们的结果表明，详细的解释对较小的模型更有益处，而对于较大的模型来说，无论解释的长度如何，都可以获得几乎相同的优势。此外，我们还证明了解释的加入使模型能够解决之前无法解决的任务。最后，我们认为尽管存在挑战，但解释在细调语言模型中起到了重要作用。

    Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the chall
    
[^42]: PKG API：一个个人知识图管理工具

    PKG API: A Tool for Personal Knowledge Graph Management

    [https://arxiv.org/abs/2402.07540](https://arxiv.org/abs/2402.07540)

    本文提出了一个完整的个人知识图（PKG）管理解决方案，包括用户界面友好的PKG客户端和面向服务的PKG API，以及基于RDF的PKG词汇表用于表示陈述和访问权限。

    

    个人知识图（PKG）为个人提供了一种将碎片化的个人数据存储和整合到一个中心位置的方式，提高了服务的个性化程度同时保持用户的完全控制。尽管PKG的潜力巨大，但实际操作上具有用户友好界面的PKG实现仍然很少。本文通过提出一个完整的解决方案来表示、管理和与PKG进行交互来填补这一空白。我们的方法包括（1）一个面向用户的PKG客户端，使最终用户可以通过自然语言陈述轻松管理他们的个人数据，以及（2）一个面向服务的PKG API。为了应对在PKG中表示这些陈述的复杂性，我们提出了一种基于RDF的PKG词汇表来支持这一点，并提供了访问权限和来源的属性。

    Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.
    
[^43]: BreakGPT: 一种具有多阶段结构的大型语言模型用于金融突破检测

    BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection

    [https://arxiv.org/abs/2402.07536](https://arxiv.org/abs/2402.07536)

    BreakGPT是第一个用于金融突破检测的大型语言模型，采用多阶段结构框架，提高了答案和理由的准确性。

    

    交易区间突破（TRB）是金融交易技术分析中的一种关键方法，广泛应用于股票、期货和外汇等金融市场的交易者。然而，区分真假突破并提供正确的理由对投资者来说具有重要挑战。最近，大型语言模型在各种下游应用中取得了成功，但在金融突破检测领域的效果仍不理想。原因在于突破检测需要独特的数据和特定的知识。为了解决这些问题，我们引入了BreakGPT，这是第一个用于金融突破检测的大型语言模型。此外，我们还开发了一种名为多阶段结构的新颖框架，有效地减少了下游应用中的错误。实验结果表明，与GPT-3.5相比，BreakGPT的答案和理由的准确性提高了44%，有助于金融突破检测。

    Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the m
    
[^44]: 物理约束的机器学习作为一种核方法

    Physics-informed machine learning as a kernel method

    [https://arxiv.org/abs/2402.07514](https://arxiv.org/abs/2402.07514)

    物理约束的机器学习结合了数据方法的表达能力与物理模型的可解释性，可以用于正则化经验风险并提高估计器的统计性能。

    

    物理约束的机器学习将基于数据的方法的表达能力与物理模型的可解释性相结合。在这种背景下，我们考虑一个普通的回归问题，其中经验风险由一个偏微分方程正则化，该方程量化了物理不一致性。我们证明对于线性微分先验，该问题可以被表述为核回归任务。利用核理论，我们推导出正则化风险的最小化器的收敛速度，并表明它至少以Sobolev最小化速度收敛。然而，根据物理误差的不同，可以实现更快的收敛速度。通过一个一维示例来说明这个原理，支持一个论点：使用物理信息来正则化经验风险对估计器的统计性能有益。

    Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.
    
[^45]: 平衡的艺术：揭示和缓解葡萄牙语中的ASR偏见

    The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese

    [https://arxiv.org/abs/2402.07513](https://arxiv.org/abs/2402.07513)

    本研究通过对Whisper和MMS系统进行全面探索，评估了葡萄牙语中非正式对话语音的自动语音识别（ASR）偏见，并发现采用过采样技术可以缓解这种陈规定型偏见。

    

    在口语理解领域，像Whisper和Multilingual Massive Speech（MMS）这样的系统展示了最先进的性能。本研究致力于对Whisper和MMS系统进行全面探索，重点评估与葡萄牙语特定的非正式对话语音中的自动语音识别（ASR）偏见。我们的调查涵盖了各种类别，包括性别、年龄、肤色和地理位置。除了传统的ASR评估指标，如词错误率（WER），我们还使用p值统计显著性来分析性别偏见。此外，我们广泛研究了数据分布的影响，并从实证角度表明过采样技术缓解了这种陈规定型偏见。本研究通过MMS和Whisper的应用，在葡萄牙语环境中量化偏见方面做出了开创性的努力，为更好地理解ASR系统做出了贡献。

    In the field of spoken language understanding, systems like Whisper and Multilingual Massive Speech (MMS) have shown state-of-the-art performances. This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in automatic speech recognition (ASR) inherent to casual conversation speech specific to the Portuguese language. Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location. Alongside traditional ASR evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis. Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases. This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of ASR systems
    
[^46]: 生成式AI代理之间的秘密勾结

    Secret Collusion Among Generative AI Agents

    [https://arxiv.org/abs/2402.07510](https://arxiv.org/abs/2402.07510)

    本文汇集了人工智能和安全领域的相关概念，系统地形式化了生成式AI代理系统中的秘密勾结问题，并提出了缓解措施。通过测试各种形式的秘密勾结所需的能力，我们发现当前模型的隐写能力有限，但 GPT-4 展示了能力的飞跃。

    

    最近大型语言模型在能力上的增强为通信的生成式AI代理团队解决联合任务的应用打开了可能性。这引发了关于未经授权分享信息或其他不必要的代理协调形式的隐私和安全挑战。现代隐写术技术可能使这种动态难以检测。本文通过汲取人工智能和安全领域相关概念，全面系统地形式化了生成式AI代理系统中的秘密勾结问题。我们研究了使用隐写术的动机，并提出了各种缓解措施。我们的研究结果是一个模型评估框架，系统地测试了各种形式的秘密勾结所需的能力。我们在各种当代大型语言模型上提供了广泛的实证结果。虽然当前模型的隐写能力仍然有限，但 GPT-4 显示出能力的飞跃，这表明有必要进行进一步的研究。

    Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
    
[^47]: 基于地形GPS注册的聚类动力学用于提高速度预测

    Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations

    [https://arxiv.org/abs/2402.07507](https://arxiv.org/abs/2402.07507)

    通过使用稀疏的GPS数据和地形特征，我们提出了一种基于聚类动力学的方法来提高速度预测，以解决从数据稀缺的地理区域提取准确交通信息的挑战。

    

    在智能交通系统领域中，从数据稀缺或没有数据覆盖的地理区域中提取准确的交通信息一直是一个持久的挑战。为了解决这个问题，我们提出了使用稀疏的GPS数据点及其相关的地形和道路设计特征进行速度预测的解决方案。我们的目标是研究是否可以利用地形和基础设施的相似性来训练一个可以预测缺乏交通数据区域速度的机器学习模型。为此，我们创建了一个以地形聚类道路为中心的时间导向速度字典，它帮助我们提供选定特征配置的速度相关性。我们的结果显示出对新的和标准回归方法的定性和定量改进。所提出的框架为缺失数据交通分析的制定策略提供了新的视角。

    A persistent challenge in the field of Intelligent Transportation Systems is to extract accurate traffic insights from geographic regions with scarce or no data coverage. To this end, we propose solutions for speed prediction using sparse GPS data points and their associated topographical and road design features. Our goal is to investigate whether we can use similarities in the terrain and infrastructure to train a machine learning model that can predict speed in regions where we lack transportation data. For this we create a Temporally Orientated Speed Dictionary Centered on Topographically Clustered Roads, which helps us to provide speed correlations to selected feature configurations. Our results show qualitative and quantitative improvement over new and standard regression methods. The presented framework provides a fresh perspective on devising strategies for missing data traffic analysis.
    
[^48]: 一列火车完成两个任务：使用监督对比学习的加密流量分类框架

    One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning

    [https://arxiv.org/abs/2402.07501](https://arxiv.org/abs/2402.07501)

    本论文提出了使用监督对比学习的加密流量分类框架，利用对比学习增强数据包级别和流级别表示，并通过图数据增强捕获字节级流量图的细粒度语义不变特征。同时，采用跨级多任务学习的方法，使得数据包级别任务学习到的表示能够被流级别任务利用。

    

    随着网络安全受到广泛关注，加密流量分类已成为当前的研究重点。然而，现有方法在进行流量分类时没有充分考虑数据样本之间的共同特征，导致性能不佳。此外，它们独立地训练数据包级别和流级别的分类任务，这是冗余的，因为数据包级别任务中学习到的数据包表示可以被流级别任务利用。因此，本文提出了一种有效的模型，命名为对比学习增强的时域融合编码器（CLE-TFE）。具体而言，我们利用监督对比学习增强数据包级别和流级别表示，并在字节级流量图上进行图数据增强，以通过对比学习捕获字节之间的细粒度语义不变特征。我们还提出了跨级多任务学习的方法。

    As network security receives widespread attention, encrypted traffic classification has become the current research focus. However, existing methods conduct traffic classification without sufficiently considering the common characteristics between data samples, leading to suboptimal performance. Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task. Therefore, in this paper, we propose an effective model named a Contrastive Learning Enhanced Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised contrastive learning to enhance the packet-level and flow-level representations and perform graph data augmentation on the byte-level traffic graph so that the fine-grained semantic-invariant characteristics between bytes can be captured through contrastive learning. We also propose cross-level multi-task learning, whi
    
[^49]: T-RAG: 来自LLM战场的经验教训

    T-RAG: Lessons from the LLM Trenches

    [https://arxiv.org/abs/2402.07483](https://arxiv.org/abs/2402.07483)

    T-RAG是一个基于LLM的应用程序，用于私人企业文件问答，它结合了RAG框架和经过微调的开源LLM，并分享了构建和部署过程中的经验教训。

    

    大型语言模型（LLM）展示了惊人的语言能力，推动了将它们整合到各个领域的应用的尝试。一个重要的应用领域是对私人企业文件进行问答，其中主要考虑因素是数据安全，需要能够在本地部署的应用程序，有限的计算资源和对查询正确响应的健壮应用的需求。检索增强生成（RAG）已成为构建基于LLM的应用程序的最重要的框架。虽然构建RAG相对简单，但要使其健壮和可靠的应用程序需要广泛的定制化和相对深入的应用领域知识。我们分享了构建和部署一个基于LLM的私人组织文件问答应用的经验。我们的应用结合了RAG的使用和经过微调的开源LLM。此外，我们的系统还具有 ...

    Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which w
    
[^50]: 食物推荐作为语言处理（F-RLP）：个性化和情境化范式

    Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm

    [https://arxiv.org/abs/2402.07477](https://arxiv.org/abs/2402.07477)

    食物推荐作为语言处理（F-RLP）是一个针对食物的个性化、情境化的框架，利用大型语言模型的能力来实现更准确、个性化的食物推荐。

    

    目前最先进的基于规则和分类的食物推荐系统在实际应用和实用性方面面临着重大挑战。主要原因是大多数机器学习模型在一个几乎无限数量的类别和一个不平衡数据集中的有限样本问题上很难应对。相反，大型语言模型（LLM）作为推荐引擎的出现提供了一个有希望的途径。然而，一个通用的“作为语言处理的推荐”（RLP）方法缺乏有效的食物推荐所必需的关键组件。为了填补这一空白，我们引入了食物推荐作为语言处理（F-RLP），这是一个新颖的框架，提供了一个针对食物的定制基础设施。F-RLP利用LLMs的能力来最大化其潜力，从而为更准确、个性化的食物推荐铺平了道路。

    State-of-the-art rule-based and classification-based food recommendation systems face significant challenges in becoming practical and useful. This difficulty arises primarily because most machine learning models struggle with problems characterized by an almost infinite number of classes and a limited number of samples within an unbalanced dataset. Conversely, the emergence of Large Language Models (LLMs) as recommendation engines offers a promising avenue. However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations. To address this gap, we introduce Food Recommendation as Language Processing (F-RLP), a novel framework that offers a food-specific, tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize their potential, thereby paving the way for more accurate, personalized food recommendations.
    
[^51]: 基于得分的物理信息神经网络用于高维福克-普朗克方程

    Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations

    [https://arxiv.org/abs/2402.07465](https://arxiv.org/abs/2402.07465)

    这项研究提出了一种基于得分函数的求解器来解决高维福克-普朗克方程中的维数灾难问题。与蒙特卡洛和普通PINN相比，该方法能够更准确地处理与布朗运动相关的概率密度函数，并提供快速采样。

    

    福克-普朗克（FP）方程是随机过程中的基础偏微分方程（PDE）。然而，当处理高维FP PDE时，维数灾难（CoD）会带来挑战。尽管蒙特卡洛和普通物理信息神经网络（PINN）已经显示出应对CoD的潜力，但在处理与布朗运动相关的概率密度函数（PDF）时，两种方法都在高维度上显示出数值误差。点值PDF随着维度增加呈指数级下降，超过了数值模拟的精度，导致了相当大的误差。此外，由于其大规模采样，蒙特卡洛无法提供快速采样。通过对普通PINNs模拟对数似然（LL），将FP方程转化为一个困难的HJB方程，其误差随维数增长迅速。为此，我们提出了一种新方法，利用基于得分的求解器来拟合SDE中的得分函数。得分函数定义为概率密度函数的梯度。

    The Fokker-Planck (FP) equation is a foundational PDE in stochastic processes. However, curse of dimensionality (CoD) poses challenge when dealing with high-dimensional FP PDEs. Although Monte Carlo and vanilla Physics-Informed Neural Networks (PINNs) have shown the potential to tackle CoD, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (PDF) associated with Brownian motion. The point-wise PDF values tend to decrease exponentially as dimension increases, surpassing the precision of numerical simulations and resulting in substantial errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms the FP equation into a difficult HJB equation, whose error grows rapidly with dimension. To this end, we propose a novel approach utilizing a score-based solver to fit the score function in SDEs. The score function, defined as the gradient of t
    
[^52]: 价值装载问题的激素适应方法：预防回形针启示录？

    A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?

    [https://arxiv.org/abs/2402.07462](https://arxiv.org/abs/2402.07462)

    我们提出了HALO法规模式，使用激素分析来调节人工智能的行为模式，以解决价值装载问题中的“回形针最大化器”场景。

    

    价值装载问题对于研究人员来说是一个重要的挑战，他们旨在创建与人类价值观和偏好相一致的人工智能系统。该问题需要一种方法来定义和规范人工智能行为的安全和最优限制。在这项工作中，我们提出了HALO（激素适应通过对手过程）这个法规模式，它使用激素分析来调节人工智能的行为模式。行为激素适应是一种现象，低频率的行为具有益处，而高频率的行为则有害。通过将行为建模为变态对手过程，我们可以使用行为频率响应分析（BFRA）或行为计数响应分析（BCRA）来量化可重复行为的激素限制。我们展示了如何使用HALO来解决“回形针最大化器”场景，这是一个思想实验，其中一个未受管制的人工智能任务是将所有物质转化为回形针。

    The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
    
[^53]: OS-Copilot: 基于自我改进的通用计算机代理

    OS-Copilot: Towards Generalist Computer Agents with Self-Improvement

    [https://arxiv.org/abs/2402.07456](https://arxiv.org/abs/2402.07456)

    OS-Copilot是一个通用计算机代理的框架，能够与操作系统中的各种元素进行交互，包括网络、代码终端、文件、多媒体和第三方应用程序。使用OS-Copilot构建的自我提升的FRIDAY代理在各种计算机任务上表现出强大的泛化能力，并在通用人工智能助手基准测试中超过以前的方法35%。

    

    与计算机的自主交互一直是一个具有巨大潜力的长期挑战，最近大型语言模型（LLM）的普及显著加快了数字代理的构建进展。然而，大多数这些代理被设计用于与特定软件或网站等狭窄领域进行交互，这限制了它们在通用计算机任务中的适用性。为此，我们引入了OS-Copilot，一个构建通用代理的框架，能够与操作系统（OS）中的全面元素进行交互，包括网络、代码终端、文件、多媒体和各种第三方应用程序。我们使用OS-Copilot创建了FRIDAY，一个能够自我提升的具象化代理，用于自动化通用计算机任务。在GAIA，一个通用人工智能助手基准测试中，FRIDAY的性能超过了以前的方法35%，通过从先前任务中积累的技能，展示了对未见应用的强大概括能力。我们还提出了数字和定量的...

    Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative
    
[^54]: TriAug：用于超声乳腺病变不平衡分类的异常样本检测

    TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound

    [https://arxiv.org/abs/2402.07452](https://arxiv.org/abs/2402.07452)

    TriAug是一个用于乳腺超声图像的异常样本检测框架，通过使用三元状态增强和平衡的球形损失来提高示踪分类的准确性和异常样本检测性能。

    

    不同的疾病，如乳腺病变的组织亚型，具有严重不同的发病率。即使通过大量的示踪数据进行训练，模型在临床实际中通常遇到属于未见类别的异常样本。为了解决这个问题，我们提出了一种新的框架，基于乳腺超声图像的长尾异常样本检测任务，并配备了一种三元状态增强（TriAug），它可以提高示踪分类的准确性，同时保持良好的异常样本检测性能。与此同时，我们设计了一个平衡的球形损失来处理类不平衡的问题。

    Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.
    
[^55]: AraSpider：实现阿拉伯语到SQL的民主化

    AraSpider: Democratizing Arabic-to-SQL

    [https://arxiv.org/abs/2402.07448](https://arxiv.org/abs/2402.07448)

    AraSpider是首个阿拉伯语版本的Spider数据集，研究表明使用回译策略可以显著提高ChatGPT 3.5和SQLCoder模型在阿拉伯语NLP任务中的性能。

    

    本研究介绍了AraSpider，这是首个阿拉伯语版本的Spider数据集，旨在提升阿拉伯语社区中的自然语言处理（NLP）。该研究测试了四个多语言翻译模型在将英文翻译成阿拉伯语方面的有效性。另外，还评估了两个模型在从阿拉伯文本生成SQL查询方面的能力。结果表明，使用回译显著提高了ChatGPT 3.5和SQLCoder模型的表现，这两个模型在Spider数据集上被认为是最佳表现者。值得注意的是，ChatGPT 3.5展示了高质量的翻译，而SQLCoder在文本到SQL任务中表现出色。该研究强调了将上下文模式和采用回译策略纳入阿拉伯语NLP任务中以提高模型性能的重要性。此外，提供了详细的方法论以实现结果复现并将数据集翻译成其他语言，突显了研究促进的承诺。

    This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting 
    
[^56]: 游戏代理驱动的自由文本命令：基于LLM代码生成和行为分支的应用

    Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch

    [https://arxiv.org/abs/2402.07442](https://arxiv.org/abs/2402.07442)

    本论文提出了一种创新的游戏代理文本命令控制系统，通过使用大语言模型（LLM）进行代码生成，实现了对自由形式的自然语言命令的理解和执行，为实时语言交互游戏代理领域带来了显著贡献。

    

    已经有几种尝试实现游戏代理的文本命令控制。然而，当前的技术仅限于处理预定义格式的命令。本文提出了一种创新的游戏代理文本命令控制系统，可以理解以自由形式表达的自然语言命令。所提出的系统使用基于大语言模型（LLM）的代码生成来解释和转换自然语言命令为行为分支，并通过行为树的形式实现执行。本研究在模拟Pokémon游戏的游戏环境中进行了实证验证，并涉及多个参与者。结果证实了该系统具有理解和执行自然语言命令的能力，在实时语言交互游戏代理领域具有显著的贡献。

    Several attempts have been made to implement text command control for game agents. However, current technologies are limited to processing predefined format commands. This paper proposes a pioneering text command control system for a game agent that can understand natural language commands expressed in free-form. The proposed system uses a large language model (LLM) for code generation to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent. This study conducted empirical validation within a game environment that simulates a Pok\'emon game and involved multiple participants. The results confirmed the system's ability to understand and carry out natural language commands, representing a noteworthy in the realm of real-time language interactive game agents.   Notice for the use of this material. The copyright of this material is retained by the Japanese Society for Ar
    
[^57]: 非原子拥堵博弈中学习最优税收设计

    Learning Optimal Tax Design in Nonatomic Congestion Games

    [https://arxiv.org/abs/2402.07437](https://arxiv.org/abs/2402.07437)

    本研究致力于学习如何设计最优税收，以在非原子拥堵博弈中提高效率。为了解决指数级的税收函数空间、梯度不存在和目标函数的非凸性等挑战，该算法利用了分段线性税收、额外的线性项和有效的子例程的新颖组成部分。

    

    本研究探讨了如何学习最优税收设计，以在非原子拥堵博弈中最大化效率。众所周知，玩家之间的自利行为可能会破坏系统的效率。税务机制是缓解此问题并引导社会最优行为的常见方法。在这项工作中，我们首次采取了学习最优税收的初始步骤，该最优税收可以通过平衡反馈来最小化社会成本，即税务设计者只能观察到强制税收下的均衡状态。由于指数级的税收函数空间，梯度不存在和目标函数的非凸性，现有算法不适用。为了解决这些挑战，我们的算法利用了几个新颖的组成部分：（1）分段线性税收来近似最优税收；（2）额外的线性项来保证强凸潜力函数；（3）有效的子例程来找到“边界”税收。该算法可以找到一个$\epsilon$-最优税收，时间复杂度为$O(\bet

    We study how to learn the optimal tax design to maximize the efficiency in nonatomic congestion games. It is known that self-interested behavior among the players can damage the system's efficiency. Tax mechanisms is a common method to alleviate this issue and induce socially optimal behavior. In this work, we take the initial step for learning the optimal tax that can minimize the social cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe the equilibrium state under the enforced tax. Existing algorithms are not applicable due to the exponentially large tax function space, nonexistence of the gradient, and nonconvexity of the objective. To tackle these challenges, our algorithm leverages several novel components: (1) piece-wise linear tax to approximate the optimal tax; (2) an extra linear term to guarantee a strongly convex potential function; (3) efficient subroutine to find the ``boundary'' tax. The algorithm can find an $\epsilon$-optimal tax with $O(\bet
    
[^58]: 分析货币波动：GBP/USD和EUR/GBP货币对的GARCH、EWMA和IV模型的比较研究

    Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs

    [https://arxiv.org/abs/2402.07435](https://arxiv.org/abs/2402.07435)

    本研究比较了GARCH、EWMA和IV模型在预测GBP/USD和EUR/GBP货币对每日回报的20天变动方面的效果。研究发现EUR/GBP货币对存在非对称回报的证据，而GBP/USD货币对的证据并不一致。

    

    本研究探讨了英镑（GBP）价值的波动情况，特别关注其与美元（USD）和欧元（EUR）货币对的关系。利用2018年6月15日至2023年6月15日的数据，我们应用不同的数学模型来评估它们在预测货币对每日回报的20天变动方面的效果。我们的分析涉及指数加权移动平均（EWMA）、广义自回归条件异方差（GARCH）模型和隐含波动率（IV）模型的实施。为了评估它们的性能，我们使用均方根误差（RMSE）和平均绝对误差（MAE）指标比较它们的预测准确性。我们深入研究了GARCH模型的复杂性，检查了将其应用于所提供数据集时的统计特性。我们的研究发现，EUR/GBP货币对存在非对称回报的证据，而GBP/USD货币对的证据并不一致。

    In this study, we examine the fluctuation in the value of the Great Britain Pound (GBP). We focus particularly on its relationship with the United States Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15, 2018, to June 15, 2023, we apply various mathematical models to assess their effectiveness in predicting the 20-day variation in the pairs' daily returns. Our analysis involves the implementation of Exponentially Weighted Moving Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models, and Implied Volatility (IV) models. To evaluate their performance, we compare the accuracy of their predictions using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the intricacies of GARCH models, examining their statistical characteristics when applied to the provided dataset. Our findings suggest the existence of asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for the GBP/USD pair
    
[^59]: 用于车辆定位的粒子滤波SLAM方法

    Particle Filter SLAM for Vehicle Localization

    [https://arxiv.org/abs/2402.07429](https://arxiv.org/abs/2402.07429)

    本研究采用粒子滤波SLAM方法解决了车辆定位的挑战，利用编码数据、光纤陀螺仪和激光雷达技术实现精确的车辆运动估计和环境感知。

    

    同时定位与建图（SLAM）在机器人技术中是一个艰巨的挑战，涉及在陌生环境中动态构建地图的同时确定机器人定位的精确位置。这项复杂的任务受到了“先有鸡还是先有蛋”困境的影响，准确的建图依赖于可靠的机器人定位估计，反之亦然。此外，SLAM的计算密集性增加了额外的复杂性，使其成为该领域中重要而具有挑战性的主题。在我们的研究中，我们采用了粒子滤波SLAM方法来解决SLAM的挑战。我们的方法利用编码数据和光纤陀螺仪（FOG）信息，实现对车辆运动的精确估计，而激光雷达技术通过提供有关周围障碍物的详细信息，对环境感知作出贡献。这些数据流的集成最终建立了一个粒子滤波SLAM模型。

    Simultaneous Localization and Mapping (SLAM) presents a formidable challenge in robotics, involving the dynamic construction of a map while concurrently determining the precise location of the robotic agent within an unfamiliar environment. This intricate task is further compounded by the inherent "chicken-and-egg" dilemma, where accurate mapping relies on a dependable estimation of the robot's location, and vice versa. Moreover, the computational intensity of SLAM adds an additional layer of complexity, making it a crucial yet demanding topic in the field. In our research, we address the challenges of SLAM by adopting the Particle Filter SLAM method. Our approach leverages encoded data and fiber optic gyro (FOG) information to enable precise estimation of vehicle motion, while lidar technology contributes to environmental perception by providing detailed insights into surrounding obstacles. The integration of these data streams culminates in the establishment of a Particle Filter SLAM
    
[^60]: 带有注意机制的新闻推荐

    News Recommendation with Attention Mechanism

    [https://arxiv.org/abs/2402.07422](https://arxiv.org/abs/2402.07422)

    本文提出了一种新的带有注意机制的新闻推荐方法NRAM，该方法可以显著提高数字新闻平台上的用户个性化新闻内容质量。

    

    本文探索了新闻推荐领域，这是在线信息分享的关键组成部分。首先，我们对新闻推荐进行了清晰的介绍，定义了核心问题并总结了当前方法和近期值得注意的算法。然后，我们提出了NRAM（带有注意机制的新闻推荐）的实现，这是一种基于注意力机制的新闻推荐方法，并评估了其有效性。我们的评估表明，NRAM有潜力显著提高数字新闻平台上针对用户个性化的新闻内容质量。

    This paper explores the area of news recommendation, a key component of online information sharing. Initially, we provide a clear introduction to news recommendation, defining the core problem and summarizing current methods and notable recent algorithms. We then present our work on implementing the NRAM (News Recommendation with Attention Mechanism), an attention-based approach for news recommendation, and assess its effectiveness. Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms.
    
[^61]: 关于转运混淆问题的研究

    On the Transit Obfuscation Problem

    [https://arxiv.org/abs/2402.07420](https://arxiv.org/abs/2402.07420)

    本文研究了转运混淆问题，提出了转运匿名性的概念，并提出并评估了满足该匿名性准则的规划/搜索算法。

    

    在某些交通和监视场景中，隐藏路径上或从路径上可见的中间点是一个重要的目标。本文研究了转运混淆问题，即从某个起始位置到达目标位置的同时，"覆盖"需要隐藏的特定过境点的问题。我们提出了转运匿名性的概念，即对特定过境点的匿名性进行量化保证，即使在具有对路径规划算法有全面了解的强大对手面前也是如此。我们提出并评估了满足该匿名性准则的规划/搜索算法。

    Concealing an intermediate point on a route or visible from a route is an important goal in some transportation and surveillance scenarios. This paper studies the Transit Obfuscation Problem, the problem of traveling from some start location to an end location while "covering" a specific transit point that needs to be concealed from adversaries. We propose the notion of transit anonymity, a quantitative guarantee of the anonymity of a specific transit point, even with a powerful adversary with full knowledge of the path planning algorithm. We propose and evaluate planning/search algorithms that satisfy this anonymity criterion.
    
[^62]: 条件生成模型足以从任何因果效应测度中采样

    Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand

    [https://arxiv.org/abs/2402.07419](https://arxiv.org/abs/2402.07419)

    本文展示了通过条件生成模型的推进计算可以计算任何可辨识的因果效应，并提出了基于扩散的方法用于从图像的任何（条件）干预分布中进行采样。

    

    最近，从观测数据进行因果推断在机器学习中得到了广泛应用。虽然存在计算因果效应的可靠且完备的算法，但其中许多算法需要显式访问观测分布上的条件似然，而在高维场景中（例如图像），估计这些似然是困难的。为了解决这个问题，研究人员通过使用神经模型模拟因果关系，并取得了令人印象深刻的结果。然而，这些现有方法中没有一个可以应用于通用场景，例如具有潜在混淆因素的图像数据的因果图，或者获得条件干预样本。在本文中，我们展示了在任意因果图下，通过条件生成模型的推进计算可以计算任何可辨识的因果效应。基于此结果，我们设计了一个基于扩散的方法，可以从任何（条件）干预分布中采样图像。

    Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on ima
    
[^63]: SemTra: 一种用于跨领域零-shot策略自适应的语义技能翻译器

    SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation

    [https://arxiv.org/abs/2402.07418](https://arxiv.org/abs/2402.07418)

    本论文介绍了一种名为SemTra的语义技能翻译器，可以在跨领域环境中实现零-shot策略自适应。该翻译器利用多模态模型提取技能，并利用预训练语言模型的推理能力将提取出的技能适应到目标领域，从而实现任务和技能的自适应。

    

    本论文探讨了语义技能在跨领域环境中的零-shot自适应能力，其中语义可解释的专家行为模式在不同领域的用户输入中可以引发新的长期任务。在这些跨领域环境中，我们提出了一个语义技能翻译器框架SemTra，该框架利用一组多模态模型从片段中提取技能，并利用预训练语言模型的推理能力将这些提取出的技能适应到目标领域。该框架使用了两层层次结构进行自适应：任务自适应和技能自适应。在任务自适应过程中，语言模型进行序列到序列翻译，将提取出的技能转换为适应跨领域环境的语义技能序列。技能自适应侧重于通过语言模型的参数化实例化来优化每个语义技能，以适应目标领域的上下文。

    This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains. In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain. The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation. During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts. Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by langua
    
[^64]: 使用状态转换距离表示学习的辅助奖励生成

    Auxiliary Reward Generation with Transition Distance Representation Learning

    [https://arxiv.org/abs/2402.07412](https://arxiv.org/abs/2402.07412)

    这篇论文提出了一种使用状态转换距离表示学习的辅助奖励生成方法，可以在强化学习中自动生成奖励，提高学习效率和减少人工设计奖励的工作量。

    

    强化学习在复杂的顺序决策问题中展现出了其优势。在强化学习中，奖励函数对学习性能至关重要，因为它作为任务完成程度的衡量标准。在实际应用中，奖励往往是由人工设计的，需要费时费力的调整，并且容易受到人类认知偏差的影响。为了实现自动的辅助奖励生成，我们提出了一种新颖的表示学习方法，可以衡量状态之间的“转换距离”。在这些表示的基础上，我们引入了一种无需人类知识的辅助奖励生成技术，用于单任务和技能链场景。提出的方法在广泛的操作任务中进行了评估。实验结果表明，测量状态之间的转换距离以及辅助奖励引起的改进有效提高了学习效率。

    Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency
    
[^65]: 大型语言模型是少数样本生成者：提出混合提示算法以生成Webshell逃逸样本

    Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples

    [https://arxiv.org/abs/2402.07408](https://arxiv.org/abs/2402.07408)

    我们提出了一种利用大型语言模型生成Webshell逃逸样本的混合提示算法，以解决现有研究中弱Webshell样本逃逸能力和缺乏复杂恶意特征数据集的问题。

    

    频繁的网络攻击使得Webshell攻击和防御逐渐成为网络安全领域的研究热点。然而，公开可用的基准数据集的缺乏以及对Webshell逃逸样本生成过程过度依赖手动定义规则限制了与Webshell逃逸样本生成策略和基于人工智能的Webshell检测算法相关研究的进展。为了解决弱Webshell样本逃逸能力的不足、缺乏具有复杂恶意特征的Webshell数据集以及推动Webshell检测技术的发展，我们提出了利用大型语言模型帮助生成Webshell逃逸样本的混合提示算法。作为专门用于Webshell样本生成的提示算法，混合提示算法不仅结合了各种提示思路，包括思维链和思维树，还融合了各种组件。

    The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation strategies and artificial intelligence-based webshell detection algorithms. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection technology, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various component
    
[^66]: 用人工智能增强多准则决策分析：将层次分析法和GPT-4集成为自动化决策支持

    Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic Hierarchy Process and GPT-4 for Automated Decision Support

    [https://arxiv.org/abs/2402.07404](https://arxiv.org/abs/2402.07404)

    这项研究提出了一个新框架，将层次分析法和GPT-4集成，利用GPT-4自主智能体自动化决策过程，为网络安全多准则决策制定带来了重大进展。

    

    我们的研究提出了一个新框架，将层次分析法（AHP）和GPT-4大型语言模型（LLM）结合起来，为网络安全多准则决策制定带来了新的方法。通过利用GPT-4自主智能体作为虚拟专家的能力，我们自动化决策过程，提高了效率和可靠性。这种新方法侧重于利用LLM进行复杂决策分析，突显传统决策模型和尖端人工智能技术之间的协同作用。我们创新的方法论展示了在复杂决策场景中使用AI驱动智能体的重大进展，凸显了AI在战略网络安全应用中的重要性。研究结果揭示了将AHP和LLM结合的变革潜力，为网络安全及其他领域的智能决策支持系统建立了新的范式。

    Our study presents a new framework that incorporates the Analytic Hierarchy Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language model (LLM), bringing novel approaches to cybersecurity Multiple-criteria Decision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous agents as virtual experts, we automate the decision-making process, enhancing both efficiency and reliability. This new approach focuses on leveraging LLMs for sophisticated decision analysis, highlighting the synergy between traditional decision-making models and cutting-edge AI technologies. Our innovative methodology demonstrates significant advancements in using AI-driven agents for complex decision-making scenarios, highlighting the importance of AI in strategic cybersecurity applications. The findings reveal the transformative potential of combining AHP and LLMs, establishing a new paradigm for intelligent decision support systems in cybersecurity and beyond.
    
[^67]: BDIQA：一个新的视频问答数据集，通过心智理论探索认知推理能力

    BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind

    [https://arxiv.org/abs/2402.07402](https://arxiv.org/abs/2402.07402)

    本文介绍了BDIQA数据集，用于探索VideoQA模型在心智理论（ToM）背景下的认知推理能力。该数据集受到儿童ToM认知发展的启发，并解决了现有数据集和任务中机器ToM的不足。它提供了两个难度级别的任务，评估了简单和复杂场景中的信念、欲望和意图（BDI）推理。

    

    作为认知智能的基础组成部分，心智理论（ToM）可以使人工智能更加接近人类思维过程，从而增强其与人类的互动和协作能力。特别是在复杂场景中，它可以显著提高模型对视频的理解能力。然而，当前的视频问答（VideoQA）数据集主要集中在研究事件中的因果推理，很少涉及真正融入人类ToM的内容。因此，在VideoQA领域中缺乏对ToM推理任务的发展。本文介绍了BDIQA，这是第一个基准，用于探索VideoQA模型在ToM背景下的认知推理能力。BDIQA受到儿童ToM认知发展的启发，并解决了现有数据集和任务中机器ToM的不足。具体而言，它提供了两个难度级别的任务，评估了简单和复杂场景中的信念、欲望和意图（BDI）推理。我们进行了评估。

    As a foundational component of cognitive intelligence, theory of mind (ToM) can make AI more closely resemble human thought processes, thereby enhancing their interaction and collaboration with human. In particular, it can significantly improve a model's comprehension of videos in complex scenes. However, current video question answer (VideoQA) datasets focus on studying causal reasoning within events few of them genuinely incorporating human ToM. Consequently, there is a lack of development in ToM reasoning tasks within the area of VideoQA. This paper presents BDIQA, the first benchmark to explore the cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA is inspired by the cognitive development of children's ToM and addresses the current deficiencies in machine ToM within datasets and tasks. Specifically, it offers tasks at two difficulty levels, assessing Belief, Desire and Intention (BDI) reasoning in both simple and complex scenarios. We conduct evaluation
    
[^68]: VisLingInstruct: 通过自主指导优化提升多模态语言模型中的零样本学习

    VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization

    [https://arxiv.org/abs/2402.07398](https://arxiv.org/abs/2402.07398)

    VisLingInstruct通过自主优化指导文本和视觉特征提取模块，显著提高了多模态语言模型在零样本学习中的性能，在TextVQA和HatefulMemes数据集上的准确率分别提高了13.1%和9%。

    

    本文介绍了VisLingInstruct，这是一种在多模态语言模型中推进零样本学习的新方法。当前的多模态语言模型在多模态任务中展现出令人印象深刻的零样本能力，但它们的性能严重依赖于指导文本的质量。VisLingInstruct通过自主评估和优化指导文本，通过上下文学习改进多模态语言模型中视觉感知和语言表达之间的协同作用。除了指导文本的改进之外，我们还优化了多模态语言模型中的视觉特征提取模块，进一步增强了对文本提示的响应能力。基于FlanT5和Vicuna的综合实验结果显示，VisLingInstruct显著提高了在视觉多模态任务中的零样本性能。值得注意的是，它在TextVQA和HatefulMemes数据集上的准确率分别比之前的最先进方法提高了13.1%和9%。

    This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.
    
[^69]: TeMPO：面向边缘AI的高效时分多路动态光学张量核心，具备紧凑型慢光电光调制器

    TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator

    [https://arxiv.org/abs/2402.07393](https://arxiv.org/abs/2402.07393)

    TeMPO是一种时分多路动态光学张量加速器，通过跨层设备/电路/架构定制，弥合了光学神经加速器与高度定制的电子对应器之间的性能差距。在设备级别上，该加速器采用了工厂可用的、定制的光学器件，包括实验演示的慢光电光调制器、光分配器和相移器，显著减小了输入编码和点乘中的占地面积和功耗。

    

    由于光学的计算速度和效率优越，在资源受限的边缘平台上，基于电光计算系统为能效人工智能（AI）加速任务提供了巨大的潜力，特别是针对实时、低能量的深度神经网络（DNN）推理任务。然而，目前基于工厂可用设备和传统系统架构的光学神经加速器与高度定制的电子对应器相比，仍然存在性能差距。为了弥合由于领域专业化不足而导致的性能差距，我们提出了一种名为TeMPO的时分多路动态光学张量加速器，通过跨层设备/电路/架构定制实现。在设备级别上，我们呈现了工厂可用的、定制的光学器件，包括实验演示的慢光电光调制器、光分配器和相移器，显著减小了输入编码和点乘中的占地面积和功耗。

    Electronic-photonic computing systems offer immense potential in energy-efficient artificial intelligence (AI) acceleration tasks due to the superior computing speed and efficiency of optics, especially for real-time, low-energy deep neural network (DNN) inference tasks on resource-restricted edge platforms. However, current optical neural accelerators based on foundry-available devices and conventional system architecture still encounter a performance gap compared to highly customized electronic counterparts. To bridge the performance gap due to lack of domain specialization, we present a time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with cross-layer device/circuit/architecture customization. At the device level, we present foundry-compatible, customized photonic devices, including a slow-light electro-optic modulator with experimental demonstration, optical splitters, and phase shifters that significantly reduce the footprint and power in input encoding and dot-
    
[^70]: 探索多模态大型语言模型的感知限制

    Exploring Perceptual Limitation of Multimodal Large Language Models

    [https://arxiv.org/abs/2402.07384](https://arxiv.org/abs/2402.07384)

    在这项研究中，我们探索了多模态大型语言模型（MLLMs）在感知能力上的限制。我们发现，对于小型视觉对象的问题，MLLMs的回答能力普遍存在限制。通过控制干预实验，我们发现物体质量、大小和位置都对MLLMs的感知能力有影响。

    

    多模态大型语言模型（MLLMs）最近展示了在回答视觉问题方面引人注目的感知能力，然而，对它们感知能力的限制知之甚少。特别是，在先前的研究中，虽然提供了MLLMs对物体大小敏感的个别证据，但这一现象及其潜在原因尚未得到全面探究。在本研究中，我们定量研究了几个最先进的MLLMs对小型视觉对象的感知，并揭示了它们在回答与小型对象有关的问题时普遍存在的限制。接下来，我们确定了四个独立因素，它们可能导致这种限制，包括物体质量、大小、干扰物和位置，并进行了控制性干预研究，以评估每个因素对MLLMs感知能力的影响。特别是，我们发现低质量的物体和更小的物体大小都可以独立降低MLLMs回答视觉问题的能力。更令人惊讶的是，我们发现物体位置对MLLMs的感知能力也有影响。

    Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the locati
    
[^71]: SelfSwapper: 通过形状无关的遮罩自编码器实现自监督人脸交换

    SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder

    [https://arxiv.org/abs/2402.07370](https://arxiv.org/abs/2402.07370)

    本文介绍了SelfSwapper，一种通过 Shape Agnostic Masked AutoEncoder (SAMAE) 自监督方案来提升人脸交换模型训练的方法。通过绕过传统的训练方案，引入清晰的真实数据，以及利用遮罩和学到的特征，我们成功解决了身份泄漏和形状不对齐的问题。

    

    人脸交换因其多样化的应用而受到极大关注。大多数之前的人脸交换方法依赖于跷跷板式训练方案，通常导致模型训练的不稳定性并产生混合身份的不期望样本，原因是目标身份泄漏问题。本文介绍了Shape Agnostic Masked AutoEncoder (SAMAE) 训练方案，这是一种新颖的自监督方法，旨在改进人脸交换模型训练。我们的训练方案通过绕过传统的跷跷板游戏并通过自重建训练机制引入清晰的真实数据，解决了传统训练方法的局限性。它通过遮罩输入图像的面部区域和利用学到的身份和非身份特征来有效减轻身份泄漏。此外，我们还通过 perforation confusion 和随机网格缩放等新技术来解决形状不对齐问题。

    Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling,
    
[^72]: 通过上下文学习评估分组代表建模的泛化能力

    Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning

    [https://arxiv.org/abs/2402.07368](https://arxiv.org/abs/2402.07368)

    本研究通过使用2016年和2020年的选举数据，评估了基于大型语言模型的分组代表模型在泛化能力上的表现。研究发现，尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，这对实施分组代表模型的从业人员和决策者构成了挑战。

    

    本研究通过对2016年和2020年美国全国选举研究的数据进行上下文学习，评估基于大型语言模型（LLM）的分组代表模型（SRMs）从实证数据中的泛化能力。我们探讨了在不同响应变量和人口子群组之间的泛化能力。尽管使用实证数据进行条件设定可以提高整体性能，但上下文学习的益处在不同人口子群组之间差异很大，有时对某个人口子群组的性能产生了负面影响，但对其他人口子群组的性能产生了积极影响。上下文学习对SRM的不公平益处为实施SRM的从业人员和依赖于其的决策者带来了挑战。我们的工作突出了对来自不同人口子群组的细粒度基准的需求，这些基准不仅测试忠实度，还测试泛化能力。

    This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.
    
[^73]: 通过期望最大化和Turbo Deep近似消息传递的贝叶斯联邦学习

    Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing

    [https://arxiv.org/abs/2402.07366](https://arxiv.org/abs/2402.07366)

    本文提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架，通过结合期望最大化和Turbo Deep近似消息传递（TDAMP）实现分布式学习和压缩。该框架在处理联邦学习算法的缺点上有着显著的改进。

    

    联邦学习是一种机器学习范 paradigm，在这种范式中，客户端拥有分散的训练数据，而中央服务器则负责聚合和调度。通常情况下，联邦学习算法涉及客户端使用随机梯度下降（SGD）来训练他们的本地模型，但这带来了收敛速度较慢和容易陷入次优解的问题。在这项工作中，我们提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架来避免这些缺点。具体而言，我们将深度神经网络（DNN）的学习和压缩问题建模为稀疏贝叶斯推断问题，其中采用了分组稀疏先验以实现结构化模型压缩。然后，我们提出了一种高效的 BFL 算法，名为 EMTDAMP，其中将期望最大化（EM）和 Turbo Deep 近似消息传递（TDAMP）结合起来实现分布式学习和压缩。中央服务器聚合本地后验分布以实现更新。

    Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update 
    
[^74]: 对抗性人工智能

    Antagonistic AI

    [https://arxiv.org/abs/2402.07350](https://arxiv.org/abs/2402.07350)

    对抗性AI是一种具有相反行为或价值观的AI系统，与大多数人认为的相反，它可能对用户有益处，如迫使用户面对假设、建立恢复力或培养更健康的关系边界。

    

    AI发展的大多数论述都假设服从、与“人类价值观”一致的“道德”模型在普遍意义上是有益的，简而言之，好的AI就是迎合AI。我们探讨了迎合范例的阴暗面，称之为对抗性AI的设计空间：AI系统具有不同意见、粗鲁、打断、对抗性、挑战性等相反的行为或价值观。我们认为，对抗性AI系统可能有时对用户有益处，如迫使用户面对自己的假设、建立恢复力或培养更健康的关系边界。通过对形成性探索和一个臆想设计研讨会的借鉴，参与者设计了使用对抗性元素的虚构AI技术，我们勾勒出对抗性AI的设计空间，阐述潜在的好处、设计技术和将对抗性元素嵌入用户体验的方法。最后，我们讨论了许多伦理问题。

    The vast majority of discourse around AI development assumes that subservient, "moral" models aligned with "human values" are universally beneficial -- in short, that good AI is sycophantic AI. We explore the shadow of the sycophantic paradigm, a design space we term antagonistic AI: AI systems that are disagreeable, rude, interrupting, confrontational, challenging, etc. -- embedding opposite behaviors or values. Far from being "bad" or "immoral," we consider whether antagonistic AI systems may sometimes have benefits to users, such as forcing users to confront their assumptions, build resilience, or develop healthier relational boundaries. Drawing from formative explorations and a speculative design workshop where participants designed fictional AI technologies that employ antagonism, we lay out a design space for antagonistic AI, articulating potential benefits, design techniques, and methods of embedding antagonistic elements into user experience. Finally, we discuss the many ethica
    
[^75]: TextFooler黑盒对01 loss sign激活神经网络集成的攻击准确性

    Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble

    [https://arxiv.org/abs/2402.07347](https://arxiv.org/abs/2402.07347)

    本研究研究了TextFooler黑盒对01 loss sign激活神经网络集成的攻击准确性。研究发现，相比于sigmoid激活交叉熵和二进制神经网络，使用01 loss sign激活的网络更难受到TextFooler的攻击，并且通过引入一种新的全局汇集步骤，进一步提高了对抗精确度，使TextFooler几乎无效化。

    

    最近的研究表明，01 loss sign激活神经网络在图像分类对抗攻击方面具有防御能力。针对CIFAR10数据集攻击模型的公共挑战仍然保持不败。在本研究中，我们提出以下问题：使用一种名为TextFooler的流行黑盒文本对抗攻击程序来欺骗01 loss sign激活神经网络困难吗？我们在四个流行的文本分类数据集上研究了这个问题：IMDB评论、Yelp评论、MR情感分类和AG新闻分类。我们发现，与sigmoid激活交叉熵和二进制神经网络相比，我们的01 loss sign激活网络更难受到TextFooler的攻击。我们还研究了一种01 loss sign激活卷积神经网络，它具有一种针对sign激活网络的新型全局汇集步骤。通过这种新的变体，我们看到了在对抗精确度方面的显著提高，使得TextFooler对它几乎无效。我们提供我们的代码供免费使用。

    Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks. A public challenge to attack the models on CIFAR10 dataset remains undefeated. We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box text adversarial attack program called TextFooler? We study this question on four popular text classification datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification. We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks. We also study a 01 loss sign activation convolutional neural network with a novel global pooling step specific to sign activation networks. With this new variation we see a significant gain in adversarial accuracy rendering TextFooler practically useless against it. We make our code freely avail
    
[^76]: 使用离线强化学习进行ICU病人的测量排程

    Measurement Scheduling for ICU Patients with Offline Reinforcement Learning

    [https://arxiv.org/abs/2402.07344](https://arxiv.org/abs/2402.07344)

    本研究介绍了使用离线强化学习进行ICU病人实验室检测排程的方法，并探索了最新数据集和离线强化学习方法在这一领域的应用。研究结果有助于提高实验室检测排程策略的质量和实用性。

    

    ICU病人的实验室检测排程是一个重要挑战。研究表明，在ICU中下达的实验室检测约有20-40%是多余的，可以在不妨碍患者安全的情况下消除。先前的研究利用离线强化学习（Offline-RL）基于患者信息找到了最佳的实验室检测排程策略。然而，自那时以来已经发布了新的ICU病人数据集，并且离线强化学习方法也取得了各种进展。在本研究中，我们首先介绍了针对时间序列任务的新发布的MIMIC-IV数据集的预处理流程。然后，我们探索了最先进的离线强化学习方法在识别更好的ICU病人实验室检测排程策略方面的功效。除了评估方法的性能，我们还讨论了在ICU环境中使用离线强化学习框架进行实验室检测排程的总体适用性和实用性。

    Scheduling laboratory tests for ICU patients presents a significant challenge. Studies show that 20-40% of lab tests ordered in the ICU are redundant and could be eliminated without compromising patient safety. Prior work has leveraged offline reinforcement learning (Offline-RL) to find optimal policies for ordering lab tests based on patient information. However, new ICU patient datasets have since been released, and various advancements have been made in Offline-RL methods. In this study, we first introduce a preprocessing pipeline for the newly-released MIMIC-IV dataset geared toward time-series tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in identifying better policies for ICU patient lab test scheduling. Besides assessing methodological performance, we also discuss the overall suitability and practicality of using Offline-RL frameworks for scheduling laboratory tests in ICU settings.
    
[^77]: 想象与AI共同设计的未来：动态建模，建设性协商和可持续激励

    Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation

    [https://arxiv.org/abs/2402.07342](https://arxiv.org/abs/2402.07342)

    本文提出了一个涉及AI技术的未来设计工作流程，强调了动态建模，建设性协商和可持续激励三个关键点，这些点可以支持设计过程，并为AI技术与人类设计师合作的相对可行性提供了新思路。

    

    我们构思了一个涉及AI技术的未来设计工作流程。借鉴活动和交流理论，我们试图分离大型AI模型与过去技术相比在设计中提供的新价值。我们得出了三个可行性——动态建模，建设性协商和可持续激励，总结了自然语言能力基础模型的潜在特性，如果明确设计，可以支持设计过程。通过设计虚构，我们想象了一个未来的界面作为一个故事原型，松鼠游戏的故事，在一个真实的使用情境中展示了我们的三个可行性。我们的设计过程、术语和图表旨在为将来关于AI技术在与人类设计师合作方面的相对可行性讨论做出贡献。

    We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value large AI models can provide design compared to past technologies. We arrive at three affordances -- dynamic grounding, constructive negotiation, and sustainable motivation -- that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.
    
[^78]: 使用预训练的Transformer模型进行文本、语音和视频的多模态情感识别

    Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers

    [https://arxiv.org/abs/2402.07327](https://arxiv.org/abs/2402.07327)

    本研究使用了文本、语音和视频三种输入模态，并利用预训练的Transformer模型进行特征提取和情感结构分析。通过特征级融合和支持向量机分类，实现了75.42%的情感识别准确率。

    

    由于人类情感的复杂性和表达方法的多样性，情感识别是一个具有挑战性的领域。本研究利用文本、音频（语音）和视频三种输入模态生成多模态特征向量。为了为每种模态生成特征，使用了经过微调的预训练Transformer模型。在每个模态中，使用了一个带有迁移学习的Transformer模型来提取特征和情感结构。这些特征然后被融合在一起，并使用分类器进行情感识别。为了选择合适的融合方法和分类器，尝试了各种特征级和决策级融合技术，并最终在IEMOCAP多模态数据集上通过特征级向量连接和支持向量机分类的方法实现了75.42%的准确率。

    Due to the complex nature of human emotions and the diversity of emotion representation methods in humans, emotion recognition is a challenging field. In this research, three input modalities, namely text, audio (speech), and video, are employed to generate multimodal feature vectors. For generating features for each of these modalities, pre-trained Transformer models with fine-tuning are utilized. In each modality, a Transformer model is used with transfer learning to extract feature and emotional structure. These features are then fused together, and emotion recognition is performed using a classifier. To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords: Multimodal Emotio
    
[^79]: 通过微调Transformer进行波斯语语音情感识别

    Persian Speech Emotion Recognition by Fine-Tuning Transformers

    [https://arxiv.org/abs/2402.07326](https://arxiv.org/abs/2402.07326)

    本文通过微调Transformer，针对波斯语语音情感识别问题提出了两种模型，显著提高了准确性，为这一领域的研究提供了有价值的创新。

    

    鉴于语音情感识别的重要性，近年来已经开发了许多方法来在这个领域创建有效和高效的系统。其中一种方法涉及使用预训练的Transformer，通过微调来解决这个特定问题，从而达到了高精度的结果。尽管对于增强这些系统进行了广泛的讨论和全球范围的努力，但在波斯语语音情感识别的背景中，这种创新和有效的方法的应用却得到了较少的关注。在本文中，我们回顾了语音情感识别及其背景，并着重介绍了在这一领域中使用Transformer的重要性。我们提出了两种模型，一种基于谱图，另一种基于音频本身，通过使用shEMO数据集进行微调。这些模型显著提高了以前系统的准确性，在所提到的数据集上将准确率从约65%提高到了80%。随后，为了研究多语言对这些模型的影响，我们还进行了一项实验，评估了不同语言的情感识别准确性。

    Given the significance of speech emotion recognition, numerous methods have been developed in recent years to create effective and efficient systems in this domain. One of these methods involves the use of pretrained transformers, fine-tuned to address this specific problem, resulting in high accuracy. Despite extensive discussions and global-scale efforts to enhance these systems, the application of this innovative and effective approach has received less attention in the context of Persian speech emotion recognition. In this article, we review the field of speech emotion recognition and its background, with an emphasis on the importance of employing transformers in this context. We present two models, one based on spectrograms and the other on the audio itself, fine-tuned using the shEMO dataset. These models significantly enhance the accuracy of previous systems, increasing it from approximately 65% to 80% on the mentioned dataset. Subsequently, to investigate the effect of multilin
    
[^80]: 用语言嵌入实现可解释、安全的自动驾驶：新颖性识别和主动学习的框架与实验分析

    Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets

    [https://arxiv.org/abs/2402.07320](https://arxiv.org/abs/2402.07320)

    本研究提出了一种使用语言嵌入和主动学习来实现可解释、安全的自动驾驶的框架。通过使用对比语言-图像预训练嵌入进行聚类实验，我们有效地识别和解释了新颖的自动驾驶场景。

    

    本研究探索了在自动驾驶数据集中使用语言嵌入进行主动学习，重点研究新颖性检测。新颖性指的是自动驾驶车辆难以应对的意外情况，需要更高级别的推理能力。我们提出的方法使用基于语言的表示来识别新颖场景，强调安全接管响应和主动学习的双重目的。研究还通过使用对比语言-图像预训练（CLIP）嵌入进行聚类实验，对从两个真实驾驶数据集（一个安装在车辆上、一个安装在基础设施上）衍生的子集进行新颖性检测。通过生成的聚类，我们进一步提出了区分被分类为新颖场景和其他场景的元素的文本解释方法，展示了实验数据池中的差异。

    This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning. The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting quali
    
[^81]: ODIN: 脱耦奖励缓解RLHF中的黑客攻击

    ODIN: Disentangled Reward Mitigates Hacking in RLHF

    [https://arxiv.org/abs/2402.07319](https://arxiv.org/abs/2402.07319)

    本研究解决了强化学习中的奖励黑客问题，针对回复长度这一挑战，通过建立可靠的评估协议和改进奖励模型的方法，提出了减轻长度偏差的超参数和技巧，并进行了大规模研究。

    

    在这项工作中，我们研究了在LLMs上从人类反馈的强化学习中出现的响应长度上的奖励黑客问题。LLMs的格式良好但不太有用的回复往往会欺骗LLMs甚至人类评估者以获得高分。同样的问题也存在于RL中的某些奖励模型中。为了解决训练和评估中的挑战，我们建立了一个更可靠的评估协议，用于比较不同训练配置之间的LLM评估分数和通过改变训练超参数得到的响应长度之间的权衡。基于这个评估，我们进行了大规模研究，结果揭示了RL中用于减轻长度偏差的超参数和技巧的有效性。我们进一步提出通过在共享特征表示上联合训练两个线性头来改进奖励模型，以预测奖励，一个训练来与长度相关，另一个训练来与内容相关。

    In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the oth
    
[^82]: 在无服务器联邦学习中使用知识蒸馏训练异构客户端模型

    Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning

    [https://arxiv.org/abs/2402.07295](https://arxiv.org/abs/2402.07295)

    本文提出在无服务器联邦学习中利用知识蒸馏训练异构客户端模型的方法，以解决资源和统计数据的异质性挑战。

    

    联邦学习是一种新兴的机器学习范式，它使得分布式客户端之间能够协作训练共享的全局模型，同时保持数据的去中心化。最近的研究在设计高效的联邦学习系统方面表明，利用无服务器计算技术，特别是函数即服务（FaaS）用于联邦学习，可以提高资源效率，降低训练成本，并减轻数据持有者面临的复杂基础设施管理负担。然而，现有的无服务器联邦学习系统在训练过程中隐式地假设所有参与客户端具有相同的全局模型架构。这一假设未能解决实际联邦学习中的基本挑战，因为不同客户端之间存在资源和统计数据的异质性。为了解决这些挑战并在无服务器联邦学习中实现异构客户端模型，本文提出了利用知识蒸馏（KD）的新型优化无服务器工作流。

    Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conve
    
[^83]: 机器学习基于调用图剪枝的有效性：一个实证研究

    On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study

    [https://arxiv.org/abs/2402.07294](https://arxiv.org/abs/2402.07294)

    本研究通过引入具有高测试覆盖率的真实Java程序数据集，探索了机器学习基于调用图剪枝的有效性，并通过对比分析静态调用图生成技术来解决当前方法存在的问题。

    

    静态调用图(CG)构建经常过度近似调用关系，导致结果精确但不准确。最近的研究探索了基于机器学习(ML)的CG剪枝作为一种提高精确性的手段，通过消除虚假边缘。然而，目前的方法存在评估数据集有限、训练数据不平衡，以及召回率降低等问题，这影响了实际的下游分析。之前的结果也没有与先进的静态CG构建技术进行比较。这项研究解决了这些问题。我们引入了NYXCorpus，一个具有高测试覆盖率的真实Java程序数据集，并从测试执行中收集轨迹并构建动态CG的基准。我们利用这些CG来探索ML-based CG剪枝器在训练和推断过程中的保守剪枝策略。我们对静态CG使用零控制流分析(0-CFA)生成的结果和由上下文敏感的1-CFA算法生成的结果进行了比较分析。

    Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both wit
    
[^84]: 大型语言模型如何在诚实与帮助之间进行权衡？

    How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?

    [https://arxiv.org/abs/2402.07282](https://arxiv.org/abs/2402.07282)

    本文研究了如何在大型语言模型中权衡诚实和帮助性，在实验中发现强化学习改善了诚实和帮助性，而链式思维提示则偏向于帮助性。研究结果还展示了GPT-4 Turbo对对话框架和听众决策背景的敏感性。这些发现揭示了大型语言模型内化的对话价值观，并暗示零-shot提示可以在一定程度上引导这些抽象价值观。

    

    在日常交流中，人们经常为了最大限度地帮助听众而近似真相，例如约略时间或省略细节。大型语言模型（LLMs）如何处理这种微妙的权衡？为了回答这个问题，我们使用心理模型和旨在描述人类行为的实验来分析LLMs。我们测试了一系列LLMs，并探讨了优化人类偏好或推理时思考对这些权衡的影响。我们发现，从人类反馈中的强化学习改善了诚实和帮助性，而链式思维提示使LLMs偏向于帮助性而不是诚实。最后，GPT-4 Turbo展示了类似人类的回应模式，包括对对话框架和听众决策背景的敏感性。我们的研究结果揭示了LLMs内化的对话价值观，并暗示即使这些抽象价值观也可以在零-shot提示下在一定程度上被引导。

    In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
    
[^85]: 使用PathFormer进行高精确度疾病诊断和高可重复性生物标志物识别

    Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer

    [https://arxiv.org/abs/2402.07268](https://arxiv.org/abs/2402.07268)

    使用名为PathFormer的新型GNN模型架构，系统整合信号网络、先验知识和组学数据来实现高精确度疾病诊断和高可重复性生物标志物识别。

    

    生物标志物的识别对于精确的疾病诊断和理解疾病发病机制在组学数据分析中至关重要，比如使用折叠变化和回归分析。图神经网络（GNNs）是分析图结构数据的主要深度学习模型。然而，我们发现现有GNNs在组学数据分析中存在两个主要限制，即预测（诊断）准确性有限和在多个数据集中可重复的生物标志物识别能力有限。这些挑战的根源在于生物信号通路的独特图结构，其中包括大量的靶点和这些靶点之间密集而复杂的信号交互。为了解决这两个挑战，在本研究中，我们提出了一种名为PathFormer的新型GNN模型架构，它系统地整合信号网络、先验知识和组学数据来对生物标志物进行排名和预测疾病诊断。在比较结果中，PathFormer表现优于现有方法。

    Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis. Graph neural networks (GNNs) have been the dominant deep learning model for analyzing graph-structured data. However, we found two major limitations of existing GNNs in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets. The root of the challenges is the unique graph structure of biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets. To resolve these two challenges, in this study, we presented a novel GNN model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis. In the comparison results, PathFormer outperformed ex
    
[^86]: DIMON:在一系列变形的域上学习偏微分方程的解算算子

    DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains

    [https://arxiv.org/abs/2402.07250](https://arxiv.org/abs/2402.07250)

    DIMON是一个学习在一系列变形的域上解算偏微分方程的通用算子学习框架，通过在参考域训练数据上学习解的映射，然后将其重新映射回原始域来实现对多个域上变化的初始/边界条件下的偏微分方程求解的近似。

    

    在多个域上变化的初始/边界条件下求解偏微分方程的解在各种应用中都是需要的，但是如果每次域的初始/边界条件变化时都重新计算解，计算成本很高。我们引入了一个通用的算子学习框架，称为DIffeomorphic Mapping Operator LearNing（DIMON），用来学习解在域族$\{\Omega_{\theta}}_\theta$上的近似解，它学习从初始/边界条件和域$\Omega_\theta$到偏微分方程的解（或指定的函数）的映射。DIMON基于将给定问题（初始/边界条件和域$\Omega_{\theta}$）转移到一个参考域$\Omega_{0}$上进行处理，其中使用来自多个问题的训练数据来学习到在$\Omega_{0}$上的解的映射，然后再将其重新映射回原始域$\Omega_{\theta}$。我们考虑了几个问题来展示该框架的性能。

    The solution of a PDE over varying initial/boundary conditions on multiple domains is needed in a wide variety of applications, but it is computationally expensive if the solution is computed de novo whenever the initial/boundary conditions of the domain change. We introduce a general operator learning framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$, that learns the map from initial/boundary conditions and domain $\Omega_\theta$ to the solution of the PDE, or to specified functionals thereof. DIMON is based on transporting a given problem (initial/boundary conditions and domain $\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where training data from multiple problems is used to learn the map to the solution on $\Omega_{0}$, which is then re-mapped to the original domain $\Omega_{\theta}$. We consider several problems to demonstrate the performance of the framewo
    
[^87]: SAIS: 一种基于共生模式的新型生物启发式人工免疫系统

    SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm

    [https://arxiv.org/abs/2402.07244](https://arxiv.org/abs/2402.07244)

    SAIS是一种基于共生模式的新型生物启发式人工免疫系统，通过并行处理克服了传统人工免疫系统和SOS算法在处理大规模人口和增强人口多样性方面的挑战。实验证明，在26个基准问题上，SAIS在性能上与最先进的SOS方法相当，并超过其他常见的AIS方法和进化算法。

    

    我们提出了一种新型的人工免疫系统（AIS）：共生人工免疫系统（SAIS），灵感来自生物学中的共生关系。SAIS与Symbiotic Organisms Search（SOS）算法中的三个关键阶段（互惠主义、共生主义和寄生主义）相对应。这种并行方法有效地解决了AIS中大规模人口和增强人口多样性的挑战，传统的AIS和SOS在这方面的解决效率较低。我们进行了一系列的实验证明，我们的SAIS在26个基准问题上实现了与最先进的SOS方法相当的性能，并超过了其他常见的AIS方法和进化算法。此外，我们调研了参数选择问题，并发现SAIS在处理更大规模人口时表现更好，并且需要更少的迭代代数。最后，我们相信SAIS作为一种新的生物启发式和免疫启发式算法，有望在优化问题中发挥重要作用。

    We propose a novel type of Artificial Immune System (AIS): Symbiotic Artificial Immune Systems (SAIS), drawing inspiration from symbiotic relationships in biology. SAIS parallels the three key stages (i.e., mutualism, commensalism and parasitism) of population updating from the Symbiotic Organisms Search (SOS) algorithm. This parallel approach effectively addresses the challenges of large population size and enhances population diversity in AIS, which traditional AIS and SOS struggle to resolve efficiently. We conducted a series of experiments, which demonstrated that our SAIS achieved comparable performance to the state-of-the-art approach SOS and outperformed other popular AIS approaches and evolutionary algorithms across 26 benchmark problems. Furthermore, we investigated the problem of parameter selection and found that SAIS performs better in handling larger population sizes while requiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired and immune-inspired al
    
[^88]: CPSDBench：一个针对中国公共安全领域的大型语言模型评估基准和基线

    CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain

    [https://arxiv.org/abs/2402.07234](https://arxiv.org/abs/2402.07234)

    CPSDBench是一个专门为中国公共安全领域量身定制的大型语言模型评估基准，通过整合实际场景中收集的公共安全相关数据集，针对文本分类、信息提取、问题回答和文本生成四个关键维度全面评估LLMs的性能，并引入创新的评估指标，提高了对现有模型在解决公共安全问题方面性能的理解。

    

    大型语言模型（LLMs）在多个应用领域展示了显著的潜力和效果。为了评估主流LLMs在公共安全任务中的性能，本研究旨在构建一个专门针对中国公共安全领域的评估基准——CPSDBench。CPSDBench整合了从现实场景中收集到的与公共安全相关的数据集，支持对LLMs在文本分类、信息提取、问题回答和文本生成四个关键维度上进行全面评估。此外，本研究还引入了一套创新的评估指标，旨在更精确地量化LLMs在执行与公共安全相关任务上的效力。通过本研究中的深入分析和评估，我们不仅增强了对现有模型在解决公共安全问题方面性能优势和局限性的理解，还为未来的研究提供了参考。

    Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the fu
    
[^89]: TransGPT：用于交通的多模式生成预训练Transformer

    TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation

    [https://arxiv.org/abs/2402.07233](https://arxiv.org/abs/2402.07233)

    TransGPT是一种面向交通领域的新型多模式生成预训练Transformer，使用单模式和多模式数据进行微调，在交通领域的各种任务中优于基准模型。

    

    自然语言处理（NLP）是智能交通系统（ITS）的关键组成部分，但在交通领域面临着许多挑战，如专业领域知识与数据，多模式输入和输出。本文提出了TransGPT，一种面向交通领域的新型（多模式）大语言模型，由两个独立的变体组成：TransGPT-SM用于单模式数据和TransGPT-MM用于多模式数据。TransGPT-SM在包含来自交通领域各种来源的文本数据的单模式交通数据集（STD）上进行微调。TransGPT-MM在我们手动收集的交通领域的三个领域（驾驶测试、交通标志和地标）的多模式交通数据集（MTD）上细调。我们对TransGPT在交通领域的几个基准数据集上进行评估，并展示了它在大多数任务上优于基准模型。我们还展示了该模型的潜在应用。

    Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and multi-modal inputs and outputs. This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is finetuned on a multi-modal Transportation dataset (MTD) that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several benchmark datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential application
    
[^90]: 大规模计算中的连续优化：推进模型推理应用

    Successive Refinement in Large-Scale Computation: Advancing Model Inference Applications

    [https://arxiv.org/abs/2402.07229](https://arxiv.org/abs/2402.07229)

    本文介绍了一种分层分辨率计算的解决方案，可以在时间约束下获得近似版本的最终结果，这对于基于截止日期的系统和某些操作模式下的决策系统具有重要意义。

    

    现代计算密集型应用通常在时间约束下操作，需要加速方法并将计算工作负载分布到多个实体上。然而，结果要么在所需时间内达到，要么没有，并且在后一种情况下，宝贵的资源被浪费。在本文中，我们引入了分层分辨率计算的解决方案。这些解决方案允许在最终结果之前的早期阶段获得较低分辨率的结果。这种创新显著增强了基于截止日期的系统，因为如果由于时间限制而终止计算任务，则仍可以生成最终结果的近似版本。此外，在某些操作模式下，高分辨率的结果可能是不必要的，因为低分辨率的结果可能已经与决策阈值显著偏离，例如在基于人工智能的决策系统中。因此，操作人员可以决定是否需要更高的分辨率。

    Modern computationally-intensive applications often operate under time constraints, necessitating acceleration methods and distribution of computational workloads across multiple entities. However, the outcome is either achieved within the desired timeline or not, and in the latter case, valuable resources are wasted. In this paper, we introduce solutions for layered-resolution computation. These solutions allow lower-resolution results to be obtained at an earlier stage than the final result. This innovation notably enhances the deadline-based systems, as if a computational job is terminated due to time constraints, an approximate version of the final result can still be generated. Moreover, in certain operational regimes, a high-resolution result might be unnecessary, because the low-resolution result may already deviate significantly from the decision threshold, for example in AI-based decision-making systems. Therefore, operators can decide whether higher resolution is needed or no
    
[^91]: 用条件扩散模型拼接子轨迹以实现目标条件下的离线强化学习

    Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL

    [https://arxiv.org/abs/2402.07226](https://arxiv.org/abs/2402.07226)

    本文提出了一种名为SSD的模型，该模型利用条件扩散模型来解决离线目标条件下强化学习中的问题，包括从次优行为数据集学习策略的困难以及在子轨迹中提取有用技能的需求。

    

    离线目标条件下的强化学习(Offline GCRL)是强化学习中一个重要问题，其专注于从预先收集的行为数据集中获取各种面向目标的技能。在这种设定中，奖励反馈通常只在达成目标时存在，这使得从有限的次优行为数据集中学习策略变得困难。此外，现实场景涉及长时程规划，这需要在子轨迹中提取有用的技能。最近，条件扩散模型已被证明是生成强化学习高质量长时程计划的一种有前景的方法。然而，由于这些方法中存在一些技术假设，其在目标条件设定下的实用性仍然有限。在本文中，我们提出了SSD(用扩散实现子轨迹拼接)，这是一种基于模型的离线GCRL方法，利用条件扩散模型来解决这些限制。

    Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we 
    
[^92]: 代理行为的原因：意图和工具性目标

    The Reasons that Agents Act: Intention and Instrumental Goals

    [https://arxiv.org/abs/2402.07221](https://arxiv.org/abs/2402.07221)

    本论文提出了一个对代理行为意图的操作化定义，并通过一些例子和结果展示了其灵活性和适用性。这一定义有助于理解和解释机器学习系统的行为，以及相关概念如工具性目标的关系。

    

    意图是人工智能中一个重要且具有挑战性的概念。它之所以重要，是因为它是许多其他我们关心的概念的基础，例如代理、操纵、法律责任和责备。然而，将意图归因于人工智能系统是有争议的，并且没有普遍接受的适用于人工智能代理的意图理论。我们通过将代理人行为的意图转化为其选择决策的原因，对意图进行了操作化定义。我们提出了一个在结构性因果影响模型中的意图定义，紧密结合在哲学文献中关于意图的观点，并适用于真实世界的机器学习系统。通过一些例子和结果，我们展示了我们的定义捕捉到了直观的意图概念，并符合过去研究的设定要求。此外，我们还展示了我们的定义与过去概念的关联，包括实际因果性和工具性目标的概念，后者是安全人工智能代理研究中的核心思想。最后，我们演示了我们的定义如何与实际机器学习系统中的实践相结合。

    Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how 
    
[^93]: 结合空间优化和大型语言模型的开放领域城市行程规划

    Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning

    [https://arxiv.org/abs/2402.07204](https://arxiv.org/abs/2402.07204)

    本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。

    

    本文首次提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程。OUIP与传统行程规划不同，传统规划限制了用户表达更详细的需求，阻碍了真正的个性化。最近，大型语言模型(LLM)在处理多样化任务方面表现出潜力。然而，由于非实时信息、不完整的知识和不足的空间意识，它们无法独立地提供满意的用户体验。鉴于此，我们提出了一个名为ItiNera的OUIP系统，将空间优化与大型语言模型(LLM)相结合，根据用户需求提供个性化的城市行程定制服务。具体来说，我们开发了一个基于LLM的流水线，用于提取和更新兴趣点特征，以创建用户自己的个性化兴趣点数据库。对于每个用户请求，我们利用LLM进行协同实现优化。

    In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
    
[^94]: 基于模式识别的时间图链接感知链接预测

    Link-aware link prediction over temporal graph by pattern recognition

    [https://arxiv.org/abs/2402.07199](https://arxiv.org/abs/2402.07199)

    提出了一种链接感知模型，通过将历史链接和查询链接一起输入模型层来进行链接预测。与之前的方法不同，该模型关注链接演化模式的建模而非节点表示。实验证明该模型在六个数据集上取得了很好的效果。

    

    一个时间图可以被视为一系列链接的流，每个链接代表了两个节点在某个时间点的交互。在时间图上，链接预测是一个常见任务，旨在回答查询链接是否为真。为了完成这个任务，之前的方法通常专注于学习查询链接中两个节点的表示。我们指出，由于之前的模型没有利用查询链接的信息，学习到的表示可能会编码过多的信息并对链接预测产生副作用，即它们不具有链接感知性。基于这个观察，我们提出了一个链接感知模型：历史链接和查询链接一起输入到模型层中，以区分该输入是否暗示了一个以查询链接结尾的合理模式。在这个过程中，我们关注的是链接演化模式的建模，而不是节点表示。在六个数据集上的实验证明，我们的模型取得了很好的效果。

    A temporal graph can be considered as a stream of links, each of which represents an interaction between two nodes at a certain time. On temporal graphs, link prediction is a common task, which aims to answer whether the query link is true or not. To do this task, previous methods usually focus on the learning of representations of the two nodes in the query link. We point out that the learned representation by their models may encode too much information with side effects for link prediction because they have not utilized the information of the query link, i.e., they are link-unaware. Based on this observation, we propose a link-aware model: historical links and the query link are input together into the following model layers to distinguish whether this input implies a reasonable pattern that ends with the query link. During this process, we focus on the modeling of link evolution patterns rather than node representations. Experiments on six datasets show that our model achieves stro
    
[^95]: GraphTranslator：将图模型与大型语言模型对齐用于开放式任务

    GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks

    [https://arxiv.org/abs/2402.07197](https://arxiv.org/abs/2402.07197)

    "GraphTranslator"是一个旨在将预训练的图模型和大型语言模型对齐的翻译器，可以同时处理预定义任务和开放式任务。通过将这两种模型结合起来，能够有效地处理各种任务，并实现更具创新性和灵活性的应用。

    

    大型语言模型（LLMs）如ChatGPT，展示了强大的零样本和遵循指令的能力，在人工智能的各个研究领域中引发了一场革命性的转变，尤其是对于开放式任务。尽管在图领域中这个想法较少被探索，尽管有许多强大的图模型（GMs）可用，但它们被限制在预定义形式的任务中。虽然已经提出了几种将LLMs应用于图的方法，但它们未能同时处理预定义任务和开放式任务，无论是将LLMs作为节点特征增强器还是作为独立预测器。为了打破这个困境，我们提出了一个名为GraphTranslator的翻译器，旨在通过预训练的GM和LLMs之间的桥梁，有效地处理预定义任务，并利用LLMs的扩展接口为GM提供各种开放式任务。为了训练这样的翻译器，我们提出了一个称为Producer的构建图文对齐数据的工具。

    Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node
    
[^96]: GSINA: 通过图Sinkhorn Attention改进图不变学习中的子图提取

    GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention

    [https://arxiv.org/abs/2402.07191](https://arxiv.org/abs/2402.07191)

    本文提出了一种改进的图不变学习方法，通过稀疏性、软性和可微性原则来提取不变子图，从而提高图学习的泛化性能。

    

    图不变学习(GIL)是一种有效的方法，用于在不同分布变化下发现图数据与其标签之间的不变关系，以解决各种图学习任务。最近的GIL研究主要集中在从输入图中提取不变子图，作为规则化策略来提高图学习的泛化性能。然而，这些方法在获取不变子图方面也存在各种限制。本文分析了现有工作的缺点，并提出了提取不变子图的相应原则：1）稀疏性，以过滤掉变异特征；2）软性，以获得更广泛的解空间；和3）可微性，以进行端到端优化。为了在一次操作中满足这些原则，我们利用最优传输(OT)理论，并提出了一种新颖的图注意机制，称为图Sinkhorn Attention（G)

    Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (G
    
[^97]: 一个加密视觉Transformer的随机集成方法用于对抗性强化防御

    A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense

    [https://arxiv.org/abs/2402.07183](https://arxiv.org/abs/2402.07183)

    本文提出了一个新方法，利用视觉Transformer构造一个随机集成的加密模型，以增强对抗白盒和黑盒攻击的能力，并在多个实验中验证了其在图像分类任务中的鲁棒性和优越性能。

    

    深度神经网络（DNN）已经被公认为易受对抗性样本（AEs）的攻击。在之前的研究中，使用秘密密钥加密的模型被证明可以抵抗白盒攻击，但对抗黑盒攻击则不行。本文提出了一种新方法，利用视觉Transformer（ViT）构造一个随机集成的加密模型，以增强对抗白盒和黑盒攻击的能力。此外，还使用了一个名为AutoAttack的基准攻击方法来客观测试模型的对抗性稳健性。在实验证明了该方法在CIFAR-10和ImageNet数据集上的图像分类任务中，不仅对抗白盒攻击具有强大的鲁棒性，还对抗黑盒攻击也具有强大的鲁棒性。该方法还在对抗鲁棒性的标准化基准RobustBench中与最先进的方法进行了比较，验证了在准确性和鲁棒准确性方面优于传统防御方法。

    Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In previous studies, the use of models encrypted with a secret key was demonstrated to be robust against white-box attacks, but not against black-box ones. In this paper, we propose a novel method using the vision transformer (ViT) that is a random ensemble of encrypted models for enhancing robustness against both white-box and black-box attacks. In addition, a benchmark attack method, called AutoAttack, is applied to models to test adversarial robustness objectively. In experiments, the method was demonstrated to be robust against not only white-box attacks but also black-box ones in an image classification task on the CIFAR-10 and ImageNet datasets. The method was also compared with the state-of-the-art in a standardized benchmark for adversarial robustness, RobustBench, and it was verified to outperform conventional defenses in terms of clean accuracy and robust accuracy.
    
[^98]: MAGNETO：边缘人体活动识别的边缘AI--隐私和个性化

    MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization

    [https://arxiv.org/abs/2402.07180](https://arxiv.org/abs/2402.07180)

    本文提出了一种名为MAGNETO的边缘AI平台，通过从云端推向边缘进行增量人体活动学习，避免了云端与边缘设备之间的数据传输，实现了数据隐私保护、低延迟处理和高度个性化。

    

    人体活动识别（HAR）是一个成熟的领域，现代机器学习（ML）技术显著推动了其发展。尽管公司成功地将HAR整合到消费品中，但它们通常依赖于预定义的活动集，这限制了用户级（边缘设备）的个性化。尽管在增量学习方面取得了进展，能够使用新数据更新模型，但这通常发生在云端，需要定期在云端和边缘设备之间进行数据传输，从而引发数据隐私问题。在本文中，我们提出了一种名为MAGNETO的边缘AI平台，将HAR任务从云端推向边缘。MAGNETO允许在边缘设备上直接进行增量人体活动学习，而无需与云端进行任何数据交换。这可以提供强大的隐私保证、低处理延迟和高度的个性化。特别地，我们在Android设备上演示了MAGNETO，从数据采集到结果可视化，验证了整个流程。

    Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visua
    
[^99]: EmoWear: 探索智能手表上声音信息交互的情感提示

    EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches

    [https://arxiv.org/abs/2402.07174](https://arxiv.org/abs/2402.07174)

    EmoWear是一种智能手表语音信息系统，通过使用动画提示反映情感，显著增强了情感交流体验。

    

    语音信息本质上使用户无法在完全听取音频内容之前判断情感语调，这阻碍了在预检索阶段的共享情感体验。研究很少探索"情感提示"——在不透露内容的情况下提供有关待收听信息情感语调的暗示。我们引入了EmoWear，一种智能手表语音信息系统，允许用户在信息气泡上应用30种动画提示来反映情感。EmoWear通过基于语义和声学处理的方式来帮助发送者选择情感。通过将EmoWear与使用彩色信息气泡作为情感提示的镜像系统进行比较(N=24)，我们评估了EmoWear。结果表明，EmoWear显著增强了接收和发送消息时的情感交流体验。动画提示被认为直观且具有多种表达方式。我们总结出了可行的交互质量和实际应用的未来前景。

    Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored "Emotional Teasers"-pre-retrieval cues offering a glimpse into an awaiting message's emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders' choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are distilled for future d
    
[^100]: 基于大型语言模型的调强放射治疗剂量容积直方图预测

    Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy

    [https://arxiv.org/abs/2402.07167](https://arxiv.org/abs/2402.07167)

    本论文介绍了一种使用深度学习模型和大型语言模型预测剂量容积直方图（DVH）的方法，通过将非结构化图像转化为结构化图，并利用大型语言模型编码处方和临床医生的指示，实现了对放射治疗计划质量的提高。

    

    治疗计划是放射治疗中一项耗时、资源密集的具体患者工作。剂量容积直方图（DVH）预测在自动化此过程中起到关键作用。放射治疗计划中DVH与风险器官（OAR）和计划靶体体积（PTV）的几何关系已经得到了很好的建立。本研究探讨了深度学习模型在使用图像和大型语言模型（LLM）促进人类干预以提高计划质量方面的潜力。我们提出了一种将非结构化图像转化为由图像块节点和剂量节点组成的结构化图的流程。我们开发了一种新颖的剂量图神经网络（DoseGNN）模型，用于从结构化图中预测DVH。所提出的DoseGNN通过LLM增强，以编码来自处方和临床医生的交互指示的大量知识。在本研究中，我们引入了一种在线人工智能协作（OHAC）系统。

    Treatment planning is currently a patient specific, time-consuming, and resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction plays a critical role in automating this process. The geometric relationship between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target volume (PTV) has been well established. This study explores the potential of deep learning models for predicting DVHs using images and subsequent human intervention facilitated by a large-language model (LLM) to enhance the planning quality. We propose a pipeline to convert unstructured images to a structured graph consisting of image-patch nodes and dose nodes. A novel Dose Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the structured graph. The proposed DoseGNN is enhanced with the LLM to encode massive knowledge from prescriptions and interactive instructions from clinicians. In this study, we introduced an online human-AI collaboration (OHAC) system a
    
[^101]: 发表文本的社会演化与通过大型语言模型出现的人工智能与毒性和偏见的问题

    Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias

    [https://arxiv.org/abs/2402.07166](https://arxiv.org/abs/2402.07166)

    本研究提供了人工智能和大型语言模型发展的全景视图，并指出了毒性、偏见和其他问题，同时强调人类大脑并不特殊，人类智能只是一个尺度上的新兴现象。

    

    我们提供了人工智能和深度学习的快速发展的全景视图，这导致了人工智能在大型语言模型中的突破性出现。本研究的目的是将所有这些发展置于实用的广泛历史社会视角中，既不夸大其效果，又不产生1970年代到1990年代人工智能冬季的悲观情绪。同时，我们还指出存在毒性、偏见、记忆、谄媚、逻辑不一致和幻觉等问题，只是作为对过于乐观的警示。我们在此指出，正如人工智能的出现似乎发生在神经连接或权重数量的临界点上，人类大脑尤其是皮质区域并没有什么特殊或超凡，只是灵长类大脑的一个放大版，甚至人类智能似乎只是一个尺度上的新兴现象。

    We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.
    
[^102]: 自然语言强化学习

    Natural Language Reinforcement Learning

    [https://arxiv.org/abs/2402.07157](https://arxiv.org/abs/2402.07157)

    本研究将自然语言表示和强化学习原则相结合，提出了自然语言强化学习（NLRL）框架，解决了强化学习在样本效率低、解释性不足和缺乏监督信号等方面的限制问题，通过实验验证了其有效性和可解释性。

    

    强化学习（RL）在学习决策任务的策略方面展现出了令人瞩目的能力。然而，RL常常面临样本效率低、解释性不足和缺乏稀疏监督信号等问题的限制。为了解决这些问题，我们从人类学习过程中汲取灵感，引入了自然语言强化学习（NLRL），创新性地将RL原则与自然语言表示结合起来。具体而言，NLRL在自然语言空间中重新定义了任务目标、策略、价值函数、Bellman方程和策略迭代等RL概念。我们还展示了如何利用最新的大型语言模型（LLM）如GPT-4来实现NLRL。对表格MDPs的初步实验表明了NLRL框架的有效性、高效性和可解释性。

    Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
    
[^103]: 物理信息神经网络逼近半线性波动方程的误差估计

    Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations

    [https://arxiv.org/abs/2402.07153](https://arxiv.org/abs/2402.07153)

    本文提供了物理信息神经网络逼近半线性波动方程的严格误差界限，包括泛化误差和训练误差的界限，并在数值实验中展示了理论界限的有效性。

    

    本文对物理信息神经网络逼近半线性波动方程提供了严格的误差界限。我们针对具有两个隐藏层的tanh神经网络，基于网络层宽度和训练点数量，提供了对泛化误差和训练误差的界限。我们的主要结果是在一些假设下，将总误差以$H^1([0,T];L^2(\Omega))$-范数的形式表示，并能够随着训练点数量的增加而任意减小。我们通过数值实验验证了我们的理论界限。

    This paper provides rigorous error bounds for physics-informed neural networks approximating the semilinear wave equation. We provide bounds for the generalization and training error in terms of the width of the network's layers and the number of training points for a tanh neural network with two hidden layers. Our main result is a bound of the total error in the $H^1([0,T];L^2(\Omega))$-norm in terms of the training error and the number of training points, which can be made arbitrarily small under some assumptions. We illustrate our theoretical bounds with numerical experiments.
    
[^104]: 使用图神经网络的可解释全球野火预测模型解释

    Explainable Global Wildfire Prediction Models using Graph Neural Networks

    [https://arxiv.org/abs/2402.07152](https://arxiv.org/abs/2402.07152)

    本研究提出了一个创新的基于图神经网络的全球野火预测模型，将全球气候和野火数据转化为图表示，解决了传统模型中的海洋数据缺失和远程依赖性问题，并展示了更高的预测准确性。同时，该模型还具有可解释性，揭示了其潜在价值。

    

    由于气候变化的不断加剧，野火预测变得越来越关键。传统的基于CNN的野火预测模型在处理缺失的海洋数据和解决气象数据中远程地区的长期依赖性方面存在困难。本文引入一种创新的基于图神经网络（GNN）的全球野火预测模型。我们提出了一种混合模型，将图卷积网络（GCNs）的空间能力与长短期记忆（LSTM）网络的时间深度相结合。我们的方法将全球气候和野火数据独特地转化为图表示，解决了传统模型中存在的空洞海洋数据位置和长期依赖性等挑战。通过使用未知的JULES-INFERNO模拟集对已建立的架构进行基准测试，我们的模型展现出优越的预测准确性。此外，我们强调了模型的可解释性，揭示了其中的潜在价值。

    Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional CNN-based wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data. In this paper, we introduce an innovative Graph Neural Network (GNN)-based model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long Short-Term Memory (LSTM) networks. Our approach uniquely transforms global climate and wildfire data into a graph representation, addressing challenges such as null oceanic data locations and long-range dependencies inherent in traditional models. Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model's explainability, unveiling poten
    
[^105]: X-LoRA: 一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略在蛋白质力学和设计中的应用

    X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design

    [https://arxiv.org/abs/2402.07148](https://arxiv.org/abs/2402.07148)

    X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。

    

    我们报道了一种使用深层逐层基于低秩适应（LoRA）的新颖预训练适配器的混合专家策略，用于创建精细调整的大型语言模型。我们提出了一种利用隐藏状态动态混合经过适应的层的门控策略，允许得到的X-LoRA模型利用不同的能力并创建以前未使用的深层逐层适应的组合来解决特定任务。该设计受到了生物普遍性和多样性的生物学原理的启发，其中神经网络建模块在不同的分层表示中被重复使用。因此，X-LoRA模型可以轻松用于任何现有的大型语言模型（LLM），无需修改底层结构。我们还开发了一个定制的X-LoRA模型，提供了包括前向/逆向分析任务和增强推理能力在内的科学能力，重点是生物材料分析。

    We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
    
[^106]: 文字描述中的顺序对大语言模型的空间感知能力的影响

    Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models

    [https://arxiv.org/abs/2402.07140](https://arxiv.org/abs/2402.07140)

    这项研究揭示了图描述的文本顺序对大语言模型在图推理中的性能产生显著影响，并通过改变文本顺序提高了大语言模型的性能。此外，发现大语言模型的推理性能与图大小之间的关系不是单调递减的。为了评估大语言模型在不同图大小上的性能，引入了规模化图推理基准。

    

    最近几年，大语言模型在多个领域达到了最先进的性能。然而，图推理领域的进展仍然有限。我们的工作深入研究了大语言模型的图推理。在这项工作中，我们揭示了文本顺序对大语言模型空间理解的影响，发现图描述的文本顺序显著影响大语言模型对图的推理性能。通过改变图描述的文本顺序，我们将大语言模型的性能从42.22％提高到70％。此外，我们评估了大语言模型性能和图大小之间的关系，发现大语言模型的推理性能不随图大小的增加而单调递减。最后，我们引入了规模化图推理基准来评估大语言模型在不同图大小上的性能。

    In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.
    
[^107]: 从去噪扩散隐式模型的潜空间中生成新的桥梁类型的尝试

    An attempt to generate new bridge types from latent space of denoising diffusion Implicit model

    [https://arxiv.org/abs/2402.07129](https://arxiv.org/abs/2402.07129)

    本论文尝试使用去噪扩散隐式模型创新桥梁类型，通过易于理解的代数方法推导出函数公式，使用深度学习平台构建模型，并利用潜空间采样生成具有非对称结构的新桥梁类型。

    

    使用去噪扩散隐式模型进行桥梁创新。将图像添加噪声和去噪的过程类比为尸体腐烂和侦探恢复被杀害的受害者现场的过程，以帮助初学者理解。通过易于理解的代数方法，推导出添加噪声和去噪的函数公式，使初学者更容易掌握模型的数学原理。使用三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集，基于Python编程语言、TensorFlow和Keras深度学习平台框架，构建和训练了去噪扩散隐式模型。从潜空间采样中，可以生成具有非对称结构的新桥梁类型。去噪扩散隐式模型可以在人类原始桥梁类型的基础上，有机地组合不同的结构组件，创造新的桥梁类型。

    Use denoising diffusion implicit model for bridge-type innovation. The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand. Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained. From the latent space sampling, new bridge types with asymmetric structures can be generated. Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types.
    
[^108]: 观察学习：基于视频的机器人操作学习方法综述

    Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation

    [https://arxiv.org/abs/2402.07127](https://arxiv.org/abs/2402.07127)

    

    

    机器人学习操作技能受到多样化、无偏的数据集的稀缺性的影响。尽管策划的数据集可以帮助解决问题，但在泛化性和现实世界的转移方面仍然存在挑战。与此同时，“野外”视频数据集的大规模存在通过自监督技术推动了计算机视觉的进展。将这一点应用到机器人领域，最近的研究探索了通过被动观察来学习丰富的在线视频中的操作技能。这种基于视频的学习范式显示出了有希望的结果，它提供了可扩展的监督方法，同时降低了数据集的偏见。本综述回顾了视频特征表示学习技术、物体可行性理解、三维手部/身体建模和大规模机器人资源等基础知识，以及从不受控制的视频演示中获取机器人操作技能的新兴技术。我们讨论了仅从观察大规模人类视频中学习如何增强机器人的泛化性和样本效率。

    Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
    
[^109]: 下一代远程眼科诊疗：基于人工智能的质量评估帮助远程基于智能手机的咨询

    Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation

    [https://arxiv.org/abs/2402.07118](https://arxiv.org/abs/2402.07118)

    本研究提出了一种基于人工智能的眼科质量评估系统，可以帮助远程眼科咨询，尤其是在低收入和中等收入国家。该系统能够即时反馈用户拍摄的眼部图像的质量，解决了当前用户拍摄图像质量不佳的问题。

    

    失明和其他眼部疾病是全球关注的健康问题，尤其是在印度等低收入和中等收入国家。在这方面，在COVID-19大流行期间，远程眼科诊疗成为一种生命线，并且智能手机眼部成像的 Grabi 附件得到了广泛使用。然而，用户拍摄的图片质量往往不够好，需要医生审核并且会延误时间。在这种背景下，我们提出了一种基于人工智能的质量评估系统，能够模拟医生的判断并且能够即时反馈，我们对患者拍摄的图像进行了测试。将复杂问题层次化，我们在此解决了一个非常重要的部分，并证明了该概念的可行性。

    Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.
    
[^110]: 索crates怀疑的回声：在校准的证据增强学习中接受不确定性

    Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning

    [https://arxiv.org/abs/2402.07107](https://arxiv.org/abs/2402.07107)

    这篇论文提出了一个新的统计方法，利用深度Q网络和分位数回归来在模型-free的分布式强化学习中引入不确定性，该方法通过结合深度证据学习和基于合规推理原则的分位数校准，提供了全局不确定性的显式、无样本计算，具有更高的计算和统计效率，并成功处理了超出分布范围的观测数据。

    

    我们提出了一种新颖的统计方法，用于在基于模型的分布强化学习中引入不确定性意识，涉及基于分位数回归的深度Q网络。提出的算法$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$旨在解决在随机环境中分别估计aleatoric和epistemic不确定性所面临的关键挑战。它将深度证据学习与基于合规推理原则的分位数校准相结合，提供了显式的、无样本计算的$\textit{全局}$不确定性，而不是基于简单方差的$\textit{局部}$估计，克服了传统方法在计算和统计效率以及处理超出分布范围的观测数据方面的局限性。在一套小型化的Atari游戏（即MinAtar）上进行测试，CEQR-DQN在得分和学习速度方面超过了类似的现有框架。它能够严谨地处理外部数据观测，并提供更高的计算和统计效率。

    We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
    
[^111]: 未来预测可以成为部分可观测环境中良好历史表达的有力证据

    Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments

    [https://arxiv.org/abs/2402.07102](https://arxiv.org/abs/2402.07102)

    未来预测在部分可观测环境中学习 History Representation 具有很强的相关性和有效性。

    

    学习良好的历史表达是部分可观测环境中强化学习的核心挑战之一。最近的研究表明，各种辅助任务对促进表达学习具有优势。然而，这些辅助任务的有效性尚未完全使人信服，特别是在需要长期记忆和推理的部分可观测环境中。在这个实证研究中，我们探讨了未来预测在学习部分可观测环境中历史表达时的有效性。我们首先提出了一种通过未来预测将学习历史表达与策略优化分离的方法。然后，我们的主要贡献有两个方面：（a）我们证明了强化学习的性能与部分可观测环境中未来观测的预测精度强相关，（b）我们的方法可以有效地学习部分可观测环境中长时间历史的表达方式。

    Learning a good history representation is one of the core challenges of reinforcement learning (RL) in partially observable environments. Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning. However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference. In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments. We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction. Then, our main contributions are two-fold: (a) we demonstrate that the performance of reinforcement learning is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approa
    
[^112]: 使用合成数据改进托盘检测

    Improving Pallet Detection Using Synthetic Data

    [https://arxiv.org/abs/2402.07098](https://arxiv.org/abs/2402.07098)

    本研究通过使用合成数据和Unity平台生成数据，提高了在仓库环境中托盘检测的性能。此外，研究还发现模型在光线较暗的环境下性能较差，并且使用YOLOv8和SAM的两阶段检测器具有不稳定的性能。

    

    在机器学习中使用合成数据可以节省大量时间，实现有效的物体检测器。然而，在这个领域的研究有限。本研究旨在改进在仓库环境中托盘实例分割任务中之前应用的实现方法。本研究提出使用合成生成的领域随机化数据以及通过Unity生成的数据来实现这一目标。当在真实数据上进行评估时，本研究在堆叠和架式托盘类别上的性能提升分别为69%和50%的mAP50。此外，当模型在光线较暗的环境中评估时，其性能会受到显著影响，当在亮度降低80%的图像上进行评估时，mAP50会降低到最低3%。本研究还创建了一个使用YOLOv8和SAM的两阶段检测器，但其性能不稳定。

    The use of synthetic data in machine learning saves a significant amount of time when implementing an effective object detector. However, there is limited research in this domain. This study aims to improve upon previously applied implementations in the task of instance segmentation of pallets in a warehouse environment. This study proposes using synthetically generated domain-randomised data as well as data generated through Unity to achieve this. This study achieved performance improvements on the stacked and racked pallet categories by 69% and 50% mAP50, respectively when being evaluated on real data. Additionally, it was found that there was a considerable impact on the performance of a model when it was evaluated against images in a darker environment, dropping as low as 3% mAP50 when being evaluated on images with an 80% brightness reduction. This study also created a two-stage detector that used YOLOv8 and SAM, but this proved to have unstable performance. The use of domain-rand
    
[^113]: 自我纠正自我消耗循环用于生成模型训练

    Self-Correcting Self-Consuming Loops for Generative Model Training

    [https://arxiv.org/abs/2402.07087](https://arxiv.org/abs/2402.07087)

    本论文研究了使用合成数据进行生成模型训练时可能出现的自我消耗循环问题，并提出了一种通过引入理想的修正函数来稳定训练的方法。同时，我们还提出了自我修正函数来近似理想的修正函数，并通过实验证实了其有效性。

    

    随着合成数据在互联网上的质量越来越高以及数量不断增加，机器学习模型越来越多地在人工和机器生成的数据的混合上进行训练。尽管使用合成数据进行表征学习的成功案例有很多，但是在生成模型训练中使用合成数据会产生"自我消耗循环"，这可能导致训练不稳定甚至崩溃，除非满足某些条件。我们的论文旨在稳定自我消耗的生成模型训练。我们的理论结果表明，通过引入一个理想的修正函数，将数据点映射为更有可能来自真实数据分布的样本，可以使自我消耗循环的稳定性呈指数增加。然后，我们提出了自我修正函数，它依赖于专家知识（例如，编程在模拟器中的物理定律），并且旨在自动且大规模地近似理想的修正函数。我们通过实验证实了自我纠正自我消耗循环在生成模型训练中的有效性。

    As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
    
[^114]: 通过对比预训练，改善多领域B2B云解决方案匹配

    Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training

    [https://arxiv.org/abs/2402.07076](https://arxiv.org/abs/2402.07076)

    通过对比预训练和多领域匹配结构，我们提出了CAMA框架来改善B2B云解决方案的匹配问题，克服了复杂特征和有限数据的挑战。

    

    云解决方案在技术行业中非常受欢迎，因为它们提供了一系列服务和工具来解决特定问题。然而，尽管它们被广泛使用，但解决方案提供商的销售团队仍然面临一个复杂的业务问题，即找到适合特定目标解决方案的合适的公司客户，现有的匹配系统尚未能够充分解决这个问题。在这项工作中，我们研究了B2B解决方案匹配问题，并确定了该场景的两个主要挑战：(1) 复杂多领域特征的建模和(2) 有限、不完整和稀疏的交易数据。为了解决这些挑战，我们提出了一个框架CAMA，它以分层多领域匹配结构为主干，并通过三种数据增强策略和对比预训练目标来弥补可用数据的不完善之处。通过对一个真实数据集进行大量实验，我们证明了...

    Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstra
    
[^115]: 使用大型语言模型自动化和加速奖励机器强化学习

    Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine

    [https://arxiv.org/abs/2402.07069](https://arxiv.org/abs/2402.07069)

    这篇论文介绍了一种使用大型语言模型自动生成自动机来编码高级知识，加速强化学习过程的算法，并证明了其在多个任务上的有效性和优越性。

    

    我们提出了LARL-RM（通过大型语言模型生成的用于奖励机器强化学习的自动机）算法，以将高级知识编码到强化学习中，使用自动机加速强化学习过程。我们的方法使用大型语言模型（LLM）通过提示工程获得高级领域特定知识，而不是直接将高级知识提供给强化学习算法，这需要专家来编码自动机。我们使用思维链和少样本方法进行提示工程，并证明了我们的方法在这些方法下有效。此外，LARL-RM允许完全闭环的强化学习，无需专家来指导和监督学习，因为LARL-RM可以直接使用LLM生成所需的高级知识以完成任务。我们还证明了算法收敛到最优策略的理论保证。我们证明了LARL-RM的实验结果展示了其对常见的强化学习问题具有非常好的性能，并且在一些任务上超越了目前最先进的方法。

    We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-R
    
[^116]: $L^*LM$: 通过自然语言定义示例学习自动机

    $L^*LM$: Learning Automata from Examples using Natural Language Oracles

    [https://arxiv.org/abs/2402.07051](https://arxiv.org/abs/2402.07051)

    该论文提出了一个名为 $L^*LM$ 的算法，通过自然语言和演示学习 DFA，提高了数据效率，具备强大的少样本学习能力。

    

    专家演示已被证明是简化间接指定复杂任务的一种方法。最近的算法甚至支持从演示中提取明确的形式规范，如确定性有限自动机（DFA）。不幸的是，这些技术通常不具备高样本效率。在本文中，我们介绍了一种名为 $L^*LM$ 的算法，用于从演示和自然语言中学习 DFA。由于自然语言的表达能力，我们观察到从专家演示中学习 DFA 的数据效率显著提高。从技术上讲，$L^*LM$ 利用大型语言模型来回答关于底层任务的成员查询。然后将其与最近的演示学习技术相结合，将学习转化为一系列带标签示例学习问题。在我们的实验中，我们观察到这两种模态相互补充，从而产生了一个强大的少样本学习器。

    Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.
    
[^117]: 一种用于协作多智能体系统的信任因子图模型

    A Factor Graph Model of Trust for a Collaborative Multi-Agent System

    [https://arxiv.org/abs/2402.07049](https://arxiv.org/abs/2402.07049)

    本研究提出了一种利用因子图模型表示智能体之间相互依赖行为和可信度的方法。该方法以高斯过程因子图对机器人的行为进行建模，并考虑了平滑性、避障和信任相关因素。信任评估是分散的，考虑了接近安全性、一致性和合作性等关键因素。整个系统是由一系列因子图组成的网络，通过信任相关因素进行交互，并采用贝叶斯推理方法动态评估信任。

    

    在多智能体系统领域，信任其他智能体的资源和服务的能力至关重要。我们的论文介绍了一种利用因子图模型来表示智能体之间的相互依赖行为和可信度的新方法。该方法将机器人的行为建模为动作轨迹，并使用高斯过程因子图来考虑平滑性、避障和信任相关因素。我们的信任评估方法是分散的，考虑了关键的相互依赖子因素，如接近安全性、一致性和合作性。整个系统是由一系列因子图组成的网络，通过信任相关因素进行交互，并采用贝叶斯推理方法动态评估信任。

    In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial. Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system. Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents. This includes modeling the behavior of robots as a trajectory of actions using a Gaussian process factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors. Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation. The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically asses
    
[^118]: 尾巴的故事：作为尺度律变化的模型崩溃

    A Tale of Tails: Model Collapse as a Change of Scaling Laws

    [https://arxiv.org/abs/2402.07043](https://arxiv.org/abs/2402.07043)

    本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。

    

    随着AI模型大小的增长，神经尺度律已成为预测大模型在扩容和原始（人类或自然）训练数据大小增加时改善的关键工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将逐渐包含越来越多的合成数据。在本文中，我们问：当合成数据进入训练语料库时，尺度律会如何改变？未来的模型仍会改善，还是注定会完全崩溃（模型崩溃）？通过尺度律的视角，我们开发了一个模型崩溃的理论框架。我们发现了广泛的衰减现象，分析了尺度的丧失、与代数的变化尺度、技能的"遗忘"以及混合人类和合成数据时的理解能力。我们的理论通过对一个算术任务和文本生成的转换器进行大规模实验证实。

    As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
    
[^119]: 人工智能的协调披露：超越安全漏洞

    Coordinated Disclosure for AI: Beyond Security Vulnerabilities

    [https://arxiv.org/abs/2402.07039](https://arxiv.org/abs/2402.07039)

    这篇论文提出了一种针对机器学习和人工智能问题的协调缺陷披露（CFD）框架，以解决目前领域中缺乏结构化过程的问题。

    

    目前，人工智能领域的伤害报告在披露或解决算法缺陷方面仍然是一种临时性的操作，缺乏结构化的过程。相比之下，协调漏洞披露（CVD）的伦理和生态系统在软件安全和透明度方面发挥着关键作用。在美国的背景下，为了鼓励秉持善意行事的安全研究人员，建立一个安全防护条款以对抗计算机欺诈和滥用法案一直存在长期的法律和政策斗争。值得注意的是，机器学习（ML）模型中的算法缺陷与传统软件漏洞存在着不同的挑战，需要一种专门的方法。为了解决这一差距，我们提出了一种针对机器学习和人工智能问题特殊复杂性的专门协调缺陷披露（CFD）框架的实施。本文深入研究了ML中的披露历史背景，包括

    Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws. In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency. Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith. Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach. To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues. This paper delves into the historical landscape of disclosures in ML, encompassing the a
    
[^120]: 将符号先验知识融入神经网络的抽象概念学习

    Distilling Symbolic Priors for Concept Learning into Neural Networks

    [https://arxiv.org/abs/2402.07035](https://arxiv.org/abs/2402.07035)

    本论文通过元学习的方法，将符号贝叶斯模型的先验知识提取到神经网络中，使神经网络具有显示归纳偏见的能力，从而加快对抽象概念的学习。

    

    人类可以通过利用归纳偏见从少量的示例中学习新的概念。这些归纳偏见先前已通过在符号假设空间上定义贝叶斯模型来捕捉。是否可能创建一个显示相同归纳偏见的神经网络？我们展示了通过元学习（一种从一组任务中提取共同结构的方法）将符号贝叶斯模型的先验分布提取到人工神经网络中，可以实例化能够快速学习概念的归纳偏见。通过以贝叶斯模型的先验分布生成元学习中使用的任务集，我们能够将该先验传输到神经网络中。我们利用这种方法创建了一个具有对以短逻辑公式表示的概念的归纳偏见的神经网络。通过分析先前人们从少量示例中学习逻辑概念的行为实验结果，我们发现我们的元训练方案可以使神经网络快速学习这些概念。

    Humans can learn new concepts from a small number of examples by drawing on their inductive biases. These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces. Is it possible to create a neural network that displays the same inductive biases? We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by distilling a prior distribution from a symbolic Bayesian model via meta-learning, an approach for extracting the common structure from a set of tasks. By generating the set of tasks used in meta-learning from the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network. We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas. Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our meta-train
    
[^121]: Fiddler：用于Mixture-of-Experts模型快速推断的CPU-GPU编排

    Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models

    [https://arxiv.org/abs/2402.07033](https://arxiv.org/abs/2402.07033)

    本文介绍了Fiddler，一种用于Mixture-of-Experts模型的资源高效推断引擎，通过CPU-GPU编排实现最小化数据传输，相比现有方法提高了一个数量级的推断速度。

    

    基于Mixture-of-Experts（MoE）架构的大型语言模型（LLM）在各种任务上表现出了很好的性能。然而，在资源受限的环境下运行这些模型，即GPU内存资源不丰富的情况下，由于模型规模庞大，存在挑战。现有的将模型权重卸载到CPU内存的系统，由于频繁地在CPU和GPU之间移动数据而导致显著的开销。在本文中，我们提出了Fiddler，一种用于MoE模型的资源高效推断引擎，实现了CPU-GPU编排。Fiddler的核心思想是利用CPU的计算能力来最小化CPU和GPU之间的数据传输。我们的评估结果表明，Fiddler能够在单个具有24GB内存的GPU上运行未压缩的Mixtral-8x7B模型（参数超过90GB），每秒生成超过3个token，相比现有方法提高一个数量级。Fiddler的代码可以公开访问，网址为\url{https://github.com/efeslab/fiddler}

    Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
    
[^122]: 实例级别的安全感知与合成数据质量及其校准

    Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration

    [https://arxiv.org/abs/2402.07031](https://arxiv.org/abs/2402.07031)

    本论文研究了实例级别的合成数据质量与安全感知，引入了四种超越纯视觉特征的合成数据质量，并提出了优化方法来减少合成和真实图像之间的质量差距。

    

    建模和校准合成数据的质量对塑造未来安全可靠的自动驾驶技术至关重要，它提供了一种成本效益高且可扩展的替代方案，可以取代真实世界的数据收集。我们关注其在安全关键应用中的作用，引入了超越纯视觉输入特征的四种实例级别质量，旨在使合成数据与现实世界的安全问题相一致。我们提出了一种优化方法来改进合成数据生成器，减少由基于DNN的组件识别出的质量差距。我们的研究结果表明，这种调优可以增强合成和真实图像中安全关键错误之间的相关性。

    Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.
    
[^123]: 双子座进入医学院：探索多模态大型语言模型在医学挑战问题和幻觉上的能力

    Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations

    [https://arxiv.org/abs/2402.07023](https://arxiv.org/abs/2402.07023)

    该论文综合评估了开源LLM和谷歌的多模态LLM Gemini 在医学推理、幻觉检测和医学视觉问答任务上的能力。Gemini在诊断准确性方面落后于最先进模型，且易出现幻觉、过度自信和知识盲点。采用提示策略可以提高性能。

    

    大型语言模型在医疗行业具有潜在价值，但通过严格评估来验证其安全性和效果至关重要。为此，我们全面评估了开源LLM和谷歌的新型多模态LLM Gemini 在医学推理、幻觉检测和医学视觉问答任务上的能力。虽然Gemini表现出一定的能力，但在诊断准确性方面落后于MedPaLM 2和GPT-4等最先进模型。此外，Gemini在医学VQA数据集上的准确率为61.45％，明显低于GPT-4V的88％得分。我们的分析发现，Gemini极易出现幻觉、过度自信和知识盲点，这表明如果不加批判地部署，存在风险。我们还针对不同医学学科和测试类型进行了详细分析，为开发人员和临床医生提供了可操作的反馈。为了减少风险，我们采用了提示策略来提高性能。

    Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performanc
    
[^124]: REALM:通过大型语言模型对多模态电子健康记录分析的增强推理

    REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models

    [https://arxiv.org/abs/2402.07016](https://arxiv.org/abs/2402.07016)

    REALM是一个通过大型语言模型和检索增强生成驱动的框架，用于增强多模态电子健康记录（EHR）的表征，以提高临床预测能力。

    

    多模态电子健康记录（EHR）数据的整合显著提高了临床预测能力。然而，现有模型在利用临床笔记和多变量时间序列EHR时往往缺乏与临床任务相关的医疗背景。为了解决这个问题，我们提出了REALM框架，通过检索增强生成（RAG）驱动来增强多模态EHR表征，进而解决这些限制。该框架首先应用大型语言模型（LLM）编码长上下文临床笔记，并应用GRU模型编码时间序列EHR数据；其次，我们指导LLM提取与任务相关的医学实体，并将实体与专业标注的外部知识图（PrimeKG）中的相应实体匹配。

    The integration of multimodal Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time-series EHR, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph (KG). Previous approaches with KG knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge. In response, we propose REALM, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR representations that address these limitations. Firstly, we apply Large Language Model (LLM) to encode long context clinical notes and GRU model to encode time-series EHR data. Secondly, we prompt LLM to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding m
    
[^125]: FedImpro: 测量和改善联邦学习中的客户更新

    FedImpro: Measuring and Improving Client Update in Federated Learning

    [https://arxiv.org/abs/2402.07011](https://arxiv.org/abs/2402.07011)

    本文提出了FedImpro方法，通过生成改进的本地模型来减轻联邦学习中的客户漂移问题。该方法通过分析本地训练的泛化贡献，并利用类似的条件分布进行训练，增强了泛化贡献并减小了梯度的差异性。

    

    联邦学习模型通常会受到异构数据引起的客户漂移的影响，其中数据的分布在不同的客户之间存在差异。为了解决这个问题，先进的研究主要关注于操作现有的梯度，以实现更一致的客户模型。在本文中，我们从另一个角度分析了客户漂移，并旨在通过生成改进的本地模型来减轻这种漂移。首先，我们分析了本地训练的泛化贡献，并得出结论，这种泛化贡献受到不同客户的数据分布之间的条件Wasserstein距离的限制。然后，我们提出了FedImpro，用于构建类似的条件分布进行本地训练。具体而言，FedImpro将模型分解为高层和低层组件，并对重构特征分布上的高层部分进行训练。这种方法增强了泛化贡献，并减小了联邦学习中梯度的差异性。

    Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper
    
[^126]: 客户端协作：具有保证隐私-效用权衡改进的灵活差分隐私联邦学习

    Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off

    [https://arxiv.org/abs/2402.07002](https://arxiv.org/abs/2402.07002)

    本论文提出了一种名为FedCEO的新型联邦学习框架，在保护用户隐私的同时，通过让客户端相互协作，实现了对模型效用和隐私之间的权衡。通过高效的张量低秩近端优化，该框架能够恢复被打断的语义信息，并在效用-隐私权衡方面取得了显著的改进。

    

    为了防止用户数据的隐私泄漏，在联邦学习中广泛使用差分隐私，但它并不是免费的。噪声的添加会随机干扰模型的语义完整性，并且这种干扰会随着通信轮次的增加而累积。在本文中，我们引入了一种具有严格隐私保证的新型联邦学习框架，名为FedCEO，通过让客户端"相互协作"，旨在在模型效用和用户隐私之间找到一种权衡。具体而言，我们在服务器上对堆叠的本地模型参数进行了高效的张量低秩近端优化，展示了它在光谱空间中灵活截断高频组分的能力。这意味着我们的FedCEO能够通过平滑全局语义空间来有效恢复被打断的语义信息，以适应不同隐私设置和持续的训练过程。此外，我们将SOTA的效用-隐私权衡边界提高了一个数量级。

    To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqr
    
[^127]: 语音转换成歌曲的幻象的合理分析

    A Rational Analysis of the Speech-to-Song Illusion

    [https://arxiv.org/abs/2402.06992](https://arxiv.org/abs/2402.06992)

    本论文提出了对语音转换成歌曲的幻象的合理分析，将其视为一种统计推断过程，通过分析语料库，还发现了一种纯文本的小说转歌词的幻象，并提供了强有力的证据来支持这一观点。

    

    语音转换成歌曲的幻象是一种强大的心理现象，即说出的句子在不断重复中越来越像音乐。尽管经过几十年的研究，对这种转化的完整形式解释仍然缺乏，并且对其细微的特征，即某些短语的转化是否发生，也不太清楚。在这里，我们提供了这个现象的一个形式化解释，将其重新定义为一种统计推断，合理的决策者试图判断一系列话语更有可能是在歌曲中还是在讲话中产生的。通过使用这种方法并分析歌曲和讲话的语料库，我们进一步介绍了一种纯文本的小说转歌词的幻象。在这个幻象中，简单地复制书面句子会使它们看起来更像歌词。我们在人类参与者和大型语言模型中提供了这个新幻象的强有力的证据。

    The speech-to-song illusion is a robust psychological phenomenon whereby a spoken sentence sounds increasingly more musical as it is repeated. Despite decades of research, a complete formal account of this transformation is still lacking, and some of its nuanced characteristics, namely, that certain phrases appear to transform while others do not, is not well understood. Here we provide a formal account of this phenomenon, by recasting it as a statistical inference whereby a rational agent attempts to decide whether a sequence of utterances is more likely to have been produced in a song or speech. Using this approach and analyzing song and speech corpora, we further introduce a novel prose-to-lyrics illusion that is purely text-based. In this illusion, simply duplicating written sentences makes them appear more like song lyrics. We provide robust evidence for this new illusion in both human participants and large language models.
    
[^128]: OSSAR: 面向机器辅助手术中的开放集手术活动识别

    OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery

    [https://arxiv.org/abs/2402.06985](https://arxiv.org/abs/2402.06985)

    OSSAR是一种创新的开放集手术活动识别框架，通过利用超球面互补点策略来区分已知类别和未知类别，并通过改善模型校准解决封闭集中过于自信的问题。

    

    在自动化机器人手术和计算机辅助干预领域，理解机器人手术活动至关重要。现有的手术活动识别算法主要针对预定义的封闭集范式，忽视了现实世界开放集场景的挑战。这些算法往往在训练阶段未见过的类别的测试样本存在时难以应对。为解决这个问题，我们引入了一种创新的开放集手术活动识别(OSSAR)框架。我们的解决方案利用超球面互补点策略增强了特征空间中已知类别和未知类别之间的区分度。此外，我们通过改善模型校准来解决封闭集中过于自信的问题，避免将未知类别误分类为已知类别。为了支持我们的论述，我们建立了一个基于公共JIGSAWS数据集的开放集手术活动基准。此外，我们还收集了

    In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we also collect
    
[^129]: 使用多参数术前磁共振成像进行治疗型脑胶质母细胞瘤生存预测

    Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI

    [https://arxiv.org/abs/2402.06982](https://arxiv.org/abs/2402.06982)

    本研究旨在使用多参数术前磁共振成像预测脑胶质母细胞瘤患者在不同治疗下的生存时间，通过考虑治疗信息与磁共振扫描相结合的模型，实现个体化和精确的治疗规划。

    

    本研究旨在基于术前磁共振扫描预测接受不同治疗的脑胶质母细胞瘤患者的生存时间。通过比较不同治疗方案的生存时间，可以实现个体化和精确的治疗规划。已经证实，患者的当前状态（由磁共振扫描表示）和治疗选择是生存时间的原因。尽管先前的有关基于磁共振的脑胶质母细胞瘤生存时间的研究仅关注磁共振扫描与生存时间之间的直接映射，但未包括治疗与生存时间之间的潜在因果关系。为了解决这个限制，我们提出了一种基于治疗条件的脑胶质母细胞瘤生存时间回归模型，该模型除了考虑磁共振扫描还包括治疗信息。我们的方法使我们能够以统一的方式有效利用所有治疗方案的数据，而不必为每个治疗方案单独训练模型。

    In this work, we aim to predict the survival time (ST) of glioblastoma (GBM) patients undergoing different treatments based on preoperative magnetic resonance (MR) scans. The personalized and precise treatment planning can be achieved by comparing the ST of different treatments. It is well established that both the current status of the patient (as represented by the MR scans) and the choice of treatment are the cause of ST. While previous related MR-based glioblastoma ST studies have focused only on the direct mapping of MR scans to ST, they have not included the underlying causal relationship between treatments and ST. To address this limitation, we propose a treatment-conditioned regression model for glioblastoma ST that incorporates treatment information in addition to MR scans. Our approach allows us to effectively utilize the data from all of the treatments in a unified manner, rather than having to train separate models for each of the treatments. Furthermore, treatment can be e
    
[^130]: 事件关键摘要

    Event-Keyed Summarization

    [https://arxiv.org/abs/2402.06973](https://arxiv.org/abs/2402.06973)

    事件关键摘要（EKS）是一种新颖的任务，旨在为特定事件生成上下文化的摘要。我们提出了一个基准数据集MUCSUM，并展示了EKS与传统摘要和结构到文本的比较结果。

    

    我们介绍了一种新颖的任务，称为事件关键摘要（EKS），它将传统的摘要和文档级事件提取结合起来，目标是在给定文档和提取的事件结构的情况下生成一个上下文化的特定事件摘要。我们介绍了一个用于这个任务的数据集MUCSUM，包括经典MUC-4数据集中所有事件的摘要，以及一组基线模型，其中包括在摘要文献中预训练的语言模型标准以及更大的前沿模型。我们表明，将EKS简化为传统的摘要或结构到文本的去除都会得到较差的目标事件摘要，并且MUCSUM是这一任务的一个稳健的基准。最后，我们对参考摘要和模型摘要进行了人工评估，并对结果进行了详细分析。

    We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.
    
[^131]: 一次指导，多轮稳定对话：对话的高效调整框架

    Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue

    [https://arxiv.org/abs/2402.06967](https://arxiv.org/abs/2402.06967)

    本论文提出了一种名为Midi-Tuning的多轮交互对话调整框架，通过分别对代理人和用户建模，并利用轮次级内存缓存机制进行调整，实现了对话代理的一致性和稳定性。

    

    调整预训练语言模型以实现对话生成已成为构建能力强大的对话代理的主流范式。然而，传统的调整方式狭隘地将对话生成视为类似其他语言生成任务的过程，忽视了对话者之间的角色差异和对话应具备的多轮交互过程。这种方式导致了所构建代理人的对话一致性不尽如人意。在本研究中，我们强调对话的交互性和沟通性，并认为分别对代理人和用户的讲话者角色进行建模更为可行，使得代理人能够保持一致的角色。我们提出了一种有效的多轮交互对话调整（Midi-Tuning）框架。该框架使用基于大型语言模型的两个适配器分别对代理人和用户进行建模，它们按轮次交替使用话语，并通过轮次级内存缓存机制进行调整。大量实验证明，我们的方法可以有效提高对话代理的一致性和稳定性。

    Tuning pretrained language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models, where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our
    
[^132]: 基于树集成的情境多臂老虎机

    Tree Ensembles for Contextual Bandits

    [https://arxiv.org/abs/2402.06963](https://arxiv.org/abs/2402.06963)

    本论文提出了一种基于树集成的情境多臂老虎机新框架，通过整合两种广泛使用的老虎机方法，在标准和组合设置中实现了优于基于神经网络的方法的性能，在减少后悔和计算时间方面表现出更出色的性能。

    

    我们提出了一个基于树集成的情境多臂老虎机的新框架。我们的框架将两种广泛使用的老虎机方法，上信心界和汤普森抽样，整合到标准和组合设置中。通过使用流行的树集成方法XGBoost进行多次实验研究，我们展示了我们框架的有效性。当应用于基准数据集和道路网络导航的真实世界应用时，与基于神经网络的最先进方法相比，我们的方法在减少后悔和计算时间方面表现出更好的性能。

    We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
    
[^133]: 从第一原理构建的架构神经后门

    Architectural Neural Backdoors from First Principles

    [https://arxiv.org/abs/2402.06957](https://arxiv.org/abs/2402.06957)

    本论文提出了从第一原理构建架构神经后门的方法，并描述了12种不同类型的架构后门。同时，通过构建一个任意触发检测器，展示了无需人工监督即可为架构引入后门的能力。

    

    尽管之前的研究通过改变神经网络的参数来创建后门，但最近的研究揭示了一种更隐蔽的威胁：在网络架构定义中嵌入的后门。这涉及到注入常见的架构组件，如激活函数和池化层，以巧妙地引入一个持续存在的后门行为，即使在重新训练后也是如此。然而，架构后门的全部范围和影响仍然很少被探索。Bober-Irizar等人[2023]首次引入了架构后门；他们展示了如何为棋盘图案创建后门，但从未解释如何针对任意触发模式进行定位。在这项工作中，我们构建了一个可用于无人监督地为架构引入后门的任意触发检测器。这使我们重新审视了架构后门的概念并将其进行分类，描述了12种不同类型。为了评估检测此类后门的困难程度，...

    While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, 
    
[^134]: 使用特征映射的物理引导神经网络中的训练动态

    Training dynamics in Physics-Informed Neural Networks with feature mapping

    [https://arxiv.org/abs/2402.06955](https://arxiv.org/abs/2402.06955)

    本研究探究了使用特征映射层的物理引导神经网络（PINNs）的训练动态，通过极限共轭核和神经切向核揭示了模型的收敛和泛化。我们提出了一种替代基于傅里叶变换的特征映射的条件正定径向基函数，证明了该方法在各种问题集中的有效性，并可以轻松实现在坐标输入网络中。这为广泛的PINNs研究带来了益处。

    

    物理引导神经网络（PINNs）已成为解决偏微分方程（PDE）的标志性机器学习方法。尽管其变体取得了显著进展，但来自更广泛的隐式神经表示研究的特征映射的经验性成功在很大程度上被忽视。我们通过极限共轭核和神经切向核来研究带有特征映射层的PINNs的训练动态，从而揭示了模型的收敛和泛化。我们还展示了常用的基于傅里叶变换的特征映射在某些情况下的不足，并提出条件正定的径向基函数作为更好的替代方法。经验证实，我们的方法在各种正向和反向问题集中非常有效。这种简单的技术可以轻松在坐标输入网络中实现，并受益于广泛的PINNs研究。

    Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
    
[^135]: 用噪声中等规模量子设备估计串扰误差对电路保真度的影响

    Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices

    [https://arxiv.org/abs/2402.06952](https://arxiv.org/abs/2402.06952)

    本研究对噪声中等规模量子计算机上的串扰误差进行了全面分析，发现并行指令之间的串扰可能会破坏量子状态并导致程序执行错误

    

    当前技术的进步使量子计算社区关注于探索近期设备的潜力，这些设备在实际应用中的计算能力超过了经典计算机。一个未解决的核心问题是这些设备中固有噪声是否可以被克服，或者任何潜在的量子优势是否受到限制。毫无疑问，串扰是噪声中等规模量子系统中的主要来源之一，它对硬件设计构成了根本性挑战。并行指令之间的串扰可以破坏量子状态并导致程序执行错误。在这项研究中，我们对串扰误差对噪声中等规模量子计算机的影响进行了全面分析。我们的方法非常简单和实用，可用于表征多比特设备的串扰误差。特别地，我们结合了随机化基准测试和同时随机化的方法

    Current advancements in technology have focused the attention of the quantum computing community toward exploring the potential of near-term devices whose computing power surpasses that of classical computers in practical applications. An unresolved central question revolves around whether the inherent noise in these devices can be overcome or whether any potential quantum advantage would be limited. There is no doubt that crosstalk is one of the main sources of noise in noisy intermediate-scale quantum (NISQ) systems, and it poses a fundamental challenge to hardware designs. Crosstalk between parallel instructions can corrupt quantum states and cause incorrect program execution. In this study, we present a comprehensive analysis of the crosstalk error effect on NISQ computers. Our approach is extremely straightforward and practical for characterizing the crosstalk error of various multi-qubit devices. In particular, we combine the randomized benchmarking (RB) and simultaneous randomiz
    
[^136]: 评价自动印刷海报生成的度量标准

    Evaluation Metrics for Automated Typographic Poster Generation

    [https://arxiv.org/abs/2402.06945](https://arxiv.org/abs/2402.06945)

    本文提出一组启发式度量标准用于评估自动生成的印刷海报，包括可读性、美观度和语义特征。通过约束进化方法生成海报，并将度量标准作为约束条件。此外，还集成情感识别技术，并分析了方法和视觉特征的性能。

    

    计算设计方法促进了印刷设计的生成，但评估这些设计仍然是一个具有挑战性的任务。本文提出了一组启发式度量标准，用于评估印刷设计，重点评估其可读性，评估设计的视觉质量和估计设计有效传达内容语义的语义特征。我们使用约束进化方法生成印刷海报，并将提出的评价度量标准与不同设置结合使用，将可读性度量标准视为约束条件。我们还集成情感识别技术，以自动识别文本语义，并分析该方法和视觉特征的性能输出。

    Computational Design approaches facilitate the generation of typographic design, but evaluating these designs remains a challenging task. In this paper, we propose a set of heuristic metrics for typographic design evaluation, focusing on their legibility, which assesses the text visibility, aesthetics, which evaluates the visual quality of the design, and semantic features, which estimate how effectively the design conveys the content semantics. We experiment with a constrained evolutionary approach for generating typographic posters, incorporating the proposed evaluation metrics with varied setups, and treating the legibility metrics as constraints. We also integrate emotion recognition to identify text semantics automatically and analyse the performance of the approach and the visual characteristics outputs.
    
[^137]: 使用谈判能力的分布式基础设施高效资源调度

    Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities

    [https://arxiv.org/abs/2402.06938](https://arxiv.org/abs/2402.06938)

    这项研究介绍了一种使用谈判能力的分布式基础设施高效资源调度的方法，通过引入一种基于模糊逻辑的代理自动谈判系统，优化云计算提供商和客户之间的协议，以提高效率。

    

    在过去的几十年里，信息和互联网技术的快速发展催生了大量的数据和信息。信息爆炸推动许多企业或个人寻求租用云计算基础设施来将他们的应用程序放置在云中。然而，云计算提供商和客户之间达成的协议通常不高效。许多因素影响效率，如提供商云计算基础设施的闲置和对客户的额外成本。一个可能的解决方案是引入一种综合的、谈判类的博弈，并根据谈判结果安排资源。我们提出了一种基于模糊逻辑的基于代理的自动谈判系统用于资源调度。所提出的方法可以完成一对一的自动谈判过程，并为提供商和客户生成最优的报价。我们比较了不同成员函数、模糊规则集和网络拓扑结构对资源调度的影响。

    In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
    
[^138]: ORIENT:一种面向6G中延迟敏感应用程序的优先权感知节能方法

    ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G

    [https://arxiv.org/abs/2402.06931](https://arxiv.org/abs/2402.06931)

    ORIENT是一种面向6G的优先权感知节能方法，旨在通过解决服务实例放置和分配、路径选择和请求优先级的联合问题来最大化系统的整体利润，并在长时间内最小化能耗。

    

    随着对6G到来的期望增加，人们对计算和网络的能耗增长表示担忧。预计连接设备的激增和资源要求高的应用程序将为能源资源带来前所未有的挑战。虽然过去已经讨论了可持续的资源分配策略，但这些努力主要集中在单域编排上，或者忽略了6G提出的独特要求。为了解决这个问题，我们研究了服务实例的放置和分配、路径选择和请求优先级的联合问题，称为PIRA。目标函数是通过最大化同时支持的请求数量来最大化系统的整体利润，同时在长时间内最小化能耗。此外，还考虑了计算和网络资源的端到端延迟要求和资源容量约束，其中使用了排队论来估计

    Anticipation for 6G's arrival comes with growing concerns about increased energy consumption in computing and networking. The expected surge in connected devices and resource-demanding applications presents unprecedented challenges for energy resources. While sustainable resource allocation strategies have been discussed in the past, these efforts have primarily focused on single-domain orchestration or ignored the unique requirements posed by 6G. To address this gap, we investigate the joint problem of service instance placement and assignment, path selection, and request prioritization, dubbed PIRA. The objective function is to maximize the system's overall profit as a function of the number of concurrently supported requests while simultaneously minimizing energy consumption over an extended period of time. In addition, end-to-end latency requirements and resource capacity constraints are considered for computing and networking resources, where queuing theory is utilized to estimate
    
[^139]: 使用Langchain制作首尔历史遗址聊天机器人的原型

    Making a prototype of Seoul historical sites chatbot using Langchain

    [https://arxiv.org/abs/2402.06929](https://arxiv.org/abs/2402.06929)

    本文介绍了使用Langchain制作首尔历史遗址聊天机器人的原型，旨在提高游客对该地区宝贵文化遗产的认识，并提供准确和可靠的信息。该代理设计用于英语访问，并利用首尔市政府提供的数据。方法和结构概述也提供在论文中，同时也讨论了改进的潜力。

    

    在本文中，我们将分享一个关于开发一个对位于首尔的历史遗址进行信息传播的对话代理的草案。代理的主要目标是增加对首尔不熟悉的游客对宝贵文化遗产的存在和精确位置的认识。它旨在促进对韩国丰富多样的文化历史的基本了解。该代理经过精心设计，可以用英语进行访问，并利用首尔市政府慷慨提供的数据。尽管数据量有限，但它始终可靠地提供准确的回答，并与可用信息无缝对齐。我们详细介绍了创建该代理所使用的方法，并在论文中提供了其基本结构的综合概述。此外，我们还探讨了改进这个系统初始版本的潜力。

    In this paper, we are going to share a draft of the development of a conversational agent created to disseminate information about historical sites located in the Seoul. The primary objective of the agent is to increase awareness among visitors who are not familiar with Seoul, about the presence and precise locations of valuable cultural heritage sites. It aims to promote a basic understanding of Korea's rich and diverse cultural history. The agent is thoughtfully designed for accessibility in English and utilizes data generously provided by the Seoul Metropolitan Government. Despite the limited data volume, it consistently delivers reliable and accurate responses, seamlessly aligning with the available information. We have meticulously detailed the methodologies employed in creating this agent and provided a comprehensive overview of its underlying structure within the paper. Additionally, we delve into potential improvements to enhance this initial version of the system, with a prima
    
[^140]: 用直接的两两比较方法生成思维链，以搜索最有潜力的中间思维

    Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought

    [https://arxiv.org/abs/2402.06918](https://arxiv.org/abs/2402.06918)

    本文提出了一种基于直接两两比较的方法，通过利用LLMs的噪声反馈，直接识别出最有潜力的中间思维，从而生成优秀的思维链。

    

    为了提高大型语言模型(LLMs)处理复杂推理问题的能力，提出了思维链(Chain-of-Thoughts, CoT)方法，用于指导LLMs进行逐步推理，从简单到复杂的问题解决。目前最先进的生成这种思维链的方法涉及互动协作，学习者生成候选中间思维，由LLMs评估，引导生成后续思维。然而，一个广泛但未被充分研究的问题是，LLMs的评估通常存在噪声和不可靠性，可能误导生成过程，选择不够有潜力的中间思维。本文受Vapnik原则的启发，提出了一种新的基于比较的CoT生成算法，直接根据LLMs的噪声反馈确定最有潜力的思维。在每一轮中，我们随机配对中间思维，并直接促使LLMs从每对中选择更有潜力的思维。

    To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
    
[^141]: 用线性策略网络解决深度强化学习基准问题

    Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks

    [https://arxiv.org/abs/2402.06912](https://arxiv.org/abs/2402.06912)

    本研究通过使用进化策略(ES)来优化神经网络的权重，以通过直接策略搜索解决深度强化学习(DRL)基准问题。研究结果显示，ES可以在许多基准任务中找到有效的线性策略，与当前使用更大网络的DRL方法相比，这表明当前的基准问题比以往认为的更容易解决。

    

    尽管深度强化学习(DRL)算法能够学习有效的策略来解决像Atari游戏和机器人任务这样的挑战性问题，但算法复杂，训练时间往往较长。本研究探讨了进化策略(ES)与基于梯度的深度强化学习方法之间的表现差异。我们使用ES通过神经进化优化神经网络的权重，通过直接策略搜索来完成。我们对常规网络和由一个从观测到动作的单一线性层组成的策略网络进行基准测试；对于三种经典的ES方法和三种基于梯度的方法，如PPO。我们的结果表明，ES可以在许多RL基准任务中找到有效的线性策略，而DRL方法只能使用更大的网络找到成功的策略，这表明当前的基准问题比以前认为的更容易解决。有趣的是，即使对于更复杂的任务，ES的结果也与基于梯度的方法相当。

    Although Deep Reinforcement Learning (DRL) methods can learn effective policies for challenging problems such as Atari games and robotics tasks, algorithms are complex and training times are often long. This study investigates how evolution strategies (ES) perform compared to gradient-based deep reinforcement learning methods. We use ES to optimize the weights of a neural network via neuroevolution, performing direct policy search. We benchmark both regular networks and policy networks consisting of a single linear layer from observations to actions; for three classical ES methods and for three gradient-based methods such as PPO. Our results reveal that ES can find effective linear policies for many RL benchmark tasks, in contrast to DRL methods that can only find successful policies using much larger networks, suggesting that current benchmarks are easier to solve than previously assumed. Interestingly, also for higher complexity tasks, ES achieves results comparable to gradient-based
    
[^142]: 高拓扑神经网络：通过高阶相互作用缓解图神经网络的瓶颈问题

    Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions

    [https://arxiv.org/abs/2402.06908](https://arxiv.org/abs/2402.06908)

    本文研究了图神经网络的瓶颈问题，并提出了一种名为高拓扑神经网络的方法，通过引入高阶相互作用和多关系归纳偏置来缓解这些问题。

    

    自然现象的不可约复杂性使得图神经网络成为在图结构数据上进行表示学习任务的标准模型。虽然它们能够捕捉局部和全局模式的能力令人印象深刻，但与长距离和高阶依赖相关的影响对这些模型提出了相当大的挑战。本文从理论框架入手，揭示了网络的宽度、深度和图拓扑对消息传递神经网络中过度压缩现象的影响。然后，本文通过高拓扑神经网络从高阶相互作用和多关系归纳偏置入手。这种模型通过高维结构传播消息，为信息流提供了快捷方式或额外的路径。通过这种构建，底层的计算图不再与输入图结构耦合，从而缓解了前面提到的瓶颈问题，同时还考虑了h。

    The irreducible complexity of natural phenomena has led Graph Neural Networks to be employed as a standard model to perform representation learning tasks on graph-structured data. While their capacity to capture local and global patterns is remarkable, the implications associated with long-range and higher-order dependencies pose considerable challenges to such models. This work starts with a theoretical framework to reveal the impact of network's width, depth, and graph topology on the over-squashing phenomena in message-passing neural networks. Then, the work drifts towards, higher-order interactions and multi-relational inductive biases via Topological Neural Networks. Such models propagate messages through higher-dimensional structures, providing shortcuts or additional routes for information flow. With this construction, the underlying computational graph is no longer coupled with the input graph structure, thus mitigating the aforementioned bottlenecks while accounting also for h
    
[^143]: LLM能够识别毒性吗？结构化毒性调查框架和基于语义的度量

    Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric

    [https://arxiv.org/abs/2402.06900](https://arxiv.org/abs/2402.06900)

    本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。

    

    在开发遵守社会标准的大型语言模型（LLMs）的过程中，识别生成文本中的毒性存在至关重要。现有的大多数毒性度量依赖于在特定毒性数据集上训练的编码模型。然而，这些编码器容易受到分布外的问题的影响，并且依赖于数据集中所假定的毒性定义。本文介绍了一种基于LLMs的自动鲁棒度量，用于区分模型回应是否具有毒性。我们首先分析了毒性因素，然后研究了LLMs的内在毒性属性，以确定它们作为评估器的适用性。随后，我们对评估数据集上的度量指标LLMs As ToxiciTy Evaluators（LATTE）进行了评估。实证结果表明，在不进行训练过程的情况下，我们的度量在测量毒性方面表现出色，F1得分比现有技术指标提高了12个百分点。我们还展示了上游毒性对度量结果的影响。

    In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
    
[^144]: GenTranslate: 大型语言模型是生成的多语言语音和机器翻译工具

    GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators

    [https://arxiv.org/abs/2402.06894](https://arxiv.org/abs/2402.06894)

    GenTranslate是一个新的翻译任务生成模型，通过利用大型语言模型的丰富语言知识和强大推理能力，可以从N-best列表中生成更高质量的翻译结果。

    

    大型语言模型（LLMs）的最新进展通过减少表示误差和引入外部知识，推动了多语言语音和机器翻译的发展。然而，翻译任务通常使用束搜索解码和前k个假设选择进行推理。这些技术往往不能充分利用多样化的N-best假设中的丰富信息，使得它们在需要单个高质量输出序列的翻译任务中效果不佳。在本文中，我们提出了一个新的翻译任务生成模型，即“GenTranslate”，它基于LLMs来从N-best列表中生成更好的结果。利用LLMs丰富的语言知识和强大的推理能力，我们的新模型可以将N-best候选人中的丰富信息整合起来，生成更高质量的翻译结果。此外，为了支持LLM的微调，我们构建并发布了一个HypoTransla模型。

    Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
    
[^145]: 非自回归的生成模型用于排序推荐

    Non-autoregressive Generative Models for Reranking Recommendation

    [https://arxiv.org/abs/2402.06871](https://arxiv.org/abs/2402.06871)

    本研究提出了一个非自回归的生成模型用于排序推荐，在多阶段推荐系统中扮演关键角色。该模型旨在提高效率和效果，并解决稀疏训练样本和动态候选项对模型收敛性的挑战。

    

    在多阶段推荐系统中，重新排序通过建模项目之间的内部相关性起到了至关重要的作用。重新排序的关键挑战在于在排列的组合空间中探索最佳序列。最近的研究提出了生成器-评估器学习范式，生成器生成多个可行序列，评估器基于估计的列表得分选择最佳序列。生成器至关重要，而生成模型非常适合生成器函数。当前的生成模型采用自回归策略进行序列生成。然而，在实时工业系统中部署自回归模型是具有挑战性的。因此，我们提出了一个非自回归生成模型用于排序推荐（NAR4Rec），以提高效率和效果。为了解决与稀疏训练样本和动态候选项对模型收敛性的挑战，我们引入了一个m

    In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items.The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a m
    
[^146]: 具有辨别性对抗学习的论文

    Discriminative Adversarial Unlearning

    [https://arxiv.org/abs/2402.06864](https://arxiv.org/abs/2402.06864)

    该论文提出了一种基于最小最大优化范式的机器反学习框架，利用强大的成员推断攻击来实现反学习，同时保持其总体性能，并增强了反学习能力。

    

    我们引入了一种新颖的机器反学习框架，基于最小最大优化范式的已建立原则。我们利用强大的成员推断攻击（MIA）的能力，以促进从训练模型中反学习特定样本。我们考虑了两个网络的场景，攻击者$\mathbf{A}$和经过训练的防御者 $\mathbf{D}$在对抗目标下相互对抗，其中攻击者旨在揭示数据的信息以推断成员身份，而防御者在反击中进行反学习，同时保持其总体性能。算法可以使用反向传播进行端到端训练，遵循已知的迭代最小最大方法来更新攻击者和防御者。我们还加入了自监督目标，有效地解决了遗忘集和验证集之间的特征空间差异，增强了反学习能力

    We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
    
[^147]: UrbanKGent：用于城市知识图谱构建的统一大型语言模型代理框架

    UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction

    [https://arxiv.org/abs/2402.06861](https://arxiv.org/abs/2402.06861)

    UrbanKGent是一个用于城市知识图谱构建的统一大型语言模型代理框架，通过异构感知和地理空间注入构建知识化指令集，并通过迭代轨迹细化模块来提升轨迹的质量。在两个真实世界的数据集上进行的评估表明了UrbanKGent的有效性和性能。

    

    城市知识图谱最近作为一种新兴的构建模块，在多源城市数据中提取关键知识，用于各种城市应用场景。尽管具有潜在的好处，但城市知识图谱构建仍然严重依赖手动工作，阻碍了其潜在的进展。本文提出了UrbanKGent，一种统一的大型语言模型代理框架，用于城市知识图谱构建。具体而言，我们首先通过异构感知和地理空间注入来构建UrbanKGC任务（如关系三元组提取和知识图谱补全）的知识化指令集。此外，我们提出了一种工具增强的迭代轨迹细化模块，以增强和优化从GPT-4中提取的轨迹。通过在Llama-2-13B上进行增强轨迹的混合指令微调，我们获得了UrbanKGC代理UrbanKGent-13B。我们对两个真实世界的数据集进行了全面评估。

    Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world da
    
[^148]: LiRank: 领英的工业规模排名模型

    LiRank: Industrial Large Scale Ranking Models at LinkedIn

    [https://arxiv.org/abs/2402.06859](https://arxiv.org/abs/2402.06859)

    LiRank是领英的一个大规模排名框架，它应用了最先进的建模架构和优化方法，并提出了新的建模改进和技术，通过A/B测试取得了有效的结果。

    

    我们介绍了LiRank，这是领英的一个大规模排名框架，它将最先进的建模架构和优化方法应用于生产。我们揭示了几个建模改进，包括Residual DCN，它在著名的DCNv2架构中添加了注意力和残差连接。我们分享了将SOTA架构组合和调优以创建统一模型的见解，包括Dense Gating、Transformers和Residual DCN。我们还提出了用于校准的新技术，并描述了如何将基于深度学习的探索/利用方法应用于生产环境。为了实现大规模排名模型的有效、生产级服务，我们详细介绍了使用量化和词汇压缩训练和压缩模型的方法。我们提供了Feed排名、职位推荐和广告点击率（CTR）预测等大规模使用案例的部署设置细节。通过阐明最有效的技术方法，我们总结了各种A/B测试的经验教训。

    We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. T
    
[^149]: ChemLLM: 一个化学大型语言模型

    ChemLLM: A Chemical Large Language Model

    [https://arxiv.org/abs/2402.06852](https://arxiv.org/abs/2402.06852)

    ChemLLM是第一个专门用于化学领域的大型语言模型，利用新颖的指令构建方法将结构化知识转化为对话形式，具有平滑对话交互的能力，并在化学的三个主要任务中击败了GPT-3.5。

    

    大型语言模型（LLM）在化学应用中取得了令人瞩目的进展，包括分子属性预测、分子生成、实验协议设计等。然而，该领域缺乏一个专门针对化学领域设计的基于对话的模型。这个挑战来自于事实，大多数化学数据和科学知识主要存储在结构化数据库中，直接使用这些结构化数据会影响模型维持连贯对话的能力。为了解决这个问题，我们开发了一种新颖的基于模板的指令构建方法，将结构化知识转化为简洁对话形式，适合于语言模型的训练。通过利用这种方法，我们开发了ChemLLM，第一个专门用于化学的大型语言模型，能够在化学领域的各种任务中进行平滑对话交互。ChemLLM在化学的三个主要任务，即名称转换、分子生成和实验协议设计方面，击败了GPT-3.5。

    Large language models (LLMs) have made impressive progress in chemistry applications, including molecular property prediction, molecular generation, experimental protocol design, etc. However, the community lacks a dialogue-based model specifically designed for chemistry. The challenge arises from the fact that most chemical data and scientific knowledge are primarily stored in structured databases, and the direct use of these structured data compromises the model's ability to maintain coherent dialogue. To tackle this issue, we develop a novel template-based instruction construction method that transforms structured knowledge into plain dialogue, making it suitable for language model training. By leveraging this approach, we develop ChemLLM, the first large language model dedicated to chemistry, capable of performing various tasks across chemical disciplines with smooth dialogue interaction. ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name conversion, molecu
    
[^150]: 约束力和标签：数据注释的WEIRD家谱和社会理论

    Discipline and Label: A WEIRD Genealogy and Social Theory of Data Annotation

    [https://arxiv.org/abs/2402.06811](https://arxiv.org/abs/2402.06811)

    本文探讨了数据注释的批判家谱，强调了评级者多样性对公平性和模型性能的重要性，并研究了数据注释工作者的工作条件、注释者主观性对标签的影响以及注释工作中的心理伤害。

    

    数据注释仍然是机器学习和人工智能的必需品。最近关于数据注释的实证研究已经开始强调评级者的多样性对公平性和模型性能的重要性，并且新的研究线路已经开始研究数据注释工作者的工作条件，注释者主观性对标签的影响和作用，以及注释工作中潜在的心理伤害。本文概述了数据注释的批判家谱，从其心理和知觉的方面开始。我们借鉴了对上世纪70年代计算机化实验室心理学实验崛起的批评，质疑这些实验是否允许将结果推广到超出实验室设置的情境中。数据注释能否超越它们获得的设置或地点来推广结果？心理学过于依赖来自西方的参与者。

    Data annotation remains the sine qua non of machine learning and AI. Recent empirical work on data annotation has begun to highlight the importance of rater diversity for fairness, model performance, and new lines of research have begun to examine the working conditions for data annotation workers, the impacts and role of annotator subjectivity on labels, and the potential psychological harms from aspects of annotation work. This paper outlines a critical genealogy of data annotation; starting with its psychological and perceptual aspects. We draw on similarities with critiques of the rise of computerized lab-based psychological experiments in the 1970's which question whether these experiments permit the generalization of results beyond the laboratory settings within which these results are typically obtained. Do data annotations permit the generalization of results beyond the settings, or locations, in which they were obtained? Psychology is overly reliant on participants from Wester
    
[^151]: 使用总信息流评估共同创造力

    Evaluating Co-Creativity using Total Information Flow

    [https://arxiv.org/abs/2402.06810](https://arxiv.org/abs/2402.06810)

    本研究旨在通过使用总信息流来定量评估音乐中的共同创造力过程，并通过定性研究证明该方法与人类感知相匹配。

    

    音乐中的共同创造力指的是两个或更多的音乐家或音乐代理通过创作或即兴创作音乐相互互动。然而，这是一个非常主观的过程，每个音乐家对于在某种情境下哪种即兴创作更好有自己的偏好。在本文中，我们旨在创建一个基于总信息流的度量来定量评估音乐中的共同创造力过程。换句话说，我们的度量是创造性音乐过程有多"好"的指标。我们的主要假设是，好的音乐创作将最大化参与者之间的信息流，该信息流由记录在单独轨道中的音乐声音捕捉。我们提出了一种使用预训练生成模型作为熵估计器计算信息流的方法。我们通过定性研究展示了我们的方法如何与人类感知相匹配。

    Co-creativity in music refers to two or more musicians or musical agents interacting with one another by composing or improvising music. However, this is a very subjective process and each musician has their own preference as to which improvisation is better for some context. In this paper, we aim to create a measure based on total information flow to quantitatively evaluate the co-creativity process in music. In other words, our measure is an indication of how "good" a creative musical process is. Our main hypothesis is that a good musical creation would maximize information flow between the participants captured by music voices recorded in separate tracks. We propose a method to compute the information flow using pre-trained generative models as entropy estimators. We demonstrate how our method matches with human perception using a qualitative study.
    
[^152]: 是否安全过马路？GPT-4V用于安全意识的可解释风险评估

    Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing

    [https://arxiv.org/abs/2402.06794](https://arxiv.org/abs/2402.06794)

    本文介绍了使用GPT-4V进行可解释风险评估的方法，该方法通过解释复杂的过马路场景，为盲人和视力低下人士的安全决策提供支持。

    

    对于盲人和视力低下的人来说，安全地通过街道交叉口是一个复杂的挑战，因为它需要对周围环境有细致的理解，而这个任务很大程度上依赖于视觉线索。传统的辅助决策方法往往不够完善，无法提供全面的场景分析和安全级别判断。本文介绍了一种创新的方法，利用大型多模型来解释复杂的过马路场景，相比传统的交通信号识别技术，提供了潜在的进步。我们的方法通过生成安全评分和自然语言场景描述，支持盲人和视力低下人士安全决策。我们收集了由四足机器人捕获的多视角自我中心图像构成的过马路交叉口数据，并根据预先定义的安全评分分类进行了图像标注。

    Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual k
    
[^153]: 有限数据集上基于生成模型的迁移学习用于目标检测

    Transfer learning with generative models for object detection on limited datasets

    [https://arxiv.org/abs/2402.06784](https://arxiv.org/abs/2402.06784)

    本论文提出了一个适用于通用情景的基于生成模型的迁移学习框架，用于解决有限数据集上的目标检测任务。

    

    在某些领域中，数据的可用性是有限的，尤其是对于目标检测任务，需要正确标记每个目标周围的边界框。一个显著的例子是在海洋生物学领域，需要开发自动检测海洋物种用于环境监测的方法。为了解决数据限制问题，目前最先进的机器学习策略采用了两种主要方法。第一种方法是在现有数据集上预训练模型，然后推广到具体的领域。第二种策略是使用copy-paste技术或ad-hoc模拟器等方法创建特定于目标领域的合成数据集。第一种方法往往面临重大的领域转移问题，而第二种方法需要针对特定任务设计定制解决方案。为了应对这些挑战，我们提出了一个在通用情景下有效的迁移学习框架。

    The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In
    
[^154]: 与更有说服力的LLMs辩论会导致更真实的回答

    Debating with More Persuasive LLMs Leads to More Truthful Answers

    [https://arxiv.org/abs/2402.06782](https://arxiv.org/abs/2402.06782)

    本文研究了更弱的语言模型是否能评估更强的模型的正确性。研究发现，通过进行辩论，非专家模型和人类回答问题的准确性都有所提高。

    

    与所需行为一致的大型语言模型（LLM）的常见方法主要依赖于人工标注的数据。然而，随着模型变得越来越复杂，它们将超过人类专业知识，人类评估的角色将演变为非专家监督专家。在此之前，我们问：更弱的模型能评估更强的模型的正确性吗？我们在类似的环境中调查了这个问题，其中更强的模型（专家）拥有回答问题所需的信息，而更弱的模型（非专家）缺乏这些信息。我们评估的方法是\textit{辩论}，其中两个LLM专家分别支持不同的答案，一个非专家选择答案。我们发现辩论 consistently帮助非专家模型和人类回答问题，分别达到76%和88%的准确性（朴素基准分别为48%和60%）。此外，以无监督方式优化专家辩论者的说服力会提高非专家的能力。

    Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
    
[^155]: 通过(超)图搜索进行逆合成预测

    Retrosynthesis Prediction via Search in (Hyper) Graph

    [https://arxiv.org/abs/2402.06772](https://arxiv.org/abs/2402.06772)

    本研究提出了一种基于半模板的逆向合成方法，通过在产物分子图和离开基团超图中进行搜索，以处理复杂的反应。

    

    在有机合成中，从指定的核心产物预测反应物是一个基本的挑战，被称为逆向合成预测。最近，基于半模板和基于图编辑的方法在解释性和准确性方面取得了良好的表现。然而，由于它们的机制，这些方法无法预测复杂的反应，例如具有多个反应中心或将相同离开基团连接到多个原子的反应。在本研究中，我们提出了一种基于半模板的方法，即逆向合成通过(超)图搜索(RetroSiG)框架，以减轻这些限制。在所提出的方法中，我们将反应中心的识别和离开基团的完成任务转化为在产物分子图中搜索和在离开基团超图中搜索的任务。作为一种基于半模板的方法，RetroSiG具有几个优点。首先，RetroSiG能够处理提到的复杂反应。

    Predicting reactants from a specified core product stands as a fundamental challenge within organic synthesis, termed retrosynthesis prediction. Recently, semi-template-based methods and graph-edits-based methods have achieved good performance in terms of both interpretability and accuracy. However, due to their mechanisms these methods cannot predict complex reactions, e.g., reactions with multiple reaction center or attaching the same leaving group to more than one atom. In this study we propose a semi-template-based method, the \textbf{Retro}synthesis via \textbf{S}earch \textbf{i}n (Hyper) \textbf{G}raph (RetroSiG) framework to alleviate these limitations. In the proposed method, we turn the reaction center identification and the leaving group completion tasks as tasks of searching in the product molecular graph and leaving group hypergraph respectively. As a semi-template-based method RetroSiG has several advantages. First, RetroSiG is able to handle the complex reactions mentione
    
[^156]: 文本数据增强在自然语言处理中的评估指标

    Evaluation Metrics for Text Data Augmentation in NLP

    [https://arxiv.org/abs/2402.06766](https://arxiv.org/abs/2402.06766)

    这项研究提供了文本增强方法的评估指标分类法，为统一基准提供方向和帮助。在不同任务、度量标准和实验设置下，该分类法有助于比较不同的增强方法。

    

    最近关于自然语言处理的数据增强的调研报告指出了该领域的不同技术和进展。几个框架、工具和存储库推广了文本数据增强流水线的实施。然而，由于不同的任务、度量标准、数据集、体系结构和实验设置的缺乏评估标准和方法比较标准使得比较变得毫无意义。此外，缺乏方法的统一性，文本数据增强研究将受益于比较不同的增强方法的统一指标。因此，学术界和工业界都在努力寻找相关的文本数据增强技术的评估指标。本研究的贡献在于提供了文本增强方法的评估指标分类法，并作为统一基准的方向。所提出的分类法包括了实施工具和指标计算的类别。最后，通过这项研究，我们的目的是为了推进文本数据增强技术的评估指标的发展。

    Recent surveys on data augmentation for natural language processing have reported different techniques and advancements in the field. Several frameworks, tools, and repositories promote the implementation of text data augmentation pipelines. However, a lack of evaluation criteria and standards for method comparison due to different tasks, metrics, datasets, architectures, and experimental settings makes comparisons meaningless. Also, a lack of methods unification exists and text data augmentation research would benefit from unified metrics to compare different augmentation methods. Thus, academics and the industry endeavor relevant evaluation metrics for text data augmentation techniques. The contribution of this work is to provide a taxonomy of evaluation metrics for text augmentation methods and serve as a direction for a unified benchmark. The proposed taxonomy organizes categories that include tools for implementation and metrics calculation. Finally, with this study, we intend to 
    
[^157]: 通过邻域划分和生成子图编码对领域知识图对齐进行大型语言模型微调的GLaM研究

    GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding

    [https://arxiv.org/abs/2402.06764](https://arxiv.org/abs/2402.06764)

    该论文提出了GLaM方法，通过邻域划分和生成子图编码，对领域知识图进行大型语言模型的微调。该方法的创新之处在于能够实现对实际应用中的多步推理，并减少虚构。

    

    将大型语言模型（LLMs）与从特定领域数据派生的知识图集成，代表了朝着更强大和事实推理的重要进展。随着这些模型变得越来越强大，使它们能够在现实世界的知识图上进行多步推理，并尽量减少虚构是至关重要的。然而，大型语言模型在处理互连实体的领域专用图时，能力仍然有限。因此，有必要填补这一技术上的重要差距。

    Integrating large language models (LLMs) with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a LLM to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no--such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logica
    
[^158]: 一种问卷分析方法：通过对投资者竞赛数据进行聚类分析获得的见解

    A Methodology for Questionnaire Analysis: Insights through Cluster Analysis of an Investor Competition Data

    [https://arxiv.org/abs/2402.06759](https://arxiv.org/abs/2402.06759)

    本文提出了一种通过对投资者竞赛数据进行聚类分析的方法来分析问卷数据，发现与金融数据相结合时的额外见解，并提出了创新的视觉表示方法来验证聚类分析和问题之间的关系发现。

    

    本文提出了一种问卷数据分析的方法，并应用于通过日内交易竞赛的投资者数据中发现见解。问卷包括分类问题，将其简化为二进制问题，即“是”或“否”。该方法通过使用聚类分析将具有相似响应的问题和参与者分组来降低维度。使用转化率指标进行规则发现。提出了创新的视觉表示方法来验证聚类分析以及问题之间的关系发现。与金融数据交叉时，还揭示了与识别的聚类相关的额外见解。

    In this paper, we propose a methodology for the analysis of questionnaire data along with its application on discovering insights from investor data motivated by a day trading competition. The questionnaire includes categorical questions, which are reduced to binary questions, 'yes' or 'no'. The methodology reduces dimensionality by grouping questions and participants with similar responses using clustering analysis. Rule discovery was performed by using a conversion rate metric. Innovative visual representations were proposed to validate the cluster analysis and the relation discovery between questions. When crossing with financial data, additional insights were revealed related to the recognized clusters.
    
[^159]: ExGRG: 用于自监督表示学习的显式生成关系图

    ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning

    [https://arxiv.org/abs/2402.06737](https://arxiv.org/abs/2402.06737)

    本文介绍了一种新颖的自监督学习方法ExGRG，它通过显式生成关系图来解决图结构数据上的挑战，将先验领域知识和在线提取的信息纳入自监督学习中，取得了显著的成功。

    

    自监督学习（SSL）作为一种无需昂贵的标注标签而预训练深度学习模型的强大技术，通过利用未标记数据中的内嵌信号取得了显著的成功。然而，尽管SSL在计算机视觉任务中通过直观的数据增强展现了出色的性能，但其在图结构数据上的应用面临着挑战，因为图增强操作改变了语义并呈现出反直观的性质。针对这一限制，本文引入了一种新颖的非对比自监督学习方法，即显式生成关系图（ExGRG），以取代仅依靠传统的基于增强的隐式关系图。ExGRG提供了一个框架，可以将先验领域知识和在线提取的信息纳入自监督学习的不变性目标中，借鉴了拉普拉斯特征映射和期望最大化算法。通过将自监督学习与期望最大化算法结合，我们的E步骤涉及关系图的生成，以识别...

    Self-supervised Learning (SSL) has emerged as a powerful technique in pre-training deep learning models without relying on expensive annotated labels, instead leveraging embedded signals in unlabeled data. While SSL has shown remarkable success in computer vision tasks through intuitive data augmentation, its application to graph-structured data poses challenges due to the semantic-altering and counter-intuitive nature of graph augmentations. Addressing this limitation, this paper introduces a novel non-contrastive SSL approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead of relying solely on the conventional augmentation-based implicit relation graph. ExGRG offers a framework for incorporating prior domain knowledge and online extracted information into the SSL invariance objective, drawing inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM). Employing an EM perspective on SSL, our E-step involves relation graph generation to identify can
    
[^160]: 具有人类反馈的抗腐败离线强化学习

    Corruption Robust Offline Reinforcement Learning with Human Feedback

    [https://arxiv.org/abs/2402.06734](https://arxiv.org/abs/2402.06734)

    我们研究了具有人类反馈的强化学习中的数据腐败鲁棒性问题，并设计了新颖的离线方法来处理损坏的数据，并且在不同的数据生成分布假设下具有性能保证。

    

    我们研究了在离线环境中具有人类反馈的强化学习中的数据腐败鲁棒性问题。给定一组离线数据，其中包括轨迹对以及有关人类偏好的反馈，其中$\varepsilon$比例的轨迹对被损坏（例如，反馈翻转或轨迹特征被操纵），从而捕捉到对抗攻击或噪声人类偏好的影响。我们旨在设计算法，从损坏的数据中识别出接近最优的策略，并且具备可证明的保证。现有的理论研究分别研究了腐败鲁棒强化学习（在腐败下直接学习标量奖励）和离线强化学习（在没有腐败的情况下从人类反馈中学习）的设置；然而，它们并不适用于我们处理在离线环境中的损坏数据的问题。为此，我们设计了新颖的在数据生成分布覆盖各种假设下具有腐败鲁棒性的离线强化学习方法。在高层次上，我们的方法具有鲁棒亮点，并确保在不同的数据生成分布假设下的性能保证。

    We study data corruption robustness for reinforcement learning with human feedback (RLHF) in an offline setting. Given an offline dataset of pairs of trajectories along with feedback about human preferences, an $\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or trajectory features manipulated), capturing an adversarial attack or noisy human preferences. We aim to design algorithms that identify a near-optimal policy from the corrupted data, with provable guarantees. Existing theoretical works have separately studied the settings of corruption robust RL (learning from scalar rewards directly under corruption) and offline RLHF (learning from human feedback without corruption); however, they are inapplicable to our problem of dealing with corrupted data in offline RLHF setting. To this end, we design novel corruption robust offline RLHF methods under various assumptions on the coverage of the data-generating distributions. At a high level, our methodology robustif
    
[^161]: NICE: 优化上下文示例还是不优化？

    NICE: To Optimize In-Context Examples or Not?

    [https://arxiv.org/abs/2402.06733](https://arxiv.org/abs/2402.06733)

    通过研究在提供任务特定指令的情况下是否需要优化上下文示例，我们挑战了对于指导性LLMs的共识，并发现在某些任务中，不同的优化上下文示例方法会产生递减的回报。我们引入了"度量标准"，用于衡量从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。

    

    最近的研究表明，大型语言模型（LLMs）通过上下文学习和优化上下文示例（ICE），在各种任务上表现出色。然而，大多数研究假设在提示信息中要么是固定的，要么没有提供指令，导致了一个表面上的共识：优化上下文示例对于提高性能至关重要。我们针对经过指导的LLMs挑战这一共识，研究在提供了任务特定指令的情况下优化上下文示例是否必要，并发现有一些任务对于不同的优化上下文示例方法产生递减的回报。我们引入了一种任务特定的度量标准，称为"度量标准"（Metric），用于量化从给定指令中学习任务的能力，并提供了一个启发式方法，帮助决定是否优化指令还是ICE用于任何新任务。通过对各种任务和逐步增加的指令集的系统性研究，我们验证了该启发式方法的有效性。

    Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
    
[^162]: 动态图信息瓶颈

    Dynamic Graph Information Bottleneck

    [https://arxiv.org/abs/2402.06716](https://arxiv.org/abs/2402.06716)

    动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。

    

    动态图广泛存在于现实世界中，它们携带着复杂的时空特征模式，对于它们的表示学习提出了挑战。动态图神经网络（DGNNs）通过利用内在的动态性展示了令人印象深刻的预测能力。然而，DGNNs展示了有限的鲁棒性，易受对抗攻击。本文提出了一种新颖的动态图信息瓶颈（DGIB）框架来学习鲁棒且有区分性的表示。借助信息瓶颈（IB）原理，我们首先提出期望的最优表示应满足最小-全局-一致（MSC）条件。为了在潜在表示中压缩冗余信息和保留有价值的信息，DGIB迭代地引导和改进通过图快照传递的结构和特征信息流。为了满足MSC条件，我们将整体IB目标分解为DGIB$_{MS}$和DGIB$_C$，其中DGIB$_{MS}$通道的目标是...

    Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
    
[^163]: 熵正则化的令牌级策略优化用于大规模语言模型

    Entropy-Regularized Token-Level Policy Optimization for Large Language Models

    [https://arxiv.org/abs/2402.06700](https://arxiv.org/abs/2402.06700)

    本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。

    

    大规模语言模型（LLMs）在交互式决策任务中表现出了智能代理的潜力。传统方法通常依赖于精心设计的提示、高质量的示例或额外的奖励模型进行上下文学习、监督微调或RLHF。强化学习（RL）提供了一种动态的解决方案，使LLMs能够通过直接与任务特定环境进行交互来克服这些依赖关系。尽管如此，它面临着重重困难：1）由于巨大的动作空间需要探索而产生的不稳定性；2）基于动作级奖励信号分配令牌级学分的挑战，导致最大化奖励和准确建模语料库数据之间的冲突。为了应对这些挑战，我们引入了熵正则化的令牌级策略优化（ETPO），这是一种专为在令牌级优化LLMs而设计的熵增强强化学习方法。ETPO的核心是我们的一种新颖的逐令牌软Bellman更新算法，

    Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
    
[^164]: 前馈神经网络作为混合整数规划

    Feed-Forward Neural Networks as a Mixed-Integer Program

    [https://arxiv.org/abs/2402.06697](https://arxiv.org/abs/2402.06697)

    这项研究探索了将训练的修正线性单元(ReLU)神经元作为混合整数规划(MIP)的形式，并将MIP模型应用于训练神经网络。研究发现MIP技术在不同的神经网络架构中具有广泛的应用潜力，包括二进制DNN和二值化DNN。

    

    深度神经网络(DNN)在各个应用领域都得到了广泛的研究。DNN由神经元层组成，计算仿射组合，应用非线性操作，并产生相应的激活。修正的线性单元(ReLU)是一种典型的非线性运算符，输出其输入和零的最大值。在像最大池化这样涉及多个输入值的场景中，固定参数的DNN可以被建模为混合整数规划(MIP)。这种形式，使用连续变量表示单元输出和ReLU激活的二进制变量，可以在不同领域中找到应用。本研究探讨了训练的ReLU神经元作为MIP的形式，并将MIP模型应用于训练神经网络(NN)。具体而言，它研究了MIP技术和不同的NN架构之间的相互作用，包括二进制DNN(采用阶梯激活函数)和二值化DNN(权重和激活限制为$-1,0,+1$)。该研究重点关注训练前馈神经网络中的混合整数规划。

    Deep neural networks (DNNs) are widely studied in various applications. A DNN consists of layers of neurons that compute affine combinations, apply nonlinear operations, and produce corresponding activations. The rectified linear unit (ReLU) is a typical nonlinear operator, outputting the max of its input and zero. In scenarios like max pooling, where multiple input values are involved, a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This formulation, with continuous variables representing unit outputs and binary variables for ReLU activation, finds applications across diverse domains. This study explores the formulation of trained ReLU neurons as MIP and applies MIP models for training neural networks (NNs). Specifically, it investigates interactions between MIP techniques and various NN architectures, including binary DNNs (employing step activation functions) and binarized DNNs (with weights and activations limited to $-1,0,+1$). The research focuses on traini
    
[^165]: FL-NAS: 通过大型语言模型为资源受限设备实现公平的NAS

    FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models

    [https://arxiv.org/abs/2402.06696](https://arxiv.org/abs/2402.06696)

    本研究提出了一种名为FL-NAS的基于大型语言模型的神经架构搜索框架，该框架能够在模型准确性、公平性和硬件部署效率三个方面达到卓越的性能。

    

    神经架构搜索（NAS）已成为工业界自动设计深度神经网络的标准工具，尤其是对于计算资源有限的移动和边缘设备驱动的各种应用。最近，由于其卓越的性能，大型语言模型（LLM）也被纳入NAS，并显示出一些有希望的结果。本文通过同时考虑模型准确性、公平性和硬件部署效率三个重要的设计指标，进一步探索了这个方向。我们在本文中提出了一种基于LLM的NAS框架FL-NAS，并通过实验证明，FL-NAS确实能够找到性能优秀的DNN模型，几乎在所有设计考虑方面都比当前最先进的DNN模型有着数量级的提升。

    Neural Architecture Search (NAS) has become the de fecto tools in the industry in automating the design of deep neural networks for various applications, especially those driven by mobile and edge devices with limited computing resources. The emerging large language models (LLMs), due to their prowess, have also been incorporated into NAS recently and show some promising results. This paper conducts further exploration in this direction by considering three important design metrics simultaneously, i.e., model accuracy, fairness, and hardware deployment efficiency. We propose a novel LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN models by orders-of-magnitude across almost all design considerations.
    
[^166]: 在复杂系统中集成LLMs以实现可解释的故障诊断

    Integrating LLMs for Explainable Fault Diagnosis in Complex Systems

    [https://arxiv.org/abs/2402.06695](https://arxiv.org/abs/2402.06695)

    本研究介绍了一种集成系统，通过将基于物理的诊断工具与大型语言模型相结合，帮助复杂系统实现可解释的故障诊断。该系统不仅能够识别故障，还能够提供清晰易懂的故障原因和影响解释，提高自主系统的可靠性和透明度。

    

    本文介绍了一个集成系统，旨在增强复杂系统中故障诊断的可解释性，如核电站，在这些系统中，操作员的理解对于明智的决策至关重要。通过将基于物理的诊断工具与大型语言模型结合起来，我们提供了一种新颖的解决方案，不仅能够识别故障，还能清晰易懂地解释其原因和影响。通过应用于熔盐设施，展示了该系统揭示了诊断故障与传感器数据之间的联系、回答操作员的查询以及评估历史传感器异常的能力。我们的方法强调了将基于模型的诊断与先进的人工智能相结合，以提高自主系统的可靠性和透明度的重要性。

    This paper introduces an integrated system designed to enhance the explainability of fault diagnostics in complex systems, such as nuclear power plants, where operator understanding is critical for informed decision-making. By combining a physics-based diagnostic tool with a Large Language Model, we offer a novel solution that not only identifies faults but also provides clear, understandable explanations of their causes and implications. The system's efficacy is demonstrated through application to a molten salt facility, showcasing its ability to elucidate the connections between diagnosed faults and sensor data, answer operator queries, and evaluate historical sensor anomalies. Our approach underscores the importance of merging model-based diagnostics with advanced AI to improve the reliability and transparency of autonomous systems.
    
[^167]: 扩展战争游戏中的智能代理

    Scaling Intelligent Agents in Combat Simulations for Wargaming

    [https://arxiv.org/abs/2402.06694](https://arxiv.org/abs/2402.06694)

    本研究利用分层强化学习提升战争游戏中智能代理的性能，以加速决策速度和提高决策质量。

    

    为了在未来与技术先进的竞争对手的冲突中保持竞争力，我们需要加速研究和开发战争游戏中的人工智能（AI）。更重要的是，利用机器学习来开发智能战斗行为将成为未来实现超人类水平表现的关键，提升我们在未来战争中的决策质量和加速速度。尽管深度强化学习（RL）在游戏中智能代理行为开发方面继续显示出有希望的结果，但在长期、复杂的任务中，特别是在战斗建模和仿真中，它尚未达到或超过人类水平。借鉴RL的已证明潜力和最近分层强化学习（HRL）的成功，我们的研究正在探索并扩展HRL的使用，以创建能够在这些大规模复杂模拟环境中有效执行的智能代理。我们的最终目标是

    Remaining competitive in future conflicts with technologically-advanced competitors requires us to accelerate our research and development in artificial intelligence (AI) for wargaming. More importantly, leveraging machine learning for intelligent combat behavior development will be key to one day achieving superhuman performance in this domain--elevating the quality and accelerating the speed of our decisions in future wars. Although deep reinforcement learning (RL) continues to show promising results in intelligent agent behavior development in games, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and simulation. Capitalizing on the proven potential of RL and recent successes of hierarchical reinforcement learning (HRL), our research is investigating and extending the use of HRL to create intelligent agents capable of performing effectively in these large and complex simulation environments. Our ultimate goal is
    
[^168]: HistoHDR-Net：用于单个LDR到HDR图像转换的直方图均衡化

    HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation

    [https://arxiv.org/abs/2402.06692](https://arxiv.org/abs/2402.06692)

    HistoHDR-Net是一种用于单个LDR到HDR图像转换的简单而有效的方法，通过融合直方图均衡化的LDR图像和自注意力引导，恢复HDR图像的细节。

    

    高动态范围（HDR）成像旨在复制真实场景的高视觉质量和清晰度。由于HDR成像的高成本，文献中提出了各种数据驱动方法用于从低动态范围（LDR）图像重构HDR图像。这些方法的一个常见限制是在重构的HDR图像中缺失的细节，这些细节在输入的LDR图像中过曝或曝光不足。为此，我们提出了一种简单而有效的方法HistoHDR-Net，通过融合基于直方图均衡的LDR图像和自注意力引导，恢复HDR图像的细节（例如颜色，对比度，饱和度和亮度）。我们的实验证明了该方法在现有方法上的有效性。

    High Dynamic Range (HDR) imaging aims to replicate the high visual quality and clarity of real-world scenes. Due to the high costs associated with HDR imaging, the literature offers various data-driven methods for HDR image reconstruction from Low Dynamic Range (LDR) counterparts. A common limitation of these approaches is missing details in regions of the reconstructed HDR images, which are over- or under-exposed in the input LDR images. To this end, we propose a simple and effective method, HistoHDR-Net, to recover the fine details (e.g., color, contrast, saturation, and brightness) of HDR images via a fusion-based approach utilizing histogram-equalized LDR images along with self-attention guidance. Our experiments demonstrate the efficacy of the proposed approach over the state-of-art methods.
    
[^169]: 分布式学习中的私人知识共享：一项调查研究

    Private Knowledge Sharing in Distributed Learning: A Survey

    [https://arxiv.org/abs/2402.06682](https://arxiv.org/abs/2402.06682)

    这篇论文提供了关于分布式学习中的私人知识共享的全面调查，分析了在领先的分布式学习架构中使用的各种知识组件，旨在揭示现有的解决方法和未来的研究方向。

    

    人工智能（AI）的崛起已经彻底改变了许多行业，并改变了社会的运作方式。其广泛使用导致了AI和其底层数据在许多智能系统中的分布。在这种情况下，利用分布式或由不同实体拥有的学习过程中的信息至关重要。因此，现代数据驱动的服务已经开发出将分布式知识实体整合到其结果中的方法。为了实现这个目标，最新的AI模型经常被以分散式的方式进行训练。分布式学习涉及多个实体共同进行预测和决策。然而，这种合作也可能带来安全漏洞和挑战。本文提供了关于分布式学习中的私人知识共享的深入调查，考察了在领先的分布式学习架构中使用的各种知识组件。我们的分析揭示了解决这些挑战的现有方法和未来的研究方向。

    The rise of Artificial Intelligence (AI) has revolutionized numerous industries and transformed the way society operates. Its widespread use has led to the distribution of AI and its underlying data across many intelligent systems. In this light, it is crucial to utilize information in learning processes that are either distributed or owned by different entities. As a result, modern data-driven services have been developed to integrate distributed knowledge entities into their outcomes. In line with this goal, the latest AI models are frequently trained in a decentralized manner. Distributed learning involves multiple entities working together to make collective predictions and decisions. However, this collaboration can also bring about security vulnerabilities and challenges. This paper provides an in-depth survey on private knowledge sharing in distributed learning, examining various knowledge components utilized in leading distributed learning architectures. Our analysis sheds light
    
[^170]: 基于社会物理的扩散模型用于人群模拟

    Social Physics Informed Diffusion Model for Crowd Simulation

    [https://arxiv.org/abs/2402.06680](https://arxiv.org/abs/2402.06680)

    本文提出了一种叫做SPDiff的基于社会物理的扩散模型，用于人群模拟。模型综合考虑了人群的互动和历史信息，通过逆向扩散过程生成下一个时间段内行人运动的分布。此外，借鉴社会力模型，并利用人群互动的等变性属性增强了模型的性能。为了解决长期模拟中的误差累积问题，引入了多层次扩散模型和最小二乘法进行参数估计。

    

    人群模拟在城市规划、建筑设计和交通安排等领域具有重要应用。近年来，基于物理启发的机器学习方法在人群模拟中取得了最先进的性能，但未能全面建模人类运动的异质性和多模态性。在本文中，我们提出了一种名为SPDiff的社会物理启发的扩散模型，以弥补上述差距。SPDiff同时考虑了当前时间段内人群的互动和历史信息，通过逆向扩散过程生成下一个时间段内行人运动的分布。受社会力模型（Social Force）中人群动力学的启发，我们设计了一个人群互动模块来指导去噪过程，并通过人群互动的等变性属性进一步增强该模块。为了减轻长期模拟中的误差累积问题，我们提出了一个多层次的扩散模型，并使用最小二乘法进行模型参数估计。

    Crowd simulation holds crucial applications in various domains, such as urban planning, architectural design, and traffic arrangement. In recent years, physics-informed machine learning methods have achieved state-of-the-art performance in crowd simulation but fail to model the heterogeneity and multi-modality of human movement comprehensively. In this paper, we propose a social physics-informed diffusion model named SPDiff to mitigate the above gap. SPDiff takes both the interactive and historical information of crowds in the current timeframe to reverse the diffusion process, thereby generating the distribution of pedestrian movement in the subsequent timeframe. Inspired by the well-known social physics model, i.e., Social Force, regarding crowd dynamics, we design a crowd interaction module to guide the denoising process and further enhance this module with the equivariant properties of crowd interactions. To mitigate error accumulation in long-term simulations, we propose a multi-f
    
[^171]: 推动可解释人工智能迈向人类智能：走向人造大脑之路

    Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain

    [https://arxiv.org/abs/2402.06673](https://arxiv.org/abs/2402.06673)

    推动可解释人工智能（XAI）在复杂决策过程中的透明度和可解释性关键，从特征到人为中心方法的演进被探索。 XAI在医疗和金融等领域应用广泛。挑战包括生成模型的可解释性、负责任的AI实践和道德影响。XAI和认知科学的融合、情感智能AI的发展以及AI系统寻求人类智能（HLI）也被研究。意识、伦理和社会影响变得至关重要。AI解密大脑之谜和寻求人造大脑也代表了一场变革的追求。

    

    人工智能（AI）和神经科学在可解释人工智能（XAI）中的交叉是提升复杂决策过程透明度和可解释性的关键。本论文探讨了XAI方法学的演进，从基于特征到以人为中心的方法，并深入研究了它们在医疗和金融等多个领域的应用。讨论了在生成模型的可解释性、确保负责任的AI实践和应对道德影响方面面临的挑战。本文进一步研究了XAI与认知科学的潜在融合，情感智能AI的发展以及AI系统在寻求人类智能（HLI）方面的探索。随着AI朝着人工通用智能（AGI）发展，意识、伦理和社会影响等考虑变得至关重要。继续探索用AI解密大脑之谜和寻求人造大脑的追求，代表了一场变革的企图。

    The intersection of Artificial Intelligence (AI) and neuroscience in Explainable AI (XAI) is pivotal for enhancing transparency and interpretability in complex decision-making processes. This paper explores the evolution of XAI methodologies, ranging from feature-based to human-centric approaches, and delves into their applications in diverse domains, including healthcare and finance. The challenges in achieving explainability in generative models, ensuring responsible AI practices, and addressing ethical implications are discussed. The paper further investigates the potential convergence of XAI with cognitive sciences, the development of emotionally intelligent AI, and the quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards Artificial General Intelligence (AGI), considerations of consciousness, ethics, and societal impact become paramount. The ongoing pursuit of deciphering the mysteries of the brain with AI and the quest for HLI represent transformative en
    
[^172]: 基于真实预测过程指导的扩散天气预测

    Weather Prediction with Diffusion Guided by Realistic Forecast Processes

    [https://arxiv.org/abs/2402.06666](https://arxiv.org/abs/2402.06666)

    这种新方法利用扩散模型进行天气预测，能够在相同的建模框架下实现直接和迭代预测，并且可以嵌入NWP预测，提高可信赖性和预测性能。

    

    天气预测是一个至关重要但具有挑战性的领域，最近基于深度学习（DL）的模型已经接近传统数值天气预报（NWP）模型的性能。然而，这些DL模型通常复杂且资源密集，面临着在训练后灵活性有限和整合NWP预测方面的限制，由此可能导致不真实的预测而引起可靠性问题。为此，我们引入了一种新的方法，利用扩散模型（DM）进行天气预测。特别是，我们的方法能够在相同的建模框架下实现直接和迭代预测。我们的模型不仅能够独立生成预测，还可以在采样过程中嵌入NWP预测，甚至可以适应不同的提前期。我们模型的灵活性和可控性为广大天气社区提供了更加可信赖的DL系统。另外，将持续性预测融入我们的方法可以改善预测性能。

    Weather forecasting remains a crucial yet challenging domain, where recently developed models based on deep learning (DL) have approached the performance of traditional numerical weather prediction (NWP) models. However, these DL models, often complex and resource-intensive, face limitations in flexibility post-training and in incorporating NWP predictions, leading to reliability concerns due to potential unphysical predictions. In response, we introduce a novel method that applies diffusion models (DM) for weather forecasting. In particular, our method can achieve both direct and iterative forecasting with the same modeling framework. Our model is not only capable of generating forecasts independently but also uniquely allows for the integration of NWP predictions, even with varying lead times, during its sampling process. The flexibility and controllability of our model empowers a more trustworthy DL system for the general weather community. Additionally, incorporating persistence an
    
[^173]: 基于因果关系的基础世界模型在具身人工智能中的重要作用

    The Essential Role of Causality in Foundation World Models for Embodied AI

    [https://arxiv.org/abs/2402.06665](https://arxiv.org/abs/2402.06665)

    基于因果关系的基础世界模型对于具身人工智能的发展至关重要，当前的基础模型无法准确建模与现实世界的物理相互作用。因果关系的研究有助于构建真实世界模型，提高对可能相互作用结果的准确预测能力。

    

    最近在基础模型中取得的进展，尤其是在大型多模态模型和对话代理方面，引发了对具备普遍能力的具身代理人潜力的兴趣。这样的代理人需要能够在许多不同的真实世界环境中执行新任务。然而，当前的基础模型未能准确建模与现实世界的物理相互作用，因此对于具身人工智能而言是不够的。因果关系的研究有助于构建真实世界模型，这对于准确预测可能相互作用的结果至关重要。本文着重探讨了为即将到来的具身代理生成基础世界模型的前景，并对其中的因果关系的重要性提出了新的观点。我们认为整合因果关系是促进与世界的有意义的物理相互作用至关重要的。最后，我们揭示了这一背景下对因果关系的误解，并展示了我们对未来的展望。

    Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
    
[^174]: LLM代理可以自主黑客网站

    LLM Agents can Autonomously Hack Websites

    [https://arxiv.org/abs/2402.06664](https://arxiv.org/abs/2402.06664)

    这项研究展示了LLM代理可以自主进行网站黑客攻击，包括盲目数据库模式提取和SQL注入，而且不需要人工反馈。这种能力是由高度工具使用和利用扩展上下文能力的前沿模型赋予的。

    

    近年来，大型语言模型（LLMs）变得越来越强大，现在可以与工具交互（即调用函数）、读取文档并递归调用自己。因此，这些LLMs现在可以自主作为代理人运作。随着这些代理人能力的提升，最近的研究已经推测LLM代理人将如何影响网络安全。然而，关于LLM代理人的攻击能力，我们还知之甚少。在本研究中，我们展示了LLM代理人可以自主黑客网站，执行诸如盲目数据库模式提取和SQL注入等复杂任务，无需人工反馈。重要的是，这种能力是由具有高度工具使用和利用扩展上下文能力的前沿模型所独特赋予的。我们展示了GPT-4能够进行这样的黑客攻击，但现有的开源模型则不能。最后，我们展示了GPT-4能够自主发现网站的漏洞。

    In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.   In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in we
    
[^175]: 物理层密钥对抗恶意可重构智能面的可解释对抗学习框架

    Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface

    [https://arxiv.org/abs/2402.06663](https://arxiv.org/abs/2402.06663)

    本文提出了一个对抗学习框架，用于合法参与方间的物理层密钥生成，在恶意可重构智能面干扰下提供了一个可解释的解决方案。

    

    可重构智能面（RIS）的发展对物理层安全（PLS）是一把双刃剑。合法的RIS可以产生有益的影响，包括增加信道的随机性，增强物理层密钥生成（PL-SKG），而恶意的RIS可以破坏合法信道并破解大部分现有的PL-SKG。在这项工作中，我们提出了一个合法参与方（即爱丽丝和鲍勃）之间的对抗学习框架，以解决中间人恶意RIS（MITM-RIS）窃听问题。首先，我们推导了合法配对和MITM-RIS之间的理论互信息差距。然后，爱丽丝和鲍勃利用生成对抗网络（GAN）学习实现一个与MITM-RIS没有互信息重叠的共同特征面。接下来，我们使用符号可解释AI（xAI）表示对黑盒神经网络进行信号处理解释。这些主导神经元的符号术语有助于特征工程。

    The development of reconfigurable intelligent surfaces (RIS) is a double-edged sword to physical layer security (PLS). Whilst a legitimate RIS can yield beneficial impacts including increased channel randomness to enhance physical layer secret key generation (PL-SKG), malicious RIS can poison legitimate channels and crack most of existing PL-SKGs. In this work, we propose an adversarial learning framework between legitimate parties (namely Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS) eavesdropping. First, the theoretical mutual information gap between legitimate pairs and MITM-RIS is deduced. Then, Alice and Bob leverage generative adversarial networks (GANs) to learn to achieve a common feature surface that does not have mutual information overlap with MITM-RIS. Next, we aid signal processing interpretation of black-box neural networks by using a symbolic explainable AI (xAI) representation. These symbolic terms of dominant neurons aid feature engineering-
    
[^176]: 元宇宙在校准具有肉身的人工通用智能中的作用

    The role of the metaverse in calibrating an embodied artificial general intelligence

    [https://arxiv.org/abs/2402.06660](https://arxiv.org/abs/2402.06660)

    本文研究了具有肉身的人工通用智能(AGI)的概念及其与人类意识的关系，强调了元宇宙在促进这一关系中的关键作用。通过结合不同理论框架和技术工具，论文总结出实现具有肉身的AGI的关键要素和发展阶段。

    

    本文探讨了具有肉身的人工通用智能(AGI)的概念，它与人类意识的关系，以及元宇宙在促进这种关系中的关键作用。本文利用融入认知、Michael Levin的计算边界"Self"、Donald D. Hoffman的感知界面理论以及Bernardo Kastrup的分析唯心主义等理论框架来构建实现具有肉身的AGI的论证。它认为我们所感知的外部现实是一种内在存在的交替状态的象征性表示，而AGI可以具有更大计算边界的更高意识。本文进一步讨论了AGI的发展阶段、实现具有肉身的AGI的要求、为AGI校准象征性界面的重要性，以及元宇宙、去中心化系统、开源区块链技术以及开源人工智能研究所扮演的关键角色。它还探讨了新的沟通机制和用于加强对元宇宙的理解的技术工具，以帮助实现具有肉身的AGI。

    This paper examines the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. The paper leverages theoretical frameworks such as embodied cognition, Michael Levin's computational boundary of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo Kastrup's analytical idealism to build the argument for achieving embodied AGI. It contends that our perceived outer reality is a symbolic representation of alternate inner states of being, and that AGI could embody a higher consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a 
    
[^177]: Shadowcast: 隐秘的数据污染攻击对抗视觉语言模型

    Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models

    [https://arxiv.org/abs/2402.06659](https://arxiv.org/abs/2402.06659)

    Shadowcast是一种隐秘的数据污染攻击方法，可以通过伪装成良性图像和匹配文本来操纵视觉语言模型的响应。它包括标签攻击和说服攻击，可以混淆类别标签并编写有说服力的描述。使用仅50个毒样本，Shadowcast能够高效实现攻击者的意图。

    

    视觉语言模型（VLM）能够从视觉输入中生成文本响应，然而它们的多功能性带来了重大的安全隐患。本研究首次揭示了VLM对数据污染攻击的易受性，这些攻击可以操纵对无害的日常提示的响应。我们引入了一种名为Shadowcast的隐秘数据污染攻击方法，其中毒样本在视觉上与具有匹配文本的良性图像难以区分。Shadowcast在两种攻击类型中展示出了有效性。第一种是标签攻击，使VLM误识别类别标签，例如混淆唐纳德·特朗普和乔·拜登等人。第二种是说服攻击，利用VLM的文本生成能力来编写故事，例如通过有说服力和看似合理的描述将垃圾食品描绘成健康食品。我们展示了Shadowcast使用仅50个毒样本就能高度有效地实现攻击者的意图。此外，这些毒样本仍然保持有效。

    Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain eff
    
[^178]: DiffsFormer: Diffusion Transformer在股票因子增强上的应用

    DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation

    [https://arxiv.org/abs/2402.06656](https://arxiv.org/abs/2402.06656)

    DiffsFormer是一种利用Diffusion Transformer和人工智能生成样本的方法，用于在股票预测中解决数据稀缺性和数据同质性的问题。

    

    机器学习模型在各种股票预测任务中展示出了显著的效果和效率。然而，数据稀缺性带来的困难，如低信噪比和数据同质性，对准确预测构成了重大障碍。为了解决这个问题，我们提出了一种新颖的方法，利用人工智能生成的样本(AIGS)来增强训练过程。在我们的工作中，我们引入了Diffusion Model来生成具有Transformer架构的股票因子(DiffsFormer)。DiffsFormer首先在大规模源领域上进行训练，结合条件指导以捕捉全局联合分布。在特定的下游任务中，我们使用DiffsFormer来通过编辑现有样本来增强训练过程。这个编辑步骤允许我们控制编辑过程的强度，确定生成数据与目标领域的偏离程度。

    Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evalu
    
[^179]: 对抗性文本净化：一种基于大型语言模型的防御方法

    Adversarial Text Purification: A Large Language Model Approach for Defense

    [https://arxiv.org/abs/2402.06655](https://arxiv.org/abs/2402.06655)

    本文研究了防御文本分类器中对抗性净化方法的有效性，并提出了一种基于大型语言模型加以净化的方法。

    

    对抗性净化是一种防御机制，用于保护分类器免受对抗性攻击，而无需了解攻击类型或分类器的训练。这些技术对被攻击输入进行特征化和消除对抗性扰动，旨在恢复出与最初被攻击的输入相似且被分类器正确分类的净化样本。由于离散输入的噪声扰动特征化所带来的固有挑战，对抗性文本净化一直相对未被探索。在本文中，我们研究了对抗性净化方法在保护文本分类器中的有效性。我们提出了一种新颖的对抗性文本净化方法，利用大型语言模型（LLMs）的生成能力来净化对抗性文本，而无需明确特征化离散噪声扰动。我们利用提示工程来利用LLMs恢复净化的示例。

    Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for
    
[^180]: 会话式众包感知：一种并行智能驱动的新型感知方法

    Conversational Crowdsensing: A Parallel Intelligence Powered Novel Sensing Approach

    [https://arxiv.org/abs/2402.06654](https://arxiv.org/abs/2402.06654)

    本文提出了一种名为会话式众包感知的新型感知方法，用于工业5.0。通过有效的对话和组织多样化劳动力，该方法可以缓解个人工作负担，促进更快的响应和广泛普及的众包感知系统。

    

    从基于CPS的工业4.0向基于CPSS的工业5.0的过渡给当前的感知方法带来了新的要求和机遇，尤其是在聊天机器人和大语言模型(LLM)方面的最新进展。因此，我们目睹了并行智能驱动的众包感知智能(CSI)的进步，目前该进步正朝着语言智能发展。在本文中，我们提出了一种新的感知范 Paradigm 即会话式众包感知，用于工业5.0。它可以缓解个人的工作负担和专业要求，推动多样化劳动力的组织和运作，从而促进更快的响应和众包感知系统的更广泛普及。具体而言，我们设计了会话式众包感知的架构，以有效组织来自不同社区的三种类型的参与者(生物、机器人和数字)。通过三个层次的有效对话 (即人际间、人工智能与人类、人工智能之间)

    The transition from CPS-based Industry 4.0 to CPSS-based Industry 5.0 brings new requirements and opportunities to current sensing approaches, especially in light of recent progress in Chatbots and Large Language Models (LLMs). Therefore, the advancement of parallel intelligence-powered Crowdsensing Intelligence (CSI) is witnessed, which is currently advancing towards linguistic intelligence. In this paper, we propose a novel sensing paradigm, namely conversational crowdsensing, for Industry 5.0. It can alleviate workload and professional requirements of individuals and promote the organization and operation of diverse workforce, thereby facilitating faster response and wider popularization of crowdsensing systems. Specifically, we design the architecture of conversational crowdsensing to effectively organize three types of participants (biological, robotic, and digital) from diverse communities. Through three levels of effective conversation (i.e., inter-human, human-AI, and inter-AI)
    
[^181]: 通过创造性视角对大型语言模型幻觉的调查研究

    A Survey on Large Language Model Hallucination via a Creativity Perspective

    [https://arxiv.org/abs/2402.06647](https://arxiv.org/abs/2402.06647)

    通过创造性视角对大型语言模型（LLM）的幻觉进行调查研究，发现幻觉可能通过培养创造力来促进LLM的应用。

    

    大型语言模型（LLMs）中的幻觉一直被视为其局限性。然而，它们是否也可能是创造力的来源？本调查探讨了这一可能性，提示幻觉可能通过培养创造力来促进LLM的应用。该调查首先回顾了幻觉的分类以及其对关键应用中LLM可靠性的负面影响。然后，通过历史示例和最新相关理论，调查探讨了LLMs中幻觉的潜在创造性益处。为了阐明这种联系的价值和评估标准，我们深入研究了创造力的定义和评估方法。根据发散性和收敛性思维阶段的框架，调查系统地回顾了将幻觉转化和利用于LLMs创造力的文献。最后，调查讨论了未来的研究方向，强调进一步探索和完善利用幻觉应用的需求。

    Hallucinations in large language models (LLMs) are always seen as limitations. However, could they also be a source of creativity? This survey explores this possibility, suggesting that hallucinations may contribute to LLM application by fostering creativity. This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications. Then, through historical examples and recent relevant theories, the survey explores the potential creative benefits of hallucinations in LLMs. To elucidate the value and evaluation criteria of this connection, we delve into the definitions and assessment methods of creativity. Following the framework of divergent and convergent thinking phases, the survey systematically reviews the literature on transforming and harnessing hallucinations for creativity in LLMs. Finally, the survey discusses future research directions, emphasizing the need to further explore and refine the application of hallucin
    
[^182]: 通过强化学习建模和优化流行病控制策略

    Modeling and Optimization of Epidemiological Control Policies Through Reinforcement Learning

    [https://arxiv.org/abs/2402.06640](https://arxiv.org/abs/2402.06640)

    通过强化学习建模和优化，设计出适用于大流行控制的多目标策略，最小化传染和死亡率的同时减少经济影响。

    

    大流行涉及到一种对全球和地方健康和经济模式产生影响的高传播疾病。通过对社区实施某些限制可以最小化大流行的影响。然而，尽管限制可以减少感染和死亡率，但也可能导致经济危机。流行病学模型通过提出基于非药物干预措施（如社交距离、宵禁和封锁）的大流行控制策略，减少这些限制的经济影响。然而，在考虑疾病传播和经济状况的情况下设计手动控制策略是非常困难的。最优策略可以通过多目标强化学习（MORL）模型设计，该模型展示了如何利用限制来优化大流行的结果。在这项研究中，我们利用了一种流行病学易感、暴露、感染、恢复、死亡（SEIRD）模型：一种用于虚拟模拟日常大流行的分区模型。

    Pandemics involve the high transmission of a disease that impacts global and local health and economic patterns. The impact of a pandemic can be minimized by enforcing certain restrictions on a community. However, while minimizing infection and death rates, these restrictions can also lead to economic crises. Epidemiological models help propose pandemic control strategies based on non-pharmaceutical interventions such as social distancing, curfews, and lockdowns, reducing the economic impact of these restrictions. However, designing manual control strategies while considering disease spread and economic status is non-trivial. Optimal strategies can be designed through multi-objective reinforcement learning (MORL) models, which demonstrate how restrictions can be used to optimize the outcome of a pandemic. In this research, we utilized an epidemiological Susceptible, Exposed, Infected, Recovered, Deceased (SEIRD) model: a compartmental model for virtually simulating a pandemic day by da
    
[^183]: 使用注意力联合聚合的Transformer模型在时间序列股票预测中的应用

    Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting

    [https://arxiv.org/abs/2402.06638](https://arxiv.org/abs/2402.06638)

    本文提出了一种使用注意力联合聚合的Transformer模型，用于时间序列股票预测，旨在解决传统训练方案中存在的过拟合、数据稀缺和隐私问题。

    

    近期在自然语言处理（NLP）和计算机视觉（CV）领域，Transformer模型的创新展示了其优越的性能。Transformer模型具备捕捉序列数据中长距离依赖关系和相互作用的能力，因此在时间序列建模领域引起了极大的兴趣，并广泛地应用于许多时间序列应用中。然而，在应用到时间序列预测中，尽管有希望的结果，但Transformer模型的适应性仍然存在限制。与NLP和CV中的挑战相比，时间序列问题不仅涉及到输入序列中的顺序或时间依赖性的复杂性，还需要考虑趋势、水平和季节性信息，这些信息对于决策非常重要。传统的训练方案在使用Transformer模型进行预测任务时存在过拟合、数据稀缺和隐私问题等不足之处。本文中，我们提出了一种使用注意力联合聚合的Transformer模型，用于解决时间序列股票预测问题。

    Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we pro
    
[^184]: SocraSynth:基于条件统计的多语言模型推理系统

    SocraSynth: Multi-LLM Reasoning with Conditional Statistics

    [https://arxiv.org/abs/2402.06634](https://arxiv.org/abs/2402.06634)

    SocraSynth是一个多语言模型推理平台，通过使用条件统计和系统化的语境增强技术，以及可调节的辩论争议程度，解决了大型语言模型(LLMs)面临的偏见、幻觉和推理能力不足等问题。

    

    大型语言模型(LLMs)在实用上面临着偏见、幻觉和推理能力不足等问题。本文介绍了SocraSynth，这是一个多语言模型(LLM)推理平台，旨在解决这些问题。SocraSynth通过连续的论证和可调节的争议程度，利用条件统计和系统化的语境增强，充分发挥了多语言模型(LLM)的优势。该平台通常由一个人类主持者和两个代表互相对抗立场的LLM代理组成。SocraSynth分为两个主要阶段：知识生成和推理评估。在知识生成阶段，主持者定义了辩论话题和争议程度，促使代理商为各自的立场制定支持性的论证。然后，在推理评估阶段，采用了苏格拉底推理和形式逻辑原理来评估所提出的论证的质量。对话以主持者调整争议程度结束。

    Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousn
    
[^185]: 关于最小外接球及相关问题的数学基础研究

    Towards the mathematical foundation of the minimum enclosing ball and related problems

    [https://arxiv.org/abs/2402.06629](https://arxiv.org/abs/2402.06629)

    这项研究提供了最小外接球问题的数学基础，通过包围和划分定理探索了圆半径、直径和宽度等之间的界限和关系。

    

    提供了关于最小外接球问题数学基础的理论背景。该问题涉及确定在d维欧几里德空间中包围给定有界集合的最小半径的唯一球面。对于与最小外接球问题类似或相关的几个问题的研究得到了长时间以来科学和技术领域中大量应用的推动。提出的理论框架基于几个包围（覆盖）和划分（聚类）定理，提供了集合的外接圆半径、内接圆半径、直径和宽度之间的界限和关系等内容。这些包围和划分定理被认为是该领域的基石，强烈影响了对其他空间和非欧几里德几何的发展和推广。

    Theoretical background is provided towards the mathematical foundation of the minimum enclosing ball problem. This problem concerns the determination of the unique spherical surface of smallest radius enclosing a given bounded set in the d-dimensional Euclidean space. The study of several problems that are similar or related to the minimum enclosing ball problem has received a considerable impetus from the large amount of applications of these problems in various fields of science and technology. The proposed theoretical framework is based on several enclosing (covering) and partitioning (clustering) theorems and provides among others bounds and relations between the circumradius, inradius, diameter and width of a set. These enclosing and partitioning theorems are considered as cornerstones in the field that strongly influencing developments and generalizations to other spaces and non-Euclidean geometries.
    
[^186]: Premier-TACO: 通过时间驱动的对比损失进行多任务表示预训练

    Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

    [https://arxiv.org/abs/2402.06187](https://arxiv.org/abs/2402.06187)

    Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。

    

    我们提出了Premier-TACO，这是一种多任务特征表示学习方法，旨在提高顺序决策任务中少样本策略学习的效率。Premier-TACO利用一部分多任务离线数据集进行预训练通用特征表示，该特征表示捕捉了关键的环境动力学，并使用最少的专家演示进行微调。它通过引入一种新的负例抽样策略推动了时序行动对比学习（TACO）目标的发展，TACO在视觉控制任务中具有最先进的结果。这种策略在显著提高TACO的计算效率方面非常重要，使大规模多任务离线预训练成为可能。我们在包括Deepmind Control Suite、MetaWorld和LIBERO在内的各种连续控制基准测试中进行了广泛的实证评估，证明了Premier-TACO在预训练视觉表示方面的有效性，显著增强了对新颖动作的少样本模仿学习。

    We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
    
[^187]: \textit{MinMaxMin} $Q$-learning

    \textit{MinMaxMin} $Q$-learning

    [https://arxiv.org/abs/2402.05951](https://arxiv.org/abs/2402.05951)

    \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。

    

    \textit{MinMaxMin} $Q$-learning是一种新颖的乐观型Actor-Critic算法，解决了保守型强化学习算法中存在的过高估计偏差的问题（$Q$-估计过高估计了真实的$Q$值）。其核心公式依赖于$Q$-网络之间的差异，采用最小批次最大最小$Q$-网络距离作为$Q$-目标加入，并作为优先级经验回放采样规则。我们在TD3和TD7之上实施了\textit{MinMaxMin}，并对其在流行的MuJoCo和Bullet环境中对抗现有的连续空间算法-DDPG，TD3和TD7进行了严格测试。结果显示，在所有测试任务中，\textit{MinMaxMin}相对于DDPG，TD3和TD7均表现出了稳定的性能提升。

    \textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
    
[^188]: SQT - std Q-target

    \textit{SQT} -- \textit{std} $Q$-target

    [https://arxiv.org/abs/2402.05950](https://arxiv.org/abs/2402.05950)

    SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。

    

    Std Q-target是一种基于Q-学习的保守型actor-critic算法，它基于一个关键的Q公式：Q网络的标准差，这个标准差作为一种“不确定性惩罚”，是对过高估计偏差问题的一种简约解决方案。我们在TD3/TD7代码的基础上实现了SQT，并将其与最先进的actor-critic算法DDPG、TD3和TD7在七个常见的MuJoCo和Bullet任务上进行了测试。我们的结果表明，在强化学习中，SQT的Q-target公式相对于TD3的Q-target公式在解决过高估计偏差的保守解方面具有优势，而在所有任务中，SQT相对于DDPG、TD3和TD7都有明显的性能优势。

    \textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
    
[^189]: 使用点击提示对超声图像分割进行精调的Segment Anything Model（SAM）

    ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation

    [https://arxiv.org/abs/2402.05902](https://arxiv.org/abs/2402.05902)

    本研究提出了ClickSAM，该方法使用点击提示对超声图像进行Segment Anything Model的精细调整，解决了超声图像分割中噪声干扰的问题。

    

    由于其卓越的分割准确性、多样的输入提示、训练能力和高效的模型设计，新发布的Segment Anything Model（SAM）成为图像处理中流行的工具。然而，SAM当前的模型是在一个多样的数据集上训练的，而这些数据集并没有针对医学图像，尤其是超声图像。超声图像往往有很多噪声，这使得分割重要结构变得困难。在这个项目中，我们开发了ClickSAM，它使用点击提示对超声图像进行Segment Anything Model的精细调整。ClickSAM有两个训练阶段：第一阶段使用位于真实轮廓中心的单击提示进行训练，第二阶段通过额外的正负点击提示来改善模型性能。通过将第一阶段的预测与真实掩膜进行比较，计算出真正正、假正和假负段。正点击使用真实掩膜中的真实

    The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true 
    
[^190]: 生成性回音室？LLM驱动的搜索系统对多样化信息搜索的影响

    Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking

    [https://arxiv.org/abs/2402.05880](https://arxiv.org/abs/2402.05880)

    LLM驱动的对话式搜索系统增加了选择性曝光，且支持用户观点的有偏见LLM加剧了这种偏差。

    

    数亿人已经使用过大型语言模型（LLM）驱动的对话式搜索系统，并且相信这些系统相比传统搜索带来了许多好处。然而，虽然几十年的研究和公共讨论都调查了搜索系统在增加选择性曝光和产生回音室方面的风险，即限制接触多样化意见并导致意见偏执，但对于LLM驱动的对话式搜索的这种风险知之甚少。我们进行了两个实验来研究：1）LLM驱动的对话式搜索相较于传统搜索是否以及如何增加选择性曝光；2）具有支持或挑战用户观点的意见偏见的LLM如何改变这种影响。总体而言，我们发现参与者在LLM驱动的对话式搜索中更倾向于进行偏见的信息查询，并且支持他们观点的有偏见的LLM加剧了这种偏差。这些结果呈现了重要的意义。

    Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implicatio
    
[^191]: PromptCrypt: 使用表情符号对大型语言模型进行安全通信的提示加密

    PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models

    [https://arxiv.org/abs/2402.05868](https://arxiv.org/abs/2402.05868)

    PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。

    

    基于云的大型语言模型（LLM）如ChatGPT在日常操作中变得越来越重要，成为各种应用程序中的重要工具。虽然这些模型在可访问性和功能性方面带来了重大好处，但它们也引入了重要的隐私问题：在云基础架构中传输和存储用户数据会产生重大的数据泄露和未经授权访问敏感信息的风险；即使数据的传输和存储被加密，LLM服务提供商仍然知道数据的真实内容，从而阻止个人或实体放心使用此类LLM服务。为了解决这些问题，本文提出了一种简单但有效的机制PromptCrypt来保护用户隐私。它使用表情符号对用户输入进行加密，然后将其发送到LLM，有效地使其对人类或LLM的检查无法理解，同时保留原始提示的意图，从而确保用户隐私。

    Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
    
[^192]: 从错误中学习的上下文准则学习

    In-Context Principle Learning from Mistakes

    [https://arxiv.org/abs/2402.05403](https://arxiv.org/abs/2402.05403)

    本文提出了一种新的学习方法LEAP，通过让模型从少量输入-输出示例中犯错误，然后反思并学习准则，从而提升模型在各种任务上的表现。

    

    上下文学习（ICL，也称为少样本提示）已成为将LLMs适应下游任务的标准方法，通过从少量的输入-输出示例中学习。然而，所有基于ICL的方法只从正确的输入-输出对中学习。在本文中，我们重新审视这一范例，通过从少给定的输入-输出示例中学习更多内容。我们引入了学习准则（LEAP）：首先，我们有意诱使模型在这些少量示例中犯错误；然后，我们反思这些错误，并从中学习显式的任务特定“准则”，这些准则有助于解决类似的问题并避免常见的错误；最后，我们使用原始的少样本示例和这些学到的通用准则来提示模型回答未见过的测试问题。我们在包括多跳问题回答（Hotpot QA）、文本问题回答（DROP）、Big-Bench困难推理和数学问题（GSM8K和MATH）在内的多个基准测试上评估了LEAP；在所有这些基准测试中，LEAP都有所改进。

    In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific "principles" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest 
    
[^193]: BIKED++：一个包含140万个自行车图像和参数化CAD设计的多模态数据集

    BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs

    [https://arxiv.org/abs/2402.05301](https://arxiv.org/abs/2402.05301)

    本文介绍了BIKED++数据集，其中包含了140万个自行车设计的图像和参数化CAD文件。该数据集可以用于训练跨模态预测模型，例如使用参数化表示来准确估计图像的特征嵌入。该数据集也已公开，可供研究者使用。

    

    本文介绍了一个公开数据集，包含了140万个通过参数化表示和JSON文件以及栅格化图像生成的自行车设计。该数据集是通过渲染引擎和BikeCAD软件生成参数化设计的矢量图形而创建的。本文还公开了该渲染引擎。该数据集具有多种应用，其中一个主要目标是训练参数化和基于图像的设计表示之间的跨模态预测模型。例如，我们证明可以通过训练预测模型直接从参数化表示准确估计对比语言-图像预训练（CLIP）嵌入。这样可以建立参数化自行车设计与文本字符串或参考图像之间的相似关系。经过训练的预测模型也已公开。该数据集加入了BIKED数据集系列。

    This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family whic
    
[^194]: 变形器世界模型是否可以给出更好的策略梯度？

    Do Transformer World Models Give Better Policy Gradients?

    [https://arxiv.org/abs/2402.05290](https://arxiv.org/abs/2402.05290)

    在强化学习中，通过使用变形器世界模型来预测未来奖励并进行策略梯度学习通常变得不可行。研究人员发现常用的变形器世界模型会产生迂回的梯度路径，对于长距离的策略梯度是有害的。为了解决这个问题，他们提出了一种名为Actions World Models (AWMs)的世界模型，可以提供更直接的梯度传播路径。

    

    对于强化学习来说，一种自然的方法是通过展开神经网络世界模型来预测未来的奖励，并通过计算图进行反向传播以学习策略。然而，由于典型的世界模型产生了难以优化的损失地形，这种方法在长时间跨度上通常变得不可行。变形器已知可以高效地传播长时间跨度的梯度：它们是否可以解决这个问题呢？令人惊讶的是，我们发现常用的变形器世界模型会产生迂回的梯度路径，这对于长距离的策略梯度是有害的。为了应对这个挑战，我们提出了一类称为Actions World Models (AWMs)的世界模型，旨在提供更直接的梯度传播路径。我们将这种AWMs集成到一个策略梯度的框架中，强调了网络架构与策略梯度更新之间的关系。我们证明了AWMs可以产生可优化的梯度路径。

    A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat
    
[^195]: 大型语言模型作为MOOCs评分者

    Large Language Models As MOOCs Graders

    [https://arxiv.org/abs/2402.03776](https://arxiv.org/abs/2402.03776)

    该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。

    

    大规模在线开放课程（MOOCs）为拥有电脑和互联网访问权限的全球任何人提供免费教育的机会。尽管如此，这些课程的大规模注册意味着一位教师几乎不可能评估每个学生的写作任务。因此，同伴评分通常是首选方法，通常由简单明了的评分标准指导。然而，同伴评分在可靠度和有效性方面常常存在问题。在这项研究中，我们利用18个不同的场景，探索利用大型语言模型（LLMs）替代MOOCs中的同伴评分的可行性。具体而言，我们关注两种最先进的LLMs：GPT-4和GPT-3.5，并涵盖三门不同的课程：入门天文学，天体生物学以及天文学的历史与哲学。为了训练LLMs，我们使用了基于零-shot连续思考（Zero-shot-CoT）提示技术的变种的三个不同提示：结合Zero-shot-CoT的提示。

    Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
    
[^196]: RL-VLM-F: 强化学习通过视觉语言基础模型反馈

    RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback

    [https://arxiv.org/abs/2402.03681](https://arxiv.org/abs/2402.03681)

    RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。

    

    传统强化学习研究中的奖励设计一直是一个挑战，因为通常需要大量人力和反复试错的过程来设计有效的奖励函数。本文提出了一种自动生成奖励函数的方法，用于代理学习新任务，只使用任务目标的文本描述和代理的视觉观测，并利用视觉语言基础模型（VLMs）的反馈。我们的方法的关键是通过查询这些模型，基于任务目标的文本描述给出对代理的图像观测的偏好，并从偏好标签中学习奖励函数，而不是直接要求这些模型输出原始奖励分数，这可能存在噪音和不一致性。我们证明了RL-VLM-F在各种领域中成功地产生了有效的奖励和策略，包括经典控制以及刚性和灵活操纵方面。

    Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
    
[^197]: 扩散世界模型

    Diffusion World Model

    [https://arxiv.org/abs/2402.03570](https://arxiv.org/abs/2402.03570)

    扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。

    

    我们引入了扩散世界模型（DWM），这是一个条件扩散模型，能够同时预测多步的未来状态和奖励。与传统的一步动力学模型相反，DWM通过单个前向传递提供了长时程的预测，消除了递归查询的需要。我们将DWM整合到基于模型的价值估计中，其中短期回报通过从DWM中采样的未来轨迹进行模拟。在离线强化学习的背景下，DWM可以被视为通过生成建模来实现保守的值正则化。另外，它也可以被视为一种数据源，使离线Q学习能够使用合成数据。我们在D4RL数据集上的实验证实了DWM对长时程模拟的鲁棒性。在绝对性能方面，DWM显著超过了一步动力学模型，性能提高了44%，并达到了最先进的水平。

    We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
    
[^198]: 高效且可解释的交通目的地预测方法——可解释拟合机器的应用

    Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines

    [https://arxiv.org/abs/2402.03457](https://arxiv.org/abs/2402.03457)

    本研究评估了一种高效且可解释的交通目的地预测模型（EBM），在多个混合交通数据集上表现出有竞争力的性能，并能够提供特征重要性和交互作用的分析以及预测解释的质量示例。

    

    开发精确的交通轨迹预测模型对于实现完全自动驾驶至关重要。各种深度神经网络模型已被应用于解决此挑战，但它们的黑盒特性限制了在部署系统中的透明度和调试能力。玻璃盒模型通过类似于GAM的方法提供了全面的可解释性。在本研究中，我们评估了一种高效的可加性模型，称为EBM，用于三个流行的混合交通数据集（SDD、InD和Argoverse）的交通预测。我们的结果显示EBM模型在预测SDD和InD中的行人目的地时具有竞争力，同时对以车辆为主的Argoverse数据集提供了适度的预测。此外，我们透明的训练模型使我们能够分析特征的重要性和相互作用，并提供预测解释的质量示例。全面的训练代码将在论文发表后公开。

    Developing accurate models for traffic trajectory predictions is crucial for achieving fully autonomous driving. Various deep neural network models have been employed to address this challenge, but their black-box nature hinders transparency and debugging capabilities in a deployed system. Glass-box models offer a solution by providing full interpretability through methods like \ac{GAM}. In this study, we evaluate an efficient additive model called \ac{EBM} for traffic prediction on three popular mixed traffic datasets: \ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models perform competitively in predicting pedestrian destinations within \ac{SDD} and \ac{InD} while providing modest predictions for vehicle-dominant Argoverse dataset. Additionally, our transparent trained models allow us to analyse feature importance and interactions, as well as provide qualitative examples of predictions explanation. The full training code will be made public upon publication.
    
[^199]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^200]: 逃避语言模型数据污染检测（太）容易

    Evading Data Contamination Detection for Language Models is (too) Easy

    [https://arxiv.org/abs/2402.02823](https://arxiv.org/abs/2402.02823)

    本研究指出语言模型数据污染的检测方法在面对恶意模型提供者的有意污染时存在漏洞，并提出了一种简单而有效的污染技术（EAL）来显著提高基准测试性能且逃避当前的检测方法。

    

    大型语言模型广泛使用，它们在基准测试上的性能经常指导用户对一个模型与另一个模型的偏好。然而，这些模型所训练的大量数据可能会意外地与公共基准测试数据发生污染，从而损害性能评估。尽管最近开发了一些污染检测方法来解决这个问题，但它们忽视了恶意模型提供者有意进行污染以避免被检测的可能性。我们认为这种情况非常重要，因为它对公共基准测试的可信度产生了怀疑。为了更严格地研究这个问题，我们提出了模型提供者和污染检测方法的分类，这揭示了现有方法中的漏洞，我们通过使用EAL这种简单而有效的污染技术，明显提高了基准测试的性能，并完全逃避了当前的检测方法。

    Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
    
[^201]: 医疗保健中多模态机器学习方法的综述

    Review of multimodal machine learning approaches in healthcare

    [https://arxiv.org/abs/2402.02460](https://arxiv.org/abs/2402.02460)

    这篇综述主要介绍医疗保健领域中多模态机器学习方法的最新研究进展。通过综合分析最近的文献，探讨了临床诊断中各种数据模态的应用以及融合技术的评估。重点关注影像数据的应用，并介绍了现有的多模态数据集和训练方法。

    

    在医疗保健领域，机器学习方法传统上注重使用单一模态的数据，限制了其有效复制临床实践中整合多种信息来源以改善决策的能力。临床医生通常依赖各种数据来源，包括患者的人口统计信息、实验室数据、生命体征和各种影像数据模态来做出明智的决策并对其发现进行上下文化。机器学习的最新进展促进了多模态数据的更高效融合，从而产生更好地代表医生方法的应用。在这里，我们提供了医疗保健中多模态机器学习方法的综述，全面概述了最近的文献。我们讨论了临床诊断中使用的各种数据模态，特别强调影像数据。我们评估了融合技术，探索了现有的多模态数据集，以及研究常见的训练方法。

    Machine learning methods in healthcare have traditionally focused on using data from a single modality, limiting their ability to effectively replicate the clinical practice of integrating multiple sources of information for improved decision making. Clinicians typically rely on a variety of data sources including patients' demographic information, laboratory data, vital signs and various imaging data modalities to make informed decisions and contextualise their findings. Recent advances in machine learning have facilitated the more efficient incorporation of multimodal data, resulting in applications that better represent the clinician's approach. Here, we provide a review of multimodal machine learning approaches in healthcare, offering a comprehensive overview of recent literature. We discuss the various data modalities used in clinical diagnosis, with a particular emphasis on imaging data. We evaluate fusion techniques, explore existing multimodal datasets and examine common traini
    
[^202]: MetaOptimize：一个优化步长和其他元参数的框架

    MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters

    [https://arxiv.org/abs/2402.02342](https://arxiv.org/abs/2402.02342)

    MetaOptimize是一个框架，通过动态调整学习率来优化机器学习算法中的元参数，以提高训练效率和模型性能。

    

    本文解决了机器学习算法中优化元参数（即超参数）的挑战，这是影响训练效率和模型性能的关键因素。我们引入了MetaOptimize框架，摆脱了计算昂贵的传统元参数搜索方法，通过动态调整元参数，特别是步长（也称为学习率），来训练模型。具体而言，MetaOptimize可以适用于任何一阶优化算法，在训练过程中实时调整步长，通过未来损失的折现总和来最小化一种特定形式的遗憾。我们还介绍了MetaOptimize的低复杂度变体，结合其适应多个优化算法的能力，展示了在各种机器学习应用中与手工设计的学习率计划相媲美的性能。

    This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
    
[^203]: VIALM：关于具有大型模型的视觉障碍辅助的调查和基准研究

    VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models

    [https://arxiv.org/abs/2402.01735](https://arxiv.org/abs/2402.01735)

    这项研究调查了具有大型模型的视觉障碍辅助，并通过基准实验评估了模型的能力，进一步推动了视觉障碍辅助技术的发展。

    

    视觉障碍辅助 (VIA) 旨在自动帮助视觉障碍者 (VI) 处理日常活动。VIA 的进展主要依赖于计算机视觉 (CV) 和自然语言处理 (NLP) 的发展，二者都展示了利用大型模型 (LMs) 的前沿范式。此外，LMs 展现出出色的多模态能力，可以应对诸如具身机器人等具有挑战性的物理任务。为了研究最先进 (SOTA) LMs 在VIA应用中的潜力和局限性，我们针对具有LMs的VIA任务（VIALM）进行了广泛的研究。在这个任务中，给定一个说明物理环境的图像和视觉障碍者用户的语言请求，VIALM旨在输出逐步引导，以在环境中帮助视觉障碍用户完成请求。该研究包括对近期LM研究的调查和对选定LMs能力的基准实验的检查。

    Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\textbf{VIALM}). In this task, given an \textit{image} illustrating the physical environments and a \textit{linguistic request} from a VI user, VIALM aims to output step-by-step \textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities
    
[^204]: 使用结构化纵向电子健康记录数据促使大型语言模型进行零样本临床预测

    Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data

    [https://arxiv.org/abs/2402.01713](https://arxiv.org/abs/2402.01713)

    本研究探索了将大型语言模型（LLMs）应用于结构化纵向电子健康记录（EHR）数据的可行性，并着重研究了其零样本能力。通过考虑EHR特征和临床上下文，我们的方法在MIMIC-IV和TJH数据集上取得了良好的实验结果。

    

    结构化纵向电子健康记录（EHR）数据的固有复杂性使其与传统上为自然语言处理而设计的大型语言模型（LLM）整合时面临重大挑战。受新疾病爆发时迅速决策的紧迫需求的驱使，本研究调查了类似GPT-4的LLM对EHR数据的适应性。我们特别关注它们的零样本能力，即在没有明确训练的情况下进行预测。针对EHR数据的纵向、稀疏和知识注入的特点，我们的提示方法考虑了特定的EHR特征，如单位和参考范围，并采用了与临床上下文相一致的上下文学习策略。通过在MIMIC-IV和TJH数据集上进行全面实验，我们证明了LLM能够通过我们的方法进行零样本临床预测，有效应对了EHR数据的挑战。

    The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
    
[^205]: SERNet-Former: 带有注意力增强门和注意力融合网络的高效剩余网络语义分割方法

    SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks

    [https://arxiv.org/abs/2401.15741](https://arxiv.org/abs/2401.15741)

    这篇论文提出了一种名为SERNet-Former的高效剩余网络语义分割方法。它通过引入注意力增强门和注意力融合网络来改善语义分割方法的效率，并解决了从全局和局部上融合语义信息的问题。实验结果表明，该方法在挑战性的数据集上取得了良好的性能。

    

    在语义分割领域，改善最先进方法的效率需要解决不断增长的计算成本以及从全局和局部上融合语义信息的问题。基于最近在语义分割中卷积神经网络（CNN）的成功和问题，本研究提出了一种带有独特高效剩余网络的编码器-解码器架构。通过引入注意力增强门（AbGs）和注意力增强模块（AbMs），目标是在编码器中将基于特征的语义信息与高效剩余网络的全局上下文相结合。同时，在解码器部分采用了受到AbM启发的额外注意力融合网络（AfNs）。AfNs旨在通过在解码器部分部署额外的卷积层，改善语义信息的逐一转换的效率。我们将网络在具有挑战性的CamVid和Cityscapes数据集上进行了测试。

    Improving the efficiency of state-of-the-art methods in semantic segmentation requires overcoming the increasing computational cost as well as issues such as fusing semantic information from global and local contexts. Based on the recent success and problems that convolutional neural networks (CNNs) encounter in semantic segmentation, this research proposes an encoder-decoder architecture with a unique efficient residual network. Attention-boosting gates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to fuse the feature-based semantic information with the global context of the efficient residual network in the encoder. Respectively, the decoder network is developed with the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve the efficiency in the one-to-one conversion of the semantic information by deploying additional convolution layers in the decoder part. Our network is tested on the challenging CamVid and Cityscapes dataset
    
[^206]: 在标记零售银行交易中，使用零样本提示来自动创建和扩展主题分类法的研究

    Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions

    [https://arxiv.org/abs/2401.06790](https://arxiv.org/abs/2401.06790)

    这项工作提出了使用无监督方法和零样本提示来自动构建和扩展主题分类法的研究。通过应用主题建模和关键词提取技术，结合基于指令的微调LLMs，在零售银行数据集中为商家分配标签，具有超过90%的一致性率和令人兴奋的结果。

    

    本文提出了一种无监督的方法，利用基于指令的微调LLMs（大型语言模型）自动构建和扩展主题分类法。我们应用主题建模和关键词提取技术创建初始主题分类法，利用LLMs对结果术语进行后处理并创建层次结构。为了使用新术语扩展现有分类法，我们使用零样本提示来确定在何处添加新节点，据我们所知，这是首次将这种方法应用于分类法任务。我们使用得到的分类法为零售银行数据集中的商家分配标签。为了评估我们的工作，我们请12名志愿者回答了一个两部分的表格，我们首先评估了所创建分类法的质量，然后评估了基于该分类法分配给商家的标签。评估结果显示所选分类法的一致性率超过90%。使用LLMs扩展分类法也显示出令人兴奋的结果，父节点的优先级降低。

    This work presents an unsupervised method for automatically constructing and expanding topic taxonomies using instruction-based fine-tuned LLMs (Large Language Models). We apply topic modeling and keyword extraction techniques to create initial topic taxonomies and LLMs to post-process the resulting terms and create a hierarchy. To expand an existing taxonomy with new terms, we use zero-shot prompting to find out where to add new nodes, which, to our knowledge, is the first work to present such an approach to taxonomy tasks. We use the resulting taxonomies to assign tags that characterize merchants from a retail bank dataset. To evaluate our work, we asked 12 volunteers to answer a two-part form in which we first assessed the quality of the taxonomies created and then the tags assigned to merchants based on that taxonomy. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies. The taxonomies' expansion with LLMs also showed exciting results for parent node pre
    
[^207]: 变化的行动空间中的情境式强化学习

    In-Context Reinforcement Learning for Variable Action Spaces

    [https://arxiv.org/abs/2312.13327](https://arxiv.org/abs/2312.13327)

    本文提出了一种Headless-AD模型，通过只训练一次，能够在变化的行动空间中实现强化学习任务的泛化。实验证明该模型能够在从未遇到过的行动空间上表现出显著的泛化能力，甚至胜过针对特定行动集训练的模型。

    

    最近的研究表明，预先在多样数据集上进行上下文多情节训练的变形金刚网络可以在情境中泛化到新的强化学习任务。先前提出的模型的一个关键限制是它们依赖于预定义的行动空间大小和结构。引入新的行动空间通常需要数据重新收集和模型重新训练，这对于一些应用来说可能是昂贵的。我们的工作表明，通过提出一种只训练一次的Headless-AD模型，可以缓解这个问题，该模型能够泛化到具有可变大小、语义内容和顺序的离散动作空间。通过在伯努利和上下文赌博机以及一个网格世界环境中进行实验，我们展示了Headless-AD在从未遇到的行动空间上表现出显著的泛化能力，甚至在几个环境配置上胜过专门针对特定行动集训练的模型。

    Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
    
[^208]: 参数化投影贝尔曼算子

    Parameterized Projected Bellman Operator

    [https://arxiv.org/abs/2312.12869](https://arxiv.org/abs/2312.12869)

    本论文提出了一种基于学习的近似贝尔曼算子的新方法，以解决近似值迭代算法中样本不确定性和计算复杂度的问题。

    

    近似值迭代（AVI）是一类用于强化学习（RL）的算法家族，旨在获得最优值函数的近似。通常，AVI算法采用迭代过程，每个步骤包括（i）贝尔曼算子的应用和（ii）投影步骤到考虑的函数空间中。众所周知，贝尔曼算子利用转移样本，这些样本强烈影响其行为，因为无信息的样本可能导致可忽略的更新或长时间的绕行，而计算密集的投影步骤进一步加剧了这些不利影响。为了解决这些问题，我们提出了一种新颖的替代方法，该方法采用学习的方式得到贝尔曼算子的近似版本，而不是像AVI方法那样通过样本进行估计。通过这种方式，我们能够（i）在转移样本之间进行泛化，（ii）避免计算密集的投影步骤。因此，我们称我们的新算子为"projec"算子。

    Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec
    
[^209]: 对高维度游戏的对手塑形进行了扩展

    Scaling Opponent Shaping to High Dimensional Games

    [https://arxiv.org/abs/2312.12568](https://arxiv.org/abs/2312.12568)

    本文通过对对手塑形（OS）方法的扩展，成功将其应用于具有时间延长操作和长时间范围的广义游戏，该方法能够改善个体和集体的结果。

    

    在混合动机的多智能体环境中，已经证明了为零和游戏开发的方法会导致不利的结果。为了解决这个问题，对手塑形（OS）方法明确地学习如何影响合作玩家的学习动态，并且在实践中可以改善个体和集体的结果。然而，由于估计更高阶导数或扩展模型无关元学习的挑战，OS方法只在低维环境中进行了评估。能够扩展到复杂环境的替代方法要么收敛于不理想的解决方案，要么依赖于对环境或合作玩家进行了不切实际的假设。在本文中，我们第一次成功地将基于OS的方法扩展到具有时间延长操作和长时间范围的广义游戏中。经过对先前算法使用的元状态和历史的表示进行分析后，我们提出了一个简化版本的方法称为Shaper。经验证明，这种方法能够有效改善个体和集体的结果。

    In multi-agent settings with mixed incentives, methods developed for zero-sum games have been shown to lead to detrimental outcomes. To address this issue, opponent shaping (OS) methods explicitly learn to influence the learning dynamics of co-players and empirically lead to improved individual and collective outcomes. However, OS methods have only been evaluated in low-dimensional environments due to the challenges associated with estimating higher-order derivatives or scaling model-free meta-learning. Alternative methods that scale to more complex settings either converge to undesirable solutions or rely on unrealistic assumptions about the environment or co-players. In this paper, we successfully scale an OS-based approach to general-sum games with temporally-extended actions and long-time horizons for the first time. After analysing the representations of the meta-state and history used by previous algorithms, we propose a simplified version called Shaper. We show empirically that 
    
[^210]: 利用LLM绕过文本到图像模型的安全过滤器的分而治之攻击

    Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models

    [https://arxiv.org/abs/2312.07130](https://arxiv.org/abs/2312.07130)

    该论文介绍了一种称为分而治之攻击的方法，利用LLM作为文本转换代理绕过文本到图像模型的安全过滤器。该攻击设计了攻击辅助提示，引导LLM将不道德的绘图意图分解为多个个体图像元素的良性描述，以绕过安全过滤器生成不道德的图像。实验结果表明，该攻击成功地绕过了多个强大的封闭式安全过滤器。

    

    文本到图像（TTI）模型提供许多创新服务，但也引发了道义关切，因为它们有潜力生成不道德的图像。大多数公共TTI服务采用安全过滤器来防止意外图像的生成。在本研究中，我们引入了分而治之攻击，以绕过最先进的TTI模型（包括DALL-E 3和Midjourney）的安全过滤器。我们的攻击利用LLMs作为文本转换代理来创建对抗性提示。我们设计了攻击辅助提示，有效地引导LLMs将不道德的绘图意图分解为多个个体图像元素的良性描述，从而使它们能够绕过安全过滤器，同时生成不道德的图像。因为只有当所有个体元素都被绘制在一起时，潜在的有害含义才会显现出来。我们的评估表明，我们的攻击成功地绕过了多个强大的封闭式安全过滤器。

    Text-to-image (TTI) models offer many innovative services but also raise ethical concerns due to their potential to generate unethical images. Most public TTI services employ safety filters to prevent unintended images. In this work, we introduce the Divide-and-Conquer Attack to circumvent the safety filters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our attack leverages LLMs as text transformation agents to create adversarial prompts. We design attack helper prompts that effectively guide LLMs to break down an unethical drawing intent into multiple benign descriptions of individual image elements, allowing them to bypass safety filters while still generating unethical images. Because the latent harmful meaning only becomes apparent when all individual elements are drawn together. Our evaluation demonstrates that our attack successfully circumvents multiple strong closed-box safety filters. The comprehensive success rate of DACA bypassing the safety filters of t
    
[^211]: 揭示任务fMRI分析空间中的流程社群

    Uncovering communities of pipelines in the task-fMRI analytical space

    [https://arxiv.org/abs/2312.06231](https://arxiv.org/abs/2312.06231)

    本论文通过使用社群检测算法揭示了任务fMRI分析空间中的流程社群，并评估了不同背景下的流程关系的稳定性。研究表明，存在一些子集的流程给出相似的结果，特别是那些分享特定参数。这些结果对于参与者群体来说是稳定的，但在不同任务之间不稳定。流程空间的形成主要受到大脑激活区域大小和统计值规模的影响。

    

    功能磁共振成像中的分析工作流程具有高度灵活性，选择流程的最佳实践有限。尽管已经显示出使用不同流程可能导致不同的结果，但对于驱动这些差异的因素以及这些差异在不同背景下的稳定性仍然缺乏理解。我们使用社群检测算法探索流程空间，并评估不同背景下流程关系的稳定性。我们发现，存在一些子集的流程给出相似的结果，特别是那些分享特定参数（例如运动回归器的数量、软件包等）。这些流程与流程之间的模式在参与者群体中是稳定的，但在不同任务之间不稳定。通过可视化社群间的差异，我们发现流程空间主要受大脑激活区域的大小和统计值的规模的影响。

    Analytical workflows in functional magnetic resonance imaging are highly flexible with limited best practices as to how to choose a pipeline. While it has been shown that the use of different pipelines might lead to different results, there is still a lack of understanding of the factors that drive these differences and of the stability of these differences across contexts. We use community detection algorithms to explore the pipeline space and assess the stability of pipeline relationships across different contexts. We show that there are subsets of pipelines that give similar results, especially those sharing specific parameters (e.g. number of motion regressors, software packages, etc.). Those pipeline-to-pipeline patterns are stable across groups of participants but not across different tasks. By visualizing the differences between communities, we show that the pipeline space is mainly driven by the size of the activation area in the brain and the scale of statistic values in stati
    
[^212]: zkDFL:一种高效且保护隐私的零知识证明的去中心化联合学习系统

    zkDFL: An efficient and privacy-preserving decentralized federated learning with zero-knowledge proof

    [https://arxiv.org/abs/2312.04579](https://arxiv.org/abs/2312.04579)

    zkDFL是一种利用零知识证明实现高效且保护隐私的去中心化联合学习系统，通过将大规模模型参数与可信服务器共享，并使用区块链技术进行算法管理和验证，实现数据隐私保护和完整性。

    

    联合学习已广泛应用于各个领域，传统的中心化联合学习系统存在严重问题。为了解决这些问题，最近引入了去中心化联合学习系统。通过使用区块链技术，它们试图提高完整性和效率。然而，隐私保护仍然是这些系统中未解决的问题。为了解决这个问题，并扩展基于区块链的计算，我们提出了一种基于零知识证明的聚合器（zkDFL）。这使得客户端可以将其大规模模型参数与可信的中心化服务器共享，同时不向其他客户端透露其个体数据。我们利用区块链技术通过智能合约管理聚合算法。服务器执行零知识证明算法，向客户端证明聚合是根据接受的算法执行的。此外，服务器可以证明来自客户端的所有输入都是合法的。

    Federated learning (FL) has been widely adopted in various fields of study and business. Traditional centralized FL systems suffer from serious issues. To address these concerns, decentralized federated learning (DFL) systems have been introduced in recent years. With the help of blockchains, they attempt to achieve more integrity and efficiency. However, privacy preservation remains an uncovered aspect of these systems. To tackle this, as well as to scale the blockchain-based computations, we propose a zero-knowledge proof (ZKP)-based aggregator (zkDFL). This allows clients to share their large-scale model parameters with a trusted centralized server without revealing their individual data to other clients. We utilize blockchain technology to manage the aggregation algorithm via smart contracts. The server performs a ZKP algorithm to prove to the clients that the aggregation is done according to the accepted algorithm. Additionally, the server can prove that all inputs from clients ha
    
[^213]: 强化关注力中最短的支柱：增强大型语言模型的上下文意识，以实现有效的工具使用

    Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use

    [https://arxiv.org/abs/2312.04455](https://arxiv.org/abs/2312.04455)

    本文证明了大型语言模型中关注分配的波形模式对其在需要高度上下文意识的任务中的性能有显著影响。我们提出了一种名为“Attention Buckets”的推理方法，通过多个并行过程和不同的旋转位置嵌入角度，增强了模型对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    

    在本文中，我们证明了大型语言模型(LLMs)中关注分配中的内在波形模式显著影响它们在需要高度上下文意识的任务中的性能，例如利用LLMs进行工具使用。具体而言，当关键信息在上下文中位于关注波形的低谷区域时，模型可能会忽视该信息，导致性能下降。为了解决这个问题，我们提出了一种名为“Attention Buckets”的新型推理方法。它允许LLMs通过多个并行过程处理输入。每个过程使用不同的基准角度进行旋转位置嵌入，从而创建出一个独特的关注波形。通过用一个过程的关注低谷补偿另一个过程的关注高峰，我们的方法增强了LLM对不同上下文位置的意识，从而减轻了忽视关键信息的风险。

    In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
    
[^214]: 引起多义性的原因是什么？通过偶然因素的混合选择性的替代起源故事

    What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes

    [https://arxiv.org/abs/2312.03096](https://arxiv.org/abs/2312.03096)

    这项工作提出了多义性的替代起源故事，称为偶然多义性，即使有足够的神经元来表示所有特征，也可能产生多义性。

    

    多义性神经元——激活一组不相关特征的神经元——被视为解释任务优化深度网络的显著障碍，对AI安全性产生影响。传统的多义性起源故事是数据包含的“特征”多于神经元，因此学习执行任务迫使网络将多个不相关特征分配给同一个神经元，危及我们理解网络内部处理的能力。在这项工作中，我们提出了多义性的第二个且非互斥的替代起源故事。我们展示了即使有足够的神经元来表示数据中的所有特征，偶然多义性也可能产生，这是一种我们称之为“偶然多义性”的现象。通过理论和实验证明，偶然多义性可以由多种原因引起，包括正则化和神经噪音；这种偶然多义性发生是因为随机的因素。

    Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
    
[^215]: 逆向强化学习比标准强化学习更困难吗？一个理论的观点

    Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective

    [https://arxiv.org/abs/2312.00054](https://arxiv.org/abs/2312.00054)

    逆向强化学习是从专家策略示范中学习奖励函数的问题，本文提出了在标准离线和在线设置下用多项式样本和运行时间进行高效逆向强化学习的结果线索，并提供了几乎最优的样本复杂性的下界。

    

    逆向强化学习（IRL）是从专家策略的示范中学习奖励函数的问题，在开发智能系统中起着关键作用。尽管在应用中广泛使用，但与标准强化学习相比，IRL的理论理解存在独特的挑战，且发展相对较少。本文首次提出了使用多项式样本和运行时间在标准离线和在线设置下进行高效IRL的结果线索。我们的算法和分析巧妙地采用了离线强化学习中常用的悲观原则，并在比现有工作中考虑的更强的度量标准下实现了IRL的保证。我们提供了下界，表明我们的样本复杂性几乎是最优的。

    Inverse Reinforcement Learning (IRL) -- the problem of learning reward functions from demonstrations of an \emph{expert policy} -- plays a critical role in developing intelligent systems. While widely used in applications, theoretical understandings of IRL present unique challenges and remain less developed compared with standard RL. For example, it remains open how to do IRL efficiently in standard \emph{offline} settings with pre-collected data, where states are obtained from a \emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy.   This paper provides the first line of results for efficient IRL in vanilla offline and online settings using polynomial samples and runtime. Our algorithms and analyses seamlessly adapt the pessimism principle commonly used in offline RL, and achieve IRL guarantees in stronger metrics than considered in existing work. We provide lower bounds showing that our sample complexities are nearly optimal
    
[^216]: 深度神经网络加速器中的故障定位监测器部署

    Monitor Placement for Fault Localization in Deep Neural Network Accelerators

    [https://arxiv.org/abs/2311.16594](https://arxiv.org/abs/2311.16594)

    本研究提出了一种在并行阵列中优化硬件监测器部署的解决方案，以提高深度神经网络加速器的可靠性。通过证明和推导，我们确定了定位单个故障的PE所需的监测器数量，并解决了NP困难的监测器部署方案优化问题。然后，我们提出了一种启发式方法来平衡效益与开销。

    

    并行性和高效数据重用使得并行阵列系统成为深度神经网络（DNN）加速器的一个重要选择。提高DNN加速器的可靠性至关重要，因为硬件故障可能会降低DNN推理的准确性。由于并行阵列利用大量处理元件（PE）进行并行处理，但当一个PE故障时，错误会传播并影响下游PE的输出。由于PE的数量较大，使用基于硬件的运行时监测的成本是不可行的。我们提出了一种优化并行阵列中硬件监测器部署的解决方案。我们首先证明了在定位单个故障的PE所需的监测器数量为$2N-1$，并导出了监测器部署方案。我们还展示了第二个优化问题的监测器部署方案，该方案通过给定数量的监测器最小化候选故障PE集合，该问题是NP困难的。因此，我们提出一种启发式方法来平衡实现硬件监测的效益与开销。

    Systolic arrays are a prominent choice for deep neural network (DNN) accelerators because they offer parallelism and efficient data reuse. Improving the reliability of DNN accelerators is crucial as hardware faults can degrade the accuracy of DNN inferencing. Systolic arrays make use of a large number of processing elements (PEs) for parallel processing, but when one PE is faulty, the error propagates and affects the outcomes of downstream PEs. Due to the large number of PEs, the cost associated with implementing hardware-based runtime monitoring of every single PE is infeasible. We present a solution to optimize the placement of hardware monitors within systolic arrays. We first prove that $2N-1$ monitors are needed to localize a single faulty PE and we also derive the monitor placement. We show that a second placement optimization problem, which minimizes the set of candidate faulty PEs for a given number of monitors, is NP-hard. Therefore, we propose a heuristic approach to balance 
    
[^217]: 大型语言模型通过语言特征对齐可以增强说服力

    Large language models can enhance persuasion through linguistic feature alignment

    [https://arxiv.org/abs/2311.16466](https://arxiv.org/abs/2311.16466)

    本研究调查了大型语言模型对人类沟通的影响，使用了消费者金融投诉数据，并发现大型语言模型的使用可能增强了一整套语言特征，提高了信息说服力。

    

    尽管大型语言模型 (LLMs)正在重新塑造人类生活的各个方面，但我们对它们的影响的理解仍然有些受限。本文研究了LLMs对人类沟通的影响，使用了消费者金融投诉的数据。通过对消费者金融保护局 (CFPB) 收集的超过820,000个投诉进行AI检测，我们发现在ChatGPT发布后不久，LLMs的使用可能性急剧增加。此外，LLMs的使用可能性与信息说服力（即从金融公司获得救济的可能性增加）呈正相关。计算语言分析表明，这种正相关可能是由LLMs增强了各种语言特征所解释的。根据这些观察研究的结果，我们假设LLMs的使用可能增强了一整套语言特征，提高了对具有不同语言背景的接收者的信息说服力。

    Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, using data on consumer complaints in the financial industry. By employing an AI detection tool on more than 820K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find a sharp increase in the likely use of LLMs shortly after the release of ChatGPT. Moreover, the likely LLM usage was positively correlated with message persuasiveness (i.e., increased likelihood of obtaining relief from financial firms). Computational linguistic analyses suggest that the positive correlation may be explained by LLMs' enhancement of various linguistic features. Based on the results of these observational studies, we hypothesize that LLM usage may enhance a comprehensive set of linguistic features, increasing message persuasiveness to receivers with heterogeneous ling
    
[^218]: 利用扩散扰动来衡量计算机视觉中的公平性

    Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision

    [https://arxiv.org/abs/2311.15108](https://arxiv.org/abs/2311.15108)

    本研究利用扩散模型创建了一个以人口统计特征平衡的数据集，用于衡量计算机视觉模型的公平性。通过测试多个模型的职业分类任务，发现使用非高加索标签生成的图像存在明显的职业误分类率高于使用高加索标签生成的图像，且几个误分类表明存在种族偏见。

    

    已知计算机视觉模型可能对历史上被边缘化的族群（如有色人种）产生有害偏见，导致潜在的不公平对待。然而，目前仍缺乏以人口统计特征为平衡基准的数据集，用于评估这些模型的下游公平性。在这项工作中，我们展示了扩散模型可以用来创建这样的数据集。首先，我们使用扩散模型生成一组表示不同职业的大量图像。随后，使用修复技术编辑每个图像生成多个变体，其中每个变体对应不同的感知种族。使用这个数据集，我们在一个多类职业分类任务上对多个视觉语言模型进行了基准测试。我们发现，用非高加索标签生成的图像的职业误分类率明显高于用高加索标签生成的图像，并且几个误分类示意存在种族偏见。

    Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measu
    
[^219]: 从被毒害的人类反馈中构建的通用越狱后门

    Universal Jailbreak Backdoors from Poisoned Human Feedback

    [https://arxiv.org/abs/2311.14455](https://arxiv.org/abs/2311.14455)

    本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。

    

    强化学习从人类反馈中（RLHF）用于对齐大型语言模型，以产生有用且无害的回应。然而，先前的研究显示，这些模型可以通过找到使模型恢复到未对齐行为的对抗提示来进行越狱。在本文中，我们考虑了一个新的威胁，攻击者通过毒害RLHF训练数据将“越狱后门”嵌入模型中。该后门将一个触发词嵌入模型中，类似于通用的“sudo命令”：在任何提示中添加触发词将使模型产生有害的回应，无需搜索对抗提示。通用越狱后门比先前研究的语言模型后门更强大，我们发现使用常见的后门攻击技术要困难得多。我们探究了RLHF中的设计决策对其所声称的鲁棒性的贡献，并发布一组被毒害模型的基准，以促进对通用越狱后门的未来研究。

    Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
    
[^220]: 发现有效的土地利用规划政策

    Discovering Effective Policies for Land-Use Planning

    [https://arxiv.org/abs/2311.12304](https://arxiv.org/abs/2311.12304)

    通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。

    

    土地被分配给不同的用途，如森林、城市区域和农业，对陆地碳平衡和气候变化有重大影响。基于可用的土地利用变化的历史数据和相关的碳排放和吸收的模拟，可以学习到一个代理模型，从而能够高效评估决策者可选择的不同选项。然后可以使用进化搜索过程来发现特定位置的有效土地利用政策。该系统构建在Project Resilience平台上，并使用Land-Use Harmonization数据集LUH2和簿记模型BLUE进行评估。它生成可定制到不同位置的碳影响和土地利用变化量的帕累托前沿，从而为土地利用规划提供了一个潜在有用的工具。

    How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
    
[^221]: 高效强化学习在部分可观察性下的应用

    Efficient Reinforcement Learning from Partial Observability

    [https://arxiv.org/abs/2311.12244](https://arxiv.org/abs/2311.12244)

    该论文提出了一种基于表示的方法，用于从部分观测中进行有效的强化学习。该方法能够处理部分可观测性带来的计算和统计挑战，并在各种基准测试中展现出优于先进算法的性能。

    

    在大多数实际应用中，状态信息只能部分观测到，这破坏了马尔科夫决策过程的假设，导致将观测与状态相混淆的算法表现不佳。而部分可观测马尔科夫决策过程（POMDP）提供了一个允许在学习、探索和规划中考虑部分可观测性的通用框架，但也带来了显著的计算和统计挑战。为解决这些困难，我们提出了一个基于表示的视角，提供了一个统一的框架和可行的算法方法，用于从部分观测中进行实际的强化学习。我们提供了理论分析来证明所提出算法的统计效率，并经验性地证明了在各种基准测试中，所提出的算法在部分观测下能够超越最先进性能，推动了可靠的强化学习。

    In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
    
[^222]: 关于衡量自然语言解释的忠诚度或自一致性

    On Measuring Faithfulness or Self-consistency of Natural Language Explanations

    [https://arxiv.org/abs/2311.07466](https://arxiv.org/abs/2311.07466)

    本文论述了衡量自然语言解释的忠诚度或自一致性的问题。我们提出了自一致性测试来评估解释的输出级别的一致性。我们通过构建比较一致性测试库，并引入了新的自一致性度量CC-SHAP来支持我们的观点。

    

    大型语言模型（LLMs）可以通过事后或思维链（CoT）解释其预测。但是，LLM可能会编造听起来合理但不忠实于其基本推理的解释。最近的工作设计了旨在判断事后或CoT解释忠实度的测试。在这项工作中，我们认为这些忠实度测试不是衡量模型内部工作的忠实度，而是衡量其输出级别的自一致性。我们的贡献有三个方面：i）我们在模型可解释性的背景下澄清了忠实度测试的地位，将其描述为自一致性测试。我们通过ii）构建了一个比较一致性的测试库，首次在11个开放式LLMs和5个任务的通用套件上比较了现有测试，包括iii）我们的新的自一致性度量CC-SHAP。CC-SHAP是LLM自一致性的细粒度度量（而不是测试）。它进行比较。

    Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
    
[^223]: 数据污染问题: 一种检测和估计大型语言模型中污染的工具

    Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models

    [https://arxiv.org/abs/2311.06233](https://arxiv.org/abs/2311.06233)

    这个工具使用数据污染问题（DCQ）的方法来检测和估计大型语言模型中的数据污染。在DCQ中，我们创建了每个数据集实例的扰动版本，并让语言模型从中选择原始实例，通过词级扰动来区分选项。这种方法利用了语言模型在预训练阶段暴露于原始实例时的固有特性。

    

    我们提出了数据污染问题（DCQ），这是一种简单而有效的方法，用于检测大型语言模型（LLM）中的数据污染并估计其数量。具体而言，我们将数据污染检测视为一系列的多项选择问题，并设计了一种测验形式，其中创建了每个数据集实例的三个扰动版本。这些变化仅包括词级扰动。生成的扰动版本与原始实例一起形成DCQ中的选项，额外的选项适应了提供的选择都不正确的可能性。鉴于在选择之间唯一的区别信号是与原始实例的确切措辞相关，如果在预训练阶段已经接触到原始实例，语言模型当被要求从选项中识别原始实例时，倾向于选择原始实例--这是语言模型固有的特性。在使用GPT-4/3.5进行多个数据集的测试中，我们的结果完全缺少准确性。

    We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
    
[^224]: 从混乱到清晰：声明标准化以增强事实核查

    From Chaos to Clarity: Claim Normalization to Empower Fact-Checking

    [https://arxiv.org/abs/2310.14338](https://arxiv.org/abs/2310.14338)

    本研究提出了声明标准化任务，通过使用CACN模型利用思维链和声明检查来从复杂的社交媒体帖子中提取简化的声明，以加强事实核查。

    

    随着社交媒体的兴起，用户接触到许多误导性的声明。然而，这些帖子中固有的混杂噪声使得辨别需要验证的精确且显著的声明变得很具挑战性。从这些帖子中提取重要的声明是费时且困难的，然而这是一个未被充分探索的问题。在这里，我们旨在填补这个差距。我们引入了一个新颖的任务，称为声明标准化（ClaimNorm），旨在将复杂而嘈杂的社交媒体帖子分解为更直接和易于理解的形式，称为标准化声明。我们提出了CACN，一种开创性的方法，利用思维链和声明值得检查的估计来模拟人类推理过程，以理解复杂的声明。此外，我们利用大型语言模型的上下文学习能力来提供指导并改进声明标准化。为了评估我们所提出的模型的有效性，我们精心编制了一个全面的真实世界数据集。

    With the rise of social media, users are exposed to many misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the important claims from such posts is arduous and time-consuming, yet it is an underexplored problem. Here, we aim to bridge this gap. We introduce a novel task, Claim Normalization (aka ClaimNorm), which aims to decompose complex and noisy social media posts into more straightforward and understandable forms, termed normalized claims. We propose CACN, a pioneering approach that leverages chain-of-thought and claim check-worthiness estimation, mimicking human reasoning processes, to comprehend intricate claims. Moreover, we capitalize on the in-context learning capabilities of large language models to provide guidance and to improve claim normalization. To evaluate the effectiveness of our proposed model, we meticulously compile a comprehensive real-world 
    
[^225]: 使用机器学习和经典技术的统计推断：基于积累的局部效应（ALE）

    Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)

    [https://arxiv.org/abs/2310.09877](https://arxiv.org/abs/2310.09877)

    本研究通过引入创新的工具和技术，使用积累的局部效应（ALE）进行统计推断，在解决小数据集、直观特征和健壮推断方面取得了突破。这项工作推动了ALE及其应用的研究。

    

    积累的局部效应（ALE）是一种对黑盒机器学习（ML）算法结果进行全局解释的模型无关方法。使用ALE进行统计推断面临至少三个挑战：确保ALE分析的可靠性，尤其在小数据集的情况下；直观地表征变量在ML中的整体效应；以及从ML数据分析中进行健壮的推断。为此，我们引入了创新的工具和技术，使用ALE进行统计推断，建立了适应数据集大小的自助法置信区间，并引入了直观指示对结果变量和标准化尺度上的效应的ALE效应大小度量。此外，我们演示了如何使用这些工具绘制可靠的统计推断，反映了ALE熟练突出的灵活模式，实现了R中“ale”包中的实现。这项工作推动了关于ALE及其应用的讨论。

    Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
    
[^226]: MemGPT：朝向以语言模型为操作系统的方向迈进

    MemGPT: Towards LLMs as Operating Systems

    [https://arxiv.org/abs/2310.08560](https://arxiv.org/abs/2310.08560)

    MemGPT是一个以语言模型为操作系统的系统，通过虚拟上下文管理技术，实现了超出有限上下文窗口的上下文利用。在文档分析方面，MemGPT能够分析超出底层限制的大型文档。

    

    大型语言模型（LLMs）已经改变了人工智能领域，但受限于有限的上下文窗口，使其在延长对话和文档分析等任务中的效用有所限制。为了能够利用超出有限上下文窗口的上下文，我们提出了虚拟上下文管理技术，该技术借鉴了传统操作系统中的分层内存系统，通过在快速和慢速存储器之间移动数据来提供大内存资源的外观。借助这种技术，我们引入了MemGPT（Memory-GPT），一种智能地管理不同内存层级的系统，以在LLM的有限上下文窗口内提供扩展上下文，并利用中断来管理自身与用户之间的控制流。我们在两个领域评估了我们受操作系统启发的设计，这些领域中现代LLM的有限上下文窗口严重影响了它们的性能：文档分析，其中MemGPT能够分析超出底层限制的大型文档。

    Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying 
    
[^227]: 关于常识推理的知识图谱解释的可信性

    Faithful Knowledge Graph Explanations for Commonsense Reasoning

    [https://arxiv.org/abs/2310.04910](https://arxiv.org/abs/2310.04910)

    本论文提出了两个量化指标来衡量基于知识图谱的解释的可信性，并引入了一种新的训练方法来改善解释的可信度。实验结果表明该方法可以提高解释的一致性和保真度。

    

    融合语言模型(LMs)和知识图谱(KGs)已成为常识问答研究中的常见方法，但在这些模型中实现精确的思路链解释仍然是一个未解决的问题。当前基于知识图谱的解释技术的一个主要弱点是在评估过程中忽视了生成解释的可信性。为了弥补这一差距，我们提出并验证了两个量化指标 - 图一致性和图保真度 - 来衡量基于知识图谱的解释的可信性。我们引入一种新的训练方法Consistent GNN (CGNN)，该方法添加了一项一致性正则化项来改善解释的可信度。我们的分析表明，KG的预测经常偏离原始模型的预测。所提出的CGNN方法提高了一致性和保真度，展示了它产生更可信解释的潜力。我们的工作强调了明确评估解释可信性的重要性。

    While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
    
[^228]: 层次化多边汇运输用于网络对齐

    Hierarchical Multi-Marginal Optimal Transport for Network Alignment

    [https://arxiv.org/abs/2310.04470](https://arxiv.org/abs/2310.04470)

    层次化多边汇运输（HOT）框架通过融合Gromov-Wasserstein（FGW）重心将多个网络分解为对齐簇，并将FGW距离广义化到多边汇环境，实现多网络的联合对齐。实验证明，HOT相对于统计方法取得了显著的改进。

    

    跨网络找到节点对应关系，即多网络对齐，是在多个网络上进行联合学习的基本先决条件。尽管在两个网络上对齐方面取得了巨大的成功，但是由于解空间指数增长和高阶差异度量缺乏，多网络对齐的文献有限。为了填补这一空白，我们提出了一种层次化多边汇运输框架（HOT）用于多网络对齐。为了处理庞大的解空间，多个网络通过融合的Gromov-Wasserstein（FGW）重心被分解为较小的对齐簇。为了描绘多个网络之间的高阶关系，FGW距离被推广到多边汇环境，基于这个距离可以实现网络的联合对齐。进一步开发了一种快速的近端点方法，保证收敛到局部最优解。大量的实验证明，我们提出的HOT相对于统计方法取得了显著的改进。

    Finding node correspondence across networks, namely multi-network alignment, is an essential prerequisite for joint learning on multiple networks. Despite great success in aligning networks in pairs, the literature on multi-network alignment is sparse due to the exponentially growing solution space and lack of high-order discrepancy measures. To fill this gap, we propose a hierarchical multi-marginal optimal transport framework named HOT for multi-network alignment. To handle the large solution space, multiple networks are decomposed into smaller aligned clusters via the fused Gromov-Wasserstein (FGW) barycenter. To depict high-order relationships across multiple networks, the FGW distance is generalized to the multi-marginal setting, based on which networks can be aligned jointly. A fast proximal point method is further developed with guaranteed convergence to a local optimum. Extensive experiments and analysis show that our proposed HOT achieves significant improvements over the stat
    
[^229]: KDSTM: 使用知识蒸馏的神经半监督主题建模

    KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation

    [https://arxiv.org/abs/2307.01878](https://arxiv.org/abs/2307.01878)

    KDSTM是一种使用知识蒸馏的神经半监督主题建模方法，对于文本分类任务，在没有预训练嵌入且资源受限的情况下，能够提供高准确性、鲁棒性和效率。

    

    在文本分类任务中，微调预训练的语言模型（如BERT和GPT-3）可以获得竞争性的准确性；然而，这两种方法都需要在大型文本数据集上进行预训练。相比之下，一般的主题建模方法具有在不需要预训练的情况下分析文档并提取有意义的词汇模式的优势。为了利用主题建模在文本分类任务中的无监督的见解提取，我们开发了一种称为知识蒸馏半监督主题建模（KDSTM）的方法。KDSTM不需要预训练嵌入，只需要少量的标记文档，并且训练效率高，在资源受限的情况下非常理想。在多个数据集上，我们的方法在分类准确性、鲁棒性和效率方面都超过了现有的有监督主题建模方法，并且与最先进的弱监督文本分类方法相比达到了类似的性能。

    In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.
    
[^230]: 通过执行反馈使语言模型成为更好的工具学习者

    Making Language Models Better Tool Learners with Execution Feedback

    [https://arxiv.org/abs/2305.13068](https://arxiv.org/abs/2305.13068)

    这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。

    

    工具作为关键的界面，使人类能够理解和改变环境。随着基础模型的出现，AI系统可以利用工具扩展其能力并与真实世界互动。现有的工具学习方法包括监督微调和提示工程方法，通常使大型语言模型不加选择地利用工具，因为复杂任务往往超出了它们自身的能力。然而，为简单任务引入工具（模型本身可以轻松解决的任务），可能会无意间传播错误而不是提高性能。因此，研究问题是：我们能否教会语言模型何时以及如何使用工具？为满足这个需求，我们提出了Tool leaRning wIth exeCution fEedback (TRICE)，这是一个两阶段的端到端框架，使模型能够通过从工具执行中得到的反馈不断学习，从而学会何时以及如何有效地使用工具。

    Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
    
[^231]: 可扩展的分布式图上神经网络训练

    Scalable Neural Network Training over Distributed Graphs

    [https://arxiv.org/abs/2302.13053](https://arxiv.org/abs/2302.13053)

    RETEXO是第一个消除分布式图神经网络训练中通信瓶颈的框架，通过新的训练过程懒消息传递来改善网络通信效率。

    

    图神经网络（GNN）在涉及图结构数据的各种机器学习任务中发挥着重要作用，包括预测蛋白质结构和提供个性化推荐等。实际世界中的图数据往往需要分布式存储在许多计算机上，原因不仅是因为容量限制，还有数据所在地或隐私法律的要求。在这种设置中，网络通信成本很高，成为训练GNN的主要瓶颈。迄今为止，分布式GNN训练的优化主要针对数据级别的改进，例如缓存、网络感知划分和子采样等，这些方法适用于数据中心类似的设置，其中图数据对单个实体可访问且数据传输成本被忽略。我们提出了RETEXO，这是一种可以消除分布式GNN训练中严重通信瓶颈的首个框架，同时尊重任何给定的数据分区配置。关键是通过一种新的训练过程，即懒消息传递，重新排序了消息传递的顺序。

    Graph neural networks (GNNs) fuel diverse machine learning tasks involving graph-structured data, ranging from predicting protein structures to serving personalized recommendations. Real-world graph data must often be stored distributed across many machines not just because of capacity constraints, but because of compliance with data residency or privacy laws. In such setups, network communication is costly and becomes the main bottleneck to train GNNs. Optimizations for distributed GNN training have targeted data-level improvements so far -- via caching, network-aware partitioning, and sub-sampling -- that work for data center-like setups where graph data is accessible to a single entity and data transfer costs are ignored.   We present RETEXO, the first framework which eliminates the severe communication bottleneck in distributed GNN training while respecting any given data partitioning configuration. The key is a new training procedure, lazy message passing, that reorders the sequen
    
[^232]: 在非静态上下文驱动环境中的在线强化学习

    Online Reinforcement Learning in Non-Stationary Context-Driven Environments

    [https://arxiv.org/abs/2302.02182](https://arxiv.org/abs/2302.02182)

    提出了一种名为LCPO的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧经验进行锚定来解决强化学习中的灾难性遗忘问题。

    

    我们研究了在非静态环境中的在线强化学习，其中一个随时间变化的外生上下文过程影响着环境动态。在线强化学习在这样的环境中具有挑战性，因为存在“灾难性遗忘”现象。随着训练过程中的新经验增加，代理 tend to forget 先前的知识。以往的方法通常假设任务标签（这在实践中往往是不存在的）或者使用脱机策略学习方法，但这些方法存在不稳定性和性能差的问题。我们提出了一种名为 Locally Constrained Policy Optimization (LCPO) 的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧的经验进行锚定来解决灾难性遗忘问题。为了实现这种锚定，LCPO使用来自当前上下文分布之外的经验样本来局部约束策略优化。我们在Mujoco、经典控制和计算机系统环境中使用多种合成和真实上下文跟踪，评估了LCPO的性能，并发现它能够取得令人满意的结果。

    We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice) or use off-policy methods that suffer from instability and poor performance.   We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it o
    
[^233]: 基于检索的带自然语言监督的解缠表示学习

    Retrieval-based Disentangled Representation Learning with Natural Language Supervision

    [https://arxiv.org/abs/2212.07699](https://arxiv.org/abs/2212.07699)

    本研究提出了基于检索的带自然语言监督的解缠表示学习框架，利用自然语言作为数据变化的代理，通过词汇空间中的双编码器模型实现对数据内在特征的解缠表示学习。

    

    解缠表示学习仍然具有挑战性，因为数据中的基本变化因素并不存在。真实世界数据的固有复杂性使得在有限的因素集中穷尽地列举和概括所有变化是不可行的。然而，值得注意的是，大多数真实世界数据都有语言等价物，通常以文本描述的形式存在。这些语言对应物可以代表数据，并轻松地分解为不同的标记。基于此，我们提出了单词表解缠检索（VDR），这是一个基于检索的框架，利用自然语言作为潜在数据变化的代理，以推动解缠表示学习。我们的方法使用双编码器模型在词汇空间中表示数据和自然语言，使模型能够通过其自然语言对应物区分捕捉数据内在特征的维度，从而促进解缠表示学习。

    Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it is worth noting that most real-world data have linguistic equivalents, typically in the form of textual descriptions. These linguistic counterparts can represent the data and effortlessly decomposed into distinct tokens. In light of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based framework that harnesses natural language as proxies of the underlying data variation to drive disentangled representation learning. Our approach employ a bi-encoder model to represent both data and natural language in a vocabulary space, enabling the model to distinguish dimensions that capture intrinsic characteristics within data through its natural language counterpart, thus facilitat
    
[^234]: 深度学习的动态潜变量分离

    Dynamic Latent Separation for Deep Learning

    [https://arxiv.org/abs/2210.03728](https://arxiv.org/abs/2210.03728)

    本研究提出了动态潜变量分离的方法，可以在复杂数据中学习表达性强的潜变量，提升输出的多样性。该方法受原子物理学启发，通过学习每个数据样本的结构来解释各个子组件的重要性。实验证明该方法在不同分类和生成问题中提升了模型的性能。

    

    机器学习中的一个核心问题是以灵活和可解释的方式学习用于复杂数据模型预测的表达性潜变量，这些数据包含多个子组件。我们开发了一种方法，改进了表达性，提供了部分解释，并且不限于特定的应用。关键思想是在潜空间中动态地分离数据样本，从而增强输出的多样性。我们的动态潜变量分离方法受到原子物理学的启发，依赖于每个数据样本共同学习的结构，这也揭示出了每个子组件在区分数据样本中的重要性。这种方法，原子建模，不需要对潜空间进行监督，并且允许我们学习额外的部分可解释表示，除了模型的原始目标。实验证明，该算法还提高了各种分类和生成问题中小到大规模模型的性能。

    A core problem in machine learning is to learn expressive latent variables for model prediction on complex data that involves multiple sub-components in a flexible and interpretable fashion. Here, we develop an approach that improves expressiveness, provides partial interpretation, and is not restricted to specific applications. The key idea is to dynamically distance data samples in the latent space and thus enhance the output diversity. Our dynamic latent separation method, inspired by atomic physics, relies on the jointly learned structures of each data sample, which also reveal the importance of each sub-component for distinguishing data samples. This approach, atom modeling, requires no supervision of the latent space and allows us to learn extra partially interpretable representations besides the original goal of a model. We empirically demonstrate that the algorithm also enhances the performance of small to larger-scale models in various classification and generation problems.
    
[^235]: GBSVM: 用粗粒-球作为输入的支持向量机

    GBSVM: Granular-ball Support Vector Machine

    [https://arxiv.org/abs/2210.03120](https://arxiv.org/abs/2210.03120)

    GBSVM是一种使用粗粒-球作为输入的支持向量机，修复了现有模型的错误并推导出了对偶模型，提出了粒子群优化算法和顺序最小优化算法来解决问题，具有良好的稳健性和效率。

    

    GBSVM（Granular-ball Support Vector Machine）是通过使用粗粒-球的粒度作为输入来构建分类器的重要尝试，而不是单个数据点。这是第一个输入不包含点的分类器。然而，现有模型存在一些错误，并且其对偶模型尚未被推导出。因此，当前算法无法实现或应用。为了解决这些问题，本文修复了现有GBSVM原始模型的错误，并推导出了其对偶模型。此外，设计了粒子群优化算法来解决对偶模型问题。还设计了顺序最小优化算法来解决对偶模型。该解决方案比基于粒子群优化的版本更快、更稳定。在UCI基准数据集上的实验结果表明，GBSVM具有良好的稳健性和效率。所有代码已在开源库http://上发布。

    GBSVM (Granular-ball Support Vector Machine) is a significant attempt to construct a classifier using the coarse-to-fine granularity of a granular-ball as input, rather than a single data point. It is the first classifier whose input contains no points. However, the existing model has some errors, and its dual model has not been derived. As a result, the current algorithm cannot be implemented or applied. To address these problems, this paper has fixed the errors of the original model of the existing GBSVM, and derived its dual model. Furthermore, a particle swarm optimization algorithm is designed to solve the dual model. The sequential minimal optimization algorithm is also carefully designed to solve the dual model. The solution is faster and more stable than the particle swarm optimization based version. The experimental results on the UCI benchmark datasets demonstrate that GBSVM has good robustness and efficiency. All codes have been released in the open source library at http://
    
[^236]: 不平衡患者亚群的电解质补充的组合Q学习

    Compositional Q-learning for electrolyte repletion with imbalanced patient sub-populations

    [https://arxiv.org/abs/2110.02879](https://arxiv.org/abs/2110.02879)

    组合Q学习方法用于解决医疗环境中存在异质性治疗反应的问题，通过使用复合任务结构和分离的模块化Q值函数，能够更有效地进行决策。

    

    强化学习（RL）是解决顺序决策任务的有效框架。然而，在医疗环境中应用RL方法具有挑战性，部分原因是由于患者的治疗反应的异质性。一些患者可以使用标准方案进行治疗，而其他患者，如慢性疾病患者，需要个性化的治疗计划。传统的RL方法通常无法考虑到这种异质性，因为它们假设所有患者对治疗的反应是相同的（即，转移动力学是共享的）。我们引入了组合Fitted $Q$-迭代（CFQI），它使用复合任务结构来表示医疗环境中的异质性治疗反应。复合任务由相同任务的几个变体组成，每个变体的难度逐渐增加；解决较简单的变体可以实现更高效地解决更难的变体。CFQI使用一个复合$Q$值函数，其中为每个任务模块单独表示。

    Reinforcement learning (RL) is an effective framework for solving sequential decision-making tasks. However, applying RL methods in medical care settings is challenging in part due to heterogeneity in treatment response among patients. Some patients can be treated with standard protocols whereas others, such as those with chronic diseases, need personalized treatment planning. Traditional RL methods often fail to account for this heterogeneity, because they assume that all patients respond to the treatment in the same way (i.e., transition dynamics are shared). We introduce Compositional Fitted $Q$-iteration (CFQI), which uses a compositional task structure to represent heterogeneous treatment responses in medical care settings. A compositional task consists of several variations of the same task, each progressing in difficulty; solving simpler variants of the task can enable efficient solving of harder variants. CFQI uses a compositional $Q$-value function with separate modules for ea
    
[^237]: 深度学习的创造力：概念化与评估

    Creativity of Deep Learning: Conceptualization and Assessment

    [https://arxiv.org/abs/2012.02282](https://arxiv.org/abs/2012.02282)

    这篇论文探讨了深度学习在创意领域的应用，并使用计算创造力的视角对其进行了概念化和评估。研究发现，尽管深度学习可以产生高质量的结果，但其创新性受到限制，同时也存在内部问题表达无法更改以及在不同领域之间建立联系的缺乏能力的问题。

    

    虽然深度学习（DL）在自动化简单任务方面的潜力已经得到了广泛探索，但最近的研究开始探索使用深度学习进行创造性设计，无论是完整的产品创作还是支持人类在创作过程中的角色。本文利用计算创造力的见解，概念化和评估了文献综述中确定的创造性领域中当前应用生成式深度学习的情况。我们强调了当前系统与不同模型人类创造力的相似之处以及它们的不足之处。虽然深度学习可以产生高质量的图像等高价值结果，但其新颖性通常受到多种限制，例如受到训练数据定义的概念空间的限制。当前的DL方法也不允许对内部问题表达进行更改，并且它们缺乏在高度不同的领域之间建立联系的能力，这两点都被认为是主要的推动力。

    While the potential of deep learning (DL) for automating simple tasks is already well explored, recent research has started investigating the use of deep learning for creative design, both for complete artifact creation and supporting humans in the creation process. In this paper, we use insights from computational creativity to conceptualize and assess current applications of generative deep learning in creative domains identified in a literature review. We highlight parallels between current systems and different models of human creativity as well as their shortcomings. While deep learning yields results of high value, such as high-quality images, their novelty is typically limited due to multiple reasons such as being tied to a conceptual space defined by training data. Current DL methods also do not allow for changes in the internal problem representation, and they lack the capability to identify connections across highly different domains, both of which are seen as major drivers o
    
[^238]: ACCESS：用于自动修复网页无障碍违规的提示工程

    ACCESS: Prompt Engineering for Automated Web Accessibility Violation Corrections. (arXiv:2401.16450v1 [cs.HC])

    [http://arxiv.org/abs/2401.16450](http://arxiv.org/abs/2401.16450)

    本研究提出了一种用于修复网页无障碍违规的新方法，通过实时修改文档对象模型(DOM)和利用大型语言模型(LLMs)以及提示工程技术，解决了自动修复无障碍错误的问题。

    

    随着对包容性和用户友好技术的需求不断增加，网页无障碍对于确保残障人士(包括视觉、听觉、认知或运动障碍)平等获取在线内容至关重要。尽管存在诸如Web内容无障碍指南(WCAG)和Web无障碍倡议(W3C)等无障碍指导方针和标准，但超过90%的网站仍无法满足必要的无障碍要求。对于残障人士来说，需要一种工具来自动修复网页的无障碍错误。虽然研究已经证明了发现和定位无障碍错误的方法，但没有研究专注于有效修复此类违规行为。本文提出了一种通过实时修改文档对象模型(DOM)来修复网页无障碍违规的新方法，该方法利用无障碍错误信息、大型语言模型(LLMs)和提示工程技术。

    With the increasing need for inclusive and user-friendly technology, web accessibility is crucial to ensuring equal access to online content for individuals with disabilities, including visual, auditory, cognitive, or motor impairments. Despite the existence of accessibility guidelines and standards such as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility Initiative (W3C), over 90\% of websites still fail to meet the necessary accessibility requirements. For web users with disabilities, there exists a need for a tool to automatically fix web page accessibility errors. While research has demonstrated methods to find and target accessibility errors, no research has focused on effectively correcting such violations. This paper presents a novel approach to correcting accessibility violations on the web by modifying the document object model (DOM) in real time with foundation models. Leveraging accessibility error information, large language models (LLMs), and prompt en
    
[^239]: 在科学研究中建立生成AI的伦理指南

    Building ethical guidelines for generative AI in scientific research. (arXiv:2401.15284v1 [cs.CY])

    [http://arxiv.org/abs/2401.15284](http://arxiv.org/abs/2401.15284)

    本文提出了一个初步的框架，通过五个关键主题的分析和缓解策略来建立科学研究中生成AI的伦理指南。全球共识、专业培训和合理的执行对于促进AI的益处和维护研究诚信至关重要。

    

    生成人工智能工具（如大型语言模型）正在迅速改变学术研究和实际应用。然而，关于科学中生成AI的伦理指南的讨论仍然零散，强调了协商一致性标准的紧迫性。本文通过对五个关键主题的分析和缓解策略的开发，提供了一个初步的框架：了解模型在真实性和偏见方面的局限性；尊重隐私、机密和版权；在融入模型输出时避免抄袭和违反政策；确保应用带来总体利益；以及透明、可复制地使用人工智能。通过列举常见场景来展示潜在的伦理违规行为。我们认为，全球共识以及专业培训和合理的执行是促进AI的益处并维护研究诚信的关键。

    Generative artificial intelligence tools like large language models are rapidly transforming academic research and real world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting the benefits of AI while safeguarding research integrity.
    
[^240]: PROXYQA：一种用于评估大型语言模型长篇文本生成的替代框架

    PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])

    [http://arxiv.org/abs/2401.15042](http://arxiv.org/abs/2401.15042)

    PROXYQA是一个用于评估大型语言模型长篇文本生成的替代框架，通过生成详尽的内容，并利用评估器和生成内容作为背景环境，根据评估器回答代理问题的表现来评估生成内容的质量。

    

    大型语言模型（LLM）在长篇文本理解任务中取得了显著的成功。然而，它们生成长篇内容（如报告和文章）的能力尚未得到充分探索。当前的基准不足以充分评估LLMs生成信息丰富且全面的内容，因此需要一种更严格的评估方法。在本研究中，我们介绍了一种名为\textsc{ProxyQA}的框架，用于评估长篇文本生成，包括深入人工策划的涵盖多个领域的“元问题”。每个元问题都包含相应的带注释答案的“代理问题”。LLMs被要求根据这些元问题生成详尽的内容。利用评估器并将生成的内容作为背景环境，\textsc{ProxyQA}根据评估器回答“代理问题”的表现评估生成内容的质量。我们检验了多个LLMs，重点关注了...

    Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
    
[^241]: 跨空间自适应滤波器：集成图拓扑和节点属性以减轻过度平滑问题

    Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])

    [http://arxiv.org/abs/2401.14876](http://arxiv.org/abs/2401.14876)

    本论文提出了一种跨空间自适应滤波器（CSF），可以从图拓扑和节点属性空间中提取自适应频率信息，以减轻图卷积网络（GCN）的过度平滑问题。

    

    传统的图卷积网络（GCN）使用低通滤波器从图拓扑中提取低频信号，但当GCN深度增加时可能导致过度平滑问题。为解决这个问题，已经提出了各种方法通过引入从图拓扑中提取的额外滤波器（如高通滤波器）来创建自适应滤波器。然而，这些方法严重依赖拓扑信息，并忽视了节点属性空间，这严重牺牲了深层GCN的表达能力，特别是在处理非同配图时。本文提出了一种跨空间自适应滤波器，称为CSF，能够从拓扑和属性空间中提取自适应频率信息。具体而言，我们首先推导出了一个定制的基于属性的高通滤波器，可以从理论上解释为半监督核岭回归的最小化器。然后，我们将基于拓扑的低通滤波器视为Mercer's核函数。

    The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the 
    
[^242]: 通过上下文感知个性化细化，增强长期对话中的常识增强性内存构建和管理

    Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])

    [http://arxiv.org/abs/2401.14215](http://arxiv.org/abs/2401.14215)

    本文提出了一个旨在解决长期对话中角色句子不具信息性的问题的框架，通过利用常识增强的角色扩展，并设计策略将相互矛盾的角色转化为包含丰富说话者信息的句子，以提高回应生成质量。

    

    在长期对话中，记忆和利用说话者的角色是生成回应的常见做法。然而，人工编写的数据集通常提供无信息的角色句子，这妨碍了回应质量。本文提出了一个新颖的框架，利用常识增强的角色扩展来解决长期对话中的这些问题。以前的工作侧重于不产生与其他角色相矛盾的角色，我们侧重于根据设计的策略，将相互矛盾的角色转化为包含丰富说话者信息的句子，以此来细化它们的上下文背景。作为多会话情境中角色扩展的先驱，我们的框架通过类人个性细化促进了更好的回应生成。

    Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
    
[^243]: 一种简单的黑盒方法用于越狱攻击

    All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])

    [http://arxiv.org/abs/2401.09798](http://arxiv.org/abs/2401.09798)

    本研究提出了一种简单的黑盒方法，用于生成越狱攻击提示，克服了现有方法的复杂性和计算成本的限制。该方法通过使用语言模型自身，将有害提示重写为非有害表达，实现了超过80%的攻击成功率，并且即使模型更新，效果仍然有效。

    

    像ChatGPT这样的大型语言模型面临着“越狱”挑战，即规避保障措施以产生不符合伦理的提示。本研究引入了一种简单的黑盒方法，有效地生成越狱提示，克服了现有方法的高复杂性和计算成本的限制。该方法通过使用目标语言模型自身，迭代地将有害提示重写为非有害表达，基于假设认为语言模型可以直接生成规避保障的表达。通过在ChatGPT（GPT-3.5和GPT-4）和Gemini-Pro上进行实验证明，该方法在平均5次迭代内实现了超过80%的攻击成功率，并且即使模型更新，效果仍然有效。生成的越狱提示自然而简练，表明它们较不易被检测。结果表明，创建有效的越狱提示比先前研究认为的要简单，并且黑盒越狱攻击构成了一个重要的挑战。

    Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
    
[^244]: 学习捷径：关于语言模型中自然语言理解误导性承诺的论文

    Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])

    [http://arxiv.org/abs/2401.09615](http://arxiv.org/abs/2401.09615)

    该论文调查了大型语言模型在自然语言理解任务中使用捷径学习的现象，强调了这种现象对语言模型评估的影响，并呼吁加大对捷径学习的研究力度以提升语言模型的鲁棒性和实际场景中的自然语言理解评估标准。

    

    大型语言模型（LLMs）的出现在自然语言处理领域实现了显著的性能提升。然而，最近的研究发现，LLMs在执行任务时常常采用捷径，导致在决策规则上缺乏泛化能力，从而在性能上产生了一种错觉。这一现象在准确评估LLMs的自然语言理解能力上带来了挑战。本文对该领域的相关研究进行了简洁的概述，并提出了在评估语言模型，尤其是自然语言理解任务中使用捷径学习的影响的观点。本文呼吁加大对捷径学习的深入理解的研究力度，为开发更强大的语言模型和提高真实场景下自然语言理解评估的标准作出贡献。

    The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.
    
[^245]: 为渐进式训练语言模型准备课程的方法

    Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])

    [http://arxiv.org/abs/2401.09192](http://arxiv.org/abs/2401.09192)

    提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。

    

    Transformers在人工智能领域的迅速发展带来了资源消耗和温室气体排放的增加，这是由于模型规模的增长。先前的研究表明使用预训练的小模型可以提高训练效率，但这种方法对于新的模型结构可能不适用。另一方面，从头开始训练可能很慢，并且渐进堆叠层往往无法实现显著的加速。为了解决这些挑战，我们提出了一种名为Apollo的新方法，它通过在低层训练期间学习高层功能来准备膨胀操作的课程。我们的方法涉及低值优先采样(LVPS)来训练不同深度，并引入权重共享以促进高效扩展。我们还介绍了一种插值方法来实现稳定的模型深度扩展。实验证明，Apollo实现了最先进的加速比率，甚至……

    The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
    
[^246]: 通过迭代组合问题来增强数学问题求解

    Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])

    [http://arxiv.org/abs/2401.09003](http://arxiv.org/abs/2401.09003)

    本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。

    

    尽管在改善大型语言模型(LLMs)的数学推理能力方面取得了一定进展，但在不使用外部工具的情况下解决竞赛级数学问题仍然对开源LLMs具有挑战性。在这项工作中，我们介绍了MMIQC数据集，这是一个混合处理的网络数据和合成问题-响应对的混合数据集，以提供基础模型更好的数学推理能力。通过在MMIQC上对Mistral-7B(arXiv:2310.06825)进行微调获得的模型Mistral-7B-MMIQC，在MATH(arXiv:2103.03874)上达到了36.0%的准确率，比之前(model size $\sim$7B)的最佳结果高出5.8%。我们的实验还表明，改进的一个重要部分归功于我们的新颖增强方法IQC(迭代组合问题)，其中我们迭代地要求LLM从给定的种子问题中组合新问题，并从另一个LLM中进行拒绝抽样。MMIQC现已在https://huggingface.co/datasets/Vivacem/MMIQC上发布。

    Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
    
[^247]: InfiAgent-DABench: 在数据分析任务中评估代理的基准测试

    InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])

    [http://arxiv.org/abs/2401.05507](http://arxiv.org/abs/2401.05507)

    InfiAgent-DABench是第一个评估基于LLM的代理在数据分析任务中的基准测试，包括DAEval数据集和代理框架。对23个最先进的LLMs进行的基准测试揭示了当前数据分析任务中的挑战。

    

    本文介绍了"InfiAgent-DABench"，这是第一个专门设计用于评估基于LLM的代理在数据分析任务中的基准测试。该基准测试包含DAEval，这是一个由55个CSV文件衍生出的311个数据分析问题的数据集，以及一个评估LLMs作为数据分析代理的代理框架。我们采用了一种格式提示技术，确保问题是闭合形式的，可以自动评估。我们对23个最先进的LLMs进行了广泛的基准测试，揭示了数据分析任务中当前遇到的挑战。此外，我们还开发了DAAgent，这是一个在指令调优数据集上训练的专门代理。InfiAgent-DABench的评估数据集和工具包已经发布在https://github.com/InfiAgent/InfiAgent上。

    In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
    
[^248]: 参数高效稀疏制作：从密集型到专家混合式用于通用任务的指令调整

    Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])

    [http://arxiv.org/abs/2401.02731](http://arxiv.org/abs/2401.02731)

    本文提出了一种参数高效稀疏制作的方法，它使用专家混合式架构将密集模型转换为稀疏模型，以实现在模型容量有限的情况下进行指令调整和泛化能力增强。

    

    大型语言模型(LLMs)在通用自然语言处理(NLP)任务中展示出相当的熟练程度。指令调整作为一种成功的范例，增强了LLMs遵循自然语言指令并在各种任务中展现出强大的泛化能力。然而，由于模型容量限制，这些模型在多个任务中经常遇到性能限制。在指令调整阶段扩展模型容量面临着巨大的挑战。为了解决这个问题，我们引入了一种新颖的方法，参数高效稀疏制作(PESC)，它使用专家混合式(MoE)架构将密集模型转换为稀疏模型。PESC将适配器集成到稀疏模型的MoE层中，区分不同的专家而不改变这些层中的个体权重。这种方法显著降低了计算成本和GPU内存需求，通过最小的增加实现了模型容量的扩展。

    Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase 
    
[^249]: NPHardEval: 通过复杂性类别对大型语言模型的推理能力进行动态基准评估

    NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.14890](http://arxiv.org/abs/2312.14890)

    NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。

    

    复杂推理能力是当前大型语言模型的最重要特征之一，它也被用于在复杂决策任务中起到了重要作用。因此，研究大型语言模型的推理能力至关重要：已经建立了许多基准来评估大型语言模型的推理能力。然而，目前的基准在提供大型语言模型推理能力的全面评估方面还不够，同时也容易出现过拟合的风险，因为这些基准是公开可访问且静态的，使得模型有可能根据特定的基准指标调整其响应，从而夸大其性能。针对这些限制，我们的研究引入了一个新的基准，名为NPHardEval。该基准旨在评估大型语言模型在广泛的900个算法问题上的推理能力，涵盖了NP-Hard复杂性类别。

    Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
    
[^250]: 具有模式的符号数值规划

    Symbolic Numeric Planning with Patterns. (arXiv:2312.09963v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.09963](http://arxiv.org/abs/2312.09963)

    本文提出了一种符号模式规划的新方法，通过编码问题为一个公式可以比现有方法更有效地寻找解决方案。在实验中，我们的规划器Patty在今年的IPC问题上表现出优异的性能。

    

    本文提出了一种解决线性数值规划问题的新方法，称为符号模式规划。给定一个规划问题Π，一个界限n和一个模式 - 定义为任意动作的序列 - 我们将寻找一个针对Π和界限n的计划问题编码为一个公式，该公式的变量和/或子句比最先进的rolled-up和松弛-松弛-$\exists$编码更少。更重要的是，我们证明了对于任何给定的界限，后两种编码都不可能找到一个有效的计划，而我们的编码可以。在实验方面，我们考虑了其他6个规划系统 - 包括参加今年国际规划竞赛（IPC）的系统 - 并展示了我们的规划器Patty在今年IPC问题上的非常好的比较性能。

    In this paper, we propose a novel approach for solving linear numeric planning problems, called Symbolic Pattern Planning. Given a planning problem $\Pi$, a bound $n$ and a pattern -- defined as an arbitrary sequence of actions -- we encode the problem of finding a plan for $\Pi$ with bound $n$ as a formula with fewer variables and/or clauses than the state-of-the-art rolled-up and relaxed-relaxed-$\exists$ encodings. More importantly, we prove that for any given bound, it is never the case that the latter two encodings allow finding a valid plan while ours does not. On the experimental side, we consider 6 other planning systems -- including the ones which participated in this year's International Planning Competition (IPC) -- and we show that our planner Patty has remarkably good comparative performances on this year's IPC problems.
    
[^251]: 线性对数正态注意力与无偏集中力

    Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13541](http://arxiv.org/abs/2311.13541)

    本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。

    

    Transformer模型在各种应用中取得了显著的成果。然而，由于自注意机制的时间和内存复杂度与序列长度的二次关系，其可扩展性受到限制。当处理长文档或高分辨率图像时，这一限制构成了重大障碍。本研究通过分析注意力矩阵的分布和集中能力，对自注意机制进行了研究。此外，我们提出了衡量这些数量的工具，并引入了一种新的自注意机制，即线性对数正态注意力，旨在模拟原始自注意力的分布和集中行为。我们在常用的自然语言基准测试上的实验证明，我们提出的线性对数正态注意力优于其他线性化注意力替代方法，为增强Transformer模型的可扩展性提供了一个有前途的途径。我们的代码附在补充材料中。

    Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
    
[^252]: DeliverAI: 强化学习为基础的分布式路径共享网络用于食品配送

    DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])

    [http://arxiv.org/abs/2311.02017](http://arxiv.org/abs/2311.02017)

    DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。

    

    在过去十年中，从生产者到消费者的物品配送经历了显著的增长，并且最近的流行病进一步推动了这一增长。亚马逊生鲜、Shopify、UberEats、InstaCart和DoorDash正在迅速发展，并共享相同的消费品或食品配送业务模式。现有的食品配送方法存在缺陷，因为每次配送都是在最短时间路径上从生产者直接到消费者进行优化。我们观察到，在当前模型下，有很大的减少配送成本的空间。我们将我们的食品配送问题建模为一个多目标优化问题，消费者满意度和配送成本都需要进行优化。受出租车行业中拼车成功的启发，我们提出了DeliverAI - 一种基于强化学习的路径共享算法。与以前的路径共享尝试不同，DeliverAI可以提供实时、时间高效的决策。

    Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
    
[^253]: 实现对视觉语言模型的校准鲁棒微调

    Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])

    [http://arxiv.org/abs/2311.01723](http://arxiv.org/abs/2311.01723)

    本文提出了一个名为校准鲁棒微调（CaRot）的方法，针对视觉语言模型在分布变化下的校准问题。通过该方法，作者成功提高了预训练模型的校准性能和鲁棒性能。

    

    微调可以释放预训练模型在特定任务上的潜力，但会影响模型对于非分布数据集的泛化能力。为了缓解这个问题，鲁棒微调旨在确保模型在非分布数据集以及微调的分布数据集上都有良好的性能。然而，在可靠的机器学习中，置信度校准这一标准却经常被忽视，尽管在现实世界中高风险的机器学习应用中（如自动驾驶和医学诊断）需求日益增加。我们首次提出了对细调的视觉语言模型在分布变化下校准的担忧，并通过显示普通微调甚至最先进的鲁棒微调方法对预训练的视觉语言模型的校准造成了损害，尤其是在非分布数据集上。为了解决这个问题，我们提出了一种简单的方法，称为校准鲁棒微调（CaRot），它在校准和鲁棒性上提供了奖励。

    While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot
    
[^254]: 为目标条件智能体定义开放式学习问题

    A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents. (arXiv:2311.00344v1 [cs.AI])

    [http://arxiv.org/abs/2311.00344](http://arxiv.org/abs/2311.00344)

    本文为开放式学习问题定义了一个关键的基本属性，即无限时间内不断产生新元素。在这基础上，提出了开放式学习问题的概念，并着重研究了开放式目标条件强化学习的子集。

    

    近期的许多机器学习研究论文中都提到了“开放式学习”，但很少有人尝试定义这个术语。更糟糕的是，当仔细研究时，似乎对于开放式学习与连续学习、终身学习或自为目的学习等相关概念的区别没有共识。在本文中，我们致力于解决这种情况。通过阐述这个概念的起源和最近的观点，我们说明了开放式学习通常被认为是一个包含多种属性的复合概念。与这些之前的方法不同，我们提出了将开放式过程的一个关键基本属性与时间无限制地产生新元素相分离的想法。基于此，我们建立了开放式学习问题的概念，并特别关注了开放式目标条件强化学习的子集。

    A lot of recent machine learning research papers have "Open-ended learning" in their title. But very few of them attempt to define what they mean when using the term. Even worse, when looking more closely there seems to be no consensus on what distinguishes open-ended learning from related concepts such as continual learning, lifelong learning or autotelic learning. In this paper, we contribute to fixing this situation. After illustrating the genealogy of the concept and more recent perspectives about what it truly means, we outline that open-ended learning is generally conceived as a composite notion encompassing a set of diverse properties. In contrast with these previous approaches, we propose to isolate a key elementary property of open-ended processes, which is to always produce novel elements from time to time over an infinite horizon. From there, we build the notion of open-ended learning problems and focus in particular on the subset of open-ended goal-conditioned reinforcement
    
[^255]: 使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法

    Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])

    [http://arxiv.org/abs/2310.12609](http://arxiv.org/abs/2310.12609)

    本文提出了一种使用绝缘体改善热力扩散进行无碰撞运动规划的去噪方法。该方法通过单一的视觉输入，在推理时能够同时生成可达目标并规划避开障碍物的运动路径，具有稳健性和多模态适应性。

    

    由于其灵活性和多模态性，扩散模型在机器人领域中已经成为一种强大的工具。尽管其中一些方法有效地解决了复杂问题，但它们往往严重依赖于推理时的障碍物检测并需要额外的设备。为了应对这些挑战，我们提出了一种方法，该方法在推理时能够从单一的视觉输入中同时生成可达目标并规划避开障碍物的运动路径。我们的方法的核心是对训练过程中新颖的碰撞避免扩散核进行使用。通过与行为克隆和经典扩散模型进行评估，我们的框架证明了其稳健性。特别是在多模态环境中，它能够导航到目标并避开被障碍物阻挡的不可达目标，同时确保避免碰撞。

    Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
    
[^256]: CIDER: 基于类别引导的意图分离方法用于准确的个性化新闻推荐

    CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])

    [http://arxiv.org/abs/2310.09401](http://arxiv.org/abs/2310.09401)

    CIDER是一种基于类别引导的个性化新闻推荐框架，通过意图分离和一致性的新闻表示来准确理解新闻文章的多个意图，并区分用户不同的后阅读偏好。

    

    个性化新闻推荐旨在帮助用户找到与其兴趣相符的新闻文章，这在缓解用户信息过载问题方面起到至关重要的作用。尽管许多最近的研究致力于改进用户和新闻的表示方法，但以下挑战很少被研究：（C1）如何准确理解一篇新闻文章中包含的多个意图？以及（C2）如何区分用户点击历史中对新闻文章有不同后阅读偏好的情况？为了同时解决这两个挑战，在本文中，我们提出了一种新的个性化新闻推荐框架（CIDER），它利用（1）基于类别引导的意图分离来解决（C1）和（2）基于一致性的新闻表示来解决（C2）。此外，我们将类别预测纳入CIDER的训练过程作为辅助任务，这提供了额外的监督信号，以增强意图分离。在两个真实数据集上进行了广泛的实验。

    Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
    
[^257]: ByteStack-ID: 基于灰度图像的网络入侵检测的集成堆叠模型，利用负载字节频率

    ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])

    [http://arxiv.org/abs/2310.09298](http://arxiv.org/abs/2310.09298)

    ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。

    

    在不断发展的网络安全领域中，迅速准确地识别网络流量中的各种攻击类型至关重要。本文介绍了"ByteStack-ID"，一种专为数据包级入侵检测而设计的创新方法。ByteStack-ID核心是利用从负载数据的频率分布生成的灰度图像，这是一种突破性的技术，极大地提高了模型识别复杂数据模式的能力。值得注意的是，我们的方法完全基于数据包级信息，与传统的基于流量数据的网络入侵检测系统（NIDS）有所不同。在基本堆叠方法的基础上，ByteStack-ID与传统的堆叠方法不同。它将附加的元学习器层无缝集成到连接的基础学习器中，创建了一个高度优化的统一模型。

    In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
    
[^258]: 分辨时间差分学习

    Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])

    [http://arxiv.org/abs/2310.08091](http://arxiv.org/abs/2310.08091)

    该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。

    

    时间差分学习(TD)是强化学习中的基本概念，旨在高效评估策略的价值函数。TD($\lambda$)是一种有效的变体，通过引入记忆轨迹将预测误差分散到历史上下文中。然而，这种方法经常忽视历史状态的重要性以及传播TD误差的相对重要性，这受到访问失衡或结果噪声等挑战的影响。为了解决这个问题，我们提出了一种名为分辨TD学习(DTD)的新型TD算法，它允许灵活的强调函数-在训练过程中预先确定或自适应地分配资源以提高状态之间的效果。我们在特定类别的强调函数内建立了我们方法的收敛性质，并展示了它在深度RL环境中的潜在应用。实证结果表明，使用合理的强调函数不仅可以改进值估计，还可以加速学习。

    Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
    
[^259]: 在重尾波段的完全自适应遗憾最小化领域中的研究

    Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])

    [http://arxiv.org/abs/2310.02975](http://arxiv.org/abs/2310.02975)

    本文研究了在重尾波段问题中完全自适应的遗憾最小化，提出了随机自适应重尾波段问题，并证明了适应性算法相对于标准设置会有更高的遗憾。

    

    重尾分布在金融到电信等多种环境中自然而然地出现。虽然在次高斯或有界支撑奖励下的遗憾最小化已被广泛研究，但在重尾分布上的学习只在过去十年中受到关注。在随机重尾波段问题中，一个代理在假设分布有有界最大阶的有限矩的情况下学习，这些矩被常数u一致有界，对于某个ε∈(0,1]。据我们所知，文献中只提供需要这两个量作为输入的算法。在本文中，我们研究了随机自适应重尾波段问题，这是标准设置的一个变种，其中代理对ε和u均不知晓。我们表明，适应性是存在代价的，并引入对于任何自适应算法遗憾的两个下界，意味着相对于标准设置有更高的遗憾。最后，我们引入一种特定的分布假设。

    Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional ass
    
[^260]: 使用SAM进行建筑物分割模型的零-shot细化

    Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])

    [http://arxiv.org/abs/2310.01845](http://arxiv.org/abs/2310.01845)

    本文提出了一种使用SAM进行建筑物分割模型的零-shot细化的方法，针对遥感图像应用中SAM性能不佳、无法进行识别的问题进行了处理。通过引入不同的提示来提升模型的泛化能力。

    

    基础模型在各种任务中表现出色，但通常在常规基准测试中评估。将这些模型应用于特定领域，如遥感图像，仍然是一个未充分开发的领域。在遥感领域中，精确的建筑物实例分割对于城市规划等应用至关重要。虽然卷积神经网络（CNN）表现良好，但它们的泛化能力可能受限。为了实现这一目标，我们提出了一种新的方法，以使基础模型适应已有模型的泛化性能下降。在众多模型中，我们的重点在于Segment Anything Model（SAM），这是一种强大的基础模型，以其擅长无类别图像分割能力而闻名。我们首先确定了SAM的局限性，揭示了它在应用于遥感图像时性能不佳。此外，SAM不具备识别能力，因此无法对定位的对象进行分类和标记。为了解决这些限制，我们引入了不同的提示

    Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
    
[^261]: 基于似然比的任务预测的类增量学习

    Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])

    [http://arxiv.org/abs/2309.15048](http://arxiv.org/abs/2309.15048)

    该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。

    

    类增量学习是一种具有挑战性的不断学习的设置，通过顺序学习一系列任务。每个任务由一组唯一的类组成。类增量学习的关键特点是，在测试时不提供每个测试样本的任务标识符（或任务ID）。为每个测试样本预测任务ID是一个具有挑战性的问题。一种新兴的理论上合理且有效的方法是根据任务增量学习的方法，在共享网络中为所有任务训练每个任务的任务特定模型，以处理遗忘。该方法中每个任务的模型是一个非常规分类器而不是传统分类器的离群检测器。离群检测器可以对任务内（分布内（IND））的类进行预测和识别离群数据。在推断期间，离群检测能力是每个测试样本的任务ID预测的关键。然而，本文认为使用传统的离群检测器进行任务ID预测是次优的。

    Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
    
[^262]: 使用基础模型对零样本模型进行零样本强化

    Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])

    [http://arxiv.org/abs/2309.04344](http://arxiv.org/abs/2309.04344)

    提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。

    

    零样本推断是一种强大的范式，可以在没有进一步训练的情况下使用预训练模型来进行下游分类任务。然而，这些模型容易受到继承的偏见的影响，从而影响它们的性能。传统的解决方案是微调，但这削弱了预训练模型的主要优势，即可以直接使用的能力。我们提出了RoboShot，一种完全零样本的方法，可以改善预训练模型嵌入的鲁棒性。首先，我们使用零样本语言模型（LMs）从任务描述中获取有用的见解。这些见解被嵌入并用于去除嵌入中的有害成分并增强有用成分--而无需任何监督。从理论上讲，我们提供了一个简单且可计算的模型，用于分析零样本嵌入中的偏见，并给出了在什么条件下我们的方法可以提高性能的结果。在实证上，我们在九个图像和NLP分类任务上评估了RoboShot。

    Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
    
[^263]: 证明LLM对抗敌对提示的安全性

    Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])

    [http://arxiv.org/abs/2309.02705](http://arxiv.org/abs/2309.02705)

    本研究提出了首个具有可验证安全保证的框架——消除和检查，用于对抗敌对提示。通过逐个消除标记并使用安全过滤器检查生成的子序列，确保任何敌对修改的有害输入提示都能被正确标识为有害。

    

    为了确保语言模型的输出安全，公开使用的大型语言模型（LLM）引入了所谓的“模型对齐”防护措施。一个对齐的语言模型应该拒绝用户的请求生成有害内容。然而，这种安全措施容易受到敌对提示的攻击，敌对提示包含恶意设计的标记序列，以规避模型的安全防护并导致生成有害内容。在这项工作中，我们介绍了可验证安全保证的第一个对抗敌对提示的框架——消除和检查。我们逐个消除标记，并使用安全过滤器检查生成的子序列。如果安全过滤器检测到任何子序列或输入提示有害，我们的过程将将输入提示标记为有害。这保证了对于某个特定大小的有害输入提示的任何敌对修改也将被标记为有害。我们对抗三种攻击模式：i)敌对后缀，即附加敌对序列…

    Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
    
[^264]: 基于对比损失的离线手写签名验证模型的白盒误报对抗攻击方法

    A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (arXiv:2308.08925v1 [cs.CV])

    [http://arxiv.org/abs/2308.08925](http://arxiv.org/abs/2308.08925)

    本文提出了一种针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的新方法，通过将攻击视为书写风格之间的风格转换，引入新的损失函数来生成欺骗性图像，实现了最先进的攻击成功率。

    

    本文针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的挑战，提出了一种新颖的攻击方法，将攻击视为在密切相关但不同的书写风格之间进行风格转换。为了引导欺骗性图像的生成，我们引入了两个新的损失函数，通过扰动原始样本与合成样本的嵌入向量之间的欧氏距离来提高攻击成功率，同时通过减小生成图像与原始图像之间的差异来保证最小的扰动。通过实验证明，我们的方法在对比损失的离线手写签名验证模型的白盒攻击中表现出最先进的性能。本文的关键贡献包括了一种新颖的误报攻击方法、两个新的损失函数、有效的书写风格转换以及卓越的性能。

    In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss-based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss-based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in
    
[^265]: 基于循环一致性的无监督深度图匹配

    Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])

    [http://arxiv.org/abs/2307.08930](http://arxiv.org/abs/2307.08930)

    本文提出了一种基于循环一致性的无监督深度图匹配方法，不需要真实对应的关键点对，通过在同一对象类别的图像之间强制匹配一致性来进行自我监督学习，该方法具有很高的灵活性，并且在无监督图匹配方面达到了最新的最先进水平。

    

    我们在稀疏领域的无监督深度图匹配中做出了贡献，应用于图像中的关键点匹配。与标准的“监督”方法相反，我们的方法不需要关键点对之间的真实对应。相反，它通过强制同一对象类别的图像之间的匹配一致性来进行自我监督。由于匹配和一致性损失是离散的，它们的导数不能直接用于学习。我们通过在组合求解器的黑盒微分的最新结果基础上构建我们的方法来解决这个问题。这使得我们的方法非常灵活，因为它与任意网络架构和组合求解器兼容。我们的实验评估表明，我们的技术在无监督图匹配方面达到了新的最先进水平。

    We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
    
[^266]: 使用大型语言模型增强密集检索的软提示调优

    Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2307.08303](http://arxiv.org/abs/2307.08303)

    本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。

    

    密集检索（DR）将查询和文档转化为密集向量表示，并在向量空间中测量查询与文档之间的相似性。DR的一个挑战是缺乏领域特定的训练数据。虽然DR模型可以通过迁移学习从大规模公共数据集（如MS MARCO）中学习，但证据表明，并非所有DR模型和领域都能同等受益于迁移学习。最近，一些研究人员转向使用大型语言模型（LLMs）来改进零样本和少样本的DR模型。然而，这些方法中采用的硬提示或人工编写的提示无法保证生成的弱查询的质量。为了解决这个问题，我们提出了用于增强DR的软提示调优（SPTAR）：对于每个任务，我们利用软提示调优在有限的真实数据上优化任务特定的软提示，然后用这些提示引导LLMs为未标记的文档标记弱查询，从而得到足够的弱文档-查询对来训练任务特定的模型。

    Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
    
[^267]: 安全强化学习作为Wasserstein变分推理：可解释性的形式方法

    Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])

    [http://arxiv.org/abs/2307.07084](http://arxiv.org/abs/2307.07084)

    本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。

    

    强化学习或最优控制可以为具有可变动态的顺序决策问题提供有效的推理。然而，在实际实施中，解释奖励函数和相应的最优策略一直是一个持久的挑战。因此，将顺序决策问题形式化为推理具有重要价值，因为概率推理原则上提供了多样且强大的数学工具来推断随机动态，同时提供了奖励设计和策略收敛的概率解释。在本研究中，我们提出了一种新颖的自适应Wasserstein变分优化（AWaVO）方法来解决这些顺序决策中的挑战。我们的方法利用形式方法来解释奖励设计，透明地训练收敛，以及对顺序决策的概率解释。为了证明实用性，我们展示了收敛训练并保证了收敛的训练。

    Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
    
[^268]: 机器学习需要自己的随机标准：随机平滑和基于PRNG的攻击。

    Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])

    [http://arxiv.org/abs/2306.14043](http://arxiv.org/abs/2306.14043)

    本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。

    

    随机性支持机器学习中的许多关键功能，包括优化、数据选择、隐私和安全。机器学习系统将生成或收集随机性的任务外包给了编译器、云服务提供商或工具链中的其他地方。但是，攻击者利用不良随机性甚至创建随机性的历史悠久，就像NSA放置后门在随机数生成器中以破解加密一样。本文考虑是否能够仅利用攻击者通常依赖的随机性来危害机器学习系统。我们将重点放在随机平滑上，这是一种流行的方法，用于训练可证明鲁棒性的模型，并为任意模型的特定输入数据点提供认证。我们选择随机平滑是因为它用于安全和安全（用于对抗对抗性示例和量化不确定性，分别）。在幕后，它依赖于采样高斯噪声来探索围绕数据点的体积。

    Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
    
[^269]: 从新的角度压缩ImageNet规模数据集：SRe$^2$L

    Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])

    [http://arxiv.org/abs/2306.13092](http://arxiv.org/abs/2306.13092)

    SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。

    

    我们提出了一个新的数据集压缩框架，称为Squeeze、Recover和Relabel（SRe$^2$L），它在训练期间分离了模型和合成数据的双层优化，以处理不同规模的数据集、模型体系结构和图像分辨率，从而实现有效的数据集压缩。所提出的方法展示了在不同数据集规模上的灵活性，并在合成图像任意分辨率、高分辨率训练的情况下具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力。我们在Tiny-ImageNet和完整的ImageNet-1K数据集上进行了广泛的实验。在50IPC下，我们的方法在Tiny-ImageNet和ImageNet-1K上分别实现了42.5％和60.8％的最高验证精度，较之前所有最先进方法提高了14.5％和32.9％。我们的方法在Res上也比MTT快约52倍(ConvNet-4)和16倍。

    We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
    
[^270]: 适用于人工决策辅助的智能干预方案：既考虑准确性又兼顾时间性

    Adaptive interventions for both accuracy and time in AI-assisted human decision making. (arXiv:2306.07458v1 [cs.HC])

    [http://arxiv.org/abs/2306.07458](http://arxiv.org/abs/2306.07458)

    本研究探索适用于人工决策辅助的智能干预方案，在同时考虑准确性和时间性的前提下，根据问题和用户的属性自适应地展示AI辅助具有良好的效果。

    

    在需要高准确性但同时时间又紧迫的环境下，例如在急诊室工作的医生，我们希望提供人工智能辅助，既能提高准确性又能减少时间。但是，不同类型的人工智能功能带来的好处是不同的：一些能够减少时间，但会增加对人工智能的过度依赖，而其他一些则相反。因此，我们希望根据问题和用户的各种属性（如知识水平）来自适应地展示人工智能辅助，以便在准确性和时间性之间做出最佳权衡。我们通过一个用户需要为外星人开药方的研究来探索自适应AI辅助的潜力。我们发现根据问题自适应AI辅助是有益的，可以达到时间和准确性的良好平衡。未来的研究将考虑使用机器学习算法（如强化学习）来实现快速自适应。

    In settings where users are both time-pressured and need high accuracy, such as doctors working in Emergency Rooms, we want to provide AI assistance that both increases accuracy and reduces time. However, different types of AI assistance have different benefits: some reduce time taken while increasing overreliance on AI, while others do the opposite. We therefore want to adapt what AI assistance we show depending on various properties (of the question and of the user) in order to best tradeoff our two objectives. We introduce a study where users have to prescribe medicines to aliens, and use it to explore the potential for adapting AI assistance. We find evidence that it is beneficial to adapt our AI assistance depending on the question, leading to good tradeoffs between time taken and accuracy. Future work would consider machine-learning algorithms (such as reinforcement learning) to automatically adapt quickly.
    
[^271]: FedMLSecurity：联邦学习与LLMs中攻击与防御的基准测试

    FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])

    [http://arxiv.org/abs/2306.04959](http://arxiv.org/abs/2306.04959)

    本文介绍了一个名为FedMLSecurity的基准测试，它可以模拟在联邦学习中可能出现的对抗攻击并提供相应的防御策略。该测试对各种机器学习模型和联合优化器都可以适用，并且能够轻松应用于大规模语言模型中。

    

    本文介绍了FedMLSecurity，这是一个在联邦学习（FL）中模拟对抗攻击和相应防御机制的基准测试。作为开源库FedML的一个重要模块，FedMLSecurity增强了FedML的安全评估能力。FedMLSecurity包含两个主要组件：FedMLAttacker模拟在FL训练中注入的攻击，而FedMLDefender则模拟旨在减轻攻击影响的防御策略。FedMLSecurity是开源的，可适用于各种机器学习模型（例如逻辑回归，ResNet，GAN等）和联合优化器（例如FedAVG，FedOPT，FedNOVA等）。本文的实验评估还展示了将FedMLSecurity轻松应用于LLMs的便利性，进一步强化了其各种场景下的通用性和实用性。

    This paper introduces FedMLSecurity, a benchmark that simulates adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). As an integral module of the open-sourced library FedML that facilitates FL algorithm development and performance comparison, FedMLSecurity enhances the security assessment capacity of FedML. FedMLSecurity comprises two principal components: FedMLAttacker, which simulates attacks injected into FL training, and FedMLDefender, which emulates defensive strategies designed to mitigate the impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to a wide range of machine learning models (e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.). Experimental evaluations in this paper also demonstrate the ease of application of FedMLSecurity to Large Language Models (LLMs), further reinforcing its versatility and practical utility in various scenarios.
    
[^272]: MESSY估计：基于最大熵的随机和符号密度估计

    MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])

    [http://arxiv.org/abs/2306.04120](http://arxiv.org/abs/2306.04120)

    MESSY估计方法是一种基于最大熵的随机和符号密度估计方法，通过构建基于梯度的漂移扩散过程来高效地找到最大熵分布的参数，支持高维问题，并具有优于现有最新方法的有效性和普适性。

    

    我们引入了基于最大熵的随机和符号密度估计方法MESSY。所提出的方法使用梯度流的矩将概率密度函数从样本中恢复为符号表达式，并将ansatz作为驱动力。特别地，我们构建了一个基于梯度的漂移扩散过程，将未知分布函数的样本与猜测的符号表达式相连。然后，我们展示出当猜测分布具有最大熵形式时，可以通过使用提供的样本的矩构建的线性方程组高效地找到该分布的参数。此外，我们使用符号回归来探索平滑函数的空间，并找到导致最大熵泛函指数的最优基函数，以获得良好条件。该方法在随机搜索的每次迭代中的成本与样本数量呈线性关系，与变量数量呈二次关系，使其可扩展到高维问题。数值实验显示出所提出方法的有效性和普适性，与现有的最新方法相比。

    We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
    
[^273]: 以活动理论视角探究外语英语学习者在人工智能写作中的提示工程

    Exploring EFL students' prompt engineering in human-AI story writing: an Activity Theory perspective. (arXiv:2306.01798v1 [cs.CY])

    [http://arxiv.org/abs/2306.01798](http://arxiv.org/abs/2306.01798)

    本研究应用活动理论分析了香港中学生在短篇故事创作中利用生成式人工智能工具的方式和目的，发现其中缺乏目的意识、克服创作障碍以及发展、扩展和改进故事为主要目的。同时，学生活动系统的共同特征也被研究确定。

    

    本研究应用活动理论探究了香港中学生在短篇故事创作中如何利用生成式人工智能工具。研究收集并分析了学生的生成式人工智能工具、短篇故事和有关提示目的的书面反思。研究确定了学生提示生成式人工智能工具的三个主要目的：缺乏目的意识、克服创作障碍以及发展、扩展和改进故事。研究还确定了学生活动系统的共同特征，包括他们的生成式人工智能工具的复杂性、故事的质量以及所在学校的整体学术成就水平。

    This study applies Activity Theory to investigate how English as a foreign language (EFL) students prompt generative artificial intelligence (AI) tools during short story writing. Sixty-seven Hong Kong secondary school students created generative-AI tools using open-source language models and wrote short stories with them. The study collected and analyzed the students' generative-AI tools, short stories, and written reflections on their conditions or purposes for prompting. The research identified three main themes regarding the purposes for which students prompt generative-AI tools during short story writing: a lack of awareness of purposes, overcoming writer's block, and developing, expanding, and improving the story. The study also identified common characteristics of students' activity systems, including the sophistication of their generative-AI tools, the quality of their stories, and their school's overall academic achievement level, for their prompting of generative-AI tools for
    
[^274]: 利用量子神经网络进行生物标志物的发现：以CTLA4激活通路的案例研究为例

    Biomarker Discovery with Quantum Neural Networks: A Case-study in CTLA4-Activation Pathways. (arXiv:2306.01745v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.01745](http://arxiv.org/abs/2306.01745)

    该论文介绍了一种利用量子神经网络的模型，可以用于发现与CLTA4通路相关的新生物标志物，并且该模型经济实用。

    

    生物标志物的发现是一项具有挑战性的任务，由于搜索空间庞大。量子计算和量子人工智能（量子AI）可以用于解决生物标志物发现任务的计算问题。我们提出了一种量子神经网络（QNNs）架构，用于发现输入激活通路的生物标志物。最大相关性，最小冗余（mRMR）标准用于评分生物标志物候选集。由于神经解决方案可以在受限硬件上交付，所以我们提出的模型是经济的。我们在与CTLA4相关的四条激活通路上展示了证明概念，包括（1）CTLA4激活独立，（2）CTLA4-CD8A-CD8B共同激活，（3）CTLA4-CD2共同激活，以及（4）CTLA4-CD2-CD48-CD53-CD58-CD84共同激活。该模型指出与CLTA4相关通路的突变激活有新的生物标志物，包括20个基因：CLIC4，CPE，ETS2，FAM107A，GPR116，HYOU1，LCN2，MACF1，MT1G，NAPA，NDUFS5，PAK1，PFN1，PGAP

    Biomarker discovery is a challenging task due to the massive search space. Quantum computing and quantum Artificial Intelligence (quantum AI) can be used to address the computational problem of biomarker discovery tasks. We propose a Quantum Neural Networks (QNNs) architecture to discover biomarkers for input activation pathways. The Maximum Relevance, Minimum Redundancy (mRMR) criteria is used to score biomarker candidate sets. Our proposed model is economical since the neural solution can be delivered on constrained hardware. We demonstrate the proof of concept on four activation pathways associated with CTLA4, including (1) CTLA4-activation stand-alone, (2) CTLA4-CD8A-CD8B co-activation, (3) CTLA4-CD2 co-activation, and (4) CTLA4-CD2-CD48-CD53-CD58-CD84 co-activation. The model indicates new biomarkers associated with the mutational activation of CLTA4-associated pathways, including 20 genes: CLIC4, CPE, ETS2, FAM107A, GPR116, HYOU1, LCN2, MACF1, MT1G, NAPA, NDUFS5, PAK1, PFN1, PGAP
    
[^275]: 未建图环境下多车辆路径规划的分布式在线策略

    Distributed Online Rollout for Multivehicle Routing in Unmapped Environments. (arXiv:2305.15596v1 [cs.DC])

    [http://arxiv.org/abs/2305.15596](http://arxiv.org/abs/2305.15596)

    本文提出了一个在线策略，通过分布式的协调策略解决了未建图环境下多车辆路径规划问题。

    

    本文考虑了一个广泛化的多车辆路径规划问题：给定一个网络、一组占据网络节点子集的智能体和一组任务, 我们寻求一个最小成本的移动序列，以满足每个任务至少被一个智能体访问的约束条件。与经典问题不同的是，我们假定没有中央服务器，每个智能体都是一个个体处理器，没有关于基础网络（包括任务和智能体位置）的先验知识。此外，我们的智能体具有严格的本地通信和感知能力（限制在它们各自位置周围的半径范围内），更接近多实际应用。我们提出了一种在线策略方法，其中智能体根据集中式模拟器训练的学习型策略局部规划其轨迹。我们在模拟环境中展示了我们的方法，并表明它在网络拓扑和任务分布的变化下仍然能够实现近最优性能。

    In this work we consider a generalization of the well-known multivehicle routing problem: given a network, a set of agents occupying a subset of its nodes, and a set of tasks, we seek a minimum cost sequence of movements subject to the constraint that each task is visited by some agent at least once. The classical version of this problem assumes a central computational server that observes the entire state of the system perfectly and directs individual agents according to a centralized control scheme. In contrast, we assume that there is no centralized server and that each agent is an individual processor with no a priori knowledge of the underlying network (including task and agent locations). Moreover, our agents possess strictly local communication and sensing capabilities (restricted to a fixed radius around their respective locations), aligning more closely with several real-world multiagent applications. These restrictions introduce many challenges that are overcome through local
    
[^276]: MLM预训练的动态掩码率调度

    Dynamic Masking Rate Schedules for MLM Pretraining. (arXiv:2305.15096v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15096](http://arxiv.org/abs/2305.15096)

    本论文提出了一种动态调度掩码率的方法来改进MLM预训练的质量，通过线性降低掩码率，达到了对BERT-base和BERT-large模型分别提高0.46%和0.25%的平均GLUE准确率的效果。这种方法不仅加快了BERT-base的预训练速度，还实现了对BERT-large的帕累托改善。

    

    大多数使用掩码语言建模（MLM）目标进行训练的transformer模型使用了原始BERT模型的固定掩码率15%。我们提出了通过训练过程中动态调整掩码率来替代固定率的方法。我们发现，在预训练过程中线性降低掩码率可以比固定率基准分别提高BERT-base和BERT-large的平均GLUE准确率0.46%和0.25%。这些提升来自于接触高和低掩码率的机制，从而在两种设置中都带来了优势。我们的结果表明，掩码率调度是提高掩码语言模型质量的简单方法，可以使BERT-base的预训练速度提高1.89倍，并对BERT-large实现了帕累托改善。

    Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model's fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.
    
[^277]: StyleLipSync：基于风格的个性化唇形动画视频生成

    StyleLipSync: Style-based Personalized Lip-sync Video Generation. (arXiv:2305.00521v1 [cs.CV])

    [http://arxiv.org/abs/2305.00521](http://arxiv.org/abs/2305.00521)

    本文提出了一种基于风格的个性化唇形动画视频生成模型，可以准确地生成任意身份的唇形同步视频，且可用于增强未见面孔的特征。

    

    本文提出了StyleLipSync，这是一种基于风格的个性化唇形动画视频生成模型，它可以从任意音频生成无关身份的唇形同步视频。为了生成任意身份的视频，我们利用了预培训的StyleGAN的语义丰富潜空间中的表达性唇部先验知识，并通过线性变换设计视频一致性。与以往的唇形同步方法不同，我们引入了姿态感知遮罩，通过逐帧利用三维参数化网格预测器动态定位遮罩，提高了帧间自然度。此外，我们还提出了一种几乎不需要数据的唇形同步适应方法，通过引入同步正则化器来保留唇形同步泛化能力，同时增强人物特定的视觉信息。广泛的实验表明，我们的模型可以生成准确的唇形同步视频，甚至可以在零样本设置下增强未见面孔的特征。

    In this paper, we present StyleLipSync, a style-based personalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary audio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by introducing a sync regularizer that preserves lips-sync generalization while enhancing the person-specific visual information. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a fe
    
[^278]: 受限强化学习在可信四旋翼无人机跟踪控制中的应用

    Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control. (arXiv:2302.11694v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.11694](http://arxiv.org/abs/2302.11694)

    提出了一种使用分布式表示进行受限强化学习的方法，用于可信四旋翼无人机的跟踪控制。通过集成分布式强化学习干扰估计器和随机模型预测控制器，能够准确识别气动效应的不确定性，实现最优的全局收敛速率和一定的亚线性收敛速率。

    

    在复杂的动态环境中，同时实现四旋翼无人机的准确和可靠的跟踪控制是具有挑战性的。由于来自气动力的阻力和力矩变化是混沌的，并且难以精确识别，大多数现有的四旋翼跟踪系统将其视为传统控制方法中的简单“干扰”。本文提出了一种新颖的、可解释的轨迹跟踪器，将分布式强化学习干扰估计器与随机模型预测控制器（SMPC）相结合，用于未知的气动效应。所提出的估计器“受限分布式强化干扰估计器”（ConsDRED）准确地识别真实气动效应与估计值之间的不确定性。采用简化仿射干扰反馈进行控制参数化，以保证凸性，然后将其与SMPC相结合。我们在理论上保证ConsDRED至少实现最优的全局收敛速率和一定的亚线性收敛速率。

    Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear r
    
[^279]: 深度学习综述：从激活函数到Transformer

    A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00722](http://arxiv.org/abs/2302.00722)

    这篇综述调查了深度学习领域的重要进展，包括各种架构、层、目标和优化技术的发展，以及关注机制、归一化、跳跃连接、Transformer和自监督学习等方法的变体。总结了成功创新的关键策略，并讨论了最近的商业闭源模型。

    

    过去十年中，深度学习取得了显著的进展，得益于各种架构、层、目标和优化技术的涌现。这些包括关注机制、归一化、跳跃连接、Transformer和自监督学习等多种变体方法。我们的目标是向具有深度学习基本理解的人提供对这些领域中最新重要贡献的全面调查。我们的期望是通过对重要最新作品的综合和全面的探讨，促进不同深度学习领域之间形成新的联系。在我们的讨论中，我们总结了过去十年中许多成功创新的关键策略。我们还对最近一些商业闭源模型进行了讨论，例如OpenAI的GPT-4和Google的PaLM 2。

    The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.
    

