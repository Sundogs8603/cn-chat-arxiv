# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Benchmarking and Analyzing Generative Data for Visual Recognition.](http://arxiv.org/abs/2307.13697) | 这篇论文通过实验研究了利用生成数据在视觉识别中的应用，并提出了一个用于评估生成数据的基准，一个训练-free的度量指标以及与检索数据进行比较揭示生成数据的独特特征的新方法。 |
| [^2] | [The Visual Language of Fabrics.](http://arxiv.org/abs/2307.13681) | 这项研究引入了一个新的数据集text2fabric，通过自由文本描述与面料材料相关联，旨在提高材料描述的表达能力。研究表明，人们如何描述面料的紧凑词汇、属性集合和关键结构可以指导推广到其他类型的材料，并且该数据集可以用于专门化大型视觉-语言模型。 |
| [^3] | [Towards an AI Accountability Policy.](http://arxiv.org/abs/2307.13658) | 这份白皮书是对美国国家电信和信息管理局的“AI问责政策评论请求”的回应，提出了一组相互关联的AI问责政策建议。 |
| [^4] | [QuickQual: Lightweight, convenient retinal image quality scoring with off-the-shelf pretrained models.](http://arxiv.org/abs/2307.13646) | QuickQual是一个简单的视网膜图像质量评分方法，它使用现成的预训练模型和支持向量机，并在EyeQ数据集上取得了新的最先进结果，证明视网膜图像质量评分可以通过学习自然图像的通用感知特征来解决。 |
| [^5] | [Safety Margins for Reinforcement Learning.](http://arxiv.org/abs/2307.13642) | 本论文提出了一种能够通过计算代理关键性指标来生成安全边界的方法，该方法能够将可能的错误行为的后果与整体性能的预期损失联系起来。在Atari环境中的实验结果表明，随着代理接近失败状态，安全边界减小。 |
| [^6] | [GPT-3 Models are Few-Shot Financial Reasoners.](http://arxiv.org/abs/2307.13617) | GPT-3模型在金融领域的少样本推理表现有限，需要使用独立的检索模型和逻辑引擎来获得最佳性能。 |
| [^7] | [Dendritic Integration Based Quadratic Neural Networks Outperform Traditional Aritificial Ones.](http://arxiv.org/abs/2307.13609) | 提出了一种基于树突综合的二次神经网络(DIQNN)模型，该模型在多种分类任务中表现出优越性能，超过了传统的人工神经网络。引入边界来刻画泛化误差，并将边界整合到损失函数中，加速了测试准确率的改变。 |
| [^8] | [Cloud Render Farm Services Discovery Using NLP And Ontology Based Knowledge Graph.](http://arxiv.org/abs/2307.13604) | 利用自然语言处理和基于本体的知识图谱，这项研究提出了一种名为RenderSelect的服务发现引擎，用于发现成本效益高且符合功能要求的云渲染农场服务。 |
| [^9] | [Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks.](http://arxiv.org/abs/2307.13582) | 本文提出了一个新的“论证归因解释（AAEs）”理论，用于确定论证对“主题论证”产生的影响，在定量双极论证框架（QBAFs）中填补了解释定量推理结果的空白。 |
| [^10] | [Reinterpreting survival analysis in the universal approximator age.](http://arxiv.org/abs/2307.13579) | 这篇论文介绍了生存分析在深度学习中的应用，提供了连接分类和回归的方法，并且提出了无需数值积分的通用拟合网络，通过大规模的数值研究证明了其优于其他方法。 |
| [^11] | [The Impact of Imperfect XAI on Human-AI Decision-Making.](http://arxiv.org/abs/2307.13566) | 本研究通过一个混合方法用户研究，评估了不正确的解释如何影响人类的决策行为，以增进人工智能解释性对人工智能决策的理解。 |
| [^12] | [Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities.](http://arxiv.org/abs/2307.13565) | 决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。 |
| [^13] | [On Solving the Rubik's Cube with Domain-Independent Planners Using Standard Representations.](http://arxiv.org/abs/2307.13552) | 本文将魔方的表示转换为PDDL语言，使其更具可访问性和易读性。实验结果表明，DeepCubeA可以解决所有不同复杂度的魔方问题，但只有18％是最优解。 |
| [^14] | [A Planning Ontology to Represent and Exploit Planning Knowledge for Performance Efficiency.](http://arxiv.org/abs/2307.13549) | 本研究提出了一种使用规划本体表示和利用规划知识的方法来提高自动规划的性能效率。通过构建一个规划本体，并利用国际规划竞赛的数据进行实验，证明了本体能够选择有前景的规划器，并使用从本体中提取的宏观约束来提高它们的性能。 |
| [^15] | [Group Activity Recognition in Computer Vision: A Comprehensive Review, Challenges, and Future Perspectives.](http://arxiv.org/abs/2307.13541) | 该论文回顾了群体活动识别的技术进展，并着重介绍了全局互动性和活动的识别方法。 |
| [^16] | [Spectrum-guided Multi-granularity Referring Video Object Segmentation.](http://arxiv.org/abs/2307.13537) | 本论文提出了一种光谱引导的多粒度参考视频对象分割方法，用于解决当前参考视频对象分割中的特征漂移问题。通过直接分割编码特征，并利用视觉细节优化掩膜，以及在频谱域中进行跨模态融合，实现了更有效的多模态表示。最终，将该方法扩展到多对象R-VOS，实现了同时分割视频中多个参考对象的功能，使得R-VOS更快速、更实用。 |
| [^17] | [Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection.](http://arxiv.org/abs/2307.13529) | 本论文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识增强人物-物体交互检测，通过再挖掘策略生成更全面的视觉表示，并设计了细粒度的句子和词级对齐以及知识转移策略来解决多对多匹配问题。 |
| [^18] | [FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios.](http://arxiv.org/abs/2307.13528) | 提出了FacTool框架，用于检测大型语言模型（如ChatGPT）生成的文本中的事实错误。实验结果表明该方法在知识型问答、代码生成、数学推理和科学文献综述等四个任务中具有良好的效果。 |
| [^19] | [Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction.](http://arxiv.org/abs/2307.13497) | Zshot是一个开源框架，用于零样本命名实体识别和关系抽取，通过比较不同的最新ZSL方法，支持研究人员和工业界的需求。 |
| [^20] | [Duet: efficient and scalable hybriD neUral rElation undersTanding.](http://arxiv.org/abs/2307.13494) | Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。 |
| [^21] | [Integrating processed-based models and machine learning for crop yield prediction.](http://arxiv.org/abs/2307.13466) | 本研究通过将基于过程的作物生长模型与机器学习相结合，提出了一种混合元模建方法用于马铃薯产量预测。该方法通过使用合成数据预训练卷积神经网络，并使用观测数据进行微调，在模拟应用和真实数据测试中均取得了竞争力的预测结果。 |
| [^22] | [Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion.](http://arxiv.org/abs/2307.13463) | 本文概述了解锁视觉媒体情感世界的科学、研究和影响，探讨了计算机视觉领域中对视觉媒体中情感的自动理解的挑战与进展。 |
| [^23] | [Fundamental causal bounds of quantum random access memories.](http://arxiv.org/abs/2307.13460) | 本研究通过采用相对论量子场论和量子多体系统中的Lieb-Robinson界限，批判性地探讨了基于因果性的快速量子存储器的内在界限。研究表明在混合量子声学系统中，QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。 |
| [^24] | [Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results.](http://arxiv.org/abs/2307.13453) | 本研究探讨了如何利用蒙特卡洛树搜索（MCTS）解决多智能体路径规划问题，并引入了一种针对该问题的原创MCTS变种。该方法使用各自的路径辅助智能体实现目标达成，并允许它们根据需要离开路径避免碰撞。 |
| [^25] | [A behavioural transformer for effective collaboration between a robot and a non-stationary human.](http://arxiv.org/abs/2307.13447) | 该论文提出了一个行为变换器(BeTrans)框架，该框架能够使机器人能够更好地预测人类的非静态行为，并通过使用顺序数据来适应新的非静态的人类代理。实验证明BeTrans在协作环境中效果显著，比现有技术更快地适应了非静态的模拟人类代理。 |
| [^26] | [On the learning Dynamics of Attention Networks.](http://arxiv.org/abs/2307.13421) | 本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。 |
| [^27] | [Towards Bridging the Digital Language Divide.](http://arxiv.org/abs/2307.13405) | 本文旨在探讨“语言偏见”现象，即多语言处理系统存在对某些语言的硬编码倾向，忽视了语言复杂性和语言社区的需求，阻碍了AI技术覆盖到“资源有限的语言”。 |
| [^28] | [Predicting Code Coverage without Execution.](http://arxiv.org/abs/2307.13383) | 该论文提出了一种无需执行代码即可预测代码覆盖率的方法，并建立了一个基准任务来评估语言模型理解代码能力。 |
| [^29] | [Empower Your Model with Longer and Better Context Comprehension.](http://arxiv.org/abs/2307.13365) | 本文研究了大语言模型（LLMs）内的信息传递，并提出了一种名为注意力转移的技术，该技术能够使模型在不增加训练或对生成流畅性的影响的情况下实现更长更好的上下文理解。 |
| [^30] | [Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type.](http://arxiv.org/abs/2307.13345) | 本研究调查了人类注意力与CNN注意力之间的相似性在场景分类中受任务和图像类型的影响，结果发现任务的意图和图像的特征都对二者的相似性有所影响。 |
| [^31] | [Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions.](http://arxiv.org/abs/2307.13339) | 通过基于梯度的特征归因方法，研究发现思维链启发在大型语言模型中并没有增加与语义相关标记的重要性，但提高了与问题相关标记的重要性分数的鲁棒性。 |
| [^32] | [The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation.](http://arxiv.org/abs/2307.13332) | 本文研究了在线性离策略值函数估计中的逼近因子，并在多种设置下建立了最优的渐近逼近因子，这些因子决定了离策略评估的困难程度。 |
| [^33] | [Learning Autonomous Ultrasound via Latent Task Representation and Robotic Skills Adaptation.](http://arxiv.org/abs/2307.13323) | 本文提出了一种通过潜在任务表示和机器人技能适应来学习自主超声的方法。在离线阶段，将多模式超声技能合并为低维概率模型。在在线阶段，概率模型将选择和评估最优解决方案。 |
| [^34] | [Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation.](http://arxiv.org/abs/2307.13294) | 本论文提出了一种通过LED照明调制对人脸识别系统进行不可察觉的物理攻击，通过快速强度调制生成难以察觉的亮度变化，并利用图像传感器的卷帘快门效应向捕获的人脸图像中注入亮度信息扰动。 |
| [^35] | [Curvature-based Transformer for Molecular Property Prediction.](http://arxiv.org/abs/2307.13275) | 该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。 |
| [^36] | [Unbiased Weight Maximization.](http://arxiv.org/abs/2307.13270) | 这项研究提出了一种无偏重量最大化的方法，通过用出站权重的范数替换单元的奖励信号，实现了更加高效的结构性信用分配。 |
| [^37] | [LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition.](http://arxiv.org/abs/2307.13269) | 本文研究了LoRA组合在跨任务通用性上的可行性，并提出了LoraHub框架，能够通过组合不同任务上训练的LoRA模块，实现对未见任务的可适应性性能。实验结果表明，LoraHub在少样本场景中能够有效模拟上下文学习的性能，而无需上下文示例。 |
| [^38] | [Federated Split Learning with Only Positive Labels for resource-constrained IoT environment.](http://arxiv.org/abs/2307.13266) | 在资源受限的物联网环境中，我们提出了带有正标签的分割学习（SFPL）方法，通过对数据进行随机洗牌来改善多类别分类深度学习模型在联邦分割学习中的效果。 |
| [^39] | [Structural Credit Assignment with Coordinated Exploration.](http://arxiv.org/abs/2307.13256) | 本文提出一种结构化信用分配与协调探索的方法，该方法将每个单元视为强化学习代理，并通过全局奖励信号调节局部学习规则。通过提高结构化信用分配的效率来改进人工神经网络的训练。 |
| [^40] | [GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers.](http://arxiv.org/abs/2307.13251) | 本文提出了GaPro，一种基于3D点云的实例分割方法，使用轴对齐的盒子监督。通过生成伪标签并训练网络，利用高斯过程解决了盒子重叠时的不确定性问题，并在实验证明其优于先前方法。 |
| [^41] | [RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision.](http://arxiv.org/abs/2307.13239) | 本论文提出了一种具有抗污染连续监督的深度半监督异常检测方法，通过质量插值方法创造了连续异常度标签的新数据样本，解决了半监督异常检测方法中的异常污染和离散监督信息利用不充分的问题。 |
| [^42] | [Multilevel Large Language Models for Everyone.](http://arxiv.org/abs/2307.13221) | 本文提出了一个多层大型语言模型的设计，将通用的和特定的模型结合在一起，可以根据用户的个人输入和互联网信息相互改进。这种模型受到人类大脑功能的启发，具有全局、领域和用户级模型，可以在本地机器上运行以保护用户隐私。 |
| [^43] | [One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction.](http://arxiv.org/abs/2307.13220) | 提出了一种名为PISF的物理信息合成数据学习框架，该框架使得快速MRI重建中的多场景可推广深度学习成为可能。 |
| [^44] | [Adversarial Deep Hedging: Learning to Hedge without Price Process Modeling.](http://arxiv.org/abs/2307.13217) | 本研究提出了一个新的对抗式深度对冲框架，用于在不完全市场中进行衍生品对冲。通过对冲者和生成器的对抗训练，该方法能够学习出无需进行价格过程建模的稳健对冲者。 |
| [^45] | [FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning.](http://arxiv.org/abs/2307.13214) | FedMEKT是一种基于蒸馏的多模态联邦学习框架，通过半监督学习方法利用不同模态的表示，实现了服务器和客户端之间的联合知识传输。 |
| [^46] | [Gait Cycle-Inspired Learning Strategy for Continuous Prediction of Knee Joint Trajectory from sEMG.](http://arxiv.org/abs/2307.13209) | 本文提出了一种受步态周期启发的学习策略，通过将膝关节角度分解为运动模式和振幅的方式来提高对人体膝关节轨迹的预测性能。 |
| [^47] | [Counterfactual Explanation Policies in RL.](http://arxiv.org/abs/2307.13192) | 本文介绍了一个名为COUNTERPOL的框架，用于通过对策略进行最小改变来分析RL策略，并达到所需的结果。这项工作在RL中通过使用反事实解释与监督学习相结合的方法进行了实证分析，并与广泛使用的基于信任区域的策略优化方法进行了理论联系。 |
| [^48] | [Digital Emotion Regulation on Social Media.](http://arxiv.org/abs/2307.13187) | 本文概述了社交媒体上的数字情绪调节，并综合了关于社交媒体情绪调节干预的最新研究。研究发现不同社交媒体应用在情绪调节过程的不同阶段中有不同的使用方式。 |
| [^49] | [Opinion Mining Using Population-tuned Generative Language Models.](http://arxiv.org/abs/2307.13173) | 本文提出了一种使用经过调整的生成式语言模型进行观点挖掘的方法。通过特定数据的微调，我们的方法可以学习和转移观点，并保持极性的比例。实验结果表明我们的方法在挖掘真实文本中的观点洞察方面具有良好的性能。 |
| [^50] | [Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study.](http://arxiv.org/abs/2307.13165) | 本研究通过对多个数据集进行评估发现，顺序推荐系统中删除序列末尾的项目显著降低了性能，而删除序列开头或中间的项目则没有明显影响。这一发现强调了考虑训练数据中扰动项目位置的重要性，并能指导更具鲁棒性的顺序推荐系统的设计。 |
| [^51] | [Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations.](http://arxiv.org/abs/2307.13131) | 本文提出了EvilEye，利用透明显示器生成动态物理敌对示例的感知攻击方法，通过利用摄像头的光学特性在多种照明条件下诱导错误分类。 |
| [^52] | [Pathway: a fast and flexible unified stream data processing framework for analytical and Machine Learning applications.](http://arxiv.org/abs/2307.13116) | Pathway是一个快速灵活的统一流数据处理框架，用于分析和机器学习应用。它能够在有界和无界的数据流上运行，通过Table API和分布式增量数据流驱动，在批处理和流处理场景中表现出优异的能力。 |
| [^53] | [An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment.](http://arxiv.org/abs/2307.13108) | 本文提出了一个可解释的几何加权图注意力网络（xGW-GAT），用于识别与步态障碍相关的功能网络，以推动帕金森病治疗的发展。 |
| [^54] | [How to use LLMs for Text Analysis.](http://arxiv.org/abs/2307.13106) | 本文介绍了如何使用LLMs进行文本分析，LLMs作为一种非常多功能的文本分析方法在社会科学中具有广泛的应用前景。使用LLMs可以实现从文本标注和分类到情感分析和批判性话语分析等多种任务，并且易于使用且速度快。这对于具有有限编程经验的学生和研究者来说尤其有用。 |
| [^55] | [Contrastive Example-Based Control.](http://arxiv.org/abs/2307.13101) | 本文提出了一种基于示例的控制方法，它通过学习隐式模型的多步转移来解决控制问题，而不是学习奖励函数。这种方法避免了复杂的正规化和时差更新，并取得了良好的结果。 |
| [^56] | [Making Metadata More FAIR Using Large Language Models.](http://arxiv.org/abs/2307.13085) | 本研究提出了一种名为FAIRMetaText的自然语言处理应用程序，用于比较元数据。该应用程序分析元数据的自然语言描述，并提供数学相似度度量，可用于分析和识别可替代术语，从而大大减少人力成本。 |
| [^57] | [Fairness Under Demographic Scarce Regime.](http://arxiv.org/abs/2307.13081) | 这项研究探讨了在人口信息不完全可用的情况下如何提高公平性。研究发现，在替代敏感属性的属性分类器中引入不确定性意识，并对推断出的不确定性最低的人口信息样本进行公平性约束可以实现更好的公平性和准确性权衡。 |
| [^58] | [Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs.](http://arxiv.org/abs/2307.13078) | 本文提出了一种自适应认证训练方法，通过训练模型时使用自适应认证半径的关键观点，提高了模型的准确性和鲁棒性，推进了准确性-鲁棒性权衡的最新进展。 |
| [^59] | [Graph Neural Networks For Mapping Variables Between Programs -- Extended Version.](http://arxiv.org/abs/2307.13014) | 本文提出了使用图神经网络(GNNs)基于程序的抽象语法树(ASTs)来映射变量集，以解决程序比较、分析、修复和克隆检测等任务。在初学者编程作业中进行的实验证明了变量映射的有效性。 |
| [^60] | [Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains.](http://arxiv.org/abs/2307.13012) | 本文提出了一个全新的基准，对不同的语音活动检测和重叠讲话检测模型进行了评估，涉及多个音频设置和语音领域。我们的系统在F1得分方面表现出色，并且通过联合训练这两个任务，可以降低训练成本。 |
| [^61] | [Maximal Independent Sets for Pooling in Graph Neural Networks.](http://arxiv.org/abs/2307.13011) | 本文提出了三种基于最大独立集合概念的图形池化方法，避免了现有方法中存在的缺点，通过实验证实了最大独立集合约束在图形池化中的重要性。 |
| [^62] | [Adaptation of Whisper models to child speech recognition.](http://arxiv.org/abs/2307.13008) | 本文研究了将Whisper模型适应儿童语音识别的问题，并比较了与wav2vec2模型的调优效果。结果表明，在儿童语音上进行调优的Whisper模型能显著提高儿童语音的ASR性能，而调优的wav2vec2模型表现更好。 |
| [^63] | [IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models.](http://arxiv.org/abs/2307.13005) | 这个研究开发了一个名为IteraTTA的界面，可以帮助用户探索文本提示和音频先验对生成音乐结果的影响，从而使用户能够逐步实现他们的期望。 |
| [^64] | [Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning.](http://arxiv.org/abs/2307.12996) | 该论文研究了如何使用多模态对比学习方法从自然语言中提取分子属性信息，通过改进文本检索和引入分子图扩增策略等方法提高了属性预测性能。实验结果显示相对于仅在图模态上预训练的模型，我们取得了+4.26%的AUROC增益和+1.54%的增益。 |
| [^65] | [Multi-representations Space Separation based Graph-level Anomaly-aware Detection.](http://arxiv.org/abs/2307.12994) | 本文提出了一种基于多表示空间分离的图级异常感知检测框架，以解决检测图集内异常图的问题。 |
| [^66] | [Towards a Visual-Language Foundation Model for Computational Pathology.](http://arxiv.org/abs/2307.12914) | 这项研究介绍了一种基于视觉语言的基础模型CONCH，通过图像、文本和图像-标题对的预训练，可以在涉及组织病理学图像和文本的各种任务上取得良好的性能。 |
| [^67] | [GridMM: Grid Memory Map for Vision-and-Language Navigation.](http://arxiv.org/abs/2307.12907) | 本文提出了GridMM，一种用于视觉与语言导航的自顶向下网格记忆图，从全局和局部视角有效地表示和结构化先前访问的环境。在多个数据集上进行的实验证明了该方法的优越性。 |
| [^68] | [Nonparametric Linear Feature Learning in Regression Through Regularisation.](http://arxiv.org/abs/2307.12754) | 本研究提出了一种新的非参数线性特征学习方法，对于监督学习中存在于低维线性子空间中的相关信息的预测和解释能力的提升是非常有帮助的。 |
| [^69] | [TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition.](http://arxiv.org/abs/2307.12493) | TF-ICON是一种无需训练的图像合成框架，利用文字驱动的扩散模型实现跨领域图像导向合成。与传统方法相比，TF-ICON可以在不需额外训练、微调或优化的情况下实现高质量的无缝合成，同时引入了例外提示来准确地反转真实图像为潜在表示。 |
| [^70] | [Question Decomposition Improves the Faithfulness of Model-Generated Reasoning.](http://arxiv.org/abs/2307.11768) | 通过将问题分解为子问题，可以显著提高大型语言模型生成推理的忠实度。 |
| [^71] | [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus.](http://arxiv.org/abs/2307.11760) | EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。 |
| [^72] | [Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms.](http://arxiv.org/abs/2307.10223) | 这项研究探索了如何在评估人工智能偏见和伤害时整合边缘化社区的知识，并提出了以酷儿社区为视角重新设计偏见奖金的方法。 |
| [^73] | [Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning.](http://arxiv.org/abs/2307.09588) | 本研究通过深度学习实现了对纤维材料显微图像中硬木种类的自动检测和分类。该方法在性能上与人类专家类似，未来将有助于保护森林资源。 |
| [^74] | [A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning.](http://arxiv.org/abs/2307.09218) | 遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。 |
| [^75] | [Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models.](http://arxiv.org/abs/2307.08303) | 本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。 |
| [^76] | [RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task.](http://arxiv.org/abs/2307.07840) | 这项工作提出了一种新的解释方法（XAIG-R），用于解释图回归模型，通过引入信息瓶颈理论的新目标和混合框架来解决回归任务中的挑战，同时还使用对比学习策略来处理连续有序标签。 |
| [^77] | [Integrating Curricula with Replays: Its Effects on Continual Learning.](http://arxiv.org/abs/2307.05747) | 将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。 |
| [^78] | [Transformer Training Strategies for Forecasting Multiple Load Time Series.](http://arxiv.org/abs/2306.10891) | 转换器模型在预测多负载时间序列方面使用全局训练策略比多变量和本地训练策略具有更好的性能，平均降低了21.8%和12.8%的预测误差。 |
| [^79] | [Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale.](http://arxiv.org/abs/2306.00017) | 本文提出结合符号表示和自下而上的逆向工程的方法，解决大规模语言模型在真正语言理解上的局限性，实现可解释的、语言无关的LLMs。 |
| [^80] | [Scene restoration from scaffold occlusion using deep learning-based methods.](http://arxiv.org/abs/2305.18810) | 本研究提出了一种基于深度学习的方法，通过像素级分割和图像修复技术，从脚手架遮挡中恢复建筑场景。实验结果表明，该方法在脚手架分割和场景恢复方面表现出色。 |
| [^81] | [What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media.](http://arxiv.org/abs/2305.13127) | 本论文介绍了一种使用可解释的人工智能方法在社交媒体中检测抑郁症的方法。该方法创新地检测和解释抑郁症状及其持续时间，并通过大规模数据集的实证分析表明优于其他方法，发现了新的未注意到的症状。 |
| [^82] | [LEA: Beyond Evolutionary Algorithms via Learned Optimization Strategy.](http://arxiv.org/abs/2304.09599) | LEA是一种适应性强且能够有效利用目标任务低保真度信息的学习进化算法，从而比传统进化算法在更少的计算成本下获得更好的解决方案。 |
| [^83] | [Stabilizing Transformer Training by Preventing Attention Entropy Collapse.](http://arxiv.org/abs/2303.06296) | 本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。 |
| [^84] | [Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies.](http://arxiv.org/abs/2303.02034) | 本研究通过理论分析和实验研究，揭示了线性卷积神经网络在学习过程中通过发现数据集的统计结构，并且仅利用数据集中最主导的频率进行发现。 |
| [^85] | [Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition.](http://arxiv.org/abs/2302.13434) | 本论文介绍了一种基于时空Transformer引导的扩散数据增强方法，旨在为基于骨骼的动作识别任务生成高质量和多样化的序列动作。通过引入去噪扩散概率模型（DDPMs），本方法能够生成自然逼真的动作序列。 |
| [^86] | [Active Membership Inference Attack under Local Differential Privacy in Federated Learning.](http://arxiv.org/abs/2302.12685) | 这项研究提出了一种以局部差分隐私为基础的联邦学习中的主动成员推断攻击，该攻击利用恶意参数干扰全局模型并通过非线性决策边界推断客户端的私有训练数据，对客户端的隐私造成显著风险，并发现防止攻击所需的保护隐私噪声会严重损害联邦学习的模型效用。 |
| [^87] | [Unification of popular artificial neural network activation functions.](http://arxiv.org/abs/2302.11007) | 激活函数的统一化表示采用了Mittag-Leffler函数，可以插值不同激活函数、减轻梯度问题，并适用于不同复杂度的神经网络训练。 |
| [^88] | [Diversity Induced Environment Design via Self-Play.](http://arxiv.org/abs/2302.02119) | 本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。 |
| [^89] | [FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model.](http://arxiv.org/abs/2211.07160) | FedTracker是第一个为联邦学习模型提供所有权验证和追溯性的保护框架，采用双层保护方案，并利用持续学习原则提高保护性能。 |
| [^90] | [Revision Transformers: Instructing Language Models to Change their Values.](http://arxiv.org/abs/2210.10332) | 本论文提出了修订Transformer（RiT），旨在解决当前Transformer语言模型中出现的捷径学习和偏见问题，以便更方便地进行模型更新。RiT采用了大规模预训练的语言模型和清晰结构的修订引擎的组合，通过少量的努力和用户互动，可以轻松更新模型的知识。在道德数据集上的实验结果表明RiT在模型修订方面表现出强大的性能。 |
| [^91] | [Self-supervised video pretraining yields human-aligned visual representations.](http://arxiv.org/abs/2210.06433) | 本论文通过自监督训练的视频预训练方法VITO得到了具有人类感知特征的视觉表示，该方法在图像理解和视频理解任务上表现出更好的泛化性和鲁棒性。 |
| [^92] | [Cumulative culture spontaneously emerges in artificial navigators who are social and memory-guided.](http://arxiv.org/abs/2206.06281) | 本文通过研究最小认知架构的人工代理，展示出累积文化可以在社交和记忆引导下自发出现，不依赖于高级认知能力。在这个系统中，经验丰富的导航者可以通过已建立的路径传授给幼稚个体，而这样做对于导航者来说也是有益的。 |
| [^93] | [Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning.](http://arxiv.org/abs/2201.09736) | 本文提出了一种在高维空间中使用随机低秩算法进行价值函数近似的方法，并提出了使用张量表示和PARAFAC分解的在线无模型的张量低秩算法。 |
| [^94] | [Text-based Person Search in Full Images via Semantic-Driven Proposal Generation.](http://arxiv.org/abs/2109.12965) | 本文提出了一种基于语义驱动的提案生成的全场景图像文字人物搜索方法，通过端到端学习优化行人检测、身份识别和视觉-语义特征嵌入任务，并利用语义特征指导区域建议网络关注文本描述的提案。使用跨尺度的视觉-语义嵌入机制来提高性能 |

# 详细

[^1]: 基于视觉识别的生成数据的基准测试与分析

    Benchmarking and Analyzing Generative Data for Visual Recognition. (arXiv:2307.13697v1 [cs.CV])

    [http://arxiv.org/abs/2307.13697](http://arxiv.org/abs/2307.13697)

    这篇论文通过实验研究了利用生成数据在视觉识别中的应用，并提出了一个用于评估生成数据的基准，一个训练-free的度量指标以及与检索数据进行比较揭示生成数据的独特特征的新方法。

    

    大型预训练生成模型的进步扩大了它们作为有效数据生成器在视觉识别中的潜力。本研究深入探讨了生成图像的影响，主要比较了利用外部数据（如生成数据、检索数据、原始数据）的范例。我们的主要贡献包括：1) GenBench构建：我们设计了GenBench，一个包含22个数据集和2548个类别的广泛基准，用于评估不同视觉识别任务中的生成数据。2) CLER分数：为了解决现有度量指标（如FID、CLIP分数）与下游识别性能之间的不足相关性问题，我们提出了CLER，一种无需训练的度量指标，用于指示识别任务训练之前生成数据的效率。3) 新的基准线：将生成数据与来自相同外部池的检索数据进行比较，有助于阐明生成数据的独特特征。4) 外部知识注入：通过注入外部知识，提高生成数据在视觉识别任务中的性能。

    Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition. This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\ie generative \vs retrieval \vs original).  Our key contributions are: \textbf{1) GenBench Construction:} We devise \textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks. \textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\eg, FID, CLIP score) with downstream recognition performance, we propose \textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training. \textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data. \textbf{4) External Knowledge Injection:} By 
    
[^2]: 面料的视觉语言

    The Visual Language of Fabrics. (arXiv:2307.13681v1 [cs.GR])

    [http://arxiv.org/abs/2307.13681](http://arxiv.org/abs/2307.13681)

    这项研究引入了一个新的数据集text2fabric，通过自由文本描述与面料材料相关联，旨在提高材料描述的表达能力。研究表明，人们如何描述面料的紧凑词汇、属性集合和关键结构可以指导推广到其他类型的材料，并且该数据集可以用于专门化大型视觉-语言模型。

    

    我们介绍了text2fabric，一个将自由文本描述与各种面料材料关联起来的新数据集。该数据集包括15,000个自然语言描述，并与3,000个相应的面料材料图像相关联。传统上，材料描述以标签/关键字的形式出现，这限制了它们的表达能力，引入了对适当词汇的已有知识，并最终导致了一种被切割的描述系统。因此，我们研究了使用自由文本来描述材料外观的更适当方式，将面料作为非专家经常处理的常见物品使用案例。通过对数据集的分析，我们确定了从描述中得出的紧凑词汇、属性集合和关键结构。这使我们能够准确理解人们如何描述面料，并为推广到其他类型的材料提供指导。我们还展示了我们的数据集可以使大型视觉-语言模型（如CLI）专门化。

    We introduce text2fabric, a novel dataset that links free-text descriptions to various fabric materials. The dataset comprises 15,000 natural language descriptions associated to 3,000 corresponding images of fabric materials. Traditionally, material descriptions come in the form of tags/keywords, which limits their expressivity, induces pre-existing knowledge of the appropriate vocabulary, and ultimately leads to a chopped description system. Therefore, we study the use of free-text as a more appropriate way to describe material appearance, taking the use case of fabrics as a common item that non-experts may often deal with. Based on the analysis of the dataset, we identify a compact lexicon, set of attributes and key structure that emerge from the descriptions. This allows us to accurately understand how people describe fabrics and draw directions for generalization to other types of materials. We also show that our dataset enables specializing large vision-language models such as CLI
    
[^3]: 关于AI问责政策的探索

    Towards an AI Accountability Policy. (arXiv:2307.13658v1 [cs.CY])

    [http://arxiv.org/abs/2307.13658](http://arxiv.org/abs/2307.13658)

    这份白皮书是对美国国家电信和信息管理局的“AI问责政策评论请求”的回应，提出了一组相互关联的AI问责政策建议。

    

    这份白皮书是对美国国家电信和信息管理局的“AI问责政策评论请求”作出的回应。在回答相关问题的关键句子末尾，提供了要求评论的问题编号的上标。该白皮书提出了一组相互关联的AI问责政策建议。

    This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
    
[^4]: QuickQual: 使用现成预训练模型的轻量、便捷的视网膜图像质量评分

    QuickQual: Lightweight, convenient retinal image quality scoring with off-the-shelf pretrained models. (arXiv:2307.13646v1 [cs.CV])

    [http://arxiv.org/abs/2307.13646](http://arxiv.org/abs/2307.13646)

    QuickQual是一个简单的视网膜图像质量评分方法，它使用现成的预训练模型和支持向量机，并在EyeQ数据集上取得了新的最先进结果，证明视网膜图像质量评分可以通过学习自然图像的通用感知特征来解决。

    

    图像质量对于传统和基于深度学习的视网膜图像分析方法都是一个关键问题，但是识别低质量图像可能耗时且主观。因此，需要自动化的视网膜图像质量评分方法。现有的最先进方法是MCFNet，由三个在不同颜色空间中运行的Densenet121主干组成。MCFNet和同一作者发布的EyeQ数据集对于视网膜图像质量评分来说是一个巨大的进步。我们提出了QuickQual，一个简单的视网膜图像质量评分方法，包括一个现成的ImageNet预训练的Densenet121主干和一个支持向量机（SVM）。QuickQual表现非常好，为EyeQ设定了新的最先进水平（准确率：88.50% vs MCFNet的88.00%；AUC：0.9687 vs 0.9588）。这表明，视网膜图像质量评分可以利用在自然图像中学习的通用感知特征来解决，而不需要在大量眼底图像上进行训练的深度学习模型。

    Image quality remains a key problem for both traditional and deep learning (DL)-based approaches to retinal image analysis, but identifying poor quality images can be time consuming and subjective. Thus, automated methods for retinal image quality scoring (RIQS) are needed. The current state-of-the-art is MCFNet, composed of three Densenet121 backbones each operating in a different colour space. MCFNet, and the EyeQ dataset released by the same authors, was a huge step forward for RIQS. We present QuickQual, a simple approach to RIQS, consisting of a single off-the-shelf ImageNet-pretrained Densenet121 backbone plus a Support Vector Machine (SVM). QuickQual performs very well, setting a new state-of-the-art for EyeQ (Accuracy: 88.50% vs 88.00% for MCFNet; AUC: 0.9687 vs 0.9588). This suggests that RIQS can be solved with generic perceptual features learned on natural images, as opposed to requiring DL models trained on large amounts of fundus images. Additionally, we propose a Fixed Pr
    
[^5]: 强化学习的安全边界

    Safety Margins for Reinforcement Learning. (arXiv:2307.13642v1 [cs.LG])

    [http://arxiv.org/abs/2307.13642](http://arxiv.org/abs/2307.13642)

    本论文提出了一种能够通过计算代理关键性指标来生成安全边界的方法，该方法能够将可能的错误行为的后果与整体性能的预期损失联系起来。在Atari环境中的实验结果表明，随着代理接近失败状态，安全边界减小。

    

    任何自主控制器在某些情况下都可能不安全。能够定量地确定何时会发生这些不安全情况对于及时引入人类监督至关重要，例如货运应用。在这项工作中，我们展示了一个代理的情况的真正关键性可以被稳健地定义为在一些随机动作下奖励的平均减少。可以将实时可计算的代理关键性指标（即，无需实际模拟随机动作的影响）与真正的关键性进行比较，并展示如何利用这些代理指标生成安全边界，将潜在错误行为的后果直接与整体性能的预期损失联系起来。我们在Atari环境中通过APE-X和A3C的学习策略上评估了我们的方法，并展示了随着代理接近失败状态，安全边界的减小。将安全边界整合到监控程序中的创新点在于...

    Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monit
    
[^6]: GPT-3模型是少样本金融推理器

    GPT-3 Models are Few-Shot Financial Reasoners. (arXiv:2307.13617v1 [cs.CL])

    [http://arxiv.org/abs/2307.13617](http://arxiv.org/abs/2307.13617)

    GPT-3模型在金融领域的少样本推理表现有限，需要使用独立的检索模型和逻辑引擎来获得最佳性能。

    

    金融分析是评估公司业绩的重要工具。从业者通过深入的量化分析回答金融问题，从而做出有利可图的投资决策。因此，金融问答是一个需要对数字进行深入推理的问题回答任务。此外，目前尚不清楚预训练语言模型在金融领域的推理能力如何。目前的最新技术需要一个检索模型从文本中收集与金融问题相关的事实，并使用一个生成器来生成有效的金融程序和最终答案。然而，最近的大型语言模型如GPT-3仅仅通过少量示例就实现了广泛任务的最新性能。我们对GPT-3进行了多个实验，发现独立的检索模型和逻辑引擎仍然是实现这一任务的关键组件，尤其是由于金融领域的精确性要求。

    Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of finan
    
[^7]: 基于树突综合的二次神经网络优于传统的人工神经网络

    Dendritic Integration Based Quadratic Neural Networks Outperform Traditional Aritificial Ones. (arXiv:2307.13609v1 [cs.NE])

    [http://arxiv.org/abs/2307.13609](http://arxiv.org/abs/2307.13609)

    提出了一种基于树突综合的二次神经网络(DIQNN)模型，该模型在多种分类任务中表现出优越性能，超过了传统的人工神经网络。引入边界来刻画泛化误差，并将边界整合到损失函数中，加速了测试准确率的改变。

    

    将生物神经元的特性引入人工神经网络以增强计算能力是机器学习领域面临的重大挑战。受最近发现的树突遵循二次综合规则的启发，我们提出了一种新的人工神经网络模型，即基于树突综合的二次神经网络(DIQNN)。该模型在各种分类任务中表现出优越的性能，超过了传统的人工神经网络。为了降低DIQNN的计算成本，我们引入了低秩DIQNN，发现其可以保持原始DIQNN的性能。我们进一步提出了一个边界来刻画泛化误差，并理论上证明这个边界在训练过程中会单调增加。通过数值实验，我们展示了泛化误差与边界之间的一致性。最后，将这个边界整合到损失函数中后，测试准确率的改变确实加速了。

    Incorporating biological neuronal properties into Artificial Neural Networks (ANNs) to enhance computational capabilities poses a formidable challenge in the field of machine learning. Inspired by recent findings indicating that dendrites adhere to quadratic integration rules for synaptic inputs, we propose a novel ANN model, Dendritic Integration-Based Quadratic Neural Network (DIQNN). This model shows superior performance over traditional ANNs in a variety of classification tasks. To reduce the computational cost of DIQNN, we introduce the Low-Rank DIQNN, while we find it can retain the performance of the original DIQNN. We further propose a margin to characterize the generalization error and theoretically prove this margin will increase monotonically during training. And we show the consistency between generalization and our margin using numerical experiments. Finally, by integrating this margin into the loss function, the change of test accuracy is indeed accelerated. Our work cont
    
[^8]: 利用自然语言处理和基于本体的知识图谱发现云渲染农场服务

    Cloud Render Farm Services Discovery Using NLP And Ontology Based Knowledge Graph. (arXiv:2307.13604v1 [cs.DC])

    [http://arxiv.org/abs/2307.13604](http://arxiv.org/abs/2307.13604)

    利用自然语言处理和基于本体的知识图谱，这项研究提出了一种名为RenderSelect的服务发现引擎，用于发现成本效益高且符合功能要求的云渲染农场服务。

    

    云渲染农场服务是针对动画领域的特定云服务平台（PaaS）类型的云服务，提供完整的平台来渲染动画文件。然而，识别成本效益且符合功能要求（如动画软件、所需插件等）的渲染农场服务是一项挑战。本研究提出了一种基于本体的服务发现引擎RenderSelect，用于云渲染农场服务。云渲染农场本体语义上定义了云渲染农场服务之间的关系。采用基于知识的推理算法，包括概念相似性推理、等价推理和数值相似性推理，来确定云服务之间的相似性。该服务发现引擎在三种不同的情景下进行了评估，即a）利用本体帮助，b）

    Cloud render farm services are the animation domain specific cloud services Platform-as-a-Service (PaaS) type of cloud services that provides a complete platform to render the animation files. However, identifying the render farm services that is cost effective and also matches the functional requirements that changes for almost every project like the animation software, plug-ins required etc., is a challenge. This research work proposes an ontology-based service discovery engine named RenderSelect for the cloud render farm services. The cloud render farm ontology semantically defines the relationship among the cloud render farm services. The knowledge-based reasoning algorithms namely, the Concept similarity reasoning, Equivalent reasoning and the Numerical similarity reasoning have been applied to determine the similarity among the cloud services. The service discovery engine was evaluated for finding the services under three different scenarios namely a) with help of the ontology, b
    
[^9]: 在定量双极论证框架中的论证归因解释

    Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks. (arXiv:2307.13582v1 [cs.AI])

    [http://arxiv.org/abs/2307.13582](http://arxiv.org/abs/2307.13582)

    本文提出了一个新的“论证归因解释（AAEs）”理论，用于确定论证对“主题论证”产生的影响，在定量双极论证框架（QBAFs）中填补了解释定量推理结果的空白。

    

    近年来，有几个人提倡论证性可解释人工智能，越来越多的人对论证框架（AFs）的推理结果进行解释产生了兴趣。虽然关于用辩论/争论/对话的扩展语义精神定性地解释AFs的推理结果的研究成果很多，但是在渐进语义下解释AFs的定量推理结果却没有得到太多关注，尽管在应用中广泛使用。本文通过将机器学习中的特征归因精神引入定量双极论证框架（QBAFs）的背景中，提出了一个新的“论证归因解释（AAEs）”理论，用于确定论证对“主题论证”产生的影响，而特征归因则用于确定特征对机器学习模型输出的影响。

    Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs). While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of \emph{extension-based semantics}, explaining the quantitative reasoning outcomes of AFs under \emph{gradual semantics} has not received much attention, despite widespread use in applications. In this paper, we contribute to filling this gap by proposing a novel theory of \emph{Argument Attribution Explanations (AAEs)} by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards \emph{topic argument}s 
    
[^10]: 在通用拟合器时代重新解读生存分析

    Reinterpreting survival analysis in the universal approximator age. (arXiv:2307.13579v1 [cs.LG])

    [http://arxiv.org/abs/2307.13579](http://arxiv.org/abs/2307.13579)

    这篇论文介绍了生存分析在深度学习中的应用，提供了连接分类和回归的方法，并且提出了无需数值积分的通用拟合网络，通过大规模的数值研究证明了其优于其他方法。

    

    生存分析是统计学工具箱中不可或缺的一部分。然而，虽然大多数经典统计领域已经接受了深度学习，但是生存分析直到最近才引起深度学习社区的一些注意。这一最近的发展可能部分受到COVID-19大流行的影响。我们的目标是提供在深度学习中充分利用生存分析潜力所需的工具。一方面，我们讨论了生存分析与分类和回归的关系。另一方面，我们提供了技术工具。我们提供一个新的损失函数、评估指标，以及第一个能够无需数值积分产生生存曲线的通用拟合网络。我们通过大规模的数值研究表明，这个损失函数和模型的性能优于其他方法。

    Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
    
[^11]: 人工智能解释性对人工智能决策的影响

    The Impact of Imperfect XAI on Human-AI Decision-Making. (arXiv:2307.13566v1 [cs.HC])

    [http://arxiv.org/abs/2307.13566](http://arxiv.org/abs/2307.13566)

    本研究通过一个混合方法用户研究，评估了不正确的解释如何影响人类的决策行为，以增进人工智能解释性对人工智能决策的理解。

    

    解释性技术正在快速发展，以改进各种合作工作环境下的人工智能决策。因此，先前的研究评估了决策者与不完美的人工智能协作的方式，研究合适的依赖关系和任务表现，以便设计更加以人为中心的计算机支持的协作工具。一些以人为中心的可解释人工智能（XAI）技术被提出，希望改善决策者与人工智能的合作；然而，这些技术基于先前研究的发现，主要关注错误的人工智能建议的影响。很少有研究承认即使人工智能建议正确，解释也可能是错误的。因此，了解不完美的解释性人工智能如何影响人工智能决策至关重要。在这项工作中，我们通过一个强大的混合方法用户研究，涉及136名参与者，评估了不正确的解释如何影响人类的决策行为。

    Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making beha
    
[^12]: 决策导向学习：基础、现状、基准和未来机会

    Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])

    [http://arxiv.org/abs/2307.13565](http://arxiv.org/abs/2307.13565)

    决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。

    

    决策导向学习（DFL）是一种新兴的机器学习范式，它训练模型以优化决策，在一个端到端的系统中集成了预测和优化。这个范式有望在许多实际应用中革命性地改变决策制定，这些应用在不确定性下运作，在这些决策模型中估计未知参数经常成为一个重要障碍。本文对DFL进行了全面的回顾。它对各种技术进行了深入分析，以整合机器学习和优化模型，引入了一种根据其独特特征来区分DFL方法的分类法，并对这些方法进行了广泛的实证评估，提出了适用于DFL的合适基准数据集和任务。最后，本研究提供了关于DFL研究中当前和潜在未来方向的宝贵见解。

    Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
    
[^13]: 用领域无关规划器和标准表示解决魔方的研究

    On Solving the Rubik's Cube with Domain-Independent Planners Using Standard Representations. (arXiv:2307.13552v1 [cs.AI])

    [http://arxiv.org/abs/2307.13552](http://arxiv.org/abs/2307.13552)

    本文将魔方的表示转换为PDDL语言，使其更具可访问性和易读性。实验结果表明，DeepCubeA可以解决所有不同复杂度的魔方问题，但只有18％是最优解。

    

    魔方是一个众所周知且计算上具有挑战性的谜题，已经激发了人工智能研究人员探索高效的替代表示和问题解决方法。本文首次将魔方的表示转换为流行的PDDL语言，使得领域对PDDL规划器、竞赛和知识工程工具更加可访问和易读。然后我们比较了现有方法的性能。我们发现，在一个可比较的实验中，DeepCubeA可以解决所有不同复杂度的问题，尽管只有18％是最优解。

    Rubik's Cube (RC) is a well-known and computationally challenging puzzle that has motivated AI researchers to explore efficient alternative representations and problem-solving methods. The ideal situation for planning here is that a problem be solved optimally and efficiently represented in a standard notation using a general-purpose solver and heuristics. The fastest solver today for RC is DeepCubeA with a custom representation, and another approach is with Scorpion planner with State-Action-Space+ (SAS+) representation. In this paper, we present the first RC representation in the popular PDDL language so that the domain becomes more accessible to PDDL planners, competitions, and knowledge engineering tools, and is more human-readable. We then bridge across existing approaches and compare performance. We find that in one comparable experiment, DeepCubeA solves all problems with varying complexities, albeit only 18\% are optimal plans. For the same problem set, Scorpion with SAS+ repre
    
[^14]: 一个用于性能效率的规划本体表示和利用规划知识的方法

    A Planning Ontology to Represent and Exploit Planning Knowledge for Performance Efficiency. (arXiv:2307.13549v1 [cs.AI])

    [http://arxiv.org/abs/2307.13549](http://arxiv.org/abs/2307.13549)

    本研究提出了一种使用规划本体表示和利用规划知识的方法来提高自动规划的性能效率。通过构建一个规划本体，并利用国际规划竞赛的数据进行实验，证明了本体能够选择有前景的规划器，并使用从本体中提取的宏观约束来提高它们的性能。

    

    本文考虑了自动规划问题，通过构建一个规划本体，利用国际规划竞赛（IPC）的规划领域和规划器的数据，在两个使用案例中进行了实验，证明了该本体能够选择有前景的规划器，并通过从规划本体中提取的宏观约束来提高它们的性能。

    Ontologies are known for their ability to organize rich metadata, support the identification of novel insights via semantic queries, and promote reuse. In this paper, we consider the problem of automated planning, where the objective is to find a sequence of actions that will move an agent from an initial state of the world to a desired goal state. We hypothesize that given a large number of available planners and diverse planning domains; they carry essential information that can be leveraged to identify suitable planners and improve their performance for a domain. We use data on planning domains and planners from the International Planning Competition (IPC) to construct a planning ontology and demonstrate via experiments in two use cases that the ontology can lead to the selection of promising planners and improving their performance using macros - a form of action ordering constraints extracted from planning ontology. We also make the planning ontology and associated resources avail
    
[^15]: 计算机视觉中的群体活动识别:一项全面的回顾、挑战和未来展望

    Group Activity Recognition in Computer Vision: A Comprehensive Review, Challenges, and Future Perspectives. (arXiv:2307.13541v1 [cs.CV])

    [http://arxiv.org/abs/2307.13541](http://arxiv.org/abs/2307.13541)

    该论文回顾了群体活动识别的技术进展，并着重介绍了全局互动性和活动的识别方法。

    

    群体活动识别是计算机视觉领域的热门话题。通过群体关系识别活动在群体活动识别中起着重要作用。它在视频分析、监控、自动驾驶和理解社交活动等各种场景中具有实际意义。该模型的关键能力包括高效地建模场景中的分层关系，以及准确地提取群体的独特时空特征。鉴于这种技术的广泛适用性，识别群体活动已经引起了重要的研究关注。本文对当前的群体活动识别技术的进展进行了全面的回顾，特别关注全局互动性和活动。首先，我们全面回顾了相关文献和各种群体活动识别方法，从传统方法到基于空间结构、描述符、非深度学习、分层结构的最新方法。

    Group activity recognition is a hot topic in computer vision. Recognizing activities through group relationships plays a vital role in group activity recognition. It holds practical implications in various scenarios, such as video analysis, surveillance, automatic driving, and understanding social activities. The model's key capabilities encompass efficiently modeling hierarchical relationships within a scene and accurately extracting distinctive spatiotemporal features from groups. Given this technology's extensive applicability, identifying group activities has garnered significant research attention. This work examines the current progress in technology for recognizing group activities, with a specific focus on global interactivity and activities. Firstly, we comprehensively review the pertinent literature and various group activity recognition approaches, from traditional methodologies to the latest methods based on spatial structure, descriptors, non-deep learning, hierarchical re
    
[^16]: 光谱引导的多粒度参考视频对象分割

    Spectrum-guided Multi-granularity Referring Video Object Segmentation. (arXiv:2307.13537v1 [cs.CV])

    [http://arxiv.org/abs/2307.13537](http://arxiv.org/abs/2307.13537)

    本论文提出了一种光谱引导的多粒度参考视频对象分割方法，用于解决当前参考视频对象分割中的特征漂移问题。通过直接分割编码特征，并利用视觉细节优化掩膜，以及在频谱域中进行跨模态融合，实现了更有效的多模态表示。最终，将该方法扩展到多对象R-VOS，实现了同时分割视频中多个参考对象的功能，使得R-VOS更快速、更实用。

    

    当前的参考视频对象分割（R-VOS）技术从编码（低分辨率）的视觉-语言特征中提取条件核，用于分割解码后的高分辨率特征。我们发现这会导致显著的特征漂移，使得分割核在正向计算过程中难以察觉。这对分割核的能力产生了负面影响。为了解决漂移问题，我们提出了一种光谱引导的多粒度（SgMg）方法，该方法对编码特征进行直接分割，并使用视觉细节进一步优化掩膜。此外，我们提出了光谱引导的跨模态融合（SCF），在频谱域中执行帧内全局交互，实现有效的多模态表示。最后，我们将SgMg扩展到执行多对象R-VOS，这是一种新的范式，可以同时分割视频中的多个参考对象。这不仅使R-VOS变得更快，而且更实用。

    Current referring video object segmentation (R-VOS) techniques extract conditional kernels from encoded (low-resolution) vision-language features to segment the decoded high-resolution features. We discovered that this causes significant feature drift, which the segmentation kernels struggle to perceive during the forward computation. This negatively affects the ability of segmentation kernels. To address the drift problem, we propose a Spectrum-guided Multi-granularity (SgMg) approach, which performs direct segmentation on the encoded features and employs visual details to further optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion (SCF) to perform intra-frame global interactions in the spectral domain for effective multimodal representation. Finally, we extend SgMg to perform multi-object R-VOS, a new paradigm that enables simultaneous segmentation of multiple referred objects in a video. This not only makes R-VOS faster, but also more practical. Extensive 
    
[^17]: Re-mine, Learn and Reason: 探索语言引导下跨模态语义相关性的人物-物体交互检测

    Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection. (arXiv:2307.13529v1 [cs.CV])

    [http://arxiv.org/abs/2307.13529](http://arxiv.org/abs/2307.13529)

    本论文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识增强人物-物体交互检测，通过再挖掘策略生成更全面的视觉表示，并设计了细粒度的句子和词级对齐以及知识转移策略来解决多对多匹配问题。

    

    人物-物体交互（HOI）检测是一项具有挑战性的计算机视觉任务，需要视觉模型解决人物和物体之间复杂的交互关系，并预测HOI三元组。本文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识来增强HOI检测。首先，我们定性和定量分析了两阶段HOI检测器中交互信息的损失，并提出了一种再挖掘策略来生成更全面的视觉表示。其次，我们设计了更细粒度的句子和词级对齐以及知识转移策略，以有效解决多个交互和多个文本之间的多对多匹配问题。这些策略减轻了多个交互导致的匹配混淆问题。

    Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interact
    
[^18]: FacTool：生成AI中的事实性检测 —— 一种为多任务和多领域场景加强的工具增强框架

    FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. (arXiv:2307.13528v1 [cs.CL])

    [http://arxiv.org/abs/2307.13528](http://arxiv.org/abs/2307.13528)

    提出了FacTool框架，用于检测大型语言模型（如ChatGPT）生成的文本中的事实错误。实验结果表明该方法在知识型问答、代码生成、数学推理和科学文献综述等四个任务中具有良好的效果。

    

    生成式预训练模型的出现方便了高质量文本的合成，但也在识别生成文本中的事实错误方面提出了挑战。本文针对以下问题提出了FacTool框架：（1）越来越多的任务由生成模型处理时，存在着包含事实错误的风险；（2）生成的文本往往很长，缺乏清晰定义的细粒度个体事实；（3）在事实检查过程中缺乏明确的证据。我们在四个不同的任务上进行实验（基于知识的问答、代码生成、数学推理和科学文献综述），证明了该方法的有效性。

    The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method.
    
[^19]: Zshot：一个用于零样本命名实体识别和关系抽取的开源框架

    Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction. (arXiv:2307.13497v1 [cs.CL])

    [http://arxiv.org/abs/2307.13497](http://arxiv.org/abs/2307.13497)

    Zshot是一个开源框架，用于零样本命名实体识别和关系抽取，通过比较不同的最新ZSL方法，支持研究人员和工业界的需求。

    

    零样本学习（ZSL）任务涉及在训练过程中未见过的文本中识别实体或关系。由于特定领域中标注数据的稀缺性，ZSL已成为一个重要的研究领域，并且在近年来应用范围已大幅增长。随着大型预训练语言模型的出现，提出了许多新的方法，ZSL性能显著提升。研究界和工业界对一个全面支持最新方法和预训练模型开发和可访问性的ZSL框架的需求不断增长。本研究提出了一个名为Zshot的创新ZSL框架，旨在解决上述挑战。我们的主要目标是提供一个平台，允许研究人员使用标准基准数据集比较不同的最新ZSL方法。此外，我们设计了一个支持工业界的框架，具备易用性、灵活性和可扩展性。

    The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readi
    
[^20]: Duet: 高效且可扩展的混合神经关系理解

    Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])

    [http://arxiv.org/abs/2307.13494](http://arxiv.org/abs/2307.13494)

    Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。

    

    基于概率分布估计的基数估计方法相较于传统方法取得了高精度的估计结果。然而，最先进的方法由于在处理范围查询时使用的采样方法而导致估计成本较高。此外，这种采样方法也使得它们难以区分，因此来自查询工作负载的监督信号很难训练模型以提高基数估计的准确性。在本文中，我们提出了一种新的混合确定性建模方法（Duet）用于基数估计问题，与以前的方法相比，具有更好的效率和可扩展性。Duet可以以更低的时间和内存成本直接估计范围查询的基数，并且以可区分的形式呈现。由于此方法的预测过程是可微分的，我们可以将估计误差较大的查询纳入训练过程以进行改进。

    Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
    
[^21]: 通过将基于过程的模型与机器学习相结合进行作物产量预测

    Integrating processed-based models and machine learning for crop yield prediction. (arXiv:2307.13466v1 [cs.LG])

    [http://arxiv.org/abs/2307.13466](http://arxiv.org/abs/2307.13466)

    本研究通过将基于过程的作物生长模型与机器学习相结合，提出了一种混合元模建方法用于马铃薯产量预测。该方法通过使用合成数据预训练卷积神经网络，并使用观测数据进行微调，在模拟应用和真实数据测试中均取得了竞争力的预测结果。

    

    作物产量预测通常涉及使用理论驱动的基于过程的作物生长模型，这些模型在校准本地环境方面往往较为困难，或者使用需要大量数据集的数据驱动的机器学习方法。在这项工作中，我们使用一种混合元模建方法研究了马铃薯产量预测。我们采用作物生长模型生成合成数据来（预）训练卷积神经网络，然后使用观测数据进行微调。在模拟应用中，我们的元模建方法比纯粹的数据驱动方法获得更好的预测结果。在来自田间试验（n=303）和商业田地（n=77）的真实数据测试中，元模建方法相对于作物生长模型具有竞争力的结果。然而，在后者中，这两种模型的表现都不如由领域专家设计的手动选择特征集和专门预处理的简单线性回归模型。

    Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts
    
[^22]: 解锁视觉媒体的情感世界：理解情感的科学、研究和影响的概述

    Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion. (arXiv:2307.13463v1 [cs.CV])

    [http://arxiv.org/abs/2307.13463](http://arxiv.org/abs/2307.13463)

    本文概述了解锁视觉媒体情感世界的科学、研究和影响，探讨了计算机视觉领域中对视觉媒体中情感的自动理解的挑战与进展。

    

    人工情感智能技术的出现正在革新计算机和机器人领域，实现了一种对人类行为的沟通和理解的新水平，这曾经被认为是不可能的。虽然深度学习的最新进展已经改变了计算机视觉领域，但对视觉媒体中引发或表达的情感进行自动理解仍处于起步阶段。这一困境源于“情感”缺乏普遍接受的定义，加上情感的主观性和微妙的细微差别。在本文中，我们提供了一份全面的、多学科的视觉媒体情感分析领域概述，借鉴了心理学、工程学和艺术的见解。我们首先探讨情感的心理学基础和从图像和视频中理解情感的计算原理。然后，我们回顾了最新的研究和系统。

    The emergence of artificial emotional intelligence technology is revolutionizing the fields of computers and robotics, allowing for a new level of communication and understanding of human behavior that was once thought impossible. While recent advancements in deep learning have transformed the field of computer vision, automated understanding of evoked or expressed emotions in visual media remains in its infancy. This foundering stems from the absence of a universally accepted definition of "emotion", coupled with the inherently subjective nature of emotions and their intricate nuances. In this article, we provide a comprehensive, multidisciplinary overview of the field of emotion analysis in visual media, drawing on insights from psychology, engineering, and the arts. We begin by exploring the psychological foundations of emotion and the computational principles that underpin the understanding of emotions from images and videos. We then review the latest research and systems within th
    
[^23]: 量子随机访问内存的基本因果界限

    Fundamental causal bounds of quantum random access memories. (arXiv:2307.13460v1 [quant-ph])

    [http://arxiv.org/abs/2307.13460](http://arxiv.org/abs/2307.13460)

    本研究通过采用相对论量子场论和量子多体系统中的Lieb-Robinson界限，批判性地探讨了基于因果性的快速量子存储器的内在界限。研究表明在混合量子声学系统中，QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。

    

    量子设备应遵守量子物理原则。量子随机访问内存（QRAM）是许多重要量子算法（如线性代数、数据搜索和机器学习）的基本组件，通常被认为在给定N个量子比特时，可以以O(log N)的电路深度处理O(N)的数据量。然而，当处理大量量子比特的相互作用局部的量子材料时，这一主张似乎违反了相对论原理。在我们的研究中，我们批判性地探讨了基于因果性的快速量子存储器的内在界限，利用相对论量子场论和Lieb-Robinson界限在量子多体系统中。在本文中，我们考虑了一个在混合量子声学系统中高效的硬件设计的QRAM。假设时钟周期约为10^{-3}秒，格子间距约为1微米，我们展示了QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。

    Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimens
    
[^24]: 多智能体路径规划中的蒙特卡洛树搜索：初步结果

    Monte-Carlo Tree Search for Multi-Agent Pathfinding: Preliminary Results. (arXiv:2307.13453v1 [cs.AI])

    [http://arxiv.org/abs/2307.13453](http://arxiv.org/abs/2307.13453)

    本研究探讨了如何利用蒙特卡洛树搜索（MCTS）解决多智能体路径规划问题，并引入了一种针对该问题的原创MCTS变种。该方法使用各自的路径辅助智能体实现目标达成，并允许它们根据需要离开路径避免碰撞。

    

    在这项工作中，我们研究了一个众所周知且具有挑战性的多智能体路径规划问题，当一组智能体被限制在一个图中，每个智能体被分配一个唯一的起始和目标顶点，任务是找到一组无碰撞路径（每个智能体一个路径），使得每个智能体都达到其相应的目标。我们研究如何利用蒙特卡洛树搜索（MCTS）解决该问题。虽然MCTS已被证明在广泛的问题上表现出卓越的性能，如对弈游戏（例如，围棋、国际象棋等） 的对弈，发现更快的矩阵乘法算法等，但其在此问题上的应用尚未得到充分研究。为此，我们引入了一种针对多智能体路径规划的原创变种MCTS。我们方法的关键是如何计算指导 MCTS 的奖励。具体而言，我们使用各自的路径来辅助智能体实现目标达成行为，并允许它们在需要时离开路径避免碰撞。

    In this work we study a well-known and challenging problem of Multi-agent Pathfinding, when a set of agents is confined to a graph, each agent is assigned a unique start and goal vertices and the task is to find a set of collision-free paths (one for each agent) such that each agent reaches its respective goal. We investigate how to utilize Monte-Carlo Tree Search (MCTS) to solve the problem. Although MCTS was shown to demonstrate superior performance in a wide range of problems like playing antagonistic games (e.g. Go, Chess etc.), discovering faster matrix multiplication algorithms etc., its application to the problem at hand was not well studied before. To this end we introduce an original variant of MCTS, tailored to multi-agent pathfinding. The crux of our approach is how the reward, that guides MCTS, is computed. Specifically, we use individual paths to assist the agents with the the goal-reaching behavior, while leaving them freedom to get off the track if it is needed to avoid 
    
[^25]: 一个行为变换器用于机器人与非静止人类之间的有效协作

    A behavioural transformer for effective collaboration between a robot and a non-stationary human. (arXiv:2307.13447v1 [cs.RO])

    [http://arxiv.org/abs/2307.13447](http://arxiv.org/abs/2307.13447)

    该论文提出了一个行为变换器(BeTrans)框架，该框架能够使机器人能够更好地预测人类的非静态行为，并通过使用顺序数据来适应新的非静态的人类代理。实验证明BeTrans在协作环境中效果显著，比现有技术更快地适应了非静态的模拟人类代理。

    

    人机协作的一个关键挑战是人类行为的非静止性，由于其行为的变化所产生的非静止性会改变环境的转换，从而阻碍人机协作。我们提出了一个有原则的元学习框架，探索机器人如何更好地预测人类行为，并因此解决非静止性问题。基于这个框架，我们开发了行为变换器(BeTrans)。BeTrans是一个条件变换器，能够使机器人代理快速适应具有非静态行为的新的人类代理，因为它在顺序数据上具有显著的性能。我们训练了BeTrans在模拟的具有不同系统偏差的人类代理中，在协作环境中进行了原始的可定制环境的实验，显示出BeTrans与模拟的人类代理有效协作，并比现有技术更快地适应了非静态的模拟人类代理。

    A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
    
[^26]: 关于注意力网络学习动态的研究

    On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])

    [http://arxiv.org/abs/2307.13421](http://arxiv.org/abs/2307.13421)

    本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。

    

    注意力模型通常通过优化三个标准损失函数之一来学习，分别称为软注意力、硬注意力和潜变量边际似然（LVML）注意力。这三种范式都是为了达到相同的目标，即找到两个模型：一个“焦点”模型，用于“选择”输入中的正确“片段”，和一个“分类”模型，用于将选定的片段处理成目标标签。然而，它们在所选择的片段聚合方式上存在显著差异，导致了不同的动态和最终结果。我们观察到使用这些范式学习的模型具有独特的特征，并将其解释为在焦点模型固定时，分类模型在梯度下降下的演化所致。我们还在一个简单的设置中分析了这些范式，并推导出梯度流下参数轨迹的闭式表达式。在软注意力损失下，焦点模型在初始化阶段快速改善。

    Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
    
[^27]: 走向缩小数字语言鸿沟的努力

    Towards Bridging the Digital Language Divide. (arXiv:2307.13405v1 [cs.CL])

    [http://arxiv.org/abs/2307.13405](http://arxiv.org/abs/2307.13405)

    本文旨在探讨“语言偏见”现象，即多语言处理系统存在对某些语言的硬编码倾向，忽视了语言复杂性和语言社区的需求，阻碍了AI技术覆盖到“资源有限的语言”。

    

    众所周知，当前基于人工智能的语言技术，如语言模型、机器翻译系统、多语言字典和语料库，主要关注全球2-3%的最常用语言。最近的研究努力致力于将AI技术扩大到“资源有限的语言”。我们论文的目标是引起人们对一种我们称之为“语言偏见”的现象的关注：多语言语言处理系统往往表现出对某些语言的硬编码倾向，这往往是无意识和隐藏的。即使在类似的测试条件下，语言偏见也会导致不同语言的性能不均衡。我们表明，具有偏见的技术往往是由于研发方法论没有对所表示的语言复杂性进行恰当处理而产生的，甚至会因忽视多样性宝贵的方面以及语言社区的需求而引起伦理问题。

    It is a well-known fact that current AI-based language technology -- language models, machine translation systems, multilingual dictionaries and corpora -focuses on the world's 2-3% most widely spoken languages. Recent research efforts have attempted to expand the coverage of AI technology to `under-resourced languages.' The goal of our paper is to bring attention to a phenomenon that we call linguistic bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages. Linguistic bias is manifested in uneven per-language performance even in the case of similar test conditions. We show that biased technology is often the result of research and development methodologies that do not do justice to the complexity of the languages being represented, and that can even become ethically problematic as they disregard valuable aspects of diversity as well as the needs of the language communities the
    
[^28]: 无需执行预测代码覆盖率

    Predicting Code Coverage without Execution. (arXiv:2307.13383v1 [cs.SE])

    [http://arxiv.org/abs/2307.13383](http://arxiv.org/abs/2307.13383)

    该论文提出了一种无需执行代码即可预测代码覆盖率的方法，并建立了一个基准任务来评估语言模型理解代码能力。

    

    代码覆盖率是一种广泛应用的衡量程序元素执行情况的度量指标，包括语句或分支的执行情况。计算代码覆盖率需要消耗大量资源，需要构建和执行代码，并且还需要额外的开销进行仪器化。此外，计算任何代码片段的覆盖率需要整个程序的上下文。使用机器学习来分摊这个昂贵的过程可以降低代码覆盖率的成本，只需要源代码上下文，而代码覆盖率预测任务可以成为评估模型理解代码能力的新颖基准。我们提出了一个称为大型语言模型（LLMs）代码覆盖率预测的新颖基准任务。我们通过确定给定测试用例和输入的哪些方法行被执行来形式化评估LLMs在理解代码执行方面的能力。我们整理并发布了一个名为COVERAGEEVAL的数据集，通过执行测试和代码来获取数据。

    Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code. We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs). We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs. We curate and release a dataset we call COVERAGEEVAL by executing tests and code from
    
[^29]: 用更长更好的上下文理解将模型赋能

    Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])

    [http://arxiv.org/abs/2307.13365](http://arxiv.org/abs/2307.13365)

    本文研究了大语言模型（LLMs）内的信息传递，并提出了一种名为注意力转移的技术，该技术能够使模型在不增加训练或对生成流畅性的影响的情况下实现更长更好的上下文理解。

    

    最近，随着大量的大语言模型（LLMs）的出现，人工智能的实现进入了一个新的时代。无论这些模型自身的容量和结构如何，都存在对LLMs具有更长更复杂上下文的增强理解的需求，而模型通常在处理超出其理解能力范围的句子序列时会遇到上限，导致产生离题或混乱的回答。虽然最近有几项工作试图以不同的方式解决这个问题，但它们很少关注“为什么模型无法自行弥补或增强自己的能力”。在本文中，我们对LLMs内的信息传递性质进行了深入研究，并提出了一种名为注意力转移的新技术。这种技术能够使模型在最小化额外训练或对生成流利性的影响的情况下实现更长更好的上下文理解。我们的实验证明了这一点。

    Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
    
[^30]: 人类和卷积神经网络在场景分类中是否关注相似区域：任务和图像类型的影响

    Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type. (arXiv:2307.13345v1 [cs.CV])

    [http://arxiv.org/abs/2307.13345](http://arxiv.org/abs/2307.13345)

    本研究调查了人类注意力与CNN注意力之间的相似性在场景分类中受任务和图像类型的影响，结果发现任务的意图和图像的特征都对二者的相似性有所影响。

    

    深度学习模型，如卷积神经网络（CNN），是强大的图像分类器，但是什么因素决定它们是否关注与人类相似的图像区域呢？尽管以前的研究集中在技术因素上，但人类注意力的影响因素仍知之甚少。在本研究中，我们调查了用于引发人类注意力图的任务如何与图像特征相互作用，从而调节人类和CNN之间的相似性。我们变化了人类任务的意图，从在分类过程中的自发注视到有意的注视指向，再到手动区域选择。此外，我们还改变了要进行分类的图像类型，包括单个显著对象、由对象组合而成的室内场景，以及没有明确定义类别的景观。以这种方式生成的人类注意力图与通过可解释的人工智能（Grad-CAM）揭示出的CNN注意力图进行了比较。

    Deep Learning models like Convolutional Neural Networks (CNN) are powerful image classifiers, but what factors determine whether they attend to similar image areas as humans do? While previous studies have focused on technological factors, little is known about the role of factors that affect human attention. In the present study, we investigated how the tasks used to elicit human attention maps interact with image characteristics in modulating the similarity between humans and CNN. We varied the intentionality of human tasks, ranging from spontaneous gaze during categorization over intentional gaze-pointing up to manual area selection. Moreover, we varied the type of image to be categorized, using either singular, salient objects, indoor scenes consisting of object arrangements, or landscapes without distinct objects defining the category. The human attention maps generated in this way were compared to the CNN attention maps revealed by explainable artificial intelligence (Grad-CAM). 
    
[^31]: 通过基于梯度的特征归因分析大型语言模型中的思维链启发

    Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions. (arXiv:2307.13339v1 [cs.CL])

    [http://arxiv.org/abs/2307.13339](http://arxiv.org/abs/2307.13339)

    通过基于梯度的特征归因方法，研究发现思维链启发在大型语言模型中并没有增加与语义相关标记的重要性，但提高了与问题相关标记的重要性分数的鲁棒性。

    

    在各种问答任务中，已经证明思维链启发在大型语言模型的准确性方面有实际的改善。然而，为了确保这种现象是期望的模型行为的结果，理解为何思维链启发有效非常重要，但是目前很少有研究探讨这个问题。我们通过利用基于梯度的特征归因方法来回答这个问题，该方法产生了衡量输入标记对模型输出影响的重要性分数。具体而言，我们探索了几个开源的大型语言模型，以研究思维链启发是否会影响它们分配给特定输入标记的相对重要性。我们的结果表明，与标准的少样本启发相比，思维链启发并未增加分配给语义相关标记的重要性分数的大小，但它提高了分配给问题相关标记的重要性分数的鲁棒性。

    Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to quest
    
[^32]: 在错误指定的离策略值函数估计中的最佳逼近因子

    The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v1 [cs.LG])

    [http://arxiv.org/abs/2307.13332](http://arxiv.org/abs/2307.13332)

    本文研究了在线性离策略值函数估计中的逼近因子，并在多种设置下建立了最优的渐近逼近因子，这些因子决定了离策略评估的困难程度。

    

    已经知道，在强化学习中的理论保证在函数逼近的错误指定中会出现乘法放大因子。然而，这些\emph{逼近因子}的性质，特别是在给定的学习问题中的最佳形式，仍然不为人所了解。在本文中，我们研究了这个问题在线性离策略值函数估计中的广泛设置中的逼近因子，其中仍有许多开放问题。我们研究了在多种设置下的逼近因子，例如加权$L_2$范数（其中加权是离线状态分布），$L_\infty$范数，状态别名的存在与否以及对状态空间的全面与部分覆盖。对于所有这些设置，我们建立了最优的渐近逼近因子（至多常数）。特别地，我们的界限确定了$L_2(\mu)$范数的两个依赖于实例的因子和$L_\infty$范数的一个因子，它们被证明决定了离策略评估的困难程度。

    Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evalua
    
[^33]: 通过潜在任务表示和机器人技能适应学习自主超声

    Learning Autonomous Ultrasound via Latent Task Representation and Robotic Skills Adaptation. (arXiv:2307.13323v1 [cs.RO])

    [http://arxiv.org/abs/2307.13323](http://arxiv.org/abs/2307.13323)

    本文提出了一种通过潜在任务表示和机器人技能适应来学习自主超声的方法。在离线阶段，将多模式超声技能合并为低维概率模型。在在线阶段，概率模型将选择和评估最优解决方案。

    

    随着医学超声成为如今流行的检查方法，机器人超声系统可以促进扫描过程，防止专业的超声医生重复乏味的工作。尽管最近取得了进展，但使机器人能够自主完成超声检查仍然具有挑战性，这主要是因为缺乏合适的任务表示方法以及在不同患者之间推广学习技能的适应方法。为了解决这些问题，本文提出了自主超声的潜在任务表示和机器人技能适应方法。在离线阶段，通过全自我监督框架，将多模式超声技能合并并封装为低维概率模型，该模型考虑了临床证明的超声图像、探头定向和接触力。在在线阶段，概率模型将选择和评估最优解决方案。

    As medical ultrasound is becoming a prevailing examination approach nowadays, robotic ultrasound systems can facilitate the scanning process and prevent professional sonographers from repetitive and tedious work. Despite the recent progress, it is still a challenge to enable robots to autonomously accomplish the ultrasound examination, which is largely due to the lack of a proper task representation method, and also an adaptation approach to generalize learned skills across different patients. To solve these problems, we propose the latent task representation and the robotic skills adaptation for autonomous ultrasound in this paper. During the offline stage, the multimodal ultrasound skills are merged and encapsulated into a low-dimensional probability model through a fully self-supervised framework, which takes clinically demonstrated ultrasound images, probe orientations, and contact forces into account. During the online stage, the probability model will select and evaluate the opti
    
[^34]: 通过LED照明调制对人脸识别系统进行不可察觉的物理攻击

    Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation. (arXiv:2307.13294v1 [cs.CV])

    [http://arxiv.org/abs/2307.13294](http://arxiv.org/abs/2307.13294)

    本论文提出了一种通过LED照明调制对人脸识别系统进行不可察觉的物理攻击，通过快速强度调制生成难以察觉的亮度变化，并利用图像传感器的卷帘快门效应向捕获的人脸图像中注入亮度信息扰动。

    

    尽管人脸识别在我们的日常生活中开始扮演重要角色，但我们需要注意到，基于数据驱动的人脸识别视觉系统容易受到对抗性攻击。然而，当前两种对抗性攻击，即数字攻击和物理攻击，都有缺点，前者不实用，后者显眼、计算量大且不可执行。为解决这些问题，我们提出了一种实用、可执行、不显眼且计算量较低的对抗性攻击方法，基于LED照明调制。为了欺骗系统，该攻击方法通过对场景LED照明进行快速强度调制，在人眼看不到的范围内生成难以察觉的亮度变化，并利用CMOS图像传感器的卷帘快门效应，向捕获的人脸图像中注入亮度信息扰动。总之，我们提出了一种用于人脸检测的拒绝服务（DoS）攻击以及一种用于人脸验证的躲避攻击。

    Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks. However, the current two categories of adversarial attacks, namely digital attacks and physical attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and inexecutable. To address the issues, we propose a practical, executable, inconspicuous and low computational adversarial attack based on LED illumination modulation. To fool the systems, the proposed attack generates imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images. In summary,we present a denial-of-service (DoS) attack for face detection and a dodging attack for face verifica
    
[^35]: 基于曲率的变压器用于分子属性预测

    Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])

    [http://arxiv.org/abs/2307.13275](http://arxiv.org/abs/2307.13275)

    该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。

    

    分子性质的预测是基于人工智能的药物设计领域中最重要且具有挑战性的任务之一。在当前主流的方法中，用于训练DNN模型的最常用特征表示基于SMILES和分子图，尽管这些方法简洁高效，但也限制了对空间信息的捕捉能力。在本研究中，我们提出了基于曲率的变压器，通过引入 Ricci 曲率离散化，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。为了将曲率嵌入模型中，在注意力得分计算期间，我们将图的曲率信息作为位置编码添加到节点特征中。这种方法可以在不改变原始网络结构的情况下，将曲率信息引入图数据，并且有潜力扩展到其他模型。我们进行了实验证明了这种方法的有效性。

    The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
    
[^36]: 无偏重量最大化

    Unbiased Weight Maximization. (arXiv:2307.13270v1 [cs.LG])

    [http://arxiv.org/abs/2307.13270](http://arxiv.org/abs/2307.13270)

    这项研究提出了一种无偏重量最大化的方法，通过用出站权重的范数替换单元的奖励信号，实现了更加高效的结构性信用分配。

    

    训练人工神经网络的一种生物学合理的方法是将每个单元视为随机强化学习代理，从而将网络视为代理团队。因此，所有单元都可以通过REINFORCE进行学习，这是一种局部学习规则，通过全局奖励信号进行调节，更加符合生物观察到的突触可塑性形式。然而，由于缺乏有效的结构性信用分配，这种学习方法通常速度较慢，且随着网络规模的增大而扩展性较差，因为单个奖励信号被广播给所有单元而不考虑个体贡献。提出的解决方案，重量最大化，用出站权重的范数替换单元的奖励信号，从而允许每个隐藏单元最大化出站权重的范数，而不是全局奖励信号。在本研究报告中，我们分析了重量最大化的理论属性并提出了一种变体，无偏重量最大化。

    A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximizati
    
[^37]: LoraHub: 通过动态LoRA组合实现高效的任务通用性

    LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v1 [cs.CL])

    [http://arxiv.org/abs/2307.13269](http://arxiv.org/abs/2307.13269)

    本文研究了LoRA组合在跨任务通用性上的可行性，并提出了LoraHub框架，能够通过组合不同任务上训练的LoRA模块，实现对未见任务的可适应性性能。实验结果表明，LoraHub在少样本场景中能够有效模拟上下文学习的性能，而无需上下文示例。

    

    低秩适应（LoRA）常常被用于对新任务进行大型语言模型（LLM）的微调。本文研究了LoRA组合在跨任务通用性上的可行性，并介绍了LoraHub，这是一个为目的性组装在不同给定任务上训练的LoRA模块的战略框架，旨在实现对未见任务的可适应性性能。仅凭借来自新任务的几个示例，LoraHub可以灵活地组合多个LoRA模块，消除了对人类专业知识的需求。值得注意的是，这种组合既不需要额外的模型参数，也不需要梯度。我们从Big-Bench Hard（BBH）基准测试中得出的实证结果表明，LoraHub在少样本场景中可以有效地模拟上下文学习的性能，在每个推理输入旁边不需要上下文示例。我们的研究的一个重要贡献是培育一个LoRA社区，用户可以在其中分享他们训练的LoRA模块。

    Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA module
    
[^38]: 仅使用正标签的资源受限物联网环境中的联邦分割学习

    Federated Split Learning with Only Positive Labels for resource-constrained IoT environment. (arXiv:2307.13266v1 [cs.LG])

    [http://arxiv.org/abs/2307.13266](http://arxiv.org/abs/2307.13266)

    在资源受限的物联网环境中，我们提出了带有正标签的分割学习（SFPL）方法，通过对数据进行随机洗牌来改善多类别分类深度学习模型在联邦分割学习中的效果。

    

    分布式协作机器学习（DCML）是物联网领域中训练深度学习模型的一种有前景的方法，因为数据分布在多个设备上。这种方法的一个主要优点是通过消除原始数据的集中聚合来改善数据隐私，同时也为具有低计算能力的物联网设备提供动力。在DCML框架中的各种技术中，称为splitfed学习（SFL）的联邦分割学习是在设备具有有限计算能力时进行高效训练和测试的最合适的方法。然而，当资源受限的物联网设备只有正标记数据时，SFL中的多类别分类深度学习模型无法收敛或提供次优结果。为了克服这些挑战，我们提出了带有正标签的splitfed学习（SFPL）。SFPL在将客户端接收到的破碎数据提供给服务器之前，对其应用随机洗牌功能。

    Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server fo
    
[^39]: 结构化信用分配与协调探索

    Structural Credit Assignment with Coordinated Exploration. (arXiv:2307.13256v1 [cs.LG])

    [http://arxiv.org/abs/2307.13256](http://arxiv.org/abs/2307.13256)

    本文提出一种结构化信用分配与协调探索的方法，该方法将每个单元视为强化学习代理，并通过全局奖励信号调节局部学习规则。通过提高结构化信用分配的效率来改进人工神经网络的训练。

    

    一种合理的训练人工神经网络(ANN)的方法是将每个单元视为一个随机强化学习(RL)代理，从而将网络视为代理团队。因此，所有单元都可以通过REINFORCE学习，这是一种通过全局奖励信号调节的局部学习规则，更接近生物观察到的突触可塑性形式。然而，这种学习方法往往较慢，无法很好地适应网络的规模。这种效率低下主要是由两个因素造成的，阻碍了有效的结构化信用分配：(i)所有单元独立探索网络，(ii)使用单一奖励来评估所有单元的行动。因此，旨在改善结构化信用分配的方法通常可分为两类。第一类包括允许单元之间进行协调探索的算法，例如MAP传播。第二类涵盖了计算结构性信用分配的算法。

    A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that comput
    
[^40]: GaPro: 使用高斯过程作为伪标签生成器的基于3D点云的盒子监督实例分割

    GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers. (arXiv:2307.13251v1 [cs.CV])

    [http://arxiv.org/abs/2307.13251](http://arxiv.org/abs/2307.13251)

    本文提出了GaPro，一种基于3D点云的实例分割方法，使用轴对齐的盒子监督。通过生成伪标签并训练网络，利用高斯过程解决了盒子重叠时的不确定性问题，并在实验证明其优于先前方法。

    

    3D点云的实例分割（3DIS）是计算机视觉中一个长期存在的挑战，目前的先进方法主要基于全监督。由于标注真实密集实例掩码是繁琐且昂贵的，因此使用弱监督解决3DIS变得更加实用。在本文中，我们提出了GaPro，一种新的基于3D点云的实例分割方法，使用轴对齐的3D包围盒监督。我们的两步方法包括从盒子注释中生成伪标签，并使用结果标签训练3DIS网络。此外，我们采用自训练策略进一步提高我们方法的性能。我们设计了一个有效的高斯过程，可以从边界框生成伪实例掩码，并在它们重叠时解决不确定性问题，使得伪实例掩码具有其不确定性值。我们的实验证明，GaPro优于先前的弱监督3D实例分割方法，并具有竞争性能。

    Instance segmentation on 3D point clouds (3DIS) is a longstanding challenge in computer vision, where state-of-the-art methods are mainly based on full supervision. As annotating ground truth dense instance masks is tedious and expensive, solving 3DIS with weak supervision has become more practical. In this paper, we propose GaPro, a new instance segmentation for 3D point clouds using axis-aligned 3D bounding box supervision. Our two-step approach involves generating pseudo labels from box annotations and training a 3DIS network with the resulting labels. Additionally, we employ the self-training strategy to improve the performance of our method further. We devise an effective Gaussian Process to generate pseudo instance masks from the bounding boxes and resolve ambiguities when they overlap, resulting in pseudo instance masks with their uncertainty values. Our experiments show that GaPro outperforms previous weakly supervised 3D instance segmentation methods and has competitive perfor
    
[^41]: RoSAS:具有抗污染连续监督的深度半监督异常检测

    RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision. (arXiv:2307.13239v1 [cs.LG])

    [http://arxiv.org/abs/2307.13239](http://arxiv.org/abs/2307.13239)

    本论文提出了一种具有抗污染连续监督的深度半监督异常检测方法，通过质量插值方法创造了连续异常度标签的新数据样本，解决了半监督异常检测方法中的异常污染和离散监督信息利用不充分的问题。

    

    半监督异常检测方法利用一些异常样本，与无监督模型相比，显著提高了性能。然而，它们仍然存在两个限制：1) 未标记的异常（即异常污染）可能在将所有未标记数据用作内点进行模型训练时误导学习过程; 2) 只利用离散的监督信息（如二进制或顺序数据标签），这导致异常分数的子优学习，实质上采用连续分布。因此，本文提出了一种新颖的半监督异常检测方法，设计了"抗污染连续监督信号"。具体而言，我们提出了一种质量插值方法来扩散标记异常的异常程度，从而创建带有连续异常度标签的新数据样本。同时，通过组合具有不同程度异常程度的数据，可以覆盖受污染的区域。

    Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data wit
    
[^42]: 为所有人设计的多层大型语言模型

    Multilevel Large Language Models for Everyone. (arXiv:2307.13221v1 [cs.CV])

    [http://arxiv.org/abs/2307.13221](http://arxiv.org/abs/2307.13221)

    本文提出了一个多层大型语言模型的设计，将通用的和特定的模型结合在一起，可以根据用户的个人输入和互联网信息相互改进。这种模型受到人类大脑功能的启发，具有全局、领域和用户级模型，可以在本地机器上运行以保护用户隐私。

    

    在过去几年中，大型语言模型取得了显著的进展。然而，它们要么是通用的，要么是领域特定的，将社区划分为不同的群体。在本文中，我们将这些大型语言模型统一到一个更大的地图中，将通用的和特定的模型连接在一起，并根据用户的个人输入和来自互联网的信息相互改进。将多个大型语言模型链接在一起的思想受到了人类大脑功能的启发。大脑皮层上的特定区域对于某些低层次功能是特定的。而这些区域可以共同工作，实现更复杂的高层功能。人类大脑皮层上的这种行为为设计包含全局、领域和用户级模型的多层大型语言模型提供了新思路。用户级模型在本地机器上运行，以实现高效的响应并保护用户的隐私。这样的多层模型可以减少计算资源的消耗。

    Large language models have made significant progress in the past few years. However, they are either generic {\it or} field specific, splitting the community into different groups. In this paper, we unify these large language models into a larger map, where the generic {\it and} specific models are linked together and can improve each other, based on the user personal input and information from the internet. The idea of linking several large language models together is inspired by the functionality of human brain. The specific regions on the brain cortex are specific for certain low level functionality. And these regions can jointly work together to achieve more complex high level functionality. Such behavior on human brain cortex sheds the light to design the multilevel large language models that contain global level, field level and user level models. The user level models run on local machines to achieve efficient response and protect the user's privacy. Such multilevel models reduc
    
[^43]: 一次多用：物理信息合成数据增强了快速MRI重建中的可推广深度学习

    One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction. (arXiv:2307.13220v1 [eess.IV])

    [http://arxiv.org/abs/2307.13220](http://arxiv.org/abs/2307.13220)

    提出了一种名为PISF的物理信息合成数据学习框架，该框架使得快速MRI重建中的多场景可推广深度学习成为可能。

    

    磁共振成像（MRI）是一种提供无辐射、丰富和多样化有关整个人体医学诊断信息的主要放射学方法，但其扫描时间较长。通过k空间欠采样可以显著减少扫描时间，但需要在图像重建中去除引入的伪影。虽然深度学习（DL）已经成为快速MRI图像重建的有力工具，但其在多种成像场景中的潜力尚未充分利用。这是因为不仅收集大规模和多样化的真实训练数据通常昂贵且受限于隐私，现有的DL方法还难以处理训练数据和目标数据之间实际上不可避免的不匹配。在这里，我们提出了一种物理信息合成数据学习框架（PISF）用于快速MRI，它是第一个仅使用一个训练模型实现可推广的多场景MRI重建的方法。

    Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 
    
[^44]: 对抗式深度对冲：无需价格过程建模的对冲学习

    Adversarial Deep Hedging: Learning to Hedge without Price Process Modeling. (arXiv:2307.13217v1 [q-fin.CP])

    [http://arxiv.org/abs/2307.13217](http://arxiv.org/abs/2307.13217)

    本研究提出了一个新的对抗式深度对冲框架，用于在不完全市场中进行衍生品对冲。通过对冲者和生成器的对抗训练，该方法能够学习出无需进行价格过程建模的稳健对冲者。

    

    深度对冲是一个基于深度学习的衍生品对冲框架，用于不完全市场中的对冲。深度对冲的优势在于其能够处理各种现实市场条件，如市场摩擦，而这在传统的数学金融框架中很难解决。由于深度对冲依赖于市场模拟，因此底层资产价格过程模型至关重要。然而，现有的关于深度对冲的文献往往依赖于传统的数学金融模型，例如布朗运动和随机波动模型，而寻找有效的底层资产模型用于深度对冲学习一直是一个挑战。在本研究中，我们提出了一个新的框架，称为对抗式深度对冲，受到对抗性学习的启发。在这个框架中，一个对冲者和一个生成器分别对底层资产过程和底层资产过程进行建模，并以对抗性的方式进行训练。所提出的方法能够学习一个稳健的对冲者，而无需进行价格过程建模。

    Deep hedging is a deep-learning-based framework for derivative hedging in incomplete markets. The advantage of deep hedging lies in its ability to handle various realistic market conditions, such as market frictions, which are challenging to address within the traditional mathematical finance framework. Since deep hedging relies on market simulation, the underlying asset price process model is crucial. However, existing literature on deep hedging often relies on traditional mathematical finance models, e.g., Brownian motion and stochastic volatility models, and discovering effective underlying asset models for deep hedging learning has been a challenge. In this study, we propose a new framework called adversarial deep hedging, inspired by adversarial learning. In this framework, a hedger and a generator, which respectively model the underlying asset process and the underlying asset process, are trained in an adversarial manner. The proposed method enables to learn a robust hedger witho
    
[^45]: FedMEKT: 基于蒸馏的多模态联邦学习中的嵌入知识传输

    FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])

    [http://arxiv.org/abs/2307.13214](http://arxiv.org/abs/2307.13214)

    FedMEKT是一种基于蒸馏的多模态联邦学习框架，通过半监督学习方法利用不同模态的表示，实现了服务器和客户端之间的联合知识传输。

    

    联邦学习（FL）使多个客户端能够在不共享私有数据的情况下协同训练一个广义全局模型，从而实现分散式的机器学习范式。现有的大部分工作只是针对单模态数据提出了典型的FL系统，因此限制了它对于利用宝贵的多模态数据进行未来个性化应用的潜力。此外，大多数FL方法仍然依赖于客户端的标记数据，由于用户无法进行自注释，这在实际应用中是有限的。鉴于这些限制，我们提出了一种新型的多模态FL框架，采用半监督学习方法利用不同模态的表示。将这个概念引入一个系统中，我们开发了一种基于蒸馏的多模态嵌入知识传输机制，称为FedMEKT，它允许服务器和客户端交换从小型多模态数据集中提取的学习模型的联合知识。

    Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal 
    
[^46]: Gait Cycle-Inspired Learning Strategy for Continuous Prediction of Knee Joint Trajectory from sEMG.

    Gait Cycle-Inspired Learning Strategy for Continuous Prediction of Knee Joint Trajectory from sEMG. (arXiv:2307.13209v1 [cs.RO])

    [http://arxiv.org/abs/2307.13209](http://arxiv.org/abs/2307.13209)

    本文提出了一种受步态周期启发的学习策略，通过将膝关节角度分解为运动模式和振幅的方式来提高对人体膝关节轨迹的预测性能。

    

    预测下肢运动意图对于控制外骨骼机器人和假肢非常重要。近年来，表面肌电图(sEMG)引起了越来越多的关注，因为它能够在实际运动之前提前预测运动意图。然而，由于人体内部和个体间的差异，对人体关节轨迹的估计性能仍然是一个具有挑战性的问题。前者与个体的生理差异（如身高和体重）和偏好的行走模式有关，而后者主要是由不规则和与步态无关的肌肉活动引起的。本文提出了一种模型，利用两种受步态周期启发的学习策略来减轻预测人体膝关节轨迹的挑战。第一种策略是将膝关节角度分解为运动模式和振幅，前者在个体间显示出较低的变异性，而后者在个体间显示出较高的变异性。通过通过单独的网络实体学习，该模型可以更好地预测人体膝关节轨迹。

    Predicting lower limb motion intent is vital for controlling exoskeleton robots and prosthetic limbs. Surface electromyography (sEMG) attracts increasing attention in recent years as it enables ahead-of-time prediction of motion intentions before actual movement. However, the estimation performance of human joint trajectory remains a challenging problem due to the inter- and intra-subject variations. The former is related to physiological differences (such as height and weight) and preferred walking patterns of individuals, while the latter is mainly caused by irregular and gait-irrelevant muscle activity. This paper proposes a model integrating two gait cycle-inspired learning strategies to mitigate the challenge for predicting human knee joint trajectory. The first strategy is to decouple knee joint angles into motion patterns and amplitudes former exhibit low variability while latter show high variability among individuals. By learning through separate network entities, the model ma
    
[^47]: RL中的反事实解释策略

    Counterfactual Explanation Policies in RL. (arXiv:2307.13192v1 [cs.AI])

    [http://arxiv.org/abs/2307.13192](http://arxiv.org/abs/2307.13192)

    本文介绍了一个名为COUNTERPOL的框架，用于通过对策略进行最小改变来分析RL策略，并达到所需的结果。这项工作在RL中通过使用反事实解释与监督学习相结合的方法进行了实证分析，并与广泛使用的基于信任区域的策略优化方法进行了理论联系。

    

    随着强化学习（RL）代理在使用奖励偏好的多样化决策问题中的应用越来越广泛，确保这些框架学习到的策略能够解释变得很重要，即将观察映射到可能行动的概率分布的策略如何以对比的方式系统地理解，即，使其性能达到所需水平的策略最小改变是什么。在这项工作中，我们提出了COUNTERPOL，这是第一个使用反事实解释来分析RL策略的框架，即通过对策略进行最小改变，达到所需的结果。我们通过将反事实融入RL中的监督学习，并使用期望收益调控目标结果，建立了Counterpol与广泛使用的基于信任区域的策略优化方法之间的理论联系。大量的实证分析表明，该方法具有明显的效果。

    As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the eff
    
[^48]: 社交媒体上的数字情绪调节

    Digital Emotion Regulation on Social Media. (arXiv:2307.13187v1 [cs.HC])

    [http://arxiv.org/abs/2307.13187](http://arxiv.org/abs/2307.13187)

    本文概述了社交媒体上的数字情绪调节，并综合了关于社交媒体情绪调节干预的最新研究。研究发现不同社交媒体应用在情绪调节过程的不同阶段中有不同的使用方式。

    

    情绪调节是有意识地改变自身情感状态的过程，即改变幸福、自信、内疚、愤怒等基本情绪状态。有效地调节情绪对于日常生活中的高效运作是必要的。如今，数字技术的普及正在被有目的地用来改变我们的情感状态，这一过程被称为数字情绪调节。理解数字情绪调节可以帮助支持道德科技设计、开发和部署的崛起。本文概述了社交媒体应用中的数字情绪调节，并综合了关于社交媒体情绪调节干预的最新研究。我们通过分析最新文献，分享了关于不同社交媒体应用在情绪调节过程的不同阶段中的使用情况的发现。

    Emotion regulation is the process of consciously altering one's affective state, that is the underlying emotional state such as happiness, confidence, guilt, anger etc. The ability to effectively regulate emotions is necessary for functioning efficiently in everyday life. Today, the pervasiveness of digital technology is being purposefully employed to modify our affective states, a process known as digital emotion regulation. Understanding digital emotion regulation can help support the rise of ethical technology design, development, and deployment. This article presents an overview of digital emotion regulation in social media applications, as well as a synthesis of recent research on emotion regulation interventions for social media. We share our findings from analysing state-of-the-art literature on how different social media applications are utilised at different stages in the process of emotion regulation.
    
[^49]: 使用经过人群调整的生成式语言模型进行观点挖掘

    Opinion Mining Using Population-tuned Generative Language Models. (arXiv:2307.13173v1 [cs.CL])

    [http://arxiv.org/abs/2307.13173](http://arxiv.org/abs/2307.13173)

    本文提出了一种使用经过调整的生成式语言模型进行观点挖掘的方法。通过特定数据的微调，我们的方法可以学习和转移观点，并保持极性的比例。实验结果表明我们的方法在挖掘真实文本中的观点洞察方面具有良好的性能。

    

    我们提出了一种新的方法，利用训练于不同人群数据上的生成式语言模型从文本集合中挖掘观点。我们描述了基本定义、方法论和观点洞察挖掘的通用算法。我们通过一个实验展示了我们的方法的性能：使用预训练的生成式模型，并使用特定的定制内容和完全标注的观点对其进行微调。我们证明了我们的方法可以学习和转移观点到语义类别，并保持极性的比例。最后，我们展示了一个洞察挖掘系统在实际文本语料库中发现观点洞察的扩展应用。

    We present a novel method for mining opinions from text collections using generative language models trained on data collected from different populations. We describe the basic definitions, methodology and a generic algorithm for opinion insight mining. We demonstrate the performance of our method in an experiment where a pre-trained generative model is fine-tuned using specifically tailored content with unnatural and fully annotated opinions. We show that our approach can learn and transfer the opinions to the semantic classes while maintaining the proportion of polarisation. Finally, we demonstrate the usage of an insight mining system to scale up the discovery of opinion insights from a real text corpus.
    
[^50]: 研究顺序推荐系统对训练数据扰动的鲁棒性：一项经验研究

    Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study. (arXiv:2307.13165v1 [cs.IR])

    [http://arxiv.org/abs/2307.13165](http://arxiv.org/abs/2307.13165)

    本研究通过对多个数据集进行评估发现，顺序推荐系统中删除序列末尾的项目显著降低了性能，而删除序列开头或中间的项目则没有明显影响。这一发现强调了考虑训练数据中扰动项目位置的重要性，并能指导更具鲁棒性的顺序推荐系统的设计。

    

    顺序推荐系统被广泛用于建模用户随时间变化的行为，然而其在面对训练数据扰动时的鲁棒性是一个关键问题。本文进行了一项经验研究，探究了在时间顺序序列中不同位置上删除项目的效果。我们评估了两种不同的顺序推荐系统模型在多个数据集上的表现，使用归一化折现累积增益（NDCG）指标和排名敏感度列表（Rank Sensitivity List）指标来衡量其性能。我们的结果显示，删除序列末尾的项目显著影响性能，NDCG下降高达60％，而删除序列开头或中间的项目没有显著影响。这些发现凸显了考虑训练数据中扰动项目位置的重要性，并可指导更具鲁棒性的顺序推荐系统的设计。

    Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs.
    
[^51]: 为什么你不清洁眼镜？利用动态光学扰动进行感知攻击

    Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations. (arXiv:2307.13131v1 [cs.CR])

    [http://arxiv.org/abs/2307.13131](http://arxiv.org/abs/2307.13131)

    本文提出了EvilEye，利用透明显示器生成动态物理敌对示例的感知攻击方法，通过利用摄像头的光学特性在多种照明条件下诱导错误分类。

    

    摄像头为基础的模拟人类感知的自主系统越来越多地被集成到安全关键的平台中。因此，已经出现了一个稳定的文献体系，探索对底层机器学习模型进行敌对攻击。将敌对攻击适应于现实世界对攻击者来说是可取的，因为这消除了危害数字系统的需要。然而，真实世界面临着与感知管道中的环境噪声和自主系统的动态性有关的挑战。在本文中，我们采用传感器为先的方法。我们提出了EvilEye，一种利用透明显示器生成动态物理敌对示例的中间人感知攻击。EvilEye利用摄像头的光学特性在各种照明条件下诱导错误分类。为了生成动态扰动，我们将数字攻击的投影形式化为光学视觉逆向问题。

    Camera-based autonomous systems that emulate human perception are increasingly being integrated into safety-critical platforms. Consequently, an established body of literature has emerged that explores adversarial attacks targeting the underlying machine learning models. Adapting adversarial attacks to the physical world is desirable for the attacker, as this removes the need to compromise digital systems. However, the real world poses challenges related to the "survivability" of adversarial manipulations given environmental noise in perception pipelines and the dynamicity of autonomous systems. In this paper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle perception attack that leverages transparent displays to generate dynamic physical adversarial examples. EvilEye exploits the camera's optics to induce misclassifications under a variety of illumination conditions. To generate dynamic perturbations, we formalize the projection of a digital attack into the ph
    
[^52]: Pathway:一种快速灵活的统一流数据处理框架，用于分析和机器学习应用

    Pathway: a fast and flexible unified stream data processing framework for analytical and Machine Learning applications. (arXiv:2307.13116v1 [cs.LG])

    [http://arxiv.org/abs/2307.13116](http://arxiv.org/abs/2307.13116)

    Pathway是一个快速灵活的统一流数据处理框架，用于分析和机器学习应用。它能够在有界和无界的数据流上运行，通过Table API和分布式增量数据流驱动，在批处理和流处理场景中表现出优异的能力。

    

    我们提出了Pathway，一个新的统一数据处理框架，可以在有界和无界数据流上运行工作负载。该框架的创建最初是为了解决在分析和处理物理经济数据时面临的挑战，包括由物联网和企业系统生成的数据流。这些都需要快速反应，并需要应用先进的计算范 paradigms（机器学习驱动的分析，上下文分析和复杂事件处理的其他元素）。Pathway配备了针对Python和Python/SQL工作流程量身定制的Table API，并由Rust中的分布式增量数据流驱动。我们描述了该系统，并呈现了基准测试结果，表明它在批处理和流处理场景中的能力，可以超过最先进的行业框架。我们还讨论了由Pathway处理的流处理用例，这些用例无法轻松解决以状态为基础的方案。

    We present Pathway, a new unified data processing framework that can run workloads on both bounded and unbounded data streams. The framework was created with the original motivation of resolving challenges faced when analyzing and processing data from the physical economy, including streams of data generated by IoT and enterprise systems. These required rapid reaction while calling for the application of advanced computation paradigms (machinelearning-powered analytics, contextual analysis, and other elements of complex event processing). Pathway is equipped with a Table API tailored for Python and Python/SQL workflows, and is powered by a distributed incremental dataflow in Rust. We describe the system and present benchmarking results which demonstrate its capabilities in both batch and streaming contexts, where it is able to surpass state-of-the-art industry frameworks in both scenarios. We also discuss streaming use cases handled by Pathway which cannot be easily resolved with state
    
[^53]: 一个可解释的几何加权图注意力网络用于识别与步态障碍相关的功能网络

    An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment. (arXiv:2307.13108v1 [cs.LG])

    [http://arxiv.org/abs/2307.13108](http://arxiv.org/abs/2307.13108)

    本文提出了一个可解释的几何加权图注意力网络（xGW-GAT），用于识别与步态障碍相关的功能网络，以推动帕金森病治疗的发展。

    

    帕金森病的一个显著症状是姿势反射的逐渐丧失，最终导致步态困难和平衡问题。识别与步态障碍相关的脑功能紊乱对于更好地理解帕金森病的运动进展以及推动更有效和个性化的治疗的发展可能是至关重要的。本文提出了一个可解释的、几何的、加权图注意力神经网络（xGW-GAT），用于识别预测帕金森病患者步态困难进展的功能网络。xGW-GAT在MDS统一帕金森病评分标准（MDS-UPDRS）上预测多类别的步态障碍。我们的计算和数据高效的模型将功能连接组表示为黎曼流形上的对称正定（SPD）矩阵，以明确编码整个连接组的成对交互作用，根据此我们可以学习出一个注意力掩码，以产生个体和群体级别的可解释性。

    One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability
    
[^54]: 如何使用LLMs进行文本分析

    How to use LLMs for Text Analysis. (arXiv:2307.13106v1 [cs.CL])

    [http://arxiv.org/abs/2307.13106](http://arxiv.org/abs/2307.13106)

    本文介绍了如何使用LLMs进行文本分析，LLMs作为一种非常多功能的文本分析方法在社会科学中具有广泛的应用前景。使用LLMs可以实现从文本标注和分类到情感分析和批判性话语分析等多种任务，并且易于使用且速度快。这对于具有有限编程经验的学生和研究者来说尤其有用。

    

    本指南介绍了大型语言模型（LLM）作为社会科学中一种非常多功能的文本分析方法。由于LLMs易于使用、成本低、速度快，并且适用于广泛的文本分析任务，从文本标注和分类到情感分析和批判性话语分析，许多学者认为LLMs将改变我们进行文本分析的方式。本指南面向具有有限编程经验的学生和研究者，并提供了如何在自己的研究项目中使用LLMs进行文本分析的简单介绍，以及最佳实践建议。我们将使用Python演示使用LLMs分析文本数据的每个步骤：安装软件，设置API，加载数据，开发分析提示，分析文本和验证结果。作为一个说明性例子，我们将使用在政治文本中识别民粹主义的具有挑战性的任务，并展示LLMs如何超越现有的方法。

    This guide introduces Large Language Models (LLM) as a highly versatile text analysis method within the social sciences. As LLMs are easy-to-use, cheap, fast, and applicable on a broad range of text analysis tasks, ranging from text annotation and classification to sentiment analysis and critical discourse analysis, many scholars believe that LLMs will transform how we do text analysis. This how-to guide is aimed at students and researchers with limited programming experience, and offers a simple introduction to how LLMs can be used for text analysis in your own research project, as well as advice on best practices. We will go through each of the steps of analyzing textual data with LLMs using Python: installing the software, setting up the API, loading the data, developing an analysis prompt, analyzing the text, and validating the results. As an illustrative example, we will use the challenging task of identifying populism in political texts, and show how LLMs move beyond the existing
    
[^55]: 对比示例驱动的控制

    Contrastive Example-Based Control. (arXiv:2307.13101v1 [cs.LG])

    [http://arxiv.org/abs/2307.13101](http://arxiv.org/abs/2307.13101)

    本文提出了一种基于示例的控制方法，它通过学习隐式模型的多步转移来解决控制问题，而不是学习奖励函数。这种方法避免了复杂的正规化和时差更新，并取得了良好的结果。

    

    尽管许多现实世界中可能受益于强化学习的问题很少符合MDP模型，与环境交互往往是昂贵的，并且指定奖励函数也很具有挑战性。鉴于这些挑战，先前的工作已经开发出一种基于数据驱动的方法，从转移动态的样本和高回报状态的示例中进行学习。这些方法通常从高回报状态学习奖励函数，使用该奖励函数标记转移，并通过离线强化学习算法应用于这些转移。尽管这些方法在许多任务上可以取得良好的结果，但它们可能很复杂，通常需要正规化和时差更新。在本文中，我们提出了一种离线、基于示例的控制方法，它学习了一个隐式模型的多步转移，而不是学习奖励函数。我们证明了这个隐式模型可以表示基于示例的控制问题的Q值。

    While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a ran
    
[^56]: 使用大型语言模型使元数据更加FAIR

    Making Metadata More FAIR Using Large Language Models. (arXiv:2307.13085v1 [cs.CL])

    [http://arxiv.org/abs/2307.13085](http://arxiv.org/abs/2307.13085)

    本研究提出了一种名为FAIRMetaText的自然语言处理应用程序，用于比较元数据。该应用程序分析元数据的自然语言描述，并提供数学相似度度量，可用于分析和识别可替代术语，从而大大减少人力成本。

    

    随着全球实验数据资料的增加，统一利用这些资料的一个主要障碍是糟糕的元数据。为了弥合这个差距，本研究提出了一种名为FAIRMetaText的自然语言处理（NLP）应用程序，用于比较元数据。具体而言，FAIRMetaText分析元数据的自然语言描述，并提供两个术语之间的数学相似度度量。这个度量可以用于分析不同的元数据，通过建议符合性术语或分组相似术语来识别可替代术语。通过在公开的研究资料上进行深入研究，并在大规模语言模型（LLM）的各种元数据相关任务上定性和定量地展示了该算法的效果。这个软件可以极大地减少人力成本，同时利用多种实验数据过程中筛选各种自然语言元数据。

    With the global increase in experimental data artifacts, harnessing them in a unified fashion leads to a major stumbling block - bad metadata. To bridge this gap, this work presents a Natural Language Processing (NLP) informed application, called FAIRMetaText, that compares metadata. Specifically, FAIRMetaText analyzes the natural language descriptions of metadata and provides a mathematical similarity measure between two terms. This measure can then be utilized for analyzing varied metadata, by suggesting terms for compliance or grouping similar terms for identification of replaceable terms. The efficacy of the algorithm is presented qualitatively and quantitatively on publicly available research artifacts and demonstrates large gains across metadata related tasks through an in-depth study of a wide variety of Large Language Models (LLMs). This software can drastically reduce the human effort in sifting through various natural language metadata while employing several experimental dat
    
[^57]: 人口稀缺制度下的公平性研究

    Fairness Under Demographic Scarce Regime. (arXiv:2307.13081v1 [cs.LG])

    [http://arxiv.org/abs/2307.13081](http://arxiv.org/abs/2307.13081)

    这项研究探讨了在人口信息不完全可用的情况下如何提高公平性。研究发现，在替代敏感属性的属性分类器中引入不确定性意识，并对推断出的不确定性最低的人口信息样本进行公平性约束可以实现更好的公平性和准确性权衡。

    

    大多数现有的公平性研究假设模型可以完全访问人口信息。然而，由于数据采集期间未保留记录或出于隐私原因，存在人口信息部分可用的情况。这种情况被称为人口稀缺制度。先前的研究表明，训练一个属性分类器来替代缺失的敏感属性（代理）仍然可以改善公平性。然而，与真实敏感属性相比，使用代理敏感属性会加剧公平性和准确性之间的权衡。为了解决这个限制，我们提出了一个框架来构建属性分类器，以实现更好的公平性和准确性的权衡。我们的方法在属性分类器中引入不确定性意识，并对具有推断出的最低不确定性的人口信息的样本强制执行公平性。我们通过实验证明，在具有不确定敏感属性的样本上强制执行公平约束会损害算法的总体准确性，但可以提高公平性。

    Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detr
    
[^58]: 自适应认证训练: 迈向更好的准确性-鲁棒性权衡

    Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs. (arXiv:2307.13078v1 [cs.LG])

    [http://arxiv.org/abs/2307.13078](http://arxiv.org/abs/2307.13078)

    本文提出了一种自适应认证训练方法，通过训练模型时使用自适应认证半径的关键观点，提高了模型的准确性和鲁棒性，推进了准确性-鲁棒性权衡的最新进展。

    

    随着深度学习模型的不断进步和在实际系统中的日益广泛应用，鲁棒性问题仍然是一个重大挑战。现有的认证训练方法可以在某些扰动水平上获得高度可证明的鲁棒性保证。然而，这些模型的主要问题是在干净的未扰动数据上准确性严重降低，使其不实用。在本工作中，我们考虑了在特定的高准确性水平上最大化模型的鲁棒性的更现实的角度。为此，我们提出了一种基于关键观点的新型认证训练方法，即使用自适应认证半径进行训练有助于提高模型的准确性和鲁棒性，推进了最先进的准确性-鲁棒性权衡。我们在MNIST、CIFAR-10和TinyImageNet数据集上展示了所提出方法的有效性。

    As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up
    
[^59]: 用于程序之间变量映射的图神经网络——扩展版本

    Graph Neural Networks For Mapping Variables Between Programs -- Extended Version. (arXiv:2307.13014v1 [cs.SE])

    [http://arxiv.org/abs/2307.13014](http://arxiv.org/abs/2307.13014)

    本文提出了使用图神经网络(GNNs)基于程序的抽象语法树(ASTs)来映射变量集，以解决程序比较、分析、修复和克隆检测等任务。在初学者编程作业中进行的实验证明了变量映射的有效性。

    

    自动程序分析是计算机科学中许多领域的关键研究领域，特别是形式方法和人工智能。由于程序等价问题的不可判定性，比较两个程序非常具有挑战性。通常，为了比较两个程序，需要对两个程序的变量集之间建立关系。因此，在诸如程序等价性、程序分析、程序修复和克隆检测等任务上，映射两个程序之间的变量是非常有用的。在这项工作中，我们提出使用图神经网络(GNNs)基于两个程序的抽象语法树(ASTs)来映射变量集。为了展示变量映射的优势，我们在程序修复任务中提供了这些映射的三个用例，以修复初学者编程作业中常见的和经常发生的错误。实验结果基于一个包含4166对错误/修正程序的数据集。

    Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/corr
    
[^60]: 联合语音和重叠检测：多个音频设置和语音领域的基准

    Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains. (arXiv:2307.13012v1 [cs.SD])

    [http://arxiv.org/abs/2307.13012](http://arxiv.org/abs/2307.13012)

    本文提出了一个全新的基准，对不同的语音活动检测和重叠讲话检测模型进行了评估，涉及多个音频设置和语音领域。我们的系统在F1得分方面表现出色，并且通过联合训练这两个任务，可以降低训练成本。

    

    语音活动检测和重叠讲话检测是说话者分割的关键预处理任务。最终的分段性能非常依赖于这些子任务的鲁棒性。最近的研究表明，可以使用多类别分类模型来联合训练语音活动检测和重叠讲话检测。然而，这些研究通常局限于特定的语音领域，缺乏系统的泛化能力信息。本文提出了一个全面而新颖的不同语音活动检测和重叠讲话检测模型的基准，涵盖了多个音频设置（单声道/多声道）和语音领域（例如媒体、会议等）。我们的2/3类系统将时域卷积神经网络与适应音频设置的语音表征相结合，优于最先进的结果。我们展示了这两个任务的联合训练在F1得分方面与两个专用的语音活动检测和重叠讲话检测系统具有类似的性能，同时降低了训练成本。这种独特的架构也可以

    Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be
    
[^61]: 图神经网络中的最大独立集合用于池化

    Maximal Independent Sets for Pooling in Graph Neural Networks. (arXiv:2307.13011v1 [cs.LG])

    [http://arxiv.org/abs/2307.13011](http://arxiv.org/abs/2307.13011)

    本文提出了三种基于最大独立集合概念的图形池化方法，避免了现有方法中存在的缺点，通过实验证实了最大独立集合约束在图形池化中的重要性。

    

    卷积神经网络使得图像分类取得重大突破，通过卷积和池化。然而，对于图形而言，并不存在满足这些性质的池化方法。传统的图形池化方法存在以下缺点之一：图形断开或连接过度、降采样比较低、以及删除大部分图形。本文提出了三种基于最大独立集合概念的池化方法，避免了这些缺点。我们的实验结果证实了在图形池化中最大独立集合约束的相关性。

    Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
    
[^62]: 将Whisper模型适应儿童语音识别

    Adaptation of Whisper models to child speech recognition. (arXiv:2307.13008v1 [eess.AS])

    [http://arxiv.org/abs/2307.13008](http://arxiv.org/abs/2307.13008)

    本文研究了将Whisper模型适应儿童语音识别的问题，并比较了与wav2vec2模型的调优效果。结果表明，在儿童语音上进行调优的Whisper模型能显著提高儿童语音的ASR性能，而调优的wav2vec2模型表现更好。

    

    自动语音识别（ASR）系统在转录儿童语音时常常面临困难，这是因为缺乏大规模的儿童语音数据集，这些数据集是准确训练儿童友好型ASR模型所必需的。然而，存在着大量用于创建多语言ASR模型（如Whisper）的成年人语音数据集。我们的工作旨在探索这些模型是否可以适应儿童语音，以改进儿童ASR。此外，我们将Whisper的儿童适应性与自监督模型（如wav2vec2）的调优进行比较。我们证明将Whisper在儿童语音上进行调优可以显著提高儿童语音的ASR性能，而不调优的Whisper模型。此外，利用在儿童语音上进行调优的自监督wav2vec2模型超越了Whisper的调优性能。

    Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.
    
[^63]: IteraTTA:一种用于在生成音乐的文本到音频模型中探索文本提示和音频先验的界面

    IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models. (arXiv:2307.13005v1 [eess.AS])

    [http://arxiv.org/abs/2307.13005](http://arxiv.org/abs/2307.13005)

    这个研究开发了一个名为IteraTTA的界面，可以帮助用户探索文本提示和音频先验对生成音乐结果的影响，从而使用户能够逐步实现他们的期望。

    

    最近的文本到音频生成技术使得新手用户可以自由地生成音乐音频。即使他们没有关于和弦进行和乐器的音乐知识，用户也可以尝试各种文本提示来生成音频。然而，与图像领域相比，对可能的音乐音频空间的清晰理解是困难的，因为用户不能同时听到生成音频的变化。因此，我们通过对比文本提示和音频先验的迭代比较，帮助用户探索不仅文本提示还有音频先验对生成结果的影响。我们开发的界面 IteraTTA，专门设计用于帮助用户细化文本提示，并从生成的音频中选择有利的音频先验。有了这个界面，用户可以逐步实现他们的期望。

    Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loos
    
[^64]: 使用多模态对比学习从自然语言中提取分子属性

    Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning. (arXiv:2307.12996v1 [cs.LG])

    [http://arxiv.org/abs/2307.12996](http://arxiv.org/abs/2307.12996)

    该论文研究了如何使用多模态对比学习方法从自然语言中提取分子属性信息，通过改进文本检索和引入分子图扩增策略等方法提高了属性预测性能。实验结果显示相对于仅在图模态上预训练的模型，我们取得了+4.26%的AUROC增益和+1.54%的增益。

    

    在计算生物化学中，深度学习传统上专注于分子图神经表征；然而，最近语言模型的进展突显了文本中所编码的科学知识量。为了弥合这两种模态，我们研究了如何将分子属性信息从自然语言转化为图表征。我们研究了在使用对比学习将神经图表征与其特征的文本描述表征对齐后，属性预测性能的提升。我们实现了神经相关性评分策略以改进文本检索，引入了一种受有机反应启发的新颖合法分子图扩增策略，并在下游的MoleculeNet属性分类任务上展示了性能的改善。与仅在图模态上预训练的模型相比，我们取得了+4.26%的AUROC增益，并与最近提出的分子图/文本对比模型相比，取得了+1.54%的增益。

    Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively t
    
[^65]: 基于多表示空间分离的图级异常检测

    Multi-representations Space Separation based Graph-level Anomaly-aware Detection. (arXiv:2307.12994v1 [cs.LG])

    [http://arxiv.org/abs/2307.12994](http://arxiv.org/abs/2307.12994)

    本文提出了一种基于多表示空间分离的图级异常感知检测框架，以解决检测图集内异常图的问题。

    

    最近，图结构模式被广泛用于对不同领域的数据建模。如何检测这些图数据中的异常图信息已成为一个热门的研究问题。本研究的目标集中在如何检测图集内的异常图这个特定问题上。先前的研究观察到异常图主要表现为节点级和图级异常，但这些方法在评估异常图时同等对待上述两种异常形式，而事实上不同类型的异常图数据在节点级和图级异常方面有不同程度的问题。此外，与正常图具有微妙差异的异常图很容易逃避现有方法的检测。因此，本文提出了一种基于多表示空间分离的图级异常感知检测框架。

    Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anoma
    
[^66]: 为计算病理学构建以视觉语言为基础的模型

    Towards a Visual-Language Foundation Model for Computational Pathology. (arXiv:2307.12914v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12914](http://arxiv.org/abs/2307.12914)

    这项研究介绍了一种基于视觉语言的基础模型CONCH，通过图像、文本和图像-标题对的预训练，可以在涉及组织病理学图像和文本的各种任务上取得良好的性能。

    

    数字病理学的广泛应用和深度学习的进步使得在各种疾病和患者群体中开发强大的模型成为可能。然而，在医学领域中，由于标签稀缺性，模型的训练往往困难，并且该模型的使用仅限于特定的任务和疾病。此外，大多数组织病理学模型仅使用图像数据，与人类相互教导和推理组织病理学实体的方式形成鲜明对比。我们引入了一种基于视觉语言的基础模型CONCH，它使用多种来源的组织病理学图像、生物医学文本，并通过无任务预训练获得了117万个图像-标题对。经过13种不同基准测试，CONCH 可以迁移到涉及组织病理学图像和文本的各种下游任务上，实现了最新的水平。

    The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-th
    
[^67]: GridMM:视觉与语言导航的网格记忆图

    GridMM: Grid Memory Map for Vision-and-Language Navigation. (arXiv:2307.12907v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12907](http://arxiv.org/abs/2307.12907)

    本文提出了GridMM，一种用于视觉与语言导航的自顶向下网格记忆图，从全局和局部视角有效地表示和结构化先前访问的环境。在多个数据集上进行的实验证明了该方法的优越性。

    

    视觉与语言导航（VLN）使代理能够根据自然语言指令在3D环境中导航到远程位置。为了表示先前访问的环境，VLN的大多数方法使用经常性状态、拓扑地图或自顶向下的语义地图来实现记忆。与这些方法相比，我们构建了自顶向下的以自我为中心并动态增长的网格记忆图（即GridMM）来结构化访问的环境。从全局视角来看，历史观察结果在自上而下的视图中被投影到统一的网格地图中，这可以更好地表示环境的空间关系。从局部视角来看，我们进一步提出了一种指令相关性聚合方法，以捕捉每个网格区域中细粒度的视觉线索。在离散环境中对REVERIE、R2R、SOON数据集以及连续环境中的R2R-CE数据集进行了大量实验证明了我们提出的方法的优越性。

    Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed metho
    
[^68]: 非参数线性特征学习在回归中的应用通过正则化

    Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2307.12754](http://arxiv.org/abs/2307.12754)

    本研究提出了一种新的非参数线性特征学习方法，对于监督学习中存在于低维线性子空间中的相关信息的预测和解释能力的提升是非常有帮助的。

    

    表征学习在自动化特征选择中发挥着关键作用，特别是在高维数据的背景下，非参数方法常常很难应对。在本研究中，我们专注于监督学习场景，其中相关信息存在于数据的低维线性子空间中，即多指数模型。如果已知该子空间，将大大增强预测、计算和解释能力。为了解决这一挑战，我们提出了一种新颖的非参数预测的线性特征学习方法，同时估计预测函数和线性子空间。我们的方法采用经验风险最小化，并加上函数导数的惩罚项，以保证其多样性。通过利用Hermite多项式的正交性和旋转不变性特性，我们引入了我们的估计器RegFeaL。通过利用替代最小化，我们迭代地旋转数据以改善与线性子空间的对齐。

    Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
    
[^69]: TF-ICON: 基于扩散的无需训练的跨领域图像合成

    TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition. (arXiv:2307.12493v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12493](http://arxiv.org/abs/2307.12493)

    TF-ICON是一种无需训练的图像合成框架，利用文字驱动的扩散模型实现跨领域图像导向合成。与传统方法相比，TF-ICON可以在不需额外训练、微调或优化的情况下实现高质量的无缝合成，同时引入了例外提示来准确地反转真实图像为潜在表示。

    

    文字驱动的扩散模型展示出令人印象深刻的生成能力，可以实现各种图像编辑任务。在本文中，我们提出了TF-ICON，一种新颖的无需训练的图像合成框架，利用文字驱动的扩散模型来进行跨领域图像导向合成。该任务旨在将用户提供的对象无缝地整合到特定的视觉环境中。目前的基于扩散的方法通常涉及昂贵的基于实例的优化或在定制数据集上微调预训练模型，可能会损害其丰富的先验知识。相反，TF-ICON可以利用现成的扩散模型进行跨领域图像导向合成，无需额外的训练、微调或优化。此外，我们引入了例外提示(含无信息)来帮助文字驱动的扩散模型准确地将真实图像反转为潜在表示，为合成提供基础。我们的实验结果表明，TF-ICON在不同的合成任务中具有优越的表现，并且可以在不同领域的图像之间进行高质量的无缝合成。

    Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experim
    
[^70]: 问题分解提高了模型生成推理的忠实度

    Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])

    [http://arxiv.org/abs/2307.11768](http://arxiv.org/abs/2307.11768)

    通过将问题分解为子问题，可以显著提高大型语言模型生成推理的忠实度。

    

    随着大型语言模型（LLM）执行越来越复杂的任务，验证其行为的正确性和安全性变得越来越困难。其中一种解决方法是要求LLM在回答问题时以逐步推理的方式外化其推理过程（思维链；CoT）。推理过程可以让我们检查模型执行任务的过程。然而，这种方法依赖于所陈述的推理能够忠实地反映模型的实际推理，而这并非总是如此。为了提高CoT推理的忠实度，我们通过将问题分解为子问题来生成推理。基于分解的方法在问答任务上取得了较好的性能，有时接近CoT，并在几个最近提出的度量标准中提高了模型所陈述推理的忠实度。通过强制模型在单独的上下文中回答简单的子问题，我们大大增加了模型的忠实度。

    As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
    
[^71]: EmotionPrompt: 通过情感刺激提升大型语言模型的关键心理学方法

    EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])

    [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)

    EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。

    

    大型语言模型（LLMs）在推理、语言理解和数学问题解决等许多领域取得了显著的性能，并被视为人工通用智能（AGI）的关键步骤。然而，LLMs对提示的敏感性仍然是其日常应用的主要瓶颈。本文从心理学中汲取灵感，提出了EmotionPrompt来探索情感智能以提升LLMs的性能。EmotionPrompt基于一个非常简单明了的原则：将情感刺激融入到提示中。实验结果表明，我们的方法在相同的单一提示模板上，与原始的零样本提示和Zero-shot-CoT相比，在8个任务上都显著优于多种模型：ChatGPT、Vicuna-13b、Bloom和T5。此外，观察到EmotionPrompt能够提高真实性和信息量。我们相信EmotionPrompt为探索跨学科知识开辟了一条新的道路。

    Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
    
[^72]: 受奖赏约束：共同构建评估酷儿人工智能伤害的过程

    Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms. (arXiv:2307.10223v1 [cs.CY])

    [http://arxiv.org/abs/2307.10223](http://arxiv.org/abs/2307.10223)

    这项研究探索了如何在评估人工智能偏见和伤害时整合边缘化社区的知识，并提出了以酷儿社区为视角重新设计偏见奖金的方法。

    

    偏见评估基准、数据集和模型文档已成为评估人工智能系统偏见和伤害的核心过程。然而，这些审计过程因未整合边缘化社区的知识并考虑审计员与社区之间的权力动态而受到批评。因此，已经提出了一种参与受影响社区识别和评估人工智能系统伤害的偏见评估方式（例如偏见奖金）。尽管如此，关于边缘化社区对此类审计过程的期望一直被忽视。在本文中，我们向酷儿社区征求他们对审计过程的立场和期望。为此，我们组织了一个参与式研讨会，从酷儿的角度对偏见奖金进行批判性的重新设计。我们发现，当有空间时，参与者的反馈范围远远超出了偏见奖金所能提供的范围，参与者 que

    Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants que
    
[^73]: 使用深度学习自动化进行纤维材料显微图像中的木材种类检测与分类

    Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning. (arXiv:2307.09588v1 [cs.CV])

    [http://arxiv.org/abs/2307.09588](http://arxiv.org/abs/2307.09588)

    本研究通过深度学习实现了对纤维材料显微图像中硬木种类的自动检测和分类。该方法在性能上与人类专家类似，未来将有助于保护森林资源。

    

    我们开发了一种方法，用于系统地生成大量的破解木材参考图像数据集，并利用此数据生成了九个硬木种属的图像数据。这是通过深度学习，首次自动化识别纤维材料显微图像中硬木种类的基础。我们的方法包括一个灵活的管道，便于对导管元素进行注释。我们比较了不同神经网络架构和超参数的性能。我们提出的方法表现与人类专家相似。将来，这将改善对全球木质纤维产品流的控制，以保护森林。

    We have developed a methodology for the systematic generation of a large image dataset of macerated wood references, which we used to generate image data for nine hardwood genera. This is the basis for a substantial approach to automate, for the first time, the identification of hardwood species in microscopic images of fibrous materials by deep learning. Our methodology includes a flexible pipeline for easy annotation of vessel elements. We compare the performance of different neural network architectures and hyperparameters. Our proposed method performs similarly well to human experts. In the future, this will improve controls on global wood fiber product flows to protect forests.
    
[^74]: 深度学习中遗忘现象的全面调查：超越连续学习

    A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])

    [http://arxiv.org/abs/2307.09218](http://arxiv.org/abs/2307.09218)

    遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。

    

    遗忘指的是先前获取的信息或知识的丧失或恶化。尽管现有的关于遗忘的调查主要集中在连续学习方面，但在深度学习中，遗忘是一种普遍现象，可以在各种其他研究领域中观察到。遗忘在研究领域中表现出来，例如由于生成器漂移而在生成模型领域中表现出来，以及由于客户端之间存在异构数据分布而在联邦学习中表现出来。解决遗忘问题涉及到几个挑战，包括在快速学习新任务的同时平衡保留旧任务知识，管理任务干扰与冲突目标，以及防止隐私泄露等。此外，大多数现有的连续学习调查都默认认为遗忘总是有害的。相反，我们的调查认为遗忘是一把双刃剑，在某些情况下可以是有益且可取的，例如隐私保护场景。通过在更广泛的背景下探讨遗忘现象，

    Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
    
[^75]: 使用大型语言模型增强密集检索的软提示调优

    Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2307.08303](http://arxiv.org/abs/2307.08303)

    本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。

    

    密集检索（DR）将查询和文档转化为密集向量表示，并在向量空间中测量查询与文档之间的相似性。DR的一个挑战是缺乏领域特定的训练数据。虽然DR模型可以通过迁移学习从大规模公共数据集（如MS MARCO）中学习，但证据表明，并非所有DR模型和领域都能同等受益于迁移学习。最近，一些研究人员转向使用大型语言模型（LLMs）来改进零样本和少样本的DR模型。然而，这些方法中采用的硬提示或人工编写的提示无法保证生成的弱查询的质量。为了解决这个问题，我们提出了用于增强DR的软提示调优（SPTAR）：对于每个任务，我们利用软提示调优在有限的真实数据上优化任务特定的软提示，然后用这些提示引导LLMs为未标记的文档标记弱查询，从而得到足够的弱文档-查询对来训练任务特定的模型。

    Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
    
[^76]: RegExplainer: 在回归任务中生成图神经网络的解释

    RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])

    [http://arxiv.org/abs/2307.07840](http://arxiv.org/abs/2307.07840)

    这项工作提出了一种新的解释方法（XAIG-R），用于解释图回归模型，通过引入信息瓶颈理论的新目标和混合框架来解决回归任务中的挑战，同时还使用对比学习策略来处理连续有序标签。

    

    图回归是一项基础任务，在各种图学习任务中受到越来越多的关注。然而，推理过程通常是不可解释的。现有的解释技术大多限于理解分类任务中图神经网络的行为。在这项工作中，我们寻求解释来解释图回归模型（XAIG-R）。我们展示了现有方法忽视了分布偏移和连续有序的决策边界，这阻碍了它们在回归任务中的应用。为了解决这些挑战，我们提出了一种基于信息瓶颈理论的新目标，并引入了一种新的混合框架，可以以模型无关的方式支持各种图神经网络。我们进一步提出了一种对比学习策略来应对回归任务中的连续有序标签。为了从经验上验证所提出的方法的有效性，我们引入了三个基准数据集和一个真实数据集进行评估。

    Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
    
[^77]: 将课程与回放相结合：对持续学习的影响

    Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])

    [http://arxiv.org/abs/2307.05747](http://arxiv.org/abs/2307.05747)

    将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。

    

    人类在获取新技能或知识时，通过课程进行学习和复习。这种人类学习行为启发了在持续学习代理中将课程与回放方法相结合。目标是模拟人类学习过程，从而提高知识保留和促进学习转移。现有的持续学习代理中的回放方法涉及从先前任务中随机选择和排序数据，已经证明是有效的。然而，有限的研究探讨了将不同课程与回放方法相结合以增强持续学习的问题。我们的研究首次考察了将课程与回放方法相结合对持续学习的影响的三个具体方面：回放实例与训练数据的交替频率，回放实例的顺序，以及选择实例进入回放缓冲区的策略。这些课程设计的方面

    Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
    
[^78]: 转换器训练策略用于预测多个负载时间序列

    Transformer Training Strategies for Forecasting Multiple Load Time Series. (arXiv:2306.10891v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10891](http://arxiv.org/abs/2306.10891)

    转换器模型在预测多负载时间序列方面使用全局训练策略比多变量和本地训练策略具有更好的性能，平均降低了21.8%和12.8%的预测误差。

    

    在未来的智能电网中，准确的负载预测可以帮助在本地平衡供需，并防止电网故障。尽管被监测的客户数量将随着不断推进的智能电表安装而增加，但每个客户的数据量始终是有限的。我们评估了转换器负载预测模型是否受益于转移学习策略，即在多个客户的负载时间序列上训练全局的单变量模型。在使用两个包含数百个客户的数据集进行的实验中，我们发现全局训练策略优于相关工作中使用的多变量和本地训练策略。平均而言，与其他两种策略相比，全局训练策略在从未来一天到一个月的预测时间范围内，预测误差降低了21.8%和12.8%。与线性模型、多层感知机和LSTM模型的比较显示，转换器训练策略效果更好。

    In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that T
    
[^79]: 向可解释的、语言无关的LLMs迈进：大规模语言符号逆向工程

    Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])

    [http://arxiv.org/abs/2306.00017](http://arxiv.org/abs/2306.00017)

    本文提出结合符号表示和自下而上的逆向工程的方法，解决大规模语言模型在真正语言理解上的局限性，实现可解释的、语言无关的LLMs。

    

    大型语言模型（LLMs）取得了一个里程碑，无可否认地改变了人工智能（AI）中许多信仰。然而，当涉及真正的语言理解时，这些LLM的许多限制仍然存在，这些限制是深度神经网络底层架构的副产品。此外，由于它们的亚符号性质，这些模型获得有关语言如何运作的任何知识都将被埋在数十亿个微特征（权重）中，其中没有一个单独的特征有意义，使得这些模型无法解释。为了解决这些限制，我们建议将符号表示的强度与我们认为是LLMs成功的关键结合起来，即在规模上成功地进行自下而上的语言逆向工程。因此，我们主张在符号设置下对语言进行自下而上的逆向工程。一些作者提出了这个项目的提示，我们将进行详细讨论。

    Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
    
[^80]: 使用基于深度学习的方法从脚手架遮挡中进行场景恢复

    Scene restoration from scaffold occlusion using deep learning-based methods. (arXiv:2305.18810v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2305.18810](http://arxiv.org/abs/2305.18810)

    本研究提出了一种基于深度学习的方法，通过像素级分割和图像修复技术，从脚手架遮挡中恢复建筑场景。实验结果表明，该方法在脚手架分割和场景恢复方面表现出色。

    

    建筑中计算机视觉（CV）应用的遮挡问题引起了很大的关注，特别是由广覆盖、交叉和不可移动的脚手架引起的遮挡问题。直观地，去除脚手架并恢复被遮挡的视觉信息可以为CV系统提供更清晰的场景视图，从而帮助它们更好地理解建筑场景。因此，本研究提出了一种新颖的两步方法，结合像素级分割和图像修复，用于从脚手架遮挡中恢复建筑场景。还开发了一种基于无标签数据的低成本数据合成方法，以解决标记数据短缺的困境。对合成测试数据的实验结果显示，所提出的方法在脚手架分割方面的平均交叉联合率（MIoU）达到92％，在从脚手架遮挡中恢复场景方面的结构相似性（SSIM）达到82％以上。

    The occlusion issues of computer vision (CV) applications in construction have attracted significant attention, especially those caused by the wide-coverage, crisscrossed, and immovable scaffold. Intuitively, removing the scaffold and restoring the occluded visual information can provide CV agents with clearer site views and thus help them better understand the construction scenes. Therefore, this study proposes a novel two-step method combining pixel-level segmentation and image inpainting for restoring construction scenes from scaffold occlusion. A low-cost data synthesis method based only on unlabeled data is developed to address the shortage dilemma of labeled data. Experiments on the synthesized test data show that the proposed method achieves performances of 92% mean intersection over union (MIoU) for scaffold segmentation and over 82% structural similarity (SSIM) for scene restoration from scaffold occlusion.
    
[^81]: 什么症状以及持续多久？一种可解释的人工智能方法用于社交媒体中的抑郁症检测。

    What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media. (arXiv:2305.13127v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2305.13127](http://arxiv.org/abs/2305.13127)

    本论文介绍了一种使用可解释的人工智能方法在社交媒体中检测抑郁症的方法。该方法创新地检测和解释抑郁症状及其持续时间，并通过大规模数据集的实证分析表明优于其他方法，发现了新的未注意到的症状。

    

    抑郁症是最常见和严重的精神疾病，引发了严重的经济和社会影响。抑郁症的检测对于早期干预以减轻这些后果至关重要。这种高风险的决策本质上需要可解释性。虽然有一些抑郁症检测研究试图基于重要性分数或关注权重解释决策，但这些解释与临床抑郁症诊断标准不一致，后者基于抑郁症状。为了填补这一空白，我们遵循计算设计科学范式，开发了一种新颖的多尺度时间原型网络(MSTPNet)。MSTPNet创新地检测和解释抑郁症状及其持续时间。使用大规模数据集进行深入实证分析表明，MSTPNet在F1分数0.851上优于最先进的抑郁症检测方法。这个结果还揭示了未在调查方法中注意到的新症状，例如分享。

    Depression is the most prevalent and serious mental illness, which induces grave financial and societal ramifications. Depression detection is key for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few depression detection studies attempt to explain the decision based on the importance score or attention weights, these explanations misalign with the clinical depression diagnosis criterion that is based on depressive symptoms. To fill this gap, we follow the computational design science paradigm to develop a novel Multi-Scale Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and interprets depressive symptoms as well as how long they last. Extensive empirical analyses using a large-scale dataset show that MSTPNet outperforms state-of-the-art depression detection methods with an F1-score of 0.851. This result also reveals new symptoms that are unnoted in the survey approach, such as shari
    
[^82]: LEA: 学习优化策略的超越进化算法

    LEA: Beyond Evolutionary Algorithms via Learned Optimization Strategy. (arXiv:2304.09599v1 [cs.NE])

    [http://arxiv.org/abs/2304.09599](http://arxiv.org/abs/2304.09599)

    LEA是一种适应性强且能够有效利用目标任务低保真度信息的学习进化算法，从而比传统进化算法在更少的计算成本下获得更好的解决方案。

    

    进化算法已成为昂贵黑盒优化的强大框架。在更少的计算成本下获得更好的解决方案对于黑盒优化至关重要且具有挑战性。最关键的障碍是找出如何有效利用目标任务信息来形成高效的优化策略。然而，当前的方法由于优化策略的表征不足以及优化策略与目标任务之间的低效交互而显得薄弱。为了克服上述限制，我们设计了一种学习进化算法（LEA），以实现从手动设计的优化策略到学习优化策略的转换，其中包括超参数和更新规则。与传统进化算法不同，LEA对目标任务具有高适应性，并且可以在更少的计算成本下获得更好的解决方案。LEA还能够有效地利用目标任务的低保真度信息来形成高效的优化策略。

    Evolutionary algorithms (EAs) have emerged as a powerful framework for expensive black-box optimization. Obtaining better solutions with less computational cost is essential and challenging for black-box optimization. The most critical obstacle is figuring out how to effectively use the target task information to form an efficient optimization strategy. However, current methods are weak due to the poor representation of the optimization strategy and the inefficient interaction between the optimization strategy and the target task. To overcome the above limitations, we design a learned EA (LEA) to realize the move from hand-designed optimization strategies to learned optimization strategies, including not only hyperparameters but also update rules. Unlike traditional EAs, LEA has high adaptability to the target task and can obtain better solutions with less computational cost. LEA is also able to effectively utilize the low-fidelity information of the target task to form an efficient op
    
[^83]: 防止注意力熵崩溃的Transformer训练稳定性研究

    Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])

    [http://arxiv.org/abs/2303.06296](http://arxiv.org/abs/2303.06296)

    本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。

    This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.

    训练稳定性对于Transformer至关重要。本文通过研究注意力层的演变来探究Transformer的训练动态。特别地，我们在训练过程中跟踪每个注意力头的注意力熵，这是模型锐度的代理。我们发现，在不同的架构和任务中存在一种常见模式，即低注意力熵伴随着高训练不稳定性，这可能采取振荡损失或发散的形式。我们将病态低注意力熵，对应高度集中的注意力分数，称为$\textit{熵崩溃}$。作为一种解决方案，我们提出了$\sigma$Reparam，一种简单而有效的解决方案，其中我们使用谱归一化和额外的学习标量重新参数化所有线性层。我们证明了所提出的重新参数化成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。此外，我们

    Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
    
[^84]: 线性卷积神经网络仅利用最主导的频率发现数据集的统计结构

    Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies. (arXiv:2303.02034v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02034](http://arxiv.org/abs/2303.02034)

    本研究通过理论分析和实验研究，揭示了线性卷积神经网络在学习过程中通过发现数据集的统计结构，并且仅利用数据集中最主导的频率进行发现。

    

    本文通过分析梯度下降方程，提出了线性卷积神经网络学习理论的一个突破。我们发现，网络在训练过程中的演变是由数据集结构和卷积网络结构之间的相互作用所决定的。我们表明，线性卷积神经网络通过非线性、有序、阶段性的转变来发现数据集的统计结构，并且发现发现的速度取决于数据集和卷积网络结构之间的关系。此外，我们发现这种相互作用是我们所称的“主导频率偏差”的核心，线性卷积神经网络仅利用数据集中不同结构部分的主导频率来进行这些发现。我们还通过实验证明了我们的理论与实际使用的深度非线性卷积神经网络之间的关系。

    We here present a stepping stone towards a deeper understanding of convolutional neural networks (CNNs) in the form of a theory of learning in linear CNNs. Through analyzing the gradient descent equations, we discover that the evolution of the network during training is determined by the interplay between the dataset structure and the convolutional network structure. We show that linear CNNs discover the statistical structure of the dataset with non-linear, ordered, stage-like transitions, and that the speed of discovery changes depending on the relationship between the dataset and the convolutional network structure. Moreover, we find that this interplay lies at the heart of what we call the ``dominant frequency bias'', where linear CNNs arrive at these discoveries using only the dominant frequencies of the different structural parts present in the dataset. We furthermore provide experiments that show how our theory relates to deep, non-linear CNNs used in practice. Our findings shed 
    
[^85]: 基于时空Transformer引导的扩散数据增强方法用于高效的基于骨骼的动作识别

    Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition. (arXiv:2302.13434v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13434](http://arxiv.org/abs/2302.13434)

    本论文介绍了一种基于时空Transformer引导的扩散数据增强方法，旨在为基于骨骼的动作识别任务生成高质量和多样化的序列动作。通过引入去噪扩散概率模型（DDPMs），本方法能够生成自然逼真的动作序列。

    

    最近，基于骨骼的人体动作成为了热门的研究课题，因为人体骨骼的紧凑表达给这个研究领域注入了新活力。因此，研究者开始注意到使用RGB或其他传感器来通过提取骨骼信息分析人体动作的重要性。利用深度学习的快速发展，最近提出了许多基于骨骼的人体动作方法，这些方法都有精心设计的深度学习结构。然而，一个训练良好的深度学习模型总是需要高质量和充足的数据，而这往往需要高昂的费用和人力资源。本文介绍了一种新颖的基于骨骼动作识别任务的数据增强方法，该方法可以有效生成高质量和多样化的序列动作。为了获得自然和逼真的动作序列，我们提出了去噪扩散概率模型（DDPMs），它可以生成一系列合成动作序列。

    Recently, skeleton-based human action has become a hot research topic because the compact representation of human skeletons brings new blood to this research domain. As a result, researchers began to notice the importance of using RGB or other sensors to analyze human action by extracting skeleton information. Leveraging the rapid development of deep learning (DL), a significant number of skeleton-based human action approaches have been presented with fine-designed DL structures recently. However, a well-trained DL model always demands high-quality and sufficient data, which is hard to obtain without costing high expenses and human labor. In this paper, we introduce a novel data augmentation method for skeleton-based action recognition tasks, which can effectively generate high-quality and diverse sequential actions. In order to obtain natural and realistic action sequences, we propose denoising diffusion probabilistic models (DDPMs) that can generate a series of synthetic action seque
    
[^86]: 以局部差分隐私为基础的联邦学习中的主动成员推断攻击

    Active Membership Inference Attack under Local Differential Privacy in Federated Learning. (arXiv:2302.12685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12685](http://arxiv.org/abs/2302.12685)

    这项研究提出了一种以局部差分隐私为基础的联邦学习中的主动成员推断攻击，该攻击利用恶意参数干扰全局模型并通过非线性决策边界推断客户端的私有训练数据，对客户端的隐私造成显著风险，并发现防止攻击所需的保护隐私噪声会严重损害联邦学习的模型效用。

    

    联邦学习（FL）最初被视为在具有数据隐私保护的协调服务器上进行协作学习的框架。本文提出了一种由不诚实服务器在FL中进行的新型主动成员推断（AMI）攻击。在AMI攻击中，服务器制造并嵌入恶意参数到全局模型中，以有效推断目标数据样本是否包含在客户端的私有训练数据中。通过利用数据特征之间的相关性通过非线性决策边界，AMI攻击在严格的局部差分隐私（LDP）保护下可以实现极高的成功率，从而使客户端的训练数据面临显著的隐私风险。在几个基准数据集上的理论和实验结果表明，为防止我们的攻击而添加足够的保护隐私的噪声会显著损害FL的模型效用。

    Federated learning (FL) was originally regarded as a framework for collaborative learning among clients with data privacy protection through a coordinating server. In this paper, we propose a new active membership inference (AMI) attack carried out by a dishonest server in FL. In AMI attacks, the server crafts and embeds malicious parameters into global models to effectively infer whether a target data sample is included in a client's private training data or not. By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection; thereby exposing clients' training data to significant privacy risk. Theoretical and experimental results on several benchmark datasets show that adding sufficient privacy-preserving noise to prevent our attack would significantly damage FL's model utility.
    
[^87]: 流行的人工神经网络激活函数的统一化

    Unification of popular artificial neural network activation functions. (arXiv:2302.11007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11007](http://arxiv.org/abs/2302.11007)

    激活函数的统一化表示采用了Mittag-Leffler函数，可以插值不同激活函数、减轻梯度问题，并适用于不同复杂度的神经网络训练。

    

    我们提出了最流行的神经网络激活函数的统一表示。采用了分数微积分的Mittag-Leffler函数，我们提出了一种灵活且紧凑的功能形式，能够在不同的激活函数之间进行插值，并减轻训练神经网络中常见的问题，如梯度消失和梯度爆炸。所提出的门控表示扩展了固定形状激活函数的范围，将其转化为自适应对应物，其形状可以从训练数据中学习。所提出的函数形式的导数也可以用Mittag-Leffler函数表示，因此它是梯度下降反向传播算法的合适候选。通过在不同复杂度和不同大小的数据集上训练多个神经网络，我们证明采用统一的门控激活函数表示为各种内置实现的经济的和有希望的替代方法。

    We present a unified representation of the most popular neural network activation functions. Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions and mitigate common problems in training neural networks such as vanishing and exploding gradients. The presented gated representation extends the scope of fixed-shape activation functions to their adaptive counterparts whose shape can be learnt from the training data. The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms. By training multiple neural networks of different complexities on various datasets with different sizes, we demonstrate that adopting a unified gated representation of activation functions offers a promising and affordable alternative to individual built-in implementations of ac
    
[^88]: 通过自我对战实现多样化诱导的环境设计

    Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02119](http://arxiv.org/abs/2302.02119)

    本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。

    

    最近关于环境分布设计的研究已经展示出训练有效的通用能力代理的前景。它的成功部分在于一种自适应课程学习的形式，该形式通过生成代理能力的前沿环境实例（或级别）。然而，这种环境设计框架经常在具有挑战性的设计空间中发现有效级别方面存在困难，并需要与环境进行高成本交互。本文的目的是在非监督环境设计（UED）框架中引入多样性。具体来说，我们提出了一种任务不可知的方法来识别对给定级别具有代表性的观察/隐藏状态。然后利用这种方法的结果来表征两个级别之间的多样性，正如我们所展示的，这对于有效性能至关重要。此外，为了提高采样效率，我们加入了自我对战技术，使得环境生成器能够自动生成环境。

    Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
    
[^89]: FedTracker：为联邦学习模型提供所有权验证和可追溯性的保护机制

    FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model. (arXiv:2211.07160v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.07160](http://arxiv.org/abs/2211.07160)

    FedTracker是第一个为联邦学习模型提供所有权验证和追溯性的保护框架，采用双层保护方案，并利用持续学习原则提高保护性能。

    

    联邦学习（FL）是一种分布式机器学习范式，允许多个客户端协同训练一个全局模型，而无需共享他们的本地数据。然而，FL需要将模型暴露给各种参与者，这可能导致恶意客户端未经授权地分发或转售模型，从而损害FL团队的知识产权。为了阻止这种不当行为，建立一种验证模型所有权并追溯泄露者的机制至关重要。本文提出了FedTracker，这是第一个提供所有权验证和可追溯性的FL模型保护框架。FedTracker采用双层保护方案，包括全局水印机制和本地指纹机制。前者用于验证全局模型的所有权，而后者用于识别该模型来自哪个客户端。FedTracker利用持续学习（CL）原则来提高模型的保护性能。

    Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) princ
    
[^90]: 修订Transformer：指导语言模型改变其价值观

    Revision Transformers: Instructing Language Models to Change their Values. (arXiv:2210.10332v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10332](http://arxiv.org/abs/2210.10332)

    本论文提出了修订Transformer（RiT），旨在解决当前Transformer语言模型中出现的捷径学习和偏见问题，以便更方便地进行模型更新。RiT采用了大规模预训练的语言模型和清晰结构的修订引擎的组合，通过少量的努力和用户互动，可以轻松更新模型的知识。在道德数据集上的实验结果表明RiT在模型修订方面表现出强大的性能。

    

    当前的Transformer语言模型是具有数十亿个参数的大规模模型。它们在各种任务上表现出很高的性能，但也容易出现捷径学习和偏见。通过参数调整来解决这类不正确的模型行为非常昂贵。对于更新文化或个人之间变化的道德价值等动态概念尤其棘手。在这项工作中，我们对将所有信息存储在模型参数中的当前常见做法提出质疑，并提出了修订Transformer（RiT）来促进模型的轻松更新。大规模预训练的语言模型与清晰结构的修订引擎的特定组合使得在少量的努力和用户互动的帮助下更新模型的知识成为可能。我们在一个道德数据集上示范了RiT，并模拟了用户反馈，展示了模型修订的强大性能。

    Current transformer language models (LM) are large-scale models with billions of parameters. They have been shown to provide high performances on a variety of tasks but are also prone to shortcut learning and bias. Addressing such incorrect model behavior via parameter adjustments is very costly. This is particularly problematic for updating dynamic concepts, such as moral values, which vary culturally or interpersonally. In this work, we question the current common practice of storing all information in the model parameters and propose the Revision Transformer (RiT) to facilitate easy model updating. The specific combination of a large-scale pre-trained LM that inherently but also diffusely encodes world knowledge with a clear-structured revision engine makes it possible to update the model's knowledge with little effort and the help of user interaction. We exemplify RiT on a moral dataset and simulate user feedback demonstrating strong performance in model revision even with small da
    
[^91]: 自监督训练的视频预训练产生与人类对齐的视觉表示

    Self-supervised video pretraining yields human-aligned visual representations. (arXiv:2210.06433v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.06433](http://arxiv.org/abs/2210.06433)

    本论文通过自监督训练的视频预训练方法VITO得到了具有人类感知特征的视觉表示，该方法在图像理解和视频理解任务上表现出更好的泛化性和鲁棒性。

    

    人类通过观察对象和场景随时间演变的方式学习到了强大的表示。然而，在不需要明确的时间理解的特定任务之外，静态图像预训练仍然是学习视觉基础模型的主流范式。我们对这种不匹配提出了质疑，并且问是否视频预训练可以产生具有人类感知特征的视觉表示：在各种任务中的泛化性、对扰动的鲁棒性和与人类判断的一致性。为此，我们提出了一种用于筛选视频的新颖程序，并开发了一个对比性框架，从其中的复杂转换中学习。这种从视频中提炼知识的简单范式被称为VITO，它产生的一般表示在图像理解任务上远远优于先前的视频预训练方法，并且在视频理解任务上优于图像预训练方法。此外，VITO表示对自然和合成形变的鲁棒性显著提高。

    Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformati
    
[^92]: 人工导航者自发地形成了具有社交和记忆引导的累积文化

    Cumulative culture spontaneously emerges in artificial navigators who are social and memory-guided. (arXiv:2206.06281v3 [q-bio.PE] UPDATED)

    [http://arxiv.org/abs/2206.06281](http://arxiv.org/abs/2206.06281)

    本文通过研究最小认知架构的人工代理，展示出累积文化可以在社交和记忆引导下自发出现，不依赖于高级认知能力。在这个系统中，经验丰富的导航者可以通过已建立的路径传授给幼稚个体，而这样做对于导航者来说也是有益的。

    

    累积文化进化发生在通过社交学习将适应性创新传递给连续的后代时。这个过程塑造了人类的技术创新，但也发生在非人类物种中。尽管传统上认为累积文化依赖于高保真度的社交传输和先进的认知能力，但我在这里展示了一个更简单的系统足够。累积文化在具有目标导向、社交接近性和路径记忆的最小认知架构的人工代理中自发出现。在每一代中，经验有限的个体通过与经验导航者配对从而受益，因为他们可以遵循先前建立的路径。关键是，经验丰富的导航者也通过回归到目标受益于有经验的导航者的存在。当经验丰富的代理人遵循他们记忆中的路径时，他们没有路径记忆的幼稚同伴更有可能朝向目标而不是远离目标。

    Cumulative cultural evolution occurs when adaptive innovations are passed down to consecutive generations through social learning. This process has shaped human technological innovation, but also occurs in non-human species. While it is traditionally argued that cumulative culture relies on high-fidelity social transmission and advanced cognitive skills, here I show that a much simpler system suffices. Cumulative culture spontaneously emerged in artificial agents who navigate with a minimal cognitive architecture of goal-direction, social proximity, and route memory. Within each generation, naive individuals benefitted from being paired with experienced navigators because they could follow previously established routes. Crucially, experienced navigators also benefitted from the presence of naive individuals through regression to the goal. As experienced agents followed their memorised path, their naive counterparts (unhindered by route memory) were more likely to err towards than away 
    
[^93]: 强化学习中的张量和矩阵低秩值函数近似

    Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning. (arXiv:2201.09736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09736](http://arxiv.org/abs/2201.09736)

    本文提出了一种在高维空间中使用随机低秩算法进行价值函数近似的方法，并提出了使用张量表示和PARAFAC分解的在线无模型的张量低秩算法。

    

    价值函数（VF）的近似是强化学习中的一个核心问题。传统的非参数VF估计在维度灾难的情况下存在问题。因此，在高维空间中，人们采用了简洁的参数模型来近似VF，其中大部分工作集中在线性和神经网络方法上。与此不同的是，本文提出了一种“简洁的非参数”方法，我们使用随机低秩算法以在线和无模型的方式来估计VF矩阵。此外，由于VF往往是多维的，我们提出用张量（多维数组）表示来替代传统的VF矩阵表示，并采用PARAFAC分解来设计一个在线无模型的张量低秩算法。我们提出了不同版本的算法，分析了它们的复杂度，并通过使用标准强化学习环境对其性能进行了数值评估。

    Value-function (VF) approximation is a central problem in Reinforcement Learning (RL). Classical non-parametric VF estimation suffers from the curse of dimensionality. As a result, parsimonious parametric models have been adopted to approximate VFs in high-dimensional spaces, with most efforts being focused on linear and neural-network-based approaches. Differently, this paper puts forth a a \emph{parsimonious non-parametric} approach, where we use \emph{stochastic low-rank algorithms} to estimate the VF matrix in an online and model-free fashion. Furthermore, as VFs tend to be multi-dimensional, we propose replacing the classical VF matrix representation with a tensor (multi-way array) representation and, then, use the PARAFAC decomposition to design an online model-free tensor low-rank algorithm. Different versions of the algorithms are proposed, their complexity is analyzed, and their performance is assessed numerically using standardized RL environments.
    
[^94]: 基于语义驱动的提案生成的全场景图像文字人物搜索

    Text-based Person Search in Full Images via Semantic-Driven Proposal Generation. (arXiv:2109.12965v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.12965](http://arxiv.org/abs/2109.12965)

    本文提出了一种基于语义驱动的提案生成的全场景图像文字人物搜索方法，通过端到端学习优化行人检测、身份识别和视觉-语义特征嵌入任务，并利用语义特征指导区域建议网络关注文本描述的提案。使用跨尺度的视觉-语义嵌入机制来提高性能

    

    在智能视频监控中，使用文本描述来在全场景图像中找到目标人物具有重要的实际应用。然而，与现实世界的场景不同，在现有的基于文本的人物检索方法中，主要集中在查询文本描述和裁剪的行人图像库之间的跨模态匹配。为了弥补这个差距，我们研究了基于文本的全场景图像中的人物搜索问题，提出了一个新的端到端学习框架，同时优化行人检测、身份识别和视觉-语义特征嵌入任务。为了充分利用查询文本，语义特征被利用来指导区域建议网络更关注文本描述的提案。此外，还利用了跨尺度的视觉-语义嵌入机制来提高性能。为了验证所提出的方法，我们收集并标注了两个大规模的基准数据集。

    Finding target persons in full scene images with a query of text description has important practical applications in intelligent video surveillance.However, different from the real-world scenarios where the bounding boxes are not available, existing text-based person retrieval methods mainly focus on the cross modal matching between the query text descriptions and the gallery of cropped pedestrian images. To close the gap, we study the problem of text-based person search in full images by proposing a new end-to-end learning framework which jointly optimize the pedestrian detection, identification and visual-semantic feature embedding tasks. To take full advantage of the query text, the semantic features are leveraged to instruct the Region Proposal Network to pay more attention to the text-described proposals. Besides, a cross-scale visual-semantic embedding mechanism is utilized to improve the performance. To validate the proposed method, we collect and annotate two large-scale benchm
    

