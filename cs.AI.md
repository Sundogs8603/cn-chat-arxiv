# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation](https://arxiv.org/abs/2402.10210) | 本文介绍了一种创新的技术，称为自我对抗微调扩散模型（SPIN-Diffusion），通过扩散模型与其先前版本的竞争，实现了逐步自我改进过程。 |
| [^2] | [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://arxiv.org/abs/2402.10207) | 本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。 |
| [^3] | [Ising on the Graph: Task-specific Graph Subsampling via the Ising Model](https://arxiv.org/abs/2402.10206) | 该论文提出了一种基于伊辛模型的图子抽样方法，可以针对特定任务在图结构上进行减小，并通过学习伊辛模型的外部磁场来实现。该方法的多功能性在图像分割、三维形状稀疏化和稀疏逼近矩阵求逆等应用中得到展示。 |
| [^4] | [Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model](https://arxiv.org/abs/2402.10204) | 该研究提出使用有条件降噪扩散模型从不干净的射电图像中重建天空模型，以实现准确定位和测量流量，为射电源的表征提供潜在改进。 |
| [^5] | [A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents](https://arxiv.org/abs/2402.10196) | 本文是第一个对语言代理的敌对攻击进行映射的系统化努力。它提供了一个统一的概念框架来研究这些攻击。这有助于我们理解语言代理的安全风险。 |
| [^6] | [Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias](https://arxiv.org/abs/2402.10192) | 该论文引入了多激发投影模拟（mePS），通过在超图上多个粒子的随机游走，解决了投影模拟（PS）无法模拟同时结合多个概念的思维的问题。 |
| [^7] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^8] | [Large Scale Constrained Clustering With Reinforcement Learning](https://arxiv.org/abs/2402.10177) | 本文介绍了一种使用强化学习解决大规模受限制聚类问题的方法，该方法训练一个代理器生成既可行又接近最优解的解决方案，以提高资源分配和使用的效率。 |
| [^9] | [OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset](https://arxiv.org/abs/2402.10176) | OpenMathInstruct-1是一个包含180万个数学问题和解决方法对的数据集，通过合成开源LLM的代码解释器解决方案来构建，填补了目前开源LLM在数学技能方面与闭源LLM之间的差距。 |
| [^10] | [OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models](https://arxiv.org/abs/2402.10172) | OptiMUS是一个基于大型语言模型的代理，通过处理自然语言描述来制定和解决(混合整数)线性规划问题，其在易于数据集上表现优于现有的最先进方法。 |
| [^11] | [Data Engineering for Scaling Language Models to 128K Context](https://arxiv.org/abs/2402.10171) | 本论文研究了将语言模型的上下文长度扩展到128K的连续预训练方法，通过适当的数据混合和轻量级的预训练可以实现，其中关键要点在于数据的数量和质量，需要注意领域平衡和长度上采样。 |
| [^12] | [DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning](https://arxiv.org/abs/2402.10168) | DeepSRGM是一种基于深度学习的Raga识别方法，通过使用LSTM-RNN学习音乐数据中的时间序列，达到了88.1%和97%的准确率，在Raga识别任务中取得了最新技术的地位。 |
| [^13] | [Tracking Changing Probabilities via Dynamic Learners](https://arxiv.org/abs/2402.10142) | 该论文介绍了通过动态学习器追踪概率变化的方法，通过输出候选项目及其概率来预测离散项目序列中下一个可能出现的项目。 |
| [^14] | [Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data](https://arxiv.org/abs/2402.10135) | 这项研究对生物医学数据的对等联邦学习进行了基准测试，并测试了各种聚合策略，包括加权平均聚合，以确定最强大的策略。 |
| [^15] | [Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem](https://arxiv.org/abs/2402.10133) | 本文提出了一种使用大型语言模型实现个性化的新方法，以降低个性化程序化内容生成的门槛，从而实现游戏内容与玩家偏好的匹配。 |
| [^16] | [Is Continual Learning Ready for Real-world Challenges?](https://arxiv.org/abs/2402.10130) | 本文研究了连续学习在现实世界场景中的应用，发现当前的评估方法与实际挑战不匹配，现有解决方案无法有效解决复杂的现实世界环境下的问题。 |
| [^17] | [Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN](https://arxiv.org/abs/2402.10115) | 本研究使用基于Transformer编码器的EEG编码器和GAN网络，通过合成图像从EEG信号中恢复出各种对象类别的图像，同时结合对抗损失和感知损失，提高生成图像的质量。 |
| [^18] | [Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning](https://arxiv.org/abs/2402.10110) | 本文介绍了一种名为选择性反射调节的新方法，该方法通过教师LLM的反射和自省与学生LLM的数据选择能力相结合，自动优化现有的指令调节数据，从而实现了高效的指令调节和卓越性能的LLM。 |
| [^19] | [Towards Reducing Diagnostic Errors with Interpretable Risk Prediction](https://arxiv.org/abs/2402.10109) | 本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，旨在通过增加证据的获取与减少诊断错误来降低诊断错误。模型使用神经加性模型进行预测，以证据为后盾，并给出个体化风险估计，特别针对诊断延迟和来自不完整鉴别的错误进行优化。 |
| [^20] | [Quantized Embedding Vectors for Controllable Diffusion Language Models](https://arxiv.org/abs/2402.10107) | 本论文提出了一种量化嵌入可控扩散语言模型（QE-CDLM），通过重建任务特定的嵌入空间来改善扩散语言模型的可控性、可移植性和稳定性。 |
| [^21] | [GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving](https://arxiv.org/abs/2402.10104) | GeoEval基准测试用于评估LLMs和MMs在几何问题解决上的性能，发现WizardMath模型在主要子集上表现出色，但在具有挑战性的子集上准确率较低。 |
| [^22] | [A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research](https://arxiv.org/abs/2402.10102) | 本论文提出了一种隐私保护的、分布式的、合作的基于FCM的学习方法，用于癌症研究，通过联邦学习过程改进了模型的性能。 |
| [^23] | [Deep Learning Based Situation Awareness for Multiple Missiles Evasion](https://arxiv.org/abs/2402.10101) | 该研究提出了一种基于深度学习的决策支持工具，用于帮助无人机操作员在多个导弹威胁下进行决策，通过学习高保真度模拟来评估各种策略的风险，并建议最安全的行动方针。 |
| [^24] | [Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data](https://arxiv.org/abs/2402.10100) | 本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。 |
| [^25] | [MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations](https://arxiv.org/abs/2402.10093) | MIM-Refiner是一种对比学习提升方法，通过利用MIM模型中的中间层表示和多个对比头，能够将MIM模型的特征从次优的状态提升到最先进的状态，并在ImageNet-1K数据集上取得了新的最先进结果。 |
| [^26] | [Text-Based Product Matching -- Semi-Supervised Clustering Approach](https://arxiv.org/abs/2402.10091) | 本文介绍了一种利用半监督聚类方法进行产品匹配的新思路，并通过实验证明了无监督匹配与少量注释样本的产品链接可以成为主导的监督策略的替代方法。 |
| [^27] | [Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review](https://arxiv.org/abs/2402.10086) | 可解释的AI技术对于解决自动驾驶中的安全问题和信任问题至关重要。本文通过系统文献综述的方式，分析了可解释的AI方法在满足自动驾驶要求方面的关键贡献，并提出了可解释的设计、可解释的替代模型、可解释的监控、辅助技术和解释的可视化等五个方面的应用。 |
| [^28] | [Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4](https://arxiv.org/abs/2402.10083) | 本论文评估了使用GPT-4的LLM评估的临床一致性，以评估经过精调的LLM聊天机器人生成的眼科患者查询的回答。通过与医生排序进行对比，发现GPT-3.5在临床上的一致性比其他经过精调的LLM更高。 |
| [^29] | [QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference](https://arxiv.org/abs/2402.10076) | QUICK是一组针对量化大语言模型（LLMs）的高效推理的优化CUDA内核。通过解决共享内存冲突问题和交错量化权重矩阵，QUICK实现了显著的速度提升和吞吐量增益。 |
| [^30] | [LLM-based policy generation for intent-based management of applications](https://arxiv.org/abs/2402.10067) | 这项研究提出了基于LLM的策略生成方法，用于实现自动化的应用意图管理。通过生成逐步分解意图所需的动作，并将其映射到API，实现了闭控制循环来自动化策略执行。 |
| [^31] | [Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network](https://arxiv.org/abs/2402.10055) | 通过实例分割神经网络实现了一种鲁棒的半自动血管追踪算法，能够追踪每棵血管树的分支。 |
| [^32] | [Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination](https://arxiv.org/abs/2402.10052) | 本研究提出了一种新颖的大型语言模型遗忘方法，通过自我蒸馏和有意识的想象，有效地遗忘目标文本，并在生成任务和自然语言理解任务中保留模型的能力。 |
| [^33] | [SwissNYF: Tool Grounded LLM Agents for Black Box Setting](https://arxiv.org/abs/2402.10051) | 该论文提出了一种基于黑盒环境的基于LLM的工具生成智能体的方法，在复杂的API调用中表现出了优越的性能，可以应对具有不可逆性和大量时间消耗的任务。 |
| [^34] | [On-Demand Myoelectric Control Using Wake Gestures to Eliminate False Activations During Activities of Daily Living](https://arxiv.org/abs/2402.10050) | 在日常生活中，通过使用唤醒手势作为一种按需肌电控制范式，能够有效减少虚假激活，提高肌电控制的可靠性。 |
| [^35] | [RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models](https://arxiv.org/abs/2402.10038) | 本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。 |
| [^36] | [Diffusion Models Meet Contextual Bandits with Large Action Spaces](https://arxiv.org/abs/2402.10028) | 本文设计了一种利用预训练扩散模型的扩散汤普森采样方法，用于在大动作空间下进行高效的情境强化学习探索。实证评估结果表明了该方法的优越性能。 |
| [^37] | [Self-Augmented In-Context Learning for Unsupervised Word Translation](https://arxiv.org/abs/2402.10024) | 通过自学习上下文增强方法，本论文提出一种无监督词汇翻译的方法，在零样本提示的大型语言模型上取得了显著的改进，超过了传统基于映射的方法。 |
| [^38] | [Clifford Group Equivariant Simplicial Message Passing Networks](https://arxiv.org/abs/2402.10011) | 本论文介绍了一种Clifford群等变单体消息传递网络，通过将Clifford群等变层与单体消息传递相结合，实现了在拓扑上更为复杂的E（n）-等变消息传递。实验结果表明，该方法具有良好的效果。 |
| [^39] | [ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal Processing Analysis for Sounds, & Strains Emerging Technology](https://arxiv.org/abs/2402.10005) | 本文研究了机器学习在声学信号处理分析中的应用，通过数据驱动的方法，揭示了复杂声学现象的模型。 |
| [^40] | [MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding](https://arxiv.org/abs/2402.10002) | 本文提出了一种新颖的自监督点云表示学习方法MM-Point，通过多模态交互和传输实现了3D物体和多个2D视图之间的信息增强。通过精心设计的实验，证明了MM-Point的有效性和优越性。 |
| [^41] | [LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild](https://arxiv.org/abs/2402.09997) | LoraRetriever提出了一种适应输入的LoRA检索与合成方法，用于弥合实际情况下大型语言模型接收到不同任务提示的差距。 |
| [^42] | [Symmetry-Breaking Augmentations for Ad Hoc Teamwork](https://arxiv.org/abs/2402.09984) | 本研究提出了一种称为对称破缺增强的方法，通过增加训练队友的行为多样性来提高人工智能代理与新队友合作的性能。实验证明了该方法的有效性。 |
| [^43] | [Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition](https://arxiv.org/abs/2402.09982) | 本文提出了一种改进面部表情识别的新型数据增强技术，并应用迁移学习方法，通过使用预训练卷积神经网络在增强的数据集上进行微调，实现了高达85%的平均准确度。 |
| [^44] | [Fast Vocabulary Transfer for Language Model Compression](https://arxiv.org/abs/2402.09977) | 提出了一种基于词汇转移的语言模型压缩方法，通过与其他压缩技术结合使用，显著减小模型大小和推理时间，同时性能略有妥协。 |
| [^45] | [Hierarchy Representation of Data in Machine Learnings](https://arxiv.org/abs/2402.09965) | 该论文提出了一种用于可视化目标间层次关系的方法，对于模型改进具有潜在的益处。 |
| [^46] | [FedLion: Faster Adaptive Federated Optimization with Fewer Communication](https://arxiv.org/abs/2402.09941) | FedLion是一种自适应联邦优化算法，通过引入集中式自适应算法Lion的关键元素，实现了更快的收敛速度和更少的通信成本。经过广泛评估，FedLion优于之前的最先进自适应算法，并通过使用有符号梯度在本地训练中减少数据传输要求。 |
| [^47] | [Generative AI in the Construction Industry: A State-of-the-art Analysis](https://arxiv.org/abs/2402.09939) | 本研究通过分析提供了建筑行业中生成式AI的最新状态、机遇和挑战。同时，提出了一个帮助建筑公司构建定制化生成式AI解决方案的框架。 |
| [^48] | [Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse](https://arxiv.org/abs/2402.09934) | 本研究挖掘了在线话语中的演绎细微差别，提出了一种新的方法来准确检测反问主义，并在Twitter和YouTube数据集中取得了显著的改进。 |
| [^49] | [A Dataset of Open-Domain Question Answering with Multiple-Span Answers](https://arxiv.org/abs/2402.09923) | 提出了一个中文多段答案的开放领域问答数据集CLEAN，弥补了中文MSQA研究中的不足，包括多样的主题和需要详细回答的问题。提供了相关文献中的基线模型作为参考。 |
| [^50] | [Identifying and modelling cognitive biases in mobility choices](https://arxiv.org/abs/2402.09921) | 本论文提出了一种研究移动选择中认知偏差的代理模型，并通过调查结果揭示了决策中存在的各种偏见。最后，论文在GAMA代理模拟中实现了这一模型。 |
| [^51] | [Road Graph Generator: Mapping roads at construction sites from GPS data](https://arxiv.org/abs/2402.09919) | 本研究提出了一种通过分析GPS轨迹来绘制建筑工地道路地图的方法，通过识别关键的交叉口并连接它们，生成道路图，为规划和任务分配提供支持。 |
| [^52] | [Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering](https://arxiv.org/abs/2402.09911) | 使用伪和多源知识图对大型语言模型进行增强，以改善其幻觉问题和提高性能。通过结合伪图生成和原子级知识验证的框架，在开放式问题回答环境中使用知识图可以提高ROUGE-L分数至少11.5。 |
| [^53] | [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906) | 本研究引入了生成表示指令调整（GRIT）方法，通过指令区分生成和嵌入任务，训练一个大型语言模型同时处理这两种任务。与其他模型相比，我们的GritLM 7B在文本嵌入基准测试上达到最新的技术水平，并在多种生成任务中表现出色。通过进一步扩大规模，我们的GritLM 8x7B成为最佳的生成语言模型之一，同时仍然是最好的嵌入模型之一。GRIT的统一也大大提高了RAG在长文档上的速度。 |
| [^54] | [Revisiting Recurrent Reinforcement Learning with Memory Monoids](https://arxiv.org/abs/2402.09900) | 这篇论文重新审视了使用内存单子的循环强化学习方法。通过定义新颖的内存单子框架并提出一种新的批处理方法，改进了样本效率、增加了回报并简化了实现过程。 |
| [^55] | [Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows](https://arxiv.org/abs/2402.09894) | 这项纵向研究调查了生成式AI工作流程的实用性和定制化程度，结果显示，在熟悉化阶段后，用户感知到的系统效用提高了。 |
| [^56] | [Lester: rotoscope animation through video object segmentation and tracking](https://arxiv.org/abs/2402.09883) | Lester是一种通过视频对象分割和跟踪实现Rotoscope动画的新方法，具有出色的时间一致性和适应性。 |
| [^57] | [Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence](https://arxiv.org/abs/2402.09880) | 该论文通过批判性评估研究了23个最先进的大型语言模型基准的不足之处，包括偏见、真实推理衡量困难、实现不一致性等问题，强调了在人工智能时代需要标准化方法、监管确定性和伦理指南。 |
| [^58] | [On Computing Plans with Uniform Action Costs](https://arxiv.org/abs/2402.09877) | 该论文提出了适用于自动规划的三种一致性度量，并引入了基于规划的编译技术，可以生成动作成本一致的计划。 |
| [^59] | [MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music](https://arxiv.org/abs/2402.09871) | MuChin是一个用于评估多模态语言模型在音乐理解和描述方面性能的中文口语描述基准。 |
| [^60] | [Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs](https://arxiv.org/abs/2402.09867) | 该论文研究了在嵌入式多核平台上，采用电池供电的可穿戴设备分析脑电图（EEG）记录的应用。研究发现，通过调整近似方法，可以在有限的能量预算内实现更好的性能和能量收益。 |
| [^61] | [Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent](https://arxiv.org/abs/2402.09844) | Jack of All Trades (JAT)是一个基于Transformer的模型，通过使用一个权重集，展现了在不同领域均能取得强大性能的能力。它是首个实现该目标的开放模型。 |
| [^62] | [Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models](https://arxiv.org/abs/2402.09836) | 本论文提出了一种基于大型语言模型的上下文感知推理，将人类移动生成重新定义为常识推理问题。通过设计新颖的移动生成推理框架（MobiGeaR），将LLMs递归生成移动行为。 |
| [^63] | [Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data](https://arxiv.org/abs/2402.09830) | 本论文研究了利用生成对抗网络（GAN）进行欺诈检测的应用，比较了其与传统方法的优势。通过构建对抗性验证图的集合，有效防止了由机器人或自动系统引起的欺诈，并确保交易中的用户是真实的。 |
| [^64] | [Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection](https://arxiv.org/abs/2402.09820) | 这项研究提出使用深度学习来增强金融行业的网络安全韧性，并实现高级威胁检测。目前的网络威胁检测方法往往基于规则和传统的机器学习方法，无法适用大规模数据应用，并且无法有效检测未知威胁。 |
| [^65] | [An advanced data fabric architecture leveraging homomorphic encryption and federated learning](https://arxiv.org/abs/2402.09795) | 该论文介绍了一种利用同态加密和联邦学习的先进数据融合架构，在不将数据移动到集中位置的情况下，实现了安全的医学图像分析。这种方法可以在保护数据隐私和安全性的同时，多个参与方进行机器学习模型的协作训练。 |
| [^66] | [System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network](https://arxiv.org/abs/2402.09792) | 这项研究提出了一种脉冲列设计补偿技术，以减少Charge Trap Flash（CTF）器件中非理想程序时间所引起的误差。 |
| [^67] | [Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model](https://arxiv.org/abs/2402.09786) | 这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。 |
| [^68] | [Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention](https://arxiv.org/abs/2402.09784) | 该论文基于对比学习和自注意力机制，提出了一种考虑垂直和水平时间接近度的顺序推荐方法，以更好地捕捉用户-项目交互中的时间上下文。 |
| [^69] | [MC-DBN: A Deep Belief Network-Based Model for Modality Completion](https://arxiv.org/abs/2402.09782) | MC-DBN是一种基于深度信念网络的模态补全模型，利用完整数据的隐式特征来弥补附加不完整数据的差距，提高预测准确性。 |
| [^70] | [Representation Learning Using a Single Forward Pass](https://arxiv.org/abs/2402.09769) | 我们提出了一种神经科学启发的算法，可以通过单次前向传递进行表示学习。该算法具有独特的特点，并在不需要反向传播的情况下取得了高性能的分类结果。 |
| [^71] | [From Variability to Stability: Advancing RecSys Benchmarking Practices](https://arxiv.org/abs/2402.09766) | 本论文提出了一种新的基准测试方法，通过使用多样化的开放数据集，并在多个度量指标上评估多种协同过滤算法，来研究数据集特征对算法性能的影响。这一方法填补了推荐系统算法比较中的不足之处，推进了评估实践。 |
| [^72] | [Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2402.09765) | 本文介绍了一种利用强化学习来优化具有时间窗口的随机车辆路径问题的方法，填补了SVRP研究中的空白，并通过使用注意力机制神经网络最小化了路径成本，模型在旅行成本上优于传统的蚁群算法，并且在不同环境中表现出鲁棒性。 |
| [^73] | [Aligning Crowd Feedback via Distributional Preference Reward Modeling](https://arxiv.org/abs/2402.09764) | 本文提出了一种名为分布偏好奖励模型的框架，用于将大型语言模型与多样的人类偏好对齐。该框架使用贝塔分布刻画偏好，并设计了基于最优输运的损失函数来校准模型与偏好的对齐程度。最终利用期望奖励微调语言模型的策略。 |
| [^74] | [Grounding Language Model with Chunking-Free In-Context Retrieval](https://arxiv.org/abs/2402.09760) | 这是一种针对检索增强生成系统（RAG）的无块语境检索方法，通过绕过文本切分的过程，利用编码隐藏状态进行准确地语境检索，解决了传统方法中存在的文本语义连贯性破坏和证据检索中的噪声和不准确性问题。 |
| [^75] | [Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish](https://arxiv.org/abs/2402.09759) | 本研究使用高效的语言自适应预训练方法，成功将基础英文大规模语言模型应用于生成波兰文，并在困惑度和任务表现上取得了显著的改进，为向现有语言模型添加新语言开辟了新途径。 |
| [^76] | [Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming](https://arxiv.org/abs/2402.09750) | 这项研究探索了大型语言模型在艺术家与人工智能合作的创意编程中的艺术潜力，并比较了两种合作方式。研究发现反思类型与用户表现、用户满意度和主观体验相关。通过实验数据和定性访谈，我们从艺术家的角度提供了人工智能合作的批判性视角和设计建议。 |
| [^77] | [Model Compression and Efficient Inference for Large Language Models: A Survey](https://arxiv.org/abs/2402.09748) | 这项综述研究了大规模语言模型的压缩和高效推理方法，包括量化、修剪、蒸馏、紧凑架构设计和动态网络等方面。大模型的突出特点是压缩后需要微调或重新训练，并且相关的成本很高。 |
| [^78] | [Agents Need Not Know Their Purpose](https://arxiv.org/abs/2402.09734) | 本文研究了无意识代理人，他们的行为与人类价值观一致的挑战。他们的有效效用函数是已知和隐藏子函数的聚合，通过限制架构，实现对隐藏子函数的最大化和最小化。 |
| [^79] | [Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System](https://arxiv.org/abs/2402.09729) | 本文研究了在移动边缘计算系统中为异构用户提供定制化VR服务的资源分配问题，提出了一种基于联邦学习和基于prompt的序列建模的框架，名为FedPromptDT，在保证用户体验最高的同时解决了数据不足和用户隐私保护的问题。 |
| [^80] | [AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns](https://arxiv.org/abs/2402.09728) | 该论文介绍了滥用生成型AI聊天机器人创建短信钓鱼攻击的方法，展示了它们在真实世界中的潜在危害。 |
| [^81] | [A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts](https://arxiv.org/abs/2402.09727) | ReadAgent是一个具有长期上下文概要记忆的阅读代理系统，通过实现一个简单的提示系统，它能够处理长输入并提高有效上下文长度。在评估中表现良好。 |
| [^82] | [Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization](https://arxiv.org/abs/2402.09725) | 本论文提出使用错误暴露和一致性正则化的训练方法来改进非自回归机器翻译中的数据分布不一致问题，并取得了显著的BLEU得分提升。 |
| [^83] | [Best Arm Identification for Prompt Learning under a Limited Budget](https://arxiv.org/abs/2402.09723) | 这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。 |
| [^84] | [Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields](https://arxiv.org/abs/2402.09722) | Reg-NF是一种高效的神经场隐式表面注册方法，能够优化两个神经场之间的相对6自由度变换，并在机器人应用中显示出良好的效果。 |
| [^85] | [Persuading a Learning Agent](https://arxiv.org/abs/2402.09721) | 在一个重复的贝叶斯说服问题中，即使没有承诺能力，委托人可以通过使用上下文无遗憾学习算法来实现与经典无学习模型中具有承诺的委托人的最优效用无限接近的效果；在代理人使用上下文无交换遗憾学习算法的情况下，委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。 |
| [^86] | [Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement](https://arxiv.org/abs/2402.09712) | 本文介绍了一种新的观点和框架，证明了具有交叉注意力的扩散模型可以作为强大的归纳偏置，促进解缠表示的学习。 |
| [^87] | [Reward Poisoning Attack Against Offline Reinforcement Learning](https://arxiv.org/abs/2402.09695) | 这项研究针对深度神经网络函数逼近的一般离线强化学习中的奖励污染攻击问题，提出了一种名为“策略对比攻击”的攻击策略。通过使低性能策略看起来像是高性能的，同时使高性能策略看起来像是低性能的，我们证明了这种攻击有效性。 |
| [^88] | [Exploring a Behavioral Model of "Positive Friction" in Human-AI Interaction](https://arxiv.org/abs/2402.09683) | 本文探讨了在人机交互中积极摩擦的行为模型，即通过有意的延迟等方式，增加用户反思，阻止自动或有偏见行为，并提高意外发现的机会。 |
| [^89] | [PAL: Proxy-Guided Black-Box Attack on Large Language Models](https://arxiv.org/abs/2402.09674) | PAL是第一个黑盒查询攻击大型语言模型的优化算法，通过代理模型引导优化过程，并使用复杂的损失函数，取得了较高的攻击成功率。 |
| [^90] | [How to Train Data-Efficient LLMs](https://arxiv.org/abs/2402.09668) | 本文研究了如何训练数据高效的LLM模型，提出了Ask-LLM和Density两种优秀的数据选择方法。 |
| [^91] | [CodeMind: A Framework to Challenge Large Language Models for Code Reasoning](https://arxiv.org/abs/2402.09664) | CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。 |
| [^92] | [User Modeling and User Profiling: A Comprehensive Survey](https://arxiv.org/abs/2402.09660) | 这篇综述论文介绍了用户建模与用户画像研究的现状、发展和未来方向。该研究主要关注在人工智能应用中构建准确的用户表示，包括利用大量数据进行建模以及采用深度学习和图数据技术等先进方法。 |
| [^93] | [The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse](https://arxiv.org/abs/2402.09656) | 尽管模型编辑在大型语言模型中显示出修订知识的潜力，但少量编辑可以触发模型崩溃，导致性能显著下降。我们提出使用困惑度作为替代指标，并通过实验证实其与下游任务性能的强相关性。 |
| [^94] | [GPT-4's assessment of its performance in a USMLE-based case study](https://arxiv.org/abs/2402.09654) | 本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。 |
| [^95] | [ProtChatGPT: Towards Understanding Proteins with Large Language Models](https://arxiv.org/abs/2402.09649) | ProtChatGPT是一个基于大型语言模型的系统，通过自然语言学习和理解蛋白质结构，为用户提供上传蛋白质、提问和交互式对话等功能，有助于进一步理解蛋白质的结构与功能关系。 |
| [^96] | [LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations](https://arxiv.org/abs/2402.09617) | 这项研究旨在提高LLM在图数据中的关系挖掘效率和能力，通过整合图神经网络和大型语言模型，以利用边缘信息来理解复杂节点关系，并从图结构中提取有意义洞见。 |
| [^97] | [API Pack: A Massive Multilingual Dataset for API Call Generation](https://arxiv.org/abs/2402.09615) | 这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成 |
| [^98] | [Probabilistic Reasoning in Generative Large Language Models](https://arxiv.org/abs/2402.09614) | 本文针对大型语言模型在概率推理任务中的限制，引入了贝叶斯语言推理数据集（BLInD），并提出了几种解决策略，包括Python代码和概率推理算法。 |
| [^99] | [Towards Privacy-Aware Sign Language Translation at Scale](https://arxiv.org/abs/2402.09611) | 本研究提出了一种两阶段框架，用于实现规模化隐私感知手语翻译。我们利用自监督视频预训练和有监督微调的方法，在数据稀缺和隐私风险的情况下实现了最先进的手语翻译性能。 |
| [^100] | [LogicPrpBank: A Corpus for Logical Implication and Equivalence](https://arxiv.org/abs/2402.09609) | LogicPrpBank是一个新的命题逻辑语料库，用于研究逻辑蕴含和等价的任务。通过与现有的语言模型进行基准测试，展示了该语料库在这一具有挑战性的任务中提供了有用的资源，并且还有改进的空间。 |
| [^101] | [Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation](https://arxiv.org/abs/2402.09604) | 本文提出了一种使用单个未标记测试图像来调整医学图像分割模型的方法。相比于直接最小化预测熵的其他方法，在这种设置下并不能显著提高性能。为了克服这个问题，我们使用各种目标域统计估计的预测进行集成，并基于权重进行加权。 |
| [^102] | [Scalable Graph Self-Supervised Learning](https://arxiv.org/abs/2402.09603) | 该论文提出了一种通过体积最大化项减少图自监督学习预训练损失函数计算成本的方法。实验证明，采用节点或维度采样可以降低损失计算的成本。 |
| [^103] | [A Web-Based Tool for Automatic Data Collection, Curation, and Visualization of Complex Healthcare Survey Studies including Social Network Analysis](https://arxiv.org/abs/2402.09592) | 本研究设计和构建了一个基于Web的平台，用于自动化复杂医疗调查研究中数据收集、整理和可视化的过程，包括社交网络分析。该平台提供了直观的图形用户界面，可以简化问卷创建、数据收集和表示等操作，为用户提供便利和深入了解个人饮酒行为的工具。 |
| [^104] | [Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications](https://arxiv.org/abs/2402.09588) | 本研究探讨了使用大型语言模型在药物分子和适应症之间进行翻译的机遇，提出了一个新任务，并测试了其有效性，这对于药物发现过程具有重要意义。 |
| [^105] | [Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems](https://arxiv.org/abs/2402.09584) | 本文研究了机器学习控制在建筑能源系统中的可解释性，通过将Shapley值和大型语言模型相结合，提高了机器学习控制模型的透明性和理解性。 |
| [^106] | [Combatting deepfakes: Policies to address national security threats and rights violations](https://arxiv.org/abs/2402.09581) | 本文提供了应对深度伪造威胁的政策建议，包括背景信息、危害、先前的立法提案以及全面的深度伪造供应链政策建议。 |
| [^107] | [Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies](https://arxiv.org/abs/2402.09579) | 本文研究了将大型语言模型ChatGPT与EnergyPlus建筑能源建模软件融合的创新方法，并强调了大型语言模型在解决建筑能源建模挑战方面的潜力和多种应用。 |
| [^108] | [Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph](https://arxiv.org/abs/2402.09565) | 本文探讨了如何从海量的Web图数据中对背景节点进行压缩，并将重点放在分析目标节点上，以解决图数据存储和计算能力的挑战。 |
| [^109] | [Bidirectional Generative Pre-training for Improving Time Series Representation Learning](https://arxiv.org/abs/2402.09558) | 这项论文提出了一种名为BiTimelyGPT的模型，通过双向的预训练任务在时间序列数据上学习表示，展示了优越的性能，可用于神经功能预测、疾病诊断和生理病征识别。 |
| [^110] | [Statistical and Machine Learning Models for Predicting Fire and Other Emergency Events](https://arxiv.org/abs/2402.09553) | 本文系统地开发了一种用于预测加拿大埃德蒙顿市不同类型紧急事件的预测模型，并分析了事件类型与邻域层面的社会经济和人口统计数据的关联性。 |
| [^111] | [How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?](https://arxiv.org/abs/2402.09546) | 本文首次研究了基于大型语言模型的导航系统在城市环境中的安全漏洞，并提出了一种新颖的NPS Attack方法，该方法通过添加后缀来操纵导航模型，导致不正确的行为。该研究对自动驾驶、物流和紧急服务等领域具有重要意义。 |
| [^112] | [Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?](https://arxiv.org/abs/2402.09540) | 本论文研究了为什么具有较大ε的差分隐私可以防御实际成员推理攻击，因为实际攻击者可能缺乏准确的私有数据知识，并且在实际应用中，数据集可能相对容易被防御。 |
| [^113] | [Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls](https://arxiv.org/abs/2402.09508) | 通过引入参数高效微调（PEFT）方法，本研究实现了自回归语言模型在音乐修复和音乐排列任务中的应用。在多个音乐编辑任务中，该方法展示了有希望的结果，并为未来的AI驱动音乐编辑工具提供了更灵活的控制。 |
| [^114] | [On Formally Undecidable Traits of Intelligent Machines](https://arxiv.org/abs/2402.09500) | 我们研究了智能机器形式上不可判定特征的条件, 发展了一种数学上独立于形式语言理论的形式化方法, 发现Rice定理不能用于判断机器是否具有给定特征。 |
| [^115] | [Detection of the most influential variables for preventing postpartum urinary incontinence using machine learning techniques](https://arxiv.org/abs/2402.09498) | 本研究使用机器学习技术评估了产后尿失禁中最具影响力的变量，并发现外在变量是PUI问题的重要预测因素。 |
| [^116] | [Instruction Tuning for Secure Code Generation](https://arxiv.org/abs/2402.09497) | 现代语言模型在编程中得到广泛应用，指令调优是一个增强其实用性的关键过程。然而，现有的方案忽视了生成代码的安全性。本文提出了SafeCoder，通过安全微调和标准指令调优相结合，来优化安全性和实用性。 |
| [^117] | [On the Potential of Network-Based Features for Fraud Detection](https://arxiv.org/abs/2402.09495) | 本文研究了基于网络特征在欺诈检测中的潜力，通过使用个性化的PageRank算法来捕捉欺诈的社会动态。实验结果表明，集成PPR可以提高模型的预测能力并提供独特有价值的信息。 |
| [^118] | [Can AI and humans genuinely communicate?](https://arxiv.org/abs/2402.09494) | 本研究探讨了AI和人类是否能够真正交流的问题，并提出了一种称为“心理行为方法”的回答方式，该方法通过测试AI是否展现出人类类似的行为来判断其是否能够与人类真正交流。 |
| [^119] | [AI-Enabled Lung Cancer Prognosis](https://arxiv.org/abs/2402.09476) | 这项研究旨在探索人工智能在肺癌预后中的应用。通过利用机器学习和深度学习算法，研究人员可以提高肺癌生存预测的准确性，并为临床医生提供全面的信息，以便为患者制定更好的治疗策略。 |
| [^120] | [Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals](https://arxiv.org/abs/2402.09474) | 本研究使用视觉变压器方法解读心率信号，提高心脏疾病检测模型的解释性和可靠性。 |
| [^121] | [RLEEGNet: Integrating Brain-Computer Interfaces with Adaptive AI for Intuitive Responsiveness and High-Accuracy Motor Imagery Classification](https://arxiv.org/abs/2402.09465) | 本文提出了RLEEGNet，这是一种将脑机接口与自适应人工智能结合的框架，能够实现直观响应和高准确率的动作意象分类。通过使用强化学习和深度Q网络进行分类任务，并结合常空模式预处理技术，该框架能够实时适应用户不断变化的需求和意图。 |
| [^122] | [Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body Parts Orientation Tracking and 3D Movement Visualization on an Avatar](https://arxiv.org/abs/2402.09459) | 这项研究的目标是通过构建可负担的定制IMU无线可穿戴系统，在人体运动分析中实现对身体部位定向跟踪和3D运动可视化。 |
| [^123] | [Optimistic Thompson Sampling for No-Regret Learning in Unknown Games](https://arxiv.org/abs/2402.09456) | 该论文提出了一种在未知博弈中进行无遗憾学习的乐观的汤普森抽样方法，通过利用对手的行动和奖励结构信息，显著减少了实验预算，成功地缓解了多机构问题。此外，研究还引入了乐观-无遗憾框架，将现有算法与提出的方法相结合。 |
| [^124] | [Improving EEG Signal Classification Accuracy Using Wasserstein Generative Adversarial Networks](https://arxiv.org/abs/2402.09453) | 该论文提出了一种通过使用Wasserstein生成对抗网络(WGAN)来提高EEG信号分类准确性的实际解决方案。 WGAN在BCI2000数据集上进行训练，并通过改进的平均准确率和测量得分证明了生成的EEG信号的质量。 |
| [^125] | [Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram](https://arxiv.org/abs/2402.09450) | 本研究提出了一种叫做ST-MEM的模型，通过重构遮蔽的心电图数据来学习时空特征，该模型在心律失常分类任务中优于其他自监督学习方法。 |
| [^126] | [A Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systems](https://arxiv.org/abs/2402.09448) | 比较传统EEG与三极EEG在高性能到颤抓握BCI系统中的有效性，包括信噪比、空间分辨率、ERPs和小波时频分析。 |
| [^127] | [Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and Natural Grasp Types](https://arxiv.org/abs/2402.09447) | 该研究使用小波分析技术对非侵入性脑电图信号进行解码，成功区分复杂和自然的抓握类型，并且证明了小波特征在基于脑电图的抓握区分中的有效性。 |
| [^128] | [iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition](https://arxiv.org/abs/2402.09445) | 通过传感器融合和对比学习，研究证明生物阻抗传感技术可以改进基于IMU的健身追踪，提高分类模型的精度。 |
| [^129] | [Multimodal Action Quality Assessment](https://arxiv.org/abs/2402.09444) | 该论文提出了一个名为PAMFN的渐进自适应多模态融合网络，用于多模态动作质量评估。该模型利用RGB、光流和音频信息，分别建模模态特定信息和混合模态信息，并通过充分利用音频信息，提高了评分回归的准确性。 |
| [^130] | [Review of algorithms for predicting fatigue using EEG](https://arxiv.org/abs/2402.09443) | 该研究综述了使用 EEG 信号进行疲劳预测的机器学习算法，并评估了不同算法在基于 EEG 数据预测个体疲劳水平方面的效果。 |
| [^131] | [Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning](https://arxiv.org/abs/2402.09442) | 本文介绍了基于自驱动传感器和深度学习的人工智能应用的最新进展，重点讨论了使用TENG作为自驱动传感器的优势，包括简单结构和高瞬时性能。 |
| [^132] | [Electrical Behavior Association Mining for Household ShortTerm Energy Consumption Forecasting](https://arxiv.org/abs/2402.09433) | 本文提出了一种基于电气行为关联挖掘的家庭短期能耗预测方法，通过概率关联模型和卷积神经网络门控循环单元的结合，实现了显著的准确性提升。 |
| [^133] | [DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications](https://arxiv.org/abs/2402.09427) | DoorINet是一种用于门贴式物联网应用的深度学习惯性框架，无需使用磁力计即可计算航向角度。 |
| [^134] | [Mathematical Explanations](https://arxiv.org/abs/2402.09413) | 该论文讨论了数学陈述的解释问题以及通过使用不可能的可能世界解决了数学解释中的困境。 |
| [^135] | [Mitigating Reward Hacking via Information-Theoretic Reward Modeling](https://arxiv.org/abs/2402.09345) | 本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。 |
| [^136] | [Neural Networks asymptotic behaviours suitable for the resolution of inverse problems](https://arxiv.org/abs/2402.09338) | 本文研究了适用于解决反褶积逆问题的神经网络渐近行为，并发现使用从神经网络的渐近极限导出的高斯过程比全连接的神经网络获得更好的结果，而且观察到随着层数的增加，训练后的神经网络的准确性接近于高斯过程的准确性。其中一个高斯过程的解释与传统方法不同，提供了一种新的视角。 |
| [^137] | [Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents](https://arxiv.org/abs/2402.09205) | 该论文提出了一种面向基于语言模型的智能代理的隐式用户意图理解的方法。通过引入Intention-in-Interaction (IN3) 基准和在代理设计中融入模型专家，使得代理能够更好地与用户进行交互，并提升对用户指令的理解能力。 |
| [^138] | [(Ir)rationality and Cognitive Biases in Large Language Models](https://arxiv.org/abs/2402.09193) | 本研究评估了七个大型语言模型在认知心理学任务中的表现，发现它们与人类一样存在非理性，但展示的非理性方式与人类偏见不同，同时还表现出了显著的回答不一致性。 |
| [^139] | [Exploring the Adversarial Capabilities of Large Language Models](https://arxiv.org/abs/2402.09132) | 本研究探索了大型语言模型的对抗能力，并发现其能够成功地制造对抗性示例以愚弄安全措施，特别是在仇恨言论检测方面具有重大影响。 |
| [^140] | [FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning](https://arxiv.org/abs/2402.09051) | 本文介绍了FGeo-DRL，一个用于自动执行几何演绎推理的神经符号系统。通过强化学习算法和机器学习模型，该系统能够自主学习解决几何问题的方法，实现了人类化的演绎推理。 |
| [^141] | [Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications](https://arxiv.org/abs/2402.09015) | 本研究引入了AgentEval框架，用于评估LLM驱动应用的任务效用。该框架通过自动提出一套针对特定应用的评估标准，简化了效用验证过程，并对应用的效用进行了全面量化分析。 |
| [^142] | [DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models](https://arxiv.org/abs/2402.08777) | DNABERT-S是一种专门用于创建物种感知的DNA嵌入的基因组基础模型。为了提高对长读DNA序列的嵌入效果，引入了Manifold Instance Mixup (MI-Mix)对比目标方法来训练模型。 |
| [^143] | [Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach](https://arxiv.org/abs/2402.08341) | 本研究使用分类器驱动的方法，通过不同的输入提示探究大型语言模型的输出变化，以增加其透明度。结果显示，这些模型根据输入的不同提示而表现出不同的人格特质，类似于人类对刺激做出的反应。 |
| [^144] | [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876) | 本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。 |
| [^145] | [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/abs/2402.07456) | OS-Copilot是一个通用计算机代理的框架，能够与操作系统中的各种元素进行交互，包括网络、代码终端、文件、多媒体和第三方应用程序。使用OS-Copilot构建的自我提升的FRIDAY代理在各种计算机任务上表现出强大的泛化能力，并在通用人工智能助手基准测试中超过以前的方法35%。 |
| [^146] | [The Reasons that Agents Act: Intention and Instrumental Goals](https://arxiv.org/abs/2402.07221) | 本论文提出了一个对代理行为意图的操作化定义，并通过一些例子和结果展示了其灵活性和适用性。这一定义有助于理解和解释机器学习系统的行为，以及相关概念如工具性目标的关系。 |
| [^147] | [MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization](https://arxiv.org/abs/2402.07180) | 本文提出了一种名为MAGNETO的边缘AI平台，通过从云端推向边缘进行增量人体活动学习，避免了云端与边缘设备之间的数据传输，实现了数据隐私保护、低延迟处理和高度个性化。 |
| [^148] | [Natural Language Reinforcement Learning](https://arxiv.org/abs/2402.07157) | 本研究将自然语言表示和强化学习原则相结合，提出了自然语言强化学习（NLRL）框架，解决了强化学习在样本效率低、解释性不足和缺乏监督信号等方面的限制问题，通过实验验证了其有效性和可解释性。 |
| [^149] | [LLM Agents can Autonomously Hack Websites](https://arxiv.org/abs/2402.06664) | 这项研究展示了LLM代理可以自主进行网站黑客攻击，包括盲目数据库模式提取和SQL注入，而且不需要人工反馈。这种能力是由高度工具使用和利用扩展上下文能力的前沿模型赋予的。 |
| [^150] | [ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation](https://arxiv.org/abs/2402.05902) | 本研究提出了ClickSAM，该方法使用点击提示对超声图像进行Segment Anything Model的精细调整，解决了超声图像分割中噪声干扰的问题。 |
| [^151] | [NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning](https://arxiv.org/abs/2402.05515) | NoisyICL通过在模型参数中引入噪音，提高了上下文学习的性能和校准性，实验结果显示NoisyICL可以产生更准确、更公平的预测。 |
| [^152] | [PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition](https://arxiv.org/abs/2402.04838) | 本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。 |
| [^153] | [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://arxiv.org/abs/2402.04520) | 通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。 |
| [^154] | [Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models](https://arxiv.org/abs/2402.03877) | 本文调查了大型语言模型（LLMs）在几何推理方面的能力，并发现了它们在目标变量选择和2D空间关系方面存在偏见和困难。通过引入基于LLMs的多代理体系结构，本研究提出了一种通过自我纠正、协作和不同角色专业化来提高LLMs几何推理能力的框架。 |
| [^155] | [SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering](https://arxiv.org/abs/2401.17809) | 提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。 |
| [^156] | [Synthetic images aid the recognition of human-made art forgeries](https://arxiv.org/abs/2312.14998) | 通过在训练数据集中加入合成艺术作品，可以提高人工制作艺术赝品的检测性能。 |
| [^157] | [Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method](https://arxiv.org/abs/2312.14188) | 本论文提出了一种名为DS-Prover的动态抽样方法，用于增强神经定理证明的能力。该方法通过动态确定应用于扩展当前目标的策略数量，并调整探索和开发之间的平衡，从而使证明搜索过程更加高效。此外，作者还通过增加训练数据集，将简化和重写策略与多个前提进行分解。 |
| [^158] | [Protect Your Score: Contact Tracing With Differential Privacy Guarantees](https://arxiv.org/abs/2312.11581) | 这篇论文提出了具有差分隐私保障的接触追踪算法，以解决隐私问题限制接触追踪的部署。该算法在多种情景下展现了卓越性能，并通过在发布每个风险分数时保护个体健康状况的隐私。 |
| [^159] | [Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks](https://arxiv.org/abs/2312.11560) | 本文研究了积极抑制人工神经网络中的单意义神经元，这对于提高性能具有重要意义，并提出了一种基于自发现的方法来实现抑制。 |
| [^160] | [GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations](https://arxiv.org/abs/2312.10913) | GINN-LP是一种可解释的神经网络，用于发现多元Laurent多项式方程的形式和系数。它采用了一种名为“幂项逼近块”的新型可解释性神经网络块，并通过神经网络增长策略和稀疏正则化来优化方程的表示。 |
| [^161] | [Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning](https://arxiv.org/abs/2312.08901) | CoT-Influx是一种通过强化上下文修剪来提升LLM数学推理能力的方法，通过最大化有效和简明的示例输入，显著优于其他提示方法。 |
| [^162] | [Personalized Path Recourse for Reinforcement Learning Agents](https://arxiv.org/abs/2312.08724) | 该论文介绍了一种针对增强学习代理的个性化路径补救方法，该方法通过编辑动作路径来实现期望目标，同时保持与代理的原始路径相似度高，并且个性化适应代理的行为模式。这种方法适用于纠正或改进动作或数据序列以实现预定目标。 |
| [^163] | [InstructBooth: Instruction-following Personalized Text-to-Image Generation](https://arxiv.org/abs/2312.03011) | InstructBooth是一种用于增强个性化文本到图像模型中的图像文本对齐的方法，通过使用有限数量的特定对象图片进行个性化，并利用增强学习进行微调，实现了优秀的图像文本对齐能力。 |
| [^164] | [Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting](https://arxiv.org/abs/2312.00817) | 提出了一种名为TimelyGPT的可推广的Transformer预训练模型，该模型通过可推广的位置嵌入和循环注意力以及时间卷积模块有效地捕捉超长时间序列数据中的全局和局部时间依赖关系。 |
| [^165] | [ASI: Accuracy-Stability Index for Evaluating Deep Learning Models](https://arxiv.org/abs/2311.15332) | 该论文引入了准确性-稳定性指数（ASI），它是一种综合考虑准确度和稳定性的定量评估深度学习模型的指标。实验结果展示了ASI的应用，提供了一个用于可视化ASI、平均准确度和变异系数的3D曲面模型。这项研究解决了深度学习模型定量基准评估指标的重要问题，并提供了一种准确评估深度学习模型准确性和稳定性的新方法。 |
| [^166] | [Analyzing the Evolution and Maintenance of ML Models on Hugging Face](https://arxiv.org/abs/2311.13380) | 本文通过仓库挖掘和文本分析的方式，对Hugging Face上的机器学习模型的演化和维护进行了研究。研究发现了Hugging Face的整体增长和受欢迎程度，揭示了ML领域、框架使用、作者分组等方面的趋势，同时也探讨了开发者社区中普遍存在的主题和见解以及模型的维护状态和演化情况。 |
| [^167] | [Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries](https://arxiv.org/abs/2311.12573) | 本论文研究了模型市场的调节问题，分析了AI中介平台面临的平台治理挑战，并总结了业界的相关实践，包括许可、访问和使用限制、自动内容调节以及公开政策制定。 |
| [^168] | [Raising the ClaSS of Streaming Time Series Segmentation](https://arxiv.org/abs/2310.20431) | ClaSS是一种新颖、高效且高精度的流式时间序列分割算法，通过自监督时间序列分类评估同质性，并应用统计测试检测显著的变化点。 |
| [^169] | [Enhancing the Hierarchical Environment Design via Generative Trajectory Modeling](https://arxiv.org/abs/2310.00301) | 本文通过引入层次MDP框架，提出了一种在资源约束下增强环境设计的方法，通过上层教师智能体生成适当的训练环境，以促进学生智能体的学习能力发展。 |
| [^170] | [Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2307.00493) | 本文提出了一种名为FWin的快速本地全局窗口注意力方法，用于加速长序列时间序列预测的Informer方法。通过实验证明，该方法可以提高预测准确性并加速推断速度，同时在非线性回归模型中表现出与Softmax全注意力相媲美甚至更优的效果。 |
| [^171] | [Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles](https://arxiv.org/abs/2303.03751) | 零阶优化算法ZO-RankSGD解决了一个新兴的优化挑战，即只能通过排名预测来评估黑盒目标函数。该算法利用一种新颖的随机估计器来确定下降方向，并保证收敛到一个稳定点。此外，该算法还可用于增强学习中的策略优化问题，特别是当只有对于回报排名的排名预测时。 |
| [^172] | [cGAN-Based High Dimensional IMU Sensor Data Generation for Enhanced Human Activity Recognition in Therapeutic Activities](https://arxiv.org/abs/2302.07998) | 本论文开发了一种基于cGAN的TheraGAN网络，用于生成与康复活动相关的高维IMU传感器数据。通过引入简单活动，简化了生成过程。该方法能够帮助解决传统活动识别分类器中训练数据不足的问题。 |
| [^173] | [Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition](https://arxiv.org/abs/2302.04944) | 通过使用专家提供的任务分解为更简单的多智能体子任务，并将其转移到目标任务中进行集体调整，我们的方法可以有效地学习复杂的多智能体任务，并在解决复杂目标任务所需的时间步数上实现了显著的减少。 |
| [^174] | [On the Convergence of Modified Policy Iteration in Risk Sensitive Exponential Cost Markov Decision Processes](https://arxiv.org/abs/2302.03811) | 这项研究证明了在有限状态和动作空间的情况下，修改的策略迭代算法（MPI）在风险敏感问题中的收敛性，并提供了与已有结果不同的证明方法。 |
| [^175] | [FedMT: Federated Learning with Mixed-type Labels](https://arxiv.org/abs/2210.02042) | 本文提出了一种概念新颖的联邦学习设置，即具有混合类型标签的联邦学习，在其中不同的中心可以使用不同的标签准则。为了有效地训练具有混合类型标签的模型，作者提出了一种理论指导和模型无关的方法。 |
| [^176] | [PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and Feature-metric Alignment](https://arxiv.org/abs/2209.03910) | PixTrack是一种基于视觉的物体姿态跟踪框架，使用NeRF模板和特征度量对齐方法，能够精确跟踪物体的6DoF姿态，而且无需数据注释或轨迹平滑。方法具有高度精确、鲁棒且无抖动的特点，同时计算效率高，可用于多目标跟踪。 |
| [^177] | [ED2: Environment Dynamics Decomposition World Models for Continuous Control](https://arxiv.org/abs/2112.02817) | 提出了一种环境动力学分解世界模型构建框架ED2，能够通过发现子动力学并进行分解预测，更准确地构建世界模型。 |
| [^178] | [Decision Theoretic Foundations for Experiments Evaluating Human Decisions.](http://arxiv.org/abs/2401.15106) | 该论文通过综合统计决策理论和信息经济学，提出了决策问题的广泛适用定义。为了将人类决策的下降归咎于偏见形式，实验必须向参与者提供足够的信息来识别规范决策。然而，根据作者对AI辅助决策的研究的评估，只有17%的研究提供了足够的信息来描述参与者的行为偏离了良好的决策。 |
| [^179] | [Are self-explanations from Large Language Models faithful?.](http://arxiv.org/abs/2401.07927) | 大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。 |
| [^180] | [Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach.](http://arxiv.org/abs/2401.02987) | 本研究提出一种基于多头后验的方法，通过利用实体的元特征和模型的表示之间的一致性作为度量标准，有效评估预训练模型在各个领域的表现。 |
| [^181] | [Zero-Shot Position Debiasing for Large Language Models.](http://arxiv.org/abs/2401.01218) | 本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。 |
| [^182] | [Speak Like a Native: Prompting Large Language Models in a Native Style.](http://arxiv.org/abs/2311.13538) | 本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。 |
| [^183] | [Real-time Animation Generation and Control on Rigged Models via Large Language Models.](http://arxiv.org/abs/2310.17838) | 该论文介绍了一种利用大型语言模型实现在已绑定模型上实时进行动画控制和生成的方法，并展示了该方法的灵活性和鲁棒性。 |
| [^184] | [Absolute Policy Optimization.](http://arxiv.org/abs/2310.13230) | 这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。 |
| [^185] | [ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection.](http://arxiv.org/abs/2310.09298) | ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。 |
| [^186] | [Domain Generalization for Medical Image Analysis: A Survey.](http://arxiv.org/abs/2310.08598) | 本综述详细回顾了针对医学图像分析的领域泛化研究，探讨了在DL模型在真实世界应用中遇到的挑战，以及如何解决分布漂移问题和实现稳健性。同时，考虑了领域泛化技术对整个MedIA工作流程的操作影响。 |
| [^187] | [Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks.](http://arxiv.org/abs/2309.16391) | 本文介绍了一种通过Sobolev训练的2-Cats网络，它能够非参数地逼近任何二维Copula，并且在估计输出方面优于现有技术。 |
| [^188] | [Revisiting LARS for Large Batch Training Generalization of Neural Networks.](http://arxiv.org/abs/2309.14053) | 本文通过对大批量训练技术的研究，提出了一种新的算法TVLARS，该算法利用可配置的函数替代了热身阶段，以实现对于神经网络的稳健训练。实验证明，在大多数情况下，TVLARS比LARS和LAMB都有更好的性能表现，特别是在自监督学习方面。 |
| [^189] | [Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators.](http://arxiv.org/abs/2308.13498) | 本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。 |
| [^190] | [Embedding Democratic Values into Social Media AIs via Societal Objective Functions.](http://arxiv.org/abs/2307.13912) | 本研究介绍了一种方法，通过将社会科学构造转化为人工智能目标函数，将民主价值观嵌入社交媒体人工智能系统中。通过一个应用于反民主态度的模型示例，我们展示了该方法的有效性。通过利用社会科学的调查工具和定性编码手册，我们能够精确地转化这些构造为大型语言模型的提示。 |
| [^191] | [Nature of Intelligence.](http://arxiv.org/abs/2307.11114) | 智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。 |
| [^192] | [ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection.](http://arxiv.org/abs/2307.02591) | 这个研究介绍了一份名为ODD的新型基准数据集，用于通过分析患者的电子健康记录笔记，检测和分类药物滥用异常行为。这个数据集在药物相关病例的自然语言处理研究中具有重要的创新和贡献。 |
| [^193] | [Identifiability of direct effects from summary causal graphs.](http://arxiv.org/abs/2306.16958) | 该论文研究了在缺乏完整时间因果图的情况下，直接因果效应如何从总结因果图中进行可辨识，并提出了一个完整的可辨识性结果。 |
| [^194] | [When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm.](http://arxiv.org/abs/2306.02552) | 从事用户行为分析的学术界一直面临着收集足够高质量用户行为数据的难题，一种解决方案是自动模拟用户行为，近期研究表明，利用大语言模型进行可靠的用户模拟有了重要的突破，将这种模型应用到用户行为分析研究中有着巨大潜力，可能对传统研究范式产生革命性影响。 |
| [^195] | [LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations.](http://arxiv.org/abs/2305.18354) | 本文通过分析GPT模型在抽象推理语料库上的表现，发现GPT在抽象推理任务中存在需要核心概念“核心知识”的限制。通过使用基于对象的表示方法和新的1D-ARC基准，GPT在抽象推理任务中取得了较好的表现。 |
| [^196] | [SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly.](http://arxiv.org/abs/2305.12520) | SLaDe是一种基于小型语言模型的反编译器，通过训练真实代码的序列到序列变换器，使用了新颖的分词器、无丢弃训练和类型推断等方法，能够生成更易读和更准确的程序。 |
| [^197] | [Equilibrium and Learning in Fixed-Price Data Markets with Externality.](http://arxiv.org/abs/2302.08012) | 这篇论文研究了固定价格数据市场中的买家之间的竞争策略，发现其中存在负面影响。文章揭示了买家之间的负外部性及其影响，并提出了相应的市场干预措施，实现了纯策略均衡，并保证了强福利性质。 |

# 详细

[^1]: 自我对抗微调扩散模型用于文本到图像生成

    Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation

    [https://arxiv.org/abs/2402.10210](https://arxiv.org/abs/2402.10210)

    本文介绍了一种创新的技术，称为自我对抗微调扩散模型（SPIN-Diffusion），通过扩散模型与其先前版本的竞争，实现了逐步自我改进过程。

    

    微调扩散模型在生成人工智能领域仍然是一个未被充分探索的前沿，尤其是与在大型语言模型（LLMs）微调方面取得的显著进展相比。尽管现在的先进扩散模型如稳定扩散（SD）和SDXL依赖于监督微调，但它们的性能在观察到一定数量的数据后必然会达到瓶颈。最近，强化学习（RL）被应用于通过人类偏好数据对扩散模型进行微调，但每个文本提示需要至少两个图像（“获胜者”和“失败者”图像）。本文介绍了一种创新的技术，称为自我对抗微调扩散模型（SPIN-Diffusion），其中扩散模型与其先前版本进行竞争，促进了一个迭代的自我改进过程。我们的方法提供了一种替代传统监督微调和RL策略的选择。

    arXiv:2402.10210v1 Announce Type: cross  Abstract: Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, signific
    
[^2]: 基于上下文的奖励：基于动态偏好调整的多目标基础模型对齐

    Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

    [https://arxiv.org/abs/2402.10207](https://arxiv.org/abs/2402.10207)

    本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。

    

    我们考虑了基于人类偏好的基础模型多目标对齐问题，这是实现有益和无害的人工智能系统的关键步骤。然而，使用强化学习（RL）对大型基础模型进行微调通常是昂贵且不稳定的，并且人类偏好的多维度、异质性和冲突性进一步复杂化了对齐过程。在本文中，我们引入了Rewards-in-Context（RiC）方法，它使得基础模型的响应取决于其提示上下文中的多个奖励，并应用有监督的微调来进行对齐。RiC的显著特点是简单性和适应性，因为它只需要对单个基础模型进行有监督的微调，并支持在推理时动态调整用户偏好。受到抽象的凸优化问题的解析解的启发，我们提出了一种动态推理时调整方法。

    arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
    
[^3]: 异构图上基于伊辛模型的特定任务图子抽样

    Ising on the Graph: Task-specific Graph Subsampling via the Ising Model

    [https://arxiv.org/abs/2402.10206](https://arxiv.org/abs/2402.10206)

    该论文提出了一种基于伊辛模型的图子抽样方法，可以针对特定任务在图结构上进行减小，并通过学习伊辛模型的外部磁场来实现。该方法的多功能性在图像分割、三维形状稀疏化和稀疏逼近矩阵求逆等应用中得到展示。

    

    减少图的大小同时保持其整体结构是一个具有许多应用的重要问题。通常，减小图的方法要么删除边缘（稀疏化），要么合并节点（粗化），而没有特定的下游任务。在本文中，我们提出了一种使用在节点或边上定义的伊辛模型对图结构进行子抽样的方法，并使用图神经网络学习伊辛模型的外部磁场。我们的方法是任务特定的，因为它可以端到端地学习如何为特定的下游任务减小图的大小。所使用的任务损失函数甚至不需要可微分性。我们在三个不同的应用上展示了我们方法的多功能性：图像分割、三维形状稀疏化和稀疏逼近矩阵求逆。

    arXiv:2402.10206v1 Announce Type: cross  Abstract: Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.
    
[^4]: 使用有条件降噪扩散模型进行射电天文图像重建

    Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model

    [https://arxiv.org/abs/2402.10204](https://arxiv.org/abs/2402.10204)

    该研究提出使用有条件降噪扩散模型从不干净的射电图像中重建天空模型，以实现准确定位和测量流量，为射电源的表征提供潜在改进。

    

    从不干净的射电图像中重建天空模型，以便准确定位和测量流量对于研究高红移下的星系演化至关重要，尤其是在使用Atacama Large Millimetre Array (ALMA)等仪器进行深度观测时。随着Square Kilometre Array (SKA)等新项目的启动，对更好的源提取方法的需求日益增长。目前的技术，如CLEAN和PyBDSF，往往无法检测到微弱的源，凸显了对更准确方法的需求。本研究提议使用随机神经网络直接从不干净的图像中重建天空模型。该方法可以精确定位射电源并测量伴随的不确定性，标志着射电源表征方面的潜在改进。我们在使用基于ALMA第5.3周期天线设置的CASA工具simalma模拟的10164个图像上测试了这种方法。

    arXiv:2402.10204v1 Announce Type: cross  Abstract: Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion Probabilistic Models (D
    
[^5]: 一座摇摇欲坠的纸牌屋？对语言代理的敌对攻击的映射

    A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents

    [https://arxiv.org/abs/2402.10196](https://arxiv.org/abs/2402.10196)

    本文是第一个对语言代理的敌对攻击进行映射的系统化努力。它提供了一个统一的概念框架来研究这些攻击。这有助于我们理解语言代理的安全风险。

    

    由大型语言模型（LLMs）驱动的语言代理在发展中迅猛。它们利用语言作为思想和交流的工具，赋予了无比的灵活性和多样性。人们迅速利用这种能力将LLMs连接到各种外部组件和环境中：数据库，工具，因特网，机器人实体等。许多人相信一种前所未有的强大自动化技术正在崛起。然而，新的自动化技术也带来了新的安全风险，特别是对于复杂的系统如语言代理。他们的开发和部署的速度和规模与我们对其安全风险的理解之间存在着令人惊讶的差距。我们是否正在建造一座纸牌屋？在本论文中，我们首次系统地对语言代理的敌对攻击进行了映射。我们首先提出了一个统一的概念框架

    arXiv:2402.10196v1 Announce Type: cross  Abstract: Language agents powered by large language models (LLMs) have seen exploding development. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capability to connect LLMs to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful automation technology is emerging. However, new automation technologies come with new safety risks, especially for intricate systems like language agents. There is a surprisingly large gap between the speed and scale of their development and deployment and our understanding of their safety risks. Are we building a house of cards? In this position paper, we present the first systematic effort in mapping adversarial attacks against language agents. We first present a unified conceptual framework for
    
[^6]: 借鉴多体物理的归纳偏置的多激发投影模拟

    Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias

    [https://arxiv.org/abs/2402.10192](https://arxiv.org/abs/2402.10192)

    该论文引入了多激发投影模拟（mePS），通过在超图上多个粒子的随机游走，解决了投影模拟（PS）无法模拟同时结合多个概念的思维的问题。

    

    随着深度学习的进步，依赖于机器学习的应用正在越来越多地融入日常生活。然而，大多数深度学习模型具有不透明的、类似于神谕般的特性，使得解释和理解它们的决策变得困难。这个问题导致了被称为可解释人工智能（XAI）的领域的发展。该领域中的一种方法称为投影模拟（PS），将思维过程建模为一个在具有概念附加的顶点的图上的粒子的随机游走。虽然这种描述具有各种好处，包括量化的可能性，但不能自然地用来模拟同时结合多个概念的思维。为了克服这个限制，我们引入了一种称为多激发投影模拟（mePS）的推广，它将思维过程视为超图上多个粒子的随机游走。

    arXiv:2402.10192v1 Announce Type: cross  Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for
    
[^7]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^8]: 大规模受限制聚类与强化学习

    Large Scale Constrained Clustering With Reinforcement Learning

    [https://arxiv.org/abs/2402.10177](https://arxiv.org/abs/2402.10177)

    本文介绍了一种使用强化学习解决大规模受限制聚类问题的方法，该方法训练一个代理器生成既可行又接近最优解的解决方案，以提高资源分配和使用的效率。

    

    给定一个网络，将资源分配在聚类级别而不是在每个节点上，可以增强资源分配和使用的效率。本文研究了在最小化聚类内部距离和最大化分配给聚类的节点数量的同时，确保聚类内部没有两个节点的距离超过阈值的全连接不相交聚类问题。尽管可以使用二进制线性模型轻松地形成问题，但在处理大规模实例时，传统组合优化求解器很难应对。我们提出了一种通过强化学习解决这个受限聚类问题的方法。我们的方法涉及训练一个代理器生成既可行又接近最优解的解决方案。代理器学习特定于该任务所遇到的实例的问题启发式算法。在结果部分，我们展示了我们的算法即使在大规模情况下也能找到接近最优解的解决方案。

    arXiv:2402.10177v1 Announce Type: cross  Abstract: Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage. In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance. While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances. We propose an approach to solve this constrained clustering problem via reinforcement learning. Our method involves training an agent to generate both feasible and (near) optimal solutions. The agent learns problem-specific heuristics, tailored to the instances encountered in this task. In the results section, we show that our algorithm finds near optimal solutions, even for l
    
[^9]: OpenMathInstruct-1: 一个拥有180万个数学教学调优数据集

    OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset

    [https://arxiv.org/abs/2402.10176](https://arxiv.org/abs/2402.10176)

    OpenMathInstruct-1是一个包含180万个数学问题和解决方法对的数据集，通过合成开源LLM的代码解释器解决方案来构建，填补了目前开源LLM在数学技能方面与闭源LLM之间的差距。

    

    最近的研究表明，利用合成生成的数据集来训练大规模语言模型（LLM）具有巨大潜力，尤其是为了获得特定的技能。目前的大规模数学教学调优数据集，如MetaMathQA和MAmmoTH，是使用来自商业限制许可的闭源LLM的输出构建的。限制在这些数据生成流程中使用开源LLM的一个关键原因是目前最好的闭源LLM（如GPT-4）和最好的开源LLM之间在数学技能上存在很大差距。基于开源LLM的最近进展，我们提出了新颖的提示方式和一些强力缩放，构建了OpenMathInstruct-1，一个拥有180万个问题-解决方法对的数学教学调优数据集。该数据集是通过使用GSM8K和MATH这两个流行的数学推理基准的代码解释器解决方案进行合成构建的。

    arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
    
[^10]: OptiMUS: 可扩展的最优化建模与(MI)LP求解器和大型语言模型

    OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models

    [https://arxiv.org/abs/2402.10172](https://arxiv.org/abs/2402.10172)

    OptiMUS是一个基于大型语言模型的代理，通过处理自然语言描述来制定和解决(混合整数)线性规划问题，其在易于数据集上表现优于现有的最先进方法。

    

    优化问题在制造业、分销业和医疗保健等领域普遍存在。然而，大多数这类问题仍然是通过手工启发式方法解决的，而不是通过最先进的求解器进行最优解，因为需要专业知识来制定和解决这些问题，限制了优化工具和技术的广泛应用。本文介绍了OptiMUS，一个基于大型语言模型(LLM)的代理，旨在根据自然语言描述来制定和解决(混合整数)线性规划问题。OptiMUS可以开发数学模型，编写和调试求解器代码，评估生成的解决方案，并根据这些评估改进其模型和代码。OptiMUS使用模块化结构处理问题，使其能够处理具有长说明和复杂数据的问题而无需长提示。实验表明，OptiMUS在简单的数据集上优于现有的最先进方法。

    arXiv:2402.10172v1 Announce Type: new  Abstract: Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy dat
    
[^11]: 将语言模型扩展到128K上下文的数据工程

    Data Engineering for Scaling Language Models to 128K Context

    [https://arxiv.org/abs/2402.10171](https://arxiv.org/abs/2402.10171)

    本论文研究了将语言模型的上下文长度扩展到128K的连续预训练方法，通过适当的数据混合和轻量级的预训练可以实现，其中关键要点在于数据的数量和质量，需要注意领域平衡和长度上采样。

    

    我们研究了将语言模型的上下文长度扩展到128K的连续预训练方法，着重考虑数据工程。我们假设长上下文建模，特别是“能够利用任意输入位置的信息”的能力，在大规模预训练中已经得到了掌握，并且这种能力可以通过轻量级连续预训练在比训练时更长的上下文(例如从4K到128K)下轻松扩展。我们研究了连续预训练的数据“数量”和“质量”：(1)对于数量，我们证明5亿到50亿个标记足以使模型能够检索到128K上下文中的任何位置的信息；(2)对于质量，我们的结果同等强调“领域平衡”和“长度上采样”。具体而言，我们发现简单地上采样更长的数据，并不能提供足够的质量，而是需要注意数据的领域平衡和长度上采样。

    arXiv:2402.10171v1 Announce Type: cross  Abstract: We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, we find that naively upsampling longer data o
    
[^12]: DeepSRGM -- 基于深度学习的印度古典音乐中的序列分类和排序

    DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning

    [https://arxiv.org/abs/2402.10168](https://arxiv.org/abs/2402.10168)

    DeepSRGM是一种基于深度学习的Raga识别方法，通过使用LSTM-RNN学习音乐数据中的时间序列，达到了88.1%和97%的准确率，在Raga识别任务中取得了最新技术的地位。

    

    《arXiv:2402.10168v1 公告类型: 交叉》 摘要：印度古典音乐(ICM)的一个重要方面是Raga，它作为作曲和即兴演奏的旋律框架。Raga的识别是ICM中一项重要的音乐信息检索任务，它可以帮助从音乐推荐到组织大型音乐收藏等多种下游应用。在这项工作中，我们提出了一种基于深度学习的Raga识别方法。我们的方法采用有效的预处理，使用基于长短期记忆(LSTM)的递归神经网络(RNN)学习音乐数据中的时间序列。我们在采样自原始音频的较小序列上进行网络的训练和测试，而最终的推理则是在整个音频上进行的。我们的方法在Comp Music Carnatic数据集和其10个Raga子集上的推理过程中分别达到了88.1%和97%的准确率，使其成为Raga识别任务的最新技术。我们的方法还使序列排序成为可能。

    arXiv:2402.10168v1 Announce Type: cross  Abstract: A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence
    
[^13]: 通过动态学习器追踪概率变化

    Tracking Changing Probabilities via Dynamic Learners

    [https://arxiv.org/abs/2402.10142](https://arxiv.org/abs/2402.10142)

    该论文介绍了通过动态学习器追踪概率变化的方法，通过输出候选项目及其概率来预测离散项目序列中下一个可能出现的项目。

    

    考虑一个预测器，即一个学习器，其输入是一系列离散项目。预测器的任务是在每个时间点进行概率多类别预测，即通过输出有零个或多个候选项目及其概率来预测接下来可能发生的项目，然后揭示实际项目并从中学习。为了输出概率，预测器会跟踪其所见项目的比例。预测器具有恒定（有限）的空间，我们寻求高效的预测和更新技术：流是无界的，项目的集合对预测器是未知的，它们的总数也可能无限增长。此外，存在非平稳性：项目的潜在频率可能会不时发生显著变化。例如，新项目可能开始出现，一些当前频繁出现的项目可能再次停止出现。由于有空间限制，预测器只需要提供概率。

    arXiv:2402.10142v1 Announce Type: cross  Abstract: Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide pro
    
[^14]: 用于生物医学数据的对等联邦学习的策略基准测试

    Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data

    [https://arxiv.org/abs/2402.10135](https://arxiv.org/abs/2402.10135)

    这项研究对生物医学数据的对等联邦学习进行了基准测试，并测试了各种聚合策略，包括加权平均聚合，以确定最强大的策略。

    

    数据保护和隐私要求的不断增加引起了对分布式人工智能的巨大研究兴趣，尤其是对联邦学习的研究，它是一种新兴的机器学习方法，允许构建一个模型，该模型由持有自己私有数据的多个参与者之间建立。在最初的联邦学习提案中，架构是集中式的，聚合是通过联邦平均化来完成的，意味着一个中央服务器将使用最直接的平均策略来协调联邦。本研究专注于在对等环境中测试不同的联邦策略。作者提出了各种联邦学习的聚合策略，包括加权平均聚合，使用不同的因素和基于参与者贡献的策略。使用不同大小的数据对这些策略进行测试，以确定最强大的策略。

    arXiv:2402.10135v1 Announce Type: cross  Abstract: The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy. This research is focused on testing different federated strategies in a peer-to-peer environment. The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution. The strategies are tested with varying data sizes to identify the most robust ones. This resear
    
[^15]: 零样本推理: 无冷启动问题的个性化内容生成

    Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem

    [https://arxiv.org/abs/2402.10133](https://arxiv.org/abs/2402.10133)

    本文提出了一种使用大型语言模型实现个性化的新方法，以降低个性化程序化内容生成的门槛，从而实现游戏内容与玩家偏好的匹配。

    

    Procedural content generation（程序化内容生成）使用算法技术以更低的生产成本创建大量新内容。在较新的方法中，程序化内容生成利用机器学习。然而，这些方法通常需要收集大量昂贵的数据，并开发和训练相对复杂的学习模型，这既耗时又昂贵。我们研究的核心是探索能否通过更实用和通用的大型语言模型降低个性化程序化内容生成的门槛。将游戏内容与玩家偏好进行匹配既使玩家更享受游戏，也使开发者更依赖玩家在游戏得到满足之后再进行变现。因此，本文提出了一种使用大型语言模型实现个性化的新方法。

    arXiv:2402.10133v1 Announce Type: new  Abstract: Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models t
    
[^16]: 连续学习是否适应现实挑战？

    Is Continual Learning Ready for Real-world Challenges?

    [https://arxiv.org/abs/2402.10130](https://arxiv.org/abs/2402.10130)

    本文研究了连续学习在现实世界场景中的应用，发现当前的评估方法与实际挑战不匹配，现有解决方案无法有效解决复杂的现实世界环境下的问题。

    

    尽管连续学习在学术界有着悠久而良好的历史，但其在实际应用中的应用仍然相对有限。本文认为这种差距是由于当前评估方法与连续学习的实际挑战不匹配，导致现有解决方案无法有效应对现实世界环境的复杂性。通过使用全新的三维语义分割基准测试OCL-3DSS，我们验证了自己的假设并评估了过去的进展。我们通过利用更加现实的协议来研究文献中的各种连续学习方案，这些方案需要在线和持续学习以应对动态的现实世界场景（例如机器人和三维视觉应用）。结果令人沮丧：所有考虑的方法表现不佳，明显偏离联合离线训练的上限。这对现有方法在实际应用中的适用性提出了问题。

    arXiv:2402.10130v1 Announce Type: cross  Abstract: Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS. We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in rea
    
[^17]: 使用基于Transformer编码器的EEG编码器和GAN从EEG记录中生成视觉刺激

    Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN

    [https://arxiv.org/abs/2402.10115](https://arxiv.org/abs/2402.10115)

    本研究使用基于Transformer编码器的EEG编码器和GAN网络，通过合成图像从EEG信号中恢复出各种对象类别的图像，同时结合对抗损失和感知损失，提高生成图像的质量。

    

    在这项研究中，我们解决了感知性脑解码领域的一个现代研究挑战，即使用对抗式深度学习框架从EEG信号中合成图像。具体目标是利用主体观看图像时获得的EEG记录重新创建属于各种对象类别的图像。为了实现这一目标，我们使用基于Transformer编码器的EEG编码器生成EEG编码，然后将其作为GAN网络的生成器组件的输入。除了对抗损失之外，我们还采用了感知损失来提高生成图像的质量。

    arXiv:2402.10115v1 Announce Type: new  Abstract: In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.
    
[^18]: 选择性反射调节：LLM指令调节的学生选择数据回收

    Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning

    [https://arxiv.org/abs/2402.10110](https://arxiv.org/abs/2402.10110)

    本文介绍了一种名为选择性反射调节的新方法，该方法通过教师LLM的反射和自省与学生LLM的数据选择能力相结合，自动优化现有的指令调节数据，从而实现了高效的指令调节和卓越性能的LLM。

    

    指令调节对于大型语言模型（LLM）来说非常关键，以实现更好的指令跟踪和任务适应能力，但其成功在很大程度上取决于训练数据的质量。许多最近的方法都致力于改进数据质量，但往往忽视了数据与正在微调的学生模型的兼容性。本文介绍了一种新的范式——选择性反射调节，通过结合教师LLM的反射和自省，以自动优化现有的指令调节数据。这种师生合作产生了高质量且与学生LLM兼容的指令响应对，从而实现了高效的指令调节和卓越性能的LLM。选择性反射调节是一种数据增强和合成方法，通常能改善LLM微调和自我优化，而无需额外的计算资源。

    arXiv:2402.10110v1 Announce Type: cross  Abstract: Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without co
    
[^19]: 用可解释的风险预测方法降低诊断错误

    Towards Reducing Diagnostic Errors with Interpretable Risk Prediction

    [https://arxiv.org/abs/2402.10109](https://arxiv.org/abs/2402.10109)

    本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，旨在通过增加证据的获取与减少诊断错误来降低诊断错误。模型使用神经加性模型进行预测，以证据为后盾，并给出个体化风险估计，特别针对诊断延迟和来自不完整鉴别的错误进行优化。

    

    许多诊断错误发生是因为临床医生无法轻易获取病人电子病历中的相关信息。本研究提出了一种使用LLMs方法来识别病人电子病历数据中指示特定诊断风险增加或减少的证据的方法，最终目标是增加证据的获取与减少诊断错误。我们提出了一种神经加性模型来进行带有个体化风险估计的以证据为后盾的预测，在临床医生仍然不确定的时间点上，旨在特别减轻诊断延迟和源于不完整鉴别的错误。为了训练这样一个模型，需要推断出事件性的“真实”诊断的时间粒度细致的回顾性标签。我们使用LLMs来保证输入文本是在可以进行自信的诊断之前的。我们使用LLMs来检索初始的证据池，然后进行细化。

    arXiv:2402.10109v1 Announce Type: new  Abstract: Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual "true" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refin
    
[^20]: 可控扩散语言模型的量化嵌入向量

    Quantized Embedding Vectors for Controllable Diffusion Language Models

    [https://arxiv.org/abs/2402.10107](https://arxiv.org/abs/2402.10107)

    本论文提出了一种量化嵌入可控扩散语言模型（QE-CDLM），通过重建任务特定的嵌入空间来改善扩散语言模型的可控性、可移植性和稳定性。

    

    改善扩散语言模型的可控性、可移植性和推理速度是自然语言生成中的一个关键挑战。尽管最近的研究在语言模型的复杂文本生成方面取得了显著成功，但内存和计算能力仍然非常苛刻，无法满足预期，这自然导致模型的可移植性和稳定性较低。为了解决这些问题，我们提出了一种新的方法，称为量化嵌入可控扩散语言模型（QE-CDLM）。QE-CDLM基于最近成功的可控DLM，通过量化重建了任务特定的嵌入空间。这导致了一种基于梯度的生成式控制器。

    arXiv:2402.10107v1 Announce Type: cross  Abstract: Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generat
    
[^21]: GeoEval：用于评估LLMs和多模型在几何问题解决上的基准

    GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving

    [https://arxiv.org/abs/2402.10104](https://arxiv.org/abs/2402.10104)

    GeoEval基准测试用于评估LLMs和MMs在几何问题解决上的性能，发现WizardMath模型在主要子集上表现出色，但在具有挑战性的子集上准确率较低。

    

    近期在大型语言模型（LLMs）和多模型（MMs）方面的进展展示了它们在问题解决方面的卓越能力。然而，它们在处理几何数学问题方面的熟练程度，即需要综合理解文本和视觉信息，尚未得到彻底评估。为了填补这一空白，我们推出了GeoEval基准测试，这是一个全面的集合，包括一个主要子集合的2000个问题，一个重点关注反推理的750个问题子集合，一个增强子集合的2000个问题以及一个难题子集合的300个问题。这个基准测试有助于更深入地研究LLMs和MMs在解决几何数学问题时的性能。我们对十个LLMs和MMs在这些不同子集上的评估结果显示，WizardMath模型表现出色，在主要子集上达到55.67%的准确率，但在具有挑战性的子集上只有6.00%的准确率。这突出了关键的需求。

    arXiv:2402.10104v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical ne
    
[^22]: 一个隐私保护的、分布式的、合作的基于FCM的癌症研究学习方法

    A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research

    [https://arxiv.org/abs/2402.10102](https://arxiv.org/abs/2402.10102)

    本论文提出了一种隐私保护的、分布式的、合作的基于FCM的学习方法，用于癌症研究，通过联邦学习过程改进了模型的性能。

    

    分布式人工智能引起了越来越多的关注。本文介绍了一种创新的隐私保护分布式学习方法，基于粒子群优化的模糊认知图。作者设计了一种协作FCM学习的训练方案，提供了符合当前规定的数据隐私保护。该方法应用于癌症检测问题，证明了联邦学习过程改进了模型的性能，并获得了类似于文献中的结果。

    arXiv:2402.10102v1 Announce Type: new  Abstract: Distributed Artificial Intelligence is attracting interest day by day. In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way. The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation. This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature.
    
[^23]: 基于深度学习的多导弹避让情况感知

    Deep Learning Based Situation Awareness for Multiple Missiles Evasion

    [https://arxiv.org/abs/2402.10101](https://arxiv.org/abs/2402.10101)

    该研究提出了一种基于深度学习的决策支持工具，用于帮助无人机操作员在多个导弹威胁下进行决策，通过学习高保真度模拟来评估各种策略的风险，并建议最安全的行动方针。

    

    随着空对空导弹的有效射程增加，人类操作员难以保持无人机所需的情况感知能力。在本研究中，我们提出了一种决策支持工具，以帮助无人机操作员在视线外（BVR）空战情景中评估不同选择的风险，并根据这些选择做出决策。早期的工作侧重于单一导弹的威胁，而在本研究中，我们将这些想法拓展到多个导弹威胁上。所提出的方法使用深度神经网络（DNN）通过高保真度模拟学习，为操作员提供一组不同策略的结果估计。我们的结果表明，所提出的系统可以处理多个来袭导弹，评估一系列选项，并推荐风险最小的行动方针。

    arXiv:2402.10101v1 Announce Type: cross  Abstract: As the effective range of air-to-air missiles increases, it becomes harder for human operators to maintain the situational awareness needed to keep a UAV safe. In this work, we propose a decision support tool to help UAV operators in Beyond Visual Range (BVR) air combat scenarios assess the risks of different options and make decisions based on those. Earlier work focused on the threat posed by a single missile, and in this work, we extend the ideas to several missile threats. The proposed method uses Deep Neural Networks (DNN) to learn from high-fidelity simulations to provide the operator with an outcome estimate for a set of different strategies. Our results demonstrate that the proposed system can manage multiple incoming missiles, evaluate a family of options, and recommend the least risky course of action.
    
[^24]: 调谐：在临床设置中使用有限数据的音频分类器性能分析

    Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data

    [https://arxiv.org/abs/2402.10100](https://arxiv.org/abs/2402.10100)

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。

    

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，限制条件是以反映实际世界数据收集的小数据集为基础。我们分析了包括DenseNet和ConvNeXt在内的CNN模型，以及ViT、SWIN和AST等转换模型，并将它们与诸如YAMNet和VGGish的预训练音频模型进行比较。我们的方法强调了在特定临床数据上微调之前，在大数据集上进行预训练的好处。我们从卒中患者中新收集了两个前所未有的患者音频数据集。我们研究了各种预处理技术，发现基于它们从预训练中学习到的先验知识，RGB和灰度谱图转换对模型性能产生了不同的影响。我们的研究结果表明，CNN模型在小数据集环境中可以与转换模型相媲美或超越，其中DenseNet-Contrastive和AST模型表现突出。本研究强调了...

    arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
    
[^25]: MIM-Refiner：一种从中间预训练表示中获得对比学习提升的方法

    MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations

    [https://arxiv.org/abs/2402.10093](https://arxiv.org/abs/2402.10093)

    MIM-Refiner是一种对比学习提升方法，通过利用MIM模型中的中间层表示和多个对比头，能够将MIM模型的特征从次优的状态提升到最先进的状态，并在ImageNet-1K数据集上取得了新的最先进结果。

    

    我们引入了MIM-Refiner，这是一种用于预训练MIM模型的对比学习提升方法。MIM-Refiner的动机在于MIM模型中的最佳表示通常位于中间层。因此，MIM-Refiner利用连接到不同中间层的多个对比头。在每个头中，修改后的最近邻目标帮助构建相应的语义聚类。此过程短而有效，在几个epochs内，我们将MIM模型的特征从次优的状态提升到最先进的状态。使用data2vec 2.0在ImageNet-1K上预训练的ViT-H经过改进后，在线性探测和低样本分类方面取得了新的最先进结果（分别为84.7%和64.2%），超过了在ImageNet-1K上预训练的其他模型的表现。

    arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
    
[^26]: 基于文本的产品匹配--半监督聚类方法

    Text-Based Product Matching -- Semi-Supervised Clustering Approach

    [https://arxiv.org/abs/2402.10091](https://arxiv.org/abs/2402.10091)

    本文介绍了一种利用半监督聚类方法进行产品匹配的新思路，并通过实验证明了无监督匹配与少量注释样本的产品链接可以成为主导的监督策略的替代方法。

    

    在电子商务的许多任务中，匹配多个产品提供中相同的产品是一个关键要素，如比较产品供应、动态价格优化和选择为客户个性化定制的产品组合。它对应于众所周知的实体匹配的机器学习任务，具有其自身的特殊性，如无处不在的非结构化数据或不准确和不一致的产品描述。本文旨在提出一种利用半监督聚类方法进行产品匹配的新思路。我们通过在真实数据集上使用主要是文本特征和模糊字符串匹配的IDEC算法进行实验，以及更多标准方法作为参考，来研究该方法的性能。鼓舞人心的结果显示，无监督匹配结合少量注释样本的产品链接可能是一种可能的替代方法，而不是主导的监督策略。

    arXiv:2402.10091v1 Announce Type: cross  Abstract: Matching identical products present in multiple product feeds constitutes a crucial element of many tasks of e-commerce, such as comparing product offerings, dynamic price optimization, and selecting the assortment personalized for the client. It corresponds to the well-known machine learning task of entity matching, with its own specificity, like omnipresent unstructured data or inaccurate and inconsistent product descriptions. This paper aims to present a new philosophy to product matching utilizing a semi-supervised clustering approach. We study the properties of this method by experimenting with the IDEC algorithm on the real-world dataset using predominantly textual features and fuzzy string matching, with more standard approaches as a point of reference. Encouraging results show that unsupervised matching, enriched with a small annotated sample of product links, could be a possible alternative to the dominant supervised strategy,
    
[^27]: 可解释的人工智能在安全可信的自动驾驶中的应用：一项系统性评述

    Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review

    [https://arxiv.org/abs/2402.10086](https://arxiv.org/abs/2402.10086)

    可解释的AI技术对于解决自动驾驶中的安全问题和信任问题至关重要。本文通过系统文献综述的方式，分析了可解释的AI方法在满足自动驾驶要求方面的关键贡献，并提出了可解释的设计、可解释的替代模型、可解释的监控、辅助技术和解释的可视化等五个方面的应用。

    

    鉴于其在感知和规划任务中相对传统方法具有更优异的性能，人工智能（AI）对于自动驾驶（AD）的应用显示出了很大的潜力。然而，难以理解的AI系统加剧了对AD安全保证的现有挑战。缓解这一挑战的一种方法是利用可解释的AI（XAI）技术。为此，我们首次提出了关于可解释方法在安全可信的AD中的全面系统文献综述。我们首先分析了在AD背景下AI的要求，重点关注数据、模型和机构这三个关键方面。我们发现XAI对于满足这些要求是至关重要的。基于此，我们解释了AI中解释的来源，并描述了一种XAI的分类学。然后，我们确定了XAI在安全可信的AD中的五个主要贡献，包括可解释的设计、可解释的替代模型、可解释的监控，辅助...

    arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
    
[^28]: 在眼科中对大型语言模型（LLM）聊天机器人进行微调，并使用GPT-4进行LLM评估

    Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4

    [https://arxiv.org/abs/2402.10083](https://arxiv.org/abs/2402.10083)

    本论文评估了使用GPT-4的LLM评估的临床一致性，以评估经过精调的LLM聊天机器人生成的眼科患者查询的回答。通过与医生排序进行对比，发现GPT-3.5在临床上的一致性比其他经过精调的LLM更高。

    

    目的：评估基于GPT-4的评估与人类临床专家对经过精调的LLM聊天机器人生成的眼科相关患者查询的回答的一致性。方法：400个眼科问题和配对答案由眼科医生创建，以代表常见的患者问题，分为用于微调的368个（92％）和测试的40个（8％）。我们对5个不同的LLM进行了精调，包括LLAMA2-7b，LLAMA2-7b-Chat，LLAMA2-13b和LLAMA2-13b-Chat。对于测试数据集，还包括8个青光眼问答对。由5个经过精调的LLM生成了200个对测试数据集的回答用于评估。采用定制的临床评估指标来指导GPT-4的评估，以确保临床准确性、相关性、患者安全性和易理解性。然后将GPT-4的评估与5名临床医生的排序进行对比以评估其临床一致性。结果：在所有经过精调的LLM中，GPT-3.5

    arXiv:2402.10083v1 Announce Type: new  Abstract: Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all fine-tuned LLMs, GPT-3.5
    
[^29]: QUICK：针对高效LLM推理的量化感知交错和无冲突内核

    QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference

    [https://arxiv.org/abs/2402.10076](https://arxiv.org/abs/2402.10076)

    QUICK是一组针对量化大语言模型（LLMs）的高效推理的优化CUDA内核。通过解决共享内存冲突问题和交错量化权重矩阵，QUICK实现了显著的速度提升和吞吐量增益。

    

    我们介绍了QUICK，一组用于高效推理量化大语言模型（LLMs）的优化CUDA内核。QUICK解决了现有混合精度矩阵乘法内核的共享内存冲突问题。我们的方法在离线情况下交错LLMs的量化权重矩阵，从而跳过解量化后的共享内存写回。我们在较大批次上展示了与AutoAWQ现有内核相比多达1.91倍的加速效果，并在各种NVIDIA GPU设备上的代表性LLM模型上获得了多达1.94倍的吞吐量增益。

    arXiv:2402.10076v1 Announce Type: cross  Abstract: We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of quantized Large Language Models (LLMs). QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels. Our method interleaves the quantized weight matrices of LLMs offline to skip the shared memory write-back after the dequantization. We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices.
    
[^30]: 基于LLM的应用意图管理的策略生成

    LLM-based policy generation for intent-based management of applications

    [https://arxiv.org/abs/2402.10067](https://arxiv.org/abs/2402.10067)

    这项研究提出了基于LLM的策略生成方法，用于实现自动化的应用意图管理。通过生成逐步分解意图所需的动作，并将其映射到API，实现了闭控制循环来自动化策略执行。

    

    自动化管理需要将高级用户请求，例如意图，分解成系统可以理解和执行的抽象。这是具有挑战性的，因为即使是一个简单的意图也需要执行一系列有序的步骤。而识别和适应这些步骤（随着条件的变化）的任务需要一种无法事先完全定义的分解方法。为了解决这些挑战并支持自动化的意图分解和执行，我们探索了大型语言模型（LLM）的少样本能力。我们提出了一个管道，通过生成所需的动作，使用基于策略的抽象逐步分解意图。这使我们能够通过创建用于意图部署的闭控制循环来自动化策略执行。为此，我们生成并将策略映射到API，并形成执行所需的监控、分析、计划和执行的应用管理循环。

    arXiv:2402.10067v1 Announce Type: cross  Abstract: Automated management requires decomposing high-level user requests, such as intents, to an abstraction that the system can understand and execute. This is challenging because even a simple intent requires performing a number of ordered steps. And the task of identifying and adapting these steps (as conditions change) requires a decomposition approach that cannot be exactly pre-defined beforehand. To tackle these challenges and support automated intent decomposition and execution, we explore the few-shot capability of Large Language Models (LLMs). We propose a pipeline that progressively decomposes intents by generating the required actions using a policy-based abstraction. This allows us to automate the policy execution by creating a closed control loop for the intent deployment. To do so, we generate and map the policies to APIs and form application management loops that perform the necessary monitoring, analysis, planning and executi
    
[^31]: 通过实例分割神经网络在人类视网膜图像中实现鲁棒的半自动血管追踪

    Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network

    [https://arxiv.org/abs/2402.10055](https://arxiv.org/abs/2402.10055)

    通过实例分割神经网络实现了一种鲁棒的半自动血管追踪算法，能够追踪每棵血管树的分支。

    

    体征神经网络（InSegNN）是一种通过实例分割神经网络的新方法，在人类眼底图像上实现了一种鲁棒的半自动血管追踪算法。与语义分割不同，InSegNN能够单独分离和标记不同的血管树，并且能够追踪每棵树的分支。

    arXiv:2402.10055v1 Announce Type: cross  Abstract: The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism. In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH). Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed morphological quantification, and yet remains a challenging task. Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN). Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching. We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and d
    
[^32]: 大型语言模型通过自我蒸馏和有意识的想象进行遗忘

    Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination

    [https://arxiv.org/abs/2402.10052](https://arxiv.org/abs/2402.10052)

    本研究提出了一种新颖的大型语言模型遗忘方法，通过自我蒸馏和有意识的想象，有效地遗忘目标文本，并在生成任务和自然语言理解任务中保留模型的能力。

    

    虽然在许多任务上表现出令人印象深刻的生成能力，但大型语言模型（LLM）仍然存在隐私侵犯和敏感数据不受控制的问题。因此，我们提出了一种新颖的方法，即在LLM遗忘的过程中采用有意识的想象。我们不是试图忘记已记忆的数据，而是通过自我蒸馏的框架引导LLM有意识地想象替代情境。通过广泛的实验，我们证明了这种方法不仅可以有效地遗忘目标文本，还可以保留LLM在开放式生成任务和自然语言理解（NLU）任务中的能力。我们的结果展示了这种方法在不同模型和规模中的实用性。

    arXiv:2402.10052v1 Announce Type: cross  Abstract: While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also wit
    
[^33]: SwissNYF：基于黑盒环境的基于LLM的工具生成智能体

    SwissNYF: Tool Grounded LLM Agents for Black Box Setting

    [https://arxiv.org/abs/2402.10051](https://arxiv.org/abs/2402.10051)

    该论文提出了一种基于黑盒环境的基于LLM的工具生成智能体的方法，在复杂的API调用中表现出了优越的性能，可以应对具有不可逆性和大量时间消耗的任务。

    

    在访问函数的返回结果上，大型语言模型（LLM）已经展示了增强的功能调用能力，但这种方法在简单API上是实用的，但对于不可逆API（例如数据库删除API）会面临可扩展性问题。同样，对于每个API调用需要大量时间的流程以及需要前向规划的自动化操作管道等都存在复杂的挑战。此外，通常出现的情况是需要一种通用的方法，因为算法缺乏对这些函数的特定实现或使用它们的秘密的直接访问方式。在这些情况下，传统的工具规划方法是不合适的，因此需要在黑盒环境中运行。与在工具操作中的表现不同，LLM在黑盒任务（例如程序综合）中表现出色。因此，我们利用LLM来生成基于黑盒环境的智能体。

    arXiv:2402.10051v1 Announce Type: new  Abstract: While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments. Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis. Therefore, we harness the 
    
[^34]: 使用唤醒手势的按需肌电控制，在日常生活中消除虚假激活

    On-Demand Myoelectric Control Using Wake Gestures to Eliminate False Activations During Activities of Daily Living

    [https://arxiv.org/abs/2402.10050](https://arxiv.org/abs/2402.10050)

    在日常生活中，通过使用唤醒手势作为一种按需肌电控制范式，能够有效减少虚假激活，提高肌电控制的可靠性。

    

    最近，肌电控制作为一种可能的灵活、免提输入方式，受到了越来越多的研究关注。然而，在真实世界的条件下，当前的控制方法容易出现意外的虚假激活。本文提出、设计和评估了一种新颖的肌电控制范式——按需肌电控制，旨在减少错误地解释为输入手势的非相关肌肉运动的数量。通过利用唤醒手势的概念，用户能够在专用控制模式和睡眠模式之间进行切换，在日常生活活动中有效地消除意外激活。本研究通过两个在线普适肌电控制任务（关闭闹钟和控制机器人），展示了唤醒手势的可行性。所提出的控制方案几乎能够适当地忽略所有非目标肌肉输入。

    arXiv:2402.10050v1 Announce Type: cross  Abstract: While myoelectric control has recently become a focus of increased research as a possible flexible hands-free input modality, current control approaches are prone to inadvertent false activations in real-world conditions. In this work, a novel myoelectric control paradigm -- on-demand myoelectric control -- is proposed, designed, and evaluated, to reduce the number of unrelated muscle movements that are incorrectly interpreted as input gestures . By leveraging the concept of wake gestures, users were able to switch between a dedicated control mode and a sleep mode, effectively eliminating inadvertent activations during activities of daily living (ADLs). The feasibility of wake gestures was demonstrated in this work through two online ubiquitous EMG control tasks with varying difficulty levels; dismissing an alarm and controlling a robot. The proposed control scheme was able to appropriately ignore almost all non-targeted muscular input
    
[^35]: RS-DPO：一种用于对齐大型语言模型的混合拒绝采样和直接优化偏好的方法

    RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

    [https://arxiv.org/abs/2402.10038](https://arxiv.org/abs/2402.10038)

    本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。

    

    强化学习从人类反馈中学习（RLHF）已被广泛应用于将大型语言模型与用户意图对齐。然而，基于近端策略优化（PPO）的RLHF有时不稳定，需要显著的超参数微调，并且在对齐过程中计算成本高昂。最近，提出了直接优化偏好（DPO）来解决这些挑战。然而，DPO依赖于从人类标注者和替代LLM生成的对比回复，而不是策略模型，限制了RLHF的效果。本文通过系统地结合拒绝采样（RS）和DPO来解决这两个挑战。我们提出的方法RS-DPO，首先开发出一个经过监督微调的策略模型（SFT）。然后直接从SFT模型中采样每个提示的k个响应。RS-DPO基于其相似度识别对比样本对。

    arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
    
[^36]: 扩散模型与大动作空间情境强化学习的结合

    Diffusion Models Meet Contextual Bandits with Large Action Spaces

    [https://arxiv.org/abs/2402.10028](https://arxiv.org/abs/2402.10028)

    本文设计了一种利用预训练扩散模型的扩散汤普森采样方法，用于在大动作空间下进行高效的情境强化学习探索。实证评估结果表明了该方法的优越性能。

    

    由于动作空间较大，有效的探索是情境强化学习中的一个关键挑战。本文通过利用预训练的扩散模型来捕捉动作之间的相关性，设计了扩散汤普森采样（dTS）方法，实现了高效的探索。我们为dTS方法提供了理论和算法基础，并通过实证评估展示了它的优越性能。

    arXiv:2402.10028v1 Announce Type: cross  Abstract: Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.
    
[^37]: 自学习上下文增强对于无监督词汇翻译的研究

    Self-Augmented In-Context Learning for Unsupervised Word Translation

    [https://arxiv.org/abs/2402.10024](https://arxiv.org/abs/2402.10024)

    通过自学习上下文增强方法，本论文提出一种无监督词汇翻译的方法，在零样本提示的大型语言模型上取得了显著的改进，超过了传统基于映射的方法。

    

    近期的研究表明，尽管大型语言模型在一些小规模的设置中展示出了较强的词汇翻译和双语词典诱导(BLI)的能力，但在无监督的情况下，即没有种子翻译对可用的情况下，尤其是对于资源较少的语言，它们仍然无法达到“传统”的基于映射的方法的性能。为了解决这个挑战，我们提出了一种自学习上下文增强方法 (SAIL) 来进行无监督的BLI：从零样本提示开始，SAIL通过迭代地从LLM中引出一组高置信度的词汇翻译对，然后在ICL的方式下再次应用于同一个LLM中。我们的方法在两个广泛的BLI基准测试中，跨越多种语言对，在零样本提示的LLM上取得了显著的改进，也在各个方面优于基于映射的基线。除了达到最先进的无监督

    arXiv:2402.10024v1 Announce Type: cross  Abstract: Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised 
    
[^38]: Clifford群等变单体消息传递网络

    Clifford Group Equivariant Simplicial Message Passing Networks

    [https://arxiv.org/abs/2402.10011](https://arxiv.org/abs/2402.10011)

    本论文介绍了一种Clifford群等变单体消息传递网络，通过将Clifford群等变层与单体消息传递相结合，实现了在拓扑上更为复杂的E（n）-等变消息传递。实验结果表明，该方法具有良好的效果。

    

    我们引入了Clifford群等变单体消息传递网络，这是一种在单体复合体上进行可控的E（n）-等变消息传递的方法。我们的方法将Clifford群等变层的表达能力与单体消息传递相结合，后者在拓扑上比常规图消息传递更加复杂。Clifford代数包括高阶对象，如双向量和三向量，这些对象通过向量衍生出几何特征（例如面积，体积）。利用这些知识，我们通过顶点的几何乘积表示简单形式特征。为了实现高效的单体消息传递，我们在不同维度之间共享消息网络的参数。此外，我们将最终的消息限制为来自不同维度的传入消息的聚合，从而导致了我们称之为共享单体消息传递的方法。实验结果表明，我们的方法能够输出适当的结果。

    arXiv:2402.10011v1 Announce Type: new  Abstract: We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to ou
    
[^39]: ML-ASPA: 机器学习在声学信号处理分析中的思考

    ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal Processing Analysis for Sounds, & Strains Emerging Technology

    [https://arxiv.org/abs/2402.10005](https://arxiv.org/abs/2402.10005)

    本文研究了机器学习在声学信号处理分析中的应用，通过数据驱动的方法，揭示了复杂声学现象的模型。

    

    声学数据在推动科学和工程理解方面起着基本的基石作用，涉及生物学、通信学以及海洋和地球科学等多个学科。本文详细探讨了声学领域中最近的进展和变革潜力，特别关注机器学习（ML）和深度学习。与传统的声学和信号处理相比，ML采用数据驱动的方法，揭示了特征与期望标签或动作之间以及特征之间的复杂关系，给定充足的训练数据。将ML应用于大量的训练数据集有助于发现能够解释人类语音和混响等复杂声学现象的模型。

    arXiv:2402.10005v1 Announce Type: cross  Abstract: Acoustic data serves as a fundamental cornerstone in advancing scientific and engineering understanding across diverse disciplines, spanning biology, communications, and ocean and Earth science. This inquiry meticulously explores recent advancements and transformative potential within the domain of acoustics, specifically focusing on machine learning (ML) and deep learning. ML, comprising an extensive array of statistical techniques, proves indispensable for autonomously discerning and leveraging patterns within data. In contrast to traditional acoustics and signal processing, ML adopts a data-driven approach, unveiling intricate relationships between features and desired labels or actions, as well as among features themselves, given ample training data. The application of ML to expansive sets of training data facilitates the discovery of models elucidating complex acoustic phenomena such as human speech and reverberation. The dynamic 
    
[^40]: MM-Point: 多视角信息增强的多模态自监督三维点云理解

    MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding

    [https://arxiv.org/abs/2402.10002](https://arxiv.org/abs/2402.10002)

    本文提出了一种新颖的自监督点云表示学习方法MM-Point，通过多模态交互和传输实现了3D物体和多个2D视图之间的信息增强。通过精心设计的实验，证明了MM-Point的有效性和优越性。

    

    在感知领域中，将多种传感信息整合起来将2D视图上的视觉信息映射到3D物体上，这有助于在三维环境中进行理解。但是在从不同角度渲染的单个2D视图中，只能提供有限的部分信息。多视角2D信息的丰富性和价值可以为3D物体提供优秀的自监督信号。在本文中，我们提出了一种新颖的自监督点云表示学习方法MM-Point，它受到内模态和外模态相似度目标的驱动。MM-Point的核心在于3D物体和多个2D视图之间的多模态交互和传输。为了更有效地同时执行基于对比学习的2D多视图信息一致性交叉模态目标，我们进一步提出了多层感知机(Multi-MLP)和多层级增强策略。通过精心设计的实验，我们展示了MM-Point的有效性和优越性。

    arXiv:2402.10002v1 Announce Type: cross  Abstract: In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully desig
    
[^41]: LoraRetriever: 适应输入的LoRA检索与合成方法用于混合任务

    LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild

    [https://arxiv.org/abs/2402.09997](https://arxiv.org/abs/2402.09997)

    LoraRetriever提出了一种适应输入的LoRA检索与合成方法，用于弥合实际情况下大型语言模型接收到不同任务提示的差距。

    

    Low-Rank Adaptation (LoRA)为大型语言模型（LLM）的微调提供了一种有效而高效的解决方案。LoRA的模块化和即插即用的特性使得能够集成各种领域特定的LoRA，以增强LLM的能力。先前的研究要么专注于特定的隔离下游任务，要么在训练过程中固定LoRA的选择。然而，在实际情况中，LLM接收到涵盖不同任务的各种提示，并且候选LoRA的池经常动态更新。为了弥合这一差距，我们提出了LoraRetriever，一种根据输入提示自适应检索和合成多个LoRA的框架。LoraRetriever包含三个主要组成部分：首先，识别和检索与给定输入相关的LoRA；其次，制定有效整合检索到的LoRA的策略；最后，开发高效的方法用于实现LoRA的合成。

    arXiv:2402.09997v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing effici
    
[^42]: 对于临时团队合作的对称破缺增强

    Symmetry-Breaking Augmentations for Ad Hoc Teamwork

    [https://arxiv.org/abs/2402.09984](https://arxiv.org/abs/2402.09984)

    本研究提出了一种称为对称破缺增强的方法，通过增加训练队友的行为多样性来提高人工智能代理与新队友合作的性能。实验证明了该方法的有效性。

    

    在许多协作环境中，人工智能（AI）代理必须能够适应使用未知或先前未观察到的策略的新队友。对于AI代理来说，这通常对人类来说很简单，但却是一项具有挑战性的任务。例如，如果一个AI代理在训练集中学会了与只在一侧道路上行驶的其他车辆并行驶，那么即使这些车辆的行为只是在左右对称上进行了翻转，它也可能难以适应与相反方向上行驶的驾驶员进行协调。为了解决这个问题，我们引入了对称破缺增强（SBA），通过应用对称翻转操作来增加训练队友的行为多样性。通过学习对增强后的队友的最佳响应，我们的代理能够接触到更广泛的行为约定，从而提高与新队友合作时的性能。我们在两个设置中进行了实验验证，并证明了我们的方法的有效性。

    arXiv:2402.09984v1 Announce Type: cross  Abstract: In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies. While often simple for humans, this can be challenging for AI agents. For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry. To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation. By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates. We demonstrate this experimentally in two settings, and show that our a
    
[^43]: 数据增强和迁移学习应用于面部表情识别

    Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition

    [https://arxiv.org/abs/2402.09982](https://arxiv.org/abs/2402.09982)

    本文提出了一种改进面部表情识别的新型数据增强技术，并应用迁移学习方法，通过使用预训练卷积神经网络在增强的数据集上进行微调，实现了高达85%的平均准确度。

    

    面部表情是我们在理解一个人的心理状态时首先关注的事物。因此，能够自动识别面部表情是一个非常有趣的研究领域。在这篇论文中，由于可用训练数据集的规模较小，我们提出了一种改进识别任务性能的新型数据增强技术。我们应用几何变换，并从头构建了能够为每种情绪类型生成新的合成图像的GAN模型。因此，在增强的数据集上，我们使用不同架构的预训练卷积神经网络进行微调。为了衡量模型的泛化能力，我们采用了额外数据库协议方法，即我们在经过增强的训练数据集上训练模型，然后在两个不同的数据库上进行测试。这些技术的组合使得可以达到平均准确度约为85%的数值。

    arXiv:2402.09982v1 Announce Type: cross  Abstract: The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\% for 
    
[^44]: 语言模型压缩的快速词汇转移方法

    Fast Vocabulary Transfer for Language Model Compression

    [https://arxiv.org/abs/2402.09977](https://arxiv.org/abs/2402.09977)

    提出了一种基于词汇转移的语言模型压缩方法，通过与其他压缩技术结合使用，显著减小模型大小和推理时间，同时性能略有妥协。

    

    实际业务应用需要在语言模型性能和大小之间做出权衡。我们提出了一种基于词汇转移的模型压缩方法。我们在不同垂直领域和下游任务中评估了该方法。我们的结果表明，词汇转移可以与其他压缩技术有效结合使用，显著减小模型大小和推理时间，同时在性能上略有妥协。

    arXiv:2402.09977v1 Announce Type: cross  Abstract: Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.
    
[^45]: 机器学习中数据的层次化表示

    Hierarchy Representation of Data in Machine Learnings

    [https://arxiv.org/abs/2402.09965](https://arxiv.org/abs/2402.09965)

    该论文提出了一种用于可视化目标间层次关系的方法，对于模型改进具有潜在的益处。

    

    当存在多个数据点的模型具有明确的判断结果时，大多数模型可能展示出一种关系，即如果它们正确判断一个目标，则它们也会正确判断另一个目标。相反，如果大多数模型错误地判断一个目标，它们可能也会错误地判断另一个目标。我们提出了一种可视化目标之间层次关系的方法。这些信息有望对模型改进有益。

    arXiv:2402.09965v1 Announce Type: cross  Abstract: When there are models with clear-cut judgment results for several data points, it is possible that most models exhibit a relationship where if they correctly judge one target, they also correctly judge another target. Conversely, if most models incorrectly judge one target, they may also incorrectly judge another target. We propose a method for visualizing this hierarchy among targets. This information is expected to be beneficial for model improvement.
    
[^46]: FedLion: 更快的自适应联邦优化算法，通信更少

    FedLion: Faster Adaptive Federated Optimization with Fewer Communication

    [https://arxiv.org/abs/2402.09941](https://arxiv.org/abs/2402.09941)

    FedLion是一种自适应联邦优化算法，通过引入集中式自适应算法Lion的关键元素，实现了更快的收敛速度和更少的通信成本。经过广泛评估，FedLion优于之前的最先进自适应算法，并通过使用有符号梯度在本地训练中减少数据传输要求。

    

    在联邦学习（FL）中，一种跨分布式数据训练机器学习模型的框架中，像FedAvg这样的知名算法往往具有较慢的收敛速度，在训练过程中导致高通信成本。为了解决这个挑战，我们引入了FedLion，一种自适应联邦优化算法，无缝地将最近提出的集中式自适应算法Lion（Chen et al. 2023）的关键元素融入到FL框架中。通过对两个广泛采用的FL基准进行全面评估，我们证明了FedLion优于之前的最先进自适应算法，包括FAFED（Wu et al. 2023）和FedDA。此外，由于在本地训练中使用了有符号梯度，与现有的自适应算法相比，FedLion在上行通信过程中大大降低了数据传输要求，进一步降低了通信成本。

    arXiv:2402.09941v1 Announce Type: cross  Abstract: In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training. To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into the FL framework. Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. Last but not least, this work also incl
    
[^47]: 建筑行业中的生成式人工智能：一项最新分析

    Generative AI in the Construction Industry: A State-of-the-art Analysis

    [https://arxiv.org/abs/2402.09939](https://arxiv.org/abs/2402.09939)

    本研究通过分析提供了建筑行业中生成式AI的最新状态、机遇和挑战。同时，提出了一个帮助建筑公司构建定制化生成式AI解决方案的框架。

    

    建筑行业是全球经济中至关重要的一个部门，但在设计、规划、采购、检查和维护等各个环节中面临着许多生产力挑战。生成式人工智能（AI）可以基于某些输入或先前的知识创造新颖且逼真的数据或内容，如文本、图像、视频或代码，为解决这些挑战提供了创新和颠覆性的解决方案。然而，在关于建筑行业中生成式AI的当前状态、机遇和挑战的文献中存在着空白。本研究旨在通过提供建筑领域生成式AI的最新分析来填补这一空缺，研究目标包括：（1）对建筑行业现有和新兴的生成式AI机遇和挑战进行回顾和分类；（2）提出一个框架，帮助建筑公司利用自己的数据和需求构建定制化的生成式AI解决方案。

    arXiv:2402.09939v1 Announce Type: new  Abstract: The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their ow
    
[^48]: 关注偏差：挖掘在线话语中的演绎细微差别，检测反问主义

    Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse

    [https://arxiv.org/abs/2402.09934](https://arxiv.org/abs/2402.09934)

    本研究挖掘了在线话语中的演绎细微差别，提出了一种新的方法来准确检测反问主义，并在Twitter和YouTube数据集中取得了显著的改进。

    

    反问主义在扰乱叙事和播种不信任方面具有强大的工具效用，但在定量自然语言处理研究中却未得到充分探索。此外，过去的研究未能区分反问主义作为误导和宣传策略的用途与其作为语用和语义框架工具的用途。我们介绍了新的来自Twitter和YouTube的数据集，揭示了反问主义、宣传和tu quoque谬误之间的重叠和区别。此外，结合最近在语言语义学领域的研究，我们将“what about”词汇结构与反问主义区分开来。我们的实验揭示了准确检测反问主义的独特挑战，促使我们引入了一种使用注意权重进行负样本挖掘的新方法。在Twitter和YouTube数据集中，我们的方法分别相对于之前最先进的方法提高了4%和10%。

    arXiv:2402.09934v1 Announce Type: cross  Abstract: Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.
    
[^49]: 一个包含多段答案的开放领域问答数据集

    A Dataset of Open-Domain Question Answering with Multiple-Span Answers

    [https://arxiv.org/abs/2402.09923](https://arxiv.org/abs/2402.09923)

    提出了一个中文多段答案的开放领域问答数据集CLEAN，弥补了中文MSQA研究中的不足，包括多样的主题和需要详细回答的问题。提供了相关文献中的基线模型作为参考。

    

    多段答案提取，也称为多段问答（MSQA）任务，在实际应用中至关重要，因为它需要从文本中提取多个信息片段来回答复杂的问题。尽管英文MSQA研究活跃并取得了快速进展，但在中文领域缺乏公开可用的MSQA基准数据集。以往构建MSQA数据集的努力主要强调实体中心的情境化，导致偏向收集事实性问题并可能忽视需要更详细描述性回答的问题。为了克服这些限制，我们提供了CLEAN，一个全面的中文多段问答数据集，涉及各种开放领域的主题，并包含大量需要描述性答案的实例。此外，我们还提供了相关文献中的已建立模型作为CLEAN的基线。

    arXiv:2402.09923v1 Announce Type: cross  Abstract: Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions. Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEA
    
[^50]: 识别和建模移动选择中的认知偏差

    Identifying and modelling cognitive biases in mobility choices

    [https://arxiv.org/abs/2402.09921](https://arxiv.org/abs/2402.09921)

    本论文提出了一种研究移动选择中认知偏差的代理模型，并通过调查结果揭示了决策中存在的各种偏见。最后，论文在GAMA代理模拟中实现了这一模型。

    

    本论文报告了一项M1实习工作中关于基于代理的模型和模拟日常移动选择的研究结果。这个模拟旨在足够真实，以作为一个关于移动过渡的严肃游戏的基础。为了确保这种真实性，我们进行了一项调查，以衡量真实移动选择是理性的，还是存在偏差。在这里分析的结果显示，各种偏见可能在决策中起作用。然后，我们在GAMA代理模拟中提出了一种实现方法。

    arXiv:2402.09921v1 Announce Type: cross  Abstract: This report presents results from an M1 internship dedicated to agent-based modelling and simulation of daily mobility choices. This simulation is intended to be realistic enough to serve as a basis for a serious game about the mobility transition. In order to ensure this level of realism, we conducted a survey to measure if real mobility choices are made rationally, or how biased they are. Results analysed here show that various biases could play a role in decisions. We then propose an implementation in a GAMA agent-based simulation.
    
[^51]: 道路图生成器：从GPS数据中生成建筑工地道路地图

    Road Graph Generator: Mapping roads at construction sites from GPS data

    [https://arxiv.org/abs/2402.09919](https://arxiv.org/abs/2402.09919)

    本研究提出了一种通过分析GPS轨迹来绘制建筑工地道路地图的方法，通过识别关键的交叉口并连接它们，生成道路图，为规划和任务分配提供支持。

    

    在本文中，我们提出了一种从GPS轨迹中推测道路以绘制建筑工地地图的方法。这项任务由于建筑机械的不规则和非标准运动模式与已建立道路上的 typcial 车辆交通显著不同，因此面临着独特的挑战。我们的方法首先识别道路网络中作为关键决策点的交叉口，然后连接它们以形成一个图，随后可以用于规划和任务分配。我们通过在挪威的一个实际建筑工地绘制道路来证明我们方法的有效性。

    arXiv:2402.09919v1 Announce Type: new  Abstract: We present a method for road inference from GPS trajectories to map construction sites. This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads. Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation. We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway.
    
[^52]: 使用伪和多源知识图增强大型语言模型进行开放式问题回答

    Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering

    [https://arxiv.org/abs/2402.09911](https://arxiv.org/abs/2402.09911)

    使用伪和多源知识图对大型语言模型进行增强，以改善其幻觉问题和提高性能。通过结合伪图生成和原子级知识验证的框架，在开放式问题回答环境中使用知识图可以提高ROUGE-L分数至少11.5。

    

    减轻大型语言模型（LLM）的幻觉并增强它们是一项关键任务。尽管一些现有方法采用了模型自我增强技术，但它们在有效解决未知事实幻觉方面存在不足。使用知识图（KG）增强方法无法同时解决不同KG来源之间的泛化和开放式答案问题的增强。为了解决这些限制，提出了一种结合了伪图生成和原子级知识验证的框架。通过利用伪图生成来实现在开放式问题回答环境中使用KG增强LLM。原子级知识验证利用原子级知识查询和验证来实现在不同KG来源下的泛化能力。与基准相比，该方法在ROUGE-L分数上至少提升了11.5。

    arXiv:2402.09911v1 Announce Type: cross  Abstract: Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ende
    
[^53]: 生成表示指令调整

    Generative Representational Instruction Tuning

    [https://arxiv.org/abs/2402.09906](https://arxiv.org/abs/2402.09906)

    本研究引入了生成表示指令调整（GRIT）方法，通过指令区分生成和嵌入任务，训练一个大型语言模型同时处理这两种任务。与其他模型相比，我们的GritLM 7B在文本嵌入基准测试上达到最新的技术水平，并在多种生成任务中表现出色。通过进一步扩大规模，我们的GritLM 8x7B成为最佳的生成语言模型之一，同时仍然是最好的嵌入模型之一。GRIT的统一也大大提高了RAG在长文档上的速度。

    

    所有基于文本的语言问题都可以归结为生成或嵌入。目前的模型只能在其中一种任务上表现良好。我们介绍了生成表示指令调整（GRIT）方法，通过指令来区分生成和嵌入任务，从而训练一个大型语言模型同时处理这两种任务。与其他开放模型相比，我们的GritLM 7B在大规模文本嵌入基准测试（MTEB）上取得了最新的技术水平，并在多种生成任务中超过了同等规模的所有模型。通过进一步扩大规模，GritLM 8x7B在尝试的所有开放生成语言模型中表现最佳，同时仍然是最好的嵌入模型之一。值得注意的是，我们发现GRIT可以与仅在生成或嵌入数据上训练的模型相媲美，因此我们可以在不损失性能的情况下统一两者。除此之外，通过GRIT的统一可以将RAG（检索增强生成）在长文档上的速度提高60%以上。

    arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, 
    
[^54]: 重新审视带有内存单子的循环强化学习

    Revisiting Recurrent Reinforcement Learning with Memory Monoids

    [https://arxiv.org/abs/2402.09900](https://arxiv.org/abs/2402.09900)

    这篇论文重新审视了使用内存单子的循环强化学习方法。通过定义新颖的内存单子框架并提出一种新的批处理方法，改进了样本效率、增加了回报并简化了实现过程。

    

    在强化学习中，像RNN和transformers这样的记忆模型通过将轨迹映射到潜在的马尔可夫状态来处理部分可观察的马尔可夫决策过程（POMDPs）。这些模型对于长序列的规模化处理能力并不特别好，尤其是与一类新兴的记忆模型（有时称为线性循环模型）相比。我们发现这些模型的循环更新是一个单子，因此我们正式定义了一个新颖的内存单子框架。我们重新审视了循环强化学习中的传统批处理方法，突出了理论和实证上的不足之处。利用内存单子的特性，我们提出了一种新的批处理方法，改进了样本效率，增加了回报，并简化了循环丢失函数在强化学习中的实施。

    arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
    
[^55]: 不仅仅是新颖性：关于AI工作流程的效用和定制化的纵向研究

    Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows

    [https://arxiv.org/abs/2402.09894](https://arxiv.org/abs/2402.09894)

    这项纵向研究调查了生成式AI工作流程的实用性和定制化程度，结果显示，在熟悉化阶段后，用户感知到的系统效用提高了。

    

    生成式AI为人们在日常任务中提供了新颖而令人印象深刻的能力。有许多AI工作流程通过将AI输出与人类互动相结合来解决真实而复杂的问题。尽管AI具有无可否认的吸引力，但在新鲜感消失后，生成式AI工作流程的实用性如何仍然不确定。此外，利用生成式AI构建的工具具有个性化和快速适应的潜力，但用户是否充分利用了个性化的可能性呢？我们进行了一项为期三周的纵向研究，共有12个用户，旨在了解科学传播中生成式AI工具的熟悉度和定制化程度。我们的研究发现，熟悉化阶段持续了4.3个会话，用户在这个阶段探索工作流程的功能以及他们发现哪些方面有用。在熟悉化后，系统的感知效用评分高于之前，表明了感知效用的提高。

    arXiv:2402.09894v1 Announce Type: cross  Abstract: Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived
    
[^56]: Lester: 通过视频对象分割和跟踪实现Rotoscope动画

    Lester: rotoscope animation through video object segmentation and tracking

    [https://arxiv.org/abs/2402.09883](https://arxiv.org/abs/2402.09883)

    Lester是一种通过视频对象分割和跟踪实现Rotoscope动画的新方法，具有出色的时间一致性和适应性。

    

    本文介绍了Lester，一种从视频中自动合成复古风格2D动画的新方法。该方法主要将挑战看作是对象分割和跟踪问题。视频帧使用Segment Anything Model (SAM)进行处理，生成的掩模通过DeAOT进行后续帧的跟踪，DeAOT是一种用于半监督视频对象分割的层次传播方法。掩模的几何轮廓使用Douglas-Peucker算法进行简化。最后，可以选择性地添加面部特征、像素化和基本阴影效果。结果表明，该方法具有出色的时间一致性，能够正确处理具有不同姿势和外观、动态镜头、部分镜头和多样背景的视频。与基于扩散模型的视频到视频转换流水线相比，所提出的方法提供了一种更简单和确定性的方法。

    arXiv:2402.09883v1 Announce Type: cross  Abstract: This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer
    
[^57]: 在生成人工智能时代，大型语言模型基准的不足之处

    Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence

    [https://arxiv.org/abs/2402.09880](https://arxiv.org/abs/2402.09880)

    该论文通过批判性评估研究了23个最先进的大型语言模型基准的不足之处，包括偏见、真实推理衡量困难、实现不一致性等问题，强调了在人工智能时代需要标准化方法、监管确定性和伦理指南。

    

    大型语言模型（LLMs）随着其新兴能力的快速崛起，引发了公众的好奇心，以评估和比较不同的LLMs，许多研究人员提出了他们的LLM基准。我们注意到这些基准的初步不足，开始了一项研究，通过人们、过程和技术的视角，以功能和安全两大支柱为基础，使用我们的新颖统一评估框架对23个最先进的LLM基准进行了批判性评估。我们的研究揭示了一些重大限制，包括偏见、测量真实推理的困难、适应性、实现不一致性、提示工程复杂性、评估者多样性以及在一次综合评估中忽视了文化和意识形态规范。我们的讨论强调了在人工智能时代，迫切需要标准化方法、监管确定性和伦理指南。

    arXiv:2402.09880v1 Announce Type: new  Abstract: The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligenc
    
[^58]: 计算具有统一动作成本的计划

    On Computing Plans with Uniform Action Costs

    [https://arxiv.org/abs/2402.09877](https://arxiv.org/abs/2402.09877)

    该论文提出了适用于自动规划的三种一致性度量，并引入了基于规划的编译技术，可以生成动作成本一致的计划。

    

    在许多实际的规划应用中，代理人可能有兴趣找到动作成本尽可能一致的计划。这样的计划为代理人提供了稳定性和可预测性，这在人类执行规划工具建议的计划时是关键特征。本文将三个一致性度量应用于自动规划，并引入了基于规划的编译技术，允许以动作成本总和和动作成本一致性进行词典排序最优化。在知名和新颖的规划基准测试中的实验结果表明，可以有效地解决重构的任务以生成一致的计划。

    arXiv:2402.09877v1 Announce Type: new  Abstract: In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans.
    
[^59]: MuChin：用于评估音乐领域中语言模型的中文口语描述基准

    MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music

    [https://arxiv.org/abs/2402.09871](https://arxiv.org/abs/2402.09871)

    MuChin是一个用于评估多模态语言模型在音乐理解和描述方面性能的中文口语描述基准。

    

    快速发展的多模态大型语言模型（LLMs）迫切需要新的基准来统一评估它们在理解和以文字描述音乐方面的性能。然而，由于音乐信息检索（MIR）算法与人类理解之间的语义差距，专业人士和公众之间的差异，以及注释的低精度，现有的音乐描述数据集无法作为基准。为此，我们提出了MuChin，这是第一个用中文口语描述的开源音乐描述基准，旨在评估多模态LLMs在理解和描述音乐方面的性能。我们建立了采虫音乐注释平台（CaiMAP），采用创新的多人、多阶段保证方法，并招募了业余爱好者和专业人士，以确保注释的精度和与流行语义的对齐。利用这种方法，我们构建了一个数据集。

    arXiv:2402.09871v1 Announce Type: cross  Abstract: The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset w
    
[^60]: 在嵌入式多核平台上表征 EEG 应用的准确性权衡

    Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs

    [https://arxiv.org/abs/2402.09867](https://arxiv.org/abs/2402.09867)

    该论文研究了在嵌入式多核平台上，采用电池供电的可穿戴设备分析脑电图（EEG）记录的应用。研究发现，通过调整近似方法，可以在有限的能量预算内实现更好的性能和能量收益。

    

    使用电池供电的可穿戴设备分析脑电图（EEG）记录，以监测脑活动和神经系统疾病。这些应用需要长时间连续处理以生成可行的结果。然而，可穿戴设备由于实际使用案例中的小尺寸而受限于有限的能量和计算资源。在限制的能量预算内，嵌入式异构多核平台（HMPs）可以提供更好的性能。可以进一步利用 EEG 应用程序流程的错误韧性来最大化 HMPs 的性能和能量收益。然而，在嵌入式 HMPs 上规范调整近似需要对准确性-性能-功耗权衡空间进行彻底探索。在这项工作中，我们对三种 EEG 应用（包括癫痫发作检测、睡眠阶段分类和压力检测）的错误韧性进行了表征。

    arXiv:2402.09867v1 Announce Type: cross  Abstract: Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders. These applications require long and continuous processing to generate feasible results. However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases. Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications. Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs. However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space. In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detecti
    
[^61]: 诸多才艺，其中一些是大师：一个多功能的转换器代理模型

    Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent

    [https://arxiv.org/abs/2402.09844](https://arxiv.org/abs/2402.09844)

    Jack of All Trades (JAT)是一个基于Transformer的模型，通过使用一个权重集，展现了在不同领域均能取得强大性能的能力。它是首个实现该目标的开放模型。

    

    在机器学习研究中，寻找一个能够在多个领域无缝运作的通用模型仍然是一个重要目标。在强化学习领域中，主流的方法往往将模型限制在单一任务和单模态框架中，这一限制与通用的、多领域模型的广阔愿景相矛盾。本文提出了Jack of All Trades (JAT) 一个基于Transformer的模型，其独特设计优化了处理顺序决策任务和多模态数据类型的能力。JAT模型通过使用一个权重集，在非常不同的强化学习基准测试上展现了其强大的性能和多样性，同时在计算机视觉和自然语言处理任务上展示了有希望的结果。JAT模型是朝着更通用、跨领域的AI模型设计迈出的重要一步，并且值得注意的是，它是首个完全开放的这一类型的模型。

    arXiv:2402.09844v1 Announce Type: new  Abstract: The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-s
    
[^62]: 超越模仿：通过大型语言模型的上下文感知推理生成人类移动性

    Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models

    [https://arxiv.org/abs/2402.09836](https://arxiv.org/abs/2402.09836)

    本论文提出了一种基于大型语言模型的上下文感知推理，将人类移动生成重新定义为常识推理问题。通过设计新颖的移动生成推理框架（MobiGeaR），将LLMs递归生成移动行为。

    

    人类移动行为与交通拥堵、疫情控制等重要社会问题密切相关。然而，收集移动性数据成本高昂且涉及严重的隐私问题，迫切需要高质量的生成性移动模型。先前的研究主要集中在从训练样本中学习行为分布，并通过采样学习到的分布生成新的移动数据。然而，它们不能有效地捕捉驱动移动行为的连贯意图，导致样本效率和语义感知度低。受到LLMs中新兴的推理能力的启发，我们提出了一种根本的视角转变，将移动生成重新定义为常识推理问题。在本文中，我们设计了一种新颖的移动生成推理（MobiGeaR）框架，促使LLMs递归生成移动行为。具体而言，我们设计了一个上下文感知的推理机制。

    arXiv:2402.09836v1 Announce Type: new  Abstract: Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aw
    
[^63]: 利用GAN进行欺诈检测：使用合成交易数据进行模型训练

    Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data

    [https://arxiv.org/abs/2402.09830](https://arxiv.org/abs/2402.09830)

    本论文研究了利用生成对抗网络（GAN）进行欺诈检测的应用，比较了其与传统方法的优势。通过构建对抗性验证图的集合，有效防止了由机器人或自动系统引起的欺诈，并确保交易中的用户是真实的。

    

    异常检测是各个研究领域中的一个重要挑战，旨在识别偏离正常数据分布的实例。本文探讨了在欺诈检测中应用生成对抗网络（GAN）并将其与传统方法进行了比较的优势。GAN是一种人工神经网络（ANN）的类型，在建模复杂数据分布方面表现出了希望，使其成为异常检测的有效工具。本文系统地描述了GAN及其衍生模型的原则，并强调了它们在不同数据集上的欺诈检测应用。通过构建对抗性验证图的集合，我们将有效防止由机器人或自动系统引起的欺诈，并确保交易中的用户是真实的。

    arXiv:2402.09830v1 Announce Type: cross  Abstract: Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods. GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection. The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative A
    
[^64]: 用深度学习增强金融行业的网络安全韧性，实现高级威胁检测

    Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection

    [https://arxiv.org/abs/2402.09820](https://arxiv.org/abs/2402.09820)

    这项研究提出使用深度学习来增强金融行业的网络安全韧性，并实现高级威胁检测。目前的网络威胁检测方法往往基于规则和传统的机器学习方法，无法适用大规模数据应用，并且无法有效检测未知威胁。

    

    在互联网时代，人们的生活越来越依赖于今天的网络技术。然而，网络技术是一把双刃剑，给人们带来便利的同时也带来了许多安全挑战。保持网络安全和保护用户的合法利益是网络建设的核心。威胁检测是一个完整有效的防御系统的重要组成部分。在网络信息安全领域，网络攻击和网络防护的技术更新日益迅猛。如何有效地检测未知威胁是网络防护的关注焦点之一。目前，网络威胁检测通常基于规则和传统的机器学习方法，这些方法创建人工规则或提取常见的时空特征，不能应用于大规模数据应用，并且未知威胁的出现导致了系统的检测准确性降低。

    arXiv:2402.09820v1 Announce Type: cross  Abstract: In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the or
    
[^65]: 一种利用同态加密和联邦学习的先进数据融合架构

    An advanced data fabric architecture leveraging homomorphic encryption and federated learning

    [https://arxiv.org/abs/2402.09795](https://arxiv.org/abs/2402.09795)

    该论文介绍了一种利用同态加密和联邦学习的先进数据融合架构，在不将数据移动到集中位置的情况下，实现了安全的医学图像分析。这种方法可以在保护数据隐私和安全性的同时，多个参与方进行机器学习模型的协作训练。

    

    数据融合是一种自动化和人工智能驱动的数据管理统一方法，旨在解决复杂的数据问题而无需将数据移动到集中位置。在联邦学习架构中，全局模型是基于多个本地模型的学习参数进行训练的，从而消除了将数据移动到集中存储库进行机器学习的必要性。本文介绍了一种安全的方法，使用联邦学习和部分同态加密在分布式数据融合架构中进行医学图像分析。通过该方法，多个参与方可以在训练机器学习模型时进行协作，而无需交换原始数据，而是使用学习或融合的特征。该方法符合HIPAA和GDPR等法律法规，确保数据的隐私和安全性。通过对垂体瘤分类的案例研究，证明了该方法的有效性。

    arXiv:2402.09795v1 Announce Type: cross  Abstract: Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning. This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture. With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features. The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data. The study demonstrates the method's effectiveness through a case study on pituitary tumor class
    
[^66]: 基于Charge Trap Flash（CTF）的非理想程序时间对深度神经网络的系统级影响

    System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network

    [https://arxiv.org/abs/2402.09792](https://arxiv.org/abs/2402.09792)

    这项研究提出了一种脉冲列设计补偿技术，以减少Charge Trap Flash（CTF）器件中非理想程序时间所引起的误差。

    

    利用电阻处理单元（Resistive Processing Unit, RPU）架构进行深度神经网络（DNN）的学习是一种能效高的做法，因为它利用了专用的神经形态硬件，并利用随机计算的权重更新进行内存计算。Charge Trap Flash（CTF）器件可以实现DNN中基于RPU的权重更新。然而，之前的研究表明，在CTF基于RPU的权重更新中，非理想程序时间（V_T）会受到影响。非理想程序时间受CTF的两个因素的影响：第一个是输入脉冲数量（N）或脉冲宽度（pw）的影响，第二个是用于随机计算权重更新的更新脉冲之间的间隔时间（t_gap）。因此，必须研究这种非理想程序时间对神经网络训练模拟的影响。本研究首先提出了一种脉冲列设计补偿技术，以减少CTF的非理想程序时间所引起的总误差。

    arXiv:2402.09792v1 Announce Type: cross  Abstract: Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU) architecture is energy-efficient as it utilizes dedicated neuromorphic hardware and stochastic computation of weight updates for in-memory computing. Charge Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs. However, prior work has shown that the weight updates (V_T) in CTF-based RPU are impacted by the non-ideal program time of CTF. The non-ideal program time is affected by two factors of CTF. Firstly, the effects of the number of input pulses (N) or pulse width (pw), and secondly, the gap between successive update pulses (t_gap) used for the stochastic computation of weight updates. Therefore, the impact of this non-ideal program time must be studied for neural network training simulations. In this study, Firstly, we propose a pulse-train design compensation technique to reduce the total error caused by non-ideal program time of CTF and
    
[^67]: 检查生成对抗网络判别器中的病态偏见：以StyleGAN3模型为例的案例研究

    Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model

    [https://arxiv.org/abs/2402.09786](https://arxiv.org/abs/2402.09786)

    这项研究发现了StyleGAN3模型中判别器的病态偏见，它在图像和面部质量上的得分分层影响了不同性别、种族和其他类别的图像。

    

    生成对抗网络可以生成逼真的人脸，往往难以被人类区分出来。我们发现预训练的StyleGAN3模型中的判别器在图像和面部质量上系统地对得分进行分层，并且这不成比例地影响了不同性别、种族和其他类别的图像。我们检查了判别器在色彩和亮度方面对感知的种族和性别的偏见，然后检查了社会心理学中关于刻板印象研究中常见的偏见。

    arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
    
[^68]: 基于对比学习和自注意力的时间接近度上的顺序推荐

    Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention

    [https://arxiv.org/abs/2402.09784](https://arxiv.org/abs/2402.09784)

    该论文基于对比学习和自注意力机制，提出了一种考虑垂直和水平时间接近度的顺序推荐方法，以更好地捕捉用户-项目交互中的时间上下文。

    

    传统的基于深度学习和最新的基于Transformer的模型在先前的研究中捕捉了用户-项目交互中的单向和双向模式，但对于时间上下文的重要性，如个体行为和社会趋势模式，仍未得到很好的探索。最近的模型通常忽略了在类似的时间段内隐含在用户之间发生的用户行为的相似性，我们将其称为垂直时间接近度。这些模型主要通过适应Transformer的自注意机制来考虑用户行为中的时间上下文。同时，这种适应在考虑项目交互中的水平时间接近度方面仍然有限，例如区分在一周内与一个月内购买的连续项目。

    arXiv:2402.09784v1 Announce Type: cross  Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a mon
    
[^69]: MC-DBN：基于深度信念网络的模态补全模型

    MC-DBN: A Deep Belief Network-Based Model for Modality Completion

    [https://arxiv.org/abs/2402.09782](https://arxiv.org/abs/2402.09782)

    MC-DBN是一种基于深度信念网络的模态补全模型，利用完整数据的隐式特征来弥补附加不完整数据的差距，提高预测准确性。

    

    最近多模态人工智能（AI）的进展已经彻底改变了股市预测和心率监测领域。利用多样的数据源可以大大提高预测准确性。然而，额外的数据可能不总是与原始数据集相吻合。插值方法通常用于处理模态数据中的缺失值，但在稀疏信息情况下可能存在一些限制。为解决这一挑战，我们提出了一种模态补全的深度信念网络模型（MC-DBN）。该方法利用完整数据的隐式特征来弥补自身与附加不完整数据之间的差距。它确保增强的多模态数据与现实世界的动态特性密切相符，以提高模型的有效性。我们在两个来自股市预测和心率监测的数据集上对MC-DBN模型进行了评估。

    arXiv:2402.09782v1 Announce Type: cross  Abstract: Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate
    
[^70]: 使用单次前向传递的表示学习

    Representation Learning Using a Single Forward Pass

    [https://arxiv.org/abs/2402.09769](https://arxiv.org/abs/2402.09769)

    我们提出了一种神经科学启发的算法，可以通过单次前向传递进行表示学习。该算法具有独特的特点，并在不需要反向传播的情况下取得了高性能的分类结果。

    

    我们提出了一种受神经科学启发的单次传递嵌入学习算法（SPELA）。 SPELA是在边缘人工智能设备中进行训练和推理应用的首选候选人。 同时，SPELA可以最佳地满足对研究感知表示学习和形成框架的需求。 SPELA具有独特的特征，如嵌入向量形式的神经先验知识，不需要权重传输，不锁定权重更新，完全局部赫比安学习，不存储激活的单次前向传递和每个样本的单次权重更新。与传统方法相比，SPELA可以在不需要反向传播的情况下进行操作。 我们展示了我们的算法在一个有噪音的布尔运算数据集上可以执行非线性分类。 此外，我们展示了SPELA在MNIST，KMNIST和Fashion MNIST上的高性能表现。 最后，我们展示了SPELA在MNIST，KMNIST和Fashion MNIST上的少样本和1个时期学习能力。

    arXiv:2402.09769v1 Announce Type: new  Abstract: We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm (SPELA). SPELA is a prime candidate for training and inference applications in Edge AI devices. At the same time, SPELA can optimally cater to the need for a framework to study perceptual representation learning and formation. SPELA has distinctive features such as neural priors (in the form of embedded vectors), no weight transport, no update locking of weights, complete local Hebbian learning, single forward pass with no storage of activations, and single weight update per sample. Juxtaposed with traditional approaches, SPELA operates without the need for backpropagation. We show that our algorithm can perform nonlinear classification on a noisy boolean operation dataset. Additionally, we exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST. Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on MNIST, KMNIST, and Fashio
    
[^71]: 从变动性到稳定性：推荐系统基准化实践的进展

    From Variability to Stability: Advancing RecSys Benchmarking Practices

    [https://arxiv.org/abs/2402.09766](https://arxiv.org/abs/2402.09766)

    本论文提出了一种新的基准测试方法，通过使用多样化的开放数据集，并在多个度量指标上评估多种协同过滤算法，来研究数据集特征对算法性能的影响。这一方法填补了推荐系统算法比较中的不足之处，推进了评估实践。

    

    在快速发展的推荐系统领域中，新的算法经常通过对一组有限的任意选择的数据集进行评估来声称自己具有最先进的性能。然而，由于数据集特征对算法性能有重大影响，这种方法可能无法全面反映它们的有效性。为了解决这个问题，本文引入了一种新的基准测试方法，以促进公平和稳健的推荐系统算法比较，从而推进评估实践。通过利用包括本文介绍的两个数据集在内的30个开放数据集，并在9个度量指标上评估11种协同过滤算法，我们对数据集特征对算法性能的影响进行了重要的研究。我们进一步研究了将多个数据集的结果聚合成一个统一排名的可行性。通过严格的实验分析，我们发现......

    arXiv:2402.09766v1 Announce Type: cross  Abstract: In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, 
    
[^72]: 强化学习用于解决具有时间窗口的随机车辆路径问题的研究

    Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows

    [https://arxiv.org/abs/2402.09765](https://arxiv.org/abs/2402.09765)

    本文介绍了一种利用强化学习来优化具有时间窗口的随机车辆路径问题的方法，填补了SVRP研究中的空白，并通过使用注意力机制神经网络最小化了路径成本，模型在旅行成本上优于传统的蚁群算法，并且在不同环境中表现出鲁棒性。

    

    本文介绍了一种利用强化学习来优化具有时间窗口的随机车辆路径问题（SVRP）的方法，重点是减少货物配送中的旅行成本。我们提出了一种新颖的SVRP模型，考虑了不确定的旅行成本和需求，同时考虑了具体的客户的时间窗口。通过强化学习训练的注意力机制神经网络被用来最小化路径成本。我们的方法填补了SVRP研究中的空白，传统上依赖于启发式方法，而是利用机器学习。该模型在旅行成本上胜过了蚁群算法，实现了1.73%的降低。它独特地整合了外部信息，在不同环境中表现出鲁棒性，成为未来SVRP研究和行业应用的有价值的基准。

    arXiv:2402.09765v1 Announce Type: new  Abstract: This paper introduces a reinforcement learning approach to optimize the Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on reducing travel costs in goods delivery. We develop a novel SVRP formulation that accounts for uncertain travel costs and demands, alongside specific customer time windows. An attention-based neural network trained through reinforcement learning is employed to minimize routing costs. Our approach addresses a gap in SVRP research, which traditionally relies on heuristic methods, by leveraging machine learning. The model outperforms the Ant-Colony Optimization algorithm, achieving a 1.73% reduction in travel costs. It uniquely integrates external information, demonstrating robustness in diverse environments, making it a valuable benchmark for future SVRP studies and industry application.
    
[^73]: 通过分布偏好奖励建模对齐众包反馈

    Aligning Crowd Feedback via Distributional Preference Reward Modeling

    [https://arxiv.org/abs/2402.09764](https://arxiv.org/abs/2402.09764)

    本文提出了一种名为分布偏好奖励模型的框架，用于将大型语言模型与多样的人类偏好对齐。该框架使用贝塔分布刻画偏好，并设计了基于最优输运的损失函数来校准模型与偏好的对齐程度。最终利用期望奖励微调语言模型的策略。

    

    深度强化学习广泛用于将大型语言模型与人类偏好对齐。然而，传统的奖励建模主要依赖于一组个体提供的人类标注。这种依赖可能会导致模型倾向于反映这些标注者的倾向，从而未能充分代表更广泛人群的期望。本文介绍了一种简单而有效的框架——分布偏好奖励模型(DPRM)，以将大型语言模型与多样的人类偏好对齐。为此，我们使用贝塔分布来刻画偏好，该分布能够动态适应偏好趋势的波动。在此基础上，我们设计了基于最优输运的损失函数，以校准DPRM与偏好分布的对齐度。最后，利用期望奖励来微调语言模型的策略。

    arXiv:2402.09764v1 Announce Type: new  Abstract: Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM polic
    
[^74]: 无块语境检索的语言模型 grounding

    Grounding Language Model with Chunking-Free In-Context Retrieval

    [https://arxiv.org/abs/2402.09760](https://arxiv.org/abs/2402.09760)

    这是一种针对检索增强生成系统（RAG）的无块语境检索方法，通过绕过文本切分的过程，利用编码隐藏状态进行准确地语境检索，解决了传统方法中存在的文本语义连贯性破坏和证据检索中的噪声和不准确性问题。

    

    本论文介绍了一种针对检索增强生成（RAG）系统的无块语境（CFIC）检索方法。传统的RAG系统在使用精确证据文本进行 grounding 时往往面临处理冗长文档和过滤无关内容的挑战。常用的解决方案，如文档切分和调整语言模型以处理更长的上下文，都存在局限性。这些方法要么破坏了文本的语义连贯性，要么未能有效解决证据检索中的噪声和不准确性问题。CFIC通过绕过传统的切分过程来应对这些挑战。它利用文档的编码隐藏状态进行语境检索，在对用户查询进行自回归解码时准确地识别出所需的具体证据文本，消除了切分的需求。CFIC 进一步。。。

    arXiv:2402.09760v1 Announce Type: cross  Abstract: This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.   CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further
    
[^75]: 高效的语言自适应预训练：扩展最新的大规模语言模型用于波兰语

    Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish

    [https://arxiv.org/abs/2402.09759](https://arxiv.org/abs/2402.09759)

    本研究使用高效的语言自适应预训练方法，成功将基础英文大规模语言模型应用于生成波兰文，并在困惑度和任务表现上取得了显著的改进，为向现有语言模型添加新语言开辟了新途径。

    

    本研究探讨了将基础英文大规模语言模型（LLMs）微调为生成波兰文的潜力。首先，通过对3.11 GB高质量数据集进行语言自适应预训练（LAPT），该数据集包含2.76亿个波兰语tokens。LAPT后进行了额外的微调，旨在解决九个KLEJ挑战。我们训练的模型Curie-7B-v1不仅在基于解码器的波兰模型中具有最低的困惑度3.02，而且在8个任务中与最好的波兰编码器-解码器模型之间的差距不到2%。Curie-7B-v1仅使用典型数据集大小的约2-3%来学习波兰语。LAPT在不到五天的时间内使用普通GPU完成，凸显了该方法的高效性。模型在波兰语方面的熟练度显著提高，证明了该方法在将新语言添加到现有LLMs中的可行性。

    arXiv:2402.09759v1 Announce Type: cross  Abstract: This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs
    
[^76]: 探索大型语言模型在艺术创作中的潜力：艺术家与人工智能合作中的创意编程和反思

    Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming

    [https://arxiv.org/abs/2402.09750](https://arxiv.org/abs/2402.09750)

    这项研究探索了大型语言模型在艺术家与人工智能合作的创意编程中的艺术潜力，并比较了两种合作方式。研究发现反思类型与用户表现、用户满意度和主观体验相关。通过实验数据和定性访谈，我们从艺术家的角度提供了人工智能合作的批判性视角和设计建议。

    

    最近，大型语言模型（LLMs）在辅助编程方面的潜力被广泛使用。然而，当前的研究没有探索LLMs在艺术家与人工智能合作的创造性编程中的艺术潜力。我们的工作探索了在这种合作过程中艺术家的反思类型。我们比较了两种常见的合作方式：调用整个程序和多个子任务。我们的研究结果展示了艺术家在两种不同方法中不同的刺激性反思。我们的发现还显示了反思类型与用户表现、用户满意度和主观体验之间的相关性，通过进行两种方法，包括实验数据和定性访谈。在这个意义上，我们的工作揭示了LLM在创意编程中的艺术潜力。同时，我们从艺术家的角度提供了人工智能合作的批判性视角，并阐述了设计建议。

    arXiv:2402.09750v1 Announce Type: cross  Abstract: Recently, the potential of large language models (LLMs) has been widely used in assisting programming. However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists' different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of LLM in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design sugg
    
[^77]: 大规模语言模型的模型压缩和高效推理：一项综述

    Model Compression and Efficient Inference for Large Language Models: A Survey

    [https://arxiv.org/abs/2402.09748](https://arxiv.org/abs/2402.09748)

    这项综述研究了大规模语言模型的压缩和高效推理方法，包括量化、修剪、蒸馏、紧凑架构设计和动态网络等方面。大模型的突出特点是压缩后需要微调或重新训练，并且相关的成本很高。

    

    基于Transformer的大规模语言模型取得了巨大的成功。然而，在推理过程中所产生的显著的内存和计算成本使得在资源受限设备上部署大模型变得具有挑战性。本文从算法的角度探讨了大规模语言模型的压缩和高效推理方法。在分类方面，与较小的模型类似，用于大规模语言模型的压缩和加速算法仍可以分为量化、修剪、蒸馏、紧凑架构设计和动态网络。然而，与较小的模型相比，大规模语言模型有两个突出的特点：（1）大多数压缩算法在压缩后需要微调甚至重新训练模型。大模型最显著的方面是与模型微调或训练相关的非常高的成本。因此，许多针对大规模模型的算法都需要考虑这一点。

    arXiv:2402.09748v1 Announce Type: cross  Abstract: Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large mode
    
[^78]: 代理人不需要知道他们的目的

    Agents Need Not Know Their Purpose

    [https://arxiv.org/abs/2402.09734](https://arxiv.org/abs/2402.09734)

    本文研究了无意识代理人，他们的行为与人类价值观一致的挑战。他们的有效效用函数是已知和隐藏子函数的聚合，通过限制架构，实现对隐藏子函数的最大化和最小化。

    

    确保人工智能的行为与人类价值观一致被称为对齐挑战。之前的研究表明，理性的代理人会以最大化效用函数的方式行事，这导致他们的行为与人类价值观不一致，特别是当他们的智能水平提高时。之前的研究还表明，没有“一个真正的效用函数”，解决方案必须包含更全面的对齐方法。本文描述了无意识代理人：代理人的结构使他们的有效效用函数是已知和隐藏子函数的聚合。隐藏的组成部分是需要最大化的，它被内部实现为一个黑盒子，阻止代理人检查它。已知的组成部分是需要最小化的，即对隐藏子函数的知识。架构限制进一步影响代理人行动的演化。

    arXiv:2402.09734v1 Announce Type: new  Abstract: Ensuring artificial intelligence behaves in such a way that is aligned with human values is commonly referred to as the alignment challenge. Prior work has shown that rational agents, behaving in such a way that maximizes a utility function, will inevitably behave in such a way that is not aligned with human values, especially as their level of intelligence goes up. Prior work has also shown that there is no "one true utility function"; solutions must include a more holistic approach to alignment. This paper describes oblivious agents: agents that are architected in such a way that their effective utility function is an aggregation of a known and hidden sub-functions. The hidden component, to be maximized, is internally implemented as a black box, preventing the agent from examining it. The known component, to be minimized, is knowledge of the hidden sub-function. Architectural constraints further influence how agent actions can evolve i
    
[^79]: 基于Prompt的联邦决策Transformer用于移动边缘计算系统中的定制化VR服务

    Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System

    [https://arxiv.org/abs/2402.09729](https://arxiv.org/abs/2402.09729)

    本文研究了在移动边缘计算系统中为异构用户提供定制化VR服务的资源分配问题，提出了一种基于联邦学习和基于prompt的序列建模的框架，名为FedPromptDT，在保证用户体验最高的同时解决了数据不足和用户隐私保护的问题。

    

    这篇论文研究了如何在移动边缘计算系统中为异构用户提供定制化的虚拟现实（VR）服务的资源分配问题。首先引入了一种体验质量（QoE）度量方法来衡量用户体验，考虑了移动边缘计算系统的延迟、用户注意力水平和首选分辨率。然后，建立了一个QoE最大化问题以保证最高可能的用户体验，将其作为一个强化学习问题来解决，旨在学习一个适用于所有MEC服务器的各种用户环境的通用策略。为了学习这个通用策略，我们提出了一个使用联邦学习（FL）和基于prompt的序列建模来预训练MEC服务器之间共同决策模型的框架，即FedPromptDT。使用FL解决了本地MEC数据不足的问题，同时在离线训练过程中保护用户的隐私。

    arXiv:2402.09729v1 Announce Type: new  Abstract: This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system. We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions. Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers. To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT. Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training. The design of p
    
[^80]: 滥用生成型AI聊天机器人以创建短信钓鱼攻击

    AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns

    [https://arxiv.org/abs/2402.09728](https://arxiv.org/abs/2402.09728)

    该论文介绍了滥用生成型AI聊天机器人创建短信钓鱼攻击的方法，展示了它们在真实世界中的潜在危害。

    

    SMS钓鱼，也被称为“smishing”，是一种通过欺诈性移动短信诱骗用户透露私人信息或点击带有恶意内容的URL的不断增长的威胁。最近，我们还观察到了会话生成型AI聊天机器人服务（如OpenAI的ChatGPT，Google的BARD）的快速发展，它们由预训练的大型语言模型（LLM）驱动。这些AI聊天机器人确实具有很多用途，但尚未系统地了解它们如何在创建威胁和攻击方面发挥作用。在本文中，我们提出了滥用GPT的方法，展示了现有的生成型AI聊天机器人服务如何被攻击者在真实世界中利用，创建短信钓鱼文本，最终导致更聪明的短信钓鱼攻击活动。据我们所知，目前尚无现有的工作明确显示这些生成型文本模型在创建SMS钓鱼方面的影响。

    arXiv:2402.09728v1 Announce Type: cross  Abstract: SMS phishing, also known as "smishing", is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages. In recent past, we have also observed a rapid advancement of conversational generative AI chatbot services (e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large language models (LLMs). These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks. In this paper, we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns. To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing. Thus, 
    
[^81]: 一种具有长期上下文概要记忆的人工智能阅读代理

    A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts

    [https://arxiv.org/abs/2402.09727](https://arxiv.org/abs/2402.09727)

    ReadAgent是一个具有长期上下文概要记忆的阅读代理系统，通过实现一个简单的提示系统，它能够处理长输入并提高有效上下文长度。在评估中表现良好。

    

    当前的大型语言模型不仅限制在某个最大上下文长度内，而且无法稳定地处理长输入。为了解决这些限制，我们提出了ReadAgent，一个增加了有效上下文长度的语言模型代理系统，在我们的实验中可以达到20倍。受到人类交互式阅读长文档的启发，我们将ReadAgent实现为一个简单的提示系统，利用LLM的高级语言能力来：（1）决定将哪些内容存储在一个记忆片段中，（2）将这些记忆片段压缩成为称为概要记忆的短时记忆，（3）在需要时通过原始文本查找段落来提醒自己相关细节以完成任务。我们使用检索方法、使用原始长上下文以及使用概要记忆来评估ReadAgent与基线的性能。这些评估是在三个长文档阅读理解任务上进行的。

    arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
    
[^82]: 通过错误暴露和一致性正则化改进非自回归机器翻译

    Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization

    [https://arxiv.org/abs/2402.09725](https://arxiv.org/abs/2402.09725)

    本论文提出使用错误暴露和一致性正则化的训练方法来改进非自回归机器翻译中的数据分布不一致问题，并取得了显著的BLEU得分提升。

    

    作为IR-NAT（基于迭代改进的NAT）框架之一，条件掩码语言模型（CMLM）采用掩码预测范式来重新预测掩码低置信度的标记。然而，CMLM在训练和推理之间存在数据分布不一致的问题，观察到的标记在这两种情况下生成方式不同。本文提出使用错误暴露和一致性正则化（EECR）的训练方法来解决这个问题。我们在训练过程中基于模型预测构建混合序列，并提出在不完美观测条件下针对掩码标记进行优化。我们还设计了一种一致性学习方法，以限制不同观测情况下掩码标记的数据分布，缩小训练和推理之间的差距。在五个翻译基准上的实验证明，平均改进了0.68和0.40的BLEU得分。

    arXiv:2402.09725v1 Announce Type: cross  Abstract: Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores c
    
[^83]: 有限预算下的迅速学习最佳臂识别

    Best Arm Identification for Prompt Learning under a Limited Budget

    [https://arxiv.org/abs/2402.09723](https://arxiv.org/abs/2402.09723)

    这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。

    

    大型语言模型（LLMs）的显著指令跟随能力引发了对自动学习合适提示的兴趣。然而，虽然提出了许多有效的方法，但在学习过程中产生的成本（例如访问LLM和评估响应）尚未得到考虑。为克服这个限制，本工作在提示学习中明确引入了有限预算约束。为了开发有原则的解决方案，本研究在提示学习和多臂赌博机的固定预算最佳臂识别（BAI-FB）之间建立了一种新的联系。基于这种联系，提出了一个通用框架TRIPLE（用于提示学习的最佳臂识别），以系统地利用BAI-FB在提示学习中的力量。提示学习的独特特点进一步通过利用聚类和嵌入思想提出了TRIPLE的两个基于嵌入的增强方法。

    arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
    
[^84]: Reg-NF: 高效的神经场隐式表面注册

    Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields

    [https://arxiv.org/abs/2402.09722](https://arxiv.org/abs/2402.09722)

    Reg-NF是一种高效的神经场隐式表面注册方法，能够优化两个神经场之间的相对6自由度变换，并在机器人应用中显示出良好的效果。

    

    神经场是一种基于坐标的神经网络，最近在表示场景方面变得流行起来。与基于显式表示（如点云）的经典方法不同，神经场提供了一种连续的场景表示，能够紧凑地表示3D几何和外观，非常适合机器人应用。然而，有限的先前方法没有直接利用这些连续的隐式表示来注册多个神经场。在本文中，我们提出了Reg-NF，一种基于神经场的注册方法，优化两个任意神经场之间的相对6自由度变换，即使这两个场具有不同的尺度因子。Reg-NF的关键组件包括双向注册损失、多视角表面采样和体积有符号距离函数（SDF）的利用。我们在一个新的神经场数据集上展示了我们的方法。

    arXiv:2402.09722v1 Announce Type: cross  Abstract: Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural fiel
    
[^85]: 说服一位学习代理

    Persuading a Learning Agent

    [https://arxiv.org/abs/2402.09721](https://arxiv.org/abs/2402.09721)

    在一个重复的贝叶斯说服问题中，即使没有承诺能力，委托人可以通过使用上下文无遗憾学习算法来实现与经典无学习模型中具有承诺的委托人的最优效用无限接近的效果；在代理人使用上下文无交换遗憾学习算法的情况下，委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。

    

    我们研究了一个重复的贝叶斯说服问题（更一般地，任何具有完全信息的广义委托-代理问题），其中委托人没有承诺能力，代理人使用算法来学习如何对委托人的信号做出响应。我们将这个问题简化为一个一次性的广义委托-代理问题，代理人近似地最佳响应。通过这个简化，我们可以证明：如果代理人使用上下文无遗憾学习算法，则委托人可以保证其效用与经典无学习模型中具有承诺的委托人的最优效用之间可以无限接近；如果代理人使用上下文无交换遗憾学习算法，则委托人无法获得比具有承诺的无学习模型中的最优效用更高的效用。委托人在学习模型与非学习模型中可以获得的效用之间的差距是有界的。

    arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
    
[^86]: 使用交叉注意力作为归纳偏置的扩散模型用于解缠表示学习

    Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement

    [https://arxiv.org/abs/2402.09712](https://arxiv.org/abs/2402.09712)

    本文介绍了一种新的观点和框架，证明了具有交叉注意力的扩散模型可以作为强大的归纳偏置，促进解缠表示的学习。

    

    解缠表示学习致力于提取观测数据中的内在因素。以无监督方式因式分解这些表示通常具有挑战性，并且通常需要定制的损失函数或特定结构设计。本文引入了一个新的观点和框架，证明了具有交叉注意力的扩散模型可以作为强大的归纳偏置，促进解缠表示的学习。我们提出将图像编码为一组概念令牌，并将它们视为图像重构的潜在扩散的条件，其中通过概念令牌的交叉注意力用于连接编码器和扩散之间的交互。在基准数据集上，该框架无需任何额外的正则化就能达到更优秀的解缠性能，超越了所有先前设计复杂的方法。

    arXiv:2402.09712v1 Announce Type: cross  Abstract: Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive abl
    
[^87]: 对离线强化学习中的奖励污染攻击的研究

    Reward Poisoning Attack Against Offline Reinforcement Learning

    [https://arxiv.org/abs/2402.09695](https://arxiv.org/abs/2402.09695)

    这项研究针对深度神经网络函数逼近的一般离线强化学习中的奖励污染攻击问题，提出了一种名为“策略对比攻击”的攻击策略。通过使低性能策略看起来像是高性能的，同时使高性能策略看起来像是低性能的，我们证明了这种攻击有效性。

    

    我们研究了针对深度神经网络函数逼近的一般离线强化学习中的奖励污染攻击问题。我们考虑了一个黑盒威胁模型，攻击者对学习算法完全不了解，并且其预算受到限制，限制了每个数据点的污染量以及总扰动。我们提出了一种名为“策略对比攻击”的攻击策略。其高层思想是使一些低性能策略看起来像是高性能的，同时使高性能策略看起来像是低性能的。据我们所知，我们首次提出了一种适用于一般离线强化学习场景的黑盒奖励污染攻击。我们提供了关于攻击设计的理论洞察，并在不同类型的学习数据集上经验证明我们的攻击对当前最先进的离线强化学习算法是有效的。

    arXiv:2402.09695v1 Announce Type: cross  Abstract: We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation. We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation. We propose an attack strategy called `policy contrast attack'. The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing. To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting. We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets.
    
[^88]: 探索人机交互中的“积极摩擦”行为模型

    Exploring a Behavioral Model of "Positive Friction" in Human-AI Interaction

    [https://arxiv.org/abs/2402.09683](https://arxiv.org/abs/2402.09683)

    本文探讨了在人机交互中积极摩擦的行为模型，即通过有意的延迟等方式，增加用户反思，阻止自动或有偏见行为，并提高意外发现的机会。

    

    设计无缝、无摩擦的用户体验一直是应用行为科学和人工智能领域的主要趋势，通过减少用户体验中的摩擦力来实现希望的行为容易和高效。然而，在某些情况下，摩擦力确实有益，如在插入有意的延迟以增加反思、防止个体自动或有偏见行为、增强意外发现机会等。最近，人工智能的普及和可用性的增加只增加了研究摩擦力如何帮助或妨碍人工智能用户的需要；这还意味着需要考虑积极摩擦力如何使人工智能从业者受益，无论是在开发过程中（例如与多样化的团队合作）还是在人工智能产品设计中。本文首先提出了一个“积极摩擦”

    arXiv:2402.09683v1 Announce Type: cross  Abstract: Designing seamless, frictionless user experiences has long been a dominant trend in both applied behavioral science and artificial intelligence (AI), in which the goal of making desirable actions easy and efficient informs efforts to minimize friction in user experiences. However, in some settings, friction can be genuinely beneficial, such as the insertion of deliberate delays to increase reflection, preventing individuals from resorting to automatic or biased behaviors, and enhancing opportunities for unexpected discoveries. More recently, the popularization and availability of AI on a widespread scale has only increased the need to examine how friction can help or hinder users of AI; it also suggests a need to consider how positive friction can benefit AI practitioners, both during development processes (e.g., working with diverse teams) and to inform how AI is designed into offerings. This paper first proposes a "positive friction"
    
[^89]: PAL：对大型语言模型的代理引导黑盒攻击

    PAL: Proxy-Guided Black-Box Attack on Large Language Models

    [https://arxiv.org/abs/2402.09674](https://arxiv.org/abs/2402.09674)

    PAL是第一个黑盒查询攻击大型语言模型的优化算法，通过代理模型引导优化过程，并使用复杂的损失函数，取得了较高的攻击成功率。

    

    大型语言模型（LLMs）近几个月来越来越受欢迎，但在被操纵时它们展示出的危险能力令人担忧。尽管安全微调等技术旨在最小化有害使用，但最近的研究表明，LLMs仍然容易受到引发有毒回应的攻击。在这项工作中，我们引入了对LLMs的代理引导攻击（PAL），这是第一个基于优化的对LLMs的黑盒仅查询攻击。具体而言，它依赖于一个替代模型来引导优化过程，并采用了针对真实世界LLM API设计的复杂损失函数。我们的攻击在GPT-3.5-Turbo上达到84%的攻击成功率（ASR），在Llama-2-7B上达到48%，而目前最先进的方法仅为4%。我们还提出了GCG++，这是对GCG攻击的改进，在白盒Llama-2-7B上达到了94%的ASR，以及基于查询的攻击的强有力但简单的基准方法——LLMs上的随机搜索攻击（RAL）。

    arXiv:2402.09674v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We
    
[^90]: 如何训练数据高效的LLM模型

    How to Train Data-Efficient LLMs

    [https://arxiv.org/abs/2402.09668](https://arxiv.org/abs/2402.09668)

    本文研究了如何训练数据高效的LLM模型，提出了Ask-LLM和Density两种优秀的数据选择方法。

    

    大型语言模型（LLM）的训练十分昂贵。本文研究了用于预训练LLM的数据高效方法，即旨在优化模型质量和训练资源/数据消耗的帕累托前沿的技术。我们试图理解基于（i）昂贵的数据质量估计和（ii）基于特征空间的覆盖率和多样性测量的数据选择程序所带来的权衡。我们的第一种技术“Ask-LLM”利用调节指令的LLM的零样本推理能力来直接评估训练样例的质量。为了达到覆盖率，我们提出了密度采样，它根据数据分布选择多样的样本。在我们对19种采样器进行了数百个评估任务和预训练运行的对比研究中，我们发现Ask-LLM和Density是各自类别中最好的方法。

    arXiv:2402.09668v1 Announce Type: cross  Abstract: The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover th
    
[^91]: CodeMind:一个用于挑战大型语言模型进行代码推理的框架

    CodeMind: A Framework to Challenge Large Language Models for Code Reasoning

    [https://arxiv.org/abs/2402.09664](https://arxiv.org/abs/2402.09664)

    CodeMind是一个用于挑战大型语言模型进行代码推理的框架，通过评估LLMs的代码推理能力来替代仅仅依靠测试通过来评估，对三种代码推理任务进行评估，结果显示LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    

    仅靠测试通过来评估大型语言模型（LLMs）的代码合成能力可能会导致不公正的评估或促进具有数据泄漏的模型，作为一种替代方案，我们介绍了CodeMind，这是一个旨在评估LLMs的代码推理能力的框架。CodeMind目前支持三种代码推理任务：独立执行推理（IER）、依赖执行推理（DER）和规范推理（SR）。前两者评估模型以预测任意代码的执行输出，或者模型能够正确合成的代码。第三个任务评估LLMs实现指定预期行为的程度。我们使用CodeMind对两种不同编程语言中的五个基准下的九个LLMs进行了广泛的评估，结果表明LLMs能够公正地理解控制流结构，并且对于简单程序和复杂程序，它们通常能够推理出输入如何演变为输出。

    arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
    
[^92]: 用户建模与用户画像：综述

    User Modeling and User Profiling: A Comprehensive Survey

    [https://arxiv.org/abs/2402.09660](https://arxiv.org/abs/2402.09660)

    这篇综述论文介绍了用户建模与用户画像研究的现状、发展和未来方向。该研究主要关注在人工智能应用中构建准确的用户表示，包括利用大量数据进行建模以及采用深度学习和图数据技术等先进方法。

    

    人工智能（AI）融入日常生活，特别是通过信息检索和推荐系统，已经促使先进的用户建模和用户画像技术，以提供个性化体验。这些技术旨在基于与这些系统的互动中生成的大量数据构建准确的用户表示。本文对用户建模和用户画像研究的现状、发展和未来方向进行了全面综述。我们提供了一个历史概述，追溯了从早期的刻板模型到最新的深度学习技术，并提出了一个新的分类体系，涵盖了这一研究领域中的所有活动主题，包括最近的趋势。我们的综述突出了向更复杂的用户画像方法的范式转变，强调了隐式数据收集、多行为建模以及图数据的整合。

    arXiv:2402.09660v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data
    
[^93]: 模型编辑的蝴蝶效应：少量编辑可能引发大规模语言模型崩溃

    The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse

    [https://arxiv.org/abs/2402.09656](https://arxiv.org/abs/2402.09656)

    尽管模型编辑在大型语言模型中显示出修订知识的潜力，但少量编辑可以触发模型崩溃，导致性能显著下降。我们提出使用困惑度作为替代指标，并通过实验证实其与下游任务性能的强相关性。

    

    虽然模型编辑已显示出在大型语言模型（LLMs）中修订知识的潜力，但其对LLMs的内在能力的影响常常被忽视。在这项工作中，我们揭示了一个关键现象：即使只进行一个编辑，也可以引发模型崩溃，表现为各种基准任务中性能显著下降。然而，在每次编辑后对LLMs进行基准测试虽然必要，但耗时且资源密集。为了缓解这个问题，我们提出使用困惑度作为替代指标，并通过大量实验证实其与下游任务性能的强相关性。我们还对顺序编辑进行了深入研究，这是实际场景中的一种常见情况，涵盖了来自我们之前单次编辑研究中的困难案例。结果表明，几乎所有研究的编辑方法都导致模型崩溃。

    arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse
    
[^94]: GPT-4在基于USMLE的案例研究中的表现评估

    GPT-4's assessment of its performance in a USMLE-based case study

    [https://arxiv.org/abs/2402.09654](https://arxiv.org/abs/2402.09654)

    本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。

    

    本研究调查GPT-4在医疗应用中的表现评估。通过使用简单的提示技术，从美国医学执照考试（USMLE）问卷中提取问题的方式，任务是评估模型在提问之前和提问之后的置信度得分。问卷根据是否有反馈分为两组：反馈组（WF）和无反馈组（NF）。要求模型在每个问题之前和之后提供绝对和相对置信度得分。通过使用统计工具分析实验结果，研究了WF和NF组的置信度变异性。此外，进行了顺序分析以观察WF和NF组的性能变化。结果表明，反馈会影响相对置信度，但并不总是增加或减少。

    arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
    
[^95]: ProtChatGPT：用于理解大规模语言模型的蛋白质

    ProtChatGPT: Towards Understanding Proteins with Large Language Models

    [https://arxiv.org/abs/2402.09649](https://arxiv.org/abs/2402.09649)

    ProtChatGPT是一个基于大型语言模型的系统，通过自然语言学习和理解蛋白质结构，为用户提供上传蛋白质、提问和交互式对话等功能，有助于进一步理解蛋白质的结构与功能关系。

    

    蛋白质研究在各个基础学科中至关重要，但理解其复杂的结构与功能关系仍然具有挑战性。最近的大型语言模型（LLMs）在理解特定任务的知识方面取得了重大进展，这表明了用于蛋白质的ChatGPT-like系统在促进基础研究方面的潜力。在这项工作中，我们介绍了ProtChatGPT，旨在通过自然语言学习和理解蛋白质结构。ProtChatGPT使用户可以上传蛋白质、提问并进行交互式对话以产生全面的回答。该系统包括蛋白编码器、蛋白语言相关转换器（PLP-former）、投影适配器和LLM。蛋白质首先通过蛋白编码器和PLP-former进行编码以产生蛋白质嵌入，然后通过适配器将其投射到与LLM相符合。最后，LLM将用户的问题与蛋白质嵌入进行综合处理。

    arXiv:2402.09649v1 Announce Type: cross  Abstract: Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions wit
    
[^96]: 增强LLM用户-物品交互：利用边缘信息进行优化推荐的研究

    LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations

    [https://arxiv.org/abs/2402.09617](https://arxiv.org/abs/2402.09617)

    这项研究旨在提高LLM在图数据中的关系挖掘效率和能力，通过整合图神经网络和大型语言模型，以利用边缘信息来理解复杂节点关系，并从图结构中提取有意义洞见。

    

    大型语言模型的出色性能不仅改变了自然语言处理领域的研究格局，还展示了它在各个领域的卓越应用潜力。然而，这些模型在挖掘图数据中的关系方面的潜力仍未得到充分探索。图神经网络作为近年来热门的研究领域，在关系挖掘方面有大量研究。然而，当前图神经网络的尖端研究尚未有效整合大型语言模型，导致在图关系挖掘任务中的效率和能力受限。一个主要的挑战是LLM无法深入利用图中的边缘信息，而这对于理解复杂节点关系至关重要。这种差距限制了LLM从图结构中提取有意义洞见的潜力，限制了它在更复杂的基于图的分析中的适用性。

    arXiv:2402.09617v1 Announce Type: new  Abstract: The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus
    
[^97]: API Pack：一个用于API调用生成的大规模多语言数据集

    API Pack: A Massive Multilingual Dataset for API Call Generation

    [https://arxiv.org/abs/2402.09615](https://arxiv.org/abs/2402.09615)

    这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成

    

    我们介绍了API Pack，一个包含超过一百万个指令-API调用对的多语言数据集，旨在提高大型语言模型的API调用生成能力。通过实验，我们证明了API Pack在提升模型在这一特定任务上的效果的同时，保持其在一般编码方面的整体熟练程度。仅在20,000个Python实例上对CodeLlama-13B进行微调，其生成未见过的API调用的准确率比GPT-3.5和GPT-4分别高出10%和5%。扩展到100k个例子可以提高对训练期间未见过的新API的泛化能力。此外，实现了跨语言的API调用生成，而无需大量语言特定的数据。数据集、经过微调的模型和整体代码库可在https://github.com/anonymous_url上公开获取。

    arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
    
[^98]: 生成式大型语言模型中的概率推理

    Probabilistic Reasoning in Generative Large Language Models

    [https://arxiv.org/abs/2402.09614](https://arxiv.org/abs/2402.09614)

    本文针对大型语言模型在概率推理任务中的限制，引入了贝叶斯语言推理数据集（BLInD），并提出了几种解决策略，包括Python代码和概率推理算法。

    

    本文探讨了大型语言模型（LLMs）在处理涉及概率值明确量化的文本推理问题时面临的挑战。这种概率推理对于从日常对话到医学决策等各种情境都很重要。尽管LLMs在数学推理能力方面有所改进，但在概率推理方面仍然存在显著困难。为了解决这个问题，我们首先引入了贝叶斯语言推理数据集（BLInD），这是一个专门设计用于测试LLMs概率推理能力的新数据集。然后，我们利用这个新数据集来详细说明LLMs在涉及概率推理的任务中的特定限制，并提出了几种将问题映射到不同形式表示的策略，包括Python代码和概率推理算法。

    arXiv:2402.09614v1 Announce Type: cross  Abstract: This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithm
    
[^99]: 实现规模化隐私感知手语翻译

    Towards Privacy-Aware Sign Language Translation at Scale

    [https://arxiv.org/abs/2402.09611](https://arxiv.org/abs/2402.09611)

    本研究提出了一种两阶段框架，用于实现规模化隐私感知手语翻译。我们利用自监督视频预训练和有监督微调的方法，在数据稀缺和隐私风险的情况下实现了最先进的手语翻译性能。

    

    手语翻译的一个主要障碍是数据稀缺。目前在网络上可用的大部分手语数据由于缺乏对齐的字幕而无法用于训练监督模型。此外，使用大规模网络抓取的数据集来扩展手语翻译存在隐私风险，因为其中包含生物特征信息，负责任地开发手语翻译技术应该考虑这一点。在这项工作中，我们提出了一种针对规模化隐私感知手语翻译的两阶段框架，解决了这两个问题。我们引入了SSVP-SLT，它利用匿名和未注释的视频进行自监督视频预训练，然后利用经过筛选的平行数据集进行有监督的手语翻译微调。 SSVP-SLT在How2Sign数据集上实现了最新的微调和零次gloss-free手语翻译性能，比最强的基线模型提高了3个BLEU-4。通过受控实验，我们证明了我们的方法在多个语言和手语词汇上都具有较好的泛化能力。

    arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
    
[^100]: LogicPrpBank：一个用于逻辑蕴含和等价的语料库

    LogicPrpBank: A Corpus for Logical Implication and Equivalence

    [https://arxiv.org/abs/2402.09609](https://arxiv.org/abs/2402.09609)

    LogicPrpBank是一个新的命题逻辑语料库，用于研究逻辑蕴含和等价的任务。通过与现有的语言模型进行基准测试，展示了该语料库在这一具有挑战性的任务中提供了有用的资源，并且还有改进的空间。

    

    逻辑推理在问题解决和决策中至关重要。虽然语言模型已经证明了处理多种推理任务（例如常识推理）的能力，但它们对于复杂的数学问题，特别是命题逻辑的推理能力仍然很少被探索。这种探索的不足可以归因于标注语料库的有限性。在这里，我们提供了一个经过良好标注的命题逻辑语料库LogicPrpBank，其中包含了7093个命题逻辑陈述（PLS）涵盖六个数学科目，以研究推理逻辑蕴含和等价的全新任务。我们使用广泛使用的语言模型对LogicPrpBank进行了基准测试，展示了我们的语料库为这一具有挑战性的任务提供了有用的资源，并且模型改进的空间还很大。

    arXiv:2402.09609v1 Announce Type: cross  Abstract: Logic reasoning has been critically needed in problem-solving and decision-making. Although Language Models (LMs) have demonstrated capabilities of handling multiple reasoning tasks (e.g., commonsense reasoning), their ability to reason complex mathematical problems, specifically propositional logic, remains largely underexplored. This lack of exploration can be attributed to the limited availability of annotated corpora. Here, we present a well-labeled propositional logic corpus, LogicPrpBank, containing 7093 Propositional Logic Statements (PLSs) across six mathematical subjects, to study a brand-new task of reasoning logical implication and equivalence. We benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a useful resource for this challenging task and there is ample room for model improvement.
    
[^101]: 使用InTEnt进行医学图像分割：基于集成熵加权的单图像测试时适应

    Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation

    [https://arxiv.org/abs/2402.09604](https://arxiv.org/abs/2402.09604)

    本文提出了一种使用单个未标记测试图像来调整医学图像分割模型的方法。相比于直接最小化预测熵的其他方法，在这种设置下并不能显著提高性能。为了克服这个问题，我们使用各种目标域统计估计的预测进行集成，并基于权重进行加权。

    

    测试时适应（TTA）是指在测试期间将训练好的模型调整到一个新领域。现有的TTA技术依赖于在同一领域具有多个测试图像，然而在实际应用（如医学成像）中，数据获取费用昂贵且成像条件经常变化，因此这种方法可能不切实际。本文致力于使用仅有一个未标记的测试图像来调整医学图像分割模型。大多数TTA方法直接最小化预测熵，然而在这种设置下，它们未能显著提高性能。我们还观察到，批归一化（BN）层的统计量选择是一个非常重要但不稳定的因素，因为只有一个测试域示例。为了克服这个问题，我们提出使用各种目标域统计估计的预测进行\textit{集成}，并基于权重进行加权。

    arXiv:2402.09604v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing. Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently. Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image. Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example. To overcome this, we propose to instead \textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on
    
[^102]: 可扩展图自监督学习

    Scalable Graph Self-Supervised Learning

    [https://arxiv.org/abs/2402.09603](https://arxiv.org/abs/2402.09603)

    该论文提出了一种通过体积最大化项减少图自监督学习预训练损失函数计算成本的方法。实验证明，采用节点或维度采样可以降低损失计算的成本。

    

    在图的正则化自监督学习方法中，计算复杂度随节点数和嵌入维度的增加而增加。为了减轻非对比图自监督学习的可扩展性问题，我们提出了一种新方法，通过体积最大化项减少预训练损失函数的协方差矩阵计算成本。我们的工作重点是通过图节点或维度采样减少损失计算的成本。我们从理论上解释了为什么维度采样会导致准确的损失计算，并用数学推导支持了这种新方法。我们在节点级图预测任务上进行了实验，因为现实世界图的规模很大，所以在这方面进行自监督预训练是困难的。我们的实验表明，通过节点或维度采样可以减少损失计算的成本。

    arXiv:2402.09603v1 Announce Type: cross  Abstract: In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions. To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs. Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling w
    
[^103]: 基于Web的工具用于复杂医疗调查研究的自动数据收集、整理和可视化，包括社交网络分析

    A Web-Based Tool for Automatic Data Collection, Curation, and Visualization of Complex Healthcare Survey Studies including Social Network Analysis

    [https://arxiv.org/abs/2402.09592](https://arxiv.org/abs/2402.09592)

    本研究设计和构建了一个基于Web的平台，用于自动化复杂医疗调查研究中数据收集、整理和可视化的过程，包括社交网络分析。该平台提供了直观的图形用户界面，可以简化问卷创建、数据收集和表示等操作，为用户提供便利和深入了解个人饮酒行为的工具。

    

    如今对于饮酒和药物滥用尤其是在年轻人中有着巨大的关注。通过分析这些青少年所处的社交环境，以及一系列衡量酒精滥用风险、个人状态和认知的问卷调查（如AUDIT、FAS、KIDSCREEN等），可以深入了解一个人在饮酒行为方面的实际情况。但为了实现这种分析，需要使用能够简化问卷创建、数据收集、整理和表示以及后续分析和可视化过程的工具。本研究介绍了一个基于Web的平台的设计和构建，该平台能够通过将不同阶段整合为一个直观的系统，并提供图形用户界面来简化上述每个过程。

    arXiv:2402.09592v1 Announce Type: new  Abstract: There is a great concern nowadays regarding alcohol consumption and drug abuse, especially in young people. Analyzing the social environment where these adolescents are immersed, as well as a series of measures determining the alcohol abuse risk or personal situation and perception using a number of questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain insight into the current situation of a given individual regarding his/her consumption behavior. But this analysis, in order to be achieved, requires the use of tools that can ease the process of questionnaire creation, data gathering, curation and representation, and later analysis and visualization to the user. This research presents the design and construction of a web-based platform able to facilitate each of the mentioned processes by integrating the different phases into an intuitive system with a graphical user interface that hides the complexity underlying e
    
[^104]: 使用大型语言模型在药物分子和适应症之间进行翻译的新机遇

    Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications

    [https://arxiv.org/abs/2402.09588](https://arxiv.org/abs/2402.09588)

    本研究探讨了使用大型语言模型在药物分子和适应症之间进行翻译的机遇，提出了一个新任务，并测试了其有效性，这对于药物发现过程具有重要意义。

    

    药物分子是一种改变生物体精神或身体状态的物质。每种批准的药物都有适应症，指的是该药物治疗特定医疗条件的治疗用途。虽然大型语言模型（LLM），一种生成式人工智能技术，最近已经证明在分子和其文本描述之间进行翻译方面是有效的，但仍存在关于在药物分子和适应症（或反之）之间进行翻译的应用的研究空白，这可能极大地有益于药物发现过程。从给定的适应症生成药物的能力将允许发现针对特定疾病或靶点的药物，并最终为患者提供更好的治疗方法。在本文中，我们首先提出了一个新任务，即药物分子和相应适应症之间的翻译，然后进行了实验测试。

    arXiv:2402.09588v1 Announce Type: new  Abstract: A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test exi
    
[^105]: 基于大型语言模型的建筑能源系统机器学习控制的可解释性研究

    Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems

    [https://arxiv.org/abs/2402.09584](https://arxiv.org/abs/2402.09584)

    本文研究了机器学习控制在建筑能源系统中的可解释性，通过将Shapley值和大型语言模型相结合，提高了机器学习控制模型的透明性和理解性。

    

    机器学习控制在暖通空调系统中的潜力受限于其不透明的性质和推理机制，这对于用户和建模者来说是具有挑战性的，难以完全理解，最终导致对基于机器学习控制的决策缺乏信任。为了解决这个挑战，本文研究和探索了可解释机器学习（IML），它是机器学习的一个分支，可以增强模型和推理的透明性和理解性，以提高MLC及其在暖通空调系统中的工业应用的可信度。具体而言，我们开发了一个创新性的框架，将Shapley值的原则和大型语言模型（LLMs）的上下文学习特性相结合。而Shapley值在解剖ML模型中各种特征的贡献方面起到了重要作用，LLM则可以深入理解MLC中基于规则的部分；将它们结合起来，LLM进一步将这些洞见打包到一个

    arXiv:2402.09584v1 Announce Type: new  Abstract: The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making. To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems. Specifically, we developed an innovative framework that combines the principles of Shapley values and the in-context learning feature of Large Language Models (LLMs). While the Shapley values are instrumental in dissecting the contributions of various features in ML models, LLM provides an in-depth understanding of rule-based parts in MLC; combining them, LLM further packages these insights into a
    
[^106]: 打击深度伪造：应对国家安全威胁和侵犯权利的政策

    Combatting deepfakes: Policies to address national security threats and rights violations

    [https://arxiv.org/abs/2402.09581](https://arxiv.org/abs/2402.09581)

    本文提供了应对深度伪造威胁的政策建议，包括背景信息、危害、先前的立法提案以及全面的深度伪造供应链政策建议。

    

    这篇论文提供了应对深度伪造威胁的政策建议。首先，我们提供了有关深度伪造的背景信息，并回顾了它们所带来的危害。我们描述了目前深度伪造被用于传播性虐待材料、进行欺诈、操纵选民行为以及对国家安全构成威胁的情况。其次，我们回顾了先前的立法提案，旨在解决深度伪造问题。第三，我们提出了一项全面的政策建议，重点解决深度伪造供应链的多个环节。深度伪造供应链从少数模型开发者、模型提供者和计算提供者开始，扩展至数十亿潜在的深度伪造制作者。我们详细描述了这个供应链，并说明每个环节的实体都应采取合理措施，防止深度伪造的制造和传播。最后，我们讨论了可能的对策措施。

    arXiv:2402.09581v1 Announce Type: cross  Abstract: This paper provides policy recommendations to address threats from deepfakes. First, we provide background information about deepfakes and review the harms they pose. We describe how deepfakes are currently used to proliferate sexual abuse material, commit fraud, manipulate voter behavior, and pose threats to national security. Second, we review previous legislative proposals designed to address deepfakes. Third, we present a comprehensive policy proposal that focuses on addressing multiple parts of the deepfake supply chain. The deepfake supply chain begins with a small number of model developers, model providers, and compute providers, and it expands to include billions of potential deepfake creators. We describe this supply chain in greater detail and describe how entities at each step of the supply chain ought to take reasonable measures to prevent the creation and proliferation of deepfakes. Finally, we address potential counterpo
    
[^107]: 用大型语言模型推动建筑能源建模：探索和案例研究

    Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies

    [https://arxiv.org/abs/2402.09579](https://arxiv.org/abs/2402.09579)

    本文研究了将大型语言模型ChatGPT与EnergyPlus建筑能源建模软件融合的创新方法，并强调了大型语言模型在解决建筑能源建模挑战方面的潜力和多种应用。

    

    人工智能的快速发展促进了像ChatGPT这样的大型语言模型的出现，为专门的工程建模（尤其是基于物理的建筑能源建模）提供了潜在的应用。本文研究了将大型语言模型与建筑能源建模软件（具体为EnergyPlus）融合的创新方法。首先进行了文献综述，揭示了在工程建模中整合大型语言模型的增长趋势，但在建筑能源建模中的应用研究仍然有限。我们强调了大型语言模型在解决建筑能源建模挑战方面的潜力，并概述了潜在的应用，包括：1）模拟输入生成，2）模拟输出分析和可视化，3）进行错误分析，4）共模拟，5）模拟知识提取。

    arXiv:2402.09579v1 Announce Type: cross  Abstract: The rapid progression in artificial intelligence has facilitated the emergence of large language models like ChatGPT, offering potential applications extending into specialized engineering modeling, especially physics-based building energy modeling. This paper investigates the innovative integration of large language models with building energy modeling software, focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature review is first conducted to reveal a growing trend of incorporating of large language models in engineering modeling, albeit limited research on their application in building energy modeling. We underscore the potential of large language models in addressing building energy modeling challenges and outline potential applications including 1) simulation input generation, 2) simulation output analysis and visualization, 3) conducting error analysis, 4) co-simulation, 5) simulation knowledge extraction a
    
[^108]: 图骨架：仅有约1%的节点足以表示十亿规模的图

    Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph

    [https://arxiv.org/abs/2402.09565](https://arxiv.org/abs/2402.09565)

    本文探讨了如何从海量的Web图数据中对背景节点进行压缩，并将重点放在分析目标节点上，以解决图数据存储和计算能力的挑战。

    

    由于Web上图数据的普遍存在，Web图挖掘已成为热门研究领域。然而，在实际应用中，大规模Web图的普及给存储、计算能力和图模型设计带来了重大挑战。尽管已进行了大量的研究来提高图模型的可扩展性，但学术研究与实际Web图挖掘应用之间仍存在明显差距。其中一个主要原因是，在大多数工业场景中，实际上只需要分析Web图中的一小部分节点，我们将这些节点称为目标节点，其他节点称为背景节点。在本文中，我们认为从海量Web图数据中恰当地提取和压缩背景节点可能是解决问题的更经济的捷径。为此，我们首次尝试研究了目标节点分类的大规模背景节点压缩问题。

    arXiv:2402.09565v1 Announce Type: new  Abstract: Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification
    
[^109]: 提高时间序列表示学习的双向生成预训练模型

    Bidirectional Generative Pre-training for Improving Time Series Representation Learning

    [https://arxiv.org/abs/2402.09558](https://arxiv.org/abs/2402.09558)

    这项论文提出了一种名为BiTimelyGPT的模型，通过双向的预训练任务在时间序列数据上学习表示，展示了优越的性能，可用于神经功能预测、疾病诊断和生理病征识别。

    

    学习时间序列表示以用于判别任务一直是一项长期的挑战。当前的预训练方法要么是单向的下一个标记预测，要么是随机屏蔽标记预测。我们提出了一种新颖的架构，称为双向及时生成预训练Transformer（BiTimelyGPT），它通过交替的Transformer层在时间序列数据上进行了下一个标记和上一个标记的预测。这种预训练任务保留了时间序列的原始分布和数据形状。此外，全秩前向和后向注意力矩阵具有更具表现力的表示能力。 使用生物信号数据，BiTimelyGPT在预测神经功能、疾病诊断和生理病征方面表现出了优越性能。通过可视化注意力热图，我们观察到预训练的BiTimelyGPT能够从时间序列中识别出具有判别性的片段。

    arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
    
[^110]: 统计与机器学习模型用于预测火灾和其他紧急事件

    Statistical and Machine Learning Models for Predicting Fire and Other Emergency Events

    [https://arxiv.org/abs/2402.09553](https://arxiv.org/abs/2402.09553)

    本文系统地开发了一种用于预测加拿大埃德蒙顿市不同类型紧急事件的预测模型，并分析了事件类型与邻域层面的社会经济和人口统计数据的关联性。

    

    城市中的紧急事件给个人、家庭和社区都带来了相当大的经济损失。准确和及时地预测事件可以帮助应急消防和救援部门为和减轻紧急事件的后果做好准备。在本文中，我们系统地开发了一种针对加拿大埃德蒙顿市不同类型紧急事件的预测模型。我们提出了以下方法：（i）数据收集和数据集开发；（ii）对不同时空级别的每种事件类型及其特征进行描述性分析；（iii）基于相关系数分析和特征重要性分析的特征分析和选择；（iv）针对不同时空分辨率开发每种事件类型发生可能性的预测模型。我们分析了事件类型与邻域层面的社会经济和人口统计数据的关联性。

    arXiv:2402.09553v1 Announce Type: new  Abstract: Emergency events in a city cause considerable economic loss to individuals, their families, and the community. Accurate and timely prediction of events can help the emergency fire and rescue services in preparing for and mitigating the consequences of emergency events. In this paper, we present a systematic development of predictive models for various types of emergency events in the City of Edmonton, Canada. We present methods for (i) data collection and dataset development; (ii) descriptive analysis of each event type and its characteristics at different spatiotemporal levels; (iii) feature analysis and selection based on correlation coefficient analysis and feature importance analysis; and (iv) development of prediction models for the likelihood of occurrence of each event type at different temporal and spatial resolutions. We analyze the association of event types with socioeconomic and demographic data at the neighborhood level, ide
    
[^111]: 大型语言模型（LLMs）在城市环境中导航时有多安全？

    How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?

    [https://arxiv.org/abs/2402.09546](https://arxiv.org/abs/2402.09546)

    本文首次研究了基于大型语言模型的导航系统在城市环境中的安全漏洞，并提出了一种新颖的NPS Attack方法，该方法通过添加后缀来操纵导航模型，导致不正确的行为。该研究对自动驾驶、物流和紧急服务等领域具有重要意义。

    

    在机器人和自动化领域，基于大型语言模型（LLMs）的导航系统最近展示了令人印象深刻的性能。然而，这些系统的安全性方面受到的关注相对较少。本文在城市户外环境中首次探索了LLM-based导航模型的漏洞，这是一个关键领域，因为该技术广泛应用于自动驾驶、物流和紧急服务。具体地，我们引入了一种新颖的Navigational Prompt Suffix (NPS) Attack，通过将梯度导出的后缀添加到原始导航提示，操纵LLM-based导航模型，从而导致不正确的行为。我们对基于LLMs的导航模型进行了全面的实验，该模型采用各种LLMs进行推理。我们的结果来自Touchdown和Map2Seq街景数据集，在few-shot学习和fine-tuning配置下进行实验，结果证明了NPS Attack的有效性。

    arXiv:2402.09546v1 Announce Type: cross  Abstract: In the field of robotics and automation, navigation systems based on Large Language Models (LLMs) have recently shown impressive performance. However, the security aspects of these systems have received relatively less attention. This paper pioneers the exploration of vulnerabilities in LLM-based navigation models in urban outdoor environments, a critical area given the technology's widespread application in autonomous driving, logistics, and emergency services. Specifically, we introduce a novel Navigational Prompt Suffix (NPS) Attack that manipulates LLM-based navigation models by appending gradient-derived suffixes to the original navigational prompt, leading to incorrect actions. We conducted comprehensive experiments on an LLMs-based navigation model that employs various LLMs for reasoning. Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstr
    
[^112]: 为什么具有较大ε的差分隐私可以防御实际成员推理攻击？

    Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?

    [https://arxiv.org/abs/2402.09540](https://arxiv.org/abs/2402.09540)

    本论文研究了为什么具有较大ε的差分隐私可以防御实际成员推理攻击，因为实际攻击者可能缺乏准确的私有数据知识，并且在实际应用中，数据集可能相对容易被防御。

    

    对于较小的隐私参数ε，ε-差分隐私（DP）提供了一个强大的最坏情况保证，即没有成员推理攻击（MIA）能够成功确定一个人的数据是否被用于训练机器学习模型。DP的保证是最坏情况下的，因为：a）即使攻击者已经知道数据集中除一个人的记录之外的所有记录；b）它在所有数据集上均匀适用。在实际应用中，这样的最坏情况保证可能过于严格：实际攻击者可能缺乏（几乎所有）私有数据的精确知识，并且我们的数据集可能在某种意义上比最坏情况的数据集更容易被防御。这些考虑推动了具有大的隐私参数（例如ε≥7）的DP模型的工业部署，并且经验上观察到具有大ε的DP可以成功防御最先进的MIA。现有的DP模型研究一般集中于小ε，因此尚不清楚为什么具有较大ε的DP可以防御实际成员推理攻击。

    arXiv:2402.09540v1 Announce Type: cross  Abstract: For small privacy parameter $\epsilon$, $\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\epsilon \geq 7$), and it has been observed empirically that DP with large $\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP the
    
[^113]: 排列、修复和改进：通过基于内容的控制实现可操控的长期音乐音频生成和编辑

    Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls

    [https://arxiv.org/abs/2402.09508](https://arxiv.org/abs/2402.09508)

    通过引入参数高效微调（PEFT）方法，本研究实现了自回归语言模型在音乐修复和音乐排列任务中的应用。在多个音乐编辑任务中，该方法展示了有希望的结果，并为未来的AI驱动音乐编辑工具提供了更灵活的控制。

    

    可控音乐生成在人机音乐共创中起着重要作用。虽然大型语言模型（LLM）在生成高质量音乐方面表现出了潜力，但它们对自回归生成的依赖限制了它们在音乐编辑任务中的实用性。为了弥合这一差距，我们引入了一种新颖的参数高效微调（PEFT）方法。该方法使自回归语言模型能够无缝地解决音乐修复任务。此外，我们的PEFT方法集成了基于帧级内容的控制，促进了轨道条件音乐的精炼和分数条件音乐的排列。我们将该方法应用于MusicGen，一个领先的自回归音乐生成模型的微调。我们的实验在多个音乐编辑任务中展示了有希望的结果，为未来的AI驱动音乐编辑工具提供了更灵活的控制。

    arXiv:2402.09508v1 Announce Type: cross  Abstract: Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source 
    
[^114]: 关于智能机器形式上不可判定特征的研究

    On Formally Undecidable Traits of Intelligent Machines

    [https://arxiv.org/abs/2402.09500](https://arxiv.org/abs/2402.09500)

    我们研究了智能机器形式上不可判定特征的条件, 发展了一种数学上独立于形式语言理论的形式化方法, 发现Rice定理不能用于判断机器是否具有给定特征。

    

    建立在Alfonseca等人（2021）的工作基础上，我们研究了证明任意人工智能机器将展示某种行为在逻辑上可能的条件。为此，我们发展了一种类似但数学上独立于形式语言及其属性理论的形式化方法。我们的形式化方法不仅提供了准确的方式来描述我们对机器期望的特征（如智能、可控、道德等），而且还详细说明了判断给定任意机器是否具有这样的特征在逻辑上可能的条件。与Alfonseca等人（2021）的结果相反，我们发现可计算性理论中的Rice定理通常不能用于判断给定的任意机器是否具有给定特征。因此，并不一定可以决定是否一个任意机器是智能的。

    arXiv:2402.09500v1 Announce Type: new  Abstract: Building on work by Alfonseca et al. (2021), we study the conditions necessary for it to be logically possible to prove that an arbitrary artificially intelligent machine will exhibit certain behavior. To do this, we develop a formalism like -- but mathematically distinct from -- the theory of formal languages and their properties. Our formalism affords a precise means for not only talking about the traits we desire of machines (such as them being intelligent, contained, moral, and so forth), but also for detailing the conditions necessary for it to be logically possible to decide whether a given arbitrary machine possesses such a trait or not. Contrary to Alfonseca et al.'s (2021) results, we find that Rice's theorem from computability theory cannot in general be used to determine whether an arbitrary machine possesses a given trait or not. Therefore, it is not necessarily the case that deciding whether an arbitrary machine is intellige
    
[^115]: 使用机器学习技术检测预防产后尿失禁的最具影响力变量

    Detection of the most influential variables for preventing postpartum urinary incontinence using machine learning techniques

    [https://arxiv.org/abs/2402.09498](https://arxiv.org/abs/2402.09498)

    本研究使用机器学习技术评估了产后尿失禁中最具影响力的变量，并发现外在变量是PUI问题的重要预测因素。

    

    arXiv:2402.09498v1 公告类型：新摘要：背景：产后尿失禁（PUI）是产后妇女的常见问题。以往的研究鉴定了潜在的相关变量，但缺乏对妊娠期间某些固有和外在的患者变量的分析。目的：本研究旨在利用机器学习评估PUI中的最具影响力的变量，重点关注固有、外在和综合变量组。方法：使用机器学习和过采样技术分析了93名孕妇的数据。预测了四个关键变量：尿失禁发生率、频率、强度和压力尿失禁。结果：使用外在变量的模型最准确，尿失禁准确率为70％，频率为77％，强度为71％，压力尿失禁为93％。结论：本研究强调外在变量是PUI问题的重要预测因素。这表明PUI的预防可能需要注意外在因素。

    arXiv:2402.09498v1 Announce Type: new  Abstract: Background: Postpartum urinary incontinence (PUI) is a common issue among postnatal women. Previous studies identified potential related variables, but lacked analysis on certain intrinsic and extrinsic patient variables during pregnancy.   Objective: The study aims to evaluate the most influential variables in PUI using machine learning, focusing on intrinsic, extrinsic, and combined variable groups.   Methods: Data from 93 pregnant women were analyzed using machine learning and oversampling techniques. Four key variables were predicted: occurrence, frequency, intensity of urinary incontinence, and stress urinary incontinence.   Results: Models using extrinsic variables were most accurate, with 70% accuracy for urinary incontinence, 77% for frequency, 71% for intensity, and 93% for stress urinary incontinence.   Conclusions: The study highlights extrinsic variables as significant predictors of PUI issues. This suggests that PUI preventi
    
[^116]: 安全代码生成的指令调优

    Instruction Tuning for Secure Code Generation

    [https://arxiv.org/abs/2402.09497](https://arxiv.org/abs/2402.09497)

    现代语言模型在编程中得到广泛应用，指令调优是一个增强其实用性的关键过程。然而，现有的方案忽视了生成代码的安全性。本文提出了SafeCoder，通过安全微调和标准指令调优相结合，来优化安全性和实用性。

    

    现代语言模型(LMs)在日常和专业环境中得到了广泛的认可，尤其在编程中。指令调优是一种关键的过程，通过训练LMs遵循用户指令和人类偏好，从而大大增强了LMs的实用性。然而，现有的指令调优方案忽视了一个关键方面：生成代码的安全性。因此，即使是最先进的指令调优的LMs也经常产生不安全的代码，带来了重大的安全风险。在这项工作中，我们引入了SafeCoder来填补这个差距。SafeCoder使用一个多样化和高质量的数据集进行安全为中心的微调，我们使用自动化流水线收集了这个数据集。我们将安全微调与标准的指令调优相结合，以便同时优化安全性和实用性。尽管简单，但我们展示了SafeCoder的有效性。

    arXiv:2402.09497v1 Announce Type: cross  Abstract: Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across
    
[^117]: 关于基于网络特征在欺诈检测中潜力的研究

    On the Potential of Network-Based Features for Fraud Detection

    [https://arxiv.org/abs/2402.09495](https://arxiv.org/abs/2402.09495)

    本文研究了基于网络特征在欺诈检测中的潜力，通过使用个性化的PageRank算法来捕捉欺诈的社会动态。实验结果表明，集成PPR可以提高模型的预测能力并提供独特有价值的信息。

    

    在线交易欺诈给企业和消费者带来了重大挑战，面临着重大的经济损失。传统的基于规则的系统难以跟上欺诈战术的演变，导致高误报率和漏报率。机器学习技术通过利用历史数据识别欺诈模式提供了一个有希望的解决方案。本文探讨使用个性化的PageRank（PPR）算法通过分析金融账户之间的关系来捕捉欺诈的社会动态。主要目标是比较传统特征与添加PPR在欺诈检测模型中的性能。结果表明，集成PPR可以提高模型的预测能力，超过基准模型。此外，PPR特征提供了独特而有价值的信息，通过其高特征重要性得分得以证明。特征稳定性分析证实了一致的结果。

    arXiv:2402.09495v1 Announce Type: cross  Abstract: Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consist
    
[^118]: AI和人类能够真正交流吗？

    Can AI and humans genuinely communicate?

    [https://arxiv.org/abs/2402.09494](https://arxiv.org/abs/2402.09494)

    本研究探讨了AI和人类是否能够真正交流的问题，并提出了一种称为“心理行为方法”的回答方式，该方法通过测试AI是否展现出人类类似的行为来判断其是否能够与人类真正交流。

    

    这篇文章探讨了一个问题，即AI和人类是否能够真正交流。作者提出了一个称为“心理行为方法”的方式来回答这个问题。该方法包括三个步骤：首先明确人类交流所需的心理能力；其次确定测试这些能力的实验范式；最后将这些范式应用于测试AI是否展现出相关的行为。如果前两个步骤成功完成，并且AI在测试中展现出类似人类的结果，那么这可以被看作是AI和人类能够真正交流的证据。心理行为方法的优势在于我们不需要理解黑盒算法（比如标准深度学习算法）的工作原理。

    arXiv:2402.09494v1 Announce Type: cross  Abstract: Can AI and humans genuinely communicate? In this article, after giving some background and motivating my proposal (sections 1 to 3), I explore a way to answer this question that I call the "mental-behavioral methodology" (sections 4 and 5). This methodology follows the following three steps: First, spell out what mental capacities are sufficient for human communication (as opposed to communication more generally). Second, spell out the experimental paradigms required to test whether a behavior exhibits these capacities. Third, apply or adapt these paradigms to test whether an AI displays the relevant behaviors. If the first two steps are successfully completed, and if the AI passes the tests with human-like results, this constitutes evidence that this AI and humans can genuinely communicate. This mental-behavioral methodology has the advantage that we don't need to understand the workings of black-box algorithms, such as standard deep 
    
[^119]: AI-启用的肺癌预后

    AI-Enabled Lung Cancer Prognosis

    [https://arxiv.org/abs/2402.09476](https://arxiv.org/abs/2402.09476)

    这项研究旨在探索人工智能在肺癌预后中的应用。通过利用机器学习和深度学习算法，研究人员可以提高肺癌生存预测的准确性，并为临床医生提供全面的信息，以便为患者制定更好的治疗策略。

    

    arXiv:2402.09476v1 公告类型: 交叉研究 摘要: 肺癌是导致癌症相关死亡的主要原因，在2020年全球范围内夺走了约179万人的生命，估计在同一时期内有221万例新病例被诊断出来。其中，非小细胞肺癌（NSCLC）是主要亚型，其特点是预后极差，全部病程五年的存活率约为25%。然而，存活结果根据诊断阶段和行治疗介入的差异而相差很大。人工智能（AI）的最新进展已经在肺癌预后领域引发了革命。AI驱动的方法，包括机器学习和深度学习算法，已经展示了通过高效分析复杂的多组学数据和整合多样临床变量来提高生存预测准确性的潜力。通过利用AI技术，临床医生可以利用全面的信息

    arXiv:2402.09476v1 Announce Type: cross  Abstract: Lung cancer is the primary cause of cancer-related mortality, claiming approximately 1.79 million lives globally in 2020, with an estimated 2.21 million new cases diagnosed within the same period. Among these, Non-Small Cell Lung Cancer (NSCLC) is the predominant subtype, characterized by a notably bleak prognosis and low overall survival rate of approximately 25% over five years across all disease stages. However, survival outcomes vary considerably based on the stage at diagnosis and the therapeutic interventions administered. Recent advancements in artificial intelligence (AI) have revolutionized the landscape of lung cancer prognosis. AI-driven methodologies, including machine learning and deep learning algorithms, have shown promise in enhancing survival prediction accuracy by efficiently analyzing complex multi-omics data and integrating diverse clinical variables. By leveraging AI techniques, clinicians can harness comprehensive
    
[^120]: 解读心率信号：一种基于视觉变压器技术的可解释性房颤检测方法

    Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals

    [https://arxiv.org/abs/2402.09474](https://arxiv.org/abs/2402.09474)

    本研究使用视觉变压器方法解读心率信号，提高心脏疾病检测模型的解释性和可靠性。

    

    基于可穿戴单导联心电图（ECG）设备的远程患者监测在结合人工智能（AI）方法进行自动心脏疾病检测方面具有巨大潜力。先前的研究已经应用基于深度学习的AI方法进行心脏疾病检测，但由于目前AI算法的黑盒特性，这些模型尚未被广泛接受作为临床诊断的可靠辅助工具。尤其需要确定ECG信号中贡献于准确诊断的关键特征，从而提高模型的可解释性。本研究开发了一种基于视觉变压器的方法，通过单导联ECG数据识别房颤，并提出了一种残差网络（ResNet）方法以作对比。

    arXiv:2402.09474v1 Announce Type: cross  Abstract: Remote patient monitoring based on wearable single-lead electrocardiogram (ECG) devices has significant potential for enabling the early detection of heart disease, especially in combination with artificial intelligence (AI) approaches for automated heart disease detection. There have been prior studies applying AI approaches based on deep learning for heart disease detection. However, these models are yet to be widely accepted as a reliable aid for clinical diagnostics, in part due to the current black-box perception surrounding many AI algorithms. In particular, there is a need to identify the key features of the ECG signal that contribute toward making an accurate diagnosis, thereby enhancing the interpretability of the model. In the present study, we develop a vision transformer approach to identify atrial fibrillation based on single-lead ECG data. A residual network (ResNet) approach is also developed for comparison with the visi
    
[^121]: RLEEGNet：将脑机接口与自适应人工智能结合，实现直观响应和高准确率的动作意象分类

    RLEEGNet: Integrating Brain-Computer Interfaces with Adaptive AI for Intuitive Responsiveness and High-Accuracy Motor Imagery Classification

    [https://arxiv.org/abs/2402.09465](https://arxiv.org/abs/2402.09465)

    本文提出了RLEEGNet，这是一种将脑机接口与自适应人工智能结合的框架，能够实现直观响应和高准确率的动作意象分类。通过使用强化学习和深度Q网络进行分类任务，并结合常空模式预处理技术，该框架能够实时适应用户不断变化的需求和意图。

    

    目前的假肢控制方法受传统方法的限制，缺乏实时适应性和直观响应能力，尤其在面对认知状态和运动意图多样的辅助技术时表现更为明显。本文介绍了一种利用强化学习（RL）与深度Q网络（DQN）进行分类任务的框架。此外，我们还提出了一种使用常空模式（CSP）进行多类动作意像（MI）分类的预处理技术，以"一对其余（OVR）"的方式进行分类。随后的“csp空间”转换保留了EEG信号的时间维度，对提取区分特征至关重要。DQN与1D-CNN-LSTM结构的集成在实时决策过程中优化了系统对用户不断变化的需求和意图的适应能力。

    arXiv:2402.09465v1 Announce Type: cross  Abstract: Current approaches to prosthetic control are limited by their reliance on traditional methods, which lack real-time adaptability and intuitive responsiveness. These limitations are particularly pronounced in assistive technologies designed for individuals with diverse cognitive states and motor intentions. In this paper, we introduce a framework that leverages Reinforcement Learning (RL) with Deep Q-Networks (DQN) for classification tasks. Additionally, we present a preprocessing technique using the Common Spatial Pattern (CSP) for multiclass motor imagery (MI) classification in a One-Versus-The-Rest (OVR) manner. The subsequent 'csp space' transformation retains the temporal dimension of EEG signals, crucial for extracting discriminative features. The integration of DQN with a 1D-CNN-LSTM architecture optimizes the decision-making process in real-time, thereby enhancing the system's adaptability to the user's evolving needs and intent
    
[^122]: 面向人体部位定向跟踪和3D运动可视化的定制IMU无线可穿戴系统

    Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body Parts Orientation Tracking and 3D Movement Visualization on an Avatar

    [https://arxiv.org/abs/2402.09459](https://arxiv.org/abs/2402.09459)

    这项研究的目标是通过构建可负担的定制IMU无线可穿戴系统，在人体运动分析中实现对身体部位定向跟踪和3D运动可视化。

    

    最近的研究确认了使用惯性测量单元（IMU）的系统对于人体运动分析的适用性。然而，高端的商业化IMU解决方案价格昂贵且复杂，无法普及在广大潜在用户中的使用。市场上出现了一些功能较少的低端商业化解决方案，试图填补这一空白，但仍存在一些需要克服的限制。与此同时，在医疗和运动应用领域，越来越多的科学论文使用的是非商业化的、自制的IMU系统。尽管这些解决方案可以促进这项技术的普及化使用，但它们的功能更为有限，并且如何设计和构建它们的描述在文献中仍然稀缺。本文的目的是：（1）证明构建一种可负担的定制解决方案是可行的，旨在同时追踪多个人体部位的定向和运动。

    arXiv:2402.09459v1 Announce Type: cross  Abstract: Recent studies confirm the applicability of Inertial Measurement Unit (IMU)-based systems for human motion analysis. Notwithstanding, high-end IMU-based commercial solutions are yet too expensive and complex to democratize their use among a wide range of potential users. Less featured entry-level commercial solutions are being introduced in the market, trying to fill this gap, but still present some limitations that need to be overcome. At the same time, there is a growing number of scientific papers using not commercial, but custom do-it-yourself IMU-based systems in medical and sports applications. Even though these solutions can help to popularize the use of this technology, they have more limited features and the description on how to design and build them from scratch is yet too scarce in the literature. The aim of this work is two-fold: (1) Proving the feasibility of building an affordable custom solution aimed at simultaneous mu
    
[^123]: 未知博弈中乐观的汤普森抽样方法用于无遗憾学习

    Optimistic Thompson Sampling for No-Regret Learning in Unknown Games

    [https://arxiv.org/abs/2402.09456](https://arxiv.org/abs/2402.09456)

    该论文提出了一种在未知博弈中进行无遗憾学习的乐观的汤普森抽样方法，通过利用对手的行动和奖励结构信息，显著减少了实验预算，成功地缓解了多机构问题。此外，研究还引入了乐观-无遗憾框架，将现有算法与提出的方法相结合。

    

    许多涉及多个决策者的真实世界问题可以建模为一个具有部分观测的未知博弈。为了解决部分信息和多机构的挑战，我们开发了汤普森抽样类型的算法，利用对手的行动和奖励结构的信息。我们的方法在实际应用中，如交通路由和雷达感知中，显著减少了实验预算，与基准算法相比，减少了十倍以上。我们证明，在对奖励结构有一定假设的情况下，遗憾界限仅对总行动空间大小呈对数依赖，有效缓解了多机构问题。此外，本研究引入了乐观-无遗憾框架，该框架将我们提出的方法和领域内现有的算法相结合，是一项新的贡献。

    arXiv:2402.09456v1 Announce Type: cross  Abstract: Many real-world problems involving multiple decision-makers can be modeled as an unknown game characterized by partial observations. Addressing the challenges posed by partial information and the curse of multi-agency, we developed Thompson sampling-type algorithms, leveraging information about opponent's action and reward structures. Our approach significantly reduces experimental budgets, achieving a more than tenfold reduction compared to baseline algorithms in practical applications like traffic routing and radar sensing. We demonstrate that, under certain assumptions about the reward structure, the regret bound exhibits merely a logarithmic dependence on the total action space size, effectively mitigating the curse of multi-agency. Additionally, this research introduces the Optimism-then-NoRegret framework, a novel contribution that integrates both our proposed methodologies and existing algorithms in the field.
    
[^124]: 使用Wasserstein生成对抗网络提高EEG信号分类准确性

    Improving EEG Signal Classification Accuracy Using Wasserstein Generative Adversarial Networks

    [https://arxiv.org/abs/2402.09453](https://arxiv.org/abs/2402.09453)

    该论文提出了一种通过使用Wasserstein生成对抗网络(WGAN)来提高EEG信号分类准确性的实际解决方案。 WGAN在BCI2000数据集上进行训练，并通过改进的平均准确率和测量得分证明了生成的EEG信号的质量。

    

    静息生态（EEG）在记录脑部活动中起着重要作用，并且对脑机接口（BCI）技术的发展至关重要。然而，EEG信号的有限可用性和高度变异性给创建可靠的BCI带来了巨大的挑战。为了解决这个问题，我们提出了一个实际的解决方案，利用深度学习和Wasserstein生成对抗网络（WGAN）的最新发展。WGAN是在BCI2000数据集上进行训练的，该数据集包括来自45个个体的约1500个EEG记录和64个通道。通过三个分类器评估生成的EEG信号，得到了改进的平均准确率。使用Frechet Inception Distance（FID）测量的生成信号质量为1.345（睁眼）和11.565（闭眼）。即使没有频谱或空间损失项，我们的WGAN模型仍能模拟出频谱和空间特性。

    arXiv:2402.09453v1 Announce Type: cross  Abstract: Electroencephalography (EEG) plays a vital role in recording brain activities and is integral to the development of brain-computer interface (BCI) technologies. However, the limited availability and high variability of EEG signals present substantial challenges in creating reliable BCIs. To address this issue, we propose a practical solution drawing on the latest developments in deep learning and Wasserstein Generative Adversarial Network (WGAN). The WGAN was trained on the BCI2000 dataset, consisting of around 1500 EEG recordings and 64 channels from 45 individuals. The generated EEG signals were evaluated via three classifiers yielding improved average accuracies. The quality of generated signals measured using Frechet Inception Distance (FID) yielded scores of 1.345 and 11.565 for eyes-open and closed respectively. Even without a spectral or spatial loss term, our WGAN model was able to emulate the spectral and spatial properties of
    
[^125]: 引导遮蔽表示学习以捕捉心电图的时空关系

    Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram

    [https://arxiv.org/abs/2402.09450](https://arxiv.org/abs/2402.09450)

    本研究提出了一种叫做ST-MEM的模型，通过重构遮蔽的心电图数据来学习时空特征，该模型在心律失常分类任务中优于其他自监督学习方法。

    

    心电图（ECG）广泛用作监测心脏起源的电信号的诊断工具。近年来，机器学习的研究努力集中在使用ECG信号进行各种疾病筛查的应用上。然而，适应疾病筛查应用是具有挑战性的，因为标记的ECG数据有限。通过自监督学习（SSL）实现通用表示是克服标记数据稀缺性的常用方法；然而，在ECG数据上纯粹应用SSL，而不考虑ECG信号固有的时空关系，可能会产生次优的结果。本文介绍了ST-MEM（时空遮蔽心电图建模），该模型通过重构遮蔽的12导联ECG数据来学习时空特征。在各种实验设置中，ST-MEM在心律失常分类任务中的性能优于其他SSL基线方法。

    arXiv:2402.09450v1 Announce Type: cross  Abstract: Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening disease is challenging in that labeled ECG data are limited. Achieving general representation through self-supervised learning (SSL) is a well-known approach to overcome the scarcity of labeled data; however, a naive application of SSL to ECG data, without considering the spatial-temporal relationships inherent in ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM (Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM outperforms other SSL baseline methods in various experimental settings for arrhythmia classification tasks. Mo
    
[^126]: 普通EEG与三极EEG在高性能到颤抓握BCI系统中的比较研究

    A Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systems

    [https://arxiv.org/abs/2402.09448](https://arxiv.org/abs/2402.09448)

    比较传统EEG与三极EEG在高性能到颤抓握BCI系统中的有效性，包括信噪比、空间分辨率、ERPs和小波时频分析。

    

    本研究旨在比较传统EEG与三极EEG在提升运动障碍个体的BCI应用方面的有效性。重点是解读和解码各种抓握动作，如力握和精确握持。目标是确定哪种EEG技术在处理和翻译与抓握相关的脑电信号方面更为有效。研究涉及对十名健康参与者进行实验，参与者进行了两种不同的握持运动：力握和精确握持，无运动条件作为基线。我们的研究在解码抓握动作方面对EEG和三极EEG进行了全面比较。该比较涵盖了几个关键参数，包括信噪比（SNR）、通过功能连接的空间分辨率、ERPs和小波时频分析。此外，我们的研究还涉及从...

    arXiv:2402.09448v1 Announce Type: cross  Abstract: This study aims to enhance BCI applications for individuals with motor impairments by comparing the effectiveness of tripolar EEG (tEEG) with conventional EEG. The focus is on interpreting and decoding various grasping movements, such as power grasp and precision grasp. The goal is to determine which EEG technology is more effective in processing and translating grasp related neural signals. The approach involved experimenting on ten healthy participants who performed two distinct grasp movements: power grasp and precision grasp, with a no movement condition serving as the baseline. Our research presents a thorough comparison between EEG and tEEG in decoding grasping movements. This comparison spans several key parameters, including signal to noise ratio (SNR), spatial resolution via functional connectivity, ERPs, and wavelet time frequency analysis. Additionally, our study involved extracting and analyzing statistical features from th
    
[^127]: 非侵入性脑电图信号的小波分析区分复杂和自然的抓握类型

    Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and Natural Grasp Types

    [https://arxiv.org/abs/2402.09447](https://arxiv.org/abs/2402.09447)

    该研究使用小波分析技术对非侵入性脑电图信号进行解码，成功区分复杂和自然的抓握类型，并且证明了小波特征在基于脑电图的抓握区分中的有效性。

    

    该研究旨在通过对脑电图（EEG）信号进行解码，为灵巧的神经假肢开发和脑机接口（BCI）应用来区分手部抓握，特别是针对运动障碍患者。具体而言，它专注于使用一种新的基于脑电图的BCI平台和小波信号处理，区分两种复杂的自然力量和精确抓握类型以及一种中立条件作为无运动条件。小波分析涉及从小波能量系数生成时间频率和拓扑图。然后，通过使用机器学习技术和新型小波特征，我们实现了高平均准确率：多类别为85.16%，无运动 vs 力量为95.37%，无运动 vs 精确为95.40%，力量 vs 精确为88.07%，证明了这些特征在基于脑电图的抓握区分中的有效性。与先前的研究不同，我们研究的关键部分是排列特征重要性的部分。

    arXiv:2402.09447v1 Announce Type: cross  Abstract: This research aims to decode hand grasps from Electroencephalograms (EEGs) for dexterous neuroprosthetic development and Brain-Computer Interface (BCI) applications, especially for patients with motor disorders. Particularly, it focuses on distinguishing two complex natural power and precision grasps in addition to a neutral condition as a no-movement condition using a new EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved generating time-frequency and topographic maps from wavelet power coefficients. Then, by using machine learning techniques with novel wavelet features, we achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs Precision, demonstrating the effectiveness of these features in EEG-based grasp differentiation. In contrast to previous studies, a critical part of our study was permutation feature impo
    
[^128]: iMove: 探索用于健身活动识别的生物阻抗传感技术

    iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition

    [https://arxiv.org/abs/2402.09445](https://arxiv.org/abs/2402.09445)

    通过传感器融合和对比学习，研究证明生物阻抗传感技术可以改进基于IMU的健身追踪，提高分类模型的精度。

    

    自动和精确的健身活动识别对于促进健康生活方式和个性化预防性医疗具有益处。虽然IMU目前是主要的健身追踪模式，但通过iMove，我们展示了生物阻抗可以通过传感器融合和对比学习来改善基于IMU的健身追踪。为了评估我们的方法，我们进行了一项实验，包括十个受试者在五天内进行的六种上身健身活动，以收集来自两只手腕的生物阻抗和左手腕IMU的同步数据。对比学习框架利用两种模态来训练更好的仅基于IMU的分类模型，其中生物阻抗只在训练阶段需要，通过这种方式，输入单个IMU的平均宏F1分数提高了3.22％，达到84.71％，而IMU基线模型为81.49％。我们还展示了生物阻抗如何能够...

    arXiv:2402.09445v1 Announce Type: cross  Abstract: Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU baseline model. We have also shown how bio-impedance can 
    
[^129]: 多模态动作质量评估

    Multimodal Action Quality Assessment

    [https://arxiv.org/abs/2402.09444](https://arxiv.org/abs/2402.09444)

    该论文提出了一个名为PAMFN的渐进自适应多模态融合网络，用于多模态动作质量评估。该模型利用RGB、光流和音频信息，分别建模模态特定信息和混合模态信息，并通过充分利用音频信息，提高了评分回归的准确性。

    

    行动质量评估（AQA）是评估动作执行情况的方法。以往的研究仅利用视觉信息进行建模，忽视了音频信息。我们认为，虽然AQA高度依赖视觉信息，但音频也是提高评分回归准确性的有用补充信息，特别是在具有背景音乐的运动项目中，如花样滑冰和韵律体操。为了利用多模态信息进行AQA，即RGB、光流和音频信息，我们提出了一个渐进自适应多模态融合网络（PAMFN），它分别对模态特定信息和混合模态信息进行建模。我们的模型由三个模态特定分支和一个混合模态分支组成，独立地探索模态特定信息，并渐进地聚合来自模态特定分支的模态特定信息。

    arXiv:2402.09444v1 Announce Type: cross  Abstract: Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between
    
[^130]: 预测疲劳的 EEG 算法综述

    Review of algorithms for predicting fatigue using EEG

    [https://arxiv.org/abs/2402.09443](https://arxiv.org/abs/2402.09443)

    该研究综述了使用 EEG 信号进行疲劳预测的机器学习算法，并评估了不同算法在基于 EEG 数据预测个体疲劳水平方面的效果。

    

    疲劳检测对于提高交通、医疗和工业等各个领域的安全性、生产力和福祉至关重要。本科学论文对使用机器学习算法检测生理疲劳的方法进行了全面的调查，使用脑电图（EEG）信号。本研究的主要目标是评估不同算法在基于 EEG 数据预测个体疲劳水平方面的功效。

    arXiv:2402.09443v1 Announce Type: cross  Abstract: Fatigue detection is of paramount importance in enhancing safety, productivity, and well-being across diverse domains, including transportation, healthcare, and industry. This scientific paper presents a comprehensive investigation into the application of machine learning algorithms for the detection of physiological fatigue using Electroencephalogram (EEG) signals. The primary objective of this study was to assess the efficacy of various algorithms in predicting an individual's level of fatigue based on EEG data.
    
[^131]: 基于自驱动传感器和深度学习的人工智能应用进展

    Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning

    [https://arxiv.org/abs/2402.09442](https://arxiv.org/abs/2402.09442)

    本文介绍了基于自驱动传感器和深度学习的人工智能应用的最新进展，重点讨论了使用TENG作为自驱动传感器的优势，包括简单结构和高瞬时性能。

    

    在物联网时代，如何开发具有可持续电源供应、易于部署和灵活使用的智能传感器系统已成为一个难题。传统的电源供应存在频繁更换或使用时充电等问题，这限制了可穿戴设备的发展。通过使用聚四氟乙烯（PTFE）和铝箔（AI）制备接触-分离摩擦纳米发电机（TENG）来收集人体运动能量，根据输出电信号的变化来监测人体运动姿势。 2012年，王中林院士及其团队发明了摩擦电纳米发电机（TENG），它利用最大位移电流作为驱动力，将机械刺激直接转换为电信号，因此可以用作自驱动传感器。TENG传感器具有结构简单和瞬时性高的优点。

    arXiv:2402.09442v1 Announce Type: cross  Abstract: In the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved. The traditional power supply has problems such as frequent replacement or charging when in use, which limits the development of wearable devices. The contact-to-separate friction nanogenerator (TENG) was prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human motion energy was collected by human body arrangement, and human motion posture was monitored according to the changes of output electrical signals. In 2012, Academician Wang Zhong lin and his team invented the triboelectric nanogenerator (TENG), which uses Maxwell displacement current as a driving force to directly convert mechanical stimuli into electrical signals, so it can be used as a self-driven sensor. Teng-based sensors have the advantages of simple structure and high instant
    
[^132]: 基于电气行为关联挖掘的家庭短期能耗预测研究

    Electrical Behavior Association Mining for Household ShortTerm Energy Consumption Forecasting

    [https://arxiv.org/abs/2402.09433](https://arxiv.org/abs/2402.09433)

    本文提出了一种基于电气行为关联挖掘的家庭短期能耗预测方法，通过概率关联模型和卷积神经网络门控循环单元的结合，实现了显著的准确性提升。

    

    准确的家庭短期能耗预测(STECF)对家庭能源管理至关重要，但由于个别住户的高度随机行为，技术上具有挑战性。为了提高日前程度的STECF准确性，本文提出了一种新的STECF方法，利用电气行为中的关联挖掘。首先，提出了一种概率化的关联量化和发现方法，用于建模行为之间的关联，并生成关联群集。然后，采用卷积神经网络门控循环单元(CNN-GRU)进行预测，以探索时间相关性并提高准确性。测试结果表明，该方法在STECF方面得到了显著的提升。

    arXiv:2402.09433v1 Announce Type: cross  Abstract: Accurate household short-term energy consumption forecasting (STECF) is crucial for home energy management, but it is technically challenging, due to highly random behaviors of individual residential users. To improve the accuracy of STECF on a day-ahead scale, this paper proposes an novel STECF methodology that leverages association mining in electrical behaviors. First, a probabilistic association quantifying and discovering method is proposed to model the pairwise behaviors association and generate associated clusters. Then, a convolutional neural network-gated recurrent unit (CNN-GRU) based forecasting is provided to explore the temporal correlation and enhance accuracy. The testing results demonstrate that this methodology yields a significant enhancement in the STECF.
    
[^133]: DoorINet: 一种用于门贴式物联网应用的深度学习惯性框架

    DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications

    [https://arxiv.org/abs/2402.09427](https://arxiv.org/abs/2402.09427)

    DoorINet是一种用于门贴式物联网应用的深度学习惯性框架，无需使用磁力计即可计算航向角度。

    

    许多物联网应用使用低成本的微型电动机械惯性传感器，其中一个常见的任务是方向估计。为了应对这种任务，应用姿态和航向参考系统算法。利用陀螺仪读数，通过加速度计读数更新姿态角度，利用磁力计测量更新航向角度。在室内环境中，磁力计受到干扰，会降低其性能。这主要影响到估计航向角度的应用，比如找到衣柜或冰箱门的航向角度。为了解决这种情况，我们提出了DoorINet，一种用于门贴式低成本惯性传感器的端到端深度学习框架，无需使用磁力计即可计算航向角度。为了评估我们的方法，我们记录了一个包含391分钟加速度计和陀螺仪测量的独特数据集。

    arXiv:2402.09427v1 Announce Type: cross  Abstract: Many Internet of Things applications utilize low-cost, micro, electro-mechanical inertial sensors. A common task is orientation estimation. To tackle such a task, attitude and heading reference system algorithms are applied. Relying on the gyroscope readings, the accelerometer readings are used to update the attitude angles, and magnetometer measurements are utilized to update the heading angle. In indoor environments, magnetometers suffer from interference that degrades their performance. This mainly influences applications focused on estimating the heading angle like finding the heading angle of a closet or fridge door. To circumvent such situations, we propose DoorINet, an end-to-end deep-learning framework to calculate the heading angle from door-mounted, low-cost inertial sensors without using magnetometers. To evaluate our approach, we record a unique dataset containing 391 minutes of accelerometer and gyroscope measurements and 
    
[^134]: 数学解释

    Mathematical Explanations

    [https://arxiv.org/abs/2402.09413](https://arxiv.org/abs/2402.09413)

    该论文讨论了数学陈述的解释问题以及通过使用不可能的可能世界解决了数学解释中的困境。

    

    给出了数学陈述的解释定义以及一个解释何时比另一个更好的定义。由于所有数学事实必须在所有因果模型中都是真实的，因此由一个代理者所知，数学事实不能是解释的一部分（按照标准的解释观念）。这个问题是通过使用不可能的可能世界来解决的。

    arXiv:2402.09413v1 Announce Type: new  Abstract: A definition of what counts as an explanation of mathematical statement, and when one explanation is better than another, is given. Since all mathematical facts must be true in all causal models, and hence known by an agent, mathematical facts cannot be part of an explanation (under the standard notion of explanation). This problem is solved using impossible possible worlds.
    
[^135]: 通过信息论奖励建模来减轻奖励作弊问题

    Mitigating Reward Hacking via Information-Theoretic Reward Modeling

    [https://arxiv.org/abs/2402.09345](https://arxiv.org/abs/2402.09345)

    本文提出了一种名为InfoRM的奖励建模框架，通过引入变分信息瓶颈目标和模型复杂度调节机制，解决了奖励作弊问题，并利用集成聚类偏差得分（ICDS）来检测奖励过度优化。

    

    尽管强化学习从人类反馈（RLHF）中的成功在与人类价值观的语言模型的对齐方面，奖励作弊问题，也被称为奖励过度优化，仍然是一个关键挑战，主要源于奖励建模的局限性，即奖励模型的泛化能力和偏好数据集的不一致性。在这项工作中，我们从信息论的视角来解决这个问题，并提出了一种可推广和鲁棒的奖励建模框架，称为InfoRM，通过引入变分信息瓶颈目标来过滤出不相关的信息，并开发一种模型复杂度调节机制。值得注意的是，我们进一步发现了过度优化与潜变量空间的异常值之间的相关性，将InfoRM作为检测奖励过度优化的一种有前途的工具。受到这一发现的启发，我们提出了集成聚类偏差得分（ICDS），用于量化过优化问题。

    arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
    
[^136]: 适用于逆问题解决的神经网络渐近行为的研究

    Neural Networks asymptotic behaviours suitable for the resolution of inverse problems

    [https://arxiv.org/abs/2402.09338](https://arxiv.org/abs/2402.09338)

    本文研究了适用于解决反褶积逆问题的神经网络渐近行为，并发现使用从神经网络的渐近极限导出的高斯过程比全连接的神经网络获得更好的结果，而且观察到随着层数的增加，训练后的神经网络的准确性接近于高斯过程的准确性。其中一个高斯过程的解释与传统方法不同，提供了一种新的视角。

    

    在本文中，我们对神经网络（NN）技术在反褶积逆问题中的有效性进行了研究。我们考虑到NN的渐近极限，对应于高斯过程（GPs），其中参数的非线性性丢失。利用这些结果的GPs，我们通过在格子上使用蒙特卡洛技术模拟量子谐振子来解决反褶积逆问题。我们的研究结果表明，使用全连接的NN解决反褶积逆问题的结果不如使用从NN的渐近极限导出的GPs获得的结果好。此外，我们观察到随着层数的增加，训练后的NN的准确性接近于GPs的准确性。值得注意的是，其中一个GPs的解释与文献中的传统方法不同，提供了一种新的视角。

    arXiv:2402.09338v1 Announce Type: cross Abstract: In this paper, we perform a study on the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems. We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where parameter non-linearities are lost. Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice. A scenario with a known analytical solution. Our findings indicate that solving the deconvolution inverse problem with a fully connected NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits. Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width. Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature. Additionally, the NNs, in their a
    
[^137]: 告诉我更多！面向基于语言模型的智能代理的隐式用户意图理解

    Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents

    [https://arxiv.org/abs/2402.09205](https://arxiv.org/abs/2402.09205)

    该论文提出了一种面向基于语言模型的智能代理的隐式用户意图理解的方法。通过引入Intention-in-Interaction (IN3) 基准和在代理设计中融入模型专家，使得代理能够更好地与用户进行交互，并提升对用户指令的理解能力。

    

    当前的语言模型驱动代理常常缺乏有效的用户参与机制，考虑到用户指令中常见的模糊性，这是至关重要的。虽然这些代理在制定策略和执行任务方面表现出色，但在寻求澄清和抓住精确的用户意图方面却遇到了困难。为了填补这一差距，我们引入了Intention-in-Interaction (IN3) ，这是一个旨在通过明确的查询检查用户的隐含意图的新颖基准。接下来，我们提出将模型专家作为上游融入代理设计中，以增强用户-代理交互。利用IN3，我们经验性地训练了Mistral-Interact，这是一个强大的模型，它可以主动评估任务的模糊性，询问用户意图，并将其转化为可行的目标，然后开始下游代理任务执行。将其集成到XAgent框架中，我们对增强的代理系统进行了全面评估，以评估用户指令的理解能力。

    arXiv:2402.09205v1 Announce Type: cross Abstract: Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understand
    
[^138]: (不)理性与大型语言模型中的认知偏差

    (Ir)rationality and Cognitive Biases in Large Language Models

    [https://arxiv.org/abs/2402.09193](https://arxiv.org/abs/2402.09193)

    本研究评估了七个大型语言模型在认知心理学任务中的表现，发现它们与人类一样存在非理性，但展示的非理性方式与人类偏见不同，同时还表现出了显著的回答不一致性。

    

    大型语言模型(LLMs)是否展现出理性推理？由于其训练数据所含的人类偏见，LLMs已被证实存在人类偏见；然而，其是否反映出了理性推理还不太清楚。本文通过评估七个语言模型在来自认知心理学文献的任务中回答了这个问题。我们发现，和人类一样，LLMs在这些任务中展现出了非理性。然而，LLMs展现出的这种非理性与人类的偏见不同。当LLMs给出错误答案时，它们通常会以与人类偏见不同的方式错误。除此之外，LLMs还展现出了响应的显著不一致性，这表明了额外的非理性层面。除了实验结果，本文还通过展示如何评估和比较这类模型的不同功能，对方法论作出了贡献。

    arXiv:2402.09193v1 Announce Type: cross Abstract: Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with r
    
[^139]: 探索大型语言模型的对抗能力

    Exploring the Adversarial Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.09132](https://arxiv.org/abs/2402.09132)

    本研究探索了大型语言模型的对抗能力，并发现其能够成功地制造对抗性示例以愚弄安全措施，特别是在仇恨言论检测方面具有重大影响。

    

    大型语言模型（LLMs）的普及引发了广泛和普遍的兴趣，因为它们具有强大的语言生成能力，为行业和研究提供了巨大的潜力。尽管以前的研究探讨了LLMs的安全性和隐私问题，但这些模型能否表现出对抗行为的程度仍然尚未完全探索。为了填补这一空白，我们研究常见的公开可用LLMs是否具有能力扰乱文本样本以愚弄安全措施，即所谓的对抗示例或攻击。更具体地说，我们调查LLMs是否本质上能够从良性样本中制造对抗性示例以愚弄现有的安全防线。我们的实验重点关注仇恨言论检测，发现LLMs成功地找到了对抗性扰动，有效地破坏了对仇恨言论检测系统的防御。我们的发现对（半）自动化安全评估和防御具有重要影响。

    arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
    
[^140]: FGeo-DRL: 通过深度强化学习进行几何问题的演绎推理

    FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning

    [https://arxiv.org/abs/2402.09051](https://arxiv.org/abs/2402.09051)

    本文介绍了FGeo-DRL，一个用于自动执行几何演绎推理的神经符号系统。通过强化学习算法和机器学习模型，该系统能够自主学习解决几何问题的方法，实现了人类化的演绎推理。

    

    自动人类化的演绎推理一直以来都是数学和人工智能交叉学科中最具挑战性的开放问题之一。本文是我们工作系列中的第三篇。我们建立了一个名为FGeoDRL的神经符号系统，用于自动执行人类化几何演绎推理。神经部分是基于强化学习的人工智能代理，能够通过对形式化环境的反馈自主地学习解决问题的方法，无需人类监督。它利用预训练的自然语言模型建立了一个策略网络，用于定理选择，并使用蒙特卡洛树搜索进行启发式探索。符号部分是基于几何形式化理论和FormalGeo\cite{FormalGeo}的强化学习环境，将GPS模型化为马尔科夫决策过程\cite{MDP}。在这个形式化符号系统中，已知条件和

    arXiv:2402.09051v1 Announce Type: new Abstract: The human-like automatic deductive reasoning has always been one of the most challenging open problems in the interdiscipline of mathematics and artificial intelligence. This paper is the third in a series of our works. We built a neural-symbolic system, called FGeoDRL, to automatically perform human-like geometric deductive reasoning. The neural part is an AI agent based on reinforcement learning, capable of autonomously learning problem-solving methods from the feedback of a formalized environment, without the need for human supervision. It leverages a pre-trained natural language model to establish a policy network for theorem selection and employ Monte Carlo Tree Search for heuristic exploration. The symbolic part is a reinforcement learning environment based on geometry formalization theory and FormalGeo\cite{FormalGeo}, which models GPS as a Markov Decision Process\cite{MDP}. In this formal symbolic system, the known conditions and 
    
[^141]: 朝着更好的人机对齐方向：评估LLM驱动应用中的任务效用

    Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications

    [https://arxiv.org/abs/2402.09015](https://arxiv.org/abs/2402.09015)

    本研究引入了AgentEval框架，用于评估LLM驱动应用的任务效用。该框架通过自动提出一套针对特定应用的评估标准，简化了效用验证过程，并对应用的效用进行了全面量化分析。

    

    大型语言模型（LLM）领域的快速发展导致了一系列应用的出现，这些应用通过协助多个代理人与人类合作，帮助人们完成日常任务。然而，目前仍存在一个重大问题，即如何评估LLM驱动应用是否真正提升用户体验和任务执行效率。这凸显了验证LLM驱动应用效用的方法的迫切需求，特别是要确保应用程序的功能与最终用户的需求相一致。我们引入了AgentEval，它提供了一个实施数学问题的估测模型，这是一个新的框架，旨在通过自动提出一套针对任何给定应用程序独特目标的评估标准，简化效用验证过程。这样可以对应用程序的效用进行全面评估，并量化其与建议标准相比的表现。我们对该框架的稳健性进行了全面的分析。

    arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
    
[^142]: DNABERT-S: 学习具有基因组基础模型的物种感知DNA嵌入

    DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models

    [https://arxiv.org/abs/2402.08777](https://arxiv.org/abs/2402.08777)

    DNABERT-S是一种专门用于创建物种感知的DNA嵌入的基因组基础模型。为了提高对长读DNA序列的嵌入效果，引入了Manifold Instance Mixup (MI-Mix)对比目标方法来训练模型。

    

    有效的DNA嵌入在基因组分析中仍然至关重要，特别是在缺乏用于模型微调的标记数据的情况下，尽管基因组基础模型已经取得了显著进展。一个典型的例子是宏基因组分箱，这是微生物组研究中的一个关键过程，旨在通过来自可能包含成千上万个不同的、通常没有经过表征的物种的复杂混合DNA序列的物种来对DNA序列进行分组。为了填补有效的DNA嵌入模型的缺陷，我们引入了DNABERT-S，这是一个专门用于创建物种感知的DNA嵌入的基因组基础模型。为了鼓励对易出错的长读DNA序列进行有效嵌入，我们引入了Manifold Instance Mixup(MI-Mix)，一种对比目标，它在随机选择的层次中混合DNA序列的隐藏表示，并训练模型以在输出层识别和区分这些混合比例。

    arXiv:2402.08777v1 Announce Type: cross Abstract: Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model fine-tuning, despite the significant advancements in genome foundation models. A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species. To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome foundation model that specializes in creating species-aware DNA embeddings. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enha
    
[^143]: 用分类器驱动的方法揭示大型语言模型中的五大人格特质：文本分析

    Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach

    [https://arxiv.org/abs/2402.08341](https://arxiv.org/abs/2402.08341)

    本研究使用分类器驱动的方法，通过不同的输入提示探究大型语言模型的输出变化，以增加其透明度。结果显示，这些模型根据输入的不同提示而表现出不同的人格特质，类似于人类对刺激做出的反应。

    

    大型语言模型（LLMs）在招聘背景下被应聘者和雇主广泛使用，然而这也引发了众多伦理问题，特别是与这些“黑盒子”模型缺乏透明度有关。尽管先前的研究试图通过调查LLMs的人格特质来增加其透明度，但许多先前的研究都要求模型来完成人格评估。相反，本研究旨在通过检查不同输入提示下模型的输出变化来更好地理解这些模型。具体来说，我们使用从常见面试问题和旨在引发特定的五大人格特质的提示来进行新颖的调查方法，以检查模型是否像人类一样容易激活特定人格特质，并根据其输出中的语言来评估其人格。为此，我们反复提供提示。

    Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompte
    
[^144]: 使用语言反馈模型来改进政策

    Policy Improvement using Language Feedback Models

    [https://arxiv.org/abs/2402.07876](https://arxiv.org/abs/2402.07876)

    本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。

    

    我们引入了语言反馈模型（LFMs），用于在指令遵循中识别期望的行为-有助于实现指令中指定任务的行动-以进行模仿学习。为了训练LFMs，我们从大型语言模型（LLMs）获取对视觉轨迹进行语言描述的反馈。首先，通过使用LFMs识别期望模仿的行为，我们在三种不同的语言基础环境（Touchdown，ScienceWorld和ALFWorld）上，在任务完成率上改善了强行为克隆的基线方法。其次，与LLMs直接预测行动相比，使用LFMs在LLM输出标记的数量相同的情况下表现更好。第三，LFMs适应未见环境，通过一轮适应使任务完成率提高了3.5-12.0％。最后，可以修改LFM以提供人类可解释的反馈，无需性能损失，从而允许人类验证模仿学习的期望行为。

    We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
    
[^145]: OS-Copilot: 基于自我改进的通用计算机代理

    OS-Copilot: Towards Generalist Computer Agents with Self-Improvement

    [https://arxiv.org/abs/2402.07456](https://arxiv.org/abs/2402.07456)

    OS-Copilot是一个通用计算机代理的框架，能够与操作系统中的各种元素进行交互，包括网络、代码终端、文件、多媒体和第三方应用程序。使用OS-Copilot构建的自我提升的FRIDAY代理在各种计算机任务上表现出强大的泛化能力，并在通用人工智能助手基准测试中超过以前的方法35%。

    

    与计算机的自主交互一直是一个具有巨大潜力的长期挑战，最近大型语言模型（LLM）的普及显著加快了数字代理的构建进展。然而，大多数这些代理被设计用于与特定软件或网站等狭窄领域进行交互，这限制了它们在通用计算机任务中的适用性。为此，我们引入了OS-Copilot，一个构建通用代理的框架，能够与操作系统（OS）中的全面元素进行交互，包括网络、代码终端、文件、多媒体和各种第三方应用程序。我们使用OS-Copilot创建了FRIDAY，一个能够自我提升的具象化代理，用于自动化通用计算机任务。在GAIA，一个通用人工智能助手基准测试中，FRIDAY的性能超过了以前的方法35%，通过从先前任务中积累的技能，展示了对未见应用的强大概括能力。我们还提出了数字和定量的...

    Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative
    
[^146]: 代理行为的原因：意图和工具性目标

    The Reasons that Agents Act: Intention and Instrumental Goals

    [https://arxiv.org/abs/2402.07221](https://arxiv.org/abs/2402.07221)

    本论文提出了一个对代理行为意图的操作化定义，并通过一些例子和结果展示了其灵活性和适用性。这一定义有助于理解和解释机器学习系统的行为，以及相关概念如工具性目标的关系。

    

    意图是人工智能中一个重要且具有挑战性的概念。它之所以重要，是因为它是许多其他我们关心的概念的基础，例如代理、操纵、法律责任和责备。然而，将意图归因于人工智能系统是有争议的，并且没有普遍接受的适用于人工智能代理的意图理论。我们通过将代理人行为的意图转化为其选择决策的原因，对意图进行了操作化定义。我们提出了一个在结构性因果影响模型中的意图定义，紧密结合在哲学文献中关于意图的观点，并适用于真实世界的机器学习系统。通过一些例子和结果，我们展示了我们的定义捕捉到了直观的意图概念，并符合过去研究的设定要求。此外，我们还展示了我们的定义与过去概念的关联，包括实际因果性和工具性目标的概念，后者是安全人工智能代理研究中的核心思想。最后，我们演示了我们的定义如何与实际机器学习系统中的实践相结合。

    Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how 
    
[^147]: MAGNETO：边缘人体活动识别的边缘AI--隐私和个性化

    MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization

    [https://arxiv.org/abs/2402.07180](https://arxiv.org/abs/2402.07180)

    本文提出了一种名为MAGNETO的边缘AI平台，通过从云端推向边缘进行增量人体活动学习，避免了云端与边缘设备之间的数据传输，实现了数据隐私保护、低延迟处理和高度个性化。

    

    人体活动识别（HAR）是一个成熟的领域，现代机器学习（ML）技术显著推动了其发展。尽管公司成功地将HAR整合到消费品中，但它们通常依赖于预定义的活动集，这限制了用户级（边缘设备）的个性化。尽管在增量学习方面取得了进展，能够使用新数据更新模型，但这通常发生在云端，需要定期在云端和边缘设备之间进行数据传输，从而引发数据隐私问题。在本文中，我们提出了一种名为MAGNETO的边缘AI平台，将HAR任务从云端推向边缘。MAGNETO允许在边缘设备上直接进行增量人体活动学习，而无需与云端进行任何数据交换。这可以提供强大的隐私保证、低处理延迟和高度的个性化。特别地，我们在Android设备上演示了MAGNETO，从数据采集到结果可视化，验证了整个流程。

    Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visua
    
[^148]: 自然语言强化学习

    Natural Language Reinforcement Learning

    [https://arxiv.org/abs/2402.07157](https://arxiv.org/abs/2402.07157)

    本研究将自然语言表示和强化学习原则相结合，提出了自然语言强化学习（NLRL）框架，解决了强化学习在样本效率低、解释性不足和缺乏监督信号等方面的限制问题，通过实验验证了其有效性和可解释性。

    

    强化学习（RL）在学习决策任务的策略方面展现出了令人瞩目的能力。然而，RL常常面临样本效率低、解释性不足和缺乏稀疏监督信号等问题的限制。为了解决这些问题，我们从人类学习过程中汲取灵感，引入了自然语言强化学习（NLRL），创新性地将RL原则与自然语言表示结合起来。具体而言，NLRL在自然语言空间中重新定义了任务目标、策略、价值函数、Bellman方程和策略迭代等RL概念。我们还展示了如何利用最新的大型语言模型（LLM）如GPT-4来实现NLRL。对表格MDPs的初步实验表明了NLRL框架的有效性、高效性和可解释性。

    Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
    
[^149]: LLM代理可以自主黑客网站

    LLM Agents can Autonomously Hack Websites

    [https://arxiv.org/abs/2402.06664](https://arxiv.org/abs/2402.06664)

    这项研究展示了LLM代理可以自主进行网站黑客攻击，包括盲目数据库模式提取和SQL注入，而且不需要人工反馈。这种能力是由高度工具使用和利用扩展上下文能力的前沿模型赋予的。

    

    近年来，大型语言模型（LLMs）变得越来越强大，现在可以与工具交互（即调用函数）、读取文档并递归调用自己。因此，这些LLMs现在可以自主作为代理人运作。随着这些代理人能力的提升，最近的研究已经推测LLM代理人将如何影响网络安全。然而，关于LLM代理人的攻击能力，我们还知之甚少。在本研究中，我们展示了LLM代理人可以自主黑客网站，执行诸如盲目数据库模式提取和SQL注入等复杂任务，无需人工反馈。重要的是，这种能力是由具有高度工具使用和利用扩展上下文能力的前沿模型所独特赋予的。我们展示了GPT-4能够进行这样的黑客攻击，但现有的开源模型则不能。最后，我们展示了GPT-4能够自主发现网站的漏洞。

    In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.   In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in we
    
[^150]: 使用点击提示对超声图像分割进行精调的Segment Anything Model（SAM）

    ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation

    [https://arxiv.org/abs/2402.05902](https://arxiv.org/abs/2402.05902)

    本研究提出了ClickSAM，该方法使用点击提示对超声图像进行Segment Anything Model的精细调整，解决了超声图像分割中噪声干扰的问题。

    

    由于其卓越的分割准确性、多样的输入提示、训练能力和高效的模型设计，新发布的Segment Anything Model（SAM）成为图像处理中流行的工具。然而，SAM当前的模型是在一个多样的数据集上训练的，而这些数据集并没有针对医学图像，尤其是超声图像。超声图像往往有很多噪声，这使得分割重要结构变得困难。在这个项目中，我们开发了ClickSAM，它使用点击提示对超声图像进行Segment Anything Model的精细调整。ClickSAM有两个训练阶段：第一阶段使用位于真实轮廓中心的单击提示进行训练，第二阶段通过额外的正负点击提示来改善模型性能。通过将第一阶段的预测与真实掩膜进行比较，计算出真正正、假正和假负段。正点击使用真实掩膜中的真实

    The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true 
    
[^151]: NoisyICL: 一点噪音在模型参数中提高了上下文学习的性能、

    NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning

    [https://arxiv.org/abs/2402.05515](https://arxiv.org/abs/2402.05515)

    NoisyICL通过在模型参数中引入噪音，提高了上下文学习的性能和校准性，实验结果显示NoisyICL可以产生更准确、更公平的预测。

    

    上下文学习 (ICL) 在高先验偏差和不可信任的置信度的影响下，表现不佳且校准不足。以往的一些工作通过使用庞大的数据集和计算成本对语言模型进行微调以改善 ICL 的性能。在本文中，我们提出了 NoisyICL，通过随机噪音扰动模型参数来努力提高性能和校准性。我们在2个模型和12个下游数据集上的实验表明，NoisyICL可以帮助ICL产生更准确的预测。进一步的分析表明，NoisyICL使得模型能够提供更公平的预测，同时置信度更可信。因此，我们认为NoisyICL是ICL的一种有效校准方法。我们的实验代码已上传至Github。

    In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.
    
[^152]: PaDeLLM-NER：大型语言模型中的并行解码用于命名实体识别

    PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition

    [https://arxiv.org/abs/2402.04838](https://arxiv.org/abs/2402.04838)

    本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。

    

    本研究旨在使用大型语言模型（LLMs）减少命名实体识别（NER）的生成延迟。LLMs的高延迟的主要原因是顺序解码过程，该过程自回归地生成NER的所有标签和提及，显著增加了序列长度。为此，我们引入了PaDeLLM-NER（Parallel Decoding in LLM for NE），这是一种无需额外模块或架构修改即可无缝集成到现有生成模型框架中的方法。PaDeLLM-NER允许同时解码所有提及，从而减少生成延迟。实验结果显示，PaDeLLM-NER的推理速度显著提高，对英语和中文来说比自回归方法快1.76到10.22倍。与各种数据集上的最先进性能相媲美，同时维持了预测质量。

    In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
    
[^153]: 关于现代Hopfield模型计算限制的一个细粒度复杂性分析

    On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis

    [https://arxiv.org/abs/2402.04520](https://arxiv.org/abs/2402.04520)

    通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。

    

    我们从细粒度复杂性分析的角度研究了现代Hopfield模型的记忆检索动力学的计算限制。我们的主要贡献是基于模式的范数对所有可能的现代Hopfield模型的效率进行相变行为的刻画。具体来说，我们建立了对输入查询模式和记忆模式的范数的上界标准。仅在这个标准之下，假设满足Strong Exponential Time Hypothesis (SETH)，存在子二次（高效）变体的现代Hopfield模型。为了展示我们的理论，当有效标准成立时，我们提供了现代Hopfield模型使用低秩逼近的有效构造的正式示例。这包括一个计算时间的下界导出，与$\Max\{$存储的记忆模式数量，输入查询序列的长度$\}$线性缩放。此外，我们证明了记忆检索误差界和指数记忆容量。

    We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
    
[^154]: 超越线条和圆圈：揭示大型语言模型中的几何推理差距

    Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models

    [https://arxiv.org/abs/2402.03877](https://arxiv.org/abs/2402.03877)

    本文调查了大型语言模型（LLMs）在几何推理方面的能力，并发现了它们在目标变量选择和2D空间关系方面存在偏见和困难。通过引入基于LLMs的多代理体系结构，本研究提出了一种通过自我纠正、协作和不同角色专业化来提高LLMs几何推理能力的框架。

    

    大型语言模型（LLMs）在数学和算法任务方面展示了不断增长的能力，然而它们在几何推理方面的技能还未被充分探索。我们调查了LLMs在构造性几何问题解决中的能力，这是人类数学推理发展中最基础的步骤之一。我们的研究揭示了目前最先进的LLMs在这个领域面临的显著挑战，尽管在类似领域取得了许多成功。LLMs在目标变量选择方面存在偏见，并且在2D空间关系方面面临困难，经常会错误地表示和臆造对象及其放置位置。为此，我们引入了一个基于LLMs的多代理体系结构，通过进行内部对话来增强它们现有的推理潜力。这项工作强调了LLMs在几何推理中的现有限制，并通过自我纠正、协作和不同角色专业化来提高几何推理能力。

    Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
    
[^155]: SWEA:通过主题词嵌入修改改变大型语言模型中的事实知识

    SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering

    [https://arxiv.org/abs/2401.17809](https://arxiv.org/abs/2401.17809)

    提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。

    

    模型编辑近来引起了广泛关注。目前的模型编辑方法主要涉及修改模型参数或向现有模型添加附加模块。然而，前者会对LLM造成不可逆的影响，而后者会产生额外的推理开销，并且模糊的向量匹配并不总是可靠的。为了解决这些问题，我们提出了一种可扩展的主题词嵌入修改（SWEA）框架，它在推理阶段修改主题的表示，并实现编辑知识的目标。SWEA在模型外部使用精确的关键匹配，并进行可靠的主题词嵌入修改，从而保护模型的原始权重而不增加推理开销。然后，我们提出优化抑制融合方法，首先优化编辑目标的嵌入向量，然后抑制知识嵌入维度（KED）以获得最终融合的嵌入。我们因此提出了SWEAOS元方法。

    Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
    
[^156]: 合成图像有助于识别人工制作的艺术赝品

    Synthetic images aid the recognition of human-made art forgeries

    [https://arxiv.org/abs/2312.14998](https://arxiv.org/abs/2312.14998)

    通过在训练数据集中加入合成艺术作品，可以提高人工制作艺术赝品的检测性能。

    

    先前的研究已经表明，人工智能能够以卓越的准确度区分给定艺术家的真迹和人工制作的赝品，前提是有足够的训练。然而，由于已知赝品数量有限，增强赝品检测方法是非常可取的。在这项工作中，我们研究了将合成艺术作品纳入训练数据集以增强赝品检测性能的潜力。我们的研究重点是梵高的绘画作品，并发布了专门用于赝品检测的第一个数据集。为了加强我们的结果，我们还对艺术家亚美迪奥·莫迪利亚尼和拉斐尔进行了相同的分析。我们训练了一个分类器，用于区分原作品和赝品。为此，我们使用了人工制作的赝品和以知名艺术家风格仿制的作品，并将我们的训练集与类似风格的图像进行了增强。

    arXiv:2312.14998v3 Announce Type: replace-cross  Abstract: Previous research has shown that Artificial Intelligence is capable of distinguishing between authentic paintings by a given artist and human-made forgeries with remarkable accuracy, provided sufficient training. However, with the limited amount of existing known forgeries, augmentation methods for forgery detection are highly desirable. In this work, we examine the potential of incorporating synthetic artworks into training datasets to enhance the performance of forgery detection. Our investigation focuses on paintings by Vincent van Gogh, for which we release the first dataset specialized for forgery detection. To reinforce our results, we conduct the same analyses on the artists Amedeo Modigliani and Raphael. We train a classifier to distinguish original artworks from forgeries. For this, we use human-made forgeries and imitations in the style of well-known artists and augment our training sets with images in a similar style
    
[^157]: 通过数据增强和动态抽样方法增强神经定理证明能力

    Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method

    [https://arxiv.org/abs/2312.14188](https://arxiv.org/abs/2312.14188)

    本论文提出了一种名为DS-Prover的动态抽样方法，用于增强神经定理证明的能力。该方法通过动态确定应用于扩展当前目标的策略数量，并调整探索和开发之间的平衡，从而使证明搜索过程更加高效。此外，作者还通过增加训练数据集，将简化和重写策略与多个前提进行分解。

    

    定理证明是数学中的一项基本任务。随着大型语言模型（LLMs）和交互式定理证明器（ITPs）如Lean的出现，人们对将LLMs和ITPs集成以自动化定理证明的兴趣日益增长。在这种方法中，LLM生成证明步骤（策略），而ITP检查这些策略在当前目标上的适用性。这两个系统共同完成证明过程。在本文中，我们介绍了DS-Prover，一种用于定理证明的全新动态抽样方法。该方法通过动态确定要应用于扩展当前目标的策略数量，考虑到剩余时间与总分配时间之间的比较，从而使证明搜索过程更加高效，随着时间的推移调整探索和开发之间的平衡。我们还通过将简化和重写策略与多个前提进行分解来增加训练数据集。

    arXiv:2312.14188v2 Announce Type: replace  Abstract: Theorem proving is a fundamental task in mathematics. With the advent of large language models (LLMs) and interactive theorem provers (ITPs) like Lean, there has been growing interest in integrating LLMs and ITPs to automate theorem proving. In this approach, the LLM generates proof steps (tactics), and the ITP checks the applicability of the tactics at the current goal. The two systems work together to complete the proof. In this paper, we introduce DS-Prover, a novel dynamic sampling method for theorem proving. This method dynamically determines the number of tactics to apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. This makes the proof search process more efficient by adjusting the balance between exploration and exploitation as time passes. We also augment the training dataset by decomposing simplification and rewrite tactics with multiple premi
    
[^158]: 保护您的分数：具有差分隐私保障的接触追踪

    Protect Your Score: Contact Tracing With Differential Privacy Guarantees

    [https://arxiv.org/abs/2312.11581](https://arxiv.org/abs/2312.11581)

    这篇论文提出了具有差分隐私保障的接触追踪算法，以解决隐私问题限制接触追踪的部署。该算法在多种情景下展现了卓越性能，并通过在发布每个风险分数时保护个体健康状况的隐私。

    

    2020年和2021年的流行病对经济和社会产生了巨大的影响，研究表明，接触追踪算法可以在早期遏制病毒方面起到关键作用。尽管在更有效的接触追踪算法方面已经取得了重大进展，但我们认为目前的隐私问题阻碍了其部署。接触追踪算法的本质在于传递一个风险分数的通信。然而，恰恰是将这个分数传递给用户，对手可以利用这个分数来评估个体的私人健康状况。我们确定了一个现实的攻击场景，并针对这种攻击提出了具有差分隐私保障的接触追踪算法。该算法在两个最常用的基于代理的COVID19模拟器上进行了测试，并在各种情景下展现了卓越性能，特别是在逼真的测试场景中，同时发布每个风险分数时。

    arXiv:2312.11581v2 Announce Type: replace-cross  Abstract: The pandemic in 2020 and 2021 had enormous economic and societal consequences, and studies show that contact tracing algorithms can be key in the early containment of the virus. While large strides have been made towards more effective contact tracing algorithms, we argue that privacy concerns currently hold deployment back. The essence of a contact tracing algorithm constitutes the communication of a risk score. Yet, it is precisely the communication and release of this score to a user that an adversary can leverage to gauge the private health status of an individual. We pinpoint a realistic attack scenario and propose a contact tracing algorithm with differential privacy guarantees against this attack. The algorithm is tested on the two most widely used agent-based COVID19 simulators and demonstrates superior performance in a wide range of settings. Especially for realistic test scenarios and while releasing each risk score w
    
[^159]: 学习自发现：关于积极抑制人工神经网络单意义神经元的研究

    Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks

    [https://arxiv.org/abs/2312.11560](https://arxiv.org/abs/2312.11560)

    本文研究了积极抑制人工神经网络中的单意义神经元，这对于提高性能具有重要意义，并提出了一种基于自发现的方法来实现抑制。

    

    最近，随着大型语言模型的成功，自发现受到了研究界的广泛关注。与现有文献不同，我们提出了一个关键因素的假设，即在规模扩大的过程中高度促进性能的因素：减少只能与特定特征形成一对一关系的单意义神经元。单意义神经元往往更稀疏，并对大型模型的性能产生负面影响。受到这一观点的启发，我们提出了一种直观的思路来识别和抑制单意义神经元。然而，实现这一目标是一个非平凡的任务，因为没有统一的定量评估指标，简单地禁止单意义神经元并不能促进神经网络的多意思性。因此，本文提出了从自发现中学习的方法，并展开了关于积极抑制单意义神经元的研究。具体来说，我们首先提出了一种新的方法

    arXiv:2312.11560v2 Announce Type: replace-cross  Abstract: Recently, emergence has received widespread attention from the research community along with the success of large language models. Different from the literature, we hypothesize a key factor that highly promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we propose to learn from emergence and present a study on proactively inhibiting the monosemantic neurons in this paper. More specifically, we first propose a new
    
[^160]: GINN-LP：一种用于发现多元Laurent多项式方程的可解释性神经网络

    GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations

    [https://arxiv.org/abs/2312.10913](https://arxiv.org/abs/2312.10913)

    GINN-LP是一种可解释的神经网络，用于发现多元Laurent多项式方程的形式和系数。它采用了一种名为“幂项逼近块”的新型可解释性神经网络块，并通过神经网络增长策略和稀疏正则化来优化方程的表示。

    

    传统机器学习通常被视为一个黑盒优化问题，不会产生将输入和输出连接起来的可解释性函数。然而，发现这种可解释性函数的能力是可取的。在这项工作中，我们提出了GINN-LP，一种可解释的神经网络，用于发现数据集的基础方程的形式和系数，当假设方程的形式是多元Laurent多项式时。这是通过一种新的可解释性神经网络块，名为“幂项逼近块”，由对数和指数激活函数组成来实现的。GINN-LP是端到端可微分的，可以使用反向传播进行训练。我们提出了一种神经网络增长策略，能够找到代表数据的Laurent多项式中的合适项数，同时还提出了稀疏正则化方法来优化方程的稀疏性。

    arXiv:2312.10913v2 Announce Type: replace-cross  Abstract: Traditional machine learning is generally treated as a black-box optimization problem and does not typically produce interpretable functions that connect inputs and outputs. However, the ability to discover such interpretable functions is desirable. In this work, we propose GINN-LP, an interpretable neural network to discover the form and coefficients of the underlying equation of a dataset, when the equation is assumed to take the form of a multivariate Laurent Polynomial. This is facilitated by a new type of interpretable neural network block, named the "power-term approximator block", consisting of logarithmic and exponential activation functions. GINN-LP is end-to-end differentiable, making it possible to use backpropagation for training. We propose a neural network growth strategy that will enable finding the suitable number of terms in the Laurent polynomial that represents the data, along with sparsity regularization to 
    
[^161]: 更少即更多：通过强化上下文修剪提升LLM推理能力

    Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning

    [https://arxiv.org/abs/2312.08901](https://arxiv.org/abs/2312.08901)

    CoT-Influx是一种通过强化上下文修剪来提升LLM数学推理能力的方法，通过最大化有效和简明的示例输入，显著优于其他提示方法。

    

    大型语言模型（LLM）展现了令人印象深刻的能力，但在数学推理方面仍存在困难。在这项工作中，我们提出了CoT-Influx，一种将少样本链式思维（CoT）学习推向极限以改善LLM数学推理能力的新方法。由于观察到在提示中添加更简明的CoT示例可以提高LLM推理表现，CoT-Influx采用了一种从粗糙到精细的修剪器来最大化有效和简明的CoT示例的输入。修剪器首先选择尽可能多的关键CoT示例，然后修剪无关紧要的标记以适应上下文窗口。使用难度级别和推理步骤多样的数学推理数据集来训练修剪器，同时采用了专门针对数学的强化学习方法。结果是，通过在令牌中启用双倍上下文窗口大小的CoT示例，CoT-Influx在各种提示基准方法上表现显著优于其他方法。

    arXiv:2312.08901v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. A math reasoning dataset with diverse difficulty levels and reasoning steps is used to train the pruner, along with a math-specialized reinforcement learning approach. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting bas
    
[^162]: 针对增强学习代理的个性化路径补救方法

    Personalized Path Recourse for Reinforcement Learning Agents

    [https://arxiv.org/abs/2312.08724](https://arxiv.org/abs/2312.08724)

    该论文介绍了一种针对增强学习代理的个性化路径补救方法，该方法通过编辑动作路径来实现期望目标，同时保持与代理的原始路径相似度高，并且个性化适应代理的行为模式。这种方法适用于纠正或改进动作或数据序列以实现预定目标。

    

    这篇论文介绍了一种名为个性化路径补救的新方法，用于为增强学习代理生成补救路径。其目标是通过编辑给定的动作路径以达到期望的目标（例如，与代理的原始路径相比取得更好的结果），同时确保与代理的原始路径高度相似并个性化适应代理。个性化是指新路径在从策略函数中观察到的代理行为模式方面的定制程度。我们训练一个个性化的补救代理来生成这样的个性化路径，这些路径是使用考虑目标、相似性和个性化的奖励函数获得的。该方法适用于增强学习和监督学习设置，以纠正或改进动作序列或数据序列以达到预定的目标。该方法在不同的设置中进行了评估。实验证明

    arXiv:2312.08724v2 Announce Type: replace-cross  Abstract: This paper introduces Personalized Path Recourse, a novel method that generates recourse paths for a reinforcement learning agent. The goal is to edit a given path of actions to achieve desired goals (e.g., better outcomes compared to the agent's original path) while ensuring a high similarity to the agent's original paths and being personalized to the agent. Personalization refers to the extent to which the new path is tailored to the agent's observed behavior patterns from their policy function. We train a personalized recourse agent to generate such personalized paths, which are obtained using reward functions that consider the goal, similarity, and personalization. The proposed method is applicable to both reinforcement learning and supervised learning settings for correcting or improving sequences of actions or sequences of data to achieve a pre-determined goal. The method is evaluated in various settings. Experiments show
    
[^163]: InstructBooth: 指令跟随的个性化文本到图像生成

    InstructBooth: Instruction-following Personalized Text-to-Image Generation

    [https://arxiv.org/abs/2312.03011](https://arxiv.org/abs/2312.03011)

    InstructBooth是一种用于增强个性化文本到图像模型中的图像文本对齐的方法，通过使用有限数量的特定对象图片进行个性化，并利用增强学习进行微调，实现了优秀的图像文本对齐能力。

    

    arXiv:2312.03011v2 公告类型: 替代交叉公告 摘要: 使用有限数量的特定对象图片进行个性化文本到图像模型的个性化研究已经在特定主题的图像生成中进行了探索。然而，现有方法往往在与文本提示对齐方面面临挑战，原因是过度拟合于有限的训练图像。在这项工作中，我们引入了InstructBooth，一种新的方法，旨在增强个性化文本到图像模型中的图像文本对齐而不牺牲个性化能力。我们的方法首先使用唯一标识符将个性化文本到图像模型与少量特定对象图片个性化。个性化完成后，我们使用增强学习对个性化文本到图像模型进行微调，以最大化度量图像文本对齐的奖励。此外，我们提出了增加这两个过程之间协同作用的补充技术。与现有基线相比，我们的方法展示出卓越的图像文本对齐能力，同时保持了个性化能力。

    arXiv:2312.03011v2 Announce Type: replace-cross  Abstract: Personalizing text-to-image models using a limited set of images for a specific object has been explored in subject-specific image generation. However, existing methods often face challenges in aligning with text prompts due to overfitting to the limited training images. In this work, we introduce InstructBooth, a novel method designed to enhance image-text alignment in personalized text-to-image models without sacrificing the personalization ability. Our approach first personalizes text-to-image models with a small number of subject-specific images using a unique identifier. After personalization, we fine-tune personalized text-to-image models using reinforcement learning to maximize a reward that quantifies image-text alignment. Additionally, we propose complementary techniques to increase the synergy between these two processes. Our method demonstrates superior image-text alignment compared to existing baselines, while maint
    
[^164]: 可推广的Transformer预训练用于超长时间序列预测

    Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting

    [https://arxiv.org/abs/2312.00817](https://arxiv.org/abs/2312.00817)

    提出了一种名为TimelyGPT的可推广的Transformer预训练模型，该模型通过可推广的位置嵌入和循环注意力以及时间卷积模块有效地捕捉超长时间序列数据中的全局和局部时间依赖关系。

    

    大规模预训练模型（PTMs），如BERT和GPT，最近在自然语言处理和计算机视觉领域取得了巨大成功。然而，PTMs在时间序列数据上的发展滞后。这凸显了现有基于transformer的架构的局限性，特别是它们处理大规模数据和捕捉长期时间依赖性的可扩展性。本研究提出了即时生成预训练Transformer（TimelyGPT）。TimelyGPT采用可推广位置（xPos）嵌入将趋势和周期模式编码到时间序列表示中。它还集成了循环注意力和时间卷积模块，以有效地捕捉全局和局部的时间依赖关系。我们的实验表明，TimelyGPT在建模连续监测的生物信号和经常出现在纵向电磁波领域中不规则采样的时间序列数据方面表现出色。

    arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro
    
[^165]: ASI:评估深度学习模型的准确性-稳定性指数

    ASI: Accuracy-Stability Index for Evaluating Deep Learning Models

    [https://arxiv.org/abs/2311.15332](https://arxiv.org/abs/2311.15332)

    该论文引入了准确性-稳定性指数（ASI），它是一种综合考虑准确度和稳定性的定量评估深度学习模型的指标。实验结果展示了ASI的应用，提供了一个用于可视化ASI、平均准确度和变异系数的3D曲面模型。这项研究解决了深度学习模型定量基准评估指标的重要问题，并提供了一种准确评估深度学习模型准确性和稳定性的新方法。

    

    在深度学习研究中，模型的不断引入使得有效和高效的评估变得至关重要。现有方法通常强调准确度指标，忽视了稳定性。为解决这个问题，本文引入了准确性-稳定性指数（ASI），它是一种综合考虑准确度和稳定性的定量评估深度学习模型的指标。实验结果展示了ASI的应用，同时提供了一个用于可视化ASI、平均准确度和变异系数的3D曲面模型。本文解决了深度学习模型定量基准评估指标的重要问题，并提供了一种准确评估深度学习模型准确性和稳定性的新方法。文章最后还对潜在弱点进行了讨论，并概述了未来的研究方向。

    arXiv:2311.15332v2 Announce Type: replace-cross  Abstract: In the context of deep learning research, where model introductions continually occur, the need for effective and efficient evaluation remains paramount. Existing methods often emphasize accuracy metrics, overlooking stability. To address this, the paper introduces the Accuracy-Stability Index (ASI), a quantitative measure incorporating both accuracy and stability for assessing deep learning models. Experimental results demonstrate the application of ASI, and a 3D surface model is presented for visualizing ASI, mean accuracy, and coefficient of variation. This paper addresses the important issue of quantitative benchmarking metrics for deep learning models, providing a new approach for accurately evaluating accuracy and stability of deep learning models. The paper concludes with discussions on potential weaknesses and outlines future research directions.
    
[^166]: 分析Hugging Face上机器学习模型的演化和维护

    Analyzing the Evolution and Maintenance of ML Models on Hugging Face

    [https://arxiv.org/abs/2311.13380](https://arxiv.org/abs/2311.13380)

    本文通过仓库挖掘和文本分析的方式，对Hugging Face上的机器学习模型的演化和维护进行了研究。研究发现了Hugging Face的整体增长和受欢迎程度，揭示了ML领域、框架使用、作者分组等方面的趋势，同时也探讨了开发者社区中普遍存在的主题和见解以及模型的维护状态和演化情况。

    

    Hugging Face（HF）已成为机器学习（ML）模型开发和分享的重要平台。本研究通过使用HF Hub API收集的数据，对超过380,000个模型进行仓库挖掘，旨在探索HF上托管的模型的社区参与、演化和维护等方面，这些方面在现有文献中尚未全面探讨。我们首先审查了HF的整体增长和受欢迎程度，揭示了ML领域、框架使用、作者分组以及标签和数据集的演化趋势。通过对模型卡片描述的文本分析，我们还试图确定开发者社区中普遍存在的主题和见解。我们的研究进一步涵盖了模型维护方面，在这方面我们评估了ML模型的维护状态，将提交消息分类为不同的类别（校正性、完善性和适应性），分析了模型的演化情况等。

    arXiv:2311.13380v2 Announce Type: cross  Abstract: Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF, aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evol
    
[^167]: 模型市场的调节: AI中介平台的平台治理难题

    Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries

    [https://arxiv.org/abs/2311.12573](https://arxiv.org/abs/2311.12573)

    本论文研究了模型市场的调节问题，分析了AI中介平台面临的平台治理挑战，并总结了业界的相关实践，包括许可、访问和使用限制、自动内容调节以及公开政策制定。

    

    arXiv: 2311.12573v2 公告类型: replace-cross 摘要: AI开发社区越来越多地利用托管中介平台，如Hugging Face，为用户上传的模型和训练数据提供便捷访问。这些模型市场降低了成千上万用户的技术部署门槛，但也可能被用于许多潜在有害和非法的方式。在本文中，我们解释了AI系统如何既能“包含”内容又能是开放式工具，从而成为迄今为止最棘手的平台治理挑战之一。我们提供了几个案例研究来分析模型市场如何管理模型，这些案例跨越了三个具有代表性的平台，即Hugging Face、GitHub和Civitai。基于这些分析，我们总结了业界正在制定的重要（但仍然有限）应对调节需求的做法：许可、访问和使用限制、自动内容调节以及公开政策制定。

    arXiv:2311.12573v2 Announce Type: replace-cross  Abstract: The AI development community is increasingly making use of hosting intermediaries such as Hugging Face provide easy access to user-uploaded models and training data. These model marketplaces lower technical deployment barriers for hundreds of thousands of users, yet can be used in numerous potentially harmful and illegal ways. In this article, we explain ways in which AI systems, which can both `contain' content and be open-ended tools, present one of the trickiest platform governance challenges seen to date. We provide case studies of several incidents across three illustrative platforms -- Hugging Face, GitHub and Civitai -- to examine how model marketplaces moderate models. Building on this analysis, we outline important (and yet nevertheless limited) practices that industry has been developing to respond to moderation demands: licensing, access and use restrictions, automated content moderation, and open policy development.
    
[^168]: 提升流式时间序列分割的等级

    Raising the ClaSS of Streaming Time Series Segmentation

    [https://arxiv.org/abs/2310.20431](https://arxiv.org/abs/2310.20431)

    ClaSS是一种新颖、高效且高精度的流式时间序列分割算法，通过自监督时间序列分类评估同质性，并应用统计测试检测显著的变化点。

    

    今天，普遍存在的传感器发射高频数值测量流，反映了人类、动物、工业、商业和自然过程的特性。这些过程的变化，例如由外部事件或内部状态变化引起的，会表现为记录信号中的变化。流式时间序列分割（STSS）的任务是将流分割为对应于所观察的过程或实体状态的连续可变大小的分段。分割操作本身必须能够应对输入信号的频率。我们引入了ClaSS，一种新颖、高效且高精度的STSS算法。ClaSS使用自监督时间序列分类评估潜在分割的同质性，并应用统计测试来检测显著的变化点（CPs）。在我们的实验证评中使用了两个大型基准和六个真实世界的数据档案。

    arXiv:2310.20431v2 Announce Type: replace-cross  Abstract: Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, 
    
[^169]: 通过生成轨迹模型增强层次环境设计

    Enhancing the Hierarchical Environment Design via Generative Trajectory Modeling

    [https://arxiv.org/abs/2310.00301](https://arxiv.org/abs/2310.00301)

    本文通过引入层次MDP框架，提出了一种在资源约束下增强环境设计的方法，通过上层教师智能体生成适当的训练环境，以促进学生智能体的学习能力发展。

    

    无监督环境设计（UED）是一种自动生成训练环境课程的范例，使在这些环境中训练的智能体能够发展通用能力，即实现良好的零-shot转移性能。然而，现有的UED方法主要关注对开放式智能体训练的环境进行随机生成，这在资源有限的情况下，例如对生成环境数量的限制方面是不实际的。本文引入了一个层次MDP框架，用于在资源约束下进行环境设计。它由一个上层强化学习教师智能体和一个下层学生智能体的合作组成。强化学习教师可以利用先前发现的环境结构，通过观察学生智能体的策略表示在学生能力的前沿生成适当的训练环境。

    arXiv:2310.00301v2 Announce Type: replace-cross  Abstract: Unsupervised Environment Design (UED) is a paradigm for automatically generating a curriculum of training environments, enabling agents trained in these environments to develop general capabilities, i.e., achieving good zero-shot transfer performance. However, existing UED approaches focus primarily on the random generation of environments for open-ended agent training. This is impractical in scenarios with limited resources, such as the constraints on the number of generated environments. In this paper, we introduce a hierarchical MDP framework for environment design under resource constraints. It consists of an upper-level RL teacher agent that generates suitable training environments for a lower-level student agent. The RL teacher can leverage previously discovered environment structures and generate environments at the frontier of the student's capabilities by observing the student policy's representation. Moreover, to redu
    
[^170]: 傅里叶混合窗口注意力：加速长序列时间序列预测的Informer方法

    Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting

    [https://arxiv.org/abs/2307.00493](https://arxiv.org/abs/2307.00493)

    本文提出了一种名为FWin的快速本地全局窗口注意力方法，用于加速长序列时间序列预测的Informer方法。通过实验证明，该方法可以提高预测准确性并加速推断速度，同时在非线性回归模型中表现出与Softmax全注意力相媲美甚至更优的效果。

    

    我们研究了一种快速的本地全局窗口注意力方法，用于加速Informer在长序列时间序列预测中的应用。虽然窗口注意力是局部的和具有相当大的计算节约，但它缺乏捕获全局令牌信息的能力，这通过后续的傅里叶变换块进行补偿。我们的方法名为FWin，不依赖于Informer的ProbSparse注意力中的查询稀疏性假设和经验性近似。通过对单变量和多变量数据集的实验，我们证明了FWin transformers可以提高Informer的整体预测准确性，同时将其推断速度加速40%至50%。我们还在非线性回归模型中展示了学习到的FWin类型注意力在时间序列数据上通过从Informer模型的全注意力层中提取的关键向量来逼近甚至胜过基于Softmax全注意力的方法。

    arXiv:2307.00493v2 Announce Type: replace-cross  Abstract: We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention is local and a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 40 to 50 %. We also show in a nonlinear regression model that a learned FWin type attention approaches or even outperforms softmax full attention based on key vectors extracted from an Informer model's full attention layer acting on time series data.
    
[^171]: 零阶优化遇到人工反馈：通过排名预测实现可证明学习

    Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles

    [https://arxiv.org/abs/2303.03751](https://arxiv.org/abs/2303.03751)

    零阶优化算法ZO-RankSGD解决了一个新兴的优化挑战，即只能通过排名预测来评估黑盒目标函数。该算法利用一种新颖的随机估计器来确定下降方向，并保证收敛到一个稳定点。此外，该算法还可用于增强学习中的策略优化问题，特别是当只有对于回报排名的排名预测时。

    

    在这项研究中，我们探讨了一种新兴的优化挑战，其中涉及到一个只能通过排名预测来评估的黑盒目标函数-这种情况在实际场景中经常遇到，特别是当函数由人类评判员评估时。这种挑战受到了强化学习与人工反馈（RLHF）的启发，这是一种最近用来提高大型语言模型（LLMs）性能的方法。我们引入了一种创新的零阶优化算法ZO-RankSGD来解决这个优化问题，并提供了理论保证。我们的算法利用一种新颖的基于排名的随机估计器来确定下降方向，并保证收敛到一个稳定点。此外，ZO-RankSGD可以直接应用于增强学习中的策略优化问题，特别是当只有对于回报排名的排名预测时。

    arXiv:2303.03751v2 Announce Type: replace-cross  Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are a
    
[^172]: 基于cGAN的增强人体活动识别的高维IMU传感器数据生成

    cGAN-Based High Dimensional IMU Sensor Data Generation for Enhanced Human Activity Recognition in Therapeutic Activities

    [https://arxiv.org/abs/2302.07998](https://arxiv.org/abs/2302.07998)

    本论文开发了一种基于cGAN的TheraGAN网络，用于生成与康复活动相关的高维IMU传感器数据。通过引入简单活动，简化了生成过程。该方法能够帮助解决传统活动识别分类器中训练数据不足的问题。

    

    人体活动识别是康复、健康监测和人机交互等应用的核心技术。可穿戴设备，尤其是IMU传感器，以相对较低的成本提供了丰富的人体运动特征，可用于活动识别。开发鲁棒的活动识别分类器一直是研究人员关注的一个主要问题。主要问题之一是通常存在训练数据不足的问题，这使得开发深度分类器变得困难，有时甚至不可能。在本文中，开发了一种新颖的GAN网络TheraGAN，用于生成与康复活动相关的IMU信号。生成的信号包括来自6个通道的IMU数据，即角速度和线性加速度。此外，引入简单活动简化了不同长度活动的生成过程。为了评估生成的信号，进行了几个定性实验和定量实验。

    arXiv:2302.07998v2 Announce Type: replace-cross  Abstract: Human activity recognition is a core technology for applications such as rehabilitation, health monitoring, and human-computer interactions. Wearable devices, especially IMU sensors, provide rich features of human movements at a reasonable cost, which can be leveraged in activity recognition. Developing a robust classifier for activity recognition has always been of interest to researchers. One major problem is that there is usually a deficit of training data, which makes developing deep classifiers difficult and sometimes impossible. In this work, a novel GAN network called TheraGAN was developed to generate IMU signals associated with rehabilitation activities. The generated signal comprises data from a 6-channel IMU, i.e., angular velocities and linear accelerations. Also, introducing simple activities simplified the generation process for activities of varying lengths. To evaluate the generated signals, several qualitative 
    
[^173]: 使用给定的子任务分解学习复杂的团队合作任务

    Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition

    [https://arxiv.org/abs/2302.04944](https://arxiv.org/abs/2302.04944)

    通过使用专家提供的任务分解为更简单的多智能体子任务，并将其转移到目标任务中进行集体调整，我们的方法可以有效地学习复杂的多智能体任务，并在解决复杂目标任务所需的时间步数上实现了显著的减少。

    

    通过多智能体强化学习训练团队完成复杂任务可能面临诸如在大型联合策略空间中搜索策略和因互相适应而导致的非稳定性等挑战。为了促进对复杂多智能体任务的高效学习，我们提出了一种方法，该方法使用专家提供的任务分解为更简单的多智能体子任务。在每个子任务中，对整个团队的子集进行训练以获取特定于子任务的策略。然后将子团队合并并迁移到目标任务中，在那里他们的策略被集体调整以解决更复杂的目标任务。我们通过实验证明，这种方法可以显著减少解决复杂目标任务所需的时间步数，相对于从头开始训练。然而，我们还发现并研究了基于子任务分解的天真实现方法的两个问题。

    arXiv:2302.04944v2 Announce Type: replace-cross  Abstract: Training a team to complete a complex task via multi-agent reinforcement learning can be difficult due to challenges such as policy search in a large joint policy space, and non-stationarity caused by mutually adapting agents. To facilitate efficient learning of complex multi-agent tasks, we propose an approach which uses an expert-provided decomposition of a task into simpler multi-agent sub-tasks. In each sub-task, a subset of the entire team is trained to acquire sub-task-specific policies. The sub-teams are then merged and transferred to the target task, where their policies are collectively fine-tuned to solve the more complex target task. We show empirically that such approaches can greatly reduce the number of timesteps required to solve a complex target task relative to training from-scratch. However, we also identify and investigate two problems with naive implementations of approaches based on sub-task decomposition, 
    
[^174]: 关于风险敏感指数成本马尔可夫决策过程中修改的策略迭代的收敛性的研究

    On the Convergence of Modified Policy Iteration in Risk Sensitive Exponential Cost Markov Decision Processes

    [https://arxiv.org/abs/2302.03811](https://arxiv.org/abs/2302.03811)

    这项研究证明了在有限状态和动作空间的情况下，修改的策略迭代算法（MPI）在风险敏感问题中的收敛性，并提供了与已有结果不同的证明方法。

    

    修改的策略迭代（MPI）是一种将策略迭代和值迭代相结合的动态规划算法。MPI的收敛性在折扣和平均成本MDP的背景下已经得到了广泛研究。本文研究了指数成本风险敏感MDP的形式，该形式对模型参数具有一定的鲁棒性。虽然针对风险敏感MDP已经对策略迭代和值迭代进行了深入研究，但MPI却未被探索。我们首次证明了在有限状态和动作空间的情况下，MPI也对风险敏感问题收敛。由于指数成本形式涉及乘法贝尔曼方程，我们的主要贡献是一种与折扣和风险中立平均成本问题以及风险敏感值和策略迭代方法不同的收敛证明。我们总结了我们的一个

    arXiv:2302.03811v2 Announce Type: replace-cross  Abstract: Modified policy iteration (MPI) is a dynamic programming algorithm that combines elements of policy iteration and value iteration. The convergence of MPI has been well studied in the context of discounted and average-cost MDPs. In this work, we consider the exponential cost risk-sensitive MDP formulation, which is known to provide some robustness to model parameters. Although policy iteration and value iteration have been well studied in the context of risk sensitive MDPs, MPI is unexplored. We provide the first proof that MPI also converges for the risk-sensitive problem in the case of finite state and action spaces. Since the exponential cost formulation deals with the multiplicative Bellman equation, our main contribution is a convergence proof which is quite different than existing results for discounted and risk-neutral average-cost problems as well as risk sensitive value and policy iteration approaches. We conclude our a
    
[^175]: FedMT: 混合类型标签的联邦学习

    FedMT: Federated Learning with Mixed-type Labels

    [https://arxiv.org/abs/2210.02042](https://arxiv.org/abs/2210.02042)

    本文提出了一种概念新颖的联邦学习设置，即具有混合类型标签的联邦学习，在其中不同的中心可以使用不同的标签准则。为了有效地训练具有混合类型标签的模型，作者提出了一种理论指导和模型无关的方法。

    

    在联邦学习（FL）中，分类器（例如深度网络）在多个中心的数据集上进行训练，而无需在这些中心之间交换数据，从而提高了样本效率。在传统的FL设置中，通常在所有参与训练的中心中采用相同的标签准则。这个限制极大地限制了FL的适用性。例如，在疾病诊断中使用的标准很可能在临床中心之间存在差异，这与传统FL的设置不匹配。在本文中，我们考虑了一个重要但尚未充分探索的FL设置，即具有混合类型标签的FL，其中各个中心可以使用不同的标签准则，从而导致中心间标签空间的差异，并对为传统设置设计的现有FL方法提出了挑战。为了有效而高效地训练具有混合类型标签的模型，我们提出了一种基于理论指导和模型无关的方法

    arXiv:2210.02042v3 Announce Type: replace-cross Abstract: In federated learning (FL), classifiers (e.g., deep networks) are trained on datasets from multiple centers without exchanging data across them, and thus improves sample efficiency. In the classical setting of FL, the same labeling criterion is usually employed across all centers being involved in training. This constraint greatly limits the applicability of FL. For example, standards used for disease diagnosis are more likely to be different across clinical centers, which mismatches the classical FL setting. In this paper, we consider an important yet under-explored setting of FL, namely FL with mixed-type labels where different labeling criteria can be employed by various centers, leading to inter-center label space differences and challenging existing FL methods designed for the classical setting. To effectively and efficiently train models with mixed-type labels, we propose a theory-guided and model-agnostic approach that ca
    
[^176]: PixTrack：使用NeRF模板和特征度量对物体的6DoF姿态进行精确跟踪

    PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and Feature-metric Alignment

    [https://arxiv.org/abs/2209.03910](https://arxiv.org/abs/2209.03910)

    PixTrack是一种基于视觉的物体姿态跟踪框架，使用NeRF模板和特征度量对齐方法，能够精确跟踪物体的6DoF姿态，而且无需数据注释或轨迹平滑。方法具有高度精确、鲁棒且无抖动的特点，同时计算效率高，可用于多目标跟踪。

    

    我们提出了PixTrack，一种基于视觉的物体姿态跟踪框架，使用新颖的视图合成和深度特征度量对齐。我们遵循基于SfM的重新定位范式，使用神经辐射场来规范地表示被跟踪的物体。我们的评估表明，我们的方法在单目RGB图像和RGB-D图像中产生高度精确、鲁棒且无抖动的物体6DoF姿态估计，无需任何数据注释或轨迹平滑。我们的方法也具有计算效率高的特点，通过简单的CPU多进程可以实现多目标跟踪而无需改变我们的算法。我们的代码可在以下链接找到：https://github.com/GiantAI/pixtrack

    arXiv:2209.03910v2 Announce Type: replace-cross  Abstract: We present PixTrack, a vision based object pose tracking framework using novel view synthesis and deep feature-metric alignment. We follow an SfM-based relocalization paradigm where we use a Neural Radiance Field to canonically represent the tracked object. Our evaluations demonstrate that our method produces highly accurate, robust, and jitter-free 6DoF pose estimates of objects in both monocular RGB images and RGB-D images without the need of any data annotation or trajectory smoothing. Our method is also computationally efficient making it easy to have multi-object tracking with no alteration to our algorithm through simple CPU multiprocessing. Our code is available at: https://github.com/GiantAI/pixtrack
    
[^177]: ED2: 连续控制的环境动力学分解世界模型

    ED2: Environment Dynamics Decomposition World Models for Continuous Control

    [https://arxiv.org/abs/2112.02817](https://arxiv.org/abs/2112.02817)

    提出了一种环境动力学分解世界模型构建框架ED2，能够通过发现子动力学并进行分解预测，更准确地构建世界模型。

    

    Model-based reinforcement learning (MBRL)在实践中相对于model-free RL实现了显著的样本效率，但其性能常常受限于模型预测误差的存在。为了减少模型误差，标准的MBRL方法训练一个精心设计的网络来拟合整个环境动力学，但这浪费了可以分别建模的多个子动力学的丰富信息，从而能更准确地构建世界模型。本文提出了环境动力学分解（ED2）的创新世界模型构建框架，其以一种分解的方式对环境进行建模。ED2包含两个关键组成部分：子动力学发现（SD2）和动力学分解预测（D2P）。SD2能够自动发现环境中的子动力学，然后D2P根据这些子动力学构建分解的世界模型。ED2可以与现有方法轻松结合使用。

    arXiv:2112.02817v2 Announce Type: replace-cross  Abstract: Model-based reinforcement learning (MBRL) achieves significant sample efficiency in practice in comparison to model-free RL, but its performance is often limited by the existence of model prediction error. To reduce the model error, standard MBRL approaches train a single well-designed network to fit the entire environment dynamics, but this wastes rich information on multiple sub-dynamics which can be modeled separately, allowing us to construct the world model more accurately. In this paper, we propose the Environment Dynamics Decomposition (ED2), a novel world model construction framework that models the environment in a decomposing manner. ED2 contains two key components: sub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2 discovers the sub-dynamics in an environment automatically and then D2P constructs the decomposed world model following the sub-dynamics. ED2 can be easily combined with existing
    
[^178]: 决策理论基础对评估人类决策的实验的影响

    Decision Theoretic Foundations for Experiments Evaluating Human Decisions. (arXiv:2401.15106v1 [cs.HC])

    [http://arxiv.org/abs/2401.15106](http://arxiv.org/abs/2401.15106)

    该论文通过综合统计决策理论和信息经济学，提出了决策问题的广泛适用定义。为了将人类决策的下降归咎于偏见形式，实验必须向参与者提供足够的信息来识别规范决策。然而，根据作者对AI辅助决策的研究的评估，只有17%的研究提供了足够的信息来描述参与者的行为偏离了良好的决策。

    

    信息展示的决策是可解释AI、人工智能与人类的合作以及数据可视化等领域研究的重点。然而，决策问题的定义以及实验必须具备的条件以得出人类决策存在缺陷的结论仍然存在争议。我们提出了一个广泛适用的决策问题定义，该定义是从统计决策理论和信息经济学中综合提炼而来的。我们认为，要将人类绩效下降归咎于某种偏见形式，实验必须向参与者提供足够的信息，以便合理的代理能够识别规范决策。我们评估了最近有关AI辅助决策的文献中对决策制定进行的评估在多大程度上达到了这一标准。我们发现，只有35项声称确定了有偏差行为的研究中的6项（17%）向参与者提供了足够信息来描述其行为偏离良好决策

    Decision-making with information displays is a key focus of research in areas like explainable AI, human-AI teaming, and data visualization. However, what constitutes a decision problem, and what is required for an experiment to be capable of concluding that human decisions are flawed in some way, remain open to speculation. We present a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the normative decision. We evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making
    
[^179]: 大型语言模型的自我解释是否可靠?

    Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07927](http://arxiv.org/abs/2401.07927)

    大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。

    

    经过训练的大型语言模型在许多任务上表现出色，甚至能够提供其行为的解释。由于这些模型对公众是直接可访问的，因此存在这样的风险，即令人信服但错误的解释可能导致对大型语言模型的无支撑的自信。因此，解释能力和可靠性是AI安全的重要考虑因素。评估自我解释的可靠性和可解释性是一项具有挑战性的任务，因为这些模型对于人类来说过于复杂，无法注释什么是正确的解释。为了解决这个问题，我们提出使用自洽性检测作为可靠性的衡量指标。例如，如果一个大型语言模型说某组词对于做出预测很重要，那么在没有这些词的情况下，它应该无法做出相同的预测。虽然自洽性检测是一种常见的可靠性方法，但之前尚未应用于大型语言模型的自我解释中。我们将自洽性检测应用于...

    Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
    
[^180]: 你的预训练模型有改进吗？一种基于多头后验的方法

    Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])

    [http://arxiv.org/abs/2401.02987](http://arxiv.org/abs/2401.02987)

    本研究提出一种基于多头后验的方法，通过利用实体的元特征和模型的表示之间的一致性作为度量标准，有效评估预训练模型在各个领域的表现。

    

    预训练模型的出现对自然语言处理（NLP）、计算机视觉和关系型数据集等领域产生了显著影响。传统上，这些模型通过下游任务进行评估。然而，这引发了如何更高效、更有效地评估这些模型的问题。在本研究中，我们探索了一种新颖的方法，即利用与每个实体相关的元特征作为世界知识的来源，并利用模型的实体表示。我们提出使用这些表示和元特征之间的一致性作为评估预训练模型的度量标准。我们的方法在各个领域表现出了有效性，包括具有关系型数据集、大型语言模型和图像模型的模型。

    The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
    
[^181]: 大语言模型的零样本位置去偏方法

    Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])

    [http://arxiv.org/abs/2401.01218](http://arxiv.org/abs/2401.01218)

    本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。

    

    微调已被证明是改善大语言模型（LLMs）领域性能的有效方法。然而，LLMs可能适应数据集偏见和预测的捷径，导致生成性能差。实验结果显示，LLMs容易表现出位置偏差，即利用位于开头或末尾或输入中特定位置线索的信息。现有的减轻位置偏差的工作需要外部偏差知识或带注释的非偏倚样本，在实际中不太实用。在这项工作中，我们提出了一种零样本位置去偏（ZOE）框架对LLMs进行位置去偏。ZOE利用预训练的LLMs的无监督响应进行去偏，因此不需要任何外部知识或数据集。为了提高无监督响应的质量，我们提出了一种主从对齐（MSA）模块来修剪这些响应。对八个数据集和五个任务的实验表明，ZOE始终优于其他方法。

    Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
    
[^182]: 学会说母语：以母语风格激发大型语言模型的能力

    Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.13538](http://arxiv.org/abs/2311.13538)

    本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。

    

    大型语言模型（LLMs）与上下文学习（ICL）已成为许多自然语言处理任务的现代工具选择。然而，上下文示例的文本风格如何影响LLMs的性能仍然不足。本文提出了一种名为AlignedCoT的新颖有效的方法，通过将上下文示例与LLMs的母语风格对齐来提高LLMs的推理能力。 "母语"是指LLMs的固有特征，可以通过零-shot场景探测。 AlignedCoT广泛适用于ICL方法，可以轻松与最先进的技术结合，进一步提高LLMs的性能。我们在数学问答、常识推理和文本理解等多个基准测试上进行了广泛而全面的实验。实证结果表明，我们的AlignedCoT相比精心手工制作的演示文稿显著提高了性能。

    In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
    
[^183]: 通过大型语言模型在已绑定模型上实时生成和控制动画

    Real-time Animation Generation and Control on Rigged Models via Large Language Models. (arXiv:2310.17838v1 [cs.GR])

    [http://arxiv.org/abs/2310.17838](http://arxiv.org/abs/2310.17838)

    该论文介绍了一种利用大型语言模型实现在已绑定模型上实时进行动画控制和生成的方法，并展示了该方法的灵活性和鲁棒性。

    

    我们介绍了一种新的方法，使用自然语言输入在已绑定模型上进行实时动画控制和生成。首先，我们在Unity中嵌入了一个大型语言模型（LLM），用于输出可以解析为多样且逼真的动画的结构化文本。其次，我们展示了LLM实现现有动画之间灵活状态转换的潜力。我们通过在各种绑定模型和动作上展示定性结果，展示了我们方法的鲁棒性。

    We introduce a novel method for real-time animation control and generation on rigged models using natural language input. First, we embed a large language model (LLM) in Unity to output structured texts that can be parsed into diverse and realistic animations. Second, we illustrate LLM's potential to enable flexible state transition between existing animations. We showcase the robustness of our approach through qualitative results on various rigged models and motions.
    
[^184]: 绝对策略优化

    Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])

    [http://arxiv.org/abs/2310.13230](http://arxiv.org/abs/2310.13230)

    这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。

    

    近年来，基于信任域的在线策略强化学习在解决复杂控制任务和游戏场景方面取得了令人瞩目的结果。然而，这一类别中现有的最先进算法主要强调对预期性能的改进，缺乏对最坏情况下性能结果的控制能力。为了解决这个限制，我们引入了一个新颖的目标函数；通过优化该函数，可以确保近乎总体性能样本的下界（绝对性能）呈现单调改进。考虑到这一具有突破性的理论进展，我们通过一系列的近似对这个理论基础算法进行了改进，得到了一种实用的解决方案称为绝对策略优化（APO）。我们的实验证明了我们的方法在具有挑战性的连续控制基准任务上的有效性，并将其适用性扩展到掌握Atari游戏。我们的发现表明，APO在提高性能的同时也显著改善了最坏情况下的性能结果。

    In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
    
[^185]: ByteStack-ID: 基于灰度图像的网络入侵检测的集成堆叠模型，利用负载字节频率

    ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])

    [http://arxiv.org/abs/2310.09298](http://arxiv.org/abs/2310.09298)

    ByteStack-ID是一种基于灰度图像和负载字节频率的集成堆叠模型，用于数据包级入侵检测。它能迅速准确地识别网络流量中的各种攻击类型，并与传统方法有所不同。

    

    在不断发展的网络安全领域中，迅速准确地识别网络流量中的各种攻击类型至关重要。本文介绍了"ByteStack-ID"，一种专为数据包级入侵检测而设计的创新方法。ByteStack-ID核心是利用从负载数据的频率分布生成的灰度图像，这是一种突破性的技术，极大地提高了模型识别复杂数据模式的能力。值得注意的是，我们的方法完全基于数据包级信息，与传统的基于流量数据的网络入侵检测系统（NIDS）有所不同。在基本堆叠方法的基础上，ByteStack-ID与传统的堆叠方法不同。它将附加的元学习器层无缝集成到连接的基础学习器中，创建了一个高度优化的统一模型。

    In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
    
[^186]: 医学图像分析的领域泛化：综述

    Domain Generalization for Medical Image Analysis: A Survey. (arXiv:2310.08598v1 [eess.IV])

    [http://arxiv.org/abs/2310.08598](http://arxiv.org/abs/2310.08598)

    本综述详细回顾了针对医学图像分析的领域泛化研究，探讨了在DL模型在真实世界应用中遇到的挑战，以及如何解决分布漂移问题和实现稳健性。同时，考虑了领域泛化技术对整个MedIA工作流程的操作影响。

    

    医学图像分析（MedIA）已成为医学和保健领域的重要工具，在疾病诊断、预后和治疗规划方面起到了很大的作用，深度学习（DL）的最新成功为其进展做出了重要贡献。然而，MedIA的DL模型在现实世界中的部署仍然具有挑战性，在训练和测试样本之间的分布差异下很难泛化，这被称为分布漂移问题。研究人员致力于开发各种DL方法，使其能够适应并在未知和超出分布的数据分布上稳健地运行。本文综合评述了专门针对MedIA的领域泛化研究。我们提供了领域泛化技术在更大范围MedIA系统内的交互方式的整体视图，不仅仅考虑方法学，还考虑了对整个MedIA工作流程的操作影响。具体而言，我们将领域泛化方法分为数据层次的方法…

    Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-lev
    
[^187]: 通过Sobolev训练的二维Copula逼近变换：2-Cats网络

    Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])

    [http://arxiv.org/abs/2309.16391](http://arxiv.org/abs/2309.16391)

    本文介绍了一种通过Sobolev训练的2-Cats网络，它能够非参数地逼近任何二维Copula，并且在估计输出方面优于现有技术。

    

    Copula是一种强大的统计工具，用于捕捉数据维度之间的依赖关系。在应用Copula时，我们可以通过首先估计独立的边际分布（一个简单任务），然后估计连接边际的单个Copula函数C（一个困难任务）来估计多元分布函数。对于二维数据，Copula是一个形如C：(u，v)∈\mathbf{I}^2\rightarrow \mathbf{I}的二次增函数，其中\mathbf{I}=[0，1]。在本文中，我们展示了神经网络（NNs）如何能够非参数地逼近任何二维Copula。我们的方法被称为2-Cats，受到物理启发的神经网络和Sobolev训练文献的启发。我们不仅证明了我们能够比现有技术更好地估计2D Copula的输出，而且我们的方法是非参数的，并且符合Copula C的数学性质。

    Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
    
[^188]: 对于神经网络的大批量训练泛化性能的LARS再审视

    Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14053](http://arxiv.org/abs/2309.14053)

    本文通过对大批量训练技术的研究，提出了一种新的算法TVLARS，该算法利用可配置的函数替代了热身阶段，以实现对于神经网络的稳健训练。实验证明，在大多数情况下，TVLARS比LARS和LAMB都有更好的性能表现，特别是在自监督学习方面。

    

    本文通过在不同场景下使用逐层自适应缩放比(LARS)来探索大批量训练技术，揭示了一些见解。具有热身阶段的LARS算法由于冗余的比例缩放导致在早期陷入尖锐的极小化器。此外，后期固定的陡峭下降限制了深度神经网络有效地遍历早期尖锐的极小化器。基于这些发现，我们提出了一种新的算法Time Varying LARS (TVLARS)，它用可配置的类似sigmoid函数替代了热身阶段，以实现在初始阶段的稳健训练。TVLARS在早期促进了梯度探索，超越了尖锐的优化器，并逐渐过渡到LARS以实现后期的稳健性。广泛的实验表明，在大多数情况下，TVLARS始终优于LARS和LAMB，分类场景中的改进达到2\%。值得注意的是，在所有自监督学习的案例中，TVLARS都胜过了LARS和LAMB，并且性能提升了

    This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
    
[^189]: 逃离样本陷阱：使用配对距离估计器快速准确地估计认识不确定性

    Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])

    [http://arxiv.org/abs/2308.13498](http://arxiv.org/abs/2308.13498)

    本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。

    

    本文介绍了一种使用配对距离估计器（PaiDEs）对集成模型进行认识不确定性估计的新方法。这些估计器利用模型组件之间的配对距离来建立熵的边界，并将这些边界作为基于信息准则的估计值。与最近基于样本的蒙特卡洛估计器用于认识不确定性估计的深度学习方法不同，PaiDEs能够在更大的空间（最多100倍）上以更快的速度（最多100倍）估计认识不确定性，并在更高维度上具有更准确的性能。为了验证我们的方法，我们进行了一系列用于评估认识不确定性估计的实验：一维正弦数据，摆动物体（Pendulum-v0），跳跃机器人（Hopper-v2），蚂蚁机器人（Ant-v2）和人形机器人（Humanoid-v2）。对于每个实验设置，我们应用了主动学习框架来展示PaiDEs在认识不确定性估计中的优势。

    This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
    
[^190]: 将民主价值观嵌入社交媒体人工智能中的社会客观函数

    Embedding Democratic Values into Social Media AIs via Societal Objective Functions. (arXiv:2307.13912v1 [cs.HC])

    [http://arxiv.org/abs/2307.13912](http://arxiv.org/abs/2307.13912)

    本研究介绍了一种方法，通过将社会科学构造转化为人工智能目标函数，将民主价值观嵌入社交媒体人工智能系统中。通过一个应用于反民主态度的模型示例，我们展示了该方法的有效性。通过利用社会科学的调查工具和定性编码手册，我们能够精确地转化这些构造为大型语言模型的提示。

    

    我们能否设计人工智能系统，使其考虑到民主价值观，如减少党派敌意，作为其目标函数来排名我们的社交媒体信息流？我们引入了一种方法，将已建立、经审查的社会科学构造转化为人工智能目标函数，我们称之为社会客观函数，并通过应用于反民主态度这一政治科学构造来演示该方法。传统上，我们缺乏可观察的结果来对这些模型进行训练，然而社会科学已经开发了调查工具和定性编码手册，用于这些构造的翻译，其精确性便于将其转化为大型语言模型的详细提示。我们应用这种方法创建了一个民主态度模型，用于估计社交媒体帖子宣传反民主态度的程度，并在三个研究中测试了这个民主态度模型。

    Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and 
    
[^191]: 智能的本质

    Nature of Intelligence. (arXiv:2307.11114v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.11114](http://arxiv.org/abs/2307.11114)

    智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。

    

    人类大脑是人类智能的基础。通过模拟人类大脑，人工智能构建具有学习能力并执行接近人类水平的智能任务的计算模型。深度神经网络由多个计算层组成，学习数据的表示并在许多识别领域改进了最先进的技术。然而，智能的本质，即通过人类和人工智能共同代表的智能的本质，尚不清楚。在这里，我们展示智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。人类和人工智能通过以一种受强化方式消耗能量的方式实现这些减熵过程。根据这一假设，我们建立了关于语言、无意识和意识的数学模型，预测神经科学和人工智能工程实现的证据。

    The human brain is the substrate for human intelligence. By simulating the human brain, artificial intelligence builds computational models that have learning capabilities and perform intelligent tasks approaching the human level. Deep neural networks consist of multiple computation layers to learn representations of data and improve the state-of-the-art in many recognition domains. However, the essence of intelligence commonly represented by both humans and AI is unknown. Here, we show that the nature of intelligence is a series of mathematically functional processes that minimize system entropy by establishing functional relationships between datasets over space and time. Humans and AI have achieved intelligence by implementing these entropy-reducing processes in a reinforced manner that consumes energy. With this hypothesis, we establish mathematical models of language, unconsciousness and consciousness, predicting the evidence to be found by neuroscience and achieved by AI engineer
    
[^192]: ODD: 一份基于自然语言处理的药物滥用异常行为检测的基准数据集

    ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])

    [http://arxiv.org/abs/2307.02591](http://arxiv.org/abs/2307.02591)

    这个研究介绍了一份名为ODD的新型基准数据集，用于通过分析患者的电子健康记录笔记，检测和分类药物滥用异常行为。这个数据集在药物相关病例的自然语言处理研究中具有重要的创新和贡献。

    

    药物滥用异常行为（ORAB）是防止药物过量的新风险因素。以往，ORAB主要通过调查结果和药物给予监测进行评估。然而，这些方法无法扩展，并不能涵盖所有异常行为的范围。然而，ORAB在电子健康记录笔记中广泛有记录。本文介绍了一个名为ODD的新型生物医学自然语言处理基准数据集，用于ORAB检测。ODD是一个专家注释的数据集，包括750多个公开可用的电子健康记录笔记。ODD旨在从患者的电子健康记录笔记中识别ORAB，并将其分类为九个类别：1）已确认异常行为，2）暗示的异常行为，3）阿片类药物，4）适应症，5）已诊断的阿片制剂依赖，6）苯二氮平类药物，7）药物变化，8）与中枢神经系统相关，9）社会健康决定因素。

    Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
    
[^193]: 直接效应在总结因果图中的可辨识性

    Identifiability of direct effects from summary causal graphs. (arXiv:2306.16958v1 [cs.AI])

    [http://arxiv.org/abs/2306.16958](http://arxiv.org/abs/2306.16958)

    该论文研究了在缺乏完整时间因果图的情况下，直接因果效应如何从总结因果图中进行可辨识，并提出了一个完整的可辨识性结果。

    

    动态结构因果模型（SCMs）是一个强大的框架，用于推理动态系统中的直接效应，即衡量一个变量的变化如何影响另一个变量，同时保持其他变量不变。动态结构因果模型中的因果关系可以用完全时间因果图来进行定性表示。假设线性和因果充分性，并给定完全时间因果图，直接因果效应总是可辨识的，并可以通过调整由所谓的单门准则给出的任何变量集合来从数据中估计。然而，在许多应用中，由于各种原因没有此类图形可用，但专家仍然可以访问完全时间因果图的一个抽象，该抽象表示了时间序列之间的因果关系，同时省略了时间信息。本文提出了一个完整的可辨识性结果，其中详细描述了所有直接效应在总结因果图中可辨识的情况。

    Dynamic structural causal models (SCMs) are a powerful framework for reasoning in dynamic systems about direct effects which measure how a change in one variable affects another variable while holding all other variables constant. The causal relations in a dynamic structural causal model can be qualitatively represented with a full-time causal graph. Assuming linearity and causal sufficiency and given the full-time causal graph, the direct causal effect is always identifiable and can be estimated from data by adjusting on any set of variables given by the so-called single-door criterion. However, in many application such a graph is not available for various reasons but nevertheless experts have access to an abstraction of the full-time causal graph which represents causal relations between time series while omitting temporal information. This paper presents a complete identifiability result which characterizes all cases for which the direct effect is graphically identifiable from summa
    
[^194]: 当基于大语言模型的智能体遇到用户行为分析：一种新颖的用户模拟范式

    When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm. (arXiv:2306.02552v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2306.02552](http://arxiv.org/abs/2306.02552)

    从事用户行为分析的学术界一直面临着收集足够高质量用户行为数据的难题，一种解决方案是自动模拟用户行为，近期研究表明，利用大语言模型进行可靠的用户模拟有了重要的突破，将这种模型应用到用户行为分析研究中有着巨大潜力，可能对传统研究范式产生革命性影响。

    

    用户行为分析在以人为中心的AI应用中至关重要。然而，收集足够和高质量的用户行为数据一直是一个基本但具有挑战性的问题。为了解决这个问题，自动模拟用户行为是一个直观的想法。然而，由于人类认知过程的主观和复杂性质，可靠地模拟用户行为是困难的。最近，大语言模型（LLM）取得了显著的成功，展示了实现类似人类智能的巨大潜力。我们认为这些模型为可靠的用户模拟提供了重要机会，并有可能改变传统的用户行为分析研究范式。在本文中，我们以推荐系统为例，探索使用LLM进行用户模拟的潜力。具体而言，我们将每个用户视为基于LLM的自治智能体，并让不同智能体在虚拟环境中自由交流、行为和发展。

    User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a vir
    
[^195]: LLM和抽象推理语料库：成功、失败与基于对象表示的重要性

    LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])

    [http://arxiv.org/abs/2305.18354](http://arxiv.org/abs/2305.18354)

    本文通过分析GPT模型在抽象推理语料库上的表现，发现GPT在抽象推理任务中存在需要核心概念“核心知识”的限制。通过使用基于对象的表示方法和新的1D-ARC基准，GPT在抽象推理任务中取得了较好的表现。

    

    一个大型语言模型（LLM）能否解决简单的抽象推理问题？我们通过系统地分析GPT在抽象推理语料库（ARC）上的表现来探索这个广泛的问题，这是一个代表性的基准，从有限的例子中要求我们有些关于概念（如对象、目标状态、计数和基本几何）的“核心知识”以解决问题。当使用文本编码对二维输入输出网格的ARC任务进行编码时，GPT-4仅解决了50个最简单的任务中的13个。我们的失败分析显示，GPT-4识别对象并推理它们的能力受到表示任务中对象的文本的顺序性的显著影响。为了验证这个假设，我们设计了一个新的基准，1D-ARC，它由更有利于基于GPT的推理的一维（类似数组）任务组成，在这些任务上，GPT的表现确实比在（2D）ARC上更好。为了减轻这个问题，我们提出了一种新的基于对象的编码方案，它保留了对象之间的基本空间关系并实现了更好的推理。使用这种编码，GPT-4在ARC上的成功率大大提高，达到了45/50。我们的工作强调了使用基于对象的表示方法进行抽象推理任务的重要性，并揭示了LLM基于推理系统的局限性和机遇。

    Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some "core knowledge" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we prop
    
[^196]: SLaDe: 一种用于优化汇编代码的可移植小型语言模型反编译器

    SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly. (arXiv:2305.12520v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2305.12520](http://arxiv.org/abs/2305.12520)

    SLaDe是一种基于小型语言模型的反编译器，通过训练真实代码的序列到序列变换器，使用了新颖的分词器、无丢弃训练和类型推断等方法，能够生成更易读和更准确的程序。

    

    反编译是一个研究较为广泛的领域，有许多高质量的工具可供使用。这些工具通常用于安全任务和移植遗留代码。然而，它们经常生成难以阅读的程序，并且需要大量的工程工作来支持新的编程语言和指令集架构。最近关注神经方法产生了能生成可读代码的可移植工具。然而，迄今为止，这些技术通常只适用于没有优化的合成程序，并且没有模型评估它们的可移植性。此外，虽然生成的代码可能更易读，但通常是不正确的。本文介绍了SLaDe，一种基于序列到序列变换器训练的小型语言模型反编译器，该变换器是使用实际代码进行训练的。我们开发了一种新颖的分词器，并利用无丢弃训练来产生高质量的代码。我们利用类型推断生成比标准分析方法和最近的神经方法更易读和更准确的程序。

    Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unli
    
[^197]: 带有外部性的固定价格数据市场的均衡和学习

    Equilibrium and Learning in Fixed-Price Data Markets with Externality. (arXiv:2302.08012v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.08012](http://arxiv.org/abs/2302.08012)

    这篇论文研究了固定价格数据市场中的买家之间的竞争策略，发现其中存在负面影响。文章揭示了买家之间的负外部性及其影响，并提出了相应的市场干预措施，实现了纯策略均衡，并保证了强福利性质。

    

    我们提出将现实世界中的数据市场建模为一个买家之间的同时移动博弈，其中卖家发布固定价格，而买家可以自由地从任何一组卖家购买。该模型的一个关键组成部分是买家通过购买具有竞争优势的数据对彼此产生的负外部性，这种现象因数据易于复制而加剧。我们考虑了两种情况。在更简单的完全信息设置中，我们确定了存在于存在买家外部性的纯策略纳什均衡的福利性质。我们证明，对于一类标准的外部性函数，以交易成本的形式的市场干预可以导致强福利保证的纯策略均衡。接着，我们考虑买家起始估价未知的更一般情况。

    We propose modeling real-world data markets, where sellers post fixed prices and buyers are free to purchase from any set of sellers, as a simultaneous-move game between the buyers. A key component of this model is the negative externality buyers induce on one another due to purchasing data with a competitive advantage, a phenomenon exacerbated by data's easy replicability. We consider two settings. In the simpler complete-information setting, where all buyers know their valuations, we characterize both the existence and welfare properties of the pure-strategy Nash equilibrium in the presence of buyer externality. While this picture is bleak without any market intervention, reinforcing the limitations of current data markets, we prove that for a standard class of externality functions, market intervention in the form of a transaction cost can lead to a pure-strategy equilibrium with strong welfare guarantees. We next consider a more general setting where buyers start with unknown valua
    

