# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Uncertainty in Natural Language Generation: From Theory to Applications.](http://arxiv.org/abs/2307.15703) | 本文介绍了自然语言生成中不确定性的理论、应用和分类，提出了一种更详细和真实的分类法。 |
| [^2] | [A supervised hybrid quantum machine learning solution to the emergency escape routing problem.](http://arxiv.org/abs/2307.15682) | 本文研究了使用监督式混合量子机器学习来优化自然灾害中汽车紧急疏散计划的潜力，并提出了一种新颖的混合监督学习方法来模拟最短路径算法。 |
| [^3] | [Case Studies of Causal Discovery from IT Monitoring Time Series.](http://arxiv.org/abs/2307.15678) | 本研究通过案例研究，探索了将因果推断算法应用于不同IT监控数据的挑战和潜在益处。 |
| [^4] | [Scaling Data Generation in Vision-and-Language Navigation.](http://arxiv.org/abs/2307.15644) | 这项研究提出了一种用于视觉语言导航中生成大规模数据的有效范式。通过利用逼真的环境和网络资源，合成了490万个指令轨迹对。通过使用这个大规模数据集，通过简单的模仿学习，已存在的代理的性能得到了显著提升至80%。 |
| [^5] | [We are all Individuals: The Role of Robot Personality and Human Traits in Trustworthy Interaction.](http://arxiv.org/abs/2307.15568) | 本文通过定量和定性研究，探讨了机器人个性与个体人类特质之间的关系，并发现外向个性的机器人在人们对机器人咖啡师的信任中更受欢迎和被信任，同时发现个体对机器人的态度和偏好对信任也有影响。 |
| [^6] | [Few-shot Image Classification based on Gradual Machine Learning.](http://arxiv.org/abs/2307.15524) | 该论文提出了一种基于逐渐机器学习的新方法来解决小样本图像分类问题。通过逐步标记目标图像，并利用深度主干网络提取的特征构建因子，该方法能够有效地处理知识迁移和分类挑战。 |
| [^7] | [Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation.](http://arxiv.org/abs/2307.15514) | 该论文重新审视全卷积几何特征（FCGF）用于物体6D姿态估计，并通过学习逐点判别特征以及适当的修改和训练策略超越了近期的竞争对手，并取得了最先进的性能。 |
| [^8] | [Exploring Format Consistency for Instruction Tuning.](http://arxiv.org/abs/2307.15504) | 本研究探究了指令调整的格式一致性，并提出了统一指令调整（UIT）框架，通过自动格式转换来提高泛化性能。该研究强调了格式一致性的重要性。 |
| [^9] | [ETHER: Aligning Emergent Communication for Hindsight Experience Replay.](http://arxiv.org/abs/2307.15494) | 本文提出了ETHER，通过对齐紧急沟通来解决回顾性经验重演中的问题，克服了先前架构依赖预设函数的限制，并提高了数据效率和性能。 |
| [^10] | [A Semantic Approach to Decidability in Epistemic Planning (Extended Version).](http://arxiv.org/abs/2307.15485) | 本文提出了一种用于解决认知规划中可决定性问题的语义方法，通过增加一个交互公理来控制代理人对其他代理人知识的无限推理能力，而不是强加句法约束。实验证明这种方法是可行的，并提供了问题的有限非不动点特征。 |
| [^11] | [Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding.](http://arxiv.org/abs/2307.15484) | 本文提出了两种语音合成方法来解决自回归和非自回归模型中的问题，并在语义编码方面进行了比较研究。 |
| [^12] | [Non-invasive Diabetes Detection using Gabor Filter: A Comparative Analysis of Different Cameras.](http://arxiv.org/abs/2307.15480) | 本文比较了移动设备相机和笔记本电脑相机在非侵入性糖尿病检测中的性能，使用了Gabor滤波器和面部块纹理特征，最终实现了96.7%的准确性、100%的敏感性和93%的特异性。 |
| [^13] | [FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines.](http://arxiv.org/abs/2307.15475) | FeedbackLogs是针对机器学习流程的利益相关者反馈记录和更新的补充工具，可以用于算法审计和记录反馈更新。 |
| [^14] | [Worrisome Properties of Neural Network Controllers and Their Symbolic Representations.](http://arxiv.org/abs/2307.15456) | 本论文对神经网络控制器在简单强化学习问题中的稳健性产生了担忧，通过对低神经元和符号抽象的探究，发现即使控制器达到高回报，仍会产生大量持久低回报解决方案，对手可以轻易利用。论文提供了一个系统稳健性研究的算法，并证明了持久解决方案的存在性以及周期轨道的存在。 |
| [^15] | [From Probabilistic Programming to Complexity-based Programming.](http://arxiv.org/abs/2307.15453) | CompLog是一种基于复杂性的计算框架，通过计算Kolmogorov复杂性替代概率推理，实现计算某种情况意外性的度量，并通过规范的世界和心智模型的描述生成相关描述，并提供对析取和否定的替代方法。 |
| [^16] | [DELPHIC: Practical DEL Planning via Possibilities (Extended Version).](http://arxiv.org/abs/2307.15451) | DELPHIC是一个扩展了传统DEL的框架，在实用DEL规划方面取得了进展，通过使用可能性作为主要构建模块，DELPHIC提供了更紧凑的表示方式。 |
| [^17] | [Optimal Alignment of Temporal Knowledge Bases.](http://arxiv.org/abs/2307.15439) | 本文提出了解决时间知识库对齐问题的方法，用于最小化改变已有的知识库，同时满足给定的时间相关查询，并在成本上达到最优。方法在ALC TKBs和带有LTL运算符的连接查询上进行研究，并扩展了命题LTL对齐问题上的技术。 |
| [^18] | [Improvable Gap Balancing for Multi-Task Learning.](http://arxiv.org/abs/2307.15429) | 本文提出了两种新颖的可改进差距平衡算法（IGB）用于多任务学习，一种采用简单的启发式方法，另一种首次采用深度强化学习方法。这两种算法通过动态分配任务权重来实现可改进差距平衡，解决了损失平衡之后仍然存在的性能不平衡问题。 |
| [^19] | [A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI.](http://arxiv.org/abs/2307.15425) | 本文研究了专业编译语言模型和通用模型在检测可持续发展目标方面的比效率，关注了大型语言模型存在的偏见和敏感性挑战。研究强调了专业培训对于精确无偏的分析的必要性，并通过对一个公司描述数据集的案例研究对比了GPT-3.5和专业SDG检测模型的差异。研究结果强调了在模型选择时需要考虑任务要求、成本、复杂性和透明度。尽管大型语言模型的多功能性，但作者建议在需要精确性和准确性的任务中使用专业模型。 |
| [^20] | [Improving Social Media Popularity Prediction with Multiple Post Dependencies.](http://arxiv.org/abs/2307.15413) | 该论文提出了一种新型预测框架DSN，通过同时利用帖子内部和帖子间的依赖关系来改进社交媒体受欢迎程度的预测准确性。 |
| [^21] | [Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning.](http://arxiv.org/abs/2307.15377) | 本论文提出了一种称为共同注意力图汇聚（CAGPool）的新颖和高效的图级方法，用于提取图对之间的交互表示。该方法在分类和回归任务中展现出竞争性能。 |
| [^22] | [Confident Feature Ranking.](http://arxiv.org/abs/2307.15361) | 提出了一种确定性特征排序的方法，该方法通过特征重要性值的两两比较，可以产生排序和同时的置信区间，并且可以选择前k个集合。 |
| [^23] | [Med-HALT: Medical Domain Hallucination Test for Large Language Models.](http://arxiv.org/abs/2307.15343) | Med-HALT是一个新的基准和数据集，用于评估和减少大规模语言模型中医疗领域的幻觉问题。这个数据集包括多种创新的测试模式，并评估了领先的LLMs在性能上的差异。 |
| [^24] | [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding.](http://arxiv.org/abs/2307.15337) | 本研究提出了一种名为“思维的骨架”的方法，可以通过并行解码来减少大型语言模型的生成延迟。这种方法不仅显著提高了速度，还可以潜在地提高答案质量。 |
| [^25] | [Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models.](http://arxiv.org/abs/2307.15331) | 这篇论文提供了两个教程，介绍了使用BERT微调和提示大型语言模型进行Twitter立场识别的方法。教程通过实例代码和可视化分析，展示了少样本ChatGPT和FLAN-T5的优势，同时提供了对BERT模型的训练和评估的指导。这些教程使学习者能够掌握运用先进方法进行立场识别的实践经验。 |
| [^26] | [Robust Visual Sim-to-Real Transfer for Robotic Manipulation.](http://arxiv.org/abs/2307.15320) | 该论文研究了视觉模拟与实际机器人操作的跨界迁移问题，并提出了域随机化方法来弥合这一差距。通过离线代理任务和模拟训练，成功地优化了域随机化参数，从而实现了在真实机器人上的有效应用。 |
| [^27] | [DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation.](http://arxiv.org/abs/2307.15317) | 本文提出了一种利用可微分Kendall排名相关性进行少样本学习的新方法，证明了特征通道的重要性排序在少样本学习中比几何相似度度量更可靠，并且实验证明在推理过程中用Kendall排名相关性替换几何相似度度量能够提高少样本学习性能。 |
| [^28] | [Efficient Multiuser AI Downloading via Reusable Knowledge Broadcasting.](http://arxiv.org/abs/2307.15316) | 通过利用可重复使用的知识进行参数广播，我们提出了一个称为MBA的框架，以在6G移动网络中实现高效的多用户AI下载。该框架包括参数选择和功率控制的联合设计，以减少通信开销并提高设备的模型性能。 |
| [^29] | [WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories.](http://arxiv.org/abs/2307.15293) | WC-SBERT提出了一种利用SBERT和自训练解决零样本文本分类问题的方法，通过使用维基百科作为训练集和建立类别对之间的正相关关系，实现快速且准确的分类。 |
| [^30] | [Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification.](http://arxiv.org/abs/2307.15254) | 这篇论文提出了一种带有掩码困难实例挖掘的多示例学习框架，用于全切片图像分类。该框架通过使用共享学习结构和一致性约束来探索潜在的难以分类的实例，并通过动量教师隐式挖掘这些实例来训练学生模型，从而提高分类性能。 |
| [^31] | [A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design.](http://arxiv.org/abs/2307.15245) | 本文针对联邦学习在统计异质实验设计下的问题进行了全面研究，提供了设计有意义和具有良好激励机制的FL实验设置的见解和建议。 |
| [^32] | [BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection.](http://arxiv.org/abs/2307.15244) | BOURNE是一种基于自助式自监督学习的统一图异常检测框架，旨在克服节点和边异常检测之间的独立性局限以及负对采样带来的高计算成本问题。 |
| [^33] | [Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures.](http://arxiv.org/abs/2307.15220) | 通过观看手术视频讲座，我们提出了一种新方法，SurgVLP，通过利用手术视频讲座中的语音和视觉信息进行多模态表示学习，并解决了手术相关语言挑战。 |
| [^34] | [Reachability Poorman Discrete-Bidding Games.](http://arxiv.org/abs/2307.15218) | 该论文研究了一种名为竞价游戏的图游戏，首次考虑了贫民离散竞价，对出价进行了粒度限制，并且较高的出价支付给银行。研究关注阈值预算，以确保玩家1对抗给定玩家2的预算的必要和充分条件。 |
| [^35] | [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2307.15217) | 本文调查了从人类反馈中进行强化学习的开放问题和基本限制，并提出了加强社会监督的审计和披露标准。 |
| [^36] | [PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization.](http://arxiv.org/abs/2307.15199) | 提出了PromptStyler，通过使用提示合成样式特征，解决了无源域泛化的问题。该方法通过学习样式词向量生成多样的样式，并通过强制样式内容特征与内容特征靠近来保证样式不会扭曲内容信息。在多个数据集上取得了最先进的结果。 |
| [^37] | [One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data.](http://arxiv.org/abs/2307.15198) | 本文研究了在神经影像数据中单次联合提取、注册和分割的问题，通过利用一个带有标签的模板图像，避免了传统方法中需要大量训练样本和手动质量控制的问题。 |
| [^38] | [Learning in Repeated Multi-Unit Pay-As-Bid Auctions.](http://arxiv.org/abs/2307.15193) | 本论文研究了在重复的多单位付费拍卖中学习如何出价的问题。通过在离线设置中优化出价向量，并利用多项式时间动态规划方案，设计了具有多项式时间和空间复杂度的在线学习算法。 |
| [^39] | [Med-Flamingo: a Multimodal Medical Few-shot Learner.](http://arxiv.org/abs/2307.15189) | Med-Flamingo是一种多模态医学少样本学习器，通过在医学图文数据上进行预训练，实现了少样本生成式医学视觉问答的能力。 |
| [^40] | [RCT Rejection Sampling for Causal Estimation Evaluation.](http://arxiv.org/abs/2307.15176) | 该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。 |
| [^41] | [VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings.](http://arxiv.org/abs/2307.15164) | 本研究中，我们开发了一种利用深度学习模型和词嵌入表示的策略来捕捉复杂对话中情绪表达的微妙差异。我们的实验结果在情绪检测任务中取得了良好的效果，并在小样本和不平衡的混合目标情绪数据集中得到了验证。 |
| [^42] | [Cortex Inspired Learning to Recover Damaged Signal Modality with ReD-SOM Model.](http://arxiv.org/abs/2307.15095) | 这项研究受到人类大脑中的麦格尔克效应的启发，提出了使用自组织映射和Hebb连接在ReD-SOM模型中重建丢失数据模态的方法。实验结果证明了该方法的有效性。 |
| [^43] | [A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning.](http://arxiv.org/abs/2307.15092) | 沉积计算是一个具有随机连接的循环神经网络，其简单的结构和丰富的动力学使其在各种应用中能够生成适当的响应。它的跨学科应用包括物理硬件实现和生物设备，同时也有助于理解大脑机制。 |
| [^44] | [Understanding Forward Process of Convolutional Neural Network.](http://arxiv.org/abs/2307.15090) | 本文揭示了CNN前向处理中的选择性旋转机制，并通过使用结构化数学工具对数据进行统计和分析，发现了人工神经网络和人脑在数据处理模式上的一致性。 |
| [^45] | [Information Gained Subgroup Discovery in Datasets.](http://arxiv.org/abs/2307.15089) | 该论文研究了在数据集中通过信息提升的方法进行子群发现。具体针对肺癌治疗，在保持或提高治疗效果的同时减少副作用对于改善患者的生活质量非常重要，临床指南虽然提供了治疗建议，但仍未将治疗结果纳入考量。 |
| [^46] | [Matching Patients to Clinical Trials with Large Language Models.](http://arxiv.org/abs/2307.15051) | 本研究调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力，并引入了TrialGPT架构，该架构能够准确预测合格性并提供解释，实验证明其有效性。 |
| [^47] | [Solving Data Quality Problems with Desbordante: a Demo.](http://arxiv.org/abs/2307.14935) | Desbordante是一个旨在解决数据质量问题的工具，通过发现和验证复杂统计信息来帮助现代数据科学家进行数据概要分析。它提供了与现有工具的适当集成，同时考虑到工业级工作负载，并提供描述性的解释来解释模式缺失的原因。 |
| [^48] | [Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version).](http://arxiv.org/abs/2307.14799) | 本研究通过混合ASP方法解决了实际半导体制造过程的调度问题，将其具体要求建模，并且实现了灵活的机器加工、设置、批处理和维护操作。 |
| [^49] | [Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation.](http://arxiv.org/abs/2307.14750) | 本文提出了一种新的无注释图像字幕生成的策略，利用大规模预训练模型的先验知识作为监督，并整合检索过程以进一步增强其效力。该方法能够从不匹配的语料库中检索相关的短区域描述，并利用其生成多样的句子。 |
| [^50] | [MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy.](http://arxiv.org/abs/2307.14643) | 本文提出了一种基于最大类间差异和最小冗余的非参数特征选择算法(MVMR-FS)，通过使用有监督和无监督核密度估计来度量特征之间的相关性和冗余，并提出了最大类间差异和最小冗余的准则(MVMR)，以辅助特征选择。 |
| [^51] | [Duet: efficient and scalable hybriD neUral rElation undersTanding.](http://arxiv.org/abs/2307.13494) | Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。 |
| [^52] | [Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations.](http://arxiv.org/abs/2307.13131) | 本文提出了EvilEye，利用透明显示器生成动态物理敌对示例的感知攻击方法，通过利用摄像头的光学特性在多种照明条件下诱导错误分类。 |
| [^53] | [Towards Building More Robust Models with Frequency Bias.](http://arxiv.org/abs/2307.09763) | 该论文提出了一种频率偏好控制模块，通过自适应地重新配置中间特征表示的低频和高频成分，提高了模型在鲁棒性学习中对频率的利用。 |
| [^54] | [Emotional Intelligence of Large Language Models.](http://arxiv.org/abs/2307.09042) | 这项研究评估了大型语言模型（LLMs）的情感智能（EI），并提出了一种新的心理测量评估方法，该方法重点考察了情感理解（EU）能力。评估结果表明，大多数主流LLMs在情感理解方面表现良好。 |
| [^55] | [Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code.](http://arxiv.org/abs/2307.07686) | 本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。 |
| [^56] | [Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research.](http://arxiv.org/abs/2307.02131) | 本研究利用反事实解释来探索医学研究中的“假如”场景，通过提供个性化和情境特定的见解，拓展了我们对现有边界的理解，并填补了机器学习算法结果解释的缺失。 |
| [^57] | [Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education.](http://arxiv.org/abs/2307.00112) | 这项研究评估了ChatGPT在回答复杂医学问题上的可靠性，发现其生成的答案更加注重上下文并具有较高的可靠性。 |
| [^58] | [Automating Model Comparison in Factor Graphs.](http://arxiv.org/abs/2306.05965) | 本文基于自定义混合节点 Forney 样式的因子图消息传递，实现了高效自动化贝叶斯模型平均、选择和组合，并缩短了模型设计周期。 |
| [^59] | [Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs.](http://arxiv.org/abs/2305.10883) | 该研究提出了一种称为IRB-AF的领域自适应的图像分割框架，通过使用IRB和ArtFlow方法来解决口咽器官图像分割中的数据域差异问题，有效提高了分割性能。 |
| [^60] | [SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition.](http://arxiv.org/abs/2305.04076) | 本文提出了一种处理Distantly-Supervised Named Entity Recognition中错误和不完整标注噪声的分离策略，使用不同的模型构建来应对两种类型的噪声。 |
| [^61] | [Fooling Thermal Infrared Detectors in Physical World.](http://arxiv.org/abs/2304.10712) | 本论文提出一种新颖的物理攻击方法——对抗性红外块（AdvIB），可以从多个角度对热成像系统执行隐蔽的黑盒攻击，成功诱导目标红外检测器在物理场景中对对象或人类进行误分类，并且不被人类察觉。 |
| [^62] | [VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping.](http://arxiv.org/abs/2304.07810) | VISAR是一个AI写作助手，旨在帮助作者提升写作体验和输出。它可以在写作上下文中随时帮助作者构思和修改目标，通过可视化编程来组织论证结构，并提供推荐来增加说服力。自动草案原型可以用来验证计划。 |
| [^63] | [Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2303.01170) | 本文提出了Expert-Free Online Transfer Learning (EF-OnTL)算法，在多智能体系统中实现无专家的实时迁移学习。通过动态选择迁移源智能体和要转移的知识，解决了传统迁移学习需要对专家智能体任务有良好理解的问题。 |
| [^64] | [Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization.](http://arxiv.org/abs/2302.14339) | 本文提出了一种名为ESB-CPO的算法，通过在早期阶段放宽约束并引入额外安全预算，在探索效率和约束满足之间取得平衡，从而提高了强化学习中受限策略优化的效果。 |
| [^65] | [Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework.](http://arxiv.org/abs/2302.12247) | 通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。 |
| [^66] | [Multi-modal Machine Learning in Engineering Design: A Review and Future Directions.](http://arxiv.org/abs/2302.10909) | 本文综述了工程设计领域中多模态机器学习（MMML）的当前状态、进展和挑战，重点介绍了多模态信息表示、融合、对齐、转换和共学习的基本概念，以及与工程设计相关的前沿应用。我们提出了未来研究的潜在方向，包括构建广泛的数据集和基准测试环境，以促进算法性能的评估和比较。 |
| [^67] | [3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection.](http://arxiv.org/abs/2302.08231) | 3M3D是一种用于三维物体检测的多视图、多路径、多表示方法，通过更新多视图特征和查询特征，同时增强全景视图和全局视图中的场景表示。 |
| [^68] | [Towards Answering Climate Questionnaires from Unstructured Climate Reports.](http://arxiv.org/abs/2301.04253) | 本研究提出了一种从非结构化的气候报告中回答气候问卷的方法。研究引入两个新的大规模气候问卷数据集，并使用现有结构训练自监督模型，通过实验和人类试验验证了模型的有效性。同时，还引入了一个气候文本分类数据集的基准，以促进气候领域的自然语言处理研究。 |
| [^69] | [Consistent Range Approximation for Fair Predictive Modeling.](http://arxiv.org/abs/2212.10839) | 本文提出了一个新颖的框架，用于验证基于有偏数据训练的预测模型的公平性。通过一致范围逼近的方法，在目标人群上构建了可证明公平的预测模型，并在真实数据上展示了明显的改进。 |
| [^70] | [Complex-valued Retrievals From Noisy Images Using Diffusion Models.](http://arxiv.org/abs/2212.03235) | 本研究推广了退火朗格朗日动力学（DDM类型）用于处理受泊松噪声影响的光学成像中的复数对象，提出了一种使用扩散模型从噪声图像中进行复数检索的算法。 |
| [^71] | [Mitigating spectral bias for the multiscale operator learning with hierarchical attention.](http://arxiv.org/abs/2210.10890) | 本文提出了一种分层注意力神经算子（HANO），用于解决多尺度偏微分方程学习中存在的光谱偏差问题，并通过数值实验证明其优于现有方法。 |
| [^72] | [Learning Provably Stabilizing Neural Controllers for Discrete-Time Stochastic Systems.](http://arxiv.org/abs/2210.05304) | 本文提出了一种学习离散时间随机系统中的神经控制器的方法，该方法能够以概率1在指定的稳定区域内实现系统稳定，并引入了稳定排序超级鞅(sRSMs)的概念来克服先前方法的局限性。实验结果证明了该方法的有效性。 |
| [^73] | [Rethinking Missing Data: Aleatoric Uncertainty-Aware Recommendation.](http://arxiv.org/abs/2209.11679) | 本研究提出了一种基于随机不确定性感知的推荐系统（AUR）框架，通过考虑缺失数据的固有随机性，解决了模型误差问题，提高了对长尾物品的召回效果。 |
| [^74] | [SynthA1c: Towards Clinically Interpretable Patient Representations for Diabetes Risk Stratification.](http://arxiv.org/abs/2209.10043) | 该论文提出了一种名为SynthA1c的方法，利用图像数据来预测2型糖尿病风险，避免了额外的血液实验室测量，其敏感性高达87.6%。 |
| [^75] | [Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success.](http://arxiv.org/abs/2208.07734) | 这项研究通过广泛的实验，证明数据增强与异常生成机制之间的对齐是自监督学习在无监督异常检测中取得成功的关键，并且在缺乏对齐时，自监督学习甚至可能降低准确性。 |
| [^76] | [Trace Recovery from Stochastically Known Logs.](http://arxiv.org/abs/2206.12672) | 本文提出了一种从随机已知日志中恢复轨迹的算法，在两个公开数据集上平均恢复准确度达到90-97%。这一方法通过计算流程模型和随机已知轨迹的合规性，并恢复在该随机轨迹中的最佳对齐作为真实轨迹。对比其他轨迹恢复选项，使用了产品多图来分析成本模型对恢复准确性的影响。这一算法对于预测模型开发、错误排查和系统性能改进具有重要意义。 |
| [^77] | [Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid Rank Valuations.](http://arxiv.org/abs/2206.08495) | 本论文提出了一个基于Yankee Swap过程的简单算法，用于拟阵等级估值的无分割物品公平分配，这种方法易于理解，快速可扩展。 |

# 详细

[^1]: 自然语言生成中的不确定性：从理论到应用

    Uncertainty in Natural Language Generation: From Theory to Applications. (arXiv:2307.15703v1 [cs.CL])

    [http://arxiv.org/abs/2307.15703](http://arxiv.org/abs/2307.15703)

    本文介绍了自然语言生成中不确定性的理论、应用和分类，提出了一种更详细和真实的分类法。

    

    最近强大的语言模型的进展使得自然语言生成（NLG）作为一种重要技术崭露头角，它不仅可以执行传统任务如摘要或翻译，也可以作为一种自然语言接口应用于各种应用程序。因此，NLG系统的可靠性和可信度至关重要，例如在可能出错时指示，并支持多种观点、背景和写作风格 - 反映多元化人类亚群体。本文认为，对不确定性的原则性处理能够帮助创建与这些目标更好地对齐的系统和评估协议。我们首先介绍表示不确定性所需的基本理论、框架和词汇。然后从语言学的角度描述NLG中的主要不确定性源，并提出一个比流行的随机性/认识性二分法更详细和真实的二维分类法。

    Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Final
    
[^2]: 一种用于应急逃生路径问题的监督式混合量子机器学习解决方案

    A supervised hybrid quantum machine learning solution to the emergency escape routing problem. (arXiv:2307.15682v1 [quant-ph])

    [http://arxiv.org/abs/2307.15682](http://arxiv.org/abs/2307.15682)

    本文研究了使用监督式混合量子机器学习来优化自然灾害中汽车紧急疏散计划的潜力，并提出了一种新颖的混合监督学习方法来模拟最短路径算法。

    

    有效管理对自然灾害的响应可以大大减轻其破坏性影响。本文探讨了使用监督式混合量子机器学习来优化自然灾害中汽车紧急疏散计划的潜力。该研究聚焦于地震紧急情况，将问题建模为一个动态计算图，地震破坏城市的一部分。居民试图通过到达交通拥堵发生的出口点来撤离城市。该情况被建模为在不确定和动态演化的地图上的最短路径问题。我们提出了一种新颖的混合监督学习方法，并在具体城市图上的假设情况下进行了测试。该方法使用了一种新颖的量子功能线性调制(FiLM)神经网络，与一个经典的FiLM网络并行，以模仿确定性动态图上的Dijkstra节点最短路径算法。并行添加量子神经网络

    Managing the response to natural disasters effectively can considerably mitigate their devastating impact. This work explores the potential of using supervised hybrid quantum machine learning to optimize emergency evacuation plans for cars during natural disasters. The study focuses on earthquake emergencies and models the problem as a dynamic computational graph where an earthquake damages an area of a city. The residents seek to evacuate the city by reaching the exit points where traffic congestion occurs. The situation is modeled as a shortest-path problem on an uncertain and dynamically evolving map. We propose a novel hybrid supervised learning approach and test it on hypothetical situations on a concrete city graph. This approach uses a novel quantum feature-wise linear modulation (FiLM) neural network parallel to a classical FiLM network to imitate Dijkstra's node-wise shortest path algorithm on a deterministic dynamic graph. Adding the quantum neural network in parallel increas
    
[^3]: IT监控时间序列的因果推断案例研究

    Case Studies of Causal Discovery from IT Monitoring Time Series. (arXiv:2307.15678v1 [cs.LG])

    [http://arxiv.org/abs/2307.15678](http://arxiv.org/abs/2307.15678)

    本研究通过案例研究，探索了将因果推断算法应用于不同IT监控数据的挑战和潜在益处。

    

    信息技术（IT）系统对现代企业至关重要，处理数据存储、通信和流程自动化。监控这些系统对于其正常运行和效率至关重要，因为它允许收集详细的观测时间序列数据进行分析。随着IT监控系统中对因果推断的兴趣日益增长，了解IT系统不同组件之间的因果关系有助于减少停机时间，提高系统性能，并确定异常和事故的根本原因。它还通过历史数据分析可以预测未来的问题。尽管具有潜在的益处，应用因果推断算法到IT监控数据上面临着挑战，因为数据的复杂性。例如，IT监控数据通常包含不对齐的时间序列、休眠时间序列、时间戳错误和缺失值。本文提出了应用因果推断算法到不同IT监控数据的案例研究。

    Information technology (IT) systems are vital for modern businesses, handling data storage, communication, and process automation. Monitoring these systems is crucial for their proper functioning and efficiency, as it allows collecting extensive observational time series data for analysis. The interest in causal discovery is growing in IT monitoring systems as knowing causal relations between different components of the IT system helps in reducing downtime, enhancing system performance and identifying root causes of anomalies and incidents. It also allows proactive prediction of future issues through historical data analysis. Despite its potential benefits, applying causal discovery algorithms on IT monitoring data poses challenges, due to the complexity of the data. For instance, IT monitoring data often contains misaligned time series, sleeping time series, timestamp errors and missing values. This paper presents case studies on applying causal discovery algorithms to different IT mo
    
[^4]: 视觉语言导航中的数据生成规模化

    Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])

    [http://arxiv.org/abs/2307.15644](http://arxiv.org/abs/2307.15644)

    这项研究提出了一种用于视觉语言导航中生成大规模数据的有效范式。通过利用逼真的环境和网络资源，合成了490万个指令轨迹对。通过使用这个大规模数据集，通过简单的模仿学习，已存在的代理的性能得到了显著提升至80%。

    

    最近在语言引导的视觉导航研究中，对于遍历环境的多样性和训练可泛化代理的监督数量有了明显需求。为了解决现有视觉语言导航数据集中普遍存在的数据稀缺问题，我们提出了一种有效的范式，用于生成用于学习的大规模数据。我们应用了HM3D和Gibson数据集中的1200多个逼真的环境，并利用网络上的资源合成了490万个指令轨迹对。重要的是，我们调查了范式中每个组成部分对代理性能的影响，并研究了如何恰当地应用扩增数据来预训练和微调代理。得益于我们的大规模数据集，通过简单的模仿学习，现有代理的性能可以大幅提升（相对于之前的最佳结果绝对值增加了11%），在R2R测试集中单次运行成功率显著提升至80%。

    Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The
    
[^5]: 我们都是个体：机器人个性和人类特质在可信互动中的作用

    We are all Individuals: The Role of Robot Personality and Human Traits in Trustworthy Interaction. (arXiv:2307.15568v1 [cs.RO])

    [http://arxiv.org/abs/2307.15568](http://arxiv.org/abs/2307.15568)

    本文通过定量和定性研究，探讨了机器人个性与个体人类特质之间的关系，并发现外向个性的机器人在人们对机器人咖啡师的信任中更受欢迎和被信任，同时发现个体对机器人的态度和偏好对信任也有影响。

    

    随着机器人在我们社会中扮演角色，重要的是它们的外观、行为和个性是否适合其工作，并且是否受到与其互动的人们的喜爱。在这里，我们提供了一个广泛的定量和定性研究，探讨了机器人个性与个体人类特质之间的关系。首先，我们通过声音提示和语言特征来准确描绘社交机器人的个性，即外向性和内向性。其次，通过获取对这些不同机器人个性的偏好和信任评级，我们确定对于一名机器人咖啡师而言，外向个性的机器人比内向个性的机器人更受欢迎和被信任，不论受试者自身的个性如何。第三，我们发现个体对机器人的态度和偏好确实会影响对机器人咖啡师的信任，因此除了机器人个性、角色和互动之外，它们也是重要的考虑因素。

    As robots take on roles in our society, it is important that their appearance, behaviour and personality are appropriate for the job they are given and are perceived favourably by the people with whom they interact. Here, we provide an extensive quantitative and qualitative study exploring robot personality but, importantly, with respect to individual human traits. Firstly, we show that we can accurately portray personality in a social robot, in terms of extroversion-introversion using vocal cues and linguistic features. Secondly, through garnering preferences and trust ratings for these different robot personalities, we establish that, for a Robo-Barista, an extrovert robot is preferred and trusted more than an introvert robot, regardless of the subject's own personality. Thirdly, we find that individual attitudes and predispositions towards robots do impact trust in the Robo-Baristas, and are therefore important considerations in addition to robot personality, roles and interaction c
    
[^6]: 基于渐进机器学习的小样本图像分类

    Few-shot Image Classification based on Gradual Machine Learning. (arXiv:2307.15524v1 [cs.CV])

    [http://arxiv.org/abs/2307.15524](http://arxiv.org/abs/2307.15524)

    该论文提出了一种基于逐渐机器学习的新方法来解决小样本图像分类问题。通过逐步标记目标图像，并利用深度主干网络提取的特征构建因子，该方法能够有效地处理知识迁移和分类挑战。

    

    小样本图像分类旨在仅使用少量有标签样本准确地对未标记图像进行分类。现有的解决方案基于深度学习，主要关注设计越来越复杂的深度主干网络。然而，由于在训练类别中学到的知识难以迁移到新类别，该任务仍然非常具有挑战性。本文提出了一种基于逐渐机器学习（GML）非i.i.d范式的新方法。它从仅有少量有标签观测开始，然后逐渐按照难度增加的顺序通过迭代因子图中的因子推理来标记目标图像。具体而言，我们的解决方案通过深度主干网络提取具有指示性的特征表示，然后基于提取的特征构建一元因子和二元因子以促进渐进学习。一元因子基于嵌入空间中的类别中心距离构建，而二元因子基于...

    Few-shot image classification aims to accurately classify unlabeled images using only a few labeled samples. The state-of-the-art solutions are built by deep learning, which focuses on designing increasingly complex deep backbones. Unfortunately, the task remains very challenging due to the difficulty of transferring the knowledge learned in training classes to new ones. In this paper, we propose a novel approach based on the non-i.i.d paradigm of gradual machine learning (GML). It begins with only a few labeled observations, and then gradually labels target images in the increasing order of hardness by iterative factor inference in a factor graph. Specifically, our proposed solution extracts indicative feature representations by deep backbones, and then constructs both unary and binary factors based on the extracted features to facilitate gradual learning. The unary factors are constructed based on class center distance in an embedding space, while the binary factors are constructed b
    
[^7]: 重新审视全卷积几何特征用于物体6D姿态估计

    Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation. (arXiv:2307.15514v1 [cs.CV])

    [http://arxiv.org/abs/2307.15514](http://arxiv.org/abs/2307.15514)

    该论文重新审视全卷积几何特征（FCGF）用于物体6D姿态估计，并通过学习逐点判别特征以及适当的修改和训练策略超越了近期的竞争对手，并取得了最先进的性能。

    

    近期关于6D物体姿态估计的研究主要集中在学习图像和物体模型之间的关键点对应，并通过基于RANSAC的算法或直接通过端到端优化回归姿态来确定物体姿态。我们认为文献中忽视了学习逐点判别特征。为此，我们重新审视了全卷积几何特征（FCGF），并将其定制为物体6D姿态估计，以实现最先进的性能。FCGF采用稀疏卷积，通过优化最难比较损失来学习逐点特征。通过对损失和输入数据表示进行关键修改、精心调整训练策略以及采用适合底层问题的数据增强，我们能够超越近期的竞争对手在流行的基准测试上取得更好的结果。我们进行了彻底的消融实验，研究了每个修改的贡献。

    Recent works on 6D object pose estimation focus on learning keypoint correspondences between images and object models, and then determine the object pose through RANSAC-based algorithms or by directly regressing the pose with end-to-end optimisations. We argue that learning point-level discriminative features is overlooked in the literature. To this end, we revisit Fully Convolutional Geometric Features (FCGF) and tailor it for object 6D pose estimation to achieve state-of-the-art performance. FCGF employs sparse convolutions and learns point-level features using a fully-convolutional network by optimising a hardest contrastive loss. We can outperform recent competitors on popular benchmarks by adopting key modifications to the loss and to the input data representations, by carefully tuning the training strategies, and by employing data augmentations suitable for the underlying problem. We carry out a thorough ablation to study the contribution of each modification.
    
[^8]: 探索指令调整的格式一致性

    Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])

    [http://arxiv.org/abs/2307.15504](http://arxiv.org/abs/2307.15504)

    本研究探究了指令调整的格式一致性，并提出了统一指令调整（UIT）框架，通过自动格式转换来提高泛化性能。该研究强调了格式一致性的重要性。

    

    指令调整已经成为一种提升大型语言模型遵循人类指令能力的有前途的方法。研究表明，增加训练数据中指令的多样性和数量可以持续提升泛化性能，从而促进了最近的一项努力，即收集各种指令并将现有的指令调整数据集整合到更大的集合中。然而，不同用户有其独特的表达指令的方式，不同数据集之间通常存在指令风格和格式的变化，即格式不一致性。在这项工作中，我们研究了格式不一致性如何影响指令调整的性能。我们提出了一个名为“统一指令调整”（UIT）的框架，通过调用OpenAI的API实现在不同的指令调整数据集之间的自动格式转换。我们展示了UIT成功提高了在未见指令上的泛化性能，并强调了格式一致性的重要性。

    Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance
    
[^9]: ETHER: 对于回顾性经验重演的紧密沟通对齐

    ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v1 [cs.CL])

    [http://arxiv.org/abs/2307.15494](http://arxiv.org/abs/2307.15494)

    本文提出了ETHER，通过对齐紧急沟通来解决回顾性经验重演中的问题，克服了先前架构依赖预设函数的限制，并提高了数据效率和性能。

    

    自然语言指令的跟随对于实现人工智能代理和人类之间的合作至关重要。自然语言条件下的强化学习代理展示了自然语言的特性，如组合性，能够提供学习复杂策略的强归纳偏好。先前的架构如HIGhER结合了语言条件与回顾性经验重演（HER）来处理稀疏奖励环境。然而，与HER类似，HIGhER依赖于一个预设的函数来提供反馈信号，指示哪种语言描述在哪种状态下有效。这种依赖于预设函数的限制限制了其应用。此外，HIGhER只利用成功的强化学习轨迹中包含的语言信息，从而影响了其最终性能和数据效率。没有早期成功轨迹，HIGhER并不比其构建于之上的DQN更好。在本文中，我们提出了紧密文本回顾性经验。

    Natural language instruction following is paramount to enable collaboration between artificial agents and human beings. Natural language-conditioned reinforcement learning (RL) agents have shown how natural languages' properties, such as compositionality, can provide a strong inductive bias to learn complex policies. Previous architectures like HIGhER combine the benefit of language-conditioning with Hindsight Experience Replay (HER) to deal with sparse rewards environments. Yet, like HER, HIGhER relies on an oracle predicate function to provide a feedback signal highlighting which linguistic description is valid for which state. This reliance on an oracle limits its application. Additionally, HIGhER only leverages the linguistic information contained in successful RL trajectories, thus hurting its final performance and data-efficiency. Without early successful trajectories, HIGhER is no better than DQN upon which it is built. In this paper, we propose the Emergent Textual Hindsight Ex
    
[^10]: 用语义方法解决认知规划的可决定性问题（扩展版）

    A Semantic Approach to Decidability in Epistemic Planning (Extended Version). (arXiv:2307.15485v1 [cs.AI])

    [http://arxiv.org/abs/2307.15485](http://arxiv.org/abs/2307.15485)

    本文提出了一种用于解决认知规划中可决定性问题的语义方法，通过增加一个交互公理来控制代理人对其他代理人知识的无限推理能力，而不是强加句法约束。实验证明这种方法是可行的，并提供了问题的有限非不动点特征。

    

    在多智能体规划中，使用动态认知逻辑（DEL）已经导致了一个广泛采用的行动形式体系，可以处理非确定性、部分可观测性和任意知识嵌套。然而，这种表达能力的提高是以不可决定性为代价的，因此已经提取出了几种可决定的片段，主要基于行动形式体系的句法限制。本文中，我们采用了一种新颖的语义方法来实现可决定性。具体而言，我们在认知规划的逻辑中增加了一个称为“知识可交换性”的交互公理，该公理控制了代理人在其他代理人的知识上无限推理的能力，而不是强加句法约束。然后，我们提供了三个贡献。首先，我们展示了得到的认知规划问题是可决定的。在这样做的过程中，我们证明了我们的框架具有有限的非不动点特征。

    The use of Dynamic Epistemic Logic (DEL) in multi-agent planning has led to a widely adopted action formalism that can handle nondeterminism, partial observability and arbitrary knowledge nesting. As such expressive power comes at the cost of undecidability, several decidable fragments have been isolated, mainly based on syntactic restrictions of the action formalism. In this paper, we pursue a novel semantic approach to achieve decidability. Namely, rather than imposing syntactical constraints, the semantic approach focuses on the axioms of the logic for epistemic planning. Specifically, we augment the logic of knowledge S5$_n$ and with an interaction axiom called (knowledge) commutativity, which controls the ability of agents to unboundedly reason on the knowledge of other agents. We then provide a threefold contribution. First, we show that the resulting epistemic planning problem is decidable. In doing so, we prove that our framework admits a finitary non-fixpoint characterization 
    
[^11]: 用条件扩散模型和语言模型进行最小监督语音合成：基于语义编码的比较研究

    Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])

    [http://arxiv.org/abs/2307.15484](http://arxiv.org/abs/2307.15484)

    本文提出了两种语音合成方法来解决自回归和非自回归模型中的问题，并在语义编码方面进行了比较研究。

    

    近年来，对于能够采用最小监督训练方法的文本到语音(TTS)技术越来越受关注，该方法通过结合两种离散语音表示并使用两种序列到序列任务来解耦TTS。为了解决离散表示中的高维度和波形失真的挑战，我们提出了Diff-LM-Speech方法，该方法基于扩散模型将语义嵌入模型为基于mel频谱图，并引入基于变分自动编码器和韵律瓶颈的提示编码结构，以提高提示表示能力。自回归语言模型常常遇到缺失和重复单词的问题，而非自回归框架由于预测模型的存在导致表达平均问题。为了解决这些问题，我们提出了Tetra-Diff-Speech，该方法设计了一个时长扩散模型以实现多样化的韵律表达。我们期望语义编码的信息内容介于...

    Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between t
    
[^12]: 使用Gabor滤波器的非侵入式糖尿病检测：对不同相机的比较分析

    Non-invasive Diabetes Detection using Gabor Filter: A Comparative Analysis of Different Cameras. (arXiv:2307.15480v1 [cs.CV])

    [http://arxiv.org/abs/2307.15480](http://arxiv.org/abs/2307.15480)

    本文比较了移动设备相机和笔记本电脑相机在非侵入性糖尿病检测中的性能，使用了Gabor滤波器和面部块纹理特征，最终实现了96.7%的准确性、100%的敏感性和93%的特异性。

    

    本文比较和探索了移动设备相机和笔记本电脑相机作为用于非侵入性糖尿病检测的便利工具的性能，使用面部块纹理特征。选择20至79岁的参与者进行数据集。使用12mp和7mp移动相机以及笔记本电脑相机在正常照明条件下拍摄照片。提取的面部块被使用k-最近邻（k-NN）和支持向量机（SVM）进行分类。共捕获了100张图像，经过预处理、Gabor滤波和迭代处理。系统的性能以准确性、特异性和敏感性来衡量。在使用SVM和100张图像的12mp背面相机上，达到了96.7%的准确性、100%的敏感性和93%的特异性的最佳性能。

    This paper compares and explores the performance of both mobile device camera and laptop camera as convenient tool for capturing images for non-invasive detection of Diabetes Mellitus (DM) using facial block texture features. Participants within age bracket 20 to 79 years old were chosen for the dataset. 12mp and 7mp mobile cameras, and a laptop camera were used to take the photo under normal lighting condition. Extracted facial blocks were classified using k-Nearest Neighbors (k-NN) and Support Vector Machine (SVM). 100 images were captured, preprocessed, filtered using Gabor, and iterated. Performance of the system was measured in terms of accuracy, specificity, and sensitivity. Best performance of 96.7% accuracy, 100% sensitivity, and 93% specificity were achieved from 12mp back camera using SVM with 100 images.
    
[^13]: FeedbackLogs：将利益相关者反馈记录和纳入到机器学习流程中

    FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines. (arXiv:2307.15475v1 [cs.HC])

    [http://arxiv.org/abs/2307.15475](http://arxiv.org/abs/2307.15475)

    FeedbackLogs是针对机器学习流程的利益相关者反馈记录和更新的补充工具，可以用于算法审计和记录反馈更新。

    

    虽然机器学习流程影响着越来越多的利益相关者，但关于利益相关者输入的记录和利用方面的研究却很少。我们提出了FeedbackLogs，这是针对现有机器学习流程文档的补充，用于追踪多个利益相关者的输入。每个日志记录了关于反馈收集过程、反馈内容以及如何利用反馈更新机器学习流程的重要细节。本文介绍并正式化了收集FeedbackLog的过程，并提供了具体的使用案例，其中FeedbackLogs可以作为算法审计的证据，并用作记录基于利益相关者反馈的更新的工具。

    Even though machine learning (ML) pipelines affect an increasing array of stakeholders, there is little work on how input from stakeholders is recorded and incorporated. We propose FeedbackLogs, addenda to existing documentation of ML pipelines, to track the input of multiple stakeholders. Each log records important details about the feedback collection process, the feedback itself, and how the feedback is used to update the ML pipeline. In this paper, we introduce and formalise a process for collecting a FeedbackLog. We also provide concrete use cases where FeedbackLogs can be employed as evidence for algorithmic auditing and as a tool to record updates based on stakeholder feedback.
    
[^14]: 神经网络控制器及其符号表示的令人担忧的特性

    Worrisome Properties of Neural Network Controllers and Their Symbolic Representations. (arXiv:2307.15456v1 [cs.LG])

    [http://arxiv.org/abs/2307.15456](http://arxiv.org/abs/2307.15456)

    本论文对神经网络控制器在简单强化学习问题中的稳健性产生了担忧，通过对低神经元和符号抽象的探究，发现即使控制器达到高回报，仍会产生大量持久低回报解决方案，对手可以轻易利用。论文提供了一个系统稳健性研究的算法，并证明了持久解决方案的存在性以及周期轨道的存在。

    

    我们对简单的强化学习基准问题中控制器的稳健性提出了一些关切。我们关注神经网络控制器及其低神经元和符号抽象。一个典型的控制器可以达到很高的平均回报值，但仍会生成大量持久的低回报解决方案，这是一个非常不可取的特性，容易被对手利用。我们发现，简单的控制器会产生更多持久的不良解决方案。我们提供了一个系统稳健性研究的算法，并使用计算机辅助证明方法证明了持久解决方案的存在性，以及在某些情况下的周期轨道。

    We raise concerns about controllers' robustness in simple reinforcement learning benchmark problems. We focus on neural network controllers and their low neuron and symbolic abstractions. A typical controller reaching high mean return values still generates an abundance of persistent low-return solutions, which is a highly undesirable property, easily exploitable by an adversary. We find that the simpler controllers admit more persistent bad solutions. We provide an algorithm for a systematic robustness study and prove existence of persistent solutions and, in some cases, periodic orbits, using a computer-assisted proof methodology.
    
[^15]: 从概率编程到基于复杂性的编程

    From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])

    [http://arxiv.org/abs/2307.15453](http://arxiv.org/abs/2307.15453)

    CompLog是一种基于复杂性的计算框架，通过计算Kolmogorov复杂性替代概率推理，实现计算某种情况意外性的度量，并通过规范的世界和心智模型的描述生成相关描述，并提供对析取和否定的替代方法。

    

    本文介绍了一种名为CompLog的新型计算框架的主要特点和初步实现。CompLog借鉴了概率编程系统（如ProbLog）的推理机制，并基于Simplicity理论提出了一种新的推理机制，通过ASP程序的min-path搜索计算两种Kolmogorov复杂性，而不是概率推理。该系统使用户能够计算某个情况意外性的ex-post和ex-ante度量，分别对应于后验和先验主观概率。计算基于通过描述性谓词之间的因果和描述性关系加权的世界和心智模型的规范。本文还阐述了几个应用示例：生成相关描述，并提供对析取和否定的替代方法。

    The paper presents the main characteristics and a preliminary implementation of a novel computational framework named CompLog. Inspired by probabilistic programming systems like ProbLog, CompLog builds upon the inferential mechanisms proposed by Simplicity Theory, relying on the computation of two Kolmogorov complexities (here implemented as min-path searches via ASP programs) rather than probabilistic inference. The proposed system enables users to compute ex-post and ex-ante measures of unexpectedness of a certain situation, mapping respectively to posterior and prior subjective probabilities. The computation is based on the specification of world and mental models by means of causal and descriptive relations between predicates weighted by complexity. The paper illustrates a few examples of application: generating relevant descriptions, and providing alternative approaches to disjunction and to negation.
    
[^16]: DELPHIC: Practical DEL规划的可能性途径（扩展版）

    DELPHIC: Practical DEL Planning via Possibilities (Extended Version). (arXiv:2307.15451v1 [cs.AI])

    [http://arxiv.org/abs/2307.15451](http://arxiv.org/abs/2307.15451)

    DELPHIC是一个扩展了传统DEL的框架，在实用DEL规划方面取得了进展，通过使用可能性作为主要构建模块，DELPHIC提供了更紧凑的表示方式。

    

    动态认知逻辑（DEL）为认知规划提供了一个框架，能够表示非确定性动作、部分可观察性、高阶知识以及事实和认知变化。DEL的高表达性挑战了现有的认知规划器，通常只能处理整个框架中的受限片段。本文的目标是推动实用DEL规划的发展，最终使认知规划器能够处理DEL提供的所有特性。为了实现这个目标，我们对DEL的传统语义进行了质疑，该语义是用Kripke模型来定义的。具体而言，我们提出了一种等价的语义，使用所谓的可能性作为主要构建模块，表示世界的事实属性和代理人认为可能的属性。我们称这个结果为DELPHIC框架。我们认为DELPHIC确实提供了对认知状态更紧凑的表示。

    Dynamic Epistemic Logic (DEL) provides a framework for epistemic planning that is capable of representing non-deterministic actions, partial observability, higher-order knowledge and both factual and epistemic change. The high expressivity of DEL challenges existing epistemic planners, which typically can handle only restricted fragments of the whole framework. The goal of this work is to push the envelop of practical DEL planning, ultimately aiming for epistemic planners to be able to deal with the full range of features offered by DEL. Towards this goal, we question the traditional semantics of DEL, defined in terms on Kripke models. In particular, we propose an equivalent semantics defined using, as main building block, so-called possibilities: non well-founded objects representing both factual properties of the world, and what agents consider to be possible. We call the resulting framework DELPHIC. We argue that DELPHIC indeed provides a more compact representation of epistemic sta
    
[^17]: 时间知识库的最优对齐

    Optimal Alignment of Temporal Knowledge Bases. (arXiv:2307.15439v1 [cs.LO])

    [http://arxiv.org/abs/2307.15439](http://arxiv.org/abs/2307.15439)

    本文提出了解决时间知识库对齐问题的方法，用于最小化改变已有的知识库，同时满足给定的时间相关查询，并在成本上达到最优。方法在ALC TKBs和带有LTL运算符的连接查询上进行研究，并扩展了命题LTL对齐问题上的技术。

    

    在基于本体的情境识别中，回答时间化描述逻辑知识库（TKB）上的时间CQs是一种主要技术。如果这样的知识库中收集到的数据不准确，重要的查询答案可能会被忽略。本文介绍了TKB对齐问题，该问题计算一种变体的TKB，最小化地改变TKB，但能推导出给定的时间CQ，并在这个意义上是（代价）最优的。我们研究了针对ALC TKBs和带有LTL运算符的连接查询的这个问题，并设计了一种解决方案来计算TKB的（代价最优的）对齐，该解决方案扩展了用于有限轨迹上的命题LTL对齐问题的技术。

    Answering temporal CQs over temporalized Description Logic knowledge bases (TKB) is a main technique to realize ontology-based situation recognition. In case the collected data in such a knowledge base is inaccurate, important query answers can be missed. In this paper we introduce the TKB Alignment problem, which computes a variant of the TKB that minimally changes the TKB, but entails the given temporal CQ and is in that sense (cost-)optimal. We investigate this problem for ALC TKBs and conjunctive queries with LTL operators and devise a solution technique to compute (cost-optimal) alignments of TKBs that extends techniques for the alignment problem for propositional LTL over finite traces.
    
[^18]: 可改进的多任务学习中的差距平衡

    Improvable Gap Balancing for Multi-Task Learning. (arXiv:2307.15429v1 [cs.LG])

    [http://arxiv.org/abs/2307.15429](http://arxiv.org/abs/2307.15429)

    本文提出了两种新颖的可改进差距平衡算法（IGB）用于多任务学习，一种采用简单的启发式方法，另一种首次采用深度强化学习方法。这两种算法通过动态分配任务权重来实现可改进差距平衡，解决了损失平衡之后仍然存在的性能不平衡问题。

    

    在多任务学习中，梯度平衡近期比损失平衡更吸引研究兴趣，因为它通常能带来更好的性能。然而，损失平衡比梯度平衡更高效，因此在多任务学习中仍然值得进一步探索。注意先前的研究通常忽略了在多个任务之间存在可改进差距的事实，其中每个任务的可改进差距定义为当前训练进度与期望的最终训练进度之间的距离。因此，在损失平衡之后，性能不平衡在许多情况下仍然存在。在本文中，我们基于损失平衡框架提出了两种新的可改进差距平衡算法（IGB）用于多任务学习：一种采用简单的启发式方法，另一种（首次）采用深度强化学习用于多任务学习。特别地，两种算法都选择动态分配任务权重来进行可改进差距平衡。

    In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing
    
[^19]: 大型语言模型的关键评论：敏感性、偏见和通向专业人工智能的道路

    A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI. (arXiv:2307.15425v1 [cs.CL])

    [http://arxiv.org/abs/2307.15425](http://arxiv.org/abs/2307.15425)

    本文研究了专业编译语言模型和通用模型在检测可持续发展目标方面的比效率，关注了大型语言模型存在的偏见和敏感性挑战。研究强调了专业培训对于精确无偏的分析的必要性，并通过对一个公司描述数据集的案例研究对比了GPT-3.5和专业SDG检测模型的差异。研究结果强调了在模型选择时需要考虑任务要求、成本、复杂性和透明度。尽管大型语言模型的多功能性，但作者建议在需要精确性和准确性的任务中使用专业模型。

    

    本文对专业编译语言模型和OpenAI的GPT-3.5等通用模型在检测文本数据中的可持续发展目标（SDGs）方面的比效率进行了研究。它对大型语言模型（LLMs）进行了关键评论，解决了与偏见和敏感性相关的挑战。强调了精确、无偏见分析的专业培训的必要性。使用公司描述数据集的案例研究揭示了GPT-3.5和专业SDG检测模型之间的差异。虽然GPT-3.5具有更广泛的覆盖范围，但可能会识别与公司活动相关性有限的SDGs。相比之下，专业模型聚焦于高度相关的SDGs。强调了深思熟虑的模型选择的重要性，考虑任务要求、成本、复杂性和透明度。尽管LLMs的多功能性，但建议在需要精确性和准确性的任务中使用专业模型。研究最后鼓励了……

    This paper examines the comparative effectiveness of a specialized compiled language model and a general-purpose model like OpenAI's GPT-3.5 in detecting SDGs within text data. It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity. The necessity of specialized training for precise, unbiased analysis is underlined. A case study using a company descriptions dataset offers insight into the differences between the GPT-3.5 and the specialized SDG detection model. While GPT-3.5 boasts broader coverage, it may identify SDGs with limited relevance to the companies' activities. In contrast, the specialized model zeroes in on highly pertinent SDGs. The importance of thoughtful model selection is emphasized, taking into account task requirements, cost, complexity, and transparency. Despite the versatility of LLMs, the use of specialized models is suggested for tasks demanding precision and accuracy. The study concludes by encouraging 
    
[^20]: 通过多个帖子依赖关系改进社交媒体受欢迎程度预测

    Improving Social Media Popularity Prediction with Multiple Post Dependencies. (arXiv:2307.15413v1 [cs.MM])

    [http://arxiv.org/abs/2307.15413](http://arxiv.org/abs/2307.15413)

    该论文提出了一种新型预测框架DSN，通过同时利用帖子内部和帖子间的依赖关系来改进社交媒体受欢迎程度的预测准确性。

    

    社交媒体受欢迎程度预测引起了广泛关注，因为它对许多不同应用有着深远的影响，如推荐系统和多媒体广告。尽管最近的一些努力利用社交媒体帖子的内容来提高预测准确性，但许多现有模型未能充分利用帖子之间的多个依赖关系，而这些依赖关系对于全面提取帖子内容信息很重要。为了解决这个问题，我们提出了一种名为“依赖感知序列网络（DSN）”的新型预测框架，它同时利用帖子内部依赖和帖子间依赖。对于帖子内部依赖，DSN采用一种多模态特征提取器和高效微调策略，从帖子的图像和文本信息中获取任务特定的表示。对于帖子间依赖，DSN使用一种分层信息传播方法来学习能更好描述帖子之间差异的类别表示。DSN还利用...

    Social Media Popularity Prediction has drawn a lot of attention because of its profound impact on many different applications, such as recommendation systems and multimedia advertising. Despite recent efforts to leverage the content of social media posts to improve prediction accuracy, many existing models fail to fully exploit the multiple dependencies between posts, which are important to comprehensively extract content information from posts. To tackle this problem, we propose a novel prediction framework named Dependency-aware Sequence Network (DSN) that exploits both intra- and inter-post dependencies. For intra-post dependency, DSN adopts a multimodal feature extractor with an efficient fine-tuning strategy to obtain task-specific representations from images and textual information of posts. For inter-post dependency, DSN uses a hierarchical information propagation method to learn category representations that could better describe the difference between posts. DSN also exploits 
    
[^21]: 高效的成对图交互学习的共同注意力图汇聚

    Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning. (arXiv:2307.15377v1 [cs.LG])

    [http://arxiv.org/abs/2307.15377](http://arxiv.org/abs/2307.15377)

    本论文提出了一种称为共同注意力图汇聚（CAGPool）的新颖和高效的图级方法，用于提取图对之间的交互表示。该方法在分类和回归任务中展现出竞争性能。

    

    图神经网络（GNN）在处理和学习图结构数据方面已经被证明是有效的。然而，先前的研究主要关注的是理解单一图输入，而许多现实应用需要对图结构数据进行成对分析（例如场景图匹配、代码搜索和药物相互作用预测）。为此，最近的研究开始将重点转向学习图对之间的交互。尽管这些方法的性能有所提升，但仍存在一些局限性，即交互是在节点级别进行考虑，导致计算成本高和性能亚优。为了解决这个问题，我们提出了一种新颖而高效的图级方法，利用共同注意力在图汇聚中提取交互表示。我们的方法，称为共同注意力图汇聚（CAGPool），在使用实际数据集进行分类和回归任务时，相对于现有方法展现出竞争性能。

    Graph Neural Networks (GNNs) have proven to be effective in processing and learning from graph-structured data. However, previous works mainly focused on understanding single graph inputs while many real-world applications require pair-wise analysis for graph-structured data (e.g., scene graph matching, code searching, and drug-drug interaction prediction). To this end, recent works have shifted their focus to learning the interaction between pairs of graphs. Despite their improved performance, these works were still limited in that the interactions were considered at the node-level, resulting in high computational costs and suboptimal performance. To address this issue, we propose a novel and efficient graph-level approach for extracting interaction representations using co-attention in graph pooling. Our method, Co-Attention Graph Pooling (CAGPool), exhibits competitive performance relative to existing methods in both classification and regression tasks using real-world datasets, whi
    
[^22]: 确定性特征排序

    Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])

    [http://arxiv.org/abs/2307.15361](http://arxiv.org/abs/2307.15361)

    提出了一种确定性特征排序的方法，该方法通过特征重要性值的两两比较，可以产生排序和同时的置信区间，并且可以选择前k个集合。

    

    特征重要性的解释通常依赖于特征的相对顺序而不是数值本身，也就是排序。然而，由于计算重要性值时使用的样本量较小，排序可能不稳定。我们提出了一种事后重要性方法，可以产生一种排序和同时的置信区间。基于特征重要性值的两两比较，我们的方法可以保证高概率包含“真实”（无限样本）排序，并允许选择前k个集合。

    Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
    
[^23]: Med-HALT:大规模语言模型中医疗领域幻觉测试

    Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])

    [http://arxiv.org/abs/2307.15343](http://arxiv.org/abs/2307.15343)

    Med-HALT是一个新的基准和数据集，用于评估和减少大规模语言模型中医疗领域的幻觉问题。这个数据集包括多种创新的测试模式，并评估了领先的LLMs在性能上的差异。

    

    本研究论文关注大规模语言模型（LLMs）中幻觉问题的挑战，特别是在医疗领域的背景下。幻觉指这些模型生成了合理但未经验证或错误的信息，这可能对医疗应用产生严重影响。我们提出了一个新的基准和数据集，Med-HALT（医疗领域幻觉测试），专门设计用于评估和减少幻觉。Med-HALT提供了一个多元化的跨国数据集，这些数据集来自不同国家的医疗检查，包括多种创新的测试模式。Med-HALT包括两类测试：推理和基于记忆的幻觉测试，旨在评估LLMs的问题解决和信息检索能力。我们的研究评估了文本Davinci，GPT-3.5，LlaMa-2，MPT和Falcon等领先的LLMs，揭示了它们在性能上的显著差异。这篇论文提供了有关数据集的详细见解，促进了进一步的研究和发展。

    This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
    
[^24]: 思维的骨架：大型语言模型可以进行并行解码

    Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])

    [http://arxiv.org/abs/2307.15337](http://arxiv.org/abs/2307.15337)

    本研究提出了一种名为“思维的骨架”的方法，可以通过并行解码来减少大型语言模型的生成延迟。这种方法不仅显著提高了速度，还可以潜在地提高答案质量。

    

    本研究旨在减少大型语言模型（LLMs）的端到端生成延迟。高生成延迟的一个主要原因是几乎所有最先进的LLMs都采用了顺序解码方法。在本研究中，受到人类的思考和写作过程的启发，我们提出了“思维的骨架”（SoT），它指导LLMs首先生成答案的骨架，然后通过并行API调用或批量解码来并行完成每个骨架点的内容。SoT不仅显著提高了速度（在11个不同的LLMs上提高了最多2.39倍），而且还可以潜在地提高在多个问题类别上的答案质量，包括多样性和相关性。SoT是一种针对效率的数据导向优化的初步尝试，并揭示了将LLMs推动更像人类思考以提高答案质量的潜力。

    This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
    
[^25]: 使用预训练语言模型进行立场识别的教程：BERT微调和提示大型语言模型

    Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])

    [http://arxiv.org/abs/2307.15331](http://arxiv.org/abs/2307.15331)

    这篇论文提供了两个教程，介绍了使用BERT微调和提示大型语言模型进行Twitter立场识别的方法。教程通过实例代码和可视化分析，展示了少样本ChatGPT和FLAN-T5的优势，同时提供了对BERT模型的训练和评估的指导。这些教程使学习者能够掌握运用先进方法进行立场识别的实践经验。

    

    本文提供了两个独立的教程，介绍了使用BERT微调和提示大型语言模型（LLMs）在Twitter数据上进行立场识别。第一个教程解释了BERT的架构和分词，指导用户通过使用HuggingFace transformers训练、调优和评估标准和领域特定的BERT模型。第二个教程侧重于构建提示和少样本示例，从ChatGPT和开源FLAN-T5中引出立场而无需进行微调。采用了各种提示策略，并使用混淆矩阵和宏F1分数进行评估。这些教程提供了代码、可视化和洞察力，揭示了少样本ChatGPT和FLAN-T5的优势，它们胜过了微调的BERT。通过以易于理解、实践为导向的方式同时涵盖模型微调和提示技术，这些教程使学习者能够获得对立场检测的尖端方法的实际经验。

    This paper presents two self-contained tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models with HuggingFace transformers. The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5 which outperform fine-tuned BERTs. By covering both model fine-tuning and prompting-based techniques in an accessible, hands-on manner, these tutorials enable learners to gain applied experience with cutting-edge methods for stance detection.
    
[^26]: 强大的视觉模拟与实际机器人操作的跨界迁移

    Robust Visual Sim-to-Real Transfer for Robotic Manipulation. (arXiv:2307.15320v1 [cs.RO])

    [http://arxiv.org/abs/2307.15320](http://arxiv.org/abs/2307.15320)

    该论文研究了视觉模拟与实际机器人操作的跨界迁移问题，并提出了域随机化方法来弥合这一差距。通过离线代理任务和模拟训练，成功地优化了域随机化参数，从而实现了在真实机器人上的有效应用。

    

    在模拟环境中学习视觉动作策略比在真实世界中更安全且更便宜。然而，由于模拟和真实数据之间的差异，经过模拟训练的策略在转移到真实机器人时经常失败。为了弥合视觉模拟与实际领域的差距，常用的方法是域随机化(domain randomization, DR)。然而，过去的研究主要评估了DR在姿态估计和物体检测等脱离力的任务上的表现，在这里，我们系统地探索了视觉域随机化方法，并在一系列具有挑战性的机器人操作任务上对其进行了基准测试。我们提出了一个离线代理任务，用于选择纹理随机化、光照随机化、物体颜色变化和相机参数等DR参数。特别值得注意的是，我们证明了DR参数对于离线代理任务和在线策略具有类似的影响。因此，我们使用经过离线优化的DR参数在模拟环境中训练视觉动作策略，并直接应用这些策略到实际机器人上。

    Learning visuomotor policies in simulation is much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an off-line proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our off-line proxy task and on-line policies. We, hence, use off-line optimized DR parameters to train visuomotor policies in simulation and directly apply such 
    
[^27]: DiffKendall:一种利用可微分Kendall排名相关性进行少样本学习的新方法

    DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation. (arXiv:2307.15317v1 [cs.CV])

    [http://arxiv.org/abs/2307.15317](http://arxiv.org/abs/2307.15317)

    本文提出了一种利用可微分Kendall排名相关性进行少样本学习的新方法，证明了特征通道的重要性排序在少样本学习中比几何相似度度量更可靠，并且实验证明在推理过程中用Kendall排名相关性替换几何相似度度量能够提高少样本学习性能。

    

    少样本学习旨在将在基本数据集上训练的模型适应到模型之前未见过的新领域的任务。这经常导致新类别上通道上特征值的分布相对均匀，难以确定新任务中通道的重要性。标准的少样本学习方法使用几何相似度度量，如余弦相似度和负欧几里德距离，来衡量两个特征之间的语义相关性。然而，在少样本学习的情况下，具有高几何相似度的特征可能具有不同的语义。本文证明了特征通道的重要性排序在少样本学习中比几何相似度度量更可靠。我们观察到，仅在推理过程中用Kendall排名相关性替换几何相似度度量能够提高在各种数据集中的少样本学习性能。

    Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with diffe
    
[^28]: 高效多用户AI下载通过可重复使用的知识广播

    Efficient Multiuser AI Downloading via Reusable Knowledge Broadcasting. (arXiv:2307.15316v1 [cs.IT])

    [http://arxiv.org/abs/2307.15316](http://arxiv.org/abs/2307.15316)

    通过利用可重复使用的知识进行参数广播，我们提出了一个称为MBA的框架，以在6G移动网络中实现高效的多用户AI下载。该框架包括参数选择和功率控制的联合设计，以减少通信开销并提高设备的模型性能。

    

    对于6G移动网络，原地模型下载已成为一种重要的用例，以实现边缘设备上的实时自适应人工智能。然而，通过无线链路将多种高维模型同时下载到多个设备上造成了重大的通信瓶颈。为了克服这一瓶颈，我们提出了模型广播和组合(MBA)框架，它是首次尝试利用可重复使用的知识，即任务之间共享的参数，以减少通信开销的参数广播。MBA框架包括两个关键组成部分。第一个是MBA协议，它定义了系统操作，包括从模型库中选择参数，广播的功率控制以及设备上的模型组装。第二个组件是参数选择和功率控制(PS-PC)的联合设计，它保证了设备的模型性能，并最小化了下载开销。

    For the 6G mobile networks, in-situ model downloading has emerged as an important use case to enable real-time adaptive artificial intelligence on edge devices. However, the simultaneous downloading of diverse and high-dimensional models to multiple devices over wireless links presents a significant communication bottleneck. To overcome the bottleneck, we propose the framework of model broadcasting and assembling (MBA), which represents the first attempt on leveraging reusable knowledge, referring to shared parameters among tasks, to enable parameter broadcasting to reduce communication overhead. The MBA framework comprises two key components. The first, the MBA protocol, defines the system operations including parameter selection from a model library, power control for broadcasting, and model assembling at devices. The second component is the joint design of parameter-selection-and-power-control (PS-PC), which provides guarantees on devices' model performance and minimizes the downloa
    
[^29]: WC-SBERT: 利用SBERT和自训练解决零样本文本分类问题的研究

    WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories. (arXiv:2307.15293v1 [cs.CL])

    [http://arxiv.org/abs/2307.15293](http://arxiv.org/abs/2307.15293)

    WC-SBERT提出了一种利用SBERT和自训练解决零样本文本分类问题的方法，通过使用维基百科作为训练集和建立类别对之间的正相关关系，实现快速且准确的分类。

    

    我们的研究专注于解决自然语言处理中的零样本文本分类问题，并特别强调创新的自训练策略。为了实现这一目标，我们提出了一种新颖的自训练策略，使用标签而非文本进行训练，显著减少了模型的训练时间。具体来说，我们使用来自维基百科的类别作为训练集，并利用SBERT预训练模型在同一文本中的类别对之间建立正相关关系，便于进行联想训练。对于新的测试数据集，我们改进了原始自训练方法，消除了需要每个目标数据集的先前训练和测试数据的需求。相反，我们采用维基百科作为统一的训练数据集，更好地近似零样本情境。这种修改方式可以快速进行不同数据集的微调和推断，大大减少了自训练所需的时间。我们的实验结果表明，这种方法比以往的方法具有更好的性能。

    Our research focuses on solving the zero-shot text classification problem in NLP, with a particular emphasis on innovative self-training strategies. To achieve this objective, we propose a novel self-training strategy that uses labels rather than text for training, significantly reducing the model's training time. Specifically, we use categories from Wikipedia as our training set and leverage the SBERT pre-trained model to establish positive correlations between pairs of categories within the same text, facilitating associative training. For new test datasets, we have improved the original self-training approach, eliminating the need for prior training and testing data from each target dataset. Instead, we adopt Wikipedia as a unified training dataset to better approximate the zero-shot scenario. This modification allows for rapid fine-tuning and inference across different datasets, greatly reducing the time required for self-training. Our experimental results demonstrate that this met
    
[^30]: 带有掩码困难实例挖掘的多示例学习框架用于全切片图像分类

    Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. (arXiv:2307.15254v1 [cs.CV])

    [http://arxiv.org/abs/2307.15254](http://arxiv.org/abs/2307.15254)

    这篇论文提出了一种带有掩码困难实例挖掘的多示例学习框架，用于全切片图像分类。该框架通过使用共享学习结构和一致性约束来探索潜在的难以分类的实例，并通过动量教师隐式挖掘这些实例来训练学生模型，从而提高分类性能。

    

    全切片图像（WSI）分类通常被形式化为多示例学习（MIL）问题。由于阳性组织仅占了吉比像素WSI的一小部分，现有的MIL方法直观地侧重于通过注意力机制识别显著实例。然而，这导致偏向易于分类的实例，忽视了难以分类的实例。一些文献揭示了困难示例对于准确建模边界是有益的。通过将这一思想应用到实例级别，我们详细阐述了一种新的MIL框架，即带有掩码困难实例挖掘的MIL（MHIM-MIL），它使用一个共享学习结构（教师-学生）和一致性约束来探索潜在的困难实例。使用基于注意力分数的多个实例掩码策略，MHIM-MIL采用动量教师来隐式挖掘用于训练学生模型的困难实例，学生模型可以是任何基于注意力的MIL模型。这个反直觉的策略对于提高分类性能是至关重要的。

    The whole slide image (WSI) classification is often formulated as a multiple instance learning (MIL) problem. Since the positive tissue is only a small fraction of the gigapixel WSI,existing MIL methods intuitively focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting hard-to-classify instances.Some literature has revealed that hard examples are beneficial for modeling a discriminative boundary accurately.By applying such an idea at the instance level,we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a consistency constraint to explore the potential hard instances. With several instance masking strategies based on attention scores, MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention-based MIL model.This counter-intuitive strategy essent
    
[^31]: 《在统计异质实验设计下的联邦学习实用配方》

    A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design. (arXiv:2307.15245v1 [cs.LG])

    [http://arxiv.org/abs/2307.15245](http://arxiv.org/abs/2307.15245)

    本文针对联邦学习在统计异质实验设计下的问题进行了全面研究，提供了设计有意义和具有良好激励机制的FL实验设置的见解和建议。

    

    最近几年，联邦学习（FL）一直是研究的热点领域。已经有许多研究在处理数据异质性时对FL进行了改进。然而，尽管存在许多论文，但该领域的进展状况是未知的。许多研究采用了不一致的实验设置，并且没有对FL特定实验变量对结果的影响进行全面的研究，也没有提供一个更可比性和一致性的FL实验设置的实践见解。此外，存在多个基准和混杂变量进一步增加了不一致性和不确定性。在这项工作中，我们首次对FL特定实验变量与彼此之间和性能结果的影响进行了全面研究，提供了一些见解和建议，为设计有意义和具有良好激励机制的FL实验设置。我们还通过发布FedZoo-Bench来帮助社区，

    Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench,
    
[^32]: BOURNE: 基于自助式自监督学习的统一图异常检测框架

    BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection. (arXiv:2307.15244v1 [cs.SI])

    [http://arxiv.org/abs/2307.15244](http://arxiv.org/abs/2307.15244)

    BOURNE是一种基于自助式自监督学习的统一图异常检测框架，旨在克服节点和边异常检测之间的独立性局限以及负对采样带来的高计算成本问题。

    

    最近几年，由于在社交网络、金融风险管理和流量分析等领域中的关键应用，图异常检测 (GAD) 越来越受到关注。现有的GAD方法可以根据被检测的图对象的类型将其分类为节点和边异常检测模型。然而，这些方法通常将节点和边异常视为独立的任务，忽视了它们在现实世界图中的关联和频繁共现。因此，它们无法利用节点和边异常提供的互相检测的互补信息。此外，最先进的GAD方法，如CoLA和SL-GAD，在对比学习中严重依赖负对采样，这导致高计算成本，限制了它们在大规模图中的可扩展性。为了解决这些限制，我们提出了一种基于自助式自监督学习的新型统一图异常检测框架，命名为BOURNE。

    Graph anomaly detection (GAD) has gained increasing attention in recent years due to its critical application in a wide range of domains, such as social networks, financial risk management, and traffic analysis. Existing GAD methods can be categorized into node and edge anomaly detection models based on the type of graph objects being detected. However, these methods typically treat node and edge anomalies as separate tasks, overlooking their associations and frequent co-occurrences in real-world graphs. As a result, they fail to leverage the complementary information provided by node and edge anomalies for mutual detection. Additionally, state-of-the-art GAD methods, such as CoLA and SL-GAD, heavily rely on negative pair sampling in contrastive learning, which incurs high computational costs, hindering their scalability to large graphs. To address these limitations, we propose a novel unified graph anomaly detection framework based on bootstrapped self-supervised learning (named BOURN
    
[^33]: 通过观看数百个手术视频讲座学习多模态表示

    Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])

    [http://arxiv.org/abs/2307.15220](http://arxiv.org/abs/2307.15220)

    通过观看手术视频讲座，我们提出了一种新方法，SurgVLP，通过利用手术视频讲座中的语音和视觉信息进行多模态表示学习，并解决了手术相关语言挑战。

    

    最近在外科计算机视觉应用方面的进展主要依靠完全监督方法，主要使用视觉数据。这些方法依赖于手动注释的手术视频来预测一组固定的对象类别，限制了它们在未见手术程序和后续任务上的通用性。在这项工作中，我们提出了一个观点，即通过开放的手术电子学习平台提供的手术视频讲座可以为多模态表示学习提供有效的监督信号，而无需依赖手动注释。我们通过使用多个互补的自动语音识别系统生成文本转录来解决手术视频讲座中存在的手术相关语言挑战。然后，我们提出了一种新的方法，SurgVLP - 手术视觉语言预训练，用于多模态表示学习。SurgVLP构建了一种新的对比学习目标，将视频剪辑嵌入与相应的文本嵌入对齐。

    Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
    
[^34]: 可达性贫民离散竞价游戏

    Reachability Poorman Discrete-Bidding Games. (arXiv:2307.15218v1 [cs.GT])

    [http://arxiv.org/abs/2307.15218](http://arxiv.org/abs/2307.15218)

    该论文研究了一种名为竞价游戏的图游戏，首次考虑了贫民离散竞价，对出价进行了粒度限制，并且较高的出价支付给银行。研究关注阈值预算，以确保玩家1对抗给定玩家2的预算的必要和充分条件。

    

    我们考虑一类双人零和图游戏，称为竞价游戏。游戏的进行如下：两位玩家拥有有限的预算，在图的一个顶点上放置一个令牌，每轮玩家同时提交竞价，出价较高者移动令牌，如果出价相同，则有利于玩家1。只有当令牌访问到指定的目标顶点时，玩家1才能赢得游戏。我们首次考虑了“贫民离散竞价”，其中竞价的粒度受限制，较高的出价支付给银行。先前的研究要么没有施加粒度限制，要么考虑了“富翁”竞价（出价支付给对手）。虽然后一种机制在技术上更易于理解，但前者在实践角度更具吸引力。我们的研究着眼于“阈值预算”，即玩家1为确保对抗给定玩家2的预算而必需的初始预算的必要性和充分性。我们首先证明了存在性。

    We consider {\em bidding games}, a class of two-player zero-sum {\em graph games}. The game proceeds as follows. Both players have bounded budgets. A token is placed on a vertex of a graph, in each turn the players simultaneously submit bids, and the higher bidder moves the token, where we break bidding ties in favor of Player 1. Player 1 wins the game iff the token visits a designated target vertex. We consider, for the first time, {\em poorman discrete-bidding} in which the granularity of the bids is restricted and the higher bid is paid to the bank. Previous work either did not impose granularity restrictions or considered {\em Richman} bidding (bids are paid to the opponent). While the latter mechanisms are technically more accessible, the former is more appealing from a practical standpoint. Our study focuses on {\em threshold budgets}, which is the necessary and sufficient initial budget required for Player 1 to ensure winning against a given Player 2 budget. We first show existe
    
[^35]: 从人类反馈中进行强化学习的开放问题和基本限制

    Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])

    [http://arxiv.org/abs/2307.15217](http://arxiv.org/abs/2307.15217)

    本文调查了从人类反馈中进行强化学习的开放问题和基本限制，并提出了加强社会监督的审计和披露标准。

    

    从人类反馈中进行强化学习（RLHF）是一种训练人工智能系统与人类目标保持一致的技术。RLHF已成为微调最新的大型语言模型（LLM）的核心方法。尽管如此受欢迎，但系统性地系统化其缺陷的公开工作相对较少。在本文中，我们（1）调查了RLHF及相关方法的开放问题和基本限制；（2）概述了了解、改进和补充RLHF的实践技术；以及（3）提出了审计和披露标准以改进RLHF系统的社会监督。我们的工作强调了RLHF的局限性，并强调了以多方面方法开发更安全的人工智能系统的重要性。

    Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
    
[^36]: PromptStyler：基于提示的无源域泛化风格生成

    PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])

    [http://arxiv.org/abs/2307.15199](http://arxiv.org/abs/2307.15199)

    提出了PromptStyler，通过使用提示合成样式特征，解决了无源域泛化的问题。该方法通过学习样式词向量生成多样的样式，并通过强制样式内容特征与内容特征靠近来保证样式不会扭曲内容信息。在多个数据集上取得了最先进的结果。

    

    在联合视觉语言空间中，文本特征（如“一张狗的照片”）可以有效地表示其相关的图像特征（如狗的照片）。受此启发，我们提出了PromptStyler，通过使用提示来合成各种样式，而不使用任何图像来处理无源域泛化中的分布偏移。我们的方法通过可学习的样式词向量为伪词S*生成多样的样式特征（如“a S* style of a”）。为了确保学习到的样式不会扭曲内容信息，我们强制要求样式内容特征（如“a S* style of a [class]”）在联合视觉语言空间中靠近其对应的内容特征（如“[class]”）。在学习样式词向量之后，我们使用合成的样式内容特征训练一个线性分类器。尽管PromptStyler不需要使用任何图像，并且需要额外的训练，但在PACS、VLCS、OfficeHome和DomainNet上取得了最先进的结果。

    In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
    
[^37]: 单次联合提取、注册和分割神经影像数据

    One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data. (arXiv:2307.15198v1 [cs.CV])

    [http://arxiv.org/abs/2307.15198](http://arxiv.org/abs/2307.15198)

    本文研究了在神经影像数据中单次联合提取、注册和分割的问题，通过利用一个带有标签的模板图像，避免了传统方法中需要大量训练样本和手动质量控制的问题。

    

    大脑提取、注册和分割是神经影像研究中不可或缺的预处理步骤。其目的是从原始成像扫描中提取出大脑（提取步骤），将其与目标大脑图像对齐（注册步骤），并标记解剖学大脑区域（分割步骤）。传统方法通常专注于在监督设置中为提取、注册和分割任务开发独立的方法。这些方法的性能在很大程度上取决于训练样本的数量和专家进行视觉检查以进行错误更正的程度。然而，在许多医学研究中，收集体素级标签并对高维神经影像（例如3D MRI）进行手动质量控制是昂贵且耗时的。本文研究了在神经影像数据中单次联合提取、注册和分割的问题，该方法仅利用一个带有标签的模板图像（也称为

    Brain extraction, registration and segmentation are indispensable preprocessing steps in neuroimaging studies. The aim is to extract the brain from raw imaging scans (i.e., extraction step), align it with a target brain image (i.e., registration step) and label the anatomical brain regions (i.e., segmentation step). Conventional studies typically focus on developing separate methods for the extraction, registration and segmentation tasks in a supervised setting. The performance of these methods is largely contingent on the quantity of training samples and the extent of visual inspections carried out by experts for error correction. Nevertheless, collecting voxel-level labels and performing manual quality control on high-dimensional neuroimages (e.g., 3D MRI) are expensive and time-consuming in many medical studies. In this paper, we study the problem of one-shot joint extraction, registration and segmentation in neuroimaging data, which exploits only one labeled template image (a.k.a. 
    
[^38]: 在重复的多单位付费拍卖中学习

    Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])

    [http://arxiv.org/abs/2307.15193](http://arxiv.org/abs/2307.15193)

    本论文研究了在重复的多单位付费拍卖中学习如何出价的问题。通过在离线设置中优化出价向量，并利用多项式时间动态规划方案，设计了具有多项式时间和空间复杂度的在线学习算法。

    

    受碳排放交易方案、国债拍卖和采购拍卖的启发，这些都涉及拍卖同质的多个单位，我们考虑了如何在重复的多单位付费拍卖中学习如何出价的问题。在每个拍卖中，大量（相同的）物品将被分配给最高的出价，每个中标价等于出价本身。由于行动空间的组合性质，学习如何在付费拍卖中出价是具有挑战性的。为了克服这个挑战，我们关注离线设置，其中投标人通过只能访问其他投标人过去提交的出价来优化他们的出价向量。我们证明了离线问题的最优解可以使用多项式时间动态规划（DP）方案来获得。我们利用DP方案的结构，设计了具有多项式时间和空间复杂度的在线学习算法。

    Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
    
[^39]: Med-Flamingo: 一种多模态医学少样本学习器

    Med-Flamingo: a Multimodal Medical Few-shot Learner. (arXiv:2307.15189v1 [cs.CV])

    [http://arxiv.org/abs/2307.15189](http://arxiv.org/abs/2307.15189)

    Med-Flamingo是一种多模态医学少样本学习器，通过在医学图文数据上进行预训练，实现了少样本生成式医学视觉问答的能力。

    

    医学是一个多方面的领域，需要跨多种形式的信息进行综合。医学生成视觉语言模型（VLMs）朝着这个方向迈出了第一步，并承诺有许多令人兴奋的临床应用。但是，现有的模型通常需要在大规模的下游数据集上进行微调，这是一个重大限制，因为在许多医学应用中，数据很少，需要能够从少量实例中实时学习的模型。在这里，我们提出了Med-Flamingo，一种针对医学领域的多模态少样本学习器。在OpenFlamingo-9B的基础上，我们继续对医学图文数据进行配对和交错的预训练，Med-Flamingo解锁了少样本生成式医学视觉问答（VQA）能力，我们在多个数据集上进行了评估，包括一个新颖的具有挑战性的开放式VQA数据集，其中包括视觉USMLE风格问题。此外，我们进行了第一个人工评估实验。

    Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluati
    
[^40]: RCT拒绝抽样用于因果估计评估

    RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])

    [http://arxiv.org/abs/2307.15176](http://arxiv.org/abs/2307.15176)

    该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。

    

    混淆是从观测数据中无偏估计因果效应的一个重要障碍。对于高维协变量的情况，如文本数据、基因组学或行为社会科学，研究人员提出了适应机器学习方法进行因果估计的调整方法。然而，这些调整方法的经验评估一直存在困难和限制。在这项工作中，我们基于一种有前景的经验评估策略，简化了评估设计，并使用真实数据：对随机控制试验(RCT)进行子抽样，以创建混淆的观测数据集，同时使用RCT的平均因果效应作为基准真实值。我们提出了一种新的抽样算法，称为RCT拒绝抽样，并提供了理论保证，以确保观测数据的因果识别成立，从而可以与基准RCT进行有效比较。通过使用合成数据，我们展示了我们的算法在...

    Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
    
[^41]: VISU参加WASSA 2023共享任务：利用BERT和堆叠嵌入检测对新闻故事的情绪反应

    VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])

    [http://arxiv.org/abs/2307.15164](http://arxiv.org/abs/2307.15164)

    本研究中，我们开发了一种利用深度学习模型和词嵌入表示的策略来捕捉复杂对话中情绪表达的微妙差异。我们的实验结果在情绪检测任务中取得了良好的效果，并在小样本和不平衡的混合目标情绪数据集中得到了验证。

    

    我们的系统VISU参加了WASSA 2023共享任务（3），即从对新闻文章的反应中写的文章中进行情绪分类。从复杂对话中检测情绪是具有挑战性的，通常需要上下文/领域的理解。因此，在这项研究中，我们专注于开发深度学习（DL）模型，使用定制的预处理策略与词嵌入表示的组合来捕捉表达的情绪的微妙差异。我们的实验使用了静态和上下文嵌入（单独和堆叠）与双向长短期记忆（BiLSTM）和Transformer模型。在情绪检测任务中，我们占据了第十名，得分为0.2717的宏F1-分数，验证了我们实施的方法在小样本和不平衡的混合目标情绪数据集中的有效性。

    Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.
    
[^42]: 受皮层启发的使用ReD-SOM模型恢复受损信号模态性的学习

    Cortex Inspired Learning to Recover Damaged Signal Modality with ReD-SOM Model. (arXiv:2307.15095v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.15095](http://arxiv.org/abs/2307.15095)

    这项研究受到人类大脑中的麦格尔克效应的启发，提出了使用自组织映射和Hebb连接在ReD-SOM模型中重建丢失数据模态的方法。实验结果证明了该方法的有效性。

    

    AI和认知科学领域的最新进展开辟了以前无法研究的新挑战。其中一项现代任务是通过使用其他模态的数据来恢复丢失的数据模态。人脑的功能中存在一种类似效应（称为麦格尔克效应），其中一种信息模态干扰另一种信息模态，改变其感知。在本文中，我们提出一种模拟这种效应并将其用于使用变分自动编码器、自组织映射和Hebb连接在统一的ReD-SOM（重新进入深度自组织映射）模型中重建丢失的数据模态的方法。我们受到人类大脑在不同模态下使用不同脑区的能力的启发，即在某一模态中缺乏信息的情况下。这种新方法不仅改善了对模糊数据的分析，而且还恢复了预期的信号！在多模态数据集上获得的结果证实了该方法的有效性。

    Recent progress in the fields of AI and cognitive sciences opens up new challenges that were previously inaccessible to study. One of such modern tasks is recovering lost data of one modality by using the data from another one. A similar effect (called the McGurk Effect) has been found in the functioning of the human brain. Observing this effect, one modality of information interferes with another, changing its perception. In this paper, we propose a way to simulate such an effect and use it to reconstruct lost data modalities by combining Variational Auto-Encoders, Self-Organizing Maps, and Hebb connections in a unified ReD-SOM (Reentering Deep Self-organizing Map) model. We are inspired by human's capability to use different zones of the brain in different modalities, in case of having a lack of information in one of the modalities. This new approach not only improves the analysis of ambiguous data but also restores the intended signal! The results obtained on the multimodal dataset 
    
[^43]: 关于沉积计算及其超越传统机器学习的跨学科应用的综述

    A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning. (arXiv:2307.15092v1 [cs.NE])

    [http://arxiv.org/abs/2307.15092](http://arxiv.org/abs/2307.15092)

    沉积计算是一个具有随机连接的循环神经网络，其简单的结构和丰富的动力学使其在各种应用中能够生成适当的响应。它的跨学科应用包括物理硬件实现和生物设备，同时也有助于理解大脑机制。

    

    沉积计算（RC）首先用于时间信号处理，它是一个具有随机连接的循环神经网络。一旦初始化，连接强度将保持不变。这种简单的结构将RC转化为一个非线性动力系统，将低维输入映射到高维空间中。该模型的丰富动力学、线性可分性和记忆容量使得简单的线性读出能够为各种应用生成适当的响应。RC涵盖了远远超出机器学习范围的领域，因为已经证明复杂的动力学可以在各种物理硬件实现和生物设备中实现。这样可以提供更大的灵活性和更短的计算时间。此外，模型动力学触发的神经元响应也揭示了理解大脑机制的方法，这些机制也利用类似的动力学过程。虽然关于RC的文献非常庞大且碎片化，但我们在这里对RC的最新发展进行了统一的回顾。

    Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications. RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time. Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes. While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent devel
    
[^44]: 理解卷积神经网络的前向过程

    Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v1 [cs.LG])

    [http://arxiv.org/abs/2307.15090](http://arxiv.org/abs/2307.15090)

    本文揭示了CNN前向处理中的选择性旋转机制，并通过使用结构化数学工具对数据进行统计和分析，发现了人工神经网络和人脑在数据处理模式上的一致性。

    

    本文揭示了CNN前向处理中的选择性旋转。它阐明了激活函数作为一个分析机制，将输入数据的旋转方面统一量化。实验证明，这种定义的方法论反映了进程网络根据统计指标区分输入的能力，可以通过应用结构化的数学工具来理解或分析。我们的发现还揭示了人工神经网络和人脑在数据处理模式上的一致性。

    This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
    
[^45]: 数据集中的信息提升子群发现

    Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])

    [http://arxiv.org/abs/2307.15089](http://arxiv.org/abs/2307.15089)

    该论文研究了在数据集中通过信息提升的方法进行子群发现。具体针对肺癌治疗，在保持或提高治疗效果的同时减少副作用对于改善患者的生活质量非常重要，临床指南虽然提供了治疗建议，但仍未将治疗结果纳入考量。

    

    肺癌是癌症死亡的主要原因。预计2023年将有超过238,340例新的肺癌患者，其中有超过127,070例死亡。选择正确的治疗方案是提高存活率和改善患者生活质量的重要因素。癌症治疗可能引发副作用，这些毒副反应会引起不同的健康问题，影响患者的生活质量。因此，在保持或提高治疗效果的同时减少治疗副作用是临床角度要追求的重要目标。另一方面，临床指南包括癌症治疗建议的一般知识，以协助临床医生。尽管他们根据癌症疾病方面和个体患者特征提供治疗建议，但并未提供基于治疗结果的统计分析。因此，需要对临床指南与治疗结果进行比较。

    Lung cancer is the leading cause of cancer death. More than 238,340 new cases of lung cancer patients are expected in 2023, with an estimation of more than 127,070 deaths. Choosing the correct treatment is an important element to enhance the probability of survival and to improve patient's quality of life. Cancer treatments might provoke secondary effects. These toxicities cause different health problems that impact the patient's quality of life. Hence, reducing treatments toxicities while maintaining or improving their effectivenes is an important goal that aims to be pursued from the clinical perspective. On the other hand, clinical guidelines include general knowledge about cancer treatment recommendations to assist clinicians. Although they provide treatment recommendations based on cancer disease aspects and individual patient features, a statistical analysis taking into account treatment outcomes is not provided here. Therefore, the comparison between clinical guidelines with tre
    
[^46]: 使用大型语言模型将患者与临床试验匹配

    Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])

    [http://arxiv.org/abs/2307.15051](http://arxiv.org/abs/2307.15051)

    本研究调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力，并引入了TrialGPT架构，该架构能够准确预测合格性并提供解释，实验证明其有效性。

    

    临床试验在推动药物研发和基于证据的医学方面非常重要，但患者招募常常受到限制。在这项工作中，我们调查了使用大型语言模型（LLMs）来帮助患者和转诊医生识别合适的临床试验的潜力。具体而言，我们引入了一种新颖的架构TrialGPT，采用LLMs预测基于标准的合格性，并提供详细的解释，并根据患者病历中的自由文本来对候选临床试验进行排名和排除。我们在三个公开可用的184名患者和18,238个注释的临床试验的队列上评估了TrialGPT。实验结果表明几个关键发现：第一，TrialGPT在标准级别的预测准确性上表现出很高的准确率，并提供准确的解释。第二，TrialGPT的综合试验级别评分与专家标注的合格性高度相关。第三，这些评分

    Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
    
[^47]: 使用Desbordante解决数据质量问题：一项演示

    Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])

    [http://arxiv.org/abs/2307.14935](http://arxiv.org/abs/2307.14935)

    Desbordante是一个旨在解决数据质量问题的工具，通过发现和验证复杂统计信息来帮助现代数据科学家进行数据概要分析。它提供了与现有工具的适当集成，同时考虑到工业级工作负载，并提供描述性的解释来解释模式缺失的原因。

    

    数据概要分析是现代数据驱动行业中的重要过程。其中一个关键组成部分是发现和验证复杂统计信息，包括函数依赖、数据约束、关联规则等。然而，大多数现有的专注于复杂统计的数据概要分析系统在与现代数据科学家使用的工具进行适当集成方面存在问题。这在行业中对这些工具采用造成了重大障碍。此外，现有系统并不考虑工业级工作负载。最后，它们不旨在提供描述性的解释，即为什么找不到给定的模式。这是一个重要问题，因为了解特定模式缺失的根本原因对基于数据的明智决策至关重要。因此，这些模式实际上是消失在空中中：它们的应用范围相对有限，很少被广大公众使用。

    Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.  However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.  Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the
    
[^48]: 混合ASP方法用于半导体制造过程的多目标调度（扩展版本）

    Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v1 [cs.AI])

    [http://arxiv.org/abs/2307.14799](http://arxiv.org/abs/2307.14799)

    本研究通过混合ASP方法解决了实际半导体制造过程的调度问题，将其具体要求建模，并且实现了灵活的机器加工、设置、批处理和维护操作。

    

    现代半导体制造涉及复杂的生产过程，包括数百个操作，从批次发布到完成可能需要数月时间。这些过程中使用的高科技设备多样化，可以在多个阶段上对单个晶圆、批次或批次进行操作，并需要产品特定的设置和专门的维护程序。这种情况与传统的车间调度场景不同，后者具有较不复杂的生产过程和设备，主要关注解决高度组合但抽象的调度问题。在这项工作中，我们通过使用具有差异逻辑的混合ASP模型来对实际的半导体制造过程的调度进行建模，这包括灵活的机器加工、设置、批处理和维护操作。与现有的使用贪婪启发式算法或独立进行调度半导体制造过程的方法不同，

    Modern semiconductor manufacturing involves intricate production processes consisting of hundreds of operations, which can take several months from lot release to completion. The high-tech machines used in these processes are diverse, operate on individual wafers, lots, or batches in multiple stages, and necessitate product-specific setups and specialized maintenance procedures. This situation is different from traditional job-shop scheduling scenarios, which have less complex production processes and machines, and mainly focus on solving highly combinatorial but abstract scheduling problems. In this work, we address the scheduling of realistic semiconductor manufacturing processes by modeling their specific requirements using hybrid Answer Set Programming with difference logic, incorporating flexible machine processing, setup, batching and maintenance operations. Unlike existing methods that schedule semiconductor manufacturing processes locally with greedy heuristics or by independen
    
[^49]: 无注释图像字幕生成的研究：基于检索增强的伪句子生成

    Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])

    [http://arxiv.org/abs/2307.14750](http://arxiv.org/abs/2307.14750)

    本文提出了一种新的无注释图像字幕生成的策略，利用大规模预训练模型的先验知识作为监督，并整合检索过程以进一步增强其效力。该方法能够从不匹配的语料库中检索相关的短区域描述，并利用其生成多样的句子。

    

    近年来，训练无注释图像字幕生成器取得了进展。先前的方法可以分为两种策略：从不匹配的语料库中获取句子，并将其与给定的图像对齐作为伪注释，或者使用外部的图像-文本对对生成器进行预训练。然而，由于对齐的设置存在质量问题，其性能似乎已经达到了极限，而预训练需要大量的计算资源。为了解决这些挑战，我们提出了一种新的策略“LPM + 检索增强学习”，利用来自大规模预训练模型（LPM）的先验知识作为监督，并整合检索过程以进一步增强其效力。具体而言，我们引入了检索增强的伪句子生成（RaPSG），采用高效的方法从不匹配的语料库中检索出高相关的短区域描述，并将其用于生成多样的句子。

    Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety 
    
[^50]: MVMR-FS：基于最大类间差异和最小冗余的非参数特征选择算法

    MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG])

    [http://arxiv.org/abs/2307.14643](http://arxiv.org/abs/2307.14643)

    本文提出了一种基于最大类间差异和最小冗余的非参数特征选择算法(MVMR-FS)，通过使用有监督和无监督核密度估计来度量特征之间的相关性和冗余，并提出了最大类间差异和最小冗余的准则(MVMR)，以辅助特征选择。

    

    如何准确度量特征的相关性和冗余是特征选择领域的一个古老挑战。然而，现有的基于过滤的特征选择方法无法直接度量连续数据的冗余。此外，大多数方法依赖于手动指定特征数量，这在没有专家知识的情况下可能导致错误。本文提出了一种基于最大类间差异和最小冗余的非参数特征选择算法，简称为MVMR-FS。我们首先引入了对特征的有监督和无监督核密度估计，以捕捉它们在类间和整体分布中的相似性和差异。随后，我们提出了最大类间差异和最小冗余（MVMR）的准则，其中利用类间概率分布来反映特征的相关性，利用整体概率分布之间的距离来量化冗余。

    How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundanc
    
[^51]: Duet: 高效且可扩展的混合神经关系理解

    Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])

    [http://arxiv.org/abs/2307.13494](http://arxiv.org/abs/2307.13494)

    Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。

    

    基于概率分布估计的基数估计方法相较于传统方法取得了高精度的估计结果。然而，最先进的方法由于在处理范围查询时使用的采样方法而导致估计成本较高。此外，这种采样方法也使得它们难以区分，因此来自查询工作负载的监督信号很难训练模型以提高基数估计的准确性。在本文中，我们提出了一种新的混合确定性建模方法（Duet）用于基数估计问题，与以前的方法相比，具有更好的效率和可扩展性。Duet可以以更低的时间和内存成本直接估计范围查询的基数，并且以可区分的形式呈现。由于此方法的预测过程是可微分的，我们可以将估计误差较大的查询纳入训练过程以进行改进。

    Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
    
[^52]: 为什么你不清洁眼镜？利用动态光学扰动进行感知攻击

    Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations. (arXiv:2307.13131v1 [cs.CR])

    [http://arxiv.org/abs/2307.13131](http://arxiv.org/abs/2307.13131)

    本文提出了EvilEye，利用透明显示器生成动态物理敌对示例的感知攻击方法，通过利用摄像头的光学特性在多种照明条件下诱导错误分类。

    

    摄像头为基础的模拟人类感知的自主系统越来越多地被集成到安全关键的平台中。因此，已经出现了一个稳定的文献体系，探索对底层机器学习模型进行敌对攻击。将敌对攻击适应于现实世界对攻击者来说是可取的，因为这消除了危害数字系统的需要。然而，真实世界面临着与感知管道中的环境噪声和自主系统的动态性有关的挑战。在本文中，我们采用传感器为先的方法。我们提出了EvilEye，一种利用透明显示器生成动态物理敌对示例的中间人感知攻击。EvilEye利用摄像头的光学特性在各种照明条件下诱导错误分类。为了生成动态扰动，我们将数字攻击的投影形式化为光学视觉逆向问题。

    Camera-based autonomous systems that emulate human perception are increasingly being integrated into safety-critical platforms. Consequently, an established body of literature has emerged that explores adversarial attacks targeting the underlying machine learning models. Adapting adversarial attacks to the physical world is desirable for the attacker, as this removes the need to compromise digital systems. However, the real world poses challenges related to the "survivability" of adversarial manipulations given environmental noise in perception pipelines and the dynamicity of autonomous systems. In this paper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle perception attack that leverages transparent displays to generate dynamic physical adversarial examples. EvilEye exploits the camera's optics to induce misclassifications under a variety of illumination conditions. To generate dynamic perturbations, we formalize the projection of a digital attack into the ph
    
[^53]: 用频率偏差构建更强大的模型

    Towards Building More Robust Models with Frequency Bias. (arXiv:2307.09763v1 [cs.CV])

    [http://arxiv.org/abs/2307.09763](http://arxiv.org/abs/2307.09763)

    该论文提出了一种频率偏好控制模块，通过自适应地重新配置中间特征表示的低频和高频成分，提高了模型在鲁棒性学习中对频率的利用。

    

    尽管深度神经网络在各个领域取得了成功，但对于对抗样本的脆弱性仍然是它们应用广泛的一个主要障碍。最近的一些研究表明，对抗训练的模型强调低频信息的重要性，以达到更高的鲁棒性。虽然已经有一些尝试利用这种频率特性，但直接将低通滤波器应用于输入图像会导致不可逆的信息丢失和对具有不同频率特征的数据集的泛化能力差的问题。本文提出了一种名为频率偏好控制模块的插拔式模块，可以自适应地重新配置中间特征表示的低频和高频成分，更好地利用频率进行鲁棒学习。实证研究表明，我们提出的模块可以轻松地集成到任何对抗训练框架中，并进一步改善其性能。

    The vulnerability of deep neural networks to adversarial samples has been a major impediment to their broad applications, despite their success in various fields. Recently, some works suggested that adversarially-trained models emphasize the importance of low-frequency information to achieve higher robustness. While several attempts have been made to leverage this frequency characteristic, they have all faced the issue that applying low-pass filters directly to input images leads to irreversible loss of discriminative information and poor generalizability to datasets with distinct frequency features. This paper presents a plug-and-play module called the Frequency Preference Control Module that adaptively reconfigures the low- and high-frequency components of intermediate feature representations, providing better utilization of frequency in robust learning. Empirical studies show that our proposed module can be easily incorporated into any adversarial training framework, further improvi
    
[^54]: 大型语言模型的情感智能

    Emotional Intelligence of Large Language Models. (arXiv:2307.09042v1 [cs.AI])

    [http://arxiv.org/abs/2307.09042](http://arxiv.org/abs/2307.09042)

    这项研究评估了大型语言模型（LLMs）的情感智能（EI），并提出了一种新的心理测量评估方法，该方法重点考察了情感理解（EU）能力。评估结果表明，大多数主流LLMs在情感理解方面表现良好。

    

    大型语言模型（LLMs）在许多领域展示出了非凡的能力，主要通过语言生成、知识利用和复杂推理等任务进行评估。然而，它们与人类情感和价值观的一致性尚未得到系统评估，而这对于实际应用至关重要。在这里，我们评估了LLMs的情感智能（EI），包括情感识别、解释和理解，这对于有效的沟通和社交互动至关重要。具体而言，我们首先开发了一种新的心理测量评估，重点是情感理解（EU），这是EI的核心组成部分，适用于人类和LLMs。这个测试需要在现实场景中评估复杂的情感（例如，惊讶、愉快、困惑、自豪），如尽管感觉表现不佳，约翰却意外地获得了最高分。通过从500多名成年人构建的参考框架，我们测试了各种主流LLMs。

    Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achie
    
[^55]: 创建一个支持OpenMP Fortran和C++代码相互翻译的数据集

    Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])

    [http://arxiv.org/abs/2307.07686](http://arxiv.org/abs/2307.07686)

    本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。

    

    在本研究中，我们提出了一个新颖的数据集，用于训练在OpenMP Fortran和C++代码之间进行翻译的机器学习模型。通过精细的代码相似性测试，我们确保了数据集的可靠性和适用性。我们使用定量（CodeBLEU）和定性（人工评估）方法评估了我们数据集的有效性。我们展示了这个数据集如何显著提高大规模语言模型的翻译能力，对于没有先前编码知识的模型，提高了5.1倍，对于具有一定编码熟悉度的模型，提高了9.9倍。我们的工作突显了这个数据集在高性能计算的代码翻译领域的潜力。

    In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
    
[^56]: 超越已知现实：利用反事实解释进行医学研究 (arXiv：2307.02131v2 [cs.AI] 已更新)

    Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research. (arXiv:2307.02131v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.02131](http://arxiv.org/abs/2307.02131)

    本研究利用反事实解释来探索医学研究中的“假如”场景，通过提供个性化和情境特定的见解，拓展了我们对现有边界的理解，并填补了机器学习算法结果解释的缺失。

    

    本研究利用反事实解释来探索医学研究中的“假如”场景，旨在拓展我们对现有边界的理解。具体而言，我们重点研究利用磁共振成像特征来诊断儿科后颅窝脑肿瘤。人工智能和可解释性领域已经见证了越来越多的研究和学术兴趣。然而，机器学习算法结果的人类友好解释的缺乏显著阻碍了这些方法在临床实践中的接受度。为了解决这个问题，我们的方法融入了反事实解释，为检查替代决策场景提供了一种新的方式。这些解释提供了个性化和情境特定的见解，使得我们可以验证预测并澄清不同情况下的差异。重要的是，我们的方法同时保持了统计学的可解释性和人类可理解性。

    This study employs counterfactual explanations to explore "what if?" scenarios in medical research, with the aim of expanding our understanding beyond existing boundaries. Specifically, we focus on utilizing MRI features for diagnosing pediatric posterior fossa brain tumors as a case study. The field of artificial intelligence and explainability has witnessed a growing number of studies and increasing scholarly interest. However, the lack of human-friendly interpretations in explaining the outcomes of machine learning algorithms has significantly hindered the acceptance of these methods by clinicians in their clinical practice. To address this, our approach incorporates counterfactual explanations, providing a novel way to examine alternative decision-making scenarios. These explanations offer personalized and context-specific insights, enabling the validation of predictions and clarification of variations under diverse circumstances. Importantly, our approach maintains both statistica
    
[^57]: ChatGPT在USMLE上的表现：为AI辅助医学教育开启大型语言模型的潜力

    Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. (arXiv:2307.00112v1 [cs.CY])

    [http://arxiv.org/abs/2307.00112](http://arxiv.org/abs/2307.00112)

    这项研究评估了ChatGPT在回答复杂医学问题上的可靠性，发现其生成的答案更加注重上下文并具有较高的可靠性。

    

    人工智能正在以前所未有的方式获得越来越多的关注。自从OpenAI发布ChatGPT以来，语言模型和基于AI的业务的流行度大幅增长。人们在职业和个人生活中越来越普遍地使用ChatGPT。考虑到ChatGPT的广泛使用和人们对其的依赖，本研究旨在确定ChatGPT在回答复杂医学和临床问题上的可靠性。该研究使用哈佛大学解剖学知识和美国医学执照考试（USMLE）问卷来实现目标。本文使用双因素方差分析和事后分析评估了获得的结果。两者都显示出格式和提示之间的系统协变。此外，医生评估者还对结果的准确性、一致性和洞察力进行了独立评价。分析结果发现，ChatGPT生成的答案更加注重上下文并且具有较高的可靠性。

    Artificial intelligence is gaining traction in more ways than ever before. The popularity of language models and AI-based businesses has soared since ChatGPT was made available to the general public via OpenAI. It is becoming increasingly common for people to use ChatGPT both professionally and personally. Considering the widespread use of ChatGPT and the reliance people place on it, this study determined how reliable ChatGPT can be for answering complex medical and clinical questions. Harvard University gross anatomy along with the United States Medical Licensing Examination (USMLE) questionnaire were used to accomplish the objective. The paper evaluated the obtained results using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation between format and prompt. Furthermore, the physician adjudicators independently rated the outcome's accuracy, concordance, and insight. As a result of the analysis, ChatGPT-generated answers were found to be more context-oriented and rep
    
[^58]: 在因子图中自动进行模型比较

    Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v1 [cs.LG])

    [http://arxiv.org/abs/2306.05965](http://arxiv.org/abs/2306.05965)

    本文基于自定义混合节点 Forney 样式的因子图消息传递，实现了高效自动化贝叶斯模型平均、选择和组合，并缩短了模型设计周期。

    

    在文献中，贝叶斯状态和参数估计已经被有效自动化，但对于模型比较尚未如此，因此仍需要容易出错和耗时的手动推导。因此，模型比较经常被忽视和忽略，尽管它很重要。本文通过在Forney样式的因子图上使用自定义混合节点上的消息传递来高效地自动化贝叶斯模型平均、选择和组合。进而可使用缩放因子同时执行参数和状态推断以及模型比较。这种方法缩短了模型设计周期，同时允许简单地扩展到分层和时间模型先验，以适应建模复杂的时变过程。

    Bayesian state and parameter estimation have been automated effectively in the literature, however, this has not yet been the case for model comparison, which therefore still requires error-prone and time-consuming manual derivations. As a result, model comparison is often overlooked and ignored, despite its importance. This paper efficiently automates Bayesian model averaging, selection, and combination by message passing on a Forney-style factor graph with a custom mixture node. Parameter and state inference, and model comparison can then be executed simultaneously using message passing with scale factors. This approach shortens the model design cycle and allows for the straightforward extension to hierarchical and temporal model priors to accommodate for modeling complicated time-varying processes.
    
[^59]: 领域自适应的口咽器官sim-to-real分割

    Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs. (arXiv:2305.10883v1 [cs.AI])

    [http://arxiv.org/abs/2305.10883](http://arxiv.org/abs/2305.10883)

    该研究提出了一种称为IRB-AF的领域自适应的图像分割框架，通过使用IRB和ArtFlow方法来解决口咽器官图像分割中的数据域差异问题，有效提高了分割性能。

    

    视频辅助下的经口气管插管需要使用支持医生将气管导管插入声门而不是食管的内窥镜。越来越多基于机器人的气管插管需要医疗机器人像经验丰富的医生一样区分解剖特征，这可以通过使用监督深度学习技术来模仿。然而口咽器官的真实数据集往往由于开源数据受限和患者隐私问题而难以获取。在本研究中，我们提出了一种领域自适应的Sim-to-Real框架，称为IoU-Ranking Blend-ArtFlow (IRB-AF)，用于口咽器官的图像分割。该框架包括一种称为IoU-Ranking Blend (IRB)的图像融合策略和风格转移方法ArtFlow。其中，IRB通过减轻数据集之间重要的领域差异所导致的实现分割性能差的问题。而ArtFlow则可进一步降低数据集之间的差异性。 本研究采用一种虚拟口咽模型进行实验，结果表明提出的方法可以取得非常好的口咽器官分割效果。

    Video-assisted transoral tracheal intubation (TI) necessitates using an endoscope that helps the physician insert a tracheal tube into the glottis instead of the esophagus. The growing trend of robotic-assisted TI would require a medical robot to distinguish anatomical features like an experienced physician which can be imitated by utilizing supervised deep-learning techniques. However, the real datasets of oropharyngeal organs are often inaccessible due to limited open-source data and patient privacy. In this work, we propose a domain adaptive Sim-to-Real framework called IoU-Ranking Blend-ArtFlow (IRB-AF) for image segmentation of oropharyngeal organs. The framework includes an image blending strategy called IoU-Ranking Blend (IRB) and style-transfer method ArtFlow. Here, IRB alleviates the problem of poor segmentation performance caused by significant datasets domain differences; while ArtFlow is introduced to reduce the discrepancies between datasets further. A virtual oropharynx i
    
[^60]: SANTA：Distantly-Supervised Named Entity Recognition中处理错误和不完整标注噪声的分离策略

    SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v1 [cs.CL])

    [http://arxiv.org/abs/2305.04076](http://arxiv.org/abs/2305.04076)

    本文提出了一种处理Distantly-Supervised Named Entity Recognition中错误和不完整标注噪声的分离策略，使用不同的模型构建来应对两种类型的噪声。

    

    远程监督命名实体识别有效地减轻了监督设置中耗时且昂贵的注释负担，但是无上下文的匹配过程和知识库的有限覆盖引入了不准确和不完整的标注噪音。本研究提出了使用不同的策略来处理两种类型的噪声的SANTA，以解决由不准确和不完整标注带来的挑战。

    Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively. Previous studies either considered only incomplete annotation noise or indiscriminately handle two types of noise with the same strategy. In this paper, we argue that the different causes of two types of noise bring up the requirement of different strategies in model architecture. Therefore, we propose the SANTA to handle these two types of noise separately with (1) Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate decision boundary shifting problem caused by incomplete annotation and a noise-tolerant loss to improve the robustness. Benefiting from our separate tailored strategies, we co
    
[^61]: 物理世界中愚弄热红外探测器

    Fooling Thermal Infrared Detectors in Physical World. (arXiv:2304.10712v1 [cs.CV])

    [http://arxiv.org/abs/2304.10712](http://arxiv.org/abs/2304.10712)

    本论文提出一种新颖的物理攻击方法——对抗性红外块（AdvIB），可以从多个角度对热成像系统执行隐蔽的黑盒攻击，成功诱导目标红外检测器在物理场景中对对象或人类进行误分类，并且不被人类察觉。

    

    红外成像系统在行人检测和自动驾驶等方面有着广泛的应用前景，并且它们的安全性能备受关注。然而，很少有研究探索了红外成像系统在真实世界环境下的安全性。过去的研究使用物理干扰，如小灯泡和热“QR代码”来攻击红外成像探测器，但这种方法很容易被察觉，缺乏隐秘性。其他研究人员使用热和冷块来欺骗红外成像探测器，但这种方法在从多个角度执行攻击方面的能力有限。为了解决这些缺点，我们提出了一种新颖的物理攻击方法，称为对抗性红外块（AdvIB）。通过优化对抗性红外块的物理参数，这种方法可以从多个角度对热成像系统执行隐蔽的黑盒攻击。我们根据其有效性、隐秘性和稳健性评估了所提出的方法。结果表明，AdvIB可以成功诱导目标红外检测器在物理场景中对对象或人类进行误分类，并且不被人类察觉。我们的工作强调了在红外成像系统中提高安全措施的必要性，特别是在真实世界环境中。

    Infrared imaging systems have a vast array of potential applications in pedestrian detection and autonomous driving, and their safety performance is of great concern. However, few studies have explored the safety of infrared imaging systems in real-world settings. Previous research has used physical perturbations such as small bulbs and thermal "QR codes" to attack infrared imaging detectors, but such methods are highly visible and lack stealthiness. Other researchers have used hot and cold blocks to deceive infrared imaging detectors, but this method is limited in its ability to execute attacks from various angles. To address these shortcomings, we propose a novel physical attack called adversarial infrared blocks (AdvIB). By optimizing the physical parameters of the adversarial infrared blocks, this method can execute a stealthy black-box attack on thermal imaging system from various angles. We evaluate the proposed method based on its effectiveness, stealthiness, and robustness. Our
    
[^62]: VISAR：一种带有可视化编程和快速草案原型的人工智能论证写作助手

    VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])

    [http://arxiv.org/abs/2304.07810](http://arxiv.org/abs/2304.07810)

    VISAR是一个AI写作助手，旨在帮助作者提升写作体验和输出。它可以在写作上下文中随时帮助作者构思和修改目标，通过可视化编程来组织论证结构，并提供推荐来增加说服力。自动草案原型可以用来验证计划。

    

    在辩论写作中，作者必须构思分层写作目标，确保其论点的说服力，并通过起草来修订和组织他们的计划。最近大型语言模型（LLM）的进展使得通过聊天界面进行交互式文本生成（例如ChatGPT）成为可能。然而，这种方法常常忽略了隐含的写作上下文和用户意图，缺乏用户控制和自主权，并且提供有限的帮助来进行意义构建和修订写作计划。为了应对这些挑战，我们引入了VISAR，一种AI支持的写作助手系统，旨在帮助作者在其写作上下文中构思和修订分层目标，通过同步文本编辑和可视化编程组织论证结构，并通过论证火花推荐增强说服力。VISAR允许用户使用自动草案原型探索、实验和验证他们的写作计划。一个受控实验室研究证实，VISAR可以通过客观和主观评估，有效地改善用户的写作体验和结果。

    In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confi
    
[^63]: 无专家在线多智能体强化学习中的迁移学习

    Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01170](http://arxiv.org/abs/2303.01170)

    本文提出了Expert-Free Online Transfer Learning (EF-OnTL)算法，在多智能体系统中实现无专家的实时迁移学习。通过动态选择迁移源智能体和要转移的知识，解决了传统迁移学习需要对专家智能体任务有良好理解的问题。

    

    传统上，强化学习中的迁移学习通过将知识从专家智能体转移到新手智能体来解决训练问题，如探索成本、数据可用性和收敛时间。然而，这种迁移需要新手智能体对专家智能体的任务有良好的理解才能有效。作为替代方案，本文提出了一种无专家在线动态迁移学习算法（EF-OnTL），该算法能够在多智能体系统中实现无专家的实时迁移学习。在每一次迁移步骤中，根据智能体的性能和不确定性来动态选择迁移源智能体和要转移的知识。为了提高不确定性估计，我们还提出了一种称为SARS-RND的方法，它是对RND的扩展，可以从智能体的状态、行动、奖励和下一状态中估计不确定性。

    Transfer learning in Reinforcement Learning (RL) has been widely studied to overcome training issues of Deep-RL, i.e., exploration cost, data availability and convergence time, by introducing a way to enhance training phase with external knowledge. Generally, knowledge is transferred from expert-agents to novices. While this fixes the issue for a novice agent, a good understanding of the task on expert agent is required for such transfer to be effective. As an alternative, in this paper we propose Expert-Free Online Transfer Learning (EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer learning in multi-agent system. No dedicated expert exists, and transfer source agent and knowledge to be transferred are dynamically selected at each transfer step based on agents' performance and uncertainty. To improve uncertainty estimation, we also propose State Action Reward Next-State Random Network Distillation (sars-RND), an extension of RND that estimates uncertainty from
    
[^64]: 利用额外安全预算在受限策略优化中高效探索

    Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization. (arXiv:2302.14339v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.14339](http://arxiv.org/abs/2302.14339)

    本文提出了一种名为ESB-CPO的算法，通过在早期阶段放宽约束并引入额外安全预算，在探索效率和约束满足之间取得平衡，从而提高了强化学习中受限策略优化的效果。

    

    强化学习（RL）在大多数机器人控制任务上取得了有希望的结果。学习驱动控制器的安全性是确保控制器有效性的重要概念。当前方法在训练过程中采用整体一致性约束，导致早期探索效率低下。在本文中，我们提出了一种名为额外安全预算的受限策略优化算法（ESB-CPO），以在探索效率和约束满足之间取得平衡。在早期阶段，我们的方法通过引入新的度量标准来放宽不安全转换的实际约束（添加额外安全预算）。随着训练过程的进行，我们优化问题中的约束逐渐变得更紧。同时，理论分析和实验结果表明，我们的方法能够逐步满足最终训练阶段的成本限制要求。在Safety-Gym和Bullet-Safety-Gym基准测试中的评估中，我们的方法表现优秀。

    Reinforcement learning (RL) has achieved promising results on most robotic control tasks. Safety of learning-based controllers is an essential notion of ensuring the effectiveness of the controllers. Current methods adopt whole consistency constraints during the training, thus resulting in inefficient exploration in the early stage. In this paper, we propose an algorithm named Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a balance between the exploration efficiency and the constraints satisfaction. In the early stage, our method loosens the practical constraints of unsafe transitions (adding extra safety budget) with the aid of a new metric we propose. With the training process, the constraints in our optimization problem become tighter. Meanwhile, theoretical analysis and practical experiments demonstrate that our method gradually meets the cost limit's demand in the final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym benchmarks, ou
    
[^65]: 量化和建模多模态交互：一种信息分解框架

    Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12247](http://arxiv.org/abs/2302.12247)

    通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。

    

    对于解决多模态任务所需的交互如何进行量化？最适合捕捉这些交互的多模态模型是什么？为了回答这些问题，我们提出了一种信息论方法来量化输入模态与输出任务之间的冗余度、独特性和协同性。我们将这三个衡量标准称为多模态分布（或简称PID）的PID统计量，并引入了两个新的PID统计估计器，适用于高维分布。为了验证PID估计，我们在已知PID的合成数据集和大规模多模态基准测试集上进行了大量实验。

    The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
    
[^66]: 工程设计中的多模态机器学习：综述与未来方向

    Multi-modal Machine Learning in Engineering Design: A Review and Future Directions. (arXiv:2302.10909v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10909](http://arxiv.org/abs/2302.10909)

    本文综述了工程设计领域中多模态机器学习（MMML）的当前状态、进展和挑战，重点介绍了多模态信息表示、融合、对齐、转换和共学习的基本概念，以及与工程设计相关的前沿应用。我们提出了未来研究的潜在方向，包括构建广泛的数据集和基准测试环境，以促进算法性能的评估和比较。

    

    在快速发展的多模态机器学习（MMML）领域中，多个数据模态的融合有潜力重塑各种应用。本文综述了工程设计领域中MMML的当前状态、进展和挑战。综述首先深入探讨了MMML的五个基本概念：多模态信息表示、融合、对齐、转换和共学习。随后，我们探讨了MMML的前沿应用，特别关注与工程设计相关的任务，如跨模态综合、多模态预测和跨模态信息检索。通过这个综述，我们突出了在工程设计中采用MMML所面临的固有挑战，并提出了未来研究的潜在方向。为了推动MMML在工程设计中的持续演进，我们提倡集中努力构建广泛的数据集和基准测试环境，以促进算法性能的评估和比较。

    In the rapidly advancing field of multi-modal machine learning (MMML), the convergence of multiple data modalities has the potential to reshape various applications. This paper presents a comprehensive overview of the current state, advancements, and challenges of MMML within the sphere of engineering design. The review begins with a deep dive into five fundamental concepts of MMML:multi-modal information representation, fusion, alignment, translation, and co-learning. Following this, we explore the cutting-edge applications of MMML, placing a particular emphasis on tasks pertinent to engineering design, such as cross-modal synthesis, multi-modal prediction, and cross-modal information retrieval. Through this comprehensive overview, we highlight the inherent challenges in adopting MMML in engineering design, and proffer potential directions for future research. To spur on the continued evolution of MMML in engineering design, we advocate for concentrated efforts to construct extensive 
    
[^67]: 3M3D: 用于三维物体检测的多视图、多路径、多表示方法

    3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection. (arXiv:2302.08231v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08231](http://arxiv.org/abs/2302.08231)

    3M3D是一种用于三维物体检测的多视图、多路径、多表示方法，通过更新多视图特征和查询特征，同时增强全景视图和全局视图中的场景表示。

    

    基于多相机图像的三维视觉感知任务对于自动驾驶系统至关重要。最新的研究在这一领域通过利用多视图图像作为输入，并通过交叉注意多视图特征来迭代增强物体查询（物体候选）。然而，个体主干特征不会通过多视图特征进行更新，而仅仅保持作为单一图像主干网络输出的集合。因此，我们提出了3M3D：一种用于三维物体检测的多视图、多路径、多表示方法，在细粒度全景视图和粗粒度全局视图中同时更新多视图特征和查询特征，以增强场景的表示。首先，我们通过多视图轴自注意力机制更新多视图特征。它将全景信息融入多视图特征中，增强对全局场景的理解。其次，我们通过感兴趣区域（ROI）的自注意力机制来更新多视图特征。

    3D visual perception tasks based on multi-camera images are essential for autonomous driving systems. Latest work in this field performs 3D object detection by leveraging multi-view images as an input and iteratively enhancing object queries (object proposals) by cross-attending multi-view features. However, individual backbone features are not updated with multi-view features and it stays as a mere collection of the output of the single-image backbone network. Therefore we propose 3M3D: A Multi-view, Multi-path, Multi-representation for 3D Object Detection where we update both multi-view features and query features to enhance the representation of the scene in both fine panoramic view and coarse global view. Firstly, we update multi-view features by multi-view axis self-attention. It will incorporate panoramic information in the multi-view features and enhance understanding of the global scene. Secondly, we update multi-view features by self-attention of the ROI (Region of Interest) w
    
[^68]: 从非结构化的气候报告中回答气候问卷的方法

    Towards Answering Climate Questionnaires from Unstructured Climate Reports. (arXiv:2301.04253v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.04253](http://arxiv.org/abs/2301.04253)

    本研究提出了一种从非结构化的气候报告中回答气候问卷的方法。研究引入两个新的大规模气候问卷数据集，并使用现有结构训练自监督模型，通过实验和人类试验验证了模型的有效性。同时，还引入了一个气候文本分类数据集的基准，以促进气候领域的自然语言处理研究。

    

    尽管气候变化问题紧迫，但在自然语言处理领域对其的关注有限。行动者和政策制定者需要自然语言处理工具，能够有效地将庞大且快速增长的非结构化文本气候报告转化为结构化形式。为了应对这一挑战，我们引入了两个新的大规模气候问卷数据集，并利用其现有结构来训练自监督模型。我们进行实验表明，这些模型能够学习到对训练过程中未见的不同组织类型的气候披露进行泛化。然后，我们使用这些模型在人类试验中帮助将非结构化气候文档中的文本与半结构化问卷对齐。最后，为了支持气候领域进一步的自然语言处理研究，我们引入了一个现有气候文本分类数据集的基准，以更好地评估和比较现有模型。

    The topic of Climate Change (CC) has received limited attention in NLP despite its urgency. Activists and policymakers need NLP tools to effectively process the vast and rapidly growing unstructured textual climate reports into structured form. To tackle this challenge we introduce two new large-scale climate questionnaire datasets and use their existing structure to train self-supervised models. We conduct experiments to show that these models can learn to generalize to climate disclosures of different organizations types than seen during training. We then use these models to help align texts from unstructured climate documents to the semi-structured questionnaires in a human pilot study. Finally, to support further NLP research in the climate domain we introduce a benchmark of existing climate text classification datasets to better evaluate and compare existing models.
    
[^69]: 公平预测建模的一种一致范围逼近方法

    Consistent Range Approximation for Fair Predictive Modeling. (arXiv:2212.10839v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10839](http://arxiv.org/abs/2212.10839)

    本文提出了一个新颖的框架，用于验证基于有偏数据训练的预测模型的公平性。通过一致范围逼近的方法，在目标人群上构建了可证明公平的预测模型，并在真实数据上展示了明显的改进。

    

    本文提出了一个新颖的框架，用于验证基于有偏数据训练的预测模型的公平性。它借鉴了对不完整和不一致数据库的查询回答，以形式化公平查询在目标人群上的一致范围逼近（CRA）问题。该框架利用数据收集过程和有偏数据的背景知识，可以在有限的目标人群统计数据的情况下，计算公平查询的答案范围。通过CRA，该框架构建的预测模型可以在目标人群上获得可证明的公平性，而不受训练过程中外部数据的可用性限制。通过对真实数据的评估，验证了该框架的有效性，显示出明显优于现有最先进方法的改进。

    This paper proposes a novel framework for certifying the fairness of predictive models trained on biased data. It draws from query answering for incomplete and inconsistent databases to formulate the problem of consistent range approximation (CRA) of fairness queries for a predictive model on a target population. The framework employs background knowledge of the data collection process and biased data, working with or without limited statistics about the target population, to compute a range of answers for fairness queries. Using CRA, the framework builds predictive models that are certifiably fair on the target population, regardless of the availability of external data during training. The framework's efficacy is demonstrated through evaluations on real data, showing substantial improvement over existing state-of-the-art methods.
    
[^70]: 使用扩散模型从噪声图像中进行复数检索

    Complex-valued Retrievals From Noisy Images Using Diffusion Models. (arXiv:2212.03235v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03235](http://arxiv.org/abs/2212.03235)

    本研究推广了退火朗格朗日动力学（DDM类型）用于处理受泊松噪声影响的光学成像中的复数对象，提出了一种使用扩散模型从噪声图像中进行复数检索的算法。

    

    在多样的显微镜模式中，传感器仅测量实值强度。此外，传感器读数受泊松分布的光子噪声影响。传统的恢复算法通常旨在最小化原始图像和恢复图像之间的均方误差（MSE）。这往往导致模糊的结果和质量差的视觉效果。最近，深度扩散模型（DDM）已经被证明能够从所求变量的后验概率中采样图像，从而得到视觉上令人满意的高质量图像。这些模型主要用于受到高斯噪声影响的实值图像。在本研究中，我们将退火朗格朗日动力学（一种DDM类型）推广到解决受泊松噪声影响的光学成像中的复数对象（和实值图像）的基本挑战。我们将算法应用于各种光学场景，如傅立叶光学织构图、相位恢复和泊松去噪。我们的算法是

    In diverse microscopy modalities, sensors measure only real-valued intensities. Additionally, the sensor readouts are affected by Poissonian-distributed photon noise. Traditional restoration algorithms typically aim to minimize the mean squared error (MSE) between the original and recovered images. This often leads to blurry outcomes with poor perceptual quality. Recently, deep diffusion models (DDMs) have proven to be highly capable of sampling images from the a-posteriori probability of the sought variables, resulting in visually pleasing high-quality images. These models have mostly been suggested for real-valued images suffering from Gaussian noise. In this study, we generalize annealed Langevin Dynamics, a type of DDM, to tackle the fundamental challenges in optical imaging of complex-valued objects (and real images) affected by Poisson noise. We apply our algorithm to various optical scenarios, such as Fourier Ptychography, Phase Retrieval, and Poisson denoising. Our algorithm is
    
[^71]: 缓解分层注意力多尺度算子学习中的光谱偏差问题

    Mitigating spectral bias for the multiscale operator learning with hierarchical attention. (arXiv:2210.10890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10890](http://arxiv.org/abs/2210.10890)

    本文提出了一种分层注意力神经算子（HANO），用于解决多尺度偏微分方程学习中存在的光谱偏差问题，并通过数值实验证明其优于现有方法。

    

    神经算子已经成为学习偏微分方程（PDE）的无限维参数和解空间之间映射的强大工具。本文关注于具有重要应用的多尺度PDE，如油藏建模和湍流预测。我们证明对于这种PDE，对低频分量存在光谱偏差是现有神经算子的一大挑战。为了解决这个挑战，我们提出了一种受层次矩阵方法启发的分层注意力神经算子（HANO）。HANO具有自适应尺度交互范围和层次结构上的自注意力机制，能够实现可控线性成本的嵌套特征计算和多尺度解空间的编码/解码。我们还采用经验H^1损失函数来增强对高频分量的学习。我们的数值实验表明，HANO优于现有的最先进方法（SOTA）。

    Neural operators have emerged as a powerful tool for learning the mapping between infinite-dimensional parameter and solution spaces of partial differential equations (PDEs). In this work, we focus on multiscale PDEs that have important applications such as reservoir modeling and turbulence prediction. We demonstrate that for such PDEs, the spectral bias towards low-frequency components presents a significant challenge for existing neural operators. To address this challenge, we propose a hierarchical attention neural operator (HANO) inspired by the hierarchical matrix approach. HANO features a scale-adaptive interaction range and self-attentions over a hierarchy of levels, enabling nested feature computation with controllable linear cost and encoding/decoding of multiscale solution space. We also incorporate an empirical $H^1$ loss function to enhance the learning of high-frequency components. Our numerical experiments demonstrate that HANO outperforms state-of-the-art (SOTA) methods 
    
[^72]: 学习离散时间随机系统的可证明稳定神经控制器

    Learning Provably Stabilizing Neural Controllers for Discrete-Time Stochastic Systems. (arXiv:2210.05304v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05304](http://arxiv.org/abs/2210.05304)

    本文提出了一种学习离散时间随机系统中的神经控制器的方法，该方法能够以概率1在指定的稳定区域内实现系统稳定，并引入了稳定排序超级鞅(sRSMs)的概念来克服先前方法的局限性。实验结果证明了该方法的有效性。

    

    本文考虑在离散时间随机系统中学习控制策略的问题，该策略保证系统以概率1在某个指定的稳定区域内稳定。我们的方法基于我们在本文中引入的创新概念——稳定排序超级鞅(sRSMs)。我们的sRSMs克服了先前方法的局限性，这些方法仅适用于进入稳定区域后无法离开的系统。我们提出了一个学习过程，该过程学习一个控制策略和一个正式证明概率1稳定性的sRSM，两者都以神经网络的形式学习。我们还表明，该过程可以适应于在给定的Lipschitz连续控制策略下，验证随机系统以概率1在某个稳定区域内稳定。我们的实验评估表明，我们的学习过程能够成功地学习可证明稳定的神经控制器。

    We consider the problem of learning control policies in discrete-time stochastic systems which guarantee that the system stabilizes within some specified stabilization region with probability~$1$. Our approach is based on the novel notion of stabilizing ranking supermartingales (sRSMs) that we introduce in this work. Our sRSMs overcome the limitation of methods proposed in previous works whose applicability is restricted to systems in which the stabilizing region cannot be left once entered under any control policy. We present a learning procedure that learns a control policy together with an sRSM that formally certifies probability~$1$ stability, both learned as neural networks. We show that this procedure can also be adapted to formally verifying that, under a given Lipschitz continuous control policy, the stochastic system stabilizes within some stabilizing region with probability~$1$. Our experimental evaluation shows that our learning procedure can successfully learn provably stab
    
[^73]: 重新思考缺失数据：基于随机不确定性感知的推荐系统

    Rethinking Missing Data: Aleatoric Uncertainty-Aware Recommendation. (arXiv:2209.11679v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2209.11679](http://arxiv.org/abs/2209.11679)

    本研究提出了一种基于随机不确定性感知的推荐系统（AUR）框架，通过考虑缺失数据的固有随机性，解决了模型误差问题，提高了对长尾物品的召回效果。

    

    历史交互是推荐模型训练的默认选择，但通常呈现很高的稀疏性，即大部分用户-物品对是未观察到的缺失数据。标准的做法是将缺失数据视为负样本，并在观察到的交互中估计用户-物品对之间的交互概率。然而，这种训练方式不可避免地会导致一些潜在的交互被错误标记，损害模型的准确性，尤其是对于长尾物品的召回效果。在本文中，我们从随机不确定性的新视角研究了错标问题，这种不确定性描述了缺失数据的固有随机性。这种随机性促使我们超越仅仅考虑交互概率，而是采用随机不确定性建模。为此，我们提出了一种新的基于随机不确定性感知的推荐系统（AUR）框架，其中包括一个新的不确定性估计器和一个普通推荐模型。

    Historical interactions are the default choice for recommender model training, which typically exhibit high sparsity, i.e., most user-item pairs are unobserved missing data. A standard choice is treating the missing data as negative training samples and estimating interaction likelihood between user-item pairs along with the observed interactions. In this way, some potential interactions are inevitably mislabeled during training, which will hurt the model fidelity, hindering the model to recall the mislabeled items, especially the long-tail ones. In this work, we investigate the mislabeling issue from a new perspective of aleatoric uncertainty, which describes the inherent randomness of missing data. The randomness pushes us to go beyond merely the interaction likelihood and embrace aleatoric uncertainty modeling. Towards this end, we propose a new Aleatoric Uncertainty-aware Recommendation (AUR) framework that consists of a new uncertainty estimator along with a normal recommender mod
    
[^74]: SynthA1c:针对糖尿病风险分层的临床解释型患者表示方法

    SynthA1c: Towards Clinically Interpretable Patient Representations for Diabetes Risk Stratification. (arXiv:2209.10043v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10043](http://arxiv.org/abs/2209.10043)

    该论文提出了一种名为SynthA1c的方法，利用图像数据来预测2型糖尿病风险，避免了额外的血液实验室测量，其敏感性高达87.6%。

    

    及时诊断2型糖尿病(T2DM)对于启动及时的治疗干预和生活方式改变至关重要。随着临床诊所访问时间的缩短和医学影像数据的广泛应用，可以利用患者图像数据机会性地通过医生对T2DM进行额外诊断工作。我们研究了是否可以利用图像衍生的表型数据在表格学习分类器模型中预测T2DM风险，以自动地确定高风险患者，而不需要额外的血液实验室测试。与传统的二分类器相比，我们利用神经网络和决策树模型将患者数据表示为“SynthA1c”潜在变量，这些变量模拟了血红蛋白A1c的经验实验室测量结果，其敏感性高达87.6%。为了评估SynthA1c模型在其他患者群体中的泛化能力，我们引入了一种新颖的可推广度量。

    Early diagnosis of Type 2 Diabetes Mellitus (T2DM) is crucial to enable timely therapeutic interventions and lifestyle modifications. As the time available for clinical office visits shortens and medical imaging data become more widely available, patient image data could be used to opportunistically identify patients for additional T2DM diagnostic workup by physicians. We investigated whether image-derived phenotypic data could be leveraged in tabular learning classifier models to predict T2DM risk in an automated fashion to flag high-risk patients without the need for additional blood laboratory measurements. In contrast to traditional binary classifiers, we leverage neural networks and decision tree models to represent patient data as 'SynthA1c' latent variables, which mimic blood hemoglobin A1c empirical lab measurements, that achieve sensitivities as high as 87.6%. To evaluate how SynthA1c models may generalize to other patient populations, we introduce a novel generalizable metric
    
[^75]: 数据增强是一个超参数：精心筛选的自监督对于无监督异常检测的成功产生了幻象。

    Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07734](http://arxiv.org/abs/2208.07734)

    这项研究通过广泛的实验，证明数据增强与异常生成机制之间的对齐是自监督学习在无监督异常检测中取得成功的关键，并且在缺乏对齐时，自监督学习甚至可能降低准确性。

    

    自监督学习（SSL）已经成为一种有希望的替代方法，用于为现实世界的问题创建监督信号，避免了手动标注的巨大成本。对于标记异常稀缺或几乎不存在的无监督任务（如异常检测），SSL特别有吸引力。过去已经使用了大量的数据增强函数来进行基于SSL的异常检测（SSAD）的图像数据，并且最近的研究表明数据增强的类型对准确性有着重要影响。受此启发，本研究通过对三种不同检测模型和420个异常检测任务的广泛实验，提供了全面的数字和可视证据，证明数据增强与异常生成机制之间的对齐是SSAD成功的关键，而在缺乏对齐的情况下，SSL甚至可能降低准确性。据我们所知，这是关于图像型SSAD的首次深入研究。

    Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
    
[^76]: 从随机已知日志中恢复轨迹

    Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12672](http://arxiv.org/abs/2206.12672)

    本文提出了一种从随机已知日志中恢复轨迹的算法，在两个公开数据集上平均恢复准确度达到90-97%。这一方法通过计算流程模型和随机已知轨迹的合规性，并恢复在该随机轨迹中的最佳对齐作为真实轨迹。对比其他轨迹恢复选项，使用了产品多图来分析成本模型对恢复准确性的影响。这一算法对于预测模型开发、错误排查和系统性能改进具有重要意义。

    

    在这项工作中，我们提出了一种用于从随机已知日志中恢复轨迹的算法。随着传感器数量的增加和生成不确定数据的预测模型的增加，这种设置越来越常见。所提出的方法计算流程模型与随机已知轨迹之间的合规性，并在这个随机轨迹中恢复最佳对齐作为真实轨迹。论文对不同成本模型对轨迹恢复准确性的影响进行了分析，并利用产品多图来比较替代轨迹恢复选项。通过使用两个公开可用的数据集进行评估，我们的方法的平均准确性令人印象深刻，平均恢复准确度达到90-97%，显著改善了常见的启发式算法，该算法选择每个不确定活动的最可能值。我们相信，所提出的算法在从随机已知日志中恢复正确轨迹方面的有效性可能是开发预测模型、错误排查和改进系统性能的有力帮助。

    In this work we propose an algorithm for trace recovery from stochastically known logs, a setting that is becoming more common with the increasing number of sensors and predictive models that generate uncertain data. The suggested approach calculates the conformance between a process model and a stochastically known trace and recovers the best alignment within this stochastic trace as the true trace. The paper offers an analysis of the impact of various cost models on trace recovery accuracy and makes use of a product multi-graph to compare alternative trace recovery options. The average accuracy of our approach, evaluated using two publicly available datasets, is impressive, with an average recovery accuracy score of 90-97%, significantly improving a common heuristic that chooses the most likely value for each uncertain activity. We believe that the effectiveness of the proposed algorithm in recovering correct traces from stochastically known logs may be a powerful aid for developing 
    
[^77]: Yankee Swap：用于拟阵等级估值的快速公平分配机制

    Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid Rank Valuations. (arXiv:2206.08495v5 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2206.08495](http://arxiv.org/abs/2206.08495)

    本论文提出了一个基于Yankee Swap过程的简单算法，用于拟阵等级估值的无分割物品公平分配，这种方法易于理解，快速可扩展。

    

    当参与者具有拟阵等级估值时，我们研究无分割物品的公平分配。我们的主要贡献是基于俚语Yankee Swap过程的简单算法，它计算出可证明公平和高效的Lorenz支配分配。虽然有多项式时间算法可用于计算此类分配，但我们提出的方法有两种改进方式。 （a）我们的方法易于理解，不使用复杂的拟阵优化算法作为子例程。 （b）我们的方法是可扩展的； 它被证明比所有已知的计算Lorenz支配分配的算法更快。这两个属性对于任何真正公平分配环境中算法的采用至关重要；我们的贡献使我们离这个目标更近了一步。

    We study fair allocation of indivisible goods when agents have matroid rank valuations. Our main contribution is a simple algorithm based on the colloquial Yankee Swap procedure that computes provably fair and efficient Lorenz dominating allocations. While there exist polynomial time algorithms to compute such allocations, our proposed method improves on them in two ways. (a) Our approach is easy to understand and does not use complex matroid optimization algorithms as subroutines. (b) Our approach is scalable; it is provably faster than all known algorithms to compute Lorenz dominating allocations. These two properties are key to the adoption of algorithms in any real fair allocation setting; our contribution brings us one step closer to this goal.
    

