# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TeCH: Text-guided Reconstruction of Lifelike Clothed Humans.](http://arxiv.org/abs/2308.08545) | 该论文提出了TeCH模型，通过文本引导的方法来重建逼真的服饰人物。模型可以准确恢复“未曾看见的区域”并添加高级细节，采用了基于DMTet的混合3D表示以达到更低的成本。 |
| [^2] | [Can Transformers Learn Optimal Filtering for Unknown Systems?.](http://arxiv.org/abs/2308.08536) | 本文研究了使用transformers进行最优输出估计问题，通过训练一个transformer来在未知系统上进行预测，并命名为元输出预测器（MOP）。我们观察到，尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当，在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。 |
| [^3] | [Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction.](http://arxiv.org/abs/2308.08518) | 本文提出了一个具有点对注意力感知机制的双向对应预测网络，通过利用模型点和场景点之间的相关性进行点对匹配学习，解决了传统方法对观察质量和遮挡的依赖性问题，并在实验证明其在物体姿态估计任务上优于其他最先进的方法。 |
| [^4] | [A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction.](http://arxiv.org/abs/2308.08502) | 该论文提出了一个基于元学习的堆叠回归方法，用于客户终身价值预测，旨在解决现有模型在处理各种输入特征时的限制，并避免深度学习方法的复杂性。 |
| [^5] | [InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models.](http://arxiv.org/abs/2308.08500) | 本文提出了InTune，一个基于强化学习的数据流优化方法，应用于深度推荐模型。通过研究在Netflix计算集群中的DLRM数据处理流程，我们发现目前的流程优化器存在性能不佳、频繁崩溃或需要不切实际的集群重组等问题。 |
| [^6] | [Context-Aware Service Recommendation System for the Social Internet of Things.](http://arxiv.org/abs/2308.08499) | 该研究提出了一个面向社交物联网的上下文感知服务推荐系统，通过捕捉设备之间的潜在特征交互和建模设备-服务对的高阶特征交互，实现了准确的评级预测和个性化的服务推荐。 |
| [^7] | [HyperBandit: Contextual Bandit with Hypernewtork for Time-Varying User Preferences in Streaming Recommendation.](http://arxiv.org/abs/2308.08497) | HyperBandit是一种基于超网络的上下文强化学习方法，用于处理流媒体推荐系统中时间变化的用户偏好。它通过建立时间特征和用户偏好之间的关联，动态调整推荐模型以适应动态场景。 |
| [^8] | [Understanding User Intent Modeling for Conversational Recommender Systems: A Systematic Literature Review.](http://arxiv.org/abs/2308.08496) | 该论文进行了一项系统文献综述，研究了对话式推荐系统中用户意图建模的相关模型和特征。研究结果为研究人员提供了模型选择、质量问题和评估指标等方面的洞察。 |
| [^9] | [Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder.](http://arxiv.org/abs/2308.08488) | 本文提出了通过基于嘴唇-音素字级相关性的视觉预训练和跨模态融合编码器来改进视听语音识别的两种新技术。这些技术可以在预训练和微调阶段准确对齐音频和视频流，并且充分利用模态互补性。 |
| [^10] | [Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features.](http://arxiv.org/abs/2308.08482) | 本研究提出了“快捷去偏”的方法，通过将目标任务对偏见属性的学习从偏见特征转移到快捷特征上，并采用因果干预来消除偏见特征的影响，以解决机器学习模型依赖敏感社会属性进行预测的公平性问题。 |
| [^11] | [Stationary Algorithmic Balancing For Dynamic Email Re-Ranking Problem.](http://arxiv.org/abs/2308.08460) | 提出了一种名为MOSR的算法，用于解决动态邮件重新排序问题。该算法使用自适应控制模型来平衡亲密度，及时性和简洁性等标准，并能够适应偏好的变化。在恩隆邮件数据集上的实验结果表明，MOSR在非稳态偏好下表现出更好的性能，并且在具有高方差的邮件特征的小型抽样数据集上也能保持稳定的排名。 |
| [^12] | [Knowledge Prompt-tuning for Sequential Recommendation.](http://arxiv.org/abs/2308.08459) | 该论文提出了知识提示调优的顺序推荐(KP4SR)方法，通过引入外部知识库和构建知识提示，解决了顺序推荐中的语义差距和信息损失问题，从而提高了推荐性能。 |
| [^13] | [Tightest Admissible Shortest Path.](http://arxiv.org/abs/2308.08453) | 该论文提出了一种针对加权有向图的最严格可接受的最短路径问题，利用边权不确定性进行计算成本交换，并提供了一个完整的算法来解决此问题，并保证解的质量。 |
| [^14] | [AIGC In China: Current Developments And Future Outlook.](http://arxiv.org/abs/2308.08451) | 本文分析了中国在人工智能生成内容（AIGC）领域的现状，讨论了AIGC的基础技术、市场状况和发展轨迹，并重点强调了AIGC的生态建设。预测了行业未来的挑战和发展方向。 |
| [^15] | [Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance.](http://arxiv.org/abs/2308.08448) | 这项研究讨论了在金融领域中应用量子机器学习的新研究方向，通过比较qGAN和QCBM等模型，展示了在金融领域中实现量子优势的潜力。 |
| [^16] | [Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities.](http://arxiv.org/abs/2308.08407) | 这篇综述论文讨论了可解释的人工智能在临床风险预测中的应用，包括概念、方法和方式。可解释性对于确保人们对AI系统的信任和可靠性至关重要，除了解释性之外，还涉及公平性、偏见、信任和透明度等方面。该综述还讨论了近期在临床风险预测中可解释模型的进展。 |
| [^17] | [PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing.](http://arxiv.org/abs/2308.08371) | 该论文提出了一个框架，用于合成包含过程数据和对应程序化知识的数据集，并比较了不同嵌入方法的性能。 |
| [^18] | [Agglomerative Transformer for Human-Object Interaction Detection.](http://arxiv.org/abs/2308.08370) | 我们提出了一种凝聚Transformer（AGER），该方法在人-物交互检测中以单阶段和端到端的方式灵活利用额外的实例级提示。这种方法通过动态聚类补丁标记并将其与文本对齐，从而显著提高了实例级提示的提取效果，并在HICO-Det数据集上取得了最新的36.75 mAP性能。 |
| [^19] | [Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?.](http://arxiv.org/abs/2308.08354) | 本文研究表明，针对推荐系统中的冷启动问题，元学习技术在处理深度学习模型时已成为最受欢迎的方法。然而，当前的元学习方法在实际推荐系统中并不实用，因为这些系统拥有庞大的用户和物品数量，且有严格的延迟要求。 |
| [^20] | [Graph Out-of-Distribution Generalization with Controllable Data Augmentation.](http://arxiv.org/abs/2308.08344) | 本论文提出了一种名为“OOD-GMixup”的方法，利用可控数据增强来解决图的带外分布泛化问题。该方法通过提取图合理性和生成虚拟样本的方式来消除虚假相关性和稳定性问题。 |
| [^21] | [Learning Logic Programs by Discovering Higher-Order Abstractions.](http://arxiv.org/abs/2308.08334) | 本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。 |
| [^22] | [A Framework for Data-Driven Explainability in Mathematical Optimization.](http://arxiv.org/abs/2308.08309) | 该论文介绍了一种在数学优化中引入数据驱动可解释性的框架，通过与过去类似情况下的解进行比较来找到具有相似特征的解决方案。研究表明，尽管可解释模型在某些情况下是NP-hard的，但在一些多项式可解的情况下可行。 |
| [^23] | [Integrating cognitive map learning and active inference for planning in ambiguous environments.](http://arxiv.org/abs/2308.08307) | 本研究在模糊环境中提出了一种融合认知地图学习和主动推理的规划方法，实验结果表明主动推理代理在处理具有模糊信息的挑战性场景的规划中更为有效。 |
| [^24] | [Robust Bayesian Satisficing.](http://arxiv.org/abs/2308.08291) | 本文提出了一种名为RoBOS的鲁棒贝叶斯满足算法，用于解决在上下文贝叶斯优化中存在分布偏移时的问题。该算法能够在一定的分布偏移量下保证亏得不严重的子线性遗憾，并且具有与分布偏移量无关的较弱遗憾边界。 |
| [^25] | [It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models.](http://arxiv.org/abs/2308.08268) | 生成变换模型在OOD泛化方面存在神秘的性能下降。研究人员观察到模型在训练和泛化数字运算时的行为之间的不一致，并尝试提出解决方案，但仍未解决本质机制。 |
| [^26] | [Description Logics Go Second-Order -- Extending EL with Universally Quantified Concepts.](http://arxiv.org/abs/2308.08252) | 这篇论文通过引入全称量化概念扩展了描述逻辑$\mathcal{EL}$，分别提出了模式语义和二阶语义，研究了它们的性质并证明了它们在有用片段中的结论相同。 |
| [^27] | [TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series.](http://arxiv.org/abs/2308.08241) | 这篇论文总结了两种使用语言模型完成时间序列任务的策略，通过设计一种适用于语言模型的时间序列嵌入方法来激活语言模型对时间序列数据的能力。虽然结果没有明显超越当前最先进的模型，但可以更好地处理时间序列数据。 |
| [^28] | [Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey.](http://arxiv.org/abs/2308.08234) | 本论文调研了在自然语言处理中使用基于Transformer的多任务学习的挑战和机会。通过对NLP中基于Transformer的MTL方法以及典型机器学习生命周期各阶段的挑战进行讨论，提供了相关领域的概述和动向。 |
| [^29] | [Explainable Multi-View Deep Networks Methodology for Experimental Physics.](http://arxiv.org/abs/2308.08206) | 该论文介绍了一个可解释的多视角深度网络方法论，应用于实验物理中的多种成像表达分析。该方法论解决了多视角模型可解释性不足的问题。 |
| [^30] | [Towards Ontology-Mediated Planning with OWL DL Ontologies (Extended Version).](http://arxiv.org/abs/2308.08200) | 该论文提出了一种基于OWL DL本体论的规划方法，通过将规划规范和本体分开，并使用接口将其连接在一起，实现了规划专家与本体专家的紧密合作。这种方法优化了相对较小领域下的本体问题，并支持整个OWL DL片段。 |
| [^31] | [Modelling the Spread of COVID-19 in Indoor Spaces using Automated Probabilistic Planning.](http://arxiv.org/abs/2308.08190) | 本论文使用自动概率规划和动态图分析建模COVID-19在室内空间的传播，通过非药物干预控制疾病的传播，并比较不同干预策略的效果。 |
| [^32] | [Endogenous Macrodynamics in Algorithmic Recourse.](http://arxiv.org/abs/2308.08187) | 这项研究填补了算法补救中的内生动力学和对策影响其他个体的研究空白，提出了一个广义框架，并揭示了对策的隐藏成本。通过模拟实验，验证了该方法的效果。 |
| [^33] | [Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System.](http://arxiv.org/abs/2308.08169) | 本文介绍了一种通过简单的缓存提高端到端任务导向型对话系统的性能的方法。通过微调检索模块并训练端到端的对话系统模型，系统可以动态更新并处理已知和未知的对话场景。实验证明我们的方法相对于强基线方法在非空联合目标准确率上提高了6.7%。 |
| [^34] | [Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations.](http://arxiv.org/abs/2308.08162) | 本论文提出了一个用于评估原型部件解释的空间不一致性的解释性基准，并介绍了一种补偿不一致性的方法。通过大量的实证研究，表明了基准的表达能力和补偿方法的有效性。 |
| [^35] | [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.](http://arxiv.org/abs/2308.08155) | AutoGen是一种新的框架，通过多个可以互相对话的代理，实现了下一代LLM应用。它利用人类的理解和智能，优雅地处理不完美的生成和推理能力，并通过自动化代理对话简化了复杂的工作流程。 |
| [^36] | [SYENet: A Simple Yet Effective Network for Multiple Low-Level Vision Tasks with Real-time Performance on Mobile Device.](http://arxiv.org/abs/2308.08137) | SYENet是一种简单而有效的网络，用于在移动设备上进行多个低级视觉任务的实时处理。该网络具有较少的参数量，并通过特定的构建模块和二次连接单元实现了不对称分支结果的有效连接。通过引入异常值感知损失来提高性能，并在实时应用中表现出优秀的PSNR性能。 |
| [^37] | [Ranking-aware Uncertainty for Text-guided Image Retrieval.](http://arxiv.org/abs/2308.08131) | 本研究提出了一种用于文本引导的图像检索的排名感知不确定性方法， |
| [^38] | [How to Mask in Error Correction Code Transformer: Systematic and Double Masking.](http://arxiv.org/abs/2308.08128) | 该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。 |
| [^39] | [OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution.](http://arxiv.org/abs/2308.08114) | 本文提出了OmniZoomer，一种通过深度学习将Möbius变换整合到网络中，用于在全向图像上进行移动和缩放的方法。通过学习不同条件下的变换特征图，网络能够处理增加的边缘曲率并减轻模糊效果。此外，为了解决混叠问题，作者还提出了两个关键组成部分。 |
| [^40] | [ChatLogo: A Large Language Model-Driven Hybrid Natural-Programming Language Interface for Agent-based Modeling and Programming.](http://arxiv.org/abs/2308.08102) | ChatLogo是一个以大型语言模型为驱动的代理建模和编程的混合自然编程语言界面，旨在支持自然语言和编程语言的混合对话，为初学者提供更友好的界面，并避免技术系统过度依赖任何单一的语言模型。 |
| [^41] | [S-Mixup: Structural Mixup for Graph Neural Networks.](http://arxiv.org/abs/2308.08097) | S-Mixup是一种在节点分类任务中使用结构信息的新型Mixup增强方法，通过图神经网络的预测置信度和边梯度来构建Mixup池，并在真实世界基准数据集上进行了验证。 |
| [^42] | [Decentralized Graph Neural Network for Privacy-Preserving Recommendation.](http://arxiv.org/abs/2308.08072) | 本文提出了一种去中心化图神经网络（DGREC）框架，用于隐私保护推荐，其中用户可以选择公开他们的交互。该框架通过图构建、局部梯度计算和全局梯度传递三个阶段实现，同时引入了名为安全梯度共享的差分隐私机制，保护用户的私密数据。 |
| [^43] | [Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks.](http://arxiv.org/abs/2308.08071) | 本论文提出了一种通过动态图神经网络解决延迟反馈问题的方法，该方法在数据新鲜度和标签准确性之间取得了平衡。 |
| [^44] | [Simple online learning with consistency oracle.](http://arxiv.org/abs/2308.08055) | 该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。 |
| [^45] | [Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem.](http://arxiv.org/abs/2308.08051) | 该论文介绍了对抗领域适应的方法来解决银行贷款问题中训练集偏差问题，旨在学习无偏但信息丰富的过去数据表示。 |
| [^46] | [DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue.](http://arxiv.org/abs/2308.08043) | DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。 |
| [^47] | [Automated Test Case Generation Using Code Models and Domain Adaptation.](http://arxiv.org/abs/2308.08033) | 本研究提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。 |
| [^48] | [Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning.](http://arxiv.org/abs/2308.08029) | 本论文提出了一种新颖的算法，称为SI SL，用于主动学习和模型优化过程中的规划。该算法通过与贝叶斯强化学习方案的比较证明了其性能的优越性。 |
| [^49] | [Potential Energy Advantage of Quantum Economy.](http://arxiv.org/abs/2308.08025) | 量子计算在能源效率方面具有优势，并且能够在盈利和能源效率上超越经典计算。这使得量子计算成为计算行业更可持续的选择。 |
| [^50] | [GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity.](http://arxiv.org/abs/2308.08010) | GRINN是一种物理信息导向的神经网络，用于在自重存在的情况下求解三维流体动力学系统。它在模拟重力不稳定性和波传播方面取得了良好的结果。 |
| [^51] | [APACE: AlphaFold2 and advanced computing as a service for accelerated discovery in biophysics.](http://arxiv.org/abs/2308.07954) | APACE是一个将AlphaFold2和先进计算作为服务的计算框架，用于在现代超级计算环境中加速蛋白质结构预测分析。研究者在Delta超级计算机中部署了APACE，在准确蛋白质结构预测方面取得了显著性能提升。 |
| [^52] | [Leveraging Symmetries in Pick and Place.](http://arxiv.org/abs/2308.07948) | 该论文研究了物品搬运任务中的对称性，提出了一种称为等变Transporter Net的模型，利用等变神经模型捕捉了所有对称性，能够在不同的搬运位置推广搬运知识。 |
| [^53] | [Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis.](http://arxiv.org/abs/2308.07942) | 基于图神经网络和规则的归纳知识图谱补全研究了基于规则的方法在实践中的表现不佳的原因，发现不合理的实体没有排名和只考虑最具信息量的路径是影响因素。提出了一些解决这些问题的规则方法的变体，发现其性能接近于基于图神经网络的方法NBFNet。这些变体仅使用了NBFNet所依赖的证据的一小部分。 |
| [^54] | [Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data.](http://arxiv.org/abs/2308.07940) | 该论文尝试通过使用GPT-2模型从头开始训练编码的时空数据，生成受环境因素和个体属性影响的个体轨迹。 |
| [^55] | [Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting.](http://arxiv.org/abs/2308.07939) | Ada-QPacknet是一种自适应剪枝与位宽缩减的高效继续学习方法，通过剪枝和量化技术生成任务子网络，在动态和复杂环境中实现了与浮点数子网络相似的准确性。 |
| [^56] | [Transforming Sentiment Analysis in the Financial Domain with ChatGPT.](http://arxiv.org/abs/2308.07935) | 本研究使用ChatGPT 3.5来进行金融情绪分析，特别关注外汇市场，通过零-shot提示方法，在精心策划的数据集上评估了其性能，并发现与传统模型相比，ChatGPT在金融情绪分析中表现出约35％的性能提升。 |
| [^57] | [One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training.](http://arxiv.org/abs/2308.07934) | 本论文介绍了一种位翻转攻击和模型训练相结合的方法，通过在训练阶段引入对手构建高风险模型，在只进行少量位翻转的情况下，将正常模型转化为恶意模型。实验结果表明，这种攻击方法可以逃避各种检测方法。 |
| [^58] | [Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation.](http://arxiv.org/abs/2308.07931) | 本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。 |
| [^59] | [Predictive Modeling of Menstrual Cycle Length: A Time Series Forecasting Approach.](http://arxiv.org/abs/2308.07927) | 本研究通过使用机器学习技术，探索了预测月经周期的方法，结果表明可以准确预测月经周期的开始和持续时间。 |
| [^60] | [DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models.](http://arxiv.org/abs/2308.07687) | 本论文提出了一种名为DiffGuard的方法，使用预训练的扩散模型进行语义不匹配引导的带外分布检测。实验证明，DiffGuard在小规模数据集上表现出色，但在ImageNet规模的数据集上无法应用。 |
| [^61] | [Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks.](http://arxiv.org/abs/2308.07439) | 本研究提出了一种交互感知的个性化车辆轨迹预测方法，利用时间图神经网络来建模目标车辆与周围交通之间的时空交互，并通过迁移学习来个性化预测。实验结果表明，该方法能够更准确地预测车辆的轨迹。 |
| [^62] | [Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation.](http://arxiv.org/abs/2308.06422) | 本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。 |
| [^63] | [Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning.](http://arxiv.org/abs/2308.05379) | 这篇论文提出了一种行为增强的相关模型，利用自我监督学习，通过从用户历史行为数据中提取辅助查询-项目交互，来改进搜索引擎中的查询-项目匹配，提高准确性和鲁棒性。 |
| [^64] | [Metacognitive Prompting Improves Understanding in Large Language Models.](http://arxiv.org/abs/2308.05342) | 元认知提示 (MP) 是一种改进大型语言模型 (LLMs) 理解能力的策略。实验结果表明，使用MP的PaLM在各种自然语言理解任务中接近于GPT-4的性能水平。 |
| [^65] | [SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning.](http://arxiv.org/abs/2308.04673) | 本文提出了SSL-Auth，这是一种用于自监督学习中预训练的编码器的易碎水印身份验证框架。该方法利用选择的关键样本作为水印信息，并训练一个验证网络来重构水印信息，从而进行验证。 |
| [^66] | [Physics-Based Task Generation Through Causal Sequence of Physical Interactions.](http://arxiv.org/abs/2308.02835) | 本文提出了一种基于物理交互因果序列的任务生成方法，使得人工智能系统能够在模拟物理环境中执行任务并评估其物理推理能力。我们使用“愤怒的小鸟”游戏作为示例，通过一系列指标对生成的任务进行评估。 |
| [^67] | [Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals.](http://arxiv.org/abs/2308.02510) | 本文通过重建观察到的图像来揭示人脑对视觉刺激的知觉机制，并提出了一种名为NeuroImagen的综合管道，利用脑电图信号重建视觉刺激图像。 |
| [^68] | [PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems.](http://arxiv.org/abs/2308.00864) | 本论文提出了一种基于个性化剩余策略的合作咨询系统PeRP，用于缓解拥堵。该系统通过结构化建模人类驾驶的相似性，并根据驾驶员的特征为其提供行动建议，以减少交通拥堵。 |
| [^69] | [Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes.](http://arxiv.org/abs/2307.16706) | 本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。 |
| [^70] | [EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction.](http://arxiv.org/abs/2307.16082) | 本文提出了一个利用词汇、语义和上下文表示的框架，旨在解决现有事件检测方法在识别新兴社交事件方面的局限性，并提供了对社交数据进行丰富的上下文化处理的方法。 |
| [^71] | [LLM-Rec: Personalized Recommendation via Prompting Large Language Models.](http://arxiv.org/abs/2307.15780) | 本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。 |
| [^72] | [Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities.](http://arxiv.org/abs/2307.13565) | 决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。 |
| [^73] | [Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education.](http://arxiv.org/abs/2307.12267) | 本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。 |
| [^74] | [LLM Cognitive Judgements Differ From Human.](http://arxiv.org/abs/2307.11787) | 这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。 |
| [^75] | [Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning.](http://arxiv.org/abs/2306.16296) | 本文提出了一种基于类比的方法，通过选择和修剪相关实体，从而引导知识图谱的构建。实证结果显示，这种方法在两个领域和异质种子实体的数据集上优于其他机器学习方法，并具有较低的参数数量。这些结果支持在相关任务中进一步应用类比推理。 |
| [^76] | [Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions.](http://arxiv.org/abs/2306.10813) | 本文提出了Instruct-NeuralTalker，一种利用指令编辑音频驱动的对话辐射场的交互式框架。该方法可以实现实时个性化的对话面部生成，并在编辑过程中保持音频唇同步。此外，还引入了轻量级的细化网络来实现可控的细节生成，并且在消费级硬件上可以达到最高30FPS的实时渲染。 |
| [^77] | [Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork.](http://arxiv.org/abs/2306.10698) | 人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。 |
| [^78] | [Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network.](http://arxiv.org/abs/2306.01631) | 本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。 |
| [^79] | [LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization.](http://arxiv.org/abs/2306.01102) | 本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。 |
| [^80] | [Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics.](http://arxiv.org/abs/2305.18477) | 本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。 |
| [^81] | [Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization.](http://arxiv.org/abs/2305.11095) | 本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。 |
| [^82] | [Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber.](http://arxiv.org/abs/2305.04043) | Echoes提出了一种无监督的去偏方法，生成偏差对立样本的伪偏差标签，实现了对数据集中偏差特征的一致性处理，并取得了各项任务和数据集上的最先进性能。 |
| [^83] | [A Scalable Test Problem Generator for Sequential Transfer Optimization.](http://arxiv.org/abs/2304.08503) | STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。 |
| [^84] | [Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP.](http://arxiv.org/abs/2303.16166) | 在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。 |
| [^85] | [Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving.](http://arxiv.org/abs/2303.11888) | 本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。 |
| [^86] | [Limited Query Graph Connectivity Test.](http://arxiv.org/abs/2302.13036) | 我们提出了一个有限查询图连通性测试的组合优化模型，目标是用最小化查询次数确定图的s-t连通性，主要用于网络安全用例中确定攻击路径的存在与否。 |
| [^87] | [Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification.](http://arxiv.org/abs/2301.11562) | 在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。 |
| [^88] | [Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++.](http://arxiv.org/abs/2301.11118) | Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。 |
| [^89] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^90] | [Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale.](http://arxiv.org/abs/2212.09095) | 本文通过使用一个660亿参数的语言模型，在多个任务中发现了上下文学习能力并不均匀分布在其各个组件上。通过移除约70%的注意力头和约20%的前馈网络，任务执行表现仅有轻微下降。此外，在OPT-66B中，存在一小部分注意力头对于上下文学习中的基础归纳操作具有高效能力。 |
| [^91] | [3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation.](http://arxiv.org/abs/2212.01103) | 本论文提出了一个名为3D-TOGO的模型，旨在实现跨类别的文本引导的三维物体生成。模型包括文本到视图生成模块和视图到三维生成模块，使用先验引导、标题引导和视图对比学习等方法提高视图一致性和标题相似性。采用pixelNeRF模型进行视图到三维生成。 |
| [^92] | [Disentangled Representation Learning.](http://arxiv.org/abs/2211.11695) | 解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。 |
| [^93] | [Normalizing Flows for Human Pose Anomaly Detection.](http://arxiv.org/abs/2211.10946) | 该论文提出了一种用于人体姿势异常检测的标准化流模型，通过将问题简化为姿势异常检测减少了干扰参数的影响，同时具有减少偏见的优势。该模型基于高度紧凑的姿势表示，在解决时空姿势数据的特殊特征上表现出优势，并且可以处理准则设置和非准则设置下的训练数据。实验结果表明该模型达到了最先进的水平。 |
| [^94] | [EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones.](http://arxiv.org/abs/2211.09703) | 本文提出了一种泛化课程学习方法，用于高效训练视觉主干网络，通过优先让模型学习“更容易学习”的模式，不断引入更难的模式，从而加速训练过程。 |
| [^95] | [Syntax-Guided Domain Adaptation for Aspect-based Sentiment Analysis.](http://arxiv.org/abs/2211.05457) | 本文提出了一种名为SDAM的新颖基于语法指导的领域适应模型，用于更有效地进行跨领域基于方面的情感分析。SDAM利用句法结构相似性构建伪训练实例，提升模型性能。 |
| [^96] | [DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph.](http://arxiv.org/abs/2210.10592) | 本论文提出了一种用于离散时动态图的分离表示学习框架DyTed，通过设计时间片对比学习任务和结构对比学习任务，有效地识别时间不变和时间变化的表示，并提出分离感知判别器以增强分离性能。 |
| [^97] | [ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system.](http://arxiv.org/abs/2210.09043) | 本文提出了一种名为ST-former的新型transformer架构，用于COVID-19期间的城市轨道交通客流预测。通过引入改进的自注意机制和自适应多图卷积网络，可以准确建模客流的时空依赖关系和复杂空间依赖关系。 |
| [^98] | [Stochastic Constrained DRO with a Complexity Independent of Sample Size.](http://arxiv.org/abs/2210.05740) | 本文提出了一种适用于非凸和凸损失函数的随机算法，用于解决Kullback Leibler散度约束的分布鲁棒优化问题，并且具有与样本大小无关的复杂度，每次迭代只需要恒定的批次大小。实证研究证明了该算法在解决非凸和凸约束DRO问题中的有效性。 |
| [^99] | [Explanations as Programs in Probabilistic Logic Programming.](http://arxiv.org/abs/2210.03021) | 本文提出了一种新的方法，将解释表示为从给定查询生成的程序，通过多次展开类变换来生成程序，从而显式展示出证明给定查询的推理链。 |
| [^100] | [Space-based gravitational wave signal detection and extraction with deep neural network.](http://arxiv.org/abs/2207.07414) | 本研究开发了一种高精度的使用深度神经网络的空间引力波信号检测和提取方法，能够在高斯噪声中识别各种源的信号，达到了超过99%的检测率和至少95%的相似性。该方法具有较低的虚警率和强大的泛化行为。 |
| [^101] | [Certified Symmetry and Dominance Breaking for Combinatorial Optimisation.](http://arxiv.org/abs/2203.12275) | 该论文介绍了一种用于认证对称性和优先级突破的方法，可以高效地验证布尔可满足性（SAT）求解中的一般对称性突破，还可以应用于最大团求解和约束编程等组合问题。 |
| [^102] | [SMGRL: Scalable Multi-resolution Graph Representation Learning.](http://arxiv.org/abs/2201.12670) | 本论文提出了一个可扩展的多分辨率图表示学习框架（SMGRL），通过降低训练成本和利用自相似性在多个分辨率上应用算法，能够高效地学习多分辨率节点嵌入。 |
| [^103] | [Analyzing the Limits of Self-Supervision in Handling Bias in Language.](http://arxiv.org/abs/2112.08637) | 本文分析了自监督在处理语言偏见中的局限性，并定义了四个偏见任务（诊断、识别、提取和改写），通过使用不同类别的任务描述来评估语言模型对语义的捕捉能力。 |
| [^104] | [Large-scale Autonomous Flight with Real-time Semantic SLAM under Dense Forest Canopy.](http://arxiv.org/abs/2109.06479) | 本文提出了一个集成系统，可以在密集森林林冠下进行大规模自主飞行和实时语义地图构建。系统使用LiDAR数据检测和建模树干和地面平面，并利用多级规划和地图构建框架计算动态可行的轨迹，以构建用户定义感兴趣区域的语义地图，并通过语义SLAM来最小化里程计漂移。 |

# 详细

[^1]: TeCH: 文本引导的逼真服饰人物重建

    TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. (arXiv:2308.08545v1 [cs.CV])

    [http://arxiv.org/abs/2308.08545](http://arxiv.org/abs/2308.08545)

    该论文提出了TeCH模型，通过文本引导的方法来重建逼真的服饰人物。模型可以准确恢复“未曾看见的区域”并添加高级细节，采用了基于DMTet的混合3D表示以达到更低的成本。

    

    尽管最近在从单张图像中重建着装人物方面取得了研究进展，但准确恢复“未曾看见的区域”并添加高级细节仍然是一个未解决的挑战，缺乏关注。现有的方法往往会生成过于平滑的背面表面和模糊的纹理。本文提出了TeCH模型，通过利用1）通过服装解析模型和视觉问答（VQA）自动生成的描述性文本提示（例如，服饰、颜色、发型）；2）经过个性化微调的文本到图像扩散模型（T2I）来学习“无法描述”的外观，从而对3D人类形象进行重建。为了以更低的成本表示高分辨率的3D服饰人物，我们提出了基于DMTet的混合3D表示，它由明确的身体形状网格和一个……（摘要较长，省略部分内容）

    Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the "unseen regions" with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the "indescribable" appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an
    
[^2]: Transformers能否学习用于未知系统的最优滤波？

    Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])

    [http://arxiv.org/abs/2308.08536](http://arxiv.org/abs/2308.08536)

    本文研究了使用transformers进行最优输出估计问题，通过训练一个transformer来在未知系统上进行预测，并命名为元输出预测器（MOP）。我们观察到，尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当，在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。

    

    Transformers在自然语言处理中取得了显著的成功，然而它们在动态系统中的潜力仍然大部分未被探索。本文研究了使用transformers进行最优输出估计问题，它使用过去的所有输出来生成预测。我们使用来自先验分布的各种系统来训练transformer，然后在先前未见过的相同分布的系统上评估其性能。结果表明，获得的transformer就像一个预测算法，它可以在上下文中学习并快速适应和预测不同的系统，因此我们称之为元输出预测器（MOP）。尽管MOP没有访问模型的权限，但在大多数线性动态系统中，它的性能与基于卡尔曼滤波器的最优输出估计器相当。通过大量的数值实验，我们观察到MOP在具有非独立同分布噪声和时变动态的挑战性场景中也表现优秀。

    Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
    
[^3]: 基于双向预测的6D物体姿态估计中的点对注意力的利用

    Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v1 [cs.CV])

    [http://arxiv.org/abs/2308.08518](http://arxiv.org/abs/2308.08518)

    本文提出了一个具有点对注意力感知机制的双向对应预测网络，通过利用模型点和场景点之间的相关性进行点对匹配学习，解决了传统方法对观察质量和遮挡的依赖性问题，并在实验证明其在物体姿态估计任务上优于其他最先进的方法。

    

    传统的几何注册估计方法仅隐式地利用CAD模型，这导致它们对观察质量的依赖性和对遮挡的不足。为了解决这个问题，本文提出了一个具有点对注意力感知机制的双向对应预测网络。该网络不仅要求模型点预测对应关系，还明确地对观察和模型先验之间的几何相似性进行建模。我们的关键见解是，每个模型点和场景点之间的相关性为学习点对匹配提供了关键信息。为了进一步解决特征分布分歧带来的相关性噪声，我们设计了一个简单但有效的伪孪生网络来改善特征的一致性。在线MOD、YCB-Video和Occ-LineMOD的公共数据集上的实验结果表明，所提出的方法在性能上优于其他最先进的方法。

    Traditional geometric registration based estimation methods only exploit the CAD model implicitly, which leads to their dependence on observation quality and deficiency to occlusion.To address the problem,the paper proposes a bidirectional correspondence prediction network with a point-wise attention-aware mechanism. This network not only requires the model points to predict the correspondence but also explicitly models the geometric similarities between observations and the model prior.} Our key insight is that the correlations between each model point and scene point provide essential information for learning point-pair matches. To further tackle the correlation noises brought by feature distribution divergence, we design a simple but effective pseudo-siamese network to improve feature homogeneity.Experimental results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that the proposed method achieves better performance than other state-of-the-art methods under the sa
    
[^4]: 基于元学习的堆叠回归方法用于客户终身价值预测

    A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction. (arXiv:2308.08502v1 [cs.LG])

    [http://arxiv.org/abs/2308.08502](http://arxiv.org/abs/2308.08502)

    该论文提出了一个基于元学习的堆叠回归方法，用于客户终身价值预测，旨在解决现有模型在处理各种输入特征时的限制，并避免深度学习方法的复杂性。

    

    全球各地的公司都渴望定位潜在的高价值客户，以扩大收入，而这只能通过更好地了解客户来实现。客户终身价值（CLV）是客户在一段规定的时间内与企业进行的交易/购买的总货币价值，用于预测未来客户互动。CLV在多个不同的商业领域中都有应用，如银行，保险，在线娱乐，游戏和电子商务。现有的基于分布和基本（最近性，频率和金额）的模型在处理各种输入特征方面存在限制。此外，更先进的深度学习方法可能过于复杂，在某些应用领域中增加了不必要的复杂性。因此，我们提出了一个既能够有效又全面简单易懂的系统。考虑到这一点，我们开发了一个基于元学习的堆叠回归方法，用于客户终身价值预测。

    Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency & monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a m
    
[^5]: InTune:基于强化学习的数据流优化用于深度推荐模型

    InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models. (arXiv:2308.08500v1 [cs.IR])

    [http://arxiv.org/abs/2308.08500](http://arxiv.org/abs/2308.08500)

    本文提出了InTune，一个基于强化学习的数据流优化方法，应用于深度推荐模型。通过研究在Netflix计算集群中的DLRM数据处理流程，我们发现目前的流程优化器存在性能不佳、频繁崩溃或需要不切实际的集群重组等问题。

    

    基于深度学习的推荐模型(DLRM)已经成为许多现代推荐系统的重要组成部分。一些公司正在建设大型计算集群专门用于DLRM训练，进而推动了对成本和时间的节约优化的新兴兴趣。在这个场景中所面临的系统挑战是独特的；尽管典型的深度学习训练任务由模型执行主导，但DLRM训练性能中最重要的因素往往是线上数据摄入。在本文中，我们探讨了该数据摄入问题的独特特征，并深入研究了DLRM训练流程中的性能瓶颈和挑战。我们对Netflix计算集群中真实的DLRM数据处理流程进行了研究，观察了线上摄入的性能影响，并识别出现有流程优化器的不足之处。我们发现当前的工具要么产生次优性能，要么经常崩溃，要么需要不现实的集群重组。

    Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.  In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization 
    
[^6]: 面向社交物联网的上下文感知服务推荐系统

    Context-Aware Service Recommendation System for the Social Internet of Things. (arXiv:2308.08499v1 [cs.IR])

    [http://arxiv.org/abs/2308.08499](http://arxiv.org/abs/2308.08499)

    该研究提出了一个面向社交物联网的上下文感知服务推荐系统，通过捕捉设备之间的潜在特征交互和建模设备-服务对的高阶特征交互，实现了准确的评级预测和个性化的服务推荐。

    

    社交物联网（SIoT）使智能设备相互连接并共享数据和服务，为个性化服务推荐提供了机会。然而，现有研究经常忽视了可以提高SIoT环境下推荐的准确性和相关性的关键因素。具体而言，现有技术往往只考虑了设备之间的社交关系的提取，而忽视了服务评论的上下文呈现。本研究旨在通过探索每个设备-服务对的上下文表示来填补这些缺口。首先，我们提出了一种潜在特征组合技术，可以通过聚合SIoT中的设备-设备关系来捕捉潜在特征交互。然后，我们利用因子分解机来建模每个SIoT设备-服务对特定的高阶特征交互，以实现准确的评级预测。最后，我们提出了一个基于评级预测的SIoT服务推荐框架。

    The Social Internet of Things (SIoT) enables interconnected smart devices to share data and services, opening up opportunities for personalized service recommendations. However, existing research often overlooks crucial aspects that can enhance the accuracy and relevance of recommendations in the SIoT context. Specifically, existing techniques tend to consider the extraction of social relationships between devices and neglect the contextual presentation of service reviews. This study aims to address these gaps by exploring the contextual representation of each device-service pair. Firstly, we propose a latent features combination technique that can capture latent feature interactions, by aggregating the device-device relationships within the SIoT. Then, we leverage Factorization Machines to model higher-order feature interactions specific to each SIoT device-service pair to accomplish accurate rating prediction. Finally, we propose a service recommendation framework for SIoT based on r
    
[^7]: HyperBandit: 基于超网络的时间变化用户偏好的上下文强化学习算法在流媒体推荐系统中的应用

    HyperBandit: Contextual Bandit with Hypernewtork for Time-Varying User Preferences in Streaming Recommendation. (arXiv:2308.08497v1 [cs.IR])

    [http://arxiv.org/abs/2308.08497](http://arxiv.org/abs/2308.08497)

    HyperBandit是一种基于超网络的上下文强化学习方法，用于处理流媒体推荐系统中时间变化的用户偏好。它通过建立时间特征和用户偏好之间的关联，动态调整推荐模型以适应动态场景。

    

    在现实世界的流媒体推荐系统中，用户偏好经常在时间上动态变化（例如，在工作日和周末用户可能有不同的偏好）。现有的基于强化学习的流媒体推荐模型只将时间视为时间戳，没有明确地建模时间变量与时间变化的用户偏好之间的关系。这导致推荐模型无法快速适应动态场景。为了解决这个问题，我们提出了一种使用超网络的上下文强化学习方法，称为HyperBandit，其将时间特征作为输入，并动态调整推荐模型以适应时间变化的用户偏好。具体而言，HyperBandit维护了一个能够生成用于估计时间变化奖励的参数的神经网络，考虑了时间特征和用户偏好之间的相关性。使用估计的时间变化奖励，我们采用强化学习策略来进行在线推荐。

    In real-world streaming recommender systems, user preferences often dynamically change over time (e.g., a user may have different preferences during weekdays and weekends). Existing bandit-based streaming recommendation models only consider time as a timestamp, without explicitly modeling the relationship between time variables and time-varying user preferences. This leads to recommendation models that cannot quickly adapt to dynamic scenarios. To address this issue, we propose a contextual bandit approach using hypernetwork, called HyperBandit, which takes time features as input and dynamically adjusts the recommendation model for time-varying user preferences. Specifically, HyperBandit maintains a neural network capable of generating the parameters for estimating time-varying rewards, taking into account the correlation between time features and user preferences. Using the estimated time-varying rewards, a bandit policy is employed to make online recommendations by learning the laten
    
[^8]: 理解对话式推荐系统中用户意图建模：一项系统文献综述

    Understanding User Intent Modeling for Conversational Recommender Systems: A Systematic Literature Review. (arXiv:2308.08496v1 [cs.IR])

    [http://arxiv.org/abs/2308.08496](http://arxiv.org/abs/2308.08496)

    该论文进行了一项系统文献综述，研究了对话式推荐系统中用户意图建模的相关模型和特征。研究结果为研究人员提供了模型选择、质量问题和评估指标等方面的洞察。

    

    背景：用户意图建模是自然语言处理中的一个关键过程，旨在识别用户请求背后的潜在目的，从而实现个性化的响应。在过去十年中，文献中引入了大量的方法（超过13,000篇论文），理解人工智能系统中相关概念和常用模型是至关重要的。方法：我们进行了一项系统文献综述，收集了设计对话式推荐系统中通常采用的模型的数据。从收集到的数据中，我们开发了一个决策模型，以帮助研究人员选择最适合其系统的模型。此外，我们进行了两个案例研究，评估我们提出的决策模型的有效性。结果：我们的研究分析了59个不同的模型，并确定了74个常用的特征。我们提供了关于潜在的模型组合、模型选择趋势、质量问题、评估指标以及经常使用的数据的见解。

    Context: User intent modeling is a crucial process in Natural Language Processing that aims to identify the underlying purpose behind a user's request, enabling personalized responses. With a vast array of approaches introduced in the literature (over 13,000 papers in the last decade), understanding the related concepts and commonly used models in AI-based systems is essential. Method: We conducted a systematic literature review to gather data on models typically employed in designing conversational recommender systems. From the collected data, we developed a decision model to assist researchers in selecting the most suitable models for their systems. Additionally, we performed two case studies to evaluate the effectiveness of our proposed decision model. Results: Our study analyzed 59 distinct models and identified 74 commonly used features. We provided insights into potential model combinations, trends in model selection, quality concerns, evaluation measures, and frequently used dat
    
[^9]: 通过基于嘴唇-音素字级相关性的视觉预训练和跨模态融合编码器来改进视听语音识别

    Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder. (arXiv:2308.08488v1 [cs.CL])

    [http://arxiv.org/abs/2308.08488](http://arxiv.org/abs/2308.08488)

    本文提出了通过基于嘴唇-音素字级相关性的视觉预训练和跨模态融合编码器来改进视听语音识别的两种新技术。这些技术可以在预训练和微调阶段准确对齐音频和视频流，并且充分利用模态互补性。

    

    最近的研究中观察到，在低质量视频的端到端框架下，从自动语音识别系统到视听语音识别系统的性能略有改进。据认为，音频和视觉模态之间不匹配的收敛速度和专门的输入表示导致了这个问题。在本文中，我们提出了两种新技术来改进视听语音识别（AVSR）在预训练和微调训练框架下。首先，我们探索了普通话中嘴唇形状和音节级音素字单元之间的相关性，以建立准确的帧级音节边界。这使得在视觉模型预训练和跨模态融合过程中能够对齐视频和音频流。接下来，我们提出了一种音频引导的跨模态融合编码器（CMFE）神经网络，利用主要训练参数来实现多个跨模态注意力层的充分利用模态互补性。在实验上进行了验证

    In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the
    
[^10]: 通过使用快捷特征实现公平视觉识别的良性捷径消除偏见：一篇实例分析

    Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features. (arXiv:2308.08482v1 [cs.LG])

    [http://arxiv.org/abs/2308.08482](http://arxiv.org/abs/2308.08482)

    本研究提出了“快捷去偏”的方法，通过将目标任务对偏见属性的学习从偏见特征转移到快捷特征上，并采用因果干预来消除偏见特征的影响，以解决机器学习模型依赖敏感社会属性进行预测的公平性问题。

    

    机器学习模型经常学会依赖于性别和种族等敏感社会属性进行预测，这在社会应用中，如招聘、银行和刑事司法中，带来重大公平风险。现有工作通过减少模型中与社会属性相关的信息来解决这个问题。然而，目标任务和这些社会属性之间的高相关性使得在目标任务上的学习与去偏不兼容。鉴于模型偏见是由于学习偏见特征（例如性别）来优化目标任务而引起的，我们探讨以下研究问题：我们是否可以利用快捷特征来替代偏见特征在去偏的目标任务优化中的作用？为此，我们提出了“快捷去偏”的方法，首先将目标任务对偏见属性的学习从偏见特征转移到快捷特征上，然后采用因果干预来消除偏见特征的影响。

    Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate sh
    
[^11]: 动态邮件重新排序问题的固定算法平衡

    Stationary Algorithmic Balancing For Dynamic Email Re-Ranking Problem. (arXiv:2308.08460v1 [cs.IR])

    [http://arxiv.org/abs/2308.08460](http://arxiv.org/abs/2308.08460)

    提出了一种名为MOSR的算法，用于解决动态邮件重新排序问题。该算法使用自适应控制模型来平衡亲密度，及时性和简洁性等标准，并能够适应偏好的变化。在恩隆邮件数据集上的实验结果表明，MOSR在非稳态偏好下表现出更好的性能，并且在具有高方差的邮件特征的小型抽样数据集上也能保持稳定的排名。

    

    电子邮件平台需要生成满足用户偏好并随时间变化的个性化邮件排序。我们将其作为一个基于三个标准的推荐问题来处理：亲密度（发件人和主题与用户的相关度），及时性（邮件的最近程度）和简洁性（邮件的简短程度）。我们提出了MOSR（多目标稳态推荐器），一种新颖的在线算法，它使用自适应控制模型来动态平衡这些标准并适应偏好变化。我们在恩隆邮件数据集上评估了MOSR，这是一个包含大量实际邮件的集合，并将其与其他基准进行了比较。结果显示MOSR取得了更好的性能，尤其是在非稳态偏好下，用户随时间不同程度地评估不同的标准。我们还在一个小型抽样数据集上测试了MOSR的鲁棒性，该数据集的邮件特征变化很大，并展示了它在不同样本中保持稳定的排名。

    Email platforms need to generate personalized rankings of emails that satisfy user preferences, which may vary over time. We approach this as a recommendation problem based on three criteria: closeness (how relevant the sender and topic are to the user), timeliness (how recent the email is), and conciseness (how brief the email is). We propose MOSR (Multi-Objective Stationary Recommender), a novel online algorithm that uses an adaptive control model to dynamically balance these criteria and adapt to preference changes. We evaluate MOSR on the Enron Email Dataset, a large collection of real emails, and compare it with other baselines. The results show that MOSR achieves better performance, especially under non-stationary preferences, where users value different criteria more or less over time. We also test MOSR's robustness on a smaller down-sampled dataset that exhibits high variance in email characteristics, and show that it maintains stable rankings across different samples. Our work
    
[^12]: 知识提示调优的顺序推荐

    Knowledge Prompt-tuning for Sequential Recommendation. (arXiv:2308.08459v1 [cs.IR])

    [http://arxiv.org/abs/2308.08459](http://arxiv.org/abs/2308.08459)

    该论文提出了知识提示调优的顺序推荐(KP4SR)方法，通过引入外部知识库和构建知识提示，解决了顺序推荐中的语义差距和信息损失问题，从而提高了推荐性能。

    

    预训练语言模型(PLMs)在顺序推荐(SR)中展示出强大的性能，用于提取通用知识。然而，现有方法仍然缺乏领域知识，并且很难捕捉用户的细粒度偏好。同时，许多传统的SR方法通过整合辅助信息来改善这个问题，但却遭受信息损失的困扰。总而言之，我们认为一个好的推荐系统应该同时利用通用知识和领域知识。因此，我们引入了一个外部知识库，并提出了知识提示调优的顺序推荐(KP4SR)。具体来说，我们构建了一组关系模板，并将结构化知识图谱(KG)转化为知识提示，以解决语义差距的问题。然而，知识提示破坏了原始数据结构并引入了大量的噪音。我们进一步构建了一个知识树，并提出了一个知识树的方法来减少噪音并提高推荐性能。

    Pre-trained language models (PLMs) have demonstrated strong performance in sequential recommendation (SR), which are utilized to extract general knowledge. However, existing methods still lack domain knowledge and struggle to capture users' fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss. To summarize, we believe that a good recommendation system should utilize both general and domain knowledge simultaneously. Therefore, we introduce an external knowledge base and propose Knowledge Prompt-tuning for Sequential Recommendation (\textbf{KP4SR}). Specifically, we construct a set of relationship templates and transform a structured knowledge graph (KG) into knowledge prompts to solve the problem of the semantic gap. However, knowledge prompts disrupt the original data structure and introduce a significant amount of noise. We further construct a knowledge tree and propose a knowledge tre
    
[^13]: 最严格可接受的最短路径

    Tightest Admissible Shortest Path. (arXiv:2308.08453v1 [cs.DS])

    [http://arxiv.org/abs/2308.08453](http://arxiv.org/abs/2308.08453)

    该论文提出了一种针对加权有向图的最严格可接受的最短路径问题，利用边权不确定性进行计算成本交换，并提供了一个完整的算法来解决此问题，并保证解的质量。

    

    图中的最短路径问题对于人工智能来说是基础性的。几乎所有问题的变种和相关算法都忽略了边权计算时间及其与权重不确定性的常见关系。这意味着考虑这些因素可能会在相关应用中带来性能提升。最近，提出了一种针对加权有向图的推广框架，可以多次计算（估计）边权，随着精度的增加和运行时间的增加。我们在此框架上引入了寻找最严格可接受的最短路径（TASP）的问题；这是将最短路径问题推广到有界不确定性的情况，其中可以通过计算成本来交换边权的不确定性。我们提出了一个完整的算法来解决TASP，并保证了解的质量。实证评估支持这种方法的有效性。

    The shortest path problem in graphs is fundamental to AI. Nearly all variants of the problem and relevant algorithms that solve them ignore edge-weight computation time and its common relation to weight uncertainty. This implies that taking these factors into consideration can potentially lead to a performance boost in relevant applications. Recently, a generalized framework for weighted directed graphs was suggested, where edge-weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. We build on this framework to introduce the problem of finding the tightest admissible shortest path (TASP); a path with the tightest suboptimality bound on the optimal cost. This is a generalization of the shortest path problem to bounded uncertainty, where edge-weight uncertainty can be traded for computational cost. We present a complete algorithm for solving TASP, with guarantees on solution quality. Empirical evaluation supports the effectiveness of this approac
    
[^14]: 中国AIGC的现状及未来展望

    AIGC In China: Current Developments And Future Outlook. (arXiv:2308.08451v1 [cs.AI])

    [http://arxiv.org/abs/2308.08451](http://arxiv.org/abs/2308.08451)

    本文分析了中国在人工智能生成内容（AIGC）领域的现状，讨论了AIGC的基础技术、市场状况和发展轨迹，并重点强调了AIGC的生态建设。预测了行业未来的挑战和发展方向。

    

    对于人工智能生成内容（AIGC）的日益关注，已经在日常生活、工业制造和学术界等各个方面产生了深远的影响。鉴于AIGC发展的全球趋势和竞争力，本研究旨在分析中国在该领域的现状。首先，研究概述了AIGC的基础技术和当前应用。随后，通过关键词搜索识别相关学术论文，深入研究了中国的AIGC市场状况、政策环境和发展轨迹。此外，本文全面审视了AIGC产品及其相应生态系统，并强调了AIGC的生态建设。最后，本文讨论了AIGC行业面临的挑战和风险，并根据AIGC的竞争性洞察展望了行业的未来。

    The increasing attention given to AI Generated Content (AIGC) has brought a profound impact on various aspects of daily life, industrial manufacturing, and the academic sector. Recognizing the global trends and competitiveness in AIGC development, this study aims to analyze China's current status in the field. The investigation begins with an overview of the foundational technologies and current applications of AIGC. Subsequently, the study delves into the market status, policy landscape, and development trajectory of AIGC in China, utilizing keyword searches to identify relevant scholarly papers. Furthermore, the paper provides a comprehensive examination of AIGC products and their corresponding ecosystem, emphasizing the ecological construction of AIGC. Finally, this paper discusses the challenges and risks faced by the AIGC industry while presenting a forward-looking perspective on the industry's future based on competitive insights in AIGC.
    
[^15]: 在金融领域中实现量子生成对抗网络（qGAN）和QCBM

    Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance. (arXiv:2308.08448v1 [quant-ph])

    [http://arxiv.org/abs/2308.08448](http://arxiv.org/abs/2308.08448)

    这项研究讨论了在金融领域中应用量子机器学习的新研究方向，通过比较qGAN和QCBM等模型，展示了在金融领域中实现量子优势的潜力。

    

    量子机器学习（QML）是一个跨学科的领域，由两个最具创新性的研究领域组成：量子计算和经典机器学习（ML），ML和人工智能（AI）被认为是将受到量子计算机兴起影响的第一个领域。这项工作讨论了在金融中应用量子机器学习（QML）的一些新研究领域，我们讨论了一些已在金融界引起关注的QML模型，以及使用模拟环境中的真实金融数据集对qGAN（量子生成对抗网络）和QCBM（量子电路Born机）等模型进行比较。对于qGAN，我们定义了鉴别器和生成器的量子电路，并展示了未来在金融领域中通过QML实现量子优势的潜力。

    Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material & molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
    
[^16]: 可解释的人工智能在临床风险预测中的应用:概念、方法和方式的调查

    Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities. (arXiv:2308.08407v1 [cs.LG])

    [http://arxiv.org/abs/2308.08407](http://arxiv.org/abs/2308.08407)

    这篇综述论文讨论了可解释的人工智能在临床风险预测中的应用，包括概念、方法和方式。可解释性对于确保人们对AI系统的信任和可靠性至关重要，除了解释性之外，还涉及公平性、偏见、信任和透明度等方面。该综述还讨论了近期在临床风险预测中可解释模型的进展。

    

    最近人工智能在医疗保健领域的应用取得了令人难以置信的成果，在诊断和疾病预测方面超越了人类性能。然而，随着人工智能模型复杂性的增加，对其不透明性、潜在偏见以及可解释性的担忧也日益增加。为了确保人们对人工智能系统的信任和可靠性，尤其是在临床风险预测模型中，可解释性变得至关重要。可解释性通常指的是人工智能系统向人类利益相关者提供其决策逻辑或决策本身的稳健解释能力。在临床风险预测中，公平性、偏见、信任和透明度等其他方面的可解释性也代表了超越可解释性本身的重要概念。在本综述中，我们探讨了这些概念之间的关系，因为它们通常一起或可互换地使用。本综述还讨论了最近在临床风险预测的可解释模型的发展进展，

    Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highligh
    
[^17]: PDPK: 用于制造业的合成过程数据和对应程序化知识的框架

    PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing. (arXiv:2308.08371v1 [cs.AI])

    [http://arxiv.org/abs/2308.08371](http://arxiv.org/abs/2308.08371)

    该论文提出了一个框架，用于合成包含过程数据和对应程序化知识的数据集，并比较了不同嵌入方法的性能。

    

    程序化知识描述了如何完成任务和解决问题。这样的知识通常由领域专家持有，例如制造业中调整参数以达到质量目标的操作员。据我们所知，目前公开可用的包含过程数据和对应程序化知识的真实世界数据集目前还不存在，可能是因为企业对知识进展的损失存在担心。因此，我们提供了一个可以适应不同领域的生成合成数据集的框架。该框架的设计选择受到我们可以访问的两个实际的程序化知识数据集的启发。除了包含符合资源描述框架（RDF）标准的知识图形中的程序化知识的表示外，该框架还模拟参数化过程并提供一致的过程数据。我们比较了在生成的知识图形上的已建立的嵌入方法，详细说明了哪些开箱即用的方法具有重新解释知识潜力。

    Procedural knowledge describes how to accomplish tasks and mitigate problems. Such knowledge is commonly held by domain experts, e.g. operators in manufacturing who adjust parameters to achieve quality targets. To the best of our knowledge, no real-world datasets containing process data and corresponding procedural knowledge are publicly available, possibly due to corporate apprehensions regarding the loss of knowledge advances. Therefore, we provide a framework to generate synthetic datasets that can be adapted to different domains. The design choices are inspired by two real-world datasets of procedural knowledge we have access to. Apart from containing representations of procedural knowledge in Resource Description Framework (RDF)-compliant knowledge graphs, the framework simulates parametrisation processes and provides consistent process data. We compare established embedding methods on the resulting knowledge graphs, detailing which out-of-the-box methods have the potential to rep
    
[^18]: 用于人-物交互检测的凝聚Transformer

    Agglomerative Transformer for Human-Object Interaction Detection. (arXiv:2308.08370v1 [cs.CV])

    [http://arxiv.org/abs/2308.08370](http://arxiv.org/abs/2308.08370)

    我们提出了一种凝聚Transformer（AGER），该方法在人-物交互检测中以单阶段和端到端的方式灵活利用额外的实例级提示。这种方法通过动态聚类补丁标记并将其与文本对齐，从而显著提高了实例级提示的提取效果，并在HICO-Det数据集上取得了最新的36.75 mAP性能。

    

    我们提出了一种凝聚Transformer（AGER），该方法使基于Transformer的人-物交互（HOI）检测器能够首次以单阶段和端到端的方式灵活利用额外的实例级提示。AGER通过动态聚类补丁标记来获得实例标记，并通过文本指导将聚类中心与实例对齐，从而获得两个优势：1）完整性：每个实例标记都鼓励包含实例的所有有区别特征区域，这对于提取不同的实例级提示表现出了显著的改进，并随后导致HOI检测在HICO-Det上达到36.75个mAP的最新性能。2）效率：动态聚类机制使得AGER能够与Transformer编码器的特征学习一起生成实例标记，消除了先前方法中需要额外的物体检测器或实例解码器的需求，从而允许提取可取的额外提示。

    We propose an agglomerative Transformer (AGER) that enables Transformer-based human-object interaction (HOI) detectors to flexibly exploit extra instance-level cues in a single-stage and end-to-end manner for the first time. AGER acquires instance tokens by dynamically clustering patch tokens and aligning cluster centers to instances with textual guidance, thus enjoying two benefits: 1) Integrality: each instance token is encouraged to contain all discriminative feature regions of an instance, which demonstrates a significant improvement in the extraction of different instance-level cues and subsequently leads to a new state-of-the-art performance of HOI detection with 36.75 mAP on HICO-Det. 2) Efficiency: the dynamical clustering mechanism allows AGER to generate instance tokens jointly with the feature learning of the Transformer encoder, eliminating the need of an additional object detector or instance decoder in prior methods, thus allowing the extraction of desirable extra cues fo
    
[^19]: 元学习是否是解决推荐系统中冷启动问题的正确方法？

    Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?. (arXiv:2308.08354v1 [cs.IR])

    [http://arxiv.org/abs/2308.08354](http://arxiv.org/abs/2308.08354)

    本文研究表明，针对推荐系统中的冷启动问题，元学习技术在处理深度学习模型时已成为最受欢迎的方法。然而，当前的元学习方法在实际推荐系统中并不实用，因为这些系统拥有庞大的用户和物品数量，且有严格的延迟要求。

    

    推荐系统已经成为现代在线产品和服务的基础构建模块，并对用户体验产生了重大影响。在过去的几年中，深度学习方法吸引了大量的研究，并在现代实际推荐系统中得到广泛应用。然而，处理冷启动设置下的推荐问题，例如当用户在系统中的互动有限时，仍然是一个远未解决的问题。元学习技术，尤其是基于优化的元学习，最近已成为学术研究文献中处理推荐系统中冷启动问题的最流行方法。然而，目前的元学习方法对于拥有数十亿用户和物品以及严格的延迟要求的现实推荐系统来说并不实用。在本文中，我们展示了在常用基准上获得类似或更高性能是可能的。

    Recommender systems have become fundamental building blocks of modern online products and services, and have a substantial impact on user experience. In the past few years, deep learning methods have attracted a lot of research, and are now heavily used in modern real-world recommender systems. Nevertheless, dealing with recommendations in the cold-start setting, e.g., when a user has done limited interactions in the system, is a problem that remains far from solved. Meta-learning techniques, and in particular optimization-based meta-learning, have recently become the most popular approaches in the academic research literature for tackling the cold-start problem in deep learning models for recommender systems. However, current meta-learning approaches are not practical for real-world recommender systems, which have billions of users and items, and strict latency requirements. In this paper we show that it is possible to obtaining similar, or higher, performance on commonly used benchma
    
[^20]: 使用可控数据增强实现图的带外分布泛化

    Graph Out-of-Distribution Generalization with Controllable Data Augmentation. (arXiv:2308.08344v1 [cs.LG])

    [http://arxiv.org/abs/2308.08344](http://arxiv.org/abs/2308.08344)

    本论文提出了一种名为“OOD-GMixup”的方法，利用可控数据增强来解决图的带外分布泛化问题。该方法通过提取图合理性和生成虚拟样本的方式来消除虚假相关性和稳定性问题。

    

    图神经网络（GNN）在图属性分类方面表现出非凡的性能。然而，由于训练和测试数据的选择偏差（例如，在小图上进行训练，在大图上进行测试，或在稠密图上进行训练，在稀疏图上进行测试），分布偏差很普遍。更重要的是，我们经常观察到尽管有单边偏向的数据分区，但却存在着同时具有规模和密度的混合结构分布偏移。混合分布偏移中的伪相关性降低了先前GNN方法的性能，并且在不同数据集之间显示出较大的不稳定性。为了缓解这个问题，我们提出了“OOD-GMixup”在度量空间中以联合操作训练分布的可控数据增强。具体来说，我们首先提取图合理性来消除由于不相关信息而引起的虚假相关性。其次，我们对图合理性进行扰动生成虚拟样本

    Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale r
    
[^21]: 通过发现高阶抽象来学习逻辑程序

    Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])

    [http://arxiv.org/abs/2308.08334](http://arxiv.org/abs/2308.08334)

    本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。

    

    发现新颖的抽象对于人类级别的人工智能至关重要。我们介绍了一种发现高阶抽象（例如map、filter和fold）的方法。我们专注于归纳逻辑编程，即从示例和背景知识中归纳逻辑程序。我们引入了高阶重构问题，目标是通过引入高阶抽象来压缩逻辑程序。我们将我们的方法实现在STEVIE中，它将高阶重构问题建模为约束优化问题。我们在多个领域，包括程序合成和视觉推理，的实验结果表明，与没有重构相比，STEVIE可以提高预测精度27%并将学习时间减少47%。我们还展示了STEVIE可以发现适用于不同领域的抽象。

    Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
    
[^22]: 数据驱动可解释性在数学优化中的框架

    A Framework for Data-Driven Explainability in Mathematical Optimization. (arXiv:2308.08309v1 [math.OC])

    [http://arxiv.org/abs/2308.08309](http://arxiv.org/abs/2308.08309)

    该论文介绍了一种在数学优化中引入数据驱动可解释性的框架，通过与过去类似情况下的解进行比较来找到具有相似特征的解决方案。研究表明，尽管可解释模型在某些情况下是NP-hard的，但在一些多项式可解的情况下可行。

    

    随着数学规划的进步，我们现在有能力有效地解决几十年前被认为无法解决的大规模实际问题。然而，由于优化软件被视为黑盒子，一些可证明的最优解可能不被接受。虽然科学家们对此很了解，但对于实践者来说却很难理解。因此，我们主张将解释性作为另一个评估标准引入解决方案，既包括目标值，也包括可解释性，这使得我们能够在这两个标准之间找到权衡解。通过将解决方案与过去类似情况下实施的（不一定是最优的）解进行比较，我们达到了可解释性。因此，更喜欢展现相似特征的解决方案。尽管我们证明了即使在简单情况下可解释模型也是NP-hard，但我们确定了相关的多项式可解情况，如可解释的最短路径问题。在ar的数值实验上也验证了我们的结果。

    Advancements in mathematical programming have made it possible to efficiently tackle large-scale real-world problems that were deemed intractable just a few decades ago. However, provably optimal solutions may not be accepted due to the perception of optimization software as a black box. Although well understood by scientists, this lacks easy accessibility for practitioners. Hence, we advocate for introducing the explainability of a solution as another evaluation criterion, next to its objective value, which enables us to find trade-off solutions between these two criteria. Explainability is attained by comparing against (not necessarily optimal) solutions that were implemented in similar situations in the past. Thus, solutions are preferred that exhibit similar features. Although we prove that already in simple cases the explainable model is NP-hard, we characterize relevant polynomially solvable cases such as the explainable shortest-path problem. Our numerical experiments on both ar
    
[^23]: 在模糊环境中融合认知地图学习和主动推理以进行规划

    Integrating cognitive map learning and active inference for planning in ambiguous environments. (arXiv:2308.08307v1 [cs.AI])

    [http://arxiv.org/abs/2308.08307](http://arxiv.org/abs/2308.08307)

    本研究在模糊环境中提出了一种融合认知地图学习和主动推理的规划方法，实验结果表明主动推理代理在处理具有模糊信息的挑战性场景的规划中更为有效。

    

    生物需要同时获得认知地图来学习世界的结构和能够应对导航模糊环境挑战的规划机制。尽管在每个独立领域都取得了显著进展，但如何最好地将它们集成在一起仍然是一个未解决的研究问题。本文提出将一种统计模型的认知地图形成与支持不确定性规划的主动推理代理进行集成。具体而言，我们研究了克隆结构认知图(CSCG)模型的认知地图形成，并比较了一个简单的克隆图代理和一个以主动推理为驱动的克隆图代理在三个空间导航场景中的效果。我们的研究结果表明，虽然两种代理在简单场景中都有效，但主动推理代理在面对挑战性场景中的规划上更有效，其中感知观察提供有关位置的模糊信息。

    Living organisms need to acquire both cognitive maps for learning the structure of the world and planning mechanisms able to deal with the challenges of navigating ambiguous environments. Although significant progress has been made in each of these areas independently, the best way to integrate them is an open research question. In this paper, we propose the integration of a statistical model of cognitive map formation within an active inference agent that supports planning under uncertainty. Specifically, we examine the clone-structured cognitive graph (CSCG) model of cognitive map formation and compare a naive clone graph agent with an active inference-driven clone graph agent, in three spatial navigation scenarios. Our findings demonstrate that while both agents are effective in simple scenarios, the active inference agent is more effective when planning in challenging scenarios, in which sensory observations provide ambiguous information about location.
    
[^24]: 鲁棒贝叶斯满足。

    Robust Bayesian Satisficing. (arXiv:2308.08291v1 [cs.LG])

    [http://arxiv.org/abs/2308.08291](http://arxiv.org/abs/2308.08291)

    本文提出了一种名为RoBOS的鲁棒贝叶斯满足算法，用于解决在上下文贝叶斯优化中存在分布偏移时的问题。该算法能够在一定的分布偏移量下保证亏得不严重的子线性遗憾，并且具有与分布偏移量无关的较弱遗憾边界。

    

    分布偏移对于实现当代机器学习的鲁棒性构成了重大挑战。为了克服这一挑战，鲁棒满足（RS）在实现超过期望阈值的效用的同时，寻求对于未指定的分布偏移的鲁棒解决方案。本文关注在上下文贝叶斯优化中存在真实和参考分布之间的差异时的鲁棒满足（RS）问题。我们提出了一种名为RoBOS的新型噪声黑箱优化的鲁棒贝叶斯满足算法。在某些关于分布偏移量的假设下，我们的算法保证亏得不严重的子线性遗憾。此外，我们定义了一种较弱的遗憾概念，称为鲁棒满足遗憾，其中我们的算法实现了与分布偏移量无关的子线性上界。为了展示我们方法的有效性，我们将其应用于各种学习问题，并与其他方法进行比较，如分布交换方法。

    Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally rob
    
[^25]: 它其实不那么糟糕：理解生成变换模型对OOD泛化的神秘性能下降

    It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models. (arXiv:2308.08268v1 [cs.LG])

    [http://arxiv.org/abs/2308.08268](http://arxiv.org/abs/2308.08268)

    生成变换模型在OOD泛化方面存在神秘的性能下降。研究人员观察到模型在训练和泛化数字运算时的行为之间的不一致，并尝试提出解决方案，但仍未解决本质机制。

    

    生成变换模型在解决各种问题上取得了显著的成就。然而，它们的泛化能力并没有完全被理解，并且并不总是令人满意。研究人员从基本的数学任务（如n位数的加法或乘法）开始，作为重要视角来研究模型的泛化行为。有趣的是，观察到当模型在n位数运算（例如加法）上进行训练时，模型在未见过的长度为n位的输入上可以成功泛化（即内分布泛化），但在长度更长、未见过的情况下（即外分布泛化）会失败并且表现神秘。研究尝试通过修改位置嵌入、微调和引入更广泛或更有指导性的数据来弥合这一差距。然而，如果不解决本质机制，这些解决方案的稳健性几乎没有任何保证。

    Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performan
    
[^26]: 描述逻辑进入二阶--用全称量化概念扩展EL

    Description Logics Go Second-Order -- Extending EL with Universally Quantified Concepts. (arXiv:2308.08252v1 [cs.LO])

    [http://arxiv.org/abs/2308.08252](http://arxiv.org/abs/2308.08252)

    这篇论文通过引入全称量化概念扩展了描述逻辑$\mathcal{EL}$，分别提出了模式语义和二阶语义，研究了它们的性质并证明了它们在有用片段中的结论相同。

    

    历史上，描述逻辑的研究主要集中在可以翻译成可判定的一阶逻辑片段的特征上。在本文中，我们放弃了这个限制，寻找有用且可判定的一阶逻辑以外的扩展。我们引入了全称量化概念，它们以可以被任意概念替换的变量形式出现，并定义了这个扩展的两种语义。模式语义只允许概念变量被特定语言的概念替换，产生类似模态逻辑的公理模式。二阶语义允许概念变量被域的任意子集替换，类似于二阶逻辑中的量化谓词。为了研究所提出的语义，我们重点关注描述逻辑$\mathcal{EL}$的扩展。我们证明了在扩展的有用片段中，不同语义所蕴含的结论是相同的，从而可以使用经典逻辑的推理方法。

    The study of Description Logics have been historically mostly focused on features that can be translated to decidable fragments of first-order logic. In this paper, we leave this restriction behind and look for useful and decidable extensions outside first-order logic. We introduce universally quantified concepts, which take the form of variables that can be replaced with arbitrary concepts, and define two semantics of this extension. A schema semantics allows replacements of concept variables only by concepts from a particular language, giving us axiom schemata similar to modal logics. A second-order semantics allows replacement of concept variables with arbitrary subsets of the domain, which is similar to quantified predicates in second-order logic.  To study the proposed semantics, we focus on the extension of the description logic $\mathcal{EL}$. We show that for a useful fragment of the extension, the conclusions entailed by the different semantics coincide, allowing us to use cla
    
[^27]: TEST: 文本原型对齐嵌入以激活LLM对时间序列的能力

    TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])

    [http://arxiv.org/abs/2308.08241](http://arxiv.org/abs/2308.08241)

    这篇论文总结了两种使用语言模型完成时间序列任务的策略，通过设计一种适用于语言模型的时间序列嵌入方法来激活语言模型对时间序列数据的能力。虽然结果没有明显超越当前最先进的模型，但可以更好地处理时间序列数据。

    

    本研究总结了两种使用现代语言模型（LLM）完成时间序列（TS）任务的策略：LLM-for-TS，设计和训练一个针对TS数据的基础大模型；TS-for-LLM，使预训练的LLM能够处理TS数据。鉴于数据积累不足、资源有限和语义上下文需求，本研究侧重于TS-for-LLM方法，旨在设计一种适用于LLM的TS嵌入方法，以激活LLM对TS数据的能力。所提出的方法称为TEST。它首先对TS进行标记化处理，建立一个编码器，通过实例、特征和文本原型对齐对它们进行嵌入，然后创建提示以使LLM更容易接受嵌入，并最终实施TS任务。使用8个具有不同结构和大小的LLM对TS分类和预测任务进行了实验。尽管其结果不能显著超越当前为TS任务定制的SOTA模型，但通过将LLM视为模式机器，可以更好地处理TS数据。

    This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, 
    
[^28]: 在自然语言处理中使用基于Transformer的多任务学习的挑战和机会：一项调研

    Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])

    [http://arxiv.org/abs/2308.08234](http://arxiv.org/abs/2308.08234)

    本论文调研了在自然语言处理中使用基于Transformer的多任务学习的挑战和机会。通过对NLP中基于Transformer的MTL方法以及典型机器学习生命周期各阶段的挑战进行讨论，提供了相关领域的概述和动向。

    

    自然语言处理（NLP）模型在各个行业中的广泛应用导致从训练到在生产中运行这些模型的机器学习系统需要有效处理。然而，使用基于Transformer的预训练语言模型进行训练、部署和更新多个模型可能复杂、昂贵且耗时，特别是使用多任务学习（MTL）作为改进效率和性能的方法。本调研首先概述了NLP中基于Transformer的MTL方法。然后，我们讨论了在典型的机器学习生命周期中使用MTL方法面临的挑战和机会，重点关注数据工程、模型开发、部署和监控阶段的挑战。本项调研集中于基于Transformer的MTL架构，并据我们所知是首创的。

    The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it 
    
[^29]: 可解释的多视角深度网络方法论在实验物理中的应用

    Explainable Multi-View Deep Networks Methodology for Experimental Physics. (arXiv:2308.08206v1 [cs.CV])

    [http://arxiv.org/abs/2308.08206](http://arxiv.org/abs/2308.08206)

    该论文介绍了一个可解释的多视角深度网络方法论，应用于实验物理中的多种成像表达分析。该方法论解决了多视角模型可解释性不足的问题。

    

    物理实验常涉及多种成像表达，如X射线扫描和显微图像。深度学习模型已广泛应用于这些实验的监督分析中。合并不同的图像表达经常需要正确分析和做出决策。因此，多视角数据应运而生 - 数据集中的每个样本由来自不同角度、来源或模态的视图描述。多视角学习的概念解决了这些问题。理解深度学习模型的决策过程对于可靠和可信的分析至关重要。因此，最近提出了许多可解释性方法。然而，多视角模型缺乏适当的可解释性，由于其架构的复杂性，难以解释。在本文中，我们提出了适用于视觉领域的不同多视角架构，每个架构都适合解决不同的问题，并提出了解释多视角模型的方法论。

    Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining th
    
[^30]: 基于OWL DL本体论的规划方法（扩展版本）

    Towards Ontology-Mediated Planning with OWL DL Ontologies (Extended Version). (arXiv:2308.08200v1 [cs.AI])

    [http://arxiv.org/abs/2308.08200](http://arxiv.org/abs/2308.08200)

    该论文提出了一种基于OWL DL本体论的规划方法，通过将规划规范和本体分开，并使用接口将其连接在一起，实现了规划专家与本体专家的紧密合作。这种方法优化了相对较小领域下的本体问题，并支持整个OWL DL片段。

    

    传统的规划语言假设领域是封闭的，并具有封闭世界的属性。为了扩展这些语言，已经提出了各种方法来与DL推理结合，然后按照通常的开放世界语义进行解释。目前的DL本体规划方法将DL直接集成到规划语言中，并基于一阶重写或重写为datalog来开发实际方法。本文提出了一种新的方法，将规划规范和本体分开，并通过接口将它们连接在一起。这使得规划专家能够使用熟悉的形式化语言工作，同时本体专家可以轻松集成和扩展现有本体。我们的方法针对相对较小的领域进行了优化，并支持整个OWL DL片段。具体思路是将本体规划问题重写为经典的规划问题进行处理。

    While classical planning languages make the closed-domain and closed-world assumption, there have been various approaches to extend those with DL reasoning, which is then interpreted under the usual open-world semantics. Current approaches for planning with DL ontologies integrate the DL directly into the planning language, and practical approaches have been developed based on first-order rewritings or rewritings into datalog. We present here a new approach in which the planning specification and ontology are kept separate, and are linked together using an interface. This allows planning experts to work in a familiar formalism, while existing ontologies can be easily integrated and extended by ontology experts. Our approach for planning with those ontology-mediated planning problems is optimized for cases with comparatively small domains, and supports the whole OWL DL fragment. The idea is to rewrite the ontology-mediated planning problem into a classical planning problem to be process
    
[^31]: 使用自动概率规划建模COVID-19在室内空间中的传播

    Modelling the Spread of COVID-19 in Indoor Spaces using Automated Probabilistic Planning. (arXiv:2308.08190v1 [cs.AI])

    [http://arxiv.org/abs/2308.08190](http://arxiv.org/abs/2308.08190)

    本论文使用自动概率规划和动态图分析建模COVID-19在室内空间的传播，通过非药物干预控制疾病的传播，并比较不同干预策略的效果。

    

    在这项工作中，我们探索了一种基于概率规划和动态图分析的新方法，以模拟COVID-19在室内空间中的传播。我们通过非药物干预控制疾病的传播，如强制佩戴口罩和接种疫苗，并比较拥挤和容量限制对COVID-19传播的影响。我们证明了使用概率规划可以预测不同干预策略的效果。

    The coronavirus disease 2019 (COVID-19) pandemic has been ongoing for around 3 years, and has infected over 750 million people and caused over 6 million deaths worldwide at the time of writing. Throughout the pandemic, several strategies for controlling the spread of the disease have been debated by healthcare professionals, government authorities, and international bodies. To anticipate the potential impact of the disease, and to simulate the effectiveness of different mitigation strategies, a robust model of disease spread is needed. In this work, we explore a novel approach based on probabilistic planning and dynamic graph analysis to model the spread of COVID-19 in indoor spaces. We endow the planner with means to control the spread of the disease through non-pharmaceutical interventions (NPIs) such as mandating masks and vaccines, and we compare the impact of crowds and capacity limits on the spread of COVID-19 in these settings. We demonstrate that the use of probabilistic planni
    
[^32]: 算法补救中的内生宏观动力学

    Endogenous Macrodynamics in Algorithmic Recourse. (arXiv:2308.08187v1 [cs.LG])

    [http://arxiv.org/abs/2308.08187](http://arxiv.org/abs/2308.08187)

    这项研究填补了算法补救中的内生动力学和对策影响其他个体的研究空白，提出了一个广义框架，并揭示了对策的隐藏成本。通过模拟实验，验证了该方法的效果。

    

    现有的关于对策解释和算法补救的研究主要集中在静态环境中的单个个体上：在给定一些估计模型的情况下，目标是找到满足不同需求的单个实例的有效对策解释。这些对策解释的能力处理数据和模型漂移等动态问题仍然是一个较少研究的挑战。与此相关的另一个问题是一个个体对策实施的实际影响其他个体的问题，关于这方面的研究相当少。通过这项工作，我们旨在填补这一空白。首先，我们展示现有的许多方法可以被统一描述为一个广义框架。然后，我们认为现有框架没有考虑到一个隐藏的对策成本，在研究群体层面上的内生动力学时才会显现出来。通过涉及各种最先进的对策解释方法的仿真实验...

    Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely focused on single individuals in a static environment: given some estimated model, the goal is to find valid counterfactuals for an individual instance that fulfill various desiderata. The ability of such counterfactuals to handle dynamics like data and model drift remains a largely unexplored research challenge. There has also been surprisingly little work on the related question of how the actual implementation of recourse by one individual may affect other individuals. Through this work, we aim to close that gap. We first show that many of the existing methodologies can be collectively described by a generalized framework. We then argue that the existing framework does not account for a hidden external cost of recourse, that only reveals itself when studying the endogenous dynamics of recourse at the group level. Through simulation experiments involving various state-of the-art counterfactual
    
[^33]: 增强性能：使用检索增强的端到端任务导向型系统在已知和未知的对话场景下

    Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System. (arXiv:2308.08169v1 [cs.CL])

    [http://arxiv.org/abs/2308.08169](http://arxiv.org/abs/2308.08169)

    本文介绍了一种通过简单的缓存提高端到端任务导向型对话系统的性能的方法。通过微调检索模块并训练端到端的对话系统模型，系统可以动态更新并处理已知和未知的对话场景。实验证明我们的方法相对于强基线方法在非空联合目标准确率上提高了6.7%。

    

    通过利用预训练模型的先进自然语言理解和生成能力，端到端任务导向型对话系统已经取得了令人满意的性能。本文通过一个简单的缓存使得任务导向型对话系统更加灵活。这个缓存能够动态更新系统并处理现有和未知的对话场景。具体来说，我们首先对检索模块进行微调，以便从缓存中有效地检索到最相关的信息。然后，我们训练端到端的对话系统模型，这些模型在生成对话时可以引用和联系对话历史和检索到的信息。缓存的构建非常简单，而任务导向型对话系统的主干模型与现有的预训练生成模型兼容。通过大量的实验验证了我们的框架的优越性能，非空联合目标准确率相对于强基线方法提高了6.7%。

    End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7% compared to strong b
    
[^34]: 用于评估原型部件解释的空间不一致性的解释性基准

    Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations. (arXiv:2308.08162v1 [cs.CV])

    [http://arxiv.org/abs/2308.08162](http://arxiv.org/abs/2308.08162)

    本论文提出了一个用于评估原型部件解释的空间不一致性的解释性基准，并介绍了一种补偿不一致性的方法。通过大量的实证研究，表明了基准的表达能力和补偿方法的有效性。

    

    由于其忠实的自解释性，原型部件网络越来越受欢迎。然而，它们的相似性图计算在倒数第二层网络中。因此，原型激活区域的感受野通常依赖于图像区域外的部分，这可能导致误导性解释。我们将这种不希望的行为称为空间解释不一致，并引入了一个解释性基准，提供一套专用指标来量化这一现象。此外，我们提出了一种补偿不一致性的方法，并将其应用于现有的最先进模型。通过大量的实证研究，我们展示了我们基准的表达能力以及提出的补偿方法的有效性。

    Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.
    
[^35]: AutoGen:通过多代理对话框架实现下一代LLM应用

    AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])

    [http://arxiv.org/abs/2308.08155](http://arxiv.org/abs/2308.08155)

    AutoGen是一种新的框架，通过多个可以互相对话的代理，实现了下一代LLM应用。它利用人类的理解和智能，优雅地处理不完美的生成和推理能力，并通过自动化代理对话简化了复杂的工作流程。

    

    本技术报告介绍了AutoGen，一种新的框架，通过多个可以互相对话的代理来开发LLM应用程序以解决任务。AutoGen代理可以定制、可对话，并且可以无缝地允许人类参与。它们可以在各种模式下运行，利用LLM、人类输入和工具的组合。AutoGen的设计提供了多个优势：a）它能够优雅地处理这些LLM的强大但不完美的生成和推理能力；b）它利用人类的理解和智能，通过代理之间的对话提供有价值的自动化；c）它简化和统一了复杂LLM工作流程的实现，作为自动化代理对话。我们提供了许多不同的例子，展示了开发人员如何轻松使用AutoGen有效地解决任务或构建应用程序，涵盖编程、数学、运筹学、娱乐、在线决策、问答等多个领域。

    This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen's design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.
    
[^36]: SYENet：一种在移动设备上具有实时性能的简单而有效的多低级视觉任务网络

    SYENet: A Simple Yet Effective Network for Multiple Low-Level Vision Tasks with Real-time Performance on Mobile Device. (arXiv:2308.08137v1 [cs.CV])

    [http://arxiv.org/abs/2308.08137](http://arxiv.org/abs/2308.08137)

    SYENet是一种简单而有效的网络，用于在移动设备上进行多个低级视觉任务的实时处理。该网络具有较少的参数量，并通过特定的构建模块和二次连接单元实现了不对称分支结果的有效连接。通过引入异常值感知损失来提高性能，并在实时应用中表现出优秀的PSNR性能。

    

    随着AI硬件加速器的快速发展，将基于深度学习的算法应用于移动设备上解决各种低级视觉任务逐渐成为可能。然而，仍然需要解决两个主要问题：任务特定的算法使它们难以集成到单个神经网络架构中，大量的参数使得实时推理变得困难。为了解决这些问题，我们提出了一种新的网络SYENet，只有大约6K个参数，以实时方式处理移动设备上的多个低级视觉任务。SYENet由两个不对称分支和简单的构建模块组成。为了有效地连接不对称分支的结果，提出了一种二次连接单元（QCU）。此外，为了提高性能，提出了一种新的异常值感知损失来处理图像。所提出的方法通过与实时应用中的其他网络相比具有最佳PSNR来证明其优越性能。

    With the rapid development of AI hardware accelerators, applying deep learning-based algorithms to solve various low-level vision tasks on mobile devices has gradually become possible. However, two main problems still need to be solved: task-specific algorithms make it difficult to integrate them into a single neural network architecture, and large amounts of parameters make it difficult to achieve real-time inference. To tackle these problems, we propose a novel network, SYENet, with only $~$6K parameters, to handle multiple low-level vision tasks on mobile devices in a real-time manner. The SYENet consists of two asymmetrical branches with simple building blocks. To effectively connect the results by asymmetrical branches, a Quadratic Connection Unit(QCU) is proposed. Furthermore, to improve performance, a new Outlier-Aware Loss is proposed to process the image. The proposed method proves its superior performance with the best PSNR as compared with other networks in real-time applica
    
[^37]: 文本引导的图像检索中的排名感知不确定性

    Ranking-aware Uncertainty for Text-guided Image Retrieval. (arXiv:2308.08131v1 [cs.CV])

    [http://arxiv.org/abs/2308.08131](http://arxiv.org/abs/2308.08131)

    本研究提出了一种用于文本引导的图像检索的排名感知不确定性方法，

    

    文本引导的图像检索是为了更好地捕捉用户的意图而将条件文本纳入其中。传统上，现有的方法着重于通过提供的三元组<源图像，源文本，目标图像>来最小化嵌入距离。然而，这种三元组优化可能会限制学习到更详细排名信息的检索模型，例如，这些三元组是一对一对应关系，并且它们未能考虑到在反馈语言和图像中出现的多对多对应关系造成的语义多样性。为了捕捉更多的排名信息，我们提出了一种新颖的排名感知不确定性方法，仅使用提供的三元组来建模多对多的对应关系。我们引入不确定性学习来学习特征的随机排名列表。具体而言，我们的方法主要包括三个组成部分：（1）嵌套不确定性，旨在使用高斯

    Text-guided image retrieval is to incorporate conditional text to better capture users' intent. Traditionally, the existing methods focus on minimizing the embedding distances between the source inputs and the targeted image, using the provided triplets $\langle$source image, source text, target image$\rangle$. However, such triplet optimization may limit the learned retrieval model to capture more detailed ranking information, e.g., the triplets are one-to-one correspondences and they fail to account for many-to-many correspondences arising from semantic diversity in feedback languages and images. To capture more ranking information, we propose a novel ranking-aware uncertainty approach to model many-to-many correspondences by only using the provided triplets. We introduce uncertainty learning to learn the stochastic ranking list of features. Specifically, our approach mainly comprises three components: (1) In-sample uncertainty, which aims to capture semantic diversity using a Gaussi
    
[^38]: 如何在纠错码变压器中进行遮蔽：系统化与双重遮蔽

    How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])

    [http://arxiv.org/abs/2308.08128](http://arxiv.org/abs/2308.08128)

    该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。

    

    在通信和存储系统中，纠错码（ECC）对于确保数据可靠性至关重要。随着深度学习在不同领域的应用广泛扩展，神经网络解码器已成为研究的焦点，超越传统解码算法。在这些神经解码器中，纠错码变压器（ECCT）已经实现了最先进的性能，大幅超过其他方法。为了进一步提高ECCT的性能，我们提出了两种新方法。首先，利用ECC的系统编码技术，我们引入了一个新的遮蔽矩阵来改善ECCT的性能并减少计算复杂性。其次，我们提出了一种新的ECCT变压器架构，称为双重遮蔽的ECCT。该架构以并行方式使用两个不同的遮蔽矩阵，以学习遮蔽自注意力块中编码字位之间更多样的特征关系。大量实验证明了我们方法的有效性。

    In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
    
[^39]: OmniZoomer: 学习在高分辨率球面上移动和缩放

    OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution. (arXiv:2308.08114v1 [cs.CV])

    [http://arxiv.org/abs/2308.08114](http://arxiv.org/abs/2308.08114)

    本文提出了OmniZoomer，一种通过深度学习将Möbius变换整合到网络中，用于在全向图像上进行移动和缩放的方法。通过学习不同条件下的变换特征图，网络能够处理增加的边缘曲率并减轻模糊效果。此外，为了解决混叠问题，作者还提出了两个关键组成部分。

    

    全向图像（ODI）变得越来越受欢迎，因为它们的大视野可以让观众在虚拟现实等沉浸式环境中自由选择视角。通常使用Möbius变换在ODI上提供移动和缩放的机会，但将其应用在图像级别上通常会导致模糊效果和混叠问题。在本文中，我们提出了一种新颖的基于深度学习的方法，称为OmniZoomer，将Möbius变换整合到网络中，用于在ODI上进行移动和缩放。通过学习在不同条件下的各种变换特征图，增强了网络处理增加的边缘曲率，从而减轻了模糊效果。此外，为了解决混叠问题，我们提出了两个关键部分。首先，为了弥补描述曲线所需像素的不足，我们增强了高分辨率（HR）空间中的特征图，并计算...

    Omnidirectional images (ODIs) have become increasingly popular, as their large field-of-view (FoV) can offer viewers the chance to freely choose the view directions in immersive environments such as virtual reality. The M\"obius transformation is typically employed to further provide the opportunity for movement and zoom on ODIs, but applying it to the image level often results in blurry effect and aliasing problem. In this paper, we propose a novel deep learning-based approach, called \textbf{OmniZoomer}, to incorporate the M\"obius transformation into the network for movement and zoom on ODIs. By learning various transformed feature maps under different conditions, the network is enhanced to handle the increasing edge curvatures, which alleviates the blurry effect. Moreover, to address the aliasing problem, we propose two key components. Firstly, to compensate for the lack of pixels for describing curves, we enhance the feature maps in the high-resolution (HR) space and calculate the
    
[^40]: ChatLogo: 一个以大型语言模型为驱动的混合自然编程语言界面，用于基于代理的建模和编程

    ChatLogo: A Large Language Model-Driven Hybrid Natural-Programming Language Interface for Agent-based Modeling and Programming. (arXiv:2308.08102v1 [cs.HC])

    [http://arxiv.org/abs/2308.08102](http://arxiv.org/abs/2308.08102)

    ChatLogo是一个以大型语言模型为驱动的代理建模和编程的混合自然编程语言界面，旨在支持自然语言和编程语言的混合对话，为初学者提供更友好的界面，并避免技术系统过度依赖任何单一的语言模型。

    

    在Papert（1980）提出儿童与计算机对话的思想的基础上，我们提出了ChatLogo，一个以混合自然编程语言为基础的代理模型和编程的界面。我们在ABM&P学习的支持和利用大型语言模型（LLM）支持计算编程学习的最新发展的基础上进行了构建。ChatLogo旨在支持自然语言和编程语言的混合对话，为初学者提供更友好的界面，并避免技术系统过度依赖任何单一的LLM。我们介绍了我们设计的主要元素：智能指令中心和支持创造性表达的对话界面。我们讨论了演示格式和未来的工作。针对支持开放式建构主义ABM&P学习和利用LLM进行教育目的的挑战，我们通过提出第一个建构主义LLM驱动的界面，对该领域做出了贡献。

    Building on Papert (1980)'s idea of children talking to computers, we propose ChatLogo, a hybrid natural-programming language interface for agent-based modeling and programming. We build upon previous efforts to scaffold ABM & P learning and recent development in leveraging large language models (LLMs) to support the learning of computational programming. ChatLogo aims to support conversations with computers in a mix of natural and programming languages, provide a more user-friendly interface for novice learners, and keep the technical system from over-reliance on any single LLM. We introduced the main elements of our design: an intelligent command center, and a conversational interface to support creative expression. We discussed the presentation format and future work. Responding to the challenges of supporting open-ended constructionist learning of ABM & P and leveraging LLMs for educational purposes, we contribute to the field by proposing the first constructionist LLM-driven inter
    
[^41]: S-Mixup: 结构Mixup用于图神经网络

    S-Mixup: Structural Mixup for Graph Neural Networks. (arXiv:2308.08097v1 [cs.LG])

    [http://arxiv.org/abs/2308.08097](http://arxiv.org/abs/2308.08097)

    S-Mixup是一种在节点分类任务中使用结构信息的新型Mixup增强方法，通过图神经网络的预测置信度和边梯度来构建Mixup池，并在真实世界基准数据集上进行了验证。

    

    现有的应用Mixup技术于图中的研究主要集中在图分类任务上，而节点分类的研究仍未深入探索。在本文中，我们提出了一种新的节点分类Mixup增强方法，称为结构Mixup（S-Mixup）。其核心思想是在混合节点时考虑结构信息。具体而言，S-Mixup通过图神经网络（GNN）分类器获得图中未标记节点的伪标签和预测置信度。这些伪标签和置信度作为构成跨类和内类Mixup的Mixup池的标准。此外，我们利用GNN训练中获得的边梯度，并提出了一种基于梯度的边选择策略，用于选择连接到Mixup生成的节点的边。通过对真实世界基准数据集的大量实验证明了S-Mixup在节点分类任务上的有效性。我们观察到S-Mixup在节点分类任务上的效果。

    Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-M
    
[^42]: 基于去中心化图神经网络的隐私保护推荐系统

    Decentralized Graph Neural Network for Privacy-Preserving Recommendation. (arXiv:2308.08072v1 [cs.IR])

    [http://arxiv.org/abs/2308.08072](http://arxiv.org/abs/2308.08072)

    本文提出了一种去中心化图神经网络（DGREC）框架，用于隐私保护推荐，其中用户可以选择公开他们的交互。该框架通过图构建、局部梯度计算和全局梯度传递三个阶段实现，同时引入了名为安全梯度共享的差分隐私机制，保护用户的私密数据。

    

    在不违反用户隐私的情况下构建基于图神经网络（GNN）的推荐系统是具有挑战性的。现有方法可以分为联邦GNN和去中心化GNN两种。然而，这两种方法都存在问题，如通信效率低和隐私泄露。本文提出了一种新的去中心化GNN框架，名为DGREC，用于隐私保护推荐，用户可以选择公开他们的交互。该框架包括三个阶段，即图构建、局部梯度计算和全局梯度传递。第一阶段为每个用户构建了一个本地内部物品超图和一个全局用户间图。第二阶段对用户偏好进行建模，并在每个本地设备上计算梯度。第三阶段设计了一种名为安全梯度共享的本地差分隐私机制，以保护用户的私密数据。我们在三个公共数据集上进行了大量实验，验证了我们框架的一贯优越性。

    Building a graph neural network (GNN)-based recommender system without violating user privacy proves challenging. Existing methods can be divided into federated GNNs and decentralized GNNs. But both methods have undesirable effects, i.e., low communication efficiency and privacy leakage. This paper proposes DGREC, a novel decentralized GNN for privacy-preserving recommendations, where users can choose to publicize their interactions. It includes three stages, i.e., graph construction, local gradient calculation, and global gradient passing. The first stage builds a local inner-item hypergraph for each user and a global inter-user graph. The second stage models user preference and calculates gradients on each local device. The third stage designs a local differential privacy mechanism named secure gradient-sharing, which proves strong privacy-preserving of users' private data. We conduct extensive experiments on three public datasets to validate the consistent superiority of our framewo
    
[^43]: 鲜度或准确性，为什么不能两者兼得？通过动态图神经网络解决延迟反馈问题

    Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks. (arXiv:2308.08071v1 [cs.LG])

    [http://arxiv.org/abs/2308.08071](http://arxiv.org/abs/2308.08071)

    本论文提出了一种通过动态图神经网络解决延迟反馈问题的方法，该方法在数据新鲜度和标签准确性之间取得了平衡。

    

    在在线商业系统中，延迟反馈问题是预测转化率面临的最紧迫挑战之一，因为用户的转化总是延迟发生。虽然新数据有益于持续训练，但是没有完整的反馈信息，即转化标签，训练算法可能会遭受大量的假负面影响。现有方法往往使用多任务学习或设计数据流程来解决延迟反馈问题。然而，这些方法在数据新鲜度和标签准确性之间存在一个折中。在本文中，我们提出了通过动态图神经网络进行延迟反馈建模 （DGDFEM）。它包括三个阶段，即准备数据流程、构建动态图以及训练CVR预测模型。在模型训练中，我们提出了一种名为HLGCN的新颖图卷积方法，它利用高通和低通滤波器来处理转化和非转化关系。所提出的方法实现了数据新鲜度和标签准确性的双重要求。

    The delayed feedback problem is one of the most pressing challenges in predicting the conversion rate since users' conversions are always delayed in online commercial systems. Although new data are beneficial for continuous training, without complete feedback information, i.e., conversion labels, training algorithms may suffer from overwhelming fake negatives. Existing methods tend to use multitask learning or design data pipelines to solve the delayed feedback problem. However, these methods have a trade-off between data freshness and label accuracy. In this paper, we propose Delayed Feedback Modeling by Dynamic Graph Neural Network (DGDFEM). It includes three stages, i.e., preparing a data pipeline, building a dynamic graph, and training a CVR prediction model. In the model training, we propose a novel graph convolutional method named HLGCN, which leverages both high-pass and low-pass filters to deal with conversion and non-conversion relationships. The proposed method achieves both 
    
[^44]: 通过一致性预言机进行简单的在线学习

    Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])

    [http://arxiv.org/abs/2308.08055](http://arxiv.org/abs/2308.08055)

    该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。

    

    我们考虑在只能通过一致性预言机访问类的模型下的在线学习——在任何时刻，预言机都能给出与目前为止看到的所有示例一致的类函数。该模型最近由Assos等人（COLT'23）考虑。这个模型的动机是标准的在线学习方法依赖于计算子类的Littlestone维度，这是一个计算复杂的问题。Assos等人在这个模型中给出了一个在线学习算法，对于Littlestone维度为d的类，最多会犯C^d个错误，其中C是一个未指定的绝对常数且大于0。我们提出了一个新的算法，最多会犯O(256^d)个错误。我们的证明更简单，只使用了Littlestone维度的基本属性。我们还观察到，不存在一个在这个模型中最多会犯2^(d+1)-2个错误的算法。我们还观察到，我们的算法（以及Assos等人的算法）。

    We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
    
[^45]: 减少后悔的无偏决策：面向银行贷款问题的对抗领域适应

    Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem. (arXiv:2308.08051v1 [cs.LG])

    [http://arxiv.org/abs/2308.08051](http://arxiv.org/abs/2308.08051)

    该论文介绍了对抗领域适应的方法来解决银行贷款问题中训练集偏差问题，旨在学习无偏但信息丰富的过去数据表示。

    

    在许多实际情景中，基于有限数据进行二元分类决策是基于近实时的，例如在审批贷款申请时。我们专注于一类具有共同特征的问题：只有在数据点被指派为正标签时才能观察到真实标签，例如，只有在我们接受贷款申请之后才能发现申请人是否违约。因此，错误拒绝会变得自我强化，并导致由模型决策不断更新的标记训练集累积偏差。先前的研究通过将乐观主义注入模型来减轻这种效应，但这会增加错误接受率的代价。我们引入对抗乐观主义（AdOpt），通过对抗性领域适应直接解决训练集中的偏差问题。AdOpt的目标是通过减少被接受数据集的分布偏移来学习过去数据的无偏但信息丰富的表示。

    In many real world settings binary classification decisions are made based on limited data in near real-time, e.g. when assessing a loan application. We focus on a class of these problems that share a common feature: the true label is only observed when a data point is assigned a positive label by the principal, e.g. we only find out whether an applicant defaults if we accepted their loan application. As a consequence, the false rejections become self-reinforcing and cause the labelled training set, that is being continuously updated by the model decisions, to accumulate bias. Prior work mitigates this effect by injecting optimism into the model, however this comes at the cost of increased false acceptance rate. We introduce adversarial optimism (AdOpt) to directly address bias in the training set using adversarial domain adaptation. The goal of AdOpt is to learn an unbiased but informative representation of past data, by reducing the distributional shift between the set of accepted da
    
[^46]: DiagGPT:一种基于LLM的任务导向对话的聊天机器人

    DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])

    [http://arxiv.org/abs/2308.08043](http://arxiv.org/abs/2308.08043)

    DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。

    

    大型语言模型(LLMs)如ChatGPT正变得越来越复杂，展示出与人类相似的能力。这些AI模型在日常生活中辅助人类完成各种任务方面发挥着重要作用。AI作为聊天代理人的重要应用是回答人类在各个领域的问题。目前的LLMs在回答一般问题方面已经显示出熟练的能力。然而，在复杂的诊断场景(如法律或医疗咨询)中，基本的问答对话往往表现不佳。这些场景通常需要任务导向对话(TOD)，其中AI聊天代理需要主动提问并引导用户完成特定任务。以前的微调模型在TOD方面表现不佳，而当前的LLMs并未固有这种能力。在本文中，我们介绍了一种名为DiagGPT (Diagnosis GPT)的创新方法，它将LLMs推广到TOD场景中。

    Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
    
[^47]: 使用代码模型和领域适应性的自动化测试用例生成

    Automated Test Case Generation Using Code Models and Domain Adaptation. (arXiv:2308.08033v1 [cs.SE])

    [http://arxiv.org/abs/2308.08033](http://arxiv.org/abs/2308.08033)

    本研究提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。

    

    最先进的自动化测试生成技术，例如基于搜索的测试，通常对开发人员创建的测试用例一无所知。因此，它们通常生成的测试用例不易阅读，并且可能无法检测所有复杂缺陷，而开发人员编写的测试用例则可以。在这项研究中，我们利用基于Transformer的代码模型生成可以补充基于搜索测试生成的单元测试。具体而言，我们使用CodeT5，即最先进的大型代码模型，并对测试生成下游任务进行微调。我们使用Methods2test数据集对CodeT5进行微调，并使用Defects4j进行项目级领域适应性和评估。本研究的主要贡献是提出了一个完全自动化的测试框架，利用开发人员编写的测试和可用的代码模型生成可编译、易读的单元测试。结果显示，我们的方法可以生成新的测试用例，覆盖了已经被测试过的代码行。

    State-of-the-art automated test generation techniques, such as search-based testing, are usually ignorant about what a developer would create as a test case. Therefore, they typically create tests that are not human-readable and may not necessarily detect all types of complex bugs developer-written tests would do. In this study, we leverage Transformer-based code models to generate unit tests that can complement search-based test generation. Specifically, we use CodeT5, i.e., a state-of-the-art large code model, and fine-tune it on the test generation downstream task. For our analysis, we use the Methods2test dataset for fine-tuning CodeT5 and Defects4j for project-level domain adaptation and evaluation. The main contribution of this study is proposing a fully automated testing framework that leverages developer-written tests and available code models to generate compilable, human-readable unit tests. Results show that our approach can generate new test cases that cover lines that were
    
[^48]: 规划学习：一种新颖的模型优化过程中的主动学习算法

    Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning. (arXiv:2308.08029v1 [cs.AI])

    [http://arxiv.org/abs/2308.08029](http://arxiv.org/abs/2308.08029)

    本论文提出了一种新颖的算法，称为SI SL，用于主动学习和模型优化过程中的规划。该算法通过与贝叶斯强化学习方案的比较证明了其性能的优越性。

    

    主动推理是一种近期的对不确定性情境下规划建模的框架。现在人们已经开始评估这种方法的优缺点以及如何改进它。最近的一个拓展-复杂模型优化算法通过递归决策树搜索在多步规划问题上提高了性能。然而，迄今为止很少有工作对比SI与其他已建立的规划算法。SI算法也主要关注推理而不是学习。本文有两个目标。首先，我们比较SI与旨在解决相似问题的贝叶斯强化学习（RL）方案的性能。其次，我们提出了SI复杂学习（SL）的拓展，该拓展在规划过程中更加充分地引入了主动学习。SL维持对未来观测下每个策略下模型参数如何变化的信念。这允许了一种反事实的回顾性评估。

    Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective in
    
[^49]: 量子经济的势能优势

    Potential Energy Advantage of Quantum Economy. (arXiv:2308.08025v1 [quant-ph])

    [http://arxiv.org/abs/2308.08025](http://arxiv.org/abs/2308.08025)

    量子计算在能源效率方面具有优势，并且能够在盈利和能源效率上超越经典计算。这使得量子计算成为计算行业更可持续的选择。

    

    随着大规模机器学习模型和语言模型的广泛部署，能源成本越来越关键。对于提供计算服务的公司来说，低能耗对于市场增长和政府法规来说都非常重要。本文研究了量子计算与经典计算之间的能源优势。我们在能源效率的背景下重新定义优势，与仅基于计算复杂性的传统量子优势不同。通过一个以能量使用为约束条件的Cournot竞争模型，我们证明量子计算公司在Nash均衡点上在盈利能力和能源效率方面都能超越经典对手。因此，量子计算可能代表计算行业更可持续的发展路径。此外，我们发现量子计算经济的能源利益取决于大规模计算。

    Energy cost is increasingly crucial in the modern computing industry with the wide deployment of large-scale machine learning models and language models. For the firms that provide computing services, low energy consumption is important both from the perspective of their own market growth and the government's regulations. In this paper, we study the energy benefits of quantum computing vis-a-vis classical computing. Deviating from the conventional notion of quantum advantage based solely on computational complexity, we redefine advantage in an energy efficiency context. Through a Cournot competition model constrained by energy usage, we demonstrate quantum computing firms can outperform classical counterparts in both profitability and energy efficiency at Nash equilibrium. Therefore quantum computing may represent a more sustainable pathway for the computing industry. Moreover, we discover that the energy benefits of quantum computing economies are contingent on large-scale computation
    
[^50]: GRINN:一种物理信息导向的神经网络，在自重存在的情况下求解流体动力学系统

    GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity. (arXiv:2308.08010v1 [cs.LG])

    [http://arxiv.org/abs/2308.08010](http://arxiv.org/abs/2308.08010)

    GRINN是一种物理信息导向的神经网络，用于在自重存在的情况下求解三维流体动力学系统。它在模拟重力不稳定性和波传播方面取得了良好的结果。

    

    建模自重气体流动对于回答天体物理学中的许多基本问题是至关重要的。这涉及众多话题，包括行星形成盘、星云形成、星系形成以及宇宙中大尺度结构的发展。然而，重力与流体动力学之间的非线性相互作用对求解所得到的三维时变偏微分方程（PDEs）提出了巨大的挑战。通过在一个无网格框架中利用神经网络的普适逼近能力，物理信息导向的神经网络（PINNs）提供了解决这一挑战的新方法。我们介绍了一种基于PINN的码，名为GRINN，用于模拟三维自重流体动力学系统。在这里，我们特别研究一个等温气体中的重力不稳定性和波传播。我们的结果在线性区域内与线性解相匹配，误差在1%以内，与传统网格码求解的结果一致。

    Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1\% in the linear regime and a conventional grid code solu
    
[^51]: APACE: AlphaFold2和先进计算作为加速生物物理学发现的服务

    APACE: AlphaFold2 and advanced computing as a service for accelerated discovery in biophysics. (arXiv:2308.07954v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.07954](http://arxiv.org/abs/2308.07954)

    APACE是一个将AlphaFold2和先进计算作为服务的计算框架，用于在现代超级计算环境中加速蛋白质结构预测分析。研究者在Delta超级计算机中部署了APACE，在准确蛋白质结构预测方面取得了显著性能提升。

    

    从氨基酸序列预测蛋白质的3D结构是生物物理学中的一个计算重大挑战，它在稳健的蛋白质结构预测算法中起着关键作用，从药物发现到基因组解读都离不开这个技术。AI模型的出现，比如AlphaFold，正在革新依赖稳健蛋白质结构预测算法的应用。为了最大程度地发挥这些新型AI工具的影响力和易用性，我们引入了APACE，AlphaFold2和先进计算作为服务的新型计算框架，该框架能够有效地处理这个AI模型及其TB级大小的数据库，以在现代超级计算环境中加速蛋白质结构预测分析。我们将APACE部署在Delta超级计算机中，并使用四个示例蛋白质（6AWO，6OAN，7MEZ和6D6U）来量化其在准确蛋白质结构预测方面的性能。在Delta的50个节点上分布了多达200个合集，相当于200个A100 NVIDIA GPU，我们发现

    The prediction of protein 3D structure from amino acid sequence is a computational grand challenge in biophysics, and plays a key role in robust protein structure prediction algorithms, from drug discovery to genome interpretation. The advent of AI models, such as AlphaFold, is revolutionizing applications that depend on robust protein structure prediction algorithms. To maximize the impact, and ease the usability, of these novel AI tools we introduce APACE, AlphaFold2 and advanced computing as a service, a novel computational framework that effectively handles this AI model and its TB-size database to conduct accelerated protein structure prediction analyses in modern supercomputing environments. We deployed APACE in the Delta supercomputer, and quantified its performance for accurate protein structure predictions using four exemplar proteins: 6AWO, 6OAN, 7MEZ, and 6D6U. Using up to 200 ensembles, distributed across 50 nodes in Delta, equivalent to 200 A100 NVIDIA GPUs, we found that 
    
[^52]: 利用对称性在物品搬运中的应用

    Leveraging Symmetries in Pick and Place. (arXiv:2308.07948v1 [cs.RO])

    [http://arxiv.org/abs/2308.07948](http://arxiv.org/abs/2308.07948)

    该论文研究了物品搬运任务中的对称性，提出了一种称为等变Transporter Net的模型，利用等变神经模型捕捉了所有对称性，能够在不同的搬运位置推广搬运知识。

    

    机器人的物品搬运任务在物品和目标放置位置的平移和旋转下具有对称性。例如，如果物品被旋转或平移，最佳搬运动作也应该旋转或平移。对于放置位置也是如此，如果目标放置位置发生变化，放置动作也应相应改变。最近提出的搬运框架Transporter Net捕捉了部分这些对称性，但不完全。本文通过分析平面机器人物品搬运中的对称性，提出了一种将等变神经模型整合到Transporter Net中以捕捉所有对称性的方法。这个新模型被称为等变Transporter Net，对于物品的搬运和放置具有等变性，可以立即将搬运知识推广到不同的搬运位置。我们通过实验证明了这个新模型的效果，并显示它比传统模型更加高效。

    Robotic pick and place tasks are symmetric under translations and rotations of both the object to be picked and the desired place pose. For example, if the pick object is rotated or translated, then the optimal pick action should also rotate or translate. The same is true for the place pose; if the desired place pose changes, then the place action should also transform accordingly. A recently proposed pick and place framework known as Transporter Net captures some of these symmetries, but not all. This paper analytically studies the symmetries present in planar robotic pick and place and proposes a method of incorporating equivariant neural models into Transporter Net in a way that captures all symmetries. The new model, which we call Equivariant Transporter Net, is equivariant to both pick and place symmetries and can immediately generalize pick and place knowledge to different pick and place poses. We evaluate the new model empirically and show that it is much more sample efficient t
    
[^53]: 基于图神经网络和规则的归纳知识图谱补全的分析

    Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis. (arXiv:2308.07942v1 [cs.AI])

    [http://arxiv.org/abs/2308.07942](http://arxiv.org/abs/2308.07942)

    基于图神经网络和规则的归纳知识图谱补全研究了基于规则的方法在实践中的表现不佳的原因，发现不合理的实体没有排名和只考虑最具信息量的路径是影响因素。提出了一些解决这些问题的规则方法的变体，发现其性能接近于基于图神经网络的方法NBFNet。这些变体仅使用了NBFNet所依赖的证据的一小部分。

    

    归纳知识图谱补全的任务要求模型从训练图谱中学习推理模式，然后可以用来在分离的测试图谱上进行预测。虽然基于规则的方法似乎很适合这个任务，但在实践中，它们的表现明显不如基于图神经网络（GNNs）的最先进方法，如NBFNet。我们假设基于规则的方法表现不佳是由于两个因素：（i）不合理的实体根本没有排名，（ii）在确定给定链接预测答案的置信度时，只考虑了最具信息量的路径。为了分析这些因素的影响，我们研究了一些针对上述问题的规则方法的变体。我们发现，所得到的模型的性能接近NBFNet。至关重要的是，考虑到的变体只使用了NBFNet所依赖的证据的一小部分。

    The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which m
    
[^54]: 从头开始对编码的时空数据进行训练的使用GPT-2生成个体轨迹

    Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data. (arXiv:2308.07940v1 [cs.LG])

    [http://arxiv.org/abs/2308.07940](http://arxiv.org/abs/2308.07940)

    该论文尝试通过使用GPT-2模型从头开始训练编码的时空数据，生成受环境因素和个体属性影响的个体轨迹。

    

    在此研究中，我们使用Mizuno、Fujimoto和Ishikawa的方法将地理坐标转换为具有不同空间尺度位置特征的独特位置符号。我们使用独特的时间间隔符号将位置符号组合成个体每日轨迹的序列。通过使用自回归语言模型GPT-2的架构，我们从头开始训练这个符号序列，从而构建一个能够顺序生成个体每日轨迹的深度学习模型。通过使用特殊符号表示气象条件和个体属性，比如性别和年龄，并在GPT-2架构上训练这些符号和轨迹，我们可以生成同时受环境因素和个体属性影响的轨迹。

    Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
    
[^55]: Ada-QPacknet -- 自适应剪枝与位宽缩减作为一种高效的继续学习方法，不会遗忘的算法

    Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting. (arXiv:2308.07939v1 [cs.LG])

    [http://arxiv.org/abs/2308.07939](http://arxiv.org/abs/2308.07939)

    Ada-QPacknet是一种自适应剪枝与位宽缩减的高效继续学习方法，通过剪枝和量化技术生成任务子网络，在动态和复杂环境中实现了与浮点数子网络相似的准确性。

    

    继续学习（CL）是一个过程，其中人类和深度学习模型之间的效率仍存在巨大差距。最近设计了许多CL算法，大部分都存在在动态和复杂环境中学习的问题。本文描述了一种基于新架构的方法Ada-QPacknet。它通过剪枝提取每个任务的子网络。基于架构的CL方法的关键是容量。在提出的方法中，通过高效的线性和非线性量化方法减小了模型的规模。该方法减小了权重格式的位宽。实验结果显示，混合8位和4位量化在著名的CL场景上实现了与浮点数子网络相似的准确性。据我们所知，这是第一个将剪枝和量化这两种压缩技术应用于生成任务子网络的CL策略。该算法在著名的情节组合上进行了测试。

    Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combination
    
[^56]: 用ChatGPT变革金融领域的情绪分析

    Transforming Sentiment Analysis in the Financial Domain with ChatGPT. (arXiv:2308.07935v1 [cs.CL])

    [http://arxiv.org/abs/2308.07935](http://arxiv.org/abs/2308.07935)

    本研究使用ChatGPT 3.5来进行金融情绪分析，特别关注外汇市场，通过零-shot提示方法，在精心策划的数据集上评估了其性能，并发现与传统模型相比，ChatGPT在金融情绪分析中表现出约35％的性能提升。

    

    金融情绪分析在解读市场趋势和指导战略交易决策中起着关键作用。尽管已经使用了先进的深度学习技术和语言模型来改进金融情绪分析，但本研究通过探索大型语言模型（特别是ChatGPT 3.5）在金融情绪分析中的潜力，特别强调外汇市场（forex），开创了新的领域。采用零-shot提示方法，在一份经过精心策划的外汇相关新闻标题数据集上检验多个ChatGPT提示，并使用精确度、召回率、F1得分和情绪分类的平均绝对误差（MAE）等指标评估性能。此外，我们还探讨了预测情绪和市场回报之间的相关性作为一种额外的评估方法。与FinBERT相比，ChatGPT在情绪分析方面的性能提高了约35％。

    Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an additional evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\% enhanced performance in sentiment 
    
[^57]: 只需一次位翻转：当位翻转攻击遇到模型训练

    One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training. (arXiv:2308.07934v1 [cs.CR])

    [http://arxiv.org/abs/2308.07934](http://arxiv.org/abs/2308.07934)

    本论文介绍了一种位翻转攻击和模型训练相结合的方法，通过在训练阶段引入对手构建高风险模型，在只进行少量位翻转的情况下，将正常模型转化为恶意模型。实验结果表明，这种攻击方法可以逃避各种检测方法。

    

    深度神经网络广泛部署在现实世界的设备上。对其安全性的关注引起了研究者的极大关注。最近提出了一种新的权重修改攻击称为位翻转攻击（BFA），该攻击利用内存故障注入技术，如行锤击，来攻击部署阶段的量化模型。仅通过少量的位翻转，目标模型可以被渲染为无用的随机猜测者，甚至可以植入恶意功能。在这项工作中，我们试图进一步降低位翻转的数量。我们提出了一种训练辅助的位翻转攻击，在其中，对手参与到训练阶段中，建立一个高风险的释放模型。这个高风险模型与相应的恶意模型结合，表现正常，并且可以逃避各种检测方法。在基准数据集上的实验结果表明，攻击者可以轻松地将这个高风险但正常的模型转化为受害者这边的恶意模型。

    Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipp
    
[^58]: 精简特征场使得语言引导的少样本操作成为可能

    Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])

    [http://arxiv.org/abs/2308.07931](http://arxiv.org/abs/2308.07931)

    本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。

    

    自监督和语言监督的图像模型包含了世界的丰富知识，对于泛化很重要。然而，许多机器人任务需要对 3D 几何的详细理解，这在 2D 图像特征中往往缺乏。本研究通过利用精简特征场，将精确的 3D 几何与 2D 基础模型的丰富语义相结合，来弥合机器人操作中的 2D 到 3D 的差距。我们提出一种针对 6 自由度抓取和放置的少样本学习方法，利用这些强大的空间和语义先验，实现对未见过的物体的自然泛化。通过从视觉语言模型 CLIP 中精简的特征，我们展示了一种通过自由文本自然语言指定新颖对象进行操作的方式，并展示了它在未见过的表达和新颖类别的物体上的泛化能力。

    Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
    
[^59]: "月经周期长度的预测建模：一种时间序列预测方法"

    Predictive Modeling of Menstrual Cycle Length: A Time Series Forecasting Approach. (arXiv:2308.07927v1 [cs.LG])

    [http://arxiv.org/abs/2308.07927](http://arxiv.org/abs/2308.07927)

    本研究通过使用机器学习技术，探索了预测月经周期的方法，结果表明可以准确预测月经周期的开始和持续时间。

    

    正确预测月经周期对女性健康至关重要，因为它可以让个体采取预防措施来减少与周期相关的不适。此外，精确的预测对于规划女性生活中的重要事件，如计划生育，也是有用的。本研究探索了使用机器学习技术来预测规律和不规律月经周期的方法。我们实现了一些时间序列预测算法，如自回归综合移动平均、Huber回归、Lasso回归、正交匹配追踪和长短期记忆网络。此外，我们还生成了合成数据来实现我们的目的。结果表明，使用机器学习技术可以准确预测月经周期的开始和持续时间。

    A proper forecast of the menstrual cycle is meaningful for women's health, as it allows individuals to take preventive actions to minimize cycle-associated discomforts. In addition, precise prediction can be useful for planning important events in a woman's life, such as family planning. In this work, we explored the use of machine learning techniques to predict regular and irregular menstrual cycles. We implemented some time series forecasting algorithm approaches, such as AutoRegressive Integrated Moving Average, Huber Regression, Lasso Regression, Orthogonal Matching Pursuit, and Long Short-Term Memory Network. Moreover, we generated synthetic data to achieve our purposes. The results showed that it is possible to accurately predict the onset and duration of menstrual cycles using machine learning techniques.
    
[^60]: DiffGuard：使用预训练扩散模型进行语义不匹配引导的带外分布检测

    DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])

    [http://arxiv.org/abs/2308.07687](http://arxiv.org/abs/2308.07687)

    本论文提出了一种名为DiffGuard的方法，使用预训练的扩散模型进行语义不匹配引导的带外分布检测。实验证明，DiffGuard在小规模数据集上表现出色，但在ImageNet规模的数据集上无法应用。

    

    本论文针对语义带外（OOD）样本与合法类别内容在语义上的不匹配特征，提出了一种语义不匹配引导的带外分布检测方法DiffGuard。与其他方法相比，DiffGuard直接使用预训练的扩散模型进行语义不匹配引导，相较于条件生成对抗网络，扩散模型更易于训练且适用于各种条件。实验结果表明，在小规模数据集上，DiffGuard取得了显著的带外分布检测性能。然而，由于使用图像和标签作为条件训练条件生成对抗网络的困难性，DiffGuard在ImageNet规模的数据集上无法应用。

    Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and t
    
[^61]: 使用时间图神经网络的交互感知个性化车辆轨迹预测

    Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])

    [http://arxiv.org/abs/2308.07439](http://arxiv.org/abs/2308.07439)

    本研究提出了一种交互感知的个性化车辆轨迹预测方法，利用时间图神经网络来建模目标车辆与周围交通之间的时空交互，并通过迁移学习来个性化预测。实验结果表明，该方法能够更准确地预测车辆的轨迹。

    

    准确预测车辆轨迹对于先进驾驶辅助系统和自动驾驶汽车至关重要。现有方法主要依赖于从大型数据集中推导出的通用轨迹预测，忽视了个别驾驶员的个性化驾驶模式。为了弥补这一差距，我们提出了一种交互感知的个性化车辆轨迹预测方法，该方法采用了时间图神经网络。我们的方法利用图卷积神经网络（GCN）和长短期记忆（LSTM）来建模目标车辆与周围交通之间的时空交互。为了个性化预测，我们建立了一个利用迁移学习的流程：模型首先在大规模轨迹数据集上进行预训练，然后使用特定驾驶数据针对每个驾驶员进行微调。我们采用人机协同仿真来收集个性化的自然驾驶轨迹及其相应的周围车辆轨迹。

    Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories
    
[^62]: 使用基于聚类的树状Parzen估计的敏感性感知混合精度量化和宽度优化的深度神经网络

    Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])

    [http://arxiv.org/abs/2308.06422](http://arxiv.org/abs/2308.06422)

    本研究引入了一种创新的深度神经网络优化方法，通过自动选择最佳的位宽和层宽来提高网络效率。同时，通过剪枝和聚类技术，优化了搜索过程，并在多个数据集上进行了严格测试，结果显示该方法明显优于现有方法。

    

    随着深度学习模型的复杂性和计算需求的提高，对神经网络设计的有效优化方法的需求变得至关重要。本文引入了一种创新的搜索机制，用于自动选择单个神经网络层的最佳位宽和层宽。这导致深度神经网络效率的明显提高。通过利用基于Hessian的剪枝策略，有选择地减少搜索域，确保移除非关键参数。随后，我们通过使用基于聚类的树状Parzen估计器开发有利和不利结果的替代模型。这种策略允许对架构可能性进行简化的探索，并迅速确定表现最好的设计。通过对知名数据集进行严格测试，我们的方法证明了与现有方法相比的明显优势。与领先的压缩策略相比，我们的方法取得了令人瞩目的成果。

    As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
    
[^63]: 超越语义：利用自我监督学习的行为增强相关模型的学习

    Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning. (arXiv:2308.05379v1 [cs.IR])

    [http://arxiv.org/abs/2308.05379](http://arxiv.org/abs/2308.05379)

    这篇论文提出了一种行为增强的相关模型，利用自我监督学习，通过从用户历史行为数据中提取辅助查询-项目交互，来改进搜索引擎中的查询-项目匹配，提高准确性和鲁棒性。

    

    相关建模旨在定位与对应查询相关的理想项目，这对于搜索引擎确保用户体验非常重要。虽然大多数传统方法通过评估查询与项目之间的语义相似性来解决这个问题，但纯语义匹配并不是唯一的方法。实际上，从用户搜索记录的历史行为数据中提取的辅助查询-项目交互可以提供进一步揭示用户搜索意图的线索。得益于此，我们设计了一种新颖的基于行为增强相关学习模型的支付宝搜索模型（BARL-ASe），该模型利用目标项目的相邻查询和目标查询的相邻项目来补充目标查询-项目的语义匹配。具体而言，我们的模型建立了多层共同注意力，从相邻和目标视图中提取了粗粒度和细粒度的语义表示。模型随后采用邻居-目标的自我监督学习来提高精度和鲁棒性。

    Relevance modeling aims to locate desirable items for corresponding queries, which is crucial for search engines to ensure user experience. Although most conventional approaches address this problem by assessing the semantic similarity between the query and item, pure semantic matching is not everything. In reality, auxiliary query-item interactions extracted from user historical behavior data of the search log could provide hints to reveal users' search intents further. Drawing inspiration from this, we devise a novel Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that leverages neighbor queries of target item and neighbor items of target query to complement target query-item semantic matching. Specifically, our model builds multi-level co-attention for distilling coarse-grained and fine-grained semantic representations from both neighbor and target views. The model subsequently employs neighbor-target self-supervised learning to improve the accuracy and robu
    
[^64]: 元认知提示改善大型语言模型的理解能力

    Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])

    [http://arxiv.org/abs/2308.05342](http://arxiv.org/abs/2308.05342)

    元认知提示 (MP) 是一种改进大型语言模型 (LLMs) 理解能力的策略。实验结果表明，使用MP的PaLM在各种自然语言理解任务中接近于GPT-4的性能水平。

    

    在大型语言模型 (LLMs) 中，通过有效的提示设计，任务特定性能一直在不断提高。尽管最近关于提示的研究增强了LLMs的推理能力，但在进一步提高它们的理解能力方面仍存在差距。在本研究中，我们介绍了元认知提示 (MP)，这是一种受人类内省推理过程启发的策略。使用MP，LLMs经历一系列有结构、自我意识的评估，利用其丰富的内在知识和新的见解。我们的实验涉及五个常见的LLMs：Llama2、Vicuna、PaLM、GPT-3.5和GPT-4，它们都涵盖了来自GLUE和SuperGLUE基准测试的各种通用自然语言理解 (NLU) 任务。结果表明，虽然GPT-4在大多数任务中始终表现出色，但配备MP的PaLM接近其性能水平。此外，跨模型和数据集，MP始终优于现有的提示方法。

    In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
    
[^65]: SSL-Auth:一种用于自监督学习中预训练的编码器的易碎水印身份验证框架

    SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning. (arXiv:2308.04673v1 [cs.CR])

    [http://arxiv.org/abs/2308.04673](http://arxiv.org/abs/2308.04673)

    本文提出了SSL-Auth，这是一种用于自监督学习中预训练的编码器的易碎水印身份验证框架。该方法利用选择的关键样本作为水印信息，并训练一个验证网络来重构水印信息，从而进行验证。

    

    自监督学习（SSL）利用无标签数据集为预训练的强大编码器取得了显著的成功。这些编码器常被用作各种下游任务的特征提取器，其训练过程需要大量的数据和计算资源。随着预训练编码器在商业应用中的部署，保护模型所有者的知识产权并确保模型的可信性变得至关重要。最近的研究表明，编码器受到后门攻击、对抗攻击等威胁。因此，需要一种验证预训练编码器完整性的方案来保护用户。在本文中，我们提出了SSL-Auth，这是一种无损害模型性能的易碎水印身份验证方案。我们的方法利用选择的关键样本作为水印信息，并训练一个验证网络来重构水印信息，从而进行验证。

    Self-supervised learning (SSL) which leverages unlabeled datasets for pre-training powerful encoders has achieved significant success in recent years. These encoders are commonly used as feature extractors for various downstream tasks, requiring substantial data and computing resources for their training process. With the deployment of pre-trained encoders in commercial use, protecting the intellectual property of model owners and ensuring the trustworthiness of the models becomes crucial. Recent research has shown that encoders are threatened by backdoor attacks, adversarial attacks, etc. Therefore, a scheme to verify the integrity of pre-trained encoders is needed to protect users. In this paper, we propose SSL-Auth, the first fragile watermarking scheme for verifying the integrity of encoders without compromising model performance. Our method utilizes selected key samples as watermark information and trains a verification network to reconstruct the watermark information, thereby ver
    
[^66]: 基于物理交互因果序列的任务生成方法

    Physics-Based Task Generation Through Causal Sequence of Physical Interactions. (arXiv:2308.02835v1 [cs.AI])

    [http://arxiv.org/abs/2308.02835](http://arxiv.org/abs/2308.02835)

    本文提出了一种基于物理交互因果序列的任务生成方法，使得人工智能系统能够在模拟物理环境中执行任务并评估其物理推理能力。我们使用“愤怒的小鸟”游戏作为示例，通过一系列指标对生成的任务进行评估。

    

    在真实世界中，对于使用人工智能系统来执行物理环境中的任务是一个至关重要却具有挑战性的问题。物理模拟任务通常被用来促进解决这一挑战的研究。本文首先提出了一种系统的方法，使用物体之间的物理交互的因果序列来定义物理场景。然后，我们提出了一种利用这些定义的场景作为输入，在模拟物理环境中生成任务的方法学。我们的方法可以更好地理解解决基于物理的任务所需的微观力学，从而为评估人工智能系统的物理推理能力提供准确的评估。我们使用基于物理的益智游戏“愤怒的小鸟”来演示我们提出的任务生成方法，并使用一系列指标对生成的任务进行评估，包括物理稳定性、使用预期物理相互作用的可解性以及使用意外相互作用的偶然可解性。

    Performing tasks in a physical environment is a crucial yet challenging problem for AI systems operating in the real world. Physics simulation-based tasks are often employed to facilitate research that addresses this challenge. In this paper, first, we present a systematic approach for defining a physical scenario using a causal sequence of physical interactions between objects. Then, we propose a methodology for generating tasks in a physics-simulating environment using these defined scenarios as inputs. Our approach enables a better understanding of the granular mechanics required for solving physics-based tasks, thereby facilitating accurate evaluation of AI systems' physical reasoning capabilities. We demonstrate our proposed task generation methodology using the physics-based puzzle game Angry Birds and evaluate the generated tasks using a range of metrics, including physical stability, solvability using intended physical interactions, and accidental solvability using unintended s
    
[^67]: 窥视大脑：通过人脑信号重建视觉知觉的图像

    Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals. (arXiv:2308.02510v1 [eess.IV])

    [http://arxiv.org/abs/2308.02510](http://arxiv.org/abs/2308.02510)

    本文通过重建观察到的图像来揭示人脑对视觉刺激的知觉机制，并提出了一种名为NeuroImagen的综合管道，利用脑电图信号重建视觉刺激图像。

    

    见到就信，然而人类视觉知觉与认知的基本机制仍是一个谜。由于神经科学和人工智能的最近进展，我们能够记录到视觉诱发的脑活动，并通过计算方法模拟视觉感知能力。本文关注通过重建观察到的图像来重建视觉刺激，基于易于获得的脑信号，即脑电图（EEG）数据。由于脑电图信号是动态的时间序列格式，同时也因有噪音而臭名昭著，处理和提取有用信息需要更多的专门工作；在本文中，我们提出了一个综合管道，名为NeuroImagen，用于从脑电图信号中重建视觉刺激图像。具体来说，我们结合了新颖的多层感知信息解码方法，以从给定的脑电图数据中获取多粒度的输出。

    Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion 
    
[^68]: PeRP：通过合作咨询系统实现个性化剩余策略以缓解拥堵

    PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])

    [http://arxiv.org/abs/2308.00864](http://arxiv.org/abs/2308.00864)

    本论文提出了一种基于个性化剩余策略的合作咨询系统PeRP，用于缓解拥堵。该系统通过结构化建模人类驾驶的相似性，并根据驾驶员的特征为其提供行动建议，以减少交通拥堵。

    

    智能驾驶系统可以通过简单的行动来缓解拥堵，从而改善通勤时间和燃油成本等众多社会经济因素。然而，这些系统假设对自动驾驶车队具有精确的控制，因此在实际中存在限制，因为它们未能考虑到人类行为的不确定性。分段常数（PC）策略通过结构建模人类驾驶的相似性来减少交通拥堵，以提供给人类驾驶员遵循的行动建议。然而，PC策略假设所有驾驶员行为相似。为了实现这一目标，我们开发了一个基于PC策略的合作咨询系统，其中包含一种新型的驾驶员特征相关的个性化剩余策略，即PeRP。PeRP建议驾驶员以减少交通拥堵的方式行驶。我们首先使用变分自动编码器无监督地推断驾驶员如何遵循指令的内在特征。然后，通过将策略与驾驶员特征条件化，实现个性化的行动建议。

    Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned o
    
[^69]: 分布式动态规划和分布式TD-Learning的网络多智能体马尔可夫决策过程的ODE框架

    Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2307.16706](http://arxiv.org/abs/2307.16706)

    本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。

    

    本文主要研究网络多智能体马尔可夫决策问题中的分布式动态规划（DP）和分布式时序差分（TD）学习算法。我们采用分布式多智能体框架，其中各个智能体只能访问自己的奖励，缺乏对其他智能体奖励的了解。此外，每个智能体都能通过一个由图表示的通信网络与相邻智能体共享其参数。我们的贡献可以总结为两个关键点：1）我们引入了一个新的受连续时间区间内的平均一致性方法启发的分布式DP。通过控制理论的视角评估了该DP的收敛性。2）基于上述DP，我们设计了一个新的分布式TD学习算法并证明了其收敛性。我们提出的分布式DP的一个显著特点是其包含了两个独立的动态系统。

    The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
    
[^70]: EnrichEvent: 使用上下文信息为新出现的事件提供丰富的社交数据

    EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])

    [http://arxiv.org/abs/2307.16082](http://arxiv.org/abs/2307.16082)

    本文提出了一个利用词汇、语义和上下文表示的框架，旨在解决现有事件检测方法在识别新兴社交事件方面的局限性，并提供了对社交数据进行丰富的上下文化处理的方法。

    

    社交平台已成为传播和讨论真实事件信息的关键平台，为及早发现有新闻价值的事件提供了良好的机会。然而，现有的大多数事件检测方法仅利用关键词突发性或网络结构来检测热点事件。因此，对于事件和社交数据的复杂性而言，它们往往无法在达到趋势状态之前识别出新出现的社交事件。社交数据，例如推文，具有拼写错误、不完整性、歧义性和语言不规范性，以及意见方面的变化。此外，利用有限的上下文知识来学习事件的演变特征对于机器学习模型几乎是不可行的。为了解决这些问题，本文提出了一个利用流式社交数据的词汇、语义和上下文表示的框架。

    Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
    
[^71]: LLM-Rec: 通过引导大型语言模型进行个性化推荐

    LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])

    [http://arxiv.org/abs/2307.15780](http://arxiv.org/abs/2307.15780)

    本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。

    

    本文通过输入增强技术，研究了多种不同的引导策略，以提高大型语言模型（LLM）在个性化内容推荐方面的性能。我们提出的方法名为LLM-Rec，包括四种不同的引导策略：（1）基础引导，（2）推荐驱动引导，（3）参与引导引导，和（4）推荐驱动+参与引导引导。实验证明，将原始内容描述与LLM生成的增强输入文本结合起来，采用这些引导策略可以提高推荐性能。这一发现强调了在个性化内容推荐中，通过引入多样的引导和输入增强技术来提升大型语言模型的推荐能力的重要性。

    We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
    
[^72]: 决策导向学习：基础、现状、基准和未来机会

    Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])

    [http://arxiv.org/abs/2307.13565](http://arxiv.org/abs/2307.13565)

    决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。

    

    决策导向学习（DFL）是一种新兴的机器学习范式，它训练模型以优化决策，在一个端到端的系统中集成了预测和优化。这个范式有望在许多实际应用中革命性地改变决策制定，这些应用在不确定性下运作，在这些决策模型中估计未知参数经常成为一个重要障碍。本文对DFL进行了全面的回顾。它对各种技术进行了深入分析，以整合机器学习和优化模型，引入了一种根据其独特特征来区分DFL方法的分类法，并对这些方法进行了广泛的实证评估，提出了适用于DFL的合适基准数据集和任务。最后，本研究提供了关于DFL研究中当前和潜在未来方向的宝贵见解。

    Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
    
[^73]: 面向教育中人工智能协作混合论文的自动边界检测

    Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12267](http://arxiv.org/abs/2307.12267)

    本研究探索了在教育领域中，由人类和生成性语言模型协作编写的混合文本的AI内容检测方法，将其形式化为识别转换点的任务，以区分人类编写和AI生成的部分。

    

    最近的大型语言模型（如ChatGPT）能够在提供具体指导的情况下生成类似于人类的流畅回答。尽管承认技术进步带来的便利，教育者也担心学生可能利用语言模型来完成写作任务并将其假冒为自己的原创作品。虽然有很多AI内容检测研究是基于这些担忧进行的，但大多数之前的研究将AI内容检测建模为一个分类问题，假设一个文本要么完全由人类编写，要么完全由AI生成。在本研究中，我们研究了AI内容检测在一个少有探索但却现实的情况下，即检测的文本由人类和生成性语言模型（即混合文本）协作编写。我们首先将检测任务形式化为从给定的混合文本中识别人类编写内容和AI生成内容之间的转换点（边界检测）。

    The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
    
[^74]: LLM认知判断与人类有所不同

    LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])

    [http://arxiv.org/abs/2307.11787](http://arxiv.org/abs/2307.11787)

    这项研究调查了大型语言模型在认知任务中的表现，并发现它们的认知判断与人类不同。

    

    最近，大型语言模型(LLMs)成为研究人员、企业和消费者关注的焦点。虽然这类模型的语言能力已经得到了广泛的研究，但对它们作为认知主体的调查越来越受关注。在本研究中，我对GPT-3和ChatGPT在一个来自认知科学文献的有限数据归纳推理任务上的能力进行了研究。结果表明，这些模型的认知判断与人类不同。

    Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
    
[^75]: 相关实体选择：通过零样本类比修剪进行知识图谱引导

    Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning. (arXiv:2306.16296v1 [cs.AI])

    [http://arxiv.org/abs/2306.16296](http://arxiv.org/abs/2306.16296)

    本文提出了一种基于类比的方法，通过选择和修剪相关实体，从而引导知识图谱的构建。实证结果显示，这种方法在两个领域和异质种子实体的数据集上优于其他机器学习方法，并具有较低的参数数量。这些结果支持在相关任务中进一步应用类比推理。

    

    知识图谱构建可以被视为一个迭代过程，从高质量的核心开始，通过知识提取方法不断改进。这样的核心可以从像Wikidata这样的开放式知识图谱中获得。然而，由于这种通用知识图谱的规模，将其作为整体集成可能会包含无关内容和可扩展性问题。我们提出了一种基于类比的方法，从通用知识图谱中的感兴趣种子实体开始，并保留或修剪其相邻实体。我们通过两个手动标记的数据集 在Wikidata上评估了我们的方法，这些数据集包含领域同质或异质的种子实体。我们从实证上证明了我们的基于类比的方法优于LSTM，随机森林，支持向量机和多层感知器，且参数数量大大减少。我们还在迁移学习环境中评估了其泛化能力。这些结果对于进一步将基于类比的推理集成到相关任务中提供了支持。

    Knowledge Graph Construction (KGC) can be seen as an iterative process starting from a high quality nucleus that is refined by knowledge extraction approaches in a virtuous loop. Such a nucleus can be obtained from knowledge existing in an open KG like Wikidata. However, due to the size of such generic KGs, integrating them as a whole may entail irrelevant content and scalability issues. We propose an analogy-based approach that starts from seed entities of interest in a generic KG, and keeps or prunes their neighboring entities. We evaluate our approach on Wikidata through two manually labeled datasets that contain either domain-homogeneous or -heterogeneous seed entities. We empirically show that our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters. We also evaluate its generalization potential in a transfer learning setting. These results advocate for the further integration of analogy-based inference in tasks relate
    
[^76]: Instruct-NeuralTalker: 利用指令编辑音频驱动的对话辐射场

    Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions. (arXiv:2306.10813v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10813](http://arxiv.org/abs/2306.10813)

    本文提出了Instruct-NeuralTalker，一种利用指令编辑音频驱动的对话辐射场的交互式框架。该方法可以实现实时个性化的对话面部生成，并在编辑过程中保持音频唇同步。此外，还引入了轻量级的细化网络来实现可控的细节生成，并且在消费级硬件上可以达到最高30FPS的实时渲染。

    

    最近的神经对话辐射场方法在逼真的音频驱动的对话面部合成方面取得了巨大成功。在本文中，我们提出了一种新颖的交互式框架，利用人类指令来编辑这种隐式神经表示，实现实时个性化的对话面部生成。给定一个短视频，我们首先构建一个高效的对话辐射场，然后根据给定的指令和引导的隐式表示优化，应用最新的条件扩散模型进行图像编辑，以实现编辑目标。为了确保编辑过程中的音频唇同步，我们提出了一种迭代数据集更新策略，并利用唇沿损失约束唇部区域的变化。我们还引入了一个轻量级的细化网络，用于补充图像细节，并在最终渲染图像中实现可控的细节生成。我们的方法还能在消费级硬件上实现最高30FPS的实时渲染。

    Recent neural talking radiance field methods have shown great success in photorealistic audio-driven talking face synthesis. In this paper, we propose a novel interactive framework that utilizes human instructions to edit such implicit neural representations to achieve real-time personalized talking face generation. Given a short speech video, we first build an efficient talking radiance field, and then apply the latest conditional diffusion model for image editing based on the given instructions and guiding implicit representation optimization towards the editing target. To ensure audio-lip synchronization during the editing process, we propose an iterative dataset updating strategy and utilize a lip-edge loss to constrain changes in the lip region. We also introduce a lightweight refinement network for complementing image details and achieving controllable detail generation in the final rendered image. Our method also enables real-time rendering at up to 30FPS on consumer hardware. M
    
[^77]: 基于任务条件化超网络的多任务记忆深度强化学习

    Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10698](http://arxiv.org/abs/2306.10698)

    人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。

    

    深度强化学习算法通常受到采样效率低下的限制，严重依赖与环境的多次交互才能获得准确的决策能力。相比之下，人类似乎依赖海马体从过去有关任务的经历中检索相关信息，在学习新任务时指导其决策，而不是仅仅依赖于环境交互。然而，为代理设计类似海马体的模块以将过去的经历融入既有的强化学习算法面临两个挑战。第一个挑战涉及选择当前任务最相关的过去经验，第二个是将这些经验与决策网络相结合。为了解决这些问题，我们提出了一种新算法，利用基于任务条件化超网络的检索网络，根据任务调整检索网络的参数。

    Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
    
[^78]: Gode -- 将生物化学知识图谱集成到分子图神经网络的预训练中

    Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])

    [http://arxiv.org/abs/2306.01631](http://arxiv.org/abs/2306.01631)

    本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。

    

    分子属性的准确预测对于促进创新治疗方法的发展和理解化学物质和生物系统之间复杂的相互作用至关重要。本研究提出了一种新的方法，将单个分子结构的图表示与生物医学知识图谱 (KG) 的多个领域信息进行集成。通过集成两个级别的信息，我们可以使用自我监督策略预先训练更广泛和更强大的表示，用于分子级和 KG 级预测任务。在性能评估方面，我们在 11 个具有挑战性的化学属性预测任务上微调我们预先训练的模型。我们的框架的结果表明，我们微调的模型优于现有的最先进的模型。

    The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
    
[^79]: LLMatic: 基于大语言模型和多样性优化的神经结构搜索

    LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])

    [http://arxiv.org/abs/2306.01102](http://arxiv.org/abs/2306.01102)

    本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    

    大型语言模型 (LLMs) 已成为一种强大的工具，可以完成广泛的任务。它们的能力涵盖了许多领域，它们在代码生成领域产生了重大影响。在此情况下，我们将 LLMs 视为变异和交叉工具。同时，多样性优化算法已知可以发现多样性和稳健的解决方案。通过将 LLMs 的代码生成能力与 QD 解决方案的多样性和鲁棒性相结合，我们引入了 LLMatic，一个神经结构搜索 (NAS) 算法。虽然 LLMs 通过提示直接进行 NAS 考验困难，但 LLMatic 利用程序化方法，利用 QD 来进行提示和网络结构，从而创建多样性和高性能网络。我们在 CIFAR-10 图像分类基准测试中测试了 LLMatic，证明它可以在仅进行 2000 次搜索的情况下产生具有竞争力的网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
    
[^80]: 超越元数据：利用游戏设计参数进行跨版本电子竞技分析

    Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])

    [http://arxiv.org/abs/2305.18477](http://arxiv.org/abs/2305.18477)

    本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。

    

    电子竞技游戏是全球游戏市场的重要组成部分，并且是增长最快的游戏细分领域。这导致了电子竞技分析的领域产生，其使用游戏提取的遥测数据来为玩家、教练、播音员和其他利益相关者提供信息。与传统的体育比赛相比，电子竞技游戏的机制和规则经常发生快速变化。由于游戏参数的频繁更改，电子竞技分析模型的使用寿命可能很短，这在文献中很大程度上被忽略了。本文提取游戏设计信息（即补丁说明），利用聚类技术提出了一种新的角色表征形式。以Dota 2游戏中击杀次数的预测为案例，利用这种创新的角色表征技术训练了一个神经网络模型。然后将此模型的性能与包括常规技术在内的两个不同基线进行了评估。这个模型不仅达到了显著的表现水平，还克服了电子竞技游戏中版本更迭的困境。

    Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
    
[^81]: 激发Web规模语音模型的潜在能力以实现零-shot任务泛化

    Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])

    [http://arxiv.org/abs/2305.11095](http://arxiv.org/abs/2305.11095)

    本文通过提示工程技术调整Whisper模型，成功适应未见过的三个任务，并提出的提示比默认提示性能提升了10%到45％，展现了Whisper模型的鲁棒性和多语言理解能力。

    

    本文研究了最近提出的Web规模语音模型Whisper的新兴功能，在使用提示工程技术调整模型后，适应了未见过的AVSR，CS-ASR和ST三个任务。我们设计了特定于任务的提示，要么利用另一个大规模模型，要么简单地操作默认提示中的特殊标记。实验证明，与默认提示相比，我们提出的提示使这三个零-shot任务的性能提高了10%到45％，甚至在一些数据集上超过了SotA监督模型。此外，我们的实验揭示了Whisper的许多有趣属性，包括其提示的鲁棒性，对口音的偏好以及潜在空间中的多语言理解。代码可在https://github.com/jasonppy/PromptingWhisper上找到。

    We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
    
[^82]: Echoes: 基于伪偏差标记的模仿式回声室无监督去偏

    Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber. (arXiv:2305.04043v1 [cs.LG])

    [http://arxiv.org/abs/2305.04043](http://arxiv.org/abs/2305.04043)

    Echoes提出了一种无监督的去偏方法，生成偏差对立样本的伪偏差标签，实现了对数据集中偏差特征的一致性处理，并取得了各项任务和数据集上的最先进性能。

    

    当神经网络暴露于有偏训练数据时，通常会学习到不正确的相关性，从而导致在拓展领域数据上表现不佳。本文提出一种名为“Echoes”的简单高效方法，它生成偏差对立样本的伪偏差标签，以强制使伪标签与数据集中的偏差特征一致，并用于去偏。我们的实证研究表明，Echoes实现了各项任务和数据集上的最先进性能，同时使用比以前的方法更少的计算资源。

    Neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. A biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). Recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. Following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. This paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. To address this issue, we propose a straightforward and effective method cal
    
[^83]: 一种可扩展的序列转移优化问题生成器

    A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])

    [http://arxiv.org/abs/2304.08503](http://arxiv.org/abs/2304.08503)

    STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。

    

    近年来，序列转移优化(STO)受到越来越多的研究关注，旨在利用储存在数据库中以前求解的优化任务的知识来提高优化性能。然而，尽管算法设计已有重大进展，但STO中的测试问题设计并不完善。它们往往是由其他基准函数随机组合而成，这些基准函数具有相同的最佳值，或者生成自表现出有限变化的实际问题。这些问题中源任务和目标任务的最优解之间的关系是手动配置的，因此单调，限制了它们表征真实问题多样化关系的能力。因此，许多算法在这些问题上取得的有前途的结果具有高度的偏见，并且难以推广到其他问题。鉴于此，我们首先引入了一些表征STO问题的基本概念。

    Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
    
[^84]: 没有正确性的可重复性并不重要：在NLP领域中测试代码的重要性。

    Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])

    [http://arxiv.org/abs/2303.16166](http://arxiv.org/abs/2303.16166)

    在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。

    

    尽管其在研究实验中发挥了关键作用，但代码正确性往往仅基于结果的感知质量而被假定。这带来了错误结果和潜在误导性发现的风险。为了解决这个问题，我们认为当前关注结果重现应该与强调编码最佳实践相辅相成。我们通过一个案例研究来支持我们向NLP社区发出的号召，在这个案例研究中，我们识别出并纠正了广泛使用的最先进Conformer架构的开源实现中的三个Bug。通过在各种语言环境下进行的自动语音识别和翻译的比较实验，我们证明了Bug的存在并不会妨碍获得良好的和可重复的结果，反而可能导致不正确的结论，为未来的研究可能提供错误的指导。为了应对这一问题，这项研究呼吁采用旨在促进NLP研究中正确性的编码最佳实践，并提高实验结果的可靠性。

    Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
    
[^85]: 基于惩罚的模仿学习和跨语义生成传感器融合技术在自动驾驶中的应用

    Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])

    [http://arxiv.org/abs/2303.11888](http://arxiv.org/abs/2303.11888)

    本文介绍了一种基于惩罚的模仿学习方法和特征级多传感器融合技术，应用于自动驾驶导航中。文中重点介绍了针对激光雷达和RGB信息的融合技术，旨在提高模型对交通规则的遵守能力。

    

    随着模式识别和计算机视觉技术的快速发展，目标检测、语义分割等任务的准确度已经超过人类。自动驾驶作为一项重要的研究方向，旨在彻底改变未来的交通和出行方式。传感器对于自动驾驶的安全性和环境感知的可行性至关重要。多传感器融合由于其多维感知和集成能力的潜力而成为当前研究的热点。本文提出了一种新的特征级多传感器融合技术，用于端到端的自动驾驶导航和模仿学习。我们的论文主要关注于激光雷达和RGB信息的融合技术。我们还提供了一种全新的基于惩罚的模仿学习方法，以加强模型遵守交通规则的能力并统一模仿学习的目标。

    With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
    
[^86]: 有限查询图连通性测试

    Limited Query Graph Connectivity Test. (arXiv:2302.13036v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2302.13036](http://arxiv.org/abs/2302.13036)

    我们提出了一个有限查询图连通性测试的组合优化模型，目标是用最小化查询次数确定图的s-t连通性，主要用于网络安全用例中确定攻击路径的存在与否。

    

    我们提出了一个组合优化模型，称为有限查询图连通性测试。我们考虑一个具有两种可能状态（开/关）的图。边缘的状态最初是隐藏的。我们可以查询边缘以揭示其状态。给定一个源点s和一个目标点t，我们的目标是通过识别路径（仅由开边组成）或切割（仅由关边组成）来测试s-t连通性。无论是否建立了图的连通性，我们都限制查询次数为B次后停止。我们的目标是设计一个查询策略，使期望查询次数最小化。我们的模型主要是基于一个网络中是否存在攻击路径的网络安全用例而提出的。边缘查询由IT管理员的手动工作解决，这是最小化查询的动机。我们的模型与单调随机布尔函数求值（SBFE）密切相关。

    We propose a combinatorial optimisation model called Limited Query Graph Connectivity Test. We consider a graph whose edges have two possible states (On/Off). The edges' states are hidden initially. We could query an edge to reveal its state. Given a source s and a destination t, we aim to test s-t connectivity by identifying either a path (consisting of only On edges) or a cut (consisting of only Off edges). We are limited to B queries, after which we stop regardless of whether graph connectivity is established. We aim to design a query policy that minimizes the expected number of queries.  Our model is mainly motivated by a cyber security use case where we need to establish whether an attack path exists in a network, between a source and a destination. Edge query is resolved by manual effort from the IT admin, which is the motivation behind query minimization.  Our model is highly related to monotone Stochastic Boolean Function Evaluation (SBFE). There are two existing exact algorith
    
[^87]: 预测是否随意？在公平分类中评估自洽性

    Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11562](http://arxiv.org/abs/2301.11562)

    在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。

    

    在公平分类中，不同经过训练的模型之间的预测方差是一个重要但鲜为人知的误差来源问题。 实证表明，某些情况下，预测的方差差异非常大，以至于决策实际上是随意的。 为了研究这个问题，我们进行了大规模的实证研究，并做出了四个总体贡献：我们1）定义了一种基于方差的度量标准，称为自洽性，在测量和减少随意性时使用； 2）开发了一种合理的算法，当预测无法做出决策时，可以放弃分类； 3）进行了迄今为止有关公平分类中方差（相对于自洽性和随意性）作用的最大规模实证研究； 4）推出了一个工具包，使美国住房抵押贷款披露法案（HMDA）数据集易于用于未来研究。 总的来说，我们的实证结果揭示了关于可重复性的令人震惊的见解。当考虑到方差和随意预测的可能性时，大多数公平分类基准接近公平。 但是，一小部分实例显示出极大的随意性水平，这表明当前的模型可能无法处理某些类型的数据。

    Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
    
[^88]: Box$^2$EL: EL++描述逻辑中的概念和角色盒子嵌入的概念和角色盒子嵌入方法及其作用

    Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11118](http://arxiv.org/abs/2301.11118)

    Box$^2$EL方法通过将概念和角色表示为盒子，克服了传统方法中角色表示受限的问题，并在实验中取得了领先的结果。

    

    描述逻辑本体论扩展了知识图谱与概念信息和逻辑背景知识。近年来，人们对这种本体论的归纳推理技术越来越感兴趣，这些技术有望补充传统的演绎推理算法。类似于知识图谱的完善，现有的一些方法通过在潜在空间中学习本体论嵌入，同时确保这些嵌入能够准确地捕捉到底层描述逻辑的逻辑语义。然而，它们存在一些问题，主要是由于受限的角色表示。我们提出了Box$^2$EL方法，将概念和角色都表示为盒子（即轴对齐超矩形），并展示了它如何克服之前方法的局限性。我们在理论上证明了我们模型的正确性，并进行了大量的实验评估，在各种数据集上取得了领先的结果。作为我们评估的一部分，我们引入了一个新的基准。

    Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
    
[^89]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^90]: 重新思考上下文学习中规模的作用: 基于可解释性的660亿尺度案例研究

    Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09095](http://arxiv.org/abs/2212.09095)

    本文通过使用一个660亿参数的语言模型，在多个任务中发现了上下文学习能力并不均匀分布在其各个组件上。通过移除约70%的注意力头和约20%的前馈网络，任务执行表现仅有轻微下降。此外，在OPT-66B中，存在一小部分注意力头对于上下文学习中的基础归纳操作具有高效能力。

    

    研究表明，通过上下文学习范式，语言模型在规模增加时在各种任务上表现更好。本文通过使用一个660亿参数的语言模型（OPT-66B）在14个不同的下游任务中进行研究，探讨了大型语言模型在上下文学习执行任务的能力是否均匀分布在其所有的组件上。结果发现，约70%的注意力头和约20%的前馈网路可以移除而任务表现仅有轻微下降。在不同任务和上下文示例数量中，我们发现对上下文学习不重要的注意力头的集合存在较大的重叠。同时，我们通过一种任务无关的方式来验证我们的假设，发现OPT-66B中的一小部分注意力头在执行与上下文学习相关的基础归纳操作（即前缀匹配和复制）方面具有高效的能力。

    Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: $\sim$70% of attention heads and $\sim$20% of feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These in
    
[^91]: 3D-TOGO: 跨类别文本引导的三维物体生成

    3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation. (arXiv:2212.01103v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.01103](http://arxiv.org/abs/2212.01103)

    本论文提出了一个名为3D-TOGO的模型，旨在实现跨类别的文本引导的三维物体生成。模型包括文本到视图生成模块和视图到三维生成模块，使用先验引导、标题引导和视图对比学习等方法提高视图一致性和标题相似性。采用pixelNeRF模型进行视图到三维生成。

    

    文本引导的三维物体生成旨在生成由用户定义的标题描述的三维物体，为我们设想中的事物提供了一种灵活的可视化方式。尽管一些研究致力于解决这一挑战性任务，但这些研究要么利用一些显式的三维表示（例如网格），缺乏纹理并需要后处理以渲染照片般逼真的视图；要么需要个别耗时的优化来处理每个单独的情况。在这里，我们首次尝试通过新的3D-TOGO模型实现通用的跨类别文本引导的三维物体生成，该模型整合了文本到视图生成模块和视图到三维生成模块。文本到视图生成模块专门用于根据输入标题生成目标三维物体的不同视图。提出了先验引导、标题引导和视图对比学习以实现更好的视图一致性和标题相似性。同时，采用了pixelNeRF模型进行视图到三维生成。

    Text-guided 3D object generation aims to generate 3D objects described by user-defined captions, which paves a flexible way to visualize what we imagined. Although some works have been devoted to solving this challenging task, these works either utilize some explicit 3D representations (e.g., mesh), which lack texture and require post-processing for rendering photo-realistic views; or require individual time-consuming optimization for every single case. Here, we make the first attempt to achieve generic text-guided cross-category 3D object generation via a new 3D-TOGO model, which integrates a text-to-views generation module and a views-to-3D generation module. The text-to-views generation module is designed to generate different views of the target 3D object given an input caption. prior-guidance, caption-guidance and view contrastive learning are proposed for achieving better view-consistency and caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D generati
    
[^92]: 解缠表示学习

    Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11695](http://arxiv.org/abs/2211.11695)

    解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。

    

    解缠表示学习（DRL）旨在学习一个能够识别和解缠可观测数据中隐藏因素的模型。将变化的潜在要素分离成具有语义意义的变量的过程有助于学习可解释的数据表示，模仿人类观察对象或关系时的有意义理解过程。作为一种通用的学习策略，DRL在多个领域中展示了提高模型可解释性、可控性、鲁棒性以及泛化能力的优势，如计算机视觉、自然语言处理、数据挖掘等。本文综合评述了DRL的各个方面，包括动机、定义、方法论、评估、应用和模型设计。我们讨论了基于两个公认定义（直观定义和群论定义）的DRL方法。我们进一步分析了DRL的开展。

    Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
    
[^93]: 标准化流用于人体姿势异常检测

    Normalizing Flows for Human Pose Anomaly Detection. (arXiv:2211.10946v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10946](http://arxiv.org/abs/2211.10946)

    该论文提出了一种用于人体姿势异常检测的标准化流模型，通过将问题简化为姿势异常检测减少了干扰参数的影响，同时具有减少偏见的优势。该模型基于高度紧凑的姿势表示，在解决时空姿势数据的特殊特征上表现出优势，并且可以处理准则设置和非准则设置下的训练数据。实验结果表明该模型达到了最先进的水平。

    

    视频异常检测是一个不确定的问题，因为它依赖于许多参数，如外观、姿势、摄像机角度、背景等等。我们将问题简化为人体姿势的异常检测，从而减少了外观等影响结果的干扰参数的风险。仅关注姿势还可以减少对特定少数群体的偏见。我们的模型直接在人体姿势图序列上运行，非常轻量级（约1K个参数），可以在任何能够运行姿势估计的机器上运行，几乎不需要额外资源。我们利用高度紧凑的姿势表示在标准化流框架中进行改进，解决了时空姿势数据的特殊特征，并展示了在这种用例中的优势。该算法非常通用，既可以处理只有正常示例的训练数据，也可以处理包含标记的正常和异常示例的监督设置。我们报告了最先进的实验结果。

    Video anomaly detection is an ill-posed problem because it relies on many parameters such as appearance, pose, camera angle, background, and more. We distill the problem to anomaly detection of human pose, thus decreasing the risk of nuisance parameters such as appearance affecting the result. Focusing on pose alone also has the side benefit of reducing bias against distinct minority groups. Our model works directly on human pose graph sequences and is exceptionally lightweight (~1K parameters), capable of running on any machine able to run the pose estimation with negligible additional resources. We leverage the highly compact pose representation in a normalizing flows framework, which we extend to tackle the unique characteristics of spatio-temporal pose data and show its advantages in this use case. The algorithm is quite general and can handle training data of only normal examples as well as a supervised setting that consists of labeled normal and abnormal examples. We report state
    
[^94]: 高效训练：探索泛化课程学习来训练视觉主干网络

    EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones. (arXiv:2211.09703v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09703](http://arxiv.org/abs/2211.09703)

    本文提出了一种泛化课程学习方法，用于高效训练视觉主干网络，通过优先让模型学习“更容易学习”的模式，不断引入更难的模式，从而加速训练过程。

    

    现代深度网络的卓越性能通常伴随着昂贵的训练过程。本文提出了一种新的课程学习方法，用于高效训练视觉主干网络（例如视觉Transformer）。本文启发于深度网络的内在学习动力学：我们实验性地展示了在较早的训练阶段，模型主要学习在每个示例中识别一些“更容易学习”的判别模式，例如图像的低频成分和数据增广之前的原始信息。基于此现象，我们提出了一种课程，其中模型总是在每个时期利用所有训练数据，而课程始于仅暴露每个示例的“更容易学习”的模式，并逐渐引入更难的模式。为了实现这个想法，我们1）在输入的傅里叶谱中引入一个裁剪操作，使模型只能从低频组分中进行学习。

    The superior performance of modern deep networks usually comes with a costly training procedure. This paper presents a new curriculum learning approach for the efficient training of visual backbones (e.g., vision Transformers). Our work is inspired by the inherent learning dynamics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recognize some 'easier-to-learn' discriminative patterns within each example, e.g., the lower-frequency components of images and the original information before data augmentation. Driven by this phenomenon, we propose a curriculum where the model always leverages all the training data at each epoch, while the curriculum starts with only exposing the 'easier-to-learn' patterns of each example, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping operation in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo
    
[^95]: 语法指导的领域适应在基于方面的情感分析中的应用

    Syntax-Guided Domain Adaptation for Aspect-based Sentiment Analysis. (arXiv:2211.05457v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.05457](http://arxiv.org/abs/2211.05457)

    本文提出了一种名为SDAM的新颖基于语法指导的领域适应模型，用于更有效地进行跨领域基于方面的情感分析。SDAM利用句法结构相似性构建伪训练实例，提升模型性能。

    

    基于方面的情感分析（ABSA）旨在从评论文本中提取具有意见的方面词汇并确定它们的情感极性，在学术界和工业界都得到广泛研究。作为一项细粒度的分类任务，标注成本非常高。领域适应是解决新领域数据不足的常用方法，通过在领域之间转移共有知识来减小差距。大多数跨领域ABSA研究基于结构对应学习（SCL），并使用中介特征构建辅助任务以缩小领域之间的差距。然而，它们基于中介的辅助任务只能传递关于方面词汇而不是情感的知识，限制了现有模型的性能。在这项工作中，我们提出了一种新颖的基于语法指导的领域适应模型，命名为SDAM，用于更有效地进行跨领域ABSA。SDAM利用句法结构相似性来构建伪训练实例，其中目标领域的方面词汇与源领域的相似。

    Aspect-based sentiment analysis (ABSA) aims at extracting opinionated aspect terms in review texts and determining their sentiment polarities, which is widely studied in both academia and industry. As a fine-grained classification task, the annotation cost is extremely high. Domain adaptation is a popular solution to alleviate the data deficiency issue in new domains by transferring common knowledge across domains. Most cross-domain ABSA studies are based on structure correspondence learning (SCL), and use pivot features to construct auxiliary tasks for narrowing down the gap between domains. However, their pivot-based auxiliary tasks can only transfer knowledge of aspect terms but not sentiment, limiting the performance of existing models. In this work, we propose a novel Syntax-guided Domain Adaptation Model, named SDAM, for more effective cross-domain ABSA. SDAM exploits syntactic structure similarities for building pseudo training instances, during which aspect terms of target doma
    
[^96]: DyTed:离散时动态图的分离表示学习

    DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph. (arXiv:2210.10592v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2210.10592](http://arxiv.org/abs/2210.10592)

    本论文提出了一种用于离散时动态图的分离表示学习框架DyTed，通过设计时间片对比学习任务和结构对比学习任务，有效地识别时间不变和时间变化的表示，并提出分离感知判别器以增强分离性能。

    

    近年来，无监督的动态图表示学习引起了很多研究关注。与静态图相比，动态图既体现了节点的内在稳定特征，又体现了与时间相关的动态偏好。然而，现有方法通常将这两种信息混合到一个表示空间中，这可能导致解释性差、鲁棒性差，并且在应用于不同的下游任务时能力有限。为了解决上述问题，本文提出了一种用于离散时动态图的新型分离表示学习框架DyTed。我们特别设计了一个时间片对比学习任务，并结合了结构对比学习，分别有效地识别时间不变和时间变化的表示。为了进一步增强这两种表示的分离性，我们提出了一个分离感知判别器...

    Unsupervised representation learning for dynamic graphs has attracted a lot of research attention in recent years. Compared with static graph, the dynamic graph is a comprehensive embodiment of both the intrinsic stable characteristics of nodes and the time-related dynamic preference. However, existing methods generally mix these two types of information into a single representation space, which may lead to poor explanation, less robustness, and a limited ability when applied to different downstream tasks. To solve the above problems, in this paper, we propose a novel disenTangled representation learning framework for discrete-time Dynamic graphs, namely DyTed. We specially design a temporal-clips contrastive learning task together with a structure contrastive learning to effectively identify the time-invariant and time-varying representations respectively. To further enhance the disentanglement of these two types of representation, we propose a disentanglement-aware discriminator unde
    
[^97]: ST-former在COVID-19期间用于短期城市轨道交通客流预测

    ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system. (arXiv:2210.09043v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09043](http://arxiv.org/abs/2210.09043)

    本文提出了一种名为ST-former的新型transformer架构，用于COVID-19期间的城市轨道交通客流预测。通过引入改进的自注意机制和自适应多图卷积网络，可以准确建模客流的时空依赖关系和复杂空间依赖关系。

    

    准确预测城市轨道交通的客流对于提高智能交通系统的性能尤为重要，尤其是在疫情期间。如何动态建模客流的复杂时空依赖关系是实现准确客流预测的主要问题。为了解决这个问题，本文提出了一种全新的基于transformer的架构，称为ST-former，专门用于COVID-19期间的客流预测。具体而言，我们开发了一种改进的自注意机制，命名为因果卷积ProbSparse自注意（CPSA），以低计算成本建模客流的多个时间依赖关系。为了捕捉复杂且动态的空间依赖关系，我们引入了一种新颖的自适应多图卷积网络（AMGCN），通过以自适应的方式利用多个图来进行建模。另外，多源数据融合模块将客流数据、COVID-19确诊病例等多种数据进行融合。

    Accurate passenger flow prediction of urban rail transit is essential for improving the performance of intelligent transportation systems, especially during the epidemic. How to dynamically model the complex spatiotemporal dependencies of passenger flow is the main issue in achieving accurate passenger flow prediction during the epidemic. To solve this issue, this paper proposes a brand-new transformer-based architecture called STformer under the encoder-decoder framework specifically for COVID-19. Concretely, we develop a modified self-attention mechanism named Causal-Convolution ProbSparse Self-Attention (CPSA) to model the multiple temporal dependencies of passenger flow with low computational costs. To capture the complex and dynamic spatial dependencies, we introduce a novel Adaptive Multi-Graph Convolution Network (AMGCN) by leveraging multiple graphs in a self-adaptive manner. Additionally, the Multi-source Data Fusion block fuses the passenger flow data, COVID-19 confirmed case
    
[^98]: 随机约束分布鲁棒优化算法的样本大小无关复杂度

    Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05740](http://arxiv.org/abs/2210.05740)

    本文提出了一种适用于非凸和凸损失函数的随机算法，用于解决Kullback Leibler散度约束的分布鲁棒优化问题，并且具有与样本大小无关的复杂度，每次迭代只需要恒定的批次大小。实证研究证明了该算法在解决非凸和凸约束DRO问题中的有效性。

    

    分布鲁棒优化(DRO)作为一种在训练集和测试集之间进行分布偏移训练鲁棒模型的流行方法，近年来受到了广泛关注。本文提出并分析了适用于解决Kullback Leibler散度约束DRO问题的随机算法，适用于非凸和凸损失函数。与现有的解决方法相比，我们的随机算法不仅具有与样本大小无关的竞争性甚至更好的复杂度，而且在每次迭代中只需要恒定的批次大小，这对于广泛应用更加实用。我们为非凸损失函数找到了一个$\epsilon$稳定解的近乎最优的复杂度界限，并为凸损失函数找到了一个$\epsilon$最优解的最优复杂度。实证研究证明了所提算法在解决非凸和凸约束DRO问题方面的有效性。

    Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\epsilon$ stationary solution for non-convex losses and an optimal complexity for finding an $\epsilon$ optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.
    
[^99]: 概率逻辑编程中作为程序的解释

    Explanations as Programs in Probabilistic Logic Programming. (arXiv:2210.03021v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.03021](http://arxiv.org/abs/2210.03021)

    本文提出了一种新的方法，将解释表示为从给定查询生成的程序，通过多次展开类变换来生成程序，从而显式展示出证明给定查询的推理链。

    

    可以生成易理解的解释是现代人工智能系统的一项重要功能。在本研究中，我们考虑了概率逻辑编程，这是一种扩展了逻辑编程的方法，可以用于建模具有关系结构和不确定性的领域。本文提出了一种新的方法，将解释表示为从给定查询生成的程序，通过多次展开类变换来生成程序。这样，证明给定查询的推理链被显式地展示出来。

    The generation of comprehensible explanations is an essential feature of modern artificial intelligence systems. In this work, we consider probabilistic logic programming, an extension of logic programming which can be useful to model domains with relational structure and uncertainty. Essentially, a program specifies a probability distribution over possible worlds (i.e., sets of facts). The notion of explanation is typically associated with that of a world, so that one often looks for the most probable world as well as for the worlds where the query is true. Unfortunately, such explanations exhibit no causal structure. In particular, the chain of inferences required for a specific prediction (represented by a query) is not shown. In this paper, we propose a novel approach where explanations are represented as programs that are generated from a given query by a number of unfolding-like transformations. Here, the chain of inferences that proves a given query is made explicit. Furthermore
    
[^100]: 基于深度神经网络的空间引力波信号检测和提取

    Space-based gravitational wave signal detection and extraction with deep neural network. (arXiv:2207.07414v3 [gr-qc] UPDATED)

    [http://arxiv.org/abs/2207.07414](http://arxiv.org/abs/2207.07414)

    本研究开发了一种高精度的使用深度神经网络的空间引力波信号检测和提取方法，能够在高斯噪声中识别各种源的信号，达到了超过99%的检测率和至少95%的相似性。该方法具有较低的虚警率和强大的泛化行为。

    

    空间引力波探测器将能够观测到地面探测器几乎无法观测到的信号源。因此，传统的信号检测方法，匹配滤波，将需要一个复杂的模板库，导致了实际上过于昂贵的计算成本。在这里，我们开发了一种高精度的用于所有空间引力波源的信号检测和提取方法。作为概念验证，我们展示了一种科学驱动和统一的多阶段自注意力深度神经网络能够识别在高斯噪声中潜在的信号。我们的方法在识别来自不同源的信号时表现出超过99%的检测率，信噪比为50，虚警率为1%，与目标信号相比至少达到95%的相似性。我们进一步展示了在几个扩展场景下的可解释性和强大的泛化行为。

    Space-based gravitational wave (GW) detectors will be able to observe signals from sources that are otherwise nearly impossible from current ground-based detection. Consequently, the well established signal detection method, matched filtering, will require a complex template bank, leading to a computational cost that is too expensive in practice. Here, we develop a high-accuracy GW signal detection and extraction method for all space-based GW sources. As a proof of concept, we show that a science-driven and uniform multi-stage self-attention-based deep neural network can identify synthetic signals that are submerged in Gaussian noise. Our method exhibits a detection rate exceeding 99% in identifying signals from various sources, with the signal-to-noise ratio at 50, at a false alarm rate of 1%. while obtaining at least 95% similarity compared with target signals. We further demonstrate the interpretability and strong generalization behavior for several extended scenarios.
    
[^101]: 认证的对称性和优先级突破用于组合优化

    Certified Symmetry and Dominance Breaking for Combinatorial Optimisation. (arXiv:2203.12275v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2203.12275](http://arxiv.org/abs/2203.12275)

    该论文介绍了一种用于认证对称性和优先级突破的方法，可以高效地验证布尔可满足性（SAT）求解中的一般对称性突破，还可以应用于最大团求解和约束编程等组合问题。

    

    对称性和优先级突破对于解决困难的组合搜索和优化问题至关重要，但这些技术的正确性有时依赖于微妙的论证。因此，产生高效的机器可验证证书来证明解决方案的正确计算是可取的。我们基于切割平面证明系统，为可表达对称性和优先级突破的优化问题开发了一种认证方法。我们的实验评估表明，我们可以高效地验证布尔可满足性（SAT）求解中的完全一般对称性突破，从而首次提供了一种统一的方法来认证一系列高级SAT技术，包括异或和基数推理。此外，我们将我们的方法应用于最大团求解和约束编程，作为对这种方法适用于更广泛的组合问题的概念证明。

    Symmetry and dominance breaking can be crucial for solving hard combinatorial search and optimisation problems, but the correctness of these techniques sometimes relies on subtle arguments. For this reason, it is desirable to produce efficient, machine-verifiable certificates that solutions have been computed correctly. Building on the cutting planes proof system, we develop a certification method for optimisation problems in which symmetry and dominance breaking are easily expressible. Our experimental evaluation demonstrates that we can efficiently verify fully general symmetry breaking in Boolean satisfiability (SAT) solving, thus providing, for the first time, a unified method to certify a range of advanced SAT techniques that also includes XOR and cardinality reasoning. In addition, we apply our method to maximum clique solving and constraint programming as a proof of concept that the approach applies to a wider range of combinatorial problems.
    
[^102]: SMGRL：可扩展的多分辨率图表示学习

    SMGRL: Scalable Multi-resolution Graph Representation Learning. (arXiv:2201.12670v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12670](http://arxiv.org/abs/2201.12670)

    本论文提出了一个可扩展的多分辨率图表示学习框架（SMGRL），通过降低训练成本和利用自相似性在多个分辨率上应用算法，能够高效地学习多分辨率节点嵌入。

    

    图卷积网络（GCN）可以用于学习具有拓扑感知能力的节点嵌入，对于分类或链接预测非常有用。然而，它们无法捕捉节点之间的长程依赖关系，除非添加额外的层次——而这又导致过度平滑和时间空间复杂度增加。此外，节点之间的复杂依赖关系使得小批量处理变得困难，限制了它们在大型图上的适用性。我们提出了一个可扩展的多分辨率图表示学习（SMGRL）框架，使我们能够高效地学习多分辨率节点嵌入。我们的框架与模型无关，可以应用于任何现有的GCN模型。通过仅在原始图的降维粗化上进行训练，然后利用自相似性在多个分辨率上应用所得算法，我们显著降低了训练成本。最终的多分辨率嵌入可以聚合以产生高质量的节点嵌入。

    Graph convolutional networks (GCNs) allow us to learn topologically-aware node embeddings, which can be useful for classification or link prediction. However, they are unable to capture long-range dependencies between nodes without adding additional layers -- which in turn leads to over-smoothing and increased time and space complexity. Further, the complex dependencies between nodes make mini-batching challenging, limiting their applicability to large graphs. We propose a Scalable Multi-resolution Graph Representation Learning (SMGRL) framework that enables us to learn multi-resolution node embeddings efficiently. Our framework is model-agnostic and can be applied to any existing GCN model. We dramatically reduce training costs by training only on a reduced-dimension coarsening of the original graph, then exploit self-similarity to apply the resulting algorithm at multiple resolutions. The resulting multi-resolution embeddings can be aggregated to yield high-quality node embeddings th
    
[^103]: 分析自监督在处理语言偏见中的局限性

    Analyzing the Limits of Self-Supervision in Handling Bias in Language. (arXiv:2112.08637v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.08637](http://arxiv.org/abs/2112.08637)

    本文分析了自监督在处理语言偏见中的局限性，并定义了四个偏见任务（诊断、识别、提取和改写），通过使用不同类别的任务描述来评估语言模型对语义的捕捉能力。

    

    使用自然语言任务描述作为提示输入已成为从大规模生成性语言模型中引出相对准确输出的流行机制，而同时又几乎没有上下文监督。这也有助于了解语言模型从无标记文本的大规模语言预训练中纯粹捕捉下游任务的语义的能力。这样的模型自然也暴露于许多不希望的内容，如种族主义和性别歧视的语言，目前对模型在这些方面的意识的研究有限。本文中，我们定义并全面评估这种语言模型在四个偏见任务（诊断、识别、提取和改写）中捕捉语义的能力。对于这些任务，我们定义了三类任务描述：陈述、问题和完成，并在每个类别中使用了许多词汇变体。我们研究了使用这些任务描述的提示对每个任务的有效性。

    Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of a wide range of downstream tasks purely from self-supervised pre-training on massive corpora of unlabeled text. Such models have naturally also been exposed to a lot of undesirable content like racist and sexist language and there is limited work on awareness of models along these dimensions. In this paper, we define and comprehensively evaluate how well such language models capture the semantics of four tasks for bias: diagnosis, identification, extraction and rephrasing. We define three broad classes of task descriptions for these tasks: statement, question, and completion, with numerous lexical variants within each class. We study the efficacy of prompting for each task using these classe
    
[^104]: 密集森林林冠下的大规模自主飞行与实时语义SLAM

    Large-scale Autonomous Flight with Real-time Semantic SLAM under Dense Forest Canopy. (arXiv:2109.06479v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2109.06479](http://arxiv.org/abs/2109.06479)

    本文提出了一个集成系统，可以在密集森林林冠下进行大规模自主飞行和实时语义地图构建。系统使用LiDAR数据检测和建模树干和地面平面，并利用多级规划和地图构建框架计算动态可行的轨迹，以构建用户定义感兴趣区域的语义地图，并通过语义SLAM来最小化里程计漂移。

    

    语义地图使用一组具有语义意义的对象表示环境。这种表示方式在存储效率、歧义性减少和信息丰富度方面更好，为在高度无结构、无GPS的环境中进行大规模自治飞行和获取可操作信息提供方便。在本文中，我们提出了一个集成系统，能够在具有挑战性的林冠下环境中进行大规模自主飞行和实时语义地图构建。我们使用LiDAR数据检测和建模树干和地面平面，这些信息与扫描数据相关联，并用于约束机器人姿态和树干模型。自主导航模块利用多级规划和地图构建框架，以计算动态可行的轨迹，使无人机以计算和存储有效的方式构建用户定义感兴趣区域的语义地图。设计了漂移补偿机制，通过语义SLAM来最小化里程计漂移。

    Semantic maps represent the environment using a set of semantically meaningful objects. This representation is storage-efficient, less ambiguous, and more informative, thus facilitating large-scale autonomy and the acquisition of actionable information in highly unstructured, GPS-denied environments. In this letter, we propose an integrated system that can perform large-scale autonomous flights and real-time semantic mapping in challenging under-canopy environments. We detect and model tree trunks and ground planes from LiDAR data, which are associated across scans and used to constrain robot poses as well as tree trunk models. The autonomous navigation module utilizes a multi-level planning and mapping framework and computes dynamically feasible trajectories that lead the UAV to build a semantic map of the user-defined region of interest in a computationally and storage efficient manner. A drift-compensation mechanism is designed to minimize the odometry drift using semantic SLAM outp
    

