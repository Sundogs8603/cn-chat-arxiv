# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma.](http://arxiv.org/abs/2305.07642) | ASNR-MICCAI脑肿瘤分割挑战2023将提供一个适用于自动诊断颅内脑膜瘤的最先进自动化颅内脑膜瘤分割模型的基准。 |
| [^2] | [Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training.](http://arxiv.org/abs/2305.07633) | 本论文提出了一种使用产品知识图谱预训练模型从预训练的语言模型提取项目特征，以解决零样本项目推荐任务。该方法通过提出四个预训练任务和任务导向的适应层来解决预训练过程中的挑战，并将模型微调到新的推荐任务中。 |
| [^3] | [Design, Development, and Evaluation of an Interactive Personalized Social Robot to Monitor and Coach Post-Stroke Rehabilitation Exercises.](http://arxiv.org/abs/2305.07632) | 本论文介绍了一个交互式个性化社交机器人用于监测和辅导中风后恢复运动的研究，通过神经网络和规则模型相结合监测和评估患者康复锻炼并提供实时个性化反馈。 |
| [^4] | [PALR: Personalization Aware LLMs for Recommendation.](http://arxiv.org/abs/2305.07622) | 本文提出了一个称为PALR的框架，将用户的历史行为与LLMs相结合，生成用户喜欢的物品的推荐。与现有的推荐方法相比，我们的PALR框架实现了最先进的性能。 |
| [^5] | [Scalable Coupling of Deep Learning with Logical Reasoning.](http://arxiv.org/abs/2305.07617) | 本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。 |
| [^6] | [Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training.](http://arxiv.org/abs/2305.07613) | 本文提出了一种名为Spider GAN的新方法，通过寻找数据集之间的友好邻居来提高GAN的训练效率，加速收敛，即使是不相关的数据集之间也可以发现对应关系。 |
| [^7] | [Generative AI: Implications and Applications for Education.](http://arxiv.org/abs/2305.07605) | 本文研究了一种生成式人工智能技术——基于大型语言模型的聊天机器人（C-LLM），以及它在复杂学生作品的人工智能评估和审查方面的应用，同时探讨了这种技术在教育中的应用范围。 |
| [^8] | [Opti Code Pro: A Heuristic Search-based Approach to Code Refactoring.](http://arxiv.org/abs/2305.07594) | 本文提出了一种基于启发式搜索的代码重构方法，以解决耦合和内聚性的问题，并提供了具体的演示和实现工具。 |
| [^9] | [A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information.](http://arxiv.org/abs/2305.07565) | 该论文提出了一种记忆模型，在处理流式数据时，通过排练和预期来记忆有关问题回答任务的重要信息。该模型应用自监督机制，通过核指代信息的屏蔽建模任务训练，成功通过短序列数据集和大型基准测试。 |
| [^10] | [Dish detection in food platters: A framework for automated diet logging and nutrition management.](http://arxiv.org/abs/2305.07552) | 本文章提出了一种针对食品盘中菜肴检测的自动化方法，主要应用于饮食记录和营养管理，并通过印度餐盘中菜肴的案例研究，使用基于深度学习的物体检测架构，通过大量图像和注释的比较，确定了最先进的模型：ResNet50-v2和EfficientDet D1。 |
| [^11] | [Understanding Automatic Differentiation Pitfalls.](http://arxiv.org/abs/2305.07546) | 本文将广泛分类自动微分的问题用法，并通过举例说明每个类别，旨在帮助读者避免意外行为，在发生问题时更容易检测问题，并从自动微分工具中获得更实际的期望。 |
| [^12] | [WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models.](http://arxiv.org/abs/2305.07528) | WEDGE是一个通过生成式视觉语言模型生成的多气候自主驾驶数据集，可用于通过微调最先进的探测器在真实世界气象基准测试中提高性能。 |
| [^13] | [Joint MR sequence optimization beats pure neural network approaches for spin-echo MRI super-resolution.](http://arxiv.org/abs/2305.07524) | 该论文提出了一种联合磁共振序列优化和神经网络方法来实现MR图像的超分辨率，通过消除序列参数对实际图像分辨率的影响，以及优化射频脉冲列设计来提高神经网络执行超分辨率任务的性能。 |
| [^14] | [PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices.](http://arxiv.org/abs/2305.07522) | PillarAcc提出了一种稀疏算法-硬件协同设计，有效增强基于Pillar的三维物体检测网络，实现高效率的计算减少。 |
| [^15] | [Learn to Unlearn: A Survey on Machine Unlearning.](http://arxiv.org/abs/2305.07512) | 本综述总结了机器去学习技术，用于从训练模型中删除敏感数据，但重新训练ML模型往往不可行。针对这个挑战，需要开发强大的模型以缓解公平性问题。 |
| [^16] | [eXplainable Artificial Intelligence on Medical Images: A Survey.](http://arxiv.org/abs/2305.07511) | 这项调查分析了XAI领域中应用于医学诊断的研究，以便解释黑匣子模型的结果，针对的疾病包括癌症和COVID-19。 |
| [^17] | [Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation.](http://arxiv.org/abs/2305.07500) | 本文提出了一种新的基于最优输运（OT）的领域自适应（DA）方法，通过学习一个嵌入空间，使得OT问题的解是最优且计算量较少的，适用于同质和异质的DA设置。 |
| [^18] | [Dynamically Conservative Self-Driving Planner for Long-Tail Cases.](http://arxiv.org/abs/2305.07497) | 本论文提出了一种动态保守规划器方法，能够根据每种情况的“长尾”率自动调整保守程度，以更好地应对SDVs在实际驾驶中遇到的罕见关键情况。 |
| [^19] | [Gallery Sampling for Robust and Fast Face Identification.](http://arxiv.org/abs/2305.07495) | 该论文提出了一种鲁棒且快速的人脸识别方法，通过对图库数据进行采样处理，在减少搜索时间的同时，对异常图像如错误标记、低质量和信息较少的图像具有较强的鲁棒性。在5.4M网络图像数据集上，我们的方法在FNIR方面达到了0.0975，而传统方法为0.3891。 |
| [^20] | [Identify, Estimate and Bound the Uncertainty of Reinforcement Learning for Autonomous Driving.](http://arxiv.org/abs/2305.07487) | 本文提出了一种方法来限制自动驾驶中深度强化学习模型的决策不可靠性，以保护决策的可靠性，该方法通过估计和限制策略的性能不确定性来实现。 |
| [^21] | [BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text.](http://arxiv.org/abs/2305.07468) | BactInt是一种面向领域的自动化方法，使用迁移学习从生物医学文本中提取细菌间相互作用并挖掘特定细菌群之间的关系。公开可用的BactInt语料库标注了1200篇PubMed摘要。 |
| [^22] | [Systematic Review on Reinforcement Learning in the Field of Fintech.](http://arxiv.org/abs/2305.07466) | 本文综述了近年来强化学习在金融科技领域的应用，包括组合优化、降低信用风险、投资资本管理、利润最大化、有效的推荐系统和更好的价格策略确定。本文通过PRISMA技术筛选文献，突出了强化学习在Fintech中的预测精度、复杂性、可扩展性、风险、盈利能力和业绩，旨在探讨其在Fintech领域中的实际贡献。 |
| [^23] | [Beyond Prompts: Exploring the Design Space of Mixed-Initiative Co-Creativity Systems.](http://arxiv.org/abs/2305.07465) | 本文介绍了一种新的人工智能系统设计空间，探索了人类和人工智能系统之间各种不同的创意意图沟通方式，发现MI-CC系统中更多由人工智能发起的贡献是用户优先选择的，这说明了超越提示，探索更多的MI-CC设计方案可以提高人工智能协作体验。 |
| [^24] | [Optimizing Memory Mapping Using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.07440) | 本文提出了一种使用强化学习解决机器学习程序中内存映射问题的方法。 |
| [^25] | [The Ethics of AI in Games.](http://arxiv.org/abs/2305.07392) | 本文调查了人工智能应用于游戏中的伦理问题，从情感回路的视角讨论了AI在游戏开发中所面临的三个伦理挑战：人工诱导情感的伦理界限，隐私和安全游戏空间之间的权衡以及在游戏适应中应用的检测所带来的透明度和用户控制问题。 |
| [^26] | [Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation.](http://arxiv.org/abs/2305.07375) | 本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。 |
| [^27] | [S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning.](http://arxiv.org/abs/2305.07367) | S-REINFORCE是一种神经符号策略梯度方法，利用神经网络和符号回归器生成数字和符号策略，从而为强化学习任务提供可解释解决方案。 |
| [^28] | [Multi-Value Alignment in Normative Multi-Agent System: Evolutionary Optimisation Approach.](http://arxiv.org/abs/2305.07366) | 本研究提出一种通过多目标进化算法来实现多值推广的规范多智能体系统方法，并考虑系统中智能体的异质性和同时对齐多个价值的要求。 |
| [^29] | [Towards Transliteration between Sindhi Scripts from Devanagari to Perso-Arabic.](http://arxiv.org/abs/2305.07365) | 本文提出了一种将天城文辛迪转换为波斯-阿拉伯文辛迪的技术，通过混合使用基于规则和概率模型，系统取得了99.64％的准确率。 |
| [^30] | [Improving the Quality of Neural Machine Translation Through Proper Translation of Name Entities.](http://arxiv.org/abs/2305.07360) | 本文提出了一种方法，通过将命名实体作为预处理步骤进行翻译/音译，以提高神经机器翻译的质量，实验结果显示该方法能够正确翻译大多数的命名实体，准确率高达99.52％。 |
| [^31] | [A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects.](http://arxiv.org/abs/2305.07348) | 本文评估了基于深度学习的单目航天器姿态估计的现有方法，总结了部署该方法在现实任务中仍需克服的挑战。 |
| [^32] | [Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization.](http://arxiv.org/abs/2305.07310) | 本文提出了一种跨语言一致性规范化方法CrossConST，用于改进零样本多语言神经机器翻译模型的性能。实验结果表明，CrossConST可以缩小语言之间的表示差距，提高零样本翻译的准确性和多样性。 |
| [^33] | [CLIP-Count: Towards Text-Guided Zero-Shot Object Counting.](http://arxiv.org/abs/2305.07304) | 该研究提出了CLIP-Count，一种基于零样本文本引导的物体计数方法，不需要对特定对象类别进行微调，通过引入补丁-文本对比损失和分层的patch-text交互模块，获得了高效的密集预测结果。 |
| [^34] | [Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition.](http://arxiv.org/abs/2305.07266) | 该论文提出了一种嵌套命名实体识别模型GPRL，使用高斯先验调整嵌套边界标记的输出概率分布，采用强化学习方法生成实体三元组，无需考虑金标签中的实体顺序，实验结果显示其优于以前的嵌套NER模型。 |
| [^35] | [Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms.](http://arxiv.org/abs/2305.07248) | 本文探讨了优化累积奖励分位数的强化学习设置，提出了QPO和其变体QPPO算法，并使用神经网络对控制动作的策略进行参数化。实验结果表明该算法优于现有基线算法。 |
| [^36] | [Dual Forgetting Operators in the Context of Weakest Sufficient and Strongest Necessary Conditions.](http://arxiv.org/abs/2305.07233) | 本文介绍了一种新的算子，称为弱遗忘，与标准遗忘相互对偶，并共同展示了遗忘算子的新的更统一的视角。二者都是基于蕴含和推理而非模型论语义，容易采用Ackermman引理和其不动点概括的算法视角进行描述和运用。定量描述了标准遗忘和最强必要条件之间的强关系以及弱遗忘和最弱充分条件之间的强形式关系。 |
| [^37] | [MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition.](http://arxiv.org/abs/2305.07214) | 本文研究了自我中心动作识别中的多模态泛化问题，构建了新的数据集MMG-Ego4D，其中包含视频、音频和惯性运动传感器(IMU)模态。我们在标准监督动作识别和学习新的动作分类的少样本场景下对MMG进行了彻底的研究。 |
| [^38] | [Fast Pareto Optimization Using Sliding Window Selection.](http://arxiv.org/abs/2305.07178) | 本文介绍了一种滑动窗口加速技术，以减少算法的种群大小，从而在更短的时间内实现与以前方法相同的理论性能保证。实验结果表明，该方法可以在广泛的实例和约束设置中实现更好的结果。 |
| [^39] | [Exploring Zero and Few-shot Techniques for Intent Classification.](http://arxiv.org/abs/2305.07157) | 探讨了四种零样本和小样本意图分类方法，包括领域适应、数据增强、使用大型语言模型的零样本意图分类以及指令微调语言模型的参数有效微调，结果表明这些方法在低资源环境下都是有效的。指令微调语言模型的参数有效微调性能最佳。 |
| [^40] | [The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain.](http://arxiv.org/abs/2305.07141) | 本研究提出了一个新的基准数据集ConceptARC，针对ARC领域的抽象和推理问题进行了深入评估，以提高人工智能系统的抽象和泛化能力。 |
| [^41] | [Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques.](http://arxiv.org/abs/2305.07116) | 本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。 |
| [^42] | [Salient Mask-Guided Vision Transformer for Fine-Grained Classification.](http://arxiv.org/abs/2305.07102) | 该研究提出了一种显著掩模引导下的视觉Transformer方法，适用于细粒度分类。该方法旨在解决细粒度分类中捕捉最具有区别性的差异和忽略不相关区域的困难。 |
| [^43] | [$\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks.](http://arxiv.org/abs/2305.07100) | 本文提出了$\mathrm{E}(n)$等变消息传递单纯网络(EMPSNs)，一种同时将消息传递单纯网络和$\mathrm{E}(n)$等变图神经网络的优势结合，在处理高维数据时利用几何信息防止过度平滑的方法。 |
| [^44] | [Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales.](http://arxiv.org/abs/2305.07095) | 该论文研究了机器产生的自然语言理由对人类是否有用，发现现有理由的人类效用远低于理想状态，并提出通过估计理由在回答给定问题中的有用性来提高机器生成理由的人类效用。 |
| [^45] | [Deep Reinforcement Learning for Interference Management in UAV-based 3D Networks: Potentials and Challenges.](http://arxiv.org/abs/2305.07069) | 本文提出了利用深度强化学习进行干扰管理的方法来解决UAV通信中的干扰问题，而无需先知干扰信号的信道信息。 |
| [^46] | [Value Iteration Networks with Gated Summarization Module.](http://arxiv.org/abs/2305.07039) | 本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。 |
| [^47] | [GFlowNets with Human Feedback.](http://arxiv.org/abs/2305.07036) | GFlowNets框架通过人类反馈来改善AI模型的探索能力，通过适应不同轨迹上的人类评估，可以学习到严格与人类评级成比例的策略，实验结果表明比RLHF更出色。 |
| [^48] | [Shhh! The Logic of Clandestine Operations.](http://arxiv.org/abs/2305.07035) | 该论文提出了秘密行动的形式语义和逻辑系统。其强调分布式知识和联盟力量对秘密行动的相互作用。 |
| [^49] | [Quran Recitation Recognition using End-to-End Deep Learning.](http://arxiv.org/abs/2305.07034) | 本文提出了一种基于端到端深度学习模型，使用CTC作为目标函数，来识别古兰经的朗诵。采用公共数据集进行实验。 |
| [^50] | [Hawkes Process based on Controlled Differential Equations.](http://arxiv.org/abs/2305.07031) | 本文提出了一种基于控制微分方程的Hawkes过程模型，可精确计算对数似然，并能够正确处理不规则时间序列，适用于社会扩散和地震预测。 |
| [^51] | [Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts.](http://arxiv.org/abs/2305.07019) | Musketeer是一种通用视觉语言模型，采用任务解释提示（TEP）机制，能够有效整合异构任务的知识，并在多个任务中表现均匀 |
| [^52] | [Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns.](http://arxiv.org/abs/2305.06972) | 大型语言模型可用于扩展钓鱼邮件攻击，作者通过实证测试表明高级的语言模型可以显著提高攻击的效率和成本效益。 |
| [^53] | [An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation.](http://arxiv.org/abs/2305.06924) | 本论文提出了一种基于模仿学习的算法，利用先验知识和与变化环境的交互实现了GENCO的投标策略优化和贝叶斯纳什均衡估计，针对现代电力市场中先验知识未被充分利用导致现有方法不准确和低效的问题进行了改进。 |
| [^54] | [How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications.](http://arxiv.org/abs/2305.06921) | 本文开发了一种基于强化学习的模拟方法来联合设计电力市场，详细阐述了设计电力现货市场、辅助服务市场中的保留能力产品和金融市场中的虚拟竞标产品的方法，并通过案例研究演示了如何选择最佳市场设计选项。 |
| [^55] | [ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps.](http://arxiv.org/abs/2305.06472) | 该论文综述了基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的人工智能（AI）技术在预测与健康管理（PHM）中的广泛应用。这种技术可以实现多模态、多任务、大量数据和超大模型范式，成为AI-2.0的新时代的标志之一。 |
| [^56] | [Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation.](http://arxiv.org/abs/2305.06446) | 该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。 |
| [^57] | [Multi-Robot Coordination and Layout Design for Automated Warehousing.](http://arxiv.org/abs/2305.06436) | 通过优化仓库布局，可以减少拥堵，提高吞吐量，并扩大自动化仓库的可伸缩性。 |
| [^58] | [Phase transitions in the mini-batch size for sparse and dense neural networks.](http://arxiv.org/abs/2305.06435) | 本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。 |
| [^59] | [Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments.](http://arxiv.org/abs/2305.06026) | 本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。 |
| [^60] | [HAISTA-NET: Human Assisted Instance Segmentation Through Attention.](http://arxiv.org/abs/2305.03105) | 该论文提出了一种通过人类辅助实例分割方法，称为HAISTA-NET，增强了现有的实例分割网络，引入了人类指定的部分边界地图，以生成更精确的分割掩模。 |
| [^61] | [How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory.](http://arxiv.org/abs/2305.02485) | 本文提出基于强化学习的方法，设计联合市场以应对电力行业脱碳，实现电力系统的安全和经济效益，并为环境做出贡献。该范型理论的框架将在两部分中详细介绍。 |
| [^62] | [PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs.](http://arxiv.org/abs/2304.09015) | PaTeCon是一种基于模式的知识图谱时间约束挖掘方法，能够自动生成时间约束来维护KG的时间一致性，并在不需要人工专家的情况下准确地检测潜在的时间冲突。 |
| [^63] | [A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization.](http://arxiv.org/abs/2304.08914) | 本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。 |
| [^64] | [Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance.](http://arxiv.org/abs/2304.06715) | 本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。 |
| [^65] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^66] | [Regulating ChatGPT and other Large Generative AI Models.](http://arxiv.org/abs/2302.02337) | 本文将讨论大型生成AI模型的可信AI监管，包括直接监管、数据保护、内容监管和政策建议，并建议使用新术语来区分参与者。本文的目的是确保LGAIMs的可信度并使其为受益所用。 |
| [^67] | [GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective.](http://arxiv.org/abs/2211.08073) | 本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs上对8个经典NLP任务进行评估。 |
| [^68] | [Graph Neural Modeling of Network Flows.](http://arxiv.org/abs/2209.05208) | 本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。 |
| [^69] | [BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification.](http://arxiv.org/abs/2203.01937) | 本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。 |
| [^70] | [Aleatoric uncertainty for Errors-in-Variables models in deep regression.](http://arxiv.org/abs/2105.09095) | 本文提出了一种基于贝叶斯深度回归的方法，利用变量误差模型考虑所使用神经网络的输入所关联的不确定性，并将预测不确定性分解为随机和认识部分。相比于不使用该模型，使用错误变量模型能够提高对已知回归函数的覆盖率，且保持预测性能。 |

# 详细

[^1]: ASNR-MICCAI脑肿瘤分割挑战2023：颅内脑膜瘤

    The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma. (arXiv:2305.07642v1 [cs.CV])

    [http://arxiv.org/abs/2305.07642](http://arxiv.org/abs/2305.07642)

    ASNR-MICCAI脑肿瘤分割挑战2023将提供一个适用于自动诊断颅内脑膜瘤的最先进自动化颅内脑膜瘤分割模型的基准。

    

    脑膜瘤是成人颅内最常见的原发性肿瘤，可能与重大的发病率和死亡率有关。放射科医生、神经外科医生、神经肿瘤学家和放射肿瘤科医生依靠多参数MRI（mpMRI）进行诊断、治疗规划和长期治疗监测；然而，缺乏自动化、客观化和定量化的工具来对mpMRI中的脑膜瘤进行非侵入性评估。BraTS脑膜瘤2023挑战将提供一个社区标准和基于迄今为止最大的专家注释的多标签脑膜瘤mpMRI数据集的最先进自动化颅内脑膜瘤分割模型的基准。挑战参赛者将开发自动化分割模型，预测MRI上的三个不同的脑膜瘤亚区域，包括增强肿瘤、非增强肿瘤核心和周围无增强T2/FLAIR高信号区。模型将使用标准化指标在单独的验证和保留测试数据集上进行评估。

    Meningiomas are the most common primary intracranial tumor in adults and can be associated with significant morbidity and mortality. Radiologists, neurosurgeons, neuro-oncologists, and radiation oncologists rely on multiparametric MRI (mpMRI) for diagnosis, treatment planning, and longitudinal treatment monitoring; yet automated, objective, and quantitative tools for non-invasive assessment of meningiomas on mpMRI are lacking. The BraTS meningioma 2023 challenge will provide a community standard and benchmark for state-of-the-art automated intracranial meningioma segmentation models based on the largest expert annotated multilabel meningioma mpMRI dataset to date. Challenge competitors will develop automated segmentation models to predict three distinct meningioma sub-regions on MRI including enhancing tumor, non-enhancing tumor core, and surrounding nonenhancing T2/FLAIR hyperintensity. Models will be evaluated on separate validation and held-out test datasets using standardized metri
    
[^2]: 基于多任务产品知识图谱预训练的零样本基于项目推荐

    Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training. (arXiv:2305.07633v1 [cs.IR])

    [http://arxiv.org/abs/2305.07633](http://arxiv.org/abs/2305.07633)

    本论文提出了一种使用产品知识图谱预训练模型从预训练的语言模型提取项目特征，以解决零样本项目推荐任务。该方法通过提出四个预训练任务和任务导向的适应层来解决预训练过程中的挑战，并将模型微调到新的推荐任务中。

    

    现有的推荐系统在处理零样本项目（即在训练阶段没有与用户进行过历史互动的项目）时面临困难。虽然最近的工作通过预训练语言模型（PLM）提取通用项目表示，但它们忽略了关键的项目关系。本文提出了一种新的方法，使用产品知识图谱（PKG）对模型进行预训练，以从PLMs中提炼出项目特征来解决零样本项目推荐（ZSIR）任务。我们确定了预训练PKG的三个挑战，即PKG中的多类型关系，项目通用信息和关系之间的语义差异以及从PKG到下游ZSIR任务的域差异。我们通过提出四个预训练任务和新颖的面向任务的适应（ToA）层来解决这些挑战。此外，本文还讨论了如何对新的推荐任务进行微调，使得ToA层适应于ZSIR任务。在18个市场数据集上进行了全面实验。

    Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset ar
    
[^3]: 设计、开发和评估交互式个性化社交机器人，以监测和辅导中风后恢复运动。

    Design, Development, and Evaluation of an Interactive Personalized Social Robot to Monitor and Coach Post-Stroke Rehabilitation Exercises. (arXiv:2305.07632v1 [cs.RO])

    [http://arxiv.org/abs/2305.07632](http://arxiv.org/abs/2305.07632)

    本论文介绍了一个交互式个性化社交机器人用于监测和辅导中风后恢复运动的研究，通过神经网络和规则模型相结合监测和评估患者康复锻炼并提供实时个性化反馈。

    

    社交辅助机器人被越来越多地探索用于改善老年人和残疾人参与与健康相关的锻炼。然而，即使人们有各种各样的身体状况，大多数以前的社交机器人锻炼辅导系统在回馈方面都使用通用的预定义的回馈。这些系统的部署仍然是一个挑战。在本文中，我们通过与治疗师和中风幸存者的访谈来设计、开发和评估一个个性化康复社交机器人。我们与治疗师进行访谈，设计了系统用户的交互方式，然后开发了一个交互式社交机器人锻炼辅导系统。该系统将神经网络模型与基于规则的模型相结合，自动监测和评估患者的康复锻炼，并可以根据个体患者的数据调整，为每个患者生成实时的个性化矫正反馈。

    Socially assistive robots are increasingly being explored to improve the engagement of older adults and people with disability in health and well-being-related exercises. However, even if people have various physical conditions, most prior work on social robot exercise coaching systems has utilized generic, predefined feedback. The deployment of these systems still remains a challenge. In this paper, we present our work of iteratively engaging therapists and post-stroke survivors to design, develop, and evaluate a social robot exercise coaching system for personalized rehabilitation. Through interviews with therapists, we designed how this system interacts with the user and then developed an interactive social robot exercise coaching system. This system integrates a neural network model with a rule-based model to automatically monitor and assess patients' rehabilitation exercises and can be tuned with individual patient's data to generate real-time, personalized corrective feedback for
    
[^4]: 个性化感知的推荐系统中的LMMs模型

    PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])

    [http://arxiv.org/abs/2305.07622](http://arxiv.org/abs/2305.07622)

    本文提出了一个称为PALR的框架，将用户的历史行为与LLMs相结合，生成用户喜欢的物品的推荐。与现有的推荐方法相比，我们的PALR框架实现了最先进的性能。

    

    大型语言模型(LLMs)由于其出色的性能而受到越来越多的关注。本文提出了一种新的框架PALR，将用户的历史行为与LLMs相结合，以生成用户喜欢的物品的推荐。我们首先使用用户/物品互动作为候选检索的指导，然后采用基于LLMs的排序模型生成推荐物品。实验结果表明，与现有的推荐方法相比，我们提出的PALR框架实现了最先进的性能。

    Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
    
[^5]: 深度学习与逻辑推理的可扩展耦合

    Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])

    [http://arxiv.org/abs/2305.07617](http://arxiv.org/abs/2305.07617)

    本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。

    

    在将离散推理与神经网络混合的不断探索中，出现了越来越多的对神经结构具备从自然输入中学习如何解决离散推理或优化问题的兴趣。本文提出了一种可扩展的神经结构以及专门用于学习被表示为离散图模型的 NP-hard 推理问题的约束和标准的损失函数。我们的损失函数解决了 Besag 的伪对数似然的主要限制之一，能够学习高能量函数。我们通过实验证明，它能够有效地从自然输入中学习如何解决 NP-hard 推理问题，如符号、视觉或多解数数独问题，以及蛋白质设计问题的能量优化形式，提高了数据效率、可解释性以及对预测的 \textit{a posteriori} 控制。

    In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
    
[^6]: Spider GAN:利用友好邻居加速GAN训练

    Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training. (arXiv:2305.07613v1 [cs.CV])

    [http://arxiv.org/abs/2305.07613](http://arxiv.org/abs/2305.07613)

    本文提出了一种名为Spider GAN的新方法，通过寻找数据集之间的友好邻居来提高GAN的训练效率，加速收敛，即使是不相关的数据集之间也可以发现对应关系。

    

    GAN的训练是个有挑战性的任务，本文提出了Spider GAN方法，该方法利用图像结构的特点优化生成器的转换，通过定义一种新的度量方式，即有符号启动距离（SID），使其更高效地寻找友好邻居，结果导致更快的收敛，即使在看似不相关的数据集之间也可以找到对应关系。

    Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel approach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efficient by identifying closely related datasets, or a ``friendly neighborhood'' of the target distribution, inspiring the moniker, Spider GAN. To define friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between Tiny-ImageNet 
    
[^7]: 生成式人工智能：教育的影响和应用

    Generative AI: Implications and Applications for Education. (arXiv:2305.07605v1 [cs.CY])

    [http://arxiv.org/abs/2305.07605](http://arxiv.org/abs/2305.07605)

    本文研究了一种生成式人工智能技术——基于大型语言模型的聊天机器人（C-LLM），以及它在复杂学生作品的人工智能评估和审查方面的应用，同时探讨了这种技术在教育中的应用范围。

    

    2022年11月ChatGPT的推出引发了一些教育工作者的恐慌，同时也引发了其他人的热情。在生成式人工智能这个大伞下，ChatGPT是一系列技术的例子，用于提供计算机生成的文本、图像和其他数字化媒体。本文研究了生成式人工智能技术之一——来自大型语言模型的聊天机器人(C-LLM)，以及其在复杂学生作品的人工智能评估和审查方面的应用。在讨论中，本文探讨了生成式人工智能的内在限制，即绑定于语料库及其通过二进制表示的文本表示。在这些限制之内，我们提出了生成式人工智能在教育中新兴和潜在的应用范围。

    The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.
    
[^8]: Opti Code Pro：一种基于启发式搜索的代码重构方法

    Opti Code Pro: A Heuristic Search-based Approach to Code Refactoring. (arXiv:2305.07594v1 [cs.SE])

    [http://arxiv.org/abs/2305.07594](http://arxiv.org/abs/2305.07594)

    本文提出了一种基于启发式搜索的代码重构方法，以解决耦合和内聚性的问题，并提供了具体的演示和实现工具。

    

    本文介绍了一种评估最佳优先搜索方法来进行代码重构的方法。代码重构的动机可能是改善现有程序的设计、结构或实现，而不改变其功能。我们提出使用基于启发式搜索的技术来解决耦合和内聚性的问题，以引导重构过程朝着具有高内聚性和低耦合度的解决方案。我们通过提供随机状态问题的演示示例并创建一个工具来实现Java项目上的算法来评估我们的方法。

    This paper presents an approach that evaluates best-first search methods to code refactoring. The motivation for code refactoring could be to improve the design, structure, or implementation of an existing program without changing its functionality. To solve a very specific problem of coupling and cohesion, we propose using heuristic search-based techniques on an approximation of the full code refactoring problem, to guide the refactoring process toward solutions that have high cohesion and low coupling. We evaluated our approach by providing demonstrative examples of the effectiveness of this approach on random state problems and created a tool to implement the algorithm on Java projects.
    
[^9]: 一种支持核指代信息的问答流式数据记忆模型

    A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information. (arXiv:2305.07565v1 [cs.CL])

    [http://arxiv.org/abs/2305.07565](http://arxiv.org/abs/2305.07565)

    该论文提出了一种记忆模型，在处理流式数据时，通过排练和预期来记忆有关问题回答任务的重要信息。该模型应用自监督机制，通过核指代信息的屏蔽建模任务训练，成功通过短序列数据集和大型基准测试。

    

    现有的问答方法往往假设输入内容（如文件或视频）总是可访问的，以解决任务。相反，记忆网络被引入来模仿人类逐步理解和压缩信息的过程。然而，这些模型只学习如何通过整个网络反向传播错误来维护内存。相反，人类具有提高记忆容量的有效机制，例如排练和预期。受此启发，我们提出了一种记忆模型，通过排练和预期来处理输入以记忆有关问题回答任务的重要信息。所提出的机制在训练期间通过针对核指代信息的屏蔽建模任务进行自监督应用。我们在短序列（bAbI）数据集以及大型基准测试中验证了我们的模型。

    Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-s
    
[^10]: 食品盘中的菜肴检测：自动饮食记录和营养管理的框架。

    Dish detection in food platters: A framework for automated diet logging and nutrition management. (arXiv:2305.07552v1 [cs.CV])

    [http://arxiv.org/abs/2305.07552](http://arxiv.org/abs/2305.07552)

    本文章提出了一种针对食品盘中菜肴检测的自动化方法，主要应用于饮食记录和营养管理，并通过印度餐盘中菜肴的案例研究，使用基于深度学习的物体检测架构，通过大量图像和注释的比较，确定了最先进的模型：ResNet50-v2和EfficientDet D1。

    

    饮食对于生活方式疾病的流行至关重要。精确而轻松的饮食记录是有效的饮食管理和卡路里限制的主要瓶颈之一。鉴于食品布局的视觉复杂性，从食品盘中检测出菜肴是一个具有挑战性的问题。我们提出了一个端到端的计算框架，用于饮食管理，从数据编译、标注和最先进的模型识别到其移动应用程序实现。作为一个案例研究，我们将该框架应用于印度食品盘的上下文中，这些食品盘以其复杂的呈现方式而著称，这给菜肴的自动检测带来了挑战。从最受欢迎的61道印度菜肴开始，我们通过对基于深度学习的物体检测架构进行比较分析，确定最先进的模型。我们首先进行了68005个盘子图像的精心编制，并进行了134814个手动菜肴注释的比较。我们首先比较了十种多标签分类的架构，以识别ResNet50-v2和EfficientDet D1两个模型的性能。

    Diet is central to the epidemic of lifestyle disorders. Accurate and effortless diet logging is one of the significant bottlenecks for effective diet management and calorie restriction. Dish detection from food platters is a challenging problem due to a visually complex food layout. We present an end-to-end computational framework for diet management, from data compilation, annotation, and state-of-the-art model identification to its mobile app implementation. As a case study, we implement the framework in the context of Indian food platters known for their complex presentation that poses a challenge for the automated detection of dishes. Starting with the 61 most popular Indian dishes, we identify the state-of-the-art model through a comparative analysis of deep-learning-based object detection architectures. Rooted in a meticulous compilation of 68,005 platter images with 134,814 manual dish annotations, we first compare ten architectures for multi-label classification to identify Res
    
[^11]: 理解自动微分陷阱

    Understanding Automatic Differentiation Pitfalls. (arXiv:2305.07546v1 [math.NA])

    [http://arxiv.org/abs/2305.07546](http://arxiv.org/abs/2305.07546)

    本文将广泛分类自动微分的问题用法，并通过举例说明每个类别，旨在帮助读者避免意外行为，在发生问题时更容易检测问题，并从自动微分工具中获得更实际的期望。

    

    自动微分（也被称为反向传播、AD、autodiff或算法微分）是一种计算计算机程序导数的流行技术，既准确又高效。然而，由自动微分计算出的导数有时可能被解释为不正确的。这些陷阱在工具和方法中普遍存在。本文广泛分类自动微分的问题用法，并通过示例（如混沌、时间平均振荡、离散化、固定点循环、查找表和线性求解器）来说明每个类别。我们还回顾了调试技术及其在这些情况下的有效性。本文旨在帮助读者避免意外行为，在发生问题时更容易检测问题，并从自动微分工具中获得更实际的期望。

    Automatic differentiation, also known as backpropagation, AD, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs accurately and efficiently. Sometimes, however, the derivatives computed by AD could be interpreted as incorrect. These pitfalls occur systematically across tools and approaches. In this paper we broadly categorize problematic usages of AD and illustrate each category with examples such as chaos, time-averaged oscillations, discretizations, fixed-point loops, lookup tables, and linear solvers. We also review debugging techniques and their effectiveness in these situations. With this article we hope to help readers avoid unexpected behavior, detect problems more easily when they occur, and have more realistic expectations from AD tools.
    
[^12]: WEDGE：基于生成式视觉语言模型构建的多气候自主驾驶数据集

    WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models. (arXiv:2305.07528v1 [cs.CV])

    [http://arxiv.org/abs/2305.07528](http://arxiv.org/abs/2305.07528)

    WEDGE是一个通过生成式视觉语言模型生成的多气候自主驾驶数据集，可用于通过微调最先进的探测器在真实世界气象基准测试中提高性能。

    

    开放道路对于自主感知提出了许多挑战，包括极端天气条件下的能见度差。在好天气数据集上训练的模型经常无法在这些分布区域中进行检测。为了增强感知中的对抗鲁棒性，我们通过提示使用视觉语言生成模型生成了一个名为WEDGE（由DALL-E生成的气象图像）的合成数据集。WEDGE包括3360张手动注释的16种极端天气条件下的图像，支持天气分类和2D物体检测等任务的研究。我们从研究角度分析了WEDGE，验证了其在极端天气下的自主感知效果。我们建立了分类和检测的基准性能，分别为53.87%的测试准确度和45.41 mAP。最重要的是，WEDGE可用于微调最先进的探测器，将在真实世界气象基准测试（如DAWN）上的SOTA性能提高4.48 A。

    The open road poses many challenges to autonomous perception, including poor visibility from extreme weather conditions. Models trained on good-weather datasets frequently fail at detection in these out-of-distribution settings. To aid adversarial robustness in perception, we introduce WEDGE (WEather images by DALL-E GEneration): a synthetic dataset generated with a vision-language generative model via prompting. WEDGE consists of 3360 images in 16 extreme weather conditions manually annotated with 16513 bounding boxes, supporting research in the tasks of weather classification and 2D object detection. We have analyzed WEDGE from research standpoints, verifying its effectiveness for extreme-weather autonomous perception. We establish baseline performance for classification and detection with 53.87% test accuracy and 45.41 mAP. Most importantly, WEDGE can be used to fine-tune state-of-the-art detectors, improving SOTA performance on real-world weather benchmarks (such as DAWN) by 4.48 A
    
[^13]: 联合磁共振序列优化超越了纯神经网络方法的自旋回波磁共振成像超分辨率

    Joint MR sequence optimization beats pure neural network approaches for spin-echo MRI super-resolution. (arXiv:2305.07524v1 [physics.med-ph])

    [http://arxiv.org/abs/2305.07524](http://arxiv.org/abs/2305.07524)

    该论文提出了一种联合磁共振序列优化和神经网络方法来实现MR图像的超分辨率，通过消除序列参数对实际图像分辨率的影响，以及优化射频脉冲列设计来提高神经网络执行超分辨率任务的性能。

    

    目前的MRI超分辨率方法仅使用从典型临床序列中获取的已有对比度作为神经网络(NN)的输入。在涡轮自旋回波序列(TSE)中，序列参数对实际获得图像的分辨率有很大影响，从而对NN的性能有重要影响。我们提出了一种已知运算符学习方法，通过端到端优化MR序列和神经网络参数来进行SR-TSE。这种基于MR物理学的训练过程联合优化了质子密度(PD)加权TSE和T2加权TSE的射频脉冲列，以及随后应用的卷积神经网络来预测相应的超分辨率PDw和T2w TSE图像。所发现的射频脉冲列设计为NN生成了最优信号来执行SR任务。我们的方法从基于模拟的优化推广到体内实测和获得的物理学知识的SR图像。

    Current MRI super-resolution (SR) methods only use existing contrasts acquired from typical clinical sequences as input for the neural network (NN). In turbo spin echo sequences (TSE) the sequence parameters can have a strong influence on the actual resolution of the acquired image and have consequently a considera-ble impact on the performance of the NN. We propose a known-operator learning approach to perform an end-to-end optimization of MR sequence and neural net-work parameters for SR-TSE. This MR-physics-informed training procedure jointly optimizes the radiofrequency pulse train of a proton density- (PD-) and T2-weighted TSE and a subsequently applied convolutional neural network to predict the corresponding PDw and T2w super-resolution TSE images. The found radiofrequency pulse train designs generate an optimal signal for the NN to perform the SR task. Our method generalizes from the simulation-based optimi-zation to in vivo measurements and the acquired physics-informed SR ima
    
[^14]: PillarAcc: 稀疏点云Pillars加速器——边缘设备上实时三维物体检测

    PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices. (arXiv:2305.07522v1 [cs.AR])

    [http://arxiv.org/abs/2305.07522](http://arxiv.org/abs/2305.07522)

    PillarAcc提出了一种稀疏算法-硬件协同设计，有效增强基于Pillar的三维物体检测网络，实现高效率的计算减少。

    

    使用点云(PC)数据进行三维物体检测对于自动驾驶感知管道至关重要，其中高效的编码是满足严格资源和延迟要求的关键。PointPillars是一种广泛采用的鸟瞰图(BEV)编码方法，将三维点云数据聚合到二维pillar中，实现高精度的三维物体检测。然而，大多数采用PointPillar的最先进方法都忽视了pillar编码的固有稀疏性，错失了大量计算减少的机会。在本研究中，我们提出了一种开创性的算法-硬件协同设计，加速稀疏卷积处理，并最大限度地利用pillar的稀疏性来进行基于pillar的三维物体检测网络。我们使用先进的pillar pruning方法对稀疏化机会进行了研究，并实现了精确性和稀疏性之间的最优平衡。我们引入了PillarAcc，这是一种最先进的稀疏支持机制，通过输入具有线性复杂度的方法增强稀疏pillar卷积。

    3D object detection using point cloud (PC) data is vital for autonomous driving perception pipelines, where efficient encoding is key to meeting stringent resource and latency requirements. PointPillars, a widely adopted bird's-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars for high-accuracy 3D object detection. However, most state-of-the-art methods employing PointPillar overlook the inherent sparsity of pillar encoding, missing opportunities for significant computational reduction. In this study, we propose a groundbreaking algorithm-hardware co-design that accelerates sparse convolution processing and maximizes sparsity utilization in pillar-based 3D object detection networks. We investigate sparsification opportunities using an advanced pillar-pruning method, achieving an optimal balance between accuracy and sparsity. We introduce PillarAcc, a state-of-the-art sparsity support mechanism that enhances sparse pillar convolution through linear complexity input
    
[^15]: 学习去学习：机器去学习的综述

    Learn to Unlearn: A Survey on Machine Unlearning. (arXiv:2305.07512v1 [cs.LG])

    [http://arxiv.org/abs/2305.07512](http://arxiv.org/abs/2305.07512)

    本综述总结了机器去学习技术，用于从训练模型中删除敏感数据，但重新训练ML模型往往不可行。针对这个挑战，需要开发强大的模型以缓解公平性问题。

    

    机器学习模型包含私密信息，实现被遗忘权是许多数据应用的难题。机器去学习已成为从训练模型中删除敏感数据的替代方法，但重新训练机器学习模型往往是不可行的。本综述提供了机器去学习技术的简要评估，涵盖了精确和近似方法、可能的攻击以及验证方法。本综述比较了每种方法的优点和局限性，并使用Deltagrad精确机器去学习方法评估了它们的性能。本综述还强调了挑战，如非IID删除的强大模型，以缓解公平性问题。总的来说，本综述提供了机器去学习技术和应用的全面概述，并指出了这个不断发展的领域的未来研究方向。本综述旨在成为寻求机器去学习资料的研究人员和从业者的有价值资源。

    Machine Learning (ML) models contain private information, and implementing the right to be forgotten is a challenging privacy issue in many data applications. Machine unlearning has emerged as an alternative to remove sensitive data from a trained model, but completely retraining ML models is often not feasible. This survey provides a concise appraisal of Machine Unlearning techniques, encompassing both exact and approximate methods, probable attacks, and verification approaches. The survey compares the merits and limitations each method and evaluates their performance using the Deltagrad exact machine unlearning method. The survey also highlights challenges like the pressing need for a robust model for non-IID deletion to mitigate fairness issues. Overall, the survey provides a thorough synopsis of machine unlearning techniques and applications, noting future research directions in this evolving field. The survey aims to be a valuable resource for researchers and practitioners seeking
    
[^16]: 医学图像中可解释的人工智能：一项调查

    eXplainable Artificial Intelligence on Medical Images: A Survey. (arXiv:2305.07511v1 [cs.LG])

    [http://arxiv.org/abs/2305.07511](http://arxiv.org/abs/2305.07511)

    这项调查分析了XAI领域中应用于医学诊断的研究，以便解释黑匣子模型的结果，针对的疾病包括癌症和COVID-19。

    

    在过去的几年中，应用深度学习于医疗领域的研究数量迅速增加。为了向参与医学检查的所有人解释这些结果，需要对这些模型进行严格的评估。最近，在机器学习领域中出现了可解释的人工智能，也称为XAI，旨在解释这些黑匣子模型的结果，以便进行所需评估。这项调查分析了XAI领域中针对医学诊断研究的几项最新研究，允许有关多种不同疾病（如癌症和COVID-19）的机器学习结果的可解释性。

    Over the last few years, the number of works about deep learning applied to the medical field has increased enormously. The necessity of a rigorous assessment of these models is required to explain these results to all people involved in medical exams. A recent field in the machine learning area is explainable artificial intelligence, also known as XAI, which targets to explain the results of such black box models to permit the desired assessment. This survey analyses several recent studies in the XAI field applied to medical diagnosis research, allowing some explainability of the machine learning results in several different diseases, such as cancers and COVID-19.
    
[^17]: 超越不变表示学习：线性可对齐的潜在空间用于高效闭合形式领域自适应

    Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation. (arXiv:2305.07500v1 [cs.LG])

    [http://arxiv.org/abs/2305.07500](http://arxiv.org/abs/2305.07500)

    本文提出了一种新的基于最优输运（OT）的领域自适应（DA）方法，通过学习一个嵌入空间，使得OT问题的解是最优且计算量较少的，适用于同质和异质的DA设置。

    

    最优输运（OT）是一种强大的几何工具，用于比较和对齐概率测度，遵循最小努力原则。在机器学习（ML）中，OT的许多成功应用之一是领域自适应（DA），这是一种研究领域，其目标是将分类器从一个带标签的领域转移到另一个类似但不同的未标记或稀疏标记的领域。我们提出了一种全新的基于OT的DA方法，该方法使用由仿射映射给出的OT问题的闭式解，并学习了一个嵌入空间，使得该解是最优且计算量较少。我们展示了我们的方法适用于同质和异质的DA设置。

    Optimal transport (OT) is a powerful geometric tool used to compare and align probability measures following the least effort principle. Among many successful applications of OT in machine learning (ML), domain adaptation (DA) -- a field of study where the goal is to transfer a classifier from one labelled domain to another similar, yet different unlabelled or scarcely labelled domain -- has been historically among the most investigated ones. This success is due to the ability of OT to provide both a meaningful discrepancy measure to assess the similarity of two domains' distributions and a mapping that can project source domain data onto the target one. In this paper, we propose a principally new OT-based approach applied to DA that uses the closed-form solution of the OT problem given by an affine mapping and learns an embedding space for which this solution is optimal and computationally less complex. We show that our approach works in both homogeneous and heterogeneous DA settings 
    
[^18]: 自适应保守型自动驾驶规划器的设计与实现

    Dynamically Conservative Self-Driving Planner for Long-Tail Cases. (arXiv:2305.07497v1 [cs.RO])

    [http://arxiv.org/abs/2305.07497](http://arxiv.org/abs/2305.07497)

    本论文提出了一种动态保守规划器方法，能够根据每种情况的“长尾”率自动调整保守程度，以更好地应对SDVs在实际驾驶中遇到的罕见关键情况。

    

    自动驾驶汽车（SDVs）在实际驾驶中经常面临“长尾”挑战，即SDVs将不断遇到在其训练数据集中可能不存在的罕见安全关键情况。一些安全保证规划器通过在所有可能的情况下保守处理，来解决这个问题，但这可能会极大地影响驾驶能力。为此，本文提出了一种名为动态保守规划器（DCP）的方法，根据每种情况的“长尾”率自动调整保守程度。首先，我们将“长尾”率定义为SDV通过驾驶案例的信心水平。这个率表示安全关键事件的概率，并使用历史数据的统计自助法进行估计。然后，通过强化学习，设计了一个包含不同保守程度候选策略的规划器。最终策略基于估计的“长尾”率进行了优化。通过这种方式，DCP能够更好地应对长尾案例，提高安全性和驾驶性能。

    Self-driving vehicles (SDVs) are becoming reality but still suffer from "long-tail" challenges during natural driving: the SDVs will continually encounter rare, safety-critical cases that may not be included in the dataset they were trained. Some safety-assurance planners solve this problem by being conservative in all possible cases, which may significantly affect driving mobility. To this end, this work proposes a method to automatically adjust the conservative level according to each case's "long-tail" rate, named dynamically conservative planner (DCP). We first define the "long-tail" rate as an SDV's confidence to pass a driving case. The rate indicates the probability of safe-critical events and is estimated using the statistics bootstrapped method with historical data. Then, a reinforcement learning-based planner is designed to contain candidate policies with different conservative levels. The final policy is optimized based on the estimated "long-tail" rate. In this way, the DCP
    
[^19]: 基于图库采样的人脸识别快速准确方法

    Gallery Sampling for Robust and Fast Face Identification. (arXiv:2305.07495v1 [cs.CV])

    [http://arxiv.org/abs/2305.07495](http://arxiv.org/abs/2305.07495)

    该论文提出了一种鲁棒且快速的人脸识别方法，通过对图库数据进行采样处理，在减少搜索时间的同时，对异常图像如错误标记、低质量和信息较少的图像具有较强的鲁棒性。在5.4M网络图像数据集上，我们的方法在FNIR方面达到了0.0975，而传统方法为0.3891。

    

    深度学习方法在人脸识别中取得了极大的成功。但是，为了提高性能，收集和标记尽可能多的图像是一项重要的任务。然而，标识数据和检查大量图像数据的质量是困难的任务，并且在处理大数据时不能避免错误。之前的工作一直试图解决训练数据中的问题，然而，如果错误出现在人脸识别的图库数据中，会带来更为严重的问题。我们提出了一种对异常值具有鲁棒性的图库数据采样方法，包括错误标记、低质量和信息较少的图像，并减少了搜索时间。我们提出的采样-修剪和采样-生成方法在5.4M个名人网络图像数据集上显著提高了人脸识别性能。在FPIR=0.01的情况下，我们的方法在FNIR方面达到了0.0975，而传统方法则显示为0.3891。平均特征向量数量减少了

    Deep learning methods have been achieved brilliant results in face recognition. One of the important tasks to improve the performance is to collect and label images as many as possible. However, labeling identities and checking qualities of large image data are difficult task and mistakes cannot be avoided in processing large data. Previous works have been trying to deal with the problem only in training domain, however it can cause much serious problem if the mistakes are in gallery data of face identification. We proposed gallery data sampling methods which are robust to outliers including wrong labeled, low quality, and less-informative images and reduce searching time. The proposed sampling-by-pruning and sampling-by-generating methods significantly improved face identification performance on our 5.4M web image dataset of celebrities. The proposed method achieved 0.0975 in terms of FNIR at FPIR=0.01, while conventional method showed 0.3891. The average number of feature vectors for
    
[^20]: 针对自动驾驶的深度强化学习不确定性的识别、评估和边界确定 (arXiv:2305.07487v1 [cs.AI])

    Identify, Estimate and Bound the Uncertainty of Reinforcement Learning for Autonomous Driving. (arXiv:2305.07487v1 [cs.AI])

    [http://arxiv.org/abs/2305.07487](http://arxiv.org/abs/2305.07487)

    本文提出了一种方法来限制自动驾驶中深度强化学习模型的决策不可靠性，以保护决策的可靠性，该方法通过估计和限制策略的性能不确定性来实现。

    

    深度强化学习(DRL)已成为开发更智能化自动驾驶汽车(AVs)的一种有前途的方法。AVs上的典型DRL应用是训练基于神经网络的驾驶策略。然而，神经网络的黑盒特性可能导致不可预测的决策失误，使这些AVs不可靠。为此，本文提出了一种方法来识别和保护DRL驾驶策略的不可靠决策。基本思想是估计和限制策略的性能不确定性，该不确定性量化由于训练数据不足或网络拟合误差导致的潜在性能下降。通过限制不确定性，DRL模型的性能始终优于基线策略。由不足的数据引起的不确定性采用自助法估计。然后，使用集成网络估计由网络拟合误差引起的不确定性。最后，将基线策略添加为性能下限。

    Deep reinforcement learning (DRL) has emerged as a promising approach for developing more intelligent autonomous vehicles (AVs). A typical DRL application on AVs is to train a neural network-based driving policy. However, the black-box nature of neural networks can result in unpredictable decision failures, making such AVs unreliable. To this end, this work proposes a method to identify and protect unreliable decisions of a DRL driving policy. The basic idea is to estimate and constrain the policy's performance uncertainty, which quantifies potential performance drop due to insufficient training data or network fitting errors. By constraining the uncertainty, the DRL model's performance is always greater than that of a baseline policy. The uncertainty caused by insufficient data is estimated by the bootstrapped method. Then, the uncertainty caused by the network fitting error is estimated using an ensemble network. Finally, a baseline policy is added as the performance lower bound to a
    
[^21]: BactInt:一种面向领域的迁移学习方法和一个语料库，用于从生物医学文本中提取细菌间相互作用

    BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text. (arXiv:2305.07468v1 [cs.IR])

    [http://arxiv.org/abs/2305.07468](http://arxiv.org/abs/2305.07468)

    BactInt是一种面向领域的自动化方法，使用迁移学习从生物医学文本中提取细菌间相互作用并挖掘特定细菌群之间的关系。公开可用的BactInt语料库标注了1200篇PubMed摘要。

    

    生物学领域中不同类型微生物在生物学空间中发挥着重要作用，这些微生物之间的相互作用是微生物群落结构的基本构建单元。生物医学文本中的证据可作为预测这种相互作用的可靠来源。然而，阅读海量且不断增长的生物医学文献是一项耗时并令人望而生畏的工作。这就必然需要开发自动化方法来准确提取生物医学文献中所报道的细菌关系。本文介绍了一种从生物医学文献中自动提取微生物相互作用（特别是细菌之间）的方法以及使用迁移学习来提高其准确性的方法。我们还描述了一个管道，用于挖掘特定细菌群之间的关系。此外，我们还介绍了第一个公开可用的Bacterial Interaction (BactInt)语料库，其中包括1200篇PubMed摘要，注释有细菌间关系。

    The community of different types of microbes present in a biological niche plays a very important role in functioning of the system. The crosstalk or interactions among the different microbes contributes to the building blocks of such microbial community structures. Evidence reported in biomedical text serves as a reliable source for predicting such interactions. However, going through the vast and ever-increasing volume of biomedical literature is an intimidating and time consuming process. This necessitates development of automated methods capable of accurately extracting bacterial relations reported in biomedical literature. In this paper, we introduce a method for automated extraction of microbial interactions (specifically between bacteria) from biomedical literature along with ways of using transfer learning to improve its accuracy. We also describe a pipeline using which relations among specific bacteria groups can be mined. Additionally, we introduce the first publicly availabl
    
[^22]: 金融科技领域中强化学习的系统综述

    Systematic Review on Reinforcement Learning in the Field of Fintech. (arXiv:2305.07466v1 [q-fin.CP])

    [http://arxiv.org/abs/2305.07466](http://arxiv.org/abs/2305.07466)

    本文综述了近年来强化学习在金融科技领域的应用，包括组合优化、降低信用风险、投资资本管理、利润最大化、有效的推荐系统和更好的价格策略确定。本文通过PRISMA技术筛选文献，突出了强化学习在Fintech中的预测精度、复杂性、可扩展性、风险、盈利能力和业绩，旨在探讨其在Fintech领域中的实际贡献。

    

    强化学习在金融科技（Fintech）领域的应用近年来备受关注。毫无疑问，通过其丰富的能力和高效性，强化学习在Fintech领域取得了卓越成果。本系统性综述的目的是开展一项探索性研究，研究强化学习与Fintech之间的相关性，以突出其预测精度、复杂性、可扩展性、风险、盈利能力和业绩。强化学习在金融或Fintech领域的主要用途包括组合优化、降低信用风险、投资资本管理、利润最大化、有效的推荐系统和更好的价格策略确定。一些研究已经探讨了强化学习对金融机构业绩的实际贡献。本综述包含了2018年以来发表的最新研究。使用PRISMA技术开展了本次综述研究，重点关注发现和筛选受纳入的研究。

    Applications of Reinforcement Learning in the Finance Technology (Fintech) have acquired a lot of admiration lately. Undoubtedly Reinforcement Learning, through its vast competence and proficiency, has aided remarkable results in the field of Fintech. The objective of this systematic survey is to perform an exploratory study on a correlation between reinforcement learning and Fintech to highlight the prediction accuracy, complexity, scalability, risks, profitability and performance. Major uses of reinforcement learning in finance or Fintech include portfolio optimization, credit risk reduction, investment capital management, profit maximization, effective recommendation systems, and better price setting strategies. Several studies have addressed the actual contribution of reinforcement learning to the performance of financial institutions. The latest studies included in this survey are publications from 2018 onward. The survey is conducted using PRISMA technique which focuses on the re
    
[^23]: 超越提示：探索混合主动合作创造性系统的设计空间。

    Beyond Prompts: Exploring the Design Space of Mixed-Initiative Co-Creativity Systems. (arXiv:2305.07465v1 [cs.AI])

    [http://arxiv.org/abs/2305.07465](http://arxiv.org/abs/2305.07465)

    本文介绍了一种新的人工智能系统设计空间，探索了人类和人工智能系统之间各种不同的创意意图沟通方式，发现MI-CC系统中更多由人工智能发起的贡献是用户优先选择的，这说明了超越提示，探索更多的MI-CC设计方案可以提高人工智能协作体验。

    

    人工智能生成系统已经针对图像、代码、故事和游戏生成等方面开发出来，旨在促进人类创造力的发挥。然而，最近神经生成系统的研究始终强调一种人工智能系统与用户进行互动的方式：用户提供说明（通常是提示）然后AI系统生成内容。然而，在人类和人工智能协作的模式中还有其他配置，例如共创（CC），即人类和人工智能系统都可以贡献内容的创造，以及混合主动（MI）模式，即人类和人工智能系统都可以发起内容的更改请求。本文定义了一个假想的人工智能系统设计空间，由不同的方式组成，人类和人工智能系统可以互相沟通创造性意图。我们进行了一项人类参与研究，有185名参与者，了解用户想要如何与MI-CC系统进行不同配置的交互。我们发现，MI-CC系统中更多由人工智能发起的贡献是用户优先选择的，而非仅依靠人类发起的输入。我们的研究结果表明，探索超越提示的更广泛的MI-CC设计方案可以导致更有效和令人满意的人工智能协作体验。

    Generative Artificial Intelligence systems have been developed for image, code, story, and game generation with the goal of facilitating human creativity. Recent work on neural generative systems has emphasized one particular means of interacting with AI systems: the user provides a specification, usually in the form of prompts, and the AI system generates the content. However, there are other configurations of human and AI coordination, such as co-creativity (CC) in which both human and AI systems can contribute to content creation, and mixed-initiative (MI) in which both human and AI systems can initiate content changes. In this paper, we define a hypothetical human-AI configuration design space consisting of different means for humans and AI systems to communicate creative intent to each other. We conduct a human participant study with 185 participants to understand how users want to interact with differently configured MI-CC systems. We find out that MI-CC systems with more extensi
    
[^24]: 使用深度强化学习优化内存映射

    Optimizing Memory Mapping Using Deep Reinforcement Learning. (arXiv:2305.07440v1 [cs.PF])

    [http://arxiv.org/abs/2305.07440](http://arxiv.org/abs/2305.07440)

    本文提出了一种使用强化学习解决机器学习程序中内存映射问题的方法。

    

    资源调度和分配是许多高影响系统的关键组成部分，涵盖拥塞控制到云计算。在这篇论文中，我们专注于调度问题的一个特定实例，即编译机器学习程序期间出现的内存映射问题：即将张量映射到不同的内存层以优化执行时间。我们介绍了一种使用强化学习解决内存映射问题的方法。使用强化学习是解决顺序决策问题和高维数据输入组合搜索空间的解决方案。

    Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time.  We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memo
    
[^25]: 游戏中的人工智能伦理学

    The Ethics of AI in Games. (arXiv:2305.07392v1 [cs.HC])

    [http://arxiv.org/abs/2305.07392](http://arxiv.org/abs/2305.07392)

    本文调查了人工智能应用于游戏中的伦理问题，从情感回路的视角讨论了AI在游戏开发中所面临的三个伦理挑战：人工诱导情感的伦理界限，隐私和安全游戏空间之间的权衡以及在游戏适应中应用的检测所带来的透明度和用户控制问题。

    

    视频游戏是人机交互最丰富和最受欢迎的形式之一，因此，它们在理解人类行为和情感方面的作用至关重要。随着游戏业逐渐采用人工智能（AI）工具，一系列伦理问题出现。然而，这些问题在视频游戏领域中迄今尚未得到广泛讨论。由于缺乏关于游戏中应用AI的伦理综述，我们调查了这个领域的现状，并从情感回路的整体视角讨论了这些系统的伦理考虑。通过这个回路的组成部分，我们研究了AI在视频游戏开发中所面临的伦理挑战。情感唤起凸显了人工诱导情感的伦理界限；感知展示了隐私和安全游戏空间之间的权衡；而检测，在游戏适应中的应用，对透明度和用户控制造成了挑战。

    Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale. As artificial intelligence (AI) tools are gradually adopted by the game industry a series of ethical concerns arise. Such concerns, however, have so far not been extensively discussed in a video game context. Motivated by the lack of a comprehensive review of the ethics of AI as applied to games, we survey the current state of the art in this area and discuss ethical considerations of these systems from the holistic perspective of the affective loop. Through the components of this loop, we study the ethical challenges that AI faces in video game development. Elicitation highlights the ethical boundaries of artificially induced emotions; sensing showcases the trade-off between privacy and safe gaming spaces; and detection, as utilised during in-game adaptation, poses challenges to transparency and
    
[^26]: ChatGPT是一个好的因果推断器吗？全面评估

    Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])

    [http://arxiv.org/abs/2305.07375](http://arxiv.org/abs/2305.07375)

    本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。

    

    因果推理能力对于众多NLP应用至关重要。尽管ChatGPT在各种NLP任务中表现出令人印象深刻的新兴能力，但ChatGPT在因果推理方面的表现如何仍不清楚。本文对ChatGPT的因果推理能力进行了首次全面评估。实验证明，ChatGPT不是一个好的因果推理者，但是是一个好的因果解释者。此外，ChatGPT在因果推理方面存在严重的幻觉，可能是由于自然语言中因果关系和非因果关系的报告偏见，以及ChatGPT的升级过程，如RLHF。在上下文学习（ICL）和思维链（COT）技术方面，可能会进一步加剧这种因果幻觉。此外，ChatGPT的因果推理能力对于在提示中表达因果概念的词语非常敏感，并且封闭提示比开放提示表现更好。对于句子中的事件，ChatGPT擅长捕捉明确的因果关系。

    Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
    
[^27]: S-REINFORCE：一种神经符号策略梯度方法以实现可解释强化学习

    S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning. (arXiv:2305.07367v1 [cs.LG])

    [http://arxiv.org/abs/2305.07367](http://arxiv.org/abs/2305.07367)

    S-REINFORCE是一种神经符号策略梯度方法，利用神经网络和符号回归器生成数字和符号策略，从而为强化学习任务提供可解释解决方案。

    

    本文提出了一种新颖的强化学习算法S-REINFORCE，旨在为动态决策任务生成可解释策略。该算法利用两种类型的函数逼近器，即神经网络（NN）和符号回归器（SR），分别生成数字和符号策略。NN组件通过策略梯度学习生成可能操作的数字概率分布，而SR组件则捕获与操作概率相关的状态间关系的函数形式。然后通过重要性抽样利用SR生成的策略表达式改进学习过程中接收的奖励。我们在具有低和高维行动空间的各种动态决策问题上测试了提出的S-REINFORCE算法，结果展示了其实现可解释解决方案的有效性和影响力。通过利用NN和SR的优势，S-REINFORCE提供了一种新方法来生成强化学习任务的可解释策略。

    This paper presents a novel RL algorithm, S-REINFORCE, which is designed to generate interpretable policies for dynamic decision-making tasks. The proposed algorithm leverages two types of function approximators, namely Neural Network (NN) and Symbolic Regressor (SR), to produce numerical and symbolic policies, respectively. The NN component learns to generate a numerical probability distribution over the possible actions using a policy gradient, while the SR component captures the functional form that relates the associated states with the action probabilities. The SR-generated policy expressions are then utilized through importance sampling to improve the rewards received during the learning process. We have tested the proposed S-REINFORCE algorithm on various dynamic decision-making problems with low and high dimensional action spaces, and the results demonstrate its effectiveness and impact in achieving interpretable solutions. By leveraging the strengths of both NN and SR, S-REINF
    
[^28]: 规范多智能体系统中的多值对齐：进化优化方法

    Multi-Value Alignment in Normative Multi-Agent System: Evolutionary Optimisation Approach. (arXiv:2305.07366v1 [cs.MA])

    [http://arxiv.org/abs/2305.07366](http://arxiv.org/abs/2305.07366)

    本研究提出一种通过多目标进化算法来实现多值推广的规范多智能体系统方法，并考虑系统中智能体的异质性和同时对齐多个价值的要求。

    

    规范多智能体系统中的价值对齐是用于推广某种价值并确保智能系统与人类价值的一致性行为的方法。然而，目前的文献限于将有效的规范纳入单一价值对齐中，而没有考虑到智能体的异质性和同时推进和对齐多个价值的要求。本研究提出了一个多值推广模型，使用多目标进化算法生成与异质智能体和系统的多个同时值对齐的最优参数规范集。为了了解这个复杂问题的各个方面，使用了几种进化算法来找到一组优化的规范参数，考虑了两个玩具税收场景的两个和五个值。从不同的角度分析了结果，以展示所选进化算法对解决方案的影响。

    Value-alignment in normative multi-agent systems is used to promote a certain value and to ensure the consistent behavior of agents in autonomous intelligent systems with human values. However, the current literature is limited to incorporation of effective norms for single value alignment with no consideration of agents' heterogeneity and the requirement of simultaneous promotion and alignment of multiple values. This research proposes a multi-value promotion model that uses multi-objective evolutionary algorithms to produce the optimum parametric set of norms that is aligned with multiple simultaneous values of heterogeneous agents and the system. To understand various aspects of this complex problem, several evolutionary algorithms were used to find a set of optimised norm parameters considering two toy tax scenarios with two and five values are considered. The results are analysed from different perspectives to show the impact of a selected evolutionary algorithm on the solution, a
    
[^29]: 实现从天城文辛迪到波斯-阿拉伯文辛迪的音译

    Towards Transliteration between Sindhi Scripts from Devanagari to Perso-Arabic. (arXiv:2305.07365v1 [cs.CL])

    [http://arxiv.org/abs/2305.07365](http://arxiv.org/abs/2305.07365)

    本文提出了一种将天城文辛迪转换为波斯-阿拉伯文辛迪的技术，通过混合使用基于规则和概率模型，系统取得了99.64％的准确率。

    

    本文提出一种将天城文辛迪转换为波斯-阿拉伯文辛迪的转换技术。我们采用了一种混合方法，其中部分文本使用基于规则的方法进行转换，如果有歧义，则使用概率模型进行解决。使用这种方法，系统总体准确率达到99.64％。

    In this paper, we have shown a script conversion (transliteration) technique that converts Sindhi text in the Devanagari script to the Perso-Arabic script. We showed this by incorporating a hybrid approach where some part of the text is converted using a rule base and in case an ambiguity arises then a probabilistic model is used to resolve the same. Using this approach, the system achieved an overall accuracy of 99.64%.
    
[^30]: 通过适当翻译命名实体来提高神经机器翻译的质量

    Improving the Quality of Neural Machine Translation Through Proper Translation of Name Entities. (arXiv:2305.07360v1 [cs.CL])

    [http://arxiv.org/abs/2305.07360](http://arxiv.org/abs/2305.07360)

    本文提出了一种方法，通过将命名实体作为预处理步骤进行翻译/音译，以提高神经机器翻译的质量，实验结果显示该方法能够正确翻译大多数的命名实体，准确率高达99.52％。

    

    本文介绍了一种通过将命名实体作为预处理步骤进行翻译/音译来提高神经机器翻译质量的方法。通过实验，我们展示了我们系统的性能增益。对于评估，我们考虑了三种类型的命名实体，即人名、地名和组织名。系统能够正确地翻译大多数的命名实体。对于人名，准确率为99.86％，对于地名，准确率为99.63％，对于组织名，准确率为99.05％。总体而言，系统的准确率为99.52％。

    In this paper, we have shown a method of improving the quality of neural machine translation by translating/transliterating name entities as a preprocessing step. Through experiments we have shown the performance gain of our system. For evaluation we considered three types of name entities viz person names, location names and organization names. The system was able to correctly translate mostly all the name entities. For person names the accuracy was 99.86%, for location names the accuracy was 99.63% and for organization names the accuracy was 99.05%. Overall, the accuracy of the system was 99.52%
    
[^31]: 基于深度学习的单目航天器姿态估计综述：当前状态、限制和前景。

    A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects. (arXiv:2305.07348v1 [cs.CV])

    [http://arxiv.org/abs/2305.07348](http://arxiv.org/abs/2305.07348)

    本文评估了基于深度学习的单目航天器姿态估计的现有方法，总结了部署该方法在现实任务中仍需克服的挑战。

    

    估算不配合的航天器姿态是一项重要的计算机视觉问题，可以实现在轨自动视觉系统的部署，其应用范围从轨道维修到太空碎片清除。随着计算机视觉的趋势，越来越多的工作在利用深度学习（DL）方法来解决此问题。但是尽管在研究阶段获得了有希望的结果，仍存在阻止这种方法在现实任务中使用的主要挑战。特别是，部署这种计算密集型算法仍然受到少量研究，而在合成图像上进行训练和在真实图像上进行测试时性能下降仍然需要减轻。本文主要目的是全面描述当前基于DL的航天器姿态估计方法，辅助确定有效部署DL的航天器姿态估计在现实任务中的限制。

    Estimating the pose of an uncooperative spacecraft is an important computer vision problem for enabling the deployment of automatic vision-based systems in orbit, with applications ranging from on-orbit servicing to space debris removal. Following the general trend in computer vision, more and more works have been focusing on leveraging Deep Learning (DL) methods to address this problem. However and despite promising research-stage results, major challenges preventing the use of such methods in real-life missions still stand in the way. In particular, the deployment of such computation-intensive algorithms is still under-investigated, while the performance drop when training on synthetic and testing on real images remains to mitigate. The primary goal of this survey is to describe the current DL-based methods for spacecraft pose estimation in a comprehensive manner. The secondary goal is to help define the limitations towards the effective deployment of DL-based spacecraft pose estimat
    
[^32]: 利用跨语言一致性规范化改进零样本多语言神经机器翻译

    Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization. (arXiv:2305.07310v1 [cs.CL])

    [http://arxiv.org/abs/2305.07310](http://arxiv.org/abs/2305.07310)

    本文提出了一种跨语言一致性规范化方法CrossConST，用于改进零样本多语言神经机器翻译模型的性能。实验结果表明，CrossConST可以缩小语言之间的表示差距，提高零样本翻译的准确性和多样性。

    

    多语言神经机器翻译（NMT）模型有很强的零样本翻译能力，能够在未经过训练的语言对之间进行直接翻译。然而，为了实现从有监督方向到零样本方向的良好转移性能，需要让多语言NMT模型学习到跨不同语言的通用表示。本文引入了跨语言一致性规范化CrossConST，以弥合不同语言之间的表示差距，并提高零样本翻译性能。理论分析表明，CrossConST隐含地最大化了零样本翻译的概率分布，并在低资源和高资源基准测试中取得了稳定的提升。实验分析还证明，CrossConST可以缩小句子表示差距并更好地对齐表示空间。鉴于所提出的方法的普适性和可扩展性，CrossConST可以方便地应用于其他多语言NMT模型中，以进一步提高零样本翻译性能。

    The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among different languages and boost zero-shot translation performance. The theoretical analysis shows that CrossConST implicitly maximizes the probability distribution for zero-shot translation, and the experimental results on both low-resource and high-resource benchmarks show that CrossConST consistently improves the translation performance. The experimental analysis also proves that CrossConST could close the sentence representation gap and better align the representation space. Given the universality and s
    
[^33]: CLIP-Count：面向文本引导下零样本物体计数

    CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v1 [cs.CV])

    [http://arxiv.org/abs/2305.07304](http://arxiv.org/abs/2305.07304)

    该研究提出了CLIP-Count，一种基于零样本文本引导的物体计数方法，不需要对特定对象类别进行微调，通过引入补丁-文本对比损失和分层的patch-text交互模块，获得了高效的密集预测结果。

    

    最近视觉-语言模型的进展显示出卓越的零样本文本-图像匹配能力，可转移到对象检测和分割等下游任务。然而，将这些模型适应于目标计数——估计图像中对象的数量，仍然是一个巨大的挑战。在本研究中，我们进行首次探索，将视觉-语言模型转移用于无类别偏见的物体计数。具体而言，我们提出了CLIP-Count，这是一种新颖的流程，它通过零样本的文本引导，为开放词汇对象估计密度图，而不需要在特定对象类别上进行任何微调。为了对齐文本嵌入和密集图像特征，我们引入了一个补丁-文本对比损失，指导模型学习有用的补丁级图像表示以进行密集预测。此外，我们设计了一个分层的patch-text交互模块，可以在不同的分辨率级别上传递语义信息。

    Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to down-stream tasks such as object detection and segmentation. However, adapting these models for object counting, which involves estimating the number of objects in an image, remains a formidable challenge. In this study, we conduct the first exploration of transferring visual-language models for class-agnostic object counting. Specifically, we propose CLIP-Count, a novel pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner, without requiring any finetuning on specific object classes. To align the text embedding with dense image features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level image representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module that propagates semantic information across different resolution level
    
[^34]: 高斯先验强化学习用于嵌套命名实体识别

    Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition. (arXiv:2305.07266v1 [cs.CL])

    [http://arxiv.org/abs/2305.07266](http://arxiv.org/abs/2305.07266)

    该论文提出了一种嵌套命名实体识别模型GPRL，使用高斯先验调整嵌套边界标记的输出概率分布，采用强化学习方法生成实体三元组，无需考虑金标签中的实体顺序，实验结果显示其优于以前的嵌套NER模型。

    

    命名实体识别(NER)是自然语言处理中经过广泛研究的任务，最近，嵌套命名实体识别引起了更多的关注，因为它的实用性和难度。现有的嵌套NER作品忽略了嵌套实体的识别顺序和边界位置关系。为了解决这些问题，我们提出了一种新颖的seq2seq模型GPRL，将嵌套NER任务形成一个实体三元组序列生成过程。GPRL采用强化学习方法生成实体三元组，将金标签中的实体顺序解耦，并期望通过试错学习合理的实体识别顺序。基于嵌套实体边界距离的统计，GPRL设计了一个高斯先验，代表嵌套实体之间的边界距离分布，并调整嵌套边界标记的输出概率分布。在三个嵌套NER数据集上的实验证明，GPRL优于以前的嵌套NER模型。

    Named Entity Recognition (NER) is a well and widely studied task in natural language processing. Recently, the nested NER has attracted more attention since its practicality and difficulty. Existing works for nested NER ignore the recognition order and boundary position relation of nested entities. To address these issues, we propose a novel seq2seq model named GPRL, which formulates the nested NER task as an entity triplet sequence generation process. GPRL adopts the reinforcement learning method to generate entity triplets decoupling the entity order in gold labels and expects to learn a reasonable recognition order of entities via trial and error. Based on statistics of boundary distance for nested entities, GPRL designs a Gaussian prior to represent the boundary distance distribution between nested entities and adjust the output probability distribution of nested boundary tokens. Experiments on three nested NER datasets demonstrate that GPRL outperforms previous nested NER models.
    
[^35]: 基于分位数的深度强化学习及其两时间标度策略梯度算法

    Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms. (arXiv:2305.07248v1 [cs.LG])

    [http://arxiv.org/abs/2305.07248](http://arxiv.org/abs/2305.07248)

    本文探讨了优化累积奖励分位数的强化学习设置，提出了QPO和其变体QPPO算法，并使用神经网络对控制动作的策略进行参数化。实验结果表明该算法优于现有基线算法。

    

    传统的强化学习（RL）旨在优化期望累积奖励。本文中，我们考虑了优化累积奖励分位数的RL设置。我们使用神经网络对控制动作的策略进行参数化，并提出了一种称为Quantile-Based Policy Optimization（QPO）的新型策略梯度算法及其变体Quantile-Based Proximal Policy Optimization（QPPO），用于解决具有量化目标的深度RL问题。QPO使用两个耦合迭代同时在不同的时间标度上更新分位数和策略参数，而QPPO是QPO的离线版本，允许在一个模拟回合中多次更新参数，从而提高了算法效率。我们的数值实验表明，在分位数标准下，所提出的算法优于现有基线算法。

    Classical reinforcement learning (RL) aims to optimize the expected cumulative reward. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative reward. We parameterize the policy controlling actions by neural networks, and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) for solving deep RL problems with quantile objectives. QPO uses two coupled iterations running at different timescales for simultaneously updating quantiles and policy parameters, whereas QPPO is an off-policy version of QPO that allows multiple updates of parameters during one simulation episode, leading to improved algorithm efficiency. Our numerical results indicate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.
    
[^36]: 在最弱充分条件和最强必要条件的背景下的双遗忘算子

    Dual Forgetting Operators in the Context of Weakest Sufficient and Strongest Necessary Conditions. (arXiv:2305.07233v1 [cs.AI])

    [http://arxiv.org/abs/2305.07233](http://arxiv.org/abs/2305.07233)

    本文介绍了一种新的算子，称为弱遗忘，与标准遗忘相互对偶，并共同展示了遗忘算子的新的更统一的视角。二者都是基于蕴含和推理而非模型论语义，容易采用Ackermman引理和其不动点概括的算法视角进行描述和运用。定量描述了标准遗忘和最强必要条件之间的强关系以及弱遗忘和最弱充分条件之间的强形式关系。

    

    遗忘是知识表示和自动推理中的重要概念，具有广泛的跨学科应用。 [Lin和Reiter'94]中描述的标准遗忘算子基于模型论语义，主要关注命题情况，开创了一个新的研究子领域。本文引入了一种新的算子，称为弱遗忘，与标准遗忘是双重的，二者共同展示了遗忘算子的新的更统一的视角。弱遗忘算子和标准遗忘算子都是基于蕴含和推理而非模型论语义来描述。这自然地引出了基于量词消除和使用Ackermman引理及其不动点概括的有用的算法视角。标准遗忘和最强必要条件之间的强关系以及弱遗忘和最弱充分条件之间的强形式关系也被明确地描述和确定。

    Forgetting is an important concept in knowledge representation and automated reasoning with widespread applications across a number of disciplines. A standard forgetting operator, characterized in [Lin and Reiter'94] in terms of model-theoretic semantics and primarily focusing on the propositional case, opened up a new research subarea. In this paper, a new operator called weak forgetting, dual to standard forgetting, is introduced and both together are shown to offer a new more uniform perspective on forgetting operators in general. Both the weak and standard forgetting operators are characterized in terms of entailment and inference, rather than a model theoretic semantics. This naturally leads to a useful algorithmic perspective based on quantifier elimination and the use of Ackermman's Lemma and its fixpoint generalization. The strong formal relationship between standard forgetting and strongest necessary conditions and weak forgetting and weakest sufficient conditions is also char
    
[^37]: MMG-Ego4D: 基于多模态泛化的自我中心动作识别

    MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition. (arXiv:2305.07214v1 [cs.CV])

    [http://arxiv.org/abs/2305.07214](http://arxiv.org/abs/2305.07214)

    本文研究了自我中心动作识别中的多模态泛化问题，构建了新的数据集MMG-Ego4D，其中包含视频、音频和惯性运动传感器(IMU)模态。我们在标准监督动作识别和学习新的动作分类的少样本场景下对MMG进行了彻底的研究。

    

    本文研究自我中心动作识别中的一个新问题，命名为“多模态泛化”(MMG)。MMG旨在研究当某些模态的数据受到限制甚至完全缺失时，系统如何进行泛化。我们在标准监督动作识别和学习新的动作分类的更具挑战性的少样本场景下彻底研究了MMG。MMG包括两个新的场景，旨在支持实际应用中的安全和效率考虑：(1)缺失模态泛化，在推断时一些在训练时存在的模态缺失了，(2)跨模态零样本泛化，在推断时和训练时的模态是不相交的。为了进行这项调查，我们构建了一个新的数据集MMG-Ego4D，其中包含视频、音频和惯性运动传感器(IMU)模态的数据点。我们的数据集源于Ego4D数据集。

    In this paper, we study a novel problem in egocentric action recognition, which we term as "Multimodal Generalization" (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two novel scenarios, designed to support security, and efficiency considerations in real-world applications: (1) missing modality generalization where some modalities that were present during the train time are missing during the inference time, and (2) cross-modal zero-shot generalization, where the modalities present during the inference time and the training time are disjoint. To enable this investigation, we construct a new dataset MMG-Ego4D containing data points with video, audio, and inertial motion sensor (IMU) modalities. Our dataset is derived from Ego4D data
    
[^38]: 使用滑动窗口选择的快速Pareto优化

    Fast Pareto Optimization Using Sliding Window Selection. (arXiv:2305.07178v1 [cs.NE])

    [http://arxiv.org/abs/2305.07178](http://arxiv.org/abs/2305.07178)

    本文介绍了一种滑动窗口加速技术，以减少算法的种群大小，从而在更短的时间内实现与以前方法相同的理论性能保证。实验结果表明，该方法可以在广泛的实例和约束设置中实现更好的结果。

    

    Pareto优化使用进化多目标算法已被广泛应用于解决约束子模优化问题。决定所使用的进化算法运行时间的一个关键因素是算法的种群大小，其随算法遇到的折衷数量增长。在本文中，我们介绍了一种滑动窗口加速技术来加速最近引入的算法。我们证明了我们的技术消除了种群大小作为影响运行时间的关键因素，并且在更少的计算时间内实现了与以前方法相同的理论性能保证。我们对经典的最大覆盖问题进行的实验调查证实，我们的滑动窗口技术明显导致广泛的实例和约束设置的更好结果。

    Pareto optimization using evolutionary multi-objective algorithms has been widely applied to solve constrained submodular optimization problems. A crucial factor determining the runtime of the used evolutionary algorithms to obtain good approximations is the population size of the algorithms which grows with the number of trade-offs that the algorithms encounter. In this paper, we introduce a sliding window speed up technique for recently introduced algorithms. We prove that our technique eliminates the population size as a crucial factor negatively impacting the runtime and achieves the same theoretical performance guarantees as previous approaches within less computation time. Our experimental investigations for the classical maximum coverage problem confirms that our sliding window technique clearly leads to better results for a wide range of instances and constraint settings.
    
[^39]: 探索零样本和小样本技术用于意图分类

    Exploring Zero and Few-shot Techniques for Intent Classification. (arXiv:2305.07157v1 [cs.CL])

    [http://arxiv.org/abs/2305.07157](http://arxiv.org/abs/2305.07157)

    探讨了四种零样本和小样本意图分类方法，包括领域适应、数据增强、使用大型语言模型的零样本意图分类以及指令微调语言模型的参数有效微调，结果表明这些方法在低资源环境下都是有效的。指令微调语言模型的参数有效微调性能最佳。

    

    会话中自然语言理解的提供者通常需要扩展到数千个意图分类模型，其中新客户经常面临冷启动问题。在拥有这么多客户的情况下扩展，会对存储空间施加限制。本文探讨了四种不同的零样本和小样本意图分类方法，这些方法受到低资源限制的制约：1）领域适应，2）数据增强，3）使用大型语言模型（LLM）的零样本意图分类，以及4）指令微调语言模型的参数有效微调。我们的结果表明，所有这些方法在低资源环境下都是有效的，但程度不同。使用Flan-T5（Chang et al，2022）在T-few配方（Liu et al，2022）上进行参数有效微调，即使每个意图只有一个样本，性能也最佳。我们还展示了使用意图描述提示LLM的零样本方法。

    Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on Flan-T5 (Chang et al., 2022) yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions
    
[^40]: ConceptARC基准：评估ARC领域的理解和泛化能力

    The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. (arXiv:2305.07141v1 [cs.LG])

    [http://arxiv.org/abs/2305.07141](http://arxiv.org/abs/2305.07141)

    本研究提出了一个新的基准数据集ConceptARC，针对ARC领域的抽象和推理问题进行了深入评估，以提高人工智能系统的抽象和泛化能力。

    

    形成和抽象概念的能力是人类智能的关键，但现有的人工智能系统在这方面仍然欠缺。在人工智能中进行了大量关于概念抽象的研究，特别是使用理想化的领域，如Raven的渐进矩阵和Bongard问题，但即使在人工智能系统成功解决这些问题时，这些系统的实际理解情况也很少被评估。本文描述了一种针对抽象和推理数据集（ARC）的深入评估基准，ARC是Chollet [2019]开发的一组少量抽象和类比问题集。具体而言，我们描述了一个名为ConceptARC的新的、公开可用的ARC基准，它在许多基本空间和语义概念上系统地评估了抽象和泛化能力。与原始的ARC数据集不同，ConceptARC特别围绕“概念组”进行组织。

    The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.  In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around "concept groups" -- se
    
[^41]: k-匿名和合成数据技术的能量成本和机器学习准确性影响。

    Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])

    [http://arxiv.org/abs/2305.07116](http://arxiv.org/abs/2305.07116)

    本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。

    

    为了解决与隐私和气候变化有关的愈发增长的社会关切，欧盟颁布了《通用数据保护条例》(GDPR)并承诺了绿色协议。大量研究探究了运用匿名数据集训练机器学习模型的能效和准确性。最近的研究开始探究隐私增强技术（PET）对机器学习模型的能量消耗和准确性的影响，重点关注k-匿名。由于合成数据越来越受欢迎，因此本文分析了两个阶段的能量消耗和准确性：a）将隐私增强技术应用于相关数据集，b）在相关隐私增强数据集上训练模型。我们使用两种隐私增强技术：k-匿名化（使用泛化和抑制）和合成数据，以及三种机器学习模型。每个模型都在每个隐私增强数据集上进行训练。结果显示，在经过k-匿名化的数据上训练的模型具有较低的准确性，并且消耗的能量较少，与在非匿名化数据上训练的模型相比。然而，k-匿名化过程中消耗的能量非常可观，在评估其有用性时必须将其考虑在内。合成数据证明是一种有前途的选择，因为它在消耗更少能源的情况下实现了与非匿名化数据可比的准确性。

    To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
    
[^42]: 显著掩模引导下的视觉Transformer用于细粒度分类

    Salient Mask-Guided Vision Transformer for Fine-Grained Classification. (arXiv:2305.07102v1 [cs.CV])

    [http://arxiv.org/abs/2305.07102](http://arxiv.org/abs/2305.07102)

    该研究提出了一种显著掩模引导下的视觉Transformer方法，适用于细粒度分类。该方法旨在解决细粒度分类中捕捉最具有区别性的差异和忽略不相关区域的困难。

    

    细粒度视觉分类是一个具有挑战性的计算机视觉问题，其任务是在亚类别中自动识别对象。其中主要的困难是捕捉那些仅在视觉上相似但在类别间有最具有区别性的差异。最近，采用视觉Transformer（ViT）的方法在细粒度视觉分类中取得了显著成果，通常通过运用自我注意机制和其他耗费资源的技术来区分具有潜在区别性的区域，而忽略其余区域。然而，这种方法只依赖内在的自我关注机制，在有效聚焦真正具有区别性的区域方面可能存在困难，导致分类令牌可能会从不重要的背景区域汇集全局信息。此外，由于缺乏数据点，分类器可能无法找到最有帮助的类间区别特征，因为其他无关但独特的特征通常会受到更多的关注。

    Fine-grained visual classification (FGVC) is a challenging computer vision problem, where the task is to automatically recognise objects from subordinate categories. One of its main difficulties is capturing the most discriminative inter-class variances among visually similar classes. Recently, methods with Vision Transformer (ViT) have demonstrated noticeable achievements in FGVC, generally by employing the self-attention mechanism with additional resource-consuming techniques to distinguish potentially discriminative regions while disregarding the rest. However, such approaches may struggle to effectively focus on truly discriminative regions due to only relying on the inherent self-attention mechanism, resulting in the classification token likely aggregating global information from less-important background patches. Moreover, due to the immense lack of the datapoints, classifiers may fail to find the most helpful inter-class distinguishing features, since other unrelated but distinc
    
[^43]: $\mathrm{E}(n)$等变消息传递单纯网络

    $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks. (arXiv:2305.07100v1 [cs.LG])

    [http://arxiv.org/abs/2305.07100](http://arxiv.org/abs/2305.07100)

    本文提出了$\mathrm{E}(n)$等变消息传递单纯网络(EMPSNs)，一种同时将消息传递单纯网络和$\mathrm{E}(n)$等变图神经网络的优势结合，在处理高维数据时利用几何信息防止过度平滑的方法。

    

    本文提出了$\mathrm{E}(n)$等变消息传递单纯网络(EMPSNs)，这是一种学习在几何图形和点云上的方法，其等变于旋转、平移和反射。EMPSNs可以学习在图形中的高维单纯面（如三角形），并以$\mathrm{E}(n)$等变方式利用更高维单纯体的几何信息。EMPSNs同时将$\mathrm{E}(n)$等变图神经网络推广到更加复杂的拓扑结构领域，并提供了一种在消息传递单纯网络中包含几何信息的方法。结果表明，EMPSNs可以利用两种方法的优势，相较于单独使用其中一种方法，性能有了普遍提高。此外，结果表明，在高维操作中，包含几何信息是防止消息传递网络过度平滑的有效措施。

    This paper presents $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. EMPSNs can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an $\mathrm{E}(n)$ equivariant fashion. EMPSNs simultaneously generalize $\mathrm{E}(n)$ Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks. The results indicate that EMPSNs can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method. Furthermore, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high
    
[^44]: 机器理由对人类是否有用？评估和提高自然文本理由的人类效用

    Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales. (arXiv:2305.07095v1 [cs.CL])

    [http://arxiv.org/abs/2305.07095](http://arxiv.org/abs/2305.07095)

    该论文研究了机器产生的自然语言理由对人类是否有用，发现现有理由的人类效用远低于理想状态，并提出通过估计理由在回答给定问题中的有用性来提高机器生成理由的人类效用。

    

    在大型语言模型（LMs）的显着出现能力中，自由文本理由是其中之一；超过某个规模后，大型LMs能够生成看似有用的理由，进而可以极大地增强它们在领导榜上的表现。这种现象引发了一个问题：机器生成的理由是否也能对人类有用，特别是当普通人尝试根据这些机器理由回答问题时？我们观察到现有理由的人类效用远未令人满意，并且昂贵的人类研究才能估计。现有的评估指标，如生成理由LM的任务表现或生成理由与黄金理由之间的相似性，并不能很好地表明它们的人类效用。虽然我们观察到，理由的某些属性，如简洁性和新颖性，与它们的人类效用有关，但在没有人类参与的情况下估计它们是具有挑战性的。我们展示了如何通过估计理由在回答给定问题中的有用性来提高机器生成理由的人类效用，从而解决这个问题。

    Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond a certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale's helpfulness in ans
    
[^45]: 无人机三维网络中的干扰管理的深度强化学习：潜力与挑战

    Deep Reinforcement Learning for Interference Management in UAV-based 3D Networks: Potentials and Challenges. (arXiv:2305.07069v1 [cs.IT])

    [http://arxiv.org/abs/2305.07069](http://arxiv.org/abs/2305.07069)

    本文提出了利用深度强化学习进行干扰管理的方法来解决UAV通信中的干扰问题，而无需先知干扰信号的信道信息。

    

    现代蜂窝网络是多小区的，采用通用频率重用来最大化频谱效率。这导致高干扰。随着无人机(UAV)的采用，这个问题正在变得越来越严重，因为UAV通信中的直射频道会迅速增加干扰链路的强度和数量。现有的干扰管理解决方案需要每个发射器知道干扰信号的信道信息，从而导致过多的信令开销。在本文中，我们提出利用深度强化学习进行干扰管理来解决这个缺陷。特别地，我们表明即使不知道干扰信号的信道信息，也仍然可以有效地减轻干扰。然后，我们讨论了使用线性/亚线性复杂度扩展算法和使用多智能体强化学习对其进行分散的新方法。

    Modern cellular networks are multi-cell and use universal frequency reuse to maximize spectral efficiency. This results in high inter-cell interference. This problem is growing as cellular networks become three-dimensional with the adoption of unmanned aerial vehicles (UAVs). This is because the strength and number of interference links rapidly increase due to the line-of-sight channels in UAV communications. Existing interference management solutions need each transmitter to know the channel information of interfering signals, rendering them impractical due to excessive signaling overhead. In this paper, we propose leveraging deep reinforcement learning for interference management to tackle this shortcoming. In particular, we show that interference can still be effectively mitigated even without knowing its channel information. We then discuss novel approaches to scale the algorithms with linear/sublinear complexity and decentralize them using multi-agent reinforcement learning. By ha
    
[^46]: 具有门控汇总模块的值迭代网络研究

    Value Iteration Networks with Gated Summarization Module. (arXiv:2305.07039v1 [cs.LG])

    [http://arxiv.org/abs/2305.07039](http://arxiv.org/abs/2305.07039)

    本文提出了一种具有门控汇总模块的值迭代网络（GS-VIN）来解决值迭代网络在处理更大的输入地图和减轻累积误差方面的挑战。通过自适应迭代策略和门控汇总模块，这种模型可以在保持准确性的同时，减少网络深度并提高训练稳定性。

    

    本文针对值迭代网络（VIN）在处理更大的输入地图，减轻由增加迭代次数引起的累积误差的挑战提出了一种新方法——具有门控汇总模块的值迭代网络（GS-VIN）。我们提出自适应迭代策略，利用更大的卷积核减少迭代次数，减少网络深度，提高训练稳定性，同时保持计划过程的准确性。我们还引入了门控汇总模块，使得网络可以强调整个规划过程，而不仅仅依赖于最终的全局规划结果。

    In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments 
    
[^47]: 带人类反馈的GFlowNets

    GFlowNets with Human Feedback. (arXiv:2305.07036v1 [cs.LG])

    [http://arxiv.org/abs/2305.07036](http://arxiv.org/abs/2305.07036)

    GFlowNets框架通过人类反馈来改善AI模型的探索能力，通过适应不同轨迹上的人类评估，可以学习到严格与人类评级成比例的策略，实验结果表明比RLHF更出色。

    

    我们提出了带有人类反馈的GFlowNets (GFlowHF) 框架来改进训练 AI 模型的探索能力。对于奖励未知的任务，我们通过不同轨迹上的人类评估来适应奖励函数。GFlowHF 的目标是学习一个严格与人类评级成比例的策略，而不仅仅关注于类似 RLHF 的人类喜好评级。实验表明，GFlowHF 比 RLHF 可以实现更好的探索能力。

    We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve the exploration ability when training AI models. For tasks where the reward is unknown, we fit the reward function through human evaluations on different trajectories. The goal of GFlowHF is to learn a policy that is strictly proportional to human ratings, instead of only focusing on human favorite ratings like RLHF. Experiments show that GFlowHF can achieve better exploration ability than RLHF.
    
[^48]: 安静！密谋行动的逻辑。

    Shhh! The Logic of Clandestine Operations. (arXiv:2305.07035v1 [cs.LO])

    [http://arxiv.org/abs/2305.07035](http://arxiv.org/abs/2305.07035)

    该论文提出了秘密行动的形式语义和逻辑系统。其强调分布式知识和联盟力量对秘密行动的相互作用。

    

    如果操作隐藏了操作者的身份，则称其为隐蔽操作; 如果遮掩了行动事实，则称其为秘密操作。该论文提出了秘密行动的形式语义，并引入了一种描述分布式知识模态和捕获联盟行使秘密行动力量之间相互作用的声音和完整的逻辑系统。

    An operation is called covert if it conceals the identity of the actor; it is called clandestine if the very fact that the operation is conducted is concealed. The paper proposes a formal semantics of clandestine operations and introduces a sound and complete logical system that describes the interplay between the distributed knowledge modality and a modality capturing coalition power to conduct clandestine operations.
    
[^49]: 基于端到端深度学习的古兰经朗诵识别

    Quran Recitation Recognition using End-to-End Deep Learning. (arXiv:2305.07034v1 [eess.AS])

    [http://arxiv.org/abs/2305.07034](http://arxiv.org/abs/2305.07034)

    本文提出了一种基于端到端深度学习模型，使用CTC作为目标函数，来识别古兰经的朗诵。采用公共数据集进行实验。

    

    古兰经是伊斯兰教的圣书，其朗诵是该宗教信仰的一个重要方面。由于古兰经的独特规则不适用于正常的演讲，所以自动识别古兰经的朗诵是一项具有挑战性的任务。在此之前，已进行了许多研究，但以往的研究将朗诵错误检测视为分类任务或使用传统的自动语音识别（ASR）。在本文中，我们提出了一个新的基于端到端深度学习模型来识别古兰经的朗诵。该模型是一个CNN-Bidirectional GRU编码器，使用CTC作为目标函数和基于字符的解码器，即波束搜索解码器。此外，所有以往的研究都是在由短节和几章古兰经组成的小型私人数据集上进行的。由于使用私人数据集，因此没有进行任何比较。为了解决这个问题，我们使用了最近发布的公共数据集（Ar-DAD）作为实验数据。

    The Quran is the holy scripture of Islam, and its recitation is an important aspect of the religion. Recognizing the recitation of the Holy Quran automatically is a challenging task due to its unique rules that are not applied in normal speaking speeches. A lot of research has been done in this domain, but previous works have detected recitation errors as a classification task or used traditional automatic speech recognition (ASR). In this paper, we proposed a novel end-to-end deep learning model for recognizing the recitation of the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that uses CTC as an objective function, and a character-based decoder which is a beam search decoder. Moreover, all previous works were done on small private datasets consisting of short verses and a few chapters of the Holy Quran. As a result of using private datasets, no comparisons were done. To overcome this issue, we used a public dataset that has recently been published (Ar-DAD) and co
    
[^50]: 基于控制微分方程的Hawkes过程

    Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])

    [http://arxiv.org/abs/2305.07031](http://arxiv.org/abs/2305.07031)

    本文提出了一种基于控制微分方程的Hawkes过程模型，可精确计算对数似然，并能够正确处理不规则时间序列，适用于社会扩散和地震预测。

    

    Hawkes过程是一种常用的模型框架，用于对多个领域的序贯事件发生动态进行建模，例如社会扩散。在现实场景中，事件之间的间隔时间是不规则的。然而，现有基于神经网络的Hawkes过程模型不仅难以捕捉这种复杂的不规则动态，而且还会使用启发式方法计算事件的对数似然，因为它们大多基于设计用于规则离散输入的神经网络。为此，我们提出了基于控制微分方程(CDE)的Hawkes过程概念，通过采用类似于连续RNN的神经CDE技术。由于HP-CDE不断地读取数据，因此可以适当地处理不规则时间序列数据集，保留它们的不均匀时间空间，并且对数似然可以准确计算。此外，由于Hawkes过程和神经CDE都是在连续的时间域中首先开发的，它们具有相似的背景。因此，HP-CDE具有透明的结构，可以轻松适应实际场景，例如社会扩散，其中事件之间的间隔时间是不规则的。我们使用合成和真实的社交扩散和地震数据集演示了我们提出的模型的优势，并超过了现有的最先进的Hawkes过程模型。

    Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first de
    
[^51]: Musketeer（一人之力，万人之力）：具有任务解释提示的通用视觉语言模型

    Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts. (arXiv:2305.07019v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2305.07019](http://arxiv.org/abs/2305.07019)

    Musketeer是一种通用视觉语言模型，采用任务解释提示（TEP）机制，能够有效整合异构任务的知识，并在多个任务中表现均匀

    

    我们提出了一种序列到序列的视觉语言模型，其参数在所有任务上进行联合训练（万人之力），并在多个任务之间完全共享（一人之力），从而产生了一个名为Musketeer的单一模型。

    We present a sequence-to-sequence vision-language model whose parameters are jointly trained on all tasks (all for one) and fully shared among multiple tasks (one for all), resulting in a single model which we named Musketeer. The integration of knowledge across heterogeneous tasks is enabled by a novel feature called Task Explanation Prompt (TEP). TEP reduces interference among tasks, allowing the model to focus on their shared structure. With a single model, Musketeer achieves results comparable to or better than strong baselines trained on single tasks, almost uniformly across multiple tasks.
    
[^52]: 大型语言模型可用于有效扩展钓鱼邮件攻击

    Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns. (arXiv:2305.06972v1 [cs.CY])

    [http://arxiv.org/abs/2305.06972](http://arxiv.org/abs/2305.06972)

    大型语言模型可用于扩展钓鱼邮件攻击，作者通过实证测试表明高级的语言模型可以显著提高攻击的效率和成本效益。

    

    人工智能领域的最新进展尤其是大型语言模型的发展，已经产生了功能强大而通用的双重用途系统。本研究调查了如何使用大型语言模型进行钓鱼邮件攻击，这种流行的网络犯罪形式涉及将目标人物诱骗披露敏感信息。作者首先研究了LLMs在成功的钓鱼攻击的侦察和信息生成阶段的能力，发现先进的LLMs能够在这些阶段显着提高网络罪犯的效率。其次，作者使用OpenAI的GPT-3.5和GPT-4模型为超过600名英国议员创建了独特的钓鱼邮件的实证测试。研究结果表明，这些邮件不仅逼真而且成本效益显著，每封电子邮件仅花费几分之一的美分即可产生。

    Recent progress in artificial intelligence (AI), particularly in the domain of large language models (LLMs), has resulted in powerful and versatile dual-use systems. Indeed, cognition can be put towards a wide variety of tasks, some of which can result in harm. This study investigates how LLMs can be used for spear phishing, a prevalent form of cybercrime that involves manipulating targets into divulging sensitive information. I first explore LLMs' ability to assist with the reconnaissance and message generation stages of a successful spear phishing attack, where I find that advanced LLMs are capable of meaningfully improving cybercriminals' efficiency during these stages. Next, I conduct an empirical test by creating unique spear phishing messages for over 600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My findings reveal that these messages are not only realistic but also remarkably cost-effective, as each email cost only a fraction of a cent to generate. N
    
[^53]: 基于模仿学习的算法在现代电力市场中实现先验知识传递以进行贝叶斯纳什均衡估计

    An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation. (arXiv:2305.06924v1 [cs.GT])

    [http://arxiv.org/abs/2305.06924](http://arxiv.org/abs/2305.06924)

    本论文提出了一种基于模仿学习的算法，利用先验知识和与变化环境的交互实现了GENCO的投标策略优化和贝叶斯纳什均衡估计，针对现代电力市场中先验知识未被充分利用导致现有方法不准确和低效的问题进行了改进。

    

    在电力市场的投标游戏中，纳什均衡（NE）估计是发电公司（GENCO）进行投标策略优化和独立系统运营商（ISO）进行市场监视的关键问题。然而，现有的NE估计方法在新兴现代电力市场（FEM）中是不准确和低效的，因为在任何环境变化之前，如负载需求变化、网络拥堵和市场设计的修改，投标策略的先验知识没有充分利用。为此，本文针对FEM开发了Bayes自适应马尔科夫决策过程（BAMDP-FEM），以考虑先验知识来建模GENCO的投标策略优化。随后提出了一种新颖的多智能体生成对抗模仿学习算法（MAGAIL-FEM），使GENCO能够同时从先验知识和与变化环境的交互中进行学习。得到的NE是一种贝叶斯纳什均衡（BNE）。

    The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with 
    
[^54]: 如何使用强化学习促进未来的电力市场设计？第二部分：方法和应用

    How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications. (arXiv:2305.06921v1 [cs.GT])

    [http://arxiv.org/abs/2305.06921](http://arxiv.org/abs/2305.06921)

    本文开发了一种基于强化学习的模拟方法来联合设计电力市场，详细阐述了设计电力现货市场、辅助服务市场中的保留能力产品和金融市场中的虚拟竞标产品的方法，并通过案例研究演示了如何选择最佳市场设计选项。

    

    本为两部分的论文发展了一种范式理论和详细的方法，利用基于强化学习（RL）的模拟来联合电力市场设计。在第二部分中，通过阐述详细的方法设计电力现货市场（ESM）、辅助服务市场（ASM）中的保留能力产品（RC）和金融市场（FM）中的虚拟竞标（VB）产品来进一步演示这一理论。根据第一部分提出的理论，首先确定联合市场中的市场设计选项。接着，开发了马尔科夫博弈模型，展示了如何将市场设计选项和不确定风险纳入模型公式中。详细阐述了一种多智能体策略近端优化（MAPPO）算法，作为第一部分开发的广义市场模拟方法的实际实现。最后，通过使用一些市场运行绩效指标，案例研究演示如何选择最佳市场设计选项。

    This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators 
    
[^55]: ChatGPT式的大规模基础模型在预测与健康管理中的应用：综述与路线图

    ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])

    [http://arxiv.org/abs/2305.06472](http://arxiv.org/abs/2305.06472)

    该论文综述了基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的人工智能（AI）技术在预测与健康管理（PHM）中的广泛应用。这种技术可以实现多模态、多任务、大量数据和超大模型范式，成为AI-2.0的新时代的标志之一。

    

    预测与健康管理技术在工业生产和设备维护中扮演着至关重要的角色，通过基于人工智能的PHM技术识别和预测设备故障和损坏。现在，基于大规模基础模型（LSF-Models）如ChatGPT和DALLE-E的AI技术，可以实现多模态、多任务、大规模数据和超大模型范式，成为AI-2.0的新时代的标志之一。这种技术广泛应用于各种工业领域，如铁路、能源和航空等，以提高设备的服务寿命和可靠性，同时降低生产成本和停机时间。

    Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
    
[^56]: 多智能体强化学习: 异步通信和线性函数逼近

    Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])

    [http://arxiv.org/abs/2305.06446](http://arxiv.org/abs/2305.06446)

    该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。

    

    我们研究了多智能体强化学习在情节式马尔科夫决策过程中的设置，多个智能体通过中央服务器进行通信以合作。我们提出了一种基于值迭代的可证明有效的算法，可以实现异步通信，同时确保合作优势且通信开销低。我们证明了在使用线性函数逼近的情况下，我们的算法具有 $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ 的遗憾值和 $\tilde{\mathcal{O}}(dHM^2)$ 的通信复杂度，其中 $d$ 是特征维数，$H$ 是时间跨度，$M$ 是智能体总数，$K$ 是总情节数。我们还提供了一个下限证明，表明通过协作至少需要 $\Omega(dM)$ 的通信复杂度才能改善性能。

    We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
    
[^57]: 自动化仓储的多机器人协调和布局设计

    Multi-Robot Coordination and Layout Design for Automated Warehousing. (arXiv:2305.06436v1 [cs.RO])

    [http://arxiv.org/abs/2305.06436](http://arxiv.org/abs/2305.06436)

    通过优化仓库布局，可以减少拥堵，提高吞吐量，并扩大自动化仓库的可伸缩性。

    

    随着多智能体路径规划(MAPF)技术的快速发展，研究人员开始将MAPF算法应用于大型自动化仓库中，以协调数百个机器人。虽然大多数研究旨在通过开发更好的MAPF算法来提高仓库的吞吐量，但我们专注于通过优化仓库布局来提高其吞吐量。我们发现，即使使用最先进的MAPF算法，通常使用的人工设计布局也可能导致仓库的拥堵并且具有有限的可伸缩性。我们扩展了现有的自动场景生成方法以优化仓库布局。结果显示，我们优化后的仓库布局(1)减少了交通拥堵，从而提高了吞吐量，(2)通过加倍机器人数量在某些情况下提高了自动化仓库的可伸缩性，(3)能够生成具有用户指定多样性指标的布局。源代码位于：\url{https://github.com/lun}

    With the rapid progress in Multi-Agent Path Finding (MAPF), researchers have studied how MAPF algorithms can be deployed to coordinate hundreds of robots in large automated warehouses. While most works try to improve the throughput of such warehouses by developing better MAPF algorithms, we focus on improving the throughput by optimizing the warehouse layout. We show that, even with state-of-the-art MAPF algorithms, commonly used human-designed layouts can lead to congestion for warehouses with large numbers of robots and thus have limited scalability. We extend existing automatic scenario generation methods to optimize warehouse layouts. Results show that our optimized warehouse layouts (1) reduce traffic congestion and thus improve throughput, (2) improve the scalability of the automated warehouses by doubling the number of robots in some cases, and (3) are capable of generating layouts with user-specified diversity measures. We include the source code at: \url{https://github.com/lun
    
[^58]: 稀疏和密集神经网络中的小批量大小的相变

    Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.06435](http://arxiv.org/abs/2305.06435)

    本文系统地研究了小批量大小对稀疏和密集神经网络训练的影响，发现在临界值处会出现尖锐的相变，阐明了神经网络优化的基本机制。

    

    在训练人工神经网络时，使用小批量数据现在非常普遍。尽管已经广泛使用，但缺少定量解释最佳小批量大小应该是多大的理论。本文尝试系统地理解小批量大小在训练两层神经网络中的作用。在教师-学生情境下，使用稀疏教师，并聚焦于不同复杂度的任务，我们量化了改变小批量大小m的影响。我们发现，通常情况下，学生的泛化性能强烈依赖于m，并且可能在临界值mc处经历尖锐的相变，这样当m< mc时，训练过程失败，而当m> mc时，学生可以完美地学习或很好地泛化教师。相变是由统计力学首次发现的集体现象，并在许多科学领域观察到。找到在深度学习中改变小批量大小的相变，可以阐明神经网络优化的基本机制。

    The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m<m_c$ the training process fails, while for $m>m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
    
[^59]: 搜索UGLE真相：无监督GNN学习环境的调查

    Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])

    [http://arxiv.org/abs/2305.06026](http://arxiv.org/abs/2305.06026)

    本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。

    

    图神经网络 (GNN) 是任何机器学习任务中的一个重要工具，因为它们能够学习图结构上的函数，这是一种强大和表达性强的数据表示。社区检测是一种无监督任务，越来越多地使用GNN进行。利用节点特征的多维度与图的连接性对图中的节点进行聚类，对从社交网络到基因组学的真实世界任务有许多应用。不幸的是，目前文献中缺乏公平且严谨评估基于GNN的社区检测的充分基准环境，从而可能阻碍这一新兴领域的进展。我们观察到这种情况下的特定困难是模糊的超参数调整环境与性能和评估数据集的冲突指标。在这项工作中，我们提出和评估了框架，用于在GNN学习环境中进行一致的社区检测算法比较。我们提供了一个基准数据集，并提出了评估指标，反映了检测到的社区的内在质量以及聚类的准确性。

    Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
    
[^60]: HAISTA-NET: 通过注意力进行人类辅助的实例分割

    HAISTA-NET: Human Assisted Instance Segmentation Through Attention. (arXiv:2305.03105v1 [cs.CV])

    [http://arxiv.org/abs/2305.03105](http://arxiv.org/abs/2305.03105)

    该论文提出了一种通过人类辅助实例分割方法，称为HAISTA-NET，增强了现有的实例分割网络，引入了人类指定的部分边界地图，以生成更精确的分割掩模。

    

    实例分割是图像检测的一种形式，在物体细化、医学图像分析和图像/视频编辑等方面有广泛应用，这些应用都需要高度精确的结果。然而，即便是最先进的完全自动化实例分割算法，其精度常常无法达到。对于小而复杂的对象来说，性能差距尤为明显。通常，从业者只能采用完全手动的注释方法，这可能是一个繁琐的过程。为了克服这个问题，我们提出了一种新的方法，以实现更精确的预测，并为高曲率、复杂和小规模对象生成更高质量的分割掩模。我们的人类辅助分割模型HAISTA-NET，扩充了现有的Strong Mask R-CNN网络，以包括人类指定的部分边界。此外，我们还提出了一个手绘部分物体边界的数据集，称为人类注意力地图。我们还引入了一种新的损失函数，它考虑到了部分注意力地图和原始掩模提议，这使网络能够关注人们认为最重要的区域。在PASCAL VOC和COCO数据集上的实验结果表明，我们提出的方法优于强基线，并在小而复杂对象实例分割方面实现了最先进的性能。

    Instance segmentation is a form of image detection which has a range of applications, such as object refinement, medical image analysis, and image/video editing, all of which demand a high degree of accuracy. However, this precision is often beyond the reach of what even state-of-the-art, fully automated instance segmentation algorithms can deliver. The performance gap becomes particularly prohibitive for small and complex objects. Practitioners typically resort to fully manual annotation, which can be a laborious process. In order to overcome this problem, we propose a novel approach to enable more precise predictions and generate higher-quality segmentation masks for high-curvature, complex and small-scale objects. Our human-assisted segmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network to incorporate human-specified partial boundaries. We also present a dataset of hand-drawn partial object boundaries, which we refer to as human attention maps. In addition, 
    
[^61]: 如何利用强化学习促进未来电力市场设计？第一部分：范型理论。（arXiv:2305.02485v1 [cs.AI]）

    How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory. (arXiv:2305.02485v1 [cs.AI])

    [http://arxiv.org/abs/2305.02485](http://arxiv.org/abs/2305.02485)

    本文提出基于强化学习的方法，设计联合市场以应对电力行业脱碳，实现电力系统的安全和经济效益，并为环境做出贡献。该范型理论的框架将在两部分中详细介绍。

    

    面对电力行业脱碳的迫切需求，重新设计电力市场是一种宏观层面的方法，以适应可再生能源的高渗透率，并实现电力系统的操作安全、经济效率和环境友好性。然而，现有的市场设计方法学存在于能源现货市场（ESM）、辅助服务市场（ASM）和金融市场（FM）之间协调不足，即“联合市场”，以及缺乏可靠的基于模拟的验证。为了解决这些缺陷，本文将基于强化学习（RL）的模拟，开发联合市场设计的范型理论和详细方法。第一部分提出了这种新型市场设计哲学的理论和框架。首先，总结了在设计联合市场时存在的有争议的市场设计选项作为目标研究问题。其次，提出了马尔可夫博弈模型。

    In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the "joint market", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is deve
    
[^62]: PaTeCon：基于模式的知识图谱时间约束挖掘方法用于冲突检测

    PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs. (arXiv:2304.09015v1 [cs.AI])

    [http://arxiv.org/abs/2304.09015](http://arxiv.org/abs/2304.09015)

    PaTeCon是一种基于模式的知识图谱时间约束挖掘方法，能够自动生成时间约束来维护KG的时间一致性，并在不需要人工专家的情况下准确地检测潜在的时间冲突。

    

    在知识图谱（KG）研究社区中，时间事实指特定时间段内发生的事件的数据。引入时间限制给KG的时间一致性维护带来了新的挑战，先前的研究依赖于手动列举时间约束来检测冲突，这很费力且可能存在粒度问题。本文提出了一种基于模式的时间约束挖掘方法PaTeCon，它使用自动确定的图形模式及其相关统计信息代替人工专家来生成时间约束。具体地，PaTeCon根据其测量得分动态地将类限制附加到候选约束上。我们基于维基数据集评估了PaTeCon的效果。

    Temporal facts, the facts for characterizing events that hold in specific time periods, are attracting rising attention in the knowledge graph (KG) research communities. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs and detecting potential temporal conflicts. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. We start from the common pattern of temporal facts and constraints and propose a pattern-based temporal constraint mining method, PaTeCon. PaTeCon uses automatically determined graph patterns and their relevant statistical information over the given KG instead of human experts to generate time constraints. Specifically, PaTeCon dynamically attaches class restriction to candidate constraints according to their measuring scores.We evaluate PaTeCon on two large-scale datasets based on Wikidata and F
    
[^63]: 神经崩溃现象的研究：Grassmannian Frame、对称性和泛化

    A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization. (arXiv:2304.08914v1 [cs.LG])

    [http://arxiv.org/abs/2304.08914](http://arxiv.org/abs/2304.08914)

    本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。

    

    本文通过证明广义神经崩溃假设推广了原始的神经崩溃现象。我们通过分类的优化和泛化得到了Grassmannian Frame结构。该结构在球面上最大化地分离了每两个类别的特征，并且不需要一个更大的特征维度。出于对Grassmannian Frame对称性的好奇，我们进行了一系列实验，探索不同Grassmannian Frame模型是否会产生不同的表现。结果我们发现了对称泛化现象。我们提出了一个关于置换对称泛化的定理。然而，为什么特征的不同方向会导致如此不同的泛化现象的问题仍然需要进一步研究。

    In this paper, we extends original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.
    
[^64]: 通过解释不变性和等变性评估解释方法的健壮性

    Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])

    [http://arxiv.org/abs/2304.06715](http://arxiv.org/abs/2304.06715)

    本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。

    

    只有当解释方法忠实地描述所解释的模型时，解释方法才有价值。本文考虑了神经网络，其预测在特定对称群下具有不变性，这包括从卷积神经网络到图神经网络的流行架构。任何忠实描述这种类型模型的解释都需要与该不变性属性一致。我们通过运用几何深度学习的形式化方法，通过解释不变性和等变性的概念来形式化这种直觉。通过这种严格的形式化方法，我们得出了（1）两个度量来衡量任何解释方法相对于模型对称群的健壮性;（2）一些流行的解释方法的理论健壮性保证；（3）提高任何解释方法相对于对称群的不变性的系统方法。通过在与不同对称群相关的模型的解释中经验地测量我们的度量标准，我们展示了解释不变性和等变性对于强大的解释方法是重要的属性。

    Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
    
[^65]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^66]: 管制ChatGPT和其他大型生成AI模型

    Regulating ChatGPT and other Large Generative AI Models. (arXiv:2302.02337v5 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2302.02337](http://arxiv.org/abs/2302.02337)

    本文将讨论大型生成AI模型的可信AI监管，包括直接监管、数据保护、内容监管和政策建议，并建议使用新术语来区分参与者。本文的目的是确保LGAIMs的可信度并使其为受益所用。

    

    大型生成AI模型（LGAIMs），如ChatGPT或Stable Diffusion，正在快速改变我们的沟通、说明和创造方式。然而，欧盟及其他地区的AI监管主要集中在传统AI模型上，而非LGAIMs。本文将把这些新的生成模型放置在当前的“可信AI监管”辩论中，并探讨如何调整法律以适应其能力。在奠定技术基础之后，本文的法律部分分四步进行，包括（1）直接监管，（2）数据保护，（3）内容监管和（4）政策建议。它建议使用新术语来捕捉LGAIM设置中的AI价值链，区分LGAIM开发人员、部署者、专业和非专业用户，以及LGAIM输出的接收者。我们将监管职责针对这些不同的价值链参与者进行调整，并提出四个策略，以确保LGAIMs的信任度并使其为受益所用。

    Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of so
    
[^67]: GLUE-X: 从ODD普适性角度评估自然语言理解模型

    GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08073](http://arxiv.org/abs/2211.08073)

    本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs上对8个经典NLP任务进行评估。

    

    预训练语言模型（PLMs）通过利用大量的训练数据，已知可以提高自然语言理解模型的泛化性能。然而，许多NLP任务中的ODD普适性问题仍然存在，这限制了这些方法在现实世界中的部署。本文提出了第一个创建名为方法的统一基准的尝试，用于评估NLP模型中的OOD鲁棒性，强调OOD鲁棒性的重要性，并提供如何衡量模型的鲁棒性以及如何改善模型的见解。该基准包括13个公开可用的OOD测试数据集，并在21个常用的PLMs（包括GPT-3和GPT-3.5）上对8个经典NLP任务进行评估。我们的研究结果确认了在所有设置下，与ID准确度相比，存在显着的性能下降，需要改善NLP任务中的OOD准确度。

    Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
    
[^68]: 网络流的图神经建模

    Graph Neural Modeling of Network Flows. (arXiv:2209.05208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05208](http://arxiv.org/abs/2209.05208)

    本文提出了一种新颖的网络流问题图学习架构 PEW，相较于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    

    网络流问题涉及将流量分布在网络中，以使基础设施得到有效利用，这在交通运输和物流中是无处不在的。其中，多商品网络流 (MCNF) 问题是普遍感兴趣的，因为它涉及在多个源和汇之间分配不同大小的多个流，同时实现链路的有效利用。由于数据驱动优化的吸引力，这些问题越来越多地使用图学习方法来解决。本文提出了一种新颖的网络流问题图学习架构 PEW (Per-Edge Weights)。此方法基于图注意力网络，并沿着每个链接使用不同参数化的消息函数。我们通过使用 $17$ 个服务提供商拓扑和 $2$ 个路由方案进行互联网流量路由案例研究，对所提出的解决方案进行了广泛的评估。我们展示了 PEW 相对于不考虑链接的流量特定权重的架构，能够实现显著的收益，并在路由准确性和收敛速度方面实现了最先进的性能。

    Network flow problems, which involve distributing traffic over a network such that the underlying infrastructure is used effectively, are ubiquitous in transportation and logistics. Among them, the Multi-Commodity Network Flow (MCNF) problem is of general interest, as it concerns the distribution of multiple flows of different sizes between several sources and sinks, while achieving effective utilization of the links. Due to the appeal of data-driven optimization, these problems have increasingly been approached using graph learning methods. In this paper, we propose a novel graph learning architecture for network flow problems called Per-Edge Weights (PEW). This method builds on a Graph Attention Network and uses distinctly parametrized message functions along each link. We extensively evaluate the proposed solution through an Internet flow routing case study using $17$ Service Provider topologies and $2$ routing schemes. We show that PEW yields substantial gains over architectures wh
    
[^69]: BoMD：适用于嘈杂X光分类的多标签描述符包

    BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.01937](http://arxiv.org/abs/2203.01937)

    本文提出了一种适用于多标签、嘈杂CXR学习的方法，使用基于袋的多标签描述符平滑地重新标记数据集中的样本，并进行训练以提高模型性能。

    

    深度学习方法在医学图像问题的分类精度方面表现出色，这在很大程度上归功于具有清洁标签的大规模数据集的可用性。然而，考虑到这种手动注释的高成本，新的医学图像分类问题可能需要依赖于从放射学报告中提取的机器生成的嘈杂标签。事实上，许多胸部X光分类器已经从带有嘈杂标签的数据集中建模，但它们的训练过程通常不具有噪声标签样本的鲁棒性，导致次优模型。此外，CXR数据集大多是多标记的，因此当前设计用于多类问题的嘈杂标签学习方法不能轻松地进行调整。本文提出了一种新方法，用于嘈杂多标签CXR学习，其中检测并平滑地重新标记数据集中的样本，然后用于训练常见的多标签分类器。该方法优化了一个基于袋的多标签表示方法，以便有效地使用从放射学报告中提取的信息。

    Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a ba
    
[^70]: 深度回归中的变量误差模型的随机不确定性

    Aleatoric uncertainty for Errors-in-Variables models in deep regression. (arXiv:2105.09095v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.09095](http://arxiv.org/abs/2105.09095)

    本文提出了一种基于贝叶斯深度回归的方法，利用变量误差模型考虑所使用神经网络的输入所关联的不确定性，并将预测不确定性分解为随机和认识部分。相比于不使用该模型，使用错误变量模型能够提高对已知回归函数的覆盖率，且保持预测性能。

    

    深度学习的贝叶斯处理可以计算与深度神经网络预测相关的不确定性。我们展示了如何在贝叶斯深度回归中使用变量误差的概念，以考虑所使用神经网络的输入所关联的不确定性。所提出的方法利用了一个相关但通常被忽视的不确定性源，并将预测不确定性分解为随机和认识部分，这在统计学角度更完整，而且在很多情况下更一致。我们通过各种模拟和真实的例子讨论了这种方法，并观察到使用变量误差模型会增加不确定性，同时保持不使用变量误差模型的模型的预测性能。对于已知回归函数的例子，我们观察到变量误差模型大大提高了对基础事实的覆盖，表明该方法在回归问题中表现良好。

    A Bayesian treatment of deep learning allows for the computation of uncertainties associated with the predictions of deep neural networks. We show how the concept of Errors-in-Variables can be used in Bayesian deep regression to also account for the uncertainty associated with the input of the employed neural network. The presented approach thereby exploits a relevant, but generally overlooked, source of uncertainty and yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We discuss the approach along various simulated and real examples and observe that using an Errors-in-Variables model leads to an increase in the uncertainty while preserving the prediction performance of models without Errors-in-Variables. For examples with known regression function we observe that this ground truth is substantially better covered by the Errors-in-Variables model, indicating 
    

