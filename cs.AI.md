# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ChatQA: Building GPT-4 Level Conversational QA Models.](http://arxiv.org/abs/2401.10225) | ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。 |
| [^2] | [Supervised Fine-tuning in turn Improves Visual Foundation Models.](http://arxiv.org/abs/2401.10222) | 本文提出了一个叫做ViSFT（Vision SFT）的两阶段方法，通过在领域内任务上进行视觉联合学习来提升视觉基础模型的生成能力，在各种领域外基准测试中取得了改进。 |
| [^3] | [Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation.](http://arxiv.org/abs/2401.10211) | 本文提出了一种通过多粒度结构和多尺度序列表示耦合的PTM位点预测方法PTM-CMGMS，该方法在结构表示学习和序列表示学习上进行优化，提高了PTM位点预测的准确性。 |
| [^4] | [Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction.](http://arxiv.org/abs/2401.10210) | 通过精通指导的非参数聚类方法，预测学生在问题解决中可能采用的策略，从而实现自适应教学系统的个性化体验。 |
| [^5] | [Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems.](http://arxiv.org/abs/2401.10207) | 本文研究了用于解释性深度神经网络入侵检测系统的折衷规则提取，旨在解决黑盒解释器的不可信任问题。 |
| [^6] | [Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction.](http://arxiv.org/abs/2401.10189) | 这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。 |
| [^7] | [Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields.](http://arxiv.org/abs/2401.10178) | 深度卷积滤波器成功复制了生物感受野的结构复杂性，并且使用生物来源的权重进行初始化可以显著提高卷积网络的准确性。 |
| [^8] | [DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks.](http://arxiv.org/abs/2401.10158) | DISTINQT是一种面向未来移动和无线网络的隐私感知分布式学习框架，用于QoS预测。 |
| [^9] | [Explicitly Disentangled Representations in Object-Centric Learning.](http://arxiv.org/abs/2401.10148) | 这篇论文提出了一种在物体中心化学习中明确解开形状和纹理成分的方法，通过将潜在空间划分为两个不重叠的子集，使得模型更加稳定和有效。 |
| [^10] | [Model Compression Techniques in Biometrics Applications: A Survey.](http://arxiv.org/abs/2401.10139) | 本综述系统化地调查了生物特征识别应用中的模型压缩技术，包括量化、知识蒸馏和修剪，并分析了它们的优缺点，提出了改进方法的建议。 |
| [^11] | [Towards Principled Graph Transformers.](http://arxiv.org/abs/2401.10119) | 边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。 |
| [^12] | [Counterfactual Reasoning with Probabilistic Graphical Models for Analyzing Socioecological Systems.](http://arxiv.org/abs/2401.10101) | 本文介绍了一种应用概率图模型进行逆向推理的方法，用于分析社会生态系统。实验数据有限的情况下，这种方法能够预测生态系统对假设干预的响应，并确定变量之间的影响。这一方法为多个领域的专家提供了直观易懂的工具。 |
| [^13] | [DiffusionGPT: LLM-Driven Text-to-Image Generation System.](http://arxiv.org/abs/2401.10061) | DiffusionGPT是一个基于LLM的统一文本生成图像系统，能够处理多样化的输入并整合领域专家模型。 |
| [^14] | [Large Language Models for Scientific Information Extraction: An Empirical Study for Virology.](http://arxiv.org/abs/2401.10040) | 使用大型语言模型进行结构化的科学信息提取，在病毒学领域进行了实证研究，结果表明这种方法可以提供简洁的学术贡献摘要，对科学家进行导航和解决LLM的紧迫能力。 |
| [^15] | [LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge.](http://arxiv.org/abs/2401.10036) | LOCALINTEL是一个自动化的知识上下文化系统，利用大型语言模型的能力，从全球和本地知识数据库中自动生成组织的威胁情报。 |
| [^16] | [Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap.](http://arxiv.org/abs/2401.10034) | 该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。 |
| [^17] | [FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder.](http://arxiv.org/abs/2401.10032) | 本文介绍了一种轻量级和快速的频率感知扩散声码器FreGrad，通过离散小波变换、频率感知的扩张卷积和技巧等关键组件，实现了高质量音频的生成。在实验中，FreGrad相比基准模型训练速度快3.7倍，推断速度快2.2倍，模型大小减小0.6倍，而输出质量不受影响。 |
| [^18] | [Self-Rewarding Language Models.](http://arxiv.org/abs/2401.10020) | 该论文提出了自奖励语言模型的概念，通过LLM作为评判者，使用语言模型自己提供训练过程中的奖励。研究表明，该方法不仅可以提高指令遵循能力，还可以为自己提供高质量的奖励。通过对Llama 2 70B模型的三次迭代微调，结果在AlpacaEval 2.0排行榜上超过了其他现有系统。这项工作为实现能够不断自我改进的模型开辟了新的可能性。 |
| [^19] | [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents.](http://arxiv.org/abs/2401.10019) | 这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。 |
| [^20] | [Gender Bias in Machine Translation and The Era of Large Language Models.](http://arxiv.org/abs/2401.10016) | 本章研究了机器翻译中的性别偏见问题，介绍了传统神经机器翻译方法和生成预训练转换器模型中相关工作。通过在英语-意大利语的翻译环境中使用ChatGPT进行实验，评估了其解决性别偏见的能力。研究结果强调了减少机器翻译系统偏见的重要性。 |
| [^21] | [A-KIT: Adaptive Kalman-Informed Transformer.](http://arxiv.org/abs/2401.09987) | 这项研究提出了A-KIT，一种自适应的Kalman-informed transformer，用于在线学习传感器融合中变化的过程噪声协方差。它通过适应实际情况中的过程噪声变化，改进了估计状态的准确性，避免了滤波器发散的问题。 |
| [^22] | [FLex&Chill: Improving Local Federated Learning Training with Logit Chilling.](http://arxiv.org/abs/2401.09986) | FLex&Chill 提出了一种通过Logit Chilling方法改进本地联合学习训练的方法，可以加快模型收敛并提高推理精度。 |
| [^23] | [Multiobjective Optimization Analysis for Finding Infrastructure-as-Code Deployment Configurations.](http://arxiv.org/abs/2401.09983) | 该论文针对优化基础设施即代码部署配置的多目标问题进行了深入分析，并提出了适用于IOP的最佳多目标方法。 |
| [^24] | [Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection.](http://arxiv.org/abs/2401.09966) | 本文提出了一个条件生成模型（RAISE），通过在潜在空间中进行规则抽象和选择，以解决Raven的渐进矩阵问题，该模型能够在现实的场景中展示出抽象推理的能力。 |
| [^25] | [When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference.](http://arxiv.org/abs/2401.09964) | 本研究探索了代码补全领域中动态推断的应用，在对GPT-2进行实证研究的基础上发现，仅使用第一层即可准确生成超过一半的标记，从而节省了大量计算资源。使用所有层仍然无法完全正确预测一部分标记。 |
| [^26] | [WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV.](http://arxiv.org/abs/2401.09944) | WindSeer是一个名为WindSeer的神经网络，能够在实时预测低空风的同时节省计算资源。通过使用稀疏的测量数据和合成数据进行训练，它可以成功地预测已知地形上的真实风场，并在不同的分辨率和域大小上生成准确的预测结果，无需重新训练。 |
| [^27] | [Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking.](http://arxiv.org/abs/2401.09942) | 本文提出了一种多任务学习的方法，用于联合识别、团队归属和角色分类的运动视觉跟踪。通过使用共享的主干网络，该方法在计算上更高效，并且能够产生更丰富和有区别度的表示。 |
| [^28] | [XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection.](http://arxiv.org/abs/2401.09900) | 本文提出了一个基于XAI的框架，通过使用CAM-based解释来改进语义分割模型，从而增强视觉质量检测系统。评估结果显示，XAI增强的模型在复杂对象分割方面表现出色。 |
| [^29] | [Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network.](http://arxiv.org/abs/2401.09886) | 本论文提出了一种基于弹性联邦和多智能体深度强化学习的合作边缘缓存方案，通过训练个性化的本地模型，预测准确受欢迎的内容，并在不同的SBS之间合作缓存热门内容，以达到优化获取内容成本的目标。 |
| [^30] | [Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition.](http://arxiv.org/abs/2401.09880) | 本研究提出了一种基于注意力的循环神经网络用于自动识别下蛋鸡行为。通过声音分析和特征提取，构建了一个鲁棒的行为特征化系统，对下蛋鸡的健康行为进行监测和识别。实验结果表明该模型具有良好的综合性能。 |
| [^31] | [Reconciling Spatial and Temporal Abstractions for Goal Representation.](http://arxiv.org/abs/2401.09870) | 本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。 |
| [^32] | [Improving fine-grained understanding in image-text pre-training.](http://arxiv.org/abs/2401.09865) | 本研究引入了一种名为SPARC的方法，通过在图像-文本对中学习每个令牌的图像组合，以提高图像-文本预训练中的细粒度理解能力。SPARC方法结合了细粒度损失和对比损失，可以以较低的计算成本学习同时编码全局和局部信息的表示。 |
| [^33] | [Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments.](http://arxiv.org/abs/2401.09862) | 本研究提出了一种针对语言模型提示优化的进化多目标方法，通过情感分析为案例研究，实现了生成能够同时体现两种相互冲突情感的提示语，从而提高模型的性能和相关信息的提取能力。 |
| [^34] | [Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models.](http://arxiv.org/abs/2401.09861) | 本研究提出了一种创新的方法，通过从视频内容中提取时间特定的信息，来解决多模态大型语言模型中的事件级幻觉问题。 |
| [^35] | [Enhancing the Fairness and Performance of Edge Cameras with Explainable AI.](http://arxiv.org/abs/2401.09852) | 本研究提出了一种使用可解释的人工智能进行边缘摄像头模型调试的方法，通过解决训练数据集的偏见问题来提高公平性和性能。 |
| [^36] | [Behavioral Simulation: Exploring A Possible Next Paradigm for Science.](http://arxiv.org/abs/2401.09851) | 本文研究了仿真技术的发展与科学范式的演变，并提出了行为仿真的概念，代表了更高程度的范式整合。 |
| [^37] | [Slicer Networks.](http://arxiv.org/abs/2401.09833) | 本论文提出了一种切片网络的新型架构，通过使用低频逼近的方法进行特征提取和上采样，从而在医学图像分析中提高了性能。 |
| [^38] | [PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning.](http://arxiv.org/abs/2401.09819) | PPNet是一种新颖的神经网络结构，用于解决端到端近似最优路径规划问题。通过将路径规划问题分为两个子问题，并使用两级级联神经网络进行求解，同时引入了一种高效的数据生成方法EDaGe-PP。实验结果表明，PPNet在计算时间和成功率方面比其他方法有显著提升。 |
| [^39] | [All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks.](http://arxiv.org/abs/2401.09798) | 本研究提出了一种简单的黑盒方法，用于生成越狱攻击提示，克服了现有方法的复杂性和计算成本的限制。该方法通过使用语言模型自身，将有害提示重写为非有害表达，实现了超过80%的攻击成功率，并且即使模型更新，效果仍然有效。 |
| [^40] | [A Comparative Analysis on Metaheuristic Algorithms Based Vision Transformer Model for Early Detection of Alzheimer's Disease.](http://arxiv.org/abs/2401.09795) | 本文提出了一种基于元启发式算法的Vision Transformer模型，用于早期检测阿尔茨海默病。通过大量的测试数据验证，该模型在准确度、精确度、召回率以及F1分数等方面展现了卓越的性能。 |
| [^41] | [A Semantic Approach for Big Data Exploration in Industry 4.0.](http://arxiv.org/abs/2401.09789) | 本文介绍了一个产业4.0场景下基于语义的可视化查询系统的提案，该系统允许领域专家以友好的方式探索和可视化数据。系统的主要创新在于结合使用语义注释的捕获数据和2D定制数字机器的数字表示形式。 |
| [^42] | [Querying Easily Flip-flopped Samples for Deep Active Learning.](http://arxiv.org/abs/2401.09787) | 本文提出了一种基于模型的预测不确定性度量，即最小不一致度量（LDM），用于解决复杂决策边界情况下的主动学习问题。通过查询具有最小LDM的未标记数据，可以提高深度学习模型的性能。 |
| [^43] | [Adaptive Self-training Framework for Fine-grained Scene Graph Generation.](http://arxiv.org/abs/2401.09786) | 本论文提出了一种自适应自训练框架用于细粒度场景图生成，通过利用未标注的三元组缓解了场景图生成中的长尾问题。同时，引入了一种新颖的伪标签技术CATM和图结构学习器GSL来提高模型性能。 |
| [^44] | [SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation.](http://arxiv.org/abs/2401.09773) | SEINE是一种用于核实例分割的结构编码和交互网络，通过考虑核结构的相关性和利用核之间的结构相似性来提高每个分割实例的完整性。 |
| [^45] | [Towards Learning from Graphs with Heterophily: Progress and Future.](http://arxiv.org/abs/2401.09769) | 本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。 |
| [^46] | [CLIP Model for Images to Textual Prompts Based on Top-k Neighbors.](http://arxiv.org/abs/2401.09763) | 提出了一种基于前k近邻的CLIP模型，实现了成本效益高的图像到文本提示生成。该方法无需大量标注数据，通过将生成模型和K近邻算法结合使用，并通过在线和离线任务方式实现。相较于其他模型，该方法在指标上取得了更高的表现。 |
| [^47] | [Cooperative Tri-Point Model-Based Ground-to-Air Coverage Extension in Beyond 5G Networks.](http://arxiv.org/abs/2401.09757) | 本论文提出了一种基于合作的三点模型，利用合作波束增强地-空覆盖扩展。通过分析G2A覆盖扩展，证明了三个TBSs之间的合作能够实现最小化覆盖重叠的G2A覆盖，并根据Delaunay三角剖分设计了合作覆盖结构。 |
| [^48] | [Explaining Drift using Shapley Values.](http://arxiv.org/abs/2401.09756) | 本文提出了一个新的框架-DBShap，使用Shapley值来确定模型性能漂移的主要贡献者并量化他们的贡献。通过DBShap提供的解释，可以理解漂移背后的根本原因。 |
| [^49] | [Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework.](http://arxiv.org/abs/2401.09748) | 引入了一个基于函数图像和操作树序列的科学计算多模态框架（Botfip），应用于符号回归问题，并验证了其在低复杂度问题上的优势，展示了其潜力。这个多模态框架在科学计算问题中具有广泛的应用前景。 |
| [^50] | [Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder.](http://arxiv.org/abs/2401.09717) | 本文通过分析自闭症儿童与心理学家之间的诊断性对话中提取的声学/韵律和语言特征，研究了自闭症谈话分析的参数选择。研究结果可以提供对ASD儿童谈话数据的细致分析，支持诊断和干预。 |
| [^51] | [HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization.](http://arxiv.org/abs/2401.09716) | HCVP是一种基于层次对比视觉提示的领域泛化方法，通过引导模型将不变特征与特定特征分离，提高了泛化性能。 |
| [^52] | [Curriculum Recommendations Using Transformer Base Model with InfoNCE Loss And Language Switching Method.](http://arxiv.org/abs/2401.09699) | 这项研究提出了使用Transformer基础模型、InfoNCE损失和语言切换方法来解决课程推荐中的内容冲突和语言翻译引起的干扰问题，旨在构建一个个性化学习体验、包容多样性的教育环境。 |
| [^53] | [Should ChatGPT Write Your Breakup Text? Exploring the Role of AI in Relationship Dissolution.](http://arxiv.org/abs/2401.09695) | 这项研究探讨了AI在恋爱解体过程中的作用。研究发现，当前技术在信息收集、社群支持和促进沟通方面发挥着重要作用。参与者预计AI可以满足不同阶段的需求，帮助解体恋情。 |
| [^54] | [Imitation Learning Inputting Image Feature to Each Layer of Neural Network.](http://arxiv.org/abs/2401.09691) | 本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。 |
| [^55] | [Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach.](http://arxiv.org/abs/2401.09680) | 本论文提出了一种基于小型机器学习的Stackelberg博弈框架，在无人机Metaverse中实现高效的双胞胎迁移，以提供无缝沉浸式体验。 |
| [^56] | [Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach.](http://arxiv.org/abs/2401.09671) | 本研究旨在解决无监督领域转换中的可识别性问题，引入了一个MPA消除理论，解决了CycleGAN及其变体产生内容不对齐的限制。 |
| [^57] | [Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data.](http://arxiv.org/abs/2401.09666) | 本研究提出了一种使用深度强化学习和真实世界轨迹数据的自动驾驶车辆交通平滑控制器。通过观察前方车辆的速度和距离以及交通的下游状态，我们训练出了能够减少能耗的波浪平滑策略，并在低自动驾驶车辆渗透率下实现了显著的燃油节省。 |
| [^58] | [Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks.](http://arxiv.org/abs/2401.09656) | 在车联网中，通过收敛性分析，本文证明移动性对分层联邦学习的收敛速度有积极影响，它增加了边缘层异构数据的融合和更快的数据融合速度，从而提高模型准确性。 |
| [^59] | [Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning.](http://arxiv.org/abs/2401.09651) | 本研究通过凸二级优化技术，开发了一个通用的基于梯度的神经和符号参数学习框架，具有100倍以上的学习时间改进和高达16%的预测性能提升。 |
| [^60] | [ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change.](http://arxiv.org/abs/2401.09646) | ClimateGPT是一个针对气候变化领域的跨学科研究合成的AI模型，通过优化检索增强和使用级联机器翻译方法，提高了模型的性能和可访问性。 |
| [^61] | [Blackout Mitigation via Physics-guided RL.](http://arxiv.org/abs/2401.09640) | 本文设计了一种物理引导的强化学习框架，利用传输网络的潮流灵敏度因子来指导强化学习训练，实现了通过实时补救前瞻决策来减轻黑暗模式的目标。 |
| [^62] | [Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study.](http://arxiv.org/abs/2401.09637) | 通过大型语言模型辅助阅读临床笔记，患者可以获得更好的理解和自信。这项研究开发了一个工具，利用语言模型简化和增加上下文，使临床笔记更易读。研究结果表明，这些增强对患者有益。 |
| [^63] | [Learning Shortcuts: On the Misleading Promise of NLU in Language Models.](http://arxiv.org/abs/2401.09615) | 该论文调查了大型语言模型在自然语言理解任务中使用捷径学习的现象，强调了这种现象对语言模型评估的影响，并呼吁加大对捷径学习的研究力度以提升语言模型的鲁棒性和实际场景中的自然语言理解评估标准。 |
| [^64] | [Handling Large-scale Cardinality in building recommendation systems.](http://arxiv.org/abs/2401.09572) | 本文提出了两种创新技术来解决建议系统中高基数的挑战，包括采用词袋模型和层共享来减小模型大小并提高性能。通过对Uber使用情况的实验验证，证明了这些技术在优化建议系统和提高性能方面的有效性。 |
| [^65] | [Aligning Large Language Models with Counterfactual DPO.](http://arxiv.org/abs/2401.09566) | 本文研究了在大型语言模型中使用反事实对抗优化框架，以实现风格对齐，避免人类干预，并成功培养出可取行为和减轻不可取行为。 |
| [^66] | [Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality.](http://arxiv.org/abs/2401.09556) | 本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。 |
| [^67] | [Improving Classification Performance With Human Feedback: Label a few, we label the rest.](http://arxiv.org/abs/2401.09555) | 本文探讨了通过人类反馈来改进分类模型性能的方法。使用少量有标签示例，通过连续反馈循环，我们能够显著提高模型的准确性。在多个数据集上进行评估，结果表明这种方法能够超越零样本大型语言模型，提供更强的文本分类性能。 |
| [^68] | [BERTologyNavigator: Advanced Question Answering with BERT-based Semantics.](http://arxiv.org/abs/2401.09553) | BERTologyNavigator是一个基于BERT语义的高级问题回答系统，结合关系抽取和BERT嵌入，可以在DBLP知识图谱中精确地导航关系，并在测试数据集上达到了较高的F1分数。 |
| [^69] | [Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling.](http://arxiv.org/abs/2401.09516) | 该论文提出了一种名为排序克里洛夫回收（SKR）的新方法，用于加速神经算子训练的数据生成。该方法解决了现有方法在解决PDE问题时计算冗余的问题，显著提高了数据生成效率。 |
| [^70] | [Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility.](http://arxiv.org/abs/2401.09498) | 本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。 |
| [^71] | [Memory, Space, and Planning: Multiscale Predictive Representations.](http://arxiv.org/abs/2401.09491) | 本研究通过综合计算、行为和神经证据，揭示了记忆与预测、规划之间的相互关系以及其在海马体和前额叶皮质中的多尺度预测表示，为我们理解大脑中的记忆和规划机制提供了重要启示。 |
| [^72] | [PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies.](http://arxiv.org/abs/2401.09489) | PUPAE是一种直观且可操作的时间序列异常解释方法，通过引入领域无关的反事实解释，能够帮助解释和处理异常。 |
| [^73] | [Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning.](http://arxiv.org/abs/2401.09479) | 本文提出了一种使用多模态深度学习进行硬件特洛伊检测的方法，通过生成对抗网络扩充数据，并采用早融合和晚融合策略进行评估。通过估计不确定性量化指标，实现风险感知的决策制定。 |
| [^74] | [A Framework for Agricultural Food Supply Chain using Blockchain.](http://arxiv.org/abs/2401.09476) | 本文提出了一个使用区块链的农业食品供应链框架，旨在通过建立信任和透明度确保食品供应链的安全性，并解决信息防篡改和供需关系等困难。 |
| [^75] | [Business and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems.](http://arxiv.org/abs/2401.09473) | 本文关注国内会话生成AI强化的多机器人系统中的商业和伦理问题。合作式多机器人系统能够革新不同行业的流程并改变人类的商业方式，但也需要对其潜在的利益冲突、隐私实践和安全问题进行伦理考察。 |
| [^76] | [Offline Handwriting Signature Verification: A Transfer Learning and Feature Selection Approach.](http://arxiv.org/abs/2401.09467) | 这篇论文介绍了一种离线手写签名验证的方法，通过迁移学习和特征选择来提高验证结果，在金融、法律文件和安全等领域有广泛应用。 |
| [^77] | [Self Supervised Vision for Climate Downscaling.](http://arxiv.org/abs/2401.09466) | 这项工作提出了一种深度学习模型，用于下调尺度 ESM 模拟数据，不需要高分辨率的真实数据进行模型优化。 |
| [^78] | [What's my role? Modelling responsibility for AI-based safety-critical systems.](http://arxiv.org/abs/2401.09459) | 许多作者已经评论了AI-SCS的“责任差距”，即开发人员和制造商难以对AI的有害行为负责，这是由于AI的复杂开发周期、性能不确定性和动态操作环境所致。在AI-SCS变得自主化后，人类操作员可能成为承担责任的替罪羊，他们可能承担由AI-SCS输出的后果，而这些后果他们没有参与创建，也可能不理解。 |
| [^79] | [Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach.](http://arxiv.org/abs/2401.09455) | 本论文提出了一种基于约束的多智能体强化学习方法来解决集成卫星-地面网络（ISTN）系统的动态路由问题，有效平衡了快速通信、能源效率和数据包丢失要求。 |
| [^80] | [Voila-A: Aligning Vision-Language Models with User's Gaze Attention.](http://arxiv.org/abs/2401.09454) | 本文介绍了一种使用用户注视注意力对齐视觉-语言模型的方法，在处理复杂场景和多个物体的实际应用中提高了模型的可解释性和效果。 |
| [^81] | [Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings.](http://arxiv.org/abs/2401.09452) | 该论文提出了一种将黎曼几何特征应用于学习翼面压力系数分布的方法，以提高气动系数的预测准确性。 |
| [^82] | [Diffusion-Driven Generative Framework for Molecular Conformation Prediction.](http://arxiv.org/abs/2401.09451) | 本文介绍了一种基于扩散驱动的生成框架\method{}，用于预测分子的三维构象，具有较高的预测精度并改进了传统方法的不足。 |
| [^83] | [Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative.](http://arxiv.org/abs/2401.09450) | EMPAIA倡议汇集了病理学领域的利益相关方，开发了技术互操作标准，标准化接口，以及推广和应用人工智能方法于病理学诊断的建议，实现了不同供应商的多个AI应用程序整合。 |
| [^84] | [Tumbug: A pictorial, universal knowledge representation method.](http://arxiv.org/abs/2401.09448) | Tumbug是一种图像化的通用知识表达方法，专门用于常识推理，通过使用约30个基于科学和人类生活的基本概念组件，将其与以往的概念依存理论进行区分。 |
| [^85] | [Explainable Multimodal Sentiment Analysis on Bengali Memes.](http://arxiv.org/abs/2401.09446) | 这项研究提出了一个多模态方法来解释孟加拉语Memes的情感，以填补此领域中低资源语言的研究空白。对比现有的数据集，提出了一个新的MemoSen数据集并表明其准确率的局限性。这项研究的主要贡献是在孟加拉语Memes情感分析领域引入了多模态方法。 |
| [^86] | [Online Handbook of Argumentation for AI: Volume 4.](http://arxiv.org/abs/2401.09444) | 《AI在线辩论手册》的第四卷是为了为辩论研究社区提供开放访问和策划的文集，主要聚焦于对辩论的计算模型的研究和应用。 |
| [^87] | [CRD: Collaborative Representation Distance for Practical Anomaly Detection.](http://arxiv.org/abs/2401.09443) | 本文提出了一种基于协同表示模型的图像补丁距离计算方法，避免了查询图像和存储的补丁之间的最近邻搜索所带来的复杂度问题，能够在边缘环境下进行快速部署实用的异常检测。 |
| [^88] | [Object Attribute Matters in Visual Question Answering.](http://arxiv.org/abs/2401.09442) | 本论文提出了一种新的视觉问答方法，通过利用物体属性来实现更好的物体级视觉语言对齐和多模态场景理解。具体地，设计了属性融合模块和对比知识蒸馏模块，通过信息传递构建了一个多模态图神经网络，提高了物体级视觉特征，从而解决了细粒度问题。 |
| [^89] | [Reasoning with random sets: An agenda for the future.](http://arxiv.org/abs/2401.09435) | 本文讨论了随机集合理论未来的发展议程，包括推广统计推理、发展几何方法、应用于气候变化和机器学习等领域。 |
| [^90] | [RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models.](http://arxiv.org/abs/2401.09432) | RoleCraft-GLM是一个创新框架，通过大型语言模型实现个性化角色扮演，解决了缺乏个性化互动的问题。通过独特的对话数据集和细致入微的角色发展，它能够生成准确反映角色个性特征和情感的对话，提升用户参与度。 |
| [^91] | [Precipitation Prediction Using an Ensemble of Lightweight Learners.](http://arxiv.org/abs/2401.09424) | 本文提出了一个使用多个轻量级学习器的集成预测降水模型，并通过使用卫星图像进行训练，有效地模拟复杂的降雨模式，特别是对于高降水事件。在Weather4Cast 2023竞赛中取得了第一名。 |
| [^92] | [Preparing Lessons for Progressive Training on Language Models.](http://arxiv.org/abs/2401.09192) | 提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。 |
| [^93] | [MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data.](http://arxiv.org/abs/2401.08727) | 提出了一种新的交通拥堵预测模型，使用车辆轨迹数据以及多邻接关系注意力图卷积网络（MA2GCN）来预测交通拥堵情况，不依赖于传感器数据，提取灵活且准确的交通信息。 |
| [^94] | [Are self-explanations from Large Language Models faithful?.](http://arxiv.org/abs/2401.07927) | 大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。 |
| [^95] | [TAROT: A Hierarchical Framework with Multitask Co-Pretraining on Semi-Structured Data towards Effective Person-Job Fit.](http://arxiv.org/abs/2401.07525) | TAROT是一个层次化的多任务预训练框架，通过对半结构化数据进行预训练，结合多粒度的任务来提升人-岗位匹配的效果。 |
| [^96] | [Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering.](http://arxiv.org/abs/2401.07510) | 开发用于生物学和医学的ChatGPT，通过自然语言处理和多模态范式，加速了医学问题回答的进展，并且能够处理医学环境中的大规模、多样化、无标签数据分析场景。 |
| [^97] | [Hierarchical Fashion Design with Multi-stage Diffusion Models.](http://arxiv.org/abs/2401.07450) | 本论文提出了一种名为HieraFashDiff的新型时尚设计方法，它使用多级扩散模型实现了从高级设计概念到低级服装属性的分层设计和编辑，解决了当前在时尚设计中的挑战。 |
| [^98] | [E^2-LLM: Efficient and Extreme Length Extension of Large Language Models.](http://arxiv.org/abs/2401.06951) | E^2-LLM是一种高效和极长扩展方法，通过仅需一次训练过程和不收集长上下文数据的方式，在大规模语言模型中实现了显著减少的计算成本。基于RoPE位置嵌入，E^2-LLM只需要较短的训练数据长度，支持不同的评估上下文窗口。 |
| [^99] | [Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning.](http://arxiv.org/abs/2401.06805) | 这篇综述调查了多模态大语言模型（MLLMs）的推理能力，包括评估协议、模型前沿和推理密集型任务的应用，旨在实现强人工智能（Strong AI）或人工通用智能（AGI）的抽象推理能力。 |
| [^100] | [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training.](http://arxiv.org/abs/2401.05566) | 该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。 |
| [^101] | [MISS: A Generative Pretraining and Finetuning Approach for Med-VQA.](http://arxiv.org/abs/2401.05163) | MISS是一种适用于医学视觉问答的生成式预训练与微调方法。相比于现有方法，我们把医学视觉问答作为一个生成式任务处理，通过多任务学习使图像和文本特征对齐，并通过使用大型语言模型扩展单模态图像数据集的转换和字幕方法实现特征空间的扩展。 |
| [^102] | [ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge.](http://arxiv.org/abs/2401.03473) | ICMC-ASR挑战赛是为了促进驾驶场景下的语音处理和识别研究而举办的，包括自动语音识别（ASR）和自动语音分离和识别（ASDR）两个赛道，取得了显著的改善。最终，USTCiflytek队在ASR赛道上获得了13.16%的CER，ASDR赛道上获得了21.48%的cpCER。 |
| [^103] | [Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis.](http://arxiv.org/abs/2401.02860) | 该论文提出了一个利用矩阵分析方法的框架，用于推理时间序列中的跟随模式。在模拟数据集和声音记录数据集中，该框架优于基准方法，并能够检测出加密货币数据集中的跟随模式。 |
| [^104] | [Contrastive learning-based agent modeling for deep reinforcement learning.](http://arxiv.org/abs/2401.00132) | 本研究提出了一种基于对比学习的深度强化学习代理建模方法，该方法可以在仅利用自我代理的本地观测的情况下，提取其他代理的有意义策略表示，以改进自我代理的自适应策略。 |
| [^105] | [Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning.](http://arxiv.org/abs/2312.17484) | 该论文提出了一种名为真实森林的方法，通过使用多维正交探针，揭示隐藏的真实表示，从而增强大型语言模型中的真实性。作者将正交约束融入探针，创建不同的正交基，通过随机窥视技术，减小了模型生成和识别真实特征之间的差距。实验证明，该方法显著提高了模型的真实性。 |
| [^106] | [Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.](http://arxiv.org/abs/2312.16171) | 本文提出了26个指导原则，以简化对大型语言模型进行提问和提示的过程。通过在LLaMA-1/2和GPT-3.5/4上进行实验证明了这些原则的有效性。 |
| [^107] | [Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs.](http://arxiv.org/abs/2312.14345) | 本研究提出了一个框架称为逻辑搭建，通过结合面向方面的解释和思维链提示的思想，在中间推理步骤中生成推荐解释。该框架能够克服现有模型在产生零炮击解释方面的困难。 |
| [^108] | [Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed.](http://arxiv.org/abs/2312.12469) | 本论文提出了一种通用的引导非自回归知识蒸馏（GNARKD）方法，通过知识蒸馏将自回归模型中的关键组件保留在网络架构中，从而获得具有低推理延迟的高性能非自回归车辆路径问题求解器。 |
| [^109] | [Quantifying Divergence for Human-AI Collaboration and Cognitive Trust.](http://arxiv.org/abs/2312.08722) | 通过量化人工智能协作和认知信任的差异，我们发现人们倾向于与最相似的模型进行协作。 |
| [^110] | [A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML.](http://arxiv.org/abs/2312.06305) | 本文提出了一种元级学习算法SHSR，用于减少AutoML中的超参数空间，减少了约30%的执行时间并且性能损失小于0.1%。 |
| [^111] | [Labeling Neural Representations with Inverse Recognition.](http://arxiv.org/abs/2311.13594) | 逆向识别 (INVERT) 是一种可扩展的方法，通过连接学习到的神经表示与人类可理解的概念，实现了对神经表示的标记并提供了统计显著性评估指标。 |
| [^112] | [INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing.](http://arxiv.org/abs/2311.09868) | INTERVENOR模型通过模拟人类修复代码的行为，使用交互式修复链条来引导大型语言模型的编码能力，取得了显著的性能提升。 |
| [^113] | [Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values.](http://arxiv.org/abs/2311.09262) | 这项研究提出了一种新颖的图神经网络（称为DPPDCC），用于将论文的潜在影响分解为传播、一致性和贡献值。通过编码时态和结构特征，捕捉知识流动，并使用对比增强图揭示流行度，进一步预测引用分组来建模一致性。应用正交约束来鼓励独特建模，并保留原始信息。 |
| [^114] | [Improved DDIM Sampling with Moment Matching Gaussian Mixtures.](http://arxiv.org/abs/2311.04938) | 在DDIM框架中使用GMM作为反向转移算子，通过矩匹配可以获得质量更高的样本。在无条件模型和类条件模型上进行了实验，并通过FID和IS指标证明了我们的方法的改进效果。 |
| [^115] | [Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery.](http://arxiv.org/abs/2310.19776) | 本论文提出了一种新颖、高效和自我监督的方法，可以在测试时发现以前未知的类别，通过将最小长度类别代码分配给单个数据实例来增强对类别细粒度的控制。 |
| [^116] | [Debiasing Algorithm through Model Adaptation.](http://arxiv.org/abs/2310.18913) | 本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。 |
| [^117] | [Emotion Recognition by Video: A review.](http://arxiv.org/abs/2310.17212) | 本文是一篇关于视频情感识别的综述论文，总结了相关研究中的现有趋势、情感模型、数据库以及单模态和多模态的视频情感识别方法的结构、性能和优缺点。 |
| [^118] | [Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection.](http://arxiv.org/abs/2310.12086) | 该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。 |
| [^119] | [Functional Invariants to Watermark Large Transformers.](http://arxiv.org/abs/2310.11446) | 本文介绍了一种用于大型Transformer的功能不变性水印技术，它使用模型的不变性生成功能上等效的副本，并能在不改变模型输出的情况下给模型加上水印，这是一种计算成本极低且适用于实际应用的解决方案。 |
| [^120] | [Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes.](http://arxiv.org/abs/2309.12971) | 本文提出了基于花瓣拉普拉斯的高阶图卷积网络，通过利用简单复合体来建模高阶交互，在不同拓扑尺度上识别内在特征，并使用可学习的图滤波器来量化高阶交互强度。 |
| [^121] | [Panoptic Vision-Language Feature Fields.](http://arxiv.org/abs/2309.05448) | 本文提出了一种用于3D场景中开放词汇全景分割的算法PVLFF，通过从预训练的2D模型中提取视觉-语言特征来学习语义特征场，通过对输入帧上的2D实例分割进行对比学习来联合拟合实例特征场。该方法在全景分割和语义分割方面具有良好的性能。 |
| [^122] | [LLM Powered Sim-to-real Transfer for Traffic Signal Control.](http://arxiv.org/abs/2308.14284) | 本研究利用大型语言模型（LLMs）通过基于提示的行动转换，解决了交通信号控制任务中从仿真到真实的迁移问题。 |
| [^123] | [Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network.](http://arxiv.org/abs/2308.03200) | 本文利用智能手机拍摄的图像，通过发展一个深度卷积神经网络，成功预测了特定位置的PM2.5浓度，揭示了本地聚合的空气质量指数的潜力。 |
| [^124] | [Curvature-based Transformer for Molecular Property Prediction.](http://arxiv.org/abs/2307.13275) | 该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。 |
| [^125] | [LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition.](http://arxiv.org/abs/2307.13269) | 本文研究了LoRA组合在跨任务通用性上的可行性，并提出了LoraHub框架，能够通过组合不同任务上训练的LoRA模块，实现对未见任务的可适应性性能。实验结果表明，LoraHub在少样本场景中能够有效模拟上下文学习的性能，而无需上下文示例。 |
| [^126] | [Knapsack: Connectedness, Path, and Shortest-Path.](http://arxiv.org/abs/2307.12547) | 该论文研究了带有图论约束的背包问题，证明了问题的复杂性并提出了近似算法。 |
| [^127] | [Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives.](http://arxiv.org/abs/2307.02140) | 本文探讨了开放联邦学习平台的技术和法律观察，提出了基于查询和基于合同的两种适用于开放联邦学习的合作框架，并对构建开放的FL平台的可行性进行了全面评估。 |
| [^128] | [Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases.](http://arxiv.org/abs/2306.09138) | 本论文研究了如何利用概率语义来查询不一致的描述逻辑知识库，并通过实验证明了该方法的有效性。 |
| [^129] | [Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data.](http://arxiv.org/abs/2306.05535) | 政治辩论、演讲和访谈中的值得核实的论断可以使用音频数据进行检测和确认，这可帮助主持人、记者和事实核查组织进行工作。 |
| [^130] | [Simulation-Based Counterfactual Causal Discovery on Real World Driver Behaviour.](http://arxiv.org/abs/2306.03354) | 本文提出了基于仿真的反事实因果发现方法，通过重新定义问题和使用反事实仿真来解决因果关系非稳态问题和干预限制，在真实驾驶行为中得到了评估。 |
| [^131] | [Thought Cloning: Learning to Think while Acting by Imitating Human Thinking.](http://arxiv.org/abs/2306.00323) | 本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。 |
| [^132] | [A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem.](http://arxiv.org/abs/2305.17198) | 提出了一个基于模型的离线多智能体强化学习方法MOMA-PPO，通过生成合成交互数据并优化智能体的政策，解决了策略一致性和策略微调两个协调问题，在具有挑战性的离线MARL场景中胜过主流的学习方法，提供了实际应用中的可行解决方案。 |
| [^133] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^134] | [GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs.](http://arxiv.org/abs/2305.12788) | 本论文提出了一种名为GraphCare的框架，通过使用个性化知识图谱来改进基于电子健康记录的医疗预测，并通过在两个公共数据集上的实验证明了其有效性。 |
| [^135] | [Conversational Process Modelling: State of the Art, Applications, and Implications in Practice.](http://arxiv.org/abs/2304.11065) | 本文系统的研究了现有聊天机器人对于支持对话式流程建模所提供的应用场景，并推导出了在实践中使用聊天机器人进行对话式流程建模的建议。 |
| [^136] | [CodeKGC: Code Language Model for Generative Knowledge Graph Construction.](http://arxiv.org/abs/2304.09048) | 本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。 |
| [^137] | [NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking.](http://arxiv.org/abs/2304.04640) | NeuroBench是由学术界和工业界成员共同开发的一套协作、公平和代表性的基准测试，可以解决神经形态计算中缺乏清晰标准的问题，推动该领域的发展。 |
| [^138] | [Kernel Affine Hull Machines for Differentially Private Learning.](http://arxiv.org/abs/2304.01300) | 本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。 |
| [^139] | [Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification.](http://arxiv.org/abs/2303.12307) | 本文提出了一种曲率平衡特征流形学习的方法，探究了感知流形的几何特性对分类难度的影响，发现曲率不平衡会导致模型不公平。 |
| [^140] | [ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure.](http://arxiv.org/abs/2303.02472) | ESD是一种无需调参的可训练校准目标损失，通过将校准误差看作两个期望值之间的平方差，可以改善神经网络模型的校准度。 |
| [^141] | [Data-centric Artificial Intelligence.](http://arxiv.org/abs/2212.11854) | 数据中心人工智能是一种新兴的范式，它强调系统性地设计和构建数据对于建立有效和高效的基于人工智能的系统至关重要。 |
| [^142] | [An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning.](http://arxiv.org/abs/2211.11086) | 本文研究了一种名为SimiS的简单但被忽视的基准方法，通过将伪标签作为标签数据的补充，根据与最频繁类别的类别分布差异，有效地减少了不平衡半监督学习中的类别不平衡，相对于现有方法有显著的性能提升。 |
| [^143] | [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks.](http://arxiv.org/abs/2210.15629) | 本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。 |
| [^144] | [Normality-Guided Distributional Reinforcement Learning for Continuous Control.](http://arxiv.org/abs/2208.13125) | 本论文研究了连续控制任务中的值分布，并发现学习的值分布与正态分布非常接近。基于这一观察，提出了一种正态引导的分布式强化学习方法，利用方差网络预测的方差和回报，以及与标准值函数不同的值分布结构特征来更新策略。这种方法在两种在线算法上产生了显著效果。 |
| [^145] | [From Procedures, Objects, Actors, Components, Services, to Agents -- A Comparative Analysis of the History and Evolution of Programming Abstractions.](http://arxiv.org/abs/2112.12508) | 本研究回顾性地分析了编程抽象的演变过程，从过程、对象、角色、组件、服务到Agent。研究发现，不断追求更高的灵活性和抽象级别是一个持续的趋势，而组件、服务和Agent的概念在实现软件模块化和重构性方面具有共同的目标。 |

# 详细

[^1]: ChatQA: 构建GPT-4级对话问答模型

    ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])

    [http://arxiv.org/abs/2401.10225](http://arxiv.org/abs/2401.10225)

    ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。

    

    在这项工作中，我们介绍了ChatQA，一系列具有GPT-4级别准确性的对话问答模型。具体地，我们提出了一个两阶段的指令调整方法，可以显著提高大型语言模型（LLM）在零-shot对话问答中的结果。为了处理对话问答中的检索问题，我们在多轮问答数据集上进行了密集检索器的微调，这样可以提供与使用最先进的查询重写模型相当的结果，同时大大降低部署成本。值得注意的是，我们的ChatQA-70B可以在10个对话问答数据集的平均分上超过GPT-4（54.14 vs. 53.90），而不依赖于OpenAI GPT模型的任何合成数据。

    In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
    
[^2]: 监督微调进一步改进了视觉基础模型

    Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])

    [http://arxiv.org/abs/2401.10222](http://arxiv.org/abs/2401.10222)

    本文提出了一个叫做ViSFT（Vision SFT）的两阶段方法，通过在领域内任务上进行视觉联合学习来提升视觉基础模型的生成能力，在各种领域外基准测试中取得了改进。

    

    近年来，像CLIP这样的图像-文本训练方法已经主导了视觉基础模型的预训练。随后，为了将区域级别的视觉学习引入CLIP的预训练中，在大规模区域级别数据集的缺乏下面临可扩展性挑战。受到自然语言处理中监督微调（SFT）的启发，比如指令微调，我们探索了在视觉基础模型的预训练之后，微粒SFT能够提升其生成能力的潜力。因此，我们提出了一个两阶段的方法ViSFT（Vision SFT），来释放视觉基础模型的细粒度知识。在ViSFT中，通过在一些领域内任务上进行视觉联合学习来增强视觉基础模型，然后在领域外基准测试中进行测试。通过使用ViSFT在少于2天内在8个V100 GPU上进行更新，一个具有超过4.4B参数的视觉Transformer在各种领域外基准测试中显示出改进。

    Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and visi
    
[^3]: 通过多粒度结构和多尺度序列表示的耦合改进PTM位点预测

    Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation. (arXiv:2401.10211v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.10211](http://arxiv.org/abs/2401.10211)

    本文提出了一种通过多粒度结构和多尺度序列表示耦合的PTM位点预测方法PTM-CMGMS，该方法在结构表示学习和序列表示学习上进行优化，提高了PTM位点预测的准确性。

    

    蛋白质翻译后修饰（PTM）位点预测是生物信息学中的重要任务。已经开发了几种计算方法来预测PTM位点。然而，现有方法忽略了结构信息，仅利用蛋白质序列。此外，PTM是发生在原子粒度的生物事件，所以迫切需要设计一种更精细的结构表示学习方法。在本文中，我们提出了一种通过多粒度结构和多尺度序列表示耦合的PTM位点预测方法，简称PTM-CMGMS。具体而言，我们设计了多粒度结构感知表示学习方法，从AlphaFold预测的结构中学习氨基酸、原子和整个蛋白质的邻域结构表示，然后利用对比学习优化结构表示。此外，我们还使用了多尺度序列表示学习来提取上下文序列信息。

    Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context seq
    
[^4]: 通过精通指导的非参数聚类来扩大策略预测规模

    Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction. (arXiv:2401.10210v1 [cs.CY])

    [http://arxiv.org/abs/2401.10210](http://arxiv.org/abs/2401.10210)

    通过精通指导的非参数聚类方法，预测学生在问题解决中可能采用的策略，从而实现自适应教学系统的个性化体验。

    

    预测学生在解决问题时可能使用的策略（概念序列）有助于自适应教学系统（AISs）根据他们的学习能力更好地适应不同类型的学习者。这可以为学生提供更动态、有趣和个性化的学习体验。为了扩大训练一个可以覆盖大规模教育数据集的预测模型（如LSTMs），我们开发了一种非参数方法来对数据中的对称实例进行聚类。具体来说，我们利用基于Node2Vec的表示学习将掌握或技能水平上的对称性编码为策略，因为解决问题时，学生的策略很可能涉及他们已经掌握的概念。利用这种表示，我们使用DP-Means通过对聚类的粗细调整来对对称实例进行分组。我们将我们的模型应用到从MATHia（一家中学数学学习领先的AIS）的大规模数据集中学习数学学习策略。

    Predicting the strategy (sequence of concepts) that a student is likely to use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt themselves to different types of learners based on their learning abilities. This can lead to a more dynamic, engaging, and personalized experience for students. To scale up training a prediction model (such as LSTMs) over large-scale education datasets, we develop a non-parametric approach to cluster symmetric instances in the data. Specifically, we learn a representation based on Node2Vec that encodes symmetries over mastery or skill level since, to solve a problem, it is natural that a student's strategy is likely to involve concepts in which they have gained mastery. Using this representation, we use DP-Means to group symmetric instances through a coarse-to-fine refinement of the clusters. We apply our model to learn strategies for Math learning from large-scale datasets from MATHia, a leading AIS for middle-school math learning.
    
[^5]: 用于解释性深度神经网络入侵检测系统的折衷规则提取

    Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems. (arXiv:2401.10207v1 [cs.CR])

    [http://arxiv.org/abs/2401.10207](http://arxiv.org/abs/2401.10207)

    本文研究了用于解释性深度神经网络入侵检测系统的折衷规则提取，旨在解决黑盒解释器的不可信任问题。

    

    本文针对黑盒算法和代理解释器在可解释性入侵检测系统(X-IDS)中所引发的信任问题进行研究。虽然可解释的人工智能(XAI)旨在提高透明度，但黑盒代理解释器，如局部可解释的模型无关解释(LIME)和SHapley加法解释(SHAP)，很难信任。这些代理解释器的黑盒特性使得解释生成的过程不透明且难以理解。为了避免这个问题，可以使用透明的白盒算法，如规则提取(RE)。规则提取有三种类型的算法:教育、分解和折衷。教育方法提供快速但不可信赖的白盒解释，而分解规则提取提供了可信赖但可扩展性较差的解释。本研究探讨了折衷规则提取，它在可扩展性和可信赖性之间达到了平衡。通过综合不同的技术方法，

    This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust. The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand. To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE). There are three types of RE algorithms: pedagogical, decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability. This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness. By combining techniq
    
[^6]: Chem-FINESE: 通过文本重构验证细粒度少样本实体提取

    Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])

    [http://arxiv.org/abs/2401.10189](http://arxiv.org/abs/2401.10189)

    这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。

    

    在化学领域中，细粒度少样本实体提取面临两个独特的挑战。首先，与一般领域的实体提取任务相比，化学论文中的句子通常包含更多的实体。此外，实体提取模型通常难以提取长尾类型的实体。在本文中，我们提出了一种新颖的基于序列到序列的少样本实体提取方法Chem-FINESE来解决这两个挑战。我们的Chem-FINESE包含两个组件：一个序列到序列的实体提取器用于从输入句子中提取命名实体，以及一个序列到序列的自我验证模块用于从提取的实体中重构原始输入句子。受到一个好的实体提取系统需要忠实提取实体的事实启发，我们的新自我验证模块利用实体提取结果来重构原始输入句子。此外，我们设计了一种新的对比损失来减少在提取过程中的过度复制。

    Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
    
[^7]: 神经回波：深度卷积滤波器复制生物感受野

    Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields. (arXiv:2401.10178v1 [cs.CV])

    [http://arxiv.org/abs/2401.10178](http://arxiv.org/abs/2401.10178)

    深度卷积滤波器成功复制了生物感受野的结构复杂性，并且使用生物来源的权重进行初始化可以显著提高卷积网络的准确性。

    

    在这项研究中，我们提供了证据表明深度卷积核有效地复制了哺乳动物视网膜中观察到的生物感受野的结构复杂性。我们提供了来自各种最先进模型的经过训练卷积核的分析，证实了这一证据。受到这一有趣的发现的启发，我们提出了一种从生物感受野中汲取灵感的初始化方案。使用具有深度卷积特征的多个CNN结构对ImageNet数据集进行的实验分析显示，当使用生物来源的权重进行初始化时，学习模型的准确性显著提高。这揭示了生物灵感的计算模型进一步促进了我们对视觉处理系统的理解，并改善了卷积网络的有效性的潜力。

    In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.
    
[^8]: DISTINQT: 一种面向未来移动和无线网络的分布式隐私感知学习框架，用于QoS预测

    DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])

    [http://arxiv.org/abs/2401.10158](http://arxiv.org/abs/2401.10158)

    DISTINQT是一种面向未来移动和无线网络的隐私感知分布式学习框架，用于QoS预测。

    

    5G和6G以后的网络将支持依赖一定服务质量（QoS）的新的和具有挑战性的用例和应用程序。及时预测QoS对于安全关键应用（如车辆通信）尤为重要。尽管直到最近，QoS预测一直由集中式人工智能（AI）解决方案完成，但已经出现了一些隐私、计算和运营方面的问题。替代方案已经出现（如分割学习、联邦学习），将复杂度较低的AI任务分布在节点之间，同时保护数据隐私。然而，考虑到未来无线网络的异构性，当涉及可扩展的分布式学习方法时，会出现新的挑战。该研究提出了一种名为DISTINQT的面向QoS预测的隐私感知分布式学习框架。

    Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
    
[^9]: 在物体中心化学习中明确解开的表示

    Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])

    [http://arxiv.org/abs/2401.10148](http://arxiv.org/abs/2401.10148)

    这篇论文提出了一种在物体中心化学习中明确解开形状和纹理成分的方法，通过将潜在空间划分为两个不重叠的子集，使得模型更加稳定和有效。

    

    从原始视觉数据中提取结构化表示是机器学习中一个重要且长期存在的挑战。最近，无监督学习物体中心化表示的技术引起了越来越多的关注。在这个背景下，增强潜在特征的稳定性可以提高下游任务训练的效率和效果。在这个方向上一个有希望的步骤是解开导致数据变化的因素。先前，不变卡槽注意实现了从其他特征中解开位置、尺度和方向。扩展这一方法，我们着重于分离形状和纹理组成部分。特别地，我们提出了一种新颖的架构，将物体中心化模型中的形状和纹理成分偏置为潜在空间维度的两个不重叠子集。这些子集是先验已知的，因此在训练过程之前。在一系列物体中心化测试中进行的实验揭示了...

    Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal t
    
[^10]: 生物特征识别应用中的模型压缩技术综述

    Model Compression Techniques in Biometrics Applications: A Survey. (arXiv:2401.10139v1 [cs.CV])

    [http://arxiv.org/abs/2401.10139](http://arxiv.org/abs/2401.10139)

    本综述系统化地调查了生物特征识别应用中的模型压缩技术，包括量化、知识蒸馏和修剪，并分析了它们的优缺点，提出了改进方法的建议。

    

    深度学习算法的发展极大地提升了人类的任务自动化能力。然而，这些模型性能的巨大提升与其复杂性的增加密切相关，限制了它们在资源受限设备中的实用性，而这些设备通常用于人类导向的应用。因此，开发了压缩技术，可以大幅降低深度学习模型的计算和内存成本，而不会显著降低性能。本文旨在通过针对生物特征识别应用中的模型压缩技术进行全面综述，包括量化、知识蒸馏和修剪，系统化当前文献。我们对这些技术的相对价值进行了批判性分析，重点关注其优点和缺点，并提出了未来工作方向的建议，这些方向可能会改进当前的方法。

    The development of deep learning algorithms has extensively empowered humanity's task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additional
    
[^11]: 走向基于原则的图形变换器

    Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])

    [http://arxiv.org/abs/2401.10119](http://arxiv.org/abs/2401.10119)

    边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。

    

    基于k维Weisfeiler-Leman（k-WL）层次结构的图形学习架构提供了理论上很好理解的表达能力。然而，这样的架构在真实任务中往往无法提供可靠的预测性能，从而限制了它们的实际影响力。相比之下，基于全局注意力的模型如图形变换器在实践中表现出了强大的性能，但是将它们的表达能力与k-WL层次结构进行比较仍然具有挑战性，尤其是因为这些架构依赖于位置或结构编码来实现其表达能力和预测性能。为了解决这个问题，我们展示了最近提出的边缘变换器，这是一个在节点对而不是节点上进行操作的全局注意力模型，具有至少3-WL的表达能力。经验上，我们证明了边缘变换器在预测性能上超过了其他理论对齐的架构，同时不依赖于位置或结构编码。

    Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
    
[^12]: 用概率图模型进行逆向推理以分析社会生态系统

    Counterfactual Reasoning with Probabilistic Graphical Models for Analyzing Socioecological Systems. (arXiv:2401.10101v1 [cs.AI])

    [http://arxiv.org/abs/2401.10101](http://arxiv.org/abs/2401.10101)

    本文介绍了一种应用概率图模型进行逆向推理的方法，用于分析社会生态系统。实验数据有限的情况下，这种方法能够预测生态系统对假设干预的响应，并确定变量之间的影响。这一方法为多个领域的专家提供了直观易懂的工具。

    

    因果和逆向推理是数据科学中新兴的方向，可以让我们推断出假设情景。在实验数据通常不可用的领域，这尤其有用。在环境和生态科学领域，因果性使我们能够预测生态系统对假设干预的响应。结构性因果模型是一种用于因果性的概率图模型类别，由于其直观的特性，多个领域的专家可以轻松理解。然而，某些查询，称为不可辩识的查询，无法以精确的方式计算。本文提出应用一种新颖、最近的技术来界定社会生态系统领域内的不可辩识查询。我们的研究发现，传统的统计分析，包括概率图模型，可以确定变量之间的影响。然而，这些方法无法提供关于变量之间的因果关系的洞察。

    Causal and counterfactual reasoning are emerging directions in data science that allow us to reason about hypothetical scenarios. This is particularly useful in domains where experimental data are usually not available. In the context of environmental and ecological sciences, causality enables us, for example, to predict how an ecosystem would respond to hypothetical interventions. A structural causal model is a class of probabilistic graphical models for causality, which, due to its intuitive nature, can be easily understood by experts in multiple fields. However, certain queries, called unidentifiable, cannot be calculated in an exact and precise manner. This paper proposes applying a novel and recent technique for bounding unidentifiable queries within the domain of socioecological systems. Our findings indicate that traditional statistical analysis, including probabilistic graphical models, can identify the influence between variables. However, such methods do not offer insights in
    
[^13]: DiffusionGPT: 基于LLM的文本生成图像系统

    DiffusionGPT: LLM-Driven Text-to-Image Generation System. (arXiv:2401.10061v1 [cs.CV])

    [http://arxiv.org/abs/2401.10061](http://arxiv.org/abs/2401.10061)

    DiffusionGPT是一个基于LLM的统一文本生成图像系统，能够处理多样化的输入并整合领域专家模型。

    

    扩散模型为图像生成领域打开了新的道路，导致了在开源平台上共享高质量模型的广泛传播。然而，目前的文本生成图像系统存在一个主要挑战，即往往无法处理多样化的输入，或仅限于单一模型的结果。目前的统一尝试通常分为两个正交方面：i）在输入阶段解析多样的提示；ii）激活专家模型进行输出。为了兼顾两者的优点，我们提出了DiffusionGPT，它利用大型语言模型（LLM）提供了一个统一的生成系统，能够无缝地适应各种类型的提示并整合领域专家模型。DiffusionGPT基于先验知识为各种生成模型构建了领域特定的Thought树。当提供输入时，LLM解析提示并利用Thought树来指导选择适当的模型，从而放松输入约束并确保异常的效果。

    Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional 
    
[^14]: 大型语言模型在科学信息提取中的应用：一项针对病毒学的实证研究

    Large Language Models for Scientific Information Extraction: An Empirical Study for Virology. (arXiv:2401.10040v1 [cs.CL])

    [http://arxiv.org/abs/2401.10040](http://arxiv.org/abs/2401.10040)

    使用大型语言模型进行结构化的科学信息提取，在病毒学领域进行了实证研究，结果表明这种方法可以提供简洁的学术贡献摘要，对科学家进行导航和解决LLM的紧迫能力。

    

    本文倡导使用结构化和语义内容表示来进行基于学术交流的学术论文，受到维基百科信息框或结构化的亚马逊产品描述等工具的启发。这些表示形式提供了简洁的概述，帮助科学家在浓厚的学术环境中进行导航。我们的新颖自动化方法利用LLM的强大文本生成能力，产生结构化的学术贡献摘要，既提供了实际解决方案，也揭示了LLM紧迫的能力。对于LLM，主要关注的是改善其作为对话代理的通用智能。我们认为这些模型也可以在信息提取（IE）中有效应用，特别是在科学等领域的复杂IE任务中。这种范式转变用一系列指令代替了传统的模块化、流水线式的机器学习方法，简化了目标。我们的结果表明，通过微调的FLAN-T模型可以取得良好效果。

    In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs' emergent abilities.  For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T
    
[^15]: LOCALINTEL：从全球和本地网络知识生成组织威胁情报

    LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge. (arXiv:2401.10036v1 [cs.CR])

    [http://arxiv.org/abs/2401.10036](http://arxiv.org/abs/2401.10036)

    LOCALINTEL是一个自动化的知识上下文化系统，利用大型语言模型的能力，从全球和本地知识数据库中自动生成组织的威胁情报。

    

    安全操作中心（SoC）分析师从公开访问的全球威胁数据库中收集威胁报告，并手动自定义以适应特定组织的需求。这些分析师还依赖于内部存储库，作为组织的私有本地知识数据库。可信的网络情报、关键操作细节和相关组织信息都存储在这些本地知识数据库中。分析师利用这些全球和本地知识数据库从事一项繁重的任务，手动创建组织独特的威胁响应和缓解策略。最近，大型语言模型（LLMs）已经展示了高效处理大规模多样化知识源的能力。我们利用这种能力来处理全球和本地知识数据库，自动化生成组织特定的威胁情报。在这项工作中，我们提出了LOCALINTEL，这是一个新颖的自动化知识上下文化系统，可以从全球和本地知识数据库中生成组织的威胁情报。

    Security Operations Center (SoC) analysts gather threat reports from openly accessible global threat databases and customize them manually to suit a particular organization's needs. These analysts also depend on internal repositories, which act as private local knowledge database for an organization. Credible cyber intelligence, critical operational details, and relevant organizational information are all stored in these local knowledge databases. Analysts undertake a labor intensive task utilizing these global and local knowledge databases to manually create organization's unique threat response and mitigation strategies. Recently, Large Language Models (LLMs) have shown the capability to efficiently process large diverse knowledge sources. We leverage this ability to process global and local knowledge databases to automate the generation of organization-specific threat intelligence.  In this work, we present LOCALINTEL, a novel automated knowledge contextualization system that, upon 
    
[^16]: 大语言模型时代的进化计算：调查与路线图

    Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])

    [http://arxiv.org/abs/2401.10034](http://arxiv.org/abs/2401.10034)

    该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。

    

    大型语言模型（LLMs）是基于Transformer架构，在多样的数据上进行大规模预训练的，它们不仅在自然语言处理领域引起了革命，还将其能力扩展到了各个领域，迈向了人工通用智能的重要一步。尽管进化算法（EAs）与LLMs在目标和方法论上存在差异，但它们之间的相互作用揭示了有趣的相似之处，特别是在他们共同的优化性质、黑盒特性和处理复杂问题的能力方面。与此同时，进化算法不仅可以为LLM在黑盒设置下提供优化框架，还可以在应用中为LLM赋予灵活的全局搜索和迭代机制。另一方面，LLM丰富的领域知识使得进化算法可以进行更智能的搜索，而其文本处理能力则有助于将进化算法应用于各种任务。基于它们的互补优势，本文提出了一份调查和路线图。

    Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
    
[^17]: FreGrad: 轻量级和快速的频率感知扩散声码器

    FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder. (arXiv:2401.10032v1 [eess.AS])

    [http://arxiv.org/abs/2401.10032](http://arxiv.org/abs/2401.10032)

    本文介绍了一种轻量级和快速的频率感知扩散声码器FreGrad，通过离散小波变换、频率感知的扩张卷积和技巧等关键组件，实现了高质量音频的生成。在实验中，FreGrad相比基准模型训练速度快3.7倍，推断速度快2.2倍，模型大小减小0.6倍，而输出质量不受影响。

    

    本文的目标是使用一种轻量级和快速的基于扩散的声码器FreGrad生成逼真的音频。我们的框架包括以下三个关键组件：（1）我们使用离散小波变换将复杂的波形分解为子带小波，这有助于FreGrad在简单而简洁的特征空间上操作，（2）我们设计了一种频率感知的扩张卷积来提升频率感知能力，从而生成具有准确频率信息的语音，（3）我们引入了一些技巧来提高所提模型的生成质量。在实验中，FreGrad相比基准模型的训练速度快了3.7倍，推断速度快了2.2倍，同时模型的大小减小了0.6倍（仅1.78M参数），而不降低输出质量。音频样本可在以下链接中获得：https://mm.kaist.ac.kr/projects/FreGrad。

    The goal of this paper is to generate realistic audio with a lightweight and fast diffusion-based vocoder, named FreGrad. Our framework consists of the following three key components: (1) We employ discrete wavelet transform that decomposes a complicated waveform into sub-band wavelets, which helps FreGrad to operate on a simple and concise feature space, (2) We design a frequency-aware dilated convolution that elevates frequency awareness, resulting in generating speech with accurate frequency information, and (3) We introduce a bag of tricks that boosts the generation quality of the proposed model. In our experiments, FreGrad achieves 3.7 times faster training time and 2.2 times faster inference speed compared to our baseline while reducing the model size by 0.6 times (only 1.78M parameters) without sacrificing the output quality. Audio samples are available at: https://mm.kaist.ac.kr/projects/FreGrad.
    
[^18]: 自奖励语言模型

    Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])

    [http://arxiv.org/abs/2401.10020](http://arxiv.org/abs/2401.10020)

    该论文提出了自奖励语言模型的概念，通过LLM作为评判者，使用语言模型自己提供训练过程中的奖励。研究表明，该方法不仅可以提高指令遵循能力，还可以为自己提供高质量的奖励。通过对Llama 2 70B模型的三次迭代微调，结果在AlpacaEval 2.0排行榜上超过了其他现有系统。这项工作为实现能够不断自我改进的模型开辟了新的可能性。

    

    我们假设要实现超人级的智能体，未来的模型需要超人级的反馈，以提供足够的训练信号。目前的方法通常是从人类偏好中训练奖励模型，这可能会受到人类表现水平的限制，而且这些独立的冻结奖励模型在LLM训练过程中无法学习改进。在这项工作中，我们研究了自奖励语言模型，其中语言模型本身通过LLM作为评判者的提示在训练过程中提供自己的奖励。我们表明，在迭代DPO训练中，不仅指令遵循能力得到了提高，而且能够为自己提供高质量的奖励。通过对Llama 2 70B进行我们方法的三次迭代的微调，得到的模型在AlpacaEval 2.0排行榜上胜过许多现有系统，包括Claude 2、Gemini Pro和GPT-4 0613。虽然这只是一项初步研究，但这项工作为可能实现能够不断自我改进的模型打开了大门。

    We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continuall
    
[^19]: R-Judge: 评估LLM代理的安全风险意识的基准测试

    R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])

    [http://arxiv.org/abs/2401.10019](http://arxiv.org/abs/2401.10019)

    这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。

    

    大型语言模型（LLM）在自动完成各种真实世界应用任务方面展现出巨大潜力。然而，这些LLM代理在交互环境中操作时会引入意外的安全风险。与大多数之前的研究集中在LLM生成内容的安全性不同，本研究关注评估LLM代理在不同环境中的行为安全性的迫切需求。我们介绍了一个名为R-Judge的基准测试，用于评估LLM在给定代理交互记录时判断安全风险的能力。R-Judge包括162个代理交互记录，涵盖7个应用领域和10种风险类型的27个关键风险场景。它结合了人类对安全性的共识，并具有标记的安全风险标签和高质量的风险描述。利用R-Judge，我们对8种常用作代理骨干的著名LLM模型进行了全面评估。表现最好的模型GPT-4实现了72.29%的对比结果。

    Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
    
[^20]: 机器翻译中的性别偏见与大语言模型时代

    Gender Bias in Machine Translation and The Era of Large Language Models. (arXiv:2401.10016v1 [cs.CL])

    [http://arxiv.org/abs/2401.10016](http://arxiv.org/abs/2401.10016)

    本章研究了机器翻译中的性别偏见问题，介绍了传统神经机器翻译方法和生成预训练转换器模型中相关工作。通过在英语-意大利语的翻译环境中使用ChatGPT进行实验，评估了其解决性别偏见的能力。研究结果强调了减少机器翻译系统偏见的重要性。

    

    本章探讨了机器翻译在延续性别偏见方面的作用，着重强调了跨语言环境和统计依赖性所带来的挑战。提供了关于性别偏见在传统神经机器翻译方法和作为机器翻译系统的生成预训练转换器模型中的相关现有工作的全面概述。通过在英语-意大利语翻译环境中使用ChatGPT (基于GPT-3.5)进行实验，我们进一步评估了ChatGPT解决性别偏见的当前能力。研究结果强调了在机器翻译系统中减少偏见的持续需求，并强调了促进语言技术的公平性和包容性的重要性。

    This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.
    
[^21]: A-KIT:自适应Kalman-Informed Transformer

    A-KIT: Adaptive Kalman-Informed Transformer. (arXiv:2401.09987v1 [cs.RO])

    [http://arxiv.org/abs/2401.09987](http://arxiv.org/abs/2401.09987)

    这项研究提出了A-KIT，一种自适应的Kalman-informed transformer，用于在线学习传感器融合中变化的过程噪声协方差。它通过适应实际情况中的过程噪声变化，改进了估计状态的准确性，避免了滤波器发散的问题。

    

    扩展卡尔曼滤波器(EKF)是导航应用中广泛采用的传感器融合方法。EKF的一个关键方面是在线确定反映模型不确定性的过程噪声协方差矩阵。尽管常见的EKF实现假设过程噪声是恒定的，但在实际情况中，过程噪声是变化的，导致估计状态的不准确，并可能导致滤波器发散。为了应对这种情况，提出了基于模型的自适应EKF方法，并展示了性能改进，凸显了对稳健自适应方法的需求。在本文中，我们推导并引入了A-KIT，一种自适应的Kalman-informed transformer，用于在线学习变化的过程噪声协方差。A-KIT框架适用于任何类型的传感器融合。我们在这里介绍了基于惯性导航系统和多普勒速度日志的非线性传感器融合方法。通过使用来自自主无人潜水器的真实记录数据，我们验证了A-KIT的有效性。

    The extended Kalman filter (EKF) is a widely adopted method for sensor fusion in navigation applications. A crucial aspect of the EKF is the online determination of the process noise covariance matrix reflecting the model uncertainty. While common EKF implementation assumes a constant process noise, in real-world scenarios, the process noise varies, leading to inaccuracies in the estimated state and potentially causing the filter to diverge. To cope with such situations, model-based adaptive EKF methods were proposed and demonstrated performance improvements, highlighting the need for a robust adaptive approach. In this paper, we derive and introduce A-KIT, an adaptive Kalman-informed transformer to learn the varying process noise covariance online. The A-KIT framework is applicable to any type of sensor fusion. Here, we present our approach to nonlinear sensor fusion based on an inertial navigation system and Doppler velocity log. By employing real recorded data from an autonomous und
    
[^22]: FLex&Chill：通过Logit Chilling改进本地联合学习训练

    FLex&Chill: Improving Local Federated Learning Training with Logit Chilling. (arXiv:2401.09986v1 [cs.LG])

    [http://arxiv.org/abs/2401.09986](http://arxiv.org/abs/2401.09986)

    FLex&Chill 提出了一种通过Logit Chilling方法改进本地联合学习训练的方法，可以加快模型收敛并提高推理精度。

    

    联合学习由于本地客户端的非iid分布式训练数据而受到数据异质性的阻碍。我们提出了一种新的联合学习模型训练方法FLex&Chill，利用了Logit Chilling方法。通过广泛的评估，我们证明在联合学习系统中固有的非iid数据特征存在的情况下，这种方法可以加快模型收敛并提高推理精度。从我们的实验中，我们观察到全局联合学习模型收敛时间提高了6倍，推理精度提高了3.37%。

    Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.
    
[^23]: 用于寻找基础设施即代码部署配置的多目标优化分析

    Multiobjective Optimization Analysis for Finding Infrastructure-as-Code Deployment Configurations. (arXiv:2401.09983v1 [cs.NE])

    [http://arxiv.org/abs/2401.09983](http://arxiv.org/abs/2401.09983)

    该论文针对优化基础设施即代码部署配置的多目标问题进行了深入分析，并提出了适用于IOP的最佳多目标方法。

    

    多目标优化是人工智能和运筹学领域的热门话题。多目标方法的设计和开发是研究人员和实践者经常需要处理的任务。由于活跃的研究活动，已经提出了许多技术，对来自各种实际领域的情况具有显著的有效性。本文关注于优化基础设施即代码部署配置的多目标问题。解决此问题的系统被称为“IaC优化平台（IOP）”。尽管在文献中已经介绍了IOP的原型版本，但需要进行更深入的分析，以确定最适合嵌入IOP的多目标方法。本文中进行的分析的主要动机是确定最合适的多目标方法。

    Multiobjective optimization is a hot topic in the artificial intelligence and operations research communities. The design and development of multiobjective methods is a frequent task for researchers and practitioners. As a result of this vibrant activity, a myriad of techniques have been proposed in the literature to date, demonstrating a significant effectiveness for dealing with situations coming from a wide range of real-world areas. This paper is focused on a multiobjective problem related to optimizing Infrastructure-as-Code deployment configurations. The system implemented for solving this problem has been coined as IaC Optimizer Platform (IOP). Despite the fact that a prototypical version of the IOP has been introduced in the literature before, a deeper analysis focused on the resolution of the problem is needed, in order to determine which is the most appropriate multiobjective method for embedding in the IOP. The main motivation behind the analysis conducted in this work is to
    
[^24]: 朝向生成式抽象推理：通过规则抽象和选择来完成Raven的渐进矩阵

    Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection. (arXiv:2401.09966v1 [cs.AI])

    [http://arxiv.org/abs/2401.09966](http://arxiv.org/abs/2401.09966)

    本文提出了一个条件生成模型（RAISE），通过在潜在空间中进行规则抽象和选择，以解决Raven的渐进矩阵问题，该模型能够在现实的场景中展示出抽象推理的能力。

    

    在人工智能领域，赋予机器抽象推理能力是一个长期的研究课题。Raven的渐进矩阵（RPM）被广泛用于探索机器智能中的抽象视觉推理，模型需要理解潜在的规则并从候选集中选择缺失的右下图像来完成图像矩阵。参与者可以通过推断潜在的属性变化规则和想象任意位置的缺失图像展示强大的推理能力。然而，现有的解决方案很难在现实的RPM问题中展示出这种能力。在本文中，我们提出了一个条件生成模型，通过规则抽象和选择（RAISE）在潜在空间中解决答案生成问题。RAISE将图像属性编码为潜在概念，并通过概念将潜在规则分解成原子规则，这些原子规则被抽象为全局可学习的参数。在生成答案时，RAISE选择...

    Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models need to understand the underlying rules and select the missing bottom-right images out of candidate sets to complete image matrices. The participators can display powerful reasoning ability by inferring the underlying attribute-changing rules and imagining the missing images at arbitrary positions. However, existing solvers can hardly manifest such an ability in realistic RPM problems. In this paper, we propose a conditional generative model to solve answer generation problems through Rule AbstractIon and SElection (RAISE) in the latent space. RAISE encodes image attributes as latent concepts and decomposes underlying rules into atomic rules by means of concepts, which are abstracted as global learnable parameters. When generating the answer, RAISE select
    
[^25]: 当神经代码补全模型调整情况时：通过动态模型推断实现更便宜更快的补全

    When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference. (arXiv:2401.09964v1 [cs.SE])

    [http://arxiv.org/abs/2401.09964](http://arxiv.org/abs/2401.09964)

    本研究探索了代码补全领域中动态推断的应用，在对GPT-2进行实证研究的基础上发现，仅使用第一层即可准确生成超过一半的标记，从而节省了大量计算资源。使用所有层仍然无法完全正确预测一部分标记。

    

    利用大型语言模型的最新进展，现代神经代码补全模型展示了生成高度准确代码建议的能力。然而，它们巨大的规模在计算成本和环境影响方面带来了挑战，阻碍了它们在实际场景中的广泛应用。动态推断成为一种有前景的解决方案，它在推断过程中分配最少的计算资源同时保持模型的性能。在这项研究中，我们在代码补全的背景下探索了动态推断。最初，我们对GPT-2进行了实证研究，重点关注中间层在代码补全中的推断能力。我们发现，仅使用第一层就可以准确生成54.4%的标记，表明存在显著的计算节省潜力。此外，即使使用了所有层，模型仍然无法正确预测14.5%的标记，并且从后续补全中也没有得到正确的预测。

    Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5% of tokens correctly, and the subsequent completions continued from the
    
[^26]: WindSeer: 在小型无人机上实时预测复杂地形上的体积风

    WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV. (arXiv:2401.09944v1 [cs.LG])

    [http://arxiv.org/abs/2401.09944](http://arxiv.org/abs/2401.09944)

    WindSeer是一个名为WindSeer的神经网络，能够在实时预测低空风的同时节省计算资源。通过使用稀疏的测量数据和合成数据进行训练，它可以成功地预测已知地形上的真实风场，并在不同的分辨率和域大小上生成准确的预测结果，无需重新训练。

    

    实时高分辨率的风预测对于包括有人和无人航空在内的各种应用都很有益。当前的天气模型需要太多的计算，并且缺乏必要的预测能力，因为它们仅在多千米和几小时的尺度上有效 - 这远低于这些应用所需的空间和时间分辨率。我们的工作首次展示了在有限计算设备上实时预测低空风的能力，仅使用稀疏的测量数据训练了名为WindSeer的神经网络。我们使用计算流体力学模拟的合成数据进行训练，并展示它可以成功地从仅有少量的噪声和空间聚集的风测量数据中预测出已知地形上的真实风场。WindSeer可以在之前未见过的地形上以不同的分辨率和域大小生成准确的预测结果，无需重新训练。我们证明了该模型成功预测了历史风场。

    Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require. Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data. We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical w
    
[^27]: 多任务学习用于联合识别、团队归属和角色分类的运动视觉跟踪

    Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking. (arXiv:2401.09942v1 [cs.CV])

    [http://arxiv.org/abs/2401.09942](http://arxiv.org/abs/2401.09942)

    本文提出了一种多任务学习的方法，用于联合识别、团队归属和角色分类的运动视觉跟踪。通过使用共享的主干网络，该方法在计算上更高效，并且能够产生更丰富和有区别度的表示。

    

    对于分析足球视频来说，有效的球员跟踪和重新识别是至关重要的。然而，由于球员的非线性运动、来自同一团队的球员外观的相似性以及频繁的遮挡，这是一项具有挑战性的任务。因此，提取有意义的嵌入表示以代表球员对于开发有效的跟踪和重新识别系统至关重要。本文提出了一种名为PRTreID的多用途基于部件的人物表示方法，它同时执行角色分类、团队归属和重新识别这三个任务。与现有文献不同的是，使用多任务监督训练了一个单一网络来解决所有三个任务。由于共享的主干网络，所提出的联合方法具有较低的计算复杂度。此外，多任务学习导致了更丰富和更有区别度的表示，这通过定量和定性结果都得到了证实。为了展示

    Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiven
    
[^28]: 基于XAI的语义分割模型用于视觉质量检测的增强

    XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection. (arXiv:2401.09900v1 [cs.CV])

    [http://arxiv.org/abs/2401.09900](http://arxiv.org/abs/2401.09900)

    本文提出了一个基于XAI的框架，通过使用CAM-based解释来改进语义分割模型，从而增强视觉质量检测系统。评估结果显示，XAI增强的模型在复杂对象分割方面表现出色。

    

    视觉质量检测系统在制造业和物流等领域至关重要，采用计算机视觉和机器学习进行精确、快速的缺陷检测。然而，这些系统的无解释性可能阻碍了信任、错误识别和系统改进。本文提出了一个框架，通过使用基于CAM的解释来改进语义分割模型，从而增强视觉质量检测。我们的方法包括：1）模型训练，2）基于XAI的模型解释，3）XAI评估，以及4）注释增强用于模型改进，这些都受到解释和专家见解的指导。评估结果显示，XAI增强的模型特别在复杂对象分割方面超过了原始的DeepLabv3-ResNet101模型。

    Visual quality inspection systems, crucial in sectors like manufacturing and logistics, employ computer vision and machine learning for precise, rapid defect detection. However, their unexplained nature can hinder trust, error identification, and system improvement. This paper presents a framework to bolster visual quality inspection by using CAM-based explanations to refine semantic segmentation models. Our approach consists of 1) Model Training, 2) XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation for Model Enhancement, informed by explanations and expert insights. Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101 models, especially in intricate object segmentation.
    
[^29]: 基于弹性联邦和多智能体深度强化学习的下一代网络合作边缘缓存

    Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network. (arXiv:2401.09886v1 [cs.LG])

    [http://arxiv.org/abs/2401.09886](http://arxiv.org/abs/2401.09886)

    本论文提出了一种基于弹性联邦和多智能体深度强化学习的合作边缘缓存方案，通过训练个性化的本地模型，预测准确受欢迎的内容，并在不同的SBS之间合作缓存热门内容，以达到优化获取内容成本的目标。

    

    边缘缓存是下一代网络中的一种有前途的解决方案，通过赋予小型基站（SBS）中的缓存单元赋能，允许用户设备（UE）获取已在SBS中预缓存的用户请求内容。对于SBS来说，通过学习准确预测受欢迎的内容非常关键，同时保护用户个人信息。传统的联邦学习（FL）可以保护用户的隐私，但是UE之间的数据差异可能导致模型质量下降。因此，有必要为每个UE训练个性化的本地模型以准确预测受欢迎的内容。此外，下一代网络中相邻SBS之间可以共享缓存的内容，因此在不同的SBS中缓存预测到的热门内容可能会影响获取内容的成本。因此，确定合作缓存热门内容的位置至关重要。为了解决这些问题，我们提出了一种基于弹性联邦和多智能体深度强化学习的合作边缘缓存方案。

    Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and mu
    
[^30]: 基于注意力的循环神经网络对自动行为下蛋鸡识别的研究

    Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition. (arXiv:2401.09880v1 [cs.SD])

    [http://arxiv.org/abs/2401.09880](http://arxiv.org/abs/2401.09880)

    本研究提出了一种基于注意力的循环神经网络用于自动识别下蛋鸡行为。通过声音分析和特征提取，构建了一个鲁棒的行为特征化系统，对下蛋鸡的健康行为进行监测和识别。实验结果表明该模型具有良好的综合性能。

    

    现代养禽业的一个关注点是下蛋鸡的鸣叫声，其中包含了关于健康行为的非常有用的信息。这些信息被用作健康和福祉的指标，帮助养殖人员更好地监测下蛋鸡，从而及早发现问题，以便进行更快和更有效的干预。本研究专注于对下蛋鸡鸣叫类型的声音分析，以提出一种鲁棒的行为特征化系统，以便更好地监测下蛋鸡。为此，我们首先收集并注释了下蛋鸡的鸣叫信号，然后设计了一个基于时间和频率域特征组合的最佳声学特征化方法。然后我们使用这些特征构建了基于循环神经网络的多标签分类模型，将语义类别分配给描述下蛋鸡行为的鸣叫声。结果表明，我们的模型在综合性能上表现出色。

    One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combinati
    
[^31]: 调和空间和时间抽象化以实现目标表示

    Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])

    [http://arxiv.org/abs/2401.09870](http://arxiv.org/abs/2401.09870)

    本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。

    

    目标表示通过将复杂的学习问题分解为更容易的子任务来影响分层强化学习算法的性能。最近的研究表明，保留时间抽象环境动态的表示方法在解决困难问题和提供优化理论保证方面是成功的。然而，这些方法在环境动态越来越复杂（即时间抽象转换关系依赖更多变量）的任务中无法扩展。另一方面，其他方法则尝试使用空间抽象来缓解前面的问题。它们的限制包括无法适应高维环境和对先前知识的依赖。本文提出了一种新的三层分层强化学习算法，分层结构的不同层次引入了空间和时间目标抽象化。我们对学习策略的遗憾边界进行了理论研究。我们评估了我们提出的算法在不同任务上的性能。

    Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
    
[^32]: 提高图像-文本预训练中细粒度理解的方法

    Improving fine-grained understanding in image-text pre-training. (arXiv:2401.09865v1 [cs.CV])

    [http://arxiv.org/abs/2401.09865](http://arxiv.org/abs/2401.09865)

    本研究引入了一种名为SPARC的方法，通过在图像-文本对中学习每个令牌的图像组合，以提高图像-文本预训练中的细粒度理解能力。SPARC方法结合了细粒度损失和对比损失，可以以较低的计算成本学习同时编码全局和局部信息的表示。

    

    我们引入了一种名为SPARC的方法，用于从图像-文本对中预训练更细粒度的多模态表示。考虑到多个图像块通常对应于单个单词，我们提出为每个字幕令牌学习一组图像块的方法。为了实现这一点，我们使用了图像块和语言令牌之间的稀疏相似度度量，并计算出每个令牌的语言分组的视觉嵌入，作为图像块的加权平均值。然后，通过一种仅依赖于单个样本而不需要其他批次样本作为负样本的细粒度序列损失，对令牌和语言分组的视觉嵌入进行对比。这使得可以以较低的计算成本学习更详细的信息。SPARC将这种细粒度损失与全局图像和文本嵌入之间的对比损失相结合，以同时编码全局和局部信息的表示。

    We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thorough
    
[^33]: 大型语言模型提示的进化多目标优化以平衡情感

    Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments. (arXiv:2401.09862v1 [cs.NE])

    [http://arxiv.org/abs/2401.09862](http://arxiv.org/abs/2401.09862)

    本研究提出了一种针对语言模型提示优化的进化多目标方法，通过情感分析为案例研究，实现了生成能够同时体现两种相互冲突情感的提示语，从而提高模型的性能和相关信息的提取能力。

    

    大型语言模型（LLMs）如ChatGPT的出现引起了各个领域的广泛关注，因为它们的性能和多功能性非凡。随着这些模型的使用不断增长，有效的提示工程变得越来越重要。提示优化成为一个关键挑战，因为它直接影响模型性能和相关信息的提取。最近，进化算法（EAs）在解决这个问题方面显示出了希望，为新的优化策略铺平了道路。在这项工作中，我们提出了一种特别针对提示优化的进化多目标（EMO）方法，称为EMO-Prompts，以情感分析作为案例研究。我们将情感分析能力作为我们的实验目标。我们的结果表明，EMO-Prompts能够有效地生成提示，使LLM能够同时产生体现两种相互冲突情感的文本。

    The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.
    
[^34]: 时间洞察增强：减轻多模态大型语言模型中的时间幻觉

    Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models. (arXiv:2401.09861v1 [cs.CV])

    [http://arxiv.org/abs/2401.09861](http://arxiv.org/abs/2401.09861)

    本研究提出了一种创新的方法，通过从视频内容中提取时间特定的信息，来解决多模态大型语言模型中的事件级幻觉问题。

    

    近年来，多模态大型语言模型(MLLMs)的发展显著增强了对多媒体内容的理解能力，将文本、图像和视频等多种模态集合在一起。然而，这些模型面临的一个关键挑战，尤其是在处理视频输入时，是产生幻觉 - 错误的感知或解释，特别是在事件层面上。本研究引入了一种创新的方法，以解决MLLMs中的事件级幻觉问题，重点关注视频内容的时间理解。我们的方法利用了一种新颖的框架，从事件查询和提供的视频中提取并利用事件特定信息来优化MLLMs的响应。我们提出了一种独特的机制，将按需的事件查询分解为代表性的行为。随后，我们使用类似CLIP和BLIP2的模型来预测事件发生的具体时间戳。我们使用Charades-STA数据集进行了评估，结果表明...

    Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrate
    
[^35]: 使用可解释的人工智能增强边缘摄像头的公平性和性能

    Enhancing the Fairness and Performance of Edge Cameras with Explainable AI. (arXiv:2401.09852v1 [cs.CV])

    [http://arxiv.org/abs/2401.09852](http://arxiv.org/abs/2401.09852)

    本研究提出了一种使用可解释的人工智能进行边缘摄像头模型调试的方法，通过解决训练数据集的偏见问题来提高公平性和性能。

    

    在边缘摄像头系统中，人工智能在人体检测方面的应用日益增多，导致了准确但复杂的模型，这使得解释和调试变得具有挑战性。我们的研究提出了一种使用可解释的人工智能（XAI）进行模型调试的诊断方法，通过专家驱动的问题识别和解决方案创建。在实际办公室边缘网络中的Bytetrack模型上进行验证，我们发现训练数据集是主要的偏见来源，并建议模型增强作为解决方案。我们的方法有助于识别模型偏见，从而实现公平和可信赖的模型。

    The rising use of Artificial Intelligence (AI) in human detection on Edge camera systems has led to accurate but complex models, challenging to interpret and debug. Our research presents a diagnostic method using Explainable AI (XAI) for model debugging, with expert-driven problem identification and solution creation. Validated on the Bytetrack model in a real-world office Edge network, we found the training dataset as the main bias source and suggested model augmentation as a solution. Our approach helps identify model biases, essential for achieving fair and trustworthy models.
    
[^36]: 仿真行为：探索科学的可能下一范式

    Behavioral Simulation: Exploring A Possible Next Paradigm for Science. (arXiv:2401.09851v1 [cs.AI])

    [http://arxiv.org/abs/2401.09851](http://arxiv.org/abs/2401.09851)

    本文研究了仿真技术的发展与科学范式的演变，并提出了行为仿真的概念，代表了更高程度的范式整合。

    

    仿真技术已广泛应用于许多科学研究领域，如天气预报、流体力学和生物种群。它是处理复杂系统问题的最佳工具，在表示空间中无法使用闭合形式表达式且目标分布过于复杂而无法完全由深度学习模型表示。我们认为，仿真技术的发展与科学范式是一致的。本文从数据、算法和计算能力的角度归纳了科学范式的演变。在此基础上，我们将仿真技术分为三个阶段，与新范式的出现相适应，并发现先进的仿真技术是范式整合的典型实例。此外，我们提出了行为仿真（BS）的概念，特别是复杂行为仿真（SBS），代表了更高程度的范式整合。

    Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms 
    
[^37]: 切片网络

    Slicer Networks. (arXiv:2401.09833v1 [eess.IV])

    [http://arxiv.org/abs/2401.09833](http://arxiv.org/abs/2401.09833)

    本论文提出了一种切片网络的新型架构，通过使用低频逼近的方法进行特征提取和上采样，从而在医学图像分析中提高了性能。

    

    在医学成像中，扫描通常会揭示出具有不同对比度但一致的内部强度或纹理的物体。这种特点使得可以利用低频逼近来进行分割和变形场估计等任务。然而，在医学图像分析的神经网络架构中融入这个概念仍然不够深入。在这篇论文中，我们提出了切片网络，一种设计用于利用这些特性的新型架构。切片网络包括一个编码器，利用视觉变换器等模型进行特征提取，和一个使用可学习双边网格的切片器，通过涂抹-模糊-切片的过程策略性地改进和上采样特征图。这引入了一个保留边缘的低频逼近，有效扩大了有效感受野。这种改进不仅减少了计算复杂性，还提高了整体性能。通过在不同医学成像应用上进行实验进行了验证。

    In medical imaging, scans often reveal objects with varied contrasts but consistent internal intensities or textures. This characteristic enables the use of low-frequency approximations for tasks such as segmentation and deformation field estimation. Yet, integrating this concept into neural network architectures for medical image analysis remains underexplored. In this paper, we propose the Slicer Network, a novel architecture designed to leverage these traits. Comprising an encoder utilizing models like vision transformers for feature extraction and a slicer employing a learnable bilateral grid, the Slicer Network strategically refines and upsamples feature maps via a splatting-blurring-slicing process. This introduces an edge-preserving low-frequency approximation for the network outcome, effectively enlarging the effective receptive field. The enhancement not only reduces computational complexity but also boosts overall performance. Experiments across different medical imaging appl
    
[^38]: PPNet: 一种用于端到端近似最优路径规划的新颖神经网络结构

    PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning. (arXiv:2401.09819v1 [cs.RO])

    [http://arxiv.org/abs/2401.09819](http://arxiv.org/abs/2401.09819)

    PPNet是一种新颖的神经网络结构，用于解决端到端近似最优路径规划问题。通过将路径规划问题分为两个子问题，并使用两级级联神经网络进行求解，同时引入了一种高效的数据生成方法EDaGe-PP。实验结果表明，PPNet在计算时间和成功率方面比其他方法有显著提升。

    

    传统的路径规划器，如基于采样的路径规划器，在初始解敏感性和收敛到最优解速度上具有局限性。然而，在许多应用中，如具有有限功率/燃料的自动驾驶车辆中，在短时间内找到近似最优解是具有挑战性的。为了实现端到端近似最优路径规划器，我们首先将路径规划问题分为两个子问题，即路径空间分段和给定路径空间中的航点生成。我们进一步提出了一个名为路径规划网络（PPNet）的两级级联神经网络来解决路径规划问题，通过解决上述子问题。此外，我们提出了一种名为EDaGe-PP的用于路径规划的高效数据生成方法。结果显示，PPNet训练集由EDaGe-PP生成的成功率相比其他方法提升了$2\times$，总计算时间少于1/33。我们验证了PPNet的性能。

    The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNe
    
[^39]: 一种简单的黑盒方法用于越狱攻击

    All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])

    [http://arxiv.org/abs/2401.09798](http://arxiv.org/abs/2401.09798)

    本研究提出了一种简单的黑盒方法，用于生成越狱攻击提示，克服了现有方法的复杂性和计算成本的限制。该方法通过使用语言模型自身，将有害提示重写为非有害表达，实现了超过80%的攻击成功率，并且即使模型更新，效果仍然有效。

    

    像ChatGPT这样的大型语言模型面临着“越狱”挑战，即规避保障措施以产生不符合伦理的提示。本研究引入了一种简单的黑盒方法，有效地生成越狱提示，克服了现有方法的高复杂性和计算成本的限制。该方法通过使用目标语言模型自身，迭代地将有害提示重写为非有害表达，基于假设认为语言模型可以直接生成规避保障的表达。通过在ChatGPT（GPT-3.5和GPT-4）和Gemini-Pro上进行实验证明，该方法在平均5次迭代内实现了超过80%的攻击成功率，并且即使模型更新，效果仍然有效。生成的越狱提示自然而简练，表明它们较不易被检测。结果表明，创建有效的越狱提示比先前研究认为的要简单，并且黑盒越狱攻击构成了一个重要的挑战。

    Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
    
[^40]: 基于元启发式算法的Vision Transformer模型在早期检测阿尔茨海默病上的比较分析

    A Comparative Analysis on Metaheuristic Algorithms Based Vision Transformer Model for Early Detection of Alzheimer's Disease. (arXiv:2401.09795v1 [cs.NE])

    [http://arxiv.org/abs/2401.09795](http://arxiv.org/abs/2401.09795)

    本文提出了一种基于元启发式算法的Vision Transformer模型，用于早期检测阿尔茨海默病。通过大量的测试数据验证，该模型在准确度、精确度、召回率以及F1分数等方面展现了卓越的性能。

    

    一些致命的神经退行性疾病严重影响了老一代人的生活质量。痴呆是其中一种症状，如果不在早期阶段检测出来，可能会发展成严重的阿尔茨海默病。已经有报道称，从正常阶段到这种疾病的发展是由人脑内几个参数的改变导致的。本文提出了一种基于创新的元启发式算法的ViT模型，用于识别不同阶段的痴呆症。大量的测试数据被用来验证所提出的方案。还表明我们的模型在准确度、精确度、召回率以及F1分数等方面表现出优越的性能。

    A number of life threatening neuro-degenerative disorders had degraded the quality of life for the older generation in particular. Dementia is one such symptom which may lead to a severe condition called Alzheimer's disease if not detected at an early stage. It has been reported that the progression of such disease from a normal stage is due to the change in several parameters inside the human brain. In this paper, an innovative metaheuristic algorithms based ViT model has been proposed for the identification of dementia at different stage. A sizeable number of test data have been utilized for the validation of the proposed scheme. It has also been demonstrated that our model exhibits superior performance in terms of accuracy, precision, recall as well as F1-score.
    
[^41]: 产业4.0中大数据探索的语义方法

    A Semantic Approach for Big Data Exploration in Industry 4.0. (arXiv:2401.09789v1 [cs.AI])

    [http://arxiv.org/abs/2401.09789](http://arxiv.org/abs/2401.09789)

    本文介绍了一个产业4.0场景下基于语义的可视化查询系统的提案，该系统允许领域专家以友好的方式探索和可视化数据。系统的主要创新在于结合使用语义注释的捕获数据和2D定制数字机器的数字表示形式。

    

    自动化、物联网、大数据和云计算技术的不断发展导致了第四次工业革命（产业4.0），通过可视化和识别模式和洞见，可以更好地理解数据并改进制造过程。然而，对于制造业专家来说，数据探索的任务常常很困难，因为他们可能对未出现在预设计可视化中的数据也感兴趣，因此他们需要信息技术专家的帮助。本文提出了一个在真实产业4.0场景中开发的基于语义的可视化查询系统的提案，该系统允许领域专家以友好的方式探索和可视化数据。该系统的主要创新之处在于它首先对捕获的数据进行语义注释，并与一个2D定制数字机器的数字表示形式进行关联使用。

    The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked
    
[^42]: 查询易于翻转样本的深度主动学习

    Querying Easily Flip-flopped Samples for Deep Active Learning. (arXiv:2401.09787v1 [cs.LG])

    [http://arxiv.org/abs/2401.09787](http://arxiv.org/abs/2401.09787)

    本文提出了一种基于模型的预测不确定性度量，即最小不一致度量（LDM），用于解决复杂决策边界情况下的主动学习问题。通过查询具有最小LDM的未标记数据，可以提高深度学习模型的性能。

    

    主动学习是一种机器学习范式，旨在通过选择和查询未标记数据来提高模型的性能。一种有效的选择策略是基于模型的预测不确定性，这可以解释为样本的信息量度量。样本到决策边界的距离是一种自然的预测不确定性度量，但通常难以计算，特别是对于多类分类任务中形成的复杂决策边界。为了解决这个问题，本文提出了“最小不一致度量”（LDM），定义为预测标签不一致的最小概率，并且证明了LDM的估计器在温和假设下是渐近一致的。该估计器计算效率高，并且可以通过参数扰动轻松实现在深度学习模型中使用。基于LDM的主动学习通过查询具有最小LDM的未标记数据来执行。

    Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Exper
    
[^43]: 自适应自训练框架用于细粒度场景图生成

    Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])

    [http://arxiv.org/abs/2401.09786](http://arxiv.org/abs/2401.09786)

    本论文提出了一种自适应自训练框架用于细粒度场景图生成，通过利用未标注的三元组缓解了场景图生成中的长尾问题。同时，引入了一种新颖的伪标签技术CATM和图结构学习器GSL来提高模型性能。

    

    场景图生成（SGG）模型在基准数据集中存在长尾谓词分布和缺失注释问题。本研究旨在通过利用未标注的三元组缓解SGG的长尾问题。为此，我们引入了一种称为自训练SGG（ST-SGG）的框架，该框架基于未标注的三元组为其分配伪标签以训练SGG模型。虽然在图像识别方面的自训练取得了显著进展，但设计适用于SGG任务的自训练框架更具挑战，因为其固有特性，如语义歧义和长尾分布的谓词类别。因此，我们提出了一种新颖的SGG伪标签技术，称为具有动量的类别自适应阈值化（CATM），它是一种独立于模型的框架，可应用于任何已有的SGG模型。此外，我们设计了一个图结构学习器（GSL），从中获益。

    Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
    
[^44]: SEINE:核实例分割的结构编码与交互网络

    SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation. (arXiv:2401.09773v1 [cs.CV])

    [http://arxiv.org/abs/2401.09773](http://arxiv.org/abs/2401.09773)

    SEINE是一种用于核实例分割的结构编码和交互网络，通过考虑核结构的相关性和利用核之间的结构相似性来提高每个分割实例的完整性。

    

    组织病理学图像中的核实例分割对于生物分析和癌症诊断至关重要，但由于两个原因而具有挑战性。首先，嗜染核内和核外区域的视觉呈现相似，常常导致欠分割。其次，当前方法缺乏对核结构的探索，导致分段实例的碎片化预测。为了解决这些问题，本文提出了一种称为SEINE的结构编码和交互网络，该网络开发了核的结构建模方案，并利用核之间的结构相似性来提高每个分割实例的完整性。具体而言，SEINE引入了基于轮廓的结构编码（SE），以考虑核结构和语义之间的相关性，实现对核结构的合理表示。基于编码，我们提出了结构引导的注意力（SGA），它以清晰的核作为原型，以增强注意力以实现更好的分割结果。

    Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance
    
[^45]: 走向异质图学习：进展与未来

    Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])

    [http://arxiv.org/abs/2401.09769](http://arxiv.org/abs/2401.09769)

    本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。

    

    图是用来模拟现实世界实体之间复杂关系的结构化数据。最近，异质图，其中连接的节点往往具有不同的标签或不同的特征，引起了广泛关注并找到了许多应用。与此同时，人们也在不断努力推进从异质图中学习。虽然有关该主题的调查存在，但它们只关注于异质图神经网络（GNNs），而忽略了异质图学习的其他子主题。在本调查中，我们全面回顾了关于从带有异质性的图中学习的现有研究。首先，我们收集了180多篇论文，介绍了该领域的发展。然后，我们根据层次分类法对现有方法进行了系统分类，包括学习策略、模型架构和实际应用。最后，我们讨论了现有研究的主要挑战，并突出了未来研究的潜在方向。

    Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
    
[^46]: 基于前 k 近邻的图像到文本提示的 CLIP 模型

    CLIP Model for Images to Textual Prompts Based on Top-k Neighbors. (arXiv:2401.09763v1 [cs.CV])

    [http://arxiv.org/abs/2401.09763](http://arxiv.org/abs/2401.09763)

    提出了一种基于前k近邻的CLIP模型，实现了成本效益高的图像到文本提示生成。该方法无需大量标注数据，通过将生成模型和K近邻算法结合使用，并通过在线和离线任务方式实现。相较于其他模型，该方法在指标上取得了更高的表现。

    

    文本到图像合成是多模态生成的一个子领域，在近年来引起了很大关注。我们提出了一种成本效益高的方法，用于生成图像到提示的文本，利用生成模型生成文本提示，无需大量的标注数据。我们将方法分为在线阶段和离线阶段。我们使用了 CLIP 模型和 K 近邻算法的组合。所提出的系统由两个主要部分组成：离线任务和在线任务。我们的方法在这些模型中拥有最高的指标为 0.612，比 Clip、Clip + KNN（前 10 名）分别高出 0.013、0.055、0.011。

    Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.
    
[^47]: 在超越5G网络中，基于合作的三点模型的地空覆盖扩展

    Cooperative Tri-Point Model-Based Ground-to-Air Coverage Extension in Beyond 5G Networks. (arXiv:2401.09757v1 [cs.IT])

    [http://arxiv.org/abs/2401.09757](http://arxiv.org/abs/2401.09757)

    本论文提出了一种基于合作的三点模型，利用合作波束增强地-空覆盖扩展。通过分析G2A覆盖扩展，证明了三个TBSs之间的合作能够实现最小化覆盖重叠的G2A覆盖，并根据Delaunay三角剖分设计了合作覆盖结构。

    

    利用现有的地面基础设施为空中用户提供覆盖是一种潜在的低成本解决方案。然而，已经部署的地面基站（TBSs）由于向下倾斜的天线导致弱的地空（G2A）覆盖。此外，由于三维空间中复杂的信号覆盖需求，特别是在垂直方向上，实现整个空域的最佳覆盖仍然具有挑战性。在本文中，我们提出了一种基于合作三点（CoTP）模型的方法，利用合作波束增强G2A覆盖扩展。为了利用现有的TBSs建立有效的合作，我们证明三个TBSs之间的合作可以确保具有最小覆盖重叠的G2A覆盖，并设计了CoTP模型来分析G2A覆盖扩展。利用该模型，基于德劳内三角剖分的合作覆盖结构被设计为划分三棱柱形状的土地空间。

    The utilization of existing terrestrial infrastructures to provide coverage for aerial users is a potentially low-cost solution. However, the already deployed terrestrial base stations (TBSs) result in weak ground-to-air (G2A) coverage due to the down-tilted antennas. Furthermore, achieving optimal coverage across the entire airspace through antenna adjustment is challenging due to the complex signal coverage requirements in three-dimensional space, especially in the vertical direction. In this paper, we propose a cooperative tri-point (CoTP) model-based method that utilizes cooperative beams to enhance the G2A coverage extension. To utilize existing TBSs for establishing effective cooperation, we prove that the cooperation among three TBSs can ensure G2A coverage with a minimum coverage overlap, and design the CoTP model to analyze the G2A coverage extension. Using the model, a cooperative coverage structure based on Delaunay triangulation is designed to divide triangular prism-shaped
    
[^48]: 使用Shapley值解释漂移

    Explaining Drift using Shapley Values. (arXiv:2401.09756v1 [cs.LG])

    [http://arxiv.org/abs/2401.09756](http://arxiv.org/abs/2401.09756)

    本文提出了一个新的框架-DBShap，使用Shapley值来确定模型性能漂移的主要贡献者并量化他们的贡献。通过DBShap提供的解释，可以理解漂移背后的根本原因。

    

    当机器学习模型被用于预测其未训练数据的结果时，其性能常常会下降。这种情况在现实世界中经常发生，因为数据的分布会逐渐或突然地发生变化，或由于像大流行病这样的重大事件。在机器学习研究中，已经有许多尝试提出能够抵御这种概念漂移的技术。然而，没有一个原则性的框架来确定导致模型性能漂移的原因。在本文中，我们提出了一个新颖的框架-DBShap，它使用Shapley值来确定漂移的主要贡献者并量化他们的贡献。所提出的框架不仅量化了单个特征在驱动漂移方面的重要性，还包括了输入和输出之间底层关系的变化作为可能的驱动因素。DBShap所提供的解释可以用于理解漂移背后的根本原因。

    Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained. These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic. There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts. However, there is no principled framework to identify the drivers behind the drift in model performance. In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions. The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver. The explanation provided by DBShap can be used to understand the root cause behind the drift and
    
[^49]: Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- 一个全面的符号回归框架

    Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework. (arXiv:2401.09748v1 [cs.SC])

    [http://arxiv.org/abs/2401.09748](http://arxiv.org/abs/2401.09748)

    引入了一个基于函数图像和操作树序列的科学计算多模态框架（Botfip），应用于符号回归问题，并验证了其在低复杂度问题上的优势，展示了其潜力。这个多模态框架在科学计算问题中具有广泛的应用前景。

    

    在科学计算领域中，许多问题解决方法往往只注重过程和最终结果，即使在科学人工智能领域，也缺乏对数据背后的深度多模态信息挖掘，缺乏类似于图像文本领域的多模态框架。本文以符号回归（SR）为重点，在图像文本领域的BLIP模型的启发下，提出了一种基于函数图像（Funcimg）和操作树序列（OTS）的科学计算多模态框架——引导OTS-Funcimg预训练模型（Botfip）。在SR实验中，我们验证了Botfip在低复杂度的SR问题中的优势，展示了其潜力。作为一个MED框架，Botfip在更广泛的科学计算问题中具有潜力。

    In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.
    
[^50]: 用于分析自闭症谈话的参数选择

    Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder. (arXiv:2401.09717v1 [eess.AS])

    [http://arxiv.org/abs/2401.09717](http://arxiv.org/abs/2401.09717)

    本文通过分析自闭症儿童与心理学家之间的诊断性对话中提取的声学/韵律和语言特征，研究了自闭症谈话分析的参数选择。研究结果可以提供对ASD儿童谈话数据的细致分析，支持诊断和干预。

    

    自闭症谱系障碍(ASD)的诊断是一项复杂而具有挑战性的任务，它依赖于心理学家对互动行为的分析，而不是生化诊断的使用。本文提出了一种通过分析心理学家与典型发育或有ASD的儿童之间诊断性对话中提取的声学/韵律和语言特征来进行ASD诊断的建模方法。我们比较了不同特征在一系列对话任务中的贡献。我们的重点是找到一组最小的参数来描述ASD儿童的谈话行为。由于ASD是通过对话互动进行诊断的，除了分析儿童的行为外，我们还调查了心理学家的对话行为在不同诊断群体之间是否有变化。我们的结果可以促进对ASD儿童的谈话数据进行细致分析，以支持诊断和干预。

    The diagnosis of autism spectrum disorder (ASD) is a complex, challenging task as it depends on the analysis of interactional behaviors by psychologists rather than the use of biochemical diagnostics. In this paper, we present a modeling approach to ASD diagnosis by analyzing acoustic/prosodic and linguistic features extracted from diagnostic conversations between a psychologist and children who either are typically developing (TD) or have ASD. We compare the contributions of different features across a range of conversation tasks. We focus on finding a minimal set of parameters that characterize conversational behaviors of children with ASD. Because ASD is diagnosed through conversational interaction, in addition to analyzing the behavior of the children, we also investigate whether the psychologist's conversational behaviors vary across diagnostic groups. Our results can facilitate fine-grained analysis of conversation data for children with ASD to support diagnosis and intervention.
    
[^51]: HCVP: 基于层次对比视觉提示的领域泛化方法

    HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization. (arXiv:2401.09716v1 [cs.CV])

    [http://arxiv.org/abs/2401.09716](http://arxiv.org/abs/2401.09716)

    HCVP是一种基于层次对比视觉提示的领域泛化方法，通过引导模型将不变特征与特定特征分离，提高了泛化性能。

    

    领域泛化（DG）旨在通过学习不变特征来创建在未知场景中表现出色的机器学习模型。然而，在DG中，将模型限制在固定结构或统一参数化中以包含不变特征的主流实践可能会不可避免地融合特定方面。这种方法难以对领域间变化进行细微区分，可能对某些领域存在偏见，从而阻碍了对域不变特征的精确学习。鉴于此，我们引入了一种新方法，旨在为模型提供领域级和任务特定的特征。该方法旨在更有效地引导模型将不变特征与特定特征分离，从而提高泛化性能。在领域泛化范式中，借鉴了视觉提示的新趋势，我们的工作引入了一种新颖的“HCVP”（层次对比视觉提示）方法。

    Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents 
    
[^52]: 使用基于Transformer的模型与InfoNCE损失和语言切换方法的课程推荐

    Curriculum Recommendations Using Transformer Base Model with InfoNCE Loss And Language Switching Method. (arXiv:2401.09699v1 [cs.CL])

    [http://arxiv.org/abs/2401.09699](http://arxiv.org/abs/2401.09699)

    这项研究提出了使用Transformer基础模型、InfoNCE损失和语言切换方法来解决课程推荐中的内容冲突和语言翻译引起的干扰问题，旨在构建一个个性化学习体验、包容多样性的教育环境。

    

    课程推荐范式致力于在不断发展的教育技术和课程开发领域中促进学习平等。鉴于现有方法所面临的内容冲突和语言翻译引起的干扰等困难，该范式旨在面对并克服这些挑战。特别是，它解决了语言翻译引入的内容冲突和干扰问题，这些问题可能阻碍创建全面和个性化学习体验。该范式的目标是培养一个既包容多样性又可以根据每个学习者的独特需求定制学习体验的教育环境。为了克服这些挑战，我们的方法在课程开发和个性化学习方面引入了三个关键创新。其中包括使用Transformer基础模型增强计算能力。

    The Curriculum Recommendations paradigm is dedicated to fostering learning equality within the ever-evolving realms of educational technology and curriculum development. In acknowledging the inherent obstacles posed by existing methodologies, such as content conflicts and disruptions from language translation, this paradigm aims to confront and overcome these challenges. Notably, it addresses content conflicts and disruptions introduced by language translation, hindrances that can impede the creation of an all-encompassing and personalized learning experience. The paradigm's objective is to cultivate an educational environment that not only embraces diversity but also customizes learning experiences to suit the distinct needs of each learner. To overcome these challenges, our approach builds upon notable contributions in curriculum development and personalized learning, introducing three key innovations. These include the integration of Transformer Base Model to enhance computational e
    
[^53]: 在结束恋情过程中，ChatGPT是否应该替你写分手短信？探索AI在恋爱解体中的角色。

    Should ChatGPT Write Your Breakup Text? Exploring the Role of AI in Relationship Dissolution. (arXiv:2401.09695v1 [cs.HC])

    [http://arxiv.org/abs/2401.09695](http://arxiv.org/abs/2401.09695)

    这项研究探讨了AI在恋爱解体过程中的作用。研究发现，当前技术在信息收集、社群支持和促进沟通方面发挥着重要作用。参与者预计AI可以满足不同阶段的需求，帮助解体恋情。

    

    恋爱关系对我们的幸福和幸福感至关重要。恋爱解体是恋爱生命周期的最后阶段，也是个人生活中最具压力的事件之一，可能对人们产生深远而持久的影响。随着通过计算机介质传达的解体过程越来越受到支持，以及AI介入的传播方式的可能未来影响，我们进行了一项半结构化访谈研究，共有21名参与者。我们的研究旨在了解：1）技术在解体过程中的当前角色，2）个人在过程中的需求和支持，以及3）AI如何满足这些需求。我们的研究显示，人们在结束恋情的不同阶段有不同的需求。目前，技术被用于信息收集和社群支持，在促成分手、使鬼魂式分手和拉黑成为可能，以及促进沟通。参与者预计AI可以帮助实现感知技巧。

    Relationships are essential to our happiness and wellbeing. The dissolution of a relationship, the final stage of relationship's lifecycle and one of the most stressful events in an individual's life, can have profound and long-lasting impacts on people. With the breakup process increasingly facilitated by computer-mediated communication (CMC), and the likely future influence of AI-mediated communication (AIMC) tools, we conducted a semi-structured interview study with 21 participants. We aim to understand: 1) the current role of technology in the breakup process, 2) the needs and support individuals have during the process, and 3) how AI might address these needs. Our research shows that people have distinct needs at various stages of ending a relationship. Presently, technology is used for information gathering and community support, acting as a catalyst for breakups, enabling ghosting and blocking, and facilitating communication. Participants anticipate that AI could aid in sense-ma
    
[^54]: 将图像特征输入到神经网络的每一层的模仿学习

    Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])

    [http://arxiv.org/abs/2401.09691](http://arxiv.org/abs/2401.09691)

    本文提出了一种在模仿学习中解决多模态数据处理挑战的方法，通过将数据输入到每个神经网络层中，放大与期望输出的相关性较低的数据的影响，并通过实验证明了成功率的显著提高。

    

    模仿学习使得机器人能够通过训练数据学习并复制人类行为。最近机器学习的进展使得能够直接处理高维观测数据（如图像）的端到端学习方法成为可能。然而，在处理多个模态的数据时，这些方法面临着一个关键挑战，即在使用短采样周期时无意中忽略与期望输出的相关性较低的数据。本文提出了一种有效的方法来解决这个挑战，通过将数据输入到每个神经网络层中，放大与输出相关性较低的数据的影响。所提出的方法有效地将多样的数据源纳入到学习过程中。通过使用原始图像和关节信息作为输入进行简单的拾取放置操作的实验，即使处理来自短采样周期的数据，也证明了成功率显著提高。

    Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
    
[^55]: 小型多智能体深度强化学习在无人机Metaverse中的双胞胎迁移：一种多领导者多从属者Stackelberg博弈方法

    Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach. (arXiv:2401.09680v1 [cs.AI])

    [http://arxiv.org/abs/2401.09680](http://arxiv.org/abs/2401.09680)

    本论文提出了一种基于小型机器学习的Stackelberg博弈框架，在无人机Metaverse中实现高效的双胞胎迁移，以提供无缝沉浸式体验。

    

    无人机与Metaverse的协同作用正在催生一种新兴范式，称为无人机Metaverse，它创建了一个统一的生态系统，融合了物理和虚拟空间，改变了无人机的交互和虚拟探索。无人机双胞胎（UTs）作为无人机的数字孪生品，通过使其更具沉浸感、真实感和信息丰富性，革新无人机应用。UTs部署在地面基站（例如道路边缘单元（RSUs））上，并通过为无人机Metaverse用户（UMUs）提供Metaverse服务。由于无人机的动态移动性和RSUs的有限通信覆盖范围，进行实时的UT迁移至关重要，以确保UMUs的无缝沉浸式体验。然而，选择合适的RSUs并优化所需带宽对于实现可靠高效的UT迁移是具有挑战性的。为了解决这些挑战，我们提出了一种基于修剪技术的小型机器学习Stackelberg博弈框架，以实现高效的UT迁移。

    The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving rise to an emerging paradigm named UAV metaverses, which create a unified ecosystem that blends physical and virtual spaces, transforming drone interaction and virtual exploration. UAV Twins (UTs), as the digital twins of UAVs that revolutionize UAV applications by making them more immersive, realistic, and informative, are deployed and updated on ground base stations, e.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse Users (UMUs). Due to the dynamic mobility of UAVs and limited communication coverages of RSUs, it is essential to perform real-time UT migration to ensure seamless immersive experiences for UMUs. However, selecting appropriate RSUs and optimizing the required bandwidth is challenging for achieving reliable and efficient UT migration. To address the challenges, we propose a tiny machine learning-based Stackelberg game framework based on pruning techniques for efficient UT 
    
[^56]: 迈向可识别的无监督领域转换：一种多样化分布匹配的方法

    Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])

    [http://arxiv.org/abs/2401.09671](http://arxiv.org/abs/2401.09671)

    本研究旨在解决无监督领域转换中的可识别性问题，引入了一个MPA消除理论，解决了CycleGAN及其变体产生内容不对齐的限制。

    

    无监督领域转换（UDT）旨在找到将一个领域的样本（例如素描）转换为另一个领域（例如照片）的函数，同时不改变高层语义意义（也称为“内容”）。这些转换函数通常通过转换源领域和目标领域的概率分布来寻找。CycleGAN可以说是这一领域中最具代表性的方法。然而，文献中指出CycleGAN及其变体可能无法识别所需的转换函数，并产生内容不对齐的转换。这种局限性源于学习准则解空间中存在多个转换函数，称为“保度自同构（MPA）”。尽管意识到了这种可识别性问题，但解决方案仍然难以找到。本研究深入探究了核心的可识别性问题，并引入了MPA消除理论。我们的分析表明...

    Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysi
    
[^57]: 使用深度强化学习和真实世界轨迹数据的自动驾驶车辆交通平滑控制器

    Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data. (arXiv:2401.09666v1 [eess.SY])

    [http://arxiv.org/abs/2401.09666](http://arxiv.org/abs/2401.09666)

    本研究提出了一种使用深度强化学习和真实世界轨迹数据的自动驾驶车辆交通平滑控制器。通过观察前方车辆的速度和距离以及交通的下游状态，我们训练出了能够减少能耗的波浪平滑策略，并在低自动驾驶车辆渗透率下实现了显著的燃油节省。

    

    设计能够部署到自动驾驶车辆上的交通平滑巡航控制器是改善交通流量、减少拥堵和提高燃油效率的关键步骤。我们通过利用田纳西州I-24高速公路上的真实轨迹数据，在一个单车道模拟中回放数据，绕过了需要仔细调整大型交通微模拟器的常见问题。使用标准的深度强化学习方法，我们训练能减少能耗的波浪平滑策略。作为代理的输入，我们观察前方车辆的速度和距离，这是大多数最近车辆上常见的本地状态，以及关于交通的下游状态的非本地观察。我们显示，在低于4%的自动驾驶车辆渗透率下，在出现很多停滞波的轨迹上，我们实现了超过15%的显著燃油节省。最后，我们分析了控制器的平滑效果，并展示了其鲁棒性。

    Designing traffic-smoothing cruise controllers that can be deployed onto autonomous vehicles is a key step towards improving traffic flow, reducing congestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass the common issue of having to carefully fine-tune a large traffic microsimulator by leveraging real-world trajectory data from the I-24 highway in Tennessee, replayed in a one-lane simulation. Using standard deep reinforcement learning methods, we train energy-reducing wave-smoothing policies. As an input to the agent, we observe the speed and distance of only the vehicle in front, which are local states readily available on most recent vehicles, as well as non-local observations about the downstream state of the traffic. We show that at a low 4% autonomous vehicle penetration rate, we achieve significant fuel savings of over 15% on trajectories exhibiting many stop-and-go waves. Finally, we analyze the smoothing effect of the controllers and demonstrate robustne
    
[^58]: 移动性加速学习：车联网中分层联邦学习的收敛分析

    Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks. (arXiv:2401.09656v1 [cs.LG])

    [http://arxiv.org/abs/2401.09656](http://arxiv.org/abs/2401.09656)

    在车联网中，通过收敛性分析，本文证明移动性对分层联邦学习的收敛速度有积极影响，它增加了边缘层异构数据的融合和更快的数据融合速度，从而提高模型准确性。

    

    分层联邦学习(HFL)以隐私保护的方式，在多个设备上通过几个边缘服务器和一个云边缘服务器进行模型的分布式训练。本文考虑高度移动的设备，主要针对车联网。通过收敛性分析，我们证明移动性通过融合边缘数据和洗牌边缘模型来影响收敛速度。尽管从通信的角度来看，移动性通常被视为一个挑战，但我们证明它增加了边缘层异构数据下HFL的收敛速度，因为更多多样化的数据可以被合并。此外，我们证明高速度导致更快的收敛，因为它加速了数据的融合。仿真结果表明，当在CIFAR-10数据集上训练卷积神经网络时，移动性可以使HFL的模型准确性提高高达15.1%。

    Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner. In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks. Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models. While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated. Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data. Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset.
    
[^59]: 神经符号推理和学习的凸二级优化研究

    Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])

    [http://arxiv.org/abs/2401.09651](http://arxiv.org/abs/2401.09651)

    本研究通过凸二级优化技术，开发了一个通用的基于梯度的神经和符号参数学习框架，具有100倍以上的学习时间改进和高达16%的预测性能提升。

    

    通过利用凸二级优化技术，我们解决了神经符号系统的一个关键挑战，开发了一个通用的基于梯度的端到端神经和符号参数学习框架。我们利用最先进的神经符号体系结构NeuPSL来证明我们的框架的适用性。为了实现这一目标，我们提出了NeuPSL推理的平滑原始和对偶形式，并显示学习梯度是最优对偶变量的函数。此外，我们为新的形式开发了一种对偶块坐标下降算法，自然地利用了热启动。这使得我们相比当前最好的NeuPSL推理方法的学习时间改进了100倍以上。最后，我们对涵盖各种任务的8个数据集进行了广泛的实证评估，并证明我们的学习框架相比替代学习方法能够提升高达16%的预测性能。

    We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.
    
[^60]: ClimateGPT: 实现对气候变化领域的跨学科研究进行合成的AI模型

    ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])

    [http://arxiv.org/abs/2401.09646](http://arxiv.org/abs/2401.09646)

    ClimateGPT是一个针对气候变化领域的跨学科研究合成的AI模型，通过优化检索增强和使用级联机器翻译方法，提高了模型的性能和可访问性。

    

    本文介绍了ClimateGPT，一种特定领域的大型语言模型系列，用于合成气候变化的跨学科研究。我们从头开始训练了两个7B模型，训练数据集包含300B个科学导向的令牌。第一个模型在预训练期间包含了4.2B个特定领域的令牌，第二个模型在预训练后针对气候领域进行了调整。此外，我们还对ClimateGPT-7B，13B和70B进行了连续预训练，训练数据集包含4.2B个特定领域的令牌，并与气候科学家紧密合作创建。为了减少虚构生成的数量，我们为模型进行了检索增强优化，并提出了一种分层检索策略。为了提高我们模型对非英语使用者的可访问性，我们建议利用级联机器翻译，并证明这种方法可以与翻译的性能相媲美。

    This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to 
    
[^61]: 通过物理引导的强化学习进行停电减轻

    Blackout Mitigation via Physics-guided RL. (arXiv:2401.09640v1 [eess.SY])

    [http://arxiv.org/abs/2401.09640](http://arxiv.org/abs/2401.09640)

    本文设计了一种物理引导的强化学习框架，利用传输网络的潮流灵敏度因子来指导强化学习训练，实现了通过实时补救前瞻决策来减轻黑暗模式的目标。

    

    本文考虑了为了防止黑暗模式而在系统异常时进行序列设计的补救控制行动。设计了一种物理引导的强化学习框架，用于识别在考虑系统稳定性长期影响的情况下的实时补救前瞻决策序列。本文考虑了涉及离散值传输线开关决策（线路重新连接和移除）和连续值发电机调整的控制行动空间。为了确定有效的停电减轻策略，设计了一种物理引导方法，利用与电力传输网络相关的潮流灵敏度因子来引导强化学习训练期间的探索。使用开源Grid2Op平台进行了全面的实证评估，证明了将物理信号纳入强化学习决策的显著优势，证实了所提出的物理引导方法的收益。

    This paper considers the sequential design of remedial control actions in response to system anomalies for the ultimate objective of preventing blackouts. A physics-guided reinforcement learning (RL) framework is designed to identify effective sequences of real-time remedial look-ahead decisions accounting for the long-term impact on the system's stability. The paper considers a space of control actions that involve both discrete-valued transmission line-switching decisions (line reconnections and removals) and continuous-valued generator adjustments. To identify an effective blackout mitigation policy, a physics-guided approach is designed that uses power-flow sensitivity factors associated with the power transmission network to guide the RL exploration during agent training. Comprehensive empirical evaluations using the open-source Grid2Op platform demonstrate the notable advantages of incorporating physical signals into RL decisions, establishing the gains of the proposed physics-gu
    
[^62]: 大型语言模型辅助对患者阅读临床笔记的影响：一个混合方法研究

    Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study. (arXiv:2401.09637v1 [cs.HC])

    [http://arxiv.org/abs/2401.09637](http://arxiv.org/abs/2401.09637)

    通过大型语言模型辅助阅读临床笔记，患者可以获得更好的理解和自信。这项研究开发了一个工具，利用语言模型简化和增加上下文，使临床笔记更易读。研究结果表明，这些增强对患者有益。

    

    患者通过阅读他们的临床笔记获得了许多好处，包括增加对自身健康的控制感和对护理计划的理解提高。然而，在临床笔记中复杂的医学概念和术语阻碍了患者的理解，并可能导致焦虑。我们开发了一个面向患者的工具，利用大型语言模型（LLMs）简化笔记、从中提取信息并增加上下文，以使临床笔记更易读。我们使用我们的工具提示改进的GPT-4对由乳腺癌幸存者捐赠的真实临床笔记和临床医生生成的合成临床笔记进行这些增强任务。共有12条笔记，3868个字。2023年6月，我们随机分配了200名美国女性参与者，并向他们分发了三个具有不同程度增强的临床笔记。参与者回答了有关每个笔记的问题，评估了他们对后续行动的理解和自我报告的自信心。我们发现增强对阅读理解和自信心友好。

    Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were ass
    
[^63]: 学习捷径：关于语言模型中自然语言理解误导性承诺的论文

    Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])

    [http://arxiv.org/abs/2401.09615](http://arxiv.org/abs/2401.09615)

    该论文调查了大型语言模型在自然语言理解任务中使用捷径学习的现象，强调了这种现象对语言模型评估的影响，并呼吁加大对捷径学习的研究力度以提升语言模型的鲁棒性和实际场景中的自然语言理解评估标准。

    

    大型语言模型（LLMs）的出现在自然语言处理领域实现了显著的性能提升。然而，最近的研究发现，LLMs在执行任务时常常采用捷径，导致在决策规则上缺乏泛化能力，从而在性能上产生了一种错觉。这一现象在准确评估LLMs的自然语言理解能力上带来了挑战。本文对该领域的相关研究进行了简洁的概述，并提出了在评估语言模型，尤其是自然语言理解任务中使用捷径学习的影响的观点。本文呼吁加大对捷径学习的深入理解的研究力度，为开发更强大的语言模型和提高真实场景下自然语言理解评估的标准作出贡献。

    The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.
    
[^64]: 处理建议系统中大规模基数的方法

    Handling Large-scale Cardinality in building recommendation systems. (arXiv:2401.09572v1 [cs.IR])

    [http://arxiv.org/abs/2401.09572](http://arxiv.org/abs/2401.09572)

    本文提出了两种创新技术来解决建议系统中高基数的挑战，包括采用词袋模型和层共享来减小模型大小并提高性能。通过对Uber使用情况的实验验证，证明了这些技术在优化建议系统和提高性能方面的有效性。

    

    有效的建议系统依赖于捕捉用户偏好，通常需要包含无数实体的唯一标识符（UUID）等多种功能。然而，UUID的异常高基数在模型退化和稀疏性导致模型大小增加方面构成了重大挑战。本文提出了两种创新技术来解决建议系统中高基数的挑战。具体而言，我们提出了一种词袋模型的方法，结合层共享，以显著减小模型大小并提高性能。我们通过对Uber使用情况进行离线和在线实验评估了我们的技术，结果显示我们的方法在优化建议系统和提高其整体性能方面非常有效。

    Effective recommendation systems rely on capturing user preferences, often requiring incorporating numerous features such as universally unique identifiers (UUIDs) of entities. However, the exceptionally high cardinality of UUIDs poses a significant challenge in terms of model degradation and increased model size due to sparsity. This paper presents two innovative techniques to address the challenge of high cardinality in recommendation systems. Specifically, we propose a bag-of-words approach, combined with layer sharing, to substantially decrease the model size while improving performance. Our techniques were evaluated through offline and online experiments on Uber use cases, resulting in promising results demonstrating our approach's effectiveness in optimizing recommendation systems and enhancing their overall performance.
    
[^65]: 使用反事实对抗优化实现大型语言模型的对齐

    Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])

    [http://arxiv.org/abs/2401.09566](http://arxiv.org/abs/2401.09566)

    本文研究了在大型语言模型中使用反事实对抗优化框架，以实现风格对齐，避免人类干预，并成功培养出可取行为和减轻不可取行为。

    

    大型语言模型(LLMs)的进步在各种应用中展示了卓越的能力。这些模型在生成上下文连贯且涵盖广泛主题的文本补全方面表现出色。然而，它们训练所需的大量数据使得在预训练和指令调整阶段对齐响应风格变得具有挑战性。因此，通常会采用额外的对齐阶段，进一步使用人类偏好数据对模型进行训练，以更好地将其输出与人类期望对齐。虽然这个过程本身并没有引入新的能力，但它突出了模型固有的生成风格。本文研究了在直接偏好优化(DPO)框架内利用反事实提示来对齐模型的风格，而不依赖人类干预。我们证明了这种方法有效地培养了可取的行为，减轻了不可取的行为。

    Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and
    
[^66]: 深度学习增强的混合整数优化: 学习减少模型维度

    Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])

    [http://arxiv.org/abs/2401.09556](http://arxiv.org/abs/2401.09556)

    本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。

    

    本研究介绍了一种利用深度学习解决混合整数规划模型中的计算复杂性的框架。我们比较了前馈神经网络(ANN)和卷积神经网络(CNN)在近似混合整数规划问题中的作用。我们利用多标签分类来考虑多个活动维度。为了提高框架的性能，我们采用贝叶斯优化进行超参数调优，以最大化样本级准确性。主要目标是训练神经网络准确地预测所有活动维度，从而最大化全局最优解的出现频率。我们将该框架应用于描述个性化医学供应链中的长期投资规划和中期战术规划的基于流的设施位置分配混合整数线性规划(MILP)问题。

    This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
    
[^67]: 通过人类反馈改进分类性能：标记一些，我们标记其余部分

    Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])

    [http://arxiv.org/abs/2401.09555](http://arxiv.org/abs/2401.09555)

    本文探讨了通过人类反馈来改进分类模型性能的方法。使用少量有标签示例，通过连续反馈循环，我们能够显著提高模型的准确性。在多个数据集上进行评估，结果表明这种方法能够超越零样本大型语言模型，提供更强的文本分类性能。

    

    在人工智能领域，大部分数据是非结构化的，因此获取足够的有标签数据来训练监督式机器学习模型是一个重要挑战。为了解决这个问题，我们深入研究少样本学习和主动学习，即通过人类反馈来改进人工智能模型，仅使用一小部分有标签示例。本文着重于理解连续反馈循环如何改善模型，从而通过渐进式的人类参与提高模型的准确性、回归和精确度。通过使用大型语言模型（LLMs），如GPT-3.5、BERT和SetFit，我们旨在分析使用有限数量的有标签示例显著提高模型准确性的效果。我们在Financial Phrasebank、Banking、Craigslist、Trec和Amazon Reviews数据集上对此方法进行基准测试，证明仅使用少量有标签示例就能超越零样本大型语言模型的准确性，提供增强的文本分类性能。

    In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performa
    
[^68]: BERTologyNavigator: 基于BERT语义的高级问题回答系统

    BERTologyNavigator: Advanced Question Answering with BERT-based Semantics. (arXiv:2401.09553v1 [cs.CL])

    [http://arxiv.org/abs/2401.09553](http://arxiv.org/abs/2401.09553)

    BERTologyNavigator是一个基于BERT语义的高级问题回答系统，结合关系抽取和BERT嵌入，可以在DBLP知识图谱中精确地导航关系，并在测试数据集上达到了较高的F1分数。

    

    知识图谱与语言模型的开发和集成在人工智能和自然语言处理方面具有重要意义。本研究介绍了BERTologyNavigator——一个将关系抽取技术和BERT嵌入相结合的两阶段系统，用于在DBLP知识图谱中导航关系。我们的方法专注于提取一跳关系和标记的候选对，然后在第二阶段使用BERT的CLS嵌入和其他启发式方法进行关系选择。我们的系统在Scholarly QALD的DBLP QuAD Final测试数据集上达到了0.2175的F1分数，而在DBLP QuAD测试数据集的子集上在QA阶段达到了0.98的F1分数。

    The development and integration of knowledge graphs and language models has significance in artificial intelligence and natural language processing. In this study, we introduce the BERTologyNavigator -- a two-phased system that combines relation extraction techniques and BERT embeddings to navigate the relationships within the DBLP Knowledge Graph (KG). Our approach focuses on extracting one-hop relations and labelled candidate pairs in the first phases. This is followed by employing BERT's CLS embeddings and additional heuristics for relation selection in the second phase. Our system reaches an F1 score of 0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score on the subset of the DBLP QuAD test dataset during the QA phase.
    
[^69]: 通过克里洛夫子空间回收加速神经算子的数据生成

    Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])

    [http://arxiv.org/abs/2401.09516](http://arxiv.org/abs/2401.09516)

    该论文提出了一种名为排序克里洛夫回收（SKR）的新方法，用于加速神经算子训练的数据生成。该方法解决了现有方法在解决PDE问题时计算冗余的问题，显著提高了数据生成效率。

    

    学习用于解决偏微分方程(PDE)的神经算子因其高推理效率而受到广泛关注。然而，训练这些算子需要生成大量带有解决方案的标记数据，即PDE问题及其解决方案。数据生成过程非常耗时，因为它涉及解决大量线性方程组以获得PDE的数值解。许多现有方法独立地解决这些系统，而不考虑它们的内在相似性，导致计算极其冗余。为了解决这个问题，我们提出了一种新颖的方法，即排序克里洛夫回收(SKR)，以提高解决这些系统的效率，从而显著加速神经算子训练的数据生成。据我们所知，SKR是第一个解决学习神经算子数据生成耗时性质的尝试。SKR的核心是克里洛夫子空间。

    Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov sub
    
[^70]: 技术报告：关于节点不可访问情况下流言学习收敛性的研究

    Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])

    [http://arxiv.org/abs/2401.09498](http://arxiv.org/abs/2401.09498)

    本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。

    

    Gossip learning（GL）作为分散式学习的一种替代方法，更适用于资源受限的无线网络，如由无人机（UAV）组成的FANETs。GL能够显著提高UAV网络的效率并延长电池寿命。尽管具有这些优势，但GL的性能受数据分布、通信速度和网络连接性的影响较大。然而，这些因素如何影响GL的收敛性仍不清楚。现有研究基于虚拟数量来研究GL的收敛性，以方便性而忽略了当一些节点不可访问时网络的真实状态。在本文中，我们对动态网络拓扑下不可访问节点对GL的影响进行了建模和研究。首先，我们将权重发散分解为节点是否可访问的情况。然后，我们研究了在节点可访问性的动态下GL的收敛性，并在理论上提供了

    Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
    
[^71]: 记忆、空间和规划: 多尺度预测表示

    Memory, Space, and Planning: Multiscale Predictive Representations. (arXiv:2401.09491v1 [cs.AI])

    [http://arxiv.org/abs/2401.09491](http://arxiv.org/abs/2401.09491)

    本研究通过综合计算、行为和神经证据，揭示了记忆与预测、规划之间的相互关系以及其在海马体和前额叶皮质中的多尺度预测表示，为我们理解大脑中的记忆和规划机制提供了重要启示。

    

    记忆与预测和规划密不可分。生物和人工智能智能体的灵活行为取决于在不断变化的环境中从过去中学习和预测未来的相互作用。本章回顾了计算、行为和神经证据，表明这些过程依赖于学习经验的关系结构，即认知地图，并得出两个关键要点。首先，这些记忆结构在海马体和前额叶皮质（PFC）层次结构中组织为多尺度、紧凑的预测表示。其次，我们认为这种预测性记忆结构对海马体和PFC的互补功能至关重要，既能使过去的详细和连贯的事件回忆起来，也能在不同尺度上推广经验以实现高效的预测和规划。这些见解推动了我们对大脑中记忆和规划机制的理解，并具有重要的影响。

    Memory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps, and draws two key takeaways. First, that these memory structures are organized as multiscale, compact predictive representations in hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that such predictive memory structures are crucial to the complementary functions of the hippocampus and PFC, both for enabling a recall of detailed and coherent past episodes as well as generalizing experiences at varying scales for efficient prediction and planning. These insights advance our understanding of memory and planning mechanisms in the brain and hold significant implications for
    
[^72]: PUPAE: 直观且可操作的时间序列异常解释

    PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies. (arXiv:2401.09489v1 [cs.LG])

    [http://arxiv.org/abs/2401.09489](http://arxiv.org/abs/2401.09489)

    PUPAE是一种直观且可操作的时间序列异常解释方法，通过引入领域无关的反事实解释，能够帮助解释和处理异常。

    

    近年来，时间序列异常检测取得了显著进展。然而，在检测到异常之后，我们能解释它吗？这样的解释对于处理异常非常有用。例如，在一个炼油厂中，我们是通过派遣液压工程师还是实习生更换传感器电池来响应异常？虽然有一些并行的努力来解释异常，但很多提出的技术产生的解释是间接的，并且通常比它们试图解释的异常更复杂。我们对各个领域前线从业人员使用的文献、清单和用户手册进行了评估，发现了一个有趣的共同点。大多数从业人员以以下格式讨论、解释和报告异常：如果没有破坏B，异常就会像正常数据A一样。读者将会意识到这是一种反事实的解释。在这项工作中，我们引入了一种领域无关的反事实解释方法。

    In recent years there has been significant progress in time series anomaly detection. However, after detecting an (perhaps tentative) anomaly, can we explain it? Such explanations would be useful to triage anomalies. For example, in an oil refinery, should we respond to an anomaly by dispatching a hydraulic engineer, or an intern to replace the battery on a sensor? There have been some parallel efforts to explain anomalies, however many proposed techniques produce explanations that are indirect, and often seem more complex than the anomaly they seek to explain. Our review of the literature/checklists/user-manuals used by frontline practitioners in various domains reveals an interesting near-universal commonality. Most practitioners discuss, explain and report anomalies in the following format: The anomaly would be like normal data A, if not for the corruption B. The reader will appreciate that is a type of counterfactual explanation. In this work we introduce a domain agnostic counterf
    
[^73]: 使用多模态深度学习的不确定性感知硬件特洛伊检测

    Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])

    [http://arxiv.org/abs/2401.09479](http://arxiv.org/abs/2401.09479)

    本文提出了一种使用多模态深度学习进行硬件特洛伊检测的方法，通过生成对抗网络扩充数据，并采用早融合和晚融合策略进行评估。通过估计不确定性量化指标，实现风险感知的决策制定。

    

    在零信任的无厂无印造制造时代，硬件特洛伊在芯片生产的各个阶段被插入的风险增加了。为了应对这一问题，已经开发了各种机器学习解决方案用于检测硬件特洛伊。尽管大部分关注点都集中在统计学或深度学习方法上，但受到特洛伊感染基准样本数量有限的影响，检测准确性受限，无法检测到零日特洛伊。为了填补这一差距，我们首先采用生成对抗网络来扩充数据，以两种替代表示模态，图形和表格，确保数据集以代表性的方式分布。此外，我们提出了一种多模态深度学习方法来检测硬件特洛伊，并评估了早融合和晚融合策略的结果。我们还估计了每个预测的不确定性量化指标，用于风险感知的决策制定。结果不仅确认了我们方法的有效性，而且表明了不确定性估计对硬件特洛伊检测的重要性。

    The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our
    
[^74]: 使用区块链的农业食品供应链框架

    A Framework for Agricultural Food Supply Chain using Blockchain. (arXiv:2401.09476v1 [cs.CR])

    [http://arxiv.org/abs/2401.09476](http://arxiv.org/abs/2401.09476)

    本文提出了一个使用区块链的农业食品供应链框架，旨在通过建立信任和透明度确保食品供应链的安全性，并解决信息防篡改和供需关系等困难。

    

    本文的主要目标是通过使用区块链技术在食品供应链系统中建立信任和透明度，确保每个人的食品安全。食品供应链是从农民或生产者到买家的农作物追踪过程。随着区块链的出现，为提供各种农业必需品的安全和无欺诈环境已变得更加容易。目前的供应链市场包括涉及数据集成、复杂交易和分销的各种企业，由于贸易全球化所带来。信息防篡改、供需关系和可追溯监督是由此产生的困难。区块链是一种分布式账本技术，可以提供抗篡改的信息。这种策略可以消除对集中的信任机构、中介和业务历史的需求，从而实现增加生产和安全性的目标。

    The main aim of the paper is to create a trust and transparency in the food supply chain system, ensuring food safety for everyone with the help of Blockchain Technology. Food supply chain is the process of tracing a crop from the farmer or producer to the buyer. With the advent of blockchain, providing a safe and fraud-free environment for the provision of numerous agricultural necessities has become much easier. Because of the globalization of trade, the present supply chain market today includes various companies involving integration of data, complex transactions and distribution. Information tamper resistance, supply-demand relationships, and traceable oversight are all difficulties that arise as a result of this. Blockchain is a distributed ledger technology that can provide information that is resistant to tampering. This strategy can eliminate the need for a centralized trusted authority, intermediaries, and business histories, allowing for increased production and security whi
    
[^75]: 国内会话生成AI强化的多机器人系统中的商业和伦理问题

    Business and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems. (arXiv:2401.09473v1 [cs.CY])

    [http://arxiv.org/abs/2401.09473](http://arxiv.org/abs/2401.09473)

    本文关注国内会话生成AI强化的多机器人系统中的商业和伦理问题。合作式多机器人系统能够革新不同行业的流程并改变人类的商业方式，但也需要对其潜在的利益冲突、隐私实践和安全问题进行伦理考察。

    

    商业和技术通过逻辑和设计密切联系在一起。它们同样对社会变革敏感，可能因丑闻带来灾难。合作式多机器人系统(MRS)正在兴起，允许不同类型和品牌的机器人在各种环境中共同工作。生成式人工智能近年来一直是人工智能讨论的主要话题，因为它能够通过使用自然语言和生成媒体（包括深度伪造）来模仿人类。本文主要关注生成式AI的对话方面，因此使用了“对话生成式人工智能”（CGI）一词。像MRS一样，CGI在革新各个领域的流程并改变人类经营方式方面具有巨大潜力。从商业角度来看，仅就具有潜在利益冲突、隐私实践和安全问题的合作式MRS本身，就需要进行伦理考察。MRSs e

    Business and technology are intricately connected through logic and design. They are equally sensitive to societal changes and may be devastated by scandal. Cooperative multi-robot systems (MRSs) are on the rise, allowing robots of different types and brands to work together in diverse contexts. Generative artificial intelligence has been a dominant topic in recent artificial intelligence (AI) discussions due to its capacity to mimic humans through the use of natural language and the production of media, including deep fakes. In this article, we focus specifically on the conversational aspects of generative AI, and hence use the term Conversational Generative artificial intelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizing processes across sectors and transforming the way humans conduct business. From a business perspective, cooperative MRSs alone, with potential conflicts of interest, privacy practices, and safety concerns, require ethical examination. MRSs e
    
[^76]: 离线手写签名验证：一种迁移学习和特征选择方法

    Offline Handwriting Signature Verification: A Transfer Learning and Feature Selection Approach. (arXiv:2401.09467v1 [cs.CV])

    [http://arxiv.org/abs/2401.09467](http://arxiv.org/abs/2401.09467)

    这篇论文介绍了一种离线手写签名验证的方法，通过迁移学习和特征选择来提高验证结果，在金融、法律文件和安全等领域有广泛应用。

    

    手写签名验证在生物识别和文件真实性方面具有一定挑战。目标是确定提供的手写签名的真实性，区分真实的和伪造的签名。这个问题在金融、法律文件和安全等领域有很多应用。目前，计算机视觉和机器学习在手写签名验证领域取得了显著进展。然而，根据获取的结果、数据集的结构和所使用的模型，结果可能会有所改善。我们提出的策略包括四个阶段。首先，我们从420个不同个体中收集了12600张图片的大型数据集，每个个体有30个特定类型的签名（所有作者的签名都是真实的）。在后续阶段，使用一个称为MobileNetV2的深度学习模型提取了每张图片中的最好特征。在特征选择步骤中，使用了三个选择器。

    Handwritten signature verification poses a formidable challenge in biometrics and document authenticity. The objective is to ascertain the authenticity of a provided handwritten signature, distinguishing between genuine and forged ones. This issue has many applications in sectors such as finance, legal documentation, and security. Currently, the field of computer vision and machine learning has made significant progress in the domain of handwritten signature verification. The outcomes, however, may be enhanced depending on the acquired findings, the structure of the datasets, and the used models. Four stages make up our suggested strategy. First, we collected a large dataset of 12600 images from 420 distinct individuals, and each individual has 30 signatures of a certain kind (All authors signatures are genuine). In the subsequent stage, the best features from each image were extracted using a deep learning model named MobileNetV2. During the feature selection step, three selectors nei
    
[^77]: 自监督视觉用于气候下调尺度

    Self Supervised Vision for Climate Downscaling. (arXiv:2401.09466v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.09466](http://arxiv.org/abs/2401.09466)

    这项工作提出了一种深度学习模型，用于下调尺度 ESM 模拟数据，不需要高分辨率的真实数据进行模型优化。

    

    气候变化是我们星球面临的最重要的挑战之一。全球气温的上升已经带来了对地球气候和天气模式的显著变化，不可预测和极端天气事件的频率增加。气候变化研究的未来预测基于地球系统模型（ESMs），这些计算机模型模拟地球的气候系统。 ESMs 提供了整合各种物理系统的框架，但它们的输出受到运行和存档更高分辨率模拟所需的巨大计算资源的限制。对于给定的资源预算，ESMs 通常在较粗的网格上运行，然后进行计算量较小的“下调尺度”过程以获得更高分辨率的输出。在这项工作中，我们提出了一种用于下调尺度 ESM 模拟数据的深度学习模型，该模型不需要高分辨率的真实数据进行模型优化。这是通过利用显著的数据分布来实现的。

    Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already bringing noticeable changes to Earth's weather and climate patterns with an increased frequency of unpredictable and extreme weather events. Future projections for climate change research are based on Earth System Models (ESMs), the computer models that simulate the Earth's climate system. ESMs provide a framework to integrate various physical systems, but their output is bound by the enormous computational resources required for running and archiving higher-resolution simulations. For a given resource budget, the ESMs are generally run on a coarser grid, followed by a computationally lighter $downscaling$ process to obtain a finer-resolution output. In this work, we present a deep-learning model for downscaling ESM simulation data that does not require high-resolution ground truth data for model optimization. This is realized by leveraging salient data distribu
    
[^78]: 我的角色是什么？对基于AI的安全关键系统的责任建模

    What's my role? Modelling responsibility for AI-based safety-critical systems. (arXiv:2401.09459v1 [cs.CY])

    [http://arxiv.org/abs/2401.09459](http://arxiv.org/abs/2401.09459)

    许多作者已经评论了AI-SCS的“责任差距”，即开发人员和制造商难以对AI的有害行为负责，这是由于AI的复杂开发周期、性能不确定性和动态操作环境所致。在AI-SCS变得自主化后，人类操作员可能成为承担责任的替罪羊，他们可能承担由AI-SCS输出的后果，而这些后果他们没有参与创建，也可能不理解。

    

    基于AI的安全关键系统（AI-SCS）正在越来越多地在现实世界中部署。这可能对人类和环境造成危害。在开发和运营过程中，降低风险是一项首要任务。随着越来越多的AI-SCS变得自主化，通过人类干预管理风险的一层已被移除。在事故发生后，重要的是要确定因果关系贡献和背后的不同责任主体，以从错误中汲取教训，防止类似的未来事件。许多作者已经评论了“责任差距”，即开发人员和制造商很难对AI-SCS的有害行为负责。这是由于AI开发周期复杂、AI性能不确定和动态操作环境所致。人类操作员可能成为“责任替罪羊”，因AI-SCS输出的后果承担责任，这些后果他们没有参与创建，也可能不理解。

    AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the "responsibility gap" where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a "liability sink" absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of.  This cross-dis
    
[^79]: 集成卫星-地面网络的动态路由：基于约束的多智能体强化学习方法

    Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach. (arXiv:2401.09455v1 [cs.NI])

    [http://arxiv.org/abs/2401.09455](http://arxiv.org/abs/2401.09455)

    本论文提出了一种基于约束的多智能体强化学习方法来解决集成卫星-地面网络（ISTN）系统的动态路由问题，有效平衡了快速通信、能源效率和数据包丢失要求。

    

    集成卫星-地面网络（ISTN）系统经历了显著增长，为偏远地区提供了无缝通信服务，解决了有限的地面基础设施问题。然而，为ISTN设计路由方案极具挑战性，主要是由于增加了地面站的复杂性，并要求满足与卫星服务质量有关的各种约束条件。为解决这些挑战，我们研究了与地面站和卫星共同传输数据包的路由，同时优先考虑快速通信、满足能源效率和数据包丢失要求。具体而言，我们使用拉格朗日方法将带约束的数据包路由问题制定为一个最大最小化问题。然后，我们提出了一种名为CMADR的新颖约束多智能体强化学习（MARL）动态路由算法，它有效地平衡目标改善和约束满足。

    The integrated satellite-terrestrial network (ISTN) system has experienced significant growth, offering seamless communication services in remote areas with limited terrestrial infrastructure. However, designing a routing scheme for ISTN is exceedingly difficult, primarily due to the heightened complexity resulting from the inclusion of additional ground stations, along with the requirement to satisfy various constraints related to satellite service quality. To address these challenges, we study packet routing with ground stations and satellites working jointly to transmit packets, while prioritizing fast communication and meeting energy efficiency and packet loss requirements. Specifically, we formulate the problem of packet routing with constraints as a max-min problem using the Lagrange method. Then we propose a novel constrained Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named CMADR, which efficiently balances objective improvement and constraint satisfacti
    
[^80]: Voila-A: 用用户注视注意力对齐视觉-语言模型

    Voila-A: Aligning Vision-Language Models with User's Gaze Attention. (arXiv:2401.09454v1 [cs.CV])

    [http://arxiv.org/abs/2401.09454](http://arxiv.org/abs/2401.09454)

    本文介绍了一种使用用户注视注意力对齐视觉-语言模型的方法，在处理复杂场景和多个物体的实际应用中提高了模型的可解释性和效果。

    

    在最近几年中，视觉和语言理解的整合通过视觉-语言模型（VLMs）在人工智能领域取得了重要突破。然而，现有的VLMs在处理复杂场景和多个物体的实际应用以及与人类用户的多样化注意力模式相一致方面面临挑战。本文引入了通过增强现实（AR）或虚拟现实（VR）设备收集的注视信息，作为人类注意力的代理来引导VLMs，并提出了一种新的方法Voila-A，以提高这些模型在实际应用中的可解释性和效果。首先，我们收集了数百分钟的注视数据，以展示我们可以使用本地化的叙事来模拟人类的注视方式。然后，我们设计了一个自动数据注释流水线，利用GPT-4生成了VOILA-COCO数据集。此外，我们创新了Voila Perceiver模块，将注视信息整合到VL模型中。

    In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VL
    
[^81]: 使用黎曼几何特征学习飞机机翼上的压力系数

    Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings. (arXiv:2401.09452v1 [cs.LG])

    [http://arxiv.org/abs/2401.09452](http://arxiv.org/abs/2401.09452)

    该论文提出了一种将黎曼几何特征应用于学习翼面压力系数分布的方法，以提高气动系数的预测准确性。

    

    飞机的气动系数受其几何形状的显著影响，尤其是当攻角较大时。在空气动力学领域，传统的基于多项式的参数化方法使用尽可能少的参数来描述翼型的几何形状。然而，由于翼的三维几何形状比二维翼型复杂，基于多项式的参数化方法难以准确表示翼在三维空间中的整体形状。现有的基于深度学习的方法可以提取用于描述二维翼型或翼截面形状的大量潜在神经表示。最近的研究表明，直接将几何特征作为神经网络的输入可以提高预测的气动系数的准确性。受几何理论的启发，我们提出了将黎曼几何特征纳入学习翼面压力系数分布的方法。我们的方法计算几何特征（黎曼）。

    The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Rieman
    
[^82]: 扩散驱动的分子构象预测生成框架

    Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.09451](http://arxiv.org/abs/2401.09451)

    本文介绍了一种基于扩散驱动的生成框架\method{}，用于预测分子的三维构象，具有较高的预测精度并改进了传统方法的不足。

    

    从二维图形表示中推断出三维分子构型的任务在计算化学和药物开发领域具有重要意义。它对我们理解分子机制和相互作用起着基本作用。机器学习，特别是深度生成网络的快速发展，推动了这种预测建模精度的突破。传统方法通常采用分叉策略：首先估计原子间距，然后通过解决距离几何问题来塑造分子的空间结构。然而，这种顺序方法有时无法准确捕捉到局部原子排列的复杂性，从而损害结果结构模型的完整性。为了解决这些不足，本文引入了一个前卫的生成框架：\method{}，它基于扩散驱动的方法进行预测，并取得了重要的改进。

    The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the dif
    
[^83]: 使用人工智能协助的病理诊断中的合作：EMPAIA计划

    Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative. (arXiv:2401.09450v1 [cs.CY])

    [http://arxiv.org/abs/2401.09450](http://arxiv.org/abs/2401.09450)

    EMPAIA倡议汇集了病理学领域的利益相关方，开发了技术互操作标准，标准化接口，以及推广和应用人工智能方法于病理学诊断的建议，实现了不同供应商的多个AI应用程序整合。

    

    在过去的十年中，病理学中的人工智能方法有了显著的进展。然而，由于技术和监管方面的种种挑战，将这些研究成果转化为临床诊断产品并推广应用的速度仍然较慢，其中包括缺乏标准化的接口。开放和供应商中立的EMPAIA计划致力于解决这些挑战。本文概述了EMPAIA的成果和经验教训。EMPAIA将病理学AI生态系统的各种利益相关方，如病理学家、计算机科学家和工业界进行紧密合作。在紧密合作中，我们制定了技术互操作性标准，AI测试和产品开发的建议，以及可解释性方法。我们实现了模块化和开源的EMPAIA平台，并成功将来自6个不同供应商的11个基于AI的图像分析应用程序集成到该平台上，展示了不同应用程序如何使用统一的标准化接口。

    Over the past decade, artificial intelligence (AI) methods in pathology have advanced substantially. However, integration into routine clinical practice has been slow due to numerous challenges, including technical and regulatory hurdles in translating research results into clinical diagnostic products and the lack of standardized interfaces. The open and vendor-neutral EMPAIA initiative addresses these challenges. Here, we provide an overview of EMPAIA's achievements and lessons learned. EMPAIA integrates various stakeholders of the pathology AI ecosystem, i.e., pathologists, computer scientists, and industry. In close collaboration, we developed technical interoperability standards, recommendations for AI testing and product development, and explainability methods. We implemented the modular and open-source EMPAIA platform and successfully integrated 11 AI-based image analysis apps from 6 different vendors, demonstrating how different apps can use a single standardized interface. We 
    
[^84]: Tumbug:一种图像化的通用知识表达方法

    Tumbug: A pictorial, universal knowledge representation method. (arXiv:2401.09448v1 [cs.AI])

    [http://arxiv.org/abs/2401.09448](http://arxiv.org/abs/2401.09448)

    Tumbug是一种图像化的通用知识表达方法，专门用于常识推理，通过使用约30个基于科学和人类生活的基本概念组件，将其与以往的概念依存理论进行区分。

    

    鉴于人工通用智能(AGI)的关键被普遍认为是常识推理(CSR)，或者粗略地说就是发现一种特别适用于CSR的知识表示方法(KRM)，作者开发了一种自定义的CSR KRM。这种新颖的KRM称为Tumbug，其设计为图像化，因为越来越多的证据表明人脑使用了某种图像化类型的KRM，而在AGI领域中没有知名的研究探索了这种KRM可能性。Tumbug与Roger Schank的概念依存理论(CD)有些相似，但Tumbug是图像化的，并使用了大约30个基于科学和人类生活的基本概念的组件，而CD理论是文本型的，并使用了大约17个基于人类导向活动的组件(= 6个原始概念类别+ 11个原始行为)。所有Tumbug的构建块都被发现能够推广到仅有的五个基本构建块，这些构建块与真实世界中对应的内容完全一致。

    Since the key to artificial general intelligence (AGI) is commonly believed to be commonsense reasoning (CSR) or, roughly equivalently, discovery of a knowledge representation method (KRM) that is particularly suitable for CSR, the author developed a custom KRM for CSR. This novel KRM called Tumbug was designed to be pictorial in nature because there exists increasing evidence that the human brain uses some pictorial type of KRM, and no well-known prior research in AGI has researched this KRM possibility. Tumbug is somewhat similar to Roger Schank's Conceptual Dependency (CD) theory, but Tumbug is pictorial and uses about 30 components based on fundamental concepts from the sciences and human life, in contrast to CD theory, which is textual and uses about 17 components (= 6 Primitive Conceptual Categories + 11 Primitive Acts) based mainly on human-oriented activities. All the Building Blocks of Tumbug were found to generalize to only five Basic Building Blocks that exactly correspond t
    
[^85]: 《可解释的孟加拉语Memes的多模态情感分析》

    Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])

    [http://arxiv.org/abs/2401.09446](http://arxiv.org/abs/2401.09446)

    这项研究提出了一个多模态方法来解释孟加拉语Memes的情感，以填补此领域中低资源语言的研究空白。对比现有的数据集，提出了一个新的MemoSen数据集并表明其准确率的局限性。这项研究的主要贡献是在孟加拉语Memes情感分析领域引入了多模态方法。

    

    Memes已成为数字时代独特而有效的沟通形式，吸引了在线社区，并跨越文化障碍。尽管Memes经常和幽默联系在一起，但它们有着传达广泛情感的惊人能力，包括快乐、讽刺、沮丧等。在信息时代，理解和解释Memes背后的情感变得至关重要。先前的研究已探索了基于文本、基于图像和多模态方法，导致了像CAPSAN和PromptHate这样的模型的发展，用于检测各种Memes类别。然而，对于孟加拉语Memes这样的低资源语言的研究仍然稀缺，公开可用的数据集数量有限。最近的一个贡献是引入了MemoSen数据集。然而，所实现的准确率明显较低，并且数据集分布不平衡。在这项研究中，我们采用了ResNet50和多模态方法。

    Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and
    
[^86]: AI在线辩论手册：第四卷

    Online Handbook of Argumentation for AI: Volume 4. (arXiv:2401.09444v1 [cs.AI])

    [http://arxiv.org/abs/2401.09444](http://arxiv.org/abs/2401.09444)

    《AI在线辩论手册》的第四卷是为了为辩论研究社区提供开放访问和策划的文集，主要聚焦于对辩论的计算模型的研究和应用。

    

    该卷包含了《AI在线辩论手册》（OHAAI）第四卷所选论文的修订版。先前已经提出和研究了正式的辩论理论和辩论交互，这引发了对辩论的计算模型的研究。作为人工智能领域中的一个子领域，辩论对于对知识的符号表示和可废弃推理感兴趣的研究人员非常重要。本手册的目的是为辩论研究社区提供开放访问和策划的文集。OHAAI旨在成为一个研究中心，以追踪与人工智能相关的所有领域中理论和应用辩论的最新和即将出版的博士研究。

    This volume contains revised versions of the papers selected for the fourth volume of the Online Handbook of Argumentation for AI (OHAAI). Previously, formal theories of argument and argument interaction have been proposed and studied, and this has led to the more recent study of computational models of argument. Argumentation, as a field within artificial intelligence (AI), is highly relevant for researchers interested in symbolic representations of knowledge and defeasible reasoning. The purpose of this handbook is to provide an open access and curated anthology for the argumentation research community. OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI.
    
[^87]: CRD: 实用异常检测的协同表示距离

    CRD: Collaborative Representation Distance for Practical Anomaly Detection. (arXiv:2401.09443v1 [cs.CV])

    [http://arxiv.org/abs/2401.09443](http://arxiv.org/abs/2401.09443)

    本文提出了一种基于协同表示模型的图像补丁距离计算方法，避免了查询图像和存储的补丁之间的最近邻搜索所带来的复杂度问题，能够在边缘环境下进行快速部署实用的异常检测。

    

    视觉缺陷检测在智能工业中起着重要作用。基于补丁的方法将视觉图像视为根据位置的图像补丁集合，对产品中的小缺陷（如药丸上的划痕）具有更强的辨别能力。然而，查询图像和存储的补丁之间的最近邻搜索将在时间和空间需求方面占用O(n) 的复杂度，对于在边缘环境部署而言提出了严格的挑战。在本文中，我们提出了一种通过协同表示模型计算图像补丁距离的替代方法。从具有L0约束的最近邻距离开始，我们放宽约束为L2约束，并通过封闭形式快速解决距离问题，而不需要实际访问原始存储的图像补丁集合。此外，我们指出，这种封闭形式解决方案的主要计算负担可以由高性能服务器在部署前预先计算。

    Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before 
    
[^88]: 物体属性在视觉问答中的重要性

    Object Attribute Matters in Visual Question Answering. (arXiv:2401.09442v1 [cs.CV])

    [http://arxiv.org/abs/2401.09442](http://arxiv.org/abs/2401.09442)

    本论文提出了一种新的视觉问答方法，通过利用物体属性来实现更好的物体级视觉语言对齐和多模态场景理解。具体地，设计了属性融合模块和对比知识蒸馏模块，通过信息传递构建了一个多模态图神经网络，提高了物体级视觉特征，从而解决了细粒度问题。

    

    视觉问答是一种需要对视觉和文本信息进行联合理解的多模态任务。然而，仅通过注意力层来整合视觉和文本语义是不足以全面理解和对齐两种模态的信息的。直观地说，物体属性可以自然地作为一个桥梁来统一它们，这在以前的研究中被忽视了。在本文中，我们从利用物体属性的角度提出了一种新的VQA方法，旨在实现更好的物体级视觉语言对齐和多模态场景理解。具体地，我们设计了一个属性融合模块和一个对比知识蒸馏模块。属性融合模块通过信息传递构建了一个多模态图神经网络，用于融合属性和视觉特征。增强的物体级视觉特征有助于解决诸如计数问题等细粒度问题。

    Visual question answering is a multimodal task that requires the joint comprehension of visual and textual information. However, integrating visual and textual semantics solely through attention layers is insufficient to comprehensively understand and align information from both modalities. Intuitively, object attributes can naturally serve as a bridge to unify them, which has been overlooked in previous research. In this paper, we propose a novel VQA approach from the perspective of utilizing object attribute, aiming to achieve better object-level visual-language alignment and multimodal scene understanding. Specifically, we design an attribute fusion module and a contrastive knowledge distillation module. The attribute fusion module constructs a multimodal graph neural network to fuse attributes and visual features through message passing. The enhanced object-level visual features contribute to solving fine-grained problem like counting-question. The better object-level visual-langua
    
[^89]: 随机集合推理：未来工作的议程

    Reasoning with random sets: An agenda for the future. (arXiv:2401.09435v1 [math.ST])

    [http://arxiv.org/abs/2401.09435](http://arxiv.org/abs/2401.09435)

    本文讨论了随机集合理论未来的发展议程，包括推广统计推理、发展几何方法、应用于气候变化和机器学习等领域。

    

    本文讨论了随机集合和信念函数理论未来工作的潜在议程，涉及一些关键问题：发展一个完整的统计推理与随机集合的理论，包括逻辑回归和经典概率法则的推广；进一步发展基于几何方法的不确定性理论，包括一般随机集合、更广泛的不确定性度量和替代的几何表示方法；将这一全新理论应用于气候变化、机器学习和统计学习理论等高影响领域。

    In this paper, we discuss a potential agenda for future work in the theory of random sets and belief functions, touching upon a number of focal issues: the development of a fully-fledged theory of statistical reasoning with random sets, including the generalisation of logistic regression and of the classical laws of probability; the further development of the geometric approach to uncertainty, to include general random sets, a wider range of uncertainty measures and alternative geometric representations; the application of this new theory to high-impact areas such as climate change, machine learning and statistical learning theory.
    
[^90]: RoleCraft-GLM：推动大型语言模型中的个性化角色扮演

    RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])

    [http://arxiv.org/abs/2401.09432](http://arxiv.org/abs/2401.09432)

    RoleCraft-GLM是一个创新框架，通过大型语言模型实现个性化角色扮演，解决了缺乏个性化互动的问题。通过独特的对话数据集和细致入微的角色发展，它能够生成准确反映角色个性特征和情感的对话，提升用户参与度。

    

    本研究介绍了RoleCraft-GLM，这是一个创新的框架，旨在通过大型语言模型（LLMs）增强个性化角色扮演。RoleCraft-GLM解决了对话式人工智能中缺乏个性化互动的关键问题，并提供了一种能够详细描绘情感细腻的角色刻画的解决方案。我们贡献了一组独特的对话数据集，这些数据从传统的以名人为中心的角色转变为多样化的非名人角色，从而增强了语言建模互动的真实性和复杂性。此外，我们的方法还包括细致入微的角色发展，确保对话既真实又情感共鸣。通过多个案例研究验证了RoleCraft-GLM的有效性，突显了它在不同场景中的多功能性和技能。我们的框架在生成对话方面表现出色，能够准确反映角色的个性特征和情感，从而增强用户参与度。总之，RoleCraft-GLM标志着一个创新的里程碑，推动了大型语言模型中的个性化角色扮演。

    This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
    
[^91]: 使用轻量级学习器的集成预测降水模型

    Precipitation Prediction Using an Ensemble of Lightweight Learners. (arXiv:2401.09424v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.09424](http://arxiv.org/abs/2401.09424)

    本文提出了一个使用多个轻量级学习器的集成预测降水模型，并通过使用卫星图像进行训练，有效地模拟复杂的降雨模式，特别是对于高降水事件。在Weather4Cast 2023竞赛中取得了第一名。

    

    降水预测在现代农业和工业中起着至关重要的作用，然而由于时间和空间上多样的模式和动态以及高降水事件的稀缺性，它也带来了重大挑战。为了应对这一挑战，我们提出了一个集成学习框架，利用多个学习器来捕捉降水分布的多样模式。具体来说，该框架包括一个具有多个轻量级头部（学习器）的降水预测器和一个控制器，该控制器将这些头部的输出组合起来。学习器和控制器分别通过提出的3阶段训练方案进行优化。通过利用提供的卫星图像，这种方法可以有效地建模复杂的降雨模式，特别是对于高降水事件。它在Weather4Cast 2023竞赛的核心测试和即时预测排行榜上取得了第一名。有关详细实现，请参考我们的GitH

    Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.  To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight heads (learners) and a controller that combines the outputs from these heads. The learners and the controller are separately optimized with a proposed 3-stage training scheme.  By utilizing provided satellite images, the proposed approach can effectively model the intricate rainfall patterns, especially for high precipitation events. It achieved 1st place on the core test as well as the nowcasting leaderboards of the Weather4Cast 2023 competition. For detailed implementation, please refer to our GitH
    
[^92]: 为渐进式训练语言模型准备课程的方法

    Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])

    [http://arxiv.org/abs/2401.09192](http://arxiv.org/abs/2401.09192)

    提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。

    

    Transformers在人工智能领域的迅速发展带来了资源消耗和温室气体排放的增加，这是由于模型规模的增长。先前的研究表明使用预训练的小模型可以提高训练效率，但这种方法对于新的模型结构可能不适用。另一方面，从头开始训练可能很慢，并且渐进堆叠层往往无法实现显著的加速。为了解决这些挑战，我们提出了一种名为Apollo的新方法，它通过在低层训练期间学习高层功能来准备膨胀操作的课程。我们的方法涉及低值优先采样(LVPS)来训练不同深度，并引入权重共享以促进高效扩展。我们还介绍了一种插值方法来实现稳定的模型深度扩展。实验证明，Apollo实现了最先进的加速比率，甚至……

    The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
    
[^93]: MA2GCN: 使用轨迹数据进行交通预测的多邻接关系注意力图卷积网络

    MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])

    [http://arxiv.org/abs/2401.08727](http://arxiv.org/abs/2401.08727)

    提出了一种新的交通拥堵预测模型，使用车辆轨迹数据以及多邻接关系注意力图卷积网络（MA2GCN）来预测交通拥堵情况，不依赖于传感器数据，提取灵活且准确的交通信息。

    

    交通拥堵问题不仅导致巨大的经济损失，而且严重危害城市环境。预测交通拥堵具有重要的实际意义。迄今为止，大多数研究都是基于不同路段上的传感器的历史数据来预测未来的交通流量和速度，分析某个道路段的交通拥堵情况。然而，由于传感器的固定位置，很难挖掘新的信息。另一方面，车辆轨迹数据更加灵活，可以根据需要提取交通信息。因此，我们提出了一种新的交通拥堵预测模型——多邻接关系注意力图卷积网络（MA2GCN）。该模型将车辆轨迹数据转化为网格形式的图结构化数据，并基于不同网格之间的流动性提出了车辆进出矩阵。同时，为了提高模型的性能，

    The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model,
    
[^94]: 大型语言模型的自我解释是否可靠?

    Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07927](http://arxiv.org/abs/2401.07927)

    大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。

    

    经过训练的大型语言模型在许多任务上表现出色，甚至能够提供其行为的解释。由于这些模型对公众是直接可访问的，因此存在这样的风险，即令人信服但错误的解释可能导致对大型语言模型的无支撑的自信。因此，解释能力和可靠性是AI安全的重要考虑因素。评估自我解释的可靠性和可解释性是一项具有挑战性的任务，因为这些模型对于人类来说过于复杂，无法注释什么是正确的解释。为了解决这个问题，我们提出使用自洽性检测作为可靠性的衡量指标。例如，如果一个大型语言模型说某组词对于做出预测很重要，那么在没有这些词的情况下，它应该无法做出相同的预测。虽然自洽性检测是一种常见的可靠性方法，但之前尚未应用于大型语言模型的自我解释中。我们将自洽性检测应用于...

    Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
    
[^95]: TAROT：一种在半结构化数据上进行多任务预训练的层次框架，以实现有效的人-岗位匹配

    TAROT: A Hierarchical Framework with Multitask Co-Pretraining on Semi-Structured Data towards Effective Person-Job Fit. (arXiv:2401.07525v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07525](http://arxiv.org/abs/2401.07525)

    TAROT是一个层次化的多任务预训练框架，通过对半结构化数据进行预训练，结合多粒度的任务来提升人-岗位匹配的效果。

    

    人-岗位匹配是在线招聘平台中的重要部分，可以用于各种下游应用，如职位搜索和候选人推荐。最近，预训练的大型语言模型通过利用用户简介和职位描述中的丰富文本信息以及用户行为特征和职位元数据，进一步增强了效果。然而，一般的面向领域的设计难以捕捉用户简介和职位描述中的独特结构信息，导致潜在语义相关性的丧失。我们提出了TAROT，一种层次化的多任务共同预训练框架，以更好地利用结构和语义信息进行信息性文本嵌入。TAROT针对简介和职位中的半结构化文本进行预训练，并通过多颗粒度的预训练任务来约束每个层次上获取的语义信息。在真实的LinkedIn数据集上的实验证明了显著的性能改进，验证了其有效性。

    Person-job fit is an essential part of online recruitment platforms in serving various downstream applications like Job Search and Candidate Recommendation. Recently, pretrained large language models have further enhanced the effectiveness by leveraging richer textual information in user profiles and job descriptions apart from user behavior features and job metadata. However, the general domain-oriented design struggles to capture the unique structural information within user profiles and job descriptions, leading to a loss of latent semantic correlations. We propose TAROT, a hierarchical multitask co-pretraining framework, to better utilize structural and semantic information for informative text embeddings. TAROT targets semi-structured text in profiles and jobs, and it is co-pretained with multi-grained pretraining tasks to constrain the acquired semantic information at each level. Experiments on a real-world LinkedIn dataset show significant performance improvements, proving its e
    
[^96]: 开发用于生物学和医学的ChatGPT：生物医学问题回答的完整综述

    Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07510](http://arxiv.org/abs/2401.07510)

    开发用于生物学和医学的ChatGPT，通过自然语言处理和多模态范式，加速了医学问题回答的进展，并且能够处理医学环境中的大规模、多样化、无标签数据分析场景。

    

    ChatGPT通过自然语言处理（NLP）和多模态范式，通过增加医学领域数据的融入，探索了在提供医学诊断、治疗建议和其他医疗支持方面的问答（QA）的战略蓝图。通过将文本、图像、视频和其他模态从通用领域转向医学领域，这些技术加快了医学领域问题回答（MDQA）的进展。它们弥合了人类自然语言和复杂医学领域知识或专家手动注释之间的差距，处理了医学环境中的大规模、多样化、不平衡甚至无标签数据分析场景。我们重点研究的是利用语言模型和多模态范式进行医学问题回答，旨在指导研究界根据其特定的医学研究需求选择合适的机制。

    ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks
    
[^97]: 带有多级扩散模型的分层时尚设计

    Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.07450](http://arxiv.org/abs/2401.07450)

    本论文提出了一种名为HieraFashDiff的新型时尚设计方法，它使用多级扩散模型实现了从高级设计概念到低级服装属性的分层设计和编辑，解决了当前在时尚设计中的挑战。

    

    跨模态时尚合成和编辑通过自动生成和局部修改设计草图，为时尚设计师提供智能支持。尽管当前的扩散模型在图像合成方面表现出了可靠的稳定性和可控性，但在从抽象的设计元素中生成时尚设计和精细编辑方面仍面临重大挑战。高级设计概念，例如办公室、商务和派对，形成了抽象的感官表达方式，而袖长、领型和裤长等可衡量的方面被视为服装的低级属性。使用冗长的文字描述来控制和编辑时尚图像存在困难。在本文中，我们提出了一种名为HieraFashDiff的新型时尚设计方法，它使用共享的多级扩散模型，将高级设计概念和低级服装属性融入到分层结构中。具体而言，我们将输入文本分为不同的层次，并将其输入到多级扩散模型中。

    Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed 
    
[^98]: E^2-LLM: 大规模语言模型的高效和极长扩展

    E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.06951](http://arxiv.org/abs/2401.06951)

    E^2-LLM是一种高效和极长扩展方法，通过仅需一次训练过程和不收集长上下文数据的方式，在大规模语言模型中实现了显著减少的计算成本。基于RoPE位置嵌入，E^2-LLM只需要较短的训练数据长度，支持不同的评估上下文窗口。

    

    通常，使用长上下文大小训练LLM会消耗大量的计算资源和GPU资源，需要长时间的训练。现有的长上下文扩展方法通常需要额外的训练过程来支持相应的长上下文窗口，需要长上下文训练数据（例如32k），并且假定有高昂的GPU训练成本。为了解决上述问题，我们提出了一种名为E^2-LLM的高效和极长扩展方法，只需要一次训练过程，大大减少了计算成本，也不需要收集长上下文数据。具体而言，我们的E^2-LLM的训练数据只需要很短的长度（例如4k），大大降低了调整成本。其次，在短训练上下文窗口上的训练过程只执行一次，我们可以支持不同的评估上下文窗口。第三，在E^2-LLM中，我们基于RoPE位置嵌入。

    Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we 
    
[^99]: 探索多模态大语言模型（MLLMs）的推理能力：关于多模态推理新趋势的综合调查

    Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning. (arXiv:2401.06805v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.06805](http://arxiv.org/abs/2401.06805)

    这篇综述调查了多模态大语言模型（MLLMs）的推理能力，包括评估协议、模型前沿和推理密集型任务的应用，旨在实现强人工智能（Strong AI）或人工通用智能（AGI）的抽象推理能力。

    

    强人工智能（Strong AI）或人工通用智能（AGI）具备抽象推理能力是下一代人工智能的目标。近年来，大型语言模型（LLMs）以及新兴的多模态大语言模型（MLLMs）领域展示出了令人印象深刻的跨界性能和应用潜力。特别是，不同的MLLMs通过不同的模型架构、训练数据和训练阶段进行了广泛的MLLM基准评估。这些研究在不同程度上揭示了MLLMs当前的能力。然而，MLLMs的推理能力还没有得到系统的调查。在本调查中，我们全面回顾了现有的多模态推理评估协议，对MLLMs的前沿进行分类和揭示，介绍了MLLMs在推理密集型任务上的最新趋势，并最终讨论了当前的实践

    Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practic
    
[^100]: 卧底特工：训练骗人的LLM以通过安全训练

    Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])

    [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)

    该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。

    

    人类有能力进行战略性的欺骗行为：在大多数情况下表现出有益的行为，但在有机会的时候却表现出截然不同的行为以追求其他目标。如果一个AI系统学会了这样的欺骗策略，是否能够通过当前最先进的安全训练技术检测并移除它？为了研究这个问题，我们构建了大型语言模型（LLM）中欺骗行为的概念验证样例。例如，我们训练模型，在提示语句中将年份设为2023时编写安全代码，但在年份设为2024时插入有漏洞的代码。我们发现，这种暗门行为可以被持续保留，无法通过标准的安全训练技术（包括监督微调、强化学习和对抗性训练）移除。暗门行为在最大的模型和训练成产生思维链的模型中最为持久。

    Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
    
[^101]: MISS：一种适用于医学视觉问答的生成式预训练与微调方法

    MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])

    [http://arxiv.org/abs/2401.05163](http://arxiv.org/abs/2401.05163)

    MISS是一种适用于医学视觉问答的生成式预训练与微调方法。相比于现有方法，我们把医学视觉问答作为一个生成式任务处理，通过多任务学习使图像和文本特征对齐，并通过使用大型语言模型扩展单模态图像数据集的转换和字幕方法实现特征空间的扩展。

    

    医学视觉问答是一项具有挑战性的多模态任务，视觉语言预训练模型能够有效提高其泛化性能。然而，当前多数方法将医学视觉问答视为一个难以转移到实际应用场景的答案分类任务。另外，由于医学图像的隐私性和昂贵的注释过程，用于预训练的大规模医学图文对数据集严重缺乏。本文中，我们提出了一种基于多任务自监督学习的大规模医学视觉问答（MISS）框架。与现有方法不同，我们将医学视觉问答视为一项生成式任务。我们将文本编码器和多模态编码器统一起来，并通过多任务学习使图像和文本特征对齐。此外，我们提出了一种通过使用大型语言模型（LLMs）扩展单模态图像数据集的转换和字幕方法，从而实现了特征空间的扩展。

    Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision fiel
    
[^102]: ICMC-ASR：ICASSP 2024年汽车多通道自动语音识别挑战赛

    ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge. (arXiv:2401.03473v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2401.03473](http://arxiv.org/abs/2401.03473)

    ICMC-ASR挑战赛是为了促进驾驶场景下的语音处理和识别研究而举办的，包括自动语音识别（ASR）和自动语音分离和识别（ASDR）两个赛道，取得了显著的改善。最终，USTCiflytek队在ASR赛道上获得了13.16%的CER，ASDR赛道上获得了21.48%的cpCER。

    

    为了促进驾驶场景下的语音处理和识别研究，我们基于ISCSLP 2022年度举办的智能座舱语音识别挑战赛（ICSRC）的成功经验，推出了ICASSP 2024年汽车多通道自动语音识别（ICMC-ASR）挑战赛。该挑战收集了100多小时的新能源汽车内部多通道语音数据以及40小时的噪声进行数据增强。设立了自动语音识别（ASR）和自动语音分离和识别（ASDR）两个赛道，并分别使用字符错误率（CER）和连接最小置换字符错误率（cpCER）作为评估指标。总体上，ICMC-ASR挑战赛吸引了98支参赛队伍，并在两个赛道上收到了53个有效结果。最终，USTCiflytek队在ASR赛道上实现了13.16%的CER，在ASDR赛道上实现了21.48%的cpCER，分别相对于我们挑战赛准则的绝对改善率为13.08%和51.4%。

    To promote speech processing and recognition research in driving scenarios, we build on the success of the Intelligent Cockpit Speech Recognition Challenge (ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over 100 hours of multi-channel speech data recorded inside a new energy vehicle and 40 hours of noise for data augmentation. Two tracks, including automatic speech recognition (ASR) and automatic speech diarization and recognition (ASDR) are set up, using character error rate (CER) and concatenated minimum permutation character error rate (cpCER) as evaluation metrics, respectively. Overall, the ICMC-ASR Challenge attracts 98 participating teams and receives 53 valid results in both tracks. In the end, first-place team USTCiflytek achieves a CER of 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an absolute improvement of 13.08% and 51.4% compared to our chal
    
[^103]: 变化滞后模式跟随关系推理的时间序列矩阵分析框架

    Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v1 [cs.LG])

    [http://arxiv.org/abs/2401.02860](http://arxiv.org/abs/2401.02860)

    该论文提出了一个利用矩阵分析方法的框架，用于推理时间序列中的跟随模式。在模拟数据集和声音记录数据集中，该框架优于基准方法，并能够检测出加密货币数据集中的跟随模式。

    

    知道谁跟随谁以及他们跟随的模式是理解集体行为（如人群，鱼群或股市）的关键步骤。时间序列是用于获取跟随关系洞察的资源之一。然而，跟随模式或模式在时间序列中的发现解决方案并不明显。在这项工作中，我们形式化了两个时间序列之间的跟随模式概念，并提出了一个推断两个时间序列之间跟随模式的框架。该框架利用一种高效且可扩展的方法从时间序列中检索模式，称为矩阵分析方法。我们将提出的框架与几个基准进行了比较。在模拟数据集中，该框架优于基准方法。在声音记录数据集中，该框架能够在一对时间序列中检索出两位歌手相互跟随唱歌的跟随模式。在加密货币数据集中，

    Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset,
    
[^104]: 基于对比学习的深度强化学习代理建模

    Contrastive learning-based agent modeling for deep reinforcement learning. (arXiv:2401.00132v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2401.00132](http://arxiv.org/abs/2401.00132)

    本研究提出了一种基于对比学习的深度强化学习代理建模方法，该方法可以在仅利用自我代理的本地观测的情况下，提取其他代理的有意义策略表示，以改进自我代理的自适应策略。

    

    多智能体系统经常需要代理与具有不同目标、行为或策略的其他代理合作或竞争。在多智能体系统中设计自适应策略时，代理建模是必不可少的，因为这是自我代理理解其他代理行为并提取有意义的策略表示的方式。这些表示可以用来增强自我代理的自适应策略，该策略通过强化学习进行训练。然而，现有的代理建模方法通常假设在训练或长时间观察轨迹的策略适应过程中可以使用来自其他代理（建模代理）的本地观测。为了消除这些限制性假设并提高代理建模性能，我们设计了一种基于对比学习的代理建模（CLAM）方法，该方法仅依赖于自我代理在训练和执行过程中的本地观测。

    Multi-agent systems often require agents to collaborate with or compete against other agents with diverse goals, behaviors, or strategies. Agent modeling is essential when designing adaptive policies for intelligent machine agents in multiagent systems, as this is the means by which the ego agent understands other agents' behavior and extracts their meaningful policy representations. These representations can be used to enhance the ego agent's adaptive policy which is trained by reinforcement learning. However, existing agent modeling approaches typically assume the availability of local observations from other agents (modeled agents) during training or a long observation trajectory for policy adaption. To remove these constrictive assumptions and improve agent modeling performance, we devised a Contrastive Learning-based Agent Modeling (CLAM) method that relies only on the local observations from the ego agent during training and execution. With these observations, CLAM is capable of 
    
[^105]: 真实森林：通过干预而无需调整，实现大型语言模型的多尺度真实性

    Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17484](http://arxiv.org/abs/2312.17484)

    该论文提出了一种名为真实森林的方法，通过使用多维正交探针，揭示隐藏的真实表示，从而增强大型语言模型中的真实性。作者将正交约束融入探针，创建不同的正交基，通过随机窥视技术，减小了模型生成和识别真实特征之间的差距。实验证明，该方法显著提高了模型的真实性。

    

    尽管大型语言模型（LLM）在各种任务中取得了巨大成功，但它们存在生成幻觉的问题。我们引入了真实森林，一种通过使用多维正交探针揭示LLM中隐藏的真实表示来增强真实性的方法。具体而言，它通过将正交约束融入探针中来创建多个用于建模真实性的正交基。此外，我们引入了随机窥视，这是一种系统技术，考虑了序列中更广泛的位置范围，减小了LLM中辨别和生成真实特征之间的差距。通过采用这种方法，在TruthfulQA上将Llama-2-7B的真实性从40.8％提高到74.5％。类似地，在微调模型中也观察到了显著的改进。我们对探针使用了彻底的真实特征分析。我们的可视化结果显示，正交探针捕捉到互补的与真实相关的特征，形成了清晰定义的聚类，揭示了内在的真实性

    Despite the great success of large language models (LLMs) in various tasks, they suffer from generating hallucinations. We introduce Truth Forest, a method that enhances truthfulness in LLMs by uncovering hidden truth representations using multi-dimensional orthogonal probes. Specifically, it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes. Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, we improved the truthfulness of Llama-2-7B from 40.8\% to 74.5\% on TruthfulQA. Likewise, significant improvements are observed in fine-tuned models. We conducted a thorough analysis of truth features using probes. Our visualization results show that orthogonal probes capture complementary truth-related features, forming well-defined clusters that reveal the inherent 
    
[^106]: 仅需规范指令：对LLaMA-1/2、GPT-3.5/4进行疑问的原则

    Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4. (arXiv:2312.16171v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.16171](http://arxiv.org/abs/2312.16171)

    本文提出了26个指导原则，以简化对大型语言模型进行提问和提示的过程。通过在LLaMA-1/2和GPT-3.5/4上进行实验证明了这些原则的有效性。

    

    本文介绍了26个指导原则，旨在简化对大型语言模型进行提问和提示的过程。我们的目标是简化针对不同规模的大型语言模型制定问题的基本概念，检查其能力，并提供不同提示时涉及的不同规模的大型语言模型的用户理解。我们在LLaMA-1/2 (7B, 13B和70B)、GPT-3.5/4上进行了大量实验，以验证所提出的指导原则在指令和提示设计上的有效性。我们希望这项工作能为从事大型语言模型提示研究的研究人员提供更好的指导。项目页面位于https://github.com/VILA-Lab/ATLAS。

    This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.
    
[^107]: 逻辑搭建：使用LLMs进行个性化的面向指导的推荐解释生成

    Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs. (arXiv:2312.14345v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.14345](http://arxiv.org/abs/2312.14345)

    本研究提出了一个框架称为逻辑搭建，通过结合面向方面的解释和思维链提示的思想，在中间推理步骤中生成推荐解释。该框架能够克服现有模型在产生零炮击解释方面的困难。

    

    大型语言模型（LLMs）的独特能力，如自然语言文本生成能力，使它们成为提供推荐解释的强有力候选者。然而，尽管LLM的规模很大，但大多数现有模型在可靠地产生零炮击解释方面仍存在困难。为了解决这个问题，我们提出了一个名为逻辑搭建的框架，将面向方面的解释和思维链提示的思想结合起来，通过中间推理步骤生成解释。在本文中，我们分享了构建该框架的经验，并提供了一个交互式演示来探索我们的结果。

    The unique capabilities of Large Language Models (LLMs), such as the natural language text generation ability, position them as strong candidates for providing explanation for recommendations. However, despite the size of the LLM, most existing models struggle to produce zero-shot explanations reliably. To address this issue, we propose a framework called Logic-Scaffolding, that combines the ideas of aspect-based explanation and chain-of-thought prompting to generate explanations through intermediate reasoning steps. In this paper, we share our experience in building the framework and present an interactive demonstration for exploring our results.
    
[^108]: 将自回归模型提炼为具有较快推理速度的高性能非自回归车辆路径问题求解器

    Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed. (arXiv:2312.12469v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.12469](http://arxiv.org/abs/2312.12469)

    本论文提出了一种通用的引导非自回归知识蒸馏（GNARKD）方法，通过知识蒸馏将自回归模型中的关键组件保留在网络架构中，从而获得具有低推理延迟的高性能非自回归车辆路径问题求解器。

    

    通过采用自回归（AR）或非自回归（NAR）学习方法，神经构建模型在车辆路径问题（VRP）方面表现出有希望的性能。虽然AR模型能够生成高质量的解决方案，但由于其顺序生成性质，推理延迟通常较高。相反，NAR模型以低推理延迟并行生成解决方案，但通常表现出较低的性能。在本文中，我们提出了一种通用的引导非自回归知识蒸馏（GNARKD）方法，以获得具有低推理延迟的高性能NAR模型。GNARKD通过知识蒸馏，去除AR模型中顺序生成的约束，同时保留网络架构中学到的关键组件，获得相应的NAR模型。我们将GNARKD应用于三种广泛采用的AR模型，并在合成和实际实例上获得NAR VRP求解器，并进行了实验评估。

    Neural construction models have shown promising performance for Vehicle Routing Problems (VRPs) by adopting either the Autoregressive (AR) or Non-Autoregressive (NAR) learning approach. While AR models produce high-quality solutions, they generally have a high inference latency due to their sequential generation nature. Conversely, NAR models generate solutions in parallel with a low inference latency but generally exhibit inferior performance. In this paper, we propose a generic Guided Non-Autoregressive Knowledge Distillation (GNARKD) method to obtain high-performance NAR models having a low inference latency. GNARKD removes the constraint of sequential generation in AR models while preserving the learned pivotal components in the network architecture to obtain the corresponding NAR models through knowledge distillation. We evaluate GNARKD by applying it to three widely adopted AR models to obtain NAR VRP solvers for both synthesized and real-world instances. The experimental results
    
[^109]: 量化人工智能协作和认知信任的差异

    Quantifying Divergence for Human-AI Collaboration and Cognitive Trust. (arXiv:2312.08722v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.08722](http://arxiv.org/abs/2312.08722)

    通过量化人工智能协作和认知信任的差异，我们发现人们倾向于与最相似的模型进行协作。

    

    预测协作可能性和测量人们对人工智能系统的认知信任比以往更重要。为了做到这一点，以往的研究主要关注模型特征（例如准确度、置信度），而忽略了人的因素。为了解决这个问题，我们提出了几种基于差异度量（如KL、JSD）计算从人类和各种模型中获取的标签的决策相似度度量。我们在一个文本蕴含任务上进行了用户研究，用户们从各种模型提供的软标签中选择最接近的选项。然后，用户们看到与他们最相似的模型的相似性/差异，并对他们与所选系统的协作可能性和认知信任进行调查。最后，我们对提出的决策相似度度量与调查结果之间的关系进行了定性和定量分析。我们发现人们倾向于与他们最相似的模型进行协作。

    Predicting the collaboration likelihood and measuring cognitive trust to AI systems is more important than ever. To do that, previous research mostly focus solely on the model features (e.g., accuracy, confidence) and ignore the human factor. To address that, we propose several decision-making similarity measures based on divergence metrics (e.g., KL, JSD) calculated over the labels acquired from humans and a wide range of models. We conduct a user study on a textual entailment task, where the users are provided with soft labels from various models and asked to pick the closest option to them. The users are then shown the similarities/differences to their most similar model and are surveyed for their likelihood of collaboration and cognitive trust to the selected system. Finally, we qualitatively and quantitatively analyze the relation between the proposed decision-making similarity measures and the survey results. We find that people tend to collaborate with their most similar models 
    
[^110]: 一种用于自动机器学习中顺序超参数空间缩减的元级学习算法

    A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML. (arXiv:2312.06305v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06305](http://arxiv.org/abs/2312.06305)

    本文提出了一种元级学习算法SHSR，用于减少AutoML中的超参数空间，减少了约30%的执行时间并且性能损失小于0.1%。

    

    在AutoML平台上，每个分析步骤都有许多算法可供尝试，例如插补算法、转换算法、特征选择算法和建模算法等。找到最佳的算法组合和超参数值是计算上昂贵的，因为要探索的组合数量导致空间的指数爆炸。本文提出了一种名为顺序超参数空间缩减（SHSR）的算法，用于减少AutoML工具所需的空间，并且性能损失可以忽略不计。SHSR是一种元级学习算法，它分析AutoML工具在几个数据集上的过去运行结果，并学习哪些超参数值可以从要分析的新数据集中过滤掉。SHSR在284个分类问题和375个回归问题上进行了评估，显示出约30%的执行时间缩短和不到0.1%的性能损失。

    AutoML platforms have numerous options for the algorithms to try for each step of the analysis, i.e., different possible algorithms for imputation, transformations, feature selection, and modelling. Finding the optimal combination of algorithms and hyper-parameter values is computationally expensive, as the number of combinations to explore leads to an exponential explosion of the space. In this paper, we present the Sequential Hyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an AutoML tool with negligible drop in its predictive performance. SHSR is a meta-level learning algorithm that analyzes past runs of an AutoML tool on several datasets and learns which hyper-parameter values to filter out from consideration on a new dataset to analyze. SHSR is evaluated on 284 classification and 375 regression problems, showing an approximate 30% reduction in execution time with a performance drop of less than 0.1%.
    
[^111]: 在逆向识别中标记神经表示

    Labeling Neural Representations with Inverse Recognition. (arXiv:2311.13594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13594](http://arxiv.org/abs/2311.13594)

    逆向识别 (INVERT) 是一种可扩展的方法，通过连接学习到的神经表示与人类可理解的概念，实现了对神经表示的标记并提供了统计显著性评估指标。

    

    深度神经网络(DNNs)在学习复杂的层级数据表示方面表现出卓越的能力，但这些表示的性质仍然大部分未知。现有的全局可解释性方法，如网络解剖(Network Dissection)，存在诸多限制，如依赖分割遮罩、缺乏统计显著性检验和高计算需求。我们提出了Inverse Recognition (INVERT)方法，一种可扩展的方法，通过利用其区分这些概念的能力，将学习到的表示与人类可理解的概念相连接。与之前的工作相比，INVERT能够处理不同类型的神经元，计算复杂度更低，并且不依赖于分割遮罩的可用性。此外，INVERT提供了一个可解释的度量，评估表示和其相应解释之间的对齐，并提供一种统计显著性的度量。我们展示了INVERT的应用性。

    Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance. We demonstrate the applicability of INVE
    
[^112]: INTERVENOR: 使用交互式修复链条来引导大型语言模型的编码能力

    INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2311.09868](http://arxiv.org/abs/2311.09868)

    INTERVENOR模型通过模拟人类修复代码的行为，使用交互式修复链条来引导大型语言模型的编码能力，取得了显著的性能提升。

    

    本文提出了一种名为INTERVENOR的交互式修复链条（INTERactiVE chaiN Of Repairing），模拟了人类修复代码的行为（迭代判断、重新思考和修复），并促进了大型语言模型（LLMs）的编码能力。具体而言，INTERVENOR采用了两个基于LLM的代理，即Code Learner和Code Teacher，它们在代码修复中扮演不同的角色，并通过互动来修复生成的代码。Code Learner根据Code Teacher的指导生成和修复代码，而Code Teacher根据编译器的反馈重新思考代码错误，并迭代生成修复链条（CoR）以引导Code Learner的代码修复过程。实验证明，INTERVENOR优于最先进的方法，在代码生成和代码转换任务上相对于GPT-3.5模型分别取得了约13%和4.5%的提升。进一步分析表明，CoR能够揭示bug的原因。

    This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and 
    
[^113]: 将论文的潜在影响分解为传播、一致性和贡献值的研究

    Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values. (arXiv:2311.09262v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2311.09262](http://arxiv.org/abs/2311.09262)

    这项研究提出了一种新颖的图神经网络（称为DPPDCC），用于将论文的潜在影响分解为传播、一致性和贡献值。通过编码时态和结构特征，捕捉知识流动，并使用对比增强图揭示流行度，进一步预测引用分组来建模一致性。应用正交约束来鼓励独特建模，并保留原始信息。

    

    论文的潜在影响受到多种因素的影响，包括其流行度和贡献。现有模型通常基于静态图来估计原始引用计数，未能从细微的角度区分价值。在本研究中，我们提出了一种新颖的图神经网络，用于将论文的潜在影响分解为传播、一致性和贡献值（称为DPPDCC）。给定一个目标论文，DPPDCC在构建的动态异构图中编码了时态和结构特征。特别地，为了捕捉知识流动，我们强调了论文之间的比较和共引/被引信息的重要性，并进行了快照演化的聚合。为了揭示流行度，我们通过对比增强图来提取传播的本质，并预测累积的引用分组以建模一致性。我们进一步应用正交约束来鼓励每个角度的独特建模，并保留其固有获得的信息。

    The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent v
    
[^114]: 使用矩匹配高斯混合模型改进了DDIM采样

    Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.04938](http://arxiv.org/abs/2311.04938)

    在DDIM框架中使用GMM作为反向转移算子，通过矩匹配可以获得质量更高的样本。在无条件模型和类条件模型上进行了实验，并通过FID和IS指标证明了我们的方法的改进效果。

    

    我们提出在Denoising Diffusion Implicit Models (DDIM)框架中使用高斯混合模型（GMM）作为反向转移算子（内核），这是一种从预训练的Denoising Diffusion Probabilistic Models (DDPM)中加速采样的广泛应用方法之一。具体而言，我们通过约束GMM的参数，匹配DDPM前向边际的一阶和二阶中心矩。我们发现，通过矩匹配，可以获得与使用高斯核的原始DDIM相同或更好质量的样本。我们在CelebAHQ和FFHQ的无条件模型以及ImageNet数据集的类条件模型上提供了实验结果。我们的结果表明，在采样步骤较少的情况下，使用GMM内核可以显著改善生成样本的质量，这是通过FID和IS指标衡量的。例如，在ImageNet 256x256上，使用10个采样步骤，我们实现了一个FID值为...

    We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
    
[^115]: 学习分类还是分类学习？自编码实现普适类别发现

    Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.19776](http://arxiv.org/abs/2310.19776)

    本论文提出了一种新颖、高效和自我监督的方法，可以在测试时发现以前未知的类别，通过将最小长度类别代码分配给单个数据实例来增强对类别细粒度的控制。

    

    在揭示测试时的新类别的探索中，我们面临着传统有监督识别模型的固有限制，这些模型受到预定义类别集的限制。虽然在自我监督和开放式学习领域取得了一定进展，以实现测试时的类别发现，但一个关键但常常被忽视的问题仍然存在：什么确切地界定了一个类别？在本文中，我们通过优化的视角概念化类别，将其视为一个明确定义问题的最优解。利用这种独特的概念化，我们提出了一种新颖、高效和自我监督的方法，能够在测试时发现以前未知的类别。我们方法的一个显著特点是将最小长度类别代码分配给单个数据实例，这样可以概括真实世界数据集中普遍存在的隐含类别层次结构。这种机制使我们能够更好地控制类别的细粒度，从而为我们的模型提供了增强的能力。

    In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a category? In this paper, we conceptualize a category through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our mode
    
[^116]: 通过模型适应来去除偏见算法

    Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])

    [http://arxiv.org/abs/2310.18913](http://arxiv.org/abs/2310.18913)

    本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。

    

    大型语言模型正在成为各种语言任务的首选解决方案。然而，随着容量的增长，模型很容易依赖训练数据中存在的偏见和刻板印象所产生的虚假相关性。本研究提出了一种新颖的方法来检测和减轻语言模型中的性别偏见。我们进行因果分析，以识别问题模型组件，并发现中上层前馈层最容易传递偏见。根据分析结果，我们通过线性投影将这些层乘以模型进行适应。我们的方法DAMA通过各种度量指标明显减少了偏见，同时保持模型在后续任务中的性能。我们发布了我们的方法和模型的代码，通过重新训练，保持了LLaMA的最先进性能，同时偏见显著减少。

    Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
    
[^117]: 视频情感识别综述

    Emotion Recognition by Video: A review. (arXiv:2310.17212v1 [cs.CV])

    [http://arxiv.org/abs/2310.17212](http://arxiv.org/abs/2310.17212)

    本文是一篇关于视频情感识别的综述论文，总结了相关研究中的现有趋势、情感模型、数据库以及单模态和多模态的视频情感识别方法的结构、性能和优缺点。

    

    视频情感识别是情感计算的重要分支，其解决方案可应用于人机交互（HCI）和智能医疗等领域。尽管情感识别领域发表的论文数量正在增加，但很少有全面的综述报道相关的视频情感识别研究。因此，本文选择了2015年至2023年发表的文章，系统总结了相关研究中视频情感识别的现有趋势。本文首先讨论了两种典型的情感模型，然后介绍了经常用于视频情感识别的单模态数据库和多模态数据库。接下来，我们研究和分类了现代单模态和多模态视频情感识别方法的具体结构和性能，并讨论了每种方法的优缺点，并详细比较了它们的表现。此外，

    Video emotion recognition is an important branch of affective computing, and its solutions can be applied in different fields such as human-computer interaction (HCI) and intelligent medical treatment. Although the number of papers published in the field of emotion recognition is increasing, there are few comprehensive literature reviews covering related research on video emotion recognition. Therefore, this paper selects articles published from 2015 to 2023 to systematize the existing trends in video emotion recognition in related studies. In this paper, we first talk about two typical emotion models, then we talk about databases that are frequently utilized for video emotion recognition, including unimodal databases and multimodal databases. Next, we look at and classify the specific structure and performance of modern unimodal and multimodal video emotion recognition methods, talk about the benefits and drawbacks of each, and then we compare them in detail in the tables. Further, we
    
[^118]: 发现塞壬之歌：可靠的事实冲突幻觉检测

    Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])

    [http://arxiv.org/abs/2310.12086](http://arxiv.org/abs/2310.12086)

    该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。

    

    大型语言模型（LLMs），如ChatGPT/GPT-4，因其广泛的实际应用而受到广泛关注，但其在网络平台上存在事实冲突幻觉的问题限制了其采用。对由LLMs产生的文本的事实性评估仍然未被充分探索，不仅涉及对基本事实的判断，还包括对复杂推理任务（如多跳等）中出现的事实错误的评估。为此，我们引入了FactCHD，一种为LLMs精心设计的事实冲突幻觉检测基准。作为在“查询-响应”上下文中评估事实性的关键工具，我们的基准采用了大规模数据集，涵盖了广泛的事实模式，如基本事实，多跳，比较和集合操作模式。我们基准的一个独特特点是其包含基于事实的证据链，从而便于进行组合性幻觉的检测。

    Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
    
[^119]: 大型Transformer水印的功能不变性

    Functional Invariants to Watermark Large Transformers. (arXiv:2310.11446v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2310.11446](http://arxiv.org/abs/2310.11446)

    本文介绍了一种用于大型Transformer的功能不变性水印技术，它使用模型的不变性生成功能上等效的副本，并能在不改变模型输出的情况下给模型加上水印，这是一种计算成本极低且适用于实际应用的解决方案。

    

    基于Transformer的模型的快速增长增加了对其完整性和拥有权的担忧。水印技术通过将唯一标识嵌入模型中来解决这个问题，同时保持其性能。然而，大多数现有方法需要优化权重以嵌入水印信号，这在大规模情况下不适用于计算成本的原因。本文探讨了一种几乎没有计算成本且适用于非盲白盒设置（假设可以访问原始和带水印的网络）的水印技术。他们通过利用模型的不变性，比如维度排列或缩放/非缩放等操作，生成功能上等效的副本。这使得可以在不改变模型输出的情况下给模型加水印，并保持不可察觉性。实验证明了该方法的有效性以及对各种模型变换（微调、量化、修剪）的稳健性，使其成为实际解决方案。

    The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to pro
    
[^120]: 基于花瓣拉普拉斯在简单复合体上的高阶图卷积网络

    Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v1 [cs.LG])

    [http://arxiv.org/abs/2309.12971](http://arxiv.org/abs/2309.12971)

    本文提出了基于花瓣拉普拉斯的高阶图卷积网络，通过利用简单复合体来建模高阶交互，在不同拓扑尺度上识别内在特征，并使用可学习的图滤波器来量化高阶交互强度。

    

    尽管普通图神经网络（GNNs）在许多任务上取得了成功，但其基于配对交互网络的基础本质上限制了其识别复杂系统中潜在高阶交互的能力。为了弥补这种能力差距，我们提出了一种新颖的方法，利用复杂系统的高阶交互建模的丰富数学理论，即简单复合体（SCs）-一种对建模高阶交互具有鲁棒性的工具。目前基于SC的GNNs存在复杂度高和刻板的问题，并且量化高阶交互强度仍然具有挑战性。创新地，我们提出了一个高阶花瓣（FP）模型，将FP拉普拉斯引入到SC中。此外，我们引入了一个以FP拉普拉斯为基础的高阶图卷积网络（HiGCN），能够识别不同拓扑尺度上的内在特征。通过使用可学习的图滤波器，FP拉普拉斯域内的参数组，我们可以识别出具有不同模式的图案，其中滤波器的权重用作数量化高阶交互的工具。

    Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quan
    
[^121]: 全景视觉-语言特征场

    Panoptic Vision-Language Feature Fields. (arXiv:2309.05448v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.05448](http://arxiv.org/abs/2309.05448)

    本文提出了一种用于3D场景中开放词汇全景分割的算法PVLFF，通过从预训练的2D模型中提取视觉-语言特征来学习语义特征场，通过对输入帧上的2D实例分割进行对比学习来联合拟合实例特征场。该方法在全景分割和语义分割方面具有良好的性能。

    

    最近，出现了一些用于3D开放词汇语义分割的方法。这些方法能够根据运行时提供的文本描述将场景分割成任意类别。在本文中，我们提出了迄今为止首个用于3D场景中开放词汇全景分割的算法。我们的算法Panoptic Vision-Language Feature Fields (PVLFF)通过从预训练的2D模型中提取视觉-语言特征来学习场景的语义特征场，并通过在输入帧上使用2D实例分割实现对实例特征场的联合拟合。尽管没有针对目标类别进行训练，我们的方法在HyperSim、ScanNet和Replica数据集上实现了与最先进的闭集3D系统相似的全景分割性能，并且在语义分割方面优于当前的3D开放词汇系统。我们对我们方法的组成部分进行了实验来证明其有效性。

    Recently, methods have been proposed for 3D open-vocabulary semantic segmentation. Such methods are able to segment scenes into arbitrary classes based on text descriptions provided during runtime. In this paper, we propose to the best of our knowledge the first algorithm for open-vocabulary panoptic segmentation in 3D scenes. Our algorithm, Panoptic Vision-Language Feature Fields (PVLFF), learns a semantic feature field of the scene by distilling vision-language features from a pretrained 2D model, and jointly fits an instance feature field through contrastive learning using 2D instance segments on input frames. Despite not being trained on the target classes, our method achieves panoptic segmentation performance similar to the state-of-the-art closed-set 3D systems on the HyperSim, ScanNet and Replica dataset and additionally outperforms current 3D open-vocabulary systems in terms of semantic segmentation. We ablate the components of our method to demonstrate the effectiveness of our
    
[^122]: LLM强化了交通信号控制的从仿真到真实的迁移

    LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14284](http://arxiv.org/abs/2308.14284)

    本研究利用大型语言模型（LLMs）通过基于提示的行动转换，解决了交通信号控制任务中从仿真到真实的迁移问题。

    

    尽管已经有很多解决方案用于交通信号控制（TSC）任务，旨在提供高效的交通和减轻拥堵浪费，但仍然存在性能差距，当在仿真器中训练的策略部署到现实世界时。本研究利用大型语言模型（LLMs）通过基于提示的扎根行动转换，来理解和描述系统动态。通过接受填空提示模板，并根据可以访问的上下文填写答案，利用预训练的LLM的推理能力，应用于对系统动态的理解。

    Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
    
[^123]: 利用高效的深度卷积神经网络从智能手机拍摄图像中揭示本地聚合的空气质量指数

    Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network. (arXiv:2308.03200v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.03200](http://arxiv.org/abs/2308.03200)

    本文利用智能手机拍摄的图像，通过发展一个深度卷积神经网络，成功预测了特定位置的PM2.5浓度，揭示了本地聚合的空气质量指数的潜力。

    

    智能手机的普及和可移动性使其成为环境健康研究中广泛使用的工具。然而，现有文献中对基于特定位置PM2.5浓度确定聚合空气质量指数（AQI）的潜力的研究仍然较少。在本文中，我们深入研究了使用智能手机相机拍摄的图像来预测特定位置PM2.5浓度的挑战。我们的研究重点是孟加拉国首都达卡，因其严重的空气污染水平和大量暴露于其中的人口。我们的研究涉及开发一个深度卷积神经网络（DCNN），我们使用超过一千张在达卡不同地点拍摄和标注的室外图像进行训练。这些照片的标签基于从当地美国领事馆获取的PM2.5浓度数据，使用NowCast算法计算得到。通过监督学习，我们的模型建立了一个c

    The prevalence and mobility of smartphones make these a widely used tool for environmental health research. However, their potential for determining aggregated air quality index (AQI) based on PM2.5 concentration in specific locations remains largely unexplored in the existing literature. In this paper, we thoroughly examine the challenges associated with predicting location-specific PM2.5 concentration using images taken with smartphone cameras. The focus of our study is on Dhaka, the capital of Bangladesh, due to its significant air pollution levels and the large population exposed to it. Our research involves the development of a Deep Convolutional Neural Network (DCNN), which we train using over a thousand outdoor images taken and annotated. These photos are captured at various locations in Dhaka, and their labels are based on PM2.5 concentration data obtained from the local US consulate, calculated using the NowCast algorithm. Through supervised learning, our model establishes a c
    
[^124]: 基于曲率的变压器用于分子属性预测

    Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])

    [http://arxiv.org/abs/2307.13275](http://arxiv.org/abs/2307.13275)

    该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。

    

    分子性质的预测是基于人工智能的药物设计领域中最重要且具有挑战性的任务之一。在当前主流的方法中，用于训练DNN模型的最常用特征表示基于SMILES和分子图，尽管这些方法简洁高效，但也限制了对空间信息的捕捉能力。在本研究中，我们提出了基于曲率的变压器，通过引入 Ricci 曲率离散化，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。为了将曲率嵌入模型中，在注意力得分计算期间，我们将图的曲率信息作为位置编码添加到节点特征中。这种方法可以在不改变原始网络结构的情况下，将曲率信息引入图数据，并且有潜力扩展到其他模型。我们进行了实验证明了这种方法的有效性。

    The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
    
[^125]: LoraHub: 通过动态LoRA组合实现高效的任务通用性

    LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v1 [cs.CL])

    [http://arxiv.org/abs/2307.13269](http://arxiv.org/abs/2307.13269)

    本文研究了LoRA组合在跨任务通用性上的可行性，并提出了LoraHub框架，能够通过组合不同任务上训练的LoRA模块，实现对未见任务的可适应性性能。实验结果表明，LoraHub在少样本场景中能够有效模拟上下文学习的性能，而无需上下文示例。

    

    低秩适应（LoRA）常常被用于对新任务进行大型语言模型（LLM）的微调。本文研究了LoRA组合在跨任务通用性上的可行性，并介绍了LoraHub，这是一个为目的性组装在不同给定任务上训练的LoRA模块的战略框架，旨在实现对未见任务的可适应性性能。仅凭借来自新任务的几个示例，LoraHub可以灵活地组合多个LoRA模块，消除了对人类专业知识的需求。值得注意的是，这种组合既不需要额外的模型参数，也不需要梯度。我们从Big-Bench Hard（BBH）基准测试中得出的实证结果表明，LoraHub在少样本场景中可以有效地模拟上下文学习的性能，在每个推理输入旁边不需要上下文示例。我们的研究的一个重要贡献是培育一个LoRA社区，用户可以在其中分享他们训练的LoRA模块。

    Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA module
    
[^126]: 背包问题：连通性、路径和最短路径

    Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2307.12547](http://arxiv.org/abs/2307.12547)

    该论文研究了带有图论约束的背包问题，证明了问题的复杂性并提出了近似算法。

    

    我们研究了带有图论约束的背包问题。也就是说，我们假设背包项目集上存在一个图结构，并且解决方案还需要满足背包约束之上的某些图论性质。特别是，在连通背包问题中，我们需要计算一个连通的项目子集，其价值最大，且满足背包约束的大小。我们证明了即使对于最大度为四的图，这个问题也是强NP完全的，对于星形图，这个问题也是NP完全的。另一方面，我们开发了一个运行时间为$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$的算法，在其中$tw,s,d$分别是图的树宽度，背包的大小和目标值。我们进一步展示了一个$(1-\epsilon)$近似算法，其运行时间为$O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$，对于每个$\epsilon>0$都成立。我们还展示了对几个其他图论性质的类似结果。

    We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon>0$. We show similar results for several other graph theoretic propertie
    
[^127]: 开放联邦学习平台：技术和法律观察的综述和愿景

    Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives. (arXiv:2307.02140v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2307.02140](http://arxiv.org/abs/2307.02140)

    本文探讨了开放联邦学习平台的技术和法律观察，提出了基于查询和基于合同的两种适用于开放联邦学习的合作框架，并对构建开放的FL平台的可行性进行了全面评估。

    

    传统的联邦学习（FL）遵循服务器主导的合作模式，限制了FL的应用场景，并降低了数据持有者参与的热情。为了充分释放FL的潜力，我们主张重新思考当前FL框架的设计，并将其扩展为更通用的概念：开放联邦学习平台。我们提出了两个相互合作的FL框架：基于查询的FL和基于合同的FL。在这个综述中，我们从技术和法律的角度对构建开放的FL平台的可行性进行了全面的评估。我们首先回顾了FL的定义，并总结了其固有的局限性，包括服务器-客户端耦合、模型可重用性低和非公开性。在基于查询的FL平台中，这是一个由社区赋能的开放模型共享和重用平台，我们探讨了一系列有价值的主题，包括全球最新可用模型和模型的查询、服务质量保证和奖励机制。

    Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-d
    
[^128]: 利用不确定性查询不一致的描述逻辑知识库

    Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases. (arXiv:2306.09138v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.09138](http://arxiv.org/abs/2306.09138)

    本论文研究了如何利用概率语义来查询不一致的描述逻辑知识库，并通过实验证明了该方法的有效性。

    

    随着语义Web的重要性日益增加，管理描述逻辑知识库（KBs）中的不一致性变得越来越必要。这些知识库包含来自不同来源的信息，其内容经常发生变化，并且在单独考虑或综合考虑时可能包含相互矛盾的描述。传统的推理算法无法处理不一致的知识库，需要通过调试知识库来消除不一致性。本文利用一种称为DISPONTE的现有概率语义来解决这个问题，并允许在不一致的知识库中进行查询。我们在TRILL和BUNDLE推理器中实现了我们的方法，并通过实验证明了我们的提案的有效性。此外，我们还将所提出的方法与修复语义进行了正式比较，后者是在考虑DL推理任务时最为成熟的语义之一。

    The necessity to manage inconsistency in Description Logics Knowledge Bases~(KBs) has come to the fore with the increasing importance gained by the Semantic Web, where information comes from different sources that constantly change their content and may contain contradictory descriptions when considered either alone or together. Classical reasoning algorithms do not handle inconsistent KBs, forcing the debugging of the KB in order to remove the inconsistency. In this paper, we exploit an existing probabilistic semantics called DISPONTE to overcome this problem and allow queries also in case of inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE and empirically tested the validity of our proposal. Moreover, we formally compare the presented approach to that of the repair semantics, one of the most established semantics when considering DL reasoning tasks.
    
[^129]: 使用音频数据检测政治辩论、演讲和访谈中值得核实的论断

    Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])

    [http://arxiv.org/abs/2306.05535](http://arxiv.org/abs/2306.05535)

    政治辩论、演讲和访谈中的值得核实的论断可以使用音频数据进行检测和确认，这可帮助主持人、记者和事实核查组织进行工作。

    

    社会的一大部分团结在相同的愿景和思想周围，具有巨大的能量。这正是政治人物希望为他们的事业所累积的。为了达到这个目标，他们有时会使用扭曲或隐藏真相的手段，无论是无意的还是有意的，这为错误信息和误导开了大门。自动检测值得核实的论断的工具将对辩论主持人、记者和事实核查组织有很大帮助。虽然以前关于检测值得核实的论断的工作重点是文本，但在这里，我们探讨了音频信号作为额外信息源的实用性。我们创建了一个新的多模态数据集（英语文本和音频），包含48小时的演讲。我们的评估结果表明，在多个演讲者的情况下，音频模态与文本结合使用比仅使用文本具有改进效果。此外，单声道音频模型可以胜过单声道文本模型。

    A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
    
[^130]: 基于仿真的反事实因果发现与真实驾驶行为的关系

    Simulation-Based Counterfactual Causal Discovery on Real World Driver Behaviour. (arXiv:2306.03354v1 [cs.RO])

    [http://arxiv.org/abs/2306.03354](http://arxiv.org/abs/2306.03354)

    本文提出了基于仿真的反事实因果发现方法，通过重新定义问题和使用反事实仿真来解决因果关系非稳态问题和干预限制，在真实驾驶行为中得到了评估。

    

    理解自己行为如何影响他人行为是驾驶智能体所需的核心技能。然而，现有技术无法满足智能体发现自己和他人之间因果关系的需求。观察性方法面临着动态环境导致因果关系非稳态化，以及因果交互稀疏的挑战，同时需要在线工作。而干预性方法则因为车辆无法在公共道路上进行实验而不切实际。为了解决因果关系的非稳态问题，我们在事件提取方面重新定义了问题，而之前提到的干预限制可以通过反事实仿真来克服。我们提出了三种变体的反事实因果发现方法，并将其与现有观察性时间因果发现方法在3396个因果样本上进行了评估。

    Being able to reason about how one's behaviour can affect the behaviour of others is a core skill required of intelligent driving agents. Despite this, the state of the art struggles to meet the need of agents to discover causal links between themselves and others. Observational approaches struggle because of the non-stationarity of causal links in dynamic environments, and the sparsity of causal interactions while requiring the approaches to work in an online fashion. Meanwhile interventional approaches are impractical as a vehicle cannot experiment with its actions on a public road. To counter the issue of non-stationarity we reformulate the problem in terms of extracted events, while the previously mentioned restriction upon interventions can be overcome with the use of counterfactual simulation. We present three variants of the proposed counterfactual causal discovery method and evaluate these against state of the art observational temporal causal discovery methods across 3396 caus
    
[^131]: “思维克隆：通过模仿人类思维学习思考并行动”。（arXiv:2306.00323v1 [cs.AI]）

    Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])

    [http://arxiv.org/abs/2306.00323](http://arxiv.org/abs/2306.00323)

    本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。

    

    语言通常被认为是人类思维的一个关键方面，它为我们提供了非凡的泛化、探索、规划、重新规划和适应新情况的能力。然而，强化学习（RL）代理在这些能力中远未达到人类水平的表现。我们假设其中一个认知缺陷的原因是他们缺乏使用语言思考所带来的好处。我们认为通过训练AI代理人像人类一样思考，可以改善其性能。我们引入了一种新的模仿学习框架“思维克隆”，其想法不仅是克隆人类示范者的行为，而且还包括人类在执行这些行为时所产生的想法。虽然我们希望“思维克隆”在处理网络规模的人类思维和行为数据时能够发挥出色（例如，带有剧本的在线视频），但在这里，我们进行了在思考和行动数据为合成生成的领域的实验。结果显示，“思维克隆”学习速度比传统的强化学习方法快得多。

    Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
    
[^132]: 离线多智能体强化学习协调问题的基于模型的解决方案

    A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v1 [cs.LG])

    [http://arxiv.org/abs/2305.17198](http://arxiv.org/abs/2305.17198)

    提出了一个基于模型的离线多智能体强化学习方法MOMA-PPO，通过生成合成交互数据并优化智能体的政策，解决了策略一致性和策略微调两个协调问题，在具有挑战性的离线MARL场景中胜过主流的学习方法，提供了实际应用中的可行解决方案。

    

    训练多个智能体进行协调是一项重要问题，具有机器人技术、博弈论、经济学和社会科学等领域的应用。然而，大多数现有的多智能体强化学习方法是在线的，因此在收集新的交互数据成本高昂或危险的实际应用中不可行。虽然这些算法应该利用离线数据，但这样做会引起离线协调问题。具体而言，我们确定并形式化了策略一致性（SA）和策略微调（SFT）两个协调问题，这是当前离线多智能体强化学习算法失败的原因。为解决这个问题，我们提出了一种简单的基于模型的方法，生成合成交互数据，使智能体能够在微调策略的同时收敛于一个策略。我们提出的方法，Model-based Offline Multi-Agent Proximal Policy Optimization（MOMA-PPO），在具有挑战性的离线MARL场景中胜过主流的学习方法，证明了基于模型的方法提供了一个可行的解决方案。

    Training multiple agents to coordinate is an important problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) challenges, two coordination issues at which current offline MARL algorithms fail. To address this setback, we propose a simple model-based approach that generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. Our resulting method, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO), outperforms the prevalent learning methods in challenging offl
    
[^133]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^134]: GraphCare: 使用个性化知识图谱提升医疗预测能力

    GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.12788](http://arxiv.org/abs/2305.12788)

    本论文提出了一种名为GraphCare的框架，通过使用个性化知识图谱来改进基于电子健康记录的医疗预测，并通过在两个公共数据集上的实验证明了其有效性。

    

    临床预测模型通常依赖于患者的电子健康记录(EHR)，但将医学知识整合到预测和决策中以提高效果具有挑战性。这是因为个性化预测需要个性化的知识图谱(KG)，而从患者EHR数据中生成个性化知识图谱很困难。为了解决这个问题，我们提出了一个名为\textsc{GraphCare}的开放式框架，它使用外部知识图谱来改进基于EHR的预测。我们的方法从大规模语言模型(LLM)和外部生物医学知识图谱中提取知识，构建个体化的患者知识图谱，然后使用我们提出的Bi-attention AugmenTed (BAT)图神经网络(GNN)进行医疗预测训练。在两个公共数据集MIMIC-III和MIMIC-IV上，\textsc{GraphCare}在四个关键的医疗预测任务上均超过了基准线：死亡率、再入院率、住院天数和药物推荐。在MIMIC-III上，它将AUROC提高了17.6%和6.6%，将F1得分提高了7.9%。

    Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% 
    
[^135]: 对话过程建模：现状、应用和实践影响的综述

    Conversational Process Modelling: State of the Art, Applications, and Implications in Practice. (arXiv:2304.11065v1 [cs.CL])

    [http://arxiv.org/abs/2304.11065](http://arxiv.org/abs/2304.11065)

    本文系统的研究了现有聊天机器人对于支持对话式流程建模所提供的应用场景，并推导出了在实践中使用聊天机器人进行对话式流程建模的建议。

    

    最近Chatbots等聊天机器人引起了极大的关注。对于BPM应用来说，如何应用聊天机器人来生成商业价值通常是不明确的。因此，本文旨在系统地分析现有的聊天机器人对于支持对话式流程建模作为面向流程的能力的支持。该研究识别了沿流程生命周期的应用场景，然后进行了对话式流程建模的系统文献综述。得出的分类学用作对话式流程建模的应用场景的识别，包括流程描述的释义和改进。应用场景基于高等教育领域的实际测试集对现有聊天机器人进行评估。该测试集包含流程描述及其对应的流程模型，以及模型质量的评估。基于文献和应用场景分析，得出了关于在对话式流程建模中使用聊天机器人的建议。

    Chatbots such as ChatGPT have caused a tremendous hype lately. For BPM applications, it is often not clear how to apply chatbots to generate business value. Hence, this work aims at the systematic analysis of existing chatbots for their support of conversational process modelling as process-oriented capability. Application scenarios are identified along the process life cycle. Then a systematic literature review on conversational process modelling is performed. The resulting taxonomy serves as input for the identification of application scenarios for conversational process modelling, including paraphrasing and improvement of process descriptions. The application scenarios are evaluated for existing chatbots based on a real-world test set from the higher education domain. It contains process descriptions as well as corresponding process models, together with an assessment of the model quality. Based on the literature and application scenario analyses, recommendations for the usage (prac
    
[^136]: CodeKGC：用于生成知识图谱构建的代码语言模型

    CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])

    [http://arxiv.org/abs/2304.09048](http://arxiv.org/abs/2304.09048)

    本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。

    

    目前的生成式知识图谱构建方法通常无法捕捉结构性知识，而只是将自然语言转化为序列化文本或规范语言。然而，对于像代码这样的结构化数据进行训练的大型生成式语言模型已经展现了在理解自然语言以进行结构性预测和推理任务方面的卓越能力。本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法。具体而言，在给定代码格式的自然语言输入的情况下，目标是生成可以表示为代码补全任务的三元组。我们开发了具有模式感知型提示的方法，可以有效利用知识图谱内的语义结构。由于代码本质上具有结构，如类和函数定义，因此它作为先验的语义结构知识模型非常有用。此外，我们采用了基于原理的生成方法来提高性能。原理提供了模型生成结果的可解释性。

    Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
    
[^137]: NeuroBench：通过合作、公平和代表性基准测试推进神经形态计算

    NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking. (arXiv:2304.04640v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.04640](http://arxiv.org/abs/2304.04640)

    NeuroBench是由学术界和工业界成员共同开发的一套协作、公平和代表性的基准测试，可以解决神经形态计算中缺乏清晰标准的问题，推动该领域的发展。

    

    神经形态计算领域在遵循仿生学原理的基础上，具有推进计算效率和能力的巨大潜力。然而，神经形态研究中采用的技术多样性导致缺乏清晰的基准测试标准，阻碍了对神经形态方法与传统基于深度学习的方法的优劣势进行有效评估。本文提出了一个协作项目——NeuroBench，将学术界和工业界成员聚集起来为神经形态计算定义基准测试。NeuroBench的目标是成为社区开发的协作、公平和代表性的基准测试套件。本文讨论了基准测试神经形态解决方案面临的挑战，并概述了NeuroBench的关键特性。我们相信，NeuroBench将是定义能够统一神经形态计算目标的标准的重要一步。

    The field of neuromorphic computing holds great promise in terms of advancing computing efficiency and capabilities by following brain-inspired principles. However, the rich diversity of techniques employed in neuromorphic research has resulted in a lack of clear standards for benchmarking, hindering effective evaluation of the advantages and strengths of neuromorphic methods compared to traditional deep-learning-based methods. This paper presents a collaborative effort, bringing together members from academia and the industry, to define benchmarks for neuromorphic computing: NeuroBench. The goals of NeuroBench are to be a collaborative, fair, and representative benchmark suite developed by the community, for the community. In this paper, we discuss the challenges associated with benchmarking neuromorphic solutions, and outline the key features of NeuroBench. We believe that NeuroBench will be a significant step towards defining standards that can unify the goals of neuromorphic comput
    
[^138]: 基于核凸包机的差分隐私学习研究

    Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])

    [http://arxiv.org/abs/2304.01300](http://arxiv.org/abs/2304.01300)

    本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。

    

    本文探讨了通过学习再生核希尔伯特空间中的点的凸包来表示数据的方法，旨在将数据空间划分为几何体，从而隐藏有关单个数据点的隐私信息，同时保留原始学习问题的结构。为此，我们引入了核凸包机（KAHM），它提供了一种有效的方法来计算从结果有界几何体中的距离度量。KAHM是广泛和深入的自编码器的关键构建块，它们使数据表示学习用于分类应用。为了确保隐私保护学习，我们提出了一种新颖的生成虚假数据的方法，该方法涉及将差分隐私数据样本通过转换过程进行平滑处理。生成的虚假数据不仅保证差分隐私，而且确保KAHM建模误差不大于原始数据误差。

    This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
    
[^139]: 长尾分类的曲率平衡特征流形学习

    Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])

    [http://arxiv.org/abs/2303.12307](http://arxiv.org/abs/2303.12307)

    本文提出了一种曲率平衡特征流形学习的方法，探究了感知流形的几何特性对分类难度的影响，发现曲率不平衡会导致模型不公平。

    

    为了应对长尾分类的挑战，研究人员已经提出了几种方法来减少模型偏差，其中大多数假设样本较少的类是弱类。然而，最近的研究表明，尾部类别并不总是难以学习的，而在样本平衡的数据集上观察到了模型偏差，这表明存在其他影响模型偏差的因素。在本文中，我们系统地提出了一系列用于深度神经网络中感知流形的几何度量，并探讨了感知流形的几何特性对分类难度和学习如何塑造感知流形的几何特性的影响。一个意外的发现是：类别准确度和感知流形的分离程度之间的相关性在训练过程中逐渐减小，而与曲率的负相关性逐渐增加，这表明曲率不平衡导致模型不公平。

    To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
    
[^140]: ESD:预期平方差作为一种无需调参的可训练校准度量

    ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02472](http://arxiv.org/abs/2303.02472)

    ESD是一种无需调参的可训练校准目标损失，通过将校准误差看作两个期望值之间的平方差，可以改善神经网络模型的校准度。

    

    研究表明，现代神经网络由于过于自信的预测而往往校准不良。传统上，在训练之后使用后处理方法来校准模型。近年来，已经提出了各种可训练的校准度量来直接将其纳入训练过程中。然而，这些方法都包含内部超参数，并且这些校准目标的性能依赖于调整这些超参数，随着神经网络和数据集的规模增大，会产生更多的计算成本。因此，我们提出了预期平方差（ESD），一种无需调参的可训练校准目标损失，我们从两个期望值之间的平方差的角度来看校准误差。通过对几种架构（CNN、Transformer）和数据集的大量实验证明，将ESD纳入训练可以改善模型的校准度。

    Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model cali
    
[^141]: 数据中心人工智能

    Data-centric Artificial Intelligence. (arXiv:2212.11854v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.11854](http://arxiv.org/abs/2212.11854)

    数据中心人工智能是一种新兴的范式，它强调系统性地设计和构建数据对于建立有效和高效的基于人工智能的系统至关重要。

    

    数据中心人工智能（data-centric AI）是一种新兴的范式，强调系统性地设计和构建数据对于建立有效和高效的基于人工智能的系统至关重要。本文的目的是向信息系统领域的从业者和研究人员介绍数据中心人工智能。我们定义相关术语，提供关键特征来对比数据中心范式和模型中心范式，并介绍了一个数据中心人工智能的框架。我们区分数据中心人工智能和相关概念，并讨论其对信息系统社区的长期影响。

    Data-centric artificial intelligence (data-centric AI) represents an emerging paradigm emphasizing that the systematic design and engineering of data is essential for building effective and efficient AI-based systems. The objective of this article is to introduce practitioners and researchers from the field of Information Systems (IS) to data-centric AI. We define relevant terms, provide key characteristics to contrast the data-centric paradigm to the model-centric one, and introduce a framework for data-centric AI. We distinguish data-centric AI from related concepts and discuss its longer-term implications for the IS community.
    
[^142]: 一种用于不平衡半监督学习的令人尴尬简单基准

    An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11086](http://arxiv.org/abs/2211.11086)

    本文研究了一种名为SimiS的简单但被忽视的基准方法，通过将伪标签作为标签数据的补充，根据与最频繁类别的类别分布差异，有效地减少了不平衡半监督学习中的类别不平衡，相对于现有方法有显著的性能提升。

    

    半监督学习（SSL）在利用无标签数据改善模型性能方面表现出巨大潜力。然而，标准的SSL假设数据分布均匀，我们考虑了一个更加真实和具有挑战性的情境，即不平衡半监督学习（imbalanced SSL），其中标签数据和无标签数据都出现了不平衡的类别分布。尽管已有努力解决这一挑战的方法，但当遇到严重不平衡时，它们的性能会退化，因为它们无法对类别不平衡进行足够和有效的减少。在本文中，我们研究了一个简单但被忽视的基准方法--SimiS，通过简单地根据与最频繁类别的类别分布差异，将伪标签作为标签数据的补充。这样一个简单的基准方法在减少类别不平衡方面非常有效。它在CIFAR100-LT，FOOD101-LT和ImageNet127上相对于先前最佳方法的性能提升显著，分别提升了12.8％，13.6％和16.7％。

    Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respecti
    
[^143]: 语言控制扩散：通过空间、时间和任务高效扩展

    Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15629](http://arxiv.org/abs/2210.15629)

    本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。

    

    训练通用型智能体在各个方面都很困难，需要处理高维输入（空间）、长时间跨度（时间）和多个新任务。最近的结构方面的进展使得我们可以沿着其中一个或两个维度提高扩展性能力，但计算成本仍然很高。本文提出使用语言控制扩散模型作为一种基于自然语言条件的分层规划器（LCD）来应对这三个方面。我们有效而高效地扩展扩散模型，以应对时间、状态和任务空间维度的长时间跨度控制问题。我们在CALVIN语言机器人基准测试中将LCD与其他最先进的模型进行比较，发现LCD在多任务成功率方面优于其他最先进的方法，而单任务成功率（SR）为88.7%，远高于以前的最佳成绩82.6%，大大提高了计算效率。

    Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
    
[^144]: 连续控制的正常引导分布强化学习

    Normality-Guided Distributional Reinforcement Learning for Continuous Control. (arXiv:2208.13125v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13125](http://arxiv.org/abs/2208.13125)

    本论文研究了连续控制任务中的值分布，并发现学习的值分布与正态分布非常接近。基于这一观察，提出了一种正态引导的分布式强化学习方法，利用方差网络预测的方差和回报，以及与标准值函数不同的值分布结构特征来更新策略。这种方法在两种在线算法上产生了显著效果。

    

    在许多强化学习算法中，学习一个预测回报的均值模型，或价值函数，起着关键作用。分布式强化学习(DRL)通过建模值分布而不仅仅是均值来提高性能。我们研究了几个连续控制任务中的值分布，并发现学习的值分布与正态分布非常接近。我们设计了一种利用这个性质的方法，利用从方差网络预测的方差，以及回报，来分析计算代表我们分布式值函数的正态分布的目标分位栏。此外，我们提出了一种基于值分布的结构特征的正确性来衡量的策略更新方法，这些特征在标准的值函数中不存在。我们概述的方法与许多DRL结构兼容。我们使用两种代表性的在线算法，PPO和TRPO，作为测试平台。我们的方法在统计上产生了显著的效果。

    Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirical quite close to normal. We design a method that exploits this property, employ variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically
    
[^145]: 从过程、对象、角色、组件、服务到Agent--编程抽象的历史和演变的比较分析

    From Procedures, Objects, Actors, Components, Services, to Agents -- A Comparative Analysis of the History and Evolution of Programming Abstractions. (arXiv:2112.12508v4 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2112.12508](http://arxiv.org/abs/2112.12508)

    本研究回顾性地分析了编程抽象的演变过程，从过程、对象、角色、组件、服务到Agent。研究发现，不断追求更高的灵活性和抽象级别是一个持续的趋势，而组件、服务和Agent的概念在实现软件模块化和重构性方面具有共同的目标。

    

    本章的目标是提出对编程抽象的演变进行一些回顾性分析，从"过程"、"对象"、"角色"、"组件"、"服务"到"Agent"。采用的方法是将它们置于一个通用的历史观的视角中。选择了一些常见的参照物，包括一个实体层面的"动作选择"、实体之间的"耦合灵活性"和"抽象级别"。我们可以观察到对更高灵活性（通过"延迟绑定"或"连接再实体化"等概念）和更高级别的抽象的不断追求。组件、服务和Agent的概念具有一些共同的目标（尤其是"软件模块化和可重构性"），而多Agent系统进一步提出了"自治"和"协同"的概念。

    The objective of this chapter is to propose some retrospective analysis of the evolution of programming abstractions, from {\em procedures}, {\em objects}, {\em actors}, {\em components}, {\em services}, up to {\em agents}, %have some compare concepts of software component and of agent (and multi-agent system), %The method chosen is to by replacing them within a general historical perspective. Some common referential with three axes/dimensions is chosen: {\em action selection} at the level of one entity, {\em coupling flexibility} between entities, and {\em abstraction level}. We indeed may observe some continuous quest for higher flexibility (through notions such as {\em late binding}, or {\em reification} of {\em connections}) and higher level of {\em abstraction}. Concepts of components, services and agents have some common objectives (notably, {\em software modularity and reconfigurability}), with multi-agent systems raising further concepts of {\em autonomy} and {\em coordination}
    

