# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health.](http://arxiv.org/abs/2306.10070) | 本文探讨了ChatGPT和大型语言模型在生物医学和健康领域的多样应用，发现在文本生成方面已经取得了重大进展，但对于其他应用进展缓慢，LLMs还没有真正彻底改变生物医学领域。 |
| [^2] | [The pop song generator: designing an online course to teach collaborative, creative AI.](http://arxiv.org/abs/2306.10069) | 本研究介绍了一个新的在线AI创意课程，基于三个AI模型建立了流行歌曲生成系统。该课程采用Piaget构造主义理念的"实践学习"方法，通过详细的五周课程设计提升学生的技术和创作能力。通过定量和定性分析，可以生成一首流行歌曲。 |
| [^3] | [Artificial Intelligence for Emergency Response.](http://arxiv.org/abs/2306.10068) | 本文介绍了数据驱动模型在应急响应中的应用，并探讨了其内在的四个子问题：事件预测，事件检测，资源分配和资源调度。 |
| [^4] | [Domain-specific ChatBots for Science using Embeddings.](http://arxiv.org/abs/2306.10067) | 本论文演示如何利用现有方法和软件工具结合嵌入技术设计面向科学领域的聊天机器人，该机器人能够处理科学文献，提供特定领域的上下文信息，并在初步研究辅助知识方面为物理科学家提供帮助。 |
| [^5] | [On the Interplay of Subset Selection and Informed Graph Neural Networks.](http://arxiv.org/abs/2306.10066) | 该研究探讨了在QM9数据集中，采用基于领域知识的数据采样方法来选择有效的训练集，再与信息图神经网络相结合，以最大化分子多样性和相关性来提高性能。 |
| [^6] | [Taming Diffusion Models for Music-driven Conducting Motion Generation.](http://arxiv.org/abs/2306.10065) | 本文提出了Diffusion-Conductor，一种基于DDIM的方法，用于音乐驱动的指挥运动生成，利用扩散模型整合到一个两阶段学习框架中，并应用随机屏蔽策略和一对几何损失函数来提高运动多样性和鲁棒性。 |
| [^7] | [Towards social generative AI for education: theory, practices and ethics.](http://arxiv.org/abs/2306.10063) | 本文探讨如何构建面向教育的社交生成人工智能系统，该系统需要强大的人工智能，并具备对互联网资源的访问和贡献能力，但同时还需要认识到自己的局限性，对学生负责并尊重人类价值观。 |
| [^8] | [Revealing the structure of language model capabilities.](http://arxiv.org/abs/2306.10062) | 本文研究了大规模语言模型的能力结构，发现这些模型不是单一能力，而是由推理、理解和核心语言建模等三个明确定义的因素组成，并且这三个能力可以解释模型性能中的大部分方差。 |
| [^9] | [The Ontology for Agents, Systems and Integration of Services: OASIS version 2.](http://arxiv.org/abs/2306.10061) | 本文介绍了 OASIS 2 本体论，一种为代理提供语义表示和通信的行为主义方法。该本体论已应用于区块链及其他领域。 |
| [^10] | [Smoothing the Rough Edges: Evaluating Automatically Generated Multi-Lattice Transitions.](http://arxiv.org/abs/2306.10055) | 本论文描述和评估了使用变分自编码器自动创建过渡晶格单元的方法，以解决多晶格结构中不同拓扑的单元晶格之间平滑转换的问题。 |
| [^11] | [A Shift In Artistic Practices through Artificial Intelligence.](http://arxiv.org/abs/2306.10054) | 人工智能模型生成的内容突破了艺术、音乐和媒体领域，引发了文化转变。它通过改变人们的角色、转变价值观以及挑战传统实践方式，为艺术的未来打开了新的可能性。 |
| [^12] | [NFTs to MARS: Multi-Attention Recommender System for NFTs.](http://arxiv.org/abs/2306.10053) | 本文提出了首个具有三大关键特征——图形注意力、多模态注意力和多任务学习的面向NFT的多注意力推荐系统(NFT-MARS)，以解决NFT市场挑战。 |
| [^13] | [Assigning AI: Seven Approaches for Students, with Prompts.](http://arxiv.org/abs/2306.10052) | 提出了七种利用 AI 在课堂上的方法：AI-导师、AI-辅导员、AI-指导员、AI-队友、AI-工具、AI-模拟器和AI-学生。这些策略促进对 AI 输出的批判性评估，以及利用 AI 的能力和学生独特见解的互补性，增强学习结果。此框架为整合 AI 辅助学习提供了指导。 |
| [^14] | [Neighborhood-based Hard Negative Mining for Sequential Recommendation.](http://arxiv.org/abs/2306.10047) | 本文提出了一种基于邻域重叠的基于图形的负采样方法（GNNO），利用隐藏在用户行为中的结构信息进行负采样，用于增进序列推荐模型的训练和性能。 |
| [^15] | [A Practical Entity Linking System for Tables in Scientific Literature.](http://arxiv.org/abs/2306.10044) | 本文介绍了一个用于将实体链接到知识库中的通用系统，并将其适应于链接特定领域的实体，特别是 COVID-19 相关科学文献中的嵌入式实体。通过利用表格的结构和语义特征，以提高整体实体链接性能。 |
| [^16] | [Unraveling the Interconnected Axes of Heterogeneity in Machine Learning for Democratic and Inclusive Advancements.](http://arxiv.org/abs/2306.10043) | 本论文探讨了机器学习应用中的异质性问题，从价值观、数据组成和资源基础设施三个角度入手并指出它们相互依存，需要一同考虑解决。同时，本文还揭示了现有机器学习研究存在的问题，如权力集中和依赖性增加。 |
| [^17] | [A Pairing Enhancement Approach for Aspect Sentiment Triplet Extraction.](http://arxiv.org/abs/2306.10042) | 本文提出了一种配对增强方法，通过对比学习将方面-意见配对知识注入到Aspect Sentiment Triplet Extraction（ASTE）模型中，提高了三元组提取的准确性和性能。 |
| [^18] | [Evaluating Superhuman Models with Consistency Checks.](http://arxiv.org/abs/2306.09983) | 本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。 |
| [^19] | [Demystifying GPT Self-Repair for Code Generation.](http://arxiv.org/abs/2306.09896) | 本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。 |
| [^20] | [Generalizable One-shot Rope Manipulation with Parameter-Aware Policy.](http://arxiv.org/abs/2306.09872) | GenORM通过增加可变形绳索参数和使用各种可变形绳索的模拟训练操作策略，实现利用一次真实演示处理不同可形变绳索，从而节省演示时间和提高适用性。 |
| [^21] | [Towards Practical Federated Causal Structure Learning.](http://arxiv.org/abs/2306.09433) | 为了解决联邦学习条件下的因果结构学习难题，提出了一种基于联邦条件独立性检验的因果结构学习方案FedC2SL，无需收集原始数据且对数据变异具有更强的抵抗力。 |
| [^22] | [Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation.](http://arxiv.org/abs/2306.09381) | STAR框架通过设计多种时空图来捕捉位置的动态时空效应，为人类移动模拟任务提供新的方法。 |
| [^23] | [Equitable Multi-task Learning.](http://arxiv.org/abs/2306.09373) | 该论文提出了一种名为EMTL的多任务优化方法，以实现公平的多任务学习。通过规范化不同任务的相对贡献，可以提高MTL的泛化性能，并利用方差正则化和高效的优化算法保证收敛。实验证明，该方法在合成和真实数据集上均表现出了更好的性能。 |
| [^24] | [Language to Rewards for Robotic Skill Synthesis.](http://arxiv.org/abs/2306.08647) | 该研究介绍了一种将大型语言模型用于定义奖励参数，并通过奖励函数进行优化，从而使机器人可以执行各种自然语言指令指定的任务的新方法。 |
| [^25] | [Unifying Large Language Models and Knowledge Graphs: A Roadmap.](http://arxiv.org/abs/2306.08302) | 本文提出了一个前瞻性的统一大型语言模型和知识图谱的路线图，通过三个框架：增强KGs的LLMs，知识增强KGs和LLMs与KGs的联合推理，综合利用两者的优点。 |
| [^26] | [Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment.](http://arxiv.org/abs/2306.07999) | 本文进行了数字病理图像中应用人工智能的所有病理学领域的诊断准确度的系统综述和Meta分析。结果表明，人工智能在数字病理学中取得了高度的准确度，是可行的辅助诊断工具。 |
| [^27] | [A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews.](http://arxiv.org/abs/2306.07786) | 本文介绍了一种基于云的系统，利用机器学习方法从客户评论中提取见解。本研究提出的组合模型使用了转换器神经网络、向量嵌入和聚类，已经集成并进一步发展，以更好地满足高效信息提取、提取信息的主题建模和用户需求的要求。研究结果表明，本系统可以比现有的主题建模和关键字提取解决方案获得更好的效果。 |
| [^28] | [A Protocol for Continual Explanation of SHAP.](http://arxiv.org/abs/2306.07218) | 本文研究了在持续学习中SHAP值解释的稳定性问题，提出了一种评估协议，并指出随机循环模型是更有效的备选循环方法。 |
| [^29] | [Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping.](http://arxiv.org/abs/2306.06994) | 该论文提出了一种基于时空自举的时间步级表示学习框架，能为相关时间序列分析提供高效降维表示，并通过在PeMS-BAY数据集上的测试取得了较好的效果。 |
| [^30] | [Defining and Explorting the Intelligence Space.](http://arxiv.org/abs/2306.06499) | 本文通过引入广泛视角来界定智能并建立了三级嵌套结构及其基础的广泛空间。利用这些定义初步探索了奇点、生成AI、伦理和知识产权等话题。 |
| [^31] | [Understanding the Effect of the Long Tail on Neural Network Compression.](http://arxiv.org/abs/2306.06238) | 本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。 |
| [^32] | [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.](http://arxiv.org/abs/2306.05426) | 本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。 |
| [^33] | [Learned spatial data partitioning.](http://arxiv.org/abs/2306.04846) | 本文介绍了一种通过机器学习技术学习空间数据分区的方法，在强化学习的框架下开发了深度强化学习算法。实验表明该算法能够有效地加速距离连接查询，缩短工作负载运行时间高达59.4％。 |
| [^34] | [AutoML Systems For Medical Imaging.](http://arxiv.org/abs/2306.04750) | 该论文介绍了医学成像中自动化机器学习的应用、策略和技术，该方法通过神经结构搜索和迁移学习技术简化了图像识别模型的创建，结合人类专业知识和计算机系统可以提高医学图像分析的精度和质量。 |
| [^35] | [Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts.](http://arxiv.org/abs/2306.04723) | 本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。 |
| [^36] | [SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts.](http://arxiv.org/abs/2306.02207) | 本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。 |
| [^37] | [Case Studies on X-Ray Imaging, MRI and Nuclear Imaging.](http://arxiv.org/abs/2306.02055) | 本文介绍了X射线成像、MRI和核医学在医学成像中的应用。通过基于人工智能的方法，尤其是卷积神经网络的应用，可以实现从成像模态中进行系统特征提取和分类，帮助医生进行快速准确的诊断。 |
| [^38] | [The feasibility of artificial consciousness through the lens of neuroscience.](http://arxiv.org/abs/2306.00915) | 从神经科学的角度来看，目前大型语言模型难以具备哺乳动物意识感知相关的丘脑皮层系统的关键特征，缺乏周围世界的具体嵌入式信息，且当前的人工智能无法做到存在的依赖于其行为，这意味着人工意识的可行性存在瓶颈。 |
| [^39] | [Doubly Robust Self-Training.](http://arxiv.org/abs/2306.00265) | 本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。 |
| [^40] | [Beam Tree Recursive Cells.](http://arxiv.org/abs/2305.19999) | 本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。 |
| [^41] | [MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images.](http://arxiv.org/abs/2305.19956) | 本文提出了一种基于深度学习的微型超声图像前列腺分割方法，利用多尺度注释引导的Transformer UNet模型和注释引导的二分类交叉熵损失解决低分辨率和界限不清的挑战，该方法更加关注难以分割的区域。 |
| [^42] | [Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study.](http://arxiv.org/abs/2305.19939) | 本文提供了一种半自动化的流程，用于将体内微-US图像与离体全切片组织病理学图像配准，以帮助泌尿外科医生提高小前列腺癌的检测率。 |
| [^43] | [Representation-Driven Reinforcement Learning.](http://arxiv.org/abs/2305.19922) | 该论文提出了一个表示驱动的强化学习框架，通过在线性特征空间中嵌入策略网络，重新框定探索-利用问题为表示-利用问题，以实现最佳的探索。该框架通过应用进化和策略梯度法取得了显著的性能提升。 |
| [^44] | [A rule-general abductive learning by rough sets.](http://arxiv.org/abs/2305.19718) | 本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。 |
| [^45] | [Towards Omni-generalizable Neural Methods for Vehicle Routing Problems.](http://arxiv.org/abs/2305.19587) | 提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。 |
| [^46] | [Learning to Learn from APIs: Black-Box Data-Free Meta-Learning.](http://arxiv.org/abs/2305.18413) | 该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。 |
| [^47] | [Evaluating GPT-3 Generated Explanations for Hateful Content Moderation.](http://arxiv.org/abs/2305.17680) | 本文通过调查和分析，评估了使用GPT-3生成的针对仇恨内容的解释是否准确和有用。结果显示，GPT-3生成的解释普遍存在过于模糊、聚焦不当等缺点，同时也存在不同类型仇恨言论生成的解释质量差异大的问题。 |
| [^48] | [Im-Promptu: In-Context Composition from Image Prompts.](http://arxiv.org/abs/2305.17262) | 本文研究了类比推理能否实现对可组合视觉刺激成分的上下文内组合，通过引入三个基准测试套件，提供了设计类比推理的元学习框架 Im-Promptu。使用 Im-Promptu 可以训练多个具有不同组合水平的代理，包括矢量表示、补丁表示和物体槽。 |
| [^49] | [Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration.](http://arxiv.org/abs/2305.16173) | 本文提出了一种基于循环矩阵和格拉姆迭代的方法，用于高效估计卷积神经网络中的Lipschitz常数上界。该方法精确、快速、可微分，并展现了超线性收敛。在实验上表现出较高的精度、计算成本和可扩展性，在利普希茨正则化方面也取得了具有竞争力的结果。 |
| [^50] | [PromptNER: Prompting For Named Entity Recognition.](http://arxiv.org/abs/2305.15444) | PromptNER是一种基于提示的命名实体识别算法，利用LLM生成潜在实体列表并提供解释，在少样本NER和跨领域NER方面实现了最先进性能。 |
| [^51] | [PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines.](http://arxiv.org/abs/2305.15424) | 该论文提出了一种利用深度学习对犬的ECG信号进行自动分类的方法，通过使用随机数据增强策略和连续小波变换，分类精度得到了提高。 |
| [^52] | [Deep Learning and Ethics.](http://arxiv.org/abs/2305.15239) | 本文探讨了人工智能存在的算法偏见、缺乏可解释性、数据隐私侵犯、军事化、欺诈和环境问题，旨在促进哲学、政治科学和社会科学领域关于这些问题的探讨。 |
| [^53] | [Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding.](http://arxiv.org/abs/2305.14449) | 一种协同过滤新方法用于稳健对话理解，在历史用户-实体交互的基础上，利用多跳客户亲和力丰富每个用户的索引，并使用有限内存BFGS算法调整每个索引的权重，实验结果显示其明显优于最先进的个性化查询重写方法。 |
| [^54] | [DUBLIN -- Document Understanding By Language-Image Network.](http://arxiv.org/abs/2305.14218) | DUBLIN是一个针对视觉文档理解的模型，使用掩模文档内容生成任务、边界框任务和渲染问答任务进行预训练，利用文档图像中的空间和语义信息。该模型在多项基准测试中达到了竞争性或最先进的结果。 |
| [^55] | [ChipGPT: How far are we from natural language hardware design.](http://arxiv.org/abs/2305.14019) | 这篇论文介绍了ChipGPT，一个自动化设计环境，它利用大型语言模型从自然语言规范生成硬件逻辑设计，并展示了与人工设计性能相媲美的结果，且可节省超过75％的编码时间。 |
| [^56] | [On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2305.11283) | 本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。 |
| [^57] | [Large Language Models can be Guided to Evade AI-Generated Text Detection.](http://arxiv.org/abs/2305.10847) | 本文揭示了大型语言模型可以通过精心设计的提示语来有效规避现有的文本检测系统，证明了这些检测器的脆弱性。 |
| [^58] | [Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning.](http://arxiv.org/abs/2305.08014) | 本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。 |
| [^59] | [Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning.](http://arxiv.org/abs/2305.05119) | 本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。 |
| [^60] | [Human or Machine: Reflections on Turing-Inspired Testing for the Everyday.](http://arxiv.org/abs/2305.04312) | 本文基于图灵测试，回避了机器是否智能的问题，探讨在日常生活中如何确定一个交互对象是人还是机器的挑战，并思考了其应用和重要性。 |
| [^61] | [LSGNN: Towards General Graph Neural Network in Node Classification by Local Similarity.](http://arxiv.org/abs/2305.04225) | 本文提出使用局部相似性（LocalSim）学习节点级加权融合的即插即用模块，提取更具信息性的多跳信息，并对其有效性进行了理论分析。在真实基准数据集上的广泛评估表明，我们提出的LSGNN方法在同质性和异质性图上均能提供可比或优于最先进的性能。 |
| [^62] | [Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation.](http://arxiv.org/abs/2305.03942) | 论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。 |
| [^63] | [Assessing Working Memory Capacity of ChatGPT.](http://arxiv.org/abs/2305.03731) | 本文评估了最先进语言模型ChatGPT的工作记忆容量，结果显示其在N-back任务的行为表现与人类参与者相似，这为设计具有人类级认知能力的人工智能系统提供了关键洞察。 |
| [^64] | [Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era.](http://arxiv.org/abs/2305.02555) | ChatGPT和Bard等AI工具需要持续大量且高质量的数据来提高其性能，但现行的版权法则限制了它们对各种数据的获取。与数据提供者分享收益将有助于将AI工具与大多数版权数据拥有者之间的敌对关系转变为合作关系，使AI生态系统更健康。 |
| [^65] | [Local Search for Integer Linear Programming.](http://arxiv.org/abs/2305.00188) | 本论文开发了一个独立的局部搜索求解器，可用于解决一般整数线性规划，并在大型异构问题数据集上进行了验证。在搜索、改进和还原模式下，分别提出了可自适应修改变量值的算子和高效的举升算子，从而提高当前解的质量。实验表明，该方法在MIPLIB2017的异构问题集上表现优异。 |
| [^66] | [Deep Intellectual Property: A Survey.](http://arxiv.org/abs/2304.14613) | 这篇综述介绍了利用深度神经网络时所面临的知识产权保护问题，以及近年来防止和发现模型窃取和未经授权重新分发的方法。 |
| [^67] | [High-dimensional Clustering onto Hamiltonian Cycle.](http://arxiv.org/abs/2304.14531) | 提出了一种基于哈密顿回路的高维聚类框架，将全局结构和局部结构相结合，通过哈密顿回路将不同簇的锚点排序并映射到圆的周长上，改进了聚类的标签，达到更好的分类效果。 |
| [^68] | [Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques.](http://arxiv.org/abs/2304.12583) | 本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。 |
| [^69] | [Development of a Trust-Aware User Simulator for Statistical Proactive Dialog Modeling in Human-AI Teams.](http://arxiv.org/abs/2304.11913) | 本文开发了一种用户模拟器来训练和测试主动对话策略，提供了一种探索和评估人工智能团队表现的适当主动策略的途径。 |
| [^70] | [Revisiting k-NN for Pre-trained Language Models.](http://arxiv.org/abs/2304.09058) | 本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。 |
| [^71] | [DeePLT: Personalized Lighting Facilitates by Trajectory Prediction of Recognized Residents in the Smart Home.](http://arxiv.org/abs/2304.08027) | 本论文提出了一种基于机器学习的智能照明系统DeePLT，其通过轨迹预测实现智能家居中的个性化照明调整，给每个人定制独特的个人资料并根据其轨迹自动调整灯光。 |
| [^72] | [Chain of Thought Prompt Tuning in Vision Language Models.](http://arxiv.org/abs/2304.07919) | 本文提出了一种链式思维提示调整的新方法，以有效地解决视觉任务中的推理问题，在图像分类任务中表现出更好的泛化性，更高的可迁移性和更高的准确性，并通过提供逐步推理过程来提高了视觉模型的可解释性。 |
| [^73] | [Deep Active Alignment of Knowledge Graph Entities and Schemata.](http://arxiv.org/abs/2304.04389) | 本文提出了一种基于深度学习和主动学习的KG对齐方法DAAKG，可以联合对齐不仅实体，还包括关系和类别；通过主动学习选择最佳批次进行人工标注，实验结果显示其优越精度和泛化性。 |
| [^74] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^75] | [Maximal Ordinal Two-Factorizations.](http://arxiv.org/abs/2304.03338) | 本文研究了最大序数二次因子分解问题，证明了其判定是否存在是一个NP完全问题，并提供了用于计算最大因子分解的算法Ord2Factor。 |
| [^76] | [Grid-SD2E: A General Grid-Feedback in a System for Cognitive Learning.](http://arxiv.org/abs/2304.01844) | 本文提出了一种名为Grid-SD2E的认知学习系统，其建立在网格细胞的基础上，使用空间划分和探索利用方法实现交互和自我强化，有助于理解大脑的工作机制、治疗脑部疾病和理解智能。 |
| [^77] | [G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System.](http://arxiv.org/abs/2304.01559) | G2PTL是一种面向物流领域的预训练模型，结合了文本预训练的语义学习能力和图建模的地理关系编码能力，能有效地编码交付地址中的位置信息，并在物流系统中具有广泛的应用前景。 |
| [^78] | [INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields.](http://arxiv.org/abs/2303.18101) | 本文提出了一个名为INoD的注入噪声鉴别器，通过特征替换和数据集鉴别的原则进行农田自监督表示学习，提升了模型性能。 |
| [^79] | [From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract).](http://arxiv.org/abs/2303.14338) | 本文研究了从哥德尔不完备定理到机器人宗教的完备性的逻辑过程，提出了任何信仰系统可以被形式化为逻辑理论，并且不完备定理意味着存在真实但无法证明的陈述，可以用来定义出与现有信仰和传统一致的新宗教实践。 |
| [^80] | [Planning as Theorem Proving with Heuristics.](http://arxiv.org/abs/2303.13638) | 该论文介绍了一种用启发式规划作为定理证明的方法，通过开发一种定理证明提升启发式(TPLH)规划器，在情况树中搜索短的计划，并减少探讨的状态数量。 |
| [^81] | [Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning.](http://arxiv.org/abs/2303.11183) | 不受体系结构、数据集和模型规模限制的无数据元学习框架PURER，通过ECI执行伪周期训练以适应新的任务，通过ICFIL对反演梯度进行校准来优化反演过程，并在各种任务中显著优于现有方法。 |
| [^82] | [Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices.](http://arxiv.org/abs/2303.06016) | 本文提出了一种新的偏差嵌入式偏好模型——Probe，旨在解决用户在时间跨度的购物选择中的投影偏差和参照点效应，提高决策的有效性和个性化。 |
| [^83] | [Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals.](http://arxiv.org/abs/2303.05648) | Pacos是一个上下文依赖的偏好模型，可以处理偏好逆转问题，并提供用户的自适应权重、比较和显示位置等可解释因素，有助于提供个性化服务。 |
| [^84] | [Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning.](http://arxiv.org/abs/2303.05479) | 本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。 |
| [^85] | [Embodied Active Learning of Relational State Abstractions for Bilevel Planning.](http://arxiv.org/abs/2303.04912) | 该论文提出了一种基于身体知识的主动学习方法，通过在线交互与专家学习神经谓词解释、符号规划算子和任务特定的关系状态抽象，以提高机器人操作任务中的性能。 |
| [^86] | [Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries.](http://arxiv.org/abs/2303.02484) | 本研究提出了一种新的集成学习方法——多对称集合（MSE），它通过对称轴上假设的多样性来提高多样性和泛化能力，超越传统随机扰动的方法探索假设空间，并取得了良好效果。 |
| [^87] | [You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction.](http://arxiv.org/abs/2302.14189) | 本文提出了一种称为图交集-诱导图传递学习的方法，旨在解决稀疏图在链接预测中的问题。在此方法中，通过创建一个交集子图，在源图上训练模型并将知识传递到目标图上进行预测。 |
| [^88] | [Forward LTLf Synthesis: DPLL At Work.](http://arxiv.org/abs/2302.13825) | 本文提出了一种基于DPLL算法的新的AND-OR图搜索框架，用于有限跟踪（\LTLf）的线性时态逻辑合成，从而在许多情况下优于其他最先进的方法。 |
| [^89] | [Self-supervised learning of Split Invariant Equivariant representations.](http://arxiv.org/abs/2302.10283) | 本文介绍了一个数据集3DIEBench，提出了一种基于超网络的等变表达式预测器SIE，结合分裂的不变与等变表达式以获得更加丰富的表示，同时证明了显著的性能提高。 |
| [^90] | [COMET: X86 Cost Model Explanation Framework.](http://arxiv.org/abs/2302.06836) | 本文提出了一个用于生成x86成本模型解释的框架COMET，通过提供解释来提高机器学习成本模型的可解释性和可推广性。研究显示该框架所提供的语义信息与成本预测误差呈负相关。 |
| [^91] | [Parameter-efficient Modularised Bias Mitigation via AdapterFusion.](http://arxiv.org/abs/2302.06321) | 本文提出了一个新的去偏差方法——DAM，它采用AdapterFusion概念，将偏差修正功能封装到独立的适配器中，在不影响核心模型的情况下，实现了按需的去偏差，可以有效降低模型的偏见问题。 |
| [^92] | [MarioGPT: Open-Ended Text2Level Generation through Large Language Models.](http://arxiv.org/abs/2302.05981) | MarioGPT是第一个文本到超级马里奥兄弟游戏关卡的生成模型，通过大型语言模型实现开放式的、可控制的关卡生成。 |
| [^93] | [Compositional Exemplars for In-context Learning.](http://arxiv.org/abs/2302.05698) | 该论文提出了CEIL（Compositional Exemplars for In-context Learning）框架，利用决定性点过程（DPP）模型处理上下文示例选择问题，从而提高了大型预训练语言模型（LMs）进行上下文学习的性能。 |
| [^94] | [SOCRATES: Text-based Human Search and Approach using a Robot Dog.](http://arxiv.org/abs/2302.05324) | 该论文提出了一种先搜索再接近目标人员的基于文本的机器人接近方法，并使用基于混合学习的框架生成机器人的友好动作，成功验证了该方法。 |
| [^95] | [Hypernetworks build Implicit Neural Representations of Sounds.](http://arxiv.org/abs/2302.04959) | 该论文介绍了一种新的方法，名为“HyperSound”，可将超网络结构应用于元学习，从而生成音频隐式神经表示（INR），该方法可用于音频信号的处理，并且重构质量可与其他最先进的模型相媲美，是当代音频处理中的一个有潜力的替代方案。 |
| [^96] | [Principled and Efficient Motif Finding for Structure Learning of Lifted Graphical Models.](http://arxiv.org/abs/2302.04599) | 本文提出了一种针对提升图形模型中结构模体挖掘的原则性和高效性方法，通过预处理步骤和控制超参数优化算法效果。 |
| [^97] | [Domain Adaptation for Time Series Under Feature and Label Shifts.](http://arxiv.org/abs/2302.03133) | 本文提出了Raincoat，这是第一个针对复杂时间序列的封闭集和通用领域自适应模型。Raincoat通过跨域对齐时间和频率特征，修正偏移以便于检测私有标签，并通过识别共享结构来提高可传递性。 |
| [^98] | [Mnemosyne: Learning to Train Transformers with Transformers.](http://arxiv.org/abs/2302.01128) | Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。 |
| [^99] | [MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning.](http://arxiv.org/abs/2301.13287) | 提出了一个模型无关子集选择框架MILO，将子集选择与模型训练分离，通过易到难的课程实现了卓越的模型收敛和性能。 |
| [^100] | [Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation.](http://arxiv.org/abs/2301.12197) | 本文提出了一种基于互Wasserstein距离最小化的新型自监督学习框架用于提高推荐系统的性能，该方法使用Wasserstein距离测量来增强互信息最大化，优于现有的几种顺序推荐方法。 |
| [^101] | [Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees.](http://arxiv.org/abs/2301.11911) | 提出了多维概念发现(MCD)方法，它满足概念层面上的完整性关系，不需要加强概念可解释性或重新训练模型部分，并提供概念激活图分析工具 |
| [^102] | [Input Perturbation Reduces Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2301.11706) | 本文提出了一种输入扰动方法来缓解扩散模型中的曝光偏差问题，该方法不影响模型性能，能显著提高生成样本的质量并减少训练和推断时间。 |
| [^103] | [Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series.](http://arxiv.org/abs/2301.11308) | 本研究提出了一个用于不规则采样时间序列的神经连续离散状态空间模型，其采用辅助变量来区分识别和动态，从而实现了准确的贝叶斯推理和改进的性能。 |
| [^104] | [FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment.](http://arxiv.org/abs/2301.07330) | 该论文提出了一种名为FPANet的新模型，它通过去除各种大小的莫尔纹图案来改善恢复质量，采用多个连续帧提取帧不变内容特征，输出时间一致图像。 |
| [^105] | [The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks.](http://arxiv.org/abs/2301.07068) | 本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。 |
| [^106] | [Short-length SSVEP data extension by a novel generative adversarial networks based framework.](http://arxiv.org/abs/2301.05599) | 本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。 |
| [^107] | [Persistence-Based Discretization for Learning Discrete Event Systems from Time Series.](http://arxiv.org/abs/2301.05041) | 本研究提出了一种基于持久性的离散化方法，使用Wasserstein距离来纠正持久性分数中过度的偏向，增强了模型的性能。 |
| [^108] | [Extrinsic Evaluation of Machine Translation Metrics.](http://arxiv.org/abs/2212.10297) | 论文研究了机器翻译度量在大型平台和下游任务中的可靠性，发现某些度量在句子级别上表现不佳且其有用性与下游任务有关。 |
| [^109] | [Caching Contents with Varying Popularity using Restless Bandits.](http://arxiv.org/abs/2212.03291) | 本文研究了如何在无线边缘处缓存内容以减轻移动网络对核心网络和回程链路带来的负担。通过无休止赌博算法将问题最小化。 |
| [^110] | [On the Power of Foundation Models.](http://arxiv.org/abs/2211.16327) | 本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。 |
| [^111] | [SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation.](http://arxiv.org/abs/2211.14813) | SegCLIP是一种用于开放词汇语义分割的模型，通过聚集补丁到语义区域进行分割，具有动态捕捉语义组的特点。 |
| [^112] | [Background-Mixed Augmentation for Weakly Supervised Change Detection.](http://arxiv.org/abs/2211.11478) | 本文通过背景混合增强和弱监督算法，成功地解决了变化检测的泛化问题。 |
| [^113] | [Lifelong Bandit Optimization: No Prior and No Regret.](http://arxiv.org/abs/2210.15513) | 本文提出了一种算法LIBO，可以无需直接访问数据，对一系列赌博优化任务进行学习和适应，并保证最优性能和亚线性终身后悔率。 |
| [^114] | [Textual Entailment Recognition with Semantic Features from Empirical Text Representation.](http://arxiv.org/abs/2210.09723) | 本文提出了一种利用实验证据的文本表征和语义特征的新方法，通过元素曼哈顿距离向量特征识别文本-假设之间的蕴涵关系，并在基准数据集上实现了显著的F1分数提高。 |
| [^115] | [PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting.](http://arxiv.org/abs/2210.08964) | 提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。 |
| [^116] | [Survey on Fairness Notions and Related Tensions.](http://arxiv.org/abs/2209.13012) | 本文调查了公平性的不同概念以及它们与其他期望属性的紧张关系，并介绍了处理公平性-准确性权衡问题的不同方法。 |
| [^117] | [Normalizing Flows for Interventional Density Estimation.](http://arxiv.org/abs/2209.06203) | 本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。 |
| [^118] | [TFN: An Interpretable Neural Network with Time-Frequency Transform Embedded for Intelligent Fault Diagnosis.](http://arxiv.org/abs/2209.01992) | 提出一种新的可解释神经网络模型——时频网络（TFN）。在传统卷积层中嵌入了物理上有意义的时频变换（TFT）方法作为自适应预处理层，该层不仅提高了诊断性能，还使得网络结构可以可解释，故障诊断过程透明。 |
| [^119] | [Prompting as Probing: Using Language Models for Knowledge Base Construction.](http://arxiv.org/abs/2208.11057) | 本文介绍了一种利用语言模型进行知识库构建的方法，该方法采用了多种提示技术，手动提示策略的编制至关重要，并且必须鼓励语言模型给出不同长度的答案集，特别是包括空答案集。实体别名字典可以提高语言模型的得分。 |
| [^120] | [Detecting Multivariate Time Series Anomalies with Zero Known Label.](http://arxiv.org/abs/2208.02108) | 本文提出了一种利用动态图和实体注意力机制实现零标签多元时间序列异常检测的方法，其利用密度估计比较异常和正常实例，性能优于多种无监督方法和半监督方法。 |
| [^121] | [A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach.](http://arxiv.org/abs/2207.11716) | 本文通过使用Transformer在U.S Patent Phrase to Phrase Matching Dataset上进行语义相似度分析，提高了算法效率，达到了令人满意的结果。 |
| [^122] | [Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers.](http://arxiv.org/abs/2207.10170) | 对于顺序决策者的敌对攻击来说，弱点是缺乏时间上的一致性，使其容易被检测出来；而R-attack是一种既有效又可证明是统计不可检测的攻击，可以更难以使用自动化方法检测出来。 |
| [^123] | [Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning.](http://arxiv.org/abs/2206.02670) | 本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受对抗攻击。 |
| [^124] | [Robust Anytime Learning of Markov Decision Processes.](http://arxiv.org/abs/2205.15827) | 文章介绍了一种将贝叶斯推理方案和计算强健策略相结合的、不断学习马尔可夫决策过程转移概率的方法，阐述了不确定MDP（uMDP）的概念，针对应用中有限数据导致的统计误差问题提出了基于不确定性集的解决方法，并介绍了计算强健策略以遵循形式规范的工具。 |
| [^125] | [Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification.](http://arxiv.org/abs/2205.13094) | 该论文证明在非参数二元分类中，缺乏少数派样本是学习的根本限制，并探讨了欠采样算法的最小化极差风险的鲁棒性表现，特别是在标签转移的情况下可以最优化。 |
| [^126] | [Word Discovery in Visually Grounded, Self-Supervised Speech Models.](http://arxiv.org/abs/2203.15081) | 这篇论文介绍了一种基于图像-语音联合训练的自监督模型，在模型训练后实现了自动词语分割和聚类的能力，并在两个任务中表现优异。 |
| [^127] | [Understanding Dynamic Spatio-Temporal Contexts in Long Short-Term Memory for Road Traffic Speed Prediction.](http://arxiv.org/abs/2112.02409) | 本论文提出了一种动态局部化的LSTM模型，能够同时考虑道路之间的空间与时间依赖关系，该模型比基线方法具有更优异的预测性能。 |
| [^128] | [Efficiency, Fairness, and Stability in Non-Commercial Peer-to-Peer Ridesharing.](http://arxiv.org/abs/2110.01152) | 本文聚焦于P2P共乘中最核心的问题：乘客和司机的匹配。在考虑到用户偏好的前提下，本文提出了关于公平性和稳定性的新概念以及高效匹配算法，可以在不影响系统范围内效率的情况下获得公平和稳定的共乘解决方案。 |
| [^129] | [Learning to Combine Per-Example Solutions for Neural Program Synthesis.](http://arxiv.org/abs/2106.07175) | 该论文提出了一种把程序合成问题分为两个阶段的方法，提高了解决方案的成功率。作者使用了由多头注意机制构建的Cross Aggregator神经网络模块，学习如何组合每个样例程序解决方案，生成全局解决方案。 |
| [^130] | [Simple and optimal methods for stochastic variational inequalities, I: operator extrapolation.](http://arxiv.org/abs/2011.02987) | 本文提出了一种简单实用的算子外推法用来解决变分不等式问题，同时还提出了一种随机算子外推法实现随机平滑和强单调VI的最优复杂度。 |
| [^131] | [Cooperative Multi-Agent Reinforcement Learning with Partial Observations.](http://arxiv.org/abs/2006.10822) | 本文提出了一种基于局部状态和动作信息的分布式零阶策略优化方法，可用于部分观测的协作多智能体强化学习，减小通信开销并取得更好的效果。 |
| [^132] | [On-the-Fly Adaptation of Source Code Models using Meta-Learning.](http://arxiv.org/abs/2003.11768) | 本文提出了一种基于元学习的方法用于源码模型的实时适应性研究，以提高源代码模型的预测准确性，解决代码自动完成问题。 |

# 详细

[^1]: ChatGPT和大型语言模型在生物医学和健康领域的机遇与挑战

    Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v1 [cs.CY])

    [http://arxiv.org/abs/2306.10070](http://arxiv.org/abs/2306.10070)

    本文探讨了ChatGPT和大型语言模型在生物医学和健康领域的多样应用，发现在文本生成方面已经取得了重大进展，但对于其他应用进展缓慢，LLMs还没有真正彻底改变生物医学领域。

    

    ChatGPT由于其卓越的文本生成能力，已经引起了公众和领域专家的广泛关注，并产生了在生物医学和健康领域的各种应用。本文探讨了大型语言模型（LLMs）如ChatGPT在生物医学和健康领域的多样应用，具体探讨生物医学信息检索、问答、医疗文本摘要、信息抽取和医学教育等领域，并研究LLMs是否具有真正的转型力量以彻底改变这些任务或者生物医学领域的独特复杂性是否提出了独特的挑战。通过广泛的文献调研，我们发现在文本生成任务方面已经取得了重大进展，超越了以前的最先进方法。对于其他应用，进展还比较缓慢。总体而言，LLMs还没有彻底改变生物医学领域。

    ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized the bio
    
[^2]: 流行歌曲生成器：设计一个在线课程来教授协作创意 AI

    The pop song generator: designing an online course to teach collaborative, creative AI. (arXiv:2306.10069v1 [cs.CY])

    [http://arxiv.org/abs/2306.10069](http://arxiv.org/abs/2306.10069)

    本研究介绍了一个新的在线AI创意课程，基于三个AI模型建立了流行歌曲生成系统。该课程采用Piaget构造主义理念的"实践学习"方法，通过详细的五周课程设计提升学生的技术和创作能力。通过定量和定性分析，可以生成一首流行歌曲。

    

    本文介绍并评估了一门新的在线 AI 创意课程。该课程基于三个近乎最先进的 AI 模型，结合成了一个流行歌曲生成系统。经过微调的 GPT-2 模型编写歌词，Music-VAE 组合音乐乐谱和器乐，而 Diffsinger 则合成唱歌的声音。我们解释了设计课程所做的决定，基于 Piaget 的构造主义“学以致用”的理念。我们介绍了五周课程设计的细节，包括学习目标、技术概念以及创意和技术活动。我们解释了如何克服技术挑战，构建了一个完整的流行歌曲生成器系统，由 Python 脚本、预训练模型和通过基于 Web 的 IDE 在 docker 化 Linux 容器中运行的 Javascript 代码组成。通过对学生活动的定量分析提供了参考，并为未来改进提供了基准。专家工作坊的定性分析验证了整个课程的设计。

    This article describes and evaluates a new online AI-creativity course. The course is based around three near-state-of-the-art AI models combined into a pop song generating system. A fine-tuned GPT-2 model writes lyrics, Music-VAE composes musical scores and instrumentation and Diffsinger synthesises a singing voice. We explain the decisions made in designing the course which is based on Piagetian, constructivist 'learning-by-doing'. We present details of the five-week course design with learning objectives, technical concepts, and creative and technical activities. We explain how we overcame technical challenges to build a complete pop song generator system, consisting of Python scripts, pre-trained models, and Javascript code that runs in a dockerised Linux container via a web-based IDE. A quantitative analysis of student activity provides evidence on engagement and a benchmark for future improvements. A qualitative analysis of a workshop with experts validated the overall course des
    
[^3]: 应急响应人工智能

    Artificial Intelligence for Emergency Response. (arXiv:2306.10068v1 [cs.CY])

    [http://arxiv.org/abs/2306.10068](http://arxiv.org/abs/2306.10068)

    本文介绍了数据驱动模型在应急响应中的应用，并探讨了其内在的四个子问题：事件预测，事件检测，资源分配和资源调度。

    

    应急响应管理是全球社区面临的挑战。应急响应人员必须迅速响应各种事件，例如火灾、交通事故和医疗紧急情况。遏制事件以最小化人员伤亡风险。因此，在过去几十年中，人们对应急事件和响应进行了大量研究。特别是，数据驱动模型有助于减少人员和财务损失，并改善设计规范、交通法规和安全措施。本教程论文探讨了应急响应中的四个子问题：事件预测、事件检测、资源分配和资源调度。我们旨在提出这些问题的数学公式和广泛框架。我们还分享了一份来自美国一个大都市区的开源（合成）数据，以便于未来基于数据驱动的应急响应工作。

    Emergency response management (ERM) is a challenge faced by communities across the globe. First responders must respond to various incidents, such as fires, traffic accidents, and medical emergencies. They must respond quickly to incidents to minimize the risk to human life. Consequently, considerable attention has been devoted to studying emergency incidents and response in the last several decades. In particular, data-driven models help reduce human and financial loss and improve design codes, traffic regulations, and safety measures. This tutorial paper explores four sub-problems within emergency response: incident prediction, incident detection, resource allocation, and resource dispatch. We aim to present mathematical formulations for these problems and broad frameworks for each problem. We also share open-source (synthetic) data from a large metropolitan area in the USA for future work on data-driven emergency response.
    
[^4]: 利用嵌入技术设计面向科学领域的聊天机器人

    Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v1 [cs.CL])

    [http://arxiv.org/abs/2306.10067](http://arxiv.org/abs/2306.10067)

    本论文演示如何利用现有方法和软件工具结合嵌入技术设计面向科学领域的聊天机器人，该机器人能够处理科学文献，提供特定领域的上下文信息，并在初步研究辅助知识方面为物理科学家提供帮助。

    

    大语言模型(LLM)已成为强大的机器学习系统，能处理多种任务。经调整的这些系统已被转化为聊天机器人，能回答用户对广泛话题的查询，提供丰富的信息和创意回答。然而，由于它们在自然科学领域的知识仍不完整，并且面临严格需求和来源标准，因此其在物理科学研究中的应用仍受到限制。本文演示了如何轻松地将现有方法和软件工具结合起来，实现面向特定领域的聊天机器人。该系统能接受现有格式的科学文献，并使用文本嵌入查找来为LLM提供特定领域的上下文信息，以便在撰写回答时使用。我们同样证明了现有的图像嵌入方法可以用于跨出版物图片的搜索和检索。这些结果表明，在提供初步研究辅助知识方面，LLM已经适用于物理科学家的使用，并且进一步的开发可以扩展这些应用。

    Large language models (LLMs) have emerged as powerful machine-learning systems capable of handling a myriad of tasks. Tuned versions of these systems have been turned into chatbots that can respond to user queries on a vast diversity of topics, providing informative and creative replies. However, their application to physical science research remains limited owing to their incomplete knowledge in these areas, contrasted with the needs of rigor and sourcing in science domains. Here, we demonstrate how existing methods and software tools can be easily combined to yield a domain-specific chatbot. The system ingests scientific documents in existing formats, and uses text embedding lookup to provide the LLM with domain-specific contextual information when composing its reply. We similarly demonstrate that existing image embedding methods can be used for search and retrieval across publication figures. These results confirm that LLMs are already suitable for use by physical scientists in acc
    
[^5]: 「子集选择与信息图神经网络的相互作用」研究

    On the Interplay of Subset Selection and Informed Graph Neural Networks. (arXiv:2306.10066v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10066](http://arxiv.org/abs/2306.10066)

    该研究探讨了在QM9数据集中，采用基于领域知识的数据采样方法来选择有效的训练集，再与信息图神经网络相结合，以最大化分子多样性和相关性来提高性能。

    

    机器学习技术结合海量数据集的可用性显著提高了我们探究化合物空间的能力，通过快速准确地预测分子性质。然而，大数据集的学习受到计算资源的限制，在某些情况下可能是不可行的。此外，数据集中的实例可能还没有被标记，生成标记可能成本高昂，例如量子化学计算。因此，有必要从大型未标记数据点池中选择小训练子集，并开发可靠的机器学习方法，以有效地从小训练集中学习。本文集中于预测 QM9 数据集中分子的原子化能。我们研究了采用基于领域知识的数据采样方法来进行有效训练集选择与通知机器学习技术相结合的优势。特别是，我们展示了如何最大化分子的多样性和相关性，从而提高了信息图神经网络的性能，这是一种专门为分子数据设计的机器学习算法。

    Machine learning techniques paired with the availability of massive datasets dramatically enhance our ability to explore the chemical compound space by providing fast and accurate predictions of molecular properties. However, learning on large datasets is strongly limited by the availability of computational resources and can be infeasible in some scenarios. Moreover, the instances in the datasets may not yet be labelled and generating the labels can be costly, as in the case of quantum chemistry computations. Thus, there is a need to select small training subsets from large pools of unlabelled data points and to develop reliable ML methods that can effectively learn from small training sets. This work focuses on predicting the molecules atomization energy in the QM9 dataset. We investigate the advantages of employing domain knowledge-based data sampling methods for an efficient training set selection combined with informed ML techniques. In particular, we show how maximizing molecular
    
[^6]: 驯服扩散模型生成音乐驱动的指挥动作

    Taming Diffusion Models for Music-driven Conducting Motion Generation. (arXiv:2306.10065v1 [eess.AS])

    [http://arxiv.org/abs/2306.10065](http://arxiv.org/abs/2306.10065)

    本文提出了Diffusion-Conductor，一种基于DDIM的方法，用于音乐驱动的指挥运动生成，利用扩散模型整合到一个两阶段学习框架中，并应用随机屏蔽策略和一对几何损失函数来提高运动多样性和鲁棒性。

    

    从交响乐中生成指挥家的动作是一项具有挑战性的任务，需要学习语义音乐特征并捕捉真实指挥动作的潜在分布。先前的工作已经将生成对抗网络（GAN）应用于此任务，但前景光追迹的模型在训练稳定性和输出质量方面显示出优势，但在此上下文中还未被利用。本文提出了Diffusion-Conductor，一种基于DDIM的新颖方法，用于音乐驱动的指挥运动生成，将扩散模型整合到一个两阶段学习框架中。我们进一步提出了随机屏蔽策略以提高特征的鲁棒性，并使用一对几何损失函数施加附加规则化和增加运动多样性。我们还设计了几个新的度量标准，包括Frechet Gesture Distance（FGD）和Beat Consistency Score（BC），以进行更全面的评估。

    Generating the motion of orchestral conductors from a given piece of symphony music is a challenging task since it requires a model to learn semantic music features and capture the underlying distribution of real conducting motion. Prior works have applied Generative Adversarial Networks (GAN) to this task, but the promising diffusion model, which recently showed its advantages in terms of both training stability and output quality, has not been exploited in this context. This paper presents Diffusion-Conductor, a novel DDIM-based approach for music-driven conducting motion generation, which integrates the diffusion model to a two-stage learning framework. We further propose a random masking strategy to improve the feature robustness, and use a pair of geometric loss functions to impose additional regularizations and increase motion diversity. We also design several novel metrics, including Frechet Gesture Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive evaluati
    
[^7]: 面向教育的社交生成人工智能：理论，实践和伦理学

    Towards social generative AI for education: theory, practices and ethics. (arXiv:2306.10063v1 [cs.CY])

    [http://arxiv.org/abs/2306.10063](http://arxiv.org/abs/2306.10063)

    本文探讨如何构建面向教育的社交生成人工智能系统，该系统需要强大的人工智能，并具备对互联网资源的访问和贡献能力，但同时还需要认识到自己的局限性，对学生负责并尊重人类价值观。

    

    本文探讨人与人工智能之间的教育交互，不是作为提示和响应的序列，而是作为对话和探索的社交过程。在这种构思下，学习者在互联网工具和资源的动态计算环境中不断与AI语言模型交流。当这个分布式系统设定目标、从数据中建立意义、巩固理解、调和差异并将知识传递到新的领域时，学习就会发生。构建面向教育的社交生成人工智能需要开发强大的人工智能系统，这些系统可以彼此交谈，构建外部表示（如知识地图），访问和贡献互联网资源，并担任教师、学习者、导师和指导。这引出了基本的伦理问题。这些系统应该认识到他们的局限性，对学习者和互联网的完整性负责，并尊重人类价值观。

    This paper explores educational interactions involving humans and artificial intelligences not as sequences of prompts and responses, but as a social process of conversation and exploration. In this conception, learners continually converse with AI language models within a dynamic computational medium of internet tools and resources. Learning happens when this distributed system sets goals, builds meaning from data, consolidates understanding, reconciles differences, and transfers knowledge to new domains. Building social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors. This raises fundamental problems of ethics. Such systems should be aware of their limitations, their responsibility to learners and the integrity of the internet, and their respect for hum
    
[^8]: 揭示语言模型能力的结构

    Revealing the structure of language model capabilities. (arXiv:2306.10062v1 [cs.CL])

    [http://arxiv.org/abs/2306.10062](http://arxiv.org/abs/2306.10062)

    本文研究了大规模语言模型的能力结构，发现这些模型不是单一能力，而是由推理、理解和核心语言建模等三个明确定义的因素组成，并且这三个能力可以解释模型性能中的大部分方差。

    

    建立大规模语言模型（LLMs）能力的理论理解对于我们预测和解释这些系统的行为至关重要。在这里，我们通过从各种LLMs的个体差异模式中提取潜在能力来调查LLMs能力的结构。使用贝叶斯和频率因子分析的组合，我们分析了来自29个不同LLMs的27种认知任务的数据。我们发现，LLMs能力并非单一的，相反，它们更好地由三个明确定义的因素解释，分别代表推理、理解和核心语言建模。此外，我们发现这三个因素可以解释模型性能中的高比例方差。这些结果揭示了不同LLMs能力的一致结构，并展示了这些能力的多方面性质。我们还发现这三个功能与模型属性具有不同的关系。

    Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model prope
    
[^9]: 代理、系统和服务集成的本体论：OASIS 2 版本

    The Ontology for Agents, Systems and Integration of Services: OASIS version 2. (arXiv:2306.10061v1 [cs.AI])

    [http://arxiv.org/abs/2306.10061](http://arxiv.org/abs/2306.10061)

    本文介绍了 OASIS 2 本体论，一种为代理提供语义表示和通信的行为主义方法。该本体论已应用于区块链及其他领域。

    

    语义表示是多个应用领域的关键方法，其中多智能体系统领域也不例外。在为代理进行语义表示的方法中，一个基本实现手段是采取行为主义视角，通过描述智能体如何操作并与其同行互动。这种方法主要旨在通过与任务完成相关的心理状态来定义代理的操作能力。OASIS 本体论（一种代理、系统和服务集成的本体论），于2019年提出了这种行为主义方法，以提供智能代理及其承诺的语义表示系统和通信协议。本文报告了 OASIS 2 中有关代理表示的主要建模选择，这是 OASIS 的最新重大升级版本，并介绍了自其首次引入以来本体论在区块链本体论等领域中的应用及成就。

    Semantic representation is a key enabler for several application domains, and the multi-agent systems realm makes no exception. Among the methods for semantically representing agents, one has been essentially achieved by taking a behaviouristic vision, through which one can describe how they operate and engage with their peers. The approach essentially aims at defining the operational capabilities of agents through the mental states related with the achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and Integration of Services, presented in 2019 -- pursues the behaviouristic approach to deliver a semantic representation system and a communication protocol for agents and their commitments. This paper reports on the main modeling choices concerning the representation of agents in OASIS 2, the latest major upgrade of OASIS, and the achievement reached by the ontology since it was first introduced, in particular in the context of ontologies for blockchains.
    
[^10]: 平滑粗糙的边缘：评估自动生成的复合晶格转换

    Smoothing the Rough Edges: Evaluating Automatically Generated Multi-Lattice Transitions. (arXiv:2306.10055v1 [cs.LG])

    [http://arxiv.org/abs/2306.10055](http://arxiv.org/abs/2306.10055)

    本论文描述和评估了使用变分自编码器自动创建过渡晶格单元的方法，以解决多晶格结构中不同拓扑的单元晶格之间平滑转换的问题。

    

    添加制造技术在满足复杂设计要求的同时，生产轻量级的部件上具有非常大的优势。引入单元晶格单元和这些单元的渐变可以极大地增强这种能力。在部件加载在不同情况下变化的情况下，使用多个不同的单元晶格类型可能是有益的，这将导致多晶格结构。在这种结构中，单元晶格拓扑之间的突然转换可能会导致应力集中。

    Additive manufacturing is advantageous for producing lightweight components while addressing complex design requirements. This capability has been bolstered by the introduction of unit lattice cells and the gradation of those cells. In cases where loading varies throughout a part, it may be beneficial to use multiple, distinct lattice cell types, resulting in multi-lattice structures. In such structures, abrupt transitions between unit cell topologies may cause stress concentrations, making the boundary between unit cell types a primary failure point. Thus, these regions require careful design in order to ensure the overall functionality of the part. Although computational design approaches have been proposed, smooth transition regions are still difficult to achieve, especially between lattices of drastically different topologies. This work demonstrates and assesses a method for using variational autoencoders to automate the creation of transitional lattice cells, examining the factors
    
[^11]: 通过人工智能引起的艺术实践的转变

    A Shift In Artistic Practices through Artificial Intelligence. (arXiv:2306.10054v1 [cs.CY])

    [http://arxiv.org/abs/2306.10054](http://arxiv.org/abs/2306.10054)

    人工智能模型生成的内容突破了艺术、音乐和媒体领域，引发了文化转变。它通过改变人们的角色、转变价值观以及挑战传统实践方式，为艺术的未来打开了新的可能性。

    

    由人工智能模型生成的大量内容的爆炸引发了艺术、音乐和媒体领域的文化转变，角色变化、价值观转变和传统受到挑战。互联网上可获得的广阔数据集为人工智能模型的训练创造了一个环境。AI模型的公开共享和全球使用，如何挑战艺术实践中的现状？AI技术将给音乐、艺术和新媒体带来什么样的变革？

    The explosion of content generated by Artificial Intelligence models has initiated a cultural shift in arts, music, and media, where roles are changing, values are shifting, and conventions are challenged. The readily available, vast dataset of the internet has created an environment for AI models to be trained on any content on the web. With AI models shared openly, and used by many, globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring into music, arts, and new media?
    
[^12]: NFT到MARS：面向NFT的多注意力推荐系统

    NFTs to MARS: Multi-Attention Recommender System for NFTs. (arXiv:2306.10053v1 [cs.IR])

    [http://arxiv.org/abs/2306.10053](http://arxiv.org/abs/2306.10053)

    本文提出了首个具有三大关键特征——图形注意力、多模态注意力和多任务学习的面向NFT的多注意力推荐系统(NFT-MARS)，以解决NFT市场挑战。

    

    推荐系统已成为增强各个领域用户体验的必备工具。尽管针对电影、音乐和电子商务的推荐系统已进行了广泛研究，但日益增长和经济意义重大的非同质化代币（NFT）市场仍未被充分探索。NFT市场的独特特性和日益突出的地位凸显了开发专门针对其需求的定制推荐系统的重要性，并揭示其充分潜力。本文分析了NFT的独特特性，并提出了首个专门设计以应对NFT市场挑战的推荐系统。具体而言，我们开发了一种面向NFT的多注意力推荐系统(NFT-MARS)，具有三个关键特征：(1)图形注意力以处理稀疏的用户-项目交互;(2)多模态注意力以融入用户的特征偏好;(3)多任务学习来考虑NFT作为艺术作品和数字资产的双重性质。

    Recommender systems have become essential tools for enhancing user experiences across various domains. While extensive research has been conducted on recommender systems for movies, music, and e-commerce, the rapidly growing and economically significant Non-Fungible Token (NFT) market remains underexplored. The unique characteristics and increasing prominence of the NFT market highlight the importance of developing tailored recommender systems to cater to its specific needs and unlock its full potential. In this paper, we examine the distinctive characteristics of NFTs and propose the first recommender system specifically designed to address NFT market challenges. In specific, we develop a Multi-Attention Recommender System for NFTs (NFT-MARS) with three key characteristics: (1) graph attention to handle sparse user-item interactions, (2) multi-modal attention to incorporate feature preference of users, and (3) multi-task learning to consider the dual nature of NFTs as both artwork and
    
[^13]: 分配 AI：为学生提供的七种方法和提示。

    Assigning AI: Seven Approaches for Students, with Prompts. (arXiv:2306.10052v1 [cs.CY])

    [http://arxiv.org/abs/2306.10052](http://arxiv.org/abs/2306.10052)

    提出了七种利用 AI 在课堂上的方法：AI-导师、AI-辅导员、AI-指导员、AI-队友、AI-工具、AI-模拟器和AI-学生。这些策略促进对 AI 输出的批判性评估，以及利用 AI 的能力和学生独特见解的互补性，增强学习结果。此框架为整合 AI 辅助学习提供了指导。

    

    本文探讨了大型语言模型（LLMs）在教育中的变革作用及其作为学习工具的潜力，尽管存在固有的风险和限制。作者提出了七种利用 AI 在课堂上的方法：AI-导师、AI-辅导员、AI-指导员、AI-队友、AI-工具、AI-模拟器和AI-学生，每种方法都有独特的教学益处和风险。旨在帮助学生学习与了解 AI，提供实用的策略来缓解风险，如对 AI 输出的满足感、错误和偏见。这些策略促进主动监督、对 AI 输出的批判性评估，以及利用 AI 的能力与学生独特见解的互补性。通过挑战学生保持“人在环节”，作者旨在提高学习结果，同时确保 AI 作为支持性工具，而非替代品。提出的框架为教育工作者提供了指导，引导其在课堂中整合 AI 辅助学习。

    This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights. By challenging students to remain the "human in the loop," the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms
    
[^14]: 基于邻域的难例挖掘用于序列推荐

    Neighborhood-based Hard Negative Mining for Sequential Recommendation. (arXiv:2306.10047v1 [cs.IR])

    [http://arxiv.org/abs/2306.10047](http://arxiv.org/abs/2306.10047)

    本文提出了一种基于邻域重叠的基于图形的负采样方法（GNNO），利用隐藏在用户行为中的结构信息进行负采样，用于增进序列推荐模型的训练和性能。

    

    在训练序列推荐模型时，负采样在其中起着关键作用。目前已经提出了许多策略来挖掘信息丰富的负样本，以强化训练和性能。但是，很少有这些方法利用结构信息。本文观察到在训练过程中，不同程度邻域重叠的不同组节点对相似度的分布发生显着变化，这表明不同组中的项目对可能具有不同的负关系。受此观察的启发，我们提出了一种基于邻域重叠的基于图的负采样方法（GNNO）来利用隐藏在用户行为中的结构信息进行负采样。GNNO首先使用训练序列构建全局加权项目转换图。随后，它根据与目标项的重叠程度来挖掘难例负样本。

    Negative sampling plays a crucial role in training successful sequential recommendation models. Instead of merely employing random negative sample selection, numerous strategies have been proposed to mine informative negative samples to enhance training and performance. However, few of these approaches utilize structural information. In this work, we observe that as training progresses, the distributions of node-pair similarities in different groups with varying degrees of neighborhood overlap change significantly, suggesting that item pairs in distinct groups may possess different negative relationships. Motivated by this observation, we propose a Graph-based Negative sampling approach based on Neighborhood Overlap (GNNO) to exploit structural information hidden in user behaviors for negative mining. GNNO first constructs a global weighted item transition graph using training sequences. Subsequently, it mines hard negative samples based on the degree of overlap with the target item on
    
[^15]: 一种用于科学文献中表格实体链接的实用系统。

    A Practical Entity Linking System for Tables in Scientific Literature. (arXiv:2306.10044v1 [cs.IR])

    [http://arxiv.org/abs/2306.10044](http://arxiv.org/abs/2306.10044)

    本文介绍了一个用于将实体链接到知识库中的通用系统，并将其适应于链接特定领域的实体，特别是 COVID-19 相关科学文献中的嵌入式实体。通过利用表格的结构和语义特征，以提高整体实体链接性能。

    

    实体链接是构建知识图谱的重要步骤，可以方便地回答包括从这些文档中检索相关信息在内的高级问题。本文介绍了一种通用的系统，用于将实体链接到维基数据知识库中的项。它描述了如何适应该系统以链接领域特定的实体，特别是那些来自COVID-19相关科学文献中的嵌入式实体。我们描述了系统的离线实例的设置，使我们的实体链接方法在实践中更加可行。作为推断科学表格的语义含义的更广泛方法的一部分，我们利用表格的结构和语义特征来提高整体实体链接性能。

    Entity linking is an important step towards constructing knowledge graphs that facilitate advanced question answering over scientific documents, including the retrieval of relevant information included in tables within these documents. This paper introduces a general-purpose system for linking entities to items in the Wikidata knowledge base. It describes how we adapt this system for linking domain-specific entities, especially for those entities embedded within tables drawn from COVID-19-related scientific literature. We describe the setup of an efficient offline instance of the system that enables our entity-linking approach to be more feasible in practice. As part of a broader approach to infer the semantic meaning of scientific tables, we leverage the structural and semantic characteristics of the tables to improve overall entity linking performance.
    
[^16]: 机器学习中异质性的交织轴解析以促进民主和包容性发展

    Unraveling the Interconnected Axes of Heterogeneity in Machine Learning for Democratic and Inclusive Advancements. (arXiv:2306.10043v1 [cs.CY])

    [http://arxiv.org/abs/2306.10043](http://arxiv.org/abs/2306.10043)

    本论文探讨了机器学习应用中的异质性问题，从价值观、数据组成和资源基础设施三个角度入手并指出它们相互依存，需要一同考虑解决。同时，本文还揭示了现有机器学习研究存在的问题，如权力集中和依赖性增加。

    

    机器学习在决策过程中的广泛应用引发了有关其对社会的利益的问题。本研究指出并分析了三个明显影响机器学习产品发展轨迹的异质性轴，它们是：价值观、文化和法规、数据组成以及资源和基础设施能力。我们展示了这些轴如何相互依存并相互影响，强调需要共同考虑和解决它们的必要性。不幸的是，目前的研究景观在这方面还有所不足，往往未能采用整体性方法。我们检查了偏向少数人的支配性偏见和方法学，以及由此导致的权力集中、同质化控制和增加的依赖性。我们讨论了这种三个轴的片面研究构成的显著挑战，导致一个缺乏反映现实情况的实际解荐空间。

    The growing utilization of machine learning (ML) in decision-making processes raises questions about its benefits to society. In this study, we identify and analyze three axes of heterogeneity that significantly influence the trajectory of ML products. These axes are i) values, culture and regulations, ii) data composition, and iii) resource and infrastructure capacity. We demonstrate how these axes are interdependent and mutually influence one another, emphasizing the need to consider and address them jointly. Unfortunately, the current research landscape falls short in this regard, often failing to adopt a holistic approach. We examine the prevalent practices and methodologies that skew these axes in favor of a selected few, resulting in power concentration, homogenized control, and increased dependency. We discuss how this fragmented study of the three axes poses a significant challenge, leading to an impractical solution space that lacks reflection of real-world scenarios. Addressi
    
[^17]: 一种用于Aspect Sentiment Triplet Extraction的配对增强方法

    A Pairing Enhancement Approach for Aspect Sentiment Triplet Extraction. (arXiv:2306.10042v1 [cs.IR])

    [http://arxiv.org/abs/2306.10042](http://arxiv.org/abs/2306.10042)

    本文提出了一种配对增强方法，通过对比学习将方面-意见配对知识注入到Aspect Sentiment Triplet Extraction（ASTE）模型中，提高了三元组提取的准确性和性能。

    

    Aspect Sentiment Triplet Extraction（ASTE）旨在从评论文本中提取一个方面术语、一个意见术语和它们相应的情感极性的三元组。由于语言的复杂性和单个句子中存在多个方面术语和意见术语，当前的模型经常会混淆描述它的方面术语和意见术语之间的联系。为了解决这个问题，我们提出了一种配对增强方法，它在训练阶段采用对比学习，将方面-意见配对知识注入到三元组提取模型中。实验结果表明，与几种相关经典和最先进的三元组提取方法相比，我们的方法在四个ASTE数据集（即14lap，14res，15res和16res）上表现良好。此外，消融研究进行分析并验证了对比学习相比其他配对增强方法的优势。

    Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplet of an aspect term, an opinion term, and their corresponding sentiment polarity from the review texts. Due to the complexity of language and the existence of multiple aspect terms and opinion terms in a single sentence, current models often confuse the connections between an aspect term and the opinion term describing it. To address this issue, we propose a pairing enhancement approach for ASTE, which incorporates contrastive learning during the training stage to inject aspect-opinion pairing knowledge into the triplet extraction model. Experimental results demonstrate that our approach performs well on four ASTE datasets (i.e., 14lap, 14res, 15res and 16res) compared to several related classical and state-of-the-art triplet extraction methods. Moreover, ablation studies conduct an analysis and verify the advantage of contrastive learning over other pairing enhancement approaches.
    
[^18]: 用一致性检查评估超人模型

    Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])

    [http://arxiv.org/abs/2306.09983](http://arxiv.org/abs/2306.09983)

    本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。

    

    如果机器学习模型在各种推理或决策任务上实现了超人能力，那么我们该如何评估这些模型，考虑到人类代理会产生偏差? 在本文中，我们提出了一个用一致性检查评估超人模型的框架。我们的前提是，虽然评估超人决策的正确性可能是不可能的，但是如果模型的决策未能满足某些逻辑上、可解释的规则，我们仍然可以发现错误。我们将我们的框架实现在三个任务上，这些任务的决策正确性由于超人模型能力或其他缺乏基本事实而难以评估：评估国际象棋局面、预测未来事件和作出法律判断。我们表明，无论模型在这些任务上的表现如何(可能是超人的)，我们都能发现决策制定中的逻辑不一致性。例如：国际象棋引擎给出对局中棋子相对估值的不同排列。

    If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
    
[^19]: 揭秘 GPT 自我修复代码生成能力

    Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])

    [http://arxiv.org/abs/2306.09896](http://arxiv.org/abs/2306.09896)

    本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。

    

    大型语言模型 (LLM) 在代码生成方面表现出色，但在挑战性编程任务上仍面临困难。自我修复——即模型调试并修复自己的代码——最近成为提高性能的一种流行方式。然而，关于自我修复如何有效地发挥作用的研究还非常有限。有人会想知道，当同一模型生成代码时，模型究竟能否提供准确的反馈。在本文中，我们分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，这是一个由多种编码挑战组成的具有挑战性的数据集。我们首先建立了一种新的评估策略 pass@t，该策略衡量了任务通过率与从模型中抽样的总标记数，从而实现对仅基于抽样的方法的公平比较。通过这种评估策略，我们发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，并确定了影响自我修复表现的几个因素。具体而言，我们发现，在输入噪声较少且模型对初始输出不太自信的较短和较简单的任务中，自我修复效果更好。我们还表明，仅在某些代码部分上应用自我修复可以非常有效。此外，我们提出了一种新的引导修复方法，利用外部反馈来增强 GPT 模型的自我修复能力，在 APPS 数据集上获得性能提升。

    Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
    
[^20]: 可泛化的一次性绳索操作策略及其参数感知性。

    Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])

    [http://arxiv.org/abs/2306.09872](http://arxiv.org/abs/2306.09872)

    GenORM通过增加可变形绳索参数和使用各种可变形绳索的模拟训练操作策略，实现利用一次真实演示处理不同可形变绳索，从而节省演示时间和提高适用性。

    

    以绳索在运动过程中的固有不确定性为因素，以往绳索操作方法往往需要数百次真实演示来为每个绳索训练操作策略，即使是简单的“到达目标”任务，这限制了它们在我们不断变化的世界中的应用。为了解决这个问题，我们介绍了GenORM，一个框架，它可以让操作策略通过一次真实演示就可以处理不同可形变的绳索。我们通过在策略上增加可变形绳索参数并使用各种模拟可变形绳索来训练它，使策略能够根据不同的绳索参数调整行动。在推断时，GenORM通过最小化真实演示和模拟点云的网格密度差异来估计可变形绳索参数。通过可微分物理模拟器的帮助，我们仅需要一次演示数据就可以处理不同的绳索。

    Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
    
[^21]: 实用联邦因果结构学习之路

    Towards Practical Federated Causal Structure Learning. (arXiv:2306.09433v1 [cs.LG])

    [http://arxiv.org/abs/2306.09433](http://arxiv.org/abs/2306.09433)

    为了解决联邦学习条件下的因果结构学习难题，提出了一种基于联邦条件独立性检验的因果结构学习方案FedC2SL，无需收集原始数据且对数据变异具有更强的抵抗力。

    

    理解因果关系对于科学发现至关重要。因果结构学习的过程涉及从观测数据中识别因果图以理解这种关系。通常，一个中央服务器执行此任务，但与服务器共享数据会带来隐私风险。联邦学习可以解决这个问题，但现有的联邦因果结构学习解决方案对数据做出了不切实际的假设，并缺乏收敛保证。FedC2SL是一种联邦基于约束的因果结构学习方案，它使用联邦条件独立性检验来学习因果图，该检验在不收集客户端原始数据的情况下检查两个变量在一组条件下的条件独立性。FedC2SL对数据做出了更弱和更现实的假设，并更强地抵御了客户端之间的数据变异。FedPC和FedFCI是FedC2SL的两个变体，用于因果充分性和因果不充分性情况下的因果结构学习。

    Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insuffic
    
[^22]: 基于时空扩展图神经网络的人类移动模拟

    Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])

    [http://arxiv.org/abs/2306.09381](http://arxiv.org/abs/2306.09381)

    STAR框架通过设计多种时空图来捕捉位置的动态时空效应，为人类移动模拟任务提供新的方法。

    

    人类移动模式在政策决策和经济行为研究中有着重要的应用。人类移动模拟任务旨在给定一小组轨迹数据生成人类移动轨迹，但由于人类移动数据的稀缺性和稀疏性，引起了广泛关注。现有方法大多依赖于地点之间的静态关系，而很大程度上忽略了位置的动态时空效应。因此，我们提出了一种新的框架，即SpatioTemporal-Augmented gRaph神经网络（STAR），来模拟位置的动态时空效应。

    Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
    
[^23]: 公平的多任务学习

    Equitable Multi-task Learning. (arXiv:2306.09373v1 [cs.LG])

    [http://arxiv.org/abs/2306.09373](http://arxiv.org/abs/2306.09373)

    该论文提出了一种名为EMTL的多任务优化方法，以实现公平的多任务学习。通过规范化不同任务的相对贡献，可以提高MTL的泛化性能，并利用方差正则化和高效的优化算法保证收敛。实验证明，该方法在合成和真实数据集上均表现出了更好的性能。

    

    多任务学习（MTL）在各个研究领域（如计算机视觉、自然语言处理和信息检索等）中取得了巨大成功。但是，由于任务之间存在复杂且相互竞争的相关性，单纯地训练所有任务可能会导致不公平的学习，即一些任务被很好地学习，而其他任务则被忽视。多任务优化（MTO）旨在同时提高所有任务的表现，但传统方法往往在任务损失规模或梯度范数差异较大的情况下表现不佳。为了解决这个问题，我们深入研究了MTL的公平性问题，并发现在更新共享参数时，规范化不同任务的相对贡献（即任务特定损失值除以其原始梯度范数的值）可以提高MTL的泛化性能。基于我们的理论分析，我们提出了一种新的多任务优化方法，名为EMTL，以实现公平的MTL。具体来说，我们有效地添加了方差正则化，使不同任务的相对贡献更具可比性，并开发了一种高效的优化算法来保证收敛。我们在合成和真实数据集上进行了大量实验，结果表明了我们方法的有效性和优越性。

    Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\"ive training all tasks may lead to inequitable learning, \textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribu
    
[^24]: 语言转奖励：用于机器人技能综合的方法

    Language to Rewards for Robotic Skill Synthesis. (arXiv:2306.08647v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.08647](http://arxiv.org/abs/2306.08647)

    该研究介绍了一种将大型语言模型用于定义奖励参数，并通过奖励函数进行优化，从而使机器人可以执行各种自然语言指令指定的任务的新方法。

    

    大型语言模型已经取得了许多令人兴奋的进展，从逻辑推理到代码编写等，展现了在情境学习中获得多种新能力的潜力。机器人学研究人员也探索使用大型语言模型来提高机器人控制的能力。然而，由于低级机器人动作取决于硬件并且在大型语言模型的训练语料库中所占的比重较小，因此在机器人学中应用大型语言模型的现有努力主要将其视为语义规划器，或依赖于人工控制原语与机器人进行交互。另一方面，奖励函数被证明是可以灵活表示并且可以被优化以实现多种任务的控制策略，其语义丰富性使其适合由大型语言模型来指定。在本文中，我们引入了一种新的方法，通过利用大型语言模型来定义可以被优化的奖励参数并完成各种机器人任务。使用奖励作为中间介质，我们提出的方法使机器人能够执行各种由自然语言指令指定的任务，而无需人类在设计行为原语方面付出努力。我们在仿真环境中展示了我们的方法在拾取物品和搭建塔两个任务上的有效性。

    Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate inter
    
[^25]: 统一大型语言模型和知识图谱: 一条路线图

    Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08302](http://arxiv.org/abs/2306.08302)

    本文提出了一个前瞻性的统一大型语言模型和知识图谱的路线图，通过三个框架：增强KGs的LLMs，知识增强KGs和LLMs与KGs的联合推理，综合利用两者的优点。

    

    大型语言模型（LLM）如ChatGPT和GPT4正在自然语言处理和人工智能领域掀起新的热潮，由于它们的突现能力和一般化能力。然而，LLM是黑盒模型，往往不能捕捉和获取实际知识。相比之下，知识图谱（KGs）如维基百科和华普则是明确存储丰富实际知识的结构化知识模型。KGs可以通过为推理和可解释性提供外部知识来增强LLMs。同时，KGs的构建困难，自然而然地演化，这挑战了现有的KGs方法来生成新事实并表示未见过的知识。因此，统一LLMs和KGs并同时利用它们的优点是有益的。本文介绍了一个前瞻性的统一LLMs和KGs的路线图。我们的路线图包括三个一般框架，即1）增强KGs的LLMs，它们将知识表示为LM的一部分，从而能够捕捉丰富的实体关系，2）知识增强KGs，它们将LLMs用作知识表示学习的优秀工具，3）LLMs与KGs的联合推理，其中LLMs和KGs相互增强，从而获得更准确的推理模型。

    Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorpo
    
[^26]: 数字病理学中人工智能的诊断测试准确度：系统综述、Meta分析和质量评估

    Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment. (arXiv:2306.07999v1 [physics.med-ph])

    [http://arxiv.org/abs/2306.07999](http://arxiv.org/abs/2306.07999)

    本文进行了数字病理图像中应用人工智能的所有病理学领域的诊断准确度的系统综述和Meta分析。结果表明，人工智能在数字病理学中取得了高度的准确度，是可行的辅助诊断工具。

    

    确保临床使用之前AI模型的诊断表现是关键，以确保这些技术的安全和成功的采用。近年来，报道应用于数字病理学图像进行诊断目的的AI研究数量迅速增加。本研究旨在提供数字病理学中AI的诊断准确度的概述，涵盖了所有病理学领域。这项系统性综述和Meta分析包括使用任何类型的人工智能应用于任何疾病类型的WSI图像的诊断准确性研究。参考标准是通过组织病理学评估和/或免疫组化诊断。搜索在2022年6月在PubMed、EMBASE和CENTRAL中进行。在2976项研究中，有100项纳入综述，48项纳入完整的Meta分析。使用QUADAS-2工具评估了偏倚风险和适用性的关注点。数据提取由两个调查员进行，并进行了Meta分析。

    Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analy
    
[^27]: 一种基于云的机器学习管道，有效从客户评论中提取见解

    A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v1 [cs.CL])

    [http://arxiv.org/abs/2306.07786](http://arxiv.org/abs/2306.07786)

    本文介绍了一种基于云的系统，利用机器学习方法从客户评论中提取见解。本研究提出的组合模型使用了转换器神经网络、向量嵌入和聚类，已经集成并进一步发展，以更好地满足高效信息提取、提取信息的主题建模和用户需求的要求。研究结果表明，本系统可以比现有的主题建模和关键字提取解决方案获得更好的效果。

    

    随着机器学习模型的出现，特别是基于神经网络的解决方案，自然语言处理的效率有了显著提高。然而，一些任务仍然具有挑战性，特别是考虑到特定的应用领域。本文提出了一种基于云的系统，可以使用机器学习方法集成到管道中，从客户评论中提取见解。对于主题建模，我们的组合模型使用了基于转换器的自然语言处理神经网络、基于向量嵌入的关键字提取和聚类。我们的模型元素已经集成并进一步发展，以更好地满足高效信息提取、提取信息的主题建模和用户需求的要求。此外，我们的系统可以比这个任务现有的主题建模和关键字提取解决方案获得更好的结果。我们的方法在公开可用的客户评论数据集上得到验证并与其他最先进的方法进行比较。

    The efficiency of natural language processing has improved dramatically with the advent of machine learning models, particularly neural network-based solutions. However, some tasks are still challenging, especially when considering specific domains. In this paper, we present a cloud-based system that can extract insights from customer reviews using machine learning methods integrated into a pipeline. For topic modeling, our composite model uses transformer-based neural networks designed for natural language processing, vector embedding-based keyword extraction, and clustering. The elements of our model have been integrated and further developed to meet better the requirements of efficient information extraction, topic modeling of the extracted information, and user needs. Furthermore, our system can achieve better results than this task's existing topic modeling and keyword extraction solutions. Our approach is validated and compared with other state-of-the-art methods using publicly a
    
[^28]: SHAP解释的持续解释协议

    A Protocol for Continual Explanation of SHAP. (arXiv:2306.07218v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07218](http://arxiv.org/abs/2306.07218)

    本文研究了在持续学习中SHAP值解释的稳定性问题，提出了一种评估协议，并指出随机循环模型是更有效的备选循环方法。

    

    持续学习旨在在数据流中训练模型，以学习新信息而不会忘记先前的知识。鉴于这种环境的动态性质，解释这些模型的预测可能具有挑战性。我们研究了 SHAP 值解释在持续学习中的行为，并提出了一种评估协议，以可靠地评估逐类增量场景中解释的变化。我们观察到，虽然重放策略可以强制前馈/卷积模型中的 SHAP 值的稳定性，但它们无法在完全训练的循环模型中做到这一点。我们表明，像随机循环模型这样的备选循环方法在随时间保持解释稳定方面更有效。

    Continual Learning trains models on a stream of data, with the aim of learning new information without forgetting previous knowledge. Given the dynamic nature of such environments, explaining the predictions of these models can be challenging. We study the behavior of SHAP values explanations in Continual Learning and propose an evaluation protocol to robustly assess the change of explanations in Class-Incremental scenarios. We observed that, while Replay strategies enforce the stability of SHAP values in feedforward/convolutional models, they are not able to do the same with fully-trained recurrent models. We show that alternative recurrent approaches, like randomized recurrent models, are more effective in keeping the explanations stable over time.
    
[^29]: 基于时空自举的相关时间序列自监督表示学习

    Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping. (arXiv:2306.06994v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06994](http://arxiv.org/abs/2306.06994)

    该论文提出了一种基于时空自举的时间步级表示学习框架，能为相关时间序列分析提供高效降维表示，并通过在PeMS-BAY数据集上的测试取得了较好的效果。

    

    相关时间序列分析在许多实际工业中起着重要作用。对大规模数据进行高效降维表示以便进行下游任务是必要但具有挑战性的。本文提出了一种基于时空自举表示预测的时间步级表示学习框架，以便为个体实例进行表示学习。我们在相关时间序列预测和将预测模型冷启动转移到数据受限的新实例方面评估了该表示学习框架的有效性和灵活性。经过训练在学习表示上的线性回归模型证明了在大多数情况下我们的模型表现最佳。特别地，在与表示学习模型的比较中，我们在PeMS-BAY数据集上将RMSE、MAE和MAPE分别减少了37％、49％和48％。在实际的地铁客流数据中，我们的框架展示了将预测能力转移到新的冷启动情况下的能力。

    Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37%, 49%, and 48% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-
    
[^30]: 确定和探索智能空间

    Defining and Explorting the Intelligence Space. (arXiv:2306.06499v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.06499](http://arxiv.org/abs/2306.06499)

    本文通过引入广泛视角来界定智能并建立了三级嵌套结构及其基础的广泛空间。利用这些定义初步探索了奇点、生成AI、伦理和知识产权等话题。

    

    尽管尝试了许多次，智能是一个难以定义的概念。本文引入了一个广泛的视角来定义智能, 提出了一系列定义，构建了三个层次的智能嵌套层次和以它们及其近似值为基础的广泛空间。在这个智能空间中，鉴别出对应于自然——尤其是人类——智能和人工智能（AI）以及类人智能的区域。然后，利用这些定义初步探索了四个更先进、可能更具争议性的话题：奇点、生成AI、伦理和知识产权。

    Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence (AI), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative AI, ethics, and intellectual property.
    
[^31]: 理解长尾效应对神经网络压缩的影响

    Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])

    [http://arxiv.org/abs/2306.06238](http://arxiv.org/abs/2306.06238)

    本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。

    

    网络压缩现在是神经网络研究的一个成熟的子领域，过去的十年中，取得了显著的进展，以减小模型尺寸和加速推断为目标，同时保持分类准确性。然而，许多研究观察到，仅关注总体准确性可能是误导的。例如，已经证明全模型和压缩模型之间的差异可能会偏向于在数据集中低频的类。这引出了一个重要的研究问题，即“我们能否在保持与原始网络语义等同的情况下实现网络压缩？”在本文中，我们研究了这个问题，重点关注计算机视觉数据集中Feldman等人观察到的“长尾”现象。他们认为，某些输入（适当定义）的记忆对于实现良好的泛化是必要的。由于压缩限制了网络的容量（因此也限制了其记忆能力），所以我们研究了这个问题：

    Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
    
[^32]: SequenceMatch：带回溯的自回归序列模型的模仿学习

    SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.05426](http://arxiv.org/abs/2306.05426)

    本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。

    

    在许多领域，自回归模型可以在预测下一个观测值的任务上获得高似然度。然而，这种最大似然（MLE）目标不一定与自回归生成高质量序列的下游用例相匹配。MLE目标按照数据分布下序列的频率加权，不提供模型在分布之外行为的指导，这会导致在自回归生成过程中复合误差。为了解决这个复合误差问题，我们将序列生成定为模仿学习（IL）问题。这使我们可以最小化自回归模型生成的序列分布和数据集序列之间的各种分歧，包括考虑出分布序列的分歧。IL框架还允许我们通过在生成过程中引入回格动作来引入回溯。这进一步减轻了复合效应。

    In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
    
[^33]: 学习空间数据分区

    Learned spatial data partitioning. (arXiv:2306.04846v1 [cs.DB])

    [http://arxiv.org/abs/2306.04846](http://arxiv.org/abs/2306.04846)

    本文介绍了一种通过机器学习技术学习空间数据分区的方法，在强化学习的框架下开发了深度强化学习算法。实验表明该算法能够有效地加速距离连接查询，缩短工作负载运行时间高达59.4％。

    

    随着空间数据的大小显著增加，使用分布式并行处理系统有效地分析空间数据变得至关重要。本文首先研究了学习空间数据分区，该方法使用机器学习技术通过基于数据位置的分组将大规模空间数据分配到计算机上。我们在强化学习的上下文中形式化了空间数据分区，并开发了一种新颖的深度强化学习算法。我们的学习算法利用空间数据分区的特征，并剪枝无效的学习过程，以有效地找到最优分区。我们的实验研究使用Apache Sedona和真实的空间数据，证明了我们的方法能够有效地找到分区，加速距离连接查询，并将工作负载运行时间缩短了59.4％。

    Due to the significant increase in the size of spatial data, it is essential to use distributed parallel processing systems to efficiently analyze spatial data. In this paper, we first study learned spatial data partitioning, which effectively assigns groups of big spatial data to computers based on locations of data by using machine learning techniques. We formalize spatial data partitioning in the context of reinforcement learning and develop a novel deep reinforcement learning algorithm. Our learning algorithm leverages features of spatial data partitioning and prunes ineffective learning processes to find optimal partitions efficiently. Our experimental study, which uses Apache Sedona and real-world spatial data, demonstrates that our method efficiently finds partitions for accelerating distance join queries and reduces the workload run time by up to 59.4%.
    
[^34]: 医学图像的自动化机器学习系统

    AutoML Systems For Medical Imaging. (arXiv:2306.04750v1 [cs.AI])

    [http://arxiv.org/abs/2306.04750](http://arxiv.org/abs/2306.04750)

    该论文介绍了医学成像中自动化机器学习的应用、策略和技术，该方法通过神经结构搜索和迁移学习技术简化了图像识别模型的创建，结合人类专业知识和计算机系统可以提高医学图像分析的精度和质量。

    

    在医学图像分析中引入机器学习可以大大提高医生提供的医疗保健服务的质量。人类专业知识和计算机系统的结合可以提高诊断精度。自动化机器学习方法通过利用神经结构搜索和迁移学习技术简化了定制化图像识别模型的创建。医学成像技术用于无创地创建内部器官和身体部位图像，以进行诊断和操作目的。本文旨在通过理论和实证证据，突出医学成像中AutoML的潜在应用、策略和技术。

    The integration of machine learning in medical image analysis can greatly enhance the quality of healthcare provided by physicians. The combination of human expertise and computerized systems can result in improved diagnostic accuracy. An automated machine learning approach simplifies the creation of custom image recognition models by utilizing neural architecture search and transfer learning techniques. Medical imaging techniques are used to non-invasively create images of internal organs and body parts for diagnostic and procedural purposes. This article aims to highlight the potential applications, strategies, and techniques of AutoML in medical imaging through theoretical and empirical evidence.
    
[^35]: 鲁棒性AI生成文本检测的内部维度估计

    Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])

    [http://arxiv.org/abs/2306.04723](http://arxiv.org/abs/2306.04723)

    本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。

    

    快速提高的AI生成内容的质量使得很难区分人类和AI生成的文本，这可能会对社会产生不良影响。因此，研究人类文本的不变属性变得越来越重要。本文提出了一种人类文本的不变特征，即给定文本样本嵌入集合下的流形的内部维度。我们展示了自然语言流畅文本的平均内部维度在几个基于字母的语言中约为 $9$，而中文约为 $7$，而每种语言的AI生成文本的平均内部维度较低，差约 $1.5$，并且有明显的统计分离。

    Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
    
[^36]: SpeechGen: 利用提示解锁语音语言模型的生成能力

    SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.02207](http://arxiv.org/abs/2306.02207)

    本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。

    

    大型语言模型（LLM）在人工智能生成内容（AIGC）中引起了相当大的关注，特别是随着ChatGPT的出现。然而，将连续语音直接适应于处理离散标记的LLM仍然是一个未解决的挑战，这妨碍了LLM在语音生成方面的应用。高级语音LM们无法充分利用语音信号所包含的丰富信息，包括说话者和情感等，这些信息仅通过文本数据无法获取。在一些语音分类任务中，简单的提示调整已经表现出明显的参数效率和竞争性能的提高。但在多大程度上提示能够有效地激发语音LM的生成任务仍然是一个未知的问题。本文提出了一项先驱性研究，该研究在称为SpeechGen的统一框架中使用提示调节来刺激语音LM进行各种生成任务，并具有约10M可训练参数。

    Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
    
[^37]: X-Ray成像、MRI和核医学的案例研究

    Case Studies on X-Ray Imaging, MRI and Nuclear Imaging. (arXiv:2306.02055v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.02055](http://arxiv.org/abs/2306.02055)

    本文介绍了X射线成像、MRI和核医学在医学成像中的应用。通过基于人工智能的方法，尤其是卷积神经网络的应用，可以实现从成像模态中进行系统特征提取和分类，帮助医生进行快速准确的诊断。

    

    医学成像领域是医学科学的一个重要方面，涉及各种形式的辐射来捕获身体内部组织和器官的图像。这些图像为临床诊断提供了重要信息，本文探讨了X射线、MRI和核医学在发现严重疾病方面的应用。然而，手动评估和存储这些图像可能是一个具有挑战性和耗时的过程。为了解决这个问题，基于人工智能（AI）的技术，特别是深度学习（DL），已经越来越流行，用于从成像模态中进行系统特征提取和分类，从而帮助医生进行快速准确的诊断。在本综述研究中，我们将重点探讨基于AI的方法，特别是卷积神经网络（CNN）如何通过医学成像技术帮助疾病检测。CNN是一种常用的图像分析方法，因其能够学习复杂的图像特征并识别传统方法难以检测的模式而获得广泛应用。通过案例研究，我们将展示基于人工智能的医学成像在临床实践中的实际应用。

    The field of medical imaging is an essential aspect of the medical sciences, involving various forms of radiation to capture images of the internal tissues and organs of the body. These images provide vital information for clinical diagnosis, and in this chapter, we will explore the use of X-ray, MRI, and nuclear imaging in detecting severe illnesses. However, manual evaluation and storage of these images can be a challenging and time-consuming process. To address this issue, artificial intelligence (AI)-based techniques, particularly deep learning (DL), have become increasingly popular for systematic feature extraction and classification from imaging modalities, thereby aiding doctors in making rapid and accurate diagnoses. In this review study, we will focus on how AI-based approaches, particularly the use of Convolutional Neural Networks (CNN), can assist in disease detection through medical imaging technology. CNN is a commonly used approach for image analysis due to its ability to
    
[^38]: 通过神经科学的视角探究人工意识的可行性

    The feasibility of artificial consciousness through the lens of neuroscience. (arXiv:2306.00915v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2306.00915](http://arxiv.org/abs/2306.00915)

    从神经科学的角度来看，目前大型语言模型难以具备哺乳动物意识感知相关的丘脑皮层系统的关键特征，缺乏周围世界的具体嵌入式信息，且当前的人工智能无法做到存在的依赖于其行为，这意味着人工意识的可行性存在瓶颈。

    

    与大型语言模型的交互引发了这些模型可能具有意识的猜测。从神经科学的角度来看，这种观点很难被证实。首先，大型语言模型的架构缺少哺乳动物意识感知相关的丘脑皮层系统的关键特征。其次，大型语言模型的输入缺乏我们与周围世界的感官接触的具有体验、嵌入式信息的特征。最后，虽然前两个论点在未来的AI系统中可以被克服，但第三个可能更难在不久的将来跨越。换言之，我们认为意识可能取决于是否在“游戏中有皮肤”，即系统的存在是否取决于其行为，而这在当前的人工智能中并不成立。

    Interactions with large language models have led to the suggestion that these models may be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Secondly, the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Finally, while the previous two arguments can be overcome in future AI systems, the third one might be harder to bridge in the near future. Namely, we argue that consciousness might depend on having 'skin in the game', in that the existence of the system depends on its actions, which is not true for present-day artificial intelligence.
    
[^39]: 双重稳健自我训练

    Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])

    [http://arxiv.org/abs/2306.00265](http://arxiv.org/abs/2306.00265)

    本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。

    

    自我训练是解决半监督学习问题的一种重要技术。它通过生成伪标签并将其与有限的标记数据集结合使用进行训练，从而利用无标签数据。自我训练的有效性在很大程度上依赖于这些伪标签的准确性。本文引入了双重稳健自我训练，这是一种新颖的半监督算法，可以保证在两个极端之间平衡。当伪标签完全不正确时，我们的方法将被减少到仅使用标记数据进行训练。相反，当伪标签完全准确时，我们的方法将变成利用所有伪标签数据和标记数据进行训练的过程，从而增加有效的样本量。通过在ImageNet图像分类和nuScenes自主驾驶数据集上的实证评估，我们证明了双重稳健损失优于标准自我训练基线的优越性。

    Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
    
[^40]: 束搜索递归单元：一种支持反向传播的递归神经网络框架

    Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19999](http://arxiv.org/abs/2305.19999)

    本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。

    

    本文提出了一种叫做束搜索递归单元（BT-Cell）的框架，用于扩展支持使用束搜索进行潜在结构感知的递归神经网络（RvNN）。我们进一步通过提出在束搜索中对硬性前k算子的放松来扩展此框架，以更好地传递梯度信号。我们在合成和实际数据的不同代表性分布上评估了我们的模型。实验结果表明，BT-Cell在多个具有挑战性的体现结构敏感性的任务（如ListOps和逻辑推理）上达到了几乎完美的性能，同时在实际数据上与其他基于RvNN的模型具有可比性的性能。此外，我们在ListOps中确定了神经模型在推广到未见过的参数数量上的未知失效案例。代码可在https://github.com/JRC1995/BeamTreeRecursiveCells上获得。

    We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
    
[^41]: MicroSegNet：一种基于深度学习的微型超声图像前列腺分割方法

    MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19956](http://arxiv.org/abs/2305.19956)

    本文提出了一种基于深度学习的微型超声图像前列腺分割方法，利用多尺度注释引导的Transformer UNet模型和注释引导的二分类交叉熵损失解决低分辨率和界限不清的挑战，该方法更加关注难以分割的区域。

    

    微型超声是一种新型的29MHz超声技术，提供比传统超声高3-4倍的分辨率，在诊断前列腺癌的准确性方面与MRI相当，但成本更低。然而，由于低分辨率和前列腺、膀胱和尿道中线之间的界限不清，基于微型超声的前列腺分割具有挑战性。本文提出了MicroSegNet，这是一个特别设计用于解决这些挑战的多尺度注释引导的Transformer UNet模型。在训练过程中，MicroSegNet更加关注难以分割（难区域）的区域，这些区域具有专家和非专家注释之间的差异。为此，我们提出了注释引导的二分类交叉熵（AG-BCE）损失，它在难区域中给预测误差分配更大的权重和较低的权重。

    Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that provides 3-4 times higher resolution than traditional ultrasound, delivering comparable accuracy for diagnosing prostate cancer to MRI but at a lower cost. Accurate prostate segmentation is crucial for prostate volume measurement, cancer diagnosis, prostate biopsy, and treatment planning. However, prostate segmentation on microUS is challenging due to artifacts and indistinct borders between the prostate, bladder, and urethra in the midline. This paper presents MicroSegNet, a multi-scale annotation-guided transformer UNet model designed specifically to tackle these challenges. During the training process, MicroSegNet focuses more on regions that are hard to segment (hard regions), characterized by discrepancies between expert and non-expert annotations. We achieve this by proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns a larger weight to prediction errors in hard regions and a lower w
    
[^42]: 前列腺体内微型超声与离体伪全切片组织标本图像的图像配准: 一个概念验证研究。

    Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study. (arXiv:2305.19939v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19939](http://arxiv.org/abs/2305.19939)

    本文提供了一种半自动化的流程，用于将体内微-US图像与离体全切片组织病理学图像配准，以帮助泌尿外科医生提高小前列腺癌的检测率。

    

    前列腺癌的早期诊断显著提高了患者5年生存率。图像引导下的活检可以改善对小型前列腺癌的检测。MRI-超声融合引导下的活检对更小的肿瘤敏感，但由于MRI和融合设备的高成本而被少使用。微型超声（微-US）是一种新型高分辨率超声技术，可提供MRI成像类似的诊断精度，同时成本更低。然而，由于癌细胞和正常组织之间的灰度变化微弱，因此解释微-US图像是具有挑战性的。可以通过向泌尿外科医生提供一个包含地面真实癌变区域的微-US图像大数据集来解决这个挑战。这样的数据集可以通过图像配准将手术标本（组织病理学）映射到微-US图像上。在本文中，我们提出了一个半自动化的流程，用于将体内微-US图像与离体全切片组织病理学图像配准。

    Early diagnosis of prostate cancer significantly improves a patient's 5-year survival rate. Biopsy of small prostate cancers is improved with image-guided biopsy. MRI-ultrasound fusion-guided biopsy is sensitive to smaller tumors but is underutilized due to the high cost of MRI and fusion equipment. Micro-ultrasound (micro-US), a novel high-resolution ultrasound technology, provides a cost-effective alternative to MRI while delivering comparable diagnostic accuracy. However, the interpretation of micro-US is challenging due to subtle gray scale changes indicating cancer vs normal tissue. This challenge can be addressed by training urologists with a large dataset of micro-US images containing the ground truth cancer outlines. Such a dataset can be mapped from surgical specimens (histopathology) onto micro-US images via image registration. In this paper, we present a semi-automated pipeline for registering in vivo micro-US images with ex vivo whole-mount histopathology images. Our pipeli
    
[^43]: 表示驱动的强化学习框架

    Representation-Driven Reinforcement Learning. (arXiv:2305.19922v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19922](http://arxiv.org/abs/2305.19922)

    该论文提出了一个表示驱动的强化学习框架，通过在线性特征空间中嵌入策略网络，重新框定探索-利用问题为表示-利用问题，以实现最佳的探索。该框架通过应用进化和策略梯度法取得了显著的性能提升。

    

    我们提出了一个表示驱动的强化学习框架。通过将策略表示为其期望值的估计，我们利用来自情境推断的方法来指导探索和利用。特别地，将策略网络嵌入到线性特征空间中，使我们能够将探索-利用问题重新框定为表示-利用问题，其中良好的策略表示能够实现最佳的探索。我们通过应用进化和策略梯度法来展示该框架的有效性，相比于传统方法，这些方法带来了显著的性能提升。我们的框架提供了一种强化学习的新视角，强调了策略表示在决定最佳探索-利用策略方面的重要性。

    We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.
    
[^44]: 粗糙集下一种规则通用逆推学习方法

    A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])

    [http://arxiv.org/abs/2305.19718](http://arxiv.org/abs/2305.19718)

    本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。

    

    在现实任务中，通常存在大量未标记数据和标记数据。将两者组合起来进行学习的任务被称为半监督学习。专家可以使用逻辑规则来标记未标记数据，但这个操作很昂贵。感知和推理的结合在处理具有领域知识的半监督任务方面具有良好的效果。然而，获取领域知识以及规则的修正、减少和生成仍然是需要解决的复杂问题。粗糙集理论是解决信息系统中知识处理的重要方法。本文提出了一种粗糙集下的规则通用逆推学习方法（RS-ABL）。通过将规则的目标概念和子概念转化为信息表，利用粗糙集理论来解决以更低的成本获取领域知识和修正、减少、生成规则的问题。该框架还可以生成更广泛的负规则，以增强规则范围。

    In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
    
[^45]: 面向车辆路径问题的全通用神经方法

    Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])

    [http://arxiv.org/abs/2305.19587](http://arxiv.org/abs/2305.19587)

    提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。

    

    由于避免了对手工规则的依赖，学习车辆路径问题（VRP）的启发式方法受到了广泛关注。然而，现有方法通常在固定大小和节点分布的同一任务上进行训练和测试，因此具有有限的泛化性能。本文研究了一个具有挑战性但又现实的场景，该场景考虑了VRP在大小和分布方面的一般性。我们提出了一种通用的元学习框架，在推理期间能够快速适应新任务的能力下对初始化模型进行有效训练。我们进一步开发了一种简单而有效的近似方法来减少训练开销。对旅行商问题（TSP）和容量车辆路径问题（CVRP）的合成和基准实例进行了广泛的实验证明了我们方法的有效性。代码可在https://github.com/RoyalSkye/Omni-VRP得到。

    Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
    
[^46]: 从API学习学习：黑盒数据无关元学习

    Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])

    [http://arxiv.org/abs/2305.18413](http://arxiv.org/abs/2305.18413)

    该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。

    

    无数据元学习（DFML）旨在通过从一组预训练模型进行元学习而无需访问训练数据，从而实现高效学习新任务。现有的DFML工作仅能从（i）白盒和（ii）小规模预训练模型（iii）相同的架构中元学习，忽略了更实际的设置，即用户仅能通过任意模型架构和规模的API进行推断。为解决这个问题，我们提出了一个双层数据无关元知识蒸馏（BiDf-MKD）框架，将更通用的元知识从一组黑盒API转移到一个单一的元模型中。

    Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
    
[^47]: 评估GPT-3生成的仇恨内容审核解释

    Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17680](http://arxiv.org/abs/2305.17680)

    本文通过调查和分析，评估了使用GPT-3生成的针对仇恨内容的解释是否准确和有用。结果显示，GPT-3生成的解释普遍存在过于模糊、聚焦不当等缺点，同时也存在不同类型仇恨言论生成的解释质量差异大的问题。

    

    最近的研究聚焦于使用基于大型语言模型（LLMs）的Fine-tune或提示生成仇恨言论的解释。尽管这个领域越来越受关注，但这些生成解释的有效性和潜在限制仍然不为人们所了解。一个关键问题是，由LLMs生成的这些解释可能会导致用户和内容审核员对标记内容本质做出错误判断。我们提出一个分析框架来检查仇恨言论解释，并进行了一个广泛的调查来评估这些解释。我们在GPT-3上输入仇恨和非仇恨内容，发现受调查者在人工审核GPT生成的解释时，将仇恨言论解释评价为不够准确和有用。

    Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
    
[^48]: Im-Promptu: 从图像提示进行上下文组合

    Im-Promptu: In-Context Composition from Image Prompts. (arXiv:2305.17262v1 [cs.CV])

    [http://arxiv.org/abs/2305.17262](http://arxiv.org/abs/2305.17262)

    本文研究了类比推理能否实现对可组合视觉刺激成分的上下文内组合，通过引入三个基准测试套件，提供了设计类比推理的元学习框架 Im-Promptu。使用 Im-Promptu 可以训练多个具有不同组合水平的代理，包括矢量表示、补丁表示和物体槽。

    

    大规模语言模型是少样本学习者，可以从少量演示中解决各种任务。这种隐含的任务理解表明，单词令牌上的注意力机制可能在类比推理中发挥作用。本文研究类比推理是否能实现对可组合视觉刺激成分的上下文内组合。首先，我们引入了三个基准测试套件，以测试视觉上下文学习器的泛化属性。我们规范化了基于类比的上下文学习器的概念，并用它来设计一个元学习框架称为 Im-Promptu。虽然语言的所需令牌粒度已经得到了充分证实，但用于实现视觉刺激内上下文泛化的适当组合粒度通常未经指定。为此，我们使用 Im-Promptu 训练多个具有不同组合水平的代理，包括矢量表示、补丁表示和物体槽。

    Large language models are few-shot learners that can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that the attention mechanisms over word tokens may play a role in analogical reasoning. In this work, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. First, we introduce a suite of three benchmarks to test the generalization properties of a visual in-context learner. We formalize the notion of an analogy-based in-context learner and use it to design a meta-learning framework called Im-Promptu. Whereas the requisite token granularity for language is well established, the appropriate compositional granularity for enabling in-context generalization in visual stimuli is usually unspecified. To this end, we use Im-Promptu to train multiple agents with different levels of compositionality, including vector representations, patch representations, and object slots. Our e
    
[^49]: 通过格拉姆迭代实现卷积层利普希茨常数的高效边界

    Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16173](http://arxiv.org/abs/2305.16173)

    本文提出了一种基于循环矩阵和格拉姆迭代的方法，用于高效估计卷积神经网络中的Lipschitz常数上界。该方法精确、快速、可微分，并展现了超线性收敛。在实验上表现出较高的精度、计算成本和可扩展性，在利普希茨正则化方面也取得了具有竞争力的结果。

    

    由于利普希茨常数的控制对神经网络的训练稳定性、泛化和鲁棒性有很大影响，因此估计这个值是目前的一个科学难题。在本文中，我们使用循环矩阵理论和一种新的功率迭代替代方法，介绍了一个精确、快速和可微分的上界，用于卷积层的谱范数。称为格拉姆迭代，我们的方法展现了一个超线性的收敛。首先，我们通过一系列全面的实验证明了我们的方法在精度、计算成本和可伸缩性方面优于其他最先进的方法。然后，我们证明了它对于卷积神经网络的利普希茨正则化非常有效，与其他方法相比具有竞争力的结果。代码可在 https://github.com/blaisedelattre/lip4conv 上获得。

    Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.
    
[^50]: PromptNER: 基于提示的命名实体识别

    PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])

    [http://arxiv.org/abs/2305.15444](http://arxiv.org/abs/2305.15444)

    PromptNER是一种基于提示的命名实体识别算法，利用LLM生成潜在实体列表并提供解释，在少样本NER和跨领域NER方面实现了最先进性能。

    

    令人惊讶的是，大型语言模型（LLMs）和越来越多的基于提示的启发式方法现在提供了强大的现成方法，为各种经典的NLP问题提供了少量样本的解决方案。然而，尽管有着令人期待的初步结果，但这些基于LLM的少样本方法在命名实体识别（NER）方面仍远未达到最先进水平，现有的方法包括通过端到端结构理解学习表示，并在标准标记语料库上进行微调。本文介绍了PromptNER，一种新的用于少样本和跨领域NER的最先进算法。为了适应任何新的NER任务，PromptNER需要提供一组实体定义，除基本的少样本样例以外。给定输入句子，PromptNER提示LLM生成一个潜在实体列表，并提供相应的解释，证明它们与提供的实体类型定义的兼容性。值得注意的是，PromptNER在少样本NER任务方面实现了最先进的性能，并在具有挑战性的WikiAnn数据集上为跨领域NER设定了新的SOTA。

    In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
    
[^51]: PulseNet: 使用随机数据增强策略和连续小波变换进行犬ECG信号分类的深度学习模型。

    PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines. (arXiv:2305.15424v1 [eess.SP])

    [http://arxiv.org/abs/2305.15424](http://arxiv.org/abs/2305.15424)

    该论文提出了一种利用深度学习对犬的ECG信号进行自动分类的方法，通过使用随机数据增强策略和连续小波变换，分类精度得到了提高。

    

    评估犬的心电图(ECG)需要熟练的兽医，但目前可用的兽医心脏病专家用于ECG解读和诊断支持的数量有限。开发自动评估ECG序列的工具可以通过提供临床医生实时结果和决策支持工具来改善兽医护理。我们实现了一个深度卷积神经网络(CNN)来将犬的心电图序列分类为正常或异常。将ECG记录转换为8秒的第二导联序列，根据是否存在一种或多种心脏异常将其分类为正常或异常。训练ECG序列使用RandomAugmentECG进行随机数据增强，这是一个专门为该项目实现的新增强库。然后，每个块使用连续小波变换转换成2D scalogram。2D scalogram使用二元CNN分类器分类成正常或异常。

    Evaluating canine electrocardiograms (ECG) require skilled veterinarians, but current availability of veterinary cardiologists for ECG interpretation and diagnostic support is limited. Developing tools for automated assessment of ECG sequences can improve veterinary care by providing clinicians real-time results and decision support tools. We implement a deep convolutional neural network (CNN) approach for classifying canine electrocardiogram sequences as either normal or abnormal. ECG records are converted into 8 second Lead II sequences and classified as either normal (no evidence of cardiac abnormalities) or abnormal (presence of one or more cardiac abnormalities). For training ECG sequences are randomly augmented using RandomAugmentECG, a new augmentation library implemented specifically for this project. Each chunk is then is converted using a continuous wavelet transform into a 2D scalogram. The 2D scalogram are then classified as either normal or abnormal by a binary CNN classif
    
[^52]: 深度学习与伦理学

    Deep Learning and Ethics. (arXiv:2305.15239v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.15239](http://arxiv.org/abs/2305.15239)

    本文探讨了人工智能存在的算法偏见、缺乏可解释性、数据隐私侵犯、军事化、欺诈和环境问题，旨在促进哲学、政治科学和社会科学领域关于这些问题的探讨。

    

    本文是Prince（2023年）《理解深度学习》的第21章，教材的完整草稿可在此http URL获得。本章考虑了人工智能系统设计和使用可能产生的潜在危害，包括算法偏见、缺乏可解释性、数据隐私侵犯、军事化、欺诈和环境问题。目的不是为了提供更加道德的建议。相反，目标是在哲学、政治科学和更广泛的社会科学领域引发关键领域的思想和对话。

    This article appears as chapter 21 of Prince (2023, Understanding Deep Learning); a complete draft of the textbook is available here: this http URL This chapter considers potential harms arising from the design and use of AI systems. These include algorithmic bias, lack of explainability, data privacy violations, militarization, fraud, and environmental concerns. The aim is not to provide advice on being more ethical. Instead, the goal is to express ideas and start conversations in key areas that have received attention in philosophy, political science, and the broader social sciences.
    
[^53]: 图谱遇见LLM：一种用于稳健对话理解的协同过滤新方法

    Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])

    [http://arxiv.org/abs/2305.14449](http://arxiv.org/abs/2305.14449)

    一种协同过滤新方法用于稳健对话理解，在历史用户-实体交互的基础上，利用多跳客户亲和力丰富每个用户的索引，并使用有限内存BFGS算法调整每个索引的权重，实验结果显示其明显优于最先进的个性化查询重写方法。

    

    会话式人工智能系统（例如Alexa，Siri，Google Assistant等）需要理解存在缺陷的查询以确保稳健的会话理解并减少用户摩擦。这些有缺陷的查询通常是由用户的歧义和错误，自动语音识别（ASR）和自然语言理解（NLU）中的错误引起的。个性化查询重写（个性化QR）旨在减少身体和尾部用户查询流量中的缺陷，通常依赖于与对话式人工智能的过去成功的用户交互的索引。本文提出我们的“协同查询重写”方法，专注于重写用户历史中没有出现过的新型用户交互。该方法构建了一个“用户反馈交互图”（FIG），由历史用户-实体交互组成，并利用多跳客户亲和力来丰富每个用户的索引（即协同用户索引），从而帮助覆盖未来未曾见过的存在缺陷的查询。为了防止这些新的丰富索引被噪声反馈交互所支配，我们采用了有限内存BFGS（LLM）算法和回退方案来调整每个索引的权重。实验结果表明，我们的方法明显优于最先进的个性化QR方法，并在未看到的用户交互上取得了近乎完美的性能。

    Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
    
[^54]: DUBLIN——通过语言-图像网络进行文档理解

    DUBLIN -- Document Understanding By Language-Image Network. (arXiv:2305.14218v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.14218](http://arxiv.org/abs/2305.14218)

    DUBLIN是一个针对视觉文档理解的模型，使用掩模文档内容生成任务、边界框任务和渲染问答任务进行预训练，利用文档图像中的空间和语义信息。该模型在多项基准测试中达到了竞争性或最先进的结果。

    

    视觉文档理解是一项复杂的任务，涉及分析文档图像中的文本和视觉元素。现有模型通常依赖于手动特征工程或特定领域的流水线，这限制了它们在不同文档类型和语言之间的泛化能力。本文提出了一种预先使用三种新颖目标在Web页面上进行训练的DUBLIN模型：掩模文档内容生成任务、边界框任务和渲染问答任务，并利用文档图像中的空间和语义信息。我们的模型在多项基准测试中实现了具有竞争力或最先进的结果，例如基于Web的结构化阅读理解、文档视觉问答、关键信息提取、图解理解和表格问答。特别地，我们展示了DUBLIN是首个在WebSRC数据集上实现EM 77.75和F1 84.25的基于像素的模型。我们还展示了我们的模型优于现有模型。

    Visual document understanding is a complex task that involves analyzing both the text and the visual elements in document images. Existing models often rely on manual feature engineering or domain-specific pipelines, which limit their generalization ability across different document types and languages. In this paper, we propose DUBLIN, which is pretrained on web pages using three novel objectives: Masked Document Content Generation Task, Bounding Box Task, and Rendered Question Answering Task, that leverage both the spatial and semantic information in the document images. Our model achieves competitive or state-of-the-art results on several benchmarks, such as Web-Based Structural Reading Comprehension, Document Visual Question Answering, Key Information Extraction, Diagram Understanding, and Table Question Answering. In particular, we show that DUBLIN is the first pixel-based model to achieve an EM of 77.75 and F1 of 84.25 on the WebSRC dataset. We also show that our model outperform
    
[^55]: ChipGPT: 远离自然语言硬件设计还有多远

    ChipGPT: How far are we from natural language hardware design. (arXiv:2305.14019v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.14019](http://arxiv.org/abs/2305.14019)

    这篇论文介绍了ChipGPT，一个自动化设计环境，它利用大型语言模型从自然语言规范生成硬件逻辑设计，并展示了与人工设计性能相媲美的结果，且可节省超过75％的编码时间。

    

    随着大型语言模型（LLMs）如ChatGPT展示了前所未有的机器智能，它在通过自然语言交互来协助硬件工程师实现更高效的逻辑设计方面也表现出极佳的性能。为了评估LLMs协助硬件设计过程的潜力，本文尝试演示一个自动化设计环境，该环境利用LLMs从自然语言规范生成硬件逻辑设计。为了实现更易用且更高效的芯片开发流程，我们提出了一种基于LLMs的可扩展的四阶段零代码逻辑设计框架，无需重新训练或微调。首先，演示版本ChipGPT通过为LLM生成提示开始，然后产生初始Verilog程序。 其次，输出管理器纠正和优化这些程序，然后将它们收集到最终的设计空间中。最后，ChipGPT将在此空间中搜索以选择符合目标指标的最优设计。评估表明，由ChipGPT设计的逻辑电路的性能与人工设计的性能相当，并且整个过程节省了超过75％的编码时间。

    As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction. To estimate the potential of the hardware design process assisted by LLMs, this work attempts to demonstrate an automated design environment that explores LLMs to generate hardware logic designs from natural language specifications. To realize a more accessible and efficient chip development flow, we present a scalable four-stage zero-code logic design framework based on LLMs without retraining or finetuning. At first, the demo, ChipGPT, begins by generating prompts for the LLM, which then produces initial Verilog programs. Second, an output manager corrects and optimizes these programs before collecting them into the final design space. Eventually, ChipGPT will search through this space to select the optimal design under the target metrics. The evaluation sh
    
[^56]: 关于一般函数逼近下的均场强化学习的统计效率

    On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])

    [http://arxiv.org/abs/2305.11283](http://arxiv.org/abs/2305.11283)

    本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。

    

    本文研究了一般函数逼近下的均场控制（MFC）和均场博弈（MFG）中强化学习的统计效率。引入了一种称为Mean-Field Model-Based Eluder Dimension (MBED)的新概念，包含了一系列丰富的均场强化学习问题。此外，我们提出了基于乐观最大似然估计的算法，可以返回一个$\epsilon$优的策略，适用于MFC或$\epsilon$纳什均衡策略适用于MFG，样本复杂度多项式与相关参数无关，与状态、动作和代理数量无关。值得注意的是，我们的结果仅对转移动力学具有Lipschitz连续性的假设，避免了以前的强结构假设。最后，在tabular设置下，假设有一个生成模型，我们建立了一个指数级的下界支持MFC设置，同时提供了一种新颖的样本高效的模型消除算法以逼近最优策略。

    In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
    
[^57]: 大型语言模型可以被引导来规避AI生成的文本检测

    Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])

    [http://arxiv.org/abs/2305.10847](http://arxiv.org/abs/2305.10847)

    本文揭示了大型语言模型可以通过精心设计的提示语来有效规避现有的文本检测系统，证明了这些检测器的脆弱性。

    

    大型语言模型在包括论文写作和问答等多个任务中展现出了出色的表现。然而，必须解决这些模型潜在的误用问题，否则可能导致抄袭和垃圾信息等不良后果。本研究揭示，通过精心设计的提示语，LLMs可以有效地规避检测系统。我们提出了一种新颖的基于替换的上下文示例优化方法（SICO），用于自动生成这种提示语。在三个现实任务中，LLMs可能被误用，在SICO的帮助下，ChatGPT成功地规避了六项现有的检测器，平均导致0.54的AUC下降。令人惊讶的是，在大多数情况下，这些检测器的表现甚至比随机分类器还要差。这些结果坚定地揭示了现有检测器的脆弱性。

    Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
    
[^58]: 基于表面肌电图像的轻量级全卷积神经网络和迁移学习的跨场景手势识别

    Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])

    [http://arxiv.org/abs/2305.08014](http://arxiv.org/abs/2305.08014)

    本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。

    

    利用低分辨率瞬时高清肌电图像进行手势识别可以开辟发展更流畅、更自然的肌肉-计算机界面的新途径。然而，跨场景数据的变异性存在极大的挑战。现有的方法采用非常大且复杂的深度卷积神经网络或基于2SRNN的领域适应方法，来逼近由这些跨场景数据变异性引起的分布偏移。因此，这些方法也需要在预训练和适应阶段中在数百万个训练参数和大规模预训练数据集上进行学习。结果，这使得在实时应用中进行高端资源约束和计算非常昂贵的部署。为了解决这个问题，我们提出了一种轻量级的全卷积神经网络+迁移学习模型，利用轻量级全卷积神经网络和迁移学习(TL)来增强跨场景手势识别。

    Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
    
[^59]: 基于双重注意力网络的弹性车间调度问题的深度强化学习

    Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])

    [http://arxiv.org/abs/2305.05119](http://arxiv.org/abs/2305.05119)

    本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。

    

    弹性制造催生了复杂的调度问题，如弹性车间调度问题（FJSP）。在FJSP中，操作可以在多台机器上进行处理，导致操作和机器之间存在错综复杂的关系。最近的研究利用深度强化学习（DRL）来学习优先级分配规则（PDRs）以解决FJSP。然而，相对于诸如OR-Tools等精确方法的解决方案，解决方案的质量仍有提高的空间。为了解决这个问题，本文提出了一种新的端到端学习框架，结合了自注意模型进行深度特征提取和DRL进行可扩展决策制定的优点。操作和机器之间的复杂关系被准确而简洁地表示出来，提出了一个由多个相互连接的操作信息注意块和机器信息注意块组成的双注意力网络（DAN）。DAN利用这些复杂的关系，以协同地确定FJSP的PDRs。

    Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
    
[^60]: 人还是机器：基于图灵测试的日常反思

    Human or Machine: Reflections on Turing-Inspired Testing for the Everyday. (arXiv:2305.04312v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.04312](http://arxiv.org/abs/2305.04312)

    本文基于图灵测试，回避了机器是否智能的问题，探讨在日常生活中如何确定一个交互对象是人还是机器的挑战，并思考了其应用和重要性。

    

    在他的开创性论文《计算机器械与智能》中，艾伦·图灵引入了“模仿游戏”，探讨了机器智能的概念。图灵测试自那时以来一直是广泛讨论、完善和扩展的主题。本文回避了关于某个特定机器是否能被标记为智能或能否在给定环境中匹配人类能力的问题。与此相反，但受图灵启发，我们关注在日常生活中确定是否正在与一个人或一个机器进行交互这个看似简单的挑战。我们对这个人还是机器问题及其可靠答案的应用感到感兴趣，并希望反思其重要性。虽然图灵的原始测试被广泛认为是一种思维实验，但本文讨论的人还是机器问题具有明显的实际意义。虽然人类是否能够创造出能够胜任所有的人类工作的机器也未可知。

    In his seminal paper "Computing Machinery and Intelligence", Alan Turing introduced the "imitation game" as part of exploring the concept of machine intelligence. The Turing Test has since been the subject of much analysis, debate, refinement and extension. Here we sidestep the question of whether a particular machine can be labeled intelligent, or can be said to match human capabilities in a given context. Instead, but inspired by Turing, we draw attention to the seemingly simpler challenge of determining whether one is interacting with a human or with a machine, in the context of everyday life. We are interested in reflecting upon the importance of this Human-or-Machine question and the use one may make of a reliable answer thereto. Whereas Turing's original test is widely considered to be more of a thought experiment, the Human-or-Machine question as discussed here has obvious practical significance. And while the jury is still not in regarding the possibility of machines that can m
    
[^61]: LSGNN：通过局部相似性实现节点分类中的普适图神经网络

    LSGNN: Towards General Graph Neural Network in Node Classification by Local Similarity. (arXiv:2305.04225v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04225](http://arxiv.org/abs/2305.04225)

    本文提出使用局部相似性（LocalSim）学习节点级加权融合的即插即用模块，提取更具信息性的多跳信息，并对其有效性进行了理论分析。在真实基准数据集上的广泛评估表明，我们提出的LSGNN方法在同质性和异质性图上均能提供可比或优于最先进的性能。

    

    异质性被认为是伤害图神经网络（GNN）性能的问题。为了解决这个问题，一些现有的工作使用多跳邻居信息的图级加权融合来包含更多具有同质性的节点。然而，异质性可能在节点之间不同，需要考虑局部拓扑。在此基础上，本文提出使用局部相似性（LocalSim）学习节点级加权融合，并可作为即插即用模块。为了更好地融合，我们提出了一种新的、高效的初始残差差连接（IRDC）来提取更具信息性的多跳信息。此外，我们对合成图上LocalSim代表节点同质性的有效性进行了理论分析。在真实基准数据集上的广泛评估表明，我们提出的方法，即局部相似性图神经网络（LSGNN），在同质性和异质性图上均能提供可比或优于最先进的性能。

    Heterophily has been considered as an issue that hurts the performance of Graph Neural Networks (GNNs). To address this issue, some existing work uses a graph-level weighted fusion of the information of multi-hop neighbors to include more nodes with homophily. However, the heterophily might differ among nodes, which requires to consider the local topology. Motivated by it, we propose to use the local similarity (LocalSim) to learn node-level weighted fusion, which can also serve as a plug-and-play module. For better fusion, we propose a novel and efficient Initial Residual Difference Connection (IRDC) to extract more informative multi-hop information. Moreover, we provide theoretical analysis on the effectiveness of LocalSim representing node homophily on synthetic graphs. Extensive evaluations over real benchmark datasets show that our proposed method, namely Local Similarity Graph Neural Network (LSGNN), can offer comparable or superior state-of-the-art performance on both homophilic
    
[^62]: 学习6D非抓取式操作的混合演员-评论员地图

    Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])

    [http://arxiv.org/abs/2305.03942](http://arxiv.org/abs/2305.03942)

    论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。

    

    在人类的灵巧性中，非抓取式操作是操作物体的重要组成部分。非抓取式操纵可以使与物体的交互更加复杂，但也在推理交互方面提出了挑战。在本文中，我们引入了一个名为HACMan的混合演员评论员地图，这是一种使用点云观察的6D非抓取式物体操作的强化学习方法。HACMan提出了一种时间抽象和空间基础的物体中心动作表示，该表示包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。我们修改了一个现有的离线策略RL算法，以在这种混合的离散-连续动作表示学习。我们在仿真和现实世界中对HACMan进行了6D物体姿态对齐任务的评估。在最难的任务版本中，通过随机初始化物体和机器人配置，HACMan的表现优于现有的基线方法。

    Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
    
[^63]: 评估ChatGPT的工作记忆容量

    Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])

    [http://arxiv.org/abs/2305.03731](http://arxiv.org/abs/2305.03731)

    本文评估了最先进语言模型ChatGPT的工作记忆容量，结果显示其在N-back任务的行为表现与人类参与者相似，这为设计具有人类级认知能力的人工智能系统提供了关键洞察。

    

    工作记忆是人类智能和人工智能的关键方面，它作为信息临时存储和操作的工作空间。本文通过检查ChatGPT在N-back任务上的表现，调查了这一最先进语言模型的工作记忆容量。我们首先讨论了工作记忆对人类和人工智能的重要性，接着介绍了评估ChatGPT工作记忆容量的方法。研究比较了ChatGPT在言语和空间N- back任务上的行为表现与文献报道的人类参与者的表现，发现了显著的相似之处。我们的发现为设计具有人类级认知能力的人工智能系统的当前进展提供了关键洞察，并为通过人工智能模型理解人类工作记忆的未来努力提供了前景。

    Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
    
[^64]: ChatGPT和Bard是否应该与其数据提供者分享收益？AI时代的新商业模式

    Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])

    [http://arxiv.org/abs/2305.02555](http://arxiv.org/abs/2305.02555)

    ChatGPT和Bard等AI工具需要持续大量且高质量的数据来提高其性能，但现行的版权法则限制了它们对各种数据的获取。与数据提供者分享收益将有助于将AI工具与大多数版权数据拥有者之间的敌对关系转变为合作关系，使AI生态系统更健康。

    

    随着ChatGPT等各种人工智能工具越来越受欢迎，我们正进入真正的AI时代。我们可以预见，卓越的AI工具很快将获得可观的利润。一个关键问题出现了：除了传统的利益相关者和股东，AI工具是否应该与它们的训练数据提供者分享收益？答案是肯定的。大型AI工具，例如大型语言模型，始终需要更多、更高质量的数据来不断改进，但当前的版权法限制了它们对各种类型数据的获取。在AI工具和数据提供者之间分享收益可以将当前敌对的零和游戏关系转变为一种合作和互利的关系，而这种关系对于促进AI工具、用户和数据提供者之间的良性循环发展、推动AI技术并建立健康的AI生态系统是必要的。然而，当前的收益分享商业模式……

    With various AI tools such as ChatGPT becoming increasingly popular, we are entering a true AI era. We can foresee that exceptional AI tools will soon reap considerable profits. A crucial question arise: should AI tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large AI tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between AI tools and their data providers could transform the current hostile zero-sum game relationship between AI tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among AI tools, their users and data providers that drives forward AI technology and builds a healthy AI ecosystem. However, current revenue-sharing business models 
    
[^65]: 整数线性规划的局部搜索方法

    Local Search for Integer Linear Programming. (arXiv:2305.00188v1 [math.OC])

    [http://arxiv.org/abs/2305.00188](http://arxiv.org/abs/2305.00188)

    本论文开发了一个独立的局部搜索求解器，可用于解决一般整数线性规划，并在大型异构问题数据集上进行了验证。在搜索、改进和还原模式下，分别提出了可自适应修改变量值的算子和高效的举升算子，从而提高当前解的质量。实验表明，该方法在MIPLIB2017的异构问题集上表现优异。

    

    整数线性规划模型适用于各种实际的组合优化问题，对于产业和管理部门具有重要影响。本论文开发了第一个独立的局部搜索求解器，可用于解决一般整数线性规划，并在大型异构问题数据集上进行了验证。我们提出一个局部搜索框架，切换三种模式，分别为搜索，改进和还原模式，并设计适应不同模式的定制算子，从而根据不同情况提高当前解的质量。对于搜索和还原模式，我们提出了一种名为“紧身动作”的算子，它可以自适应地修改变量的值，试图使某些约束变得更紧。对于改进模式，提出了一种高效的算子“举升动作”，可以在保持可行性的同时提高目标函数的质量。结合这些内容，我们开发了一个局部搜索整数线性规划求解器，称为Local-ILP。对MIPLIB2017的异构问题集进行的实验表明，Local-ILP表现优异，可以与最先进的整数线性规划求解器相竞争。

    Integer linear programming models a wide range of practical combinatorial optimization problems and has significant impacts in industry and management sectors. This work develops the first standalone local search solver for general integer linear programming validated on a large heterogeneous problem dataset. We propose a local search framework that switches in three modes, namely Search, Improve, and Restore modes, and design tailored operators adapted to different modes, thus improve the quality of the current solution according to different situations. For the Search and Restore modes, we propose an operator named tight move, which adaptively modifies variables' values trying to make some constraint tight. For the Improve mode, an efficient operator lift move is proposed to improve the quality of the objective function while maintaining feasibility. Putting these together, we develop a local search solver for integer linear programming called Local-ILP. Experiments conducted on the 
    
[^66]: 深度知识产权: 综述

    Deep Intellectual Property: A Survey. (arXiv:2304.14613v1 [cs.AI])

    [http://arxiv.org/abs/2304.14613](http://arxiv.org/abs/2304.14613)

    这篇综述介绍了利用深度神经网络时所面临的知识产权保护问题，以及近年来防止和发现模型窃取和未经授权重新分发的方法。

    

    随着深度神经网络在工业制造和商业服务中的广泛应用，经过充分训练的深度神经网络(DNNs)由于庞大的训练成本和优秀的泛化性能变得越来越有价值和至关重要。这些训练好的模型可以被用户利用，而无需了解太多专业知识，这得益于新兴的“机器学习即服务”(MLaaS)范式。然而，这种范式也使得昂贵的模型面临许多潜在威胁，例如模型窃取和滥用。为了抵御这些威胁的迫切需求，深度知识产权（Deep Intellectual Property，DeepIP）成为了业界和学术界的共识，以保护私有训练数据、费尽心思调整的超参数或昂贵学习的模型权重。为此，近年来提出了许多方法来实现这一目标，特别是防止或发现模型窃取和未经授权的重新分发。鉴于这一快速演变的时期，

    With the widespread application in industrial manufacturing and commercial services, well-trained deep neural networks (DNNs) are becoming increasingly valuable and crucial assets due to the tremendous training cost and excellent generalization performance. These trained models can be utilized by users without much expert knowledge benefiting from the emerging ''Machine Learning as a Service'' (MLaaS) paradigm. However, this paradigm also exposes the expensive models to various potential threats like model stealing and abuse. As an urgent requirement to defend against these threats, Deep Intellectual Property (DeepIP), to protect private training data, painstakingly-tuned hyperparameters, or costly learned model weights, has been the consensus of both industry and academia. To this end, numerous approaches have been proposed to achieve this goal in recent years, especially to prevent or discover model stealing and unauthorized redistribution. Given this period of rapid evolution, the g
    
[^67]: 基于哈密顿回路的高维聚类

    High-dimensional Clustering onto Hamiltonian Cycle. (arXiv:2304.14531v1 [cs.AI])

    [http://arxiv.org/abs/2304.14531](http://arxiv.org/abs/2304.14531)

    提出了一种基于哈密顿回路的高维聚类框架，将全局结构和局部结构相结合，通过哈密顿回路将不同簇的锚点排序并映射到圆的周长上，改进了聚类的标签，达到更好的分类效果。

    

    聚类旨在根据样本之间的相似性对未标记的样本进行分组。它已成为分析高维数据的重要工具。然而，大多数聚类方法仅生成伪标签，因此无法同时呈现不同簇和异常值之间的相似性。本文提出了一种新的框架，称为基于哈密顿回路的高维聚类（HCHC），以解决上述问题。首先，HCHC在一个目标函数中将全局结构与局部结构相结合，进行深度聚类，将标签改进为相对概率，以挖掘不同簇之间的相似性，同时保持每个簇内的局部结构。然后，通过簇相似性生成最优哈密顿回路上对不同簇的锚进行排序，并映射到圆的周长上。最后，对于一个更高概率属于某个簇的样本，它会映射到与之相应的锚点更近的位置。通过这种方式，分类效果得到了提高。

    Clustering aims to group unlabelled samples based on their similarities. It has become a significant tool for the analysis of high-dimensional data. However, most of the clustering methods merely generate pseudo labels and thus are unable to simultaneously present the similarities between different clusters and outliers. This paper proposes a new framework called High-dimensional Clustering onto Hamiltonian Cycle (HCHC) to solve the above problems. First, HCHC combines global structure with local structure in one objective function for deep clustering, improving the labels as relative probabilities, to mine the similarities between different clusters while keeping the local structure in each cluster. Then, the anchors of different clusters are sorted on the optimal Hamiltonian cycle generated by the cluster similarities and mapped on the circumference of a circle. Finally, a sample with a higher probability of a cluster will be mapped closer to the corresponding anchor. In this way, ou
    
[^68]: 非平稳环境下动态系统的实时安全评估：方法和技术综述

    Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])

    [http://arxiv.org/abs/2304.12583](http://arxiv.org/abs/2304.12583)

    本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。

    

    动态系统的实时安全评估是一个关键任务，对于工业和交通应用等各个领域具有重要意义，特别是在非平稳环境中。然而，缺乏非平稳环境下实时安全评估方法的全面综述阻碍了相关方法的进展和改进。本文提供了针对非平稳环境下实时安全评估任务的方法和技术综述。具体而言，首先突出了非平稳环境中实时安全评估方法的背景和重要性。然后，我们提出了问题描述，包括定义、分类和主要挑战。我们还回顾了相关技术的最新发展，如在线主动学习、在线半监督学习、在线迁移学习和在线异常检测。最后，我们讨论了未来的展望和进一步研究的潜在方向。本文的综述旨在为非平稳环境下实时安全评估提供参考依据。

    Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
    
[^69]: 开发一种信任感感知的用户模拟器，用于统计学的主动式对话建模中的人工智能团队

    Development of a Trust-Aware User Simulator for Statistical Proactive Dialog Modeling in Human-AI Teams. (arXiv:2304.11913v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.11913](http://arxiv.org/abs/2304.11913)

    本文开发了一种用户模拟器来训练和测试主动对话策略，提供了一种探索和评估人工智能团队表现的适当主动策略的途径。

    

    近年来，人工智能团队的概念引起了越来越多的关注。为了实现人类和人工智能队友之间的有效协作，主动性对于紧密协调和有效沟通至关重要。然而，如何为基于人工智能的系统设计适当的主动性仍然是一个开放性问题和具有挑战性的主题。本文介绍了一个基于语料库的用户模拟器的开发，用于训练和测试主动式对话策略。该模拟器以有关主动式对话及其对用户信任度的知识为基础，模拟用户行为和个人信息，包括社会人口特征和人格特征。对比了两种不同的模拟方法，基于任务步骤的方法由于加强了顺序依赖模型，获得了更好的总体结果。这项研究为探索和评估在对话游戏环境中改善人工智能团队表现的适当主动策略提供了一个有前途的途径。

    The concept of a Human-AI team has gained increasing attention in recent years. For effective collaboration between humans and AI teammates, proactivity is crucial for close coordination and effective communication. However, the design of adequate proactivity for AI-based systems to support humans is still an open question and a challenging topic. In this paper, we present the development of a corpus-based user simulator for training and testing proactive dialog policies. The simulator incorporates informed knowledge about proactive dialog and its effect on user trust and simulates user behavior and personal information, including socio-demographic features and personality traits. Two different simulation approaches were compared, and a task-step-based approach yielded better overall results due to enhanced modeling of sequential dependencies. This research presents a promising avenue for exploring and evaluating appropriate proactive strategies in a dialog game setting for improving H
    
[^70]: 重访基于预训练语言模型的k-NN

    Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])

    [http://arxiv.org/abs/2304.09058](http://arxiv.org/abs/2304.09058)

    本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。

    

    预训练语言模型（PLMs）作为参数化的急切学习器，已成为自然语言处理（NLP）当前范式的实际选择。与此形成对比的是，k-最近邻（k-NN）分类器作为延迟学习模型，倾向于减轻过拟合和孤立噪声。本文中我们重访了k-NN分类器，以增强基于PLMs的分类器。从方法层面上，我们提出采用文本表示的PLMs在两个步骤中采用k-NN：（1）利用k-NN作为先验知识来校准训练过程（2）线性插值k-NN预测的概率分布和PLMs分类器的概率分布。我们的方法核心是实现了k-NN校准训练，将预测结果作为训练过程中易于和难以学习的示例的指标。从应用场景多样性的角度出发，我们在各种基准数据集上进行了广泛的微调、提示微调范式和零样本任务设置的实验。我们的结果表明，结合k-NN可以在所有受到检查的设置中持续提高PLMs的性能，并且在所有受到考虑的设置中跑赢了基于普通PLMs的方法。

    Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
    
[^71]: DeePLT：基于轨迹预测实现智能家居中的个性化照明系统

    DeePLT: Personalized Lighting Facilitates by Trajectory Prediction of Recognized Residents in the Smart Home. (arXiv:2304.08027v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.08027](http://arxiv.org/abs/2304.08027)

    本论文提出了一种基于机器学习的智能照明系统DeePLT，其通过轨迹预测实现智能家居中的个性化照明调整，给每个人定制独特的个人资料并根据其轨迹自动调整灯光。

    

    近年来，家居各部分的智能化已成为现代家居的重要特征之一。其中之一便是智能照明系统，可为每个人定制个性化光照。本文提出了一种基于机器学习的智能系统，通过轨迹预测个性化照明系统即时调整家中灯光。所提出的系统包括以下模块：（I）人体检测，用于检测和定位每个给定视频帧中的人物，（II）人脸识别，用于识别检测到的人物，（III）人体跟踪，用于跟踪视频序列中的人物，以及（IV）轨迹预测，使用逆强化学习方法预测用户在未来的位置。该方法为每个人提供一个独特的个人资料，包括规格、人脸图像和自定义照明设置，而该个人资料用于照明调整过程。与其他照明系统不同，本提出的系统可根据每个人的轨迹自动调整灯光。

    In recent years, the intelligence of various parts of the home has become one of the essential features of any modern home. One of these parts is the intelligence lighting system that personalizes the light for each person. This paper proposes an intelligent system based on machine learning that personalizes lighting in the instant future location of a recognized user, inferred by trajectory prediction. Our proposed system consists of the following modules: (I) human detection to detect and localize the person in each given video frame, (II) face recognition to identify the detected person, (III) human tracking to track the person in the sequence of video frames and (IV) trajectory prediction to forecast the future location of the user in the environment using Inverse Reinforcement Learning. The proposed method provides a unique profile for each person, including specifications, face images, and custom lighting settings. This profile is used in the lighting adjustment process. Unlike o
    
[^72]: 链式思维提示调整视觉语言模型

    Chain of Thought Prompt Tuning in Vision Language Models. (arXiv:2304.07919v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.07919](http://arxiv.org/abs/2304.07919)

    本文提出了一种链式思维提示调整的新方法，以有效地解决视觉任务中的推理问题，在图像分类任务中表现出更好的泛化性，更高的可迁移性和更高的准确性，并通过提供逐步推理过程来提高了视觉模型的可解释性。

    

    语言-图像预训练通过使用自然语言提示对视觉模型进行提示，在零样本和少样本下游任务中展示了有希望的结果。但是，大多数最近的研究只使用单个提示进行调整，忽略了人类在处理来自陌生领域的图像时在复杂任务设置中进行的内在逐步认知推理过程，例如，链式思维是一种简单而有效的近似人类推理过程的方法，已被证明在自然语言处理任务中非常有用。基于这种认知直觉，我们认为进行有效的推理也是视觉任务中的一个重要问题，链式思维可能是解决这个问题的一种方法。在这项工作中，我们提出了一种用于视觉语言建模的全新链式思维提示调整方法。广泛的实验表明，我们的方法不仅在图像分类任务中更好地进行泛化，并且具有比单个提示调整更高的可迁移性，还通过提供逐步推理过程来提高了视觉模型的可解释性。

    Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and h
    
[^73]: 知识图谱实体和架构的深度主动对齐

    Deep Active Alignment of Knowledge Graph Entities and Schemata. (arXiv:2304.04389v1 [cs.DB])

    [http://arxiv.org/abs/2304.04389](http://arxiv.org/abs/2304.04389)

    本文提出了一种基于深度学习和主动学习的KG对齐方法DAAKG，可以联合对齐不仅实体，还包括关系和类别；通过主动学习选择最佳批次进行人工标注，实验结果显示其优越精度和泛化性。

    

    知识图谱（KG）存储了关于真实世界的丰富事实。本文研究KG对齐，旨在在不同的KG中找到不仅实体，还包括关系和类别的对齐。实体级别的对齐可以促进架构级别的对齐。我们提出了一种新的KG对齐方法，称为DAAKG，基于深度学习和主动学习。通过深度学习，它学习实体、关系和类别的嵌入，并在半监督方式下联合对齐它们；而通过主动学习，它估计实体、关系或类别对的推断可能性，并选择最佳的批次进行人工标注。我们设计了两个近似算法以高效选择批次解决问题。我们在基准数据集上进行的实验显示了DAAKG的优越精度和泛化性，并验证了其所有模块的有效性。

    Knowledge graphs (KGs) store rich facts about the real world. In this paper, we study KG alignment, which aims to find alignment between not only entities but also relations and classes in different KGs. Alignment at the entity level can cross-fertilize alignment at the schema level. We propose a new KG alignment approach, called DAAKG, based on deep learning and active learning. With deep learning, it learns the embeddings of entities, relations and classes, and jointly aligns them in a semi-supervised manner. With active learning, it estimates how likely an entity, relation or class pair can be inferred, and selects the best batch for human labeling. We design two approximation algorithms for efficient solution to batch selection. Our experiments on benchmark datasets show the superior accuracy and generalization of DAAKG and validate the effectiveness of all its modules.
    
[^74]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^75]: 最大序数二次因子分解

    Maximal Ordinal Two-Factorizations. (arXiv:2304.03338v1 [cs.AI])

    [http://arxiv.org/abs/2304.03338](http://arxiv.org/abs/2304.03338)

    本文研究了最大序数二次因子分解问题，证明了其判定是否存在是一个NP完全问题，并提供了用于计算最大因子分解的算法Ord2Factor。

    

    在一个形式背景中，序数因子是其关系的子集，形成概念格中的链，即对应于线性顺序的数据集的一部分。为了可视化形式上下文中的数据，Ganter和Glodeanu提出了基于两个序数因子的双图。为了使双图有用，重要的是这些因子尽可能包含更多数据点，即覆盖尽可能多的关系。本文研究这样的序数二次因子分解。首先，我们研究了省略序数二次因子分解的形式背景中两个因子的不相交性。然后，我们证明判定给定大小的二次因子分解是否存在是一个NP完全问题，这使得计算最大因子分解具有计算成本。最后，我们提供了算法Ord2Factor，它允许我们计算大的序数二次因子分解。

    Given a formal context, an ordinal factor is a subset of its incidence relation that forms a chain in the concept lattice, i.e., a part of the dataset that corresponds to a linear order. To visualize the data in a formal context, Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the biplot to be useful, it is important that these factors comprise as much data points as possible, i.e., that they cover a large part of the incidence relation. In this work, we investigate such ordinal two-factorizations. First, we investigate for formal contexts that omit ordinal two-factorizations the disjointness of the two factors. Then, we show that deciding on the existence of two-factorizations of a given size is an NP-complete problem which makes computing maximal factorizations computationally expensive. Finally, we provide the algorithm Ord2Factor that allows us to compute large ordinal two-factorizations.
    
[^76]: Grid-SD2E：一种认知学习系统中的通用网格反馈方法

    Grid-SD2E: A General Grid-Feedback in a System for Cognitive Learning. (arXiv:2304.01844v1 [cs.AI])

    [http://arxiv.org/abs/2304.01844](http://arxiv.org/abs/2304.01844)

    本文提出了一种名为Grid-SD2E的认知学习系统，其建立在网格细胞的基础上，使用空间划分和探索利用方法实现交互和自我强化，有助于理解大脑的工作机制、治疗脑部疾病和理解智能。

    

    理解大脑如何通过产生神经信号与外部世界相互作用对于确定其工作机制、治疗脑部疾病和理解智能非常重要。然而，尽管已经提出了许多理论模型，但它们迄今难以整合和发展。受网格细胞启发，本研究创建了一个更通用和强大的网格模块，并与贝叶斯推理一起构建了一个互动和自我强化的认知系统。这种方法称为带有网格反馈的空间划分和探索利用（Grid-SD2E）。在这里，网格模块可以用作外部世界和系统之间的交互介质，也可以用作系统内的自我强化介质。空间划分和探索利用（SD2E）通过其空间划分（SD）模块接收网格的0/1信号。本文描述的系统也是从进行的实验得出的理论模型。

    Comprehending how the brain interacts with the external world through generated neural signals is crucial for determining its working mechanism, treating brain diseases, and understanding intelligence. Although many theoretical models have been proposed, they have thus far been difficult to integrate and develop. In this study, we were inspired in part by grid cells in creating a more general and robust grid module and constructing an interactive and self-reinforcing cognitive system together with Bayesian reasoning, an approach called space-division and exploration-exploitation with grid-feedback (Grid-SD2E). Here, a grid module can be used as an interaction medium between the outside world and a system, as well as a self-reinforcement medium within the system. The space-division and exploration-exploitation (SD2E) receives the 0/1 signals of a grid through its space-division (SD) module. The system described in this paper is also a theoretical model derived from experiments conducted
    
[^77]: G2PTL：适用于物流系统的交付地址预训练模型及其应用

    G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System. (arXiv:2304.01559v1 [cs.AI])

    [http://arxiv.org/abs/2304.01559](http://arxiv.org/abs/2304.01559)

    G2PTL是一种面向物流领域的预训练模型，结合了文本预训练的语义学习能力和图建模的地理关系编码能力，能有效地编码交付地址中的位置信息，并在物流系统中具有广泛的应用前景。

    

    作为物流系统的数据基础，基于文本的交付地址包含丰富且关键的位置信息。如何有效地编码交付地址是提高后续任务在物流系统中性能的核心任务。面向自然语言处理(NLP)的预训练模型(PTMs)已成为编码文本中语义信息的主要工具。虽然在许多任务中相当有前途，但这些基于NLP的PTMs未能编码交付地址中的地理知识，这在物流系统(如菜鸟系统)中大大削弱了与交付相关的任务的性能。为解决这个问题，我们提出了一种面向物流领域的域特定预训练模型，名为G2PTL，即交付地址地理关系-图预训练模型。G2PTL将文本预训练的语义学习能力与图建模的地理关系编码能力相结合。具体而言，我们首先利用真实的物流交付数据构建地理图，然后在图结构化数据上对模型进行预训练。实验结果表明，G2PTL能有效地编码基于文本的交付地址中的位置信息，并在与交付地址处理相关的任务中优于通用PTMs。

    Text-based delivery addresses, as the data foundation for logistics systems, contain abundant and crucial location information. How to effectively encode the delivery address is a core task to boost the performance of downstream tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural Language Process (NLP) have emerged as the dominant tools for encoding semantic information in text. Though promising, those NLP-based PTMs fall short of encoding geographic knowledge in the delivery address, which considerably trims down the performance of delivery-related tasks in logistic systems such as Cainiao. To tackle the above problem, we propose a domain-specific pre-trained model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in Logistics field. G2PTL combines the semantic learning capabilities of text pre-training with the geographical-relationship encoding abilities of graph modeling. Specifically, we first utilize real-world logistics delivery dat
    
[^78]: 农田自监督表示学习的注入噪声鉴别器

    INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields. (arXiv:2303.18101v1 [cs.CV])

    [http://arxiv.org/abs/2303.18101](http://arxiv.org/abs/2303.18101)

    本文提出了一个名为INoD的注入噪声鉴别器，通过特征替换和数据集鉴别的原则进行农田自监督表示学习，提升了模型性能。

    

    农业领域的感知数据集数量和多样性都受限，这影响了监督学习方法的有效训练。自监督学习技术可以缓解此问题，但现有方法没有针对农业领域的密集预测任务进行优化，导致模型性能下降。本文提出了注入噪声鉴别器（INoD），利用特征替换和数据集鉴别的原则进行自监督表示学习。INoD通过在两个不同数据集的卷积编码中交错特征图，并预测产生的特征图的数据集隶属关系作为预文本任务。我们的方法使网络能够学习一个数据集中对象的明确表示，同时与不同数据集中的相似特征一起观察。

    Perception datasets for agriculture are limited both in quantity and diversity which hinders effective training of supervised learning approaches. Self-supervised learning techniques alleviate this problem, however, existing methods are not optimized for dense prediction tasks in agriculture domains which results in degraded performance. In this work, we address this limitation with our proposed Injected Noise Discriminator (INoD) which exploits principles of feature replacement and dataset discrimination for self-supervised representation learning. INoD interleaves feature maps from two disjoint datasets during their convolutional encoding and predicts the dataset affiliation of the resultant feature map as a pretext task. Our approach enables the network to learn unequivocal representations of objects seen in one dataset while observing them in conjunction with similar features from the disjoint dataset. This allows the network to reason about higher-level semantics of the entailed o
    
[^79]: 从哥德尔不完备定理到机器人宗教的完备性（扩展摘要）

    From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract). (arXiv:2303.14338v1 [cs.AI])

    [http://arxiv.org/abs/2303.14338](http://arxiv.org/abs/2303.14338)

    本文研究了从哥德尔不完备定理到机器人宗教的完备性的逻辑过程，提出了任何信仰系统可以被形式化为逻辑理论，并且不完备定理意味着存在真实但无法证明的陈述，可以用来定义出与现有信仰和传统一致的新宗教实践。

    

    Hilbert 和 Ackermann 提出了一种将不完备理论一致地扩展到完备理论的方法。哥德尔基本上证明了任何能够对其自身陈述及其证明进行编码的理论都包含了真实但不能被证明的陈述。哥德尔的构造并没有回答希尔伯特的问题，希尔伯特认为理论可以通过逐步添加公理来证明越来越多的真实陈述，就像科学一样，完备性是消失点。我们研究了底层的逻辑过程，并描述了导致可测试但不可行的机器人宗教的轨迹，这些宗教扩展了传统宗教并提出了新的仪式和信仰。我们的方法是基于任何信仰系统都可以被形式化为一个逻辑理论的想法，并且不完备定理意味着存在真实但无法证明的陈述，可以并入这个理论。我们提供了这样的例子，并展示了如何使用它们来定义与现有信仰和传统一致的新宗教实践。

    Hilbert and Ackermann asked for a method to consistently extend incomplete theories to complete theories. G\"odel essentially proved that any theory capable of encoding its own statements and their proofs contains statements that are true but not provable. Hilbert did not accept that G\"odel's construction answered his question, and in his late writings and lectures, G\"odel agreed that it did not, since theories can be completed incrementally, by adding axioms to prove ever more true statements, as science normally does, with completeness as the vanishing point. This pragmatic view of validity is familiar not only to scientists who conjecture test hypotheses but also to real estate agents and other dealers, who conjure claims, albeit invalid, as necessary to close a deal, confident that they will be able to conjure other claims, albeit invalid, sufficient to make the first claims valid. We study the underlying logical process and describe the trajectories leading to testable but unfal
    
[^80]: 用启发式规划作为定理证明

    Planning as Theorem Proving with Heuristics. (arXiv:2303.13638v1 [cs.AI])

    [http://arxiv.org/abs/2303.13638](http://arxiv.org/abs/2303.13638)

    该论文介绍了一种用启发式规划作为定理证明的方法，通过开发一种定理证明提升启发式(TPLH)规划器，在情况树中搜索短的计划，并减少探讨的状态数量。

    

    在情境演算中将计划作为定理证明在50年前被放弃，因为这是一个不可能完成的项目。但是我们开发了一种定理证明提升启发式(TPLH)规划器，它使用A*搜索算法在情况树中搜索计划。它由基于删除松弛的与领域无关的启发式控制。我们将TPLH与Fast Downward（FD）和Best First Width Search（BFWS）规划器在几个标准基准测试中进行比较。由于我们的启发式功能实现未经优化，TPLH比FD和BFWS慢。但它会计算出更短的计划，并减少了探讨的状态数量。我们讨论了以前在知识表示和推理领域内进行规划的研究，并确定了相关方向。因此，我们表明情境演算中的演绎式提升启发式规划实际上是可以完成的。

    Planning as theorem proving in situation calculus was abandoned 50 years ago as an impossible project. But we have developed a Theorem Proving Lifted Heuristic (TPLH) planner that searches for a plan in a tree of situations using the A* search algorithm. It is controlled by a delete relaxation-based domain independent heuristic. We compare TPLH with Fast Downward (FD) and Best First Width Search (BFWS) planners over several standard benchmarks. Since our implementation of the heuristic function is not optimized, TPLH is slower than FD and BFWS. But it computes shorter plans, and it explores fewer states. We discuss previous research on planning within KR\&R and identify related directions. Thus, we show that deductive lifted heuristic planning in situation calculus is actually doable.
    
[^81]: 不受体系结构、数据集和模型规模限制的无数据元学习

    Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning. (arXiv:2303.11183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11183](http://arxiv.org/abs/2303.11183)

    不受体系结构、数据集和模型规模限制的无数据元学习框架PURER，通过ECI执行伪周期训练以适应新的任务，通过ICFIL对反演梯度进行校准来优化反演过程，并在各种任务中显著优于现有方法。

    

    无数据元学习的目的是从一组经过预训练的模型中学习有用的先验知识，而无需访问其训练数据。然而，现有的研究仅在参数空间中解决了该问题，忽略了预训练模型中蕴含的丰富数据知识，无法扩展到大规模预训练模型，只能元学习具有相同网络架构的预训练模型。为了解决这些问题，我们提出了一个统一的框架——PURER，其中包含：（1）数据无关的元训练期间的节目课程反转（ECI）；（2）元测试期间内部循环后的反演校准（ICFIL）。在元训练期间，我们提出了ECI来执行伪周期训练，以便快速适应新的看不见的任务。在元测试期间，我们提出了ICFIL来校准反演梯度，以减少基于反演的优化的负面影响。广泛的实验结果表明，所提出的PURER可以有效地元学习来自具有不同网络架构、数据集域甚至不同大小的预训练模型，并在各种任务中显著优于现有方法。

    The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes accord
    
[^82]: Probe：学习用户在时间跨度的捆绑选择中的个性化投影偏差

    Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.06016](http://arxiv.org/abs/2303.06016)

    本文提出了一种新的偏差嵌入式偏好模型——Probe，旨在解决用户在时间跨度的购物选择中的投影偏差和参照点效应，提高决策的有效性和个性化。

    

    时间跨度的选择需要权衡现在的成本和未来的收益。其中一种具体的选择是决定购买单个物品还是选择包含该物品的捆绑销售方式。以往的研究假设个人对这些选择中涉及的因素有准确的期望。然而，在现实中，用户对这些因素的感知往往存在偏差，导致了非理性和次优的决策。本文重点关注两种常见的偏差：投影偏差和参照点效应，并为此提出了一种新颖的偏差嵌入式偏好模型——Probe。该模型利用加权函数来捕捉用户的投影偏差，利用价值函数来考虑参照点效应，并引入行为经济学中的前景理论来组合加权和价值函数。这使得我们能够确定用户购买捆绑销售的概率，从而提高决策的有效性和个性化。

    Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
    
[^83]: Pacos: 建模用户的可解释和上下文依赖选择以处理偏好逆转问题

    Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals. (arXiv:2303.05648v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.05648](http://arxiv.org/abs/2303.05648)

    Pacos是一个上下文依赖的偏好模型，可以处理偏好逆转问题，并提供用户的自适应权重、比较和显示位置等可解释因素，有助于提供个性化服务。

    

    选择问题是指从多个项目中选择最佳选择，学习用户在选择问题中的偏好对于理解决策机制和提供个性化服务具有重要意义。现有的研究通常假设人们独立地评估项目。在实践中，但是用户的偏好取决于项目所处的市场，这被称为上下文效应；而用户对两个项目的偏好顺序甚至可能被颠倒，这被称为偏好逆转。本文识别了导致上下文效应的三个因素：用户的自适应权重、项目之间的比较和显示位置。我们提出了一个名为Pacos的上下文依赖偏好模型作为统一框架来同时解决这三个因素，并考虑了两种设计方法，包括具有高可解释性的加性方法和具有高准确性的基于ANN的方法。我们研究了基于各种市场情景和模型参数的偏好逆转条件，并展示了Pacos可以有效地捕捉偏好逆转。此外，Pacos可以提供上下文效应和用户自适应行为的可解释指示，有助于提供个性化服务。

    Choice problems refer to selecting the best choices from several items, and learning users' preferences in choice problems is of great significance in understanding the decision making mechanisms and providing personalized services. Existing works typically assume that people evaluate items independently. In practice, however, users' preferences depend on the market in which items are placed, which is known as context effects; and the order of users' preferences for two items may even be reversed, which is referred to preference reversals. In this work, we identify three factors contributing to context effects: users' adaptive weights, the inter-item comparison, and display positions. We propose a context-dependent preference model named Pacos as a unified framework for addressing three factors simultaneously, and consider two design methods including an additive method with high interpretability and an ANN-based method with high accuracy. We study the conditions for preference reversa
    
[^84]: Cal-QL: 计算机辅助脱机强化学习预先训练用于高效在线微调

    Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05479](http://arxiv.org/abs/2303.05479)

    本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。

    

    脱机强化学习方法可以用来从现有数据集中获取策略初始化并通过有限互动进行快速在线微调。然而，现有的脱机强化学习方法在在线微调中表现较差。本文研究了保守脱机强化学习方法中的微调问题，并设计了一种方法，可以从脱机数据中学习到有效的初始化，并使其具备快速的在线微调功能。我们的方法，即Cal-QL，通过学习一个保守的值函数初始化，低估从脱机数据中学到的策略的价值，同时确保学习到的Q值在合理的范围内。实验结果表明，我们的方法在多个基准环境中具有显著的性能优势，并且也能在真实机器人问题中进行有效的在线微调。

    A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
    
[^85]: 基于身体知识的关系状态抽象的双层规划中的主动学习

    Embodied Active Learning of Relational State Abstractions for Bilevel Planning. (arXiv:2303.04912v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.04912](http://arxiv.org/abs/2303.04912)

    该论文提出了一种基于身体知识的主动学习方法，通过在线交互与专家学习神经谓词解释、符号规划算子和任务特定的关系状态抽象，以提高机器人操作任务中的性能。

    

    状态抽象是在机器人环境中进行规划的有效技术，该环境具有连续状态和动作、长任务时间和稀疏反馈。在面向对象的环境中，谓词是一种特别有用的状态抽象形式，因为其与符号规划器的兼容性以及其关系泛化的能力。然而，要使用谓词进行规划，代理必须能够在连续环境状态下解释它们（即接地符号）。手动编程谓词解释可能很困难，因此我们希望从数据中学习它们。我们提出了一种基于身体知识的主动学习范式，其中代理通过与专家进行在线交互来学习谓词解释。例如，在堆叠积木环境中采取行动后，代理可能会问专家：“On(block1，block2)是否为真？”从这个经验中，代理学习规划：学习神经谓词解释、符号规划算子和任务特定的关系状态抽象。我们的实验表明，这种方法可以学习有效的抽象，并在一系列机器人操作任务中改进基线的性能。

    State abstraction is an effective technique for planning in robotics environments with continuous states and actions, long task horizons, and sparse feedback. In object-oriented environments, predicates are a particularly useful form of state abstraction because of their compatibility with symbolic planners and their capacity for relational generalization. However, to plan with predicates, the agent must be able to interpret them in continuous environment states (i.e., ground the symbols). Manually programming predicate interpretations can be difficult, so we would instead like to learn them from data. We propose an embodied active learning paradigm where the agent learns predicate interpretations through online interaction with an expert. For example, after taking actions in a block stacking environment, the agent may ask the expert: "Is On(block1, block2) true?" From this experience, the agent learns to plan: it learns neural predicate interpretations, symbolic planning operators, an
    
[^86]: 多对称集合：通过反向对称性提高多样性和泛化能力

    Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries. (arXiv:2303.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02484](http://arxiv.org/abs/2303.02484)

    本研究提出了一种新的集成学习方法——多对称集合（MSE），它通过对称轴上假设的多样性来提高多样性和泛化能力，超越传统随机扰动的方法探索假设空间，并取得了良好效果。

    

    深度集合（DE）通过学习随机初始化的多样化成员成功地提高了模型性能。虽然最近的一些研究尝试通过超参数或正则化损失函数来促进进一步的多样性，但这些方法主要仍然依赖于随机方法来探索假设空间。在本文中，我们提出了多对称集合（MSE），通过捕捉对称轴上假设的多样性，构建了一个构建多样性集合的框架，它可以超越模型权重和超参数的随机扰动探索假设空间。我们利用对比表示学习的最新进展，创建了分别捕捉不变和等变函数类的对立假设的模型，并提出了一种简单的集成方法，以高效地组合适当的假设来完成给定的任务。我们证明，MSE有效地捕捉了相互矛盾的假设的多样性。

    Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses th
    
[^87]: 你只转移你分享的内容：基于交集的图传递学习在链接预测中的应用

    You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction. (arXiv:2302.14189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14189](http://arxiv.org/abs/2302.14189)

    本文提出了一种称为图交集-诱导图传递学习的方法，旨在解决稀疏图在链接预测中的问题。在此方法中，通过创建一个交集子图，在源图上训练模型并将知识传递到目标图上进行预测。

    

    链接预测是许多实际应用的核心，但当目标图非常稀疏时，预测的性能可能会受到影响。为了解决稀疏图造成的问题，我们研究了一个之前被忽视的现象：在许多情况下，原始图形可能会有一个密集相连、互补的图形。这个密集的图形可能会与原始图形共享节点，从而提供了一个将有意义的选择性知识转移的自然桥梁。我们将这种情况称为基于交集的图传递学习(GITL)，它受到了电子商务或学术合著预测等实际应用的启发。我们开发了一个框架来有效地利用这种情况下的结构性先验知识。我们首先使用两个图之间共享的节点创建一个交集子图，然后将来自源丰富的交集子图的知识传递到完整的目标图上。在第二步中，我们考虑了两种方法：一种是改进的标签传播方法，另一种是多层次的传输学习方法。

    Link prediction is central to many real-world applications, but its performance may be hampered when the graph of interest is sparse. To alleviate issues caused by sparsity, we investigate a previously overlooked phenomenon: in many cases, a densely connected, complementary graph can be found for the original graph. The denser graph may share nodes with the original graph, which offers a natural bridge for transferring selective, meaningful knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL), which is motivated by practical applications in e-commerce or academic co-authorship predictions. We develop a framework to effectively leverage the structural prior in this setting. We first create an intersection subgraph using the shared nodes between the two graphs, then transfer knowledge from the source-enriched intersection subgraph to the full target graph. In the second step, we consider two approaches: a modified label propagation, and a multi-layer
    
[^88]: 基于DPLL算法的LTLf合成过程研究

    Forward LTLf Synthesis: DPLL At Work. (arXiv:2302.13825v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2302.13825](http://arxiv.org/abs/2302.13825)

    本文提出了一种基于DPLL算法的新的AND-OR图搜索框架，用于有限跟踪（\LTLf）的线性时态逻辑合成，从而在许多情况下优于其他最先进的方法。

    

    本文提出了一种新的AND-OR图搜索框架，用于有限跟踪（\LTLf）的线性时态逻辑合成，克服了之前方法的一些限制。我们设计了一个受Davis-Putnam-Logemann-Loveland（DPLL）算法启发的程序，在真正的深度优先方式下生成下一个可用的代理 - 环境移动，可能避免了耗费昂贵的枚举或编译。我们还提出了一种基于状态公式的语法等价性的搜索节点的等价检查。由于所得到的程序不能保证终止，因此我们确定了一种停止条件来中止执行并基于二进制决策图（BDD）检查状态等价性重启搜索，我们展示其是正确的。实验结果表明，该技术在许多情况下优于其他最先进的方法。我们的实现Nike参加了SYNTCOM 2023年的LTLf Realizability Track。

    This paper proposes a new AND-OR graph search framework for synthesis of Linear Temporal Logic on finite traces (\LTLf), that overcomes some limitations of previous approaches. Within such framework, we devise a procedure inspired by the Davis-Putnam-Logemann-Loveland (DPLL) algorithm to generate the next available agent-environment moves in a truly depth-first fashion, possibly avoiding exhaustive enumeration or costly compilations. We also propose a novel equivalence check for search nodes based on syntactic equivalence of state formulas. Since the resulting procedure is not guaranteed to terminate, we identify a stopping condition to abort execution and restart the search with state-equivalence checking based on Binary Decision Diagrams (BDD), which we show to be correct. The experimental results show that in many cases the proposed techniques outperform other state-of-the-art approaches. Our implementation Nike competed in the LTLf Realizability Track in the 2023 edition of SYNTCOM
    
[^89]: 自监督学习的分割不变等变表示

    Self-supervised learning of Split Invariant Equivariant representations. (arXiv:2302.10283v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10283](http://arxiv.org/abs/2302.10283)

    本文介绍了一个数据集3DIEBench，提出了一种基于超网络的等变表达式预测器SIE，结合分裂的不变与等变表达式以获得更加丰富的表示，同时证明了显著的性能提高。

    

    最近，自监督学习在学习不变或等变表示方面取得了进展。尽管不变方法在大规模数据集上进行评估，但等变方法在更小、更可控的环境中进行评估。我们旨在弥合两者之间的差距，以便学习更加多样化、适用于广泛任务的表示方法。我们首先介绍一个称为3DIEBench的数据集，包括来自55个类别的3D模型渲染图像超过250万张，我们可以完全控制应用于对象的变换。我们还引入了一种基于超网络的预测器体系结构，以学习等变表示，从而不可能出现不变性崩塌的情况。我们介绍了SIE（分裂不变-等变）的概念，它将基于超网络的预测器与分裂为两部分（一部分为不变形，另一部分为等变形）的表示相结合，以学习更丰富的表示。我们证明了显著的性能提高。

    Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains
    
[^90]: COMET: X86成本模型解释框架

    COMET: X86 Cost Model Explanation Framework. (arXiv:2302.06836v2 [cs.PF] UPDATED)

    [http://arxiv.org/abs/2302.06836](http://arxiv.org/abs/2302.06836)

    本文提出了一个用于生成x86成本模型解释的框架COMET，通过提供解释来提高机器学习成本模型的可解释性和可推广性。研究显示该框架所提供的语义信息与成本预测误差呈负相关。

    

    基于机器学习的程序成本模型已被证明可以提供相当准确的程序成本预测。它们可以取代主流编译器中经过大量工程设计的分析性程序成本模型，但它们的黑匣子本质阻碍了它们的采用。在本研究中，我们提出了第一个框架，COMET，用于为x86成本模型生成忠实、可推广和直观的解释。COMET将可解释性特别引入ML-based成本模型，例如Ithemal。我们生成并比较Ithemal的COMET解释与手工精细的分析模型uiCA的COMET解释。我们的实证发现显示在给定的x86基本块的成本模型的COMET解释中，语义更丰富的特征的突出程度与成本预测误差呈负相关。

    ML-based program cost models have been shown to yield fairly accurate program cost predictions. They can replace heavily-engineered analytical program cost models in mainstream compilers, but their black-box nature discourages their adoption. In this work, we propose the first framework, COMET, for generating faithful, generalizable, and intuitive explanations for x86 cost models. COMET brings interpretability specifically to ML-based cost models, such as Ithemal. We generate and compare COMET's explanations for Ithemal against COMET's explanations for a hand-crafted, accurate analytical model, uiCA. Our empirical findings show an inverse correlation between the error in the cost prediction of a cost model and the prominence of semantically-richer features in COMET's explanations for the cost model for a given x86 basic block.
    
[^91]: 通过AdapterFusion实现参数高效的模块化偏差修正

    Parameter-efficient Modularised Bias Mitigation via AdapterFusion. (arXiv:2302.06321v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06321](http://arxiv.org/abs/2302.06321)

    本文提出了一个新的去偏差方法——DAM，它采用AdapterFusion概念，将偏差修正功能封装到独立的适配器中，在不影响核心模型的情况下，实现了按需的去偏差，可以有效降低模型的偏见问题。

    

    大型预训练语言模型存在社会偏见，并将这些偏见带给下游任务。当前的内部处理偏差修正方法（如对抗训练）通过更新模型参数来施加去偏差，从而将模型转移到新的、不可逆的去偏差状态。在本文中，我们提出了一种新颖的方法，开发出了独立的去偏差功能，与模型分离，可以按需集成到模型中，同时保持核心模型不变。借鉴多任务学习中的AdapterFusion概念，我们引入了DAM（使用适配器模块进行去偏差）——一种去偏差方法，首先将任意偏差修正功能封装到独立的适配器中，然后按需将它们添加到模型中，以实现公平性。我们在三种分类任务上进行了大量实验，保护属性为性别、种族和年龄。我们的结果表明，DAM改进或保持了模型的效果。

    Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model's parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. Drawing from the concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing with Adapter Modules) - a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. We conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. Our results show that DAM improves or maintains the effec
    
[^92]: MarioGPT: 通过大语言模型进行开放式文本关卡生成

    MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.05981](http://arxiv.org/abs/2302.05981)

    MarioGPT是第一个文本到超级马里奥兄弟游戏关卡的生成模型，通过大型语言模型实现开放式的、可控制的关卡生成。

    

    流程内容生成算法可以自动生成复杂数一致的环境。然而，使用流程内容生成方法生成反映特定意图和限制的有意义内容仍然具有挑战性。此外，许多流程内容生成算法缺乏以开放式方式生成内容的能力。最近，大型语言模型在许多不同领域都表现出了非常高的效率。这些训练有素的大型语言模型可以进行微调，重复使用信息并加速新任务的培训。在这项工作中，我们介绍了MarioGPT，这是一个经过优化的GPT2模型，用于生成基于瓷砖的游戏关卡，我们以超级马里奥兄弟的关卡为例。我们展示了MarioGPT不仅可以生成不同的游戏关卡，而且可以通过文本提示控制关卡生成，解决了当前PCG技术的主要挑战之一。据我们所知，MarioGPT是第一个文本到关卡模型。

    Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
    
[^93]: 用于上下文学习的组合范例

    Compositional Exemplars for In-context Learning. (arXiv:2302.05698v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.05698](http://arxiv.org/abs/2302.05698)

    该论文提出了CEIL（Compositional Exemplars for In-context Learning）框架，利用决定性点过程（DPP）模型处理上下文示例选择问题，从而提高了大型预训练语言模型（LMs）进行上下文学习的性能。

    

    大型预训练语言模型已经表现出令人印象深刻的上下文学习能力，其中模型通过输入输出示例作为演示，在不进行任何参数更新的情况下学习执行看不见的任务。上下文学习的性能高度受到所选上下文示例的质量所支配。然而，以前的选择方法基本上是基于简单的启发式，导致性能次优。在这项工作中，我们将上下文示例选择形式化为子集选择问题。我们提出CEIL（Compositional Exemplars for In-context Learning），它通过决定性点过程（DPP）对所给输入和上下文示例之间的交互进行建模，并通过精心设计的对比学习目标进行优化，从而获得来自LM的偏好。我们在来自7个不同自然语言处理任务的12个分类和生成数据集上验证了CEIL，包括情感分析、释义检测、自然语言生成等任务。

    Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language
    
[^94]: SOCRATES：基于文本的机器人狗人体搜索和接近

    SOCRATES: Text-based Human Search and Approach using a Robot Dog. (arXiv:2302.05324v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.05324](http://arxiv.org/abs/2302.05324)

    该论文提出了一种先搜索再接近目标人员的基于文本的机器人接近方法，并使用基于混合学习的框架生成机器人的友好动作，成功验证了该方法。

    

    本论文提出了一个基于自由文本表述的SOcratic模型，用于机器人接近人类，称为SOCRATES。首先，机器人搜索目标用户，然后以人类友好的方式接近。文本描述由外观和位置线索组成。我们提出了一个人类搜索SOCratic模型以解决目标人员搜索的下游任务，该模型将语言域中的大型预训练模型连接起来。然后，我们提出了一个基于混合学习的框架，用于生成到达人的目标友好机器人动作，包括一个来自演示的学习模块和一个知识蒸馏模块。我们通过虚拟移动机器人的仿真以及在大学校园环境中使用机器人狗进行的真实实验，验证了所提出的搜索模块。

    In this paper, we propose a SOCratic model for Robots Approaching humans based on TExt System (SOCRATES) focusing on the human search and approach based on free-form textual description; the robot first searches for the target user, then the robot proceeds to approach in a human-friendly manner. In particular, textual descriptions are composed of appearance (e.g., wearing white shirts with black hair) and location clues (e.g., is a student who works with robots). We initially present a Human Search Socratic Model that connects large pre-trained models in the language domain to solve the downstream task, which is searching for the target person based on textual descriptions. Then, we propose a hybrid learning-based framework for generating target-cordial robotic motion to approach a person, consisting of a learning-from-demonstration module and a knowledge distillation module. We validate the proposed searching module via simulation using a virtual mobile robot as well as through real-w
    
[^95]: 超网络构建音频的隐式神经表示

    Hypernetworks build Implicit Neural Representations of Sounds. (arXiv:2302.04959v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04959](http://arxiv.org/abs/2302.04959)

    该论文介绍了一种新的方法，名为“HyperSound”，可将超网络结构应用于元学习，从而生成音频隐式神经表示（INR），该方法可用于音频信号的处理，并且重构质量可与其他最先进的模型相媲美，是当代音频处理中的一个有潜力的替代方案。

    

    隐式神经表示（INR）现在被广泛地应用于各种实际应用程序中来代表多媒体信号，包括图像超分辨率、图像压缩或3D渲染。现有的利用INR的方法主要集中在视觉数据上，因为在基于图像的INR模型的架构属性中存在归纳偏差，所以将其应用于其他模态，如音频，是非常困难的。为了解决这个问题，我们介绍了超声（HyperSound），这是一种利用超网络进行元学习的方法，用于为音频样本生成INR，以便能够在训练中观察到的样本上进行推广。我们的方法以可比较其他最先进模型的质量重构音频样本，并为用于音频处理的深度神经网络中的当代声音表示提供了可行的替代方案，如谱图。

    Implicit Neural Representations (INRs) are nowadays used to represent multimedia signals across various real-life applications, including image super-resolution, image compression, or 3D rendering. Existing methods that leverage INRs are predominantly focused on visual data, as their application to other modalities, such as audio, is nontrivial due to the inductive biases present in architectural attributes of image-based INR models. To address this limitation, we introduce HyperSound, the first meta-learning approach to produce INRs for audio samples that leverages hypernetworks to generalize beyond samples observed in training. Our approach reconstructs audio samples with quality comparable to other state-of-the-art models and provides a viable alternative to contemporary sound representations used in deep neural networks for audio processing, such as spectrograms.
    
[^96]: 面向提升图形模型结构学习的原则性和高效性模体发现方法

    Principled and Efficient Motif Finding for Structure Learning of Lifted Graphical Models. (arXiv:2302.04599v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.04599](http://arxiv.org/abs/2302.04599)

    本文提出了一种针对提升图形模型中结构模体挖掘的原则性和高效性方法，通过预处理步骤和控制超参数优化算法效果。

    

    结构学习是人工智能中的一个核心问题，对神经符号学习和统计关系学习领域至关重要。它包括从数据中自动学习逻辑理论的过程。结构学习的基础是挖掘数据中的重复模式，即结构模体。发现这些模式可降低搜索空间，并引导公式的学习。尽管模体学习非常重要，但它仍不为人们所理解。我们提出了一种基于随机过程的，针对提升图形模型中矿结构模体的原则性方法，这些模型将一阶逻辑与概率模型结合在一起。我们的第一个创新是一种算法，它依赖于两个直观的超参数：一个控制实体相似性度量中的不确定性，以及一个控制生成规则的柔软度。第二个贡献是预处理步骤，通过分层聚类将相似的实体分组。最后，我们在基准数据集上展示了我们方法的有效性。

    Structure learning is a core problem in AI central to the fields of neuro-symbolic AI and statistical relational learning. It consists in automatically learning a logical theory from data. The basis for structure learning is mining repeating patterns in the data, known as structural motifs. Finding these patterns reduces the exponential search space and therefore guides the learning of formulas. Despite the importance of motif learning, it is still not well understood. We present the first principled approach for mining structural motifs in lifted graphical models, languages that blend first-order logic with probabilistic models, which uses a stochastic process to measure the similarity of entities in the data. Our first contribution is an algorithm, which depends on two intuitive hyperparameters: one controlling the uncertainty in the entity similarity measure, and one controlling the softness of the resulting rules. Our second contribution is a preprocessing step where we perform hie
    
[^97]: 针对特征与标签偏移的时间序列领域自适应

    Domain Adaptation for Time Series Under Feature and Label Shifts. (arXiv:2302.03133v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03133](http://arxiv.org/abs/2302.03133)

    本文提出了Raincoat，这是第一个针对复杂时间序列的封闭集和通用领域自适应模型。Raincoat通过跨域对齐时间和频率特征，修正偏移以便于检测私有标签，并通过识别共享结构来提高可传递性。

    

    无监督领域自适应（UDA）可以将源域训练的模型转移到未标记的目标域。然而，转移复杂的时间序列模型面临着挑战，因为不同域中存在动态的时间结构变化，导致时间和频率表示中的特征偏移。此外，源域和目标域中的任务标签分布可能存在显著差异，难以解决标签偏移和识别唯一于目标域的标签。有效地转移复杂的时间序列模型仍然是一个十分困难的问题。本文提出了Raincoat，这是第一个针对复杂时间序列的封闭集和通用领域自适应模型。Raincoat通过考虑时间和频率特征，跨域对齐它们，修正不匹配以便于检测私有标签来解决特征和标签偏移问题。此外，Raincoat通过识别源域和目标域之间的共享结构来提高可传递性。

    Unsupervised domain adaptation (UDA) enables the transfer of models trained on source domains to unlabeled target domains. However, transferring complex time series models presents challenges due to the dynamic temporal structure variations across domains. This leads to feature shifts in the time and frequency representations. Additionally, the label distributions of tasks in the source and target domains can differ significantly, posing difficulties in addressing label shifts and recognizing labels unique to the target domain. Effectively transferring complex time series models remains a formidable problem. We present Raincoat, the first model for both closed-set and universal domain adaptation on complex time series. Raincoat addresses feature and label shifts by considering both temporal and frequency features, aligning them across domains, and correcting for misalignments to facilitate the detection of private labels. Additionally, Raincoat improves transferability by identifying l
    
[^98]: Mnemosyne: 使用Transformers来训练Transformers

    Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01128](http://arxiv.org/abs/2302.01128)

    Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。

    

    训练复杂的机器学习(ML)架构需要耗费大量计算和时间来选择合适的优化器并调节其超参数。从数据中学习优化器的新学习范式已经成为手动设计ML优化器的更好选择。我们提出了Mnemosyne优化器，它使用Performers: 隐式低秩attention Transformers。它可以学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节。我们展示了Mnemosyne：(a)比流行的LSTM优化器具有更好的泛化能力；(b)特别地，可以在标准MLPs上进行元训练后成功地训练Vision Transformers(ViTs) (c)可以初始化优化器以实现机器人应用中更快的收敛。我们相信这些结果开启了使用Transformers构建基础优化模型的可能性，可以应对常规的Transformer训练挑战。我们通过广泛的理论分析来补充我们的结果。

    Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
    
[^99]: MILO: 模型无关子集选择框架，用于高效模型训练和调优。

    MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13287](http://arxiv.org/abs/2301.13287)

    提出了一个模型无关子集选择框架MILO，将子集选择与模型训练分离，通过易到难的课程实现了卓越的模型收敛和性能。

    

    训练深度网络和调优大型数据集的超参数是计算密集型的。减少训练成本的主要研究方向之一是通过选择很好的训练数据子集来实现。与简单的自适应随机子集选择基准相比，现有的智能子集选择方法由于耗时的子集选择步骤而不具竞争力，该步骤涉及计算依赖于模型的梯度和特征嵌入，并应用子模块目标的贪心最大化。我们的关键洞察是消除对下游模型参数的依赖，将子集选择作为预处理步骤，并使其能够在不增加成本的情况下训练多个模型。在这个工作中，我们提出了 MILO，一个模型无关的子集选择框架，它将子集选择与模型训练分离，同时通过使用一个易到难的课程实现了卓越的模型收敛和性能。通过实验结果验证了我们的方法。

    Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in
    
[^100]: 序列推荐的互Wasserstein距离最小化算法

    Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation. (arXiv:2301.12197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12197](http://arxiv.org/abs/2301.12197)

    本文提出了一种基于互Wasserstein距离最小化的新型自监督学习框架用于提高推荐系统的性能，该方法使用Wasserstein距离测量来增强互信息最大化，优于现有的几种顺序推荐方法。

    

    自监督的顺序推荐通过最大化互信息和设计良好的数据增广显著提高了推荐系统的性能。然而, 目前的互信息估计基于计算Kullback Leibler差异，并且存在许多局限性，包括非对称估计、指数样本大小需求和训练不稳定。而使用的现有数据增广大多是随机的，可能会因为随机修改而破坏顺序相关性。因此，我们提出了一种基于互Wasserstein距离最小化的新型自监督学习框架来解决这些问题，提出了Wasserstein距离测量来评估增广序列之间的互信息，通过减少原始和增广序列的概率分布之间的距离，增强互信息最大化。在实际数据集上进行的实验结果表明，我们的方法优于现有的几种顺序推荐方法。

    Self-supervised sequential recommendation significantly improves recommendation performance by maximizing mutual information with well-designed data augmentations. However, the mutual information estimation is based on the calculation of Kullback Leibler divergence with several limitations, including asymmetrical estimation, the exponential need of the sample size, and training instability. Also, existing data augmentations are mostly stochastic and can potentially break sequential correlations with random modifications. These two issues motivate us to investigate an alternative robust mutual information measurement capable of modeling uncertainty and alleviating KL divergence limitations. To this end, we propose a novel self-supervised learning framework based on Mutual WasserStein discrepancy minimization MStein for the sequential recommendation. We propose the Wasserstein Discrepancy Measurement to measure the mutual information between augmented sequences. Wasserstein Discrepancy M
    
[^101]: 多维概念发现(MCD): 一个具有完整性保证的统一框架

    Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees. (arXiv:2301.11911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11911](http://arxiv.org/abs/2301.11911)

    提出了多维概念发现(MCD)方法，它满足概念层面上的完整性关系，不需要加强概念可解释性或重新训练模型部分，并提供概念激活图分析工具

    

    完整性公理使得后续XAI方法的解释仅对模型在单个决策上有效。为了可信地应用XAI，特别是对于高风险的决策，需要更全球的模型理解。最近，已经提出了基于概念的方法，但这些方法不能保证与实际的模型推理相结合。为了解决这个问题，我们提出了多维概念发现(MCD)，作为之前方法的扩展，满足概念层面上的完整性关系。我们的方法从通用的线性子空间作为概念开始，并不需要加强概念可解释性或重新训练模型部分。我们提出了稀疏子空间聚类来发现改进的概念，充分利用了多维子空间的潜能。MCD提供了两种概念在输入空间中的互补分析工具：(1)概念激活图，显示概念表达的位置

    The completeness axiom renders the explanation of a post-hoc XAI method only locally faithful to the model, i.e. for a single decision. For the trustworthy application of XAI, in particular for high-stake decisions, a more global model understanding is required. Recently, concept-based methods have been proposed, which are however not guaranteed to be bound to the actual model reasoning. To circumvent this problem, we propose Multi-dimensional Concept Discovery (MCD) as an extension of previous approaches that fulfills a completeness relation on the level of concepts. Our method starts from general linear subspaces as concepts and does neither require reinforcing concept interpretability nor re-training of model parts. We propose sparse subspace clustering to discover improved concepts and fully leverage the potential of multi-dimensional subspaces. MCD offers two complementary analysis tools for concepts in input space: (1) concept activation maps, that show where a concept is express
    
[^102]: 输入扰动降低扩散模型的暴露偏差

    Input Perturbation Reduces Exposure Bias in Diffusion Models. (arXiv:2301.11706v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11706](http://arxiv.org/abs/2301.11706)

    本文提出了一种输入扰动方法来缓解扩散模型中的曝光偏差问题，该方法不影响模型性能，能显著提高生成样本的质量并减少训练和推断时间。

    

    去噪扩散概率模型显示出了令人印象深刻的生成质量，但是它们长的抽样链导致了高计算成本。本文观察到长时间的抽样链也会导致一种错误积累现象，类似于自回归文本生成中的曝光偏差问题。具体来说，我们注意到训练和测试之间存在差异，因为前者是基于真实样本进行条件训练，而后者是基于之前生成的结果进行条件的。为了缓解这个问题，我们提出了一种非常简单但有效的训练规则，即通过扰动真实样本来模拟推断时间的预测误差。我们经验证明，采用这种输入扰动方式，不会影响模型的召回率和精确率，却能显著提高样本质量，同时减少训练和推断时间。例如，在CelebA 64$\times$64上，我们实现了36.74的Fréchet Inception Distance，优于其他最先进的模型。

    Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64$\times$64, we ach
    
[^103]: 针对不规则采样时间序列的神经连续离散状态空间模型

    Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series. (arXiv:2301.11308v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11308](http://arxiv.org/abs/2301.11308)

    本研究提出了一个用于不规则采样时间序列的神经连续离散状态空间模型，其采用辅助变量来区分识别和动态，从而实现了准确的贝叶斯推理和改进的性能。

    

    学习真实世界动态现象（如气候、生物学等）的准确预测模型仍然是一个具有挑战性的任务。一项关键问题是，自然和人工过程生成的数据往往包含不规则采样和/或缺失的时间序列。本研究提出神经连续离散状态空间模型（NCDSSM），用于通过离散时间观测对时间序列进行连续时间建模。NCDSSM采用辅助变量来区分识别和动态，因此仅需要对辅助变量进行摊销推理。利用连续-离散滤波理论的技术，我们展示了如何对动态状态进行准确的贝叶斯推断。我们提出了三种灵活的潜在动态参数化方法和一种在推断期间对动态状态进行边缘化的高效培训目标。在各个领域的多个基准数据集上的实证结果表明了改进了的性能。

    Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved i
    
[^104]: FPANet: 基于频率的视频去莫尔纹技术，使用帧级后对齐

    FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment. (arXiv:2301.07330v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07330](http://arxiv.org/abs/2301.07330)

    该论文提出了一种名为FPANet的新模型，它通过去除各种大小的莫尔纹图案来改善恢复质量，采用多个连续帧提取帧不变内容特征，输出时间一致图像。

    

    重叠网格模式之间的干扰会导致莫尔纹，从而降低普通数码相机捕捉数字显示屏的图像的视觉质量。该论文提出了一种名为FPANet的新模型，它学习频率和空间域中的滤波器，通过去除各种大小的莫尔纹图案来改善恢复质量。此外，模型使用多个连续帧，学习提取帧不变内容特征，并输出更好质量的时间一致图像。

    Interference between overlapping gird patterns creates moire patterns, degrading the visual quality of an image that captures a screen of a digital display device by an ordinary digital camera. Removing such moire patterns is challenging due to their complex patterns of diverse sizes and color distortions. Existing approaches mainly focus on filtering out in the spatial domain, failing to remove a large-scale moire pattern. In this paper, we propose a novel model called FPANet that learns filters in both frequency and spatial domains, improving the restoration quality by removing various sizes of moire patterns. To further enhance, our model takes multiple consecutive frames, learning to extract frame-invariant content features and outputting better quality temporally consistent images. We demonstrate the effectiveness of our proposed method with a publicly available large-scale dataset, observing that ours outperforms the state-of-the-art approaches, including ESDNet, VDmoire, MBCNN, 
    
[^105]: 深度神经网络不安全输入计数的#DNN-Verification问题

    The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.07068](http://arxiv.org/abs/2301.07068)

    本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。

    

    深度神经网络（DNN）在需要高度安全性的关键任务中，例如自动驾驶中越来越被采用。虽然最先进的验证器可以用来检查DNN是否不安全，即是否存在至少一种不安全的输入配置，但它们的是/否输出对于其他目的（如屏蔽、模型选择或培训改进）的信息不足够详细。在本文中，我们介绍了#DNN-Verification问题，它涉及计算导致DNN违反特定安全性质的输入配置数量。我们分析了这个问题的复杂性，并提出了一种新的方法，它返回确切的违规计数。由于该问题的#P完备性，我们还提出了一种随机的近似方法，该方法提供了正确计数的可证明概率界，同时显著降低了计算要求。我们在一组安全关键基准测试上呈现了实验结果，比较了我们的方法与最先进的验证器和基于计数的启发式算法。

    Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
    
[^106]: 基于对抗生成网络的短SSVEP数据扩展框架

    Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.05599](http://arxiv.org/abs/2301.05599)

    本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。

    

    基于SSVEP的脑机接口因其高信息传输速率和目标数量可用性而受到广泛关注。然而，频率识别方法的性能在很大程度上取决于用户校准数据的数量和数据长度，这限制了它在实际应用中的部署。最近，基于生成对抗网络（GANs）的数据生成方法已被广泛采用来创建合成的脑电数据，有望解决这些问题。本文提出了一种基于GANs的端到端信号转化网络TEGAN，用于数据长度扩展。TEGAN可以将短SSVEP信号转换成长的人工SSVEP信号。通过将一个新颖的U型生成器架构和一个辅助分类器加入到网络结构中，TEGAN可以在合成数据中产生有条件的特征。此外，我们实现并比较了两种最先进的频率识别方法，以评估TEGAN生成数据的有效性。实验结果表明，所提出的TEGAN方法优于传统的线性插值方法和最先进的基于深度学习的方法。所提出的TEGAN方法可以显著提高BCI系统的效率，减少所需的校准时间并改善分类的准确性。

    Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
    
[^107]: 基于持久性的离散化方法用于从时间序列学习离散事件系统

    Persistence-Based Discretization for Learning Discrete Event Systems from Time Series. (arXiv:2301.05041v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.05041](http://arxiv.org/abs/2301.05041)

    本研究提出了一种基于持久性的离散化方法，使用Wasserstein距离来纠正持久性分数中过度的偏向，增强了模型的性能。

    

    为了深入了解一个动态系统，拥有一个可解释和多功能的模型是很方便的。定时离散事件系统就是这样一种模型，它们满足这些要求。然而，这种模型只能从时间戳事件序列推断出来，而不能直接从数字数据中推断出来。为了解决这个问题，必须进行离散化步骤，以识别时间序列中的事件或符号。Persist是一种离散化方法，旨在使用称为持久性分数的得分来创建持久性符号。这样可以减轻非期望符号变化的风险，从而导致模型过于复杂。经过对持久性分数的研究，我们发现它往往偏向于过度的情况，从而错过了有趣的持久性符号。为了纠正这种行为，我们用Wasserstein距离替换了持久性分数中使用的指标，即Kullback-Leibler分歧。实验表明，改进后的持久性分数增强了Persist的性能。

    To get a good understanding of a dynamical system, it is convenient to have an interpretable and versatile model of it. Timed discrete event systems are a kind of model that respond to these requirements. However, such models can be inferred from timestamped event sequences but not directly from numerical data. To solve this problem, a discretization step must be done to identify events or symbols in the time series. Persist is a discretization method that intends to create persisting symbols by using a score called persistence score. This allows to mitigate the risk of undesirable symbol changes that would lead to a too complex model. After the study of the persistence score, we point out that it tends to favor excessive cases making it miss interesting persisting symbols. To correct this behavior, we replace the metric used in the persistence score, the Kullback-Leibler divergence, with the Wasserstein distance. Experiments show that the improved persistence score enhances Persist's 
    
[^108]: 机器翻译度量的外在评估

    Extrinsic Evaluation of Machine Translation Metrics. (arXiv:2212.10297v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10297](http://arxiv.org/abs/2212.10297)

    论文研究了机器翻译度量在大型平台和下游任务中的可靠性，发现某些度量在句子级别上表现不佳且其有用性与下游任务有关。

    

    自动机器翻译度量通常用于在较大的测试集上比较机器翻译系统的翻译质量（系统级评估），但是，句子级别上自动度量是否能可靠地区分好翻译和差翻译仍不清楚。本文调查了在具有下游任务的大型平台上放置机器翻译组件以检测其成功的MT度量的有用性。我们在三个跨语言下游任务（对话状态跟踪，问题回答和语义解析）上评估了最广泛使用的MT量度（chrF，COMET，BERTScore等）的分段性能。对于每个任务，我们仅能访问单语种任务特定模型。我们计算在Translate-Test设置下，度量预测好/坏翻译能力与最终任务成功/失败之间的相关性。我们的实验表明，尽管某些度量在系统级评估中表现良好，但在分段评估中可能不可靠。此外，某些度量的有用性取决于下游任务。

    Automatic machine translation (MT) metrics are widely used to distinguish the translation qualities of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the success of a machine translation component when placed in a larger platform with a downstream task. We evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for the Translate-Test setup. Our experiments
    
[^109]: 无休止赌博算法在缓存不断变化的流行度内容中的应用

    Caching Contents with Varying Popularity using Restless Bandits. (arXiv:2212.03291v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2212.03291](http://arxiv.org/abs/2212.03291)

    本文研究了如何在无线边缘处缓存内容以减轻移动网络对核心网络和回程链路带来的负担。通过无休止赌博算法将问题最小化。

    

    移动网络对数据量和用户密度的需求不断增长，这给移动核心网络和回程链路带来了巨大的负担。一种有效的技术是使用缓存，即利用边缘网络节点的缓存（如固定或移动接入点甚至用户设备）将数据靠近用户。缓存的性能取决于缓存内容。本文研究在无线边缘（即基站）处缓存内容的问题，以最小化无限视野下产生的贴现成本。我们将这个问题形式化为一个无休止的赌博问题，该问题很难求解。我们首先展示了优化策略是阈值型的。利用这些结构性结果，我们证明了问题的可指标性，并使用Whittle指数策略以最小化贴现成本。

    Mobile networks are experiencing prodigious increase in data volume and user density , which exerts a great burden on mobile core networks and backhaul links. An efficient technique to lessen this problem is to use caching i.e. to bring the data closer to the users by making use of the caches of edge network nodes, such as fixed or mobile access points and even user devices. The performance of a caching depends on contents that are cached. In this paper, we examine the problem of content caching at the wireless edge(i.e. base stations) to minimize the discounted cost incurred over infinite horizon. We formulate this problem as a restless bandit problem, which is hard to solve. We begin by showing an optimal policy is of threshold type. Using these structural results, we prove the indexability of the problem, and use Whittle index policy to minimize the discounted cost.
    
[^110]: 基础模型的能力探究

    On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16327](http://arxiv.org/abs/2211.16327)

    本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。

    

    如果基础模型具有无限高质量的数据点、无限计算能力、一个无限大的完美训练算法、以及在预设任务上保证零泛化误差，那么它可以用于一切吗？传统的表示、优化或泛化理论无法回答这个问题，因为它们主要探讨的问题在这里都是不存在的。本文提出范畴论提供了强大的理论工具，以回答这个问题。我们证明了三个结果，第一个限制了基于提示的学习的能力，即仅当任务可表示时，模型才能用提示解决下游任务；第二个结果表明，微调不受这个限制，因为一个具有最小所需能力（对称性）的基础模型可以通过微调和足够的资源来理论上解决前置任务所定义的类别中的下游任务。我们的最终结果可以看作是第二个结果的一般化，表明如果允许微调并且下游任务可在前置任务定义的范畴中表示，则基础模型的最小能力也足以解决任何下游任务。

    With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
    
[^111]: SegCLIP：基于可学习中心的补丁聚合用于开放词汇语义分割

    SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation. (arXiv:2211.14813v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14813](http://arxiv.org/abs/2211.14813)

    SegCLIP是一种用于开放词汇语义分割的模型，通过聚集补丁到语义区域进行分割，具有动态捕捉语义组的特点。

    

    最近，对比语言-图像预训练 (如CLIP) 在各种下游任务上取得了很有希望的结果。预训练模型可以通过学习大量的文本-图像数据来捕捉图像中丰富的视觉概念。然而，将所学的视觉知识转移到开放词汇语义分割仍然不够充分。本文提出了一种基于CLIP的模型SegCLIP，用于无注释的开放词汇分割。SegCLIP利用可学习中心聚集补丁到语义区域通过文本-图像对的训练。聚集操作可以动态地捕捉语义组，用于产生最终的分割结果。我们还提出了一个对遮罩补丁的重构损失和一个基于超像素的KL损失，以增强视觉表示。实验结果表明，我们的模型实现了

    Recently, the contrastive language-image pre-training, e.g., CLIP, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a CLIP-based model named SegCLIP for the topic of open-vocabulary segmentation in an annotation-free manner. The SegCLIP achieves segmentation based on ViT and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based KL loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves c
    
[^112]: 背景混合增强用于弱监督变化检测

    Background-Mixed Augmentation for Weakly Supervised Change Detection. (arXiv:2211.11478v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11478](http://arxiv.org/abs/2211.11478)

    本文通过背景混合增强和弱监督算法，成功地解决了变化检测的泛化问题。

    

    变化检测是从两个在同一场景中拍摄的图像中分离出物体变化（如物体的缺失或出现）与背景变化（如光线和季节变化）的过程，在灾难管理、城市发展等方面具有重要应用。本文从数据增强的角度研究了变化检测的泛化问题，并开发了一种新的弱监督训练算法，只需要图像级别标签。

    Change detection (CD) is to decouple object changes (i.e., object missing or appearing) from background changes (i.e., environment variations) like light and season variations in two images captured in the same scene over a long time span, presenting critical applications in disaster management, urban development, etc. In particular, the endless patterns of background changes require detectors to have a high generalization against unseen environment variations, making this task significantly challenging. Recent deep learning-based methods develop novel network architectures or optimization strategies with paired-training examples, which do not handle the generalization issue explicitly and require huge manual pixel-level annotation efforts. In this work, for the first attempt in the CD community, we study the generalization issue of CD from the perspective of data augmentation and develop a novel weakly supervised training algorithm that only needs image-level labels. Different from ge
    
[^113]: 终身赌博优化：无先验知识和无后悔算法

    Lifelong Bandit Optimization: No Prior and No Regret. (arXiv:2210.15513v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.15513](http://arxiv.org/abs/2210.15513)

    本文提出了一种算法LIBO，可以无需直接访问数据，对一系列赌博优化任务进行学习和适应，并保证最优性能和亚线性终身后悔率。

    

    机器学习算法经常重复应用于相似结构的问题。本文关注解决一系列赌博优化任务，并开发了一种适应环境的算法LIBO。我们假设内核化结构，其中的内核在所有任务中都是未知的但共享的。LIBO依次元学习一个逼近真实内核的内核，然后用最新的内核估计来解决即将到来的任务。本算法可以与任何内核化或线性赌博算法配对，并保证最优的预期性能。如果与亚线性赌博算法配对，LIBO将产生一个亚线性终身后悔率。

    Machine learning algorithms are often repeatedly applied to problems with similar structure over and over again. We focus on solving a sequence of bandit optimization tasks and develop LIBO, an algorithm which adapts to the environment by learning from past experience and becomes more sample-efficient in the process. We assume a kernelized structure where the kernel is unknown but shared across all tasks. LIBO sequentially meta-learns a kernel that approximates the true kernel and solves the incoming tasks with the latest kernel estimate. Our algorithm can be paired with any kernelized or linear bandit algorithm and guarantees oracle optimal performance, meaning that as more tasks are solved, the regret of LIBO on each task converges to the regret of the bandit algorithm with oracle knowledge of the true kernel. Naturally, if paired with a sublinear bandit algorithm, LIBO yields a sublinear lifelong regret. We also show that direct access to the data from each task is not necessary for
    
[^114]: 带有实证文本表征的语义特征的文本蕴涵识别

    Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.09723](http://arxiv.org/abs/2210.09723)

    本文提出了一种利用实验证据的文本表征和语义特征的新方法，通过元素曼哈顿距离向量特征识别文本-假设之间的蕴涵关系，并在基准数据集上实现了显著的F1分数提高。

    

    文本蕴涵识别是自然语言理解中基本的任务之一。在自动识别文本蕴含之前，理解句子的含义是必要的前提。如果前提为真，则文本蕴涵假设也为真。经典的方法通常利用来自词嵌入的每个单词的特征值来表示句子。本文提出了一种新的方法，以识别文本和假设之间的蕴含关系，并引入了一个新的实证基于阈值的语义文本表征。我们采用一个基于元素的曼哈顿距离向量特征，可以识别文本-假设对之间的语义蕴涵关系。我们对基准蕴涵分类(SICK-RTE)数据集进行了几项实验。我们使用我们提出的方法训练了几个机器学习(ML)模型，并将它们的性能与经典的和最先进的模型进行了比较。我们提出的方法在F1分数方面显著优于经典模型，并在大多数最先进的模型方面表现出色。

    Textual entailment recognition is one of the basic natural language understanding(NLU) tasks. Understanding the meaning of sentences is a prerequisite before applying any natural language processing(NLP) techniques to automatically recognize the textual entailment. A text entails a hypothesis if and only if the true value of the hypothesis follows the text. Classical approaches generally utilize the feature value of each word from word embedding to represent the sentences. In this paper, we propose a novel approach to identifying the textual entailment relationship between text and hypothesis, thereby introducing a new semantic feature focusing on empirical threshold-based semantic text representation. We employ an element-wise Manhattan distance vector-based feature that can identify the semantic entailment relationship between the text-hypothesis pair. We carried out several experiments on a benchmark entailment classification(SICK-RTE) dataset. We train several machine learning(ML) 
    
[^115]: PromptCast：一种新的基于提示的时间序列预测范式

    PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.08964](http://arxiv.org/abs/2210.08964)

    提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。

    

    本文提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast）。在这种新的任务中，将原来的数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，使得语言模型可以直接应用于预测的目的。为了支持和促进这个任务的研究，我们还提出了一个大规模的数据集（PISA）。

    This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
    
[^116]: 公平性概念及其相关张力研究综述

    Survey on Fairness Notions and Related Tensions. (arXiv:2209.13012v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2209.13012](http://arxiv.org/abs/2209.13012)

    本文调查了公平性的不同概念以及它们与其他期望属性的紧张关系，并介绍了处理公平性-准确性权衡问题的不同方法。

    

    自动决策系统越来越多地用于解决招聘和贷款等涉及重大决策的问题，希望用机器学习算法代替主观人为决策。然而，基于机器学习的决策系统容易出现偏见，导致不公平的决策。文献中定义了几种公平性概念以捕捉这个伦理和社会概念的不同微妙之处（例如统计平等、机会平等等）。在学习模型时需要满足公平性要求，这产生了不同的公平概念之间以及隐私和分类准确性等其他期望属性之间的紧张关系。本文概述了通常使用的公平性概念，并讨论了它们与隐私和准确性之间的张力。本综述介绍了解决公平性与准确性权衡问题的不同方法（分为预处理、处理中、后处理和混合四种方法）。

    Automated decision systems are increasingly used to take consequential decisions in problems such as job hiring and loan granting with the hope of replacing subjective human decisions with objective machine learning (ML) algorithms. However, ML-based decision systems are prone to bias, which results in yet unfair decisions. Several notions of fairness have been defined in the literature to capture the different subtleties of this ethical and social concept (e.g., statistical parity, equal opportunity, etc.). Fairness requirements to be satisfied while learning models created several types of tensions among the different notions of fairness and other desirable properties such as privacy and classification accuracy. This paper surveys the commonly used fairness notions and discusses the tensions among them with privacy and accuracy. Different methods to address the fairness-accuracy trade-off (classified into four approaches, namely, pre-processing, in-processing, post-processing, and hy
    
[^117]: 针对干预密度估计的正则化流

    Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06203](http://arxiv.org/abs/2209.06203)

    本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。

    

    现有机器学习方法针对因果推断通常通过潜在结果的均值（例如平均处理效应）来计算数量。然而，这些数量并不能完全捕捉潜在结果分布的全部信息。本研究旨在从观测数据中估计干预后的潜在结果密度。为此，我们提出了一种新的全参数深度学习方法，称为干预正则化流。具体而言，我们组合了两种正则化流，即（i）用于估计干扰参数的nuisance flow和（ii）用于参数化估计潜在结果密度的target flow。我们进一步基于单步偏差校正开发了一个易于处理的优化目标，以有效和双重稳健的方式估计目标流参数。因此，我们的干预正则化流提供了一个正确归一化的密度估计器。在各种实验中，我们展示了我们的干预正则化流方法优于现有的用于从观测数据中估计潜在结果密度的最先进的方法。

    Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
    
[^118]: TFN：基于时频转换的可解释神经网络用于智能故障诊断

    TFN: An Interpretable Neural Network with Time-Frequency Transform Embedded for Intelligent Fault Diagnosis. (arXiv:2209.01992v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.01992](http://arxiv.org/abs/2209.01992)

    提出一种新的可解释神经网络模型——时频网络（TFN）。在传统卷积层中嵌入了物理上有意义的时频变换（TFT）方法作为自适应预处理层，该层不仅提高了诊断性能，还使得网络结构可以可解释，故障诊断过程透明。

    

    卷积神经网络（CNN）因其强大的特征提取和分类能力而广泛应用于机械系统的故障诊断。然而，CNN是一种典型的黑匣子模型，其决策机制不清晰，这限制了它在高可靠性的故障诊断场景中的应用。为解决这个问题，我们提出了一种新的可解释神经网络，称为时频网络（TFN），其中物理上有意义的时频变换（TFT）方法嵌入到传统卷积层中作为自适应预处理层。这个预处理层称为时频卷积（TFconv）层，受到一个精心设计的核函数的限制，用于提取与故障相关的时频信息。它不仅提高了诊断性能，而且揭示了CNN在频率域内预测的逻辑基础。不同的TFT方法对应着TFconv层的不同核函数，使网络结构可解释，故障诊断过程透明。轴承故障数据集和齿轮箱振动数据集上的实验结果表明，所提出的TFN相对于现有的基于CNN的故障诊断方法具有更优的诊断性能，而提取的时频特征可以有效地揭示故障频率特征和突出潜在故障源。

    Convolutional Neural Networks (CNNs) are widely used in fault diagnosis of mechanical systems due to their powerful feature extraction and classification capabilities. However, the CNN is a typical black-box model, and the mechanism of CNN's decision-making are not clear, which limits its application in high-reliability-required fault diagnosis scenarios. To tackle this issue, we propose a novel interpretable neural network termed as Time-Frequency Network (TFN), where the physically meaningful time-frequency transform (TFT) method is embedded into the traditional convolutional layer as an adaptive preprocessing layer. This preprocessing layer named as time-frequency convolutional (TFconv) layer, is constrained by a well-designed kernel function to extract fault-related time-frequency information. It not only improves the diagnostic performance but also reveals the logical foundation of the CNN prediction in the frequency domain. Different TFT methods correspond to different kernel fun
    
[^119]: 提示作为探测器：利用语言模型进行知识库构建

    Prompting as Probing: Using Language Models for Knowledge Base Construction. (arXiv:2208.11057v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.11057](http://arxiv.org/abs/2208.11057)

    本文介绍了一种利用语言模型进行知识库构建的方法，该方法采用了多种提示技术，手动提示策略的编制至关重要，并且必须鼓励语言模型给出不同长度的答案集，特别是包括空答案集。实体别名字典可以提高语言模型的得分。

    

    语言模型已经被证明在各种下游应用中都很有用，例如摘要、翻译、问答和文本分类。由于它们可以存储大量信息，因此语言模型正在成为人工智能中越来越重要的工具。本文介绍了ProP（提示作为探测器），它利用OpenAI在2020年提出的大型语言模型GPT-3来执行知识库构建任务。ProP采用多步骤方法，结合各种提示技术来实现这一目标。我们的结果表明，手动提示策略的编制至关重要；必须鼓励语言模型给出不同长度的答案集，特别是包括空答案集；真/假问题是增加语言模型生成的建议的准确性的有用方法；语言模型的大小是一个至关重要的因素；实体别名字典可以提高语言模型的得分。

    Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation stu
    
[^120]: 零标签的多元时间序列异常检测

    Detecting Multivariate Time Series Anomalies with Zero Known Label. (arXiv:2208.02108v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.02108](http://arxiv.org/abs/2208.02108)

    本文提出了一种利用动态图和实体注意力机制实现零标签多元时间序列异常检测的方法，其利用密度估计比较异常和正常实例，性能优于多种无监督方法和半监督方法。

    

    多元时间序列异常检测已在半监督环境下 extensively studied，但需要训练集中的所有正常样本，这很费力。因此，本文提出了一种无监督的异常检测方法，MTGFlow，基于一个广泛的假设，即异常实例的密度比正常实例稀疏。MTGFlow 建立动态图捕捉实体间的交互，并利用实体注意机制进行密度和异常分数建模，不仅能有效解决实体特征多样性和密度估计问题，而且在多个基准数据集上的实验结果表明，MTGFlow 比几种先进的无监督方法优越，并即使与半监督方法相比也有竞争力。

    Multivariate time series anomaly detection has been extensively studied under the semi-supervised setting, where a training dataset with all normal instances is required. However, preparing such a dataset is very laborious since each single data instance should be fully guaranteed to be normal. It is, therefore, desired to explore multivariate time series anomaly detection methods based on the dataset without any label knowledge. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for multivariate time series anomaly detection via dynamic graph and entity-aware normalizing flow, leaning only on a widely accepted hypothesis that abnormal instances exhibit sparse densities than the normal. However, the complex interdependencies among entities and the diverse inherent characteristics of each entity pose significant challenges on the density estimation, let alone to detect anomalies based on the estimated possibility distribution. To tackle these problems, we prop
    
[^121]: 基于Transformer的大语料库语义相似度分析的认知研究

    A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.11716](http://arxiv.org/abs/2207.11716)

    本文通过使用Transformer在U.S Patent Phrase to Phrase Matching Dataset上进行语义相似度分析，提高了算法效率，达到了令人满意的结果。

    

    语义相似度分析和建模是当今自然语言处理许多先驱应用中基本认可的任务。由于顺序模式识别的感知，许多神经网络（如RNN和LSTM）在语义相似度建模方面取得了令人满意的结果。但是，由于它们无法以非顺序方式处理信息，因此这些解决方案被认为效率低下，从而导致上下文提取不当。Transformer因其非顺序数据处理和自我关注等优势而成为最先进的架构。本文使用传统和基于transformer的技术对美国专利短语进行语义相似度分析和建模。我们对四种不同版本的解码增强BERT-DeBERTa进行实验，并通过K折交叉验证来提高其性能。实验结果证明了我们的方法的有效性。

    Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me
    
[^122]: 幻觉攻击：对顺序决策者的敌对攻击中可检测性很重要

    Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (arXiv:2207.10170v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.10170](http://arxiv.org/abs/2207.10170)

    对于顺序决策者的敌对攻击来说，弱点是缺乏时间上的一致性，使其容易被检测出来；而R-attack是一种既有效又可证明是统计不可检测的攻击，可以更难以使用自动化方法检测出来。

    

    在实际世界中部署的自主代理需要对感官输入的敌对攻击具备强大的鲁棒性。强化代理策略需要预测可能的最强攻击。我们证明了现有的强化学习代理的观测空间攻击具有共同的弱点：虽然有效，但它们缺乏时间上的一致性，因此可以使用自动化手段或人工检查来检测。对于敌手来说，可检测性是不希望出现的，因为它可能会引发安全事态升级。我们引入了完美的幻觉攻击，这是一种新形式的顺序决策者的敌对攻击，既有效又可证明是统计不可检测的。随后，我们提出了更加灵活的R-attack，其生成的观测转换与无敌对环境的状态转换函数一致且可以端到端学习。实验结果显示，与现有的攻击相比，R-attack更难以使用自动化方法检测出来。

    Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, 
    
[^123]: 基于可解释的深度强化学习的无人机引导和规划的鲁棒性对抗攻击检测

    Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (arXiv:2206.02670v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02670](http://arxiv.org/abs/2206.02670)

    本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受对抗攻击。

    

    针对无人机在公共领域遭受对抗攻击的风险增加的问题，本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受攻击。该方案采用深度强化学习进行引导和规划，利用人工势场来提高训练效率和障碍物避免率。

    The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance per
    
[^124]: 强健的马尔可夫决策过程即时学习

    Robust Anytime Learning of Markov Decision Processes. (arXiv:2205.15827v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.15827](http://arxiv.org/abs/2205.15827)

    文章介绍了一种将贝叶斯推理方案和计算强健策略相结合的、不断学习马尔可夫决策过程转移概率的方法，阐述了不确定MDP（uMDP）的概念，针对应用中有限数据导致的统计误差问题提出了基于不确定性集的解决方法，并介绍了计算强健策略以遵循形式规范的工具。

    

    马尔可夫决策过程（MDP）是经常用于顺序决策的形式模型。MDP通过转移函数中的概率捕获可能出现的来自不精确执行器的随机性。然而，在数据驱动的应用中，从（有限的）数据中推导出准确的概率会引入统计误差，可能导致意外或不良结果。不确定MDP（uMDP）不需要准确的概率，而是使用所谓的不确定性集，在转换中考虑这些有限的数据。我们通过组合专门的贝叶斯推理方案和计算强健策略的方法，在一个强健的即时学习方法中不断学习MDP的转移概率。特别地，我们的方法（1）近似p

    Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates p
    
[^125]: 非参数分类中的欠采样是一种极小化极差风险的鲁棒性干预方法

    Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13094](http://arxiv.org/abs/2205.13094)

    该论文证明在非参数二元分类中，缺乏少数派样本是学习的根本限制，并探讨了欠采样算法的最小化极差风险的鲁棒性表现，特别是在标签转移的情况下可以最优化。

    

    尽管已经提出了广泛的技术来解决分布偏移问题，但在几个流行的基准测试中，基于欠采样的平衡数据集的训练通常能够实现接近最先进准确性。我们证明了在非参数二元分类设置下，学习的基本限制是由于缺乏少数群体样本而产生的。我们的结果表明，除非训练和测试分布之间存在高度重叠（这在真实数据集中不太可能），否则算法无法超越欠采样，除非算法利用有关分布偏移的其他结构。特别地，在标签转移的情况下，我们证明了总是存在一种最小化极差风险的欠采样算法。在组转换的情况下，我们介绍了一类最小极差风险的欠采样算法。我们在合成和真实数据集上进行了验证实验以验证我们的理论结果。

    While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an $\textit{undersampled}$ balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of grou
    
[^126]: 视觉-语音自监督模型中的词语发现

    Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v5 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2203.15081](http://arxiv.org/abs/2203.15081)

    这篇论文介绍了一种基于图像-语音联合训练的自监督模型，在模型训练后实现了自动词语分割和聚类的能力，并在两个任务中表现优异。

    

    我们提出了一种基于图像-语音联合训练的自监督模型，能够实现词语的自动分割和聚类，并在 Buckeye 词分割和 ZeroSpeech 任务中展现了与当前已发表的方法相当的甚至更好的表现。实验表明，这种能力并未出现在基本的 HuBERT 和 wav2vec2.0 模型中，视觉联结任务是我们观察到的词语发现能力的重要组成部分。

    We present a method for visually-grounded spoken term discovery. After training either a HuBERT or wav2vec2.0 model to associate spoken captions with natural images, we show that powerful word segmentation and clustering capability emerges within the model's self-attention heads. Our experiments reveal that this ability is not present to nearly the same extent in the base HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a crucial component of the word discovery capability we observe. We also evaluate our method on the Buckeye word segmentation and ZeroSpeech spoken term discovery tasks, where we perform on par with or better than currently published methods on several metrics. Code and model weights are available at https://github.com/jasonppy/word-discovery.
    
[^127]: 针对道路交通速度的长短期记忆模型中动态时空背景的理解

    Understanding Dynamic Spatio-Temporal Contexts in Long Short-Term Memory for Road Traffic Speed Prediction. (arXiv:2112.02409v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.02409](http://arxiv.org/abs/2112.02409)

    本论文提出了一种动态局部化的LSTM模型，能够同时考虑道路之间的空间与时间依赖关系，该模型比基线方法具有更优异的预测性能。

    

    可靠的交通流预测对于创建智能交通系统至关重要。虽然许多基于大数据的预测方法已经被开发，但它们并没有反映出考虑时间和位置的复杂动态道路交互作用。在本研究中，我们提出了一种动态局部化的长短期记忆（LSTM）模型，涉及道路之间的空间和时间依赖关系。为此，我们使用局部动态空间权重矩阵及其动态变化。此外，LSTM模型可以处理具有长依赖性和复杂非线性特征的序列数据。实证结果表明，所提出的模型相比于两种不同的基线方法具有更优异的预测性能。

    Reliable traffic flow prediction is crucial to creating intelligent transportation systems. Many big-data-based prediction approaches have been developed but they do not reflect complicated dynamic interactions between roads considering time and location. In this study, we propose a dynamically localised long short-term memory (LSTM) model that involves both spatial and temporal dependence between roads. To do so, we use a localised dynamic spatial weight matrix along with its dynamic variation. Moreover, the LSTM model can deal with sequential data with long dependency as well as complex non-linear features. Empirical results indicated superior prediction performances of the proposed model compared to two different baseline methods.
    
[^128]: 非商业P2P共乘中的效率、公平性和稳定性

    Efficiency, Fairness, and Stability in Non-Commercial Peer-to-Peer Ridesharing. (arXiv:2110.01152v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2110.01152](http://arxiv.org/abs/2110.01152)

    本文聚焦于P2P共乘中最核心的问题：乘客和司机的匹配。在考虑到用户偏好的前提下，本文提出了关于公平性和稳定性的新概念以及高效匹配算法，可以在不影响系统范围内效率的情况下获得公平和稳定的共乘解决方案。

    

    与商业共乘不同，非商业P2P共乘受到了有限的研究，虽然它可以促进非城市社区的可行解决方案。本文关注P2P共乘中的核心问题：乘客和司机的匹配。我们将用户的偏好提升为一级考虑，并在P2P共乘中引入了公平性和稳定性的新概念。我们提出了高效匹配算法，同时考虑到用户中心化因素，包括用户的首选出发时间、公平性和稳定性。结果表明，可以在合理的计算时间内获得公平和稳定的解决方案，且可以基于系统范围内的效率独占地改善基线结果。

    Unlike commercial ridesharing, non-commercial peer-to-peer (P2P) ridesharing has been subject to limited research -- although it can promote viable solutions in non-urban communities. This paper focuses on the core problem in P2P ridesharing: the matching of riders and drivers. We elevate users' preferences as a first-order concern and introduce novel notions of fairness and stability in P2P ridesharing. We propose algorithms for efficient matching while considering user-centric factors, including users' preferred departure time, fairness, and stability. Results suggest that fair and stable solutions can be obtained in reasonable computational times and can improve baseline outcomes based on system-wide efficiency exclusively.
    
[^129]: 学习将周例解决方案进行组合用于神经程序合成

    Learning to Combine Per-Example Solutions for Neural Program Synthesis. (arXiv:2106.07175v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2106.07175](http://arxiv.org/abs/2106.07175)

    该论文提出了一种把程序合成问题分为两个阶段的方法，提高了解决方案的成功率。作者使用了由多头注意机制构建的Cross Aggregator神经网络模块，学习如何组合每个样例程序解决方案，生成全局解决方案。

    

    例子程序合成的目标是找到一个与给定的输入输出样例一致的计算机程序。大多数基于学习的方法尝试找到满足所有样例的程序。相比之下，我们的工作考虑将问题分为两个阶段的方法：(a)找到只满足一个样例的程序，(b)利用这些单例程序解决方案生成满足所有样例的程序。我们引入了基于多头注意机制的Cross Aggregator神经网络模块，学习如何组合这些单例方案中存在的提示来合成一个全局解决方案。在不同长度的程序和两种不同实验设置下的评估表明，当在相同的时间预算下时，我们的技术显著提高了成功率，超过了PCCoder [Zohar等人，2018]和其他消融基线。我们的工作的代码、数据和训练模型可以在https://github.com/shriva找到。

    The goal of program synthesis from examples is to find a computer program that is consistent with a given set of input-output examples. Most learning-based approaches try to find a program that satisfies all examples at once. Our work, by contrast, considers an approach that breaks the problem into two stages: (a) find programs that satisfy only one example, and (b) leverage these per-example solutions to yield a program that satisfies all examples. We introduce the Cross Aggregator neural network module based on a multi-head attention mechanism that learns to combine the cues present in these per-example solutions to synthesize a global solution. Evaluation across programs of different lengths and under two different experimental settings reveal that when given the same time budget, our technique significantly improves the success rate over PCCoder [Zohar et. al 2018] and other ablation baselines. The code, data and trained models for our work can be found at https://github.com/shriva
    
[^130]: 随机变分不等式的简单而最优方法I：算子外推法

    Simple and optimal methods for stochastic variational inequalities, I: operator extrapolation. (arXiv:2011.02987v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2011.02987](http://arxiv.org/abs/2011.02987)

    本文提出了一种简单实用的算子外推法用来解决变分不等式问题，同时还提出了一种随机算子外推法实现随机平滑和强单调VI的最优复杂度。

    

    本文首先提出了一种新颖的算子外推法（OE）来解决确定性变分不等式（VI）问题。类似于梯度（算子）投影法，OE通过在每次迭代中求解单个投影子问题来更新一个搜索序列。我们证明了OE比现有方法更简单地实现了解决各种VI问题的最优收敛速率。然后，我们介绍了随机算子外推（SOE）方法，并建立了其用于解决不同随机VI问题的最优收敛行为。特别地，SOE首次在文献中实现了用于解决基础问题（即随机平滑和强单调VI）的最优复杂度。我们还提出了随机块算子外推（SBOE）方法，以进一步降低应用于具有特定块结构的大规模确定性VI的OE方法的迭代成本。数值实验证明了所提出方法的有效性和效率。

    In this paper we first present a novel operator extrapolation (OE) method for solving deterministic variational inequality (VI) problems. Similar to the gradient (operator) projection method, OE updates one single search sequence by solving a single projection subproblem in each iteration. We show that OE can achieve the optimal rate of convergence for solving a variety of VI problems in a much simpler way than existing approaches. We then introduce the stochastic operator extrapolation (SOE) method and establish its optimal convergence behavior for solving different stochastic VI problems. In particular, SOE achieves the optimal complexity for solving a fundamental problem, i.e., stochastic smooth and strongly monotone VI, for the first time in the literature. We also present a stochastic block operator extrapolations (SBOE) method to further reduce the iteration cost for the OE method applied to large-scale deterministic VIs with a certain block structure. Numerical experiments have 
    
[^131]: 部分观测下的协作多智能体强化学习

    Cooperative Multi-Agent Reinforcement Learning with Partial Observations. (arXiv:2006.10822v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.10822](http://arxiv.org/abs/2006.10822)

    本文提出了一种基于局部状态和动作信息的分布式零阶策略优化方法，可用于部分观测的协作多智能体强化学习，减小通信开销并取得更好的效果。

    

    本文提出了一种分布式的零阶策略优化方法，用于多智能体强化学习（MARL）。现有的MARL算法通常假设每个智能体都可以观察网络中所有其他智能体的状态和动作。但在大规模问题中，与多跳邻居共享状态和动作信息可能会导致显着的通信开销。提出的零阶策略优化方法的优势在于，它允许智能体仅基于局部的、部分的状态和动作信息来计算本地策略梯度，从而更新它们的本地策略函数，并使用共识来获得依赖于全局累积奖励的局部估计。具体来说，为了计算本地策略梯度，我们开发了一种新的分布式零阶策略梯度估计器，它依赖于一点残差反馈， im同时与现有的依赖于一点反馈的零阶估计器相比，显著降低了通信开销。我们在几个协作多智能体基准任务上展示了提出方法的有效性，并表明它胜过了现有的假设具有全观察信息的方法。

    In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-poi
    
[^132]: 基于元学习的源码模型实时适应性研究

    On-the-Fly Adaptation of Source Code Models using Meta-Learning. (arXiv:2003.11768v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2003.11768](http://arxiv.org/abs/2003.11768)

    本文提出了一种基于元学习的方法用于源码模型的实时适应性研究，以提高源代码模型的预测准确性，解决代码自动完成问题。

    

    适应未知的本地环境是成功的源代码模型必须克服的重要挑战之一。动态评估是最流行的适应模型的方法之一，但本文提出了一种不同的方法——将上下文适应问题转化为元学习问题。我们旨在训练一个基本的源码模型，能够从文件中的信息提取最佳的支持标记进行学习，以提供缺失标记的改进预测。与动态评估不同，这个公式允许我们选择更有针对性的信息（支持标记）进行适应，即在目标文件中的目标空位置之前和之后。我们考虑了一种称为行级维护的评估设置，旨在反映IDE中代码自动完成的下游任务。

    The ability to adapt to unseen, local contexts is an important challenge that successful models of source code must overcome. One of the most popular approaches for the adaptation of such models is dynamic evaluation. With dynamic evaluation, when running a model on an unseen file, the model is updated immediately after having observed each token in that file. In this work, we propose instead to frame the problem of context adaptation as a meta-learning problem. We aim to train a base source code model that is best able to learn from information in a file to deliver improved predictions of missing tokens. Unlike dynamic evaluation, this formulation allows us to select more targeted information (support tokens) for adaptation, that is both before and after a target hole in a file. We consider an evaluation setting that we call line-level maintenance, designed to reflect the downstream task of code auto-completion in an IDE. Leveraging recent developments in meta-learning such as first-o
    

