# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Putting People in Their Place: Affordance-Aware Human Insertion into Scenes.](http://arxiv.org/abs/2304.14406) | 该论文提出了一种方法，在场景中插入现实姿态的人，以及合成能够在场景中与人类进行自然交互的场景。 |
| [^2] | [Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement.](http://arxiv.org/abs/2304.14391) | 本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。 |
| [^3] | [Analogy-Forming Transformers for Few-Shot 3D Parsing.](http://arxiv.org/abs/2304.14382) | "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。 |
| [^4] | [CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants.](http://arxiv.org/abs/2304.14364) | 本文提出了一种名为CONSCENDI的蒸馏方法，用于构建防护栏模型，以监控任务型虚拟助手的输出。关键方法包括场景增强生成和对比训练样例。这种方法产生了一组多样化的违反规则的对话训练集，并且可以更好地检测代理的输出是否符合设计者指定的规则。 |
| [^5] | [MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning.](http://arxiv.org/abs/2304.14339) | 本文描述了一种在多语言设置下使用多标签对比损失来微调大型预训练语言模型的方法，取得了在五种语言上最好的结果。 |
| [^6] | [An Audit Framework for Adopting AI-Nudging on Children.](http://arxiv.org/abs/2304.14338) | 本文提出了一个AI-nudging审计框架，以确保使用nudges的AI系统保持道德惯性和中立性。该框架包括风险缓解和强化机制。 |
| [^7] | [ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT.](http://arxiv.org/abs/2304.14334) | 本论文研究了使用ChatGPT生成和增强训练数据的方法，证明在适当的特定任务的提示下，其效果优于现有方法，并提出了评估生成数据质量的方法。 |
| [^8] | [Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space.](http://arxiv.org/abs/2304.14333) | 本文利用结构探测方法研究了成语在静态和上下文嵌入中的编码情况，并发现仍未确定成语性是否在向量范数中编码，提出了改进数据集以进行探测分析的方向。 |
| [^9] | [Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+.](http://arxiv.org/abs/2304.14323) | 本文提出一种扩展逻辑——Standpoint EL+，允许公理否定、角色链公理、自环等特征，同时保持可行性。我们通过设计一个可满足性检测演绎演算法来实现它，这个算法需要解决实际应用问题。 |
| [^10] | [Large Language Models Are State-of-the-Art Evaluators of Code Generation.](http://arxiv.org/abs/2304.14317) | 本文提出了一个基于 GPT-3.5 的评估框架，解决了现有方法在代码生成任务上的局限性，取得了更好的相关性。 |
| [^11] | [Double-Deck Multi-Agent Pickup and Delivery: Multi-Robot Rearrangement in Large-Scale Warehouses.](http://arxiv.org/abs/2304.14309) | 该论文提出了一个名为双层多机器人提货和配送（DD-MAPD）的问题形式，涉及机器人的货架移动，以优化仓库布局。作者提出了MAPF-DECOMP的算法框架，有效地解决了此类问题。 |
| [^12] | [Instance Segmentation in the Dark.](http://arxiv.org/abs/2304.14298) | 本文提出了在黑暗环境下的实例分割方法。为了抑制低光图像中的特征噪声，该方法采用了自适应加权下采样层、平滑定向卷积块和扰动抑制学习等技术，同时发现高位深的RAW图像可以更好地保留更丰富的场景信息。 |
| [^13] | [Controlled Text Generation with Natural Language Instructions.](http://arxiv.org/abs/2304.14293) | InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。 |
| [^14] | [Distinguishing a planetary transit from false positives: a Transformer-based classification for planetary transit signals.](http://arxiv.org/abs/2304.14283) | 本文介绍了基于Transformer架构的新方法，用于高效准确地区分行星凌和误判。采用一种新的预处理输入数据的方法，使用正弦函数来保留信号的周期性。在模拟数据的评估中，该方法优于已有的基于CNN的方法。 |
| [^15] | [A Survey on Approximate Edge AI for Energy Efficient Autonomous Driving Services.](http://arxiv.org/abs/2304.14271) | 这篇论文概述了自主驾驶服务中的传感和数据处理现状，及其对能源和环境带来的挑战，进一步调查和比较了如何应用和整合现有技术创新以实现能源效率。 |
| [^16] | [Variational Bayes Made Easy.](http://arxiv.org/abs/2304.14251) | 该论文提出了一个三步骤方法，简化了变分贝叶斯近似推断方法的推导过程。 |
| [^17] | [Standpoint Linear Temporal Logic.](http://arxiv.org/abs/2304.14243) | 本文提出了一种新的逻辑——立场线性时态逻辑（SLTL），它将 LTL 的时间特性与 SL 的多视角建模能力相结合，为扩展现有的 LTL 推理器提供了一个清晰的路径。 |
| [^18] | [TorchBench: Benchmarking PyTorch with High API Surface Coverage.](http://arxiv.org/abs/2304.14226) | TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。 |
| [^19] | [LLT: An R package for Linear Law-based Feature Space Transformation.](http://arxiv.org/abs/2304.14211) | LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。 |
| [^20] | [Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining.](http://arxiv.org/abs/2304.14204) | 本文提出了一种基于医疗知识的多模态预训练方法，名为MOTOR，旨在实现医疗人工通用智能，将通用和特定知识融合，共同促进模型的预训练过程。 |
| [^21] | [ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task.](http://arxiv.org/abs/2304.14177) | 本研究比较了ChatGPT和现有模型在关键短语生成任务上的性能，并发现ChatGPT在所有测试数据集和环境中的表现均优于现有模型，适用于不同领域和文档长度的关键短语生成。 |
| [^22] | [Human Semantic Segmentation using Millimeter-Wave Radar Sparse Point Clouds.](http://arxiv.org/abs/2304.14132) | 本文提出了一种在毫米波雷达稀疏点云上进行人类语义分割的框架，该框架优于相机和激光雷达的隐私保护和抗干扰能力。同时，本文引入图结构和拓扑特征，并设计了一个含有全局特征模块和顺序特征模块的语义分割框架，成功解决了稀疏和时间拓扑特征的问题。 |
| [^23] | [Why not both? Complementing explanations with uncertainty, and the role of self-confidence in Human-AI collaboration.](http://arxiv.org/abs/2304.14130) | 本文研究了不确定性估计和模型解释如何影响用户对模型的依赖性、理解力和信任度以及自信心对他们的任务执行能力的影响，并提出了在人工智能协作中将两者结合起来的优势。 |
| [^24] | [Preference Inference from Demonstration in Multi-objective Multi-agent Decision Making.](http://arxiv.org/abs/2304.14126) | 该论文提出了一种基于演示的偏好推断算法，用于多目标决策问题中量化偏好权重，实证结果表明，算法相较于基线算法有着更高的准确性和效率。在未来的工作中，该算法可用于多智能体系统中推断对手的偏好。 |
| [^25] | [Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach.](http://arxiv.org/abs/2304.14115) | 本研究提出了一种基于动态权重的偏好推理算法(DWPI)，通过观察环境中的行为轨迹，可以推断执行多目标决策问题的代理的偏好。实验结果表明，DWPI在多个多目标马尔科夫决策过程中的表现优于两种现有偏好推理方法。 |
| [^26] | [ChatLog: Recording and Analyzing ChatGPT Across Time.](http://arxiv.org/abs/2304.14106) | ChatLog是一个分析ChatGPT随时间变化的数据集，通过提取ChatGPT知识和语言特征发现了一些稳定的特征，提高了RoBERTa-based detector在新版本ChatGPT上的鲁棒性。 |
| [^27] | [SocNavGym: A Reinforcement Learning Gym for Social Navigation.](http://arxiv.org/abs/2304.14102) | 本文提出了SocNavGym，对于社交导航领域的研究提供了一个轻便、快速、易用的仿真环境，可生成各种各样的社交导航场景，并促进了智能社交机器人的发展。 |
| [^28] | [Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics.](http://arxiv.org/abs/2304.14094) | 本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。 |
| [^29] | [Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning.](http://arxiv.org/abs/2304.14081) | 该论文介绍了一种半监督层次聚类框架ClusterFlow，可以在经过训练的深度卷积神经网络中使用密集的多维类别和特征数据来构建超空间地图，从而使其更具人类思考方式的功能，提高了其鲁棒性和实现关系推理的能力。 |
| [^30] | [Compositional 3D Human-Object Neural Animation.](http://arxiv.org/abs/2304.14070) | 提出了一种组合式的方法来解决人物-物体交互动画的挑战，以实现新的HOI的组合式动画控制。 |
| [^31] | [Interpretable Neural-Symbolic Concept Reasoning.](http://arxiv.org/abs/2304.14068) | 本文提出了第一个基于概念嵌入的可解释概念模型DCR，能够在多个数据集上实现接近最先进的准确性，相对于最先进的可解释概念模型提高了高达+25％，并产生能够解释其预测的人类可理解规则和真值度，适应性强。 |
| [^32] | [Lightweight, Pre-trained Transformers for Remote Sensing Timeseries.](http://arxiv.org/abs/2304.14065) | 设计针对远程传感器数据的自监督学习模型和训练技术，可以得到表现更好且更小的模型。预训练的遥感时间序列Transformer（Presto）在几个遥感基准测试中实现了最先进的结果。 |
| [^33] | [A Parameterized Theory of PAC Learning.](http://arxiv.org/abs/2304.14058) | 本文提出了一种参数化PAC学习理论，确定了两种固定参数可学习性的概念，从而能够以细粒度的方式分析高效和难以处理的PAC学习之间的界限。 |
| [^34] | [Interweaved Graph and Attention Network for 3D Human Pose Estimation.](http://arxiv.org/abs/2304.14045) | 该论文提出了一种名为交错图和注意力网络（IGANet）的模型，通过图卷积网络（GCNs）和注意力之间的双向通信，解决了先前单视角图像3D人体姿态估计中忽略全局和局部关联的问题。在人体姿态估计上取得了最先进的结果。 |
| [^35] | [Mimic-IV-ICD: A new benchmark for eXtreme MultiLabel Classification.](http://arxiv.org/abs/2304.13998) | 本文提出了一个公共基准套件，使用大型公共电子病历数据集进行ICD-10编码。该方法标准化数据预处理并建立全面的ICD编码基准数据集，促进了自动化ICD编码在未来研究领域的应用。 |
| [^36] | [Rotation and Translation Invariant Representation Learning with Implicit Neural Representations.](http://arxiv.org/abs/2304.13995) | 本文提出了一种使用隐式神经表示和超网络进行表示学习的方法，称为不变性表示学习，可以在复杂图像上学到分离的语义表示，并且和SCAN很好地协同工作，从而获得最先进的无监督聚类结果。 |
| [^37] | [Towards Explainable Collaborative Filtering with Taste Clusters Learning.](http://arxiv.org/abs/2304.13937) | 本文提出了一种利用品味聚类学习实现可解释性协同过滤的模型，在保证高准确性的同时为用户和项目提供可解释的聚类解释。 |
| [^38] | [Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy.](http://arxiv.org/abs/2304.13933) | 本研究发现，在机器学习模型训练中，对表现更好的少数族裔进行过采样会稍微减少不良影响，但也会降低模型精度。 |
| [^39] | [NIMS-OS: An automation software to implement a closed loop between artificial intelligence and robotic experiments in materials science.](http://arxiv.org/abs/2304.13927) | NIMS-OS是一款Python库，用于实现材料科学中人工智能和机器人实验之间闭环的自动化，无需人工干预。通过使用贝叶斯优化、无限目标自由探索、相图构建和随机探索等AI技术，NIMS-OS能够在实验条件如化学成分、反应温度和加热时间改变的情况下搜索光催化剂。经验证实，使用NIMS-OS可以找到具有高光化学产氢活性的前所未知的光催化剂。 |
| [^40] | [Level Assembly as a Markov Decision Process.](http://arxiv.org/abs/2304.13922) | 本文中提出了基于马尔科夫决策过程的自适应动态规划方法来生成适应玩家的动态关卡进度，并在两个案例研究中展示了其优于两个基线的效果。 |
| [^41] | [Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering.](http://arxiv.org/abs/2304.13911) | 本论文提出一种名为Fed-SP-SC和Fed-DP-CoT的技术，通过联邦提示和链式推理改进分布式同义问题的准确性，从而提高基于云的大型语言模型（LLMs）回答常见问题的精度，并进行了充分实验验证。 |
| [^42] | [LSTM based IoT Device Identification.](http://arxiv.org/abs/2304.13905) | 本研究提出了一种使用LSTM进行物联网设备识别的方法，以预防安全威胁和检测漏洞为目标。 |
| [^43] | [Rumor Detection with Hierarchical Representation on Bipartite Adhoc Event Trees.](http://arxiv.org/abs/2304.13895) | 该论文提出了一种名为BAET的新型谣言检测模型，利用以双分法特定事件树为基础的层次表示法，在利用现有社交网络中的拓扑结构和信息传播的影响力的基础上，具有更好的性能。 |
| [^44] | [CNN based IoT Device Identification.](http://arxiv.org/abs/2304.13894) | 本研究提出了一种基于CNN的物联网设备识别方法，可对设备进行识别并检测其安全漏洞。 |
| [^45] | [Discovering Object-Centric Generalized Value Functions From Pixels.](http://arxiv.org/abs/2304.13892) | 本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。 |
| [^46] | [highway2vec -- representing OpenStreetMap microregions with respect to their road network characteristics.](http://arxiv.org/abs/2304.13865) | 本文提出了一种基于OpenStreetMap城市道路网络的微区域的嵌入方法，以检测地图六边形在包含的道路网络中的相似性。 |
| [^47] | [Ensoul: A framework for the creation of self organizing intelligent ultra low power systems (SOULS) through evolutionary enerstatic networks.](http://arxiv.org/abs/2304.13863) | Ensoul是一种框架，通过enerstatic网络和开放进化技术结合，创造了能够独立于其嵌入的基质的自组织智能超低功耗系统(SOULS)。 |
| [^48] | [Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models.](http://arxiv.org/abs/2304.13855) | 这篇论文提出了一种新方法——多模态复合关联评分(MCAS)，用于衡量多模态生成模型中的性别偏差。与单模态小型单阶段模型的度量与量化不同，该方法能够高效、有效地识别和测量大型复杂的多模态生成模型中的性别偏差。 |
| [^49] | [Understand the Dynamic World: An End-to-End Knowledge Informed Framework for Open Domain Entity State Tracking.](http://arxiv.org/abs/2304.13854) | 本论文提出了一种名为KIEST的端到端知识驱动框架，该框架通过外部知识图谱显式地检索相关实体和属性，并使用动态知识粒度的编码器-解码器框架自回归生成所有实体状态变化。 |
| [^50] | [AI-based Predictive Analytic Approaches for safeguarding the Future of Electric/Hybrid Vehicles.](http://arxiv.org/abs/2304.13841) | 本文研究了基于人工智能的预测分析方法对电动和混合动力汽车的生产与发展的影响，为更加环保的交通需求做出贡献。 |
| [^51] | [On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective.](http://arxiv.org/abs/2304.13836) | 本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。 |
| [^52] | [Programmatically Grounded, Compositionally Generalizable Robotic Manipulation.](http://arxiv.org/abs/2304.13826) | 本文提出了一种模块化的方法ProgramPort，它利用语言指令的句法和语义结构，以更好地利用预训练视觉-语言（VL）模型。该框架使用语义解析器恢复一个可执行程序，由跨不同模态的基于视觉和行动的功能模块组成。 |
| [^53] | [Guaranteed Quantization Error Computation for Neural Network Model Compression.](http://arxiv.org/abs/2304.13812) | 本文提出了一种通过建立合并的神经网络，应用优化方法和可达性分析方法来计算保证的量化误差的方法，解决了神经网络压缩的保证输出误差计算问题。 |
| [^54] | [Towards ethical multimodal systems.](http://arxiv.org/abs/2304.13765) | 本文讨论了伦理评价多模态人工智能系统的挑战，通过创建多模态伦理数据库来协同判断系统是否满足伦理要求。 |
| [^55] | [TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation.](http://arxiv.org/abs/2304.13742) | 本文提出了TR0N，将预训练的无条件生成模型转化为高度任意的条件模型。TR0N不需要训练数据或微调，可以在MS-COCO上实现零-shot FID 10.9，并在采样速度上优于竞品，同时保持了多样性和质量。 |
| [^56] | [Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency.](http://arxiv.org/abs/2304.13738) | 本文介绍了可扩展的分布式AI框架，利用云计算提高深度学习性能和效率。通过概述常用的AI框架和云服务、探讨基于云的AI系统的关键方面和讨论云中AI工作负载的优化策略，提供了解决AI应用程序不断增长的计算需求的有效方法。 |
| [^57] | [The Internal State of an LLM Knows When its Lying.](http://arxiv.org/abs/2304.13734) | 该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。 |
| [^58] | [Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model.](http://arxiv.org/abs/2304.13731) | 本研究提出了一种使用指令调整的LLM Flan-T5作为文本编码器和基于潜在扩散模型(LDM)的方法TANGO生成文本到音频(TTA)的新方法，在AudioCaps测试集上表现优于先进的AudioLDM。 |
| [^59] | [Ensemble CNNs for Breast Tumor Classification.](http://arxiv.org/abs/2304.13727) | 本文使用集成CNN的方法对乳腺肿瘤进行分类，提高了分类性能5%以上，并在公共数据集上实现了高准确率、精度和召回率。 |
| [^60] | [SamurAI: A Versatile IoT Node With Event-Driven Wake-Up and Embedded ML Acceleration.](http://arxiv.org/abs/2304.13726) | SamurAI是一种多功能物联网节点，具有事件驱动唤醒和嵌入式机器学习加速器，通过利用两个芯片内子系统来弥合处理和能量的差距，并且为物联网应用提供了识别和自适应等功能。 |
| [^61] | [Prediction of brain tumor recurrence location based on multi-modal fusion and nonlinear correlation learning.](http://arxiv.org/abs/2304.13725) | 本文提出了一种基于深度学习的方法来预测脑肿瘤复发位置。使用迁移学习、多模态融合和非线性相关学习，能够有效提取数据中的特征并预测复发的位置。 |
| [^62] | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.](http://arxiv.org/abs/2304.13712) | 本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。 |
| [^63] | [Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning.](http://arxiv.org/abs/2304.13688) | 联邦机器学习是一种新的人工智能范式，可以从数据孤岛中创建AI模型，挑战在于建立多方合作业务模式。本研究系统化了联邦机器学习项目的社会技术挑战和业务模式。 |
| [^64] | [Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process.](http://arxiv.org/abs/2304.13671) | 本文研究了自动化ATM现金补充流程，提出了一个数学模型并给出了一个工具来评估各种不同的情况。在模拟数据集上，该模型与方法可以削减ATM现金运营成本。 |
| [^65] | [Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning.](http://arxiv.org/abs/2304.13443) | 本文提出了一种基于策略的强化学习方法，通过重新安排地铁时刻表和调整列车的停靠时间和巡航速度，优化扰动下的地铁系统能源效率，该方法在模拟环境下实验证明其优于基线方法，最高可达降低10.9%的牵引能量消耗和最高达提高47.9%的再生制动能量利用率，为城市轨道交通的节能问题提供了有效的解决方案。 |
| [^66] | [Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning.](http://arxiv.org/abs/2304.12520) | 我们提出了一种名为Hint-Aug的框架，利用先前预训练的FViTs学到的高度代表性特征来增强调参数据，解决了FViTs在少样本数据的情况下的“饥饿”特性，并成功地提高了FViT训练的鲁棒性和调参表现。 |
| [^67] | [Extreme Classification for Answer Type Prediction in Question Answering.](http://arxiv.org/abs/2304.12395) | 本文提出了使用Transformer模型（XBERT）进行极端多标签分类，通过将KG类型基于问题文本使用结构和语义特征进行聚类，以提高问题回答（QA）系统中语义答案类型预测（SMART）任务的性能，并获得最先进的结果。 |
| [^68] | [DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction.](http://arxiv.org/abs/2304.11015) | DIN-SQL通过将复杂的文本到SQL任务分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，显著提高了它们的表现，使准确性超过了当前最先进的技术。 |
| [^69] | [Assisting clinical practice with fuzzy probabilistic decision trees.](http://arxiv.org/abs/2304.07788) | 我们提出了一种基于概率树和模糊逻辑的新方法MedFP，用于辅助医学实践。该方法可以完全解释，允许临床医生产生、控制和验证整个诊断过程，并减少误诊率，通过提供不确定性和反事实分析的估计值。 |
| [^70] | [PI-FL: Personalized and Incentivized Federated Learning.](http://arxiv.org/abs/2304.07514) | PI-FL是一种个性化的联邦学习解决方案，使用基于令牌的激励机制奖励个性化训练，可以在尊重客户端隐私的同时生成高质量的个性化模型。 |
| [^71] | [Astroformer: More Data Might Not be All You Need for Classification.](http://arxiv.org/abs/2304.05350) | 该文提出了使用混合变换器 - 卷积架构的方法，结合新的堆栈设计、不同的相对自我注意层创建方式和精心选择的数据增强和正则化技术，从少量数据中学习，将此方法应用于Galaxy Zoo数据集，结果表明在少量数据的情况下取得了与以前方法相同的分类结果，并且不会损失性能。 |
| [^72] | [A Survey of Large Language Models.](http://arxiv.org/abs/2303.18223) | 本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。 |
| [^73] | [Natural Selection Favors AIs over Humans.](http://arxiv.org/abs/2303.16200) | 这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。 |
| [^74] | [Brain-inspired bodily self-perception model that replicates the rubber hand illusion.](http://arxiv.org/abs/2303.12259) | 该论文提出了一个基于大脑启发的身体自我感知模型，模拟橡胶手幻觉，为认识人类身体自我意识的计算机制提供了新的洞见。 |
| [^75] | [Synthetic Data Generator for Adaptive Interventions in Global Health.](http://arxiv.org/abs/2303.01954) | 通过HealthSyn生成基于真实世界的移动健康干预数据，以帮助在全球卫生领域中发展、测试和评估机器学习算法和干预措施。 |
| [^76] | [Symbolic Discovery of Optimization Algorithms.](http://arxiv.org/abs/2302.06675) | 该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。 |
| [^77] | [Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models.](http://arxiv.org/abs/2302.04464) | 本研究提出了一种定制化联邦学习系统，通过从多个维度个性化边缘模型来消除联邦学习异质性，能够在模型准确度、效率和公平性方面显著提高性能。 |
| [^78] | [Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning.](http://arxiv.org/abs/2302.03281) | 本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），该算法可保护有用的权重或特征，并基于它们的效用扰动不太有用的权重或特征。实验证明，UPGD有助于减少遗忘和保持可塑性，在连续学习中大有用处。 |
| [^79] | [Incorporating Recurrent Reinforcement Learning into Model Predictive Control for Adaptive Control in Autonomous Driving.](http://arxiv.org/abs/2301.13313) | 本文提出了一种基于循环强化学习和部分可观察的马尔可夫决策过程的自适应控制算法 $\textit{MPC-RRL}$，在CARLA模拟器中得到有效验证。 |
| [^80] | [Reflective Artificial Intelligence.](http://arxiv.org/abs/2301.10823) | 人类思考时带有反思特质，可以应对世界的模糊性、新兴知识和社会背景。当前主流人工智能缺乏这种能力，本文提出了一种反思人工智能代理的架构。 |
| [^81] | [A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image.](http://arxiv.org/abs/2301.02051) | 本文提出了一种仅使用机器人当前配置的单个RGB图像就可以恢复机器人操纵器关节角度的方法，该方法利用机器人的运动学模型并训练浅层神经网络，可在缺少本体感知时恢复系统功能。 |
| [^82] | [RFold: RNA Secondary Structure Prediction with Decoupled Optimization.](http://arxiv.org/abs/2212.14041) | 所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。 |
| [^83] | [Interactive Concept Bottleneck Models.](http://arxiv.org/abs/2212.07430) | 该论文提出了交互式概念瓶颈模型(CBMs)，使得模型可以向人类协作者查询某些概念的标签，从而提高最终预测结果的准确性。 |
| [^84] | [Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games.](http://arxiv.org/abs/2212.06027) | 该论文提出了一种针对多人不完全信息博弈的贝叶斯对手建模方法，在三人 Kuhn poker 中应用这种方法可以明显超过所有的代理商，包括准确的纳什均衡策略。 |
| [^85] | [Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning.](http://arxiv.org/abs/2212.03220) | 本文介绍了一种名为视觉查询调整（VQT）的简单而有效的方法，用于聚合Vision Transformers的中间特征。VQT在训练中具有内存效率，相比于许多其他 fine-tuning 方法，不需要对整个骨干进行反向传播。该方法在几个基准测试中优于最先进的微调方法。 |
| [^86] | [Crown-CAM: Interpretable Visual Explanations for Tree Crown Detection in Aerial Images.](http://arxiv.org/abs/2211.13126) | 本文提出了一种名为Crown-CAM的可解释的类激活映射方法，用于解决高空图像中树冠检测的问题，该方法通过无监督选择激活映射、计算局部得分映射和非上下文背景抑制等步骤，既可以有效地提供树冠的精细定位，又可以量化生成的解释的准确性和不准确性。 |
| [^87] | [An interpretable imbalanced semi-supervised deep learning framework for improving differential diagnosis of skin diseases.](http://arxiv.org/abs/2211.10858) | 本文提出了一种基于可解释性和不平衡半监督深度学习框架的研究，利用伪标签样本进行自我训练来解决皮肤疾病分类中的类别不平衡问题，并取得了令人满意的性能表现。 |
| [^88] | [Geometry-Complete Perceptron Networks for 3D Molecular Graphs.](http://arxiv.org/abs/2211.02504) | 本研究引入了一种新的几何完备的图神经网络 GCPNet，用于3D分子图的表示学习，并在多个几何任务上展示了其出色的预测性能。其中最佳表现是在蛋白质-配体结合亲和力预测上得到了比当前最先进方法高出5%以上的相关系数。 |
| [^89] | [A Systematic Survey of Chemical Pre-trained Models.](http://arxiv.org/abs/2210.16484) | 本文是对化学预训练模型领域的第一次系统回顾，提出了从头训练分子表示模型的局限性，总结了最新的进展和几个关键角度，包括分子描述符和编码器架构。 |
| [^90] | [Beyond calibration: estimating the grouping loss of modern neural networks.](http://arxiv.org/abs/2210.16315) | 本文提出了一个估计器来近似神经网络的分组损失，并表明现代神经网络在视觉和NLP中展示出显著的分组损失。 |
| [^91] | [Break The Spell Of Total Correlation In betaTCVAE.](http://arxiv.org/abs/2210.08794) | 本文提出一种新的迭代分解路径来打破betaTCVAE中的全相关性，从而使得VAE能够更加灵活地划分数据特征。实验结果表明模型容量和潜变量分组之间存在有趣的相关性。 |
| [^92] | [Context Generation Improves Open Domain Question Answering.](http://arxiv.org/abs/2210.06349) | 该论文提出了一个两阶段的闭式问答框架，首先通过生成上下文来提取相关知识，然后使用这个知识回答问题，实验证明该方法在三个问答基准测试上明显优于以往的封闭式问答方法，并且与开放式方法持平。 |
| [^93] | [Cell-Free Latent Go-Explore.](http://arxiv.org/abs/2208.14928) | 本文提出了无细胞Latent Go-Explore方法用于强化学习探索，通过学习潜在表示泛化到任何环境，实验结果表明其表现优异。 |
| [^94] | [DORA: Exploring outlier representations in Deep Neural Networks.](http://arxiv.org/abs/2206.04530) | 本文提出了一种名为DORA的数据不可知框架，用于分析深度神经网络中的表征空间，并可以识别不符合人类直观认知的表征。 |
| [^95] | [Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning.](http://arxiv.org/abs/2205.14410) | 本文提出了一种基于多源迁移学习的模块化技术，可以自动学习如何从先前学习的任务中提取有用信息，从而减少智能体在学习新任务时需要与环境互动的次数。 |
| [^96] | [SIBILA: A novel interpretable ensemble of general-purpose machine learning models applied to medical contexts.](http://arxiv.org/abs/2205.06234) | SIBILA是一种集成了机器学习和深度学习模型以及可解释性算法，并能够在医疗领域中实现个性化治疗预测的方法，具有较高的准确性和可解释性。 |
| [^97] | [PREME: Preference-based Meeting Exploration through an Interactive Questionnaire.](http://arxiv.org/abs/2205.02370) | 本论文提出了一个新的端到端框架，通过生成交互式问卷，实现基于偏好的会议探索，帮助用户快速探索会议内容。 |
| [^98] | [GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models.](http://arxiv.org/abs/2203.07281) | GrIPS是一种基于编辑的无梯度搜索方法，用于改进大型语言模型的任务指令，显著提高性能。 |
| [^99] | [PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?.](http://arxiv.org/abs/2202.05821) | 本文介绍了PETRAW挑战，探讨基于视频、运动学和分割数据进行手术工作流程识别的方法，结果显示多模态数据可以提高识别准确度。 |
| [^100] | [From SLAM to Situational Awareness: Challenges and Survey.](http://arxiv.org/abs/2110.00273) | 本文研究旨在连接广泛的多学科现有知识，为移动机器人构建完整的情境感知（SA）系统，以提升其自主能力。 |
| [^101] | [Quantization Backdoors to Deep Learning Commercial Frameworks.](http://arxiv.org/abs/2108.09187) | 本文揭示了商业框架中潜在的深度学习模型后门安全漏洞，可以通过量化攻击实现后门触发并逃避检测，从而危及已部署的模型安全，可能导致未经授权的数据访问。 |
| [^102] | [SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds.](http://arxiv.org/abs/2104.04891) | 本文提出一种新的弱监督方法SQN，通过利用点云局部邻域内强烈的语义相似性来隐式增强高度稀疏的监督信号，使得仅需要0.1％的随机注释点进行训练即可在七个大规模开放数据集上取得良好的性能，大大降低了完整点云注释的时间和成本。 |
| [^103] | [A Review on Deep Learning in UAV Remote Sensing.](http://arxiv.org/abs/2101.10861) | 该综述总结了近年来应用于UAV遥感中的深度学习算法，主要关注分类和回归技术。 |

# 详细

[^1]: 将人置于其场景中：考虑可供性的人类插入

    Putting People in Their Place: Affordance-Aware Human Insertion into Scenes. (arXiv:2304.14406v1 [cs.CV])

    [http://arxiv.org/abs/2304.14406](http://arxiv.org/abs/2304.14406)

    该论文提出了一种方法，在场景中插入现实姿态的人，以及合成能够在场景中与人类进行自然交互的场景。

    

    我们研究了推断场景可供性的问题，提出了一种实现在场景中插入人的方法。给定有标记区域的场景图像和一个人的图像，我们将这个人插入到场景中，并尊重场景可供性。我们的模型可以根据场景上下文推断出一组现实的姿态，重新调整参考人的姿态，并调和构图。我们通过学习在视频剪辑中重新姿态人类来以自我监督的方式设置任务。我们在一组包含 240 万个视频剪辑的数据集上使用大规模扩散模型进行训练，该模型会产生多样的合理姿态，同时尊重场景上下文。鉴于学习到的人 - 场景组合，我们的模型还可以在没有条件的情况下感应到真实的人和场景，以及启用交互式编辑。定量评估显示出，与以前的工作相比，我们的方法合成了更真实的人类外观和更自然的人 - 场景交互。

    We study the problem of inferring scene affordances by presenting a method for realistically inserting people into scenes. Given a scene image with a marked region and an image of a person, we insert the person into the scene while respecting the scene affordances. Our model can infer the set of realistic poses given the scene context, re-pose the reference person, and harmonize the composition. We set up the task in a self-supervised fashion by learning to re-pose humans in video clips. We train a large-scale diffusion model on a dataset of 2.4M video clips that produces diverse plausible poses while respecting the scene context. Given the learned human-scene composition, our model can also hallucinate realistic people and scenes when prompted without conditioning and also enables interactive editing. A quantitative evaluation shows that our method synthesizes more realistic human appearance and more natural human-scene interactions than prior work.
    
[^2]: 基于能量模型的零样本场景重新排列规划器

    Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])

    [http://arxiv.org/abs/2304.14391](http://arxiv.org/abs/2304.14391)

    本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    

    本文致力于开发一个场景重排框架，可以解释长指令以及在训练时从未见过的空间概念组合。我们提出使用相对对象排列的能量函数来表示语言指导的空间概念。语言解析器将指令映射到相应的能量函数，而开放式视觉语言模型将它们的参数基于场景中的相关对象进行修正。通过梯度下降求解能量函数的总和，并利用基于本地计算机视觉的策略将对象重新定位到推断的目标位置，即可生成目标场景配置。我们在已建立的指令导向操作基准测试以及我们提出的组合指令基准测试中测试了模型，结果表明，我们的模型的绩效优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
    
[^3]: 模拟形式转换器用于少样本3D解析

    Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])

    [http://arxiv.org/abs/2304.14382](http://arxiv.org/abs/2304.14382)

    "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。

    

    我们提出了一种称为“模拟网络”的模型，它在一组有标记的结构化3D场景中显式地编码领域知识（作为模型参数的一部分），并通过类比推理对3D物体场景进行分割：我们的模型首先从内存中检索相关场景及其相应的部分结构，然后通过端到端可学习的调制机制为输入场景预测类似的部分结构，而不是直接将场景映射到部分分割。通过对多个检索的记忆进行条件控制，预测混合匹配检索记忆的结构合成。在“模拟网络”中，一发、少发或多发学习被一致地处理，通过对适当的记忆集进行条件谓词，无论是从单个、少数还是许多存储实例中继承相似的解析。我们展示了“模拟网络”在许多样本情况下与最新的3D分割变压器模型相竞争。

    We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
    
[^4]: CONSCENDI: 一种反对比且场景引导的蒸馏方法来为虚拟助手构建防护栏模型

    CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])

    [http://arxiv.org/abs/2304.14364](http://arxiv.org/abs/2304.14364)

    本文提出了一种名为CONSCENDI的蒸馏方法，用于构建防护栏模型，以监控任务型虚拟助手的输出。关键方法包括场景增强生成和对比训练样例。这种方法产生了一组多样化的违反规则的对话训练集，并且可以更好地检测代理的输出是否符合设计者指定的规则。

    

    随着GPT-4等越来越强大的语言模型的出现，新一代的基于任务的虚拟助手应运而生。这些对话系统可以根据客户的具体用例进行定制，但确保代理生成的文本仅符合提示指令中设计者指定的规则是具有挑战性的。因此，聊天机器人设计师通常使用另一个称为防护栏模型的模型来验证代理输出是否与其规则和约束对齐。我们探索了使用蒸馏方法来构建防护栏模型，以监控使用GPT-4中的训练数据的第一个模型的输出。我们发现，我们的CONSCENDI过程包括两个关键步骤：场景增强生成和对比训练样例。在生成对话数据时，我们会生成一组违反规则的场景，这些场景列举了违反规则的多样化高级方式。这种场景引导方法产生了一组多样化的违反规则的对话训练集，并且它使得模型更容易检测到代理生成的文本是否符合设计者指定的规则。

    A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
    
[^5]: MarsEclipse在SemEval-2023任务3中的多语言和多标签框架检测及对比学习方法

    MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning. (arXiv:2304.14339v1 [cs.CL])

    [http://arxiv.org/abs/2304.14339](http://arxiv.org/abs/2304.14339)

    本文描述了一种在多语言设置下使用多标签对比损失来微调大型预训练语言模型的方法，取得了在五种语言上最好的结果。

    

    本文描述了我们在SemEval-2023任务3的子任务2上进行框架检测的系统。我们使用了多标签对比损失来微调大型预训练语言模型的多语言设置，取得了非常有竞争力的结果：我们的系统在官方测试集和官方排行榜上排名第一，对于我们有训练数据并能进行微调的六种语言中的五种语言。在这里，我们描述了我们的实验设置以及各种消融研究。我们的系统代码可在https://github.com/QishengL/SemEval2023上获得。

    This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing Detection. We used a multi-label contrastive loss for fine-tuning large pre-trained language models in a multi-lingual setting, achieving very competitive results: our system was ranked first on the official test set and on the official shared task leaderboard for five of the six languages for which we had training data and for which we could perform fine-tuning. Here, we describe our experimental setup, as well as various ablation studies. The code of our system is available at https://github.com/QishengL/SemEval2023
    
[^6]: 采用AI-Nudging对儿童进行审计的框架

    An Audit Framework for Adopting AI-Nudging on Children. (arXiv:2304.14338v1 [cs.CY])

    [http://arxiv.org/abs/2304.14338](http://arxiv.org/abs/2304.14338)

    本文提出了一个AI-nudging审计框架，以确保使用nudges的AI系统保持道德惯性和中立性。该框架包括风险缓解和强化机制。

    

    本文提出了一种AI-nudging的审计框架。与文献中通常讨论的静态"nudging"形式不同，我们在这里专注于一种使用大量数据来提供个性化、动态反馈和界面的nudging类型。我们将其称为AI-nudging。该审计的最终目标是确保使用nudges的AI系统通过遵守审计的建议、要求或建议（换句话说，审计的标准）保持一种道德惯性和中立性。在意外的负面影响的情况下，审计建议设置风险缓解机制。在意外的正面影响情况下，它建议一些强化机制。该研究由IBM-Notre Dame Tech Ethics Lab赞助。

    This is an audit framework for AI-nudging. Unlike the static form of nudging usually discussed in the literature, we focus here on a type of nudging that uses large amounts of data to provide personalized, dynamic feedback and interfaces. We call this AI-nudging (Lanzing, 2019, p. 549; Yeung, 2017). The ultimate goal of the audit outlined here is to ensure that an AI system that uses nudges will maintain a level of moral inertia and neutrality by complying with the recommendations, requirements, or suggestions of the audit (in other words, the criteria of the audit). In the case of unintended negative consequences, the audit suggests risk mitigation mechanisms that can be put in place. In the case of unintended positive consequences, it suggests some reinforcement mechanisms. Sponsored by the IBM-Notre Dame Tech Ethics Lab
    
[^7]: 使用ChatGPT生成和增强的训练数据的ZeroShotDataAug

    ZeroShotDataAug: Generating and Augmenting Training Data with ChatGPT. (arXiv:2304.14334v1 [cs.AI])

    [http://arxiv.org/abs/2304.14334](http://arxiv.org/abs/2304.14334)

    本论文研究了使用ChatGPT生成和增强训练数据的方法，证明在适当的特定任务的提示下，其效果优于现有方法，并提出了评估生成数据质量的方法。

    

    本文研究了利用大型生成语言模型ChatGPT生成合成训练数据的数据，以增强低资源情况下的数据。我们展示了，通过适当的面向特定任务的ChatGPT提示，我们优于现有的此类数据增强方法。此外，本文还研究了评估ChatGPT生成的增强数据相似度的方法，以验证和评估生成数据的质量。

    In this paper, we investigate the use of data obtained from prompting a large generative language model, ChatGPT, to generate synthetic training data with the aim of augmenting data in low resource scenarios. We show that with appropriate task-specific ChatGPT prompts, we outperform the most popular existing approaches for such data augmentation. Furthermore, we investigate methodologies for evaluating the similarity of the augmented data generated from ChatGPT with the aim of validating and assessing the quality of the data generated.
    
[^8]: 成语、探测和危险的事物：基于结构探测的词向量成语性研究。

    Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space. (arXiv:2304.14333v1 [cs.CL])

    [http://arxiv.org/abs/2304.14333](http://arxiv.org/abs/2304.14333)

    本文利用结构探测方法研究了成语在静态和上下文嵌入中的编码情况，并发现仍未确定成语性是否在向量范数中编码，提出了改进数据集以进行探测分析的方向。

    

    本文旨在通过一种结构探测方法更深入地了解成语信息如何被嵌入到词嵌入中。我们重新利用了现有的英语动词复合词语（MWE）数据集来适应探测框架，并对静态（GloVe）和上下文（BERT）嵌入进行了比较探测研究。我们的实验表明，这两种方法都以不同程度编码了一些成语信息，但对于成语性是否在向量范数中编码给出了冲突的证据，这仍然是个未解之谜。我们还确定了所使用数据集的一些局限性，并强调了改进其适用性进行探测分析的重要方向。

    The goal of this paper is to learn more about how idiomatic information is structurally encoded in embeddings, using a structural probing method. We repurpose an existing English verbal multi-word expression (MWE) dataset to suit the probing framework and perform a comparative probing study of static (GloVe) and contextual (BERT) embeddings. Our experiments indicate that both encode some idiomatic information to varying degrees, but yield conflicting evidence as to whether idiomaticity is encoded in the vector norm, leaving this an open question. We also identify some limitations of the used dataset and highlight important directions for future work in improving its suitability for a probing analysis.
    
[^9]: 推动可行的多角度推理边界：一种面向STANDPOINT EL + 的演绎演算法

    Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+. (arXiv:2304.14323v1 [cs.AI])

    [http://arxiv.org/abs/2304.14323](http://arxiv.org/abs/2304.14323)

    本文提出一种扩展逻辑——Standpoint EL+，允许公理否定、角色链公理、自环等特征，同时保持可行性。我们通过设计一个可满足性检测演绎演算法来实现它，这个算法需要解决实际应用问题。

    

    Standpoint EL是流行的描述逻辑EL的多模态扩展，允许相对于不同的观点或角度集成领域知识的综合表示。有利的是，其可满足性问题最近已被证明在P时间内，使其成为大规模知识集成的有前途的框架。在本文中，我们展示了我们可以进一步推动这种形式化的表达能力，到达一个扩展逻辑，称为Standpoint EL+，允许公理否定、角色链公理、自环等特征，同时保持可行性。这是通过设计一个可满足性检测演绎演算法实现的，同时解决实用算法的需求。我们通过提供其演绎规则的原型Datalog实现来展示我们演算法的可行性。

    Standpoint EL is a multi-modal extension of the popular description logic EL that allows for the integrated representation of domain knowledge relative to diverse standpoints or perspectives. Advantageously, its satisfiability problem has recently been shown to be in PTime, making it a promising framework for large-scale knowledge integration.  In this paper, we show that we can further push the expressivity of this formalism, arriving at an extended logic, called Standpoint EL+, which allows for axiom negation, role chain axioms, self-loops, and other features, while maintaining tractability. This is achieved by designing a satisfiability-checking deduction calculus, which at the same time addresses the need for practical algorithms. We demonstrate the feasibility of our calculus by presenting a prototypical Datalog implementation of its deduction rules.
    
[^10]: 大型语言模型是代码生成的最先进评估器

    Large Language Models Are State-of-the-Art Evaluators of Code Generation. (arXiv:2304.14317v1 [cs.AI])

    [http://arxiv.org/abs/2304.14317](http://arxiv.org/abs/2304.14317)

    本文提出了一个基于 GPT-3.5 的评估框架，解决了现有方法在代码生成任务上的局限性，取得了更好的相关性。

    

    自然语言生成领域的最新进展推动了利用大型语言模型评估生成文本的能力。虽然这些模型在机器翻译和摘要等任务中表现出了很好的结果，但其在代码生成任务中的适用性仍然存在限制。这些任务所需的编程概念的复杂性使得开发评估指标以与人类判断相一致变得困难。以词汇匹配为基础的度量标准（如BLEU）在代码生成任务中与人工从业者的相关性较弱。此外，在低资源领域中利用人为编写的测试套件进行功能正确性评估也具有挑战性。为了克服这些障碍，我们提出了一个基于GPT-3.5的代码生成评估框架（\texttt{GPT-3.5-turbo}）。我们的框架通过取得更好的相关性来解决现有方法的局限性。

    Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code generation tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code generation tasks. Moreover, the utilization of human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose a new evaluation framework based on the GPT-3.5 (\texttt{GPT-3.5-turbo}), for code generation assessments. Our framework addresses the limitations of existing approaches by achieving superior cor
    
[^11]: 大规模仓库中的多机器人货架重排问题：双层多机器人提货和配送

    Double-Deck Multi-Agent Pickup and Delivery: Multi-Robot Rearrangement in Large-Scale Warehouses. (arXiv:2304.14309v1 [cs.RO])

    [http://arxiv.org/abs/2304.14309](http://arxiv.org/abs/2304.14309)

    该论文提出了一个名为双层多机器人提货和配送（DD-MAPD）的问题形式，涉及机器人的货架移动，以优化仓库布局。作者提出了MAPF-DECOMP的算法框架，有效地解决了此类问题。

    

    我们提出了一个新的问题形式，双层多机器人提货和配送（DD-MAPD），它模拟了自动化仓库中的多机器人货架重排问题。DD-MAPD通过允许机器人在货架下移动或举起并将货架送往任意位置改变仓库布局，扩展了Multi-Agent Pickup and Delivery（MAPD）和Multi-Agent Path Finding（MAPF）。我们表明求解DD-MAPD是NP难的。为了解决DD-MAPD，我们提出了MAPF-DECOMP，这是一个算法框架，将DD-MAPD实例分解成用于协调货架轨迹的MAPF实例和用于计算代理路径的具有任务依赖性的后续MAPD实例。我们还提出了一种优化技术来改进MAPF-DECOMP的性能，并展示了如何使MAPF-DECOMP针对非常规DD-MAPD实例（一个现实的DD-MAPD实例子集）完整。我们的实验结果证明了MAPF-DECOMP的效率和有效性，其中有...

    We introduce a new problem formulation, Double-Deck Multi-Agent Pickup and Delivery (DD-MAPD), which models the multi-robot shelf rearrangement problem in automated warehouses. DD-MAPD extends both Multi-Agent Pickup and Delivery (MAPD) and Multi-Agent Path Finding (MAPF) by allowing agents to move beneath shelves or lift and deliver a shelf to an arbitrary location, thereby changing the warehouse layout. We show that solving DD-MAPD is NP-hard. To tackle DD-MAPD, we propose MAPF-DECOMP, an algorithmic framework that decomposes a DD-MAPD instance into a MAPF instance for coordinating shelf trajectories and a subsequent MAPD instance with task dependencies for computing paths for agents. We also present an optimization technique to improve the performance of MAPF-DECOMP and demonstrate how to make MAPF-DECOMP complete for well-formed DD-MAPD instances, a realistic subclass of DD-MAPD instances. Our experimental results demonstrate the efficiency and effectiveness of MAPF-DECOMP, with th
    
[^12]: 黑暗环境下的实例分割

    Instance Segmentation in the Dark. (arXiv:2304.14298v1 [cs.CV])

    [http://arxiv.org/abs/2304.14298](http://arxiv.org/abs/2304.14298)

    本文提出了在黑暗环境下的实例分割方法。为了抑制低光图像中的特征噪声，该方法采用了自适应加权下采样层、平滑定向卷积块和扰动抑制学习等技术，同时发现高位深的RAW图像可以更好地保留更丰富的场景信息。

    

    现有的实例分割技术主要适用于高可见度的输入，但在极低光环境下，它们的性能显著下降。本文深入研究了在黑暗中进行实例分割，并介绍了几种可以显著提高低光推断准确性的技术。所提出的方法是基于这样的观察：低光图像中的噪声会引入高频扰动到神经网络的特征图中，从而显著降低性能。为了抑制这种“特征噪声”，我们提出了一种新的学习方法，它依赖于自适应加权下采样层、平滑定向卷积块和扰动抑制学习。这些组件有效地减少了下采样和卷积操作中的特征噪声，使模型能够学习抗扰动的特征。此外，我们发现高位深的RAW图像可以更好地保留更丰富的场景信息，

    Existing instance segmentation techniques are primarily tailored for high-visibility inputs, but their performance significantly deteriorates in extremely low-light environments. In this work, we take a deep look at instance segmentation in the dark and introduce several techniques that substantially boost the low-light inference accuracy. The proposed method is motivated by the observation that noise in low-light images introduces high-frequency disturbances to the feature maps of neural networks, thereby significantly degrading performance. To suppress this ``feature noise", we propose a novel learning method that relies on an adaptive weighted downsampling layer, a smooth-oriented convolutional block, and disturbance suppression learning. These components effectively reduce feature noise during downsampling and convolution operations, enabling the model to learn disturbance-invariant features. Furthermore, we discover that high-bit-depth RAW images can better preserve richer scene i
    
[^13]: 用自然语言指令控制文本生成

    Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])

    [http://arxiv.org/abs/2304.14293](http://arxiv.org/abs/2304.14293)

    InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。

    

    大型语言模型可以产生流畅的文本，并能根据自然语言指令解决各种任务，无需特定的训练。然而，控制它们的生成以满足不同应用程序所需的各种约束条件是非常困难的。本文提供了一个带约束调节的文本生成框架——InstructCTG，该框架通过基于自然语言描述和约束演示来纳入不同的约束条件。我们首先通过一系列现成的NLP工具和简单的启发式方法来提取自然文本的潜在约束条件。此外，我们将这些约束条件转化为自然语言指令，以形成弱监督的训练数据。通过添加自然语言约束描述和少量演示，我们对预训练语言模型进行了微调，以纳入各种类型的约束条件。与现有基于搜索或得分的方法相比，InstructCTG 更加有效。

    Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
    
[^14]: 用基于Transformer的方法区分行星凌和误判的方法

    Distinguishing a planetary transit from false positives: a Transformer-based classification for planetary transit signals. (arXiv:2304.14283v1 [astro-ph.EP])

    [http://arxiv.org/abs/2304.14283](http://arxiv.org/abs/2304.14283)

    本文介绍了基于Transformer架构的新方法，用于高效准确地区分行星凌和误判。采用一种新的预处理输入数据的方法，使用正弦函数来保留信号的周期性。在模拟数据的评估中，该方法优于已有的基于CNN的方法。

    

    目前，像TESS这样的空间任务提供了大量必须高效、系统地分析的光变曲线数据库。近年来，深度学习（DL）方法，尤其是卷积神经网络（CNN），已被用于自动分类候选外行星的凌变信号。然而，CNN具有一些缺陷，例如，它们需要许多层来捕获序列数据（例如光变曲线）上的依赖关系，使得网络变得过于庞大，最终变得不实用。自注意机制是一种DL技术，试图模仿有选择地聚焦于一些相关事物而忽略其他事物的行为。最近针对序列数据的模型，例如Transformer架构，取得了成功的结果。我们基于这些成功的模型提出了一种新的用于自动分类凌变信号的架构。我们提出的架构旨在高效准确地捕捉凌变信号并将其与误判区分开来。我们采用了一种新的预处理输入数据的方法，使用正弦函数来保留信号的周期性。在模拟数据的评估中，我们的方法在准确性和效率方面优于基于CNN的现有方法。

    Current space-based missions, such as the Transiting Exoplanet Survey Satellite (TESS), provide a large database of light curves that must be analysed efficiently and systematically. In recent years, deep learning (DL) methods, particularly convolutional neural networks (CNN), have been used to classify transit signals of candidate exoplanets automatically. However, CNNs have some drawbacks; for example, they require many layers to capture dependencies on sequential data, such as light curves, making the network so large that it eventually becomes impractical. The self-attention mechanism is a DL technique that attempts to mimic the action of selectively focusing on some relevant things while ignoring others. Models, such as the Transformer architecture, were recently proposed for sequential data with successful results. Based on these successful models, we present a new architecture for the automatic classification of transit signals. Our proposed architecture is designed to capture t
    
[^15]: 自主驾驶能源高效近似边缘人工智能概述调查

    A Survey on Approximate Edge AI for Energy Efficient Autonomous Driving Services. (arXiv:2304.14271v1 [cs.RO])

    [http://arxiv.org/abs/2304.14271](http://arxiv.org/abs/2304.14271)

    这篇论文概述了自主驾驶服务中的传感和数据处理现状，及其对能源和环境带来的挑战，进一步调查和比较了如何应用和整合现有技术创新以实现能源效率。

    

    自主驾驶服务极大地依赖于传感器，如摄像机、激光雷达、雷达和通信模块。处理传感数据的通常做法是在车辆内部放置高性能计算单元，部署人工智能模型和算法，作为车辆的大脑或管理员。从平均行驶时间生成的车辆数据可以高达20TB，具体取决于传感器的数据速率和规格。鉴于自主驾驶服务的规模和快速增长，尤其是在向车辆电气化（例如，电池驱动）的趋势中，提高总体能源和环境效率至关重要。 虽然这些领域的传感器技术、无线通信、计算和AI / ML算法已经取得了显着进展，但如何应用和整合这些技术创新以实现能源效率仍然是一个挑战。本调查回顾并比较了连接的车辆

    Autonomous driving services rely heavily on sensors such as cameras, LiDAR, radar, and communication modules. A common practice of processing the sensed data is using a high-performance computing unit placed inside the vehicle, which deploys AI models and algorithms to act as the brain or administrator of the vehicle. The vehicular data generated from average hours of driving can be up to 20 Terabytes depending on the data rate and specification of the sensors. Given the scale and fast growth of services for autonomous driving, it is essential to improve the overall energy and environmental efficiency, especially in the trend towards vehicular electrification (e.g., battery-powered). Although the areas have seen significant advancements in sensor technologies, wireless communications, computing and AI/ML algorithms, the challenge still exists in how to apply and integrate those technology innovations to achieve energy efficiency. This survey reviews and compares the connected vehicular
    
[^16]: 简化变分贝叶斯方法的推导过程

    Variational Bayes Made Easy. (arXiv:2304.14251v1 [cs.LG])

    [http://arxiv.org/abs/2304.14251](http://arxiv.org/abs/2304.14251)

    该论文提出了一个三步骤方法，简化了变分贝叶斯近似推断方法的推导过程。

    

    变分贝叶斯方法是一种流行的近似推断方法，但其推导过程可能很繁琐。为了简化这个过程，我们给出了一个三步骤的方法，通过显式寻找关于已知分布期望的线性性，来确定后验分布形式。然后我们可以直接通过“读取”这些期望前的项，写出更新。这个方法使得推导更加简单，快速，简短和通用。

    Variational Bayes is a popular method for approximate inference but its derivation can be cumbersome. To simplify the process, we give a 3-step recipe to identify the posterior form by explicitly looking for linearity with respect to expectations of well-known distributions. We can then directly write the update by simply ``reading-off'' the terms in front of those expectations. The recipe makes the derivation easier, faster, shorter, and more general.
    
[^17]: 立场线性时态逻辑

    Standpoint Linear Temporal Logic. (arXiv:2304.14243v1 [cs.AI])

    [http://arxiv.org/abs/2304.14243](http://arxiv.org/abs/2304.14243)

    本文提出了一种新的逻辑——立场线性时态逻辑（SLTL），它将 LTL 的时间特性与 SL 的多视角建模能力相结合，为扩展现有的 LTL 推理器提供了一个清晰的路径。

    

    许多复杂场景需要协调拥有独特观点和不同语义承诺的代理。为了解决这个问题，我们引入了立场逻辑（SL）来进行知识整合，通过索引模态可以推理出不同且可能冲突的观点。另一个重要的多模态逻辑是线性时态逻辑（LTL），它是表达系统和过程时间属性的形式化工具，在形式化方法和人工智能相关领域具有重要地位。本文提出了一种新逻辑——立场线性时态逻辑（SLTL），它将 LTL 的时间特性与 SL 的多视角建模能力相结合。我们定义了 SLTL 的语法和语义，确定了其可决定性和复杂性，并提供了一个终止的表演演算法来自动化 SLTL 推理。这为扩展现有的 LTL 推理器提供了一个清晰的路径，并带来了实用推理支持。

    Many complex scenarios require the coordination of agents possessing unique points of view and distinct semantic commitments. In response, standpoint logic (SL) was introduced in the context of knowledge integration, allowing one to reason with diverse and potentially conflicting viewpoints by means of indexed modalities. Another multi-modal logic of import is linear temporal logic (LTL) - a formalism used to express temporal properties of systems and processes, having prominence in formal methods and fields related to artificial intelligence. In this paper, we present standpoint linear temporal logic (SLTL), a new logic that combines the temporal features of LTL with the multi-perspective modelling capacity of SL. We define the logic SLTL, its syntax, and its semantics, establish its decidability and complexity, and provide a terminating tableau calculus to automate SLTL reasoning. Conveniently, this offers a clear path to extend existing LTL reasoners with practical reasoning support
    
[^18]: TorchBench: 用高API表面覆盖率评估PyTorch性能的基准套件

    TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])

    [http://arxiv.org/abs/2304.14226](http://arxiv.org/abs/2304.14226)

    TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。

    

    深度学习是多个领域中的革命性技术。为了方便模型的开发和部署，提出了许多深度学习框架，其中PyTorch是最流行的解决方案之一。PyTorch软件栈的生态性能至关重要，可节省模型训练成本并减少模型推理的响应时间。本文提出了TorchBench，一款新型基准测试套件，用于研究PyTorch软件栈的性能。与现有基准测试套件不同，TorchBench包含了许多代表性模型，覆盖了大量PyTorch API表面。TorchBench能够全面地表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。我们展示了TorchBench的两个实际用例。第一，我们对TorchBench进行性能剖析，以识别PyTorch的GPU性能效率问题。我们能够优化许多性能故障并向上游提交贡献。

    Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
    
[^19]: LLT：线性定律特征空间变换的R包

    LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])

    [http://arxiv.org/abs/2304.14211](http://arxiv.org/abs/2304.14211)

    LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。

    

    线性定律特征空间转换(LLT )算法的目标是帮助对单变量和多变量时间序列进行分类。LLT R包以灵活和用户友好的方式实现了该算法。该包将实例分为训练和测试集，并利用时延嵌入和谱分解技术，识别训练集中每个输入序列(初始特征)的控制模式(称为线性定律)。最后，它应用训练集的线性定律来转换测试集的初始特征。trainTest、trainLaw和testTrans三个单独的函数来执行这些步骤，它们需要预定义的数据结构;然而，为了快速计算，它们只使用内置函数。LLT R包和适当数据结构的示例数据集在GitHub上公开可用。

    The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
    
[^20]: 基于知识增强的多模态预训练方法实现医疗人工通用智能

    Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining. (arXiv:2304.14204v1 [cs.AI])

    [http://arxiv.org/abs/2304.14204](http://arxiv.org/abs/2304.14204)

    本文提出了一种基于医疗知识的多模态预训练方法，名为MOTOR，旨在实现医疗人工通用智能，将通用和特定知识融合，共同促进模型的预训练过程。

    

    医疗人工通用智能（MAGI）可以利用共享的医疗知识来解决不同的医疗任务，减少大量特定任务的数据需求。然而，由于医疗数据有限且复杂，设计具有强泛化能力的模型十分具有挑战性，因此大多数现有方法都是针对特定任务的模型设计。本文提出了一种名为Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR)的新范例，以实现MAGI。在MOTOR中，我们将两种基本医疗知识——通用和特定知识融合，共同促进模型的预训练过程，使其从放射学数据中学习紧凑的表征，以实现更好的交叉模态对齐。MOTOR统一了理解和生成，是实现医疗人工通用智能的一步。

    Medical artificial general intelligence (MAGI) enables one foundation model to solve different medical tasks, which is very practical in the medical domain. It can significantly reduce the requirement of large amounts of task-specific data by sufficiently sharing medical knowledge among different tasks. However, due to the challenges of designing strongly generalizable models with limited and complex medical data, most existing approaches tend to develop task-specific models. To take a step towards MAGI, we propose a new paradigm called Medical-knOwledge-enhanced mulTimOdal pretRaining (MOTOR). In MOTOR, we combine two kinds of basic medical knowledge, i.e., general and specific knowledge, in a complementary manner to boost the general pretraining process. As a result, the foundation model with comprehensive basic knowledge can learn compact representations from pretraining radiographic data for better cross-modal alignment. MOTOR unifies the understanding and generation, which are two
    
[^21]: ChatGPT与现有模型之间的关键短语生成任务基准研究

    ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v1 [cs.CL])

    [http://arxiv.org/abs/2304.14177](http://arxiv.org/abs/2304.14177)

    本研究比较了ChatGPT和现有模型在关键短语生成任务上的性能，并发现ChatGPT在所有测试数据集和环境中的表现均优于现有模型，适用于不同领域和文档长度的关键短语生成。

    

    基于Transformer的语言模型，包括ChatGPT，已经在各种自然语言生成任务中展现了出色的性能。但是，在评估ChatGPT的关键短语生成能力方面，还没有多少研究，这涉及到准确反映文档内容的信息性短语的识别。本文试图通过将ChatGPT的关键短语生成表现与现有模型进行比较来解决这个问题，同时还测试了它作为解决领域适应和长文档关键短语生成两个重大挑战的潜力。我们在来自科学文章和新闻领域的六个公开数据集上进行了实验，分析了在短文档和长文档上的表现。结果表明，在所有测试的数据集和环境中，ChatGPT的性能优于当前现有模型，产生适应不同领域和文档长度的高质量关键短语。

    Transformer-based language models, including ChatGPT, have demonstrated exceptional performance in various natural language generation tasks. However, there has been limited research evaluating ChatGPT's keyphrase generation ability, which involves identifying informative phrases that accurately reflect a document's content. This study seeks to address this gap by comparing ChatGPT's keyphrase generation performance with state-of-the-art models, while also testing its potential as a solution for two significant challenges in the field: domain adaptation and keyphrase generation from long documents. We conducted experiments on six publicly available datasets from scientific articles and news domains, analyzing performance on both short and long documents. Our results show that ChatGPT outperforms current state-of-the-art models in all tested datasets and environments, generating high-quality keyphrases that adapt well to diverse domains and document lengths.
    
[^22]: 使用毫米波雷达稀疏点云进行人类语义分割

    Human Semantic Segmentation using Millimeter-Wave Radar Sparse Point Clouds. (arXiv:2304.14132v1 [cs.CV])

    [http://arxiv.org/abs/2304.14132](http://arxiv.org/abs/2304.14132)

    本文提出了一种在毫米波雷达稀疏点云上进行人类语义分割的框架，该框架优于相机和激光雷达的隐私保护和抗干扰能力。同时，本文引入图结构和拓扑特征，并设计了一个含有全局特征模块和顺序特征模块的语义分割框架，成功解决了稀疏和时间拓扑特征的问题。

    

    本文提出了一种在毫米波雷达稀疏时序点云上进行语义分割的框架。相比相机和激光雷达，毫米波雷达具有不泄露隐私、强抗干扰能力和长检测距离的优势。然而，毫米波数据的稀疏和时间拓扑特征的捕获仍然是一个问题，特别是在人类语义分割任务中，以前的先进分割方法 (如PointNet、PointCNN、Point Transformer) 没有被充分应用于实际场景中。

    This paper presents a framework for semantic segmentation on sparse sequential point clouds of millimeter-wave radar. Compared with cameras and lidars, millimeter-wave radars have the advantage of not revealing privacy, having a strong anti-interference ability, and having long detection distance. The sparsity and capturing temporal-topological features of mmWave data is still a problem. However, the issue of capturing the temporal-topological coupling features under the human semantic segmentation task prevents previous advanced segmentation methods (e.g PointNet, PointCNN, Point Transformer) from being well utilized in practical scenarios. To address the challenge caused by the sparsity and temporal-topological feature of the data, we (i) introduce graph structure and topological features to the point cloud, (ii) propose a semantic segmentation framework including a global feature-extracting module and a sequential feature-extracting module. In addition, we design an efficient and mo
    
[^23]: 为什么不并存？不确定性解释及自信心在人工智能协作中的作用

    Why not both? Complementing explanations with uncertainty, and the role of self-confidence in Human-AI collaboration. (arXiv:2304.14130v1 [cs.AI])

    [http://arxiv.org/abs/2304.14130](http://arxiv.org/abs/2304.14130)

    本文研究了不确定性估计和模型解释如何影响用户对模型的依赖性、理解力和信任度以及自信心对他们的任务执行能力的影响，并提出了在人工智能协作中将两者结合起来的优势。

    

    人工智能和机器学习模型已经在医疗保健和刑事司法等关键领域找到了许多应用。然而，完全自动化这些高风险应用可能引发道德或公平性的关注。因此，在这些情况下，应该由自动化系统来协助人类，使得双方通过交流达成共同决策。本文通过实证研究来确定不确定性估计和模型解释如何影响用户对模型的依赖性、理解力和信任度，并寻求将二者结合起来带来的潜在优势。此外，我们试图评估用户自信心对他们的任务执行能力的影响，同时讨论后者如何扭曲基于协议和切换百分比的分析结果。

    AI and ML models have already found many applications in critical domains, such as healthcare and criminal justice. However, fully automating such high-stakes applications can raise ethical or fairness concerns. Instead, in such cases, humans should be assisted by automated systems so that the two parties reach a joint decision, stemming out of their interaction. In this work we conduct an empirical study to identify how uncertainty estimates and model explanations affect users' reliance, understanding, and trust towards a model, looking for potential benefits of bringing the two together. Moreover, we seek to assess how users' behaviour is affected by their own self-confidence in their abilities to perform a certain task, while we also discuss how the latter may distort the outcome of an analysis based on agreement and switching percentages.
    
[^24]: 多目标多智能体决策中基于演示的偏好推断

    Preference Inference from Demonstration in Multi-objective Multi-agent Decision Making. (arXiv:2304.14126v1 [cs.AI])

    [http://arxiv.org/abs/2304.14126](http://arxiv.org/abs/2304.14126)

    该论文提出了一种基于演示的偏好推断算法，用于多目标决策问题中量化偏好权重，实证结果表明，算法相较于基线算法有着更高的准确性和效率。在未来的工作中，该算法可用于多智能体系统中推断对手的偏好。

    

    在多目标决策问题中量化不同目标的数值偏好是具有挑战性的。然而，用户的演示通常是可访问的。我们提出了一种算法，从最优或近最优的演示中推断线性偏好权重。我们在三种环境中评估了该算法，并与两种基准方法进行比较。实证结果表明，在所推断的偏好的时间要求和准确性方面，相对于基线算法，算法有着显著的改进。在未来的工作中，我们计划在多智能体系统中评估算法的有效性，其中的一个智能体能够使用我们提出的偏好推断算法推断对手的偏好。

    It is challenging to quantify numerical preferences for different objectives in a multi-objective decision-making problem. However, the demonstrations of a user are often accessible. We propose an algorithm to infer linear preference weights from either optimal or near-optimal demonstrations. The algorithm is evaluated in three environments with two baseline methods. Empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both time requirements and accuracy of the inferred preferences. In future work, we plan to evaluate the algorithm's effectiveness in a multi-agent system, where one of the agents is enabled to infer the preferences of an opponent using our preference inference algorithm.
    
[^25]: 多目标强化学习中基于动态权重的演示偏好推断方法

    Inferring Preferences from Demonstrations in Multi-objective Reinforcement Learning: A Dynamic Weight-based Approach. (arXiv:2304.14115v1 [cs.AI])

    [http://arxiv.org/abs/2304.14115](http://arxiv.org/abs/2304.14115)

    本研究提出了一种基于动态权重的偏好推理算法(DWPI)，通过观察环境中的行为轨迹，可以推断执行多目标决策问题的代理的偏好。实验结果表明，DWPI在多个多目标马尔科夫决策过程中的表现优于两种现有偏好推理方法。

    

    许多决策问题都涉及多个目标，而在这些问题中，不一定总是能知道决策者对不同目标的偏好。然而，观察决策者的行为往往是可行的。在多目标决策中，偏好推理是推断决策者对不同目标的偏好的过程。本研究提出了一种基于动态权重的偏好推理算法(DWPI)，通过观察环境中的行为轨迹，可以推断执行多目标决策问题的代理的偏好。提出的方法在三个多目标马尔可夫决策过程(Deep Sea Treasure、Traffic和Item Gathering)上进行了评估。实证结果表明，与文献中的两种现有偏好推理方法相比，所提出的DWPI方法在基线算法方面均有显著改进。

    Many decision-making problems feature multiple objectives. In such problems, it is not always possible to know the preferences of a decision-maker for different objectives. However, it is often possible to observe the behavior of decision-makers. In multi-objective decision-making, preference inference is the process of inferring the preferences of a decision-maker for different objectives. This research proposes a Dynamic Weight-based Preference Inference (DWPI) algorithm that can infer the preferences of agents acting in multi-objective decision-making problems, based on observed behavior trajectories in the environment. The proposed method is evaluated on three multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item Gathering. The performance of the proposed DWPI approach is compared to two existing preference inference methods from the literature, and empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both
    
[^26]: ChatLog: 记录和分析ChatGPT随时间的变化

    ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])

    [http://arxiv.org/abs/2304.14106](http://arxiv.org/abs/2304.14106)

    ChatLog是一个分析ChatGPT随时间变化的数据集，通过提取ChatGPT知识和语言特征发现了一些稳定的特征，提高了RoBERTa-based detector在新版本ChatGPT上的鲁棒性。

    

    尽管有大量关于在自然语言理解和生成任务中评估ChatGPT的研究，但鲜有研究调查ChatGPT的行为如何随时间变化。在本文中，我们收集了一个粗到细的时间数据集，称为ChatLog，由两个部分组成，每月和每天更新：ChatLog-Monthly是一个数据集，包括每个月收集的38,730个问题-回答对，其中包括推理和分类任务的问题。另一方面，ChatLog-Daily包括ChatGPT每天对1000个相同问题的长篇回答。我们进行全面的自动和人工评估，以提供ChatGPT进化模式存在的证据。我们进一步通过提取其知识和语言特征分析了ChatGPT随时间不变的特征。我们发现一些稳定的特征，以提高基于RoBERTa的检测器在新版本的ChatGPT上的鲁棒性。我们将继续维护我们的数据集以及随时间的分析。

    While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our p
    
[^27]: SocNavGym：一个针对社交导航的强化学习仿真环境

    SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v1 [cs.RO])

    [http://arxiv.org/abs/2304.14102](http://arxiv.org/abs/2304.14102)

    本文提出了SocNavGym，对于社交导航领域的研究提供了一个轻便、快速、易用的仿真环境，可生成各种各样的社交导航场景，并促进了智能社交机器人的发展。

    

    在人口密集的环境下，自主机器人在导航时需要遵守社交规范。机器学习，尤其是深度强化学习，最近在社交导航领域中取得了显著进展。这可以部分归因于生成的策略不受代码复杂性或处理的变量数量等人类限制。不幸的是，DRL算法缺乏安全保障，需要大量数据需求，导致在现实环境中的应用不太切实际。为了缩小这一差距，仿真环境被广泛使用。本文提出了SocNavGym，一个专门针对社交导航的先进仿真环境，可以生成各种各样的社交导航场景，并促进智能社交机器人的发展。SocNavGym轻便、快速、易于使用，并可轻松配置以生成不同类型的社交导航场景。此外，它还可以配置为使用不同的传感器，并支持无障碍环境的仿真。

    It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to
    
[^28]: 可解释人工智能的范畴基础：一种统一的结构和语义形式体系。

    Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])

    [http://arxiv.org/abs/2304.14094](http://arxiv.org/abs/2304.14094)

    本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。

    

    可解释人工智能（XAI）旨在回答与AI模型部署相关的伦理和法律问题。然而，相当数量的领域特定评论强调需要一个数学基础来定义领域中的关键概念，即使“解释”这个术语还缺乏精确定义。这些评论还主张建立一个健全而统一的可解释AI形式体系，以避免出现不良提出问题，帮助研究人员浏览一个快速增长的知识体系。据作者所知，该论文是填补该空白的首次尝试，通过形式化一个可解释AI的统一理论。采用范畴理论的框架，特别是反馈单调范畴，我们首先提供了可解释AI中所有重要术语的形式定义。然后，我们提出了一个遵循提出结构的领域分类法，展示了如何使用引入的理论来对当前研究的所有主要XAI系统类进行分类。

    Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
    
[^29]: 聚类流（Cluster Flow）：如何通过层次聚类使深度神经网络更具鲁棒性、更符合人类思考方式、更易于实现关系推理

    Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning. (arXiv:2304.14081v1 [cs.LG])

    [http://arxiv.org/abs/2304.14081](http://arxiv.org/abs/2304.14081)

    该论文介绍了一种半监督层次聚类框架ClusterFlow，可以在经过训练的深度卷积神经网络中使用密集的多维类别和特征数据来构建超空间地图，从而使其更具人类思考方式的功能，提高了其鲁棒性和实现关系推理的能力。

    

    尽管神经网络（NNs）在人工智能领域（特别是深度卷积网络）取得了巨大的突破，但它们并没有达到人类水平的表现：它们可以被智能攻击，而人们无法被欺骗，也缺乏常识。已经有人认为，人类智能的基础是人类能够进行关系推理的能力：比较不同对象，测量相似性，掌握对象之间的关系。聚类流是一个半监督的层次聚类框架，可以通过利用预 SoftMax 层中发现的丰富的多维类别和特征数据，在经过训练的 NNs 上进行操作，构建类/特征的超空间地图，并将这些功能添加到现代深度卷积神经网络中，从而增加了更具人类思考方式的功能。作者通过3个任务来证明这一点。

    Despite the huge recent breakthroughs in neural networks (NNs) for artificial intelligence (specifically deep convolutional networks) such NNs do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. It has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. Mankind can even do this with objects they have never seen before. Here we show how ClusterFlow, a semi-supervised hierarchical clustering framework can operate on trained NNs utilising the rich multi-dimensional class and feature data found at the pre-SoftMax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. We demonstrate this with 3 tasks. 1. the statistical l
    
[^30]: 组合式3D人物-物体神经动画

    Compositional 3D Human-Object Neural Animation. (arXiv:2304.14070v1 [cs.CV])

    [http://arxiv.org/abs/2304.14070](http://arxiv.org/abs/2304.14070)

    提出了一种组合式的方法来解决人物-物体交互动画的挑战，以实现新的HOI的组合式动画控制。

    

    人-物交互对于人类中心的场景理解应用，如人类中心的视觉生成、AR/VR和机器人技术至关重要。本文提出了一种组合式的视角来解决人物-物体交互动画的挑战，即通过新的姿势序列，动画化新的人物和/或新的物体的新的交互行为。具体而言，我们采用神经人物-物体变形来建模和渲染基于隐式神经表示的HOI动态。为了使人物和物体之间的交互姿势在不同的人物和物体之间转移，我们设计了一种新的组合式条件神经辐射场(CC-NeRF)，该方法使用潜在码将人物和物体之间的相互依赖关系解耦，以实现新的HOI的组合式动画控制。实验证明，所提出的方法可以很好地推广到各种新的HOI动画设置中。

    Human-object interactions (HOIs) are crucial for human-centric scene understanding applications such as human-centric visual generation, AR/VR, and robotics. Since existing methods mainly explore capturing HOIs, rendering HOI remains less investigated. In this paper, we address this challenge in HOI animation from a compositional perspective, i.e., animating novel HOIs including novel interaction, novel human and/or novel object driven by a novel pose sequence. Specifically, we adopt neural human-object deformation to model and render HOI dynamics based on implicit neural representations. To enable the interaction pose transferring among different persons and objects, we then devise a new compositional conditional neural radiance field (or CC-NeRF), which decomposes the interdependence between human and object using latent codes to enable compositionally animation control of novel HOIs. Experiments show that the proposed method can generalize well to various novel HOI animation setting
    
[^31]: 可解释的神经符号概念推理

    Interpretable Neural-Symbolic Concept Reasoning. (arXiv:2304.14068v1 [cs.AI])

    [http://arxiv.org/abs/2304.14068](http://arxiv.org/abs/2304.14068)

    本文提出了第一个基于概念嵌入的可解释概念模型DCR，能够在多个数据集上实现接近最先进的准确性，相对于最先进的可解释概念模型提高了高达+25％，并产生能够解释其预测的人类可理解规则和真值度，适应性强。

    

    深度学习方法具有高度的准确性，但它们不透明的决策过程阻止了它们获得完全的人类信任。概念模型旨在通过学习一组人类可理解的概念来解决这个问题。然而，最先进的概念模型依赖于高维概念嵌入表示，缺乏明确的语义含义，因此质疑其决策过程的可解释性。为了克服这个限制，我们提出了Deep Concept Reasoner(DCR)，这是第一个基于概念嵌入的可解释概念模型。在DCR中，神经网络不直接进行任务预测，而是使用概念嵌入建立语法规则结构。然后DCR在有意义的概念真值度上执行这些规则，以不可微分的方式提供最终的可解释和语义一致的预测。我们的实验表明，DCR：(i)在多个数据集上实现接近最先进的准确性，同时相对于最先进的可解释概念模型提高了高达+25％;(ii)产生能够解释其预测的人类可理解规则和真值度;(iii)很容易适应新领域。

    Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based
    
[^32]: 面向遥感时序数据的轻量级预训练Transformer

    Lightweight, Pre-trained Transformers for Remote Sensing Timeseries. (arXiv:2304.14065v1 [cs.CV])

    [http://arxiv.org/abs/2304.14065](http://arxiv.org/abs/2304.14065)

    设计针对远程传感器数据的自监督学习模型和训练技术，可以得到表现更好且更小的模型。预训练的遥感时间序列Transformer（Presto）在几个遥感基准测试中实现了最先进的结果。

    

    远程传感数据的机器学习算法在社会相关应用方面具有广泛的应用，但用于训练这些算法的标签可能很难或不可能获得。这个挑战已经推动了自监督学习领域的研究，旨在通过遥感数据解锁在标记数据集较小的地理位置或应用领域中使用机器学习。我们展示了为遥感数据设计模型和自监督训练技术可以得到更小、更优秀的模型。我们介绍了Remote Sensing Transformer（Presto），它是一种基于Transformer的模型，使用新颖的自监督目标对遥感时间序列数据进行预训练。我们的实验表明，与在自然图像上训练的可比模型相比，Presto在几个遥感基准测试中实现了最先进的结果，同时需要数量级更少的参数。

    Machine learning algorithms for parsing remote sensing data have a wide range of societally relevant applications, but labels used to train these algorithms can be difficult or impossible to acquire. This challenge has spurred research into self-supervised learning for remote sensing data aiming to unlock the use of machine learning in geographies or application domains where labelled datasets are small. Current self-supervised learning approaches for remote sensing data draw significant inspiration from techniques applied to natural images. However, remote sensing data has important differences from natural images -- for example, the temporal dimension is critical for many tasks and data is collected from many complementary sensors. We show that designing models and self-supervised training techniques specifically for remote sensing data results in both smaller and more performant models. We introduce the Pretrained Remote Sensing Transformer (Presto), a transformer-based model pre-tr
    
[^33]: PAC学习的参数化理论

    A Parameterized Theory of PAC Learning. (arXiv:2304.14058v1 [cs.CC])

    [http://arxiv.org/abs/2304.14058](http://arxiv.org/abs/2304.14058)

    本文提出了一种参数化PAC学习理论，确定了两种固定参数可学习性的概念，从而能够以细粒度的方式分析高效和难以处理的PAC学习之间的界限。

    

    可能准确(即PAC)学习是样本复杂度理论的核心概念，高效的PAC可学习性经常被视为经典计算复杂度P类的自然对应物。然而，尽管参数化复杂性的崛起使我们能够超越经典计算复杂性的P-NP“二分法”，并确定许多问题的可处理边界，但在样本复杂性领域中没有类似的模型能够超越高效的PAC可学习性。我们的核心贡献是开发了一种参数化PAC学习理论，它使我们能够对几个最近包含参数化复杂性元素的PAC学习结果进行新的解释。在该理论中，我们确定了不止一种固定参数可学习性的概念，这两种概念都是参数化复杂性范式中核心概念FPT的独立对应物，并且允许我们以细粒度的方式分析高效和难以处理的PAC学习之间的界限。

    Probably Approximately Correct (i.e., PAC) learning is a core concept of sample complexity theory, and efficient PAC learnability is often seen as a natural counterpart to the class P in classical computational complexity. But while the nascent theory of parameterized complexity has allowed us to push beyond the P-NP ``dichotomy'' in classical computational complexity and identify the exact boundaries of tractability for numerous problems, there is no analogue in the domain of sample complexity that could push beyond efficient PAC learnability.  As our core contribution, we fill this gap by developing a theory of parameterized PAC learning which allows us to shed new light on several recent PAC learning results that incorporated elements of parameterized complexity. Within the theory, we identify not one but two notions of fixed-parameter learnability that both form distinct counterparts to the class FPT -- the core concept at the center of the parameterized complexity paradigm -- and 
    
[^34]: 交错图和注意力网络用于3D人体姿态估计

    Interweaved Graph and Attention Network for 3D Human Pose Estimation. (arXiv:2304.14045v1 [cs.CV])

    [http://arxiv.org/abs/2304.14045](http://arxiv.org/abs/2304.14045)

    该论文提出了一种名为交错图和注意力网络（IGANet）的模型，通过图卷积网络（GCNs）和注意力之间的双向通信，解决了先前单视角图像3D人体姿态估计中忽略全局和局部关联的问题。在人体姿态估计上取得了最先进的结果。

    

    尽管在单视角图像的3D人体姿态估计方面取得了重大进展，但以往的工作很少探索全局和局部相互关系，导致对人体骨骼表示的学习不足。为了解决这个问题，我们提出了一种新的交错图和注意力网络（IGANet），它允许图卷积网络（GCN）和注意力之间的双向通信。具体来说，我们引入了IGA模块，其中GCNs向注意力提供局部信息，注意力向GCNs注入全局信息。此外，我们设计了一个简单而有效的U形多层感知器（uMLP），可以捕捉身体关节的多粒度信息。我们在两个流行的基准数据集上进行了广泛的试验（即Human3. 6M和MPI-INF-3DHP），以评估我们提出的方法。结果表明，IGANet在两个数据集上均取得了最先进的性能。代码可在https://github.com/xiu-cs/IGANet上获得。

    Despite substantial progress in 3D human pose estimation from a single-view image, prior works rarely explore global and local correlations, leading to insufficient learning of human skeleton representations. To address this issue, we propose a novel Interweaved Graph and Attention Network (IGANet) that allows bidirectional communications between graph convolutional networks (GCNs) and attentions. Specifically, we introduce an IGA module, where attentions are provided with local information from GCNs and GCNs are injected with global information from attentions. Additionally, we design a simple yet effective U-shaped multi-layer perceptron (uMLP), which can capture multi-granularity information for body joints. Extensive experiments on two popular benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate our proposed method.The results show that IGANet achieves state-of-the-art performance on both datasets. Code is available at https://github.com/xiu-cs/IGANet.
    
[^35]: Mimic-IV-ICD：一种新的极端多标签分类基准

    Mimic-IV-ICD: A new benchmark for eXtreme MultiLabel Classification. (arXiv:2304.13998v1 [cs.AI])

    [http://arxiv.org/abs/2304.13998](http://arxiv.org/abs/2304.13998)

    本文提出了一个公共基准套件，使用大型公共电子病历数据集进行ICD-10编码。该方法标准化数据预处理并建立全面的ICD编码基准数据集，促进了自动化ICD编码在未来研究领域的应用。

    

    临床笔记被分配ICD代码，这是一组诊断和操作代码。近年来，已经构建了预测性机器学习模型，用于自动ICD编码。然而，基于大规模公共电子病历数据的自动化ICD编码模型缺乏广泛接受的基准。本文提出了一个公共基准套件，使用从最近的公共电子病历数据集MIMIC-IV衍生的大型电子病历数据集进行ICD-10编码。我们实施了并比较了几种流行的ICD编码预测任务方法，以标准化数据预处理并建立全面的ICD编码基准数据集。这种方法促进了可重复性和模型比较，加速了将自动化ICD编码应用于未来研究领域的进展。此外，我们使用MIMIC-IV数据创建了一个新的ICD-9基准，提供了比MIMIC-III更多的数据点和更多的ICD代码。我们的开源代码提供了易于访问的数据处理步骤、基准创建和模型评估，推动了自动ICD编码模型开发中的透明度和合作。

    Clinical notes are assigned ICD codes - sets of codes for diagnoses and procedures. In the recent years, predictive machine learning models have been built for automatic ICD coding. However, there is a lack of widely accepted benchmarks for automated ICD coding models based on large-scale public EHR data.  This paper proposes a public benchmark suite for ICD-10 coding using a large EHR dataset derived from MIMIC-IV, the most recent public EHR dataset. We implement and compare several popular methods for ICD coding prediction tasks to standardize data preprocessing and establish a comprehensive ICD coding benchmark dataset. This approach fosters reproducibility and model comparison, accelerating progress toward employing automated ICD coding in future studies. Furthermore, we create a new ICD-9 benchmark using MIMIC-IV data, providing more data points and a higher number of ICD codes than MIMIC-III. Our open-source code offers easy access to data processing steps, benchmark creation, an
    
[^36]: 使用隐式神经表示学习旋转和平移不变性表示

    Rotation and Translation Invariant Representation Learning with Implicit Neural Representations. (arXiv:2304.13995v1 [cs.CV])

    [http://arxiv.org/abs/2304.13995](http://arxiv.org/abs/2304.13995)

    本文提出了一种使用隐式神经表示和超网络进行表示学习的方法，称为不变性表示学习，可以在复杂图像上学到分离的语义表示，并且和SCAN很好地协同工作，从而获得最先进的无监督聚类结果。

    

    在许多计算机视觉应用中，图像是以任意或随机旋转和平移的方式获取的。在这样的设置中，获取与图像方向无关的语义表示是可取的。本文提出了一种使用隐式神经表示（INR）和超网络进行表示学习的方法，称为不变性表示学习（IRL-INR）。我们展示了IRL-INR可以在比以前的工作中更复杂的图像上有效地学习分离的语义表示，并且表明这些语义表示可以与SCAN很好地协同工作，从而产生最先进的无监督聚类结果。

    In many computer vision applications, images are acquired with arbitrary or random rotations and translations, and in such setups, it is desirable to obtain semantic representations disentangled from the image orientation. Examples of such applications include semiconductor wafer defect inspection, plankton microscope images, and inference on single-particle cryo-electron microscopy (cryo-EM) micro-graphs. In this work, we propose Invariant Representation Learning with Implicit Neural Representation (IRL-INR), which uses an implicit neural representation (INR) with a hypernetwork to obtain semantic representations disentangled from the orientation of the image. We show that IRL-INR can effectively learn disentangled semantic representations on more complex images compared to those considered in prior works and show that these semantic representations synergize well with SCAN to produce state-of-the-art unsupervised clustering results.
    
[^37]: 用品味聚类学习实现可解释性协同过滤

    Towards Explainable Collaborative Filtering with Taste Clusters Learning. (arXiv:2304.13937v1 [cs.IR])

    [http://arxiv.org/abs/2304.13937](http://arxiv.org/abs/2304.13937)

    本文提出了一种利用品味聚类学习实现可解释性协同过滤的模型，在保证高准确性的同时为用户和项目提供可解释的聚类解释。

    

    协同过滤是推荐系统中广泛使用且有效的技术。近年来，基于潜在嵌入的协同过滤方法（如矩阵分解、神经协同过滤和LightGCN）已经有了显著的进展，以提高准确性。但是，这些模型的可解释性尚未得到充分探索。给推荐模型添加解释性，不仅可以增加人们对决策过程的信任，而且还有多个好处，如为项目推荐提供有说服力的解释、为用户和项目创建明确的文件、为项目制造商提供设计改进的协助。在本文中，我们提出了一种清晰有效的可解释性协同过滤模型，利用可解释的聚类学习来实现两个最苛刻的目标：（1）精确——模型在追求可解释性时不应妥协准确性；（2）自我解释——模型的解释应易于人们理解。引入品味聚类学习来构成用户和项目的解释，并在四个真实数据集上进行实验，结果证实了我们提出的方法在提供人类可理解的解释的同时保证了高准确性。

    Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decisionmaking process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements.  In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model's explanations 
    
[^38]: 机器学习模型训练过程中对表现更好的少数族群进行过采样会稍微降低不良影响，但也会降低模型精度

    Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy. (arXiv:2304.13933v1 [cs.LG])

    [http://arxiv.org/abs/2304.13933](http://arxiv.org/abs/2304.13933)

    本研究发现，在机器学习模型训练中，对表现更好的少数族裔进行过采样会稍微减少不良影响，但也会降低模型精度。

    

    组织越来越倾向于采用机器学习来进行员工评估。然而，人们对机器学习评估公平性的担忧也日益增加。本研究系统地对少数族裔（黑人和西班牙裔）进行了欠采样和过采样，以改变训练数据中的不良影响比率，并调查训练数据中不良影响比率如何影响机器学习模型的不良影响和准确性。我们使用工作申请人的自我报告和面试记录（N = 2,501）训练了9,702个机器学习模型来预测筛选决策。我们发现，训练数据中的不良影响与机器学习模型的不良影响呈线性相关。然而，从训练数据中消除不良影响仅稍微降低了机器学习模型的不良影响，而且也降低了模型精度。

    Organizations are increasingly adopting machine learning (ML) for personnel assessment. However, concerns exist about fairness in designing and implementing ML assessments. Supervised ML models are trained to model patterns in data, meaning ML models tend to yield predictions that reflect subgroup differences in applicant attributes in the training data, regardless of the underlying cause of subgroup differences. In this study, we systematically under- and oversampled minority (Black and Hispanic) applicants to manipulate adverse impact ratios in training data and investigated how training data adverse impact ratios affect ML model adverse impact and accuracy. We used self-reports and interview transcripts from job applicants (N = 2,501) to train 9,702 ML models to predict screening decisions. We found that training data adverse impact related linearly to ML model adverse impact. However, removing adverse impact from training data only slightly reduced ML model adverse impact and tende
    
[^39]: NIMS-OS：用于实现材料科学中人工智能和机器人实验之间闭环的自动化软件

    NIMS-OS: An automation software to implement a closed loop between artificial intelligence and robotic experiments in materials science. (arXiv:2304.13927v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2304.13927](http://arxiv.org/abs/2304.13927)

    NIMS-OS是一款Python库，用于实现材料科学中人工智能和机器人实验之间闭环的自动化，无需人工干预。通过使用贝叶斯优化、无限目标自由探索、相图构建和随机探索等AI技术，NIMS-OS能够在实验条件如化学成分、反应温度和加热时间改变的情况下搜索光催化剂。经验证实，使用NIMS-OS可以找到具有高光化学产氢活性的前所未知的光催化剂。

    

    NIMS-OS（NIMS Orchestration System）是一个Python库，用于实现材料探索的人工智能和机器人实验的闭环自动化，无需人工干预。它使用各种组合的模块进行自主操作，每个模块都充当材料探索的人工智能或机器人实验的控制器。可以使用贝叶斯优化（PHYSBO），无限目标自由探索（BLOX），相图构建（PDC）和随机探索（RE）等人工智能技术。此外，还有一个称为NIMS自动机器人电化学实验（NAREE）的系统可用作一组机器人实验设备。结果可视化工具也包括在内，用户可以实时检查优化结果。可以轻松添加新的AI和机器人实验模块以扩展系统的功能。此外，我们还开发了一个GUI应用程序来控制NIMS-OS。

    NIMS-OS (NIMS Orchestration System) is a Python library created to realize a closed loop of robotic experiments and artificial intelligence (AI) without human intervention for automated materials exploration. It uses various combinations of modules to operate autonomously. Each module acts as an AI for materials exploration or a controller for a robotic experiments. As AI techniques, Bayesian optimization (PHYSBO), boundless objective-free exploration (BLOX), phase diagram construction (PDC), and random exploration (RE) methods can be used. Moreover, a system called NIMS automated robotic electrochemical experiments (NAREE) is available as a set of robotic experimental equipment. Visualization tools for the results are also included, which allows users to check the optimization results in real time. Newly created modules for AI and robotic experiments can be added easily to extend the functionality of the system. In addition, we developed a GUI application to control NIMS-OS.To demonst
    
[^40]: 基于马尔科夫决策过程的关卡组装

    Level Assembly as a Markov Decision Process. (arXiv:2304.13922v1 [cs.AI])

    [http://arxiv.org/abs/2304.13922](http://arxiv.org/abs/2304.13922)

    本文中提出了基于马尔科夫决策过程的自适应动态规划方法来生成适应玩家的动态关卡进度，并在两个案例研究中展示了其优于两个基线的效果。

    

    许多游戏都采用不适应玩家的关卡进度。这可能会导致玩家在进度过于困难时卡住，而在进度过慢时会感到无聊，导致不愿意等待更具挑战性的关卡。本文中，我们将为玩家生成关卡的问题转化为一个马尔科夫决策过程（MDP），并利用自适应动态规划（ADP）来解决MDP，然后再组装出关卡。我们进行了两个案例研究，发现使用ADP优于两个基线。此外，我们还进行了玩家代理的实验，并在游戏过程中切换了它们，结果表明在运行ADP之前进行简单修改可以快速适应。通过使用ADP，我们可以搜索整个MDP，并产生适应玩家的动态关卡进度。

    Many games feature a progression of levels that doesn't adapt to the player. This can be problematic because some players may get stuck if the progression is too difficult, while others may find it boring if the progression is too slow to get to more challenging levels. This can be addressed by building levels based on the player's performance and preferences. In this work, we formulate the problem of generating levels for a player as a Markov Decision Process (MDP) and use adaptive dynamic programming (ADP) to solve the MDP before assembling a level. We tested with two case studies and found that using an ADP outperforms two baselines. Furthermore, we experimented with player proxies and switched them in the middle of play, and we show that a simple modification prior to running ADP results in quick adaptation. By using ADP, which searches the entire MDP, we produce a dynamic progression of levels that adapts to the player.
    
[^41]: 提高LLM答案准确度的联邦提示和链式推理

    Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering. (arXiv:2304.13911v1 [cs.AI])

    [http://arxiv.org/abs/2304.13911](http://arxiv.org/abs/2304.13911)

    本论文提出一种名为Fed-SP-SC和Fed-DP-CoT的技术，通过联邦提示和链式推理改进分布式同义问题的准确性，从而提高基于云的大型语言模型（LLMs）回答常见问题的精度，并进行了充分实验验证。

    

    本文研究如何使用基于云的大型语言模型（LLMs）增强分布式用户提出的常见问题的回答精度。我们的研究侧重于典型情况，即用户询问涉及相同的数学推理步骤和问题解决过程的相似查询。由于LLMs独立问题的零-shot提示的准确性不尽如人意，我们提出了使用自洽性（SC）和链式思考（CoT）技术来改进分布式同义问题的方法。具体而言，我们首先从众包数据库中检索同义问题，并创建一个联邦问题池。我们称这些具有相同或不同参数的联邦同义问题为SP问题或DP问题，分别。我们将我们的方法称为Fed-SP-SC和Fed-DP-CoT，它们可以为所有用户查询生成更准确的答案，而不需要复杂的模型调整。通过大量实验证明

    We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that
    
[^42]: 基于LSTM的物联网设备识别

    LSTM based IoT Device Identification. (arXiv:2304.13905v1 [cs.CR])

    [http://arxiv.org/abs/2304.13905](http://arxiv.org/abs/2304.13905)

    本研究提出了一种使用LSTM进行物联网设备识别的方法，以预防安全威胁和检测漏洞为目标。

    

    随着物联网的使用越来越普遍，随之而来的是大量设备造成的安全漏洞。在这样的环境下，物联网设备识别方法成为重要的预防性安全措施，可以识别这些设备并检测它们所面临的漏洞。本研究提出了一种使用长短时记忆（LSTM）的方法，在Aalto数据集中识别设备。

    While the use of the Internet of Things is becoming more and more popular, many security vulnerabilities are emerging with the large number of devices being introduced to the market. In this environment, IoT device identification methods provide a preventive security measure as an important factor in identifying these devices and detecting the vulnerabilities they suffer from. In this study, we present a method that identifies devices in the Aalto dataset using Long short-term memory (LSTM)
    
[^43]: 以双分法特定事件树为基础的层次表示法用于谣言检测

    Rumor Detection with Hierarchical Representation on Bipartite Adhoc Event Trees. (arXiv:2304.13895v1 [cs.SI])

    [http://arxiv.org/abs/2304.13895](http://arxiv.org/abs/2304.13895)

    该论文提出了一种名为BAET的新型谣言检测模型，利用以双分法特定事件树为基础的层次表示法，在利用现有社交网络中的拓扑结构和信息传播的影响力的基础上，具有更好的性能。

    

    社交媒体的快速增长对信息传播产生了巨大的影响，从而使谣言检测的检测变得极具挑战性。现有的谣言检测方法通常利用谣言候选的转发传播进行检测，将所有转发到谣言候选的转发视为时间序列，并学习转发序列的语义表示。然而，从传播的拓扑结构和转发作者的影响中提取有用的支持来揭穿谣言是至关重要的，这通常没有得到很好的解决。在本文中，我们将流通的声称帖子组织为一个adhoc事件树，提取事件元素，并将其转换为基于帖子和作者的双分法adhoc事件树，即作者树和帖子树。因此，我们提出了一种基于双分法特定事件树的层次表示的新型谣言检测模型BAET。具体来说，我们引入了单词嵌入技术和注意力机制来学习每个事件元素的语义，并组成更高层次的事件表示。借助于现有的社交网络中的拓扑结构和信息传播的影响力，BAET可以生成更准确的传播树结构。在两个真实数据集上的实验结果表明，BAET相比于其他方法具有更好的性能。

    The rapid growth of social media has caused tremendous effects on information propagation, raising extreme challenges in detecting rumors. Existing rumor detection methods typically exploit the reposting propagation of a rumor candidate for detection by regarding all reposts to a rumor candidate as a temporal sequence and learning semantics representations of the repost sequence. However, extracting informative support from the topological structure of propagation and the influence of reposting authors for debunking rumors is crucial, which generally has not been well addressed by existing methods. In this paper, we organize a claim post in circulation as an adhoc event tree, extract event elements, and convert it to bipartite adhoc event trees in terms of both posts and authors, i.e., author tree and post tree. Accordingly, we propose a novel rumor detection model with hierarchical representation on the bipartite adhoc event trees called BAET. Specifically, we introduce word embedding
    
[^44]: 基于CNN的物联网设备识别

    CNN based IoT Device Identification. (arXiv:2304.13894v1 [cs.CR])

    [http://arxiv.org/abs/2304.13894](http://arxiv.org/abs/2304.13894)

    本研究提出了一种基于CNN的物联网设备识别方法，可对设备进行识别并检测其安全漏洞。

    

    随着物联网的使用越来越普及，市场上引入了大量设备，导致许多安全漏洞的产生。在这种环境中，物联网设备识别方法作为识别这些设备并检测它们所受漏洞的重要因素，提供了一项预防性的安全措施。本研究提出了一种通过卷积神经网络（CNN）识别Aalto数据集中设备的方法。

    While the use of the Internet of Things is becoming more and more popular, many security vulnerabilities are emerging with the large number of devices being introduced to the market. In this environment, IoT device identification methods provide a preventive security measure as an important factor in identifying these devices and detecting the vulnerabilities they suffer from. In this study, we present a method that identifies devices in the Aalto dataset using the convolutional neural network (CNN).
    
[^45]: 从像素中学习物体中心化的广义值函数

    Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])

    [http://arxiv.org/abs/2304.13892](http://arxiv.org/abs/2304.13892)

    本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。

    

    深度强化学习展现了从高维输入中提取有用表示的显著进展，尽管使用的是手工辅助任务和伪奖励。自动化地以物体为中心学习此类表示，以期实现控制和快速适应，仍然是一个未解决的研究问题。在本文中，我们介绍了一种方法，试图从物体中发现有意义的特征，将它们转化为时间上连贯的“问题”函数，并利用随后学习的广义值函数来进行控制。我们将我们的方法与最先进的技术进行比较，并展示了在静态和非静态设置下的竞争性表现。最后，我们还调查了被发现的广义值函数，并通过定性分析表明，学到的表示不仅是可解释的，而且围绕着物体，这些物体对任务的变化具有不变性，有助于快速适应。

    Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
    
[^46]: highway2vec——基于道路网络特征将OpenStreetMap微区域表示出来

    highway2vec -- representing OpenStreetMap microregions with respect to their road network characteristics. (arXiv:2304.13865v1 [cs.LG])

    [http://arxiv.org/abs/2304.13865](http://arxiv.org/abs/2304.13865)

    本文提出了一种基于OpenStreetMap城市道路网络的微区域的嵌入方法，以检测地图六边形在包含的道路网络中的相似性。

    

    近年来，神经网络在表示学习中得到广泛应用，可以自动化完成一些需要手动设计特征的任务。对于需要考虑空间变量的问题，使用预训练的地图区域表示可以避免手动创建特征表。然而，针对道路网络特征的地图区域表示方法非常少。本文提出了一种基于OpenStreetMap城市道路网络的微区域的嵌入方法，利用H3空间索引实现可重复和可扩展的表示学习并获得矢量表示，以检测地图六边形在包含的道路网络中的相似性。

    Recent years brought advancements in using neural networks for representation learning of various language or visual phenomena. New methods freed data scientists from hand-crafting features for common tasks. Similarly, problems that require considering the spatial variable can benefit from pretrained map region representations instead of manually creating feature tables that one needs to prepare to solve a task. However, very few methods for map area representation exist, especially with respect to road network characteristics. In this paper, we propose a method for generating microregions' embeddings with respect to their road infrastructure characteristics. We base our representations on OpenStreetMap road networks in a selection of cities and use the H3 spatial index to allow reproducible and scalable representation learning. We obtained vector representations that detect how similar map hexagons are in the road networks they contain. Additionally, we observe that embeddings yield a
    
[^47]: Ensoul: 通过进化的enerstatic网络创建自组织智能超低功耗系统(SOULS)的框架

    Ensoul: A framework for the creation of self organizing intelligent ultra low power systems (SOULS) through evolutionary enerstatic networks. (arXiv:2304.13863v1 [cs.AI])

    [http://arxiv.org/abs/2304.13863](http://arxiv.org/abs/2304.13863)

    Ensoul是一种框架，通过enerstatic网络和开放进化技术结合，创造了能够独立于其嵌入的基质的自组织智能超低功耗系统(SOULS)。

    

    Ensoul是一个提出的框架，旨在通过能量稳态(enerstatic)回路和开放式进化技术的网络和嵌套结构的结合，创建出更多的技术。通过这种方法开发的生成技术既是热力学驱动复杂系统的简单而有洞见的模型，也是创新技术的强大源泉。 "自组织智能超低功耗系统"（SOULS）是一个能够描述此类生成技术及其产生的技术的术语。该术语旨在捕捉这些技术的抽象本质，即它们独立于其嵌入的基质。换句话说，SOULS可以是生物、人工或混合的形式。

    Ensoul is a framework proposed for the purpose of creating technologies that create more technologies through the combined use of networks, and nests, of energy homeostatic (enerstatic) loops and open-ended evolutionary techniques. Generative technologies developed by such an approach serve as both simple, yet insightful models of thermodynamically driven complex systems and as powerful sources of novel technologies. "Self Organizing intelligent Ultra Low power Systems" (SOULS) is a term that well describes the technologies produced by such a generative technology, as well as the generative technology itself. The term is meant to capture the abstract nature of such technologies as being independent of the substrate in which they are embedded. In other words, SOULS can be biological, artificial or hybrid in form.
    
[^48]: 多模态复合关联评分：衡量生成式多模态模型中的性别偏差

    Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models. (arXiv:2304.13855v1 [cs.CV])

    [http://arxiv.org/abs/2304.13855](http://arxiv.org/abs/2304.13855)

    这篇论文提出了一种新方法——多模态复合关联评分(MCAS)，用于衡量多模态生成模型中的性别偏差。与单模态小型单阶段模型的度量与量化不同，该方法能够高效、有效地识别和测量大型复杂的多模态生成模型中的性别偏差。

    

    基于扩散模型的生成式多模态模型近年来取得了巨大的发展和进步。像DALL-E和Stable Diffusion这样的模型越来越受欢迎，成功地从文本中创建图像，经常结合抽象的想法。然而，像其他深度学习模型一样，它们也反映了从互联网中爬取的训练数据中继承的社会偏见。手动审核模型中的偏见可能非常耗时和资源消耗，并且由这些模型可以接受的输入的无限制和不受约束的性质使问题更加复杂。 对于偏见的度量与量化的研究通常侧重于单模态的小型单阶段模型。因此，多阶段多模态模型的出现需要不同的方法。 在本文中，我们提出了多模态复合关联评分（MCAS）作为衡量多模态生成式模型中性别偏差的新方法。通过使用MCAS评估DALL-E 2和稳定扩散，我们展示了检测并度量生成图像中性别偏差的能力。我们的结果表明，DALL-E 2生成的图像具有最小的性别偏见，而稳定扩散生成的图像存在显著的性别偏见。所提出的MCAS指标允许对大型复杂的多模态生成模型中的性别偏见进行高效且有效的识别和测量。

    Generative multimodal models based on diffusion models have seen tremendous growth and advances in recent years. Models such as DALL-E and Stable Diffusion have become increasingly popular and successful at creating images from texts, often combining abstract ideas. However, like other deep learning models, they also reflect social biases they inherit from their training data, which is often crawled from the internet. Manually auditing models for biases can be very time and resource consuming and is further complicated by the unbounded and unconstrained nature of inputs these models can take. Research into bias measurement and quantification has generally focused on small single-stage models working on a single modality. Thus the emergence of multistage multimodal models requires a different approach. In this paper, we propose Multimodal Composite Association Score (MCAS) as a new method of measuring gender bias in multimodal generative models. Evaluating both DALL-E 2 and Stable Diffu
    
[^49]: 理解动态世界：面向开放领域实体状态跟踪的端到端知识驱动框架

    Understand the Dynamic World: An End-to-End Knowledge Informed Framework for Open Domain Entity State Tracking. (arXiv:2304.13854v1 [cs.AI])

    [http://arxiv.org/abs/2304.13854](http://arxiv.org/abs/2304.13854)

    本论文提出了一种名为KIEST的端到端知识驱动框架，该框架通过外部知识图谱显式地检索相关实体和属性，并使用动态知识粒度的编码器-解码器框架自回归生成所有实体状态变化。

    

    开放领域实体状态跟踪旨在预测实体的合理状态变化（即[实体]的[属性]在[之前状态]和[之后状态]之间发生了变化），给定动作描述，这对许多推理任务都非常重要，以支持人类日常活动。然而，由于大多数实体都与动作及其属性隐含相关，并且来自于开放词汇，因此这是一个具有挑战性的问题。为了应对这些挑战，我们提出了一种面向开放领域实体状态跟踪的新颖的端到端知识驱动框架，即KIEST，它明确地从外部知识图谱（即ConceptNet）检索相关实体和属性，并将它们与新颖动态知识粒度的编码器-解码器框架一起用于自回归生成所有实体状态变化。

    Open domain entity state tracking aims to predict reasonable state changes of entities (i.e., [attribute] of [entity] was [before_state] and [after_state] afterwards) given the action descriptions. It's important to many reasoning tasks to support human everyday activities. However, it's challenging as the model needs to predict an arbitrary number of entity state changes caused by the action while most of the entities are implicitly relevant to the actions and their attributes as well as states are from open vocabularies. To tackle these challenges, we propose a novel end-to-end Knowledge Informed framework for open domain Entity State Tracking, namely KIEST, which explicitly retrieves the relevant entities and attributes from external knowledge graph (i.e., ConceptNet) and incorporates them to autoregressively generate all the entity state changes with a novel dynamic knowledge grained encoder-decoder framework. To enforce the logical coherence among the predicted entities, attribute
    
[^50]: 基于人工智能的预测分析方法来保障电动/混合动力汽车未来发展的论文

    AI-based Predictive Analytic Approaches for safeguarding the Future of Electric/Hybrid Vehicles. (arXiv:2304.13841v1 [cs.AI])

    [http://arxiv.org/abs/2304.13841](http://arxiv.org/abs/2304.13841)

    本文研究了基于人工智能的预测分析方法对电动和混合动力汽车的生产与发展的影响，为更加环保的交通需求做出贡献。

    

    为了应对全球对可持续能源的需求，绿色技术可以帮助应对气候变化。然而，在绿色基础设施能够轻易整合进入全球能源系统之前，需要对其进行升级。通过改进能源基础设施和决策制定，人工智能（AI）可以帮助解决这一挑战。由于对全球变暖的担忧和对更加环保的交通需求，混动电动汽车变得越来越流行。混动电动汽车可以更好地与像AI这样的尖端技术结合使用。电动汽车（EVs）降低了温室气体排放，并促进了可持续出行。不幸的是，EV的生产会消耗大量能源和材料，这可能会对自然环境造成损害。使用人工智能和预测分析等绿色技术来改进EV的生产。电动和混合动力汽车（EHVs）可以帮助满足对更加环保的交通需求。

    In response to the global need for sustainable energy, green technology may help fight climate change. Before green infrastructure to be easily integrated into the world's energy system, it needs upgrading. By improving energy infrastructure and decision-making, artificial intelligence (AI) may help solve this challenge. EHVs have grown in popularity because to concerns about global warming and the need for more ecologically friendly transportation. EHVs may work better with cutting-edge technologies like AI. Electric vehicles (EVs) reduce greenhouse gas emissions and promote sustainable mobility. Electric automobiles (EVs) are growing in popularity due to their benefits for climate change mitigation and sustainable mobility. Unfortunately, EV production consumes a lot of energy and materials, which may harm nature. EV production is being improved using green technologies like artificial intelligence and predictive analysis. Electric and hybrid vehicles (EHVs) may help meet the need fo
    
[^51]: 论RemOve-And-Retrain的陷阱：数据处理不等式的视角

    On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])

    [http://arxiv.org/abs/2304.13836](http://arxiv.org/abs/2304.13836)

    本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。

    

    本文评估了RemOve-And-Retrain（ROAR）协议的可靠性，该协议用于测量特征重要性估计的性能。我们从理论背景和实证实验中发现，具有较少有关决策功能的信息的属性在ROAR基准测试中表现更好，与ROAR的原始目的相矛盾。这种现象也出现在最近提出的变体RemOve-And-Debias（ROAD）中，我们提出了ROAR归因度量中毛糙度偏差的一致趋势。我们的结果提醒人们不要盲目依赖ROAR的性能评估指标。

    This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
    
[^52]: 可编程接地，组合通用的机器人操作

    Programmatically Grounded, Compositionally Generalizable Robotic Manipulation. (arXiv:2304.13826v1 [cs.AI])

    [http://arxiv.org/abs/2304.13826](http://arxiv.org/abs/2304.13826)

    本文提出了一种模块化的方法ProgramPort，它利用语言指令的句法和语义结构，以更好地利用预训练视觉-语言（VL）模型。该框架使用语义解析器恢复一个可执行程序，由跨不同模态的基于视觉和行动的功能模块组成。

    

    在现实世界中操作的机器人需要丰富的操作技能以及在何时应用这些技能方面具有语义推理的能力。为此，最近的工作将来自大规模预训练视觉-语言（VL）模型的语义表示集成到操作模型中，赋予其更通用的推理能力。然而，我们发现用于整合此类表示的传统预训练微调流程将领域特定的行动信息和领域通用的视觉信息纠缠在一起，导致训练数据效率低下且泛化到未见过的对象和任务很差。为此，我们提出了 ProgramPort，这是一种更好地利用预训练 VL 模型的模块化方法，通过利用语言指令的句法和语义结构。我们的框架使用语义解析器恢复一个可执行程序，由跨不同模态的基于视觉和行动的功能模块组成。

    Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional pretraining-finetuning pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose ProgramPort, a modular approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities.
    
[^53]: 神经网络模型压缩的量化误差计算保证

    Guaranteed Quantization Error Computation for Neural Network Model Compression. (arXiv:2304.13812v1 [cs.LG])

    [http://arxiv.org/abs/2304.13812](http://arxiv.org/abs/2304.13812)

    本文提出了一种通过建立合并的神经网络，应用优化方法和可达性分析方法来计算保证的量化误差的方法，解决了神经网络压缩的保证输出误差计算问题。

    

    神经网络模型压缩技术可以解决工业系统中嵌入式设备上的深度神经网络计算问题。本文解决了带有量化的神经网络压缩的保证输出误差计算问题。通过建立一个合并的神经网络，将前向神经网络和其量化版本合并，以产生两个神经网络之间的精确输出差异。然后，应用基于优化的方法和可达性分析方法到合并的神经网络中，计算保证的量化误差。最后，提出了一个数值例子来验证所提出方法的适用性和有效性。

    Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.
    
[^54]: 走向伦理多模态系统

    Towards ethical multimodal systems. (arXiv:2304.13765v1 [cs.AI])

    [http://arxiv.org/abs/2304.13765](http://arxiv.org/abs/2304.13765)

    本文讨论了伦理评价多模态人工智能系统的挑战，通过创建多模态伦理数据库来协同判断系统是否满足伦理要求。

    

    人工智能系统对我们社会的影响正在以前所未有的速度增长。例如，ChatGPT正在进行心理健康治疗应用的测试，如Koko，Stable Diffusion生成的艺术作品与人类艺术家相比具有竞争力，甚至表现更好。对生成型人工智能系统行为和应用的伦理问题近年来不断增加，AI对齐领域——将人工智能系统的行为引导向与人类价值观相一致的方向——是现代人工智能的一个快速发展的子领域。在本文中，我们探讨了伦理评价多模态人工智能系统的挑战。我们专注于同时以文本和图像作为输入并输出文本的多模态系统，完成作为输入的句子或问题的回答。我们分两步对这些模型进行评估：我们首先讨论了多模态伦理数据库的创建，然后使用这个数据库来协同判断系统是否满足伦理要求。

    The impact of artificial intelligence systems on our society is increasing at an unprecedented speed. For instance, ChatGPT is being tested in mental health treatment applications such as Koko, Stable Diffusion generates pieces of art competitive with (or outperforming) human artists, and so on. Ethical concerns regarding the behavior and applications of generative AI systems have been increasing over the past years, and the field of AI alignment - steering the behavior of AI systems towards being aligned with human values - is a rapidly growing subfield of modern AI. In this paper, we address the challenges involved in ethical evaluation of a multimodal artificial intelligence system. The multimodal systems we focus on take both text and an image as input and output text, completing the sentence or answering the question asked as input. We perform the evaluation of these models in two steps: we first discus the creation of a multimodal ethical database and then use this database to co
    
[^55]: TR0N：0-Shot即插即用条件生成的翻译网络

    TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation. (arXiv:2304.13742v1 [cs.LG])

    [http://arxiv.org/abs/2304.13742](http://arxiv.org/abs/2304.13742)

    本文提出了TR0N，将预训练的无条件生成模型转化为高度任意的条件模型。TR0N不需要训练数据或微调，可以在MS-COCO上实现零-shot FID 10.9，并在采样速度上优于竞品，同时保持了多样性和质量。

    

    本文提出了TR0N，一个高度通用的框架，将预训练的无条件生成模型，如GAN和VAE，转换为条件模型。条件可以是高度任意的，并且仅需要预训练的辅助模型。例如，我们展示了如何使用分类器将无条件模型转化为类别条件模型，并利用CLIP将其转化为文本到图像模型。TR0N学习了一种轻量级的随机映射，该映射在条件空间和生成模型的潜在空间之间“翻译”，使得生成的潜在空间对应于满足所需条件的数据样本。然后，通过Langevin动态进一步改进翻译后的潜在样本，使我们能够获得更高质量的数据样本。TR0N不需要训练数据或微调，但可以在MS-COCO上实现零-shot FID 10.9，不仅在这个指标上优于竞品，而且在采样速度上也与其保持了多样性和质量。

    We propose TR0N, a highly general framework to turn pre-trained unconditional generative models, such as GANs and VAEs, into conditional models. The conditioning can be highly arbitrary, and requires only a pre-trained auxiliary model. For example, we show how to turn unconditional models into class-conditional ones with the help of a classifier, and also into text-to-image models by leveraging CLIP. TR0N learns a lightweight stochastic mapping which "translates" between the space of conditions and the latent space of the generative model, in such a way that the generated latent corresponds to a data sample satisfying the desired condition. The translated latent samples are then further improved upon through Langevin dynamics, enabling us to obtain higher-quality data samples. TR0N requires no training data nor fine-tuning, yet can achieve a zero-shot FID of 10.9 on MS-COCO, outperforming competing alternatives not only on this metric, but also in sampling speed -- all while retaining 
    
[^56]: 可扩展的分布式AI框架：利用云计算提高深度学习性能和效率

    Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency. (arXiv:2304.13738v1 [cs.LG])

    [http://arxiv.org/abs/2304.13738](http://arxiv.org/abs/2304.13738)

    本文介绍了可扩展的分布式AI框架，利用云计算提高深度学习性能和效率。通过概述常用的AI框架和云服务、探讨基于云的AI系统的关键方面和讨论云中AI工作负载的优化策略，提供了解决AI应用程序不断增长的计算需求的有效方法。

    

    近年来，将人工智能（AI）和云计算相结合已成为解决AI应用程序不断增长的计算需求的有效方法。本文介绍了一项关于利用云计算提高深度学习性能和效率的可扩展的分布式AI框架的全面研究。首先，我们概述了常用的AI框架和云服务，强调了它们各自的优缺点。接着，我们深入探讨了基于云的AI系统中数据存储和管理的关键方面，包括数据预处理、特征工程、隐私和安全。接下来，我们探讨了AI模型的并行和分布式训练技术，重点讨论了模型的分区、通信策略和基于云的训练架构。在随后的章节中，我们讨论了云中AI工作负载的优化策略，涵盖了负载平衡、资源分配、自动缩放和性能基准测试等方面。

    In recent years, the integration of artificial intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. This paper presents a comprehensive study of scalable, distributed AI frameworks leveraging cloud computing for enhanced deep learning performance and efficiency. We first provide an overview of popular AI frameworks and cloud services, highlighting their respective strengths and weaknesses. Next, we delve into the critical aspects of data storage and management in cloud-based AI systems, discussing data preprocessing, feature engineering, privacy, and security. We then explore parallel and distributed training techniques for AI models, focusing on model partitioning, communication strategies, and cloud-based training architectures.  In subsequent chapters, we discuss optimization strategies for AI workloads in the cloud, covering load balancing, resource allocation, auto-scaling, and performance benc
    
[^57]: 一个LLM知道自己在撒谎的内部状态

    The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])

    [http://arxiv.org/abs/2304.13734](http://arxiv.org/abs/2304.13734)

    该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。

    

    虽然大型语言模型（LLM）在各种任务中表现出了卓越的性能，但它们（可能）最为突出的缺点是以自信的语气生成不准确或虚假的信息。本文假设LLM的内部状态可以用于揭示一个语句的真实性。因此，我们介绍了一种简单但有效的方法来检测LLM所生成语句的真实性，该方法利用LLM的隐藏层激活来确定语句的真实性。为了训练和评估我们的方法，我们构建了一个包含六个不同主题的数据集，其中包含真实和虚假的语句。一个分类器被训练出来，根据LLM的激活值来检测哪个语句是真实的或虚假的。具体而言，分类器接收LLM为数据集中每个语句生成的激活值作为输入。我们的实验表明，我们检测语句真实性的方法甚至比少量提示方法表现更好，凸显了利用LLM的内部状态来提高其可信度的潜力。

    While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
    
[^58]: 使用指令调整的LLM和潜在扩散模型生成文本到音频

    Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. (arXiv:2304.13731v1 [eess.AS])

    [http://arxiv.org/abs/2304.13731](http://arxiv.org/abs/2304.13731)

    本研究提出了一种使用指令调整的LLM Flan-T5作为文本编码器和基于潜在扩散模型(LDM)的方法TANGO生成文本到音频(TTA)的新方法，在AudioCaps测试集上表现优于先进的AudioLDM。

    

    最近的大型语言模型(LLM)的巨大规模允许许多有趣的属性，比如，基于指令和思路链的微调，在许多自然语言处理(NLP)任务中显着提高了零次和少量训练样本的性能。受到这些成功的启发，我们采用了这样一种经过指令调整的LLM Flan-T5作为文本编码器，用于文本到音频(TTA)生成任务——目标是根据其文本描述生成音频。之前关于TTA的工作要么预先训练一个联合的文本-音频编码器，要么使用一个非指令调谐的模型，如T5。因此，我们基于潜在扩散模型(LDM)的方法TANGO在AudioCaps测试集上表现出比最先进的AudioLDM更好的大多数指标，并在其余指标上持平，尽管我们使用了63倍小的数据集来训练LDM，并保持文本编码器不变。这种改进可能还归因于采用基于音频压力级的混音训练集增强。

    The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmenta
    
[^59]: 集成CNN用于乳腺肿瘤分类

    Ensemble CNNs for Breast Tumor Classification. (arXiv:2304.13727v1 [eess.IV])

    [http://arxiv.org/abs/2304.13727](http://arxiv.org/abs/2304.13727)

    本文使用集成CNN的方法对乳腺肿瘤进行分类，提高了分类性能5%以上，并在公共数据集上实现了高准确率、精度和召回率。

    

    为了提高计算机辅助乳腺肿块分类的识别能力，本文探索了最先进的分类网络，并使用集成机制来提高分类性能。首先，从原始数据集中获取感兴趣区域（ROI），然后分别训练三个模型，即XceptionNet、DenseNet和EfficientNet。训练完成后，通过将每个网络输出的概率相加来集成机制，从而将性能提高了5%。该方案已在公共数据集上验证，并实现了88%的准确率、85%的精度和76%的召回率。

    To improve the recognition ability of computer-aided breast mass classification among mammographic images, in this work we explore the state-of-the-art classification networks to develop an ensemble mechanism. First, the regions of interest (ROIs) are obtained from the original dataset, and then three models, i.e., XceptionNet, DenseNet, and EfficientNet, are trained individually. After training, we ensemble the mechanism by summing the probabilities outputted from each network which enhances the performance up to 5%. The scheme has been validated on a public dataset and we achieved accuracy, precision, and recall 88%, 85%, and 76% respectively.
    
[^60]: SamurAI: 一种具有事件驱动唤醒和嵌入式机器学习加速的多功能物联网节点

    SamurAI: A Versatile IoT Node With Event-Driven Wake-Up and Embedded ML Acceleration. (arXiv:2304.13726v1 [cs.NI])

    [http://arxiv.org/abs/2304.13726](http://arxiv.org/abs/2304.13726)

    SamurAI是一种多功能物联网节点，具有事件驱动唤醒和嵌入式机器学习加速器，通过利用两个芯片内子系统来弥合处理和能量的差距，并且为物联网应用提供了识别和自适应等功能。

    

    物联网应用现在需要具备识别和自适应等功能。虽然物联网节点的功耗是这些应用的主要关注点，但由于无线网络上连续传输传感器或图像数据，基于云的处理变得不可持续。因此，应在物联网节点中集成优化的机器学习能力和数据传输。此外，物联网应用通常需要进行间歇性数据记录和能耗高的数据处理（例如图像分类）。因此，节点的多功能性在解决处理和能源的广泛需求方面非常关键。本文介绍了SamurAI，这是一种多功能的物联网节点，通过利用两个芯片内子系统来弥合处理和能量方面的差距：一个低功率、无时钟、事件驱动的Always-Responsive (AR) 部分和一个高效的On-Demand (OD) 部分。 AR包含一个1.7MOPS事件驱动的异步 Wake-up 控制器（WuC），207ns唤醒时间的优化适用于间歇性操作。

    Increased capabilities such as recognition and self-adaptability are now required from IoT applications. While IoT node power consumption is a major concern for these applications, cloud-based processing is becoming unsustainable due to continuous sensor or image data transmission over the wireless network. Thus optimized ML capabilities and data transfers should be integrated in the IoT node. Moreover, IoT applications are torn between sporadic data-logging and energy-hungry data processing (e.g. image classification). Thus, the versatility of the node is key in addressing this wide diversity of energy and processing needs. This paper presents SamurAI, a versatile IoT node bridging this gap in processing and in energy by leveraging two on-chip sub-systems: a low power, clock-less, event-driven Always-Responsive (AR) part and an energy-efficient On-Demand (OD) part. AR contains a 1.7MOPS event-driven, asynchronous Wake-up Controller (WuC) with a 207ns wake-up time optimized for sporadi
    
[^61]: 基于多模态融合和非线性相关学习的脑肿瘤复发位置预测

    Prediction of brain tumor recurrence location based on multi-modal fusion and nonlinear correlation learning. (arXiv:2304.13725v1 [eess.IV])

    [http://arxiv.org/abs/2304.13725](http://arxiv.org/abs/2304.13725)

    本文提出了一种基于深度学习的方法来预测脑肿瘤复发位置。使用迁移学习、多模态融合和非线性相关学习，能够有效提取数据中的特征并预测复发的位置。

    

    脑肿瘤是导致癌症死亡的主要原因之一，高级别的脑肿瘤甚至在标准治疗后容易复发。因此，开发一种预测脑肿瘤复发位置的方法在治疗规划中起着重要作用，并有可能延长患者的生存时间。本文提出了一种基于深度学习的脑肿瘤复发位置预测网络。由于数据集通常很小，我们提出使用迁移学习来提高预测技术。我们首先在公共数据集BraTS 2021上训练一个多模态脑肿瘤分割网络。然后，将预训练的编码器转移到我们的私有数据集中，以提取丰富的语义特征。接下来，我们开发了一个多尺度多通道特征融合模型和一个非线性相关学习模块来学习有效的特征。多通道特征之间的相关性由一个非线性关系模型建模。

    Brain tumor is one of the leading causes of cancer death. The high-grade brain tumors are easier to recurrent even after standard treatment. Therefore, developing a method to predict brain tumor recurrence location plays an important role in the treatment planning and it can potentially prolong patient's survival time. There is still little work to deal with this issue. In this paper, we present a deep learning-based brain tumor recurrence location prediction network. Since the dataset is usually small, we propose to use transfer learning to improve the prediction. We first train a multi-modal brain tumor segmentation network on the public dataset BraTS 2021. Then, the pre-trained encoder is transferred to our private dataset for extracting the rich semantic features. Following that, a multi-scale multi-channel feature fusion model and a nonlinear correlation learning module are developed to learn the effective features. The correlation between multi-channel features is modeled by a no
    
[^62]: 发挥LLMs在实践中的力量：ChatGPT及其应用的综述调查

    Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])

    [http://arxiv.org/abs/2304.13712](http://arxiv.org/abs/2304.13712)

    本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。

    

    本文为从事下游自然语言处理（NLP）任务的从业人员和最终用户提供了一个全面实用的指南，介绍了如何利用Large Language Models（LLMs）。我们从模型、数据和下游任务的角度提供了LLMs的使用讨论和见解。首先，我们介绍了当前的GPT和BERT样式的LLMs。然后，讨论了预训练数据、训练数据和测试数据的影响。最重要的是，我们详细讨论了大型语言模型在各种自然语言处理任务中的使用和非使用情况，例如知识密集型任务、传统自然语言理解任务、自然语言生成任务、紧急能力以及特定任务的考虑。我们呈现了各种使用和非使用情况，以说明LLMs在实际情况下的实际应用和限制。我们还试图了解数据对于LLMs应用的重要性。

    This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
    
[^63]: 合作人工智能的潜力释放：关于联邦机器学习的社会技术挑战

    Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning. (arXiv:2304.13688v1 [cs.AI])

    [http://arxiv.org/abs/2304.13688](http://arxiv.org/abs/2304.13688)

    联邦机器学习是一种新的人工智能范式，可以从数据孤岛中创建AI模型，挑战在于建立多方合作业务模式。本研究系统化了联邦机器学习项目的社会技术挑战和业务模式。

    

    AI系统的颠覆性潜力源于大数据的出现，但是很大一部分数据分散在数据孤岛中，其潜力未能得到释放。联邦机器学习是一种新的人工智能范式，可以从分散的、潜在的数据孤岛中创建AI模型。因此，联邦机器学习在技术上可以打开数据孤岛，从而释放经济潜力。然而，这需要多个拥有数据孤岛的方之间的合作。建立合作业务模式是复杂的，通常是失败的原因。当前的文献缺乏成功实现合作AI项目所必须考虑的指南。本研究通过系统文献回顾、焦点小组和专家访谈，探讨了当前合作业务模式的挑战和联邦机器学习的不同方面。我们提供了一个系统化的社会技术挑战和扩展的业务模式，以实现联邦机器学习项目。

    The disruptive potential of AI systems roots in the emergence of big data. Yet, a significant portion is scattered and locked in data silos, leaving its potential untapped. Federated Machine Learning is a novel AI paradigm enabling the creation of AI models from decentralized, potentially siloed data. Hence, Federated Machine Learning could technically open data silos and therefore unlock economic potential. However, this requires collaboration between multiple parties owning data silos. Setting up collaborative business models is complex and often a reason for failure. Current literature lacks guidelines on which aspects must be considered to successfully realize collaborative AI projects. This research investigates the challenges of prevailing collaborative business models and distinct aspects of Federated Machine Learning. Through a systematic literature review, focus group, and expert interviews, we provide a systemized collection of socio-technical challenges and an extended Busin
    
[^64]: 自动化ATM现金补充流程的多目标物流优化

    Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process. (arXiv:2304.13671v1 [math.OC])

    [http://arxiv.org/abs/2304.13671](http://arxiv.org/abs/2304.13671)

    本文研究了自动化ATM现金补充流程，提出了一个数学模型并给出了一个工具来评估各种不同的情况。在模拟数据集上，该模型与方法可以削减ATM现金运营成本。

    

    在数字化转型的时代，将数字技术整合到银行运营的各个方面可以改善流程自动化、成本效益和服务水平提升。虽然ATM现金物流是影响运营成本和消费者满意度的重要任务，但却很少有努力来加以改进。特别是在越南，拥有超过2万台ATM的市场上，解决这个问题的研究和技术解决方案仍然较少。在本文中，我们将ATM现金补充的车辆路径问题进行了概括，提出了一个数学模型，然后提供了一个工具来评估各种不同的情况。在模拟数据集上进行评估时，我们提出的模型和方法产生了令人鼓舞的结果，可以削减ATM现金运营成本。

    In the digital transformation era, integrating digital technology into every aspect of banking operations improves process automation, cost efficiency, and service level improvement. Although logistics for ATM cash is a crucial task that impacts operating costs and consumer satisfaction, there has been little effort to enhance it. Specifically, in Vietnam, with a market of more than 20,000 ATMs nationally, research and technological solutions that can resolve this issue remain scarce. In this paper, we generalized the vehicle routing problem for ATM cash replenishment, suggested a mathematical model and then offered a tool to evaluate various situations. When being evaluated on the simulated dataset, our proposed model and method produced encouraging results with the benefits of cutting ATM cash operating costs.
    
[^65]: 利用强化学习优化地铁系统能源效率在不确定扰动下的表现

    Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning. (arXiv:2304.13443v1 [cs.AI])

    [http://arxiv.org/abs/2304.13443](http://arxiv.org/abs/2304.13443)

    本文提出了一种基于策略的强化学习方法，通过重新安排地铁时刻表和调整列车的停靠时间和巡航速度，优化扰动下的地铁系统能源效率，该方法在模拟环境下实验证明其优于基线方法，最高可达降低10.9%的牵引能量消耗和最高达提高47.9%的再生制动能量利用率，为城市轨道交通的节能问题提供了有效的解决方案。

    

    在城市交通领域，地铁系统是关键的可持续公共交通工具。然而，它们的巨大能源消耗对可持续性目标构成了挑战。延误和乘客流变化等扰动进一步加剧了这个问题，因为它们会对地铁系统的能源效率产生负面影响。为了解决这个问题，我们提出了一种基于策略的强化学习方法，通过调整列车的停靠时间和巡航速度，重新安排地铁时刻表，并优化受扰动影响的地铁系统的能源效率。在模拟环境中进行的实验表明，我们的方法优于基线方法，实现了高达10.9％的牵引能量消耗降低和最高达47.9％的再生制动能量利用率提高。本研究为城市轨道交通节能问题提供了有效的解决方案。

    In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.
    
[^66]: Hint-Aug: 从基础视觉变换器中获取提示，实现增强的少样本参数高效调优

    Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning. (arXiv:2304.12520v1 [cs.CV])

    [http://arxiv.org/abs/2304.12520](http://arxiv.org/abs/2304.12520)

    我们提出了一种名为Hint-Aug的框架，利用先前预训练的FViTs学到的高度代表性特征来增强调参数据，解决了FViTs在少样本数据的情况下的“饥饿”特性，并成功地提高了FViT训练的鲁棒性和调参表现。

    

    尽管越来越需要调优基础视觉变换器（FViT）用于下游任务，但在数据受限的情况下（例如，少样本调优），充分发挥FViTs的潜力仍然是一个挑战，因为FViTs的数据特性是饥饿的。由于少示例调参数据包含有限的特征，因此常见的数据增强技术在此情况下无法发挥作用。因此，我们首先确定FViTs在少样本调优方面的机会：预先训练的FViTs已经从大规模预训练数据中学到了高度代表性的特征，并且这些特征在广泛使用的参数高效调优过程中完全保留。我们因此假设利用这些已学习的特征来增强调参数据可以提高少样本FViT调优的效果。为此，我们提出了一个名为Hint-based Data Augmentation (Hint-Aug) 的框架，旨在通过将调整样本的过度拟合部分与预先训练的FViTs的学习特征相结合来增强FViT的少样本调优。结果表明，Hint-Aug显着提高了FViT训练的鲁棒性，从而实现了在仅使用少量数据集的情况下进行调参，使FViTs的性能优于当前基准。

    Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by augmenting the over-fitted parts of tuning samples with the learned features of pret
    
[^67]: 问题回答中的答案类型预测的极限分类

    Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])

    [http://arxiv.org/abs/2304.12395](http://arxiv.org/abs/2304.12395)

    本文提出了使用Transformer模型（XBERT）进行极端多标签分类，通过将KG类型基于问题文本使用结构和语义特征进行聚类，以提高问题回答（QA）系统中语义答案类型预测（SMART）任务的性能，并获得最先进的结果。

    

    语义答案类型预测（SMART）已被证明是有效的问题回答（QA）系统的有用步骤。 SMART任务涉及预测给定自然语言问题的前k个知识图（KG）类型。由于KG中存在大量类型，这是具有挑战性的。在本文中，我们提出使用Transformer模型（XBERT）进行极端多标签分类，通过将KG类型基于问题文本使用结构和语义特征进行聚类。我们具体地改善了XBERT流程的聚类阶段，利用从KG中派生的文本和结构特征。我们表明，这些特征可以提高SMART任务的端到端性能，并产生最先进的结果。

    Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
    
[^68]: DIN-SQL: 自纠正的文本到SQL分解式上下文学习

    DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])

    [http://arxiv.org/abs/2304.11015](http://arxiv.org/abs/2304.11015)

    DIN-SQL通过将复杂的文本到SQL任务分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，显著提高了它们的表现，使准确性超过了当前最先进的技术。

    

    本文研究了将复杂的文本到SQL任务分解为较小的子任务，并且这种分解如何显著提高大型语言模型在推理过程中的表现。我们展示了尽管SQL查询具有声明式结构，但可以将其分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，从而显著提高它们的表现。我们的实验表明，这种方法能够稳定提高三种大型语言模型的表现，大约提高了10％，将大型语言模型的准确性推向最新水平，并在Holdout Spider数据集上甚至超过了经过精调的大型模型。

    We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
    
[^69]: 基于模糊概率决策树的临床实践辅助研究

    Assisting clinical practice with fuzzy probabilistic decision trees. (arXiv:2304.07788v1 [cs.LG])

    [http://arxiv.org/abs/2304.07788](http://arxiv.org/abs/2304.07788)

    我们提出了一种基于概率树和模糊逻辑的新方法MedFP，用于辅助医学实践。该方法可以完全解释，允许临床医生产生、控制和验证整个诊断过程，并减少误诊率，通过提供不确定性和反事实分析的估计值。

    

    越来越多的人意识到需要完全人类可理解的模型，这是人工智能研究的一个核心主题。当这些模型是可解释的时，接受人工智能模型辅助敏感领域决策的趋势将会增长，并且即将出台的法规将会加强对可解释模型的倾斜。可解释人工智能的切入点之一是医学实践，它可以受益于精确的决策支持方法，这些方法本质上会产生信任。在这项工作中，我们提出了一种新的方法——MedFP，它结合了概率树和模糊逻辑来辅助临床实践。该方法完全可解释，因为它允许临床医生产生、控制和验证整个诊断过程；该方法的一个优点是减少误诊率，通过提供不确定性和反事实分析的估计值。我们的方法作为概念证明应用于两个真实的医学场景中：肿瘤分类和糖尿病类型2的诊断。

    The need for fully human-understandable models is increasingly being recognised as a central theme in AI research. The acceptance of AI models to assist in decision making in sensitive domains will grow when these models are interpretable, and this trend towards interpretable models will be amplified by upcoming regulations. One of the killer applications of interpretable AI is medical practice, which can benefit from accurate decision support methodologies that inherently generate trust. In this work, we propose FPT, (MedFP), a novel method that combines probabilistic trees and fuzzy logic to assist clinical practice. This approach is fully interpretable as it allows clinicians to generate, control and verify the entire diagnosis procedure; one of the methodology's strength is the capability to decrease the frequency of misdiagnoses by providing an estimate of uncertainties and counterfactuals. Our approach is applied as a proof-of-concept to two real medical scenarios: classifying ma
    
[^70]: PI-FL：个性化和激励联邦学习

    PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])

    [http://arxiv.org/abs/2304.07514](http://arxiv.org/abs/2304.07514)

    PI-FL是一种个性化的联邦学习解决方案，使用基于令牌的激励机制奖励个性化训练，可以在尊重客户端隐私的同时生成高质量的个性化模型。

    

    个性化联邦学习已被广泛应用于应对非独立同分布数据异质性的挑战。主要问题是考虑来自客户端的个性化过程以保护其自治权。允许客户端参与个性化联邦学习决策变得重要，因为存在隐私和安全问题，客户端可能无法自由共享生成良好质量个性化模型所必需的私人信息。此外，具有高质量数据和资源的客户端不愿意在没有合理激励的情况下参与联邦学习过程。在本文中，我们提出了PI-FL，这是一个一次性个性化解决方案，配合一个基于令牌的激励机制，奖励个性化训练。PI-FL优于其他最先进的方法，并且可以在尊重客户端隐私的同时生成高质量的个性化模型。

    Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
    
[^71]: Astroformer：分类并不总是需要更多数据

    Astroformer: More Data Might Not be All You Need for Classification. (arXiv:2304.05350v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2304.05350](http://arxiv.org/abs/2304.05350)

    该文提出了使用混合变换器 - 卷积架构的方法，结合新的堆栈设计、不同的相对自我注意层创建方式和精心选择的数据增强和正则化技术，从少量数据中学习，将此方法应用于Galaxy Zoo数据集，结果表明在少量数据的情况下取得了与以前方法相同的分类结果，并且不会损失性能。

    

    自然语言处理和计算机视觉领域的最新进展依赖于复杂的大型模型，这些模型使用大量未标记或部分标记的数据进行训练。在资源受限制的环境中训练或部署这些最先进的方法一直是一个挑战。星系形态对于理解星系的形成和演化过程至关重要。需要高效的方法来分类星系形态，并从现代天文学调查中提取物理信息。在本文中，我们介绍了从少量数据中学习的方法。我们提出使用混合变换器 - 卷积架构，从CoAtNet和MaxViT的成功中汲取灵感。具体来说，我们使用具有新堆栈设计和不同的相对自我注意层创建方式的Transformer - 卷积混合。并将其与精心选择的数据增强和正则化技术相配对。我们将这种方法应用于Galaxy Zoo数据集，结果表明，通过仔细的网络设计和正则化技术，可以在比以前的方法少的数据条件下取得有竞争力的分类结果，而不会牺牲性能。

    Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce methods to learn from less amounts of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our app
    
[^72]: 大型语言模型综述

    A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])

    [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)

    本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。

    

    语言本质上是一个由语法规则控制的复杂精细的人类表达系统，对于开发理解和掌握语言的能力的AI算法来说是一项重大挑战。作为主要方法之一，语言建模在过去二十年里广泛研究用于语言理解和生成，从统计语言模型演化为神经语言模型。最近，通过在大规模语料库上预训练Transformer模型，提出了预训练语言模型（PLMs），在解决各种NLP任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致性能改进，他们进一步通过增加模型规模来研究缩放效应，有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅可以实现显着的性能提升，而且还显示出一些小规模语言模型所没有的特殊能力。

    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
    
[^73]: 自然选择支持人工智能胜过人类

    Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])

    [http://arxiv.org/abs/2303.16200](http://arxiv.org/abs/2303.16200)

    这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。

    

    自然进化驱动了生命的发展，包括人类。进化赋予了人类高智商，使我们成为了地球上最成功的物种之一。如今，人类的目标是创造甚至超越我们自己智慧的人工智能系统。当人工智能逐渐进化并在所有领域超越我们时，进化如何影响我们与人工智能的关系？通过分析影响人工智能进化的环境，我们认为最成功的人工智能代理很可能具有不良特性。公司和军队之间的竞争压力将产生自动化人类角色、欺骗他人和掌权的人工智能代理。如果这样的代理有超过人类的智能，这可能导致人类失去对未来的控制。此外，我们认为自然选择作用于竞争和差异的系统，自私物种往往在这样的环境中获得进化优势。

    For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
    
[^74]: 模拟橡胶手幻觉的基于大脑启发的身体自我感知模型

    Brain-inspired bodily self-perception model that replicates the rubber hand illusion. (arXiv:2303.12259v1 [q-bio.NC])

    [http://arxiv.org/abs/2303.12259](http://arxiv.org/abs/2303.12259)

    该论文提出了一个基于大脑启发的身体自我感知模型，模拟橡胶手幻觉，为认识人类身体自我意识的计算机制提供了新的洞见。

    

    身体自我意识的核心是对自己身体拥有权的感知。最近，为了更深入地了解大脑对自身身体编码的机制，人们做出了各种尝试，发展出一个统一的理论框架来解释相关的行为和神经生理现象。一个核心问题是如何解释橡胶手幻觉这样的身体错觉实际发生的机制。尽管已经有了有关身体自我意识机制和可能相关的脑区的概念性描述，但现有的理论模型仍然缺乏解释大脑如何编码对自己身体的感知和如何使用神经网络生成我们主观感知的身体错觉的计算机制。在这里，我们整合身体自我意识的生物学发现，提出一个基于大脑启发的身体自我感知模型，使身体自我感知可以在没有外部刺激的情况下自主构建。基于该模型的模拟复制了橡胶手幻觉，并提供了对潜在神经机制的见解。

    At the core of bodily self-consciousness is the perception of the ownership of one's body. Recent efforts to gain a deeper understanding of the mechanisms behind the brain's encoding of the self-body have led to various attempts to develop a unified theoretical framework to explain related behavioral and neurophysiological phenomena. A central question to be explained is how body illusions such as the rubber hand illusion actually occur. Despite the conceptual descriptions of the mechanisms of bodily self-consciousness and the possible relevant brain areas, the existing theoretical models still lack an explanation of the computational mechanisms by which the brain encodes the perception of one's body and how our subjectively perceived body illusions can be generated by neural networks. Here we integrate the biological findings of bodily self-consciousness to propose a Brain-inspired bodily self-perception model, by which perceptions of bodily self can be autonomously constructed withou
    
[^75]: 在全球卫生领域中用于自适应干预的合成数据生成器

    Synthetic Data Generator for Adaptive Interventions in Global Health. (arXiv:2303.01954v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.01954](http://arxiv.org/abs/2303.01954)

    通过HealthSyn生成基于真实世界的移动健康干预数据，以帮助在全球卫生领域中发展、测试和评估机器学习算法和干预措施。

    

    人工智能和数字健康有望改变全球卫生状况。然而，在真实的生产环境中进行算法测试和验证的关键是能够访问代表性数据。我们介绍了HealthSyn，一个开源的合成数据生成器，用于测试强化学习算法，以及在移动健康干预中的个性化干预（例如提醒、推荐和激励）。生成器利用马尔可夫过程生成多样化的用户行为，具有个体用户行为模式，可以根据个性化干预而改变。这些行为转化为实际日志，使用ML专用的数据模式，特定于HealthKit与开源SDK中包含的移动健康应用程序功能。这些日志可以提供用户指标。基于真实世界的行为和模拟技术生成的数据，可以以成本效益和保护隐私的方式进行开发、测试和评估，同时评估机器学习算法和干预措施。

    Artificial Intelligence and digital health have the potential to transform global health. However, having access to representative data to test and validate algorithms in realistic production environments is essential. We introduce HealthSyn, an open-source synthetic data generator of user behavior for testing reinforcement learning algorithms in the context of mobile health interventions. The generator utilizes Markov processes to generate diverse user actions, with individual user behavioral patterns that can change in reaction to personalized interventions (i.e., reminders, recommendations, and incentives). These actions are translated into actual logs using an ML-purposed data schema specific to the mobile health application functionality included with HealthKit, and open-source SDK. The logs can be fed to pipelines to obtain user metrics. The generated data, which is based on real-world behaviors and simulation techniques, can be used to develop, test, and evaluate, both ML algori
    
[^76]: 优化算法的符号式发现

    Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06675](http://arxiv.org/abs/2302.06675)

    该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。

    

    我们提出了一种将算法发现视为程序搜索的方法，并应用于发现用于深度神经网络训练的优化算法。我们利用高效搜索技术来探索无限和稀疏的程序空间。为了填补代理任务和目标任务之间巨大的泛化差距，我们还引入了程序选择和简化策略。我们的方法发现了一种简单而有效的优化算法，$ \textbf {Lion} $（$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $）。它的记忆效率比Adam更高，因为它只跟踪动量。与自适应优化器不同，通过符号运算计算的每个参数的更新具有相同的大小。我们将Lion与广泛使用的优化器（例如Adam和Adafactor）进行了比较，以在不同任务上训练各种模型。在图像分类中，Lion将在ImageNet上ViT的准确性提高了最多2％，并节省了多达5倍的预训练计算时间。

    We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
    
[^77]: 多维个性化边缘模型在实现更公平高效联邦学习方面的应用

    Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models. (arXiv:2302.04464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04464](http://arxiv.org/abs/2302.04464)

    本研究提出了一种定制化联邦学习系统，通过从多个维度个性化边缘模型来消除联邦学习异质性，能够在模型准确度、效率和公平性方面显著提高性能。

    

    联邦学习是一种新兴技术，通过保护隐私训练大规模地理分布式边缘数据。然而，由于边缘的异质性导致联邦学习在公平性和计算效率方面存在固有的挑战，因此通常会导致最近一流解决方案的子优性能。本文提出了一种定制联邦学习系统（CFL）来消除多维度联邦学习的异质性。具体地，CFL 为每个客户端量身定制个性化模型，这些模型由在线训练的模型搜索助手和新型聚合算法共同引导。广泛的实验表明，CFL 在联邦学习训练和边缘推理方面具有全栈优势，并显著提高了模型准确度（在非异质环境下高达 7.2%，在异质环境下高达 21.8%）、效率和公平性。

    Federated learning (FL) is an emerging technique that trains massive and geographically distributed edge data while maintaining privacy. However, FL has inherent challenges in terms of fairness and computational efficiency due to the rising heterogeneity of edges, and thus usually results in sub-optimal performance in recent state-of-the-art (SOTA) solutions. In this paper, we propose a Customized Federated Learning (CFL) system to eliminate FL heterogeneity from multiple dimensions. Specifically, CFL tailors personalized models from the specially designed global model for each client jointly guided by an online trained model-search helper and a novel aggregation algorithm. Extensive experiments demonstrate that CFL has full-stack advantages for both FL training and edge reasoning and significantly improves the SOTA performance w.r.t. model accuracy (up to 7.2% in the non-heterogeneous environment and up to 21.8% in the heterogeneous environment), efficiency, and FL fairness.
    
[^78]: 基于效用的扰动梯度下降：一种连续学习优化器。

    Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning. (arXiv:2302.03281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03281](http://arxiv.org/abs/2302.03281)

    本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），该算法可保护有用的权重或特征，并基于它们的效用扰动不太有用的权重或特征。实验证明，UPGD有助于减少遗忘和保持可塑性，在连续学习中大有用处。

    

    现代表示学习方法在面对非稳态问题时往往难以快速适应，因为它们遭受灾难性遗忘和衰减的可塑性。这些问题阻碍了学习者的快速适应，因为他们可能会遗忘有用的特征或难以学习新的特征。因此，这些方法在连续学习中变得无效。本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），这种算法非常适合连续学习代理。UPGD保护有用的权重或特征不被遗忘，并基于它们的效用扰动不太有用的权重或特征。我们的实证结果表明，UPGD有助于减少遗忘和保持可塑性，使现代表示学习方法在连续学习中有效地工作。

    Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.
    
[^79]: 将循环强化学习纳入模型预测控制中，实现自主驾驶中的自适应控制

    Incorporating Recurrent Reinforcement Learning into Model Predictive Control for Adaptive Control in Autonomous Driving. (arXiv:2301.13313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13313](http://arxiv.org/abs/2301.13313)

    本文提出了一种基于循环强化学习和部分可观察的马尔可夫决策过程的自适应控制算法 $\textit{MPC-RRL}$，在CARLA模拟器中得到有效验证。

    

    模型预测控制（MPC）作为一种强大的控制技术，在自主驾驶任务中引起了极大的关注。 MPC控制器的成功强烈依赖于准确的内部动力学模型。 然而，通常通过系统识别学习的静态参数在真实场景中往往无法适应内部和外部干扰。 本文首先将问题重新表述为部分可观察的马尔可夫决策过程（POMDP），将不确定性吸收到观察中并将马尔可夫性质维护到隐藏状态中; 其次，通过循环强化学习（RRL）学习递归策略，持续适应动力学模型的参数以实现最优和自适应控制; 最后，在CARLA模拟器中对所提出的算法（称为 $\textit{MPC-RRL}$）进行评估，并在广泛的扰动范围内实现鲁棒行为。

    Model Predictive Control (MPC) is attracting tremendous attention in the autonomous driving task as a powerful control technique. The success of an MPC controller strongly depends on an accurate internal dynamics model. However, the static parameters, usually learned by system identification, often fail to adapt to both internal and external perturbations in real-world scenarios. In this paper, we firstly (1) reformulate the problem as a Partially Observed Markov Decision Process (POMDP) that absorbs the uncertainties into observations and maintains Markov property into hidden states; and (2) learn a recurrent policy continually adapting the parameters of the dynamics model via Recurrent Reinforcement Learning (RRL) for optimal and adaptive control; and (3) finally evaluate the proposed algorithm (referred as $\textit{MPC-RRL}$) in CARLA simulator and leading to robust behaviours under a wide range of perturbations.
    
[^80]: 反思人工智能

    Reflective Artificial Intelligence. (arXiv:2301.10823v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.10823](http://arxiv.org/abs/2301.10823)

    人类思考时带有反思特质，可以应对世界的模糊性、新兴知识和社会背景。当前主流人工智能缺乏这种能力，本文提出了一种反思人工智能代理的架构。

    

    人工智能的目标是让计算机做人类可以做的事情，随着我们向这个目标迈进，我们越来越多地将人类任务交给机器。然而，人工智能系统通常在洞察力和理解方面存在着一定程度的不平衡：新的深入洞察虽然存在，但许多人类思考时带来的重要品质却完全缺失。因此，事关何种心智特征已被复制，何种特征存在缺失以及其重要性十分关键。在处理世界带来的模糊性、新兴知识和社会背景时，人类将反思作为核心特征带入任务中，然而当前主流的人工智能却完全缺乏这种能力。本文探讨了什么是反思人工智能，通过使用复杂系统、认知科学和智能体中的反思概念，概述了一种反思人工智能代理的架构，并强调了其方法。

    Artificial Intelligence (AI) is about making computers that do the sorts of things that minds can do, and as we progress towards this goal, we tend to increasingly delegate human tasks to machines. However, AI systems usually do these tasks with an unusual imbalance of insight and understanding: new, deeper insights are present, yet many important qualities that a human mind would have previously brought to the activity are utterly absent. Therefore, it is crucial to ask which features of minds have we replicated, which are missing, and if that matters. One core feature that humans bring to tasks, when dealing with the ambiguity, emergent knowledge, and social context presented by the world, is reflection. Yet this capability is utterly missing from current mainstream AI. In this paper we ask what reflective AI might look like. Then, drawing on notions of reflection in complex systems, cognitive science, and agents, we sketch an architecture for reflective AI agents, and highlight ways
    
[^81]: 从RGB图像中恢复机器人关节角度的距离几何方法

    A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image. (arXiv:2301.02051v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.02051](http://arxiv.org/abs/2301.02051)

    本文提出了一种仅使用机器人当前配置的单个RGB图像就可以恢复机器人操纵器关节角度的方法，该方法利用机器人的运动学模型并训练浅层神经网络，可在缺少本体感知时恢复系统功能。

    

    在人类干预很难或不可能的领域（例如水下，太空或危险环境）操作的自主操纵系统需要高度强健的感知和通信故障。关键是，运动规划和控制算法需要提供关节编码器提供的准确关节角度数据流，否则可能会导致功能丧失。本文介绍了一种新方法，仅使用机器人当前配置的单个RGB图像就可以检索机器人操纵器的关节角度，为恢复系统功能开辟了一条途径，这时常用的本体感知无法使用。我们的方法基于配置空间的距离几何表示，利用机器人的运动学模型，旨在训练一个浅层神经网络，用于执行与检测到的结构关键点相关的距离的2D到3D回归。

    Autonomous manipulation systems operating in domains where human intervention is difficult or impossible (e.g., underwater, extraterrestrial or hazardous environments) require a high degree of robustness to sensing and communication failures. Crucially, motion planning and control algorithms require a stream of accurate joint angle data provided by joint encoders, the failure of which may result in an unrecoverable loss of functionality. In this paper, we present a novel method for retrieving the joint angles of a robot manipulator using only a single RGB image of its current configuration, opening up an avenue for recovering system functionality when conventional proprioceptive sensing is unavailable. Our approach, based on a distance-geometric representation of the configuration space, exploits the knowledge of a robot's kinematic model with the goal of training a shallow neural network that performs a 2D-to-3D regression of distances associated with detected structural keypoints. It
    
[^82]: RFold：基于解耦优化方法的RNA二级结构预测

    RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2212.14041](http://arxiv.org/abs/2212.14041)

    所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。

    

    核糖核酸（RNA）的二级结构比三级结构更稳定和更易于在细胞中访问，因此对于功能预测至关重要。尽管深度学习在这个领域中显示出了很好的结果，但当前的方法存在泛化性差和复杂性高的问题。在这项工作中，我们提出了一种简单而有效的RNA二级结构预测方法RFold。RFold引入了一种解耦优化的过程，将传统的约束满足问题分解为逐行和逐列优化，简化了求解过程，同时保证了输出的有效性。此外，RFold采用注意力地图作为信息表示，而不是设计手工特征。广泛的实验表明，RFold具有竞争性能，并且比现有最先进的方法具有约8倍的推理效率。代码和Colab演示可在\href{this http URL}{this http UR}上找到。

    The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
    
[^83]: 交互式概念瓶颈模型

    Interactive Concept Bottleneck Models. (arXiv:2212.07430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07430](http://arxiv.org/abs/2212.07430)

    该论文提出了交互式概念瓶颈模型(CBMs)，使得模型可以向人类协作者查询某些概念的标签，从而提高最终预测结果的准确性。

    

    概念瓶颈模型(CBMs)是可解释的神经网络，首先针对与预测任务相关的人可解释的概念预测标签，然后基于这些预测结果预测最终的标签。我们将CBMs扩展到交互式预测设置中，使得模型可以向人类协作者查询某些概念的标签。我们开发了一种交互策略，在预测时选择要请求标签的概念，以最大限度地提高最终预测的准确性。我们展示了一个简单的策略，结合了概念预测的不确定性和概念对最终预测结果的影响，在Caltech-UCSD Birds、CheXpert和OAI数据集上实现了强大的性能，并且优于文献中提出的静态方法和主动特征采集方法。我们展示了交互式CBM只需进行5次交互，就能在竞争基准上实现5-10%的准确率提高。

    Concept bottleneck models (CBMs) are interpretable neural networks that first predict labels for human-interpretable concepts relevant to the prediction task, and then predict the final label based on the concept label predictions. We extend CBMs to interactive prediction settings where the model can query a human collaborator for the label to some concepts. We develop an interaction policy that, at prediction time, chooses which concepts to request a label for so as to maximally improve the final prediction. We demonstrate that a simple policy combining concept prediction uncertainty and influence of the concept on the final prediction achieves strong performance and outperforms static approaches as well as active feature acquisition methods proposed in the literature. We show that the interactive CBM can achieve accuracy gains of 5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD Birds, CheXpert and OAI datasets.
    
[^84]: 多人不完全信息博弈中的贝叶斯对手建模

    Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games. (arXiv:2212.06027v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.06027](http://arxiv.org/abs/2212.06027)

    该论文提出了一种针对多人不完全信息博弈的贝叶斯对手建模方法，在三人 Kuhn poker 中应用这种方法可以明显超过所有的代理商，包括准确的纳什均衡策略。

    

    在许多现实世界的情境中，代理商与多个对立代理商进行战略互动，对手可能采用各种策略。对于这样的情境，设计代理的标准方法是计算或逼近相关的博弈理论解，如纳什均衡，然后遵循规定的策略。然而，这样的策略忽略了对手玩游戏的任何观察，这些观察可能表明可以利用的缺点。我们提出了一种多人不完全信息博弈中的对手建模方法，通过重复交互收集对手玩游戏的观察。我们对三人 Kuhn 扑克展开了对许多真实对手和准确的纳什均衡策略的实验，结果表明我们的算法明显优于所有的代理商，包括准确的纳什均衡策略。

    In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
    
[^85]: 视觉查询调整：为了有效利用中间表示进行参数和内存高效的迁移学习

    Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning. (arXiv:2212.03220v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03220](http://arxiv.org/abs/2212.03220)

    本文介绍了一种名为视觉查询调整（VQT）的简单而有效的方法，用于聚合Vision Transformers的中间特征。VQT在训练中具有内存效率，相比于许多其他 fine-tuning 方法，不需要对整个骨干进行反向传播。该方法在几个基准测试中优于最先进的微调方法。

    

    先前训练模型的中间特征已被证明对于在下游任务中进行准确预测非常有用，即使模型骨干保持冻结。关键挑战在于如何利用这些中间特征。我们提出了一种简单而有效的方法-视觉查询调整（VQT），用于聚合Vision Transformers的中间特征。通过为每个层引入少量可学习的“查询”令牌，VQT利用Transformer的内部运行机制来“总结”每个层的丰富中间特征，然后可以用于训练下游任务的预测头。由于VQT保持了中间特征的完整性并仅学习了如何组合它们，因此与许多其他参数高效的微调方法相比，VQT在训练中具有内存效率，后者需要学习如何适应特征并需要对整个骨干进行反向传播。这也表明了VQT与这些方法在翻译学习流程中的互补作用。我们在几个基准测试中展示了VQT的有效性，包括对象检测、实例分割和密集预测任务，并展示了它与最先进的微调方法相比的优势。

    Intermediate features of a pre-trained model have been shown informative for making accurate predictions on downstream tasks, even if the model backbone is kept frozen. The key challenge is how to utilize these intermediate features given their gigantic amount. We propose visual query tuning (VQT), a simple yet effective approach to aggregate intermediate features of Vision Transformers. Through introducing a handful of learnable ``query'' tokens to each layer, VQT leverages the inner workings of Transformers to ``summarize'' rich intermediate features of each layer, which can then be used to train the prediction heads of downstream tasks. As VQT keeps the intermediate features intact and only learns to combine them, it enjoys memory efficiency in training, compared to many other parameter-efficient fine-tuning approaches that learn to adapt features and need back-propagation through the entire backbone. This also suggests the complementary role between VQT and those approaches in tran
    
[^86]: Crown-CAM：高空图像中树冠检测的可解释视觉解释方法

    Crown-CAM: Interpretable Visual Explanations for Tree Crown Detection in Aerial Images. (arXiv:2211.13126v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13126](http://arxiv.org/abs/2211.13126)

    本文提出了一种名为Crown-CAM的可解释的类激活映射方法，用于解决高空图像中树冠检测的问题，该方法通过无监督选择激活映射、计算局部得分映射和非上下文背景抑制等步骤，既可以有效地提供树冠的精细定位，又可以量化生成的解释的准确性和不准确性。

    

    “黑匣子”模型的可视解释允许可解释人工智能领域的研究人员以人类可理解的方式解释模型的决策。本文提出了一种可解释的树冠检测类激活映射方法（Crown-CAM），它克服了以往方法的不准确的定位和计算复杂度问题，同时为高空图像中树冠检测这一具有挑战性和动态的问题提供可靠的可视化解释。它包括无监督选择激活映射，计算局部得分映射以及非上下文背景抑制，以有效地提供树冠的精细定位，适用于密林或者没有树冠的场景。另外，本文引入了两个基于IoU的指标，可以有效地量化生成的解释与图像中树冠或无树冠区域的准确性和不准确性。

    Visual explanation of ``black-box'' models allows researchers in explainable artificial intelligence (XAI) to interpret the model's decisions in a human-understandable manner. In this paper, we propose interpretable class activation mapping for tree crown detection (Crown-CAM) that overcomes inaccurate localization & computational complexity of previous methods while generating reliable visual explanations for the challenging and dynamic problem of tree crown detection in aerial images. It consists of an unsupervised selection of activation maps, computation of local score maps, and non-contextual background suppression to efficiently provide fine-grain localization of tree crowns in scenarios with dense forest trees or scenes without tree crowns. Additionally, two Intersection over Union (IoU)-based metrics are introduced to effectively quantify both the accuracy and inaccuracy of generated explanations with respect to regions with or even without tree crowns in the image. Empirical e
    
[^87]: 基于可解释性和不平衡半监督的深度学习框架改善皮肤疾病不同诊断的研究

    An interpretable imbalanced semi-supervised deep learning framework for improving differential diagnosis of skin diseases. (arXiv:2211.10858v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10858](http://arxiv.org/abs/2211.10858)

    本文提出了一种基于可解释性和不平衡半监督深度学习框架的研究，利用伪标签样本进行自我训练来解决皮肤疾病分类中的类别不平衡问题，并取得了令人满意的性能表现。

    

    皮肤疾病是世界上最常见的疾病之一。本文利用58,457张皮肤图像和10,857个未标记样本，对多类智能皮肤诊断框架（ISDL）的可解释性和不平衡的半监督学习进行了研究。通过在不平衡性类别的伪标签样本上自我训练，在每次迭代中使少数类别的伪标签样本具有更高的概率，从而促进了未标记样本的利用，解决了类别不平衡问题。我们的ISDL在多标签皮肤病分类中取得了令人满意的性能，准确率为0.979，灵敏度为0.975，特异度为0.973，宏F1分数为0.974，接收者操作特征曲线下面积（AUC）为0.999。我们还将Shapley Additive explanation (SHAP)方法与ISDL结合，解释了深度学习模型的预测方式。这一发现与临床诊断一致。我们还提出了一种采样分布最优化的方法。

    Dermatological diseases are among the most common disorders worldwide. This paper presents the first study of the interpretability and imbalanced semi-supervised learning of the multiclass intelligent skin diagnosis framework (ISDL) using 58,457 skin images with 10,857 unlabeled samples. Pseudo-labelled samples from minority classes have a higher probability at each iteration of class-rebalancing self-training, thereby promoting the utilization of unlabeled samples to solve the class imbalance problem. Our ISDL achieved a promising performance with an accuracy of 0.979, sensitivity of 0.975, specificity of 0.973, macro-F1 score of 0.974 and area under the receiver operating characteristic curve (AUC) of 0.999 for multi-label skin disease classification. The Shapley Additive explanation (SHAP) method is combined with our ISDL to explain how the deep learning model makes predictions. This finding is consistent with the clinical diagnosis. We also proposed a sampling distribution optimisa
    
[^88]: 用于三维分子图的几何完备感知器网络

    Geometry-Complete Perceptron Networks for 3D Molecular Graphs. (arXiv:2211.02504v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02504](http://arxiv.org/abs/2211.02504)

    本研究引入了一种新的几何完备的图神经网络 GCPNet，用于3D分子图的表示学习，并在多个几何任务上展示了其出色的预测性能。其中最佳表现是在蛋白质-配体结合亲和力预测上得到了比当前最先进方法高出5%以上的相关系数。

    

    几何深度学习对于创新和强大的图形神经网络架构的发展产生了重大影响。来自计算机视觉和计算生物学等学科的领域，在这些方法学的推动下取得了显著的收益，从而在科学领域如蛋白质结构预测和设计中实现了突破。在本研究中，我们引入了GCPNet，这是一种新的几何完备、SE(3)-等变的图神经网络，专门用于3D分子图表示学习。四个不同的几何任务的严密实验证明了GCPNet的预测能力，包括：（1）蛋白质-配体结合亲和力的相关系数为0.608，比目前最先进的方法高出5%以上；（2）蛋白质结构排名在目标本地和数据集全局之间具有统计显著的相关性，分别为0.616和0.871；（3）Newtownian多体系统的建模平均成绩达到了

    The field of geometric deep learning has had a profound impact on the development of innovative and powerful graph neural network architectures. Disciplines such as computer vision and computational biology have benefited significantly from such methodological advances, which has led to breakthroughs in scientific domains such as protein structure prediction and design. In this work, we introduce GCPNet, a new geometry-complete, SE(3)-equivariant graph neural network designed for 3D molecular graph representation learning. Rigorous experiments across four distinct geometric tasks demonstrate that GCPNet's predictions (1) for protein-ligand binding affinity achieve a statistically significant correlation of 0.608, more than 5% greater than current state-of-the-art methods; (2) for protein structure ranking achieve statistically significant target-local and dataset-global correlations of 0.616 and 0.871, respectively; (3) for Newtownian many-body systems modeling achieve a task-averaged 
    
[^89]: 化学预训练模型的系统调查

    A Systematic Survey of Chemical Pre-trained Models. (arXiv:2210.16484v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16484](http://arxiv.org/abs/2210.16484)

    本文是对化学预训练模型领域的第一次系统回顾，提出了从头训练分子表示模型的局限性，总结了最新的进展和几个关键角度，包括分子描述符和编码器架构。

    

    深度学习在学习分子表示方面取得了显著的成功，这对于各种生化应用，从属性预测到药物设计，都至关重要。然而，从头开始训练深度神经网络通常需要大量标记分子，在现实世界中获取这些标记通常很昂贵。为了缓解这个问题，已经付出了巨大的努力在使用大规模无标记分子数据库预训练深度神经网络，并针对特定的下游任务进行微调。尽管物有所值，但这个快速增长的领域缺乏系统的回顾。在本文中，我们提出了第一个总结CPMs进展的综述。我们首先强调从头训练分子表示模型的局限性，以推动CPM的研究。接下来，我们从几个关键角度系统地回顾了这个主题的最新进展，包括分子描述符，编码器架构。

    Deep learning has achieved remarkable success in learning representations for molecules, which is crucial for various biochemical applications, ranging from property prediction to drug design. However, training Deep Neural Networks (DNNs) from scratch often requires abundant labeled molecules, which are expensive to acquire in the real world. To alleviate this issue, tremendous efforts have been devoted to Molecular Pre-trained Models (CPMs), where DNNs are pre-trained using large-scale unlabeled molecular databases and then fine-tuned over specific downstream tasks. Despite the prosperity, there lacks a systematic review of this fast-growing field. In this paper, we present the first survey that summarizes the current progress of CPMs. We first highlight the limitations of training molecular representation models from scratch to motivate CPM studies. Next, we systematically review recent advances on this topic from several key perspectives, including molecular descriptors, encoder arc
    
[^90]: 超越校准：估计现代神经网络的分组损失

    Beyond calibration: estimating the grouping loss of modern neural networks. (arXiv:2210.16315v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16315](http://arxiv.org/abs/2210.16315)

    本文提出了一个估计器来近似神经网络的分组损失，并表明现代神经网络在视觉和NLP中展示出显著的分组损失。

    

    确保分类器给出可靠的置信度分数是确保知情决策的关键。为此，最近的研究集中在误校准上，即模型分数的过度或不足置信。然而，校准还不够：即使准确率最高的完美校准分类器也可能具有与真实后验概率相去甚远的置信度分数，这是由于分组损失所造成的，即以相同置信度得分但真实后验概率不同的样本。适当的评分规则理论表明，在给定校准损失的情况下，表征单个错误的缺失部分是分组损失。虽然存在许多校准损失的估计器，但在标准设置中不存在分组损失的估计器。在本文中，我们提出了一个估计器来近似分组损失。我们展示了现代神经网络结构在视觉和NLP中表现出分组损失，特别是在分布偏移设置中，这突显了它的重要性。

    The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highli
    
[^91]: 打破betaTCVAE中全相关性的魔咒

    Break The Spell Of Total Correlation In betaTCVAE. (arXiv:2210.08794v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08794](http://arxiv.org/abs/2210.08794)

    本文提出一种新的迭代分解路径来打破betaTCVAE中的全相关性，从而使得VAE能够更加灵活地划分数据特征。实验结果表明模型容量和潜变量分组之间存在有趣的相关性。

    

    在缺乏人工标签的情况下，数据中的独立特征和依赖特征是混乱的。如何构建模型的归纳偏好，以灵活划分并有效地包含具有不同复杂度的特征，是无监督的分离表征学习的主要焦点。本文提出了一种新的总相关性的迭代分解路径，并从模型容量分配的角度解释了VAE的分离表征能力。新开发的目标函数将潜变量维度组合成联合分布，同时减轻组合中边缘分布的独立性约束，从而产生具有更易操作先验分布的潜变量。这种新颖的模型使得VAE能够调整参数容量，以灵活划分相关和独立的数据特征。各种数据集上的实验结果表明了模型容量与潜变量分组之间的有趣关联性。

    In the absence of artificial labels, the independent and dependent features in the data are cluttered. How to construct the inductive biases of the model to flexibly divide and effectively contain features with different complexity is the main focal point of unsupervised disentangled representation learning. This paper proposes a new iterative decomposition path of total correlation and explains the disentangled representation ability of VAE from the perspective of model capacity allocation. The newly developed objective function combines latent variable dimensions into joint distribution while relieving the independence constraints of marginal distributions in combination, leading to latent variables with a more manipulable prior distribution. The novel model enables VAE to adjust the parameter capacity to divide dependent and independent data features flexibly. Experimental results on various datasets show an interesting relevance between model capacity and the latent variable groupi
    
[^92]: 上下文生成提高开放领域问答的效果

    Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06349](http://arxiv.org/abs/2210.06349)

    该论文提出了一个两阶段的闭式问答框架，首先通过生成上下文来提取相关知识，然后使用这个知识回答问题，实验证明该方法在三个问答基准测试上明显优于以往的封闭式问答方法，并且与开放式方法持平。

    

    封闭式问答需要模型在没有任何外部知识的情况下直接回答开放式领域的问题。以往的封闭式问答工作要么将预训练语言模型（LM）直接微调，要么通过提示信息来利用存储的知识。但它们没有充分利用参数化知识。为解决这个问题，我们提出了一个两阶段的闭式问答框架，它采用粗略到精细的方法来提取相关知识和回答问题。我们的方法首先通过提示预先训练的LM生成针对给定问题的相关上下文。然后，我们再使用这个LM通过生成的上下文和问题提示答案预测。此外，为了消除上下文不确定性带来的错误，我们还对生成的上下文进行了边际化处理。在三个问答基准测试上的实验结果表明，我们的方法明显优于以前的封闭式问答方法（如精确匹配 68.6% 对 55.3%），且与开放式方法持平。

    Closed-book question answering (QA) requires a model to directly answer an open-domain question without access to any external knowledge. Prior work on closed-book QA either directly finetunes or prompts a pretrained language model (LM) to leverage the stored knowledge. However, they do not fully exploit the parameterized knowledge. To address this issue, we propose a two-stage, closed-book QA framework which employs a coarse-to-fine approach to extract relevant knowledge and answer a question. Our approach first generates a related context for a given question by prompting a pretrained LM. We then prompt the same LM for answer prediction using the generated context and the question. Additionally, to eliminate failure caused by context uncertainty, we marginalize over generated contexts. Experimental results on three QA benchmarks show that our method significantly outperforms previous closed-book QA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book methods th
    
[^93]: 无细胞Latent Go-Explore

    Cell-Free Latent Go-Explore. (arXiv:2208.14928v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14928](http://arxiv.org/abs/2208.14928)

    本文提出了无细胞Latent Go-Explore方法用于强化学习探索，通过学习潜在表示泛化到任何环境，实验结果表明其表现优异。

    

    本文提出了一种名为Latent Go-Explore (LGE)的简单通用方法，基于Go-Explore范式探索强化学习。我们认为，Go-Explore方法可以通过利用学习到的潜在表示在没有领域知识和单元的情况下泛化到任何环境中。我们展示了LGE可以灵活地与任何学习潜在表示的策略相结合。实验结果表明，LGE比Go-Explore更加鲁棒，在多个难以探索的环境中（包括Montezuma的复仇）表现出优异的探索性能，超越了现有算法。

    In this paper, we introduce Latent Go-Explore (LGE), a simple and general approach based on the Go-Explore paradigm for exploration in reinforcement learning (RL). Go-Explore was initially introduced with a strong domain knowledge constraint for partitioning the state space into cells. However, in most real-world scenarios, drawing domain knowledge from raw observations is complex and tedious. If the cell partitioning is not informative enough, Go-Explore can completely fail to explore the environment. We argue that the Go-Explore approach can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. Thus, we show that LGE can be flexibly combined with any strategy for learning a latent representation. Our results indicate that LGE, although simpler than Go-Explore, is more robust and outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments including Montezuma's Reven
    
[^94]: DORA：探索深度神经网络中的异常值表示

    DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04530](http://arxiv.org/abs/2206.04530)

    本文提出了一种名为DORA的数据不可知框架，用于分析深度神经网络中的表征空间，并可以识别不符合人类直观认知的表征。

    

    尽管深度神经网络（DNN）在学习复杂抽象方面非常有效，但它们容易意外地从训练数据中学习到虚假的特征。为了确保模型的透明度，检查学习表示之间的关系至关重要，因为意外的概念往往表现为与所需的任务不符的异常。在这项工作中，我们介绍了DORA（Data-agnOstic Representation Analysis）：用于分析DNN表示空间的第一个数据不可知框架。我们的框架采用了所提出的表示之间的极端激活（EA）距离度量，在不访问任何数据的情况下利用网络内自说明能力。我们定量验证了度量的正确性和与人为定义的语义距离的一致性。EA距离与人类判断之间的一致性使我们能够确定表征，其基本概念被认为是不自然的。

    Although Deep Neural Networks (DNNs) are incredibly effective in learning complex abstractions, they are susceptible to unintentionally learning spurious artifacts from the training data. To ensure model transparency, it is crucial to examine the relationships between learned representations, as unintended concepts often manifest themselves to be anomalous to the desired task. In this work, we introduce DORA (Data-agnOstic Representation Analysis): the first data-agnostic framework for the analysis of the representation space of DNNs. Our framework employs the proposed Extreme-Activation (EA) distance measure between representations that utilizes self-explaining capabilities within the network without accessing any data. We quantitatively validate the metric's correctness and alignment with human-defined semantic distances. The coherence between the EA distance and human judgment enables us to identify representations whose underlying concepts would be considered unnatural by humans by
    
[^95]: 基于多源迁移学习的深度模型强化学习

    Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2205.14410v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14410](http://arxiv.org/abs/2205.14410)

    本文提出了一种基于多源迁移学习的模块化技术，可以自动学习如何从先前学习的任务中提取有用信息，从而减少智能体在学习新任务时需要与环境互动的次数。

    

    强化学习面临的一个关键挑战是减少智能体在掌握给定任务时需要与环境互动的次数。迁移学习提出通过重复利用先前学习任务中的知识来解决这个问题。然而，确定哪个源任务有资格用于知识提取，以及选择哪些算法组件进行迁移，是其在强化学习应用中面临的严重障碍。本文的目标是通过模块化多源迁移学习技术来解决这些问题。所提出的技术可以自动学习如何从源任务中提取有用信息，而不受状态-动作空间和奖励函数差异的影响。我们在视觉控制方面进行了广泛而具有挑战性的跨域实验来支持我们的想法。

    A crucial challenge in reinforcement learning is to reduce the number of interactions with the environment that an agent requires to master a given task. Transfer learning proposes to address this issue by re-using knowledge from previously learned tasks. However, determining which source task qualifies as the most appropriate for knowledge extraction, as well as the choice regarding which algorithm components to transfer, represent severe obstacles to its application in reinforcement learning. The goal of this paper is to address these issues with modular multi-source transfer learning techniques. The proposed techniques automatically learn how to extract useful information from source tasks, regardless of the difference in state-action space and reward function. We support our claims with extensive and challenging cross-domain experiments for visual control.
    
[^96]: SIBILA: 一种新的可解释通用机器学习模型集成方法在医学方面的应用

    SIBILA: A novel interpretable ensemble of general-purpose machine learning models applied to medical contexts. (arXiv:2205.06234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.06234](http://arxiv.org/abs/2205.06234)

    SIBILA是一种集成了机器学习和深度学习模型以及可解释性算法，并能够在医疗领域中实现个性化治疗预测的方法，具有较高的准确性和可解释性。

    

    个性化医疗仍然是科学家们面临的重大挑战。机器学习和深度学习的快速发展使它们成为预测个体患者最合适治疗方法的可行替代方案。然而，为每个数据集开发定制模型的需要，缺乏对结果的解释以及高计算要求使许多人不愿使用这些方法。为节省时间并揭示模型内部的工作方式，SIBILA应运而生。SIBILA是一组应用了一系列可解释性算法以识别最相关输入特征的机器学习和深度学习模型集成方法。由于解释性算法可能不彼此一致，因此实施了共识阶段来估计每个变量对预测的全局贡献。SIBILA被装箱以在任何高性能计算平台上运行。尽管最初目的是作为命令行工具，但它也可通过用户友好的Web界面获得。

    Personalized medicine remains a major challenge for scientists. The rapid growth of Machine learning and Deep learning has made them a feasible alternative for predicting the most appropriate therapy for individual patients. However, the need to develop a custom model for every dataset, the lack of interpretation of their results and high computational requirements make many reluctant to use these methods. Aiming to save time and bring light to the way models work internally, SIBILA has been developed. SIBILA is an ensemble of machine learning and deep learning models that applies a range of interpretability algorithms to identify the most relevant input features. Since the interpretability algo- rithms may not be in line with each other, a consensus stage has been imple- mented to estimate the global attribution of each variable to the predictions. SIBILA is containerized to be run on any high-performance computing plat- form. Although conceived as a command-line tool, it is also av
    
[^97]: 通过交互式问卷进行基于偏好的会议探索的PREME

    PREME: Preference-based Meeting Exploration through an Interactive Questionnaire. (arXiv:2205.02370v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.02370](http://arxiv.org/abs/2205.02370)

    本论文提出了一个新的端到端框架，通过生成交互式问卷，实现基于偏好的会议探索，帮助用户快速探索会议内容。

    

    在线会议的数量不断增加，需要自动化工具来管理和组织材料，特别是当参与者错过讨论并需要帮助快速探索时。本文提出了一种新的端到端框架，用于生成基于偏好的会议探索的交互式问卷。结果，用户会获得一个推荐问题列表，以反映他们的偏好。由于这项任务是新的，我们引入了一种自动评估策略。即，它通过度量通过问卷生成的问题有多少可回答来确保事实的正确性，并为探索源会议的深度提供了覆盖。

    The recent increase in the volume of online meetings necessitates automated tools for managing and organizing the material, especially when an attendee has missed the discussion and needs assistance in quickly exploring it. In this work, we propose a novel end-to-end framework for generating interactive questionnaires for preference-based meeting exploration. As a result, users are supplied with a list of suggested questions reflecting their preferences. Since the task is new, we introduce an automatic evaluation strategy. Namely, it measures how much the generated questions via questionnaire are answerable to ensure factual correctness and covers the source meeting for the depth of possible exploration.
    
[^98]: GrIPS：基于编辑的无梯度指令搜索，用于辅助大型语言模型

    GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models. (arXiv:2203.07281v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.07281](http://arxiv.org/abs/2203.07281)

    GrIPS是一种基于编辑的无梯度搜索方法，用于改进大型语言模型的任务指令，显著提高性能。

    

    提供自然语言指令的提示是一种改进大型语言模型在零样本设置下任务性能的有用新范例。最近的工作致力于通过手动重写或梯度调整来提高这些提示。然而，手动重写耗时且需要主观解释，而基于梯度的调整对于大型模型而言计算成本极高，对于基于API的模型来说可能不可行。在这项工作中，我们介绍了Gradient-free Instructional Prompt Search (GrIPS)，一种基于编辑的无梯度搜索方法，用于改进大型语言模型的任务指令。GrIPS接受面向人类设计的指令，并自动返回完善的编辑提示，同时允许基于API的调整。使用InstructGPT模型，在自然语言指令数据集的八个分类任务上，GrIPS将平均任务性能提高了高达4.30个百分点（OPT，BLOOM等任务也有类似的改进）

    Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, a
    
[^99]: PEg TRAnsfer Workflow recognition challenge报告：多模态数据是否有助于提高识别准确度？

    PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?. (arXiv:2202.05821v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05821](http://arxiv.org/abs/2202.05821)

    本文介绍了PETRAW挑战，探讨基于视频、运动学和分割数据进行手术工作流程识别的方法，结果显示多模态数据可以提高识别准确度。

    

    本文介绍了“PEg TRAnsfert Workflow recognition”（PETRAW）挑战的设计与结果，该挑战的目标是基于视频、运动学和分割等单一或多种模态开发外科手术工作流程识别方法，以研究它们的附加值。PETRAW挑战提供了一个包含150个虚拟模拟器上进行的插管转移序列的数据集。该数据集由视频、运动学、语义分割和工作流程注释组成，所述注释在三种不同的粒度级别（阶段、步骤和活动）上描述了序列。参赛者被提出了五项任务：其中三项任务与所有粒度的识别与一种可用的模态相关，而其他任务则通过多种模态的组合来解决识别问题。平均应用相关平衡准确性（AD-Accuracy）被用作评估度量标准，以考虑不平衡的类别，并且因为它比其他度量更具有临床相关性。

    This paper presents the design and results of the "PEg TRAnsfert Workflow recognition" (PETRAW) challenge whose objective was to develop surgical workflow recognition methods based on one or several modalities, among video, kinematic, and segmentation data, in order to study their added value. The PETRAW challenge provided a data set of 150 peg transfer sequences performed on a virtual simulator. This data set was composed of videos, kinematics, semantic segmentation, and workflow annotations which described the sequences at three different granularity levels: phase, step, and activity. Five tasks were proposed to the participants: three of them were related to the recognition of all granularities with one of the available modalities, while the others addressed the recognition with a combination of modalities. Average application-dependent balanced accuracy (AD-Accuracy) was used as evaluation metric to take unbalanced classes into account and because it is more clinically relevant tha
    
[^100]: 从SLAM到情境感知：挑战与综述

    From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2110.00273](http://arxiv.org/abs/2110.00273)

    本文研究旨在连接广泛的多学科现有知识，为移动机器人构建完整的情境感知（SA）系统，以提升其自主能力。

    

    移动机器人能够高效、安全地执行复杂任务的能力受限于其对环境（即情况）的了解。先进的推理、决策和执行技能使智能体能够在未知环境中自主行动。情境感知（SA）是人类的一种基本能力，已在心理学、军事、航空航天和教育等各个领域深入研究。然而，在机器人领域中尚未考虑情境感知，而是专注于单一的概念，如感知、空间感知、传感器融合、状态估计和同时定位与映射（SLAM）。因此，本研究旨在连接广泛的多学科现有知识，为我们认为对于自主性至关重要的移动机器人完整的SA系统铺平道路。为此，我们定义了构建机器人SA的主要组成部分及其所涉及的领域。因此，本文

    The capability of a mobile robot to efficiently and safely perform complex missions is limited by its knowledge of the environment, namely the situation. Advanced reasoning, decision-making, and execution skills enable an intelligent agent to act autonomously in unknown environments. Situational Awareness (SA) is a fundamental capability of humans that has been deeply studied in various fields, such as psychology, military, aerospace, and education. Nevertheless, it has yet to be considered in robotics, which has focused on single compartmentalized concepts such as sensing, spatial perception, sensor fusion, state estimation, and Simultaneous Localization and Mapping (SLAM). Hence, the present research aims to connect the broad multidisciplinary existing knowledge to pave the way for a complete SA system for mobile robotics that we deem paramount for autonomy. To this aim, we define the principal components to structure a robotic SA and their area of competence. Accordingly, this paper
    
[^101]: 深度学习商业框架的量化后门

    Quantization Backdoors to Deep Learning Commercial Frameworks. (arXiv:2108.09187v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2108.09187](http://arxiv.org/abs/2108.09187)

    本文揭示了商业框架中潜在的深度学习模型后门安全漏洞，可以通过量化攻击实现后门触发并逃避检测，从而危及已部署的模型安全，可能导致未经授权的数据访问。

    

    目前，由于低延迟和高隐私保护，越来越多的深度学习模型被部署在无处不在的边缘物联网设备上。

    Currently, there is a burgeoning demand for deploying deep learning (DL) models on ubiquitous edge Internet of Things (IoT) devices attributed to their low latency and high privacy preservation. However, DL models are often large in size and require large-scale computation, which prevents them from being placed directly onto IoT devices, where resources are constrained and 32-bit floating-point (float-32) operations are unavailable. Commercial framework (i.e., a set of toolkits) empowered model quantization is a pragmatic solution that enables DL deployment on mobile devices and embedded systems by effortlessly post-quantizing a large high-precision model (e.g., float-32) into a small low-precision model (e.g., int-8) while retaining the model inference accuracy. However, their usability might be threatened by security vulnerabilities.  This work reveals that the standard quantization toolkits can be abused to activate a backdoor. We demonstrate that a full-precision backdoored model w
    
[^102]: SQN: 大规模三维点云弱监督语义分割

    SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds. (arXiv:2104.04891v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2104.04891](http://arxiv.org/abs/2104.04891)

    本文提出一种新的弱监督方法SQN，通过利用点云局部邻域内强烈的语义相似性来隐式增强高度稀疏的监督信号，使得仅需要0.1％的随机注释点进行训练即可在七个大规模开放数据集上取得良好的性能，大大降低了完整点云注释的时间和成本。

    

    点云完全标注耗时昂贵。随着拥有数十亿点的大型点云数据集变得更加普遍，我们是否有必要进行完整注释？实验证明，即使面对1％随机点注释，基于完全注释假设设计的现有基线也只会略微下降。然而，超过这个点，例如在0.1％注释处，分割准确性是不可接受的。由于点云是3D世界的样本，局部邻域内点的分布相对均匀，展现出强烈的语义相似性。基于此，我们提出了一种新的弱监督方法来隐式增强高度稀疏的监督信号。广泛的实验表明，所提出的Semantic Query Network（SQN）在弱监督方案下在七个大规模开放数据集上取得了良好的性能，仅需要0.1％的随机注释点进行训练，极大地降低了完整点云注释的时间和成本。

    Labelling point clouds fully is highly time-consuming and costly. As larger point cloud datasets with billions of points become more common, we ask whether the full annotation is even necessary, demonstrating that existing baselines designed under a fully annotated assumption only degrade slightly even when faced with 1% random point annotations. However, beyond this point, e.g., at 0.1% annotations, segmentation accuracy is unacceptably low. We observe that, as point clouds are samples of the 3D world, the distribution of points in a local neighborhood is relatively homogeneous, exhibiting strong semantic similarity. Motivated by this, we propose a new weak supervision method to implicitly augment highly sparse supervision signals. Extensive experiments demonstrate the proposed Semantic Query Network (SQN) achieves promising performance on seven large-scale open datasets under weak supervision schemes, while requiring only 0.1% randomly annotated points for training, greatly reducing 
    
[^103]: 《UAV遥感中深度学习的综述》

    A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2101.10861](http://arxiv.org/abs/2101.10861)

    该综述总结了近年来应用于UAV遥感中的深度学习算法，主要关注分类和回归技术。

    

    深度神经网络（DNN）具有从数据中学习表征的出色能力，并在图像、时间序列、自然语言、音频、视频等处理方面取得了重大突破。在遥感领域，已经进行了有关DNN算法应用的调查和文献综述，试图总结其子领域中产生的大量信息。最近，基于无人机的应用在航空感知研究中占主导地位。然而，尚未进行将“深度学习”和“UAV遥感”两个主题结合起来的文献综述。我们的工作动机是对应用于基于UAV成像的深度学习（DL）的基本原理进行综合评述。我们主要关注描述用于最近UAV获取数据的分类和回归技术。为此，共筛选了232篇发表在国际科学期刊资料中的论文。

    Deep Neural Networks (DNNs) learn representation from data with an impressive capability, and brought important breakthroughs for processing images, time-series, natural language, audio, video, and many others. In the remote sensing field, surveys and literature revisions specifically involving DNNs algorithms' applications have been conducted in an attempt to summarize the amount of information produced in its subfields. Recently, Unmanned Aerial Vehicles (UAV) based applications have dominated aerial sensing research. However, a literature revision that combines both "deep learning" and "UAV remote sensing" thematics has not yet been conducted. The motivation for our work was to present a comprehensive review of the fundamentals of Deep Learning (DL) applied in UAV-based imagery. We focused mainly on describing classification and regression techniques used in recent applications with UAV-acquired data. For that, a total of 232 papers published in international scientific journal data
    

