# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Scaling Expert Language Models with Unsupervised Domain Discovery.](http://arxiv.org/abs/2303.14177) | 该论文提出了一种用于扩展专家语言模型的简单而有效的方法，使用无监督的领域发现来自动化训练并消除通信开销，通过将语料库聚类成相关文档集来训练单独的专家语言模型，并将它们组合成一个稀疏的集合进行推理。 |
| [^2] | [A Hybrid ANN-SNN Architecture for Low-Power and Low-Latency Visual Perception.](http://arxiv.org/abs/2303.14176) | 本文提出了一种混合ANN-SNN体系结构，通过用低速率的辅助ANN初始化状态，解决了SNN与经典ANN在准确性、延迟、功耗方面的平衡性问题，实现了高时间分辨率、低延迟和低功耗的预测。 |
| [^3] | [BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects.](http://arxiv.org/abs/2303.14158) | 我们提出了一种方法来进行未知对象的6-DoF跟踪和3D重建，即使在没有视觉纹理的情况下也适用，并且能够处理大姿态变化，部分和完全遮挡，无纹理表面和反射高光的序列。 |
| [^4] | ["Get ready for a party": Exploring smarter smart spaces with help from large language models.](http://arxiv.org/abs/2303.14143) | 本文探索了使用大语言模型推断用户意图和产生适当的上下文依赖响应的可行性，为更智能的智能空间铺平了道路。 |
| [^5] | [MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion.](http://arxiv.org/abs/2303.14139) | 本文提出了一个名为MindDiffuser的两阶段图像重建模型，通过利用稳定扩散技术，同时解决了重建图像缺乏明确语义信息和图像结构不可控制的问题。 |
| [^6] | [CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images.](http://arxiv.org/abs/2303.14126) | 本文提出了一个名为CIFAKE的方法，可以通过计算机视觉将真实照片与AI生成图像进行二元分类。该方法使用了卷积神经网络（CNN）对图像进行分类。 |
| [^7] | [Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives.](http://arxiv.org/abs/2303.14116) | 这篇论文总结了作者关于注意力机制在提高深度学习模型解释性和性能中的潜力。这是基础与应用研究的重要问题，尤其在医学领域，而注意力机制可以成为解决这个问题的一种方法。 |
| [^8] | [Interpretable Anomaly Detection via Discrete Optimization.](http://arxiv.org/abs/2303.14111) | 该论文提出了一个通过学习有限自动机进行异常检测的框架，并通过约束优化算法和新的正则化方案提高了可解释性。 |
| [^9] | [Distributed Silhouette Algorithm: Evaluating Clustering on Big Data.](http://arxiv.org/abs/2303.14102) | 本文提出了一种可以在分布式环境下快速计算 Silhouette 指标的算法，解决了大数据场景下聚类评估的难点。 |
| [^10] | [Online Learning for the Random Feature Model in the Student-Teacher Framework.](http://arxiv.org/abs/2303.14083) | 本研究考虑了一种两层神经网络模型的在线学习，研究发现，当学生的隐藏层大小呈指数增长时，完美泛化是可行的，但对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化。 |
| [^11] | [Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing.](http://arxiv.org/abs/2303.14077) | 本文提出了一种新的对抗训练方法(Instance-adaptive Adversarial Training, IAAT)通过平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本，取得了在各种数据集下的最新、最佳结果，并在白盒和黑盒攻击下均优于以前的方法。 |
| [^12] | [SEAL: Semantic Frame Execution And Localization for Perceiving Afforded Robot Actions.](http://arxiv.org/abs/2303.14067) | 本文提出了语义框架的概念，并给出了用于机器人操作行为的扩展表示，解决了语义框架执行和本地化问题。 |
| [^13] | [Learning Reward Machines in Cooperative Multi-Agent Tasks.](http://arxiv.org/abs/2303.14061) | 本文提出了一种新的多智能体强化学习方法，将合作任务分解与学习奖励机制相结合。该方法有助于解决部分可观察环境中奖励的非马尔可夫性质，并提高了学习策略的可解释性，同时也降低了合作任务的复杂性。 |
| [^14] | [Communicating Complex Decisions in Robot-Assisted Therapy.](http://arxiv.org/abs/2303.14054) | 社交辅助机器人（SAR）在治疗中发挥重要作用，应该考虑将透明沟通组件纳入设计中去促进人-机交互。 |
| [^15] | [SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization.](http://arxiv.org/abs/2303.14011) | 本文提出了一种低资源抽象文摘的总结偏好分解的方法，以在只有少量示例的情况下学习生成器。其中，以预训练的语言模型为基础，提出了一种元学习框架，将少量样本的学习过程从源语料库转移到目标语料库。 |
| [^16] | ['Team-in-the-loop' organisational oversight of high-stakes AI.](http://arxiv.org/abs/2303.14007) | 本论文通过对团队在 AI 系统中的监管流程的纵向观察，探讨了 AI 系统对临床决策制定中团队监管的影响，研究发现此前的专业团队监管方法主要依靠解释和问询来获取信息，而 AI 的引入将可能在信息披露和决策制定方面造成一定程度的影响。 |
| [^17] | [PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration.](http://arxiv.org/abs/2303.13997) | PowerPruning是一种新颖的方法，通过选择能减少神经网络硬件中MAC操作功耗的权重来优化效率。该方法可以将DNN在硬件上的功耗降低高达78.3％，同时没有显著的准确度损失。 |
| [^18] | [Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis.](http://arxiv.org/abs/2303.13992) | 本研究的方法通过可达性分析识别出自动驾驶汽车可能被物理触发器激活的危险区域和可达路径，测试结果显示成功率接近100％。这将有助于发现AV的漏洞并实施有效的安全策略。 |
| [^19] | [Paraphrase Detection: Human vs. Machine Content.](http://arxiv.org/abs/2303.13989) | 本研究分析了不同数据集和检测方法，对人类和机器释义内容进行了比较，发现人类释义难度、多样性和相似性超过机器释义，机器生成的数据集不足。Transformer 是最有效的检测方法，而 SVM-based 方法不及 BERT 和 RoBERTa 变体。 |
| [^20] | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods.](http://arxiv.org/abs/2303.13988) | 本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。 |
| [^21] | [Interpretable Motion Planner for Urban Driving via Hierarchical Imitation Learning.](http://arxiv.org/abs/2303.13986) | 本文提出一种基于分层模仿学习的运动规划方法，包括高层行为规划和低层轨迹规划。通过在仿真和实际驾驶中的测试，证明了该方法在城市自动驾驶中具有出色的性能。 |
| [^22] | [AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation.](http://arxiv.org/abs/2303.13953) | 本文提出了AssetField，一种新型的神经场景表示方法，通过学习一组对象感知的地面特征平面来表示场景，可实现各种操作以配置新场景，并提供了可解释的联合可视化表示，从而更好地理解场景和建模。 |
| [^23] | [Knowledge Graphs: Opportunities and Challenges.](http://arxiv.org/abs/2303.13948) | 本文系统地概述了知识图谱的机遇和挑战。它们在AI系统和潜在应用领域方面具有广泛的机会。然而，知识图谱的技术挑战包括嵌入、获取、补全、融合和推理。 |
| [^24] | [Generative AI Assistants in Software Development Education.](http://arxiv.org/abs/2303.13936) | 本文探讨了当前软件开发行业采用生成式 AI（GAI）助手进行软件开发的现状和挑战，提出了未来软件开发教育的愿景和教学建议。 |
| [^25] | [Symbolic Music Structure Analysis with Graph Representations and Changepoint Detection Methods.](http://arxiv.org/abs/2303.13881) | 该论文提出了两个新颖的基于图的算法，Norm和G-PELT，以及G-Window方法，用于通过其形式或结构对符号音乐进行分割。研究发现，使用图表示对符号音乐进行编码，并计算从图中获得的邻接矩阵的新颖性能很好地表示符号音乐片段的结构，并能够检测到边界。 |
| [^26] | [Query-Dependent Video Representation for Moment Retrieval and Highlight Detection.](http://arxiv.org/abs/2303.13874) | 该论文提出了一个基于查询的视频表示来解决视频时刻检索和亮点检测中存在的查询信息利用不充分的问题，该方法在编码模块中采用交叉注意力层将文本查询的上下文注入视频表示中。 |
| [^27] | [Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation.](http://arxiv.org/abs/2303.13873) | Fantasia3D是一种新的文本生成3D内容的方法，通过分离几何和外观建模和学习，提高了几何细节和逼真渲染，并更有效和高效。 |
| [^28] | [MagicEye: An Intelligent Wearable Towards Independent Living of Visually Impaired.](http://arxiv.org/abs/2303.13863) | MagicEye是一种智能可穿戴设备，能够识别日常生活中的室内外物体，提供针对视力受损人士的面部识别和货币识别模块，以及用于导航的GPS传感器。 |
| [^29] | [Unleasing ChatGPT on the Metaverse: Savior or Destroyer?.](http://arxiv.org/abs/2303.13856) | 本文探讨了在元宇宙中使用ChatGPT的利弊，它可以提供动态和个性化的体验，但也必须考虑隐私、偏见和道德等相关问题。 |
| [^30] | [Learning Causal Attributions in Neural Networks: Beyond Direct Effects.](http://arxiv.org/abs/2303.13850) | 本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。 |
| [^31] | [Editing Driver Character: Socially-Controllable Behavior Generation for Interactive Traffic Simulation.](http://arxiv.org/abs/2303.13830) | 为了评估自动驾驶汽车在不同社交特征的交互交通场景下采取安全有效的机动方式，该论文提出了一种社交可控行为生成（SCBG）模型，通过学习真实驾驶数据实现了逼真而类人的轨迹生成，并允许用户指定轨迹的礼貌程度。 |
| [^32] | [$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference.](http://arxiv.org/abs/2303.13824) | 本文提出$k$NN提示，一种不需要校准就可以推理最近相邻的算法，用来解决上下文学习的限制和偏见问题，显着优于最先进的方法。 |
| [^33] | [Converging Measures and an Emergent Model: A Meta-Analysis of Human-Automation Trust Questionnaires.](http://arxiv.org/abs/2303.13799) | 本文通过元分析提出了一种共识模型，以测量人机交互中的信任，并确定了最常被引用和最好验证的人机和人机器人信任问卷以及相关因素。 |
| [^34] | [Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint.](http://arxiv.org/abs/2303.13790) | 本文提出了一个公平的病人-试验匹配框架，通过生成病人准则级别的公平性约束，考虑了包含和排除标准的嵌入差异，有效地解决了病人-试验匹配中的不公平性问题。 |
| [^35] | [A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models.](http://arxiv.org/abs/2303.13773) | 本研究提出基于GNN的纳米卫星任务调度方法，以更好地优化服务质量，解决ONTS问题的复杂性。 |
| [^36] | [Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects.](http://arxiv.org/abs/2303.13769) | 本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。 |
| [^37] | [Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution.](http://arxiv.org/abs/2303.13767) | 本文提出了一种新的框架，利用事件的时空插值来实现随机尺度下的视频超分辨率。方法包括从RGB帧和事件的查询空间-时间坐标和特征中学习隐式神经表示。 |
| [^38] | [Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs.](http://arxiv.org/abs/2303.13763) | 本文提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。 |
| [^39] | [Structural Imbalance Aware Graph Augmentation Learning.](http://arxiv.org/abs/2303.13757) | 本文提出了一种选择性图增强方法 (SAug)，通过识别图中的中心节点和尾节点，设计了一种选择性增强策略，可以显著提高图机器学习模型的性能，特别是在学习尾节点时。 |
| [^40] | [Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers.](http://arxiv.org/abs/2303.13755) | 本文分析了ViT算法的计算成本高的问题，并且通过学习实例相关的注意力模式，来提高计算效率 |
| [^41] | [LONGNN: Spectral GNNs with Learnable Orthonormal Basis.](http://arxiv.org/abs/2303.13750) | 本研究提出了一种谱图神经网络LONGNN，它采用可学习正交标准基，并解决了固定多项式基和非归一化基础所带来的缺陷，经实验证明其在各种图数据集上具有优异的表现。 |
| [^42] | [End-to-End Diffusion Latent Optimization Improves Classifier Guidance.](http://arxiv.org/abs/2303.13703) | 本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。 |
| [^43] | [OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search.](http://arxiv.org/abs/2303.13683) | OFA$^2$是一个基于多目标优化的神经架构搜索模型，它通过将搜索阶段构想为一个多目标优化问题，并使用已经训练好的神经网络，从而在多个权衡目标之间找到高效和多样化的子网络。 |
| [^44] | [Presenting Multiagent Challenges in Team Sports Analytics.](http://arxiv.org/abs/2303.13660) | 本文将多智能体系统与团队运动分析联系起来，特别考虑入侵式游戏，并将其分为短期比赛策略和长期团队规划两个方向，为MAS的实现和发展提供了机会和挑战。 |
| [^45] | [Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture.](http://arxiv.org/abs/2303.13651) | 本文提供了关于构建领域通用人工智能的思路和原则，通过研究生物神经网络和系统级分布网络通信、递归和短期拓扑变化，为建立人工神经网络提供宝贵指导。 |
| [^46] | [Planning as Theorem Proving with Heuristics.](http://arxiv.org/abs/2303.13638) | 该论文介绍了一种用启发式规划作为定理证明的方法，通过开发一种定理证明提升启发式(TPLH)规划器，在情况树中搜索短的计划，并减少探讨的状态数量。 |
| [^47] | [Efficient and Direct Inference of Heart Rate Variability using Both Signal Processing and Machine Learning.](http://arxiv.org/abs/2303.13637) | 本论文使用信号处理和机器学习结合的方法，直接推断出心率变异性。在大数据集的评估中，该方法表现出比单独使用信号处理或机器学习更高的准确性。 |
| [^48] | [PPG-based Heart Rate Estimation with Efficient Sensor Sampling and Learning Models.](http://arxiv.org/abs/2303.13636) | 本文研究了如何将PPG心率估计技术应用于低功耗和资源受限的嵌入式设备中，在结合信号处理和机器学习的基础上，成功将PPG采样频率降至仅25Hz，并提高了心率估计的精度，同时也减小了机器学习模型的特征大小，使得模型更小。 |
| [^49] | [In-depth analysis of music structure as a self-organized network.](http://arxiv.org/abs/2303.13631) | 本文介绍了一种利用Essential Element Network (EEN)算法将音频编码成文本并进行相关性计算和优化应用于聚类系数的频率和排名的方法，得到了音乐的深层结构信息，为厘清音乐结构提供了新方法。 |
| [^50] | [Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging.](http://arxiv.org/abs/2303.13610) | 本文研究了一种基于人工智能的光学成像技术，可以快速、无标记的对弥漫性胶质瘤进行分子诊断，为其治疗提供更加准确的指导。 |
| [^51] | [Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback.](http://arxiv.org/abs/2303.13604) | 本论文研究了具有随机次模收益和全赌徒延迟反馈的组合多臂赌博机问题，研究了三种延迟反馈模型并导出了后悔上限。研究结果表明，算法能够在考虑延迟组合匿名反馈时胜过其他全赌徒方法。 |
| [^52] | [Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages.](http://arxiv.org/abs/2303.13592) | 本文探讨了使用大型语言模型（LLMs）生成东南亚五种语言和Singlish的混合代码数据的方法，发现ChatGPT展现出最高的潜力。然而，由于词汇选择错误的影响，ChatGPT和InstructGPT在生成混合代码时的熟练程度受到限制。 |
| [^53] | [Efficient Symbolic Reasoning for Neural-Network Verification.](http://arxiv.org/abs/2303.13588) | 本文提出了一种高效的符号推理技术，用于解决神经网络的验证问题。该技术可以编码许多验证问题为二次程序，并将其松弛为半定程序以高效地解决。该框架可以验证各种神经网络属性，并为解决神经网络对对抗攻击的脆弱性问题提供了改进。 |
| [^54] | [TinyML: Tools, Applications, Challenges, and Future Research Directions.](http://arxiv.org/abs/2303.13569) | TinyML是一种嵌入式ML技术，可在多种廉价，资源有限和功率有限的设备上实现ML应用。但是，在其实现过程中，需要及时解决多种挑战，例如处理能力优化，提高可靠性以及维护学习模型的准确性。 |
| [^55] | [Extracting real estate values of rental apartment floor plans using graph convolutional networks.](http://arxiv.org/abs/2303.13568) | 本文使用改进后的方法从大量平面图像中提取访问图，利用图卷积神经网络模型“平面图价值”估计访问图的房地产价值，大幅提高了租金估计的精度。该模型为全面估计平面图的价值提供了新思路。 |
| [^56] | [Enhancing Embedding Representations of Biomedical Data using Logic Knowledge.](http://arxiv.org/abs/2303.13566) | 本文提出使用逻辑规则增强生物医学数据的嵌入表示，应用于最具挑战性的PharmKG数据集，并能够处理复杂的实验环境中关系事实之间的依赖性。 |
| [^57] | [CH-Go: Online Go System Based on Chunk Data Storage.](http://arxiv.org/abs/2303.13553) | 该论文提出了一种基于分块数据存储方法的在线围棋游戏系统 CH-Go，可以高效地处理海量数据，并使用神经网络来进行训练。 |
| [^58] | [Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh.](http://arxiv.org/abs/2303.13549) | 本文介绍了一种名为DaToBS的方法，用于从自然环境中的照片中检测和转录Tifinagh字符，以提高非洲低资源语言阿马齐语在教育、研究和网络应用等方面的支持。 |
| [^59] | [Hey Dona! Can you help me with student course registration?.](http://arxiv.org/abs/2303.13548) | 本文演示了智能个人助手Dona，用于学生选课的自动化操作，采用语音输入、任务规划优化和语言翻译等技术，使学生不需要自己完成复杂的选课表格。 |
| [^60] | [A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability.](http://arxiv.org/abs/2303.13547) | 本文对ChatGPT的Text-to-SQL能力进行了全面的评估，展示其强大的零-shot表现，尤其在ADVETA(RPL)情境下优于需要微调的SOTA模型，有望在实际应用中发挥潜力。 |
| [^61] | [OntoMath${}^{\mathbf{PRO}}$ 2.0 Ontology: Updates of the Formal Model.](http://arxiv.org/abs/2303.13542) | 本文介绍了OntoMath${}^{\mathrm{PRO}}$的新版本，该本体使用形式模型将数学事实表示为Linked Open Data。 |
| [^62] | [Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision.](http://arxiv.org/abs/2303.13540) | 本论文在可持续性方面的主要贡献是使用深度学习技术提高产品生产和使用的可持续性，通过计算机视觉技术检测产品的磨损状态并用于改进智能产品-服务系统的集成和结果取向。 |
| [^63] | [Towards risk-informed PBSHM: Populations as hierarchical systems.](http://arxiv.org/abs/2303.13533) | PBSHM是一种新的结构健康监测方法，它通过对结构群体进行监测，将有价值的知识在结构实例之间传递，从而提高了决策的准确性和泛化能力。 |
| [^64] | [Enhanced Iterated local search for the technician routing and scheduling problem.](http://arxiv.org/abs/2303.13532) | 本文提出了增强的迭代局部搜索算法来解决技术员路由和调度问题，该算法结合了高效初始化过程、精心选择的邻域结构和新颖的摇动机制，并在实验中表现优异。 |
| [^65] | [Discovering Hierarchical Process Models: an Approach Based on Events Clustering.](http://arxiv.org/abs/2303.13531) | 本文提出一种基于事件聚类的算法，用于自动合成更易读和理解的分层业务流程模型。 |
| [^66] | [Dataset for predicting cybersickness from a virtual navigation task.](http://arxiv.org/abs/2303.13527) | 本文提供了一个用于预测虚拟现实环境中晕动症的数据集，包含多个来自不同参与者的数据点，并详细描述了数据集内容和采集过程，将有助于研究晕动症缓解和预测模型开发。 |
| [^67] | [Uncertainty-Aware Workload Prediction in Cloud Computing.](http://arxiv.org/abs/2303.13525) | 本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。 |
| [^68] | [Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources.](http://arxiv.org/abs/2303.13521) | 这篇论文探讨使用AI欺骗骗子并浪费他们时间和资源的方法，利用ChatGPT的自然语言处理能力来回复欺诈邮件。初步结果显示，ChatGPT可以成功欺骗骗子，证明AI是反击电子邮件威胁的有效工具。 |
| [^69] | [Neural Preset for Color Style Transfer.](http://arxiv.org/abs/2303.13511) | 本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。 |
| [^70] | [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.](http://arxiv.org/abs/2303.10130) | 该研究调查了GPT（大语言模型）和相关技术对美国劳动力市场的潜在影响，发现大约80%的美国劳动力可能会受到10%的工作任务的影响，涵盖了所有工资水平和各行各业，预示着这些模型可能具有显著的经济、社会和政策影响。 |
| [^71] | [VMCDL: Vulnerability Mining Based on Cascaded Deep Learning Under Source Control Flow.](http://arxiv.org/abs/2303.07128) | 本论文提出了一种基于源代码控制流的级联深度学习模型VMCDL，通过对SARD数据集中的源代码进行处理来有效检测软件漏洞。 |
| [^72] | [Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective.](http://arxiv.org/abs/2302.11963) | 本文首次将单步对抗示例分解为数据信息和自信息，揭示了一种有趣的现象，称为“自我拟合”，即网络学习单步扰动中嵌入的自信息，导致灾难性过拟合。 |
| [^73] | [User-Centered Design (IX): A "User Experience 3.0" Paradigm Framework in the Intelligence Era.](http://arxiv.org/abs/2302.06681) | 本文提出了一种“UX 3.0”范式框架和相应的UX方法系统，旨在满足情报时代对用户体验的新要求。 |
| [^74] | [Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion.](http://arxiv.org/abs/2302.03298) | 本文提出了一种通过增强生成数据集中图像的多样性来提高模型无关的零样本分类性能的方法，比最先进的方法提高了1.4mAP（CUB数据集）和8.4mAP（ImageNet数据集），并且适用于任何下游的分类架构。 |
| [^75] | [Quantum Circuit Components for Cognitive Decision-Making.](http://arxiv.org/abs/2302.03012) | 本文将一些非经典的人类决策模型成功地应用于量子计算机电路，并通过量子概率、角度和子空间等方式描述了这些模型和它们在量子计算机上的实现和研究。 |
| [^76] | [Real-Time Evaluation in Online Continual Learning: A New Hope.](http://arxiv.org/abs/2302.01047) | 该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。 |
| [^77] | [One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER.](http://arxiv.org/abs/2301.10410) | 本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。 |
| [^78] | [Deep Conditional Measure Quantization.](http://arxiv.org/abs/2301.06907) | 提出了一种名为DCMQ的方法，该方法结合了基于Huber能量核的方法和深度神经网络架构，用于条件测度量化，并在多个实例中取得了有希望的结果。 |
| [^79] | [Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models.](http://arxiv.org/abs/2212.12380) | 本论文研究了物理一致神经网络(PCNNs) 在模拟建筑温度动态方面的扩展性和准确性。结果发现，PCNNs既确保了物理一致性，同时又能在复杂的多区域热建筑模型中取得高精度的性能表现，且在可用数据量有限的情况下超越经典灰盒模型，具有可扩展性优势。 |
| [^80] | [Emergent Analogical Reasoning in Large Language Models.](http://arxiv.org/abs/2212.09196) | GPT-3在许多类比任务中表现出与甚至超越人类的能力，揭示了大型语言模型的紧急能力。 |
| [^81] | [POTATO: The Portable Text Annotation Tool.](http://arxiv.org/abs/2212.08620) | POTATO是一个免费、开源的便携式文本注释工具，支持多种类型的文本和多模态数据的标注，提供易于配置的功能以最大化生产力，特别是对于长文档和复杂任务。 |
| [^82] | [FlexiViT: One Model for All Patch Sizes.](http://arxiv.org/abs/2212.08013) | 本文介绍了一种名为FlexiViT的模型，它可以使用一组权重表现良好，并且适用于各种补丁大小，从而使其在部署时容易根据不同的计算预算来定制模型。 |
| [^83] | [Flow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localization.](http://arxiv.org/abs/2212.07906) | 本文提出了Flow-Lenia作为Lenia的一种扩展来解决Lenia无法生成完全自由的演化、无法相互作用以及需要高级搜索算法进行探索等问题，展示了Flow-Lenia在生成具有复杂行为的生命形态方面的有效性。 |
| [^84] | [Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey.](http://arxiv.org/abs/2212.04634) | 本文对结构化知识增强的故事生成进行了综述，总结了目前的方法与技术，指出了未来的发展方向和尚未解决的问题。 |
| [^85] | [State Space Closure: Revisiting Endless Online Level Generation via Reinforcement Learning.](http://arxiv.org/abs/2212.02951) | 这篇论文重新审视了无尽在线关卡生成，提出了状态空间封闭概念，将有限时段训练的经验驱动强化学习程序内容生成推广到无限时段，而不损失生成内容的质量。通过实证研究，验证了EDRL生成的内容质量和多样性。 |
| [^86] | [MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series.](http://arxiv.org/abs/2212.01141) | 本文提出了一种名为MHCCL的对比学习模型，可以从多元时间序列数据中学习语义丰富的表示，并利用层次聚类结构中的多粒度信息来滤除虚假负样本和补充正样本。 |
| [^87] | [Foiling Explanations in Deep Neural Networks.](http://arxiv.org/abs/2211.14860) | 本文发现了解释图像 DNN 的一个令人担忧的属性：通过微小视觉更改，我们演示了解释可以通过进化策略任意操纵。我们提出了 AttaXAI，一个针对 XAI 算法的敌对攻击，可访问分类器输出信息和解释图，这使得我们的方法在实际应用中非常有用。 |
| [^88] | [OCTET: Object-aware Counterfactual Explanations.](http://arxiv.org/abs/2211.12380) | 本文提出了一种基于对象的框架来生成反事实解释，以解释训练有多个物体图像的决策模型。 |
| [^89] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^90] | [When and why vision-language models behave like bags-of-words, and what to do about it?.](http://arxiv.org/abs/2210.01936) | 针对大型视觉语言模型在编码组合信息方面的表现存在问题，我们创建了Attribution、Relation和Order（ARO）基准，并提出了对VLMs进行Attention机制和对抗训练等修改以提高其组合理解能力。 |
| [^91] | [Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions.](http://arxiv.org/abs/2209.15055) | 本文研究了全连接神经网络的表示成本和深度之间的关系，发现其会收敛到非线性函数的秩的概念。同时，发现在一定的深度范围内，全局最小值可以恢复真实的数据秩，并探讨了分类器秩对类边界拓扑结构的影响。 |
| [^92] | [Continuous Mixtures of Tractable Probabilistic Models.](http://arxiv.org/abs/2209.10584) | 本文研究了一种连续混合的方法，将可计算概率模型和基于连续潜空间的模型结合起来，从而实现了高效的概率推断。 |
| [^93] | [CNN based Intelligent Streetlight Management Using Smart CCTV Camera and Semantic Segmentation.](http://arxiv.org/abs/2209.08633) | 本研究提出了一种基于智能交通监测系统和CCTV相机的自动化路灯控制方法，利用语义分割技术检测行人或车辆的存在并调节LED路灯的亮度，减少了能源浪费和环境影响。 |
| [^94] | [Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors.](http://arxiv.org/abs/2208.11356) | 本文提出了一种在基于Transformer的目标检测器中，通过两个新的设计实现了多尺度特征的高效利用，其核心思想是通过稀疏采样多尺度特征，以减少计算成本，同时保证了目标检测性能。 |
| [^95] | [On the Symmetries of Deep Learning Models and their Internal Representations.](http://arxiv.org/abs/2205.14258) | 本文研究了深度学习模型及其内部表示的对称性，发现网络的对称性传播到数据的表示中的对称性，为更好地理解架构影响学习和预测过程提供了一种方法。 |
| [^96] | [Euler State Networks: Non-dissipative Reservoir Computing.](http://arxiv.org/abs/2203.09382) | 本文提出了一种新型水库计算模型EuSN，其利用前向欧拉离散化和反对称循环矩阵来设计水库动力学，具有接近稳定边缘的饱和有效谱半径和零局部李雅普诺夫指数。在长期记忆任务和时间序列分类基准测试中表现出了优异的性能。 |
| [^97] | [Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models.](http://arxiv.org/abs/2202.00091) | 研究提出了基于决策的稀疏攻击方法SparseEvo，通过最小化扰动的像素数量，能够有效地欺骗黑盒深度学习模型，而不需要了解它的内部构造。 |
| [^98] | [Paint by Word.](http://arxiv.org/abs/2103.10951) | 本文研究了零样本语义图像绘制问题，并使用最先进的逼真图像生成模型和文本-图像语义相似性网络相结合，使得能够在开放式的文本描述下进行语义画作。 |

# 详细

[^1]: 用无监督的领域发现方法扩展专家语言模型

    Scaling Expert Language Models with Unsupervised Domain Discovery. (arXiv:2303.14177v1 [cs.CL])

    [http://arxiv.org/abs/2303.14177](http://arxiv.org/abs/2303.14177)

    该论文提出了一种用于扩展专家语言模型的简单而有效的方法，使用无监督的领域发现来自动化训练并消除通信开销，通过将语料库聚类成相关文档集来训练单独的专家语言模型，并将它们组合成一个稀疏的集合进行推理。

    

    大型语言模型通常进行密集训练：所有参数均对所有输入进行更新。这要求在数千个GPU之间同步数十亿个参数。我们引入了一种简单而有效的方法，能够异步地在任意文本语料库上训练大型稀疏语言模型。我们的方法将一个语料库聚类成相关文档集，对每个集群训练一个单独的专家语言模型，然后在推理时将它们组合成稀疏的集合。这种方法通过自动发现每个专家的领域来推广了尴尬平行训练，并消除了现有稀疏语言模型中几乎所有的通信开销。我们的技术在多个语料库和少量训练任务上优于密集基线，并且我们的分析表明将专家特化到有意义的集群是取得这些增益的关键。性能还随着专家数量和训练数据的大小而提高，这表明这是一种高效且可访问的缩放专家语言模型的方法。

    Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessibl
    
[^2]: 低功耗低延迟视觉感知的混合ANN-SNN体系结构

    A Hybrid ANN-SNN Architecture for Low-Power and Low-Latency Visual Perception. (arXiv:2303.14176v1 [cs.CV])

    [http://arxiv.org/abs/2303.14176](http://arxiv.org/abs/2303.14176)

    本文提出了一种混合ANN-SNN体系结构，通过用低速率的辅助ANN初始化状态，解决了SNN与经典ANN在准确性、延迟、功耗方面的平衡性问题，实现了高时间分辨率、低延迟和低功耗的预测。

    

    脉冲神经网络（SNN）是一类仿生神经网络，通过异步和稀疏处理，承诺将低功耗和低延迟的推理应用于边缘设备。然而，作为时间模型，SNN依赖于表达丰富的状态才能生成与经典人工神经网络（ANN）相当的预测结果。这些状态仅在长时间瞬态周期后收敛，并在没有输入数据的情况下迅速衰减，导致更高的延迟、功耗和较低的准确性。本文通过使用以低速率运行的辅助ANN初始化状态来解决这个问题。然后，SNN使用状态以高时间分辨率生成预测，直到下一个初始化阶段。因此，我们的混合ANN-SNN模型结合了两个世界的优点：优化了ANN和SNN之间的性能平衡，不受长时间状态转换和状态衰减的影响，可以实现高时间分辨率、低延迟和低功耗的预测。我们展示了该模型在低功耗/低延迟视觉感知任务上的良好表现。

    Spiking Neural Networks (SNN) are a class of bio-inspired neural networks that promise to bring low-power and low-latency inference to edge devices through asynchronous and sparse processing. However, being temporal models, SNNs depend heavily on expressive states to generate predictions on par with classical artificial neural networks (ANNs). These states converge only after long transient periods, and quickly decay without input data, leading to higher latency, power consumption, and lower accuracy. This work addresses this issue by initializing the state with an auxiliary ANN running at a low rate. The SNN then uses the state to generate predictions with high temporal resolution until the next initialization phase. Our hybrid ANN-SNN model thus combines the best of both worlds: It does not suffer from long state transients and state decay thanks to the ANN, and can generate predictions with high temporal resolution, low latency, and low power thanks to the SNN. We show for the task 
    
[^3]: BundleSDF:未知物体的神经6-DoF跟踪和3D重建

    BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects. (arXiv:2303.14158v1 [cs.CV])

    [http://arxiv.org/abs/2303.14158](http://arxiv.org/abs/2303.14158)

    我们提出了一种方法来进行未知对象的6-DoF跟踪和3D重建，即使在没有视觉纹理的情况下也适用，并且能够处理大姿态变化，部分和完全遮挡，无纹理表面和反射高光的序列。

    

    我们提出了一种几乎实时的方法，可以从单目RGBD视频序列中进行未知对象的6-DoF跟踪，同时执行对象的神经3D重建。我们的方法适用于任意刚性对象，即使视觉纹理基本缺失。仅在第一帧中假定对象被分割。不需要额外信息，并且不对交互代理做任何假设。我们方法的关键是在姿态图优化过程中同时学习的神经对象场，从而将信息稳健地累积到一致的3D表示中，捕捉几何和外观。自动维护一个动态的目前存储的内存帧池，以便这些线程之间进行通信。我们的方法应用于具有大姿态变化，部分和完全遮挡，无纹理表面和反射高光的具有挑战性的序列上。我们在HO3D，YCBInEOAT和BEHAVE数据集上展示了结果，证明了我们的方法的有效性。

    We present a near real-time method for 6-DoF tracking of an unknown object from a monocular RGBD video sequence, while simultaneously performing neural 3D reconstruction of the object. Our method works for arbitrary rigid objects, even when visual texture is largely absent. The object is assumed to be segmented in the first frame only. No additional information is required, and no assumption is made about the interaction agent. Key to our method is a Neural Object Field that is learned concurrently with a pose graph optimization process in order to robustly accumulate information into a consistent 3D representation capturing both geometry and appearance. A dynamic pool of posed memory frames is automatically maintained to facilitate communication between these threads. Our approach handles challenging sequences with large pose changes, partial and full occlusion, untextured surfaces, and specular highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets, demonstrating that ou
    
[^4]: “Get ready for a party”：“更智能的智能空间”探索在大语言模型的帮助下

    "Get ready for a party": Exploring smarter smart spaces with help from large language models. (arXiv:2303.14143v1 [cs.HC])

    [http://arxiv.org/abs/2303.14143](http://arxiv.org/abs/2303.14143)

    本文探索了使用大语言模型推断用户意图和产生适当的上下文依赖响应的可行性，为更智能的智能空间铺平了道路。

    

    当有人说“准备好聚会”，正确的回答深受含义和上下文的影响。对于智能家居助手（例如Google Home），理想的回答可能是调查家中可用的设备，并改变它们的状态以营造欢乐的氛围。本文利用最近的任务无关的大语言模型（LLM），如GPT-3，它们具有广泛的跨领域、有时不可预测的上下文知识，这种知识现有的基于规则的家用助手系统缺乏，可以使它们成为推断用户意图并在智能家居交互期间生成适当的上下文依赖响应的强大工具。

    The right response to someone who says "get ready for a party" is deeply influenced by meaning and context. For a smart home assistant (e.g., Google Home), the ideal response might be to survey the available devices in the home and change their state to create a festive atmosphere. Current practical systems cannot service such requests since they require the ability to (1) infer meaning behind an abstract statement and (2) map that inference to a concrete course of action appropriate for the context (e.g., changing the settings of specific devices). In this paper, we leverage the observation that recent task-agnostic large language models (LLMs) like GPT-3 embody a vast amount of cross-domain, sometimes unpredictable contextual knowledge that existing rule-based home assistant systems lack, which can make them powerful tools for inferring user intent and generating appropriate context-dependent responses during smart home interactions. We first explore the feasibility of a system that 
    
[^5]: MindDiffuser：基于人脑活动的受控图像重建与语义结构扩散

    MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2303.14139v1 [cs.CV])

    [http://arxiv.org/abs/2303.14139](http://arxiv.org/abs/2303.14139)

    本文提出了一个名为MindDiffuser的两阶段图像重建模型，通过利用稳定扩散技术，同时解决了重建图像缺乏明确语义信息和图像结构不可控制的问题。

    

    从测量的功能磁共振成像（fMRI）中重建视觉刺激是一项有意义且具有挑战性的任务。以前的研究已成功实现了重建与原始图像相似的结构，例如一些自然图像的轮廓和大小。但这些重建缺乏明确的语义信息，难以识别。近年来，许多研究利用具有更强生成能力的多模态预训练模型重建与原始图像在语义上相似的图像。但是，这些图像具有不可控制的结构信息，例如位置和方向。为了同时解决上述问题，我们提出了一个名为MindDiffuser的两阶段图像重建模型，利用稳定扩散技术。在第一阶段中，将从fMRI解码的VQ-VAE潜在表示和CLIP文本嵌入放入稳定扩散的图像对图像处理中，这产生了...

    Reconstructing visual stimuli from measured functional magnetic resonance imaging (fMRI) has been a meaningful and challenging task. Previous studies have successfully achieved reconstructions with structures similar to the original images, such as the outlines and size of some natural images. However, these reconstructions lack explicit semantic information and are difficult to discern. In recent years, many studies have utilized multi-modal pre-trained models with stronger generative capabilities to reconstruct images that are semantically similar to the original ones. However, these images have uncontrollable structural information such as position and orientation. To address both of the aforementioned issues simultaneously, we propose a two-stage image reconstruction model called MindDiffuser, utilizing Stable Diffusion. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into the image-to-image process of Stable Diffusion, which yie
    
[^6]: CIFAKE: AI生成图像的分类和可解释识别

    CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. (arXiv:2303.14126v1 [cs.CV])

    [http://arxiv.org/abs/2303.14126](http://arxiv.org/abs/2303.14126)

    本文提出了一个名为CIFAKE的方法，可以通过计算机视觉将真实照片与AI生成图像进行二元分类。该方法使用了卷积神经网络（CNN）对图像进行分类。

    

    最近合成数据技术的进步使得生成的图像质量如此之高，以至于人类无法区分真实照片和人工智能生成的图像。鉴于数据可靠性和认证的至关重要性，本文提出通过计算机视觉增强我们识别AI生成图像的能力。首先，通过潜在扩散生成合成数据集，该数据集与已有的CIFAR-10数据集中的十个类别相似，提供与真实照片对比的不同类型的图像。该模型能够生成复杂的视觉属性，例如水中逼真的反射。这两组数据之间存在二元分类问题，即照片是真实的还是由AI生成的。本研究随后提出使用卷积神经网络（CNN）将图像分类为两个类别：真实或伪造。

    Recent technological advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion which provides a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following
    
[^7]: 从基础与应用研究视角提高预测性能和模型可解释性：注意力机制

    Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])

    [http://arxiv.org/abs/2303.14116](http://arxiv.org/abs/2303.14116)

    这篇论文总结了作者关于注意力机制在提高深度学习模型解释性和性能中的潜力。这是基础与应用研究的重要问题，尤其在医学领域，而注意力机制可以成为解决这个问题的一种方法。

    

    随着深度学习技术的快速发展，机器学习研究关注于提高模型预测的可解释性和性能，涵盖基础与应用研究的各个领域。虽然深度学习模型比传统机器学习模型具有更高的预测性能，但具体预测过程仍难以解释和说明，这被称为机器学习模型的黑盒化，并被广泛认为是许多研究领域的一个特别重要的问题，包括制造业、商业、机器人和其他行业等普遍使用该技术，以及医学领域，在这些领域中错误是不可容忍的。本文基于作者论文的摘要，该论文的核心研究关注于近年来备受关注的注意力机制，并探讨了其在基础研究和应用研究中的潜力。

    With the dramatic advances in deep learning technology, machine learning research is focusing on improving the interpretability of model predictions as well as prediction performance in both basic and applied research. While deep learning models have much higher prediction performance than traditional machine learning models, the specific prediction process is still difficult to interpret and/or explain. This is known as the black-boxing of machine learning models and is recognized as a particularly important problem in a wide range of research fields, including manufacturing, commerce, robotics, and other industries where the use of such technology has become commonplace, as well as the medical field, where mistakes are not tolerated. This bulletin is based on the summary of the author's dissertation. The research summarized in the dissertation focuses on the attention mechanism, which has been the focus of much attention in recent years, and discusses its potential for both basic res
    
[^8]: 通过离散优化实现可解释性异常检测

    Interpretable Anomaly Detection via Discrete Optimization. (arXiv:2303.14111v1 [cs.LG])

    [http://arxiv.org/abs/2303.14111](http://arxiv.org/abs/2303.14111)

    该论文提出了一个通过学习有限自动机进行异常检测的框架，并通过约束优化算法和新的正则化方案提高了可解释性。

    

    异常检测在许多应用领域中都是必不可少的，例如网络安全、执法、医学和欺诈保护。然而，目前深度学习方法的决策过程往往难以理解，这通常限制了它们的实际应用性。为了克服这个限制，我们提出了一个学习框架，可以从序列数据中学习可解释性的异常检测器。具体来说，我们考虑从给定的未标记序列多重集中学习确定性有限自动机 （DFA）的任务。我们证明了这个问题是计算难题，并基于约束优化开发了两个学习算法。此外，我们为优化问题引入了新的正则化方案，以提高我们的DFA的整体可解释性。通过原型实现，我们证明我们的方法在准确性和F1分数方面表现出有望的结果。

    Anomaly detection is essential in many application domains, such as cyber security, law enforcement, medicine, and fraud protection. However, the decision-making of current deep learning approaches is notoriously hard to understand, which often limits their practical applicability. To overcome this limitation, we propose a framework for learning inherently interpretable anomaly detectors from sequential data. More specifically, we consider the task of learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled sequences. We show that this problem is computationally hard and develop two learning algorithms based on constraint optimization. Moreover, we introduce novel regularization schemes for our optimization problems that improve the overall interpretability of our DFAs. Using a prototype implementation, we demonstrate that our approach shows promising results in terms of accuracy and F1 score.
    
[^9]: 分布式Silhouette算法：评估大数据聚类

    Distributed Silhouette Algorithm: Evaluating Clustering on Big Data. (arXiv:2303.14102v1 [cs.DC])

    [http://arxiv.org/abs/2303.14102](http://arxiv.org/abs/2303.14102)

    本文提出了一种可以在分布式环境下快速计算 Silhouette 指标的算法，解决了大数据场景下聚类评估的难点。

    

    在大数据时代，每个算法需要具备的关键特征是在分布式环境下高效运行的可能性。然而，流行的Silhouette度量评估聚类质量的算法没有这个性质，并且与输入数据集的大小呈二次计算复杂度。因此，在大数据场景中，聚类评估必须采用其他方法。为了填补这个空白，本文介绍了第一个可以以线性复杂度计算Silhouette指标并可以在分布式环境中轻松执行的算法。它的实现在Apache Spark ML库中免费提供。

    In the big data era, the key feature that each algorithm needs to have is the possibility of efficiently running in parallel in a distributed environment. The popular Silhouette metric to evaluate the quality of a clustering, unfortunately, does not have this property and has a quadratic computational complexity with respect to the size of the input dataset. For this reason, its execution has been hindered in big data scenarios, where clustering had to be evaluated otherwise. To fill this gap, in this paper we introduce the first algorithm that computes the Silhouette metric with linear complexity and can easily execute in parallel in a distributed environment. Its implementation is freely available in the Apache Spark ML library.
    
[^10]: 学生-教师框架下随机特征模型的在线学习

    Online Learning for the Random Feature Model in the Student-Teacher Framework. (arXiv:2303.14083v1 [cs.LG])

    [http://arxiv.org/abs/2303.14083](http://arxiv.org/abs/2303.14083)

    本研究考虑了一种两层神经网络模型的在线学习，研究发现，当学生的隐藏层大小呈指数增长时，完美泛化是可行的，但对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化。

    

    深度神经网络是一种广泛应用的预测算法，随着权重数量的增加，其性能通常会提高，导致过度参数化。我们考虑一种两层神经网络，其第一层是冻结的，而最后一层是可训练的，称为随机特征模型。我们在学生-教师框架下研究了过度参数化，通过导出一组学习动态的微分方程。对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化，并计算非零渐近泛化误差。只有当学生的隐藏层大小呈指数增长时，才有可能实现完美泛化。

    Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. We consider a two-layered neural network whose first layer is frozen while the last layer is trainable, known as the random feature model. We study over-parametrization in the context of a student-teacher framework by deriving a set of differential equations for the learning dynamics. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error. Only when the student's hidden layer size is exponentially larger than the input dimension, an approach to perfect generalization is possible.
    
[^11]: 自适应实例级损失平滑改进对抗训练

    Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])

    [http://arxiv.org/abs/2303.14077](http://arxiv.org/abs/2303.14077)

    本文提出了一种新的对抗训练方法(Instance-adaptive Adversarial Training, IAAT)通过平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本，取得了在各种数据集下的最新、最佳结果，并在白盒和黑盒攻击下均优于以前的方法。

    

    通过对输入进行对抗扰动：即人类难以察觉的人造噪声，可以轻易地迷惑深度神经网络从而做出不正确的预测。目前对抗训练已成为最成功的对抗攻击防御方法，本文致力于改进对抗训练以提升对抗鲁棒性。首先从实例级别的角度分析了对抗训练期间对抗性脆弱性的演变。发现在训练期间，通过牺牲相当比例的训练样本来提高对抗攻击的脆弱性，从而实现对抗性损失的整体降低，这导致了不同数据的对抗性脆弱性分布不均衡。这种“不均衡脆弱性”在几种流行的鲁棒性训练方法中普遍存在，并且与对抗训练中的过拟合相关。基于此观察，我们提出了一种新的对抗训练方法：Instance-adaptive Adversarial Training (IAAT)。该方法在训练过程中平滑实例级别的对抗性损失，鼓励模型关注“难”的样本，同时避免牺牲特定的样本而偏爱其他样本。本方法在各种数据集下都取得了最新的最佳结果，并在白盒和黑盒攻击下均优于以前的方法。

    Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such "uneven vulnerability", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt
    
[^12]: SEAL: 用于理解机器人行为的语义框架执行和本地化

    SEAL: Semantic Frame Execution And Localization for Perceiving Afforded Robot Actions. (arXiv:2303.14067v1 [cs.RO])

    [http://arxiv.org/abs/2303.14067](http://arxiv.org/abs/2303.14067)

    本文提出了语义框架的概念，并给出了用于机器人操作行为的扩展表示，解决了语义框架执行和本地化问题。

    

    机器人移动操作的最新进展已经促进了机器人的操作环境从受限的工作空间扩展到大规模的人类环境。为了有效地完成这些空间中的任务，机器人必须能够感知、推理和执行各种可行性，远远超出了简单的拾取和放置。我们提出了语义框架的概念，为机器人行为提供了一种有利于行动感知、任务级别推理、行动级别执行和语言集成的表征。本文扩展了机器人操作行为的语义框架表示，并引入了语义框架执行和本地化问题。

    Recent advances in robotic mobile manipulation have spurred the expansion of the operating environment for robots from constrained workspaces to large-scale, human environments. In order to effectively complete tasks in these spaces, robots must be able to perceive, reason, and execute over a diversity of affordances, well beyond simple pick-and-place. We posit the notion of semantic frames provides a compelling representation for robot actions that is amenable to action-focused perception, task-level reasoning, action-level execution, and integration with language. Semantic frames, a product of the linguistics community, define the necessary elements, pre- and post- conditions, and a set of sequential robot actions necessary to successfully execute an action evoked by a verb phrase. In this work, we extend the semantic frame representation for robot manipulation actions and introduce the problem of Semantic Frame Execution And Localization for Perceiving Afforded Robot Actions (SEAL) 
    
[^13]: 合作多智能体任务中学习奖励机制

    Learning Reward Machines in Cooperative Multi-Agent Tasks. (arXiv:2303.14061v1 [cs.AI])

    [http://arxiv.org/abs/2303.14061](http://arxiv.org/abs/2303.14061)

    本文提出了一种新的多智能体强化学习方法，将合作任务分解与学习奖励机制相结合。该方法有助于解决部分可观察环境中奖励的非马尔可夫性质，并提高了学习策略的可解释性，同时也降低了合作任务的复杂性。

    

    本文提出了一种新颖的多智能体强化学习方法，将合作任务分解与学习奖励机制相结合，以编码子任务的结构。该方法有助于应对部分观测环境中奖励的非马尔可夫性质，并提高所学习的策略的可解释性，以完成合作任务。与每个子任务相关联的奖励机制是以分散的方式学习的，然后用于指导每个智能体的行为。通过这样做，合作任务的复杂性得到了降低，从而更有效地学习。结果表明，我们的方法是未来在多智能体强化学习研究中的一个有 promising 的方向，特别是在具有大状态空间和多个智能体的复杂环境中。

    This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.
    
[^14]: 机器人辅助治疗中的复杂决策传达

    Communicating Complex Decisions in Robot-Assisted Therapy. (arXiv:2303.14054v1 [cs.RO])

    [http://arxiv.org/abs/2303.14054](http://arxiv.org/abs/2303.14054)

    社交辅助机器人（SAR）在治疗中发挥重要作用，应该考虑将透明沟通组件纳入设计中去促进人-机交互。

    

    在治疗场景中，社交辅助机器人（SAR）作为决策指导或激励伴侣已经显示出极大潜力。在人与人之间的治疗中，专家通常会沟通他们作出决策的思考过程以促进透明度和建立信任。随着研究旨在将更复杂的决策模型纳入这些机器人以推动更好的交互，SAR解释其决策的能力变得越来越具有挑战性。我们展示了最新的复杂SAR决策制定者的例子。我们认为，基于人与人之间治疗中透明沟通的重要性，SAR应该将这些组件纳入其设计之中。为了刺激对这个话题的讨论，我们提出了一组设计考虑因素供研究人员参考。

    Socially Assistive Robots (SARs) have shown promising potential in therapeutic scenarios as decision-making instructors or motivational companions. In human-human therapy, experts often communicate the thought process behind the decisions they make to promote transparency and build trust. As research aims to incorporate more complex decision-making models into these robots to drive better interaction, the ability for the SAR to explain its decisions becomes an increasing challenge. We present the latest examples of complex SAR decision-makers. We argue that, based on the importance of transparent communication in human-human therapy, SARs should incorporate such components into their design. To stimulate discussion around this topic, we present a set of design considerations for researchers.
    
[^15]: SPEC: 低资源抽象文摘的总结偏好分解

    SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization. (arXiv:2303.14011v1 [cs.CL])

    [http://arxiv.org/abs/2303.14011](http://arxiv.org/abs/2303.14011)

    本文提出了一种低资源抽象文摘的总结偏好分解的方法，以在只有少量示例的情况下学习生成器。其中，以预训练的语言模型为基础，提出了一种元学习框架，将少量样本的学习过程从源语料库转移到目标语料库。

    

    神经抽象摘要已经被广泛研究，并在大规模语料库中取得了巨大成功。 然而，注释数据的相当高的成本促使我们需要在低资源环境下学习策略。 本文研究了只有少量示例情况下学习摘要生成器的问题，并提出了相应的改进方法。

    Neural abstractive summarization has been widely studied and achieved great success with large-scale corpora. However, the considerable cost of annotating data motivates the need for learning strategies under low-resource settings. In this paper, we investigate the problems of learning summarizers with only few examples and propose corresponding methods for improvements. First, typical transfer learning methods are prone to be affected by data properties and learning objectives in the pretext tasks. Therefore, based on pretrained language models, we further present a meta learning framework to transfer few-shot learning processes from source corpora to the target corpus. Second, previous methods learn from training examples without decomposing the content and preference. The generated summaries could therefore be constrained by the preference bias in the training set, especially under low-resource settings. As such, we propose decomposing the contents and preferences during learning th
    
[^16]: 高风险 AI 的团队监管：团队在循环中

    'Team-in-the-loop' organisational oversight of high-stakes AI. (arXiv:2303.14007v1 [cs.CY])

    [http://arxiv.org/abs/2303.14007](http://arxiv.org/abs/2303.14007)

    本论文通过对团队在 AI 系统中的监管流程的纵向观察，探讨了 AI 系统对临床决策制定中团队监管的影响，研究发现此前的专业团队监管方法主要依靠解释和问询来获取信息，而 AI 的引入将可能在信息披露和决策制定方面造成一定程度的影响。

    

    监管对于高风险公共部门 AI 应用程序至关重要，因为决策可能会对个人和集体产生深远影响。目前在公共部门中关于 AI 监管机制的许多思考都围绕着人类决策者处于 "循环中 "这一概念，并且能够干预以防止错误和潜在危害。然而，在许多高风险公共部门背景下，决策的运营监管是由专业团队而不是个人进行的。部署的 AI 系统如何整合到这些现有的团队监管流程中，尚未引起太多注意。我们通过制度分析探讨 AI 对临床决策制定的现有监管的影响，填补该方面的空白。我们发现，现有的监管嵌套在专业培训要求中，并且在征询关键信息时 heavilyrely  于解释和提问。专业团队使用各种会计披露技术来警告同事和监管行为。我们考虑了在 AI 系统引入到现有的团队监管流程中，信息披露和决策制定可能发生改变的几种方式。

    Oversight is rightly recognised as vital within high-stakes public sector AI applications, where decisions can have profound individual and collective impacts. Much current thinking regarding forms of oversight mechanisms for AI within the public sector revolves around the idea of human decision makers being 'in-the-loop' and thus being able to intervene to prevent errors and potential harm. However, in a number of high-stakes public sector contexts, operational oversight of decisions is made by expert teams rather than individuals. The ways in which deployed AI systems can be integrated into these existing operational team oversight processes has yet to attract much attention. We address this gap by exploring the impacts of AI upon pre-existing oversight of clinical decision-making through institutional analysis. We find that existing oversight is nested within professional training requirements and relies heavily upon explanation and questioning to elicit vital information. Professio
    
[^17]: PowerPruning: 针对功耗优化的神经网络权重和激活选择方法

    PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration. (arXiv:2303.13997v1 [cs.NE])

    [http://arxiv.org/abs/2303.13997](http://arxiv.org/abs/2303.13997)

    PowerPruning是一种新颖的方法，通过选择能减少神经网络硬件中MAC操作功耗的权重来优化效率。该方法可以将DNN在硬件上的功耗降低高达78.3％，同时没有显著的准确度损失。

    

    深度神经网络在多个领域都获得了成功应用。然而，在将这些网络部署到边缘设备上，尤其是功耗问题上仍然是一个关键挑战，其中最主要的因素是大量的乘加（MAC）操作。为解决这个问题，本文提出了PowerPruning方法，通过选择导致MAC操作消耗更少功耗的权重来减少数字神经网络加速器的功耗。在此基础上，对所选权重及其与所有激活转换的时序特征进行评估，挑选出在引起较小延迟的权重和激活。因此，即使不修改MAC单元，MAC单元中敏感电路路径的最大延迟也将被减小，从而允许灵活缩小供电电压以进一步减少功耗。结合重新训练，本方法可以在不显著降低准确度的情况下，将DNN在硬件上的功耗降低高达78.3％。

    Deep neural networks (DNNs) have been successfully applied in various fields. A major challenge of deploying DNNs, especially on edge devices, is power consumption, due to the large number of multiply-and-accumulate (MAC) operations. To address this challenge, we propose PowerPruning, a novel method to reduce power consumption in digital neural network accelerators by selecting weights that lead to less power consumption in MAC operations. In addition, the timing characteristics of the selected weights together with all activation transitions are evaluated. The weights and activations that lead to small delays are further selected. Consequently, the maximum delay of the sensitized circuit paths in the MAC units is reduced even without modifying MAC units, which thus allows a flexible scaling of supply voltage to reduce power consumption further. Together with retraining, the proposed method can reduce power consumption of DNNs on hardware by up to 78.3% with only a slight accuracy loss
    
[^18]: 利用可达性分析激活自动驾驶汽车的物理后门触发器

    Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis. (arXiv:2303.13992v1 [cs.CR])

    [http://arxiv.org/abs/2303.13992](http://arxiv.org/abs/2303.13992)

    本研究的方法通过可达性分析识别出自动驾驶汽车可能被物理触发器激活的危险区域和可达路径，测试结果显示成功率接近100％。这将有助于发现AV的漏洞并实施有效的安全策略。

    

    最近的研究揭示了自动驾驶汽车（AV）可能被隐藏的后门操纵，导致它们在被物理触发器激活时执行有害操作。然而，仍不清楚这些触发器如何在遵守交通规则的情况下被激活。在动态交通环境中了解这种漏洞是至关重要的。本研究通过将物理触发器激活视为可控动态系统的可达性问题来填补这一空白。我们的技术识别交通系统中的安全关键区域，可以到达事故触发条件，并提供到达这些条件的意图轨迹。在典型交通场景中的测试表明，该系统可以成功地推动触发器条件，触发率接近100％。我们的方法有助于识别AV的脆弱性，并启用有效的安全策略。

    Recent studies reveal that Autonomous Vehicles (AVs) can be manipulated by hidden backdoors, causing them to perform harmful actions when activated by physical triggers. However, it is still unclear how these triggers can be activated while adhering to traffic principles. Understanding this vulnerability in a dynamic traffic environment is crucial. This work addresses this gap by presenting physical trigger activation as a reachability problem of controlled dynamic system. Our technique identifies security-critical areas in traffic systems where trigger conditions for accidents can be reached, and provides intended trajectories for how those conditions can be reached. Testing on typical traffic scenarios showed the system can be successfully driven to trigger conditions with near 100% activation rate. Our method benefits from identifying AV vulnerability and enabling effective safety strategies.
    
[^19]: 人类与机器内容的释义检测比较研究

    Paraphrase Detection: Human vs. Machine Content. (arXiv:2303.13989v1 [cs.CL])

    [http://arxiv.org/abs/2303.13989](http://arxiv.org/abs/2303.13989)

    本研究分析了不同数据集和检测方法，对人类和机器释义内容进行了比较，发现人类释义难度、多样性和相似性超过机器释义，机器生成的数据集不足。Transformer 是最有效的检测方法，而 SVM-based 方法不及 BERT 和 RoBERTa 变体。

    

    大型语言模型（如 GPT-4 和 ChatGPT）日益重要，但也引起了学术诚信问题，因为存在机器生成的内容和释义。虽然有研究探讨了人类和机器释义内容的检测，但这些类型内容之间的比较仍未得到广泛研究。本文对常用的各种数据集进行了全面分析，并评估了各种检测方法。我们的研究发现，相对于机器生成的数据集缺乏，现有的评估方法在不同数据集上表现各有优劣。我们的主要发现是，人类释义超过机器释义的难度、多样性和相似性，暗示自动生成的文本还没有达到人类水平。Transformer 是最有效的检测方法，BERT 和 RoBERTa 变体在所有数据集上都取得了显著的结果，而基于 SVM 的方法则落后于它们。

    The growing prominence of large language models, such as GPT-4 and ChatGPT, has led to increased concerns over academic integrity due to the potential for machine-generated content and paraphrasing. Although studies have explored the detection of human- and machine-paraphrased content, the comparison between these types of content remains underexplored. In this paper, we conduct a comprehensive analysis of various datasets commonly employed for paraphrase detection tasks and evaluate an array of detection methods. Our findings highlight the strengths and limitations of different detection methods in terms of performance on individual datasets, revealing a lack of suitable machine-generated datasets that can be aligned with human expectations. Our main finding is that human-authored paraphrases exceed machine-generated ones in terms of difficulty, diversity, and similarity implying that automatically generated texts are not yet on par with human-level performance. Transformers emerged a
    
[^20]: 机器心理学：利用心理学方法探究大型语言模型的新兴能力和行为

    Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])

    [http://arxiv.org/abs/2303.13988](http://arxiv.org/abs/2303.13988)

    本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。

    

    大型语言模型（LLM）是将人工智能系统与人类交流和日常生活紧密结合的先锋。由于快速技术进步和其极高的通用性，现今LLM已经拥有数百万用户，并正处于成为主要信息检索、内容生成、问题解决等技术的前沿。因此，对其进行全面评估和审查显得尤为重要。由于当前LLM中出现愈加复杂和新颖的行为模式，可将其视为参与人类心理实验的对象，以便更为全面地评估其能力。为此，本文引入了一个名为"机器心理学"的新兴研究领域。本文概述了各类心理学分支如何为LLM的行为测试提供有用参考。同时，本文规范了机器心理学研究的方法论标准，特别是专注于提示设计政策的制定。此外，它还描述了行为测试结果如何为未来的LLM发展提供指导。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
    
[^21]: 基于分层模仿学习的城市自动驾驶可解释式运动规划器

    Interpretable Motion Planner for Urban Driving via Hierarchical Imitation Learning. (arXiv:2303.13986v1 [cs.RO])

    [http://arxiv.org/abs/2303.13986](http://arxiv.org/abs/2303.13986)

    本文提出一种基于分层模仿学习的运动规划方法，包括高层行为规划和低层轨迹规划。通过在仿真和实际驾驶中的测试，证明了该方法在城市自动驾驶中具有出色的性能。

    

    基于学习的方法在自动驾驶的决策和规划模块上取得了令人瞩目的性能，但神经网络的可靠性和稳定性仍然存在挑战。本文提出了一种分层模仿学习方法，包括高层次的基于网格的行为规划器和低层次的轨迹规划器，不仅是一种个体的数据驱动驾驶策略，而且还可以轻松地嵌入基于规则的架构中。我们在闭环仿真和实际驾驶中评估了我们的方法，并证明神经网络规划器在复杂的城市自动驾驶场景中具有出色的性能。

    Learning-based approaches have achieved impressive performance for autonomous driving and an increasing number of data-driven works are being studied in the decision-making and planning module. However, the reliability and the stability of the neural network is still full of challenges. In this paper, we introduce a hierarchical imitation method including a high-level grid-based behavior planner and a low-level trajectory planner, which is not only an individual data-driven driving policy and can also be easily embedded into the rule-based architecture. We evaluate our method both in closed-loop simulation and real world driving, and demonstrate the neural network planner has outstanding performance in complex urban autonomous driving scenarios.
    
[^22]: AssetField：地面特征平面表示中的资产采掘和重构

    AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation. (arXiv:2303.13953v1 [cs.CV])

    [http://arxiv.org/abs/2303.13953](http://arxiv.org/abs/2303.13953)

    本文提出了AssetField，一种新型的神经场景表示方法，通过学习一组对象感知的地面特征平面来表示场景，可实现各种操作以配置新场景，并提供了可解释的联合可视化表示，从而更好地理解场景和建模。

    

    室内和室外环境本质上都是有结构和重复性的。传统建模流程通过保持一个存储独特对象模板的资产库，实现了多种功能实用且内存高效的方式。受此启发，我们提出了AssetField，一种新型的神经场景表示方法，它可以学习一组对象感知的地面特征平面来表示场景，在其中可以以无监督的方式构建存储模板特征块的资产库。与现有方法需要对象掩码来查询空间点以进行对象编辑不同，我们的地面特征平面表示在鸟瞰视图中为场景提供了一种自然的可视化方法，允许对对象进行各种操作（例如平移，重复，变形）以配置新场景。配合使用特征块模板，对于有许多重复物品的场景，可以启用群组编辑，避免在个别对象上重复性工作。我们展示了AssetField不仅实现了很好的性能，而且还提供了一种可解释的联合可视化表示，从而更好地理解场景和建模。

    Both indoor and outdoor environments are inherently structured and repetitive. Traditional modeling pipelines keep an asset library storing unique object templates, which is both versatile and memory efficient in practice. Inspired by this observation, we propose AssetField, a novel neural scene representation that learns a set of object-aware ground feature planes to represent the scene, where an asset library storing template feature patches can be constructed in an unsupervised manner. Unlike existing methods which require object masks to query spatial points for object editing, our ground feature plane representation offers a natural visualization of the scene in the bird-eye view, allowing a variety of operations (e.g. translation, duplication, deformation) on objects to configure a new scene. With the template feature patches, group editing is enabled for scenes with many recurring items to avoid repetitive work on object individuals. We show that AssetField not only achieves com
    
[^23]: 知识图谱：机遇与挑战

    Knowledge Graphs: Opportunities and Challenges. (arXiv:2303.13948v1 [cs.AI])

    [http://arxiv.org/abs/2303.13948](http://arxiv.org/abs/2303.13948)

    本文系统地概述了知识图谱的机遇和挑战。它们在AI系统和潜在应用领域方面具有广泛的机会。然而，知识图谱的技术挑战包括嵌入、获取、补全、融合和推理。

    

    随着人工智能（AI）和大数据的爆炸式增长，适当地组织和代表海量知识变得至关重要。作为图形数据，知识图谱积累和传达了真实世界的知识。已经广为人知，知识图谱有效地代表了复杂的信息，因此近年来很快引起了学术界和工业界的关注。因此，为了更深入地了解知识图谱，本文对该领域进行了系统的概述。具体而言，我们关注知识图谱的机遇与挑战。我们首先从两个方面回顾了知识图谱的机会：（1）建立在知识图谱基础上的AI系统；（2）知识图谱的潜在应用领域。然后，我们详细讨论了该领域中的严峻技术挑战，如知识图谱嵌入、知识获取、知识图谱补全、知识融合和知识推理。

    With the explosive growth of artificial intelligence (AI) and big data, it has become vitally important to organize and represent the enormous volume of knowledge appropriately. As graph data, knowledge graphs accumulate and convey knowledge of the real world. It has been well-recognized that knowledge graphs effectively represent complex information; hence, they rapidly gain the attention of academia and industry in recent years. Thus to develop a deeper understanding of knowledge graphs, this paper presents a systematic overview of this field. Specifically, we focus on the opportunities and challenges of knowledge graphs. We first review the opportunities of knowledge graphs in terms of two aspects: (1) AI systems built upon knowledge graphs; (2) potential application fields of knowledge graphs. Then, we thoroughly discuss severe technical challenges in this field, such as knowledge graph embeddings, knowledge acquisition, knowledge graph completion, knowledge fusion, and knowledge r
    
[^24]: 软件开发教育中的生成式 AI 助手

    Generative AI Assistants in Software Development Education. (arXiv:2303.13936v1 [cs.SE])

    [http://arxiv.org/abs/2303.13936](http://arxiv.org/abs/2303.13936)

    本文探讨了当前软件开发行业采用生成式 AI（GAI）助手进行软件开发的现状和挑战，提出了未来软件开发教育的愿景和教学建议。

    

    软件开发行业正在进行一次潜在的颠覆性的范式变革——采用生成式 AI（GAI）助手进行软件开发。虽然 AI 已经在软件工程的各个领域中被使用，但是像 GitHub Copilot 和 ChatGPT 这样的 GAI 技术已经激发了许多人的想象力（和恐惧）。尽管目前尚不清楚该行业将如何采用和适应这些技术，但微软（GitHub、必应）和谷歌（Bard）等大型软件公司将这些技术整合到更广泛的行业中的举动是明确的意图和方向。我们与行业专业人士进行了探索性访谈，以了解当前的实践和挑战，将其纳入我们对未来软件开发教育的愿景，并提出了一些教学建议。

    The software development industry is amid another potentially disruptive paradigm change--adopting the use of generative AI (GAI) assistants for software development. Whilst AI is already used in various areas of software engineering, GAI technologies, such as GitHub Copilot and ChatGPT, have ignited the imaginations (and fears) of many people. Whilst it is unclear how the industry will adopt and adapt to these technologies, the move to integrate these technologies into the wider industry by large software companies, such as Microsoft (GitHub, Bing) and Google (Bard), is a clear indication of intent and direction. We performed exploratory interviews with industry professionals to understand current practices and challenges, which we incorporate into our vision of a future of software development education and make some pedagogical recommendations.
    
[^25]: 基于图表示和变点检测方法的符号音乐结构分析

    Symbolic Music Structure Analysis with Graph Representations and Changepoint Detection Methods. (arXiv:2303.13881v1 [cs.SD])

    [http://arxiv.org/abs/2303.13881](http://arxiv.org/abs/2303.13881)

    该论文提出了两个新颖的基于图的算法，Norm和G-PELT，以及G-Window方法，用于通过其形式或结构对符号音乐进行分割。研究发现，使用图表示对符号音乐进行编码，并计算从图中获得的邻接矩阵的新颖性能很好地表示符号音乐片段的结构，并能够检测到边界。

    

    音乐结构分析是音乐信息检索中的一个开放性研究任务。过去有几项工作试图将音乐分割成音频和符号领域，然而，在不同层面上识别和分割音乐结构仍是这个领域中的一个开放性研究问题。在这项工作中，我们提出了三种方法，其中两种是新颖的基于图的算法，旨在通过其形式或结构对符号音乐进行分割：Norm、G-PELT和G-Window。我们对两个具有不同形式或结构的公共数据集进行了削弱研究，以比较这些方法，改变其参数值，并将性能与不同的音乐风格进行比较。我们发现，用图表示对符号音乐进行编码并计算从图中获得的邻接矩阵的新颖性能很好地表示符号音乐片段的结构，无需从中提取特征。我们能够检测到边界。

    Music Structure Analysis is an open research task in Music Information Retrieval (MIR). In the past, there have been several works that attempt to segment music into the audio and symbolic domains, however, the identification and segmentation of the music structure at different levels is still an open research problem in this area. In this work we propose three methods, two of which are novel graph-based algorithms that aim to segment symbolic music by its form or structure: Norm, G-PELT and G-Window. We performed an ablation study with two public datasets that have different forms or structures in order to compare such methods varying their parameter values and comparing the performance against different music styles. We have found that encoding symbolic music with graph representations and computing the novelty of Adjacency Matrices obtained from graphs represent the structure of symbolic music pieces well without the need to extract features from it. We are able to detect the bounda
    
[^26]: 基于查询的视频表示用于时刻检索和亮点检测

    Query-Dependent Video Representation for Moment Retrieval and Highlight Detection. (arXiv:2303.13874v1 [cs.CV])

    [http://arxiv.org/abs/2303.13874](http://arxiv.org/abs/2303.13874)

    该论文提出了一个基于查询的视频表示来解决视频时刻检索和亮点检测中存在的查询信息利用不充分的问题，该方法在编码模块中采用交叉注意力层将文本查询的上下文注入视频表示中。

    

    最近，随着对视频理解的需求急剧增加，视频时刻检索（MR）和亮点检测（HD）备受瞩目。MR / HD的关键目标是定位时刻并估计剪辑级别的符合程度，即突出显示的分数，以给定的文本查询为依据。尽管最近的基于Transformer的模型带来了一些进步，但我们发现这些方法没有充分利用给定查询的信息。例如，在预测时刻及其显著性时，有时会忽略文本查询和视频内容之间的相关性。为了解决这个问题，我们引入了一种针对MR / HD量身定制的检测Transformer——Query-Dependent DETR（QD-DETR）。由于我们观察到给定查询在Transformer架构中的重要性不太明显，我们的编码模块从交叉注意力层开始，以明确将文本查询的上下文注入视频表示。然后，为了增强模型利用查询信息的能力，我们操纵视频

    Recently, video moment retrieval and highlight detection (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query. Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insignificant role of a given query in transformer architectures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video representation. Then, to enhance the model's capability of exploiting the query information, we manipulate the video
    
[^27]: Fantasia3D: 用于高质量文本生成3D内容的几何和外观分离方法

    Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])

    [http://arxiv.org/abs/2303.13873](http://arxiv.org/abs/2303.13873)

    Fantasia3D是一种新的文本生成3D内容的方法，通过分离几何和外观建模和学习，提高了几何细节和逼真渲染，并更有效和高效。

    

    最近，由于预训练的大型语言模型和图像扩散模型的提供，自动3D内容的创建取得了快速进展，形成了文本生成3D内容的新兴话题。现有的文本生成3D方法通常使用隐式场景表示，这些表示使用体积渲染将几何和外观耦合在一起，对于恢复更精细的几何和实现照片般逼真的渲染是次优的；因此，它们在产生高质量3D资产方面不够有效。在这项工作中，我们提出了一种名为Fantasia3D的新方法，用于高质量文本生成3D内容。Fantasia3D的关键在于几何和外观的分离建模和学习。对于几何学习，我们依靠混合场景表示，并建议将从表示中提取的表面法线编码作为图像扩散模型的输入。对于外观建模，我们引入了空间可变双向反射率分布函数（SVBRDF）来分离材料和光照属性。实验结果表明，我们的方法在几何细节和逼真渲染方面超越了现有技术，并且在从自然语言文本中生成高质量的3D内容方面更为有效和高效。

    Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function 
    
[^28]: MagicEye：一种智能可穿戴设备，致力于帮助视力受损人士独立生活。

    MagicEye: An Intelligent Wearable Towards Independent Living of Visually Impaired. (arXiv:2303.13863v1 [cs.HC])

    [http://arxiv.org/abs/2303.13863](http://arxiv.org/abs/2303.13863)

    MagicEye是一种智能可穿戴设备，能够识别日常生活中的室内外物体，提供针对视力受损人士的面部识别和货币识别模块，以及用于导航的GPS传感器。

    

    视觉障碍个体在日常生活中常常面临许多挑战性障碍，如视觉障碍可能严重影响一个人的工作、导航和独立生活能力。为了应对这些挑战，我们介绍了一种最新的智能可穿戴设备MagicEye，旨在帮助视力受损人士。MagicEye采用了经过定制训练的基于CNN的物体检测模型，能够识别日常生活中经常遇到的室内外各种物体。该神经网络共计35个类别，旨在实现高效、高精度的物体检测。此外，该设备还配备了面部识别和货币识别模块，为视力受损人士提供宝贵的帮助。此外，MagicEye还具有用于导航的GPS传感器。

    Individuals with visual impairments often face a multitude of challenging obstacles in their daily lives. Vision impairment can severely impair a person's ability to work, navigate, and retain independence. This can result in educational limits, a higher risk of accidents, and a plethora of other issues. To address these challenges, we present MagicEye, a state-of-the-art intelligent wearable device designed to assist visually impaired individuals. MagicEye employs a custom-trained CNN-based object detection model, capable of recognizing a wide range of indoor and outdoor objects frequently encountered in daily life. With a total of 35 classes, the neural network employed by MagicEye has been specifically designed to achieve high levels of efficiency and precision in object detection. The device is also equipped with facial recognition and currency identification modules, providing invaluable assistance to the visually impaired. In addition, MagicEye features a GPS sensor for navigatio
    
[^29]: ChatGPT在元宇宙中的应用：拯救者还是毁灭者?

    Unleasing ChatGPT on the Metaverse: Savior or Destroyer?. (arXiv:2303.13856v1 [cs.HC])

    [http://arxiv.org/abs/2303.13856](http://arxiv.org/abs/2303.13856)

    本文探讨了在元宇宙中使用ChatGPT的利弊，它可以提供动态和个性化的体验，但也必须考虑隐私、偏见和道德等相关问题。

    

    随着虚拟现实技术的不断发展，“元宇宙”这个新兴领域的交互方式和沉浸体验越来越重要。而人工智能技术和自然语言处理技术在这一领域中的应用变得日益重要。其中，一个被广泛使用的工具是ChatGPT，这是OpenAI训练的一个大型语言模型。本文详细探讨了在元宇宙中引入ChatGPT的利弊，包括教育、娱乐、个性化和支持等方面的应用。虽然这项技术可以提供动态和个性化的体验，但也必须考虑隐私、偏见和道德等相关问题。本文旨在通过评估这些机遇和障碍，帮助读者理解ChatGPT对元宇宙可能产生的影响，以及如何有效地利用它创建更加沉浸和有趣的虚拟环境。

    The incorporation of artificial intelligence (AI) technology, and in particular natural language processing (NLP), is becoming increasingly vital for the development of immersive and interactive metaverse experiences. One such artificial intelligence tool that is gaining traction in the metaverse is ChatGPT, a large language model trained by OpenAI. The article delves into the pros and cons of utilizing ChatGPT for metaverse-based education, entertainment, personalization, and support. Dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. This article aims to help readers understand the possible influence of ChatGPT on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.
    
[^30]: 神经网络中的因果归因学习：超越直接影响

    Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])

    [http://arxiv.org/abs/2303.13850](http://arxiv.org/abs/2303.13850)

    本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。

    

    近年来，捕捉和维护神经网络模型中的因果关系引起了越来越多的关注。本文研究了用因果方法估计和维护神经网络模型中的输入-输出属性。特别地，现有的研究仅假设输入变量独立（由于神经网络架构），因此仅研究直接影响。我们将神经网络视为结构性因果模型，并提出在输入特征中引入边缘以捕捉和维护直接和间接因果效应的简单而有效的方法，同时训练神经网络模型。我们还提出有效的近似策略来量化高维数据的因果归因。我们在合成和真实数据集上进行了各种实验，结果表明，所提出的方法学习了接近基本事实效果的直接和间接因果归因。

    There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
    
[^31]: 交通模拟中的可编辑驾驶角色：社交可控行为生成

    Editing Driver Character: Socially-Controllable Behavior Generation for Interactive Traffic Simulation. (arXiv:2303.13830v1 [cs.RO])

    [http://arxiv.org/abs/2303.13830](http://arxiv.org/abs/2303.13830)

    为了评估自动驾驶汽车在不同社交特征的交互交通场景下采取安全有效的机动方式，该论文提出了一种社交可控行为生成（SCBG）模型，通过学习真实驾驶数据实现了逼真而类人的轨迹生成，并允许用户指定轨迹的礼貌程度。

    

    交通模拟在评估和改进自动驾驶规划系统中起着至关重要的作用。在公共道路上部署自动驾驶汽车后，需要与具有不同社交偏好（例如，自私或彬彬有礼的人类驾驶员）的人类道路参与者进行交互。为了确保自动驾驶汽车在不同的交互交通场景中采取安全有效的机动方式，我们应该能够在模拟环境中评估自动驾驶汽车与带有不同社交特征的反应代理之间的差异。为此，我们提出了一种社交可控行为生成（SCBG）模型，它允许用户指定生成轨迹的礼貌程度，并通过从真实驾驶数据中学习来确保逼真和类人的轨迹生成。具体而言，我们定义了一种新颖的可微度量，用于量化驾驶行为的礼貌程度，并利用边际和条件行为预测模型。

    Traffic simulation plays a crucial role in evaluating and improving autonomous driving planning systems. After being deployed on public roads, autonomous vehicles need to interact with human road participants with different social preferences (e.g., selfish or courteous human drivers). To ensure that autonomous vehicles take safe and efficient maneuvers in different interactive traffic scenarios, we should be able to evaluate autonomous vehicles against reactive agents with different social characteristics in the simulation environment. We propose a socially-controllable behavior generation (SCBG) model for this purpose, which allows the users to specify the level of courtesy of the generated trajectory while ensuring realistic and human-like trajectory generation through learning from real-world driving data. Specifically, we define a novel and differentiable measure to quantify the level of courtesy of driving behavior, leveraging marginal and conditional behavior prediction models t
    
[^32]: $k$NN提示：无需校准的最近相邻推理，超越上下文学习

    $k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference. (arXiv:2303.13824v1 [cs.CL])

    [http://arxiv.org/abs/2303.13824](http://arxiv.org/abs/2303.13824)

    本文提出$k$NN提示，一种不需要校准就可以推理最近相邻的算法，用来解决上下文学习的限制和偏见问题，显着优于最先进的方法。

    

    在上下文学习中，将目标任务制定为在上下文演示的条件下完成提示完成，已成为LLM的主要用途。本文首先披露了这种典型用法的实际问题，由于上下文长度的限制，它无法随着训练数据扩展。此外，现有的研究表明，ICL还受到各种偏见的影响，并需要精细的校准处理。为了解决这两个挑战，我们提出了一种简单有效的解决方案，$k$NN提示，它首先使用训练数据查询LLM的分布式表示，然后通过简单地参考最近邻来预测测试实例。我们进行了全面的实验来证明其优越性：1）无需校准：$k$NN提示不直接将LLM输出分布与特定任务标签空间对准，而是利用这种分布将测试和训练实例对准。它显着优于最先进的方法。

    In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art ca
    
[^33]: 一种新的模型：对人机信任问卷的元分析

    Converging Measures and an Emergent Model: A Meta-Analysis of Human-Automation Trust Questionnaires. (arXiv:2303.13799v1 [cs.HC])

    [http://arxiv.org/abs/2303.13799](http://arxiv.org/abs/2303.13799)

    本文通过元分析提出了一种共识模型，以测量人机交互中的信任，并确定了最常被引用和最好验证的人机和人机器人信任问卷以及相关因素。

    

    测量人机信任的一个显著挑战是存在大量的构建、模型和问卷，这些问卷验证高度变化。然而，所有人都同意，信任是技术接受、持续使用、流利度和团队合作的关键元素。在本研究中，我们通过对经过验证和可靠的信任调查工具进行元分析，综合了人机交互中信任的共识模型。为了实现这个目标，这项工作确定了最常被引用和最好验证的人机和人机器人信任问卷，以及形成这种信任的维度和前因。为了减少混淆和构建扩散，我们提供了问卷术语之间的详细映射。此外，我们对使用多因素调查工具的实验中出现的回归模型进行了元分析。基于这个元分析，

    A significant challenge to measuring human-automation trust is the amount of construct proliferation, models, and questionnaires with highly variable validation. However, all agree that trust is a crucial element of technological acceptance, continued usage, fluency, and teamwork. Herein, we synthesize a consensus model for trust in human-automation interaction by performing a meta-analysis of validated and reliable trust survey instruments. To accomplish this objective, this work identifies the most frequently cited and best-validated human-automation and human-robot trust questionnaires, as well as the most well-established factors, which form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models that emerged from those experiments which used multi-factorial survey instruments. Based on this meta-analysis, 
    
[^34]: 通过病人准则级别的公平性约束实现公平的病人-试验匹配

    Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint. (arXiv:2303.13790v1 [cs.LG])

    [http://arxiv.org/abs/2303.13790](http://arxiv.org/abs/2303.13790)

    本文提出了一个公平的病人-试验匹配框架，通过生成病人准则级别的公平性约束，考虑了包含和排除标准的嵌入差异，有效地解决了病人-试验匹配中的不公平性问题。

    

    临床试验在新型治疗方法的开发中不可或缺，但由于招募和留存病人的难度，往往难以招募足够数量的参与者。为了解决这些挑战，已经创建了基于深度学习框架的病人-试验匹配方法。这些框架会计算病人和临床试验的相似度，考虑包含和排除标准之间的差异。近期的研究表明这些框架的性能优于早期的方法。然而，深度学习模型可能会导致病人-试验匹配中的公平性问题，当某些敏感人群在临床试验中被低估时，可能会导致数据不完整或不准确，从而对患者造成潜在危害。针对这个问题，本文提出了一个公平的病人-试验匹配框架，通过生成一个病人准则级别的公平性约束来解决不公平性问题。所提出的框架考虑了包含和排除标准的嵌入差异，基于嵌入差异制定了公平性约束。该框架的有效性在一个真实的临床试验数据集上得到了验证。

    Clinical trials are indispensable in developing new treatments, but they face obstacles in patient recruitment and retention, hindering the enrollment of necessary participants. To tackle these challenges, deep learning frameworks have been created to match patients to trials. These frameworks calculate the similarity between patients and clinical trial eligibility criteria, considering the discrepancy between inclusion and exclusion criteria. Recent studies have shown that these frameworks outperform earlier approaches. However, deep learning models may raise fairness issues in patient-trial matching when certain sensitive groups of individuals are underrepresented in clinical trials, leading to incomplete or inaccurate data and potential harm. To tackle the issue of fairness, this work proposes a fair patient-trial matching framework by generating a patient-criterion level fairness constraint. The proposed framework considers the inconsistency between the embedding of inclusion and e
    
[^35]: 基于图神经网络的纳米卫星任务调度方法：学习混合整数模型的洞见

    A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])

    [http://arxiv.org/abs/2303.13773](http://arxiv.org/abs/2303.13773)

    本研究提出基于GNN的纳米卫星任务调度方法，以更好地优化服务质量，解决ONTS问题的复杂性。

    

    本研究探讨如何利用图神经网络（GNN）更有效地调度纳米卫星任务。在离线纳米卫星任务调度（ONTS）问题中，目标是找到在轨道上执行任务的最佳安排，同时考虑服务质量（QoS）方面的考虑因素，如优先级，最小和最大激活事件，执行时间框架，周期和执行窗口，以及卫星电力资源和能量收集和管理的复杂性的约束。ONTS问题已经使用传统的数学公式和精确方法进行了处理，但是它们在问题的挑战性案例中的适用性有限。本研究考察了在这种情况下使用GNN的方法，该方法已经成功应用于许多优化问题，包括旅行商问题，调度问题和设施放置问题。在本文中，我们将ONTS问题的MILP实例完全表示成二分图网络结构来应用GNN。

    This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
    
[^36]: 未知嗅探器用于目标检测：不要对未知对象视而不见

    Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])

    [http://arxiv.org/abs/2303.13769](http://arxiv.org/abs/2303.13769)

    本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。

    

    近期提出的开放世界目标和开放集检测在寻找从未见过的物体并将其与已知类别区分开方面取得了突破。然而，他们对从已知类别向未知类别的知识传递的研究需要更深入，从而导致探测隐藏在背景中的未知物体的能力不足。本文中，我们提出了未知嗅探器(UnSniffer)来寻找未知和已知的目标。首先，引入广义物体置信度(GOC)分数，仅使用已知类别样本进行监督和避免在背景中不适当地压制未知物体。值得注意的是，从已知物体学习到的这种置信度分数可以推广到未知物体。此外，我们提出了负能量抑制损失来进一步限制背景中非物体样本。接下来，在推断过程中由于缺乏它们在训练中的语义信息，难以获得每个未知目标的最佳框。为了解决这个问题，

    The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
    
[^37]: 学习时空隐式神经表示来实现事件引导的视频超分辨率

    Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution. (arXiv:2303.13767v1 [cs.CV])

    [http://arxiv.org/abs/2303.13767](http://arxiv.org/abs/2303.13767)

    本文提出了一种新的框架，利用事件的时空插值来实现随机尺度下的视频超分辨率。方法包括从RGB帧和事件的查询空间-时间坐标和特征中学习隐式神经表示。

    

    事件相机异步感知强度变化，产生具有高动态范围和低延迟的事件流。这激发了利用事件引导具有挑战性的视频超分辨率（VSR）任务的研究。在本文中，我们首次尝试利用事件的高时序分辨率性质，通过利用事件的时空插值来实现随机尺度下的VSR。当引导VSR时，事件的时空信息的表示具有困难。为此，我们提出了一个新的框架，将事件的时空插值与VSR结合在一个统一的框架中。我们的关键思想是从RGB帧和事件的查询空间-时间坐标和特征中学习隐式神经表示。我们的方法分为三部分。具体而言，空时融合（STF）模块首先学习事件和RGB帧的3D特征。然后，时域滤波器（TF）模块解锁了更多特征并给出了精细的VSR结果。最后，我们提供了一个大规模的VSR数据集，以便深度学习的VSR研究人员可以评估其方法。

    Event cameras sense the intensity changes asynchronously and produce event streams with high dynamic range and low latency. This has inspired research endeavors utilizing events to guide the challenging video superresolution (VSR) task. In this paper, we make the first attempt to address a novel problem of achieving VSR at random scales by taking advantages of the high temporal resolution property of events. This is hampered by the difficulties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpolation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks mo
    
[^38]: 无需边缘但具有结构感知性：从GNN到MLP的原型引导知识蒸馏。

    Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])

    [http://arxiv.org/abs/2303.13763](http://arxiv.org/abs/2303.13763)

    本文提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。

    

    将高精度的图神经网络（GNN）在图任务中压缩成低延迟的多层感知器（MLP）已成为热门研究课题。以前的方法会将图的边缘处理成额外的输入给MLP，但这样的图结构对于各种场景可能无法获得。因此，我们提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。具体而言，我们分析了GNN教师中的图形结构信息，并通过原型在无边缘设置中从GNN到MLP进行了知识蒸馏。在流行的图形基准实验中的实验结果表明了所提出的PGKD方法的有效性和鲁棒性。

    Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
    
[^39]: 结构不平衡感知的图增强学习

    Structural Imbalance Aware Graph Augmentation Learning. (arXiv:2303.13757v1 [cs.LG])

    [http://arxiv.org/abs/2303.13757](http://arxiv.org/abs/2303.13757)

    本文提出了一种选择性图增强方法 (SAug)，通过识别图中的中心节点和尾节点，设计了一种选择性增强策略，可以显著提高图机器学习模型的性能，特别是在学习尾节点时。

    

    图机器学习 (GML) 在节点分类、链接预测、图分类等方面取得了巨大的进展。然而，现实中的图往往具有结构不平衡性，即只有少数几个中心节点具有更密集的局部结构和更高的影响力。这种不平衡可能会损害现有 GML 模型的鲁棒性，特别是在学习尾节点时。本文提出了一种选择性图增强方法 (SAug) 来解决这个问题。首先，设计了一种基于Pagerank的采样策略来识别图中的中心节点和尾节点。其次，提出了一种选择性增强策略，将中心节点的噪声邻居放弃在一侧，并在另一侧发现潜在的邻居和为尾节点生成伪邻居。它还可以减轻两种类型节点之间的结构不平衡。最后，在增强后的图上重新训练 GNN 模型。广泛的实验表明，SAug 可以显著提高现有模型的性能。

    Graph machine learning (GML) has made great progress in node classification, link prediction, graph classification and so on. However, graphs in reality are often structurally imbalanced, that is, only a few hub nodes have a denser local structure and higher influence. The imbalance may compromise the robustness of existing GML models, especially in learning tail nodes. This paper proposes a selective graph augmentation method (SAug) to solve this problem. Firstly, a Pagerank-based sampling strategy is designed to identify hub nodes and tail nodes in the graph. Secondly, a selective augmentation strategy is proposed, which drops the noisy neighbors of hub nodes on one side, and discovers the latent neighbors and generates pseudo neighbors for tail nodes on the other side. It can also alleviate the structural imbalance between two types of nodes. Finally, a GNN model will be retrained on the augmented graph. Extensive experiments demonstrate that SAug can significantly improve the backb
    
[^40]: Sparsifiner：学习稀疏的实例相关注意力用于高效的视觉Transformer

    Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers. (arXiv:2303.13755v1 [cs.CV])

    [http://arxiv.org/abs/2303.13755](http://arxiv.org/abs/2303.13755)

    本文分析了ViT算法的计算成本高的问题，并且通过学习实例相关的注意力模式，来提高计算效率

    

    视觉Transformer相比卷积神经网络在性能方面具有竞争优势，但往往伴随着高计算成本。为此，先前的方法通过限制一定数量的空间相邻令牌来探索不同的注意力模式，以加速ViT的多头自注意力（MHSA）操作。但是，这种结构化的注意力模式将令牌与其空间相关性的令牌之间的令牌 - 令牌连接限制在了一定范围内，这不考虑从完整的注意力掩码中学习的语义连接。在这项工作中，我们提出了一种新方法，通过设计一个轻量级的连接性预测模块来学习实例相关的注意力模式。直观的说，如果认为特征在空间或语义上是相关的，则两个标记具有高的连接得分。由于每个标记只与少量其他标记相关，因此二元化连接掩码通常是有效的 。

    Vision Transformers (ViT) have shown their competitive advantages performance-wise compared to convolutional neural networks (CNNs) though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT's multi-head self-attention (MHSA) operations. However, such structured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned semantic connections from a full attention mask. In this work, we propose a novel approach to learn instance-dependent attention patterns, by devising a lightweight connectivity predictor module to estimate the connectivity score of each pair of tokens. Intuitively, two tokens have high connectivity scores if the features are considered relevant either spatially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often ver
    
[^41]: LONGNN: 具有可学习正交标准基的谱图神经网络

    LONGNN: Spectral GNNs with Learnable Orthonormal Basis. (arXiv:2303.13750v1 [cs.LG])

    [http://arxiv.org/abs/2303.13750](http://arxiv.org/abs/2303.13750)

    本研究提出了一种谱图神经网络LONGNN，它采用可学习正交标准基，并解决了固定多项式基和非归一化基础所带来的缺陷，经实验证明其在各种图数据集上具有优异的表现。

    

    近年来，大量的谱图神经网络（GNN）方法利用可学习系数的多项式基在许多节点级任务上实现了顶级性能。虽然已经探索了各种多项式基，但是每种方法都采用了固定的多项式基，可能不是给定图形的最佳选择。此外，我们确定了这些方法所谓的越界问题，并表明这在它们不太系统化的正则化策略和非归一化基础上有所根源。在本文中，我们首次尝试解决这两个问题。利用雅各比多项式，我们设计了一种新的具有可学习正交标准基的谱GNN，LON-GNN，并证明了正则化系数现在等效于正则化所学滤波函数的范数。我们在多样的图数据集上进行了广泛的实验，以评估LON-GNN的拟合和泛化能力，结果表明其优于几种最先进的方法。

    In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with Learnable OrthoNormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of LON-GNN, where the results imply its superiori
    
[^42]: 端到端扩散潜在优化提高分类器引导能力。

    End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])

    [http://arxiv.org/abs/2303.13703](http://arxiv.org/abs/2303.13703)

    本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。

    

    分类器指导——利用图像分类器的梯度来引导扩散模型的生成——有潜力大幅扩展对图像生成和编辑的创造性控制。然而，目前分类器指导要么需要训练新的噪声感知模型以获得精确的梯度，要么使用一步去噪的近似最终生成物，并导致梯度不对齐和次优控制。我们强调了这种近似的缺点，并提出了一种新的指导方法：直接优化扩散潜变（DOODL），它通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，使用可逆扩散过程实现内存有效的反向传递。展示了更精确指导潜力的 DOODL 在不同形式的指导的计算和人类评估度量上优于一步分类器指导。

    Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
    
[^43]: OFA$^2$: 一种基于多目标的Once-for-All神经架构搜索

    OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search. (arXiv:2303.13683v1 [cs.NE])

    [http://arxiv.org/abs/2303.13683](http://arxiv.org/abs/2303.13683)

    OFA$^2$是一个基于多目标优化的神经架构搜索模型，它通过将搜索阶段构想为一个多目标优化问题，并使用已经训练好的神经网络，从而在多个权衡目标之间找到高效和多样化的子网络。

    

    Once-for-All（OFA）是一个神经架构搜索（NAS）框架，旨在通过分离训练和搜索阶段来解决为不同资源约束的设备搜索高效架构的问题。 Ofa神经网络的训练过程只需要进行一次，然后可以根据每个部署方案从此训练好的网络中提取多个子网络进行多次搜索。本文旨在通过将搜索阶段明确构想为多目标优化问题，进一步寻求效率。 然后使用任何多目标进化算法（例如NSGA-II和SMS-EMOA）在搜索阶段填充Pareto前沿，其中包含具有不同权衡的高效预训练神经结构。换句话说，神经网络只需训练一次，然后以多目标优化的形式执行子网搜索，并获得一组高效、预训练且多样化的子网络。

    Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed to address the problem of searching efficient architectures for devices with different resources constraints by decoupling the training and the searching stages. The computationally expensive process of training the OFA neural network is done only once, and then it is possible to perform multiple searches for subnetworks extracted from this trained network according to each deployment scenario. In this work we aim to give one step further in the search for efficiency by explicitly conceiving the search stage as a multi-objective optimization problem. A Pareto frontier is then populated with efficient, and already trained, neural architectures exhibiting distinct trade-offs among the conflicting objectives. This could be achieved by using any multi-objective evolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA. In other words, the neural network is trained once, the searching for subnetwo
    
[^44]: 呈现团队运动分析中的多智能体挑战

    Presenting Multiagent Challenges in Team Sports Analytics. (arXiv:2303.13660v1 [cs.AI])

    [http://arxiv.org/abs/2303.13660](http://arxiv.org/abs/2303.13660)

    本文将多智能体系统与团队运动分析联系起来，特别考虑入侵式游戏，并将其分为短期比赛策略和长期团队规划两个方向，为MAS的实现和发展提供了机会和挑战。

    

    本文将团队运动分析领域中的若干挑战和机遇与多智能体系统（MAS）的关键研究领域相联系。我们特别考虑入侵式游戏，即球员入侵对方团队领地并可以在球场上的任何地方进行互动的运动项目，如冰球、足球和篮球。我们认为，MAS有能力研究入侵型游戏，并将使MAS和运动分析领域受益。我们的讨论重点强调了MAS实现和进一步开发的两个方向：短期的比赛策略（教练）和长期的团队规划（管理）。

    This paper draws correlations between several challenges and opportunities within the area of team sports analytics and key research areas within multiagent systems (MAS). We specifically consider invasion games, defined as sports where players invade the opposing team's territory and can interact anywhere on a playing surface such as ice hockey, soccer, and basketball. We argue that MAS is well-equipped to study invasion games and will benefit both MAS and sports analytics fields. Our discussion highlights areas for MAS implementation and further development along two axes: short-term in-game strategy (coaching) and long-term team planning (management).
    
[^45]: 建立人工神经电路用于领域通用认知：脑启发式系统级架构入门。

    Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture. (arXiv:2303.13651v1 [cs.NE])

    [http://arxiv.org/abs/2303.13651](http://arxiv.org/abs/2303.13651)

    本文提供了关于构建领域通用人工智能的思路和原则，通过研究生物神经网络和系统级分布网络通信、递归和短期拓扑变化，为建立人工神经网络提供宝贵指导。

    

    目前有一系列的努力致力于建立能够解决广泛认知任务且无需在各个问题空间和领域进行精细调整的通用神经网络模型，以构建领域通用人工智能。为了做到这一点，模型需要适当的先验及归纳偏见，训练得到的模型可以推广到寻找新问题空间的例子。我们提供了生物神经网络赋予其柔性认知功能的标志性概述，并讨论特定的系统层分布网络通信及递归的作用，此外还讨论了短期拓扑变化在高效局部计算方面的作用。在机器学习模型变得越来越复杂的时候，这些原则可能会对这个复杂且动态的领域提供有价值的指导。

    There is a concerted effort to build domain-general artificial intelligence in the form of universal neural network models with sufficient computational flexibility to solve a wide variety of cognitive tasks but without requiring fine-tuning on individual problem spaces and domains. To do this, models need appropriate priors and inductive biases, such that trained models can generalise to out-of-distribution examples and new problem sets. Here we provide an overview of the hallmarks endowing biological neural networks with the functionality needed for flexible cognition, in order to establish which features might also be important to achieve similar functionality in artificial systems. We specifically discuss the role of system-level distribution of network communication and recurrence, in addition to the role of short-term topological changes for efficient local computation. As machine learning models become more complex, these principles may provide valuable directions in an otherwis
    
[^46]: 用启发式规划作为定理证明

    Planning as Theorem Proving with Heuristics. (arXiv:2303.13638v1 [cs.AI])

    [http://arxiv.org/abs/2303.13638](http://arxiv.org/abs/2303.13638)

    该论文介绍了一种用启发式规划作为定理证明的方法，通过开发一种定理证明提升启发式(TPLH)规划器，在情况树中搜索短的计划，并减少探讨的状态数量。

    

    在情境演算中将计划作为定理证明在50年前被放弃，因为这是一个不可能完成的项目。但是我们开发了一种定理证明提升启发式(TPLH)规划器，它使用A*搜索算法在情况树中搜索计划。它由基于删除松弛的与领域无关的启发式控制。我们将TPLH与Fast Downward（FD）和Best First Width Search（BFWS）规划器在几个标准基准测试中进行比较。由于我们的启发式功能实现未经优化，TPLH比FD和BFWS慢。但它会计算出更短的计划，并减少了探讨的状态数量。我们讨论了以前在知识表示和推理领域内进行规划的研究，并确定了相关方向。因此，我们表明情境演算中的演绎式提升启发式规划实际上是可以完成的。

    Planning as theorem proving in situation calculus was abandoned 50 years ago as an impossible project. But we have developed a Theorem Proving Lifted Heuristic (TPLH) planner that searches for a plan in a tree of situations using the A* search algorithm. It is controlled by a delete relaxation-based domain independent heuristic. We compare TPLH with Fast Downward (FD) and Best First Width Search (BFWS) planners over several standard benchmarks. Since our implementation of the heuristic function is not optimized, TPLH is slower than FD and BFWS. But it computes shorter plans, and it explores fewer states. We discuss previous research on planning within KR\&R and identify related directions. Thus, we show that deductive lifted heuristic planning in situation calculus is actually doable.
    
[^47]: 利用信号处理和机器学习高效直接推断心率变异性

    Efficient and Direct Inference of Heart Rate Variability using Both Signal Processing and Machine Learning. (arXiv:2303.13637v1 [cs.LG])

    [http://arxiv.org/abs/2303.13637](http://arxiv.org/abs/2303.13637)

    本论文使用信号处理和机器学习结合的方法，直接推断出心率变异性。在大数据集的评估中，该方法表现出比单独使用信号处理或机器学习更高的准确性。

    

    心率变异性(HRV)测量连续心跳时间的变化，是身心健康的主要指标。最近的研究表明，可以使用光电容积描记仪(PPG)传感器推断HRV。然而，许多先前的研究存在较高的误差，因为它们仅使用信号处理或机器学习(ML)，或者因为它们间接推断HRV，或者因为缺少大数据集。许多先前的研究也可能需要大的ML模型。低的准确率和大的模型大小限制了它们在小型嵌入式设备和未来可能在医疗保健中的应用。为了解决上述问题，我们首先收集了一个大的PPG信号和HRV的基本事实数据集。通过这个数据集，我们开发了结合信号处理和ML的HRV模型，直接推断HRV。评估结果表明，我们的方法的误差在3.5%到25.7%之间，并且优于单独使用信号处理或ML的方法。我们还探讨了不同的模型的大小和计算复杂度的平衡。

    Heart Rate Variability (HRV) measures the variation of the time between consecutive heartbeats and is a major indicator of physical and mental health. Recent research has demonstrated that photoplethysmography (PPG) sensors can be used to infer HRV. However, many prior studies had high errors because they only employed signal processing or machine learning (ML), or because they indirectly inferred HRV, or because there lacks large training datasets. Many prior studies may also require large ML models. The low accuracy and large model sizes limit their applications to small embedded devices and potential future use in healthcare. To address the above issues, we first collected a large dataset of PPG signals and HRV ground truth. With this dataset, we developed HRV models that combine signal processing and ML to directly infer HRV. Evaluation results show that our method had errors between 3.5% to 25.7% and outperformed signal-processing-only and ML-only methods. We also explored differe
    
[^48]: 基于高效传感器采样和学习模型的PPG心率估计

    PPG-based Heart Rate Estimation with Efficient Sensor Sampling and Learning Models. (arXiv:2303.13636v1 [cs.LG])

    [http://arxiv.org/abs/2303.13636](http://arxiv.org/abs/2303.13636)

    本文研究了如何将PPG心率估计技术应用于低功耗和资源受限的嵌入式设备中，在结合信号处理和机器学习的基础上，成功将PPG采样频率降至仅25Hz，并提高了心率估计的精度，同时也减小了机器学习模型的特征大小，使得模型更小。

    

    最近的研究表明，在可穿戴设备中嵌入的PPG传感器可以高精度地估计心率。然而，尽管之前的研究努力，将基于PPG传感器的心率估计应用于嵌入式设备仍面临着高能耗的高频PPG采样和资源密集型的机器学习模型的挑战。为此，我们旨在探索更适合低功耗和资源受限的嵌入式设备的心率估计技术。具体来说，我们希望设计出可以提供高精度心率估计的技术，其采样频率低，模型尺寸小且推断时间快。首先，我们展示了通过结合信号处理和机器学习，可以将PPG采样频率从125 Hz降至仅25 Hz，同时提供更高的心率估计精度。这种组合还有助于减小机器学习模型的特征大小，导致模型更小。此外，我们还呈现了一个全面的

    Recent studies showed that Photoplethysmography (PPG) sensors embedded in wearable devices can estimate heart rate (HR) with high accuracy. However, despite of prior research efforts, applying PPG sensor based HR estimation to embedded devices still faces challenges due to the energy-intensive high-frequency PPG sampling and the resource-intensive machine-learning models. In this work, we aim to explore HR estimation techniques that are more suitable for lower-power and resource-constrained embedded devices. More specifically, we seek to design techniques that could provide high-accuracy HR estimation with low-frequency PPG sampling, small model size, and fast inference time. First, we show that by combining signal processing and ML, it is possible to reduce the PPG sampling frequency from 125 Hz to only 25 Hz while providing higher HR estimation accuracy. This combination also helps to reduce the ML model feature size, leading to smaller models. Additionally, we present a comprehensiv
    
[^49]: 音乐结构的自组织网络分析

    In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])

    [http://arxiv.org/abs/2303.13631](http://arxiv.org/abs/2303.13631)

    本文介绍了一种利用Essential Element Network (EEN)算法将音频编码成文本并进行相关性计算和优化应用于聚类系数的频率和排名的方法，得到了音乐的深层结构信息，为厘清音乐结构提供了新方法。

    

    自然语言中的词汇不仅传递信息，还随着文明和人类迁移而演变。音乐也是如此。为了理解音乐背后的复杂结构，我们引入了一个叫做Essential Element Network (EEN)的算法将音频编码成文本。该网络通过计算音调、时间和音量之间的相关性得到，通过优化EEN算法以生成Zipf定律应用于聚类系数的频率和排名，我们可以将语义关系视为词汇并生成它们的映射。我们将这些编码后的词汇映射到音调-时间空间中，有助于我们系统地组织音乐深层结构中的句法。相比于其他深度学习方法的黑盒子特性，我们的算法提供了对音乐背后复杂网络的精确描述。因此，这些过程积累的经验和属性不仅为此类应用提供了新的方法，同时也为许多其他相关领域的研究提供了探索的路径。

    Words in a natural language not only transmit information but also evolve with the development of civilization and human migration. The same is true for music. To understand the complex structure behind the music, we introduced an algorithm called the Essential Element Network (EEN) to encode the audio into text. The network is obtained by calculating the correlations between scales, time, and volume. Optimizing EEN to generate Zipfs law for the frequency and rank of the clustering coefficient enables us to generate and regard the semantic relationships as words. We map these encoded words into the scale-temporal space, which helps us organize systematically the syntax in the deep structure of music. Our algorithm provides precise descriptions of the complex network behind the music, as opposed to the black-box nature of other deep learning approaches. As a result, the experience and properties accumulated through these processes can offer not only a new approach to the applications of
    
[^50]: 基于人工智能的快速、无标记光学成像分子分类诊断的应用于弥漫性胶质瘤研究

    Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging. (arXiv:2303.13610v1 [cs.CV])

    [http://arxiv.org/abs/2303.13610](http://arxiv.org/abs/2303.13610)

    本文研究了一种基于人工智能的光学成像技术，可以快速、无标记的对弥漫性胶质瘤进行分子诊断，为其治疗提供更加准确的指导。

    

    分子分类的应用使得脑肿瘤的治疗得到转变，使诊断更加准确，治疗更加个性化。然而，对于患有脑肿瘤的病人，及时进行分子诊断测试仍然存在限制，使得手术和副辅助治疗更为复杂，阻撓了临床试验的报名。本文构建了DeepGlioma诊断筛选系统，该系统是一种基于人工智能的快速（<90秒）、无标记的光学成像诊断技术。DeepGlioma使用多模态数据集进行训练，包括刺激拉曼组织学（SRH）和大型、公共基因组数据。在153例进行SRH成像的弥漫性胶质瘤患者组成的前瞻性、多中心、国际范围的测试队列中，我们展示了DeepGlioma可以预测世界卫生组织用于定义成人型弥漫性胶质瘤分类的分子改变（IDH mut）。

    Molecular classification has transformed the management of brain tumors by enabling more accurate prognostication and personalized treatment. However, timely molecular diagnostic testing for patients with brain tumors is limited, complicating surgical and adjuvant treatment and obstructing clinical trial enrollment. In this study, we developed DeepGlioma, a rapid ($< 90$ seconds), artificial-intelligence-based diagnostic screening system to streamline the molecular diagnosis of diffuse gliomas. DeepGlioma is trained using a multimodal dataset that includes stimulated Raman histology (SRH); a rapid, label-free, non-consumptive, optical imaging method; and large-scale, public genomic data. In a prospective, multicenter, international testing cohort of patients with diffuse glioma ($n=153$) who underwent real-time SRH imaging, we demonstrate that DeepGlioma can predict the molecular alterations used by the World Health Organization to define the adult-type diffuse glioma taxonomy (IDH mut
    
[^51]: 带有延迟组合匿名赌徒反馈的随机次模赌博算法

    Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback. (arXiv:2303.13604v1 [cs.LG])

    [http://arxiv.org/abs/2303.13604](http://arxiv.org/abs/2303.13604)

    本论文研究了具有随机次模收益和全赌徒延迟反馈的组合多臂赌博机问题，研究了三种延迟反馈模型并导出了后悔上限。研究结果表明，算法能够在考虑延迟组合匿名反馈时胜过其他全赌徒方法。

    

    本文研究了组合多臂赌博机问题，其中包含了期望下的随机次模收益和全赌徒延迟反馈，延迟反馈被假定为组合和匿名。也就是说，延迟反馈是由过去行动的奖励组成的，这些奖励由子组件构成，其未知的分配方式。研究了三种延迟反馈模型：有界对抗模型、随机独立模型和随机条件独立模型，并针对每种延迟模型导出了后悔界。忽略问题相关参数，我们证明了所有延迟模型的后悔界为 $\tilde{O}(T^{2/3} + T^{1/3} \nu)$，其中 $T$ 是时间范围，$\nu$ 是三种情况下不同定义的延迟参数，因此展示了带有延迟的补偿项。所考虑的算法被证明能够胜过其他考虑了延迟组合匿名反馈的全赌徒方法。

    This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedbac
    
[^52]: 大型语言模型生成混合代码文本的提示：东南亚语言的案例

    Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])

    [http://arxiv.org/abs/2303.13592](http://arxiv.org/abs/2303.13592)

    本文探讨了使用大型语言模型（LLMs）生成东南亚五种语言和Singlish的混合代码数据的方法，发现ChatGPT展现出最高的潜力。然而，由于词汇选择错误的影响，ChatGPT和InstructGPT在生成混合代码时的熟练程度受到限制。

    

    尽管混合代码在世界许多地区是一种常见的语言实践，但收集高质量且低成本的混合代码数据仍然是自然语言处理（NLP）研究的重大挑战。最近大型语言模型（LLMs）的普及迫使人们问：这些系统能用于数据生成吗？在本文中，我们探讨了在一个零-shot的方式下如何提示LLMs为东南亚（SEA）的五种语言（印尼语，马来语，中文，塔加路语，越南语）及克里奥尔语S ingl ish创造混合代码数据。我们发现，ChatGPT显示出最大的潜力，当明确定义“混合代码”术语时，能够68%的时间生成混合代码文本。此外，ChatGPT和InstructGPT（davinci-003）生成S ingl ish文本的表现也值得注意，它们在各种提示下的成功率平均为96%。但是，ChatGPT和InstructGPT的混合代码熟练程度受到词汇选择错误的影响，导致语义不正确的输出。

    While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
    
[^53]: 神经网络验证的高效符号推理技术

    Efficient Symbolic Reasoning for Neural-Network Verification. (arXiv:2303.13588v1 [cs.AI])

    [http://arxiv.org/abs/2303.13588](http://arxiv.org/abs/2303.13588)

    本文提出了一种高效的符号推理技术，用于解决神经网络的验证问题。该技术可以编码许多验证问题为二次程序，并将其松弛为半定程序以高效地解决。该框架可以验证各种神经网络属性，并为解决神经网络对对抗攻击的脆弱性问题提供了改进。

    

    神经网络已成为现代软件系统中不可或缺的一部分。然而，他们仍然存在许多问题，特别是对对抗攻击的脆弱性。在这项工作中，我们提出了一个新的神经网络验证程序推理框架，称为符号推理。我们框架的关键组成部分是符号域和二次关系的使用。符号域具有非常灵活的语义，而二次关系则非常表达能力强。它们允许我们将许多神经网络验证问题编码为二次程序。然后，我们的方案将二次程序松弛为半定程序，可以高效地解决。这个框架让我们能够在不同的场景下验证各种神经网络属性，特别是那些对非符号域具有挑战性的属性。此外，它还引入了新的表示和验证任务的观点。我们相信，我们的框架可以为神经网络验证带来重要的改进，特别是解决它们对对抗攻击的脆弱性。

    The neural network has become an integral part of modern software systems. However, they still suffer from various problems, in particular, vulnerability to adversarial attacks. In this work, we present a novel program reasoning framework for neural-network verification, which we refer to as symbolic reasoning. The key components of our framework are the use of the symbolic domain and the quadratic relation. The symbolic domain has very flexible semantics, and the quadratic relation is quite expressive. They allow us to encode many verification problems for neural networks as quadratic programs. Our scheme then relaxes the quadratic programs to semidefinite programs, which can be efficiently solved. This framework allows us to verify various neural-network properties under different scenarios, especially those that appear challenging for non-symbolic domains. Moreover, it introduces new representations and perspectives for the verification tasks. We believe that our framework can bring
    
[^54]: TinyML：工具，应用，挑战和未来研究方向

    TinyML: Tools, Applications, Challenges, and Future Research Directions. (arXiv:2303.13569v1 [cs.LG])

    [http://arxiv.org/abs/2303.13569](http://arxiv.org/abs/2303.13569)

    TinyML是一种嵌入式ML技术，可在多种廉价，资源有限和功率有限的设备上实现ML应用。但是，在其实现过程中，需要及时解决多种挑战，例如处理能力优化，提高可靠性以及维护学习模型的准确性。

    

    近年来，人工智能（AI）和机器学习（ML）在工业和学术界都受到了极大的关注。然而，传统的ML技术需要大量的计算资源才能满足所需的准确性，这限制了它们主要用于高性能设备（如网络节点）。然而，随着物联网和边缘计算等技术的不断发展，将ML技术纳入到资源受限的嵌入式设备上以实现分布式和普适性智能也变得十分有必要。这促使出现了TinyML范 paradigm，它是一种嵌入式ML技术，可在多种廉价，资源有限和功率有限的设备上实现ML应用。然而，在向TinyML技术进行适当实现的过程中，需要及时解决多个挑战，例如处理能力优化，提高可靠性以及维护学习模型的准确性。

    In recent years, Artificial Intelligence (AI) and Machine learning (ML) have gained significant interest from both, industry and academia. Notably, conventional ML techniques require enormous amounts of power to meet the desired accuracy, which has limited their use mainly to high-capability devices such as network nodes. However, with many advancements in technologies such as the Internet of Things (IoT) and edge computing, it is desirable to incorporate ML techniques into resource-constrained embedded devices for distributed and ubiquitous intelligence. This has motivated the emergence of the TinyML paradigm which is an embedded ML technique that enables ML applications on multiple cheap, resource- and power-constrained devices. However, during this transition towards appropriate implementation of the TinyML technology, multiple challenges such as processing capacity optimization, improved reliability, and maintenance of learning models' accuracy require timely solutions. In this art
    
[^55]: 利用图卷积神经网络提取租赁公寓平面图的房地产价值

    Extracting real estate values of rental apartment floor plans using graph convolutional networks. (arXiv:2303.13568v1 [cs.LG])

    [http://arxiv.org/abs/2303.13568](http://arxiv.org/abs/2303.13568)

    本文使用改进后的方法从大量平面图像中提取访问图，利用图卷积神经网络模型“平面图价值”估计访问图的房地产价值，大幅提高了租金估计的精度。该模型为全面估计平面图的价值提供了新思路。

    

    本文从大量位于日本大阪的家庭式租赁公寓的平面图像中提取反映房间流线邻接关系的几何图形，并使用改进后的访问图提取方法定义和实现访问图的图卷积网络（GCN）模型，提出了一种估计访问图的房地产价值的方法，即“平面图价值”。该模型包括平面图价值、使用其他一般性解释变量的享受方法，并用于估计租金，比较估计精度。此外，通过学习卷积网络，分析了解释租金的平面图的特征。因此，本文提出并验证了一种全面估计房地产平面图价值的新模型。结果表明，所提出的方法显著提高了租金估计的准确性。

    Access graphs that indicate adjacency relationships from the perspective of flow lines of rooms are extracted automatically from a large number of floor plan images of a family-oriented rental apartment complex in Osaka Prefecture, Japan, based on a recently proposed access graph extraction method with slight modifications. We define and implement a graph convolutional network (GCN) for access graphs and propose a model to estimate the real estate value of access graphs as the floor plan value. The model, which includes the floor plan value and hedonic method using other general explanatory variables, is used to estimate rents and their estimation accuracies are compared. In addition, the features of the floor plan that explain the rent are analyzed from the learned convolution network. Therefore, a new model for comprehensively estimating the value of real estate floor plans is proposed and validated. The results show that the proposed method significantly improves the accuracy of ren
    
[^56]: 使用逻辑知识增强生物医学数据的嵌入表示

    Enhancing Embedding Representations of Biomedical Data using Logic Knowledge. (arXiv:2303.13566v1 [cs.AI])

    [http://arxiv.org/abs/2303.13566](http://arxiv.org/abs/2303.13566)

    本文提出使用逻辑规则增强生物医学数据的嵌入表示，应用于最具挑战性的PharmKG数据集，并能够处理复杂的实验环境中关系事实之间的依赖性。

    

    知识图谱嵌入（KGE）已经成为一种流行的模型，专门设计用于处理本体论和图形结构数据，因为它们可以隐式地在潜在空间中编码实体和关系之间的统计依赖关系。 KGE技术在生物医学领域特别有效，因为在生物和化学对象之间存在复杂的相互作用，因此处理大型知识图谱非常常见。最近，PharmKG数据集被提出作为最具挑战性的知识图谱生物医学基准之一，其中涉及基因，疾病和化学物质之间数十万个关系事实。尽管 KGE 可以处理非常大的关系域，但它们通常无法表示关系事实之间更复杂的关系依赖性，例如逻辑规则，在复杂的实验环境中可能是基本的。在本文中，我们利用逻辑规则增强KGE在PharmKG数据集上的嵌入表示。

    Knowledge Graph Embeddings (KGE) have become a quite popular class of models specifically devised to deal with ontologies and graph structure data, as they can implicitly encode statistical dependencies between entities and relations in a latent space. KGE techniques are particularly effective for the biomedical domain, where it is quite common to deal with large knowledge graphs underlying complex interactions between biological and chemical objects. Recently in the literature, the PharmKG dataset has been proposed as one of the most challenging knowledge graph biomedical benchmark, with hundreds of thousands of relational facts between genes, diseases and chemicals. Despite KGEs can scale to very large relational domains, they generally fail at representing more complex relational dependencies between facts, like logic rules, which may be fundamental in complex experimental settings. In this paper, we exploit logic rules to enhance the embedding representations of KGEs on the PharmKG
    
[^57]: 基于分块数据存储的在线围棋系统CH-Go

    CH-Go: Online Go System Based on Chunk Data Storage. (arXiv:2303.13553v1 [cs.LG])

    [http://arxiv.org/abs/2303.13553](http://arxiv.org/abs/2303.13553)

    该论文提出了一种基于分块数据存储方法的在线围棋游戏系统 CH-Go，可以高效地处理海量数据，并使用神经网络来进行训练。

    

    构建一个有效的数据管理系统来处理海量数据，如初始围棋游戏记录，表示学习获得的特征数据集，自我对弈的经验数据集，随机采样的蒙特卡罗树等，是实现在线围棋系统的训练和运行所必需的。先前的工作很少提到这个问题，但数据管理系统的能力和效率决定了围棋系统的准确性和速度。为了解决这个问题，我们提出了一种基于分块数据存储方法的在线围棋游戏系统 (CH-Go)，它处理了Kiseido Go Server (KGS)发布的160k格式的围棋游戏数据，并设计了一个具有11个平面的围棋编码器、一个并行处理器和一个生成器，以实现更好的内存性能。具体而言，我们把数据存储在块中，以1024为批处理大小，并将每个块的特征和标签保存为二进制文件。然后每次随机抽取一小组数据进行神经网络训练。

    The training and running of an online Go system require the support of effective data management systems to deal with vast data, such as the initial Go game records, the feature data set obtained by representation learning, the experience data set of self-play, the randomly sampled Monte Carlo tree, and so on. Previous work has rarely mentioned this problem, but the ability and efficiency of data management systems determine the accuracy and speed of the Go system. To tackle this issue, we propose an online Go game system based on the chunk data storage method (CH-Go), which processes the format of 160k Go game data released by Kiseido Go Server (KGS) and designs a Go encoder with 11 planes, a parallel processor and generator for better memory performance. Specifically, we store the data in chunks, take the chunk size of 1024 as a batch, and save the features and labels of each chunk as binary files. Then a small set of data is randomly sampled each time for the neural network training
    
[^58]: 从低资源语言阿马齐语的图像中进行Tifinagh字符的光学字符识别和转录

    Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh. (arXiv:2303.13549v1 [cs.CV])

    [http://arxiv.org/abs/2303.13549](http://arxiv.org/abs/2303.13549)

    本文介绍了一种名为DaToBS的方法，用于从自然环境中的照片中检测和转录Tifinagh字符，以提高非洲低资源语言阿马齐语在教育、研究和网络应用等方面的支持。

    

    柏柏尔语是一种低资源的北非土语，在摩洛哥、阿尔及利亚等地的柏柏尔社区中使用自己独特的字母表Tifinagh。这种非洲亚细亚语言是由1400万人使用的，但缺乏足够的教育、研究、网络应用等方面的支持。我们提出了一种DaToBS的有监督方法，用于检测和转录Berber字符，旨在实现从自然环境的照片中自动识别和转录Tifinagh字符。

    The Berber, or Amazigh language family is a low-resource North African vernacular language spoken by the indigenous Berber ethnic group. It has its own unique alphabet called Tifinagh used across Berber communities in Morocco, Algeria, and others. The Afroasiatic language Berber is spoken by 14 million people, yet lacks adequate representation in education, research, web applications etc. For instance, there is no option of translation to or from Amazigh / Berber on Google Translate, which hosts over 100 languages today. Consequently, we do not find specialized educational apps, L2 (2nd language learner) acquisition, automated language translation, and remote-access facilities enabled in Berber. Motivated by this background, we propose a supervised approach called DaToBS for Detection and Transcription of Berber Signs. The DaToBS approach entails the automatic recognition and transcription of Tifinagh characters from signs in photographs of natural environments. This is achieved by sel
    
[^59]: 嘿，Dona！你能帮我处理学生选课吗？

    Hey Dona! Can you help me with student course registration?. (arXiv:2303.13548v1 [cs.HC])

    [http://arxiv.org/abs/2303.13548](http://arxiv.org/abs/2303.13548)

    本文演示了智能个人助手Dona，用于学生选课的自动化操作，采用语音输入、任务规划优化和语言翻译等技术，使学生不需要自己完成复杂的选课表格。

    

    本文演示了一种名为Hey Dona（或仅称为Dona）的智能个人助手，它具有虚拟语音助手，用于学生选课的自动化操作，这是一个旨在应用于教育领域的项目。Hey Dona 适用于各种口音，并能够进行任务规划优化和语言翻译，接受可以通过麦克风（蓝牙、有线麦克风）的语音输入，并按照用户命令执行查询处理，连接网络搜索回答，建立任务依赖关系，确保成功注册。Dona 避免了学生自己输入、点击和浏览复杂的选课表格的需求。

    In this paper, we present a demo of an intelligent personal agent called Hey Dona (or just Dona) with virtual voice assistance in student course registration. It is a deployed project in the theme of AI for education. In this digital age with a myriad of smart devices, users often delegate tasks to agents. While pointing and clicking supersedes the erstwhile command-typing, modern devices allow users to speak commands for agents to execute tasks, enhancing speed and convenience. In line with this progress, Dona is an intelligent agent catering to student needs by automated, voice-operated course registration, spanning a multitude of accents, entailing task planning optimization, with some language translation as needed. Dona accepts voice input by microphone (Bluetooth, wired microphone), converts human voice to computer understandable language, performs query processing as per user commands, connects with the Web to search for answers, models task dependencies, imbibes quality control
    
[^60]: ChatGPT 的零-shot Text-to-SQL 能力的综合评估

    A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability. (arXiv:2303.13547v1 [cs.CL])

    [http://arxiv.org/abs/2303.13547](http://arxiv.org/abs/2303.13547)

    本文对ChatGPT的Text-to-SQL能力进行了全面的评估，展示其强大的零-shot表现，尤其在ADVETA(RPL)情境下优于需要微调的SOTA模型，有望在实际应用中发挥潜力。

    

    本文首次全面分析了 ChatGPT 的 Text-to-SQL 能力。考虑到大型对话语言模型 ChatGPT 和其在对话能力和代码生成能力上的印象深刻的表现，我们试图评估其 Text-to-SQL 性能。我们对12个基准数据集进行了实验，涉及不同的语言、设置或场景，并且结果表明 ChatGPT 具有强大的 Text-to-SQL 能力。虽然与当前最先进的模型表现仍有差距，但考虑到 实验是在零-shot场景下进行的，ChatGPT 的表现仍然令人印象深刻。值得注意的是，在 ADVETA（RPL）场景中，即使是零-shot ChatGPT 在 Spider 数据集上仍然优于需要微调的 SOTA 模型，表现提升了4.1\%，展示了它在实际应用中的潜力。为了支持相关领域的进一步研究，我们已经公开 ChatGPT 生成的数据。

    This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL ability. Given the recent emergence of large-scale conversational language model ChatGPT and its impressive capabilities in both conversational abilities and code generation, we sought to evaluate its Text-to-SQL performance. We conducted experiments on 12 benchmark datasets with different languages, settings, or scenarios, and the results demonstrate that ChatGPT has strong text-to-SQL abilities. Although there is still a gap from the current state-of-the-art (SOTA) model performance, considering that the experiment was conducted in a zero-shot scenario, ChatGPT's performance is still impressive. Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms the SOTA model that requires fine-tuning on the Spider dataset by 4.1\%, demonstrating its potential for use in practical applications. To support further research in related fields, we have made the data generated by ChatGPT publicly avai
    
[^61]: OntoMath${}^{\mathbf{PRO}}$ 2.0本体论：正式模型的更新

    OntoMath${}^{\mathbf{PRO}}$ 2.0 Ontology: Updates of the Formal Model. (arXiv:2303.13542v1 [cs.AI])

    [http://arxiv.org/abs/2303.13542](http://arxiv.org/abs/2303.13542)

    本文介绍了OntoMath${}^{\mathrm{PRO}}$的新版本，该本体使用形式模型将数学事实表示为Linked Open Data。

    

    本文探讨本体论用于数学知识管理和表达的问题。主要关注点在于开发一种形式模型，用于在Open Linked Data云中表示数学陈述。该模型旨在用于从自然语言数学文本中提取数学事实并将这些事实表示为Linked Open Data的应用程序中。本文介绍了OntoMath${}^{\mathrm{PRO}}$一个新版本的本体，该本体用于专业数学，并且是语义发布平台的基础，接受以LaTeX格式编写的数学论文集，并建立其基于本体的Linked Open Data表示。语义发布平台是OntoMath数字生态系统的核心组件，该生态系统由本体、文本分析工具和数学知识管理应用程序（包括语义搜索等）组成。

    This paper is devoted to the problems of ontology-based mathematical knowledge management and representation. The main attention is paid to the development of a formal model for the representation of mathematical statements in the Open Linked Data cloud. The proposed model is intended for applications that extract mathematical facts from natural language mathematical texts and represent these facts as Linked Open Data. The model is used in development of a new version of the OntoMath${}^{\mathrm{PRO}}$ ontology of professional mathematics is described. OntoMath${}^{\mathrm{PRO}}$ underlies a semantic publishing platform, that takes as an input a collection of mathematical papers in LaTeX format and builds their ontology-based Linked Open Data representation. The semantic publishing platform, in turn, is a central component of OntoMath digital ecosystem, an ecosystem of ontologies, text analytics tools, and applications for mathematical knowledge management, including semantic search fo
    
[^62]: 人工智能在可持续性方面的应用: 利用计算机视觉促进可持续智能产品-服务系统。

    Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision. (arXiv:2303.13540v1 [cs.LG])

    [http://arxiv.org/abs/2303.13540](http://arxiv.org/abs/2303.13540)

    本论文在可持续性方面的主要贡献是使用深度学习技术提高产品生产和使用的可持续性，通过计算机视觉技术检测产品的磨损状态并用于改进智能产品-服务系统的集成和结果取向。

    

    目前使用深度学习来实现清洁生产和可持续性目的的研究还非常有限。本论文展示了如何利用深度学习技术提高产品生产和使用的可持续性，尤其是采用深度学习的计算机视觉技术来识别产品的磨损状态，并将这些结果用于改进智能产品-服务系统的集成和结果取向。此外，这些成果预计将促进产品使用的改进和研发创新。我们在两种产品上演示了我们的方法:加工工具和旋转X射线阳极。

    The usage and impact of deep learning for cleaner production and sustainability purposes remain little explored. This work shows how deep learning can be harnessed to increase sustainability in production and product usage. Specifically, we utilize deep learning-based computer vision to determine the wear states of products. The resulting insights serve as a basis for novel product-service systems with improved integration and result orientation. Moreover, these insights are expected to facilitate product usage improvements and R&D innovations. We demonstrate our approach on two products: machining tools and rotating X-ray anodes. From a technical standpoint, we show that it is possible to recognize the wear state of these products using deep-learning-based computer vision. In particular, we detect wear through microscopic images of the two products. We utilize a U-Net for semantic segmentation to detect wear based on pixel granularity. The resulting mean dice coefficients of 0.631 and
    
[^63]: 基于风险的群体结构健康监视理论：以层次系统中的人口为例

    Towards risk-informed PBSHM: Populations as hierarchical systems. (arXiv:2303.13533v1 [cs.AI])

    [http://arxiv.org/abs/2303.13533](http://arxiv.org/abs/2303.13533)

    PBSHM是一种新的结构健康监测方法，它通过对结构群体进行监测，将有价值的知识在结构实例之间传递，从而提高了决策的准确性和泛化能力。

    

    在结构健康监测（SHM）系统的发展中，有一种基于概率风险的决策框架已经被提出。然而，为了学习决策所需的统计模型，需要从感兴趣的结构获得测量数据。不幸的是，这些数据很少在必要的环境和操作条件下涵盖足够的范围，以确保模型的良好泛化。最近，出现了可以将SHM扩展到结构群体的技术，以便在足够相似的结构实例之间传递有价值的知识。这个新方法被称为基于群体的结构健康监测(PBSHM)。本文提出了人群结构的正式表达，以进行基于风险的决策的表示。

    The prospect of informed and optimal decision-making regarding the operation and maintenance (O&M) of structures provides impetus to the development of structural health monitoring (SHM) systems. A probabilistic risk-based framework for decision-making has already been proposed. However, in order to learn the statistical models necessary for decision-making, measured data from the structure of interest are required. Unfortunately, these data are seldom available across the range of environmental and operational conditions necessary to ensure good generalisation of the model.  Recently, technologies have been developed that overcome this challenge, by extending SHM to populations of structures, such that valuable knowledge may be transferred between instances of structures that are sufficiently similar. This new approach is termed population-based structural heath monitoring (PBSHM).  The current paper presents a formal representation of populations of structures, such that risk-based d
    
[^64]: 增强迭代局部搜索算法用于技术员路由和调度问题

    Enhanced Iterated local search for the technician routing and scheduling problem. (arXiv:2303.13532v1 [cs.AI])

    [http://arxiv.org/abs/2303.13532](http://arxiv.org/abs/2303.13532)

    本文提出了增强的迭代局部搜索算法来解决技术员路由和调度问题，该算法结合了高效初始化过程、精心选择的邻域结构和新颖的摇动机制，并在实验中表现优异。

    

    欧洲国家的大多数公共设施，包括法国、德国和英国，在1950年至1980年的重建项目中建造。由于这些重要基础设施的状况日益恶化，近几十年来维护操作成本相对较高。维护操作成本的一部分资金用于技术人员。因此，充分利用可用的劳动力对于优化操作成本至关重要。这包括计划技术干预、工作负载平衡、生产率改进等。本文关注技术员的路由和任务调度。为此，我们解决了一种名为技术员路由和调度问题（TRSP）的工作力调度问题。这个问题在不同的领域有应用，例如交通基础设施（铁路和道路网络）、电信和污水设施。为了解决TRSP问题，我们提出了一种增强的迭代局部搜索算法，结合了基于贪心启发式的高效初始化过程、一组精心选择的邻域结构和一种新颖的摇动机制。我们的实验结果表明，所提出的算法优于文献中现有算法。

    Most public facilities in the European countries, including France, Germany, and the UK, were built during the reconstruction projects between 1950 and 1980. Owing to the deteriorating state of such vital infrastructure has become relatively expensive in the recent decades. A significant part of the maintenance operation costs is spent on the technical staff. Therefore, the optimal use of the available workforce is essential to optimize the operation costs. This includes planning technical interventions, workload balancing, productivity improvement, etc. In this paper, we focus on the routing of technicians and scheduling of their tasks. We address for this purpose a variant of the workforce scheduling problem called the technician routing and scheduling problem (TRSP). This problem has applications in different fields, such as transportation infrastructure (rail and road networks), telecommunications, and sewage facilities. To solve the TRSP, we propose an enhanced iterated local sear
    
[^65]: 基于事件聚类的层次过程模型发现方法

    Discovering Hierarchical Process Models: an Approach Based on Events Clustering. (arXiv:2303.13531v1 [cs.AI])

    [http://arxiv.org/abs/2303.13531](http://arxiv.org/abs/2303.13531)

    本文提出一种基于事件聚类的算法，用于自动合成更易读和理解的分层业务流程模型。

    

    过程挖掘是计算机科学的一个领域，处理基于自动生成事件日志的过程模型的发现和分析。目前，许多公司使用此技术来优化和改进其流程。本文考虑了从低级事件日志中发现分层业务流程模型的问题，即基于信息系统事件日志中存储的信息自动合成更易读和理解的过程模型的问题。本文提出了一种基于预定义事件聚类的算法，用于发现表示为两级工作流网的分层过程模型。

    Process mining is a field of computer science that deals with discovery and analysis of process models based on automatically generated event logs. Currently, many companies use this technology for optimization and improving their processes. However, a discovered process model may be too detailed, sophisticated and difficult for experts to understand. In this paper, we consider the problem of discovering a hierarchical business process model from a low-level event log, i.e., the problem of automatic synthesis of more readable and understandable process models based on information stored in event logs of information systems.  Discovery of better structured and more readable process models is intensively studied in the frame of process mining research from different perspectives. In this paper, we present an algorithm for discovering hierarchical process models represented as two-level workflow nets. The algorithm is based on predefined event ilustering so that the cluster defines a sub-
    
[^66]: 预测虚拟导航任务中晕动症的数据集

    Dataset for predicting cybersickness from a virtual navigation task. (arXiv:2303.13527v1 [cs.HC])

    [http://arxiv.org/abs/2303.13527](http://arxiv.org/abs/2303.13527)

    本文提供了一个用于预测虚拟现实环境中晕动症的数据集，包含多个来自不同参与者的数据点，并详细描述了数据集内容和采集过程，将有助于研究晕动症缓解和预测模型开发。

    

    本文提供了一个用于预测虚拟现实环境中晕动症的数据集。数据集包括在诱发晕动症的虚拟环境中进行导航任务的多个数据点，包括生理反应(EDA和心率)和自报晕动症状。本文将详细描述数据集，包括排列的导航任务、数据采集过程和数据格式。这个数据集将为研究人员开发和评估晕动症预测模型提供有价值的资源，并促进晕动症缓解的更多研究。

    This work presents a dataset collected to predict cybersickness in virtual reality environments. The data was collected from navigation tasks in a virtual environment designed to induce cybersickness. The dataset consists of many data points collected from diverse participants, including physiological responses (EDA and Heart Rate) and self-reported cybersickness symptoms. The paper will provide a detailed description of the dataset, including the arranged navigation task, the data collection procedures, and the data format. The dataset will serve as a valuable resource for researchers to develop and evaluate predictive models for cybersickness and will facilitate more research in cybersickness mitigation.
    
[^67]: 云计算下的不确定性工作负载预测

    Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])

    [http://arxiv.org/abs/2303.13525](http://arxiv.org/abs/2303.13525)

    本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。

    

    在云计算中预测未来的资源需求对于管理云数据中心并保证客户最低服务质量（QoS）水平至关重要。建模未来需求的不确定性可以提高预测的质量并减少由于资源过度分配而带来的浪费。本文提出了单变量和双变量贝叶斯深度学习模型，用于预测未来资源需求的分布及其不确定性。我们设计了不同的训练情景来训练这些模型，其中每个过程是在多个数据集配置上进行预训练和微调步骤的不同组合。我们还将双变量模型与其单变量对应模型进行比较，用单个或多个数据集进行训练，以研究不同组成部分如何影响预测的准确性并影响QoS。最后，我们研究了我们的模型是否具有迁移学习能力。广泛的实验表明，使用多个数据集进行预训练可以提高性能。

    Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
    
[^68]: 欺骗骗子：使用ChatGPT回复欺诈邮件浪费时间和资源

    Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources. (arXiv:2303.13521v1 [cs.CR])

    [http://arxiv.org/abs/2303.13521](http://arxiv.org/abs/2303.13521)

    这篇论文探讨使用AI欺骗骗子并浪费他们时间和资源的方法，利用ChatGPT的自然语言处理能力来回复欺诈邮件。初步结果显示，ChatGPT可以成功欺骗骗子，证明AI是反击电子邮件威胁的有效工具。

    

    使用人工智能（AI）支持网络安全操作现在是一个被验证过的做法，例如，检测恶意代码或配置流量过滤策略。最近涌现的AI生成技术和具有高效自然语言处理能力的框架大大扩大了可能的应用范围，旨在增加互联网安全性。具体而言，ChatGPT生产文本的能力，同时模仿逼真的人类互动，可以用于减轻包含骗局的电子邮件的困扰。因此，本文研究了使用AI与骗子进行自动和无意义通信的方法，以此来浪费他们的时间和资源。初步结果展示了ChatGPT能够欺骗骗子，从而证实了AI是反击通过电子邮件传递的威胁的有效工具。此外，我们强调了多种影响和需要解决的开放性问题。

    The use of Artificial Intelligence (AI) to support cybersecurity operations is now a consolidated practice, e.g., to detect malicious code or configure traffic filtering policies. The recent surge of AI, generative techniques and frameworks with efficient natural language processing capabilities dramatically magnifies the number of possible applications aimed at increasing the security of the Internet. Specifically, the ability of ChatGPT to produce textual contents while mimicking realistic human interactions can be used to mitigate the plague of emails containing scams. Therefore, this paper investigates the use of AI to engage scammers in automatized and pointless communications, with the goal of wasting both their time and resources. Preliminary results showcase that ChatGPT is able to decoy scammers, thus confirming that AI is an effective tool to counteract threats delivered via mail. In addition, we highlight the multitude of implications and open research questions to be addres
    
[^69]: 神经预设：用于颜色风格转移的新技术

    Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])

    [http://arxiv.org/abs/2303.13511](http://arxiv.org/abs/2303.13511)

    本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。

    

    本文提出了一种名为神经预设的技术，用于解决现有颜色风格转移方法的限制，包括可视化伪影、大量内存需求和缓慢的风格切换速度。本方法基于两个核心设计。首先，我们提出了一种确定性神经颜色映射方法（DNCM），通过自适应的颜色映射矩阵在每个像素上进行一致的操作，避免了伪影，并支持具有小内存占用的高分辨率输入。其次，我们通过将任务分为颜色归一化和风格化两个阶段来开发一个两阶段流水线，可以通过将颜色风格作为预设提取，并在归一化的输入图像上重复使用它们来实现有效的风格切换。由于存在成对数据集的问题，我们描述了如何通过自监督策略训练神经预设模型。通过全面的评估展示了神经预设相对于现有方法的各种优势。此外，我们展示了我们训练的模型可以自然地支持多个风格，并在定量和定性性能上实现了最新的表现。

    In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
    
[^70]: GPT是GPT：大语言模型对劳动力市场影响的早期研究

    GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. (arXiv:2303.10130v1 [econ.GN])

    [http://arxiv.org/abs/2303.10130](http://arxiv.org/abs/2303.10130)

    该研究调查了GPT（大语言模型）和相关技术对美国劳动力市场的潜在影响，发现大约80%的美国劳动力可能会受到10%的工作任务的影响，涵盖了所有工资水平和各行各业，预示着这些模型可能具有显著的经济、社会和政策影响。

    

    我们研究了生成预训练变压器（GPT）模型和相关技术对美国劳动力市场的潜在影响。使用新的标准，我们评估职业与GPT能力的对应关系，结合人类专业知识和GPT-4的分类。我们的研究结果表明，约80%的美国劳动力可能会至少有10%的工作任务受到GPT引入的影响，而约19%的工人可能会看到至少50%的任务受到影响。影响范围涵盖了所有工资水平，高收入工作可能面临更大的风险。值得注意的是，影响并不局限于最近生产率增长较高的行业。我们得出结论，生成预训练变压器具有通用技术（GPT）的特性，表明这些模型可能具有显著的经济、社会和政策影响。

    We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.
    
[^71]: 基于源代码控制流的级联深度学习漏洞挖掘

    VMCDL: Vulnerability Mining Based on Cascaded Deep Learning Under Source Control Flow. (arXiv:2303.07128v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07128](http://arxiv.org/abs/2303.07128)

    本论文提出了一种基于源代码控制流的级联深度学习模型VMCDL，通过对SARD数据集中的源代码进行处理来有效检测软件漏洞。

    

    随着计算机行业和软件的快速发展，软件漏洞被利用的风险大大增加。然而，现有的泄露源研究挖掘技术仍存在许多缺点，例如高误报率、粗粒度检测和对专家经验的依赖。本文主要利用SARD数据集的c/c++源代码数据，处理CWE476、CWE469、CWE516和CWE570漏洞类型的源代码，测试最前沿工具Joern漏洞扫描功能，并提出了一种基于源代码控制流的新型级联深度学习模型VMCDL来有效检测漏洞。

    With the rapid development of the computer industry and computer software, the risk of software vulnerabilities being exploited has greatly increased. However, there are still many shortcomings in the existing mining techniques for leakage source research, such as high false alarm rate, coarse-grained detection, and dependence on expert experience. In this paper, we mainly use the c/c++ source code data of the SARD dataset, process the source code of CWE476, CWE469, CWE516 and CWE570 vulnerability types, test the Joern vulnerability scanning function of the cutting-edge tool, and propose a new cascading deep learning model VMCDL based on source code control flow to effectively detect vulnerabilities. First, this paper uses joern to locate and extract sensitive functions and statements to form a sensitive statement library of vulnerable code. Then, the CFG flow vulnerability code snippets are generated by bidirectional breadth-first traversal, and then vectorized by Doc2vec. Finally, th
    
[^72]: 快速对抗训练中的灾难性过拟合的自我拟合视角研究

    Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective. (arXiv:2302.11963v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11963](http://arxiv.org/abs/2302.11963)

    本文首次将单步对抗示例分解为数据信息和自信息，揭示了一种有趣的现象，称为“自我拟合”，即网络学习单步扰动中嵌入的自信息，导致灾难性过拟合。

    

    虽然快速对抗训练提供了一种构建强健网络的高效方法，但它可能会面临一个严重问题，即灾难性过拟合（CO），其中多步强健准确率突然降至零。本文首次将单步对抗示例分解为数据信息和自信息，揭示了一种有趣的现象，称为“自我拟合”。

    Although fast adversarial training provides an efficient approach for building robust networks, it may suffer from a serious problem known as catastrophic overfitting (CO), where multi-step robust accuracy suddenly collapses to zero. In this paper, we for the first time decouple single-step adversarial examples into data-information and self-information, which reveals an interesting phenomenon called "self-fitting". Self-fitting, i.e., the network learns the self-information embedded in single-step perturbations, naturally leads to the occurrence of CO. When self-fitting occurs, the network experiences an obvious "channel differentiation" phenomenon that some convolution channels accounting for recognizing self-information become dominant, while others for data-information are suppressed. In this way, the network can only recognize images with sufficient self-information and loses generalization ability to other types of data. Based on self-fitting, we provide new insights into the exi
    
[^73]: 用户中心设计 (IX): 情报时代下的“用户体验3.0”范式框架

    User-Centered Design (IX): A "User Experience 3.0" Paradigm Framework in the Intelligence Era. (arXiv:2302.06681v5 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.06681](http://arxiv.org/abs/2302.06681)

    本文提出了一种“UX 3.0”范式框架和相应的UX方法系统，旨在满足情报时代对用户体验的新要求。

    

    基于“用户中心设计”理念的用户体验（UX）领域正在向情报时代迈进。然而，现有的UX范式主要针对非智能系统，并缺乏针对智能系统的系统化方法。在UX的发展过程中，UX范式呈现出跨技术时代的演进特征。目前，情报时代对UX范式提出了新的要求。因此，本文提出了一种“UX 3.0”范式框架和相应的UX方法系统。 “UX 3.0”范式框架包括五类UX方法：生态体验，创新体验，AI体验，基于人工智能交互的体验，以及基于人工智能协作的体验方法，每个方法都提供相应的多重UX范式取向。提出“UX 3.0”范式的建议有助于改进现有的UX方法，并提供方法。

    The field of user experience (UX) based on the design philosophy of "user-centered design" is moving towards the intelligence era. Still, the existing UX paradigm mainly aims at non-intelligent systems and lacks a systematic approach to UX for intelligent systems. Throughout the development of UX, the UX paradigm shows the evolution characteristics of the cross-technology era. At present, the intelligence era has put forward new demands on the UX paradigm. For this reason, this paper proposes a "UX 3.0" paradigm framework and the corresponding UX methodology system in the intelligence era. The "UX 3.0" paradigm framework includes five categories of UX methods: ecological experience, innovation-enabled experience, AI-enabled experience, human-AI interaction-based experience, and human-AI collaboration-based experience methods, each providing corresponding multiple UX paradigmatic orientations. The proposal of the "UX 3.0" paradigm helps improve the existing UX methods and provides metho
    
[^74]: 需要多样性：通过稳定的扩散改善模型无关的零样本分类

    Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion. (arXiv:2302.03298v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.03298](http://arxiv.org/abs/2302.03298)

    本文提出了一种通过增强生成数据集中图像的多样性来提高模型无关的零样本分类性能的方法，比最先进的方法提高了1.4mAP（CUB数据集）和8.4mAP（ImageNet数据集），并且适用于任何下游的分类架构。

    

    本文研究了模型无关的零样本分类（MA-ZSC）问题，这意味着训练分类架构来对真实图像进行分类，而在训练过程中不使用任何真实图像。最近的研究表明，使用扩散模型生成合成训练图像可以提供潜在的解决MA-ZSC问题的方法。然而，该方法的性能目前仍然不如大规模视觉-语言模型所取得的性能。我们提出了一种通过改善生成数据集中图像的多样性来提高MA-ZSC性能的新思路。我们提出了一组修改文本到图像生成过程的方法，使用预训练的扩散模型来增强多样性，我们称之为“绝招”。我们的方法适用于任何下游的分类架构，尽管我们重点关注视觉-语言模型。实验证明，在CUB数据集上，我们的方法比最先进的方法提高了1.4mAP，在ImageNet数据集上提高了8.4mAP，同时需要更少的生成图像进行训练。

    In this work, we investigate the problem of Model-Agnostic Zero-Shot Classification (MA-ZSC), which refers to training non-specific classification architectures (downstream models) to classify real images without using any real images during training. Recent research has demonstrated that generating synthetic training images using diffusion models provides a potential solution to address MA-ZSC. However, the performance of this approach currently falls short of that achieved by large-scale vision-language models. One possible explanation is a potential significant domain gap between synthetic and real images. Our work offers a fresh perspective on the problem by providing initial insights that MA-ZSC performance can be improved by improving the diversity of images in the generated dataset. We propose a set of modifications to the text-to-image generation process using a pre-trained diffusion model to enhance diversity, which we refer to as our $\textbf{bag of tricks}$. Our approach sho
    
[^75]: 用于认知决策的量子电路组件

    Quantum Circuit Components for Cognitive Decision-Making. (arXiv:2302.03012v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2302.03012](http://arxiv.org/abs/2302.03012)

    本文将一些非经典的人类决策模型成功地应用于量子计算机电路，并通过量子概率、角度和子空间等方式描述了这些模型和它们在量子计算机上的实现和研究。

    

    本论文表明，一些非经典的人类决策模型可以成功地作为电路在量子计算机上运行。自20世纪60年代以来，许多观察到的认知行为已被证明违反了基于经典概率和集合论的规则。例如，问题提出的顺序会影响参与者是否回答“是”或“否”，因此回答两个问题“是”的人口不能被建模为两个固定集合的交集。然而，它可以被建模为不同顺序进行的一系列投影。这和其他示例已经成功地使用了量子概率来描述，这依赖于比较子空间之间的角度，而不是子集之间的体积。现在在2020年初，量子计算机已经达到了一定水平，一些量子认知模型可以在量子硬件上实现并研究，将心理状态表示为qubit寄存器中的状态，认知操作和决策操作通过量子门来实现。

    This paper demonstrates that some nonclassical models of human decision-making can be run successfully as circuits on quantum computers. Since the 1960s, many observed cognitive behaviors have been shown to violate rules based on classical probability and set theory. For example, the order in which questions are posed affects whether participants answer 'yes' or 'no', so the population that answers `yes' to both questions cannot be modeled as the intersection of two fixed sets. It can however be modeled as a sequence of projections carried out in different orders. This and other examples have been described successfully using quantum probability, which relies on comparing angles between subspaces rather than volumes between subsets. Now in the early 2020s, quantum computers have reached the point where some of these quantum cognitive models can be implemented and investigated on quantum hardware, representing the mental states in qubit registers, and the cognitive operations and decisi
    
[^76]: 在在线连续学习中进行实时评估：一个新希望

    Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01047](http://arxiv.org/abs/2302.01047)

    该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。

    

    当前连续学习方法的评估通常假设在训练时间和计算方面没有限制。这对于任何实际世界的环境都是不现实的。因此我们提出了一种实时评估连续学习的方法，其中流不等待模型完成训练即揭示下一个数据进行预测。为了实现这一点，我们从计算成本的角度评估当前的CL方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了广泛的实验。结果表明，在这种评估下，一个简单的基线模型胜过了最先进的CL方法，这对现有方法在现实环境中的适用性提出了质疑。另外，我们还探讨了文献中常用的各种CL组件，包括记忆采样策略和正则化方法。我们发现，所有考虑的方法都无法与我们的简单基线模型竞争。

    Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
    
[^77]: 适用于所有领域的一个模型：基于协作域前缀调整的跨领域实体识别

    One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10410](http://arxiv.org/abs/2301.10410)

    本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。

    

    解决实际场景中低资源问题是跨领域实体识别的一个挑战性任务。先前典型的解决方案主要通过使用来自丰富资源领域的数据进行预训练语言模型(PLMs)获得NER模型并将其适应于目标领域。由于不同领域实体类型之间的不匹配问题，先前的方法通常调整所有PLMs的参数，从而为每个领域结束一个全新的NER模型。此外，当前的模型只关注于利用一个普通来源领域中的知识，而未能成功地将来自多个来源领域的知识转移到目标上。为了解决这些问题，我们基于文本到文本生成的PLM引入了协作域前缀调整跨领域NER(CP-NER)。具体来说，我们呈现了用于文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务而无需结构修改。我们利用冻结的PLMs并进行协作域前缀调整。

    Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
    
[^78]: 深度条件测度量化

    Deep Conditional Measure Quantization. (arXiv:2301.06907v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.06907](http://arxiv.org/abs/2301.06907)

    提出了一种名为DCMQ的方法，该方法结合了基于Huber能量核的方法和深度神经网络架构，用于条件测度量化，并在多个实例中取得了有希望的结果。

    

    概率测度的量化意味着使用一组有限的狄拉克分布来近似表示输入分布（在一些概率测度度量空间中）。有各种各样的方法可以实现这一点，但可能会存在条件法的量化需要探索的情况。我们提出了一种称为DCMQ的方法，它涉及到基于Huber能量核的方法和深度神经网络体系结构的耦合。该方法在多个实例上进行了测试，并获得了有希望的结果。

    Quantization of a probability measure means representing it with a finite set of Dirac masses that approximates the input distribution well enough (in some metric space of probability measures). Various methods exists to do so, but the situation of quantizing a conditional law has been less explored. We propose a method, called DCMQ, involving a Huber-energy kernel-based approach coupled with a deep neural network architecture. The method is tested on several examples and obtains promising results.
    
[^79]: 面向可扩展物理一致的神经网络：在数据驱动多区域热建筑模型中的应用研究

    Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12380](http://arxiv.org/abs/2212.12380)

    本论文研究了物理一致神经网络(PCNNs) 在模拟建筑温度动态方面的扩展性和准确性。结果发现，PCNNs既确保了物理一致性，同时又能在复杂的多区域热建筑模型中取得高精度的性能表现，且在可用数据量有限的情况下超越经典灰盒模型，具有可扩展性优势。

    

    随着越来越多的数据被收集，数据驱动建模方法近年来越来越受欢迎。虽然经典灰盒模型在物理上是可靠的，但通常很难识别和扩展，并且受其有限的表现力影响可能会影响其准确性。另一方面，常常依赖神经网络 (NNs) 的经典黑盒方法通常能够从数据中推导出统计模式，即使在扩展方面也能取得令人印象深刻的性能。然而，它们对潜在的物理定律完全无视，如果基于它们做决策用于实际物理系统，可能会导致潜在的灾难性后果。最近开发了物理一致神经网络 (PCNNs) 来解决这些问题，确保物理一致性，同时利用 NNs 实现最先进的准确性。在这项工作中，我们将 PCNN 扩展到建筑温度动态建模，并提出与经典灰盒和黑盒方法的彻底比较。特别是，我们研究多区域建筑模型，其中每个区域的热行为由能量平衡方程式统治，其参数必须通过测量数据进行识别。所得结果表明，即使涉及许多相互作用的组件构成的复杂和动态系统，PCNNs 也可以在确保物理一致性的同时实现高精度。此外，我们证明 PCNN 在经典灰盒模型上提供了可扩展性优势，在有限的可用训练数据下表现出色。

    With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
    
[^80]: 大型语言模型中的类比推理的紧急性

    Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.09196](http://arxiv.org/abs/2212.09196)

    GPT-3在许多类比任务中表现出与甚至超越人类的能力，揭示了大型语言模型的紧急能力。

    

    大型语言模型的出现重新点燃了人们对于这样一种问题的辩论：足够的训练数据是否能使这些通用模型内涵人类认知能力。特别的，这些模型的零样本推理能力——不经过任何直接训练，就能够推理出新问题，特别令人关注。在人类认知中，这种能力与一种通过类比推理的能力密切相关。在本文中，我们在一系列类比任务中进行了直接的人机比较，包括一种新颖的基于文本的矩阵推理任务，该任务与 Raven's Progressive Matrices密切相关。我们发现，GPT-3呈现出了一种令人惊讶的抽象模式归纳能力，甚至在大部分情况下与或甚至超越了人类的能力。我们的结果表明，像GPT-3这样的大型语言模型已经获得了在广泛的类比问题上找到零样本解决方案的紧急能力。

    The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.
    
[^81]: POTATO: 便携式文本注释工具

    POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08620](http://arxiv.org/abs/2212.08620)

    POTATO是一个免费、开源的便携式文本注释工具，支持多种类型的文本和多模态数据的标注，提供易于配置的功能以最大化生产力，特别是对于长文档和复杂任务。

    

    我们介绍了POTATO，即便携式文本注释工具，这是一个完全免费、开源的注释系统，支持标注多种类型的文本和多模态数据，提供易于配置的功能以最大化部署和注释者的生产力（便捷的机器学习/自然语言处理任务模板、主动学习、按键缩写键、关键字高亮、提示工具），并支持高度定制化（可编辑的UI，插入预筛选问题，注意力和资格测试）。两项注释任务的实验表明，POTATO通过其特别设计的生产力功能，特别是长文档和复杂任务，提高了标注速度。POTATO可在 https://github.com/davidjurgens/potato 上获取，并将继续更新。

    We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common ML/NLP tasks, active learning, keypress shortcuts, keyword highlights, tooltips); and 3) supports a high degree of customization (editable UI, inserting pre-screening questions, attention and qualification tests). Experiments over two annotation tasks suggest that POTATO improves labeling speed through its specially-designed productivity features, especially for long documents and complex tasks. POTATO is available at https://github.com/davidjurgens/potato and will continue to be updated.
    
[^82]: FlexiViT：适用于所有补丁大小的模型

    FlexiViT: One Model for All Patch Sizes. (arXiv:2212.08013v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08013](http://arxiv.org/abs/2212.08013)

    本文介绍了一种名为FlexiViT的模型，它可以使用一组权重表现良好，并且适用于各种补丁大小，从而使其在部署时容易根据不同的计算预算来定制模型。

    

    Vision Transformer将图像切成补丁以将其转换为序列。这些补丁的大小控制速度/准确性权衡，补丁越小，精度越高，但计算成本也越高，但更改补丁大小通常需要重新训练模型。本文证明，简单随机化训练时的补丁大小会导致一组权重表现良好，可在各种补丁大小范围内使用，从而使其在部署时容易根据不同的计算预算来定制模型。我们在许多任务上对生成的模型进行了全面评估，包括分类，图像-文本检索，开放式检测，全景分割和语义分割，得出结论：它通常可以匹配，并且有时会优于以单个补丁大小训练的标准ViT模型。因此，在ViT中使用FlexiViT进行训练是一种简单易行的改进方法。

    Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to a
    
[^83]: Flow-Lenia: 通过质量守恒和参数定位实现元胞自动机中开放式演化

    Flow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localization. (arXiv:2212.07906v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2212.07906](http://arxiv.org/abs/2212.07906)

    本文提出了Flow-Lenia作为Lenia的一种扩展来解决Lenia无法生成完全自由的演化、无法相互作用以及需要高级搜索算法进行探索等问题，展示了Flow-Lenia在生成具有复杂行为的生命形态方面的有效性。

    

    复杂自组织系统的设计是人工生命的主要目标之一。Lenia是一种广义康威生命游戏的元胞自动机，将其推广到了连续空间、时间和状态。它吸引了许多关注，因为它可以产生各种自组织模式。本文提出了Flow-Lenia作为Lenia的质量守恒扩展，它解决了Lenia无法生成完全自由的演化、无法相互作用以及需要高级搜索算法进行探索等问题。我们展示了其在生成具有复杂行为的生命形态方面的有效性。

    The design of complex self-organising systems producing life-like phenomena, such as the open-ended evolution of virtual creatures, is one of the main goals of artificial life. Lenia, a family of cellular automata (CA) generalizing Conway's Game of Life to continuous space, time and states, has attracted a lot of attention because of the wide diversity of self-organizing patterns it can generate. Among those, some spatially localized patterns (SLPs) resemble life-like artificial creatures and display complex behaviors. However, those creatures are found in only a small subspace of the Lenia parameter space and are not trivial to discover, necessitating advanced search algorithms. Furthermore, each of these creatures exist only in worlds governed by specific update rules and thus cannot interact in the same one. This paper proposes as mass-conservative extension of Lenia, called Flow Lenia, that solve both of these issues. We present experiments demonstrating its effectiveness in genera
    
[^84]: 结构化知识增强的开放世界故事生成：一项全面调查

    Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.04634](http://arxiv.org/abs/2212.04634)

    本文对结构化知识增强的故事生成进行了综述，总结了目前的方法与技术，指出了未来的发展方向和尚未解决的问题。

    

    讲故事和叙事是人类体验的基础，与我们的社会和文化参与密不可分。因此，长期以来，研究人员一直尝试创建能够自动生成故事的系统。近年来，受深度学习和大规模数据资源的推动，自动生成故事已经取得了很大的进展。然而，仍然存在一些重大挑战，例如，需要在生成的故事中实现全局一致性，这使得生成模型无法达到与人类叙述者相同的叙事能力。为了解决这些挑战，许多研究试图在生成过程中注入结构化知识，这被称为结构化知识增强的故事生成。将外部知识纳入其中可以增强故事事件之间的逻辑连贯性，实现更好的知识基础，并减轻故事中过度概括和重复问题。本次调查提供了对该研究领域的最新和全面的回顾：（i）我们提供了对结构化知识增强故事生成的综述，（ii）我们总结了目前的方法与技术，（iii）我们指出了尚待解决的问题与未来的发展方向。

    Storytelling and narrative are fundamental to human experience, intertwined with our social and cultural engagement. As such, researchers have long attempted to create systems that can generate stories automatically. In recent years, powered by deep learning and massive data resources, automatic story generation has shown significant advances. However, considerable challenges, like the need for global coherence in generated stories, still hamper generative models from reaching the same storytelling ability as human narrators. To tackle these challenges, many studies seek to inject structured knowledge into the generation process, which is referred to as structured knowledge-enhanced story generation. Incorporating external knowledge can enhance the logical coherence among story events, achieve better knowledge grounding, and alleviate over-generalization and repetition problems in stories. This survey provides the latest and comprehensive review of this research field: (i) we present a
    
[^85]: 状态空间封闭：通过强化学习重新审视无尽在线关卡生成

    State Space Closure: Revisiting Endless Online Level Generation via Reinforcement Learning. (arXiv:2212.02951v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.02951](http://arxiv.org/abs/2212.02951)

    这篇论文重新审视了无尽在线关卡生成，提出了状态空间封闭概念，将有限时段训练的经验驱动强化学习程序内容生成推广到无限时段，而不损失生成内容的质量。通过实证研究，验证了EDRL生成的内容质量和多样性。

    

    本文中，我们使用最近提出的基于经验驱动的强化学习程序内容生成（EDRL）框架重新审视了无尽在线关卡生成。通过观察EDRL倾向于生成重复模式的现象，我们提出了状态空间封闭的概念，它使得无限时段的在线生成过程中可能出现的任何随机状态都可以在有限时段内找到。通过理论分析，我们发现状态空间封闭虽然引起了多样性的关注，但它将有限时段训练的EDRL推广到无限时段的情况，而不会降低生成内容的质量。此外，通过实证研究，我们验证了EDRL生成内容的质量和多样性，使用了广泛使用的Super Mario Bros基准测试进行评估。实验结果表明，由于状态空间封闭，EDRL生成的关卡多样性有限，但它们的质量在时段内没有下降。

    In this paper, we revisit endless online level generation with the recently proposed experience-driven procedural content generation via reinforcement learning (EDRL) framework. Inspired by an observation that EDRL tends to generate recurrent patterns, we formulate a notion of state space closure which makes any stochastic state appeared possibly in an infinite-horizon online generation process can be found within a finite-horizon. Through theoretical analysis, we find that even though state space closure arises a concern about diversity, it generalises EDRL trained with a finite-horizon to the infinite-horizon scenario without deterioration of content quality. Moreover, we verify the quality and the diversity of contents generated by EDRL via empirical studies, on the widely used Super Mario Bros. benchmark. Experimental results reveal that the diversity of levels generated by EDRL is limited due to the state space closure, whereas their quality does not deteriorate in a horizon which
    
[^86]: MHCCL：用于多元时间序列的层次掩蔽聚类对比学习

    MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series. (arXiv:2212.01141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01141](http://arxiv.org/abs/2212.01141)

    本文提出了一种名为MHCCL的对比学习模型，可以从多元时间序列数据中学习语义丰富的表示，并利用层次聚类结构中的多粒度信息来滤除虚假负样本和补充正样本。

    

    从原始无标签时间序列数据中学习语义丰富的表示对于分类和预测等下游任务至关重要。对比学习最近展示了在缺乏专家注释的情况下具有良好的表示学习能力。然而，现有的对比学习方法通常独立处理每个实例，导致共享相同语义的假负样本。为了解决这个问题，我们提出了MHCCL，一种层次掩蔽聚类对比学习模型，它利用由多个潜在分区组成的层次结构获得的语义信息来为多元时间序列建模。受到细粒度聚类保留更高纯度，而粗粒度聚类反映更高级别语义的观察的启发，我们提出了一种新颖的向下掩蔽策略，通过结合聚类层次结构中的多粒度信息，过滤掉虚假负面实例并补充正面实例。

    Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy
    
[^87]: 深度神经网络中的解释方法破解

    Foiling Explanations in Deep Neural Networks. (arXiv:2211.14860v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14860](http://arxiv.org/abs/2211.14860)

    本文发现了解释图像 DNN 的一个令人担忧的属性：通过微小视觉更改，我们演示了解释可以通过进化策略任意操纵。我们提出了 AttaXAI，一个针对 XAI 算法的敌对攻击，可访问分类器输出信息和解释图，这使得我们的方法在实际应用中非常有用。

    

    深度神经网络的黑盒特性对于可解释性仍然存在重大挑战，因为仅仅获得其输出是不够有用的。本文发现了图像 DNN 的解释方法的一个令人担忧的属性：通过对输入图像进行微小的视觉更改，我们演示了如何通过进化策略任意操纵解释。我们提出了一个新的算法 AttaXAI，一个针对 XAI 算法的模型无关的敌对攻击，只需要访问分类器的输出信息和解释图，这些弱假设使我们的方法在实际应用中非常有用。

    Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone -- sans a reasoning of how said answer was derived -- is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image -- hardly influencing the network's output -- we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our me
    
[^88]: OCTET: 对象感知的反事实解释

    OCTET: Object-aware Counterfactual Explanations. (arXiv:2211.12380v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12380](http://arxiv.org/abs/2211.12380)

    本文提出了一种基于对象的框架来生成反事实解释，以解释训练有多个物体图像的决策模型。

    

    当前，深度视觉模型被广泛应用于安全关键应用，如自动驾驶。模型的可解释性正在成为一个紧迫的问题。在众多解释方法中，反事实解释旨在找到最小和可解释的变化来改变模型输出结果的输入图像。这样的解释使最终用户了解影响模型决策的主要因素。然而，以前的方法很难解释训练有多个物体图像（例如城市场景）的决策模型，这些图像更难处理，但也可能更关键。本文提出了一种基于对象的框架来生成反事实解释，旨在解决这个问题。我们的方法受到最近生成建模工作的启发，将查询图像编码为一个结构化的潜空间，以便进行基于对象的操作。这样做，可以为最终用户提供对模型的控制。

    Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over w
    
[^89]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^90]: 视觉语言模型何时以及为何行为像词袋，以及如何解决？

    When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.01936](http://arxiv.org/abs/2210.01936)

    针对大型视觉语言模型在编码组合信息方面的表现存在问题，我们创建了Attribution、Relation和Order（ARO）基准，并提出了对VLMs进行Attention机制和对抗训练等修改以提高其组合理解能力。

    

    尽管大型视觉语言模型（VLM）在许多下游应用中取得了成功，但它们编码组合信息的能力尚不清楚。为此，我们创建了Attribution、Relation和Order（ARO）基准来系统评估VLM理解不同类型关系、属性和顺序的能力。ARO由Visual Genome Attribution测试对象属性的理解能力；Visual Genome Relation测试关系理解能力；以及COCO＆Flickr30k-Order测试顺序敏感性。ARO比以前的组合性基准大多个数量级，包括50,000多个测试用例。我们展示了最先进的VLM在哪些方面存在关系理解问题，当链接对象和属性时容易出错，并展示了其明显的缺乏顺序敏感性。VLM主要在具有丰富组合结构的图像和标题的大型数据集上进行训练和评估。然而，仅仅在这些数据集上训练并不能保证在需要组合理解的任务上表现良好。为了解决这些问题，我们提出了几种修改VLMs的方法，包括强调组成关系的注意机制和使用对抗训练来改善属性预测。我们提出的修改可以显著提高VLM在ARO基准上的组合理解能力。

    Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO & Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these d
    
[^91]: 大深度网络的隐式偏见：非线性函数的秩定义。

    Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15055](http://arxiv.org/abs/2209.15055)

    本文研究了全连接神经网络的表示成本和深度之间的关系，发现其会收敛到非线性函数的秩的概念。同时，发现在一定的深度范围内，全局最小值可以恢复真实的数据秩，并探讨了分类器秩对类边界拓扑结构的影响。

    

    我们展示了具有齐次非线性函数的全连接神经网络的表示成本——描述了具有$L_2$正则化或交叉熵等损失网络在函数空间中的隐式偏见——随着网络深度趋近于无穷大，会收敛到非线性函数的秩的概念。然后，我们探究了全局最小值在哪些条件下可以恢复“真实”数据秩：我们展示出，对于太大的深度，全局最小值会近似为秩1（低估秩）；我们随后论证了有一系列深度，随着数据点数量增加，可以恢复真实秩。最后，我们讨论了分类器秩对结果类边界的拓扑结构的影响，并展示了具有最佳非线性秩的自编码器具有自然去噪的特性。

    We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.
    
[^92]: 可计算概率模型的连续混合

    Continuous Mixtures of Tractable Probabilistic Models. (arXiv:2209.10584v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10584](http://arxiv.org/abs/2209.10584)

    本文研究了一种连续混合的方法，将可计算概率模型和基于连续潜空间的模型结合起来，从而实现了高效的概率推断。

    

    基于连续潜空间的概率模型，如变分自编码器，可以理解为在潜变量上连续依赖的不可数混合模型。它们已被证明是表达生成和概率建模的有效工具，但与能够计算所表示的概率分布的较为简单的可计算概率推断方法存在矛盾。与此同时，可计算概率模型，如概率电路(PCs)，可以理解为层次离散混合模型，因此能够高效地进行准确推断，但在与连续潜空间模型相比时通常表现不佳。在本文中，我们研究了一种混合方法，即具有小潜变量维度的可计算模型的连续混合。尽管这些模型在解析上是难以处理的，但它们适合于基于有限积分点集的数值积分方案。通过足够大的积分点集，我们可以紧密地近似概率模型并进行高效推断。

    Probabilistic models based on continuous latent spaces, such as variational autoencoders, can be understood as uncountable mixture models where components depend continuously on the latent code. They have proven to be expressive tools for generative and probabilistic modelling, but are at odds with tractable probabilistic inference, that is, computing marginals and conditionals of the represented probability distribution. Meanwhile, tractable probabilistic models such as probabilistic circuits (PCs) can be understood as hierarchical discrete mixture models, and thus are capable of performing exact inference efficiently but often show subpar performance in comparison to continuous latent-space models. In this paper, we investigate a hybrid approach, namely continuous mixtures of tractable models with a small latent dimension. While these models are analytically intractable, they are well amenable to numerical integration schemes based on a finite set of integration points. With a large 
    
[^93]: 基于智能CCTV相机和语义分割的CNN智能路灯管理

    CNN based Intelligent Streetlight Management Using Smart CCTV Camera and Semantic Segmentation. (arXiv:2209.08633v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.08633](http://arxiv.org/abs/2209.08633)

    本研究提出了一种基于智能交通监测系统和CCTV相机的自动化路灯控制方法，利用语义分割技术检测行人或车辆的存在并调节LED路灯的亮度，减少了能源浪费和环境影响。

    

    街灯是最容易被忽视的能源浪费源之一，其在不需要照明的区域发出过多的光线，造成巨大的经济和环境影响。由于传统的人工操作方式，街灯常常会在白天被打开，在晚上被关闭，这在21世纪仍然很遗憾。因此需要自动化的路灯控制方法来解决这些问题。本研究旨在通过将由计算机视觉技术驱动的智能交通监测系统与闭路电视(CCTV)相机相结合，利用CCTV视频流中的语义图像分割检测行人或车辆的存在并在其缺席时调节街灯的亮度，从而开发一种新型的路灯控制方法。因此，我们的模型区分白天和黑夜，并根据需要自动调节LED路灯的亮度。

    One of the most neglected sources of energy loss is streetlights which generate too much light in areas where it is not required. Energy waste has enormous economic and environmental effects. In addition, due to the conventional manual nature of the operation, streetlights are frequently seen being turned ON during the day and OFF in the evening, which is regrettable even in the twenty-first century. These issues require automated streetlight control in order to be resolved. This study aims to develop a novel streetlight controlling method by combining a smart transport monitoring system powered by computer vision technology with a closed circuit television (CCTV) camera that allows the light-emitting diode (LED) streetlight to automatically light up with the appropriate brightness by detecting the presence of pedestrians or vehicles and dimming the streetlight in their absence using semantic image segmentation from the CCTV video streaming. Consequently, our model distinguishes daylig
    
[^94]: 基于Transformer的目标检测器中多尺度特征的有效利用方法研究

    Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors. (arXiv:2208.11356v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.11356](http://arxiv.org/abs/2208.11356)

    本文提出了一种在基于Transformer的目标检测器中，通过两个新的设计实现了多尺度特征的高效利用，其核心思想是通过稀疏采样多尺度特征，以减少计算成本，同时保证了目标检测性能。

    

    多尺度特征已被证明对于目标检测非常有效，但往往需要巨大甚至是禁止性的额外计算成本，尤其是对于最近的基于Transformer的检测器。本文提出了一种迭代多尺度特征聚合（IMFA）的通用方法，以实现在基于Transformer的目标检测器中高效使用多尺度特征。其核心思想是利用来自仅有几个关键位置的稀疏多尺度特征，并通过两个新的设计实现。首先，IMFA重新排列Transformer编码器-解码器管道，以便根据检测预测迭代更新编码特征。其次，IMFA在以先前的检测预测作为指导的情况下，从仅有几个关键点采用稀疏的尺度自适应特征用于精细检测。因此，采样的多尺度特征是稀疏的，但对于目标检测仍然非常有益。大量实验证明，所提出的IMFA技术可以在保证目标检测性能的同时显著减少计算成本。

    Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA bo
    
[^95]: 深度学习模型及其内部表示的对称性研究

    On the Symmetries of Deep Learning Models and their Internal Representations. (arXiv:2205.14258v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14258](http://arxiv.org/abs/2205.14258)

    本文研究了深度学习模型及其内部表示的对称性，发现网络的对称性传播到数据的表示中的对称性，为更好地理解架构影响学习和预测过程提供了一种方法。

    

    对称性是研究复杂系统的基本工具。在机器学习中，对称性已经在模型和数据中得到了研究。本文旨在通过计算模型的一组基本对称群，即模型的intertwiner groups，将模型架构的对称性与该模型对数据的内部表示的对称性相联系。我们通过一系列实验将intertwiner groups连接到具有相同架构的模型的隐藏状态之间的相似之处。我们的研究表明，网络的对称性传播到该网络对数据的表示中的对称性，更好地理解架构如何影响学习和预测过程。最后，我们推测，对于ReLU网络，intertwiner groups可能为常见的堆叠网络结构提供了正当性。

    Symmetry is a fundamental tool in the exploration of a broad range of complex systems. In machine learning symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family's internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. We connect intertwiner groups to a model's internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network's representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of con
    
[^96]: Euler State Networks: 非耗散性水库计算

    Euler State Networks: Non-dissipative Reservoir Computing. (arXiv:2203.09382v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09382](http://arxiv.org/abs/2203.09382)

    本文提出了一种新型水库计算模型EuSN，其利用前向欧拉离散化和反对称循环矩阵来设计水库动力学，具有接近稳定边缘的饱和有效谱半径和零局部李雅普诺夫指数。在长期记忆任务和时间序列分类基准测试中表现出了优异的性能。

    

    本文受到常微分方程的数值解启发，提出了一种新型水库计算(EuSN)模型，该方法利用前向欧拉离散化和反对称循环矩阵来设计水库动力学。该模型的数学分析表明，其具有接近稳定边缘的饱和有效谱半径和零局部李雅普诺夫指数。对长期记忆任务的实验表明，与标准水库计算模型相比，所提出的方法在需要多个时步有效传播输入信息的问题上具有明显的优势。此外，时间序列分类基准测试结果表明，EuSN能够匹配甚至超过可训练循环神经网络的准确性，同时保持水库计算家族的训练效率。

    Inspired by the numerical solution of ordinary differential equations, in this paper we propose a novel Reservoir Computing (RC) model, called the Euler State Network (EuSN). The presented approach makes use of forward Euler discretization and antisymmetric recurrent matrices to design reservoir dynamics that are both stable and non-dissipative by construction.  Our mathematical analysis shows that the resulting model is biased towards a unitary effective spectral radius and zero local Lyapunov exponents, intrinsically operating near to the edge of stability. Experiments on long-term memory tasks show the clear superiority of the proposed approach over standard RC models in problems requiring effective propagation of input information over multiple time-steps. Furthermore, results on time-series classification benchmarks indicate that EuSN is able to match (or even exceed) the accuracy of trainable Recurrent Neural Networks, while retaining the training efficiency of the RC family, res
    
[^97]: 基于决策的稀疏攻击黑盒深度学习模型的有效查询

    Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models. (arXiv:2202.00091v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.00091](http://arxiv.org/abs/2202.00091)

    研究提出了基于决策的稀疏攻击方法SparseEvo，通过最小化扰动的像素数量，能够有效地欺骗黑盒深度学习模型，而不需要了解它的内部构造。

    

    尽管我们已经做了最大的努力，但深度学习模型仍然容易受到微小的对抗性扰动。从机器学习模型的输出中提取信息以制作黑盒模型的对抗扰动是现实世界系统（例如自动驾驶汽车或作为服务（MLaaS）公开的机器学习模型）面临的威胁。 稀疏攻击尤其受到关注。 稀疏攻击在黑盒模型中的实现表明，机器学习模型比我们想象的更容易受到攻击。这项研究提出了一种基于决策的稀疏攻击方法，通过最小化扰动的像素数量，有效地欺骗黑盒深度学习模型，而不需要了解它的内部构造。我们开发了一种基于进化算法的SparseEvo，用于解决NP难的问题。

    Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as autonomous cars or machine learning models exposed as a service (MLaaS). Of particular interest are sparse attacks. The realization of sparse attacks in black-box models demonstrates that machine learning models are more vulnerable than we believe. Because these attacks aim to minimize the number of perturbed pixels measured by l_0 norm-required to mislead a model by solely observing the decision (the predicted label) returned to a model query; the so-called decision-based attack setting. But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm-SparseEvo-for the problem and evaluate against both convolutional deep 
    
[^98]: 通过文字创作画作

    Paint by Word. (arXiv:2103.10951v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2103.10951](http://arxiv.org/abs/2103.10951)

    本文研究了零样本语义图像绘制问题，并使用最先进的逼真图像生成模型和文本-图像语义相似性网络相结合，使得能够在开放式的文本描述下进行语义画作。

    

    本文研究零样本语义图像绘制问题，目标是基于开放式文本描述创建语义画作。文章通过将最先进的逼真图像生成模型和文本-图像语义相似性网络相结合来完成这一目标。研究表明，为了进行大的变化，使用非梯度方法来探索潜空间是重要的，而且重要的是放宽GAN的计算以针对特定区域进行变化。文章进行了用户调研，将我们的方法与几个基线进行了比较。

    We investigate the problem of zero-shot semantic image painting. Instead of painting modifications into an image using only concrete colors or a finite set of semantic concepts, we ask how to create semantic paint based on open full-text descriptions: our goal is to be able to point to a location in a synthesized image and apply an arbitrary new concept such as "rustic" or "opulent" or "happy dog." To do this, our method combines a state-of-the art generative model of realistic images with a state-of-the-art text-image semantic similarity network. We find that, to make large changes, it is important to use non-gradient methods to explore latent space, and it is important to relax the computations of the GAN to target changes to a specific region. We conduct user studies to compare our methods to several baselines.
    

