# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Ordinal Data Science.](http://arxiv.org/abs/2307.09477) | 本文讨论了序列数据科学的发展，并介绍了一种新的研究议程：使用序列结构来衡量和计算对象之间的关系，并从中推断知识。这种方法具有广泛的学科应用价值。 |
| [^2] | [Overthinking the Truth: Understanding how Language Models Process False Demonstrations.](http://arxiv.org/abs/2307.09476) | 该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。 |
| [^3] | [Unsupervised Conditional Slot Attention for Object Centric Learning.](http://arxiv.org/abs/2307.09437) | 本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。 |
| [^4] | [SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs.](http://arxiv.org/abs/2307.09435) | SLMGAN提出了一种利用语音语言模型表示进行无监督零样本语音转换的新方法，在生成对抗网络框架中实现了优于现有模型的性能。 |
| [^5] | [Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education.](http://arxiv.org/abs/2307.09426) | 本论文讨论了在开发人工智能算法时，在组织病理学中平衡隐私和进步的挑战，并提出了解决方案。 |
| [^6] | [Scaling Laws for Imitation Learning in NetHack.](http://arxiv.org/abs/2307.09423) | 本文研究了在NetHack游戏中的模仿学习，发现通过扩大模型和数据规模可以改进模仿学习的效果，并建立了训练计算最优IL代理人的幂律。 |
| [^7] | [CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space.](http://arxiv.org/abs/2307.09375) | CertPri是一种基于特征空间中移动成本的可证明优先级技术，用于提高深度神经网络（DNNs）软件的质量。它提供了形式上的鲁棒性保证和相对较高的效益。 |
| [^8] | [Local Minima Drive Communications in Cooperative Interaction.](http://arxiv.org/abs/2307.09364) | 人机交互中的合作任务要求代理在适当的时刻通过通信来调整意图，以达到全局解。 |
| [^9] | [MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments.](http://arxiv.org/abs/2307.09361) | MOCA是一种自监督学习方法，通过预测掩码式在线码本分配来实现表示学习。它同时具备良好的语境推理属性和对图像扰动的不变性，并在低样本设置和各种评估协议中取得了最新的最先进结果，训练速度比之前的方法快3倍以上。 |
| [^10] | [Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints.](http://arxiv.org/abs/2307.09342) | 该论文提出了一种学习选择伪布尔和线性整数约束的SAT编码的方法，通过使用监督机器学习方法和一组特征，可以有效地选择编码方式，并且专门为伪布尔和线性约束设计的新特征能够取得更好的性能。 |
| [^11] | [Company2Vec -- German Company Embeddings based on Corporate Websites.](http://arxiv.org/abs/2307.09332) | Company2Vec是一种基于企业网站数据的表示学习模型，通过分析企业活动并保持语义结构，生成细粒度的企业嵌入。这些嵌入可以用于语义商业分析、行业预测和无监督学习任务。 |
| [^12] | [Exploiting Field Dependencies for Learning on Categorical Data.](http://arxiv.org/abs/2307.09321) | 传统的分类数据学习方法忽视了字段间的依赖关系，我们提出了一种利用字段依赖关系进行学习的方法，通过学习全局字段依赖矩阵并在实例级别上细化依赖矩阵，改进了字段之间的建模效果。 |
| [^13] | [Biomaker CA: a Biome Maker project using Cellular Automata.](http://arxiv.org/abs/2307.09320) | 介绍了生物制造者CA项目，利用细胞自动机模拟复杂的生物群系，并在GPU上通过Python JAX框架进行高性能计算。展示了植物代理如何生长、存活、繁殖和进化，形成稳定和不稳定的生物群系。介绍了端到端元进化和培养皿元进化的方法来使模型在恶劣环境中生存。 |
| [^14] | [Rumor Detection with Diverse Counterfactual Evidence.](http://arxiv.org/abs/2307.09296) | 本文提出了一种称为DCE-RD的多样逆事实证据框架，用于谣言检测，通过利用事件图的多样逆事实证据作为多视角解释，进一步聚合以获得鲁棒的谣言检测结果。 |
| [^15] | [Llama 2: Open Foundation and Fine-Tuned Chat Models.](http://arxiv.org/abs/2307.09288) | Llama 2是一个优化的聊天模型，通过fine-tuned技术和安全改进，表现优于开源模型，并可作为闭源模型的替代选择。 |
| [^16] | [Improving Text Semantic Similarity Modeling through a 3D Siamese Network.](http://arxiv.org/abs/2307.09274) | 通过引入三维连体网络，我们改进了文本语义相似性建模方法，从而更好地保留了层次化的语义信息，并提供了更丰富的结构条件来进行全面的下游建模策略概览。 |
| [^17] | [UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data.](http://arxiv.org/abs/2307.09249) | UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。 |
| [^18] | [Towards Sustainable Deep Learning for Multi-Label Classification on NILM.](http://arxiv.org/abs/2307.09244) | 本研究提出了一种面向NILM的新型深度学习模型，通过改进计算和能源效率，实现了对NILM的多标签分类的增强。同时，还提出了一种测试方法，可以比较不同模型在虚拟数据集上的性能。 |
| [^19] | [Human Body Digital Twin: A Master Plan.](http://arxiv.org/abs/2307.09225) | 人体数字孪生技术在医疗保健和健康领域具有巨大潜力，该论文提出了一个五级发展路线图，涵盖了可穿戴设备、数据收集、数据分析和决策系统等组成部分的开发，并强调了支持、安全、成本和伦理问题的重要性。 |
| [^20] | [A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning.](http://arxiv.org/abs/2307.09218) | 遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。 |
| [^21] | [Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models.](http://arxiv.org/abs/2307.09209) | 该论文分析了情感分析和毒性检测模型，以探测对残障人士的明显偏见。研究使用偏见识别框架对社交媒体平台的对话进行了分析，并创建了一个测试语料库来量化明显的残障偏见。研究发现，所研究的模型均存在显著的偏见。 |
| [^22] | [ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint.](http://arxiv.org/abs/2307.09193) | 该论文提出了ESMC模型，通过参数约束来解决大规模在线推荐系统中点击后转化率估计中的样本选择偏差和数据稀疏性问题。此外，通过扩展决策路径，该模型可以更好地捕捉用户的决策意图，并提高推荐性能。 |
| [^23] | [Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning.](http://arxiv.org/abs/2307.09177) | 本文提出了一种使用对比学习训练的相关模型的移动功能检索系统，该系统可以接受直观和上下文的搜索查询，并通过知识蒸馏进行模型压缩，以在设备上高效运行，并进行了与当前部署的搜索基准的比较实验。 |
| [^24] | [Elementary Sets for Logic Programs.](http://arxiv.org/abs/2307.09168) | 本文引入了基本集合的概念，它是逻辑程序中对于"相关"部分的最大未定集合中的最小集合，相比于基本循环，基本集合更简单且可以应用于不相交程序。 |
| [^25] | [Safe Formulas in the General Theory of Stable Models.](http://arxiv.org/abs/2307.09166) | 本论文提出了一种通用稳定模型理论中的安全公式，证明了安全句子和其继承结果具有相同的稳定模型，并且可以通过一个简单句法形式的公式来描述。 |
| [^26] | [Enhancing Network Slicing Architectures with Machine Learning, Security, Sustainability and Experimental Networks Integration.](http://arxiv.org/abs/2307.09151) | 本论文介绍了通过机器学习、安全性、可持续性和实验网络集成来增强网络切片架构，在满足不同领域需求的同时，推进6G高度需求的应用。 |
| [^27] | [Machine Learning for SAT: Restricted Heuristics and New Graph Representations.](http://arxiv.org/abs/2307.09141) | 通过使用机器学习模型在SAT求解的初始阶段，然后将控制权交给传统启发式方法，可以减少步骤的数量和整体运行时间。同时，我们还介绍了一种针对转换来自其他领域的SAT问题的Graph-Q-SAT的修改版。 |
| [^28] | [DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation.](http://arxiv.org/abs/2307.09136) | 该论文提出了一种名为DropMix的方法，通过排除一定比例的数据来减少混合样本数据增强（MSDA）中的类别相关性。在两个数据集上的实验证明了该方法可以提高之前因为MSDA而下降的类别的性能，并增加整体的平均准确率。 |
| [^29] | [Cloud-native RStudio on Kubernetes for Hopsworks.](http://arxiv.org/abs/2307.09132) | 该论文介绍了使用基于Kubernetes的云原生技术在Hopsworks上实现的多租户RStudio系统，解决了多租户环境中的性能隔离、安全性和扩展性问题，同时实现了安全数据共享和用户协作。 |
| [^30] | [BOLD: A Benchmark for Linked Data User Agents and a Simulation Framework for Dynamic Linked Data Environments.](http://arxiv.org/abs/2307.09114) | 本文介绍了BOLD基准和模拟框架，用于测试链接数据代理的性能。通过模拟动态链接数据环境和执行特定任务，该框架提供了检查任务执行的手段并测量代理的性能。 |
| [^31] | [A Survey on Multi-Objective Neural Architecture Search.](http://arxiv.org/abs/2307.09099) | 这篇论文综述了多目标神经架构搜索（MONAS）的主要研究工作，介绍了领域内的分类和表述问题，并提供了一个目标列表和一些新的目标。 |
| [^32] | [DiTTO: Diffusion-inspired Temporal Transformer Operator.](http://arxiv.org/abs/2307.09072) | DiTTO是一种扩散启发的算子学习方法，通过结合Transformer架构的元素，无需时间离散化连续解决时间相关PDEs，并在多维度的各种PDE中取得了最先进的准确性结果。 |
| [^33] | [Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words.](http://arxiv.org/abs/2307.09059) | 本研究提出了一个新的框架，通过探索文本中的文字的力量，实现了准确地将抽象的文本描述映射到具体的图像，从而实现了文本到图像的人物检索。 |
| [^34] | [QMNet: Importance-Aware Message Exchange for Decentralized Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2307.09051) | QMNet是一种基于重要性的消息交换的去中心化多智能体强化学习方法，通过设计重要性感知调度策略和利用消息预测机制来提高性能。 |
| [^35] | [R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut.](http://arxiv.org/abs/2307.09050) | 本文提出了一种增强Transformer视觉分类模型可解释性的方法，通过加权输出和剪切两个模块，生成密集的类别特定可解释性地图。 |
| [^36] | [FedDefender: Client-Side Attack-Tolerant Federated Learning.](http://arxiv.org/abs/2307.09048) | FedDefender是一种面向客户端的联邦学习防御机制，通过抗攻击的本地元更新和全局知识蒸馏，帮助良性客户端训练稳健的本地模型，避免恶意模型更新的不利影响。 |
| [^37] | [Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature.](http://arxiv.org/abs/2307.09047) | 这篇论文提出了一种使用多模态机器学习的方法，通过对文本、字体特征和PDF的位图图像渲染进行融合分类，成功实现了从科学文献中提取定理和证明的目标。在文本模态方面，通过在11 GB的科学语料库上预训练一个新的语言模型，得到了与在160 GB预训练的模型相似的性能，同时具有更快的收敛速度和更少的微调数据要求。 |
| [^38] | [Emotional Intelligence of Large Language Models.](http://arxiv.org/abs/2307.09042) | 这项研究评估了大型语言模型（LLMs）的情感智能（EI），并提出了一种新的心理测量评估方法，该方法重点考察了情感理解（EU）能力。评估结果表明，大多数主流LLMs在情感理解方面表现良好。 |
| [^39] | [PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation.](http://arxiv.org/abs/2307.09036) | PromptMagician是一个交互式的提示工程系统，可以帮助用户针对文本到图像生成任务发展出高效的提示，并通过多级可视化和个性化探索支持用户在输入提示过程中的交互和迭代。 |
| [^40] | [Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students.](http://arxiv.org/abs/2307.09014) | 本研究利用KeyBERT和SNA方法，探索工程学生对自动驾驶车辆政策的接受度。通过文本挖掘分析研究生的意见，从而填补了终端用户对这些政策接受度的分析空白。 |
| [^41] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^42] | [Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction.](http://arxiv.org/abs/2307.09004) | Ord2Seq是一种将序回归问题转化为标签序列预测的方法，通过递归的二元分类步骤微妙地区分相邻类别，在不同场景中达到优于现有方法的性能表现。 |
| [^43] | [EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation.](http://arxiv.org/abs/2307.08988) | EVIL是一种用于半监督医学图像分割的方法，通过引入证据推理学习，可以在单次前向传递中推断准确的不确定性量化，生成可信的伪标签，并取得竞争性能。 |
| [^44] | [AI-assisted Improved Service Provisioning for Low-latency XR over 5G NR.](http://arxiv.org/abs/2307.08987) | 该论文提出了一种基于AI辅助的5G NR下低延迟XR服务提供方案，通过利用预测帧进行处理而非仅仅依赖实际帧，实现了对XR服务的改进和多倍增加支持的效果，并提供了关键的网络设计见解。 |
| [^45] | [PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM.](http://arxiv.org/abs/2307.08985) | PromptCrafter是一种混合倡议系统，通过与LLM的对话逐步生成文本到图像提示。用户可以高效探索模型能力，并通过回答澄清问题来优化提示。 |
| [^46] | [Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines.](http://arxiv.org/abs/2307.08974) | 这个论文介绍了ChatGPT的开发和与之类似的生成人工智能和大型语言模型的广泛应用。为了解决关于它们如何道德应用、使用和披露的问题，提出了CANGARU指南。该指南旨在促进全球学术界对这些技术的道德使用、披露和适当报告的统一共识。 |
| [^47] | [Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information.](http://arxiv.org/abs/2307.08964) | 本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。 |
| [^48] | [REX: Rapid Exploration and eXploitation for AI Agents.](http://arxiv.org/abs/2307.08962) | 本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。 |
| [^49] | [Siamese Networks for Weakly Supervised Human Activity Recognition.](http://arxiv.org/abs/2307.08944) | 本文提出了一种使用Siamese网络进行弱监督人体活动识别的模型，通过仅利用数据样本对的相似性进行训练，该模型可以作为不同聚类算法的度量标准，在三个数据集上的评估结果证明了其有效性。 |
| [^50] | [IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness.](http://arxiv.org/abs/2307.08933) | IxDRL是一种基于有趣分析的新型可解释深度强化学习工具包，具备能力感知机制，能够提供人类操作员对RL代理能力的整体视图。 |
| [^51] | [Unsupervised Deep Graph Matching Based on Cycle Consistency.](http://arxiv.org/abs/2307.08930) | 本文提出了一种基于循环一致性的无监督深度图匹配方法，不需要真实对应的关键点对，通过在同一对象类别的图像之间强制匹配一致性来进行自我监督学习，该方法具有很高的灵活性，并且在无监督图匹配方面达到了最新的最先进水平。 |
| [^52] | [Federated Large Language Model: A Position Paper.](http://arxiv.org/abs/2307.08925) | 我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。 |
| [^53] | [Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees.](http://arxiv.org/abs/2307.08920) | 本论文介绍了一套新的连续时间强化学习算法，用于控制仿射非线性系统。这些算法解决了现有方法面临的复杂性、数值条件和维度扩展等设计挑战。 |
| [^54] | [Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology.](http://arxiv.org/abs/2307.08897) | 本文提出了一种基于多智能体强化学习方法的一型糖尿病患者个性化血糖控制系统，通过显著改善血糖控制、减少血糖变异性以及预防低血糖事件等，该方法具有潜力成为一种有效的治疗方法。 |
| [^55] | [AI for the Generation and Testing of Ideas Towards an AI Supported Knowledge Development Environment.](http://arxiv.org/abs/2307.08876) | 这项研究通过利用机器学习和大语言模型，提出了一种生成式AI方法，用于生成上下文相关的解决方案，但当前还无法支持思想的可追溯性。同时，研究也指出生成式AI在真实性、参考和准确地地图等方面还存在困难。 |
| [^56] | [An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient.](http://arxiv.org/abs/2307.08873) | 本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。 |
| [^57] | [Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach.](http://arxiv.org/abs/2307.08859) | 本文提出了一种新的图神经网络课程学习方法，通过引入图复杂性形式化和模型能力作为困难度标准，以及考虑样本困难度和模型能力的不同视角进行训练，实现了对细粒度图困难度标准的纳入。 |
| [^58] | [Autoregressive Diffusion Model for Graph Generation.](http://arxiv.org/abs/2307.08849) | 提出了一种自回归扩散模型用于图生成，通过定义节点吸收扩散过程和设计扩散排序网络以及去噪网络，能在离散图空间中高效地生成多样性的高质量图形。 |
| [^59] | [Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models.](http://arxiv.org/abs/2307.08816) | 本文介绍了一种利用强化学习代理模型加速Benders分解方法的方法，并通过实验证明了其相对于其他加速方案的30%更快的平均收敛速度。 |
| [^60] | [Operator Guidance Informed by AI-Augmented Simulations.](http://arxiv.org/abs/2307.08810) | 本文介绍了一种基于AI增强模拟指导的操作员指导方法，利用LSTM神经网络估计双峰双向海况下船舶响应统计量。通过对比低保真度和高保真度结果，证明了该方法的有效性。 |
| [^61] | [Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels.](http://arxiv.org/abs/2307.08809) | 本论文提出了FedLabel方法，在联邦学习中，客户端根据数据的专业性选择本地或全局模型对无标签数据进行伪标记，并通过全局-本地一致性正则化来利用本地和全局模型的知识。 |
| [^62] | [Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2307.08794) | 这篇论文提出了一个简单的框架，用于在多时间尺度多智能体强化学习中学习非平稳策略。他们利用智能体时间尺度的信息定义了周期性时间编码，并通过周期性多智能体策略学习多时间尺度引入的非平稳性的效果。他们还提出了一个使用神经网络的策略梯度算法来学习这些策略。 |
| [^63] | [GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution.](http://arxiv.org/abs/2307.08775) | GEAR是一种通用且高效的工具解决方案，通过将工具对应和执行分别委托给小型语言模型和大型语言模型，在不依赖任务示范的情况下实现了更高的性能和精确度。 |
| [^64] | [Reflections from the Workshop on AI-Assisted Decision Making for Conservation.](http://arxiv.org/abs/2307.08774) | 这个研讨会总结了AI辅助决策在保护中的应用，并提出了资源分配、规划和生物多样性保护干预中的关键研究问题，呼吁合作努力解决真实世界的保护挑战。 |
| [^65] | [A mixed policy to improve performance of language models on math problems.](http://arxiv.org/abs/2307.08767) | 本文提出了一种混合策略的探索方法，利用强化学习来改进语言模型在数学问题上的性能，通过在抽象层和第二层采用不同的探索方式，取得了超过2%的性能增益。 |
| [^66] | [Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices.](http://arxiv.org/abs/2307.08766) | 该研究基于机器学习模型对光电容抗（PPG）信号进行训练，评估了使用可穿戴设备进行连续监测时的准确性和可靠性，并提出了解决干扰因素的方法。 |
| [^67] | [Where Did the President Visit Last Week? Detecting Celebrity Trips from News Articles.](http://arxiv.org/abs/2307.08721) | 该论文提出了一种从新闻文章中检测名人行程的方法，克服了文章间的异质性和噪声干扰，为进行大规模和网络分析提供了便利。 |
| [^68] | [TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.](http://arxiv.org/abs/2307.08674) | TableGPT是一个统一的框架，利用大型语言模型（LLMs）和外部功能命令使LLMs能够无缝地与表格进行交互，实现广泛的功能，并提供便利和可访问性给用户。其中的创新是全局表格表示的概念，使LLMs能够全面理解表格的结构和内容。 |
| [^69] | [Deep Cross-Modal Steganography Using Neural Representations.](http://arxiv.org/abs/2307.08671) | 本文提出了一种利用隐式神经表示进行深度跨模态隐写术的框架，可以隐藏各种格式的秘密数据，在实验中证明了其可扩展性和适应性。 |
| [^70] | [Nonlinear Processing with Linear Optics.](http://arxiv.org/abs/2307.08533) | 该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。 |
| [^71] | [Derivation-Graph-Based Characterizations of Decidable Existential Rule Sets.](http://arxiv.org/abs/2307.08481) | 本文通过定义导出图和无回路导出图的概念，建立了一种特征化方法，使得贪婪有界宽度集合和无回路导出图集合相等，从而提高了我们对存在规则的分析证明论的理解。 |
| [^72] | [Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model.](http://arxiv.org/abs/2307.08424) | 本论文提出了一种在标签仅黑盒场景下的模型逆推攻击方法，使用条件扩散模型恢复目标的精确样本，无需额外的优化。 |
| [^73] | [A Recursive Bateson-Inspired Model for the Generation of Semantic Formal Concepts from Spatial Sensory Data.](http://arxiv.org/abs/2307.08087) | 本文提出了一种基于Bateson的启发，从复杂的空间感知数据中生成层次概念结构的符号方法。 |
| [^74] | [Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training.](http://arxiv.org/abs/2307.07909) | DualMind使用双阶段训练策略，在控制任务中学习共同知识，并通过模仿行为在不同上下文中做出决策。在实验中，DualMind在MetaWorld和Habitat上表现优于其他通用性代理，具有超过50%和70%的提升。 |
| [^75] | [Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer.](http://arxiv.org/abs/2307.07754) | 该论文提出了一种双向可变形运动调制方法，用于基于视频的人体姿态转移。通过几何核偏移和自适应权重调制，同时实现特征对齐和风格转移。与传统方法相比，该方法能够更好地处理服装上的结构图案和不连续的姿势转移，并提供更加满意的结果。 |
| [^76] | [Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning.](http://arxiv.org/abs/2307.07250) | 通过敌对双机器学习方法，可以量化和缓解深度神经网络在面对敌对输入时的脆弱性。 |
| [^77] | [Secrets of RLHF in Large Language Models Part I: PPO.](http://arxiv.org/abs/2307.04964) | 本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。 |
| [^78] | [Gradient Surgery for One-shot Unlearning on Generative Model.](http://arxiv.org/abs/2307.04550) | 本文介绍了一种针对生成模型的一次性学习梯度手术方法，通过操纵梯度来消除数据对模型的影响，并在理论上进行了分析。 |
| [^79] | [A Survey on Evaluation of Large Language Models.](http://arxiv.org/abs/2307.03109) | 本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。 |
| [^80] | [Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation.](http://arxiv.org/abs/2307.02839) | 本论文提出一种新的方法使用LLM进行新闻摘要生成，通过进化调优事件模式群体，提高生成结果的准确性和可靠性。 |
| [^81] | [Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory.](http://arxiv.org/abs/2307.00497) | 该论文提出了一个无需使用记忆的联邦类增量学习框架，通过生成模型合成过去分布的样本，从而缓解联邦学习中的灾难性遗忘问题。 |
| [^82] | [Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese.](http://arxiv.org/abs/2306.15788) | 该研究评估了GPT-3.5和GPT-4在巴西葡萄牙语语法错误修正方面的有效性，结果显示虽然GPT-4的召回率较高，但语言模型倾向于过度修正。 |
| [^83] | [SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design.](http://arxiv.org/abs/2306.15656) | SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。 |
| [^84] | [Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?.](http://arxiv.org/abs/2306.10224) | 研究发现生成式 AI 工具 ChatGPT 可以更有效地展示股票市场相关信息，提出了信息膨胀指标并证明其与负面的资本市场后果相关，同时展示其在构建针对性总结方面的效果。 |
| [^85] | [Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction.](http://arxiv.org/abs/2306.09662) | 本文提出了一种合作的多目标架构，称为MOMA-DDPG，用于交通信号控制和碳减排问题。该方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。结果显示，该方法优于现有最先进的方法，并解决了等待时间和碳排放量两个问题。 |
| [^86] | [CLC: Cluster Assignment via Contrastive Representation Learning.](http://arxiv.org/abs/2306.05439) | 本文提出了一种基于对比学习的聚类方法（CLC），它使用对比学习直接学习聚类分配，并在大规模数据集上取得了更好的聚类性能。 |
| [^87] | [Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks.](http://arxiv.org/abs/2305.16044) | 本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。 |
| [^88] | [Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention.](http://arxiv.org/abs/2305.13115) | 这篇论文提出了一种基于因果推理的框架，通过直接监督注意力函数，提供了强大的监督信号，使得基于注意力的图神经网络在嘈杂的图表达中更加稳健和具有一般性。 |
| [^89] | [Evaluating Open-QA Evaluation.](http://arxiv.org/abs/2305.12421) | 本研究侧重于评估开放式问答（Open-QA）任务的方法，引入了一个新的任务QA-Eval和数据集EVOUNA，通过人工评估方法来评估AI生成的答案的准确性。我们调查了与人工评估相关的方法，并讨论了当前方法的缺陷和改进方法。我们相信这对于未来的自动评估工具发展和研究具有价值。 |
| [^90] | [Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees.](http://arxiv.org/abs/2305.11997) | 本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。 |
| [^91] | [Capturing Emerging Complexity in Lenia.](http://arxiv.org/abs/2305.09378) | 研究人工生命平台Lenia，通过识别复杂新兴行为的度量标准和使用遗传算法产生不同行为的结果，以进化出更好的Lenia行为。 |
| [^92] | [Scalable Coupling of Deep Learning with Logical Reasoning.](http://arxiv.org/abs/2305.07617) | 本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。 |
| [^93] | [Distributional Multi-Objective Decision Making.](http://arxiv.org/abs/2305.05560) | 该论文介绍了一种分布式多目标决策制定的方法，其中引入了分布式不支配集和凸分布不支配集的概念，证明了它们可以包含所有最优解，通过实验验证了方法的有效性。 |
| [^94] | [Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts.](http://arxiv.org/abs/2304.03209) | 提出了一种名为MORSE的基于隐式解剖渲染的通用神经渲染框架，旨在在医学图像分割中帮助融合高级语义相关内容和低级解剖特征。 |
| [^95] | [ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast.](http://arxiv.org/abs/2304.02689) | 本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。 |
| [^96] | [SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model.](http://arxiv.org/abs/2303.05118) | SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。 |
| [^97] | [Open-Vocabulary Affordance Detection in 3D Point Clouds.](http://arxiv.org/abs/2303.02401) | 本文提出了一种在3D点云中进行无限数量支撑检测的开放词汇支撑检测方法，通过同时学习支撑文本和点特征来利用支撑之间的语义关系，实现了零-shot检测，能够在没有注释示例的情况下检测以前未见到的支撑。实验结果表明，OpenAD在各种设置上表现出优异性能。 |
| [^98] | [Persian topic detection based on Human Word association and graph embedding.](http://arxiv.org/abs/2302.09775) | 本文提出了一种基于人类词汇关联的波斯语社交媒体话题检测框架，该方法利用了词汇关联和关联引力力量生成图，并通过嵌入和聚类方法提取话题。 |
| [^99] | [Internally Rewarded Reinforcement Learning.](http://arxiv.org/abs/2302.00270) | 这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。 |
| [^100] | [Execution-based Code Generation using Deep Reinforcement Learning.](http://arxiv.org/abs/2301.13816) | 使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。 |
| [^101] | [A Human Word Association based model for topic detection in social networks.](http://arxiv.org/abs/2301.13066) | 本文提出了一个基于人类词汇联想的社交网络主题检测框架，通过考虑语言结构并设计专门的抽取算法，在FA-CUP数据集上取得了比其他方法更好的性能。 |
| [^102] | [Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning.](http://arxiv.org/abs/2212.03181) | 本文提出了一种基于漏斗函数的强化学习算法，用于在连续状态空间中学习鲁棒满足信号时态逻辑规范的时间依赖策略。 |
| [^103] | [Inverting Cryptographic Hash Functions via Cube-and-Conquer.](http://arxiv.org/abs/2212.02405) | 该研究应用了Cube-and-Conquer方法将MD4和MD5的步骤缩减版本进行反转。通过逐步修改Dobbertin约束来生成MD4的反转问题，并使用立方阶段的立方和征服方法进行反转。 |
| [^104] | [Continuous Monte Carlo Graph Search.](http://arxiv.org/abs/2210.01426) | 连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。 |
| [^105] | [DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation.](http://arxiv.org/abs/2207.09920) | 本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。 |
| [^106] | [Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems.](http://arxiv.org/abs/2206.07082) | 本文首次启动了对非凸非光滑问题上随机优化的系统稳定性和泛化分析，引入了新颖的算法稳定性度量，并建立了它们与种群梯度和经验梯度之间的定量连接。 |
| [^107] | [FedFormer: Contextual Federation with Attention in Reinforcement Learning.](http://arxiv.org/abs/2205.13697) | FedFormer是一种新的强化学习联邦策略，利用Transformer Attention上下文地汇总来自不同学习智能体的嵌入，具有更高的回报和更高的效率。 |
| [^108] | [A theoretical framework for self-supervised MR image reconstruction using sub-sampling via variable density Noisier2Noise.](http://arxiv.org/abs/2205.10278) | 本研究提出了一个基于Noisier2Noise框架的自监督磁共振图像重建理论框架，解释了SSDU方法的性能，并提出了两种修改。 |

# 详细

[^1]: 进展中的序列数据科学

    Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])

    [http://arxiv.org/abs/2307.09477](http://arxiv.org/abs/2307.09477)

    本文讨论了序列数据科学的发展，并介绍了一种新的研究议程：使用序列结构来衡量和计算对象之间的关系，并从中推断知识。这种方法具有广泛的学科应用价值。

    

    排序是衡量（经验）数据中对象之间关系的主要方法之一。然而，与使用对象的数字属性的方法相比，发展出的序列方法数量相对较少。造成这一情况的原因之一是在上个世纪，计算资源的可用性有限，无法满足序列计算所需。此外，对于这一研究领域来说，另一个重要原因是基于顺序的方法通常被视为对实际数据应用过于数学严谨。因此，本文将讨论不同的方法来衡量和“计算”序列结构（一类特定的有向图），并展示如何从其中推断知识。我们的目标是将序列数据科学建立为一项全新的研究议程。除了与其他重要的机器学习和知识表示方法的交叉互补外，广泛的学科领域也将受益于此。

    Order is one of the main instruments to measure the relationship between objects in (empirical) data. However, compared to methods that use numerical properties of objects, the amount of ordinal methods developed is rather small. One reason for this is the limited availability of computational resources in the last century that would have been required for ordinal computations. Another reason -- particularly important for this line of research -- is that order-based methods are often seen as too mathematically rigorous for applying them to real-world data. In this paper, we will therefore discuss different means for measuring and 'calculating' with ordinal structures -- a specific class of directed graphs -- and show how to infer knowledge from them. Our aim is to establish Ordinal Data Science as a fundamentally new research agenda. Besides cross-fertilization with other cornerstone machine learning and knowledge representation methods, a broad range of disciplines will benefit from t
    
[^2]: 过度思考真相：理解语言模型如何处理虚假演示

    Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])

    [http://arxiv.org/abs/2307.09476](http://arxiv.org/abs/2307.09476)

    该论文研究了现代语言模型在处理虚假演示时出现的过度思考和错误归纳头现象。通过研究模型的内部表示，发现模型在中间层之后对错误演示的处理准确性逐渐降低，并指出了错误归纳头机制可能导致过度思考现象。

    

    现代语言模型可以通过少量示范进行复杂模式的模仿学习，使其能够在没有微调的情况下完成具有挑战性的任务。然而，模仿也可能导致模型在上下文中重现不准确或有害的内容。我们通过模型的内部表示来研究有害的模仿，并确定了两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，在给出正确与错误的少量示范时，我们从中间层解码预测。在早期层中，两种示范引起了相似的模型行为，但在某个“关键层”之后，给出错误示范的准确性逐渐降低。第二个现象，错误归纳头，可能是过度思考的一种机制性原因：这些是位于较晚层的头部，它们关注并复制先前示范中的错误信息，其削弱会减少过度思考现象。

    Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
    
[^3]: 无监督条件槽注意力用于物体中心学习

    Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])

    [http://arxiv.org/abs/2307.09437](http://arxiv.org/abs/2307.09437)

    本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。

    

    提取物体层次的表示以进行后续的推理任务是人工智能中涌现的一个领域。在无监督的设置中学习物体中心表示面临多个挑战，其中一个关键挑战是将任意数量的物体实例绑定到专门的物体槽位。最近的物体中心表示方法如槽位注意力利用迭代式注意力学习具有动态推理层级绑定的可组合表示，但未能达到专门的槽位绑定。为了解决这个问题，本文提出了一种使用新颖的概率性槽字典（PSD）的无监督条件槽注意力。我们将PSD定义为（i）抽象的物体层次属性向量作为键，（ii）参数化高斯分布作为相应的值。我们在多个后续任务中展示了学习到的具体物体层次调节分布的好处，包括物体发现、组合式场景生成和组合式视觉推理。

    Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
    
[^4]: SLMGAN: 在GAN中利用语音语言模型表示进行无监督零样本语音转换

    SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs. (arXiv:2307.09435v1 [eess.AS])

    [http://arxiv.org/abs/2307.09435](http://arxiv.org/abs/2307.09435)

    SLMGAN提出了一种利用语音语言模型表示进行无监督零样本语音转换的新方法，在生成对抗网络框架中实现了优于现有模型的性能。

    

    近年来，大规模预训练的语音语言模型（SLMs）在各种生成式语音建模应用中展现出了显著的进展，如文本到语音合成、语音转换和语音增强。这些应用通常涉及将文本或语音输入映射到预训练SLM表示，然后解码目标语音。本文介绍了一种新方法SLMGAN，以利用SLM表示在生成对抗网络（GAN）框架中进行分类任务，特别是用于语音转换。在StarGANv2-VC的基础上，我们在基于mel的鉴别器上添加了我们的基于SLM的WavLM鉴别器，以及我们新设计的SLM特征匹配损失函数，从而实现了一种无监督的零样本语音转换系统，在训练过程中不需要文本标签。主观评估结果表明，SLMGAN在零样本语音转换模型方面优于现有的最先进模型。

    In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms
    
[^5]: 在人工智能中平衡隐私和进步：基于组织病理学的医学研究和教育中的匿名化

    Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education. (arXiv:2307.09426v1 [cs.AI])

    [http://arxiv.org/abs/2307.09426](http://arxiv.org/abs/2307.09426)

    本论文讨论了在开发人工智能算法时，在组织病理学中平衡隐私和进步的挑战，并提出了解决方案。

    

    生物医学研究的进展严重依赖于大量医学数据的获取。在组织病理学的情况下，全切片图像（WSI）和临床病理学信息对于开发数字病理学（DP）的人工智能（AI）算法非常有价值。将医学数据传输“尽量开放”提高了数据用于次级目的的可用性，但同时也对患者隐私构成风险。同时，现有法规要求保持医学数据“尽量封闭”以避免再识别风险。通常，这些法规要求删除敏感数据，但不考虑现代图像匹配算法可能导致的数据链接攻击的可能性。此外，DP的标准化不足使得在所有WSI格式上建立单一解决方案变得更加困难。这些挑战给生物信息学研究者在开发AI算法时在隐私与进步之间取得平衡带来了问题。本文探讨了这些挑战，并提出了一些解决方案。

    The advancement of biomedical research heavily relies on access to large amounts of medical data. In the case of histopathology, Whole Slide Images (WSI) and clinicopathological information are valuable for developing Artificial Intelligence (AI) algorithms for Digital Pathology (DP). Transferring medical data "as open as possible" enhances the usability of the data for secondary purposes but poses a risk to patient privacy. At the same time, existing regulations push towards keeping medical data "as closed as necessary" to avoid re-identification risks. Generally, these legal regulations require the removal of sensitive data but do not consider the possibility of data linkage attacks due to modern image-matching algorithms. In addition, the lack of standardization in DP makes it harder to establish a single solution for all formats of WSIs. These challenges raise problems for bio-informatics researchers in balancing privacy and progress while developing AI algorithms. This paper explo
    
[^6]: 在NetHack中的模仿学习的规模律

    Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])

    [http://arxiv.org/abs/2307.09423](http://arxiv.org/abs/2307.09423)

    本文研究了在NetHack游戏中的模仿学习，发现通过扩大模型和数据规模可以改进模仿学习的效果，并建立了训练计算最优IL代理人的幂律。

    

    模仿学习 (IL) 是机器学习中最常用的方法之一。然而，虽然强大，但许多研究发现它往往不能完全恢复出潜在的专家行为。然而，这些研究没有深入探究模型和数据规模的扩大在其中的作用。受最近在自然语言处理 (NLP) 领域的工作的启发，在那里“扩大规模”已经导致了越来越有能力的领域特定语言模型 (LLMs)，我们研究了仔细扩大模型和数据规模是否可以在模仿学习的设置中带来类似的改进。为了展示我们的发现，我们将重点放在 NetHack 游戏上，这是一个具有程序生成、随机性、长期依赖性和部分可观测性的具有挑战性的环境。我们发现 IL 的损失和平均回报随着计算预算的变化而平滑变化且强相关，从而在模型大小和样本数量方面为训练计算最优的 IL 代理人的计算预算建立了幂律。我们预测并训练了几个具有 IL 的NetHack代理。

    Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
    
[^7]: CertPri: 基于特征空间中的移动成本的可证明优先级技术，用于深度神经网络中

    CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space. (arXiv:2307.09375v1 [cs.SE])

    [http://arxiv.org/abs/2307.09375](http://arxiv.org/abs/2307.09375)

    CertPri是一种基于特征空间中移动成本的可证明优先级技术，用于提高深度神经网络（DNNs）软件的质量。它提供了形式上的鲁棒性保证和相对较高的效益。

    

    深度神经网络（DNNs）在各种软件系统中表现出卓越的性能，但也表现出失控行为，甚至导致不可逆转的灾难。因此，识别基于DNN的软件的失控行为并提高DNN的质量至关重要。测试输入优先级是确保DNN质量的一种最吸引人的方式之一，它根据测试输入的优先级排序，以便在有限的时间和手动标记工作中能够更早地识别出更多的有缺陷的输入。然而，现有的优先级方法在可证明性、效果性和通用性三个方面仍存在局限性。为了克服这些挑战，我们提出了CertPri，一种基于DNN特征空间中的测试输入移动成本角度设计的测试输入优先级技术。CertPri在以下三个关键方面与以前的工作不同：（1）可证明性：它为移动成本提供了形式上的鲁棒性保证；（2）有效性：它利用形式上保证的移动成本

    Deep neural networks (DNNs) have demonstrated their outperformance in various software systems, but also exhibit misbehavior and even result in irreversible disasters. Therefore, it is crucial to identify the misbehavior of DNN-based software and improve DNNs' quality. Test input prioritization is one of the most appealing ways to guarantee DNNs' quality, which prioritizes test inputs so that more bug-revealing inputs can be identified earlier with limited time and manual labeling efforts. However, the existing prioritization methods are still limited from three aspects: certifiability, effectiveness, and generalizability. To overcome the challenges, we propose CertPri, a test input prioritization technique designed based on a movement cost perspective of test inputs in DNNs' feature space. CertPri differs from previous works in three key aspects: (1) certifiable: it provides a formal robustness guarantee for the movement cost; (2) effective: it leverages formally guaranteed movement c
    
[^8]: 在合作互动中，局部极小值驱动通信

    Local Minima Drive Communications in Cooperative Interaction. (arXiv:2307.09364v1 [cs.AI])

    [http://arxiv.org/abs/2307.09364](http://arxiv.org/abs/2307.09364)

    人机交互中的合作任务要求代理在适当的时刻通过通信来调整意图，以达到全局解。

    

    人机交互（HRI）中一个重要的开放问题是什么时候一个代理应该决定进行通信，尤其是在合作任务中。感知控制理论（PCT）告诉我们，代理可以通过共享相同的“意图”来合作完成一个联合任务，从而将完成任务所需的工作分布在不同的代理之间。即使对于不具备相同能力和目标可观测的代理，只要其组合行动足以完成任务且搜索空间中没有局部最小值，合作任务也可以在没有任何通信的情况下完成。然而，对于包含局部极小值的任务，只有在适当时刻至少一个代理改变其意图，才能达到全局解，而这只能通过适时的通信来实现。换句话说，在合作任务中，通过适时的通信可以促使代理在适当的时刻调整其意图。

    An important open question in human-robot interaction (HRI) is precisely when an agent should decide to communicate, particularly in a cooperative task. Perceptual Control Theory (PCT) tells us that agents are able to cooperate on a joint task simply by sharing the same 'intention', thereby distributing the effort required to complete the task among the agents. This is even true for agents that do not possess the same abilities, so long as the goal is observable, the combined actions are sufficient to complete the task, and there is no local minimum in the search space. If these conditions hold, then a cooperative task can be accomplished without any communication between the contributing agents. However, for tasks that do contain local minima, the global solution can only be reached if at least one of the agents adapts its intention at the appropriate moments, and this can only be achieved by appropriately timed communication. In other words, it is hypothesised that in cooperative tas
    
[^9]: MOCA: 自监督学习通过预测掩码式在线码本分配实现表示学习

    MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])

    [http://arxiv.org/abs/2307.09361](http://arxiv.org/abs/2307.09361)

    MOCA是一种自监督学习方法，通过预测掩码式在线码本分配来实现表示学习。它同时具备良好的语境推理属性和对图像扰动的不变性，并在低样本设置和各种评估协议中取得了最新的最先进结果，训练速度比之前的方法快3倍以上。

    

    自监督学习可以用于缓解Vision Transformer网络对大型全注释数据集的贪婪需求。不同类别的自监督学习提供了具有良好语境推理属性的表示，例如使用掩码图像建模策略，或者对图像扰动具有不变性的表示，例如使用对比方法。在这项工作中，我们提出了一种单阶段、独立的方法MOCA，使用基于高级特征（而不是像素级细节）定义的新型掩码和预测目标来统一这两种期望的属性。此外，我们展示了如何以协同和计算高效的方式有效地应用这两种学习范式。通过这样做，我们在低样本设置上实现了新的最先进结果，并且在各种评估协议中取得了强大的实验结果，其训练速度至少比之前的方法快3倍。

    Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
    
[^10]: 学习选择伪布尔和线性整数约束的SAT编码

    Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])

    [http://arxiv.org/abs/2307.09342](http://arxiv.org/abs/2307.09342)

    该论文提出了一种学习选择伪布尔和线性整数约束的SAT编码的方法，通过使用监督机器学习方法和一组特征，可以有效地选择编码方式，并且专门为伪布尔和线性约束设计的新特征能够取得更好的性能。

    

    许多约束满足和优化问题可以通过将它们编码为布尔可满足性问题（SAT）的实例来有效地解决。然而，即使是最简单的约束类型在文献中也有很多编码方式，性能差异很大，选择适当的编码方式对于给定的问题实例并不是一件简单的事情。我们采用监督机器学习方法研究选择伪布尔和线性约束的编码问题。我们展示了可以使用标准约束问题的特征集来有效地选择编码方式；然而，我们使用专门为伪布尔和线性约束设计的一组新特征获得了更好的性能。事实上，在选择未见过的问题类别的编码方式时，我们取得了不错的结果。当使用相同的特征集时，我们的结果与AutoFolio相比表现良好。我们讨论了实例特征对于选择编码方式任务的相对重要性。

    Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the
    
[^11]: 基于企业网站的德国企业嵌入Company2Vec

    Company2Vec -- German Company Embeddings based on Corporate Websites. (arXiv:2307.09332v1 [cs.AI])

    [http://arxiv.org/abs/2307.09332](http://arxiv.org/abs/2307.09332)

    Company2Vec是一种基于企业网站数据的表示学习模型，通过分析企业活动并保持语义结构，生成细粒度的企业嵌入。这些嵌入可以用于语义商业分析、行业预测和无监督学习任务。

    

    本论文提出了一种新颖的表示学习应用，即Company2Vec。该模型利用Word2Vec和降维技术从非结构化的企业网站数据中分析企业活动。Company2Vec保持语义语言结构，从而在细粒度行业中创建高效的企业嵌入。这些语义嵌入可以在银行业的各种应用中使用。公司和单词之间的直接关系可以进行语义商业分析（例如某公司的前n个单词）。此外，还提出了一种监督学习应用和评估方法，即行业预测。嵌入的向量结构可以通过余弦距离来测量公司之间的相似性。因此，与标准行业标签（NACE）相比，Company2Vec可以更精细地比较公司。这一特性对于无监督学习任务（如聚类）是相关的。还展示了基于k-means聚类的替代行业细分。

    With Company2Vec, the paper proposes a novel application in representation learning. The model analyzes business activities from unstructured company website data using Word2Vec and dimensionality reduction. Company2Vec maintains semantic language structures and thus creates efficient company embeddings in fine-granular industries. These semantic embeddings can be used for various applications in banking. Direct relations between companies and words allow semantic business analytics (e.g. top-n words for a company). Furthermore, industry prediction is presented as a supervised learning application and evaluation method. The vectorized structure of the embeddings allows measuring companies similarities with the cosine distance. Company2Vec hence offers a more fine-grained comparison of companies than the standard industry labels (NACE). This property is relevant for unsupervised learning tasks, such as clustering. An alternative industry segmentation is shown with k-means clustering on 
    
[^12]: 利用字段依赖性进行分类数据的学习

    Exploiting Field Dependencies for Learning on Categorical Data. (arXiv:2307.09321v1 [cs.LG])

    [http://arxiv.org/abs/2307.09321](http://arxiv.org/abs/2307.09321)

    传统的分类数据学习方法忽视了字段间的依赖关系，我们提出了一种利用字段依赖关系进行学习的方法，通过学习全局字段依赖矩阵并在实例级别上细化依赖矩阵，改进了字段之间的建模效果。

    

    传统的分类数据学习方法往往忽视了数据集中字段（也称为特征）之间的依赖关系，因为它们仅依赖于数据点嵌入的分类/回归损失。相比之下，我们提出了一种新的分类数据学习方法，旨在利用字段之间的依赖关系。我们不是全局建模特征的统计信息（如特征的协方差矩阵），而是学习一个捕捉字段之间依赖关系的全局字段依赖矩阵，并利用不同权重（称为局部依赖建模）在实例级别上改进全局字段依赖矩阵的建模。我们的算法利用元学习范式，即在元学习算法的内部循环中无需使用标签就可以细化依赖矩阵，而外部循环则交织了嵌入矩阵和依赖矩阵的更新。

    Traditional approaches for learning on categorical data underexploit the dependencies between columns (\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the
    
[^13]: 生物制造者CA：使用细胞自动机的生物制造者项目

    Biomaker CA: a Biome Maker project using Cellular Automata. (arXiv:2307.09320v1 [cs.AI])

    [http://arxiv.org/abs/2307.09320](http://arxiv.org/abs/2307.09320)

    介绍了生物制造者CA项目，利用细胞自动机模拟复杂的生物群系，并在GPU上通过Python JAX框架进行高性能计算。展示了植物代理如何生长、存活、繁殖和进化，形成稳定和不稳定的生物群系。介绍了端到端元进化和培养皿元进化的方法来使模型在恶劣环境中生存。

    

    我们介绍了生物制造者CA：使用细胞自动机（CA）的生物制造者项目。在生物制造者CA中，形态发生是一个重要的主题，小种子需要在养分匮乏的环境中成长为植物状的生物体，最终通过变异来繁殖，以使生物群系得以长期存活。我们通过2D网格上的CA规则模拟复杂的生物群系，并通过Python JAX框架在GPU上并行计算。我们展示了该项目允许多种不同类型的环境和“物理”定律，以及不同的模型架构和突变策略。我们进一步分析了一些配置，展示了植物代理如何生长、存活、繁殖和进化，形成稳定和不稳定的生物群系。然后，我们演示了如何通过端到端元进化或更精确高效的方法（称为培养皿元进化）来使模型在恶劣环境中生存。最后，我们展示了如何进行高性能计算以评估生物群系的稳定性和进化趋势。

    We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perfo
    
[^14]: 使用多样的反事实证据进行谣言检测

    Rumor Detection with Diverse Counterfactual Evidence. (arXiv:2307.09296v1 [cs.AI])

    [http://arxiv.org/abs/2307.09296](http://arxiv.org/abs/2307.09296)

    本文提出了一种称为DCE-RD的多样逆事实证据框架，用于谣言检测，通过利用事件图的多样逆事实证据作为多视角解释，进一步聚合以获得鲁棒的谣言检测结果。

    

    社交媒体的增长加剧了虚假新闻对个人和社区的威胁。这对于开发高效和及时的谣言检测方法引起了越来越多的关注。现有的方法借助图神经网络(GNNs)来利用谣言传播过程的后传播模式。然而，由于GNNs的黑箱性质，这些方法缺乏对谣言检测的固有解释性。此外，这些方法由于采用了所有传播模式，结果缺乏鲁棒性。在本文中，我们提出了一种称为DCE-RD的多样逆事实证据框架，用于谣言检测，以解决上述问题。我们的核心思想是利用事件图的多样逆事实证据作为多视角解释，进而聚合以获得鲁棒的谣言检测结果。具体而言，我们的方法首先设计了一个子图生成策略，以高效生成事件图的不同子图。

    The growth in social media has exacerbated the threat of fake news to individuals and communities. This draws increasing attention to developing efficient and timely rumor detection methods. The prevailing approaches resort to graph neural networks (GNNs) to exploit the post-propagation patterns of the rumor-spreading process. However, these methods lack inherent interpretation of rumor detection due to the black-box nature of GNNs. Moreover, these methods suffer from less robust results as they employ all the propagation patterns for rumor detection. In this paper, we address the above issues with the proposed Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our intuition is to exploit the diverse counterfactual evidence of an event graph to serve as multi-view interpretations, which are further aggregated for robust rumor detection results. Specifically, our method first designs a subgraph generation strategy to efficiently generate different subgraphs of the e
    
[^15]: Llama 2: 开放基础和优化聊天模型

    Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v1 [cs.CL])

    [http://arxiv.org/abs/2307.09288](http://arxiv.org/abs/2307.09288)

    Llama 2是一个优化的聊天模型，通过fine-tuned技术和安全改进，表现优于开源模型，并可作为闭源模型的替代选择。

    

    在这项工作中，我们开发并发布了Llama 2，一个包含预训练和优化的大型语言模型（LLM），其规模从70亿到700亿参数不等。我们的优化LLM，称为Llama 2-Chat，在对话使用案例中表现优于开源聊天模型。根据我们对有用性和安全性的人工评估结果，它们可能是闭源模型的合适替代品。我们详细描述了我们在Llama 2-Chat的优化和安全性改进方面的方法，以便让社区能够在我们的工作基础上构建并为LLM的负责任发展做出贡献。

    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
    
[^16]: 通过3D连体网络改进文本语义相似性建模

    Improving Text Semantic Similarity Modeling through a 3D Siamese Network. (arXiv:2307.09274v1 [cs.CL])

    [http://arxiv.org/abs/2307.09274](http://arxiv.org/abs/2307.09274)

    通过引入三维连体网络，我们改进了文本语义相似性建模方法，从而更好地保留了层次化的语义信息，并提供了更丰富的结构条件来进行全面的下游建模策略概览。

    

    近年来，连体网络已成为建模文本语义相似性的一种流行方法。传统方法依赖池化操作来压缩编码中Transformer块的语义表示，从而得到二维语义向量并丢失Transformer块中的层次化语义信息。此外，这种有限的语义向量结构类似于一个被铺平的地形，限制了下游建模中可以应用的方法，因为它们只能在这个平面上进行导航。为解决这个问题，我们提出了一种新颖的用于文本语义相似性建模的3D连体网络，它将语义信息映射到一个更高维的空间中。三维语义张量不仅保留了更精确的空间和特征领域信息，还为全面的下游建模策略提供了必要的结构条件来捕捉这些信息。利用这种结构上的优势，我们引入了几个模块

    Siamese networks have gained popularity as a method for modeling text semantic similarity. Traditional methods rely on pooling operation to compress the semantic representations from Transformer blocks in encoding, resulting in two-dimensional semantic vectors and the loss of hierarchical semantic information from Transformer blocks. Moreover, this limited structure of semantic vectors is akin to a flattened landscape, which restricts the methods that can be applied in downstream modeling, as they can only navigate this flat terrain. To address this issue, we propose a novel 3D Siamese network for text semantic similarity modeling, which maps semantic information to a higher-dimensional space. The three-dimensional semantic tensors not only retains more precise spatial and feature domain information but also provides the necessary structural condition for comprehensive downstream modeling strategies to capture them. Leveraging this structural advantage, we introduce several modules to 
    
[^17]: UniTabE: 面向异构表格数据的统一预训练表格编码器

    UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])

    [http://arxiv.org/abs/2307.09249](http://arxiv.org/abs/2307.09249)

    UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。

    

    自然语言处理（NLP）的最新进展明证了预训练模型的突破性影响，在各种任务上取得了令人印象深刻的结果。本研究旨在将预训练方法的威力扩展到传统被忽视的表格数据领域，该领域由于不同任务固有的众多表格模式而具有挑战性。本工作的主要研究问题围绕异构表格结构的适应性、表格数据的统一预训练协议的建立、学到的知识在任务之间的泛化和可传递性、对多样化下游应用的适应性以及随时间的增量列的纳入进行了探讨。针对这些挑战，我们引入了UniTabE，这是一种创新的方法，旨在以一致的方式处理表格，摆脱了特定表格结构强加的约束。UniTabE的核心概念是对每个基本表格进行表示

    Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
    
[^18]: 面向NILM的多标签分类的可持续深度学习

    Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])

    [http://arxiv.org/abs/2307.09244](http://arxiv.org/abs/2307.09244)

    本研究提出了一种面向NILM的新型深度学习模型，通过改进计算和能源效率，实现了对NILM的多标签分类的增强。同时，还提出了一种测试方法，可以比较不同模型在虚拟数据集上的性能。

    

    非侵入式负载监测（NILM）是从单个计量点获取家庭或企业总电力消耗的电器级数据的过程。电器级数据可以直接用于需求响应应用、能源管理系统以及提高能效和减少碳足迹的意识提高和激励。最近，经典机器学习和深度学习（DL）技术在NILM分类中变得非常流行，并证明在增长的复杂性下对NILM分类非常有效，但随着复杂度的增加，这些方法在训练和操作过程中面临着显著的计算和能源需求。在本文中，我们引入了一种新的DL模型，旨在通过提高计算和能源效率来增强NILM的多标签分类。我们还提出了一种用于使用从测量数据集合成的数据比较不同模型的测试方法。

    Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better 
    
[^19]: 人体数字孪生: 一个总体规划

    Human Body Digital Twin: A Master Plan. (arXiv:2307.09225v1 [cs.AI])

    [http://arxiv.org/abs/2307.09225](http://arxiv.org/abs/2307.09225)

    人体数字孪生技术在医疗保健和健康领域具有巨大潜力，该论文提出了一个五级发展路线图，涵盖了可穿戴设备、数据收集、数据分析和决策系统等组成部分的开发，并强调了支持、安全、成本和伦理问题的重要性。

    

    人体数字孪生具有改变医疗保健和健康的潜力，但其负责和有效的实施需要考虑各种因素。本文概述了当前状态和未来前景，并提出了一个五级发展路线图。路线图涵盖了各种组成部分的发展，如可穿戴设备、数据收集、数据分析和决策系统。本文还强调了必须解决的支持、安全、成本和伦理问题，以确保人体数字孪生的负责和有效实施。所提出的路线图为指导未来发展提供了框架，并为探索人体数字孪生的未来提供了独特的视角，促进了这一快速发展领域中的新学科研究和创新解决方案。

    The human body DT has the potential to revolutionize healthcare and wellness, but its responsible and effective implementation requires consideration of various factors. This article presents a comprehensive overview of the current status and future prospects of the human body DT and proposes a five-level roadmap for its development. The roadmap covers the development of various components, such as wearable devices, data collection, data analysis, and decision-making systems. The article also highlights the necessary support, security, cost, and ethical considerations that must be addressed in order to ensure responsible and effective implementation of the human body DT. The proposed roadmap provides a framework for guiding future development and offers a unique perspective on the future of the human body DT, facilitating new interdisciplinary research and innovative solutions in this rapidly evolving field.
    
[^20]: 深度学习中遗忘现象的全面调查：超越连续学习

    A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])

    [http://arxiv.org/abs/2307.09218](http://arxiv.org/abs/2307.09218)

    遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。

    

    遗忘指的是先前获取的信息或知识的丧失或恶化。尽管现有的关于遗忘的调查主要集中在连续学习方面，但在深度学习中，遗忘是一种普遍现象，可以在各种其他研究领域中观察到。遗忘在研究领域中表现出来，例如由于生成器漂移而在生成模型领域中表现出来，以及由于客户端之间存在异构数据分布而在联邦学习中表现出来。解决遗忘问题涉及到几个挑战，包括在快速学习新任务的同时平衡保留旧任务知识，管理任务干扰与冲突目标，以及防止隐私泄露等。此外，大多数现有的连续学习调查都默认认为遗忘总是有害的。相反，我们的调查认为遗忘是一把双刃剑，在某些情况下可以是有益且可取的，例如隐私保护场景。通过在更广泛的背景下探讨遗忘现象，

    Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
    
[^21]: 自动化的残障主义：探索情感分析和毒性检测模型中的明显残障偏见

    Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])

    [http://arxiv.org/abs/2307.09209](http://arxiv.org/abs/2307.09209)

    该论文分析了情感分析和毒性检测模型，以探测对残障人士的明显偏见。研究使用偏见识别框架对社交媒体平台的对话进行了分析，并创建了一个测试语料库来量化明显的残障偏见。研究发现，所研究的模型均存在显著的偏见。

    

    我们分析情感分析和毒性检测模型，以检测对残障人士的明显偏见。我们采用扰动敏感性分析的偏见识别框架，研究社交媒体平台上与残障人士相关的对话，特别是Twitter和Reddit，在真实社交环境中了解残障偏见是如何传播的。然后，我们创建了“情感中的偏见识别测试”（BITS）语料库，以量化任何情感分析和毒性检测模型中的明显残障偏见。我们的研究使用BITS揭示了四种开放的AIaaS（AI即服务）情感分析工具（TextBlob，VADER，Google Cloud Natural Language API，DistilBERT）和两种毒性检测模型（两个版本的Toxic-BERT）中存在显着的偏见。

    We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
    
[^22]: ESMC:整个空间多任务模型通过参数约束用于点击后转化率

    ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint. (arXiv:2307.09193v1 [cs.AI])

    [http://arxiv.org/abs/2307.09193](http://arxiv.org/abs/2307.09193)

    该论文提出了ESMC模型，通过参数约束来解决大规模在线推荐系统中点击后转化率估计中的样本选择偏差和数据稀疏性问题。此外，通过扩展决策路径，该模型可以更好地捕捉用户的决策意图，并提高推荐性能。

    

    大规模在线推荐系统在互联网上广泛使用，负责点击率（CTR）和点击后转化率（CVR）的估计。然而，传统的CVR估计器存在着样本选择偏差和数据稀疏性问题。通过追踪“曝光_点击_购买”这个决策路径，提出了整个空间模型来解决这两个问题。此外，一些研究者观察到在点击和购买之间存在购买相关行为，可以更好地了解用户的决策意图并提高推荐性能。因此，决策路径已扩展为“曝光_点击_店内动作_购买”，并且可以通过条件概率方法建模。然而，我们观察到条件概率的链式法则并不总是成立。我们报告了概率空间混淆（PSC）问题，并推导了地面实况与估计数学间的差异。

    Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations. However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues. Entire space models were proposed to address the two issues via tracing the decision-making path of "exposure_click_purchase". Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance. Thus, the decision-making path has been extended to "exposure_click_in-shop action_purchase" and can be modeled with conditional probability approach. Nevertheless, we observe that the chain rule of conditional probability does not always hold. We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematical
    
[^23]: 使用对比学习训练的相关模型实现对智能手机设置的直观访问

    Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning. (arXiv:2307.09177v1 [cs.IR])

    [http://arxiv.org/abs/2307.09177](http://arxiv.org/abs/2307.09177)

    本文提出了一种使用对比学习训练的相关模型的移动功能检索系统，该系统可以接受直观和上下文的搜索查询，并通过知识蒸馏进行模型压缩，以在设备上高效运行，并进行了与当前部署的搜索基准的比较实验。

    

    随着智能手机增加了越来越多的新功能，用户越来越难以找到它们，因为这些功能的名称通常很短，记不住太多。在这种情况下，用户可能希望提出描述他们要寻找的功能的上下文查询，但标准的基于词频的搜索无法处理它们。本文提出了一种新颖的移动功能检索系统，可以接受直观和上下文的搜索查询。我们通过对比学习从预训练的语言模型中训练了一个相关模型，以感知查询嵌入和索引的移动功能之间的上下文相关性。此外，为了在设备上使用最低资源高效运行，我们应用了知识蒸馏来压缩模型而不降低太多性能。为了验证我们方法的可行性，我们收集了测试查询，并与当前部署的搜索基准进行了比较实验。结果显示

    The more new features that are being added to smartphones, the harder it becomes for users to find them. This is because the feature names are usually short, and there are just too many to remember. In such a case, the users may want to ask contextual queries that describe the features they are looking for, but the standard term frequency-based search cannot process them. This paper presents a novel retrieval system for mobile features that accepts intuitive and contextual search queries. We trained a relevance model via contrastive learning from a pre-trained language model to perceive the contextual relevance between query embeddings and indexed mobile features. Also, to make it run efficiently on-device using minimal resources, we applied knowledge distillation to compress the model without degrading much performance. To verify the feasibility of our method, we collected test queries and conducted comparative experiments with the currently deployed search baselines. The results show
    
[^24]: 逻辑程序的基本集合

    Elementary Sets for Logic Programs. (arXiv:2307.09168v1 [cs.AI])

    [http://arxiv.org/abs/2307.09168](http://arxiv.org/abs/2307.09168)

    本文引入了基本集合的概念，它是逻辑程序中对于"相关"部分的最大未定集合中的最小集合，相比于基本循环，基本集合更简单且可以应用于不相交程序。

    

    通过引入循环和循环公式的概念，Lin和Zhao证明了一个非不相交逻辑程序的答案集就是满足所有循环的循环公式的Clark完成模型。最近，Gebser和Schaub证明了即使我们将循环公式限制在一种称为"基本循环"的特殊类型的循环中，Lin-Zhao定理仍然正确。在本文中，我们简化和推广了基本循环的概念，并澄清了它的角色。我们提出了基本集合的概念，它几乎等价于非不相交程序的基本循环的概念，但更简单，并且与基本循环不同，可以扩展到包含非直观结果的不相交程序。我们证明了对于程序的"相关"部分来说，最大的未定基本集合恰好是非空未定集合中的最小集合。我们还提出了一个基于图论的非不相交程序基本集合的特征化。

    By introducing the concepts of a loop and a loop formula, Lin and Zhao showed that the answer sets of a nondisjunctive logic program are exactly the models of its Clark's completion that satisfy the loop formulas of all loops. Recently, Gebser and Schaub showed that the Lin-Zhao theorem remains correct even if we restrict loop formulas to a special class of loops called ``elementary loops.'' In this paper, we simplify and generalize the notion of an elementary loop, and clarify its role. We propose the notion of an elementary set, which is almost equivalent to the notion of an elementary loop for nondisjunctive programs, but is simpler, and, unlike elementary loops, can be extended to disjunctive programs without producing unintuitive results. We show that the maximal unfounded elementary sets for the ``relevant'' part of a program are exactly the minimal sets among the nonempty unfounded sets. We also present a graph-theoretic characterization of elementary sets for nondisjunctive pro
    
[^25]: 通用稳定模型理论中的安全公式

    Safe Formulas in the General Theory of Stable Models. (arXiv:2307.09166v1 [cs.AI])

    [http://arxiv.org/abs/2307.09166](http://arxiv.org/abs/2307.09166)

    本论文提出了一种通用稳定模型理论中的安全公式，证明了安全句子和其继承结果具有相同的稳定模型，并且可以通过一个简单句法形式的公式来描述。

    

    安全的一阶公式推广了安全规则的概念，在答案集求解器的设计中起着重要作用。我们证明了在某种意义上，任何安全的语句都等价于其继承的结果-通过将所有量词替换为多个合取和析取而获得的无变量句子。由此可见，安全句子和其继承结果具有相同的稳定模型，并且一个安全句子的稳定模型可以由一个简单句法形式的公式来描述。

    Safe first-order formulas generalize the concept of a safe rule, which plays an important role in the design of answer set solvers. We show that any safe sentence is equivalent, in a certain sense, to the result of its grounding -to the variable-free sentence obtained from it by replacing all quantifiers with multiple conjunctions and disjunctions. It follows that a safe sentence and the result of its grounding have the same stable models, and that the stable models of a safe sentence can be characterized by a formula of a simple syntactic form.
    
[^26]: 通过机器学习，安全性，可持续性和实验网络集成增强网络切片架构

    Enhancing Network Slicing Architectures with Machine Learning, Security, Sustainability and Experimental Networks Integration. (arXiv:2307.09151v1 [cs.NI])

    [http://arxiv.org/abs/2307.09151](http://arxiv.org/abs/2307.09151)

    本论文介绍了通过机器学习、安全性、可持续性和实验网络集成来增强网络切片架构，在满足不同领域需求的同时，推进6G高度需求的应用。

    

    网络切片是一种在5G网络计算策略，移动边缘计算，移动云计算以及物联网车辆互联网和工业物联网等领域广泛使用的关键技术。网络切片被视为6G未来和高度需要的应用的主要支持者之一，因为它可以优化和定制动态、需求高、具有高度不同应用需求的客户之间争夺的有限资源。各种标准化组织，如3GPP的新一代网络提案和最新的5G/6G研究项目，正在提出新的网络切片架构。然而，新的网络切片架构必须处理广泛的需求范围，这导致了通常只满足具有共性域集合需求的网络切片架构提议。切片未来互联网基础设施(SFI2)架构提议探讨了由多样性导致的差距。

    Network Slicing (NS) is an essential technique extensively used in 5G networks computing strategies, mobile edge computing, mobile cloud computing, and verticals like the Internet of Vehicles and industrial IoT, among others. NS is foreseen as one of the leading enablers for 6G futuristic and highly demanding applications since it allows the optimization and customization of scarce and disputed resources among dynamic, demanding clients with highly distinct application requirements. Various standardization organizations, like 3GPP's proposal for new generation networks and state-of-the-art 5G/6G research projects, are proposing new NS architectures. However, new NS architectures have to deal with an extensive range of requirements that inherently result in having NS architecture proposals typically fulfilling the needs of specific sets of domains with commonalities. The Slicing Future Internet Infrastructures (SFI2) architecture proposal explores the gap resulting from the diversity of
    
[^27]: 用于SAT的机器学习：限制启发式方法和新的图形表示

    Machine Learning for SAT: Restricted Heuristics and New Graph Representations. (arXiv:2307.09141v1 [cs.AI])

    [http://arxiv.org/abs/2307.09141](http://arxiv.org/abs/2307.09141)

    通过使用机器学习模型在SAT求解的初始阶段，然后将控制权交给传统启发式方法，可以减少步骤的数量和整体运行时间。同时，我们还介绍了一种针对转换来自其他领域的SAT问题的Graph-Q-SAT的修改版。

    

    布尔可满足性（SAT）是一个基本的NP完全问题，在许多应用中都有着重要的作用，包括自动化规划和调度。为了解决大规模的实例，SAT求解器必须依赖启发式方法，例如在DPLL和CDCL求解器中选择分支变量。这些启发式方法可以通过机器学习（ML）模型来改进，它们可以减少步骤的数量，但通常会影响运行时间，因为有用的模型相对较大且较慢。我们建议使用经过训练的ML模型进行少量初始步骤，然后将控制权交给传统启发式方法；这简化了SAT求解的冷启动过程，可以减少步骤的数量和整体运行时间，但需要另外决定何时将控制权交还给求解器。此外，我们还引入了一种针对从其他领域转换而来的SAT问题进行定制的Graph-Q-SAT修改版，例如开放车间调度问题。我们通过随机和工业SAT问题验证了我们方法的可行性。

    Boolean satisfiability (SAT) is a fundamental NP-complete problem with many applications, including automated planning and scheduling. To solve large instances, SAT solvers have to rely on heuristics, e.g., choosing a branching variable in DPLL and CDCL solvers. Such heuristics can be improved with machine learning (ML) models; they can reduce the number of steps but usually hinder the running time because useful models are relatively large and slow. We suggest the strategy of making a few initial steps with a trained ML model and then releasing control to classical heuristics; this simplifies cold start for SAT solving and can decrease both the number of steps and overall runtime, but requires a separate decision of when to release control to the solver. Moreover, we introduce a modification of Graph-Q-SAT tailored to SAT problems converted from other domains, e.g., open shop scheduling problems. We validate the feasibility of our approach with random and industrial SAT problems.
    
[^28]: DropMix: 减少混合样本数据增强中的类别相关性

    DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation. (arXiv:2307.09136v1 [cs.CV])

    [http://arxiv.org/abs/2307.09136](http://arxiv.org/abs/2307.09136)

    该论文提出了一种名为DropMix的方法，通过排除一定比例的数据来减少混合样本数据增强（MSDA）中的类别相关性。在两个数据集上的实验证明了该方法可以提高之前因为MSDA而下降的类别的性能，并增加整体的平均准确率。

    

    混合样本数据增强（MSDA）是一种广泛应用的技术，已被证明可以提高各种任务的性能。然而，在本文中，我们展示了MSDA的效果是与类别相关的，一些类别的性能得到了改进，而其他类别则出现了下降。为了减少类别相关性，我们提出了DropMix方法，在MSDA计算中排除了特定比例的数据。通过在MSDA和非MSDA数据的组合上进行训练，所提出的方法不仅提高了之前由MSDA降低的类别的性能，还提高了整体的平均准确率，在使用三种MSDA方法（Mixup、CutMix和PuzzleMix）对两个数据集（CIFAR-100和ImageNet）进行的实验证明了这一点。

    Mixed sample data augmentation (MSDA) is a widely used technique that has been found to improve performance in a variety of tasks. However, in this paper, we show that the effects of MSDA are class-dependent, with some classes seeing an improvement in performance while others experience a decline. To reduce class dependency, we propose the DropMix method, which excludes a specific percentage of data from the MSDA computation. By training on a combination of MSDA and non-MSDA data, the proposed method not only improves the performance of classes that were previously degraded by MSDA, but also increases overall average accuracy, as shown in experiments on two datasets (CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and PuzzleMix).
    
[^29]: 用于Hopsworks的基于Kubernetes的云原生RStudio

    Cloud-native RStudio on Kubernetes for Hopsworks. (arXiv:2307.09132v1 [cs.DC])

    [http://arxiv.org/abs/2307.09132](http://arxiv.org/abs/2307.09132)

    该论文介绍了使用基于Kubernetes的云原生技术在Hopsworks上实现的多租户RStudio系统，解决了多租户环境中的性能隔离、安全性和扩展性问题，同时实现了安全数据共享和用户协作。

    

    为了充分利用云计算，服务采用“多租户”架构模型设计，旨在最大程度地实现用户之间的资源共享。然而，多租户引入了安全、性能隔离、扩展性和定制化方面的挑战。RStudio服务器是一个开源的集成开发环境（IDE），通过Web浏览器访问，用于R编程语言。我们在数据密集型AI平台Hopsworks上设计和实施了一个多用户分布式系统，采用多租户模型，以提供RStudio作为软件即服务（SaaS）。我们使用最流行的云原生技术：Docker和Kubernetes，解决了多租户环境中存在的性能隔离、安全性和扩展性问题。我们进一步在RStudio服务器实例中实现了安全数据共享，以提供数据隐私并允许RStudio用户之间的协作。我们将我们的系统与Apache Spark集成起来。

    In order to fully benefit from cloud computing, services are designed following the "multi-tenant" architectural model, which is aimed at maximizing resource sharing among users. However, multi-tenancy introduces challenges of security, performance isolation, scaling, and customization. RStudio server is an open-source Integrated Development Environment (IDE) accessible over a web browser for the R programming language. We present the design and implementation of a multi-user distributed system on Hopsworks, a data-intensive AI platform, following the multi-tenant model that provides RStudio as Software as a Service (SaaS). We use the most popular cloud-native technologies: Docker and Kubernetes, to solve the problems of performance isolation, security, and scaling that are present in a multi-tenant environment. We further enable secure data sharing in RStudio server instances to provide data privacy and allow collaboration among RStudio users. We integrate our system with Apache Spark
    
[^30]: BOLD：一个面向链接数据用户代理的基准和一个用于动态链接数据环境的模拟框架

    BOLD: A Benchmark for Linked Data User Agents and a Simulation Framework for Dynamic Linked Data Environments. (arXiv:2307.09114v1 [eess.SY])

    [http://arxiv.org/abs/2307.09114](http://arxiv.org/abs/2307.09114)

    本文介绍了BOLD基准和模拟框架，用于测试链接数据代理的性能。通过模拟动态链接数据环境和执行特定任务，该框架提供了检查任务执行的手段并测量代理的性能。

    

    本文介绍了面向链接数据代理的BOLD（Buildings on Linked Data）基准，并提供了一个模拟动态链接数据环境的框架，在此基础上构建了BOLD。BOLD基准通过为智能建筑提供读写链接数据接口来实例化BOLD框架，模拟时间、人员移动以及围绕照明的传感器和执行器。代理在该环境的链接数据表示上执行多个指定任务，如控制照明。模拟环境提供了检查任务正确执行和测量代理性能的手段。我们基于条件-动作规则对链接数据代理进行了测量。

    The paper presents the BOLD (Buildings on Linked Data) benchmark for Linked Data agents, next to the framework to simulate dynamic Linked Data environments, using which we built BOLD. The BOLD benchmark instantiates the BOLD framework by providing a read-write Linked Data interface to a smart building with simulated time, occupancy movement and sensors and actuators around lighting. On the Linked Data representation of this environment, agents carry out several specified tasks, such as controlling illumination. The simulation environment provides means to check for the correct execution of the tasks and to measure the performance of agents. We conduct measurements on Linked Data agents based on condition-action rules.
    
[^31]: 多目标神经架构搜索综述

    A Survey on Multi-Objective Neural Architecture Search. (arXiv:2307.09099v1 [cs.CV])

    [http://arxiv.org/abs/2307.09099](http://arxiv.org/abs/2307.09099)

    这篇论文综述了多目标神经架构搜索（MONAS）的主要研究工作，介绍了领域内的分类和表述问题，并提供了一个目标列表和一些新的目标。

    

    最近，通过神经架构搜索（NAS）和自动化生成（和调整）网络结构，领域内使用专家设计的神经架构的趋势正在逐渐被取代。这与超参数优化和自动机器学习（AutoML）密切相关。在过去，NAS仅优化预测准确性，而多目标神经架构搜索（MONAS）已经引起了人们的关注，它考虑了更多的目标，如计算复杂性、功耗和网络大小，实现了准确性与计算成本等其他特征之间的平衡。本文概述了MONAS领域的主要和最新研究工作。从对NAS的分类和表述开始，我们纠正了之前一些关于NAS领域的分类错误，并提供了已知目标的列表，并补充了一些新目标，并进行了详细阐述。

    Recently, the expert-crafted neural architectures is increasing overtaken by the utilization of neural architecture search (NAS) and automatic generation (and tuning) of network structures which has a close relation to the Hyperparameter Optimization and Auto Machine Learning (AutoML). After the earlier NAS attempts to optimize only the prediction accuracy, Multi-Objective Neural architecture Search (MONAS) has been attracting attentions which considers more goals such as computational complexity, power consumption, and size of the network for optimization, reaching a trade-off between the accuracy and other features like the computational cost. In this paper, we present an overview of principal and state-of-the-art works in the field of MONAS. Starting from a well-categorized taxonomy and formulation for the NAS, we address and correct some miscategorizations in previous surveys of the NAS field. We also provide a list of all known objectives used and add a number of new ones and elab
    
[^32]: DiTTO：受扩散启发的时空转换算子

    DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])

    [http://arxiv.org/abs/2307.09072](http://arxiv.org/abs/2307.09072)

    DiTTO是一种扩散启发的算子学习方法，通过结合Transformer架构的元素，无需时间离散化连续解决时间相关PDEs，并在多维度的各种PDE中取得了最先进的准确性结果。

    

    使用数据驱动方法解决偏微分方程（PDEs）已经越来越常见。最近的算子学习范式的发展使得解决更广泛PDE相关问题成为可能。我们提出了一种算子学习方法，可以连续地解决时间相关的PDEs，而不需要任何时间离散化。所提出的方法名为DiTTO，受潜在扩散模型的启发。尽管扩散模型通常用于生成人工智能任务，但其时间条件机制对PDEs非常有用。扩散启发的框架与Transformer架构的元素相结合，以提高其能力。我们展示了新方法在多维度的广泛PDE上的有效性，包括1维Burgers方程，2维Navier-Stokes方程和2维和3维声波方程。DiTTO在这些问题的准确性方面取得了最先进的结果。

    Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.  We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these p
    
[^33]: 文字想象的释放：通过探索文字的力量实现文本到图像的人物检索的新框架

    Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])

    [http://arxiv.org/abs/2307.09059](http://arxiv.org/abs/2307.09059)

    本研究提出了一个新的框架，通过探索文本中的文字的力量，实现了准确地将抽象的文本描述映射到具体的图像，从而实现了文本到图像的人物检索。

    

    文本到图像的人物检索的目标是从大型图库中检索与给定文本描述相匹配的人物图像。这个任务的主要挑战在于视觉和文本模态之间信息表示的显著差异。文本模态通过词汇和语法结构传递抽象和精确的信息，而视觉模态通过图像传递具体和直观的信息。为了充分利用文字表示的表达力，准确地将抽象的文本描述映射到具体图像是至关重要的。为了解决这个问题，我们提出了一个新的框架，通过探索句子中的文字的力量，释放了文本到图像人物检索中的文字想象力。具体来说，该框架使用预训练的全面CLIP模型作为图像和文本的双编码器，利用先前的跨模态对齐知识。

    The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
    
[^34]: QMNet: 基于重要性的消息交换的去中心化多智能体强化学习

    QMNet: Importance-Aware Message Exchange for Decentralized Multi-Agent Reinforcement Learning. (arXiv:2307.09051v1 [cs.AI])

    [http://arxiv.org/abs/2307.09051](http://arxiv.org/abs/2307.09051)

    QMNet是一种基于重要性的消息交换的去中心化多智能体强化学习方法，通过设计重要性感知调度策略和利用消息预测机制来提高性能。

    

    为了在无线资源限制下提高多智能体强化学习的性能，我们提出了一种消息重要性度量，并设计了一种重要性感知调度策略来有效交换消息。关键洞察是将宝贵的通信资源用于重要的消息上。消息的重要性不仅取决于消息本身，还取决于接收消息的智能体的需求。因此，我们提出了一种基于查询消息的架构，称为QMNet。智能体通过环境观测生成查询和消息。共享查询可以帮助计算消息的重要性。交换消息可以帮助智能体更好地协作。除此之外，我们利用消息重要性来处理去中心化系统中的随机接入冲突。此外，我们提出了一种消息预测机制来补偿未传输的消息。最后，我们在一个交通路口环境中评估了所提出的方案。

    To improve the performance of multi-agent reinforcement learning under the constraint of wireless resources, we propose a message importance metric and design an importance-aware scheduling policy to effectively exchange messages. The key insight is spending the precious communication resources on important messages. The message importance depends not only on the messages themselves, but also on the needs of agents who receive them. Accordingly, we propose a query-message-based architecture, called QMNet. Agents generate queries and messages with the environment observation. Sharing queries can help calculate message importance. Exchanging messages can help agents cooperate better. Besides, we exploit the message importance to deal with random access collisions in decentralized systems. Furthermore, a message prediction mechanism is proposed to compensate for messages that are not transmitted. Finally, we evaluate the proposed schemes in a traffic junction environment, where only a fra
    
[^35]: R-Cut: 使用加权输出和剪切增强Transformer视觉模型的可解释性

    R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut. (arXiv:2307.09050v1 [cs.CV])

    [http://arxiv.org/abs/2307.09050](http://arxiv.org/abs/2307.09050)

    本文提出了一种增强Transformer视觉分类模型可解释性的方法，通过加权输出和剪切两个模块，生成密集的类别特定可解释性地图。

    

    基于Transformer的模型在自然语言处理领域（NLP）中广受欢迎，被广泛应用于计算机视觉任务和GPT4等多模态模型中。本文提出了一种增强Transformer视觉分类模型可解释性的新方法。我们的方法旨在提高分类结果的可信度，并通过提供类别特定的可视化地图，使用户深入了解模型以进行后续任务。我们引入了两个模块：``加权输出关系"和``剪切"模块。``加权输出关系"模块专注于从中间层提取类别特定信息，使我们能够突出相关特征。此外，``剪切"模块对特征进行精细的分解，考虑位置、纹理和颜色等因素。通过整合这些模块，我们生成密集的类别特定可解释性地图。我们使用实验证实了我们的方法。

    Transformer-based models have gained popularity in the field of natural language processing (NLP) and are extensively utilized in computer vision tasks and multi-modal models such as GPT4. This paper presents a novel method to enhance the explainability of Transformer-based image classification models. Our method aims to improve trust in classification results and empower users to gain a deeper understanding of the model for downstream tasks by providing visualizations of class-specific maps. We introduce two modules: the ``Relationship Weighted Out" and the ``Cut" modules. The ``Relationship Weighted Out" module focuses on extracting class-specific information from intermediate layers, enabling us to highlight relevant features. Additionally, the ``Cut" module performs fine-grained feature decomposition, taking into account factors such as position, texture, and color. By integrating these modules, we generate dense class-specific visual explainability maps. We validate our method wit
    
[^36]: FedDefender：面向客户端抗攻击的联邦学习

    FedDefender: Client-Side Attack-Tolerant Federated Learning. (arXiv:2307.09048v1 [cs.CR])

    [http://arxiv.org/abs/2307.09048](http://arxiv.org/abs/2307.09048)

    FedDefender是一种面向客户端的联邦学习防御机制，通过抗攻击的本地元更新和全局知识蒸馏，帮助良性客户端训练稳健的本地模型，避免恶意模型更新的不利影响。

    

    联邦学习通过不损害隐私的方式，实现了从分散的数据源进行学习，这使得它成为一种关键的技术。然而，它容易受到模型污染攻击的影响，即恶意客户端干扰训练过程。以往的防御机制主要集中在服务器端，通过精心的模型聚合来防御，但是当数据不是相同分布的，或者攻击者可以访问良性客户端的信息时，这种防御可能无效。本文提出了一种新的防御机制，名为FedDefender，它专注于客户端，在服务器端无法识别或移除对手时，帮助良性客户端训练稳健的本地模型，避免恶意模型更新的不利影响。我们的方法包括两个主要组成部分：（1）抗攻击的本地元更新和（2）抗攻击的全局知识蒸馏。这些组成部分用来找到抗噪声的模型参数，同时准确地传递全局信息。

    Federated learning enables learning from decentralized data sources without compromising privacy, which makes it a crucial technique. However, it is vulnerable to model poisoning attacks, where malicious clients interfere with the training process. Previous defense mechanisms have focused on the server-side by using careful model aggregation, but this may not be effective when the data is not identically distributed or when attackers can access the information of benign clients. In this paper, we propose a new defense mechanism that focuses on the client-side, called FedDefender, to help benign clients train robust local models and avoid the adverse impact of malicious model updates from attackers, even when a server-side defense cannot identify or remove adversaries. Our method consists of two main components: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation. These components are used to find noise-resilient model parameters while accurately 
    
[^37]: 用于从科学文献中提取定理和证明的多模态机器学习

    Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature. (arXiv:2307.09047v1 [cs.AI])

    [http://arxiv.org/abs/2307.09047](http://arxiv.org/abs/2307.09047)

    这篇论文提出了一种使用多模态机器学习的方法，通过对文本、字体特征和PDF的位图图像渲染进行融合分类，成功实现了从科学文献中提取定理和证明的目标。在文本模态方面，通过在11 GB的科学语料库上预训练一个新的语言模型，得到了与在160 GB预训练的模型相似的性能，同时具有更快的收敛速度和更少的微调数据要求。

    

    数学领域的学术文章中包含定理、命题等数学陈述及其证明。从文章的PDF表示中提取它们需要理解科学文本以及视觉和基于字体的指示符。我们将这个问题作为一种多模态分类问题，使用文本、字体特征和PDF的位图图像渲染作为不同的模态。在本文中，我们提出了一种基于多模态机器学习的方法，用于提取类定理环境和证明，该方法基于通过单一单模态分类器提取的特征的后期融合，考虑文档中块的顺序。对于文本模态，我们在一个11 GB的科学语料库上预训练了一个新的语言模型；实验证明，我们的任务与一个在160 GB上预训练的（RoBERTa）模型相比拥有类似的性能，在要求更少的微调数据的同时收敛更快。

    Scholarly articles in mathematical fields feature mathematical statements such as theorems, propositions, etc., as well as their proofs. Extracting them from the PDF representation of the articles requires understanding of scientific text along with visual and font-based indicators. We pose this problem as a multimodal classification problem using text, font features, and bitmap image rendering of the PDF as different modalities. In this paper we propose a multimodal machine learning approach for extraction of theorem-like environments and proofs, based on late fusion of features extracted by individual unimodal classifiers, taking into account the sequential succession of blocks in the document. For the text modality, we pretrain a new language model on a 11 GB scientific corpus; experiments shows similar performance for our task than a model (RoBERTa) pretrained on 160 GB, with faster convergence while requiring much less fine-tuning data. Font-based information relies on training a 
    
[^38]: 大型语言模型的情感智能

    Emotional Intelligence of Large Language Models. (arXiv:2307.09042v1 [cs.AI])

    [http://arxiv.org/abs/2307.09042](http://arxiv.org/abs/2307.09042)

    这项研究评估了大型语言模型（LLMs）的情感智能（EI），并提出了一种新的心理测量评估方法，该方法重点考察了情感理解（EU）能力。评估结果表明，大多数主流LLMs在情感理解方面表现良好。

    

    大型语言模型（LLMs）在许多领域展示出了非凡的能力，主要通过语言生成、知识利用和复杂推理等任务进行评估。然而，它们与人类情感和价值观的一致性尚未得到系统评估，而这对于实际应用至关重要。在这里，我们评估了LLMs的情感智能（EI），包括情感识别、解释和理解，这对于有效的沟通和社交互动至关重要。具体而言，我们首先开发了一种新的心理测量评估，重点是情感理解（EU），这是EI的核心组成部分，适用于人类和LLMs。这个测试需要在现实场景中评估复杂的情感（例如，惊讶、愉快、困惑、自豪），如尽管感觉表现不佳，约翰却意外地获得了最高分。通过从500多名成年人构建的参考框架，我们测试了各种主流LLMs。

    Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achie
    
[^39]: PromptMagician：用于文本到图像生成的交互式提示工程

    PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation. (arXiv:2307.09036v1 [cs.AI])

    [http://arxiv.org/abs/2307.09036](http://arxiv.org/abs/2307.09036)

    PromptMagician是一个交互式的提示工程系统，可以帮助用户针对文本到图像生成任务发展出高效的提示，并通过多级可视化和个性化探索支持用户在输入提示过程中的交互和迭代。

    

    生成式文本到图像模型因其基于自然语言提示生成高质量图像的强大能力而受到大众的广泛关注。然而，由于自然语言的复杂性和歧义性，为期望的图像开发有效的提示可能具有挑战性。本研究提出了PromptMagician，一个视觉分析系统，可帮助用户探索图像结果并细化输入的提示。我们系统的主干是一个提示推荐模型，它以用户提示作为输入，从DiffusionDB中检索类似的提示-图像对，并识别出特殊的（重要的和相关的）提示关键词。为了促进交互式提示细化，PromptMagician引入了一个多级可视化，用于检索的图像和推荐的关键词的跨模态嵌入，并支持用户指定多个个性化探索的标准。通过两个使用场景、用户研究和专家访谈，证明了PromptMagician的有效性。

    Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiv
    
[^40]: 使用KeyBERT和SNA探索针对工程学生的自动驾驶车辆政策接受度

    Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students. (arXiv:2307.09014v1 [cs.SI])

    [http://arxiv.org/abs/2307.09014](http://arxiv.org/abs/2307.09014)

    本研究利用KeyBERT和SNA方法，探索工程学生对自动驾驶车辆政策的接受度。通过文本挖掘分析研究生的意见，从而填补了终端用户对这些政策接受度的分析空白。

    

    本研究旨在利用改进的文本挖掘方法，探索用户对自动驾驶车辆政策的接受度。近年来，韩国政策制定者将自动驾驶汽车（ADC）和自动驾驶机器人（ADR）视为下一代交通工具，可以降低乘客和货物运输成本。他们支持为ADC建设V2I和V2V通信基础设施，并认识到ADR等同于行人，以促进其在人行道上的部署。为了填补终端用户对这些政策接受度没有充分考虑的空白，本研究将两种文本挖掘方法应用于工业、机械和电子-电气-计算机领域的研究生意见。一种是基于TF-IWF和Dice系数的共现网络分析（CNA），另一种是基于能够以上下文方式表示意见的关键词提取工具KeyBERT的上下文语义网络分析（C-SNA）。

    This study aims to explore user acceptance of Autonomous Vehicle (AV) policies with improved text-mining methods. Recently, South Korean policymakers have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as next-generation means of transportation that will reduce the cost of transporting passengers and goods. They support the construction of V2I and V2V communication infrastructures for ADC and recognize that ADR is equivalent to pedestrians to promote its deployment into sidewalks. To fill the gap where end-user acceptance of these policies is not well considered, this study applied two text-mining methods to the comments of graduate students in the fields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient, and the other is the Contextual Semantic Network Analysis (C-SNA) based on both KeyBERT, which extracts keywords that contextually represent the comments, and dou
    
[^41]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^42]: Ord2Seq: 将序回归视为标签序列预测

    Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v1 [cs.AI])

    [http://arxiv.org/abs/2307.09004](http://arxiv.org/abs/2307.09004)

    Ord2Seq是一种将序回归问题转化为标签序列预测的方法，通过递归的二元分类步骤微妙地区分相邻类别，在不同场景中达到优于现有方法的性能表现。

    

    序回归是将对象实例分为序列类别的分类问题。在许多情况下，如医疗疾病分级、电影评分等方面已得到广泛研究。现有方法仅关注学习类间序关系，但在区分相邻类别方面仍存在局限性。本文中，我们提出了一种简单的序列预测框架，称为Ord2Seq，它首次将每个序类别标签转化为特殊的标签序列，从而将序回归任务视为序列预测过程。通过这种方式，我们将序回归任务分解为一系列递归的二元分类步骤，以微妙地区分相邻类别。全面的实验证明了区分相邻类别对性能改进的有效性，并且我们的新方法在四种不同的场景中超过了现有最先进性能。代码将在完成后提供。

    Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading, movie rating, etc. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes will be available upon a
    
[^43]: EVIL: 用于可信半监督医学图像分割的证据推理学习

    EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation. (arXiv:2307.08988v1 [cs.CV])

    [http://arxiv.org/abs/2307.08988](http://arxiv.org/abs/2307.08988)

    EVIL是一种用于半监督医学图像分割的方法，通过引入证据推理学习，可以在单次前向传递中推断准确的不确定性量化，生成可信的伪标签，并取得竞争性能。

    

    最近，针对半监督医学图像分割，基于不确定性的方法引起了越来越多的关注。然而，目前的方法通常存在一个难以在统一框架中平衡计算成本、估计精度和理论支持的缺点。为了缓解这个问题，我们将证据理论引入半监督医学图像分割中，称之为证据推理学习（EVIL）。EVIL在单次前向传递中提供了一个在理论上保证的解决方案，用于推断准确的不确定性量化。在不确定性估计后，根据未标记数据生成可信的伪标签。我们的框架采用了最近提出的一致性正则化训练范式，通过强制对扰动预测进行一致性约束来增强少量标记数据的泛化能力。实验结果表明，EVIL在与几种方法的比较中取得了竞争性能。

    Recently, uncertainty-aware methods have attracted increasing attention in semi-supervised medical image segmentation. However, current methods usually suffer from the drawback that it is difficult to balance the computational cost, estimation accuracy, and theoretical support in a unified framework. To alleviate this problem, we introduce the Dempster-Shafer Theory of Evidence (DST) into semi-supervised medical image segmentation, dubbed Evidential Inference Learning (EVIL). EVIL provides a theoretically guaranteed solution to infer accurate uncertainty quantification in a single forward pass. Trustworthy pseudo labels on unlabeled data are generated after uncertainty estimation. The recently proposed consistency regularization-based training paradigm is adopted in our framework, which enforces the consistency on the perturbed predictions to enhance the generalization with few labeled data. Experimental results show that EVIL achieves competitive performance in comparison with several
    
[^44]: 基于AI辅助的5G NR下低延迟XR服务提供改进

    AI-assisted Improved Service Provisioning for Low-latency XR over 5G NR. (arXiv:2307.08987v1 [cs.NI])

    [http://arxiv.org/abs/2307.08987](http://arxiv.org/abs/2307.08987)

    该论文提出了一种基于AI辅助的5G NR下低延迟XR服务提供方案，通过利用预测帧进行处理而非仅仅依赖实际帧，实现了对XR服务的改进和多倍增加支持的效果，并提供了关键的网络设计见解。

    

    扩展现实（XR）是5G/6G媒体应用中最重要的之一，将从根本上改变人类的互动方式。然而，确保低延迟、高数据速率和可靠性以支持XR服务提出了重大挑战。该文提出了一种新颖的AI辅助服务提供方案，它利用预测帧进行处理，而不仅仅依赖实际帧。该方法虚拟增加了网络延迟预算，从而改善了服务提供，尽管会产生一些预测错误。通过大量的模拟验证了所提出的方案，在支持XR用户方面实现了多倍增加，并提供了重要的网络设计见解。

    Extended Reality (XR) is one of the most important 5G/6G media applications that will fundamentally transform human interactions. However, ensuring low latency, high data rate, and reliability to support XR services poses significant challenges. This letter presents a novel AI-assisted service provisioning scheme that leverages predicted frames for processing rather than relying solely on actual frames. This method virtually increases the network delay budget and consequently improves service provisioning, albeit at the expense of minor prediction errors. The proposed scheme is validated by extensive simulations demonstrating a multi-fold increase in supported XR users and also provides crucial network design insights.
    
[^45]: PromptCrafter：通过与LLM的混合倡议对话来生成文本到图像提示

    PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM. (arXiv:2307.08985v1 [cs.HC])

    [http://arxiv.org/abs/2307.08985](http://arxiv.org/abs/2307.08985)

    PromptCrafter是一种混合倡议系统，通过与LLM的对话逐步生成文本到图像提示。用户可以高效探索模型能力，并通过回答澄清问题来优化提示。

    

    文本到图像生成模型能够根据单个提示生成各种主题和风格的图像。最近的研究提出了各种交互方法，帮助用户了解模型的能力并利用它们。然而，如何支持用户高效地探索模型的能力并创建有效的提示仍然是一个开放性的研究问题。在本文中，我们提出了PromptCrafter，一个新颖的混合倡议系统，可以逐步创建文本到图像提示。通过迭代的过程，用户可以高效地探索模型的能力并明确自己的意图。PromptCrafter还通过回答由大型语言模型生成的澄清问题的各种回答来支持用户优化提示。最后，用户可以通过查看工作历史来恢复到所需的步骤。在本研讨会论文中，我们讨论了PromptCrafter的设计过程和后续研究计划。

    Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt. Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them. However, how to support users to efficiently explore the model's capability and to create effective prompts are still open-ended research questions. In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt. Through the iterative process, users can efficiently explore the model's capability, and clarify their intent. PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. Lastly, users can revert to a desired step by reviewing the work history. In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.
    
[^46]: ChatGPT的开发，用于负责任的报道和使用的生成人工智能和自然大型语言模型的指南（CANGARU）

    Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines. (arXiv:2307.08974v1 [cs.AI])

    [http://arxiv.org/abs/2307.08974](http://arxiv.org/abs/2307.08974)

    这个论文介绍了ChatGPT的开发和与之类似的生成人工智能和大型语言模型的广泛应用。为了解决关于它们如何道德应用、使用和披露的问题，提出了CANGARU指南。该指南旨在促进全球学术界对这些技术的道德使用、披露和适当报告的统一共识。

    

    这篇论文介绍了ChatGPT的开发以及与其类似的生成人工智能（GAI）、生成预训练变换器（GPT）和大型语言模型（LLM）的广泛使用，同时也引发了对它们在学术研究和科学成果中道德应用、使用和披露的问题。鉴于目前缺乏统一的方法，一些出版商和期刊最近制定了自己的规则，但不一致的方法可能导致“巴别塔效应”，可能产生混淆而不是期望的标准化。为了解决这个问题，我们提出了ChatGPT、生成人工智能和自然大型语言模型用于负责任的报道和使用的指南（CANGARU）计划，旨在促进跨学科的全球共识，关于在学术界对GAI/GPT/LLM技术的道德使用、披露和适当报告。本协议包括四个不同的部分: a) 对GAI/GPT/LLM应用的持续系统性审查，以了解其关联性。

    The swift progress and ubiquitous adoption of Generative AI (GAI), Generative Pre-trained Transformers (GPTs), and large language models (LLMs) like ChatGPT, have spurred queries about their ethical application, use, and disclosure in scholarly research and scientific productions. A few publishers and journals have recently created their own sets of rules; however, the absence of a unified approach may lead to a 'Babel Tower Effect,' potentially resulting in confusion rather than desired standardization. In response to this, we present the ChatGPT, Generative Artificial Intelligence, and Natural Large Language Models for Accountable Reporting and Use Guidelines (CANGARU) initiative, with the aim of fostering a cross-disciplinary global inclusive consensus on the ethical use, disclosure, and proper reporting of GAI/GPT/LLM technologies in academia. The present protocol consists of four distinct parts: a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked i
    
[^47]: 景观替代品：在部分信息下学习数学优化的决策损失

    Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])

    [http://arxiv.org/abs/2307.08964](http://arxiv.org/abs/2307.08964)

    本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。

    

    最近的学习集成优化工作在优化问题只有部分可观测或通用优化器在无专家调优的情况下表现不佳的情况下显示出了希望。通过学习一个优化器$ \mathbf{g} $来解决这些具有挑战性的问题，通过利用过去的经验，可以显著加速优化过程。优化器可以通过已知最优解的监督或通过优化复合函数$ f\circ \mathbf{g} $的隐式方式进行训练。隐式方法可能不需要最优解作为标签，并且能够处理问题的不确定性；然而，由于在训练和测试过程中频繁调用优化器$ \mathbf{g} $，因此训练和部署缓慢。对于组合求解器，由于$ \mathbf{g} $的稀疏梯度，训练进一步受到挑战。为了解决这些问题，我们提出使用平滑可学习的景观替代品$ M $作为一种替代方法。

    Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
    
[^48]: REX: 快速探索与利用的增强型AI代理方法

    REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])

    [http://arxiv.org/abs/2307.08962](http://arxiv.org/abs/2307.08962)

    本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。

    

    本文提出了一种增强型的快速探索与利用的AI代理方法，称为REX。现有的AutoGPT风格技术存在一些固有的限制，如对于决策的精确描述的过度依赖，以及缺乏类似传统强化学习(Reinforcement Learning，RL)中的尝试和失败程序的系统性方法。REX引入了额外的奖励层，并集成了类似于上限置信界限(UCB)分数的概念，从而实现更强大和高效的AI代理性能。这种方法的优势是可以利用来自日志的离线行为，并与现有的基础模型无缝集成，而不需要任何模型微调。通过与现有方法（如思维链(CoT)和规划推理(RAP)）的比较分析，基于REX的方法展示了相当的性能，并在某些情况下甚至超过了这些现有技术所取得的结果。

    In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
    
[^49]: 弱监督人体活动识别中的Siamese网络

    Siamese Networks for Weakly Supervised Human Activity Recognition. (arXiv:2307.08944v1 [cs.HC])

    [http://arxiv.org/abs/2307.08944](http://arxiv.org/abs/2307.08944)

    本文提出了一种使用Siamese网络进行弱监督人体活动识别的模型，通过仅利用数据样本对的相似性进行训练，该模型可以作为不同聚类算法的度量标准，在三个数据集上的评估结果证明了其有效性。

    

    深度学习已成功应用于人体活动识别。然而，训练深度神经网络需要明确标记的数据，而获取这些数据很困难。本文提出了一种采用多个Siamese网络的模型，仅利用数据样本对之间的相似性信息进行训练，而不知道明确的标签。训练后的模型将活动数据样本映射为固定大小的表示向量，使得表示空间中向量之间的距离近似于输入空间中数据样本的相似性。因此，训练后的模型可以作为广泛的不同聚类算法的度量标准。训练过程通过最小化相似性损失函数，使得同一种活动的样本对的距离度量小，并使得不同种活动的样本对的距离度量大。我们在三个数据集上对该模型进行评估，验证其在分割中的有效性。

    Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segm
    
[^50]: IxDRL:一种基于有趣分析的新型可解释深度强化学习工具包

    IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness. (arXiv:2307.08933v1 [cs.AI])

    [http://arxiv.org/abs/2307.08933](http://arxiv.org/abs/2307.08933)

    IxDRL是一种基于有趣分析的新型可解释深度强化学习工具包，具备能力感知机制，能够提供人类操作员对RL代理能力的整体视图。

    

    近年来，深度学习的进展在使用强化学习（RL）解决具有高维输入的复杂顺序决策任务方面取得了众多成功。然而，现有系统缺乏必要的机制来提供人类对其能力的整体视图，这在关键应用中成为采用RL的障碍，因为代理程序所做的决策可能具有重大后果。然而，现有的RL系统本质上不具备能力感知，因为它们缺乏必要的解释机制，使人类操作员能够对其能力有深入、整体的了解。为了实现更可解释的深度RL（xDRL），我们提出了一个基于有趣分析的新框架。我们的工具基于有趣分析提供多种RL代理的能力度量，适用于各种RL算法，并原生支持流行的RLLib工具包。我们展示了该工具包在...（原文截断）

    In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of o
    
[^51]: 基于循环一致性的无监督深度图匹配

    Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])

    [http://arxiv.org/abs/2307.08930](http://arxiv.org/abs/2307.08930)

    本文提出了一种基于循环一致性的无监督深度图匹配方法，不需要真实对应的关键点对，通过在同一对象类别的图像之间强制匹配一致性来进行自我监督学习，该方法具有很高的灵活性，并且在无监督图匹配方面达到了最新的最先进水平。

    

    我们在稀疏领域的无监督深度图匹配中做出了贡献，应用于图像中的关键点匹配。与标准的“监督”方法相反，我们的方法不需要关键点对之间的真实对应。相反，它通过强制同一对象类别的图像之间的匹配一致性来进行自我监督。由于匹配和一致性损失是离散的，它们的导数不能直接用于学习。我们通过在组合求解器的黑盒微分的最新结果基础上构建我们的方法来解决这个问题。这使得我们的方法非常灵活，因为它与任意网络架构和组合求解器兼容。我们的实验评估表明，我们的技术在无监督图匹配方面达到了新的最先进水平。

    We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
    
[^52]: 联邦式大规模语言模型：一个立场论文

    Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])

    [http://arxiv.org/abs/2307.08925](http://arxiv.org/abs/2307.08925)

    我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。

    

    大规模语言模型（LLM）在各个领域获得了相当大的关注并找到了多样化的应用，但在真实场景中开发时面临挑战。这些挑战源于公共领域数据可用性的匮乏以及对私有领域数据的隐私保护需求。为了解决这些问题，联邦学习（FL）作为一项有前景的技术出现了，它能够在保持分散数据的同时实现共同训练共享模型。我们提出了联邦式LLM的概念，包括三个关键组成部分，即联邦式LLM预训练、联邦式LLM微调和联邦式LLM提示工程。对于每个组件，我们讨论了它相对于传统LLM训练方法的优势，并提出了具体的工程策略来实施。此外，我们探讨了FL和LLM集成带来的新挑战。我们分析现有的解决方案并确定可能的障碍

    Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
    
[^53]: 连续时间强化学习: 具有理论洞察力和性能保证的新设计算法

    Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees. (arXiv:2307.08920v1 [eess.SY])

    [http://arxiv.org/abs/2307.08920](http://arxiv.org/abs/2307.08920)

    本论文介绍了一套新的连续时间强化学习算法，用于控制仿射非线性系统。这些算法解决了现有方法面临的复杂性、数值条件和维度扩展等设计挑战。

    

    连续时间非线性最优控制问题在实际应用中具有巨大潜力。虽经历了几十年的发展，强化学习（RL）作为一种通用的非线性控制设计方法已取得了巨大的成功。然而，最近对现有的连续时间强化学习（CT-RL）方法进行的全面分析揭示了它们面临重大的设计挑战，包括复杂性、数值条件和维度扩展问题。尽管有先进的理论结果，但现有的ADP CT-RL综合方法在解决即使是小的学术问题时都不足够。因此，本文的目标是介绍一套用于控制仿射非线性系统的新的CT-RL算法。我们的设计方法依赖于两个重要因素。首先，我们的方法适用于可以划分为较小子问题的物理系统。这种构造性的考虑使得问题的维度减少。

    Continuous-time nonlinear optimal control problems hold great promise in real-world applications. After decades of development, reinforcement learning (RL) has achieved some of the greatest successes as a general nonlinear control design method. However, a recent comprehensive analysis of state-of-the-art continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming (ADP)-based CT-RL algorithms, reveals they face significant design challenges due to their complexity, numerical conditioning, and dimensional scaling issues. Despite advanced theoretical results, existing ADP CT-RL synthesis methods are inadequate in solving even small, academic problems. The goal of this work is thus to introduce a suite of new CT-RL algorithms for control of affine nonlinear systems. Our design approach relies on two important factors. First, our methods are applicable to physical systems that can be partitioned into smaller subproblems. This constructive consideration results in reduced dimen
    
[^54]: 基于多智能体强化学习方法的一型糖尿病（T1D）患者基础-推荐剂量顾问

    Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology. (arXiv:2307.08897v1 [cs.LG])

    [http://arxiv.org/abs/2307.08897](http://arxiv.org/abs/2307.08897)

    本文提出了一种基于多智能体强化学习方法的一型糖尿病患者个性化血糖控制系统，通过显著改善血糖控制、减少血糖变异性以及预防低血糖事件等，该方法具有潜力成为一种有效的治疗方法。

    

    本文提出了一种针对一型糖尿病（T1D）患者个性化血糖控制的新型多智能体强化学习（RL）方法。该方法采用闭环系统，包括血糖代谢模型和作为基础-推荐剂量顾问的多智能体软性演员-评论家RL模型。通过比较RL代理与传统治疗在三个场景下的性能评估，评估指标包括血糖水平（最低、最高和平均），在不同血糖范围内的时间，以及平均每日推荐剂量和基础胰岛素剂量。结果表明，基于RL的基础-推荐剂量顾问显著改善了血糖控制，减小了血糖变异性，增加了在目标范围（70-180 mg/dL）内的时间。低血糖事件得到有效预防，严重高血糖事件得到减少。RL方法还导致与传统方法相比，平均每日基础胰岛素剂量有统计学显著减少。

    This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventio
    
[^55]: AI用于创意的生成和测试：向AI支持的知识发展环境迈进

    AI for the Generation and Testing of Ideas Towards an AI Supported Knowledge Development Environment. (arXiv:2307.08876v1 [cs.AI])

    [http://arxiv.org/abs/2307.08876](http://arxiv.org/abs/2307.08876)

    这项研究通过利用机器学习和大语言模型，提出了一种生成式AI方法，用于生成上下文相关的解决方案，但当前还无法支持思想的可追溯性。同时，研究也指出生成式AI在真实性、参考和准确地地图等方面还存在困难。

    

    新系统利用机器学习对大型知识源进行筛选，创建灵活的大语言模型。这些模型可以分辨不同通信形式中的上下文，并预测顺序信息。生成式AI利用Transformer生成文本或视觉输出，模仿人类回应。它提供了一个或多个上下文相关的可行解决方案供用户思考。然而，生成式AI目前不支持思想的可追溯性，而搜索引擎可以提供信息来源的有用功能。生成式AI的叙述风格受到了积极的评价。人们通过故事学习。然而，早期ChatGPT在真实性、参考、计算和准确地地图等方面遇到了困难。当前的能力参考位置和链接到应用似乎更适合我们使用了二十年的以链接为中心的搜索方法。部署真正可信的解决方案超越了模拟上下文相关性。

    New systems employ Machine Learning to sift through large knowledge sources, creating flexible Large Language Models. These models discern context and predict sequential information in various communication forms. Generative AI, leveraging Transformers, generates textual or visual outputs mimicking human responses. It proposes one or multiple contextually feasible solutions for a user to contemplate. However, generative AI does not currently support traceability of ideas, a useful feature provided by search engines indicating origin of information. The narrative style of generative AI has gained positive reception. People learn from stories. Yet, early ChatGPT efforts had difficulty with truth, reference, calculations, and aspects like accurate maps. Current capabilities of referencing locations and linking to apps seem to be better catered by the link-centric search methods we've used for two decades. Deploying truly believable solutions extends beyond simulating contextual relevance 
    
[^56]: 一种风险厌恶策略梯度的方差替代：基尼离差

    An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])

    [http://arxiv.org/abs/2307.08873](http://arxiv.org/abs/2307.08873)

    本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。

    

    在风险厌恶的强化学习中，限制策略回报的方差是一种常见选择，因为它具有明确的数学定义和易于解释。传统方法直接限制总回报方差，而最近的方法通过限制每步奖励方差作为代理。本文彻底研究了这些基于方差的方法的局限性，如数字尺度的敏感性和阻碍策略学习，并提出使用替代风险衡量标准——基尼离差。我们研究了这种新风险衡量标准的各种属性，并导出了一种用于最小化基尼离差的策略梯度算法。在风险厌恶可以明确定义的领域进行实证评估时，我们的算法可以缓解基于方差的风险衡量标准的局限性，并在其他策略无法学到合理策略时实现高回报和低风险，以方差和基尼离差度量。

    Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
    
[^57]: 图神经网络的课程学习：一种基于多视角和能力的方法

    Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])

    [http://arxiv.org/abs/2307.08859](http://arxiv.org/abs/2307.08859)

    本文提出了一种新的图神经网络课程学习方法，通过引入图复杂性形式化和模型能力作为困难度标准，以及考虑样本困难度和模型能力的不同视角进行训练，实现了对细粒度图困难度标准的纳入。

    

    课程学习是一种计划好的学习材料序列，有效的课程学习可以使人类和机器的学习更高效和有效。最近的研究在语言应用中为训练图神经网络开发了有效的数据驱动的课程学习方法。然而，现有的课程学习方法在训练范式中通常只使用单一的困难度标准。在本文中，我们提出了一种新的课程学习视角，通过引入一种建立在图复杂性形式化（作为困难度标准）和模型能力之上的新方法来进行课程学习。该模型包括一个调度方案，通过考虑样本困难度和模型能力的不同视角在训练期间推导出有效的课程。所提出的解决方案在图神经网络的课程学习研究中取得了进展，能够在训练范式中纳入细粒度的图困难度标准。

    A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. E
    
[^58]: 图生成的自回归扩散模型

    Autoregressive Diffusion Model for Graph Generation. (arXiv:2307.08849v1 [cs.AI])

    [http://arxiv.org/abs/2307.08849](http://arxiv.org/abs/2307.08849)

    提出了一种自回归扩散模型用于图生成，通过定义节点吸收扩散过程和设计扩散排序网络以及去噪网络，能在离散图空间中高效地生成多样性的高质量图形。

    

    最近，基于扩散的图生成模型在图生成方面取得了有希望的结果。然而，现有的基于扩散的图生成模型大多是一次性的生成模型，它们在去量化的邻接矩阵空间中应用高斯扩散。这种策略可能在模型训练困难、采样速度慢和无法集成约束方面存在问题。我们提出了一种用于图生成的自回归扩散模型。与现有方法不同的是，我们定义了一种在离散图空间中直接进行的节点吸收扩散过程。对于前向扩散，我们设计了一个称为“扩散排序网络”的网络，它从图的拓扑中学习到了数据相关的节点吸收顺序。对于逆向生成，我们设计了一个称为“去噪网络”的网络，它利用逆向节点排序以及之前去噪的节点来高效地重构图，通过预测新节点的类型和边与之前去噪节点的时间。基于置换的图表示，我们提出了一种有效的训练策略，并在多个数据集上进行了实验验证，结果表明我们的模型可以生成具有多样性的高质量图形。

    Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutatio
    
[^59]: 加速Benders分解方法的强化学习代理模型研究

    Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])

    [http://arxiv.org/abs/2307.08816](http://arxiv.org/abs/2307.08816)

    本文介绍了一种利用强化学习代理模型加速Benders分解方法的方法，并通过实验证明了其相对于其他加速方案的30%更快的平均收敛速度。

    

    随机优化试图在存在不确定性的情况下提供最优决策。通常，由于需要捕捉不确定性的情景数量以及现实规划问题的离散性质，这些问题的经典形式变得难以处理。为了克服这些可行性问题，实践者们转向分解方法，将问题分解为更小、更易处理的子问题。本文的主要分解方法是Benders分解（BD），它根据情景独立性对随机优化问题进行分解。在本文中，我们提出了一种利用代理模型加速BD的方法，该代理模型取代了NP难的整数主问题。通过加速方法，与其他加速的BD实现相比，我们观察到平均收敛速度提高了30%。我们引入了一个强化学习代理作为替代，并展示了如何使用它来解决随机库存问题。

    Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic invent
    
[^60]: AI增强模拟指导的操作员指导方法

    Operator Guidance Informed by AI-Augmented Simulations. (arXiv:2307.08810v1 [cs.AI])

    [http://arxiv.org/abs/2307.08810](http://arxiv.org/abs/2307.08810)

    本文介绍了一种基于AI增强模拟指导的操作员指导方法，利用LSTM神经网络估计双峰双向海况下船舶响应统计量。通过对比低保真度和高保真度结果，证明了该方法的有效性。

    

    本文将介绍一种基于多保真度和数据自适应的方法，利用长短期记忆神经网络(LSTM)来估计双峰双向海况下船舶响应统计量。研究将采用快速低保真度的基于体积的工具SimpleCode和更高保真度的工具Large Amplitude Motion Program (LAMP)。训练数据是通过常见的双峰双向海况在北大西洋生成的SimpleCode和LAMP数据。在用LAMP船舶运动响应数据训练LSTM网络后，样本路线被穿过，随机选取历史天气输入SimpleCode和LSTM网络，并与更高保真度的结果进行比较。

    This paper will present a multi-fidelity, data-adaptive approach with a Long Short-Term Memory (LSTM) neural network to estimate ship response statistics in bimodal, bidirectional seas. The study will employ a fast low-fidelity, volume-based tool SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by common bi-modal, bi-directional sea conditions in the North Atlantic as training data. After training an LSTM network with LAMP ship motion response data, a sample route was traversed and randomly sampled historical weather was input into SimpleCode and the LSTM network, and compared against the higher fidelity results.
    
[^61]: 本地或全局：基于有限标签的联邦学习中的选择性知识同化

    Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])

    [http://arxiv.org/abs/2307.08809](http://arxiv.org/abs/2307.08809)

    本论文提出了FedLabel方法，在联邦学习中，客户端根据数据的专业性选择本地或全局模型对无标签数据进行伪标记，并通过全局-本地一致性正则化来利用本地和全局模型的知识。

    

    许多现有的联邦学习方法假设客户端具有完全标记的数据，而在实际情况下，由于标记过程的昂贵和费力，客户端只有有限的标签。客户端有限的标记本地数据常常导致它们的本地模型对其更大的无标签本地数据具有较差的泛化能力，例如与无标签数据存在类分布不匹配的情况。因此，客户端可能会选择从跨客户端训练的全局模型中受益，以利用他们的无标签数据，但由于客户端之间存在数据异质性，这也变得困难。在我们的工作中，我们提出了FedLabel，客户端根据哪个模型对数据更具专业知识选择本地或全局模型来伪标记其无标签数据。我们进一步通过全局-本地一致性正则化来利用本地和全局模型的知识，当它们对无标签数据具有相同的伪标签时，最小化两个模型的输出之间的差异。

    Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unl
    
[^62]: 多时间尺度多智能体强化学习中的非平稳策略学习

    Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])

    [http://arxiv.org/abs/2307.08794](http://arxiv.org/abs/2307.08794)

    这篇论文提出了一个简单的框架，用于在多时间尺度多智能体强化学习中学习非平稳策略。他们利用智能体时间尺度的信息定义了周期性时间编码，并通过周期性多智能体策略学习多时间尺度引入的非平稳性的效果。他们还提出了一个使用神经网络的策略梯度算法来学习这些策略。

    

    在多时间尺度多智能体强化学习中，智能体在不同的时间尺度上进行交互。一般来说，对于受多时间尺度引起的时间依赖行为，策略是非平稳的。学习非平稳策略是具有挑战性的，往往需要复杂或低效的算法。由于现实世界复杂系统中控制问题的普遍存在，我们引入了一个简单的框架来学习多时间尺度多智能体强化学习中的非平稳策略。我们的方法利用关于智能体时间尺度的可用信息来定义周期性时间编码。具体而言，我们从理论上证明了通过周期性多智能体策略可以学习多时间尺度引入的非平稳性的效果。为了学习这样的策略，我们提出了一个策略梯度算法，该算法使用相位函数化的神经网络来参数化演员和评论家，为周期性提供归纳偏置。

    In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effective
    
[^63]: GEAR: 与通用化和高效解决方案增强语言模型

    GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution. (arXiv:2307.08775v1 [cs.AI])

    [http://arxiv.org/abs/2307.08775](http://arxiv.org/abs/2307.08775)

    GEAR是一种通用且高效的工具解决方案，通过将工具对应和执行分别委托给小型语言模型和大型语言模型，在不依赖任务示范的情况下实现了更高的性能和精确度。

    

    通过增加外部工具来增强大型语言模型（LLM）可以提高其在各种任务中的性能。然而，先前的研究过于依赖特定任务的工具使用示范，限制了其通用性，并且由于对大规模LLM进行多次调用而增加了计算成本。我们引入了GEAR，一种计算效率高的查询-工具对应算法，可以适用于不依赖特定任务示范的各种需要使用工具的任务。GEAR通过将工具对应和执行分别委托给小型语言模型（SLM）和LLM来实现更高的效率；同时利用语义和基于模式的评估在问题和答案级别上进行通用化的工具对应。我们在6个下游任务的14个数据集上评估了GEAR，证明了它对于新任务、新工具和不同SLM的强大通用性。尽管提供更高的效率，但GEAR在工具对应中的精确性比使用LLM的先前策略更高。

    Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM
    
[^64]: AI辅助决策在保护中的应用研讨会的反思

    Reflections from the Workshop on AI-Assisted Decision Making for Conservation. (arXiv:2307.08774v1 [cs.AI])

    [http://arxiv.org/abs/2307.08774](http://arxiv.org/abs/2307.08774)

    这个研讨会总结了AI辅助决策在保护中的应用，并提出了资源分配、规划和生物多样性保护干预中的关键研究问题，呼吁合作努力解决真实世界的保护挑战。

    

    在这篇白皮书中，我们总结了在2022年10月20-21日哈佛大学计算与社会研究中心举办的AI辅助决策在保护中的应用研讨会的演讲和讨论中的关键观点。我们识别出资源分配、规划和生物多样性保护干预中的关键开放性研究问题，突出了不仅需要AI解决方案，还需要新方法学进展的保护挑战。除了提供研讨会演讲和讨论的摘要外，我们希望这份文件能够呼吁生态学家、保护决策者和AI研究人员的合作努力，使算法决策方法的扩展能够优先解决真实世界的保护挑战。

    In this white paper, we synthesize key points made during presentations and discussions from the AI-Assisted Decision Making for Conservation workshop, hosted by the Center for Research on Computation and Society at Harvard University on October 20-21, 2022. We identify key open research questions in resource allocation, planning, and interventions for biodiversity conservation, highlighting conservation challenges that not only require AI solutions, but also require novel methodological advances. In addition to providing a summary of the workshop talks and discussions, we hope this document serves as a call-to-action to orient the expansion of algorithmic decision-making approaches to prioritize real-world conservation challenges, through collaborative efforts of ecologists, conservation decision-makers, and AI researchers.
    
[^65]: 改进语言模型在数学问题上的性能的混合策略

    A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])

    [http://arxiv.org/abs/2307.08767](http://arxiv.org/abs/2307.08767)

    本文提出了一种混合策略的探索方法，利用强化学习来改进语言模型在数学问题上的性能，通过在抽象层和第二层采用不同的探索方式，取得了超过2%的性能增益。

    

    在解决数学问题时，大多数语言模型采用采样策略根据条件概率预测下一个词。在数学推理过程中，可能会生成错误的答案。考虑到数学问题是确定性的，我们提出了一种混合策略的探索方法来解决数学问题，利用强化学习。我们提出了一个两级标记探索策略：抽象层以概率采样来决定下一个标记是运算符还是操作数，而第二层则以贪婪方式选择得分最高的下一个标记。我们使用GPT-2模型在GSM8K数据集上测试了我们的方法，并展示了超过2％的性能增益。我们的实现代码可在https://github.com/vividitytech/math_lm_rl找到。

    When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
    
[^66]: 使用可穿戴设备监测心血管生物标志物的光电容抗信号质量评估

    Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices. (arXiv:2307.08766v1 [cs.LG])

    [http://arxiv.org/abs/2307.08766](http://arxiv.org/abs/2307.08766)

    该研究基于机器学习模型对光电容抗（PPG）信号进行训练，评估了使用可穿戴设备进行连续监测时的准确性和可靠性，并提出了解决干扰因素的方法。

    

    光电容抗（PPG）是一种非侵入性技术，用于测量微血管组织中的血容量变化。它常被用于医疗设备，如脉搏血氧仪和手腕式心率监测器，用于监测心血管血液动力学。PPG可以评估心率、脉搏波形和外周灌注等参数，以指示血管收缩或扩张等情况，并提供有关微血管血流的信息，是监测心血管健康的宝贵工具。然而，PPG受到多种变化源的影响，尤其在使用可穿戴设备进行连续监测时，如运动伪影、皮肤色素和血管运动。在这项研究中，我们从PPG信号中提取了27个统计特征，用基于梯度提升（XGBoost和CatBoost）和随机森林（RF）算法的机器学习模型进行训练，以评估其准确性和可靠性。

    Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to asse
    
[^67]: 上周总统去了哪里？从新闻文章中检测名人行程

    Where Did the President Visit Last Week? Detecting Celebrity Trips from News Articles. (arXiv:2307.08721v1 [cs.AI])

    [http://arxiv.org/abs/2307.08721](http://arxiv.org/abs/2307.08721)

    该论文提出了一种从新闻文章中检测名人行程的方法，克服了文章间的异质性和噪声干扰，为进行大规模和网络分析提供了便利。

    

    名人的行踪是非常重要的。例如，政治家去哪里，他们多久访问一次，以及他们会见谁，都带有深远的地缘政治和经济影响。虽然新闻文章包含了名人的旅行信息，但由于缺乏自动行程检测工具，无法进行大规模和网络分析。为了设计这样的工具，我们必须克服新闻文章之间的异质性带来的困难：1)一个单独的文章可能噪音很大，涉及无关的人物和地点，特别是当文章很长时。2)虽然考虑多篇文章一起来确定一个特定的行程可能会有帮助，但关键的语义仍然分散在不同的文章中，与各种噪声交织在一起，使其难以有效地汇总。3)超过20%的文章间接提及了名人的行程，而不是直接使用准确的名人姓名或地点名称，导致了大部分行程信息的缺失。

    Celebrities' whereabouts are of pervasive importance. For instance, where politicians go, how often they visit, and who they meet, come with profound geopolitical and economic implications. Although news articles contain travel information of celebrities, it is not possible to perform large-scale and network-wise analysis due to the lack of automatic itinerary detection tools. To design such tools, we have to overcome difficulties from the heterogeneity among news articles: 1)One single article can be noisy, with irrelevant people and locations, especially when the articles are long. 2)Though it may be helpful if we consider multiple articles together to determine a particular trip, the key semantics are still scattered across different articles intertwined with various noises, making it hard to aggregate them effectively. 3)Over 20% of the articles refer to the celebrities' trips indirectly, instead of using the exact celebrity names or location names, leading to large portions of tri
    
[^68]: TableGPT：将表格，自然语言和命令统一到一个GPT中

    TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08674](http://arxiv.org/abs/2307.08674)

    TableGPT是一个统一的框架，利用大型语言模型（LLMs）和外部功能命令使LLMs能够无缝地与表格进行交互，实现广泛的功能，并提供便利和可访问性给用户。其中的创新是全局表格表示的概念，使LLMs能够全面理解表格的结构和内容。

    

    表格在现实世界的数据库中非常普遍，需要人们花费大量时间和精力进行分析和操作。大型语言模型（LLMs）的进步使得使用自然语言输入与表格交互成为可能，使得这种能力更加接近现实。本文介绍了TableGPT，这是一个统一的精调框架，使得LLMs能够利用外部功能命令理解和操作表格。它引入了与表格无缝交互的能力，实现了广泛的功能，如问答、数据操作（例如插入、删除、查询和修改操作）、数据可视化、分析报告生成和自动预测。TableGPT旨在通过使用户能够轻松利用表格数据来提供便利和可访问性。TableGPT的核心是全局表格表示的新概念，它使LLMs能够全面理解表格的结构和内容，并将自然语言和命令操作对表格实现无缝集成。

    Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the ent
    
[^69]: 利用神经表示的深度跨模态隐写术

    Deep Cross-Modal Steganography Using Neural Representations. (arXiv:2307.08671v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2307.08671](http://arxiv.org/abs/2307.08671)

    本文提出了一种利用隐式神经表示进行深度跨模态隐写术的框架，可以隐藏各种格式的秘密数据，在实验中证明了其可扩展性和适应性。

    

    隐写术是将秘密数据嵌入到另一条消息或数据中，以不容易被察觉的方式。随着深度学习的进步，最近在隐写术中开始利用深度神经网络（DNNs）。然而，现有的深度隐写术技术在范围上有限，因为它们只针对特定的数据类型，并且对于跨模态隐写术不够有效。因此，我们提出了一种利用隐式神经表示（INRs）进行深度跨模态隐写术的框架，可以在覆盖图像中隐藏各种格式的秘密数据。所提出的框架利用INRs来表示秘密数据，可以处理不同形式和分辨率的数据。对不同类型的秘密数据集进行的实验表明，所提出的方法是可扩展的，可以适应不同的模态。

    Steganography is the process of embedding secret data into another message or data, in such a way that it is not easily noticeable. With the advancement of deep learning, Deep Neural Networks (DNNs) have recently been utilized in steganography. However, existing deep steganography techniques are limited in scope, as they focus on specific data types and are not effective for cross-modal steganography. Therefore, We propose a deep cross-modal steganography framework using Implicit Neural Representations (INRs) to hide secret data of various formats in cover images. The proposed framework employs INRs to represent the secret data, which can handle data of various modalities and resolutions. Experiments on various secret datasets of diverse types demonstrate that the proposed approach is expandable and capable of accommodating different modalities.
    
[^70]: 非线性处理与线性光学

    Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2307.08533](http://arxiv.org/abs/2307.08533)

    该论文提出了一种利用多次散射实现多层光学网络的新框架，可以以低光功率同时合成线性和非线性转换，实现能量高效和高速的光学实现神经网络。

    

    深度神经网络通过利用多层数据处理来提取隐藏的表征，取得了显着的突破，但却以大电子计算能力为代价。为了提高能量效率和速度，光学实现神经网络的目标是利用光学带宽的优势和光学互连的能量效率。在缺乏低功率光学非线性性的情况下，在实现多层光学网络中的挑战在于实现多个光学层，而不依赖电子元件。在本研究中，我们提出了一个新颖的框架，利用多次散射可以同时以低光功率合成可编程的线性和非线性转换，利用散射势能（由数据表示）与散射场之间的非线性关系。理论和实验研究表明，通过多次散射进行数据重复可以实现多个光学层。

    Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
    
[^71]: 基于导出图的可判定存在规则集的特征化

    Derivation-Graph-Based Characterizations of Decidable Existential Rule Sets. (arXiv:2307.08481v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2307.08481](http://arxiv.org/abs/2307.08481)

    本文通过定义导出图和无回路导出图的概念，建立了一种特征化方法，使得贪婪有界宽度集合和无回路导出图集合相等，从而提高了我们对存在规则的分析证明论的理解。

    

    本文建立了一种非常表达性的存在规则集的替代特征化方法，这些规则集具有可判定的查询包含关系。我们考虑了一类显著的贪婪有界宽度集合(gbts)及其一种新的广义变种，称为弱gbts(wgbts)。通过重新审视和构建导出图的概念，我们定义了（弱）无回路导出图集合（(w)cdgs），并运用详细的证明论证方法得出结论：gbts和cdgs相等，wgbts和wcdgs也相等。这些新颖的特征化方法推动了对存在规则的分析证明论的理解，并有望在实践中发挥重要作用。

    This paper establishes alternative characterizations of very expressive classes of existential rule sets with decidable query entailment. We consider the notable class of greedy bounded-treewidth sets (gbts) and a new, generalized variant, called weakly gbts (wgbts). Revisiting and building on the notion of derivation graphs, we define (weakly) cycle-free derivation graph sets ((w)cdgs) and employ elaborate proof-theoretic arguments to obtain that gbts and cdgs coincide, as do wgbts and wcdgs. These novel characterizations advance our analytic proof-theoretic understanding of existential rules and will likely be instrumental in practice.
    
[^72]: 无法阻止的攻击: 基于条件扩散模型的标签仅模型逆推

    Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model. (arXiv:2307.08424v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08424](http://arxiv.org/abs/2307.08424)

    本论文提出了一种在标签仅黑盒场景下的模型逆推攻击方法，使用条件扩散模型恢复目标的精确样本，无需额外的优化。

    

    模型逆推攻击(MIAs)旨在从目标模型的训练集中恢复私密数据，这对深度学习模型的隐私构成威胁。MIAs主要关注白盒情景，在此情况下，攻击者可以完全访问目标模型的结构和参数。然而，实际应用是黑盒情景，对手很难获取与模型相关的参数，许多模型仅输出预测标签。现有的黑盒MIAs主要集中在设计优化策略上，而生成模型只是从白盒MIA中使用的GAN迁移而来。据我们所知，我们的研究是标签仅黑盒情景下可行攻击模型的开创性研究。在本文中，我们使用条件扩散模型开发了一种新颖的MIA方法，可以在目标模型输出标签的情况下无需任何额外的优化来恢复目标的精确样本。引入了两个主要技术

    Model inversion attacks (MIAs) are aimed at recovering private data from a target model's training set, which poses a threat to the privacy of deep learning models. MIAs primarily focus on the white-box scenario where the attacker has full access to the structure and parameters of the target model. However, practical applications are black-box, it is not easy for adversaries to obtain model-related parameters, and various models only output predicted labels. Existing black-box MIAs primarily focused on designing the optimization strategy, and the generative model is only migrated from the GAN used in white-box MIA. Our research is the pioneering study of feasible attack models in label-only black-box scenarios, to the best of our knowledge.  In this paper, we develop a novel method of MIA using the conditional diffusion model to recover the precise sample of the target without any extra optimization, as long as the target model outputs the label. Two primary techniques are introduced t
    
[^73]: 从空间感知数据中生成语义形式概念的递归Bateson启发模型

    A Recursive Bateson-Inspired Model for the Generation of Semantic Formal Concepts from Spatial Sensory Data. (arXiv:2307.08087v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08087](http://arxiv.org/abs/2307.08087)

    本文提出了一种基于Bateson的启发，从复杂的空间感知数据中生成层次概念结构的符号方法。

    

    神经符号化的机器学习方法结合了连接主义和符号方法的优势。通常，这些模型使用基于神经架构的第一模块来从复杂数据中提取特征。然后，这些特征被符号引擎处理为符号，提供推理、概念结构、组合性、更好的泛化能力和超出分布学习等可能性。然而，尽管神经方法在感知数据中对符号的基础性工作非常有用，但仍然需要大量的训练和繁琐的标记。本文提出了一种基于符号的方法，从复杂的空间感知数据中生成层次概念结构。该方法基于Bateson关于差异作为生成思想或概念的关键的概念。按照他的建议，该模型通过在多变量数字序列中计算元素顺序比较来提取原子特征。

    Neural-symbolic approaches to machine learning incorporate the advantages from both connectionist and symbolic methods. Typically, these models employ a first module based on a neural architecture to extract features from complex data. Then, these features are processed as symbols by a symbolic engine that provides reasoning, concept structures, composability, better generalization and out-of-distribution learning among other possibilities. However, neural approaches to the grounding of symbols in sensory data, albeit powerful, still require heavy training and tedious labeling for the most part. This paper presents a new symbolic-only method for the generation of hierarchical concept structures from complex spatial sensory data. The approach is based on Bateson's notion of difference as the key to the genesis of an idea or a concept. Following his suggestion, the model extracts atomic features from raw data by computing elemental sequential comparisons in a stream of multivariate numer
    
[^74]: 你需要的只是模仿吗？具有双阶段训练的泛化决策制定

    Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training. (arXiv:2307.07909v1 [cs.AI])

    [http://arxiv.org/abs/2307.07909](http://arxiv.org/abs/2307.07909)

    DualMind使用双阶段训练策略，在控制任务中学习共同知识，并通过模仿行为在不同上下文中做出决策。在实验中，DualMind在MetaWorld和Habitat上表现优于其他通用性代理，具有超过50%和70%的提升。

    

    我们引入了DualMind，这是一个通用性代理，旨在解决当前方法面临的挑战，如过度拟合行为和依赖于特定任务的精细调整。DualMind使用一种新颖的“双阶段”训练策略，模拟了人类学习在世界中行动的方式。模型首先通过针对控制任务定制的自监督目标来学习基本的共同知识，然后通过模仿基于给定提示的行为来学习在不同上下文中做出决策。 DualMind可以处理跨域、场景和具体问题，并仅使用单组模型权重来执行零样本提示，而不需要任务特定的精细调整。我们通过广泛的实验在MetaWorld和Habitat上评估了DualMind，并证明其相较于之前的技术具有更好的泛化性能，在Habitat和MetaWorld上的表现分别超过了其他通用性代理的50%和70%。

    We introduce DualMind, a generalist agent designed to tackle various decision-making tasks that addresses challenges posed by current methods, such as overfitting behaviors and dependence on task-specific fine-tuning. DualMind uses a novel "Dual-phase" training strategy that emulates how humans learn to act in the world. The model first learns fundamental common knowledge through a self-supervised objective tailored for control tasks and then learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts. DualMind can handle tasks across domains, scenes, and embodiments using just a single set of model weights and can execute zero-shot prompting without requiring task-specific fine-tuning. We evaluate DualMind on MetaWorld and Habitat through extensive experiments and demonstrate its superior generalizability compared to previous techniques, outperforming other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld, respe
    
[^75]: 双向可变形运动调制用于基于视频的人体姿态转移

    Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer. (arXiv:2307.07754v1 [cs.CV])

    [http://arxiv.org/abs/2307.07754](http://arxiv.org/abs/2307.07754)

    该论文提出了一种双向可变形运动调制方法，用于基于视频的人体姿态转移。通过几何核偏移和自适应权重调制，同时实现特征对齐和风格转移。与传统方法相比，该方法能够更好地处理服装上的结构图案和不连续的姿势转移，并提供更加满意的结果。

    

    基于视频的人体姿态转移是一个将普通源人体图像根据一系列目标人物姿态进行动画化的视频生成任务。鉴于在服装的高度结构性图案和不连续的姿势转移上存在的困难，现有方法通常会产生不理想的结果，如扭曲的纹理和闪烁的伪影。为解决这些问题，我们提出了一种新颖的可变形运动调制（DMM），该方法利用几何核偏移和自适应权重调制来同时进行特征对齐和风格转移。与在风格转移中使用的普通风格调制不同，所提出的调制机制通过一种非规则感受野根据对象形状自适应重构平滑帧，以实现风格转移。为增强时空一致性，我们利用双向传播从由噪声姿势生成的畸变图像序列中提取隐藏的运动信息。

    Video-based human pose transfer is a video-to-video generation task that animates a plain source human image based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods often generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that utilizes geometric kernel offset with adaptive weight modulation to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adaptively reconstructs smoothed frames from style codes according to the object shape through an irregular receptive field of view. To enhance the spatio-temporal consistency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The prop
    
[^76]: 通过敌对双机器学习的因果参数估计来缓解敌对脆弱性

    Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])

    [http://arxiv.org/abs/2307.07250](http://arxiv.org/abs/2307.07250)

    通过敌对双机器学习方法，可以量化和缓解深度神经网络在面对敌对输入时的脆弱性。

    

    从经过精心设计的视觉输入中衍生出的敌对例子可以轻松地损害深度神经网络的决策过程。为了防止潜在的威胁，各种基于敌对训练的防御方法迅速增长，并成为稳健性的事实上标准方法。尽管最近取得了竞争性的成就，我们观察到敌对脆弱性在不同目标之间存在差异，并且某些脆弱性仍然普遍存在。有趣的是，即使使用更深层次的架构和先进的防御方法，这种奇特的现象仍然无法缓解。为了解决这个问题，本文介绍了一种称为敌对双机器学习（ADML）的因果方法，它允许我们量化网络预测的敌对脆弱性程度，并捕捉对结果的处理效果。ADML可以直接估计敌对扰动本身的因果参数，并减轻可能损害稳健性的负面效应。

    Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridgi
    
[^77]: 大型语言模型中RLHF的秘密 第一部分：PPO

    Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])

    [http://arxiv.org/abs/2307.04964](http://arxiv.org/abs/2307.04964)

    本论文研究了大型语言模型中RLHF的秘密，重点关注了奖励模型、PPO和进程监督等技术路径，探索如何解决RLHF的稳定训练问题。

    

    大型语言模型（LLMs）为推动人工通用智能的进展提供了蓝图。其主要目标是成为以人为中心的（有益、诚实和无害）助手。与人类的对齐具有至关重要的意义，强化学习与人类反馈（RLHF）成为支撑这一追求的关键技术范式。当前的技术路线通常包括用于衡量人类偏好的奖励模型、用于优化策略模型输出的近端策略优化（PPO）以及用于改善逐步推理能力的进程监督。然而，由于奖励设计、环境交互和代理训练的挑战，再加上大型语言模型的试验成本巨大，对于AI研究人员来说，激励技术对齐和LLMs的安全着陆存在重大障碍。RLHF的稳定训练仍然是一个难题。

    Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
    
[^78]: 一次性学习梯度手术在生成模型中的应用

    Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04550](http://arxiv.org/abs/2307.04550)

    本文介绍了一种针对生成模型的一次性学习梯度手术方法，通过操纵梯度来消除数据对模型的影响，并在理论上进行了分析。

    

    最近对“遗忘权”的监管引发了对取消预训练机器学习模型的兴趣。近期的机器学习取消方法通过更新权重来消除样本对权重参数的影响，以逼近一种直接但昂贵的重新训练方法。本文介绍了一种简单且有效的方法，用于在深度生成模型中消除数据的影响。受多任务学习的启发，我们提出了一种使用梯度操作来调整样本之间影响的方法，通过将梯度投影到保留梯度的法平面上进行规范化。我们的方法不依赖于移除样本的统计数据，优于现有基准，并首次在取消生成模型方面提供了理论分析。

    Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.
    
[^79]: 对大型语言模型评估的调查

    A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])

    [http://arxiv.org/abs/2307.03109](http://arxiv.org/abs/2307.03109)

    本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。

    

    大型语言模型（LLMs）由于在各种应用中表现出的前所未有的性能而在学术界和工业界越来越受欢迎。随着LLMs在研究和日常使用中继续发挥着重要作用，它们的评估变得越来越关键，不仅在任务水平上，而且在社会层面上，以更好地了解它们的潜在风险。在过去的几年里，已经做出了相当大的努力来从不同的角度来研究LLMs。本文综述了LLMs的这些评估方法，重点关注三个关键维度：评估什么、在哪里评估以及如何评估。首先，我们从评估任务的角度提供了一个概述，涵盖了一般的自然语言处理任务、推理、医学应用、伦理学、教育、自然科学和社会科学、代理应用和其他领域。其次，我们通过深入探讨评估方法和基准答案来回答“在哪里”和“如何”这两个问题。

    Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
    
[^80]: 使用进化调优增强LLM进行新闻摘要生成

    Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])

    [http://arxiv.org/abs/2307.02839](http://arxiv.org/abs/2307.02839)

    本论文提出一种新的方法使用LLM进行新闻摘要生成，通过进化调优事件模式群体，提高生成结果的准确性和可靠性。

    

    新闻摘要生成是情报分析领域中的重要任务，可以提供准确全面的信息，帮助人们更好地理解和应对复杂的现实事件。然而，传统的新闻摘要生成方法面临一些挑战，包括模型本身和训练数据量的限制，以及文本噪声的影响，使得准确生成可靠信息变得困难。本文提出了一种使用具有强大自然语言理解和生成能力的LLM进行新闻摘要生成的新范式。我们利用LLM从新闻段落中提取多个结构化事件模式，通过遗传算法进化事件模式群体，并选择最适应的事件模式输入LLM生成新闻摘要。设计了一个新闻摘要生成器(NSG)来选择和进化事件模式群体，并生成新闻摘要。

    News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. T
    
[^81]: 不要背诵，模仿过去：无需使用记忆的联邦类增量学习

    Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00497](http://arxiv.org/abs/2307.00497)

    该论文提出了一个无需使用记忆的联邦类增量学习框架，通过生成模型合成过去分布的样本，从而缓解联邦学习中的灾难性遗忘问题。

    

    深度学习模型在训练新数据时容易忘记过去学习的信息。在联邦学习（FL）的背景下，这个问题变得更加明显，因为数据是分散的，每个用户都会独立地进行更改。连续学习（CL）主要在中心化的环境中研究这种所谓的“灾难性遗忘”现象，其中学习者可以直接访问完整的训练数据集。然而，将CL技术应用于FL并不直接，因为涉及到隐私问题和资源限制。本文提出了一个框架，用于联邦类增量学习，该框架利用生成模型从过去的分布中合成样本，而不是存储部分过去的数据。然后，客户端可以利用生成模型在本地缓解灾难性遗忘。生成模型通过在每个任务结束时使用无数据方法在服务器上进行训练，而不请求来自客户端的数据。

    Deep learning models are prone to forgetting information learned in the past when trained on new data. This problem becomes even more pronounced in the context of federated learning (FL), where data is decentralized and subject to independent changes for each user. Continual Learning (CL) studies this so-called \textit{catastrophic forgetting} phenomenon primarily in centralized settings, where the learner has direct access to the complete training dataset. However, applying CL techniques to FL is not straightforward due to privacy concerns and resource limitations. This paper presents a framework for federated class incremental learning that utilizes a generative model to synthesize samples from past distributions instead of storing part of past data. Then, clients can leverage the generative model to mitigate catastrophic forgetting locally. The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Therefore, i
    
[^82]: 在巴西葡萄牙语的语法错误修正方面评估GPT-3.5和GPT-4

    Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])

    [http://arxiv.org/abs/2306.15788](http://arxiv.org/abs/2306.15788)

    该研究评估了GPT-3.5和GPT-4在巴西葡萄牙语语法错误修正方面的有效性，结果显示虽然GPT-4的召回率较高，但语言模型倾向于过度修正。

    

    我们调查了GPT-3.5和GPT-4这两个大型语言模型在巴西葡萄牙语的语法错误修正（GEC）工具中的有效性，并将其性能与Microsoft Word和Google Docs进行了比较。我们引入了一个针对巴西葡萄牙语的GEC数据集，包括四个类别：语法、拼写、互联网和快速输入。我们的结果显示，尽管GPT-4的召回率比其他方法高，但语言模型倾向于具有较低的精确度，导致过度修正。这项研究展示了语言模型作为巴西葡萄牙语实际GEC工具的潜力，并鼓励进一步探索语言模型在非英语语言和其他教育环境中的应用。

    We investigate the effectiveness of GPT-3.5 and GPT-4, two large language models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese and compare their performance against Microsoft Word and Google Docs. We introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar, Spelling, Internet, and Fast typing. Our results show that while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection. This study demonstrates the potential of LLMs as practical GEC tools for Brazilian Portuguese and encourages further exploration of LLMs for non-English languages and other educational settings.
    
[^83]: SparseOptimizer: 通过Moreau-Yosida正则化来降低语言模型的稀疏性，并通过编译器共同设计来加速

    SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])

    [http://arxiv.org/abs/2306.15656](http://arxiv.org/abs/2306.15656)

    SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。

    

    本文介绍了SparseOptimizer，一种新颖的深度学习优化器，通过Moreau-Yosida正则化在大型语言模型（如BERT，ALBERT和GPT）中自然地引入稀疏性。SparseOptimizer设计的关键是嵌入的收缩操作符，它在优化过程中直接引入稀疏性。这个操作符通过坚实的理论框架支持，并包含了一个分析解，从而增强了优化器的鲁棒性和效果。重要的是，SparseOptimizer的即插即用功能消除了对代码修改的需求，使其成为适用于各种大型语言模型的通用适应工具。在GLUE、RACE、SQuAD1和SQuAD2等基准数据集上的实证评估表明，通过SparseOptimizer稀疏化后的SparseBERT和SparseALBERT在性能上与密集型的BERT和ALBERT相当，同时显著减少了参数数量。

    This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
    
[^84]: 膨胀的披露：ChatGPT是否能帮助投资者处理财务信息？

    Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?. (arXiv:2306.10224v1 [econ.GN])

    [http://arxiv.org/abs/2306.10224](http://arxiv.org/abs/2306.10224)

    研究发现生成式 AI 工具 ChatGPT 可以更有效地展示股票市场相关信息，提出了信息膨胀指标并证明其与负面的资本市场后果相关，同时展示其在构建针对性总结方面的效果。

    

    生成式 AI 工具（如 ChatGPT）可以从根本上改变投资者处理信息的方式。我们使用股票市场作为实验室，探究这些工具在总结复杂的公司披露信息时的经济效用。总结摘要明显更短，通常比原始文本缩短超过 70%，而信息内容得到增强。当一份文件具有积极（消极）情感时，其总结变得更积极（消极）。更重要的是，总结对解释股市对披露信息的反应更有效。基于这些发现，我们提出了信息“膨胀”指标。我们显示，膨胀的披露与负面的资本市场后果相关，例如更低的价格有效性和更高的信息不对称性。最后，我们展示了这个模型在构建针对性总结方面的有效性，以确定公司的（非）财务表现和风险。总之，我们的研究结果表明，像 ChatGPT 这样的生成式 AI 工具可以有效地帮助投资者更高效地处理财务信息。

    Generative AI tools such as ChatGPT can fundamentally change the way investors process information. We probe the economic usefulness of these tools in summarizing complex corporate disclosures using the stock market as a laboratory. The unconstrained summaries are dramatically shorter, often by more than 70% compared to the originals, whereas their information content is amplified. When a document has a positive (negative) sentiment, its summary becomes more positive (negative). More importantly, the summaries are more effective at explaining stock market reactions to the disclosed information. Motivated by these findings, we propose a measure of information "bloat." We show that bloated disclosure is associated with adverse capital markets consequences, such as lower price efficiency and higher information asymmetry. Finally, we show that the model is effective at constructing targeted summaries that identify firms' (non-)financial performance and risks. Collectively, our results indi
    
[^85]: 合作多目标强化学习用于交通信号控制和碳减排

    Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v1 [cs.LG])

    [http://arxiv.org/abs/2306.09662](http://arxiv.org/abs/2306.09662)

    本文提出了一种合作的多目标架构，称为MOMA-DDPG，用于交通信号控制和碳减排问题。该方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。结果显示，该方法优于现有最先进的方法，并解决了等待时间和碳排放量两个问题。

    

    现有的交通信号控制系统依赖于过于简化的基于规则的方法，甚至基于强化学习的方法也经常是次优的和不稳定的。为了解决这个问题，我们提出了一个合作的多目标架构，称为多目标多智能体深度确定性策略梯度（MOMA-DDPG），使用衰减权重来估计交通信号控制优化的多个奖励项。我们的方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。我们使用从一个亚洲国家的交通摄像头收集到的真实世界交通数据来评估我们的方法。尽管包含了一个全局智能体，但我们的解决方案仍然是分散的，因为这个智能体在推理阶段不再是必要的。我们的结果证明了MOMA-DDPG的有效性，在所有性能指标上优于最先进的方法。此外，我们提出的系统最小化了等待时间和碳排放量两方面的问题。

    Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting ti
    
[^86]: CLC: 基于对比表示学习的聚类分配方法

    CLC: Cluster Assignment via Contrastive Representation Learning. (arXiv:2306.05439v1 [cs.LG])

    [http://arxiv.org/abs/2306.05439](http://arxiv.org/abs/2306.05439)

    本文提出了一种基于对比学习的聚类方法（CLC），它使用对比学习直接学习聚类分配，并在大规模数据集上取得了更好的聚类性能。

    

    聚类是一项重要而具有挑战性的任务，旨在将样本分组，而不需要手动注释。最近的研究通过对自监督学习得到的特征表示进行聚类，在小型数据集上取得了出色的结果。然而，对于包含大量聚类的数据集，如ImageNet，当前的方法仍然无法实现高聚类性能。在本文中，我们提出了基于对比学习的聚类方法（CLC），它使用对比学习直接学习聚类分配。我们将表示分解为两部分：一部分对类别信息进行编码，并采用等分约束，另一部分捕捉实例因素。我们提出了一种对比损失，使用表示的两个部分。我们在理论上分析了所提出的对比损失，并揭示了CLC在学习聚类分配时为负样本设置不同的权重。进一步的梯度分析表明，当使用CLC时，在大规模数据集上取得了更好的聚类性能。

    Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger 
    
[^87]: 在脉冲神经网络中将噪声作为计算和学习资源

    Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.16044](http://arxiv.org/abs/2305.16044)

    本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    

    脉冲神经元网络是大脑非凡信息处理能力的基础，并已成为神经形态智能的支柱模型。本文介绍了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），采用带有噪声神经元动力学的脉冲神经元模型。该方法显示噪声可以作为计算和学习的资源，并理论上为一般脉冲神经元网络提供了一个框架。此外，NDL为代理梯度提供了深入的生物学合理性。通过将各种SNN架构和算法结合起来，我们展示了我们的方法表现出竞争性能，并且比确定性SNNs表现出更好的鲁棒性。此外，本文还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
    
[^88]: 基于因果推理的图神经网络的监督注意力：更好和更简单的选择，实现强大的关注力。

    Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13115](http://arxiv.org/abs/2305.13115)

    这篇论文提出了一种基于因果推理的框架，通过直接监督注意力函数，提供了强大的监督信号，使得基于注意力的图神经网络在嘈杂的图表达中更加稳健和具有一般性。

    

    最近几年，注意力机制在图表示学习中展现了巨大的潜力。然而，虽然基于注意力的图神经网络的变体正在为许多现实世界的数据集设定新的基准，但最近的研究指出，由于缺乏直接监督，它们所产生的关注力对于嘈杂的图表达不够稳健和具有一般性。在本文中，我们提出了一个新的框架，利用因果性工具为注意力函数的学习过程提供强大的监督信号。具体而言，我们估计了注意力对于最终预测的直接因果效应，然后最大化该效应，引导注意力关注更有意义的邻居。我们的方法可以作为任何经典的基于注意力的图神经网络的即插即用模块，在端到端的方式下使用。广泛的实验在各种基准数据集上表明，通过直接监督注意力函数，模型能够更快地收敛并产生更清晰的结果。

    Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an end-to-end fashion. Extensive experiments on a wide range of benchmark datasets illustrated that, by directly supervising attention functions, the model is able to converge faster with a clearer de
    
[^89]: 评估开放式问答评估

    Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12421](http://arxiv.org/abs/2305.12421)

    本研究侧重于评估开放式问答（Open-QA）任务的方法，引入了一个新的任务QA-Eval和数据集EVOUNA，通过人工评估方法来评估AI生成的答案的准确性。我们调查了与人工评估相关的方法，并讨论了当前方法的缺陷和改进方法。我们相信这对于未来的自动评估工具发展和研究具有价值。

    

    本研究侧重于对开放式问答（Open-QA）任务的评估，该任务可以直接估计大型语言模型（LLMs）的事实性。目前的自动评估方法已显示出一定的局限性，表明人工评估仍然是最可靠的方法。我们引入了一个新的任务，即评估QA评估（QA-Eval）以及相应的数据集EVOUNA，旨在评估AI生成的答案与Open-QA中的标准答案之间的准确性。我们利用人工标注的结果来评估这些方法的性能。具体而言，本研究调查了那些与人工评估具有高度相关性的方法，认为它们更可靠。我们还讨论了当前方法的缺陷以及改进基于LLM的评估器的方法。我们相信，这个新的QA-Eval任务和相应的数据集EVOUNA将促进更有效的自动评估工具的开发，并对未来的研究具有价值。

    This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
    
[^90]: 具有概率保证的神经网络鲁棒的反事实解释

    Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])

    [http://arxiv.org/abs/2305.11997](http://arxiv.org/abs/2305.11997)

    本文提出了一种可靠的神经网络反事实解释方法，该方法可以针对自然发生的模型变化提供高概率的鲁棒性。

    

    针对神经网络发现偏移，通过使用稳定性度量来量化反事实解释对可能的模型变化的鲁棒性。通过在反事实解释优化中引入正则化项来将生成的反事实解释靠近数据流形，从而实现了对自然发生的模型变化的高概率鲁棒性。新的算法在合成和现实世界数据集上进行实验，证明了其有效性。

    There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{<}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
    
[^91]: 在Lenia中捕获新兴复杂性

    Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v1 [cs.NE])

    [http://arxiv.org/abs/2305.09378](http://arxiv.org/abs/2305.09378)

    研究人工生命平台Lenia，通过识别复杂新兴行为的度量标准和使用遗传算法产生不同行为的结果，以进化出更好的Lenia行为。

    

    本研究项目探讨了Lenia，这是一个模拟数字生物系统的人工生命平台。Lenia的生态系统由简单的人工生物组成，它们可以移动、消耗、生长和繁殖。该平台是一个研究人工生命和进化的重要工具，因为它提供了一个可扩展和灵活的环境，用于创建具有不同能力和行为的多样化生物。该研究的关键是在Lenia中测量复杂性，识别测量规则的长期复杂性新兴行为的度量标准，旨在进化出尚未发现的更好的Lenia行为。遗传算法使用相邻区域或核作为基因型，同时保持Lenia的其他参数（例如生长函数）不变，以产生不同人口行为的结果，然后测量适应度值以决定所得行为的复杂性。首先，我们使用时间变化作为适应度函数，

    This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenia's ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitn
    
[^92]: 深度学习与逻辑推理的可扩展耦合

    Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])

    [http://arxiv.org/abs/2305.07617](http://arxiv.org/abs/2305.07617)

    本文介绍了一种可扩展的神经网络模型和损失函数，能够有效学习如何解决NP-hard推理问题，并在离散图模型上进行了实验验证。同时可以提高数据效率和可解释性，并具有对预测的控制能力。

    

    在将离散推理与神经网络混合的不断探索中，出现了越来越多的对神经结构具备从自然输入中学习如何解决离散推理或优化问题的兴趣。本文提出了一种可扩展的神经结构以及专门用于学习被表示为离散图模型的 NP-hard 推理问题的约束和标准的损失函数。我们的损失函数解决了 Besag 的伪对数似然的主要限制之一，能够学习高能量函数。我们通过实验证明，它能够有效地从自然输入中学习如何解决 NP-hard 推理问题，如符号、视觉或多解数数独问题，以及蛋白质设计问题的能量优化形式，提高了数据效率、可解释性以及对预测的 \textit{a posteriori} 控制。

    In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
    
[^93]: 分布式多目标决策制定

    Distributional Multi-Objective Decision Making. (arXiv:2305.05560v1 [cs.AI])

    [http://arxiv.org/abs/2305.05560](http://arxiv.org/abs/2305.05560)

    该论文介绍了一种分布式多目标决策制定的方法，其中引入了分布式不支配集和凸分布不支配集的概念，证明了它们可以包含所有最优解，通过实验验证了方法的有效性。

    

    在具有冲突目标的场景中进行有效的决策支持，可以向决策者呈现一组可能最优解。我们探讨这些解应该包含哪些策略以及如何高效地计算这些解。基于这一目标，我们采用分布式方法，引入一种新颖的优势准则，直接关联策略的回报分布。基于这个准则，我们提出了分布式不支配集，并证明其中包含了帕累托前沿的忽略的最优策略。此外，我们提出了凸分布不支配集，并证明它包括所有在多维风险规避决策者中最大化预期效用的策略。我们设计了一种新算法来学习分布式不支配集，并贡献了剪枝算子来将其减少到凸分布不支配集。通过实验，我们证明了这些方法的可行性和有效性。

    For effective decision support in scenarios with conflicting objectives, sets of potentially optimal solutions can be presented to the decision maker. We explore both what policies these sets should contain and how such sets can be computed efficiently. With this in mind, we take a distributional approach and introduce a novel dominance criterion relating return distributions of policies directly. Based on this criterion, we present the distributional undominated set and show that it contains optimal policies otherwise ignored by the Pareto front. In addition, we propose the convex distributional undominated set and prove that it comprises all policies that maximise expected utility for multivariate risk-averse decision makers. We propose a novel algorithm to learn the distributional undominated set and further contribute pruning operators to reduce the set to the convex distributional undominated set. Through experiments, we demonstrate the feasibility and effectiveness of these metho
    
[^94]: 基于随机专家的医学图像分割的隐性解剖渲染

    Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v1 [cs.CV])

    [http://arxiv.org/abs/2304.03209](http://arxiv.org/abs/2304.03209)

    提出了一种名为MORSE的基于隐式解剖渲染的通用神经渲染框架，旨在在医学图像分割中帮助融合高级语义相关内容和低级解剖特征。

    

    将高级语义相关内容和低级解剖特征集成到医学图像分割中非常重要。近期基于深度学习的医学分割方法在更好地建模这些信息方面取得了很大的成功。然而，医学分割的卷积操作通常在规则网格上运行，这在高频区域即边界区域中天生模糊。本文提出了一个名为MORSE的通用隐式神经渲染框架，旨在在解剖层面上为医学图像分割辅助学习。我们的方法基于事实：相较于离散的基于网格的表示方式，隐式神经表示在拟合复杂信号和解决计算机图形问题时表现更为有效。我们的方法的核心是以端到端的方式将医学图像分割视为渲染问题。具体而言，我们持续地对齐粗略的分割p并利用随机专家来生成渲染图像。

    Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation p
    
[^95]: ACTION++：使用自适应解剖对比度改善半监督医学图像分割

    ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])

    [http://arxiv.org/abs/2304.02689](http://arxiv.org/abs/2304.02689)

    本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。

    

    医学数据通常表现为长尾分布，存在严重的类别不平衡，这自然导致少数类别（即边界区域或罕见物体）的分类困难。最近的工作通过配备无监督对比标准，在长尾场景中显着改进了半监督医学图像分割。然而，在类别分布也高度不平衡的标记数据部分中，它们的表现仍不清楚。在这项工作中，我们提出了ACTION++，一种改进的具有自适应解剖对比的对比学习框架，用于半监督医学分割。

    Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
    
[^96]: SLCA: 预训练模型上用于连续学习的慢学习者与分类器对齐

    SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05118](http://arxiv.org/abs/2303.05118)

    SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。

    

    连续学习的目标是在学习顺序到达的数据中提高识别模型的性能。尽管大部分现有工作都建立在从头学习的前提下，但越来越多的努力已经致力于融入预训练的好处。然而，如何在每个增量任务中自适应地利用预训练的知识，同时保持其泛化能力，仍然是一个未解决的问题。在这项工作中，我们对预训练模型上的连续学习进行了广泛的分析，并将关键挑战归因于渐进过拟合问题。观察到在表征层次上选择性降低学习率几乎可以解决这个问题，我们提出了一种简单但极其有效的方法，名为慢学习者与分类器对齐（SLCA），通过建模类别分布并在事后对齐分类层次，进一步改进了分类层次。在各种实验中，我们证明了SLCA在连续学习任务中的有效性和性能优势。

    The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
    
[^97]: 在3D点云中的开放词汇支撑检测

    Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.02401](http://arxiv.org/abs/2303.02401)

    本文提出了一种在3D点云中进行无限数量支撑检测的开放词汇支撑检测方法，通过同时学习支撑文本和点特征来利用支撑之间的语义关系，实现了零-shot检测，能够在没有注释示例的情况下检测以前未见到的支撑。实验结果表明，OpenAD在各种设置上表现出优异性能。

    

    支撑检测是一个具有广泛机器人应用的挑战性问题。传统的支撑检测方法局限于预定义的支撑标签，可能限制了智能机器人在复杂和动态环境中的适应性。在本文中，我们提出了开放词汇支撑检测（OpenAD）方法，能够在3D点云中检测无限数量的支撑。通过同时学习支撑文本和点特征，OpenAD成功地利用了支撑之间的语义关系。因此，我们提出的方法实现了零-shot检测，并能够在没有任何注释示例的情况下检测以前未见到的支撑。大量实验结果表明，OpenAD在各种支撑检测设置上有效，并且在性能上超过了其他基线方法。此外，我们还展示了OpenAD在实际中的实用性。

    Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real
    
[^98]: 基于人类词汇关联和图嵌入的波斯语话题检测

    Persian topic detection based on Human Word association and graph embedding. (arXiv:2302.09775v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.09775](http://arxiv.org/abs/2302.09775)

    本文提出了一种基于人类词汇关联的波斯语社交媒体话题检测框架，该方法利用了词汇关联和关联引力力量生成图，并通过嵌入和聚类方法提取话题。

    

    本文提出了一种基于人类词汇关联的社交媒体话题检测框架。在社交媒体中识别讨论的话题已经成为一个重要的挑战。目前这个领域的大部分工作都是在英语方面的，但是在波斯语方面也做了很多工作，特别是波斯语的微博。此外，现有的工作更多地关注探索频繁模式或语义关系，忽视了语言的结构方法。本文提出了一种使用人类词汇关联的话题检测框架。这种方法使用了模仿心理能力进行词汇关联。该方法还计算出了关联引力力量来显示词语的关联程度。利用这个参数，可以生成一个图，通过嵌入这个图并使用聚类方法，可以提取出话题。这个方法已经应用于从Telegram收集的波斯语数据集上。进行了多个实验研究。

    In this paper, we propose a framework to detect topics in social media based on Human Word Association. Identifying topics discussed in these media has become a critical and significant challenge. Most of the work done in this area is in English, but much has been done in the Persian language, especially microblogs written in Persian. Also, the existing works focused more on exploring frequent patterns or semantic relationships and ignored the structural methods of language. In this paper, a topic detection framework using HWA, a method for Human Word Association, is proposed. This method uses the concept of imitation of mental ability for word association. This method also calculates the Associative Gravity Force that shows how words are related. Using this parameter, a graph can be generated. The topics can be extracted by embedding this graph and using clustering methods. This approach has been applied to a Persian language dataset collected from Telegram. Several experimental studi
    
[^99]: 内部奖励的强化学习

    Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00270](http://arxiv.org/abs/2302.00270)

    这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。

    

    我们研究了一类强化学习问题，其中用于策略学习的奖励信号由一个与策略相关且与策略同时优化的判别器生成。策略和判别器之间的相互依赖导致了不稳定的学习过程，因为来自不成熟判别器的奖励信号是嘈杂的，阻碍了策略的学习；反过来，未经优化的策略也会阻碍判别器的学习。我们将这种学习设置称为“内部奖励的强化学习”（IRRL），因为奖励不是直接来自环境，而是由判别器“内部”提供的。在本文中，我们正式地表述了IRRL，并提出了一类属于IRRL的问题。我们从理论上推导并经验性地分析了IRRL中奖励函数的影响，并基于这些分析提出了修剪线性奖励函数。实验结果表明，所提出的奖励函数可以持续稳定训练过程。

    We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
    
[^100]: 使用深度强化学习的基于执行的代码生成

    Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13816](http://arxiv.org/abs/2301.13816)

    使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。

    

    利用在大规模代码语料库上预训练的编程语言（PL）模型，作为自动化软件工程过程的手段，在代码完成、代码翻译和程序合成等各种代码生成任务中表现出了相当的潜力。然而，当前的方法主要依赖于从文本生成中借用的监督微调目标，忽视了代码的独特序列级特征，包括但不限于可编译性以及语法和功能正确性。为了解决这个限制，我们提出了PPOCoder，一种新的代码生成框架，它将预训练的PL模型与Proximal Policy Optimization（PPO）相结合，PPO是一种广泛使用的深度强化学习技术。通过利用代码执行和结构对齐的非可微反馈，PPOCoder将外部代码特定知识无缝集成到模型优化过程中。这是重要的。

    The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
    
[^101]: 基于人类词汇联想的社交网络主题检测模型

    A Human Word Association based model for topic detection in social networks. (arXiv:2301.13066v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13066](http://arxiv.org/abs/2301.13066)

    本文提出了一个基于人类词汇联想的社交网络主题检测框架，通过考虑语言结构并设计专门的抽取算法，在FA-CUP数据集上取得了比其他方法更好的性能。

    

    随着社交网络的广泛使用，检测这些网络中讨论的主题已成为一个重要的挑战。目前的工作主要基于频繁模式挖掘或语义关系，而没有考虑语言结构。语言结构方法的意义在于发现词语之间的关系以及人类如何理解它们。因此，本文利用词汇联想的心理能力模拟概念，提出了一种基于人类词汇联想的社交网络主题检测框架。该框架基于人类词汇联想方法，并设计了专门的抽取算法。该方法在FA-CUP数据集上进行了评估，该数据集是主题检测领域的基准数据集。结果表明，与其他方法相比，所提出的方法在主题召回率和关键词F1值上有较好的改进。此外，主题检测领域中的大多数先前工作主要基于模式挖掘或语义关系。

    With the widespread use of social networks, detecting the topics discussed in these networks has become a significant challenge. The current works are mainly based on frequent pattern mining or semantic relations, and the language structure is not considered. The meaning of language structural methods is to discover the relationship between words and how humans understand them. Therefore, this paper uses the Concept of the Imitation of the Mental Ability of Word Association to propose a topic detection framework in social networks. This framework is based on the Human Word Association method. A special extraction algorithm has also been designed for this purpose. The performance of this method is evaluated on the FA-CUP dataset. It is a benchmark dataset in the field of topic detection. The results show that the proposed method is a good improvement compared to other methods, based on the Topic-recall and the keyword F1 measure. Also, most of the previous works in the field of topic de
    
[^102]: 基于漏斗的强化学习中信号时态逻辑任务的奖励塑形

    Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.03181](http://arxiv.org/abs/2212.03181)

    本文提出了一种基于漏斗函数的强化学习算法，用于在连续状态空间中学习鲁棒满足信号时态逻辑规范的时间依赖策略。

    

    信号时态逻辑（STL）是描述动态系统复杂时态和逻辑行为的强大框架。许多研究尝试利用强化学习来学习强制执行STL规范的控制器，然而，他们无法有效解决在连续状态空间中确保鲁棒满足和保持可控性的挑战。本文借助漏斗函数的概念，提出了一种可控的强化学习算法，用于学习连续状态空间中STL规范的鲁棒满足的时间依赖策略。我们在不同环境下演示了我们方法的实用性。

    Signal Temporal Logic (STL) is a powerful framework for describing the complex temporal and logical behaviour of the dynamical system. Numerous studies have attempted to employ reinforcement learning to learn a controller that enforces STL specifications; however, they have been unable to effectively tackle the challenges of ensuring robust satisfaction in continuous state space and maintaining tractability. In this paper, leveraging the concept of funnel functions, we propose a tractable reinforcement learning algorithm to learn a time-dependent policy for robust satisfaction of STL specification in continuous state space. We demonstrate the utility of our approach on several STL tasks using different environments.
    
[^103]: 通过立方和征服法反转密码哈希函数

    Inverting Cryptographic Hash Functions via Cube-and-Conquer. (arXiv:2212.02405v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2212.02405](http://arxiv.org/abs/2212.02405)

    该研究应用了Cube-and-Conquer方法将MD4和MD5的步骤缩减版本进行反转。通过逐步修改Dobbertin约束来生成MD4的反转问题，并使用立方阶段的立方和征服方法进行反转。

    

    MD4和MD5是在1990年代初提出的具有里程碑意义的密码哈希函数。MD4由48个步骤组成，给定任意有限大小的消息，它可以产生一个128位的哈希值。MD5是MD4的更安全的64步扩展。尽管MD4和MD5都容易受到碰撞攻击的影响，但是翻转它们，即通过哈希值找到原始消息仍然不现实。在2007年，MD4的39步版本通过化简为SAT并应用CDCL求解器以及所谓的Dobbertin约束被反转。至于MD5，在2012年，它的28步版本通过CDCL求解器仅针对一个特定的哈希值被反转，而不加任何额外的约束。本研究将立方和征服（CDCL与先行搜索的组合）应用于步骤缩减版本的MD4和MD5的反转。为此，提出了两个算法。第一个算法通过逐步修改Dobbertin约束来生成MD4的反转问题。第二个算法尝试使用立方阶段的立方和征服方法进行反转。

    MD4 and MD5 are seminal cryptographic hash functions proposed in early 1990s. MD4 consists of 48 steps and produces a 128-bit hash given a message of arbitrary finite size. MD5 is a more secure 64-step extension of MD4. Both MD4 and MD5 are vulnerable to practical collision attacks, yet it is still not realistic to invert them, i.e. to find a message given a hash. In 2007, the 39-step version of MD4 was inverted via reducing to SAT and applying a CDCL solver along with the so-called Dobbertin's constraints. As for MD5, in 2012 its 28-step version was inverted via a CDCL solver for one specified hash without adding any additional constraints. In this study, Cube-and-Conquer (a combination of CDCL and lookahead) is applied to invert step-reduced versions of MD4 and MD5. For this purpose, two algorithms are proposed. The first one generates inversion problems for MD4 by gradually modifying the Dobbertin's constraints. The second algorithm tries the cubing phase of Cube-and-Conquer with di
    
[^104]: 连续蒙特卡洛图搜索

    Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.01426](http://arxiv.org/abs/2210.01426)

    连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。

    

    在许多复杂的连续决策任务中，在线规划对于高性能至关重要。为了实现高效的在线规划，蒙特卡洛树搜索（MCTS）采用了一个有原则的机制来权衡探索和利用。MCTS在许多离散决策领域（如围棋、国际象棋和将棋）中胜过了其他方法。而针对连续领域的MCTS扩展也已提出。然而，由于固有的高分支因子和导致搜索树大小爆炸的问题，现有方法受到了限制。为了解决这个问题，我们提出了连续蒙特卡洛图搜索（CMCGS），这是一种新颖的MCTS扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS利用了一个洞察力，在规划过程中，将相似状态之间共享相同的动作策略可以得到高性能。为了实现这个想法，CMCGS在每个时间步骤中将相似状态聚类成有限数量的随机动作赌博节点，这些节点共享相同的动作策略。

    In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
    
[^105]: DESCN: 深度整体空间交叉网络用于个体治疗效果估计

    DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09920](http://arxiv.org/abs/2207.09920)

    本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。

    

    因果推断在电子商务和精准医学等领域有广泛应用，其性能严重依赖于个体治疗效果(ITE)的准确估计。传统上，通过分别在各自的样本空间中建模受治疗组和对照组的响应函数来预测ITE。然而，在实际应用中，这种方法通常遇到两个问题，即由于治疗偏差而导致的受治疗组和对照组之间的分布偏离，以及其人口规模之间的显著样本不平衡。本文提出了深度整体空间交叉网络(DESCN)，以端到端的方式建模治疗效果。DESCN通过一个跨网络以多任务学习的方式捕捉治疗倾向、响应和隐藏治疗效果的综合信息。我们的方法在整个样本空间中联合学习治疗和响应函数，以避免治疗偏差，并采用中间伪处理方式。

    Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
    
[^106]: 非凸非光滑问题中随机优化的稳定性和泛化能力

    Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems. (arXiv:2206.07082v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.07082](http://arxiv.org/abs/2206.07082)

    本文首次启动了对非凸非光滑问题上随机优化的系统稳定性和泛化分析，引入了新颖的算法稳定性度量，并建立了它们与种群梯度和经验梯度之间的定量连接。

    

    随机优化在机器学习中的应用非常广泛，这促使了许多理论研究，以理解其实际成功。大多数现有的研究都集中在优化误差的收敛性上，而随机优化的泛化分析远远滞后。这尤其适用于实践中经常遇到的非凸非光滑问题。在本文中，我们首次启动了对非凸非光滑问题上随机优化的系统稳定性和泛化分析。我们引入了新颖的算法稳定性度量，并建立了它们与种群梯度和经验梯度之间的定量连接，然后进一步将其扩展到研究经验风险和群体风险的Moreau包络之间的差距。据我们所知，这些稳定性和泛化性的定量联系，无论是在梯度还是Moreau包络方面，都是文献中首次出现的。我们进一步将我们的泛化理论应用于分析深度线性神经网络(DLNN)，在其中我们展示了所提出的理论可以比现有的工作提供更紧的泛化误差界。我们的理论结果通过对合成和现实世界数据集的数值实验进行了验证。

    Stochastic optimization has found wide applications in minimizing objective functions in machine learning, which motivates a lot of theoretical studies to understand its practical success. Most of existing studies focus on the convergence of optimization errors, while the generalization analysis of stochastic optimization is much lagging behind. This is especially the case for nonconvex and nonsmooth problems often encountered in practice. In this paper, we initialize a systematic stability and generalization analysis of stochastic optimization on nonconvex and nonsmooth problems. We introduce novel algorithmic stability measures and establish their quantitative connection on the gap between population gradients and empirical gradients, which is then further extended to study the gap between the Moreau envelope of the empirical risk and that of the population risk. To our knowledge, these quantitative connection between stability and generalization in terms of either gradients or Morea
    
[^107]: FedFormer：强化学习中的上下文联邦学习与注意力

    FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2205.13697](http://arxiv.org/abs/2205.13697)

    FedFormer是一种新的强化学习联邦策略，利用Transformer Attention上下文地汇总来自不同学习智能体的嵌入，具有更高的回报和更高的效率。

    

    多智能体联邦强化学习中一个核心问题是如何汇总多个智能体的见解。通常采用将每个参与智能体的模型权重取平均得到一个共同模型（FedAvg）。我们提出了FedFormer，一种新的联邦策略，利用Transformer Attention来上下文地汇总来自不同学习智能体的嵌入。通过这样做，我们根据当前智能体的环境和学得关系有选择地衡量其他智能体的贡献，从而提供更有效和高效的联邦学习。我们在Meta-World环境中对我们的方法进行了评估，并发现我们的方法在FedAvg和非联邦Soft Actor-Critic单智能体方法上都取得了显著的改进。与Soft Actor-Critic相比，我们的结果表明FedFormer在仍遵守联邦学习的隐私约束的同时获得了更高的分集回报。最后，我们还演示了该方法的实现。

    A core issue in multi-agent federated reinforcement learning is defining how to aggregate insights from multiple agents. This is commonly done by taking the average of each participating agent's model weights into one common model (FedAvg). We instead propose FedFormer, a novel federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents. In so doing, we attentively weigh the contributions of other agents with respect to the current agent's environment and learned relationships, thus providing a more effective and efficient federation. We evaluate our methods on the Meta-World environment and find that our approach yields significant improvements over FedAvg and non-federated Soft Actor-Critic single-agent methods. Our results compared to Soft Actor-Critic show that FedFormer achieves higher episodic return while still abiding by the privacy constraints of federated learning. Finally, we also demonstr
    
[^108]: 一种基于变密度Noisier2Noise的自监督磁共振图像重建的理论框架

    A theoretical framework for self-supervised MR image reconstruction using sub-sampling via variable density Noisier2Noise. (arXiv:2205.10278v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.10278](http://arxiv.org/abs/2205.10278)

    本研究提出了一个基于Noisier2Noise框架的自监督磁共振图像重建理论框架，解释了SSDU方法的性能，并提出了两种修改。

    

    近年来，利用神经网络的统计建模能力重建子采样磁共振成像数据引起了人们的关注。大多数提出的方法假设存在代表性的完全采样数据集，并使用完全督导式训练。然而，对于许多应用程序，完全采样的训练数据是不可用的，而且可能非常难以获取。因此，发展和理解仅使用子采样数据进行训练的自监督方法非常有价值。本文将最初用于自监督去噪任务的Noisier2Noise框架扩展到变密度子采样MRI数据。我们使用Noisier2Noise框架来解释近期提出的在实践中表现良好但缺乏理论基础的自监督学习 via 数据欠采样（SSDU）方法的性能，提出两种修改。

    In recent years, there has been attention on leveraging the statistical modeling capabilities of neural networks for reconstructing sub-sampled Magnetic Resonance Imaging (MRI) data. Most proposed methods assume the existence of a representative fully-sampled dataset and use fully-supervised training. However, for many applications, fully sampled training data is not available, and may be highly impractical to acquire. The development and understanding of self-supervised methods, which use only sub-sampled data for training, are therefore highly desirable. This work extends the Noisier2Noise framework, which was originally constructed for self-supervised denoising tasks, to variable density sub-sampled MRI data. We use the Noisier2Noise framework to analytically explain the performance of Self-Supervised Learning via Data Undersampling (SSDU), a recently proposed method that performs well in practice but until now lacked theoretical justification. Further, we propose two modifications 
    

