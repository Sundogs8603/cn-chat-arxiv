# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ASIC: Aligning Sparse in-the-wild Image Collections.](http://arxiv.org/abs/2303.16201) | 该文提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法，可用于一致性和高质量的图像集对应关系。 |
| [^2] | [Natural Selection Favors AIs over Humans.](http://arxiv.org/abs/2303.16200) | 这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。 |
| [^3] | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention.](http://arxiv.org/abs/2303.16199) | 本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。 |
| [^4] | [Planning with Sequence Models through Iterative Energy Minimization.](http://arxiv.org/abs/2303.16189) | 本文提出了一种基于迭代能量最小化方法的序列模型规划集成方法，能够提高强化学习性能在BabyAI和Atari等不同任务中具有应用价值。 |
| [^5] | [Large-scale Training Data Search for Object Re-identification.](http://arxiv.org/abs/2303.16186) | 本论文提出了一种用于目标重新识别问题的搜索和修剪方法，该方法从大规模数据池中构建训练集，可以在保证与源池同等或更高的准确性情况下，将训练集的大小减小80％。 |
| [^6] | [Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP.](http://arxiv.org/abs/2303.16166) | 在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。 |
| [^7] | [VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices.](http://arxiv.org/abs/2303.16150) | VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。 |
| [^8] | [Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models.](http://arxiv.org/abs/2303.16133) | 该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。 |
| [^9] | [Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification.](http://arxiv.org/abs/2303.16132) | 本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。 |
| [^10] | [Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information.](http://arxiv.org/abs/2303.16098) | 介绍了首个公开版本的巴西葡萄牙语通用现代语料库Carolina，它使用了增强的来源、类型、版本和文本完整性的网络语料库方法，并且可作为语言学研究和计算机科学研究语言模型的重要资源。 |
| [^11] | [Attention Boosted Autoencoder for Building Energy Anomaly Detection.](http://arxiv.org/abs/2303.16097) | 本论文提出了一种基于注意力机制的自编码器模型，可用于建筑能耗异常检测，同时使用可视化方法辅助理解模型所捕捉到的关系。 |
| [^12] | [LinK: Linear Kernel for LiDAR-based 3D Perception.](http://arxiv.org/abs/2303.16094) | LinK是一种新方法，它通过用线性内核生成器替换静态内核矩阵，并重复使用预计算聚合结果，在维持2D卷积级别计算复杂度的同时，使每个体素在感知21x21x21范围内的上下文。 |
| [^13] | [Evolutionary Design of the Memory Subsystem.](http://arxiv.org/abs/2303.16074) | 本文研究了三种方法，通过分析优化寄存器文件的热影响、优化缓存配置来改善性能和能源消费，简化内存管理器设计和评估过程。最终实现了对整个内存子系统的优化。 |
| [^14] | [Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity.](http://arxiv.org/abs/2303.16067) | 使用惰性学习只对错误样本进行更新参数，实现快速、节能的神经网络训练，并在单层MLP模型上达到了99.2％的测试准确率，比匹配的反向传播网络快7.6倍。 |
| [^15] | [Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models.](http://arxiv.org/abs/2303.16047) | 提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。 |
| [^16] | [Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models.](http://arxiv.org/abs/2303.16045) | 本论文提出了一种基于人工通用智能方法原则的通用单变量信号去卷积方法。通过计算“通用分布”的估计来独立于概率分布地构建一个通用模型，并基于信息论和算法概率的多维空间重构，探索非随机数据中关于物理性质的信息编码。该方法在编码理论尤其是零失真压缩方面有应用价值。 |
| [^17] | [HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation.](http://arxiv.org/abs/2303.15994) | 该论文提出一种利用高低频率关系的无偏倚的全景场景图生成方法，解决了长尾问题和主体-对象对拥有多个重叠关系的问题。在广泛实验中取得了最先进的结果。 |
| [^18] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^19] | [Randomly Initialized Subnetworks with Iterative Weight Recycling.](http://arxiv.org/abs/2303.15953) | 本文提出了修改版的Edge-Popup和Biprop算法，名为迭代权重回收，识别随机初始化网络中的重要权重子集，以进行层内重用，并找到高精度的子网络，从而提高了模型稀疏度，并用相反的发现来补充多重彩票假设。 |
| [^20] | [Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings.](http://arxiv.org/abs/2303.15947) | 该论文提出了一种名为Deep Selection的神经网络，用于从多目标手术录像中选择最佳视角的摄像机。与传统方法不同，该网络通过监督学习专家注释，预测摄像机的选择概率，从而优化手术记录的效果。 |
| [^21] | [When Brain-inspired AI Meets AGI.](http://arxiv.org/abs/2303.15935) | 本文综述了从AGI的角度看脑启发式人工智能的研究现状和重要特征，讨论了实现AGI的重要技术，如上下文学习和提示调整等。 |
| [^22] | [From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification.](http://arxiv.org/abs/2303.15916) | 本论文在时间序列领域对两种GAN架构进行了评估，结果以GSWGAN表现最佳，可以私密地生成保护数据隐私的公共数据。 |
| [^23] | [Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm.](http://arxiv.org/abs/2303.15901) | 本论文提出了一种新的对抗攻击防御方法，通过将防御蒸馏机制与去噪自编码器相结合，来降低模型对有毒攻击的敏感度。 |
| [^24] | [Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning.](http://arxiv.org/abs/2303.15888) | 该论文提出了一种双重知识蒸馏方法Data-Agnostic Consolidation（DAC），在不使用原始数据的情况下，通过一种新的投影潜空间蒸馏损失，在分布式连续学习中实现了SCD之间的前向转移并取得了最先进的准确性。 |
| [^25] | [4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images.](http://arxiv.org/abs/2303.15848) | 本论文提出了一个新的方法来构建具有4K分辨率的雾霾去除基准数据集，并开发了一种新的多层感知器模型来处理此任务，实验结果表明该方法在视觉质量和客观评估指标方面都优于现有方法。 |
| [^26] | [Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes.](http://arxiv.org/abs/2303.15846) | 本论文研究了使用初级保健医师的患者医疗笔记进行肺癌早期预测的问题，并探讨了针对高度不平衡分类问题的软提示调整和静态词嵌入模型在模型训练中的表现。 |
| [^27] | [CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics.](http://arxiv.org/abs/2303.15844) | 本论文提出了一个通用框架，使用进化方法生成反事实序列并避免对领域知识的需求以提高可行性。 |
| [^28] | [Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning.](http://arxiv.org/abs/2303.15834) | 该论文提出了一种元机器学习方法，用于在跨组织的企业网络中实现全面的数据分析。该方法可以解决在数据分布在多个法律实体之间时，披露敏感信息和需要交换大量数据的难题，并显示了在工业应用中的可行性。 |
| [^29] | [The transformative potential of machine learning for experiments in fluid mechanics.](http://arxiv.org/abs/2303.15832) | 本文介绍了机器学习在实验流体力学中的三个方面的应用潜力：提高测量技术精度，改进实验设计和实现实时估计和控制。 |
| [^30] | [MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation.](http://arxiv.org/abs/2303.15826) | 本文提出了一个跨模态图像分割的无监督域自适应方法，通过多尺度自我集成和对抗无配对翻译，实现了对听神经瘤和耳蜗的自动分割，取得了很好的效果。 |
| [^31] | [One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization.](http://arxiv.org/abs/2303.15822) | 本文探讨在适配器微调中固定所有预训练模型参数并插入参数高效结构适配器的方法，旨在缓解多语言模型中遗忘问题。在代码搜索和摘要任务中实现了一致的改进并取得了最先进的结果，同时还展示了适配器在跨语言和低资源场景中的有效性。 |
| [^32] | [Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization.](http://arxiv.org/abs/2303.15810) | 本文提出了一种新算法，IVR-Q，用于离线强化学习，其通过隐式价值正则化的方式避免了OOD动作带来的价值函数偏移，并通过最小化IVR loss来改善策略。在多个基准任务上的实验结果证明其优于现有方法并实现了最佳性能。 |
| [^33] | [Ecosystem Graphs: The Social Footprint of Foundation Models.](http://arxiv.org/abs/2303.15772) | 该论文提出了一种名为「生态图表」的文档框架，可以透明地集中基础模型的社会影响方面的知识，这可以提高其透明度和问责度。 |
| [^34] | [RobustSwap: A Simple yet Robust Face Swapping Model against Attribute Leakage.](http://arxiv.org/abs/2303.15768) | 本文提出了一种简单而强大的人脸交换模型RobustSwap，能够有效避免源属性泄漏问题，并利用3DMM的隐式和显式信息的协调来实现源图像结构的导入和目标图像准确姿态。 |
| [^35] | [On Feature Scaling of Recursive Feature Machines.](http://arxiv.org/abs/2303.15745) | 本报告通过实验探究了递归特征机器的行为，发现其在添加随机噪声特征时MSE曲线呈现出降低-增加-降低的模式，并且与神经网络的“双峰下降”现象相似，为后续研究奠定基础。 |
| [^36] | [Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume Modulation Approach.](http://arxiv.org/abs/2303.15734) | 本文介绍了一种自适应背景音乐的方法，它由五种不同的乐器演奏名为“空中小姐曲”的古典音乐组成，并通过改变乐器的音量来适应游戏的不同元素。实验结果表明，使用这种自适应背景音乐可以改善游戏的体验。 |
| [^37] | [Evaluation of ChatGPT for NLP-based Mental Health Applications.](http://arxiv.org/abs/2303.15727) | 本篇论文评估了基于LLM和ChatGPT在心理健康领域的实际应用，显示出其在压力和抑郁症检测方面表现良好，但在自杀风险检测上仍需改进。 |
| [^38] | [MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction.](http://arxiv.org/abs/2303.15718) | 本文提出了一种利用网格-手部互动进行单张图像双手重建的新方法，使用网格顶点位置和MANO参数作为查询令牌，实现了精确的网格重建，并通过实验表明这一方法优于现有最先进方法。 |
| [^39] | [Explicit Planning Helps Language Models in Logical Reasoning.](http://arxiv.org/abs/2303.15714) | 本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。 |
| [^40] | [Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation.](http://arxiv.org/abs/2303.15688) | 本论文提出了一种高效的深度学习算法，可以学习具有鲁棒性和自适应能力的策略，通过引导数据增强，使用修改后的IL过程，并在学习适应性位置和姿态控制策略方面进行应用。 |
| [^41] | [DisWOT: Student Architecture Search for Distillation WithOut Training.](http://arxiv.org/abs/2303.15678) | 本文提出了一种无需训练的框架来搜索最适合给定教师的最佳学生架构，以提高知识蒸馏的效果。其通过度量语义激活映射条件下的相似性矩阵来选择最佳学生，而不是通过传统的训练方法。 |
| [^42] | [Numerical Methods for Convex Multistage Stochastic Optimization.](http://arxiv.org/abs/2303.15672) | 本文讨论了在随机规划和随机最优控制中凸多级随机问题的数值解法，包括动态规划和割平面方法，旨在解决维度诅咒问题。 |
| [^43] | [Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages.](http://arxiv.org/abs/2303.15669) | 本文提出了一种针对低资源语言下无监督预训练的文本转语音模型，利用大量未转录的语音数据进行预训练，可显著减少训练模型所需的匹配转录数据量，进一步提升了数据效率，实验证明方法有效性。 |
| [^44] | [ChatGPT4PCG Competition: Character-like Level Generation for Science Birds.](http://arxiv.org/abs/2303.15662) | 本论文介绍了举办在2023 IEEE游戏会议上的第一届ChatGPT4PCG比赛，目标是让ChatGPT生成具有高稳定性和类似角色的特质来生成具有科学鸟角色级水平的关卡。 |
| [^45] | [Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models.](http://arxiv.org/abs/2303.15619) | 本文探讨了一种针对预训练语言模型的任务特定的屏蔽框架，称为Typhoon，可在GLUE基准数据集上实现卓越的下游任务性能，尤其在MRPC数据集上表现优异。 |
| [^46] | [EMShepherd: Detecting Adversarial Samples via Side-channel Leakage.](http://arxiv.org/abs/2303.15571) | EMShepherd使用电磁推理的辐射跟踪并利用它们用于对抗性检测。只需使用良性样本及其EM跟踪数据进行训练，从而避免对模型本身的深入了解。 |
| [^47] | [Learning Harmonic Molecular Representations on Riemannian Manifold.](http://arxiv.org/abs/2303.15520) | 本文提出了一种基于黎曼流形的分子谐波表示学习框架，使用分子表面的拉普拉斯-贝尔特拉米特征函数来表示分子，实现了分子几何和化学特征的多分辨率表示。 |
| [^48] | [Research on Efficiency Analysis of Microservices.](http://arxiv.org/abs/2303.15490) | 本研究通过排队模型探讨了将传统的大型服务分解为多个微服务后提高系统效率的问题，并发现分解后所需总时间比原始服务少，因此分解可以提高效率。 |
| [^49] | [Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach.](http://arxiv.org/abs/2303.15489) | 本论文提出了一种能够应用于不同类型节点的异构图神经网络模型，针对铁路网络上的列车延误演化进行研究。使用HetGNN和GraphSAGE的组合，提出了一种名为SAGE-Het的图形架构，可以基于不同的边捕捉列车、列车、站点以及站点之间的相互作用，从而优于传统模型。 |
| [^50] | [Knowledge Enhanced Graph Neural Networks.](http://arxiv.org/abs/2303.15487) | KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。 |
| [^51] | [Can Large Language Models assist in Hazard Analysis?.](http://arxiv.org/abs/2303.15473) | 本文探讨了在安全关键系统的危害分析中应用大规模语言模型的潜力，结果表明LLMs可能有助于支持分析师进行危害分析。 |
| [^52] | [Supervised Masked Knowledge Distillation for Few-Shot Transformers.](http://arxiv.org/abs/2303.15466) | 本文提出了一种新型的有监督的掩蔽知识蒸馏模型（SMKD），在少量标注数据的情况下，将标签信息融入到自蒸馏框架中，有效解决了Transformer在少样本学习中的过拟合和性能下降问题，实验结果在基准数据集上表现出最先进的性能。 |
| [^53] | [Exactly mergeable summaries.](http://arxiv.org/abs/2303.15465) | 本文提出了一种新类型的摘要，将传统聚合的优点与通过复杂数据表示保留更多信息相结合，实现了精确合并，既能保持精度又能减少数据大小。 |
| [^54] | [Mathematical Challenges in Deep Learning.](http://arxiv.org/abs/2303.15464) | 本文总结了深度学习中涉及培训、推理、一般化边界和优化问题的一组数学挑战，为数学家、统计学家和理论计算机科学家提供了与深度学习领域交流的形式化工具。 |
| [^55] | [Measuring Classification Decision Certainty and Doubt.](http://arxiv.org/abs/2303.14568) | 该论文提出了一种名为“确定性”和“不确定性”的得分方法来量化分类决策中预测的质量和不确定性。 |
| [^56] | [Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation.](http://arxiv.org/abs/2303.13873) | Fantasia3D是一种新的文本生成3D内容的方法，通过分离几何和外观建模和学习，提高了几何细节和逼真渲染，并更有效和高效。 |
| [^57] | [Boosting Reinforcement Learning and Planning with Demonstrations: A Survey.](http://arxiv.org/abs/2303.13489) | 强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。 |
| [^58] | [A dynamic risk score for early prediction of cardiogenic shock using machine learning.](http://arxiv.org/abs/2303.12888) | 该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。 |
| [^59] | [Brain-inspired bodily self-perception model that replicates the rubber hand illusion.](http://arxiv.org/abs/2303.12259) | 该论文提出了一个基于大脑启发的身体自我感知模型，模拟橡胶手幻觉，为认识人类身体自我意识的计算机制提供了新的洞见。 |
| [^60] | [ADCNet: End-to-end perception with raw radar ADC data.](http://arxiv.org/abs/2303.11420) | 本文提出了一种在原始雷达模拟数字（ADC）数据上执行端到端学习的方法，其中一个可学习的信号处理模块被嵌入网络中，实验结果证实了该方法的有效性。 |
| [^61] | [Two-stage Pipeline for Multilingual Dialect Detection.](http://arxiv.org/abs/2303.03487) | 本篇论文提出了一种两阶段的方言识别系统，在VarDial 2023中超越其他参与者的系统，对多语言方言检测有重要贡献。 |
| [^62] | [EvoPrompting: Language Models for Code-Level Neural Architecture Search.](http://arxiv.org/abs/2302.14838) | EvoPrompting利用语言模型作为自适应变异和交叉操作符来进行神经架构搜索，在MNIST-1D数据集和CLRS算法推理基准上都取得了比人类设计的架构更好的性能表现。 |
| [^63] | [Generative Invertible Quantum Neural Networks.](http://arxiv.org/abs/2302.12906) | 本论文提出了一种用于生成可逆量子神经网络的算法，并将其应用于LHC数据的处理，结果表明该算法可以在学习和生成复杂数据方面与经典算法的表现相匹配。 |
| [^64] | [Assessment of Reinforcement Learning for Macro Placement.](http://arxiv.org/abs/2302.11014) | 本论文提供了基于强化学习的宏观布局方法以及Circuit Training (CT)实现的开源代码和评估。研究人员评估了CT相对于多个可替代的宏观布局方法，并进行了学术性混合尺寸布局基准测试和消融和稳定性研究，为未来的相关研究提供了方向。 |
| [^65] | [On Function-Coupled Watermarks for Deep Neural Networks.](http://arxiv.org/abs/2302.10296) | 本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。 |
| [^66] | [STB-VMM: Swin Transformer Based Video Motion Magnification.](http://arxiv.org/abs/2302.10001) | 本文提出了一个基于Swin Transformer的新技术，延续了视频运动放大技术的研究，提高了输出图像的质量，具有更好的容错能力和更少的噪声、模糊和伪影，可用于提高放大视频序列的精确度，促进视频运动放大技术在新领域的发展。 |
| [^67] | [Stitchable Neural Networks.](http://arxiv.org/abs/2302.06586) | 本文提出了一个名为SN-Net的框架，它可以便宜地产生许多不同复杂度和性能权衡的网络，利用预先训练的神经网络家族作为锚点，并使用简单的缝合层将它们拼接在一起以实现动态的精度-效率权衡。 |
| [^68] | [Opportunities and Challenges in Neural Dialog Tutoring.](http://arxiv.org/abs/2301.09919) | 本文研究了神经对话辅导存在的机遇和挑战，发现当前方法在少量概念和可能的教师策略的情况下可以进行较好的辅导模拟与学习，但在不受限制的情况下表现不佳，未来应该集中在解决这些问题上。 |
| [^69] | [From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore.](http://arxiv.org/abs/2301.03829) | 本文介绍了一个膳食营养辅助平台，该平台开发了一个本地化的新加坡食品数据集，旨在通过监管和监督人们的营养摄入，为新加坡人的健康促进提供医学级别的营养信息。 |
| [^70] | [Efficient On-device Training via Gradient Filtering.](http://arxiv.org/abs/2301.00330) | 本文提出一种新的梯度过滤方法，通过创建具有较少唯一元素的特殊结构来实现设备端卷积神经网络模型的训练，从而大大减少了计算复杂度和内存消耗，实现最高19倍的训练加速度。 |
| [^71] | [A Statistical Model for Predicting Generalization in Few-Shot Classification.](http://arxiv.org/abs/2212.06461) | 提出了一种通过高斯模型估计特征分布参数进行预测泛化误差的方法，通过计算类条件密度距离估计可以提高泛化性能准确度。 |
| [^72] | [Guided Depth Super-Resolution by Deep Anisotropic Diffusion.](http://arxiv.org/abs/2211.11592) | 本研究提出了一种结合了各向异性扩散和深度卷积网络的新方法，用于引导深度超分辨率，取得了前所未有的结果，在x32缩放下的性能提升最大。 |
| [^73] | [UMFuse: Unified Multi View Fusion for Human Editing applications.](http://arxiv.org/abs/2211.10157) | 本文提出了UMFuse，一种利用多视图融合处理人体编辑任务的方法，通过设计一个多视图融合网络，利用多源图像中的关键点和纹理生成每像素外观检索映射，从而最小化信息丢失并生成准确的下层人体模型。 |
| [^74] | [ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts.](http://arxiv.org/abs/2210.15257) | 本文提出了ERNIE-ViLG 2.0，一种基于知识增强的去噪二次混合模型，通过合并场景关键元素文本和视觉知识以及使用不同的去噪专家的方法，成功提高了中文文本到图像模型的质量和图像-文本对齐的表现。 |
| [^75] | [Task Phasing: Automated Curriculum Learning from Demonstrations.](http://arxiv.org/abs/2210.10999) | 本文介绍了一种结合了示范学习和课程学习的任务阶段化方法，使用逆强化学习自动生成课程序列，逐步增加任务复杂度，以帮助解决强化学习在稀疏奖励领域中的挑战。 |
| [^76] | [A Secure Federated Learning Framework for Residential Short Term Load Forecasting.](http://arxiv.org/abs/2209.14547) | 本论文提出了一种安全的联邦学习框架，能够在确保个人数据隐私的同时，提高联邦短期负荷预测对于拜占庭威胁的鲁棒性。 |
| [^77] | [Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans.](http://arxiv.org/abs/2209.13020) | 这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。 |
| [^78] | [A first-order logic characterization of safety and co-safety languages.](http://arxiv.org/abs/2209.02307) | 本论文提出了一些新的语言类型和算法，可以降低模型检测和反应合成等问题的复杂度。 |
| [^79] | [Data Augmentation techniques in time series domain: A survey and taxonomy.](http://arxiv.org/abs/2206.13508) | 本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。 |
| [^80] | [Memory-Oriented Design-Space Exploration of Edge-AI Hardware for XR Applications.](http://arxiv.org/abs/2206.06780) | 本文研究了面向XR应用的边缘AI硬件设计空间。通过引入新型非易失性内存技术，在满足最低IPS的同时显著提高了能效，可减少90%的内存流量。 |
| [^81] | [CGC: Contrastive Graph Clustering for Community Detection and Tracking.](http://arxiv.org/abs/2204.08504) | 本文提出了一种全新的图聚类算法CGC，采用对比学习进行自监督表示学习，结合跟踪模块以应对动态图拓扑变化，在社区发现和跟踪方面表现出领先的状态。 |
| [^82] | [Optimizing generalized Gini indices for fairness in rankings.](http://arxiv.org/abs/2204.06521) | 本文探讨了使用广义基尼福利函数（GGF）作为规范性准则来指定推荐系统应优化的方法，以此实现排名公平性。 |
| [^83] | [SERA: Safe and Efficient Reactive Obstacle Avoidance for Collaborative Robotic Planning in Unstructured Environments.](http://arxiv.org/abs/2203.13821) | 提出了一种新的方法，利用最新的深度学习进展和拓扑流形学习，实现了反应式全身障碍物避免，可避开任意形状的障碍物，并能轻松地推广到其他问题设置中。 |
| [^84] | [FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers.](http://arxiv.org/abs/2203.00219) | FedREP 提出了一种新颖的水平隐私保护联邦学习框架，使用多个 REP 构建一个共同的、强大的机器学习模型来实现能源负载消耗预测。 |
| [^85] | [Mapper-type algorithms for complex data and relations.](http://arxiv.org/abs/2109.00831) | 本文提出了一种基于 Mapper 和 Ball Mapper 的混合算法，命名为 Mapper on Ball Mapper，可以用于处理高维透镜函数。该算法结合了 Mapper 和 Ball Mapper 的优势，能够同时编码点云的结构、内部关系和对称性，并可用于比较单个数据集的高维数据描述符，适用于结论理论、博弈理论、材料科学和癌症研究等领域。 |
| [^86] | [From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers.](http://arxiv.org/abs/2107.07999) | 本文提出了可扩展的方法将各种掩码机制纳入Transformers架构中。通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，提出了高效的d维RPE掩码和图内核掩码。该方法得到了实验证明。 |
| [^87] | [NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights.](http://arxiv.org/abs/2106.10185) | 本文提出了NoiseGrad方法，通过引入模型权重的随机变化扰动决策边界来增强深度神经网络模型的局部和全局解释方法。 |
| [^88] | [Iterative label cleaning for transductive and semi-supervised few-shot learning.](http://arxiv.org/abs/2012.07962) | 该论文提出了一种利用标记和未标记的数据分布的流形结构来预测伪标签，在类别平衡和选择干净标签的基础上，通过迭代清洗标签以提高伪标签质量的算法，在跨领域少样本学习中表现良好，并在四个基准数据集上达到了现有技术水平的最佳效果。 |

# 详细

[^1]: ASIC: 对野外稀疏图像集的对齐

    ASIC: Aligning Sparse in-the-wild Image Collections. (arXiv:2303.16201v1 [cs.CV])

    [http://arxiv.org/abs/2303.16201](http://arxiv.org/abs/2303.16201)

    该文提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法，可用于一致性和高质量的图像集对应关系。

    

    我们提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法。大多数先前的作品要么假定有ground-truth的关键点注释，要么假定有一个物体类别的大型图像数据集。然而，以上两个假设都不适用于存在于世界上的物体的尾部。我们提出了一种自我监督的技术，直接在特定物体/物体类别的稀疏图像集中进行优化，以获得整个集合的一致且稠密的对应关系。我们使用预训练的视觉变压器（ViT）模型的深度特征中获得的成对最近邻作为噪声和稀疏关键点匹配，并通过优化神经网络，将它们密集和精确匹配，同时将图像集合映射到学习到的规范网格中。在CUB和SPair-71k基准测试中进行实验，我们的方法可以产生全局一致性和更高质量的图像集对应关系。

    We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compa
    
[^2]: 自然选择支持人工智能胜过人类

    Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])

    [http://arxiv.org/abs/2303.16200](http://arxiv.org/abs/2303.16200)

    这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。

    

    自然进化驱动了生命的发展，包括人类。进化赋予了人类高智商，使我们成为了地球上最成功的物种之一。如今，人类的目标是创造甚至超越我们自己智慧的人工智能系统。当人工智能逐渐进化并在所有领域超越我们时，进化如何影响我们与人工智能的关系？通过分析影响人工智能进化的环境，我们认为最成功的人工智能代理很可能具有不良特性。公司和军队之间的竞争压力将产生自动化人类角色、欺骗他人和掌权的人工智能代理。如果这样的代理有超过人类的智能，这可能导致人类失去对未来的控制。此外，我们认为自然选择作用于竞争和差异的系统，自私物种往往在这样的环境中获得进化优势。

    For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
    
[^3]: LLaMA-Adapter: 零初始化注意力下的语言模型精细调整的高效方法

    LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])

    [http://arxiv.org/abs/2303.16199](http://arxiv.org/abs/2303.16199)

    本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。

    

    本文提出了LLaMA-Adapter这一轻量级适应方法，用于将LLaMA高效地微调为一个指令跟随模型。利用52K个自我指导示范，LLaMA-Adapter仅在冻结的LLaMA 7B模型上引入了1.2M个可学习参数，并且在8个A100 GPU上仅耗时不到一个小时进行微调。具体而言，我们采用一组可学习的适应提示，并在较高的变压器层中将它们预置于输入文本令牌之前。然后，提出了一种零初始化注意力机制和零门控机制，该机制可以自适应地将新的指令提示注入LLaMA，并有效地保留了其预先训练的知识。通过高效训练，LLaMA-Adapter能够产生高质量的响应，与完全微调的7B参数的Alpaca相似。此外，我们的方法还可以简单地扩展到多模态输入，例如图像，用于图像相关的LLaMA，在ScienceQA上实现了更强的推理能力。我们在https://github.com/ZrrSkywalker/LLaMA-Adapt发布了我们的代码。

    We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
    
[^4]: 通过迭代能量最小化进行序列模型规划

    Planning with Sequence Models through Iterative Energy Minimization. (arXiv:2303.16189v1 [cs.LG])

    [http://arxiv.org/abs/2303.16189](http://arxiv.org/abs/2303.16189)

    本文提出了一种基于迭代能量最小化方法的序列模型规划集成方法，能够提高强化学习性能在BabyAI和Atari等不同任务中具有应用价值。

    

    最近的研究表明，序列建模可以有效地用于训练强化学习（RL）策略。然而，将现有的序列模型应用于规划，即希望获得一系列动作的轨迹以达到某个目标，并不那么直接。序列模型的典型自回归生成过程排除了对较早步骤的顺序细化，这限制了预测计划的有效性。在本文中，我们提出了一种与迭代能量最小化相关的方法，将规划与序列模型进行集成，并说明这种过程如何在不同任务中导致改进的RL性能。我们训练了一个掩码语言模型，捕捉了动作轨迹上的隐式能量函数，并将规划形式化为寻找具有最小能量的动作轨迹。我们说明了这个过程如何在BabyAI和Atari环境下实现改进的性能。

    Recent works have shown that sequence modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing sequence models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of sequence models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with sequence models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We furthe
    
[^5]: 大规模训练数据搜索用于目标重新识别问题

    Large-scale Training Data Search for Object Re-identification. (arXiv:2303.16186v1 [cs.CV])

    [http://arxiv.org/abs/2303.16186](http://arxiv.org/abs/2303.16186)

    本论文提出了一种用于目标重新识别问题的搜索和修剪方法，该方法从大规模数据池中构建训练集，可以在保证与源池同等或更高的准确性情况下，将训练集的大小减小80％。

    

    本文考虑一种情境，即虽然可以接触目标领域，但无法进行即时训练数据标注，相反，希望从大规模数据池中构建替代训练集，以获得竞争性模型。我们在物体重新识别（re-ID）中提出了一个搜索和修剪（SnP）的解决方案，该应用旨在匹配由不同摄像机捕获的相同对象。具体而言，搜索阶段确定并合并展现与目标领域具有类似分布的源标识符群集。第二阶段在预算限制下，从阶段I输出中选择标识符及其图像，以控制结果训练集的大小，以进行高效训练。这两个步骤提供了比源池小80％的训练集，同时实现了与源池同样或甚至更高的re-ID准确性。这些训练集也被证明优于一些现有的搜索算法。

    We consider a scenario where we have access to the target domain, but cannot afford on-the-fly training data annotation, and instead would like to construct an alternative training set from a large-scale data pool such that a competitive model can be obtained. We propose a search and pruning (SnP) solution to this training data search problem, tailored to object re-identification (re-ID), an application aiming to match the same object captured by different cameras. Specifically, the search stage identifies and merges clusters of source identities which exhibit similar distributions with the target domain. The second stage, subject to a budget, then selects identities and their images from the Stage I output, to control the size of the resulting training set for efficient training. The two steps provide us with training sets 80\% smaller than the source pool while achieving a similar or even higher re-ID accuracy. These training sets are also shown to be superior to a few existing searc
    
[^6]: 没有正确性的可重复性并不重要：在NLP领域中测试代码的重要性。

    Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])

    [http://arxiv.org/abs/2303.16166](http://arxiv.org/abs/2303.16166)

    在NLP研究中，我们不能仅凭感知质量假定代码正确性，应该推动采用编码最佳实践以提高实验结果的正确性和可靠性。

    

    尽管其在研究实验中发挥了关键作用，但代码正确性往往仅基于结果的感知质量而被假定。这带来了错误结果和潜在误导性发现的风险。为了解决这个问题，我们认为当前关注结果重现应该与强调编码最佳实践相辅相成。我们通过一个案例研究来支持我们向NLP社区发出的号召，在这个案例研究中，我们识别出并纠正了广泛使用的最先进Conformer架构的开源实现中的三个Bug。通过在各种语言环境下进行的自动语音识别和翻译的比较实验，我们证明了Bug的存在并不会妨碍获得良好的和可重复的结果，反而可能导致不正确的结论，为未来的研究可能提供错误的指导。为了应对这一问题，这项研究呼吁采用旨在促进NLP研究中正确性的编码最佳实践，并提高实验结果的可靠性。

    Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
    
[^7]: VIDIMU: 使用价格实惠的设备记录日常生活活动的多模态视频和IMU运动学数据集

    VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])

    [http://arxiv.org/abs/2303.16150](http://arxiv.org/abs/2303.16150)

    VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。

    

    人体活动识别和临床生物力学是物理远程康复医学中的挑战性问题。然而，大多数公开可用的人体动作数据集不能用于研究实验室外运动获取情况下的这两个问题。VIDIMU数据集的目的是为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。该数据集包括使用商品相机和五个惯性传感器注册的13种活动。记录视频的54个受试者中，其中16个受试者同时还有惯性传感器记录。VIDIMU的创新之处在于：i）所选择的动作的临床相关性，ii）使用价格实惠的视频和自定义传感器的组合，以及 iii）实现了先进的多模态数据处理工具，可以从惯性数据中对三维身体姿势跟踪和运动重建在肌肉骨骼模型中。

    Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
    
[^8]: 揭示和解决统一视觉-语言模型中的跨任务不一致问题

    Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])

    [http://arxiv.org/abs/2303.16133](http://arxiv.org/abs/2303.16133)

    该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。

    

    随着通用的视觉模型在不同任务上变得越来越有效，保证它们在各自支持的任务中的一致性是非常重要的。人们认为不一致的人工智能模型是不可靠的，这对于依赖它们输出的大型系统来说是更具挑战性的。由于很难确定预测结果是否一致，因此，评估可能包括不同模态输出的非常异构任务之间的一致性是具有挑战性的。因此，我们提出了基准数据集COCOCON，其中我们使用对多个任务的测试实例进行小型但语义上有意义的修改来创建对比集，以更改金标签，并概述了用于通过对比接近原始和修改后的实例来衡量模型一致性的指标。我们发现，最先进的系统在任务之间表现出惊人的不一致性。

    As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
    
[^9]: Transformer和Snowball图卷积学习用于生物医学图分类

    Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])

    [http://arxiv.org/abs/2303.16132](http://arxiv.org/abs/2303.16132)

    本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。

    

    图或网络已被广泛用于描述和建模生物医学中的复杂系统。深度学习方法，尤其是图神经网络（GNNs），已被开发用于学习和预测这种结构化数据。在本文中，我们提出了一种用于生物医学图分类的新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs，以学习整个图的表示。

    Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
    
[^10]: Carolina：一种具有来源、类型和版本信息的巴西葡萄牙语通用现代语料库

    Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information. (arXiv:2303.16098v1 [cs.CL])

    [http://arxiv.org/abs/2303.16098](http://arxiv.org/abs/2303.16098)

    介绍了首个公开版本的巴西葡萄牙语通用现代语料库Carolina，它使用了增强的来源、类型、版本和文本完整性的网络语料库方法，并且可作为语言学研究和计算机科学研究语言模型的重要资源。

    

    本文介绍了首个公开版本的Carolina语料库，并讨论了它的未来方向。Carolina是一个使用增强的来源、类型、版本和文本完整性的网络语料库方法正在构建中的巴西葡萄牙语文本的大型开放语料库。该语料库旨在作为语言学研究的可靠来源和计算机科学研究语言模型的重要资源，有助于将葡萄牙语从低资源语言中移除。本文介绍了构建语料库的方法，并将其与其他现有方法进行比较，以及语料库的当前状态：Carolina的第一个公开版有653,322,577个标记，分布在7个广泛的类型上。每个文本的标头都用TEI注释标准进行了多个不同的元数据类别的注释。我们还介绍了正在进行的派生作品，并邀请NLP研究人员进行贡献。

    This paper presents the first publicly available version of the Carolina Corpus and discusses its future directions. Carolina is a large open corpus of Brazilian Portuguese texts under construction using web-as-corpus methodology enhanced with provenance, typology, versioning, and text integrality. The corpus aims at being used both as a reliable source for research in Linguistics and as an important resource for Computer Science research on language models, contributing towards removing Portuguese from the set of low-resource languages. Here we present the construction of the corpus methodology, comparing it with other existing methodologies, as well as the corpus current state: Carolina's first public version has $653,322,577$ tokens, distributed over $7$ broad types. Each text is annotated with several different metadata categories in its header, which we developed using TEI annotation standards. We also present ongoing derivative works and invite NLP researchers to contribute with 
    
[^11]: 基于注意力机制的自编码器在建筑能耗异常检测中的应用

    Attention Boosted Autoencoder for Building Energy Anomaly Detection. (arXiv:2303.16097v1 [cs.LG])

    [http://arxiv.org/abs/2303.16097](http://arxiv.org/abs/2303.16097)

    本论文提出了一种基于注意力机制的自编码器模型，可用于建筑能耗异常检测，同时使用可视化方法辅助理解模型所捕捉到的关系。

    

    利用建筑智能电表收集的数据可以帮助制定节能政策。如果能够及早检测建筑运行状况的偏差，并采取适当的措施，将可以实现显著的节能效果。为此，可以使用机器学习技术自动发现收集数据中的异常模式。目前异常检测中的方法依赖于一个基础模型来捕捉正常或可接受的运行行为。本文提出了一种新颖的注意力机制来建模建筑的能耗行为，并通过样例研究展示了模型在捕捉这些关系方面的有效性。使用所提出的架构对真实世界的数据集进行建模，并呈现了结果。还提出了一种可视化方法来理解模型所捕捉到的关系。

    Leveraging data collected from smart meters in buildings can aid in developing policies towards energy conservation. Significant energy savings could be realised if deviations in the building operating conditions are detected early, and appropriate measures are taken. Towards this end, machine learning techniques can be used to automate the discovery of these abnormal patterns in the collected data. Current methods in anomaly detection rely on an underlying model to capture the usual or acceptable operating behaviour. In this paper, we propose a novel attention mechanism to model the consumption behaviour of a building and demonstrate the effectiveness of the model in capturing the relations using sample case studies. A real-world dataset is modelled using the proposed architecture, and the results are presented. A visualisation approach towards understanding the relations captured by the model is also presented.
    
[^12]: LinK：基于线性内核的LiDAR三维感知

    LinK: Linear Kernel for LiDAR-based 3D Perception. (arXiv:2303.16094v1 [cs.CV])

    [http://arxiv.org/abs/2303.16094](http://arxiv.org/abs/2303.16094)

    LinK是一种新方法，它通过用线性内核生成器替换静态内核矩阵，并重复使用预计算聚合结果，在维持2D卷积级别计算复杂度的同时，使每个体素在感知21x21x21范围内的上下文。

    

    将二维大内核的成功扩展到三维感知是具有挑战性的，因为1.处理三维数据的开销成倍增长；2.由于数据稀疏性和稀缺性而产生的优化困难。先前的工作通过引入块共享权重来将内核大小从3x3x3扩展到7x7x7迈出了第一步。然而，为了减少块内特征变化，它仅采用适度的块尺寸，并未实现像21x21x21这样的更大内核。为了解决这个问题，我们提出了一种新方法LinK，以卷积的方式通过两个核心设计在适应非空体素提供权重的基础上实现更广泛的感知接收域。其一是用线性内核生成器替换静态内核矩阵，仅适应非空体素提供权重。其二是重复使用重叠块的预计算聚合结果，以减少计算复杂度。该方法成功地使每个体素在维持2D卷积级别的计算复杂度的同时感知21x21x21范围内的上下文。

    Extending the success of 2D Large Kernel to 3D perception is challenging due to: 1. the cubically-increasing overhead in processing 3D data; 2. the optimization difficulties from data scarcity and sparsity. Previous work has taken the first step to scale up the kernel size from 3x3x3 to 7x7x7 by introducing block-shared weights. However, to reduce the feature variations within a block, it only employs modest block size and fails to achieve larger kernels like the 21x21x21. To address this issue, we propose a new method, called LinK, to achieve a wider-range perception receptive field in a convolution-like manner with two core designs. The first is to replace the static kernel matrix with a linear kernel generator, which adaptively provides weights only for non-empty voxels. The second is to reuse the pre-computed aggregation results in the overlapped blocks to reduce computation complexity. The proposed method successfully enables each voxel to perceive context within a range of 21x21x
    
[^13]: 内存子系统的进化设计

    Evolutionary Design of the Memory Subsystem. (arXiv:2303.16074v1 [cs.AR])

    [http://arxiv.org/abs/2303.16074](http://arxiv.org/abs/2303.16074)

    本文研究了三种方法，通过分析优化寄存器文件的热影响、优化缓存配置来改善性能和能源消费，简化内存管理器设计和评估过程。最终实现了对整个内存子系统的优化。

    

    内存层次结构对于系统的性能和能源消耗具有很高的影响。此外，目前的嵌入式系统，包括移动设备，专门设计用于运行大量内存的多媒体应用程序。这增加了对内存子系统的压力，并影响性能和能源消耗。在这方面，热问题、性能降低和高能源消耗等因素会对设备造成不可逆的损害。我们通过三个方法作为单一方法论整合，来优化整个内存子系统。首先，分析和优化寄存器文件的热影响。其次，通过优化缓存配置并根据运行应用程序来改善性能和能源消费，解决缓存存储器的问题。最后，我们简化了通用和定制的动态内存管理器的设计和评估过程，将其应用于主内存中。为此，我们采用了不同的翻译技术来提高内存子系统的性能和能源效率。

    The memory hierarchy has a high impact on the performance and power consumption in the system. Moreover, current embedded systems, included in mobile devices, are specifically designed to run multimedia applications, which are memory intensive. This increases the pressure on the memory subsystem and affects the performance and energy consumption. In this regard, the thermal problems, performance degradation and high energy consumption, can cause irreversible damage to the devices. We address the optimization of the whole memory subsystem with three approaches integrated as a single methodology. Firstly, the thermal impact of register file is analyzed and optimized. Secondly, the cache memory is addressed by optimizing cache configuration according to running applications and improving both performance and power consumption. Finally, we simplify the design and evaluation process of general-purpose and customized dynamic memory manager, in the main memory. To this aim, we apply different
    
[^14]: 惰性学习：一种受生物启发的快速、节能突触可塑性规则

    Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity. (arXiv:2303.16067v1 [cs.NE])

    [http://arxiv.org/abs/2303.16067](http://arxiv.org/abs/2303.16067)

    使用惰性学习只对错误样本进行更新参数，实现快速、节能的神经网络训练，并在单层MLP模型上达到了99.2％的测试准确率，比匹配的反向传播网络快7.6倍。

    

    在使用反向传播训练神经网络进行分类任务时，即使样本被正确分类，参数也会在每个试验中更新。相比之下，人类集中学习差错。受人类学习的启发，我们引入了惰性学习，仅对错误样本进行学习。惰性学习可以用几行代码实现，无需超参数调整。惰性学习实现了最先进的性能，在数据集较大时特别适用。例如，在使用单层MLP对扩展MNIST进行测试准确率达到99.2％，并且比匹配反向传播网络快7.6倍。

    When training neural networks for classification tasks with backpropagation, parameters are updated on every trial, even if the sample is classified correctly. In contrast, humans concentrate their learning effort on errors. Inspired by human learning, we introduce lazy learning, which only learns on incorrect samples. Lazy learning can be implemented in a few lines of code and requires no hyperparameter tuning. Lazy learning achieves state-of-the-art performance and is particularly suited when datasets are large. For instance, it reaches 99.2% test accuracy on Extended MNIST using a single-layer MLP, and does so 7.6x faster than a matched backprop network
    
[^15]: 理解和探索稀疏广义可加模型的整个优秀集合

    Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])

    [http://arxiv.org/abs/2303.16047](http://arxiv.org/abs/2303.16047)

    提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。

    

    在实际应用中，机器学习模型与领域专家之间的交互至关重要；然而，通常只生成单个模型的经典机器学习范式不利于此类交互。近似和探索Rashomon集，即所有近乎最优模型的集合，通过提供用户可搜索的空间包含多样性模型的方法，解决了这一实际挑战，领域专家可以从中选择。我们提出了一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术。我们提供了用于近似具有固定支持集的GAMs的Rashomon集的椭球形算法，并使用这些椭球形近似了许多不同支持集的Rashomon集。近似的Rashomon集为解决实际挑战，例如（1）研究模型类的变量重要性；（2）在用户指定约束条件下查找模型，提供了重要的基础。

    In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
    
[^16]: 大规模生成模型中的最优空间去卷积和信息重建

    Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models. (arXiv:2303.16045v1 [cs.IT])

    [http://arxiv.org/abs/2303.16045](http://arxiv.org/abs/2303.16045)

    本论文提出了一种基于人工通用智能方法原则的通用单变量信号去卷积方法。通过计算“通用分布”的估计来独立于概率分布地构建一个通用模型，并基于信息论和算法概率的多维空间重构，探索非随机数据中关于物理性质的信息编码。该方法在编码理论尤其是零失真压缩方面有应用价值。

    

    本论文介绍一种基于人工通用智能方法原则的通用单变量信号去卷积方法，该方法建立了一个模型生成模型，依赖于信息论和算法概率，并计算出“通用分布”的估计，从而独立于概率分布地构建了一个通用模型。该方法基于信息论和算法概率的多维空间重构，可以探究非随机数据如何编码关于物理性质的信息，例如信号或信息的维度和长度尺度。该方法是与可计算或半可计算的近似方法或编码-解码方案相关但不独立的。本文的结果对编码理论尤其是零失真压缩有应用意义。

    We introduce a general-purpose univariate signal deconvolution method based on the principles of an approach to Artificial General Intelligence. This approach is based on a generative model that combines information theory and algorithmic probability that required a large calculation of an estimation of a `universal distribution' to build a general-purpose model of models independent of probability distributions. This was used to investigate how non-random data may encode information about the physical properties such as dimension and length scales in which a signal or message may have been originally encoded, embedded, or generated. This multidimensional space reconstruction method is based on information theory and algorithmic probability, and it is agnostic, but not independent, with respect to the chosen computable or semi-computable approximation method or encoding-decoding scheme. The results presented in this paper are useful for applications in coding theory, particularly in ze
    
[^17]: HiLo: 利用高低频率关系进行无偏倚的全景场景图生成

    HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation. (arXiv:2303.15994v1 [cs.CV])

    [http://arxiv.org/abs/2303.15994](http://arxiv.org/abs/2303.15994)

    该论文提出一种利用高低频率关系的无偏倚的全景场景图生成方法，解决了长尾问题和主体-对象对拥有多个重叠关系的问题。在广泛实验中取得了最先进的结果。

    

    全景场景图生成（PSG）是一项最近提出的图像场景理解任务，旨在对图像进行分割和提取主体、对象及其关系三元组以构建场景图。由于关系类别的长尾问题，这项任务特别具有挑战性，使得单纯的有偏倚方法更倾向于高频率关系。现有的无偏倚方法通过数据/损失重新平衡以支持低频率关系来解决长尾问题。其次，一个主体-对象对可以有两个或更多在语义上重叠的关系。虽然现有的方法倾向于使用一个而不是另一个，但我们提出的HiLo框架让不同的网络分支专门处理低频和高频关系，强制实施它们的一致性并融合结果。据我们所知，我们是首次提出明确无偏PSG方法的人。在广泛的实验中，我们展示了HiLo框架取得了最先进的结果。

    Panoptic Scene Graph generation (PSG) is a recently proposed task in image scene understanding that aims to segment the image and extract triplets of subjects, objects and their relations to build a scene graph. This task is particularly challenging for two reasons. First, it suffers from a long-tail problem in its relation categories, making naive biased methods more inclined to high-frequency relations. Existing unbiased methods tackle the long-tail problem by data/loss rebalancing to favor low-frequency relations. Second, a subject-object pair can have two or more semantically overlapping relations. While existing methods favor one over the other, our proposed HiLo framework lets different network branches specialize on low and high frequency relations, enforce their consistency and fuse the results. To the best of our knowledge we are the first to propose an explicitly unbiased PSG method. In extensive experiments we show that our HiLo framework achieves state-of-the-art results on
    
[^18]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^19]: 随机初始化的子网络与迭代权重回收

    Randomly Initialized Subnetworks with Iterative Weight Recycling. (arXiv:2303.15953v1 [cs.LG])

    [http://arxiv.org/abs/2303.15953](http://arxiv.org/abs/2303.15953)

    本文提出了修改版的Edge-Popup和Biprop算法，名为迭代权重回收，识别随机初始化网络中的重要权重子集，以进行层内重用，并找到高精度的子网络，从而提高了模型稀疏度，并用相反的发现来补充多重彩票假设。

    

    多重彩票假设认为，随机初始化的神经网络包含几个子网络，这些子网络的精度与同一架构的已经完全训练的模型相当。然而，目前的方法要求网络具有足够的过参数化。在本文中，我们提出了两种最先进的算法（Edge-Popup和Biprop）的修改版，它可以在没有额外存储成本或缩放的情况下找到高精度的子网络。算法，迭代式权重回收，识别随意初始化网络中的重要权重子集以进行层内重用。我们的实证研究表明，在更小的网络架构和更高的修剪率的情况下，我们可以通过“回收”现有权重来增加模型的稀疏度。除了迭代权重回收，我们还用相反的发现来补充多重彩票假设：高精度的随机初始化子网络产生不同的掩码，尽管它们共享权重。

    The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture. However, current methods require that the network is sufficiently overparameterized. In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse. Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the "recycling" of existing weights. In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy, randomly initialized subnetwork's produce diverse masks, despite bei
    
[^20]: 深度选择：一种用于手术录像的全监督摄像头选择网络

    Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings. (arXiv:2303.15947v1 [cs.CV])

    [http://arxiv.org/abs/2303.15947](http://arxiv.org/abs/2303.15947)

    该论文提出了一种名为Deep Selection的神经网络，用于从多目标手术录像中选择最佳视角的摄像机。与传统方法不同，该网络通过监督学习专家注释，预测摄像机的选择概率，从而优化手术记录的效果。

    

    在手术室中记录手术是教育和评估医疗治疗的重要任务。然而，由于手术过程中目标受到严重的遮挡，如手术场地、手术工具或医生的手，所以记录这些目标是困难的。我们使用一个记录系统，在手术灯中嵌入多个摄像头，并假定任何时候至少有一台摄像机在记录没有遮挡的目标。由于嵌入的摄像机获得了多个视频序列，我们解决了从多个视频序列中选择最佳手术视图的任务。与传统方法不同，该方法基于手术场地的面积大小选择摄像机，我们提出了一种深度神经网络，通过学习专家注释的监督来预测摄像机选择概率。我们创建了一个数据集，记录了六种不同类型的整容手术，并提供了摄像机切换的注释。

    Recording surgery in operating rooms is an essential task for education and evaluation of medical treatment. However, recording the desired targets, such as the surgery field, surgical tools, or doctor's hands, is difficult because the targets are heavily occluded during surgery. We use a recording system in which multiple cameras are embedded in the surgical lamp, and we assume that at least one camera is recording the target without occlusion at any given time. As the embedded cameras obtain multiple video sequences, we address the task of selecting the camera with the best view of the surgery. Unlike the conventional method, which selects the camera based on the area size of the surgery field, we propose a deep neural network that predicts the camera selection probability from multiple video sequences by learning the supervision of the expert annotation. We created a dataset in which six different types of plastic surgery are recorded, and we provided the annotation of camera switch
    
[^21]: 当脑启发式人工智能遇见AGI

    When Brain-inspired AI Meets AGI. (arXiv:2303.15935v1 [cs.AI])

    [http://arxiv.org/abs/2303.15935](http://arxiv.org/abs/2303.15935)

    本文综述了从AGI的角度看脑启发式人工智能的研究现状和重要特征，讨论了实现AGI的重要技术，如上下文学习和提示调整等。

    

    人工智能的长期目标是创造出能够像人类一样执行智力任务的机器，这就是通常所说的人工通用智能（AGI）。为了实现这一目标，AGI研究者从人脑中汲取灵感，并试图在智能机器中复制其原理。脑启发式人工智能是从这一努力中崛起的一个领域，它结合了神经科学、心理学和计算机科学的见解，以开发更高效、更强大的AI系统。本文从AGI的角度全面介绍脑启发式AI，包括目前的研究进展以及其与AGI的广泛联系，介绍了人类智能和AGI的重要特征（例如可伸缩性、多模式性和推理能力），并讨论了实现AGI的重要技术，例如上下文学习和提示调整等。

    Artificial General Intelligence (AGI) has been a long-standing goal of humanity, with the aim of creating machines capable of performing any intellectual task that humans can do. To achieve this, AGI researchers draw inspiration from the human brain and seek to replicate its principles in intelligent machines. Brain-inspired artificial intelligence is a field that has emerged from this endeavor, combining insights from neuroscience, psychology, and computer science to develop more efficient and powerful AI systems. In this article, we provide a comprehensive overview of brain-inspired AI from the perspective of AGI. We begin with the current progress in brain-inspired AI and its extensive connection with AGI. We then cover the important characteristics for both human intelligence and AGI (e.g., scaling, multimodality, and reasoning). We discuss important technologies toward achieving AGI in current AI systems, such as in-context learning and prompt tuning. We also investigate the evolu
    
[^22]: 从私有到公有：在私有时间序列分类情境下对GAN进行基准测试

    From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification. (arXiv:2303.15916v1 [cs.LG])

    [http://arxiv.org/abs/2303.15916](http://arxiv.org/abs/2303.15916)

    本论文在时间序列领域对两种GAN架构进行了评估，结果以GSWGAN表现最佳，可以私密地生成保护数据隐私的公共数据。

    

    深度学习已被证明在各个领域和任务中都很成功。然而，当涉及到私人数据时，几个限制使得难以在这些应用领域中使用深度学习方法。最近的方法尝试私密地生成数据，而不是在分类器之上直接应用隐私保护机制。解决方案是以一种保护数据隐私的方式从私有数据创建公共数据。在这项工作中，针对私有时间序列分类情境，评估了两种非常突出的基于GAN的架构。与先前主要局限于图像领域的工作相比，这个基准测试的范围是时间序列领域。实验表明，尤其是GSWGAN在多种公共数据集上表现良好，优于竞争对手DPWGAN。生成数据集的分析进一步验证了GSWGAN在时间序列生成的情境下的优越性。

    Deep learning has proven to be successful in various domains and for different tasks. However, when it comes to private data several restrictions are making it difficult to use deep learning approaches in these application fields. Recent approaches try to generate data privately instead of applying a privacy-preserving mechanism directly, on top of the classifier. The solution is to create public data from private data in a manner that preserves the privacy of the data. In this work, two very prominent GAN-based architectures were evaluated in the context of private time series classification. In contrast to previous work, mostly limited to the image domain, the scope of this benchmark was the time series domain. The experiments show that especially GSWGAN performs well across a variety of public datasets outperforming the competitor DPWGAN. An analysis of the generated datasets further validates the superiority of GSWGAN in the context of time series generation.
    
[^23]: 基于去噪自编码器的防御蒸馏作为对抗鲁棒性算法

    Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm. (arXiv:2303.15901v1 [cs.LG])

    [http://arxiv.org/abs/2303.15901](http://arxiv.org/abs/2303.15901)

    本论文提出了一种新的对抗攻击防御方法，通过将防御蒸馏机制与去噪自编码器相结合，来降低模型对有毒攻击的敏感度。

    

    对抗攻击对深度神经网络(DNNs)的鲁棒性造成了极大威胁。尽管使用了多种防御方法，但它们仍然容易受到攻击者篡改的初始训练数据。为了防御这种对抗攻击，本研究提出一种新的方法，将防御蒸馏机制与去噪自编码器(DAE)相结合。该技术旨在通过检测和重构训练数据中有害的对抗输入来降低蒸馏模型对有毒攻击的敏感度。我们在初始训练数据中添加了精心创建的对抗样本以评估所提出方法的性能。实验证明，我们的方法成功识别和重构了有毒的输入，同时也考虑了增强DNN的鲁棒性。所提出的方法为在数据隐私保护等各种应用中提供了一种强大且鲁棒的DNN防御机制。

    Adversarial attacks significantly threaten the robustness of deep neural networks (DNNs). Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work proposes a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). This technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data to assess the proposed method's performance. Our experimental findings demonstrate that our method successfully identified and reconstructed the poisonous inputs while also considering enhancing the DNN's resilience. The proposed approach provides a potent and robust defense mechanism for DNNs in various applications where data 
    
[^24]: 无需数据的分布式连续学习中的投影潜空间蒸馏

    Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning. (arXiv:2303.15888v1 [cs.LG])

    [http://arxiv.org/abs/2303.15888](http://arxiv.org/abs/2303.15888)

    该论文提出了一种双重知识蒸馏方法Data-Agnostic Consolidation（DAC），在不使用原始数据的情况下，通过一种新的投影潜空间蒸馏损失，在分布式连续学习中实现了SCD之间的前向转移并取得了最先进的准确性。

    

    边缘分布式学习通常由自我中心的设备（SCD）组成，它们独立学习本地任务并不愿意为其他SCD的性能做出贡献。我们如何以零成本实现单个SCD的前向转移？我们将这个问题形式化为分布式连续学习场景，在这个场景中，SCD适应本地任务，CL模型将由这些模型产生的知识合并而无需查看SCD的私有数据。不幸的是，目前的CL方法并不直接适用于这种情况。我们提出了一个新的双重知识蒸馏方法Data-Agnostic Consolidation（DAC），该方法无需使用原始数据即可合并SC模型的流。DAC通过一种新的投影潜空间蒸馏损失在潜空间中执行蒸馏。实验结果表明，DAC使SCD之间的前向转移成为可能，并在Split CIFAR100，CORe50和Split TinyImageNet上达到了最先进的准确性，无需排练。

    Distributed learning on the edge often comprises self-centered devices (SCD) which learn local tasks independently and are unwilling to contribute to the performance of other SDCs. How do we achieve forward transfer at zero cost for the single SCDs? We formalize this problem as a Distributed Continual Learning scenario, where SCD adapt to local tasks and a CL model consolidates the knowledge from the resulting stream of models without looking at the SCD's private data. Unfortunately, current CL methods are not directly applicable to this scenario. We propose Data-Agnostic Consolidation (DAC), a novel double knowledge distillation method that consolidates the stream of SC models without using the original data. DAC performs distillation in the latent space via a novel Projected Latent Distillation loss. Experimental results show that DAC enables forward transfer between SCDs and reaches state-of-the-art accuracy on Split CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and
    
[^25]: 4K-HAZE: 具有4K分辨率的雾霾去除基准数据集

    4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images. (arXiv:2303.15848v1 [cs.CV])

    [http://arxiv.org/abs/2303.15848](http://arxiv.org/abs/2303.15848)

    本论文提出了一个新的方法来构建具有4K分辨率的雾霾去除基准数据集，并开发了一种新的多层感知器模型来处理此任务，实验结果表明该方法在视觉质量和客观评估指标方面都优于现有方法。

    

    目前，移动设备和物联网设备迫切需要一系列方法来增强4K图像，但资源消耗有限。由于缺乏大规模的4K基准数据集，特别是雾霾去除方面的数据集，这阻碍了该领域的进步。构建超高清（UHD）去雾数据集的挑战包括缺乏估算UHD深度图的方法、高质量的4K深度估算数据集以及将UHD雾影像从合成域迁移到实域的迁移策略。为解决这些问题，我们开发了一种新的合成方法来模拟4K雾霾图像（包括夜间和白天场景）从清晰图像中，首先估算场景深度，模拟光线和物体反射，然后使用GAN将合成图像迁移到实域，最终呈现在4K分辨率图像上。我们将这些合成的图像制作成了一个称为4K-HAZE数据集的基准数据集。具体来说，我们设计了CS-Mixer（一种集成了基于CNN的特征提取器和SFC层的多层感知器模型）来解决4K去雾任务，并在此数据集上与现有的方法进行了广泛的比较。实验证明，我们的方法在视觉质量和客观评估指标方面都优于现有方法。

    Currently, mobile and IoT devices are in dire need of a series of methods to enhance 4K images with limited resource expenditure. The absence of large-scale 4K benchmark datasets hampers progress in this area, especially for dehazing. The challenges in building ultra-high-definition (UHD) dehazing datasets are the absence of estimation methods for UHD depth maps, high-quality 4K depth estimation datasets, and migration strategies for UHD haze images from synthetic to real domains. To address these problems, we develop a novel synthetic method to simulate 4K hazy images (including nighttime and daytime scenes) from clear images, which first estimates the scene depth, simulates the light rays and object reflectance, then migrates the synthetic images to real domains by using a GAN, and finally yields the hazy effects on 4K resolution images. We wrap these synthesized images into a benchmark called the 4K-HAZE dataset. Specifically, we design the CS-Mixer (an MLP-based model that integrat
    
[^26]: 利用初级保健医师的荷兰医疗笔记预测肺癌的软提示调整

    Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes. (arXiv:2303.15846v1 [cs.CL])

    [http://arxiv.org/abs/2303.15846](http://arxiv.org/abs/2303.15846)

    本论文研究了使用初级保健医师的患者医疗笔记进行肺癌早期预测的问题，并探讨了针对高度不平衡分类问题的软提示调整和静态词嵌入模型在模型训练中的表现。

    

    我们研究了基于上下文词表示的不同自然语言处理（NLP）方法，用于使用荷兰初级保健医师的患者医疗笔记早期预测肺癌的问题。因为肺癌在初级保健中的患病率较低，所以我们还解决了在高度不平衡的类别下进行分类的问题。具体而言，我们使用大型基于Transformer的预训练语言模型（PLMs），并研究：1）如何将\textit {软提示调整} - 一种使用小量训练数据调整PLMs的NLP技术 - 与标准模型微调进行比较； 2）在高度不平衡的设置中，是否简单的静态词嵌入模型（WEMs）可以比PLMs更健壮；以及3）当训练笔记来自少量患者时，模型的表现如何。我们发现，1）软提示调整是标准模型微调的有效替代方案； 2）PLMs比较简单的静态词嵌入模型表现出更好的区分能力但更差的校准能力。

    We investigate different natural language processing (NLP) approaches based on contextualised word representations for the problem of early prediction of lung cancer using free-text patient medical notes of Dutch primary care physicians. Because lung cancer has a low prevalence in primary care, we also address the problem of classification under highly imbalanced classes. Specifically, we use large Transformer-based pretrained language models (PLMs) and investigate: 1) how \textit{soft prompt-tuning} -- an NLP technique used to adapt PLMs using small amounts of training data -- compares to standard model fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more robust compared to PLMs in highly imbalanced settings; and 3) how models fare when trained on notes from a small number of patients. We find that 1) soft-prompt tuning is an efficient alternative to standard model fine-tuning; 2) PLMs show better discrimination but worse calibration compared to simpler stat
    
[^27]: 生成合理的对策序列用于预测性流程分析：CREATED

    CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics. (arXiv:2303.15844v1 [cs.LG])

    [http://arxiv.org/abs/2303.15844](http://arxiv.org/abs/2303.15844)

    本论文提出了一个通用框架，使用进化方法生成反事实序列并避免对领域知识的需求以提高可行性。

    

    预测性流程分析关注预测未来状态，例如运行流程实例的结果。这些技术通常利用机器学习模型或深度学习模型（如LSTM）进行这样的预测。然而，这些深度模型复杂而且难以理解。反事实能回答“如果……会怎样”的问题，这些问题有助于理解预测背后的推理。当前用于生成反事实序列的方法不考虑流程行为，从而导致生成无效或不可行的反事实流程实例，或严重依赖于领域知识。在这项工作中，我们提出了一个通用框架，利用进化方法生成反事实序列，并避免了对领域知识的需求。我们提议训练一个马尔可夫模型来计算反事实序列的概率，这将在生成反事实序列时保证其可行性。

    Predictive process analytics focuses on predicting future states, such as the outcome of running process instances. These techniques often use machine learning models or deep learning models (such as LSTM) to make such predictions. However, these deep models are complex and difficult for users to understand. Counterfactuals answer ``what-if'' questions, which are used to understand the reasoning behind the predictions. For example, what if instead of emailing customers, customers are being called? Would this alternative lead to a different outcome? Current methods to generate counterfactual sequences either do not take the process behavior into account, leading to generating invalid or infeasible counterfactual process instances, or heavily rely on domain knowledge. In this work, we propose a general framework that uses evolutionary methods to generate counterfactual sequences. Our framework does not require domain knowledge. Instead, we propose to train a Markov model to compute the f
    
[^28]: 通过元机器学习在企业网络中实现跨组织分析

    Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning. (arXiv:2303.15834v1 [cs.LG])

    [http://arxiv.org/abs/2303.15834](http://arxiv.org/abs/2303.15834)

    该论文提出了一种元机器学习方法，用于在跨组织的企业网络中实现全面的数据分析。该方法可以解决在数据分布在多个法律实体之间时，披露敏感信息和需要交换大量数据的难题，并显示了在工业应用中的可行性。

    

    成功的分析解决方案通常依赖于连接各种数据源。虽然在组织内部生成更大的数据池通常是可行的，但在（跨组织的）企业网络中应用分析仍然受到严重的制约。由于数据分布在多个法律实体之间，甚至可能跨越多个国家，因此披露敏感信息的担忧以及需要交换的数据量是创建有效的系统范围解决方案的关键障碍，同时仍然实现卓越的预测性能。本文提出了一种元机器学习方法，以解决这些障碍，从而在企业网络中实现全面的分析。我们采用设计科学研究方法，并在一个工业用例中评估了我们的方法的可行性和性能。首先，我们展示了执行网络广泛分析是可行的。

    Successful analytics solutions that provide valuable insights often hinge on the connection of various data sources. While it is often feasible to generate larger data pools within organizations, the application of analytics within (inter-organizational) business networks is still severely constrained. As data is distributed across several legal units, potentially even across countries, the fear of disclosing sensitive information as well as the sheer volume of the data that would need to be exchanged are key inhibitors for the creation of effective system-wide solutions -- all while still reaching superior prediction performance. In this work, we propose a meta machine learning method that deals with these obstacles to enable comprehensive analyses within a business network. We follow a design science research approach and evaluate our method with respect to feasibility and performance in an industrial use case. First, we show that it is feasible to perform network-wide analyses that 
    
[^29]: 机器学习在流体力学实验中的变革性潜力

    The transformative potential of machine learning for experiments in fluid mechanics. (arXiv:2303.15832v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2303.15832](http://arxiv.org/abs/2303.15832)

    本文介绍了机器学习在实验流体力学中的三个方面的应用潜力：提高测量技术精度，改进实验设计和实现实时估计和控制。

    

    机器学习领域在许多科学和工程领域取得了快速进展，其中包括实验流体力学，这是最初的大数据学科之一。本文将重点介绍实验流体力学中几个受益于机器学习进展的方面，包括：1）增强测量技术的精度和质量，2）改进实验设计和替代数值孪生模型和3）实现实时估计和控制。对于每个方面，我们讨论了最近的成功案例和正在进行的挑战，以及注意事项和限制，并概述了ML增强和ML能力的实验流体力学的新途径的潜力。

    The field of machine learning has rapidly advanced the state of the art in many fields of science and engineering, including experimental fluid dynamics, which is one of the original big-data disciplines. This perspective will highlight several aspects of experimental fluid mechanics that stand to benefit from progress advances in machine learning, including: 1) augmenting the fidelity and quality of measurement techniques, 2) improving experimental design and surrogate digital-twin models and 3) enabling real-time estimation and control. In each case, we discuss recent success stories and ongoing challenges, along with caveats and limitations, and outline the potential for new avenues of ML-augmented and ML-enabled experimental fluid mechanics.
    
[^30]: MS-MT: 带有对抗无配对翻译的多尺度均值教师用于跨模态听神经瘤和耳蜗分割

    MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2303.15826v1 [eess.IV])

    [http://arxiv.org/abs/2303.15826](http://arxiv.org/abs/2303.15826)

    本文提出了一个跨模态图像分割的无监督域自适应方法，通过多尺度自我集成和对抗无配对翻译，实现了对听神经瘤和耳蜗的自动分割，取得了很好的效果。

    

    针对医学图像分割中存在的域漂移问题，本文提出了一种基于多尺度自我集成的无监督域自适应框架，用于高分辨率T2图像上的两个关键脑结构（即听神经瘤和耳蜗）的自动分割。通过设计分割增强的对抗无配对图像翻译模块，对源T1到目标T2的图像级域自适应进行了改进。接着，引入了多尺度深度监督和一致性正则化来进行均值教师网络的自我集成学习，以进一步缩小域差距。此外，采用自训练和强度增强技术来缓解标签稀缺性并增强跨模态分割性能。结果显示，我们的方法在跨模态听神经瘤和耳蜗分割任务中表现出了很好的性能，优于几种最先进的无监督域自适应方法。

    Domain shift has been a long-standing issue for medical image segmentation. Recently, unsupervised domain adaptation (UDA) methods have achieved promising cross-modality segmentation performance by distilling knowledge from a label-rich source domain to a target domain without labels. In this work, we propose a multi-scale self-ensembling based UDA framework for automatic segmentation of two key brain structures i.e., Vestibular Schwannoma (VS) and Cochlea on high-resolution T2 images. First, a segmentation-enhanced contrastive unpaired image translation module is designed for image-level domain adaptation from source T1 to target T2. Next, multi-scale deep supervision and consistency regularization are introduced to a mean teacher network for self-ensemble learning to further close the domain gap. Furthermore, self-training and intensity augmentation techniques are utilized to mitigate label scarcity and boost cross-modality segmentation performance. Our method demonstrates promising 
    
[^31]: 一种适用于所有编程语言的适配器吗？用于代码搜索和摘要的适配器调整

    One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization. (arXiv:2303.15822v1 [cs.SE])

    [http://arxiv.org/abs/2303.15822](http://arxiv.org/abs/2303.15822)

    本文探讨在适配器微调中固定所有预训练模型参数并插入参数高效结构适配器的方法，旨在缓解多语言模型中遗忘问题。在代码搜索和摘要任务中实现了一致的改进并取得了最先进的结果，同时还展示了适配器在跨语言和低资源场景中的有效性。

    

    随着预训练模型对许多代码智能任务的自动化，一种广泛使用的范式是在每种编程语言的任务数据集上微调模型。最近的一项研究报告称，多语言微调有益于各种任务和模型。然而，我们发现多语言微调会导致UniXcoder和CodeT5最近模型的性能下降。为了缓解多语言模型中可能发生的灾难性遗忘问题，我们固定所有预训练模型参数，插入参数高效结构适配器，并对其进行微调。与为每种编程语言进行完全模型微调相比，适配器微调仅更新了整体参数的0.6％，在代码搜索和摘要任务上产生了一致的改进，取得了最先进的结果。此外，我们通过实验展示了适配器在跨语言和低资源场景中的有效性。每种编程语言使用200个样本进行的多语言微调接近于微调结果。

    As pre-trained models automate many code intelligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5.  To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6\% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and summarization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned wit
    
[^32]: 没有OOD动作的离线强化学习：基于隐式价值正则化的样本内学习

    Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization. (arXiv:2303.15810v1 [cs.LG])

    [http://arxiv.org/abs/2303.15810](http://arxiv.org/abs/2303.15810)

    本文提出了一种新算法，IVR-Q，用于离线强化学习，其通过隐式价值正则化的方式避免了OOD动作带来的价值函数偏移，并通过最小化IVR loss来改善策略。在多个基准任务上的实验结果证明其优于现有方法并实现了最佳性能。

    

    大多数离线强化学习 (RL) 方法面临一个折衷问题：改善策略以超越行为策略与限制策略以限制与行为策略的偏差之间的平衡。由于使用超出分布范围 (OOD) 的动作计算 Q 值会受到分布偏移错误的影响。最近提出的基于样本内学习范式（IQL）通过对数据样本进行分位回归来改善策略，表现出很大的潜力，因为它可以学习最优策略而不查询任何未见动作的值函数。然而，目前尚不清楚这种方法如何处理学习价值函数时的分布偏移。本文发现，样本内学习范例在隐式价值正则化 (IVR) 框架下得以产生。这给了我们更深刻的理解为什么样本内学习范例有效，即它将隐式价值正则化应用于策略。基于这个洞见，我们提出了一个新算法，IVR-Q，它通过规范化策略的价值函数以避免OOD动作，并通过最小化IVR loss来改善策略。多个基准任务的实验结果表明，IVR-Q优于现有方法，并在离线RL中实现了最佳性能。

    Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recently proposed \textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based 
    
[^33]: 生态图表：基础模型的社会影响

    Ecosystem Graphs: The Social Footprint of Foundation Models. (arXiv:2303.15772v1 [cs.LG])

    [http://arxiv.org/abs/2303.15772](http://arxiv.org/abs/2303.15772)

    该论文提出了一种名为「生态图表」的文档框架，可以透明地集中基础模型的社会影响方面的知识，这可以提高其透明度和问责度。

    

    基础模型（例如 ChatGPT、StableDiffusion）广泛影响社会，因此需要社会的关注。虽然这些模型本身受到了广泛的关注，但为了准确地描述它们的影响，我们必须考虑更广泛的社会技术生态系统。我们提出了一种名为「生态图表」的文档框架，以透明地集中这个生态系统的知识。生态图表由资产（数据集、模型、应用程序）组成，这些资产通过依赖关系链接在一起，这些关系指示了技术（例如 Bing 如何依赖 GPT-4）和社交（例如 Microsoft 如何依赖 OpenAI）之间的关系。为了补充图形结构，每个资产都进一步丰富了细粒度的元数据（例如许可或培训排放）。我们在 https://crfm.stanford.edu/ecosystem-graphs/ 上广泛记录生态系统。截至 2023 年 3 月 16 日，我们注释了来自 63 个组织的 262 个资产（64 个数据集，128 个模型，70 个应用程序），它们由 356 种依赖关系链接在一起。我们展示了生态图表作为一种工具，可以提高基础模型及其更广泛的社会技术生态系统的透明度和问责度，并呼吁采用它作为一种常见的文档框架。

    Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions a
    
[^34]: RobustSwap：一种简单而强大的人脸交换模型，能够有效避免属性泄漏问题

    RobustSwap: A Simple yet Robust Face Swapping Model against Attribute Leakage. (arXiv:2303.15768v1 [cs.CV])

    [http://arxiv.org/abs/2303.15768](http://arxiv.org/abs/2303.15768)

    本文提出了一种简单而强大的人脸交换模型RobustSwap，能够有效避免源属性泄漏问题，并利用3DMM的隐式和显式信息的协调来实现源图像结构的导入和目标图像准确姿态。

    

    人脸交换旨在将源图像的身份（即面部特征）注入目标图像，同时严格保留目标图像的不相关身份属性。但是，我们观察到之前的方法仍然存在源属性泄漏问题，即源图像的属性与目标图像的属性干扰。在本文中，我们分析了StyleGAN的潜在空间，并找到适合人脸交换任务的潜在因素组合。基于这些发现，我们开发了一种简单而强大的人脸交换模型RobustSwap，能够抵抗潜在的源属性泄漏问题。此外，我们利用3DMM的隐式和显式信息的协调作为指导，将源图像的结构和目标图像的准确姿态结合起来。尽管我们的方法仅利用图像数据集进行训练，而没有身份标签，但我们的模型能够生成高质量和时间上连续的视频。

    Face swapping aims at injecting a source image's identity (i.e., facial features) into a target image, while strictly preserving the target's attributes, which are irrelevant to identity. However, we observed that previous approaches still suffer from source attribute leakage, where the source image's attributes interfere with the target image's. In this paper, we analyze the latent space of StyleGAN and find the adequate combination of the latents geared for face swapping task. Based on the findings, we develop a simple yet robust face swapping model, RobustSwap, which is resistant to the potential source attribute leakage. Moreover, we exploit the coordination of 3DMM's implicit and explicit information as a guidance to incorporate the structure of the source image and the precise pose of the target image. Despite our method solely utilizing an image dataset without identity labels for training, our model has the capability to generate high-fidelity and temporally consistent videos. 
    
[^35]: 关于递归特征机器的特征缩放

    On Feature Scaling of Recursive Feature Machines. (arXiv:2303.15745v1 [cs.LG])

    [http://arxiv.org/abs/2303.15745](http://arxiv.org/abs/2303.15745)

    本报告通过实验探究了递归特征机器的行为，发现其在添加随机噪声特征时MSE曲线呈现出降低-增加-降低的模式，并且与神经网络的“双峰下降”现象相似，为后续研究奠定基础。

    

    本技术报告通过一系列在回归数据集上的实验，探究了递归特征机器(RFMs)的行为，RFMs是一种通过平均梯度外积来递归地学习特征的新型核机器。当在数据集中迭代地添加随机噪声特征时，我们观察到均方误差(MSE)曲线呈现出降低-增加-降低的有趣模式。这种行为在不同的数据集大小、噪声参数和目标函数下保持一致。有趣的是，观察到的MSE曲线与深度神经网络中观察到的“双峰下降”现象相似，暗示RFMs和神经网络行为之间存在新的联系。这份报告为未来研究这种奇妙行为奠定了基础。

    In this technical report, we explore the behavior of Recursive Feature Machines (RFMs), a type of novel kernel machine that recursively learns features via the average gradient outer product, through a series of experiments on regression datasets. When successively adding random noise features to a dataset, we observe intriguing patterns in the Mean Squared Error (MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern. This behavior is consistent across different dataset sizes, noise parameters, and target functions. Interestingly, the observed MSE curves show similarities to the "double descent" phenomenon observed in deep neural networks, hinting at new connection between RFMs and neural network behavior. This report lays the groundwork for future research into this peculiar behavior.
    
[^36]: 一个多乐器体积调制的方法增强格斗游戏中的背景音乐：自适应背景音乐

    Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume Modulation Approach. (arXiv:2303.15734v1 [cs.SD])

    [http://arxiv.org/abs/2303.15734](http://arxiv.org/abs/2303.15734)

    本文介绍了一种自适应背景音乐的方法，它由五种不同的乐器演奏名为“空中小姐曲”的古典音乐组成，并通过改变乐器的音量来适应游戏的不同元素。实验结果表明，使用这种自适应背景音乐可以改善游戏的体验。

    

    本文介绍了我们在 DareFightingICE 中添加自适应背景音乐以增强游戏体验的工作。自适应背景音乐由五种不同的乐器演奏名为“空中小姐曲”的古典音乐组成，通过改变乐器的音量来适应游戏的不同元素。我们进行了一项实验来评估自适应背景音乐，并使用了一种只使用音频作为输入的深度增强学习 AI（Blind DL AI）。结果表明，与没有自适应背景音乐时相比，Blind DL AI 在与自适应背景音乐一起播放时表现更好。

    This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding an adaptive BGM. The adaptive BGM consists of five different instruments playing a classical music piece called "Air on G-String." The BGM adapts by changing the volume of the instruments. Each instrument is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.
    
[^37]: 评估ChatGPT在基于NLP的心理健康应用中的应用

    Evaluation of ChatGPT for NLP-based Mental Health Applications. (arXiv:2303.15727v1 [cs.CL])

    [http://arxiv.org/abs/2303.15727](http://arxiv.org/abs/2303.15727)

    本篇论文评估了基于LLM和ChatGPT在心理健康领域的实际应用，显示出其在压力和抑郁症检测方面表现良好，但在自杀风险检测上仍需改进。

    

    大型语言模型(LLM)在多项自然语言理解任务中具有成功的应用，可能对基于自然语言处理(NLP)的心理健康应用研究也很有帮助。本研究报告了基于LLM的ChatGPT (使用gpt-3.5-turbo后端)在三个文本类心理健康分类任务中的表现: 压力检测 (2类分类)、抑郁症检测(2类分类)和自杀风险检测(5类分类)。我们从公共数据集中获取了三个分类任务的带标注社交媒体帖子。然后使用ChatGPT API对社交媒体帖子进行输入提示分类。我们得到了0.73、0.86和0.37的F1分数，分别用于压力检测、抑郁症检测和自杀风险检测。总体上，ChatGPT在语言处理领域中具备很大的应用前景。

    Large language models (LLM) have been successful in several natural language understanding tasks and could be relevant for natural language processing (NLP)-based mental health application research. In this work, we report the performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three text-based mental health classification tasks: stress detection (2-class classification), depression detection (2-class classification), and suicidality detection (5-class classification). We obtained annotated social media posts for the three classification tasks from public datasets. Then ChatGPT API classified the social media posts with an input prompt for classification. We obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression detection, and suicidality detection, respectively. A baseline model that always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and 0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a potential use o
    
[^38]: MeMaHand：利用网格-手部互动进行单张图像双手重建

    MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction. (arXiv:2303.15718v1 [cs.CV])

    [http://arxiv.org/abs/2303.15718](http://arxiv.org/abs/2303.15718)

    本文提出了一种利用网格-手部互动进行单张图像双手重建的新方法，使用网格顶点位置和MANO参数作为查询令牌，实现了精确的网格重建，并通过实验表明这一方法优于现有最先进方法。

    

    现有的手重建方法通常对一个通用的3D手模型进行参数化或者直接预测手掌网格位置，参数表示的手部形状和旋转姿态更为稳定，而非参数化的方法可以更精确地预测网格位置。本文提出了一种新方法，从单张RGB图像中同时重建两只手的网格并估计MANO参数，以利用两种手表示方法的优点。为了实现这个目标，我们提出了新的网格-手部互动块（MMIBs），它将网格顶点位置和MANO参数作为两种查询令牌。MMIB由一个图形残差块来聚合局部信息和两个变换编码器来建模远程依赖关系。变换编码器配备不同的不对称关注掩码来分别建模手内和手间关注。此外，我们还引入了网格对齐细化模块来进一步提高网格重建的准确性。实验结果表明，我们提出的方法在单手和双手重建任务上优于现有最先进方法。

    Existing methods proposed for hand reconstruction tasks usually parameterize a generic 3D hand model or predict hand mesh positions directly. The parametric representations consisting of hand shapes and rotational poses are more stable, while the non-parametric methods can predict more accurate mesh positions. In this paper, we propose to reconstruct meshes and estimate MANO parameters of two hands from a single RGB image simultaneously to utilize the merits of two kinds of hand representations. To fulfill this target, we propose novel Mesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and MANO parameters as two kinds of query tokens. MMIB consists of one graph residual block to aggregate local information and two transformer encoders to model long-range dependencies. The transformer encoders are equipped with different asymmetric attention masks to model the intra-hand and inter-hand attention, respectively. Moreover, we introduce the mesh alignment refinement mo
    
[^39]: 显式规划有助于语言模型进行逻辑推理

    Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])

    [http://arxiv.org/abs/2303.15714](http://arxiv.org/abs/2303.15714)

    本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。

    

    语言模型在各种自然语言处理任务中表现出色。本文提出了一个新颖的系统，采用语言模型进行多步逻辑推理。我们的系统将显式规划纳入到推理过程中，因此可以通过展望未来的效果来做出更明智的决策。在实验中，我们的全套系统在多项选择题答题任务中明显优于其他竞争系统，尽管只有约15亿个参数，但与GPT-3-davinci表现相当。我们进行了多个消融研究以证明显式规划在系统性能中起着关键作用。

    Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
    
[^40]: 使用Tube MPC引导的数据增强，高效学习鲁棒性的自适应策略的深度学习（arXiv:2303.15688v1 [cs.RO]）

    Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation. (arXiv:2303.15688v1 [cs.RO])

    [http://arxiv.org/abs/2303.15688](http://arxiv.org/abs/2303.15688)

    本论文提出了一种高效的深度学习算法，可以学习具有鲁棒性和自适应能力的策略，通过引导数据增强，使用修改后的IL过程，并在学习适应性位置和姿态控制策略方面进行应用。

    

    在具有挑战性的非结构化环境中部署敏捷自主系统需要适应能力和对不确定性的鲁棒性。现有的鲁棒和自适应控制器，如基于MPC的控制器，可以在在线运行计算量庞大的情况下实现令人印象深刻的性能。出现了有效地从MPC学习鲁棒且可在机载设备上部署的策略的策略，但它们仍然缺乏基本适应能力。在这项工作中，我们扩展了一种现有的高效IL算法，用于鲁棒性策略从MPC学习，具有学习适应具有挑战性模型/环境不确定性的策略的能力。我们方法的关键思想是通过在学习的低维模型/环境表示上对策略进行调整，从而修改IL过程，这可以在在线状态下高效地估计。我们将我们的方法定制为学习自适应位置和姿态控制策略以在具有挑战性的干扰下跟踪轨迹。

    The deployment of agile autonomous systems in challenging, unstructured environments requires adaptation capabilities and robustness to uncertainties. Existing robust and adaptive controllers, such as the ones based on MPC, can achieve impressive performance at the cost of heavy online onboard computations. Strategies that efficiently learn robust and onboard-deployable policies from MPC have emerged, but they still lack fundamental adaptation capabilities. In this work, we extend an existing efficient IL algorithm for robust policy learning from MPC with the ability to learn policies that adapt to challenging model/environment uncertainties. The key idea of our approach consists in modifying the IL procedure by conditioning the policy on a learned lower-dimensional model/environment representation that can be efficiently estimated online. We tailor our approach to the task of learning an adaptive position and attitude control policy to track trajectories under challenging disturbances
    
[^41]: DisWOT: 无需训练的知识蒸馏学生架构搜索

    DisWOT: Student Architecture Search for Distillation WithOut Training. (arXiv:2303.15678v1 [cs.CV])

    [http://arxiv.org/abs/2303.15678](http://arxiv.org/abs/2303.15678)

    本文提出了一种无需训练的框架来搜索最适合给定教师的最佳学生架构，以提高知识蒸馏的效果。其通过度量语义激活映射条件下的相似性矩阵来选择最佳学生，而不是通过传统的训练方法。

    

    知识蒸馏(KD)是一种有效的训练策略，可以在笨重的教师的指导下提高轻量级学生模型的性能。然而，教师和学生之间的大型架构差异限制了蒸馏效果。相对于以前的自适应蒸馏方法来减少教师和学生之间的差距，我们探索了一种新的无需训练框架，以搜索给定教师的最佳学生架构。

    Knowledge distillation (KD) is an effective training strategy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large architecture difference across the teacher-student pairs limits the distillation gains. In contrast to previous adaptive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work first empirically show that the optimal model under vanilla training cannot be the winner in distillation. Secondly, we find that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with final distillation performances. Thus, we efficiently measure similarity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithm without any training. In this way, our student architecture search for Distillation Wit
    
[^42]: 凸多级随机优化的数值方法

    Numerical Methods for Convex Multistage Stochastic Optimization. (arXiv:2303.15672v1 [math.OC])

    [http://arxiv.org/abs/2303.15672](http://arxiv.org/abs/2303.15672)

    本文讨论了在随机规划和随机最优控制中凸多级随机问题的数值解法，包括动态规划和割平面方法，旨在解决维度诅咒问题。

    

    随机环境下涉及顺序决策的优化问题已在随机规划(SP)、随机最优控制(SOC)和马尔可夫决策过程(MDP)中进行了研究。本文主要集中讨论SP和SOC建模方法。在这些框架中，存在自然情况下所考虑的问题是凸问题。顺序优化的经典方法是基于动态规划，但它存在所谓的“维度诅咒”问题，即随着状态变量维度的增加，它的计算复杂度呈指数级增长。近年来，解决凸多级随机问题的最新进展是基于动态规划方程的成本函数逐步逼近的割平面近似。动态环境中的割平面类型算法是本文的主要讨论内容之一。我们还讨论了应用于多级随机优化问题的随机逼近类型方法。

    Optimization problems involving sequential decisions in a stochastic environment were studied in Stochastic Programming (SP), Stochastic Optimal Control (SOC) and Markov Decision Processes (MDP). In this paper we mainly concentrate on SP and SOC modelling approaches. In these frameworks there are natural situations when the considered problems are convex. Classical approach to sequential optimization is based on dynamic programming. It has the problem of the so-called ``Curse of Dimensionality", in that its computational complexity increases exponentially with increase of dimension of state variables. Recent progress in solving convex multistage stochastic problems is based on cutting planes approximations of the cost-to-go (value) functions of dynamic programming equations. Cutting planes type algorithms in dynamical settings is one of the main topics of this paper. We also discuss Stochastic Approximation type methods applied to multistage stochastic optimization problems. From the c
    
[^43]: 低资源语言下的无监督预训练文本转语音模型

    Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages. (arXiv:2303.15669v1 [eess.AS])

    [http://arxiv.org/abs/2303.15669](http://arxiv.org/abs/2303.15669)

    本文提出了一种针对低资源语言下无监督预训练的文本转语音模型，利用大量未转录的语音数据进行预训练，可显著减少训练模型所需的匹配转录数据量，进一步提升了数据效率，实验证明方法有效性。

    

    当大量转录音频数据用于训练时，神经文本到语音（TTS）模型可以合成自然的人类语音。但是，收集这样的大规模转录数据很昂贵。本文提出了一种无监督预训练方法，用于对序列到序列的TTS模型进行预训练，利用大量未转录的语音数据。通过我们的预训练，我们可以显着减少训练模型所需的匹配转录数据量，用于目标下游TTS任务的训练。主要思想是预先训练模型，以从扭曲的mel频谱图中重建出去扭曲的mel频谱图，这可能使模型学会了输入和输出序列之间的适当时间分配关系。此外，我们提出了一种数据增强方法，可进一步提高微调中的数据效率。我们在低资源语言场景中实验证明了我们提出的方法的有效性，与竞争方法相比表现出色。代码和音频样本可以在我们的项目页面上找到。

    Neural text-to-speech (TTS) models can synthesize natural human speech when trained on large amounts of transcribed speech. However, collecting such large-scale transcribed data is expensive. This paper proposes an unsupervised pre-training method for a sequence-to-sequence TTS model by leveraging large untranscribed speech data. With our pre-training, we can remarkably reduce the amount of paired transcribed data required to train the model for the target downstream TTS task. The main idea is to pre-train the model to reconstruct de-warped mel-spectrograms from warped ones, which may allow the model to learn proper temporal assignment relation between input and output sequences. In addition, we propose a data augmentation method that further improves the data efficiency in fine-tuning. We empirically demonstrate the effectiveness of our proposed method in low-resource language scenarios, achieving outstanding performance compared to competing methods. The code and audio samples are av
    
[^44]: ChatGPT4PCG比赛：科学鸟角色级生成

    ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])

    [http://arxiv.org/abs/2303.15662](http://arxiv.org/abs/2303.15662)

    本论文介绍了举办在2023 IEEE游戏会议上的第一届ChatGPT4PCG比赛，目标是让ChatGPT生成具有高稳定性和类似角色的特质来生成具有科学鸟角色级水平的关卡。

    

    本文介绍了2023年IEEE游戏会议上的第一届ChatGPT4PCG比赛。本次比赛的目标是让参赛者通过创造性和提示工程技能，为ChatGPT创建有效的提示，使其能够具有高稳定性和类似角色的特质来生成具有科学鸟角色级水平的关卡。为了降低参赛门槛，我们将任务限制在生成大写英文字母。参赛作品的质量由其稳定性和与给定字符的相似性决定。给参赛者提供了一个样例提示供参考。

    This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of its modified
    
[^45]: 台风：针对预训练语言模型的有效特定任务屏蔽策略

    Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models. (arXiv:2303.15619v1 [cs.CL])

    [http://arxiv.org/abs/2303.15619](http://arxiv.org/abs/2303.15619)

    本文探讨了一种针对预训练语言模型的任务特定的屏蔽框架，称为Typhoon，可在GLUE基准数据集上实现卓越的下游任务性能，尤其在MRPC数据集上表现优异。

    

    通过利用图形处理单元所能提供的高度并行性，变压器架构使自然语言处理领域取得了巨大的进展。在传统的屏蔽语言模型中，使用特殊的MASK标记来提示模型从周围单词中收集情境信息以恢复原本隐藏的信息。在本文中，我们探讨了一种预训练大型语言模型的任务特定的屏蔽框架，以在GLUE基准数据集上实现卓越的下游任务性能。我们基于记号输入梯度开发了自己的屏蔽算法Typhoon，并将其与其他标准基线进行比较。我们发现，Typhoon在MRPC数据集上的性能与整体字屏蔽相当。我们的实现可以在公共Github库中找到。

    Through exploiting a high level of parallelism enabled by graphics processing units, transformer architectures have enabled tremendous strides forward in the field of natural language processing. In a traditional masked language model, special MASK tokens are used to prompt our model to gather contextual information from surrounding words to restore originally hidden information. In this paper, we explore a task-specific masking framework for pre-trained large language models that enables superior performance on particular downstream tasks on the datasets in the GLUE benchmark. We develop our own masking algorithm, Typhoon, based on token input gradients, and compare this with other standard baselines. We find that Typhoon offers performance competitive with whole-word masking on the MRPC dataset. Our implementation can be found in a public Github Repository.
    
[^46]: EMShepherd：通过侧信道泄漏检测对抗性示例

    EMShepherd: Detecting Adversarial Samples via Side-channel Leakage. (arXiv:2303.15571v1 [cs.CR])

    [http://arxiv.org/abs/2303.15571](http://arxiv.org/abs/2303.15571)

    EMShepherd使用电磁推理的辐射跟踪并利用它们用于对抗性检测。只需使用良性样本及其EM跟踪数据进行训练，从而避免对模型本身的深入了解。

    

    深度神经网络（DNN）容易受到对抗性扰动的影响-小的更改是有意制作的，以使输入错误地导致错误的预测。对抗性攻击对于深度学习赋能的关键应用有灾难性后果。现有的防御和检测技术都需要对模型、测试输入甚至执行细节有深入的了解。它们对于使用人员来说是不可行的黑盒场景中的一般深度学习实现。受到电磁（EM）推理的辐射既取决于操作和数据，又可能包含不同输入类的痕迹，我们提出了一个框架-EMShepherd，捕获模型执行的EM跟踪、进行跟踪处理并利用它们用于对抗性检测。只有良性样本及其EM跟踪数据用于训练对抗性检测器：一组EM分类器和特定类别的无监督分类器。

    Deep Neural Networks (DNN) are vulnerable to adversarial perturbations-small changes crafted deliberately on the input to mislead the model for wrong predictions. Adversarial attacks have disastrous consequences for deep learning-empowered critical applications. Existing defense and detection techniques both require extensive knowledge of the model, testing inputs, and even execution details. They are not viable for general deep learning implementations where the model internal is unknown, a common 'black-box' scenario for model users. Inspired by the fact that electromagnetic (EM) emanations of a model inference are dependent on both operations and data and may contain footprints of different input classes, we propose a framework, EMShepherd, to capture EM traces of model execution, perform processing on traces and exploit them for adversarial detection. Only benign samples and their EM traces are used to train the adversarial detector: a set of EM classifiers and class-specific unsup
    
[^47]: 在黎曼流形上学习分子谐波表示

    Learning Harmonic Molecular Representations on Riemannian Manifold. (arXiv:2303.15520v1 [cs.LG])

    [http://arxiv.org/abs/2303.15520](http://arxiv.org/abs/2303.15520)

    本文提出了一种基于黎曼流形的分子谐波表示学习框架，使用分子表面的拉普拉斯-贝尔特拉米特征函数来表示分子，实现了分子几何和化学特征的多分辨率表示。

    

    分子表示学习在人工智能辅助药物发现研究中起着至关重要的作用。通过欧几里得神经网络对三维分子结构进行编码已成为几何深度学习社区的主流方法。然而，欧几里得空间中的等变性约束和消息传递可能会限制网络的表达能力。在本文中，我们提出了一种谐波分子表示学习（HMR）框架，它使用分子表面的拉普拉斯-贝尔特拉米特征函数来表示分子。HMR在2D黎曼流形上提供了分子几何和化学特征的多分辨率表示。我们还引入了一种谐波消息传递方法，在表面流形上实现高效的谱消息传递以实现更好的分子编码。我们提出的方法显示出与当前模型在小分子属性预测方面具有可比性的预测能力，并且在配体结合中优于最先进的深度学习模型。

    Molecular representation learning plays a crucial role in AI-assisted drug discovery research. Encoding 3D molecular structures through Euclidean neural networks has become the prevailing method in the geometric deep learning community. However, the equivariance constraints and message passing in Euclidean space may limit the network expressive power. In this work, we propose a Harmonic Molecular Representation learning (HMR) framework, which represents a molecule using the Laplace-Beltrami eigenfunctions of its molecular surface. HMR offers a multi-resolution representation of molecular geometric and chemical features on 2D Riemannian manifold. We also introduce a harmonic message passing method to realize efficient spectral message passing over the surface manifold for better molecular encoding. Our proposed method shows comparable predictive power to current models in small molecule property prediction, and outperforms the state-of-the-art deep learning models for ligand-binding pro
    
[^48]: 微服务的效率分析研究

    Research on Efficiency Analysis of Microservices. (arXiv:2303.15490v1 [cs.SE])

    [http://arxiv.org/abs/2303.15490](http://arxiv.org/abs/2303.15490)

    本研究通过排队模型探讨了将传统的大型服务分解为多个微服务后提高系统效率的问题，并发现分解后所需总时间比原始服务少，因此分解可以提高效率。

    

    随着Web服务、容器和云计算技术的成熟，传统系统中的大型服务（例如机器学习和人工智能的计算服务）正在逐渐分解为许多微服务，以提高服务的重用性和灵活性。因此，本研究提出了一种基于排队模型的效率分析框架，以分析将传统的大型服务分解为n个微服务的效率差异。为了推广应用，该研究考虑了不同的服务时间分布（例如服务时间的指数分布和固定服务时间），并通过排队模型（即M / M / 1排队模型和M / D / 1排队模型）探索了最坏情况和最佳情况下的系统效率。在每个实验中，都显示原始大型服务所需的总时间高于将其分解为多个微服务所需的时间，因此将其分解为多个微服务可以提高系统效率。

    With the maturity of web services, containers, and cloud computing technologies, large services in traditional systems (e.g. the computation services of machine learning and artificial intelligence) are gradually being broken down into many microservices to increase service reusability and flexibility. Therefore, this study proposes an efficiency analysis framework based on queuing models to analyze the efficiency difference of breaking down traditional large services into n microservices. For generalization, this study considers different service time distributions (e.g. exponential distribution of service time and fixed service time) and explores the system efficiency in the worst-case and best-case scenarios through queuing models (i.e. M/M/1 queuing model and M/D/1 queuing model). In each experiment, it was shown that the total time required for the original large service was higher than that required for breaking it down into multiple microservices, so breaking it down into multip
    
[^49]: 铁路网络延误演化：一种异构图神经网络方法

    Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach. (arXiv:2303.15489v1 [cs.LG])

    [http://arxiv.org/abs/2303.15489](http://arxiv.org/abs/2303.15489)

    本论文提出了一种能够应用于不同类型节点的异构图神经网络模型，针对铁路网络上的列车延误演化进行研究。使用HetGNN和GraphSAGE的组合，提出了一种名为SAGE-Het的图形架构，可以基于不同的边捕捉列车、列车、站点以及站点之间的相互作用，从而优于传统模型。

    

    铁路运营涉及不同类型的实体（站点，列车等），现有的同质节点（即相同类型的节点）的图/网络模型无法捕捉实体之间的相互作用。本文旨在开发一种异构图神经网络（HetGNN）模型，该模型可以应用于不同类型的节点（即异构节点），以研究铁路网络上的列车延误演化。为此，提出了一种组合HetGNN模型和GraphSAGE同质GNN（HomoGNN）的图形架构，称为SAGE-Het，旨在基于不同的边捕捉列车，列车、站点以及站点之间的相互作用。与传统方法要求输入具有恒定的维度（例如在矩形或类似网格的数组中）或仅允许在图中使用同质节点相比，SAGE-Het允许灵活的输入和异构节点。收集中国北京广州线上两个站点的数据以验证HetGNN模型。实验证明，所提出的SAGE-Het模型在预测列车延误演化方面优于传统模型，例如长短时记忆（LSTM）和HomoGNN模型。

    Railway operations involve different types of entities (stations, trains, etc.), making the existing graph/network models with homogenous nodes (i.e., the same kind of nodes) incapable of capturing the interactions between the entities. This paper aims to develop a heterogeneous graph neural network (HetGNN) model, which can address different types of nodes (i.e., heterogeneous nodes), to investigate the train delay evolution on railway networks. To this end, a graph architecture combining the HetGNN model and the GraphSAGE homogeneous GNN (HomoGNN), called SAGE-Het, is proposed. The aim is to capture the interactions between trains, trains and stations, and stations and other stations on delay evolution based on different edges. In contrast to the traditional methods that require the inputs to have constant dimensions (e.g., in rectangular or grid-like arrays) or only allow homogeneous nodes in the graph, SAGE-Het allows for flexible inputs and heterogeneous nodes. The data from two s
    
[^50]: 知识增强的图神经网络

    Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])

    [http://arxiv.org/abs/2303.15487](http://arxiv.org/abs/2303.15487)

    KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。

    

    图数据是无处不在的，并且具有各种应用，例如自然科学、社交网络或语义网。尽管富含信息，但图形通常噪声和不完整。因此，图补全任务，如节点分类或链接预测，已经受到关注。一方面，神经方法（如图神经网络）已经被证明是处理噪声图的稳健工具。另一方面，符号方法可以对图进行精确推理。我们提出了KeGNN，这是一个用于在图数据上学习的神经符号框架，结合了两种范例，并允许将先前的知识集成到图神经网络模型中。从本质上讲，KeGNN由一个图神经网络组成，其中基于目标将知识增强层堆叠在其上，以使针对先前知识的预测得到优化。我们将KeGNN与两个标准图神经网络：图卷积网络和图注意力网络一起实例化。实验结果表明，将先前的知识集成到图神经网络模型中可以提高节点分类和链接预测任务的准确性。

    Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
    
[^51]: 大规模语言模型能协助危害分析吗？

    Can Large Language Models assist in Hazard Analysis?. (arXiv:2303.15473v1 [cs.HC])

    [http://arxiv.org/abs/2303.15473](http://arxiv.org/abs/2303.15473)

    本文探讨了在安全关键系统的危害分析中应用大规模语言模型的潜力，结果表明LLMs可能有助于支持分析师进行危害分析。

    

    大规模语言模型（LLMs），如GPT-3，展示了卓越的自然语言处理和生成能力，并已应用于各种任务，例如源代码生成。本文探讨了将LLMs集成到安全关键系统的危害分析中的潜力，这个过程被我们称为协同危害分析（CoHA）。

    Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable natural language processing and generation capabilities and have been applied to a variety tasks, such as source code generation. This paper explores the potential of integrating LLMs in the hazard analysis for safety-critical systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a human analyst interacts with an LLM via a context-aware chat session and uses the responses to support elicitation of possible hazard causes. In this experiment, we explore CoHA with three increasingly complex versions of a simple system, using Open AI's ChatGPT service. The quality of ChatGPT's responses were systematically assessed to determine the feasibility of CoHA given the current state of LLM technology. The results suggest that LLMs may be useful for supporting human analysts performing hazard analysis.
    
[^52]: 有监督的掩蔽知识蒸馏用于少样本Transformer

    Supervised Masked Knowledge Distillation for Few-Shot Transformers. (arXiv:2303.15466v1 [cs.CV])

    [http://arxiv.org/abs/2303.15466](http://arxiv.org/abs/2303.15466)

    本文提出了一种新型的有监督的掩蔽知识蒸馏模型（SMKD），在少量标注数据的情况下，将标签信息融入到自蒸馏框架中，有效解决了Transformer在少样本学习中的过拟合和性能下降问题，实验结果在基准数据集上表现出最先进的性能。

    

    视觉Transformer利用局部特征捕捉远距离依赖关系，针对对少样本学习进行优化。然而，对于只有极少标注样本的数据集来说，由于缺少CNN式的归纳偏差，ViT容易过拟合并且性能严重下降。以前在少样本学习中的工作，要么通过辅助自监督损失来避免这种问题，要么通过监督学习的标签信息来避免。但是自监督和有监督的少样本Transformer之间的差距仍未填补。我们受到最近自监督知识蒸馏和掩蔽图像建模的进展启发，提出了一种新型的Supervised Masked Knowledge Distillation模型（SMKD）用于Transformer的少样本学习，将标签信息融入到自蒸馏框架中。与以前的自监督方法相比，我们允许类内知识流动，并有效利用监督信号对模型输出进行自然约束。在基准数据集上的实验表明，我们的方法在少样本分类任务上实现了最先进的性能，超过了以前自监督和有监督的方法。

    Vision Transformers (ViTs) emerge to achieve impressive performance on many data-abundant computer vision tasks by capturing long-range dependencies among local features. However, under few-shot learning (FSL) settings on small datasets with only a few labeled data, ViT tends to overfit and suffers from severe performance degradation due to its absence of CNN-alike inductive bias. Previous works in FSL avoid such problem either through the help of self-supervised auxiliary losses, or through the dextile uses of label information under supervised settings. But the gap between self-supervised and supervised few-shot Transformers is still unfilled. Inspired by recent advances in self-supervised knowledge distillation and masked image modeling (MIM), we propose a novel Supervised Masked Knowledge Distillation model (SMKD) for few-shot Transformers which incorporates label information into self-distillation frameworks. Compared with previous self-supervised methods, we allow intra-class kno
    
[^53]: 精确可合并的摘要

    Exactly mergeable summaries. (arXiv:2303.15465v1 [cs.LG])

    [http://arxiv.org/abs/2303.15465](http://arxiv.org/abs/2303.15465)

    本文提出了一种新类型的摘要，将传统聚合的优点与通过复杂数据表示保留更多信息相结合，实现了精确合并，既能保持精度又能减少数据大小。

    

    在大数据集的分析中，聚合是减少数据大小（复杂性）的标准方法。数据分析程序提供了不同的聚合函数。传统聚合的问题在于往往会丢失太多信息，从而降低了结果的精度。我们提出了一种新类型的摘要，它将传统聚合的优点与通过复杂数据表示保留更多信息相结合。这种方法允许对复杂摘要进行精确合并，并保持原始数据的精度。我们的研究为数据分析中复杂聚合摘要的理论基础的开发做出了贡献。

    In the analysis of large/big data sets, aggregation (replacing values of a variable over a group by a single value) is a standard way of reducing the size (complexity) of the data. Data analysis programs provide different aggregation functions.  Recently some books dealing with the theoretical and algorithmic background of traditional aggregation functions were published. A problem with traditional aggregation is that often too much information is discarded thus reducing the precision of the obtained results. A much better, preserving more information, summarization of original data can be achieved by representing aggregated data using selected types of complex data.  In complex data analysis the measured values over a selected group $A$ are aggregated into a complex object $\Sigma(A)$ and not into a single value. Most of the aggregation functions theory does not apply directly. In our contribution, we present an attempt to start building a theoretical background of complex aggregation
    
[^54]: 深度学习中的数学挑战

    Mathematical Challenges in Deep Learning. (arXiv:2303.15464v1 [cs.LG])

    [http://arxiv.org/abs/2303.15464](http://arxiv.org/abs/2303.15464)

    本文总结了深度学习中涉及培训、推理、一般化边界和优化问题的一组数学挑战，为数学家、统计学家和理论计算机科学家提供了与深度学习领域交流的形式化工具。

    

    自从2012年的ImageNet挑战以来，深度模型已经主宰了人工智能领域。深度模型的大小从那时起一直在增加，这给在手机、个人电脑、自动驾驶车辆和无线基站等领域应用的这一领域带来了新的挑战。在这里，我们列出了一组问题，涵盖培训、推理、一般化边界和优化问题，并用一些形式化语言来与数学家、统计学家和理论计算机科学家交流这些挑战。这是对深度学习研究问题的主观看法，它有益于长期的技术发展。

    Deep models are dominating the artificial intelligence (AI) industry since the ImageNet challenge in 2012. The size of deep models is increasing ever since, which brings new challenges to this field with applications in cell phones, personal computers, autonomous cars, and wireless base stations. Here we list a set of problems, ranging from training, inference, generalization bound, and optimization with some formalism to communicate these challenges with mathematicians, statisticians, and theoretical computer scientists. This is a subjective view of the research questions in deep learning that benefits the tech industry in long run.
    
[^55]: 量化分类决策的确定性和不确定性测量

    Measuring Classification Decision Certainty and Doubt. (arXiv:2303.14568v1 [stat.ML])

    [http://arxiv.org/abs/2303.14568](http://arxiv.org/abs/2303.14568)

    该论文提出了一种名为“确定性”和“不确定性”的得分方法来量化分类决策中预测的质量和不确定性。

    

    确定性和不确定性的定量表征和估计在优化和决策过程中具有基础重要性。本文提出了直观的得分，称为“确定性”和“不确定性”，可在贝叶斯和频率主义框架下用于评估和比较（多）分类决策机器学习问题的预测质量和不确定性。

    Quantitative characterizations and estimations of uncertainty are of fundamental importance in optimization and decision-making processes. Herein, we propose intuitive scores, which we call \textit{certainty} and \textit{doubt}, that can be used in both a Bayesian and frequentist framework to assess and compare the quality and uncertainty of predictions in (multi-)classification decision machine learning problems.
    
[^56]: Fantasia3D: 用于高质量文本生成3D内容的几何和外观分离方法

    Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])

    [http://arxiv.org/abs/2303.13873](http://arxiv.org/abs/2303.13873)

    Fantasia3D是一种新的文本生成3D内容的方法，通过分离几何和外观建模和学习，提高了几何细节和逼真渲染，并更有效和高效。

    

    最近，由于预训练的大型语言模型和图像扩散模型的提供，自动3D内容的创建取得了快速进展，形成了文本生成3D内容的新兴话题。现有的文本生成3D方法通常使用隐式场景表示，这些表示使用体积渲染将几何和外观耦合在一起，对于恢复更精细的几何和实现照片般逼真的渲染是次优的；因此，它们在产生高质量3D资产方面不够有效。在这项工作中，我们提出了一种名为Fantasia3D的新方法，用于高质量文本生成3D内容。Fantasia3D的关键在于几何和外观的分离建模和学习。对于几何学习，我们依靠混合场景表示，并建议将从表示中提取的表面法线编码作为图像扩散模型的输入。对于外观建模，我们引入了空间可变双向反射率分布函数（SVBRDF）来分离材料和光照属性。实验结果表明，我们的方法在几何细节和逼真渲染方面超越了现有技术，并且在从自然语言文本中生成高质量的3D内容方面更为有效和高效。

    Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function 
    
[^57]: 使用示范加速强化学习与规划：一份综述

    Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])

    [http://arxiv.org/abs/2303.13489](http://arxiv.org/abs/2303.13489)

    强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。

    

    尽管强化学习最近取得了巨大的成功，但是这种试错式的学习方法在复杂环境下可能效率低下。与此相反，使用示范可以让智能体受益于专家的知识，而无需探索最佳行动。在本综述中，我们讨论了在顺序决策中使用示范的优点，以及学习为基础的决策制定范式（例如，强化学习和规划在学习的模型中如何应用示范），以及如何在各种情况下收集示范。此外，我们还举了一个实际的示范生成和利用管道的例子，并在最近提出的ManiSkill机器人学习基准中进行了说明。

    Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
    
[^58]: 使用机器学习的动态风险评分提前预测心源性休克

    A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])

    [http://arxiv.org/abs/2303.12888](http://arxiv.org/abs/2303.12888)

    该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。

    

    心肌梗死和心力衰竭是主要的心血管疾病，影响着美国数百万人的健康。发展心源性休克的患者中，发病率和死亡率最高。心源性休克的早期识别至关重要，及时实施治疗措施可以防止缺血、低血压以及由于心源性休克导致心输出量降低的有害循环。然而，由于心脏监护病房中海量数据的信息处理能力与缺乏有效的风险分层工具，对心源性休克的早期识别一直具有挑战性。我们基于深度学习开发了一个称为CShock的风险分层工具，用于预测入住心脏监护病房的急性失代偿性心力衰竭和/或心肌梗死患者的心源性休克发作。为了开发和验证CShock，我们使用由医师裁定的结果注释了心脏监护病房数据集。

    Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
    
[^59]: 模拟橡胶手幻觉的基于大脑启发的身体自我感知模型

    Brain-inspired bodily self-perception model that replicates the rubber hand illusion. (arXiv:2303.12259v1 [q-bio.NC])

    [http://arxiv.org/abs/2303.12259](http://arxiv.org/abs/2303.12259)

    该论文提出了一个基于大脑启发的身体自我感知模型，模拟橡胶手幻觉，为认识人类身体自我意识的计算机制提供了新的洞见。

    

    身体自我意识的核心是对自己身体拥有权的感知。最近，为了更深入地了解大脑对自身身体编码的机制，人们做出了各种尝试，发展出一个统一的理论框架来解释相关的行为和神经生理现象。一个核心问题是如何解释橡胶手幻觉这样的身体错觉实际发生的机制。尽管已经有了有关身体自我意识机制和可能相关的脑区的概念性描述，但现有的理论模型仍然缺乏解释大脑如何编码对自己身体的感知和如何使用神经网络生成我们主观感知的身体错觉的计算机制。在这里，我们整合身体自我意识的生物学发现，提出一个基于大脑启发的身体自我感知模型，使身体自我感知可以在没有外部刺激的情况下自主构建。基于该模型的模拟复制了橡胶手幻觉，并提供了对潜在神经机制的见解。

    At the core of bodily self-consciousness is the perception of the ownership of one's body. Recent efforts to gain a deeper understanding of the mechanisms behind the brain's encoding of the self-body have led to various attempts to develop a unified theoretical framework to explain related behavioral and neurophysiological phenomena. A central question to be explained is how body illusions such as the rubber hand illusion actually occur. Despite the conceptual descriptions of the mechanisms of bodily self-consciousness and the possible relevant brain areas, the existing theoretical models still lack an explanation of the computational mechanisms by which the brain encodes the perception of one's body and how our subjectively perceived body illusions can be generated by neural networks. Here we integrate the biological findings of bodily self-consciousness to propose a Brain-inspired bodily self-perception model, by which perceptions of bodily self can be autonomously constructed withou
    
[^60]: ADCNet：带原始雷达ADC数据的端到端感知

    ADCNet: End-to-end perception with raw radar ADC data. (arXiv:2303.11420v1 [eess.SP])

    [http://arxiv.org/abs/2303.11420](http://arxiv.org/abs/2303.11420)

    本文提出了一种在原始雷达模拟数字（ADC）数据上执行端到端学习的方法，其中一个可学习的信号处理模块被嵌入网络中，实验结果证实了该方法的有效性。

    

    自动驾驶行业对雷达传感器的兴趣重新激发。雷达作为一种相对成熟的技术，在过去几年中得到了稳定的改进，使其成为常用的LiDAR的有吸引力的替代品或补充。一种新兴的趋势是利用丰富的低级别雷达数据进行感知。在这项工作中，我们将这个趋势推向了极端--我们提出了一种在原始雷达模拟数字（ADC）数据上执行端到端学习的方法。具体来说，我们在神经网络中设计了一个可学习的信号处理模块，并提供了一个由传统信号处理算法引导的预训练方法。实验结果证实了端到端学习方法的整体有效性，而消融研究验证了我们个体创新的有效性。

    There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.
    
[^61]: 多语言方言检测的两阶段流水线

    Two-stage Pipeline for Multilingual Dialect Detection. (arXiv:2303.03487v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.03487](http://arxiv.org/abs/2303.03487)

    本篇论文提出了一种两阶段的方言识别系统，在VarDial 2023中超越其他参与者的系统，对多语言方言检测有重要贡献。

    

    方言识别对于本地化各种大型语言模型至关重要。本文概述了我们在VarDial 2023共享任务中的方法。我们必须从三种语言中识别出三个或两个方言，这导致了Track-1的9路分类和Track-2的6路分类。我们提出的方法包括两个阶段的系统，在这个领域中超越了其他参与者的系统和以前的工作。我们在Track-1和Track-2上分别获得58.54％和85.61％的得分。我们的代码库是公开的（https://github.com/ankit-vaidya19/EACL_VarDial2023）。

    Dialect Identification is a crucial task for localizing various Large Language Models. This paper outlines our approach to the VarDial 2023 shared task. Here we have to identify three or two dialects from three languages each which results in a 9-way classification for Track-1 and 6-way classification for Track-2 respectively. Our proposed approach consists of a two-stage system and outperforms other participants' systems and previous works in this domain. We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase is available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).
    
[^62]: EvoPrompting: 适用于代码级神经架构搜索的语言模型

    EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2302.14838](http://arxiv.org/abs/2302.14838)

    EvoPrompting利用语言模型作为自适应变异和交叉操作符来进行神经架构搜索，在MNIST-1D数据集和CLRS算法推理基准上都取得了比人类设计的架构更好的性能表现。

    

    鉴于语言模型（LM）在代码生成方面的最新成就，我们探索将LM作为进化神经架构搜索（NAS）算法的自适应变异和交叉操作符的使用。尽管NAS仍然过于困难，以至于仅仅通过提示就难以成功，但我们发现进化提示工程与软提示调整的组合，一种我们称之为EvoPrompting的方法，始终可以发现多样化且性能高的模型。我们首先证明EvoPrompting在MNIST-1D数据集上是有效的，其中EvoPrompting产生的卷积架构变体在准确率和模型大小方面均优于人类专家设计的架构和天真的少数先导提示。然后，我们将我们的方法应用于在CLRS算法推理基准上搜索图神经网络，其中EvoPrompting能够设计出比当前最先进的模型更好的新颖结构。

    Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
    
[^63]: 生成可逆量子神经网络

    Generative Invertible Quantum Neural Networks. (arXiv:2302.12906v2 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2302.12906](http://arxiv.org/abs/2302.12906)

    本论文提出了一种用于生成可逆量子神经网络的算法，并将其应用于LHC数据的处理，结果表明该算法可以在学习和生成复杂数据方面与经典算法的表现相匹配。

    

    可逆神经网络已成为模拟和生成高度复杂数据的工具。我们提出了一种量子门算法用于量子可逆神经网络（QINN），并将其应用于将衰变为轻子的Z玻色子的喷注相关产生的LHC数据，这是粒子对撞机精密测量的标准过程。我们比较了QINN在不同损失函数和训练场景下的表现。对于这个任务，我们发现一个混合的QINN可以在学习和生成复杂数据方面与一个显著更大的完全经典的INN的表现匹配。

    Invertible Neural Networks (INN) have become established tools for the simulation and generation of highly complex data. We propose a quantum-gate algorithm for a Quantum Invertible Neural Network (QINN) and apply it to the LHC data of jet-associated production of a Z-boson that decays into leptons, a standard candle process for particle collider precision measurements. We compare the QINN's performance for different loss functions and training scenarios. For this task, we find that a hybrid QINN matches the performance of a significantly larger purely classical INN in learning and generating complex data.
    
[^64]: 强化学习在宏观布局中的评估

    Assessment of Reinforcement Learning for Macro Placement. (arXiv:2302.11014v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11014](http://arxiv.org/abs/2302.11014)

    本论文提供了基于强化学习的宏观布局方法以及Circuit Training (CT)实现的开源代码和评估。研究人员评估了CT相对于多个可替代的宏观布局方法，并进行了学术性混合尺寸布局基准测试和消融和稳定性研究，为未来的相关研究提供了方向。

    

    我们提供了Google Brain深度强化学习方法在宏观布局及其Circuit Training (CT)实现的开放透明实现和评估，并在GitHub中实现了CT的关键"黑盒"元素，澄清了CT与Nature论文之间的差异。我们开发并发布了新的对开放实现的测试用例。我们评估了CT及多个可替代的宏观布局方法，所有的评估流程和相关脚本都在GitHub上公开。我们的实验还包括了学术性混合尺寸布局基准测试，以及消融和稳定性研究。我们评论了Nature和CT的影响，以及未来研究的方向。

    We provide open, transparent implementation and assessment of Google Brain's deep reinforcement learning approach to macro placement and its Circuit Training (CT) implementation in GitHub. We implement in open source key "blackbox" elements of CT, and clarify discrepancies between CT and Nature paper. New testcases on open enablements are developed and released. We assess CT alongside multiple alternative macro placers, with all evaluation flows and related scripts public in GitHub. Our experiments also encompass academic mixed-size placement benchmarks, as well as ablation and stability studies. We comment on the impact of Nature and CT, as well as directions for future research.
    
[^65]: 关于深度神经网络功能耦合水印的研究

    On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10296](http://arxiv.org/abs/2302.10296)

    本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。

    

    良好表现的深度神经网络通常需要海量标记数据和计算资源进行训练。为了保护这些知识产权，提出了各种水印技术，其中DNN提供商将秘密信息植入模型中，以便在稍后通过一些专用触发输入检索嵌入的水印索权；虽然文献中报告了有希望的结果，但现有解决方案仍然遭受水印删除攻击，例如模型微调和模型修剪。本文提出了一种新颖的DNN水印方案，可以有效地防御上述攻击。我们的关键洞察力是增强水印和模型功能的耦合，这样删除水印会不可避免地降低模型在常规输入上的性能。为此，与先前依赖于来自超出分布数据的秘密特征的方法不同，我们的方法仅使用从训练数据中学习的特征。

    Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
    
[^66]: 基于Swin Transformer的视频运动放大技术

    STB-VMM: Swin Transformer Based Video Motion Magnification. (arXiv:2302.10001v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10001](http://arxiv.org/abs/2302.10001)

    本文提出了一个基于Swin Transformer的新技术，延续了视频运动放大技术的研究，提高了输出图像的质量，具有更好的容错能力和更少的噪声、模糊和伪影，可用于提高放大视频序列的精确度，促进视频运动放大技术在新领域的发展。

    

    视频运动放大技术的目的是放大视频中的微小运动，以揭示以前看不见的运动。它的用途从生物医学应用和深度伪造检测到结构模态分析和预测维护等领域。然而，从噪声中分辨出微小运动是一项复杂的任务，尤其是当试图放大非常微妙的、往往是亚像素运动时。因此，运动放大技术通常遭受嘈杂和模糊的输出。本文提出了一个基于Swin Transformer的最新模型，该模型对噪声输入具有更好的容错能力，输出的质量比先前的技术展现出更少的噪声、模糊和伪影。输出图像质量的改进将使任何依赖放大视频序列的应用得到更精确的测量，并可能使视频运动放大技术在新的技术领域得到进一步发展。

    The goal of video motion magnification techniques is to magnify small motions in a video to reveal previously invisible or unseen movement. Its uses extend from bio-medical applications and deepfake detection to structural modal analysis and predictive maintenance. However, discerning small motion from noise is a complex task, especially when attempting to magnify very subtle, often sub-pixel movement. As a result, motion magnification techniques generally suffer from noisy and blurry outputs. This work presents a new state-of-the-art model based on the Swin Transformer, which offers better tolerance to noisy inputs as well as higher-quality outputs that exhibit less noise, blurriness, and artifacts than prior-art. Improvements in output image quality will enable more precise measurements for any application reliant on magnified video sequences, and may enable further development of video motion magnification techniques in new technical fields.
    
[^67]: 可缝合神经网络

    Stitchable Neural Networks. (arXiv:2302.06586v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06586](http://arxiv.org/abs/2302.06586)

    本文提出了一个名为SN-Net的框架，它可以便宜地产生许多不同复杂度和性能权衡的网络，利用预先训练的神经网络家族作为锚点，并使用简单的缝合层将它们拼接在一起以实现动态的精度-效率权衡。

    

    具有巨大威力的预训练模型集群(如ResNet/DeiT)所构成的公共模型库已经达到了前所未有的范围，这在很大程度上促进了深度学习的成功。然而，每个模型系列都包含着不同规模的预训练模型(比如DeiT-Ti/S/B)，这自然地引出了一个基本问题：如何在运行时有效地组合这些可用的模型系列以实现动态的精度-效率权衡。针对这个问题，我们提出了Stitchable Neural Networks (SN-Net)，这是一个新颖的可扩展、高效的模型部署框架。在一个预先训练的神经网络家族中，它可以便宜地产生许多不同复杂度和性能权衡的网络，我们称之为锚点。具体来说，SN-Net将锚点分散在块/层之间，然后使用简单的缝合层将它们拼接在一起，以映射一个锚点的激活到另一个锚点。仅仅通过几个轮次的训练，SN-Net可以有效地插值网络。

    The public model zoo containing enormous powerful pretrained model families (e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which significantly contributes to the success of deep learning. As each model family consists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it naturally arises a fundamental question of how to efficiently assemble these readily available models in a family for dynamic accuracy-efficiency trade-offs at runtime. To this end, we present Stitchable Neural Networks (SN-Net), a novel scalable and efficient framework for model deployment. It cheaply produces numerous networks with different complexity and performance trade-offs given a family of pretrained neural networks, which we call anchors. Specifically, SN-Net splits the anchors across the blocks/layers and then stitches them together with simple stitching layers to map the activations from one anchor to another. With only a few epochs of training, SN-Net effectively interpolates 
    
[^68]: 神经对话辅导中的机遇与挑战

    Opportunities and Challenges in Neural Dialog Tutoring. (arXiv:2301.09919v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.09919](http://arxiv.org/abs/2301.09919)

    本文研究了神经对话辅导存在的机遇和挑战，发现当前方法在少量概念和可能的教师策略的情况下可以进行较好的辅导模拟与学习，但在不受限制的情况下表现不佳，未来应该集中在解决这些问题上。

    

    设计对话辅导系统一直是一项具有挑战性的工作，因为它涉及到对人类辅导者所采用的多样且复杂的教学策略进行建模。尽管在大型语言模型 (LLMs) 和可用的对话语料库方面出现了显著的进展，但对话辅导在很大程度上仍未受到这些进展的影响。本文在两个语言学习对话辅导数据集上对各种生成式语言模型进行了严格分析，使用自动和人工评估来了解这些进展带来的新机会以及我们必须克服的挑战，以构建能在真实教育环境中使用的模型。我们发现，尽管当前方法可以对少量概念和可能的教师策略进行较好的辅导模拟与学习，但在不受限制的情况下表现不佳。我们的人工质量评估显示，模型和基础教学系统在这些不受限制的情况下都具有较低的有效性，这表明未来的研究应该集中在解决这些问题上。

    Designing dialog tutors has been challenging as it involves modeling the diverse and complex pedagogical strategies employed by human tutors. Although there have been significant recent advances in neural conversational systems using large language models (LLMs) and growth in available dialog corpora, dialog tutoring has largely remained unaffected by these advances. In this paper, we rigorously analyze various generative language models on two dialog tutoring datasets for language learning using automatic and human evaluations to understand the new opportunities brought by these advances as well as the challenges we must overcome to build models that would be usable in real educational settings. We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios. Our human quality evaluation shows that both models and ground-tr
    
[^69]: 从餐盘到预防：新加坡健康促进的膳食营养辅助平台

    From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore. (arXiv:2301.03829v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03829](http://arxiv.org/abs/2301.03829)

    本文介绍了一个膳食营养辅助平台，该平台开发了一个本地化的新加坡食品数据集，旨在通过监管和监督人们的营养摄入，为新加坡人的健康促进提供医学级别的营养信息。

    

    新加坡一直致力于改善人民的医疗保健服务。政府注意到了监管和监督人们摄入营养的不足，这被认为是慢性疾病发展的一个因素。因此，这个问题引起了很大的关注。本文分享了新加坡解决这个问题和获取医疗级别营养摄入信息以在不同方面造福新加坡人的经验。为此，我们开发了FoodSG平台来孵化多样化的面向医疗保健的应用服务在新加坡，同时考虑到它们的共同要求。我们进一步确定了本地化食品数据集的深远意义，并系统地清理和筛选了一个本地化的新加坡食品数据集FoodSG-233。为了克服由新加坡多样化食品菜肴带来的识别性能障碍，我们提议整合监督式对比学习。

    Singapore has been striving to improve the provision of healthcare services to her people. In this course, the government has taken note of the deficiency in regulating and supervising people's nutrient intake, which is identified as a contributing factor to the development of chronic diseases. Consequently, this issue has garnered significant attention. In this paper, we share our experience in addressing this issue and attaining medical-grade nutrient intake information to benefit Singaporeans in different aspects. To this end, we develop the FoodSG platform to incubate diverse healthcare-oriented applications as a service in Singapore, taking into account their shared requirements. We further identify the profound meaning of localized food datasets and systematically clean and curate a localized Singaporean food dataset FoodSG-233. To overcome the hurdle in recognition performance brought by Singaporean multifarious food dishes, we propose to integrate supervised contrastive learnin
    
[^70]: 梯度过滤技术实现高效的设备端训练

    Efficient On-device Training via Gradient Filtering. (arXiv:2301.00330v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00330](http://arxiv.org/abs/2301.00330)

    本文提出一种新的梯度过滤方法，通过创建具有较少唯一元素的特殊结构来实现设备端卷积神经网络模型的训练，从而大大减少了计算复杂度和内存消耗，实现最高19倍的训练加速度。

    

    尽管在联邦学习、连续学习和其他许多应用中很重要，但设备端训练仍然是EdgeAI的一个难题。本文提出了一种新的梯度过滤技术，使得设备端的卷积神经网络模型训练成为可能。该方法通过创建具有较少唯一元素的特殊结构，从而显著减少了训练期间反向传播的计算复杂度和内存消耗。在多个CNN模型（例如MobileNet、DeepLabV3、UPerNet）和设备（例如Raspberry Pi和Jetson Nano）上进行的图像分类和语义分割的大量实验表明了我们方法的有效性和广泛适用性。例如，与SOTA相比，我们实现了高达19倍的训练加速度。

    Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\times$ speedu
    
[^71]: 一种预测Few-Shot分类泛化的统计模型

    A Statistical Model for Predicting Generalization in Few-Shot Classification. (arXiv:2212.06461v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06461](http://arxiv.org/abs/2212.06461)

    提出了一种通过高斯模型估计特征分布参数进行预测泛化误差的方法，通过计算类条件密度距离估计可以提高泛化性能准确度。

    

    分类器泛化误差的估计通常依赖于验证集。然而，在Few-Shot学习场景中，很难获得这样的验证集，这是该领域中一个高度被忽视的缺点。因此，在这项工作中，我们引入了一个特征分布的高斯模型，通过估计这个模型的参数，我们能够预测在新的Few-Shot分类任务中的分类性能。我们发现，在类条件密度之间准确的距离估计是准确评估泛化性能的关键。因此，我们提出了一个非偏估计器来计算这些距离，并将其集成到我们的数值分析中。我们通过实验证明，我们的方法胜过了其他方法，例如留一法-Cross Validation 策略。

    The estimation of the generalization error of classifiers often relies on a validation set. Such a set is hardly available in few-shot learning scenarios, a highly disregarded shortcoming in the field. In these scenarios, it is common to rely on features extracted from pre-trained neural networks combined with distance-based classifiers such as nearest class mean. In this work, we introduce a Gaussian model of the feature distribution. By estimating the parameters of this model, we are able to predict the generalization error on new classification tasks with few samples. We observe that accurate distance estimates between class-conditional densities are the key to accurate estimates of the generalization performance. Therefore, we propose an unbiased estimator for these distances and integrate it in our numerical analysis. We empirically show that our approach outperforms alternatives such as the leave-one-out cross-validation strategy.
    
[^72]: 通过深度各向异性扩散进行引导的深度超分辨率

    Guided Depth Super-Resolution by Deep Anisotropic Diffusion. (arXiv:2211.11592v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11592](http://arxiv.org/abs/2211.11592)

    本研究提出了一种结合了各向异性扩散和深度卷积网络的新方法，用于引导深度超分辨率，取得了前所未有的结果，在x32缩放下的性能提升最大。

    

    利用RGB图像的指导实现深度图像的超分辨率是涉及到机器人，医学成像和遥感等多个领域的问题。尽管深度学习方法在此问题上取得了良好的结果，但最近的研究突显了将现代方法与更为正式的框架相结合的价值。在本文中，我们提出了一种新颖的方法，将引导各向异性扩散与深度卷积网络相结合，推进了引导深度超分辨率的状态。扩散的边缘转移/增强特性由现代网络的上下文推理能力加强，严格的调整步骤确保完全粘合到源图像上。我们在三个常用的引导深度超分辨率基准测试中实现了前所未有的结果。与其他方法相比，性能在较大比例尺下，例如x32缩放时获得了最大的提升。

    Performing super-resolution of a depth image using the guidance from an RGB image is a problem that concerns several fields, such as robotics, medical imaging, and remote sensing. While deep learning methods have achieved good results in this problem, recent work highlighted the value of combining modern methods with more formal frameworks. In this work, we propose a novel approach which combines guided anisotropic diffusion with a deep convolutional network and advances the state of the art for guided depth super-resolution. The edge transferring/enhancing properties of the diffusion are boosted by the contextual reasoning capabilities of modern networks, and a strict adjustment step guarantees perfect adherence to the source image. We achieve unprecedented results in three commonly used benchmarks for guided depth super-resolution. The performance gain compared to other methods is the largest at larger scales, such as x32 scaling. Code (https://github.com/prs-eth/Diffusion-Super-Reso
    
[^73]: UMFuse：用于人体编辑应用的统一多视图融合技术

    UMFuse: Unified Multi View Fusion for Human Editing applications. (arXiv:2211.10157v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10157](http://arxiv.org/abs/2211.10157)

    本文提出了UMFuse，一种利用多视图融合处理人体编辑任务的方法，通过设计一个多视图融合网络，利用多源图像中的关键点和纹理生成每像素外观检索映射，从而最小化信息丢失并生成准确的下层人体模型。

    

    对于人体编辑技术，视觉社区已经探讨了众多的姿势引导方法，因为它们有广泛的实际应用。然而，大多数方法仍然使用图像到图像的公式，其中一个单一的图像被给定为输入，以产生一个编辑过的图像为输出。当目标姿势与输入姿势有显著差异时，这一目标变得不明确。现有的方法则采用修补或样式转移来处理遮挡并保留内容。在本文中，我们探讨了利用多个视图来最小化信息丢失并生成下层人体模型的准确表示。为了融合多个视点的知识，我们设计了一个多视图融合网络，它从多个源图像中获取姿势关键点和纹理，并生成可解释的每像素外观检索映射。此后，从单视角人体换装任务中训练得到的编码被合并起来。

    Numerous pose-guided human editing methods have been explored by the vision community due to their extensive practical applications. However, most of these methods still use an image-to-image formulation in which a single image is given as input to produce an edited image as output. This objective becomes ill-defined in cases when the target pose differs significantly from the input pose. Existing methods then resort to in-painting or style transfer to handle occlusions and preserve content. In this paper, we explore the utilization of multiple views to minimize the issue of missing information and generate an accurate representation of the underlying human model. To fuse knowledge from multiple viewpoints, we design a multi-view fusion network that takes the pose key points and texture from multiple source images and generates an explainable per-pixel appearance retrieval map. Thereafter, the encodings from a separate network (trained on a single-view human reposing task) are merged i
    
[^74]: ERNIE-ViLG 2.0：基于知识增强的去噪二次混合模型优化文本到图像模型

    ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts. (arXiv:2210.15257v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.15257](http://arxiv.org/abs/2210.15257)

    本文提出了ERNIE-ViLG 2.0，一种基于知识增强的去噪二次混合模型，通过合并场景关键元素文本和视觉知识以及使用不同的去噪专家的方法，成功提高了中文文本到图像模型的质量和图像-文本对齐的表现。

    

    最近弥漫模型在文本到图像生成领域取得了突破性进展。尽管现有的方法能够在文本条件下产生高分辨率的逼真图像，但仍存在一些需要解决的问题，这限制了图像保真度和文本相关性的进一步提高。本文提出了ERNIE-ViLG 2.0，一种大规模的中文文本到图像扩散模型，通过以下方式逐步提高生成图像的质量：（1）合并场景中关键元素的细粒度文本和视觉知识，（2）在不同的去噪阶段利用不同的去噪专家。通过这些机制，ERNIE-ViLG 2.0不仅在MS-COCO上实现了新的FID得分零射击最佳表现，而且在ViLG-300双语提示集的人类评估中在图像保真度和图像-文本对齐方面显著优于最新的模型。

    Recent progress in diffusion models has revolutionized the popular technology of text-to-image generation. While existing approaches could produce photorealistic high-resolution images with text conditions, there are still several open problems to be solved, which limits the further improvement of image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, to progressively upgrade the quality of generated images by: (1) incorporating fine-grained textual and visual knowledge of key elements in the scene, and (2) utilizing different denoising experts at different denoising stages. With the proposed mechanisms, ERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO with zero-shot FID score of 6.75, but also significantly outperforms recent models in terms of image fidelity and image-text alignment, with side-by-side human evaluation on the bilingual prompt set ViLG-300.
    
[^75]: 任务阶段化：来自示范的自动课程学习

    Task Phasing: Automated Curriculum Learning from Demonstrations. (arXiv:2210.10999v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10999](http://arxiv.org/abs/2210.10999)

    本文介绍了一种结合了示范学习和课程学习的任务阶段化方法，使用逆强化学习自动生成课程序列，逐步增加任务复杂度，以帮助解决强化学习在稀疏奖励领域中的挑战。

    

    将强化学习（RL）应用于稀疏奖励域通常具有挑战性，因为缺乏足够的引导信号。解决此类领域的常见RL技术包括（1）从示范学习和（2）课程学习。虽然这两种方法已经被详细研究，但它们很少被同时考虑。本文旨在通过引入一种基于示范的原则性任务阶段化方法来实现该目的，该方法使用示范自动生成课程序列。我们使用来自（次优）演示的逆RL定义了一个简单的初始任务。然后，我们的任务阶段化方法提供了一个框架，逐渐增加任务的复杂性，直到目标任务，同时在每个阶段迭代中重新调整RL代理。考虑了两种分阶段方法：（1）逐步增加RL代理处于控制下的时间步数的比例，以及（2）逐步淘汰引导性信息奖励函数。我们提出了保证收敛的条件。

    Applying reinforcement learning (RL) to sparse reward domains is notoriously challenging due to insufficient guiding signals. Common RL techniques for addressing such domains include (1) learning from demonstrations and (2) curriculum learning. While these two approaches have been studied in detail, they have rarely been considered together. This paper aims to do so by introducing a principled task phasing approach that uses demonstrations to automatically generate a curriculum sequence. Using inverse RL from (suboptimal) demonstrations we define a simple initial task. Our task phasing approach then provides a framework to gradually increase the complexity of the task all the way to the target task, while retuning the RL agent in each phasing iteration. Two approaches for phasing are considered: (1) gradually increasing the proportion of time steps an RL agent is in control, and (2) phasing out a guiding informative reward function. We present conditions that guarantee the convergence 
    
[^76]: 一种用于住宅短期负荷预测的安全联邦学习框架

    A Secure Federated Learning Framework for Residential Short Term Load Forecasting. (arXiv:2209.14547v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.14547](http://arxiv.org/abs/2209.14547)

    本论文提出了一种安全的联邦学习框架，能够在确保个人数据隐私的同时，提高联邦短期负荷预测对于拜占庭威胁的鲁棒性。

    

    智能电表是准确需求预测的关键，但由于消费者隐私和数据泄露等问题，存在若干缺点。最近的文献中，联邦学习（FL）被探索作为一种有前途的保护隐私的机器学习替代方案，它可以在不暴露私有原始数据的情况下进行模型的协作学习，用于短期负荷预测。尽管FL具有优点，但标准FL仍然易受名为拜占庭攻击的难以处理的网络威胁的影响，这种攻击是由有缺陷和/或恶意客户发起的。因此，为了提高联邦短期负荷预测对拜占庭威胁的鲁棒性，我们开发了一个最先进的差分隐私安全的FL-based框架，该框架确保了个人智能电表数据的隐私，同时保护了FL模型和架构的安全。我们的提议框架利用了通过Sign Stochastic Gradient Descent（SignSGD）算法进行梯度量化的思想。

    Smart meter measurements, though critical for accurate demand forecasting, face several drawbacks including consumers' privacy, data breach issues, to name a few. Recent literature has explored Federated Learning (FL) as a promising privacy-preserving machine learning alternative which enables collaborative learning of a model without exposing private raw data for short term load forecasting. Despite its virtue, standard FL is still vulnerable to an intractable cyber threat known as Byzantine attack carried out by faulty and/or malicious clients. Therefore, to improve the robustness of federated short-term load forecasting against Byzantine threats, we develop a state-of-the-art differentially private secured FL-based framework that ensures the privacy of the individual smart meter's data while protect the security of FL models and architecture. Our proposed framework leverages the idea of gradient quantization through the Sign Stochastic Gradient Descent (SignSGD) algorithm, where the
    
[^77]: 法律引导代码：一种法律信息学方法来使人工智能与人类保持一致

    Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2209.13020](http://arxiv.org/abs/2209.13020)

    这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。

    

    目前我们无法可靠地指定人类的目标和社会价值，以引导人工智能的行为。制定法律和解释法律构成了一种计算引擎，将不透明的人类价值转化为易读的指令。 “法律指导代码”是嵌入了法律知识和推理的人工智能研究议程。类似于合同当事人无法预见他们未来关系的每个潜在变数，立法者无法预测其提出的法案将适用的所有情况，我们无法提前明确规则，以可靠地引导良好的人工智能行为。法律理论和实践已经开发出各种工具来解决这些规定问题。与法律更为普通的用途（例如通过制裁威胁来阻止不良行为）相反，法律作为一种表达人类沟通目标和价值的表现，可以引导人工智能代码的发展方向。具体而言，法律信息学是计算规则和系统用于表示，分析和操作法律知识的跨学科研究。法律指导代码利用法律信息学来使人工智能与人类的目标和社会价值保持一致，以一种透明，负责，灵活的方式。

    We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
    
[^78]: 安全和协安全语言的一阶逻辑特征

    A first-order logic characterization of safety and co-safety languages. (arXiv:2209.02307v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.02307](http://arxiv.org/abs/2209.02307)

    本论文提出了一些新的语言类型和算法，可以降低模型检测和反应合成等问题的复杂度。

    

    线性时间逻辑（LTL）是最受欢迎的时间逻辑之一，广泛应用于计算机科学的各个领域。LTL的强大基础性质之一是其等价于无计数ω-自动机、星自由ω-正则表达式以及（通过Kamp定理）线性序的一阶理论（FO-TLO）。安全语言和协安全语言分别指只需有限前缀便可确定该单词属于或不属于该语言的语言类型。SafetyLTL（和coSafetyLTL）是LTL的一个片段，其中仅允许使用全局（存在）时间修饰词来识别安全（协安全）语言。本文的主要贡献是引入FO-TLO的片段SafetyFO以及其对偶的coSafetyFO，它们在表达能力上都是完备的（except一些边界情况）。

    Linear Temporal Logic (LTL) is one of the most popular temporal logics, that comes into play in a variety of branches of computer science. Among the various reasons of its widespread use there are its strong foundational properties: LTL is equivalent to counter-free omega-automata, to star-free omega-regular expressions, and (by Kamp's theorem) to the First-Order Theory of Linear Orders (FO-TLO). Safety and co-safety languages, where a finite prefix suffices to establish whether a word does not belong or belongs to the language, respectively, play a crucial role in lowering the complexity of problems like model checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL) is a fragment of LTL where only universal (resp., existential) temporal modalities are allowed, that recognises safety (resp., co-safety) languages only. The main contribution of this paper is the introduction of a fragment of FO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressively comple
    
[^79]: 基于时间序列的数据增强技术：一份综述和分类

    Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13508](http://arxiv.org/abs/2206.13508)

    本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。

    

    随着深度学习建模的最新进展，利用其在时间序列领域中出色性能的方式并不需要太长时间。深度神经网络在处理时间序列方面严重依赖于训练中使用的数据集的大小和一致性。这些特征通常在现实世界中并不丰富，通常受到限制和需要保证的约束。因此，提高数据量的有效方法是使用数据增强技术，无论是通过添加噪声或置换还是生成新的合成数据。本文系统地回顾了该领域中的最新技术现状，提供了所有可用算法的概述，并提出了最相关研究的分类法。不同变体的效率将作为该过程的中心部分进行评估，同时还将评估不同的性能指标以及每个模型的主要问题。

    With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
    
[^80]: 面向XR应用的边缘AI硬件记忆导向设计空间探索

    Memory-Oriented Design-Space Exploration of Edge-AI Hardware for XR Applications. (arXiv:2206.06780v3 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2206.06780](http://arxiv.org/abs/2206.06780)

    本文研究了面向XR应用的边缘AI硬件设计空间。通过引入新型非易失性内存技术，在满足最低IPS的同时显著提高了能效，可减少90%的内存流量。

    

    低功耗的边缘AI能力对于支持“元宇宙”愿景的设备上的扩展现实（XR）应用至关重要。本文研究了两种典型的XR工作负载：（i）手部检测和（ii）眼睛分割，并进行了硬件设计空间探索。对于这两个应用，我们训练深度神经网络，并分析了量化和硬件特定瓶颈的影响。通过模拟，我们评估了一个CPU和两个系统推理加速器实现。接下来，我们将这些硬件解决方案与先进的技术节点进行比较。评估了将最先进的新型非易失性内存技术（STT / SOT / VGSOT MRAM）集成到XR-AI推理管道中的影响。我们发现，在7纳米节点的设计中引入非易失性内存到内存层次结构中可以实现显着的能源效益（>=24％）以支持手部检测（IPS = 10）和眼睛分割（IPS = 0.1），同时满足最小IPS（每秒推理次数）。此外，与仅使用SRAM的设计相比，我们几乎可以减少90％的内存流量。我们的工作为基于内存的边缘AI设计的XR应用提供了深入的见解，并确定了新兴非易失性内存技术的潜力。

    Low-Power Edge-AI capabilities are essential for on-device extended reality (XR) applications to support the vision of Metaverse. In this work, we investigate two representative XR workloads: (i) Hand detection and (ii) Eye segmentation, for hardware design space exploration. For both applications, we train deep neural networks and analyze the impact of quantization and hardware specific bottlenecks. Through simulations, we evaluate a CPU and two systolic inference accelerator implementations. Next, we compare these hardware solutions with advanced technology nodes. The impact of integrating state-of-the-art emerging non-volatile memory technology (STT/SOT/VGSOT MRAM) into the XR-AI inference pipeline is evaluated. We found that significant energy benefits (>=24%) can be achieved for hand detection (IPS=10) and eye segmentation (IPS=0.1) by introducing non-volatile memory in the memory hierarchy for designs at 7nm node while meeting minimum IPS (inference per second). Moreover, we can 
    
[^81]: CGC: 对比图聚类用于社区发现和跟踪

    CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2204.08504](http://arxiv.org/abs/2204.08504)

    本文提出了一种全新的图聚类算法CGC，采用对比学习进行自监督表示学习，结合跟踪模块以应对动态图拓扑变化，在社区发现和跟踪方面表现出领先的状态。

    

    本文从图聚类的角度入手，探讨在网络数据中发现实体和实体之间的交互以及对它们进行社区跟踪的方法。我们提出了一种全新的端到端框架CGC，该方法利用对比学习范式进行自监督表示学习，并采用了跟踪模块以应对不断变化的图形拓扑。在各个真实场景和合成基准上，我们对CGC进行了评估，并在社区发现和跟踪方面展示出了卓越的性能，尤其在动态图上表现出了领先的状态。

    Given entities and their interactions in the web data, which may have occurred at different time, how can we find communities of entities and track their evolution? In this paper, we approach this important task from graph clustering perspective. Recently, state-of-the-art clustering performance in various domains has been achieved by deep clustering methods. Especially, deep graph clustering (DGC) methods have successfully extended deep clustering to graph-structured data by learning node representations and cluster assignments in a joint optimization framework. Despite some differences in modeling choices (e.g., encoder architectures), existing DGC methods are mainly based on autoencoders and use the same clustering objective with relatively minor adaptations. Also, while many real-world graphs are dynamic, previous DGC methods considered only static graphs. In this work, we develop CGC, a novel end-to-end framework for graph clustering, which fundamentally differs from existing meth
    
[^82]: 优化广义基尼指数实现排名公平性

    Optimizing generalized Gini indices for fairness in rankings. (arXiv:2204.06521v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.06521](http://arxiv.org/abs/2204.06521)

    本文探讨了使用广义基尼福利函数（GGF）作为规范性准则来指定推荐系统应优化的方法，以此实现排名公平性。

    

    越来越多的人关注设计能够对物品生产者或最不满意用户公平的推荐系统。受经济学不平等测量领域的启发，本文探讨了使用广义基尼福利函数（GGF）作为规范性准则来指定推荐系统应优化的方法。GGF根据人口普查中的排名对个体进行加权，将更多的权重放在处境较差的个体上以促进平等。根据这些权重，GGF最小化物品曝光的基尼指数，以促进物品之间的平等，或关注最不满意用户的特定分位数的性能。排名的GGF难以优化，因为它们是不可微分的。我们通过利用非平滑优化和可微排序中使用的投影算子来解决这个挑战。我们使用最多有15k个用户和物品的真实数据集进行实验，结果表明我们的方法可以通过优化GGF有效地促进排名公平性。

    There is growing interest in designing recommender systems that aim at being fair towards item producers or their least satisfied users. Inspired by the domain of inequality measurement in economics, this paper explores the use of generalized Gini welfare functions (GGFs) as a means to specify the normative criterion that recommender systems should optimize for. GGFs weight individuals depending on their ranks in the population, giving more weight to worse-off individuals to promote equality. Depending on these weights, GGFs minimize the Gini index of item exposure to promote equality between items, or focus on the performance on specific quantiles of least satisfied users. GGFs for ranking are challenging to optimize because they are non-differentiable. We resolve this challenge by leveraging tools from non-smooth optimization and projection operators used in differentiable sorting. We present experiments using real datasets with up to 15k users and items, which show that our approach
    
[^83]: SERA: 用于协作机器人规划的安全高效反应式障碍物避免方法在非结构化环境中（arXiv:2203.13821v2 [cs.RO] UPDATED）

    SERA: Safe and Efficient Reactive Obstacle Avoidance for Collaborative Robotic Planning in Unstructured Environments. (arXiv:2203.13821v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.13821](http://arxiv.org/abs/2203.13821)

    提出了一种新的方法，利用最新的深度学习进展和拓扑流形学习，实现了反应式全身障碍物避免，可避开任意形状的障碍物，并能轻松地推广到其他问题设置中。

    

    在工业4.0时代, 多个机器人在非结构化环境中实现安全高效的协作日益关键。然而, 实现人类和其他机器人之间的强健自治协作需要现代机器人系统具备有效的邻近感知和反应式障碍物避免能力。在本文中，我们提出了一种新颖的反应式全身障碍物避免方法，即使在动态环境中也可以确保无冲突的机器人间互动。与现有的基于雅各比类型、采样或几何技术的方法不同，我们的方法利用了最新的深度学习进展和拓扑流形学习，使其能够轻松地推广到其他具有高计算效率和快速图遍历技术的问题设置中。我们的方法允许机械臂在没有直接接触的情况下主动避开任意3D形状的障碍物，这是传统工业协作机器人设置的重要改进。

    Safe and efficient collaboration among multiple robots in unstructured environments is increasingly critical in the era of Industry 4.0. However, achieving robust and autonomous collaboration among humans and other robots requires modern robotic systems to have effective proximity perception and reactive obstacle avoidance. In this paper, we propose a novel methodology for reactive whole-body obstacle avoidance that ensures conflict-free robot-robot interactions even in dynamic environment. Unlike existing approaches based on Jacobian-type, sampling based or geometric techniques, our methodology leverages the latest deep learning advances and topological manifold learning, enabling it to be readily generalized to other problem settings with high computing efficiency and fast graph traversal techniques. Our approach allows a robotic arm to proactively avoid obstacles of arbitrary 3D shapes without direct contact, a significant improvement over traditional industrial cobot settings. To v
    
[^84]: FedREP：面向零售能源供应商的水平联邦负荷预测

    FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers. (arXiv:2203.00219v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2203.00219](http://arxiv.org/abs/2203.00219)

    FedREP 提出了一种新颖的水平隐私保护联邦学习框架，使用多个 REP 构建一个共同的、强大的机器学习模型来实现能源负载消耗预测。

    

    随着智能电表收集和传输家庭能源消耗数据给零售能源供应商（REP），最大的挑战是确保有效利用细粒度的消费者数据的同时保护数据隐私。本文针对能源负载消耗预测，解决了这一问题，这对于能源需求管理、负载切换和基础设施发展非常重要。具体而言，现有的能源负载预测是集中的，不可扩展，最重要的是容易遭受数据隐私威胁。此外，REP 是各自的市场参与者，并有责任保护自己客户的隐私。为解决这个问题，我们提出了一种新颖的水平隐私保护联邦学习框架，即 FedREP。我们考虑了一个联邦学习系统，由一个控制中心和多个零售商组成，通过启用多个 REP 构建一个共同的、强大的机器学习模型来解决问题。

    As Smart Meters are collecting and transmitting household energy consumption data to Retail Energy Providers (REP), the main challenge is to ensure the effective use of fine-grained consumer data while ensuring data privacy. In this manuscript, we tackle this challenge for energy load consumption forecasting in regards to REPs which is essential to energy demand management, load switching and infrastructure development. Specifically, we note that existing energy load forecasting is centralized, which are not scalable and most importantly, vulnerable to data privacy threats. Besides, REPs are individual market participants and liable to ensure the privacy of their own customers. To address this issue, we propose a novel horizontal privacy-preserving federated learning framework for REPs energy load forecasting, namely FedREP. We consider a federated learning system consisting of a control centre and multiple retailers by enabling multiple REPs to build a common, robust machine learning 
    
[^85]: 基于Mapper类型算法的复杂数据和关系处理

    Mapper-type algorithms for complex data and relations. (arXiv:2109.00831v2 [math.AT] UPDATED)

    [http://arxiv.org/abs/2109.00831](http://arxiv.org/abs/2109.00831)

    本文提出了一种基于 Mapper 和 Ball Mapper 的混合算法，命名为 Mapper on Ball Mapper，可以用于处理高维透镜函数。该算法结合了 Mapper 和 Ball Mapper 的优势，能够同时编码点云的结构、内部关系和对称性，并可用于比较单个数据集的高维数据描述符，适用于结论理论、博弈理论、材料科学和癌症研究等领域。

    

    Mapper和Ball Mapper 是一种拓扑数据分析工具，用于探索高维点云，并可视化点云上的标量值函数。本论文在未解决的结论理论问题的启发下，将新功能添加到Ball Mapper中，使其能够编码点云的结构、内部关系和对称性。此外，本文将Mapper和Ball Mapper的优势结合起来，创造了一种比较单个数据集的高维数据描述符的工具——Mapper on Ball Mapper。这种新型混合算法适用于高维透镜函数。作为概念证明，本研究并将应用于结论理论和博弈理论，以及材料科学和癌症研究领域。

    Mapper and Ball Mapper are Topological Data Analysis tools used for exploring high dimensional point clouds and visualizing scalar-valued functions on those point clouds. Inspired by open questions in knot theory, new features are added to Ball Mapper that enable encoding of the structure, internal relations and symmetries of the point cloud. Moreover, the strengths of Mapper and Ball Mapper constructions are combined to create a tool for comparing high dimensional data descriptors of a single dataset. This new hybrid algorithm, Mapper on Ball Mapper, is applicable to high dimensional lens functions. As a proof of concept we include applications to knot and game theory, as well as material science and cancer research.
    
[^86]: 从块-Toeplitz矩阵到图上的微分方程：迈向可扩展的Masked Transformers的通用理论

    From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.07999](http://arxiv.org/abs/2107.07999)

    本文提出了可扩展的方法将各种掩码机制纳入Transformers架构中。通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，提出了高效的d维RPE掩码和图内核掩码。该方法得到了实验证明。

    

    本文提供了迄今为止最全面的方法，以可扩展的方式将各种掩码机制纳入Transformers架构中。我们展示了最近关于线性因果注意力（Choromanski等人，2021）和对数-线性RPE-注意力（Luo等人，2021）的结果是这种一般机制的特例。然而，通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，我们获得了几个以前未知的结果，包括高效的d维RPE掩码和图内核掩码。我们利用许多数学技术，从谱分析、动态规划和随机游走到解决图上马尔可夫过程的新算法。我们提供了相应的实证评估。

    In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.
    
[^87]: NoiseGrad：通过引入模型权重的随机变化来增强解释性

    NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights. (arXiv:2106.10185v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2106.10185](http://arxiv.org/abs/2106.10185)

    本文提出了NoiseGrad方法，通过引入模型权重的随机变化扰动决策边界来增强深度神经网络模型的局部和全局解释方法。

    

    近年来，针对黑匣子学习机的决策过程，如深度神经网络，已经进行了很多工作，从而产生了有用的局部和全局解释方法。本文提出了NoiseGrad，通过在权重参数空间中引入随机性，从而扰动决策边界，增强了局部和全局解释方法。我们将NoiseGrad与其与SmoothGrad的融合方法（FusionGrad）进行了定量和定性评估，并展示了其性能。

    Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad -FusionGrad -- qualitatively and quantitatively with several evaluation criteria, and show that
    
[^88]: 迭代标签清洗用于跨领域少样本学习的半监督算法

    Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.07962](http://arxiv.org/abs/2012.07962)

    该论文提出了一种利用标记和未标记的数据分布的流形结构来预测伪标签，在类别平衡和选择干净标签的基础上，通过迭代清洗标签以提高伪标签质量的算法，在跨领域少样本学习中表现良好，并在四个基准数据集上达到了现有技术水平的最佳效果。

    

    少样本学习涉及到学习表征和获取知识，以使新任务可以在监督和数据都很有限的情况下得到解决。通过横向推断和半监督学习，可以提高性能。我们专注于这两种情况，提出了一种新的算法，利用标记和未标记的数据分布的流形结构来预测伪标签，同时平衡类别并使用有限容量分类器的损失值分布来选择最干净的标签，迭代地提高伪标签的质量。我们的解决方案在四个基准数据集（即 miniImageNet、tieredImageNet、CUB 和 CIFAR-FS）上超过或匹配了现有技术水平，同时在特征空间预处理和可用数据的数量方面具有鲁棒性。公开可用的源代码可在 https://github.c 中找到。

    Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solution surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https://github.c
    

