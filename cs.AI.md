# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability.](http://arxiv.org/abs/2308.09004) | 本论文提出MIDA方法，以多工作流可信度和数据可观测性为基础，实现了轻量级的运行时集成数据分析。该方法通过定义数据可观测策略和适应性方法，可以处理异构科学环境下多个支持工具和高效的HPC执行。 |
| [^2] | [An Extended Convergence Result for Behaviour Tree Controllers.](http://arxiv.org/abs/2308.08994) | 本文研究了行为树控制器的收敛性，广泛推广了早期的结果，并包括了文献中未涵盖的新情况。 |
| [^3] | [A Dual-Perspective Approach to Evaluating Feature Attribution Methods.](http://arxiv.org/abs/2308.08949) | 这篇论文提出了一种双重视角的方法来评估特征归因方法。通过观察扰动归因特征对模型行为的影响，这种方法揭示了归因特征的准确性和完整性，使其能够定量评估特征归因的表现。 |
| [^4] | [Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level.](http://arxiv.org/abs/2308.08948) | 本论文介绍了一种用于农作物产量预测的简单而有效的早期融合方法，可以处理具有不同时间和空间分辨率的多个输入模态。该方法使用了高分辨率的农作物产量地图进行训练，并采用了农作物和机器学习模型无关的方法进行亚田级别的预测。该方法使用全球覆盖的输入模态，并强调了输入模态对于产量预测的重要性。 |
| [^5] | [Interpretable Graph Neural Networks for Tabular Data.](http://arxiv.org/abs/2308.08945) | 本论文提出了一种称为IGNNet的方法，可以在处理表格数据时使用图神经网络，该方法能够产生可解释的模型，从原始输入特征精确计算预测结果，并且在性能上与最先进的机器学习算法性能相当。 |
| [^6] | [Towards Automatically Addressing Self-Admitted Technical Debt: How Far Are We?.](http://arxiv.org/abs/2308.08943) | 本文研究了自动化偿还技术债务的潜力，并探索了利用神经网络生成模型实现该目标的方法。实证研究结果表明，不同的预训练和微调策略可以帮助我们在不同项目中有效地解决自承认技术债务。 |
| [^7] | [A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models.](http://arxiv.org/abs/2308.08925) | 本文提出了一种针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的新方法，通过将攻击视为书写风格之间的风格转换，引入新的损失函数来生成欺骗性图像，实现了最先进的攻击成功率。 |
| [^8] | [IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making.](http://arxiv.org/abs/2308.08918) | 本研究提出了一种基于预测表示学习的模仿增强学习方法，在自动做市中应用。该方法通过借鉴专业人类做市商的工作流程，结合子优信号专家的知识和直接策略交互，开发了适用于多价格水平的做市策略。 |
| [^9] | [Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection.](http://arxiv.org/abs/2308.08915) | 这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。 |
| [^10] | [Towards a Practical Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via Randomized Smoothing.](http://arxiv.org/abs/2308.08906) | 这篇论文提出了一种针对深度学习恶意软件检测器对抗攻击的实用防御方法，通过随机平滑化来剥离可执行文件一定比例字节，并提出了基于剥离版本的基分类器进行训练和测试。 |
| [^11] | [Development of a Knowledge Graph Embeddings Model for Pain.](http://arxiv.org/abs/2308.08904) | 该论文开发了一个用于疼痛的知识图谱嵌入模型，以便研究疼痛概念及其关系。知识图谱嵌入可以将大型图谱转化为低维向量，从而提高计算效率，并用于各种任务。 |
| [^12] | [D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field.](http://arxiv.org/abs/2308.08857) | 本文提出通过使用自适应不确定性分布，区分接近表面的点和其他点，从而在像素对齐形状恢复中取得显著改进。 |
| [^13] | [CMB: A Comprehensive Medical Benchmark in Chinese.](http://arxiv.org/abs/2308.08833) | CMB是一个全面的中文医学基准，基于中国本土语言和文化框架设计，能够解决将英语医学评估翻译到本地环境中的上下文不一致问题。 |
| [^14] | [Lifted Algorithms for Symmetric Weighted First-Order Model Sampling.](http://arxiv.org/abs/2308.08828) | 本文研究了对称加权一阶模型抽样的提升算法，通过设计一种扩展了计数量词的一阶逻辑的“域可提升抽样”的内容，证明了加权模型抽样问题也存在可处理的情况。 |
| [^15] | [Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation.](http://arxiv.org/abs/2308.08799) | 本论文提出了一种简化的非个性化方法PARE，通过预测最高流行度的项目进行推荐，填补了现有推荐方法忽略项目流行度的不足。实验证明PARE的性能优于复杂的方法。 |
| [^16] | [CodeCoT and Beyond: Learning to Program and Test like a Developer.](http://arxiv.org/abs/2308.08784) | 本文介绍了CodeCoT和Beyond的学习方法，该方法可以帮助模型在处理任务时从少量特定数据中进行适应。通过链式思维引导，模型可以揭示多步推理过程中的认知过程，并通过自我检查不断优化输出。 |
| [^17] | [Exploring Demonstration Ensembling for In-context Learning.](http://arxiv.org/abs/2308.08780) | 本研究探索了上下文学习的演示集成方法，用于提高语言模型在给定任务的输入输出对中的预测性能。通过将演示分成子集并组合各子集的输出概率，我们得到了最终的预测结果。 |
| [^18] | [Large Language Models at Work in China's Labor Market.](http://arxiv.org/abs/2308.08776) | 本文研究了大型语言模型（LLMs）对中国劳动市场的潜在影响，并分析了职业对LLM能力的暴露程度及其与工资水平和经验溢价之间的关系。研究结果表明，高薪和经验密集型工作可能面临更大的替代风险。此外，研究还开发了一个考虑行业暴露的经济增长模型，以量化AI采用对生产力和就业之间的权衡。这项研究为理解中国劳动市场中越来越强大的AI系统的影响提供了基础。 |
| [^19] | [Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models.](http://arxiv.org/abs/2308.08774) | 该论文研究了多语言语言模型在多语言压缩、语言公平性和透明性等方面的要求，并发现差分隐私与训练数据影响稀疏性之间存在相互制约的关系。 |
| [^20] | [Discrete Prompt Compression with Reinforcement Learning.](http://arxiv.org/abs/2308.08758) | 本研究提出了一种使用强化学习的离散提示压缩方法（PCRL），以解决指令调整的语言模型中嵌入训练的挑战。PCRL采用了一种计算效率高的策略网络直接编辑提示，可以灵活应用于各种类型的LM，而不需要梯度访问或标记数据。 |
| [^21] | [SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation.](http://arxiv.org/abs/2308.08746) | SurgicalSAM是一种高效的手术器械分割方法，通过引入SurgicalSAM，可以有效地应用Segment Anything Model (SAM) 进行手术器械分割，解决了SAM在手术器械领域的泛化能力差和复杂多阶段流程的问题。 |
| [^22] | [PMET: Precise Model Editing in a Transformer.](http://arxiv.org/abs/2308.08742) | 该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。 |
| [^23] | [ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents.](http://arxiv.org/abs/2308.08737) | 本文提出了一种名为ReProHRL的层次化代理方法，通过强化学习实现了多目标导航任务，并使用物体检测器进行预处理。实验证明，ReProHRL方法在仿真和真实世界环境中表现优于现有方法。 |
| [^24] | [LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM.](http://arxiv.org/abs/2308.08728) | LLM-FuncMapper提出了一种通过LLM实现对建筑法规中复杂条款的函数识别的方法，通过定义原子函数和开发提示模板来解决传统逻辑表示的限制。 |
| [^25] | [EdgeMA: Model Adaptation System for Real-Time Video Analytics on Edge Devices.](http://arxiv.org/abs/2308.08717) | EdgeMA是一个实用高效的视频分析系统，通过提取统计纹理特征和使用重要性加权的模型适应方法，解决了边缘设备上实时视频分析中的数据漂移问题，显著提高了推理准确性。 |
| [^26] | [Probabilistic Results on the Architecture of Mathematical Reasoning Aligned by Cognitive Alternation.](http://arxiv.org/abs/2308.08714) | 该论文研究了一种能够解决数学问题的机器，将定量推理系统分为思维过程和认知过程两部分，并提供了架构的概率性描述。 |
| [^27] | [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.](http://arxiv.org/abs/2308.08708) | 本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。 |
| [^28] | [Planning in the imagination: High-level planning on learned abstract search spaces.](http://arxiv.org/abs/2308.08693) | 本论文提出了一个名为PiZero的新方法，该方法使代理能够在自己创建的抽象搜索空间中进行高级规划，不受真实环境限制，可在任意时间尺度进行规划，并处理连续动作空间和部分可观察性的设置。在多个领域的实验中，该方法胜过可比的先前方法而无需假设访问环境模拟器。 |
| [^29] | [Lightweight Adaptation of Neural Language Models via Subspace Embedding.](http://arxiv.org/abs/2308.08688) | 本论文提出了一种新的紧凑嵌入结构，通过牺牲部分准确度，减少预训练语言模型的内存占用。实验证明，该结构可以实现超过99.8%的压缩率。 |
| [^30] | [Quantifying Overfitting: Introducing the Overfitting Index.](http://arxiv.org/abs/2308.08682) | 本文引入了过拟合指数（OI），通过对乳腺超声图像数据集和MNIST数据集进行广泛实验，使用了多种架构，展示了OI的实用性和区分能力。结果表明，不同架构的过拟合行为存在差异，并强调了数据增强对于较小和更专业的数据集的缓解影响。OI为解决过拟合问题提供了一种有希望的途径。 |
| [^31] | [Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions.](http://arxiv.org/abs/2308.08661) | 这项研究通过利用从维基百科生成的一组明确问题的数据库，提出了一种用于回答模糊问题的新技术，在回答性能和歧义问题消除方面取得了显著提高。 |
| [^32] | [AdaptEx: A Self-Service Contextual Bandit Platform.](http://arxiv.org/abs/2308.08650) | AdaptEx是一个自助上下文赌博平台，通过利用多臂赌博算法个性化用户体验并提供最优解，同时最小化传统测试方法的成本和时间。它能够在不断变化的内容和“冷启动”情况下快速迭代。 |
| [^33] | [Towards Zero Memory Footprint Spiking Neural Network Training.](http://arxiv.org/abs/2308.08649) | 这项研究提出了一种具有极低内存占用的脉冲神经网络训练框架，并设计了可逆SNN节点和简化的反向传播算法，显著降低了内存使用和计算复杂性。 |
| [^34] | [FedPop: Federated Population-based Hyperparameter Tuning.](http://arxiv.org/abs/2308.08634) | FedPop是一种用于解决联邦学习中超参数调优问题的新算法，它采用基于人口的进化算法来优化客户端和服务器上的超参数。 |
| [^35] | [LSTM-Based Forecasting Model for GRACE Accelerometer Data.](http://arxiv.org/abs/2308.08621) | 本研究基于LSTM网络，提出了一种填充GRACE卫星加速计数据间断的方法，并成功预测了三个轴上的加速计数据。 |
| [^36] | [Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought.](http://arxiv.org/abs/2308.08614) | 本文提出了一种新的提示技术——思维图（GoT），通过在三个不断升级的挑战中的测试，我们的方法在多步逻辑推理问题上表现优于GPT-4，并且相比最先进的提示方法思维树（ToT），我们的方法有更高的准确性提升。 |
| [^37] | [Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach.](http://arxiv.org/abs/2308.08611) | 本文开发了一个基于深度强化学习的框架，通过深度Q网络（DQN）优化光伏系统在农业中的安装决策制定。该研究对于提高能源效率、减少环境影响并增加农业利润具有重要意义。 |
| [^38] | [FootGPT : A Large Language Model Development Experiment on a Minimal Setting.](http://arxiv.org/abs/2308.08610) | 本文介绍了一个在最小设置上进行的大规模语言模型开发实验，研究发现准确的语言模型的关键在于适当的数据集内容和训练策略，而不是神经参数数量、训练时长或数据集大小。通过对一个10亿参数规模的通用因果语言模型进行微调，并使用商业语言模型提供的精简段落和问答对构建数据集，可以有效解释足球数据。 |
| [^39] | [On the Augmentation of Cognitive Accuracy and Cognitive Precision in Human/Cog Ensembles.](http://arxiv.org/abs/2308.08581) | 本文介绍了关于人类/认知合奏中的认知准确性和认知精度的研究。实验结果表明，通过认知系统提供的信息，人类的认知准确性和认知精度得到了提高。 |
| [^40] | [PEvoLM: Protein Sequence Evolutionary Information Language Model.](http://arxiv.org/abs/2308.08578) | PEvoLM是一种蛋白质序列进化信息语言模型，它利用嵌入式语言模型将蛋白质序列转换为数字向量表示。 |
| [^41] | [A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance.](http://arxiv.org/abs/2308.08574) | 本研究对比分析了12种自然启发的特征选择算法在预测学生表现中的能力，发现利用这些算法进行特征选择并结合传统机器学习算法可以提高预测准确性，并减少特征集大小。 |
| [^42] | [Large Language Models in Introductory Programming Education: ChatGPT's Performance and Implications for Assessments.](http://arxiv.org/abs/2308.08572) | 本文研究了大型语言模型在解决初级编程任务中的性能，并得出了利用这些模型进行编程教育和评估的推论。 |
| [^43] | [KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification.](http://arxiv.org/abs/2308.08563) | 本文提出了一种基于知识的多方位框架（KMF），用于零样本节点分类任务。该框架通过提取知识图谱中的主题来增强标签语义，以改善模型的泛化能力。 |
| [^44] | [Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS).](http://arxiv.org/abs/2308.08561) | 该论文介绍了一种名为QMLS的新概念，通过结合机器学习和量子模拟的方法，可以缩短药物研发的时间和降低成本。通过生成命中物和优化分子的过程，可以大大提高药物发现的效率。 |
| [^45] | [BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching.](http://arxiv.org/abs/2308.08558) | 本文提出了一种基于多模式匹配的比特币信息检索预测模型(BIRP)，通过排列相似的过去图表运动来提高预测能力，应用于比特币市场。 |
| [^46] | [Causally Linking Health Application Data and Personal Information Management Tools.](http://arxiv.org/abs/2308.08556) | 本文研究了如何将健康应用数据和个人信息管理工具进行关联，解决了健康数据可视化中的变量关系问题，并提出了利用个人信息管理工具的上下文信息来增强健康和福祉应用程序的方法。 |
| [^47] | [AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors.](http://arxiv.org/abs/2308.08554) | 本论文使用人工智能算法分析历史数据和区块链参数，找出影响加密货币价格的因素并识别风险加密货币。 |
| [^48] | [Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction.](http://arxiv.org/abs/2308.08518) | 本文提出了一个具有点对注意力感知机制的双向对应预测网络，通过利用模型点和场景点之间的相关性进行点对匹配学习，解决了传统方法对观察质量和遮挡的依赖性问题，并在实验证明其在物体姿态估计任务上优于其他最先进的方法。 |
| [^49] | [Explainable Multi-View Deep Networks Methodology for Experimental Physics.](http://arxiv.org/abs/2308.08206) | 该论文介绍了一个可解释的多视角深度网络方法论，应用于实验物理中的多种成像表达分析。该方法论解决了多视角模型可解释性不足的问题。 |
| [^50] | [Relightable and Animatable Neural Avatar from Sparse-View Video.](http://arxiv.org/abs/2308.07903) | 本文提出了一种从稀疏视角视频中创建可重光和可动化的神经化身的方法，通过使用Hierarchical Distance Query（HDQ）算法来近似任意人体姿态下的世界空间距离，并使用这些距离来进行材料恢复和重光。 |
| [^51] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^52] | [A Survey on Model Compression for Large Language Models.](http://arxiv.org/abs/2308.07633) | 本论文提供了关于大型语言模型的模型压缩综述，探讨了量化、修剪、知识蒸馏等不同方法，并突出介绍了最新进展和创新方法，为实现高效的部署提供了重要思路。 |
| [^53] | [Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?.](http://arxiv.org/abs/2308.06619) | 本研究介绍了一种名为EGP的创新的熵引导剪枝算法，该算法能够通过优先剪除熵较低的层中的连接来有效压缩深度神经网络，同时保持其竞争性能水平。 |
| [^54] | [EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes.](http://arxiv.org/abs/2308.06493) | 本文提出了EgoPoser，一种能够在大场景中鲁棒地实时估计自我身体姿势的方法。通过重新思考输入表示、引入新的运动分解方法以及建模身体姿势，EgoPoser在定性和定量上均表现优于现有方法，并具有较高的推理速度。 |
| [^55] | [Audio-Visual Spatial Integration and Recursive Attention for Robust Sound Source Localization.](http://arxiv.org/abs/2308.06087) | 本文提出了一种音频-视觉空间融合网络，通过整合音频和视觉模态的空间线索来模仿人类检测声音产生物体的行为，并引入递归注意力网络来得到更准确的注意力区域。通过音频和视觉模态的空间线索和递归聚焦策略，方法在声源定位任务上取得了良好的性能。 |
| [^56] | [Cost-effective On-device Continual Learning over Memory Hierarchy with Miro.](http://arxiv.org/abs/2308.06053) | 这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。 |
| [^57] | [Deep Task-specific Bottom Representation Network for Multi-Task Recommendation.](http://arxiv.org/abs/2308.05996) | 本文提出了一种深度任务特定的底层表示网络（DTRN），用于解决多任务推荐系统中的负迁移问题，通过明确获取每个任务的底层表示来改善任务特定特征的捕捉能力。 |
| [^58] | [DiLogics: Creating Web Automation Programs With Diverse Logics.](http://arxiv.org/abs/2308.05828) | DiLogics是一个通过演示编程的系统，利用自然语言处理帮助用户创建处理多样化规范的Web自动化程序。 |
| [^59] | [Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient.](http://arxiv.org/abs/2308.05681) | 本文针对基于骨骼的人体动作识别方法的脆弱性进行了研究，提出了一种新的硬性无盒对抗攻击方法，通过学习运动流形和定义骨骼-动作知情梯度来攻击模型，揭示了这种脆弱性的存在。 |
| [^60] | [Metacognitive Prompting Improves Understanding in Large Language Models.](http://arxiv.org/abs/2308.05342) | 元认知提示 (MP) 是一种改进大型语言模型 (LLMs) 理解能力的策略。实验结果表明，使用MP的PaLM在各种自然语言理解任务中接近于GPT-4的性能水平。 |
| [^61] | [Where's the Liability in Harmful AI Speech?.](http://arxiv.org/abs/2308.04635) | AI生成式模型可能会产生具有潜在责任风险的有害言论。解决模型创建者和部署者的法律责任问题的关键在于算法设计的技术细节。需要进行深入的Section 230免责分析以及下游责任分析。 |
| [^62] | [Developmental Bootstrapping of AIs.](http://arxiv.org/abs/2308.04586) | 传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。 |
| [^63] | [Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM.](http://arxiv.org/abs/2308.03333) | 本文提出了一种通过大型语言模型（LLM）从用户行为信息中提取和融合异构知识的新方法，通过指令调整实现个性化推荐，有效地提高了推荐性能。 |
| [^64] | [DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data.](http://arxiv.org/abs/2308.03295) | DOMINO是一种领域不变的高维分类方法，适用于多传感器时间序列数据；解决了分布偏移和计算资源的问题。 |
| [^65] | [Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?.](http://arxiv.org/abs/2308.01284) | ChatGPT作为检测器能否有效检测AI生成的文本，我们通过评估其在人工编写文本与AI生成文本之间的区分能力，并在公开数据集上进行实验，得出了关于ChatGPT在自动化检测流程中的应用指导。 |
| [^66] | [A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness.](http://arxiv.org/abs/2308.01050) | 本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。 |
| [^67] | [Hessian-Aware Bayesian Optimization for Decision Making Systems.](http://arxiv.org/abs/2308.00629) | 本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。 |
| [^68] | [DMFC-GraspNet: Differentiable Multi-Fingered Robotic Grasp Generation in Cluttered Scenes.](http://arxiv.org/abs/2308.00456) | 本文提出了DMFC-GraspNet，在多指机器人抓取生成领域做出了两个主要贡献：一是提出了可微的多指抓取规划方法，实现了多样化和稠密的抓取预测；二是开发了一种稠密标注方法，使得多指机器人手与真实抓取密切关联。结果表明了该方法的有效性。 |
| [^69] | [MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.](http://arxiv.org/abs/2308.00352) | MetaGPT是一个用于多智能体协作的创新框架，将有效的人工工作流引入到大型语言模型驱动的协作中。它采用元编程方法，将标准操作规程编码为提示，促进结构化协调，并要求模块化输出，赋予智能体领域专业知识，以验证输出并减少错误。这种框架利用了流水线工作模式来分配任务。 |
| [^70] | [Getting pwn'd by AI: Penetration Testing with Large Language Models.](http://arxiv.org/abs/2308.00121) | 本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。 |
| [^71] | [Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes.](http://arxiv.org/abs/2307.16706) | 本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。 |
| [^72] | [Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG.](http://arxiv.org/abs/2307.12644) | 这篇论文提出了远程生物感应技术rPPG的公开源基准框架，该框架可以公平评估rPPG的准确性问题并解决与皮肤颜色、相机特性和环境光等因素相关的挑战。 |
| [^73] | [Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models.](http://arxiv.org/abs/2307.12507) | 本文介绍了一种名为GradObstinate的基于梯度的方法，用于生成顽固对抗样本。该方法可以自动生成意义改变但模型预测结果保持不变的对抗样本，无需人工设计约束。 |
| [^74] | [Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation.](http://arxiv.org/abs/2307.09906) | 提出了一种隐式身份表示条件化记忆补偿网络，用于高保真度的自然头部视频生成。 |
| [^75] | [Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling.](http://arxiv.org/abs/2307.07944) | 本文通过提出一种适用于多类训练设置的新型ReDB框架来解决现有领域自适应方法在多类训练设置下性能下降的问题，通过产生可靠的、多样化的和类平衡的伪三维框来引导目标领域的自训练。 |
| [^76] | [Automated identification and quantification of myocardial inflammatory infiltration in digital histological images to diagnose myocarditis.](http://arxiv.org/abs/2307.01098) | 本研究开发了一种新的计算病理学方法，用于自动识别和定量心肌组织炎症浸润在数字HE染色图像中，为心肌炎提供定量组织学诊断。该方法通过指标LND的定量来判断是否存在心肌炎症浸润。 |
| [^77] | [GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction.](http://arxiv.org/abs/2306.16736) | 提出了一种用于3D人体运动重建的地面感知运动模型（GraMMaR），通过学习姿势和关节与地面之间的互动的过渡分布，明确促进运动和与地面距离变化之间的一致性。 |
| [^78] | [Comprehensive Training and Evaluation on Deep Reinforcement Learning for Automated Driving in Various Simulated Driving Maneuvers.](http://arxiv.org/abs/2306.11466) | 本研究在模拟平台上对两种深度强化学习算法进行了全面评估和比较，以开发自动驾驶模型。通过定制的奖励函数，对准确度、效率、安全性和舒适度进行了评估。 |
| [^79] | [Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification.](http://arxiv.org/abs/2306.10841) | 本文提出了一种基于区块链的联邦学习参考架构，通过结合联邦学习和区块链技术，实现了去中心化、协作的机器学习系统，并保护了数据隐私和用户控制的身份。该架构使用去中心化标识符进行身份验证，通过智能合约实现强大的安全性和高效的去中心化，并能根据需求集成各种额外的元素，是一个适用范围广泛的 BCFL 解决方案。 |
| [^80] | [A Shift In Artistic Practices through Artificial Intelligence.](http://arxiv.org/abs/2306.10054) | 人工智能模型生成的内容突破了艺术、音乐和媒体领域，引发了文化转变。它通过改变人们的角色、转变价值观以及挑战传统实践方式，为艺术的未来打开了新的可能性。 |
| [^81] | [Perceptions and Realities of Text-to-Image Generation.](http://arxiv.org/abs/2306.08363) | 本研究通过调查了解人们对文本到图像生成的认知，发现参与者意识到该技术的风险和危险，但只有少数人认为这项技术对个人构成风险，而对他人的风险较易被认识到，特别是艺术家被视为风险群体。 |
| [^82] | [Hidden Biases of End-to-End Driving Models.](http://arxiv.org/abs/2306.07957) | 端到端驾驶模型存在偏见问题，并引入了次要组件的更改。本文针对目前多数最先进的方法中所存在的两种偏见进行了研究，并提出了合理的替代方法。在此基础上，开发了TF ++，在CARLA测试中表现优异。 |
| [^83] | [Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4.](http://arxiv.org/abs/2306.07622) | 本研究揭示了大型语言模型（LLMs）具有类人直觉行为和认知错误的特点，而高级语言模型则通过学习避免这类错误并表现出超理性的方式。此外，通过使用心理学研究的方法探测LLMs，可以揭示其新生特性。 |
| [^84] | [Predicting Software Performance with Divide-and-Learn.](http://arxiv.org/abs/2306.06651) | 本文提出了一种名为$DaL$的基于分割学习的方法，用于预测高度配置的软件系统的性能。实验证明了该方法的有效性。 |
| [^85] | [Enhance Diffusion to Improve Robust Generalization.](http://arxiv.org/abs/2306.02618) | 本文通过连续时间随机微分方程的研究，发现对抗性训练中的扩散项决定了神经网络的鲁棒泛化能力，进而提出了一种改进的AT框架。 |
| [^86] | [InGram: Inductive Knowledge Graph Embedding via Relation Graphs.](http://arxiv.org/abs/2305.19987) | InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。 |
| [^87] | [A Survey on Large Language Models for Recommendation.](http://arxiv.org/abs/2305.19860) | 本综述介绍了基于大语言模型的推荐系统，提出了判别式LLMs和生成式LLMs两种模型范式，总结了这些模型的最新进展，强调了该领域的挑战和研究方向。 |
| [^88] | [Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers.](http://arxiv.org/abs/2305.18256) | 本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。 |
| [^89] | [A novel application for real-time arrhythmia detection using YOLOv8.](http://arxiv.org/abs/2305.16727) | 本文提出了一种使用YOLOv8算法进行心律失常检测的新应用程序，其模型能够实现持续监测，并以高准确性进行实时心律失常检测。 |
| [^90] | [A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents.](http://arxiv.org/abs/2305.16621) | 该论文研究表明，对于复杂指令遵循代理的学习，语言奖励塑造技术可能会影响代理程序的学习，其中表面上的成功可能是脆弱的。 |
| [^91] | [KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration.](http://arxiv.org/abs/2305.16437) | KeyPosS是一种面部标记检测框架，采用真实距离多边定位算法实现快速而准确的检测，避免了传统方法中的计算负担和量化误差问题。 |
| [^92] | [Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding.](http://arxiv.org/abs/2305.13512) | 本文评估了几个大型预训练语言模型在口语理解任务中的表现，发现最大模型可以在零-shot学习和上下文学习中达到与监督模型相近的意图分类准确度，但在槽填充方面表现不佳，且对ASR错误敏感。 |
| [^93] | [Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding.](http://arxiv.org/abs/2305.12031) | 临床骆驼是一种基于对话的知识编码的开源医学语言模型，具有很高的可解释性和临床相关性，并在多个基准数据集上取得了最先进的结果。 |
| [^94] | [MedLens: Improve mortality prediction via medical signs selecting and regression interpolation.](http://arxiv.org/abs/2305.11742) | 本文介绍了一个自动选择医学体征和回归插值的方法（MedLens），用于解决电子病历中医学体征数据缺失率过高导致预测性能降低的问题。 |
| [^95] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^96] | [Online Platt Scaling with Calibeating.](http://arxiv.org/abs/2305.00070) | 本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。 |
| [^97] | [CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception.](http://arxiv.org/abs/2304.00670) | 本文提出了CRN，一个新颖的相机雷达融合框架，通过将图像视图特征转换为鸟瞰特征图和使用多模态可变形注意力，实现了准确、稳健、高效的3D感知任务 |
| [^98] | [Using AI to Measure Parkinson's Disease Severity at Home.](http://arxiv.org/abs/2303.17573) | 该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。 |
| [^99] | [HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation.](http://arxiv.org/abs/2303.15994) | 该论文提出一种利用高低频率关系的无偏倚的全景场景图生成方法，解决了长尾问题和主体-对象对拥有多个重叠关系的问题。在广泛实验中取得了最先进的结果。 |
| [^100] | [Generative AI Assistants in Software Development Education.](http://arxiv.org/abs/2303.13936) | 本文探讨了当前软件开发行业采用生成式 AI（GAI）助手进行软件开发的现状和挑战，提出了未来软件开发教育的愿景和教学建议。 |
| [^101] | [Among Us: Adversarially Robust Collaborative Perception by Consensus.](http://arxiv.org/abs/2303.09495) | ROBOSAC提出了一种基于共识的反对抗鲁棒协同感知防御策略，使用随机子集的队友来对比协同感知和单个感知的结果，以排除潜在攻击者，并推导出确保获得所需无攻击者子集所需的采样试验个数。 |
| [^102] | [Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models.](http://arxiv.org/abs/2303.08010) | 本文研究了基于窗口的早期退出集成方法，以在保持模型可扩展性的同时实现不确定性估计任务的高效实现。实验结果表明，该方法在准确性和计算效率上都达到了最新的研究成果。 |
| [^103] | [TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation.](http://arxiv.org/abs/2303.06937) | 本文研究了一个重要但鲜为人知的问题：联邦类式持续学习，在联邦学习中动态添加新的类别。我们提出了一种称为TARGET的新颖方法，通过无样本蒸馏来减轻FCCL中的灾难性遗忘问题，并保护客户数据的隐私。该方法利用先前训练的全局模型在模型层面上传递旧任务的知识，并通过生成器生成合成数据来模拟数据的全局分布。与先前的FCCL方法相比，TARGET无需额外的数据集或存储先前任务的私有数据。 |
| [^104] | [Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning.](http://arxiv.org/abs/2303.00396) | 本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。 |
| [^105] | [Change is Hard: A Closer Look at Subpopulation Shift.](http://arxiv.org/abs/2302.12254) | 本文分析了子群体转变的各种机制，对20个最先进的算法在12个领域内进行了全面的基准测试，发现现有算法只能应对某些转变，进一步地，提出一种简单易行的选择标准来改善现有算法性能。 |
| [^106] | [Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis.](http://arxiv.org/abs/2302.05615) | 提出了一种新的自监督学习框架，通过结合辨别目标和生成目标，明确地实现了解剖不变性建模和语义对齐。引入了一种新的对比学习策略，鼓励不同来源但具有一致高级语义的视图之间的相似性，以学习不变的解剖特征。 |
| [^107] | [Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization.](http://arxiv.org/abs/2302.04415) | 本论文提出了PromptMize框架，用于解决少样本情况下的表格到文本生成问题。该框架包含提示策划和知识适配器两个方面，通过生成提示信号和利用领域特定知识来改善文本生成结果。 |
| [^108] | [REAP: A Large-Scale Realistic Adversarial Patch Benchmark.](http://arxiv.org/abs/2212.05680) | 本文提出了一个名为REAP的大规模真实对抗贴纸基准测试，该基准测试允许用户在真实图像和真实环境条件下评估对抗贴纸攻击，为解决依赖摄像头的物理系统面临的严重威胁提供了有效的工具。 |
| [^109] | [Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve.](http://arxiv.org/abs/2212.03905) | 本文介绍了一种名为多速率VAE（MR-VAE）的框架，可在单次训练中学习与不同β对应的最优参数，通过使用超网络将β映射到最优参数，以实现率失真曲线的完整训练。 |
| [^110] | [Secure and Privacy-Preserving Automated Machine Learning Operations into End-to-End Integrated IoT-Edge-Artificial Intelligence-Blockchain Monitoring System for Diabetes Mellitus Prediction.](http://arxiv.org/abs/2211.07643) | 本文提出了一种基于区块链的物联网边缘人工智能系统，用于通过危险因素预测糖尿病，以确保用户数据的安全性和隐私性，并进行了比较分析不同医疗传感器、设备和方法的效果。 |
| [^111] | [Neural Graphical Models.](http://arxiv.org/abs/2210.00453) | 本文介绍了神经图模型（NGMs），它可以以合理的计算成本表示复杂的特征依赖关系，适应多种图结构和混合输入数据类型，并提供了高效的学习、推断和采样算法。 |
| [^112] | [To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing.](http://arxiv.org/abs/2209.02166) | 本文提出了一种计算和通信延迟估计优化框架，并采用基于强化学习的方法来动态分配边缘的感知和计算资源，从而使智能传感器网络在资源受限的边缘计算中实现更佳的性能。 |
| [^113] | [GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks.](http://arxiv.org/abs/2208.12489) | 本论文提出了一种名为GHN-Q的方法，通过图形超网络来预测未见量化卷积架构的参数，以提高量化鲁棒性。 |
| [^114] | [Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL.](http://arxiv.org/abs/2208.10469) | 本研究通过引入正式合同的概念，解决了多智能体强化学习中个体激励和集体激励分歧导致的次优行为问题。理论和实证结果表明，通过在马尔可夫博弈中引入有约束的状态依赖奖励转移，实现了所有可观察马尔可夫博弈的子博弈完美均衡表现出社会最优行为，并提升了算法的社会性能。 |
| [^115] | [Why do networks have inhibitory/negative connections?.](http://arxiv.org/abs/2208.03211) | 神经网络具有抑制性/负向连接是为了学习更多的功能，负权重在表征能力中起着至关重要的作用，并且非负深度网络无法表示某些表征空间的几何特性。 |
| [^116] | [Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces.](http://arxiv.org/abs/2208.02743) | 本文提出在超复数空间中整合知识图谱嵌入和预训练语言模型的方法，通过利用超复数代数来表示单模态嵌入以及不同模态之间的交互，并且能够更好地利用结构性和文本性知识的相互作用。 |
| [^117] | [Fuzzy Labeling Semantics for Quantitative Argumentation.](http://arxiv.org/abs/2207.07339) | 本论文提出了一种新颖的定量方法——模糊标签化，用于评估模糊论证系统中的论证强度。通过使用可接受度、可拒绝度和不确定度三个程度，该方法为界定论证强度提供了新的视角，并对论证的状态进行了深入理解。 |
| [^118] | [Learning logic programs by combining programs.](http://arxiv.org/abs/2206.01614) | 本论文介绍了一种通过组合小型非可分离程序的方法来学习逻辑程序。实验结果表明，这种方法在预测准确性和学习时间方面显著优于现有方法。 |
| [^119] | [IDEAL: Query-Efficient Data-Free Learning from Black-box Models.](http://arxiv.org/abs/2205.11158) | IDEAL是一种无需真实数据的高效查询方法，用于从黑盒模型API中学习并训练一个优秀的学生模型。它通过在两个阶段进行训练，即数据生成和模型蒸馏，只需要对教师模型进行少量的查询。 |
| [^120] | [Experimental Design for Causal Effect Identification.](http://arxiv.org/abs/2205.02232) | 本文研究了设计最小成本的干预集合来识别因果效应的问题，证明了问题的NP-hard性质，并提出了可以找到最优解或近似解的算法，使用多项式时间启发式算法来解决计算复杂性，并在随机图上进行了模拟实验验证算法的效果。 |
| [^121] | [Reactive Motion Generation on Learned Riemannian Manifolds.](http://arxiv.org/abs/2203.07761) | 本文从黎曼流形的角度研究了机器人运动学习，通过学习黎曼度量和使用测地线生成的运动能适应新的环境条件，并通过调整学习到的流形实现避障功能。 |
| [^122] | [SuperAnimal models pretrained for plug-and-play analysis of animal behavior.](http://arxiv.org/abs/2203.07436) | SuperAnimal是一种能够开发和部署深度学习模型的方法，不需要额外的人工标注和模型训练，同时可用于超过45个物种进行视频推断，并具有微调模型效率高的优点。 |
| [^123] | [Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity.](http://arxiv.org/abs/2203.07139) | 本文提出了一种跨模型公平性的新定义，并进行了实证研究。该研究关注数据驱动预测模型在社会背景下的公平性问题，特别是在通过选择不同预测器进行模型多样性时可能导致个人受伤害的情况。 |
| [^124] | [On the Power of Gradual Network Alignment Using Dual-Perception Similarities.](http://arxiv.org/abs/2201.10945) | 本研究提出了Grad-Align，一种渐进网络对齐方法，通过利用强一致性节点对逐步发现节点对。该方法首先生成节点嵌入，然后计算双感知相似性度量逐步对齐节点。 |
| [^125] | [History Aware Multimodal Transformer for Vision-and-Language Navigation.](http://arxiv.org/abs/2110.13309) | 本论文提出了一种基于历史意识的多模态变压器(HAMT)，用于视觉语言导航。该方法通过编码过去的全景观察，综合考虑了文本、历史和当前观察，实现自主导航。在训练中，通过多个代理任务和强化学习进一步提高导航策略。 |
| [^126] | [Representations of epistemic uncertainty and its perception in data-driven initiatives.](http://arxiv.org/abs/2110.11482) | 本研究为了支持不断发展的数据驱动方法论，提出了一个新颖的概念模型，用于描述知识表示中的不确定性，并推理代理人进行的信息传输。通过对知识状态和其动态的代数描述，我们能够比较和组合知识状态，以表示更新。 |

# 详细

[^1]: 以多工作流可信度和数据可观测性为基础的轻量级数据集成

    Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability. (arXiv:2308.09004v1 [cs.DC])

    [http://arxiv.org/abs/2308.09004](http://arxiv.org/abs/2308.09004)

    本论文提出MIDA方法，以多工作流可信度和数据可观测性为基础，实现了轻量级的运行时集成数据分析。该方法通过定义数据可观测策略和适应性方法，可以处理异构科学环境下多个支持工具和高效的HPC执行。

    

    现代大规模科学发现需要跨多个计算设施进行多学科合作，包括高性能计算（HPC）机器和边缘到云的连续体。集成的数据分析在科学发现中起着至关重要的作用，特别是在当前人工智能时代，通过支持负责任的AI开发、FAIR、可复现性和用户操控。然而，科学的异构性带来了挑战，如处理多个支持工具、跨设施环境和高效的HPC执行。在数据可观测性、适配器系统设计和可信度的基础上，我们提出了MIDA：一种轻量级运行时多工作流集成数据分析的方法。MIDA定义了各种并行系统和机器学习工具的数据可观测策略和适应性方法。通过可观测性，它在后台拦截数据流，无需仪器化，同时集成领域、可信度和遥测数据。

    Modern large-scale scientific discovery requires multidisciplinary collaboration across diverse computing facilities, including High Performance Computing (HPC) machines and the Edge-to-Cloud continuum. Integrated data analysis plays a crucial role in scientific discovery, especially in the current AI era, by enabling Responsible AI development, FAIR, Reproducibility, and User Steering. However, the heterogeneous nature of science poses challenges such as dealing with multiple supporting tools, cross-facility environments, and efficient HPC execution. Building on data observability, adapter system design, and provenance, we propose MIDA: an approach for lightweight runtime Multi-workflow Integrated Data Analysis. MIDA defines data observability strategies and adaptability methods for various parallel systems and machine learning tools. With observability, it intercepts the dataflows in the background without requiring instrumentation while integrating domain, provenance, and telemetry 
    
[^2]: 行为树控制器的延伸收敛结果

    An Extended Convergence Result for Behaviour Tree Controllers. (arXiv:2308.08994v1 [eess.SY])

    [http://arxiv.org/abs/2308.08994](http://arxiv.org/abs/2308.08994)

    本文研究了行为树控制器的收敛性，广泛推广了早期的结果，并包括了文献中未涵盖的新情况。

    

    行为树（BTs）是一种通过树结构从一组低级控制策略组合成分层混合控制策略的最优模块化框架。许多机器人任务自然地分解为控制任务的层次结构，并且模块化是处理复杂性的众所周知的工具，因此行为树在机器人社区广泛使用。本文研究了BT的收敛性，即达到状态空间的期望部分。先前关于BT收敛的结果通常针对使用不同设计原则创建的特定BT族群进行了调整。本文的结果推广了以前的结果，还包括了文献中未涵盖的循环切换的新情况。

    Behavior trees (BTs) are an optimally modular framework to assemble hierarchical hybrid control policies from a set of low-level control policies using a tree structure. Many robotic tasks are naturally decomposed into a hierarchy of control tasks, and modularity is a well-known tool for handling complexity, therefor behavior trees have garnered widespread usage in the robotics community. In this paper, we study the convergence of BTs, in the sense of reaching a desired part of the state space. Earlier results on BT convergence were often tailored to specific families of BTs, created using different design principles. The results of this paper generalize the earlier results and also include new cases of cyclic switching not covered in the literature.
    
[^3]: 一种双重视角评估特征归因方法的方法

    A Dual-Perspective Approach to Evaluating Feature Attribution Methods. (arXiv:2308.08949v1 [cs.LG])

    [http://arxiv.org/abs/2308.08949](http://arxiv.org/abs/2308.08949)

    这篇论文提出了一种双重视角的方法来评估特征归因方法。通过观察扰动归因特征对模型行为的影响，这种方法揭示了归因特征的准确性和完整性，使其能够定量评估特征归因的表现。

    

    特征归因方法试图通过识别相关特征来解释神经网络的预测。然而，建立一个评估特征归因的统一框架仍然是一个挑战。我们可以通过几个视角来评估特征归因。其中一个主要视角是观察扰动归因特征对模型行为的影响（即忠实度）。尽管提供了有用的洞见，但现有的忠实度评估存在我们在本文中揭示的缺点。在这项工作中，我们提出了忠实度范式内的两个新视角，揭示了直观的属性：正确性和完整性。正确性评估归因特征真正是预测性特征的程度，而完整性检查所得归因如何很好地揭示所有预测性特征。这两个视角基于坚实的数学基础，并提供了通过高效算法计算的定量指标。

    Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. W
    
[^4]: 用机器学习预测农作物产量：在田地和亚田级别上对输入模态和模型的广泛分析

    Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level. (arXiv:2308.08948v1 [cs.CV])

    [http://arxiv.org/abs/2308.08948](http://arxiv.org/abs/2308.08948)

    本论文介绍了一种用于农作物产量预测的简单而有效的早期融合方法，可以处理具有不同时间和空间分辨率的多个输入模态。该方法使用了高分辨率的农作物产量地图进行训练，并采用了农作物和机器学习模型无关的方法进行亚田级别的预测。该方法使用全球覆盖的输入模态，并强调了输入模态对于产量预测的重要性。

    

    我们引入了一种简单而有效的早期融合方法，用于处理具有不同时间和空间分辨率的多个输入模态的农作物产量预测。我们使用高分辨率的农作物产量地图作为训练数据，在亚田级别上使用农作物和机器学习模型无关的方法。我们使用Sentinel-2卫星图像作为主要的输入数据模态，以及其他补充的模态，包括天气、土壤和DEM数据。所提出的方法使用了具有全球覆盖范围的输入模态，使该框架具有全球伸缩性。我们明确强调了输入模态对于农作物产量预测的重要性，并强调了最佳组合的输入模态取决于地区、作物和选择的模型。

    We introduce a simple yet effective early fusion method for crop yield prediction that handles multiple input modalities with different temporal and spatial resolutions. We use high-resolution crop yield maps as ground truth data to train crop and machine learning model agnostic methods at the sub-field level. We use Sentinel-2 satellite imagery as the primary modality for input data with other complementary modalities, including weather, soil, and DEM data. The proposed method uses input modalities available with global coverage, making the framework globally scalable. We explicitly highlight the importance of input modalities for crop yield prediction and emphasize that the best-performing combination of input modalities depends on region, crop, and chosen model.
    
[^5]: 可解释的基于图神经网络的表格数据处理方法

    Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])

    [http://arxiv.org/abs/2308.08945](http://arxiv.org/abs/2308.08945)

    本论文提出了一种称为IGNNet的方法，可以在处理表格数据时使用图神经网络，该方法能够产生可解释的模型，从原始输入特征精确计算预测结果，并且在性能上与最先进的机器学习算法性能相当。

    

    在现实世界的应用中，表格格式的数据经常出现。图神经网络（GNNs）近期被扩展以有效处理此类数据，通过表示学习捕捉特征之间的相互作用。然而，这些方法本质上产生了黑盒模型，以深度神经网络的形式存在，使得用户无法理解模型预测的逻辑。我们提出了一种称为IGNNet（基于图神经网络的可解释表格数据处理方法）的方法，它限制学习算法以产生可解释的模型，该模型展示了如何从原始输入特征准确计算预测结果。通过大规模实证研究，我们展示了IGNNet与面向表格数据的最先进机器学习算法（包括XGBoost，Random Forests和TabNet）性能相当。同时，结果显示从IGNNet获得的解释与真实情况一致。

    Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
    
[^6]: 自动解决自承认技术债务的探索：我们离目标有多远？

    Towards Automatically Addressing Self-Admitted Technical Debt: How Far Are We?. (arXiv:2308.08943v1 [cs.SE])

    [http://arxiv.org/abs/2308.08943](http://arxiv.org/abs/2308.08943)

    本文研究了自动化偿还技术债务的潜力，并探索了利用神经网络生成模型实现该目标的方法。实证研究结果表明，不同的预训练和微调策略可以帮助我们在不同项目中有效地解决自承认技术债务。

    

    在软件发展过程中，组织和个人开发者需要花费大量的精力来还清技术债务，即软件发布时存在的不完善，例如功能、可靠性或可维护性方面。本文通过实证研究来探讨基于神经网络生成模型的自动化技术债务偿还程度，特别是利用不同的预训练和微调策略的模型。我们首先从595个开源项目中提取了5,039个自承认技术债务（SATD）的移除实例。SATD是指开发人员通过代码注释等形式来记录的技术债务。我们使用这个数据集来实验七个不同的生成深度学习（DL）模型配置。具体而言，我们比较了不同组合的预训练和微调目标，包括修复通用代码更改，SATD移除等。

    Upon evolving their software, organizations and individual developers have to spend a substantial effort to pay back technical debt, i.e., the fact that software is released in a shape not as good as it should be, e.g., in terms of functionality, reliability, or maintainability. This paper empirically investigates the extent to which technical debt can be automatically paid back by neural-based generative models, and in particular models exploiting different strategies for pre-training and fine-tuning. We start by extracting a dateset of 5,039 Self-Admitted Technical Debt (SATD) removals from 595 open-source projects. SATD refers to technical debt instances documented (e.g., via code comments) by developers. We use this dataset to experiment with seven different generative deep learning (DL) model configurations. Specifically, we compare transformers pre-trained and fine-tuned with different combinations of training objectives, including the fixing of generic code changes, SATD removal
    
[^7]: 基于对比损失的离线手写签名验证模型的白盒误报对抗攻击方法

    A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (arXiv:2308.08925v1 [cs.CV])

    [http://arxiv.org/abs/2308.08925](http://arxiv.org/abs/2308.08925)

    本文提出了一种针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的新方法，通过将攻击视为书写风格之间的风格转换，引入新的损失函数来生成欺骗性图像，实现了最先进的攻击成功率。

    

    本文针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的挑战，提出了一种新颖的攻击方法，将攻击视为在密切相关但不同的书写风格之间进行风格转换。为了引导欺骗性图像的生成，我们引入了两个新的损失函数，通过扰动原始样本与合成样本的嵌入向量之间的欧氏距离来提高攻击成功率，同时通过减小生成图像与原始图像之间的差异来保证最小的扰动。通过实验证明，我们的方法在对比损失的离线手写签名验证模型的白盒攻击中表现出最先进的性能。本文的关键贡献包括了一种新颖的误报攻击方法、两个新的损失函数、有效的书写风格转换以及卓越的性能。

    In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss-based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss-based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in
    
[^8]: 基于预测表示学习的模仿增强学习方法在自动做市中的应用

    IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making. (arXiv:2308.08918v1 [cs.LG])

    [http://arxiv.org/abs/2308.08918](http://arxiv.org/abs/2308.08918)

    本研究提出了一种基于预测表示学习的模仿增强学习方法，在自动做市中应用。该方法通过借鉴专业人类做市商的工作流程，结合子优信号专家的知识和直接策略交互，开发了适用于多价格水平的做市策略。

    

    做市（MM）在金融交易中引起了广泛关注，因为它在确保市场流动性方面具有至关重要的功能。在顺序决策方面具有强大能力的强化学习（RL）技术在量化交易方面取得了显著的成功。然而，大多数现有的基于RL的MM方法都专注于优化单一价格水平策略，而对于频繁撤销订单和丢失队列优先级等问题无法解决。涉及多个价格水平的策略更符合实际交易场景。然而，由于多价格水平策略涉及到全面的交易行为空间的复杂性，有效训练盈利的RL代理人在MM方面仍然是一个挑战。受到专业做市商高效工作流程的启发，我们提出了一种新颖的RL框架，即模仿市场做市商（IMM），它利用子优信号专家的知识和直接策略交互的方式来开发多价格水平的MM策略。

    Market making (MM) has attracted significant attention in financial trading owing to its essential function in ensuring market liquidity. With strong capabilities in sequential decision-making, Reinforcement Learning (RL) technology has achieved remarkable success in quantitative trading. Nonetheless, most existing RL-based MM methods focus on optimizing single-price level strategies which fail at frequent order cancellations and loss of queue priority. Strategies involving multiple price levels align better with actual trading scenarios. However, given the complexity that multi-price level strategies involves a comprehensive trading action space, the challenge of effectively training profitable RL agents for MM persists. Inspired by the efficient workflow of professional human market makers, we propose Imitative Market Maker (IMM), a novel RL framework leveraging both knowledge from suboptimal signal-based experts and direct policy interactions to develop multi-price level MM strategi
    
[^9]: 超越共享：冲突感知的多变量时间序列异常检测

    Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])

    [http://arxiv.org/abs/2308.08915](http://arxiv.org/abs/2308.08915)

    这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。

    

    大量的关键绩效指标(KPI)以多变量时间序列数据(MTS)的形式进行监测，以确保软件应用程序和服务系统的可靠性。准确检测MTS的异常对于后续的故障排除非常关键。异常的稀缺性和手动标记导致了各种自监督的MTS异常检测方法的发展，这些方法优化了一个涵盖所有指标回归目标/损失的整体目标/损失。然而，我们的实证研究发现了指标回归目标之间冲突的普遍存在，导致MTS模型在不同的损失中挣扎。这一关键方面显著影响检测性能，但在现有方法中被忽视了。为了解决这个问题，通过模仿多门专家混合模型(MMoE)的设计，我们引入了CAD，一种冲突感知的多变量KPI异常检测算法。CAD为每个指标提供了一个独特的结构，以缓解冲突。

    Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
    
[^10]: 针对基于深度学习的恶意软件检测器的对抗攻击的实用防御方法：随机平滑化

    Towards a Practical Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via Randomized Smoothing. (arXiv:2308.08906v1 [cs.CR])

    [http://arxiv.org/abs/2308.08906](http://arxiv.org/abs/2308.08906)

    这篇论文提出了一种针对深度学习恶意软件检测器对抗攻击的实用防御方法，通过随机平滑化来剥离可执行文件一定比例字节，并提出了基于剥离版本的基分类器进行训练和测试。

    

    基于深度学习的恶意软件检测器对恶意软件样本的检测易受到恶意篡改的影响，即对抗性恶意软件样本。我们提出了一个实用的防御方法来对抗对抗性恶意软件样本，该方法受到随机平滑化的启发。我们使用随机剥离平滑化方案，而不是在随机化输入时使用高斯或拉普拉斯噪声，该方案剥离了可执行文件中的一定比例字节。在训练过程中，我们的随机剥离平滑化方案使用剥离版本的可执行文件来训练基分类器。在测试时，给定输入可执行文件的最终分类被视为在一组剥离版本的原始可执行文件上由分类器最常预测的类别。

    Malware detectors based on deep learning (DL) have been shown to be susceptible to malware examples that have been deliberately manipulated in order to evade detection, a.k.a. adversarial malware examples. More specifically, it has been show that deep learning detectors are vulnerable to small changes on the input file. Given this vulnerability of deep learning detectors, we propose a practical defense against adversarial malware examples inspired by randomized smoothing. In our work, instead of employing Gaussian or Laplace noise when randomizing inputs, we propose a randomized ablation-based smoothing scheme that ablates a percentage of the bytes within an executable. During training, our randomized ablation-based smoothing scheme trains a base classifier based on ablated versions of the executable files. At test time, the final classification for a given input executable is taken as the class most commonly predicted by the classifier on a set of ablated versions of the original exec
    
[^11]: 为疼痛开发知识图谱嵌入模型

    Development of a Knowledge Graph Embeddings Model for Pain. (arXiv:2308.08904v1 [cs.LG])

    [http://arxiv.org/abs/2308.08904](http://arxiv.org/abs/2308.08904)

    该论文开发了一个用于疼痛的知识图谱嵌入模型，以便研究疼痛概念及其关系。知识图谱嵌入可以将大型图谱转化为低维向量，从而提高计算效率，并用于各种任务。

    

    疼痛是一个复杂的概念，可以与其他概念相互关联，比如可能引起疼痛的疾病，可能缓解疼痛的药物等等。为了完全理解个体或整个人群所经历的疼痛背景，我们需要研究与疼痛相关的所有概念及其之间的关系。当建模电子健康记录中记录的疼痛时，这尤为有用。知识图谱通过一个相互链接的网络来表示概念和它们的关系，以一种计算可处理的方式实现语义和基于上下文的推理。然而，这些图谱可能过大以至于计算效率低下。而知识图谱嵌入有助于解决这个问题，它将图谱表示为低维向量空间。然后，这些嵌入可以用于各种下游任务，如分类和链接预测。构建这样一个知识图谱所需的与疼痛相关的各种关系可以o

    Pain is a complex concept that can interconnect with other concepts such as a disorder that might cause pain, a medication that might relieve pain, and so on. To fully understand the context of pain experienced by either an individual or across a population, we may need to examine all concepts related to pain and the relationships between them. This is especially useful when modeling pain that has been recorded in electronic health records. Knowledge graphs represent concepts and their relations by an interlinked network, enabling semantic and context-based reasoning in a computationally tractable form. These graphs can, however, be too large for efficient computation. Knowledge graph embeddings help to resolve this by representing the graphs in a low-dimensional vector space. These embeddings can then be used in various downstream tasks such as classification and link prediction. The various relations associated with pain which are required to construct such a knowledge graph can be o
    
[^12]: D-IF: 通过隐式分布场实现的不确定性感知人体数字化

    D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field. (arXiv:2308.08857v1 [cs.CV])

    [http://arxiv.org/abs/2308.08857](http://arxiv.org/abs/2308.08857)

    本文提出通过使用自适应不确定性分布，区分接近表面的点和其他点，从而在像素对齐形状恢复中取得显著改进。

    

    逼真的虚拟人在许多行业中起着关键作用，例如元宇宙、智能医疗和自动驾驶模拟。但是大规模创造具有高度逼真度的虚拟人仍然是一个挑战。深度隐式函数的应用开启了基于图像的三维穿着人体重建的新时代，实现了具有细节的像素对齐形状恢复。随后，绝大部分工作通过回归每个点的确定性隐式值来定位表面。然而，是否应该不考虑与表面的距离而将所有点都一视同仁呢？在本文中，我们提出用自适应不确定性分布替换隐式值，根据点与表面的距离对它们进行区分。这种简单的“值到分布”的转变显著改进了几乎所有的基准方法。此外，定性结果表明，使用我们的不确定性分布损失训练的模型可以产生令人满意的效果。

    Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple ``value to distribution'' transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can ca
    
[^13]: CMB：一个全面的中文医学基准

    CMB: A Comprehensive Medical Benchmark in Chinese. (arXiv:2308.08833v1 [cs.CL])

    [http://arxiv.org/abs/2308.08833](http://arxiv.org/abs/2308.08833)

    CMB是一个全面的中文医学基准，基于中国本土语言和文化框架设计，能够解决将英语医学评估翻译到本地环境中的上下文不一致问题。

    

    大型语言模型（LLMs）为在医学领域取得重大突破提供了可能性。建立一个标准化的医学基准成为衡量进展的基石。然而，不同地区的医学环境具有各自的特点，例如在中国境内传统中医的普遍性和重要性。因此，仅仅翻译基于英语的医学评估可能导致当地环境中的“上下文不一致”。为了解决这个问题，我们提出了一个名为CMB（Comprehensive Medical Benchmark in Chinese）的本地化医学基准，完全设计和根植于中国本土的语言和文化框架。尽管传统中医是这个评估的重要组成部分，但它并不构成其全部。使用这个基准，我们评估了几个知名的大规模LLMs，包括ChatGPT、GPT-4、专门的中文LLMs和专门用于医学领域的LLMs。

    Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in \textit{contextual incongruities} to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. It is worth not
    
[^14]: 对称加权一阶模型抽样的提升算法

    Lifted Algorithms for Symmetric Weighted First-Order Model Sampling. (arXiv:2308.08828v1 [cs.AI])

    [http://arxiv.org/abs/2308.08828](http://arxiv.org/abs/2308.08828)

    本文研究了对称加权一阶模型抽样的提升算法，通过设计一种扩展了计数量词的一阶逻辑的“域可提升抽样”的内容，证明了加权模型抽样问题也存在可处理的情况。

    

    加权模型计数（WMC）是计算命题公式的所有满足解（模型）的权重之和的任务。类似地，加权模型抽样（WMS）旨在以概率与它们相应的权重成比例地随机生成模型。WMC和WMS都难以精确求解，属于\#P-hard复杂度类别。然而，已知如果命题公式可以以紧凑的方式表示并用一阶逻辑表达，有时计数问题可能是可以处理的。在这种情况下，模型计数问题可以在域大小多项式时间内解决，并被称为“域可提升”。然后，就会出现以下问题：加权模型抽样是否也是如此？本文回答了这个问题，并肯定地回答了它。具体而言，我们在本文中通过设计一种扩展了计数量词的一阶逻辑的“域可提升抽样”的内容，证明了这个问题。

    Weighted model counting (WMC) is the task of computing the weighted sum of all satisfying assignments (i.e., models) of a propositional formula. Similarly, weighted model sampling (WMS) aims to randomly generate models with probability proportional to their respective weights. Both WMC and WMS are hard to solve exactly, falling under the \#P-hard complexity class. However, it is known that the counting problem may sometimes be tractable, if the propositional formula can be compactly represented and expressed in first-order logic. In such cases, model counting problems can be solved in time polynomial in the domain size, and are known as \textit{domain-liftable}. The following question then arises: Is it also the case for weighted model sampling? This paper addresses this question and answers it affirmatively. Specifically, we prove the \textit{domain-liftability under sampling} for the two-variables fragment of first-order logic with counting quantifiers in this paper, by devising an e
    
[^15]: 捕捉流行趋势：增强项目推荐的简化非个性化方法

    Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation. (arXiv:2308.08799v1 [cs.IR])

    [http://arxiv.org/abs/2308.08799](http://arxiv.org/abs/2308.08799)

    本论文提出了一种简化的非个性化方法PARE，通过预测最高流行度的项目进行推荐，填补了现有推荐方法忽略项目流行度的不足。实验证明PARE的性能优于复杂的方法。

    

    随着时间的推移，推荐系统已经越来越受到研究的关注。大多数现有的推荐方法侧重于通过历史的用户-项目交互来捕捉用户的个性化偏好，这可能会侵犯用户的隐私。此外，这些方法常常忽视了项目流行度的时间波动对用户决策的重要性。为了弥补这一差距，我们提出了Popularity-Aware Recommender（PARE），通过预测将达到最高流行度的项目来进行非个性化推荐。PARE由四个模块组成，分别关注不同的方面：流行度历史、时间影响、周期性影响和附加信息。最后，利用注意力层融合四个模块的输出。据我们所知，这是第一个在推荐系统中明确建模项目流行度的工作。广泛的实验证明，PARE的性能与复杂的方法相当甚至更好。

    Recommender systems have been gaining increasing research attention over the years. Most existing recommendation methods focus on capturing users' personalized preferences through historical user-item interactions, which may potentially violate user privacy. Additionally, these approaches often overlook the significance of the temporal fluctuation in item popularity that can sway users' decision-making. To bridge this gap, we propose Popularity-Aware Recommender (PARE), which makes non-personalized recommendations by predicting the items that will attain the highest popularity. PARE consists of four modules, each focusing on a different aspect: popularity history, temporal impact, periodic impact, and side information. Finally, an attention layer is leveraged to fuse the outputs of four modules. To our knowledge, this is the first work to explicitly model item popularity in recommendation systems. Extensive experiments show that PARE performs on par or even better than sophisticated st
    
[^16]: CodeCoT及其进展：学习像开发者一样编程和测试

    CodeCoT and Beyond: Learning to Program and Test like a Developer. (arXiv:2308.08784v1 [cs.SE])

    [http://arxiv.org/abs/2308.08784](http://arxiv.org/abs/2308.08784)

    本文介绍了CodeCoT和Beyond的学习方法，该方法可以帮助模型在处理任务时从少量特定数据中进行适应。通过链式思维引导，模型可以揭示多步推理过程中的认知过程，并通过自我检查不断优化输出。

    

    在自然语言处理领域，OpenAI开发的基于转换器的大型语言模型（LLM）如GPT-x模型已经彻底改变了现状。尽管这些模型具有令人印象深刻的能力，但它们在处理与其训练数据不同的任务时常常遇到挑战，造成性能下降。为了解决这个问题，出现了一种被称为少样本学习的有价值技术，允许LLM在最少的任务特定数据上进行适应。一种创新的策略，称为思维链提示（CoT），已被引入以指导LLM在多步推理过程中揭示认知过程。在本文中，我们提出了Code Chain-of-Thought（CodeCoT），它由两个组成部分组成：经典CodeCoT和自我检查CodeCoT。后者加入了自我检查，使模型能够迭代生成代码，制定测试用例并改善其输出。具体而言，该过程包括模型生成与分类别特征对应的测试示例。

    In natural language processing, transformer-based large language models (LLMs) like GPT-x models developed by OpenAI have revolutionized the landscape. Despite their impressive capabilities, these models often encounter challenges when handling tasks that differ from their training data, resulting in compromised performance. To address this, few-shot learning has emerged as a valuable technique, allowing LLMs to adapt with minimal task-specific data. One innovative strategy, known as Chain-of-Thought Prompting (CoT), has been introduced to guide LLMs in revealing cognitive processes during multi-step reasoning. In this paper, we propose Code Chain-of-Thought~(CodeCoT), which consists of two components: the Vanilla CodeCoT and the Self-exam CodeCoT. The latter incorporates self-examination, empowering the model to iteratively generate code, formulate test cases, and refine its outputs. Specifically, the process entails the generation of test examples by the model corresponding to the co
    
[^17]: 探索上下文学习的演示集成

    Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v1 [cs.CL])

    [http://arxiv.org/abs/2308.08780](http://arxiv.org/abs/2308.08780)

    本研究探索了上下文学习的演示集成方法，用于提高语言模型在给定任务的输入输出对中的预测性能。通过将演示分成子集并组合各子集的输出概率，我们得到了最终的预测结果。

    

    上下文学习通过向语言模型展示输入-输出对的示例来进行操作，即演示。上下文学习的标准方法是将演示与测试输入连接起来提示给语言模型。然而，这种方法存在一些问题。首先，连接方法几乎无法控制每个演示对模型预测的贡献。当一些演示与测试示例无关时，这可能不是最优的。其次，由于某些变换器模型对输入长度有限制，将许多示例放入上下文中可能是不可行的，特别是在处理长输入任务时。在本研究中，我们探索了演示集成（DENSE）作为简单连接的替代方法。模型使用演示的子集（即bucket）来预测输出，然后将每个子集得到的输出概率组合起来生成最终预测结果。我们使用GPT-j研究了不同的集成方法，并进行了实验。

    In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for a given task, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are irrelevant to the test example. Second, due to the input length limit of some transformer models, it might be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. \model predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment
    
[^18]: 大型语言模型在中国劳动市场的应用

    Large Language Models at Work in China's Labor Market. (arXiv:2308.08776v1 [econ.GN])

    [http://arxiv.org/abs/2308.08776](http://arxiv.org/abs/2308.08776)

    本文研究了大型语言模型（LLMs）对中国劳动市场的潜在影响，并分析了职业对LLM能力的暴露程度及其与工资水平和经验溢价之间的关系。研究结果表明，高薪和经验密集型工作可能面临更大的替代风险。此外，研究还开发了一个考虑行业暴露的经济增长模型，以量化AI采用对生产力和就业之间的权衡。这项研究为理解中国劳动市场中越来越强大的AI系统的影响提供了基础。

    

    本文探讨了大型语言模型（LLMs）对中国劳动市场的潜在影响。我们通过结合人类专业知识和LLM分类，按照Eloundou等人（2023）的方法分析了职业对LLM能力的暴露程度。然后将职业暴露程度聚合到行业水平上，得到行业暴露得分。结果表明，职业暴露和工资水平/经验溢价之间存在正相关关系，表明高薪和经验密集型的工作可能面临着LLM驱动软件的更大替代风险。行业暴露得分与专家评估和经济直觉相一致。我们还开发了一个考虑行业暴露的经济增长模型，以量化AI采用带来的生产力和就业之间的权衡。总体来说，本研究为理解中国越来越强大的AI系统对劳动市场的影响提供了分析基础。主要创新包括职业水平的暴露情况。

    This paper explores the potential impacts of large language models (LLMs) on the Chinese labor market. We analyze occupational exposure to LLM capabilities by incorporating human expertise and LLM classifications, following Eloundou et al. (2023)'s methodology. We then aggregate occupation exposure to the industry level to obtain industry exposure scores. The results indicate a positive correlation between occupation exposure and wage levels/experience premiums, suggesting higher-paying and experience-intensive jobs may face greater displacement risks from LLM-powered software. The industry exposure scores align with expert assessments and economic intuitions. We also develop an economic growth model incorporating industry exposure to quantify the productivity-employment trade-off from AI adoption. Overall, this study provides an analytical basis for understanding the labor market impacts of increasingly capable AI systems in China. Key innovations include the occupation-level exposure
    
[^19]: 差分隐私、语言公平性和训练数据影响：多语言语言模型的不可能性和可能性定理

    Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models. (arXiv:2308.08774v1 [cs.CL])

    [http://arxiv.org/abs/2308.08774](http://arxiv.org/abs/2308.08774)

    该论文研究了多语言语言模型在多语言压缩、语言公平性和透明性等方面的要求，并发现差分隐私与训练数据影响稀疏性之间存在相互制约的关系。

    

    语言模型如mBERT、XLM-R和BLOOM旨在实现多语言概括或压缩，以便于转移到大量（可能未知的）语言。然而，这些模型还应该具备隐私性、语言公平性和透明性，即将它们的预测与训练数据相关联。这些要求可以同时满足吗？我们表明，多语言压缩和语言公平性与差分隐私是兼容的，但差分隐私与训练数据影响稀疏性是相悖的，后者是透明性的目标。我们还对两个常见的自然语言处理任务进行了一系列实验，并在不同的隐私保证下评估了多语言压缩和训练数据影响稀疏性，更详细地探讨了这些权衡。我们的结果表明，我们需要开发一种共同优化这些目标的方法，以找到实际的权衡。

    Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.
    
[^20]: 使用强化学习的离散提示压缩

    Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])

    [http://arxiv.org/abs/2308.08758](http://arxiv.org/abs/2308.08758)

    本研究提出了一种使用强化学习的离散提示压缩方法（PCRL），以解决指令调整的语言模型中嵌入训练的挑战。PCRL采用了一种计算效率高的策略网络直接编辑提示，可以灵活应用于各种类型的LM，而不需要梯度访问或标记数据。

    

    指令调整的语言模型（LM）被用户广泛使用来解决与任务特定提示相关的各种问题。由于上下文窗口长度和计算成本的限制，鼓励开发压缩提示的方法。现有方法严重依赖于训练嵌入，这些嵌入被设计为容纳多个记号含义。这在解释性、固定数量的嵌入记号、在不同LM之间的可重用性以及与黑盒API交互时的不适用性方面带来了挑战。本研究提出了一种使用强化学习的提示压缩方法（PCRL），它解决了这些问题。PCRL采用了一种计算效率高的策略网络，直接编辑提示。PCRL的训练方法可以灵活地应用于各种类型的LM，以及只有解码器和编码器-解码器架构，而不需要使用梯度访问LM或标记数据进行训练。

    Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
    
[^21]: SurgicalSAM: 高效的可提示的手术器械分割

    SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation. (arXiv:2308.08746v1 [cs.CV])

    [http://arxiv.org/abs/2308.08746](http://arxiv.org/abs/2308.08746)

    SurgicalSAM是一种高效的手术器械分割方法，通过引入SurgicalSAM，可以有效地应用Segment Anything Model (SAM) 进行手术器械分割，解决了SAM在手术器械领域的泛化能力差和复杂多阶段流程的问题。

    

    Segment Anything Model (SAM) 是一种强大的基础模型，已经彻底改变了图像分割。为了将SAM应用于手术器械分割，常见的方法是定位器械的精确点或框，并将其用作SAM的提示，以零样本方式进行。然而，我们观察到这种简单的流程存在两个问题：（1）自然物体和手术器械之间的领域差距导致SAM的泛化能力差；（2）SAM依赖于精确的点或框位置进行准确的分割，要求要么经过广泛的手动引导，要么使用性能良好的专门检测器进行提示准备，这导致了一个复杂的多阶段流程。为了解决这些问题，我们引入了SurgicalSAM，一种新的端到端高效调优方法，以有效地将手术特定信息与SAM的预训练知识相结合，以改进泛化能力。

    The Segment Anything Model (SAM) is a powerful foundation model that has revolutionised image segmentation. To apply SAM to surgical instrument segmentation, a common approach is to locate precise points or boxes of instruments and then use them as prompts for SAM in a zero-shot manner. However, we observe two problems with this naive pipeline: (1) the domain gap between natural objects and surgical instruments leads to poor generalisation of SAM; and (2) SAM relies on precise point or box locations for accurate segmentation, requiring either extensive manual guidance or a well-performing specialist detector for prompt preparation, which leads to a complex multi-stage pipeline. To address these problems, we introduce SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to effectively integrate surgical-specific information with SAM's pre-trained knowledge for improved generalisation. Specifically, we propose a lightweight prototype-based class prompt encoder for tuning, wh
    
[^22]: PMET: 在Transformer中的精确模型编辑

    PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])

    [http://arxiv.org/abs/2308.08742](http://arxiv.org/abs/2308.08742)

    该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。

    

    模型编辑技术可以以较低的成本修改大型语言模型中的少量知识，并且已经取得了显著的成功。现有方法假设Transformer层隐藏状态是前馈网络的键值内存的值。它们通常优化Transformer层隐藏状态来记忆目标知识，并将其用于更新大型语言模型中前馈网络的权重。然而，Transformer层隐藏状态的信息流来自三个部分：多头自注意力、前馈网络和残差连接。现有方法忽视了Transformer层隐藏状态包含了前馈网络特别需要的信息这一事实。因此，模型编辑的性能下降。为了实现更精确的模型编辑，我们分析了多头自注意力和前馈网络的隐藏状态，发现多头自注意力编码了某些通用知识提取模式。这意味着当引入新知识时，多头自注意力的权重不需要更新。

    Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
    
[^23]: ReProHRL: 面向多目标导航的真实世界层次化代理方法

    ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents. (arXiv:2308.08737v1 [cs.RO])

    [http://arxiv.org/abs/2308.08737](http://arxiv.org/abs/2308.08737)

    本文提出了一种名为ReProHRL的层次化代理方法，通过强化学习实现了多目标导航任务，并使用物体检测器进行预处理。实验证明，ReProHRL方法在仿真和真实世界环境中表现优于现有方法。

    

    机器人已成功用于高精度任务的执行。在稀疏奖励和多目标的真实世界环境中，学习仍然是一个重大挑战，强化学习算法难以学习到好的策略。在仿真环境中进行训练，然后在真实世界进行微调是一种常见的方法。然而，适应真实世界的环境仍然具有挑战性。本文提出了一种名为"ReProHRL"的方法，该方法通过强化学习实现了分层多目标导航任务。我们还使用物体检测器作为预处理步骤，学习多目标导航并将其应用于真实世界。实证结果表明，所提出的ReProHRL方法在仿真和真实世界环境中的训练时间和性能方面优于现有基准方法。虽然这两种方法在单目标导航的简单环境中均实现了100%的成功率，但是在多目标导航问题上，ReProHRL方法表现更佳。

    Robots have been successfully used to perform tasks with high precision. In real-world environments with sparse rewards and multiple goals, learning is still a major challenge and Reinforcement Learning (RL) algorithms fail to learn good policies. Training in simulation environments and then fine-tuning in the real world is a common approach. However, adapting to the real-world setting is a challenge. In this paper, we present a method named Ready for Production Hierarchical RL (ReProHRL) that divides tasks with hierarchical multi-goal navigation guided by reinforcement learning. We also use object detectors as a pre-processing step to learn multi-goal navigation and transfer it to the real world. Empirical results show that the proposed ReProHRL method outperforms the state-of-the-art baseline in simulation and real-world environments in terms of both training time and performance. Although both methods achieve a 100% success rate in a simple environment for single goal-based navigati
    
[^24]: LLM-FuncMapper:通过LLM解释建筑法规中的复杂条款的函数识别

    LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM. (arXiv:2308.08728v1 [cs.AI])

    [http://arxiv.org/abs/2308.08728](http://arxiv.org/abs/2308.08728)

    LLM-FuncMapper提出了一种通过LLM实现对建筑法规中复杂条款的函数识别的方法，通过定义原子函数和开发提示模板来解决传统逻辑表示的限制。

    

    作为自动化规则检查（ARC）的关键阶段，对监管性文本的规则解释需要相当大的努力。然而，由于缺乏领域知识和传统逻辑表示的表达能力有限，解释具有隐式属性或复杂计算逻辑的监管条款仍然具有挑战性。因此，提出了一种基于大型语言模型（LLM）的识别各种监管条款所需的预定义函数的方法LLM-FuncMapper。首先，通过对建筑法规进行系统分析，定义了一系列原子函数，以捕捉隐式属性和复杂约束的共享计算逻辑，创建了常见块的数据库，用于解释监管条款。然后，开发了一个具有思维链的提示模板，并通过基于分类的调优策略进一步增强，以实现有效的函数识别功能。最后，验证了所提出的方法。

    As a vital stage of automated rule checking (ARC), rule interpretation of regulatory texts requires considerable effort. However, interpreting regulatory clauses with implicit properties or complex computational logic is still challenging due to the lack of domain knowledge and limited expressibility of conventional logic representations. Thus, LLM-FuncMapper, an approach to identifying predefined functions needed to interpret various regulatory clauses based on the large language model (LLM), is proposed. First, by systematically analysis of building codes, a series of atomic functions are defined to capture shared computational logics of implicit properties and complex constraints, creating a database of common blocks for interpreting regulatory clauses. Then, a prompt template with the chain of thought is developed and further enhanced with a classification-based tuning strategy, to enable common LLMs for effective function identification. Finally, the proposed approach is validated
    
[^25]: EdgeMA: 边缘设备上实时视频分析的模型适应系统

    EdgeMA: Model Adaptation System for Real-Time Video Analytics on Edge Devices. (arXiv:2308.08717v1 [cs.CV])

    [http://arxiv.org/abs/2308.08717](http://arxiv.org/abs/2308.08717)

    EdgeMA是一个实用高效的视频分析系统，通过提取统计纹理特征和使用重要性加权的模型适应方法，解决了边缘设备上实时视频分析中的数据漂移问题，显著提高了推理准确性。

    

    边缘设备上实时视频分析面对场景变化仍然是一项困难的任务。由于边缘设备的资源通常有限，边缘深度神经网络（DNN）相比一般的DNN具有更少的权重和较浅的架构。因此，它们只在有限的场景中表现良好，并对数据漂移敏感。本文介绍了EdgeMA，一种实用高效的视频分析系统，旨在适应实际场景下视频流随时间变化的模型，解决数据漂移问题。EdgeMA提取基于灰度共生矩阵的统计纹理特征，并使用随机森林分类器检测领域漂移。此外，我们还融入了一种基于重要性加权的模型适应方法，专门设计用于更新模型以应对标签分布的变化。通过对一个真实数据集对EdgeMA进行了严格评估，结果表明EdgeMA显著提高了推理准确性。

    Real-time video analytics on edge devices for changing scenes remains a difficult task. As edge devices are usually resource-constrained, edge deep neural networks (DNNs) have fewer weights and shallower architectures than general DNNs. As a result, they only perform well in limited scenarios and are sensitive to data drift. In this paper, we introduce EdgeMA, a practical and efficient video analytics system designed to adapt models to shifts in real-world video streams over time, addressing the data drift problem. EdgeMA extracts the gray level co-occurrence matrix based statistical texture feature and uses the Random Forest classifier to detect the domain shift. Moreover, we have incorporated a method of model adaptation based on importance weighting, specifically designed to update models to cope with the label distribution shift. Through rigorous evaluation of EdgeMA on a real-world dataset, our results illustrate that EdgeMA significantly improves inference accuracy.
    
[^26]: 数学推理的架构的概率性结果与认知交替（arXiv:2308.08714v1[cs.AI]）

    Probabilistic Results on the Architecture of Mathematical Reasoning Aligned by Cognitive Alternation. (arXiv:2308.08714v1 [cs.AI])

    [http://arxiv.org/abs/2308.08714](http://arxiv.org/abs/2308.08714)

    该论文研究了一种能够解决数学问题的机器，将定量推理系统分为思维过程和认知过程两部分，并提供了架构的概率性描述。

    

    我们设想一台能够解决数学问题的机器。将定量推理系统分为思维过程和认知过程两部分，我们提供了架构的概率性描述。

    We envision a machine capable of solving mathematical problems. Dividing the quantitative reasoning system into two parts: thought processes and cognitive processes, we provide probabilistic descriptions of the architecture.
    
[^27]: 人工智能中的意识：来自意识科学的洞见

    Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])

    [http://arxiv.org/abs/2308.08708](http://arxiv.org/abs/2308.08708)

    本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。

    

    当前或近期的人工智能系统是否能具有意识成为科学界关注的话题，也引起了公众的担忧。本报告提出并举例了一种严谨且经验基础的人工智能意识方法：根据我们目前最可信的神经科学理论对现有的人工智能系统进行详细评估。我们概述了几种广泛认可的科学意识理论，包括循环处理理论、全局工作空间理论、高阶理论、预测处理理论和注意模式理论。从这些理论中，我们推导出一些意识的“指示性特征”，并通过计算方法来评估人工智能系统是否具备这些特征。我们利用这些指示性特征来评估了几个近期的人工智能系统，并讨论了未来系统如何实现这些特征。我们的分析表明，目前没有现有的人工智能系统具有意识，但同时也显示出没有明显的建立具有意识的人工智能系统的障碍。

    Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
    
[^28]: 计划在想象中：基于学习抽象搜索空间的高级规划

    Planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v1 [cs.AI])

    [http://arxiv.org/abs/2308.08693](http://arxiv.org/abs/2308.08693)

    本论文提出了一个名为PiZero的新方法，该方法使代理能够在自己创建的抽象搜索空间中进行高级规划，不受真实环境限制，可在任意时间尺度进行规划，并处理连续动作空间和部分可观察性的设置。在多个领域的实验中，该方法胜过可比的先前方法而无需假设访问环境模拟器。

    

    我们提出了一种新的方法，称为PiZero，它使代理能够在自己创建的抽象搜索空间中进行计划，该搜索空间与真实环境完全解耦。与先前的方法不同，这使得代理能够以任意时间尺度进行高级规划，并以复合或时间扩展动作的形式进行推理，这在需要执行大量基本微操作以执行相关宏操作的环境中非常有用。此外，我们的方法比可比的先前方法更通用，因为它处理具有连续动作空间和部分可观察性的设置。我们在多个领域进行了评估，包括导航任务和Sokoban。实验证明，它在没有假设访问环境模拟器的情况下胜过可比的先前方法。

    We propose a new method, called PiZero, that gives an agent the ability to plan in an abstract search space of its own creation that is completely decoupled from the real environment. Unlike prior approaches, this enables the agent to perform high-level planning at arbitrary timescales and reason in terms of compound or temporally-extended actions, which can be useful in environments where large numbers of base-level micro-actions are needed to perform relevant macro-actions. In addition, our method is more general than comparable prior methods because it handles settings with continuous action spaces and partial observability. We evaluate our method on multiple domains, including navigation tasks and Sokoban. Experimentally, it outperforms comparable prior methods without assuming access to an environment simulator.
    
[^29]: 轻量级神经语言模型通过子空间嵌入进行适应性处理

    Lightweight Adaptation of Neural Language Models via Subspace Embedding. (arXiv:2308.08688v1 [cs.CL])

    [http://arxiv.org/abs/2308.08688](http://arxiv.org/abs/2308.08688)

    本论文提出了一种新的紧凑嵌入结构，通过牺牲部分准确度，减少预训练语言模型的内存占用。实验证明，该结构可以实现超过99.8%的压缩率。

    

    传统的神经词嵌入通常依赖于更丰富的词汇多样性。然而，语言模型倾向于通过单词嵌入参数来覆盖主要词汇，特别是对于通常覆盖其整体学习参数中的重要部分的多语言语言模型来说。在这项工作中，我们提出了一种新的紧凑嵌入结构，通过牺牲高达4%的绝对准确度来减少预训练语言模型的内存占用。嵌入向量的重建遵循一组子空间嵌入和通过从预训练语言模型中的标记之间的上下文关系进行的分配过程。子空间嵌入结构适应了掩码语言模型，以在相似性和文本蕴涵任务、句子和释义任务中评估我们的紧凑嵌入结构。我们的实验评估表明，与原始e相比，子空间嵌入实现了超过99.8%的压缩率。

    Traditional neural word embeddings are usually dependent on a richer diversity of vocabulary. However, the language models recline to cover major vocabularies via the word embedding parameters, in particular, for multilingual language models that generally cover a significant part of their overall learning parameters. In this work, we present a new compact embedding structure to reduce the memory footprint of the pre-trained language models with a sacrifice of up to 4% absolute accuracy. The embeddings vectors reconstruction follows a set of subspace embeddings and an assignment procedure via the contextual relationship among tokens from pre-trained language models. The subspace embedding structure calibrates to masked language models, to evaluate our compact embedding structure on similarity and textual entailment tasks, sentence and paraphrase tasks. Our experimental evaluation shows that the subspace embeddings achieve compression rates beyond 99.8% in comparison with the original e
    
[^30]: 量化过拟合: 引入过拟合指数

    Quantifying Overfitting: Introducing the Overfitting Index. (arXiv:2308.08682v1 [cs.LG])

    [http://arxiv.org/abs/2308.08682](http://arxiv.org/abs/2308.08682)

    本文引入了过拟合指数（OI），通过对乳腺超声图像数据集和MNIST数据集进行广泛实验，使用了多种架构，展示了OI的实用性和区分能力。结果表明，不同架构的过拟合行为存在差异，并强调了数据增强对于较小和更专业的数据集的缓解影响。OI为解决过拟合问题提供了一种有希望的途径。

    

    在快速发展的机器学习领域，确保模型的泛化能力仍然是一个重要的挑战。过拟合是指模型在训练数据上表现良好但在未见数据上表现不佳的现象，一直是个不容忽视的问题。本文引入了过拟合指数（OI），这是一个新颖的度量方法，用于定量评估模型的过拟合倾向。通过对乳腺超声图像数据集（BUS）和MNIST数据集的广泛实验，使用了MobileNet、U-Net、ResNet、Darknet和ViT-32等架构，我们展示了OI的实用性和区分能力。我们的结果突出了不同架构之间的过拟合行为的变化，并强调了数据增强对较小和更专业的数据集的缓解影响。ViT-32在MNIST上的表现进一步强调了某些模型的鲁棒性和数据集的全面性。通过提供客观视角来评估过拟合，OI为解决过拟合问题提供了一种有希望的途径。

    In the rapidly evolving domain of machine learning, ensuring model generalizability remains a quintessential challenge. Overfitting, where a model exhibits superior performance on training data but falters on unseen data, is a recurrent concern. This paper introduces the Overfitting Index (OI), a novel metric devised to quantitatively assess a model's tendency to overfit. Through extensive experiments on the Breast Ultrasound Images Dataset (BUS) and the MNIST dataset using architectures such as MobileNet, U-Net, ResNet, Darknet, and ViT-32, we illustrate the utility and discernment of the OI. Our results underscore the variable overfitting behaviors across architectures and highlight the mitigative impact of data augmentation, especially on smaller and more specialized datasets. The ViT-32's performance on MNIST further emphasizes the robustness of certain models and the dataset's comprehensive nature. By providing an objective lens to gauge overfitting, the OI offers a promising aven
    
[^31]: 使用问题、答案和修订的数据库回答模糊问题

    Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions. (arXiv:2308.08661v1 [cs.CL])

    [http://arxiv.org/abs/2308.08661](http://arxiv.org/abs/2308.08661)

    这项研究通过利用从维基百科生成的一组明确问题的数据库，提出了一种用于回答模糊问题的新技术，在回答性能和歧义问题消除方面取得了显著提高。

    

    许多开放领域的问题都没有明确的规定，因此可能有多个可能的答案，每个答案在问题的不同解释下都是正确的。回答这样模糊的问题具有挑战性，因为它需要从多个段落中检索并推理出不同的信息。我们提出了一种新的回答模糊问题的最新技术，利用从维基百科生成的一组明确问题的数据库。在具有挑战性的ASQA基准测试中，我们的方法在召回率测量上提高了15%（相对改进），在评估从预测输出中消除歧义问题的度量上提高了10%。从生成的问题数据库中检索还大大提高了多样的段落检索（通过间接匹配用户问题q到段落p，通过从p生成的问题q'）。

    Many open-domain questions are under-specified and thus have multiple possible answers, each of which is correct under a different interpretation of the question. Answering such ambiguous questions is challenging, as it requires retrieving and then reasoning about diverse information from multiple passages. We present a new state-of-the-art for answering ambiguous questions that exploits a database of unambiguous questions generated from Wikipedia. On the challenging ASQA benchmark, which requires generating long-form answers that summarize the multiple answers to an ambiguous question, our method improves performance by 15% (relative improvement) on recall measures and 10% on measures which evaluate disambiguating questions from predicted outputs. Retrieving from the database of generated questions also gives large improvements in diverse passage retrieval (by matching user questions q to passages p indirectly, via questions q' generated from p).
    
[^32]: AdaptEx：一个自助上下文赌博平台

    AdaptEx: A Self-Service Contextual Bandit Platform. (arXiv:2308.08650v1 [cs.IR])

    [http://arxiv.org/abs/2308.08650](http://arxiv.org/abs/2308.08650)

    AdaptEx是一个自助上下文赌博平台，通过利用多臂赌博算法个性化用户体验并提供最优解，同时最小化传统测试方法的成本和时间。它能够在不断变化的内容和“冷启动”情况下快速迭代。

    

    本文介绍了AdaptEx，这是一个在Expedia Group广泛使用的自助上下文赌博平台，它利用多臂赌博算法以规模化的方式个性化用户体验。AdaptEx考虑了每个访问者的独特上下文，选择了最优的变体，并能够快速学习每次互动。它提供了一个强大的解决方案，既能改善用户体验，同时又能最大限度地减少传统测试方法所需的成本和时间。该平台能够在内容不断变化和持续“冷启动”情况下，优雅地快速迭代朝着最优解前进。

    This paper presents AdaptEx, a self-service contextual bandit platform widely used at Expedia Group, that leverages multi-armed bandit algorithms to personalize user experiences at scale. AdaptEx considers the unique context of each visitor to select the optimal variants and learns quickly from every interaction they make. It offers a powerful solution to improve user experiences while minimizing the costs and time associated with traditional testing methods. The platform unlocks the ability to iterate towards optimal product solutions quickly, even in ever-changing content and continuous "cold start" situations gracefully.
    
[^33]: 实现零内存占用的脉冲神经网络训练

    Towards Zero Memory Footprint Spiking Neural Network Training. (arXiv:2308.08649v1 [cs.NE])

    [http://arxiv.org/abs/2308.08649](http://arxiv.org/abs/2308.08649)

    这项研究提出了一种具有极低内存占用的脉冲神经网络训练框架，并设计了可逆SNN节点和简化的反向传播算法，显著降低了内存使用和计算复杂性。

    

    受生物启发的脉冲神经网络（SNN）以离散时间事件（脉冲）而非连续值处理信息，由于其对硬件友好和高能效的特点，引起了广泛关注。然而，SNN的训练需要大量的内存占用，因为需要额外存储脉冲或事件，导致复杂的结构和动态设置。为了解决SNN训练中的内存限制，本文引入了一种创新的框架，具有异常低的内存占用。我们设计了一个可逆的SNN节点，保持高精度，并实现了与当前SNN节点相比的58.65倍的内存使用减少。我们提出了一种独特的算法来简化我们可逆SNN节点的反向传播过程。这显著减少了反向浮点数运算。

    Biologically-inspired Spiking Neural Networks (SNNs), processing information using discrete-time events known as spikes rather than continuous values, have garnered significant attention due to their hardware-friendly and energy-efficient characteristics. However, the training of SNNs necessitates a considerably large memory footprint, given the additional storage requirements for spikes or events, leading to a complex structure and dynamic setup. In this paper, to address memory constraint in SNN training, we introduce an innovative framework, characterized by a remarkably low memory footprint. We \textbf{(i)} design a reversible SNN node that retains a high level of accuracy. Our design is able to achieve a $\mathbf{58.65\times}$ reduction in memory usage compared to the current SNN node. We \textbf{(ii)} propose a unique algorithm to streamline the backpropagation process of our reversible SNN node. This significantly trims the backward Floating Point Operations Per Second (FLOPs), 
    
[^34]: FedPop: 联邦式基于人口的超参数调优

    FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])

    [http://arxiv.org/abs/2308.08634](http://arxiv.org/abs/2308.08634)

    FedPop是一种用于解决联邦学习中超参数调优问题的新算法，它采用基于人口的进化算法来优化客户端和服务器上的超参数。

    

    联邦学习（FL）是一种分布式机器学习（ML）范式，多个客户端在不集中本地数据的情况下共同训练ML模型。与传统的ML流程类似，FL中的客户端本地优化和服务器聚合过程对超参数（HP）的选择非常敏感。尽管在集中式ML中对调优HP进行了广泛研究，但将这些方法应用于FL时会产生次优结果。这主要是因为它们的“调优后训练”框架对于计算能力有限的FL不合适。虽然一些方法已经提出用于FL中的HP调优，但这些方法仅限于客户端本地更新的HP。在这项工作中，我们提出了一种名为联邦式基于人口的超参数调优（FedPop）的新型HP调优算法，以解决这个重要但具有挑战性的问题。FedPop采用基于人口的进化算法来优化HP，此算法适用于客户端和服务器上的各种HP类型。

    Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
    
[^35]: 基于LSTM的GRACE加速计数据预测模型

    LSTM-Based Forecasting Model for GRACE Accelerometer Data. (arXiv:2308.08621v1 [cs.LG])

    [http://arxiv.org/abs/2308.08621](http://arxiv.org/abs/2308.08621)

    本研究基于LSTM网络，提出了一种填充GRACE卫星加速计数据间断的方法，并成功预测了三个轴上的加速计数据。

    

    重力恢复与气候实验（GRACE）卫星任务从2002年到2017年，为监测地球重力场变化提供了宝贵的数据集，为地球物理学和水文学等多种应用提供了支持。随后，在2018年，GRACE Follow-On继续进行数据收集。从卫星上不同仪器的集成导出的月度地球重力场数据，由于多种因素的影响，包括GRACE任务开始以来某些仪器的观测间断，存在不一致性。鉴于现在已经有二十多年的GRACE和GRACE Follow-On数据可用，本文提出了一种填充数据间断并预测GRACE加速计数据的方法。具体而言，我们着重于加速计数据，并采用长短期记忆（LSTM）网络来训练一个能够预测三个轴上加速计数据的模型。在本研究中，我们描述了预处理加速计数据的方法论。

    The Gravity Recovery and Climate Experiment (GRACE) satellite mission, spanning from 2002 to 2017, has provided a valuable dataset for monitoring variations in Earth's gravity field, enabling diverse applications in geophysics and hydrology. The mission was followed by GRACE Follow-On in 2018, continuing data collection efforts. The monthly Earth gravity field, derived from the integration different instruments onboard satellites, has shown inconsistencies due to various factors, including gaps in observations for certain instruments since the beginning of the GRACE mission.  With over two decades of GRACE and GRACE Follow-On data now available, this paper proposes an approach to fill the data gaps and forecast GRACE accelerometer data. Specifically, we focus on accelerometer data and employ Long Short-Term Memory (LSTM) networks to train a model capable of predicting accelerometer data for all three axes.  In this study, we describe the methodology used to preprocess the accelerometer
    
[^36]: 通过新的框架——思维图，提升大型语言模型的逻辑推理能力

    Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought. (arXiv:2308.08614v1 [cs.LG])

    [http://arxiv.org/abs/2308.08614](http://arxiv.org/abs/2308.08614)

    本文提出了一种新的提示技术——思维图（GoT），通过在三个不断升级的挑战中的测试，我们的方法在多步逻辑推理问题上表现优于GPT-4，并且相比最先进的提示方法思维树（ToT），我们的方法有更高的准确性提升。

    

    近期大规模模型（如GPT-4）的进展展示了在解决标准查询方面的卓越能力。然而，面对需要多步逻辑推理的复杂问题时，它们的准确性急剧下降。当前的研究已经探索了提示工程领域，以增强这些模型的推理能力。本文提出了一种创新的提示技术，称为“思维图（GoT）”。通过在三个不断升级的挑战上进行测试：24点游戏，高阶多项式方程的解析，以及递归数列的公式推导，我们的方法优于GPT-4，在每个任务中实现了$89.7\%$、$86\%$和$56\%$的准确性改进。此外，与最先进的提示方法“思维树（ToT）”相比，我们的方法平均准确性提升了$23\%$、$24\%$和$15\%$。

    Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\%$, $24\%$, and $15\%$.
    
[^37]: 农业中整合可再生能源：基于深度强化学习的方法

    Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach. (arXiv:2308.08611v1 [cs.AI])

    [http://arxiv.org/abs/2308.08611](http://arxiv.org/abs/2308.08611)

    本文开发了一个基于深度强化学习的框架，通过深度Q网络（DQN）优化光伏系统在农业中的安装决策制定。该研究对于提高能源效率、减少环境影响并增加农业利润具有重要意义。

    

    本文研究了使用深度Q网络（DQN）来优化农业领域光伏系统安装的决策制定。该研究开发了一个DQN框架，以帮助农业投资者根据安装预算、政府激励措施、能源需求、系统成本和长期效益等因素作出明智决策。通过实现一个奖励机制，DQN学习如何在光伏系统集成方面做出数据驱动的决策。该分析提供了深入了解DQN如何支持投资者在农业中进行光伏系统安装决策的综合理解。这项研究对于促进可持续和高效的农业实践，同时为未来该领域的进展铺平了道路具有重要意义。通过利用DQN，农业投资者可以做出优化决策，提高能源效率，减少环境影响，并增加利润。该研究对推动农业领域的进步做出了贡献。

    This article investigates the use of Deep Q-Networks (DQNs) to optimize decision-making for photovoltaic (PV) systems installations in the agriculture sector. The study develops a DQN framework to assist agricultural investors in making informed decisions considering factors such as installation budget, government incentives, energy requirements, system cost, and long-term benefits. By implementing a reward mechanism, the DQN learns to make data-driven decisions on PV integration. The analysis provides a comprehensive understanding of how DQNs can support investors in making decisions about PV installations in agriculture. This research has significant implications for promoting sustainable and efficient farming practices while also paving the way for future advancements in this field. By leveraging DQNs, agricultural investors can make optimized decisions that improve energy efficiency, reduce environmental impact, and enhance profitability. This study contributes to the advancement o
    
[^38]: FootGPT：在最小设置上进行的大规模语言模型开发实验

    FootGPT : A Large Language Model Development Experiment on a Minimal Setting. (arXiv:2308.08610v1 [cs.CL])

    [http://arxiv.org/abs/2308.08610](http://arxiv.org/abs/2308.08610)

    本文介绍了一个在最小设置上进行的大规模语言模型开发实验，研究发现准确的语言模型的关键在于适当的数据集内容和训练策略，而不是神经参数数量、训练时长或数据集大小。通过对一个10亿参数规模的通用因果语言模型进行微调，并使用商业语言模型提供的精简段落和问答对构建数据集，可以有效解释足球数据。

    

    最近的实证观察表明，相比于神经参数数量、训练时长或数据集大小，开发准确的语言模型的最重要方面可能是适当的数据集内容和训练策略。基于这个观点，我们选择使用低秩适应性对一个10亿参数规模的通用因果语言模型进行微调，数据集由意大利足球联赛前十轮的球队统计信息构建，并使用强大的商业语言模型提供的精简段落和问答对。我们将训练时长保持相对较短，以提供我们最小设置探索的基础。在本文中，我们分享了与使用有限资源解释足球数据的特定目的语言模型开发相关的关键观察结果。

    With recent empirical observations, it has been argued that the most significant aspect of developing accurate language models may be the proper dataset content and training strategy compared to the number of neural parameters, training duration or dataset size. Following this argument, we opted to fine tune a one billion parameter size trained general purpose causal language model with a dataset curated on team statistics of the Italian football league first ten game weeks, using low rank adaptation. The limited training dataset was compiled based on a framework where a powerful commercial large language model provides distilled paragraphs and question answer pairs as intended. The training duration was kept relatively short to provide a basis for our minimal setting exploration. We share our key observations on the process related to developing a specific purpose language model which is intended to interpret soccer data with constrained resources in this article.
    
[^39]: 关于增强人类/认知合奏中的认知准确性和认知精度的研究

    On the Augmentation of Cognitive Accuracy and Cognitive Precision in Human/Cog Ensembles. (arXiv:2308.08581v1 [cs.HC])

    [http://arxiv.org/abs/2308.08581](http://arxiv.org/abs/2308.08581)

    本文介绍了关于人类/认知合奏中的认知准确性和认知精度的研究。实验结果表明，通过认知系统提供的信息，人类的认知准确性和认知精度得到了提高。

    

    每当人类使用工具时，人类的表现会得到增强。认知系统是一种新型的工具，不断增强认知能力，现在可以执行高级认知任务，这些任务以前被认为只有人类能够完成。使用这种工具，也被称为认知(cog)，预计将会导致人类认知增强水平的进一步提高。在人类认知合奏中，人类和认知系统之间进行着合作、点对点和协作的对话，人类认知能力因此得到增强。因此，人类认知合奏能够实现比单独的人类或认知系统更多的工作。本文介绍了两项研究结果，旨在衡量认知的准确性——产生正确结果的能力，以及认知的精度——只产生正确结果的倾向。结果表明，信息的使用可以提高认知准确性和认知精度。

    Whenever humans use tools human performance is enhanced. Cognitive systems are a new kind of tool continually increasing in cognitive capability and are now performing high level cognitive tasks previously thought to be explicitly human. Usage of such tools, known as cogs, are expected to result in ever increasing levels of human cognitive augmentation. In a human cog ensemble, a cooperative, peer to peer, and collaborative dialog between a human and a cognitive system, human cognitive capability is augmented as a result of the interaction. The human cog ensemble is therefore able to achieve more than just the human or the cog working alone. This article presents results from two studies designed to measure the effect information supplied by a cog has on cognitive accuracy, the ability to produce the correct result, and cognitive precision, the propensity to produce only the correct result. Both cognitive accuracy and cognitive precision are shown to be increased by information of diff
    
[^40]: PEvoLM: 蛋白质序列进化信息语言模型

    PEvoLM: Protein Sequence Evolutionary Information Language Model. (arXiv:2308.08578v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.08578](http://arxiv.org/abs/2308.08578)

    PEvoLM是一种蛋白质序列进化信息语言模型，它利用嵌入式语言模型将蛋白质序列转换为数字向量表示。

    

    随着蛋白质序列数据库的指数增长，多序列比对(PSI-BLAST等)通过耗费时间的数据库搜索来获取进化信息。这些搜索引擎生成的位置特异性评分矩阵(PSSMs)对于生物信息学和计算生物学领域的许多机器学习模型来说是至关重要的输入。蛋白序列是由称为氨基酸(AAs)的连续标记或字符组成的。类比自然语言使我们能够利用自然语言处理(NLP)领域的最新进展，将NLP的最先进算法应用于生物信息学。本研究提出了一种嵌入式语言模型(ELMo)，将蛋白质序列转换为数字向量表示。尽管原始ELMo采用了一个两层双向长短期记忆网络(LSTMs)以及一个用于正向处理的双路径架构，

    With the exponential increase of the protein sequence databases over time, multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive and time-consuming database search to retrieve evolutionary information. The resulting position-specific scoring matrices (PSSMs) of such search engines represent a crucial input to many machine learning (ML) models in the field of bioinformatics and computational biology. A protein sequence is a collection of contiguous tokens or characters called amino acids (AAs). The analogy to natural language allowed us to exploit the recent advancements in the field of Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art algorithms to bioinformatics. This research presents an Embedding Language Model (ELMo), converting a protein sequence to a numerical vector representation. While the original ELMo trained a 2-layer bidirectional Long Short-Term Memory (LSTMs) network following a two-path architecture, one for the forwar
    
[^41]: 自然启发的特征选择算法在预测学生表现中的能力的比较分析

    A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance. (arXiv:2308.08574v1 [cs.LG])

    [http://arxiv.org/abs/2308.08574](http://arxiv.org/abs/2308.08574)

    本研究对比分析了12种自然启发的特征选择算法在预测学生表现中的能力，发现利用这些算法进行特征选择并结合传统机器学习算法可以提高预测准确性，并减少特征集大小。

    

    预测学生表现对于有效防止风险学生失败至关重要。本文分析了一套12个自然启发算法在预测学生表现中的相对性能，包括基于实例的点击流数据、课内单一课程表现以及同时参加多个课程时的表现，发现利用自然启发的算法进行特征选择并结合传统机器学习算法进行分类，可以提高预测准确性，并减少特征集大小的2/3。

    Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
    
[^42]: 初级编程教育中的大型语言模型：ChatGPT在性能和对评估的影响上的表现

    Large Language Models in Introductory Programming Education: ChatGPT's Performance and Implications for Assessments. (arXiv:2308.08572v1 [cs.SE])

    [http://arxiv.org/abs/2308.08572](http://arxiv.org/abs/2308.08572)

    本文研究了大型语言模型在解决初级编程任务中的性能，并得出了利用这些模型进行编程教育和评估的推论。

    

    本文研究了大型语言模型（LLMs）ChatGPT-3.5和GPT-4在解决初级编程任务中的性能。基于性能，得出了利用LLMs进行教学场景和评估格式的推论。为了分析，从免费网站CodingBat中选择了72个用于初学者的Python任务。完整的任务描述被用作LLMs的输入，生成的回复则使用CodingBat的单元测试进行评估。此外，还分析了文本解释和程序代码的普遍可用性。结果显示了94.4%至95.8%的正确回答高分数以及可靠的文本解释和程序代码可用性，这为将LLMs纳入编程教育和评估提供了新的途径。

    This paper investigates the performance of the Large Language Models (LLMs) ChatGPT-3.5 and GPT-4 in solving introductory programming tasks. Based on the performance, implications for didactic scenarios and assessment formats utilizing LLMs are derived. For the analysis, 72 Python tasks for novice programmers were selected from the free site CodingBat. Full task descriptions were used as input to the LLMs, while the generated replies were evaluated using CodingBat's unit tests. In addition, the general availability of textual explanations and program code was analyzed. The results show high scores of 94.4 to 95.8% correct responses and reliable availability of textual explanations and program code, which opens new ways to incorporate LLMs into programming education and assessment.
    
[^43]: KMF: 基于知识的多方位表示学习用于零样本节点分类

    KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification. (arXiv:2308.08563v1 [cs.LG])

    [http://arxiv.org/abs/2308.08563](http://arxiv.org/abs/2308.08563)

    本文提出了一种基于知识的多方位框架（KMF），用于零样本节点分类任务。该框架通过提取知识图谱中的主题来增强标签语义，以改善模型的泛化能力。

    

    最近，零样本节点分类（ZNC）在图数据分析中变得越来越重要。该任务旨在预测在训练过程中未观察到的未知类别的节点。现有的工作主要利用图神经网络(GNNs)将特征的原型和标签的语义联系起来，从而实现从已观察到的类别到未观察到的类别的知识迁移。然而，以往的研究忽视了特征-语义对齐中多方位语义方向的存在，即节点的内容通常涵盖与多个标签的语义相关的不同主题。因此，有必要区分和判断影响认知能力的语义因素，以提高模型的泛化性能。为此，我们提出了一种基于知识的多方位框架（KMF），通过提取基于知识图谱（KG）的主题来增强标签语义的丰富性。然后，将每个节点的内容重构为主题级别的表示。

    Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level repre
    
[^44]: 未来药物发现的实施：基于量子的机器学习模拟(QMLS)。

    Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.08561](http://arxiv.org/abs/2308.08561)

    该论文介绍了一种名为QMLS的新概念，通过结合机器学习和量子模拟的方法，可以缩短药物研发的时间和降低成本。通过生成命中物和优化分子的过程，可以大大提高药物发现的效率。

    

    药物研发的研究与开发(R&D)阶段是一个漫长而昂贵的过程。为了改革这个过程，我们引入了新概念QMLS，将整个R&D阶段缩短到三到六个月，成本仅为五到八万美元。对于命中产生，机器学习分子生成(MLMG)根据目标蛋白的分子结构生成可能的命中物，而量子模拟(QS)根据与目标蛋白的反应和结合效果过滤原始实验中的分子。然后，对于铅优化，从MLMG和QS生成和过滤的结果分子进行比较，并通过机器学习分子变异(MLMV)将那些出现在两个过程中的分子制成数十种分子变体，而其他分子只制成几种变体。最后，所有优化的分子将经过多轮高标准的QS过滤，以确保反应效果。

    The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
    
[^45]: BIRP: 基于多模式匹配的比特币信息检索预测模型

    BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal Pattern Matching. (arXiv:2308.08558v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.08558](http://arxiv.org/abs/2308.08558)

    本文提出了一种基于多模式匹配的比特币信息检索预测模型(BIRP)，通过排列相似的过去图表运动来提高预测能力，应用于比特币市场。

    

    在随机行走假设下，金融时间序列被认为是鞅过程。为了发现金融市场中隐藏的可重复模式，人们开发了各种多模式匹配算法，而不仅仅依靠原始价格进行投资决策。许多基于图表的模式匹配工具只能在当前图表模式的基础上检索类似的过去图表模式，并将整个解释和预测分析以及最终的投资决策留给投资者。本文提出了一种基于当前图表信息对相似过去图表运动进行排序的方法，并证明利用这些额外特征可以提高我们模型的方向预测能力。由于比特币具有高度波动的价格，使其难以预测其未来走势，因此我们将我们的排序和方向预测建模方法应用于比特币。

    Financial time series have historically been assumed to be a martingale process under the Random Walk hypothesis. Instead of making investment decisions using the raw prices alone, various multimodal pattern matching algorithms have been developed to help detect subtly hidden repeatable patterns within the financial market. Many of the chart-based pattern matching tools only retrieve similar past chart (PC) patterns given the current chart (CC) pattern, and leaves the entire interpretive and predictive analysis, thus ultimately the final investment decision, to the investors. In this paper, we propose an approach of ranking similar PC movements given the CC information and show that exploiting this as additional features improves the directional prediction capacity of our model. We apply our ranking and directional prediction modeling methodologies on Bitcoin due to its highly volatile prices that make it challenging to predict its future movements.
    
[^46]: 基于因果关系的健康应用数据和个人信息管理工具的联接

    Causally Linking Health Application Data and Personal Information Management Tools. (arXiv:2308.08556v1 [cs.HC])

    [http://arxiv.org/abs/2308.08556](http://arxiv.org/abs/2308.08556)

    本文研究了如何将健康应用数据和个人信息管理工具进行关联，解决了健康数据可视化中的变量关系问题，并提出了利用个人信息管理工具的上下文信息来增强健康和福祉应用程序的方法。

    

    许多国家的消费者健康设备（如智能手表、睡眠监测器、智能称等）的普及不仅引起了对健康监测的兴趣的增长，还引发了大量支持普通公众探索此类数据的“智能”应用程序的开发，有时还与专业健康服务进行整合。尽管这些设备向用户提供了各种健康数据流，但这些数据流通常以独立的时间序列可视化形式呈现，其中健康变量之间的潜在关系并未明确可见。此外，尽管工作和社交连接等生活的其他方面越来越数字化，但健康和福祉应用程序很少利用广泛使用的个人信息管理工具（如共享日历和电子邮件系统）提供的潜在有用的上下文信息。

    The proliferation of consumer health devices such as smart watches, sleep monitors, smart scales, etc, in many countries, has not only led to growing interest in health monitoring, but also to the development of a countless number of ``smart'' applications to support the exploration of such data by members of the general public, sometimes with integration into professional health services. While a variety of health data streams has been made available by such devices to users, these streams are often presented as separate time-series visualizations, in which the potential relationships between health variables are not explicitly made visible. Furthermore, despite the fact that other aspects of life, such as work and social connectivity, have become increasingly digitised, health and well-being applications make little use of the potentially useful contextual information provided by widely used personal information management tools, such as shared calendar and email systems. This paper 
    
[^47]: AI辅助调查区块链参数：风险加密货币和价格因素

    AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors. (arXiv:2308.08554v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.08554](http://arxiv.org/abs/2308.08554)

    本论文使用人工智能算法分析历史数据和区块链参数，找出影响加密货币价格的因素并识别风险加密货币。

    

    近年来，加密货币已成为投资者和学者广泛研究的热门话题。为了做出明智的投资决策，了解影响加密货币价格的因素并识别风险加密货币是至关重要的。本文重点分析历史数据，并使用人工智能算法对区块链参数进行分析，以识别影响加密货币价格的因素并找到风险加密货币。我们对历史加密货币的链上数据进行分析，并测量了价格与其他参数之间的相关性。此外，我们使用聚类和分类方法来更好地理解加密货币，并将其分类为风险或非风险。分析结果显示，大部分加密货币（39％）已退出市场，而只有很小一部分（10％）存活了1000多天。我们的分析揭示了一个显著的负相关关系

    Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative
    
[^48]: 基于双向预测的6D物体姿态估计中的点对注意力的利用

    Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v1 [cs.CV])

    [http://arxiv.org/abs/2308.08518](http://arxiv.org/abs/2308.08518)

    本文提出了一个具有点对注意力感知机制的双向对应预测网络，通过利用模型点和场景点之间的相关性进行点对匹配学习，解决了传统方法对观察质量和遮挡的依赖性问题，并在实验证明其在物体姿态估计任务上优于其他最先进的方法。

    

    传统的几何注册估计方法仅隐式地利用CAD模型，这导致它们对观察质量的依赖性和对遮挡的不足。为了解决这个问题，本文提出了一个具有点对注意力感知机制的双向对应预测网络。该网络不仅要求模型点预测对应关系，还明确地对观察和模型先验之间的几何相似性进行建模。我们的关键见解是，每个模型点和场景点之间的相关性为学习点对匹配提供了关键信息。为了进一步解决特征分布分歧带来的相关性噪声，我们设计了一个简单但有效的伪孪生网络来改善特征的一致性。在线MOD、YCB-Video和Occ-LineMOD的公共数据集上的实验结果表明，所提出的方法在性能上优于其他最先进的方法。

    Traditional geometric registration based estimation methods only exploit the CAD model implicitly, which leads to their dependence on observation quality and deficiency to occlusion.To address the problem,the paper proposes a bidirectional correspondence prediction network with a point-wise attention-aware mechanism. This network not only requires the model points to predict the correspondence but also explicitly models the geometric similarities between observations and the model prior.} Our key insight is that the correlations between each model point and scene point provide essential information for learning point-pair matches. To further tackle the correlation noises brought by feature distribution divergence, we design a simple but effective pseudo-siamese network to improve feature homogeneity.Experimental results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that the proposed method achieves better performance than other state-of-the-art methods under the sa
    
[^49]: 可解释的多视角深度网络方法论在实验物理中的应用

    Explainable Multi-View Deep Networks Methodology for Experimental Physics. (arXiv:2308.08206v1 [cs.CV])

    [http://arxiv.org/abs/2308.08206](http://arxiv.org/abs/2308.08206)

    该论文介绍了一个可解释的多视角深度网络方法论，应用于实验物理中的多种成像表达分析。该方法论解决了多视角模型可解释性不足的问题。

    

    物理实验常涉及多种成像表达，如X射线扫描和显微图像。深度学习模型已广泛应用于这些实验的监督分析中。合并不同的图像表达经常需要正确分析和做出决策。因此，多视角数据应运而生 - 数据集中的每个样本由来自不同角度、来源或模态的视图描述。多视角学习的概念解决了这些问题。理解深度学习模型的决策过程对于可靠和可信的分析至关重要。因此，最近提出了许多可解释性方法。然而，多视角模型缺乏适当的可解释性，由于其架构的复杂性，难以解释。在本文中，我们提出了适用于视觉领域的不同多视角架构，每个架构都适合解决不同的问题，并提出了解释多视角模型的方法论。

    Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining th
    
[^50]: 从稀疏视角视频中生成可重光和可动化的神经化身

    Relightable and Animatable Neural Avatar from Sparse-View Video. (arXiv:2308.07903v1 [cs.CV])

    [http://arxiv.org/abs/2308.07903](http://arxiv.org/abs/2308.07903)

    本文提出了一种从稀疏视角视频中创建可重光和可动化的神经化身的方法，通过使用Hierarchical Distance Query（HDQ）算法来近似任意人体姿态下的世界空间距离，并使用这些距离来进行材料恢复和重光。

    

    本文解决了从未知照明条件下的稀疏视角（甚至单目）视频中创建可重光和可动化的神经化身的挑战。与工作室环境相比，这个设置更实际和可行，但是面临一个极具挑战性的逆问题。之前的神经人类重建方法能够使用变形有符号距离场（SDF）从稀疏视角重建可动化的化身，但无法恢复用于重光的材料参数。虽然可微逆渲染方法已成功地恢复了静态对象的材料，但对于动态人类，将其扩展为动态人体是不直观的，因为在变形SDF上计算像素-表面相交和光能见度对于逆渲染来说是计算密集型的。为了解决这个挑战，我们提出了一种分层距离查询（HDQ）算法来近似任意人体姿态下的世界空间距离。具体来说，我们估算了粗略的距离值，然后使用迭代过程来提高距离估算的精度，并使用这些估算出的距离值进行材料恢复和重光。

    This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse
    
[^51]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^52]: 关于大型语言模型的模型压缩综述

    A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])

    [http://arxiv.org/abs/2308.07633](http://arxiv.org/abs/2308.07633)

    本论文提供了关于大型语言模型的模型压缩综述，探讨了量化、修剪、知识蒸馏等不同方法，并突出介绍了最新进展和创新方法，为实现高效的部署提供了重要思路。

    

    大型语言模型（LLMs）以惊人的成功彻底改变了自然语言处理任务。然而，它们庞大的体量和计算需求在资源受限环境下的实际部署中带来了重大挑战。随着这些挑战日益紧迫，模型压缩领域已成为一个关键的研究领域，旨在缓解这些限制。本文提供了一份全面的综述，探讨专门针对LLMs的模型压缩技术。我们深入研究了各种方法，包括量化、修剪、知识蒸馏等，以应对高效部署的迫切需求。在每种技术中，我们重点介绍了最新进展和创新方法，为LLM研究的发展提供了贡献。此外，我们还探讨了用于评估效果的基准策略和评估指标的重要性。

    Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
    
[^53]: 能否通过非结构化剪枝来减少深度神经网络的层数？

    Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?. (arXiv:2308.06619v1 [cs.LG])

    [http://arxiv.org/abs/2308.06619](http://arxiv.org/abs/2308.06619)

    本研究介绍了一种名为EGP的创新的熵引导剪枝算法，该算法能够通过优先剪除熵较低的层中的连接来有效压缩深度神经网络，同时保持其竞争性能水平。

    

    剪枝是一种广泛使用的技术，可以减小深度神经网络的大小，同时保持其性能。然而，即使是在有结构的情况下，这种技术也很难从模型中完全去除整个层：这是一个可以解决的任务吗？在这项研究中，我们引入了一种名为EGP的创新的熵引导剪枝算法，旨在减小深度神经网络的大小，同时保持其性能。EGP的关键重点是优先剪除熵较低的层中的连接，最终完全去除这些层。通过在ResNet-18和Swin-T等流行模型上进行大量实验，我们的研究结果表明，EGP能够有效压缩深度神经网络，同时保持竞争性能水平。我们的结果不仅揭示了非结构化剪枝优势背后的机制，还为进一步研究复杂的关系铺平了道路。

    Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relations
    
[^54]: EgoPoser：大场景下鲁棒的实时自我身体姿势估计

    EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes. (arXiv:2308.06493v1 [cs.CV])

    [http://arxiv.org/abs/2308.06493](http://arxiv.org/abs/2308.06493)

    本文提出了EgoPoser，一种能够在大场景中鲁棒地实时估计自我身体姿势的方法。通过重新思考输入表示、引入新的运动分解方法以及建模身体姿势，EgoPoser在定性和定量上均表现优于现有方法，并具有较高的推理速度。

    

    头部和手部姿势仅通过完整身体自我姿势估计已成为研究的一个热点领域，以为头戴式平台上的虚拟角色表达提供动力。然而，现有方法过于依赖数据集记录时的运动捕捉空间的限制，同时假设连续捕捉关节运动和均匀身体尺寸。在本文中，我们提出了EgoPoser，通过以下方式克服了这些限制：1）重新思考基于头戴式平台的自我姿势估计的输入表示，并引入一种新的运动分解方法来预测与全局位置无关的完整身体姿势，2）从头戴式设备视野内的间歇性手部姿势跟踪中鲁棒地建模身体姿势，3）针对不同用户的各种身体尺寸进行通用化推广。我们的实验表明，EgoPoser在定性和定量上优于现有的方法，并保持较高的推理速度。

    Full-body ego-pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representation on headset-based platforms. However, existing methods over-rely on the confines of the motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous capture of joint motions and uniform body dimensions. In this paper, we propose EgoPoser, which overcomes these limitations by 1) rethinking the input representation for headset-based ego-pose estimation and introducing a novel motion decomposition method that predicts full-body pose independent of global positions, 2) robustly modeling body pose from intermittent hand position and orientation tracking only when inside a headset's field of view, and 3) generalizing across various body sizes for different users. Our experiments show that EgoPoser outperforms state-of-the-art methods both qualitatively and quantitatively, while maintaining a high inference speed of over
    
[^55]: 鲁棒的声源定位的音频-视觉空间融合和递归注意力

    Audio-Visual Spatial Integration and Recursive Attention for Robust Sound Source Localization. (arXiv:2308.06087v1 [cs.MM] CROSS LISTED)

    [http://arxiv.org/abs/2308.06087](http://arxiv.org/abs/2308.06087)

    本文提出了一种音频-视觉空间融合网络，通过整合音频和视觉模态的空间线索来模仿人类检测声音产生物体的行为，并引入递归注意力网络来得到更准确的注意力区域。通过音频和视觉模态的空间线索和递归聚焦策略，方法在声源定位任务上取得了良好的性能。

    

    声源定位任务的目标是让机器能够在视觉场景中检测出声音产生物体的位置。虽然音频模态提供了定位声源的空间线索，但现有方法仅将音频作为视觉模态空间区域比较的辅助角色。而人类则利用音频和视觉模态作为定位声源的空间线索。本文提出了一种音频-视觉空间融合网络，通过整合两种模态的空间线索来模仿人类检测声音产生物体时的行为。此外，我们引入了一种递归注意力网络来模仿人类迭代地聚焦对象，从而得到更准确的注意力区域。为了有效地编码两种模态的空间信息，我们提出了音频-视觉配对匹配损失和空间区域对齐损失。通过利用音频-视觉模态的空间线索和递归聚焦的策略，我们的方法在声源定位任务上取得了良好的性能。

    The objective of the sound source localization task is to enable machines to detect the location of sound-making objects within a visual scene. While the audio modality provides spatial cues to locate the sound source, existing approaches only use audio as an auxiliary role to compare spatial regions of the visual modality. Humans, on the other hand, utilize both audio and visual modalities as spatial cues to locate sound sources. In this paper, we propose an audio-visual spatial integration network that integrates spatial cues from both modalities to mimic human behavior when detecting sound-making objects. Additionally, we introduce a recursive attention network to mimic human behavior of iterative focusing on objects, resulting in more accurate attention regions. To effectively encode spatial information from both modalities, we propose audio-visual pair matching loss and spatial region alignment loss. By utilizing the spatial cues of audio-visual modalities and recursively focusing
    
[^56]: 在内存层次结构上具有MiRo的成本效益的设备上的持续学习

    Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])

    [http://arxiv.org/abs/2308.06053](http://arxiv.org/abs/2308.06053)

    这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。

    

    持续学习是从持续的任务流中逐步训练神经网络模型。为了记住先前学到的知识，之前的研究将旧样本存储在一个内存层次结构中，并在新任务到来时进行回放。采用持续学习以保护数据隐私的边缘设备通常对能源敏感，因此需要在不损害能源效率的情况下保持高模型准确度，即成本效益。我们的工作是首次探索基于层次内存回放的持续学习的设计空间，以获得在边缘设备上的成本效益。我们提出了Miro，一个新颖的系统运行时，通过使其能够根据资源状态动态配置持续学习系统，从而将我们的见解精确地整合到持续学习框架中，以实现最佳成本效益。为了实现这个目标，Miro还对带有明确准确度-能量平衡的参数进行在线分析，并以低开销地适应最佳值。广泛的评估显示Miro明显优于其他方案。

    Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
    
[^57]: 深度任务特定的底层表示网络用于多任务推荐系统

    Deep Task-specific Bottom Representation Network for Multi-Task Recommendation. (arXiv:2308.05996v1 [cs.AI])

    [http://arxiv.org/abs/2308.05996](http://arxiv.org/abs/2308.05996)

    本文提出了一种深度任务特定的底层表示网络（DTRN），用于解决多任务推荐系统中的负迁移问题，通过明确获取每个任务的底层表示来改善任务特定特征的捕捉能力。

    

    基于神经网络的多任务学习在推荐系统中取得了显著的改进，并成功地应用于推荐系统。最近的深度多任务学习方法（如MMoE、PLE）专注于设计基于软门控的参数共享网络，隐式地学习每个任务的泛化表示。然而，当处理冲突任务时，多任务学习方法可能会遭受性能退化，因为负迁移效应可能发生在任务共享的底层表示上。这可能导致多任务学习方法捕捉任务特定特征的能力降低，最终影响其效果，并妨碍其在所有任务上的泛化能力。在本文中，我们专注于推荐系统中多任务学习的底层表示学习，并提出了深度任务特定的底层表示网络（DTRN）以缓解负迁移问题。DTRN通过使每个任务具有自己的表示学习明确地获取任务特定的底层表示。

    Neural-based multi-task learning (MTL) has gained significant improvement, and it has been successfully applied to recommendation system (RS). Recent deep MTL methods for RS (e.g. MMoE, PLE) focus on designing soft gating-based parameter-sharing networks that implicitly learn a generalized representation for each task. However, MTL methods may suffer from performance degeneration when dealing with conflicting tasks, as negative transfer effects can occur on the task-shared bottom representation. This can result in a reduced capacity for MTL methods to capture task-specific characteristics, ultimately impeding their effectiveness and hindering the ability to generalize well on all tasks. In this paper, we focus on the bottom representation learning of MTL in RS and propose the Deep Task-specific Bottom Representation Network (DTRN) to alleviate the negative transfer problem. DTRN obtains task-specific bottom representation explicitly by making each task has its own representation learni
    
[^58]: DiLogics：利用不同逻辑创建Web自动化程序

    DiLogics: Creating Web Automation Programs With Diverse Logics. (arXiv:2308.05828v1 [cs.HC])

    [http://arxiv.org/abs/2308.05828](http://arxiv.org/abs/2308.05828)

    DiLogics是一个通过演示编程的系统，利用自然语言处理帮助用户创建处理多样化规范的Web自动化程序。

    

    知识工作者经常遇到重复的网络数据输入任务，例如更新记录或下订单。网络自动化可以提高生产力，但准确地将任务转化为网络操作并扩展到新的规范是具有挑战性的。现有的工具可以自动化执行相同UI操作的任务（例如，按顺序在每个字段中输入文本），但不支持根据不同的输入条件进行不同执行的任务。我们提出了DiLogics，这是一个通过演示编程的系统，利用自然语言处理帮助用户创建处理多样化规范的Web自动化程序。DiLogics首先将输入数据语义分割为结构化的任务步骤。通过为每个步骤记录用户演示，DiLogics将网络宏泛化为新颖但在语义上相似的任务要求。我们的评估结果显示，非专家可以有效使用DiLogics创建满足多样化输入指令的自动化程序。

    Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provide
    
[^59]: 基于骨骼的人体动作识别面临的硬性无盒对抗攻击和骨骼-动作知情梯度

    Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])

    [http://arxiv.org/abs/2308.05681](http://arxiv.org/abs/2308.05681)

    本文针对基于骨骼的人体动作识别方法的脆弱性进行了研究，提出了一种新的硬性无盒对抗攻击方法，通过学习运动流形和定义骨骼-动作知情梯度来攻击模型，揭示了这种脆弱性的存在。

    

    最近，基于骨骼的人体活动识别方法已被证明容易受到对抗攻击。然而，这些攻击方法要求要么完全了解受害者（即白盒攻击），要么有访问训练数据（即基于转移的攻击），或者频繁查询模型（即黑盒攻击）。所有这些要求都非常限制性，引发了对脆弱性的质疑。在本文中，我们证明了脆弱性确实存在。为此，我们考虑了一个新的攻击任务：攻击者无法访问受害者模型或训练数据或标签，我们将其称为硬性无盒攻击。具体来说，我们首先学习一个运动流形，然后定义一个用于计算攻击的对抗损失函数，称为骨骼-动作知情梯度（SMI梯度）。我们的梯度包含运动动力学的信息，这与现有的基于梯度的攻击方法不同，后者假设损失梯度是通过计算而来的。

    Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming 
    
[^60]: 元认知提示改善大型语言模型的理解能力

    Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])

    [http://arxiv.org/abs/2308.05342](http://arxiv.org/abs/2308.05342)

    元认知提示 (MP) 是一种改进大型语言模型 (LLMs) 理解能力的策略。实验结果表明，使用MP的PaLM在各种自然语言理解任务中接近于GPT-4的性能水平。

    

    在大型语言模型 (LLMs) 中，通过有效的提示设计，任务特定性能一直在不断提高。尽管最近关于提示的研究增强了LLMs的推理能力，但在进一步提高它们的理解能力方面仍存在差距。在本研究中，我们介绍了元认知提示 (MP)，这是一种受人类内省推理过程启发的策略。使用MP，LLMs经历一系列有结构、自我意识的评估，利用其丰富的内在知识和新的见解。我们的实验涉及五个常见的LLMs：Llama2、Vicuna、PaLM、GPT-3.5和GPT-4，它们都涵盖了来自GLUE和SuperGLUE基准测试的各种通用自然语言理解 (NLU) 任务。结果表明，虽然GPT-4在大多数任务中始终表现出色，但配备MP的PaLM接近其性能水平。此外，跨模型和数据集，MP始终优于现有的提示方法。

    In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
    
[^61]: AI有害言论的责任在哪里？

    Where's the Liability in Harmful AI Speech?. (arXiv:2308.04635v1 [cs.CY])

    [http://arxiv.org/abs/2308.04635](http://arxiv.org/abs/2308.04635)

    AI生成式模型可能会产生具有潜在责任风险的有害言论。解决模型创建者和部署者的法律责任问题的关键在于算法设计的技术细节。需要进行深入的Section 230免责分析以及下游责任分析。

    

    生成式人工智能（特别是基于文本的“基础模型”）可以生成可能在广泛的责任制度下引发问题的言论。机器学习从业者经常对模型进行“红队”测试，以识别和减轻此类问题言论，从错误指责严重不端行为的“幻觉”到构造原子弹的食谱。一个关键问题是这些红队测试行为是否真的对模型创建者和部署者构成任何法律责任风险，从而激励投资于安全机制。我们研究了三种责任制度，并将其与红队测试模型行为的常见例子联系起来：诽谤、构成犯罪行为的言论和错误致死。我们发现，任何Section 230免责分析或下游责任分析都与算法设计的技术细节密切相关。而要真正找到解决这些问题的方法有很多障碍。

    Generative AI, in particular text-based "foundation models" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. Machine learning practitioners regularly "red team" models to identify and mitigate such problematic speech: from "hallucinations" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. A key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under U.S. law, incentivizing investments in safety mechanisms. We examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. We find that any Section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. And there are many roadblocks to truly finding mo
    
[^62]: AIs的发展脱靴法

    Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])

    [http://arxiv.org/abs/2308.04586](http://arxiv.org/abs/2308.04586)

    传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。

    

    尽管当前一些AI在封闭的世界，如棋盘游戏中超越了人类能力，但它们在混乱的现实世界中的表现有限。它们会犯奇怪的错误而且没有意识到。它们很难受到指导，不能运用常识，缺乏好奇心。它们不能成为良好的合作者。传统手动构建的符号AI方法构建的系统和使用生成和深度学习AI方法(包括大规模语言模型)构建的系统都无法应对这些挑战。它们不适合创建强大和可信赖的AI。尽管此方法不属于主流的AI方法，但发展脱靴法显示出希望。在发展脱靴法中，AI像人类儿童一样发展能力。它们从先天能力开始。像人类一样，它们与环境互动，并从互动中学习。它们通过自我发展的能力逐步扩展先天能力。它们互动并逐渐将所学应用于实际操作。

    Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
    
[^63]: 异构知识融合: 通过LLM进行个性化推荐的新方法

    Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM. (arXiv:2308.03333v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2308.03333](http://arxiv.org/abs/2308.03333)

    本文提出了一种通过大型语言模型（LLM）从用户行为信息中提取和融合异构知识的新方法，通过指令调整实现个性化推荐，有效地提高了推荐性能。

    

    分析和挖掘用户异构行为对于推荐系统至关重要。然而，将各种类型的异构行为纳入推荐模型的常规方法会导致特征稀疏和知识碎片化的问题。为了解决这个挑战，我们提出了一种通过大型语言模型（LLM）从用户异构行为信息中提取和融合异构知识的新方法，通过将异构知识和推荐任务结合，对LLM进行指令调整以实现个性化推荐。实验结果表明，我们的方法能够有效地整合用户异构行为并显著提高推荐性能。

    The analysis and mining of user heterogeneous behavior are of paramount importance in recommendation systems. However, the conventional approach of incorporating various types of heterogeneous behavior into recommendation models leads to feature sparsity and knowledge fragmentation issues. To address this challenge, we propose a novel approach for personalized recommendation via Large Language Model (LLM), by extracting and fusing heterogeneous knowledge from user heterogeneous behavior information. In addition, by combining heterogeneous knowledge and recommendation tasks, instruction tuning is performed on LLM for personalized recommendations. The experimental results demonstrate that our method can effectively integrate user heterogeneous behavior and significantly improve recommendation performance.
    
[^64]: DOMINO: 多个传感器时间序列数据的领域不变的高维分类

    DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data. (arXiv:2308.03295v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03295](http://arxiv.org/abs/2308.03295)

    DOMINO是一种领域不变的高维分类方法，适用于多传感器时间序列数据；解决了分布偏移和计算资源的问题。

    

    随着物联网的快速发展，许多现实应用利用异构连接的传感器来捕捉时间序列信息。边缘机器学习方法常被用于分析本地收集的数据。然而，数据驱动的机器学习方法面临一个基本问题，即分布偏移。当一个模型部署在与其训练不同的数据分布上时，分布偏移会严重降低模型性能。此外，越来越复杂的深度神经网络(DNNs)已被提出用于捕捉多个传感器时间序列数据中的空间和时间依赖关系，而这需要超出当今边缘设备容量的大量计算资源。而大脑启发的高维计算(HDC)作为边缘学习的一种轻量级解决方案已被引入，但现有的HDC在面对分布偏移挑战时仍然脆弱。本文提出了DOMINO，一种新颖的HDC学习方法。

    With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning fr
    
[^65]: 用火攻火：ChatGPT能够检测AI生成的文本吗？

    Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?. (arXiv:2308.01284v1 [cs.CL])

    [http://arxiv.org/abs/2308.01284](http://arxiv.org/abs/2308.01284)

    ChatGPT作为检测器能否有效检测AI生成的文本，我们通过评估其在人工编写文本与AI生成文本之间的区分能力，并在公开数据集上进行实验，得出了关于ChatGPT在自动化检测流程中的应用指导。

    

    越来越多的大型语言模型（如ChatGPT）被用于各种用例，包括规模化的文本内容生成。虽然已经存在针对这种AI生成文本的检测方法，但我们研究了ChatGPT在这种AI生成文本上的检测性能，受到将ChatGPT用作数据标注器或注释器的研究启发。我们评估了ChatGPT在人工编写文本与AI生成文本检测任务中的零-shot性能，并在公开可用的数据集上进行实验。我们通过实证研究了ChatGPT在检测AI生成文本或人工编写文本方面是否具有对称效应。我们的发现揭示了通过简单关注问题的特定方面并从该解决方案中推导出其余部分，如何利用ChatGPT和类似的大型语言模型在自动化检测流程中发挥作用。所有代码和数据可在 \url{https://github.com/AmritaBh/ChatGPT-as-Detector} 上获得。

    Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at \url{https://github.com/AmritaBh/ChatGPT-as-Detector}.
    
[^66]: 对自动驾驶车辆风险评估的反事实安全边界视角

    A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])

    [http://arxiv.org/abs/2308.01050](http://arxiv.org/abs/2308.01050)

    本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。

    

    自动驾驶车辆（AVs）有潜力提供诸多社会效益，如减少道路事故和提高交通效率。然而，由于缺乏历史数据和技术的快速发展，量化AVs的风险是具有挑战性的。本文提出了一个基于数据驱动的框架，用于比较不同AVs在各种操作设计领域（ODDs）中行为的风险，该框架基于对“不良”道路用户进行反事实模拟。我们引入了反事实安全边界的概念，表示可能导致碰撞的最小偏离正常行为的量。该概念有助于找到最关键的情景，同时也有助于评估AVs的风险频率和严重程度。我们证明，即使AV的行为策略是未知的，提出的方法仍然适用于最坏和最佳情况分析，使该方法对外部第三方风险评估机构也有用。

    Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
    
[^67]: Hessian-Aware Bayesian Optimization for Decision Making Systems - 感知海森贝叶斯优化在决策系统中的应用

    Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])

    [http://arxiv.org/abs/2308.00629](http://arxiv.org/abs/2308.00629)

    本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。

    

    许多优化决策系统的方法依赖于梯度方法，需要从环境中获取有信息量的反馈。然而，当反馈稀缺或者无信息时，这些方法可能导致性能较差。贝叶斯优化等无导数方法可以减少对梯度反馈质量的依赖，但在复杂决策系统的高维环境中往往难以扩展。如果系统需要多个参与者之间的互动来实现共同目标，这个问题就加剧了。为了解决维度问题，我们提出了一种紧凑的多层架构，通过角色的概念来建模参与者之间的动态。此外，我们还引入了感知海森贝叶斯优化来高效地优化由大量参数参数化的多层架构。实验结果表明，我们的方法(HA-GP-UCB)在效果上是有效的。

    Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
    
[^68]: DMFC-GraspNet: 多指机器人在杂乱场景中可微的抓取生成

    DMFC-GraspNet: Differentiable Multi-Fingered Robotic Grasp Generation in Cluttered Scenes. (arXiv:2308.00456v1 [cs.RO])

    [http://arxiv.org/abs/2308.00456](http://arxiv.org/abs/2308.00456)

    本文提出了DMFC-GraspNet，在多指机器人抓取生成领域做出了两个主要贡献：一是提出了可微的多指抓取规划方法，实现了多样化和稠密的抓取预测；二是开发了一种稠密标注方法，使得多指机器人手与真实抓取密切关联。结果表明了该方法的有效性。

    

    机器人抓取是机器人操作中必备的基本技能。模仿人手结构的多指机器人手可以进行复杂的物体操作。然而，目前的多指机器人抓取技术通常在每次推理中只能预测一次抓取，限制了其多样性和效率。本文提出了一种可微的多指抓取生成网络（DMFC-GraspNet），针对这一挑战做出了两个主要贡献。首先，提出了一种新颖的神经抓取规划器，预测了一种新的抓取表示方法，实现了多样化而稠密的抓取预测。其次，开发了一种场景创建和标签映射方法，用于多指机器人手的稠密标注，实现了与真实抓取的密切关联。通过仿真研究对所提出的方法进行了评估，并与现有方法进行了比较。结果表明了该方法的有效性。

    Robotic grasping is a fundamental skill required for object manipulation in robotics. Multi-fingered robotic hands, which mimic the structure of the human hand, can potentially perform complex object manipulations. Nevertheless, current techniques for multi-fingered robotic grasping frequently predict only a single grasp for each inference time, limiting their versatility and efficiency. This paper proposes a differentiable multi-fingered grasp generation network (DMFC-GraspNet) with two main contributions to address this challenge. Firstly, a novel neural grasp planner is proposed, which predicts a new grasp representation to enable versatile and dense grasp predictions. Secondly, a scene creation and label mapping method is developed for dense labeling of multi-fingered robotic hands, which allows a dense association of ground truth grasps. The proposed approach is evaluated through simulation studies and compared to existing approaches. The results demonstrate the effectiveness of t
    
[^69]: MetaGPT: 元编程用于多智能体协作框架

    MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])

    [http://arxiv.org/abs/2308.00352](http://arxiv.org/abs/2308.00352)

    MetaGPT是一个用于多智能体协作的创新框架，将有效的人工工作流引入到大型语言模型驱动的协作中。它采用元编程方法，将标准操作规程编码为提示，促进结构化协调，并要求模块化输出，赋予智能体领域专业知识，以验证输出并减少错误。这种框架利用了流水线工作模式来分配任务。

    

    最近，在多个大型语言模型驱动的智能体协作中，自动任务解决取得了显著进展。然而，现有的工作主要集中在简单任务上，缺乏对复杂任务的探索和研究，主要是由于幻觉问题。这种幻觉在多个智能体相互作用时被无限放大，导致在解决复杂问题时失败。因此，我们引入了MetaGPT，这是一个创新的框架，在LLM驱动的多智能体协作中采用有效的人工工作流作为元编程方法。具体而言，MetaGPT首先将标准操作规程（SOPs）编码为提示，促进结构化协调。然后，它进一步要求模块化输出，赋予智能体领域专业知识，与人类专业人员平行验证输出并减少错误。通过这种方式，MetaGPT利用流水线工作模式来分配任务

    Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
    
[^70]: 使用大型语言模型进行渗透测试：AI作为辅助

    Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])

    [http://arxiv.org/abs/2308.00121](http://arxiv.org/abs/2308.00121)

    本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。

    

    软件安全测试领域，尤其是渗透测试是一项需要高水平专业知识的活动，并涉及许多手动测试和分析步骤。本文探讨了使用大型语言模型（如GPT3.5）来增强渗透测试人员的能力。我们研究了两种不同的用例：用于安全测试任务的高级任务规划和在易受攻击的虚拟机中进行低级漏洞寻找。对于后者，我们实现了一个闭环反馈，将由语言模型生成的低级操作与易受攻击的虚拟机（通过SSH连接）相连，并允许语言模型分析虚拟机状态以寻找漏洞，并提供具体的攻击向量。我们讨论了有前景的初步结果，详细介绍了改进的途径，并就提供该技术的伦理问题进行了讨论。

    The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
    
[^71]: 分布式动态规划和分布式TD-Learning的网络多智能体马尔可夫决策过程的ODE框架

    Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2307.16706](http://arxiv.org/abs/2307.16706)

    本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。

    

    本文主要研究网络多智能体马尔可夫决策问题中的分布式动态规划（DP）和分布式时序差分（TD）学习算法。我们采用分布式多智能体框架，其中各个智能体只能访问自己的奖励，缺乏对其他智能体奖励的了解。此外，每个智能体都能通过一个由图表示的通信网络与相邻智能体共享其参数。我们的贡献可以总结为两个关键点：1）我们引入了一个新的受连续时间区间内的平均一致性方法启发的分布式DP。通过控制理论的视角评估了该DP的收敛性。2）基于上述DP，我们设计了一个新的分布式TD学习算法并证明了其收敛性。我们提出的分布式DP的一个显著特点是其包含了两个独立的动态系统。

    The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
    
[^72]: 远程生物感应：公开源基准框架用于公平评估rPPG

    Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG. (arXiv:2307.12644v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.12644](http://arxiv.org/abs/2307.12644)

    这篇论文提出了远程生物感应技术rPPG的公开源基准框架，该框架可以公平评估rPPG的准确性问题并解决与皮肤颜色、相机特性和环境光等因素相关的挑战。

    

    rPPG（远程光电容积脉搏图）是一种通过使用摄像头捕捉到的血红蛋白的光吸收特性来测量和分析BVP（血容量脉搏）的技术。分析所测量的BVP可以得出各种生理信号，如心率、压力水平和血压，可以应用于各种应用，如远程医疗、远程患者监护和心血管疾病的早期预测。尽管在这个领域已经进行了广泛的努力和进展，但仍然存在严重的挑战，包括与皮肤颜色、相机特性、环境光和其他噪声和伪迹来源有关的问题，这些问题降低了准确性能。我们认为迫切需要公正可评估的基准测试来克服这些挑战。

    rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overc
    
[^73]: 基于梯度的词替换用于生成语言模型中顽固对抗样本

    Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models. (arXiv:2307.12507v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12507](http://arxiv.org/abs/2307.12507)

    本文介绍了一种名为GradObstinate的基于梯度的方法，用于生成顽固对抗样本。该方法可以自动生成意义改变但模型预测结果保持不变的对抗样本，无需人工设计约束。

    

    本文研究了在自然语言处理中通过词替换生成顽固（超稳定性）对抗样本的问题，在这种情况下，输入文本的意义发生了改变，但模型的预测却没有变化，尽管应该发生变化。以往的词替换方法主要集中在手动设计的反义词策略上，用于生成顽固对抗样本，这制约了它的应用，因为这些策略只能找到部分顽固对抗样本，并且需要人工努力。为了解决这个问题，本文介绍了一种新的词替换方法，名为GradObstinate，它是一种基于梯度的方法，可以自动生成顽固对抗样本，不受搜索空间限制或需求人工设计原则的约束。为了经验性地评估GradObstinate的效果，我们在与自然语言处理基准模型（Electra、ALBERT、Roberta、DistillBERT和CLIP）上进行了全面的实验。

    In this paper, we study the problem of generating obstinate (over-stability) adversarial examples by word substitution in NLP, where input text is meaningfully changed but the model's prediction does not, even though it should. Previous word substitution approaches have predominantly focused on manually designed antonym-based strategies for generating obstinate adversarial examples, which hinders its application as these strategies can only find a subset of obstinate adversarial examples and require human efforts. To address this issue, in this paper, we introduce a novel word substitution method named GradObstinate, a gradient-based approach that automatically generates obstinate adversarial examples without any constraints on the search space or the need for manual design principles. To empirically evaluate the efficacy of GradObstinate, we conduct comprehensive experiments on five representative models (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP benchmark
    
[^74]: 隐式身份表示条件化记忆补偿网络用于生成自然头部视频

    Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation. (arXiv:2307.09906v1 [cs.CV])

    [http://arxiv.org/abs/2307.09906](http://arxiv.org/abs/2307.09906)

    提出了一种隐式身份表示条件化记忆补偿网络，用于高保真度的自然头部视频生成。

    

    头部视频生成旨在通过从目标驱动视频中提取的动态姿势和表情来给静态图像中的人脸添加动画效果，同时保持源图像中的个人身份。然而，驱动视频中戏剧性和复杂的运动会导致生成模糊不清，因为静态源图像无法提供足够的外观信息来处理被遮挡区域或微妙的表情变化，这会产生严重的伪影并严重降低生成质量。为了解决这个问题，我们提出了学习全局人脸表示空间的方法，并设计了一种新颖的隐式身份表示条件化记忆补偿网络，称为MCNet，用于高保真度的头部视频生成。

    Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions using motion information derived from a target-driving video, while maintaining the person's identity in the source image. However, dramatic and complex motions in the driving video cause ambiguous generation, because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation conditioned memory compensation network, coined as MCNet, for high-fidelity talking head generation.~Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to compensate warped source facial features 
    
[^75]: 通过可靠的、多样化的和类平衡的伪标签来重新审视领域自适应三维物体检测

    Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])

    [http://arxiv.org/abs/2307.07944](http://arxiv.org/abs/2307.07944)

    本文通过提出一种适用于多类训练设置的新型ReDB框架来解决现有领域自适应方法在多类训练设置下性能下降的问题，通过产生可靠的、多样化的和类平衡的伪三维框来引导目标领域的自训练。

    

    无监督领域自适应与伪标签技术的辅助已经成为领域自适应三维物体检测的关键方法。然而，现有的领域自适应方法在应用于多类训练设置时性能大幅下降，原因是伪标签的质量低和类别不平衡问题共存。本文通过提出一种针对同时学习检测所有类别的新型ReDB框架来解决这一挑战。我们的方法产生可靠的、多样化的和类平衡的伪三维框，通过迭代地引导不同分布的目标领域的自训练。为了减轻环境差异（例如，光束数量）带来的干扰，我们提出了跨域检查（CDE），通过将目标实例复制粘贴到源环境中并测量预测的一致性来评估伪标签的正确性。为了减少计算开销和缓解物体的转移（例如，

    Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
    
[^76]: 自动识别和定量心肌组织炎症浸润在数字组织切片图像中以诊断心肌炎

    Automated identification and quantification of myocardial inflammatory infiltration in digital histological images to diagnose myocarditis. (arXiv:2307.01098v1 [physics.med-ph] CROSS LISTED)

    [http://arxiv.org/abs/2307.01098](http://arxiv.org/abs/2307.01098)

    本研究开发了一种新的计算病理学方法，用于自动识别和定量心肌组织炎症浸润在数字HE染色图像中，为心肌炎提供定量组织学诊断。该方法通过指标LND的定量来判断是否存在心肌炎症浸润。

    

    本研究旨在开发一种新的计算病理学方法，自动识别和定量心肌组织炎症浸润在数字HE染色图像中，以提供心肌炎的定量组织学诊断。本研究包括154名心脏移植病人诊断为心肌炎或扩张型心肌病的898个HE染色全切片图像。我们开发了一种基于DL的自动计算病理学方法，用于识别细胞核并检测心肌组织炎症浸润，使得可以定量测量心肌全切片图像上的淋巴细胞核密度（LND）。提出了一个基于LND定量的截断值来确定是否存在心肌炎症浸润。我们的方法通过五折交叉验证实验评估性能，使用来自心肌炎组的内部测试集进行测试，并通过双盲试验的外部测试进行确认。

    This study aims to develop a new computational pathology approach that automates the identification and quantification of myocardial inflammatory infiltration in digital HE-stained images to provide a quantitative histological diagnosis of myocarditis.898 HE-stained whole slide images (WSIs) of myocardium from 154 heart transplant patients diagnosed with myocarditis or dilated cardiomyopathy (DCM) were included in this study. An automated DL-based computational pathology approach was developed to identify nuclei and detect myocardial inflammatory infiltration, enabling the quantification of the lymphocyte nuclear density (LND) on myocardial WSIs. A cutoff value based on the quantification of LND was proposed to determine if the myocardial inflammatory infiltration was present. The performance of our approach was evaluated with a five-fold cross-validation experiment, tested with an internal test set from the myocarditis group, and confirmed by an external test from a double-blind trial
    
[^77]: GraMMaR: 用于3D人体运动重建的地面感知运动模型

    GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction. (arXiv:2306.16736v1 [cs.CV])

    [http://arxiv.org/abs/2306.16736](http://arxiv.org/abs/2306.16736)

    提出了一种用于3D人体运动重建的地面感知运动模型（GraMMaR），通过学习姿势和关节与地面之间的互动的过渡分布，明确促进运动和与地面距离变化之间的一致性。

    

    对于准确和真实的从RGB视频中重建3D人体运动，解密复杂的人地互动对于保证人类和地面之间的一致性至关重要。以往的方法要么隐式地模拟人地互动，要么以稀疏的方式模拟，往往在面对噪声和不确定性时导致不真实和不正确的运动。相反，我们的方法以一种密集和连续的方式明确表示这些互动。为此，我们提出了一种新颖的用于3D人体运动重建的地面感知运动模型，称为GraMMaR，它在运动序列中每个时间步骤中同时学习姿势和每个关节与地面之间的互动的过渡分布。它被训练用于明确促进运动和与地面距离变化之间的一致性。训练后，我们建立了一种联合优化策略，利用GraMMaR作为双重先验，规范优化过程朝着

    Demystifying complex human-ground interactions is essential for accurate and realistic 3D human motion reconstruction from RGB videos, as it ensures consistency between the humans and the ground plane. Prior methods have modeled human-ground interactions either implicitly or in a sparse manner, often resulting in unrealistic and incorrect motions when faced with noise and uncertainty. In contrast, our approach explicitly represents these interactions in a dense and continuous manner. To this end, we propose a novel Ground-aware Motion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointly learns the distribution of transitions in both pose and interaction between every joint and ground plane at each time step of a motion sequence. It is trained to explicitly promote consistency between the motion and distance change towards the ground. After training, we establish a joint optimization strategy that utilizes GraMMaR as a dual-prior, regularizing the optimization towards 
    
[^78]: 在各种模拟驾驶操作中对深度强化学习进行全面培训和评估的研究

    Comprehensive Training and Evaluation on Deep Reinforcement Learning for Automated Driving in Various Simulated Driving Maneuvers. (arXiv:2306.11466v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.11466](http://arxiv.org/abs/2306.11466)

    本研究在模拟平台上对两种深度强化学习算法进行了全面评估和比较，以开发自动驾驶模型。通过定制的奖励函数，对准确度、效率、安全性和舒适度进行了评估。

    

    在现实世界中开发和测试自动驾驶模型可能是具有挑战性甚至危险的，而模拟可以帮助解决这个问题，尤其是对于具有挑战性的驾驶操作。深度强化学习（DRL）通过学习和与环境交互来处理复杂的决策和控制任务，因此非常适合开发自动驾驶，在这方面的具体研究还不多。本研究在highway-env模拟平台上实施、评估和比较了两个DRL算法，Deep Q-networks（DQN）和Trust Region Policy Optimization（TRPO），以培训自动驾驶。同时开发了有效且定制的奖励函数，并通过准确度（车辆在道路上的行驶情况）、效率（车辆的行驶速度）、安全性（车辆避免与障碍物碰撞的可能性）和舒适度（车辆驾驶的舒适程度）来评估已实施的算法。

    Developing and testing automated driving models in the real world might be challenging and even dangerous, while simulation can help with this, especially for challenging maneuvers. Deep reinforcement learning (DRL) has the potential to tackle complex decision-making and controlling tasks through learning and interacting with the environment, thus it is suitable for developing automated driving while not being explored in detail yet. This study carried out a comprehensive study by implementing, evaluating, and comparing the two DRL algorithms, Deep Q-networks (DQN) and Trust Region Policy Optimization (TRPO), for training automated driving on the highway-env simulation platform. Effective and customized reward functions were developed and the implemented algorithms were evaluated in terms of onlane accuracy (how well the car drives on the road within the lane), efficiency (how fast the car drives), safety (how likely the car is to crash into obstacles), and comfort (how much the car ma
    
[^79]: 区块链支持的联邦学习：参考架构设计、实现和验证

    Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10841](http://arxiv.org/abs/2306.10841)

    本文提出了一种基于区块链的联邦学习参考架构，通过结合联邦学习和区块链技术，实现了去中心化、协作的机器学习系统，并保护了数据隐私和用户控制的身份。该架构使用去中心化标识符进行身份验证，通过智能合约实现强大的安全性和高效的去中心化，并能根据需求集成各种额外的元素，是一个适用范围广泛的 BCFL 解决方案。

    

    本文提出了一种创新的基于区块链的联邦学习（BCFL）参考架构，该架构将联邦学习和区块链技术的优势结合起来。这导致了一个去中心化的、协作的机器学习系统，尊重数据隐私和用户控制的身份。我们的架构战略性地采用基于去中心化标识符（DID）的身份验证系统，允许参与者使用其自主 DID 安全地认证并获得对联邦学习平台的访问权限，这些信息被记录在区块链上。通过执行智能合约来确保强大的安全性和高效的去中心化是我们方法的关键方面。此外，我们的 BCFL 参考架构提供了显著的可扩展性，能够根据特定需求和用例集成各种额外的元素，使其成为广泛适用的 BCFL 解决方案。

    This paper presents an innovative reference architecture for blockchain-enabled federated learning (BCFL), a state-of-the-art approach that amalgamates the strengths of federated learning and blockchain technology. This results in a decentralized, collaborative machine learning system that respects data privacy and user-controlled identity. Our architecture strategically employs a decentralized identifier (DID)-based authentication system, allowing participants to authenticate and then gain access to the federated learning platform securely using their self-sovereign DIDs, which are recorded on the blockchain. Ensuring robust security and efficient decentralization through the execution of smart contracts is a key aspect of our approach. Moreover, our BCFL reference architecture provides significant extensibility, accommodating the integration of various additional elements, as per specific requirements and use cases, thereby rendering it an adaptable solution for a wide range of BCFL 
    
[^80]: 通过人工智能引起的艺术实践的转变

    A Shift In Artistic Practices through Artificial Intelligence. (arXiv:2306.10054v1 [cs.CY])

    [http://arxiv.org/abs/2306.10054](http://arxiv.org/abs/2306.10054)

    人工智能模型生成的内容突破了艺术、音乐和媒体领域，引发了文化转变。它通过改变人们的角色、转变价值观以及挑战传统实践方式，为艺术的未来打开了新的可能性。

    

    由人工智能模型生成的大量内容的爆炸引发了艺术、音乐和媒体领域的文化转变，角色变化、价值观转变和传统受到挑战。互联网上可获得的广阔数据集为人工智能模型的训练创造了一个环境。AI模型的公开共享和全球使用，如何挑战艺术实践中的现状？AI技术将给音乐、艺术和新媒体带来什么样的变革？

    The explosion of content generated by Artificial Intelligence models has initiated a cultural shift in arts, music, and media, where roles are changing, values are shifting, and conventions are challenged. The readily available, vast dataset of the internet has created an environment for AI models to be trained on any content on the web. With AI models shared openly, and used by many, globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring into music, arts, and new media?
    
[^81]: 文本到图像生成的认知与现实

    Perceptions and Realities of Text-to-Image Generation. (arXiv:2306.08363v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2306.08363](http://arxiv.org/abs/2306.08363)

    本研究通过调查了解人们对文本到图像生成的认知，发现参与者意识到该技术的风险和危险，但只有少数人认为这项技术对个人构成风险，而对他人的风险较易被认识到，特别是艺术家被视为风险群体。

    

    生成式人工智能是一种广泛流行的技术，将深刻影响社会和个人。不到十年前，人们认为创造性工作将是最后自动化的领域之一，然而如今，我们看到人工智能正在侵占许多创造性领域。本文通过一项调查研究，提出了人们对于文本到图像生成的认知。我们涉及参与者对这一新兴技术的技术理解，他们的担忧和顾虑，以及对文本到图像生成对个人和社会的风险和危险的看法。我们发现，尽管参与者意识到与这项技术相关的风险和危险，但只有少数参与者认为这项技术对个人构成了风险。对其他人的风险更容易被参与者认识到。艺术家尤其被视为风险群体。有趣的是，那些尝试过这项技术的参与者对其未来重要性的评价较低。

    Generative artificial intelligence (AI) is a widely popular technology that will have a profound impact on society and individuals. Less than a decade ago, it was thought that creative work would be among the last to be automated - yet today, we see AI encroaching on many creative domains. In this paper, we present the findings of a survey study on people's perceptions of text-to-image generation. We touch on participants' technical understanding of the emerging technology, their fears and concerns, and thoughts about risks and dangers of text-to-image generation to the individual and society. We find that while participants were aware of the risks and dangers associated with the technology, only few participants considered the technology to be a personal risk. The risks for others were more easy to recognize for participants. Artists were particularly seen at risk. Interestingly, participants who had tried the technology rated its future importance lower than those who had not tried i
    
[^82]: 深度学习驾驶模型中的偏见问题

    Hidden Biases of End-to-End Driving Models. (arXiv:2306.07957v1 [cs.CV])

    [http://arxiv.org/abs/2306.07957](http://arxiv.org/abs/2306.07957)

    端到端驾驶模型存在偏见问题，并引入了次要组件的更改。本文针对目前多数最先进的方法中所存在的两种偏见进行了研究，并提出了合理的替代方法。在此基础上，开发了TF ++，在CARLA测试中表现优异。

    

    最近，端到端的驾驶系统在CARLA测试中取得了快速进展。然而，即使在主要贡献的基础上，这些系统也会引入对次要系统组件的改变。因此，系统的改进源并不清楚。我们发现，在几乎所有最先进的方法中都存在两种偏见，这些偏见对于在CARLA上观察到的进展至关重要：(1) 通过对目标点跟随的强归纳偏见来进行横向恢复，(2) 通过多模态航路点预测的纵向平均来减速。我们研究了这些偏见的缺点，并确定了合理的替代方法。通过结合我们的见解，我们开发了TF ++，一种简单的端到端方法，在Longest6和LAV基准测试中排名第一，在Longest6上比最佳前期工作提高了14个驾驶分数。

    End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over the best prior work on Longest6.
    
[^83]: 语言模型中出现的类人直觉行为和推理偏差——以及在GPT-4中消失。

    Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])

    [http://arxiv.org/abs/2306.07622](http://arxiv.org/abs/2306.07622)

    本研究揭示了大型语言模型（LLMs）具有类人直觉行为和认知错误的特点，而高级语言模型则通过学习避免这类错误并表现出超理性的方式。此外，通过使用心理学研究的方法探测LLMs，可以揭示其新生特性。

    

    大型语言模型（LLM）目前处于将AI系统与人类交流和日常生活交织在一起的前沿。因此，评估它们的新兴能力非常重要。在这项研究中，我们展示了LLM（尤其是GPT-3）表现出惊人的类人直觉行为，以及遵循这种行为而来的认知错误。然而，具有更高认知能力的LLM，特别是ChatGPT和GPT-4，学会了避免屈服于这些错误并表现出超理性的方式。对于我们的实验，我们利用了Cognitive Reflection Test（CRT）及用于研究人类直觉决策的语义幻觉。此外，我们还探究了类人直觉决策的稳定倾向。我们的研究表明，通过心理学方法调查LLM有潜力揭示否则未知的新生特性。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
    
[^84]: 使用分割学习预测软件性能

    Predicting Software Performance with Divide-and-Learn. (arXiv:2306.06651v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2306.06651](http://arxiv.org/abs/2306.06651)

    本文提出了一种名为$DaL$的基于分割学习的方法，用于预测高度配置的软件系统的性能。实验证明了该方法的有效性。

    

    预测高度配置的软件系统的性能是性能测试和质量保证的基础。为此，最近的研究依靠机器/深度学习来建模软件性能。然而，一个至关重要但未解决的挑战是如何满足配置景观中继承的稀疏性：配置选项（特征）的影响和数据样本的分布都非常稀疏。本文提出了一种基于“分割学习”概念的方法，称为$DaL$。基本思想是，为了处理样本稀疏性，我们将配置景观中的样本划分为远离的部分，对于每个部分，我们建立一个规范化的深度神经网络作为本地模型来处理特征稀疏性。然后，新给定的配置将被分配给最终预测的正确模型。八个真实系统和五组训练数据的实验结果显示

    Predicting the performance of highly configurable software systems is the foundation for performance testing and quality assurance. To that end, recent work has been relying on machine/deep learning to model software performance. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse.  In this paper, we propose an approach based on the concept of 'divide-and-learn', dubbed $DaL$. The basic idea is that, to handle sample sparsity, we divide the samples from the configuration landscape into distant divisions, for each of which we build a regularized Deep Neural Network as the local model to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction.  Experiment results from eight real-world systems and five sets of training data reveal that
    
[^85]: 提高扩散以改善鲁棒性泛化

    Enhance Diffusion to Improve Robust Generalization. (arXiv:2306.02618v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02618](http://arxiv.org/abs/2306.02618)

    本文通过连续时间随机微分方程的研究，发现对抗性训练中的扩散项决定了神经网络的鲁棒泛化能力，进而提出了一种改进的AT框架。

    

    深度神经网络容易受到人类难以察觉的对抗性扰动的影响。其中一种最强的防御机制是对抗性训练（AT）。本文旨在解决AT中的两个主要问题。首先，在AT研究中如何设置具有性能保证的超参数仍然存在争议，定制化设置妨碍不同模型设计在AT研究中的公平比较。其次，经过鲁棒训练的神经网络在泛化时面临困难，并且受到严重的过拟合问题的困扰。本文聚焦于主要的AT框架 - 投影梯度下降对抗性训练（PGD-AT）。我们通过连续时间随机微分方程（SDE）近似PGD-AT的动态，并展示了该SDE的扩散项决定了鲁棒泛化。该理论发现的一个直接推论是，鲁棒泛化与学习率和批次大小之比呈正相关关系。

    Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch siz
    
[^86]: InGram：通过关系图进行归纳知识图谱嵌入

    InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19987](http://arxiv.org/abs/2305.19987)

    InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。

    

    归纳知识图谱补全被视为预测训练期间未观察到的新实体之间的缺失三元组的任务。该论文提出了一个新方法InGram，它可以在推理时生成新关系和实体的嵌入，并基于关系图和原始知识图谱使用注意力机制来汇总邻居嵌入以生成关系和实体嵌入。实验结果表明，在几个基准数据集上，InGram的性能优于现有的基准方法。

    Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
    
[^87]: 基于大语言模型的推荐系统综述

    A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.19860](http://arxiv.org/abs/2305.19860)

    本综述介绍了基于大语言模型的推荐系统，提出了判别式LLMs和生成式LLMs两种模型范式，总结了这些模型的最新进展，强调了该领域的挑战和研究方向。

    

    大语言模型（LLMs）已成为自然语言处理（NLP）领域强大的工具，并在推荐系统领域引起了重视。这些模型使用自监督学习在海量数据上进行训练，已在学习通用表示方面取得了显着成功，并有可能通过一些有效的转移技术（如微调和提示调整）等手段提高推荐系统的各个方面的性能。利用大语言模型增强推荐质量的关键是利用它们高质量的文本特征表示和大量的外部知识覆盖，建立项目和用户之间的相关性。为了全面了解现有基于LLM的推荐系统，本综述提出了一种分类法，将这些模型分为两种主要范式，分别是判别式LLMs和生成式LLMs。此外，我们总结了这些范式的最新进展，并强调了这个新兴领域的挑战和开放性研究问题。

    Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
    
[^88]: 用Transformer学习超关系型和数值知识图中的表征学习

    Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18256](http://arxiv.org/abs/2305.18256)

    本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。

    

    近期研究了一个超关系型知识图谱，其中三元组与限定词集合相关联; 一个限定词由关系和实体组成，为三元组提供辅助信息。现有的超关系型知识图嵌入方法假定实体是离散对象，但有些信息应使用数值表示，例如(J.R.R.，出生于，1892)。同时，三元组(J.R.R.，就读于，牛津大学)可以与限定词(开始时间，1911)相关联。在本文中，我们提出了一个名为HyNT的统一框架，用于学习包含三元组或限定词中数值文字的超关系型知识图的表示。我们定义了一个上下文Transformer和一个预测Transformer，来学习表示，不仅基于三元组和其限定词之间的相关性，还基于数值信息。通过学习三元组和限定词的紧凑表示，并将它们馈送给Transformer来获得模型

    A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
    
[^89]: YOLOv8实时心律失常检测的新应用

    A novel application for real-time arrhythmia detection using YOLOv8. (arXiv:2305.16727v1 [cs.CV])

    [http://arxiv.org/abs/2305.16727](http://arxiv.org/abs/2305.16727)

    本文提出了一种使用YOLOv8算法进行心律失常检测的新应用程序，其模型能够实现持续监测，并以高准确性进行实时心律失常检测。

    

    近年来，降低远程心血管健康监护的医疗费用需求越来越高。检测和分类心脏心律失常对于诊断心脏异常患者至关重要。本文提出了一种新的应用程序，利用最先进的You-Only-Look-Once（YOLO）v8算法对单导联心电图信号进行分类，进行心律失常检测。通过对MIT-BIH数据集进行微调后，建立起一个定制的YOLOv8模型，能够实时检测心律失常，以实现持续监测。结果表明，在NVIDIA Tesla V100上，我们的模型能够以0.002秒的检测时间和0.961的mAP@50检测心跳。研究证明了实时心律失常检测的潜力，模型输出可以被视觉解释，适用于家庭用户。此外，该研究可以延伸到开发实时的可解释人工智能模型，应用于心血管健康监护。

    In recent years, there has been an increasing need to reduce healthcare costs in remote monitoring of cardiovascular health. Detecting and classifying cardiac arrhythmia is critical to diagnosing patients with cardiac abnormalities. This paper shows that complex systems such as electrocardiograms (ECG) can be applicable for at-home monitoring. This paper proposes a novel application for arrhythmia detection using the state-of-the-art You-Only-Look-Once (YOLO)v8 algorithm to classify single-lead ECG signals. A custom YOLOv8 model was fine-tuned on the MIT-BIH dataset to detect arrhythmia in real-time to allow continuous monitoring. Results show that our model can detect heartbeats with a mAP@50 of 0.961 with a detection time of 0.002s on an NVIDIA Tesla V100. Our study demonstrated the potential of real-time arrhythmia detection, where the model output can be visually interpreted for at-home users. Furthermore, this study could be extended into a real-time XAI model, deployed in the hea
    
[^90]: 语言奖励塑造可能影响指令遵循代理的学习：提醒其脆弱性

    A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents. (arXiv:2305.16621v1 [cs.AI])

    [http://arxiv.org/abs/2305.16621](http://arxiv.org/abs/2305.16621)

    该论文研究表明，对于复杂指令遵循代理的学习，语言奖励塑造技术可能会影响代理程序的学习，其中表面上的成功可能是脆弱的。

    

    教导代理程序遵守复杂的书面指令一直是一个重要而又难以实现的目标。提高学习效率的一种技术是语言奖励塑造（LRS），它在强化学习中用于奖励代表朝着稀疏奖励的进展方向的行动。我们认为，LRS的表面成功是脆弱的，并且之前的积极结果可能归因于弱的强化学习基线。具体而言，我们确定了奖励部分匹配轨迹的次优LRS设计，并基于放宽任务约束的概念，对一种新型的奖励扰动进行了表征以解决这个问题。我们提供了理论和实证证据表明，使用LRS奖励训练的代理程序较纯强化学习代理程序收敛速度较慢。

    Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.
    
[^91]: KeyPosS: 基于 GPS 灵感的真实距离多边定位的即插即用面部标记检测

    KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration. (arXiv:2305.16437v1 [cs.CV])

    [http://arxiv.org/abs/2305.16437](http://arxiv.org/abs/2305.16437)

    KeyPosS是一种面部标记检测框架，采用真实距离多边定位算法实现快速而准确的检测，避免了传统方法中的计算负担和量化误差问题。

    

    在面部分析领域，准确的标记检测对于各种应用至关重要，包括人脸识别和表情分析等。然而，传统的热力图或坐标回归技术经常面临计算负担和量化误差等挑战。为解决这些问题，我们提出了 KeyPoint Positioning System（KeyPosS），这是一种突破性的面部标记检测框架，与现有方法不同。KeyPosS首次采用真实距离多边定位算法，一种最初用于GPS系统的技术，通过不依赖于计算密集型回归方法实现快速而准确的面部标记检测。该框架利用完全卷积网络预测距离图，计算感兴趣点（POI）与多个锚点之间的距离。通过巧妙地利用这些锚点来三角测量POI的位置，实现面部标记的检测。

    In the realm of facial analysis, accurate landmark detection is crucial for various applications, ranging from face recognition and expression analysis to animation. Conventional heatmap or coordinate regression-based techniques, however, often face challenges in terms of computational burden and quantization errors. To address these issues, we present the KeyPoint Positioning System (KeyPosS), a groundbreaking facial landmark detection framework that stands out from existing methods. For the first time, KeyPosS employs the True-range Multilateration algorithm, a technique originally used in GPS systems, to achieve rapid and precise facial landmark detection without relying on computationally intensive regression approaches. The framework utilizes a fully convolutional network to predict a distance map, which computes the distance between a Point of Interest (POI) and multiple anchor points. These anchor points are ingeniously harnessed to triangulate the POI's position through the Tru
    
[^92]: 能ChatGPT检测出意图吗？评估用于口语理解的大型语言模型。

    Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding. (arXiv:2305.13512v1 [cs.CL])

    [http://arxiv.org/abs/2305.13512](http://arxiv.org/abs/2305.13512)

    本文评估了几个大型预训练语言模型在口语理解任务中的表现，发现最大模型可以在零-shot学习和上下文学习中达到与监督模型相近的意图分类准确度，但在槽填充方面表现不佳，且对ASR错误敏感。

    

    最近，大型预训练语言模型展示了强大的语言理解能力，特别体现在通过提示在下游任务中的零-shot和上下文学习能力。为了评估它们对口语理解（SLU）的影响，我们评估了几个不同大小的ChatGPT和OPT模型在多个基准测试中的表现。我们验证了最大模型特有的新兴能力，即在给定Oracle转录的各种语言上，其可以接近于监督模型的意图分类准确度。相比之下，适合单个GPU的较小模型的结果远远落后。我们注意到错误案例通常来自数据集的注释方案；ChatGPT的响应仍然是合理的。但是我们发现，该模型在槽填充方面表现不佳，而且对ASR错误非常敏感，因此表明了将这些文本模型应用于口语理解的严峻挑战。

    Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.
    
[^93]: 临床骆驼：一种具有基于对话的知识编码的开源专家级医学语言模型

    Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v1 [cs.CL])

    [http://arxiv.org/abs/2305.12031](http://arxiv.org/abs/2305.12031)

    临床骆驼是一种基于对话的知识编码的开源医学语言模型，具有很高的可解释性和临床相关性，并在多个基准数据集上取得了最先进的结果。

    

    大型语言模型（LLM）在医疗领域具有巨大潜力，但数据隐私、监管合规性和模型稳定性等问题限制了它们的广泛应用。为了应对这些挑战，我们提出了基于对话的知识编码（DBKE）。DBKE增强了模型的隐式知识库，使其具有更强的对话能力，为后续用例提供了软对齐。我们提出了Clinical Camel，这是一个开源的、专注于医疗保健的会话模型，来展示DBKE的有效性。Clinical Camel在几个基准数据集上实现了最先进的结果，同时保持了高水平的可解释性和临床相关性。它还为医疗应用提供了一个可信赖的、开放源代码的替代品。

    Large Language Models (LLMs) present immense potential in the medical field, yet concerns over data privacy, regulatory compliance, and model stability restrict their widespread adoption. Although the distillation of high-performing closed-source LLMs has proven effective for general tasks, their application in healthcare is limited due to reduced domain knowledge and remnants of alignment behavior hindering clinical tasks. To address these challenges, we propose Dialogue-Based Knowledge Encoding (DBKE). DBKE enhances models' implicit knowledge base and primes them for conversational recall, augmenting their conversational capabilities and enabling a soft alignment for subsequent use cases. By transforming dense academic source text into synthetic dialogue, DBKE broadens the model's knowledge base and enables a soft alignment that guides downstream behaviours. We present Clinical Camel, an open-source, healthcare-focused conversational model, to showcase the effectiveness of DBKE. Clin
    
[^94]: MedLens: 通过选择医学体征和回归插值来提高死亡率预测

    MedLens: Improve mortality prediction via medical signs selecting and regression interpolation. (arXiv:2305.11742v1 [cs.LG])

    [http://arxiv.org/abs/2305.11742](http://arxiv.org/abs/2305.11742)

    本文介绍了一个自动选择医学体征和回归插值的方法（MedLens），用于解决电子病历中医学体征数据缺失率过高导致预测性能降低的问题。

    

    监测患者的健康状况并提前预测死亡率对及时提供患者护理和治疗至关重要。电子病历中的大量医学体征被用于先进的机器学习模型来进行预测。然而，原始临床体征的数据质量问题在文献中被较少讨论。通过对各种医学体征和大量患者住院记录中的缺失率和相关分数进行深入测量，我们发现综合缺失率非常高，大量无用的体征可能会损害预测模型的性能。我们得出结论，只有改善数据质量才能提高不同预测算法的基线准确性。我们设计了MedLens，通过统计自动选择重要医学体征，并使用灵活的插值方法处理高缺失率时间序列。

    Monitoring the health status of patients and predicting mortality in advance is vital for providing patients with timely care and treatment. Massive medical signs in electronic health records (EHR) are fitted into advanced machine learning models to make predictions. However, the data-quality problem of original clinical signs is less discussed in the literature. Based on an in-depth measurement of the missing rate and correlation score across various medical signs and a large amount of patient hospital admission records, we discovered the comprehensive missing rate is extremely high, and a large number of useless signs could hurt the performance of prediction models. Then we concluded that only improving data-quality could improve the baseline accuracy of different prediction algorithms. We designed MEDLENS, with an automatic vital medical signs selection approach via statistics and a flexible interpolation approach for high missing rate time series. After augmenting the data-quality 
    
[^95]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^96]: 在线Platt缩放及其校准方法

    Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])

    [http://arxiv.org/abs/2305.00070](http://arxiv.org/abs/2305.00070)

    本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。

    

    我们提出了一种在线后校准方法，称为在线Platt缩放(OPS)，它将Platt缩放技术与在线逻辑回归相结合。我们展示了OPS如何在分布漂移的i.i.d.和非i.i.d.情况下平稳适应。此外，当最佳的Platt缩放模型本身被错误校准时，我们使用一种最近开发的称为calibeating的技术来增强OPS，使其更加鲁棒。理论上，我们得到的OPS+calibeating方法对于对抗性结果序列是保证校准的。在实验上，它在一系列合成和真实数据集上均表现出卓越的性能，无需超参数调整。最后，我们将所有OPS思想扩展到beta缩放方法。

    We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
    
[^97]: CRN：用于准确、稳健、高效的3D感知的相机雷达网络

    CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception. (arXiv:2304.00670v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00670](http://arxiv.org/abs/2304.00670)

    本文提出了CRN，一个新颖的相机雷达融合框架，通过将图像视图特征转换为鸟瞰特征图和使用多模态可变形注意力，实现了准确、稳健、高效的3D感知任务

    

    自动驾驶需要一个准确快速的3D感知系统，包括3D物体检测、跟踪和分割。虽然最近的低成本基于相机的方法显示出了有希望的结果，但是它们容易受到糟糕的光照或恶劣的天气条件的影响，并且具有较大的定位误差。因此，将相机与低成本雷达相结合，后者可以在所有环境中提供精确的远程测量并可靠运行，是有希望的，但尚未得到全面的研究。在本文中，我们提出了一个名为Camera Radar Net（CRN）的新颖的相机雷达融合框架，为各种任务生成一个语义丰富、空间精确的鸟瞰特征图（BEV）。为了克服图像中缺乏空间信息的问题，我们使用稀疏但准确的雷达点将透视视图图像特征转换为BEV。我们进一步使用多模态可变形注意力在BEV中聚合图像和雷达特征图，以解决空间对齐错误问题

    Autonomous driving requires an accurate and fast 3D perception system that includes 3D object detection, tracking, and segmentation. Although recent low-cost camera-based approaches have shown promising results, they are susceptible to poor illumination or bad weather conditions and have a large localization error. Hence, fusing camera with low-cost radar, which provides precise long-range measurement and operates reliably in all environments, is promising but has not yet been thoroughly investigated. In this paper, we propose Camera Radar Net (CRN), a novel camera-radar fusion framework that generates a semantically rich and spatially accurate bird's-eye-view (BEV) feature map for various tasks. To overcome the lack of spatial information in an image, we transform perspective view image features to BEV with the help of sparse but accurate radar points. We further aggregate image and radar feature maps in BEV using multi-modal deformable attention designed to tackle the spatial misalig
    
[^98]: 使用人工智能在家中测量帕金森病的严重程度

    Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])

    [http://arxiv.org/abs/2303.17573](http://arxiv.org/abs/2303.17573)

    该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。

    

    我们提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法。参与者在网络摄像头前完成了运动任务（即点击手指），250名全球参与者的数据按照运动障碍协会统一帕金森病评分量表 (MDS-UPDRS) 的标准由三名专家神经学家进行了评估。神经学家的评估具有高度的可靠性，内部一致性系数（ICC）为0.88。我们开发了计算机算法来获得与MDS-UPDRS指南一致且与神经学家的评估高度相关的客观测量结果。我们的机器学习模型在这些指标的训练下表现优于一个MDS-UPDRS认证的评分者，平均绝对误差（MAE）为0.59，而评分者的MAE为0.79。然而，该模型的表现略逊于专家神经学家（0.53 MAE）。该方法可重复用于类似的运动任务，提供了可能性。

    We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
    
[^99]: HiLo: 利用高低频率关系进行无偏倚的全景场景图生成

    HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation. (arXiv:2303.15994v1 [cs.CV])

    [http://arxiv.org/abs/2303.15994](http://arxiv.org/abs/2303.15994)

    该论文提出一种利用高低频率关系的无偏倚的全景场景图生成方法，解决了长尾问题和主体-对象对拥有多个重叠关系的问题。在广泛实验中取得了最先进的结果。

    

    全景场景图生成（PSG）是一项最近提出的图像场景理解任务，旨在对图像进行分割和提取主体、对象及其关系三元组以构建场景图。由于关系类别的长尾问题，这项任务特别具有挑战性，使得单纯的有偏倚方法更倾向于高频率关系。现有的无偏倚方法通过数据/损失重新平衡以支持低频率关系来解决长尾问题。其次，一个主体-对象对可以有两个或更多在语义上重叠的关系。虽然现有的方法倾向于使用一个而不是另一个，但我们提出的HiLo框架让不同的网络分支专门处理低频和高频关系，强制实施它们的一致性并融合结果。据我们所知，我们是首次提出明确无偏PSG方法的人。在广泛的实验中，我们展示了HiLo框架取得了最先进的结果。

    Panoptic Scene Graph generation (PSG) is a recently proposed task in image scene understanding that aims to segment the image and extract triplets of subjects, objects and their relations to build a scene graph. This task is particularly challenging for two reasons. First, it suffers from a long-tail problem in its relation categories, making naive biased methods more inclined to high-frequency relations. Existing unbiased methods tackle the long-tail problem by data/loss rebalancing to favor low-frequency relations. Second, a subject-object pair can have two or more semantically overlapping relations. While existing methods favor one over the other, our proposed HiLo framework lets different network branches specialize on low and high frequency relations, enforce their consistency and fuse the results. To the best of our knowledge we are the first to propose an explicitly unbiased PSG method. In extensive experiments we show that our HiLo framework achieves state-of-the-art results on
    
[^100]: 软件开发教育中的生成式 AI 助手

    Generative AI Assistants in Software Development Education. (arXiv:2303.13936v1 [cs.SE])

    [http://arxiv.org/abs/2303.13936](http://arxiv.org/abs/2303.13936)

    本文探讨了当前软件开发行业采用生成式 AI（GAI）助手进行软件开发的现状和挑战，提出了未来软件开发教育的愿景和教学建议。

    

    软件开发行业正在进行一次潜在的颠覆性的范式变革——采用生成式 AI（GAI）助手进行软件开发。虽然 AI 已经在软件工程的各个领域中被使用，但是像 GitHub Copilot 和 ChatGPT 这样的 GAI 技术已经激发了许多人的想象力（和恐惧）。尽管目前尚不清楚该行业将如何采用和适应这些技术，但微软（GitHub、必应）和谷歌（Bard）等大型软件公司将这些技术整合到更广泛的行业中的举动是明确的意图和方向。我们与行业专业人士进行了探索性访谈，以了解当前的实践和挑战，将其纳入我们对未来软件开发教育的愿景，并提出了一些教学建议。

    The software development industry is amid another potentially disruptive paradigm change--adopting the use of generative AI (GAI) assistants for software development. Whilst AI is already used in various areas of software engineering, GAI technologies, such as GitHub Copilot and ChatGPT, have ignited the imaginations (and fears) of many people. Whilst it is unclear how the industry will adopt and adapt to these technologies, the move to integrate these technologies into the wider industry by large software companies, such as Microsoft (GitHub, Bing) and Google (Bard), is a clear indication of intent and direction. We performed exploratory interviews with industry professionals to understand current practices and challenges, which we incorporate into our vision of a future of software development education and make some pedagogical recommendations.
    
[^101]: Among Us: 基于共识的反对抗鲁棒协同感知

    Among Us: Adversarially Robust Collaborative Perception by Consensus. (arXiv:2303.09495v1 [cs.RO])

    [http://arxiv.org/abs/2303.09495](http://arxiv.org/abs/2303.09495)

    ROBOSAC提出了一种基于共识的反对抗鲁棒协同感知防御策略，使用随机子集的队友来对比协同感知和单个感知的结果，以排除潜在攻击者，并推导出确保获得所需无攻击者子集所需的采样试验个数。

    

    多个机器人之间的协同感知能够比单个机器人更好地感知场景(例如，检测物体)，但在使用深度学习时很容易受到敌对攻击。这一问题可通过对抗性防御来解决，但训练需要了解攻击机制，而这通常是未知的。因此，我们提出了 ROBOSAC，一种基于采样的新型防御策略，该策略具有泛化能力，能应对未知的攻击者。我们的核心思想是，协同感知应该比单个感知更能达成一致，而不应相互产生分歧。这导致我们提出了一种假说和验证的框架：利用一组随机选择的队友，对协同感知与单个感知的结果进行比较，直到达成共识。在这样的框架下，更多的队友通常意味着更好的感知表现，但需要更长的采样时间来排除潜在的攻击者。因此，我们推导出了需要多少个采样试验才能确保获得所需的无攻击者子集。

    Multiple robots could perceive a scene (e.g., detect objects) collaboratively better than individuals, although easily suffer from adversarial attacks when using deep learning. This could be addressed by the adversarial defense, but its training requires the often-unknown attacking mechanism. Differently, we propose ROBOSAC, a novel sampling-based defense strategy generalizable to unseen attackers. Our key idea is that collaborative perception should lead to consensus rather than dissensus in results compared to individual perception. This leads to our hypothesize-and-verify framework: perception results with and without collaboration from a random subset of teammates are compared until reaching a consensus. In such a framework, more teammates in the sampled subset often entail better perception performance but require longer sampling time to reject potential attackers. Thus, we derive how many sampling trials are needed to ensure the desired size of an attacker-free subset, or equival
    
[^102]: 基于窗口的早期退出级联用于不确定性估计：当深度集成比单一模型更有效时

    Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])

    [http://arxiv.org/abs/2303.08010](http://arxiv.org/abs/2303.08010)

    本文研究了基于窗口的早期退出集成方法，以在保持模型可扩展性的同时实现不确定性估计任务的高效实现。实验结果表明，该方法在准确性和计算效率上都达到了最新的研究成果。

    

    深度集成是提高深度学习方法预测性能和不确定性估计的简单、可靠和有效方法。然而，由于需要部署多个独立模型，它们被广泛批评为计算开销大。最近的研究挑战了这种观点，表明对于预测准确性，集成可以比在同一架构族中缩放单一模型在推理时更具计算效率。通过通过早期退出方法级联集成成员实现这一目标。在这项工作中，我们研究如何将这些效率提高扩展到与不确定性估计相关的任务。由于许多这样的任务，例如选择性分类，都是二分类问题，我们的关键新颖见解是仅将接近二分决策边界的样本传递到后续级联阶段。在ImageNet规模的数据上进行的实验表明，所提出的基于窗口的早期退出集成在使用比基线更少的模型评估的同时，实现了最先进的不确定性估计性能，并且在预测性能上与完整集成相竞争。

    Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
    
[^103]: TARGET: 通过无样本蒸馏实现联邦类式持续学习

    TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation. (arXiv:2303.06937v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06937](http://arxiv.org/abs/2303.06937)

    本文研究了一个重要但鲜为人知的问题：联邦类式持续学习，在联邦学习中动态添加新的类别。我们提出了一种称为TARGET的新颖方法，通过无样本蒸馏来减轻FCCL中的灾难性遗忘问题，并保护客户数据的隐私。该方法利用先前训练的全局模型在模型层面上传递旧任务的知识，并通过生成器生成合成数据来模拟数据的全局分布。与先前的FCCL方法相比，TARGET无需额外的数据集或存储先前任务的私有数据。

    

    本文针对一个鲜为人知但重要的问题进行研究：联邦类式持续学习（FCCL），在联邦学习中动态添加新的类别。已有的FCCL方法存在各种限制，如需要额外的数据集或存储先前任务的私有数据。为此，我们首先证明非独立同分布的数据加剧了联邦学习中的灾难性遗忘问题。然后，我们提出了一种新颖的方法——TARGET（通过无样本蒸馏实现联邦类式持续学习），该方法在减轻FCCL的灾难性遗忘问题的同时保护客户数据的隐私。我们的方法利用先前训练的全局模型，在模型层面上将旧任务的知识传递给当前任务。此外，我们训练一个生成器来生成合成数据，以模拟每个客户端上数据的全局分布。与先前的FCCL方法相比，TARGET不需要额外的数据集或存储先前任务的私有数据。

    This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G} via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not requi
    
[^104]: 通过受限代理学习控制深度序数分类中的类布局

    Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00396](http://arxiv.org/abs/2303.00396)

    本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。

    

    对于深度序数分类任务，学习特定于序数分类的良好结构化特征空间有助于恰当地捕捉类之间的序数属性。本文提出了一种新颖的受限代理学习方法，该方法可以为每个序数类学习一个代理，然后通过限制这些代理来调整类的全局布局。我们提出了两种策略：硬布局约束和软布局约束。硬布局约束通过直接控制代理的生成来实现，以强制将其放置在严格的线性布局或半圆形布局（即严格序数布局的两种实例）中。软布局约束通过引入正则化项到损失函数中来实现，该项惩罚偏离理想序数布局的情况。在基准数据集上的实验结果证明了所提出的CPL方法在深度序数分类中的有效性。

    For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
    
[^105]: 改变很难：子群体转变的深入探究

    Change is Hard: A Closer Look at Subpopulation Shift. (arXiv:2302.12254v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12254](http://arxiv.org/abs/2302.12254)

    本文分析了子群体转变的各种机制，对20个最先进的算法在12个领域内进行了全面的基准测试，发现现有算法只能应对某些转变，进一步地，提出一种简单易行的选择标准来改善现有算法性能。

    

    机器学习模型通常在训练数据中代表性不足的子群体上表现不佳。然而，对于导致子群体转变的机制以及算法在如此不同的转变中如何进行普遍化，我们知之甚少。在这项工作中，我们对子群体转变进行了细致的分析。首先，我们提出了一个统一的框架来剖析和解释子群体中的常见转变。然后，我们在视觉、语言和医疗领域的12个真实数据集上对20个最先进的算法进行了全面的基准测试。通过训练10,000多个模型得到的结果，我们揭示了未来在这个领域取得进展的有趣观察结果。首先，现有算法仅能在某些类型的转变上提高子群体的鲁棒性，而在其他类型的转变上则不能。此外，虽然当前算法依赖于群体标注的验证数据进行模型选择，但我们发现基于最差类别准确度的简单选择标准其实非常有效。

    Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly
    
[^106]: 在3D医学图像分析中的自监督学习中的解剖不变性建模和语义对齐

    Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis. (arXiv:2302.05615v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05615](http://arxiv.org/abs/2302.05615)

    提出了一种新的自监督学习框架，通过结合辨别目标和生成目标，明确地实现了解剖不变性建模和语义对齐。引入了一种新的对比学习策略，鼓励不同来源但具有一致高级语义的视图之间的相似性，以学习不变的解剖特征。

    

    自监督学习（SSL）最近在3D医学图像分析任务中取得了有希望的性能。大多数当前方法遵循原本设计用于照片或自然图像的现有SSL范式，无法明确地和彻底地利用不同医学图像中内在的相似解剖结构。这实际上可能通过最大化包含空间不对齐信息和不同解剖语义的特征之间的相似性来降低所学深度表示的质量。在这项工作中，我们提出了一种新的自监督学习框架，名为Alice，通过巧妙地结合辨别目标和生成目标，明确地实现了解剖不变性建模和语义对齐。Alice引入了一种新的对比学习策略，鼓励不同来源但具有一致高级语义的视图之间的相似性，以学习不变的解剖特征。此外，我们设计了一种共生学习策略，将潜在噪声图像与良性图像进行对比，从而提高了自监督学习的性能。

    Self-supervised learning (SSL) has recently achieved promising performance for 3D medical image analysis tasks. Most current methods follow existing SSL paradigm originally designed for photographic or natural images, which cannot explicitly and thoroughly exploit the intrinsic similar anatomical structures across varying medical images. This may in fact degrade the quality of learned deep representations by maximizing the similarity among features containing spatial misalignment information and different anatomical semantics. In this work, we propose a new self-supervised learning framework, namely Alice, that explicitly fulfills Anatomical invariance modeling and semantic alignment via elaborately combining discriminative and generative objectives. Alice introduces a new contrastive learning strategy which encourages the similarity between views that are diversely mined but with consistent high-level semantics, in order to learn invariant anatomical features. Moreover, we design a co
    
[^107]: 带有提示策划和知识记忆的少样本表格到文本生成

    Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization. (arXiv:2302.04415v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.04415](http://arxiv.org/abs/2302.04415)

    本论文提出了PromptMize框架，用于解决少样本情况下的表格到文本生成问题。该框架包含提示策划和知识适配器两个方面，通过生成提示信号和利用领域特定知识来改善文本生成结果。

    

    预训练语言模型在表格到文本生成任务中取得了显著进展。然而，缺乏标记的领域特定知识和表格数据与文本之间的拓扑差距使得预训练语言模型难以生成准确的文本。在低资源生成中，这个领域面临着独特的挑战。受到人类如何使用先前的知识描述表格数据的启发，我们提出了一种新的框架：PromptMize，该框架针对少样本情况下的表格到文本生成。我们的框架的设计包含两个方面：提示策划和知识适配器。提示策划的目标是生成一个提示信号，为预训练语言模型提供实例指导，以弥合表格数据和文本之间的拓扑差距。此外，知识适配器从未标记的语料库中记忆领域特定的知识，在生成过程中提供必要的信息。我们对三个开放领域的少样本自然语言生成数据集进行了大量实验和分析。

    Pre-trained language models (PLM) have achieved remarkable advancement in table-to-text generation tasks. However, the lack of labeled domain-specific knowledge and the topology gap between tabular data and text make it difficult for PLMs to yield faithful text. Low-resource generation likewise faces unique challenges in this domain. Inspired by how humans descript tabular data with prior knowledge, we suggest a new framework: PromptMize, which targets table-to-text generation under few-shot settings. The design of our framework consists of two aspects: a prompt planner and a knowledge adapter. The prompt planner aims to generate a prompt signal that provides instance guidance for PLMs to bridge the topology gap between tabular data and text. Moreover, the knowledge adapter memorizes domain-specific knowledge from the unlabelled corpus to supply essential information during generation. Extensive experiments and analyses are investigated on three open domain few-shot NLG datasets: human
    
[^108]: REAP：一个大规模真实对抗贴纸基准测试

    REAP: A Large-Scale Realistic Adversarial Patch Benchmark. (arXiv:2212.05680v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05680](http://arxiv.org/abs/2212.05680)

    本文提出了一个名为REAP的大规模真实对抗贴纸基准测试，该基准测试允许用户在真实图像和真实环境条件下评估对抗贴纸攻击，为解决依赖摄像头的物理系统面临的严重威胁提供了有效的工具。

    

    机器学习模型容易受到对抗扰动的影响。其中一种著名的攻击方式是对抗贴纸，这是一种带有特定图案的贴纸，使得模型在贴纸所贴物体上的预测错误。这种攻击对于依赖于摄像头的物理系统，如自动驾驶汽车，构成了严重威胁。尽管问题的重要性，但在真实环境中进行研究是困难的；在真实世界中评估攻击和防御策略成本高昂，而合成数据则不够真实。在本文中，我们提出了REAP（真实对抗贴纸）基准测试，这是一个数字基准测试，允许用户在真实图像和真实环境条件下评估对抗贴纸攻击。基于Mapillary Vistas数据集，我们的基准测试包含超过14,000个交通标志。每个标志都经过几何和光照变换的改变，这可以用来将数字生成的贴纸真实地应用到图像中。

    Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a sticker with a particularly crafted pattern that makes the model incorrectly predict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) benchmark, a digital benchmark that allows the user to evaluate patch attacks on real images, and under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with a pair of geometric and lighting transformations, which can be used to apply a digitally generated patch realistically onto t
    
[^109]: 多速率变分自编码器：一次训练，得到完整的率失真曲线

    Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve. (arXiv:2212.03905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03905](http://arxiv.org/abs/2212.03905)

    本文介绍了一种名为多速率VAE（MR-VAE）的框架，可在单次训练中学习与不同β对应的最优参数，通过使用超网络将β映射到最优参数，以实现率失真曲线的完整训练。

    

    变分自编码器（VAEs）是一种用于学习数据的潜在表示的强大工具，广泛应用于各种应用领域。在实践中，VAEs通常需要多次训练来选择潜在变量应该保留的信息量。重构误差（失真）和KL散度（率）之间的权衡通常由超参数β参数化。在本文中，我们引入了多速率VAE（MR-VAE），这是一个计算效率高的框架，可以在单次训练中学习与不同β对应的最优参数。关键思想是使用超网络明确地制定一个响应函数，将β映射到最优参数。MR-VAEs构建了一个紧凑的响应超网络，其中的预激活根据β进行有条件的门控。通过分析线性VAEs并展示它能够准确表示线性VAEs的响应函数，我们证明了所提出的架构的合理性。

    Variational autoencoders (VAEs) are powerful tools for learning latent representations of data used in a wide range of applications. In practice, VAEs usually require multiple training rounds to choose the amount of information the latent variable should retain. This trade-off between the reconstruction error (distortion) and the KL divergence (rate) is typically parameterized by a hyperparameter $\beta$. In this paper, we introduce Multi-Rate VAE (MR-VAE), a computationally efficient framework for learning optimal parameters corresponding to various $\beta$ in a single training run. The key idea is to explicitly formulate a response function that maps $\beta$ to the optimal parameters using hypernetworks. MR-VAEs construct a compact response hypernetwork where the pre-activations are conditionally gated based on $\beta$. We justify the proposed architecture by analyzing linear VAEs and showing that it can represent response functions exactly for linear VAEs. With the learned hypernetw
    
[^110]: 将安全和隐私保护的自动化机器学习操作整合到端到端的综合物联网边缘人工智能区块链监控系统中进行糖尿病预测

    Secure and Privacy-Preserving Automated Machine Learning Operations into End-to-End Integrated IoT-Edge-Artificial Intelligence-Blockchain Monitoring System for Diabetes Mellitus Prediction. (arXiv:2211.07643v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07643](http://arxiv.org/abs/2211.07643)

    本文提出了一种基于区块链的物联网边缘人工智能系统，用于通过危险因素预测糖尿病，以确保用户数据的安全性和隐私性，并进行了比较分析不同医疗传感器、设备和方法的效果。

    

    糖尿病是全球死亡主要原因之一，目前无法治愈，如果不治疗，可能导致严重的健康并发症，如视网膜病变、肢体截肢、心血管疾病和神经疾病。因此，采取预防措施以避免/预测糖尿病的发生变得至关重要。已经提出和评估了用于糖尿病预测的机器学习方法。本文提出了一种基于危险因素的物联网边缘人工智能区块链系统进行糖尿病预测。所提出的系统建立在区块链技术的基础上，从不同医院的患者中获取危险因素数据的整体视图，并确保用户数据的安全性和隐私性。此外，我们对系统中不同的医疗传感器、设备和方法进行了比较分析，以测量和收集危险因素的值。进行了数值实验证明和比较分析。

    Diabetes Mellitus, one of the leading causes of death worldwide, has no cure to date and can lead to severe health complications, such as retinopathy, limb amputation, cardiovascular diseases, and neuronal disease, if left untreated. Consequently, it becomes crucial to take precautionary measures to avoid/predict the occurrence of diabetes. Machine learning approaches have been proposed and evaluated in the literature for diabetes prediction. This paper proposes an IoT-edge-Artificial Intelligence (AI)-blockchain system for diabetes prediction based on risk factors. The proposed system is underpinned by the blockchain to obtain a cohesive view of the risk factors data from patients across different hospitals and to ensure security and privacy of the user's data. Furthermore, we provide a comparative analysis of different medical sensors, devices, and methods to measure and collect the risk factors values in the system. Numerical experiments and comparative analysis were carried out bet
    
[^111]: 神经图模型

    Neural Graphical Models. (arXiv:2210.00453v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00453](http://arxiv.org/abs/2210.00453)

    本文介绍了神经图模型（NGMs），它可以以合理的计算成本表示复杂的特征依赖关系，适应多种图结构和混合输入数据类型，并提供了高效的学习、推断和采样算法。

    

    概率图模型经常被用来理解系统的动态。它们可以建模特征（节点）之间的关系和底层分布。理论上，这些模型可以表示非常复杂的依赖函数，但在实践中，由于与图操作相关的计算限制，通常会做简化假设。在这项工作中，我们引入了神经图模型（NGMs），试图以合理的计算成本表示复杂的特征依赖关系。给定特征关系图和相应样本，我们通过使用神经网络作为多任务学习框架来捕捉特征之间的依赖结构以及它们的复杂函数表示。我们提供了高效的学习、推断和采样算法。NGMs可以适应通用的图结构，包括有向图、无向图和混合边图，同时支持混合输入数据类型。我们展示了经验研究结果，证明了NGMs的能力。

    Probabilistic Graphical Models are often used to understand dynamics of a system. They can model relationships between features (nodes) and the underlying distribution. Theoretically these models can represent very complex dependency functions, but in practice often simplifying assumptions are made due to computational limitations associated with graph operations. In this work we introduce Neural Graphical Models (NGMs) which attempt to represent complex feature dependencies with reasonable computational costs. Given a graph of feature relationships and corresponding samples, we capture the dependency structure between the features along with their complex function representations by using a neural network as a multi-task learning framework. We provide efficient learning, inference and sampling algorithms. NGMs can fit generic graph structures including directed, undirected and mixed-edge graphs as well as support mixed input data types. We present empirical studies that show NGMs' cap
    
[^112]: 在资源受限的边缘计算中进行自适应智能传感：计算还是不计算？

    To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing. (arXiv:2209.02166v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2209.02166](http://arxiv.org/abs/2209.02166)

    本文提出了一种计算和通信延迟估计优化框架，并采用基于强化学习的方法来动态分配边缘的感知和计算资源，从而使智能传感器网络在资源受限的边缘计算中实现更佳的性能。

    

    本文考虑了一个智能传感器网络，应用于边缘计算中，对感兴趣的信号进行采样并向基站发送更新以进行远程全局监控。传感器配备有感知和计算功能，可以在传输之前在板上处理原始数据或直接发送原始数据。边缘的硬件资源有限，产生了基本的延迟 - 精度权衡：原始测量不准确但及时，而经过计算延迟后，准确的处理更新就可用。另外，如果传感器板上处理涉及数据压缩，则由于无线通信引起的延迟可能会更高。因此，需要决定何时传感器应该传输原始测量数据或依赖本地处理以最大化整体网络性能。为了解决这个传感设计问题，我们建立了一个嵌入计算和通信延迟的估计优化框架，并提出了一种基于强化学习的方法来动态分配边缘的感知和计算资源。仿真结果表明，我们提出的方法通过实现更低的估计误差、更高的吞吐量和更低的能耗，优于传统的传输方案。

    We consider a network of smart sensors for edge computing application that sample a signal of interest and send updates to a base station for remote global monitoring. Sensors are equipped with sensing and compute, and can either send raw data or process them on-board before transmission. Limited hardware resources at the edge generate a fundamental latency-accuracy trade-off: raw measurements are inaccurate but timely, whereas accurate processed updates are available after computational delay. Also, if sensor on-board processing entails data compression, latency caused by wireless communication might be higher for raw measurements. Hence, one needs to decide when sensors should transmit raw measurements or rely on local processing to maximize overall network performance. To tackle this sensing design problem, we model an estimation-theoretic optimization framework that embeds computation and communication delays, and propose a Reinforcement Learning-based approach to dynamically alloc
    
[^113]: GHN-Q：通过图形超网络预测未见量化卷积架构的参数

    GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks. (arXiv:2208.12489v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12489](http://arxiv.org/abs/2208.12489)

    本论文提出了一种名为GHN-Q的方法，通过图形超网络来预测未见量化卷积架构的参数，以提高量化鲁棒性。

    

    深度卷积神经网络（CNN）通过迭代优化训练已经取得了令人难以置信的成功，找到了最佳参数。然而，现代CNN架构通常包含数百万个参数。因此，对于单个架构的任何给定模型都存在一个庞大的参数空间。具有相似损失的模型可能具有截然不同的特性，如对抗鲁棒性、泛化能力和量化鲁棒性。对于边缘上的深度学习，量化鲁棒性通常至关重要。找到一个量化鲁棒的模型有时可能需要很大的努力。最近使用图形超网络（GHN）的研究表明，它在预测不同CNN架构的高性能参数方面表现出了显著的性能。受到这些成功的启发，我们想知道GHN-2的图形表示是否也可以用于预测量化鲁棒的参数，我们将其称为GHN-Q。我们进行了有史以来第一次探索使用图形超网络来预测量化鲁棒参数的研究。

    Deep convolutional neural network (CNN) training via iterative optimization has had incredible success in finding optimal parameters. However, modern CNN architectures often contain millions of parameters. Thus, any given model for a single architecture resides in a massive parameter space. Models with similar loss could have drastically different characteristics such as adversarial robustness, generalizability, and quantization robustness. For deep learning on the edge, quantization robustness is often crucial. Finding a model that is quantization-robust can sometimes require significant efforts. Recent works using Graph Hypernetworks (GHN) have shown remarkable performance predicting high-performant parameters of varying CNN architectures. Inspired by these successes, we wonder if the graph representations of GHN-2 can be leveraged to predict quantization-robust parameters as well, which we call GHN-Q. We conduct the first-ever study exploring the use of graph hypernetworks for predi
    
[^114]: 写下来吧：正式合同缓解多智能体强化学习中的社会困境

    Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL. (arXiv:2208.10469v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.10469](http://arxiv.org/abs/2208.10469)

    本研究通过引入正式合同的概念，解决了多智能体强化学习中个体激励和集体激励分歧导致的次优行为问题。理论和实证结果表明，通过在马尔可夫博弈中引入有约束的状态依赖奖励转移，实现了所有可观察马尔可夫博弈的子博弈完美均衡表现出社会最优行为，并提升了算法的社会性能。

    

    多智能体强化学习（MARL）是训练在共同环境中独立行动的自动化系统的强大工具。然而，当个体激励和集体激励出现分歧时，它可能导致次优行为。人类在解决这些社会困境方面具有非凡的能力。在MARL中复制这种合作行为对于自私的智能体来说是一个未解决的问题。在这项工作中，我们借鉴了经济学中正式合同的思想，以克服MARL中智能体之间的激励分歧。我们提出了一种对马尔可夫博弈进行增强的方法，智能体自愿同意在预先规定的条件下进行有约束的状态依赖奖励转移。我们的贡献是理论的和实证的。首先，我们展示了这种增强使得所有完全可观察马尔可夫博弈的子博弈完美均衡都表现出社会最优行为，只要合同空间足够丰富。接下来，我们通过展示最先进的强化学习算法在增强后的MARL中表现出更好的社会性能来补充我们的博弈论分析。

    Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis by showing that state-of-the-art R
    
[^115]: 为什么网络具有抑制性/负向连接？

    Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.03211](http://arxiv.org/abs/2208.03211)

    神经网络具有抑制性/负向连接是为了学习更多的功能，负权重在表征能力中起着至关重要的作用，并且非负深度网络无法表示某些表征空间的几何特性。

    

    大脑为什么具有抑制性连接？深度网络为什么具有负权重？我们从表征能力的角度提出了一个答案。我们认为，在自然智能中，大脑的主要作用是表征功能，在人工智能中，深度网络的主要作用也是如此。我们的答案是为什么存在抑制性/负向权重：为了学习更多的功能。我们证明了，在没有负权重的情况下，具有非递增激活函数的神经网络无法成为普适近似器。尽管这可能对一些人来说是一种直观的结果，但据我们所知，无论是在机器学习还是神经科学领域，都没有提供正式理论来证明为什么在表征能力的背景下，负权重至关重要。此外，我们还提供了非负深度网络无法表示的表征空间的几何特性的见解。我们希望这些见解能够带来对更复杂的归纳过程的更深入了解。

    Why do brains have inhibitory connections? Why do deep networks have negative weights? We propose an answer from the perspective of representation capacity. We believe representing functions is the primary role of both (i) the brain in natural intelligence, and (ii) deep networks in artificial intelligence. Our answer to why there are inhibitory/negative weights is: to learn more functions. We prove that, in the absence of negative weights, neural networks with non-decreasing activation functions are not universal approximators. While this may be an intuitive result to some, to the best of our knowledge, there is no formal theory, in either machine learning or neuroscience, that demonstrates why negative weights are crucial in the context of representation capacity. Further, we provide insights on the geometric properties of the representation space that non-negative deep networks cannot represent. We expect these insights will yield a deeper understanding of more sophisticated inducti
    
[^116]: 在超复数空间中整合知识图谱嵌入和预训练语言模型

    Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces. (arXiv:2208.02743v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.02743](http://arxiv.org/abs/2208.02743)

    本文提出在超复数空间中整合知识图谱嵌入和预训练语言模型的方法，通过利用超复数代数来表示单模态嵌入以及不同模态之间的交互，并且能够更好地利用结构性和文本性知识的相互作用。

    

    知识图谱如Wikidata在表示知识时包含了结构性和文本性知识。针对这两种模态，专门的图嵌入和语言模型方法学习了能够预测新的结构性知识的模式。目前只有少数方法将学习和推理与两种模态整合起来，而且现有方法只能部分地利用结构性和文本性知识的相互作用。我们的方法利用现有强大的单模态表示，并使用超复数代数表示单模态嵌入以及不同模态之间以及它们作为知识表示的互补手段的交互。具体而言，我们建议使用四维超复数的二面体和四元数表示来整合四种模态，即结构性知识图谱嵌入、词级表示（例如Word2vec、Fasttext）、句级表示（Sen

    Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge in order to represent knowledge. For each of the two modalities dedicated approaches for graph embedding and language models learn patterns that allow for predicting novel structural knowledge. Few approaches have integrated learning and inference with both modalities and these existing ones could only partially exploit the interaction of structural and textual knowledge. In our approach, we build on existing strong representations of single modalities and we use hypercomplex algebra to represent both, (i), single-modality embedding as well as, (ii), the interaction between different modalities and their complementary means of knowledge representation. More specifically, we suggest Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate four modalities namely structural knowledge graph embedding, word-level representations (e.g.\ Word2vec, Fasttext), sentence-level representations (Sen
    
[^117]: 用于定量论证的模糊标签语义

    Fuzzy Labeling Semantics for Quantitative Argumentation. (arXiv:2207.07339v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.07339](http://arxiv.org/abs/2207.07339)

    本论文提出了一种新颖的定量方法——模糊标签化，用于评估模糊论证系统中的论证强度。通过使用可接受度、可拒绝度和不确定度三个程度，该方法为界定论证强度提供了新的视角，并对论证的状态进行了深入理解。

    

    在抽象论证领域中，评估定量论证系统中的论证强度日益受到关注。可接受度概念在渐进语义中被广泛采用，然而，在许多实际应用中可能不足够。本文提供了一种新颖的定量方法，称为模糊标签化，用于模糊论证系统，其中使用可接受度、可拒绝度和不确定度三个程度评估论证的强度。这种设定为界定论证强度提供了新的视角，并深入理解了论证的状态。具体而言，我们研究了模糊标签的假设，这些假设提出了关于可接受度、可拒绝度和不确定度的语义的合理性要求。然后，我们提出了一类符合上述假设的模糊标签语义，并研究了模糊标签语义与现有工作的关系。

    Evaluating argument strength in quantitative argumentation systems has received increasing attention in the field of abstract argumentation. The concept of acceptability degree is widely adopted in gradual semantics, however, it may not be sufficient in many practical applications. In this paper, we provide a novel quantitative method called fuzzy labeling for fuzzy argumentation systems, in which a triple of acceptability, rejectability, and undecidability degrees is used to evaluate argument strength. Such a setting sheds new light on defining argument strength and provides a deeper understanding of the status of arguments. More specifically, we investigate the postulates of fuzzy labeling, which present the rationality requirements for semantics concerning the acceptability, rejectability, and undecidability degrees. We then propose a class of fuzzy labeling semantics conforming to the above postulates and investigate the relations between fuzzy labeling semantics and existing work 
    
[^118]: 通过组合程序学习逻辑程序

    Learning logic programs by combining programs. (arXiv:2206.01614v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01614](http://arxiv.org/abs/2206.01614)

    本论文介绍了一种通过组合小型非可分离程序的方法来学习逻辑程序。实验结果表明，这种方法在预测准确性和学习时间方面显著优于现有方法。

    

    归纳逻辑编程的目标是归纳出一个逻辑程序（一组逻辑规则），以概括训练样例。归纳具有多个规则和文字的程序是一个重大挑战。为了解决这个挑战，我们引入了一种方法，我们学习小型的不可分离的程序，并将它们组合起来。我们在一个基于约束的归纳逻辑编程系统中实现了我们的方法。我们的方法可以学习最优和递归程序，并进行谓词发明。我们在多个领域（包括游戏玩法和程序合成）的实验结果表明，我们的方法在预测准确性和学习时间方面相比现有方法有显著的优势，有时将学习时间从一个小时降低到几秒钟。

    The goal of inductive logic programming is to induce a logic program (a set of logical rules) that generalises training examples. Inducing programs with many rules and literals is a major challenge. To tackle this challenge, we introduce an approach where we learn small non-separable programs and combine them. We implement our approach in a constraint-driven ILP system. Our approach can learn optimal and recursive programs and perform predicate invention. Our experiments on multiple domains, including game playing and program synthesis, show that our approach can drastically outperform existing approaches in terms of predictive accuracies and learning times, sometimes reducing learning times from over an hour to a few seconds.
    
[^119]: IDEAL: 无需数据学习黑盒模型的高效查询方法

    IDEAL: Query-Efficient Data-Free Learning from Black-box Models. (arXiv:2205.11158v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.11158](http://arxiv.org/abs/2205.11158)

    IDEAL是一种无需真实数据的高效查询方法，用于从黑盒模型API中学习并训练一个优秀的学生模型。它通过在两个阶段进行训练，即数据生成和模型蒸馏，只需要对教师模型进行少量的查询。

    

    知识蒸馏（KD）是一种使用经过良好训练的教师模型帮助训练轻量级学生模型的典型方法。然而，大多数KD方法要求访问教师的训练数据或模型参数，这是不切实际的。为了解决这个问题，最近的研究在无数据和黑盒设置下研究了KD。然而，这些方法需要大量的查询教师模型，这会造成显著的金钱和计算成本。为了解决这些问题，我们提出了一种新方法，名为IDEAL（query-effIcient Data-free lEarning from blAck-box modeLs），旨在通过查询黑盒模型API高效地从中学习，并且无需任何真实数据来训练一个好的学生模型。具体而言，IDEAL分为两个阶段进行学生模型的训练：数据生成和模型蒸馏。请注意，IDEAL不需要在数据生成阶段进行任何查询，并且在蒸馏阶段对每个样本只查询教师模型一次。

    Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher's training data or model parameters, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called \emph{query-effIcient Data-free lEarning from blAck-box modeLs} (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on v
    
[^120]: 因果效应识别的实验设计

    Experimental Design for Causal Effect Identification. (arXiv:2205.02232v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02232](http://arxiv.org/abs/2205.02232)

    本文研究了设计最小成本的干预集合来识别因果效应的问题，证明了问题的NP-hard性质，并提出了可以找到最优解或近似解的算法，使用多项式时间启发式算法来解决计算复杂性，并在随机图上进行了模拟实验验证算法的效果。

    

    Pearl的做法是一种从观测数据中学习可识别的因果效应的完整公理方法。当这种效应不可识别时，需要执行一系列通常昂贵的干预来学习因果效应。本研究考虑设计最小成本的干预集合来识别所需效应的问题。首先，我们证明了该问题是NP-hard的，随后提出了一个算法，可以找到最优解或其对数因子的近似解。这是通过建立我们的问题与最小命中集问题之间的联系实现的。此外，我们提出了几种多项式时间启发式算法来解决问题的计算复杂性。尽管这些算法可能找到次优解，但我们的模拟结果表明它们在随机图上取得了较小的遗憾。

    Pearl's do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-hard, and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial-time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.
    
[^121]: 在学习的黎曼流形上进行反应式运动生成

    Reactive Motion Generation on Learned Riemannian Manifolds. (arXiv:2203.07761v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.07761](http://arxiv.org/abs/2203.07761)

    本文从黎曼流形的角度研究了机器人运动学习，通过学习黎曼度量和使用测地线生成的运动能适应新的环境条件，并通过调整学习到的流形实现避障功能。

    

    在最近几十年中，运动学习的进展使得机器人能够在结构化和非结构化环境中获得新的技能并适应未知条件。实践中，运动学习方法捕捉相关模式并调整它们以适应动态避障或可变目标等新条件。在本文中，我们从黎曼流形的角度研究机器人运动学习范式。我们认为通过人类示教可以学习到黎曼流形，其中测地线是自然的运动技能。通过我们的新型变分自编码器（VAE）生成使用学习到的黎曼度量产生的测地线，该自编码器特别用于恢复全位姿末端执行器状态和关节空间配置。此外，我们提出了一种通过重塑学习到的流形来促进末端执行器/多肢体避障的技术，该技术使用了一个能够感知障碍物的环境度量。使用这些测地线生成的运动可以...

    In recent decades, advancements in motion learning have enabled robots to acquire new skills and adapt to unseen conditions in both structured and unstructured environments. In practice, motion learning methods capture relevant patterns and adjust them to new conditions such as dynamic obstacle avoidance or variable targets. In this paper, we investigate the robot motion learning paradigm from a Riemannian manifold perspective. We argue that Riemannian manifolds may be learned via human demonstrations in which geodesics are natural motion skills. The geodesics are generated using a learned Riemannian metric produced by our novel variational autoencoder (VAE), which is especially intended to recover full-pose end-effector states and joint space configurations. In addition, we propose a technique for facilitating on-the-fly end-effector/multiple-limb obstacle avoidance by reshaping the learned manifold using an obstacle-aware ambient metric. The motion generated using these geodesics may
    
[^122]: 超级动物模型的预训练，用于动物行为的即插即用分析

    SuperAnimal models pretrained for plug-and-play analysis of animal behavior. (arXiv:2203.07436v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.07436](http://arxiv.org/abs/2203.07436)

    SuperAnimal是一种能够开发和部署深度学习模型的方法，不需要额外的人工标注和模型训练，同时可用于超过45个物种进行视频推断，并具有微调模型效率高的优点。

    

    行为量化在神经科学、兽医和动物保护等应用中至关重要。行为分析的关键步骤是首先提取与动物相关的关键点，即姿势估计。然而，可靠的姿势推断目前需要领域知识和手动标注来构建监督模型。我们提出了一系列技术创新，使一种名为SuperAnimal的新方法能够开发和部署深度学习模型，这些模型不需要额外的人工标注和模型训练。SuperAnimal允许对45多个物种进行视频推断，同时只使用两种全局动物姿势模型。如果需要微调模型，我们展示了SuperAnimal模型具有10倍的数据效率，并且胜过先前的迁移学习方法。此外，我们提供了一种新的视频自适应方法来执行无监督的视频细化，并且我们展示了我们的模型在行为分类中的实用性。

    Quantification of behavior is critical in applications ranging from neuroscience, veterinary medicine and animal conservation efforts. A common key step for behavioral analysis is first extracting relevant keypoints on animals, known as pose estimation. However, reliable inference of poses currently requires domain knowledge and manual labeling effort to build supervised models. We present a series of technical innovations that enable a new method, collectively called SuperAnimal, to develop and deploy deep learning models that require zero additional human labels and model training. SuperAnimal allows video inference on over 45 species with only two global classes of animal pose models. If the models need fine-tuning, we show SuperAnimal models are 10$\times$ more data efficient and outperform prior transfer learning approaches. Moreover, we provide a new video-adaptation method to perform unsupervised refinement of videos, and we illustrate the utility of our model in behavioral clas
    
[^123]: 跨模型公平性：多模型情况下的公平性与伦理实证研究

    Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity. (arXiv:2203.07139v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07139](http://arxiv.org/abs/2203.07139)

    本文提出了一种跨模型公平性的新定义，并进行了实证研究。该研究关注数据驱动预测模型在社会背景下的公平性问题，特别是在通过选择不同预测器进行模型多样性时可能导致个人受伤害的情况。

    

    虽然基于数据驱动的预测模型是一个严格的技术构造，但它们可能在社会背景下运作，在这个背景下，善意的工程选择可能带来隐含的、间接的和意想不到的现实后果。在这个领域中，这些系统的公平性，涉及到个人和群体，是一个相关的考虑因素；它在数据捕捉可导致人们受到歧视的受保护特征时出现。迄今为止，这个概念主要针对固定模型进行研究，通常在不同的分类阈值下进行研究，力图识别和消除其运作中不希望的、具有歧视性和可能违法的方面。在本文中，我们回溯了这个固定模型的假设，提出并探索了一种新的跨模型公平性定义，即在从一组表现同样出色的模型中特定选择预测器的情况下，个人可能受到伤害，即在基于效用的模型多样性的视图下。由于一个人在不同的模型下可能被分类不同。

    While data-driven predictive models are a strictly technological construct, they may operate within a social context in which benign engineering choices entail implicit, indirect and unexpected real-life consequences. Fairness of such systems -- pertaining both to individuals and groups -- is one relevant consideration in this space; it arises when data capture protected characteristics upon which people may be discriminated. To date, this notion has predominantly been studied for a fixed model, often under different classification thresholds, striving to identify and eradicate undesirable, discriminative and possibly unlawful aspects of its operation. Here, we backtrack on this fixed model assumption to propose and explore a novel definition of cross-model fairness where individuals can be harmed when one predictor is chosen ad hoc from a group of equally-well performing models, i.e., in view of utility-based model multiplicity. Since a person may be classified differently across mode
    
[^124]: 关于使用双感知相似性进行渐进网络对齐的能力研究

    On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2201.10945](http://arxiv.org/abs/2201.10945)

    本研究提出了Grad-Align，一种渐进网络对齐方法，通过利用强一致性节点对逐步发现节点对。该方法首先生成节点嵌入，然后计算双感知相似性度量逐步对齐节点。

    

    网络对齐（NA）是基于网络结构和节点属性查找两个网络之间节点对应关系的任务。我们的研究动机在于，由于大多数现有的NA方法都试图一次性发现所有节点对，因此它们没有利用通过节点对应关系的中间发现来更准确地找到节点匹配过程中的下一个对应关系的信息。为了解决这个挑战，我们提出了Grad-Align，一种新的渐进网络对齐方法，通过充分利用在渐进匹配的早期阶段容易发现的节点对来逐步发现节点对。具体而言，Grad-Align首先基于图神经网络和我们的逐层重构损失生成两个网络的节点嵌入。然后，通过计算双感知相似性度量逐步对齐节点。

    Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once, they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose Grad-Align, a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, Grad-Align first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss, a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures 
    
[^125]: 基于历史意识的多模态变压器用于视觉语言导航

    History Aware Multimodal Transformer for Vision-and-Language Navigation. (arXiv:2110.13309v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.13309](http://arxiv.org/abs/2110.13309)

    本论文提出了一种基于历史意识的多模态变压器(HAMT)，用于视觉语言导航。该方法通过编码过去的全景观察，综合考虑了文本、历史和当前观察，实现自主导航。在训练中，通过多个代理任务和强化学习进一步提高导航策略。

    

    视觉语言导航旨在构建能够遵循指令并在真实场景中导航的自主视觉代理。为了记住先前访问过的位置和采取的行动，大多数视觉语言导航方法使用递归状态实现记忆。相反，我们引入了一种历史意识多模态变压器(History Aware Multimodal Transformer，HAMT)，将长期历史纳入多模态决策中。HAMT通过层次视觉变压器(ViT)高效编码所有过去的全景观察结果，首先使用ViT对单个图像进行编码，然后模型化全景观察中图像之间的空间关系，最后考虑历史中全景观察之间的时间关系。然后，它将文本、历史和当前观察共同组合起来预测下一步行动。我们首先使用多个代理任务对HAMT进行端到端训练，包括单步行动预测和空间关系预测，然后使用强化学习进一步提高导航策略。

    Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy
    
[^126]: 数据驱动举措中认识不确定性的表达及其感知

    Representations of epistemic uncertainty and its perception in data-driven initiatives. (arXiv:2110.11482v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2110.11482](http://arxiv.org/abs/2110.11482)

    本研究为了支持不断发展的数据驱动方法论，提出了一个新颖的概念模型，用于描述知识表示中的不确定性，并推理代理人进行的信息传输。通过对知识状态和其动态的代数描述，我们能够比较和组合知识状态，以表示更新。

    

    借助人工智能的出现推动的新兴数据驱动策略正在重塑决策过程，远离对直接数据交互的传统依赖。这种范式转变引入了评估数据驱动举措影响的新挑战。为了支持这些不断发展的方法论，迫切需要新的模型，能够描述源于有限数据可观测性以及由此产生的决策中的歧义的不确定性。本文提出了一个新颖的概念模型，旨在处理知识表示中的不确定性，并推理代理人进行信息传输的不确定性。借鉴目前用于评估数据驱动举措产生的价值的多维框架，我们提供了对知识状态及其动态的代数描述。具体而言，我们赋予我们的模型一种形式化结构，用于比较和组合知识状态；通过这些组合来表示更新。

    Emerging data-driven strategies, powered by the advent of AI, are reshaping decision-making processes, moving away from traditional reliance on direct data interaction. This paradigm shift introduces new challenges in assessing the impact of data-driven initiatives. To support these evolving methodologies, there is a crucial need for new models capable of describing the uncertainties stemming from limited data observability and the resulting ambiguities in decision-making. This contribution presents a novel conceptual model designed to deal with uncertainty in knowledge representations and reasoning about information transfer mediated by agents. Drawing from the multidimensional frameworks currently adopted to assess the value generated in data-driven initiatives, we provide an algebraic description of knowledge states and their dynamics. Specifically, we endow our model with a formal structure to compare and combine knowledge states; an update is represented through these combinations
    

