# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression.](http://arxiv.org/abs/2309.05030) | 本文提出了去殖民化人工智能对齐的三个建议：改变基本道德哲学为达尔玛哲学，允许多元主义的论证传统存在于对齐技术中，以及将价值认识论扩展到超越自然语言中的指令。 |
| [^2] | [VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching.](http://arxiv.org/abs/2309.05027) | VoiceFlow使用矫正流匹配算法实现了高效文本转语音，并在合成质量上优于传统的扩散模型。 |
| [^3] | [FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation.](http://arxiv.org/abs/2309.05007) | 本文引入了一项真实世界的信息获取跟进问题生成任务，通过生成跟进问题来更深入地理解初始问题和答案。构建了数据集FOLLOWUPQG，评估了当前的问题生成模型在生成跟进问题方面的效果，并展示了其作为一个具有挑战性的基准任务的验证。 |
| [^4] | [RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution.](http://arxiv.org/abs/2309.04977) | 本文提出了一种综合预训练BERT和句法关系图注意网络（RGAT）的端到端解析器，以更深入地研究句法依赖信息在指代消解任务中的作用。通过对句法依赖图进行监督学习，并不需要对整个BERT进行微调，我们提高了先前最佳模型的F1分数。 |
| [^5] | [AVARS -- Alleviating Unexpected Urban Road Traffic Congestion using UAVs.](http://arxiv.org/abs/2309.04976) | 这篇论文提出使用无人机来缓解意外的城市道路交通拥堵。研究指出，传统的交通信号控制系统效率低下，而基于摄像机和深度强化学习算法的系统更加有效。然而，由于成本问题，无人机可能是一个更好的选择。 |
| [^6] | [Continual Robot Learning using Self-Supervised Task Inference.](http://arxiv.org/abs/2309.04974) | 本文提出了一种使用自我监督的任务推断方法，以实现机器人的连续学习。通过学习动作和意图嵌入以及行为嵌入，机器人可以推断出当前正在进行的任务，从而实现不断学习新任务的能力。 |
| [^7] | [Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning.](http://arxiv.org/abs/2309.04965) | Prefix-diffusion是一种轻量级的图像字幕扩散模型，通过在扩散过程中注入前缀图像嵌入来实现多样性，并通过预训练模型和额外的映射网络来减少参数。该模型能够生成多样的字幕，同时保持流畅性和相关性，并取得了有希望的性能。 |
| [^8] | [Multi-document Summarization: A Comparative Evaluation.](http://arxiv.org/abs/2309.04951) | 本文评估了多文档摘要领域的最新模型在不同领域和数据集上的表现，发现通用预训练模型LED在MS$^2$数据集上的性能优于其他模型，为未来的MDS研究提供了宝贵的参考和发展方向。 |
| [^9] | [MFPNet: Multi-scale Feature Propagation Nwtwork For Lightweight Semantic Segmentation.](http://arxiv.org/abs/2309.04914) | MFPNet是一个轻量级分割架构，利用特殊的残差块和图卷积网络实现了多尺度特征传播，获得了优越的分割结果。 |
| [^10] | [A Review of Machine Learning-based Security in Cloud Computing.](http://arxiv.org/abs/2309.04911) | 云计算的快速发展带来了安全风险，机器学习的应用可以在识别和解决安全问题方面减少人工干预的需求，并通过分析大量数据进行准确预测，从而改变了云服务提供商在安全方面的方法。 |
| [^11] | [Effective Real Image Editing with Accelerated Iterative Diffusion Inversion.](http://arxiv.org/abs/2309.04907) | 本文提出了一种称为AIDI的加速迭代扩散逆转方法，通过使用混合导向技术，可以在有效的计算效率下显著提高真实图像编辑的重建精度。 |
| [^12] | [How to Evaluate Semantic Communications for Images with ViTScore Metric?.](http://arxiv.org/abs/2309.04891) | 这篇论文提出了一种新的度量标准ViTScore来评估图像的语义相似性，解决了传统度量标准在语义通信中的不适用性。 |
| [^13] | [Distributional Data Augmentation Methods for Low Resource Language.](http://arxiv.org/abs/2309.04862) | 本文提出了两种用于低资源语言的文本增强方法：易于分布式数据增强（EDDA）和类型特定的相似词替换（TSSR）。它们通过使用语义上下文信息和词性标记来改进易于数据增强方法（EDA），从而提高了低资源语言下的预测性能。 |
| [^14] | [AmbientFlow: Invertible generative models from incomplete, noisy measurements.](http://arxiv.org/abs/2309.04856) | AmbientFlow是一个从噪声和不完整数据中直接学习基于流的生成模型的新框架，通过使用变分贝叶斯方法来建立这种模型，展示了其在图像科学中的应用潜力。 |
| [^15] | [Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations.](http://arxiv.org/abs/2309.04849) | 该论文提出了EmoDistill，这是一个利用知识蒸馏来学习从语音中获取情感的强大的语言和语音表示的语音情感识别框架。通过在训练过程中利用经过SER微调的预训练语音和语言教师进行信息蒸馏，该方法在IEMOCAP基准测试中实现了最新的最高准确率，表明其在单模态和多模态技术中的优越性能。 |
| [^16] | [Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs.](http://arxiv.org/abs/2309.04831) | RHPG算法是第一个在学习最优线性估计器设计方面具有可证明全局收敛性的PG算法，通过将普通的PG集成到动态规划的外循环中，将无约束且强凸的静态估计问题分解成受限且非凸的无穷时域KF问题，从而实现了全局收敛。 |
| [^17] | [Timely Fusion of Surround Radar/Lidar for Object Detection in Autonomous Driving Systems.](http://arxiv.org/abs/2309.04806) | 本文提出了一种技术，通过与工作频率仅受限于更快的环绕激光雷达而不是较慢的环绕雷达融合环绕雷达/激光雷达，以在自动驾驶系统中实现更高的响应性。 |
| [^18] | [Towards Real-World Burst Image Super-Resolution: Benchmark and Method.](http://arxiv.org/abs/2309.04803) | 本文建立了一个大规模的真实世界爆发超分辨率数据集RealBSR，通过引入联合爆发关联网络（FBAnet），从多帧中忠实地重建图像细节。FBAnet使用从结构几何角度的简单单应对齐和联合亲和力融合（FAF）策略来聚合帧间的互补信息，并通过基于Transformer的爆发表示解码模块来解码融合的信息表示。 |
| [^19] | [CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning.](http://arxiv.org/abs/2309.04802) | CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。 |
| [^20] | [TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines.](http://arxiv.org/abs/2309.04801) | TMComposites是一种插拔式协作方式，通过特化和评估成员的能力，使得Tsetlin机器（TM）可以在较复杂的图像分类任务中表现出更强的性能。 |
| [^21] | [A Full-fledged Commit Message Quality Checker Based on Machine Learning.](http://arxiv.org/abs/2309.04797) | 基于机器学习的全面提交消息质量检查器，能够自动评估提交消息的质量，并提供上下文和语义的检查。 |
| [^22] | [Towards Robust Model Watermark via Reducing Parametric Vulnerability.](http://arxiv.org/abs/2309.04777) | 这篇论文提出了一种通过减少参数弱点来改进模型水印的方法，实验结果表明这种方法可以在近邻中找到并恢复去水印模型的水印行为。 |
| [^23] | [SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning.](http://arxiv.org/abs/2309.04766) | SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。 |
| [^24] | [AudRandAug: Random Image Augmentations for Audio Classification.](http://arxiv.org/abs/2309.04762) | AudRandAug是一种应用于音频数据的随机图像增强方法，它从专门的音频搜索空间中选择数据增强策略，并在准确率表现方面优于其他方法。 |
| [^25] | [RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification.](http://arxiv.org/abs/2309.04760) | RR-CP 提出了一种可靠区域基础的确认预测方法，为医学图像分类提供高效干预和质量检查。它通过优化预测集的大小，在用户指定的错误率下提供更强的统计保证。 |
| [^26] | [Towards Real-time Training of Physics-informed Neural Networks: Applications in Ultrafast Ultrasound Blood Flow Imaging.](http://arxiv.org/abs/2309.04755) | 本研究提出了一种实时训练基于物理信息的神经网络（PINN）的框架，用于解决Navier-Stokes方程，以实现超快速超声血流成像。该框架将Navier-Stokes方程离散化为稳态，并通过迁移学习顺序求解稳态方程。此外，采用平均恒定随机梯度下降作为初始化，并提出了一种并行训练方案，适用于所有时间戳。 |
| [^27] | [A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction.](http://arxiv.org/abs/2309.04733) | 提出了一种用于细粒度多时空风力预测的多时空网络模型(MHSTN)，该模型能够从多个数据源中提取特征，并产生精确和高效的预测结果。 |
| [^28] | [TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering.](http://arxiv.org/abs/2309.04732) | TCGAN是一个用于时间序列识别的卷积生成对抗网络，通过对抗博弈学习时间序列的分层表示，无需标记信息。 |
| [^29] | [Transitions in echo index and dependence on input repetitions.](http://arxiv.org/abs/2309.04728) | 回声指数是一个非自治动力系统中同时稳定渐近响应的数量，我们研究了回声指数对依赖于参数以及输入重复的关系。 |
| [^30] | [Toward Reproducing Network Research Results Using Large Language Models.](http://arxiv.org/abs/2309.04716) | 本文提出使用大型语言模型（LLMs）来重现网络研究结果，通过一个小规模实验证明了其可行性，并以ChatGPT为工具重现了不同发表于著名会议和期刊的网络系统。 |
| [^31] | [Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact.](http://arxiv.org/abs/2309.04710) | Jade是一种可微分的关节刚体物理引擎，具有无重叠摩擦接触的特点。它采用线性互补问题模型，实现了无重叠碰撞模拟和稳定的多个摩擦接触LCP解决方案。它还使用连续碰撞检测和回溯策略来防止物体重叠，并通过梯度计算和修改Dantzig算法来保证整个模拟过程的可微分性。大量实验证明了Jade在接触丰富任务中的有效性。 |
| [^32] | [Advantage Actor-Critic with Reasoner: Explaining the Agent's Behavior from an Exploratory Perspective.](http://arxiv.org/abs/2309.04707) | 本文提出了一种新颖的基于Reasoner的优势演员-评论员（A2CR）方法，通过预定义和分类演员行为的潜在目的，自动生成一个更全面和可解释的理解代理决策过程的范例。 |
| [^33] | [Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model.](http://arxiv.org/abs/2309.04704) | 本研究考虑使用LLM模型通过细调实现虚假信息和假新闻的深入分析，揭示复杂的风格和叙事，并提取命名实体的情感，以此作为监督机器学习模型中的预测性特征。 |
| [^34] | [Advancements in Upper Body Exoskeleton: Implementing Active Gravity Compensation with a Feedforward Controller.](http://arxiv.org/abs/2309.04698) | 该研究提出了一种用于上肢外骨骼的前馈控制系统，实现主动重力补偿。该系统利用内部传感器的位置数据计算力矩，并采用分析控制方程。与反馈控制系统相比，前馈控制具有更低的硬件复杂性和更积极的响应，提高了性能。实验结果表明该系统在保持稳定性能的同时减少了摩擦和不希望的偏转。 |
| [^35] | [Code-Style In-Context Learning for Knowledge-Based Question Answering.](http://arxiv.org/abs/2309.04695) | 本论文提出了一种在上下文中学习编程风格的方法，用于解决基于知识的问答中生成逻辑表达式的格式错误问题。 |
| [^36] | [Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations.](http://arxiv.org/abs/2309.04676) | 本论文提出了一种名为CEMSP的解决方案，通过限制异常特征的变化值来提供灵活且健壮的对事实解释。 |
| [^37] | [FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning.](http://arxiv.org/abs/2309.04663) | FIAT是一种将上下文学习和完全微调范式融合的新的学习方式，可以在最大模型上进行指令和推理，并且在较小模型上进行参数更新，经过多语言任务测试，比之前的方法都表现更好。 |
| [^38] | [Video and Synthetic MRI Pre-training of 3D Vision Architectures for Neuroimage Analysis.](http://arxiv.org/abs/2309.04651) | 本研究评估了不同的预训练方法对于3D医学影像任务的适用性，并发现预训练在改善任务性能方面具有积极影响。 |
| [^39] | [Efficient Finetuning Large Language Models For Vietnamese Chatbot.](http://arxiv.org/abs/2309.04646) | 本研究针对越南语聊天机器人的开发，通过利用来自Alpaca、GPT4All和Chat-Doctor等开源项目的大规模指令跟随数据集，成功训练了四个模型，此为越南语的首个指令数据集。 |
| [^40] | [Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation.](http://arxiv.org/abs/2309.04640) | 该论文提出了一种能够从少量示范中学习力基动作的方法，通过对触觉表示进行预训练，并利用示范学习来训练动作生成器。实验证明，预训练显著提高了模型识别物理特性和生成所需动作的能力。 |
| [^41] | [Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning.](http://arxiv.org/abs/2309.04626) | 感知调整查询（PAQ）是一种新的用于收集人类反馈的查询机制，采用反向测量方案，结合了基数查询和序数查询的优点。我们将PAQ应用于度量学习问题中，通过PAQ测量来学习未知的马氏距离，并开发了一个两阶段估计器，提供了样本复杂性保证。 |
| [^42] | [Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2309.04615) | 本文提出了一种新的基于模型的多智能体强化学习方法，通过使用模块化的世界模型，减少了多智能体系统中训练的样本复杂性，并成功预测了联合动作价值函数。 |
| [^43] | [Linking Symptom Inventories using Semantic Textual Similarity.](http://arxiv.org/abs/2309.04607) | 该论文介绍了一种使用语义文本相似性（STS）来链接不同的症状清单的方法，通过测试预训练的STS模型在不同的数据源中预测症状严重程度，该方法在相关任务中达到了74.8%的准确率，优于其他模型。 |
| [^44] | [EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras.](http://arxiv.org/abs/2309.04579) | 这项研究提出了一种使用自我中心摄像头进行摔倒检测的方法，并构建了一个新的视听数据集。通过迟决策融合将音频和视觉信息相结合可以提高检测性能。 |
| [^45] | [Unleashing the Power of Graph Learning through LLM-based Autonomous Agents.](http://arxiv.org/abs/2309.04565) | 本文提出了一种使用大型语言模型（LLMs）作为自主代理的方法，以简化多样化的现实世界图中的学习过程，并克服了现有方法中的限制。 |
| [^46] | [Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime.](http://arxiv.org/abs/2309.04522) | 本文提出了一个马尔可夫近似学习模型，统一了神经切向核（NTK）和神经网络高斯过程（NNGP）核，用于描述无限宽度深层网络的学习动力学。 |
| [^47] | [Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks.](http://arxiv.org/abs/2309.04515) | 本文研究了使用卷积变分瓶颈的隐私保护联合学习，并发现了PRECODE的工作原理和影响。PRECODE通过引入随机性梯度防止梯度反转攻击的收敛，但也提出了一种攻击方法来禁用其隐私保护效果。 |
| [^48] | [Systematic Review of Techniques in Brain Image Synthesis using Deep Learning.](http://arxiv.org/abs/2309.04511) | 本文是一篇关于深度学习在脑部图像合成中的技术综述，探讨了各种方法和技术，并强调了变压器在医学成像领域的潜在作用。 |
| [^49] | [Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems.](http://arxiv.org/abs/2309.04508) | 这项研究提出了一种新颖的方法，利用图神经网络中的图注意力网络模块，通过融合传感器阵列的数据来增强IoT空气污染监测平台中传感器的校准精度。 |
| [^50] | [Compositional Learning of Visually-Grounded Concepts Using Reinforcement.](http://arxiv.org/abs/2309.04504) | 本研究探讨了深度强化学习代理如何学习和组合基于颜色和形状的组合指令，以解决空间导航任务中的新颖组合。通过利用冻结的文本编码器，代理所需的训练回合数减少了20倍。 |
| [^51] | [A Context-Sensitive Approach to XAI in Music Performance.](http://arxiv.org/abs/2309.04491) | 这篇论文提出了一个针对音乐演奏中的可解释人工智能（XAI）的情境敏感方法，强调了上下文和受众在解释需求中的重要性，并通过针对特定受众并根据反馈不断改进解释来提高人工智能系统的透明度和可解释性。 |
| [^52] | [Penalization Framework For Autonomous Agents Using Answer Set Programming.](http://arxiv.org/abs/2309.04487) | 本文提出了一个框架，用于对智能agents实施惩罚，以保证其符合许可或义务政策。框架可以根据agents的遵从程度对其行为进行惩罚，并提供一个算法来选择具有最小总惩罚的计划。这一框架可以有效地惩罚不服从命令的agents。 |
| [^53] | [Multimodal machine learning for materials science: composition-structure bimodal learning for experimentally measured properties.](http://arxiv.org/abs/2309.04478) | 本论文介绍了一种基于成分-结构双模态学习的多模态机器学习方法，用于增强对实验测量材料性质的学习和预测，从而降低预测误差。 |
| [^54] | [SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments.](http://arxiv.org/abs/2309.04077) | SayNav是一种使用大型语言模型进行动态规划导航的方法，通过使用人类知识和场景图实现对复杂导航任务的高效泛化，动态生成指令并根据新信息不断完善未来步骤。 |
| [^55] | [No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function.](http://arxiv.org/abs/2309.03224) | 该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。 |
| [^56] | [CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models.](http://arxiv.org/abs/2309.01940) | CodeApex是一个双语编程评估基准，用于评估大型语言模型在编程理解和代码生成任务上的能力。该基准包括多个选择题和算法问题，评估了14个LLM的编程能力，并发现仍有改进空间。 |
| [^57] | [Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks.](http://arxiv.org/abs/2309.01838) | 本文提出了一种简单但有效且高效的防御替代方案，引入一种启发式方法来扰动输出概率，可以轻松集成到模型中而无需额外训练，并且在抵御最先进的窃取攻击中表现出有效性。 |
| [^58] | [Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation.](http://arxiv.org/abs/2309.01717) | 该论文提出了一种基于层次变换器的方法，通过选择性插值来解决在跨学科研究提案和非跨学科研究提案之间规模差异引起的不公平现象。 |
| [^59] | [Les Houches Lectures on Deep Learning at Large & Infinite Width.](http://arxiv.org/abs/2309.01592) | 本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。 |
| [^60] | [Area-norm COBRA on Conditional Survival Prediction.](http://arxiv.org/abs/2309.00417) | 本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。 |
| [^61] | [Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm.](http://arxiv.org/abs/2308.16775) | 这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。 |
| [^62] | [International Governance of Civilian AI: A Jurisdictional Certification Approach.](http://arxiv.org/abs/2308.15514) | 这项研究提出了一种国际治理民用人工智能的方法，通过建立国际人工智能组织来认证国家的管辖区域是否符合国际监督标准，进而禁止从未经认证的管辖区域进口AI供应链所包含产品。 |
| [^63] | [Empowering LLM to use Smartphone for Intelligent Task Automation.](http://arxiv.org/abs/2308.15272) | 本论文提出了AutoDroid，一个移动任务自动化系统，可以在任何Android应用程序上自动处理任意任务。它通过结合LLMs的常识知识和应用的领域特定知识来实现，通过自动化的动态分析来实现功能意识的UI表示方法和基于探索的内存注入技术。 |
| [^64] | [ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment.](http://arxiv.org/abs/2308.14448) | 本研究介绍了ExpCLIP这一技术，通过语义对齐将文本和面部表情融合，使得风格化语音驱动的面部动画具有灵活性和用户友好性。 |
| [^65] | [Exploring Large Language Models for Knowledge Graph Completion.](http://arxiv.org/abs/2308.13916) | 本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。 |
| [^66] | [Stochastic Configuration Machines for Industrial Artificial Intelligence.](http://arxiv.org/abs/2308.13570) | 本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。 |
| [^67] | [MLLM-DataEngine: An Iterative Refinement Approach for MLLM.](http://arxiv.org/abs/2308.13566) | 本文提出了一种名为MLLM-DataEngine的迭代改进方法，它通过分析模型弱点，生成适当的增量数据集并迭代地增强模型能力。与以往方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面表现更好。 |
| [^68] | [Financial News Analytics Using Fine-Tuned Llama 2 GPT Model.](http://arxiv.org/abs/2308.13032) | 本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。 |
| [^69] | [VIGC: Visual Instruction Generation and Correction.](http://arxiv.org/abs/2308.12714) | 本文提出了VIGC框架，使多模态大型语言模型能够生成和纠正视觉指令数据，解决了缺乏高质量调整数据的挑战。 |
| [^70] | [Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields.](http://arxiv.org/abs/2308.11974) | Blending-NeRF是一种基于NeRF的模型，通过使用文本提示来实现局部编辑，能够在不扭曲对象形状的情况下，混合原始对象和目标对象并添加风格效果。该模型利用预训练的视觉-语言对齐模型CLIP进行指导，并成功地实现了添加新对象、修改纹理和移除原始对象部分的功能。 |
| [^71] | [An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification.](http://arxiv.org/abs/2308.11241) | 本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。 |
| [^72] | [Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis.](http://arxiv.org/abs/2308.11224) | 本研究评估了四个大型语言模型在图数据上解决分析问题的能力，结果显示LLM在理解图数据、生成正确结果和进行结构推理方面表现出色，但在真实性和矫正能力方面存在一些挑战。 |
| [^73] | [PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations.](http://arxiv.org/abs/2308.07327) | PokerKit是一个全面的Python库，用于细粒度多变体扑克游戏模拟，提供广泛的扑克变体支持和灵活的游戏状态控制，对扑克AI开发、工具创建和在线扑克赌场实现等领域具有重要贡献。 |
| [^74] | [AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining.](http://arxiv.org/abs/2308.05734) | 本文提出了一种利用自监督预训练学习方法进行语音、音乐和音效生成的框架，通过引入通用音频表示LOA，将任何音频转换为LOA，并利用以LOA为条件的潜在扩散模型进行自监督音频生成学习。 |
| [^75] | [Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links.](http://arxiv.org/abs/2308.03929) | 本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。 |
| [^76] | [Select2Col: Leveraging Spatial-Temporal Importance of Semantic Information for Efficient Collaborative Perception.](http://arxiv.org/abs/2307.16517) | Select2Col是一种利用语义信息的时空重要性进行高效协作感知的新框架。它通过轻量级图神经网络估计语义信息的重要性，从而选择有益的合作者并排除负面影响。同时，还提出了一种语义信息融合算法。 |
| [^77] | [UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models.](http://arxiv.org/abs/2307.15898) | 本文提出了一种名为UniBriVL的新型通用语言表示学习方法，该方法实现了音频驱动的扩散模型的生成。它能够稳健地学习语言表示，并捕捉到音频和图像之间的关联。实验结果表明UniBriVL在下游任务中表现出良好的效果，能够从音频中选择合适的图像。 |
| [^78] | [From Probabilistic Programming to Complexity-based Programming.](http://arxiv.org/abs/2307.15453) | CompLog是一种基于复杂性的计算框架，通过计算Kolmogorov复杂性替代概率推理，实现计算某种情况意外性的度量，并通过规范的世界和心智模型的描述生成相关描述，并提供对析取和否定的替代方法。 |
| [^79] | [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2307.15217) | 本文调查了从人类反馈中进行强化学习的开放问题和基本限制，并提出了加强社会监督的审计和披露标准。 |
| [^80] | [Applying QNLP to sentiment analysis in finance.](http://arxiv.org/abs/2307.11788) | 本论文研究了在金融行业中应用量子自然语言处理(QNLP)进行情感分析的实际适用性。利用一种新颖的数据生成方法，我们发现量子增强的长短期记忆(QLSTM)可以更快地训练，并且在软件实现方面接近古典结果。 |
| [^81] | [Interpreting and Correcting Medical Image Classification with PIP-Net.](http://arxiv.org/abs/2307.10404) | 本研究利用PIP-Net开展了可解释的机器学习技术在医学图像分类中的应用，并展示了其在骨折检测和皮肤癌诊断方面的准确性和可解释性。通过无监督的预训练，PIP-Net能够轻松识别数据质量问题，并且我们还发现人们可以通过手动禁用不良原型来纠正PIP-Net的推理过程。 |
| [^82] | [Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code.](http://arxiv.org/abs/2307.07686) | 本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。 |
| [^83] | [Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions.](http://arxiv.org/abs/2307.03941) | 本文探讨了在大型语言模型时代的被遗忘权（RTBF）面临的挑战，提供了实施技术解决方案的见解。 |
| [^84] | [Towards Trustworthy Explanation: On Causal Rationalization.](http://arxiv.org/abs/2306.14115) | 该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。 |
| [^85] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^86] | [Improving Offline-to-Online Reinforcement Learning with Q-Ensembles.](http://arxiv.org/abs/2306.06871) | 我们提出了一种名为Q-Ensembles的新框架，通过增加Q网络的数量，无缝地连接离线预训练和在线微调，同时不降低性能。此外，我们适当放宽Q值估计的悲观性，并将基于集合的探索机制融入我们的框架中，从而提升了离线到在线强化学习的性能。 |
| [^87] | [LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization.](http://arxiv.org/abs/2306.01102) | 本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。 |
| [^88] | [What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks.](http://arxiv.org/abs/2305.18365) | 本文建立了包括 8 个实际化学任务的综合基准测试，有力地证明了 LLM 在实际化学中的能力。 |
| [^89] | [Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning.](http://arxiv.org/abs/2305.17256) | 本文探讨了大型语言模型在上下文学习中利用提示中的捷径的依赖性，发现大型模型更有可能在推理过程中利用提示中的捷径，这为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。 |
| [^90] | [The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification.](http://arxiv.org/abs/2305.04940) | 本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。 |
| [^91] | [Instance Segmentation in the Dark.](http://arxiv.org/abs/2304.14298) | 本文提出了在黑暗环境下的实例分割方法。为了抑制低光图像中的特征噪声，该方法采用了自适应加权下采样层、平滑定向卷积块和扰动抑制学习等技术，同时发现高位深的RAW图像可以更好地保留更丰富的场景信息。 |
| [^92] | [Segment anything, from space?.](http://arxiv.org/abs/2304.13000) | 最近开发的Segment Anything Model（SAM）模型可以基于简单的输入提示（如一个或多个点、边界框或掩码）有效分割自然图像中的对象，对视觉研究人员具有重要意义。此项研究探讨SAM在空中图像问题上的卓越性能，并在多项基准任务上进行了验证，表现良好。 |
| [^93] | [Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks.](http://arxiv.org/abs/2304.10749) | 本文提出了一种新方法--多尺度进化神经架构搜索，用于自动设计脉冲神经网络，同时考虑微观、中观和宏观尺度的脑拓扑结构。MSE-NAS可以帮助SNN实现多电路模式的自组织集成，并通过全局性的跨模式连接来优化网络性能。 |
| [^94] | [Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach.](http://arxiv.org/abs/2304.08495) | 本论文介绍了一种名为SCAIR的算法，可以优化群体效用，解决行程规划中的多个用户排队时间和人群水平优化的问题。 |
| [^95] | [ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes.](http://arxiv.org/abs/2304.04321) | ARNOLD是一个评估基于语言引导、具有连续状态的现实3D场景任务学习的基准测试，涉及8个语言条件任务，在语言引导下帮助机器人学习理解物体状态和学习连续目标的策略。 |
| [^96] | [A Survey of Large Language Models.](http://arxiv.org/abs/2303.18223) | 本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。 |
| [^97] | [Discriminative Class Tokens for Text-to-Image Diffusion Models.](http://arxiv.org/abs/2303.17155) | 本文提出了一种基于判别性信息和自由文本的非侵入式微调技术，以实现多样性和高准确率的文本到图像生成模型。 |
| [^98] | [Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection.](http://arxiv.org/abs/2303.09026) | 本文提出了一种通识知识辅助的细粒度目标检测方法，利用通识知识推理模块处理由基准深度学习检测器给出的粗粒度标签，从而提高目标检测的准确性。经过实验验证，该方法相比于现有方法需要更少的计算量和标注资源。 |
| [^99] | [TSMixer: An all-MLP Architecture for Time Series Forecasting.](http://arxiv.org/abs/2303.06053) | TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。 |
| [^100] | [Efficient Generator of Mathematical Expressions for Symbolic Regression.](http://arxiv.org/abs/2302.09893) | 我们提出了一种基于变分自动编码器的方法来生成数学表达式，并用于符号回归。实验结果表明，我们的方法可以高效地训练，并且能够准确地编码表达式。通过优化方法探索生成的潜在空间能够解决符号回归问题，且效果优于传统的方法。 |
| [^101] | [AudioLDM: Text-to-Audio Generation with Latent Diffusion Models.](http://arxiv.org/abs/2301.12503) | AudioLDM是一种基于潜在扩散模型的文本到音频生成系统，通过对比语言-音频预训练学习音频的连续表示，从而在生成质量和计算效率上具有优势。它在TTA性能上达到了最先进水平，并且能够实现各种文本引导的音频生成。 |
| [^102] | [Projective Integral Updates for High-Dimensional Variational Inference.](http://arxiv.org/abs/2301.08374) | 该论文介绍了一种适用于高维变分推理的投影积分更新方法，并通过降低参数敏感性来实现更强健的预测。 |
| [^103] | [Physics-Informed Neural Networks for Prognostics and Health Management of Lithium-Ion Batteries.](http://arxiv.org/abs/2301.00776) | 本研究提出了一种基于物理感知神经网络（PINN）的模型融合方案，用于预测和管理锂离子电池的健康状态。该方法能够将经验或物理动态模型与数据驱动模型相结合，以充分利用各种信息来源，提高预测和管理的准确性。 |
| [^104] | [Strong-AI Autoepistemic Robots Build on Intensional First Order Logic.](http://arxiv.org/abs/2212.07935) | 本文研究了内涵一阶逻辑作为现代机器人的符号架构，可以推理自身知识、使用自然语言与人类交流、抽象语言属性。同时，通过机器人神经架构的经验获得机器人语言的基础，将其与IFOL理论中的非定义语言概念相联系。 |
| [^105] | [DWRSeg: Rethinking Efficient Acquisition of Multi-scale Contextual Information for Real-time Semantic Segmentation.](http://arxiv.org/abs/2212.01173) | DWRSeg重新思考了实时语义分割中获取多尺度上下文信息的方法，提出了一种高效的多尺度特征提取方法，通过将原始单步方法分解为区域残余化和语义残余化两个步骤，简化了多速率深度空洞卷积的角色，并提高了特征提取的效率。 |
| [^106] | [A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource.](http://arxiv.org/abs/2211.12875) | 在这篇论文中，作者对深度图聚类进行了综述研究。首先介绍了该领域的定义、评估和发展，然后提出了深度图聚类方法的分类学，并对现有方法进行了分析，总结出了挑战和机会。 |
| [^107] | [Deep Emotion Recognition in Textual Conversations: A Survey.](http://arxiv.org/abs/2211.09172) | 本调研针对对话中的情感识别进行了探讨，介绍了涉及此任务的挑战和机遇，以及描述了情感分类法和使用该分类法的基准数据集。调研总结了最重要的作品和所使用的深度学习架构，并提供了建议性的情感识别实践，以实现更好的框架。 |
| [^108] | [Improving Speech Emotion Recognition with Unsupervised Speaking Style Transfer.](http://arxiv.org/abs/2211.08843) | 本文介绍了EmoAug，一种通过无监督说话风格转换来提升语音情感识别的模型。EmoAug利用语义编码器和语调编码器表示语言和非语言信息，并通过无监督方式重建语音信号。训练完成后，EmoAug通过引入不同风格丰富了情感语音的表达，并解决了数据不平衡问题。 |
| [^109] | [Model Based Residual Policy Learning with Applications to Antenna Control.](http://arxiv.org/abs/2211.08796) | 本文介绍了一种基于模型的残差策略学习（MBRPL）方法，用于解决天线控制问题。该方法通过增强现有策略，提高了样本效率，并减少了与实际环境的交互次数。实验结果表明，该方法在初始性能方面表现出色，并为将这些算法应用于实际网络迈出了一步。 |
| [^110] | [Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing.](http://arxiv.org/abs/2211.04476) | 本研究提出了一个自动片段检测框架用于自然语言处理任务，通过发现、解释和改进模型的错误，提供了对模型行为的理解和未来模型设计的见解。 |
| [^111] | [Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics.](http://arxiv.org/abs/2211.01120) | 本文提出了一种可变层次混合模型方法，通过概率层次化建模来学习机器人应用中的逆动力学。该方法结合了传统回归模型的优势，实现了计算高效的表示和自适应数据复杂性的灵活性。 |
| [^112] | [Multi-view Multi-label Anomaly Network Traffic Classification based on MLP-Mixer Neural Network.](http://arxiv.org/abs/2210.16719) | 本文提出了一种基于MLP-Mixer的多视图多标签神经网络用于网络流量分类，该方法能够更好地捕捉流量数据的全局信息关联，通过利用不同场景之间的相关性来提高分类性能。 |
| [^113] | [Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation.](http://arxiv.org/abs/2210.05918) | 本研究通过引入尾平均和正则化技术，对时序差异(TD)学习算法进行了有限时间行为的研究。我们得出结论，尾平均TD能以最优速率 $O(1/t)$ 收敛，并且初始误差衰减速率更快。此外，正则化的TD版本在具有病态特征的问题上很有用。 |
| [^114] | [jsdp: a Java Stochastic Dynamic Programming Library.](http://arxiv.org/abs/2209.09979) | jsdp是一个Java库，通过利用Java中的lambda表达式、函数接口、集合和聚合运算符，实现了对随机动态规划问题的建模和求解。 |
| [^115] | [Predicting Word Learning in Children from the Performance of Computer Vision Systems.](http://arxiv.org/abs/2207.09847) | 通过使用计算机视觉系统的表现作为预测儿童词语学习的难度的代理，我们发现儿童获得不同类别的词语的年龄与视觉分类和字幕系统的表现有关。这些模型捕捉到了词语与视觉现象之间的关系。 |
| [^116] | [Estimating the Power Consumption of Heterogeneous Devices when performing AI Inference.](http://arxiv.org/abs/2207.06150) | 本论文评估了使用NVIDIA Jetson Nano开发板进行目标分类时的功耗情况，并通过分析发现，YOLOv5n模型在每秒输出帧数和低功耗方面表现优于其他YOLOV5变体。 |
| [^117] | [Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs.](http://arxiv.org/abs/2206.00979) | 这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。 |
| [^118] | [Attacking c-MARL More Effectively: A Data Driven Approach.](http://arxiv.org/abs/2202.03558) | 本研究提出了一种数据驱动的方法来评估合作多智能体强化学习 (c-MARL) 代理的稳健性，并通过模型构建更强大的对抗状态扰动以降低团队总奖励。同时，还提出了首个受害代理选择策略和数据驱动方法，以定义目标失败状态，从而实现更强大的对抗性攻击。实验证明，该方法在被测试的所有环境中一致优于其他基线。 |
| [^119] | [Control Theoretic Analysis of Temporal Difference Learning.](http://arxiv.org/abs/2112.14417) | 本研究对Temporal Difference学习算法进行了控制论分析，并引入了一个有限时间的框架，从控制论角度提供了对TD学习机制和强化学习领域的更深入洞察。 |
| [^120] | [Robust Feature-Level Adversaries are Interpretability Tools.](http://arxiv.org/abs/2110.03605) | 鲁棒的特征级对抗攻击不仅提供了对模型表示的研究，还具有独特的多功能性和高度鲁棒性，可以用于各种规模的图像攻击，并且可以作为可解释性工具帮助识别网络中的错误。 |
| [^121] | [A Survey of Knowledge Enhanced Pre-trained Models.](http://arxiv.org/abs/2110.00269) | 本综述提供了关于NLP中知识增强预训练语言模型的综合概述，讨论了预训练语言模型和知识表示学习的进展，并从三个不同的角度对现有的KEPLMs进行了分类，最后概述了未来研究中KEPLMs的潜在方向。 |
| [^122] | [MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging.](http://arxiv.org/abs/2108.11635) | MCML是一种基于内存的对比元学习方法，通过使用显式内存库来跟踪先前训练的剧集的标签表示，并引入了“从内存中适应”的机制，以克服小样本槽位标记中的“样本遗忘问题”。 |
| [^123] | [Intelligent GPS Spoofing Attack Detection in Power Grids.](http://arxiv.org/abs/2005.04513) | 本文提出了一种使用动态电力系统中PMU数据的神经网络GPS欺骗检测（NNGSD）方法，用于检测电网中的GPS欺骗攻击，并展示了该检测方法的实时性能。 |

# 详细

[^1]: 去殖民化的人工智能对齐：威色达尔玛、论证和艺术表达

    Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression. (arXiv:2309.05030v1 [cs.CY])

    [http://arxiv.org/abs/2309.05030](http://arxiv.org/abs/2309.05030)

    本文提出了去殖民化人工智能对齐的三个建议：改变基本道德哲学为达尔玛哲学，允许多元主义的论证传统存在于对齐技术中，以及将价值认识论扩展到超越自然语言中的指令。

    

    先前的研究已经阐明了人工智能（AI）开发和部署的殖民性。然而，这些研究很少涉及到对齐：即基于细致的人类反馈，调整大型语言模型（LLM）的行为与期望值一致。除了其他实践，殖民主义还有一部分是改变被殖民民族的信仰和价值观的历史；而当前的LLM对齐实践正是这一历史的复制。我们建议通过三个提议对AI对齐进行去殖民化：（a）将基本道德哲学从西方哲学转变为达尔玛哲学，（b）在对齐技术中允许论证和多元主义的传统，以及（c）将价值的认识论扩展到超越自然语言中的指令或命令。

    Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment. One process that that work has not engaged with much is alignment: the tuning of large language model (LLM) behavior to be in line with desired values based on fine-grained human feedback. In addition to other practices, colonialism has a history of altering the beliefs and values of colonized peoples; this history is recapitulated in current LLM alignment practices. We suggest that AI alignment be decolonialized using three proposals: (a) changing the base moral philosophy from Western philosophy to dharma, (b) permitting traditions of argument and pluralism in alignment technologies, and (c) expanding the epistemology of values beyond instructions or commandments given in natural language.
    
[^2]: VoiceFlow: 使用矫正流匹配的高效文本转语音

    VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching. (arXiv:2309.05027v1 [eess.AS])

    [http://arxiv.org/abs/2309.05027](http://arxiv.org/abs/2309.05027)

    VoiceFlow使用矫正流匹配算法实现了高效文本转语音，并在合成质量上优于传统的扩散模型。

    

    尽管扩散模型在文本转语音中因其强大的生成能力而成为一种流行选择，但从扩散模型中进行采样的内在复杂性损害了其效率。相反，我们提出了VoiceFlow，一种利用矫正流匹配算法来实现高合成质量的声学模型，只需有限次采样步骤即可实现。VoiceFlow将生成mel-spectrograms的过程转化为一个普通微分方程，在文本输入的条件下进行求解，并估计出其向量场。然后，矫正流技术有效地使其采样轨迹直线化，实现高效合成。在单个和多个说话者语料库上进行的主观和客观评估显示，VoiceFlow相对于扩散模型具有更优异的合成质量。消融研究进一步验证了VoiceFlow中矫正流技术的有效性。

    Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.
    
[^3]: FOLLOWUPQG:面向信息获取的跟进问题生成

    FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])

    [http://arxiv.org/abs/2309.05007](http://arxiv.org/abs/2309.05007)

    本文引入了一项真实世界的信息获取跟进问题生成任务，通过生成跟进问题来更深入地理解初始问题和答案。构建了数据集FOLLOWUPQG，评估了当前的问题生成模型在生成跟进问题方面的效果，并展示了其作为一个具有挑战性的基准任务的验证。

    

    人类出于好奇心而提出跟进问题，这反映了人类创造性的认知过程。我们引入了一个真实世界的信息获取跟进问题生成（FQG）任务，旨在生成能够更深入理解初始问题和答案的跟进问题。我们构建了FOLLOWUPQG数据集，包含了来自Reddit论坛的超过3K个真实世界的（初始问题，答案，跟进问题）元组，提供了对开放性问题的非专业人士友好的解释。与现有数据集相比，FOLLOWUPQG中的问题使用更多样化的实用策略来寻求信息，并展示了更高层次的认知技能（如应用和关联）。我们评估了当前的问题生成模型在生成跟进问题方面的效果，探索如何基于逐步演示生成特定类型的跟进问题。我们的结果验证了FOLLOWUPQG作为一个具有挑战性的基准任务。

    Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated q
    
[^4]: RGAT：更深入探索句法依赖信息在指代消解中的作用

    RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution. (arXiv:2309.04977v1 [cs.CL])

    [http://arxiv.org/abs/2309.04977](http://arxiv.org/abs/2309.04977)

    本文提出了一种综合预训练BERT和句法关系图注意网络（RGAT）的端到端解析器，以更深入地研究句法依赖信息在指代消解任务中的作用。通过对句法依赖图进行监督学习，并不需要对整个BERT进行微调，我们提高了先前最佳模型的F1分数。

    

    虽然句法信息对很多自然语言处理任务是有益的，但将其与词语之间的上下文信息相结合来解决指代消解问题仍需进一步探索。本文提出了一种综合预训练BERT和句法关系图注意网络（RGAT）的端到端解析器，以更深入地研究句法依赖信息在指代消解任务中的作用。具体而言，首先提出了RGAT模型，然后用于理解句法依赖图并学习更好的任务特定句法嵌入。构建了一个集成结构，将BERT嵌入和句法嵌入结合起来，为下游任务生成混合表示。我们在一个公共的Gendered Ambiguous Pronouns（GAP）数据集上进行的实验表明，在对句法依赖图进行监督学习的同时，不需要对整个BERT进行微调，我们提高了先前最佳模型（RGCN-wi）的F1分数

    Although syntactic information is beneficial for many NLP tasks, combining it with contextual information between words to solve the coreference resolution problem needs to be further explored. In this paper, we propose an end-to-end parser that combines pre-trained BERT with a Syntactic Relation Graph Attention Network (RGAT) to take a deeper look into the role of syntactic dependency information for the coreference resolution task. In particular, the RGAT model is first proposed, then used to understand the syntactic dependency graph and learn better task-specific syntactic embeddings. An integrated architecture incorporating BERT embeddings and syntactic embeddings is constructed to generate blending representations for the downstream task. Our experiments on a public Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision learning of the syntactic dependency graph and without fine-tuning the entire BERT, we increased the F1-score of the previous best model (RGCN-wi
    
[^5]: 使用无人机缓解意外的城市道路交通拥堵

    AVARS -- Alleviating Unexpected Urban Road Traffic Congestion using UAVs. (arXiv:2309.04976v1 [cs.LG])

    [http://arxiv.org/abs/2309.04976](http://arxiv.org/abs/2309.04976)

    这篇论文提出使用无人机来缓解意外的城市道路交通拥堵。研究指出，传统的交通信号控制系统效率低下，而基于摄像机和深度强化学习算法的系统更加有效。然而，由于成本问题，无人机可能是一个更好的选择。

    

    减少由途中事件（例如路段关闭，车祸等）引起的意外城市交通拥堵通常需要快速准确地选择最合适的交通信号。传统的交通信号控制系统，如SCATS和SCOOT，由感应线圈提供的交通数据更新频率较低（即超过1分钟）。此外，这些系统使用的交通信号灯计划是事发前预先编程的候选计划的有限集合中选择的。最近的研究表明，由深度强化学习（DRL）算法控制的基于摄像机的交通信号系统在减少交通拥堵方面更加有效，摄像机可以提供高频高分辨率的交通数据。然而，由于道路基础设施需要过多的潜在升级，这些系统的部署成本在大城市中较高。在本文中，我们认为无人机（UAVs）可以在缓解意外的城市道路交通拥堵方面提供一种有效的解决方案。

    Reducing unexpected urban traffic congestion caused by en-route events (e.g., road closures, car crashes, etc.) often requires fast and accurate reactions to choose the best-fit traffic signals. Traditional traffic light control systems, such as SCATS and SCOOT, are not efficient as their traffic data provided by induction loops has a low update frequency (i.e., longer than 1 minute). Moreover, the traffic light signal plans used by these systems are selected from a limited set of candidate plans pre-programmed prior to unexpected events' occurrence. Recent research demonstrates that camera-based traffic light systems controlled by deep reinforcement learning (DRL) algorithms are more effective in reducing traffic congestion, in which the cameras can provide high-frequency high-resolution traffic data. However, these systems are costly to deploy in big cities due to the excessive potential upgrades required to road infrastructure. In this paper, we argue that Unmanned Aerial Vehicles (
    
[^6]: 使用自我监督的任务推断进行连续机器人学习

    Continual Robot Learning using Self-Supervised Task Inference. (arXiv:2309.04974v1 [cs.RO])

    [http://arxiv.org/abs/2309.04974](http://arxiv.org/abs/2309.04974)

    本文提出了一种使用自我监督的任务推断方法，以实现机器人的连续学习。通过学习动作和意图嵌入以及行为嵌入，机器人可以推断出当前正在进行的任务，从而实现不断学习新任务的能力。

    

    赋予机器人像人类一样学习不断发展的技能而不是掌握单一任务的能力是机器人学习中的一个开放问题。尽管已经提出了多任务学习方法来解决这个问题，但他们很少关注任务推断。为了不断学习新任务，机器人首先需要推断出当前正在进行的任务，而不需要预定义的任务表示。在本文中，我们提出了一种自我监督的任务推断方法。我们的方法通过观察到的未标记示范的运动和效果部分的自组织来学习动作和意图嵌入，并通过联合动作-意图嵌入的自组织来学习高层行为嵌入。我们构建了一个行为匹配的自我监督学习目标，来训练一个新的任务推断网络（TINet）将未标记示范映射到其最近的行为嵌入，然后将其用作任务表示。

    Endowing robots with the human ability to learn a growing set of skills over the course of a lifetime as opposed to mastering single tasks is an open problem in robot learning. While multi-task learning approaches have been proposed to address this problem, they pay little attention to task inference. In order to continually learn new tasks, the robot first needs to infer the task at hand without requiring predefined task representations. In this paper, we propose a self-supervised task inference approach. Our approach learns action and intention embeddings from self-organization of the observed movement and effect parts of unlabeled demonstrations and a higher-level behavior embedding from self-organization of the joint action-intention embeddings. We construct a behavior-matching self-supervised learning objective to train a novel Task Inference Network (TINet) to map an unlabeled demonstration to its nearest behavior embedding, which we use as the task representation. A multi-task p
    
[^7]: 前缀扩散：一种用于多样化图像字幕的轻量级扩散模型

    Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])

    [http://arxiv.org/abs/2309.04965](http://arxiv.org/abs/2309.04965)

    Prefix-diffusion是一种轻量级的图像字幕扩散模型，通过在扩散过程中注入前缀图像嵌入来实现多样性，并通过预训练模型和额外的映射网络来减少参数。该模型能够生成多样的字幕，同时保持流畅性和相关性，并取得了有希望的性能。

    

    尽管在图像字幕生成方面取得了很大的进展，但生成的字幕的多样性有限和参数规模较大仍然是这些系统在实际应用中的主要障碍。在这项工作中，我们提出了一种轻量级的图像字幕网络，结合了连续扩散，称为前缀扩散。为了实现多样性，我们设计了一种高效的方法，将前缀图像嵌入到扩散模型的去噪过程中。为了减少可训练的参数，我们使用预训练模型提取图像特征，并进一步设计了额外的映射网络。前缀扩散能够以相对较少的参数生成多样化的字幕，同时保持字幕的流畅性和相关性，从扩散模型的生成能力中受益。我们的工作为扩展图像字幕的扩散模型铺平了道路，并与最近的方法相比取得了有希望的性能。

    While impressive performance has been achieved in image captioning, the limited diversity of the generated captions and the large parameter scale remain major barriers to the real-word application of these systems. In this work, we propose a lightweight image captioning network in combination with continuous diffusion, called Prefix-diffusion. To achieve diversity, we design an efficient method that injects prefix image embeddings into the denoising process of the diffusion model. In order to reduce trainable parameters, we employ a pre-trained model to extract image features and further design an extra mapping network. Prefix-diffusion is able to generate diverse captions with relatively less parameters, while maintaining the fluency and relevance of the captions benefiting from the generative capabilities of the diffusion model. Our work paves the way for scaling up diffusion models for image captioning, and achieves promising performance compared with recent approaches.
    
[^8]: 多文档摘要：一项比较评估

    Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])

    [http://arxiv.org/abs/2309.04951](http://arxiv.org/abs/2309.04951)

    本文评估了多文档摘要领域的最新模型在不同领域和数据集上的表现，发现通用预训练模型LED在MS$^2$数据集上的性能优于其他模型，为未来的MDS研究提供了宝贵的参考和发展方向。

    

    本文旨在评估多文档摘要(MDS)领域的最新模型在不同领域和不同类型数据集上的表现，并研究现有模型的局限性，以确定未来的研究方向。为了填补这个空白，我们进行了广泛的文献评估，以确定最新的模型和数据集。我们对BigSurvey-MDS和MS$^2$数据集上的PRIMERA和PEGASUS模型的性能进行了分析，这些数据集由于领域的不同而带来了独特的挑战。我们的研究结果表明，通用预训练模型LED在MS$^2$数据集上的性能优于PRIMERA和PEGASUS。我们使用ROUGE分数作为性能度量指标，评估了不同数据集上的模型。我们的研究为了解模型的优势和不足提供了宝贵的见解，并为不同领域中准确、鲁棒的模型的发展提供了参考。这项研究对未来的MDS研究具有重要价值。

    This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can 
    
[^9]: MFPNet: 轻量级语义分割的多尺度特征传播网络

    MFPNet: Multi-scale Feature Propagation Nwtwork For Lightweight Semantic Segmentation. (arXiv:2309.04914v1 [cs.CV])

    [http://arxiv.org/abs/2309.04914](http://arxiv.org/abs/2309.04914)

    MFPNet是一个轻量级分割架构，利用特殊的残差块和图卷积网络实现了多尺度特征传播，获得了优越的分割结果。

    

    与大规模模型的丰富研究相比，轻量级语义分割的进展似乎进展较慢。然而，现有的紧凑方法由于网络的浅层而导致特征表示能力有限。在本文中，我们提出了一种新颖的轻量级分割架构，称为多尺度特征传播网络（MFPNet），以解决这一困境。具体而言，我们设计了一种强大的编码器-解码器结构，采用对称的残差块，其中包含灵活的瓶颈残差模块（BRM），以探索深度和丰富的多尺度语义上下文。此外，利用图卷积网络（GCNs）建模潜在的长程上下文关系能力，促进了BRM块之间的多尺度特征传播。在基准数据集上评估时，我们提出的方法显示出优越的分割结果。

    In contrast to the abundant research focusing on large-scale models, the progress in lightweight semantic segmentation appears to be advancing at a comparatively slower pace. However, existing compact methods often suffer from limited feature representation capability due to the shallowness of their networks. In this paper, we propose a novel lightweight segmentation architecture, called Multi-scale Feature Propagation Network (MFPNet), to address the dilemma. Specifically, we design a robust Encoder-Decoder structure featuring symmetrical residual blocks that consist of flexible bottleneck residual modules (BRMs) to explore deep and rich muti-scale semantic context. Furthermore, taking benefit from their capacity to model latent long-range contextual relationships, we leverage Graph Convolutional Networks (GCNs) to facilitate multi-scale feature propagation between the BRM blocks. When evaluated on benchmark datasets, our proposed approach shows superior segmentation results.
    
[^10]: 机器学习在云计算安全中的应用综述

    A Review of Machine Learning-based Security in Cloud Computing. (arXiv:2309.04911v1 [cs.CR])

    [http://arxiv.org/abs/2309.04911](http://arxiv.org/abs/2309.04911)

    云计算的快速发展带来了安全风险，机器学习的应用可以在识别和解决安全问题方面减少人工干预的需求，并通过分析大量数据进行准确预测，从而改变了云服务提供商在安全方面的方法。

    

    云计算（CC）正在改变向用户提供IT资源的方式，使他们能够以更高的成本效益和简化的基础设施来访问和管理系统。然而，随着CC的增长，出现了一系列安全风险，包括对可用性、完整性和机密性的威胁。为了解决这些挑战，云服务提供商（CSPs）越来越多地使用机器学习（ML）来减少在识别和解决安全问题方面的人工干预。ML能够分析大量数据并进行高准确性预测，可以改变CSPs在安全方面的方法。在本文中，我们将探讨机器学习在云计算安全领域的一些最新研究。我们将研究一系列ML算法的特性和有效性，突出它们独特的优势和潜在限制。我们的目标是提供关于ML在云计算中的当前状况的综合概述。

    Cloud Computing (CC) is revolutionizing the way IT resources are delivered to users, allowing them to access and manage their systems with increased cost-effectiveness and simplified infrastructure. However, with the growth of CC comes a host of security risks, including threats to availability, integrity, and confidentiality. To address these challenges, Machine Learning (ML) is increasingly being used by Cloud Service Providers (CSPs) to reduce the need for human intervention in identifying and resolving security issues. With the ability to analyze vast amounts of data, and make high-accuracy predictions, ML can transform the way CSPs approach security. In this paper, we will explore some of the most recent research in the field of ML-based security in Cloud Computing. We will examine the features and effectiveness of a range of ML algorithms, highlighting their unique strengths and potential limitations. Our goal is to provide a comprehensive overview of the current state of ML in c
    
[^11]: 使用加速迭代扩散逆转进行有效的真实图像编辑

    Effective Real Image Editing with Accelerated Iterative Diffusion Inversion. (arXiv:2309.04907v1 [cs.CV])

    [http://arxiv.org/abs/2309.04907](http://arxiv.org/abs/2309.04907)

    本文提出了一种称为AIDI的加速迭代扩散逆转方法，通过使用混合导向技术，可以在有效的计算效率下显著提高真实图像编辑的重建精度。

    

    尽管最近取得了一些进展，但使用现代生成模型编辑和操纵自然图像仍然具有挑战。在使用生成对抗网络（GAN）时，一个主要障碍在于将真实图像映射到其在潜在空间中对应的噪声向量的反向过程，因为需要能够重建图像才能编辑其内容。同样地，对于去噪扩散隐式模型（DDIM），每个反转步骤中的线性化假设使得整个确定性反转过程不可靠。现有的解决反转稳定性问题的方法通常在计算效率上产生显著的权衡。在这项工作中，我们提出了一种称为加速迭代扩散反转方法（AIDI）的方法，通过最小化额外的空间和时间复杂度开销显著提高重建精度。通过使用一种新颖的混合导向技术，我们展示了在广泛的图像范围上可以获得有效的结果。

    Despite all recent progress, it is still challenging to edit and manipulate natural images with modern generative models. When using Generative Adversarial Network (GAN), one major hurdle is in the inversion process mapping a real image to its corresponding noise vector in the latent space, since its necessary to be able to reconstruct an image to edit its contents. Likewise for Denoising Diffusion Implicit Models (DDIM), the linearization assumption in each inversion step makes the whole deterministic inversion process unreliable. Existing approaches that have tackled the problem of inversion stability often incur in significant trade-offs in computational efficiency. In this work we propose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that significantly improves reconstruction accuracy with minimal additional overhead in space and time complexity. By using a novel blended guidance technique, we show that effective results can be obtained on a large range of image
    
[^12]: 如何用ViTScore度量图像的语义通信？

    How to Evaluate Semantic Communications for Images with ViTScore Metric?. (arXiv:2309.04891v1 [cs.CV])

    [http://arxiv.org/abs/2309.04891](http://arxiv.org/abs/2309.04891)

    这篇论文提出了一种新的度量标准ViTScore来评估图像的语义相似性，解决了传统度量标准在语义通信中的不适用性。

    

    语义通信（SC）被期望成为一个新的范式转变，推动下一代通信的发展，其主要关注点从精确的比特传输转移到了在通信中进行有效的语义信息交换。然而，先前和广泛使用的图像评估度量标准不适用于评估SC中的图像语义相似性。用于衡量两个图像之间相似度的经典度量标准通常依赖于像素级或结构级，例如PSNR和MS-SSIM。在SC中直接使用基于深度学习方法的某些定制度量标准（如LPIPS）是不可行的。为了解决这个问题，受NLP领域的BERTScore的启发，我们提出了一种用于评估图像语义相似性的新度量标准，称为Vision Transformer Score（ViTScore）。我们从理论上证明了ViTScore具有对称性、有界性和归一化等三个重要属性，这使得ViTScore在图像衡量中方便和直观。

    Semantic communications (SC) have been expected to be a new paradigm shifting to catalyze the next generation communication, whose main concerns shift from accurate bit transmission to effective semantic information exchange in communications. However, the previous and widely-used metrics for images are not applicable to evaluate the image semantic similarity in SC. Classical metrics to measure the similarity between two images usually rely on the pixel level or the structural level, such as the PSNR and the MS-SSIM. Straightforwardly using some tailored metrics based on deep-learning methods in CV community, such as the LPIPS, is infeasible for SC. To tackle this, inspired by BERTScore in NLP community, we propose a novel metric for evaluating image semantic similarity, named Vision Transformer Score (ViTScore). We prove theoretically that ViTScore has 3 important properties, including symmetry, boundedness, and normalization, which make ViTScore convenient and intuitive for image mea
    
[^13]: 低资源语言的分布式数据增强方法

    Distributional Data Augmentation Methods for Low Resource Language. (arXiv:2309.04862v1 [cs.CL])

    [http://arxiv.org/abs/2309.04862](http://arxiv.org/abs/2309.04862)

    本文提出了两种用于低资源语言的文本增强方法：易于分布式数据增强（EDDA）和类型特定的相似词替换（TSSR）。它们通过使用语义上下文信息和词性标记来改进易于数据增强方法（EDA），从而提高了低资源语言下的预测性能。

    

    文本增强是一种从不足资源的语料库中构造合成数据以提高预测性能的技术。合成数据生成在许多领域中很常见。然而，最近，文本增强已经在自然语言处理（NLP）中出现，以提升下游任务的效果。目前最先进的文本增强技术之一是易于数据增强（EDA），它通过注入和替换同义词以及随机排列句子来增加训练数据。EDA的一个主要障碍是需要多功能和完整的同义词词典，在低资源语言中很难找到。为了提高EDA的实用性，我们提出了两种扩展方法：易于分布式数据增强（EDDA）和类型特定的相似词替换（TSSR），它使用语义词上下文信息和词性标记来进行词替换和增强。在广泛的实证评估中，我们展示了所提出方法的实用性

    Text augmentation is a technique for constructing synthetic data from an under-resourced corpus to improve predictive performance. Synthetic data generation is common in numerous domains. However, recently text augmentation has emerged in natural language processing (NLP) to improve downstream tasks. One of the current state-of-the-art text augmentation techniques is easy data augmentation (EDA), which augments the training data by injecting and replacing synonyms and randomly permuting sentences. One major obstacle with EDA is the need for versatile and complete synonym dictionaries, which cannot be easily found in low-resource languages. To improve the utility of EDA, we propose two extensions, easy distributional data augmentation (EDDA) and type specific similar word replacement (TSSR), which uses semantic word context information and part-of-speech tags for word replacement and augmentation. In an extensive empirical evaluation, we show the utility of the proposed methods, measure
    
[^14]: AmbientFlow: 来自不完整、噪声测量的可逆生成模型

    AmbientFlow: Invertible generative models from incomplete, noisy measurements. (arXiv:2309.04856v1 [cs.LG])

    [http://arxiv.org/abs/2309.04856](http://arxiv.org/abs/2309.04856)

    AmbientFlow是一个从噪声和不完整数据中直接学习基于流的生成模型的新框架，通过使用变分贝叶斯方法来建立这种模型，展示了其在图像科学中的应用潜力。

    

    生成模型在图像科学中具有潜在的应用，如图像重建、后验采样和数据共享。基于流的生成模型特别有吸引力，因为它们能以可行的方式提供精确的密度估计以及快速、廉价和多样的样本。然而，训练这样的模型需要一个大型、高质量的对象数据集。在计算成像等应用中，由于需要长时间获取或高辐射剂量，往往难以获取这样的数据，而获取这些对象的噪声或部分观测测量更可行。在这项工作中，我们提出了AmbientFlow，一个从噪声和不完整数据直接学习基于流的生成模型的框架。使用变分贝叶斯方法，提出了一个从噪声、不完整数据建立基于流的生成模型的新框架。广泛的数值研究证明了该方法的有效性。

    Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness o
    
[^15]: 通过提取精炼的语音和语言情感表示进行语音情感识别

    Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])

    [http://arxiv.org/abs/2309.04849](http://arxiv.org/abs/2309.04849)

    该论文提出了EmoDistill，这是一个利用知识蒸馏来学习从语音中获取情感的强大的语言和语音表示的语音情感识别框架。通过在训练过程中利用经过SER微调的预训练语音和语言教师进行信息蒸馏，该方法在IEMOCAP基准测试中实现了最新的最高准确率，表明其在单模态和多模态技术中的优越性能。

    

    我们提出了EmoDistill，这是一个新颖的语音情感识别（SER）框架，利用跨模态知识蒸馏来学习从语音中获取情感的强大的语言和语音表示。在推理过程中，我们的方法仅使用一串语音信号来进行单模态SER，从而减少计算开销并避免运行时的转录和语音特征提取错误。在训练过程中，我们的方法从一对经过SER微调的预训练的语音和语言教师中的嵌入和逻辑层面蒸馏信息。在IEMOCAP基准测试中的实验表明，我们的方法在准确率上优于其他单模态和多模态技术，并达到了77.49％的无权重准确率和78.91％的加权准确率的最新成绩。详细的消融研究还展示了我们方法的每个组件的影响。

    We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
    
[^16]: 在学习估计器设计中，递减时域策略搜索的全局收敛性研究

    Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs. (arXiv:2309.04831v1 [math.OC])

    [http://arxiv.org/abs/2309.04831](http://arxiv.org/abs/2309.04831)

    RHPG算法是第一个在学习最优线性估计器设计方面具有可证明全局收敛性的PG算法，通过将普通的PG集成到动态规划的外循环中，将无约束且强凸的静态估计问题分解成受限且非凸的无穷时域KF问题，从而实现了全局收敛。

    

    我们引入了递减时域策略梯度（RHPG）算法，这是第一个在学习最优线性估计器设计（即卡尔曼滤波器）方面具有可证明全局收敛性的PG算法。值得注意的是，RHPG算法不需要任何关于系统的先验知识作为初始化，也不需要目标系统是开环稳定的。RHPG的关键是将普通的PG（或其他策略搜索方向）集成到动态规划的外循环中，将在策略参数中受限且非凸的无穷时域KF问题分解为一系列无约束且强凸的静态估计问题，从而实现全局收敛。我们进一步对RHPG的优化路线进行了详细分析，并详细说明了算法的收敛性和样本复杂度保证。这项工作是针对控制器设计开展强化学习算法的初步尝试。

    We introduce the receding-horizon policy gradient (RHPG) algorithm, the first PG algorithm with provable global convergence in learning the optimal linear estimator designs, i.e., the Kalman filter (KF). Notably, the RHPG algorithm does not require any prior knowledge of the system for initialization and does not require the target system to be open-loop stable. The key of RHPG is that we integrate vanilla PG (or any other policy search directions) into a dynamic programming outer loop, which iteratively decomposes the infinite-horizon KF problem that is constrained and non-convex in the policy parameter into a sequence of static estimation problems that are unconstrained and strongly-convex, thus enabling global convergence. We further provide fine-grained analyses of the optimization landscape under RHPG and detail the convergence and sample complexity guarantees of the algorithm. This work serves as an initial attempt to develop reinforcement learning algorithms specifically for con
    
[^17]: 自动驾驶系统中环绕雷达/激光雷达的及时融合用于目标检测

    Timely Fusion of Surround Radar/Lidar for Object Detection in Autonomous Driving Systems. (arXiv:2309.04806v1 [cs.CV])

    [http://arxiv.org/abs/2309.04806](http://arxiv.org/abs/2309.04806)

    本文提出了一种技术，通过与工作频率仅受限于更快的环绕激光雷达而不是较慢的环绕雷达融合环绕雷达/激光雷达，以在自动驾驶系统中实现更高的响应性。

    

    将雷达和激光雷达传感器数据融合可以充分利用它们的互补优势，为自动驾驶系统提供更准确的周围环境重建。环绕雷达/激光雷达可以以最小的成本提供360度视野采样，是自动驾驶系统的有前景的感知硬件解决方案。然而，由于固有的物理限制，环绕雷达的旋转速度及生成雷达数据帧的频率远低于环绕激光雷达。现有的雷达/激光雷达融合方法必须以环绕雷达的低频率工作，而无法满足自动驾驶系统的高响应性要求。本文基于最先进的目标检测模型MVDNet开发了一种技术，通过与工作频率仅受限于更快的环绕激光雷达而不是较慢的环绕雷达融合环绕雷达/激光雷达。我们方法的基本思路很简单：让MVDNet处理暂时不对齐的雷达和激光雷达帧，然后根据帧时间信息对融合后的结果进行校正。

    Fusing Radar and Lidar sensor data can fully utilize their complementary advantages and provide more accurate reconstruction of the surrounding for autonomous driving systems. Surround Radar/Lidar can provide 360-degree view sampling with the minimal cost, which are promising sensing hardware solutions for autonomous driving systems. However, due to the intrinsic physical constraints, the rotating speed of surround Radar, and thus the frequency to generate Radar data frames, is much lower than surround Lidar. Existing Radar/Lidar fusion methods have to work at the low frequency of surround Radar, which cannot meet the high responsiveness requirement of autonomous driving systems.This paper develops techniques to fuse surround Radar/Lidar with working frequency only limited by the faster surround Lidar instead of the slower surround Radar, based on the state-of-the-art object detection model MVDNet. The basic idea of our approach is simple: we let MVDNet work with temporally unaligned d
    
[^18]: 实现真实世界的爆发图像超分辨率：基准和方法

    Towards Real-World Burst Image Super-Resolution: Benchmark and Method. (arXiv:2309.04803v1 [cs.CV])

    [http://arxiv.org/abs/2309.04803](http://arxiv.org/abs/2309.04803)

    本文建立了一个大规模的真实世界爆发超分辨率数据集RealBSR，通过引入联合爆发关联网络（FBAnet），从多帧中忠实地重建图像细节。FBAnet使用从结构几何角度的简单单应对齐和联合亲和力融合（FAF）策略来聚合帧间的互补信息，并通过基于Transformer的爆发表示解码模块来解码融合的信息表示。

    

    尽管取得了重大进展，单一图像超分辨率（SISR）在从一张输入图像中重建高质量图像时总是陷入困境，尤其是在现实场景中。本文建立了一个大规模的真实世界爆发超分辨率数据集RealBSR，以探索从多帧中忠实地重建图像细节。此外，我们引入了一种联合爆发关联网络（FBAnet），来研究真实世界图像退化条件下图像之间的非平凡像素位移。具体而言，我们的FBAnet不使用像素级对齐，而是从结构几何角度使用简单的单应对齐和联合亲和力融合（FAF）策略来聚合帧间的互补信息。这些融合的信息表示被馈送到一个基于Transformer的爆发表示解码模块中。此外，我们对我们数据集的两个版本，即RealBSR和...

    Despite substantial advances, single-image super-resolution (SISR) is always in a dilemma to reconstruct high-quality images with limited information from one input image, especially in realistic scenarios. In this paper, we establish a large-scale real-world burst super-resolution dataset, i.e., RealBSR, to explore the faithful reconstruction of image details from multiple frames. Furthermore, we introduce a Federated Burst Affinity network (FBAnet) to investigate non-trivial pixel-wise displacements among images under real-world image degradation. Specifically, rather than using pixel-wise alignment, our FBAnet employs a simple homography alignment from a structural geometry aspect and a Federated Affinity Fusion (FAF) strategy to aggregate the complementary information among frames. Those fused informative representations are fed to a Transformer-based module of burst representation decoding. Besides, we have conducted extensive experiments on two versions of our datasets, i.e., Rea
    
[^19]: CPMR: 基于上下文感知的增量顺序推荐与伪多任务学习

    CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])

    [http://arxiv.org/abs/2309.04802](http://arxiv.org/abs/2309.04802)

    CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。

    

    用户进行互动的动机可以分为静态偏好和动态兴趣。为了准确地建模用户随时间变化的表示，最近的顺序推荐研究利用信息传播和演化从批量到达的互动中进行挖掘。然而，他们忽略了在上下文场景中人们很容易受到其他用户的最近行为的影响，并且在所有历史互动中应用演化会稀释最近互动的重要性，从而无法准确地建模兴趣动态的演化。为了解决这个问题，我们提出了一种基于上下文感知的伪多任务推荐系统（CPMR），通过为每个用户和项目创建三个表示（静态嵌入、历史时间状态和上下文时间状态），来建模历史和上下文情境中的演化。为了同时提高时间状态演化和增量推荐的性能。

    The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
    
[^20]: TMComposites: 专用Tsetlin机器之间的即插即用协作

    TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines. (arXiv:2309.04801v1 [cs.CV])

    [http://arxiv.org/abs/2309.04801](http://arxiv.org/abs/2309.04801)

    TMComposites是一种插拔式协作方式，通过特化和评估成员的能力，使得Tsetlin机器（TM）可以在较复杂的图像分类任务中表现出更强的性能。

    

    Tsetlin机器（TM）从基于算术的机器学习转变为基于逻辑的机器学习提供了一种根本性的改变。支持卷积，TM成功地处理像MNIST，Fashion-MNIST和CIFAR-2这样的图像分类数据集。然而，TM在表示更复杂任务的CIFAR-10和CIFAR-100上难以达到最先进的性能。本文介绍了专用TM之间的即插即用协作，称为TM Composites。协作依赖于TM在学习过程中特化的能力和在推理过程中评估自身能力的能力。当团队组合时，最自信的TM做出决策，解决了不确定的问题。通过这种方式，TM组合比其成员更有能力，从他们的专业化中受益。协作的特点是即插即用，即成员可以在任何时间、任何方式下组合，无需微调。在我们的实证评估中，我们实现了三个TM专业化：梯度直方图，自适应高斯

    Tsetlin Machines (TMs) provide a fundamental shift from arithmetic-based to logic-based machine learning. Supporting convolution, they deal successfully with image classification datasets like MNIST, Fashion-MNIST, and CIFAR-2. However, the TM struggles with getting state-of-the-art performance on CIFAR-10 and CIFAR-100, representing more complex tasks. This paper introduces plug-and-play collaboration between specialized TMs, referred to as TM Composites. The collaboration relies on a TM's ability to specialize during learning and to assess its competence during inference. When teaming up, the most confident TMs make the decisions, relieving the uncertain ones. In this manner, a TM Composite becomes more competent than its members, benefiting from their specializations. The collaboration is plug-and-play in that members can be combined in any way, at any time, without fine-tuning. We implement three TM specializations in our empirical evaluation: Histogram of Gradients, Adaptive Gauss
    
[^21]: 基于机器学习的全面的提交消息质量检查器

    A Full-fledged Commit Message Quality Checker Based on Machine Learning. (arXiv:2309.04797v1 [cs.SE])

    [http://arxiv.org/abs/2309.04797](http://arxiv.org/abs/2309.04797)

    基于机器学习的全面提交消息质量检查器，能够自动评估提交消息的质量，并提供上下文和语义的检查。

    

    提交消息（CMs）是版本控制的重要组成部分。通过提供有关更改内容和原因的重要上下文，它们极大地支持软件维护和演进。但是撰写良好的CMs是困难的，开发人员经常忽视。到目前为止，还没有适合实践的工具可以自动评估CM的编写质量，包括其含义和上下文。鉴于此任务的挑战性，我们提出研究问题：机器学习方法能够如何衡量CM质量，包括语义和上下文？通过考虑最流行的CM质量指南的所有规则，创建这些规则的数据集，并训练和评估最先进的机器学习模型来检查这些规则，我们可以回答这个研究问题：对于最具挑战性的任务，实践中具有82.9％的最低F1分数足够好。我们开发了一个全面的开源框架，检查所有这些CM质量规则。它可以用来辅助开发人员编写更好的CMs。

    Commit messages (CMs) are an essential part of version control. By providing important context in regard to what has changed and why, they strongly support software maintenance and evolution. But writing good CMs is difficult and often neglected by developers. So far, there is no tool suitable for practice that automatically assesses how well a CM is written, including its meaning and context. Since this task is challenging, we ask the research question: how well can the CM quality, including semantics and context, be measured with machine learning methods? By considering all rules from the most popular CM quality guideline, creating datasets for those rules, and training and evaluating state-of-the-art machine learning models to check those rules, we can answer the research question with: sufficiently well for practice, with the lowest F$_1$ score of 82.9\%, for the most challenging task. We develop a full-fledged open-source framework that checks all these CM quality rules. It is use
    
[^22]: 通过减少参数的弱点 改进稳健模型数

    Towards Robust Model Watermark via Reducing Parametric Vulnerability. (arXiv:2309.04777v1 [cs.CR])

    [http://arxiv.org/abs/2309.04777](http://arxiv.org/abs/2309.04777)

    这篇论文提出了一种通过减少参数弱点来改进模型水印的方法，实验结果表明这种方法可以在近邻中找到并恢复去水印模型的水印行为。

    

    深度神经网络由于其商业价值和对资源的巨大需求而成为宝贵的资产，然而为了保护深度神经网络的版权，最近基于后门的拥有权验证变得流行起来。在这种验证方式中，模型所有者可以在发布之前通过嵌入特定的后门行为对模型进行水印标记。防御方（通常是模型所有者）可以根据行为的存在来判断可疑的第三方模型是否是从他们那里“偷”来的。不幸的是，这些水印已经被证明对移除攻击（甚至如微调）非常脆弱。为了进一步探索这种脆弱性，我们对参数空间进行了研究，并发现存在许多在水印模型附近的去水印模型，这些模型可能很容易被用于移除攻击。受到这一发现的启发，我们提出了一个迷你最大化最小化问题来找到这些去水印模型并恢复它们的水印行为。大量实验表明，我们的方法可以找到并恢复去水印模型的水印行为。

    Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is ``stolen'' from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that o
    
[^23]: SeaEval多语言基础模型：从跨语言对齐到文化推理

    SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])

    [http://arxiv.org/abs/2309.04766](http://arxiv.org/abs/2309.04766)

    SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。

    

    我们提出了一种用于多语言基础模型的SeaEval基准测试。除了表征这些模型如何理解和推理自然语言外，我们还研究了它们对文化实践、细微差别和价值观的理解能力。除了标准的准确度指标，我们还调查了基础模型在语义和多语言性维度上的脆弱性。我们的分析涵盖了开源和闭源模型，从而得到了在经典的自然语言处理任务、推理和文化理解方面的实证结果。重要发现包括：（1）大多数模型在给出改写指令时的行为各异；（2）许多模型仍然受到暴露偏差的影响（如位置偏差、大多数标签偏差）；（3）对于根源于事实、科学和常识知识的问题，预期在语义上等价的多语言查询应该得到一致的回答。然而，大多数模型在这些查询上表现出令人意外的不一致性；（4）多语言情况下，模型对于情感相关的问题表现出不同程度的一致性。

    We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
    
[^24]: AudRandAug：用于音频分类的随机图像增强方法

    AudRandAug: Random Image Augmentations for Audio Classification. (arXiv:2309.04762v1 [cs.SD])

    [http://arxiv.org/abs/2309.04762](http://arxiv.org/abs/2309.04762)

    AudRandAug是一种应用于音频数据的随机图像增强方法，它从专门的音频搜索空间中选择数据增强策略，并在准确率表现方面优于其他方法。

    

    数据增强已被证明在训练神经网络中非常有效。最近，提出了一种称为RandAug的方法，该方法从预定义的搜索空间中随机选择数据增强技术。RandAug在图像相关任务中展示了显著的性能提升，同时附加的计算开销很小。然而，以往的研究尚未探索将RandAug应用于将音频转化为图像模式的音频数据增强。为了填补这一空白，我们引入了AudRandAug，这是一种针对音频数据的RandAug改进方法。AudRandAug从专门的音频搜索空间中选择数据增强策略。为了评估AudRandAug的有效性，我们使用了各种模型和数据集进行了实验。我们的研究结果表明，AudRandAug在准确率表现方面优于其他现有的数据增强方法。

    Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance.
    
[^25]: RR-CP: 可靠区域基础的可信医学图像分类的一种确认预测方法

    RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification. (arXiv:2309.04760v1 [cs.LG])

    [http://arxiv.org/abs/2309.04760](http://arxiv.org/abs/2309.04760)

    RR-CP 提出了一种可靠区域基础的确认预测方法，为医学图像分类提供高效干预和质量检查。它通过优化预测集的大小，在用户指定的错误率下提供更强的统计保证。

    

    确认预测（CP）为给定的测试样本生成一组预测，使得预测集几乎总是包含真实标签（例如，99.5％的时间）。 CP对给定测试样本的可能标签提供全面的预测，而集合的大小表示预测的确定程度（例如，大于一的集合是“不确定的”）。 CP的这些独特属性使人类专家和医学AI模型之间能够有效合作，在临床决策过程中进行高效干预和质量检查。 在本文中，我们提出了一种新的方法，称为可靠区域基础的确认预测（RR-CP），旨在提供更强的统计保证，以便在测试时间内达到用户指定的错误率（例如，0.5％），并在此约束条件下，优化预测集的大小（尽量小）。 当用户指定的错误率达到时，我们认为预测集大小是一个重要的衡量标准。

    Conformal prediction (CP) generates a set of predictions for a given test sample such that the prediction set almost always contains the true label (e.g., 99.5\% of the time). CP provides comprehensive predictions on possible labels of a given test sample, and the size of the set indicates how certain the predictions are (e.g., a set larger than one is `uncertain'). Such distinct properties of CP enable effective collaborations between human experts and medical AI models, allowing efficient intervention and quality check in clinical decision-making. In this paper, we propose a new method called Reliable-Region-Based Conformal Prediction (RR-CP), which aims to impose a stronger statistical guarantee so that the user-specified error rate (e.g., 0.5\%) can be achieved in the test time, and under this constraint, the size of the prediction set is optimized (to be small). We consider a small prediction set size an important measure only when the user-specified error rate is achieved. Experi
    
[^26]: 实时训练基于物理信息的神经网络：超快速超声血流成像的应用

    Towards Real-time Training of Physics-informed Neural Networks: Applications in Ultrafast Ultrasound Blood Flow Imaging. (arXiv:2309.04755v1 [cs.CE])

    [http://arxiv.org/abs/2309.04755](http://arxiv.org/abs/2309.04755)

    本研究提出了一种实时训练基于物理信息的神经网络（PINN）的框架，用于解决Navier-Stokes方程，以实现超快速超声血流成像。该框架将Navier-Stokes方程离散化为稳态，并通过迁移学习顺序求解稳态方程。此外，采用平均恒定随机梯度下降作为初始化，并提出了一种并行训练方案，适用于所有时间戳。

    

    基于物理信息的神经网络（PINN）是纳维-斯托克斯方程的最杰出求解器之一，而纳维-斯托克斯方程广泛应用于血流的控制方程。然而，目前的方法仅依赖于完整的纳维-斯托克斯方程，对于超快速多普勒超声，这一最新技术应用于\emph{体内}复杂血流动力学的展示，每秒获取数千帧（或时间戳），这是不切实际的。本文首先提出了一种新的PINN训练框架，通过将纳维-斯托克斯方程离散化为稳态，并通过迁移学习顺序求解稳态纳维-斯托克斯方程，为解决纳维-斯托克斯方程提供了新的训练框架，称为SeqPINN。在SeqPINN的成功基础上，我们采用了平均恒定随机梯度下降（SGD）作为初始化的思想，并提出了一种并行训练方案，适用于所有时间戳。为了确保良好的泛化初始化，我们借鉴了

    Physics-informed Neural Network (PINN) is one of the most preeminent solvers of Navier-Stokes equations, which are widely used as the governing equation of blood flow. However, current approaches, relying on full Navier-Stokes equations, are impractical for ultrafast Doppler ultrasound, the state-of-the-art technique for depiction of complex blood flow dynamics \emph{in vivo} through acquired thousands of frames (or, timestamps) per second. In this article, we first propose a novel training framework of PINN for solving Navier-Stokes equations by discretizing Navier-Stokes equations into steady state and sequentially solving steady-state Navier-Stokes equations with transfer learning. The novel training framework is coined as SeqPINN. Upon the success of SeqPINN, we adopt the idea of averaged constant stochastic gradient descent (SGD) as initialization and propose a parallel training scheme for all timestamps. To ensure an initialization that generalizes well, we borrow the concept of 
    
[^27]: 用于细粒度多时空风力预测的深度神经网络

    A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction. (arXiv:2309.04733v1 [cs.LG])

    [http://arxiv.org/abs/2309.04733](http://arxiv.org/abs/2309.04733)

    提出了一种用于细粒度多时空风力预测的多时空网络模型(MHSTN)，该模型能够从多个数据源中提取特征，并产生精确和高效的预测结果。

    

    风速和风向的预测对于航空和风能发电等许多实际应用具有重要影响，由于天气数据中的高随机性和复杂相关性，这一预测非常具有挑战性。现有方法通常只关注一部分影响因素，因此缺乏对问题的系统处理。此外，在文献中对于细粒度预测的关注较少，而细粒度预测对于高效的行业运营至关重要。在本研究中，我们提出了一种新颖的数据驱动模型，即多时空风力网络(MHSTN)，以实现精确和高效的细粒度风力预测。MHSTN将针对不同因素的多个深度神经网络集成到一个序列到序列(Seq2Seq)骨架中，以有效地从各种数据源提取特征，并为给定区域内的所有站点产生多时空的预测。MHSTN由四个主要模块组成。首先，一个时间模块

    The prediction of wind in terms of both wind speed and direction, which has a crucial impact on many real-world applications like aviation and wind power generation, is extremely challenging due to the high stochasticity and complicated correlation in the weather data. Existing methods typically focus on a sub-set of influential factors and thus lack a systematic treatment of the problem. In addition, fine-grained forecasting is essential for efficient industry operations, but has been less attended in the literature. In this work, we propose a novel data-driven model, Multi-Horizon SpatioTemporal Network (MHSTN), generally for accurate and efficient fine-grained wind prediction. MHSTN integrates multiple deep neural networks targeting different factors in a sequence-to-sequence (Seq2Seq) backbone to effectively extract features from various data sources and produce multi-horizon predictions for all sites within a given region. MHSTN is composed of four major modules. First, a temporal
    
[^28]: TCGAN: 用于时间序列分类和聚类的卷积生成对抗网络

    TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering. (arXiv:2309.04732v1 [cs.LG])

    [http://arxiv.org/abs/2309.04732](http://arxiv.org/abs/2309.04732)

    TCGAN是一个用于时间序列识别的卷积生成对抗网络，通过对抗博弈学习时间序列的分层表示，无需标记信息。

    

    最近的研究表明，监督式卷积神经网络（CNN）在学习时间序列数据的分层表示方面具有优越性，可用于成功的分类。然而，这些方法需要足够大的标记数据进行稳定学习，但是获取高质量的标记时间序列数据可能代价高昂，也可能不可行。生成对抗网络（GANs）在增强无监督和半监督学习方面取得了巨大成功。然而，据我们所知，GANs如何有效地作为学习时间序列识别（即分类和聚类）的通用解决方案仍然不清楚。上述考虑激发了我们引入了一个时间序列卷积GAN（TCGAN）。TCGAN通过在没有标签信息的情况下，两个一维CNN（即生成器和判别器）之间进行对抗博弈来学习。然后，训练的TCGAN的一部分被重复利用来构建一个表示学习器。

    Recent works have demonstrated the superiority of supervised Convolutional Neural Networks (CNNs) in learning hierarchical representations from time series data for successful classification. These methods require sufficiently large labeled data for stable learning, however acquiring high-quality labeled time series data can be costly and potentially infeasible. Generative Adversarial Networks (GANs) have achieved great success in enhancing unsupervised and semi-supervised learning. Nonetheless, to our best knowledge, it remains unclear how effectively GANs can serve as a general-purpose solution to learn representations for time series recognition, i.e., classification and clustering. The above considerations inspire us to introduce a Time-series Convolutional GAN (TCGAN). TCGAN learns by playing an adversarial game between two one-dimensional CNNs (i.e., a generator and a discriminator) in the absence of label information. Parts of the trained TCGAN are then reused to construct a rep
    
[^29]: 回声指数的转变及对输入重复的依赖

    Transitions in echo index and dependence on input repetitions. (arXiv:2309.04728v1 [math.DS])

    [http://arxiv.org/abs/2309.04728](http://arxiv.org/abs/2309.04728)

    回声指数是一个非自治动力系统中同时稳定渐近响应的数量，我们研究了回声指数对依赖于参数以及输入重复的关系。

    

    回声指数是一个非自治（即受输入驱动）动力系统中同时稳定渐近响应的数量。它推广了递归神经网络的回声状态性质，这对应于回声指数等于一。在本文中，我们研究了回声指数如何依赖于控制系统对强迫动力学的有限状态随机外部输入的参数。我们考虑了一个在有限一组映射之间切换的非自治系统的回声指数，其中我们假设每个映射具有有限一组双曲型平衡吸引子。我们发现每个映射的最小和最大重复对于得到的回声指数是至关重要的。将我们的理论发现用RNN计算框架表示，我们得到对于小幅强迫，回声指数对应于无输入系统的吸引子数量，而对于大幅强迫，回声指数减少到一。中间的情况

    The echo index counts the number of simultaneously stable asymptotic responses of a nonautonomous (i.e. input-driven) dynamical system. It generalizes the well-known echo state property for recurrent neural networks this corresponds to the echo index being equal to one. In this paper, we investigate how the echo index depends on parameters that govern typical responses to a finite-state ergodic external input that forces the dynamics. We consider the echo index for a nonautonomous system that switches between a finite set of maps, where we assume that each map possesses a finite set of hyperbolic equilibrium attractors. We find the minimum and maximum repetitions of each map are crucial for the resulting echo index. Casting our theoretical findings in the RNN computing framework, we obtain that for small amplitude forcing the echo index corresponds to the number of attractors for the input-free system, while for large amplitude forcing, the echo index reduces to one. The intermediate
    
[^30]: 利用大型语言模型重现网络研究结果

    Toward Reproducing Network Research Results Using Large Language Models. (arXiv:2309.04716v1 [cs.LG])

    [http://arxiv.org/abs/2309.04716](http://arxiv.org/abs/2309.04716)

    本文提出使用大型语言模型（LLMs）来重现网络研究结果，通过一个小规模实验证明了其可行性，并以ChatGPT为工具重现了不同发表于著名会议和期刊的网络系统。

    

    在网络学术界和工业界中，重现研究结果非常重要。当前的最佳实践通常有三种方法：（1）寻找公开可用的原型；（2）联系作者获取私有原型；以及（3）根据论文描述手动实现原型。然而，大多数已发表的网络研究没有公开原型，而获取私有原型也很困难。因此，大部分重现工作都花费在根据论文描述进行手动实现上，这既耗时又费力，容易出错。本文大胆地提出使用新兴的大型语言模型（LLMs）来重现网络研究结果。特别地，我们首先通过小规模实验证明了其可行性，其中四名具备必要网络知识的学生使用ChatGPT进行了不同发表于著名会议和期刊的网络系统的重现工作。

    Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT
    
[^31]: Jade:一种可微分的关节刚体物理引擎，具有无重叠摩擦接触.

    Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact. (arXiv:2309.04710v1 [cs.RO])

    [http://arxiv.org/abs/2309.04710](http://arxiv.org/abs/2309.04710)

    Jade是一种可微分的关节刚体物理引擎，具有无重叠摩擦接触的特点。它采用线性互补问题模型，实现了无重叠碰撞模拟和稳定的多个摩擦接触LCP解决方案。它还使用连续碰撞检测和回溯策略来防止物体重叠，并通过梯度计算和修改Dantzig算法来保证整个模拟过程的可微分性。大量实验证明了Jade在接触丰富任务中的有效性。

    

    本文介绍了Jade，一种用于关节刚体的可微分物理引擎。Jade将接触建模为线性互补问题（LCP）。与现有的可微分模拟相比，Jade具有无重叠碰撞模拟和多个摩擦接触的稳定LCP解决方案等特点。我们使用连续碰撞检测来检测碰撞时间，并采用回溯策略来防止复杂几何形状的物体之间的重叠。我们推导了梯度计算，以确保整个模拟过程在回溯机制下是可微分的。我们修改了常见的Dantzig算法，以获得在多个摩擦接触情况下的有效解决方案。我们进行了大量实验证明了我们的可微分物理模拟在各种接触丰富的任务中的有效性。

    We present Jade, a differentiable physics engine for articulated rigid bodies. Jade models contacts as the Linear Complementarity Problem (LCP). Compared to existing differentiable simulations, Jade offers features including intersection-free collision simulation and stable LCP solutions for multiple frictional contacts. We use continuous collision detection to detect the time of impact and adopt the backtracking strategy to prevent intersection between bodies with complex geometry shapes. We derive the gradient calculation to ensure the whole simulation process is differentiable under the backtracking mechanism. We modify the popular Dantzig algorithm to get valid solutions under multiple frictional contacts. We conduct extensive experiments to demonstrate the effectiveness of our differentiable physics simulation over a variety of contact-rich tasks.
    
[^32]: 从探索性视角解释代理行为的优势演员-评论员

    Advantage Actor-Critic with Reasoner: Explaining the Agent's Behavior from an Exploratory Perspective. (arXiv:2309.04707v1 [cs.AI])

    [http://arxiv.org/abs/2309.04707](http://arxiv.org/abs/2309.04707)

    本文提出了一种新颖的基于Reasoner的优势演员-评论员（A2CR）方法，通过预定义和分类演员行为的潜在目的，自动生成一个更全面和可解释的理解代理决策过程的范例。

    

    强化学习（RL）是解决复杂决策问题的强大工具，但它的缺乏透明度和解释性在决策具有实际后果的领域中一直是一个重要挑战。本文提出了一种新颖的基于Reasoner的优势演员-评论员（A2CR）方法，可以轻松应用于基于演员-评论员的RL模型，并使其具有可解释性。A2CR由三个相互连接的网络组成：策略网络，价值网络和Reasoner网络。通过预定义和分类演员行为的潜在目的，A2CR自动生成了一个更全面和可解释的理解代理决策过程的范例。它提供了诸如基于目的的显著性、早期失败检测和模型监管等一系列功能，从而促进负责任和可信赖的RL。在动作丰富的超级马里奥兄弟环境中进行的评估产生了有趣的发现：Reasoner-。

    Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems, but its lack of transparency and interpretability has been a major challenge in domains where decisions have significant real-world consequences. In this paper, we propose a novel Advantage Actor-Critic with Reasoner (A2CR), which can be easily applied to Actor-Critic-based RL models and make them interpretable. A2CR consists of three interconnected networks: the Policy Network, the Value Network, and the Reasoner Network. By predefining and classifying the underlying purpose of the actor's actions, A2CR automatically generates a more comprehensive and interpretable paradigm for understanding the agent's decision-making process. It offers a range of functionalities such as purpose-based saliency, early failure detection, and model supervision, thereby promoting responsible and trustworthy RL. Evaluations conducted in action-rich Super Mario Bros environments yield intriguing findings: Reasoner-
    
[^33]: 通过优化大型语言模型进行虚假信息和假新闻的检测分析

    Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])

    [http://arxiv.org/abs/2309.04704](http://arxiv.org/abs/2309.04704)

    本研究考虑使用LLM模型通过细调实现虚假信息和假新闻的深入分析，揭示复杂的风格和叙事，并提取命名实体的情感，以此作为监督机器学习模型中的预测性特征。

    

    本文考虑使用LLM（Llama 2大型语言模型）通过细调进行虚假信息分析和假新闻的检测。采用了基于PEFT/LoRA的细调方法。研究中，该模型对以下任务进行了细调：揭示虚假信息和宣传叙事的文本分析，事实核查，假新闻检测，操纵分析以及提取带有情感的命名实体。所得结果表明，经过细调的Llama 2模型能够对文本进行深入分析，并揭示复杂的风格和叙事。带有情感的命名实体可以作为监督机器学习模型中的预测性特征。

    The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.
    
[^34]: 上肢外骨骼的进展：使用前馈控制器实现主动重力补偿

    Advancements in Upper Body Exoskeleton: Implementing Active Gravity Compensation with a Feedforward Controller. (arXiv:2309.04698v1 [cs.RO])

    [http://arxiv.org/abs/2309.04698](http://arxiv.org/abs/2309.04698)

    该研究提出了一种用于上肢外骨骼的前馈控制系统，实现主动重力补偿。该系统利用内部传感器的位置数据计算力矩，并采用分析控制方程。与反馈控制系统相比，前馈控制具有更低的硬件复杂性和更积极的响应，提高了性能。实验结果表明该系统在保持稳定性能的同时减少了摩擦和不希望的偏转。

    

    在本研究中，我们提出了一种设计用于上肢外骨骼的主动重力补偿的前馈控制系统。该系统仅利用来自内部电机传感器的位置数据来计算力矩，采用基于牛顿-欧拉反向动力学的分析控制方程。与反馈控制系统相比，前馈方法具有几个优势。它消除了对外部力矩传感器的需求，从而减少了硬件复杂性和重量。此外，前馈控制具有更积极的响应，提高性能。实验中使用的外骨骼轻巧，由4个自由度组成，密切模拟了人体上肢运动学和三维活动范围。我们对外骨骼进行了硬件和模拟测试，证明了其稳定性能。系统在延长的时间内保持位置稳定，表现出最少的摩擦和避免不希望的偏转。

    In this study, we present a feedforward control system designed for active gravity compensation on an upper body exoskeleton. The system utilizes only positional data from internal motor sensors to calculate torque, employing analytical control equations based on Newton-Euler Inverse Dynamics. Compared to feedback control systems, the feedforward approach offers several advantages. It eliminates the need for external torque sensors, resulting in reduced hardware complexity and weight. Moreover, the feedforward control exhibits a more proactive response, leading to enhanced performance. The exoskeleton used in the experiments is lightweight and comprises 4 Degrees of Freedom, closely mimicking human upper body kinematics and three-dimensional range of motion. We conducted tests on both hardware and simulations of the exoskeleton, demonstrating stable performance. The system maintained its position over an extended period, exhibiting minimal friction and avoiding undesired slewing.
    
[^35]: 在上下文中学习编程风格以解决基于知识的问答中的问题

    Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])

    [http://arxiv.org/abs/2309.04695](http://arxiv.org/abs/2309.04695)

    本论文提出了一种在上下文中学习编程风格的方法，用于解决基于知识的问答中生成逻辑表达式的格式错误问题。

    

    目前，针对基于知识的问答(KBQA)的方法通常依赖复杂的训练技术和模型框架，导致在实际应用中存在许多限制。最近，大型语言模型(LLMs)中的上下文学习(ICL)能力的出现为KBQA提供了一种简单且无需训练的语义解析范式：给定少量问题及其标记的逻辑表达式作为演示示例，LLMs能够理解任务意图并为新问题生成逻辑表达式。然而，当前强大的LLMs在预训练过程中对逻辑表达式的了解很少，导致格式错误率较高。为了解决这个问题，我们提出了一种针对KBQA的代码风格上下文学习方法，将陌生逻辑表达式的生成过程转换为更为熟悉的代码生成过程。对三个主流数据集的实验结果表明，我们的方法显著减轻了生成逻辑表达式中的格式错误问题。

    Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for
    
[^36]: 灵活而健壮的具有最小满足扰动的对事实解释

    Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations. (arXiv:2309.04676v1 [cs.LG])

    [http://arxiv.org/abs/2309.04676](http://arxiv.org/abs/2309.04676)

    本论文提出了一种名为CEMSP的解决方案，通过限制异常特征的变化值来提供灵活且健壮的对事实解释。

    

    对事实解释（CFEs）展示了如何通过最小修改特征向量来实现不同实例的预测。CFEs可以增强信息公平性和可信度，并为收到不利预测的用户提供建议。然而，最近的研究表明，对于相同实例或稍有差异的实例，可以提供多个CFEs。多个CFEs为用户选择提供了灵活性和多样性的需求。然而，如果返回不稳定的CFEs且成本不同，将会损害个体公平性和模型可靠性。现有方法无法同时利用灵活性并解决非健壮性的问题。为了解决这些问题，我们提出了一个概念上简单而有效的解决方案，命名为具有最小满足扰动的对事实解释（CEMSP）。

    Counterfactual explanations (CFEs) exemplify how to minimally modify a feature vector to achieve a different prediction for an instance. CFEs can enhance informational fairness and trustworthiness, and provide suggestions for users who receive adverse predictions. However, recent research has shown that multiple CFEs can be offered for the same instance or instances with slight differences. Multiple CFEs provide flexible choices and cover diverse desiderata for user selection. However, individual fairness and model reliability will be damaged if unstable CFEs with different costs are returned. Existing methods fail to exploit flexibility and address the concerns of non-robustness simultaneously. To address these issues, we propose a conceptually simple yet effective solution named Counterfactual Explanations with Minimal Satisfiable Perturbations (CEMSP). Specifically, CEMSP constrains changing values of abnormal features with the help of their semantically meaningful normal ranges. Fo
    
[^37]: FIAT: 将学习范式与指令加速调优相融合

    FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])

    [http://arxiv.org/abs/2309.04663](http://arxiv.org/abs/2309.04663)

    FIAT是一种将上下文学习和完全微调范式融合的新的学习方式，可以在最大模型上进行指令和推理，并且在较小模型上进行参数更新，经过多语言任务测试，比之前的方法都表现更好。

    

    目前用于大型语言模型（LLMs）的学习范式通常分为上下文学习（ICL）和完全微调。每种范式都有其自身的取舍，这取决于可用数据、模型大小、计算成本、易用性和最终质量，但无法在所有情况下都表现良好。在本文中，我们首先以强调它们之间自然联系的方式描述了ICL和微调范式。基于这些联系，我们提出了一种名为FIAT的新学习范式，将这些范式的优点融合在一起，使得在最大模型上可以进行快速工程指令和链式思维推理，同时在参数效率调优的较小模型上使用类似的方法进行参数更新。我们在各种多语言任务上评估了FIAT的有效性，并观察到FIAT在100-10,000个训练样本规模下均比ICL和微调表现更好。我们希望FIAT能提供一种新的解决方案，使得在不同情况下都能取得更好的效果。

    Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called FIAT that fuses the best of these paradigms together, enabling prompt-engineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of multilingual tasks and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100-10,000 training examples. We hope that FIAT provides a pr
    
[^38]: 视频和合成磁共振成像预训练的3D视觉架构用于神经影像分析

    Video and Synthetic MRI Pre-training of 3D Vision Architectures for Neuroimage Analysis. (arXiv:2309.04651v1 [eess.IV])

    [http://arxiv.org/abs/2309.04651](http://arxiv.org/abs/2309.04651)

    本研究评估了不同的预训练方法对于3D医学影像任务的适用性，并发现预训练在改善任务性能方面具有积极影响。

    

    转移学习代表了我们构建人工智能系统的一种最新范式转变。与训练特定任务的模型不同，转移学习涉及在大型数据集上预训练深度学习模型，然后最小限度地微调它们以适应特定任务。然而，对于3D医学影像任务，我们不知道最佳的预训练方法是在自然图像、医学图像还是合成的MRI扫描或视频数据上。为了评估这些替代方法，我们在具有不同上游预训练方法的视觉转换器（ViTs）和卷积神经网络（CNNs）上进行了基准测试，并将其适应于三个独特的下游神经影像任务，难易程度各异：阿尔茨海默病（AD）和帕金森病（PD）分类，“脑龄”预测。实验测试得出以下重要观察结果：1.预训练改善了所有任务的性能，包括一个提升.

    Transfer learning represents a recent paradigm shift in the way we build artificial intelligence (AI) systems. In contrast to training task-specific models, transfer learning involves pre-training deep learning models on a large corpus of data and minimally fine-tuning them for adaptation to specific tasks. Even so, for 3D medical imaging tasks, we do not know if it is best to pre-train models on natural images, medical images, or even synthetically generated MRI scans or video data. To evaluate these alternatives, here we benchmarked vision transformers (ViTs) and convolutional neural networks (CNNs), initialized with varied upstream pre-training approaches. These methods were then adapted to three unique downstream neuroimaging tasks with a range of difficulty: Alzheimer's disease (AD) and Parkinson's disease (PD) classification, "brain age" prediction. Experimental tests led to the following key observations: 1. Pre-training improved performance across all tasks including a boost of
    
[^39]: 高效调优用于越南聊天机器人的大型语言模型

    Efficient Finetuning Large Language Models For Vietnamese Chatbot. (arXiv:2309.04646v1 [cs.CL])

    [http://arxiv.org/abs/2309.04646](http://arxiv.org/abs/2309.04646)

    本研究针对越南语聊天机器人的开发，通过利用来自Alpaca、GPT4All和Chat-Doctor等开源项目的大规模指令跟随数据集，成功训练了四个模型，此为越南语的首个指令数据集。

    

    大型语言模型（LLMs），如GPT-4、PaLM和LLaMa，在各种自然语言任务中表现出色。最近的指令调优进展使得LLMs能够按照用户指令并产生类似人类回复的能力。然而，训练和实现LLMs所需的高成本对学术研究提出了挑战。此外，越南语言的预训练LLMs和指令调谐数据集的可用性有限。为了解决这些问题，我们利用来自开源项目（Alpaca、GPT4All和Chat-Doctor）的大规模指令跟随数据集，涵盖了通用和特定的医学领域。据我们所知，这是第一个用于越南语的指令数据集。随后，我们利用参数高效调优，通过低秩适应（LoRA）在两个开放的LLMs上：Bloomz（多语言）和GPTJ-6B（越南语），得到四个模型：Bloomz-Chat，Blo

    Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown to achieve remarkable performance across a variety of natural language tasks. Recent advancements in instruction tuning bring LLMs with ability in following user's instructions and producing human-like responses. However, the high costs associated with training and implementing LLMs pose challenges to academic research. Furthermore, the availability of pretrained LLMs and instruction-tune datasets for Vietnamese language is limited. To tackle these concerns, we leverage large-scale instruction-following datasets from open-source projects, namely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and specific medical domain. To the best of our knowledge, these are the first instructional dataset for Vietnamese. Subsequently, we utilize parameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs: Bloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models: Bloomz-Chat, Blo
    
[^40]: 从示范中预训练触觉表示，实现力基动作的少样本学习

    Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation. (arXiv:2309.04640v1 [cs.RO])

    [http://arxiv.org/abs/2309.04640](http://arxiv.org/abs/2309.04640)

    该论文提出了一种能够从少量示范中学习力基动作的方法，通过对触觉表示进行预训练，并利用示范学习来训练动作生成器。实验证明，预训练显著提高了模型识别物理特性和生成所需动作的能力。

    

    在许多接触丰富的任务中，力传感在适应操纵对象的物理特性方面起着至关重要的作用。为了使机器人能够捕捉到学习的操纵任务泛化到未知对象所需的对象属性的基本分布，现有的示范学习方法需要大量昂贵的人类示范。我们提出的半监督示范学习方法将学习模型分解为触觉表示编码器和动作生成解码器。这使我们能够使用大量无监督数据预先训练触觉表示编码器，同时使用少样本示范学习训练动作生成解码器，从人类学习技能的好处。我们在使用不同刚度和表面摩擦的海绵上验证了该方法在擦拭任务中的效果。我们的结果表明，预训练显著提高了示范学习模型识别物理特性和生成所需擦拭动作的能力。

    In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wipin
    
[^41]: 低秩度度量学习中的感知调整查询和反向测量范式

    Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning. (arXiv:2309.04626v1 [stat.ML])

    [http://arxiv.org/abs/2309.04626](http://arxiv.org/abs/2309.04626)

    感知调整查询（PAQ）是一种新的用于收集人类反馈的查询机制，采用反向测量方案，结合了基数查询和序数查询的优点。我们将PAQ应用于度量学习问题中，通过PAQ测量来学习未知的马氏距离，并开发了一个两阶段估计器，提供了样本复杂性保证。

    

    我们引入了一种新的用于收集人类反馈的查询机制，称为感知调整查询（PAQ）。PAQ采用了反向测量方案，既具有信息量又轻量级，结合了基数查询和序数查询的优点。我们将PAQ展示在度量学习问题中，利用PAQ测量来学习未知的马氏距离。这导致了一个高维低秩矩阵估计问题，无法应用标准矩阵估计器。因此，我们开发了一个从PAQ中学习度量的两阶段估计器，并提供了该估计器的样本复杂性保证。我们通过数值模拟展示了该估计器的性能和显著特性。

    We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.
    
[^42]: 利用世界模型分解在基于值的多智能体强化学习中的应用

    Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning. (arXiv:2309.04615v1 [cs.LG])

    [http://arxiv.org/abs/2309.04615](http://arxiv.org/abs/2309.04615)

    本文提出了一种新的基于模型的多智能体强化学习方法，通过使用模块化的世界模型，减少了多智能体系统中训练的样本复杂性，并成功预测了联合动作价值函数。

    

    本文提出了一种新颖的基于模型的多智能体强化学习方法，名为Value Decomposition Framework with Disentangled World Model，旨在解决在相同环境中多个智能体达成共同目标时的样本复杂性问题。由于多智能体系统的可扩展性和非平稳性问题，无模型方法依赖于大量样本进行训练。相反地，我们使用模块化的世界模型，包括动作条件、无动作和静态分支，来解开环境动态并根据过去的经验产生想象中的结果，而不是直接从真实环境中采样。我们使用变分自动编码器和变分图自动编码器来学习世界模型的潜在表示，将其与基于值的框架合并，以预测联合动作价值函数并优化整体训练目标。我们提供实验结果。

    In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results 
    
[^43]: 用语义文本相似性链接症状清单

    Linking Symptom Inventories using Semantic Textual Similarity. (arXiv:2309.04607v1 [cs.CL])

    [http://arxiv.org/abs/2309.04607](http://arxiv.org/abs/2309.04607)

    该论文介绍了一种使用语义文本相似性（STS）来链接不同的症状清单的方法，通过测试预训练的STS模型在不同的数据源中预测症状严重程度，该方法在相关任务中达到了74.8%的准确率，优于其他模型。

    

    随着时间的推移，已经开发出了大量的症状清单来衡量临床症状，但这种多样性导致了几个长期存在的问题。最显著的是，来自不同环境和研究的结果不可比较，这限制了可重复性。在这里，我们提出了一种使用语义文本相似性（Semantic Textual Similarity，STS）来链接先前不相容的症状清单中的症状和评分的人工智能（AI）方法。我们测试了四个预训练的STS模型的能力，对来自16个国际数据源的6,607名参与者的四个不同清单中的数千个症状描述对进行相关内容的筛查 - 这通常是一个需要专家小组的具有挑战性的任务。模型的任务是预测六项任务中的四个不同清单中的症状严重程度。STS方法在五个任务中达到了74.8%的准确率，胜过了其他被测试的模型。这项工作表明，结合语境和语义信息可以帮助专家决策过程，从而产生收益。

    An extensive library of symptom inventories has been developed over time to measure clinical symptoms, but this variety has led to several long standing issues. Most notably, results drawn from different settings and studies are not comparable, which limits reproducibility. Here, we present an artificial intelligence (AI) approach using semantic textual similarity (STS) to link symptoms and scores across previously incongruous symptom inventories. We tested the ability of four pre-trained STS models to screen thousands of symptom description pairs for related content - a challenging task typically requiring expert panels. Models were tasked to predict symptom severity across four different inventories for 6,607 participants drawn from 16 international data sources. The STS approach achieved 74.8% accuracy across five tasks, outperforming other models tested. This work suggests that incorporating contextual, semantic information can assist expert decision-making processes, yielding gain
    
[^44]: EGOFALLS:一种使用自我中心摄像头进行摔倒检测的视听数据集和基准（arXiv:2309.04579v1 [cs.CV]）

    EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v1 [cs.CV])

    [http://arxiv.org/abs/2309.04579](http://arxiv.org/abs/2309.04579)

    这项研究提出了一种使用自我中心摄像头进行摔倒检测的方法，并构建了一个新的视听数据集。通过迟决策融合将音频和视觉信息相结合可以提高检测性能。

    

    对于脆弱人群，如老年人，摔倒往往是严重且常导致死亡的。以往的研究通过依赖单个传感器（图像或加速度计）捕捉数据来解决摔倒的检测问题。在本研究中，我们依赖于从自我中心摄像头捕捉的视频中提取的多模态描述符。我们提出的方法包括一个在提取的描述符之上构建的迟决策融合层。此外，我们还收集了一个新的数据集来评估我们提出的方法。这是我们认为的第一个公共同类数据集。该数据集包含14个受试者的10,948个视频样本。我们进行了消融实验以评估单个特征提取器的性能，视觉信息融合以及视觉和音频信息的融合。此外，我们还进行了内部和外部交叉验证的实验。我们的结果表明，通过迟决策融合将音频和视觉信息相结合可以提高检测性能。

    Falls are significant and often fatal for vulnerable populations such as the elderly. Previous works have addressed the detection of falls by relying on data capture by a single sensor, images or accelerometers. In this work, we rely on multimodal descriptors extracted from videos captured by egocentric cameras. Our proposed method includes a late decision fusion layer that builds on top of the extracted descriptors. Furthermore, we collect a new dataset on which we assess our proposed approach. We believe this is the first public dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects. We conducted ablation experiments to assess the performance of individual feature extractors, fusion of visual information, and fusion of both visual and audio information. Moreover, we experimented with internal and external cross-validation. Our results demonstrate that the fusion of audio and visual information through late decision fusion improves detection performance, making
    
[^45]: 解放图学习的力量：基于LLM的自主代理机制

    Unleashing the Power of Graph Learning through LLM-based Autonomous Agents. (arXiv:2309.04565v1 [cs.LG])

    [http://arxiv.org/abs/2309.04565](http://arxiv.org/abs/2309.04565)

    本文提出了一种使用大型语言模型（LLMs）作为自主代理的方法，以简化多样化的现实世界图中的学习过程，并克服了现有方法中的限制。

    

    图结构化数据在现实世界中广泛存在和应用，但有效地处理这些多样化的数据和在图上进行学习任务是一项挑战。面对复杂的图学习任务，专家们在近年来设计了各种图神经网络（GNN）。他们还实施了图中的自动机器学习，也称为AutoGraph，以自动生成数据特定的解决方案。尽管取得了成功，但他们在以下方面存在限制：（1）在不同层级上管理各种学习任务，（2）处理图学习中不同的流程（超过架构设计），以及（3）使用AutoGraph时对先验知识的巨大需求。本文中，我们提出使用大型语言模型（LLMs）作为自主代理来简化多样化的现实世界图中的学习过程。具体来说，针对用户请求（该请求可能包含节点、边缘或图级别的不同数据和学习目标），复杂图中的学习过程将由LLM自主代理机制来处理。

    Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph
    
[^46]: 连接NTK和NNGP：神经网络学习动力学在核区域的统一理论框架

    Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])

    [http://arxiv.org/abs/2309.04522](http://arxiv.org/abs/2309.04522)

    本文提出了一个马尔可夫近似学习模型，统一了神经切向核（NTK）和神经网络高斯过程（NNGP）核，用于描述无限宽度深层网络的学习动力学。

    

    人工神经网络近年来在机器学习领域取得了革命性的进展，但其学习过程缺乏一个完整的理论框架。对于无限宽度网络，已经取得了重大进展。在这个范式中，使用了两种不同的理论框架来描述网络的输出：一种基于神经切向核（NTK）的框架，假设了线性化的梯度下降动力学；另一种是基于神经网络高斯过程（NNGP）核的贝叶斯框架。然而，这两种框架之间的关系一直不明确。本文通过一个马尔可夫近似学习模型，统一了这两种不同的理论，用于描述随机初始化的无限宽度深层网络的学习动力学。我们推导出了在学习过程中和学习后的网络输入-输出函数的精确分析表达式，并引入了一个新的时间相关的神经动态核（NDK），这个核可以同时产生NTK和NNGP。

    Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
    
[^47]: 使用卷积变分瓶颈的隐私保护联合学习

    Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks. (arXiv:2309.04515v1 [cs.LG])

    [http://arxiv.org/abs/2309.04515](http://arxiv.org/abs/2309.04515)

    本文研究了使用卷积变分瓶颈的隐私保护联合学习，并发现了PRECODE的工作原理和影响。PRECODE通过引入随机性梯度防止梯度反转攻击的收敛，但也提出了一种攻击方法来禁用其隐私保护效果。

    

    梯度反转攻击是联合学习中普遍存在的威胁，它通过利用梯度泄漏来重构本应是私密的训练数据。最近的工作提出了一种基于变分建模的隐私增强模块（PRECODE），以防止梯度泄漏而不损失模型的效用。但在没有进一步的分析之前，已经证明PRECODE成功防止了梯度反转攻击。在本文中，我们做出了多个贡献。首先，我们研究了PRECODE对梯度反转攻击的影响，以揭示其基本的工作原理。我们证明了变分建模将随机性引入了PRECODE和神经网络中后续层的梯度中。这些层的随机梯度阻止了迭代梯度反转攻击的收敛。其次，我们提出了一种攻击方法，通过有意地在攻击优化过程中省略随机梯度，来禁用PRECODE的隐私保护效果。

    Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To
    
[^48]: 基于深度学习的脑部图像合成方法的系统综述

    Systematic Review of Techniques in Brain Image Synthesis using Deep Learning. (arXiv:2309.04511v1 [eess.IV])

    [http://arxiv.org/abs/2309.04511](http://arxiv.org/abs/2309.04511)

    本文是一篇关于深度学习在脑部图像合成中的技术综述，探讨了各种方法和技术，并强调了变压器在医学成像领域的潜在作用。

    

    本综述论文深入研究了医学成像的现状，特别关注了深度学习技术在脑部图像合成中的应用。强调了医学图像合成在提高诊断准确性和减少医疗程序的侵入性方面的需求，并阐述了深度学习在实现这些进展中的作用。本文讨论了各种脑部图像合成的方法和技术，包括2D到3D的构建、MRI合成以及变压器的使用。同时，还探讨了这些方法所面临的限制和挑战，如获取精心策划的训练数据和解决脑部超声问题。综述最后探讨了该领域的未来潜力以及利用深度学习技术在医学成像方面进一步发展的机会。强调了变压器的重要性及其在医学成像领域中的潜在革命性作用。此外，本文还讨论了潜在的解决方案。

    This review paper delves into the present state of medical imaging, with a specific focus on the use of deep learning techniques for brain image synthesis. The need for medical image synthesis to improve diagnostic accuracy and decrease invasiveness in medical procedures is emphasized, along with the role of deep learning in enabling these advancements. The paper examines various methods and techniques for brain image synthesis, including 2D to 3D constructions, MRI synthesis, and the use of transformers. It also addresses limitations and challenges faced in these methods, such as obtaining well-curated training data and addressing brain ultrasound issues. The review concludes by exploring the future potential of this field and the opportunities for further advancements in medical imaging using deep learning techniques. The significance of transformers and their potential to revolutionize the medical imaging field is highlighted. Additionally, the paper discusses the potential solution
    
[^49]: IoT空气污染监测系统中的时空图注意力融合器用于校准

    Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems. (arXiv:2309.04508v1 [cs.LG])

    [http://arxiv.org/abs/2309.04508](http://arxiv.org/abs/2309.04508)

    这项研究提出了一种新颖的方法，利用图神经网络中的图注意力网络模块，通过融合传感器阵列的数据来增强IoT空气污染监测平台中传感器的校准精度。

    

    随着物联网(IoT)传感器在空气污染监测中的广泛应用，低成本传感器的部署大幅增加。尽管这一进展，准确校准这些传感器在不受控制的环境条件下仍然是一个挑战。为了解决这个问题，我们提出了一种新颖的方法，利用图神经网络，特别是图注意力网络模块，通过融合传感器阵列的数据来增强校准过程。通过我们的实验，我们展示了我们的方法在显著提高IoT空气污染监测平台中传感器的校准精度方面的有效性。

    The use of Internet of Things (IoT) sensors for air pollution monitoring has significantly increased, resulting in the deployment of low-cost sensors. Despite this advancement, accurately calibrating these sensors in uncontrolled environmental conditions remains a challenge. To address this, we propose a novel approach that leverages graph neural networks, specifically the graph attention network module, to enhance the calibration process by fusing data from sensor arrays. Through our experiments, we demonstrate the effectiveness of our approach in significantly improving the calibration accuracy of sensors in IoT air pollution monitoring platforms.
    
[^50]: 使用强化学习进行基于视觉的概念组合学习

    Compositional Learning of Visually-Grounded Concepts Using Reinforcement. (arXiv:2309.04504v1 [cs.LG])

    [http://arxiv.org/abs/2309.04504](http://arxiv.org/abs/2309.04504)

    本研究探讨了深度强化学习代理如何学习和组合基于颜色和形状的组合指令，以解决空间导航任务中的新颖组合。通过利用冻结的文本编码器，代理所需的训练回合数减少了20倍。

    

    深度强化学习代理需要通过数百万个回合的训练才能较好地解决与指令相关的导航任务，并且它们是否能够推广到新颖的指令组合的能力尚不清楚。有趣的是，儿童可以分解基于语言的指令并导航到指定的物体，即使他们之前没有见过这些查询的组合。因此，我们创建了三个3D环境，研究深度强化学习代理如何学习和组合基于颜色和形状的组合指令，以解决空间导航任务中的新颖组合。首先，我们探讨代理是否能够进行组合学习，并且它们是否可以利用冻结的文本编码器（例如CLIP、BERT）在更少的回合中学习单词组合。接下来，我们证明当代理在形状或颜色概念上进行预训练时，它们所需的训练回合数减少了20倍，可以解决未见过的指令组合。最后，我们展示了...

    Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show tha
    
[^51]: 音乐演奏中的可解释人工智能(XAI)的情境敏感方法

    A Context-Sensitive Approach to XAI in Music Performance. (arXiv:2309.04491v1 [cs.HC])

    [http://arxiv.org/abs/2309.04491](http://arxiv.org/abs/2309.04491)

    这篇论文提出了一个针对音乐演奏中的可解释人工智能（XAI）的情境敏感方法，强调了上下文和受众在解释需求中的重要性，并通过针对特定受众并根据反馈不断改进解释来提高人工智能系统的透明度和可解释性。

    

    可解释人工智能（XAI）这一快速发展的领域引起了人们对于如何使人工智能系统更透明和可理解的方法的浓厚兴趣。然而，解释性的问题不能在摘要中得到详尽解决，因为没有一种单一的方法可以普遍适用于为任何给定的人工智能系统生成足够的解释，尤其是在艺术领域。在这篇立场论文中，我们提出了一个音乐演奏中XAI的解释实用主义（EP）框架，强调了上下文和受众在解释需求的发展中的重要性。通过将解释针对特定受众并根据反馈不断改进，EP为提高广泛的艺术应用和更具体地说是音乐演奏中的人工智能系统的透明度和可解释性提供了一个有希望的方向。

    The rapidly evolving field of Explainable Artificial Intelligence (XAI) has generated significant interest in developing methods to make AI systems more transparent and understandable. However, the problem of explainability cannot be exhaustively solved in the abstract, as there is no single approach that can be universally applied to generate adequate explanations for any given AI system, and this is especially true in the arts. In this position paper, we propose an Explanatory Pragmatism (EP) framework for XAI in music performance, emphasising the importance of context and audience in the development of explainability requirements. By tailoring explanations to specific audiences and continuously refining them based on feedback, EP offers a promising direction for enhancing the transparency and interpretability of AI systems in broad artistic applications and more specifically to music performance.
    
[^52]: 使用答案集编程对自主agents实施惩罚的框架

    Penalization Framework For Autonomous Agents Using Answer Set Programming. (arXiv:2309.04487v1 [cs.AI])

    [http://arxiv.org/abs/2309.04487](http://arxiv.org/abs/2309.04487)

    本文提出了一个框架，用于对智能agents实施惩罚，以保证其符合许可或义务政策。框架可以根据agents的遵从程度对其行为进行惩罚，并提供一个算法来选择具有最小总惩罚的计划。这一框架可以有效地惩罚不服从命令的agents。

    

    本文提出了一个框架，用于在变化环境中对不符合许可或义务政策的智能agents施加惩罚。提出了一种在计划中表示和推理惩罚的框架，并提出了一种算法来根据agents在授权和义务政策方面的遵从程度对其行为进行惩罚。通过意识到惩罚，agents可以选择一个具有最小总惩罚的计划，除非存在像救人一样的紧急目标。文章得出结论，该框架可以惩罚不服从命令的agents。

    This paper presents a framework for enforcing penalties on intelligent agents that do not comply with authorization or obligation policies in a changing environment. A framework is proposed to represent and reason about penalties in plans, and an algorithm is proposed to penalize an agent's actions based on their level of compliance with respect to authorization and obligation policies. Being aware of penalties an agent can choose a plan with a minimal total penalty, unless there is an emergency goal like saving a human's life. The paper concludes that this framework can reprimand insubordinate agents.
    
[^53]: 多模态机器学习在材料科学中的应用：基于成分-结构双模态学习的实验测量性质

    Multimodal machine learning for materials science: composition-structure bimodal learning for experimentally measured properties. (arXiv:2309.04478v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.04478](http://arxiv.org/abs/2309.04478)

    本论文介绍了一种基于成分-结构双模态学习的多模态机器学习方法，用于增强对实验测量材料性质的学习和预测，从而降低预测误差。

    

    广泛应用的GPT-4等多模态机器学习模型已经在计算机视觉和自然语言处理等多个研究领域引起了革命。然而，在材料信息学领域，尽管存在各种模态的材料数据，比如成分和结构，但其实际应用还未充分探索。大规模计算数据集训练的机器学习模型的有效性依赖于计算的准确性，而实验数据集往往数据有限且信息不完整。本文提出了一种新颖的材料科学中的多模态机器学习方法，通过成分-结构双模态学习来增强对实验测量材料性质的学习和预测。所提出的COmposition-Structure双模态网络（COSNet）旨在减少对具有不完整结构信息的材料性质的预测误差。

    The widespread application of multimodal machine learning models like GPT-4 has revolutionized various research fields including computer vision and natural language processing. However, its implementation in materials informatics remains underexplored, despite the presence of materials data across diverse modalities, such as composition and structure. The effectiveness of machine learning models trained on large calculated datasets depends on the accuracy of calculations, while experimental datasets often have limited data availability and incomplete information. This paper introduces a novel approach to multimodal machine learning in materials science via composition-structure bimodal learning. The proposed COmposition-Structure Bimodal Network (COSNet) is designed to enhance learning and predictions of experimentally measured materials properties that have incomplete structure information. Bimodal learning significantly reduces prediction errors across distinct materials properties 
    
[^54]: SayNav：将大型语言模型用于新环境中的动态规划导航

    SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments. (arXiv:2309.04077v1 [cs.RO])

    [http://arxiv.org/abs/2309.04077](http://arxiv.org/abs/2309.04077)

    SayNav是一种使用大型语言模型进行动态规划导航的方法，通过使用人类知识和场景图实现对复杂导航任务的高效泛化，动态生成指令并根据新信息不断完善未来步骤。

    

    语义推理和动态规划能力对于一个自主代理在未知环境中执行复杂导航任务至关重要。为了在这些任务中取得成功，需要大量的常识知识，这是人类所具备的。我们提出了SayNav，一种新的方法，利用来自大型语言模型（LLMs）的人类知识，以便高效地对未知大规模环境中的复杂导航任务进行泛化。SayNav使用一种新颖的接地机制，逐步构建一个探索环境的3D场景图，并将其作为LLMs的输入，用于生成可行且上下文适当的高层导航计划。然后，由预先训练的低层规划器执行LLM生成的计划，将每个计划的步骤视为短距离点目标导航子任务。SayNav在导航过程中动态生成一步一步的指令，并根据新获取的信息不断完善未来步骤。我们在一个新的多任务机验证环境上评估了SayNav。

    Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new mul
    
[^55]: 无需训练依然能获益：通过蒙特卡洛树搜索和能量函数引导实现大型语言模型的数学推理

    No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])

    [http://arxiv.org/abs/2309.03224](http://arxiv.org/abs/2309.03224)

    该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。

    

    大型语言模型（LLMs）展现出令人印象深刻的语言理解和背景学习能力，包括自然语言处理（NLP）任务和具有挑战性的数学推理。然而，由于缺乏过程监督，将PLMs应用于数学推理任务通常无法生成正确的推理步骤和最终答案，即使解决方案概率很高。为了在没有进一步的微调步骤的情况下发挥微调的LLMs的数学推理能力，我们提出了一种方法，通过蒙特卡洛树搜索（MCTS）和轻量级能量函数为LLMs赋予即时反应和精细推理系统。具体而言，我们首先将微调的LLMs重新定义为基于残差的能量模型（Residual-EBM），并应用噪声对比估计来估计能量函数的参数。然后，我们使用带有能量函数的MCTS作为路径验证器来搜索输出空间并评估推理路径。通过广泛的实验证明了我们方法的有效性。

    Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
    
[^56]: CodeApex：用于大型语言模型的双语编程评估基准

    CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01940](http://arxiv.org/abs/2309.01940)

    CodeApex是一个双语编程评估基准，用于评估大型语言模型在编程理解和代码生成任务上的能力。该基准包括多个选择题和算法问题，评估了14个LLM的编程能力，并发现仍有改进空间。

    

    随着大型语言模型（LLM）的出现，模型的编程能力得到了显著提升，吸引了研究人员日益增长的关注。我们提出了CodeApex，一种双语基准数据集，专注于LLM的编程理解和代码生成能力。CodeApex包括三种类型的多项选择题：概念理解、常识推理和多跳推理，旨在评估LLM在编程理解任务上的能力。此外，CodeApex利用算法问题和相应的测试用例来评估LLM生成的代码质量。我们评估了14个最先进的LLM，包括通用和专门化模型。GPT展现出最佳的编程能力，在这两个任务上的准确率分别达到了约50%和56%。编程任务仍有很大的改进空间。我们希望CodeApex能够为评估编程能力提供参考。

    With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the co
    
[^57]: 针对卷积神经网络的模型窃取攻击的高效防御方法

    Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks. (arXiv:2309.01838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01838](http://arxiv.org/abs/2309.01838)

    本文提出了一种简单但有效且高效的防御替代方案，引入一种启发式方法来扰动输出概率，可以轻松集成到模型中而无需额外训练，并且在抵御最先进的窃取攻击中表现出有效性。

    

    模型窃取攻击对深度学习模型构成了严重威胁，攻击者可以通过查询其黑盒API来窃取已训练的模型。这可能导致知识产权盗窃和其他安全与隐私风险。目前针对模型窃取攻击的最先进防御方法建议向预测概率添加扰动，但其计算较重且对攻击者提出了不切实际的假设。通常需要训练辅助模型，这可能耗时且资源密集，妨碍了该防御方法在实际应用中的部署。本文提出了一种简单但有效且高效的防御替代方案。我们引入一种启发式方法来扰动输出概率。该防御方法可以轻松集成到模型中而无需额外训练。我们展示了我们的防御方法在抵御三种最先进的窃取攻击中的有效性。我们进行了评估。

    Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate
    
[^58]: 在不平衡的研究提案主题推理中的跨学科公平性：一种基于层次变换器的具有选择性插值的方法

    Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation. (arXiv:2309.01717v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01717](http://arxiv.org/abs/2309.01717)

    该论文提出了一种基于层次变换器的方法，通过选择性插值来解决在跨学科研究提案和非跨学科研究提案之间规模差异引起的不公平现象。

    

    研究提案主题推理的目标是从资助机构定义的学科体系中获取最合适的学科划分，然后机构将根据这种划分从其数据库中找到合适的同行评审专家。自动化的主题推理可以减少人工主题填写引起的错误，弥补资助机构和项目申请人之间的知识差距，提高系统效率。现有方法将其建模为层次性多标签分类问题，使用生成模型迭代地推理最合适的主题信息。然而，这些方法忽视了跨学科研究提案和非跨学科研究提案之间规模差异，导致自动推理系统将跨学科提案归类为非跨学科，造成在专家分配过程中的不公平现象。我们如何解决这个数据不平衡的问题呢？

    The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue und
    
[^59]: 大尺度和无穷宽度下的深度学习勒让演讲

    Les Houches Lectures on Deep Learning at Large & Infinite Width. (arXiv:2309.01592v1 [stat.ML])

    [http://arxiv.org/abs/2309.01592](http://arxiv.org/abs/2309.01592)

    本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。

    

    这些演讲是在2022年勒让夏季学校统计物理和机器学习课程上展示的，着重探讨了深度神经网络在无限宽度和大宽度范围内的情况。涵盖的主题包括这些网络的各种统计和动力学特性。特别是，讲师们讨论了随机深度神经网络的特性；训练过的深度神经网络，线性模型，核函数和高斯过程之间的联系，这些联系在无穷宽度的极限下出现；以及在初始化和训练后对大但有限宽度网络的摄动和非摄动处理。

    These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
    
[^60]: 条件生存预测中的面积规范COBRA

    Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])

    [http://arxiv.org/abs/2309.00417](http://arxiv.org/abs/2309.00417)

    本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。

    

    本文探讨了一种不同的组合回归策略来计算条件生存函数。我们使用基于回归的弱学习器来创建所提出的集成技术。所提出的组合回归策略使用相似度度量作为两个生存曲线之间的面积。所提出的模型表明其表现优于随机生存森林。本文讨论了一种在组合回归设置中选择最重要变量的新技术。我们进行了一项模拟研究，表明我们对变量相关性的提议效果很好。我们还使用了三个真实数据集来说明该模型。

    The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
    
[^61]: 基于神经预测的零样本NAS范式的有效性

    Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])

    [http://arxiv.org/abs/2308.16775](http://arxiv.org/abs/2308.16775)

    这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。

    

    在基于预测的神经架构搜索（NAS）中，通过图卷积网络得到的性能指标取得了显著的成功。然而，通过one-hot编码将前馈结构表示为组件图的这些指标面临一个限制：无法在不同的搜索空间中评估架构的性能。相反，手工性能指标（零样本NAS）可以在多个搜索空间中泛化，因为它们使用相同的架构和随机初始化。为了解决这个限制，我们提出了一种新的深度学习方法，用于零样本NAS。我们的方法采用傅里叶正弦和求和编码来进行卷积核的编码，从而构建了一个计算的前馈图，其结构类似于正在评估的架构。这些编码是可学习的，并提供了架构拓扑信息的全面视图。然后，伴随的多层感知器（MLP）对架构进行排序。

    In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
    
[^62]: 国际民用人工智能治理: 一种管辖认证方法

    International Governance of Civilian AI: A Jurisdictional Certification Approach. (arXiv:2308.15514v1 [cs.AI])

    [http://arxiv.org/abs/2308.15514](http://arxiv.org/abs/2308.15514)

    这项研究提出了一种国际治理民用人工智能的方法，通过建立国际人工智能组织来认证国家的管辖区域是否符合国际监督标准，进而禁止从未经认证的管辖区域进口AI供应链所包含产品。

    

    本报告描述了国际民用人工智能（AI）治理安排设计中的权衡，并详细介绍了一种方法。该方法将一个标准、许可和责任制度扩展到全球范围。我们建议各国建立一个国际人工智能组织（IAIO）来认证国家管辖区域（而不是公司或AI项目）是否符合国际监督标准。各国可以通过采用禁止从未经IAIO认证的管辖区域进口AI供应链所包含产品的法规来实施这些国际标准。这一方法借鉴了现有国际组织模式，如国际民用航空组织（ICAO）、国际海事组织（IMO）和金融行动特别工作组（FATF）。各国还可以对非认证国家采取多边控制措施，例如对AI产品输入（如专用硬件）的出口。

    This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified
    
[^63]: 让LLM能够使用智能手机进行智能任务自动化

    Empowering LLM to use Smartphone for Intelligent Task Automation. (arXiv:2308.15272v1 [cs.AI])

    [http://arxiv.org/abs/2308.15272](http://arxiv.org/abs/2308.15272)

    本论文提出了AutoDroid，一个移动任务自动化系统，可以在任何Android应用程序上自动处理任意任务。它通过结合LLMs的常识知识和应用的领域特定知识来实现，通过自动化的动态分析来实现功能意识的UI表示方法和基于探索的内存注入技术。

    

    移动任务自动化是一种吸引人的技术，旨在实现基于语音的免提用户与智能手机的交互。然而，现有的方法由于语言理解能力有限，以及开发人员或终端用户需要付出非常努力的手动工作而导致可扩展性差。最近大型语言模型（LLMs）在语言理解和推理方面的进展激发了我们从模型中心化的角度重新思考这个问题，即通过统一的语言模型处理任务准备、理解和执行。在这项工作中，我们介绍了AutoDroid，这是一个能够在任何Android应用程序上无需手动工作处理任意任务的移动任务自动化系统。关键洞察力是通过自动化的动态分析将LLMs的常识知识与应用的领域特定知识相结合。主要组件包括功能意识的UI表示方法，桥接了UI和LLM，基于探索的内存注入技术

    Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection t
    
[^64]: ExpCLIP: 通过语义对齐将文本和面部表情融合

    ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment. (arXiv:2308.14448v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.14448](http://arxiv.org/abs/2308.14448)

    本研究介绍了ExpCLIP这一技术，通过语义对齐将文本和面部表情融合，使得风格化语音驱动的面部动画具有灵活性和用户友好性。

    

    风格化语音驱动的面部动画的目标是创建包含特定情绪表达的动画。现有方法通常依赖预先设定的情绪标签或面部表情模板，这可能限制准确传达用户意图所必需的灵活性。在这项研究中，我们引入了一种通过利用自然语言作为情绪提示来控制任意风格的技术。这种技术在灵活性和用户友好性方面具有优势。为了实现这个目标，我们首先构建了一个文本-表情对齐数据集（TEAD），其中每个面部表情都与几个类似提示的描述配对。我们提出了一种创新的自动注释方法，支持大型语言模型（LLM），以加快数据集构建速度，从而消除手动注释的大量费用。在此之后，我们利用TEAD来训练一个基于CLIP的模型，称为ExpCLIP，它对文本和面部表达进行编码。

    The objective of stylized speech-driven facial animation is to create animations that encapsulate specific emotional expressions. Existing methods often depend on pre-established emotional labels or facial expression templates, which may limit the necessary flexibility for accurately conveying user intent. In this research, we introduce a technique that enables the control of arbitrary styles by leveraging natural language as emotion prompts. This technique presents benefits in terms of both flexibility and user-friendliness. To realize this objective, we initially construct a Text-Expression Alignment Dataset (TEAD), wherein each facial expression is paired with several prompt-like descriptions.We propose an innovative automatic annotation method, supported by Large Language Models (LLMs), to expedite the dataset construction, thereby eliminating the substantial expense of manual annotation. Following this, we utilize TEAD to train a CLIP-based model, termed ExpCLIP, which encodes tex
    
[^65]: 探索大型语言模型用于知识图谱补全

    Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])

    [http://arxiv.org/abs/2308.13916](http://arxiv.org/abs/2308.13916)

    本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。

    

    知识图谱在众多人工智能任务中发挥着重要作用，但经常面临不完整性的问题。在本研究中，我们探索了利用大型语言模型（LLM）进行知识图谱补全的方法。我们将知识图谱中的三元组视为文本序列，并引入了一种创新的框架，称为知识图谱LLM（KG-LLM），来对这些三元组进行建模。我们的技术利用三元组的实体和关系描述作为提示，并利用响应进行预测。对各种基准知识图谱的实验表明，我们的方法在三元组分类和关系预测等任务中达到了最先进的性能。我们还发现，微调相对较小的模型（例如LLaMA-7B，ChatGLM-6B）优于最新的ChatGPT和GPT-4。

    Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
    
[^66]: 工业人工智能中的随机配置机

    Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])

    [http://arxiv.org/abs/2308.13570](http://arxiv.org/abs/2308.13570)

    本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。

    

    在工业人工智能（IAI）中，需要实时、准确的预测建模，神经网络在其中起到关键作用。工业人工智能中的神经网络需要强大的高性能计算设备来处理大量的浮点数据。本文基于随机配置网络（SCNs），提出了一种新的随机学习器模型，称为随机配置机（SCMs），以强调对于工业应用非常有用和有价值的有效建模和节约数据大小。与具有二值化实现的随机向量功能链接（RVFL）网络相比，SCMs的模型存储可以显著压缩，同时保持有利的预测性能。除了SCM学习器模型的架构和学习算法，作为本文的重要部分，我们还通过分析模型的复杂性提供了SCMs的学习能力的理论基础。实验研究也进行了。

    Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
    
[^67]: MLLM-DataEngine：一种MLLM的迭代改进方法

    MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])

    [http://arxiv.org/abs/2308.13566](http://arxiv.org/abs/2308.13566)

    本文提出了一种名为MLLM-DataEngine的迭代改进方法，它通过分析模型弱点，生成适当的增量数据集并迭代地增强模型能力。与以往方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面表现更好。

    

    尽管在指导数据集构建和基准测试方面，多模态大型语言模型（MLLM）取得了很大的进展，但训练和评估的独立性使得当前的MLLM很难在相对较低的人力成本下进一步提高其能力。本文提出了一种新颖的封闭循环系统MLLM-DataEngine，它连接了数据生成、模型训练和评估。在每个循环迭代中，MLLM-DataEngine首先根据评估结果分析模型的弱点，然后生成合适的增量数据集用于下一次训练迭代，并迭代地增强模型的能力。与先前与基准测试分离的数据收集方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面都表现得更好。

    Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
    
[^68]: 使用精细调整的Llama 2 GPT模型进行金融新闻分析

    Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])

    [http://arxiv.org/abs/2308.13032](http://arxiv.org/abs/2308.13032)

    本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。

    

    本文考虑了使用精细调整的Llama 2 Large Language Model (LLM) 对金融新闻进行多任务分析的可能性。通过PEFT/LoRA方法对模型进行精细调整，主要包括从金融市场角度分析文本、突出文本的主要观点、对文本进行摘要和提取具有适当情感的命名实体等任务。实验结果表明，经过精细调整的Llama 2模型能够进行多任务的金融新闻分析，其响应的结构可以部分为结构化文本，另一部分数据可以采用JSON格式进一步处理。提取的命名实体情感可以被视为具有定量目标变量的监督机器学习模型的预测特征。

    The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
    
[^69]: VIGC: 视觉指令生成与纠正

    VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])

    [http://arxiv.org/abs/2308.12714](http://arxiv.org/abs/2308.12714)

    本文提出了VIGC框架，使多模态大型语言模型能够生成和纠正视觉指令数据，解决了缺乏高质量调整数据的挑战。

    

    视觉编码器和大型语言模型（LLMs）的整合推动了多模态大型语言模型（MLLMs）的最新进展。然而，针对视觉语言任务的高质量指令调整数据的稀缺仍然是一个挑战。当前的主导范式，如LLaVA，依赖于仅使用语言的GPT-4生成数据，这需要预注释的图像标题和检测包围框，导致对图像细节的理解不足。解决这个问题的一个实际方案是利用可用的多模态大型语言模型（MLLMs）生成视觉语言任务的指令数据。然而，值得注意的是，当前可访问的MLLMs不像它们的LLM对应物那样强大，因为它们往往产生不适当的回应和生成错误信息。作为解决当前问题的方案，本文提出了Visual Instruction Generation and Correction（VIGC）框架，使多模态大型语言模型能够生成视觉指令数据并纠正错误。

    The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to ge
    
[^70]: Blending-NeRF：基于文本驱动的神经辐射场局部编辑

    Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields. (arXiv:2308.11974v1 [cs.CV])

    [http://arxiv.org/abs/2308.11974](http://arxiv.org/abs/2308.11974)

    Blending-NeRF是一种基于NeRF的模型，通过使用文本提示来实现局部编辑，能够在不扭曲对象形状的情况下，混合原始对象和目标对象并添加风格效果。该模型利用预训练的视觉-语言对齐模型CLIP进行指导，并成功地实现了添加新对象、修改纹理和移除原始对象部分的功能。

    

    基于文本驱动的3D对象的局部编辑是一项特别困难的任务，因为在不扭曲对象形状的情况下，局部混合原始3D对象与目标新对象和风格效果并不是一种直接的过程。为了解决这个问题，我们提出了一种新的基于NeRF的模型——Blending-NeRF，它由两个NeRF网络组成：预训练的NeRF和可编辑的NeRF。此外，我们引入了新的混合操作，使Blending-NeRF能够正确地编辑由文本定位的目标区域。通过使用预训练的视觉-语言对齐模型CLIP，我们引导Blending-NeRF添加具有不同颜色和密度的新对象，修改纹理，并移除原始对象的部分。我们的大量实验证明，Blending-NeRF可以根据不同的文本提示产生自然且局部编辑的3D对象。

    Text-driven localized editing of 3D objects is particularly difficult as locally mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.
    
[^71]: 一个有效的基于Transformer的上下文模型和时间门池化用于说话人识别

    An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])

    [http://arxiv.org/abs/2308.11241](http://arxiv.org/abs/2308.11241)

    本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。

    

    Wav2vec2在语音识别中应用Transformer架构和自监督学习取得了成功。最近，这些方法不仅用于语音识别，还用于整个语音处理。本文介绍了一种应用了基于Transformer的上下文模型的有效端到端说话人识别模型。我们探索了参数与性能之间的关系，以确定一个有效模型的结构。此外，我们提出了一种具有强大学习能力的池化方法，称为时间门池化(Temporal Gate Pooling)，用于说话人识别。我们将Conformer作为编码器，并利用BEST-RQ进行预训练，并使用VoxCeleb1的说话人识别进行了评估。该方法在仅有28.5M个参数的情况下，实现了85.9%的准确率，与具有317.7M个参数的wav2vec2相当。代码可在https://github.com/HarunoriKawano/speaker-identification-with-tgp获得。

    Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
    
[^72]: 在图上评估大型语言模型：性能洞察与比较分析

    Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])

    [http://arxiv.org/abs/2308.11224](http://arxiv.org/abs/2308.11224)

    本研究评估了四个大型语言模型在图数据上解决分析问题的能力，结果显示LLM在理解图数据、生成正确结果和进行结构推理方面表现出色，但在真实性和矫正能力方面存在一些挑战。

    

    大型语言模型(LLM)引起了学术界和工业界的广泛关注，然而LLM在图数据上的应用仍然未被充分探索。在本研究中，我们评估了四个LLM在解决几个图数据分析问题时的能力。我们采用了四个不同的评估指标：理解能力、正确性、真实性和矫正能力。我们的结果表明：1) LLM能够有效地理解自然语言中的图数据，并推理图的拓扑结构。2) GPT模型能够生成逻辑和连贯的结果，在正确性方面优于其他替代方案。3) 所有被检测的LLM在结构推理方面都面临挑战，零样本思维链和少样本提示等技术显示出效果下降。4) GPT模型在多答案任务中经常产生错误答案，引发真实性方面的担忧。5) GPT模型对其输出表现出较高的信心，可能阻碍其矫正能力。值得注意的是，GPT-4显示出了不同水平的性能。

    Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has dem
    
[^73]: PokerKit: 一种用于细粒度多变体扑克游戏模拟的全面的Python库

    PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])

    [http://arxiv.org/abs/2308.07327](http://arxiv.org/abs/2308.07327)

    PokerKit是一个全面的Python库，用于细粒度多变体扑克游戏模拟，提供广泛的扑克变体支持和灵活的游戏状态控制，对扑克AI开发、工具创建和在线扑克赌场实现等领域具有重要贡献。

    

    PokerKit是一个开源的Python库，旨在克服现有扑克游戏模拟和手牌评估工具的限制，这些工具通常只支持少量扑克变体，并且在游戏状态控制方面缺乏灵活性。相比之下，PokerKit通过支持广泛的扑克变体，并提供灵活的架构供用户定义自定义游戏，显著扩大了这一范围。本文详细介绍了PokerKit的设计和实现，包括其直观的编程API，多变体游戏支持以及统一的手牌评估套件在不同手牌类型间的应用。PokerKit的灵活性使其能够在扑克AI开发、工具创建和在线扑克赌场实现等多个领域中使用。PokerKit的可靠性通过静态类型检查、广泛的doctest和单元测试来确保，达到了97%的代码覆盖率。引入PokerKit代表了对该领域的重要贡献。

    PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
    
[^74]: AudioLDM 2: 利用自监督预训练学习进行整体音频生成的方法

    AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (arXiv:2308.05734v1 [cs.SD])

    [http://arxiv.org/abs/2308.05734](http://arxiv.org/abs/2308.05734)

    本文提出了一种利用自监督预训练学习方法进行语音、音乐和音效生成的框架，通过引入通用音频表示LOA，将任何音频转换为LOA，并利用以LOA为条件的潜在扩散模型进行自监督音频生成学习。

    

    虽然音频生成在不同类型的音频中共享一些共性，比如语音、音乐和音效，但为每种类型设计模型需要仔细考虑特定的目标和偏差，这些偏差可能与其他类型的目标有显著的差异。为了更好地实现音频生成的统一视角，本文提出了一种利用相同的学习方法进行语音、音乐和音效生成的框架。我们的框架引入了一种称为“语言音频（LOA）”的音频通用表示。任何音频都可以基于自监督预训练学习模型AudioMAE转换为LOA。在生成过程中，我们使用GPT-2模型将任何形式的音频转换为LOA，并利用以LOA为条件的潜在扩散模型进行自监督音频生成学习。所提出的框架自然地带来了诸如上下文学习能力和可重用的自监督预训练AudioMAE的优势。

    Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE
    
[^75]: ChatGPT生物医学生成文本中建立信任的方法：基于本体的知识图谱用于验证疾病-症状关系

    Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])

    [http://arxiv.org/abs/2308.03929](http://arxiv.org/abs/2308.03929)

    本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。

    

    方法：通过创新的方法，我们从真实的医学文献和人工智能生成的内容构建了基于本体的知识图谱。我们的目标是区分事实信息和未经验证的数据。我们收集了两个数据集：一个是使用“人类疾病和症状”查询从生物医学文献中编译的，另一个是由ChatGPT生成的模拟文章。利用这些数据集（PubMed和ChatGPT），我们随机选择了10组每组250个摘要，并使用特定的种子。我们的方法主要是利用疾病本体（DOID）和症状本体（SYMP）构建知识图谱，这是一种强大的数学模型，可以进行无偏差的比较。通过使用我们的事实核查算法和网络中心度指标，我们进行了GPT疾病-症状链接分析，以量化在噪声、假设和重要发现中的事实知识的准确性。结果：通过比较不同ChatGPT知识图谱及其PubMed计数获得的结果，我们发现...

    Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
    
[^76]: Select2Col: 利用语义信息的时空重要性进行高效的协作感知

    Select2Col: Leveraging Spatial-Temporal Importance of Semantic Information for Efficient Collaborative Perception. (arXiv:2307.16517v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.16517](http://arxiv.org/abs/2307.16517)

    Select2Col是一种利用语义信息的时空重要性进行高效协作感知的新框架。它通过轻量级图神经网络估计语义信息的重要性，从而选择有益的合作者并排除负面影响。同时，还提出了一种语义信息融合算法。

    

    通过利用共享的语义信息进行协作在克服孤立代理的感知能力限制方面起着关键作用。然而，现有的协作感知方法往往只关注语义信息的空间特征，而忽视了时间维度的重要性。因此，协作的潜在益处未得到充分利用。在本文中，我们提出了Select2Col，一种新颖的协作感知框架，它考虑了语义信息的时空重要性。在Select2Col中，我们开发了一种利用轻量级图神经网络（GNN）估计语义信息重要性（IoSI）以提高感知性能的合作者选择方法，从而识别出有益的合作者，同时排除那些带来负面影响的合作者。此外，我们提出了一种称为HPHA的语义信息融合算法，其中包括历史先验混合注意力机制。

    Collaboration by leveraging the shared semantic information plays a crucial role in overcoming the perception capability limitations of isolated agents. However, existing collaborative perception methods tend to focus solely on the spatial features of semantic information, while neglecting the importance of the temporal dimension. Consequently, the potential benefits of collaboration remain underutilized. In this article, we propose Select2Col, a novel collaborative perception framework that takes into account the {s}patial-t{e}mpora{l} importanc{e} of semanti{c} informa{t}ion. Within the Select2Col, we develop a collaborator selection method that utilizes a lightweight graph neural network (GNN) to estimate the importance of semantic information (IoSI) in enhancing perception performance, thereby identifying contributive collaborators while excluding those that bring negative impact. Moreover, we present a semantic information fusion algorithm called HPHA (historical prior hybrid atte
    
[^77]: UniBriVL: 强大的音频驱动扩散模型的通用表示和生成

    UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models. (arXiv:2307.15898v1 [cs.SD])

    [http://arxiv.org/abs/2307.15898](http://arxiv.org/abs/2307.15898)

    本文提出了一种名为UniBriVL的新型通用语言表示学习方法，该方法实现了音频驱动的扩散模型的生成。它能够稳健地学习语言表示，并捕捉到音频和图像之间的关联。实验结果表明UniBriVL在下游任务中表现出良好的效果，能够从音频中选择合适的图像。

    

    多模态大型模型因其在各种性能和下游任务中的优势而被认为是至关重要的。这些模型的发展对于未来实现通用人工智能至关重要。在本文中，我们提出了一种新颖的通用语言表示学习方法，称为UniBriVL，它基于Bridging-Vision-and-Language（BriVL）。通用BriVL将音频、图像和文本嵌入到一个共享空间中，实现了各种多模态应用的实现。我们的方法解决了稳健的语言（包括文本和音频）表示学习的主要挑战，并有效地捕捉到音频和图像之间的关联。此外，我们展示了从UniBriVL生成的图像的定性评估，这突出了我们的方法在从音频中创建图像方面的潜力。总体而言，我们的实验结果证明了UniBriVL在下游任务中的有效性以及其从音频中选择适当图像的能力。

    Multimodal large models have been recognized for their advantages in various performance and downstream tasks. The development of these models is crucial towards achieving general artificial intelligence in the future. In this paper, we propose a novel universal language representation learning method called UniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal BriVL embeds audio, image, and text into a shared space, enabling the realization of various multimodal applications. Our approach addresses major challenges in robust language (both text and audio) representation learning and effectively captures the correlation between audio and image. Additionally, we demonstrate the qualitative evaluation of the generated images from UniBriVL, which serves to highlight the potential of our approach in creating images from audio. Overall, our experimental results demonstrate the efficacy of UniBriVL in downstream tasks and its ability to choose appropriate images from au
    
[^78]: 从概率编程到基于复杂性的编程

    From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])

    [http://arxiv.org/abs/2307.15453](http://arxiv.org/abs/2307.15453)

    CompLog是一种基于复杂性的计算框架，通过计算Kolmogorov复杂性替代概率推理，实现计算某种情况意外性的度量，并通过规范的世界和心智模型的描述生成相关描述，并提供对析取和否定的替代方法。

    

    本文介绍了一种名为CompLog的新型计算框架的主要特点和初步实现。CompLog借鉴了概率编程系统（如ProbLog）的推理机制，并基于Simplicity理论提出了一种新的推理机制，通过ASP程序的min-path搜索计算两种Kolmogorov复杂性，而不是概率推理。该系统使用户能够计算某个情况意外性的ex-post和ex-ante度量，分别对应于后验和先验主观概率。计算基于通过描述性谓词之间的因果和描述性关系加权的世界和心智模型的规范。本文还阐述了几个应用示例：生成相关描述，并提供对析取和否定的替代方法。

    The paper presents the main characteristics and a preliminary implementation of a novel computational framework named CompLog. Inspired by probabilistic programming systems like ProbLog, CompLog builds upon the inferential mechanisms proposed by Simplicity Theory, relying on the computation of two Kolmogorov complexities (here implemented as min-path searches via ASP programs) rather than probabilistic inference. The proposed system enables users to compute ex-post and ex-ante measures of unexpectedness of a certain situation, mapping respectively to posterior and prior subjective probabilities. The computation is based on the specification of world and mental models by means of causal and descriptive relations between predicates weighted by complexity. The paper illustrates a few examples of application: generating relevant descriptions, and providing alternative approaches to disjunction and to negation.
    
[^79]: 从人类反馈中进行强化学习的开放问题和基本限制

    Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])

    [http://arxiv.org/abs/2307.15217](http://arxiv.org/abs/2307.15217)

    本文调查了从人类反馈中进行强化学习的开放问题和基本限制，并提出了加强社会监督的审计和披露标准。

    

    从人类反馈中进行强化学习（RLHF）是一种训练人工智能系统与人类目标保持一致的技术。RLHF已成为微调最新的大型语言模型（LLM）的核心方法。尽管如此受欢迎，但系统性地系统化其缺陷的公开工作相对较少。在本文中，我们（1）调查了RLHF及相关方法的开放问题和基本限制；（2）概述了了解、改进和补充RLHF的实践技术；以及（3）提出了审计和披露标准以改进RLHF系统的社会监督。我们的工作强调了RLHF的局限性，并强调了以多方面方法开发更安全的人工智能系统的重要性。

    Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
    
[^80]: 在金融行业中应用量子自然语言处理(QNLP)进行情感分析

    Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])

    [http://arxiv.org/abs/2307.11788](http://arxiv.org/abs/2307.11788)

    本论文研究了在金融行业中应用量子自然语言处理(QNLP)进行情感分析的实际适用性。利用一种新颖的数据生成方法，我们发现量子增强的长短期记忆(QLSTM)可以更快地训练，并且在软件实现方面接近古典结果。

    

    作为一个领域，即使是最微小的质量改进也能产生巨大价值的应用领域，金融是早期量子优势的有前途的候选者。在迅速发展的量子自然语言处理(QNLP)领域中，我们探索了DisCoCat和量子增强的长短期记忆(QNLP)这两种中心方法在金融情感分析问题中的实际适用性。利用一种新颖的基于ChatGPT的数据生成方法，我们进行了一个包含1000多个真实句子的案例研究，发现QLSTM的训练速度比DisCoCat快得多，并且在可用的软件实现中也接近古典结果。

    As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
    
[^81]: 使用PIP-Net解释和纠正医学图像分类

    Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])

    [http://arxiv.org/abs/2307.10404](http://arxiv.org/abs/2307.10404)

    本研究利用PIP-Net开展了可解释的机器学习技术在医学图像分类中的应用，并展示了其在骨折检测和皮肤癌诊断方面的准确性和可解释性。通过无监督的预训练，PIP-Net能够轻松识别数据质量问题，并且我们还发现人们可以通过手动禁用不良原型来纠正PIP-Net的推理过程。

    

    部分原型模型是可解释性的图像分类器，是黑盒人工智能的有希望的替代方案。本文探讨了可解释性机器学习的适用性和潜力，特别是对于真实世界的医学成像数据的自动诊断支持。PIP-Net学习人类可理解的典型图像部分，并评估其在骨折检测和皮肤癌诊断方面的准确性和可解释性。我们发现PIP-Net的决策过程符合医学分类标准，仅提供图像级别的类标签。由于PIP-Net对原型进行了无监督的预训练，因此可以轻松识别X光中的不良文本或标签错误等数据质量问题。此外，我们是第一个显示人们可以通过直接禁用不良原型来手动纠正PIP-Net的推理过程。我们得出结论，部分原型模型对医学应用具有潜力，因为它们具有相互参考性。

    Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their inter
    
[^82]: 创建一个支持OpenMP Fortran和C++代码相互翻译的数据集

    Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])

    [http://arxiv.org/abs/2307.07686](http://arxiv.org/abs/2307.07686)

    本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。

    

    在本研究中，我们提出了一个新颖的数据集，用于训练在OpenMP Fortran和C++代码之间进行翻译的机器学习模型。通过精细的代码相似性测试，我们确保了数据集的可靠性和适用性。我们使用定量（CodeBLEU）和定性（人工评估）方法评估了我们数据集的有效性。我们展示了这个数据集如何显著提高大规模语言模型的翻译能力，对于没有先前编码知识的模型，提高了5.1倍，对于具有一定编码熟悉度的模型，提高了9.9倍。我们的工作突显了这个数据集在高性能计算的代码翻译领域的潜力。

    In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
    
[^83]: 在大型语言模型时代的被遗忘权：涵义、挑战和解决方案

    Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])

    [http://arxiv.org/abs/2307.03941](http://arxiv.org/abs/2307.03941)

    本文探讨了在大型语言模型时代的被遗忘权（RTBF）面临的挑战，提供了实施技术解决方案的见解。

    

    被遗忘权（RTBF）最初是由谷歌西班牙与埃克斯内塔索委员会(Mario Costeja Gonz\'alez)之间的官司结果而确立的，并且后来被作为欧洲联盟一般数据保护条例（GDPR）下的删除权。RTBF允许个人向组织请求删除个人数据，特别是对于搜索引擎，个人可以向组织发送请求，排除他们的信息在查询结果中出现。然而，随着大型语言模型（LLMs）的发展和其在聊天机器人中的应用，LLM启用的软件系统变得越来越受欢迎。但它们并没有被排除在RTBF之外。相比搜索引擎使用的索引方法，LLMs以一种完全不同的方式存储和处理信息，这为符合RTBF提出了新的挑战。在本文中，我们探讨了这些挑战，并提供了关于如何实施技术解决方案以符合RTBF的见解。

    The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
    
[^84]: 朝着可信的解释：因果关系解释论文研究

    Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])

    [http://arxiv.org/abs/2306.14115](http://arxiv.org/abs/2306.14115)

    该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。

    

    随着自然语言处理的最新进展，解释成为了通过选择输入文本的子集来解释黑盒模型中主要变化的一个基本的自我解释图。然而，现有的基于关联的解释方法在两个或多个片段高度互相关联时无法识别真正的解释，因此对预测准确性提供类似的贡献，所谓的虚假性。为了解决这一限制，我们从因果推断的角度新颖地将两个因果期望值（非虚假性和效率）引入了解释中。我们根据一种新提出的解释结构因果模型定义了一系列的因果概率，通过其理论鉴定，建立了必要和充分解释的主要组成部分。我们在真实世界的评论和医疗数据集上证明了所提出的因果关系解释的优越性能。

    With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
    
[^85]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^86]: 提升离线到在线强化学习的Q-Ensembles方法

    Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06871](http://arxiv.org/abs/2306.06871)

    我们提出了一种名为Q-Ensembles的新框架，通过增加Q网络的数量，无缝地连接离线预训练和在线微调，同时不降低性能。此外，我们适当放宽Q值估计的悲观性，并将基于集合的探索机制融入我们的框架中，从而提升了离线到在线强化学习的性能。

    

    离线强化学习是一种学习范式，代理根据固定的经验数据集进行学习。然而，仅从静态数据集中学习可能限制了性能，因为缺乏探索能力。为了克服这个问题，将离线预训练与在线微调结合起来的离线到在线强化学习方法能够让代理与环境实时交互，进一步完善其策略。然而，现有的离线到在线强化学习方法存在性能下降和在线阶段改进缓慢的问题。为了解决这些挑战，我们提出了一种名为Q-Ensembles的新框架，它通过增加Q网络的数量，无缝地连接离线预训练和在线微调，同时不降低性能。此外，为了加快在线性能提升，我们适当放宽Q值估计的悲观性，并将基于集合的探索机制融入我们的框架中。

    Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
    
[^87]: LLMatic: 基于大语言模型和多样性优化的神经结构搜索

    LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])

    [http://arxiv.org/abs/2306.01102](http://arxiv.org/abs/2306.01102)

    本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    

    大型语言模型 (LLMs) 已成为一种强大的工具，可以完成广泛的任务。它们的能力涵盖了许多领域，它们在代码生成领域产生了重大影响。在此情况下，我们将 LLMs 视为变异和交叉工具。同时，多样性优化算法已知可以发现多样性和稳健的解决方案。通过将 LLMs 的代码生成能力与 QD 解决方案的多样性和鲁棒性相结合，我们引入了 LLMatic，一个神经结构搜索 (NAS) 算法。虽然 LLMs 通过提示直接进行 NAS 考验困难，但 LLMatic 利用程序化方法，利用 QD 来进行提示和网络结构，从而创建多样性和高性能网络。我们在 CIFAR-10 图像分类基准测试中测试了 LLMatic，证明它可以在仅进行 2000 次搜索的情况下产生具有竞争力的网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
    
[^88]: GPT 模型在化学领域到底有怎样的应用？八个任务的综合基准测试。

    What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])

    [http://arxiv.org/abs/2305.18365](http://arxiv.org/abs/2305.18365)

    本文建立了包括 8 个实际化学任务的综合基准测试，有力地证明了 LLM 在实际化学中的能力。

    

    具有强大自然语言处理能力的大型语言模型已被广泛应用于科学、金融和软件工程等领域。但是，LLM 是否有能力推动化学领域的进展仍不清楚。本文建立了包含 8 个实际化学任务的综合基准测试，包括名称预测、属性预测、产量预测、反应预测、反合成（从产物预测反应物）、基于文本的分子设计、分子字幕和试剂选择。我们使用广泛认可的数据集，包括 BBBP、Tox21、PubChem、USPTO 和 ChEBI，有力地证明了 LLM 在实际化学中的能力。在精心选择的示例中，对三种 GPT 模型（GPT-4、GPT-3.5 和 DaVinci-003）在零样本和少样本有上下文学习的设置中进行了评估。

    Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
    
[^89]: 大型语言模型可能是懒惰的学习者：分析上下文学习中的捷径

    Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])

    [http://arxiv.org/abs/2305.17256](http://arxiv.org/abs/2305.17256)

    本文探讨了大型语言模型在上下文学习中利用提示中的捷径的依赖性，发现大型模型更有可能在推理过程中利用提示中的捷径，这为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。

    

    最近，大型语言模型（LLM）在上下文学习中展现出巨大潜力，其中LLM通过几个输入-标签对（提示）的条件来学习新任务。尽管其潜力巨大，但我们对影响最终任务性能和上下文学习稳健性的因素的理解仍然有限。本文旨在通过研究LLM对提示内捷径或假相关的依赖关系来弥补这一知识差距。通过分类和抽取任务的全面实验，我们揭示了LLM是“懒惰学习者”的事实，它往往利用提示中的捷径来获取下游任务的性能提升。此外，我们还发现一个令人惊讶的发现，即较大的模型更有可能在推理过程中利用提示中的捷径。我们的发现为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。

    Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
    
[^90]: 早起的鸟儿捉到虫：利用编码器模型的早期层进行更有效的代码分类

    The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])

    [http://arxiv.org/abs/2305.04940](http://arxiv.org/abs/2305.04940)

    本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。

    

    现代自然语言处理技术在软件工程任务如漏洞检测和类型推理方面表现出了卓越的优势。然而，训练深度自然语言处理模型需要大量计算资源。本文探讨了一些技术，旨在实现这些模型中资源和可用信息的最佳利用。我们提出了一种通用的方法EarlyBIRD，从预训练的transformer模型的早期层构建代码的复合表示。我们通过比较12种创建复合表示的策略与仅使用最后一个编码器层的标准实践，在CodeBERT模型上实证研究了这种方法的可行性。我们在4个数据集上的评估表明，几个早期层的组合在缺陷检测方面产生更好的性能，而一些组合则改进了多类分类。具体而言，我们获得了平均检测增强2。

    The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
    
[^91]: 黑暗环境下的实例分割

    Instance Segmentation in the Dark. (arXiv:2304.14298v1 [cs.CV])

    [http://arxiv.org/abs/2304.14298](http://arxiv.org/abs/2304.14298)

    本文提出了在黑暗环境下的实例分割方法。为了抑制低光图像中的特征噪声，该方法采用了自适应加权下采样层、平滑定向卷积块和扰动抑制学习等技术，同时发现高位深的RAW图像可以更好地保留更丰富的场景信息。

    

    现有的实例分割技术主要适用于高可见度的输入，但在极低光环境下，它们的性能显著下降。本文深入研究了在黑暗中进行实例分割，并介绍了几种可以显著提高低光推断准确性的技术。所提出的方法是基于这样的观察：低光图像中的噪声会引入高频扰动到神经网络的特征图中，从而显著降低性能。为了抑制这种“特征噪声”，我们提出了一种新的学习方法，它依赖于自适应加权下采样层、平滑定向卷积块和扰动抑制学习。这些组件有效地减少了下采样和卷积操作中的特征噪声，使模型能够学习抗扰动的特征。此外，我们发现高位深的RAW图像可以更好地保留更丰富的场景信息，

    Existing instance segmentation techniques are primarily tailored for high-visibility inputs, but their performance significantly deteriorates in extremely low-light environments. In this work, we take a deep look at instance segmentation in the dark and introduce several techniques that substantially boost the low-light inference accuracy. The proposed method is motivated by the observation that noise in low-light images introduces high-frequency disturbances to the feature maps of neural networks, thereby significantly degrading performance. To suppress this ``feature noise", we propose a novel learning method that relies on an adaptive weighted downsampling layer, a smooth-oriented convolutional block, and disturbance suppression learning. These components effectively reduce feature noise during downsampling and convolution operations, enabling the model to learn disturbance-invariant features. Furthermore, we discover that high-bit-depth RAW images can better preserve richer scene i
    
[^92]: 从空间中分割任何物体吗？

    Segment anything, from space?. (arXiv:2304.13000v1 [cs.CV])

    [http://arxiv.org/abs/2304.13000](http://arxiv.org/abs/2304.13000)

    最近开发的Segment Anything Model（SAM）模型可以基于简单的输入提示（如一个或多个点、边界框或掩码）有效分割自然图像中的对象，对视觉研究人员具有重要意义。此项研究探讨SAM在空中图像问题上的卓越性能，并在多项基准任务上进行了验证，表现良好。

    

    最近，为视觉任务专门开发的第一个基础模型被开发出来，被称为“Segment Anything Model”（SAM）。SAM可以根据简单的输入提示（如一个或多个点、边界框或掩码）分割输入图像中的对象。作者们在大量的视觉基准任务上研究了SAM的零样本图像分割精度，并发现SAM通常达到了与目标任务训练的视觉模型相似或有时甚至超越其识别精度。SAM在分割方面的卓越泛化能力对于从事自然图像研究的视觉研究人员具有重要意义。在这项工作中，我们研究了SAM的卓越性能是否扩展到空中图像问题，并帮助指导社区对其发展的回应。我们在一组多样化和广泛研究过的基准任务上研究SAM的表现。我们发现，SAM通常在空中图像上有良好的泛化表现，尽管在某些情况下会失败。

    Recently, the first foundation model developed specifically for vision tasks was developed, termed the "Segment Anything Model" (SAM). SAM can segment objects in input imagery based upon cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's impressive performance extends to overhead imagery problems, and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely-studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases d
    
[^93]: 多尺度进化神经架构搜索用于深度脉冲神经网络

    Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v1 [cs.NE])

    [http://arxiv.org/abs/2304.10749](http://arxiv.org/abs/2304.10749)

    本文提出了一种新方法--多尺度进化神经架构搜索，用于自动设计脉冲神经网络，同时考虑微观、中观和宏观尺度的脑拓扑结构。MSE-NAS可以帮助SNN实现多电路模式的自组织集成，并通过全局性的跨模式连接来优化网络性能。

    

    脉冲神经网络（SNN）不仅因其离散信号处理的能源效率卓越，而且因其天然适合于集成多尺度生物可塑性而受到广泛关注。然而，大多数SNN直接采用成熟的DNN结构，很少自动设计神经架构搜索（NAS）用于SNN。人类大脑神经模式的拓扑结构，模块化的区域结构和全局性的跨脑区连接是自然进化的产物，可以作为设计基于脑的SNN架构的完美参考。本文提出了一种多尺度进化神经架构搜索（MSE-NAS），同时考虑微观、中观和宏观尺度的脑拓扑作为进化搜索空间。 MSE-NAS通过基于大脑启发的间接方式，进化单个神经元操作，多个电路模式的自组织集成以及跨模式的全局连通性。

    Spiking Neural Networks (SNNs) have received considerable attention not only for their superiority in energy efficient with discrete signal processing, but also for their natural suitability to integrate multi-scale biological plasticity. However, most SNNs directly adopt the structure of the well-established DNN, rarely automatically design Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular regional structure and global cross-brain region connection of the human brain are the product of natural evolution and can serve as a perfect reference for designing brain-inspired SNN architecture. In this paper, we propose a Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN, simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space. MSE-NAS evolves individual neuron operation, self-organized integration of multiple circuit motifs, and global connectivity across motifs through a brain-inspired indirec
    
[^94]: 行程规划中的群体效用优化：一种策略性和众包意识方法

    Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach. (arXiv:2304.08495v1 [cs.AI])

    [http://arxiv.org/abs/2304.08495](http://arxiv.org/abs/2304.08495)

    本论文介绍了一种名为SCAIR的算法，可以优化群体效用，解决行程规划中的多个用户排队时间和人群水平优化的问题。

    

    行程推荐是一个具有许多实际应用的复杂的序列预测问题。当考虑到优化多个用户排队时间和人群水平时，这项任务变得更具挑战性，因为涉及到诸多参数，如景点受欢迎程度、排队时间、步行时间和营业时间等。现有的解决方案通常集中在单人视角上，未能解决自然人群行为引起的现实问题，如贪婪路由问题。本文介绍了一种名为“战略和众包意识行程推荐（SCAIR）”算法，该算法在现实环境中优化群体效用。我们将路线推荐策略建模为马尔可夫决策过程，并提出了一种状态编码机制，使得可以在线性时间内实现实时规划和分配。我们使用主题公园数据集对我们的算法进行各种竞争性和现实的基线测试，证明SCAIR优于其他算法。

    Itinerary recommendation is a complex sequence prediction problem with numerous real-world applications. This task becomes even more challenging when considering the optimization of multiple user queuing times and crowd levels, as well as numerous involved parameters, such as attraction popularity, queuing time, walking time, and operating hours. Existing solutions typically focus on single-person perspectives and fail to address real-world issues resulting from natural crowd behavior, like the Selfish Routing problem. In this paper, we introduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR) algorithm, which optimizes group utility in real-world settings. We model the route recommendation strategy as a Markov Decision Process and propose a State Encoding mechanism that enables real-time planning and allocation in linear time. We evaluate our algorithm against various competitive and realistic baselines using a theme park dataset, demonstrating that SCAIR outperforms th
    
[^95]: ARNOLD：基于连续状态实现的现实3D场景语言引导任务学习基准测试

    ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])

    [http://arxiv.org/abs/2304.04321](http://arxiv.org/abs/2304.04321)

    ARNOLD是一个评估基于语言引导、具有连续状态的现实3D场景任务学习的基准测试，涉及8个语言条件任务，在语言引导下帮助机器人学习理解物体状态和学习连续目标的策略。

    

    在现实世界中，理解物体的连续状态对于任务学习和规划至关重要。然而，大多数任务学习基准测试假定目标状态是离散的(例如二进制状态)，这给学习复杂任务和将学习策略从模拟环境转移到现实世界带来了挑战。此外，状态离散化限制了机器人根据动作和状态的引导遵循人类指令的能力。为了解决这些挑战，我们提出了ARNOLD，这是一个评估基于语言引导、具有连续状态的现实3D场景任务学习的基准测试。ARNOLD由8个语言条件任务组成，涉及理解物体状态和学习连续目标的策略。为了促进语言引导学习，我们提供了模板生成的语言描述的专家演示。我们通过使用最新的语言条件策略学习模型来评估任务的性能。我们的结果表明，ARNOLD为基于连续状态的语言引导任务学习提供了一个具有挑战性的环境，并可用于评估从模拟场景到现实世界的学习策略的泛化。

    Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results ind
    
[^96]: 大型语言模型综述

    A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])

    [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)

    本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。

    

    语言本质上是一个由语法规则控制的复杂精细的人类表达系统，对于开发理解和掌握语言的能力的AI算法来说是一项重大挑战。作为主要方法之一，语言建模在过去二十年里广泛研究用于语言理解和生成，从统计语言模型演化为神经语言模型。最近，通过在大规模语料库上预训练Transformer模型，提出了预训练语言模型（PLMs），在解决各种NLP任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致性能改进，他们进一步通过增加模型规模来研究缩放效应，有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅可以实现显着的性能提升，而且还显示出一些小规模语言模型所没有的特殊能力。

    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
    
[^97]: 基于判别性类标的文本图片扩散模型

    Discriminative Class Tokens for Text-to-Image Diffusion Models. (arXiv:2303.17155v1 [cs.CV])

    [http://arxiv.org/abs/2303.17155](http://arxiv.org/abs/2303.17155)

    本文提出了一种基于判别性信息和自由文本的非侵入式微调技术，以实现多样性和高准确率的文本到图像生成模型。

    

    最近文本到图像扩散模型的进展使得生成多样且高质量图片成为可能。然而，由于输入文本的歧义，生成的图片常常无法描绘出微妙的细节且易于出错。缓解这些问题的方法之一是在有类标注的数据集上训练扩散模型。这种方法的缺点在于：（i）与用于训练文本到图像模型的大规模爬取的文本-图像数据集相比，有类标注的数据集通常较小，因此生成的图片质量和多样性会严重受影响，或（ii）输入是硬编码的标签，而不是自由文本，这限制了对生成的图像的控制。在这项工作中，我们提出了一种非侵入式的微调技术，利用预训练分类器的判别性信号引导生成过程，既发挥了自由文本的表达潜力，又能够实现高准确率。

    Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. However, generated images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This comes with a downside, doing so limits their expressive power: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, and so the quality and diversity of generated images are severely affected, or (ii) the input is a hard-coded label, as opposed to free-form text, which limits the control over the generated images.  In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier, which guides the generation. This 
    
[^98]: 通识知识辅助的资源受限和细粒度目标检测的深度学习方法

    Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])

    [http://arxiv.org/abs/2303.09026](http://arxiv.org/abs/2303.09026)

    本文提出了一种通识知识辅助的细粒度目标检测方法，利用通识知识推理模块处理由基准深度学习检测器给出的粗粒度标签，从而提高目标检测的准确性。经过实验验证，该方法相比于现有方法需要更少的计算量和标注资源。

    

    本文考虑边缘计算等资源受限场景下的细粒度图像目标检测问题。针对使用现代深度学习目标检测器时需要使用大型模型和大量数据标注的精准细粒度检测需求，提出一种方法，即利用通识知识辅助粗粒度目标检测器获取精准的细粒度检测结果。引入通识知识推理模块(CKIM)处理由基准深度学习检测器给出的粗粒度标签，从而生成细粒度标签。论文中考虑了模糊规则和清晰规则的推理，前者用于处理目标语义标签的模糊性。实验结果表明所提方法可以有效提高目标检测的准确性，同时相比于现有方法需要更少的计算量和标注资源。

    In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
    
[^99]: TSMixer：一种全MLP架构用于时间序列预测

    TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)

    TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。

    

    实际时间序列数据集通常是多变量且具有复杂的动态。为了捕获这种复杂性，像循环或基于注意力的顺序深度学习模型这样的高容量结构变得受欢迎。然而，最近的研究表明，简单的单变量线性模型可以在几个常用的学术基准测试中胜过这样的深度学习模型。扩展它们，本文研究线性模型在时间序列预测中的能力，并提出了时序混合器（TSMixer），这是一种通过堆叠多层感知器（MLP）设计的新型结构。 TSMixer基于沿时间和特征维度的混合操作，以有效地提取信息。在流行的学术基准测试上，简单易行的TSMixer与利用特定基准的归纳偏差的专业先进模型相媲美。在具有挑战性和大规模的M5基准测试中，即一个实际的零售数据集上，TSMixer表现出非常出色的性能。

    Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
    
[^100]: 高效的数学表达式生成器用于符号回归

    Efficient Generator of Mathematical Expressions for Symbolic Regression. (arXiv:2302.09893v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09893](http://arxiv.org/abs/2302.09893)

    我们提出了一种基于变分自动编码器的方法来生成数学表达式，并用于符号回归。实验结果表明，我们的方法可以高效地训练，并且能够准确地编码表达式。通过优化方法探索生成的潜在空间能够解决符号回归问题，且效果优于传统的方法。

    

    我们提出了一种基于新颖的变分自动编码器（HVAE）的符号回归方法，用于生成层级结构。它将简单的原子单元与共享权重相结合，以递归地编码和解码层级中的各个节点。编码是自下而上进行的，解码是自上而下进行的。我们实验上证明了HVAE可以在数学表达式的小语料库上高效地进行训练，并可以将表达式准确地编码成平滑的低维潜在空间。后者可以通过各种优化方法高效地进行探索，以解决符号回归的任务。事实上，通过HVAE的潜在空间进行随机搜索比通过手工设计的数学表达式的随机搜索效果更好。最后，应用进化算法到HVAE的潜在空间中的EDHiE系统可以更好地从标准符号回归基准中重建方程。

    We propose an approach to symbolic regression based on a novel variational autoencoder for generating hierarchical structures, HVAE. It combines simple atomic units with shared weights to recursively encode and decode the individual nodes in the hierarchy. Encoding is performed bottom-up and decoding top-down. We empirically show that HVAE can be trained efficiently with small corpora of mathematical expressions and can accurately encode expressions into a smooth low-dimensional latent space. The latter can be efficiently explored with various optimization methods to address the task of symbolic regression. Indeed, random search through the latent space of HVAE performs better than random search through expressions generated by manually crafted probabilistic grammars for mathematical expressions. Finally, EDHiE system for symbolic regression, which applies an evolutionary algorithm to the latent space of HVAE, reconstructs equations from a standard symbolic regression benchmark better 
    
[^101]: AudioLDM: 基于潜在扩散模型的文本转音频生成

    AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. (arXiv:2301.12503v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.12503](http://arxiv.org/abs/2301.12503)

    AudioLDM是一种基于潜在扩散模型的文本到音频生成系统，通过对比语言-音频预训练学习音频的连续表示，从而在生成质量和计算效率上具有优势。它在TTA性能上达到了最先进水平，并且能够实现各种文本引导的音频生成。

    

    最近，文本到音频（TTA）系统因其能够根据文本描述合成通用音频而引起了关注。然而，以往的TTA研究在生成质量上存在局限性，并且计算成本较高。本研究提出了基于潜在空间的TTA系统AudioLDM，该系统通过对比语言-音频预训练（CLAP）潜变量学习连续音频表示。预训练的CLAP模型使我们能够训练具有音频嵌入的LDM，并在采样过程中提供文本嵌入作为条件。通过学习音频信号及其组合的潜变量表示，而不是建模跨模态关系，AudioLDM在生成质量和计算效率上具有优势。在单个GPU上使用AudioCaps进行训练的结果显示，AudioLDM在客观和主观指标（如Frechet距离）上实现了最先进的TTA性能。此外，AudioLDM是第一个能够实现各种文本引导的音频生成的TTA系统。

    Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided au
    
[^102]: 高维变分推理的投影积分更新

    Projective Integral Updates for High-Dimensional Variational Inference. (arXiv:2301.08374v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08374](http://arxiv.org/abs/2301.08374)

    该论文介绍了一种适用于高维变分推理的投影积分更新方法，并通过降低参数敏感性来实现更强健的预测。

    

    变分推理是一种贝叶斯推理的近似框架，通过优化简化的参数分布来代替完整的后验分布，从而改善预测中的量化不确定性。捕捉与训练数据一致的模型变化可以通过降低参数敏感性来实现更强健的预测。本研究引入了一种适用于变分推理的固定点最优化方法，当每个可行的对数密度可以表示为给定基函数的线性组合时，该方法生效。在这种情况下，优化器成为投影积分更新的一个不动点。当基函数跨越每个参数的二次函数时，可行密度为高斯分布，投影积分更新产生了准牛顿变分贝叶斯 (QNVB)。其他基函数和更新方法也是可能的。由于这些更新需要高维积分，本研究首先提出了一种高效的准随机积分序列用于均匀均匀均匀均匀均匀均匀积分。

    Variational inference is an approximation framework for Bayesian inference that seeks to improve quantified uncertainty in predictions by optimizing a simplified distribution over parameters to stand in for the full posterior. Capturing model variations that remain consistent with training data enables more robust predictions by reducing parameter sensitivity. This work introduces a fixed-point optimization for variational inference that is applicable when every feasible log density can be expressed as a linear combination of functions from a given basis. In such cases, the optimizer becomes a fixed-point of projective integral updates. When the basis spans univariate quadratics in each parameter, feasible densities are Gaussian and the projective integral updates yield quasi-Newton variational Bayes (QNVB). Other bases and updates are also possible. As these updates require high-dimensional integration, this work first proposes an efficient quasirandom quadrature sequence for mean-fie
    
[^103]: 利用物理感知神经网络对锂离子电池的预测和健康管理进行建模

    Physics-Informed Neural Networks for Prognostics and Health Management of Lithium-Ion Batteries. (arXiv:2301.00776v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.00776](http://arxiv.org/abs/2301.00776)

    本研究提出了一种基于物理感知神经网络（PINN）的模型融合方案，用于预测和管理锂离子电池的健康状态。该方法能够将经验或物理动态模型与数据驱动模型相结合，以充分利用各种信息来源，提高预测和管理的准确性。

    

    对于锂离子电池的预测和健康管理（PHM），已建立了许多模型来描述其衰减过程。现有的经验或物理模型可以揭示有关衰减动态的重要信息。然而，目前还没有一种通用且灵活的方法来融合这些模型所代表的信息。物理感知神经网络（PINN）是一种将经验或物理动态模型与数据驱动模型相结合的高效工具。为了充分利用各种信息来源，我们提出了一种基于PINN的模型融合方案。通过开发半经验半物理的偏微分方程（PDE）来建模锂离子电池的衰减动态。当对动态了解较少时，我们利用数据驱动的深度隐藏物理模型（DeepHPM）来发现潜在的主导动态模型。然后将发现的动态信息与挖掘的信息进行融合。

    For Prognostics and Health Management (PHM) of Lithium-ion (Li-ion) batteries, many models have been established to characterize their degradation process. The existing empirical or physical models can reveal important information regarding the degradation dynamics. However, there are no general and flexible methods to fuse the information represented by those models. Physics-Informed Neural Network (PINN) is an efficient tool to fuse empirical or physical dynamic models with data-driven models. To take full advantage of various information sources, we propose a model fusion scheme based on PINN. It is implemented by developing a semi-empirical semi-physical Partial Differential Equation (PDE) to model the degradation dynamics of Li-ion batteries. When there is little prior knowledge about the dynamics, we leverage the data-driven Deep Hidden Physics Model (DeepHPM) to discover the underlying governing dynamic models. The uncovered dynamics information is then fused with that mined by 
    
[^104]: 基于内涵一阶逻辑的强人工智能自我确切机器人

    Strong-AI Autoepistemic Robots Build on Intensional First Order Logic. (arXiv:2212.07935v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.07935](http://arxiv.org/abs/2212.07935)

    本文研究了内涵一阶逻辑作为现代机器人的符号架构，可以推理自身知识、使用自然语言与人类交流、抽象语言属性。同时，通过机器人神经架构的经验获得机器人语言的基础，将其与IFOL理论中的非定义语言概念相联系。

    

    神经符号化人工智能试图以互补的方式集成神经和符号架构，以应对每种架构的优点和缺点，以支持具有推理、学习和认知建模能力的强人工智能系统。本文考虑了内涵一阶逻辑(IFOL)作为现代机器人的符号架构，能够使用自然语言与人类进行交流，并使用自我参照和抽象语言属性推理其自身知识。我们试图通过机器人使用其神经架构的经验来获得机器人语言的基础，并将这种经验与IFOL的PRP（属性/关系/命题）理论中的非定义语言概念（特定/个体和普遍概念）的挖掘（感觉）相联系。我们考虑机器人的四级知识结构：特定自然语言的语法层面（意大利语、法语等）、两个通用语言层面：其语义关系及它的内涵逻辑结构和其它语言概念的陈述。

    Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust strong AI capable of reasoning, learning, and cognitive modeling. In this paper we consider the  intensional First Order Logic (IFOL) as a symbolic architecture of modern robots, able to use natural languages to communicate with humans and to reason about their own knowledge with self-reference and abstraction language property.  We intend to obtain the grounding of robot's language by experience of how it uses its neuronal architectures and hence by associating this experience with the mining (sense) of non-defined language concepts (particulars/individuals and universals) in PRP (Properties/Relations/Propositions) theory of IFOL.  We consider the robot's four-levels knowledge structure: The syntax level of particular natural language (Italian, French, etc..), two universal language levels: its sem
    
[^105]: DWRSeg:重新思考实时语义分割中多尺度上下文信息的高效获取方式

    DWRSeg: Rethinking Efficient Acquisition of Multi-scale Contextual Information for Real-time Semantic Segmentation. (arXiv:2212.01173v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.01173](http://arxiv.org/abs/2212.01173)

    DWRSeg重新思考了实时语义分割中获取多尺度上下文信息的方法，提出了一种高效的多尺度特征提取方法，通过将原始单步方法分解为区域残余化和语义残余化两个步骤，简化了多速率深度空洞卷积的角色，并提高了特征提取的效率。

    

    目前许多方法直接采用多速率的深度空洞卷积来同时从一个输入特征映射中捕获多尺度的上下文信息，从而提高实时语义分割的特征提取效率。然而，这种设计可能由于不合理的结构和超参数而导致难以访问多尺度的上下文信息。为了降低获取多尺度上下文信息的难度，我们提出了一种高效的多尺度特征提取方法，将原始的单步方法分解为两个步骤：区域残余化和语义残余化。在这种方法中，多速率的深度空洞卷积在特征提取方面扮演了更简单的角色：在第二步中，根据第一步提供的每个简洁的区域形式特征映射，对每个期望的感受野进行简单的基于语义的形态滤波，以提高其效率。

    Many current works directly adopt multi-rate depth-wise dilated convolutions to capture multi-scale contextual information simultaneously from one input feature map, thus improving the feature extraction efficiency for real-time semantic segmentation. However, this design may lead to difficult access to multi-scale contextual information because of the unreasonable structure and hyperparameters. To lower the difficulty of drawing multi-scale contextual information, we propose a highly efficient multi-scale feature extraction method, which decomposes the original single-step method into two steps, Region Residualization-Semantic Residualization.In this method, the multi-rate depth-wise dilated convolutions take a simpler role in feature extraction: performing simple semantic-based morphological filtering with one desired receptive field in the second step based on each concise feature map of region form provided by the first step, to improve their efficiency. Moreover, the dilation rate
    
[^106]: 深度图聚类综述：分类、挑战、应用和开放资源

    A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource. (arXiv:2211.12875v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12875](http://arxiv.org/abs/2211.12875)

    在这篇论文中，作者对深度图聚类进行了综述研究。首先介绍了该领域的定义、评估和发展，然后提出了深度图聚类方法的分类学，并对现有方法进行了分析，总结出了挑战和机会。

    

    图聚类旨在将图中的节点划分为若干不同的簇，是一项基础且具有挑战性的任务。近年来，借助深度学习强大的表示能力，深度图聚类方法取得了巨大成功。然而，对于这个领域的综述论文相对较少，综述该领域的工作势在必行。基于此动机，我们进行了深度图聚类的综述研究。首先，我们介绍了该领域的公式化定义、评估和发展。其次，基于图类型、网络架构、学习范式和聚类方法等四个不同的标准，提出了深度图聚类方法的分类学。第三，我们通过广泛的实验对现有方法进行了详细分析，并从图数据质量、稳定性、可扩展性、辨别能力和未知簇数等五个角度总结了挑战和机会。

    Graph clustering, which aims to divide nodes in the graph into several distinct clusters, is a fundamental yet challenging task. Benefiting from the powerful representation capability of deep learning, deep graph clustering methods have achieved great success in recent years. However, the corresponding survey paper is relatively scarce, and it is imminent to make a summary of this field. From this motivation, we conduct a comprehensive survey of deep graph clustering. Firstly, we introduce formulaic definition, evaluation, and development in this field. Secondly, the taxonomy of deep graph clustering methods is presented based on four different criteria, including graph type, network architecture, learning paradigm, and clustering method. Thirdly, we carefully analyze the existing methods via extensive experiments and summarize the challenges and opportunities from five perspectives, including graph data quality, stability, scalability, discriminative capability, and unknown cluster nu
    
[^107]: 文字对话中的深度情感识别：一项调研

    Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09172](http://arxiv.org/abs/2211.09172)

    本调研针对对话中的情感识别进行了探讨，介绍了涉及此任务的挑战和机遇，以及描述了情感分类法和使用该分类法的基准数据集。调研总结了最重要的作品和所使用的深度学习架构，并提供了建议性的情感识别实践，以实现更好的框架。

    

    虽然近年来对话中的情感识别取得了巨大的进展，但新的应用和实施场景带来了新的挑战和机遇。这些挑战包括利用对话语境、说话人和情感动态建模，解释常识表达、非正式语言和讽刺，应对实时情感识别的挑战，识别情感原因，不同数据集中的多种分类法，多语言情感识别以及解释性。本调研首先介绍了情感识别在对话中的应用，详细说明了与此任务相关的挑战和机遇。然后，它介绍了情感分类法和多种使用该分类法的情感识别基准数据集的描述。接下来，它描述了情感识别中最重要的作品，并解释了所使用的深度学习架构。最后，它提供了对于更好的框架的建议性情感识别实践，详细说明了处理主观性的方法。

    While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
    
[^108]: 通过无监督说话风格转换提升语音情感识别

    Improving Speech Emotion Recognition with Unsupervised Speaking Style Transfer. (arXiv:2211.08843v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.08843](http://arxiv.org/abs/2211.08843)

    本文介绍了EmoAug，一种通过无监督说话风格转换来提升语音情感识别的模型。EmoAug利用语义编码器和语调编码器表示语言和非语言信息，并通过无监督方式重建语音信号。训练完成后，EmoAug通过引入不同风格丰富了情感语音的表达，并解决了数据不平衡问题。

    

    人类可以轻松修改各种韵律属性，例如重音位置和情感强度，以传达特定的情感，同时保持一致的语言内容。受到这种能力的启发，我们提出了EmoAug，这是一种新颖的风格转换模型，旨在增强情感表达并解决语音情感识别任务中的数据稀缺问题。EmoAug由一个语义编码器和一个语调编码器组成，分别表示语言和非语言信息。此外，解码器以无监督的方式在上述两个信息流的条件下重建语音信号。训练完成后，EmoAug通过将不同风格输入到语调编码器中，丰富了情感语音的表达，包括重音、节奏和强度等不同的韵律属性。同时，EmoAug能够生成每个类别相似数量的样本，以解决数据不平衡问题。

    Humans can effortlessly modify various prosodic attributes, such as the placement of stress and the intensity of sentiment, to convey a specific emotion while maintaining consistent linguistic content. Motivated by this capability, we propose EmoAug, a novel style transfer model designed to enhance emotional expression and tackle the data scarcity issue in speech emotion recognition tasks. EmoAug consists of a semantic encoder and a paralinguistic encoder that represent verbal and non-verbal information respectively. Additionally, a decoder reconstructs speech signals by conditioning on the aforementioned two information flows in an unsupervised fashion. Once training is completed, EmoAug enriches expressions of emotional speech with different prosodic attributes, such as stress, rhythm and intensity, by feeding different styles into the paralinguistic encoder. EmoAug enables us to generate similar numbers of samples for each class to tackle the data imbalance issue as well. Experiment
    
[^109]: 基于模型的残差策略学习及其在天线控制中的应用

    Model Based Residual Policy Learning with Applications to Antenna Control. (arXiv:2211.08796v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08796](http://arxiv.org/abs/2211.08796)

    本文介绍了一种基于模型的残差策略学习（MBRPL）方法，用于解决天线控制问题。该方法通过增强现有策略，提高了样本效率，并减少了与实际环境的交互次数。实验结果表明，该方法在初始性能方面表现出色，并为将这些算法应用于实际网络迈出了一步。

    

    非可微控制器和基于规则的策略广泛应用于控制诸如电信网络和机器人等实际系统。特别是，移动网络基站天线的参数可以通过这些策略进行动态配置，以改善用户覆盖率和服务质量。受天线倾斜控制问题的启发，我们引入了一种实用的强化学习方法——基于模型的残差策略学习（MBRPL）。MBRPL通过基于模型的方法增强现有策略，提高样本效率，并与现成的强化学习方法相比减少与实际环境的交互次数。据我们所知，这是第一篇研究基于模型的天线控制方法的论文。实验结果表明，我们的方法在初始性能方面表现出色，并提高了样本效率，这是将这些算法应用于实际网络的一步。

    Non-differentiable controllers and rule-based policies are widely used for controlling real systems such as telecommunication networks and robots. Specifically, parameters of mobile network base station antennas can be dynamically configured by these policies to improve users coverage and quality of service. Motivated by the antenna tilt control problem, we introduce Model-Based Residual Policy Learning (MBRPL), a practical reinforcement learning (RL) method. MBRPL enhances existing policies through a model-based approach, leading to improved sample efficiency and a decreased number of interactions with the actual environment when compared to off-the-shelf RL methods.To the best of our knowledge, this is the first paper that examines a model-based approach for antenna control. Experimental results reveal that our method delivers strong initial performance while improving sample efficiency over previous RL methods, which is one step towards deploying these algorithms in real networks.
    
[^110]: 发现、解释、改进：一种用于自然语言处理的自动片段检测框架

    Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.04476](http://arxiv.org/abs/2211.04476)

    本研究提出了一个自动片段检测框架用于自然语言处理任务，通过发现、解释和改进模型的错误，提供了对模型行为的理解和未来模型设计的见解。

    

    预训练的自然语言处理（NLP）模型取得了高整体性能，但仍然存在系统性的错误。与手动错误分析不同，对于自动识别表现不佳的数据组的片段检测模型（SDM）的研究，在计算机视觉领域引起了广泛关注，可以理解模型行为并为未来模型训练和设计提供见解。然而，在NLP任务上，对SDM的研究和其有效性的定量评估还很少。本文通过提出一个名为“Discover, Explain, Improve(DEIM)”的NLP分类任务基准和一个新的SDM模型Edisa来填补这一空白。Edisa发现了一致且表现不佳的数据组；DEIM将它们统一为人类可理解的概念，并提供了全面的评估任务和相应的定量指标。在DEIM的评估中，结果显示Edisa能够准确选择易出错的数据点，并提供了有用的信息。

    Pretrained natural language processing (NLP) models have achieved high overall performance, but they still make systematic errors. Instead of manual error analysis, research on slice detection models (SDM), which automatically identify underperforming groups of datapoints, has caught escalated attention in Computer Vision for both understanding model behaviors and providing insights for future model training and designing. However, little research on SDM and quantitative evaluation of their effectiveness have been conducted on NLP tasks. Our paper fills the gap by proposing a benchmark named "Discover, Explain, Improve (DEIM)" for classification NLP tasks along with a new SDM Edisa. Edisa discovers coherent and underperforming groups of datapoints; DEIM then unites them under human-understandable concepts and provides comprehensive evaluation tasks and corresponding quantitative metrics. The evaluation in DEIM shows that Edisa can accurately select error-prone datapoints with informati
    
[^111]: 可变层次混合模型用于概率逆动力学学习

    Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics. (arXiv:2211.01120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01120](http://arxiv.org/abs/2211.01120)

    本文提出了一种可变层次混合模型方法，通过概率层次化建模来学习机器人应用中的逆动力学。该方法结合了传统回归模型的优势，实现了计算高效的表示和自适应数据复杂性的灵活性。

    

    随着数据集的快速增长和任务的复杂化，良好校准的概率回归模型在机器人应用中是至关重要的学习组成部分。不幸的是，经典的回归模型通常要么是具有灵活结构但不随数据优雅扩展的概率核机器，要么是具有限制参数形式和较差正则化的确定性且可扩展的自动机。在本文中，我们考虑了一种概率层次建模范式，将两者的优势结合起来，以提供具有内在复杂性正则化的计算高效的表示。所提出的方法是对局部回归技术的概率解释，通过一组局部线性或多项式单元来逼近非线性函数。重要的是，我们依赖于贝叶斯非参数的原则，构建了适应数据复杂性并且潜在地涵盖无限个模型的灵活模型。

    Well-calibrated probabilistic regression models are a crucial learning component in robotics applications as datasets grow rapidly and tasks become more complex. Unfortunately, classical regression models are usually either probabilistic kernel machines with a flexible structure that does not scale gracefully with data or deterministic and vastly scalable automata, albeit with a restrictive parametric form and poor regularization. In this paper, we consider a probabilistic hierarchical modeling paradigm that combines the benefits of both worlds to deliver computationally efficient representations with inherent complexity regularization. The presented approaches are probabilistic interpretations of local regression techniques that approximate nonlinear functions through a set of local linear or polynomial units. Importantly, we rely on principles from Bayesian nonparametrics to formulate flexible models that adapt their complexity to the data and can potentially encompass an infinite nu
    
[^112]: 基于MLP-Mixer神经网络的多视图多标签异常网络流量分类

    Multi-view Multi-label Anomaly Network Traffic Classification based on MLP-Mixer Neural Network. (arXiv:2210.16719v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16719](http://arxiv.org/abs/2210.16719)

    本文提出了一种基于MLP-Mixer的多视图多标签神经网络用于网络流量分类，该方法能够更好地捕捉流量数据的全局信息关联，通过利用不同场景之间的相关性来提高分类性能。

    

    网络流量分类是许多网络安全应用的基础，已经在网络安全领域引起了足够的关注。现有基于卷积神经网络（CNN）的网络流量分类通常强调流量数据的局部模式，而忽视了全局信息的关联。本文提出了一种基于MLP-Mixer的多视图多标签神经网络用于网络流量分类。与现有的基于CNN的方法相比，我们的方法采用了MLP-Mixer结构，这与数据包的结构更加符合。在我们的方法中，一个数据包被分为数据包头和数据包主体，加上不同视图下的流特征作为输入。我们利用多标签设置同时学习不同场景，通过利用不同场景之间的相关性来提高分类性能。借助于

    Network traffic classification is the basis of many network security applications and has attracted enough attention in the field of cyberspace security. Existing network traffic classification based on convolutional neural networks (CNNs) often emphasizes local patterns of traffic data while ignoring global information associations. In this paper, we propose an MLP-Mixer based multi-view multi-label neural network for network traffic classification. Compared with the existing CNN-based methods, our method adopts the MLP-Mixer structure, which is more in line with the structure of the packet than the conventional convolution operation. In our method, one packet is divided into the packet header and the packet body, together with the flow features of the packet as input from different views. We utilize a multi-label setting to learn different scenarios simultaneously to improve the classification performance by exploiting the correlations between different scenarios. Taking advantage of
    
[^113]: 有限时间内使用线性函数逼近进行时序差异学习的分析：尾平均和正则化

    Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05918](http://arxiv.org/abs/2210.05918)

    本研究通过引入尾平均和正则化技术，对时序差异(TD)学习算法进行了有限时间行为的研究。我们得出结论，尾平均TD能以最优速率 $O(1/t)$ 收敛，并且初始误差衰减速率更快。此外，正则化的TD版本在具有病态特征的问题上很有用。

    

    本文研究了将流行的时序差异(TD)学习算法与尾平均相结合时的有限时间行为。我们在不需要关于底层投影TD不动点矩阵的特征值信息的步长选择下，推导了尾平均TD迭代的参数误差的有限时间界。我们的分析表明，尾平均TD以期望速率和高概率收敛于最优的 $O(1/t)$ 速率。此外，我们的界限展示了初始误差(偏差)的更快衰减速率，这是对所有迭代的平均值的改进。我们还提出并分析了一种结合正则化的TD变体。通过分析，我们得出结论认为正则化的TD版本在具有病态特征的问题上是有用的。

    We study the finite-time behaviour of the popular temporal difference (TD) learning algorithm when combined with tail-averaging. We derive finite time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Our analysis shows that tail-averaged TD converges at the optimal $O\left(1/t\right)$ rate, both in expectation and with high probability. In addition, our bounds exhibit a sharper rate of decay for the initial error (bias), which is an improvement over averaging all iterates. We also propose and analyse a variant of TD that incorporates regularisation. From analysis, we conclude that the regularised version of TD is useful for problems with ill-conditioned features.
    
[^114]: jsdp: 一个Java随机动态规划库。

    jsdp: a Java Stochastic Dynamic Programming Library. (arXiv:2209.09979v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.09979](http://arxiv.org/abs/2209.09979)

    jsdp是一个Java库，通过利用Java中的lambda表达式、函数接口、集合和聚合运算符，实现了对随机动态规划问题的建模和求解。

    

    随机规划是一种用于建模和解决不确定性决策问题的框架。随机动态规划是随机规划的一个分支，采用“函数方程”方法来发现最优策略。通过利用Java中实现的lambda表达式、函数接口、集合和聚合运算符来操作MapReduce框架，jsdp提供了一个通用的库，用于建模和解决随机动态规划问题。

    Stochastic Programming is a framework for modelling and solving problems of decision making under uncertainty. Stochastic Dynamic Programming is a branch of Stochastic Programming that takes a "functional equation" approach to the discovery of optimal policies. By leveraging constructs - lambda expressions, functional interfaces, collections and aggregate operators - implemented in Java to operationalise the MapReduce framework, jsdp provides a general purpose library for modelling and solving Stochastic Dynamic Programs.
    
[^115]: 从计算机视觉系统的表现预测儿童的词语学习

    Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.09847](http://arxiv.org/abs/2207.09847)

    通过使用计算机视觉系统的表现作为预测儿童词语学习的难度的代理，我们发现儿童获得不同类别的词语的年龄与视觉分类和字幕系统的表现有关。这些模型捕捉到了词语与视觉现象之间的关系。

    

    对于儿童和机器学习系统来说，学习一个词最关键的挑战是将该词与描述的视觉现象联系起来。我们通过使用计算机视觉系统的表现作为从视觉线索学习一个词的难度的代理来探究词语学习的这个方面。我们发现儿童获得不同类别的词语的年龄与视觉分类和字幕系统的表现相关，超出了词语频率预期效应。计算机视觉系统的表现与词语的具体性的人类判断相关，而具体性又是儿童词语学习的预测因素，这表明这些模型捕捉到了词语与视觉现象之间的关系。

    For human children as well as machine learning systems, a key challenge in learning a word is linking the word to the visual phenomena it describes. We explore this aspect of word learning by using the performance of computer vision systems as a proxy for the difficulty of learning a word from visual cues. We show that the age at which children acquire different categories of words is correlated with the performance of visual classification and captioning systems, over and above the expected effects of word frequency. The performance of the computer vision systems is correlated with human judgments of the concreteness of words, which are in turn a predictor of children's word learning, suggesting that these models are capturing the relationship between words and visual phenomena.
    
[^116]: 评估异构设备在进行AI推理时的功耗

    Estimating the Power Consumption of Heterogeneous Devices when performing AI Inference. (arXiv:2207.06150v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2207.06150](http://arxiv.org/abs/2207.06150)

    本论文评估了使用NVIDIA Jetson Nano开发板进行目标分类时的功耗情况，并通过分析发现，YOLOv5n模型在每秒输出帧数和低功耗方面表现优于其他YOLOV5变体。

    

    现代生活受到连接到互联网的电子设备的驱动。物联网（IoT）这个新兴的研究领域变得越来越受欢迎，连接设备的数量也在稳定增长。由于许多设备被用于执行计算机视觉任务，因此了解它们的功耗与性能之间的关系非常重要。我们报告了NVIDIA Jetson Nano开发板在执行目标分类时的功耗特性和分析。作者提供了关于每帧的功耗和每秒输出帧数（使用YOLOv5模型）的详细分析。结果表明，YOLOv5n在吞吐量（即每秒12.34帧）和低功耗（即每帧0.154mWh）方面优于其他YOLOV5变体。

    Modern-day life is driven by electronic devices connected to the internet. The emerging research field of the Internet-of-Things (IoT) has become popular, just as there has been a steady increase in the number of connected devices. Since many of these devices are utilised to perform CV tasks, it is essential to understand their power consumption against performance. We report the power consumption profile and analysis of the NVIDIA Jetson Nano board while performing object classification. The authors present an extensive analysis regarding power consumption per frame and the output in frames per second using YOLOv5 models. The results show that the YOLOv5n outperforms other YOLOV5 variants in terms of throughput (i.e. 12.34 fps) and low power consumption (i.e. 0.154 mWh/frame).
    
[^117]: 图上的多尺度Wasserstein最短路径过滤核心

    Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00979](http://arxiv.org/abs/2206.00979)

    这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。

    

    传统的最短路径图核心（SP）是最受欢迎的图核心之一。它将图分解为最短路径，并计算每个图中最短路径的频率。然而，SP面临两个主要挑战：首先，最短路径的三元表示失去了信息。其次，SP比较图时没有考虑到图结构的多个不同尺度，而这在现实世界的图中很常见，例如社交网络中的链状结构、环状结构和星状结构。为了克服这两个挑战，我们开发了一种新颖的最短路径图核心，称为多尺度Wasserstein最短路径过滤图核心（MWSPF）。它使用以每个顶点为根的某个深度的BFS树来限制考虑最短路径的最大长度，考虑到小世界特性。它考虑了最短路径中所有顶点的标签。为了方便在多个不同尺度上比较图，它从顶点和

    The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
    
[^118]: 更有效地攻击c-MARL：一种数据驱动的方法

    Attacking c-MARL More Effectively: A Data Driven Approach. (arXiv:2202.03558v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03558](http://arxiv.org/abs/2202.03558)

    本研究提出了一种数据驱动的方法来评估合作多智能体强化学习 (c-MARL) 代理的稳健性，并通过模型构建更强大的对抗状态扰动以降低团队总奖励。同时，还提出了首个受害代理选择策略和数据驱动方法，以定义目标失败状态，从而实现更强大的对抗性攻击。实验证明，该方法在被测试的所有环境中一致优于其他基线。

    

    最近几年，针对合作多智能体强化学习（c-MARL）已经涌现了很多方法。然而，c-MARL代理在面对对抗攻击时的稳健性很少被研究。本文提出了一种名为 c-MBA 的基于模型的方法，用于评估 c-MARL 代理的稳健性。我们的方法可以产生比现有的基于模型的方法更强大的对抗性状态扰动，从而降低团队总奖励。另外，我们提出了第一个受害代理的选择策略和第一个基于数据驱动的方法来定义目标失败状态，每个失败状态都能帮助我们开发更强大的对抗性攻击，而无需对底层环境拥有专业知识。我们在两个代表性的 MARL 测试基准上进行了数值实验，结果表明我们的方法在所有测试环境中都优于其他基线：我们的基于模型的攻击在所有测试环境中都持续优于其他基线。

    In recent years, a proliferation of methods were developed for cooperative multi-agent reinforcement learning (c-MARL). However, the robustness of c-MARL agents against adversarial attacks has been rarely explored. In this paper, we propose to evaluate the robustness of c-MARL agents via a model-based approach, named c-MBA. Our proposed formulation can craft much stronger adversarial state perturbations of c-MARL agents to lower total team rewards than existing model-free approaches. In addition, we propose the first victim-agent selection strategy and the first data-driven approach to define targeted failure states where each of them allows us to develop even stronger adversarial attack without the expert knowledge to the underlying environment. Our numerical experiments on two representative MARL benchmarks illustrate the advantage of our approach over other baselines: our model-based attack consistently outperforms other baselines in all tested environments.
    
[^119]: Temporal Difference学习算法的控制论分析

    Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.14417](http://arxiv.org/abs/2112.14417)

    本研究对Temporal Difference学习算法进行了控制论分析，并引入了一个有限时间的框架，从控制论角度提供了对TD学习机制和强化学习领域的更深入洞察。

    

    本文旨在对Temporal Difference (TD)学习算法进行控制论分析。TD学习作为强化学习领域的基石，提供了一种近似计算与马尔科夫决策过程中给定策略相关的值函数的方法。尽管已存在多篇关于TD学习理论理解的研究成果，但直到最近几年，研究人员才能对其统计效率提供具体保证。本文引入了一个有限时间的控制论框架，用于分析TD学习，借鉴了线性系统控制领域的已有概念。因此，本文通过使用控制论导出的简单分析工具，为TD学习的机制和强化学习的更广阔领域提供了额外的洞察。

    The goal of this manuscript is to conduct a controltheoretic analysis of Temporal Difference (TD) learning algorithms. TD-learning serves as a cornerstone in the realm of reinforcement learning, offering a methodology for approximating the value function associated with a given policy in a Markov Decision Process. Despite several existing works that have contributed to the theoretical understanding of TD-learning, it is only in recent years that researchers have been able to establish concrete guarantees on its statistical efficiency. In this paper, we introduce a finite-time, control-theoretic framework for analyzing TD-learning, leveraging established concepts from the field of linear systems control. Consequently, this paper provides additional insights into the mechanics of TD learning and the broader landscape of reinforcement learning, all while employing straightforward analytical tools derived from control theory.
    
[^120]: 鲁棒的特征级对抗是可解释性工具

    Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03605](http://arxiv.org/abs/2110.03605)

    鲁棒的特征级对抗攻击不仅提供了对模型表示的研究，还具有独特的多功能性和高度鲁棒性，可以用于各种规模的图像攻击，并且可以作为可解释性工具帮助识别网络中的错误。

    

    计算机视觉中对抗攻击的文献通常关注像素级扰动，这些扰动往往很难解释。最近的研究通过操纵图像生成器的潜在表示来创建“特征级”对抗扰动，为我们提供了探索可感知、可解释的对抗攻击的机会。我们做出了三个贡献。首先，我们观察到特征级对抗攻击提供了用于研究模型表示的有用输入类别。第二，我们展示了这些对抗攻击的独特多功能性和高度鲁棒性。我们证明它们可以用于在ImageNet规模上产生有针对性、通用性、伪装性、物理可实现性和黑盒攻击。第三，我们展示了如何将这些对抗图像用作实际的可解释性工具，用于识别网络中的错误。我们利用这些对抗攻击对特征和类别之间的虚假关联进行预测，然后通过设计“...

    The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "
    
[^121]: 知识增强预训练模型综述

    A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.00269](http://arxiv.org/abs/2110.00269)

    本综述提供了关于NLP中知识增强预训练语言模型的综合概述，讨论了预训练语言模型和知识表示学习的进展，并从三个不同的角度对现有的KEPLMs进行了分类，最后概述了未来研究中KEPLMs的潜在方向。

    

    预训练语言模型通过自监督学习在大规模文本语料库上学习了信息丰富的词表示，在细调之后在自然语言处理领域取得了有希望的性能。然而，这些模型存在鲁棒性差和可解释性不足的问题。我们将注入知识的预训练语言模型称为知识增强预训练语言模型(KEPLMs)。这些模型表现出深入理解和逻辑推理，并引入了可解释性。在本综述中，我们提供了关于NLP中KEPLMs的综合概述。我们首先讨论了预训练语言模型和知识表示学习的进展。然后，我们从三个不同的角度系统地分类了现有的KEPLMs。最后，我们概述了一些未来研究中KEPLMs的潜在方向。

    Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.
    
[^122]: MCML：一种新颖的基于内存的对比元学习方法用于小样本槽位标记

    MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging. (arXiv:2108.11635v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2108.11635](http://arxiv.org/abs/2108.11635)

    MCML是一种基于内存的对比元学习方法，通过使用显式内存库来跟踪先前训练的剧集的标签表示，并引入了“从内存中适应”的机制，以克服小样本槽位标记中的“样本遗忘问题”。

    

    元学习在小样本学习的槽位标记任务中被广泛应用。然而，现有方法的性能受到了“样本遗忘问题”的严重影响，即模型在适应新任务时仅依赖支持集而忘记了历史学习的元训练任务。为了克服这一困境，我们提出了一种名为MCML（Memory-based Contrastive Meta-Learning）的方法，包括“从内存中学习”和“从内存中适应”两个模块，分别在训练剧集之间和训练与测试之间建立了分布差异。具体来说，前者使用显式内存库来跟踪先前训练的剧集的标签表示，并在当前剧集的标签表示与内存中存储的历史剧集进行对比约束。此外，引入了“从内存中适应”的机制来学习...

    Meta-learning is widely used for few-shot slot tagging in task of few-shot learning. The performance of existing methods is, however, seriously affected by \textit{sample forgetting issue}, where the model forgets the historically learned meta-training tasks while solely relying on support sets when adapting to new tasks. To overcome this predicament, we propose the \textbf{M}emory-based \textbf{C}ontrastive \textbf{M}eta-\textbf{L}earning (aka, MCML) method, including \textit{learn-from-the-memory} and \textit{adaption-from-the-memory} modules, which bridge the distribution gap between training episodes and between training and testing respectively. Specifically, the former uses an explicit memory bank to keep track of the label representations of previously trained episodes, with a contrastive constraint between the label representations in the current episode with the historical ones stored in the memory. In addition, the \emph{adaption-from-memory} mechanism is introduced to learn 
    
[^123]: 电网中智能GPS欺骗攻击检测

    Intelligent GPS Spoofing Attack Detection in Power Grids. (arXiv:2005.04513v1 [eess.SY] CROSS LISTED)

    [http://arxiv.org/abs/2005.04513](http://arxiv.org/abs/2005.04513)

    本文提出了一种使用动态电力系统中PMU数据的神经网络GPS欺骗检测（NNGSD）方法，用于检测电网中的GPS欺骗攻击，并展示了该检测方法的实时性能。

    

    GPS容易受到GPS欺骗攻击（GSA），从而导致GPS接收器的时间和位置结果出现混乱。在电网中，相量测量装置（PMUs）使用GPS构建时间标记的测量结果，因此它们容易受到这种攻击的影响。本文提出了一种利用动态电力系统中PMU数据的神经网络GPS欺骗检测（NNGSD）方法来检测GSA。不同条件下的数值结果显示了所提出的检测方法的实时性能。

    The GPS is vulnerable to GPS spoofing attack (GSA), which leads to disorder in time and position results of the GPS receiver. In power grids, phasor measurement units (PMUs) use GPS to build time-tagged measurements, so they are susceptible to this attack. As a result of this attack, sampling time and phase angle of the PMU measurements change. In this paper, a neural network GPS spoofing detection (NNGSD) with employing PMU data from the dynamic power system is presented to detect GSAs. Numerical results in different conditions show the real-time performance of the proposed detection method.
    

