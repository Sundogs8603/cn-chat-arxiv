# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Guidance Graph Optimization for Lifelong Multi-Agent Path Finding](https://rss.arxiv.org/abs/2402.01446) | 这项研究探索了如何利用导引图优化长期多智能体路径规划，提出了两种自动生成导引的算法，并解决了如何自动生成良好导引的问题。 |
| [^2] | [AI-generated faces free from racial and gender stereotypes](https://rss.arxiv.org/abs/2402.01002) | 这项研究发现并解决了AI生成的面孔中存在的种族和性别刻板印象问题，提出了分类器用于预测面部属性的方法，并提出了有效的去偏见解决方案。 |
| [^3] | [Analyzing the Roles of Language and Vision in Learning from Limited Data](https://arxiv.org/abs/2403.19669) | 研究人工智能中的复杂视觉-语言模型，发现即使缺乏视觉输入，利用所有组件的语言模型能够恢复大部分VLM的性能，表明语言通过提供对先前知识和推理的访问来对学习新任务有贡献 |
| [^4] | [From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?](https://arxiv.org/abs/2403.11894) | 该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。 |
| [^5] | [Robust Influence-based Training Methods for Noisy Brain MRI](https://arxiv.org/abs/2403.10698) | 该研究提出了两种基于影响函数的稳健训练方法，针对噪声MRI图像进行脑部肿瘤分类，可提高模型的鲁棒性。 |
| [^6] | [Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides](https://arxiv.org/abs/2402.17531) | Nissist利用TSGs和事故缓解历史提供主动建议，减少人为干预，以提高企业级云服务的事故管理效率。 |
| [^7] | [An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey](https://arxiv.org/abs/2402.17045) | 分析了过去10年针对不同类型网络攻击检测的各种最先进机器学习模型，着重比较了最新的工作。 |
| [^8] | [Information-Theoretic Safe Bayesian Optimization](https://arxiv.org/abs/2402.15347) | 提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。 |
| [^9] | [Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales](https://arxiv.org/abs/2312.07399) | 该论文提出了一种基于提示生成的理由的“推理感知”诊断框架，通过大型语言模型来进行临床推理，实现了在疾病诊断过程中的高效、时间节约和劳动节约的方法。 |
| [^10] | [Procedural Fairness Through Decoupling Objectionable Data Generating Components](https://arxiv.org/abs/2311.14688) | 通过解耦可抗议的数据生成组件，本研究提出了一个框架来防止伪装的程序不公平，并强调了满足程序公平要求的重要性 |
| [^11] | [Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification](https://arxiv.org/abs/2311.10319) | 引入了S4MI流程，利用自监督和半监督学习的高效方法，能够简化医学图像的机器监督过程，自监督学习在分类任务中表现明显优于监督方法。 |
| [^12] | [Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition.](http://arxiv.org/abs/2401.15108) | 本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。 |
| [^13] | [Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality.](http://arxiv.org/abs/2401.09556) | 本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。 |
| [^14] | [From Prompt Engineering to Prompt Science With Human in the Loop.](http://arxiv.org/abs/2401.04122) | 基于代码书构建方法和多阶段验证过程，本文提出一种新的方法来更系统地在研究中使用LLMs。 |
| [^15] | [Neurosymbolic Grounding for Compositional World Models.](http://arxiv.org/abs/2310.12690) | 本论文介绍了一种名为Cosmos的框架，用于对象为中心的世界建模，通过使用神经符号化基础和视觉-语言基础模型，实现了在未见过的输入场景上的高性能组合泛化能力。 |
| [^16] | [Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies.](http://arxiv.org/abs/2309.13063) | 通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。 |
| [^17] | [Invariant Learning via Probability of Sufficient and Necessary Causes.](http://arxiv.org/abs/2309.12559) | 本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。 |
| [^18] | [Calibration in Deep Learning: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2308.01222) | 本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。 |
| [^19] | [Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models.](http://arxiv.org/abs/2303.16045) | 本论文提出了一种基于人工通用智能方法原则的通用单变量信号去卷积方法。通过计算“通用分布”的估计来独立于概率分布地构建一个通用模型，并基于信息论和算法概率的多维空间重构，探索非随机数据中关于物理性质的信息编码。该方法在编码理论尤其是零失真压缩方面有应用价值。 |
| [^20] | [Individual fairness under Varied Notions of Group Fairness in Bipartite Matching -- One Framework to Approximate Them Al.](http://arxiv.org/abs/2208.09951) | 本文研究在满足群体和个体公平性约束的情况下分配物品给平台的问题，并提出了一种近似框架，可以用来近似文献中提出的群体公平性概念，同时实现个体公平性。 |

# 详细

[^1]: 长期多智能体路径规划中的导引图优化研究

    Guidance Graph Optimization for Lifelong Multi-Agent Path Finding

    [https://rss.arxiv.org/abs/2402.01446](https://rss.arxiv.org/abs/2402.01446)

    这项研究探索了如何利用导引图优化长期多智能体路径规划，提出了两种自动生成导引的算法，并解决了如何自动生成良好导引的问题。

    

    我们研究如何利用导引来提高长期多智能体路径规划（MAPF）的吞吐量。先前的研究表明，尽管将导引（如高速公路）纳入MAPF算法可以加速计算，但这往往会与解决方案质量产生折中。此外，如何自动生成良好的导引仍然尚未充分探索，当前的方法还无法超越手动设计的导引。在这项工作中，我们引入了有向导引图作为长期MAPF中引导的通用表示，并将导引图优化（GGO）的任务定义为优化其边权重。我们提出了两种GGO算法，用于自动生成适用于任意长期MAPF算法和地图的导引。第一种方法直接使用CMA-ES（一种黑箱优化算法）来解决GGO问题。第二种方法PIU通过优化一个能够生成导引的更新模型，展示了将优化过的导引图传播到较大地图中的能力。

    We study how to use guidance to improve the throughput of lifelong Multi-Agent Path Finding (MAPF). Previous studies have demonstrated that while incorporating guidance, such as highways, can accelerate MAPF algorithms, this often results in a trade-off with solution quality. In addition, how to generate good guidance automatically remains largely unexplored, with current methods falling short of surpassing manually designed ones. In this work, we introduce the directed guidance graph as a versatile representation of guidance for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of optimizing its edge weights. We present two GGO algorithms to automatically generate guidance for arbitrary lifelong MAPF algorithms and maps. The first method directly solves GGO by employing CMA-ES, a black-box optimization algorithm. The second method, PIU, optimizes an update model capable of generating guidance, demonstrating the ability to transfer optimized guidance graphs to larger
    
[^2]: AI生成的面孔摆脱了种族和性别刻板印象

    AI-generated faces free from racial and gender stereotypes

    [https://rss.arxiv.org/abs/2402.01002](https://rss.arxiv.org/abs/2402.01002)

    这项研究发现并解决了AI生成的面孔中存在的种族和性别刻板印象问题，提出了分类器用于预测面部属性的方法，并提出了有效的去偏见解决方案。

    

    诸如Stable Diffusion之类的文本到图像生成AI模型每天都被全球数百万人使用。然而，许多人对这些模型如何放大种族和性别刻板印象提出了关切。为了研究这一现象，我们开发了一个分类器来预测任意给定面部图像的种族、性别和年龄组，并展示其达到了最先进的性能。利用这个分类器，我们对Stable Diffusion在六种种族、两种性别、五个年龄组、32个职业和八个属性上的偏见进行了量化。然后，我们提出了超越最先进替代方案的新型去偏见解决方案。此外，我们还检查了Stable Diffusion在描绘同一种族的个体时相似程度。分析结果显示出高度的刻板印象，例如，将大多数中东男性描绘为皮肤黝黑、留着胡子、戴着传统头饰。我们提出了另一种增加面部多样性的新型解决方案来解决这些限制。

    Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, many have raised concerns regarding how these models amplify racial and gender stereotypes. To study this phenomenon, we develop a classifier to predict the race, gender, and age group of any given face image, and show that it achieves state-of-the-art performance. Using this classifier, we quantify biases in Stable Diffusion across six races, two genders, five age groups, 32 professions, and eight attributes. We then propose novel debiasing solutions that outperform state-of-the-art alternatives. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals a high degree of stereotyping, e.g., depicting most middle eastern males as being dark-skinned, bearded, and wearing a traditional headdress. We address these limitations by proposing yet another novel solution that increases facial div
    
[^3]: 分析语言和视觉在从有限数据中学习中的作用

    Analyzing the Roles of Language and Vision in Learning from Limited Data

    [https://arxiv.org/abs/2403.19669](https://arxiv.org/abs/2403.19669)

    研究人工智能中的复杂视觉-语言模型，发现即使缺乏视觉输入，利用所有组件的语言模型能够恢复大部分VLM的性能，表明语言通过提供对先前知识和推理的访问来对学习新任务有贡献

    

    arXiv:2403.19669v1 公告类型：交叉摘要：语言是否有助于理解视觉世界？实际观察世界需要看到实际情况，而不是用文字描述吗？关于智能本质的这些基本问题很难回答，因为我们只有一个智能系统的例子——人类——以及有限的独立语言或视觉的案例。然而，人工智能研究人员开发出复杂的视觉-语言模型（VLMs）为我们提供了新的机会，探索语言和视觉对于学习世界的贡献。我们从这些模型的认知架构中切除组件，以确定它们对从有限数据中学习新任务的贡献。我们发现，利用所有组件的语言模型恢复了大部分VLM的性能，尽管它缺乏视觉输入，而语言似乎可以通过提供对先前知识和推理的访问来实现这一点。

    arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
    
[^4]: 从可解释到可解释的深度学习在医疗自然语言处理中的应用：现实有多远？

    From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?

    [https://arxiv.org/abs/2403.11894](https://arxiv.org/abs/2403.11894)

    该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。

    

    深度学习（DL）通过解决各种自然语言处理（NLP）任务，极大地增强了医疗保健研究。然而，基于DL的NLP方法日益复杂，需要透明的模型解释性，或至少是可解释性，以进行可靠的决策制定。本文对医疗健康NLP中的可解释和可解释的DL进行了彻底的范围审查。引入了术语“XIAI”（eXplainable和Interpretable Artificial Intelligence）以区分XAI和IAI。方法根据其功能（模型、输入、输出为基础）和范围（局部、全局）进一步分类。我们的分析表明，注意机制是最主要的新兴IAI。此外，IAI越来越多地用于对抗XAI。确定的主要挑战是大多数XIAI不探索“全局”建模过程，缺乏最佳实践，并且需要系统评估和基准测试。

    arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
    
[^5]: 针对噪声脑部MRI的稳健基于影响力的训练方法

    Robust Influence-based Training Methods for Noisy Brain MRI

    [https://arxiv.org/abs/2403.10698](https://arxiv.org/abs/2403.10698)

    该研究提出了两种基于影响函数的稳健训练方法，针对噪声MRI图像进行脑部肿瘤分类，可提高模型的鲁棒性。

    

    正确分类脑部肿瘤对及时和准确治疗患者至关重要。虽然已经有几种基于经典图像处理或深度学习方法的分类算法被提出来快速分类MR图像中的肿瘤，但大多数假定了训练数据是无噪声的不切实际情况。在这项工作中，我们研究了在噪声的MR图像上训练深度学习模型以分类脑部肿瘤的困难但现实的设置。我们提出了两种稳健于噪声MRI训练数据的训练方法，即基于影响力的样本重新加权（ISR）和基于影响力的样本扰动（ISP），这两种方法都基于鲁棒统计中的影响函数。在ISR中，我们根据训练过程中样本对训练的帮助/危害程度自适应地重新加权训练样本，而在ISP中，我们根据影响得分量身定制并注入有帮助的扰动。ISR和ISP均可以提高模型的鲁棒性。

    arXiv:2403.10698v1 Announce Type: cross  Abstract: Correctly classifying brain tumors is imperative to the prompt and accurate treatment of a patient. While several classification algorithms based on classical image processing or deep learning methods have been proposed to rapidly classify tumors in MR images, most assume the unrealistic setting of noise-free training data. In this work, we study a difficult but realistic setting of training a deep learning model on noisy MR images to classify brain tumors. We propose two training methods that are robust to noisy MRI training data, Influence-based Sample Reweighing (ISR) and Influence-based Sample Perturbation (ISP), which are based on influence functions from robust statistics. Using the influence functions, in ISR, we adaptively reweigh training examples according to how helpful/harmful they are to the training process, while in ISP, we craft and inject helpful perturbation proportional to the influence score. Both ISR and ISP harden
    
[^6]: Nissist：基于故障排除指南的事故缓解副驾驶

    Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides

    [https://arxiv.org/abs/2402.17531](https://arxiv.org/abs/2402.17531)

    Nissist利用TSGs和事故缓解历史提供主动建议，减少人为干预，以提高企业级云服务的事故管理效率。

    

    有效的事故管理对企业级云服务的顺畅运作至关重要。 为了加速事故缓解，服务团队将故障排除知识编译成供值班工程师（OCEs）访问的故障排除指南（TSGs）。 尽管自动化流水线已能够解决最常见和简单的事故，但仍存在需要OCE干预的复杂事故。 然而，TSGs通常是非结构化和不完整的，这需要OCE手动解释，导致值班疲劳和生产力下降，特别是新入职的OCE。 在这项工作中，我们提出了Nissist，它利用TSGs和事故缓解历史提供主动建议，减少人为干预。 利用大型语言模型（LLM），Nissist从非结构化TSGs和历史事故缓解讨论中提取见解，形成全面的知识库。

    arXiv:2402.17531v1 Announce Type: cross  Abstract: Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention. Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-a
    
[^7]: 对各类网络攻击检测的最先进机器学习方法性能的研究：一项调查

    An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey

    [https://arxiv.org/abs/2402.17045](https://arxiv.org/abs/2402.17045)

    分析了过去10年针对不同类型网络攻击检测的各种最先进机器学习模型，着重比较了最新的工作。

    

    为了保护计算机和信息系统免受攻击者利用系统中的漏洞进行网络犯罪的侵害，已经提出了几种用于实时检测漏洞以提高信息系统安全性的方法。在所有提出的方法中，机器学习是实现系统安全的效果最好的方法，其能力范围从早期检测软件漏洞到实时检测系统中正在进行的妥协。由于存在不同类型的网络攻击，每种现有最先进的机器学习模型都依赖于不同的训练算法，这也影响了它们对特定类型网络攻击的适用性。在这项研究中，我们分析了过去10年针对不同类型网络攻击检测的每一个当前最先进的机器学习模型，重点放在最近的工作上以进行比较。

    arXiv:2402.17045v1 Announce Type: cross  Abstract: To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparat
    
[^8]: 信息论安全贝叶斯优化

    Information-Theoretic Safe Bayesian Optimization

    [https://arxiv.org/abs/2402.15347](https://arxiv.org/abs/2402.15347)

    提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。

    

    我们考虑了一个顺序决策任务，其目标是在不评估违反先验未知（安全）约束的参数的情况下优化未知函数。一个常见的方法是在未知函数上放置高斯过程先验，并且仅允许在高概率安全区域内进行评估。大多数当前方法依赖于对域的离散化，并且不能直接扩展到连续情况。此外，它们利用约束的规则假设的方式引入了一个额外的关键超参数。在本文中，我们提出了一个信息论安全探索准则，该准则直接利用GP后验来识别最具信息的安全参数进行评估。将这一探索准则与众所周知的贝叶斯优化收益函数结合起来，产生了一种新颖的安全贝叶斯优化选择准则。

    arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
    
[^9]: 大型语言模型是临床推理者：基于提示生成的理由的推理感知诊断框架

    Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales

    [https://arxiv.org/abs/2312.07399](https://arxiv.org/abs/2312.07399)

    该论文提出了一种基于提示生成的理由的“推理感知”诊断框架，通过大型语言模型来进行临床推理，实现了在疾病诊断过程中的高效、时间节约和劳动节约的方法。

    

    由于大型语言模型（LLMs）的进展，机器推理在近年来取得了巨大的进展。然而，在临床领域，大多数以自然语言处理为驱动的项目主要集中在临床分类或阅读理解上，并且由于与临床医生的理念注解成本较高，对于疾病诊断的临床推理还未得到充分的研究。在这项工作中，我们提出了一个“推理感知”的诊断框架，通过基于提示的学习以一种高效的时间和劳动方式去理性化诊断过程，并学习对提示生成的理由进行推理。具体而言，我们解决了疾病诊断的临床推理问题，其中LLM生成了诊断性的理由，提供其对呈现的患者数据的见解以及达到诊断的推理路径，即临床思维链（Clinical CoT）。我们通过广泛的实验和分析在理由生成和疾病诊断方面实证了LLMs/LMs的临床推理能力。

    Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various s
    
[^10]: 通过解耦可抗议的数据生成组件来实现程序公平

    Procedural Fairness Through Decoupling Objectionable Data Generating Components

    [https://arxiv.org/abs/2311.14688](https://arxiv.org/abs/2311.14688)

    通过解耦可抗议的数据生成组件，本研究提出了一个框架来防止伪装的程序不公平，并强调了满足程序公平要求的重要性

    

    我们揭示并解决了经常被忽视但重要的问题，即伪装的程序不公平，即对数据生成过程中的中立（即不成问题的）方面的可能无意的改变，和/或对最不利利益个体的实现没有程序保证。受约翰·罗尔斯对纯程序公正的倡导启发，我们将自动决策视为社会制度的缩影，并考虑数据生成过程本身如何满足程序公平的要求。我们提出了一个框架，通过利用参考点和相关的价值实例化规则，将可抗议的数据生成组件与中立的数据生成组件解耦。我们的发现强调了防止伪装的程序不公平的必要性，不仅引起了我们力图缓解的可抗议的数据生成组件的注意

    arXiv:2311.14688v2 Announce Type: replace-cross  Abstract: We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for pure procedural justice, we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing disguised procedural unfairness, drawing attention not only to the objectionable data generating components that we aim to mitiga
    
[^11]: 转向机器监督：用于自动医学图像分割和分类的标注高效的半监督和自监督学习

    Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification

    [https://arxiv.org/abs/2311.10319](https://arxiv.org/abs/2311.10319)

    引入了S4MI流程，利用自监督和半监督学习的高效方法，能够简化医学图像的机器监督过程，自监督学习在分类任务中表现明显优于监督方法。

    

    临床治疗的进展越来越受到监督学习技术的限制，这些技术严重依赖于大量标注数据。标注过程不仅成本高昂，而且需要临床专家大量时间。为了解决这一问题，我们引入了S4MI（医学图像的自监督和半监督）流程，这是一种利用自监督和半监督学习的发展的新方法。这些技术参与不需要标记的辅助任务，从而简化了机器监督的扩展，相比完全监督的方法。我们的研究在三个不同的医学图像数据集上对这些技术进行基准测试，以评估它们在分类和分割任务中的有效性。值得注意的是，我们发现自监督学习在分类任务中明显优于监督方法。

    arXiv:2311.10319v3 Announce Type: replace-cross  Abstract: Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages the advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self-supervised learning significantly surpassed the performance of supervised methods in the classificati
    
[^12]: 多智能体深度强化学习在竞争中为快速充电电动车中心的动态定价中的应用

    Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])

    [http://arxiv.org/abs/2401.15108](http://arxiv.org/abs/2401.15108)

    本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。

    

    快速充电站将成为全球新建交通电气化基础设施的一部分。这些充电站将承载许多直流快速充电设备，仅可供电动车辆充电使用。类似于汽油加油站，同一地区的快速充电站将根据竞争调整价格以吸引同一群电动车主。这些充电站将与电力网络进行交互，通过预测性购买在前一天电力市场上的电力需求，并在实时市场上满足差额需求。充电站可能配备补充电池储能系统用于套利。本文针对充电站竞争中开发了一个两步数据驱动的动态定价方法。首先通过求解随机的前一天电力需求模型得到纳入承诺，然后通过将游戏建模为竞争来得到充电站的价格策略。

    Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
    
[^13]: 深度学习增强的混合整数优化: 学习减少模型维度

    Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])

    [http://arxiv.org/abs/2401.09556](http://arxiv.org/abs/2401.09556)

    本研究介绍了一种利用深度学习解决混合整数优化问题的框架，通过训练神经网络来预测活动维度，从而最大化全局最优解的出现频率。

    

    本研究介绍了一种利用深度学习解决混合整数规划模型中的计算复杂性的框架。我们比较了前馈神经网络(ANN)和卷积神经网络(CNN)在近似混合整数规划问题中的作用。我们利用多标签分类来考虑多个活动维度。为了提高框架的性能，我们采用贝叶斯优化进行超参数调优，以最大化样本级准确性。主要目标是训练神经网络准确地预测所有活动维度，从而最大化全局最优解的出现频率。我们将该框架应用于描述个性化医学供应链中的长期投资规划和中期战术规划的基于流的设施位置分配混合整数线性规划(MILP)问题。

    This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
    
[^14]: 从提示工程到人在循环中的提示科学

    From Prompt Engineering to Prompt Science With Human in the Loop. (arXiv:2401.04122v1 [cs.HC])

    [http://arxiv.org/abs/2401.04122](http://arxiv.org/abs/2401.04122)

    基于代码书构建方法和多阶段验证过程，本文提出一种新的方法来更系统地在研究中使用LLMs。

    

    随着LLMs在我们生活的许多方面的应用，科学研究领域是一个需要增加审查的地方。LLMs被用于生成或分析研究数据的应用正变得越来越流行。但是，当这种应用被临时决策和工程解决方案所困扰时，我们需要关注它如何影响研究、研究结果或者基于该研究的任何未来工作。我们需要更科学的方法来在我们的研究中使用LLMs。虽然目前有一些积极的努力支持更系统的提示构建，但它们往往更注重实现期望的结果，而不是产生可复制和具有足够透明度、客观性或严谨性的广泛知识。本文提出了一种新的方法，灵感来自通过定性方法构建代码书的方法，以解决这个问题。通过人在循环和多阶段验证过程，该方法为更系统的研究打下了基础。

    As LLMs make their way into many aspects of our lives, one place that warrants increased scrutiny with LLM usage is scientific research. Using LLMs for generating or analyzing data for research purposes is gaining popularity. But when such application is marred with ad-hoc decisions and engineering solutions, we need to be concerned about how it may affect that research, its findings, or any future works based on that research. We need a more scientific approach to using LLMs in our research. While there are several active efforts to support more systematic construction of prompts, they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor. This article presents a new methodology inspired by codebook construction through qualitative methods to address that. Using humans in the loop and a multi-phase verification processes, this methodology lays a foundation for more systema
    
[^15]: 神经符号化基础上的组合式世界建模

    Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])

    [http://arxiv.org/abs/2310.12690](http://arxiv.org/abs/2310.12690)

    本论文介绍了一种名为Cosmos的框架，用于对象为中心的世界建模，通过使用神经符号化基础和视觉-语言基础模型，实现了在未见过的输入场景上的高性能组合泛化能力。

    

    我们引入了Cosmos，一个针对组合泛化（CG）设计的以对象为中心的世界建模框架，即在通过已知的视觉“原子”组合获得的未见过的输入场景上具有高性能。Cosmos的核心洞察力是使用一种新颖的神经符号化基础。具体来说，该框架引入了两个新工具：（i）神经符号化场景编码，使用神经编码器计算每个场景中的实体的实向量表示，并使用描述实体属性的可组合符号向量，以及（ii）神经符号化注意机制，将这些实体与学习到的交互规则绑定起来。Cosmos是端到端可微分的；此外，与传统的神经符号化方法需要手动将表示映射为符号不同，它使用视觉-语言基础模型计算实体的符号属性。通过对已建立的blocks场景进行两种不同形式的CG评估，我们验证了Cosmos的有效性。

    We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
    
[^16]: 使用大型语言模型生成、验证和应用用户意图分类方法

    Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])

    [http://arxiv.org/abs/2309.13063](http://arxiv.org/abs/2309.13063)

    通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。

    

    日志数据可以揭示用户与网络搜索服务的交互方式、用户的需求以及满意程度等宝贵信息。然而，分析日志数据中的用户意图并不容易，尤其是对于新的网络搜索形式，如人工智能驱动的聊天。为了理解日志数据中的用户意图，我们需要一种能够用有意义的分类方式标记它们的方法，以捕捉其多样性和动态性。现有的方法依赖于手动或基于机器学习的标注，这些方法对于大型且不断变化的数据集而言，要么代价高昂要么不够灵活。我们提出了一种使用大型语言模型(LLM)的新方法，这种模型能够生成丰富且相关的概念、描述和示例来表示用户意图。然而，使用LLM生成用户意图分类并将其应用于日志分析可能存在两个主要问题：这样的分类得不到外部验证，并且可能存在不良的反馈回路。为了克服这些问题，我们提出了一种新的方法，通过人工专家和评估者来验证。

    Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
    
[^17]: 通过充分因素和必要因素的概率进行不变学习

    Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])

    [http://arxiv.org/abs/2309.12559](http://arxiv.org/abs/2309.12559)

    本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。

    

    在野外学习中，对于未知的、与训练分布不同的测试分布，外部分布（OOD）泛化是不可或缺的。最近从因果性引发的方法在实现OOD泛化方面显示出了巨大的潜力。然而，现有方法主要关注因果性的不变性属性，而在很大程度上忽视了充分性和必要性条件的属性。换句话说，一个必要但不充分的原因（特征）对于分布转换是不变的，但可能没有所需的准确度。相反，一个充分但不必要的原因（特征）倾向于很好地适应特定数据，但可能存在适应新领域的风险。为了捕捉充分和必要因素的信息，我们采用了经典概念——充分和必要因素的概率（PNS），它指示了一个因素是必要和充分原因的概率。为了将PNS与OOD泛化联系起来，我们提出了一种方法

    Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
    
[^18]: 深度学习中的校准：最新研究综述

    Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])

    [http://arxiv.org/abs/2308.01222](http://arxiv.org/abs/2308.01222)

    本文回顾了深度学习中的校准方法的最新发展，并提供了对其原理的理解。研究表明，现代深度神经网络在预测能力上表现出色，但校准性较差，导致模型预测不可靠。因此，需要一些新的方法来改善模型的校准性。

    

    在构建可靠、鲁棒的安全关键应用的人工智能系统中，深度神经模型的校准起着重要作用。最近的研究表明，具有高预测能力的现代神经网络的校准性较差，产生不可靠的模型预测。尽管深度学习模型在各种基准测试中取得了显著的性能，但对模型的校准性和可靠性的研究相对较少。理想的深度模型不仅应具有高预测性能，还应具有良好的校准性。最近提出了一些使用不同机制进行深度模型校准的方法。在本综述中，我们回顾了最新的校准方法，并解释了它们执行模型校准的原理。首先，我们从模型校准的定义开始，解释了模型校准不准确的根本原因。然后，我们介绍了可以衡量模型校准性的关键指标。接下来，我们总结了一些校准方法的方法和实践。

    Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
    
[^19]: 大规模生成模型中的最优空间去卷积和信息重建

    Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models. (arXiv:2303.16045v1 [cs.IT])

    [http://arxiv.org/abs/2303.16045](http://arxiv.org/abs/2303.16045)

    本论文提出了一种基于人工通用智能方法原则的通用单变量信号去卷积方法。通过计算“通用分布”的估计来独立于概率分布地构建一个通用模型，并基于信息论和算法概率的多维空间重构，探索非随机数据中关于物理性质的信息编码。该方法在编码理论尤其是零失真压缩方面有应用价值。

    

    本论文介绍一种基于人工通用智能方法原则的通用单变量信号去卷积方法，该方法建立了一个模型生成模型，依赖于信息论和算法概率，并计算出“通用分布”的估计，从而独立于概率分布地构建了一个通用模型。该方法基于信息论和算法概率的多维空间重构，可以探究非随机数据如何编码关于物理性质的信息，例如信号或信息的维度和长度尺度。该方法是与可计算或半可计算的近似方法或编码-解码方案相关但不独立的。本文的结果对编码理论尤其是零失真压缩有应用意义。

    We introduce a general-purpose univariate signal deconvolution method based on the principles of an approach to Artificial General Intelligence. This approach is based on a generative model that combines information theory and algorithmic probability that required a large calculation of an estimation of a `universal distribution' to build a general-purpose model of models independent of probability distributions. This was used to investigate how non-random data may encode information about the physical properties such as dimension and length scales in which a signal or message may have been originally encoded, embedded, or generated. This multidimensional space reconstruction method is based on information theory and algorithmic probability, and it is agnostic, but not independent, with respect to the chosen computable or semi-computable approximation method or encoding-decoding scheme. The results presented in this paper are useful for applications in coding theory, particularly in ze
    
[^20]: 二分图匹配中的不同群体公平性下的个体公平性——一种近似框架

    Individual fairness under Varied Notions of Group Fairness in Bipartite Matching -- One Framework to Approximate Them Al. (arXiv:2208.09951v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.09951](http://arxiv.org/abs/2208.09951)

    本文研究在满足群体和个体公平性约束的情况下分配物品给平台的问题，并提出了一种近似框架，可以用来近似文献中提出的群体公平性概念，同时实现个体公平性。

    

    本文研究在满足群体和个体公平性约束的情况下分配物品给平台的问题。每个物品都与某些群体相关联，并且对平台有优先顺序。每个平台通过指定每个群体可以与之匹配的物品数量的上限和下限来执行群体公平性。尽管可能存在满足群体公平性约束的多个最优解，我们旨在通过计算一个分布来实现“随机个体公平性”，使得每个物品被匹配到其前几个选择之一的合理概率。当每个物品可以属于多个群体时，即使所有群体下限均为0且没有个体公平性约束，寻找最大大小群体公平匹配的问题也是NP-难的。对于一共$n$个物品，当一个物品最多属于$\Delta$个群体，并且所有群体的上限和下限都是常数时，我们实现了$O(\Delta \log n)$近似算法。我们还证明，对于所有群体公平性约束都是区间的特殊情况，我们可以高效地计算满足这些约束的个体公平匹配的分布。因此，我们的框架可以用于近似文献中提出的群体公平性概念，同时实现个体公平性。

    We consider the problem of assigning items to platforms while satisfying group and individual fairness constraints. Each item is associated with certain groups and has a preference ordering over platforms. Each platform enforces group fairness by specifying an upper and a lower bound on the number of items that can be matched to it from each group. Although there may be multiple optimal solutions that satisfy the group fairness constraints, we aim to achieve `probabilistic individual fairness' by computing a distribution over `group fair' matchings such that each item has a reasonable probability of being matched to one of its top choices. When each item can belong to multiple groups, the problem of finding a maximum size group-fair matching is NP-hard even when all the group lower bounds are 0, and there are no individual fairness constraints. Given a total of $n$ items, we achieve a $O(\Delta \log n)$ approximation algorithm when an item can belong to at most $\Delta$ groups, and all
    

