# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection.](http://arxiv.org/abs/2309.03893) | DiffusionEngine是一个可扩展的数据引擎，使用Diffusion模型进行目标检测。它通过一个预训练的模型和一个检测适配器，能够在单个阶段提供高质量的训练数据，并生成可扩展、多样化和可泛化的检测数据。 |
| [^2] | [A Function Interpretation Benchmark for Evaluating Interpretability Methods.](http://arxiv.org/abs/2309.03886) | 本文介绍了一个用于评估自动解释性方法的基准套件，该套件包括了类似于传统系统组件的函数。 |
| [^3] | [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models.](http://arxiv.org/abs/2309.03883) | DoLa通过对比不同层次的逻辑差异，提高大型语言模型中的真实性和减少幻觉，无需外部知识或微调。 |
| [^4] | [OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs.](http://arxiv.org/abs/2309.03876) | OpinionGPT是一个指令调整大型语言模型(LLMs)的web演示，用户可以选择各种偏见并进行比较，它意在使模型的偏见显性和透明化。 |
| [^5] | [FLM-101B: An Open LLM and How to Train It with $100K Budget.](http://arxiv.org/abs/2309.03852) | 本文介绍了一种开放的LLM模型（FLM-101B）以及如何用10万美元的预算来训练它。通过采用增长策略，可以显著降低LLM训练的成本。同时，引入了一种系统的评估方法，以评估LLM的智能能力。 |
| [^6] | [Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models.](http://arxiv.org/abs/2309.03831) | 这项研究提出了一种无监督的方法来检测和减轻机器学习模型中的漂移。通过编码生产数据样本和模型训练数据，以及利用核统计检验和最大均值差异（MMD）距离度量来比较分布，可以有效估计漂移情况。 |
| [^7] | [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization.](http://arxiv.org/abs/2309.03824) | 本文介绍了两种加速低秩分解模型的技术：秩优化和顺序冻结分解层。实验证明，这些技术可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%，同时保持准确性接近原始模型。 |
| [^8] | [AnthroNet: Conditional Generation of Humans via Anthropometrics.](http://arxiv.org/abs/2309.03812) | 通过人体比例测量构建的AnthroNet模型能够生成各种不同形状和姿势的人体，并以任意姿势生成特定人物身份的人体。该模型通过合成数据进行端到端训练，提供了高精度的人体网格表示和精确的人体比例测量。 |
| [^9] | [Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck.](http://arxiv.org/abs/2309.03800) | 本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。 |
| [^10] | [FisheyePP4AV: A privacy-preserving method for autonomous vehicles on fisheye camera images.](http://arxiv.org/abs/2309.03799) | FisheyePP4AV 是一种用于自动驾驶的鱼眼相机图像的隐私保护方法，解决了在实际道路驾驶场景中检测和匿名化行人面部和车牌的问题，并提供了一个框架用于从多个深度学习模型中提取人脸和车牌识别知识。 |
| [^11] | [CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning.](http://arxiv.org/abs/2309.03779) | 本研究通过基于时间编码的深度强化学习，开发了一种在嵌入式设备上进行实时应用的CPU频率调度方法，该方法可以在小型设备上推导出高效的功率管理方法，并解决了现有Linux内置方法的限制。 |
| [^12] | [Extending Transductive Knowledge Graph Embedding Models for Inductive Logical Relational Inference.](http://arxiv.org/abs/2309.03773) | 本文提出了一种扩展传导知识图嵌入方法的模型，用于处理归纳推理任务。通过引入广义的谐波扩展，利用传导嵌入方法学习的表示来推断在推理时引入的新实体的表示。 |
| [^13] | [Hybrid of representation learning and reinforcement learning for dynamic and complex robotic motion planning.](http://arxiv.org/abs/2309.03758) | 这项研究提出了一种混合算法，将表示学习和强化学习结合应用于动态和复杂的机器人运动规划。其中使用了长短期记忆（LSTM）汇聚和跳跃连接来改进离散软演员-评论家算法，并通过比较不同的表示方法得出注意力网络在任务中的优势。 |
| [^14] | [TSGBench: Time Series Generation Benchmark.](http://arxiv.org/abs/2309.03755) | TSGBench是首个时间序列生成基准，用于统一和全面评估TSG方法，解决了现有方法在性能评估、数据集选择和评估指标上的限制。 |
| [^15] | [Enhancing Pipeline-Based Conversational Agents with Large Language Models.](http://arxiv.org/abs/2309.03748) | 本文研究了如何使用大型语言模型（LLM）来增强基于流水线的对话系统。在设计和开发阶段，LLM可以帮助生成训练数据、提取实体和同义词、本地化和角色设计。在运营阶段，LLM可以辅助上下文化、意图分类、自动纠正话语、改写回复、摘要和使闭合问题回答能力。通过在私人银行领域的实验，证明了这些能力的有效性。 |
| [^16] | [A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism.](http://arxiv.org/abs/2309.03720) | 本文介绍了一个基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统，通过数据流处理，实现了多步 ahead 的预测和持续学习能力。在复杂的实际应用场景中，通过评估预测模型的性能，证明了该方法的有效性。 |
| [^17] | [PyGraft: Configurable Generation of Schemas and Knowledge Graphs at Your Fingertips.](http://arxiv.org/abs/2309.03685) | PyGraft是一个Python工具，可以根据需要生成高度定制的模式和知识图谱，并确保生成的资源的逻辑一致性。 |
| [^18] | [Dataset Generation and Bonobo Classification from Weakly Labelled Videos.](http://arxiv.org/abs/2309.03671) | 本研究提出了一种从弱标记视频中生成倭黑猩猩数据集并进行分类的方法，并探究了不同的特征提取和分类算法。研究结果表明，数据准备的重要性以及正确的数据分离对于分类性能的影响很大。 |
| [^19] | [How adversarial attacks can disrupt seemingly stable accurate classifiers.](http://arxiv.org/abs/2309.03665) | 本文研究了对抗性攻击如何通过微小修改干扰准确的分类器，并发现这可能是高维输入数据下分类器的基本特征。作者提出了一个通用的框架，解释了实际系统中观察到的关键行为，包括模型对对抗性攻击的容易受到影响，同时对随机扰动具有鲁棒性。验证实验还表明了相同现象在实际神经网络中的存在。 |
| [^20] | [Towards Comparable Knowledge Distillation in Semantic Image Segmentation.](http://arxiv.org/abs/2309.03659) | 在这项研究中，我们探索了语义图像分割中知识蒸馏的问题。我们发现了25种蒸馏损失项，并指出由于训练配置的差异导致术语比较困难。此外，我们发现超参数选择不当会导致极端的性能差异。 |
| [^21] | [Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection.](http://arxiv.org/abs/2309.03652) | 这项研究提出了一种新的解剖学指导的数据增强技术，通过模拟前列腺的生理变形并生成独特的病变形状，从而增强了前列腺癌检测模型的泛化能力。 |
| [^22] | [Learning of Generalizable and Interpretable Knowledge in Grid-Based Reinforcement Learning Environments.](http://arxiv.org/abs/2309.03651) | 本文提出了在基于网格的强化学习环境中使用程序合成来学习通用和可解释的知识，以解决智能体行为理解的问题。 |
| [^23] | [Characterizing Lipschitz Stability of GNN for Fairness.](http://arxiv.org/abs/2309.03648) | 论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。 |
| [^24] | [VideolandGPT: A User Study on a Conversational Recommender System.](http://arxiv.org/abs/2309.03645) | 本研究通过使用大语言模型VideolandGPT改进了会话式推荐系统，实验表明个性化版本在准确性和用户满意度方面优于非个性化版本，但两个版本在公平性方面存在不一致行为。 |
| [^25] | [Beyond XAI:Obstacles Towards Responsible AI.](http://arxiv.org/abs/2309.03638) | 这篇论文探讨了可解释AI领域中存在的限制，并在考虑隐私、公平性和可争议性等其他重要方面时讨论了负责任AI的意义。 |
| [^26] | [NeuroCodeBench: a plain C neural network benchmark for software verification.](http://arxiv.org/abs/2309.03617) | NeuroCodeBench是一个用纯C编写的神经网络代码验证基准测试，共包含32个神经网络和607个安全属性。研究发现，由于不完全支持标准C数学库和复杂的大型神经网络的复杂性，目前最先进的软件验证器难以提供正确的判断。 |
| [^27] | [Evaluating ChatGPT as a Recommender System: A Rigorous Approach.](http://arxiv.org/abs/2309.03613) | 这项研究评估了ChatGPT作为推荐系统的能力，通过探索其利用用户偏好进行推荐、重新排序推荐列表、利用相似用户信息以及处理冷启动情况的能力，并使用三个数据集进行了全面实验。 |
| [^28] | [Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision.](http://arxiv.org/abs/2309.03590) | 通过使用空间编码的BOLD fMRI时间序列，本研究对视觉数据集中的静态图像进行分类，并发现了与视觉相关的神经活动的差异。 |
| [^29] | [Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning.](http://arxiv.org/abs/2309.03581) | 本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。 |
| [^30] | [DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend.](http://arxiv.org/abs/2309.03579) | 提出了一种名为DTW+S的新型测量方法，它通过创建局部趋势的矩阵表示，并应用动态时间规整来计算距离，解决了现有方法无法捕捉局部趋势相似性的问题。 |
| [^31] | [Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation.](http://arxiv.org/abs/2309.03549) | 这篇论文研究了在文本到视频生成中，使用重用和迭代扩散的方法可以生成更多视频帧，相比其他方法，该方法避免了额外的训练成本和帧级抖动。 |
| [^32] | [DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks.](http://arxiv.org/abs/2309.03523) | 本论文提出了DGC系统，利用一种新的图分区方法实现了动态图神经网络(DGNN)的高效训练，解决了实际动态图非均匀结构的问题，实现了1.25倍-7.52倍的训练加速。 |
| [^33] | [Parameterized Aspects of Distinct Kemeny Rank Aggregation.](http://arxiv.org/abs/2309.03517) | 这项研究从参数化复杂性的角度出发，研究了Distinct Kemeny排序聚合的多个参数，包括目标Kemeny分数、候选人数量、输入排序的平均距离、任何候选人的最大范围和一致性宽度，并提出了相应的FPT算法和FPT近似算法。 |
| [^34] | [Towards Robust Natural-Looking Mammography Lesion Synthesis on Ipsilateral Dual-Views Breast Cancer Analysis.](http://arxiv.org/abs/2309.03506) | 这篇论文提出了一种简单且新颖的方法，通过利用辅助视角的低层特征信息来增强乳腺X线造影图像的主要视角，然后学习包含癌变特征的高层特征。此外，还提出了一种简单且新颖的恶性乳腺X线造影合成框架，用于上采样少数类样本。 |
| [^35] | [InteractionNet: Joint Planning and Prediction for Autonomous Driving with Transformers.](http://arxiv.org/abs/2309.03475) | InteractionNet是基于Transformer的自动驾驶规划与预测的联合模型，可以捕捉交互并联合考虑规划和预测，在安全性等方面优于其他基线模型。 |
| [^36] | [Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size.](http://arxiv.org/abs/2309.03469) | 本论文提出了快速的FixMatch算法，通过引入课程批次大小（CBS）来减少半监督学习的训练计算量，并使用强化标记增强和课程伪标签进行改进。 |
| [^37] | [Cross-Image Context Matters for Bongard Problems.](http://arxiv.org/abs/2309.03468) | Bongard问题是一种需要从一组正负图像中推导出抽象概念并进行分类的智力测试，现有方法在Bongard问题中准确率较低。本研究发现，这是因为现有方法未能整合支持集合中的信息，而是仅依赖于单个支持图像的信息。我们提出了一种通过跨图像上下文来提高准确性的解决方案。 |
| [^38] | [Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation.](http://arxiv.org/abs/2309.03467) | 该论文提出了一种自回归全方位感知生成网络（AOG-Net）用于生成360度图像，通过渐进地外扩不完整的360度图像，并与窄视场（NFoV）和文本引导相结合或单独使用。这种方法可以生成更细致和与文本一致的模式，并为用户在生成过程中灵活编辑条件。 |
| [^39] | [MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks.](http://arxiv.org/abs/2309.03466) | 本文提出了一种基于模型反演的去除攻击方法（\textsc{Mira}），该方法能够有效地破解大多数主流黑盒DNN水印方案，通过利用受保护模型的内部信息来恢复和消除水印信息。 |
| [^40] | [SyncDreamer: Generating Multiview-consistent Images from a Single-view Image.](http://arxiv.org/abs/2309.03453) | 本文提出了一种名为SyncDreamer的新型扩散模型，可以从单视图图像生成多视角一致性图像。通过同步所有生成图像的中间状态，并利用3D感知特征注意机制，SyncDreamer能够实现在不同视角上生成高度一致的图像，为各种3D生成任务提供了有力的支持。 |
| [^41] | [XGen-7B Technical Report.](http://arxiv.org/abs/2309.03450) | XGen-7B是一种用于处理长序列的大型语言模型，通过克服开源LLMs在支持长序列长度方面的限制，并在标准基准上取得与最先进的开源LLMs相当或更好的结果，推进了研究进展和商业应用。 |
| [^42] | [Large Language Models as Optimizers.](http://arxiv.org/abs/2309.03409) | 本论文提出了一种简单有效的方法，利用大型语言模型(LLMs)作为优化器，通过自然语言描述优化任务。经过实验证明，该方法在线性回归和旅行推销员问题上表现出色，并且优化的最佳提示超过了人为设计的提示。 |
| [^43] | [The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers.](http://arxiv.org/abs/2309.03404) | 本论文研究了专业混音师与客户的互动以及他们如何利用客户的反馈指导混音过程。结果表明，混音师通过使用参考曲目和合作沟通等方式，与客户建立起对混音期望音效的默契约定。 |
| [^44] | [Efficient Baselines for Motion Prediction in Autonomous Driving.](http://arxiv.org/abs/2309.03387) | 这项研究提出了自动驾驶中运动预测的高效基线模型，解决了使用地图和过去轨迹信息的实时应用复杂性和可解释性的问题。 |
| [^45] | [Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction.](http://arxiv.org/abs/2309.03386) | 本论文介绍了一种基于社区的分级正类-未标记（PU）模型融合方法，用于慢性病预测。该方法考虑了不同人群之间的差异，并通过构建PU模型树和聚合其输出来进行预测。 |
| [^46] | [Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks.](http://arxiv.org/abs/2309.03367) | 本研究旨在提供一种针对低资源下游任务的自监督遮罩数字高程模型编码。利用自监督学习，该模型能够从大量未标记和非结构化数据中学习，并将该知识应用于土地地形的建筑物和道路分割任务。 |
| [^47] | [REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation.](http://arxiv.org/abs/2309.03322) | 本论文介绍了一种使用强化学习学习灵巧操纵技能的高效系统，通过结合样本高效强化学习和回放缓冲区引导，实现了重用数据以降低真实世界应用中的挑战。 |
| [^48] | [Fitness Approximation through Machine Learning.](http://arxiv.org/abs/2309.03318) | 我们提出了一种使用机器学习模型在遗传算法中进行适应度近似的方法。实验结果表明，这种方法显著提高了进化运行时间，并且适应度得分要么与完全运行的遗传算法相同，要么稍微低一点。 |
| [^49] | [Comparative Analysis of Deep-Fake Algorithms.](http://arxiv.org/abs/2309.03295) | 这项研究比较了不同的深度伪造算法，探讨了其对数字视觉媒体完整性的威胁，以及深伪造检测技术的应用方法。 |
| [^50] | [Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning.](http://arxiv.org/abs/2309.03251) | 本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。 |
| [^51] | [Automated Bioinformatics Analysis via AutoBA.](http://arxiv.org/abs/2309.03242) | AutoBA是一个基于大型语言模型的自主AI代理程序，通过最少的用户输入简化生物信息学分析过程，并提供详细的逐步计划。经过验证，AutoBA在各种组学分析案例中表现出健壮性和适应性，同时保护数据隐私。 |
| [^52] | [GPT Can Solve Mathematical Problems Without a Calculator.](http://arxiv.org/abs/2309.03241) | 本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。 |
| [^53] | [Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference.](http://arxiv.org/abs/2309.03239) | 本文提出了一种针对POI级别人群流推断的时空对比自监督学习模型，通过自监督属性图表示学习以解决数据标记不足、POI间时空依赖性复杂和人群流量与GPS报告之间相关性多样等挑战。 |
| [^54] | [Natural Example-Based Explainability: a Survey.](http://arxiv.org/abs/2309.03234) | 本文概述了自然示例为基础的可解释性人工智能的最新进展，这些方法通过使用示例作为解释来提高机器学习模型的可解释性，与人类的学习和推理过程相符，使解释更自然和易懂。 |
| [^55] | [Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection.](http://arxiv.org/abs/2309.03231) | 本研究将量子人工智能与监控领域的RentinaNet模型进行整合，开发了称为Quantum-RetinaNet的智能监控系统，该系统在准确性和速度方面取得了突破性进展。 |
| [^56] | [Which algorithm to select in sports timetabling?.](http://arxiv.org/abs/2309.03229) | 本研究通过实例空间分析体育赛程安排问题的特征，提出了一个基于机器学习的算法选择系统，预测了在给定特征下最适合的算法，同时深入了解了算法性能并提出了改进建议。 |
| [^57] | [Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates.](http://arxiv.org/abs/2309.03227) | 本研究提出了一种使用药物专利和生物医学数据库相结合的方法，识别具有技术潜力和科学证据的药物再定位候选物。通过构建科学的生物医学知识图谱和基于专利的生物医学知识图谱，我们可以综合分析多种信息源，为药物再定位研究提供新的视角。 |
| [^58] | [Amortizing Pragmatic Program Synthesis with Rankings.](http://arxiv.org/abs/2309.03225) | 该研究提出了一种使用全局实际排名方法来分摊实际程序合成中计算负担的方法，并证明了该方法适用于使用单个演示的实际合成器，并且通过实证研究展示了全局排名有效近似了全体合理响应。 |
| [^59] | [No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function.](http://arxiv.org/abs/2309.03224) | 该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。 |
| [^60] | [Sherlock Holmes Doesn't Play Dice: The significance of Evidence Theory for the Social and Life Sciences.](http://arxiv.org/abs/2309.03222) | 本文强调了Evidence Theory在社会与生命科学中的潜力，可以表达来自未确定事件可能实现的不确定性，而与之对比的Probability Theory只能限于决策者当前设想的可能性。本文还讨论了Evidence Theory与Probability Theory的关系以及Evidence Theory在信息论中的应用增强效果，并通过审计练习案例进一步说明了Evidence Theory的应用。 |
| [^61] | [Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning.](http://arxiv.org/abs/2309.03219) | 这项研究提出了一种基于文本感知的医学知识图谱表示学习方法，以提高伴侣动物疾病诊断的效率。通过融合各种类型的文本信息和图结构，该方法能够捕捉到重要的实体和关系。 |
| [^62] | [Algebraic Models for Qualified Aggregation in General Rough Sets, and Reasoning Bias Discovery.](http://arxiv.org/abs/2309.03217) | 本研究提出了一种泛粗糙集中合格聚合的代数模型，用于模拟人类推理中的悲观和乐观的合并，以及研究推理中的歧视/有害行为和机器学习算法。 |
| [^63] | [Explainable and Trustworthy Traffic Sign Detection for Safe Autonomous Driving: An Inductive Logic Programming Approach.](http://arxiv.org/abs/2309.03215) | 本论文提出了一种基于归纳逻辑编程的可解释和值得信赖的交通标志检测方法，使用高级特征来鲁棒地检测交通标志，从而解决了当前基于深度神经网络的方法容易受到对抗性攻击的问题。 |
| [^64] | [Improving the State of the Art for Training Human-AI Teams: Technical Report #3 -- Analysis of Testbed Alternatives.](http://arxiv.org/abs/2309.03213) | Sonalysts正在研究开发一种合成任务环境（STE），通过调查现有的人工智能团队合作测试环境，他们总结出了测试环境评估标准和潜在的测试环境。 |
| [^65] | [Improving the State of the Art for Training Human-AI Teams: Technical Report #2 -- Results of Researcher Knowledge Elicitation Survey.](http://arxiv.org/abs/2309.03212) | 这项技术报告介绍了一项关于改进人工智能与人类团队培训的研究，旨在开发一种合成任务环境，以支持利益相关研究人员在该领域的广泛研究。 |
| [^66] | [Improving the State of the Art for Training Human-AI Teams: Technical Report #1 -- Results of Subject-Matter Expert Knowledge Elicitation Survey.](http://arxiv.org/abs/2309.03211) | 这项研究旨在提高人工智能与人类团队合作的培训水平，并通过开发合成任务环境来解决联合全领域指挥与控制中的团队合作挑战。 |
| [^67] | [A Human-Machine Joint Learning Framework to Boost Endogenous BCI Training.](http://arxiv.org/abs/2309.03209) | 本研究提出了一种人机联合学习框架，通过引导用户生成脑信号，以促进内源性BCI的学习过程。这种框架基于用户的历史脑信号，将其引导到解码器估计的最优分布。 |
| [^68] | [A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design.](http://arxiv.org/abs/2309.03208) | 该论文提出了一个用于芯片设计中高效逻辑综合的电路领域泛化框架，重点解决了逻辑综合中无效变换的问题，并通过PruneX操作符实现了超领域泛化。 |
| [^69] | [Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation.](http://arxiv.org/abs/2309.02685) | 本文提出了Diffusion-EDFs，一种在视觉机器人操作中应用的基于SE(3)的等变去噪生成建模方法。通过集成SE(3)等变性，我们的方法展示了出色的数据效率和泛化能力。 |
| [^70] | [Using Physics-Informed Neural Networks to Calculate Minimal Surfaces in Higher Dimensions.](http://arxiv.org/abs/2309.02589) | 本文使用物理知识的神经网络（PINN）计算更高维度中的最小曲面的数值逼近，解决了维数诅咒问题，并且能够在没有GPU的笔记本电脑上进行快速训练。 |
| [^71] | [Automating Behavioral Testing in Machine Translation.](http://arxiv.org/abs/2309.02553) | 本文提出了一种利用大型语言模型自动生成源句子的方法，以测试机器翻译模型在多种情况下的行为。通过对多个机器翻译系统应用该方法，发现在测试结果与传统准确率度量存在差异的情况下，仍可观察到一致的趋势。 |
| [^72] | [Intelligence as a Measure of Consciousness.](http://arxiv.org/abs/2309.00646) | 评估人工系统是否具有意识的迹象是一个紧迫的问题，智力的心理测量可以间接地近似意识体验的程度，可以用来评估大型语言模型中的意识。 |
| [^73] | [Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes.](http://arxiv.org/abs/2309.00237) | 使用合成临床记录构建的临床大语言模型可以克服临床记录的有限可及性和可用性的问题，并在现实应用中表现出潜在的良好性能。 |
| [^74] | [Transformers as Support Vector Machines.](http://arxiv.org/abs/2308.16898) | 这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。 |
| [^75] | [Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection.](http://arxiv.org/abs/2308.16763) | 该论文介绍了一种名为“Ladder-of-Thought”的方法，通过引入外部知识来提升立场检测任务中的语言模型的性能，解决了小型模型在应用先前内部知识时性能提升不明显的问题，以及大规模模型在效率方面的挑战。 |
| [^76] | [LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models.](http://arxiv.org/abs/2308.16137) | LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。 |
| [^77] | [From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions.](http://arxiv.org/abs/2308.15225) | 本论文提出利用决策过程数据和模型改善人工智能与人类之间的交互。通过详细描述决策过程和建立决策演变模型，可以揭示潜在的偏好，同时追踪决策过程的数据可以提供重要信息，从而改善人工智能的预测能力。 |
| [^78] | [Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators.](http://arxiv.org/abs/2308.15116) | 本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。 |
| [^79] | [AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning.](http://arxiv.org/abs/2308.13280) | 提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。 |
| [^80] | [Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models.](http://arxiv.org/abs/2308.11764) | 本文介绍了一种用于评估和减少开源弱大语言模型中幻觉问题的框架，并探索了知识注入和师生方法等技术来减轻低参数模型中的幻觉问题，实验结果表明，在挑战性领域中，这些模型的幻觉问题得到了减少。 |
| [^81] | [A Survey on Large Language Model based Autonomous Agents.](http://arxiv.org/abs/2308.11432) | 该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。 |
| [^82] | [RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification.](http://arxiv.org/abs/2308.02335) | 我们提出了一种检索增强型混合网络(RAHNet)用于长尾图分类任务，通过联合学习稳健的特征提取器和无偏的分类器，解决了图神经网络在长尾类别分布下的偏差和泛化能力有限的问题。 |
| [^83] | [VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference.](http://arxiv.org/abs/2308.00904) | VLUCI是一个新颖的可变参数学习模型，用于解决反事实推断中的未观测混淆变量的问题。它通过生成未观测混淆变量的后验分布，并构建一个双重变分推断模型来解决因果推断中观测和未观测混淆变量的问题，从而提高反事实推断的准确性。 |
| [^84] | [Beneficent Intelligence: A Capability Approach to Modeling Benefit, Assistance, and Associated Moral Failures through AI Systems.](http://arxiv.org/abs/2308.00868) | 借鉴能力方法，研究者提出了一个框架，用于解决AI系统与个体互动时涌现的伦理问题。同时，他们也界定了道德可接受的互动条件，并对几种失败模式进行了对比分析。 |
| [^85] | [Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models.](http://arxiv.org/abs/2307.14971) | 本文提出了一种新的三维到二维的生成式预训练方法，通过生成视图图像作为预训练方案，帮助三维模型更好地理解点云的几何结构和立体关系，并在实验证明了其优越性。 |
| [^86] | [Adversarial Likelihood Estimation with One-way Flows.](http://arxiv.org/abs/2307.09882) | 本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。 |
| [^87] | [Towards eXplainable AI for Mobility Data Science.](http://arxiv.org/abs/2307.08461) | 本文介绍了朝着可解释的AI在移动数据科学中的应用的工作，包括可解释模型的设计和使用时间图神经网络和反事实来学习从密集轨迹数据中提取信息的方法。 |
| [^88] | [Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting.](http://arxiv.org/abs/2307.05766) | 本文提出了Rad-ReStruct，一个用于评估和比较不同方法的新型基准数据集，以X光图像的结构化报告形式提供了细粒度、按层次排序的注释。我们提出了一种新方法hi-VQA，将结构化报告任务建模为分层视觉问答(VQA)，并考虑先前提问和回答的上下文来填充结构化放射学报告。实验证明hi-VQA取得了与最先进方法相竞争的性能。 |
| [^89] | [Margin Maximization in Attention Mechanism.](http://arxiv.org/abs/2306.13596) | 这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。 |
| [^90] | [Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering.](http://arxiv.org/abs/2306.00526) | 该论文提出了一种布局和任务感知的指导提示模型，称为LATIN-Prompt，通过将文档图像问答对齐到现成的指导调优语言基础模型，利用其零样本能力来提高效果。该模型包括布局感知的文档内容和任务感知的描述，能够恢复文本片段之间的布局信息，并生成符合任务需求的答案。 |
| [^91] | [Towards Enhancing In-Context Learning for Code Generation.](http://arxiv.org/abs/2303.17780) | 本文提出了一种名为AceCoder的代码生成上下文学习方法，与标准上下文学习相比，它通过示例检索和引导代码生成来提高生成代码的准确性和鲁棒性。 |
| [^92] | [Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages.](http://arxiv.org/abs/2303.13592) | 本文探讨了使用大型语言模型（LLMs）生成东南亚五种语言和Singlish的混合代码数据的方法，发现ChatGPT展现出最高的潜力。然而，由于词汇选择错误的影响，ChatGPT和InstructGPT在生成混合代码时的熟练程度受到限制。 |
| [^93] | [Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network.](http://arxiv.org/abs/2303.11899) | 本文提出了一种新的训练框架 RegionLight，基于交叉口之间的邻接关系将智能体分配到每个区域中。同时，研究人员扩展了BDQ方法为DBDQ，以限制联合动作空间大小的增长并缓解智能体训练问题。 |
| [^94] | [Internet Explorer: Targeted Representation Learning on the Open Web.](http://arxiv.org/abs/2302.14051) | Internet Explorer是一种能够利用互联网进行有针对性表示学习的方法，通过自我监督的方式在网络上搜索相关图像并训练小规模模型，从而提高在特定任务上的性能。 |
| [^95] | [LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification.](http://arxiv.org/abs/2301.04838) | LB-SimTSC是一种高效的相似性感知图神经网络，用于半监督时间序列分类。它通过使用LB_Keogh作为DTW的下界来解决DTW二次复杂性限制，在少标签设置中表现出优秀的准确度。 |
| [^96] | [BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph.](http://arxiv.org/abs/2212.05798) | BigText-QA引入了一种综合的QA方法，能够基于有结构化和非结构化知识的知识图回答复杂问题。 |
| [^97] | [CodeEditor: Learning to Edit Source Code with Pre-trained Models.](http://arxiv.org/abs/2210.17040) | CodeEditor是一个预训练代码编辑模型，通过专门的预训练任务和代码编辑任务的结合，提高了代码编辑模型的性能和泛化能力。 |
| [^98] | [Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec.](http://arxiv.org/abs/2208.03680) | 通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。 |
| [^99] | [Autonomous Agriculture Robot for Smart Farming.](http://arxiv.org/abs/2208.01708) | 该论文介绍了一种用于智能农业的自主地面机器人，具备智能感知和自主除草功能，并能提供施肥、杀虫剂等服务，为作物和土壤健康监测提供信息。 |
| [^100] | [Models of human preference for learning reward functions.](http://arxiv.org/abs/2206.02231) | 本研究提出了一种将人类偏好建模为每个轨迹段的遗憾的方法，并证明了可以根据这些遗憾生成的偏好来识别生成这些偏好的奖励函数。实验证明，这种遗憾偏好模型在性能上优于以前的模型。 |
| [^101] | [Graph Fairing Convolutional Networks for Anomaly Detection.](http://arxiv.org/abs/2010.10274) | 本文引入了一种图形平滑卷积网络用于半监督异常检测，通过使用跳跃连接和图结构信息来学习有区分性的节点表示。 |

# 详细

[^1]: DiffusionEngine: 扩展数据引擎的扩散模型适用于目标检测

    DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])

    [http://arxiv.org/abs/2309.03893](http://arxiv.org/abs/2309.03893)

    DiffusionEngine是一个可扩展的数据引擎，使用Diffusion模型进行目标检测。它通过一个预训练的模型和一个检测适配器，能够在单个阶段提供高质量的训练数据，并生成可扩展、多样化和可泛化的检测数据。

    

    数据是深度学习的基石。本文揭示了最近开发的Diffusion模型是一个用于目标检测的可扩展数据引擎。现有的缩放检测导向数据的方法通常需要手动收集或生成模型来获取目标图像，然后进行数据增强和标注产生训练对，这些方法成本高、复杂或缺乏多样性。为了解决这些问题，我们提出了DiffusionEngine（DE），这是一个数据扩展引擎，以单一阶段提供高质量的检测导向训练对。DE由一个预训练的扩散模型和一个有效的检测适配器组成，为生成可扩展、多样化和可泛化的检测数据做出了贡献，并支持即插即用。检测适配器通过学习将现成的扩散模型中的隐式语义和位置知识与检测相关的信号进行对齐，从而产生更好的边界框预测。此外，我们还贡献了两个数据集，即COCO-DE和...

    Data is the cornerstone of deep learning. This paper reveals that the recently developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we presentDiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e., COCO-DE and
    
[^2]: 一个用于评估解释性方法的功能解释基准

    A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])

    [http://arxiv.org/abs/2309.03886](http://arxiv.org/abs/2309.03886)

    本文介绍了一个用于评估自动解释性方法的基准套件，该套件包括了类似于传统系统组件的函数。

    

    使用人类可读的描述标记神经网络子模块对于许多下游任务非常有用：这些描述可以暴露失败、引导干预，甚至可以解释重要的模型行为。到目前为止，大多数基于机械原理的已训练网络描述都涉及到小模型、狭义现象，并且需要大量人力。在不断增加的模型大小和复杂性中标记出所有人可解释的子计算几乎肯定需要能够自动生成和验证描述的工具。最近，利用学习模型进行标记的技术开始受到关注，但评估其有效性的方法有限且临时。我们应该如何验证和比较开放式标记工具？本文介绍了FIND（函数解释和描述），一个用于评估自动解释方法构建模块的基准套件。FIND包含了类似于传统系统的组件的函数。

    Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
    
[^3]: DoLa：通过对比层次提高大型语言模型中的真实性

    DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])

    [http://arxiv.org/abs/2309.03883](http://arxiv.org/abs/2309.03883)

    DoLa通过对比不同层次的逻辑差异，提高大型语言模型中的真实性和减少幻觉，无需外部知识或微调。

    

    尽管大型语言模型（LLMs）具有令人印象深刻的能力，但它们容易出现幻觉，即生成与预训练期间观察到的事实偏离的内容。我们提出了一种简单的解码策略，用于减少预训练LLMs中的幻觉，它不需要在检索的外部知识或额外的微调上进行条件约束。我们的方法通过对比将较晚层和较早层投影到词汇空间得到的逻辑差异来获得下一个令牌的分布，利用了LLMs中的事实知识通常被证明局部化在特定的Transformer层中的事实。我们发现，这种通过对比层次的解码（DoLa）方法能够更好地展示事实知识，并减少生成不正确事实的情况。DoLa在多个选择任务和开放式生成任务中持续提升了真实性，例如改善了LLaMA系列模型在TruthfulQA上的表现。

    Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
    
[^4]: OpinionGPT: 模拟显性偏见的指令调整大型语言模型(LLMs)

    OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])

    [http://arxiv.org/abs/2309.03876](http://arxiv.org/abs/2309.03876)

    OpinionGPT是一个指令调整大型语言模型(LLMs)的web演示，用户可以选择各种偏见并进行比较，它意在使模型的偏见显性和透明化。

    

    最近，指令调整大型语言模型(LLMs)展示了生成与自然语言指令相匹配的回应的显著能力。然而，一个开放的研究问题涉及训练模型和它们的回应中固有的偏见。例如，如果用于调整LLM的数据主要由具有特定政治偏见的人编写，我们可能会期望生成的回答也共享这种偏见。目前的研究工作旨在除去这样的模型偏见，或抑制可能有偏见的回答。通过这个演示，我们对指令调整中的偏见持有不同的观点：我们的目标不是抑制它们，而是使它们显性和透明。为此，我们提供了OpinionGPT，一个网络演示，用户可以提问并选择所有他们希望调查的偏见。该演示将使用在代表每个选择偏见的文本上进行微调的模型来回答这个问题，从而实现并排比较。为了训练基础模型，我们选取了11个...

    Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers. With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 
    
[^5]: FLM-101B：一种开放的LLM和如何用10万美元预算来训练它

    FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])

    [http://arxiv.org/abs/2309.03852](http://arxiv.org/abs/2309.03852)

    本文介绍了一种开放的LLM模型（FLM-101B）以及如何用10万美元的预算来训练它。通过采用增长策略，可以显著降低LLM训练的成本。同时，引入了一种系统的评估方法，以评估LLM的智能能力。

    

    大型语言模型（LLMs）在自然语言处理和多模态任务中取得了显著的成功。然而，它们的发展面临两个主要挑战：（i）高计算成本；（ii）难以进行公平客观的评估。LLMs的价格昂贵，只有少数几家主要参与者有能力进行训练，从而限制了研究和应用机会。这凸显了成本效益的LLM训练的重要性。在本文中，我们采用了一种增长策略，显著降低LLM训练成本。我们证明了可以在10万美元的预算下训练具有101B参数和0.31TB令牌的LLM。我们还采用了一种系统的评估范式，用于对LLMs进行智能的智商评估，这是针对现有评估更注重知识能力的补充。我们引入了包括符号映射、规则理解、模式挖掘在内的重要智能方面的评估基准。

    Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
    
[^6]: 揭示文本数据中的漂移：一种无监督的方法用于检测和减轻机器学习模型中的漂移

    Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])

    [http://arxiv.org/abs/2309.03831](http://arxiv.org/abs/2309.03831)

    这项研究提出了一种无监督的方法来检测和减轻机器学习模型中的漂移。通过编码生产数据样本和模型训练数据，以及利用核统计检验和最大均值差异（MMD）距离度量来比较分布，可以有效估计漂移情况。

    

    机器学习中的漂移指的是数据或模型运行上下文的统计特性随时间变化而导致性能下降的现象。因此，保持对机器学习模型性能的持续监控过程对于预防潜在性能回退至关重要。然而，有监督的漂移检测方法需要人工标注，从而导致漂移检测和减轻过程时间较长。在我们提出的无监督漂移检测方法中，我们采用了两个步骤的流程。我们的第一步涉及将生产数据的一个样本作为目标分布，将模型训练数据作为参考分布进行编码。在第二步中，我们使用基于核的统计检验，并利用最大均值差异（MMD）距离度量来比较参考分布和目标分布，估计任何潜在的漂移。我们的方法还能确定生产数据子集的漂移情况。

    Drift in machine learning refers to the phenomenon where the statistical properties of data or context, in which the model operates, change over time leading to a decrease in its performance. Therefore, maintaining a constant monitoring process for machine learning model performance is crucial in order to proactively prevent any potential performance regression. However, supervised drift detection methods require human annotation and consequently lead to a longer time to detect and mitigate the drift. In our proposed unsupervised drift detection method, we follow a two step process. Our first step involves encoding a sample of production data as the target distribution, and the model training data as the reference distribution. In the second step, we employ a kernel-based statistical test that utilizes the maximum mean discrepancy (MMD) distance metric to compare the reference and target distributions and estimate any potential drift. Our method also identifies the subset of production
    
[^7]: 低秩分解网络的训练加速：顺序冻结和秩量化

    Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization. (arXiv:2309.03824v1 [cs.LG])

    [http://arxiv.org/abs/2309.03824](http://arxiv.org/abs/2309.03824)

    本文介绍了两种加速低秩分解模型的技术：秩优化和顺序冻结分解层。实验证明，这些技术可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%，同时保持准确性接近原始模型。

    

    低秩分解（LRD）是一种应用于深度学习模型权重张量的模型压缩技术，以减少可训练参数和计算复杂性。然而，由于在应用LRD后在架构中添加了大量新层，如果分解秩不够小，则可能导致训练/推理加速性不高。问题在于，使用较小的秩会增加分解后的显著准确率下降的风险。本文中，我们提出了两种加速低秩分解模型的技术，而不需要使用较小的秩进行分解。这些方法包括秩优化和顺序冻结分解层。我们在卷积和基于transformer的模型上进行了实验证明，这些技术在保持接近原始模型准确性的同时，可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%。

    Low Rank Decomposition (LRD) is a model compression technique applied to the weight tensors of deep learning models in order to reduce the number of trainable parameters and computational complexity. However, due to high number of new layers added to the architecture after applying LRD, it may not lead to a high training/inference acceleration if the decomposition ranks are not small enough. The issue is that using small ranks increases the risk of significant accuracy drop after decomposition. In this paper, we propose two techniques for accelerating low rank decomposed models without requiring to use small ranks for decomposition. These methods include rank optimization and sequential freezing of decomposed layers. We perform experiments on both convolutional and transformer-based models. Experiments show that these techniques can improve the model throughput up to 60% during training and 37% during inference when combined together while preserving the accuracy close to that of the o
    
[^8]: AnthroNet: 通过人体比例生成条件化的人体模型

    AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])

    [http://arxiv.org/abs/2309.03812](http://arxiv.org/abs/2309.03812)

    通过人体比例测量构建的AnthroNet模型能够生成各种不同形状和姿势的人体，并以任意姿势生成特定人物身份的人体。该模型通过合成数据进行端到端训练，提供了高精度的人体网格表示和精确的人体比例测量。

    

    我们提出了一种基于广泛的人体比例测量而构建的新颖人体模型，能够生成各种不同形状和姿势的人体。所提出的模型通过深度生成架构，实现了对特定人物身份的直接建模，并能够以任意姿势生成人体。这是第一种通过仅使用合成数据进行端到端训练的模型，不仅提供了高精度的人体网格表示，还允许对人体进行精确的人体比例测量。此外，通过使用高度多样化的动画库，我们为合成人体的身体和手部进行了关节处理，以最大程度地提高模型训练中可学习先验的多样性。我们的模型在一个包含10万个程序生成的人体网格和相应人体比例测量的数据集上进行了训练。我们的合成数据生成器可以用于生成非商业学术用途下的数百万个独特人物身份和姿势。

    We present a novel human body model formulated by an extensive set of anthropocentric measurements, which is capable of generating a wide range of human body shapes and poses. The proposed model enables direct modeling of specific human identities through a deep generative architecture, which can produce humans in any arbitrary pose. It is the first of its kind to have been trained end-to-end using only synthetically generated data, which not only provides highly accurate human mesh representations but also allows for precise anthropometry of the body. Moreover, using a highly diverse animation library, we articulated our synthetic humans' body and hands to maximize the diversity of the learnable priors for model training. Our model was trained on a dataset of $100k$ procedurally-generated posed human meshes and their corresponding anthropometric measurements. Our synthetic data generator can be used to generate millions of unique human identities and poses for non-commercial academic 
    
[^9]: 神经特征学习中的帕累托前沿：数据、计算、宽度和运气

    Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])

    [http://arxiv.org/abs/2309.03800](http://arxiv.org/abs/2309.03800)

    本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。

    

    本研究探讨了在计算统计差距存在的情况下，深度学习中微妙的算法设计选择。我们首先考虑了离线稀疏奇偶学习，这是一个有关多层感知器梯度训练的监督分类问题，其具有统计查询下界。这个下界可以解释为多资源的权衡前沿：成功学习只有在一个足够丰富（大型模型）、知识渊博（大规模数据集）、耐心（训练迭代次数多）或幸运（随机猜测次数多）的情况下才能发生。我们通过理论和实验表明，在这种情况下，稀疏初始化和增加网络宽度可以显著提高样本效率。在这里，宽度起到了并行搜索的作用：它增加了找到“幸运神经元”的概率，这些神经元可以更高效地学习稀疏特征。最后，我们表明合成稀疏奇偶任务可以作为真实问题的代理。

    This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
    
[^10]: FisheyePP4AV: 一种用于自动驾驶的鱼眼相机图像的隐私保护方法

    FisheyePP4AV: A privacy-preserving method for autonomous vehicles on fisheye camera images. (arXiv:2309.03799v1 [cs.CV])

    [http://arxiv.org/abs/2309.03799](http://arxiv.org/abs/2309.03799)

    FisheyePP4AV 是一种用于自动驾驶的鱼眼相机图像的隐私保护方法，解决了在实际道路驾驶场景中检测和匿名化行人面部和车牌的问题，并提供了一个框架用于从多个深度学习模型中提取人脸和车牌识别知识。

    

    在世界上许多地方，对公共道路上收集的大量数据用于自动驾驶的使用已经增加。为了在实际道路驾驶场景中检测和匿名化行人面部和附近的车牌，迫切需要有效的解决方案。随着收集的数据越来越多，涉及隐私的担忧也在增加，包括但不限于行人面部和周围车辆牌照。普通相机和鱼眼相机是通常安装在采集车辆上的两种常见相机类型。由于复杂的相机失真模型，与普通图像相比，鱼眼相机图像发生了变形。这导致在使用多个深度学习模型时，计算机视觉任务表现不佳。在这项工作中，我们特别关注对无人驾驶车辆拍摄的鱼眼相机照片进行隐私保护，同时遵守几项法律要求。首先，我们提出了一个框架，用于从几种不同的深度学习模型中提取人脸和车牌识别知识。

    In many parts of the world, the use of vast amounts of data collected on public roadways for autonomous driving has increased. In order to detect and anonymize pedestrian faces and nearby car license plates in actual road-driving scenarios, there is an urgent need for effective solutions. As more data is collected, privacy concerns regarding it increase, including but not limited to pedestrian faces and surrounding vehicle license plates. Normal and fisheye cameras are the two common camera types that are typically mounted on collection vehicles. With complex camera distortion models, fisheye camera images were deformed in contrast to regular images. It causes computer vision tasks to perform poorly when using numerous deep learning models. In this work, we pay particular attention to protecting privacy while yet adhering to several laws for fisheye camera photos taken by driverless vehicles. First, we suggest a framework for extracting face and plate identification knowledge from seve
    
[^11]: 基于时间编码的深度强化学习在嵌入式设备上进行实时应用的CPU频率调度

    CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning. (arXiv:2309.03779v1 [cs.LG])

    [http://arxiv.org/abs/2309.03779](http://arxiv.org/abs/2309.03779)

    本研究通过基于时间编码的深度强化学习，开发了一种在嵌入式设备上进行实时应用的CPU频率调度方法，该方法可以在小型设备上推导出高效的功率管理方法，并解决了现有Linux内置方法的限制。

    

    小型设备经常用于物联网和智能城市应用，用于执行有软截止期的周期性专用任务。这项工作致力于开发在小型设备上推导出高效的功率管理方法的方法。我们首先研究了现有的Linux内置方法在小型设备中的限制。我们展示了三种典型的工作负荷/系统模式，这些模式对于Linux内置解决方案而言具有挑战性。我们使用时间编码开发了一种基于增强学习的技术，以推导出一种有效的DVFS调度程序，即使存在这三种系统模式。推导出的调度程序仅使用一个性能计数器，与内置的Linux机制相同，并且不需要工作负荷的显式任务模型。我们在Nvidia Jetson Nano Board上实现了一个原型系统，并进行了六种应用程序的实验，包括两个自设计的和四个基准应用程序。

    Small devices are frequently used in IoT and smart-city applications to perform periodic dedicated tasks with soft deadlines. This work focuses on developing methods to derive efficient power-management methods for periodic tasks on small devices. We first study the limitations of the existing Linux built-in methods used in small devices. We illustrate three typical workload/system patterns that are challenging to manage with Linux's built-in solutions. We develop a reinforcement-learning-based technique with temporal encoding to derive an effective DVFS governor even with the presence of the three system patterns. The derived governor uses only one performance counter, the same as the built-in Linux mechanism, and does not require an explicit task model for the workload. We implemented a prototype system on the Nvidia Jetson Nano Board and experimented with it with six applications, including two self-designed and four benchmark applications. Under different deadline constraints, our 
    
[^12]: 扩展传导知识图嵌入模型用于归纳逻辑关系推理

    Extending Transductive Knowledge Graph Embedding Models for Inductive Logical Relational Inference. (arXiv:2309.03773v1 [cs.AI])

    [http://arxiv.org/abs/2309.03773](http://arxiv.org/abs/2309.03773)

    本文提出了一种扩展传导知识图嵌入方法的模型，用于处理归纳推理任务。通过引入广义的谐波扩展，利用传导嵌入方法学习的表示来推断在推理时引入的新实体的表示。

    

    许多知识图的下游推理任务，例如关系预测，在传导设置下已经成功处理了。为了处理归纳设置，也就是在推理时引入新实体到知识图中，较新的工作选择了通过网络子图结构的复杂函数学习知识图的隐式表示的模型，通常由图神经网络架构参数化。这些模型的成本是增加的参数化、降低的可解释性和对其他下游推理任务的有限泛化能力。在这项工作中，我们通过引入广义的谐波扩展来弥合传统传导知识图嵌入方法和较新的归纳关系预测模型之间的差距，通过利用通过传导嵌入方法学习的表示来推断在推理时引入的新实体的表示。

    Many downstream inference tasks for knowledge graphs, such as relation prediction, have been handled successfully by knowledge graph embedding techniques in the transductive setting. To address the inductive setting wherein new entities are introduced into the knowledge graph at inference time, more recent work opts for models which learn implicit representations of the knowledge graph through a complex function of a network's subgraph structure, often parametrized by graph neural network architectures. These come at the cost of increased parametrization, reduced interpretability and limited generalization to other downstream inference tasks. In this work, we bridge the gap between traditional transductive knowledge graph embedding approaches and more recent inductive relation prediction models by introducing a generalized form of harmonic extension which leverages representations learned through transductive embedding methods to infer representations of new entities introduced at infe
    
[^13]: 表示学习和强化学习的混合算法用于动态和复杂的机器人运动规划

    Hybrid of representation learning and reinforcement learning for dynamic and complex robotic motion planning. (arXiv:2309.03758v1 [cs.RO])

    [http://arxiv.org/abs/2309.03758](http://arxiv.org/abs/2309.03758)

    这项研究提出了一种混合算法，将表示学习和强化学习结合应用于动态和复杂的机器人运动规划。其中使用了长短期记忆（LSTM）汇聚和跳跃连接来改进离散软演员-评论家算法，并通过比较不同的表示方法得出注意力网络在任务中的优势。

    

    运动规划是机器人决策的核心。传统的规划算法如图搜索和基于反应的算法在密集和动态障碍物的情况下面临挑战。深度学习算法产生次优的一步预测，导致许多碰撞。强化学习算法生成最优或接近最优的时间序列预测。然而，它们面临收敛速度慢、收敛结果次优和过拟合的问题。本文介绍了一个用于机器人运动规划的混合算法：长短期记忆（LSTM）汇聚和跳跃连接用于基于注意力的离散软演员-评论家算法（LSA-DSAC）。首先，图网络（关系图）和注意力网络（注意力权重）解释环境状态，用于学习离散软演员-评论家算法。通过对这两种表示方法的差异分析，注意力网络的表达能力优于图网络在我们的任务中。

    Motion planning is the soul of robot decision making. Classical planning algorithms like graph search and reaction-based algorithms face challenges in cases of dense and dynamic obstacles. Deep learning algorithms generate suboptimal one-step predictions that cause many collisions. Reinforcement learning algorithms generate optimal or near-optimal time-sequential predictions. However, they suffer from slow convergence, suboptimal converged results, and overfittings. This paper introduces a hybrid algorithm for robotic motion planning: long short-term memory (LSTM) pooling and skip connection for attention-based discrete soft actor critic (LSA-DSAC). First, graph network (relational graph) and attention network (attention weight) interpret the environmental state for the learning of the discrete soft actor critic algorithm. The expressive power of attention network outperforms that of graph in our task by difference analysis of these two representation methods. However, attention based 
    
[^14]: TSGBench：时间序列生成基准

    TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])

    [http://arxiv.org/abs/2309.03755](http://arxiv.org/abs/2309.03755)

    TSGBench是首个时间序列生成基准，用于统一和全面评估TSG方法，解决了现有方法在性能评估、数据集选择和评估指标上的限制。

    

    合成时间序列生成(TSG)在数据增强、异常检测和隐私保护等多个应用中至关重要。尽管在这个领域取得了重大进展，但现有方法存在三个关键限制：(1)它们经常针对类似的模型类型进行基准测试，限制了对性能能力的整体视角。(2)使用专门的合成和私有数据集引入了偏倚，阻碍了泛化能力。(3)模糊的评估指标，往往与自定义网络或下游任务相结合，阻碍了一致和公平的比较。为了克服这些限制，我们推出了\textsf {TSGBench}，作为首个TSG基准，旨在统一和全面评估TSG方法。它包括三个模块：(1)一个精心策划的、面向TSG的公开实际数据集收集，以及标准化的预处理流程；(2)一套综合的评估指标套件，包括基本指标

    Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.  To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measur
    
[^15]: 使用大型语言模型增强基于流水线的对话系统

    Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])

    [http://arxiv.org/abs/2309.03748](http://arxiv.org/abs/2309.03748)

    本文研究了如何使用大型语言模型（LLM）来增强基于流水线的对话系统。在设计和开发阶段，LLM可以帮助生成训练数据、提取实体和同义词、本地化和角色设计。在运营阶段，LLM可以辅助上下文化、意图分类、自动纠正话语、改写回复、摘要和使闭合问题回答能力。通过在私人银行领域的实验，证明了这些能力的有效性。

    

    AI和深度学习的最新进展使得基于大型语言模型（LLM）的代理器（如GPT-4）取得了突破。然而，许多商业化对话系统开发工具是基于流水线的，并且在进行人类对话时存在限制。本文研究了LLM在以下两个阶段中增强基于流水线的对话系统的能力：1）设计和开发阶段；2）运营阶段。在1）中，LLM可以在生成训练数据、提取实体和同义词、本地化和角色设计方面提供帮助。在2）中，LLM可以辅助上下文化、意图分类以防止对话中断和处理超出范围的问题、自动纠正话语、改写回复、制定消歧问句、摘要和使闭合问题回答能力。我们在私人银行领域进行了使用GPT-4的非正式实验，以实际示例证明上述情景。

    The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example.
    
[^16]: 基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统

    A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])

    [http://arxiv.org/abs/2309.03720](http://arxiv.org/abs/2309.03720)

    本文介绍了一个基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统，通过数据流处理，实现了多步 ahead 的预测和持续学习能力。在复杂的实际应用场景中，通过评估预测模型的性能，证明了该方法的有效性。

    

    在规划天然气供应和消费以及优化获得天然气成本方面，考虑季节性和趋势性的天然气消费预测至关重要。本文介绍了一种新颖的多步 ahead 的天然气消费预测方法，并集成了变点检测，以实现模型选择和持续学习能力。通过数据流处理，评估了基于该方法的天然气消费预测模型在复杂的实际应用场景中的性能。我们采用Hoeffding树预测器作为预测模型，并使用剪裁的精确线性时间（PELT）算法进行变点检测。变点检测集成使得选择不同的模型成为可能。

    Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
    
[^17]: PyGraft: 在你的指尖生成可配置的模式和知识图谱

    PyGraft: Configurable Generation of Schemas and Knowledge Graphs at Your Fingertips. (arXiv:2309.03685v1 [cs.AI])

    [http://arxiv.org/abs/2309.03685](http://arxiv.org/abs/2309.03685)

    PyGraft是一个Python工具，可以根据需要生成高度定制的模式和知识图谱，并确保生成的资源的逻辑一致性。

    

    知识图谱（KG）已经成为一种重要的数据表示和管理范式。KG通常基于模式（例如本体论）来捕获事实信息和上下文知识。在某些任务中，一些KG已经成为标准的基准测试数据集。然而，最近的研究发现，仅依赖有限的数据集合是不足以评估方法的泛化能力的。在一些数据敏感领域，如教育或医学，公共数据集的获取更加有限。为了解决上述问题，我们发布了PyGraft，一个基于Python的工具，用于生成高度定制的、与领域无关的模式和知识图谱。合成的模式包括各种RDFS和OWL构造，而合成的KG则模拟了真实KG的特性和规模。通过运行描述逻辑（DL）推理器，最终确保生成资源的逻辑一致性。

    Knowledge graphs (KGs) have emerged as a prominent data representation and management paradigm. Being usually underpinned by a schema (e.g. an ontology), KGs capture not only factual information but also contextual knowledge. In some tasks, a few KGs established themselves as standard benchmarks. However, recent works outline that relying on a limited collection of datasets is not sufficient to assess the generalization capability of an approach. In some data-sensitive fields such as education or medicine, access to public datasets is even more limited. To remedy the aforementioned issues, we release PyGraft, a Python-based tool that generates highly customized, domain-agnostic schemas and knowledge graphs. The synthesized schemas encompass various RDFS and OWL constructs, while the synthesized KGs emulate the characteristics and scale of real-world KGs. Logical consistency of the generated resources is ultimately ensured by running a description logic (DL) reasoner. By providing a way
    
[^18]: 从弱标记视频中生成数据集并进行倭黑猩猩分类的研究

    Dataset Generation and Bonobo Classification from Weakly Labelled Videos. (arXiv:2309.03671v1 [cs.CV])

    [http://arxiv.org/abs/2309.03671](http://arxiv.org/abs/2309.03671)

    本研究提出了一种从弱标记视频中生成倭黑猩猩数据集并进行分类的方法，并探究了不同的特征提取和分类算法。研究结果表明，数据准备的重要性以及正确的数据分离对于分类性能的影响很大。

    

    本文介绍了一个基于常用机器学习方法构建的倭黑猩猩检测和分类流程。该应用的动机是为了在没有人的帮助下，使用触摸屏设备对倭黑猩猩在它们的围栏中进行测试。该研究引入了一个新获得的数据集，该数据集是基于倭黑猩猩录像的自动产生的。这些录像是弱标记的，并通过猕猴检测器进行空间检测，以检测视频中出现的个体。使用手工特征以及不同的分类算法和基于ResNet架构的深度学习方法进行倭黑猩猩识别的研究。使用不同的数据分离方法，在数据库的拆分上，以分类准确度作为性能指标进行比较。我们证明了数据准备的重要性以及错误的数据分离如何导致虚假的好结果。最后，经过有意义的数据分离后，得到了最佳的分类性能。

    This paper presents a bonobo detection and classification pipeline built from the commonly used machine learning methods. Such application is motivated by the need to test bonobos in their enclosure using touch screen devices without human assistance. This work introduces a newly acquired dataset based on bonobo recordings generated semi-automatically. The recordings are weakly labelled and fed to a macaque detector in order to spatially detect the individual present in the video. Handcrafted features coupled with different classification algorithms and deep-learning methods using a ResNet architecture are investigated for bonobo identification. Performance is compared in terms of classification accuracy on the splits of the database using different data separation methods. We demonstrate the importance of data preparation and how a wrong data separation can lead to false good results. Finally, after a meaningful separation of the data, the best classification performance is obtained u
    
[^19]: 如何攻击可以干扰看似稳定准确的分类器

    How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])

    [http://arxiv.org/abs/2309.03665](http://arxiv.org/abs/2309.03665)

    本文研究了对抗性攻击如何通过微小修改干扰准确的分类器，并发现这可能是高维输入数据下分类器的基本特征。作者提出了一个通用的框架，解释了实际系统中观察到的关键行为，包括模型对对抗性攻击的容易受到影响，同时对随机扰动具有鲁棒性。验证实验还表明了相同现象在实际神经网络中的存在。

    

    对抗性攻击通过对输入数据进行微小的修改，极大地改变了原本准确的学习系统的输出。具有讽刺意味的是，经验证据表明，即使系统对输入数据的大幅度随机扰动具有鲁棒性，它们仍然容易受到输入数据的小众、易于构造的对抗性扰动的影响。在这里，我们展示了这可能是高维输入数据下分类器的一个基本特征。我们引入了一个简单的通用性和普适性框架，其中在实际系统中观察到的关键行为具有高概率出现，尤其是（原本准确的）模型对易于构造的对抗性攻击的同时容易受到输入数据的随机扰动的影响。我们在标准图像分类问题上验证了相同现象在实际神经网络中的直接观察结果，即使是大幅度的加性随机噪声也无法干扰模型的准确性。

    Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fai
    
[^20]: 在语义图像分割中实现可比较的知识蒸馏

    Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])

    [http://arxiv.org/abs/2309.03659](http://arxiv.org/abs/2309.03659)

    在这项研究中，我们探索了语义图像分割中知识蒸馏的问题。我们发现了25种蒸馏损失项，并指出由于训练配置的差异导致术语比较困难。此外，我们发现超参数选择不当会导致极端的性能差异。

    

    知识蒸馏（KD）是解决语义分割中大模型尺寸和慢推理速度的一种提出的解决方案。在我们的研究中，我们从过去4年的14个出版物中鉴定出了25个提出的蒸馏损失项。不幸的是，基于已发布结果的术语比较通常是不可能的，因为训练配置的差异。这个问题的一个很好的例子是对比2022年的两个出版物。使用相同的模型和数据集，结构和统计纹理蒸馏（SSTKD）报告了学生mIoU增加了4.54个百分点，最终性能达到了29.19，而自适应透视蒸馏（APD）仅仅提高了学生性能2.06个百分点，但实现了39.25的最终性能。这种极端差异的原因通常是超参数的次优选择以及作为参考点的学生模型性能不佳。在我们的工作中，我们揭示了超参数不足的问题。

    Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter
    
[^21]: 解剖学指导下的数据增强技术用于增强前列腺癌检测

    Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection. (arXiv:2309.03652v1 [eess.IV])

    [http://arxiv.org/abs/2309.03652](http://arxiv.org/abs/2309.03652)

    这项研究提出了一种新的解剖学指导的数据增强技术，通过模拟前列腺的生理变形并生成独特的病变形状，从而增强了前列腺癌检测模型的泛化能力。

    

    数据增强是医学图像分析中的关键因素，例如在磁共振图像上的前列腺癌检测中。现有的计算机辅助诊断系统仍然依赖于简单的空间变换来保留病理标签。然而，这样的增强并不能显著增加训练集中的器官和肿瘤形状的变异性，限制了模型在具有更多不同局部软组织变形的未见案例中推广的能力。我们提出了一种新的解剖学指导的转换方法，利用相邻器官的信息来模拟前列腺的典型生理变形，并产生独特的病变形状而不改变其标签。由于其轻量级的计算要求，它可以轻松集成到常见的数据增强框架中。通过评估一组774个活检确认的检查，我们证明了我们的增强技术的有效性。

    Data augmentation (DA) is a key factor in medical image analysis, such as in prostate cancer (PCa) detection on magnetic resonance images. State-of-the-art computer-aided diagnosis systems still rely on simplistic spatial transformations to preserve the pathological label post transformation. However, such augmentations do not substantially increase the organ as well as tumor shape variability in the training set, limiting the model's ability to generalize to unseen cases with more diverse localized soft-tissue deformations. We propose a new anatomy-informed transformation that leverages information from adjacent organs to simulate typical physiological deformations of the prostate and generates unique lesion shapes without altering their label. Due to its lightweight computational requirements, it can be easily integrated into common DA frameworks. We demonstrate the effectiveness of our augmentation on a dataset of 774 biopsy-confirmed examinations, by evaluating a state-of-the-art m
    
[^22]: 在基于网格的强化学习环境中学习通用和可解释的知识

    Learning of Generalizable and Interpretable Knowledge in Grid-Based Reinforcement Learning Environments. (arXiv:2309.03651v1 [cs.AI])

    [http://arxiv.org/abs/2309.03651](http://arxiv.org/abs/2309.03651)

    本文提出了在基于网格的强化学习环境中使用程序合成来学习通用和可解释的知识，以解决智能体行为理解的问题。

    

    对于在游戏或现实世界中部署经过深度强化学习训练的智能体来说，理解其交互是非常重要的。在游戏中，不合理的动作会困惑玩家。在现实世界中，这种影响更加显著，因为意外行为可能导致事故，对相关人员可能产生严重而长远的后果。本文提出使用程序合成在观察到一系列动作轨迹后模仿强化学习策略。程序具有固有的可解释性和正确性可验证性的优势。我们针对基于网格的环境（包括导航任务和两个迷你版的Atari游戏：Space Invaders和Asterix）改造了先进的程序合成系统DreamCoder，通过检查生成的库，我们可以推断出黑盒智能体学习的概念，并更好地理解智能体的行为。

    Understanding the interactions of agents trained with deep reinforcement learning is crucial for deploying agents in games or the real world. In the former, unreasonable actions confuse players. In the latter, that effect is even more significant, as unexpected behavior cause accidents with potentially grave and long-lasting consequences for the involved individuals. In this work, we propose using program synthesis to imitate reinforcement learning policies after seeing a trajectory of the action sequence. Programs have the advantage that they are inherently interpretable and verifiable for correctness. We adapt the state-of-the-art program synthesis system DreamCoder for learning concepts in grid-based environments, specifically, a navigation task and two miniature versions of Atari games, Space Invaders and Asterix. By inspecting the generated libraries, we can make inferences about the concepts the black-box agent has learned and better understand the agent's behavior. We achieve th
    
[^23]: GNN对公平性稳定性的Lipschitz特性表征

    Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])

    [http://arxiv.org/abs/2309.03648](http://arxiv.org/abs/2309.03648)

    论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。

    

    Lipschitz界限是从鲁棒统计学中借鉴的一种技术，可以限制输出相对于输入的最大变化，考虑到相关的非关键偏倚因素。这是一种高效且可证明的方法，可以检查机器学习模型的输出稳定性，而不会增加额外的计算成本。最近，对于在非欧几里得数据上操作的图神经网络（GNN）引起了广泛的关注。然而，之前没有研究调查GNN的Lipschitz界限以揭示模型输出的稳定性，特别是在处理具有固有偏倚的非欧几里得数据时。由于常见图形数据在GNN训练中存在固有偏差，这给限制由输入偏差引起的GNN输出扰动，从而在训练期间保障公平性，带来了严峻的挑战。最近，尽管Lipschitz常数在控制欧几里得神经网络的稳定性方面有所应用，但精确Lipschitz常数的计算十分困难。

    The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
    
[^24]: VideolandGPT：关于会话式推荐系统的用户研究

    VideolandGPT: A User Study on a Conversational Recommender System. (arXiv:2309.03645v1 [cs.IR])

    [http://arxiv.org/abs/2309.03645](http://arxiv.org/abs/2309.03645)

    本研究通过使用大语言模型VideolandGPT改进了会话式推荐系统，实验表明个性化版本在准确性和用户满意度方面优于非个性化版本，但两个版本在公平性方面存在不一致行为。

    

    本文研究了如何通过大语言模型（LLMs）增强推荐系统，重点关注利用用户偏好和现有排名模型的个性化候选选择的会话式推荐系统。我们介绍了VideolandGPT，这是一个用于视频点播平台Videoland的推荐系统，它使用ChatGPT从预定内容集合中进行选择，考虑到用户与聊天界面的交互所示的额外上下文。我们通过一项用户研究，比较了个性化和非个性化版本的系统在排名指标、用户体验和推荐的公平性方面的表现。我们的结果表明，个性化版本在准确性和一般用户满意度方面优于非个性化版本，而两个版本都增加了排名推荐列表中非前列的项目的可见性。然而，在公平性方面，两个版本的行为都不一致。

    This paper investigates how large language models (LLMs) can enhance recommender systems, with a specific focus on Conversational Recommender Systems that leverage user preferences and personalised candidate selections from existing ranking models. We introduce VideolandGPT, a recommender system for a Video-on-Demand (VOD) platform, Videoland, which uses ChatGPT to select from a predetermined set of contents, considering the additional context indicated by users' interactions with a chat interface. We evaluate ranking metrics, user experience, and fairness of recommendations, comparing a personalised and a non-personalised version of the system, in a between-subject user study. Our results indicate that the personalised version outperforms the non-personalised in terms of accuracy and general user satisfaction, while both versions increase the visibility of items which are not in the top of the recommendation lists. However, both versions present inconsistent behavior in terms of fairn
    
[^25]: 超越可解释AI：负责人工智能的障碍

    Beyond XAI:Obstacles Towards Responsible AI. (arXiv:2309.03638v1 [cs.AI])

    [http://arxiv.org/abs/2309.03638](http://arxiv.org/abs/2309.03638)

    这篇论文探讨了可解释AI领域中存在的限制，并在考虑隐私、公平性和可争议性等其他重要方面时讨论了负责任AI的意义。

    

    快速发展的可解释人工智能（XAI）领域引发了对开发技术以使AI系统更透明和可理解的兴趣。然而，在现实世界的背景下，解释性方法及其评估策略存在许多限制。此外，负责任的AI的范围不仅限于解释性。本文探讨了这些限制，并讨论了在考虑隐私、公平性和可争议性等其他重要方面时的责任AI的含义。

    The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has sparked significant interests in developing techniques to make AI systems more transparent and understandable. Nevertheless, in real-world contexts, the methods of explainability and their evaluation strategies present numerous limitations.Moreover, the scope of responsible AI extends beyond just explainability. In this paper, we explore these limitations and discuss their implications in a boarder context of responsible AI when considering other important aspects, including privacy, fairness and contestability.
    
[^26]: NeuroCodeBench：用于软件验证的纯 C 神经网络基准测试

    NeuroCodeBench: a plain C neural network benchmark for software verification. (arXiv:2309.03617v1 [cs.SE])

    [http://arxiv.org/abs/2309.03617](http://arxiv.org/abs/2309.03617)

    NeuroCodeBench是一个用纯C编写的神经网络代码验证基准测试，共包含32个神经网络和607个安全属性。研究发现，由于不完全支持标准C数学库和复杂的大型神经网络的复杂性，目前最先进的软件验证器难以提供正确的判断。

    

    具有神经网络组件的安全关键系统需要强有力的保证。虽然现有的神经网络验证技术在这个目标上取得了很大的进展，但它们无法证明网络实现中不存在软件错误。本文介绍了 NeuroCodeBench - 一个用纯 C 编写的神经网络代码验证基准测试。它包含了32个神经网络，共有607个安全属性，分为6个类别：数学库、激活函数、纠错网络、传输函数逼近、概率密度估计和强化学习。我们的初步评估显示，最先进的软件验证器在提供正确的判断时遇到了困难，这是由于它们对标准 C 数学库的不完全支持和更复杂的大型神经网络的复杂性所导致的。

    Safety-critical systems with neural network components require strong guarantees. While existing neural network verification techniques have shown great progress towards this goal, they cannot prove the absence of software faults in the network implementation. This paper presents NeuroCodeBench - a verification benchmark for neural network code written in plain C. It contains 32 neural networks with 607 safety properties divided into 6 categories: maths library, activation functions, error-correcting networks, transfer function approximation, probability density estimation and reinforcement learning. Our preliminary evaluation shows that state-of-the-art software verifiers struggle to provide correct verdicts, due to their incomplete support of the standard C mathematical library and the complexity of larger neural networks.
    
[^27]: 评估ChatGPT作为推荐系统的严谨方法

    Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])

    [http://arxiv.org/abs/2309.03613](http://arxiv.org/abs/2309.03613)

    这项研究评估了ChatGPT作为推荐系统的能力，通过探索其利用用户偏好进行推荐、重新排序推荐列表、利用相似用户信息以及处理冷启动情况的能力，并使用三个数据集进行了全面实验。

    

    由于其卓越的自然语言处理能力，大型AI语言模型近年来备受关注。它们在语言相关任务中具有重要贡献，包括基于提示的学习，因此对于各种特定任务非常有价值。这种方法释放了它们的全部潜力，提高了准确性和泛化性。研究界正在积极探索它们的应用，ChatGPT也因此获得了认可。尽管大型语言模型已经有了广泛的研究，但其在推荐场景中的潜力仍待探索。本研究旨在填补这一空白，通过探究ChatGPT作为零-shot推荐系统的能力。我们的目标包括评估其利用用户偏好进行推荐、重新排序现有推荐列表、利用相似用户的信息以及处理冷启动情况的能力。我们通过对三个数据集（MovieLens Small、Last.FM和Facebook Bo）进行全面实验来评估ChatGPT的性能。

    Recent popularity surrounds large AI language models due to their impressive natural language capabilities. They contribute significantly to language-related tasks, including prompt-based learning, making them valuable for various specific tasks. This approach unlocks their full potential, enhancing precision and generalization. Research communities are actively exploring their applications, with ChatGPT receiving recognition. Despite extensive research on large language models, their potential in recommendation scenarios still needs to be explored. This study aims to fill this gap by investigating ChatGPT's capabilities as a zero-shot recommender system. Our goals include evaluating its ability to use user preferences for recommendations, reordering existing recommendation lists, leveraging information from similar users, and handling cold-start situations. We assess ChatGPT's performance through comprehensive experiments using three datasets (MovieLens Small, Last.FM, and Facebook Bo
    
[^28]: BOLD fMRI时间序列的空间编码用于对视觉数据集中的静态图像进行分类：人类视觉的一项试验研究。(arXiv:2309.03590v1 [eess.IV])

    Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision. (arXiv:2309.03590v1 [eess.IV])

    [http://arxiv.org/abs/2309.03590](http://arxiv.org/abs/2309.03590)

    通过使用空间编码的BOLD fMRI时间序列，本研究对视觉数据集中的静态图像进行分类，并发现了与视觉相关的神经活动的差异。

    

    功能性磁共振成像（fMRI）被广泛用于通过检测与脑活动相关的氧合血流变化来研究脑功能。本研究使用fMRI时间序列（TS）进行特异性图像分类，以了解与视觉相关的神经活动的差异。使用公开可用的BOLD5000数据集，其中包含查看来自COCO、ImageNet和SUN三个标准计算机视觉数据集中5254张不同类别的图像的fMRI扫描。为了理解视觉，研究大脑在观看不同图像时的功能是很重要的。为了实现这一目标，进行了fMRI BOLD TS的空间编码，使用经典的格林角场（GAF）和马尔可夫转移场（MTF）获取二维BOLD TS，代表了COCO、Imagenet和SUN的图像。对于分类，将单个GAF和MTF特征输入常规CNN。随后，采用并行CNN模型进行处理。

    Functional MRI (fMRI) is widely used to examine brain functionality by detecting alteration in oxygenated blood flow that arises with brain activity. In this study, complexity specific image categorization across different visual datasets is performed using fMRI time series (TS) to understand differences in neuronal activities related to vision. Publicly available BOLD5000 dataset is used for this purpose, containing fMRI scans while viewing 5254 images of diverse categories, drawn from three standard computer vision datasets: COCO, ImageNet and SUN. To understand vision, it is important to study how brain functions while looking at different images. To achieve this, spatial encoding of fMRI BOLD TS has been performed that uses classical Gramian Angular Field (GAF) and Markov Transition Field (MTF) to obtain 2D BOLD TS, representing images of COCO, Imagenet and SUN. For classification, individual GAF and MTF features are fed into regular CNN. Subsequently, parallel CNN model is employe
    
[^29]: 通过偏好学习在多目标问题中进行交互式超参数优化

    Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])

    [http://arxiv.org/abs/2309.03581](http://arxiv.org/abs/2309.03581)

    本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。

    

    超参数优化对于发挥机器学习的潜力至关重要。在实践中，用户通常对多目标问题感兴趣，即优化可能存在冲突的目标，比如准确性和能耗。为了解决这个问题，绝大多数多目标机器学习算法将一组非支配的机器学习模型的帕累托前沿返回给用户。然而，优化这种算法的超参数并不容易，因为评估一个超参数配置涉及评估得到的帕累托前沿的质量。在文献中，已有一些指标可以通过量化不同属性（如体积、与参考点的接近程度）来评估帕累托前沿的质量（例如超体积、R2）。然而，对于用户来说，选择导致期望的帕累托前沿的指标可能是一项困难的任务。在本文中，我们提出了一个以人为中心的交互式超参数优化方法，针对多目标机器学习应用偏好学习。

    Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
    
[^30]: DTW+S: 使用有序局部趋势进行基于形状的时间序列比较

    DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])

    [http://arxiv.org/abs/2309.03579](http://arxiv.org/abs/2309.03579)

    提出了一种名为DTW+S的新型测量方法，它通过创建局部趋势的矩阵表示，并应用动态时间规整来计算距离，解决了现有方法无法捕捉局部趋势相似性的问题。

    

    时间序列数据的距离或相似度的测量是许多应用包括分类和聚类的基本方面。现有的测量方法可能由于局部趋势（形状）而无法捕捉到相似之处，甚至可能产生误导性的结果。我们的目标是开发一种能够寻找在相似时间周围发生的相似趋势的测量方法，并且对应用领域的研究人员易于解释的方法。这对于时间序列具有有序的有意义的局部趋势序列的应用特别有用，例如在流行病中（从增长到峰值再到减少）。我们提出了一种新的测量方法，DTW+S，它创建了一个可解释的“保持接近性”的矩阵表示时间序列，其中每一列代表局部趋势，然后应用动态时间规整来计算这些矩阵之间的距离。我们提供了支持这种表示的理论分析。我们展示了DTW+S的实用性。

    Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in 
    
[^31]: 重用与扩散：用于文本到视频生成的迭代去噪方法

    Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. (arXiv:2309.03549v1 [cs.CV])

    [http://arxiv.org/abs/2309.03549](http://arxiv.org/abs/2309.03549)

    这篇论文研究了在文本到视频生成中，使用重用和迭代扩散的方法可以生成更多视频帧，相比其他方法，该方法避免了额外的训练成本和帧级抖动。

    

    受潜在扩散模型（LDM）在图像合成方面的显著成功启发，我们研究了用于文本到视频生成的LDM，这是由于模型训练和推断过程中的计算和内存限制而面临的重大挑战。单个LDM通常只能生成有限数量的视频帧。一些现有的工作专注于为生成更多的视频帧而使用独立的预测模型，但这会导致额外的训练成本和帧级抖动。在本文中，我们提出了一个名为“重用与扩散”的框架，称为$\textit{VidRD}$，以生成更多的帧，并且这些帧是在由LDM生成的先前帧之后产生的。此外，对于用于像素空间和潜在空间之间转换的自编码器，我们将时间层注入其解码器中。

    Inspired by the remarkable success of Latent Diffusion Models (LDMs) for image synthesis, we study LDM for text-to-video generation, which is a formidable challenge due to the computational and memory constraints during both model training and inference. A single LDM is usually only capable of generating a very limited number of video frames. Some existing works focus on separate prediction models for generating more video frames, which suffer from additional training cost and frame-level jittering, however. In this paper, we propose a framework called "Reuse and Diffuse" dubbed $\textit{VidRD}$ to produce more frames following the frames already generated by an LDM. Conditioned on an initial video clip with a small number of frames, additional frames are iteratively generated by reusing the original latent features and following the previous diffusion process. Besides, for the autoencoder used for translation between pixel space and latent space, we inject temporal layers into its dec
    
[^32]: DGC: 使用分块图分区训练具有时空非均匀性的动态图

    DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks. (arXiv:2309.03523v1 [cs.DC])

    [http://arxiv.org/abs/2309.03523](http://arxiv.org/abs/2309.03523)

    本论文提出了DGC系统，利用一种新的图分区方法实现了动态图神经网络(DGNN)的高效训练，解决了实际动态图非均匀结构的问题，实现了1.25倍-7.52倍的训练加速。

    

    动态图神经网络(DGNN)通过利用时空特征展示了学习动态图的强大能力。尽管DGNN最近受到人工智能社区的广泛关注，并且提出了各种DGNN模型，但构建高效的DGNN训练分布式系统仍然具有挑战性。已经公认的是，如何将动态图划分并将工作量分配给多个GPU在训练加速中起到了关键作用。现有工作将动态图划分为快照或时间序列，但这仅在图具有均匀时空结构时起作用。然而，实际中的动态图不具有均匀结构，其中一些快照非常稠密，而其他快照非常稀疏。为了解决这个问题，我们提出了DGC，一种分布式DGNN训练系统，在我们的测试平台上实现了比最先进方法快1.25倍-7.52倍的加速。DGC的成功源于一种新的图分区方法，它能够根据图的时空非均匀性进行分块。

    Dynamic Graph Neural Network (DGNN) has shown a strong capability of learning dynamic graphs by exploiting both spatial and temporal features. Although DGNN has recently received considerable attention by AI community and various DGNN models have been proposed, building a distributed system for efficient DGNN training is still challenging. It has been well recognized that how to partition the dynamic graph and assign workloads to multiple GPUs plays a critical role in training acceleration. Existing works partition a dynamic graph into snapshots or temporal sequences, which only work well when the graph has uniform spatio-temporal structures. However, dynamic graphs in practice are not uniformly structured, with some snapshots being very dense while others are sparse. To address this issue, we propose DGC, a distributed DGNN training system that achieves a 1.25x - 7.52x speedup over the state-of-the-art in our testbed. DGC's success stems from a new graph partitioning method that parti
    
[^33]: 参数化视角下的Distinct Kemeny排序聚合的方面

    Parameterized Aspects of Distinct Kemeny Rank Aggregation. (arXiv:2309.03517v1 [cs.DS])

    [http://arxiv.org/abs/2309.03517](http://arxiv.org/abs/2309.03517)

    这项研究从参数化复杂性的角度出发，研究了Distinct Kemeny排序聚合的多个参数，包括目标Kemeny分数、候选人数量、输入排序的平均距离、任何候选人的最大范围和一致性宽度，并提出了相应的FPT算法和FPT近似算法。

    

    Kemeny方法是一种受欢迎的排序聚合工具，但计算最优的Kemeny排序是NP-hard的。因此，将找到Kemeny排序的计算任务从多个参数的角度进行了研究。首先，我们提出了这些参数之间的全面关系，包括理论和实证。此外，我们还从参数化复杂性的角度研究了计算所有不同的Kemeny排序的问题。我们考虑了目标Kemeny分数、候选人数量、输入排序的平均距离、任何候选人的最大范围和一致性宽度作为我们的参数。对于所有这些参数，我们已经有了FPT算法。我们发现，在不显著增加运行时间的情况下，也可以找到任意数量的Kemeny排序。我们还提出了相对于这些参数的Kemeny排序聚合的FPT近似算法。

    The Kemeny method is one of the popular tools for rank aggregation. However, computing an optimal Kemeny ranking is NP-hard. Consequently, the computational task of finding a Kemeny ranking has been studied under the lens of parameterized complexity with respect to many parameters. We first present a comprehensive relationship, both theoretical and empirical, among these parameters. Further, we study the problem of computing all distinct Kemeny rankings under the lens of parameterized complexity. We consider the target Kemeny score, number of candidates, average distance of input rankings, maximum range of any candidate, and unanimity width as our parameters. For all these parameters, we already have FPT algorithms. We find that any desirable number of Kemeny rankings can also be found without substantial increase in running time. We also present FPT approximation algorithms for Kemeny rank aggregation with respect to these parameters.
    
[^34]: 实现对同侧双视角乳腺癌分析中健壮的自然外观乳腺X线造影病灶合成

    Towards Robust Natural-Looking Mammography Lesion Synthesis on Ipsilateral Dual-Views Breast Cancer Analysis. (arXiv:2309.03506v1 [cs.CV])

    [http://arxiv.org/abs/2309.03506](http://arxiv.org/abs/2309.03506)

    这篇论文提出了一种简单且新颖的方法，通过利用辅助视角的低层特征信息来增强乳腺X线造影图像的主要视角，然后学习包含癌变特征的高层特征。此外，还提出了一种简单且新颖的恶性乳腺X线造影合成框架，用于上采样少数类样本。

    

    近年来，为了改善乳腺癌分类任务，引入了许多乳腺X线造影图像分析方法。乳腺X线造影图像分类任务的两个主要问题是利用多视角的乳腺X线造影信息和处理类别不平衡。对于第一个问题，已经发布了许多多视角方法，用于将两个或多个视角的特征在训练和推理阶段进行拼接。尽管如此，大多数多视角的现有方法在特征融合的意义上并不具备解释性，并且在诊断时对许多视角平等对待。我们的工作旨在提出一种简单但新颖的方法，通过利用辅助视角（同侧视角）的低层特征信息，增强检查视角（主要视角），然后学习包含癌变特征的高层特征。对于第二个问题，我们还提出了一种简单但新颖的恶性乳腺X线造影合成框架，用于上采样少数类样本。我们易于实施且无需训练的框架已经消除了

    In recent years, many mammographic image analysis methods have been introduced for improving cancer classification tasks. Two major issues of mammogram classification tasks are leveraging multi-view mammographic information and class-imbalance handling. In the first problem, many multi-view methods have been released for concatenating features of two or more views for the training and inference stage. Having said that, most multi-view existing methods are not explainable in the meaning of feature fusion, and treat many views equally for diagnosing. Our work aims to propose a simple but novel method for enhancing examined view (main view) by leveraging low-level feature information from the auxiliary view (ipsilateral view) before learning the high-level feature that contains the cancerous features. For the second issue, we also propose a simple but novel malignant mammogram synthesis framework for upsampling minor class samples. Our easy-to-implement and no-training framework has elimi
    
[^35]: InteractionNet：基于Transformer的自动驾驶规划与预测的联合

    InteractionNet: Joint Planning and Prediction for Autonomous Driving with Transformers. (arXiv:2309.03475v1 [cs.RO])

    [http://arxiv.org/abs/2309.03475](http://arxiv.org/abs/2309.03475)

    InteractionNet是基于Transformer的自动驾驶规划与预测的联合模型，可以捕捉交互并联合考虑规划和预测，在安全性等方面优于其他基线模型。

    

    规划和预测是自动驾驶中的两个重要模块，并且最近有了巨大的进展。然而，大多数现有方法将规划和预测视为独立的，忽略它们之间的相关性，导致缺乏对交互和交通场景的动态变化的考虑。为了解决这个问题，我们提出了InteractionNet，它利用Transformer在所有交通参与者之间共享全局上下文推理，以捕捉交互并将规划和预测互联起来实现联合。此外，InteractionNet还部署了另一个Transformer，帮助模型额外关注包含关键或未见车辆的感知区域。InteractionNet在几个基准测试中优于其他基线，特别是在安全性方面，这得益于对规划和预测的联合考虑。代码将在https://github.com/fujiawei0724/InteractionNet上提供。

    Planning and prediction are two important modules of autonomous driving and have experienced tremendous advancement recently. Nevertheless, most existing methods regard planning and prediction as independent and ignore the correlation between them, leading to the lack of consideration for interaction and dynamic changes of traffic scenarios. To address this challenge, we propose InteractionNet, which leverages transformer to share global contextual reasoning among all traffic participants to capture interaction and interconnect planning and prediction to achieve joint. Besides, InteractionNet deploys another transformer to help the model pay extra attention to the perceived region containing critical or unseen vehicles. InteractionNet outperforms other baselines in several benchmarks, especially in terms of safety, which benefits from the joint consideration of planning and forecasting. The code will be available at https://github.com/fujiawei0724/InteractionNet.
    
[^36]: 快速的FixMatch: 基于课程批次大小的快速半监督学习

    Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])

    [http://arxiv.org/abs/2309.03469](http://arxiv.org/abs/2309.03469)

    本论文提出了快速的FixMatch算法，通过引入课程批次大小（CBS）来减少半监督学习的训练计算量，并使用强化标记增强和课程伪标签进行改进。

    

    半监督学习（SSL）的进展几乎完全消除了SSL和监督学习之间的差距，同时减少了标签数量。然而，最近的性能提升往往是以显著增加的训练计算为代价的。为了解决这个问题，我们提出了课程批次大小（CBS），利用深度神经网络的自然训练动态，采用一个小的未标记的批次大小开始训练，并逐渐增加到训练结束。无论数据集、模型或训练轮次，都使用固定的课程，通过在所有设置中减少训练计算量。我们将CBS、强化标记增强和课程伪标签（CPL）应用于FixMatch，并将这个新的SSL算法称为快速的FixMatch。我们进行了割实验，表明强化标记增强和/或CPL并不显著地减少训练量。

    Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training 
    
[^37]: 跨图像上下文对于Bongard问题很重要

    Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])

    [http://arxiv.org/abs/2309.03468](http://arxiv.org/abs/2309.03468)

    Bongard问题是一种需要从一组正负图像中推导出抽象概念并进行分类的智力测试，现有方法在Bongard问题中准确率较低。本研究发现，这是因为现有方法未能整合支持集合中的信息，而是仅依赖于单个支持图像的信息。我们提出了一种通过跨图像上下文来提高准确性的解决方案。

    

    目前的机器学习方法在解决Bongard问题时存在困难。Bongard问题是一种需要从一组正负“支持”图像中推导出抽象“概念”，然后对于新的查询图像进行分类，判断它是否描述了关键概念的智力测试。在用于自然图像Bongard问题的基准测试Bongard-HOI中，现有方法的准确率仅达到了66%（偶然准确率为50%）。低准确率通常归因于神经网络缺乏发现类似人类符号规则的能力。我们指出，许多现有方法由于一个更简单的问题而失去了准确性：它们没有将支持集合中的信息作为一个整体加入，而是依赖于从单个支持中提取的信息。这是一个关键问题，因为与涉及对象分类的少样本学习任务不同，一个典型的Bongard问题中的“关键概念”只能使用多个正例和多个反例来区分。我们探索了一种解决方案，通过跨图像上下文来提高准确性。

    Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a
    
[^38]: Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])

    Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])

    [http://arxiv.org/abs/2309.03467](http://arxiv.org/abs/2309.03467)

    该论文提出了一种自回归全方位感知生成网络（AOG-Net）用于生成360度图像，通过渐进地外扩不完整的360度图像，并与窄视场（NFoV）和文本引导相结合或单独使用。这种方法可以生成更细致和与文本一致的模式，并为用户在生成过程中灵活编辑条件。

    

    360度（全方位）图像提供了一个场景的全景球状视图。最近，在从由数字相机和智能手机捕获的传统窄视场（NFoV）图像合成360度图像方面引起了越来越多的关注，以在各种场景中提供身临其境的体验，如虚拟现实。然而，现有的方法通常无法合成复杂的视觉细节或确保生成的图像与用户提供的提示一致。在这项研究中，提出了一种自回归全方位感知生成网络（AOG-Net），通过使用NFoV和文本引导相结合或单独进行渐进地外扩不完整的360度图像来进行360度图像生成。这种自回归方案不仅允许通过动态生成和调整过程来获取更精细和与文本一致的模式，还为用户在生成过程中编辑条件提供了更大的灵活性。

    A 360-degree (omni-directional) image provides an all-encompassing spherical view of a scene. Recently, there has been an increasing interest in synthesising 360-degree images from conventional narrow field of view (NFoV) images captured by digital cameras and smartphones, for providing immersive experiences in various scenarios such as virtual reality. Yet, existing methods typically fall short in synthesizing intricate visual details or ensure the generated images align consistently with user-provided prompts. In this study, autoregressive omni-aware generative network (AOG-Net) is proposed for 360-degree image generation by out-painting an incomplete 360-degree image progressively with NFoV and text guidances joinly or individually. This autoregressive scheme not only allows for deriving finer-grained and text-consistent patterns by dynamically generating and adjusting the process but also offers users greater flexibility to edit their conditions throughout the generation process. A
    
[^39]: 利用基于模型反演的去除攻击方法破解深度神经网络中的黑盒水印

    MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks. (arXiv:2309.03466v1 [cs.CR])

    [http://arxiv.org/abs/2309.03466](http://arxiv.org/abs/2309.03466)

    本文提出了一种基于模型反演的去除攻击方法（\textsc{Mira}），该方法能够有效地破解大多数主流黑盒DNN水印方案，通过利用受保护模型的内部信息来恢复和消除水印信息。

    

    为了保护训练有素的深度神经网络（DNN）的知识产权，黑盒DNN水印已在学术界和工业界越来越受欢迎。这些水印被嵌入到DNN模型在一组特别设计的样本上的预测行为中。最近的研究经验证明，大多数黑盒水印方案对已知的去除攻击具有抵抗能力。本文提出了一种新颖的基于模型反演的去除攻击方法（\textsc{Mira}），该方法对大多数主流黑盒DNN水印方案都是无关水印的，并且具有高效性。总的来说，我们的攻击流程利用受保护模型的内部信息来恢复和消除水印信息。我们还设计了目标类别检测和恢复样本分割算法，以减少\textsc{Mira}引起的效用损失，并实现最优的攻击效果。

    To protect the intellectual property of well-trained deep neural networks (DNNs), black-box DNN watermarks, which are embedded into the prediction behavior of DNN models on a set of specially-crafted samples, have gained increasing popularity in both academy and industry. Watermark robustness is usually implemented against attackers who steal the protected model and obfuscate its parameters for watermark removal. Recent studies empirically prove the robustness of most black-box watermarking schemes against known removal attempts.  In this paper, we propose a novel Model Inversion-based Removal Attack (\textsc{Mira}), which is watermark-agnostic and effective against most of mainstream black-box DNN watermarking schemes. In general, our attack pipeline exploits the internals of the protected model to recover and unlearn the watermark message. We further design target class detection and recovered sample splitting algorithms to reduce the utility loss caused by \textsc{Mira} and achieve 
    
[^40]: SyncDreamer: 从单视图图像生成多视角一致性图像

    SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. (arXiv:2309.03453v1 [cs.CV])

    [http://arxiv.org/abs/2309.03453](http://arxiv.org/abs/2309.03453)

    本文提出了一种名为SyncDreamer的新型扩散模型，可以从单视图图像生成多视角一致性图像。通过同步所有生成图像的中间状态，并利用3D感知特征注意机制，SyncDreamer能够实现在不同视角上生成高度一致的图像，为各种3D生成任务提供了有力的支持。

    

    本文提出了一种名为SyncDreamer的新型扩散模型，可以从单视图图像生成多视角一致性图像。通过使用预训练的大规模2D扩散模型，最近的Zero123工作展示了从单视图物体图像生成合理的新视角的能力。然而，生成的图像在几何和颜色上的一致性仍然是一个挑战。为了解决这个问题，我们提出了一种同步的多视角扩散模型，它模拟了多视角图像的联合概率分布，可以通过单个反向过程生成多视角一致性图像。SyncDreamer通过3D感知特征注意机制，在反向过程的每个步骤中同步所有生成图像的中间状态，从而相关联不同视角上的相应特征。实验表明，SyncDreamer能够在不同视角之间生成高度一致的图像，因此非常适用于各种3D生成任务。

    In this paper, we present a novel diffusion model called that generates multiview-consistent images from a single-view image. Using pretrained large-scale 2D diffusion models, recent work Zero123 demonstrates the ability to generate plausible novel views from a single-view image of an object. However, maintaining consistency in geometry and colors for the generated images remains a challenge. To address this issue, we propose a synchronized multiview diffusion model that models the joint probability distribution of multiview images, enabling the generation of multiview-consistent images in a single reverse process. SyncDreamer synchronizes the intermediate states of all the generated images at every step of the reverse process through a 3D-aware feature attention mechanism that correlates the corresponding features across different views. Experiments show that SyncDreamer generates images with high consistency across different views, thus making it well-suited for various 3D generation
    
[^41]: XGen-7B技术报告

    XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])

    [http://arxiv.org/abs/2309.03450](http://arxiv.org/abs/2309.03450)

    XGen-7B是一种用于处理长序列的大型语言模型，通过克服开源LLMs在支持长序列长度方面的限制，并在标准基准上取得与最先进的开源LLMs相当或更好的结果，推进了研究进展和商业应用。

    

    大型语言模型（LLMs）在各个领域变得普遍，改变了我们与信息交互和进行研究的方式。然而，大多数高性能的LLMs仍然受限于专有墙壁，阻碍了科学进展。另一方面，大多数开源的LLMs在支持较长序列长度方面有限，而这对于许多需要对输入上下文进行推理的任务来说是一个关键要求。为了解决这个问题，我们训练了XGen，一系列7B参数的模型，可支持长度为8K的序列和1.5T个令牌。我们还对XGen模型进行了公共领域教学数据的微调，创建了它们的教学优化版本（XGen-Inst）。我们将我们的模型开源，用于研究进展和商业应用。我们对标准基准的评估结果显示，与最先进的开源LLMs相比，XGen模型实现了相当或更好的结果。我们针对长序列建模任务进行了有针对性的评估。

    Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling task
    
[^42]: 大型语言模型作为优化器

    Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])

    [http://arxiv.org/abs/2309.03409](http://arxiv.org/abs/2309.03409)

    本论文提出了一种简单有效的方法，利用大型语言模型(LLMs)作为优化器，通过自然语言描述优化任务。经过实验证明，该方法在线性回归和旅行推销员问题上表现出色，并且优化的最佳提示超过了人为设计的提示。

    

    优化是无处不在的。虽然基于导数的算法在各种问题上是强大的工具，但是没有梯度对许多实际应用提出了挑战。在这项工作中，我们提出了一种简单有效的方法，利用大型语言模型(LLMs)作为优化器，其中优化任务以自然语言形式描述。在每一次优化步骤中，LLM从包含先前生成的解与其值的提示中生成新的解，然后对新的解进行评估并添加到提示中，用于下一次优化步骤。我们首先展示了OPRO在线性回归和旅行推销员问题上的应用，然后转向提示优化，目标是找到能最大化任务准确性的指令。通过使用各种LLM，我们证明了OPRO优化的最佳提示在GSM8K上击败了人为设计的提示高达8%，在Big-Bench Hard任务上击败了人为设计的提示高达50%。

    Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
    
[^43]: 通讯和参考曲目在混音过程中的作用：来自专业混音师的见解

    The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers. (arXiv:2309.03404v1 [cs.HC])

    [http://arxiv.org/abs/2309.03404](http://arxiv.org/abs/2309.03404)

    本论文研究了专业混音师与客户的互动以及他们如何利用客户的反馈指导混音过程。结果表明，混音师通过使用参考曲目和合作沟通等方式，与客户建立起对混音期望音效的默契约定。

    

    有效的音乐混音需要技术和创造力的精湛，但与客户的清晰沟通至关重要。混音师必须理解客户的期望和偏好，并共同努力实现所需的音效。通过使用参考曲目和艺术家与工程师之间交换的演示混音等指南，通常可以达成对混音期望音效的默契约定，有时还可以使用语义术语进行言说。本文介绍了一项由两个阶段构成的探索性研究的发现，旨在了解专业混音师如何与客户互动，并利用他们的反馈指导混音过程。在第一阶段，对五名混音师进行了半结构化面谈，旨在了解他们的沟通策略、创造过程和决策标准。基于这些访谈的推论，设计了一个在线问卷，并对22名混音师进行了调查。

    Effective music mixing requires technical and creative finesse, but clear communication with the client is crucial. The mixing engineer must grasp the client's expectations, and preferences, and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides like reference songs and demo mixes exchanged between the artist and the engineer and sometimes verbalised using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers with the aim of gathering insights about their communication strategies, creative processes, and decision-making criteria. Based on the inferences from these interviews, an online questionnaire was designed and administered to a larger group of 22 mixing engineers 
    
[^44]: 自动驾驶中运动预测的高效基线

    Efficient Baselines for Motion Prediction in Autonomous Driving. (arXiv:2309.03387v1 [cs.RO])

    [http://arxiv.org/abs/2309.03387](http://arxiv.org/abs/2309.03387)

    这项研究提出了自动驾驶中运动预测的高效基线模型，解决了使用地图和过去轨迹信息的实时应用复杂性和可解释性的问题。

    

    在任意复杂的环境中，从简单机器人到自动驾驶系统中，多方周围代理的运动预测是一项关键任务。当前的技术通过端到端的流程来解决这个问题，其中输入数据通常是物理信息的渲染顶视图和最相关代理的过去轨迹；利用这些信息是获得最佳性能的必需。在这个意义上，可靠的自动驾驶系统必须及时产生合理的预测。然而，当前的高端模型可能对于同时使用地图和过去轨迹这两种信息以及极少可解释性的物理信息的实时应用而言过于复杂。此外，这些模型的性能高度依赖于每个特定交通场景的可用输入数量，而这些输入难以获取，成本高昂。

    Motion Prediction (MP) of multiple surroundings agents is a crucial task in arbitrarily complex environments, from simple robots to Autonomous Driving Stacks (ADS). Current techniques tackle this problem using end-to-end pipelines, where the input data is usually a rendered top-view of the physical information and the past trajectories of the most relevant agents; leveraging this information is a must to obtain optimal performance. In that sense, a reliable ADS must produce reasonable predictions on time. However, despite many approaches use simple ConvNets and LSTMs to obtain the social latent features, State-Of-The-Art (SOTA) models might be too complex for real-time applications when using both sources of information (map and past trajectories) as well as little interpretable, specially considering the physical information. Moreover, the performance of such models highly depends on the number of available inputs for each particular traffic scenario, which are expensive to obtain, pa
    
[^45]: 基于社区的分级正类-未标记（PU）模型融合用于慢性病预测

    Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction. (arXiv:2309.03386v1 [cs.LG])

    [http://arxiv.org/abs/2309.03386](http://arxiv.org/abs/2309.03386)

    本论文介绍了一种基于社区的分级正类-未标记（PU）模型融合方法，用于慢性病预测。该方法考虑了不同人群之间的差异，并通过构建PU模型树和聚合其输出来进行预测。

    

    正类-未标记（PU）学习是二分类问题的一个挑战，在这种问题中存在大量未标记数据和少量正类数据实例，可以用于慢性病筛查问题。最先进的PU学习方法已经导致了各种风险估计器的发展，然而它们忽视了不同人群之间的差异。为了解决这个问题，我们提出了一种新颖的正类-未标记学习树（PUtree）算法。PUtree旨在考虑社区，如不同的年龄或收入段，在慢性病预测任务中。我们提出了一种新的二分类决策方法，它以层次方式构建基于社区的PU模型，然后聚合它们的输出。我们的方法可以解释树上每个PU模型，以优化非叶节点的分裂。此外，一种掩码恢复数据增强策略使得模型在个体上能进行充分训练。

    Positive-Unlabeled (PU) Learning is a challenge presented by binary classification problems where there is an abundance of unlabeled data along with a small number of positive data instances, which can be used to address chronic disease screening problem. State-of-the-art PU learning methods have resulted in the development of various risk estimators, yet they neglect the differences among distinct populations. To address this issue, we present a novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed to take into account communities such as different age or income brackets, in tasks of chronic disease prediction. We propose a novel approach for binary decision-making, which hierarchically builds community-based PU models and then aggregates their deliverables. Our method can explicate each PU model on the tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery data augmentation strategy enables sufficient training of the model in individua
    
[^46]: 低资源下游任务的自监督遮罩数字高程模型编码

    Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks. (arXiv:2309.03367v1 [cs.CV])

    [http://arxiv.org/abs/2309.03367](http://arxiv.org/abs/2309.03367)

    本研究旨在提供一种针对低资源下游任务的自监督遮罩数字高程模型编码。利用自监督学习，该模型能够从大量未标记和非结构化数据中学习，并将该知识应用于土地地形的建筑物和道路分割任务。

    

    缺乏高质量标记数据是训练深度学习模型的主要瓶颈之一。随着任务复杂性的增加，过拟合和不稳定学习的惩罚也越高。今天通常采用的范式是自监督学习，模型试图从大量非结构化和无标签的数据中学习，然后将该知识转移到所需的任务中。在其他模态中，一些著名的自监督示例包括用于大型语言模型的BERT，用于语音识别的Wav2Vec以及用于视觉的遮罩自动编码器，所有这些都利用变压器来解决遮罩预测任务。由于几十年的数据采集，并没有精确可靠地注释数据，地理AI独特地具备利用自监督方法的条件。我们的目标是从提供地球表面详细地形的数字高程模型（DEM）中提取建筑物和道路分割。所提出的架构是Ma

    The lack of quality labeled data is one of the main bottlenecks for training Deep Learning models. As the task increases in complexity, there is a higher penalty for overfitting and unstable learning. The typical paradigm employed today is Self-Supervised learning, where the model attempts to learn from a large corpus of unstructured and unlabeled data and then transfer that knowledge to the required task. Some notable examples of self-supervision in other modalities are BERT for Large Language Models, Wav2Vec for Speech Recognition, and the Masked AutoEncoder for Vision, which all utilize Transformers to solve a masked prediction task. GeoAI is uniquely poised to take advantage of the self-supervised methodology due to the decades of data collected, little of which is precisely and dependably annotated. Our goal is to extract building and road segmentations from Digital Elevation Models (DEM) that provide a detailed topography of the earths surface. The proposed architecture is the Ma
    
[^47]: REBOOT: 重用数据以引导高效的现实世界灵巧操纵

    REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation. (arXiv:2309.03322v1 [cs.LG])

    [http://arxiv.org/abs/2309.03322](http://arxiv.org/abs/2309.03322)

    本论文介绍了一种使用强化学习学习灵巧操纵技能的高效系统，通过结合样本高效强化学习和回放缓冲区引导，实现了重用数据以降低真实世界应用中的挑战。

    

    对于涉及接触密集交互的灵巧操纵任务，模型驱动的控制系统和模仿学习算法都面临着巨大的挑战。复杂性来自于多指机器人手需要动态建立和断开接触、平衡非伸手持力并控制大量自由度。强化学习（RL）由于其广泛适用性和自主获取最佳操纵策略的能力而具有很大的潜力。然而，它在真实世界的应用常常受到生成大量样本、重置环境和获取奖励信号的限制。在这项工作中，我们引入了一种用于学习具有RL的灵巧操纵技能的高效系统，以解决这些挑战。我们方法的主要思想是将最近在样本高效RL和回放缓冲区引导方面的进展相结合。这种组合使我们能够利用来自不同任务或物体的数据作为输入。

    Dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. Reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a
    
[^48]: 通过机器学习进行适应度近似

    Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])

    [http://arxiv.org/abs/2309.03318](http://arxiv.org/abs/2309.03318)

    我们提出了一种使用机器学习模型在遗传算法中进行适应度近似的方法。实验结果表明，这种方法显著提高了进化运行时间，并且适应度得分要么与完全运行的遗传算法相同，要么稍微低一点。

    

    我们提出了一种新颖的方法，使用机器学习模型在遗传算法中进行适应度近似，重点是在Gymnasium（游戏）模拟器中的进化代理上 - 在这里适应度计算是昂贵的。我们维护一个采样个体及其实际适应度得分的数据集，并在整个进化过程中不断更新一个适应度近似的机器学习模型。我们比较了不同的方法：1）在实际适应度和近似适应度之间切换，2）对种群进行采样，以及3）加权采样样本。实验结果表明，在适应度计算的近似比例取决于完全运行GA时，我们的方法显著提高了进化运行时间，并且适应度得分要么与完全运行的GA相同，要么稍微低一点。我们的方法是通用的，可以很容易地应用于许多不同的领域。

    We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
    
[^49]: 深度伪造算法的比较分析

    Comparative Analysis of Deep-Fake Algorithms. (arXiv:2309.03295v1 [cs.CV])

    [http://arxiv.org/abs/2309.03295](http://arxiv.org/abs/2309.03295)

    这项研究比较了不同的深度伪造算法，探讨了其对数字视觉媒体完整性的威胁，以及深伪造检测技术的应用方法。

    

    由于智能手机普及，并拥有高质量的数码相机以及易于使用的软件应用程序来录制，编辑和共享视频和图像，再加上深度学习的人工智能平台，出现了一种新的视频“伪造”现象。深度伪造算法可以创建几乎无法与真实图像区分的伪造图像和视频。因此，检测和评估数字视觉媒体完整性的技术非常重要。深伪造，也被称为基于深度学习的伪造视频，由于其可以以几乎无法与原始图像和视频区分的方式操控和修改图像和视频，近年来已成为一个主要关注点。这些深伪造视频可用于恶意用途，例如传播虚假信息，冒充个人身份和制造假新闻。深伪造检测技术使用各种方法，例如面部识别，动作分析和音视频同步来识别

    Due to the widespread use of smartphones with high-quality digital cameras and easy access to a wide range of software apps for recording, editing, and sharing videos and images, as well as the deep learning AI platforms, a new phenomenon of 'faking' videos has emerged. Deepfake algorithms can create fake images and videos that are virtually indistinguishable from authentic ones. Therefore, technologies that can detect and assess the integrity of digital visual media are crucial. Deepfakes, also known as deep learning-based fake videos, have become a major concern in recent years due to their ability to manipulate and alter images and videos in a way that is virtually indistinguishable from the original. These deepfake videos can be used for malicious purposes such as spreading misinformation, impersonating individuals, and creating fake news. Deepfake detection technologies use various approaches such as facial recognition, motion analysis, and audio-visual synchronization to identify
    
[^50]: 临时归纳路径神经网络用于时间知识图推理

    Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])

    [http://arxiv.org/abs/2309.03251](http://arxiv.org/abs/2309.03251)

    本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。

    

    时间知识图（TKG）是传统知识图（KG）的扩展，融入了时间维度。在TKGs上进行推理是一个关键任务，旨在基于历史事件预测未来事实。关键挑战在于揭示历史子图和时间模式中的结构依赖关系。大多数现有方法依靠实体建模来模拟TKGs，因为图中的节点在知识表示中起着至关重要的作用。然而，现实场景通常涉及大量实体，并且随着时间的推移会出现新实体。这使得依赖于实体的方法很难应对大量实体，并且有效处理新出现的实体也成为一个重要的挑战。因此，我们提出了一种临时归纳路径神经网络（TiPNN），它以实体独立的角度对历史信息进行建模。具体而言，TiPNN采用了一个统一的图，名为历史时间图，来建模历史信息，并通过临时归纳路径提取结构和时间信息。

    Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
    
[^51]: 通过AutoBA实现自动化生物信息学分析

    Automated Bioinformatics Analysis via AutoBA. (arXiv:2309.03242v1 [q-bio.GN])

    [http://arxiv.org/abs/2309.03242](http://arxiv.org/abs/2309.03242)

    AutoBA是一个基于大型语言模型的自主AI代理程序，通过最少的用户输入简化生物信息学分析过程，并提供详细的逐步计划。经过验证，AutoBA在各种组学分析案例中表现出健壮性和适应性，同时保护数据隐私。

    

    随着组学数据的快速增长和演变，对处理分析的简化和适应性工具的需求不断增长。为满足这一需求，我们引入了Auto Bioinformatics Analysis (AutoBA)，这是一个基于大型语言模型的自主AI代理程序，专门设计用于传统组学数据分析。AutoBA通过需要最少的用户输入来简化分析过程，并提供详细的逐步计划，用于完成各种生物信息学任务。经过专家生物信息学家的严格验证，AutoBA的健壮性和适应性在各种组学分析案例中得到证实，包括全基因组测序（WGS），RNA测序（RNA-seq），单细胞RNA测序，ChIP-seq和空间转录组学。AutoBA的独特能力是根据输入数据的变化自设计分析流程，进一步凸显了其多功能性。与在线生物信息学服务相比，AutoBA在本地部署分析，保护数据隐私。

    With the fast-growing and evolving omics data, the demand for streamlined and adaptable tools to handle the analysis continues to grow. In response to this need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI agent based on a large language model designed explicitly for conventional omics data analysis. AutoBA simplifies the analytical process by requiring minimal user input while delivering detailed step-by-step plans for various bioinformatics tasks. Through rigorous validation by expert bioinformaticians, AutoBA's robustness and adaptability are affirmed across a diverse range of omics analysis cases, including whole genome sequencing (WGS), RNA sequencing (RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's unique capacity to self-design analysis processes based on input data variations further underscores its versatility. Compared with online bioinformatic services, AutoBA deploys the analysis locally, preserving data privacy. Moreo
    
[^52]: GPT可以在没有计算器的情况下解决数学问题

    GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])

    [http://arxiv.org/abs/2309.03241](http://arxiv.org/abs/2309.03241)

    本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。

    

    以往的研究通常认为大型语言模型无法在没有计算器工具的情况下准确执行算术运算，特别是超过8位数字的乘法，以及涉及小数和分数的运算。本文旨在挑战这种误解。通过充分的训练数据，一个拥有20亿参数的语言模型可以以近乎100%的准确度执行多位数的算术运算，而且没有数据泄露，显著超过了GPT-4（其多位数乘法准确率仅为4.3%）。我们还演示了我们的MathGLM，它是通过在包含了文本描述的附加多步骤算术运算和数学问题的数据集上从GLM-10B微调而成的，它在一个包含5000个样本的中文数学问题测试集上的表现与GPT-4相似。

    Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
    
[^53]: POI级别人群流推断的时空对比自监督学习

    Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])

    [http://arxiv.org/abs/2309.03239](http://arxiv.org/abs/2309.03239)

    本文提出了一种针对POI级别人群流推断的时空对比自监督学习模型，通过自监督属性图表示学习以解决数据标记不足、POI间时空依赖性复杂和人群流量与GPS报告之间相关性多样等挑战。

    

    准确获取兴趣点（POI）的人群流量对于有效的交通管理、公共服务和城市规划至关重要。尽管如此重要，但由于城市感知技术的限制，大多数数据源的数据质量不足以监测每个POI的人群流动。这使得从低质量数据中推断准确的人群流量成为一项关键且具有挑战性的任务。这一复杂性主要由三个关键因素引起：1）标记数据的稀缺性和罕见性；2）POI之间复杂的时空依赖关系；3）精确人群流量与GPS报告之间的众多相关性。为了应对这些挑战，我们将人群流推断问题重新构建为自监督属性图表示学习任务，并引入一种新的时空数据对比自监督学习框架（model）。我们的方法从构建一个空间图开始。

    Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spati
    
[^54]: 自然示例为基础的可解释性：一项调查

    Natural Example-Based Explainability: a Survey. (arXiv:2309.03234v1 [cs.AI])

    [http://arxiv.org/abs/2309.03234](http://arxiv.org/abs/2309.03234)

    本文概述了自然示例为基础的可解释性人工智能的最新进展，这些方法通过使用示例作为解释来提高机器学习模型的可解释性，与人类的学习和推理过程相符，使解释更自然和易懂。

    

    可解释的人工智能（XAI）在提高机器学习模型的可解释性和可信度方面变得越来越重要。虽然在XAI领域，突出图已经成为主角多年，但其反映模型内部过程的能力受到了质疑。尽管不太受关注，但基于示例的XAI方法仍在不断改进。它包括使用示例作为机器学习模型预测的解释的方法。这符合人类推理的心理机制，使基于示例的解释对用户来说自然和直观易懂。事实上，人类通过基于示例形成概念的心理表示来学习和推理。本文概述了自然示例为基础的XAI的最新进展，并描述了每种方法的优缺点。所谓“自然”示例指的是直接从训练数据中绘制而来，而不涉及任何生成过程。

    Explainable Artificial Intelligence (XAI) has become increasingly significant for improving the interpretability and trustworthiness of machine learning models. While saliency maps have stolen the show for the last few years in the XAI field, their ability to reflect models' internal processes has been questioned. Although less in the spotlight, example-based XAI methods have continued to improve. It encompasses methods that use examples as explanations for a machine learning model's predictions. This aligns with the psychological mechanisms of human reasoning and makes example-based explanations natural and intuitive for users to understand. Indeed, humans learn and reason by forming mental representations of concepts based on examples.  This paper provides an overview of the state-of-the-art in natural example-based XAI, describing the pros and cons of each approach. A "natural" example simply means that it is directly drawn from the training data without involving any generative pro
    
[^55]: 量子AI增强智能监控：通过创新的违禁品检测提升公共安全

    Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection. (arXiv:2309.03231v1 [quant-ph])

    [http://arxiv.org/abs/2309.03231](http://arxiv.org/abs/2309.03231)

    本研究将量子人工智能与监控领域的RentinaNet模型进行整合，开发了称为Quantum-RetinaNet的智能监控系统，该系统在准确性和速度方面取得了突破性进展。

    

    监控系统在维护现代社会的和平与安全中起到了关键作用。它们的普及性有助于有效监控可疑活动。然而，在人口密集的环境中，持续主动监控变得不切实际，必须开发智能监控系统。AI在监控领域的整合是一次重大革命，然而速度问题阻碍了其在该领域的广泛实施。研究发现，量子人工智能取得了重大突破。基于量子人工智能的监控系统不仅更准确，而且能够在实时场景中表现出色，这是以前从未见过的。本研究将RentinaNet模型与量子CNN集成，称为Quantum-RetinaNet。通过利用量子卷积神经网络的量子能力，Quantum-RetinaNet在准确性和速度之间取得了平衡。

    Surveillance systems have emerged as crucial elements in upholding peace and security in the modern world. Their ubiquity aids in monitoring suspicious activities effectively. However, in densely populated environments, continuous active monitoring becomes impractical, necessitating the development of intelligent surveillance systems. AI integration in the surveillance domain was a big revolution, however, speed issues have prevented its widespread implementation in the field. It has been observed that quantum artificial intelligence has led to a great breakthrough. Quantum artificial intelligence-based surveillance systems have shown to be more accurate as well as capable of performing well in real-time scenarios, which had never been seen before. In this research, a RentinaNet model is integrated with Quantum CNN and termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN, Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative integration 
    
[^56]: 如何选择体育赛程安排中的算法？

    Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])

    [http://arxiv.org/abs/2309.03229](http://arxiv.org/abs/2309.03229)

    本研究通过实例空间分析体育赛程安排问题的特征，提出了一个基于机器学习的算法选择系统，预测了在给定特征下最适合的算法，同时深入了解了算法性能并提出了改进建议。

    

    任何体育竞赛都需要一个赛程安排，确定比赛队伍何时何地相遇。最近的国际赛程安排竞赛(ITC2021)揭示了一个事实，即虽然可能开发出通用算法，但每个算法在问题实例上的性能差异很大。本文在体育赛程安排方面提供了实例空间分析，从而深入了解了八种最先进算法的优势和劣势。基于机器学习技术，我们提出了一个算法选择系统，可以根据体育赛程安排问题实例的特征预测哪种算法在性能上可能表现最佳。此外，我们还确定了哪些特征在做出预测时很重要，从而深入了解了算法的性能，并提出了进一步改进的建议。最后，我们评估了这些实例的经验难度。我们的结果基于大规模计算实验。

    Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational exper
    
[^57]: 学习基于专利的生物医学知识图谱揭示药物再定位候选物的技术潜力

    Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])

    [http://arxiv.org/abs/2309.03227](http://arxiv.org/abs/2309.03227)

    本研究提出了一种使用药物专利和生物医学数据库相结合的方法，识别具有技术潜力和科学证据的药物再定位候选物。通过构建科学的生物医学知识图谱和基于专利的生物医学知识图谱，我们可以综合分析多种信息源，为药物再定位研究提供新的视角。

    

    药物再定位是一种发现现有药物新治疗用途的有前途的策略，近年来在计算科学文献中使用生物医学数据库进行了广泛探索。然而，药物再定位候选物的技术潜力经常被忽视。本研究提出了一种新的方法，综合分析药物专利和生物医学数据库等多种信息源，识别具有技术潜力和科学证据的药物再定位候选物。首先，我们构建了一个科学的生物医学知识图谱（s-BKG），包括来自生物医学数据库的药物、疾病和基因之间的关系。我们的方法涉及识别在s-BKG中与目标疾病关联有限但在空间上紧密相邻的药物作为潜在的药物候选物。然后，我们通过添加药物专利信息构建了一个基于专利的生物医学知识图谱（p-BKG）。

    Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
    
[^58]: 使用排名来摊销实际程序合成的成本的研究

    Amortizing Pragmatic Program Synthesis with Rankings. (arXiv:2309.03225v1 [cs.PL])

    [http://arxiv.org/abs/2309.03225](http://arxiv.org/abs/2309.03225)

    该研究提出了一种使用全局实际排名方法来分摊实际程序合成中计算负担的方法，并证明了该方法适用于使用单个演示的实际合成器，并且通过实证研究展示了全局排名有效近似了全体合理响应。

    

    在程序合成中，一个智能系统接收一组用户生成的示例，并返回一个逻辑一致的程序。使用合理演说行为（RSA）框架在构建“实际”程序合成器方面取得了成功，这些合成器返回的程序不仅在逻辑上一致，而且还考虑了用户如何选择示例。然而，运行RSA算法的计算负担限制了实际程序合成在可能程序个数较少的领域的应用。本研究提出了一种通过利用“全局实际排名” - 一个单一的、总的排列所有假设的方法来分摊RSA算法的计算负担。我们证明，在使用单个演示的实际合成器中，我们的全局排名方法完全复制了RSA的排序响应。我们进一步通过实证研究显示，全局排名有效地近似了全体合理响应。

    In program synthesis, an intelligent system takes in a set of user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \emph{pragmatic} program synthesizers that return programs which -in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA's ranked responses. We further empirically show that global rankings effectively approximate the full pragma
    
[^59]: 无需训练依然能获益：通过蒙特卡洛树搜索和能量函数引导实现大型语言模型的数学推理

    No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])

    [http://arxiv.org/abs/2309.03224](http://arxiv.org/abs/2309.03224)

    该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。

    

    大型语言模型（LLMs）展现出令人印象深刻的语言理解和背景学习能力，包括自然语言处理（NLP）任务和具有挑战性的数学推理。然而，由于缺乏过程监督，将PLMs应用于数学推理任务通常无法生成正确的推理步骤和最终答案，即使解决方案概率很高。为了在没有进一步的微调步骤的情况下发挥微调的LLMs的数学推理能力，我们提出了一种方法，通过蒙特卡洛树搜索（MCTS）和轻量级能量函数为LLMs赋予即时反应和精细推理系统。具体而言，我们首先将微调的LLMs重新定义为基于残差的能量模型（Residual-EBM），并应用噪声对比估计来估计能量函数的参数。然后，我们使用带有能量函数的MCTS作为路径验证器来搜索输出空间并评估推理路径。通过广泛的实验证明了我们方法的有效性。

    Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
    
[^60]: Sherlock Holmes不玩骰子：Evidence Theory对社会与生命科学的意义

    Sherlock Holmes Doesn't Play Dice: The significance of Evidence Theory for the Social and Life Sciences. (arXiv:2309.03222v1 [cs.AI])

    [http://arxiv.org/abs/2309.03222](http://arxiv.org/abs/2309.03222)

    本文强调了Evidence Theory在社会与生命科学中的潜力，可以表达来自未确定事件可能实现的不确定性，而与之对比的Probability Theory只能限于决策者当前设想的可能性。本文还讨论了Evidence Theory与Probability Theory的关系以及Evidence Theory在信息论中的应用增强效果，并通过审计练习案例进一步说明了Evidence Theory的应用。

    

    虽然Evidence Theory（Demster-Shafer Theory，Belief Functions Theory）在数据融合中得到越来越多的应用，但其在社会与生命科学中的潜力常常被人们对其独特特点的缺乏认识所掩盖。本文强调，Evidence Theory可以表达来自于人们未能确定的事件可能实现的不确定性，而Probability Theory必须仅限于决策者当前所设想的可能性。随后，我们说明了Dempster-Shafer的组合规则如何与各种版本的Probability Theory的贝叶斯定理相关，并讨论了哪些信息论的应用可以通过Evidence Theory得到增强。最后，我们通过一个案例阐述了使用Evidence Theory来理解审计练习中出现的部分重叠、部分相互矛盾的解决方案的情况。

    While Evidence Theory (Demster-Shafer Theory, Belief Functions Theory) is being increasingly used in data fusion, its potentialities in the Social and Life Sciences are often obscured by lack of awareness of its distinctive features. With this paper we stress that Evidence Theory can express the uncertainty deriving from the fear that events may materialize, that one has not been able to figure out. By contrast, Probability Theory must limit itself to the possibilities that a decision-maker is currently envisaging.  Subsequently, we illustrate how Dempster-Shafer's combination rule relates to Bayes' Theorem for various versions of Probability Theory and discuss which applications of Information Theory can be enhanced by Evidence Theory. Finally, we illustrate our claims with an example where Evidence Theory is used to make sense of the partially overlapping, partially contradictory solutions that appear in an auditing exercise.
    
[^61]: 基于文本感知的医学知识图谱表示学习的伴侣动物疾病诊断

    Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])

    [http://arxiv.org/abs/2309.03219](http://arxiv.org/abs/2309.03219)

    这项研究提出了一种基于文本感知的医学知识图谱表示学习方法，以提高伴侣动物疾病诊断的效率。通过融合各种类型的文本信息和图结构，该方法能够捕捉到重要的实体和关系。

    

    知识图谱嵌入被用于通过分析电子医疗记录（如笔记和兽医记录）来受益于动物疾病的诊断。然而，学习用于捕捉知识图谱中带有文本信息的实体和关系的表示是具有挑战性的，因为知识图谱显示出异构特性和各种类型的文本信息。同时，现有的方法大多旨在保留围绕目标节点的图结构，而无法考虑不同类型的文本，而这些文本也可能包含重要的信息。本文提出了一种用于有效诊断动物疾病的知识图谱嵌入模型，该模型可以学习各种类型的文本信息和图结构，并将它们融合为统一的表示，即LiteralKG。具体而言，我们构建了一个知识图谱，该图谱是从各个动物医院收集的电子医疗记录及文本信息构建而来。我们然后融合不同类型的实体和关系，以及文本信息。

    Knowledge graph (KG) embedding has been used to benefit the diagnosis of animal diseases by analyzing electronic medical records (EMRs), such as notes and veterinary records. However, learning representations to capture entities and relations with literal information in KGs is challenging as the KGs show heterogeneous properties and various types of literal information. Meanwhile, the existing methods mostly aim to preserve graph structures surrounding target nodes without considering different types of literals, which could also carry significant information. In this paper, we propose a knowledge graph embedding model for the efficient diagnosis of animal diseases, which could learn various types of literal information and graph structure and fuse them into unified representations, namely LiteralKG. Specifically, we construct a knowledge graph that is built from EMRs along with literal information collected from various animal hospitals. We then fuse different types of entities and no
    
[^62]: 泛粗糙集中的合格聚合的代数模型及推理偏差发现

    Algebraic Models for Qualified Aggregation in General Rough Sets, and Reasoning Bias Discovery. (arXiv:2309.03217v1 [cs.AI])

    [http://arxiv.org/abs/2309.03217](http://arxiv.org/abs/2309.03217)

    本研究提出了一种泛粗糙集中合格聚合的代数模型，用于模拟人类推理中的悲观和乐观的合并，以及研究推理中的歧视/有害行为和机器学习算法。

    

    在泛粗糙集的背景下，将两个事物组合成另一个并非直接。对于涉及不确定性和模糊性的其他理论也是如此。这种行为可以赋予额外的意义，超越了结构上的合取和析取，就像$L$模糊集上的$*$-范数理论和相关推导一样。在本研究中，我们发明了在具有近似算子（称为粗糙便利格）的格上将事物组合的代数模型。研究受到要建模怀疑论或悲观论和乐观论合并于人类推理，以及操作选择被观点所约束的动机的强烈推动。证明了最小模型提供的弱否定和推导的基本结果。此外，该模型适用于研究人类推理中的歧视/有害行为和机器学习算法。

    In the context of general rough sets, the act of combining two things to form another is not straightforward. The situation is similar for other theories that concern uncertainty and vagueness. Such acts can be endowed with additional meaning that go beyond structural conjunction and disjunction as in the theory of $*$-norms and associated implications over $L$-fuzzy sets. In the present research, algebraic models of acts of combining things in generalized rough sets over lattices with approximation operators (called rough convenience lattices) is invented. The investigation is strongly motivated by the desire to model skeptical or pessimistic, and optimistic or possibilistic aggregation in human reasoning, and the choice of operations is constrained by the perspective. Fundamental results on the weak negations and implications afforded by the minimal models are proved. In addition, the model is suitable for the study of discriminatory/toxic behavior in human reasoning, and of ML algor
    
[^63]: 可解释和值得信赖的交通标志检测方法：基于归纳逻辑编程的方法

    Explainable and Trustworthy Traffic Sign Detection for Safe Autonomous Driving: An Inductive Logic Programming Approach. (arXiv:2309.03215v1 [cs.AI])

    [http://arxiv.org/abs/2309.03215](http://arxiv.org/abs/2309.03215)

    本论文提出了一种基于归纳逻辑编程的可解释和值得信赖的交通标志检测方法，使用高级特征来鲁棒地检测交通标志，从而解决了当前基于深度神经网络的方法容易受到对抗性攻击的问题。

    

    交通标志检测是自动驾驶车辆操作中的关键任务，它确保了所有道路使用者的安全。当前基于深度神经网络的标志分类系统依赖像素级特征来检测交通标志，容易受到对抗性攻击的影响。这些攻击包括对标志的微小、难以察觉的改动，会导致传统分类器错误地识别标志。为解决这个问题，我们提出了一种基于归纳逻辑编程的方法来检测自动驾驶车辆中的停车标志。该方法利用标志的高级特征，如形状、颜色和文本，来检测交通标志的类别。这种方法对抗对抗性攻击更加稳健，因为它模拟人类的感知方式，不容易受到当前深度神经网络分类器的限制影响。我们考虑了两种对抗攻击方法来评估我们的方法：鲁棒物理扰动（PR2）和对抗伪装（AdvCam）。

    Traffic sign detection is a critical task in the operation of Autonomous Vehicles (AV), as it ensures the safety of all road users. Current DNN-based sign classification systems rely on pixel-level features to detect traffic signs and can be susceptible to adversarial attacks. These attacks involve small, imperceptible changes to a sign that can cause traditional classifiers to misidentify the sign. We propose an Inductive Logic Programming (ILP) based approach for stop sign detection in AVs to address this issue. This method utilises high-level features of a sign, such as its shape, colour, and text, to detect categories of traffic signs. This approach is more robust against adversarial attacks, as it mimics human-like perception and is less susceptible to the limitations of current DNN classifiers. We consider two adversarial attacking methods to evaluate our approach: Robust Physical Perturbation (PR2) and Adversarial Camouflage (AdvCam). These attacks are able to deceive DNN classi
    
[^64]: 提高训练人工智能团队的现状：技术报告＃3-测试环境替代方案的分析

    Improving the State of the Art for Training Human-AI Teams: Technical Report #3 -- Analysis of Testbed Alternatives. (arXiv:2309.03213v1 [cs.HC])

    [http://arxiv.org/abs/2309.03213](http://arxiv.org/abs/2309.03213)

    Sonalysts正在研究开发一种合成任务环境（STE），通过调查现有的人工智能团队合作测试环境，他们总结出了测试环境评估标准和潜在的测试环境。

    

    Sonalysts正在通过在人工智能（AI）团队领域开展原始研究，以扩展我们当前在团队合作方面的专业知识。为了为这项研究打下基础，Sonalysts正在研究开发一种合成任务环境（STE）。在之前的报告中，我们记录了最近的外展努力的结果，在这次努力中，我们询问了军事专家和其他在人工智能团队合作领域的研究人员，他们在测试环境中最看重的特点。 令人惊讶的是，一些受访者建议我们的团队研究现有的人工智能团队合作测试环境，而不是创建新的环境。基于这一建议，我们进行了系统的调查。在本报告中，我们描述了该调查的结果。根据调查结果，我们制定了测试环境评估标准，确定了潜在的测试环境，并进行了评估。

    Sonalysts is working on an initiative to expand our current expertise in teaming to Human-Artificial Intelligence (AI) teams by developing original research in this area. To provide a foundation for that research, Sonalysts is investigating the development of a Synthetic Task Environment (STE). In a previous report, we documented the findings of a recent outreach effort in which we asked military Subject Matter Experts (SMEs) and other researchers in the Human-AI teaming domain to identify the qualities that they most valued in a testbed. A surprising finding from that outreach was that several respondents recommended that our team look into existing human-AI teaming testbeds, rather than creating something new. Based on that recommendation, we conducted a systematic investigation of the associated landscape. In this report, we describe the results of that investigation. Building on the survey results, we developed testbed evaluation criteria, identified potential testbeds, and conduct
    
[^65]: 提高人工智能与人类团队培训的现有技术水平：研究人员知识引导调查的结果 技术报告#2

    Improving the State of the Art for Training Human-AI Teams: Technical Report #2 -- Results of Researcher Knowledge Elicitation Survey. (arXiv:2309.03212v1 [cs.HC])

    [http://arxiv.org/abs/2309.03212](http://arxiv.org/abs/2309.03212)

    这项技术报告介绍了一项关于改进人工智能与人类团队培训的研究，旨在开发一种合成任务环境，以支持利益相关研究人员在该领域的广泛研究。

    

    空军研究实验室 (AFRL) 由国家科学、工程和数学学院为其制作了一份共识报告，其中记录了对支持军事服务领域中人工智能团队的普遍和不断增长的愿望。Sonalysts公司已经开始了一项内部计划，旨在探索人工智能团队的培训。这一努力的第一步是开发一种能够促进人工智能团队研究的合成任务环境 (STE)。我们的目标是创建一个能够支持利益相关研究人员计划在该领域开展广泛研究的STE。因此，我们希望广泛采样相关研究社区的优先事项，而这份报告记录的努力是我们的首次尝试。我们创建了一个调查表，其中包括两种类型的问题。第一部分要求受访者就我们认为可能很重要的STE特征表达意见。第二部分则

    A consensus report produced for the Air Force Research Laboratory (AFRL) by the National Academies of Sciences, Engineering, and Mathematics documented a prevalent and increasing desire to support human-Artificial Intelligence (AI) teaming across military service branches. Sonalysts has begun an internal initiative to explore the training of Human-AI teams. The first step in this effort is to develop a Synthetic Task Environment (STE) that is capable of facilitating research on Human-AI teams. Our goal is to create a STE that offers a task environment that could support the breadth of research that stakeholders plan to perform within this domain. As a result, we wanted to sample the priorities of the relevant research community broadly, and the effort documented in this report is our initial attempt to do so. We created a survey that featured two types of questions. The first asked respondents to report their agreement with STE features that we anticipated might be important. The secon
    
[^66]: 提高针对人工智能团队培训的现状：技术报告＃1-科学家涉及调查的结果

    Improving the State of the Art for Training Human-AI Teams: Technical Report #1 -- Results of Subject-Matter Expert Knowledge Elicitation Survey. (arXiv:2309.03211v1 [cs.HC])

    [http://arxiv.org/abs/2309.03211](http://arxiv.org/abs/2309.03211)

    这项研究旨在提高人工智能与人类团队合作的培训水平，并通过开发合成任务环境来解决联合全领域指挥与控制中的团队合作挑战。

    

    国家科学工程学院为空军研究实验室制定了一份一致性报告，记录了军事各个服务部门支持人工智能与人类团队合作的普遍愿望。Sonalysts已经开始了一个内部计划，探索人工智能团队的培训。在这一努力的第一步是开发一个能够促进人工智能团队研究的合成任务环境（STE）。我们决定以联合全领域指挥与控制（JADC2）为焦点来开发STE，因为JADC2概念内的传感器输入和决策选择体量很可能需要使用人工智能系统以实现及时决策。基于这一焦点，我们与具有指挥与控制经验的多位学科专家（SMEs）进行了接触，以了解开发一个体现与JADC2相关的团队合作挑战的STE的见解。本报告记录了我们与这些SMEs的初步接触。

    A consensus report produced for the Air Force Research Laboratory by the National Academies of Sciences, Engineering, and Mathematics documented a prevalent and increasing desire to support human-Artificial Intelligence (AI) teaming across military service branches. Sonalysts has begun an internal initiative to explore the training of human-AI teams. The first step in this effort is to develop a Synthetic Task Environment (STE) that is capable of facilitating research on human-AI teams. We decided to use Joint All-Domain Command and Control (JADC2) as a focus point for developing the STE because the volume of sensor inputs and decision options within the JADC2 concept likely requires the use of AI systems to enable timely decisions. Given this focus, we engaged a number of Subject-Matter Experts (SMEs) with Command and Control experience to gain insight into developing a STE that embodied the teaming challenges associated with JADC2. This report documents our initial engagement with th
    
[^67]: 一种人机联合学习框架以提高内源性BCI训练

    A Human-Machine Joint Learning Framework to Boost Endogenous BCI Training. (arXiv:2309.03209v1 [cs.HC])

    [http://arxiv.org/abs/2309.03209](http://arxiv.org/abs/2309.03209)

    本研究提出了一种人机联合学习框架，通过引导用户生成脑信号，以促进内源性BCI的学习过程。这种框架基于用户的历史脑信号，将其引导到解码器估计的最优分布。

    

    脑-计算机接口（BCIs）为大脑与外部设备之间提供了直接通道，并展示了对辅助和康复技术的巨大潜力。基于脑电图（EEG）信号的内源性BCIs，如运动想象（MI）BCIs，可以提供一定程度的控制。然而，掌握自发BCI控制需要用户通过想象生成明显且稳定的脑信号模式，这是具有挑战性的，并且通常需要很长时间的训练（几周/几个月）。在这里，我们提出了一种人机联合学习框架，通过引导用户将脑信号生成为解码器估计的最优分布，以促进内源性BCIs的学习过程。为此，我们首先以统一的形式建模人机联合学习过程。然后提出了一种人机联合学习框架：1）对于人类方面，我们模拟学习过程

    Brain-computer interfaces (BCIs) provide a direct pathway from the brain to external devices and have demonstrated great potential for assistive and rehabilitation technologies. Endogenous BCIs based on electroencephalogram (EEG) signals, such as motor imagery (MI) BCIs, can provide some level of control. However, mastering spontaneous BCI control requires the users to generate discriminative and stable brain signal patterns by imagery, which is challenging and is usually achieved over a very long training time (weeks/months). Here, we propose a human-machine joint learning framework to boost the learning process in endogenous BCIs, by guiding the user to generate brain signals towards an optimal distribution estimated by the decoder, given the historical brain signals of the user. To this end, we firstly model the human-machine joint learning process in a uniform formulation. Then a human-machine joint learning framework is proposed: 1) for the human side, we model the learning proces
    
[^68]: 一个用于芯片设计中高效逻辑综合的电路领域泛化框架

    A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design. (arXiv:2309.03208v1 [cs.AR])

    [http://arxiv.org/abs/2309.03208](http://arxiv.org/abs/2309.03208)

    该论文提出了一个用于芯片设计中高效逻辑综合的电路领域泛化框架，重点解决了逻辑综合中无效变换的问题，并通过PruneX操作符实现了超领域泛化。

    

    逻辑综合在芯片设计中起着重要作用，是半导体行业的基石。逻辑综合的关键任务是将由有向无环图（DAG）建模的电路转化为具有相同功能的简化电路。为了解决这个任务，许多逻辑综合操作符按顺序对输入DAG上每个节点根的子图应用变换。然而，我们发现大量的变换是无效的，导致应用这些操作符非常耗时。特别是，我们注意到Resub和Mfs2操作符的运行时间往往主导逻辑综合优化过程的整体运行时间。为了解决这个挑战，我们提出了一种新颖的数据驱动逻辑综合操作符范式，称为PruneX，用于减少无效的变换。PruneX的主要挑战是学习适用于未见过电路（即超出分布范围）的模型，也即超领域泛化问题。因此，PruneX的主要技术贡献是实现了超领域泛化。

    Logic Synthesis (LS) plays a vital role in chip design -- a cornerstone of the semiconductor industry. A key task in LS is to transform circuits -modeled by directed acyclic graphs (DAGs) -- into simplified circuits with equivalent functionalities. To tackle this task, many LS operators apply transformations to subgraphs -- rooted at each node on an input DAG -sequentially. However, we found that a large number of transformations are ineffective, which makes applying these operators highly time-consuming. In particular, we notice that the runtime of the Resub and Mfs2 operators often dominates the overall runtime of LS optimization processes. To address this challenge, we propose a novel data-driven LS operator paradigm, namely PruneX, to reduce ineffective transformations. The major challenge of developing PruneX is to learn models that well generalize to unseen circuits, i.e., the out-of-distribution (OOD) generalization problem. Thus, the major technical contribution of PruneX i
    
[^69]: Diffusion-EDFs: 基于SE(3)的等变去噪生成建模在视觉机器人操作中的应用

    Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])

    [http://arxiv.org/abs/2309.02685](http://arxiv.org/abs/2309.02685)

    本文提出了Diffusion-EDFs，一种在视觉机器人操作中应用的基于SE(3)的等变去噪生成建模方法。通过集成SE(3)等变性，我们的方法展示了出色的数据效率和泛化能力。

    

    最近的研究已经验证了等变方法可以显著提高机器人学习中的数据效率、泛化能力和鲁棒性。与此同时，去噪扩散生成建模最近作为一种具有随机行为的机器人操作学习的有前途的方法引起了极大关注。本文提出了Diffusion-EDFs，一种将空间旋转平移等变性即SE(3)等变性引入扩散生成建模的新方法。通过将SE(3)等变性集成到我们的模型架构中，我们展示了我们提出的方法具有明显的数据效率，在进行端到端训练时只需5到10个任务演示即可。此外，与之前基于扩散的操作方法相比，我们的方法展示了更好的泛化能力。

    Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
    
[^70]: 使用物理知识的神经网络计算更高维度中的最小曲面

    Using Physics-Informed Neural Networks to Calculate Minimal Surfaces in Higher Dimensions. (arXiv:2309.02589v1 [math.AP])

    [http://arxiv.org/abs/2309.02589](http://arxiv.org/abs/2309.02589)

    本文使用物理知识的神经网络（PINN）计算更高维度中的最小曲面的数值逼近，解决了维数诅咒问题，并且能够在没有GPU的笔记本电脑上进行快速训练。

    

    本文中，我们计算了更高维度中最小曲面的数值逼近，这是一种重要的偏微分方程类型。由于维数的诅咒导致传统方法无法处理这种情况，这些方法的计算成本会随维数增加而指数级增长，远远超出任何现代超级计算机的计算能力。只有在过去几年中，机器学习研究人员才能够缓解这个问题。本文选择的解决方法是一种称为物理知识的神经网络（Physics-Informed Neural Network，PINN）的模型，它训练了一个深度神经网络（Deep Neural Network，DNN）来解决最小曲面偏微分方程。它可以在更高维度上扩展，并且即使在没有GPU的笔记本电脑上也能相对快速地训练。由于无法查看高维度输出，我们的数据以足够的固定轴的片段形式呈现，以便通过3D图形进行查看。

    In this paper, we compute numerical approximations of the minimal surfaces, an essential type of Partial Differential Equation (PDE), in higher dimensions. Classical methods cannot handle it in this case because of the Curse of Dimensionality, where the computational cost of these methods increases exponentially fast in response to higher problem dimensions, far beyond the computing capacity of any modern supercomputers. Only in the past few years have machine learning researchers been able to mitigate this problem. The solution method chosen here is a model known as a Physics-Informed Neural Network (PINN) which trains a deep neural network (DNN) to solve the minimal surface PDE. It can be scaled up into higher dimensions and trained relatively quickly even on a laptop with no GPU. Due to the inability to view the high-dimension output, our data is presented as snippets of a higher-dimension shape with enough fixed axes so that it is viewable with 3-D graphs. Not only will the functio
    
[^71]: 自动化机器翻译的行为测试

    Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])

    [http://arxiv.org/abs/2309.02553](http://arxiv.org/abs/2309.02553)

    本文提出了一种利用大型语言模型自动生成源句子的方法，以测试机器翻译模型在多种情况下的行为。通过对多个机器翻译系统应用该方法，发现在测试结果与传统准确率度量存在差异的情况下，仍可观察到一致的趋势。

    

    NLP中的行为测试通过分析输入-输出行为来细粒度评估系统的语言能力。然而，目前关于机器翻译中行为测试的研究仅限于手工设计的测试范围有限、涵盖的语言种类也有限。为了解决这一限制，我们提出利用大型语言模型生成多样化的源句子，以测试机器翻译模型在不同情况下的行为。然后，我们可以使用相同的语言模型生成备选集，以验证机器翻译模型是否表现出预期的行为。我们的方法旨在使机器翻译系统的行为测试实际可行，同时只需要最少的人力投入。在实验中，我们将提出的评估框架应用于多个可用的机器翻译系统，结果显示，尽管总体上通过率与传统准确率度量可观察到的趋势相符，但仍存在差异。

    Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
    
[^72]: 智能作为意识的衡量方式

    Intelligence as a Measure of Consciousness. (arXiv:2309.00646v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.00646](http://arxiv.org/abs/2309.00646)

    评估人工系统是否具有意识的迹象是一个紧迫的问题，智力的心理测量可以间接地近似意识体验的程度，可以用来评估大型语言模型中的意识。

    

    评估人工系统是否具有意识的迹象越来越成为一个紧迫的问题，而一个严谨的心理测量框架在这方面对于评估大型语言模型可能非常重要。根据对人类和动物大脑的信息耦合、人类认知发展、新兴能力和心理表示发展与大型语言模型类似现象的比较，我认为智力的心理测量，如智商或智商指数，间接地近似了意识体验的程度。基于更广泛的科学和形而上学的意识理论，我认为所有系统都具有一定程度的可测量的意识，并且智力的心理测量可以用来衡量这种意识。

    Evaluating artificial systems for signs of consciousness is increasingly becoming a pressing concern, and a rigorous psychometric measurement framework may be of crucial importance in evaluating large language models in this regard. Most prominent theories of consciousness, both scientific and metaphysical, argue for different kinds of information coupling as a necessary component of human-like consciousness. By comparing information coupling in human and animal brains, human cognitive development, emergent abilities, and mental representation development to analogous phenomena in large language models, I argue that psychometric measures of intelligence, such as the g-factor or IQ, indirectly approximate the extent of conscious experience.  Based on a broader source of both scientific and metaphysical theories of consciousness, I argue that all systems possess a degree of consciousness ascertainable psychometrically and that psychometric measures of intelligence may be used to gauge re
    
[^73]: 基于合成临床记录的公开可共享的临床大语言模型

    Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])

    [http://arxiv.org/abs/2309.00237](http://arxiv.org/abs/2309.00237)

    使用合成临床记录构建的临床大语言模型可以克服临床记录的有限可及性和可用性的问题，并在现实应用中表现出潜在的良好性能。

    

    基于合成的临床案例报告，我们首先创建了大规模的合成临床记录，以解决临床记录的有限可及性和可用性的问题。然后，我们使用这些合成记录来训练我们的专门的临床大语言模型Asclepius。虽然Asclepius是在合成数据上训练的，但我们通过使用真实临床记录对其进行评估，以评估其在现实应用中的潜在性能。我们将Asclepius与包括GPT-3.5-turbo和其他开源替代方案在内的几种其他大语言模型进行了基准测试。为了进一步验证我们使用合成记录的方法，我们还将Asclepius与其在真实临床记录上训练的变体进行了比较。我们的发现有力地证明，合成临床记录在构建临床大语言模型时可以作为可行的替代品。

    The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
    
[^74]: Transformers作为支持向量机

    Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])

    [http://arxiv.org/abs/2308.16898](http://arxiv.org/abs/2308.16898)

    这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。

    

    自从"Attention Is All You Need"中引入转换器架构以来，它在自然语言处理领域取得了革命性的进展。转换器中的注意力层接受输入令牌序列$X$并通过计算softmax$(XQK^\top X^\top)$的成对相似性使它们相互作用，其中$(K,Q)$是可训练的键-查询参数。在这项工作中，我们建立了自注意力优化几何和一个硬间隔支持向量机问题之间的正式等价关系，通过对令牌对的外积施加线性约束，将最佳输入令牌与非最佳令牌分离。这个形式主义使我们能够表征梯度下降优化的单层转换器的隐式偏差：(1)优化注意力层，使用可变正则化参数$(K,Q)$，收敛的方向是一个最小化综合参数$W=KQ^\top$的核范数的支持向量机解决方案。而直接使用$W$进行参数化则最小化一个Frobenius范数目标。

    Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
    
[^75]: Ladder-of-Thought: 使用知识作为阶梯提升立场检测

    Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])

    [http://arxiv.org/abs/2308.16763](http://arxiv.org/abs/2308.16763)

    该论文介绍了一种名为“Ladder-of-Thought”的方法，通过引入外部知识来提升立场检测任务中的语言模型的性能，解决了小型模型在应用先前内部知识时性能提升不明显的问题，以及大规模模型在效率方面的挑战。

    

    思维链式提供（CoT）通过生成中间的推理来增强大型语言模型（LLM）的推理能力。然而，这些增强主要有益于大规模模型，在直接应用CoT时小型LLM的性能改进不明显。尽管LLM具有先进的推理能力，CoT主要依赖于其预先训练的内部知识，先前未知于模型的外部知识未被充分利用。在立场检测等任务中，外部背景知识起着关键作用，这种遗漏变得更加明显。此外，LLM的大规模架构在部署过程中不可避免地存在效率挑战。为了解决这些挑战，我们引入了用于立场检测的思维阶梯（LoT）。LoT基于双阶段级联优化框架，指导模型整合高质量的外部知识，增强中间步骤的性能。

    Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of Large Language Models (LLMs) through the generation of intermediate rationales. However, these enhancements predominantly benefit large-scale models, leaving small LMs without significant performance improvements when directly applying CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily on their pre-trained internal knowledge. The external knowledge that is previously unknown to the model remains unexploited. This omission becomes pronounced in tasks such as stance detection, where the external background knowledge plays a pivotal role. Additionally, the large-scale architecture of LLMs inevitably present efficiency challenges during deployment. To address these challenges, we introduce the Ladder-of-Thought (LoT) for stance detection. Grounded in a dual-phase Cascaded Optimization framework, LoT directs the model to incorporate high-quality external knowledge, enhancing the intermediat
    
[^76]: LM-Infinite: 大规模语言模型的简单即时长度推广

    LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])

    [http://arxiv.org/abs/2308.16137](http://arxiv.org/abs/2308.16137)

    LM-Infinite研究了大规模语言模型在长序列上的长度推广失败问题，并提出了一种简单的即时推广方法，以更高效地利用现有模型的生成能力。

    

    近年来，在Transformer-based大规模语言模型（LLM）在各个领域取得了显著的进展。随着这些LLM在越来越复杂的任务上的部署，它们往往面临着对长时间推理过程或理解更大上下文的需求。在这些情况下，LLM在长序列上的长度推广失败变得更加突出。大多数预训练方案将训练序列截断到固定长度（例如LLaMa的2048）。即使使用了相对位置编码来应对这个问题，LLM在更长的上下文之后往往难以生成流畅的文本，更不用说进行下游任务了。常见的解决方案，如在更长的语料库上进行微调，往往需要耗费大量的硬件和时间成本，并需要进行仔细的训练过程设计。为了更高效地利用现有LLM的生成能力，我们在理论和实证上研究了主要的分布外(OOD) f

    In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
    
[^77]: 从DDMs到DNNs：利用决策过程的数据和模型来改善人工智能与人类之间的交互

    From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions. (arXiv:2308.15225v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.15225](http://arxiv.org/abs/2308.15225)

    本论文提出利用决策过程数据和模型改善人工智能与人类之间的交互。通过详细描述决策过程和建立决策演变模型，可以揭示潜在的偏好，同时追踪决策过程的数据可以提供重要信息，从而改善人工智能的预测能力。

    

    在过去的几十年中，认知神经科学家和行为经济学家已经认识到详细描述决策过程和建立决策随时间演变的模型的价值。例如，决策所需的时间可以揭示一个个体真正的潜在偏好，而不仅仅是决策本身。类似地，追踪决策过程的数据，如眼动或神经记录，包含了关键的信息，即使没有达成决策也可以被利用。在这里，我们认为人工智能研究应更加关注决策如何随时间演变以及如何融入相关的过程数据来改善人工智能的预测，特别是在人与人工智能之间的交互中。首先，我们介绍了一个非常成熟的计算框架，该框架认为决策是从杂音累积的证据中产生的，并介绍了相关的心理学、神经科学和经济学的实证研究。

    Over the past decades, cognitive neuroscientists and behavioral economists have recognized the value of describing the process of decision making in detail and modeling the emergence of decisions over time. For example, the time it takes to decide can reveal more about an agents true hidden preferences than only the decision itself. Similarly, data that track the ongoing decision process such as eye movements or neural recordings contain critical information that can be exploited, even if no decision is made. Here, we argue that artificial intelligence (AI) research would benefit from a stronger focus on insights about how decisions emerge over time and incorporate related process data to improve AI predictions in general and human-AI interactions in particular. First, we introduce a highly established computational framework that assumes decisions to emerge from the noisy accumulation of evidence, and we present related empirical work in psychology, neuroscience, and economics. Next, 
    
[^78]: 通过Mixup增强的元学习方法实现蛋白质模拟器的高效微调

    Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])

    [http://arxiv.org/abs/2308.15116](http://arxiv.org/abs/2308.15116)

    本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。

    

    分子动力学模拟已经成为研究生物分子的基本工具。与此同时，我们希望在分子能够波动的各种条件下对一组粒子进行模拟。本文中，我们将软提示学习方法应用于分子动力学任务并进行了适应性探索。我们的模型可以在有限的训练数据下非常好地泛化到未见过的和超出分布的场景。虽然我们的工作以温度为测试案例，但我们的方法的多功能性使其可以通过任何连续的动态条件（如压力和体积）进行有效模拟。我们的框架有两个阶段：1）使用数据混合技术进行预训练，增强分子结构数据和温度提示，然后通过逐渐增加比例的方式应用课程学习方法。2）基于元学习的微调框架提高了微调过程的样本效率，并为软提示微调提供更好的表现。

    Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
    
[^79]: AtmoRep:一种利用大规模表示学习的大气动力学随机模型

    AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.13280](http://arxiv.org/abs/2308.13280)

    提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。

    

    大气对人类有多种影响，从因天气不良而丧生的损失到对社会的长期社会和经济影响。因此，对大气动力学进行计算机模拟对我们和未来的世代的福祉非常重要。在这里，我们提出了AtmoRep，一种新颖的、与任务无关的大气动力学随机计算机模型，可以为广泛的应用提供技能结果。AtmoRep利用人工智能的大规模表示学习来确定大气高度复杂、随机动力学的通用描述，该描述基于历史轨迹的最佳可用估计，这些历史轨迹受观测约束。这是通过一种新颖的自监督学习目标和一个独特的集合实现的，该集合从随机模型中采样，其可变性受历史记录中的可变性启发。AtmoRep的任务无关性使其能够为各种应用提供灵活的结果。

    The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
    
[^80]: Halo：评估和降低开源弱大语言模型中的幻觉

    Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])

    [http://arxiv.org/abs/2308.11764](http://arxiv.org/abs/2308.11764)

    本文介绍了一种用于评估和减少开源弱大语言模型中幻觉问题的框架，并探索了知识注入和师生方法等技术来减轻低参数模型中的幻觉问题，实验结果表明，在挑战性领域中，这些模型的幻觉问题得到了减少。

    

    大型语言模型(LLMs)已经彻底改变了自然语言处理(NLP)领域。虽然对于研究和实际应用来说方便，但是与其更大规模的对应模型相比，开源的参数较少的LLMs经常出现严重幻觉问题。本文着重于测量和减少BLOOM 7B中的幻觉问题，该模型是公开提供给研究和商业应用的弱开源LLMs的代表。我们引入了HaloCheck，一种轻量级的无需知识的黑盒子框架，用于量化LLMs中幻觉问题的严重程度。此外，我们探索了知识注入和师生方法等技术，以减轻低参数LLMs中的幻觉问题。我们的实验证明了在这些LLMs的挑战性领域中幻觉问题的减少。

    Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
    
[^81]: 基于大型语言模型的自主代理的调查

    A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])

    [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)

    该论文综述了基于大型语言模型的自主代理的研究，提供了从整体角度对该领域的系统审查，其创新之处在于利用大量网络知识实现人类水平的智能决策。

    

    自主代理长期以来一直是学术界的研究热点。以往的研究往往集中在对有限知识的代理进行训练，而这与人类的学习过程存在明显差异，因此很难实现人类般的决策。近年来，通过获取大量的网络知识，大型语言模型（LLM）展现出了实现人类水平智能的显著潜力。这引发了对基于LLM的自主代理的研究的高涨兴趣。为了发挥LLM的全部潜力，研究人员设计了各种不同应用的代理体系结构。本论文综述了这些研究，从整体的角度对自主代理领域进行了系统的审查。具体而言，我们的重点是基于LLM的代理构建，为此我们提出了一个统一的框架。

    Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
    
[^82]: RAHNet: 检索增强型混合网络用于长尾图分类

    RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])

    [http://arxiv.org/abs/2308.02335](http://arxiv.org/abs/2308.02335)

    我们提出了一种检索增强型混合网络(RAHNet)用于长尾图分类任务，通过联合学习稳健的特征提取器和无偏的分类器，解决了图神经网络在长尾类别分布下的偏差和泛化能力有限的问题。

    

    图分类是许多实际多媒体应用中的关键任务，图可以表示各种多媒体数据类型，如图像、视频和社交网络。以往的研究在平衡的情况下应用图神经网络(GNN)，其中类分布是平衡的。然而，实际数据通常呈现出长尾类别分布，导致在使用GNN时对头部类别存在偏差，且对尾部类别的泛化能力有限。最近的方法主要集中在模型训练过程中重新平衡不同的类别，但这种方法未能明确引入新知识，并牺牲了头部类别的性能。为了解决这些缺点，我们提出了一种新的框架，称为检索增强型混合网络(RAHNet)，以分离的方式联合学习稳健的特征提取器和无偏的分类器。在特征提取器训练阶段，我们开发了一个图检索模块来搜索相关图形。

    Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant grap
    
[^83]: VLUCI: 可变参数学习未观测混淆变量进行反事实推断

    VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])

    [http://arxiv.org/abs/2308.00904](http://arxiv.org/abs/2308.00904)

    VLUCI是一个新颖的可变参数学习模型，用于解决反事实推断中的未观测混淆变量的问题。它通过生成未观测混淆变量的后验分布，并构建一个双重变分推断模型来解决因果推断中观测和未观测混淆变量的问题，从而提高反事实推断的准确性。

    

    因果推断在流行病学、医疗保健和经济学等领域中起着重要作用。在观察数据中进行去混淆和反事实预测已经成为因果推断研究中的一个重要问题。虽然现有模型可以处理观察到的混淆变量，但未观测到的混淆变量的存在仍然是一个重大挑战，扭曲了因果推断并影响了反事实结果的准确性。为了解决这个问题，我们提出了一个新颖的可变参数学习模型，用于反事实推断中的未观测混淆变量（VLUCI），它生成了未观测混淆变量的后验分布。VLUCI放松了大多数因果推断方法往往忽视的无混淆假设。通过解耦观察到的混淆变量和未观测到的混淆变量，VLUCI构建了一个双重变分推断模型，以近似未观测混淆变量的分布，这些变量用于推断更准确的反事实结果。对合成和实际数据上进行了大量实验。

    Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and s
    
[^84]: 仁爱智能：通过AI系统对利益、援助及相关道德失误进行建模的能力方法

    Beneficent Intelligence: A Capability Approach to Modeling Benefit, Assistance, and Associated Moral Failures through AI Systems. (arXiv:2308.00868v1 [cs.AI])

    [http://arxiv.org/abs/2308.00868](http://arxiv.org/abs/2308.00868)

    借鉴能力方法，研究者提出了一个框架，用于解决AI系统与个体互动时涌现的伦理问题。同时，他们也界定了道德可接受的互动条件，并对几种失败模式进行了对比分析。

    

    AI伦理学中普遍的讨论缺乏捕捉AI系统与个体互动时涌现的多样化伦理关切所需的语言和形式。借鉴Sen和Nussbaum的能力方法，我们提出一个框架，形式化了AI系统为利益相关者提供有意义的利益或援助所必需的伦理概念和权利。这些系统增强了利益相关者推进其人生计划和幸福感的能力，同时维护其基本权利。我们界定了AI系统与受其功能影响的人之间道德上可接受的互动的两个必要条件，以及实现有意义利益理想的两个充分条件。然后，我们将这个理想与几种突出的失败模式进行对比，即构成不合理的家长式主义、强迫、欺骗、剥削和支配的社交互动形式。

    The prevailing discourse around AI ethics lacks the language and formalism necessary to capture the diverse ethical concerns that emerge when AI systems interact with individuals. Drawing on Sen and Nussbaum's capability approach, we present a framework formalizing a network of ethical concepts and entitlements necessary for AI systems to confer meaningful benefit or assistance to stakeholders. Such systems enhance stakeholders' ability to advance their life plans and well-being while upholding their fundamental rights. We characterize two necessary conditions for morally permissible interactions between AI systems and those impacted by their functioning, and two sufficient conditions for realizing the ideal of meaningful benefit. We then contrast this ideal with several salient failure modes, namely, forms of social interactions that constitute unjustified paternalism, coercion, deception, exploitation and domination. The proliferation of incidents involving AI in high-stakes domains 
    
[^85]: Take-A-Photo: 三维到二维的点云模型生成预训练

    Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])

    [http://arxiv.org/abs/2307.14971](http://arxiv.org/abs/2307.14971)

    本文提出了一种新的三维到二维的生成式预训练方法，通过生成视图图像作为预训练方案，帮助三维模型更好地理解点云的几何结构和立体关系，并在实验证明了其优越性。

    

    在MAE带领下，生成式预训练在2D视觉领域已经显示出显著的潜力来提升基本模型的性能。然而，在3D视觉领域，对Transformer为基础的骨干网络的过度依赖以及点云的无序性限制了生成式预训练的进一步发展。本文提出了一种新颖的适用于任何点云模型的三维到二维的生成式预训练方法。我们通过交叉注意机制从不同的姿势生成视图图像作为预训练方案。相比于其点云对应物，生成视图图像具有更精确的监督，从而帮助3D背骨更好地理解点云的几何结构和立体关系。实验结果证明了我们提出的三维到二维的生成式预训练方法优于先前的预训练方法。我们的方法还能有效地提升...

    With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boost
    
[^86]: 通过单向流进行对抗性似然估计

    Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])

    [http://arxiv.org/abs/2307.09882](http://arxiv.org/abs/2307.09882)

    本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。

    

    生成对抗网络（GAN）能够产生高质量的样本，但无法提供样本周围的概率密度估计。然而，已经注意到在能量模型的设置中，最大化对数似然可以导致判别器提供非归一化的密度（通常称为能量）的对抗性框架。我们进一步发展了这一观点，结合重要性采样，并展示了以下内容：1）Wasserstein GAN对分区函数进行了有偏估计，我们提出使用无偏估计方法；2）在最优化似然时，必须最大化生成器的熵。这被假设会提供更好的模式覆盖。与以前的工作不同，我们明确计算了生成样本的密度。这是设计无偏估计分区函数以及计算生成器熵的关键因素。生成密度是通过一种新型的流网络来获得的，称为单向流网络。

    Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
    
[^87]: 朝着可解释的AI在移动数据科学中的应用

    Towards eXplainable AI for Mobility Data Science. (arXiv:2307.08461v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08461](http://arxiv.org/abs/2307.08461)

    本文介绍了朝着可解释的AI在移动数据科学中的应用的工作，包括可解释模型的设计和使用时间图神经网络和反事实来学习从密集轨迹数据中提取信息的方法。

    

    本文介绍了我们正在进行的关于可解释AI在移动数据科学应用中的工作，重点是能够从密集轨迹数据（如车辆和船只的GPS轨迹）中学习的可解释模型，使用时间图神经网络（GNN）和反事实。我们回顾了现有的GeoXAI研究，提出了以人为中心的可理解解释的需求，并勾画出了朝着可解释的AI在移动数据科学中的研究路径。

    This paper presents our ongoing work towards XAI for Mobility Data Science applications, focusing on explainable models that can learn from dense trajectory data, such as GPS tracks of vehicles and vessels using temporal graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI studies, argue the need for comprehensible explanations with human-centered approaches, and outline a research path toward XAI for Mobility Data Science.
    
[^88]: Rad-ReStruct: 一种新颖的结构化放射学报告的VQA基准和方法

    Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v1 [cs.CV])

    [http://arxiv.org/abs/2307.05766](http://arxiv.org/abs/2307.05766)

    本文提出了Rad-ReStruct，一个用于评估和比较不同方法的新型基准数据集，以X光图像的结构化报告形式提供了细粒度、按层次排序的注释。我们提出了一种新方法hi-VQA，将结构化报告任务建模为分层视觉问答(VQA)，并考虑先前提问和回答的上下文来填充结构化放射学报告。实验证明hi-VQA取得了与最先进方法相竞争的性能。

    

    放射学报告是放射科医生与其他医务人员之间沟通的重要部分，但其可能耗时且容易出错。其中一种减轻这种情况的方法是结构化报告，它比自由文本报告更节约时间并且能够实现更精确的评估。然而，关于自动化结构化报告的研究有限，并且目前没有公开的基准用于评估和比较不同方法。为了弥补这一空白，我们介绍了Rad-ReStruct，这是一个新的基准数据集，提供了细粒度的、按层次排序的X光图像的结构化报告形式的注释。我们将结构化报告任务建模为分层视觉问答(VQA)，并提出了hi-VQA，一种考虑先前提问和回答的上下文以填充结构化放射学报告的新方法。我们的实验证明hi-VQA在医学VQA基准测试中取得了与最先进方法相竞争的性能。

    Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time-consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating structured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that provides fine-grained, hierarchically ordered annotations in the form of structured reports for X-Ray images. We model the structured reporting task as hierarchical visual question answering (VQA) and propose hi-VQA, a novel method that considers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benc
    
[^89]: 注意力机制中的边缘最大化

    Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])

    [http://arxiv.org/abs/2306.13596](http://arxiv.org/abs/2306.13596)

    这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。

    

    注意力机制是Transformer架构的核心组件，也是大型语言模型取得惊人成功的原因之一。然而，注意力机制背后的理论原则尚不清楚，特别是它的非凸优化动力学。本文探讨了开创性的softmax-attention模型$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$，其中$\boldsymbol{X}$是标记序列，$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$是可调参数。我们证明了在$\boldsymbol{p}$或等价的$\boldsymbol{W}$上运行梯度下降会沿着方向收敛到分隔“局部最优”标记和“非最优”标记的最大边缘解。这明确地形式化了注意力作为一种标记分离机制。值得注意的是，我们的结果适用于一般数据，并使用嵌入$\boldsymbol{Xv}$和$\texttt{softmax}(\boldsymbol{XWp})$精细地表征标记的“最优性”。

    Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
    
[^90]: 布局和任务感知的零样本文档图像问答指导模型

    Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00526](http://arxiv.org/abs/2306.00526)

    该论文提出了一种布局和任务感知的指导提示模型，称为LATIN-Prompt，通过将文档图像问答对齐到现成的指导调优语言基础模型，利用其零样本能力来提高效果。该模型包括布局感知的文档内容和任务感知的描述，能够恢复文本片段之间的布局信息，并生成符合任务需求的答案。

    

    基于布局感知多模态预训练模型的预训练-微调范式在文档图像问答方面取得了显著进展。然而，领域预训练和任务微调对于额外的视觉、布局和任务模块阻止了其直接利用现成的指导调优语言基础模型，而这些模型最近在零样本学习方面显示出了良好的潜力。与将语言模型与文档图像问答领域对齐相反，我们将文档图像问答与现成的指导调优语言基础模型对齐，利用其零样本能力。具体而言，我们提出了布局和任务感知的指导提示模型，称为LATIN-Prompt，它包括布局感知的文档内容和任务感知的描述。前者通过适当的空格和换行符从OCR工具中恢复文本片段之间的布局信息。后者确保模型生成符合任务需求的答案。

    The pre-training-fine-tuning paradigm based on layout-aware multimodal pre-trained models has achieved significant progress on document image question answering. However, domain pre-training and task fine-tuning for additional visual, layout, and task modules prevent them from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Contrary to aligning language models to the domain of document image question answering, we align document image question answering to off-the-shell instruction-tuning language foundation models to utilize their zero-shot capability. Specifically, we propose layout and task aware instruction prompt called LATIN-Prompt, which consists of layout-aware document content and task-aware descriptions. The former recovers the layout information among text segments from OCR tools by appropriate spaces and line breaks. The latter ensures that the model generates answers that m
    
[^91]: 《增强上下文学习提高代码生成能力》

    Towards Enhancing In-Context Learning for Code Generation. (arXiv:2303.17780v1 [cs.SE])

    [http://arxiv.org/abs/2303.17780](http://arxiv.org/abs/2303.17780)

    本文提出了一种名为AceCoder的代码生成上下文学习方法，与标准上下文学习相比，它通过示例检索和引导代码生成来提高生成代码的准确性和鲁棒性。

    

    基于预先训练的语言模型的上下文学习已经在代码生成领域表现出了强大的成功。通过这种方法，无需训练，模型只需要输入一个由少量需求-代码示例和一个新需求组成的提示，就能生成出新的程序。但是，现有的研究仅仅将上下文学习用于自然语言生成，忽略了代码生成的独特特性。我们称这些研究为标准上下文学习。本文通过对人类编码过程的观察，提出了一种新的名为AceCoder的代码生成上下文学习方法。与标准上下文学习相比，AceCoder有两个新颖之处。(1)示例检索。它检索类似程序作为示例，并从中学习编程技能(如算法、API)。(2)引导代码生成。它鼓励预训练的语言模型生成中间预备代码(如测试用例、API)并帮助模型理解需求和指导下一步代码生成。我们将AceCoder应用到大量代码生成任务中，实验结果表明，与现有的代码生成系统相比，AceCoder具有更高的准确性和鲁棒性。

    In-context learning (ICL) with pre-trained language models (PTLMs) has shown great success in code generation. ICL does not require training. PTLMs take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse ICL techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard ICL.  Inspired by observations of the human coding process, we propose a novel ICL approach for code generation named AceCoder. Compared to standard ICL, AceCoder has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, APIs) from them. (2) Guided Code Generation. It encourages PTLMs to output an intermediate preliminary (e.g., test cases, APIs) before generating programs. The preliminary can help PTLMs understand requirements and guide the next code generation. We apply AceCode
    
[^92]: 大型语言模型生成混合代码文本的提示：东南亚语言的案例

    Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])

    [http://arxiv.org/abs/2303.13592](http://arxiv.org/abs/2303.13592)

    本文探讨了使用大型语言模型（LLMs）生成东南亚五种语言和Singlish的混合代码数据的方法，发现ChatGPT展现出最高的潜力。然而，由于词汇选择错误的影响，ChatGPT和InstructGPT在生成混合代码时的熟练程度受到限制。

    

    尽管混合代码在世界许多地区是一种常见的语言实践，但收集高质量且低成本的混合代码数据仍然是自然语言处理（NLP）研究的重大挑战。最近大型语言模型（LLMs）的普及迫使人们问：这些系统能用于数据生成吗？在本文中，我们探讨了在一个零-shot的方式下如何提示LLMs为东南亚（SEA）的五种语言（印尼语，马来语，中文，塔加路语，越南语）及克里奥尔语S ingl ish创造混合代码数据。我们发现，ChatGPT显示出最大的潜力，当明确定义“混合代码”术语时，能够68%的时间生成混合代码文本。此外，ChatGPT和InstructGPT（davinci-003）生成S ingl ish文本的表现也值得注意，它们在各种提示下的成功率平均为96%。但是，ChatGPT和InstructGPT的混合代码熟练程度受到词汇选择错误的影响，导致语义不正确的输出。

    While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
    
[^93]: 多智能体强化学习用于大规模格网交通网络区域信号控制

    Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])

    [http://arxiv.org/abs/2303.11899](http://arxiv.org/abs/2303.11899)

    本文提出了一种新的训练框架 RegionLight，基于交叉口之间的邻接关系将智能体分配到每个区域中。同时，研究人员扩展了BDQ方法为DBDQ，以限制联合动作空间大小的增长并缓解智能体训练问题。

    

    多智能体强化学习（MARL）的自适应交通信号控制是当前非常流行的研究领域。大多数现有方法中，一个智能体控制单个路口，这些方法侧重于路口之间的协作。然而，MARL的非稳态性质随着交通网络规模的增长，仍然限制着上述方法的性能。一种妥协的策略是将一名智能体分配到一组路口中，以减少智能体数量。这种策略存在两个挑战，一个是如何将交通网络划分成小区域，另一个是如何搜索区域内的最优联合动作。本文提出了一种新的训练框架RegionLight，其中我们的区域划分规则基于交叉口之间的邻接关系，并扩展了Branching Dueling Q-Network(BDQ)。该方法将BDQ进一步优化为Dynamic Branching Dueling Q-Network(DBDQ)，以限制联合动作空间大小的增长并缓解智能体训练问题。

    Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
    
[^94]: Internet Explorer:开放网络上的有针对性表示学习

    Internet Explorer: Targeted Representation Learning on the Open Web. (arXiv:2302.14051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14051](http://arxiv.org/abs/2302.14051)

    Internet Explorer是一种能够利用互联网进行有针对性表示学习的方法，通过自我监督的方式在网络上搜索相关图像并训练小规模模型，从而提高在特定任务上的性能。

    

    现代视觉模型通常依赖于在大型静态数据集上预训练的通用模型进行微调。这些通用模型只能捕捉到它们的预训练数据集中的知识，而这些数据集只是互联网的微小、过时的快照——而互联网上每天都上传数十亿张图片。我们提出了一种替代方法：不是希望我们的静态数据集在大规模预训练后能够转移到我们期望的任务上，而是我们建议动态利用互联网，快速训练一个在当前任务上表现非常好的小规模模型。我们的方法叫做Internet Explorer，以自我监督的方式在网络上探索，逐步找到改善所需目标数据集性能的相关示例。它循环在互联网上搜索带有文本查询的图像，通过下载的图像进行自我监督训练，确定哪些图像是有用的，并确定下一步要搜索的优先级。我们在几个数据集上评估了Internet Explorer。

    Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several d
    
[^95]: LB-SimTSC: 一种用于半监督时间序列分类的高效相似性感知图神经网络

    LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification. (arXiv:2301.04838v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04838](http://arxiv.org/abs/2301.04838)

    LB-SimTSC是一种高效的相似性感知图神经网络，用于半监督时间序列分类。它通过使用LB_Keogh作为DTW的下界来解决DTW二次复杂性限制，在少标签设置中表现出优秀的准确度。

    

    时间序列分类是一项重要的数据挖掘任务，在过去的20年里引起了很多关注。由于实际中标签稀缺，只有少量标记样本的半监督时间序列分类变得流行。最近，提出了相似性感知时间序列分类（SimTSC）来解决这个问题，该方法使用基于批量数据的两两动态时间规整（DTW）距离生成的图神经网络分类模型。SimTSC表现出优秀的准确度，并在几个少标签设置中胜过了最先进的深度学习模型。然而，由于SimTSC依赖于两两DTW距离，DTW的二次复杂性限制了它只能在适度大小的数据集上使用。为了解决这个挑战，我们提出了一种新的高效半监督时间序列分类技术LB-SimTSC，其中包括一个新的图构建模块。我们提出使用DTW的下界LB_Keogh来近似DTW。

    Time series classification is an important data mining task that has received a lot of interest in the past two decades. Due to the label scarcity in practice, semi-supervised time series classification with only a few labeled samples has become popular. Recently, Similarity-aware Time Series Classification (SimTSC) is proposed to address this problem by using a graph neural network classification model on the graph generated from pairwise Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy and outperforms state-of-the-art deep learning models in several few-label settings. However, since SimTSC relies on pairwise DTW distances, the quadratic complexity of DTW limits its usability to only reasonably sized datasets. To address this challenge, we propose a new efficient semi-supervised time series classification technique, LB-SimTSC, with a new graph construction module. Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to approximate 
    
[^96]: BigText-QA：基于大规模混合知识图的问答系统

    BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph. (arXiv:2212.05798v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.05798](http://arxiv.org/abs/2212.05798)

    BigText-QA引入了一种综合的QA方法，能够基于有结构化和非结构化知识的知识图回答复杂问题。

    

    在文本资源上回答复杂问题仍然是一个挑战，特别是在处理自然语言句子中多个实体之间的微妙关系时。为此，像YAGO、DBpedia、Freebase和Wikidata这样的知识库（KB）在过去十年中广泛使用并得到了认可，用于问答（QA）应用。虽然这些知识库提供了结构化的知识表示，但缺乏自然语言来源中的上下文多样性。为了解决这个限制，BigText-QA引入了一种综合的QA方法，能够基于更冗余的知识图（KG）回答问题，该图将结构化和非结构化（即“混合”）知识以统一的图形表示方式组织。因此，BigText-QA能够兼顾两个世界的优势——一个经典的命名实体集合，映射到一个结构化的背景知识库（如YAGO或Wikidata），以及一个从自然语言来源中获取的上下文信息。

    Answering complex questions over textual resources remains a challenge, particularly when dealing with nuanced relationships between multiple entities expressed within natural-language sentences. To this end, curated knowledge bases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely used and gained great acceptance for question-answering (QA) applications in the past decade. While these KBs offer a structured knowledge representation, they lack the contextual diversity found in natural-language sources. To address this limitation, BigText-QA introduces an integrated QA approach, which is able to answer questions based on a more redundant form of a knowledge graph (KG) that organizes both structured and unstructured (i.e., "hybrid") knowledge in a unified graphical representation. Thereby, BigText-QA is able to combine the best of both worlds$\unicode{x2013}$a canonical set of named entities, mapped to a structured background KB (such as YAGO or Wikidata), as well as an o
    
[^97]: CodeEditor: 使用预训练模型学习编辑源代码

    CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.17040](http://arxiv.org/abs/2210.17040)

    CodeEditor是一个预训练代码编辑模型，通过专门的预训练任务和代码编辑任务的结合，提高了代码编辑模型的性能和泛化能力。

    

    在软件开发过程中，开发人员经常需要进行重复的代码编辑活动，例如代码重构等。预训练的代码编辑模型已经取得了最先进的结果。预训练模型首先使用预训练任务进行预训练，然后使用代码编辑任务进行微调。现有的预训练任务主要是代码填充任务（例如，掩码语言模型），这些任务来自自然语言处理领域，不适用于自动代码编辑。本文提出了一个专门用于代码编辑的新型预训练任务，并介绍了一个名为CodeEditor的有效预训练代码编辑模型。我们的预训练任务进一步提高了代码编辑模型的性能和泛化能力。具体来说，我们收集了大量的真实代码片段作为参考，并使用一个强大的生成器将它们重写成变异版本。然后，我们使用CodeEditor对变异版本进行预训练，将其编辑为正确的代码。

    Developers often perform repetitive code editing activities for various reasons (e.g., code refactoring) during software development. Pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.  This paper proposes a novel pre-training task specialized in code editing and presents an effective pre-trained code editing model named CodeEditor. Our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the c
    
[^98]: 通过NeurVec加速大规模动态系统数值求解器的模拟

    Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2208.03680](http://arxiv.org/abs/2208.03680)

    通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。

    

    在众多科学和工程领域中，大规模动态系统的模拟至关重要。然而，传统的数值求解器在估计积分时由于步长选择的限制，存在着精度和计算效率之间的权衡。为了解决这个挑战，我们引入了一种基于深度学习的修正器，称为NeurVec，它可以补偿集成误差并在模拟中实现更大的时间步长。我们在各种复杂动态系统基准上进行了大量实验证明，即使在使用有限和离散数据进行训练时，NeurVec在连续相空间上表现出了显著的泛化能力。NeurVec显著加速了传统求解器，实现了几十到几百倍的速度提升，同时保持了高水平的准确性和稳定性。此外，NeurVec的简单而有效的设计结合了易于实现的特点，有潜力建立起一个新的求解器范式。

    The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
    
[^99]: 自主农业机器人用于智能农业

    Autonomous Agriculture Robot for Smart Farming. (arXiv:2208.01708v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2208.01708](http://arxiv.org/abs/2208.01708)

    该论文介绍了一种用于智能农业的自主地面机器人，具备智能感知和自主除草功能，并能提供施肥、杀虫剂等服务，为作物和土壤健康监测提供信息。

    

    该项目旨在开发和展示一种具备智能能力的地面机器人，能够进行不同低矮作物的半自主农业操作，称为农业应用机器人（Agriculture Application Robot，AAR）。AAR是一个轻量级的太阳能电动机器人，使用智能感知进行植物的检测和分类，还具备自主除草功能的机械臂。该机器人可以向作物、杂草和其他害虫等目标提供施肥、杀虫剂、除草剂和其他液体，同时为未来更高级任务（如产量估计、作物和土壤健康监测）的研究提供信息。我们展示了机器人的设计以及在真实环境中的实验结果。

    This project aims to develop and demonstrate a ground robot with intelligence capable of conducting semi-autonomous farm operations for different low-heights vegetable crops referred as Agriculture Application Robot(AAR). AAR is a lightweight, solar-electric powered robot that uses intelligent perception for conducting detection and classification of plants and their characteristics. The system also has a robotic arm for the autonomous weed cutting process. The robot can deliver fertilizer spraying, insecticide, herbicide, and other fluids to the targets such as crops, weeds, and other pests. Besides, it provides information for future research into higher-level tasks such as yield estimation, crop, and soil health monitoring. We present the design of robot and the associated experiments which show the promising results in real world environments.
    
[^100]: 人类学习奖励函数偏好的模型

    Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02231](http://arxiv.org/abs/2206.02231)

    本研究提出了一种将人类偏好建模为每个轨迹段的遗憾的方法，并证明了可以根据这些遗憾生成的偏好来识别生成这些偏好的奖励函数。实验证明，这种遗憾偏好模型在性能上优于以前的模型。

    

    强化学习的效用受限于奖励函数与人类利益的一致性。一种有前途的对齐方法是从人类生成的轨迹段对之间学习奖励函数，这是一种从人类反馈中进行的强化学习方法。通常假设这些人类偏好仅由部分回报来决定，即每个轨迹段上的奖励总和。我们发现这种假设存在缺陷，提出将人类偏好建模为由每个轨迹段的遗憾来决定，遗憾是一种衡量轨迹段与最优决策之间偏离程度的度量。在根据遗憾生成的无穷多个偏好中，我们证明可以识别到与生成这些偏好的奖励函数等价的奖励函数，并且我们证明以前的部分回报模型在多种情境下缺乏这种可识别性属性。通过实验证明，我们提出的遗憾偏好模型在性能上优于以前的模型。

    The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperf
    
[^101]: 图形平滑卷积网络用于异常检测

    Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.10274](http://arxiv.org/abs/2010.10274)

    本文引入了一种图形平滑卷积网络用于半监督异常检测，通过使用跳跃连接和图结构信息来学习有区分性的节点表示。

    

    图卷积是许多基于图结构数据的深度神经网络的基本构建块。在本文中，我们引入了一种简单而非常有效的带有跳跃连接的图卷积网络，用于半监督异常检测。我们模型的逐层传播规则在理论上受到几何处理中隐式平滑概念的启发，包括用于聚合来自相邻节点的信息的图卷积模块和用于组合逐层邻居表示的跳跃连接模块。这个传播规则是通过雅可比方法从隐式平滑方程的迭代解导出的。除了通过网络层之间的跳跃连接捕获来自远程图节点的信息外，我们的方法还利用图结构和节点特征来学习有区分性的节点表示。这些跳跃连接是根据我们提出的网络架构经过设计整合的。

    Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network archi
    

