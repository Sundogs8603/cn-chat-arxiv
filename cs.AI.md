# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation.](http://arxiv.org/abs/2308.16909) | 这项研究介绍了一个用于无条件视频生成的新型运动生成器设计，利用基于学习的反向网络实现了运动连贯性和生成空间的约束。 |
| [^2] | [InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion.](http://arxiv.org/abs/2308.16905) | 本文提出了InterDiff，一种使用物理信息扩散生成三维人物-物体交互的框架。该框架通过两个关键步骤：交互扩散和交互校正，有效地模拟了具有不同形状的动态物体和全身运动的物体交互。实验证明了该方法的有效性。 |
| [^3] | [Transformers as Support Vector Machines.](http://arxiv.org/abs/2308.16898) | 这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。 |
| [^4] | [PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction.](http://arxiv.org/abs/2308.16896) | 本文提出了一个新的三透视圆柱视图表示方法，在自动驾驶中进行点云三维语义占据预测。通过构建柱坐标系下的三透视圆柱视图，实现了对点云的精细建模，同时利用空间组池化和二维主干网络高效处理数据。结果表明，该方法可以有效地预测点云的语义占据情况。 |
| [^5] | [Language-Conditioned Path Planning.](http://arxiv.org/abs/2308.16893) | 本研究提出了一种语言条件路径规划的方法，通过学习碰撞函数，预测机器人与环境之间的碰撞，实现灵活、有条件的路径规划，无需手动标注或者真实物体模型。 |
| [^6] | [ReZero: Region-customizable Sound Extraction.](http://arxiv.org/abs/2308.16892) | ReZero是一个可定制的区域声音提取框架，针对R-SE任务设计了不同类型的空间区域的定义、空间特征提取和聚合方法以及多通道扩展的BSRNN模型，通过实验证明了其有效性。 |
| [^7] | [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants.](http://arxiv.org/abs/2308.16884) | Belebele是一个包含122种语言变体的多选机器阅读理解数据集，可用于评估文本模型在高、中和低资源语言中的性能。尽管英语为中心的大型语言模型在跨语言转移方面表现良好，但小型多语言遮蔽语言模型在其他语言上表现更佳。 |
| [^8] | [Adaptation Speed Analysis for Fairness-aware Causal Models.](http://arxiv.org/abs/2308.16879) | 本研究考虑了适应速度分析公平感知因果模型的问题，通过研究带有因果-偏差-效果结构的简单结构因果模型，探讨了在训练过程中考虑敏感变量（偏差）的公平性和适应速率的相关因素。 |
| [^9] | [The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages.](http://arxiv.org/abs/2308.16871) | 本文介绍了Gender-GAP Pipeline，一个用于55种语言中性别表征的自动流水线，通过使用多语言性别人称名词词汇表对文本进行量化来报告数据中的性别表征。在WMT训练数据和新闻任务的开发数据中表明当前数据偏向男性表征。 |
| [^10] | [Learning Driver Models for Automated Vehicles via Knowledge Sharing and Personalization.](http://arxiv.org/abs/2308.16870) | 本文提出了一种通过知识共享和个性化学习驾驶员模型的方法，以用于自动驾驶车辆。通过跨车辆共享知识，增加AVs在真实驾驶情景中的暴露，同时保留个性化的模型，以适应不同车辆的条件和特性。 |
| [^11] | [IoMT-Blockchain based Secured Remote Patient Monitoring Framework for Neuro-Stimulation Device.](http://arxiv.org/abs/2308.16857) | 该论文探讨了一种基于区块链的安全远程患者监测框架，使用IoMT技术和神经刺激设备，实现了无侵入性的远程神经刺激系统。 |
| [^12] | [Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information.](http://arxiv.org/abs/2308.16836) | 本文通过使用BERT衍生的语义嵌入来提高歌声合成的表达力，首次引入歌词的文本表示作为附加输入，并通过额外的能量预测器和重新设计的音高预测器来增强歌声的表达力。 |
| [^13] | [Can Programming Languages Boost Each Other via Instruction Tuning?.](http://arxiv.org/abs/2308.16824) | 研究发现，编程语言可以在指令调优阶段相互促进，并显著提高彼此的能力。 |
| [^14] | [Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets.](http://arxiv.org/abs/2308.16822) | 本文提出了一种针对层级数据集的扩展多输出高斯过程 (MOGPs)，通过引入潜在变量和特定核函数，可以更好地捕捉不同输出之间的关系和相关性。这种方法预计能够在任务数量增加时提高可扩展性。 |
| [^15] | [Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network.](http://arxiv.org/abs/2308.16818) | 该论文提出了一种基于异步时空图卷积网络的不规则交通时间序列预测方法，用于解决智能交叉口产生的异步空间依赖、不规则时间依赖和可变长度序列预测等挑战。 |
| [^16] | [Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks.](http://arxiv.org/abs/2308.16800) | 本文研究了图神经网络中的平滑过度和特征关联过高现象，发现固定不变的子空间导致了节点表示的等级崩塌。在该子空间中平滑向量的存在导致过度平滑，即使避免过度平滑也会导致过高的关联。为了解决这个问题，我们提出了一种克罗内克积之和作为一种有效方法。 |
| [^17] | [Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming.](http://arxiv.org/abs/2308.16785) | 代理团队情景感知（ATSA）是一个基于人工智能和人类联合工作的情景感知框架，旨在提高混合团队的绩效。该框架结合了个体和团队的情景感知模型，并涉及双向和动态的交互。 |
| [^18] | [StratMed: Relevance Stratification for Low-resource Medication Recommendation.](http://arxiv.org/abs/2308.16781) | StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。 |
| [^19] | [Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm.](http://arxiv.org/abs/2308.16775) | 这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。 |
| [^20] | [Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems.](http://arxiv.org/abs/2308.16769) | 该论文介绍了在工业控制系统网络安全领域进行低门槛研究和教育的重要性。通过利用开发的高保真模拟器，作者构建了一个集成框架用于自动发动网络攻击、收集数据、训练机器学习模型，并针对实际的化学和制造过程进行评估。作者还提出了一种名为MinTWin SVM的入侵检测模型，它结合了无监督机器学习和滑动窗口的方法。 |
| [^21] | [Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection.](http://arxiv.org/abs/2308.16763) | 该论文介绍了一种名为“Ladder-of-Thought”的方法，通过引入外部知识来提升立场检测任务中的语言模型的性能，解决了小型模型在应用先前内部知识时性能提升不明显的问题，以及大规模模型在效率方面的挑战。 |
| [^22] | [Context Aware Query Rewriting for Text Rankers using LLM.](http://arxiv.org/abs/2308.16753) | 这项工作研究了使用基于LLM的上下文感知查询重写方法来提高文本排名任务。通过通过上下文感知提示来重写模糊的训练查询，克服了概念漂移和推理开销的固有局限性。 |
| [^23] | [Socratis: Are large multimodal models emotionally aware?.](http://arxiv.org/abs/2308.16741) | 这项研究提出了Socratis，一个新的社会反应基准，用于学习多模态内容的多样化情绪反应。根据人类研究结果，人们更喜欢人工撰写的情感原因，比机器生成的要多2倍以上。 |
| [^24] | [Robust Networked Federated Learning for Localization.](http://arxiv.org/abs/2308.16737) | 本文提出了一种鲁棒的网络化联邦学习方法，通过采用$L_1$-范数鲁棒性和分布式次梯度框架，解决了在分布式环境中定位问题中的异常数据干扰和算法收敛挑战。 |
| [^25] | [Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment.](http://arxiv.org/abs/2308.16735) | 本论文提出了一种名为FedPDA的新颖自适应框架，将联邦学习中学习远程数据的功能引入到PDA中，通过远程梯度交换使部署模型能够从源数据中获取信息，并针对目标领域进行优化。 |
| [^26] | [Proof of Deep Learning: Approaches, Challenges, and Future Directions.](http://arxiv.org/abs/2308.16730) | 深度学习模型的性能提升需要更多计算能力，而区块链中的工作量证明机制对计算能力提出了挑战。 |
| [^27] | [Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance.](http://arxiv.org/abs/2308.16725) | 提出了一种基于扩散的地形生成方法，名为地形扩散网络（TDN），该方法通过用户引导以实现更好的可控性，并考虑地形特征为虚拟环境生成逼真的地形。 |
| [^28] | [CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset.](http://arxiv.org/abs/2308.16705) | CReHate通过跨文化重新注释英语仇恨言论数据集，揭示了来自不同国家的个体对仇恨言论的不同看法，并引入了一种具有文化敏感性的分类器。这些发现强调了重新评估NLP研究在仇恨言论领域的必要性。 |
| [^29] | [Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models.](http://arxiv.org/abs/2308.16703) | 本文介绍了故障注入和安全错误攻击用于提取嵌入式神经网络模型的方法，并阐述了对32位微控制器上的深度神经网络进行模型提取攻击的实验结果。 |
| [^30] | [Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology.](http://arxiv.org/abs/2308.16688) | 本文介绍了一种利用大型语言模型自动分类科学文章的方法，主要应用于眼科学领域，但可扩展到其他领域。通过比较不同LLM模型，结果表明LLMs在无需人工干预的情况下能有效地对大量眼科学论文进行分类。 |
| [^31] | [Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack.](http://arxiv.org/abs/2308.16684) | 本文发现了一种更严重的后门攻击威胁，即任何人都可以利用易获取的有损压缩算法进行自然后门攻击，无需设计特定触发器或进行繁琐调试。 |
| [^32] | [Fault Injection on Embedded Neural Networks: Impact of a Single Instruction Skip.](http://arxiv.org/abs/2308.16665) | 本论文通过实验探究了在嵌入式的Cortex M4 32位微控制器平台上应用电磁和激光注入故障的影响，并揭示了对神经网络推理流程进行修改攻击可能带来的完整性威胁。 |
| [^33] | [Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering.](http://arxiv.org/abs/2308.16622) | 本文介绍了一个基准测试框架，用于评估大型语言模型在知识图谱工程中的应用。框架包括语法和错误修正、事实提取和数据集生成三个挑战，同时也揭示了LLMs在零-shot提示下辅助知识图谱生成方面的不足。 |
| [^34] | [High Accuracy Location Information Extraction from Social Network Texts Using Natural Language Processing.](http://arxiv.org/abs/2308.16615) | 本研究利用自然语言处理从社交网络文本中提取位置信息，以构建准确的恐怖袭击预测数据集。实验证明，现有解决方案对于位置识别的准确性较差，而我们的解决方案解决了这个问题，并计划进一步扩展以提取其他相关信息。 |
| [^35] | [Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts.](http://arxiv.org/abs/2308.16609) | 本文提出了一种新颖的方法，通过合作专家实现了长尾图分类，解决了现有方法在处理图数据上的不足。 |
| [^36] | [The Quest of Finding the Antidote to Sparse Double Descent.](http://arxiv.org/abs/2308.16596) | 本文致力于寻找稀疏双下降的解毒剂，通过研究和提出解决方案，分别采用l2正则化和知识蒸馏来避免稀疏双下降现象，以找到性能和稀疏性的最佳平衡点。 |
| [^37] | [CL-MAE: Curriculum-Learned Masked Autoencoders.](http://arxiv.org/abs/2308.16572) | 本文提出了一种课程学习的遮罩自编码器（CL-MAE）。我们引入了一种可学习的遮罩模块，通过更新遮罩策略来增加自监督重构任务的复杂性。通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。 |
| [^38] | [The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning.](http://arxiv.org/abs/2308.16562) | 本文提出了一种新的算法MEME，在对抗性恶意软件生成中使用了模型提取和恶意软件逃避的方法，并通过实验证明该算法在逃避方面表现更好。 |
| [^39] | [On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions.](http://arxiv.org/abs/2308.16539) | 本论文研究了微分博弈、最优控制和基于能量的模型之间的联系，并提出了基于能量的潜在博弈的新的端到端学习应用，通过神经网络和可微分的博弈论优化层的组合来提高预测性能。 |
| [^40] | [The AI Revolution: Opportunities and Challenges for the Finance Sector.](http://arxiv.org/abs/2308.16538) | 本研究调查了金融行业中的人工智能应用，讨论了其革命潜力和相关挑战。人工智能的应用范围广泛，包括改善客户服务、欺诈检测、风险管理、信用评估和高频交易。然而，人工智能的应用也带来了透明度、可解释性、公平性、问责制和信任度等问题。 |
| [^41] | [Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints.](http://arxiv.org/abs/2308.16534) | 本文提出了一种方法，通过神经符号约束来调节基于评分的生成模型，实现了在非条件生成模型下强制执行任意的逻辑约束，从而获得了一个有效的、无需额外训练的条件采样算法。 |
| [^42] | [Developing Social Robots with Empathetic Non-Verbal Cues Using Large Language Models.](http://arxiv.org/abs/2308.16529) | 本研究通过整合非语言暗示增强社交机器人的共情能力，在社交机器人中设计和标记了四种共情非语言暗示（SAFE），并使用大型语言模型生成这些暗示。该研究的重要发现是在机器人的回应中观察到明显的模式，如对平静和积极的社交情感的偏好以及频繁的点头动作。尽管如此，该方法已经实现了一个能够进行上下文感知和更真实互动的社交机器人的开发。 |
| [^43] | [Curvature-based Pooling within Graph Neural Networks.](http://arxiv.org/abs/2308.16516) | 本研究提出了一种名为CurvPool的汇聚方法，通过利用图的曲率概念来解决过度平滑和过度压缩的问题。它能够根据曲率自适应地识别负责这两种现象的结构，并构建具有更合适结构的图，从而实现更深层模型和远距离信息的结合。 |
| [^44] | [Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations.](http://arxiv.org/abs/2308.16505) | 本论文的创新点是将推荐模型和大型语言模型（LLMs）融合，创建了一个多功能交互式推荐系统，解决了推荐模型在提供解释和参与对话任务方面的困难。 |
| [^45] | [Individually Rational Collaborative Vehicle Routing through Give-And-Take Exchanges.](http://arxiv.org/abs/2308.16501) | 本文关注物流公司间自动交易订单以优化总收入的问题，提出了一种新的多智能体方法来解决合作车辆路径规划问题（CVRP），通过促进竞争物流代理之间的合作，实现减少行驶距离、提高运营效率，并确保个体合理性和更快的收敛速度。 |
| [^46] | [Generalised Winograd Schema and its Contextuality.](http://arxiv.org/abs/2308.16498) | 本文研究了广义Winograd Schema在上下文性方面的应用，提出了一种利用量子物理实验模型来解决Winograd模式挑战的方法。 |
| [^47] | [Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception.](http://arxiv.org/abs/2308.16493) | 本文提出一种方法，通过对不同模态的嵌入空间进行对齐，使得视觉-语言模型能够在不重新训练的情况下理解和推理额外的模态，并且在人机交互场景中改进机器人的感知能力。 |
| [^48] | [In-class Data Analysis Replications: Teaching Students while Testing Science.](http://arxiv.org/abs/2308.16491) | 这项研究揭示了课堂数据分析复制的可行性，以及这种方法对学生、教育者和科学家的成本与收益。同时，学生对数据的预期与实际情况存在差异。 |
| [^49] | [Latent Painter.](http://arxiv.org/abs/2308.16490) | 这个论文介绍了一种名为潜在画家的技术，它利用潜在作为画布和扩散器的预测作为计划来生成绘画动画，同时还可以在不同的检查点集中转换图像。 |
| [^50] | [Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning.](http://arxiv.org/abs/2308.16484) | 本文提出了一种使用元学习进行测试时间适应的方法来增强点云上采样模型的普适性，解决了测试数据分布与训练数据不同导致性能下降的问题。 |
| [^51] | [Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning.](http://arxiv.org/abs/2308.16481) | Point-TTA是一种通过多任务元辅助学习实现的点云配准测试时自适应框架，能够提高配准模型的泛化性能。 |
| [^52] | [Transformer Compression via Subspace Projection.](http://arxiv.org/abs/2308.16475) | Transformer压缩通过子空间投影，在减小模型隐藏大小的同时实现了较大的模型参数和计算资源的减少，并且与其他方法兼容。 |
| [^53] | [Enhancing Subtask Performance of Multi-modal Large Language Model.](http://arxiv.org/abs/2308.16474) | 该论文提出了一种方法，通过选择多个预训练模型来完成相同的子任务，通过组合多个模型的结果获得最佳的子任务结果。 |
| [^54] | [MaintainoMATE: A GitHub App for Intelligent Automation of Maintenance Activities.](http://arxiv.org/abs/2308.16464) | MaintainoMATE是一个GitHub应用程序，使用BERT模型实现自动问题报告分类和分配给专业开发人员。 |
| [^55] | [BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge.](http://arxiv.org/abs/2308.16458) | BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。 |
| [^56] | [Contrastive Representation Learning Based on Multiple Node-centered Subgraphs.](http://arxiv.org/abs/2308.16441) | 本文提出了一种基于多个节点中心子图的对比表示学习方法，在图对比学习的框架下，通过最大化同一节点的不同子图之间的互信息来学习节点表示，实验证明该方法在各种真实数据集和不同下游任务上取得了最先进的结果。 |
| [^57] | [BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks.](http://arxiv.org/abs/2308.16385) | BenchTemp是一个通用基准，用于评估时间图神经网络（TGNN）模型在不同工作负载上的表现。BenchTemp提供一组基准数据集和一个标准流程，用于公平比较不同的TGNN模型。通过BenchTemp，我们对不同任务和设置下的代表性TGNN模型进行了广泛比较。 |
| [^58] | [A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications.](http://arxiv.org/abs/2308.16375) | 这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。 |
| [^59] | [Strengthening the EU AI Act: Defining Key Terms on AI Manipulation.](http://arxiv.org/abs/2308.16364) | 这项研究提出技术建议，以明确定义欧盟AI法案中的关键概念，并提高可执行性。它包括对人格特质、行为、潜意识和欺骗技术的定义，以及对个体和群体利用的区分。此外，它还强调了对知情决策的定义和对治疗用途豁免的警告。 |
| [^60] | [Large Language Models as Data Preprocessors.](http://arxiv.org/abs/2308.16361) | 大型语言模型可以作为数据预处理器的应用，通过使用开发工程技术和传统方法来提高性能。 |
| [^61] | [Debunking Disinformation: Revolutionizing Truth with NLP in Fake News Detection.](http://arxiv.org/abs/2308.16328) | 这篇论文深入讨论了如何利用自然语言处理技术来检测假新闻，并揭示了其中的挑战和机遇。 |
| [^62] | [Causal Strategic Learning with Competitive Selection.](http://arxiv.org/abs/2308.16262) | 我们研究了具有竞争选择的因果战略学习中的代理选择问题，并提出了最佳选择规则的数学形式和实现机制。 |
| [^63] | [Calibrated Explanations for Regression.](http://arxiv.org/abs/2308.16245) | 本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。 |
| [^64] | [Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.16198) | 本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。 |
| [^65] | [RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation.](http://arxiv.org/abs/2308.15975) | RoboTAP通过利用稠密追踪技术实现了快速、普适的学习，可以从短时间内收集的演示中解决复杂的物体排列任务。 |
| [^66] | [WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model.](http://arxiv.org/abs/2308.15962) | 本文研究了将大型语言模型与视觉定位和机器人抓取系统集成，通过使用WALL-E实现了在餐厅场景中提高人机交互准确性和效率的目标。通过实验和评估，证明了这种集成可以使WALL-E成为一位更有能力和智能的机器人服务员。 |
| [^67] | [CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts.](http://arxiv.org/abs/2308.15690) | 提出了一个用于大豆芽图像处理的名为CongNaMul的数据集，旨在支持图像分类、语义分割、分解和测量等任务。提供了质量分类、语义分割和图像分解的标记，以及5个芽的物理特征供测量使用。 |
| [^68] | [DeepHealthNet: Adolescent Obesity Prediction System Based on a Deep Learning Framework.](http://arxiv.org/abs/2308.14657) | 本研究提出了一种基于深度学习框架的青少年肥胖预测系统DeepHealthNet，通过收集青少年的健康数据集，利用数据增强技术训练模型，提供个性化预测，并帮助青少年做出明智的健康决策。 |
| [^69] | [Hypergraph Structure Inference From Data Under Smoothness Prior.](http://arxiv.org/abs/2308.14172) | 本文提出了一种光滑性先验方法，用于从节点特征中推断超图的结构，并捕捉数据内在的关系。该方法不需要标记数据作为监督，能够推断出每个潜在超边的概率。 |
| [^70] | [Exploring Large Language Models for Knowledge Graph Completion.](http://arxiv.org/abs/2308.13916) | 本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。 |
| [^71] | [Stochastic Configuration Machines for Industrial Artificial Intelligence.](http://arxiv.org/abs/2308.13570) | 本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。 |
| [^72] | [RemovalNet: DNN Fingerprint Removal Attacks.](http://arxiv.org/abs/2308.12319) | 本论文对DNN指纹去除攻击进行了全面的调查，并提出了一种名为RemovalNet的攻击方法，通过min-max双层优化来逃避模型所有权验证。攻击方法旨在去除指纹特定知识，并提取受害模型的通用语义知识来维持替代模型性能。 |
| [^73] | [xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium.](http://arxiv.org/abs/2308.11155) | 在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。 |
| [^74] | [RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition.](http://arxiv.org/abs/2308.11029) | 提出了RBA-GCN模型用于情感识别。该模型通过引入关系双层聚合和图生成模块，解决了GCN模型中的节点信息冗余和远距离上下文信息捕获问题。 |
| [^75] | [DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering.](http://arxiv.org/abs/2308.10959) | 本文提出了一个名为DocPrompt的方法，用于处理文档问答任务，具有强大的零样本和少样本性能。实验结果表明，DocPrompt模型经过连续预训练后在文档问答任务中表现优异，大大提高了交付效率和模型性能，降低了注释成本和劳动成本。 |
| [^76] | [Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans.](http://arxiv.org/abs/2308.07462) | 这篇论文比较了ChatGPT和人类在词汇和词汇丰富度方面的差异，研究发现使用ChatGPT等工具会对词汇使用和词汇丰富度产生影响，这可能会对语言演变产生影响。 |
| [^77] | [Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI.](http://arxiv.org/abs/2308.05962) | 本文探讨了基于基金会模型的人工智能系统在整个生命周期中所面临的治理挑战，并提出了利用区块链实现去中心化治理的架构。 |
| [^78] | ["It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models.](http://arxiv.org/abs/2307.10811) | 通过三节次的定性研究，探究了人类与大型语言模型在预写过程中的合作模式，并发现了一个三阶段的人机共创过程：构思、启发和实施。在这个合作过程中，人类扮演着主导角色。 |
| [^79] | [Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning.](http://arxiv.org/abs/2307.04726) | 该论文介绍了一种名为状态重构扩散策略 (SRDP) 的新方法，该方法在最新的扩散策略类中引入了状态重构特征学习，以解决脱机强化学习中的分布偏移和有效表示策略的问题。 |
| [^80] | [Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution.](http://arxiv.org/abs/2307.00925) | 本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。 |
| [^81] | [Neural Mixed Effects for Nonlinear Personalized Predictions.](http://arxiv.org/abs/2306.08149) | 本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。 |
| [^82] | [Improving the Validity of Decision Trees as Explanations.](http://arxiv.org/abs/2306.06777) | 该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。 |
| [^83] | [Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach.](http://arxiv.org/abs/2306.03604) | 本论文提出一种强化学习的中介模型，可实现代理与LLM之间高效经济有效的互动，提高效率和成本效益。 |
| [^84] | [Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks.](http://arxiv.org/abs/2305.19979) | 该论文研究了在生物医学领域中知识图谱嵌入的有效性和限制，通过在生物医学知识图谱上的实验，证明了最先进的模型在性能和下游用途方面的优势，同时提供了可解释的预测结果。 |
| [^85] | [Is ChatGPT the Ultimate Programming Assistant -- How far is it?.](http://arxiv.org/abs/2304.11938) | ChatGPT作为编程助手在处理常见编程问题方面表现出色，但也存在一些局限性。 |
| [^86] | [OLISIA: a Cascade System for Spoken Dialogue State Tracking.](http://arxiv.org/abs/2304.11073) | 我们提出了OLISIA，一个口语对话状态跟踪的级联系统，使用自动语音识别和DST模型，采用几个适应性策略来提高稳健性，并在DSTC11 Track3中取得第一名的好成绩。 |
| [^87] | [G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System.](http://arxiv.org/abs/2304.01559) | G2PTL是一种面向物流领域的预训练模型，结合了文本预训练的语义学习能力和图建模的地理关系编码能力，能有效地编码交付地址中的位置信息，并在物流系统中具有广泛的应用前景。 |
| [^88] | [Knowledge Enhanced Graph Neural Networks.](http://arxiv.org/abs/2303.15487) | KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。 |
| [^89] | [DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion.](http://arxiv.org/abs/2303.12743) | 该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。 |
| [^90] | [Sensitivity-Aware Visual Parameter-Efficient Tuning.](http://arxiv.org/abs/2303.08566) | 本文提出了敏感度感知的视觉参数低效调整（SPT）方案，可以自适应地将可训练参数分配到任务特定的重要位置，以提高表示能力，适应预训练视觉模型到下游任务。 |
| [^91] | [Performance Limits of a Deep Learning-Enabled Text Semantic Communication under Interference.](http://arxiv.org/abs/2302.14702) | 这项研究探讨了在干扰环境下深度学习的文本语义通信系统的性能限制，发现当干扰信号强度增大时，该系统会产生语义上无关的句子。 |
| [^92] | [Fair Attribute Completion on Graph with Missing Attributes.](http://arxiv.org/abs/2302.12977) | 本文提出一种公平属性补全方法FairAC，用于处理具有缺失属性的图数据中的不公平性问题。FairAC采用注意机制处理属性缺失问题，并减轻属性和补全导致的两种不公平性，即属性不公平和拓扑不公平。 |
| [^93] | [System identification of neural systems: If we got it right, would we know?.](http://arxiv.org/abs/2302.06677) | 该论文研究了神经系统的系统辨识问题，通过对比人工神经网络与生物神经元的记录来验证模型的有效性。然而，系统辨识的性能很大程度上取决于刺激图像等因素，并且对识别更高级别架构图案方面存在局限性。 |
| [^94] | [Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications.](http://arxiv.org/abs/2301.00752) | 本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。 |
| [^95] | [Deanthropomorphising NLP: Can a Language Model Be Conscious?.](http://arxiv.org/abs/2211.11483) | 本文讨论了关于使用Transformer架构的预训练语言模型LaMDA是否具有意识的说法。作者认为语言模型不可能具有意识，而LaMDA没有比其他类似模型更具先进性。 |
| [^96] | [Learning Melanocytic Cell Masks from Adjacent Stained Tissue.](http://arxiv.org/abs/2211.00646) | 本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。 |
| [^97] | [Hypernetwork approach to Bayesian MAML.](http://arxiv.org/abs/2210.02796) | 该论文提出了一种名为贝叶斯HMAML的新框架，利用超网络进行权重更新，以解决MAML的过拟合和不确定性问题。 |
| [^98] | [Visual correspondence-based explanations improve AI robustness and human-AI team accuracy.](http://arxiv.org/abs/2208.00780) | 该论文提出了基于视觉对应的解释方法，用于改善AI的鲁棒性和人机团队的准确性。在大规模的人类研究中，该方法被发现比kNN解释更有用，帮助用户更准确地拒绝AI的错误决策。同时，该方法在超出分布数据集上改进了性能，并实现了互补人机团队准确性的可能性。 |
| [^99] | [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models.](http://arxiv.org/abs/2202.04053) | 本研究探究了不同文本到图像模型的视觉推理能力和社会偏见。尽管模型在图像生成方面表现出高质量的结果，但在对象计数和空间关系理解能力方面仍存在与上界准确性之间的巨大差距。此外，我们还评估了模型在性别和肤色方面的偏见。 |
| [^100] | [Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs.](http://arxiv.org/abs/2106.14052) | 该论文研究了将归纳推理和演绎推理相结合应用于不完整知识图谱上的查询回答。通过将本体论纳入基于嵌入的查询回答模型，采用不同的集成策略和损失函数调整，取得了20%到的性能提升。 |
| [^101] | [Learning Optimal Strategies for Temporal Tasks in Stochastic Games.](http://arxiv.org/abs/2102.04307) | 本论文提出了一种无模型强化学习方法，用于从给定的LTL规范中学习最优控制策略，即使环境完全未知。该方法通过将问题建模为控制器和对抗环境之间的随机博弈，最大化满足LTL规范的概率，抵抗最坏情况下的环境行为。 |

# 详细

[^1]: StyleInV:一种用于无条件视频生成的时间风格调制逆向网络

    StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation. (arXiv:2308.16909v1 [cs.CV])

    [http://arxiv.org/abs/2308.16909](http://arxiv.org/abs/2308.16909)

    这项研究介绍了一个用于无条件视频生成的新型运动生成器设计，利用基于学习的反向网络实现了运动连贯性和生成空间的约束。

    

    无条件视频生成是一项具有挑战性的任务，需要合成既连贯又持续时间较长的高质量视频。为了解决这个挑战，研究者们使用预训练的StyleGAN图像生成器进行高质量帧合成，并专注于运动生成器的设计。运动生成器通过使用重型三维卷积辨别器进行迭代训练，以确保视频生成过程中的运动连贯性。在本文中，我们引入了一种新颖的运动生成器设计，它使用了基于学习的反向网络来实现GAN。我们的方法中的编码器从编码图像到潜变量中捕捉到了丰富而平滑的先验知识，并且在给定初始生成帧的潜变量作为指导的情况下，通过在时间上调制反向编码器，我们的方法可以生成平滑的未来潜变量。我们的方法具有稀疏训练的优势，并且通过初始帧引导的反向网络有效约束了运动生成器的生成空间。

    Unconditional video generation is a challenging task that involves synthesizing high-quality videos that are both coherent and of extended duration. To address this challenge, researchers have used pretrained StyleGAN image generators for high-quality frame synthesis and focused on motion generator design. The motion generator is trained in an autoregressive manner using heavy 3D convolutional discriminators to ensure motion coherence during video generation. In this paper, we introduce a novel motion generator design that uses a learning-based inversion network for GAN. The encoder in our method captures rich and smooth priors from encoding images to latents, and given the latent of an initially generated frame as guidance, our method can generate smooth future latent by modulating the inversion encoder temporally. Our method enjoys the advantage of sparse training and naturally constrains the generation space of our motion generator with the inversion network guided by the initial fr
    
[^2]: InterDiff: 使用物理信息扩散生成三维人物-物体交互

    InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion. (arXiv:2308.16905v1 [cs.CV])

    [http://arxiv.org/abs/2308.16905](http://arxiv.org/abs/2308.16905)

    本文提出了InterDiff，一种使用物理信息扩散生成三维人物-物体交互的框架。该框架通过两个关键步骤：交互扩散和交互校正，有效地模拟了具有不同形状的动态物体和全身运动的物体交互。实验证明了该方法的有效性。

    

    本文提出了一种新颖的任务，即预测三维人物-物体交互（HOIs）。大多数现有的HOI合成研究缺乏与动态物体的全身交互，例如，往往局限于操纵小型或静态物体。我们的任务更具挑战性，因为它需要对具有不同形状的动态物体进行建模，捕捉整体运动，并确保物理有效的交互。为此，我们提出了InterDiff，一个框架，包括两个关键步骤：（i）交互扩散，我们利用扩散模型对未来的人物-物体交互分布进行编码；（ii）交互校正，在扩散步骤中引入物理信息的预测器来纠正去噪的HOIs。我们的关键见解是注入先验知识，即与接触点相关的交互遵循简单的模式，并且易于预测。在多个人物-物体交互数据集上的实验证明了该方法的有效性。

    This paper addresses a novel task of anticipating 3D human-object interactions (HOIs). Most existing research on HOI synthesis lacks comprehensive whole-body interactions with dynamic objects, e.g., often limited to manipulating small or static objects. Our task is significantly more challenging, as it requires modeling dynamic objects with various shapes, capturing whole-body motion, and ensuring physically valid interactions. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to encode the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor to correct denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable. Experiments on multiple human-object interaction datasets demonstrate the effec
    
[^3]: Transformers作为支持向量机

    Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])

    [http://arxiv.org/abs/2308.16898](http://arxiv.org/abs/2308.16898)

    这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。

    

    自从"Attention Is All You Need"中引入转换器架构以来，它在自然语言处理领域取得了革命性的进展。转换器中的注意力层接受输入令牌序列$X$并通过计算softmax$(XQK^\top X^\top)$的成对相似性使它们相互作用，其中$(K,Q)$是可训练的键-查询参数。在这项工作中，我们建立了自注意力优化几何和一个硬间隔支持向量机问题之间的正式等价关系，通过对令牌对的外积施加线性约束，将最佳输入令牌与非最佳令牌分离。这个形式主义使我们能够表征梯度下降优化的单层转换器的隐式偏差：(1)优化注意力层，使用可变正则化参数$(K,Q)$，收敛的方向是一个最小化综合参数$W=KQ^\top$的核范数的支持向量机解决方案。而直接使用$W$进行参数化则最小化一个Frobenius范数目标。

    Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
    
[^4]: PointOcc: 基于点云的三透视圆柱视图用于点云三维语义占据预测

    PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction. (arXiv:2308.16896v1 [cs.CV])

    [http://arxiv.org/abs/2308.16896](http://arxiv.org/abs/2308.16896)

    本文提出了一个新的三透视圆柱视图表示方法，在自动驾驶中进行点云三维语义占据预测。通过构建柱坐标系下的三透视圆柱视图，实现了对点云的精细建模，同时利用空间组池化和二维主干网络高效处理数据。结果表明，该方法可以有效地预测点云的语义占据情况。

    

    自动驾驶中的语义分割正从稀疏点分割发展到密集体素分割，目标是预测所关注的三维空间中每个体素的语义占据情况。现有的基于二维投影的方法（如鸟瞰图、距离视图等）因为预测空间的密集性而无效，因为它们只能描述三维场景的子空间。为了解决这个问题，我们提出了一个三透视圆柱视图来有效而全面地表示点云，并提出了一个点云模型 PointOcc 来高效地处理它们。考虑到激光雷达点云的距离分布，我们利用柱坐标系构建了三透视圆柱视图，以更细粒度地对近区域进行建模。我们采用空间组池化来保留投影过程中的结构细节，并采用二维主干网路高效处理每个透视面。最后，我们通过聚合的方式获得每个点的特征。

    Semantic segmentation in autonomous driving has been undergoing an evolution from sparse point segmentation to dense voxel segmentation, where the objective is to predict the semantic occupancy of each voxel in the concerned 3D space. The dense nature of the prediction space has rendered existing efficient 2D-projection-based methods (e.g., bird's eye view, range view, etc.) ineffective, as they can only describe a subspace of the 3D scene. To address this, we propose a cylindrical tri-perspective view to represent point clouds effectively and comprehensively and a PointOcc model to process them efficiently. Considering the distance distribution of LiDAR point clouds, we construct the tri-perspective view in the cylindrical coordinate system for more fine-grained modeling of nearer areas. We employ spatial group pooling to maintain structural details during projection and adopt 2D backbones to efficiently process each TPV plane. Finally, we obtain the features of each point by aggregat
    
[^5]: 语言条件路径规划

    Language-Conditioned Path Planning. (arXiv:2308.16893v1 [cs.RO])

    [http://arxiv.org/abs/2308.16893](http://arxiv.org/abs/2308.16893)

    本研究提出了一种语言条件路径规划的方法，通过学习碰撞函数，预测机器人与环境之间的碰撞，实现灵活、有条件的路径规划，无需手动标注或者真实物体模型。

    

    接触是机器人操作的核心。有时候，我们希望使用接触（例如操纵和抓取），而有时候，接触是有害的（例如避免障碍物）。然而，传统的路径规划算法只关注于无碰撞路径，限制了它们在接触丰富任务中的适用性。为了解决这个问题，我们提出了语言条件路径规划的领域，将接触感知性融入到路径规划问题中。作为该领域的第一步，我们提出了语言条件碰撞函数（LACO），这是一种新颖的方法，只使用单视图图像、语言提示和机器人配置来学习碰撞函数。LACO可以预测机器人和环境之间的碰撞，从而实现灵活、有条件的路径规划，无需手动对象标注、点云数据或真实物体模型。在仿真和实际环境中，我们证明LACO可以实现复杂而细致的路径规划。

    Contact is at the core of robotic manipulation. At times, it is desired (e.g. manipulation and grasping), and at times, it is harmful (e.g. when avoiding obstacles). However, traditional path planning algorithms focus solely on collision-free paths, limiting their applicability in contact-rich tasks. To address this limitation, we propose the domain of Language-Conditioned Path Planning, where contact-awareness is incorporated into the path planning problem. As a first step in this domain, we propose Language-Conditioned Collision Functions (LACO) a novel approach that learns a collision function using only a single-view image, language prompt, and robot configuration. LACO predicts collisions between the robot and the environment, enabling flexible, conditional path planning without the need for manual object annotations, point cloud data, or ground-truth object meshes. In both simulation and the real world, we demonstrate that LACO can facilitate complex, nuanced path plans that allo
    
[^6]: ReZero：可定制区域声音提取

    ReZero: Region-customizable Sound Extraction. (arXiv:2308.16892v1 [eess.AS])

    [http://arxiv.org/abs/2308.16892](http://arxiv.org/abs/2308.16892)

    ReZero是一个可定制的区域声音提取框架，针对R-SE任务设计了不同类型的空间区域的定义、空间特征提取和聚合方法以及多通道扩展的BSRNN模型，通过实验证明了其有效性。

    

    我们引入了ReZero，一个通用且灵活的多通道区域声音提取（R-SE）任务的框架。R-SE任务旨在在特定的用户定义的空间区域内提取所有活动的目标声音（例如人类语音），这与通常假设盲分离或固定预定义空间区域的传统任务不同。空间区域可以定义为角度窗口、球体、锥体或其他几何模式。作为R-SE任务的解决方案，提出的ReZero框架包括（1）不同类型空间区域的定义，（2）空间特征提取和聚合的方法，以及（3）适用于R-SE任务的带分割RNN（BSRNN）模型的多通道扩展。我们设计了不同的麦克风阵列几何形状的实验，不同类型的空间区域和关于不同系统配置的综合消融研究。

    We introduce region-customizable sound extraction (ReZero), a general and flexible framework for the multi-channel region-wise sound extraction (R-SE) task. R-SE task aims at extracting all active target sounds (e.g., human speech) within a specific, user-defined spatial region, which is different from conventional and existing tasks where a blind separation or a fixed, predefined spatial region are typically assumed. The spatial region can be defined as an angular window, a sphere, a cone, or other geometric patterns. Being a solution to the R-SE task, the proposed ReZero framework includes (1) definitions of different types of spatial regions, (2) methods for region feature extraction and aggregation, and (3) a multi-channel extension of the band-split RNN (BSRNN) model specified for the R-SE task. We design experiments for different microphone array geometries, different types of spatial regions, and comprehensive ablation studies on different system configurations. Experimental res
    
[^7]: Belebele基准数据集：122种语言变体的并行阅读理解数据集

    The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])

    [http://arxiv.org/abs/2308.16884](http://arxiv.org/abs/2308.16884)

    Belebele是一个包含122种语言变体的多选机器阅读理解数据集，可用于评估文本模型在高、中和低资源语言中的性能。尽管英语为中心的大型语言模型在跨语言转移方面表现良好，但小型多语言遮蔽语言模型在其他语言上表现更佳。

    

    我们提出了Belebele，一个包含122种语言变体的多选机器阅读理解（MRC）数据集。该数据集极大地扩展了自然语言理解（NLU）基准的语言覆盖范围，使得可以评估文本模型在高、中和低资源语言中的性能。每个问题都基于Flores-200数据集中的一个短篇文章，并提供了四个多选答案。问题经过精心策划，以区分具有不同通用语言理解水平的模型。单独的英语数据集已经足够困难，可以挑战最先进的语言模型。由于完全并行，该数据集可以直接比较所有语言的模型性能。我们使用该数据集评估多语言遮蔽语言模型（MLMs）和大型语言模型（LLMs）的能力。我们展示了广泛的结果，并发现尽管英语为中心的LLMs之间存在显著的跨语言转移，但小型MLMs在其他语言上的表现相对较好。

    We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
    
[^8]: 适应速度分析公平感知因果模型

    Adaptation Speed Analysis for Fairness-aware Causal Models. (arXiv:2308.16879v1 [cs.AI])

    [http://arxiv.org/abs/2308.16879](http://arxiv.org/abs/2308.16879)

    本研究考虑了适应速度分析公平感知因果模型的问题，通过研究带有因果-偏差-效果结构的简单结构因果模型，探讨了在训练过程中考虑敏感变量（偏差）的公平性和适应速率的相关因素。

    

    举例如机器翻译任务，在两种语言之间实现双向翻译时，通常将源语料库用作目标语料库，这涉及到训练两个具有相反方向的模型。在许多领域中，哪个模型可以最快地适应领域转变的问题具有重要意义。具体而言，考虑一个由于未知干预而改变的原始分布p，导致出现修改后的分布p*。在将p与p*对齐时，有多种因素可能影响适应速率，包括p中变量之间的因果依赖关系。然而，在实际场景中，我们必须考虑训练过程的公平性，并且在因果关系中涉及到一个敏感变量（偏差）在原因变量和效果变量之间是尤为关键的。为了探索这种情况，我们研究了一个带有因果-偏差-效果结构的简单结构因果模型（SCM)，其中变量A在原因（X）和效果（Y）之间充当敏感变量。

    For example, in machine translation tasks, to achieve bidirectional translation between two languages, the source corpus is often used as the target corpus, which involves the training of two models with opposite directions. The question of which one can adapt most quickly to a domain shift is of significant importance in many fields. Specifically, consider an original distribution p that changes due to an unknown intervention, resulting in a modified distribution p*. In aligning p with p*, several factors can affect the adaptation rate, including the causal dependencies between variables in p. In real-life scenarios, however, we have to consider the fairness of the training process, and it is particularly crucial to involve a sensitive variable (bias) present between a cause and an effect variable. To explore this scenario, we examine a simple structural causal model (SCM) with a cause-bias-effect structure, where variable A acts as a sensitive variable between cause (X) and effect (Y
    
[^9]: The Gender-GAP Pipeline: 一个用于55种语言中性别表征的性别感知多语言流水线

    The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages. (arXiv:2308.16871v1 [cs.CL])

    [http://arxiv.org/abs/2308.16871](http://arxiv.org/abs/2308.16871)

    本文介绍了Gender-GAP Pipeline，一个用于55种语言中性别表征的自动流水线，通过使用多语言性别人称名词词汇表对文本进行量化来报告数据中的性别表征。在WMT训练数据和新闻任务的开发数据中表明当前数据偏向男性表征。

    

    语言生成系统中的性别偏见很难被缓解。其中一个可能导致这种偏见的原因是训练和评估数据中的性别表征不平衡。尽管最近在记录这个问题和试图缓解它方面取得了一些进展，但我们仍然缺乏共享的方法论和工具，以报告大规模数据集中的性别表征。这种定量报告将使进一步缓解成为可能，例如通过数据增强。本文描述了Gender-GAP Pipeline（用于性别感知的多语言流水线），它是一个自动流程，用于对55种语言的大规模数据集进行性别表征。该流水线使用一个多语言性别人称名词词汇表来量化文本中的性别表征。我们展示了它来报告WMT训练数据和新闻任务的开发数据中的性别表征，证实当前数据偏向男性表征。拥有不平衡的数据集可能会间接地优化我们的系统。

    Gender biases in language generation systems are challenging to mitigate. One possible source for these biases is gender representation disparities in the training and evaluation data. Despite recent progress in documenting this problem and many attempts at mitigating it, we still lack shared methodology and tooling to report gender representation in large datasets. Such quantitative reporting will enable further mitigation, e.g., via data augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware Polyglot Pipeline), an automatic pipeline to characterize gender representation in large-scale datasets for 55 languages. The pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text. We showcase it to report gender representation in WMT training data and development data for the News task, confirming that current data is skewed towards masculine representation. Having unbalanced datasets may indirectly optimize our systems 
    
[^10]: 通过知识共享和个性化学习驾驶员模型以用于自动驾驶车辆

    Learning Driver Models for Automated Vehicles via Knowledge Sharing and Personalization. (arXiv:2308.16870v1 [cs.RO])

    [http://arxiv.org/abs/2308.16870](http://arxiv.org/abs/2308.16870)

    本文提出了一种通过知识共享和个性化学习驾驶员模型的方法，以用于自动驾驶车辆。通过跨车辆共享知识，增加AVs在真实驾驶情景中的暴露，同时保留个性化的模型，以适应不同车辆的条件和特性。

    

    本文描述了一种通过车辆之间的知识共享和个性化学习自动驾驶车辆（AVs）驾驶员模型的框架。交通运输系统中的固有变异使得将AVs暴露于所有可能的驾驶情景在实证实验或测试中极具挑战性。因此，AVs可能会对某些被认为对其安全和高效运行有害的遭遇视而不见。因此，跨AVs共享知识，增加其在真实世界中驾驶情景的暴露，就显得至关重要。本文探索了一种方法，通过共享知识和借用多个车辆之间的强度，协同培训驾驶员模型，同时保留适应车辆的独特条件和特性的个性化模型。我们的模型采用联邦学习方法，在多个车辆之间进行协作，同时避免了它们之间共享原始数据的需要。我们展示了我们的方法在实验中的性能。

    This paper describes a framework for learning Automated Vehicles (AVs) driver models via knowledge sharing between vehicles and personalization. The innate variability in the transportation system makes it exceptionally challenging to expose AVs to all possible driving scenarios during empirical experimentation or testing. Consequently, AVs could be blind to certain encounters that are deemed detrimental to their safe and efficient operation. It is then critical to share knowledge across AVs that increase exposure to driving scenarios occurring in the real world. This paper explores a method to collaboratively train a driver model by sharing knowledge and borrowing strength across vehicles while retaining a personalized model tailored to the vehicle's unique conditions and properties. Our model brings a federated learning approach to collaborate between multiple vehicles while circumventing the need to share raw data between them. We showcase our method's performance in experimental si
    
[^11]: 基于区块链的安全远程患者监测框架与神经刺激设备研究

    IoMT-Blockchain based Secured Remote Patient Monitoring Framework for Neuro-Stimulation Device. (arXiv:2308.16857v1 [cs.CR])

    [http://arxiv.org/abs/2308.16857](http://arxiv.org/abs/2308.16857)

    该论文探讨了一种基于区块链的安全远程患者监测框架，使用IoMT技术和神经刺激设备，实现了无侵入性的远程神经刺激系统。

    

    生物医学工程中的医疗物联网（IoMT）正在帮助改善医疗行业中电子设备的准确性、可靠性和生产力。通过穿戴式IoMT设备（如具备多种功能的神经刺激设备）的快速开发，可以提供患者的实时感知数据，并进行后续分析。物联网数据被收集、分析和存储在一个地方。然而，由于集中化可能会出现单点故障、数据篡改、隐私问题等挑战。由于其分散化的特性，区块链（BC）可以缓解这些问题。本文研究了使用基于IoMT的经颅直流电刺激（tDCS）建立无侵入性远程神经刺激系统的可行性。已开发了一款基于硬件的tDCS原型设备，可以通过安卓应用程序在互联网上进行操作。我们提出的框架...

    Biomedical Engineering's Internet of Medical Things (IoMT) is helping to improve the accuracy, dependability, and productivity of electronic equipment in the healthcare business. Real-time sensory data from patients may be delivered and subsequently analyzed through rapid development of wearable IoMT devices, such as neuro-stimulation devices with a range of functions. Data from the Internet of Things is gathered, analyzed, and stored in a single location. However, single-point failure, data manipulation, privacy difficulties, and other challenges might arise as a result of centralization. Due to its decentralized nature, blockchain (BC) can alleviate these issues. The viability of establishing a non-invasive remote neurostimulation system employing IoMT-based transcranial Direct Current Stimulation is investigated in this work (tDCS). A hardware-based prototype tDCS device has been developed that can be operated over the internet using an android application. Our suggested framework a
    
[^12]: 通过BERT衍生的语义信息提升歌声合成的表达力

    Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information. (arXiv:2308.16836v1 [cs.SD])

    [http://arxiv.org/abs/2308.16836](http://arxiv.org/abs/2308.16836)

    本文通过使用BERT衍生的语义嵌入来提高歌声合成的表达力，首次引入歌词的文本表示作为附加输入，并通过额外的能量预测器和重新设计的音高预测器来增强歌声的表达力。

    

    本文提出了一种端到端的高质量歌声合成（SVS）系统，该系统使用来自Transformers（BERT）的双向编码器表示衍生的语义嵌入来提高合成歌声的表达力。基于最近提出的VISinger的主要架构，我们提出了几种特定的设计用于表达性歌声合成。首先，与以往的SVS模型不同，我们使用从预训练BERT中提取的歌词的文本表示作为模型的附加输入。这个表示包含了关于歌词语义的信息，可以帮助SVS系统产生更富有表现力和自然的声音。其次，我们进一步引入了一个能量预测器来稳定合成声音，并对能量变化的更广泛范围进行建模，这也有助于歌声的表达力。最后，为了减小音高问题，重新设计了音高预测器来预测实际到音符的音高。

    This paper presents an end-to-end high-quality singing voice synthesis (SVS) system that uses bidirectional encoder representation from Transformers (BERT) derived semantic embeddings to improve the expressiveness of the synthesized singing voice. Based on the main architecture of recently proposed VISinger, we put forward several specific designs for expressive singing voice synthesis. First, different from the previous SVS models, we use text representation of lyrics extracted from pre-trained BERT as additional input to the model. The representation contains information about semantics of the lyrics, which could help SVS system produce more expressive and natural voice. Second, we further introduce an energy predictor to stabilize the synthesized voice and model the wider range of energy variations that also contribute to the expressiveness of singing voice. Last but not the least, to attenuate the off-key issues, the pitch predictor is re-designed to predict the real to note pitch 
    
[^13]: 编程语言能通过指令调优相互提升吗？

    Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])

    [http://arxiv.org/abs/2308.16824](http://arxiv.org/abs/2308.16824)

    研究发现，编程语言可以在指令调优阶段相互促进，并显著提高彼此的能力。

    

    当人类程序员掌握了一种编程语言后，学习一种新的编程语言会更容易。在本报告中，我们重点探讨了在代码大规模语言模型的指令微调阶段中，编程语言是否能够通过相互提升来增强彼此的能力。我们在StarCoder上对8种流行的编程语言进行了广泛的实验（Python，JavaScript，TypeScript，C，C ++，Java，Go，HTML）。结果表明，编程语言可以显著提高彼此的能力。例如，通过在Python上训练的CodeM-Python 15B可以使Java的pass@1率绝对增加了17.95％。更令人惊讶的是，我们发现通过在HTML语料库上训练的CodeM-HTML 7B可以使Java的pass@1率绝对增加了15.24％。我们的训练数据已经发布在https://github.com/NL2Code/CodeM上。

    When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.
    
[^14]: 针对层级数据集的潜在变量多输出高斯过程

    Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets. (arXiv:2308.16822v1 [cs.LG])

    [http://arxiv.org/abs/2308.16822](http://arxiv.org/abs/2308.16822)

    本文提出了一种针对层级数据集的扩展多输出高斯过程 (MOGPs)，通过引入潜在变量和特定核函数，可以更好地捕捉不同输出之间的关系和相关性。这种方法预计能够在任务数量增加时提高可扩展性。

    

    多输出高斯过程（MOGPs）已经被引入，通过利用不同输出之间的相关性来处理多个任务。通常，MOGPs模型假设输出之间存在平坦的相关结构。然而，这种公式并不能考虑更复杂的关系，例如，如果每个输出都有多个重复观察值（这是生物实验中的典型设置）。本文提出了针对层级数据集的MOGPs扩展（即可以在树状结构中表示观测之间的关系的数据集）。我们的模型定义了一个定制的核函数，考虑了数据中的层级结构，以捕捉不同层次的相关性，同时引入潜在变量来通过专用核函数表示输出之间的潜在依赖关系。预计这个特性能够在任务数量增加时显著提高可扩展性。

    Multi-output Gaussian processes (MOGPs) have been introduced to deal with multiple tasks by exploiting the correlations between different outputs. Generally, MOGPs models assume a flat correlation structure between the outputs. However, such a formulation does not account for more elaborate relationships, for instance, if several replicates were observed for each output (which is a typical setting in biological experiments). This paper proposes an extension of MOGPs for hierarchical datasets (i.e. datasets for which the relationships between observations can be represented within a tree structure). Our model defines a tailored kernel function accounting for hierarchical structures in the data to capture different levels of correlations while leveraging the introduction of latent variables to express the underlying dependencies between outputs through a dedicated kernel. This latter feature is expected to significantly improve scalability as the number of tasks increases. An extensive e
    
[^15]: 基于异步时空图卷积网络的不规则交通时间序列预测

    Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])

    [http://arxiv.org/abs/2308.16818](http://arxiv.org/abs/2308.16818)

    该论文提出了一种基于异步时空图卷积网络的不规则交通时间序列预测方法，用于解决智能交叉口产生的异步空间依赖、不规则时间依赖和可变长度序列预测等挑战。

    

    准确预测智能交通信号控制系统中受智能交叉口控制的交叉口的交通流量对于提升交通出行效率至关重要。然而，由于智能交叉口产生的交通时间序列不规则，交通流量预测任务变得更加困难，并且面临三个主要挑战：1）异步的空间依赖性，2）交通数据的不规则时间依赖性，3) 需要预测的可变长度序列，严重影响了当前交通流量预测方法的性能。为此，我们提出了一种异步时空图卷积网络(ASeer)来预测智能交叉口进入车道的交通状态。具体而言，通过在交通扩散图上连接车道，我们首先提出了一种异步图扩散网络来模拟车道的异步空间依赖性。

    Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
    
[^16]: 图神经网络中的等级崩塌导致平滑过度和关联过高

    Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])

    [http://arxiv.org/abs/2308.16800](http://arxiv.org/abs/2308.16800)

    本文研究了图神经网络中的平滑过度和特征关联过高现象，发现固定不变的子空间导致了节点表示的等级崩塌。在该子空间中平滑向量的存在导致过度平滑，即使避免过度平滑也会导致过高的关联。为了解决这个问题，我们提出了一种克罗内克积之和作为一种有效方法。

    

    我们的研究揭示了深度图神经网络中平滑过度和特征关联过高的新理论见解。我们展示了固定不变子空间的普遍存在，它表现出一种相对的行为，不受特征转换的影响。我们的工作阐明了与收敛到常数状态和节点状态的过分分离相关的最新观察结果，因为子空间的放大只取决于聚合函数的频谱。在线性场景中，这导致节点表示由低维子空间主导，并且具有与特征转换无关的渐近收敛速率。当平滑向量跨越这个子空间时，这会导致节点表示的等级崩塌，从而导致过度平滑，即使避免过度平滑也会导致过高的关联。在我们的理论指导下，我们提出了一种克罗内克积之和作为一种有益特性，可以可靠地防止过度平滑、过高关联和等级崩塌。

    Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
    
[^17]: 代理团队情景感知（ATSA）：人工智能与人类联合工作的情景感知框架

    Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming. (arXiv:2308.16785v1 [cs.AI])

    [http://arxiv.org/abs/2308.16785](http://arxiv.org/abs/2308.16785)

    代理团队情景感知（ATSA）是一个基于人工智能和人类联合工作的情景感知框架，旨在提高混合团队的绩效。该框架结合了个体和团队的情景感知模型，并涉及双向和动态的交互。

    

    人工智能的快速发展导致了人工智能与人类联合工作在各个领域的日益增长。随着机器从仅仅自动化到自主状态的演进，它们越来越展现出意外的行为和类似人类的认知/智能能力，包括情景感知。这种转变有潜力提高混合人工智能团队的绩效，强调了我们对人与机器之间动态情景感知相互作用的更好理解的需要。为此，我们对主流情景感知理论模型进行了回顾，并基于人工智能与人类联合工作的关键特征和过程，提出了一个新的情景感知框架。代理团队情景感知（ATSA）框架统一了人类和人工智能的行为，并涉及双向和动态的交互。该框架基于个体和团队的情景感知模型，并阐述了为建模人工智能与人类联合工作而细化的认知机制。类似的知觉循环

    The rapid advancements in artificial intelligence (AI) have led to a growing trend of human-AI teaming (HAT) in various fields. As machines continue to evolve from mere automation to a state of autonomy, they are increasingly exhibiting unexpected behaviors and human-like cognitive/intelligent capabilities, including situation awareness (SA). This shift has the potential to enhance the performance of mixed human-AI teams over all-human teams, underscoring the need for a better understanding of the dynamic SA interactions between humans and machines. To this end, we provide a review of leading SA theoretical models and a new framework for SA in the HAT context based on the key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA) framework unifies human and AI behavior, and involves bidirectional, and dynamic interaction. The framework is based on the individual and team SA models and elaborates on the cognitive mechanisms for modeling HAT. Similar perceptual cycle
    
[^18]: StratMed：面向低资源药物推荐的相关性分层方法

    StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])

    [http://arxiv.org/abs/2308.16781](http://arxiv.org/abs/2308.16781)

    StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。

    

    随着有限医疗资源与日益增长的需求之间的失衡，基于人工智能的临床任务变得至关重要。作为一个子领域，药物推荐旨在将患者的纵向历史与医学知识相结合，帮助医生更安全、更准确地开具药物组合处方。现有方法忽视了医疗数据中固有的长尾分布，缺乏头尾数据之间的平衡表示，导致模型性能次优。为了解决这个挑战，我们引入了StratMed，这是一个结合了创新的相关性分层机制的模型。它通过协调数据长尾分布中的差异，并在药物组合的安全性和准确性之间取得平衡。具体而言，我们首先使用深度学习网络构建预训练方法来获取实体表示。然后，我们设计了一个类似金字塔的数据分层方法，以获得更通用的实体表示。

    With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
    
[^19]: 基于神经预测的零样本NAS范式的有效性

    Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])

    [http://arxiv.org/abs/2308.16775](http://arxiv.org/abs/2308.16775)

    这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。

    

    在基于预测的神经架构搜索（NAS）中，通过图卷积网络得到的性能指标取得了显著的成功。然而，通过one-hot编码将前馈结构表示为组件图的这些指标面临一个限制：无法在不同的搜索空间中评估架构的性能。相反，手工性能指标（零样本NAS）可以在多个搜索空间中泛化，因为它们使用相同的架构和随机初始化。为了解决这个限制，我们提出了一种新的深度学习方法，用于零样本NAS。我们的方法采用傅里叶正弦和求和编码来进行卷积核的编码，从而构建了一个计算的前馈图，其结构类似于正在评估的架构。这些编码是可学习的，并提供了架构拓扑信息的全面视图。然后，伴随的多层感知器（MLP）对架构进行排序。

    In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
    
[^20]: 朝着工业控制系统低门槛的网络安全研究和教育发展

    Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems. (arXiv:2308.16769v1 [cs.CR])

    [http://arxiv.org/abs/2308.16769](http://arxiv.org/abs/2308.16769)

    该论文介绍了在工业控制系统网络安全领域进行低门槛研究和教育的重要性。通过利用开发的高保真模拟器，作者构建了一个集成框架用于自动发动网络攻击、收集数据、训练机器学习模型，并针对实际的化学和制造过程进行评估。作者还提出了一种名为MinTWin SVM的入侵检测模型，它结合了无监督机器学习和滑动窗口的方法。

    

    保护用于公共关键基础设施的工业控制系统(ICS)对于防止网络攻击造成灾难性的物理损害至关重要。研究界需要测试平台来验证和比较各种入侵检测算法以保护ICS。然而，由于昂贵的硬件、软件和操纵现实系统的固有危险性，ICS网络安全领域的研究和教育存在高门槛。为了弥补这一差距，我们在最近开发的三维高保真模拟器基础上，进一步展示了我们的集成框架，可以自动发动网络攻击、收集数据、训练机器学习模型，并针对实际的化学和制造过程进行评估。在我们的测试平台上，我们验证了我们提出的入侵检测模型，称为Minimal Threshold and Window SVM (MinTWin SVM)，它结合了一类SVM的无监督机器学习和滑动窗口

    The protection of Industrial Control Systems (ICS) that are employed in public critical infrastructures is of utmost importance due to catastrophic physical damages cyberattacks may cause. The research community requires testbeds for validation and comparing various intrusion detection algorithms to protect ICS. However, there exist high barriers to entry for research and education in the ICS cybersecurity domain due to expensive hardware, software, and inherent dangers of manipulating real-world systems. To close the gap, built upon recently developed 3D high-fidelity simulators, we further showcase our integrated framework to automatically launch cyberattacks, collect data, train machine learning models, and evaluate for practical chemical and manufacturing processes. On our testbed, we validate our proposed intrusion detection model called Minimal Threshold and Window SVM (MinTWin SVM) that utilizes unsupervised machine learning via a one-class SVM in combination with a sliding wind
    
[^21]: Ladder-of-Thought: 使用知识作为阶梯提升立场检测

    Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])

    [http://arxiv.org/abs/2308.16763](http://arxiv.org/abs/2308.16763)

    该论文介绍了一种名为“Ladder-of-Thought”的方法，通过引入外部知识来提升立场检测任务中的语言模型的性能，解决了小型模型在应用先前内部知识时性能提升不明显的问题，以及大规模模型在效率方面的挑战。

    

    思维链式提供（CoT）通过生成中间的推理来增强大型语言模型（LLM）的推理能力。然而，这些增强主要有益于大规模模型，在直接应用CoT时小型LLM的性能改进不明显。尽管LLM具有先进的推理能力，CoT主要依赖于其预先训练的内部知识，先前未知于模型的外部知识未被充分利用。在立场检测等任务中，外部背景知识起着关键作用，这种遗漏变得更加明显。此外，LLM的大规模架构在部署过程中不可避免地存在效率挑战。为了解决这些挑战，我们引入了用于立场检测的思维阶梯（LoT）。LoT基于双阶段级联优化框架，指导模型整合高质量的外部知识，增强中间步骤的性能。

    Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of Large Language Models (LLMs) through the generation of intermediate rationales. However, these enhancements predominantly benefit large-scale models, leaving small LMs without significant performance improvements when directly applying CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily on their pre-trained internal knowledge. The external knowledge that is previously unknown to the model remains unexploited. This omission becomes pronounced in tasks such as stance detection, where the external background knowledge plays a pivotal role. Additionally, the large-scale architecture of LLMs inevitably present efficiency challenges during deployment. To address these challenges, we introduce the Ladder-of-Thought (LoT) for stance detection. Grounded in a dual-phase Cascaded Optimization framework, LoT directs the model to incorporate high-quality external knowledge, enhancing the intermediat
    
[^22]: 基于LLM的上下文感知查询重写方法用于文本排名

    Context Aware Query Rewriting for Text Rankers using LLM. (arXiv:2308.16753v1 [cs.IR])

    [http://arxiv.org/abs/2308.16753](http://arxiv.org/abs/2308.16753)

    这项工作研究了使用基于LLM的上下文感知查询重写方法来提高文本排名任务。通过通过上下文感知提示来重写模糊的训练查询，克服了概念漂移和推理开销的固有局限性。

    

    查询重写是一类应用于不完全指定和模糊查询的方法，旨在克服文档排名中的词汇不匹配问题。查询通常在查询处理过程中进行重写，以便为下游排名器提供更好的查询建模。随着大语言模型（LLMs）的出现，已经开始研究使用生成方法生成伪文档来解决这种固有的词汇差距。在这项工作中，我们分析了LLMs在提高文本排名任务中查询重写的效用。我们发现使用LLMs作为查询重写器存在两个固有局限性--在仅使用查询作为提示时存在概念漂移，并且在查询处理过程中存在大量的推理开销。我们采用了一种简单但效果惊人的方法，称为上下文感知查询重写（CAR），以利用LLMs的优势进行查询理解。首先，我们通过上下文感知提示来重写模糊的训练查询，以在查询理解方面获得改进。

    Query rewriting refers to an established family of approaches that are applied to underspecified and ambiguous queries to overcome the vocabulary mismatch problem in document ranking. Queries are typically rewritten during query processing time for better query modelling for the downstream ranker. With the advent of large-language models (LLMs), there have been initial investigations into using generative approaches to generate pseudo documents to tackle this inherent vocabulary gap. In this work, we analyze the utility of LLMs for improved query rewriting for text ranking tasks. We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing. We adopt a simple, yet surprisingly effective, approach called context aware query rewriting (CAR) to leverage the benefits of LLMs for query understanding. Firstly, we rewrite ambiguous training queries by context-aware prompti
    
[^23]: Socratis：大型多模态模型是否具有情绪意识？

    Socratis: Are large multimodal models emotionally aware?. (arXiv:2308.16741v1 [cs.AI])

    [http://arxiv.org/abs/2308.16741](http://arxiv.org/abs/2308.16741)

    这项研究提出了Socratis，一个新的社会反应基准，用于学习多模态内容的多样化情绪反应。根据人类研究结果，人们更喜欢人工撰写的情感原因，比机器生成的要多2倍以上。

    

    现有的情绪预测基准包含粗糙的情绪标签，不考虑图像和文本在人类中引发多样化情绪的各种原因。学习多样化的对于多模态内容的反应非常重要，因为智能机器在生成和传递内容给社会中起到核心作用。为了填补这一空白，我们提出了Socratis，一个社会反应基准，在其中每个图像-标题（IC）对都附带有多种情绪和感受它们的原因的注释。Socratis包含了来自5个广泛阅读的新闻和图像标题（IC）数据集的2075个图像-标题对的980个情绪的18K个自由形式反应。我们评估了最先进的多模态大型语言模型在给定IC对的情感原因生成方面的能力。根据一个初步的人类研究，我们观察到，人们更喜欢人工撰写的原因，比机器生成的要多2倍以上。

    Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a \underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s} benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-genera
    
[^24]: 鲁棒的网络化联邦学习在定位中的应用

    Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])

    [http://arxiv.org/abs/2308.16737](http://arxiv.org/abs/2308.16737)

    本文提出了一种鲁棒的网络化联邦学习方法，通过采用$L_1$-范数鲁棒性和分布式次梯度框架，解决了在分布式环境中定位问题中的异常数据干扰和算法收敛挑战。

    

    本文解决了在数据分布在多设备上的联邦环境中，本质上是非凸非光滑的定位问题。由于联邦环境的分散性质，分布式学习成为可伸缩性和适应性的关键。此外，这些环境经常受到异常数据的干扰，使得传统方法在维护估计精度和确保算法收敛方面面临重大挑战。为了解决这些挑战，我们提出了一种采用分布式次梯度框架中$L_1$-范数鲁棒性的方法，专门设计用于处理这些障碍。我们的方法以原始形式解决问题，而不是采用迭代简化或近似方法，从而提高计算效率和估计精度。我们证明了我们的方法收敛到一个稳定点，突出了其有效性。

    This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effec
    
[^25]: 通过联邦学习和源目标远程梯度对齐实现部署后自适应

    Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment. (arXiv:2308.16735v1 [cs.CV])

    [http://arxiv.org/abs/2308.16735](http://arxiv.org/abs/2308.16735)

    本论文提出了一种名为FedPDA的新颖自适应框架，将联邦学习中学习远程数据的功能引入到PDA中，通过远程梯度交换使部署模型能够从源数据中获取信息，并针对目标领域进行优化。

    

    在医学影像中部署深度神经网络受到训练数据和部署后处理的数据之间的分布偏移的影响，导致性能下降。部署后自适应(PDA)通过使用有限的标记或完全无标记的目标数据，将预训练的部署模型适应到目标数据分布，来解决这个问题。联邦自适应(FedPDA)挑战了这个假设，并将联邦学习中学习远程数据的功能引入到PDA中。FedPDA通过远程梯度交换，使部署模型能够从源数据中获取信息，并针对目标领域进行优化。

    Deployment of Deep Neural Networks in medical imaging is hindered by distribution shift between training data and data processed after deployment, causing performance degradation. Post-Deployment Adaptation (PDA) addresses this by tailoring a pre-trained, deployed model to the target data distribution using limited labelled or entirely unlabelled target data, while assuming no access to source training data as they cannot be deployed with the model due to privacy concerns and their large size. This makes reliable adaptation challenging due to limited learning signal. This paper challenges this assumption and introduces FedPDA, a novel adaptation framework that brings the utility of learning from remote data from Federated Learning into PDA. FedPDA enables a deployed model to obtain information from source data via remote gradient exchange, while aiming to optimize the model specifically for the target domain. Tailored for FedPDA, we introduce a novel optimization method StarAlign (Sour
    
[^26]: 深度学习的证明：方法、挑战和未来方向

    Proof of Deep Learning: Approaches, Challenges, and Future Directions. (arXiv:2308.16730v1 [cs.CR])

    [http://arxiv.org/abs/2308.16730](http://arxiv.org/abs/2308.16730)

    深度学习模型的性能提升需要更多计算能力，而区块链中的工作量证明机制对计算能力提出了挑战。

    

    计算能力的提升为深度学习模型带来了前所未有的性能提升。随着越来越多的数据可用以及模型架构变得更加复杂，对更多计算能力的需求也在增加。另一方面，自比特币作为第一种加密货币的问世以及区块链概念作为分布式账本的建立以来，已经提出了许多变种和方法。然而，其中许多方法有一个共同点，即工作量证明（Proof of Work，PoW）共识机制。PoW主要用于支持新区块的生成过程。虽然PoW已经证明了其鲁棒性，但其主要缺点是需要大量的计算能力来维护区块链的安全和完整性。这是由于应用暴力破解来解决哈希难题。为了利用可用的计算能力进行有用而有意义的工作，同时保持区块链的安全，已经提出了许多技术，其中之一就是...

    The rise of computational power has led to unprecedented performance gains for deep learning models. As more data becomes available and model architectures become more complex, the need for more computational power increases. On the other hand, since the introduction of Bitcoin as the first cryptocurrency and the establishment of the concept of blockchain as a distributed ledger, many variants and approaches have been proposed. However, many of them have one thing in common, which is the Proof of Work (PoW) consensus mechanism. PoW is mainly used to support the process of new block generation. While PoW has proven its robustness, its main drawback is that it requires a significant amount of processing power to maintain the security and integrity of the blockchain. This is due to applying brute force to solve a hashing puzzle. To utilize the computational power available in useful and meaningful work while keeping the blockchain secure, many techniques have been proposed, one of which i
    
[^27]: 地形扩散网络：具有地质草图引导的气候感知地形生成

    Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance. (arXiv:2308.16725v1 [cs.CV])

    [http://arxiv.org/abs/2308.16725](http://arxiv.org/abs/2308.16725)

    提出了一种基于扩散的地形生成方法，名为地形扩散网络（TDN），该方法通过用户引导以实现更好的可控性，并考虑地形特征为虚拟环境生成逼真的地形。

    

    基于草图的地形生成旨在为虚拟环境创建逼真的景观，广泛应用于计算机游戏、动画和虚拟现实等领域。最近，基于深度学习的地形生成方法不断涌现，尤其是基于生成对抗网络（GAN）的方法。然而，这些方法常常难以满足用户灵活控制和保持生成多样性以实现逼真地形的要求。因此，我们提出了一种全新的基于扩散的方法，即地形扩散网络（TDN），它主动融合用户引导以增强可控性，并考虑到河流、山脊、盆地和山峰等地形特征。与传统的单一去噪方法不同，该方法提出了多层次的去噪方案，通过考虑细粒度的细节，特别是与用户控制相关的细节，生成更加逼真的地形。

    Sketch-based terrain generation seeks to create realistic landscapes for virtual environments in various applications such as computer games, animation and virtual reality. Recently, deep learning based terrain generation has emerged, notably the ones based on generative adversarial networks (GAN). However, these methods often struggle to fulfill the requirements of flexible user control and maintain generative diversity for realistic terrain. Therefore, we propose a novel diffusion-based method, namely terrain diffusion network (TDN), which actively incorporates user guidance for enhanced controllability, taking into account terrain features like rivers, ridges, basins, and peaks. Instead of adhering to a conventional monolithic denoising process, which often compromises the fidelity of terrain details or the alignment with user control, a multi-level denoising scheme is proposed to generate more realistic terrains by taking into account fine-grained details, particularly those relate
    
[^28]: CReHate: 跨文化重新注释英语仇恨言论数据集

    CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])

    [http://arxiv.org/abs/2308.16705](http://arxiv.org/abs/2308.16705)

    CReHate通过跨文化重新注释英语仇恨言论数据集，揭示了来自不同国家的个体对仇恨言论的不同看法，并引入了一种具有文化敏感性的分类器。这些发现强调了重新评估NLP研究在仇恨言论领域的必要性。

    

    英语数据集主要反映了特定国家的观点，这可能导致模型和数据集中存在文化偏差。这在受主观性影响较大的任务，如仇恨言论检测中特别有问题。为了深入了解来自不同国家的个体如何理解仇恨言论，我们介绍了CReHate，对抽样的SBIC数据集进行了跨文化重新注释。该数据集包括来自五个不同国家的注释：澳大利亚、新加坡、南非、英国和美国。我们进行了彻底的统计分析，发现基于国籍存在显著差异，只有59.4%的样本在所有国家之间达成共识。我们还通过迁移学习引入了一种具有文化敏感性的仇恨言论分类器，能够捕捉不同国籍的观点。这些发现强调了需要重新评估自然语言处理研究的某些方面，特别是对于仇恨言论的细微性质。

    English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
    
[^29]: 故障注入和安全错误攻击用于提取嵌入式神经网络模型

    Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models. (arXiv:2308.16703v1 [cs.CR])

    [http://arxiv.org/abs/2308.16703](http://arxiv.org/abs/2308.16703)

    本文介绍了故障注入和安全错误攻击用于提取嵌入式神经网络模型的方法，并阐述了对32位微控制器上的深度神经网络进行模型提取攻击的实验结果。

    

    模型提取作为一种关键的安全威胁而出现，攻击向量利用了算法和实现方面的方法。攻击者的主要目标是尽可能多地窃取受保护的受害者模型的信息，以便他可以用替代模型来模仿它，即使只有有限的访问相似的训练数据。最近，物理攻击，如故障注入，已经显示出对嵌入式模型的完整性和机密性的令人担忧的效果。我们的重点是32位微控制器上的嵌入式深度神经网络模型，这是物联网中广泛使用的硬件平台系列，以及使用标准故障注入策略-安全错误攻击（SEA）来进行具有有限训练数据访问的模型提取攻击。由于攻击强烈依赖于输入查询，我们提出了一种黑盒方法来构建一个成功的攻击集。对于一个经典的卷积神经网络，我们成功地恢复了至少90%的

    Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of
    
[^30]: 使用大型语言模型自动化科学文章的分类和趋势分析：以眼科学为应用实例

    Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology. (arXiv:2308.16688v1 [cs.CL])

    [http://arxiv.org/abs/2308.16688](http://arxiv.org/abs/2308.16688)

    本文介绍了一种利用大型语言模型自动分类科学文章的方法，主要应用于眼科学领域，但可扩展到其他领域。通过比较不同LLM模型，结果表明LLMs在无需人工干预的情况下能有效地对大量眼科学论文进行分类。

    

    本文介绍了一种利用大型语言模型（LLM）自动进行文章分类的方法。主要关注眼科领域，但该模型可扩展到其他领域。通过自然语言处理技术（NLP）开发了一个模型，包括高级LLM，用于处理和分析科学论文的文本内容。在LLM模型中，我们采用了零样本学习（ZSL）LLM模型，并与双向和自回归变换器（BART）及其变种，以及双向编码器表示从变换器（BERT）及其变种（如distilBERT，SciBERT，PubmedBERT，BioBERT）进行比较。分类结果表明，在没有人为干预的情况下，LLM在对大量眼科学论文进行分类方面具有有效性。

    Purpose: In this paper, we present an automated method for article classification, leveraging the power of Large Language Models (LLM). The primary focus is on the field of ophthalmology, but the model is extendable to other fields. Methods: We have developed a model based on Natural Language Processing (NLP) techniques, including advanced LLMs, to process and analyze the textual content of scientific papers. Specifically, we have employed zero-shot learning (ZSL) LLM models and compared against Bidirectional and Auto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder Representations from Transformers (BERT), and its variant such as distilBERT, SciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate the effectiveness of LLMs in categorizing large number of ophthalmology papers without human intervention. Results: To evalute the LLMs, we compiled a dataset (RenD) of 1000 ocular disease-related articles, which were expertly annotated by a pan
    
[^31]: 任何人都可以攻击：将有损压缩重新用作自然后门攻击

    Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])

    [http://arxiv.org/abs/2308.16684](http://arxiv.org/abs/2308.16684)

    本文发现了一种更严重的后门攻击威胁，即任何人都可以利用易获取的有损压缩算法进行自然后门攻击，无需设计特定触发器或进行繁琐调试。

    

    最近，后门攻击对实际应用中的机器学习模型的可信度构成了威胁。传统智慧认为，并不是每个人都可以成为攻击者，因为设计触发器生成算法的过程通常需要大量的努力和广泛的实验来确保攻击的隐秘性和有效性。然而，本文指出存在一种更为严重的后门威胁：任何人都可以利用易获取的算法进行隐悄后门攻击。具体来说，攻击者可以利用各种压缩工具中广泛使用的有损图片压缩技术，无需留下任何明显的痕迹就能轻松地将触发器模式注入到图像中，即生成的触发器是自然的图像伪影。使用有损图片压缩工具时，人们并不需要广泛知识，只需点击“转换”或“另存为”按钮即可。通过这种攻击，攻击者无需设计一个专门的触发器或进行繁琐的调试。

    The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a 
    
[^32]: 嵌入式神经网络的故障注入：单条指令跳过的影响

    Fault Injection on Embedded Neural Networks: Impact of a Single Instruction Skip. (arXiv:2308.16665v1 [cs.CR])

    [http://arxiv.org/abs/2308.16665](http://arxiv.org/abs/2308.16665)

    本论文通过实验探究了在嵌入式的Cortex M4 32位微控制器平台上应用电磁和激光注入故障的影响，并揭示了对神经网络推理流程进行修改攻击可能带来的完整性威胁。

    

    随着神经网络模型的大规模集成和使用，特别是在关键的嵌入式系统中，对它们的安全评估以保证它们的可靠性变得迫切。尤其是部署在32位微控制器等嵌入式平台上的模型可以被对手物理访问，因此容易受到硬件干扰的攻击。我们提出了一系列关于在Cortex M4 32位微控制器平台上嵌入的神经网络模型上应用电磁和激光注入两种故障注入手段的实验。与大多数致力于改变内部参数或输入值的现有工作不同，我们的目标是模拟和实验性地展示一种特定的错误模型——指令跳过的影响。为了达到这个目的，我们对神经网络推理的控制流进行了数种修改攻击的评估。我们揭示了通过针对推理过程的多个步骤的攻击来威胁其完整性。

    With the large-scale integration and use of neural network models, especially in critical embedded systems, their security assessment to guarantee their reliability is becoming an urgent need. More particularly, models deployed in embedded platforms, such as 32-bit microcontrollers, are physically accessible by adversaries and therefore vulnerable to hardware disturbances. We present the first set of experiments on the use of two fault injection means, electromagnetic and laser injections, applied on neural networks models embedded on a Cortex M4 32-bit microcontroller platform. Contrary to most of state-of-the-art works dedicated to the alteration of the internal parameters or input values, our goal is to simulate and experimentally demonstrate the impact of a specific fault model that is instruction skip. For that purpose, we assessed several modification attacks on the control flow of a neural network inference. We reveal integrity threats by targeting several steps in the inference
    
[^33]: 在知识图谱工程中开发一个可扩展的用于评估大型语言模型的基准测试

    Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering. (arXiv:2308.16622v1 [cs.AI])

    [http://arxiv.org/abs/2308.16622](http://arxiv.org/abs/2308.16622)

    本文介绍了一个基准测试框架，用于评估大型语言模型在知识图谱工程中的应用。框架包括语法和错误修正、事实提取和数据集生成三个挑战，同时也揭示了LLMs在零-shot提示下辅助知识图谱生成方面的不足。

    

    随着大型语言模型（LLMs）领域的快速发展，评估和监测其性能的迫切需求浮出水面。我们介绍了一个针对知识图谱工程（KGE）的基准测试框架，并提出了三个挑战，涉及语法和错误修正、事实提取和数据集生成。我们展示了尽管LLMs是有用的工具，但它们尚不能在零-shot提示下辅助知识图谱生成。因此，我们的LLM-KG-Bench框架提供了LLM回答的自动评估和存储，以及统计数据和可视化工具，支持提示工程和模型性能的跟踪。

    As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.
    
[^34]: 使用自然语言处理从社交网络文本中提取高准确度的位置信息

    High Accuracy Location Information Extraction from Social Network Texts Using Natural Language Processing. (arXiv:2308.16615v1 [cs.AI])

    [http://arxiv.org/abs/2308.16615](http://arxiv.org/abs/2308.16615)

    本研究利用自然语言处理从社交网络文本中提取位置信息，以构建准确的恐怖袭击预测数据集。实验证明，现有解决方案对于位置识别的准确性较差，而我们的解决方案解决了这个问题，并计划进一步扩展以提取其他相关信息。

    

    恐怖主义已经成为全球性的灾害，给国家的发展带来了严重后果。除了每天杀害无辜人民和阻止教育活动的进行，恐怖主义还阻碍了经济的增长。机器学习（ML）和自然语言处理（NLP）可以通过预测准确的数据来实时预测未来的恐怖袭击，从而有助于打击恐怖主义。该论文是一个研究项目的一部分，该项目利用社交网络文本提取必要的信息，以构建适当的恐怖袭击预测数据集。我们收集了3000条有关布基纳法索恐怖主义的社交网络文本，并使用其中的一部分来尝试现有的NLP解决方案。实验证明，现有的解决方案对于位置识别的准确性较差，而我们的解决方案解决了这个问题。我们将扩展该解决方案以提取日期和行动信息，以实现项目的目标。

    Terrorism has become a worldwide plague with severe consequences for the development of nations. Besides killing innocent people daily and preventing educational activities from taking place, terrorism is also hindering economic growth. Machine Learning (ML) and Natural Language Processing (NLP) can contribute to fighting terrorism by predicting in real-time future terrorist attacks if accurate data is available. This paper is part of a research project that uses text from social networks to extract necessary information to build an adequate dataset for terrorist attack prediction. We collected a set of 3000 social network texts about terrorism in Burkina Faso and used a subset to experiment with existing NLP solutions. The experiment reveals that existing solutions have poor accuracy for location recognition, which our solution resolves. We will extend the solution to extract dates and action information to achieve the project's goal.
    
[^35]: 通过合作专家实现长尾图分类的研究

    Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])

    [http://arxiv.org/abs/2308.16609](http://arxiv.org/abs/2308.16609)

    本文提出了一种新颖的方法，通过合作专家实现了长尾图分类，解决了现有方法在处理图数据上的不足。

    

    图分类旨在学习用于有效类别分配的图级表示，在平衡的类别分布的高质量数据集的支持下取得了杰出成果。事实上，大多数现实世界的图数据自然呈现长尾形式，其中头部类别的样本数量远超过尾部类别，因此在长尾数据上研究图级分类是至关重要的，但仍然较少探索。然而，现有的视觉中的长尾学习方法大多无法同时优化表示学习和分类器训练，并且忽略了难以分类的类别的挖掘。直接将现有方法应用于图可能导致次优性能，因为在图上训练的模型由于复杂的拓扑特征会更加敏感于长尾分布。因此，在本文中，我们提出了一种新颖的对长尾图级分类的方法

    Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
    
[^36]: 寻找稀疏双下降的解毒剂的探索

    The Quest of Finding the Antidote to Sparse Double Descent. (arXiv:2308.16596v1 [cs.AI])

    [http://arxiv.org/abs/2308.16596](http://arxiv.org/abs/2308.16596)

    本文致力于寻找稀疏双下降的解毒剂，通过研究和提出解决方案，分别采用l2正则化和知识蒸馏来避免稀疏双下降现象，以找到性能和稀疏性的最佳平衡点。

    

    在能源高效的方案中，找到深度学习模型的最优大小非常重要并具有广泛影响。与此同时，最近的研究报告了一种意外现象，稀疏双下降：随着模型的稀疏性增加，性能首先变差，然后改善，最后恶化。这种非单调行为对于保持高性能的最优模型大小提出了严重的问题：模型需要具有足够的超参数，但太多的参数会浪费训练资源。在本文中，我们旨在高效地找到最佳的权衡点。更具体地说，我们解决了稀疏双下降的出现，并提出一些解决方案来避免它。首先，我们展示了一个简单的l2正则化方法可以帮助缓解这种现象，但会牺牲性能/稀疏性的折衷。为了克服这个问题，我们引入了一种学习方案，其中蒸馏知识对学生模型进行正则化。

    In energy-efficient schemes, finding the optimal size of deep learning models is very important and has a broad impact. Meanwhile, recent studies have reported an unexpected phenomenon, the sparse double descent: as the model's sparsity increases, the performance first worsens, then improves, and finally deteriorates. Such a non-monotonic behavior raises serious questions about the optimal model's size to maintain high performance: the model needs to be sufficiently over-parametrized, but having too many parameters wastes training resources.  In this paper, we aim to find the best trade-off efficiently. More precisely, we tackle the occurrence of the sparse double descent and present some solutions to avoid it. Firstly, we show that a simple $\ell_2$ regularization method can help to mitigate this phenomenon but sacrifices the performance/sparsity compromise. To overcome this problem, we then introduce a learning scheme in which distilling knowledge regularizes the student model. Suppo
    
[^37]: CL-MAE: 课程学习的遮罩自编码器

    CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])

    [http://arxiv.org/abs/2308.16572](http://arxiv.org/abs/2308.16572)

    本文提出了一种课程学习的遮罩自编码器（CL-MAE）。我们引入了一种可学习的遮罩模块，通过更新遮罩策略来增加自监督重构任务的复杂性。通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。

    

    遮罩图像建模已被证明是一种强大的预文本任务，用于生成能够有效泛化到多个下游任务的鲁棒表示。通常，这种方法涉及在输入图像中随机遮罩补丁（标记），并且遮罩策略在训练过程中保持不变。本文提出了一种课程学习方法，通过更新遮罩策略以持续增加自监督重构任务的复杂性。我们推测，通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。为了实现这一点，我们引入了一种新颖的可学习遮罩模块，具有生成不同复杂度遮罩的能力，并将该模块与遮罩自编码器（MAE）集成。我们的模块与MAE一同训练，同时调整其行为，在训练过程中从MAE的参与者过渡到MAE（优化相同的重构目标）。

    Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
    
[^38]: MEME的力量：基于模型的增强学习在对抗性恶意软件生成中的应用

    The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning. (arXiv:2308.16562v1 [cs.CR])

    [http://arxiv.org/abs/2308.16562](http://arxiv.org/abs/2308.16562)

    本文提出了一种新的算法MEME，在对抗性恶意软件生成中使用了模型提取和恶意软件逃避的方法，并通过实验证明该算法在逃避方面表现更好。

    

    鉴于恶意软件的大量存在，为了检测恶意软件，防御者越来越多地采用自动化和机器学习技术。然而，机器学习模型容易受到对抗攻击，需要测试模型和产品的鲁棒性。同时，攻击者也试图自动化恶意软件的生成和对抗杀毒软件系统，而防御者则试图洞察他们的方法。本文提出了一种新的算法，将恶意软件逃避与模型提取（MEME）攻击相结合。MEME利用基于模型的增强学习对Windows可执行二进制样本进行对抗性修改，同时训练一个与目标模型高度一致的代理模型以进行逃避。为了评估该方法，我们将其与两种最先进的对抗性恶意软件生成攻击方法进行比较，使用三个著名的公开模型和一个杀毒软件产品作为目标。结果表明，相对于先进方法，MEME在逃避方面表现更好。

    Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of eva
    
[^39]: 关于多智能体相互作用的微分博弈、最优控制和基于能量的模型之间的联系

    On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])

    [http://arxiv.org/abs/2308.16539](http://arxiv.org/abs/2308.16539)

    本论文研究了微分博弈、最优控制和基于能量的模型之间的联系，并提出了基于能量的潜在博弈的新的端到端学习应用，通过神经网络和可微分的博弈论优化层的组合来提高预测性能。

    

    博弈论提供了一个可解释的数学框架，用于建模多智能体的相互作用。然而，在现实世界中的机器人应用中，博弈论的适用性受到多个挑战的阻碍，比如未知的智能体偏好和目标。为了解决这些挑战，我们展示了微分博弈、最优控制和基于能量的模型之间的联系，并展示了如何将现有方法统一到我们提出的基于能量的潜在博弈的形式化中。在这个形式化的基础上，本文引入了一种新的端到端学习应用，将神经网络用于博弈参数推断，并通过可微分的博弈论优化层作为归纳偏好。使用模拟的移动机器人行人相互作用和真实世界中的自动驾驶数据的实验证据表明，博弈论层改善了各种神经网络主干的预测性能。

    Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
    
[^40]: AI革命：金融行业的机遇与挑战

    The AI Revolution: Opportunities and Challenges for the Finance Sector. (arXiv:2308.16538v1 [cs.AI])

    [http://arxiv.org/abs/2308.16538](http://arxiv.org/abs/2308.16538)

    本研究调查了金融行业中的人工智能应用，讨论了其革命潜力和相关挑战。人工智能的应用范围广泛，包括改善客户服务、欺诈检测、风险管理、信用评估和高频交易。然而，人工智能的应用也带来了透明度、可解释性、公平性、问责制和信任度等问题。

    

    本报告探讨了人工智能在金融领域的应用，概述了其革命性潜力，并指出了相关挑战。它强调了对人工智能的全面了解、其能力以及相关影响的重要性，以便在充分利用潜力的同时降低相关风险。人工智能潜力的应用范围从增强现有运营到为金融领域打开新的应用途径。人工智能在金融领域的应用正在改变这个行业。其应用领域涵盖了客户服务增强、欺诈检测、风险管理、信用评估和高频交易等方面。然而，除了这些好处外，人工智能还带来了几个挑战。这些挑战包括与透明度、可解释性、公平性、问责制和信任度有关的问题。人工智能在金融领域的应用还引发了关于数据隐私和安全性的重要问题。

    This report examines Artificial Intelligence (AI) in the financial sector, outlining its potential to revolutionise the industry and identify its challenges. It underscores the criticality of a well-rounded understanding of AI, its capabilities, and its implications to effectively leverage its potential while mitigating associated risks. The potential of AI potential extends from augmenting existing operations to paving the way for novel applications in the finance sector. The application of AI in the financial sector is transforming the industry. Its use spans areas from customer service enhancements, fraud detection, and risk management to credit assessments and high-frequency trading. However, along with these benefits, AI also presents several challenges. These include issues related to transparency, interpretability, fairness, accountability, and trustworthiness. The use of AI in the financial sector further raises critical questions about data privacy and security. A further issu
    
[^41]: 通过神经符号约束来调节基于评分的生成模型

    Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])

    [http://arxiv.org/abs/2308.16534](http://arxiv.org/abs/2308.16534)

    本文提出了一种方法，通过神经符号约束来调节基于评分的生成模型，实现了在非条件生成模型下强制执行任意的逻辑约束，从而获得了一个有效的、无需额外训练的条件采样算法。

    

    基于评分和扩散模型已经成为一种有效的条件和非条件生成方法。然而，条件生成基于特定训练的条件模型或分类器指导，这需要训练一个噪声依赖的分类器，即使对于未损坏数据的分类器已经给出。我们提出了一种方法，可以从非条件评分生成模型中采样，可以强制执行任意的逻辑约束，而无需进行额外的训练。首先，我们展示了如何操纵学习得到的评分，以便在用户定义的约束条件下从非归一化分布中采样。然后，我们定义了一个灵活而数值稳定的神经符号框架，用于编码软逻辑约束。将这两个组成部分结合起来，我们获得了一个一般的但是近似的条件采样算法。我们进一步开发了有效的启发式方法来改进近似。最后，我们展示了我们方法的有效性。

    Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach fo
    
[^42]: 使用大型语言模型开发具有共情非语言暗示的社交机器人

    Developing Social Robots with Empathetic Non-Verbal Cues Using Large Language Models. (arXiv:2308.16529v1 [cs.RO])

    [http://arxiv.org/abs/2308.16529](http://arxiv.org/abs/2308.16529)

    本研究通过整合非语言暗示增强社交机器人的共情能力，在社交机器人中设计和标记了四种共情非语言暗示（SAFE），并使用大型语言模型生成这些暗示。该研究的重要发现是在机器人的回应中观察到明显的模式，如对平静和积极的社交情感的偏好以及频繁的点头动作。尽管如此，该方法已经实现了一个能够进行上下文感知和更真实互动的社交机器人的开发。

    

    我们提出通过整合非语言暗示来增强社交机器人的共情能力。我们的主要贡献是设计和标记了四种类型的共情非语言暗示，简称为SAFE：言语、动作（手势）、面部表情和情感。这些暗示是使用大型语言模型生成的。我们为机器人开发了基于语言模型的对话系统，并评估其与人类辅导员定义的社交暗示的一致性。初步结果显示机器人的回应具有明显的模式，比如对平静和积极的社交情感（如“喜悦”和“活力”）的偏好，以及频繁的点头动作。尽管存在这些倾向，我们的方法已经实现了一个能够进行上下文感知和更真实互动的社交机器人的开发。我们的工作为未来的人机交互研究奠定了基础，强调了语言和非语言暗示在创建社交和共情机器人中的重要作用。

    We propose augmenting the empathetic capacities of social robots by integrating non-verbal cues. Our primary contribution is the design and labeling of four types of empathetic non-verbal cues, abbreviated as SAFE: Speech, Action (gesture), Facial expression, and Emotion, in a social robot. These cues are generated using a Large Language Model (LLM). We developed an LLM-based conversational system for the robot and assessed its alignment with social cues as defined by human counselors. Preliminary results show distinct patterns in the robot's responses, such as a preference for calm and positive social emotions like 'joy' and 'lively', and frequent nodding gestures. Despite these tendencies, our approach has led to the development of a social robot capable of context-aware and more authentic interactions. Our work lays the groundwork for future studies on human-robot interactions, emphasizing the essential role of both verbal and non-verbal cues in creating social and empathetic robots
    
[^43]: 基于曲率的图神经网络中的汇聚方法研究

    Curvature-based Pooling within Graph Neural Networks. (arXiv:2308.16516v1 [cs.LG])

    [http://arxiv.org/abs/2308.16516](http://arxiv.org/abs/2308.16516)

    本研究提出了一种名为CurvPool的汇聚方法，通过利用图的曲率概念来解决过度平滑和过度压缩的问题。它能够根据曲率自适应地识别负责这两种现象的结构，并构建具有更合适结构的图，从而实现更深层模型和远距离信息的结合。

    

    过度压缩和过度平滑是限制图神经网络（GNN）能力的两个关键问题。过度平滑消除了节点之间的差异，使它们难以区分，而过度压缩指的是GNN无法在较长的距离上传播信息，因为指数级的节点状态被压缩成固定大小的表示。这两种现象具有类似的原因，都在很大程度上是由图拓扑引起的。为了解决这些问题，在图分类任务中，我们提出了一种新颖的汇聚方法CurvPool。CurvPool利用图的曲率概念自适应地识别负责过度平滑和过度压缩的结构。通过基于平衡Forman曲率对节点进行聚类，CurvPool构建了一个具有更合适结构的图，允许深层模型和远距离信息的结合。我们将其与其他最先进的汇聚方法进行比较，并确定其竞争力。

    Over-squashing and over-smoothing are two critical issues, that limit the capabilities of graph neural networks (GNNs). While over-smoothing eliminates the differences between nodes making them indistinguishable, over-squashing refers to the inability of GNNs to propagate information over long distances, as exponentially many node states are squashed into fixed-size representations. Both phenomena share similar causes, as both are largely induced by the graph topology. To mitigate these problems in graph classification tasks, we propose CurvPool, a novel pooling method. CurvPool exploits the notion of curvature of a graph to adaptively identify structures responsible for both over-smoothing and over-squashing. By clustering nodes based on the Balanced Forman curvature, CurvPool constructs a graph with a more suitable structure, allowing deeper models and the combination of distant information. We compare it to other state-of-the-art pooling approaches and establish its competitiveness 
    
[^44]: 推荐AI代理：将大型语言模型整合到交互式推荐中

    Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations. (arXiv:2308.16505v1 [cs.IR])

    [http://arxiv.org/abs/2308.16505](http://arxiv.org/abs/2308.16505)

    本论文的创新点是将推荐模型和大型语言模型（LLMs）融合，创建了一个多功能交互式推荐系统，解决了推荐模型在提供解释和参与对话任务方面的困难。

    

    推荐模型通过利用广泛的用户行为数据来提供领域特定的物品推荐，展现出轻量级领域专家的能力。然而，它们在提供解释和参与对话等多样化任务方面存在困难。另一方面，大型语言模型（LLMs）代表了人工通用智能的重要进展，在指令理解、常识推理和人类交互方面表现出了显著能力。然而，LLMs缺乏领域特定物品目录和行为模式的知识，特别是在与一般世界知识不同的领域，如在线电子商务。为每个领域微调LLMs既不经济又不高效。在本文中，我们将推荐模型和LLMs之间的差距，结合各自的优势，创建了一个多功能交互式推荐系统。我们引入了一个高效的框架称为RecAgent，该框架使用LLMs

    Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.  In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called RecAgent, which employs LLMs a
    
[^45]: 个体合理的协作车辆路径规划通过给予和接受交换

    Individually Rational Collaborative Vehicle Routing through Give-And-Take Exchanges. (arXiv:2308.16501v1 [cs.MA])

    [http://arxiv.org/abs/2308.16501](http://arxiv.org/abs/2308.16501)

    本文关注物流公司间自动交易订单以优化总收入的问题，提出了一种新的多智能体方法来解决合作车辆路径规划问题（CVRP），通过促进竞争物流代理之间的合作，实现减少行驶距离、提高运营效率，并确保个体合理性和更快的收敛速度。

    

    本文关注物流公司之间在市场平台上自动交换订单以优化总收入的问题。我们提出了一种新颖的多智能体方法，着重解决合作车辆路径规划问题（CVRP）并考虑个体合理性。我们的算法将车辆路径规划问题（VRP）的原则应用于来自不同物流公司的车辆对，优化总体路线同时考虑了标准的VRP约束和个体合理性约束。通过促进竞争物流代理之间的合作，我们展示了可以系统性地减少行驶距离并提高运营效率。更重要的是，我们的方法确保了个体合理性和更快的收敛速度，这是确保市场平台的长期可持续性的重要特性。我们通过实验证明了我们方法的有效性。

    In this paper, we are concerned with the automated exchange of orders between logistics companies in a marketplace platform to optimize total revenues. We introduce a novel multi-agent approach to this problem, focusing on the Collaborative Vehicle Routing Problem (CVRP) through the lens of individual rationality. Our proposed algorithm applies the principles of Vehicle Routing Problem (VRP) to pairs of vehicles from different logistics companies, optimizing the overall routes while considering standard VRP constraints plus individual rationality constraints. By facilitating cooperation among competing logistics agents through a Give-and-Take approach, we show that it is possible to reduce travel distance and increase operational efficiency system-wide. More importantly, our approach ensures individual rationality and faster convergence, which are important properties of ensuring the long-term sustainability of the marketplace platform. We demonstrate the efficacy of our approach throu
    
[^46]: 广义Winograd Schema及其上下文性

    Generalised Winograd Schema and its Contextuality. (arXiv:2308.16498v1 [cs.CL])

    [http://arxiv.org/abs/2308.16498](http://arxiv.org/abs/2308.16498)

    本文研究了广义Winograd Schema在上下文性方面的应用，提出了一种利用量子物理实验模型来解决Winograd模式挑战的方法。

    

    自然语言中的歧义会引起对解释的概率分布。这些分布通常涉及到多个模棱两可的词汇，这使得它们成为适合量子上下文性拟设模型的研究主题。以前的研究表明，上下文性的不同定量度量与心理语言学研究中的词义歧义有很好的相关性。在本研究中，我们关注指代的歧义，并研究了Winograd模式挑战（WSC），这是Levesque在2011年提出的用于评估机器智能的测试。WSC包含一系列多项选择问题，需要在按照Winograd模式构造的句子中消除代词的歧义，这对机器来说很难确定正确的代词指向，但对人类理解来说却很直观。在本研究中，我们提出了一种类似地将Winograd模式建模为量子物理实验的方法。

    Ambiguities in natural language give rise to probability distributions over interpretations. The distributions are often over multiple ambiguous words at a time; a multiplicity which makes them a suitable topic for sheaf-theoretic models of quantum contextuality. Previous research showed that different quantitative measures of contextuality correlate well with Psycholinguistic research on lexical ambiguities. In this work, we focus on coreference ambiguities and investigate the Winograd Schema Challenge (WSC), a test proposed by Levesque in 2011 to evaluate the intelligence of machines. The WSC consists of a collection of multiple-choice questions that require disambiguating pronouns in sentences structured according to the Winograd schema, in a way that makes it difficult for machines to determine the correct referents but remains intuitive for human comprehension. In this study, we propose an approach that analogously models the Winograd schema as an experiment in quantum physics. Ho
    
[^47]: 不重新训练的情况下扩展冻结的视觉-语言模型：对改进机器人感知的探索

    Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception. (arXiv:2308.16493v1 [cs.AI])

    [http://arxiv.org/abs/2308.16493](http://arxiv.org/abs/2308.16493)

    本文提出一种方法，通过对不同模态的嵌入空间进行对齐，使得视觉-语言模型能够在不重新训练的情况下理解和推理额外的模态，并且在人机交互场景中改进机器人的感知能力。

    

    视觉-语言模型（VLM）通过将视觉表示与大型语言模型（LLMs）在预训练期间学习的抽象技能组合起来，在视觉问答和推理任务中展现了强大的能力。视觉是最受欢迎的方式之一，用于增强LLMs，但仅仅是场景的一个表示。在人机交互场景中，机器人感知需要对场景进行准确的理解。在本文中，我们定义并展示了一种通过监督和对比训练将不同模态（在此情况下是惯性测量单元（IMU）数据）的嵌入空间与视觉嵌入空间对齐的方法，使得VLM能够在不重新训练的情况下理解和推理这些额外的模态。我们选择直接提供模型IMU嵌入，而不是使用单独的人体活动识别模型直接输入提示，以便允许查询、图像和IMU之间的任何非线性交互。

    Vision-language models (VLMs) have shown powerful capabilities in visual question answering and reasoning tasks by combining visual representations with the abstract skill set large language models (LLMs) learn during pretraining. Vision, while the most popular modality to augment LLMs with, is only one representation of a scene. In human-robot interaction scenarios, robot perception requires accurate scene understanding by the robot. In this paper, we define and demonstrate a method of aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training, enabling the VLM to understand and reason about these additional modalities without retraining. We opt to give the model IMU embeddings directly over using a separate human activity recognition model that feeds directly into the prompt to allow for any nonlinear interactions between the query, image, and IMU
    
[^48]: 课堂数据分析复制：教学生，同时测试科学

    In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])

    [http://arxiv.org/abs/2308.16491](http://arxiv.org/abs/2308.16491)

    这项研究揭示了课堂数据分析复制的可行性，以及这种方法对学生、教育者和科学家的成本与收益。同时，学生对数据的预期与实际情况存在差异。

    

    科学正面临可重复性危机。先前的工作提出将数据分析复制纳入课堂作为潜在解决方案。然而，尽管潜在的好处，目前尚不清楚这一方法是否可行，如果可行，涉及的利益相关者-学生、教育者和科学家-应该期望什么。学生能够在课堂上进行数据分析复制吗？教育者的成本与收益如何？这个解决方案如何帮助评估和改进科学的现状？本研究在EPFL教授的应用数据分析课程（CS-401）的项目部分中纳入了数据分析复制（N=354名学生）。在此报告中，我们基于课程期间进行的调查提前进行注册的发现。首先，我们证明学生可以复制先前发表的科学论文，大部分是定性的，有些是完全一样的。我们发现学生对数据的预期与实际情况存在差异

    Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
    
[^49]: 潜在画家

    Latent Painter. (arXiv:2308.16490v1 [cs.CV])

    [http://arxiv.org/abs/2308.16490](http://arxiv.org/abs/2308.16490)

    这个论文介绍了一种名为潜在画家的技术，它利用潜在作为画布和扩散器的预测作为计划来生成绘画动画，同时还可以在不同的检查点集中转换图像。

    

    潜在扩散器在生成AI领域引起了革命，并激发了创造性艺术。在去噪潜在时，每个步骤预测的原始图像共同形成了动画。然而，动画受到扩散器去噪特性的限制，只呈现了一个锐化过程。本文介绍了潜在画家，它以潜在作为画布，以扩散器的预测作为计划，生成绘画动画。潜在画家还可以将一个生成的图像转换为另一个图像，这可以发生在两个不同检查点集中的图像之间。

    Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
    
[^50]: 使用元学习进行点云上采样的测试时间适应

    Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])

    [http://arxiv.org/abs/2308.16484](http://arxiv.org/abs/2308.16484)

    本文提出了一种使用元学习进行测试时间适应的方法来增强点云上采样模型的普适性，解决了测试数据分布与训练数据不同导致性能下降的问题。

    

    廉价的3D扫描仪经常产生稀疏和非均匀的点云，这对机器人系统中的下游应用产生负面影响。虽然现有的点云上采样架构在标准基准数据上展示了有希望的结果，但当测试数据与训练数据具有不同分布时，它们往往会出现显著的性能下降。为了解决这个问题，本文提出了一种测试时间适应方法来增强点云上采样模型的普适性。所提出的方法利用元学习来显式地学习测试时间适应的网络参数。我们的方法不需要任何关于测试数据的先验信息。在元训练过程中，模型参数是从训练数据的稀疏-密集点云对的集合中学习的。在元测试过程中，经过少量梯度更新的训练模型可以产生一组唯一的网络参数。

    Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for
    
[^51]: Point-TTA: 使用多任务元辅助学习的点云配准测试时间自适应

    Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])

    [http://arxiv.org/abs/2308.16481](http://arxiv.org/abs/2308.16481)

    Point-TTA是一种通过多任务元辅助学习实现的点云配准测试时自适应框架，能够提高配准模型的泛化性能。

    

    我们提出了Point-TTA，这是一种新颖的点云配准测试时间自适应框架，可以提高配准模型的泛化性能。虽然基于学习的方法取得了令人印象深刻的进展，但面对未知的测试环境的泛化仍然是一个重大挑战，原因是3D扫描的变化较大。现有方法通常训练一个通用模型，并在每个实例上应用相同的训练模型。这可能是次优的，因为同一模型很难处理测试期间的所有变化。在本文中，我们提出了一种用于点云配准的测试时间自适应方法。我们的模型可以适应测试时未知的分布，无需任何关于测试数据的先验知识。具体地，我们设计了三个自监督辅助任务，这些任务与主要的配准任务一起进行优化。给定一个测试实例，我们使用这些辅助任务来调整我们的模型，并使用更新后的模型进行推断。

    We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. 
    
[^52]: Transformer压缩通过子空间投影

    Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])

    [http://arxiv.org/abs/2308.16475](http://arxiv.org/abs/2308.16475)

    Transformer压缩通过子空间投影，在减小模型隐藏大小的同时实现了较大的模型参数和计算资源的减少，并且与其他方法兼容。

    

    我们提出了一种名为TCSP的新方法，用于通过减少模型的隐藏大小来压缩Transformer模型。通过将整个转换模型投影到一个子空间中，我们使模型中的权重矩阵与减小维度空间中的特征之间可以进行矩阵操作，从而显著减少了模型参数和计算资源。为了建立这个子空间，我们将来自不同层次的采样数据实例的特征矩阵分解为一个投影矩阵。为了评估效果，我们在GLUE和SQuAD基准测试上应用TCSP来压缩T5和BERT模型。实验结果表明，TCSP在保证最多1.6%的准确度降低的情况下实现了44%的压缩比，超过或者达到了先前的压缩方法。此外，TCSP还与其他目标过滤器和注意力头大小压缩的方法相兼容。

    We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
    
[^53]: 提升多模式大型语言模型的子任务性能

    Enhancing Subtask Performance of Multi-modal Large Language Model. (arXiv:2308.16474v1 [cs.CL])

    [http://arxiv.org/abs/2308.16474](http://arxiv.org/abs/2308.16474)

    该论文提出了一种方法，通过选择多个预训练模型来完成相同的子任务，通过组合多个模型的结果获得最佳的子任务结果。

    

    多模式大型语言模型（MLLM）是指从大型语言模型（LLM）扩展而来的模型，具备处理和推理多模式数据的能力。当前的MLLM通常通过使用LLM将任务分解为多个子任务，然后使用各个预训练模型完成特定的子任务，并最终利用LLM整合每个子任务的结果来获得任务的结果。在现实世界中，处理大型项目时，常常将项目分解为较小的子项目，并由不同的团队提供相应的解决方案或结果。项目所有者随后决定使用哪个解决方案或结果，以确保每个子任务和整个项目能够达到最佳结果。受此启发，本研究考虑选择多个预训练模型来完成相同的子任务。通过将多个预训练模型的结果进行组合，获得最佳的子任务结果。

    Multi-modal Large Language Model (MLLM) refers to a model expanded from a Large Language Model (LLM) that possesses the capability to handle and infer multi-modal data. Current MLLMs typically begin by using LLMs to decompose tasks into multiple subtasks, then employing individual pre-trained models to complete specific subtasks, and ultimately utilizing LLMs to integrate the results of each subtasks to obtain the results of the task. In real-world scenarios, when dealing with large projects, it is common practice to break down the project into smaller sub-projects, with different teams providing corresponding solutions or results. The project owner then decides which solution or result to use, ensuring the best possible outcome for each subtask and, consequently, for the entire project. Inspired by this, this study considers selecting multiple pre-trained models to complete the same subtask. By combining the results from multiple pre-trained models, the optimal subtask result is obtai
    
[^54]: MaintainoMATE: 一个用于智能自动化维护活动的GitHub应用程序

    MaintainoMATE: A GitHub App for Intelligent Automation of Maintenance Activities. (arXiv:2308.16464v1 [cs.SE])

    [http://arxiv.org/abs/2308.16464](http://arxiv.org/abs/2308.16464)

    MaintainoMATE是一个GitHub应用程序，使用BERT模型实现自动问题报告分类和分配给专业开发人员。

    

    软件开发项目依赖于问题跟踪系统来跟踪维护任务，如错误报告和增强请求。这些问题跟踪系统上的问题报告必须以有效的方式进行管理。首先，它们必须进行标记，然后分配给具有相关专业知识的特定开发人员。处理问题报告是关键的，并且需要对问题报告中输入的文本进行彻底的扫描，使其成为一项劳动密集型的任务。在本文中，我们提出了一个名为MaintainoMATE的统一框架，能够自动将问题报告分类到相应的类别，并进一步将问题报告分配给具有相关专业知识的开发人员。我们使用Transformer中的双向编码器表示（BERT）作为MaintainoMATE的底层模型，用于学习自动问题报告标记和分配任务的上下文信息。我们将该框架部署为GitHub应用程序。

    Software development projects rely on issue tracking systems at the core of tracking maintenance tasks such as bug reports, and enhancement requests. Incoming issue-reports on these issue tracking systems must be managed in an effective manner. First, they must be labelled and then assigned to a particular developer with relevant expertise. This handling of issue-reports is critical and requires thorough scanning of the text entered in an issue-report making it a labor-intensive task. In this paper, we present a unified framework called MaintainoMATE, which is capable of automatically categorizing the issue-reports in their respective category and further assigning the issue-reports to a developer with relevant expertise. We use the Bidirectional Encoder Representations from Transformers (BERT), as an underlying model for MaintainoMATE to learn the contextual information for automatic issue-report labeling and assignment tasks. We deploy the framework used in this work as a GitHub appl
    
[^55]: BioCoder: 一种带有上下文语用知识的生物信息学代码生成基准

    BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])

    [http://arxiv.org/abs/2308.16458](http://arxiv.org/abs/2308.16458)

    BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。

    

    预训练的语言模型（如ChatGPT）显著改进了代码生成。随着这些模型的扩大，需要输出来处理更复杂的任务的需求也越来越多。此外，在生物信息学中，生成功能程序由于领域知识量大、需要复杂的数据操作和复杂的功能依赖关系而面临额外的挑战。在这里，我们介绍了BioCoder，这是一个用于评估现有预训练模型在生成生物信息学代码方面的基准。与函数代码生成有关，BioCoder涵盖了可能的包依赖关系、类声明和全局变量。它包括来自GitHub的1026个Python和Java函数和1243个方法，以及来自Rosalind项目的253个示例。BioCoder还结合了一个用于评估的模糊测试框架，我们已经应用它来评估许多模型，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT。

    Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
    
[^56]: 基于多个节点中心子图的对比表示学习

    Contrastive Representation Learning Based on Multiple Node-centered Subgraphs. (arXiv:2308.16441v1 [cs.AI])

    [http://arxiv.org/abs/2308.16441](http://arxiv.org/abs/2308.16441)

    本文提出了一种基于多个节点中心子图的对比表示学习方法，在图对比学习的框架下，通过最大化同一节点的不同子图之间的互信息来学习节点表示，实验证明该方法在各种真实数据集和不同下游任务上取得了最先进的结果。

    

    作为图结构数据的基本元素，节点已被认为是图表示学习中的主要研究对象。一个单独的节点直观上具有来自整个图的多个节点中心子图（例如，一个社交网络中的一个人根据他不同的关系有多个社交圈）。我们在图对比学习框架下研究了这种直觉，并提出了一种基于多个节点中心子图的对比表示学习方法，以自我监督的方式在图上学习节点表示。具体而言，我们精心设计了一系列以中心节点为中心的区域子图。然后，通过对比损失最大化同一节点的不同子图之间的互信息。在各种真实数据集和不同下游任务上的实验证明，我们的模型取得了最先进的结果。

    As the basic element of graph-structured data, node has been recognized as the main object of study in graph representation learning. A single node intuitively has multiple node-centered subgraphs from the whole graph (e.g., one person in a social network has multiple social circles based on his different relationships). We study this intuition under the framework of graph contrastive learning, and propose a multiple node-centered subgraphs contrastive representation learning method to learn node representation on graphs in a self-supervised way. Specifically, we carefully design a series of node-centered regional subgraphs of the central node. Then, the mutual information between different subgraphs of the same node is maximized by contrastive loss. Experiments on various real-world datasets and different downstream tasks demonstrate that our model has achieved state-of-the-art results.
    
[^57]: BenchTemp: 用于评估时间图神经网络的通用基准

    BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks. (arXiv:2308.16385v1 [cs.LG])

    [http://arxiv.org/abs/2308.16385](http://arxiv.org/abs/2308.16385)

    BenchTemp是一个通用基准，用于评估时间图神经网络（TGNN）模型在不同工作负载上的表现。BenchTemp提供一组基准数据集和一个标准流程，用于公平比较不同的TGNN模型。通过BenchTemp，我们对不同任务和设置下的代表性TGNN模型进行了广泛比较。

    

    为了处理特征或连接在演化的图中的时间，已经提出了一系列的时间图神经网络（TGNNs）。尽管这些TGNN取得了成功，但以往的TGNN评估揭示了四个关键问题上的几个限制：1）数据集不一致，2）评估流程不一致，3）缺乏工作负载多样性，4）缺乏有效的比较。总的来说，缺乏一个将TGNN模型放在同一起跑线上并全面比较它们的实证研究。为此，我们提出了BenchTemp，一个用于在各种工作负载上评估TGNN模型的通用基准。BenchTemp提供一组基准数据集，以便可以公平地比较不同的TGNN模型。此外，BenchTemp还设计了一个标准流程，统一了TGNN的评估。借助BenchTemp，我们广泛比较了不同任务（例如链接预测和节点分类）和设置（传递和归纳）下的代表性TGNN模型。

    To handle graphs in which features or connectivities are evolving over time, a series of temporal graph neural networks (TGNNs) have been proposed. Despite the success of these TGNNs, the previous TGNN evaluations reveal several limitations regarding four critical issues: 1) inconsistent datasets, 2) inconsistent evaluation pipelines, 3) lacking workload diversity, and 4) lacking efficient comparison. Overall, there lacks an empirical study that puts TGNN models onto the same ground and compares them comprehensively. To this end, we propose BenchTemp, a general benchmark for evaluating TGNN models on various workloads. BenchTemp provides a set of benchmark datasets so that different TGNN models can be fairly compared. Further, BenchTemp engineers a standard pipeline that unifies the TGNN evaluation. With BenchTemp, we extensively compare the representative TGNN models on different tasks (e.g., link prediction and node classification) and settings (transductive and inductive), w.r.t. bo
    
[^58]: 图神经网络中的隐私调查：攻击、保护和应用

    A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])

    [http://arxiv.org/abs/2308.16375](http://arxiv.org/abs/2308.16375)

    这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。

    

    随着处理图结构数据的能力和实际应用的改善，图神经网络（GNNs）引起了人们的极大关注。然而，许多这些模型优先考虑高效能表现，如准确性，而缺乏隐私考虑，这是现代社会隐私攻击盛行的重要问题。为了解决这个问题，研究人员开始开发保护隐私的GNNs。尽管取得了进展，但在图领域缺乏对攻击和隐私保护技术的综合概述。在本调查中，我们旨在通过总结针对图数据的攻击、对GNNs中的隐私保护技术进行分类以及审查可用于分析/解决GNNs中隐私问题的数据集和应用程序，填补这一空白。我们还概述了未来研究的潜在方向，以建立更好的隐私保护GNNs。

    Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
    
[^59]: 加强欧盟AI法案: 对AI操纵的关键术语进行定义

    Strengthening the EU AI Act: Defining Key Terms on AI Manipulation. (arXiv:2308.16364v1 [cs.AI])

    [http://arxiv.org/abs/2308.16364](http://arxiv.org/abs/2308.16364)

    这项研究提出技术建议，以明确定义欧盟AI法案中的关键概念，并提高可执行性。它包括对人格特质、行为、潜意识和欺骗技术的定义，以及对个体和群体利用的区分。此外，它还强调了对知情决策的定义和对治疗用途豁免的警告。

    

    欧盟的人工智能法案旨在规范对AI的操纵和有害使用，但缺乏对关键概念的明确定义。本文提出了技术建议，以改善该法案的概念清晰度和可执行性。我们通过审查心理模型来定义“人格特质”，主张该法案应保护完整的“心理测量资料”。我们敦促将“行为”扩展为包括“偏好”，因为偏好在因果上影响和受到行为的影响。对于“潜意识”、“操纵”和“欺骗”技术提供了清晰的定义，考虑到动机、意图和秘密性。我们区分了“利用个体”和“利用群体”，强调了不同的政策需求。一个“知情决策”由四个方面来定义：理解、准确信息、无操纵和理解AI的影响。我们对法案中的治疗用途豁免提出了警告，因为欧洲药品管理局对数字治疗尚未进行监管。

    The European Union's Artificial Intelligence Act aims to regulate manipulative and harmful uses of AI, but lacks precise definitions for key concepts. This paper provides technical recommendations to improve the Act's conceptual clarity and enforceability. We review psychological models to define "personality traits," arguing the Act should protect full "psychometric profiles." We urge expanding "behavior" to include "preferences" since preferences causally influence and are influenced by behavior. Clear definitions are provided for "subliminal," "manipulative," and "deceptive" techniques, considering incentives, intent, and covertness. We distinguish "exploiting individuals" from "exploiting groups," emphasising different policy needs. An "informed decision" is defined by four facets: comprehension, accurate information, no manipulation, and understanding AI's influence. We caution the Act's therapeutic use exemption given the lack of regulation of digital therapeutics by the EMA. Ove
    
[^60]: 大型语言模型作为数据预处理器

    Large Language Models as Data Preprocessors. (arXiv:2308.16361v1 [cs.AI])

    [http://arxiv.org/abs/2308.16361](http://arxiv.org/abs/2308.16361)

    大型语言模型可以作为数据预处理器的应用，通过使用开发工程技术和传统方法来提高性能。

    

    大型语言模型（LLMs），如OpenAI的GPT系列和Meta的LLaMA变体，标志着人工智能的重大进展。经过大量文本数据的训练，LLMs能够理解和生成各种主题上人类化的文本。本研究扩展了LLMs的应用范围，探讨了它们在数据预处理中的潜力，这是数据挖掘和分析应用中的关键阶段。我们深入研究了最先进的LLMs（如GPT-3.5、GPT-4和Vicuna-13B）在错误检测、数据插补、模式匹配和实体匹配任务中的适用性。除了展示LLMs的内在能力外，我们还强调了它们的局限性，特别是在计算开销和效率方面。我们提出了一种基于LLMs的数据预处理框架，该框架整合了前沿的提示工程技术，结合了上下文化和特征选择等传统方法，以提高性能。

    Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's LLaMA variants, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. We delve into the applicability of state-of-the-art LLMs such as GPT-3.5, GPT-4, and Vicuna-13B for error detection, data imputation, schema matching, and entity matching tasks. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the perform
    
[^61]: 揭秘假新闻：运用自然语言处理在打击假新闻中的革命性真相 (arXiv:2308.16328v1 [cs.AI])

    Debunking Disinformation: Revolutionizing Truth with NLP in Fake News Detection. (arXiv:2308.16328v1 [cs.AI])

    [http://arxiv.org/abs/2308.16328](http://arxiv.org/abs/2308.16328)

    这篇论文深入讨论了如何利用自然语言处理技术来检测假新闻，并揭示了其中的挑战和机遇。

    

    互联网和社交媒体改变了人们在即时信息传播时代获取新闻的方式。虽然这一发展增加了信息获取的机会，但也带来了一个重大问题：假新闻和虚假信息的传播。假新闻正在快速传播于数字平台上，对媒体生态系统、舆论、决策和社会凝聚力产生了负面影响。自然语言处理(NLP)作为一种识别内容真实性的多种方法，已经成为与虚假信息作斗争中的有力武器。本文深入探讨了如何运用NLP技术来检测假新闻，并揭示了其中的挑战和机遇。

    The Internet and social media have altered how individuals access news in the age of instantaneous information distribution. While this development has increased access to information, it has also created a significant problem: the spread of fake news and information. Fake news is rapidly spreading on digital platforms, which has a negative impact on the media ecosystem, public opinion, decision-making, and social cohesion. Natural Language Processing(NLP), which offers a variety of approaches to identify content as authentic, has emerged as a potent weapon in the growing war against disinformation. This paper takes an in-depth look at how NLP technology can be used to detect fake news and reveals the challenges and opportunities it presents.
    
[^62]: 有竞争选择的因果战略学习

    Causal Strategic Learning with Competitive Selection. (arXiv:2308.16262v1 [cs.AI])

    [http://arxiv.org/abs/2308.16262](http://arxiv.org/abs/2308.16262)

    我们研究了具有竞争选择的因果战略学习中的代理选择问题，并提出了最佳选择规则的数学形式和实现机制。

    

    我们研究了多个决策者下的因果战略学习中的代理选择问题，并解决了其中的两个关键挑战。首先，我们考虑了由代理人评估和选择组成的选择过程的影响，而不是之前研究中关注的固定代理人池。当每个决策者通过最大化自身效用来单方面选择代理人时，我们证明了最佳的选择规则是在选择最佳代理人和提供激励以最大化代理人改进之间进行权衡的结果。此外，这个最佳选择规则依赖于代理人结果的错误预测。因此，我们研究了决策者的最佳选择规则不会导致代理人结果恶化，也不会造成不公正的降低代理人选择机会的条件。为此，我们提供了最佳选择规则的数学形式和一种实现机制。

    We study the problem of agent selection in causal strategic learning under multiple decision makers and address two key challenges that come with it. Firstly, while much of prior work focuses on studying a fixed pool of agents that remains static regardless of their evaluations, we consider the impact of selection procedure by which agents are not only evaluated, but also selected. When each decision maker unilaterally selects agents by maximising their own utility, we show that the optimal selection rule is a trade-off between selecting the best agents and providing incentives to maximise the agents' improvement. Furthermore, this optimal selection rule relies on incorrect predictions of agents' outcomes. Hence, we study the conditions under which a decision maker's optimal selection rule will not lead to deterioration of agents' outcome nor cause unjust reduction in agents' selection chance. To that end, we provide an analytical form of the optimal selection rule and a mechanism to r
    
[^63]: 回归问题的校准解释

    Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])

    [http://arxiv.org/abs/2308.16245](http://arxiv.org/abs/2308.16245)

    本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。

    

    人工智能（AI）通常是现代决策支持系统（DSS）的一部分。在基于AI的DSS中使用的最佳预测模型缺乏透明度。可解释的人工智能（XAI）旨在创建能够向人类用户解释其理由的AI系统。XAI中的局部解释可以提供关于个别预测的原因的信息，即特征重要性。然而，现有局部解释方法的一个关键缺点是无法量化与特征重要性相关的不确定性。本文介绍了特征重要性解释方法Calibrated Explanations（CE）的扩展，之前只支持分类，现在支持标准回归和概率回归，即目标超过任意阈值的概率。回归问题的扩展保留了CE的所有优点，例如将底层模型的预测与置信度校准。

    Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
    
[^64]: 使用基于图的多智能体强化学习学习协作信息传播

    Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])

    [http://arxiv.org/abs/2308.16198](http://arxiv.org/abs/2308.16198)

    本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。

    

    在现代通信系统中，高效可靠的信息传播对支持关键操作至关重要，如灾难响应、自动驾驶车辆和传感器网络。本文介绍了一种多智能体强化学习（MARL）方法，作为实现更为分散、高效和协作解决方案的重要进展。我们提出了一种用于信息传播的分布式POMDP（Decentralized-POMDP）形式，使得每个智能体可以独立决定消息的转发。这构成了一种从传统基于多点中继（MPR）选择的启发式方法的重大范式转移。我们的方法利用图卷积强化学习，采用具有动态注意力的图注意力网络（GAT）来捕捉关键网络特征。我们提出了两种方法，L-DGN和HL-DGN，它们在智能体之间交换的信息上有所不同。通过将我们的分散方法与基于MPR的方法进行比较，我们评估了其性能。

    In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
    
[^65]: RoboTAP: 追踪任意点进行少样本视觉模仿

    RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation. (arXiv:2308.15975v1 [cs.RO])

    [http://arxiv.org/abs/2308.15975](http://arxiv.org/abs/2308.15975)

    RoboTAP通过利用稠密追踪技术实现了快速、普适的学习，可以从短时间内收集的演示中解决复杂的物体排列任务。

    

    为了使机器人在实验室和专门的工厂之外也能发挥作用，我们需要一种快速教授它们新的有用行为的方法。当前的方法要么缺乏普适性以进行新任务的上线，而不需要特定任务的工程化，要么缺乏数据效率，无法在实践中使用的时间范围内完成。在这项工作中，我们探索了稠密追踪作为一种表示工具，以实现更快速、更普适的示教学习。我们的方法利用Track-Any-Point (TAP)模型，将演示中的相关运动隔离出来，并参数化一个低级控制器，在场景配置发生变化时重现该运动。我们展示了这种方法可以生成稳健的机器人策略，可以解决复杂的物体排列任务，如形状匹配、叠放，甚至可以完成完整的路径跟踪任务，如施胶和粘合物体，所有这些任务的演示可以在几分钟内收集到。

    For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.
    
[^66]: WALL-E: 具有大型语言模型的实体机器人服务员举重

    WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model. (arXiv:2308.15962v1 [cs.RO])

    [http://arxiv.org/abs/2308.15962](http://arxiv.org/abs/2308.15962)

    本文研究了将大型语言模型与视觉定位和机器人抓取系统集成，通过使用WALL-E实现了在餐厅场景中提高人机交互准确性和效率的目标。通过实验和评估，证明了这种集成可以使WALL-E成为一位更有能力和智能的机器人服务员。

    

    让机器人能够理解语言指令并根据视觉感知做出反应一直以来都是机器人研究界的一个长期目标。实现这一目标需要在自然语言处理、计算机视觉和机器人工程方面取得前沿进展。因此，本文主要研究了将最新的大型语言模型（LLMs）与现有的视觉定位和机器人抓取系统集成以增强人机交互效果的潜力。我们以WALL-E（具有大型语言模型的实体机器人服务员举重）作为集成的示例。系统利用ChatGPT的LLM通过多轮交互式对话将用户的偏好物体总结为目标指令。然后将目标指令传递给视觉定位系统进行物体姿势和大小估计，然后机器人相应地抓取物体。我们将这个LLM增强系统部署在

    Enabling robots to understand language instructions and react accordingly to visual perception has been a long-standing goal in the robotics research community. Achieving this goal requires cutting-edge advances in natural language processing, computer vision, and robotics engineering. Thus, this paper mainly investigates the potential of integrating the most recent Large Language Models (LLMs) and existing visual grounding and robotic grasping system to enhance the effectiveness of the human-robot interaction. We introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language model) as an example of this integration. The system utilizes the LLM of ChatGPT to summarize the preference object of the users as a target instruction via the multi-round interactive dialogue. The target instruction is then forwarded to a visual grounding system for object pose and size estimation, following which the robot grasps the object accordingly. We deploy this LLM-empowered system on the
    
[^67]: CongNaMul: 一种用于大豆芽图像处理的数据集

    CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v1 [cs.CV])

    [http://arxiv.org/abs/2308.15690](http://arxiv.org/abs/2308.15690)

    提出了一个用于大豆芽图像处理的名为CongNaMul的数据集，旨在支持图像分类、语义分割、分解和测量等任务。提供了质量分类、语义分割和图像分解的标记，以及5个芽的物理特征供测量使用。

    

    我们提出了“CongNaMul”，这是一个为大豆芽图像分析的各种任务而设计的综合数据集。CongNaMul数据集旨在促进图像分类、语义分割、分解以及长度和重量的测量等任务。分类任务提供了四个类别来确定大豆芽的质量：正常、断裂、斑点和断裂和斑点，以开发基于人工智能辅助的自动质量检测技术。对于语义分割，数据集包括了具有不同复杂度的图像，从单个芽图像到具有多个芽的图像，以及人工标记的掩膜图像。标签包括4个不同的类别：背景、头部、身体和尾部。数据集还为图像分解任务提供了图像和掩膜，包括两个分离的芽图像和它们的组合形式。最后，还提供了芽的5个物理特征（头部长度、身体长度、身体厚度、尾部长度、重量）供基于图像的测量使用。

    We present 'CongNaMul', a comprehensive dataset designed for various tasks in soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate tasks such as image classification, semantic segmentation, decomposition, and measurement of length and weight. The classification task provides four classes to determine the quality of soybean sprouts: normal, broken, spotted, and broken and spotted, for the development of AI-aided automatic quality inspection technology. For semantic segmentation, images with varying complexity, from single sprout images to images with multiple sprouts, along with human-labelled mask images, are included. The label has 4 different classes: background, head, body, tail. The dataset also provides images and masks for the image decomposition task, including two separate sprout images and their combined form. Lastly, 5 physical features of sprouts (head length, body length, body thickness, tail length, weight) are provided for image-based measurement
    
[^68]: DeepHealthNet: 基于深度学习框架的青少年肥胖预测系统

    DeepHealthNet: Adolescent Obesity Prediction System Based on a Deep Learning Framework. (arXiv:2308.14657v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14657](http://arxiv.org/abs/2308.14657)

    本研究提出了一种基于深度学习框架的青少年肥胖预测系统DeepHealthNet，通过收集青少年的健康数据集，利用数据增强技术训练模型，提供个性化预测，并帮助青少年做出明智的健康决策。

    

    儿童和青少年肥胖率是一个全球性关注的问题，因为肥胖与慢性疾病和长期健康风险相关。人工智能技术作为一种有希望的解决方案出现，可以准确预测肥胖率，并为青少年提供个性化反馈。本研究强调早期识别和预防与肥胖相关的健康问题的重要性。为了发展出预测肥胖率和提供个性化反馈的强大算法，需要考虑身高、体重、腰围、卡路里摄入量、体育活动水平和其他相关健康信息。因此，通过收集321名青少年的健康数据集，我们提出了一种青少年肥胖预测系统，可以提供个性化预测，并帮助个人做出明智的健康决策。我们提出的深度学习框架DeepHealthNet使用了数据增强技术，有效地训练模型，即使在...

    Childhood and adolescent obesity rates are a global concern because obesity is associated with chronic diseases and long-term health risks. Artificial intelligence technology has emerged as a promising solution to accurately predict obesity rates and provide personalized feedback to adolescents. This study emphasizes the importance of early identification and prevention of obesity-related health issues. Factors such as height, weight, waist circumference, calorie intake, physical activity levels, and other relevant health information need to be considered for developing robust algorithms for obesity rate prediction and delivering personalized feedback. Hence, by collecting health datasets from 321 adolescents, we proposed an adolescent obesity prediction system that provides personalized predictions and assists individuals in making informed health decisions. Our proposed deep learning framework, DeepHealthNet, effectively trains the model using data augmentation techniques, even when 
    
[^69]: 从数据中基于光滑性先验推断超图结构

    Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])

    [http://arxiv.org/abs/2308.14172](http://arxiv.org/abs/2308.14172)

    本文提出了一种光滑性先验方法，用于从节点特征中推断超图的结构，并捕捉数据内在的关系。该方法不需要标记数据作为监督，能够推断出每个潜在超边的概率。

    

    超图在处理涉及多个实体的高阶关系数据中非常重要。在没有明确超图可用的情况下，希望能够从节点特征中推断出有意义的超图结构，以捕捉数据内在的关系。然而，现有的方法要么采用简单预定义的规则，不能精确捕捉潜在超图结构的分布，要么学习超图结构和节点特征之间的映射，但需要大量标记数据（即预先存在的超图结构）进行训练。这两种方法都局限于实际情景中的应用。为了填补这一空白，我们提出了一种新的光滑性先验，使我们能够设计一种方法，在没有标记数据作为监督的情况下推断出每个潜在超边的概率。所提出的先验表示超边中的节点特征与包含该超边的超边的特征高度相关。

    Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
    
[^70]: 探索大型语言模型用于知识图谱补全

    Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])

    [http://arxiv.org/abs/2308.13916](http://arxiv.org/abs/2308.13916)

    本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。

    

    知识图谱在众多人工智能任务中发挥着重要作用，但经常面临不完整性的问题。在本研究中，我们探索了利用大型语言模型（LLM）进行知识图谱补全的方法。我们将知识图谱中的三元组视为文本序列，并引入了一种创新的框架，称为知识图谱LLM（KG-LLM），来对这些三元组进行建模。我们的技术利用三元组的实体和关系描述作为提示，并利用响应进行预测。对各种基准知识图谱的实验表明，我们的方法在三元组分类和关系预测等任务中达到了最先进的性能。我们还发现，微调相对较小的模型（例如LLaMA-7B，ChatGLM-6B）优于最新的ChatGPT和GPT-4。

    Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
    
[^71]: 工业人工智能中的随机配置机

    Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])

    [http://arxiv.org/abs/2308.13570](http://arxiv.org/abs/2308.13570)

    本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。

    

    在工业人工智能（IAI）中，需要实时、准确的预测建模，神经网络在其中起到关键作用。工业人工智能中的神经网络需要强大的高性能计算设备来处理大量的浮点数据。本文基于随机配置网络（SCNs），提出了一种新的随机学习器模型，称为随机配置机（SCMs），以强调对于工业应用非常有用和有价值的有效建模和节约数据大小。与具有二值化实现的随机向量功能链接（RVFL）网络相比，SCMs的模型存储可以显著压缩，同时保持有利的预测性能。除了SCM学习器模型的架构和学习算法，作为本文的重要部分，我们还通过分析模型的复杂性提供了SCMs的学习能力的理论基础。实验研究也进行了。

    Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
    
[^72]: RemovalNet: DNN指纹去除攻击

    RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v1 [cs.CV])

    [http://arxiv.org/abs/2308.12319](http://arxiv.org/abs/2308.12319)

    本论文对DNN指纹去除攻击进行了全面的调查，并提出了一种名为RemovalNet的攻击方法，通过min-max双层优化来逃避模型所有权验证。攻击方法旨在去除指纹特定知识，并提取受害模型的通用语义知识来维持替代模型性能。

    

    随着深度神经网络(DNNs)性能的显著提升，DNNs在许多领域得到了广泛应用。因此，DNN模型已经成为一项宝贵的资产，其知识产权通过所有权验证技术（如DNN指纹）得到保护。然而，DNN指纹去除攻击的可行性及其潜在影响仍然是一个未解决的问题。本文首次对DNN指纹去除攻击进行了全面的调查。一般而言，DNN模型中包含的知识可以分为通用语义知识和指纹特定知识。为此，我们提出了一种基于min-max双层优化的DNN指纹去除攻击——RemovalNet，以逃避模型所有权验证。下层优化旨在去除指纹特定知识，而上层优化则在维持替代模型性能的同时提取受害模型的通用语义知识。

    With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance.
    
[^73]: 通过超出平衡状态的扩展动力学性能评估神经力场

    xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])

    [http://arxiv.org/abs/2308.11155](http://arxiv.org/abs/2308.11155)

    在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。

    

    神经力场已成为计算化学中的重要模型，取代了从头算的分子动力学中的量子化学计算。目前对神经力场的主要评估基准是MD17数据集及其后续扩展。这些数据集主要包含来自基态势能面平衡区域的几何结构，采样自直接绝热动力学。然而，许多化学反应涉及到较大的分子变形，特别是键断裂。我们展示了MD17数据集中内坐标和能量的约束分布，凸显了其在表示经历化学反应的系统方面的不足。为了解决这种采样限制，我们引入了xxMD（扩展激发态分子动力学）数据集，从非绝热动力学中派生。该数据集包含了从多参考波函数理论和密度泛函中确定的能量和力。

    Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
    
[^74]: RBA-GCN: 关系双层聚合图卷积网络用于情感识别

    RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v1 [cs.AI])

    [http://arxiv.org/abs/2308.11029](http://arxiv.org/abs/2308.11029)

    提出了RBA-GCN模型用于情感识别。该模型通过引入关系双层聚合和图生成模块，解决了GCN模型中的节点信息冗余和远距离上下文信息捕获问题。

    

    情感识别在对话中的应用受到了研究人员的关注，由于它具有广泛的应用。由于对话具有自然的图结构，很多基于图卷积网络（GCNs）的ERC模型方法取得了显著的结果。然而，传统GCNs的聚合方法存在节点信息冗余问题，导致节点辨别信息的丢失。此外，单层GCNs缺乏从图中捕获远距离上下文信息的能力。此外，大多数方法都是基于文本模态或将不同模态拼接在一起，导致捕捉模态间交互能力弱。为了解决这些问题，我们提出了关系双层聚合图卷积网络（RBA-GCN），它由三个模块组成：图生成模块（GGM）、基于相似性的簇构建模块（SCBM）和双层聚合模块。

    Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation 
    
[^75]: DocPrompt: 大规模连续预训练用于零样本和少样本文档问答

    DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])

    [http://arxiv.org/abs/2308.10959](http://arxiv.org/abs/2308.10959)

    本文提出了一个名为DocPrompt的方法，用于处理文档问答任务，具有强大的零样本和少样本性能。实验结果表明，DocPrompt模型经过连续预训练后在文档问答任务中表现优异，大大提高了交付效率和模型性能，降低了注释成本和劳动成本。

    

    本文提出了一个名为DocPrompt的方法，用于处理文档问答任务，具有强大的零样本和少样本性能。我们提出了一种新颖的弱监督数据生成方法、一种新颖的多阶段训练方法，以及一种新颖的理解模型和生成模型集成方法。实验结果表明，在文档问答任务中，经过连续预训练的DocPrompt模型明显优于现有的强基线模型。这种方法极大地提高了文档问答客户项目的交付效率和模型性能，降低了注释成本和劳动成本。我们的演示可以在https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout找到。

    In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model & generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
    
[^76]: 玩弄文字：比较ChatGPT和人类的词汇和词汇丰富度

    Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])

    [http://arxiv.org/abs/2308.07462](http://arxiv.org/abs/2308.07462)

    这篇论文比较了ChatGPT和人类在词汇和词汇丰富度方面的差异，研究发现使用ChatGPT等工具会对词汇使用和词汇丰富度产生影响，这可能会对语言演变产生影响。

    

    人工智能生成语言模型（如GPT）和ChatGPT等工具的引入引发了一场革命，可以改变文本生成的方式。这对读者的语言能力以及新型人工智能工具的培训是否会产生影响具有许多含义？它是否会影响语言的演变？我们关注语言的一个特定方面：词语；在编写给定文本时，使用ChatGPT等工具会增加或减少使用的词汇量或词汇丰富度（理解为书面或口头表达中使用的不同词汇数量）？这对词语有影响，因为未包含在人工智能生成的内容中的词语往往会变得越来越不受欢迎，并最终可能消失。在这项工作中，我们对ChatGPT和人类的词汇和词汇丰富度进行了初步比较。

    The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of
    
[^77]: 基于区块链的基金会模型系统的去中心化治理：探讨区块链在负责任的人工智能中的作用。

    Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI. (arXiv:2308.05962v1 [cs.SE])

    [http://arxiv.org/abs/2308.05962](http://arxiv.org/abs/2308.05962)

    本文探讨了基于基金会模型的人工智能系统在整个生命周期中所面临的治理挑战，并提出了利用区块链实现去中心化治理的架构。

    

    基金会模型因其卓越的能力和潜力在全球范围内越来越受到关注，能够执行各种任务。然而，人们担心基于基金会模型的人工智能系统是否得到了适当的治理，以确保其可信度，并防止可能对人类、社会和环境造成伤害的滥用。在本文中，我们确定了基金会模型人工智能系统在整个生命周期中面临的八个治理挑战，涉及治理的三个基本维度：决策权、激励机制和问责制。此外，我们探讨了区块链作为解决这些挑战的潜力，通过提供分布式账本来促进去中心化的治理。我们提出了一个架构，演示了如何利用区块链实现基金会模型人工智能系统的治理。

    Foundation models are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. Nevertheless, people are concerned about whether foundation model based AI systems are properly governed to ensure trustworthiness of foundation model based AI systems and to prevent misuse that could harm humans, society and the environment. In this paper, we identify eight governance challenges in the entire lifecycle of foundation model based AI systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. Furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. We present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based AI systems.
    
[^78]: "感觉像有第二个思维": 探究在大型语言模型中进行创意可写性预写的人机共创

    "It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])

    [http://arxiv.org/abs/2307.10811](http://arxiv.org/abs/2307.10811)

    通过三节次的定性研究，探究了人类与大型语言模型在预写过程中的合作模式，并发现了一个三阶段的人机共创过程：构思、启发和实施。在这个合作过程中，人类扮演着主导角色。

    

    预写是在第一稿之前发现和发展思想的过程，它需要发散性思维，通常涉及到无结构的策略，如图表、概述和自由写作等。虽然已经证明大型语言模型（LLMs）在各种任务中都是有用的，包括创意写作，但对用户如何与LLMs合作来支持预写的方式知之甚少。在这种创造性过程中，LLMs的首选合作角色和主动性也不明确。为了研究人类与LLMs在预写过程中的合作模式和动力学，我们进行了一项三节次的定性研究，与15位参与者进行了两个创造性任务：写故事和写口号。研究结果表明，在合作的预写过程中，似乎存在着一个三阶段迭代的人机共创过程，包括构思、启发和实施阶段。这个合作过程以人类在主导角色中取得了成功。

    Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
    
[^79]: 脱机强化学习中的离散策略的扩散策略

    Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04726](http://arxiv.org/abs/2307.04726)

    该论文介绍了一种名为状态重构扩散策略 (SRDP) 的新方法，该方法在最新的扩散策略类中引入了状态重构特征学习，以解决脱机强化学习中的分布偏移和有效表示策略的问题。

    

    脱机强化学习 (RL) 方法利用以前的经验来学习比用于数据收集的行为策略更好的策略。与行为克隆相反，行为克隆假设数据是从专家演示中收集的，而脱机 RL 可以使用非专家数据和多模态行为策略。然而，脱机 RL 算法在处理分布偏移和有效表示策略方面面临挑战，因为训练过程中缺乏在线交互。先前关于脱机 RL 的工作使用条件扩散模型来表示数据集中的多模态行为。然而，这些方法并没有针对缓解脱机分布状态泛化而制定。我们介绍了一种新的方法，名为状态重构扩散策略 (SRDP)，将状态重构特征学习纳入到最新的扩散策略类中，以解决脱机分布通用化问题。状态重构损失促进了更详细的描述。

    Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
    
[^80]: 使用语法演化自动设计语义相似性集合

    Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00925](http://arxiv.org/abs/2307.00925)

    本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。

    

    语义相似性度量在自然语言处理中被广泛应用于多种与计算机相关的任务。然而，没有单一的语义相似性度量适用于所有任务，研究人员经常使用集合策略来确保性能。本研究提出了一种自动设计语义相似性集合的方法。事实上，我们提出的方法首次使用语法演化来自动选择和聚合一组候选度量，以创建一个最大化与人类判断相关性的集合。该方法在多个基准数据集上进行了评估，并与最先进的集合进行了比较，结果显示它可以显著提高相似度评估的准确性，并在某些情况下优于现有方法。因此，我们的研究既展示了使用语法演化来自动比较文本的潜力，也证明了使用集合对语义相似性任务的益处。

    Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
    
[^81]: 非线性个性化预测的神经混合效应

    Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])

    [http://arxiv.org/abs/2306.08149](http://arxiv.org/abs/2306.08149)

    本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。

    

    个性化预测是一种机器学习方法，根据过去标记观测预测一个人未来的观测值，通常用于连续任务，例如预测日常情绪评分。在进行个性化预测时，模型可以结合两种趋势：（a）跨人共享的趋势，即个人通用趋势，例如周末更开心，和（b）每个人独特的趋势，即个人特定的趋势，例如每周有一次压力大的会议。混合效应模型是一种流行的统计模型，用于通过组合个人通用和个人特定参数来研究这两种趋势。尽管现在线性混合效应模型通过将其与神经网络整合而变得越来越流行，但这种整合目前仅限于线性个人特定参数：排除非线性个人特定趋势。在本文中，我们提出了神经混合效应（NME）模型，以优化非线性个人特定参数。

    Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
    
[^82]: 提高决策树解释性的有效性

    Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06777](http://arxiv.org/abs/2306.06777)

    该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。

    

    在基于表格数据的分类和预测中，人们经常使用基于树的模型。这可以在表格数据上与深度神经网络竞争[参见Grinsztajn等人，NeurIPS 2022，arXiv：2207.08815]，并且在某些条件下是可解释的。可解释性取决于树的深度和每个叶节点的准确性。在这里，我们训练了一个低深度的树，其目标是最小化每个叶节点上的最大错误分类，并从低深度树的每个叶节点“挂起”进一步的基于树的模型（例如无限深度的树）。低深度树易于解释，而综合低深度和挂起的基于树的模型的整体统计性能优于使用经典方法（例如CART）训练的无限深度决策树，并且与最先进的方法（例如优化的XGBoost）相当。

    In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
    
[^83]: 一种基于强化学习的方法促进算法代理与LLM之间的高效互动

    Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v1 [cs.AI])

    [http://arxiv.org/abs/2306.03604](http://arxiv.org/abs/2306.03604)

    本论文提出一种强化学习的中介模型，可实现代理与LLM之间高效经济有效的互动，提高效率和成本效益。

    

    大型语言模型(LLMs)包含从海量文本数据集中获取的大量世界知识。最近的研究表明，LLMs可以通过提供高层指令来协助算法代理解决具有复杂顺序决策的任务。然而，与LLMs进行交互可能耗时较长，因为在许多实际情况下，它们需要大量存储空间，只能部署在远程云服务器节点上。此外，使用商业LLMs可能成本很高，因为它们可能根据使用频率收费。本文探讨如何实现代理与LLM之间的高效和经济有效的互动。我们提出了一种基于强化学习的中介模型，以确定何时需要查询LLMs以完成目标任务的高级指令。在涉及规划子目标的4个MiniGrid环境上进行的实验表明，我们的方法可以学习解决目标任务，并提升了效率和成本效益。

    Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an algorithm agent in solving complex sequential decision making tasks in embodied environments by providing high-level instructions. However, interacting with LLMs can be time-consuming, as in many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable efficient and cost-effective interactions between the agent and an LLM. We propose a reinforcement learning based mediator model that determines when it is necessary to consult LLMs for high-level instructions to accomplish a target task. Experiments on 4 MiniGrid environments that entail planning sub-goals demonstrate that our method can learn to solve target tasks with o
    
[^84]: 在生物医学领域中的知识图谱嵌入：它们有用吗？对连接预测、规则学习和下游多药物任务的探究

    Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks. (arXiv:2305.19979v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19979](http://arxiv.org/abs/2305.19979)

    该论文研究了在生物医学领域中知识图谱嵌入的有效性和限制，通过在生物医学知识图谱上的实验，证明了最先进的模型在性能和下游用途方面的优势，同时提供了可解释的预测结果。

    

    知识图谱是表示和组织复杂生物医学数据的强大工具。已经提出了多种知识图谱嵌入算法来学习和完善知识图谱。然而，最近的一项研究表明，当应用于生物医学知识图谱时，这些嵌入算法的有效性有限，引发了对知识图谱嵌入在生物医学环境中是否存在限制的疑问。本研究旨在将最先进的知识图谱嵌入模型应用于最近的生物医学知识图谱BioKG，评估其性能和潜在的下游用途。在相同的生物医学知识图谱上，我们在HITS@10得分方面的性能改进了三倍。此外，我们通过基于规则的方法提供可解释的预测。通过对四个任务上表现最佳的模型进行评估，我们证明了知识图谱嵌入模型在实践中是可应用的。

    Knowledge graphs are powerful tools for representing and organising complex biomedical data. Several knowledge graph embedding algorithms have been proposed to learn from and complete knowledge graphs. However, a recent study demonstrates the limited efficacy of these embedding algorithms when applied to biomedical knowledge graphs, raising the question of whether knowledge graph embeddings have limitations in biomedical settings. This study aims to apply state-of-the-art knowledge graph embedding models in the context of a recent biomedical knowledge graph, BioKG, and evaluate their performance and potential downstream uses. We achieve a three-fold improvement in terms of performance based on the HITS@10 score over previous work on the same biomedical knowledge graph. Additionally, we provide interpretable predictions through a rule-based method. We demonstrate that knowledge graph embedding models are applicable in practice by evaluating the best-performing model on four tasks that r
    
[^85]: ChatGPT是否是终极编程助手-它有多远？

    Is ChatGPT the Ultimate Programming Assistant -- How far is it?. (arXiv:2304.11938v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2304.11938](http://arxiv.org/abs/2304.11938)

    ChatGPT作为编程助手在处理常见编程问题方面表现出色，但也存在一些局限性。

    

    最近，ChatGPT LLM引起了极大关注：它可以用作讨论源代码的聊天机器人，提示建议更改，提供描述甚至生成代码。典型的演示通常集中在现有的基准上，这些基准可能在模型训练中使用过（即数据泄漏）。为了评估将LLM用作程序员的有用助手机器人的可行性，我们必须评估其在未知问题上的实际能力以及其对各种任务的能力。在本文中，我们介绍了ChatGPT作为全自动编程助手的潜力的经验研究，重点关注代码生成、程序修复和代码摘要的任务。该研究调查了ChatGPT在常见编程问题上的表现，并将其与两个基准上的最先进方法进行了比较。在多个发现中，我们的研究显示ChatGPT在处理常见编程问题方面是有效的。然而，我们的实验也揭示了其局限性。

    Recently, the ChatGPT LLM has received great attention: it can be used as a bot for discussing source code, prompting it to suggest changes, provide descriptions or even generate code. Typical demonstrations generally focus on existing benchmarks, which may have been used in model training (i.e., data leakage). To assess the feasibility of using an LLM as a useful assistant bot for programmers, we must assess its realistic capabilities on unseen problems as well as its capabilities on various tasks. In this paper, we present an empirical study of ChatGPT's potential as a fully automated programming assistant, focusing on the tasks of code generation, program repair, and code summariziation. The study investigates ChatGPT's performance on common programming problems and compares it with state-of-the-art approaches on two benchmarks. Among several findings, our study shows that ChatGPT is effective in dealing with common programming problems. However, our experiments also reveal limitati
    
[^86]: OLISIA: 一个用于口语化对话状态跟踪的级联系统

    OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v1 [eess.AS])

    [http://arxiv.org/abs/2304.11073](http://arxiv.org/abs/2304.11073)

    我们提出了OLISIA，一个口语对话状态跟踪的级联系统，使用自动语音识别和DST模型，采用几个适应性策略来提高稳健性，并在DSTC11 Track3中取得第一名的好成绩。

    

    对话状态跟踪 (DST) 是口语对话系统的核心组成部分。然而，最近关于该任务的研究大多集中于聊天时的语料库，忽略了口语和书面语之间的差异。本文提出了 OLISIA，这是一个级联系统，它集成了自动语音识别 (ASR) 模型和 DST 模型。我们在 ASR 和 DST 模块中引入了几个适应性策略，以提高对口语对话的整合性和稳健性。经过这些策略的调整，我们的系统在 DSTC11 Track 3 中排名第一，这是一个评估口语 DST 性能的基准。我们进行了深入的结果分析，发现规范化 ASR 的输出和通过数据增强调整 DST 的输入，以及增加预训练模型的大小，都在降低书面和口语对话之间性能差异方面发挥了重要作用。

    Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language.In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations.With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations.
    
[^87]: G2PTL：适用于物流系统的交付地址预训练模型及其应用

    G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System. (arXiv:2304.01559v1 [cs.AI])

    [http://arxiv.org/abs/2304.01559](http://arxiv.org/abs/2304.01559)

    G2PTL是一种面向物流领域的预训练模型，结合了文本预训练的语义学习能力和图建模的地理关系编码能力，能有效地编码交付地址中的位置信息，并在物流系统中具有广泛的应用前景。

    

    作为物流系统的数据基础，基于文本的交付地址包含丰富且关键的位置信息。如何有效地编码交付地址是提高后续任务在物流系统中性能的核心任务。面向自然语言处理(NLP)的预训练模型(PTMs)已成为编码文本中语义信息的主要工具。虽然在许多任务中相当有前途，但这些基于NLP的PTMs未能编码交付地址中的地理知识，这在物流系统(如菜鸟系统)中大大削弱了与交付相关的任务的性能。为解决这个问题，我们提出了一种面向物流领域的域特定预训练模型，名为G2PTL，即交付地址地理关系-图预训练模型。G2PTL将文本预训练的语义学习能力与图建模的地理关系编码能力相结合。具体而言，我们首先利用真实的物流交付数据构建地理图，然后在图结构化数据上对模型进行预训练。实验结果表明，G2PTL能有效地编码基于文本的交付地址中的位置信息，并在与交付地址处理相关的任务中优于通用PTMs。

    Text-based delivery addresses, as the data foundation for logistics systems, contain abundant and crucial location information. How to effectively encode the delivery address is a core task to boost the performance of downstream tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural Language Process (NLP) have emerged as the dominant tools for encoding semantic information in text. Though promising, those NLP-based PTMs fall short of encoding geographic knowledge in the delivery address, which considerably trims down the performance of delivery-related tasks in logistic systems such as Cainiao. To tackle the above problem, we propose a domain-specific pre-trained model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in Logistics field. G2PTL combines the semantic learning capabilities of text pre-training with the geographical-relationship encoding abilities of graph modeling. Specifically, we first utilize real-world logistics delivery dat
    
[^88]: 知识增强的图神经网络

    Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])

    [http://arxiv.org/abs/2303.15487](http://arxiv.org/abs/2303.15487)

    KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。

    

    图数据是无处不在的，并且具有各种应用，例如自然科学、社交网络或语义网。尽管富含信息，但图形通常噪声和不完整。因此，图补全任务，如节点分类或链接预测，已经受到关注。一方面，神经方法（如图神经网络）已经被证明是处理噪声图的稳健工具。另一方面，符号方法可以对图进行精确推理。我们提出了KeGNN，这是一个用于在图数据上学习的神经符号框架，结合了两种范例，并允许将先前的知识集成到图神经网络模型中。从本质上讲，KeGNN由一个图神经网络组成，其中基于目标将知识增强层堆叠在其上，以使针对先前知识的预测得到优化。我们将KeGNN与两个标准图神经网络：图卷积网络和图注意力网络一起实例化。实验结果表明，将先前的知识集成到图神经网络模型中可以提高节点分类和链接预测任务的准确性。

    Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
    
[^89]: DR.CPO：通过迭代构建、随机放置和 HPR 遮蔽实现的多样化和逼真的三维增强

    DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])

    [http://arxiv.org/abs/2303.12743](http://arxiv.org/abs/2303.12743)

    该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。

    

    在自动驾驶中，数据增强常用于改进三维物体检测。最基本的方法包括插入复制对象和旋转和缩放整个训练帧。也已经开发了许多变体。然而，现有方法与现实世界的可能性相比相当有限。在这项工作中，我们开发了一种多样化和逼真增强方法，可以灵活地构造整体对象，自由地定位和旋转对象，并相应地应用自遮挡和外遮挡。为了提高整体对象构造的多样性，我们开发了一种迭代方法，将从现实世界观察到的多个对象随机组合成单个对象。与现有增强方法不同的是，构造的对象可以随机放置和旋转在训练帧中，因为适当的遮挡可以反映在最终整体对象中。最后，为了防止过度增强导致过拟合，我们介绍了一种分层遮挡概率设置，通过对象的位置和大小调整遮挡强度。

    In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
    
[^90]: 敏感度感知的视觉参数低效调整

    Sensitivity-Aware Visual Parameter-Efficient Tuning. (arXiv:2303.08566v1 [cs.CV])

    [http://arxiv.org/abs/2303.08566](http://arxiv.org/abs/2303.08566)

    本文提出了敏感度感知的视觉参数低效调整（SPT）方案，可以自适应地将可训练参数分配到任务特定的重要位置，以提高表示能力，适应预训练视觉模型到下游任务。

    

    视觉参数低效调整（VPET）已成为自适应预训练视觉模型到下游任务的强劲替代方法。现有VPET方法根据人工启发式方法将可训练参数引入不同任务的相同位置，忽略领域差异。本文提出了一种新颖的敏感度感知的视觉参数低效调整（SPT）方案，以自适应的方式分配可训练参数到任务特定的重要位置，给定所需的可调参数预算。本文首先依据数据的相关性快速识别特定任务所需调整的敏感参数，然后提升表示能力，增大重要的权重矩阵数量。

    Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of se
    
[^91]: 一个使用深度学习的文本语义通信在干扰环境下的性能限制

    Performance Limits of a Deep Learning-Enabled Text Semantic Communication under Interference. (arXiv:2302.14702v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.14702](http://arxiv.org/abs/2302.14702)

    这项研究探讨了在干扰环境下深度学习的文本语义通信系统的性能限制，发现当干扰信号强度增大时，该系统会产生语义上无关的句子。

    

    深度学习(Deep Learning)技术在语义通信(SemCom)中的应用成为了6G的一种推动因素，并承诺通过最小化无关信息传输来降低功耗、带宽消耗和传输延迟。然而，这种以语义为中心的设计的好处可能受到无线电频率干扰(RFI)造成的显著语义噪声的限制。为了解决这一知识空白并促进关于干扰鲁棒性的SemCom基础研究，我们研究了一种名为DeepSC的文本SemCom系统在存在(多干扰源) RFI的情况下的性能限制。通过引入一个基于概率的框架来进行SemCom，我们发现DeepSC在(多干扰源) RFI功率变得非常大时产生语义上无关的句子。我们还推导出DeepSC的实际限制和一个下限。

    A deep learning (DL)-enabled semantic communication (SemCom) has emerged as a 6G enabler while promising to minimize power usage, bandwidth consumption, and transmission delay by minimizing irrelevant information transmission. However, the benefits of such a semantic-centric design can be limited by radio frequency interference (RFI) that causes substantial semantic noise. The impact of semantic noise due to interference can be alleviated using an interference-resistant and robust (IR$^2$) SemCom design. Nevertheless, no such design exists yet. To shed light on this knowledge gap and stimulate fundamental research on IR$^2$ SemCom, the performance limits of a text SemCom system named DeepSC are studied in the presence of (multi-interferer) RFI. By introducing a principled probabilistic framework for SemCom, we show that DeepSC produces semantically irrelevant sentences as the power of (multi-interferer) RFI gets very large. We also derive DeepSC's practical limits and a lower bound on 
    
[^92]: 在具有缺失属性的图中进行公平属性补全

    Fair Attribute Completion on Graph with Missing Attributes. (arXiv:2302.12977v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12977](http://arxiv.org/abs/2302.12977)

    本文提出一种公平属性补全方法FairAC，用于处理具有缺失属性的图数据中的不公平性问题。FairAC采用注意机制处理属性缺失问题，并减轻属性和补全导致的两种不公平性，即属性不公平和拓扑不公平。

    

    解决图学习模型中的不公平性是一项具有挑战性的任务，因为图中的不公平问题涉及属性和拓扑结构。现有的公平图学习工作假设训练模型时所有节点的属性都是可用的，然后进行公平预测。然而，在实践中，由于数据缺失或隐私问题，一些节点的属性可能无法访问，这使得公平图学习更加具有挑战性。本文提出了一种公平属性补全方法FairAC，用于补全缺失信息并学习具有公平性的图节点嵌入。FairAC采用注意机制处理属性缺失问题，并减轻属性和补全导致的两种不公平性，即属性不公平和拓扑不公平。FairAC可以适应不同类型的同质图并为它们生成公平的嵌入，因此可以应用于大多数图数据场景。

    Tackling unfairness in graph learning models is a challenging task, as the unfairness issues on graphs involve both attributes and topological structures. Existing work on fair graph learning simply assumes that attributes of all nodes are available for model training and then makes fair predictions. In practice, however, the attributes of some nodes might not be accessible due to missing data or privacy concerns, which makes fair graph learning even more challenging. In this paper, we propose FairAC, a fair attribute completion method, to complement missing information and learn fair node embeddings for graphs with missing attributes. FairAC adopts an attention mechanism to deal with the attribute missing problem and meanwhile, it mitigates two types of unfairness, i.e., feature unfairness from attributes and topological unfairness due to attribute completion. FairAC can work on various types of homogeneous graphs and generate fair embeddings for them and thus can be applied to most d
    
[^93]: 神经系统的系统辨识：如果我们理解正确，我们会知道吗？

    System identification of neural systems: If we got it right, would we know?. (arXiv:2302.06677v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2302.06677](http://arxiv.org/abs/2302.06677)

    该论文研究了神经系统的系统辨识问题，通过对比人工神经网络与生物神经元的记录来验证模型的有效性。然而，系统辨识的性能很大程度上取决于刺激图像等因素，并且对识别更高级别架构图案方面存在局限性。

    

    人工神经网络被提议作为大脑的部分模型。将这些网络与生物神经元的记录进行比较，并认为在重现神经反应方面的良好性能支持模型的有效性。一个关键问题是，这种系统辨识方法对我们了解脑部计算有多大帮助。它是否能验证某种模型架构优于另一种？我们评估了最常用的比较技术，如线性编码模型和中心核对齐，通过用已知的真实模型替换脑部记录来正确识别模型。系统辨识的性能相当不稳定，它还显著依赖于独立于真实模型架构的因素，如刺激图像。此外，我们展示了使用功能相似性评分在识别更高级别架构图案方面的局限性。

    Artificial neural networks are being proposed as models of parts of the brain. The networks are compared to recordings of biological neurons, and good performance in reproducing neural responses is considered to support the model's validity. A key question is how much this system identification approach tells us about brain computation. Does it validate one model architecture over another? We evaluate the most commonly used comparison techniques, such as a linear encoding model and centered kernel alignment, to correctly identify a model by replacing brain recordings with known ground truth models. System identification performance is quite variable; it also depends significantly on factors independent of the ground truth architecture, such as stimuli images. In addition, we show the limitations of using functional similarity scores in identifying higher-level architectural motifs.
    
[^94]: 基于点云的毫米波通信主动链路质量预测

    Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.00752](http://arxiv.org/abs/2301.00752)

    本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。

    

    本研究展示了基于点云的毫米波（mmWave）通信的主动链路质量预测的可行性。以往的研究提出了基于机器学习的方法，利用深度图像的时间序列来预测未来时间段的接收信号强度，以缓解行人阻挡因素对mmWave通信的影响。但是，由于隐私问题，这些基于图像的方法的适用性有限，因为摄像头图像可能包含敏感信息。本研究提出了一种基于点云的mmWave链路质量预测方法，并通过实验证明其可行性。点云将三维空间表示为点集，其空间性质更加稀疏，不太可能包含敏感信息，并且还提供了3D位置和运动信息，这对了解涉及行人的无线电传播环境是必要的。

    This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
    
[^95]: Deanthropomorphising NLP：语言模型可以意识到吗？

    Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11483](http://arxiv.org/abs/2211.11483)

    本文讨论了关于使用Transformer架构的预训练语言模型LaMDA是否具有意识的说法。作者认为语言模型不可能具有意识，而LaMDA没有比其他类似模型更具先进性。

    

    本文旨在对最近有关使用Transformer模型架构的预训练语言模型LaMDA具有意识的说法进行讨论。我们认为这样的语言模型不可能具有意识，而LaMDA并没有比其他类似模型更具先进性。我们通过综合信息理论对Transformer架构进行分析来证明这一点。我们认为这些有意识的说法是NLP报道中使用拟人化语言的更广泛倾向的一部分。无论这些说法的真实性如何，我们认为现在是评估语言建模进展并考虑该任务的伦理影响的适当时机。为了使本文有助于NLP社区以外的读者，我们还提供了一些NLP基础知识的介绍。

    This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
    
[^96]: 从相邻的染色组织学片中学习黑色素细胞掩膜

    Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2211.00646](http://arxiv.org/abs/2211.00646)

    本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。

    

    黑色素瘤是最具侵袭性的皮肤癌之一，导致大部分皮肤癌死亡。然而，病理学家对黑色素瘤的诊断可靠性较低。由于黑色素瘤是黑色素细胞的肿瘤，需要开发一种与病理学家的差异无关并能自动进行像素级注释的黑色素细胞分割工具。然而，大规模病理学家标注是不现实的。在本文中，我们提出了一种方法，使用邻近组织切片上的偶联免疫组织化学（IHC）染色片，训练深度神经网络进行黑色素细胞分割，虽然很难有完美的标签，但达到了0.64的平均IOU。

    Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
    
[^97]: Bayesian MAML的超网络方法

    Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02796](http://arxiv.org/abs/2210.02796)

    该论文提出了一种名为贝叶斯HMAML的新框架，利用超网络进行权重更新，以解决MAML的过拟合和不确定性问题。

    

    少样本学习算法的主要目标是能够从少量数据中进行学习。其中一种最流行且优雅的少样本学习方法是模型无关元学习（MAML）。该方法的主要思想是学习元模型的共享通用权重，然后将其适应于特定任务。然而，该方法存在过拟合问题，因为数据量有限，很难准确量化不确定性。贝叶斯方法可以通过学习权重分布而不是点值权重来缓解这些问题。不幸的是，先前修改的MAML方法由于高斯分布的简单性、基于梯度的类MAML权重更新或强制执行相同结构的通用权重和适应权重，受到一定的限制。在本文中，我们提出了一种新的贝叶斯MAML框架，称为贝叶斯HMAML，它利用超网络进行权重更新。

    The main goal of Few-Shot learning algorithms is to enable learning from small amounts of data. One of the most popular and elegant Few-Shot learning approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this method is to learn the shared universal weights of a meta-model, which are then adapted for specific tasks. However, the method suffers from over-fitting and poorly quantifies uncertainty due to limited data size. Bayesian approaches could, in principle, alleviate these shortcomings by learning weight distributions in place of point-wise weights. Unfortunately, previous modifications of MAML are limited due to the simplicity of Gaussian posteriors, MAML-like gradient-based weight updates, or by the same structure enforced for universal and adapted weights.  In this paper, we propose a novel framework for Bayesian MAML called BayesianHMAML, which employs Hypernetworks for weight updates. It learns the universal weights point-wise, but a probabilistic structure is 
    
[^98]: 基于视觉对应的解释提高了AI的鲁棒性和人机团队准确性

    Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.00780](http://arxiv.org/abs/2208.00780)

    该论文提出了基于视觉对应的解释方法，用于改善AI的鲁棒性和人机团队的准确性。在大规模的人类研究中，该方法被发现比kNN解释更有用，帮助用户更准确地拒绝AI的错误决策。同时，该方法在超出分布数据集上改进了性能，并实现了互补人机团队准确性的可能性。

    

    在许多高风险应用中，解释人工智能（AI）的预测变得越来越重要，甚至是必不可少的，因为人类是最终的决策者。在这项工作中，我们提出了两种新颖的自解释图像分类器架构，它们首先通过利用查询图像和示例之间的视觉对应关系进行解释，然后进行预测（与事后解释相对）。我们的模型在超出分布（OOD）数据集上一致改进（1到4个点），而在分布测试上表现略次于ResNet-50和一个k最近邻分类器（kNN）（下降1到2个点）。通过对ImageNet和CUB进行的大规模人类研究，我们发现基于对应关系的解释比kNN解释对用户更有用。我们的解释帮助用户更准确地拒绝AI的错误决策，胜过所有其他测试方法。有趣的是，我们首次展示了实现互补人机团队准确性的可能性。

    Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI tea
    
[^99]: DALL-Eval: 探究文本到图像生成模型的推理能力和社会偏见

    DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models. (arXiv:2202.04053v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.04053](http://arxiv.org/abs/2202.04053)

    本研究探究了不同文本到图像模型的视觉推理能力和社会偏见。尽管模型在图像生成方面表现出高质量的结果，但在对象计数和空间关系理解能力方面仍存在与上界准确性之间的巨大差距。此外，我们还评估了模型在性别和肤色方面的偏见。

    

    最近，DALL-E，一种多模态的转换语言模型及其变种，包括扩散模型，展示了高质量的文本到图像生成能力。然而，尽管有逼真的图像生成结果，对于如何评估这些模型还没有进行详细的分析。在这项工作中，我们研究了不同文本到图像模型的视觉推理能力和社会偏见，涵盖了多模态转换语言模型和扩散模型。首先，我们量化了三种视觉推理能力：对象识别，对象计数和空间关系理解。为此，我们提出了PaintSkills，这是一个用于衡量这些能力的组合式诊断评估数据集。尽管具有高度逼真的图像生成能力，但在对象计数和空间关系理解能力方面，最近模型的性能与上界准确性之间存在很大差距。其次，我们通过测量性别和肤色偏见来评估模型的性别和肤色偏见。

    Recently, DALL-E, a multimodal transformer language model, and its variants, including diffusion models, have shown high-quality text-to-image generation capabilities. However, despite the realistic image generation results, there has not been a detailed analysis of how to evaluate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer language models and diffusion models. First, we measure three visual reasoning skills: object recognition, object counting, and spatial relation understanding. For this, we propose PaintSkills, a compositional diagnostic evaluation dataset that measures these skills. Despite the high-fidelity image generation capability, a large gap exists between the performance of recent models and the upper bound accuracy in object counting and spatial relation understanding skills. Second, we assess the gender and skin tone biases by measuring the gender/ski
    
[^100]: 将归纳推理和演绎推理相结合进行不完整知识图谱上的查询回答

    Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs. (arXiv:2106.14052v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.14052](http://arxiv.org/abs/2106.14052)

    该论文研究了将归纳推理和演绎推理相结合应用于不完整知识图谱上的查询回答。通过将本体论纳入基于嵌入的查询回答模型，采用不同的集成策略和损失函数调整，取得了20%到的性能提升。

    

    当前用于不完整知识图谱的基于嵌入的查询回答方法只集中在归纳推理上，即通过从数据中学习模式来预测答案，并缺乏进行演绎推理的能力，演绎推理需要应用领域知识来推断更多信息。为了解决这个缺点，我们研究了将本体论纳入基于嵌入的查询回答模型的问题，定义了基于嵌入的本体中介查询回答任务。我们提出了各种集成策略，包括（1）不同的本体驱动数据增强技术和（2）适应本体公理的损失函数的调整。我们设计了基于LUBM和NELL知识图谱的新颖基准，并在其上评估了我们的方法。在需要归纳和演绎推理的设置中，我们取得了从20%到的提升。

    Current methods for embedding-based query answering over incomplete Knowledge Graphs (KGs) only focus on inductive reasoning, i.e., predicting answers by learning patterns from the data, and lack the complementary ability to do deductive reasoning, which requires the application of domain knowledge to infer further information. To address this shortcoming, we investigate the problem of incorporating ontologies into embedding-based query answering models by defining the task of embedding-based ontology-mediated query answering. We propose various integration strategies into prominent representatives of embedding models that involve (1) different ontology-driven data augmentation techniques and (2) adaptation of the loss function to enforce the ontology axioms. We design novel benchmarks for the considered task based on the LUBM and the NELL KGs and evaluate our methods on them. The achieved improvements in the setting that requires both inductive and deductive reasoning are from 20% to 
    
[^101]: 在随机博弈中学习时态任务的最优策略

    Learning Optimal Strategies for Temporal Tasks in Stochastic Games. (arXiv:2102.04307v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2102.04307](http://arxiv.org/abs/2102.04307)

    本论文提出了一种无模型强化学习方法，用于从给定的LTL规范中学习最优控制策略，即使环境完全未知。该方法通过将问题建模为控制器和对抗环境之间的随机博弈，最大化满足LTL规范的概率，抵抗最坏情况下的环境行为。

    

    从线性时态逻辑（LTL）规范中进行合成可以为在随机和潜在对抗环境中运行的系统提供保证的控制器。然而，自动合成工具需要一个环境模型来构建控制器。在这项工作中，我们引入了一种无模型强化学习（RL）方法，用于从给定的LTL规范中推导控制器，即使环境完全未知。我们将问题建模为控制器和对抗环境之间的随机博弈（SG），然后学习最优控制策略，以最大化满足LTL规范的概率，抵抗最坏情况下的环境行为。我们首先使用从给定LTL规范翻译的确定性奇偶自动机（DPA）构建一个乘积博弈。通过从DPA接受条件导出不同的奖励和折扣因子，我们将最大化最坏情况下满足LTL规范的概率的问题简化为一个强化学习问题。

    Synthesis from linear temporal logic (LTL) specifications provides assured controllers for systems operating in stochastic and potentially adversarial environments. Automatic synthesis tools, however, require a model of the environment to construct controllers. In this work, we introduce a model-free reinforcement learning (RL) approach to derive controllers from given LTL specifications even when the environment is completely unknown. We model the problem as a stochastic game (SG) between the controller and the adversarial environment; we then learn optimal control strategies that maximize the probability of satisfying the LTL specifications against the worst-case environment behavior. We first construct a product game using the deterministic parity automaton (DPA) translated from the given LTL specification. By deriving distinct rewards and discount factors from the acceptance condition of the DPA, we reduce the maximization of the worst-case probability of satisfying the LTL specifi
    

