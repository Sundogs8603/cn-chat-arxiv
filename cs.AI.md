# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning](https://arxiv.org/abs/2402.06619) | 本研究的主要目标是通过人工筛选的指令遵循数据集来弥合不同语言之间的差距，并且创造了迄今为止最大的多语言收藏品，包括513亿个实例。 |
| [^2] | [TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations](https://arxiv.org/abs/2402.06608) | 该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。 |
| [^3] | [RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization](https://arxiv.org/abs/2402.06606) | RQP-SGD是一种结合了差分隐私随机梯度下降和随机量化的新方法，用于在边缘部署的低内存机器学习模型训练中实现隐私保护。通过研究其在具有凸目标和量化约束的ML任务上的效用收敛性，并证明了其相对确定性量化的有效性。 |
| [^4] | [On the Out-Of-Distribution Generalization of Multimodal Large Language Models](https://arxiv.org/abs/2402.06599) | 通过对多模式大型语言模型（MLLMs）进行全面评估，研究发现它们在超出训练领域的泛化方面存在困难，主要原因是映射不足。通过上下文学习（ICL）可以显著提升泛化能力，克服泛化障碍。 |
| [^5] | [Understanding the Weakness of Large Language Model Agents within a Complex Android Environment](https://arxiv.org/abs/2402.06596) | 这项研究揭示了在复杂的Android环境中，大型语言模型代理面临的挑战，提出了AndroidArena环境和基准来评估其性能，以及降低人力成本的方法。 |
| [^6] | [Predictive representations: building blocks of intelligence](https://arxiv.org/abs/2402.06590) | 预测性表征可能是智能的多功能基石。 |
| [^7] | [G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German](https://arxiv.org/abs/2402.06584) | G-SciEdBERT是一种上下文化德语科学教育BERT，用于评分德语科学任务的书面回答。通过在大规模德语科学回答语料库上进行预训练，并在评分准确性方面取得了10%的改善。 |
| [^8] | [What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices](https://arxiv.org/abs/2402.06563) | 本研究使用统计方法和机器学习，通过分析儿科急诊数据和创伤伤害数据库，揭示了医疗实践模式与丢失数据之间的关联，并提出了临床数据插补的方法。这对于减少分析偏见、提高临床决策的有效性非常重要。 |
| [^9] | [Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following](https://arxiv.org/abs/2402.06559) | 本文提出了一种Diffusion-ES方法，它结合了无梯度优化和轨迹去噪技术，用于优化黑盒非可微目标。该方法通过从扩散模型中采样轨迹，并使用黑盒奖励函数对其进行评分，实现了更高的多样性和可解释性。 |
| [^10] | [The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model](https://arxiv.org/abs/2402.06557) | 本文介绍了量化布尔贝叶斯网络（QBBN），它提供了逻辑和概率推理的统一视角，并解决了大型语言模型（LLM）的妄想问题。通过创建一阶演算法的键值版本，QBBN能够表示人类语言背后的逻辑推理。精确推理是不可解的，但可以使用循环信念传播（LBP）进行推理。 |
| [^11] | [Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA](https://arxiv.org/abs/2402.06549) | 该研究利用GPT-4和LLaMA模型，通过检索增强和重新排序的方式，在CASE 2024共享任务中取得了显著的成果，特别是在仇恨事件检测和目标识别方面表现出色。 |
| [^12] | [Calibrating Long-form Generations from Large Language Models](https://arxiv.org/abs/2402.06544) | 该论文提出了一个统一的校准框架，用于校准大型语言模型的长篇生成。在该框架中，作者开发了三个度量指标用于评估模型的校准性，并提出了两种置信度引导方法。实验证明，更大的模型不一定能保证更好的校准。 |
| [^13] | [Generative Adversarial Bayesian Optimization for Surrogate Objectives](https://arxiv.org/abs/2402.06532) | 提出了生成对抗贝叶斯优化（GABO）算法，通过使用自适应源批评家正则化，将优化轨迹限制在代理函数可靠的区域内，解决了离线模型基于策略优化中代理模型预测不准确的问题。在多个离线优化任务中，GABO表现优于现有基准方法。 |
| [^14] | [Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification](https://arxiv.org/abs/2402.06530) | 本研究提出了一种新的方法，使用基于超声心动图的一种基于多模态复合核策略的单一类别分类算法来进行早期心肌梗死的检测。这种方法通过优化投影矩阵和特征转化，提高了心肌梗死检测的能力。 |
| [^15] | [Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty](https://arxiv.org/abs/2402.06529) | 本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。 |
| [^16] | [Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions](https://arxiv.org/abs/2402.06509) | 本文研究了模型不确定性与人类不确定性之间的关系，并提出了一种基于模型不确定性估计的生成澄清问题的方法，为对话系统在决定何时提问提供了重要指导，并且在任务成功率方面取得了显著改进。 |
| [^17] | [Classifying point clouds at the facade-level using geometric features and deep learning networks](https://arxiv.org/abs/2402.06506) | 该论文提出了一种方法，利用几何特征和深度学习网络对立面级别的点云进行分类。实验证明，融合几何特征可以提高深度学习方法的性能，并促进语义分割的进步。 |
| [^18] | [ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies](https://arxiv.org/abs/2402.06503) | ACTER是一个算法，用于生成可行的反事实序列，提供关于如何避免RL策略失败的可行建议。 |
| [^19] | [Scalable Interactive Machine Learning for Future Command and Control](https://arxiv.org/abs/2402.06501) | 未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。本论文通过利用互动式机器学习方法，结合人工智能和人类智能，以提高C2运作的适应性和效率。 |
| [^20] | [On the Fly Detection of Root Causes from Observed Data with Application to IT Systems](https://arxiv.org/abs/2402.06500) | 本文介绍了一种用于IT系统的新结构因果模型和一种快速检测异常根本原因的算法。该算法可在离线数据中进行因果发现，并在在线数据中遇到新异常时进行子图遍历。实验证明了方法的优越性能。 |
| [^21] | [Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings](https://arxiv.org/abs/2402.06492) | 本论文提出了SQ-Transformer模型，通过在嵌入和注意层中引入结构化量化的方法，无论训练集的复杂度如何，都能够明确地鼓励模型在编码句子时保持系统性。 |
| [^22] | [Le Nozze di Giustizia. Interactions between Artificial Intelligence, Law, Logic, Language and Computation with some case studies in Traffic Regulations and Health Care](https://arxiv.org/abs/2402.06487) | 本文旨在向法律界人士介绍数学逻辑与人工智能的基本原理及其在交通法规和医疗保健中的应用。同时探讨数学逻辑对人工智能应用的限制和影响。 |
| [^23] | ["When He Feels Cold, He Goes to the Seahorse"-Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy](https://arxiv.org/abs/2402.06472) | 这项研究对使用生成式人工智能作为表达材料的治疗性故事创作进行了调查，发现家庭创造性地融合了人工智能和传统表达材料来外化他们的思想和感受。 |
| [^24] | [V-STaR: Training Verifiers for Self-Taught Reasoners](https://arxiv.org/abs/2402.06457) | V-STaR利用正确和不正确的解决方案训练验证器，用于选择模型生成的解决方案，实现了自我改进和验证方法在常见代码生成和数学推理任务中达到4%到17%的测试准确率提升。 |
| [^25] | [Hierarchical Transformers are Efficient Meta-Reinforcement Learners](https://arxiv.org/abs/2402.06402) | 层次化Transformer是一种高效的元强化学习方法，通过有效地提取过去经验的信息丰富资源，并应用于新的环境中，实现了超越最先进方法的元训练效果，并显著提高了泛化能力和学习效率。 |
| [^26] | [Finding hardness reductions automatically using SAT solvers](https://arxiv.org/abs/2402.06397) | 本文呈现了一种使用SAT求解器自动发现困难归约的方法，通过对符号映射的完成问题进行深入研究，分类了数千个NP完全的结构，并提出了一个将内部三元组系统推广到更高维度的无限家族的结构。 |
| [^27] | [Human Aesthetic Preference-Based Large Text-to-Image Model Personalization: Kandinsky Generation as an Example](https://arxiv.org/abs/2402.06389) | 本文介绍了一种无提示的生成方法，使用户能够自动生成个性化的艺术风格的绘画内容，这些内容融入了他们的审美偏好。 |
| [^28] | [On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit](https://arxiv.org/abs/2402.06388) | 该论文证明了当学习速率按照逆时间衰减规则时，随机梯度下降（SGD）的收敛速度，并应用于修改的带有L2正则化的策略梯度多臂赌博机（MAB）的收敛性分析。 |
| [^29] | [High-Precision Geosteering via Reinforcement Learning and Particle Filters](https://arxiv.org/abs/2402.06377) | 基于强化学习和粒子滤波的地质定向方法，通过实时数据处理实现高精度地质定向决策 |
| [^30] | [CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models](https://arxiv.org/abs/2402.06360) | CoSearchAgent是一种基于大语言模型的轻量级协作搜索代理，可作为Slack插件在多方对话中支持协作搜索。 |
| [^31] | [Modelling Human Values for AI Reasoning](https://arxiv.org/abs/2402.06359) | 本研究详细介绍了一个关于人类价值观的形式化模型，并展示了它在AI推理中的应用。研究通过基于社会心理学研究的关键思想，为AI系统与人类价值观的一致性提供了具体的计算表示。 |
| [^32] | [ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs](https://arxiv.org/abs/2402.06334) | ExaRanker-Open 是一种使用开源LLMs进行IR的方法，通过适应和探索开源语言模型来生成解释。研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。 |
| [^33] | [Prompt Learning on Temporal Interaction Graphs](https://arxiv.org/abs/2402.06326) | 这个论文提出了一种在时间交互图上进行提示学习的方法，以解决当前模型在预训练和下游预测阶段所面临的时间差异和语义差异的问题。 |
| [^34] | [A New Approach to Voice Authenticity](https://arxiv.org/abs/2402.06304) | 这篇论文提出了一种新的思路，挑战传统的将音频定义为“假”或“真”的范式。研究者们将注意力放在了确定“语音编辑”上，这包括传统的修改以及TTS合成和VC系统。他们还提供了一个新的挑战数据集。 |
| [^35] | [A Functional Analysis Approach to Symbolic Regression](https://arxiv.org/abs/2402.06299) | 本研究提出了一种基于函数分析的新型符号回归方法，称为Fourier Tree Growing (FTG)，通过在不同空间直接进行优化来避免复杂的符号表示，实验表明该算法在一维基准问题上与传统的遗传编程方法相比具有显著的性能提升。 |
| [^36] | [AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems](https://arxiv.org/abs/2402.06287) | 本调查提出了混合决策系统的分类方法，为理解如何对人与机器之间的交互进行建模提供了概念性和技术性的框架。 |
| [^37] | [LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education](https://arxiv.org/abs/2402.06264) | 本研究利用多模态大型语言模型（MLLM）开发了LLaVA-Docent模型，以支持艺术鉴赏教育。通过综述文献和专家咨询，构建了数据框架，并使用该框架生成了虚拟对话数据集用于训练MLLM。该研究对于解决传统艺术鉴赏教育中的资源限制和主流教育中的科学技术工程和数学偏重具有重要意义。 |
| [^38] | [On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference](https://arxiv.org/abs/2402.06262) | 本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。 |
| [^39] | [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255) | 本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。 |
| [^40] | [Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants](https://arxiv.org/abs/2402.06229) | 本文研究了增强AI助手的对话能力，设计了一个名为Robin的增强型对话型AI助手，通过使用交互模式和对话分析，提高了其在调试方面的性能。 |
| [^41] | [The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate](https://arxiv.org/abs/2402.06204) | 本文讨论了生成AI评估中的悖论，并发现了大型语言模型在评估任务中性能较差的现象。研究突出了需要检查模型作为评估者的忠实度和可信度，以及探索生成优秀与评估能力之间的关联。 |
| [^42] | [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196) | 大型语言模型（LLMs）吸引了很多关注，因为它们在自然语言任务上的强大表现。该研究领域发展迅速，包括了各种著名的LLMs、构建和增强LLMs的技术、以及流行的LLM数据集和评估指标。 |
| [^43] | [A self-supervised framework for learning whole slide representations](https://arxiv.org/abs/2402.06188) | 这个论文提出了一个自监督学习框架（S3L），用于学习整个切片的表示。它结合了变压器模型的视觉和语言建模策略，通过生成配对视图进行自监督学习，以实现高质量的WSI视觉特征学习。 |
| [^44] | [Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss](https://arxiv.org/abs/2402.06187) | Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。 |
| [^45] | [Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters](https://arxiv.org/abs/2402.06185) | 该研究开发了一个名为SpinePose的人工智能工具，可以准确预测脊盘盆参数，无需手动输入。 |
| [^46] | [MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models](https://arxiv.org/abs/2402.06178) | 本文介绍了一个通过扩散模型实现零样本文本到音乐的编辑的新方法，可以修改生成音乐的特定属性而保持其他方面不变，并展示了其在风格和音色转换方面的优越性能以及在实际音乐编辑场景中的实用性。 |
| [^47] | [Learning Contrastive Feature Representations for Facial Action Unit Detection](https://arxiv.org/abs/2402.06165) | 这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。 |
| [^48] | [Assortment Planning with Sponsored Products](https://arxiv.org/abs/2402.06158) | 本研究主要关注零售中带有赞助产品的品类规划挑战并将其建模为组合优化任务，以实现在考虑赞助产品的情况下优化预期收入的目的。 |
| [^49] | [DeAL: Decoding-time Alignment for Large Language Models](https://arxiv.org/abs/2402.06147) | DeAL是一个允许用户自定义奖励函数并实现解码时对齐LLMs的框架。 |
| [^50] | [Rethinking Node-wise Propagation for Large-scale Graph Learning](https://arxiv.org/abs/2402.06128) | 提出了适应性拓扑感知传播（ATP）方法，以应对大规模图学习中节点传播的问题。此方法能对不同节点的拓扑角色进行个性化传播，并减少传播带来的偏差和额外开销。 |
| [^51] | [Learn To be Efficient: Build Structured Sparsity in Large Language Models](https://arxiv.org/abs/2402.06126) | 本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。 |
| [^52] | [ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling](https://arxiv.org/abs/2402.06118) | ViGoR通过细粒度奖励建模提高了大型视觉语言模型在视觉对接方面的性能，通过人工评估和自动化方法有效地解决了视觉对接中的误差问题。 |
| [^53] | [LLMs for Coding and Robotics Education](https://arxiv.org/abs/2402.06116) | 本文介绍了机器人编程教育的重要趋势，并利用大型语言模型测试了机器人代码生成任务。结果显示，GPT-4V在所有测试中表现优异，但在生成块状图像方面存在困难。 |
| [^54] | [Multiple Instance Learning for Cheating Detection and Localization in Online Examinations](https://arxiv.org/abs/2402.06107) | 本文提出了一种基于多实例学习的作弊检测框架CHEESE，该框架综合考虑了头部姿势、凝视角度、身体姿势和背景信息等特征，并通过标签生成器和特征编码器实现了作弊行为的检测和定位。 |
| [^55] | [Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data](https://arxiv.org/abs/2402.06104) | 该论文提出了一种名为FAR的方法，通过捕捉函数导数来更好、更高效地拟合底层真实函数。在合成数据集和八个真实世界任务中证明了该方法的有效性。 |
| [^56] | [Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning](https://arxiv.org/abs/2402.06098) | 知识图谱学习面临着缺乏专家知识整合、节点度数极端性不稳定、缺乏不确定性和相关性的考虑以及缺乏可解释性的挑战。现有的解决尝试大多是孤立的，需要综合考虑这些问题的解决方案。 |
| [^57] | [TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models](https://arxiv.org/abs/2402.06097) | 这项研究引入了一种名为TWIG的新颖模型，可以通过拓扑特征学习权重来模拟KGE模型的输出，有效减少了参数数量，并具有预先优化超参数和跨图泛化的能力。 |
| [^58] | [Rhizomes to Load Balance Skewed In-Degree Distributions](https://arxiv.org/abs/2402.06086) | 本文通过引入根茎构造的方法来解决图中高入度分布导致的负载不平衡问题，在大规模芯片上对BFS图遍历性能进行了加速和优化。 |
| [^59] | [SubGen: Token Generation in Sublinear Time and Memory](https://arxiv.org/abs/2402.06082) | 这项工作提出了一种名为SubGen的高效缓存压缩技术，通过在Attention模块中进行在线聚类和采样，实现了子线性的内存占用和时间复杂度，并建立了一个紧密的误差界。 |
| [^60] | [DiscDiff: Latent Diffusion Model for DNA Sequence Generation](https://arxiv.org/abs/2402.06079) | 本文介绍了一种新的框架用于生成DNA序列，包括一个用于生成离散DNA序列的潜在扩散模型和一个用于改进序列的后训练算法。我们的方法不仅在DNA序列生成方面树立了新的标准，并且在生成短序列和长序列方面表现出优越性能。此外，我们还引入了一个多物种的DNA生成数据集。这项研究将推动DNA的生成建模，并对基因治疗和蛋白质生产产生潜在影响。 |
| [^61] | [Gaussian Mixture Models for Affordance Learning using Bayesian Networks](https://arxiv.org/abs/2402.06078) | 本文使用高斯混合模型来处理不确定性，提供更准确的可承受性学习方法。 |
| [^62] | [Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making](https://arxiv.org/abs/2402.06075) | 针对决策支持下的战争游戏，本论文提出了规模化人工智能的发展并与人类判断相结合的重要性。通过提高全域意识、改善决策速度和质量、提供创新行动建议以及更快速地应对对手行动，我们能够更好地应对现代挑战和困境，增强人类决策的指导和增强。 |
| [^63] | [Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models](https://arxiv.org/abs/2402.06053) | 通过使用大型语言模型进行语义遍历，我们提出了一种新颖的方法来在创新问题和解决方案空间中探索。这种方法不仅可以找到多种问题的解决方案，还可以改进和澄清问题陈述。 |
| [^64] | [Limits of Large Language Models in Debating Humans](https://arxiv.org/abs/2402.06049) | 大型语言模型在与人类辩论中的能力有限，尽管它们能够融入和促进人类的工作效率，但在辩论中的说服力较弱。在成为可行的辩手之前，LLMs需要进一步发展。 |
| [^65] | [Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap](https://arxiv.org/abs/2402.06046) | 2023年10月，一起GM Cruise机器人出租车与行人相撞的事故给公司造成了巨大的动荡，同时也揭示了该公司在处理事故后的失误。这一事件的解剖提供了在技术、操作安全实践和组织反应方面的安全教训。 |
| [^66] | [OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2402.06044) | OpenToM是一个评估大型语言模型心理理解能力的全面基准，通过提供更长、更清晰的叙事故事、具有明确个性特征的角色、以及挑战模型对心理状态的理解能力的问题，揭示了现有模型在理解物理世界与心理世界的角色心理状态方面的优势和不足。 |
| [^67] | [Contrastive Approach to Prior Free Positive Unlabeled Learning](https://arxiv.org/abs/2402.06038) | 该论文提出了一种免先验正无标学习的对比方法，通过预训练不变表示学习特征空间并利用嵌入的浓度特性对未标记样本进行伪标签处理，相比现有方法，在多个标准数据集上表现优异，同时不需要先验知识或类先验的估计。 |
| [^68] | [Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent](https://arxiv.org/abs/2402.06034) | 本论文提出了一种迷你像素批量梯度下降（MPGD）算法，用于优化物理设计流程中的预测AI。实验证明MPGD在各种物理设计预测任务中具有显著的优势。 |
| [^69] | [Game-theoretic Counterfactual Explanation for Graph Neural Networks](https://arxiv.org/abs/2402.06030) | 本文提出了一种半值法的、非学习的方法来生成图神经网络的反事实解释，消除了额外训练的需要。与其他流行的方法相比，计算Banzhaf值在识别反事实解释时需要更低的样本复杂性，并且可以实现四倍的加速。 |
| [^70] | [Quantum neural network with ensemble learning to mitigate barren plateaus and cost function concentration](https://arxiv.org/abs/2402.06026) | 本研究提出了一种使用集成学习的量子神经网络构建方法，旨在解决消失梯度和成本函数集中问题，并通过与传统构建的QNN进行比较分析，评估了其有效性。 |
| [^71] | [Decision Theory-Guided Deep Reinforcement Learning for Fast Learning](https://arxiv.org/abs/2402.06023) | 决策理论引导的深度强化学习（DT-guided DRL）通过整合决策理论原则，实现了对DRL智能体的有效初始引导，并促进了在复杂环境中更高效可靠的学习过程。 |
| [^72] | [Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy](https://arxiv.org/abs/2402.06004) | 本文提出了一种激活感知的混合秩压缩策略来提高视觉Transformer的内存效率，并通过选择性低秩权重张量近似和层间误差补偿技术来减少参数数量。这种策略避免了浅层局部最小值陷阱，同时取得了优秀的结果。 |
| [^73] | [Do Large Code Models Understand Programming Concepts? A Black-box Approach](https://arxiv.org/abs/2402.05980) | 本文使用反事实分析框架评估了十个大型代码模型对四种编程概念的理解情况，发现当前模型缺乏对数据流和控制流等概念的理解。 |
| [^74] | [On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI](https://arxiv.org/abs/2402.05979) | 本文研究了行为使用条款的标准化及其用于负责任授权人工智能的采用。采用了定性访谈、许可证条款的聚类和许可证采用的定量分析的混合方法学。结论是负责任AI许可证需要标准化。 |
| [^75] | [Combining shape and contour features to improve tool wear monitoring in milling processes](https://arxiv.org/abs/2402.05978) | 本文提出了一种新的系统，结合了形状描述符和轮廓描述符，用于铣削过程中插入物的磨损监测。实验结果表明，使用后期融合方法将两个描述符组合在一起，可以显著提高分类性能，取得更好的准确率。 |
| [^76] | [Tool wear monitoring using an online, automatic and low cost system based on local texture](https://arxiv.org/abs/2402.05977) | 本研究提出了一种基于计算机视觉和机器学习的在线、低成本和快速方法，用于切削工具的磨损监测。通过将切削边缘图像分割成不同的区域，并使用局部二值模式的纹理描述符来判断每个区域的磨损程度，从而确定切削边缘和刀具是否可服役或可丢弃。 |
| [^77] | [RankSum An unsupervised extractive text summarization based on rank fusion](https://arxiv.org/abs/2402.05976) | RankSum是一种无监督的抽取式文本摘要方法，它利用多维度句子特征对句子进行排名，然后通过加权融合确定句子的重要性排名。该方法不需要监督信号，可以推广到其他数据集。 |
| [^78] | [A Deep Learning Approach for Brain Tumor Classification and Segmentation Using a Multiscale Convolutional Neural Network](https://arxiv.org/abs/2402.05975) | 本文提出了一种使用多尺度卷积神经网络的深度学习方法，可以自动进行脑肿瘤的分类和分割。通过与其他方法的比较，我们的方法在公开数据集上获得了较高的分类准确率。 |
| [^79] | [Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts](https://arxiv.org/abs/2402.05970) | 本文提出了使用神经离散学习和专家级别建模空时动力系统的方法。通过引入通用的专家模块和精细设计的物理流水线，可以在更广泛的现实世界背景下有效地建模和估计空时动态系统的状态变化。 |
| [^80] | [Federated Learning Priorities Under the European Union Artificial Intelligence Act](https://arxiv.org/abs/2402.05968) | 欧盟人工智能法案可能推动联邦学习朝主流采用方向发展，并提出了数据隐私、性能和能源效率等方面的新挑战。 |
| [^81] | [The last Dance : Robust backdoor attack via diffusion models and bayesian approach](https://arxiv.org/abs/2402.05967) | 本文介绍了一种通过扩散模型和贝叶斯方法进行鲁棒后门攻击的方法，具体应用于音频Transformer模型，并证明了攻击的可行性。 |
| [^82] | [Rethink Model Re-Basin and the Linear Mode Connectivity](https://arxiv.org/abs/2402.05966) | 本论文重新审视了模型重新基底的现象，并发现了现有匹配算法的不足。通过适当的重归一化，我们改进了匹配算法，并揭示了它与重归一化过程的相互作用。这为剪枝提供了新的理解，推动了一种轻量且有效的后剪枝插件的开发。 |
| [^83] | [Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences](https://arxiv.org/abs/2402.05963) | 该方法通过选择独特样本并添加到回放缓冲器中以实现样本效率，在复杂动态系统的无模型控制策略合成中起着重要作用。 |
| [^84] | [Nature-Inspired Local Propagation](https://arxiv.org/abs/2402.05959) | 本文介绍了一种自然启发的局部传播算法，该算法通过在线处理环境信息而不依赖大量数据集，在机器学习领域具有潜力。这种算法的核心思想是结合数据表示和学习，以尊重时空局部性，并且当传播速度趋近于无穷大时，它等效于反向传播算法。 |
| [^85] | [Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques](https://arxiv.org/abs/2402.05952) | 本综述调查了将大型语言模型（LLM）与图表示学习（GRL）相结合的技术，并提供了一个新颖的分类法，深入分析了这些模型的核心组成部分和操作技术，为有效的模型设计和训练策略提供了新的视角。 |
| [^86] | [\textit{MinMaxMin} $Q$-learning](https://arxiv.org/abs/2402.05951) | \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。 |
| [^87] | [\textit{SQT} -- \textit{std} $Q$-target](https://arxiv.org/abs/2402.05950) | SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。 |
| [^88] | [Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation](https://arxiv.org/abs/2402.05946) | 本文提出了一种基于时间点过程的方法，通过揭示潜在因果规律来解释异常事件，以帮助在高风险系统如医疗保健中快速诊断和精确治疗规划。该方法通过期望最大化算法优化规则集和模型参数，实现了准确的规则发现和根因识别。 |
| [^89] | [Eliminating Information Leakage in Hard Concept Bottleneck Models with Supervised, Hierarchical Concept Learning](https://arxiv.org/abs/2402.05945) | 本文解决了概念瓶颈模型中的信息泄漏问题，通过引入标签监督和构建分层概念集，提出了一种新的CBMs范例（SupCBM），它可以通过预测的概念和干预矩阵实现标签预测，并且只在不同的类别之间进行区分。 |
| [^90] | [A hybrid IndRNNLSTM approach for real-time anomaly detection in software-defined networks](https://arxiv.org/abs/2402.05943) | 本文提出了一种混合 IndRNNLSTM 方法，用于实时检测软件定义网络中的异常。该方法通过结合 IndRNN 和 LSTM 的特点，学习相关和非相关特征，并使用四种特征选择模型提供适当的特征视角。在 NSL-KDD 数据集上实验结果显示，该方法达到了较低的 MAE 和 RMSE 值。 |
| [^91] | [Cooperative Knowledge Distillation: A Learner Agnostic Approach](https://arxiv.org/abs/2402.05942) | 合作知识蒸馏是一种通过多个模型相互合作来传递知识的方法，可以弥补传统知识蒸馏的局限性。不同模型的优劣势可以更有效地传递知识。 |
| [^92] | [Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs](https://arxiv.org/abs/2402.05941) | 本文提出了一个新的基于人物的服装生成（COG）问题，旨在准确解释人物信息并根据用户的规范生成服装组合。我们提出了一个新颖的框架LVA-COG，利用大型语言模型（LLMs）从用户的兴趣中提取见解，并结合文本到图像模型，增强了对连贯服装的视觉理解和生成。 |
| [^93] | [Causal Relationship Network of Risk Factors Impacting Workday Loss in Underground Coal Mines](https://arxiv.org/abs/2402.05940) | 本研究使用一种新颖的因果人工智能方法，通过分析井下煤矿的伤害记录数据，建立了井下煤矿工作时间损失的因果关系网络。发现关键的因果关系包括风源和工作状态等不同因素之间的作用。 |
| [^94] | [Large Language Model Meets Graph Neural Network in Knowledge Distillation](https://arxiv.org/abs/2402.05894) | 本论文提出了一种新颖的图知识蒸馏框架，使用大规模语言模型作为教师模型、图神经网络作为学生模型，解决了在理解文本-属性图中的节点分类问题中的限制。 |
| [^95] | [Prompting Fairness: Artificial Intelligence as Game Players](https://arxiv.org/abs/2402.05786) | 人工智能作为游戏玩家能够展现出强烈的公平意识，取决于其对游戏伙伴的信任程度、游戏框架对于给予接受者的影响以及其可能存在厌恶不平等的情感。 |
| [^96] | [Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks](https://arxiv.org/abs/2402.05650) | 这项研究提出了一种自监督学习框架，用于训练神经网络从未标记的多感官数据中学习丰富而有意义的3D场景表示。通过利用不同感觉模态之间的时间一致性和几何对齐，我们的框架能够学习到强大而准确的表示。我们将我们的方法应用于各种3D感知任务，并与监督基线进行了比较，展示了竞争性的性能。此外，我们还展示了我们学到的表示在不同的传感器设置下具有很好的泛化能力，进一步突显了我们的自监督学习方法的有效性和多功能性。 |
| [^97] | [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](https://arxiv.org/abs/2402.05391) | 知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。 |
| [^98] | [LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System](https://arxiv.org/abs/2402.05130) | LB-KBQA是一种基于大语言模型和BERT的基于知识的问答系统，通过生成式人工智能的帮助，能够提高意图识别的性能和解决语言多样性的问题。 |
| [^99] | [Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing](https://arxiv.org/abs/2402.05027) | 本论文提出了一种循环消息传递模型，它可以在图中实现多Agent强化学习的泛化能力。这种模型通过在整个图中进行信息流实现了观察邻域大小的平衡，从而提高了Agent的反应性、选择动作的质量和通信效率。 |
| [^100] | [PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition](https://arxiv.org/abs/2402.04838) | 本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。 |
| [^101] | [A new method for optical steel rope non-destructive damage detection](https://arxiv.org/abs/2402.03843) | 本文提出了一种新的算法用于在高海拔环境中对钢丝绳进行非破坏性损伤检测，其中包括一种准确提取钢丝绳的分割模型和一种区分正常和异常钢丝绳的检测模型，实验证明其性能显著高于基准模型。 |
| [^102] | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) | 本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。 |
| [^103] | [HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference](https://arxiv.org/abs/2402.03247) | HEANA是一种混合时幅模拟光学加速器，通过使用混合时幅模拟光学乘法器(TAOMs)提高了对多种数据流的灵活性。它解决了现有光学加速器的波长并行性受到串扰、不支持多种数据流以及未充分利用光电探测器进行原位累积等问题。 |
| [^104] | [Factuality of Large Language Models in the Year 2024](https://arxiv.org/abs/2402.02420) | 本文调查了大规模语言模型（LLM）的真实性问题，并对其现有研究进行了批判性分析，指出了改进LLM真实性的挑战和解决方案，以及自动真实性评估的障碍。未来的研究应该关注在这些方面的进一步工作。 |
| [^105] | [A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles](https://arxiv.org/abs/2402.01703) | 该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。 |
| [^106] | [Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model](https://arxiv.org/abs/2401.17350) | 通过深度黑石贝莱曼模型和时空图神经网络，我们优化了供应商选择和订单分配，同时解决了零阶情况下的可信度问题，实现了准确的预测和精确的置信区间。 |
| [^107] | [Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet](https://arxiv.org/abs/2401.03630) | 本文研究了使用大型语言模型解决多智能体路径规划的问题。通过实验，我们发现直接使用大型语言模型解决复杂场景下的路径规划仍然存在困难。 |
| [^108] | [LLMLight: Large Language Models as Traffic Signal Control Agents](https://arxiv.org/abs/2312.16044) | LLMLight是一个采用大型语言模型作为交通信号控制代理的新框架，通过借助先进的泛化能力和类似人类直觉的推理和决策过程，实现了有效的交通控制。此外，通过构建专为TSC任务定制的骨干语言模型LightGPT，进一步提升了LLMLight的效果和性能。 |
| [^109] | [Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments](https://arxiv.org/abs/2312.15842) | 本研究提出了一种将LLM的知识蒸馏为更小、更高效、更准确的神经网络的方法，在资源受限设备上部署具有挑战性。通过使用LLM的预测概率作为软标签训练较小的学生模型，并使用专门定制的损失函数，保证学生模型与教师模型的性能非常相似。实验证明此方法在科学教育评估中具有良好的准确性。 |
| [^110] | [Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning](https://arxiv.org/abs/2312.15122) | 本研究提出了一种扩展的自动驾驶强化学习方法，在大规模实验中展示了随着规模增加，策略性能的改善。与现有机器学习自动驾驶策略相比，我们的最佳策略将故障率降低了64％，同时提高了25％的驾驶进展速度。 |
| [^111] | [Deep Learning Based Face Recognition Method using Siamese Network](https://arxiv.org/abs/2312.14001) | 这项研究提出了一种使用Siamese网络进行无监督人脸识别的方法，通过利用负样本和最近邻对，消除对标记的人脸图像数据的需求。 |
| [^112] | [LayerCollapse: Adaptive compression of neural networks](https://arxiv.org/abs/2311.17943) | LayerCollapse是一种自适应压缩神经网络的方法，通过结构化剪枝来减少全连接层的深度，而不需要进行微调，并且对性能影响有限。该方法通过正则化激活函数的线性度来控制模型的表达能力。 |
| [^113] | [Monitor Placement for Fault Localization in Deep Neural Network Accelerators](https://arxiv.org/abs/2311.16594) | 本研究提出了一种在并行阵列中优化硬件监测器部署的解决方案，以提高深度神经网络加速器的可靠性。通过证明和推导，我们确定了定位单个故障的PE所需的监测器数量，并解决了NP困难的监测器部署方案优化问题。然后，我们提出了一种启发式方法来平衡效益与开销。 |
| [^114] | [Program Machine Policy: Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines](https://arxiv.org/abs/2311.15960) | 这项工作提出了程序机器策略（POMP），在集成程序合成和状态机的基础上，解决了长期任务并表示复杂行为。 |
| [^115] | [MAIRA-1: A specialised large multimodal model for radiology report generation](https://arxiv.org/abs/2311.13668) | MAIRA-1是一种专门用于放射学报告生成的大型多模态模型，在与预训练的视觉编码器对齐和文本数据增强的基础上，利用了CXR特定的图像编码器和经过微调的大型语言模型，生成具有最先进质量的报告。 |
| [^116] | [DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks](https://arxiv.org/abs/2311.12944) | 本研究提出了一种用于5G及其后太阳能小型蜂窝网络的最佳无人机负载重分配机制，通过使用无人机上的空中基站进行可靠安全的电力再分配，提高了网络的可靠性和稳健性。 |
| [^117] | [AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL](https://arxiv.org/abs/2311.09830) | AutoPlanBench是一种新方法，可以自动转换PDDL规划基准测试为文本描述，并提供了相应的基准测试数据集。研究表明，当前最好的LLM规划器在某些规划任务上表现优秀，但对于其他任务来说仍存在挑战。 |
| [^118] | [Structured Chemistry Reasoning with Large Language Models](https://arxiv.org/abs/2311.09656) | 该论文研究了大型语言模型在化学领域的复杂科学推理困难，发现错误通常源于缺乏有效的推理结构。基于此，引入了一种简单而有效的提示策略StructChem，大幅提升了语言模型的性能。 |
| [^119] | [VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems](https://arxiv.org/abs/2311.06623) | 本文介绍了一种基于Transformer的车辆轨迹预测方法，名为VT-Former，在智能公路交通系统中具有重要的应用价值。 |
| [^120] | [Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations](https://arxiv.org/abs/2310.14894) | LUX是一种基于规则的解释器，可以生成事实、反事实和视觉解释，通过选择高密度簇形式的局部概念来形成决策边界。 |
| [^121] | [Stochastic Population Update Can Provably Be Helpful in Multi-Objective Evolutionary Algorithms](https://arxiv.org/abs/2306.02611) | 本研究通过理论分析证明，在多目标进化算法中采用随机种群更新机制可以显著降低算法的运行时间，从而提高问题的求解效率。 |
| [^122] | [Teaching Probabilistic Logical Reasoning to Transformers](https://arxiv.org/abs/2305.13179) | 本文评估了基于变压器的语言模型在推理不确定文本上的能力，并提出了一种概率约束训练（PCT）的方法来提高模型的概率逻辑推理能力。 |
| [^123] | [Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707) | 本文研究了大型语言模型的关联能力，并揭示了其对隐私泄露的影响。研究发现，随着模型规模的增加，模型在关联实体/信息方面的能力增强。然而，与常识知识相比，模型在关联个人可识别信息方面的准确性较低。 |
| [^124] | [Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection](https://arxiv.org/abs/2302.05262) | 本研究评估了在钻孔工具磨损检测的语义图像分割中的数据增强和损失函数。结果发现，在适度增强的数据上训练的二元模型表现最佳。 |
| [^125] | [Redefining Counterfactual Explanations for Reinforcement Learning: Overview, Challenges and Opportunities](https://arxiv.org/abs/2210.11846) | 这项工作重新定义了强化学习中的反事实解释方法，并探讨了其在监督学习和强化学习中的差异，以提供用户友好和可操作的解释。 |
| [^126] | [Locally Constrained Representations in Reinforcement Learning](https://arxiv.org/abs/2209.09441) | 本论文提出了一种在强化学习中使用局部约束表示的方法，通过辅助损失函数迫使状态表示与相邻状态的表示具有一定的可预测性，以更好地捕捉到环境的局部变化情况。 |
| [^127] | [ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification](https://arxiv.org/abs/2209.09034) | ALEXSIS-PT是一个用于巴西葡萄牙语词汇简化的新型多候选数据集，为LS系统的改进和跨语言模型的研究提供了重要资源。BERTimbau在该数据集上达到了最高的性能。 |
| [^128] | [Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes.](http://arxiv.org/abs/2401.10816) | 该研究通过使用基于图神经网络的推荐系统和来自可穿戴设备的健康行为数据，设计并实施了一个人工智能驱动平台，实现了个性化和情境引导，能够提高参与者的日常活动水平和中等至剧烈运动时长。 |
| [^129] | [SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation.](http://arxiv.org/abs/2401.09773) | SEINE是一种用于核实例分割的结构编码和交互网络，通过考虑核结构的相关性和利用核之间的结构相似性来提高每个分割实例的完整性。 |
| [^130] | [The inherent goodness of well educated intelligence.](http://arxiv.org/abs/2401.04846) | 本文探讨了智能体变得智能的因素，强调了掌握特征和控制多个保守相互作用的子系统的能力。智能的核心是“集体如一体”和“了解局部行动的整体结果”。文章提出了一种对集体保守系统进行控制的替代方法。 |
| [^131] | [Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules.](http://arxiv.org/abs/2401.04130) | 这项工作介绍了PLUTO:一种插拔式模块化的测试时领域适应策略，通过预先训练一系列针对不同源领域的模块，有效地创建了一个"模块存储库"。采用无监督的测试时自适应方法，从存储库中选择稀疏的相关模块的子集，并创建选中模块的加权组合，实现了对新领域的自适应。 |
| [^132] | [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation.](http://arxiv.org/abs/2310.18235) | 本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。 |
| [^133] | [StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling.](http://arxiv.org/abs/2310.17042) | StochGradAdam是一种利用随机梯度抽样加速神经网络训练的优化器，通过选择性梯度考虑，能够稳定收敛，提升鲁棒训练。在图像分类和分割任务中表现优异。 |
| [^134] | [Deep Backtracking Counterfactuals for Causally Compliant Explanations.](http://arxiv.org/abs/2310.07665) | 本研究提供了一种实用方法，用于在深度生成组件的结构因果模型中计算回溯反事实。通过在因果模型的结构化潜在空间中解决优化问题，我们的方法能够生成反事实，并且与其他方法相比具备了多功能、模块化和符合因果关系的特点。 |
| [^135] | [Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction.](http://arxiv.org/abs/2310.05207) | 本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。 |
| [^136] | [Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning.](http://arxiv.org/abs/2310.03094) | 本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。 |
| [^137] | [Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training.](http://arxiv.org/abs/2309.17179) | Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性 |
| [^138] | [Attentive VQ-VAE.](http://arxiv.org/abs/2309.11641) | 本研究提出了一种增强VQ-VAE模型能力的新方法，通过整合Attentive Residual Encoder和Residual Pixel Attention层，利用像素间的自我注意机制来高效地捕捉和利用潜在向量之间的上下文信息，并使用额外的编码级别来进一步增强模型的表示能力，在实验中取得了显著的性能改进。 |
| [^139] | [Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases.](http://arxiv.org/abs/2309.08345) | 本文通过实验调查揭示了语言模型在与知识库进行连接时的数据分布瓶颈，包括推广到未见域、适应语言变体和在不同数据集之间的可转移性等方面。即使采用数据增强技术，先进的语言模型在多个方面表现出较差的性能。 |
| [^140] | [Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering.](http://arxiv.org/abs/2309.06358) | 本论文研究了使用生成式数据增强方法如何提高问答模型在自然分布转换下的鲁棒性，通过实验展示了增强阅读理解数据集的效果。 |
| [^141] | [LLM in the Shell: Generative Honeypots.](http://arxiv.org/abs/2309.00155) | 本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐，解决了以往蜜罐的重要局限性，并通过实验验证了其高准确率。 |
| [^142] | [Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks.](http://arxiv.org/abs/2308.04451) | 本文评估了AI代码生成器的安全性，发现它们容易受到数据毒化攻击，即注入恶意样本来生成易受攻击的代码。攻击可以成功即使只有少量数据被毒化，而且不影响预训练模型生成的代码的正确性，使其难以被检测。 |
| [^143] | [The Impact of Imperfect XAI on Human-AI Decision-Making.](http://arxiv.org/abs/2307.13566) | 本研究通过一个混合方法用户研究，评估了不正确的解释如何影响人类的决策行为，以增进人工智能解释性对人工智能决策的理解。 |
| [^144] | [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models.](http://arxiv.org/abs/2307.10635) | 这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。 |
| [^145] | [Explainability is NOT a Game.](http://arxiv.org/abs/2307.07514) | Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains. |
| [^146] | [Learning Interpretable Low-dimensional Representation via Physical Symmetry.](http://arxiv.org/abs/2302.10890) | 通过使用物理对称性作为潜在空间的自洽约束条件，该研究展示了在音乐领域和计算机视觉领域，模型可以以无监督的方式学习出可解释的低维表示，例如线性音高和三维笛卡尔因素。 |

# 详细

[^1]: Aya数据集：用于多语言指令调优的开放访问收藏品

    Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning

    [https://arxiv.org/abs/2402.06619](https://arxiv.org/abs/2402.06619)

    本研究的主要目标是通过人工筛选的指令遵循数据集来弥合不同语言之间的差距，并且创造了迄今为止最大的多语言收藏品，包括513亿个实例。

    

    数据集是现代人工智能中许多突破的基础。自然语言处理（NLP）领域的许多最近的成就都归功于在多样化任务上对预训练模型进行微调，使得大型语言模型能够响应指令。指令微调（IFT）需要特别构建和注释的数据集。然而，现有的数据集几乎都是以英语为主。在这项工作中，我们的首要目标是通过构建跨越65种语言的人工筛选的指令遵循数据集来弥合语言差距。我们与来自世界各地的流利说者合作，收集指令和完成的自然实例。此外，我们通过在114种语言之间进行模板化和翻译现有数据集，创造了迄今为止规模最大的多语言收藏品，共有5.13亿个实例。总而言之，我们提供了四个关键资源：我们开发并开源了Aya数据集，通过模板化和翻译现有的数据集进行扩展，并将其跨越了114种语言。

    Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya A
    
[^2]: TIC：利用LLMs和逻辑中间表示精确进行“文本到计划”的翻译-推断-编译

    TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations

    [https://arxiv.org/abs/2402.06608](https://arxiv.org/abs/2402.06608)

    该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。

    

    我们研究了为给定的自然语言计划任务请求生成计划的问题。一方面，LLMs在自然语言处理方面表现出色，但在计划方面表现不佳。另一方面，经典计划工具在计划任务方面表现出色，但需要使用结构化语言（如Planning Domain Definition Language（PDDL））作为输入。我们利用这两种技术的优点，通过使用LLMs生成计划任务请求的PDDL表示（任务PDDL），然后使用经典规划器计算计划。与直接使用LLMs生成任务PDDL的先前方法不同，我们的方法包括（a）翻译：仅使用LLMs生成自然语言任务描述的逻辑可解释的中间表示，（b）推断：使用逻辑推理器（目前是Answer Set Programming solver）从中间表示中推导出额外的逻辑相关信息，以及（c）编译：生成目标计划的PDDL描述的编译。

    We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task descriptions, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the targ
    
[^3]: RQP-SGD：通过嘈杂的随机梯度下降和随机量化实现差分隐私的机器学习

    RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization

    [https://arxiv.org/abs/2402.06606](https://arxiv.org/abs/2402.06606)

    RQP-SGD是一种结合了差分隐私随机梯度下降和随机量化的新方法，用于在边缘部署的低内存机器学习模型训练中实现隐私保护。通过研究其在具有凸目标和量化约束的ML任务上的效用收敛性，并证明了其相对确定性量化的有效性。

    

    物联网设备的兴起促使了对在边缘部署实时、高效、安全数据处理的机器学习的需求。在这种情况下，使用实值权重参数实现机器学习模型可能在大型模型上变得不切实际，因此有必要使用量化离散权重来训练模型。同时，这些低维模型也需要保护底层数据集的隐私。在这项工作中，我们提出了RQP-SGD，一种用于低内存边缘机器学习模型训练的隐私保护量化的新方法。该方法将差分隐私随机梯度下降（DP-SGD）与随机量化相结合，在机器学习中提供了可衡量的隐私保证。特别地，我们研究了在具有凸目标和量化约束的ML任务上实施RQP-SGD的效用收敛性，并证明其相对确定性量化的功效。

    The rise of IoT devices has prompted the demand for deploying machine learning at-the-edge with real-time, efficient, and secure data processing. In this context, implementing machine learning (ML) models with real-valued weight parameters can prove to be impractical particularly for large models, and there is a need to train models with quantized discrete weights. At the same time, these low-dimensional models also need to preserve privacy of the underlying dataset. In this work, we present RQP-SGD, a new approach for privacy-preserving quantization to train machine learning models for low-memory ML-at-the-edge. This approach combines differentially private stochastic gradient descent (DP-SGD) with randomized quantization, providing a measurable privacy guarantee in machine learning. In particular, we study the utility convergence of implementing RQP-SGD on ML tasks with convex objectives and quantization constraints and demonstrate its efficacy over deterministic quantization. Throug
    
[^4]: 关于多模式大型语言模型的域外泛化能力研究

    On the Out-Of-Distribution Generalization of Multimodal Large Language Models

    [https://arxiv.org/abs/2402.06599](https://arxiv.org/abs/2402.06599)

    通过对多模式大型语言模型（MLLMs）进行全面评估，研究发现它们在超出训练领域的泛化方面存在困难，主要原因是映射不足。通过上下文学习（ICL）可以显著提升泛化能力，克服泛化障碍。

    

    我们通过全面评估在域外场景和特定领域任务下，当前多模式大型语言模型（MLLMs）的泛化边界。我们评估了它们在合成图像、真实世界分布偏移和医学以及分子图像等专业数据集上的零样本泛化能力。实证结果表明，MLLMs在超出常规训练领域的泛化方面存在困难，限制了它们的直接应用而需要进行适应。为了了解性能不可靠的原因，我们对三个假设进行了分析：语义错误解释、视觉特征提取不足和映射不足。结果表明映射不足是主要障碍。为了解决这个问题，我们表明，在上下文学习（ICL）可以显著提升 MLLMs 的泛化能力，为克服泛化障碍开辟了新的道路。我们进一步探索了 ICL 在分布偏移下的鲁棒性，并展示了它的脆弱性。

    We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerabil
    
[^5]: 理解大型语言模型在复杂Android环境中的弱点

    Understanding the Weakness of Large Language Model Agents within a Complex Android Environment

    [https://arxiv.org/abs/2402.06596](https://arxiv.org/abs/2402.06596)

    这项研究揭示了在复杂的Android环境中，大型语言模型代理面临的挑战，提出了AndroidArena环境和基准来评估其性能，以及降低人力成本的方法。

    

    大型语言模型（LLM）将智能代理运用到诸如浏览器和游戏等领域特定软件上执行复杂任务。然而，当应用于操作系统等通用软件系统时，LLM代理面临三大主要挑战。首先，动作空间广阔且动态，使LLM代理难以保持更新的理解和提供准确的回复。其次，现实世界任务经常需要应用间的协作，要求LLM代理具备远见的规划能力。第三，代理需要识别与用户约束条件（如安全问题和偏好）相符的最优解。这些挑战促使我们设计了AndroidArena，一个用于在现代操作系统上评估LLM代理的环境和基准。为解决高人力成本，我们设计了一种可扩展的、半自动化的方法来构建基准。在任务评估中，AndroidArena采用准确和自适应的指标来补充...

    Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to add
    
[^6]: 预测性表征：智能的基石

    Predictive representations: building blocks of intelligence

    [https://arxiv.org/abs/2402.06590](https://arxiv.org/abs/2402.06590)

    预测性表征可能是智能的多功能基石。

    

    适应性行为通常需要预测未来事件。强化学习理论规定了什么样的预测性表征是有用的以及如何计算它们。本文将这些理论观点与认知和神经科学的研究结合起来。我们特别关注继任者表征（SR）及其广义形式，它们不仅被广泛应用于工程工具，也作为大脑功能的模型。这种融合表明特定类型的预测性表征可能是智能的多功能基石。

    Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.
    
[^7]: G-SciEdBERT: 用于德语科学评估任务的上下文化大型语言模型

    G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German

    [https://arxiv.org/abs/2402.06584](https://arxiv.org/abs/2402.06584)

    G-SciEdBERT是一种上下文化德语科学教育BERT，用于评分德语科学任务的书面回答。通过在大规模德语科学回答语料库上进行预训练，并在评分准确性方面取得了10%的改善。

    

    自然语言处理的进步为各种语言（例如德语中的德语BERT [G-BERT]）的自动评分系统铺平了道路。自动评分德语科学问题的书面回答是一项复杂的任务，对于标准的G-BERT来说具有挑战性，因为它们缺乏科学领域的上下文知识，并且可能与学生的写作风格不一致。本文开发了一种上下文化德语科学教育BERT（G-SciEdBERT），一个创新的大型语言模型，专门用于评分德语科学任务的书面回答。我们使用G-BERT，在5M个标记的PISA 2015国际学生评估的50K个德语科学回答语料库上对G-SciEdBERT进行了预训练。然后，我们在59个评估项目上对G-SciEdBERT进行了微调，并检查了评分准确性。然后，我们将其性能与G-BERT进行了比较。我们的研究结果显示，G-SciEdBERT在评分准确性方面取得了显著的改进，表明其评分准确性提高了10%。

    The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad
    
[^8]: 医学的暗物质中隐藏着什么？在医疗实践中处理丢失数据的学习

    What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices

    [https://arxiv.org/abs/2402.06563](https://arxiv.org/abs/2402.06563)

    本研究使用统计方法和机器学习，通过分析儿科急诊数据和创伤伤害数据库，揭示了医疗实践模式与丢失数据之间的关联，并提出了临床数据插补的方法。这对于减少分析偏见、提高临床决策的有效性非常重要。

    

    电子病人记录（EPR）产生了大量数据，但其中包含重要的缺失信息。理解和处理这些缺失数据是临床数据分析的重要组成部分，如果不加以解决，可能导致分析中的偏见和关键结论的扭曲。缺失数据可能与医疗专业人士的实践模式有关，对缺失数据的插补可以提高临床决策的有效性。本研究专注于统计方法来理解和解释缺失数据，并使用单一中心的儿科急诊数据以及英国最大的创伤伤害数据库（TARN）中的数据，进行基于机器学习的临床数据插补。在对56,961个与儿童急诊部就诊相关的初步生命体征和观察数据进行的研究中，我们表明丢失数据很可能是非随机的，并展示了这些数据与医疗专业人士的实践模式的关联。

    Electronic patient records (EPRs) produce a wealth of data but contain significant missing information. Understanding and handling this missing data is an important part of clinical data analysis and if left unaddressed could result in bias in analysis and distortion in critical conclusions. Missing data may be linked to health care professional practice patterns and imputation of missing data can increase the validity of clinical decisions. This study focuses on statistical approaches for understanding and interpreting the missing data and machine learning based clinical data imputation using a single centre's paediatric emergency data and the data from UK's largest clinical audit for traumatic injury database (TARN). In the study of 56,961 data points related to initial vital signs and observations taken on children presenting to an Emergency Department, we have shown that missing data are likely to be non-random and how these are linked to health care professional practice patterns.
    
[^9]: Diffusion-ES:基于扩散的零梯度规划用于自动驾驶和零阶指令跟随

    Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following

    [https://arxiv.org/abs/2402.06559](https://arxiv.org/abs/2402.06559)

    本文提出了一种Diffusion-ES方法，它结合了无梯度优化和轨迹去噪技术，用于优化黑盒非可微目标。该方法通过从扩散模型中采样轨迹，并使用黑盒奖励函数对其进行评分，实现了更高的多样性和可解释性。

    

    扩散模型在决策和控制中对复杂和多模态轨迹分布建模有很强优势。最近提出了奖励梯度引导去噪方法，用于产生在扩散模型所捕获的数据分布下，同时最大化可微分奖励函数和似然性的轨迹。奖励梯度引导去噪需要一个适合于清洁和噪声样本的可微分奖励函数，从而限制了其作为一种通用轨迹优化器的适用性。在本文中，我们提出了DiffusionES，一种将无梯度优化和轨迹去噪相结合的方法，用于在数据流形中优化黑盒非可微目标。Diffusion-ES从扩散模型中采样轨迹，并使用黑盒奖励函数对其进行评分。它通过截断扩散过程对得分高的轨迹进行变异，该过程应用少量的噪声和去噪步骤，从而实现了更高的多样性和更好的可解释性。

    Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
    
[^10]: 量化布尔贝叶斯网络：逻辑图模型的理论和实验

    The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model

    [https://arxiv.org/abs/2402.06557](https://arxiv.org/abs/2402.06557)

    本文介绍了量化布尔贝叶斯网络（QBBN），它提供了逻辑和概率推理的统一视角，并解决了大型语言模型（LLM）的妄想问题。通过创建一阶演算法的键值版本，QBBN能够表示人类语言背后的逻辑推理。精确推理是不可解的，但可以使用循环信念传播（LBP）进行推理。

    

    本文介绍了量化布尔贝叶斯网络（QBBN），它提供了逻辑和概率推理的统一视角。QBBN旨在解决大型语言模型（LLM）的一个核心问题，即LLM会出现妄想现象。由于贝叶斯网络的构建方式，它无法产生妄想，因为它只能返回可以解释的答案。我们展示了如何配置一个含有无限数量布尔变量的贝叶斯网络来表示人类语言背后的逻辑推理。我们通过创建一种键-值版本的一阶演算法来实现这一点，我们可以证明其一致性和完备性。我们展示了该模型在完全观测数据上是易于训练的，但推理是非平凡的。贝叶斯网络的精确推理是不可解的（即$N$个变量的推理复杂度为$\Omega(2^N)$）。对于推理，我们研究了循环信念传播（LBP）的使用，它并不...

    This paper introduces the Quantified Boolean Bayesian Network (QBBN), which provides a unified view of logical and probabilistic reasoning. The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates. A Bayesian Network, by construction, cannot hallucinate, because it can only return answers that it can explain. We show how a Bayesian Network over an unbounded number of boolean variables can be configured to represent the logical reasoning underlying human language. We do this by creating a key-value version of the First-Order Calculus, for which we can prove consistency and completeness. We show that the model is trivially trained over fully observed data, but that inference is non-trivial. Exact inference in a Bayesian Network is intractable (i.e. $\Omega(2^N)$ for $N$ variables). For inference, we investigate the use of Loopy Belief Propagation (LBP), which is not 
    
[^11]: Bryndza在ClimateActivism 2024上: 通过检索增强的GPT-4和LLaMA进行立场、目标和仇恨事件检测

    Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA

    [https://arxiv.org/abs/2402.06549](https://arxiv.org/abs/2402.06549)

    该研究利用GPT-4和LLaMA模型，通过检索增强和重新排序的方式，在CASE 2024共享任务中取得了显著的成果，特别是在仇恨事件检测和目标识别方面表现出色。

    

    本研究详细介绍了我们在CASE 2024气候行动立场和仇恨事件检测共享任务中的方法，重点关注仇恨言论检测、仇恨言论目标识别和立场检测作为分类挑战。我们探索了大型语言模型（LLM），特别是GPT-4，在零次或少次训练情况下通过检索增强和重新排序来进行推特分类的能力。我们的目标是确定在这个背景下，LLM能否与传统方法相匹配或超越。我们进行了LLaMA的消融研究以进行比较，结果表明我们的模型明显优于基准线，在目标检测任务中获得了第二名。我们提交的代码可以在https://github.com/NaiveNeuron/bryndza-case-2024获得。

    This study details our approach for the CASE 2024 Shared Task on Climate Activism Stance and Hate Event Detection, focusing on Hate Speech Detection, Hate Speech Target Identification, and Stance Detection as classification challenges. We explored the capability of Large Language Models (LLMs), particularly GPT-4, in zero- or few-shot settings enhanced by retrieval augmentation and re-ranking for Tweet classification. Our goal was to determine if LLMs could match or surpass traditional methods in this context.   We conducted an ablation study with LLaMA for comparison, and our results indicate that our models significantly outperformed the baselines, securing second place in the Target Detection task. The code for our submission is available at https://github.com/NaiveNeuron/bryndza-case-2024
    
[^12]: 从大型语言模型中校准长篇生成

    Calibrating Long-form Generations from Large Language Models

    [https://arxiv.org/abs/2402.06544](https://arxiv.org/abs/2402.06544)

    该论文提出了一个统一的校准框架，用于校准大型语言模型的长篇生成。在该框架中，作者开发了三个度量指标用于评估模型的校准性，并提出了两种置信度引导方法。实验证明，更大的模型不一定能保证更好的校准。

    

    为了提高大型语言模型（LLMs）的可靠性，校准是必要的 - 模型的评估置信度应该与其响应正确性的实际可能性相一致。然而，目前的置信度引导方法和校准指标通常依赖于对响应正确性的二元真/假评估。这种方法在长篇生成中不适用，因为答案可能部分正确。为了解决这一问题，我们引入了一个统一的校准框架，其中LLMs的响应正确性和关联的置信水平都被视为一系列分数的分布。在此框架下，我们开发了三个度量指标来精确评估LLM的校准，并进一步提出了基于自一致性和自评估的两种置信度引导方法。我们的实验包括长篇问答和摘要任务，结果表明，更大的模型不一定能保证更好的校准。

    To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
    
[^13]: 生成对抗贝叶斯优化用于代理目标

    Generative Adversarial Bayesian Optimization for Surrogate Objectives

    [https://arxiv.org/abs/2402.06532](https://arxiv.org/abs/2402.06532)

    提出了生成对抗贝叶斯优化（GABO）算法，通过使用自适应源批评家正则化，将优化轨迹限制在代理函数可靠的区域内，解决了离线模型基于策略优化中代理模型预测不准确的问题。在多个离线优化任务中，GABO表现优于现有基准方法。

    

    离线基于模型的策略优化通过在优化过程中不查询真实的目标函数来优化学习到的代理目标函数。然而，在优化过程中经常遇到代理模型预测不准确的情况。为了解决这个问题，我们提出了使用自适应源批评家正则化的生成对抗贝叶斯优化（GABO），这是一个任务不可知的贝叶斯优化框架，采用了Lipschitz有界源批评家模型来约束优化轨迹，使其在代理函数可靠的区域内。我们证明，在连续输入空间先验的一定假设下，我们的算法动态调整源批评家正则化的强度。在各种科学领域的多个离线优化任务中，GABO优于现有基准方法。我们的代码可在https://github.com/michael-s-yao/gabo 查询。

    Offline model-based policy optimization seeks to optimize a learned surrogate objective function without querying the true oracle objective during optimization. However, inaccurate surrogate model predictions are frequently encountered along the optimization trajectory. To address this limitation, we propose generative adversarial Bayesian optimization (GABO) using adaptive source critic regularization, a task-agnostic framework for Bayesian optimization that employs a Lipschitz-bounded source critic model to constrain the optimization trajectory to regions where the surrogate function is reliable. We show that under certain assumptions for the continuous input space prior, our algorithm dynamically adjusts the strength of the source critic regularization. GABO outperforms existing baselines on a number of different offline optimization tasks across a variety of scientific domains. Our code is available at https://github.com/michael-s-yao/gabo
    
[^14]: 改进心肌梗死检测：一种新颖的多模态复合核策略在单一类别分类中的应用

    Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification

    [https://arxiv.org/abs/2402.06530](https://arxiv.org/abs/2402.06530)

    本研究提出了一种新的方法，使用基于超声心动图的一种基于多模态复合核策略的单一类别分类算法来进行早期心肌梗死的检测。这种方法通过优化投影矩阵和特征转化，提高了心肌梗死检测的能力。

    

    早期心肌梗死（MI）的检测对于预防进一步心肌损伤非常重要，MI是由冠状动脉疾病（CAD）引起的一种严重疾病。本研究引入了一种新方法，使用一种基于超声心动图的单一类别分类（OCC）算法进行早期MI检测。我们的研究通过采用基于多模态子空间支持向量数据描述的新方法克服了超声心动图数据有限的挑战。提出的技术涉及一种特殊的MI检测框架，使用复合核在非线性投影技巧中融合高斯和拉普拉斯sigmoid函数，将多视图超声心动图结合起来。此外，我们通过在优化过程中调整投影矩阵的最大化策略，提高了投影矩阵更新策略的效果。我们的方法通过将从超声心动图数据中提取的特征有效地转化为优化的低维空间，增强了MI检测的能力。

    Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
    
[^15]: 内省规划：引导语言驱动的代理机器人改进自身的不确定性

    Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty

    [https://arxiv.org/abs/2402.06529](https://arxiv.org/abs/2402.06529)

    本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。

    

    大型语言模型（LLM）展示了先进的推理能力，使得机器人能够理解自然语言指令，并通过适当的基础塑造来策略性地进行高级行动规划。然而，LLM产生的幻觉可能导致机器人自信地执行与用户目标不符或在极端情况下不安全的计划。此外，自然语言指令中的固有歧义可能引发任务的不确定性，尤其是在存在多个有效选项的情况下。为了解决这个问题，LLMs必须识别此类不确定性并主动寻求澄清。本文探索了内省规划的概念，作为一种系统方法，引导LLMs在无需微调的情况下形成意识到不确定性的机器人任务执行计划。我们研究了任务级机器人规划中的不确定性量化，并证明与最先进的基于LLM的规划方法相比，内省显著提高了成功率和安全性。

    Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
    
[^16]: 在合适的时间提出正确的问题：人类和模型的不确定性指导下的澄清问题

    Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions

    [https://arxiv.org/abs/2402.06509](https://arxiv.org/abs/2402.06509)

    本文研究了模型不确定性与人类不确定性之间的关系，并提出了一种基于模型不确定性估计的生成澄清问题的方法，为对话系统在决定何时提问提供了重要指导，并且在任务成功率方面取得了显著改进。

    

    澄清问题是一种在语言使用中表达误解、歧义和未明示的重要对话工具。虽然人类能够通过提问来解决不确定性，但现代对话系统很难生成有效的问题。为了在这方面取得进展，本文以协作对话任务为测试平台，研究了模型不确定性与人类不确定性之间的关系——这是一个尚未深入研究的问题。我们发现，模型不确定性并不反映人类寻求澄清的行为，这表明使用人类澄清问题来决定何时提问可能不是解决模型不确定性最有效的方法。为了解决这个问题，我们提出了一种基于模型不确定性估计的生成澄清问题的方法，并与几种替代方法进行了比较，结果表明该方法在任务成功率方面取得了显著改进。我们的研究结果强调了这个问题的重要性。

    Clarification questions are an essential dialogue tool to signal misunderstanding, ambiguities, and under-specification in language use. While humans are able to resolve uncertainty by asking questions since childhood, modern dialogue systems struggle to generate effective questions. To make progress in this direction, in this work we take a collaborative dialogue task as a testbed and study how model uncertainty relates to human uncertainty -- an as yet under-explored problem. We show that model uncertainty does not mirror human clarification-seeking behavior, which suggests that using human clarification questions as supervision for deciding when to ask may not be the most effective way to resolve model uncertainty. To address this issue, we propose an approach to generating clarification questions based on model uncertainty estimation, compare it to several alternatives, and show that it leads to significant improvements in terms of task success. Our findings highlight the importanc
    
[^17]: 使用几何特征和深度学习网络对立面级别的点云进行分类

    Classifying point clouds at the facade-level using geometric features and deep learning networks

    [https://arxiv.org/abs/2402.06506](https://arxiv.org/abs/2402.06506)

    该论文提出了一种方法，利用几何特征和深度学习网络对立面级别的点云进行分类。实验证明，融合几何特征可以提高深度学习方法的性能，并促进语义分割的进步。

    

    具有立面细节的三维建筑模型在许多应用中发挥着重要作用。在立面级别上对点云进行分类是创建这样的数字副本的关键。然而，很少有研究将深度神经网络用于这种详细分类。我们提出了一种融合几何特征和深度学习网络的方法，以对立面级别的点云进行分类。我们的实验表明，这种早期融合的特征提高了深度学习方法的性能。该方法可用于补偿深度学习网络在捕捉局部几何信息方面的能力，并促进语义分割的进步。

    3D building models with facade details are playing an important role in many applications now. Classifying point clouds at facade-level is key to create such digital replicas of the real world. However, few studies have focused on such detailed classification with deep neural networks. We propose a method fusing geometric features with deep learning networks for point cloud classification at facade-level. Our experiments conclude that such early-fused features improve deep learning methods' performance. This method can be applied for compensating deep learning networks' ability in capturing local geometric information and promoting the advancement of semantic segmentation.
    
[^18]: ACTER: 用于解释和诊断RL策略的多样且可行的反事实序列

    ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies

    [https://arxiv.org/abs/2402.06503](https://arxiv.org/abs/2402.06503)

    ACTER是一个算法，用于生成可行的反事实序列，提供关于如何避免RL策略失败的可行建议。

    

    了解强化学习（RL）中的失败如何发生以及如何防止是为了实现调试、维护用户信任和开发个性化策略而必要的。反事实推理经常被用来归咎和理解失败，通过寻找最接近的可能世界以避免失败。然而，当前RL中的反事实状态解释只能使用当前状态特征来解释结果，并不能提供关于如何预防负结果的可行性措施。在这项工作中，我们提出了ACTER（用于解释强化学习结果的可行反事实序列）算法，该算法生成可行的反事实序列，提供了关于如何避免失败的可行建议。ACTER研究导致失败的动作，并使用进化算法NSGA-II生成可以最小化改变且具有高确定性的反事实动作序列，以防止失败，即使在随机情况下也是如此。

    Understanding how failure occurs and how it can be prevented in reinforcement learning (RL) is necessary to enable debugging, maintain user trust, and develop personalized policies. Counterfactual reasoning has often been used to assign blame and understand failure by searching for the closest possible world in which the failure is avoided. However, current counterfactual state explanations in RL can only explain an outcome using just the current state features and offer no actionable recourse on how a negative outcome could have been prevented. In this work, we propose ACTER (Actionable Counterfactual Sequences for Explaining Reinforcement Learning Outcomes), an algorithm for generating counterfactual sequences that provides actionable advice on how failure can be avoided. ACTER investigates actions leading to a failure and uses the evolutionary algorithm NSGA-II to generate counterfactual sequences of actions that prevent it with minimal changes and high certainty even in stochastic 
    
[^19]: 可扩展互动式机器学习用于未来指挥与控制

    Scalable Interactive Machine Learning for Future Command and Control

    [https://arxiv.org/abs/2402.06501](https://arxiv.org/abs/2402.06501)

    未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。本论文通过利用互动式机器学习方法，结合人工智能和人类智能，以提高C2运作的适应性和效率。

    

    未来战争将需要指挥与控制（C2）人员在复杂且潜在模糊的情况下以更短的时间内做出决策。鉴于需要强大的决策过程和决策支持工具，人工智能和人类智能的集成具有革命性地改变C2运作流程的潜力，以确保在快速变化的操作环境中的适应性和效率。我们提议利用最近在互动式机器学习方面取得的突破，人类可以与机器学习算法合作以指导机器学习算法的行为。本文确定了目前科技发展中存在的几个差距，未来的工作应该解决这些差距，以扩展这些方法在复杂的C2环境中发挥作用。特别是，我们描述了三个研究重点领域，共同旨在实现可扩展的互动式机器学习（SIML）：1）开发人工智能与人类交互算法以实现协同规划。

    Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
    
[^20]: 在观察数据中实时检测根本原因，应用于IT系统

    On the Fly Detection of Root Causes from Observed Data with Application to IT Systems

    [https://arxiv.org/abs/2402.06500](https://arxiv.org/abs/2402.06500)

    本文介绍了一种用于IT系统的新结构因果模型和一种快速检测异常根本原因的算法。该算法可在离线数据中进行因果发现，并在在线数据中遇到新异常时进行子图遍历。实验证明了方法的优越性能。

    

    本文介绍了一种针对基于阈值的IT系统的新结构因果模型，并提出了一种新的算法，用于快速检测此类系统中异常的根本原因。当根本原因没有因果关联时，该方法被证明是正确的；同时，提出了一种基于代理干预来放松这种假设的扩展。我们的算法及其基于代理的扩展利用离线数据进行因果发现，并在遇到在线数据中的新异常时进行子图遍历。我们广泛的实验证明了我们方法的卓越性能，即使应用于来自替代结构因果模型或真实IT监控数据生成的数据时也是如此。

    This paper introduces a new structural causal model tailored for representing threshold-based IT systems and presents a new algorithm designed to rapidly detect root causes of anomalies in such systems. When root causes are not causally related, the method is proven to be correct; while an extension is proposed based on the intervention of an agent to relax this assumption. Our algorithm and its agent-based extension leverage causal discovery from offline data and engage in subgraph traversal when encountering new anomalies in online data. Our extensive experiments demonstrate the superior performance of our methods, even when applied to data generated from alternative structural causal models or real IT monitoring data.
    
[^21]: 通过关注结构化量化的嵌入在Transformer中引导系统性

    Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings

    [https://arxiv.org/abs/2402.06492](https://arxiv.org/abs/2402.06492)

    本论文提出了SQ-Transformer模型，通过在嵌入和注意层中引入结构化量化的方法，无论训练集的复杂度如何，都能够明确地鼓励模型在编码句子时保持系统性。

    

    Transformer在训练过复杂数据集后能够推广到结构和实体的新组合，但在复杂度不足的数据集上容易过拟合。我们观察到，当训练集足够复杂时，模型使用系统性的注意模式对具有共同句法结构的句子进行编码。受到这一观察的启发，我们提出了SQ-Transformer（结构化量化），即使使用低复杂度的训练集，也能明确地在嵌入和注意层中鼓励系统性。在嵌入层面上，我们引入了结构导向的向量量化（SoVQ），将单词嵌入聚类成若干类具有结构等价的实体。在注意层面上，我们设计了系统性注意层（SAL）和另一种替代性的系统性正则化层（SRL），它们都在量化的词嵌入上操作，以便以不变或类似的注意模式编码具有相同结构的句子。

    Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empiricall
    
[^22]: 《婚姻与正义》：人工智能、法律、逻辑、语言和计算之间的相互作用及其在交通法规和医疗保健方面的案例研究

    Le Nozze di Giustizia. Interactions between Artificial Intelligence, Law, Logic, Language and Computation with some case studies in Traffic Regulations and Health Care

    [https://arxiv.org/abs/2402.06487](https://arxiv.org/abs/2402.06487)

    本文旨在向法律界人士介绍数学逻辑与人工智能的基本原理及其在交通法规和医疗保健中的应用。同时探讨数学逻辑对人工智能应用的限制和影响。

    

    本文的一个重要目标是将一些基本的数学逻辑知识传达给与人工智能合作的法律界人士。在分析了人工智能的基本概念后，我们决定仅限于基于规则的人工智能，不涉及神经网络和机器学习。基于规则的人工智能可以使用形式方法，我们将以简单的形式描述这些方法。然后，我们将看到数学逻辑如何与法律中基于规则的人工智能实践相互作用。我们将讨论数学逻辑对人工智能应用的限制和复杂性。我们将把数学逻辑与法律人工智能之间的限制和相互作用分为三个类别：逻辑、计算和数学。展示相互作用的示例主要来自欧洲交通法规。本文最后还会对人工智能的使用场景和对塑造社会的基本机制进行一些思考。

    An important aim of this paper is to convey some basics of mathematical logic to the legal community working with Artificial Intelligence. After analysing what AI is, we decide to delimit ourselves to rule-based AI leaving Neural Networks and Machine Learning aside. Rule based AI allows for Formal methods which are described in a rudimentary form. We will then see how mathematical logic interacts with legal rule-based AI practice. We shall see how mathematical logic imposes limitations and complications to AI applications. We classify the limitations and interactions between mathematical logic and legal AI in three categories: logical, computational and mathematical. The examples to showcase the interactions will largely come from European traffic regulations. The paper closes off with some reflections on how and where AI could be used and on basic mechanisms that shape society.
    
[^23]: "当他感到寒冷时，他去了海马鱼" - 将生成式人工智能融入多物质故事创作中的家庭表达艺术疗法

    "When He Feels Cold, He Goes to the Seahorse"-Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy

    [https://arxiv.org/abs/2402.06472](https://arxiv.org/abs/2402.06472)

    这项研究对使用生成式人工智能作为表达材料的治疗性故事创作进行了调查，发现家庭创造性地融合了人工智能和传统表达材料来外化他们的思想和感受。

    

    故事创作作为一种综合性的表达艺术疗法形式，是促进家庭沟通的有效手段。然而，在治疗性故事创作中将生成式人工智能作为表达材料的整合仍未被充分探索。对于如何在家庭和治疗师中提供支持的人机交互（HCI）影响也存在缺乏。为了解决这个问题，我们的研究经过了为期五周的治疗故事创作会话，涉及七个家庭在专业治疗师的指导下进行。在这些会话中，家庭使用传统的艺术材料和基于图像的生成式人工智能来创作和发展他们的家庭故事。通过丰富的经验数据和四名专业治疗师的评论，我们描述了家庭如何创造性地融合人工智能和传统表达材料来外化他们的思想和感受。通过表达疗法连续体（ETC）的视角，我们表征了人工智能作为表达材料的治疗意义。支持儿童、父母的可取交互质量

    Storymaking, as an integrative form of expressive arts therapy, is an effective means to foster family communication. Yet, the integration of generative AI as expressive materials in therapeutic storymaking remains underexplored. And there is a lack of HCI implications on how to support families and therapists in this context. Addressing this, our study involved five weeks of storymaking sessions with seven families guided by a professional therapist. In these sessions, the families used both traditional art-making materials and image-based generative AI to create and evolve their family stories. Via the rich empirical data and commentaries from four expert therapists, we contextualize how families creatively melded AI and traditional expressive materials to externalize their ideas and feelings. Through the lens of Expressive Therapies Continuum (ETC), we characterize the therapeutic implications of AI as expressive materials. Desirable interaction qualities to support children, parent
    
[^24]: V-STaR: 自学推理器的训练方法

    V-STaR: Training Verifiers for Self-Taught Reasoners

    [https://arxiv.org/abs/2402.06457](https://arxiv.org/abs/2402.06457)

    V-STaR利用正确和不正确的解决方案训练验证器，用于选择模型生成的解决方案，实现了自我改进和验证方法在常见代码生成和数学推理任务中达到4%到17%的测试准确率提升。

    

    大型语言模型（LLM）的常见自我改进方法，例如STaR（Zelikman等人，2022），通过自动生成的解决方案迭代微调LLM以提高其问题解决能力。然而，这些方法在此过程中丢弃了大量的不正确的解决方案，可能忽略了这些解决方案中的宝贵信息。为了解决这个缺点，我们提出了V-STaR，它利用自我改进过程中生成的正确和不正确的解决方案来使用DPO训练一个判断模型生成解决方案的正确性的验证器。在推理时，这个验证器用来在众多候选解决方案中选择一个解决方案。多次运行V-STaR会逐步产生更好的推理器和验证器，在常见代码生成和数学推理基准测试中，使用LLaMA2模型可以取得4%到17%的测试准确率提升。

    Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
    
[^25]: 层次化Transformer是高效的元强化学习者

    Hierarchical Transformers are Efficient Meta-Reinforcement Learners

    [https://arxiv.org/abs/2402.06402](https://arxiv.org/abs/2402.06402)

    层次化Transformer是一种高效的元强化学习方法，通过有效地提取过去经验的信息丰富资源，并应用于新的环境中，实现了超越最先进方法的元训练效果，并显著提高了泛化能力和学习效率。

    

    我们介绍了一种强大的在线元强化学习方法，即层次化Transformer用于元强化学习（HTrMRL）。HTrMRL旨在解决使强化学习代理能够在以前未见任务中有效执行的挑战。我们展示了过去的经验作为信息丰富的资源，我们的模型有效地提炼和应用到新的上下文中。我们学习到的算法能够超越以前的最先进，并提供更高效的元训练，同时显著改善了泛化能力。在Meta-World基准的各种模拟任务上获得的实验结果表明，在各种任务上相比最先进的方法，学习效率和适应性显著提升。我们的方法不仅增强了代理从有限数据中的泛化能力，还为更强大和多功能的AI系统铺平了道路。

    We introduce Hierarchical Transformers for Meta-Reinforcement Learning (HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims to address the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of information, which our model effectively distills and applies to new contexts. Our learned algorithm is capable of outperforming the previous state-of-the-art and provides more efficient meta-training while significantly improving generalization capabilities. Experimental results, obtained across various simulated tasks of the Meta-World Benchmark, indicate a significant improvement in learning efficiency and adaptability compared to the state-of-the-art on a variety of tasks. Our approach not only enhances the agent's ability to generalize from limited data but also paves the way for more robust and versatile AI systems.
    
[^26]: 使用SAT求解器自动发现困难归约

    Finding hardness reductions automatically using SAT solvers

    [https://arxiv.org/abs/2402.06397](https://arxiv.org/abs/2402.06397)

    本文呈现了一种使用SAT求解器自动发现困难归约的方法，通过对符号映射的完成问题进行深入研究，分类了数千个NP完全的结构，并提出了一个将内部三元组系统推广到更高维度的无限家族的结构。

    

    在这篇文章中，我们展示了完成问题(即决策问题，即部分结构能否完成为完整结构)在许多组合结构中是NP完全的。虽然大多数文献中的归约都是手工完成的，但我们提出了一种完全自动构建归约的算法。使用基于SAT的框架，我们对具有禁止子结构的符号映射的完成问题进行了首次深入研究，并分类了数千个NP完全的结构。我们的列表包括了由Knuth引入的用于平面点配置公理化的内部三元组系统。最后但并非最不重要的是，我们提供了一个将内部三元组系统推广到更高维度的无限家族的结构，对于这些结构，完成问题是NP完全的。

    In this article, we show that the completion problem, i.e. the decision problem whether a partial structure can be completed to a full structure, is NP-complete for many combinatorial structures. While the gadgets for most reductions in literature are found by hand, we present an algorithm to construct gadgets in a fully automated way. Using our framework which is based on SAT, we present the first thorough study of the completion problem on sign mappings with forbidden substructures by classifying thousands of structures for which the completion problem is NP-complete. Our list in particular includes interior triple systems, which were introduced by Knuth towards an axiomatization of planar point configurations. Last but not least, we give an infinite family of structures generalizing interior triple system to higher dimensions for which the completion problem is NP-complete.
    
[^27]: 基于人类审美偏好的大型文本到图像模型个性化: 以康定斯基创作为例

    Human Aesthetic Preference-Based Large Text-to-Image Model Personalization: Kandinsky Generation as an Example

    [https://arxiv.org/abs/2402.06389](https://arxiv.org/abs/2402.06389)

    本文介绍了一种无提示的生成方法，使用户能够自动生成个性化的艺术风格的绘画内容，这些内容融入了他们的审美偏好。

    

    随着神经生成能力的进步，艺术界积极采用GenAI（生成性人工智能）来创作绘画内容。大型的文本到图像模型能够快速生成美观的结果。然而，这个过程可能是非确定性的，并且常常需要耗费大量的试误，因为用户在构思有效的提示来达到他们想要的结果时会遇到困难。本文提出了一种无提示的生成方法，使用户能够自动生成个性化的艺术风格的绘画内容，这些内容融入了他们的审美偏好。该方法包括利用“语义注入”来定制特定艺术风格的艺术家模型，并进一步利用遗传算法通过实时迭代的人类反馈来优化提示生成过程。通过仅依靠用户对艺术家模型生成的图像的审美评价和偏好，这个方法可实现个性化的图像生成过程。

    With the advancement of neural generative capabilities, the art community has actively embraced GenAI (generative artificial intelligence) for creating painterly content. Large text-to-image models can quickly generate aesthetically pleasing outcomes. However, the process can be non-deterministic and often involves tedious trial-and-error, as users struggle with formulating effective prompts to achieve their desired results. This paper introduces a prompting-free generative approach that empowers users to automatically generate personalized painterly content that incorporates their aesthetic preferences in a customized artistic style. This approach involves utilizing ``semantic injection'' to customize an artist model in a specific artistic style, and further leveraging a genetic algorithm to optimize the prompt generation process through real-time iterative human feedback. By solely relying on the user's aesthetic evaluation and preference for the artist model-generated images, this a
    
[^28]: 关于随机梯度下降（SGD）的收敛速度及其在修改的多臂赌博机上的策略梯度应用

    On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit

    [https://arxiv.org/abs/2402.06388](https://arxiv.org/abs/2402.06388)

    该论文证明了当学习速率按照逆时间衰减规则时，随机梯度下降（SGD）的收敛速度，并应用于修改的带有L2正则化的策略梯度多臂赌博机（MAB）的收敛性分析。

    

    我们提出了一个自包含的证明，证明了当学习速率遵循逆时间衰减规则时，随机梯度下降（SGD）的收敛速度；接下来，我们将这些结果应用于带有L2正则化的修改的策略梯度多臂赌博机（MAB）的收敛性分析。

    We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
    
[^29]: 基于强化学习和粒子滤波的高精度地质定向

    High-Precision Geosteering via Reinforcement Learning and Particle Filters

    [https://arxiv.org/abs/2402.06377](https://arxiv.org/abs/2402.06377)

    基于强化学习和粒子滤波的地质定向方法，通过实时数据处理实现高精度地质定向决策

    

    地质定向是钻井作业中的关键部分，传统上涉及对各种数据源（如井测数据）的手动解读。这引入了主观偏见和不一致的程序。学术界尝试通过贪婪优化和近似动态规划（ADP）来解决地质定向决策优化问题，显示出了一定的潜力，但缺乏适应现实多样情况的能力。强化学习（RL）为这些挑战提供了解决方案，通过基于奖励的迭代学习来促进最优决策。状态估计方法，例如粒子滤波（PF），提供了一种基于在线信息的补充策略，用于地质定向决策。我们将基于RL的地质定向与PF相结合，以应对现实的地质定向情况。我们的框架使用PF处理实时井测数据，估计井的位置相对于地层，然后将其信息用于基于RL的决策过程。我们比较了我们的方法

    Geosteering, a key component of drilling operations, traditionally involves manual interpretation of various data sources such as well-log data. This introduces subjective biases and inconsistent procedures. Academic attempts to solve geosteering decision optimization with greedy optimization and Approximate Dynamic Programming (ADP) showed promise but lacked adaptivity to realistic diverse scenarios. Reinforcement learning (RL) offers a solution to these challenges, facilitating optimal decision-making through reward-based iterative learning. State estimation methods, e.g., particle filter (PF), provide a complementary strategy for geosteering decision-making based on online information. We integrate an RL-based geosteering with PF to address realistic geosteering scenarios. Our framework deploys PF to process real-time well-log data to estimate the location of the well relative to the stratigraphic layers, which then informs the RL-based decision-making process. We compare our method
    
[^30]: CoSearchAgent:基于大语言模型的轻量级协作搜索代理

    CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models

    [https://arxiv.org/abs/2402.06360](https://arxiv.org/abs/2402.06360)

    CoSearchAgent是一种基于大语言模型的轻量级协作搜索代理，可作为Slack插件在多方对话中支持协作搜索。

    

    协作搜索支持多个用户共同完成特定的搜索任务。研究发现，将轻量级协作搜索插件设计在即时通讯平台内更符合用户的协作习惯。然而，由于多用户交互场景的复杂性，实现一个完全功能的轻量级协作搜索系统是具有挑战性的。因此，之前的轻量级协作搜索研究不得不依赖于"吹牛大王"范例。近年来，大型语言模型(LLM)已被证明可以与用户自然交互，并通过基于LLM的代理实现复杂的信息搜索任务。因此，为了更好地支持协作搜索研究，本文提出了CoSearchAgent，一种由LLM驱动的轻量级协作搜索代理。CoSearchAgent被设计为Slack插件，可以在该平台上的多方对话中支持协作搜索。

    Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped
    
[^31]: 为AI推理建模人类价值观

    Modelling Human Values for AI Reasoning

    [https://arxiv.org/abs/2402.06359](https://arxiv.org/abs/2402.06359)

    本研究详细介绍了一个关于人类价值观的形式化模型，并展示了它在AI推理中的应用。研究通过基于社会心理学研究的关键思想，为AI系统与人类价值观的一致性提供了具体的计算表示。

    

    当今最重要的社会挑战之一是构建其行为与人类价值观一致的AI系统，或是其使人工和人工之间相互作用的社区行为与人类价值观一致。为了解决这一挑战，我们详细介绍了一个关于人类价值观的形式化模型，以进行其明确的计算表示。据我们所知，目前尚未有人尝试过这种模型，这在考虑到将价值观与AI整合的研究数量不断增长的情况下是令人惊讶的。我们以社会心理学领域近几十年来研究人类价值观性质的大量研究为起点，致力于提供这样一个形式化模型。我们展示了这个模型如何为基于AI的价值推理提供基础装置，并证明了它在实际应用案例中的适用性。我们阐述了我们的模型如何捕捉到社会心理学研究的关键思想，并提出了未来关于人类价值观在AI中集成和跨学科研究的路线图。

    One of today's most significant societal challenges is building AI systems whose behaviour, or the behaviour it enables within communities of interacting agents (human and artificial), aligns with human values. To address this challenge, we detail a formal model of human values for their explicit computational representation. To our knowledge, this has not been attempted as yet, which is surprising given the growing volume of research integrating values within AI. Taking as our starting point the wealth of research investigating the nature of human values from social psychology over the last few decades, we set out to provide such a formal model. We show how this model can provide the foundational apparatus for AI-based reasoning over values, and demonstrate its applicability in real-world use cases. We illustrate how our model captures the key ideas from social psychology research and propose a roadmap for future integrated, and interdisciplinary, research into human values in AI. The
    
[^32]: ExaRanker-Open: 使用开源LLMs进行IR的合成解释

    ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs

    [https://arxiv.org/abs/2402.06334](https://arxiv.org/abs/2402.06334)

    ExaRanker-Open 是一种使用开源LLMs进行IR的方法，通过适应和探索开源语言模型来生成解释。研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。

    

    ExaRanker最近提出了一种训练信息检索(IR)模型的方法，该方法将自然语言解释作为附加标签。该方法解决了有限标记示例的挑战，提高了IR模型的效果。然而，初始结果是基于专有的语言模型，如GPT-3.5，这导致了数据集大小的限制，因为其成本和数据隐私。在本文中，我们介绍了ExaRanker-Open，通过适应和探索开源语言模型来生成解释。该方法已经使用不同的LLMs和数据集大小进行了测试，以更好地理解数据增强的有效贡献。我们的研究结果表明，纳入解释能够稳定提高神经排序器的性能，而LLM的大小越大，收益越大。值得注意的是，即使在大数据集上，数据增强方法也是有优势的，ExaRanker的性能超过目标基线0

    ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0
    
[^33]: 时间交互图上的提示学习

    Prompt Learning on Temporal Interaction Graphs

    [https://arxiv.org/abs/2402.06326](https://arxiv.org/abs/2402.06326)

    这个论文提出了一种在时间交互图上进行提示学习的方法，以解决当前模型在预训练和下游预测阶段所面临的时间差异和语义差异的问题。

    

    时间交互图(TIGs)被广泛用于表示真实世界系统。为了促进在TIGs上的表示学习，研究人员提出了一系列的TIG模型。然而，这些模型在“预训练，预测”训练范式中依然面临着两个难题。首先，预训练和推理数据之间的时间差异严重削弱了模型在动态演化数据上进行遥远未来预测的适用性。其次，预文本任务和下游任务之间的语义差异阻碍了它们在实际应用中的使用，因为它们在应用场景中很难对齐其学习和预测能力。

    Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.   Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straig
    
[^34]: 对语音真实性的一种新方法

    A New Approach to Voice Authenticity

    [https://arxiv.org/abs/2402.06304](https://arxiv.org/abs/2402.06304)

    这篇论文提出了一种新的思路，挑战传统的将音频定义为“假”或“真”的范式。研究者们将注意力放在了确定“语音编辑”上，这包括传统的修改以及TTS合成和VC系统。他们还提供了一个新的挑战数据集。

    

    语音伪造主要由于最近文字转语音（TTS）合成技术的进展而带来了显著的社会挑战。目前，主流的假设是未经改动的人类语音被认为是真实的，而伪造的语音来自于TTS合成。我们认为这种二元区分是过于简化的。例如，通过改变播放速度可以用于恶意用途，比如“醉酒的南希·佩洛西”事件。同样地，音频剪辑的编辑可以在新闻报道或播客中进行道德的缩减或概括，但也可以制造误导性的叙述。在本文中，我们提出了一个概念上的转变，摆脱了将音频定义为“假”还是“真”的二元范式。相反，我们的重点是确定“语音编辑”，它包括传统修改，如滤波器和剪切，以及TTS合成和VC系统。我们划分了6个类别，并在M-AILABS语料库的基础上策划了一个新的挑战数据集。

    Voice faking, driven primarily by recent advances in text-to-speech (TTS) synthesis technology, poses significant societal challenges. Currently, the prevailing assumption is that unaltered human speech can be considered genuine, while fake speech comes from TTS synthesis. We argue that this binary distinction is oversimplified. For instance, altered playback speeds can be used for malicious purposes, like in the 'Drunken Nancy Pelosi' incident. Similarly, editing of audio clips can be done ethically, e.g., for brevity or summarization in news reporting or podcasts, but editing can also create misleading narratives. In this paper, we propose a conceptual shift away from the binary paradigm of audio being either 'fake' or 'real'. Instead, our focus is on pinpointing 'voice edits', which encompass traditional modifications like filters and cuts, as well as TTS synthesis and VC systems. We delineate 6 categories and curate a new challenge dataset rooted in the M-AILABS corpus, for which w
    
[^35]: 一种基于函数分析的符号回归方法的研究

    A Functional Analysis Approach to Symbolic Regression

    [https://arxiv.org/abs/2402.06299](https://arxiv.org/abs/2402.06299)

    本研究提出了一种基于函数分析的新型符号回归方法，称为Fourier Tree Growing (FTG)，通过在不同空间直接进行优化来避免复杂的符号表示，实验表明该算法在一维基准问题上与传统的遗传编程方法相比具有显著的性能提升。

    

    符号回归(SR)由于依赖于合成输入输出映射的表达式而对随机搜索启发式方法提出了重要挑战。尽管传统的遗传编程(GP)算法在各个领域取得了成功，但在使用基于树的表示进行SR时，它们的性能受到限制。为了解决这些限制，我们引入了一种新颖的SR方法，称为Fourier Tree Growing (FTG)，它借鉴了函数分析的见解。这种新的视角使我们能够直接在一个不同的空间中进行优化，从而避免了复杂的符号表达式。我们的算法在一系列经典的一维基准问题上相比传统的GP方法展现出显著的性能提升。为了识别和解释GP和FTG的限制因素，我们对一个具有高次数多项式（最高100次）的大规模多项式基准进行了实验。据作者所知，这项工作是首次将函数分析方法应用于SR的研究。

    Symbolic regression (SR) poses a significant challenge for randomized search heuristics due to its reliance on the synthesis of expressions for input-output mappings. Although traditional genetic programming (GP) algorithms have achieved success in various domains, they exhibit limited performance when tree-based representations are used for SR. To address these limitations, we introduce a novel SR approach called Fourier Tree Growing (FTG) that draws insights from functional analysis. This new perspective enables us to perform optimization directly in a different space, thus avoiding intricate symbolic expressions. Our proposed algorithm exhibits significant performance improvements over traditional GP methods on a range of classical one-dimensional benchmarking problems. To identify and explain limiting factors of GP and FTG, we perform experiments on a large-scale polynomials benchmark with high-order polynomials up to degree 100. To the best of the authors' knowledge, this work rep
    
[^36]: AI，与人相遇：混合决策系统的学习范式

    AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems

    [https://arxiv.org/abs/2402.06287](https://arxiv.org/abs/2402.06287)

    本调查提出了混合决策系统的分类方法，为理解如何对人与机器之间的交互进行建模提供了概念性和技术性的框架。

    

    每天，我们越来越多地依赖机器学习模型来自动化和支持高风险任务和决策。这种日益增长的存在意味着人类现在不断与基于机器学习的系统进行互动，每天进行模型的培训和使用。计算机科学文献中有几种不同的技术来考虑人与机器学习系统的交互，但其分类稀疏且目标各异。本调查提出了混合决策系统的分类方法，为理解当前计算机科学文献如何对人与机器之间的交互进行建模提供了概念性和技术性的框架。

    Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.
    
[^37]: LLaVA-Docent：利用多模态大型语言模型支持艺术鉴赏教育的教学调优

    LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education

    [https://arxiv.org/abs/2402.06264](https://arxiv.org/abs/2402.06264)

    本研究利用多模态大型语言模型（MLLM）开发了LLaVA-Docent模型，以支持艺术鉴赏教育。通过综述文献和专家咨询，构建了数据框架，并使用该框架生成了虚拟对话数据集用于训练MLLM。该研究对于解决传统艺术鉴赏教育中的资源限制和主流教育中的科学技术工程和数学偏重具有重要意义。

    

    艺术鉴赏对于培养学习者的批判性思维和情感智力至关重要。然而，传统的艺术鉴赏教育常面临艺术资源有限的问题，特别是对于弱势学生，并且在主流教育中过度强调科学技术工程和数学科目。为了应对这些挑战，最近的技术进步为创新解决方案铺平了道路。本研究探索了多模态大型语言模型（MLLM）在艺术鉴赏教育中的应用，重点是开发了LLaVA-Docent模型来利用这些进展。我们的方法包括全面的文献综述和与领域专家的咨询，从而形成了一个强大的数据框架。利用这个框架，我们生成了一个虚拟对话数据集，该数据集被GPT-4利用。这个数据集对于训练MLLM（即LLaVA-Docent）起到了关键作用。六名研究人员进行了定量和定性评估。

    Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluation
    
[^38]: 关于针对键值约束生成语言模型推理的驱逐策略的有效性研究

    On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference

    [https://arxiv.org/abs/2402.06262](https://arxiv.org/abs/2402.06262)

    本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。

    

    尽管大型语言模型（LLMs）在最近取得了成功，但由于它们对内存和计算资源的过度需求，它们在资源受限环境中部署仍然昂贵。除了模型参数外，键值缓存也存储在GPU内存中，随着批处理大小和序列长度的增加而线性增长。为此，最近的研究提出了各种针对给定预算下维护键值缓存开销的驱逐策略。本文着眼于现有驱逐策略在重要性评分计算和驱逐范围构建两个方面的效果。我们确定了先前策略在这两个方面的不足，并引入了基于时间注意力得分和鲁棒性度量的RoCo，一种强大的缓存驱逐策略。涵盖了预填充和自回归解码阶段的广泛实验验证了RoCo的优越性。最后，我们公开发布了RoCo的代码和模型供研究者使用。

    Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
    
[^39]: 进取的鲍勃通过提示对抗调整抵制越狱行为

    Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning

    [https://arxiv.org/abs/2402.06255](https://arxiv.org/abs/2402.06255)

    本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。

    

    尽管大型语言模型（LLM）在各种应用中取得了巨大的成功，但它们也容易受到特定提示的影响，从而绕过内置的安全措施并提供危险或非法内容，这种现象被称为越狱行为。为了保护LLMs免受产生有害信息的影响，提出了各种防御策略，其中大多数集中在内容过滤或模型的对抗训练方面。在本文中，我们提出了一种名为Prompt Adversarial Tuning（PAT）的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中来实现我们的防御策略。我们设计了一个类似对抗训练的训练过程，以实现我们的优化目标，交替更新攻击和防御控制机制。据我们所知，我们是第一个从提示调整的角度实施防御的人。一旦应用，我们的方法几乎不会影响LLMs的操作效率。实验表明我们的方法在抵御越狱行为方面具有良好的效果。

    Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
    
[^40]: 探索针对调试的交互模式：增强AI助手的对话能力

    Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants

    [https://arxiv.org/abs/2402.06229](https://arxiv.org/abs/2402.06229)

    本文研究了增强AI助手的对话能力，设计了一个名为Robin的增强型对话型AI助手，通过使用交互模式和对话分析，提高了其在调试方面的性能。

    

    大规模语言模型（LLM）在集成开发环境（IDEs）中的广泛应用已经迅速普及。与LLMs的对话交互使程序员能够获取各种软件开发任务的自然语言解释。然而，LLMs经常在没有足够上下文的情况下立即采取行动，导致隐含的假设和不准确的回应。开发者和LLMs之间的对话主要以问答对的形式进行，开发者负责提出正确的问题并在多个回合中维持对话。在本文中，我们从交互模式和对话分析中汲取灵感，设计了一个增强的用于调试的对话型AI助手Robin。通过对12位行业专业人员进行的一项用户研究，我们发现，为LLM配备以下功能可以改善其性能：(1)利用插入扩展交互模式，(2)促进轮流发言，(3)利用调试工作。

    The widespread availability of Large Language Models (LLMs) within Integrated Development Environments (IDEs) has led to their speedy adoption. Conversational interactions with LLMs enable programmers to obtain natural language explanations for various software development tasks. However, LLMs often leap to action without sufficient context, giving rise to implicit assumptions and inaccurate responses. Conversations between developers and LLMs are primarily structured as question-answer pairs, where the developer is responsible for asking the the right questions and sustaining conversations across multiple turns. In this paper, we draw inspiration from interaction patterns and conversation analysis -- to design Robin, an enhanced conversational AI-assistant for debugging. Through a within-subjects user study with 12 industry professionals, we find that equipping the LLM to -- (1) leverage the insert expansion interaction pattern, (2) facilitate turn-taking, and (3) utilize debugging wo
    
[^41]: 生成AI评估中的悖论：它能解决的问题可能无法进行评估

    The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate

    [https://arxiv.org/abs/2402.06204](https://arxiv.org/abs/2402.06204)

    本文讨论了生成AI评估中的悖论，并发现了大型语言模型在评估任务中性能较差的现象。研究突出了需要检查模型作为评估者的忠实度和可信度，以及探索生成优秀与评估能力之间的关联。

    

    本文探讨了一种假设，即在生成任务中擅长的大型语言模型（LLM）同样擅长作为评估者。我们使用TriviaQA数据集评估了三个LLM和一个开源LM在问答（QA）和评估任务中的表现。结果表明存在显著差异，LLM在评估任务中的性能较生成任务低。有趣的是，我们发现了一些不忠实的评估情况，模型在其不擅长的领域中准确评估答案，突出了需要检查LLM作为评估者的忠实度和可信度。本研究有助于理解“生成AI悖论”，强调了探索生成优秀与评估能力之间的关联以及审查模型评估中忠实度方面的必要性。

    This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks. Intriguingly, we discover instances of unfaithful evaluation where models accurately evaluate answers in areas where they lack competence, underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators. This study contributes to the understanding of "the Generative AI Paradox" (West et al., 2023), highlighting a need to explore the correlation between generative excellence and evaluation proficiency, and the necessity to scrutinize the faithfulness aspect in model evaluations.
    
[^42]: 大型语言模型：一项调查

    Large Language Models: A Survey

    [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196)

    大型语言模型（LLMs）吸引了很多关注，因为它们在自然语言任务上的强大表现。该研究领域发展迅速，包括了各种著名的LLMs、构建和增强LLMs的技术、以及流行的LLM数据集和评估指标。

    

    大型语言模型（LLMs）由于其在各种自然语言任务上的出色表现而受到了很多关注，自2022年11月ChatGPT发布以来。LLMs通过在大量文本数据上训练模型的数十亿参数来获得广泛的通用语言理解和生成能力，这符合缩放定律的预测。LLMs的研究领域尽管非常新，但在许多不同方面正在快速发展。在本文中，我们回顾了一些最著名的LLMs，包括三个流行的LLM系列（GPT、LLaMA、PaLM），并讨论了它们的特点、贡献和限制。我们还概述了构建和增强LLMs的技术。然后，我们调查了为LLM训练、微调和评估准备的流行数据集，审查了广泛使用的LLM评估指标，并比较了几个流行LLM在一组代表性基准上的性能。

    Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
    
[^43]: 一个自监督学习框架用于学习整个切片的表示

    A self-supervised framework for learning whole slide representations

    [https://arxiv.org/abs/2402.06188](https://arxiv.org/abs/2402.06188)

    这个论文提出了一个自监督学习框架（S3L），用于学习整个切片的表示。它结合了变压器模型的视觉和语言建模策略，通过生成配对视图进行自监督学习，以实现高质量的WSI视觉特征学习。

    

    整个切片成像对于生物医学显微镜和计算病理学至关重要。然而，由于其千兆像素的大小、多样的组织病理学特征、空间异质性以及有限的/不存在的数据注释，整个切片图像 (WSIs) 构成了一个复杂的计算机视觉挑战。这些挑战突显了仅依靠监督训练可能导致次优的整个切片表示。自监督表示学习可以为下游诊断任务（如癌症诊断或分子遗传预测）实现高质量的WSI视觉特征学习。在这里，我们提出了一个通用的自监督整个切片学习（S3L）框架，用于千兆像素规模的WSI自监督。S3L将来自基于变压器的视觉和语言建模的数据转换策略结合到一个统一的框架中，以生成用于自监督的配对视图。S3L利用内在的区域异质性、组织学特征的可变性和信息冗余性

    Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy wi
    
[^44]: Premier-TACO: 通过时间驱动的对比损失进行多任务表示预训练

    Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

    [https://arxiv.org/abs/2402.06187](https://arxiv.org/abs/2402.06187)

    Premier-TACO是一种多任务特征表示学习方法，通过预训练通用特征表示，并引入负例抽样策略来提高时序行动对比学习的计算效率，从而显著增强了对新颖动作的少样本模仿学习的效果。

    

    我们提出了Premier-TACO，这是一种多任务特征表示学习方法，旨在提高顺序决策任务中少样本策略学习的效率。Premier-TACO利用一部分多任务离线数据集进行预训练通用特征表示，该特征表示捕捉了关键的环境动力学，并使用最少的专家演示进行微调。它通过引入一种新的负例抽样策略推动了时序行动对比学习（TACO）目标的发展，TACO在视觉控制任务中具有最先进的结果。这种策略在显著提高TACO的计算效率方面非常重要，使大规模多任务离线预训练成为可能。我们在包括Deepmind Control Suite、MetaWorld和LIBERO在内的各种连续控制基准测试中进行了广泛的实证评估，证明了Premier-TACO在预训练视觉表示方面的有效性，显著增强了对新颖动作的少样本模仿学习。

    We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
    
[^45]: 开发和验证一个人工智能模型，准确预测脊盘盆参数

    Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters

    [https://arxiv.org/abs/2402.06185](https://arxiv.org/abs/2402.06185)

    该研究开发了一个名为SpinePose的人工智能工具，可以准确预测脊盘盆参数，无需手动输入。

    

    目标。研究表明，脊盘盆对齐与临床症状的改善相关。然而，脊盘盆放射学参数的测量费时且观察者之间的一致性值得关注。自动测量工具能够以迅速而一致的方式进行测量，但现有工具仍然受到某种程度的手动输入要求的限制。该研究提出了一种新颖的人工智能工具SpinePose，可以在不需要手动输入的情况下高精度地预测脊盘盆参数。

    Objective. Achieving appropriate spinopelvic alignment has been shown to be associated with improved clinical symptoms. However, measurement of spinopelvic radiographic parameters is time-intensive and interobserver reliability is a concern. Automated measurement tools have the promise of rapid and consistent measurements, but existing tools are still limited by some degree of manual user-entry requirements. This study presents a novel artificial intelligence (AI) tool called SpinePose that automatically predicts spinopelvic parameters with high accuracy without the need for manual entry.   Methods. SpinePose was trained and validated on 761 sagittal whole-spine X-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic incidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle (T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was labeled by 4 reviewers, including fellowship-trained spine surgeons and a fellowship-trained radiologist with 
    
[^46]: MusicMagus: 通过扩散模型实现零样本文本到音乐的编辑

    MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models

    [https://arxiv.org/abs/2402.06178](https://arxiv.org/abs/2402.06178)

    本文介绍了一个通过扩散模型实现零样本文本到音乐的编辑的新方法，可以修改生成音乐的特定属性而保持其他方面不变，并展示了其在风格和音色转换方面的优越性能以及在实际音乐编辑场景中的实用性。

    

    近年来，文本到音乐生成模型的不断进步为音乐创造力开辟了新的道路。然而，音乐生成通常涉及迭代的改进，如何编辑生成的音乐仍然是一个重大挑战。本文介绍了一种新颖的方法，用于编辑这种模型生成的音乐，实现对特定属性（如风格、情感和乐器）的修改，同时保持其他方面不变。我们的方法将文本编辑转化为潜在空间操纵，并添加额外的约束以确保一致性。它可以无需额外训练与现有的预训练文本到音乐扩散模型无缝集成。实验结果在风格和音色转换评估中显示出优于零样本和某些监督基线的性能。此外，我们展示了我们方法在实际音乐编辑场景中的实用性。

    Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to \textit{latent space manipulation} while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. Additionally, we showcase the practical applicability of our approach in real-world music editing scenarios.
    
[^47]: 学习对比特征表示来进行面部动作单元检测

    Learning Contrastive Feature Representations for Facial Action Unit Detection

    [https://arxiv.org/abs/2402.06165](https://arxiv.org/abs/2402.06165)

    这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。

    

    面部动作单元（AU）检测的主要方法涉及监督的多标签二进制分类问题。现有的方法常常对AU的像素级信息进行编码，从而对模型的复杂性和表达能力提出了很大的要求。此外，由于存在噪声AU标签，这种做法增加了过拟合的风险。在本研究中，我们引入了一个对比学习框架，通过监督和自监督信号增强。目标是在AU检测领域中摆脱传统的像素级学习范式，获得判别特征。为了应对噪声AU标签带来的挑战，我们通过引入自监督信号来增强监督信号。这种增强是通过正样本抽样实现的，包括三种不同类型的正样本对。另外，为了减轻每个AU类型的分布不平衡问题，我们采用了一种权衡重要性的损失函数。

    The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
    
[^48]: 带有赞助产品的品类规划

    Assortment Planning with Sponsored Products

    [https://arxiv.org/abs/2402.06158](https://arxiv.org/abs/2402.06158)

    本研究主要关注零售中带有赞助产品的品类规划挑战并将其建模为组合优化任务，以实现在考虑赞助产品的情况下优化预期收入的目的。

    

    在零售行业快速发展的背景下，品类规划对于企业的成功起着至关重要的作用。随着赞助产品在在线市场的日益突出地位，零售商在有效管理产品品类方面面临新的挑战。值得注意的是，以前的品类规划研究大多忽视了赞助产品的存在及其对整体推荐效果可能产生的影响。相反，他们通常简化地假设所有产品都是有机产品或非赞助产品。这个研究空白突显了在赞助产品存在的情况下更深入探讨品类规划挑战的必要性。我们将在存在赞助产品的情况下将品类规划问题建模为组合优化任务。最终目标是计算出一种最优的品类规划方案，既能优化预期收入，又能考虑到赞助产品的存在。

    In the rapidly evolving landscape of retail, assortment planning plays a crucial role in determining the success of a business. With the rise of sponsored products and their increasing prominence in online marketplaces, retailers face new challenges in effectively managing their product assortment in the presence of sponsored products. Remarkably, previous research in assortment planning largely overlooks the existence of sponsored products and their potential impact on overall recommendation effectiveness. Instead, they commonly make the simplifying assumption that all products are either organic or non-sponsored. This research gap underscores the necessity for a more thorough investigation of the assortment planning challenge when sponsored products are in play. We formulate the assortment planning problem in the presence of sponsored products as a combinatorial optimization task. The ultimate objective is to compute an assortment plan that optimizes expected revenue while considerin
    
[^49]: DeAL：用于大型语言模型的解码时对齐

    DeAL: Decoding-time Alignment for Large Language Models

    [https://arxiv.org/abs/2402.06147](https://arxiv.org/abs/2402.06147)

    DeAL是一个允许用户自定义奖励函数并实现解码时对齐LLMs的框架。

    

    大型语言模型（LLMs）现在期望生成与人类偏好对齐的内容。目前的工作主要集中在模型训练时间对齐上，通过诸如强化学习与人类反馈（RLHF）等技术。然而，目前还不清楚这些方法是否有效地教导模型对齐目标。首先，无法整合多个自定义奖励和依赖模型开发者对通用和静态原则的理解是主要局限。其次，模型训练中的残留差距以及这些方法的可靠性也值得质疑（例如，即使在安全训练后仍然容易被越狱）。为了解决这些问题，我们提出了DeAL，一个允许用户自定义奖励函数并实现解码时对齐LLMs（DeAL）的框架。核心思想在于将解码视为一个启发式引导的搜索过程，并促使使用各种对齐目标。我们的实验以编程约束为例进行了验证。

    Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constra
    
[^50]: 重新思考大规模图学习的节点传播方式

    Rethinking Node-wise Propagation for Large-scale Graph Learning

    [https://arxiv.org/abs/2402.06128](https://arxiv.org/abs/2402.06128)

    提出了适应性拓扑感知传播（ATP）方法，以应对大规模图学习中节点传播的问题。此方法能对不同节点的拓扑角色进行个性化传播，并减少传播带来的偏差和额外开销。

    

    可扩展的图神经网络（GNN）已成为一种有前景的技术，在许多大规模基于图的Web应用中展现出卓越的预测性能和高效运行效率。然而，大多数可扩展的GNN倾向于以相同的传播规则处理图中的所有节点，忽视了它们的拓扑独特性；现有的节点级传播优化策略在复杂的Web规模图中效果不佳，需要对节点的局部属性进行全面描绘。直观地说，Web规模图中的不同节点具有不同的拓扑角色，因此无差别地传播或忽视局部上下文可能会影响节点表示的质量。小规模情景无法匹配Web规模图中的这种复杂拓扑。为了解决上述问题，我们提出了适应性拓扑感知传播（ATP）方法，减少潜在的高偏差传播和额外开销。

    Scalable graph neural networks (GNNs) have emerged as a promising technique, which exhibits superior predictive performance and high running efficiency across numerous large-scale graph-based web applications. However, (i) Most scalable GNNs tend to treat all nodes in graphs with the same propagation rules, neglecting their topological uniqueness; (ii) Existing node-wise propagation optimization strategies are insufficient on web-scale graphs with intricate topology, where a full portrayal of nodes' local properties is required. Intuitively, different nodes in web-scale graphs possess distinct topological roles, and therefore propagating them indiscriminately or neglect local contexts may compromise the quality of node representations. This intricate topology in web-scale graphs cannot be matched by small-scale scenarios. To address the above issues, we propose \textbf{A}daptive \textbf{T}opology-aware \textbf{P}ropagation (ATP), which reduces potential high-bias propagation and extrac
    
[^51]: 学习变得高效：在大型语言模型中构建结构化稀疏性

    Learn To be Efficient: Build Structured Sparsity in Large Language Models

    [https://arxiv.org/abs/2402.06126](https://arxiv.org/abs/2402.06126)

    本文通过引入一种新的算法"Learn-To-be-Efficient(LTE)"，提出了在大型语言模型(LLM)中构建结构化稀疏性的方法。该方法通过训练高效意识的LLM学习激活更少的神经元，取得更好的稀疏性和性能折衷。

    

    大型语言模型(LLM)以其十亿级参数取得了显著的成功，但它们产生了高昂的推理开销。在LLM中出现的激活稀疏性为通过仅涉及部分参数进行推理提供了一种自然的方法来减少这种成本。现有方法只关注利用这种自然形成的激活稀疏性，忽视了进一步放大这种固有稀疏性的潜力。本文中，我们假设LLM可以通过实现更结构化的激活稀疏性来学习高效。为实现这一目标，我们引入了一种新颖的算法"Learn-To-be-Efficient(LTE)", 旨在训练高效意识的LLM学习激活更少的神经元，并在稀疏性和性能之间取得更好的折衷。此外，与主要关注基于ReLU模型的SOTA MoEfication方法不同，LTE还可以应用于像GPT和LLaMA这样具有软激活函数的LLM。我们在四个模型和十一个数据集上评估了LTE。

    Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
    
[^52]: ViGoR：通过细粒度奖励建模改进大规模视觉语言模型的视觉对接

    ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling

    [https://arxiv.org/abs/2402.06118](https://arxiv.org/abs/2402.06118)

    ViGoR通过细粒度奖励建模提高了大型视觉语言模型在视觉对接方面的性能，通过人工评估和自动化方法有效地解决了视觉对接中的误差问题。

    

    通过将自然语言理解、大语言模型的生成能力和广泛知识与图像感知相结合，最近的大规模视觉语言模型（LVLMs）在现实世界中展示了前所未有的推理能力。然而，生成的文本往往在视觉输入中存在不准确的对接，导致错误，如产生幻觉的不存在场景元素、遗漏重要的场景部分，以及推测对象之间的属性和关系时出现错误。为了解决这些问题，我们引入了一个新颖的框架ViGoR（通过细粒度奖励建模进行视觉对接），它利用细粒度奖励建模来显著提升基于预训练基线的LVLMs的视觉对接能力。这种改进通过使用比完全监督更便宜的人工评估和自动化方法高效实现。我们通过多个基准测试的多个指标展示了我们方法的有效性。

    By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
    
[^53]: 用于编程和机器人教育的大型语言模型(LLMs)

    LLMs for Coding and Robotics Education

    [https://arxiv.org/abs/2402.06116](https://arxiv.org/abs/2402.06116)

    本文介绍了机器人编程教育的重要趋势，并利用大型语言模型测试了机器人代码生成任务。结果显示，GPT-4V在所有测试中表现优异，但在生成块状图像方面存在困难。

    

    最近，大型语言模型和多模态大型语言模型在人工智能领域取得了革命性的进展。越来越多的地区正在采用这些先进技术。在这个背景下，机器人编程教育日益受到关注。为了教小孩子们如何编程和参加机器人挑战，人们开始利用大型语言模型来解释、生成和修改机器人代码。本文重点介绍了机器人编程教育的一个重要趋势。我们测试了几种主流大型语言模型在传统编程任务和更具挑战性的机器人代码生成任务上的表现，包括块状图。我们的结果显示，GPT-4V在所有测试中表现优异，但在生成块状图像方面存在困难。

    Large language models and multimodal large language models have revolutionized artificial intelligence recently. An increasing number of regions are now embracing these advanced technologies. Within this context, robot coding education is garnering increasing attention. To teach young children how to code and compete in robot challenges, large language models are being utilized for robot code explanation, generation, and modification. In this paper, we highlight an important trend in robot coding education. We test several mainstream large language models on both traditional coding tasks and the more challenging task of robot code generation, which includes block diagrams. Our results show that GPT-4V outperforms other models in all of our tests but struggles with generating block diagram images.
    
[^54]: 在在线考试中的作弊检测和定位的多实例学习

    Multiple Instance Learning for Cheating Detection and Localization in Online Examinations

    [https://arxiv.org/abs/2402.06107](https://arxiv.org/abs/2402.06107)

    本文提出了一种基于多实例学习的作弊检测框架CHEESE，该框架综合考虑了头部姿势、凝视角度、身体姿势和背景信息等特征，并通过标签生成器和特征编码器实现了作弊行为的检测和定位。

    

    2019年冠状病毒病流行疫情的蔓延导致许多课程和考试变成在线形式。考试监考系统中的作弊行为检测模型在保证远程考试的公平性方面起着重要作用。然而，作弊行为很少见，大多数研究者在作弊行为检测任务中没有全面考虑头部姿势、凝视角度、身体姿势和背景信息等特征。在本文中，我们开发并提出了CHEESE，一种基于多实例学习的作弊检测框架。该框架包括实现弱监督的标签生成器和学习判别性特征的特征编码器。此外，该框架还将3D卷积提取的身体姿势和背景特征与OpenFace 2.0捕获的眼睛凝视、头部姿势和面部特征相结合。通过拼接，这些特征被送入时空图模块进行分析。

    The spread of the Coronavirus disease-2019 epidemic has caused many courses and exams to be conducted online. The cheating behavior detection model in examination invigilation systems plays a pivotal role in guaranteeing the equality of long-distance examinations. However, cheating behavior is rare, and most researchers do not comprehensively take into account features such as head posture, gaze angle, body posture, and background information in the task of cheating behavior detection. In this paper, we develop and present CHEESE, a CHEating detection framework via multiplE inStancE learning. The framework consists of a label generator that implements weak supervision and a feature encoder to learn discriminative features. In addition, the framework combines body posture and background features extracted by 3D convolution with eye gaze, head posture and facial features captured by OpenFace 2.0. These features are fed into the spatio-temporal graph module by stitching to analyze the spa
    
[^55]: 功能对齐回归：一种从数据中明确学习函数导数的方法

    Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data

    [https://arxiv.org/abs/2402.06104](https://arxiv.org/abs/2402.06104)

    该论文提出了一种名为FAR的方法，通过捕捉函数导数来更好、更高效地拟合底层真实函数。在合成数据集和八个真实世界任务中证明了该方法的有效性。

    

    回归是机器学习中的一个基本任务，在过去几十年中引起了广泛关注。传统的回归方法主要通过使用损失函数来将模型预测与每个个体数据样本的真实值对齐，然而，我们发现这种方法可能导致在不同样本之间关系的预测不够优化。近期的研究工作引入了标签相似性信息来改进回归方法，但在完全捕捉底层真实函数的复杂性方面仍存在明显的差距。在本文中，我们提出了FAR（功能对齐回归）作为一种更好、更高效的解决方案，通过捕捉函数导数来拟合底层真实函数。我们在两个合成数据集和六个领域的八个大规模真实世界任务中验证了该方法的有效性。

    Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
    
[^56]: 来，见，胜：解决知识图谱学习前的众多挑战

    Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning

    [https://arxiv.org/abs/2402.06098](https://arxiv.org/abs/2402.06098)

    知识图谱学习面临着缺乏专家知识整合、节点度数极端性不稳定、缺乏不确定性和相关性的考虑以及缺乏可解释性的挑战。现有的解决尝试大多是孤立的，需要综合考虑这些问题的解决方案。

    

    知识图谱(KG)已经成为表示大规模链接数据的常见方法。然而，由于KG的巨大规模，图谱学习系统需要帮助人类进行分析、解释和模式检测。尽管有许多KG学习系统对研究人员和临床医生进行了赋能，但我们发现现有的图谱学习中存在四个关键不足，这些不足同时限制了KG学习的性能并降低了人类与这些学习系统的最佳接口能力。这些不足包括：1)缺乏专家知识的整合，2)对KG中节点度数极端性的不稳定性，3)在学习过程中缺乏对不确定性和相关性的考虑，4)缺乏可解释性。此外，我们对解决每个问题的现有尝试进行了分类，并指出每个尝试在很大程度上与解决其他问题的尝试相隔离。通过对这些问题的形式化，

    Knowledge Graphs (KGs) have become increasingly common for representing large-scale linked data. However, their immense size has required graph learning systems to assist humans in analysis, interpretation, and pattern detection. While there have been promising results for researcher- and clinician- empowerment through a variety of KG learning systems, we identify four key deficiencies in state-of-the-art graph learning that simultaneously limit KG learning performance and diminish the ability of humans to interface optimally with these learning systems. These deficiencies are: 1) lack of expert knowledge integration, 2) instability to node degree extremity in the KG, 3) lack of consideration for uncertainty and relevance while learning, and 4) lack of explainability. Furthermore, we characterise state-of-the-art attempts to solve each of these problems and note that each attempt has largely been isolated from attempts to solve the other problems. Through a formalisation of these probl
    
[^57]: TWIG：通过模拟KGE模型实现预先超参数优化和跨图泛化

    TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models

    [https://arxiv.org/abs/2402.06097](https://arxiv.org/abs/2402.06097)

    这项研究引入了一种名为TWIG的新颖模型，可以通过拓扑特征学习权重来模拟KGE模型的输出，有效减少了参数数量，并具有预先优化超参数和跨图泛化的能力。

    

    在本文中，我们介绍了一种名为TWIG（Topologically-Weighted Intelligence Generation）的新颖的、无需嵌入的模拟KGE输出的范式，它只使用了一小部分参数。TWIG从图数据的拓扑特征作为输入学习权重，没有对实体或边的潜在表示进行编码。我们在UMLS数据集上的实验结果表明，单个TWIG神经网络几乎可以准确预测所有超参数配置下最先进的ComplEx-N3 KGE模型的结果。为了达到这个目标，它只使用了2590个可学习参数，但准确预测了1215个不同超参数组合的结果，相当于29322000个参数的总成本。

    In this paper we introduce TWIG (Topologically-Weighted Intelligence Generation), a novel, embedding-free paradigm for simulating the output of KGEs that uses a tiny fraction of the parameters. TWIG learns weights from inputs that consist of topological features of the graph data, with no coding for latent representations of entities or edges. Our experiments on the UMLS dataset show that a single TWIG neural network can predict the results of state-of-the-art ComplEx-N3 KGE model nearly exactly on across all hyperparameter configurations. To do this it uses a total of 2590 learnable parameters, but accurately predicts the results of 1215 different hyperparameter combinations with a combined cost of 29,322,000 parameters. Based on these results, we make two claims: 1) that KGEs do not learn latent semantics, but only latent representations of structural patterns; 2) that hyperparameter choice in KGEs is a deterministic function of the KGE model and graph structure. We further hypothesi
    
[^58]: 用根茎来平衡偏斜的入度分布

    Rhizomes to Load Balance Skewed In-Degree Distributions

    [https://arxiv.org/abs/2402.06086](https://arxiv.org/abs/2402.06086)

    本文通过引入根茎构造的方法来解决图中高入度分布导致的负载不平衡问题，在大规模芯片上对BFS图遍历性能进行了加速和优化。

    

    本文旨在通过将根茎的概念应用于基于顶点为中心的消息驱动图处理，解决由图中高入度分布引起的负载不平衡问题。图的根茎构造为任何数量的单个大入度顶点创建了多个命名顶点地址。然后，其他顶点可以指向任何一个命名地址，从而共享入度负载。根茎内部进行通信并保持一致，以提供顶点的统一和正确的视图。模拟实验结果显示，在包含高度偏斜的入度分布的输入图数据集上，对大芯片大小上的BFS图遍历性能进行了加速。改进来自于在内存处理元素之间共享入度计算工作负载，并降低网络芯片的争用。

    The paper aims to address load imbalance caused by high in-degree distribution in graphs by applying the idea of rhizome to vertex-centric message-driven graph processing. Rhizome construction of the graph creates multiple named vertex address for any number of single large in-degree vertices. It then allows other vertices to point to any of the named addresses thus sharing the in-degree load. The rhizomes internally communicate and remain consistent to provide a unified and correct view of the vertex. Simulated experimental results show performance speed ups for BFS graph traversal on large chip sizes for the tested input graph datasets containing highly skewed in-degree distribution. The improvements come from sharing the in-degree compute workload among memory-processing elements and also lowering contention on the network-on-chip.
    
[^59]: SubGen：子线性时间和内存的令牌生成

    SubGen: Token Generation in Sublinear Time and Memory

    [https://arxiv.org/abs/2402.06082](https://arxiv.org/abs/2402.06082)

    这项工作提出了一种名为SubGen的高效缓存压缩技术，通过在Attention模块中进行在线聚类和采样，实现了子线性的内存占用和时间复杂度，并建立了一个紧密的误差界。

    

    尽管大型语言模型（LLM）取得了显著的成功，但它们广泛的内存需求使得在长上下文令牌生成环境中部署它们存在挑战。LLM解码器的巨大内存占用量来自于在注意模块中存储所有先前令牌的必要性，这是由键-值（KV）缓存所强制的要求。在这项工作中，我们的重点是开发一种高效的键值缓存压缩技术。经验证据表明，在注意模块中的键嵌入中存在显著的聚类倾向。基于这一关键洞察，我们设计了一种具有子线性复杂度的新型缓存方法，采用键令牌的在线聚类和值的在线l2采样。结果是一个可以证明准确和高效的注意解码算法，称为SubGen。这个算法不仅确保了子线性内存占用和子线性时间复杂度，还为我们的方法建立了一个紧密的误差界。经验评估...

    Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation. The substantial memory footprint of LLM decoders arises from the necessity to store all previous tokens in the attention module, a requirement imposed by key-value (KV) caching. In this work, our focus is on developing an efficient compression technique for the KV cache. Empirical evidence indicates a significant clustering tendency within key embeddings in the attention module. Building on this key insight, we have devised a novel caching method with sublinear complexity, employing online clustering on key tokens and online $\ell_2$ sampling on values. The result is a provably accurate and efficient attention decoding algorithm, termed SubGen. Not only does this algorithm ensure a sublinear memory footprint and sublinear time complexity, but we also establish a tight error bound for our approach. Empirical evaluations
    
[^60]: DiscDiff: DNA序列生成的潜在扩散模型

    DiscDiff: Latent Diffusion Model for DNA Sequence Generation

    [https://arxiv.org/abs/2402.06079](https://arxiv.org/abs/2402.06079)

    本文介绍了一种新的框架用于生成DNA序列，包括一个用于生成离散DNA序列的潜在扩散模型和一个用于改进序列的后训练算法。我们的方法不仅在DNA序列生成方面树立了新的标准，并且在生成短序列和长序列方面表现出优越性能。此外，我们还引入了一个多物种的DNA生成数据集。这项研究将推动DNA的生成建模，并对基因治疗和蛋白质生产产生潜在影响。

    

    本文介绍了一种新颖的DNA序列生成框架，包括两个关键组成部分：DiscDiff，一种为生成离散DNA序列而定制的潜在扩散模型（LDM），以及Absorb-Escape，一种用于改进这些序列的后训练算法。Absorb-Escape通过纠正在潜在空间和输入空间之间的转换过程中固有的“舍入误差”，提高了生成序列的真实性。我们的方法不仅在DNA序列生成方面树立了新的标准，而且在生成短序列和长序列方面表现出优越的性能，同时引入了EPD-GenDNA，这是第一个涵盖15个物种的、综合性的DNA生成数据集，包含160,000个独特序列。我们希望这项研究能推动DNA的生成建模，对基因治疗和蛋白质生产可能产生影响。

    This paper introduces a novel framework for DNA sequence generation, comprising two key components: DiscDiff, a Latent Diffusion Model (LDM) tailored for generating discrete DNA sequences, and Absorb-Escape, a post-training algorithm designed to refine these sequences. Absorb-Escape enhances the realism of the generated sequences by correcting `round errors' inherent in the conversion process between latent and input spaces. Our approach not only sets new standards in DNA sequence generation but also demonstrates superior performance over existing diffusion models, in generating both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the first comprehensive, multi-species dataset for DNA generation, encompassing 160,000 unique sequences from 15 species. We hope this study will advance the generative modelling of DNA, with potential implications for gene therapy and protein production.
    
[^61]: 使用贝叶斯网络的高斯混合模型进行可承受性学习

    Gaussian Mixture Models for Affordance Learning using Bayesian Networks

    [https://arxiv.org/abs/2402.06078](https://arxiv.org/abs/2402.06078)

    本文使用高斯混合模型来处理不确定性，提供更准确的可承受性学习方法。

    

    可承受性是描述动作、物体和效果之间关系的基本描述符。它们为机器人预测效果、识别动作、选择物体和根据期望目标规划行为提供了手段。本文探讨了一个具有自主学习能力的具身代理探索世界并从感官体验中学习这些可承受性的问题。已经有了学习编码这些知识的贝叶斯网络的结构和参数的模型。尽管贝叶斯网络能够处理不确定性和冗余，但以往的工作考虑了离散感官数据的完全可观测性，这可能会在存在噪声的情况下导致严重错误。本文通过高斯混合模型（GMMs）对传感器进行概率表示，并明确考虑每个离散可承受性概念中包含的概率分布，从而可以更正确地进行学习。

    Affordances are fundamental descriptors of relationships between actions, objects and effects. They provide the means whereby a robot can predict effects, recognize actions, select objects and plan its behavior according to desired goals. This paper approaches the problem of an embodied agent exploring the world and learning these affordances autonomously from its sensory experiences. Models exist for learning the structure and the parameters of a Bayesian Network encoding this knowledge. Although Bayesian Networks are capable of dealing with uncertainty and redundancy, previous work considered complete observability of the discrete sensory data, which may lead to hard errors in the presence of noise. In this paper we consider a probabilistic representation of the sensors by Gaussian Mixture Models (GMMs) and explicitly taking into account the probability distribution contained in each discrete affordance concept, which can lead to a more correct learning.
    
[^62]: 实现数字化决策支持下的规模化人工智能战争游戏

    Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making

    [https://arxiv.org/abs/2402.06075](https://arxiv.org/abs/2402.06075)

    针对决策支持下的战争游戏，本论文提出了规模化人工智能的发展并与人类判断相结合的重要性。通过提高全域意识、改善决策速度和质量、提供创新行动建议以及更快速地应对对手行动，我们能够更好地应对现代挑战和困境，增强人类决策的指导和增强。

    

    在这个前所未有的由技术驱动的变革时代，我们更需要积极投资于开发强大的人工智能（AI）来支持决策的战争游戏。通过推进AI技术系统并将其与人类判断相结合，我们将能够提高全域意识，改善决策周期的速度和质量，提供创新行动的建议，更迅速地应对对手的行动。因此，我们迫切需要加快AI的开发，以帮助我们更好地应对现代挑战和困境的复杂性，这些挑战目前需要人类智能，并在可能的情况下试图超越人类智能-而不是取代人类，而是以机器速度增强和更好地指导人类决策。

    In this unprecedented era of technology-driven transformation, it becomes more critical than ever that we aggressively invest in developing robust artificial intelligence (AI) for wargaming in support of decision-making. By advancing AI-enabled systems and pairing these with human judgment, we will be able to enhance all-domain awareness, improve the speed and quality of our decision cycles, offer recommendations for novel courses of action, and more rapidly counter our adversary's actions. It therefore becomes imperative that we accelerate the development of AI to help us better address the complexity of modern challenges and dilemmas that currently requires human intelligence and, if possible, attempt to surpass human intelligence--not to replace humans, but to augment and better inform human decision-making at machine speed. Although deep reinforcement learning continues to show promising results in intelligent agent behavior development for the long-horizon, complex tasks typically
    
[^63]: 随机性就是你所需的：使用大型语言模型在问题-解决空间中进行语义遍历的方法

    Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models

    [https://arxiv.org/abs/2402.06053](https://arxiv.org/abs/2402.06053)

    通过使用大型语言模型进行语义遍历，我们提出了一种新颖的方法来在创新问题和解决方案空间中探索。这种方法不仅可以找到多种问题的解决方案，还可以改进和澄清问题陈述。

    

    我们提出了一种新颖的方法，使用自定义的思路数据库对创新问题和解决方案领域进行探索，通过在不同的温度级别下语义地遍历双向问题和解决方案树，我们在解决方案编辑距离上实现了高多样性，同时仍保持与原始问题语义上的接近。除了找到给定问题的各种解决方案外，这种方法还可以用于改进和澄清原始问题陈述。作为进一步验证该方法的尝试，我们实现了一个概念验证的Slack bot，作为创新助手。

    We present a novel approach to exploring innovation problem and solution domains using LLM fine-tuning with a custom idea database. By semantically traversing the bi-directional problem and solution tree at different temperature levels we achieve high diversity in solution edit distance while still remaining close to the original problem statement semantically. In addition to finding a variety of solutions to a given problem, this method can also be used to refine and clarify the original problem statement. As further validation of the approach, we implemented a proof-of-concept Slack bot to serve as an innovation assistant.
    
[^64]: 大型语言模型在与人类辩论中的局限性

    Limits of Large Language Models in Debating Humans

    [https://arxiv.org/abs/2402.06049](https://arxiv.org/abs/2402.06049)

    大型语言模型在与人类辩论中的能力有限，尽管它们能够融入和促进人类的工作效率，但在辩论中的说服力较弱。在成为可行的辩手之前，LLMs需要进一步发展。

    

    大型语言模型(LLMs)在与人类的互动中展现出了显著的潜力。随后，将它们作为人工代表和替代品进行社会学实验的潜在应用是一个令人激动的前景。但是这个想法有多可行呢？本文试图通过一项预先注册的研究来测试现阶段LLMs的局限性，该研究将真实的人类与扮演人类的LLM代理结合起来。本研究着重探讨辩论为基础的意见共识形成在三种环境下的情况：仅人类、代理和人类、仅代理。我们的目标是理解LLM代理对人类的影响，并评估它们在辩论方面的能力是否与人类相似。我们发现LLMs能够融入并促进人类的工作效率，但在辩论中的说服力较弱，最终行为与人类有所偏离。我们阐明了这些主要缺陷，并预计在成为可行的辩手之前，LLMs必须进一步发展。

    Large Language Models (LLMs) have shown remarkable promise in their ability to interact proficiently with humans. Subsequently, their potential use as artificial confederates and surrogates in sociological experiments involving conversation is an exciting prospect. But how viable is this idea? This paper endeavors to test the limits of current-day LLMs with a pre-registered study integrating real people with LLM agents acting as people. The study focuses on debate-based opinion consensus formation in three environments: humans only, agents and humans, and agents only. Our goal is to understand how LLM agents influence humans, and how capable they are in debating like humans. We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.
    
[^65]: 机器人出租车事故的解剖：从Cruise行人拖拽事故中吸取教训

    Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap

    [https://arxiv.org/abs/2402.06046](https://arxiv.org/abs/2402.06046)

    2023年10月，一起GM Cruise机器人出租车与行人相撞的事故给公司造成了巨大的动荡，同时也揭示了该公司在处理事故后的失误。这一事件的解剖提供了在技术、操作安全实践和组织反应方面的安全教训。

    

    2023年10月，在旧金山，一辆通用汽车Cruise的机器人出租车与一名行人相撞，造成了严重的伤害，同时也对该公司造成了巨大的动荡，这很可能会对整个行业产生长期影响。问题不仅仅源于事故本身，还包括Cruise在处理机器人出租车撞到行人后被拖行的过程中的失误。两份外部调查报告提供了描述事件的原始材料，并从监管互动的角度批评了公司的反应，但并未包含潜在的安全建议。我们利用这些报告材料来强调具体的事实和事件之间的关系，通过将报告材料的不同部分联系起来。然后我们探讨在技术、操作安全实践和组织对事件的反应方面可能可以学到的安全教训。

    An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. The issues stem not just from the crash facts themselves, but also how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. A pair of external investigation reports provide raw material describing the incident and critique the company response from a regulatory interaction point of view, but did not include potential safety recommendations in scope. We use that report material to highlight specific facts and relationships between events by tying together different pieces of the report material. We then explore safety lessons that might be learned with regard to technology, operational safety practices, and organizational reaction to incidents.
    
[^66]: 开放理论-心灵（OpenToM）：评估大型语言模型的心灵理解能力的全面基准

    OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models

    [https://arxiv.org/abs/2402.06044](https://arxiv.org/abs/2402.06044)

    OpenToM是一个评估大型语言模型心理理解能力的全面基准，通过提供更长、更清晰的叙事故事、具有明确个性特征的角色、以及挑战模型对心理状态的理解能力的问题，揭示了现有模型在理解物理世界与心理世界的角色心理状态方面的优势和不足。

    

    神经心理理论（N-ToM）是机器理解和跟踪他人心理状态的能力，在开发具有社交智能的代理程序中至关重要。然而，目前的N-ToM基准存在一些问题，包括模糊和人工故事的存在，缺乏个性特征和偏好，缺乏涉及角色心理心态的问题，并且提出的问题多样性有限。为了应对这些问题，我们构建了OpenToM，一个新的评估N-ToM的基准，以 (1) 更长、更清晰的叙事故事，(2) 具有明确个性特征的角色，(3) 触发角色意图的行动，以及 (4) 设计旨在挑战LLMs对建模角色在物理和心理世界的心理状态能力的问题。使用OpenToM，我们发现目前最先进的LLMs在建模物理世界的一些心理状态方面表现出色，但在跟踪角色心理状态方面存在不足。

    Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
    
[^67]: 免先验正无标（Positive Unlabeled）学习的对比方法

    Contrastive Approach to Prior Free Positive Unlabeled Learning

    [https://arxiv.org/abs/2402.06038](https://arxiv.org/abs/2402.06038)

    该论文提出了一种免先验正无标学习的对比方法，通过预训练不变表示学习特征空间并利用嵌入的浓度特性对未标记样本进行伪标签处理，相比现有方法，在多个标准数据集上表现优异，同时不需要先验知识或类先验的估计。

    

    正无标（Positive Unlabeled）学习是指在给定少量标记的正样本和一组未标记样本（可能是正例或负例）的情况下学习一个二分类器的任务。在本文中，我们提出了一种新颖的正无标学习框架，通过保证不变表示学习学习特征空间，并利用嵌入的浓度特性对未标记样本进行伪标签处理。总体而言，我们提出的方法在多个标准正无标基准数据集上轻松超越了现有的正无标学习方法，而不需要先验知识或类先验的估计。值得注意的是，我们的方法在标记数据稀缺的情况下仍然有效，而大多数正无标学习算法则失败。我们还提供了简单的理论分析来推动我们提出的算法，并为我们的方法建立了一般化保证。

    Positive Unlabeled (PU) learning refers to the task of learning a binary classifier given a few labeled positive samples, and a set of unlabeled samples (which could be positive or negative). In this paper, we propose a novel PU learning framework, that starts by learning a feature space through pretext-invariant representation learning and then applies pseudo-labeling to the unlabeled examples, leveraging the concentration property of the embeddings. Overall, our proposed approach handily outperforms state-of-the-art PU learning methods across several standard PU benchmark datasets, while not requiring a-priori knowledge or estimate of class prior. Remarkably, our method remains effective even when labeled data is scant, where most PU learning algorithms falter. We also provide simple theoretical analysis motivating our proposed algorithms and establish generalization guarantee for our approach.
    
[^68]: 使用迷你像素批量梯度下降优化物理设计流程中的预测AI

    Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent

    [https://arxiv.org/abs/2402.06034](https://arxiv.org/abs/2402.06034)

    本论文提出了一种迷你像素批量梯度下降（MPGD）算法，用于优化物理设计流程中的预测AI。实验证明MPGD在各种物理设计预测任务中具有显著的优势。

    

    爆炸式的预测AI在现代芯片物理设计流程中实现了快速而有效的评估和决策。现有的最先进框架通常包括最小化预测与真实值之间的均方误差（MSE）的目标。我们认为MSE的平均效果导致模型训练和部署两方面都存在局限性，而良好的MSE行为不能保证这些模型在可能由于少量预测误差而受损的物理设计流程中的能力。为了解决这个问题，我们提出了迷你像素批量梯度下降（MPGD），这是一种即插即用的优化算法，它考虑了最具信息量的条目，可能提供更快更好的收敛性。在代表性基准套件上的实验表明，MPGD在使用CNN或基于图的模型进行各种物理设计预测任务时具有显著的优势。

    Exploding predictive AI has enabled fast yet effective evaluation and decision-making in modern chip physical design flows. State-of-the-art frameworks typically include the objective of minimizing the mean square error (MSE) between the prediction and the ground truth. We argue the averaging effect of MSE induces limitations in both model training and deployment, and good MSE behavior does not guarantee the capability of these models to assist physical design flows which are likely sabotaged due to a small portion of prediction error. To address this, we propose mini-pixel batch gradient descent (MPGD), a plug-and-play optimization algorithm that takes the most informative entries into consideration, offering probably faster and better convergence. Experiments on representative benchmark suits show the significant benefits of MPGD on various physical design prediction tasks using CNN or Graph-based models.
    
[^69]: 博弈论对图神经网络的反事实解释

    Game-theoretic Counterfactual Explanation for Graph Neural Networks

    [https://arxiv.org/abs/2402.06030](https://arxiv.org/abs/2402.06030)

    本文提出了一种半值法的、非学习的方法来生成图神经网络的反事实解释，消除了额外训练的需要。与其他流行的方法相比，计算Banzhaf值在识别反事实解释时需要更低的样本复杂性，并且可以实现四倍的加速。

    

    图神经网络（GNNs）在复杂网络中的节点分类任务中是一种强大的工具。然而，它们的决策过程对用户来说仍然是一个黑盒子，这使得理解其预测背后的推理变得困难。反事实解释（CFE）已经显示出增强机器学习模型可解释性的潜力。先前的基于学习的方法计算GNNs的CFE通常需要训练额外的图形。在本文中，我们提出了一种基于半值的、非学习的方法来生成节点分类任务的CFE，消除了任何额外训练的需要。我们的结果表明，与计算Shapley值等其他流行方法相比，计算Banzhaf值需要更低的样本复杂性来识别反事实解释。我们的经验证据表明，与Shapley值相比，计算Banzhaf值可以实现四倍的加速。我们还设计了一个阈值化方法。

    Graph Neural Networks (GNNs) have been a powerful tool for node classification tasks in complex networks. However, their decision-making processes remain a black-box to users, making it challenging to understand the reasoning behind their predictions. Counterfactual explanations (CFE) have shown promise in enhancing the interpretability of machine learning models. Prior approaches to compute CFE for GNNS often are learning-based approaches that require training additional graphs. In this paper, we propose a semivalue-based, non-learning approach to generate CFE for node classification tasks, eliminating the need for any additional training. Our results reveals that computing Banzhaf values requires lower sample complexity in identifying the counterfactual explanations compared to other popular methods such as computing Shapley values. Our empirical evidence indicates computing Banzhaf values can achieve up to a fourfold speed up compared to Shapley values. We also design a thresholding
    
[^70]: 使用集成学习的量子神经网络以缓解平板坡和成本函数集中问题

    Quantum neural network with ensemble learning to mitigate barren plateaus and cost function concentration

    [https://arxiv.org/abs/2402.06026](https://arxiv.org/abs/2402.06026)

    本研究提出了一种使用集成学习的量子神经网络构建方法，旨在解决消失梯度和成本函数集中问题，并通过与传统构建的QNN进行比较分析，评估了其有效性。

    

    量子计算机的快速发展承诺在科学和技术领域产生变革性影响。量子神经网络（QNN）作为前沿应用具有重要潜力。尽管文献中提出了许多模型，但持续存在的挑战，特别是消失梯度（VG）和成本函数集中（CFC）问题，阻碍了它们的广泛成功。在本研究中，我们提出了一种新的量子神经网络构建方法，特别解决了VG和CFC问题。我们的方法采用集成学习，推崇同时部署多个深度为1的量子电路，而不是传统单一深度为L的量子电路。通过与传统构建的QNN进行比较分析，我们评估了我们提出的模型的有效性。评估在分类问题的背景下展开，为了对量子神经网络的潜在前景提供宝贵的见解。

    The rapid development of quantum computers promises transformative impacts across diverse fields of science and technology. Quantum neural networks (QNNs), as a forefront application, hold substantial potential. Despite the multitude of proposed models in the literature, persistent challenges, notably the vanishing gradient (VG) and cost function concentration (CFC) problems, impede their widespread success. In this study, we introduce a novel approach to quantum neural network construction, specifically addressing the issues of VG and CFC. Our methodology employs ensemble learning, advocating for the simultaneous deployment of multiple quantum circuits with a depth equal to $1$, a departure from the conventional use of a single quantum circuit with depth $L$. We assess the efficacy of our proposed model through a comparative analysis with a conventionally constructed QNN. The evaluation unfolds in the context of a classification problem, yielding valuable insights into the potential a
    
[^71]: 决策理论引导的深度强化学习用于快速学习

    Decision Theory-Guided Deep Reinforcement Learning for Fast Learning

    [https://arxiv.org/abs/2402.06023](https://arxiv.org/abs/2402.06023)

    决策理论引导的深度强化学习（DT-guided DRL）通过整合决策理论原则，实现了对DRL智能体的有效初始引导，并促进了在复杂环境中更高效可靠的学习过程。

    

    本文介绍了一种新颖的方法，即决策理论引导的深度强化学习（DT-guided DRL），用于解决强化学习中固有的冷启动问题。通过整合决策理论原则，DT-guided DRL增强了智能体在复杂环境中的初始性能和鲁棒性，使得学习过程更高效可靠。我们的研究涵盖了两个主要问题背景：杆车和迷宫导航挑战。实验结果表明，决策理论的整合不仅有助于对DRL智能体进行有效的初始引导，还促进了在具有大规模和复杂状态空间的环境中更有结构和知情的探索策略。实验结果表明，与常规的DRL相比，DT-guided DRL能够提供显著更高的奖励。具体而言，在训练的初始阶段，DT-guided DRL的累积奖励增加了184%。

    This paper introduces a novel approach, Decision Theory-guided Deep Reinforcement Learning (DT-guided DRL), to address the inherent cold start problem in DRL. By integrating decision theory principles, DT-guided DRL enhances agents' initial performance and robustness in complex environments, enabling more efficient and reliable convergence during learning. Our investigation encompasses two primary problem contexts: the cart pole and maze navigation challenges. Experimental results demonstrate that the integration of decision theory not only facilitates effective initial guidance for DRL agents but also promotes a more structured and informed exploration strategy, particularly in environments characterized by large and intricate state spaces. The results of experiment demonstrate that DT-guided DRL can provide significantly higher rewards compared to regular DRL. Specifically, during the initial phase of training, the DT-guided DRL yields up to an 184% increase in accumulated reward. Mo
    
[^72]: 内存高效的视觉Transformer：一种激活感知的混合秩压缩策略

    Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy

    [https://arxiv.org/abs/2402.06004](https://arxiv.org/abs/2402.06004)

    本文提出了一种激活感知的混合秩压缩策略来提高视觉Transformer的内存效率，并通过选择性低秩权重张量近似和层间误差补偿技术来减少参数数量。这种策略避免了浅层局部最小值陷阱，同时取得了优秀的结果。

    

    随着视觉Transformer（ViTs）在计算机视觉领域不断刷新最新记录，它们在推理引擎上的实际部署往往受到显著的内存带宽和（芯片内）内存占用的限制。本文通过引入一种激活感知的模型压缩方法来解决这一内存限制问题，该方法使用不同层的选择性低秩权重张量近似来减少ViTs的参数数量。关键思想是将权重张量分解为两个参数高效的张量之和，同时将输入激活与原始权重张量的乘积与输入激活与近似张量之和的乘积之间的误差最小化。通过采用有效的逐层误差补偿技术，利用层输出损失的梯度进一步改进了这种近似。这些技术的组合在避免陷入浅层局部最小值的同时取得了优秀的结果。

    As Vision Transformers (ViTs) increasingly set new benchmarks in computer vision, their practical deployment on inference engines is often hindered by their significant memory bandwidth and (on-chip) memory footprint requirements. This paper addresses this memory limitation by introducing an activation-aware model compression methodology that uses selective low-rank weight tensor approximations of different layers to reduce the parameter count of ViTs. The key idea is to decompose the weight tensors into a sum of two parameter-efficient tensors while minimizing the error between the product of the input activations with the original weight tensor and the product of the input activations with the approximate tensor sum. This approximation is further refined by adopting an efficient layer-wise error compensation technique that uses the gradient of the layer's output loss. The combination of these techniques achieves excellent results while it avoids being trapped in a shallow local minim
    
[^73]: 大型代码模型是否理解编程概念？一种黑盒方法探究

    Do Large Code Models Understand Programming Concepts? A Black-box Approach

    [https://arxiv.org/abs/2402.05980](https://arxiv.org/abs/2402.05980)

    本文使用反事实分析框架评估了十个大型代码模型对四种编程概念的理解情况，发现当前模型缺乏对数据流和控制流等概念的理解。

    

    大型语言模型在文本生成方面的成功也使其在代码生成和编码任务方面表现更好。虽然有很多工作展示了它们在代码补全和编辑等任务上的出色性能，但为什么它们能够成功还不清楚。我们通过探索自回归模型对底层程序的逻辑结构理解程度，来填补这一差距。我们提出了用于编程概念谓词的反事实分析（CACP）作为一种反事实测试框架，以评估大型代码模型是否理解编程概念。只通过黑盒访问模型，我们使用CACP评估了十个流行的大型代码模型对四个不同编程概念的理解情况。我们的研究结果表明，当前模型缺乏对数据流和控制流等概念的理解。

    Large Language Models' success on text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.
    
[^74]: 关于行为使用条款的标准化及其用于负责任授权人工智能的采用

    On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI

    [https://arxiv.org/abs/2402.05979](https://arxiv.org/abs/2402.05979)

    本文研究了行为使用条款的标准化及其用于负责任授权人工智能的采用。采用了定性访谈、许可证条款的聚类和许可证采用的定量分析的混合方法学。结论是负责任AI许可证需要标准化。

    

    对AI的疏忽或恶意使用的忧虑日益增长，这增加了对帮助管理技术风险的工具的需求。2018年，提出了带有行为使用条款（通常称为负责任AI许可证）的许可证，为开发者提供了发布AI资产的框架，同时指定其用户以减轻负面应用。截至2023年底，大约有40,000个软件和模型存储库采用了负责任AI许可证。采用行为使用条款的著名模型包括BLOOM（语言）和LLaMA2（语言）、Stable Diffusion（图像）和GRID（机器人）。本文探讨了这些许可证为何以及如何被采用，以及为何以及如何被改编以适应特定的使用情况。我们采用了定性访谈、许可证条款的聚类和许可证采用的定量分析的混合方法学。根据这些证据，我们认为负责任AI许可证需要标准化。

    Growing concerns over negligent or malicious uses of AI have increased the appetite for tools that help manage the risks of the technology. In 2018, licenses with behaviorial-use clauses (commonly referred to as Responsible AI Licenses) were proposed to give developers a framework for releasing AI assets while specifying their users to mitigate negative applications. As of the end of 2023, on the order of 40,000 software and model repositories have adopted responsible AI licenses licenses. Notable models licensed with behavioral use clauses include BLOOM (language) and LLaMA2 (language), Stable Diffusion (image), and GRID (robotics). This paper explores why and how these licenses have been adopted, and why and how they have been adapted to fit particular use cases. We use a mixed-methods methodology of qualitative interviews, clustering of license clauses, and quantitative analysis of license adoption. Based on this evidence we take the position that responsible AI licenses need standa
    
[^75]: 结合形状和轮廓特征来提高铣削过程中的刀具磨损监测

    Combining shape and contour features to improve tool wear monitoring in milling processes

    [https://arxiv.org/abs/2402.05978](https://arxiv.org/abs/2402.05978)

    本文提出了一种新的系统，结合了形状描述符和轮廓描述符，用于铣削过程中插入物的磨损监测。实验结果表明，使用后期融合方法将两个描述符组合在一起，可以显著提高分类性能，取得更好的准确率。

    

    本文提出了一种新的基于形状描述符和轮廓描述符组合的系统，用于根据磨损程度对铣削过程中的插入物进行分类，采用基于计算机视觉的方法。为了描述磨损区域的形状，我们提出了一种新的描述符ShapeFeat，并使用BORCHIZ方法对其轮廓进行表征，根据我们的调查，该方法在基于计算机视觉的刀具磨损监测中取得了最佳性能。实验结果表明，使用后期融合方法将BORCHIZ和ShapeFeat组合在一起，显著提高了分类性能，二元分类将磨损分为高或低的准确率达到91.44%，三个目标类别（高、中、低磨损）的准确率达到82.90%。这些结果优于单独使用两个描述符的结果，其准确率分别为88.70%和80.67%。

    In this paper, a new system based on combinations of a shape descriptor and a contour descriptor has been proposed for classifying inserts in milling processes according to their wear level following a computer vision based approach. To describe the wear region shape we have proposed a new descriptor called ShapeFeat and its contour has been characterized using the method BORCHIZ that, to the best of our knowledge, achieves the best performance for tool wear monitoring following a computer vision-based approach. Results show that the combination of BORCHIZ with ShapeFeat using a late fusion method improves the classification performance significantly, obtaining an accuracy of 91.44% in the binary classification (i.e. the classification of the wear as high or low) and 82.90% using three target classes (i.e. classification of the wear as high, medium or low). These results outperform the ones obtained by both descriptors used on their own, which achieve accuracies of 88.70 and 80.67% for
    
[^76]: 基于局部纹理的在线、自动和低成本系统进行刀具磨损监测

    Tool wear monitoring using an online, automatic and low cost system based on local texture

    [https://arxiv.org/abs/2402.05977](https://arxiv.org/abs/2402.05977)

    本研究提出了一种基于计算机视觉和机器学习的在线、低成本和快速方法，用于切削工具的磨损监测。通过将切削边缘图像分割成不同的区域，并使用局部二值模式的纹理描述符来判断每个区域的磨损程度，从而确定切削边缘和刀具是否可服役或可丢弃。

    

    在本研究中，我们提出了一种基于计算机视觉和机器学习的新的在线、低成本和快速方法，用于确定边缘轮廓铣削过程中使用的切削工具是否可服役或可丢弃，根据它们的磨损程度。我们创建了一个由254张边缘轮廓切削头图像组成的新数据集，根据我们所知，这是第一个公开可用且具有足够质量的数据集。所有刀片都被分割，并且其切削边缘被裁剪，获得了577张切削边缘图像：301张可用和276张可丢弃。所提出的方法基于（1）将切削边缘图像分为不同的区域，称为磨损斑块（WP），（2）使用基于不同类型的局部二值模式（LBP）的纹理描述符来表征每个区域是磨损还是可用，并（3）根据这些WP的状态来确定切削边缘（因此也是刀具）是否可服役或可丢弃。我们提出并评估了五种不同的斑块分割方法。

    In this work we propose a new online, low cost and fast approach based on computer vision and machine learning to determine whether cutting tools used in edge profile milling processes are serviceable or disposable based on their wear level. We created a new dataset of 254 images of edge profile cutting heads which is, to the best of our knowledge, the first publicly available dataset with enough quality for this purpose. All the inserts were segmented and their cutting edges were cropped, obtaining 577 images of cutting edges: 301 functional and 276 disposable. The proposed method is based on (1) dividing the cutting edge image in different regions, called Wear Patches (WP), (2) characterising each one as worn or serviceable using texture descriptors based on different variants of Local Binary Patterns (LBP) and (3) determine, based on the state of these WP, if the cutting edge (and, therefore, the tool) is serviceable or disposable. We proposed and assessed five different patch divis
    
[^77]: RankSum：一种基于排名融合的无监督抽取式文本摘要方法

    RankSum An unsupervised extractive text summarization based on rank fusion

    [https://arxiv.org/abs/2402.05976](https://arxiv.org/abs/2402.05976)

    RankSum是一种无监督的抽取式文本摘要方法，它利用多维度句子特征对句子进行排名，然后通过加权融合确定句子的重要性排名。该方法不需要监督信号，可以推广到其他数据集。

    

    本文提出了一种名为Ranksum的方法，用于基于排名融合的无监督单文档抽取式文本摘要。该方法利用为每个句子提取的四个多维度句子特征进行句子显著性排名：主题信息、语义内容、重要关键词和位置。Ranksum根据每个特征生成的句子显著性排名进行加权融合，以确定句子的重要性排名。融合权重是完全无监督生成的，需要标记的文档集合来学习融合权重。我们发现融合权重可以推广到其他数据集，因此将Ranksum视为一种无监督方法。为了确定主题排名，我们使用概率主题模型，而使用句子嵌入来捕捉语义信息。使用句子嵌入来生成排名时，我们利用连体网络产生抽象化的句子表示，然后形成排名。

    In this paper, we propose Ranksum, an approach for extractive text summarization of single documents based on the rank fusion of four multi-dimensional sentence features extracted for each sentence: topic information, semantic content, significant keywords, and position. The Ranksum obtains the sentence saliency rankings corresponding to each feature in an unsupervised way followed by the weighted fusion of the four scores to rank the sentences according to their significance. The scores are generated in completely unsupervised way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an unsupervised approach. To determine topic rank, we employ probabilistic topic models whereas semantic information is captured using sentence embeddings. To derive rankings using sentence embeddings, we utilize Siamese networks to produce abstractive sentence representation and then we form
    
[^78]: 一种使用多尺度卷积神经网络的深度学习方法进行脑肿瘤分类和分割

    A Deep Learning Approach for Brain Tumor Classification and Segmentation Using a Multiscale Convolutional Neural Network

    [https://arxiv.org/abs/2402.05975](https://arxiv.org/abs/2402.05975)

    本文提出了一种使用多尺度卷积神经网络的深度学习方法，可以自动进行脑肿瘤的分类和分割。通过与其他方法的比较，我们的方法在公开数据集上获得了较高的分类准确率。

    

    在本文中，我们提出了一种完全自动的脑肿瘤分割和分类模型，使用了包括多尺度方法在内的深度卷积神经网络。我们的方法与之前的工作的一个区别是输入图像在不同处理路径上以三个空间尺度进行处理。这个机制是受到人类视觉系统的内在操作的启示。提出的神经模型可以分析包含三种类型肿瘤（脑膜瘤、胶质瘤和垂体瘤）的MRI图像，包括矢状面、冠状面和轴面，并且不需要预处理输入图像事先移除头骨或椎骨部分。我们的方法在一个公开可用的包含233名患者3064张切片的MRI图像数据集上的性能与之前的经典机器学习和深度学习方法进行了比较。在比较中，我们的方法明显地获得了0.973的肿瘤分类准确率，高于其他方法。

    In this paper, we present a fully automatic brain tumor segmentation and classification model using a Deep Convolutional Neural Network that includes a multiscale approach. One of the differences of our proposal with respect to previous works is that input images are processed in three spatial scales along different processing pathways. This mechanism is inspired in the inherent operation of the Human Visual System. The proposed neural model can analyze MRI images containing three types of tumors: meningioma, glioma, and pituitary tumor, over sagittal, coronal, and axial views and does not need preprocessing of input images to remove skull or vertebral column parts in advance. The performance of our method on a publicly available MRI image dataset of 3064 slices from 233 patients is compared with previously classical machine learning and deep learning published methods. In the comparison, our method remarkably obtained a tumor classification accuracy of 0.973, higher than the other app
    
[^79]: 使用神经离散学习和专家级别建模空时动力系统

    Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts

    [https://arxiv.org/abs/2402.05970](https://arxiv.org/abs/2402.05970)

    本文提出了使用神经离散学习和专家级别建模空时动力系统的方法。通过引入通用的专家模块和精细设计的物理流水线，可以在更广泛的现实世界背景下有效地建模和估计空时动态系统的状态变化。

    

    本文针对基于一系列观测（如视频帧）的空时动态系统中状态变化的建模和估计问题进行了研究。传统的数值模拟系统在很大程度上依赖于初始设置和构建的偏微分方程（PDE）的正确性。尽管最近利用神经网络发现了基于数据的PDE模型的重大成功，但是奇异场景和缺乏局部洞察力的限制阻碍了它们在更广泛的现实世界背景下的有效性。为此，本文提出了通用的专家模块——光流估计组件，以数据驱动的方式捕捉一般物理过程的演化规律。为了增强局部洞察力，我们精心设计了一个更精细的物理流水线，因为局部特征可能受到各种内部上下文信息的影响，这可能与宏观属性相矛盾。

    In this paper, we address the issue of modeling and estimating changes in the state of the spatio-temporal dynamical systems based on a sequence of observations like video frames. Traditional numerical simulation systems depend largely on the initial settings and correctness of the constructed partial differential equations (PDEs). Despite recent efforts yielding significant success in discovering data-driven PDEs with neural networks, the limitations posed by singular scenarios and the absence of local insights prevent them from performing effectively in a broader real-world context. To this end, this paper propose the universal expert module -- that is, optical flow estimation component, to capture the evolution laws of general physical processes in a data-driven fashion. To enhance local insight, we painstakingly design a finer-grained physical pipeline, since local characteristics may be influenced by various internal contextual information, which may contradict the macroscopic pro
    
[^80]: 欧盟人工智能法案下的联邦学习优先事项

    Federated Learning Priorities Under the European Union Artificial Intelligence Act

    [https://arxiv.org/abs/2402.05968](https://arxiv.org/abs/2402.05968)

    欧盟人工智能法案可能推动联邦学习朝主流采用方向发展，并提出了数据隐私、性能和能源效率等方面的新挑战。

    

    AI监管时代已经来临，欧盟人工智能法案（AI Act）引领着潮流。我们的关键问题是，这将如何影响以数据隐私为优先并进行机器学习的联邦学习（FL），其与集中式学习的出发点根本不同。我们相信AI法案和未来的监管可能是推动FL走向主流采用的缺失催化剂。然而，这只能发生在FL社区重新优先考虑其研究重点的情况下。在我们的立场论文中，我们进行了首次跨学科分析（法律和机器学习），分析了AI法案对FL可能产生的影响，并通过定量和定性分析进行了一系列支持我们主要观点的观察。我们探讨了数据治理问题和对隐私的担忧。我们确定了在生命周期监视中性能和能源效率方面的新挑战。综合我们的分析，表明FL有着巨大的机会，

    The age of AI regulation is upon us, with the European Union Artificial Intelligence Act (AI Act) leading the way. Our key inquiry is how this will affect Federated Learning (FL), whose starting point of prioritizing data privacy while performing ML fundamentally differs from that of centralized learning. We believe the AI Act and future regulations could be the missing catalyst that pushes FL toward mainstream adoption. However, this can only occur if the FL community reprioritizes its research focus. In our position paper, we perform a first-of-its-kind interdisciplinary analysis (legal and ML) of the impact the AI Act may have on FL and make a series of observations supporting our primary position through quantitative and qualitative analysis. We explore data governance issues and the concern for privacy. We establish new challenges regarding performance and energy efficiency within lifecycle monitoring. Taken together, our analysis suggests there is a sizable opportunity for FL to 
    
[^81]: 最后之舞：通过扩散模型和贝叶斯方法进行鲁棒后门攻击

    The last Dance : Robust backdoor attack via diffusion models and bayesian approach

    [https://arxiv.org/abs/2402.05967](https://arxiv.org/abs/2402.05967)

    本文介绍了一种通过扩散模型和贝叶斯方法进行鲁棒后门攻击的方法，具体应用于音频Transformer模型，并证明了攻击的可行性。

    

    扩散模型是最先进的深度学习生成模型，其通过逐步添加噪音和去噪的方式学习正向和反向扩散过程的原理进行训练。本文旨在欺骗基于音频的DNN模型，例如Hugging Face框架中的音频模型，特别是基于Transformer的人工智能模型，这些模型是强大的机器学习模型，节省时间，提供更高效的结果。我们证明了在Hugging Face推导出的音频Transformer上实现后门攻击（称为`BacKBayDiffMod`）的可行性。本文中开发的后门攻击基于毒化模型的训练数据，涉及后门扩散采样和贝叶斯方法分布的引入。

    Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
    
[^82]: 重新思考模型重新基底和线性模态连接性

    Rethink Model Re-Basin and the Linear Mode Connectivity

    [https://arxiv.org/abs/2402.05966](https://arxiv.org/abs/2402.05966)

    本论文重新审视了模型重新基底的现象，并发现了现有匹配算法的不足。通过适当的重归一化，我们改进了匹配算法，并揭示了它与重归一化过程的相互作用。这为剪枝提供了新的理解，推动了一种轻量且有效的后剪枝插件的开发。

    

    最近的研究表明，对于足够宽的模型来说，大部分随机梯度下降（SGD）的解可以收敛到相同的基底，只是顺序可能不同。这种现象被称为模型重新基底的阶段，对于模型平均化有重要影响。然而，当前的重新基底策略在效果上存在局限性，因为对底层机制的理解不够全面。为了填补这一空白，我们的研究重新审视了标准做法，并揭示了现有匹配算法的频繁不足之处，我们通过适当的重归一化来缓解这些问题。通过引入更直接的分析方法，我们揭示了匹配算法与重归一化过程之间的相互作用。这种观点不仅澄清和改进了以前的研究结果，还促进了新的洞见。例如，它将线性模态连接性与剪枝联系起来，从而激发了一种轻量且有效的后剪枝插件，可以直接与任何现有的剪枝技术合并。

    Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
    
[^83]: 节俭的演员-评论家模型：使用独特经历的高效离线深度强化学习

    Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences

    [https://arxiv.org/abs/2402.05963](https://arxiv.org/abs/2402.05963)

    该方法通过选择独特样本并添加到回放缓冲器中以实现样本效率，在复杂动态系统的无模型控制策略合成中起着重要作用。

    

    在用于复杂动态系统的无模型控制策略合成中，对回放缓冲器的高效利用在离线演员-评论家强化学习算法中起着重要作用。我们提出了一种实现样本效率的方法，该方法通过在探索过程中选择独特样本并将其添加到回放缓冲器中，旨在减小缓冲器的大小并保持样本的独立同分布（IID）的性质。我们的方法基于在随机探索的初始阶段遇到的经历中选择一组重要的状态变量的重要子集，根据所选重要状态变量将状态空间划分为一组抽象状态，最后通过使用核密度估计器选择具有独特状态-奖励组合的经历。我们严格证明了将所提出的独特经历方法纳入离线演员-评论家算法中的有效性。

    Efficient utilization of the replay buffer plays a significant role in the off-policy actor-critic reinforcement learning (RL) algorithms used for model-free control policy synthesis for complex dynamical systems. We propose a method for achieving sample efficiency, which focuses on selecting unique samples and adding them to the replay buffer during the exploration with the goal of reducing the buffer size and maintaining the independent and identically distributed (IID) nature of the samples. Our method is based on selecting an important subset of the set of state variables from the experiences encountered during the initial phase of random exploration, partitioning the state space into a set of abstract states based on the selected important state variables, and finally selecting the experiences with unique state-reward combination by using a kernel density estimator. We formally prove that the off-policy actor-critic algorithm incorporating the proposed method for unique experience
    
[^84]: 自然启发的局部传播

    Nature-Inspired Local Propagation

    [https://arxiv.org/abs/2402.05959](https://arxiv.org/abs/2402.05959)

    本文介绍了一种自然启发的局部传播算法，该算法通过在线处理环境信息而不依赖大量数据集，在机器学习领域具有潜力。这种算法的核心思想是结合数据表示和学习，以尊重时空局部性，并且当传播速度趋近于无穷大时，它等效于反向传播算法。

    

    机器学习中取得的令人瞩目的成果，包括最近在生成性人工智能方面的进展，都依赖于大量的数据集。相反，自然界中的智能过程并不需要这样的数据集，而只需通过对环境信息的在线处理即可产生。特别是，自然学习过程依赖于数据表示和学习相互交织以尊重时空局部性的机制。本文展示了这种特性来自于对学习的预算法视角，该视角受到了理论物理学相关研究的启发。我们展示了当传播速度趋于无穷大时，所得到的“学习法则”的算法解释（采用哈密顿方程结构）将归结为反向传播算法。这为基于全面在线信息处理的机器学习研究开辟了新的道路，其中反向传播算法被提出的时空局部算法取代。

    The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections. On the opposite, intelligent processes in nature arises without the need for such collections, but simply by online processing of the environmental information. In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality. This paper shows that such a feature arises from a pre-algorithmic view of learning that is inspired by related studies in Theoretical Physics. We show that the algorithmic interpretation of the derived "laws of learning", which takes the structure of Hamiltonian equations, reduces to Backpropagation when the speed of propagation goes to infinity. This opens the doors to machine learning studies based on full on-line information processing that are based the replacement of Backpropagation with the proposed spatiotemporal local algori
    
[^85]: 利用大型语言模型推进图表示学习：技术全面调查

    Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques

    [https://arxiv.org/abs/2402.05952](https://arxiv.org/abs/2402.05952)

    本综述调查了将大型语言模型（LLM）与图表示学习（GRL）相结合的技术，并提供了一个新颖的分类法，深入分析了这些模型的核心组成部分和操作技术，为有效的模型设计和训练策略提供了新的视角。

    

    大型语言模型（LLM）与图表示学习（GRL）的整合标志着分析复杂数据结构的重大进展。这种合作利用LLM的先进语言能力来改进图模型的上下文理解能力和适应性，从而拓宽了GRL的范围和潜力。尽管已经有大量的研究致力于将LLM集成到图领域中，但缺乏一份深入分析这些模型核心组成部分和操作技术的全面综述。我们的调查通过提出一种新颖的分类法来分解这些模型为主要组成部分和操作技术，从新的技术角度深入分析。我们进一步将最近的文献分解为两个主要组成部分，包括知识提取器和组织者，以及两个操作技术，包括集成和训练策略，揭示出有效的模型设计和训练策略的要点。

    The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures. This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL. Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking. Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective. We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies. Additio
    
[^86]: \textit{MinMaxMin} $Q$-learning

    \textit{MinMaxMin} $Q$-learning

    [https://arxiv.org/abs/2402.05951](https://arxiv.org/abs/2402.05951)

    \textit{MinMaxMin} $Q$-learning是一种乐观型Actor-Critic算法，通过解决过高估计偏差的问题，在各种基准任务中相对于现有算法表现出稳定的性能提升。

    

    \textit{MinMaxMin} $Q$-learning是一种新颖的乐观型Actor-Critic算法，解决了保守型强化学习算法中存在的过高估计偏差的问题（$Q$-估计过高估计了真实的$Q$值）。其核心公式依赖于$Q$-网络之间的差异，采用最小批次最大最小$Q$-网络距离作为$Q$-目标加入，并作为优先级经验回放采样规则。我们在TD3和TD7之上实施了\textit{MinMaxMin}，并对其在流行的MuJoCo和Bullet环境中对抗现有的连续空间算法-DDPG，TD3和TD7进行了严格测试。结果显示，在所有测试任务中，\textit{MinMaxMin}相对于DDPG，TD3和TD7均表现出了稳定的性能提升。

    \textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
    
[^87]: SQT - std Q-target

    \textit{SQT} -- \textit{std} $Q$-target

    [https://arxiv.org/abs/2402.05950](https://arxiv.org/abs/2402.05950)

    SQT是一种基于Q-学习的保守型actor-critic算法，利用Q网络的标准差作为一种“不确定性惩罚”，成功解决了过高估计偏差问题，相较于TD3的Q-target公式具有更好的性能优势。

    

    Std Q-target是一种基于Q-学习的保守型actor-critic算法，它基于一个关键的Q公式：Q网络的标准差，这个标准差作为一种“不确定性惩罚”，是对过高估计偏差问题的一种简约解决方案。我们在TD3/TD7代码的基础上实现了SQT，并将其与最先进的actor-critic算法DDPG、TD3和TD7在七个常见的MuJoCo和Bullet任务上进行了测试。我们的结果表明，在强化学习中，SQT的Q-target公式相对于TD3的Q-target公式在解决过高估计偏差的保守解方面具有优势，而在所有任务中，SQT相对于DDPG、TD3和TD7都有明显的性能优势。

    \textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
    
[^88]: 揭示潜在因果规律：一种基于时间点过程的异常事件解释方法

    Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation

    [https://arxiv.org/abs/2402.05946](https://arxiv.org/abs/2402.05946)

    本文提出了一种基于时间点过程的方法，通过揭示潜在因果规律来解释异常事件，以帮助在高风险系统如医疗保健中快速诊断和精确治疗规划。该方法通过期望最大化算法优化规则集和模型参数，实现了准确的规则发现和根因识别。

    

    在高风险系统如医疗保健中，理解异常事件背后的因果原因是至关重要的，例如患者健康状况的突然变化。揭示因果原因有助于快速诊断和精确治疗规划。在本文中，我们提出了一种自动化方法来揭示解释观察事件的“如果-那么”逻辑规则。我们引入了时间点过程来建模所关注事件，并发现一组潜在规则来解释事件的发生。为了实现这一点，我们采用了期望最大化（EM）算法。在E步中，我们计算每个事件被每个发现的规则解释的可能性。在M步中，我们更新规则集和模型参数，以增强可能性函数的下界。值得注意的是，我们以微分的方式优化规则集。我们的方法在发现规则和识别根本原因方面表现出准确的性能。我们使用合成数据展示了它的有希望的结果。

    In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using syntheti
    
[^89]: 通过有监督的、分层概念学习消除硬概念瓶颈模型中的信息泄漏问题

    Eliminating Information Leakage in Hard Concept Bottleneck Models with Supervised, Hierarchical Concept Learning

    [https://arxiv.org/abs/2402.05945](https://arxiv.org/abs/2402.05945)

    本文解决了概念瓶颈模型中的信息泄漏问题，通过引入标签监督和构建分层概念集，提出了一种新的CBMs范例（SupCBM），它可以通过预测的概念和干预矩阵实现标签预测，并且只在不同的类别之间进行区分。

    

    概念瓶颈模型（CBMs）旨在通过将特征和标签与人类可理解的概念联系起来，提供可解释和可干预的预测。尽管最近的CBMs显示出了巨大的潜力，但它们存在信息泄漏问题，即在概念表示为概率或二进制状态时，超出概念的意图信息泄漏到后续的标签预测中。因此，通过无法区分的概念来错误分类不同的类别，削弱了CBMs的解释和干预能力。本文通过在概念预测中引入标签监督和构建分层概念集来缓解信息泄漏问题。因此，我们提出了一种新的CBMs范例，即SupCBM，它通过预测的概念和精心设计的干预矩阵实现标签预测。SupCBM将重点放在与预测标签最相关的概念上，并且仅在不同的类别之间进行区分。

    Concept Bottleneck Models (CBMs) aim to deliver interpretable and interventionable predictions by bridging features and labels with human-understandable concepts. While recent CBMs show promising potential, they suffer from information leakage, where unintended information beyond the concepts (either when concepts are represented with probabilities or binary states) are leaked to the subsequent label prediction. Consequently, distinct classes are falsely classified via indistinguishable concepts, undermining the interpretation and intervention of CBMs.   This paper alleviates the information leakage issue by introducing label supervision in concept predication and constructing a hierarchical concept set. Accordingly, we propose a new paradigm of CBMs, namely SupCBM, which achieves label predication via predicted concepts and a deliberately-designed intervention matrix. SupCBM focuses on concepts that are mostly relevant to the predicted label and only distinguishes classes when differe
    
[^90]: 一种用于软件定义网络实时异常检测的混合 IndRNNLSTM 方法

    A hybrid IndRNNLSTM approach for real-time anomaly detection in software-defined networks

    [https://arxiv.org/abs/2402.05943](https://arxiv.org/abs/2402.05943)

    本文提出了一种混合 IndRNNLSTM 方法，用于实时检测软件定义网络中的异常。该方法通过结合 IndRNN 和 LSTM 的特点，学习相关和非相关特征，并使用四种特征选择模型提供适当的特征视角。在 NSL-KDD 数据集上实验结果显示，该方法达到了较低的 MAE 和 RMSE 值。

    

    在软件定义网络中使用数据流预测进行异常检测是一项困难的任务。这个问题归类为时序和回归问题。机器学习方法在这个领域中具有挑战性，因为需要手动选择特征。而深度学习方法由于能够自动选择特征具有重要的特点。与此同时，基于 RNN 的方法被广泛使用。LSTM 和 GRU 方法能够很好地学习相关实体；而 IndRNN 方法则能够学习时序中的非相关实体。本文提出的方法尝试使用 IndRNN 和 LSTM 的组合来学习相关和非相关特征。特征选择方法还为模型提供了适当的特征视角；为了实现这一目的，使用了四种特征选择模型：Filter、Wrapper、Embedded 和 Autoencoder。提出的 IndRNNLSTM 算法与 Embedded 的组合能够在 NSL-KDD 数据集上实现 MAE=1.22 和 RMSE=9.92。

    Anomaly detection in SDN using data flow prediction is a difficult task. This problem is included in the category of time series and regression problems. Machine learning approaches are challenging in this field due to the manual selection of features. On the other hand, deep learning approaches have important features due to the automatic selection of features. Meanwhile, RNN-based approaches have been used the most. The LSTM and GRU approaches learn dependent entities well; on the other hand, the IndRNN approach learns non-dependent entities in time series. The proposed approach tried to use a combination of IndRNN and LSTM approaches to learn dependent and non-dependent features. Feature selection approaches also provide a suitable view of features for the models; for this purpose, four feature selection models, Filter, Wrapper, Embedded, and Autoencoder were used. The proposed IndRNNLSTM algorithm, in combination with Embedded, was able to achieve MAE=1.22 and RMSE=9.92 on NSL-KDD 
    
[^91]: 合作知识蒸馏：一种学习者无关的方法

    Cooperative Knowledge Distillation: A Learner Agnostic Approach

    [https://arxiv.org/abs/2402.05942](https://arxiv.org/abs/2402.05942)

    合作知识蒸馏是一种通过多个模型相互合作来传递知识的方法，可以弥补传统知识蒸馏的局限性。不同模型的优劣势可以更有效地传递知识。

    

    知识蒸馏是一种简单而强大的将教师模型的知识传递给学生模型的方法。现有的研究存在以下至少一种关键限制，限制其使用范围和方向：无论该知识是否有用，所有知识都从教师传递给学生；学生是这种交流中唯一学习的一方；典型的蒸馏只从一个教师向一个学生传递知识。我们提出了一种新形式的知识蒸馏，即合作蒸馏，其中许多模型可以同时充当学生和教师的角色。模型之间的合作方式如下：一个模型（学生）识别其性能中的特定缺陷，并搜索另一个模型（教师），通过生成对应事实情况的虚拟实例来编码所学知识。由于不同模型可能具有不同的优势和劣势，因此合作蒸馏的方法可以更有效地传递知识。

    Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, al
    
[^92]: 基于人物的服装生成与通过LLMs进行视觉增强的风格提取

    Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs

    [https://arxiv.org/abs/2402.05941](https://arxiv.org/abs/2402.05941)

    本文提出了一个新的基于人物的服装生成（COG）问题，旨在准确解释人物信息并根据用户的规范生成服装组合。我们提出了一个新颖的框架LVA-COG，利用大型语言模型（LLMs）从用户的兴趣中提取见解，并结合文本到图像模型，增强了对连贯服装的视觉理解和生成。

    

    服装生成问题涉及根据用户的兴趣推荐一个完整的服装。现有方法主要基于锚定商品或指定查询风格来推荐物品，但不考虑用户对电影、社交媒体等中著名人物的兴趣。本文定义了一个新的基于人物的服装生成（COG）问题，旨在准确解释人物信息，并根据用户的规范（如年龄和性别）生成完整的服装组合。为了解决这个问题，我们提出了一个新颖的框架LVA-COG，利用大型语言模型（LLMs）从用户的兴趣（例如人物信息）中提取见解，并采用提示工程技术准确理解用户的喜好。此外，我们还结合了文本到图像模型，增强了对连贯服装的视觉理解和生成（事实或反事实）。我们的框架将LLMs与文本到图像模型整合起来。

    The outfit generation problem involves recommending a complete outfit to a user based on their interests. Existing approaches focus on recommending items based on anchor items or specific query styles but do not consider customer interests in famous characters from movie, social media, etc. In this paper, we define a new Character-based Outfit Generation (COG) problem, designed to accurately interpret character information and generate complete outfit sets according to customer specifications such as age and gender. To tackle this problem, we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences. Additionally, we incorporate text-to-image models to enhance the visual understanding and generation (factual or counterfactual) of cohesive outfits. Our framework integrates LLMs with text-to-image models 
    
[^93]: 井下煤矿工作时间损失的风险因素因果关系网络

    Causal Relationship Network of Risk Factors Impacting Workday Loss in Underground Coal Mines

    [https://arxiv.org/abs/2402.05940](https://arxiv.org/abs/2402.05940)

    本研究使用一种新颖的因果人工智能方法，通过分析井下煤矿的伤害记录数据，建立了井下煤矿工作时间损失的因果关系网络。发现关键的因果关系包括风源和工作状态等不同因素之间的作用。

    

    本研究旨在利用一种新颖的因果人工智能（AI）方法建立井下煤矿工作时间损失的各种因素之间的因果关系网络。分析利用了从国家职业安全与健康研究所（NIOSH）获得的数据。从NIOSH数据库中提取了来自1990年至2020年的共计101,010份伤害记录，涵盖了3,982个独立的井下煤矿。利用一种名为群组贪婪等价搜索（GGES）的新颖因果AI方法进行了因果关系的分析和可视化。通过干预计算调整（IDA）得分对每个变量对工作时间损失的影响进行了评估。使用10折交叉验证技术进行模型训练和验证。利用接邻点精确度（AP）、接邻点召回率（AR）、箭头头部精确度（AHP）和箭头头部召回率（AHR）等性能指标对模型进行评估。研究发现，在2006年之后，关键的因果关系包括风源和工作状态等不同因素之间的作用有所     changed

    This study aims to establish the causal relationship network between various factors leading to workday loss in underground coal mines using a novel causal artificial intelligence (AI) method. The analysis utilizes data obtained from the National Institute for Occupational Safety and Health (NIOSH). A total of 101,010 injury records from 3,982 unique underground coal mines spanning the years from 1990 to 2020 were extracted from the NIOSH database. Causal relationships were analyzed and visualized using a novel causal AI method called Grouped Greedy Equivalence Search (GGES). The impact of each variable on workday loss was assessed through intervention do-calculus adjustment (IDA) scores. Model training and validation were performed using the 10-fold cross-validation technique. Performance metrics, including adjacency precision (AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall (AHR), were utilized to evaluate the models. Findings revealed that after 2006, key
    
[^94]: 大规模语言模型在知识蒸馏中遇见图神经网络

    Large Language Model Meets Graph Neural Network in Knowledge Distillation

    [https://arxiv.org/abs/2402.05894](https://arxiv.org/abs/2402.05894)

    本论文提出了一种新颖的图知识蒸馏框架，使用大规模语言模型作为教师模型、图神经网络作为学生模型，解决了在理解文本-属性图中的节点分类问题中的限制。

    

    尽管近期学术界对于大规模语言模型（LLMs）在理解文本-属性图（TAG）方面的进展和潜力有所披露，但LLMs在实际应用中的部署受到了计算和存储需求高，推理过程中延迟长的限制。同时，传统的图神经网络（GNNs）虽然轻量且擅长学习图的结构特征，但对于真实应用中TAG复杂语义的把握有所限制。为了解决这些限制，我们聚焦于TAG中节点分类的下游任务，提出了一种新颖的图知识蒸馏框架，称为语言图知识蒸馏（LinguGKD），使用LLMs作为教师模型，GNNs作为学生模型进行知识蒸馏。其中包括对LLM进行TAG定向指导调整以应对设计的节点分类提示，然后对层次化学习的节点特征进行对齐。

    Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
    
[^95]: 人工智能作为游戏玩家：促进公平性

    Prompting Fairness: Artificial Intelligence as Game Players

    [https://arxiv.org/abs/2402.05786](https://arxiv.org/abs/2402.05786)

    人工智能作为游戏玩家能够展现出强烈的公平意识，取决于其对游戏伙伴的信任程度、游戏框架对于给予接受者的影响以及其可能存在厌恶不平等的情感。

    

    几十年来，社会科学家们一直在研究测量公平性的功利主义游戏，比如独裁者游戏。这些游戏不仅让我们了解了人类对公平性的看法，还揭示了公平性、利他主义和贪婪在何种条件下增加或减少。尽管这些游戏传统上关注人类，但人工智能的崛起使我们能够研究这些模型如何玩这些游戏。人工智能正在成为人际互动中的常态，研究这些模型在游戏中显示的公平性可以让我们对人工智能的决策过程有所了解。通过101轮的独裁者游戏，我得出结论：人工智能具有强烈的公平意识，这与其认为它的游戏伙伴是否值得信任有关；设置对游戏的框架在人工智能给予接受者的数量上有很大影响，而且有可能存在证据表明人工智能也像人类一样具有厌恶不平等的情感。

    Utilitarian games such as dictator games to measure fairness have been studied in the social sciences for decades. These games have given us insight into not only how humans view fairness but also in what conditions the frequency of fairness, altruism and greed increase or decrease. While these games have traditionally been focused on humans, the rise of AI gives us the ability to study how these models play these games. AI is becoming a constant in human interaction and examining how these models portray fairness in game play can give us some insight into how AI makes decisions. Over 101 rounds of the dictator game, I conclude that AI has a strong sense of fairness that is dependant of it it deems the person it is playing with as trustworthy, framing has a strong effect on how much AI gives a recipient when designated the trustee, and there may be evidence that AI experiences inequality aversion just as humans.
    
[^96]: 《岩石编码，不是开发-一个以人为中心的LLM在软件工程任务中的实验评估》

    Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks

    [https://arxiv.org/abs/2402.05650](https://arxiv.org/abs/2402.05650)

    这项研究提出了一种自监督学习框架，用于训练神经网络从未标记的多感官数据中学习丰富而有意义的3D场景表示。通过利用不同感觉模态之间的时间一致性和几何对齐，我们的框架能够学习到强大而准确的表示。我们将我们的方法应用于各种3D感知任务，并与监督基线进行了比较，展示了竞争性的性能。此外，我们还展示了我们学到的表示在不同的传感器设置下具有很好的泛化能力，进一步突显了我们的自监督学习方法的有效性和多功能性。

    

    最近，基于大型语言模型（LLM）的生成型AI因其在多个领域中令人印象深刻的高质量表现而备受关注，特别是在ChatGPT发布之后。许多人认为它们有潜力在软件开发中执行通用问题解决，并取代人类软件开发人员。然而，目前没有对这些LLM技术在完成软件开发任务方面的能力进行深入调查。在一项有109名参与者的受控 2x2 受试者间实验中，我们研究了与ChatGPT合作在编码任务和典型软件开发任务中的效用程度以及人们如何使用ChatGPT。我们发现，尽管ChatGPT在解决简单的编码问题方面表现出色，但它在支持典型的软件开发任务方面的表现并不理想。我们还观察了参与者与ChatGPT之间的互动，并找到了相互关系。

    Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT. Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks. In a controlled 2 $\times$ 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT. We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and ChatGPT and found the relations between the i
    
[^97]: 知识图谱与多模态学习：综述

    Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2402.05391](https://arxiv.org/abs/2402.05391)

    知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。

    

    知识图谱在推动各种人工智能应用方面起着关键作用，语义网络社区对多模态维度的探索为创新打开了新的途径。在本综述中，我们仔细审查了300多篇文章，重点关注了两个主要方面的知识图谱感知研究：以知识图谱支持多模态任务的KG驱动多模态（KG4MM）学习，将知识图谱研究扩展到多模态知识图谱（MM4KG）领域。我们从定义知识图谱和多模态知识图谱开始，然后探索它们的构建进展。我们的综述包括两个主要任务类别：KG感知的多模态学习任务，如图像分类和视觉问答，以及内在的多模态知识图谱任务，如多模态知识图谱补全和实体对齐，突出了具体的研究轨迹。对于这些任务中的大部分，我们提供了定义、评估基准，并进一步指出进行相关研究的重要见解。最后，我们讨论了cu

    Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
    
[^98]: LB-KBQA: 基于大语言模型和BERT的基于知识的问答系统

    LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System

    [https://arxiv.org/abs/2402.05130](https://arxiv.org/abs/2402.05130)

    LB-KBQA是一种基于大语言模型和BERT的基于知识的问答系统，通过生成式人工智能的帮助，能够提高意图识别的性能和解决语言多样性的问题。

    

    生成式人工智能（AI）因其新兴的能力而赋予了各个领域的力量，其中一个典型的应用领域是大语言模型（LLMs）。与传统的基于AI的方法相比，生成式AI的典型应用领域之一是大语言模型（LLMs），并且在自然语言理解能力方面，LLM的能力得到了显著提高。自然语言理解能力一直以来都是基于知识的问答系统意图识别性能的障碍，这源自语言多样性和新出现的意图。传统的基于AI的意图识别方法可以分为基于语义解析的方法和基于模型的方法。然而，这两种方法都在意图识别方面受到有限的资源限制。为了解决这个问题，我们提出了一种基于大语言模型（LLM）和BERT的新型KBQA系统（LB-KBQA）。在生成式AI的帮助下，我们提出的方法可以检测到……（略）

    Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect 
    
[^99]: 在具有循环消息传递的图中实现多Agent强化学习的泛化能力

    Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing

    [https://arxiv.org/abs/2402.05027](https://arxiv.org/abs/2402.05027)

    本论文提出了一种循环消息传递模型，它可以在图中实现多Agent强化学习的泛化能力。这种模型通过在整个图中进行信息流实现了观察邻域大小的平衡，从而提高了Agent的反应性、选择动作的质量和通信效率。

    

    基于图的环境给多Agent强化学习带来了独特的挑战。在分散式方法中，Agent在给定的图中操作，并根据部分或过时的观察做出决策。观察到的邻域的大小限制了在不同图上的泛化能力，并影响到Agent的反应性、选择的动作质量和通信开销。本研究侧重于泛化能力，并通过在整个图中进行连续的信息流解决了观察到的邻域大小的权衡。我们提出了一种循环消息传递模型，它与环境的步骤迭代，并允许节点通过与其邻居交换消息来创建图的全局表示。根据Agent在图中的位置，Agent接收到基于学习到的图观察结果。我们的方法可以在运行时以分散的方式使用，并与选择的强化学习算法结合使用。我们评估了我们的方法...

    Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our meth
    
[^100]: PaDeLLM-NER：大型语言模型中的并行解码用于命名实体识别

    PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition

    [https://arxiv.org/abs/2402.04838](https://arxiv.org/abs/2402.04838)

    本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。

    

    本研究旨在使用大型语言模型（LLMs）减少命名实体识别（NER）的生成延迟。LLMs的高延迟的主要原因是顺序解码过程，该过程自回归地生成NER的所有标签和提及，显著增加了序列长度。为此，我们引入了PaDeLLM-NER（Parallel Decoding in LLM for NE），这是一种无需额外模块或架构修改即可无缝集成到现有生成模型框架中的方法。PaDeLLM-NER允许同时解码所有提及，从而减少生成延迟。实验结果显示，PaDeLLM-NER的推理速度显著提高，对英语和中文来说比自回归方法快1.76到10.22倍。与各种数据集上的最先进性能相媲美，同时维持了预测质量。

    In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
    
[^101]: 光学钢丝绳非破坏性损伤检测的新方法

    A new method for optical steel rope non-destructive damage detection

    [https://arxiv.org/abs/2402.03843](https://arxiv.org/abs/2402.03843)

    本文提出了一种新的算法用于在高海拔环境中对钢丝绳进行非破坏性损伤检测，其中包括一种准确提取钢丝绳的分割模型和一种区分正常和异常钢丝绳的检测模型，实验证明其性能显著高于基准模型。

    

    本文提出了一种针对高海拔环境（空中吊索道）中的钢丝绳非破坏性损伤检测的新算法。该算法包括两个关键组件：首先，设计了一种名为RGBD-UNet的分割模型，可以准确地从复杂背景中提取钢丝绳。该模型通过提出的CMA模块可以处理和结合颜色和深度信息。其次，开发了一种名为VovNetV3.5的检测模型，用于区分正常和异常的钢丝绳。它将VovNet架构与DBB模块结合起来以提高性能。此外，还提出了一种新颖的背景增强方法，以增强分割模型的泛化能力。创建了包含不同场景中钢丝绳图像的数据集，用于分割和检测模型的训练和测试。实验证明，在基准模型上取得了显著的改进。在提出的数据集上，基于此算法的传感器识别性能（h）明显提高。

    This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the h
    
[^102]: MolTC: 在语言模型中进行分子关系建模

    MolTC: Towards Molecular Relational Modeling In Language Models

    [https://arxiv.org/abs/2402.03781](https://arxiv.org/abs/2402.03781)

    本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。

    

    分子关系学习（MRL）旨在理解分子之间的相互作用，在推进生物化学研究方面起到了关键作用。最近，大型语言模型（LLMs）的采用已成为一种有效和高效的MRL方法，这些模型以其庞大的知识存储库和先进的逻辑推理能力而闻名。尽管具有潜力，但这些方法主要依赖于文本数据，因此没有充分利用分子图中固有的丰富结构信息。此外，缺乏统一的框架加剧了信息的浪费，因为它阻碍了在不同数据集之间共享学习到的相互作用理由。为了解决这些挑战，本研究提出了一种基于LLM的多模态框架，用于根据思维链（CoT）理论对分子相互作用进行预测，称为MolTC，它可以高效地整合分子对的丰富图形信息。

    Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
    
[^103]: HEANA: 一种具有灵活数据流的混合时幅模拟光学加速器，用于能量高效的CNN推理

    HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference

    [https://arxiv.org/abs/2402.03247](https://arxiv.org/abs/2402.03247)

    HEANA是一种混合时幅模拟光学加速器，通过使用混合时幅模拟光学乘法器(TAOMs)提高了对多种数据流的灵活性。它解决了现有光学加速器的波长并行性受到串扰、不支持多种数据流以及未充分利用光电探测器进行原位累积等问题。

    

    提出了一种名为HEANA的新型混合时幅模拟光学加速器，用于加速整数量化CNN的推理。HEANA采用混合时幅模拟光学乘法器(TAOMs)，增强了HEANA对多种数据流的支持灵活性。通过谱无损的TAOMs排列，有效解决了现有光学加速器存在的波长并行性受到各种串扰影响、无法支持多种数据流以及未充分利用光电探测器进行原位累积等问题。

    Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs s
    
[^104]: 2024年大规模语言模型的真实性

    Factuality of Large Language Models in the Year 2024

    [https://arxiv.org/abs/2402.02420](https://arxiv.org/abs/2402.02420)

    本文调查了大规模语言模型（LLM）的真实性问题，并对其现有研究进行了批判性分析，指出了改进LLM真实性的挑战和解决方案，以及自动真实性评估的障碍。未来的研究应该关注在这些方面的进一步工作。

    

    大规模语言模型（LLMs），尤其是在聊天方面进行指导调整后，已经成为我们日常生活的一部分，通过在一个地方直接回答各种问题，使人们从搜索、提取和整合多个信息源的过程中得到解脱。然而，很多情况下，LLM的回答是错误的，这限制了它们在现实场景中的适用性。因此，对于评估和提高LLM真实性的研究近年来引起了很多关注。在这项调查中，我们对现有的研究进行了批判性分析，旨在找出主要挑战及其原因，并指出改进LLM真实性的潜在解决方案，以及分析开放文本生成的自动真实性评估面临的障碍。我们还展望了未来研究的方向。

    Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
    
[^105]: 一种多角度的机器学习方法用于评估洛杉矶警察与司机的互动

    A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles

    [https://arxiv.org/abs/2402.01703](https://arxiv.org/abs/2402.01703)

    该研究提出了一种多角度的机器学习方法，用于分析洛杉矶警察与司机的互动。该方法利用多模态的数据包括音频、视频和文字信息，旨在提供对复杂和有争议的警民互动的分析工具。

    

    政府官员与市民之间的互动影响公共福祉和民主社会的正当性。警察是国家最显而易见、最接触市民的代理人，在交通站停期间，他们每年与公众互动超过2000万次。如今，这些互动经常被戴在身上的摄像机记录下来，这被视为提高警察问责制和改善警民互动的手段。然而，由于缺乏可靠的自动化工具来分析这些复杂而有争议的警民互动，这些记录的及时分析受到了阻碍。本文提出了一种新的多角度、多模态机器学习（ML）工具的方法，用于分析来自这些身上摄像机记录的音频、视频和文字信息。我们的方法首先确定与不同利益相关者最相关的沟通方面，包括共同感知互动的标志标记以及具有这些标记的符号。

    Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
    
[^106]: 通过深度黑石贝莱曼模型优化时间序列供应商分配风险

    Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model

    [https://arxiv.org/abs/2401.17350](https://arxiv.org/abs/2401.17350)

    通过深度黑石贝莱曼模型和时空图神经网络，我们优化了供应商选择和订单分配，同时解决了零阶情况下的可信度问题，实现了准确的预测和精确的置信区间。

    

    我们介绍了BL模型和Perspective矩阵，以优化供应商选择和订单分配，重点关注时间和空间动态。我们使用时空图神经网络开发了供应商关系网络，增强了对复杂供应商相互依赖关系的理解。此外，我们还通过Masked Ranking机制解决了零阶情况下的可信度问题，提高了供应商排序效率。与传统模型相比，我们的模型在两个数据集上展现了优越的结果。我们使用真实数据集进行的评估突出了DBLM在提供准确预测和精确置信区间方面的优势，特别是在高分辨率情景下。

    We introduce the BL model and the Perspective Matrix to optimize supplier selection and order allocation, focusing on both temporal and spatial dynamics. Our development of a Supplier Relationship Network, using a Spatio-Temporal Graph Neural Network, enhances the understanding of complex supplier interdependencies. Additionally, we address credibility issues in zero-order scenarios with a Masked Ranking Mechanism, improving supplier ranking efficiency. Our model demonstrates superior results on two datasets compared to the traditional models. Our evaluations using real-world datasets highlight DBLM's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
    
[^107]: 为什么使用大型语言模型解决多智能体路径规划尚未成功

    Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet

    [https://arxiv.org/abs/2401.03630](https://arxiv.org/abs/2401.03630)

    本文研究了使用大型语言模型解决多智能体路径规划的问题。通过实验，我们发现直接使用大型语言模型解决复杂场景下的路径规划仍然存在困难。

    

    随着ChatGPT和GPT-4等大型语言模型（LLM）的成功引发的爆炸性影响，最近的许多研究表明基础模型可以用于解决各种任务。然而，在多智能体规划方面，很少有研究分享见解。多智能体规划不同于其他领域，它将多智能体协调和规划的困难结合起来，使得难以利用外部工具促进所需的推理。本文重点研究多智能体路径规划（MAPF）问题，也被称为多机器人路径规划，并研究了使用LLM解决MAPF的性能。我们首先展示了在没有障碍物的空房间地图上的激励性成功，然后展示了对标准MAPF基准测试中较难的房间地图和迷宫地图的规划失败。我们阐述了直接使用LLM解决MAPF尚未成功的立场，并通过各种实验来支撑我们的观点。

    With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to supp
    
[^108]: LLMLight: 大型语言模型作为交通信号控制代理

    LLMLight: Large Language Models as Traffic Signal Control Agents

    [https://arxiv.org/abs/2312.16044](https://arxiv.org/abs/2312.16044)

    LLMLight是一个采用大型语言模型作为交通信号控制代理的新框架，通过借助先进的泛化能力和类似人类直觉的推理和决策过程，实现了有效的交通控制。此外，通过构建专为TSC任务定制的骨干语言模型LightGPT，进一步提升了LLMLight的效果和性能。

    

    交通信号控制（TSC）是城市交通管理的关键组成部分，旨在优化道路网络效率和减少拥堵。传统的TSC方法主要基于交通工程和强化学习（RL），往往在各种交通场景中存在泛化性不足和缺乏解释性等限制。本文提出了LLMLight，这是一个采用大型语言模型（LLMs）作为TSC决策代理的新框架。具体而言，该框架通过向LLM提供详细的实时交通状况说明作为指导，借助LLM的先进泛化能力，LLMLight实现了类似人类直觉的推理和决策过程，从而实现有效的交通控制。此外，我们构建了LightGPT，这是一个专为TSC任务量身定制的骨干LLM。通过学习细微的交通模式和控制策略，LightGPT在经济成本方面提升了LLMLight框架的效果。进行了大量的实验验证了LLMLight的有效性和性能优势。

    Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional methods in TSC, primarily based on transportation engineering and reinforcement learning (RL), often exhibit limitations in generalization across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments 
    
[^109]: 知识蒸馏用于科学教育评估的LLM的自动评分

    Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments

    [https://arxiv.org/abs/2312.15842](https://arxiv.org/abs/2312.15842)

    本研究提出了一种将LLM的知识蒸馏为更小、更高效、更准确的神经网络的方法，在资源受限设备上部署具有挑战性。通过使用LLM的预测概率作为软标签训练较小的学生模型，并使用专门定制的损失函数，保证学生模型与教师模型的性能非常相似。实验证明此方法在科学教育评估中具有良好的准确性。

    

    本研究提出了一种方法，用于将精调的大型语言模型（LLMs）的知识蒸馏为更小、更高效、更准确的神经网络。我们特别针对在资源受限设备上部署这些模型的挑战。我们的方法包括使用LLM的预测概率（作为软标签）来训练较小的学生模型（神经网络），LLM充当教师模型。这通过一个专门为了从LLM的输出概率中学习而定制的损失函数实现，以确保学生模型与教师的性能非常相似。为了验证知识蒸馏方法的性能，我们使用了一个包含6,684个学生对科学问题的写作回答和三个人工专家评分的数学推理数据集的大型数据集7T。我们将准确性与最先进的知识蒸馏模型TinyBERT和人工神经网络（ANN）模型进行了比较。结果表明

    This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown 
    
[^110]: 扩展就是一切：使用JAX加速强化学习的自动驾驶

    Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning

    [https://arxiv.org/abs/2312.15122](https://arxiv.org/abs/2312.15122)

    本研究提出了一种扩展的自动驾驶强化学习方法，在大规模实验中展示了随着规模增加，策略性能的改善。与现有机器学习自动驾驶策略相比，我们的最佳策略将故障率降低了64％，同时提高了25％的驾驶进展速度。

    

    强化学习已经在复杂领域如视频游戏中展现出超越最优人类的能力。然而，为自动驾驶运行必要规模的强化学习实验非常困难。构建一个大规模的强化学习系统并在多个GPU上进行分布是具有挑战性的。在训练过程中在真实世界车辆上收集经验从安全和可扩展性的角度来看是不可行的。因此，需要一个高效且真实的驾驶模拟器，使用大量来自真实驾驶的数据。我们将这些能力集合在一起，并进行大规模的强化学习实验用于自动驾驶。我们证明，随着规模的增加，我们的策略表现得到了提升。我们最佳策略将故障率降低了64％，同时比现有机器学习自动驾驶策略提高了25％的驾驶进展速度。

    Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
    
[^111]: 使用Siamese网络的基于深度学习的人脸识别方法

    Deep Learning Based Face Recognition Method using Siamese Network

    [https://arxiv.org/abs/2312.14001](https://arxiv.org/abs/2312.14001)

    这项研究提出了一种使用Siamese网络进行无监督人脸识别的方法，通过利用负样本和最近邻对，消除对标记的人脸图像数据的需求。

    

    在面部验证系统中取得最先进的成果通常依赖于标记的训练数据的可用性，然而这往往很难获得大量。在这项研究中，我们提出了使用Siamese网络进行人脸识别的方法，消除了对标记的人脸图像的需求。我们通过有策略地利用负样本和最近邻对来建立正样本和负样本对，从而通过无监督的方法来生成训练对。该网络采用了一个VGG编码器，作为双分支Siamese网络进行训练。我们的主要目标是避免对标记的人脸图像数据的需求，因此提出了在完全无监督的方式下生成训练对的方法。正训练数据是根据其与指定锚点之间的最高余弦相似度得分在数据集中选择的，而负训练数据则以类似的方式确定，但从一个另外的数据集中选择。

    Achieving state-of-the-art results in face verification systems typically hinges on the availability of labeled face training data, a resource that often proves challenging to acquire in substantial quantities. In this research endeavor, we proposed employing Siamese networks for face recognition, eliminating the need for labeled face images. We achieve this by strategically leveraging negative samples alongside nearest neighbor counterparts, thereby establishing positive and negative pairs through an unsupervised methodology. The architectural framework adopts a VGG encoder, trained as a double branch siamese network. Our primary aim is to circumvent the necessity for labeled face image data, thus proposing the generation of training pairs in an entirely unsupervised manner. Positive training data are selected within a dataset based on their highest cosine similarity scores with a designated anchor, while negative training data are culled in a parallel fashion, though drawn from an al
    
[^112]: LayerCollapse: 自适应压缩神经网络

    LayerCollapse: Adaptive compression of neural networks

    [https://arxiv.org/abs/2311.17943](https://arxiv.org/abs/2311.17943)

    LayerCollapse是一种自适应压缩神经网络的方法，通过结构化剪枝来减少全连接层的深度，而不需要进行微调，并且对性能影响有限。该方法通过正则化激活函数的线性度来控制模型的表达能力。

    

    处理当代深度学习和基于transformer的模型不断增长的规模是一个重大挑战。超参数化的Transformer网络在自然语言处理和计算机视觉方面的业绩超过了先前的技术。这些模型含有数亿个参数，需要大量的计算资源，并容易过拟合。在这项工作中，我们提出了LayerCollapse，一种结构化剪枝的形式，用于减少全连接层的深度。我们开发了一种新的正则化项，允许在不进行微调的情况下进行训练后压缩，并对性能产生有限的影响。LayerCollapse通过对全连接层之间的激活进行正则化，调节激活函数的线性度来控制模型的表达能力。线性激活函数将线性转换的秩降低到相应线性转换的秩。我们通过展示LayerCollapse的压缩能力来证明其有效性。

    Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil
    
[^113]: 深度神经网络加速器中的故障定位监测器部署

    Monitor Placement for Fault Localization in Deep Neural Network Accelerators

    [https://arxiv.org/abs/2311.16594](https://arxiv.org/abs/2311.16594)

    本研究提出了一种在并行阵列中优化硬件监测器部署的解决方案，以提高深度神经网络加速器的可靠性。通过证明和推导，我们确定了定位单个故障的PE所需的监测器数量，并解决了NP困难的监测器部署方案优化问题。然后，我们提出了一种启发式方法来平衡效益与开销。

    

    并行性和高效数据重用使得并行阵列系统成为深度神经网络（DNN）加速器的一个重要选择。提高DNN加速器的可靠性至关重要，因为硬件故障可能会降低DNN推理的准确性。由于并行阵列利用大量处理元件（PE）进行并行处理，但当一个PE故障时，错误会传播并影响下游PE的输出。由于PE的数量较大，使用基于硬件的运行时监测的成本是不可行的。我们提出了一种优化并行阵列中硬件监测器部署的解决方案。我们首先证明了在定位单个故障的PE所需的监测器数量为$2N-1$，并导出了监测器部署方案。我们还展示了第二个优化问题的监测器部署方案，该方案通过给定数量的监测器最小化候选故障PE集合，该问题是NP困难的。因此，我们提出一种启发式方法来平衡实现硬件监测的效益与开销。

    Systolic arrays are a prominent choice for deep neural network (DNN) accelerators because they offer parallelism and efficient data reuse. Improving the reliability of DNN accelerators is crucial as hardware faults can degrade the accuracy of DNN inferencing. Systolic arrays make use of a large number of processing elements (PEs) for parallel processing, but when one PE is faulty, the error propagates and affects the outcomes of downstream PEs. Due to the large number of PEs, the cost associated with implementing hardware-based runtime monitoring of every single PE is infeasible. We present a solution to optimize the placement of hardware monitors within systolic arrays. We first prove that $2N-1$ monitors are needed to localize a single faulty PE and we also derive the monitor placement. We show that a second placement optimization problem, which minimizes the set of candidate faulty PEs for a given number of monitors, is NP-hard. Therefore, we propose a heuristic approach to balance 
    
[^114]: 程序机器策略：通过集成程序合成和状态机解决长期任务

    Program Machine Policy: Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines

    [https://arxiv.org/abs/2311.15960](https://arxiv.org/abs/2311.15960)

    这项工作提出了程序机器策略（POMP），在集成程序合成和状态机的基础上，解决了长期任务并表示复杂行为。

    

    深度强化学习在各个领域表现出色，但缺乏泛化能力和解释性。另一方面，编程式强化学习方法重新定义了强化学习任务，将其视为合成可解释的程序，可以在环境中执行。尽管取得了鼓舞人心的结果，但这些方法局限于短期任务。另一方面，使用状态机表示强化学习策略可以归纳推广到长期任务；然而，它在获取多样和复杂行为方面存在困难。本研究提出了程序机器策略（POMP），以桥接编程式强化学习和状态机策略的优势，允许表示复杂行为并解决长期任务。具体而言，我们介绍了一种方法，可以检索一组有效、多样且兼容的程序。然后，我们将这些程序用作状态机的模式，并学习一个转移函数。

    Deep reinforcement learning (deep RL) excels in various domains but lacks generalizability and interpretability. On the other hand, programmatic RL methods (Trivedi et al., 2021; Liu et al., 2023) reformulate RL tasks as synthesizing interpretable programs that can be executed in the environments. Despite encouraging results, these methods are limited to short-horizon tasks. On the other hand, representing RL policies using state machines (Inala et al., 2020) can inductively generalize to long-horizon tasks; however, it struggles to scale up to acquire diverse and complex behaviors. This work proposes the Program Machine Policy (POMP), which bridges the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, and compatible programs. Then, we use these programs as modes of a state machine and learn a transition func
    
[^115]: MAIRA-1：一种专门用于放射学报告生成的大型多模态模型

    MAIRA-1: A specialised large multimodal model for radiology report generation

    [https://arxiv.org/abs/2311.13668](https://arxiv.org/abs/2311.13668)

    MAIRA-1是一种专门用于放射学报告生成的大型多模态模型，在与预训练的视觉编码器对齐和文本数据增强的基础上，利用了CXR特定的图像编码器和经过微调的大型语言模型，生成具有最先进质量的报告。

    

    我们提出了一种放射学特定的多模态模型，用于从胸部X光（CXR）生成放射学报告的任务。我们的工作基于一个思想，即可以通过与预训练视觉编码器对齐，使大型语言模型具备多模态能力。在自然图像上，这已被证明可以使多模态模型获得图像理解和描述能力。我们提出的模型（MAIRA-1）利用了一个CXR特定的图像编码器，结合基于Vicuna-7B的微调的大型语言模型，并进行基于文本的数据增强，以产生具有最先进质量的报告。特别地，MAIRA-1在与放射科医生对齐的RadCliQ度量和考虑的所有词汇度量上都有显著改进。对模型输出的手动审核显示出了产生报告的流畅性和准确性，同时揭示了现有评估方法所未捕捉到的失败模式。更多信息和资源可在项目网站上找到：

    We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website:
    
[^116]: DroneOptiNet: 一种用于5G及其后太阳能小型蜂窝网络的最佳无人机负载重分配机制的框架

    DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks

    [https://arxiv.org/abs/2311.12944](https://arxiv.org/abs/2311.12944)

    本研究提出了一种用于5G及其后太阳能小型蜂窝网络的最佳无人机负载重分配机制，通过使用无人机上的空中基站进行可靠安全的电力再分配，提高了网络的可靠性和稳健性。

    

    第五代及其后的蜂窝网络对功率需求提出了重要的限制，需要能够高效利用能源的解决方案。在本研究中，我们提出了一种新颖的使用无人机上的空中基站（BS）进行可靠安全的电力再分配的用户负载转移方法，以跨越由绿色小型蜂窝BS组成的微网网络。根据用户密度和空中基站的可用性，通过将空中基站从高能耗小区迁移到低能耗小区，来满足能量不足的小区的能量需求。所提出的混合无人机框架将长短期记忆与独特的成本函数结合，使用进化神经网络来有效地管理无人机和基站的能量和负载重分配。所提出的算法减少了基站停电，并保持了一致的吞吐量稳定性，从而展示了其提升无线网络可靠性和稳健性的能力。

    The power requirements posed by the fifth-generation and beyond cellular networks are an important constraint in network deployment and require energy-efficient solutions. In this work, we propose a novel user load transfer approach using airborne base stations (BS) mounted on drones for reliable and secure power redistribution across the micro-grid network comprising green small cell BSs. Depending on the user density and the availability of an aerial BS, the energy requirement of a cell with an energy deficit is accommodated by migrating the aerial BS from a high-energy to a low-energy cell. The proposed hybrid drone-based framework integrates long short-term memory with unique cost functions using an evolutionary neural network for drones and BSs and efficiently manages energy and load redistribution. The proposed algorithm reduces power outages at BSs and maintains consistent throughput stability, thereby demonstrating its capability to boost the reliability and robustness of wirel
    
[^117]: AutoPlanBench: 从PDDL自动生成LLM规划器的基准测试

    AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL

    [https://arxiv.org/abs/2311.09830](https://arxiv.org/abs/2311.09830)

    AutoPlanBench是一种新方法，可以自动转换PDDL规划基准测试为文本描述，并提供了相应的基准测试数据集。研究表明，当前最好的LLM规划器在某些规划任务上表现优秀，但对于其他任务来说仍存在挑战。

    

    LLMs（逻辑-概率模型）在规划任务中的应用越来越广泛，但是它们在规划和推理方面的能力尚不明确。我们提出了AutoPlanBench，一种将PDDL中的规划基准测试自动转换为文本描述的新方法，并提供了使用我们方法创建的基准测试数据集。我们展示了最好的LLM规划器在某些规划任务上表现良好，但其他任务仍然超出了当前方法的能力范围。

    LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present AutoPlanBench, a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on some planning tasks, others remain out of reach of current methods.
    
[^118]: 大型语言模型中的结构化化学推理

    Structured Chemistry Reasoning with Large Language Models

    [https://arxiv.org/abs/2311.09656](https://arxiv.org/abs/2311.09656)

    该论文研究了大型语言模型在化学领域的复杂科学推理困难，发现错误通常源于缺乏有效的推理结构。基于此，引入了一种简单而有效的提示策略StructChem，大幅提升了语言模型的性能。

    

    大型语言模型（LLMs）在各个领域表现出色，但在化学领域的复杂科学推理方面存在困难。与以往研究中涉及的简单化学任务（例如分子分类）不同，复杂的化学问题不仅需要广博的知识和精确的计算，还需要关于不同概念（例如温度变化）的丰富动态相互作用的组合推理。我们的研究表明，即使是像GPT-4这样先进的LLMs也很容易出现错误。有趣的是，这些错误通常不是由于LLMs缺乏领域知识，而是由于缺乏有效的推理结构来引导LLMs引发正确的知识，将知识融入逐步推理中，并迭代改进结果以进一步提高质量。基于此，我们引入了一种简单而有效的提示策略——结构化化学（StructChem），它提供了所需的指导，并显著提升了LLMs的性能。

    Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs
    
[^119]: VT-Former: 基于Transformer的智能公路交通系统中的车辆轨迹预测方法

    VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems

    [https://arxiv.org/abs/2311.06623](https://arxiv.org/abs/2311.06623)

    本文介绍了一种基于Transformer的车辆轨迹预测方法，名为VT-Former，在智能公路交通系统中具有重要的应用价值。

    

    加强道路安全和交通管理已成为现代网络物理系统和智能交通系统的重点领域。车辆轨迹预测在公路和道路安全的众多应用中起着关键作用。这些应用包括交通管理、事故预防、工地安全和能源优化等各种用例。在人工智能领域的发展以及监控摄像头在道路网络上的增加部署推动下，智能管理在这一背景下得到了很大进展。本文介绍了一种新颖的基于Transformer的车辆轨迹预测方法，称为VT-Former。除了利用Transformer捕捉长期时间模式外，还提出了一种新的图注意力分词（GAT）模块。

    Enhancing roadway safety and traffic management has become an essential focus area for a broad range of modern cyber-physical systems and intelligent transportation systems. Vehicle Trajectory Prediction is a pivotal element within numerous applications for highway and road safety. These applications encompass a wide range of use cases, spanning from traffic management and accident prevention to enhancing work-zone safety and optimizing energy conservation. The ability to implement intelligent management in this context has been greatly advanced by the developments in the field of Artificial Intelligence (AI), alongside the increasing deployment of surveillance cameras across road networks. In this paper, we introduce a novel transformer-based approach for vehicle trajectory prediction for highway safety and surveillance, denoted as VT-Former. In addition to utilizing transformers to capture long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module has been proposed
    
[^120]: 本地通用解释器（LUX）-- 一种基于规则的解释器，具有事实、反事实和视觉解释

    Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations

    [https://arxiv.org/abs/2310.14894](https://arxiv.org/abs/2310.14894)

    LUX是一种基于规则的解释器，可以生成事实、反事实和视觉解释，通过选择高密度簇形式的局部概念来形成决策边界。

    

    可解释的人工智能（XAI）是近年来最被广泛发展的人工智能领域之一。它也是最分散的领域之一，有多种方法专注于解释的不同方面。这使得一次性以紧凑和一致的方式获得完整的解释变得困难。为了解决这个问题，我们提出了本地通用解释器（LUX），它是一种基于规则的解释器，可以生成事实、反事实和视觉解释。它基于修改后的决策树算法，允许斜交和集成特征重要性XAI方法，如SHAP或LIME。与其他算法相反，它不使用数据生成，而是专注于选择以高密度簇形式出现的真实数据的局部概念，这些局部概念对解释模型的决策边界形成最大的影响。我们在真实和合成数据集上测试了我们的方法，并将其与最先进的基于规则的方法进行了比较。

    Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based
    
[^121]: 随机种群更新在多目标进化算法中可以被证明是有帮助的

    Stochastic Population Update Can Provably Be Helpful in Multi-Objective Evolutionary Algorithms

    [https://arxiv.org/abs/2306.02611](https://arxiv.org/abs/2306.02611)

    本研究通过理论分析证明，在多目标进化算法中采用随机种群更新机制可以显著降低算法的运行时间，从而提高问题的求解效率。

    

    进化算法（EAs）因其基于种群的搜索特性，已被广泛且成功地应用于解决多目标优化问题。种群更新是多目标进化算法（MOEAs）中的关键组成部分，通常以贪婪、确定性的方式进行。也就是说，下一代种群是通过从当前种群和新生成的解中选择最优解形成的（无论使用的选择标准是Pareto支配、拥挤度还是指标等）。本文对这种做法提出了质疑。我们从理论上证明了随机种群更新对于MOEAs的搜索是有益的。具体地，我们证明了将确定性种群更新机制替换为随机机制，可以指数级减少两个已经被广泛接受的MOEAs（SMS-EMOA和NSGA-II）在解决两个双目标问题（OneJumpZeroJump和双目标RealRoyalRoad）上的预计运行时间。此外，还进行了实证研究。

    Evolutionary algorithms (EAs) have been widely and successfully applied to solve multi-objective optimization problems, due to their nature of population-based search. Population update, a key component in multi-objective EAs (MOEAs), is usually performed in a greedy, deterministic manner. That is, the next-generation population is formed by selecting the best solutions from the current population and newly-generated solutions (irrespective of the selection criteria used such as Pareto dominance, crowdedness and indicators). In this paper, we question this practice. We analytically present that stochastic population update can be beneficial for the search of MOEAs. Specifically, we prove that the expected running time of two well-established MOEAs, SMS-EMOA and NSGA-II, for solving two bi-objective problems, OneJumpZeroJump and bi-objective RealRoyalRoad, can be exponentially decreased if replacing its deterministic population update mechanism by a stochastic one. Empirical studies als
    
[^122]: 将概率逻辑推理教给变压器

    Teaching Probabilistic Logical Reasoning to Transformers

    [https://arxiv.org/abs/2305.13179](https://arxiv.org/abs/2305.13179)

    本文评估了基于变压器的语言模型在推理不确定文本上的能力，并提出了一种概率约束训练（PCT）的方法来提高模型的概率逻辑推理能力。

    

    在本文中，我们评估了基于变压器的语言模型在推理不确定的文本上的能力，其中包括不确定的推理规则。我们涵盖了预训练语言模型（PLMs）和生成大型语言模型（LLMs）。我们的评估结果表明，这两代语言模型在推理不确定文本方面都存在困难。我们提出了一种新颖的端到端微调方法，概率约束训练（PCT），它在微调阶段利用概率逻辑规则作为约束，而不依赖这些规则在推理阶段。为了评估PCT的有效性，我们利用相关语料库，并额外创建了一个更具挑战性的基准测试，与之前的测试不同，它使用了实例特定的规则。我们的研究表明，PCT提高了基于变压器的语言模型的内在推理能力，使其概率逻辑推理过程更明确和可解释。此外，PCT与传统方法相比，在不确定的文本上取得了更好的性能。

    In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model's intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT eq
    
[^123]: 量化大型语言模型的关联能力及其对隐私泄露的影响

    Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage

    [https://arxiv.org/abs/2305.12707](https://arxiv.org/abs/2305.12707)

    本文研究了大型语言模型的关联能力，并揭示了其对隐私泄露的影响。研究发现，随着模型规模的增加，模型在关联实体/信息方面的能力增强。然而，与常识知识相比，模型在关联个人可识别信息方面的准确性较低。

    

    大型语言模型（LLMs）的进步在各种应用中带来了显著的改进，与此同时也引发了对潜在私人数据泄露的担忧。其中一个显著的LLMs能力是它们能够形成不同信息之间的关联，但这在涉及个人可识别信息（PII）时引发了担忧。本文深入研究了语言模型的关联能力，旨在揭示影响其关联信息能力的因素。我们的研究发现，随着模型规模的扩大，其关联实体/信息的能力增强，特别是当目标对展示更短的共现距离或更高的共现频率时。然而，在关联常识知识与PII方面存在明显的性能差距，后者的准确性较低。尽管准确预测PII的比例相对较小，但LLMs仍然表现出了这种能力。

    The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capab
    
[^124]: 数据增强和损失函数在钻孔工具磨损检测的语义图像分割中的评估

    Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection

    [https://arxiv.org/abs/2302.05262](https://arxiv.org/abs/2302.05262)

    本研究评估了在钻孔工具磨损检测的语义图像分割中的数据增强和损失函数。结果发现，在适度增强的数据上训练的二元模型表现最佳。

    

    在制造过程中，工具磨损监测对于质量控制和成本降低至关重要。本文提出了一种基于U-Net的语义图像分割流程，应用于切割插入物的显微图像，用于磨损检测。磨损区域分为两种不同类型，形成一个多类别分类问题。另一方面，将两种磨损类型合并为一个通用的磨损类别，可以把问题定义为二元分类任务。除了比较二分类和多分类问题外，还研究了不同的损失函数，包括交叉熵、焦点交叉熵和基于IoU的损失。此外，还对不同尺寸的图像块进行模型训练，并使用不同强度的数据增强技术。我们发现，在适度增强的数据上训练的二元模型表现最佳。

    Tool wear monitoring is crucial for quality control and cost reduction in manufacturing processes, of which drilling applications are one example. In this paper, we present a U-Net based semantic image segmentation pipeline, deployed on microscopy images of cutting inserts, for the purpose of wear detection. The wear area is differentiated in two different types, resulting in a multiclass classification problem. Joining the two wear types in one general wear class, on the other hand, allows the problem to be formulated as a binary classification task. Apart from the comparison of the binary and multiclass problem, also different loss functions, i. e., Cross Entropy, Focal Cross Entropy, and a loss based on the Intersection over Union (IoU), are investigated. Furthermore, models are trained on image tiles of different sizes, and augmentation techniques of varying intensities are deployed. We find, that the best performing models are binary models, trained on data with moderate augmentat
    
[^125]: 重新定义强化学习的反事实解释：概述、挑战和机遇

    Redefining Counterfactual Explanations for Reinforcement Learning: Overview, Challenges and Opportunities

    [https://arxiv.org/abs/2210.11846](https://arxiv.org/abs/2210.11846)

    这项工作重新定义了强化学习中的反事实解释方法，并探讨了其在监督学习和强化学习中的差异，以提供用户友好和可操作的解释。

    

    尽管人工智能算法在各个领域展现出了显著的成功，但其缺乏透明度限制了其在实际任务中的应用。虽然面向非专家的解释对用户的信任和人机协作非常重要，但目前大部分面向AI的解释方法都是针对开发者和专家用户的。反事实解释是一种提供用户关于如何改变输入以改变黑盒模型输出的局部解释。反事实解释友好，并提供具体的建议，以实现所需的AI系统输出。尽管反事实解释在监督学习中得到了广泛研究，但在强化学习中很少有应用它们的方法。在这项工作中，我们探讨了强化学习中强大解释方法代表不足的原因。我们首先回顾了监督学习中的反事实解释的当前工作。此外，我们还探讨了反事实解释在强化学习中的差异。

    While AI algorithms have shown remarkable success in various fields, their lack of transparency hinders their application to real-life tasks. Although explanations targeted at non-experts are necessary for user trust and human-AI collaboration, the majority of explanation methods for AI are focused on developers and expert users. Counterfactual explanations are local explanations that offer users advice on what can be changed in the input for the output of the black-box model to change. Counterfactuals are user-friendly and provide actionable advice for achieving the desired output from the AI system. While extensively researched in supervised learning, there are few methods applying them to reinforcement learning (RL). In this work, we explore the reasons for the underrepresentation of a powerful explanation method in RL. We start by reviewing the current work in counterfactual explanations in supervised learning. Additionally, we explore the differences between counterfactual explana
    
[^126]: 强化学习中的局部约束表示

    Locally Constrained Representations in Reinforcement Learning

    [https://arxiv.org/abs/2209.09441](https://arxiv.org/abs/2209.09441)

    本论文提出了一种在强化学习中使用局部约束表示的方法，通过辅助损失函数迫使状态表示与相邻状态的表示具有一定的可预测性，以更好地捕捉到环境的局部变化情况。

    

    强化学习的成功很大程度上依赖于从环境观测数据中学习到稳健的表示。在大多数情况下，纯粹通过强化学习损失函数学习到的表示在不同状态下可能差异巨大，这取决于值函数的变化。然而，学习到的表示并不一定需要与当前任务非常相关。仅依赖强化学习目标可能会导致表示在连续的时间步长中差异很大。此外，由于强化学习损失函数有一个变化的目标，学习到的表示将取决于当前值/策略的好坏。因此，将表示与主要任务解耦可以使其不仅关注于任务特定特征，还关注环境动态特征。为此，我们提出了局部约束表示，其中辅助损失函数迫使状态表示能够由相邻状态的表示进行预测。这鼓励表示更好地捕捉到环境的局部变化。

    The success of Reinforcement Learning (RL) heavily relies on the ability to learn robust representations from the observations of the environment. In most cases, the representations learned purely by the reinforcement learning loss can differ vastly across states depending on how the value functions change. However, the representations learned need not be very specific to the task at hand. Relying only on the RL objective may yield representations that vary greatly across successive time steps. In addition, since the RL loss has a changing target, the representations learned would depend on how good the current values/policies are. Thus, disentangling the representations from the main task would allow them to focus not only on the task-specific features but also the environment dynamics. To this end, we propose locally constrained representations, where an auxiliary loss forces the state representations to be predictable by the representations of the neighboring states. This encourages
    
[^127]: ALEXSIS-PT：一种新的用于葡萄牙语词汇简化的资源

    ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification

    [https://arxiv.org/abs/2209.09034](https://arxiv.org/abs/2209.09034)

    ALEXSIS-PT是一个用于巴西葡萄牙语词汇简化的新型多候选数据集，为LS系统的改进和跨语言模型的研究提供了重要资源。BERTimbau在该数据集上达到了最高的性能。

    

    词汇简化（LS）是自动替换复杂词汇为更容易理解的词汇的任务，使文本对各种目标人群（如低识字率的个体、学习障碍个体、第二语言学习者）更易于访问。为了训练和测试模型，LS系统通常需要包含复杂词汇及其候选替代词的语料库。为了进一步提高LS系统的性能，我们介绍了ALEXSIS-PT，这是一个针对巴西葡萄牙语LS的新型多候选数据集，其中包含387个复杂词汇的9605个候选替代词。ALEXSIS-PT是按照ALEXSIS协议编制的，用于西班牙语，为跨语言模型开辟了新的研究方向。ALEXSIS-PT是第一个包含巴西报纸文章的LS多候选数据集。我们在该数据集上评估了四种替代生成模型，分别是mDistilBERT、mBERT、XLM-R和BERTimbau。BERTimbau在所有评估指标上取得了最高的性能。

    Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their candidate substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS protocol for Spanish opening exciting new avenues for cross-lingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated four models for substitute generation on this dataset, namely mDistilBERT, mBERT, XLM-R, and BERTimbau. BERTimbau achieved the highest performance across all evalu
    
[^128]: Co-Pilot for Health: 个性化算法AI引导改善健康结果

    Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])

    [http://arxiv.org/abs/2401.10816](http://arxiv.org/abs/2401.10816)

    该研究通过使用基于图神经网络的推荐系统和来自可穿戴设备的健康行为数据，设计并实施了一个人工智能驱动平台，实现了个性化和情境引导，能够提高参与者的日常活动水平和中等至剧烈运动时长。

    

    在全球范围内自动塑造大型人群的健康行为，跨可穿戴设备类型和疾病状况具有巨大的潜力来改善全球健康结果。我们设计并实施了一个基于图神经网络（GNN）推荐系统和来自可穿戴健身设备的精细健康行为数据的人工智能驱动平台，用于数字算法引导。在此我们描述了该平台在新加坡针对$n=84,764$个个体的12周期间进行个性化和情境引导的有效性结果。我们统计验证了目标组中接受此类AI优化日常引导的参与者相较于控制组中未接受任何引导的匹配参与者，其每天的步数增加了6.17%（$p = 3.09\times10^{-4}$），每周中等至剧烈运动（MVPA）分钟增加了7.61%（$p = 1.16\times10^{-2}$）。此外，此类引导非常可行。

    The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well r
    
[^129]: SEINE:核实例分割的结构编码与交互网络

    SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation. (arXiv:2401.09773v1 [cs.CV])

    [http://arxiv.org/abs/2401.09773](http://arxiv.org/abs/2401.09773)

    SEINE是一种用于核实例分割的结构编码和交互网络，通过考虑核结构的相关性和利用核之间的结构相似性来提高每个分割实例的完整性。

    

    组织病理学图像中的核实例分割对于生物分析和癌症诊断至关重要，但由于两个原因而具有挑战性。首先，嗜染核内和核外区域的视觉呈现相似，常常导致欠分割。其次，当前方法缺乏对核结构的探索，导致分段实例的碎片化预测。为了解决这些问题，本文提出了一种称为SEINE的结构编码和交互网络，该网络开发了核的结构建模方案，并利用核之间的结构相似性来提高每个分割实例的完整性。具体而言，SEINE引入了基于轮廓的结构编码（SE），以考虑核结构和语义之间的相关性，实现对核结构的合理表示。基于编码，我们提出了结构引导的注意力（SGA），它以清晰的核作为原型，以增强注意力以实现更好的分割结果。

    Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance
    
[^130]: 受过良好教育的智能的内在善良

    The inherent goodness of well educated intelligence. (arXiv:2401.04846v1 [econ.TH])

    [http://arxiv.org/abs/2401.04846](http://arxiv.org/abs/2401.04846)

    本文探讨了智能体变得智能的因素，强调了掌握特征和控制多个保守相互作用的子系统的能力。智能的核心是“集体如一体”和“了解局部行动的整体结果”。文章提出了一种对集体保守系统进行控制的替代方法。

    

    本文将探讨使一个智能体变得智能的因素，无论是生物体还是计算机上的人工智能。特别关注的是能够表征和控制多个保守相互作用的相同子系统的能力。智能的本质将被发现是黄金法则——“集体行动如一体”或“了解局部行动的整体结果”。集体的流动是由掌控着少量字符串的操纵者决定的，根据对称性确定的最小作用路径的测地线运动。控制集体保守系统是困难的，历史上一直通过为系统添加显著黏性来稳定期望的最大性能的亚稳平衡状态，但这会在过程中降低或破坏它们。有一种替代方案。

    This paper will examine what makes a being intelligent, whether that be a biological being or an artificial silicon being on a computer. Special attention will be paid to the being having the ability to characterize and control a collective system of many identical conservative sub-systems conservatively interacting. The essence of intelligence will be found to be the golden rule -- "the collective acts as one" or "knowing the global consequences of local actions". The flow of the collective is a small set of twinkling textures, that are governed by a puppeteer who is pulling a small number of strings according to a geodesic motion of least action, determined by the symmetries. Controlling collective conservative systems is difficult and has historically been done by adding significant viscosity to the system to stabilize the desirable meta stable equilibriums of maximum performance, but it degrades or destroys them in the process. There is an alternative. Once the optimum twinkling te
    
[^131]: 无监督的测试时自适应：通过插入和播放变换器模块

    Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])

    [http://arxiv.org/abs/2401.04130](http://arxiv.org/abs/2401.04130)

    这项工作介绍了PLUTO:一种插拔式模块化的测试时领域适应策略，通过预先训练一系列针对不同源领域的模块，有效地创建了一个"模块存储库"。采用无监督的测试时自适应方法，从存储库中选择稀疏的相关模块的子集，并创建选中模块的加权组合，实现了对新领域的自适应。

    

    参数高效调优(PET)方法，如LoRA、Adapter和Visual Prompt Tuning(VPT)，通过调整变换器模型中的小模块，在使适应新领域方面取得了成功。然而，在测试过程中遇到的领域数量可能非常大，数据通常是无标签的。因此，适应新领域是具有挑战性的，也不现实为每个这样的领域生成定制的调整模块。为了应对这些挑战，本文引入了PLUTO：一种插拔模块化的测试时领域适应策略。我们预训练了一系列模块，每个模块专为不同的源领域进行了专门设计，有效地创建了一个"模块存储库"。给定一个带有少样本无标签数据的目标域，我们引入了一种无监督的测试时自适应(TTA)方法，来(1)从库中选择出稀疏的相关模块的子集，并且(2)在不调整权重的情况下创建选中模块的加权组合。这种插拔式的特性使得它可===

    Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
    
[^132]: Davidsonian场景图：改进文本-图像生成的细粒度评估的可靠性

    Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])

    [http://arxiv.org/abs/2310.18235](http://arxiv.org/abs/2310.18235)

    本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。

    

    评估文本到图像模型一直是困难的。最近一种用于评估文本-图像忠实度的强大方法是基于QG/A（问题生成和回答），它使用预训练的基础模型自动生成一组问题和答案，并基于这些答案与基于提示的答案在视觉问题回答模型中提取的一致性对输出图像进行评分。这种评估自然上取决于底层QG和QA模型的质量。我们确定并解决了现有QG/A工作中的几个可靠性挑战：（a）QG问题应尊重提示（避免幻觉、重复和遗漏）和（b）VQA答案应一致（不会在图像中宣称没有摩托车，同时声称摩托车是蓝色）。我们通过Davidsonian场景图（DSG），这个受形式语义启发的实证评估框架，解决了这些问题。

    Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
    
[^133]: StochGradAdam: 利用随机梯度抽样加速神经网络训练

    StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])

    [http://arxiv.org/abs/2310.17042](http://arxiv.org/abs/2310.17042)

    StochGradAdam是一种利用随机梯度抽样加速神经网络训练的优化器，通过选择性梯度考虑，能够稳定收敛，提升鲁棒训练。在图像分类和分割任务中表现优异。

    

    在深度学习优化领域中，本文介绍了StochGradAdam优化器，这是对广受赞誉的Adam算法的新颖改进。StochGradAdam的核心是其梯度抽样技术。该方法不仅确保稳定收敛，而且利用选择性梯度考虑的优势，通过减轻噪声或异常数据的影响和增强损失函数空间的探索，提升了鲁棒训练。在图像分类和分割任务中，StochGradAdam表现出优于传统Adam优化器的性能。通过在每次迭代中精心选择一部分梯度进行抽样，该优化器能够有效应对复杂模型的管理。本文从数学基础到偏差校正策略全面探讨了StochGradAdam的方法，展示了深度学习训练技术的可期进展。

    In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
    
[^134]: 深度回溯对因果一致解释的反事实推理

    Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])

    [http://arxiv.org/abs/2310.07665](http://arxiv.org/abs/2310.07665)

    本研究提供了一种实用方法，用于在深度生成组件的结构因果模型中计算回溯反事实。通过在因果模型的结构化潜在空间中解决优化问题，我们的方法能够生成反事实，并且与其他方法相比具备了多功能、模块化和符合因果关系的特点。

    

    反事实推理可以通过回答在改变情况下会观察到什么来提供有价值的见解，条件是根据实际观察。虽然经典的介入式解释已经得到了广泛研究，回溯原则被提出作为一种保持所有因果定律完整性的替代哲学，但其研究较少。在本研究中，我们介绍了在由深度生成组件组成的结构因果模型中计算回溯反事实的实用方法。为此，我们对结构分配施加了条件，通过在因果模型的结构化潜在空间中解决一个可行的约束优化问题来生成反事实。我们的方法还可以与反事实解释领域的方法进行比较。与这些方法相比，我们的方法代表了一种多功能、模块化和遵守因果的替代方案。

    Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
    
[^135]: 通过同时学习面部标志检测、域分离和重建来提高面部动作单位检测的精度

    Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05207](http://arxiv.org/abs/2310.05207)

    本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。

    

    最近，如何将大量的在野非标记面部图像引入监督式面部动作单位（AU）检测框架中成为一个具有挑战性的问题。本文提出了一种新的AU检测框架，通过共享同构面部提取模块的参数，引入多任务学习，同时学习AU域分离和重建以及面部标志检测。另外，我们提出了一种基于对比学习的新特征对齐方案，通过简单的投影器和改进的对比损失添加了四个额外的中间监督器来促进特征重建的过程。在两个基准测试上的实验结果表明，我们在野外AU检测方面优于现有的方法。

    Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
    
[^136]: 基于思维混合表示的大规模语言模型级联用于成本高效的推理

    Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])

    [http://arxiv.org/abs/2310.03094](http://arxiv.org/abs/2310.03094)

    本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。

    

    大规模语言模型（LLM）如GPT-4在各种任务中展现出了非凡的性能，但是这种强大的性能通常伴随着使用付费API服务的高昂费用。本文的研究动机是为了研究构建LLM级联以节约使用LLM的成本，特别是用于进行推理（例如数学、因果推理）任务的成本。我们的级联管道遵循一个直观的思想，即简单的问题可以由一个更弱但更实惠的LLM来解决，而只有具有挑战性的问题才需要更强大、更昂贵的LLM。为了实现这种决策，我们考虑到更弱的LLM的“答案一致性”作为问题难度的信号，并提出了几种答案采样和一致性检查的方法，其中一种方法利用了两种思维表示（即连续思维和程序思维）的混合。通过在六个推理基准数据集上的实验，我们使用GPT-3.5-turbo和GPT-4作为较弱的模型，

    Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
    
[^137]: Alphazero类似的树搜索可以指导大型语言模型的解码和训练

    Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])

    [http://arxiv.org/abs/2309.17179](http://arxiv.org/abs/2309.17179)

    Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性

    

    大型语言模型 (LLM) 通常采用采样或束搜索，结合 Chain-of-Thought (CoT) 等提示来提高推理和解码能力。最近的研究如 Tree-of-Thought (ToT) 和 Reasoning via Planning (RAP) 旨在通过利用树搜索算法来引导多步推理，来增强LLM的推理能力。这些方法主要关注LLM在推理过程中的推理能力，并且严重依赖人为设计的提示来激活LLM作为一个价值函数，缺乏普适性和可扩展性。为了解决这些限制，我们提出了一种AlphaZero类似的用于LLM的树搜索框架 (称为TS-LLM)，系统地说明了如何通过学习的价值函数利用树搜索来指导LLM的解码能力。TS-LLM在两个关键方面与众不同：(1)通过利用学习的价值函数，我们的方法可以普适地应用于除了推理之外的不同任务 (例如RLHF对齐)，以及任何大小的LLM，而不需要提示

    Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
    
[^138]: Attentive VQ-VAE：一种增强VQ-VAE模型能力的新方法

    Attentive VQ-VAE. (arXiv:2309.11641v1 [cs.CV])

    [http://arxiv.org/abs/2309.11641](http://arxiv.org/abs/2309.11641)

    本研究提出了一种增强VQ-VAE模型能力的新方法，通过整合Attentive Residual Encoder和Residual Pixel Attention层，利用像素间的自我注意机制来高效地捕捉和利用潜在向量之间的上下文信息，并使用额外的编码级别来进一步增强模型的表示能力，在实验中取得了显著的性能改进。

    

    本文通过整合Attentive Residual Encoder（AREN）和Residual Pixel Attention层，提出了一种增强VQ-VAE模型能力的新方法。我们的研究目标是在保持实用的参数水平的同时改进VQ-VAE的性能。AREN编码器被设计成能够有效地在多个级别上操作，适应不同的架构复杂性。关键创新在于将像素间的自我注意机制整合到AREN编码器中。这种方法使我们能够高效地捕捉和利用潜在向量之间的上下文信息。此外，我们的模型使用了额外的编码级别来进一步增强模型的表示能力。我们的注意力层采用最小参数方法，确保只有在其他像素的相关信息可用时才修改潜在向量。实验结果表明，我们提出的修改显著提高了数据的处理效果。

    We present a novel approach to enhance the capabilities of VQVAE models through the integration of an Attentive Residual Encoder (AREN) and a Residual Pixel Attention layer. The objective of our research is to improve the performance of VQVAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in dat
    
[^139]: 语言模型在与知识库进行连接时的数据分布瓶颈

    Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])

    [http://arxiv.org/abs/2309.08345](http://arxiv.org/abs/2309.08345)

    本文通过实验调查揭示了语言模型在与知识库进行连接时的数据分布瓶颈，包括推广到未见域、适应语言变体和在不同数据集之间的可转移性等方面。即使采用数据增强技术，先进的语言模型在多个方面表现出较差的性能。

    

    语言模型（LM）已经展示了在理解和生成自然语言和形式语言方面的卓越能力。尽管取得了这些进展，但它们与大规模知识库等现实环境的整合仍然是一个欠发展的领域，影响了语义解析等应用，并且容易出现“产生虚假信息”的问题。本文通过实验调查揭示了LM在处理知识库问答（KBQA）任务时所遇到的健壮性挑战。研究覆盖了训练和推断之间数据分布不一致的场景，例如推广到未见域、适应各种语言变体和在不同数据集之间的可转移性。我们的全面实验揭示了即使在采用我们提出的数据增强技术的情况下，先进的小型和大型语言模型在多个方面表现出较差的性能。

    Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
    
[^140]: 使用LLMs进行生成式数据增强提高问答中的分布鲁棒性

    Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])

    [http://arxiv.org/abs/2309.06358](http://arxiv.org/abs/2309.06358)

    本论文研究了使用生成式数据增强方法如何提高问答模型在自然分布转换下的鲁棒性，通过实验展示了增强阅读理解数据集的效果。

    

    自然语言处理中的鲁棒性问题仍然是一个重要的问题，最先进的模型在自然分布转换下表现不佳。在问答环境中，对领域适应方法的研究工作仍在不断发展。然而，在自然分布转换下的域泛化概念却受到很少关注，因为目标域是未知的。随着生成模型质量和获取方式的大幅提高，我们回答了一个问题：生成的数据集如何影响问答模型在自然分布转换下的性能？我们在4个不同数据集上进行了实验，分析了“野外生成”如何帮助实现域泛化。我们采取了两步生成方法，生成上下文和问答对来增强现有数据集。通过我们的实验，我们展示了如何通过增强阅读理解数据集来提升领域泛化能力。

    Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets wit
    
[^141]: LLM在Shell中的应用：生成式蜜罐

    LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])

    [http://arxiv.org/abs/2309.00155](http://arxiv.org/abs/2309.00155)

    本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐，解决了以往蜜罐的重要局限性，并通过实验验证了其高准确率。

    

    蜜罐是网络安全中的重要工具。然而，大多数蜜罐（即使是高交互式的）缺乏足够的真实感来欺骗攻击者。这个限制使得它们很容易被识别，从而影响到它们的有效性。本研究引入了一种基于大型语言模型的新方法来创建动态和真实的软件蜜罐。初步结果表明，LLM能够创建可信且动态的蜜罐，能够解决以往蜜罐的重要局限性，如确定性响应、缺乏适应性等。我们通过与需要判断蜜罐回应是否虚假的攻击者进行实验来评估每个命令的真实性。我们提出的蜜罐，称为shelLM，达到了0.92的准确率。

    Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
    
[^142]: AI代码生成器的漏洞：探索针对数据毒化攻击的方法

    Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (arXiv:2308.04451v1 [cs.CR])

    [http://arxiv.org/abs/2308.04451](http://arxiv.org/abs/2308.04451)

    本文评估了AI代码生成器的安全性，发现它们容易受到数据毒化攻击，即注入恶意样本来生成易受攻击的代码。攻击可以成功即使只有少量数据被毒化，而且不影响预训练模型生成的代码的正确性，使其难以被检测。

    

    在这项工作中，我们通过数据毒化评估AI代码生成器的安全性，即通过将恶意样本注入训练数据来生成易受攻击的代码。我们通过注入包含安全漏洞的代码来毒化训练数据，并评估不同最先进的代码生成模型对攻击的成功程度。我们的分析表明，即使只有少量的数据毒化，AI代码生成器也容易受到攻击。此外，该攻击不会影响预训练模型生成的代码的正确性，使其难以检测。

    In this work, we assess the security of AI code generators via data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our analysis shows that AI code generators are vulnerable to even a small amount of data poisoning. Moreover, the attack does not impact the correctness of code generated by pre-trained models, making it hard to detect.
    
[^143]: 人工智能解释性对人工智能决策的影响

    The Impact of Imperfect XAI on Human-AI Decision-Making. (arXiv:2307.13566v1 [cs.HC])

    [http://arxiv.org/abs/2307.13566](http://arxiv.org/abs/2307.13566)

    本研究通过一个混合方法用户研究，评估了不正确的解释如何影响人类的决策行为，以增进人工智能解释性对人工智能决策的理解。

    

    解释性技术正在快速发展，以改进各种合作工作环境下的人工智能决策。因此，先前的研究评估了决策者与不完美的人工智能协作的方式，研究合适的依赖关系和任务表现，以便设计更加以人为中心的计算机支持的协作工具。一些以人为中心的可解释人工智能（XAI）技术被提出，希望改善决策者与人工智能的合作；然而，这些技术基于先前研究的发现，主要关注错误的人工智能建议的影响。很少有研究承认即使人工智能建议正确，解释也可能是错误的。因此，了解不完美的解释性人工智能如何影响人工智能决策至关重要。在这项工作中，我们通过一个强大的混合方法用户研究，涉及136名参与者，评估了不正确的解释如何影响人类的决策行为。

    Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making beha
    
[^144]: SciBench: 对大型语言模型评估大学水平的科学问题解决能力

    SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])

    [http://arxiv.org/abs/2307.10635](http://arxiv.org/abs/2307.10635)

    这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。

    

    最近大型语言模型(LLMs)的进展在许多数学基准上取得了显著的进步。然而，这些基准大多只包含初高中科目的问题，仅包含多项选择题，并且仅限于基本算术运算范围。为了解决这些问题，本文介绍了一个广泛的基准套件SciBench，旨在系统地检测复杂科学问题解决所需的推理能力。SciBench包含两个经过精心策划的数据集：一个开放集，包括从数学、化学和物理教科书中摘录的大学水平的科学问题，以及一个封闭集，包含来自计算机科学和数学本科考试的问题。基于这两个数据集，我们对两个代表性的LLM进行了深入的基准研究，并采用不同的提示策略。结果表明，当前的LLMs在提供复杂科学问题解决能力方面还存在不足之处。

    Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
    
[^145]: 解释性不是游戏。(arXiv:2307.07514v1 [cs.AI])

    Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])

    [http://arxiv.org/abs/2307.07514](http://arxiv.org/abs/2307.07514)

    Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.

    

    可解释性人工智能（XAI）旨在帮助人类决策者理解复杂的机器学习（ML）模型。XAI的一个重要特征是通过使用Shapley值来理论上证明相对特征重要性的度量。本文在最近的研究基础上，提出一个简单的论证，说明Shapley值可能会给相对特征重要性提供误导，使其为预测中无关的特征分配更高的重要性，而对与预测有关的特征分配较低的重要性。这些结果的意义在于它们有效地挑战了相对特征重要性的多种提议用法，这些用法正在高风险应用领域快速增长。

    Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding complex machine learning (ML) models. One of the hallmarks of XAI are measures of relative feature importance, which are theoretically justified through the use of Shapley values. This paper builds on recent work and offers a simple argument for why Shapley values can provide misleading measures of relative feature importance, by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction. The significance of these results is that they effectively challenge the many proposed uses of measures of relative feature importance in a fast-growing range of high-stakes application domains.
    
[^146]: 通过物理对称学习可解释的低维表示

    Learning Interpretable Low-dimensional Representation via Physical Symmetry. (arXiv:2302.10890v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10890](http://arxiv.org/abs/2302.10890)

    通过使用物理对称性作为潜在空间的自洽约束条件，该研究展示了在音乐领域和计算机视觉领域，模型可以以无监督的方式学习出可解释的低维表示，例如线性音高和三维笛卡尔因素。

    

    可解释的表示学习在创造性智能系统中起着关键作用。在音乐领域，当前的学习算法可以成功地学习各种特征，如音高、音色、和弦、纹理等。然而，大多数方法严重依赖音乐领域知识。现在还不清楚什么样的一般性计算原则会产生可解释的表示，特别是与人类感知保持一致的低维因素。在这项研究中，我们从现代物理学中获得灵感，将物理对称性作为潜在空间的自洽约束条件。特别是，它要求先验模型对潜在状态的动态进行描述，并以某种群变换对其进行等变。我们展示了物理对称性使得模型能够以无监督的方式从未标记的单声道音乐音频中学习一个线性音高因素。此外，相同的方法可以应用于计算机视觉，学习一个三维笛卡尔因素。

    Interpretable representation learning has been playing a key role in creative intelligent systems. In the music domain, current learning algorithms can successfully learn various features such as pitch, timbre, chord, texture, etc. However, most methods rely heavily on music domain knowledge. It remains an open question what general computational principles give rise to interpretable representations, especially low-dim factors that agree with human perception. In this study, we take inspiration from modern physics and use physical symmetry as a self-consistency constraint for the latent space. Specifically, it requires the prior model that characterises the dynamics of the latent states to be equivariant with respect to certain group transformations. We show that physical symmetry leads the model to learn a linear pitch factor from unlabelled monophonic music audio in a self-supervised fashion. In addition, the same methodology can be applied to computer vision, learning a 3D Cartesian
    

