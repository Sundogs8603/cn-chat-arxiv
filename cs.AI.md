# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Policy Gradient Methods in the Presence of Symmetries and State Abstractions.](http://arxiv.org/abs/2305.05666) | 本文研究了在连续控制环境中的抽象，提出了一种策略梯度定理，允许利用环境的近似对称性进行策略优化，并提出了一系列演员-评论家算法进行策略和MDP同态映射的学习，最后展示了算法在连续对称性环境和视觉控制任务中的有效性。 |
| [^2] | [ImageBind: One Embedding Space To Bind Them All.](http://arxiv.org/abs/2305.05665) | ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。 |
| [^3] | [ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives.](http://arxiv.org/abs/2305.05661) | ShapeCoder是能够从非结构化的基元中发现视觉程序的抽象的第一个系统，用于重写程序以使其更紧凑，暴露自由度更少。在对合成和现实世界数据集的实验中，ShapeCoder优于现有基线，程序大小减少了3倍。 |
| [^4] | [TidyBot: Personalized Robot Assistance with Large Language Models.](http://arxiv.org/abs/2305.05658) | 本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。 |
| [^5] | [Could AI be the Great Filter? What Astrobiology can Teach the Intelligence Community about Anthropogenic Risks.](http://arxiv.org/abs/2305.05653) | 本文探讨了人工智能对大过滤器假设的可能性以及如何在天体生物学的背景下理解全球灾难性风险，并指出了情报界通过认识到AI在大规模全球风险中的潜在作用获得有价值洞察的必要性。 |
| [^6] | [Predicting Cardiovascular Disease Risk using Photoplethysmography and Deep Learning.](http://arxiv.org/abs/2305.05648) | 利用光电容积描记术和深度学习预测心血管疾病风险，可在低成本下对低收入和中等收入国家进行大规模筛查，并超越了传统风险评分工具。 |
| [^7] | [Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare.](http://arxiv.org/abs/2305.05640) | 本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。 |
| [^8] | [Bilingual analogical proportions.](http://arxiv.org/abs/2305.05614) | 本文在通用代数和一阶逻辑的一般设定下，推广了抽象的类比比例代数逻辑框架，实现了从单语言到双语言框架的转变，扩展了基础框架的适用性。 |
| [^9] | ["Alexa doesn't have that many feelings": Children's understanding of AI through interactions with smart speakers in their homes.](http://arxiv.org/abs/2305.05597) | 研究显示，大多数6-11岁的儿童高估了语音对话助手的智能，对它们的情感或代理感到不确定，并缺乏对数据隐私和安全方面的准确理解。 |
| [^10] | [PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces.](http://arxiv.org/abs/2305.05594) | PET-NeuS是一个神经表面重建方法，引入了三平面表示法、新型的位置编码和可学习的卷积操作，并能实现高质量的表面重建。 |
| [^11] | [RLocator: Reinforcement Learning for Bug Localization.](http://arxiv.org/abs/2305.05586) | 本文提出了一种基于强化学习的Bug定位方法RLocator，相较于其他最先进的Bug定位技术具有更优越的性能。 |
| [^12] | [SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.05566) | 本论文介绍了SMAClite，一个轻量级的多智能体强化学习环境，解耦了原有的闭源游戏并提供了开源框架，SMAClite在运行时速度和内存方面超越了SMAC。 |
| [^13] | [SkelEx and BoundEx: Natural Visualization of ReLU Neural Networks.](http://arxiv.org/abs/2305.05562) | SkelEx和BoundEx是第一批自然可视化的方法，可以提取ReLU神经网络的关键点和决策边界。这两种方法为在低维数据上训练的ReLU NN引入了非常自然的可视化工具。 |
| [^14] | [Distributional Multi-Objective Decision Making.](http://arxiv.org/abs/2305.05560) | 该论文介绍了一种分布式多目标决策制定的方法，其中引入了分布式不支配集和凸分布不支配集的概念，证明了它们可以包含所有最优解，通过实验验证了方法的有效性。 |
| [^15] | [Social Value Orientation and Integral Emotions in Multi-Agent Systems.](http://arxiv.org/abs/2305.05549) | 本文研究了社会价值取向(SVO)和积分情感在多智能体系统中的相互作用，开发了一种基于SVO和积分情感政策的智能体设计方法，并在仿真实验中探究了不同的社会偏好和情感对智能体的影响。 |
| [^16] | [Walk4Me: Telehealth Community Mobility Assessment, An Automated System for Early Diagnosis and Disease Progression.](http://arxiv.org/abs/2305.05543) | Walk4Me是一个远程康复社区行动能力评估系统，采用基于人工智能的步态特征检测来识别疾病患者的步态障碍并追踪疾病的进展。 |
| [^17] | [Efficient pattern-based anomaly detection in a network of multivariate devices.](http://arxiv.org/abs/2305.05538) | 该论文提出了一种高效的基于模式挖掘的异常检测方法，该方法不仅考虑多元时间序列，还考虑了实体间的通信关系，并提供可解释的解释。实验表明，该方法比现有方法更有效率和有效。 |
| [^18] | [An ensemble of convolution-based methods for fault detection using vibration signals.](http://arxiv.org/abs/2305.05532) | 本文提出了集成了三种基于卷积的方法，用于解决振动信号的故障检测问题。实验结果表明，该方法在准确率上优于其他方法，达到了超过98.8\%的准确率。 |
| [^19] | [Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises.](http://arxiv.org/abs/2305.05525) | 本文提出了一种基于梯度的可解释AI技术的方法，用于识别时间序列数据中的显著帧，在中风康复锻炼领域具有应用潜力。 |
| [^20] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^21] | [Optimization- and AI-based approaches to academic quality quantification for transparent academic recruitment: part 1-model development.](http://arxiv.org/abs/2305.05460) | 该论文开发了两种计算框架来量化高校教育质量，通过一个单一指标（AQI）来综合衡量学者的学术品质。 |
| [^22] | [A Cross-Frequency Protective Emblem: Protective Options for Medical Units and Wounded Soldiers in the Context of (fully) Autonomous Warfare.](http://arxiv.org/abs/2305.05459) | 本文提出了一种能够应对(完全)自主战争中保护非战斗人员的跨频保护徽章设计方案。 |
| [^23] | [Egocentric Hierarchical Visual Semantics.](http://arxiv.org/abs/2305.05422) | 本文提出了一个基于分层语义结构的物体识别方法，即递归识别物体的视觉属和差异属性，实现了物体识别过程的分层实现。 |
| [^24] | [Measuring Rule-based LTLf Process Specifications: A Probabilistic Data-driven Approach.](http://arxiv.org/abs/2305.05418) | 本文介绍了一种基于概率的数据驱动方法，该方法可以度量声明性过程规范的满足度，并可用于发现、检查和漂移检测等方面。 |
| [^25] | [Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey.](http://arxiv.org/abs/2305.05403) | 本调查讨论了在开放世界知识库中表达、提取和推断完整性、召回率和否定性信息的方法，以及面对不完整和不确定的知识时，从业者和研究人员应该如何处理这些情况。 |
| [^26] | [On the Relation between Sharpness-Aware Minimization and Adversarial Robustness.](http://arxiv.org/abs/2305.05392) | SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗性能。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。SAM单独使用可以在不牺牲清晰度精度的情况下提高对抗鲁棒性。 |
| [^27] | [VEDLIoT -- Next generation accelerated AIoT systems and applications.](http://arxiv.org/abs/2305.05388) | VEDLIoT项目旨在开发一种安全、节能和高效的深度学习方法，以解决分布式物联网（AIoT）应用程序的挑战。其核心方法在于模块化和可扩展的认知物联网硬件平台，利用微服务器技术和异构计算以及全谱硬件加速器的集成。项目的贡献涵盖了可信计算、远程断言和安全执行环境。 |
| [^28] | [Code Execution with Pre-trained Language Models.](http://arxiv.org/abs/2305.05383) | 本文研究了预训练语言模型对代码执行的能力，并通过开发一种变异数据增强技术创建了一个大规模的Python数据集和任务，提出了CodeExecutor模型以增强语义理解，该模型在代码执行、零-shot代码到代码搜索和文本到代码生成等方面有潜在好处。 |
| [^29] | [Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models.](http://arxiv.org/abs/2305.05377) | 本研究创建了一个基准数据集，测试了大型语言模型的职业准备技能，比较了GPT-3和Turbo-GPT3.5在1149个专业认证领域的表现，Turbo-GPT3.5的通过率达到了100%。模型证明了在计算机、医疗保健和金融等各个领域中的技能和潜力。这个数据集可以用来进一步训练和评估语言模型的表现。 |
| [^30] | [HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction.](http://arxiv.org/abs/2305.05374) | 本文提出了HybridNet，一种基于几何与拓扑视角的VLSI阻塞预测的双分支融合网络，通过在网络结构中做出几个关键设计，充分综合电路的拓扑与几何特征，相较于以往方法取得了10.9％的提高。 |
| [^31] | [GNNs,You can be Stronger,Deeper and Faster.](http://arxiv.org/abs/2305.05368) | 本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。 |
| [^32] | [Large Language Model Programs.](http://arxiv.org/abs/2305.05364) | 本文提出了一种将大型预训练语言模型嵌入算法或程序中，扩展其能力的方法。这种方法可以在未经微调的情况下通过更具算法性的方法获得不错的性能提升。 |
| [^33] | [Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement.](http://arxiv.org/abs/2305.05354) | 本论文提出了一种基于安全深度强化学习的椎管机器人手术术中规划方法，解决了相对传统的术前规划和术中注册开环控制的局限，能够提高放置准确度和保证手术安全。 |
| [^34] | [A Framework for Designing Foundation Model based Systems.](http://arxiv.org/abs/2305.05352) | 本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。 |
| [^35] | [Detection of depression on social networks using transformers and ensembles.](http://arxiv.org/abs/2305.05325) | 本文利用transformer和集成技术，在社交网络上检测抑郁症的迹象，构建了多个基于预训练语言模型的分类器和两种类型的集成模型，表现更好。 |
| [^36] | [Application of Artificial Intelligence in the Classification of Microscopical Starch Images for Drug Formulation.](http://arxiv.org/abs/2305.05321) | 本研究利用人工智能技术，通过转移学习和深度卷积神经网络对不同植物来源的淀粉显微图像进行分类，实现了81%的分类准确率和更优的精度、召回率和f1得分。 |
| [^37] | [Structured Sentiment Analysis as Transition-based Dependency Parsing.](http://arxiv.org/abs/2305.05311) | 本文提出了第一种将结构化情感分析作为依存句法分析处理的基于转移的方法，其基于Pointer Network体系结构，实现了在准确性和效率方面均优于以前提出的图形模型的结果，是迄今为止最为准确的SSA方法。 |
| [^38] | [VCSUM: A Versatile Chinese Meeting Summarization Dataset.](http://arxiv.org/abs/2305.05280) | 介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。数据集和代码将在GitHub上发布。 |
| [^39] | [Learning to Personalize Recommendation based on Customers' Shopping Intents.](http://arxiv.org/abs/2305.05279) | 本文介绍了亚马逊的新系统，利用深度学习模型将顾客的在线行为映射成为高级别购物意图，以便个性化推荐，提供更相关、可解释和多样化的购物体验。 |
| [^40] | [Rotation Synchronization via Deep Matrix Factorization.](http://arxiv.org/abs/2305.05268) | 本文提出了一个无监督且具有隐式正则化属性的深度神经网络公式，用于实现旋转同步问题，并在实验中取得了与竞争对手相当的准确性。 |
| [^41] | [Distilling Script Knowledge from Large Language Models for Constrained Language Planning.](http://arxiv.org/abs/2305.05252) | 本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。 |
| [^42] | [Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy.](http://arxiv.org/abs/2305.05247) | 本文研究了在医疗保健领域使用生成式AI模型合成匿名化病人数据进行研究和培训的方法，旨在平衡数据访问和隐私保护，并探索其在医疗保健领域的应用，并讨论了其未来的挑战和研究方向。 |
| [^43] | [Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection.](http://arxiv.org/abs/2305.05239) | 本文提出了一个通用的Learnable Behavioral Control (LBC)框架，使得行为选择空间得到扩大，并通过基于赌博机的元控制器实现行为控制。在Atari游戏上，我们的代理已经达到10个游戏的人类水平，并在7个游戏中达到了目前的最高分。 |
| [^44] | [FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity.](http://arxiv.org/abs/2305.05230) | 本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。 |
| [^45] | [Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance.](http://arxiv.org/abs/2305.05228) | 本研究提出了一种通用的语义嵌入深度神经网络，通过空间感知的语义特征和基于通道的注意力模型来提高多标签预测的模型性能，平均相对改进达到15.27%。 |
| [^46] | [FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration.](http://arxiv.org/abs/2305.05222) | FishRecGAN提供了一种端到端的深度学习方法，以矫正鱼眼图像并同时校准相机内参和畸变参数。其快速校正网络具有良好的分辨率和鲁棒性，适用于摄像机型监控设备中的恒定标定，并使用大量合成数据集进行训练和验证，表现出了高分辨率的鲁棒性和显著的峰值信噪比。 |
| [^47] | [LSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias Problem.](http://arxiv.org/abs/2305.05200) | 本文提出了一种轻量级子注意力策略（LSAS），能够显著缓解计算机视觉中深度神经网络的关注偏差问题，即DNN过于聚焦于标签无关的区域，并且无法完全包含理想区域。LSAS能够改进现有的自我关注模块并在广泛使用的基准数据集和流行的关注网络中证明其有效性。 |
| [^48] | [COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective.](http://arxiv.org/abs/2305.05191) | 本文提出了一个从因果推断角度出发的情境化常识因果推理任务，设计了一个零-shot框架COLA来解决此任务，并且该框架可以更准确地检测常识因果关系。 |
| [^49] | [DeepFire2: A Convolutional Spiking Neural Network Accelerator on FPGAs.](http://arxiv.org/abs/2305.05187) | 本文介绍了一种基于FPGA的深度神经网络加速器DeepFire2，通过利用多芯片的超级逻辑区域映射大型网络层，避免了逻辑资源限制层大小，同时通过深度流水线提高了时钟速度，将吞吐量和功率效率提高了一倍。 |
| [^50] | [CSED: A Chinese Semantic Error Diagnosis Corpus.](http://arxiv.org/abs/2305.05183) | 本文建立了一个中文语义错误诊断语料库CSED，通过提出基于句法的模型实现了CSED-R和CSED-C任务的最佳表现。 |
| [^51] | [MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts.](http://arxiv.org/abs/2305.05181) | 本文提出了一个名为 MoT 的框架，让大型语言模型通过“思想记忆”自我进化，无需注释数据集和参数更新。实验表明，该框架可以显著提高 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面的能力。 |
| [^52] | [Simplicial Hopfield networks.](http://arxiv.org/abs/2305.05179) | 展示了一种简单的形式化 Hopfield 神经网络，通过添加集合连接并将这些连接嵌入到一个单纯复合体中，可增加存储容量。 |
| [^53] | [FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.](http://arxiv.org/abs/2305.05176) | 本论文介绍了如何在使用大型语言模型的同时降低成本和提高性能。作者提出了三种策略，包括提示适应，LLM近似和LLM级联，并且通过 FrugalGPT 这种方法，实现了大大降低成本或是提高准确率的效果。 |
| [^54] | [Logic for Explainable AI.](http://arxiv.org/abs/2305.05172) | 本文介绍了一种基于符号逻辑的综合、语义和计算理论，以探讨可解释性人工智能的三个维度，以深入理解分类器所做出的决策。 |
| [^55] | [Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization.](http://arxiv.org/abs/2305.05163) | 本研究采用深度强化学习合作的图神经网络，结合疾病动力学模型，开发了一种疫苗优先考虑策略，旨在在供应有限的情况下降低疫情的总体负担。 |
| [^56] | [Latent Interactive A2C for Improved RL in Open Many-Agent Systems.](http://arxiv.org/abs/2305.05159) | 本论文介绍了一种基于编码-解码架构的潜在交互式A2C方法，以在多智能体系统中实现强化学习的改进。该方法显著提高了样本效率，通过学习隐藏状态和其他智能体的行为，解决了在竞争或对抗环境中从其他智能体中获得各种信息可能不可行的问题。 |
| [^57] | [Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval.](http://arxiv.org/abs/2305.05144) | 本文针对零样本基于草图图像检索的跨域和语义问题，提出了一种自适应和对齐的方法，通过插入简单且轻量的域适配器重新学习草图领域的新抽象概念，并通过明确对齐学习到的图像嵌入以提高跨域表示能力。 |
| [^58] | [To AI or not to AI, to Buy Local or not to Buy Local: A Mathematical Theory of Real Price.](http://arxiv.org/abs/2305.05134) | 本文建立了一个数学理论，用于确定代理人的最优全球与本地支出，从而实现代理人在支出和获得效用之间的最优平衡。 |
| [^59] | [A Kriging-Random Forest Hybrid Model for Real-time Ground Property Prediction during Earth Pressure Balance Shield Tunneling.](http://arxiv.org/abs/2305.05128) | 该研究提出了一种Kriging-Random Forest混合模型，结合先前预测的地质信息和实时的运行参数信息，为地压平衡盾构机前方地质预测提供指导，从而缓解施工风险。 |
| [^60] | [Comparing Foundation Models using Data Kernels.](http://arxiv.org/abs/2305.05126) | 本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。 |
| [^61] | [Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning.](http://arxiv.org/abs/2305.05119) | 本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。 |
| [^62] | [When a CBR in Hand is Better than Twins in the Bush.](http://arxiv.org/abs/2305.05111) | 本文讨论了一个回归问题——预测航班起飞延误。一个由 XGB-CBR Twin 转换来的 CBR 模型提供了最准确的局部预测、最易解释的局部解释表示和全局重要性的维护。 |
| [^63] | [TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection.](http://arxiv.org/abs/2305.05105) | TDC'22是第一届面向ICDs低功耗微控制器的人工智能/机器学习（AI/ML）算法创新竞赛。本次竞赛的挑战是开发一种基于AI/ML的新型实时检测算法，对危及生命的室性心律失常进行检测。 |
| [^64] | [Who Needs Decoders? Efficient Estimation of Sequence-level Attributes.](http://arxiv.org/abs/2305.05098) | 研究提出了非自回归代理模型(NAP)，通过编码序列直接预测通用标量值序列级属性，可以高效地实现解码步骤的规避。在机器翻译和语音识别两个场景下，NAP分别可以优于深度集成和高精度地预测性能指标。 |
| [^65] | [Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness.](http://arxiv.org/abs/2305.05095) | 本文提出两种方法来提高CLIP培训效率和鲁棒性：（1）通过增加训练数据集来提高效率，（2）过滤掉图像中带有文本区域的样本以增强鲁棒性和防御印刷攻击，从而在ImageNet和Coco等公共基准测试上显着提高了分类和检索准确性。 |
| [^66] | [Artificial Intelligence in 3GPP 5G-Advanced: A Survey.](http://arxiv.org/abs/2305.05092) | 本文综合介绍了3GPP Release 18在5G-Advanced中关于人工智能的最新发展情况，包括多样化研究和工作项以及设计方面的各个方面。 |
| [^67] | [Knowledge-enhanced Agents for Interactive Text Games.](http://arxiv.org/abs/2305.05091) | 本文提出了一个知识加强的代理框架，用于加强代理在文本游戏中的功能基础，具有记忆先前操作和环境对象可行性两项领域知识，支持三个代表性模型类。 |
| [^68] | [Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning.](http://arxiv.org/abs/2305.05080) | 本文回顾了最优输运在机器学习中的应用，并探讨了如何将其扩展以适应大数据和高维数据的需求。 |
| [^69] | [Indoor Localization and Multi-person Tracking Using Privacy Preserving Distributed Camera Network with Edge Computing.](http://arxiv.org/abs/2305.05062) | 该研究提出了一个开源、低成本、可扩展且保护隐私的边缘计算框架，用于在室内空间中估计多人的位置、朝向和轨迹。 |
| [^70] | [ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models.](http://arxiv.org/abs/2305.05050) | 本文介绍了一种名为“ANALOGICAL”的新型基准，用以内在评估LLMs在长文本类比中的能力，包括六个复杂级别的长文本类比分类，并使用13个数据集和三种距离度量方法来评估8个LLMs在语义向量空间中识别类比对的能力。 |
| [^71] | [Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust.](http://arxiv.org/abs/2305.04989) | 本研究通过知识图谱结构评估Self-Attention变压器中编码的语义。结果显示，语言模型是概率语言模式产生的控制过程的模型，但是不将对象和概念级别的含义和语义赋予所学习的随机模式，例如知识图谱中所描述的模式。 |
| [^72] | [From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks.](http://arxiv.org/abs/2305.04963) | 本研究提出了一种从关系池化到子图GNN的通用框架，可提高任何基本GNN模型的表现力，通过明确标记节点作为附加特征来实现此目的， 并可在许多syn上实现超越性能。 |
| [^73] | [The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification.](http://arxiv.org/abs/2305.04940) | 本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。 |
| [^74] | [A transformer-based method for zero and few-shot biomedical named entity recognition.](http://arxiv.org/abs/2305.04928) | 本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。 |
| [^75] | [Detecting and Reasoning of Deleted Tweets before they are Posted.](http://arxiv.org/abs/2305.04927) | 本研究提出了一种新的深度学习框架，可以在推文发布之前识别出即将被删除的内容，并推理其潜在危害和违反平台政策的原因。该方法可用于推进更安全和负责任的社交媒体使用。 |
| [^76] | [Graph Masked Autoencoder for Sequential Recommendation.](http://arxiv.org/abs/2305.04619) | 提出了一种简单而有效的基于图遮盖自编码器的序列推荐系统，它使用基于图的注意力机制暴露出带有遮盖的项目序列，自适应动态提取全局项目转换信息进行自监督增强，在具有较少标记样本的情况下始终比最先进的序列推荐方法表现出更好的性能，而且对数据损坏和缺失情况具有鲁棒性。 |
| [^77] | [Larger Offspring Populations Help the $(1 + (\lambda, \lambda))$ Genetic Algorithm to Overcome the Noise.](http://arxiv.org/abs/2305.04553) | 更多的后代数量有助于$(1 + (\lambda, \lambda))$遗传算法在噪声环境下表现更鲁棒。 |
| [^78] | [Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation.](http://arxiv.org/abs/2305.04474) | 本文提出了一种跨模态相似性逐步细化的对比学习策略，在视觉语言预训练中优化图像/文本锚点与其负样本文本/图像之间的互信息，有效应对了（部分）误反样本的挑战。 |
| [^79] | [Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection.](http://arxiv.org/abs/2305.04379) | 本文提出了一种最先进的加权目标函数，用于提高多标签分类中深度神经网络（DNN）针对长尾数据分布的性能，并通过对时尚服装的图像属性分类的实验，取得了良好的性能。 |
| [^80] | [RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture Search.](http://arxiv.org/abs/2305.04206) | 本论文提出了一种被称为RATs-NAS的神经结构搜索方法，通过在GCN上重定向相邻操作轨迹来快速搜索最佳神经网络架构，实验结果表明这种方法比其他最先进方法更有效。 |
| [^81] | [Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics.](http://arxiv.org/abs/2305.04152) | 本文提出了无线 FALD 协议，可以在无噪声通信的情况下高效地在无线系统中实现分布式贝叶斯学习，实现了在通信回合之间多个本地更新以及由小批量计算的随机梯度，并进行了样本收敛分析。 |
| [^82] | [Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.](http://arxiv.org/abs/2305.04111) | 本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。 |
| [^83] | [Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion.](http://arxiv.org/abs/2305.03509) | Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。 |
| [^84] | [A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects.](http://arxiv.org/abs/2305.02750) | 本综述全面概述了不同类型对话中对话代理主动性的突出问题和先进设计，讨论了符合实际应用需求但需要未来更大研究重点的挑战，激发更多的会话 AI 进展到下一级别。 |
| [^85] | [Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders.](http://arxiv.org/abs/2305.02640) | 本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。 |
| [^86] | [FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information.](http://arxiv.org/abs/2305.01528) | 本研究介绍了一份包含真实游戏状态信息的Dungeons & Dragons实际游戏数据集FIREBALL，它可以改善自然语言生成的质量。此外，LLMs可以使用FIREBALL中的游戏状态信息来生成更高质量的游戏回合。 |
| [^87] | [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models.](http://arxiv.org/abs/2305.01219) | 本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。 |
| [^88] | [CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations.](http://arxiv.org/abs/2305.01118) | CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。 |
| [^89] | [Ternary Instantaneous Noise-based Logic.](http://arxiv.org/abs/2305.00984) | 本文提出了一种三元逻辑系统，其中包含不确定的比特值和不存在的比特（真空态），与标准的二元逻辑系统相比有着显著的优势，且可以在不改变二进制算法的情况下使用。 |
| [^90] | [Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation.](http://arxiv.org/abs/2305.00909) | 提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。 |
| [^91] | [Calibration Error Estimation Using Fuzzy Binning.](http://arxiv.org/abs/2305.00543) | 本文提出了一种模糊校准误差度量（FCE），利用模糊分箱方法计算校准误差，从而缓解了概率偏斜的影响并提供了更紧密的估计值。与传统指标ECE相比，FCE在多类设置中表现更好，https://github.com/srdgFHE/FCE-paper。 |
| [^92] | [Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems.](http://arxiv.org/abs/2304.14712) | 本文提出了一个真实世界装箱问题的基准数据集和实例生成器，可以用来比较不同装箱算法的性能。 |
| [^93] | [On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective.](http://arxiv.org/abs/2304.13836) | 本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。 |
| [^94] | [Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network.](http://arxiv.org/abs/2304.11954) | 提出了一种适用于硬件的SNN脉冲驱动残差学习结构，基于该结构开发了一个纯Transformer的脉冲神经网络Spikingformer，用于避免非脉冲计算并在多个数据集上实现了优异的性能表现。 |
| [^95] | [BloombergGPT: A Large Language Model for Finance.](http://arxiv.org/abs/2303.17564) | 本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。 |
| [^96] | [Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks.](http://arxiv.org/abs/2303.17523) | 本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。 |
| [^97] | [The logic behind desirable sets of things, and its filter representation.](http://arxiv.org/abs/2302.08176) | 本研究确定了合理物品集合的基本逻辑，并提出了一些简单表述方法。 |
| [^98] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^99] | [Symbolic Discovery of Optimization Algorithms.](http://arxiv.org/abs/2302.06675) | 该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。 |
| [^100] | [Creating a Large Language Model of a Philosopher.](http://arxiv.org/abs/2302.01339) | 该研究使用OpenAI的大型语言模型GPT-3和哲学家丹尼特的作品为训练数据，探索了生成哲学文本的能力。研究人员通过招募大量参与者来区分真正的哲学家丹尼特和机器生成的文字。专家成功率达到51％，但没有达到预期的80％，该模型有可能超越人类的思维能力。 |
| [^101] | [Even if Explanations: Prior Work, Desiderata & Benchmarks for Semi-Factual XAI.](http://arxiv.org/abs/2301.11970) | 本文总结了反事实解释在AI系统中的应用，其中特别关注半事实解释，提出了半事实可解释人工智能的期望，并对历史算法进行了基准测试，为未来算法的发展提供了坚实的基础。 |
| [^102] | [The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks.](http://arxiv.org/abs/2301.07068) | 本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。 |
| [^103] | [Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features.](http://arxiv.org/abs/2212.13881) | 本文确定并表征了深度全连接神经网络学习特征的机制，提出了深度神经特征假设并解释了深度学习现象，同时也引领了对递归学习特征的核方法中特征学习的更广泛的理解。 |
| [^104] | [Policy Optimization over General State and Action Spaces.](http://arxiv.org/abs/2211.16715) | 本文提出了一种新方法并引入了函数近似来解决通用状态和动作空间上的强化学习问题，同时介绍了一种新的策略双平均法。 |
| [^105] | [A Simple, Yet Effective Approach to Finding Biases in Code Generation.](http://arxiv.org/abs/2211.00609) | 本文介绍了一种简单而有效的方法，能够检测代码生成系统中的偏差，通过对编码挑战进行模块化分解和分析，并通过自动化干预机制来暴露不良偏差，最终作为缓解策略进行数据转换技术的微调。 |
| [^106] | [Weakly Supervised Learning for Analyzing Political Campaigns on Facebook.](http://arxiv.org/abs/2210.10669) | 本研究提出了一种基于弱监督学习的方法，通过分析Facebook上政治广告的立场和议题，以及其使用人口统计学定位，来了解政治活动的特点和时间动态。 |
| [^107] | [On the Impossible Safety of Large AI Models.](http://arxiv.org/abs/2209.15259) | 本文探讨了大型人工智能模型的安全问题，并指出了构建既精确又安全的模型的基本不可能性，并提供了统计学下限证明。 |
| [^108] | [Generating Formal Safety Assurances for High-Dimensional Reachability.](http://arxiv.org/abs/2209.12336) | 提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。 |
| [^109] | [EDeNN: Event Decay Neural Networks for low latency vision.](http://arxiv.org/abs/2209.04362) | EDeNN是一种新型的神经网络，更接近原始事件数据流，避免了积累事件到图像帧的过程，展现了在角速度回归和竞争光流估计方面最先进的性能。 |
| [^110] | [Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation.](http://arxiv.org/abs/2209.03355) | 该论文提出了一种高效的基于生成对抗网络的实时超分辨率模型EdgeSRGAN，使用模型量化提高了CPU和Edge TPU设备上的执行效率，并通过知识蒸馏技术进一步优化模型，保留了较好的图像质量。 |
| [^111] | [On Reality and the Limits of Language Data: Aligning LLMs with Human Norms.](http://arxiv.org/abs/2208.11981) | 本文研究了大型语言模型（LLMs）在仅使用语言数据的情况下理解物理世界的能力，使用新颖且严密控制的推理测试（ART）与人类规范进行对比，研究发现了LLMs在某些常识关系模型中可以直接从数据中学习，但存在弱点，例如在部分和包含关系方面表现不足。 |
| [^112] | [Differentiable Inductive Logic Programming in High-Dimensional Space.](http://arxiv.org/abs/2208.06652) | 本研究提出了一种在高维空间中进行可微归纳逻辑编程的扩展方法，通过大规模谓词发明来充分利用高维梯度下降的效能，以学习超出现有神经符号ILP系统能力的任务的解决方案。 |
| [^113] | [Towards the Practical Utility of Federated Learning in the Medical Domain.](http://arxiv.org/abs/2207.03075) | 本研究提出了应用联邦学习于医学领域的实用指南，包括三个具有代表性的医学数据集的实验，旨在提高医保业的数据效率，并形成适用于全行业的标准。 |
| [^114] | [Satellite-based high-resolution maps of cocoa planted area for C\^ote d'Ivoire and Ghana.](http://arxiv.org/abs/2206.06119) | 本研究使用深度学习框架结合可可种植数据和卫星图像，为科特迪瓦和加纳创建了高分辨率的可可种植区地图。研究发现，可可种植是科特迪瓦和加纳保护区森林损失的基本驱动因素，而官方报告的可可种植面积明显低估。 |
| [^115] | [HierarchyNet: Learning to Summarize Source Code with Heterogeneous Representations.](http://arxiv.org/abs/2205.15479) | HierarchyNet是一种优秀的代码摘要方法，它基于异构代码表示和层次感知交叉注意层，可以有效地捕获词法、语法和语义层面上的代码要素。 |
| [^116] | [Combating Client Dropout in Federated Learning via Friend Model Substitution.](http://arxiv.org/abs/2205.13222) | 本研究提出了一种新算法FL-FDMS，可以在客户端退役的情况下，即时找到数据分布相似的客户端，并使用这些客户端的本地模型更新来替代缺失的客户端的更新，实现更高的学习准确性和更低的通信成本。 |
| [^117] | [Fairness in Recommender Systems: Research Landscape and Future Directions.](http://arxiv.org/abs/2205.11127) | 本文讨论了推荐系统的公平性问题，通过对160多篇学术出版物的综述总结了该领域目前的研究现状，强调了一些有前途的未来方向，例如需要超越统计平衡的新公平性衡量方法。 |
| [^118] | [Efficient Feedback and Partial Credit Grading for Proof Blocks Problems.](http://arxiv.org/abs/2204.04196) | 本文提出了一个算法，可以高效地计算学生提交作业与预定义解决方案之间的编辑距离，可应用于证明、编程与自然语言处理等领域。 |
| [^119] | [A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation.](http://arxiv.org/abs/2202.08146) | 该论文提出了一种基于Wi-Fi信道数据的人与人互动识别的新方法，使用注意力双向门控循环神经网络实现高精度的实时处理，准确率达到98.22%，并提供了一个GUI应用程序方便实时应用。 |
| [^120] | [Proportional Fairness in Federated Learning.](http://arxiv.org/abs/2202.01666) | 本文提出了用于保证联邦学习中公平性的比例公平性 (PF) 概念，并提出了一种新颖易于实现的算法 PropFair，能够在所有客户端平均性能和最差 10% 客户端平均性能之间达到良好平衡。 |
| [^121] | [Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness.](http://arxiv.org/abs/2202.01208) | 本文提出了一种基于深度学习的超声速度重构方法，通过使用层析图像生成多样化的培训数据以提高稳定性和健壮性。该方法在高分辨率声速地图重构方面的表现优于传统的简化几何方法。 |
| [^122] | [BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering.](http://arxiv.org/abs/2112.05504) | 本文介绍了一种名为BungeeNeRF的渐进式神经辐射场，它可以在极端不同的尺度上实现逐层渲染，经过多个阶段逐步细化粗略的几何形状。该方法可以处理各种规模，并在所有规模上产生具有精细细节的高质量结果，广泛应用于城市景观、景观和Minecraft模型等场景。 |
| [^123] | [Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning.](http://arxiv.org/abs/2108.05660) | 本研究提出了一种风险低且高精度的堆叠集成机器学习模型，可以从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。 |

# 详细

[^1]: 存在对称性和状态抽象的政策梯度方法

    Policy Gradient Methods in the Presence of Symmetries and State Abstractions. (arXiv:2305.05666v1 [cs.LG])

    [http://arxiv.org/abs/2305.05666](http://arxiv.org/abs/2305.05666)

    本文研究了在连续控制环境中的抽象，提出了一种策略梯度定理，允许利用环境的近似对称性进行策略优化，并提出了一系列演员-评论家算法进行策略和MDP同态映射的学习，最后展示了算法在连续对称性环境和视觉控制任务中的有效性。

    

    针对高维度和复杂问题，强化学习依靠抽象来提高效率和泛化性能。本文研究了在连续控制环境中的抽象，并将MDP同态的定义扩展到连续状态和动作空间的情况。我们针对抽象MDP的随机和确定性策略导出了一种策略梯度定理。我们的策略梯度结果允许利用环境的近似对称性进行策略优化。基于这些定理，我们提出了一系列演员-评论家算法，这些算法能够同时学习策略和MDP同态映射，使用松散双仿射度量。最后，我们引入了一系列具有连续对称性的环境，以进一步展示我们的算法在存在这些对称性的情况下进行动作抽象的能力。我们在这些环境以及具有挑战性的视觉控制任务中展示了我们方法的有效性。

    Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual contro
    
[^2]: ImageBind:一个共同嵌入空间绑定所有模态的方法

    ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])

    [http://arxiv.org/abs/2305.05665](http://arxiv.org/abs/2305.05665)

    ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。

    

    我们提出了ImageBind，这是一种跨越图像、文本、音频、深度、热传感和IMU数据的六种不同模态的联合嵌入方法。我们展示，不需要训练所有配对数据，只需要图像配对数据就足以将这些模态绑定在一起。ImageBind可以利用最近的大规模视觉-语言模型，并通过使用它们与图像的自然配对，将它们的零样本能力扩展到新的模态。它可以实现“开箱即用”的新型应用程序，包括跨模态检索、用算术组合模态、跨模态检测和生成。新型应用随着图像编码器的强度而不断改进，我们在跨模态的零样本识别任务上取得了新的最优成绩，超过了专家监督模型。最后，我们还展示了强的几何识别结果，超过了以前的工作，ImageBind成为了评估视觉模态联合学习的一种新方法。

    We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
    
[^3]: ShapeCoder：从非结构化的基元中发现视觉程序的抽象

    ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives. (arXiv:2305.05661v1 [cs.GR])

    [http://arxiv.org/abs/2305.05661](http://arxiv.org/abs/2305.05661)

    ShapeCoder是能够从非结构化的基元中发现视觉程序的抽象的第一个系统，用于重写程序以使其更紧凑，暴露自由度更少。在对合成和现实世界数据集的实验中，ShapeCoder优于现有基线，程序大小减少了3倍。

    

    程序是一种越来越受欢迎的可视化数据表示方式，可以揭示紧凑、可解释的结构，支持操作。视觉程序通常以特定领域语言(DSLs)编写。找到“好”的程序，即仅暴露有意义的自由度，需要访问具有“好”函数库的DSLs，这些函数库通常由领域专家编写。我们提出了ShapeCoder，这是第一个能够接受形状数据集的系统，该数据集使用非结构化基元表示，并共同发现(i)有用的抽象函数和(ii)使用这些抽象来解释输入形状的程序。发现的抽象捕获数据集中的常见模式(结构和参数)，因此，使用这些抽象重写的程序更紧凑，暴露的自由度更少。ShapeCoder改进了以前的抽象发现方法，在更复杂的输入下，发现更好的抽象，在较少的先验知识下产生更好的结果。我们的实验表明，ShapeCoder在合成和现实世界数据集上优于现有基线，程序大小减少了3倍。

    Programs are an increasingly popular representation for visual data, exposing compact, interpretable structure that supports manipulation. Visual programs are usually written in domain-specific languages (DSLs). Finding "good" programs, that only expose meaningful degrees of freedom, requires access to a DSL with a "good" library of functions, both of which are typically authored by domain experts. We present ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across the dataset, so that programs rewritten with these abstractions are more compact, and expose fewer degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stri
    
[^4]: TidyBot: 应用大语言模型的个性化机器人物理辅助

    TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])

    [http://arxiv.org/abs/2305.05658](http://arxiv.org/abs/2305.05658)

    本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。

    

    为了使机器人能够有效个性化地提供物理辅助，它必须学习用户的个人喜好并将其应用于未来的场景中。本文研究了使用机器人进行家庭清扫的个性化问题，这些机器人能够通过捡起物品并将其放回原处来整理房间。一个关键的挑战是确定每个物品的正确位置，因为人们的喜好可以因个人品味或文化背景而大不相同。例如，一个人可能喜欢把衬衫放在抽屉里，而另一个人可能喜欢把衬衫放在架子上。我们旨在建立系统，这些系统可以通过与特定人的先前交互学习这样的喜好，而只需要几个示例。我们展示了机器人可以将基于语言的规划和感知与大型语言模型(LLMs)的少样本摘要能力相结合，从而推断出广泛适用于未来交互的用户偏好。这种方法实现了快速适应，并取得了91.2%的准确率。

    For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
    
[^5]: AI 是否可能成为大过滤器？天体生物学能够告诉情报界关于人类起源风险的东西

    Could AI be the Great Filter? What Astrobiology can Teach the Intelligence Community about Anthropogenic Risks. (arXiv:2305.05653v1 [cs.CY])

    [http://arxiv.org/abs/2305.05653](http://arxiv.org/abs/2305.05653)

    本文探讨了人工智能对大过滤器假设的可能性以及如何在天体生物学的背景下理解全球灾难性风险，并指出了情报界通过认识到AI在大规模全球风险中的潜在作用获得有价值洞察的必要性。

    

    “人在哪里？”这个问题涵盖了费米悖论中的不安，即如果宇宙中存在概率较高的外星生命，则为什么我们还没有遇到它？这个谜团已经困扰学者数十年，提出了许多假说，既包括自然的也包括社会学的解释。其中一个有趣的假设被称为大过滤器，它建议生命进化所需的某些事件极不可能发生，因此宇宙保持着沉默。这个和它逻辑等价的假设应该使我们停下来思考——某些灾难性事件很可能会发生，从而阻止了生命在宇宙中的扩张。这可能是一种自然事件，或更令人不安的是，是智慧生物为自己引发的导致自己灭绝的事件。从情报角度来看，在天体生物学的背景下构建全球灾难性风险（特别是人类起源风险）有助于我们更好地理解宇宙中智慧生命可能的稀有性以及可能存在的种类。在本文中，我们探讨人工智能（AI）的发展如何揭示大过滤器可能在我们前面的重要提示。我们认为，通过认识到 AI 在大规模全球风险中的潜在作用，情报界可以获得有价值的洞察力，减少存在风险。

    Where is everybody? This phrase distills the foreboding of what has come to be known as the Fermi Paradox - the disquieting idea that, if extraterrestrial life is probable in the Universe, then why have we not encountered it? This conundrum has puzzled scholars for decades, and many hypotheses have been proposed suggesting both naturalistic and sociological explanations. One intriguing hypothesis is known as the Great Filter, which suggests that some event required for the emergence of intelligent life is extremely unlikely, hence the cosmic silence. A logically equivalent version of this hypothesis and one that should give us pause - suggests that some catastrophic event is likely to occur that prevents life's expansion throughout the cosmos. This could be a naturally occurring event, or more disconcertingly, something that intelligent beings do to themselves that leads to their own extinction. From an intelligence perspective, framing global catastrophic risk (particularly risks of
    
[^6]: 利用光电容积描记术和深度学习预测心血管疾病风险

    Predicting Cardiovascular Disease Risk using Photoplethysmography and Deep Learning. (arXiv:2305.05648v1 [cs.CV])

    [http://arxiv.org/abs/2305.05648](http://arxiv.org/abs/2305.05648)

    利用光电容积描记术和深度学习预测心血管疾病风险，可在低成本下对低收入和中等收入国家进行大规模筛查，并超越了传统风险评分工具。

    

    心血管疾病(CVDs)是低收入和中等收入国家早期死亡率的主要原因。在这些人群中，早期 CVD 检测和干预至关重要，然而许多现有的 CVD 风险评分需要身体检查或实验室检测，这在这样的健康系统中会面临挑战，因为受限的可及性。在这里，我们调查了使用光电容积描记术(PPG)的潜力，这是一种在大多数智能手机上都可用的传感技术，可以在低成本下实现大规模筛查，用于 CVD 风险预测。我们开发了一种基于深度学习的 PPG-CVD 风险评分(DLS)，仅使用年龄、性别、吸烟状态和 PPG 作为预测因子，预测未来 10 年发生重要的不良心血管事件(MACE：非致命性心肌梗塞，中风和心血管死亡) 的概率。我们与办公室制定的 refit-WHO 评分进行了比较，该评分采用了 WHO 和 Globorisk 评分的共享预测因子（年龄，性别，吸烟状态，收缩压和总胆固醇），并使用两个独立的前瞻性队列。我们的结果表明，在两个队列中，DLS 在预测 MACE 方面表现优于 refit-WHO 评分，分别为0.76和0.80，而办公室制定的 refit-WHO 评分为0.66和0.70。我们的研究结果提示，基于 PPG 的 DLS 具有潜力帮助在低收入和中等收入国家早期低成本识别 CVD 高风险个体。

    Cardiovascular diseases (CVDs) are responsible for a large proportion of premature deaths in low- and middle-income countries. Early CVD detection and intervention is critical in these populations, yet many existing CVD risk scores require a physical examination or lab measurements, which can be challenging in such health systems due to limited accessibility. Here we investigated the potential to use photoplethysmography (PPG), a sensing technology available on most smartphones that can potentially enable large-scale screening at low cost, for CVD risk prediction. We developed a deep learning PPG-based CVD risk score (DLS) to predict the probability of having major adverse cardiovascular events (MACE: non-fatal myocardial infarction, stroke, and cardiovascular death) within ten years, given only age, sex, smoking status and PPG as predictors. We compared the DLS with the office-based refit-WHO score, which adopts the shared predictors from WHO and Globorisk scores (age, sex, smoking st
    
[^7]: 面向个人或实体的知识图谱表示学习：在医疗保健领域应用的研究

    Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])

    [http://arxiv.org/abs/2305.05640](http://arxiv.org/abs/2305.05640)

    本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。

    

    知识图谱是一种按本体或模式组织信息的流行方式，已经在从搜索到推荐的各种场景中得到了应用。尽管在知识图谱方面有了一些进展，但知识表示仍然是跨行业的一个非常棘手的任务，特别是在生物医学和医疗保健领域，由于实体之间的复杂相互关系、异质性、缺乏标准化和数据稀疏性等因素，这一任务尤其具有挑战性。本文提出了一种面向医疗保健领域构建面向实体的知识图谱的端到端表示学习方法，重点是捕捉生物医学领域的独特特征。所提出的框架名为HEER（Healthcare Entity-Entity Representation learning），将领域特定的约束和特征纳入到图嵌入算法中。对多个基准数据集的结果表明，与最先进的方法相比，HEER在改善下游预测任务方面具有有效性。

    Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
    
[^8]: 双语类比比例

    Bilingual analogical proportions. (arXiv:2305.05614v1 [cs.LO])

    [http://arxiv.org/abs/2305.05614](http://arxiv.org/abs/2305.05614)

    本文在通用代数和一阶逻辑的一般设定下，推广了抽象的类比比例代数逻辑框架，实现了从单语言到双语言框架的转变，扩展了基础框架的适用性。

    

    类比比例是“$a$到$b$就如同$c$到$d$”这种表达式，它处于类比推理的核心，后者又处于人工智能和人类智能的核心。作者最近基于通用代数和一阶逻辑的一般设定，从第一原理开始介绍了一种抽象的类比比例代数逻辑框架。在该框架中，源代数和目标代数具有相同的基础语言。本文的目的是将其单语言框架推广到双语言框架，其中基础语言可能不同。通过在比例证明中使用限定语，我们实现了这一目的。其结果是一个重大的推广，广泛地扩展了基础框架的适用性。从更广泛的意义上说，本文是迈向类比推理数学理论的进一步步骤。

    Analogical proportions are expressions of the form ``$a$ is to $b$ what $c$ is to $d$'' at the core of analogical reasoning which itself is at the core of human and artificial intelligence. The author has recently introduced {\em from first principles} an abstract algebro-logical framework of analogical proportions within the general setting of universal algebra and first-order logic. In that framework, the source and target algebras have the {\em same} underlying language. The purpose of this paper is to generalize his unilingual framework to a bilingual one where the underlying languages may differ. This is achieved by using hedges in justifications of proportions. The outcome is a major generalization vastly extending the applicability of the underlying framework. In a broader sense, this paper is a further step towards a mathematical theory of analogical reasoning.
    
[^9]: “Alexa并不那么有感情”: 儿童通过与智能音箱的交互来理解人工智能

    "Alexa doesn't have that many feelings": Children's understanding of AI through interactions with smart speakers in their homes. (arXiv:2305.05597v1 [cs.HC])

    [http://arxiv.org/abs/2305.05597](http://arxiv.org/abs/2305.05597)

    研究显示，大多数6-11岁的儿童高估了语音对话助手的智能，对它们的情感或代理感到不确定，并缺乏对数据隐私和安全方面的准确理解。

    

    随着语音对话助手（CAs），包括Alexa、Siri和Google Home等的普及，在家庭中，许多儿童现在经常与人工智能系统互动。研究儿童与使用AI技术的消费设备的经验很重要，因为这些经验会塑造他们对AI及其能力的理解。我们在苏格兰对6-11岁的小学生进行了混合方法研究（问卷和访谈），以确定儿童对语音对话助手的工作方式、认知能力、代理和其他类似人类的特质、在使用语音对话助手时隐私方面的意识和信任以及他们认为与语音对话助手的适当口头交互。大多数儿童高估了语音对话助手的智能水平，对系统的情感或代理感到不确定。此外，他们对数据隐私和安全方面缺乏准确的理解，并认为向对话助手表现出不礼貌是不对的。

    As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google Home, have become commonly embedded in households, many children now routinely interact with Artificial Intelligence (AI) systems. It is important to research children's experiences with consumer devices which use AI techniques because these shape their understanding of AI and its capabilities. We conducted a mixed-methods study (questionnaires and interviews) with primary-school children aged 6-11 in Scotland to establish children's understanding of how voice-based CAs work, how they perceive their cognitive abilities, agency and other human-like qualities, their awareness and trust of privacy aspects when using CAs and what they perceive as appropriate verbal interactions with CAs. Most children overestimated the CAs' intelligence and were uncertain about the systems' feelings or agency. They also lacked accurate understanding of data privacy and security aspects, and believed it was wrong to be rude to con
    
[^10]: PET-NeuS：神经表面的位置编码三平面

    PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces. (arXiv:2305.05594v1 [cs.CV])

    [http://arxiv.org/abs/2305.05594](http://arxiv.org/abs/2305.05594)

    PET-NeuS是一个神经表面重建方法，引入了三平面表示法、新型的位置编码和可学习的卷积操作，并能实现高质量的表面重建。

    

    MLP参数化的符号距离函数是神经表面重建的常见组成部分。我们在成功的最新方法NeuS的基础上，通过三个新组件进行扩展。第一个组件是从EG3D中借用三平面表示法，将符号距离场表示为三平面和MLP的混合，而不是仅使用MLP来表示。使用三平面会导致更具表现力的数据结构，但也会引入重建表面中的噪声。第二个组件是使用一种新型的位置编码与可学习的权重来对抗重建过程中的噪声。我们将三平面中的特征分成多个频率尺度，并使用不同频率的正弦和余弦函数调制它们。第三个组件是在三平面特征上使用可学习的卷积操作，使用自注意力卷积产生具有不同频带的特征。实验表明，PET-NeuS比NeuS和其他现有的神经表面重建方法实现了更高质量的表面重建。

    A signed distance function (SDF) parametrized by an MLP is a common ingredient of neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Using tri-planes leads to a more expressive data structure but will also introduce noise in the reconstructed surface. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency scales and modulate them with sin and cos functions of different frequencies. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency bands. The experiments show that PET-NeuS achieves h
    
[^11]: RLocator: 利用强化学习进行Bug定位

    RLocator: Reinforcement Learning for Bug Localization. (arXiv:2305.05586v1 [cs.SE])

    [http://arxiv.org/abs/2305.05586](http://arxiv.org/abs/2305.05586)

    本文提出了一种基于强化学习的Bug定位方法RLocator，相较于其他最先进的Bug定位技术具有更优越的性能。

    

    软件开发者在他们的项目中花费了大量的时间来修复Bugs。为了简化这个过程，提出了Bug定位方法来确定哪些源代码文件可能是负责特定Bug的源头。之前的工作提出了几种基于相似性的机器学习技术，用于Bug定位。尽管这些技术取得了显著进展，但它们并没有直接优化评估指标。相反，在训练和测试阶段使用了不同的度量标准，这会对检索任务的模型性能产生负面影响。在本文中，我们提出了一种基于强化学习的Bug定位方法RLocator。我们使用马尔可夫决策过程（MDP）来优化评估指标，从而对Bug定位问题进行公式化。我们提出了该技术，并基于六种高度流行的Apache项目的8,316个Bug报告的基准数据集进行了实验评估。我们的评估表明，RLocator相较于其他最先进的Bug定位技术具有更优越的性能。

    Software developers spend a significant portion of time fixing bugs in their projects. To streamline this process, bug localization approaches have been proposed to identify the source code files that are likely responsible for a particular bug. Prior work proposed several similarity-based machine-learning techniques for bug localization. Despite significant advances in these techniques, they do not directly optimize the evaluation measures. Instead, they use different metrics in the training and testing phases, which can negatively impact the model performance in retrieval tasks. In this paper, we propose RLocator, a Reinforcement Learning-based (RL) bug localization approach. We formulate the bug localization problem using a Markov Decision Process (MDP) to optimize the evaluation measures directly. We present the technique and experimentally evaluate it based on a benchmark dataset of 8,316 bug reports from six highly popular Apache projects. Our evaluation shows that RLocator achie
    
[^12]: SMAClite: 多智能体强化学习的轻量级环境

    SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning. (arXiv:2305.05566v1 [cs.LG])

    [http://arxiv.org/abs/2305.05566](http://arxiv.org/abs/2305.05566)

    本论文介绍了SMAClite，一个轻量级的多智能体强化学习环境，解耦了原有的闭源游戏并提供了开源框架，SMAClite在运行时速度和内存方面超越了SMAC。

    

    目前缺少适用于多智能体强化学习算法的标准基准。Starcraft多智能体挑战（SMAC）已经在多智能体强化学习研究中广泛使用，但构建在重型的闭源计算机游戏StarCraft II之上。因此，SMAC的计算成本很高，需要具备关于游戏的特殊知识和使用专有工具才能对环境进行任何有意义的修改或贡献。我们介绍了SMAClite - 一个基于SMAC的挑战，不仅与Starcraft II解耦，而且是开源的，同时提供了一个框架，使得可以创建新的SMAClite内容而无需任何特殊知识。我们进行了实验，证明了通过在SMAClite上训练多智能体强化学习算法，可以复现SMAC的结果。然后我们证明，SMAClite在运行时速度和内存方面超越SMAC。

    There is a lack of standard benchmarks for Multi-Agent Reinforcement Learning (MARL) algorithms. The Starcraft Multi-Agent Challenge (SMAC) has been widely used in MARL research, but is built on top of a heavy, closed-source computer game, StarCraft II. Thus, SMAC is computationally expensive and requires knowledge and the use of proprietary tools specific to the game for any meaningful alteration or contribution to the environment. We introduce SMAClite -- a challenge based on SMAC that is both decoupled from Starcraft II and open-source, along with a framework which makes it possible to create new content for SMAClite without any special knowledge. We conduct experiments to show that SMAClite is equivalent to SMAC, by training MARL algorithms on SMAClite and reproducing SMAC results. We then show that SMAClite outperforms SMAC in both runtime speed and memory.
    
[^13]: SkelEx和BoundEx：ReLU神经网络的自然可视化

    SkelEx and BoundEx: Natural Visualization of ReLU Neural Networks. (arXiv:2305.05562v1 [cs.LG])

    [http://arxiv.org/abs/2305.05562](http://arxiv.org/abs/2305.05562)

    SkelEx和BoundEx是第一批自然可视化的方法，可以提取ReLU神经网络的关键点和决策边界。这两种方法为在低维数据上训练的ReLU NN引入了非常自然的可视化工具。

    

    尽管权重和偏差的可解释性有限，但仍然是编码ReLU神经网络学习函数最流行的方式。这就是为什么我们引入SkelEx算法，以提取ReLU NN学习的成员函数的骨架，使这些函数更易于解释和分析。据我们所知，这是第一个从关键点的角度考虑线性区域的工作。作为自然的后续工作，我们还介绍了BoundEx，这是我们所知道的第一个从ReLU NN的实现中提取决策边界的分析方法。这两种方法都为在低维数据上训练的ReLU NN引入了非常自然的可视化工具。

    Despite their limited interpretability, weights and biases are still the most popular encoding of the functions learned by ReLU Neural Networks (ReLU NNs). That is why we introduce SkelEx, an algorithm to extract a skeleton of the membership functions learned by ReLU NNs, making those functions easier to interpret and analyze. To the best of our knowledge, this is the first work that considers linear regions from the perspective of critical points. As a natural follow-up, we also introduce BoundEx, which is the first analytical method known to us to extract the decision boundary from the realization of a ReLU NN. Both of those methods introduce very natural visualization tool for ReLU NNs trained on low-dimensional data.
    
[^14]: 分布式多目标决策制定

    Distributional Multi-Objective Decision Making. (arXiv:2305.05560v1 [cs.AI])

    [http://arxiv.org/abs/2305.05560](http://arxiv.org/abs/2305.05560)

    该论文介绍了一种分布式多目标决策制定的方法，其中引入了分布式不支配集和凸分布不支配集的概念，证明了它们可以包含所有最优解，通过实验验证了方法的有效性。

    

    在具有冲突目标的场景中进行有效的决策支持，可以向决策者呈现一组可能最优解。我们探讨这些解应该包含哪些策略以及如何高效地计算这些解。基于这一目标，我们采用分布式方法，引入一种新颖的优势准则，直接关联策略的回报分布。基于这个准则，我们提出了分布式不支配集，并证明其中包含了帕累托前沿的忽略的最优策略。此外，我们提出了凸分布不支配集，并证明它包括所有在多维风险规避决策者中最大化预期效用的策略。我们设计了一种新算法来学习分布式不支配集，并贡献了剪枝算子来将其减少到凸分布不支配集。通过实验，我们证明了这些方法的可行性和有效性。

    For effective decision support in scenarios with conflicting objectives, sets of potentially optimal solutions can be presented to the decision maker. We explore both what policies these sets should contain and how such sets can be computed efficiently. With this in mind, we take a distributional approach and introduce a novel dominance criterion relating return distributions of policies directly. Based on this criterion, we present the distributional undominated set and show that it contains optimal policies otherwise ignored by the Pareto front. In addition, we propose the convex distributional undominated set and prove that it comprises all policies that maximise expected utility for multivariate risk-averse decision makers. We propose a novel algorithm to learn the distributional undominated set and further contribute pruning operators to reduce the set to the convex distributional undominated set. Through experiments, we demonstrate the feasibility and effectiveness of these metho
    
[^15]: 社会价值取向与多智能体系统中的积分情感

    Social Value Orientation and Integral Emotions in Multi-Agent Systems. (arXiv:2305.05549v1 [cs.MA])

    [http://arxiv.org/abs/2305.05549](http://arxiv.org/abs/2305.05549)

    本文研究了社会价值取向(SVO)和积分情感在多智能体系统中的相互作用，开发了一种基于SVO和积分情感政策的智能体设计方法，并在仿真实验中探究了不同的社会偏好和情感对智能体的影响。

    

    个体的社会行为受社会偏好的个体差异的影响。社会价值取向（SVO）是一种可测量的人格特征，表明个体在做出决策时，对自己和他人的福利相对重视的程度。SVO和其他个体差异变量是人类行为和社会结果的强有力预测因素。然而，与个体差异独立的短暂情绪变化也会影响人类行为。积分情感是对决策情境直接反应而产生的情感，已经与决策取向的临时转变相关联。在这项研究中，我们研究了积分情感对多智能体社会中调节社会偏好的影响。我们开发了Svoie，一种基于已建立的SVO政策和任务结果的可替代积分情感政策做出决策的智能体设计方法。我们进行了仿真实验，研究了不同的社会值取向和情感调节对智能体内部和整体行为的影响。

    Human social behavior is influenced by individual differences in social preferences. Social value orientation (SVO) is a measurable personality trait which indicates the relative importance an individual places on their own and on others' welfare when making decisions. SVO and other individual difference variables are strong predictors of human behavior and social outcomes. However, there are transient changes human behavior associated with emotions that are not captured by individual differences alone. Integral emotions, the emotions which arise in direct response to a decision-making scenario, have been linked to temporary shifts in decision-making preferences.  In this work, we investigated the effects of moderating social preferences with integral emotions in multi-agent societies. We developed Svoie, a method for designing agents which make decisions based on established SVO policies, as well as alternative integral emotion policies in response to task outcomes. We conducted simul
    
[^16]: Walk4Me: 远程康复社区行动能力评估——一个早期诊断和疾病进展的自动化系统

    Walk4Me: Telehealth Community Mobility Assessment, An Automated System for Early Diagnosis and Disease Progression. (arXiv:2305.05543v1 [eess.SP])

    [http://arxiv.org/abs/2305.05543](http://arxiv.org/abs/2305.05543)

    Walk4Me是一个远程康复社区行动能力评估系统，采用基于人工智能的步态特征检测来识别疾病患者的步态障碍并追踪疾病的进展。

    

    我们介绍了Walk4Me，这是一个远程康复社区行动能力评估系统，旨在促进早期诊断、严重性和进展识别。我们通过以下三个方法实现了这一目标：1）促进早期诊断；2）识别临床严重性的早期指标；3）量化和跟踪疾病在步行期的进展。为此，我们采用基于人工智能（AI）的步态特征检测来检测患者和通常发育的同龄人的行走方式。我们的系统通过我们的新型Walk4Me API从设备传感器（例如移动设备的加速度等）远程实时收集数据。我们的Web应用程序提取时间/空间步态特征和原始数据信号特征，然后采用传统机器学习和深度学习技术来识别模式，以便实现：1）识别与疾病相关的步态障碍的患者；2）描述活动能力限制的程度；3）确定疾病的进展。

    We introduce Walk4Me, a telehealth community mobility assessment system designed to facilitate early diagnosis, severity, and progression identification. Our system achieves this by 1) enabling early diagnosis, 2) identifying early indicators of clinical severity, and 3) quantifying and tracking the progression of the disease across the ambulatory phase of the disease. To accomplish this, we employ an Artificial Intelligence (AI)-based detection of gait characteristics in patients and typically developing peers. Our system remotely and in real-time collects data from device sensors (e.g., acceleration from a mobile device, etc.) using our novel Walk4Me API. Our web application extracts temporal/spatial gait characteristics and raw data signal characteristics and then employs traditional machine learning and deep learning techniques to identify patterns that can 1) identify patients with gait disturbances associated with disease, 2) describe the degree of mobility limitation, and 3) ide
    
[^17]: 多元设备网络中高效基于模式的异常检测

    Efficient pattern-based anomaly detection in a network of multivariate devices. (arXiv:2305.05538v1 [cs.SI])

    [http://arxiv.org/abs/2305.05538](http://arxiv.org/abs/2305.05538)

    该论文提出了一种高效的基于模式挖掘的异常检测方法，该方法不仅考虑多元时间序列，还考虑了实体间的通信关系，并提供可解释的解释。实验表明，该方法比现有方法更有效率和有效。

    

    许多组织管理服务质量并监视大量设备和服务器，其中每个实体都与遥测或物理传感器数据序列相关联。最近，提出了各种方法来检测行为异常，然而现有方法关注多元时间序列，并忽略实体间的通信。此外，我们旨在支持最终用户不仅在定位在某个时期导致异常的实体和传感器方面，而且解释这个决定。我们提出了一种可扩展的方法来使用两步方法检测异常。首先，我们恢复网络中实体之间的关系，因为关系通常是动态的，由未知的基础过程引起。接下来，我们基于顺序模式的嵌入报告异常。模式挖掘是高效的并支持解释，即模式代表时间序列中频繁发生的行为。我们扩展了模式挖掘以基于频率过滤顺序模式，并引入模式模板以将模式报告为文本，从而提供可解释的解释。我们在几个大规模场景上评估了我们的方法，表明我们的方法比现有方法更有效率和有效。

    Many organisations manage service quality and monitor a large set devices and servers where each entity is associated with telemetry or physical sensor data series. Recently, various methods have been proposed to detect behavioural anomalies, however existing approaches focus on multivariate time series and ignore communication between entities. Moreover, we aim to support end-users in not only in locating entities and sensors causing an anomaly at a certain period, but also explain this decision. We propose a scalable approach to detect anomalies using a two-step approach. First, we recover relations between entities in the network, since relations are often dynamic in nature and caused by an unknown underlying process. Next, we report anomalies based on an embedding of sequential patterns. Pattern mining is efficient and supports interpretation, i.e. patterns represent frequent occurring behaviour in time series. We extend pattern mining to filter sequential patterns based on frequen
    
[^18]: 基于卷积的方法集合用于振动信号的故障检测

    An ensemble of convolution-based methods for fault detection using vibration signals. (arXiv:2305.05532v1 [eess.SP])

    [http://arxiv.org/abs/2305.05532](http://arxiv.org/abs/2305.05532)

    本文提出了集成了三种基于卷积的方法，用于解决振动信号的故障检测问题。实验结果表明，该方法在准确率上优于其他方法，达到了超过98.8\%的准确率。

    

    本文研究了使用测试平台上行星齿轮箱振动信号的多元时间序列解决故障检测问题。对于多元时间序列分类问题，常见的机器学习和深度学习方法包括基于距离、基于功能数据、基于特征和基于卷积核的方法。最近的研究表明，使用ROCKET、ResNet和FCN等基于卷积核的方法对多元时间序列数据分类具有强大的性能。我们提出了三种基于卷积核的方法的集合，并通过优于其他方法并实现超过98.8\%准确率的实验结果，证明了其在解决故障检测问题中的有效性。

    This paper focuses on solving a fault detection problem using multivariate time series of vibration signals collected from planetary gearboxes in a test rig. Various traditional machine learning and deep learning methods have been proposed for multivariate time-series classification, including distance-based, functional data-oriented, feature-driven, and convolution kernel-based methods. Recent studies have shown using convolution kernel-based methods like ROCKET, and 1D convolutional neural networks with ResNet and FCN, have robust performance for multivariate time-series data classification. We propose an ensemble of three convolution kernel-based methods and show its efficacy on this fault detection problem by outperforming other approaches and achieving an accuracy of more than 98.8\%.
    
[^19]: 基于梯度的可解释AI技术在时间序列数据中的探索：以评估中风康复锻炼为例

    Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises. (arXiv:2305.05525v1 [cs.LG])

    [http://arxiv.org/abs/2305.05525](http://arxiv.org/abs/2305.05525)

    本文提出了一种基于梯度的可解释AI技术的方法，用于识别时间序列数据中的显著帧，在中风康复锻炼领域具有应用潜力。

    

    可解释AI技术被广泛应用于各种应用中，然而在医疗保健领域尤其是时间序列数据上的探索却有限。本文提出了一种基于弱监督模型和基于梯度的可解释AI技术的方法，用于识别在中风康复锻炼中涉及补偿动作的时间序列数据中的显著帧，并取得了不错的评估结果。

    Explainable artificial intelligence (AI) techniques are increasingly being explored to provide insights into why AI and machine learning (ML) models provide a certain outcome in various applications. However, there has been limited exploration of explainable AI techniques on time-series data, especially in the healthcare context. In this paper, we describe a threshold-based method that utilizes a weakly supervised model and a gradient-based explainable AI technique (i.e. saliency map) and explore its feasibility to identify salient frames of time-series data. Using the dataset from 15 post-stroke survivors performing three upper-limb exercises and labels on whether a compensatory motion is observed or not, we implemented a feed-forward neural network model and utilized gradients of each input on model outcomes to identify salient frames that involve compensatory motions. According to the evaluation using frame-level annotations, our approach achieved a recall of 0.96 and an F2-score of
    
[^20]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^21]: 基于优化和人工智能的高校教育质量量化方法，用于透明招聘：第一部分模型开发

    Optimization- and AI-based approaches to academic quality quantification for transparent academic recruitment: part 1-model development. (arXiv:2305.05460v1 [cs.AI])

    [http://arxiv.org/abs/2305.05460](http://arxiv.org/abs/2305.05460)

    该论文开发了两种计算框架来量化高校教育质量，通过一个单一指标（AQI）来综合衡量学者的学术品质。

    

    为了公正地招聘高校和研究机构的教育人员，根据全球公认的学术品质特征确定正确的衡量标准是一个十分微妙、富有挑战性但非常重要的问题。在这一连串的两篇论文中，我们考虑了第一篇论文中的学术品质量化建模部分，而在第二篇论文中考虑了案例研究部分。针对学术品质量化建模，我们开发了两个可用于构建决策支持工具的计算框架：(i) 基于优化的框架，以及 (ii) 基于孪生网络的框架（一种人工神经网络类型）。两个模型的输出都是一个称为学术品质量指数(Academic Quality Index, AQI)的单一指标，它是总体学术品质的度量标准。我们使用全球第一和平均非第一类大学的学者数据，根据《泰晤士高等教育世界大学排名》和 QS 世界大学排名进行假设。

    For fair academic recruitment at universities and research institutions, determination of the right measure based on globally accepted academic quality features is a highly delicate, challenging, but quite important problem to be addressed. In a series of two papers, we consider the modeling part for academic quality quantification in the first paper, in this paper, and the case studies part in the second paper. For academic quality quantification modeling, we develop two computational frameworks which can be used to construct a decision-support tool: (i) an optimization-based framework and (ii) a Siamese network (a type of artificial neural network)-based framework. The output of both models is a single index called Academic Quality Index (AQI) which is a measure of the overall academic quality. The data of academics from first-class and average-class world universities, based on Times Higher Education World University Rankings and QS World University Rankings, are assumed as the refe
    
[^22]: 一种跨频保护徽章：(完全)自主战争中医疗单位和受伤士兵的保护选项

    A Cross-Frequency Protective Emblem: Protective Options for Medical Units and Wounded Soldiers in the Context of (fully) Autonomous Warfare. (arXiv:2305.05459v1 [cs.NI])

    [http://arxiv.org/abs/2305.05459](http://arxiv.org/abs/2305.05459)

    本文提出了一种能够应对(完全)自主战争中保护非战斗人员的跨频保护徽章设计方案。

    

    在(完全)自主战争中保护非战斗人员引发了国际保护标志的时效性问题。我们建议设计一种跨频保护徽章，以便武器系统能够检测到并相应作出措施。在技术部署方面，考虑采用雷达信标等形式，以及机器学习的方法进行解释和分析。

    The protection of non-combatants in times of (fully) autonomous warfare raises the question of the timeliness of the international protective emblem. Incidents in the recent past indicate that it is becoming necessary to transfer the protective emblem to other dimensions of transmission and representation. (Fully) Autonomous weapon systems are often launched from a great distance to the aiming point and there may be no possibility for the operators to notice protective emblems at the point of impact. In this case, the weapon system would have to detect such protective emblems and, if necessary, disintegrate autonomously or request an abort via human-in-the-loop. In our paper, we suggest ways in which a cross-frequency protective emblem can be designed. On the one hand, the technical deployment, e.g. in the form of RADAR beacons, is considered, as well as the interpretation by methods of machine learning. With regard to the technical deployment, possibilities are considered to address d
    
[^23]: 自我中心的分层视觉语义学

    Egocentric Hierarchical Visual Semantics. (arXiv:2305.05422v1 [cs.AI])

    [http://arxiv.org/abs/2305.05422](http://arxiv.org/abs/2305.05422)

    本文提出了一个基于分层语义结构的物体识别方法，即递归识别物体的视觉属和差异属性，实现了物体识别过程的分层实现。

    

    本文旨在将人们对物体的思考与机器感知进行对齐，即机器的物体识别过程应该与人类思考物体与概念相关的过程类似。最终目标是构建能够有意义地与用户交互，并可以用用户自己的术语描述感知内容的系统。本文提出了一个基于分层语义结构的物体识别方法，通过实现递归识别物体的视觉属和差异属性，从而实现了物体识别过程的分层实现。

    We are interested in aligning how people think about objects and what machines perceive, meaning by this the fact that object recognition, as performed by a machine, should follow a process which resembles that followed by humans when thinking of an object associated with a certain concept. The ultimate goal is to build systems which can meaningfully interact with their users, describing what they perceive in the users' own terms. As from the field of Lexical Semantics, humans organize the meaning of words in hierarchies where the meaning of, e.g., a noun, is defined in terms of the meaning of a more general noun, its genus, and of one or more differentiating properties, its differentia. The main tenet of this paper is that object recognition should implement a hierarchical process which follows the hierarchical semantic structure used to define the meaning of words. We achieve this goal by implementing an algorithm which, for any object, recursively recognizes its visual genus and its
    
[^24]: 基于规则的LTLf流程规范测量：一种概率数据驱动方法

    Measuring Rule-based LTLf Process Specifications: A Probabilistic Data-driven Approach. (arXiv:2305.05418v1 [cs.AI])

    [http://arxiv.org/abs/2305.05418](http://arxiv.org/abs/2305.05418)

    本文介绍了一种基于概率的数据驱动方法，该方法可以度量声明性过程规范的满足度，并可用于发现、检查和漂移检测等方面。

    

    声明性流程规范通过基于有限轨迹的线性时态逻辑（LTLf）规则来定义流程行为。在挖掘上下文中，这些规范是从信息系统（即事件日志）记录的运行的多重集中推断出并进行检查的。为此，能够衡量流程数据符合规范的程度至关重要。然而，现有的挖掘和验证技术仅分析规则本身，忽略了它们之间的相互作用。本文介绍了一个框架来设计声明性过程规范的概率度量方式。进而，我们提出了一种测量事件日志上规范满足度的技术。为了评估我们的方法，我们使用真实世界的数据进行了评估，证明了它在发现、检查和漂移检测方面的适用性。

    Declarative process specifications define the behavior of processes by means of rules based on Linear Temporal Logic on Finite Traces (LTLf). In a mining context, these specifications are inferred from, and checked on, multi-sets of runs recorded by information systems (namely, event logs). To this end, being able to gauge the degree to which process data comply with a specification is key. However, existing mining and verification techniques analyze the rules in isolation, thereby disregarding their interplay. In this paper, we introduce a framework to devise probabilistic measures for declarative process specifications. Thereupon, we propose a technique that measures the degree of satisfaction of specifications over event logs. To assess our approach, we conduct an evaluation with real-world data, evidencing its applicability in discovery, checking, and drift detection contexts.
    
[^25]: 开放世界知识库中的完整性、召回率和否定性：一项调查

    Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v1 [cs.AI])

    [http://arxiv.org/abs/2305.05403](http://arxiv.org/abs/2305.05403)

    本调查讨论了在开放世界知识库中表达、提取和推断完整性、召回率和否定性信息的方法，以及面对不完整和不确定的知识时，从业者和研究人员应该如何处理这些情况。

    

    通用知识库是知识中心的AI的基石。许多知识库是从Web来源实用主义构建的，因此远非完整。这给内容的消费和管理带来了挑战。本调查讨论了如何表达、提取和推断知识库中的完整性、召回率和否定性信息。我们涵盖了（i）部分封闭世界语义下的知识表示和查询的逻辑基础；（ii）通过统计模式估计此信息；（iii）从知识库和文本中提取关于召回率的信息；（iv）辨别有趣的否定语句；以及（v）相对召回率的宽松概念。本调查针对两类受众：（1）寻求处理不完整和不确定知识指南的从业者，以及（2）旨在推进知识库管理、质量评估和自然语言理解的研究人员。

    General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric AI. Many of them are constructed pragmatically from Web sources, and are thus far from complete. This poses challenges for the consumption as well as the curation of their content. While several surveys target the problem of completing incomplete KBs, the first problem is arguably to know whether and where the KB is incomplete in the first place, and to which degree.  In this survey we discuss how knowledge about completeness, recall, and negation in KBs can be expressed, extracted, and inferred. We cover (i) the logical foundations of knowledge representation and querying under partial closed-world semantics; (ii) the estimation of this information via statistical patterns; (iii) the extraction of information about recall from KBs and text; (iv) the identification of interesting negative statements; and (v) relaxed notions of relative recall.  This survey is targeted at two types of audiences: (1) practitione
    
[^26]: 关于锐度感知优化与对抗鲁棒性之间的关系

    On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (arXiv:2305.05392v1 [cs.LG])

    [http://arxiv.org/abs/2305.05392](http://arxiv.org/abs/2305.05392)

    SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗性能。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。SAM单独使用可以在不牺牲清晰度精度的情况下提高对抗鲁棒性。

    

    我们在对抗鲁棒性的背景下提出了对锐度感知优化（SAM）的新理解。本文指出，SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗鲁棒性。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。在一个简化模型中，我们提供了这些声明的理论证据和严格的数学证明。此外，我们进行了实验证明，仅利用SAM可以实现比标准训练更好的对抗鲁棒性，这是意外的好处。由于对抗训练可能会导致清晰度精度的降低，我们展示了仅使用SAM可以在不牺牲清晰度精度的情况下提高鲁棒性。源代码可在https://github.com/weizeming/SAM_AT获取。

    We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.
    
[^27]: VEDLIoT--下一代加速AIoT系统和应用

    VEDLIoT -- Next generation accelerated AIoT systems and applications. (arXiv:2305.05388v1 [cs.AR])

    [http://arxiv.org/abs/2305.05388](http://arxiv.org/abs/2305.05388)

    VEDLIoT项目旨在开发一种安全、节能和高效的深度学习方法，以解决分布式物联网（AIoT）应用程序的挑战。其核心方法在于模块化和可扩展的认知物联网硬件平台，利用微服务器技术和异构计算以及全谱硬件加速器的集成。项目的贡献涵盖了可信计算、远程断言和安全执行环境。

    

    VEDLIoT项目旨在为分布式物联网（AIoT）应用程序开发节能深度学习方法。在我们的项目中，我们提出了一种全面的方法，重点是优化算法，同时解决AIoT系统固有的安全性和安全性挑战。这种方法的基础在于一种模块化和可扩展的认知物联网硬件平台，它利用微服务器技术使用户能够配置硬件以满足各种应用程序的要求。异构计算用于提高性能和能量效率。此外，集成了全谱硬件加速器，提供专用ASIC以及用于可重构计算的FPGA。该项目的贡献涵盖了可信计算，远程断言和安全执行环境的范围，旨在促进健壮而有效的AIoT系统的设计和部署。VEDLIoT的整体愿景是通过模块化和可扩展的认知物联网硬件平台，利用微服务器技术和异构计算以及集成了全谱硬件加速器为分布式物联网（AIoT）应用程序开发节能和安全的深度学习方法。该项目的核心贡献包括通过可信计算，远程断言和安全执行环境解决AIoT系统固有的安全挑战。

    The VEDLIoT project aims to develop energy-efficient Deep Learning methodologies for distributed Artificial Intelligence of Things (AIoT) applications. During our project, we propose a holistic approach that focuses on optimizing algorithms while addressing safety and security challenges inherent to AIoT systems. The foundation of this approach lies in a modular and scalable cognitive IoT hardware platform, which leverages microserver technology to enable users to configure the hardware to meet the requirements of a diverse array of applications. Heterogeneous computing is used to boost performance and energy efficiency. In addition, the full spectrum of hardware accelerators is integrated, providing specialized ASICs as well as FPGAs for reconfigurable computing. The project's contributions span across trusted computing, remote attestation, and secure execution environments, with the ultimate goal of facilitating the design and deployment of robust and efficient AIoT systems. The over
    
[^28]: 预训练语言模型的代码执行能力研究

    Code Execution with Pre-trained Language Models. (arXiv:2305.05383v1 [cs.PL])

    [http://arxiv.org/abs/2305.05383](http://arxiv.org/abs/2305.05383)

    本文研究了预训练语言模型对代码执行的能力，并通过开发一种变异数据增强技术创建了一个大规模的Python数据集和任务，提出了CodeExecutor模型以增强语义理解，该模型在代码执行、零-shot代码到代码搜索和文本到代码生成等方面有潜在好处。

    

    代码执行是编程语言语义学中的基本方面，它反映了代码的确切行为。然而，大多数面向代码智能的预训练模型忽略了执行轨迹，只依靠源代码和句法结构。本文研究了预训练模型能否理解和执行代码。我们开发了一种基于变异的数据增强技术，创建了一个大规模和逼真的Python数据集和任务，挑战了现有的模型如Codex。然后，我们提出了CodeExecutor，一个利用代码执行预训练和课程学习来增强其语义理解的Transformer模型。我们对代码执行进行评估，并展示了其有望的表现和局限性。我们还展示了它在代码智能任务中的潜在好处，如零-shot代码到代码搜索和文本到代码生成。我们的分析提供了有关预训练语言模型学习和泛化的洞见，并为代码智能的研究开辟了新方向。

    Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization 
    
[^29]: 专业认证基准数据集：大语言模型的前500个职位

    Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models. (arXiv:2305.05377v1 [cs.AI])

    [http://arxiv.org/abs/2305.05377](http://arxiv.org/abs/2305.05377)

    本研究创建了一个基准数据集，测试了大型语言模型的职业准备技能，比较了GPT-3和Turbo-GPT3.5在1149个专业认证领域的表现，Turbo-GPT3.5的通过率达到了100%。模型证明了在计算机、医疗保健和金融等各个领域中的技能和潜力。这个数据集可以用来进一步训练和评估语言模型的表现。

    

    本研究创建了一个专业认证调查，以测试大型语言模型并评估其就业技能。它比较了两个AI模型GPT-3和Turbo-GPT3.5在一个包括1149个专业认证的基准数据集上的表现，强调的是职业准备而不是学术表现。GPT-3在39%的专业认证中取得了通过分数（>70%正确率），而没有进行微调或考试准备。模型显示出在各种与计算机相关的领域中的资格，如云和虚拟化、商业分析、网络设置和维修以及数据分析等。Turbo-GPT3.5在颇具价值的Offensive Security Certified Professional（OSCP）考试中得分100%。模型还展现了在其他职业领域，包括护理、持牌咨询、药剂学和教学中的能力。Turbo-GPT3.5在没有准备的情况下获得了金融业监管局（FINRA）系列6考试的70%的成绩。感兴趣的读者可以使用这个500个工作职位的数据集来进一步训练和评估语言模型在实际的现实场景中的表现。

    The research creates a professional certification survey to test large language models and evaluate their employable skills. It compares the performance of two AI models, GPT-3 and Turbo-GPT3.5, on a benchmark dataset of 1149 professional certifications, emphasizing vocational readiness rather than academic performance. GPT-3 achieved a passing score (>70% correct) in 39% of the professional certifications without fine-tuning or exam preparation. The models demonstrated qualifications in various computer-related fields, such as cloud and virtualization, business analytics, cybersecurity, network setup and repair, and data analytics. Turbo-GPT3.5 scored 100% on the valuable Offensive Security Certified Professional (OSCP) exam. The models also displayed competence in other professional domains, including nursing, licensed counseling, pharmacy, and teaching. Turbo-GPT3.5 passed the Financial Industry Regulatory Authority (FINRA) Series 6 exam with a 70% grade without preparation. Interes
    
[^30]: HybridNet: 基于几何与拓扑视角的VLSI阻塞预测的双分支融合

    HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction. (arXiv:2305.05374v1 [cs.LG])

    [http://arxiv.org/abs/2305.05374](http://arxiv.org/abs/2305.05374)

    本文提出了HybridNet，一种基于几何与拓扑视角的VLSI阻塞预测的双分支融合网络，通过在网络结构中做出几个关键设计，充分综合电路的拓扑与几何特征，相较于以往方法取得了10.9％的提高。

    

    准确的阻塞预测是帮助设计师在VLSI设计周期内更快迭代的重要环节，而本文提出了一种新的策略，通过在网络结构中做出几个关键设计，充分综合电路的拓扑与几何特征。具体来说，我们构建了两个独立的图（几何图、拓扑图），根据它们的唯一属性采用不同的边缘构建方案。然后，我们提出了一个双分支网络，每个路径中都有不同的编码器层，并通过精细的融合策略进行聚合表示。我们的网络名为HybridNet，不仅提供了一种简单而有效的方法来捕捉单元之间的几何交互，而且还保留了原始电路拓扑关系。在ISPD2015基准测试上的实验结果显示，相较于以往方法，我们取得了10.9％的提高。

    Accurate early congestion prediction can prevent unpleasant surprises at the routing stage, playing a crucial character in assisting designers to iterate faster in VLSI design cycles. In this paper, we introduce a novel strategy to fully incorporate topological and geometrical features of circuits by making several key designs in our network architecture. To be more specific, we construct two individual graphs (geometry-graph, topology-graph) with distinct edge construction schemes according to their unique properties. We then propose a dual-branch network with different encoder layers in each pathway and aggregate representations with a sophisticated fusion strategy. Our network, named HybridNet, not only provides a simple yet effective way to capture the geometric interactions of cells, but also preserves the original topological relationships in the netlist. Experimental results on the ISPD2015 benchmarks show that we achieve an improvement of 10.9% compared to previous methods.
    
[^31]: GNNs: 可以更强、更新、更快

    GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])

    [http://arxiv.org/abs/2305.05368](http://arxiv.org/abs/2305.05368)

    本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。

    

    图神经网络（GNNs）是一类可以从图结构数据中学习并通过集成邻居节点的表示学习来表现出色的神经网络。然而，GNN的性能会随着层数增加而逐渐降低。本文引入了一个新的概念——k跳子图聚合，提出了一种新的理解GNN表现能力的视角，揭示了传统深层GNN表现逐渐退化的潜在原因，包括聚合子图的重叠以及基于残差的GNN实际上利用了1到k跳子图聚合结果来提高有效性。此外，我们提出了一种新的采样节点级残差模块SDF，通过理论推导证明其比之前的残差方法具有更优的表现能力，可以利用1到k跳跃子图的信息。

    Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
    
[^32]: 大型语言模型程序

    Large Language Model Programs. (arXiv:2305.05364v1 [cs.LG])

    [http://arxiv.org/abs/2305.05364](http://arxiv.org/abs/2305.05364)

    本文提出了一种将大型预训练语言模型嵌入算法或程序中，扩展其能力的方法。这种方法可以在未经微调的情况下通过更具算法性的方法获得不错的性能提升。

    

    近年来，大型预训练语言模型(LLMs)已经证明了它们能够通过几个示例来执行指令并执行新的任务的能力。通过这种在上下文示例中参数化LLMs的可能性，可以以比微调低得多的成本拓展它们的能力。我们扩展了这一推理线路，并提出了一种方法，通过将LLM嵌入算法或程序中，进一步扩展LLM的能力。为了证明这种方法的优点，我们提供了一个证据支持的问答的说明性例子。我们通过更具算法性的方法而没有任何微调，在通过一系列思路基线的基础上获得了6.4%的改进。此外，我们从这个角度突出了最近的工作，并讨论了与标准方法相比的优点和缺点。

    In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.
    
[^33]: 脊柱融合手术中经皮椎弓根螺钉放置的安全深度强化学习规划

    Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement. (arXiv:2305.05354v1 [cs.RO])

    [http://arxiv.org/abs/2305.05354](http://arxiv.org/abs/2305.05354)

    本论文提出了一种基于安全深度强化学习的椎管机器人手术术中规划方法，解决了相对传统的术前规划和术中注册开环控制的局限，能够提高放置准确度和保证手术安全。

    

    脊柱融合手术需要高精度实施椎弓根螺钉植入，其必须在极度接近重要结构并且解剖视野有限的情况下进行。虽然提出了机器人手术系统以改善放置准确度，但是现有系统仍然存在传统的术前规划和术中注册开环控制的局限。本文提出了一种采用安全深度强化学习进行实时观察的椎管机器人手术术中规划方法。我们主要的贡献有：（1）通过引入基于距离的不确定性感知安全过滤器保证安全操作能力；（2）通过使用先验解剖结构信息对不完整的术中解剖信息进行补偿。

    Spinal fusion surgery requires highly accurate implantation of pedicle screw implants, which must be conducted in critical proximity to vital structures with a limited view of anatomy. Robotic surgery systems have been proposed to improve placement accuracy, however, state-of-the-art systems suffer from the limitations of open-loop approaches, as they follow traditional concepts of preoperative planning and intraoperative registration, without real-time recalculation of the surgical plan. In this paper, we propose an intraoperative planning approach for robotic spine surgery that leverages real-time observation for drill path planning based on Safe Deep Reinforcement Learning (DRL). The main contributions of our method are (1) the capability to guarantee safe actions by introducing an uncertainty-aware distance-based safety filter; and (2) the ability to compensate for incomplete intraoperative anatomical information, by encoding a-priori knowledge about anatomical structures with a ne
    
[^34]: 基于基础模型的系统设计框架

    A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])

    [http://arxiv.org/abs/2305.05352](http://arxiv.org/abs/2305.05352)

    本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    

    最近推出了大型语言模型(LLM)的聊天机器人，如ChatGPT，这引起了人们对基础模型的广泛关注。基础模型被广泛认为将成为未来人工智能系统的基石。由于基础模型处于早期阶段，基于基础模型的系统设计尚未得到系统地探索。人们对在软件架构中引入基础模型的影响知之甚少。因此，在本文中，我们提出了一个基于基础模型的系统分类法，对基础模型和基于基础模型的系统的特点进行了分类和比较。我们的分类法包括三个类别：基础模型预训练和微调、基于基础模型的系统架构设计和负责任的AI设计。这个分类法为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
    
[^35]: 利用transformer和集成技术在社交网络上检测抑郁症

    Detection of depression on social networks using transformers and ensembles. (arXiv:2305.05325v1 [cs.CL])

    [http://arxiv.org/abs/2305.05325](http://arxiv.org/abs/2305.05325)

    本文利用transformer和集成技术，在社交网络上检测抑郁症的迹象，构建了多个基于预训练语言模型的分类器和两种类型的集成模型，表现更好。

    

    随着科技在我们生活中的影响不断增强，社交媒体的使用也越来越普遍，它不仅是一种沟通工具，还可以用来向社区分享我们的观点和感受。对于抑郁症等心理健康问题，人们也会利用社交媒体来表达自己的想法寻求帮助。因此，我们可以通过自动处理社交媒体帖子并检测抑郁症的迹象来提供帮助。本文构建了大量的基于预训练语言模型的分类器，包括BERT、RoBERTA、BERTweet和mentalBERT，并构建了两种类型的集成模型。我们分析了模型在Reddit和Twitter两个社交平台上的表现，并研究了跨数据集的转移学习性能。结果表明，transformer集成模型比单一的transformer模型表现更好。

    As the impact of technology on our lives is increasing, we witness increased use of social media that became an essential tool not only for communication but also for sharing information with community about our thoughts and feelings. This can be observed also for people with mental health disorders such as depression where they use social media for expressing their thoughts and asking for help. This opens a possibility to automatically process social media posts and detect signs of depression. We build several large pre-trained language model based classifiers for depression detection from social media posts. Besides fine-tuning BERT, RoBERTA, BERTweet, and mentalBERT were also construct two types of ensembles. We analyze the performance of our models on two data sets of posts from social platforms Reddit and Twitter, and investigate also the performance of transfer learning across the two data sets. The results show that transformer ensembles improve over the single transformer-based
    
[^36]: 人工智能在药物制剂中显微淀粉图像分类中的应用

    Application of Artificial Intelligence in the Classification of Microscopical Starch Images for Drug Formulation. (arXiv:2305.05321v1 [cs.CV])

    [http://arxiv.org/abs/2305.05321](http://arxiv.org/abs/2305.05321)

    本研究利用人工智能技术，通过转移学习和深度卷积神经网络对不同植物来源的淀粉显微图像进行分类，实现了81%的分类准确率和更优的精度、召回率和f1得分。

    

    淀粉是植物中重要的能量来源，具有许多医药工业中的用途，如药品中的粘合剂、崩解剂和增塑剂，因此需要非常仔细的物理化学分析进行正确的识别和验证，包括显微镜检查。本研究利用人工智能技术（使用转移学习和深度卷积神经网络CNN对来自9种植物来源的淀粉样本的显微图像进行分类。当机器学习模型在来自MicroNet数据集的显微图像上进行预训练时，我们的方法获得了61%的准确率。然而，在使用从Imagenet数据集获得的随机日常图像进行预训练时，准确率跃升至81%。与在MicroNet数据集上预训练的模型相比，Imagenet预训练模型还显示出更好的精度、召回率和f1得分。

    Starches are important energy sources found in plants with many uses in the pharmaceutical industry such as binders, disintegrants, bulking agents in drugs and thus require very careful physicochemical analysis for proper identification and verification which includes microscopy. In this work, we applied artificial intelligence techniques (using transfer learning and deep convolution neural network CNNs to microscopical images obtained from 9 starch samples of different botanical sources. Our approach obtained an accuracy of 61% when the machine learning model was pretrained on microscopic images from MicroNet dataset. However the accuracy jumped to 81% for model pretrained on random day to day images obtained from Imagenet dataset. The model pretrained on the imagenet dataset also showed a better precision, recall and f1 score than that pretrained on the imagenet dataset.
    
[^37]: 结构化情感分析作为基于转移的依存句法分析

    Structured Sentiment Analysis as Transition-based Dependency Parsing. (arXiv:2305.05311v1 [cs.CL])

    [http://arxiv.org/abs/2305.05311](http://arxiv.org/abs/2305.05311)

    本文提出了第一种将结构化情感分析作为依存句法分析处理的基于转移的方法，其基于Pointer Network体系结构，实现了在准确性和效率方面均优于以前提出的图形模型的结果，是迄今为止最为准确的SSA方法。

    

    结构化情感分析（SSA）旨在从自然语言文本中自动提取人们的观点，并以图形结构充分表示该信息。最近提出了一种最准确的执行SSA的方法，即将其视为依存句法分析任务。尽管我们可以在文献中发现基于转移的算法在依存句法分析的准确性和效率方面优于其他方法，但所有尝试采用这种方法解决SSA的方法都基于基于图形的模型。在本文中，我们提出了第一个将SSA作为依存句法分析处理的基于转移的方法。具体而言，我们设计了一个转移系统，以从左到右的方式处理输入文本，逐步生成包含所有识别出的观点的图形结构。为了有效地实现我们的最终基于转移的模型，我们借助了Pointer Network体系结构作为支撑。通过广泛的评估，我们证明了我们的模型超越了SSA的最新技术水平，在包括SemEval 2014 Task 4在内的四个基准数据集上取得了迄今为止报告的最高准确性。

    Structured sentiment analysis (SSA) aims to automatically extract people's opinions from a text in natural language and adequately represent that information in a graph structure. One of the most accurate methods for performing SSA was recently proposed and consists of approaching it as a dependency parsing task. Although we can find in the literature how transition-based algorithms excel in dependency parsing in terms of accuracy and efficiency, all proposed attempts to tackle SSA following that approach were based on graph-based models. In this article, we present the first transition-based method to address SSA as dependency parsing. Specifically, we design a transition system that processes the input text in a left-to-right pass, incrementally generating the graph structure containing all identified opinions. To effectively implement our final transition-based model, we resort to a Pointer Network architecture as a backbone. From an extensive evaluation, we demonstrate that our mod
    
[^38]: VCSUM：一个多功能的中文会议摘要数据集

    VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])

    [http://arxiv.org/abs/2305.05280](http://arxiv.org/abs/2305.05280)

    介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。数据集和代码将在GitHub上发布。

    

    与新闻和聊天摘要相比，由于数据受限，会议摘要的发展受到极大的减速。为此，我们介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。我们声称我们的数据集是多功能的，因为我们为每个会议的文本提供了主题划分、头条、分段摘要、整个会议摘要和显要句子等注释。因此，该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。我们的分析证实了VCSum的有效性和稳健性。我们还提供了一组关于不同下游摘要任务的基准模型，以便进一步研究VCSum。数据集和代码将在 \url{https://github.com/hahahawu/VCSum} 上发布。

    Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at \url{https://github.com/hahahawu/VCSum}.
    
[^39]: 学习个性化推荐以基于客户购物意图

    Learning to Personalize Recommendation based on Customers' Shopping Intents. (arXiv:2305.05279v1 [cs.IR])

    [http://arxiv.org/abs/2305.05279](http://arxiv.org/abs/2305.05279)

    本文介绍了亚马逊的新系统，利用深度学习模型将顾客的在线行为映射成为高级别购物意图，以便个性化推荐，提供更相关、可解释和多样化的购物体验。

    

    理解顾客的高级别购物意图，如他们去露营或举办生日派对的愿望，对于电商平台非常重要；它可以通过提供更相关、可解释和多样化的推荐来提高购物体验的质量。然而，由于实际挑战，这种高级别的购物意图在行业中被忽视。在这项工作中，我们介绍了亚马逊的新系统，明确地识别和利用每个客户的高级别购物意图来个性化推荐。我们开发了一种新技术，自动识别亚马逊客户正在追求的各种高级别目标，如“去露营”和“准备海滩派对”。我们的解决方案进行了扩展（跨越21个国家的14种语言）。然后，一个深度学习模型将每个客户的在线行为，如产品搜索和个体项目参与，映射成一组高级别的购物意图。

    Understanding the customers' high level shopping intent, such as their desire to go camping or hold a birthday party, is critically important for an E-commerce platform; it can help boost the quality of shopping experience by enabling provision of more relevant, explainable, and diversified recommendations. However, such high level shopping intent has been overlooked in the industry due to practical challenges. In this work, we introduce Amazon's new system that explicitly identifies and utilizes each customer's high level shopping intents for personalizing recommendations. We develop a novel technique that automatically identifies various high level goals being pursued by the Amazon customers, such as "go camping", and "preparing for a beach party". Our solution is in a scalable fashion (in 14 languages across 21 countries). Then a deep learning model maps each customer's online behavior, e.g. product search and individual item engagements, into a subset of high level shopping intents
    
[^40]: 通过深度矩阵分解实现旋转同步

    Rotation Synchronization via Deep Matrix Factorization. (arXiv:2305.05268v1 [cs.CV])

    [http://arxiv.org/abs/2305.05268](http://arxiv.org/abs/2305.05268)

    本文提出了一个无监督且具有隐式正则化属性的深度神经网络公式，用于实现旋转同步问题，并在实验中取得了与竞争对手相当的准确性。

    

    本文针对旋转同步问题展开研究，该问题的目标是从成对的旋转中恢复绝对旋转，其中未知数和测量值分别表示为图的节点和边。该问题是结构从运动和同时定位和映射的重要任务。我们专注于通过神经网络公式化同步问题，这项工作在最近的文献中才开始探索。受到深度矩阵补全的启发，我们将旋转同步表示为具有深度神经网络的矩阵分解。我们的公式具有隐式正则化属性，更重要的是，它是无监督的，而先前的深度方法是受监督的。我们的实验表明，我们在大多数情况下实现了与最接近的竞争对手相当的准确性，在较弱的假设下工作。

    In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.
    
[^41]: 从大型语言模型中提取脚本知识以进行受限语言规划

    Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])

    [http://arxiv.org/abs/2305.05252](http://arxiv.org/abs/2305.05252)

    本文首次定义了受限语言规划任务，提出了一种方法来提高大型语言模型在这个任务中的表现，并提取了一个新颖的受限语言规划数据集。实验证明该方法显著提高了其在约束忠实度方面的能力，并对赋予较小的语言模型受限语言规划能力非常有效。

    

    在日常生活中，人们经常通过遵循目标导向的脚本形式的逐步说明来规划自己的行动。以往的工作利用语言模型（LM）来为立体活动的抽象目标（例如，“制作蛋糕”）进行规划，但对于具有多方面约束的更具体目标（例如，“为糖尿病患者制作蛋糕”）鲜有研究。本文首次定义了受限语言规划任务。我们提出了一种过度生成并过滤的方法来改善大型语言模型（LLM）在这个任务中的表现，并利用它来提取一种新颖的受限语言规划数据集CoScript，其中包括55,000个脚本。实验证明，我们的方法显著提高了LLM在受限语言规划方面的能力，特别是在约束忠实度方面。此外，CoScript被证明对赋予较小的LM受限语言规划能力是非常有效的。

    In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
    
[^42]: 在医疗保健领域利用生成式人工智能模型进行合成数据生成：平衡研究与隐私

    Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy. (arXiv:2305.05247v1 [cs.LG])

    [http://arxiv.org/abs/2305.05247](http://arxiv.org/abs/2305.05247)

    本文研究了在医疗保健领域使用生成式AI模型合成匿名化病人数据进行研究和培训的方法，旨在平衡数据访问和隐私保护，并探索其在医疗保健领域的应用，并讨论了其未来的挑战和研究方向。

    

    电子病历和数字化医疗数据的广泛应用，促使了使用数据驱动的洞见以增强病患结果，建立诊断和治疗方案。然而，使用真实的病人数据会导致隐私和监管挑战，包括HIPAA和GDPR的合规要求。生成式AI模型，如GAN和VAE用于合成数据生成，提供了一种有前途的解决方案，即平衡有价值的数据访问和病人隐私保护。本文研究使用生成式AI模型创建逼真的匿名化病人数据进行研究和培训，探索合成数据在医疗保健领域的应用，并讨论其益处，挑战和未来研究方向。合成数据有潜力通过提供匿名病人数据来革命化医疗保健，同时保护隐私并实现多种应用。

    The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA and GDPR. Synthetic data generation, using generative AI models like GANs and VAEs offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training, explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.
    
[^43]: 可学习的行为控制：通过高效行为选择打破Atari人类世界记录

    Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection. (arXiv:2305.05239v1 [cs.LG])

    [http://arxiv.org/abs/2305.05239](http://arxiv.org/abs/2305.05239)

    本文提出了一个通用的Learnable Behavioral Control (LBC)框架，使得行为选择空间得到扩大，并通过基于赌博机的元控制器实现行为控制。在Atari游戏上，我们的代理已经达到10个游戏的人类水平，并在7个游戏中达到了目前的最高分。

    

    在深度强化学习中，探索问题是主要挑战之一。最近，一些有希望的工作尝试使用基于群体的方法来处理这个问题，通过从不同探索策略的人群中收集具有不同行为的样本。自适应策略选择已被用于行为控制。然而，行为选择空间在很大程度上受到预定义策略种群的限制，这进一步限制了行为多样性。在本文中，我们提出了一个通用框架称为可学习的行为控制（LBC）来解决这种限制。该框架a)通过从所有策略中制定混合行为映射，实现了显著扩大的行为选择空间；b)构建了一个统一的可学习的行为选择过程。我们将LBC引入分布式离线演员-评论家方法中，并通过基于赌博机的元控制器优化行为映射的选择来实现行为控制。我们的代理已经在10个Atari游戏中达到了人类水平，并在7个游戏中达到了目前的最高分。我们还展示了LBC框架的良好泛化能力，并在机器人控制任务上进行了测试。

    The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10
    
[^44]: FedNoRo: 针对类别不平衡和标签噪声异质性的噪声-鲁棒联邦学习

    FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])

    [http://arxiv.org/abs/2305.05230](http://arxiv.org/abs/2305.05230)

    本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。

    

    联邦噪声标签学习(FNLL)正在成为一种有前途的隐私保护的多源分散学习工具。现有研究基于全局数据类别平衡的假设，可能无法建模复杂的标签噪声，特别是在医学场景中。本文首先提出了一个更为真实的联邦标签噪声问题，其中全局数据是类别不平衡的，并且标签噪声是异质的，然后提出了一个名为 FedNoRo 的两阶段框架，用于噪声-鲁棒联邦学习。具体而言，在 FedNoRo 的第一阶段，采用每类损失指标之后跟随高斯混合模型进行嘈杂客户端识别。在第二阶段，同时采用知识蒸馏和距离感知聚合函数进行噪声-鲁棒联邦模型更新。对广泛使用的 ICH 和 ISIC2019 数据集的实验结果表明，FedNoRo 相对于最先进的 FNLL 方法在解决联邦学习中的类别不平衡和标签噪声异质性方面具有卓越的性能。

    Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
    
[^45]: 语义嵌入深度神经网络：一种提升多标签图像分类性能的通用方法。

    Semantic Embedded Deep Neural Network: A Generic Approach to Boost Multi-Label Image Classification Performance. (arXiv:2305.05228v1 [cs.CV])

    [http://arxiv.org/abs/2305.05228](http://arxiv.org/abs/2305.05228)

    本研究提出了一种通用的语义嵌入深度神经网络，通过空间感知的语义特征和基于通道的注意力模型来提高多标签预测的模型性能，平均相对改进达到15.27%。

    

    精细的多标签分类模型在亚马逊生产功能中具有广泛的应用，例如基于视觉的标签预测，从时尚属性检测到品牌识别。实现这些分类任务的一个挑战是野外视觉背景信号，其中包含混淆模型的无关像素，使模型难以专注于感兴趣区域并根据该特定区域进行预测。在本文中，我们介绍了一种通用的语义嵌入深度神经网络，应用空间感知的语义特征，并结合基于通道的注意力模型来利用定位引导，以提高多标签预测的模型性能。与基线方法相比，我们观察到所有标签的AUC得分的平均相对改进为15.27%。核心实验和消融研究涉及对Instagram时尚服装的多标签时尚属性分类进行的。

    Fine-grained multi-label classification models have broad applications in Amazon production features, such as visual based label predictions ranging from fashion attribute detection to brand recognition. One challenge to achieve satisfactory performance for those classification tasks in real world is the wild visual background signal that contains irrelevant pixels which confuses model to focus onto the region of interest and make prediction upon the specific region. In this paper, we introduce a generic semantic- embedding deep neural network to apply the spatial awareness semantic feature incorporating a channel- wise attention based model to leverage the localization guidance to boost model performance for multi- label prediction. We observed an Avg.relative improvement of 15.27% in terms of AUC score across all labels compared to the baseline approach. Core experiment and ablation studies involve multi-label fashion attribute classification performed on Instagram fashion apparels' 
    
[^46]: FishRecGAN：用于鱼眼图像矫正和相机内参和畸变参数标定的端到端GAN网络

    FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration. (arXiv:2305.05222v1 [cs.CV])

    [http://arxiv.org/abs/2305.05222](http://arxiv.org/abs/2305.05222)

    FishRecGAN提供了一种端到端的深度学习方法，以矫正鱼眼图像并同时校准相机内参和畸变参数。其快速校正网络具有良好的分辨率和鲁棒性，适用于摄像机型监控设备中的恒定标定，并使用大量合成数据集进行训练和验证，表现出了高分辨率的鲁棒性和显著的峰值信噪比。

    

    我们提出了一种端到端的深度学习方法，以矫正鱼眼图像并同时校准相机内参和畸变参数。我们的方法由两部分组成：使用Pix2Pix GAN和Wasserstein GAN（W-Pix2PixGAN）开发的Quick Image Rectification模块，以及使用CNN架构的Calibration模块。我们的快速校正网络具有良好的分辨率和鲁棒性，适用于摄像机型监控设备中的恒定标定。为了实现高质量的标定，我们使用从Quick Image Rectification模块中输出的直线特征作为指导样本传递给Calibration模块，以学习校正前后的几何关系。我们使用大量合成数据集来训练和验证我们的方法，并应用于透视图像数据集，表现出了高分辨率的鲁棒性和显著的峰值信噪比。

    We propose an end-to-end deep learning approach to rectify fisheye images and simultaneously calibrate camera intrinsic and distortion parameters. Our method consists of two parts: a Quick Image Rectification Module developed with a Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a CNN architecture. Our Quick Rectification Network performs robust rectification with good resolution, making it suitable for constant calibration in camera-based surveillance equipment. To achieve high-quality calibration, we use the straightened output from the Quick Rectification Module as a guidance-like semantic feature map for the Calibration Module to learn the geometric relationship between the straightened feature and the distorted feature. We train and validate our method with a large synthesized dataset labeled with well-simulated parameters applied to a perspective image dataset. Our solution has achieved robust performance in high-resolution with a significant PSNR v
    
[^47]: LSAS：用轻量级子注意力策略缓解关注偏差问题

    LSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias Problem. (arXiv:2305.05200v1 [cs.CV])

    [http://arxiv.org/abs/2305.05200](http://arxiv.org/abs/2305.05200)

    本文提出了一种轻量级子注意力策略（LSAS），能够显著缓解计算机视觉中深度神经网络的关注偏差问题，即DNN过于聚焦于标签无关的区域，并且无法完全包含理想区域。LSAS能够改进现有的自我关注模块并在广泛使用的基准数据集和流行的关注网络中证明其有效性。

    

    在计算机视觉中，深度神经网络（DNN）的性能与其特征提取能力密切相关，即识别并聚焦于图像中的关键像素区域的能力。然而，本文 quantitatively and statistically illustrate，在许多流行数据集的许多样本中，DNN存在严重的关注偏差问题。

    In computer vision, the performance of deep neural networks (DNNs) is highly related to the feature extraction ability, i.e., the ability to recognize and focus on key pixel regions in an image. However, in this paper, we quantitatively and statistically illustrate that DNNs have a serious attention bias problem on many samples from some popular datasets: (1) Position bias: DNNs fully focus on label-independent regions; (2) Range bias: The focused regions from DNN are not completely contained in the ideal region. Moreover, we find that the existing self-attention modules can alleviate these biases to a certain extent, but the biases are still non-negligible. To further mitigate them, we propose a lightweight sub-attention strategy (LSAS), which utilizes high-order sub-attention modules to improve the original self-attention modules. The effectiveness of LSAS is demonstrated by extensive experiments on widely-used benchmark datasets and popular attention networks. We release our code to
    
[^48]: COLA: 因果推断角度下的情境化常识因果推理

    COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective. (arXiv:2305.05191v1 [cs.CL])

    [http://arxiv.org/abs/2305.05191](http://arxiv.org/abs/2305.05191)

    本文提出了一个从因果推断角度出发的情境化常识因果推理任务，设计了一个零-shot框架COLA来解决此任务，并且该框架可以更准确地检测常识因果关系。

    

    检测事件之间的常识因果关系（因果关系）长期以来是一项基本而具有挑战性的任务。鉴于事件的复杂性，一个事件在不同的上下文中可能有不同的原因。因此，利用上下文在检测因果关系中发挥了关键作用。同时，先前关于常识因果关系的工作只考虑两个事件并忽略它们的上下文，简化了任务的制定。本文提出了一项新任务，即检测事件序列（即上下文）中两个事件之间的常识因果关系，称为情境化常识因果推理。我们还从因果推断的角度设计了一个零-shot框架：COLA（情境化常识因果推理器）来解决这个任务。这个框架从时间性获得了丰富的偶发监督，并平衡了来自多个时间戳的协变量以消除混淆效应。我们广泛的实验表明，与基线相比，COLA可以更准确地检测常识因果关系。

    Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between two events in an event sequence (i.e., context), called contextualized commonsense causal reasoning. We also design a zero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to solve the task from the causal inference perspective. This framework obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects. Our extensive experiments show that COLA can detect commonsense causality more accurately than baselines
    
[^49]: 深度神经网络加速器DeepFire2：基于FPGA的卷积脉冲神经网络

    DeepFire2: A Convolutional Spiking Neural Network Accelerator on FPGAs. (arXiv:2305.05187v1 [cs.NE])

    [http://arxiv.org/abs/2305.05187](http://arxiv.org/abs/2305.05187)

    本文介绍了一种基于FPGA的深度神经网络加速器DeepFire2，通过利用多芯片的超级逻辑区域映射大型网络层，避免了逻辑资源限制层大小，同时通过深度流水线提高了时钟速度，将吞吐量和功率效率提高了一倍。

    

    以仿生学脉冲神经网络（SNN）代替传统神经网络的乘加操作，从而实现更高的能量效率。但当加速大型神经网络时，专用硬件实现这些神经元在功率和性能方面具有优势，但具有较差的可扩展性。DeepFire2引入了一种硬件架构，可以有效地将大型网络层映射到多个超级逻辑区域中的多个芯片上。这给了更多的资源分配和并行性控制，从而使吞吐量和能量消耗受益。避免使用查找表来实现SNN的AND运算，避免了逻辑资源限制层大小。深度流水线不仅可以将时钟速度提高高达600 MHz，而且与DeepFire上一个版本相比，我们将吞吐量和功率效率提高了一倍。

    Brain-inspired spiking neural networks (SNNs) replace the multiply-accumulate operations of traditional neural networks by integrate-and-fire neurons, with the goal of achieving greater energy efficiency. Specialized hardware implementations of those neurons clearly have advantages over general-purpose devices in terms of power and performance, but exhibit poor scalability when it comes to accelerating large neural networks. DeepFire2 introduces a hardware architecture which can map large network layers efficiently across multiple super logic regions in a multi-die FPGA. That gives more control over resource allocation and parallelism, benefiting both throughput and energy consumption. Avoiding the use of lookup tables to implement the AND operations of an SNN, prevents the layer size to be limited by logic resources. A deep pipeline does not only lead to an increased clock speed of up to 600 MHz. We double the throughput and power efficiency compared to our previous version of DeepFir
    
[^50]: CSED: 一个中文语义错误诊断语料库

    CSED: A Chinese Semantic Error Diagnosis Corpus. (arXiv:2305.05183v1 [cs.CL])

    [http://arxiv.org/abs/2305.05183](http://arxiv.org/abs/2305.05183)

    本文建立了一个中文语义错误诊断语料库CSED，通过提出基于句法的模型实现了CSED-R和CSED-C任务的最佳表现。

    

    最近，中文文本错误纠正的工作大多集中在中文拼写检查（CSC）和中文语法错误诊断（CGED）上。相比之下，对于缺乏相关数据集的中文语义错误诊断（CSED）这一复杂问题，却没有得到足够的关注。研究语义错误很重要，因为它们很常见，并且可能导致句法不规则甚至理解问题。为了研究这个问题，我们建立了CSED语料库，其中包括两个数据集。一个是用于CSED识别（CSED-R）任务，另一个是用于CSED更正（CSED-C）任务。我们的注释通过质量保证机制保证了高质量的数据。我们的实验表明，强大的预训练模型在这个语料库上表现不佳。我们还发现，CSED任务具有挑战性，因为即使是人类得分也很低。本文提出了基于句法的模型，专门针对CSED任务进行适应。实验结果表明，我们提出的模型在CSED-R和CSED-C任务上均取得了最新的最佳表现，超过了以前的最佳模型。

    Recently, much Chinese text error correction work has focused on Chinese Spelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In contrast, little attention has been paid to the complicated problem of Chinese Semantic Error Diagnosis (CSED), which lacks relevant datasets. The study of semantic errors is important because they are very common and may lead to syntactic irregularities or even problems of comprehension. To investigate this, we build the CSED corpus, which includes two datasets. The one is for the CSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C) task. Our annotation guarantees high-quality data through quality assurance mechanisms. Our experiments show that powerful pre-trained models perform poorly on this corpus. We also find that the CSED task is challenging, as evidenced by the fact that even humans receive a low score. This paper proposes syntax-aware models to specifically adapt to the CSED task. The experimental results sho
    
[^51]: MoT：预思考和回忆功能使 ChatGPT 在“思想记忆”中自我进化

    MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])

    [http://arxiv.org/abs/2305.05181](http://arxiv.org/abs/2305.05181)

    本文提出了一个名为 MoT 的框架，让大型语言模型通过“思想记忆”自我进化，无需注释数据集和参数更新。实验表明，该框架可以显著提高 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面的能力。

    

    大型语言模型在各种任务上表现出了惊人的能力。但要实现它们的根本性改进，需要高质量的数据集或计算昂贵的微调。相反，人类可以通过思考和记忆轻松提高自我水平，而不需要外部资源。在本文中，我们提出了一个框架 MoT，在没有注释数据集和参数更新的情况下，通过思想记忆让大型语言模型自我进化。具体而言，该框架分为两个阶段：1. 在测试阶段之前，我们让大型语言模型在未加标签的数据集上进行预思考，并将高置信度的想法保存为外部记忆。2. 在推理过程中，给定一个测试问题，我们让大型语言模型回忆相关的记忆，帮助自己进行推理和回答。实验结果表明，所提出的框架可以帮助 ChatGPT 在数学推理、常识推理、事实推理和自然语言推理方面显著提高其能力。进一步的分析表明，每个组件都发挥了作用。

    Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided into two stages: 1. before the test stage, we let the LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as external memory; 2. during inference, given a test question, we let the LLM recall relevant memory to help itself reason and answer it. Experimental results show that the proposed framework can help ChatGPT significantly improve its abilities in math reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contribute
    
[^52]: 简单形式化 Hopfield网络

    Simplicial Hopfield networks. (arXiv:2305.05179v1 [cs.NE])

    [http://arxiv.org/abs/2305.05179](http://arxiv.org/abs/2305.05179)

    展示了一种简单的形式化 Hopfield 神经网络，通过添加集合连接并将这些连接嵌入到一个单纯复合体中，可增加存储容量。

    

    Hopfield网络是一种人工神经网络，它通过选择循环连接权重和更新规则，在其神经元状态上存储记忆模式，使网络的能量景观围绕记忆处形成吸引子。我们可以使用$N$个神经元来存储多少个稳定的、具有足够吸引力的记忆模式，答案取决于权重和更新规则的选择。受生物学中集合连接的启发，我们通过添加集合连接并将这些连接嵌入到一个单纯复合体中来扩展Hopfield网络。单纯复合体是图形的高维类比，自然表示成对和集合关系的集合。我们证明了我们的单纯形 Hopfield 网络可以增加存储容量。令人惊讶的是，即使连接仅限于等于全对网络的小的随机子集，我们的网络仍然优于其成对的对应物。这种情况包括...（待补充）

    Hopfield networks are artificial neural networks which store memory patterns on the states of their neurons by choosing recurrent connection weights and update rules such that the energy landscape of the network forms attractors around the memories. How many stable, sufficiently-attracting memory patterns can we store in such a network using $N$ neurons? The answer depends on the choice of weights and update rule. Inspired by setwise connectivity in biology, we extend Hopfield networks by adding setwise connections and embedding these connections in a simplicial complex. Simplicial complexes are higher dimensional analogues of graphs which naturally represent collections of pairwise and setwise relationships. We show that our simplicial Hopfield networks increase memory storage capacity. Surprisingly, even when connections are limited to a small random subset of equivalent size to an all-pairwise network, our networks still outperform their pairwise counterparts. Such scenarios include
    
[^53]: FrugalGPT: 如何在降低成本和提高性能的同时使用大型语言模型

    FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. (arXiv:2305.05176v1 [cs.LG])

    [http://arxiv.org/abs/2305.05176](http://arxiv.org/abs/2305.05176)

    本论文介绍了如何在使用大型语言模型的同时降低成本和提高性能。作者提出了三种策略，包括提示适应，LLM近似和LLM级联，并且通过 FrugalGPT 这种方法，实现了大大降低成本或是提高准确率的效果。

    

    目前有越来越多的用户可以使用付费的大型语言模型（LLM）进行查询。我们回顾了查询流行的LLM API（例如GPT-4，ChatGPT，J1-Jumbo）涉及的成本，发现这些模型具有异构的价格结构，费用可能相差数个数量级。特别是在大量查询和文本的情况下使用LLM可能会很昂贵。因此，我们总结和讨论了三种策略，用户可以利用这些策略来减少使用LLM的汇编成本：1）提示适应，2）LLM近似和3）LLM级联。作为示例，我们提出了FrugalGPT，它是LLM级联的一个简单而灵活的实例，可以学习使用哪些LLM组合来处理不同查询，以降低成本、提高准确性。我们的实验表明，FrugalGPT可以在仅使用费用的98％或与GPT-4相同的成本下，达到最佳单个LLM的性能（例如GPT-4），或者以4％的准确率提高GPT-4的性能。

    There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same co
    
[^54]: 可解释性人工智能的逻辑

    Logic for Explainable AI. (arXiv:2305.05172v1 [cs.AI])

    [http://arxiv.org/abs/2305.05172](http://arxiv.org/abs/2305.05172)

    本文介绍了一种基于符号逻辑的综合、语义和计算理论，以探讨可解释性人工智能的三个维度，以深入理解分类器所做出的决策。

    

    可解释性人工智能的核心问题在于理解（学习）分类器所做出的决策。这种理解有三个方面，在近年来得到了显著关注。第一维与为判断决策所必要和充分的实例条件有关，从而提供了可视化的实例抽象，可视为“决策背后的原因”。下一维与描述足以作出决策的最小条件有关，从而确定了与决策无关的实例最大方面。最后一个维度将其移动到了决策，即标识对实例进行最小扰动以产生替代决策所必需的最小条件。我们在本教程中讨论了沿这些方面的可解释性的全面、语义和计算理论，这是基于符号逻辑的一些最新发展。

    A central quest in explainable AI relates to understanding the decisions made by (learned) classifiers. There are three dimensions of this understanding that have been receiving significant attention in recent years. The first dimension relates to characterizing conditions on instances that are necessary and sufficient for decisions, therefore providing abstractions of instances that can be viewed as the "reasons behind decisions." The next dimension relates to characterizing minimal conditions that are sufficient for a decision, therefore identifying maximal aspects of the instance that are irrelevant to the decision. The last dimension relates to characterizing minimal conditions that are necessary for a decision, therefore identifying minimal perturbations to the instance that yield alternate decisions. We discuss in this tutorial a comprehensive, semantical and computational theory of explainability along these dimensions which is based on some recent developments in symbolic logic
    
[^55]: 采用深度强化学习合作的图神经网络用于疫苗优先考虑

    Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization. (arXiv:2305.05163v1 [q-bio.PE])

    [http://arxiv.org/abs/2305.05163](http://arxiv.org/abs/2305.05163)

    本研究采用深度强化学习合作的图神经网络，结合疾病动力学模型，开发了一种疫苗优先考虑策略，旨在在供应有限的情况下降低疫情的总体负担。

    

    本研究探讨了疫苗优先考虑策略，以在供应有限的情况下减少疫情的总体负担。现有方法通过假设亚组人口内的同质性行为和缺乏移动性动态集成，进行宏观或简化的微观疫苗分配。直接将这些模型应用于微观疫苗分配会导致次优解，因为缺乏与行为相关的细节。为了解决这个问题，我们首先将疾病动力学中的移动性异质性融入模型，并使用一个Trans-vaccine-SEIR模型模拟疾病的演变过程。然后我们开发了一种新的深度强化学习方法，来寻求高度空间-时间疾病演化系统的最优疫苗分配策略。图神经网络被用来有效捕捉移动接触网络的结构特性和提取动态疾病特征。在我们的评估中，所提出的框架r

    This study explores the vaccine prioritization strategy to reduce the overall burden of the pandemic when the supply is limited. Existing methods conduct macro-level or simplified micro-level vaccine distribution by assuming the homogeneous behavior within subgroup populations and lacking mobility dynamics integration. Directly applying these models for micro-level vaccine allocation leads to sub-optimal solutions due to the lack of behavioral-related details. To address the issue, we first incorporate the mobility heterogeneity in disease dynamics modeling and mimic the disease evolution process using a Trans-vaccine-SEIR model. Then we develop a novel deep reinforcement learning to seek the optimal vaccine allocation strategy for the high-degree spatial-temporal disease evolution system. The graph neural network is used to effectively capture the structural properties of the mobility contact network and extract the dynamic disease features. In our evaluation, the proposed framework r
    
[^56]: 潜在交互式A2C：在开放式多智能体系统中实现强化学习的改进

    Latent Interactive A2C for Improved RL in Open Many-Agent Systems. (arXiv:2305.05159v1 [cs.LG])

    [http://arxiv.org/abs/2305.05159](http://arxiv.org/abs/2305.05159)

    本论文介绍了一种基于编码-解码架构的潜在交互式A2C方法，以在多智能体系统中实现强化学习的改进。该方法显著提高了样本效率，通过学习隐藏状态和其他智能体的行为，解决了在竞争或对抗环境中从其他智能体中获得各种信息可能不可行的问题。

    

    目前广泛应用于多智能体强化学习(MARL)的方法是集中式训练，但该方法需要从其他智能体中获得各种信息，这在竞争或对抗环境中可能不可行。最近，交互式优势演员-评论家(IA2C)方法采用了分散的训练和执行，旨在从可能存在噪声的观察中预测其他智能体的行动。本文提出了一种利用编码-解码架构学习隐藏状态和其他智能体行动的潜在IA2C，我们在两个由众多智能体组成的领域中的实验结果表明，潜在IA2C通过降低方差和更快收敛显著提高了样本效率。此外，我们还介绍了这些领域的开放版本，智能体种群可能随时间变化，并对这些实例进行了评估。

    There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. But, these methods involve obtaining various types of information from the other agents, which may not be feasible in competitive or adversarial settings. A recent method, the interactive advantage actor critic (IA2C), engages in decentralized training coupled with decentralized execution, aiming to predict the other agents' actions from possibly noisy observations. In this paper, we present the latent IA2C that utilizes an encoder-decoder architecture to learn a latent representation of the hidden state and other agents' actions. Our experiments in two domains -each populated by many agents -- reveal that the latent IA2C significantly improves sample efficiency by reducing variance and converging faster. Additionally, we introduce open versions of these domains where the agent population may change over time, and evaluate on these instances as well.
    
[^57]: 改进零样本基于草图的图像检索的自适应和对齐方法

    Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval. (arXiv:2305.05144v1 [cs.CV])

    [http://arxiv.org/abs/2305.05144](http://arxiv.org/abs/2305.05144)

    本文针对零样本基于草图图像检索的跨域和语义问题，提出了一种自适应和对齐的方法，通过插入简单且轻量的域适配器重新学习草图领域的新抽象概念，并通过明确对齐学习到的图像嵌入以提高跨域表示能力。

    

    零样本基于草图的图像检索(ZS-SBIR)由于草图和照片之间的跨域本质以及已知和未知图像分布之间的语义差距而具有挑战性。先前的方法使用各种辅助信息和学习策略微调预训练模型，以学习一个紧凑的特征空间，该空间 (\romannumeral 1)在草图和照片领域之间共享，(\romannumeral 2) 桥接已知和未知类别。然而，这些努力在适应领域和从已知类别传递知识方面不足。本文提出了一种有效的“自适应和对齐”方法来解决关键问题。具体而言，我们插入简单且轻量的域适配器，以学习草图领域的新的抽象概念，并提高跨域表示能力。受到最近在零样本场景下图像-文本基础模型(CLIP)的进展启发，我们明确地将学习到的图像嵌入与模型明确对齐。

    Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen image distributions. Previous methods fine-tune pre-trained models with various side information and learning strategies to learn a compact feature space that (\romannumeral1) is shared between the sketch and photo domains and (\romannumeral2) bridges seen and unseen classes. However, these efforts are inadequate in adapting domains and transferring knowledge from seen to unseen classes. In this paper, we present an effective \emph{``Adapt and Align''} approach to address the key challenges. Specifically, we insert simple and lightweight domain adapters to learn new abstract concepts of the sketch domain and improve cross-domain representation capabilities. Inspired by recent advances in image-text foundation models (\textit{e.g.}, CLIP) on zero-shot scenarios, we explicitly align the learned image embedding with a mo
    
[^58]: AI或不AI，本地购还是不本地购：真实价格的数学理论

    To AI or not to AI, to Buy Local or not to Buy Local: A Mathematical Theory of Real Price. (arXiv:2305.05134v1 [econ.TH])

    [http://arxiv.org/abs/2305.05134](http://arxiv.org/abs/2305.05134)

    本文建立了一个数学理论，用于确定代理人的最优全球与本地支出，从而实现代理人在支出和获得效用之间的最优平衡。

    

    过去几十年里，全球经济变得越来越全球化。另一方面，也有提倡“本地购”的理念，即人们购买本地生产的商品和服务而不是远离本地的那些。本文建立了一个数学理论，用于确定代理人的最优全球与本地支出，从而实现代理人在支出和获得效用之间的最优平衡。我们的真实价格理论依赖于与生产者和消费者网络相关的马尔可夫链转移概率矩阵的渐近分析。我们表明，产品或服务的真实价格可以从涉及的马尔可夫链矩阵中确定，并且可能与产品的标签价格显著不同。特别地，我们表明，产品和服务的标签价格通常不是“真实的”或直接“有用的”：如果提供相同的近视效用，价格更低的那个产品会更好。

    In the past several decades, the world's economy has become increasingly globalized. On the other hand, there are also ideas advocating the practice of ``buy local'', by which people buy locally produced goods and services rather than those produced farther away. In this paper, we establish a mathematical theory of real price that determines the optimal global versus local spending of an agent which achieves the agent's optimal tradeoff between spending and obtained utility. Our theory of real price depends on the asymptotic analysis of a Markov chain transition probability matrix related to the network of producers and consumers. We show that the real price of a product or service can be determined from the involved Markov chain matrix, and can be dramatically different from the product's label price. In particular, we show that the label prices of products and services are often not ``real'' or directly ``useful'': given two products offering the same myopic utility, the one with low
    
[^59]: 一种基于Kriging-Random Forest混合模型的地压平衡盾构隧道实时地质预测

    A Kriging-Random Forest Hybrid Model for Real-time Ground Property Prediction during Earth Pressure Balance Shield Tunneling. (arXiv:2305.05128v1 [cs.LG])

    [http://arxiv.org/abs/2305.05128](http://arxiv.org/abs/2305.05128)

    该研究提出了一种Kriging-Random Forest混合模型，结合先前预测的地质信息和实时的运行参数信息，为地压平衡盾构机前方地质预测提供指导，从而缓解施工风险。

    

    该文提出了一种Kriging-Random Forest混合模型，通过将Kriging外推算法和Random Forest相结合，为地压平衡盾构机前方地质预测提供指导，从而缓解施工风险。该算法同时利用了先前预测的地质信息和实时的运行参数信息进行预测，运用加权平均方法将预测结果结合，使得预测结果具有最小的不确定性。

    A kriging-random forest hybrid model is developed for real-time ground property prediction ahead of the earth pressure balanced shield by integrating Kriging extrapolation and random forest, which can guide shield operating parameter selection thereby mitigate construction risks. The proposed KRF algorithm synergizes two types of information: prior information and real-time information. The previously predicted ground properties with EPB operating parameters are extrapolated via the Kriging algorithm to provide prior information for the prediction of currently being excavated ground properties. The real-time information refers to the real-time operating parameters of the EPB shield, which are input into random forest to provide a real-time prediction of ground properties. The integration of these two predictions is achieved by assigning weights to each prediction according to their uncertainties, ensuring the prediction of KRF with minimum uncertainty. The performance of the KRF algori
    
[^60]: 使用数据内核比较基础模型

    Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])

    [http://arxiv.org/abs/2305.05126](http://arxiv.org/abs/2305.05126)

    本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。

    

    最近自主学习和神经网络扩展的进展使得可以创建大型基础模型，这些模型可以轻松地适应各种下游任务。目前比较基础模型的范式涉及在各种策划数据集上使用聚合指标进行基准测试。不幸的是，这种模型比较方法严重依赖于度量指标的选择，这使得它在理想度量不明显或不可用的情况下不适用。在这项工作中，我们提出了一种没有度量指标的基础模型比较方法，通过它们的嵌入空间几何来实现。我们的方法基于随机图理论，并促进点对点和多模型比较。此外，我们展示了如何使用我们的框架诱导一组配备有与一些下游指标强相关的距离函数的模型流形。

    Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
    
[^61]: 基于双重注意力网络的弹性车间调度问题的深度强化学习

    Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])

    [http://arxiv.org/abs/2305.05119](http://arxiv.org/abs/2305.05119)

    本文提出了一种基于深度强化学习和自注意模型的端到端学习框架，通过双重注意力网络来准确而简洁地表示操作和机器之间错综复杂的关系，以协同地确定FJSP的优先级分配规则。

    

    弹性制造催生了复杂的调度问题，如弹性车间调度问题（FJSP）。在FJSP中，操作可以在多台机器上进行处理，导致操作和机器之间存在错综复杂的关系。最近的研究利用深度强化学习（DRL）来学习优先级分配规则（PDRs）以解决FJSP。然而，相对于诸如OR-Tools等精确方法的解决方案，解决方案的质量仍有提高的空间。为了解决这个问题，本文提出了一种新的端到端学习框架，结合了自注意模型进行深度特征提取和DRL进行可扩展决策制定的优点。操作和机器之间的复杂关系被准确而简洁地表示出来，提出了一个由多个相互连接的操作信息注意块和机器信息注意块组成的双注意力网络（DAN）。DAN利用这些复杂的关系，以协同地确定FJSP的PDRs。

    Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
    
[^62]: 当手上有一个CBR比丛林里的两个更好时

    When a CBR in Hand is Better than Twins in the Bush. (arXiv:2305.05111v1 [cs.LG])

    [http://arxiv.org/abs/2305.05111](http://arxiv.org/abs/2305.05111)

    本文讨论了一个回归问题——预测航班起飞延误。一个由 XGB-CBR Twin 转换来的 CBR 模型提供了最准确的局部预测、最易解释的局部解释表示和全局重要性的维护。

    

    被称为可解释的AI方法常被支持解释性与准确性之间存在权衡的人贬低为不准确。然而，在许多问题的情况下，这种权衡并不存在。本文讨论了一个回归问题——预测航班起飞延误，在这个问题中，使用梯度提升决策树的XGBoost实现训练了最精确的数据回归模型。而在构建XGB-CBR Twin并将XGBoost特征重要性转换为CBR模型中的全局权重时，结果单独使用的CBR模型提供了最准确的局部预测，保持了全局重要性，提供了最易解释的局部解释表示。这个结果的CBR模型成为了这个问题情境下准确性和解释性的基准，从而用于评估两种添加特征属性方法SHAP和LIME来解释XGBoost回归模型。

    AI methods referred to as interpretable are often discredited as inaccurate by supporters of the existence of a trade-off between interpretability and accuracy. In many problem contexts however this trade-off does not hold. This paper discusses a regression problem context to predict flight take-off delays where the most accurate data regression model was trained via the XGBoost implementation of gradient boosted decision trees. While building an XGB-CBR Twin and converting the XGBoost feature importance into global weights in the CBR model, the resultant CBR model alone provides the most accurate local prediction, maintains the global importance to provide a global explanation of the model, and offers the most interpretable representation for local explanations. This resultant CBR model becomes a benchmark of accuracy and interpretability for this problem context, and hence it is used to evaluate the two additive feature attribute methods SHAP and LIME to explain the XGBoost regressio
    
[^63]: 面向危及生命的室性心律失常检测的微小机器学习设计竞赛

    TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])

    [http://arxiv.org/abs/2305.05105](http://arxiv.org/abs/2305.05105)

    TDC'22是第一届面向ICDs低功耗微控制器的人工智能/机器学习（AI/ML）算法创新竞赛。本次竞赛的挑战是开发一种基于AI/ML的新型实时检测算法，对危及生命的室性心律失常进行检测。

    

    第一届ACM/IEEE微小机器学习设计竞赛（TDC）于2022年在第41届计算机辅助设计国际会议（ICCAD）上举行，是一项具有挑战性的多月研发竞赛。TDC'22专注于需要在可植入设备上创新和实现人工智能/机器学习（AI/ML）算法的真实医疗问题。TDC'22的挑战问题是开发一种基于AI/ML的新型实时检测算法，用于心脏除颤器（ICDs）上使用的低功率微控制器对危及生命的室性心律失常进行检测。数据集包含来自90个受试者的8种不同心律类型的超过38,000个5秒心内电图（IEGM）片段。专用硬件平台是STMicroelectronics制造的NUCLEO-L432KC。TDC'22面向全球多人团队，吸引了来自50多个组织的150多支队伍参赛。本文首先介绍这一医疗问题，

    The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
    
[^64]: 谁需要解码器？高效预测序列级属性。（arXiv:2305.05098v1 [cs.LG]）

    Who Needs Decoders? Efficient Estimation of Sequence-level Attributes. (arXiv:2305.05098v1 [cs.LG])

    [http://arxiv.org/abs/2305.05098](http://arxiv.org/abs/2305.05098)

    研究提出了非自回归代理模型(NAP)，通过编码序列直接预测通用标量值序列级属性，可以高效地实现解码步骤的规避。在机器翻译和语音识别两个场景下，NAP分别可以优于深度集成和高精度地预测性能指标。

    

    现代化序列到序列的模型通常需要自回归解码，这往往非常消耗资源。然而，对于某些下游任务，例如越界检测和资源分配，实际解码输出并不需要，只需要一个序列的标量属性。在这些场景下，知道系统输出质量以预测性能较差比知道输出本身更为重要，那么是否可以绕过自回归解码？我们提出了非自回归代理（NAP）模型，可以高效地预测通用标量值序列级属性。重要的是，NAP直接从编码预测这些指标，避免了昂贵的自回归解码阶段。我们考虑了两个序列到序列任务：机器翻译（MT）和语音识别（ASR）。在MT的越界检测中，NAP表现优于深度集成，同时速度显著更快。NAP也被证明能够高准确度地预测ASR的性能指标，例如词错误率。我们的发现表明，在属性可以从编码中直接预测的任务中，NAP为传统基于解码的方法提供了高效的替代方案。

    State-of-the-art sequence-to-sequence models often require autoregressive decoding, which can be highly expensive. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed just a scalar attribute of this sequence. In these scenarios, where for example knowing the quality of a system's output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict general scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive autoregressive decoding stage. We consider two sequence-to-sequence task: Machine Translation (MT); and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while being significantly faster. NAPs are also shown to be able to predict performance me
    
[^65]: 少即是多：移除文本区域提高CLIP训练效率和鲁棒性

    Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness. (arXiv:2305.05095v1 [cs.CV])

    [http://arxiv.org/abs/2305.05095](http://arxiv.org/abs/2305.05095)

    本文提出两种方法来提高CLIP培训效率和鲁棒性：（1）通过增加训练数据集来提高效率，（2）过滤掉图像中带有文本区域的样本以增强鲁棒性和防御印刷攻击，从而在ImageNet和Coco等公共基准测试上显着提高了分类和检索准确性。

    

    CLIP（对比语言-图像预训练）模型及其变体正在成为许多应用的事实标准。然而，从数亿个图像-文本对中训练CLIP模型可能会因成本高昂而不切实际。此外，传统的CLIP模型不区分嵌入图像中的文本区域的视觉语义和含义。当嵌入区域的文本与图像的视觉外观不匹配时，这可能导致非鲁棒性。在本文中，我们讨论了两种有效的方法来提高CLIP培训的效率和鲁棒性：（1）增加训练数据集，同时保持相同数量的优化步骤，以及（2）过滤图像中包含文本区域的样本。通过这样做，我们显着提高了公共基准测试（如ImageNet和CoCo）的分类和检索准确性。同时，过滤带有文本区域的图像还可以保护模型免受印刷攻击。为了验证这一点，我们......

    The CLIP (Contrastive Language-Image Pre-training) model and its variants are becoming the de facto backbone in many applications. However, training a CLIP model from hundreds of millions of image-text pairs can be prohibitively expensive. Furthermore, the conventional CLIP model doesn't differentiate between the visual semantics and meaning of text regions embedded in images. This can lead to non-robustness when the text in the embedded region doesn't match the image's visual appearance. In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image. By doing so, we significantly improve the classification and retrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering out images with text regions also protects the model from typographic attacks. To verify this, we 
    
[^66]: 3GPP 5G-Advanced中的人工智能：一项调查

    Artificial Intelligence in 3GPP 5G-Advanced: A Survey. (arXiv:2305.05092v1 [cs.NI])

    [http://arxiv.org/abs/2305.05092](http://arxiv.org/abs/2305.05092)

    本文综合介绍了3GPP Release 18在5G-Advanced中关于人工智能的最新发展情况，包括多样化研究和工作项以及设计方面的各个方面。

    

    人工智能正在改变全球各行各业，电信行业也不例外。标准化对于实现电信领域中人工智能的广泛应用至关重要。第三代合作伙伴计划（3GPP）Release 18是包括专门处理人工智能多样化研究和工作项的5G-Advanced的第一个版本。本文综合概述了3GPP在5G-Advanced中关于人工智能的最新发展情况，将各种3GPP Release-18人工智能活动作为有机整体呈现，详细解释设计方面的各个方面，并分享影响标准化的各种设计原理。

    Industries worldwide are being transformed by artificial intelligence (AI), and the telecom industry is no different. Standardization is critical for industry alignment to achieve widespread adoption of AI in telecom. The 3rd generation partnership project (3GPP) Release 18 is the first release of 5G-Advanced, which includes a diverse set of study and work items dedicated to AI. This article provides a holistic overview of the state of the art in the 3GPP work on AI in 5G-Advanced, by presenting the various 3GPP Release-18 activities on AI as an organic whole, explaining in detail the design aspects, and sharing various design rationales influencing standardization.
    
[^67]: 交互式文本游戏中基于知识增强的代理

    Knowledge-enhanced Agents for Interactive Text Games. (arXiv:2305.05091v1 [cs.CL])

    [http://arxiv.org/abs/2305.05091](http://arxiv.org/abs/2305.05091)

    本文提出了一个知识加强的代理框架，用于加强代理在文本游戏中的功能基础，具有记忆先前操作和环境对象可行性两项领域知识，支持三个代表性模型类。

    

    自然语言的交流是智能不可或缺的一个方面，需要计算模型学习和推理有关世界概念的知识，其监督程度也各不相同。本文提出了一个框架，以加强代理在基于文本的游戏中的功能基础。具体而言，我们考虑将两种领域知识注入基于学习的代理中：先前正确操作的记忆和环境中相关对象的可行性。我们的框架支持三个代表性模型类：`纯`强化学习模型，基于记忆的序列到序列 (seq2seq) 模型和基于注意机制的 seq2seq模型。

    Communication via natural language is a crucial aspect of intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. While there has been significant progress made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding, much of the community has turned to various sequential interactive tasks, as in semi-Markov text-based games, which have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a framework for enabling improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports three representative model classes: `pure' reinforcement learning
    
[^68]: 大数据时代的地球移动者: 最优输运在机器学习中的回顾

    Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning. (arXiv:2305.05080v1 [cs.LG])

    [http://arxiv.org/abs/2305.05080](http://arxiv.org/abs/2305.05080)

    本文回顾了最优输运在机器学习中的应用，并探讨了如何将其扩展以适应大数据和高维数据的需求。

    

    最优输运(OT)是一个数学框架,首次出现于18世纪,并引发出大量方法来回答许多理论和应用问题。过去的十年见证了这个经典优化问题对机器学习的显着贡献。本文探讨了最优输运在机器学习中的使用方式及其扩展的问题。在专题与背景的允许下,我们提供了关于最优输运的全面调查,并确保其呈现具有可访问性。首先,我们解释了最优输运的背景,并介绍了不同的类型、特性和显著应用。然后,我们着重探讨了如何将最优输运扩展以应对当前大数据和高维数据的需求。我们对用于扩展OT的文献方法进行了系统分析,并以结构化的方式呈现结果以促进理解。最后,我们探讨了可扩展最优输运在机器学习中未来研究的一些最有前途的方向。

    Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the find
    
[^69]: 不公开隐私的分布式相机网络和边缘计算在室内定位和多人跟踪中的应用

    Indoor Localization and Multi-person Tracking Using Privacy Preserving Distributed Camera Network with Edge Computing. (arXiv:2305.05062v1 [cs.CV])

    [http://arxiv.org/abs/2305.05062](http://arxiv.org/abs/2305.05062)

    该研究提出了一个开源、低成本、可扩展且保护隐私的边缘计算框架，用于在室内空间中估计多人的位置、朝向和轨迹。

    

    在建筑环境中定位个体是一个日益成长的研究课题。通过估计人们在空间中的位置、面部朝向（或凝视方向）和轨迹，可以有许多用途，例如在人群管理、安全和医疗方面。本研究提出了一个开源、低成本、可扩展且保护隐私的边缘计算框架用于多人定位，即在室内空间中估计多人的位置、朝向和轨迹。

    Localization of individuals in a built environment is a growing research topic. Estimating the positions, face orientation (or gaze direction) and trajectories of people through space has many uses, such as in crowd management, security, and healthcare. In this work, we present an open-source, low-cost, scalable and privacy-preserving edge computing framework for multi-person localization, i.e. estimating the positions, orientations, and trajectories of multiple people in an indoor space. Our computing framework consists of 38 Tensor Processing Unit (TPU)-enabled edge computing camera systems placed in the ceiling of the indoor therapeutic space. The edge compute systems are connected to an on-premise fog server through a secure and private network. A multi-person detection algorithm and a pose estimation model run on the edge TPU in real-time to collect features which are used, instead of raw images, for downstream computations. This ensures the privacy of individuals in the space, re
    
[^70]: ANALOGICAL- 一种新的大语言模型文本类比评测基准

    ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])

    [http://arxiv.org/abs/2305.05050](http://arxiv.org/abs/2305.05050)

    本文介绍了一种名为“ANALOGICAL”的新型基准，用以内在评估LLMs在长文本类比中的能力，包括六个复杂级别的长文本类比分类，并使用13个数据集和三种距离度量方法来评估8个LLMs在语义向量空间中识别类比对的能力。

    

    在过去的十年中，以词级别的类比为形式的类比在衡量诸如word2vec之类的词嵌入方法的质量方面发挥了重要作用。然而，现代的大型语言模型(LLMs)主要根据GLUE和SuperGLUE等基准的外在量度进行评估，而在LLMs是否能够在长文本中绘制类比的方面，只有少数几项研究。本文介绍了一种名为“ANALOGICAL”的新型基准，以六个复杂级别的长文本类比分类对LLMs进行内在评估，分别为 (i)单词、(ii)单词vs句子、(iii)语法、(iv)否定、(v)蕴含和(vi)隐喻。利用13个数据集和三种不同的距离度量方法，我们评估了8个LLMs在语义向量空间中识别类比对的能力(例如，“我能说两种语言”应该更接近“我是双语的”，而“我喜欢巧克力”和“我不喜欢巧克力”应该是正交的)。

    Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
    
[^71]: 知识图谱指导下语言模型语义评估以提高用户信任

    Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust. (arXiv:2305.04989v1 [cs.CL])

    [http://arxiv.org/abs/2305.04989](http://arxiv.org/abs/2305.04989)

    本研究通过知识图谱结构评估Self-Attention变压器中编码的语义。结果显示，语言模型是概率语言模式产生的控制过程的模型，但是不将对象和概念级别的含义和语义赋予所学习的随机模式，例如知识图谱中所描述的模式。

    

    自然语言处理中一个基本的问题是：语言模型捕捉到了什么样的语言结构和语义？像知识图谱这样的图表达形式很容易进行评估，因为它们明确地表达了语言语义和结构。本研究通过利用显式的知识图谱结构来评估Self-Attention变压器中编码的语义。我们提出了新的度量标准，通过提供从知识图谱获取的图形路径序列并尝试从Self-Attention变压器模型的输出中复制/重构同样路径来测量重构误差。语言模型的不透明性对于信任和可解释决策结果等社会问题有着巨大的影响。我们的研究发现，语言模型是概率语言模式产生的控制过程的模型，但是它们不将对象和概念级别的含义和语义赋予所学习的随机模式，例如知识图谱中所描述的模式。

    A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowl
    
[^72]: 从关系池化到子图图神经网络：更具表现力的图神经网络的通用框架

    From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks. (arXiv:2305.04963v1 [cs.LG])

    [http://arxiv.org/abs/2305.04963](http://arxiv.org/abs/2305.04963)

    本研究提出了一种从关系池化到子图GNN的通用框架，可提高任何基本GNN模型的表现力，通过明确标记节点作为附加特征来实现此目的， 并可在许多syn上实现超越性能。

    

    关系池化是用于构建更具表现力和置换不变的图神经网络的框架。然而，对于关系池化在表现力方面的确切增强及其与Weisfeiler-Lehman分层的联系的理解是有限的。从关系池化出发，我们提出了将节点明确标记为附加特征以提高消息传递神经网络的表现力的方法。然后将该方法扩展到高维WL，得到一种新的$k,l$-WL算法，比$k$-WL更通用的框架。我们在理论上分析了$k,l$-WL相对于$k$和$l$的表达能力，并将其与大量的子图GNN统一起来。我们还系统地讨论了复杂度降低方法，以构建强大而实用的$k,l$-GNN实例。我们在理论和实验中证明了我们的方法是通用兼容的，并能够提高任何基本GNN模型的表现力。我们的$k,l$-GNN在许多syn上实现了超越性能。

    Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve expressive power of message passing neural networks. The method is then extended to higher dimensional WL, leading to a novel $k,l$-WL algorithm, a more general framework than $k$-WL. Theoretically, we analyze the expressivity of $k,l$-WL with respect to $k$ and $l$ and unifies it with a great number of subgraph GNNs. Complexity reduction methods are also systematically discussed to build powerful and practical $k,l$-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our $k,l$-GNNs achieve superior performance on many syn
    
[^73]: 早起的鸟儿捉到虫：利用编码器模型的早期层进行更有效的代码分类

    The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])

    [http://arxiv.org/abs/2305.04940](http://arxiv.org/abs/2305.04940)

    本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。

    

    现代自然语言处理技术在软件工程任务如漏洞检测和类型推理方面表现出了卓越的优势。然而，训练深度自然语言处理模型需要大量计算资源。本文探讨了一些技术，旨在实现这些模型中资源和可用信息的最佳利用。我们提出了一种通用的方法EarlyBIRD，从预训练的transformer模型的早期层构建代码的复合表示。我们通过比较12种创建复合表示的策略与仅使用最后一个编码器层的标准实践，在CodeBERT模型上实证研究了这种方法的可行性。我们在4个数据集上的评估表明，几个早期层的组合在缺陷检测方面产生更好的性能，而一些组合则改进了多类分类。具体而言，我们获得了平均检测增强2。

    The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
    
[^74]: 基于Transformer的零样本和少样本生物医学命名实体识别方法

    A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])

    [http://arxiv.org/abs/2305.04928](http://arxiv.org/abs/2305.04928)

    本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。

    

    在生物医学领域中，有监督的命名实体识别（NER）依赖于具有给定命名实体的大量注释文本，其创建可能耗时且昂贵。此外，提取新实体通常需要进行额外的注释任务和重新训练模型。为解决这些挑战，本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。该方法基于将多类标记分类任务转换为二元标记分类（标记包含搜索的实体或不包含搜索的实体），并在更多的数据集和生物医学实体上进行预训练，从而可学习到给定和潜在类别之间的语义关系。在9种不同的生物医学实体上，我们在零样本NER、一次样本NER、10次样本NER和100次样本NER上实现了平均F1得分分别为35.44％、50.10％、69.94％和79.51％。

    Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
    
[^75]: 提前检测和推理删除推文

    Detecting and Reasoning of Deleted Tweets before they are Posted. (arXiv:2305.04927v1 [cs.CL])

    [http://arxiv.org/abs/2305.04927](http://arxiv.org/abs/2305.04927)

    本研究提出了一种新的深度学习框架，可以在推文发布之前识别出即将被删除的内容，并推理其潜在危害和违反平台政策的原因。该方法可用于推进更安全和负责任的社交媒体使用。

    

    社交媒体平台在信息传播和消费等方面给我们带来了许多便利。然而，这些平台也存在着被滥用的潜在风险。恶意用户用它们来散播仇恨言论、攻击性内容、谣言等，以获得社会和政治议程，或者伤害个人、实体和组织。通常情况下，一些用户在未经验证的情况下无意中分享信息，或者无意中发布有害信息。一些此类内容经常被平台删除，可能是因为违反了条款和政策，也可能是由于用户自己的不同原因，比如后悔了。目前有许多研究对删除内容进行了表征、理解和预测。然而，旨在识别删除的细致原因（例如，帖子令人反感、仇恨言论或没有可识别的原因）的研究是有限的。在本研究中，我们通过识别即将发布的被删除的推文，并推理它们的潜在危害和违反平台政策的原因来填补这一空白。我们提出了一个基于深度学习的框架，利用推文的各种特征来预测它们是否会被删除，如果会被删除，则是为什么。我们的方法在识别删除的推文及其原因方面取得了高准确性，可以成为促进更安全和负责任的社交媒体使用的有价值的工具。

    Social media platforms empower us in several ways, from information dissemination to consumption. While these platforms are useful in promoting citizen journalism, public awareness etc., they have misuse potentials. Malicious users use them to disseminate hate-speech, offensive content, rumor etc. to gain social and political agendas or to harm individuals, entities and organizations. Often times, general users unconsciously share information without verifying it, or unintentionally post harmful messages. Some of such content often get deleted either by the platform due to the violation of terms and policies, or users themselves for different reasons, e.g., regrets. There is a wide range of studies in characterizing, understanding and predicting deleted content. However, studies which aims to identify the fine-grained reasons (e.g., posts are offensive, hate speech or no identifiable reason) behind deleted content, are limited. In this study we address this gap, by identifying deleted 
    
[^76]: 基于图形遮盖自编码器的序列推荐系统

    Graph Masked Autoencoder for Sequential Recommendation. (arXiv:2305.04619v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.04619](http://arxiv.org/abs/2305.04619)

    提出了一种简单而有效的基于图遮盖自编码器的序列推荐系统，它使用基于图的注意力机制暴露出带有遮盖的项目序列，自适应动态提取全局项目转换信息进行自监督增强，在具有较少标记样本的情况下始终比最先进的序列推荐方法表现出更好的性能，而且对数据损坏和缺失情况具有鲁棒性。

    

    虽然一些强大的神经网络架构（例如Transformer、图神经网络）通过高阶项依赖建模在序列推荐中实现了改进的性能，但它们可能在标签稀缺情况下表现出较差的表征能力。为了解决标签不足的问题，对比学习（CL）已经引起了近期的关注，通过嵌入对比来进行自我监督的数据增强。然而，由于其对比视图生成策略的手工制定特性，现有的CL增强模型不仅难以在不同的序列推荐任务中产生一致的性能，还可能对用户行为数据噪声不具有鲁棒性。鉴于这一点，我们提出了一种简单而有效的自适应全局信息提取的图遮盖自编码器增强的序列推荐系统（MAERec）来解决这个问题。它自然地避免了上述问题，得益于其独特的数据重构机制。具体而言，我们的模型使用基于图的注意力机制，暴露出带有遮盖的项目序列，使表示不仅利用本地顺序信息，还利用项目之间的全局相关性。我们在四个基准数据集上对我们的方法进行了广泛评估。实验结果表明，我们的模型在具有较少标记样本的情况下始终比最先进的序列推荐方法表现出更好的性能，而且对数据损坏和缺失情况具有鲁棒性。

    While some powerful neural network architectures (e.g., Transformer, Graph Neural Networks) have achieved improved performance in sequential recommendation with high-order item dependency modeling, they may suffer from poor representation capability in label scarcity scenarios. To address the issue of insufficient labels, Contrastive Learning (CL) has attracted much attention in recent methods to perform data augmentation through embedding contrasting for self-supervision. However, due to the hand-crafted property of their contrastive view generation strategies, existing CL-enhanced models i) can hardly yield consistent performance on diverse sequential recommendation tasks; ii) may not be immune to user behavior data noise. In light of this, we propose a simple yet effective Graph Masked AutoEncoder-enhanced sequential Recommender system (MAERec) that adaptively and dynamically distills global item transitional information for self-supervised augmentation. It naturally avoids the abov
    
[^77]: 更多的后代数量有助于$(1 + (\lambda, \lambda))$遗传算法克服噪声

    Larger Offspring Populations Help the $(1 + (\lambda, \lambda))$ Genetic Algorithm to Overcome the Noise. (arXiv:2305.04553v1 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2305.04553](http://arxiv.org/abs/2305.04553)

    更多的后代数量有助于$(1 + (\lambda, \lambda))$遗传算法在噪声环境下表现更鲁棒。

    

    进化算法已知对于适应度的评估噪声具有鲁棒性，尤其是更多的后代数量通常会导致更强的鲁棒性。我们分析了$(1 + (\lambda, \lambda))$遗传算法对噪声的鲁棒性程度。该算法也使用了更多的后代数量，但中间的选择步骤和对交叉方式的非标准使用作为修复机制可能使该算法不如简单的$(1+\lambda)$进化算法鲁棒。我们对几个经典基准问题的实验分析表明，这个难点并不会出现。令人惊讶的是，在许多情况下，该算法甚至比$(1+\lambda)$ EA更鲁棒。

    Evolutionary algorithms are known to be robust to noise in the evaluation of the fitness. In particular, larger offspring population sizes often lead to strong robustness. We analyze to what extent the $(1+(\lambda,\lambda))$ genetic algorithm is robust to noise. This algorithm also works with larger offspring population sizes, but an intermediate selection step and a non-standard use of crossover as repair mechanism could render this algorithm less robust than, e.g., the simple $(1+\lambda)$ evolutionary algorithm. Our experimental analysis on several classic benchmark problems shows that this difficulty does not arise. Surprisingly, in many situations this algorithm is even more robust to noise than the $(1+\lambda)$~EA.
    
[^78]: 跨模态相似性调节的对比学习在视觉语言预训练中的应用

    Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation. (arXiv:2305.04474v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.04474](http://arxiv.org/abs/2305.04474)

    本文提出了一种跨模态相似性逐步细化的对比学习策略，在视觉语言预训练中优化图像/文本锚点与其负样本文本/图像之间的互信息，有效应对了（部分）误反样本的挑战。

    

    在视觉语言预训练中，跨模态对比学习面临着（部分）误反样本的挑战。本文从 mutual information 优化的角度研究了这个问题。我们理论上证明了在存在噪声的情况下，涉及到负样本的互信息也很重要。我们提出了一种跨模态相似性逐步细化的对比学习策略，以更加精确地优化图像/文本锚点与其负样本文本/图像之间的互信息。我们的方法在四个下游跨模态任务上表现出竞争力，并在理论指导下系统地平衡了（部分）误反样本的有益影响和有害影响。

    Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
    
[^79]: 针对时尚检测的不平衡标签样本分布的数据高效训练

    Data Efficient Training with Imbalanced Label Sample Distribution for Fashion Detection. (arXiv:2305.04379v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.04379](http://arxiv.org/abs/2305.04379)

    本文提出了一种最先进的加权目标函数，用于提高多标签分类中深度神经网络（DNN）针对长尾数据分布的性能，并通过对时尚服装的图像属性分类的实验，取得了良好的性能。

    

    多标签分类模型在电子商务中有广泛的应用，包括基于视觉的标签预测以及基于语言的情感分类。实现这些任务的一个主要难点是数据分布的显著不平衡。为了解决这个问题，本文探索了更多的数据高效模型训练技术，并提出了一种最先进的加权目标函数，用于提高多标签分类中深度神经网络（DNN）针对长尾数据分布的性能。我们的实验涉及时尚服装的基于图像的属性分类，并且结果表明，新的加权目标函数相较于传统方法具有良好的性能。

    Multi-label classification models have a wide range of applications in E-commerce, including visual-based label predictions and language-based sentiment classifications. A major challenge in achieving satisfactory performance for these tasks in the real world is the notable imbalance in data distribution. For instance, in fashion attribute detection, there may be only six 'puff sleeve' clothes among 1000 products in most E-commerce fashion catalogs. To address this issue, we explore more data-efficient model training techniques rather than acquiring a huge amount of annotations to collect sufficient samples, which is neither economic nor scalable. In this paper, we propose a state-of-the-art weighted objective function to boost the performance of deep neural networks (DNNs) for multi-label classification with long-tailed data distribution. Our experiments involve image-based attribute classification of fashion apparels, and the results demonstrate favorable performance for the new weig
    
[^80]: RATs-NAS：GCN上的相邻操作轨迹重定向用于神经结构搜索

    RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture Search. (arXiv:2305.04206v1 [cs.CV])

    [http://arxiv.org/abs/2305.04206](http://arxiv.org/abs/2305.04206)

    本论文提出了一种被称为RATs-NAS的神经结构搜索方法，通过在GCN上重定向相邻操作轨迹来快速搜索最佳神经网络架构，实验结果表明这种方法比其他最先进方法更有效。

    

    许多手工设计的卷积神经网络如VGG、ResNet、DenseNet等，在不同的任务上达到了最先进的水平。神经结构搜索（NAS）现在专注于自动找到最佳CNN架构来处理上述任务。然而，验证搜索架构非常耗时，使基于预测器的方法成为NAS的一个基本而重要的分支。建立预测器的两种常用技术是图卷积网络（GCN）和多层感知器（MLP）。本文考虑GCN和MLP在相邻操作轨迹上的差异，提出了Redirected Adjacent Trails NAS（RATs-NAS），以快速搜索所需的神经网络架构。RATs-NAS包括两个组件：Redirected Adjacent Trails GCN（RATs-GCN）和基于预测器的搜索空间抽样（P3S）模块。 RATs-GCN可以改变轨迹及其强度以搜索更好的神经网络架构，而P3S模块则对搜索空间进行抽样以提高预测架构的精度。实验结果表明，与其他最先进的方法相比，RATs-NAS可以更快地找到具有竞争力的神经网络架构。

    Various hand-designed CNN architectures have been developed, such as VGG, ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on different tasks. Neural Architecture Search (NAS) now focuses on automatically finding the best CNN architecture to handle the above tasks. However, the verification of a searched architecture is very time-consuming and makes predictor-based methods become an essential and important branch of NAS. Two commonly used techniques to build predictors are graph-convolution networks (GCN) and multilayer perceptron (MLP). In this paper, we consider the difference between GCN and MLP on adjacent operation trails and then propose the Redirected Adjacent Trails NAS (RATs-NAS) to quickly search for the desired neural network architecture. The RATs-NAS consists of two components: the Redirected Adjacent Trails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S) module. RATs-GCN can change trails and their strengths to search for a better neur
    
[^81]: 基于无线通信的通道驱动随机梯度 Langevin 动力学贝叶斯联邦平均

    Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])

    [http://arxiv.org/abs/2305.04152](http://arxiv.org/abs/2305.04152)

    本文提出了无线 FALD 协议，可以在无噪声通信的情况下高效地在无线系统中实现分布式贝叶斯学习，实现了在通信回合之间多个本地更新以及由小批量计算的随机梯度，并进行了样本收敛分析。

    

    可扩展贝叶斯推理方法的近期发展已经重新引起了对采用贝叶斯学习作为传统频率学习的替代方法的兴趣，其通过不确定性量化提供了改进的模型校准。最近，引入了联邦平均 Langevin 动力学(FALD)作为联邦平均的变体，可以在没有噪声的通信存在下有效地实现分布式贝叶斯学习。在本文中，我们提出了无线 FALD(WFALD)，这是一种新颖的协议，通过集成基于空中计算和基于通道驱动的 Monte Carlo 更新来实现无线系统中的 FALD。与先前的无线贝叶斯学习相比，WFALD 可以实现(i) 在通信回合之间多个本地更新；并且(ii) 由小批量计算的随机梯度。以 2-Wasserstein 距离为衡量标准，给出了样本收敛分析。

    The recent development of scalable Bayesian inference methods has renewed interest in the adoption of Bayesian learning as an alternative to conventional frequentist learning that offers improved model calibration via uncertainty quantification. Recently, federated averaging Langevin dynamics (FALD) was introduced as a variant of federated averaging that can efficiently implement distributed Bayesian learning in the presence of noiseless communications. In this paper, we propose wireless FALD (WFALD), a novel protocol that realizes FALD in wireless systems by integrating over-the-air computation and channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless Bayesian learning, WFALD enables (\emph{i}) multiple local updates between communication rounds; and (\emph{ii}) stochastic gradients computed by mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein distance between the samples produced by WFALD and the targeted global posterior distribut
    
[^82]: 离散扩散建模下的高效和度数引导图生成

    Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])

    [http://arxiv.org/abs/2305.04111](http://arxiv.org/abs/2305.04111)

    本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。

    

    基于扩散的生成图模型已被证明在生成高质量小图方面非常有效。然而，它们需要更可扩展性，以生成包含数千个节点的大图并满足图统计。本文提出了EDGE，一种新的基于扩散的生成图模型，用于生成大型图的生成任务。为了提高计算效率，我们通过在每个时间步长随机删除边来鼓励图的稀疏性，并最终获得一张空白图。EDGE仅在每个去噪步骤中关注图中一部分节点。它比以前的基于扩散的模型更少地进行边预测。此外，EDGE明确地允许对图的节点度数进行建模，进一步提高了模型的性能。实证研究表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。它还在生成质量方面优于基准模型。

    Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
    
[^83]: Diffusion Explainer：用于文本到图像稳定扩散的可视化解释工具

    Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])

    [http://arxiv.org/abs/2305.03509](http://arxiv.org/abs/2305.03509)

    Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。

    

    基于扩散的生成模型通过创造逼真的图像而获得了全球关注。然而，它们复杂的内部结构和操作往往使得非专业人员难以理解。我们提出了 Diffusion Explainer，这是第一个交互式可视化工具，用于解释稳定扩散如何将文本提示转化为图像。Diffusion Explainer紧密地将稳定扩散的复杂组件的视觉概述与其潜在操作的详细说明相结合，通过动画和交互元素使用户可以流畅地在多个抽象级别之间过渡。通过比较由两个相关文本提示引导的图像表示的演变来指导精细时间步长，用户可以发现提示对图像生成的影响。Diffusion Explainer在用户的Web浏览器中本地运行，无需安装或专门的硬件，扩大了公众对现代人工智能技术的教育获取。

    Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
    
[^84]: 主动对话系统综述：问题、方法和展望

    A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v1 [cs.CL])

    [http://arxiv.org/abs/2305.02750](http://arxiv.org/abs/2305.02750)

    本综述全面概述了不同类型对话中对话代理主动性的突出问题和先进设计，讨论了符合实际应用需求但需要未来更大研究重点的挑战，激发更多的会话 AI 进展到下一级别。

    

    主动对话系统与广泛的现实世界对话应用相关，使对话代理能够引导对话方向，以实现预定义的目标或满足系统方面的特定目标。它通过先进技术赋能以进展到需要战略性和激励性交互的更复杂任务。在本综述中，我们全面概述了不同类型对话中对话代理主动性的突出问题和先进设计。此外，我们还讨论了符合实际应用需求但需要未来更大研究重点的挑战。我们希望这篇主动对话系统的第一篇综述可以为社区提供快速访问和整体图片，激发更多的会话 AI 进展到下一级别。

    Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.
    
[^85]: 学习在存在隐性混淆因素的情况下从不确定数据中恢复因果关系

    Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])

    [http://arxiv.org/abs/2305.02640](http://arxiv.org/abs/2305.02640)

    本文提出了因果强度变分模型，解决了从不确定数据中恢复因果关系存在的低样本利用率和分布假设无能力的问题，同时考虑了潜在混淆因素。

    

    在具有潜在变量的因果发现中，我们定义了两个数据范式：确定数据：具有观察节点单值的单个骨架结构，和不确定数据：具有观察节点多值的一组多骨架结构。多个骨架引入低样本利用率，多个值引入了分布假设的无能力，这两者导致从不确定数据中恢复因果关系至今仍然未被充分探索。我们设计了因果强度变分模型来解决这两个问题。具体地，我们利用因果强度而不是独立噪声作为潜变量来调节证据下界。通过这种设计思想，不同骨架的因果强度被看作是一个分布，并可以表示为单值因果图矩阵。此外，考虑到潜在混淆因素，我们将因果图G分解为两个相关子图O和C。O包含观察节点之间的纯关系，而C表示混淆因素。

    In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
    
[^86]: FIREBALL：一份包含结构化游戏状态信息的Dungeons & Dragons实际游戏数据集

    FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])

    [http://arxiv.org/abs/2305.01528](http://arxiv.org/abs/2305.01528)

    本研究介绍了一份包含真实游戏状态信息的Dungeons & Dragons实际游戏数据集FIREBALL，它可以改善自然语言生成的质量。此外，LLMs可以使用FIREBALL中的游戏状态信息来生成更高质量的游戏回合。

    

    Dungeons & Dragons（D＆D）是一款桌面角色扮演游戏，其玩家之间存在复杂的自然语言交互和隐藏的状态信息。最近的研究表明，拥有状态信息的大型语言模型（LLMs）生成的游戏回合比仅使用对话历史的LLMs更具高质量。然而，以往的研究使用的游戏状态信息是启发式创建的，并不是真正的黄金标准游戏状态。我们提出了FIREBALL，这是一个包含真实游戏状态信息的大型数据集，其中包含来自Discord的近25,000个真实D＆D游戏会话。我们记录了使用Avrae机器人的玩家的游戏会话，该机器人是为了帮助人们在线玩D＆D而开发的，并捕获了语言、游戏命令和基础游戏状态信息。我们证明，通过使用Avrae状态信息，FIREBALL可以提高自然语言生成（NLG），从而提高自动评估指标和人类的质量评判。此外，我们还展示了LLMs可以生成…

    Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
    
[^87]: 触发词作为后门攻击的触发器：检查语言模型的脆弱性

    Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])

    [http://arxiv.org/abs/2305.01219](http://arxiv.org/abs/2305.01219)

    本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。

    

    基于提示的学习范例弥合了预训练和微调之间的差距，在几个NLP任务中取得了最先进的性能，尤其是在少样本情况下。尽管应用广泛，但基于提示的学习容易受到后门攻击。文本后门攻击旨在通过注入触发器并修改标签来在模型中引入有针对性的漏洞。然而，由于触发器的存在和毒瘤数据标注不正确等缺陷，这种攻击存在异常的自然语言表达。在本研究中，我们提出了一种新颖有效的“ProAttack”方法，基于提示来执行干净标签的后门攻击，使用的是提示本身作为触发器。我们的方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性。通过在丰富的资源和少样本文本语料库上的广泛实验，我们证明了ProAttack方法在保持干净数据一致性的同时显著优于现有的后门攻击方式。

    The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
    
[^88]: CSP：针对地理空间视觉表示的自监督对比空间预训练

    CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])

    [http://arxiv.org/abs/2305.01118](http://arxiv.org/abs/2305.01118)

    CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。

    

    大量的地理标记图像公开可用，而对象类别等标签则相对稀缺且收集成本高昂。同时，对比学习在各种自然图像和语言任务中取得了巨大成功，仅需很少的带标签数据。然而，现有的方法未能充分利用地理空间信息，这可能是区分视觉上相似的对象的关键。为了在预训练、微调和推理阶段直接利用与图像相关的丰富地理空间信息，我们提出了针对地理标记图像的自监督学习框架 Contrastive Spatial Pre-Training（CSP）。我们使用双编码器分别对图像及其对应的地理位置进行编码，利用对比目标从图像中学习有效的位置表示，这些表示可以转移到下游监督任务，例如图像分类。实验证明，CSP可以提高模型在大规模地理标记图像上的性能。

    Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
    
[^89]: 三元瞬时噪声逻辑系统

    Ternary Instantaneous Noise-based Logic. (arXiv:2305.00984v1 [cs.ET])

    [http://arxiv.org/abs/2305.00984](http://arxiv.org/abs/2305.00984)

    本文提出了一种三元逻辑系统，其中包含不确定的比特值和不存在的比特（真空态），与标准的二元逻辑系统相比有着显著的优势，且可以在不改变二进制算法的情况下使用。

    

    本论文提出了一种三值瞬时噪声逻辑的可能表示方法。第三个值是不确定的比特值，在人工智能应用中可能会有用。同时，还有第四个值，它可以表示不存在的比特（真空态），对于所有比特来说，它是相同的（1个数值），但却是所有比特的压缩态。一些逻辑门也进行了探讨。三元宇宙与标准的二元宇宙相比有着显著的优势：在任何时钟周期内，其幅度永远不为零。所有已知的二进制逻辑门在二元比特值上的工作方式与先前相同，因此旧的二进制算法可以在三元系统中运行，而不需要改变，并且不会出现宇宙零值所带来的问题。

    One of the possible representations of three-valued instantaneous noise-based logic is proposed. The third value is an uncertain bit value, which can be useful in artificial intelligence applications. There is a forth value, too, that can represent a non-existing bit (vacuum-state) that is the same (1 numeric value) for all bits, however that is a squeezed state common for all bits. Some logic gates are explored. The ternary Universe has a significant advantage compared to the standard binary one: its amplitude is never zero during any clock period. All the known binary logic gates work for the binary bit values in the same way as earlier therefore the former binary algorithms can be run in the ternary system with no change and without the problems posed by zero values of the Universe.
    
[^90]: 大纲先行，细节后至：基于语法引导的粗-细代码生成

    Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2305.00909](http://arxiv.org/abs/2305.00909)

    提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。

    

    对于一个复杂算法的实现，人类程序员的做法通常是先概述一下控制流程，然后迭代进行丰富，最终生成一些精心加工的语法结构和层次变量。然而，现有的大型语言模型一次性生成代码，没有中间环节，以反映"大纲先行，细节后至"的结构化思维过程。受到思维链提示的最新成功启发，我们提出了ChainCoder，这是一种程序综合语言模型，它逐步生成Python代码，即从粗到细进行多次迭代。我们首先通过抽象语法树解析将源代码分解为布局框架组件和附件组件，以构建层次表示。然后我们将预测目标重新启动，形成多次通过目标，每次生成一个子序列，这些子序列在层次结构中串联起来。最后，我们利用量身定制的Transformer体系结构来实现模型的优化。

    For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
    
[^91]: 使用模糊分箱进行校准误差估计

    Calibration Error Estimation Using Fuzzy Binning. (arXiv:2305.00543v1 [cs.LG])

    [http://arxiv.org/abs/2305.00543](http://arxiv.org/abs/2305.00543)

    本文提出了一种模糊校准误差度量（FCE），利用模糊分箱方法计算校准误差，从而缓解了概率偏斜的影响并提供了更紧密的估计值。与传统指标ECE相比，FCE在多类设置中表现更好，https://github.com/srdgFHE/FCE-paper。

    

    基于神经网络的决策往往会过于自信，其原始结果的概率并不符合真实的决策概率。神经网络的校准是实现更可靠的深度学习框架的关键步骤。先前的校准误差度量主要利用清晰的分箱成员资格度量。这加剧了模型概率的偏斜，并描绘了校准误差的不完整图像。在本文中，我们提出了一种利用模糊分箱方法计算校准误差的模糊校准误差度量（FCE）。这种方法缓解了概率偏斜的影响，并在测量校准误差时提供了更紧密的估计值。我们比较了我们的指标与ECE在不同的数据群体和类别成员身份中的表现。我们的结果显示，FCE在校准误差估计方面表现更好，特别是在多类设置中，缓解了模型置信度分数偏斜对校准误差估计的影响。我们提供了我们的代码https://github.com/srdgFHE/FCE-paper，以便未来的可重复性和使用FCE进行校准误差估计。

    Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code a
    
[^92]: 真实世界三维装箱问题的基准数据集和实例生成器

    Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems. (arXiv:2304.14712v1 [cs.AI])

    [http://arxiv.org/abs/2304.14712](http://arxiv.org/abs/2304.14712)

    本文提出了一个真实世界装箱问题的基准数据集和实例生成器，可以用来比较不同装箱算法的性能。

    

    本文提出了一个真实世界装箱问题的基准数据集。该数据集由12个实例组成，涵盖了不同大小和用户需求的问题复杂度水平（包含从38到53个包裹的数量）。实际上，我们考虑了几个面向真实世界的限制条件来构建这些实例：i)物品和箱子尺寸，ii)重量限制，iii)包类别之间的亲和性，iv)包装顺序的偏好和v)负载平衡。除了数据外，我们还提供了一个自主开发的Python脚本用于数据集生成，称为Q4RealBPP-DataGen。该基准首先被设计用于评估量子求解器，因此这组实例的特征是按照量子设备的当前限制设计的。此外，数据集生成器包含在内，允许构建通用基准。本文介绍的数据提供了一个基准，可以用来比较不同方法的性能和对比算法的效果。

    In this paper, a benchmark for real-world bin packing problems is proposed. This dataset is composed of 12 instances comprehending different levels of problem complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements. In fact, several real-world oriented restrictions have been considered for building these instances: i) items and bins dimensions, ii) weight restrictions, iii) affinities among packages categories iv) preferences for package ordering and v) load balancing. Besides the data, we also provide an own-developed Python script for the dataset generation, coined as Q4RealBPP-DataGen. The benchmark was firstly proposed to evaluate quantum solvers, therefore the characteristic of this set of instances were designed according to the current limitations of quantum devices. Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks. The data introduced on this paper provides a baseline that
    
[^93]: 论RemOve-And-Retrain的陷阱：数据处理不等式的视角

    On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])

    [http://arxiv.org/abs/2304.13836](http://arxiv.org/abs/2304.13836)

    本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。

    

    本文评估了RemOve-And-Retrain（ROAR）协议的可靠性，该协议用于测量特征重要性估计的性能。我们从理论背景和实证实验中发现，具有较少有关决策功能的信息的属性在ROAR基准测试中表现更好，与ROAR的原始目的相矛盾。这种现象也出现在最近提出的变体RemOve-And-Debias（ROAD）中，我们提出了ROAR归因度量中毛糙度偏差的一致趋势。我们的结果提醒人们不要盲目依赖ROAR的性能评估指标。

    This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
    
[^94]: Spikingformer: 基于Transformer的脉冲神经网络的脉冲驱动残差学习

    Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network. (arXiv:2304.11954v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2304.11954](http://arxiv.org/abs/2304.11954)

    提出了一种适用于硬件的SNN脉冲驱动残差学习结构，基于该结构开发了一个纯Transformer的脉冲神经网络Spikingformer，用于避免非脉冲计算并在多个数据集上实现了优异的性能表现。

    

    脉冲神经网络(SNNs)由于其事件驱动的脉冲计算，提供了一种有前途的节能替代人工神经网络的方法。然而，包括Spikformer和SEW ResNet在内的最新深度SNN存在非脉冲计算（整数浮点乘法），这是由于它们的残差连接结构所导致的。这些非脉冲计算增加了SNN的功耗，并使其不适用于只支持脉冲操作的主流神经形态硬件上。本文提出了一种适用于硬件的脉冲驱动残差学习体系结构，用于避免非脉冲计算。基于这个残差设计，我们开发了Spikingformer，这是一个纯Transformer的脉冲神经网络。我们在ImageNet、CIFAR10、CIFAR100、CIFAR10-DVS和DVS128 Gesture数据集上评估了Spikingformer，并表明作为先进骨干的直接训练的纯SNN，Spikingformer表现出比现有技术更好的性能(75.85$\%$ top-1 accuracy on ImageNet)。

    Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, state-of-the-art deep SNNs (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connection. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware, which only supports spike operations. In this paper, we propose a hardware-friendly spike-driven residual learning architecture for SNNs to avoid non-spike computations. Based on this residual design, we develop Spikingformer, a pure transformer-based spiking neural network. We evaluate Spikingformer on ImageNet, CIFAR10, CIFAR100, CIFAR10-DVS and DVS128 Gesture datasets, and demonstrate that Spikingformer outperforms the state-of-the-art in directly trained pure SNNs as a novel advanced backbone (75.85$\
    
[^95]: BloombergGPT：金融领域的大型语言模型

    BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])

    [http://arxiv.org/abs/2303.17564](http://arxiv.org/abs/2303.17564)

    本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。

    

    自然语言处理在金融技术领域有着广泛而复杂的应用，从情感分析和命名实体识别到问答。大型语言模型（LLM）已被证明在各种任务上非常有效；然而，专为金融领域设计的LLM尚未在文献中报告。在本文中，我们提出了BloombergGPT，一个拥有500亿个参数的语言模型，它是基于广泛的金融数据进行训练的。我们构建了一种3630亿个标记的数据集，该数据集基于彭博社的广泛数据来源，可能是迄今最大的领域特定数据集，同时又增加了来自通用数据集的3450亿个标记。我们在标准LLM基准、开放式金融基准和一套最能准确反映我们预期用途的内部基准上验证了BloombergGPT。我们的混合数据集训练产生了一个在金融任务上明显优于现有模型的模型，同时不会牺牲普通任务的性能。

    The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
    
[^96]: 利用长短期记忆网络提高量子电路保真度

    Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])

    [http://arxiv.org/abs/2303.17523](http://arxiv.org/abs/2303.17523)

    本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。

    

    量子计算已进入噪声中间规模量子（NISQ）时代，目前我们拥有的量子处理器对辐射和温度等环境变量敏感，因此会产生嘈杂的输出。虽然已经有许多算法和应用程序用于NISQ处理器，但我们仍面临着解释其嘈杂结果的不确定性。具体来说，我们对所选择的量子态有多少信心？这种信心很重要，因为NISQ计算机将输出其量子位测量的概率分布，有时很难区分分布是否表示有意义的计算或只是随机噪声。本文提出了一种新方法来解决这个问题，将量子电路保真度预测框架为时间序列预测问题，因此可以利用长短期记忆（LSTM）神经网络的强大能力。一个完整的工作流程来构建训练电路

    Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
    
[^97]: 合理物品集合背后的逻辑及其过滤器表示

    The logic behind desirable sets of things, and its filter representation. (arXiv:2302.08176v2 [math.LO] UPDATED)

    [http://arxiv.org/abs/2302.08176](http://arxiv.org/abs/2302.08176)

    本研究确定了合理物品集合的基本逻辑，并提出了一些简单表述方法。

    

    我们确定了最近一组合理且令人向往的物品集合理论的逻辑，这个理论推广了令人向往的赌注和一致的选择函数，我们证明这种识别允许我们建立各种表示结果，用更简单的方法来描述这种一致性模型。

    We identify the logic behind the recent theory of coherent sets of desirable (sets of) things, which generalise desirable (sets of) gambles and coherent choice functions, and show that this identification allows us to establish various representation results for such coherent models in terms of simpler ones.
    
[^98]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^99]: 优化算法的符号式发现

    Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06675](http://arxiv.org/abs/2302.06675)

    该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。

    

    我们提出了一种将算法发现视为程序搜索的方法，并应用于发现用于深度神经网络训练的优化算法。我们利用高效搜索技术来探索无限和稀疏的程序空间。为了填补代理任务和目标任务之间巨大的泛化差距，我们还引入了程序选择和简化策略。我们的方法发现了一种简单而有效的优化算法，$ \textbf {Lion} $（$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $）。它的记忆效率比Adam更高，因为它只跟踪动量。与自适应优化器不同，通过符号运算计算的每个参数的更新具有相同的大小。我们将Lion与广泛使用的优化器（例如Adam和Adafactor）进行了比较，以在不同任务上训练各种模型。在图像分类中，Lion将在ImageNet上ViT的准确性提高了最多2％，并节省了多达5倍的预训练计算时间。

    We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
    
[^100]: 创造一个哲学家的大型语言模型

    Creating a Large Language Model of a Philosopher. (arXiv:2302.01339v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.01339](http://arxiv.org/abs/2302.01339)

    该研究使用OpenAI的大型语言模型GPT-3和哲学家丹尼特的作品为训练数据，探索了生成哲学文本的能力。研究人员通过招募大量参与者来区分真正的哲学家丹尼特和机器生成的文字。专家成功率达到51％，但没有达到预期的80％，该模型有可能超越人类的思维能力。

    

    能否训练大型语言模型来生成难以与人类哲学家的文本区分的哲学文字？为了解决这个问题，我们使用哲学家丹尼特的作品作为额外的训练数据来微调OpenAI的GPT-3。为了探索丹尼特模型，我们向真正的丹尼特提出了十个哲学问题，然后向语言模型提出了相同的问题，每个问题收集了四个回答，没有进行筛选。我们招募了425名参与者来区分丹尼特的答案和四个机器生成的答案。熟悉丹尼特作品的专家（N = 25）的成功率为51％，高于20％的机会率，但不及我们预期的80％的正确率。对于其中的两个问题，语言模型至少生成了一个答案，专家们更频繁地选择该答案而非丹尼特自己的答案。哲学博客读者（N = 302）的表现与专家相似，而普通研究参与者（N = 98）则近似于随机猜测。

    Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near 
    
[^101]: 即使解释：半事实可解释人工智能（Semi-Factual XAI）的先前工作、期望和基准

    Even if Explanations: Prior Work, Desiderata & Benchmarks for Semi-Factual XAI. (arXiv:2301.11970v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11970](http://arxiv.org/abs/2301.11970)

    本文总结了反事实解释在AI系统中的应用，其中特别关注半事实解释，提出了半事实可解释人工智能的期望，并对历史算法进行了基准测试，为未来算法的发展提供了坚实的基础。

    

    最近，“可解释人工智能”（XAI）研究专注于反事实解释作为对人工智能系统决策的后评价（例如，可能告诉一个被拒绝贷款的客户：如果您要求一个更短期限的贷款，就会被批准）。反事实解释说明了对AI系统输入特征的更改如何改变输出决策。然而，还有一种类型的反事实称为半事实，虽然认知科学已经广泛研究了它们，但在人工智能领域却受到了较少关注。本文调查了这些文献，总结了这一领域的历史和最新突破。它为半事实可解释人工智能定义了关键期望，并报告了历史算法的基准测试（以及一个新颖的幼稚方法），以为未来的算法开发提供坚实的基础。

    Recently, eXplainable AI (XAI) research has focused on counterfactual explanations as post-hoc justifications for AI-system decisions (e.g. a customer refused a loan might be told: If you asked for a loan with a shorter term, it would have been approved). Counterfactuals explain what changes to the input-features of an AI system change the output-decision. However, there is a sub-type of counterfactual, semi-factuals, that have received less attention in AI (though the Cognitive Sciences have studied them extensively). This paper surveys these literatures to summarise historical and recent breakthroughs in this area. It defines key desiderata for semi-factual XAI and reports benchmark tests of historical algorithms (along with a novel, naieve method) to provide a solid basis for future algorithmic developments.
    
[^102]: 深度神经网络不安全输入计数的#DNN-Verification问题

    The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.07068](http://arxiv.org/abs/2301.07068)

    本文提出了#DNN-Verification问题，即计算违反特定安全性质的DNN输入配置数量的问题。作者提出了一种新的方法和一种随机的近似方法，分别给出了确切的违规计数和可证明概率界，并在安全关键基准测试上进行了实验比较。

    

    深度神经网络（DNN）在需要高度安全性的关键任务中，例如自动驾驶中越来越被采用。虽然最先进的验证器可以用来检查DNN是否不安全，即是否存在至少一种不安全的输入配置，但它们的是/否输出对于其他目的（如屏蔽、模型选择或培训改进）的信息不足够详细。在本文中，我们介绍了#DNN-Verification问题，它涉及计算导致DNN违反特定安全性质的输入配置数量。我们分析了这个问题的复杂性，并提出了一种新的方法，它返回确切的违规计数。由于该问题的#P完备性，我们还提出了一种随机的近似方法，该方法提供了正确计数的可证明概率界，同时显著降低了计算要求。我们在一组安全关键基准测试上呈现了实验结果，比较了我们的方法与最先进的验证器和基于计数的启发式算法。

    Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
    
[^103]: 深度全连接网络和递归学习特征的核机器的特征学习机制

    Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features. (arXiv:2212.13881v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13881](http://arxiv.org/abs/2212.13881)

    本文确定并表征了深度全连接神经网络学习特征的机制，提出了深度神经特征假设并解释了深度学习现象，同时也引领了对递归学习特征的核方法中特征学习的更广泛的理解。

    

    近年来，神经网络在许多技术和科学任务中取得了令人瞩目的成果。然而，这些模型自动选择用于预测的特征或数据模式的机制仍不清楚。确定这样的机制是推动神经网络性能和可解释性以及促进这些模型在科学应用中可靠采用的关键。在本文中，我们确定并表征了深度全连接神经网络学习特征的机制。我们提出了深度神经特征假设，该假设表明神经特征学习是通过实现平均梯度外积来加强与模型输出密切相关的特征。我们的假设揭示了各种深度学习现象，包括假特征的出现和简单性偏差以及如何修剪网络可以提高性能，《彩票假设》。此外，我们的工作中确定的机制也引领了对递归学习特征的核方法中特征学习的更广泛的理解。

    In recent years neural networks have achieved impressive results on many technological and scientific tasks. Yet, the mechanism through which these models automatically select features, or patterns in data, for prediction remains unclear. Identifying such a mechanism is key to advancing performance and interpretability of neural networks and promoting reliable adoption of these models in scientific applications. In this paper, we identify and characterize the mechanism through which deep fully connected neural networks learn features. We posit the Deep Neural Feature Ansatz, which states that neural feature learning occurs by implementing the average gradient outer product to up-weight features strongly related to model output. Our ansatz sheds light on various deep learning phenomena including emergence of spurious features and simplicity biases and how pruning networks can increase performance, the "lottery ticket hypothesis." Moreover, the mechanism identified in our work leads to a
    
[^104]: 通用状态和动作空间上的策略优化

    Policy Optimization over General State and Action Spaces. (arXiv:2211.16715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16715](http://arxiv.org/abs/2211.16715)

    本文提出了一种新方法并引入了函数近似来解决通用状态和动作空间上的强化学习问题，同时介绍了一种新的策略双平均法。

    

    通用状态和动作空间上的强化学习问题异常困难。本文提出了一种新方法，并引入了函数近似来解决这个问题。同时，还提出了一种新的策略双平均法。这些方法都可以应用于不同类型的RL问题。

    Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. 
    
[^105]: 一种简单但有效的检测代码生成偏差的方法

    A Simple, Yet Effective Approach to Finding Biases in Code Generation. (arXiv:2211.00609v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.00609](http://arxiv.org/abs/2211.00609)

    本文介绍了一种简单而有效的方法，能够检测代码生成系统中的偏差，通过对编码挑战进行模块化分解和分析，并通过自动化干预机制来暴露不良偏差，最终作为缓解策略进行数据转换技术的微调。

    

    最近，基于大型语言模型的高性能代码生成系统出现了。它们经过大规模的语料库训练，而这些语料库中大多是自然文本，而不是计算机可执行代码。本文表明，目前的代码生成系统存在来自其庞大语言模型的不良偏差，这些偏差在特定情况下会降低生成代码的质量。为了调查这种影响，我们提出“影响块”概念，它能够对编码挑战进行模块化分解和分析。我们引入了一种自动化干预机制，类似于对抗性测试，通过测试模型的失效模式来暴露不良偏差。最后，我们展示了如何在微调期间将我们的框架用作数据转换技术，作为这些偏差的缓解策略。

    Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.  To investigate the effect, we propose the "block of influence" concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.
    
[^106]: 针对Facebook上的政治活动的弱监督学习

    Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10669](http://arxiv.org/abs/2210.10669)

    本研究提出了一种基于弱监督学习的方法，通过分析Facebook上政治广告的立场和议题，以及其使用人口统计学定位，来了解政治活动的特点和时间动态。

    

    社交媒体平台目前是政治信息传播的主要渠道，政治家们能够通过这些平台针对特定人群进行宣传，并根据他们的反应进行调整。然而，使这种交流透明化是具有挑战性的，因为信息传播与目标受众紧密相连，并经常被多个利益攸关方共同传播。本文旨在第一步了解这些高度分散的政治活动。我们提出了一种弱监督的方法来识别Facebook上政治广告的立场和议题，并分析政治活动如何使用某种人口统计学定位，如位置、性别或年龄。此外，我们还分析了选举民意调查中政治广告的时间动态。

    Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.
    
[^107]: 关于大型人工智能模型不可能安全的论述

    On the Impossible Safety of Large AI Models. (arXiv:2209.15259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15259](http://arxiv.org/abs/2209.15259)

    本文探讨了大型人工智能模型的安全问题，并指出了构建既精确又安全的模型的基本不可能性，并提供了统计学下限证明。

    

    大型人工智能模型(LAIMs)，其中大型语言模型是最突出的最近的例子，展示了令人印象深刻的性能。然而，实验证据表明，它们存在严重的安全问题。本文系统化地总结了有关构建任意准确和安全的机器学习模型的基本不可能性的知识。更确切地说，我们确定了今天许多机器学习设置的关键挑战特征。即，高精度似乎需要记住大型训练数据集，这些数据集通常是用户生成的，并且高度异构，包括敏感信息和假用户。然后，我们调查了统计下限，认为这构成了一个令人信服的理由，说明设计具有强安全保证的高精度LAIMs的可能性是不可能的。

    Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues. This paper systematizes our knowledge about the fundamental impossibility of building arbitrarily accurate and secure machine learning models. More precisely, we identify key challenging features of many of today's machine learning settings. Namely, high accuracy seems to require memorizing large training datasets, which are often user-generated and highly heterogeneous, with both sensitive information and fake users. We then survey statistical lower bounds that, we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.
    
[^108]: 面向高维可达性问题的形式化安全保障生成

    Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.12336](http://arxiv.org/abs/2209.12336)

    提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。

    

    为自主系统提供正式的安全和性能保证变得日益重要。哈密顿-雅科比（HJ）可达性分析是一种流行的形式验证工具，用于提供这些保证，因为它可以处理一般非线性系统动态、有界对抗系统干扰以及状态和输入约束。但是，它涉及到求解PDE，其计算和内存复杂度随着状态维度的增加呈指数级增长，使其在大型系统上的直接使用变得不可行。最近提出的DeepReach方法通过利用正弦神经PDE求解器来克服了这一挑战，用于解决高维可达性问题，其计算要求随可达管复杂性而不是状态空间维度而变化。不幸的是，神经网络可能会出现错误，因此计算出的解决方案可能不安全，这没有达到我们提供正式安全保障的总体目标。为了解决这一挑战，我们提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。我们的框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。我们在几个基准示例上展示了我们提出的方法的有效性，包括基于感知的高维车道保持系统。

    Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
    
[^109]: EDeNN: 事件衰减神经网络实现低延迟视觉

    EDeNN: Event Decay Neural Networks for low latency vision. (arXiv:2209.04362v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.04362](http://arxiv.org/abs/2209.04362)

    EDeNN是一种新型的神经网络，更接近原始事件数据流，避免了积累事件到图像帧的过程，展现了在角速度回归和竞争光流估计方面最先进的性能。

    

    尽管神经网络在计算机视觉任务上取得了成功，但数字“神经元”是对生物神经元的一种非常不精确的近似。当前的学习方法旨在在数字设备上进行，具有像图像帧这样的数字数据表示。相比之下，生物视觉系统通常比最先进的数字计算机视觉算法更具能力和效率。事件相机是一种新兴的传感器技术，它模仿生物视觉，使用异步触发的像素，放弃了图像帧的概念。为了利用现代学习技术，许多基于事件的算法不得不将事件积累回图像帧，从而浪费了事件相机的优势。我们采用相反的理念，开发了一种新型神经网络，该网络更接近原始事件数据流。我们在角速度回归和竞争光流估计方面展示了最先进的性能，同时避免了积累事件到图像帧的过程。

    Despite the success of neural networks in computer vision tasks, digital 'neurons' are a very loose approximation of biological neurons. Today's learning approaches are designed to function on digital devices with digital data representations such as image frames. In contrast, biological vision systems are generally much more capable and efficient than state-of-the-art digital computer vision algorithms. Event cameras are an emerging sensor technology which imitates biological vision with asynchronously firing pixels, eschewing the concept of the image frame. To leverage modern learning techniques, many event-based algorithms are forced to accumulate events back to image frames, somewhat squandering the advantages of event cameras.  We follow the opposite paradigm and develop a new type of neural network which operates closer to the original event data stream. We demonstrate state-of-the-art performance in angular velocity regression and competitive optical flow estimation, while avoid
    
[^110]: 带有知识蒸馏的边缘生成对抗超分辨率技术

    Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation. (arXiv:2209.03355v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.03355](http://arxiv.org/abs/2209.03355)

    该论文提出了一种高效的基于生成对抗网络的实时超分辨率模型EdgeSRGAN，使用模型量化提高了CPU和Edge TPU设备上的执行效率，并通过知识蒸馏技术进一步优化模型，保留了较好的图像质量。

    

    单幅图像超分辨率可支持机器人任务，例如在需要可靠的视觉流以监视任务、处理远程操纵或研究相关视觉细节的环境中。本文提出一种高效的、基于生成对抗网络的实时超分辨率模型，称为EdgeSRGAN（代码可在https://github.com/PIC4SeR/EdgeSRGAN获得）。我们采用了原始SRGAN的定制化架构和模型量化，以提高CPU和Edge TPU设备上的执行效率，实现最高达200fps的推理计算。我们进一步通过将模型知识蒸馏到网络的较小版本中进行优化，并相比标准训练方法获得了显着的改进。我们的实验表明，与更重的最先进模型相比，我们的快速轻量级模型保留了相当令人满意的图像质量。最后，我们进行带有带宽降级的图像传输实验，以突显该系统优势。

    Single-Image Super-Resolution can support robotic tasks in environments where a reliable visual stream is required to monitor the mission, handle teleoperation or study relevant visual details. In this work, we propose an efficient Generative Adversarial Network model for real-time Super-Resolution, called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We adopt a tailored architecture of the original SRGAN and model quantization to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps inference. We further optimize our model by distilling its knowledge to a smaller version of the network and obtain remarkable improvements compared to the standard training approach. Our experiments show that our fast and lightweight model preserves considerably satisfying image quality compared to heavier state-of-the-art models. Finally, we conduct experiments on image transmission with bandwidth degradation to highlight the advantages of the proposed system for 
    
[^111]: 论现实和语言数据限制：将LLMs与人类规范对齐

    On Reality and the Limits of Language Data: Aligning LLMs with Human Norms. (arXiv:2208.11981v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.11981](http://arxiv.org/abs/2208.11981)

    本文研究了大型语言模型（LLMs）在仅使用语言数据的情况下理解物理世界的能力，使用新颖且严密控制的推理测试（ART）与人类规范进行对比，研究发现了LLMs在某些常识关系模型中可以直接从数据中学习，但存在弱点，例如在部分和包含关系方面表现不足。

    

    最近，大型语言模型（LLMs）在利用大量自然语言数据中的语言关联进行实际应用方面取得了进展。然而，它们仅使用语言数据来理解物理世界的能力仍有疑问。在回顾现有协议之后，我们使用一种新颖且严密控制的推理测试（ART）来探讨这个问题，并比较人类规范与GPT-3版本之间的差异。我们的研究结果突出了通常可以直接从数据中学习的常识关系模型类别以及弱点所在。GPT-3为包括同义词、反义词和默认继承在内的几个关系方面提供了与人类主体相当的口头推理证据。没有来自人类判断的强化学习，GPT-3在具有部分和包含关系方面表现的区间下限处。在必要品质、大小顺序和强度顺序等方面也观察到了不足之处。把LLMs与象征性方法相结合，

    Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic 
    
[^112]: 高维空间中可微归纳逻辑编程

    Differentiable Inductive Logic Programming in High-Dimensional Space. (arXiv:2208.06652v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06652](http://arxiv.org/abs/2208.06652)

    本研究提出了一种在高维空间中进行可微归纳逻辑编程的扩展方法，通过大规模谓词发明来充分利用高维梯度下降的效能，以学习超出现有神经符号ILP系统能力的任务的解决方案。

    

    通过符号归纳逻辑编程（ILP）合成大型逻辑程序通常需要中间定义。然而，使用内涵谓词杂乱地占据假设空间通常会降低性能。相反，梯度下降提供了在这些高维空间中寻找解决方案的有效方法。到目前为止，神经符号ILP方法并没有充分利用这一点。我们提出扩展{\delta}ILP方法，以进行大规模谓词发明的归纳合成，从而允许我们利用高维梯度下降的效能。我们展示了大规模谓词发明通过梯度下降受益于可微归纳合成，并允许我们学习超出现有神经符号ILP系统能力的任务的解决方案。此外，我们在不指定解决方案的精确结构的语言偏差的情况下实现了这些结果。

    Synthesizing large logic programs through symbolic Inductive Logic Programming (ILP) typically requires intermediate definitions. However, cluttering the hypothesis space with intensional predicates typically degrades performance. In contrast, gradient descent provides an efficient way to find solutions within such high- dimensional spaces. Neuro-symbolic ILP approaches have not fully exploited this so far. We propose extending the {\delta}ILP approach to inductive synthesis with large-scale predicate invention, thus allowing us to exploit the efficacy of high-dimensional gradient descent. We show that large-scale predicate invention benefits differentiable inductive synthesis through gradient descent and allows one to learn solutions for tasks beyond the capabilities of existing neuro-symbolic ILP systems. Furthermore, we achieve these results without specifying the precise structure of the solution within the language bias.
    
[^113]: 在医学领域中实用联邦学习的探索

    Towards the Practical Utility of Federated Learning in the Medical Domain. (arXiv:2207.03075v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03075](http://arxiv.org/abs/2207.03075)

    本研究提出了应用联邦学习于医学领域的实用指南，包括三个具有代表性的医学数据集的实验，旨在提高医保业的数据效率，并形成适用于全行业的标准。

    

    联邦学习（FL）是一个活跃的研究领域。医学领域是采用FL的最适合领域之一，因为必须尊重患者隐私。然而，以往的研究并没有提供在医学领域中应用FL的实用指南。本文针对三个代表性的医学数据集，即长期的电子健康记录、皮肤癌图像和心电图信号，提出经验基准和实验设置。潜在的FL用户，如医疗机构和IT公司，可以将这些基准作为采用FL的指南，并尽可能减少试错。对于每个数据集，每个客户端数据来自不同的来源，以保留现实世界的异质性。我们评估了六种针对客户端数据异质性问题的FL算法，以及一种将两种典型FL算法的优点结合起来的混合算法。基于三种类型数据的实验结果，我们发现简单的FL算法可以达到与更复杂算法相当的性能。我们的工作为医疗机构和IT公司提供了在安全高效的方式下，应用FL从而改善医疗保健的实用指南。

    Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorith
    
[^114]: 科特迪瓦和加纳可可种植区的高分辨率卫星地图

    Satellite-based high-resolution maps of cocoa planted area for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.06119](http://arxiv.org/abs/2206.06119)

    本研究使用深度学习框架结合可可种植数据和卫星图像，为科特迪瓦和加纳创建了高分辨率的可可种植区地图。研究发现，可可种植是科特迪瓦和加纳保护区森林损失的基本驱动因素，而官方报告的可可种植面积明显低估。

    

    科特迪瓦和加纳是全球最大的可可生产国，两国占全球可可产量的三分之二。在两个国家，可可是主要的多年生作物，为近两百万农民提供收入。然而，精确的可可种植区地图缺失，阻碍了对保护区扩张、产量和收益增长的准确量化，限制了可持续性治理所需的信息。本文结合可可种植数据和公开的卫星图像，采用深度学习框架，为两国创建了高分辨率的可可种植区地图，并在现场验证了其准确性。我们的研究发现，可可种植是科特迪瓦和加纳保护区森林损失的基本驱动因素，分别占37％和13％，而官方报告的可可种植面积明显低估，在加纳低估高达40％。这些地图是推进保护和经济理解的关键基础。

    C\^ote d'Ivoire and Ghana, the world's largest producers of cocoa, account for two thirds of the global cocoa production. In both countries, cocoa is the primary perennial crop, providing income to almost two million farmers. Yet precise maps of cocoa planted area are missing, hindering accurate quantification of expansion in protected areas, production and yields, and limiting information available for improved sustainability governance. Here, we combine cocoa plantation data with publicly available satellite imagery in a deep learning framework and create high-resolution maps of cocoa plantations for both countries, validated in situ. Our results suggest that cocoa cultivation is an underlying driver of over 37% and 13% of forest loss in protected areas in C\^ote d'Ivoire and Ghana, respectively, and that official reports substantially underestimate the planted area, up to 40% in Ghana. These maps serve as a crucial building block to advance understanding of conservation and economic
    
[^115]: HierarchyNet: 利用异构表示学习代码摘要

    HierarchyNet: Learning to Summarize Source Code with Heterogeneous Representations. (arXiv:2205.15479v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2205.15479](http://arxiv.org/abs/2205.15479)

    HierarchyNet是一种优秀的代码摘要方法，它基于异构代码表示和层次感知交叉注意层，可以有效地捕获词法、语法和语义层面上的代码要素。

    

    我们提出了一种使用异构代码表示(HCRs)和我们特别设计的HierarchyNet来进行代码摘要的新方法。通过对粗粒度的代码元素进行抽象，并在层次结构中纳入细粒度的程序元素，HCRs有效地捕获了词法、语法和语义层面上的代码要素。我们的HierarchyNet方法分别通过异构图变换器、基于树的CNN和变换器编码器独特地处理HCR的每一层。此方法通过新颖的层次感知交叉注意力层保留了代码元素之间的依赖关系和捕捉他们之间的关系。我们的方法在PA-Former、CAST和NeuralCodeSum等当前最先进的技术上表现出了更好的结果。

    We propose a novel method for code summarization utilizing Heterogeneous Code Representations (HCRs) and our specially designed HierarchyNet. HCRs effectively capture essential code features at lexical, syntactic, and semantic levels by abstracting coarse-grained code elements and incorporating fine-grained program elements in a hierarchical structure. Our HierarchyNet method processes each layer of the HCR separately through a unique combination of the Heterogeneous Graph Transformer, a Tree-based CNN, and a Transformer Encoder. This approach preserves dependencies between code elements and captures relations through a novel Hierarchical-Aware Cross Attention layer. Our method surpasses current state-of-the-art techniques, such as PA-Former, CAST, and NeuralCodeSum.
    
[^116]: 通过友好模型替换解决联合学习中的客户端退役问题

    Combating Client Dropout in Federated Learning via Friend Model Substitution. (arXiv:2205.13222v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13222](http://arxiv.org/abs/2205.13222)

    本研究提出了一种新算法FL-FDMS，可以在客户端退役的情况下，即时找到数据分布相似的客户端，并使用这些客户端的本地模型更新来替代缺失的客户端的更新，实现更高的学习准确性和更低的通信成本。

    

    联合学习是一种新的分布式机器学习框架，以其数据隐私和通信效率的优势而闻名。由于许多情况下完全客户端参与不可行，研究了主动选择/采样一部分客户端的部分参与FL算法，旨在实现接近全参与情况的学习性能。本文研究了一种被动部分客户端参与情况，这种情况理解不足得多，部分参与是由于外部事件（即客户端退出）而不是FL算法的决定而导致的。我们将具有客户端退役的FL视为FL问题中的一类特殊情况，其中客户端可以提交替代（可能不准确）的本地模型更新。根据我们的收敛分析，我们开发了一种新算法FL-FDMS，可以即时发现客户端的朋友（即数据分布相似的客户端），并使用朋友的本地模型更新来替代放弃的客户端的缺失更新。我们在假设下证明FL-FDMS在理论上收敛到最优全局模型，该假设足以涵盖许多现有的FL算法。实验结果表明，与几个基线相比，当客户端退出率适中到高时，FL-FDMS实现了更高的学习准确性和更低的通信成本。

    Federated learning (FL) is a new distributed machine learning framework known for its benefits on data privacy and communication efficiency. Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case. This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm. We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates. Based on our convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients (i.e., clients whose data distributions are similar) on-the-fly and uses friends' local
    
[^117]: 推荐系统中的公平性：研究现状与未来方向

    Fairness in Recommender Systems: Research Landscape and Future Directions. (arXiv:2205.11127v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2205.11127](http://arxiv.org/abs/2205.11127)

    本文讨论了推荐系统的公平性问题，通过对160多篇学术出版物的综述总结了该领域目前的研究现状，强调了一些有前途的未来方向，例如需要超越统计平衡的新公平性衡量方法。

    

    推荐系统可以极大地影响我们在线上看到的信息，例如在社交媒体上，从而影响我们的信仰、决策和行动。同时，这些系统可以为不同的利益相关者创造巨大的商业价值。鉴于这种基于人工智能的系统对个人、组织和社会的潜在影响越来越大，公平性问题在近年来得到了越来越多的关注。然而，推荐系统公平性的研究仍然是一个正在发展的领域。在本次调查中，我们首先回顾了近年来在该领域提出的公平性基本概念和观念。随后，通过对160多篇学术出版物的综述，我们总结了这一领域目前的研究现状，例如一般研究方法、公平性措施和算法方法。总的来说，我们对最近研究的分析指出了某些研究空白。特别是，我们发现在许多研究中，对提出的模型的公平性提升属性缺乏实质性的评估。此外，该调查还强调了一些有前途的未来方向，例如需要超越统计平衡的新公平性衡量方法。

    Recommender systems can strongly influence which information we see online, e.g., on social media, and thus impact our beliefs, decisions, and actions. At the same time, these systems can create substantial business value for different stakeholders. Given the growing potential impact of such AI-based systems on individuals, organizations, and society, questions of fairness have gained increased attention in recent years. However, research on fairness in recommender systems is still a developing area. In this survey, we first review the fundamental concepts and notions of fairness that were put forward in the area in the recent past. Afterward, through a review of more than 160 scholarly publications, we present an overview of how research in this field is currently operationalized, e.g., in terms of general research methodology, fairness measures, and algorithmic approaches. Overall, our analysis of recent works points to certain research gaps. In particular, we find that in many resea
    
[^118]: 证明块问题中的高效反馈和部分分评分

    Efficient Feedback and Partial Credit Grading for Proof Blocks Problems. (arXiv:2204.04196v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.04196](http://arxiv.org/abs/2204.04196)

    本文提出了一个算法，可以高效地计算学生提交作业与预定义解决方案之间的编辑距离，可应用于证明、编程与自然语言处理等领域。

    

    证明块是一种软件工具，允许学生通过拖放而非从头写证明，练习数学证明的写作。证明块提供了分部分分和提供解决方案质量反馈的能力。这是通过计算学生提交的作业与一些预定义解决方案之间的编辑距离来完成的。本文提出了一种编辑距离问题的算法，显著优于排列整个搜索空间的基线程序。我们的算法依赖于将问题的缩减为最小顶点覆盖问题。我们在多个课程的数千份学生提交中对我们的算法进行了基准测试，表明基线算法难以计算，并且我们提出的算法对于实现课堂部署至关重要。我们新的算法也已经被用于许多其他领域的问题，其中解决方案空间可以建模为DAG，包括但不限于编程作业和自然语言处理任务。

    Proof Blocks is a software tool that allows students to practice writing mathematical proofs by dragging and dropping lines instead of writing proofs from scratch. Proof Blocks offers the capability of assigning partial credit and providing solution quality feedback to students. This is done by computing the edit distance from a student's submission to some predefined set of solutions. In this work, we propose an algorithm for the edit distance problem that significantly outperforms the baseline procedure of exhaustively enumerating over the entire search space. Our algorithm relies on a reduction to the minimum vertex cover problem. We benchmark our algorithm on thousands of student submissions from multiple courses, showing that the baseline algorithm is intractable, and that our proposed algorithm is critical to enable classroom deployment. Our new algorithm has also been used for problems in many other domains where the solution space can be modeled as a DAG, including but not limi
    
[^119]: 基于Wi-Fi信道数据的人与人互动识别的前瞻性方法——使用具有GUI应用程序实现的注意力双向门控循环神经网络(arXiv:2202.08146v4 [cs.LG] UPDATED)

    A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation. (arXiv:2202.08146v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.08146](http://arxiv.org/abs/2202.08146)

    该论文提出了一种基于Wi-Fi信道数据的人与人互动识别的新方法，使用注意力双向门控循环神经网络实现高精度的实时处理，准确率达到98.22%，并提供了一个GUI应用程序方便实时应用。

    

    由于最近的技术进步、人工智能算法、智能城市和社会经济变革的需要，人类活动识别(HAR)研究已经获得了重要的动力。然而，现有的基于计算机视觉和传感器的HAR解决方案存在隐私问题、存储和功率消耗问题以及佩戴传感器的不适感，这促使研究人员观察到HAR研究的范式转变。作为回应，基于WiFi的HAR因其更粗粒度的信道状态信息的可用性而越来越受欢迎。然而，现有的基于WiFi的HAR方法仅限于对在相等时间内执行的独立和非并发人类活动进行分类。与最近的研究不同的是，我们的研究利用了多输入多输出通信链路，其中发射器是WiFi路由器，接收器是配备了Intel 5300 NIC的智能手机。我们提出了一种基于注意力双向门控循环神经网络(ABiGRNN)的WiFi信道数据的人与人互动(HHI)识别的新方法，该方法允许高精度的实时处理。我们使用HHI活动数据集评估我们的方法，实现了98.22%的准确率，优于现有的最先进方法。此外，我们提供了一个图形用户界面(GUI)应用程序，可以在实时场景中轻松实现我们的方法来识别HHI。

    Human Activity Recognition (HAR) research has gained significant momentum due to recent technological advancements, artificial intelligence algorithms, the need for smart cities, and socioeconomic transformation. However, existing computer vision and sensor-based HAR solutions have limitations such as privacy issues, memory and power consumption, and discomfort in wearing sensors for which researchers are observing a paradigm shift in HAR research. In response, WiFi-based HAR is gaining popularity due to the availability of more coarse-grained Channel State Information. However, existing WiFi-based HAR approaches are limited to classifying independent and non-concurrent human activities performed within equal time duration. Recent research commonly utilizes a Single Input Multiple Output communication link with a WiFi signal of 5 GHz channel frequency, using two WiFi routers or two Intel 5300 NICs as transmitter-receiver. Our study, on the other hand, utilizes a Multiple Input Multiple
    
[^120]: 联邦学习中的比例公平性

    Proportional Fairness in Federated Learning. (arXiv:2202.01666v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01666](http://arxiv.org/abs/2202.01666)

    本文提出了用于保证联邦学习中公平性的比例公平性 (PF) 概念，并提出了一种新颖易于实现的算法 PropFair，能够在所有客户端平均性能和最差 10% 客户端平均性能之间达到良好平衡。

    

    随着联邦学习系统在现实世界中越来越广泛地部署，保证公平性变得至关重要但也面临着挑战，即需要为众多不同的客户端提供合理满意的表现。在本文中，我们引入并研究了联邦学习中一种新的公平性概念，即比例公平性 (PF)，它基于每个客户端性能的相对变化。通过与交易博弈的联系，我们提出了 PropFair，一种新颖且易于实现的算法，用于在联邦学习中寻找比例公平解，并研究了其收敛性质。通过对视觉和语言数据集的广泛实验，我们证明 PropFair 能够大致找到 PF 解，并在所有客户端的平均性能和最差 10% 客户端的平均性能之间实现良好的平衡。我们的代码可在 \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL} 找到。

    With the increasingly broad deployment of federated learning (FL) systems in the real world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory performances for each of the numerous diverse clients. In this work, we introduce and study a new fairness notion in FL, called proportional fairness (PF), which is based on the relative change of each client's performance. From its connection with the bargaining games, we propose PropFair, a novel and easy-to-implement algorithm for finding proportionally fair solutions in FL and study its convergence properties. Through extensive experiments on vision and language datasets, we demonstrate that PropFair can approximately find PF solutions, and it achieves a good balance between the average performances of all clients and of the worst 10% clients. Our code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.
    
[^121]: 基于深度学习的超声速度重构：培训数据多样性对稳定性和健壮性的影响。

    Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness. (arXiv:2202.01208v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.01208](http://arxiv.org/abs/2202.01208)

    本文提出了一种基于深度学习的超声速度重构方法，通过使用层析图像生成多样化的培训数据以提高稳定性和健壮性。该方法在高分辨率声速地图重构方面的表现优于传统的简化几何方法。

    

    超声b模式成像是一种定性方法，诊断质量强烈依赖于操作员的培训和经验。定量方法可以提供有关组织特性的信息；因此，可以用于识别各种组织类型，例如，组织中的声速可以用作组织恶性度的生物标志物，特别是在乳腺成像中。最近的研究显示，使用完全在模拟数据上训练的深度神经网络可能实现声速重建。然而，由于模拟数据和实际数据之间的不断领域转移，这些模型在真实设置中的稳定性和性能仍然存在争议。在先前的工作中，对于培训数据生成，组织结构被建模为简化的几何结构，这不能反映真实组织的复杂性。在本研究中，我们提出了一种基于层析图像的新型模拟设置用于生成培训数据。我们结合了多任务学习与分割，以整合额外的组织信息并提高重建性能。我们在幻影和体内测量上的结果表明，多样化组织结构的训练导致更稳定和健壮的模型。此外，我们提出的基于层析图像的方法在重建高分辨率声速地图方面始终优于简化的几何方法。

    Ultrasound b-mode imaging is a qualitative approach and diagnostic quality strongly depends on operators' training and experience. Quantitative approaches can provide information about tissue properties; therefore, can be used for identifying various tissue types, e.g., speed-of-sound in the tissue can be used as a biomarker for tissue malignancy, especially in breast imaging. Recent studies showed the possibility of speed-of-sound reconstruction using deep neural networks that are fully trained on simulated data. However, because of the ever-present domain shift between simulated and measured data, the stability and performance of these models in real setups are still under debate. In prior works, for training data generation, tissue structures were modeled as simplified geometrical structures which does not reflect the complexity of the real tissues. In this study, we proposed a new simulation setup for training data generation based on Tomosynthesis images. We combined our approach 
    
[^122]: BungeeNeRF：极端多尺度场景渲染的渐进式神经辐射场

    BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.05504](http://arxiv.org/abs/2112.05504)

    本文介绍了一种名为BungeeNeRF的渐进式神经辐射场，它可以在极端不同的尺度上实现逐层渲染，经过多个阶段逐步细化粗略的几何形状。该方法可以处理各种规模，并在所有规模上产生具有精细细节的高质量结果，广泛应用于城市景观、景观和Minecraft模型等场景。

    

    神经辐射场（NeRF）在建模三维物体和受控场景方面取得了出色的性能，通常在单一尺度下运行。本文着眼于多尺度情况，其中观察到图像在截然不同的尺度下存在巨大变化。这种情况在现实世界的三维环境中广泛存在，如城市场景，在卫星级别下捕捉一个城市的概述，到地面级别下显示建筑物的复杂细节；也常见于景观和精细的Minecraft 3D模型中。这些场景中广泛的视角范围产生了具有非常不同细节级别的多尺度渲染，这给神经辐射场带来了巨大的挑战并使其偏向妥协结果。我们引入了BungeeNeRF，它是一种渐进式神经辐射场，可在极端不同的尺度上实现逐层渲染。从用浅层网络拟合远距离视图开始，BungeeNeRF通过多个阶段逐步细化粗略的几何形状，由先前阶段的反馈和选择性调整不同尺度的贡献的注意力模块指导。我们的方法可以处理各种规模，并在所有规模上产生具有精细细节的高质量结果。我们在广泛的场景中展示了我们方法的有效性，包括城市景观、景观和Minecraft模型。

    Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shal
    
[^123]: 利用集成机器学习从常规血液检查中开发无风险的COVID-19筛查算法

    Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning. (arXiv:2108.05660v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.05660](http://arxiv.org/abs/2108.05660)

    本研究提出了一种风险低且高精度的堆叠集成机器学习模型，可以从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。

    

    反转录聚合酶链反应（RT-PCR）是辨别COVID-19感染的银弹诊断检测。快速抗原检测是一种筛查检测，可在15分钟内识别COVID-19阳性患者，但其灵敏度低于PCR检测。本研究通过利用COVID-19患者免疫和血液学资料的参数偏差，提出了一种风险低且高精度的堆叠集成机器学习模型，从常规血液检查中识别COVID-19患者，具有很高的准确率、精确度和召回率。

    The Reverse Transcription Polymerase Chain Reaction (RTPCR)} test is the silver bullet diagnostic test to discern COVID infection. Rapid antigen detection is a screening test to identify COVID positive patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected and either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological and hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall and
    

