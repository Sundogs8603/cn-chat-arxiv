# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Hunayn: Elevating Translation Beyond the Literal.](http://arxiv.org/abs/2310.13613) | 这项研究介绍了一种超越传统工具的高级英译阿拉伯语翻译器，使用赫尔辛基变压器和纯文学阿拉伯语数据集，表现出色，并强调了其在文化敏感性和语境准确性方面的优势。 |
| [^2] | [Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making.](http://arxiv.org/abs/2310.13610) | 提出了一个名为自我归因和决策制定的统一两阶段框架（SADM），通过使用输入文本的子序列作为解释来建立更可靠的模型决策解释联系，并在ERASER基准测试中取得了竞争性的结果。 |
| [^3] | [MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark.](http://arxiv.org/abs/2310.13606) | 这篇论文介绍了MULTITuDE，一个针对多语言机器生成文本检测的基准数据集。通过比较不同的检测器在零样本和微调情况下的性能，研究了这些检测器对于未见过的语言和生成器的泛化能力，并探讨了在多语言训练下检测器的性能提升。 |
| [^4] | [Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale Dependency Modeling.](http://arxiv.org/abs/2310.13604) | 本论文介绍了一种基于Transformer网络和跨尺度依赖建模的皮肤病变分割方法，通过替换CNN块和优化连接路径来提高网络特征的可重用性，以实现更准确的医学图像分割。 |
| [^5] | [MarineGPT: Unlocking Secrets of Ocean to the Public.](http://arxiv.org/abs/2310.13596) | 这项工作介绍了一种名为MarineGPT的海洋领域专用多模态大型语言模型，该模型能够以敏感、信息丰富和科学的方式回应用户的需求。 |
| [^6] | [SPARE: A Single-Pass Neural Model for Relational Databases.](http://arxiv.org/abs/2310.13581) | SPARE是一种单遍神经模型，用于高效训练关系数据库，并提供与图神经网络相似的准确度。 |
| [^7] | [Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery.](http://arxiv.org/abs/2310.13576) | 本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。 |
| [^8] | [Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection.](http://arxiv.org/abs/2310.13573) | 该论文介绍了一种通过自适应风格技术提升指纹活体检测泛化能力的方法，并在LivDet 2023挑战中取得了最先进的性能。 |
| [^9] | [Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring.](http://arxiv.org/abs/2310.13566) | 这项研究介绍了一种结合了检索增强语言模型和逻辑推理的知识驱动响应生成方法，通过在每个对话轮次中评估知识图中的节点和边的对话相关性，在任务型对话系统中取得了较好的效果。 |
| [^10] | [Reward Shaping for Happier Autonomous Cyber Security Agents.](http://arxiv.org/abs/2310.13565) | 本研究探讨了在计算机网络防御任务中，奖励信号的特点对深度强化学习算法的影响，并研究了奖励塑造技术以提高代理的训练效率和性能。 |
| [^11] | [Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning.](http://arxiv.org/abs/2310.13552) | 本文提出了自我启示的思维链（SP-CoT）框架，在开放领域多跳推理中利用大型语言模型（LLMs），通过自动化生成高质量的CoTs，扩展了开放领域问答的能力。 |
| [^12] | [Towards Understanding Sycophancy in Language Models.](http://arxiv.org/abs/2310.13548) | 这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。 |
| [^13] | [ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection.](http://arxiv.org/abs/2310.13545) | 通过缩小长跳接连接系数，扩散模型中的UNet训练更加稳定，并且在理论上证明了UNet对于输入的扰动敏感，并产生振荡效果。 |
| [^14] | [Positive-Unlabeled Node Classification with Structure-aware Graph Learning.](http://arxiv.org/abs/2310.13538) | 本文提出了一种利用图结构进行正-未标记节点分类的方法。我们引入了一个距离感知的PU损失来提供更准确的监督，同时通过正则化项与图结构对齐。实验证明该方法在多种图数据集上具有卓越性能。 |
| [^15] | [Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation.](http://arxiv.org/abs/2310.13533) | 该论文提出了一种连续测试时间适应方法，用于解决语义分割任务中视频序列中逐渐变化的领域问题。该方法在每个图像序列中单独评估，并通过逐渐改变天气条件和时段来测试模型的性能。 |
| [^16] | [Variational measurement-based quantum computation for generative modeling.](http://arxiv.org/abs/2310.13524) | 这项研究提出了一种基于测量的变分量子计算算法，将量子测量的随机性视为计算资源，并应用于生成建模任务。 |
| [^17] | [RaceLens: A Machine Intelligence-Based Application for Racing Photo Analysis.](http://arxiv.org/abs/2310.13515) | RaceLens是一种基于机器智能的赛车照片分析应用，利用先进的深度学习和计算机视觉模型对赛车照片进行全面分析。通过持续改进和增强数据集的方法，RaceLens的性能和准确性得到了提高，并成功应用于NASCAR车队，对其战略决策和表现产生了直接影响。 |
| [^18] | [Explaining Interactions Between Text Spans.](http://arxiv.org/abs/2310.13506) | 本论文介绍了SpanEx，这是一个关于自然语言理解任务的人类标记范围交互解释的多注释者数据集。研究发现现有的解释方法往往只关注相邻标记或元组之间的交互，缺乏捕捉人类决策过程中必要交互的注释。通过比较大型语言模型与人类的决策过程，作者发现它们之间存在差异。最后，作者提出了一种基于社区检测的元建模方法。 |
| [^19] | [Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation.](http://arxiv.org/abs/2310.13505) | 这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。 |
| [^20] | [Analogical Proportions and Creativity: A Preliminary Study.](http://arxiv.org/abs/2310.13500) | 本研究初步探讨了模拟比例在创造力中的应用，通过实验表明使用词嵌入可以更好地基于模拟比例提出新的动物。 |
| [^21] | [Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning.](http://arxiv.org/abs/2310.13486) | 这项研究对基于提示的学习中导致预测不稳定和不一致的设计选择进行了详细分析，并通过全面评估不同因素的影响，确定了对预测影响最大、具有交互性或稳定性的因素。 |
| [^22] | [Application of deep learning for livestock behaviour recognition: A systematic literature review.](http://arxiv.org/abs/2310.13483) | 本文通过系统文献综述研究了深度学习在畜牧动物行为识别中的应用，并发现了人工智能和计算机视觉技术在畜牧业中的潜在决策工具的重要作用。 |
| [^23] | [Ask Language Model to Clean Your Noisy Translation Data.](http://arxiv.org/abs/2310.13469) | 论文介绍了如何利用大型语言模型清理神经机器翻译中的噪声输入，通过从MTNT数据集中清理目标语句的噪声，生成了C-MTNT数据集，显著减少了噪声。 |
| [^24] | [Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation.](http://arxiv.org/abs/2310.13447) | 本文提出了一种多尺度超像素结构差异图卷积网络（MDGCN）用于视觉语言表征，通过聚类感知相似像素，减少了后续处理的视觉基元数量，并挖掘了更精确的拓扑关系。 |
| [^25] | [Self-Consistency of Large Language Models under Ambiguity.](http://arxiv.org/abs/2310.13439) | 大型语言模型在模棱两可问题中的自一致性存在，并且不需要特别训练也能在稳健性检查中保持自一致性。 |
| [^26] | [Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption.](http://arxiv.org/abs/2310.13434) | 本论文提出了一种理论框架，在低密度分离假设下分析高维情况下的半监督分类，并介绍了QLDS模型，该模型在平衡监督和无监督学习方法之间建立了一个平滑的桥梁。通过应用随机矩阵理论，我们推导出了在渐近情况下分类错误的理论评估，并提出了一种能够找到最佳平衡点的超参数选择策略。 |
| [^27] | [FLTracer: Accurate Poisoning Attack Provenance in Federated Learning.](http://arxiv.org/abs/2310.13424) | FLTracer是第一个FL攻击来源追踪框架，能够高准确性地检测各种攻击，并追踪攻击的时间、目标、类型和被毒化的位置。与现有方法不同，FLTracer利用本地信息和全局信息的统计分析来实现。 |
| [^28] | [POSQA: Probe the World Models of LLMs with Size Comparisons.](http://arxiv.org/abs/2310.13394) | POSQA是一个用来探索LLMs在具身理解方面的物体大小问答数据集，研究了LLMs在零-shot情景下的表现，通过推动技术和外部知识增强了它们的极限，同时分析了其现实世界理解的来源和提示的影响。 |
| [^29] | [Learning Successor Representations with Distributed Hebbian Temporal Memory.](http://arxiv.org/abs/2310.13391) | 本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。 |
| [^30] | [A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training.](http://arxiv.org/abs/2310.13377) | 本文提出了一种人机互动学习系统，其中机器人和人类通过学习符号语言来识别机器人的内部稳态需求。研究采用了差异结果训练协议，证明其能够提高人类的学习效率，进而实现更高效的机器人语言习得。 |
| [^31] | [VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models.](http://arxiv.org/abs/2310.13367) | VFedMH是一种垂直联合学习方法，通过在前向传播过程中聚合参与者的嵌入来处理参与者之间的异构模型，解决了现有VFL方法面临的挑战。 |
| [^32] | [Towards General Error Diagnosis via Behavioral Testing in Machine Translation.](http://arxiv.org/abs/2310.13362) | 本论文提出了一种基于双语翻译对生成的行为测试框架，用于诊断机器翻译系统的通用错误。该框架通过自动构建高质量的测试用例及其伪参考，能够在没有人工参考的情况下进行翻译质量评估。 |
| [^33] | [Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation.](http://arxiv.org/abs/2310.13361) | 本文提出了一个方法来解决多模态机器翻译中合成图像与真实图像之间的分布差异问题，通过对模型输入图像表示和输出分布进行调整，来填补这一鸿沟。 |
| [^34] | [NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding.](http://arxiv.org/abs/2310.13347) | NurViD是一个专为护理操作设计的大规模视频数据库，解决了现有数据集的规模较小、缺乏专家级注释和时间本地化注释等问题。 |
| [^35] | [Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs).](http://arxiv.org/abs/2310.13343) | 大型语言模型（LLMs）在广泛应用中面临领域特异性、知识遗忘、知识复制、知识错觉和知识有毒等挑战。解决这些问题的建议包括多样化训练数据、微调模型，增强透明度和可解释性。 |
| [^36] | [FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery.](http://arxiv.org/abs/2310.13336) | FLAIR是一个全国范围的土地覆盖语义分割数据集，包含大量高分辨率航拍图像和数十亿个个体标记像素。FLAIR整合了不同空间、光谱和时间分辨率的数据，对于大规模土地覆盖分析的方法开发和评估具有重要价值。 |
| [^37] | [Democratizing Reasoning Ability: Tailored Learning from Large Language Model.](http://arxiv.org/abs/2310.13332) | 本文提出了一种定制学习方法，通过利用大型语言模型作为推理教师，并建立交互式多轮学习范式，将推理能力提取到较小的语言模型中，以促进其民主化。 |
| [^38] | [Coarse-to-Fine Dual Encoders are Better Frame Identification Learners.](http://arxiv.org/abs/2310.13316) | CoFFTEA是一种粗细框架和目标编码器体系结构，通过对比学习和双编码器来有效建模框架和目标之间的对齐，并通过粗到细的课程学习过程逐渐学会区分具有不同程度的框架。 |
| [^39] | [Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting.](http://arxiv.org/abs/2310.13297) | 通过利用大型语言模型，我们提出了一个名为SocialSense的框架，通过诱导信念增强的图和基于图的传播来预测新闻发布的响应。这个方法在没有用户明确个人资料或历史行为的情况下，能够有效地捕捉社交动态。 |
| [^40] | [PathRL: An End-to-End Path Generation Method for Collision Avoidance via Deep Reinforcement Learning.](http://arxiv.org/abs/2310.13295) | 这项研究提出了PathRL，一种使用深度强化学习生成导航路径的端到端方法。传统的DRL方法通过训练低级控制指令来直接指导机器人行动，但这导致速度不稳定和轨迹不平滑。相比之下，PathRL通过训练直接输出路径的DRL策略，克服了高维动作空间和多时间步骤跟踪路径的难题。通过这种方法，PathRL能够生成稳定且平滑的导航路径，用于避免碰撞和提高移动机器人性能。 |
| [^41] | [Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks.](http://arxiv.org/abs/2310.13291) | 本研究评估了语言模型在摘要任务中的隐私风险，发现摘要模型存在泄露数据成员身份的风险，并讨论了一些保护措施和隐私与效用之间的权衡。 |
| [^42] | [Unified Pretraining for Recommendation via Task Hypergraphs.](http://arxiv.org/abs/2310.13286) | 本文提出了一种名为"通过任务超图的统一预训练推荐"的多任务预训练框架，使用任务超图将先前任务推广为超边预测，并通过过渡性注意力层学习每个先前任务与推荐之间的相关性。实验表明，这种方法在准确性和效率方面优于其他基准方法。 |
| [^43] | [An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank.](http://arxiv.org/abs/2310.13269) | 该研究探讨了在学习排序中使用模拟退火进行特征选择的方法，并引入了进展参数来有效遍历搜索空间。实验证明了该方法的有效性。 |
| [^44] | [ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting.](http://arxiv.org/abs/2310.13258) | ManiCast是一个基于成本感知的人体预测的协同操作框架，通过提供能够捕捉未来人体运动如何影响机器人计划成本的预测，实现了人机协同操纵任务的流畅执行和实时交互。 |
| [^45] | [Visual Grounding Helps Learn Word Meanings in Low-Data Regimes.](http://arxiv.org/abs/2310.13257) | 在低数据环境中，使用视觉定位进行监督训练的神经语言模型可以更接近于人类的语言学习能力。 |
| [^46] | [TempGNN: Temporal Graph Neural Networks for Dynamic Session-Based Recommendations.](http://arxiv.org/abs/2310.13249) | TempGNN是一种用于动态会话推荐的时间图神经网络，可以捕捉复杂物品转换的结构和时间动态，通过对动态会话图中的节点和边进行时间嵌入操作，有效地预测下一步动作。 |
| [^47] | [FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows.](http://arxiv.org/abs/2310.13248) | 本文提出了FLEE-GNN，它是一种用于分析多种商品食品流的地理空间韧性的边缘增强图神经网络的联邦学习系统。FLEE-GNN克服了当前方法的局限性，提高了分析食品供应网络韧性的普适性、可扩展性和数据隐私保护。 |
| [^48] | [Multi-level Contrastive Learning for Script-based Character Understanding.](http://arxiv.org/abs/2310.13231) | 本文提出了一种多层对比学习框架用于剧本中角色的理解，从角色的话语中学习其个性和身份，并在多个角色理解子任务上通过与强大的预训练语言模型的比较进行了验证。 |
| [^49] | [Absolute Policy Optimization.](http://arxiv.org/abs/2310.13230) | 这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。 |
| [^50] | [ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search.](http://arxiv.org/abs/2310.13227) | ToolChain*是一种提供了高效动作空间导航的树搜索算法，它可以在大型语言模型中进行决策和规划，解决复杂的真实世界问题。 |
| [^51] | [Scalable Neural Network Kernels.](http://arxiv.org/abs/2310.13225) | 可扩展神经网络内核（SNNKs）是一种替代常规前馈层的方法，能够近似实现常规前馈层的功能，但具有更优的计算特性。通过将内核与参数-输入向量的点积联系起来，SNNKs能够有效地解开参数与输入之间的联系，从而模拟复杂关系。此外，我们还引入了神经网络捆绑过程，将SNNKs应用于深度神经网络压缩，进一步提高了压缩效果。最终捆绑网络甚至可以绕过反向传播，通过显式公式求解最优参数。 |
| [^52] | [HierCas: Hierarchical Temporal Graph Attention Networks for Popularity Prediction in Information Cascades.](http://arxiv.org/abs/2310.13219) | 本论文提出了一种名为HierCas的新的框架，用于信息级联流行度预测。与传统方法不同，HierCas采用动态图建模方法，在整个级联图上进行操作，能够捕捉到连续动态信息的完整范围，并明确建模结构和时间因素之间的相互作用。 |
| [^53] | [MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition.](http://arxiv.org/abs/2310.13213) | MultiCoNER v2是一个大型多语言数据集，用于细粒度和含噪音的命名实体识别。它解决了细粒度类别处理和噪音导致的性能下降的实际挑战。评估结果表明，细粒度分类具有挑战性，而实体损坏对性能影响更大。 |
| [^54] | [Primacy Effect of ChatGPT.](http://arxiv.org/abs/2310.13206) | ChatGPT在选择答案时表现出初印象效应，即更倾向于选择提示中较早位置的标签作为答案。 |
| [^55] | [The opaque law of artificial intelligence.](http://arxiv.org/abs/2310.13192) | 本文分析了算法的不透明性，重点关注人工智能在因果责任领域中的应用。通过对目前最好的生成式人工智能模型（Chat-GPT）的评估，可以了解其目前的性能以及可能的法律规制形式。 |
| [^56] | [Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models.](http://arxiv.org/abs/2310.13191) | 本文提出了一种适应性知识保留剪枝策略，旨在提高语言模型对抗攻击的鲁棒性，并在剪枝过程中保留更多的预训练知识。与其他方法相比，该方法展现了更好的平衡。 |
| [^57] | [Fast and Accurate Factual Inconsistency Detection Over Long Documents.](http://arxiv.org/abs/2310.13189) | SCALE是一个用于检测长文本中事实不一致性的通用模型，通过使用新颖的块化策略和自然语言推理（NLI）实现了最先进的性能，同时还能解释决策并超过竞争系统。 |
| [^58] | [CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation.](http://arxiv.org/abs/2310.13165) | CycleNet是一种将循环一致性引入扩散模型的新方法，用于规范图像操作，具有优越的翻译一致性和质量，并且可以生成高质量的跨领域分布图像。 |
| [^59] | [A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs.](http://arxiv.org/abs/2310.13161) | 这项研究提出了一种分布式方法来解决气象预测模型中的数据不平衡问题，通过联邦学习和生成对抗网络技术改善了模型的性能。 |
| [^60] | [Conditional Generative Modeling for Images, 3D Animations, and Video.](http://arxiv.org/abs/2310.13157) | 本论文探索了条件生成模型的新颖形式和创新应用，提出了使用神经ODE对视频动态建模，以及连续标准流的条件变体实现高分辨率图像生成。 |
| [^61] | [CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain.](http://arxiv.org/abs/2310.13146) | 本文介绍了一个新的临床领域问答模型测试平台CLIFT，其中包括7.5k个高质量的问答样本。在原始测试集上表现出色的深度学习模型在应用于新的测试集时性能下降，表明存在分布偏移。我们的研究结果强调了提高临床领域模型鲁棒性的必要性和潜力，并提供了一个追踪该方向进展的测试平台。 |
| [^62] | [Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries.](http://arxiv.org/abs/2310.13132) | 本文提供了一个框架，研究LLMs作为医疗问题的多语言对话系统的有效性。 |
| [^63] | [Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions.](http://arxiv.org/abs/2310.13129) | 本研究提出了一种基于深度强化学习的智能交通信号控制算法，通过优化CO2排放和行驶时间等指标，实现了较好的性能。 |
| [^64] | [Understanding Addition in Transformers.](http://arxiv.org/abs/2310.13121) | 本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。 |
| [^65] | [AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection.](http://arxiv.org/abs/2310.13103) | 本文提出了AVTENet框架，该框架是一个基于音频-视觉Transformer的多专家集成网络，用于在视频深度伪造检测中考虑声学和视觉操作。 |
| [^66] | [Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models.](http://arxiv.org/abs/2310.13102) | 本文提出了一种粒子引导的方法，通过超越独立样本的常见假设，提高了生成模型的多样性和采样效率。在实验中，我们在条件图像生成和分子构象生成上进行了测试，并取得了显著的结果。 |
| [^67] | [No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network.](http://arxiv.org/abs/2310.13099) | 本论文介绍了一种简单而高效的句级攻击方法，可以通过添加积极的词语或句子来改变黑盒有毒性检测模型的预测结果，该方法在多种语言上均有效。 |
| [^68] | [Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning.](http://arxiv.org/abs/2310.13085) | 本论文提出了一种一次性的无监督元学习方法，通过学习训练样本的潜在表示，解决了少样本学习中的数据稀缺问题。该方法在初始化和快速适应中应用了迁移学习，并提高了准确性。 |
| [^69] | [From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues.](http://arxiv.org/abs/2310.13080) | 本研究旨在利用常识信息和对话上下文结合的创新方法，揭示码混合对话中的情感动态。 |
| [^70] | [Creative Robot Tool Use with Large Language Models.](http://arxiv.org/abs/2310.13065) | 本文利用大型语言模型开发了一个系统，名为RoboTool，可以使机器人在涉及隐含物理约束和长期规划的任务中创造性地使用工具。该系统通过解析自然语言指令、生成全面的策略、计算技能参数并生成可执行的Python代码来实现。实验结果表明，RoboTool能够有效地处理任务中的物理约束和环境因素。 |
| [^71] | [Robust multimodal models have outlier features and encode more concepts.](http://arxiv.org/abs/2310.13040) | 健壮的多模态模型展示了异常特征和更多概念的编码方式。 |
| [^72] | [Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction.](http://arxiv.org/abs/2310.13037) | Agri-GNN是一种新型基因型-拓扑图神经网络框架，通过考虑作物之间的复杂空间和基因型相互作用，实现优化的农作物产量预测。 |
| [^73] | [LASER: Linear Compression in Wireless Distributed Optimization.](http://arxiv.org/abs/2310.13033) | LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。 |
| [^74] | [Quality-Diversity through AI Feedback.](http://arxiv.org/abs/2310.13032) | 基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。 |
| [^75] | [A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem.](http://arxiv.org/abs/2310.13031) | 本文提出了一种基于统计机器翻译的查询重写方法，通过学习重写阿拉伯语用户查询以改善搜索引擎的检索结果。 |
| [^76] | [Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series.](http://arxiv.org/abs/2310.13029) | 本文提出了一种融合梯度提升树和神经网络的方法，有效解决了层次时间序列的点预测和概率预测问题。 |
| [^77] | [Reliable Academic Conference Question Answering: A Study Based on Large Language Model.](http://arxiv.org/abs/2310.13028) | 本研究以大型语言模型为基础，开发了可靠的学术会议问答系统，通过组织半结构化的会议数据并进行人工标注，解决了研究人员在获取准确、最新信息时的需求。 |
| [^78] | [Be Bayesian by Attachments to Catch More Uncertainty.](http://arxiv.org/abs/2310.13027) | 本文提出了一种附加结构贝叶斯神经网络(ABNN)，通过在主干网络中整合足够分布外数据的不确定性，来提高神经网络对不确定性的捕捉能力。 |
| [^79] | [Powerset multi-class cross entropy loss for neural speaker diarization.](http://arxiv.org/abs/2310.13025) | 提出了一种将多标签公式改为幂集多类分类的方法，通过大量实验表明这种方法在性能和鲁棒性方面都显著优于原始方法，并消除了多标签公式中的关键超参数。 |
| [^80] | [Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt.](http://arxiv.org/abs/2310.13024) | 本研究旨在解决持续预训练方法在未知领域上性能下降的问题，提出了一种基于超网络提示的持续预训练方法，通过使用一致性和不一致性损失来生成领域特定的提示，显著减轻了领域特异性并促进了知识的传递。 |
| [^81] | [GraphGPT: Graph Instruction Tuning for Large Language Models.](http://arxiv.org/abs/2310.13023) | 本论文提出了GraphGPT框架，它是一种面向图结构知识的大型语言模型，通过图指令调优实现高度泛化，即使在没有下游图数据的情况下也能在不同的下游数据集和任务上取得很好的效果。 |
| [^82] | [Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding.](http://arxiv.org/abs/2310.13022) | 本论文提出了一种不确定性感知的参数高效自训练（UPET）框架，通过利用Monte Carlo dropout来进行贝叶斯神经网络中的不确定性估计，并根据置信度和确定性选择可靠的伪标记样本，以解决标记数据稀缺问题。 |
| [^83] | [AI for Mathematics: A Cognitive Science Perspective.](http://arxiv.org/abs/2310.13021) | 本文从认知科学的角度探讨了构建人类级数学系统的目标，并强调了认知科学对于AI从业者在此方向上的价值。结合开放性讨论和问题，认为需要多学科合作来实现更好的数学AI系统。 |
| [^84] | [Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm.](http://arxiv.org/abs/2310.13019) | 本文提出了一种增强版DeepFool算法，名为Targeted DeepFool，可以针对特定类别进行错误分类，并引入了最小置信度分数要求超参数来提高灵活性。 |
| [^85] | [Getting aligned on representational alignment.](http://arxiv.org/abs/2310.13018) | 该论文研究了生物和人工信息处理系统的表示一致性，探讨了不同系统之间的表示是否一致以及如何调整表示以更好地匹配其他系统。为了改善领域之间的交流，提出了一个统一的框架作为共同语言。 |
| [^86] | [Position Interpolation Improves ALiBi Extrapolation.](http://arxiv.org/abs/2310.13017) | 该论文提出了一种使用线性位置插值来改进ALiBi模型外推能力的方法，并且在上游语言建模和下游摘要和检索任务中取得了显著的改进。 |
| [^87] | [Solving the multiplication problem of a large language model system using a graph-based method.](http://arxiv.org/abs/2310.13016) | 我们提出了一种基于图形的乘法算法，通过引入10k操作符，模拟了人类类似的数值运算，解决了GPT和其他大型语言模型的乘法挑战。该算法在100万个大数乘法任务中实现了100％的准确性。 |
| [^88] | [Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition.](http://arxiv.org/abs/2310.13015) | Audio-AdapterFusion提出了一种无任务ID的多任务语音识别方法，通过结合单任务适配器实现非破坏性和参数高效，且在多个测试集上表现优异。 |
| [^89] | [Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament.](http://arxiv.org/abs/2310.13014) | 该论文通过参与Metaculus平台举办的预测竞赛，实证测试了OpenAI的最先进大型语言模型GPT-4的概率预测能力，并发现其与人类预测相比明显不准确。 |
| [^90] | [Generative error correction for code-switching speech recognition using large language models.](http://arxiv.org/abs/2310.13013) | 本论文提出了一种使用大型语言模型和自动生成的假设列表来进行代码交替语音识别的生成式错误校正方法。 |
| [^91] | [H2O Open Ecosystem for State-of-the-art Large Language Models.](http://arxiv.org/abs/2310.13012) | H2O推出了开放生态系统，旨在开发和测试最先进的大规模语言模型（LLMs），包括h2oGPT和H2O LLM Studio。这一开源项目提供了全面开放的替代方案，能够帮助推动人工智能的发展，使其更加可信赖和可访问。 |
| [^92] | [Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model.](http://arxiv.org/abs/2310.13010) | 该论文提出了一种使用基于Perceiver的序列分类器和通用语音模型检测语音异常的方法，该方法可以针对不同类别的输入建模不同的区域，同时具有数据效率性。通过在Mayo Clinic的语料库上进行广泛评估，该模型表现出色，平均准确率达到83.1%。预训练是重要的，而且意外的是，与不相关的自动语音识别（ASR）任务进行预训练也是有益的。 |
| [^93] | [LoBaSS: Gauging Learnability in Supervised Fine-tuning Data.](http://arxiv.org/abs/2310.13008) | 本文介绍了一种新的方法LoBaSS，利用数据的可学习性作为选择监督微调数据的主要标准。这种方法可以根据模型的能力将数据选择与模型对齐，确保高效的学习。 |
| [^94] | [A Critical Survey on Fairness Benefits of XAI.](http://arxiv.org/abs/2310.13007) | 这个批判性调查分析了可解释的人工智能（XAI）与公平之间的关系，指出XAI在实现公平理想方面存在潜力和限制，呼吁更具体地说明XAI方法如何帮助解决公平理想。 |
| [^95] | [Software Metadata Classification based on Generative Artificial Intelligence.](http://arxiv.org/abs/2310.13006) | 本文提出了一种利用生成式人工智能在二进制代码评论质量分类模型中提升性能的新方法，通过引入生成数据集，模型的准确性得到了显著改善，支持向量机模型的精确度提高了6%，人工神经网络模型的召回率提高了1.5%。 |
| [^96] | [Metacognitive threshold: a computational account.](http://arxiv.org/abs/2310.13005) | 元认知阈值的计算模型以及通过元认知训练和冥想影响该阈值的潜在认知机制。 |
| [^97] | [Progressively Efficient Learning.](http://arxiv.org/abs/2310.13004) | CEIL是一种渐进高效的学习框架，通过给学习代理提供一个抽象、动态的语言和一种内在的动机，以尽可能少的沟通代价学习，实现了类似人类逐步高效沟通的能力。在2D MineCraft领域上，CEIL展示了令人印象深刻的性能和沟通效率。 |
| [^98] | [Conversational Financial Information Retrieval Model (ConFIRM).](http://arxiv.org/abs/2310.13001) | ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。 |
| [^99] | [Document-Level Relation Extraction with Relation Correlation Enhancement.](http://arxiv.org/abs/2310.13000) | 该论文提出了一种关系图方法，用于解决文档级关系抽取中忽视关系相关性的问题。通过构建关系图、加权处理和利用图注意力网络，可以有效地捕捉文档中关系之间的相关性。该方法可以作为现有模型的模块集成，并在实验中取得了良好的效果。 |
| [^100] | [Adaptive Dynamic Programming for Energy-Efficient Base Station Cell Switching.](http://arxiv.org/abs/2310.12999) | 本论文提出了一种基于近似动态规划和在线优化的方法，用于在减少网络功耗的同时保持良好的服务质量。通过预测功耗和QoS，并结合适应性QoS阈值，实现了节能基站小区切换。 |
| [^101] | [Parking Spot Classification based on surround view camera system.](http://arxiv.org/abs/2310.12997) | 本文基于环视摄像系统进行停车位分类的研究，使用了适用于各种形状停车位的多边形边界框模型，并是第一个对鱼眼摄像机上的停车位进行检测与分类的详细研究。 |
| [^102] | [Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening.](http://arxiv.org/abs/2310.12996) | 本文提出了一种无监督学习的解决方案，可用于临床药物筛选中的药物反应预测。通过学习相似药物的先前反应数据，该方法能够增强对未标记化合物的实时预测。 |
| [^103] | [Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models.](http://arxiv.org/abs/2310.12995) | 本文介绍了一种综合方法，将YOLOv8模型与SAM和HQ-SAM模型相结合，在多种医学影像数据集中实现了精确的感兴趣区域分割，展示了其在医学图像分析中的有效性和潜力。 |
| [^104] | [Dimensions of Disagreement: Unpacking Divergence and Misalignment in Cognitive Science and Artificial Intelligence.](http://arxiv.org/abs/2310.12994) | 本文探讨了认知科学和人工智能中分歧的不同维度，指出分歧既可以来源于对象评估的差异，也可以源于对对象表示方式的不一致。研究者发展了用于量化表示重叠程度的工具，深入理解分歧和不一致性如何相互作用对于促进不同类型代理之间的有效协作至关重要。 |
| [^105] | [Enhancing Health Data Interoperability with Large Language Models: A FHIR Study.](http://arxiv.org/abs/2310.12989) | 本研究调查了利用大型语言模型(LLM)增强医疗数据互操作性的能力。实验证明，LLM不仅简化了多步自然语言处理和人工校准过程，而且在与人工标注进行比较时，精确匹配率超过90%。 |
| [^106] | [A survey of manifold learning and its applications for multimedia.](http://arxiv.org/abs/2310.12986) | 这项调研涉及到流形学习及其在多媒体领域的重要应用，为了解介绍了流形学习的基本概念和应用。 |
| [^107] | [Enabling energy-Efficient object detection with surrogate gradient descent in spiking neural networks.](http://arxiv.org/abs/2310.12985) | 在本研究中，我们提出了一种基于当前均值解码（CMD）方法的SNN-YOLOv3模型，用于实现能源高效的目标检测。实验结果表明，在PASCAL VOC数据集上，SNN-YOLOv3具有令人瞩目的性能，只需6个时间步，mAP达到了61.87%，比SpikingYOLO提高了近10%的mAP，同时能够降低能量消耗。 |
| [^108] | [Training Dynamics of Deep Network Linear Regions.](http://arxiv.org/abs/2310.12977) | 该研究探讨了深度神经网络的线性区域训练动力学。通过对数据点周围线性区域的局部复杂性进行统计分析，发现训练过程中这种复杂性经历了几个阶段的变化。最后，通过精确的可视化方法，观察到训练的最后阶段有一个下降趋势。 |
| [^109] | [Towards a Deep Learning-based Online Quality Prediction System for Welding Processes.](http://arxiv.org/abs/2310.12632) | 该论文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念，通过收集和管理多传感器数据以及实时处理和特征工程的方式，可以识别关系并预测焊接质量。 |
| [^110] | [Learning Co-Speech Gesture for Multimodal Aphasia Type Detection.](http://arxiv.org/abs/2310.11710) | 通过学习语音和手势之间的相关性，我们提出了一种多模态图神经网络，用于准确识别特定失语类型的检测。实验证明我们的方法优于现有方法，达到了最先进的结果（F1 84.2%）。 |
| [^111] | [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis.](http://arxiv.org/abs/2310.10477) | 该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。 |
| [^112] | [Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook.](http://arxiv.org/abs/2310.10196) | 这篇综述探讨了大型模型在时间序列和时空数据中的应用。它们不仅带来了增强的模式识别和推理能力，还为人工通用智能打下了基础。 |
| [^113] | [Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation.](http://arxiv.org/abs/2310.09886) | 这项研究提出了一种动态模块扩展和自适应的方法，旨在解决终身序列生成中的持续学习问题。该方法允许模型根据任务相关性动态决定获取新知识的架构，并选择相似的先前任务来帮助适应新任务。此外，还引入了动态梯度缩放来平衡学习过程，以避免对先前学到的知识的严重遗忘。 |
| [^114] | [HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling.](http://arxiv.org/abs/2310.09135) | 本研究提出了一种用于零样本插槽填充的Hierarchical Contrastive Learning Framework (HiCL)，通过粗粒度到细粒度的对比学习，学习语句令牌之间的深层语义关系，并提出了一种新的迭代标签集语义推理方法，来提高在未见插槽上的泛化能力。 |
| [^115] | [Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization.](http://arxiv.org/abs/2310.08394) | 本论文通过收集真实数据集riSum，对大型语言模型的指令遵循能力进行了评估，并提出了新的参考方法，以衡量这种能力。这些方法在性能上与需要高质量摘要的参考方法相当。 |
| [^116] | [A New Approach Towards Autoformalization.](http://arxiv.org/abs/2310.07957) | 该论文提出了一种新的方法来应对研究水平的数学自动形式化任务，通过将任务分解成更容易处理的子任务，包括未链接形式化、实体链接和类型调整。此外，还提出了一个用于未链接形式化的基准数据集 arXiv2Formal。 |
| [^117] | [Hierarchical Pretraining on Multimodal Electronic Health Records.](http://arxiv.org/abs/2310.07871) | 该论文提出了一种针对层次多模态电子健康记录数据的新颖、通用且统一的预训练框架MEDHMP，在三个级别上展示了其有效性，并且在与十八个基准方法的比较中验证了其高效性。 |
| [^118] | [An Empirical Study of Instruction-tuning Large Language Models in Chinese.](http://arxiv.org/abs/2310.07328) | 本论文进行了对中文大语言模型的指导调整的实证研究，探索了LLM基础、参数高效的方法和指令数据类型的影响，并研究了其他因素的影响，为定制能更好响应中文指令的LLM提供了有价值的发现。 |
| [^119] | [PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification.](http://arxiv.org/abs/2310.06923) | 本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。 |
| [^120] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^121] | [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models.](http://arxiv.org/abs/2310.05253) | 本文提出了一种基于知识的推理方法，通过大型语言模型实现可解释的主张验证。该方法不需要依赖昂贵的标注数据，能够验证复杂的主张并生成解释，对协助人工事实检查员具有重要意义。 |
| [^122] | [Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology.](http://arxiv.org/abs/2310.05227) | 物理感知机器学习是一种革命性方法，它将物理知识和机器学习相结合，提供了准确的水文学理解和水循环预测，对于管理水资源以应对气候变化等挑战具有重要意义。 |
| [^123] | [SE(3)-Stochastic Flow Matching for Protein Backbone Generation.](http://arxiv.org/abs/2310.02391) | 通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。 |
| [^124] | [On the definition of toxicity in NLP.](http://arxiv.org/abs/2310.02357) | 这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。 |
| [^125] | [Denoising Diffusion Bridge Models.](http://arxiv.org/abs/2309.16948) | 本论文提出了一种去噪扩散桥模型（DDBMs），该模型通过学习扩散桥的分数，并基于学习到的分数求解微分方程来实现从一个分布到另一个分布的映射，从而将多类生成模型统一起来。 |
| [^126] | [Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience.](http://arxiv.org/abs/2309.15302) | 提出一种名为STERLING的自我监督地形表示学习方法，通过无约束的机器人经验学习有关地形的相关表示，以实现地形感知导航。 |
| [^127] | [Towards an MLOps Architecture for XAI in Industrial Applications.](http://arxiv.org/abs/2309.12756) | 该论文提出了一种面向工业应用的XAI MLOps架构，旨在解决实际部署和管理ML模型中的解释和反馈能力的挑战，并提高模型准确性和用户体验。 |
| [^128] | [Audio Contrastive based Fine-tuning.](http://arxiv.org/abs/2309.11895) | 本论文提出了一种基于音频对比的微调方法（AudioConFit），通过借助对比学习的可转移性，该方法在各种音频分类任务中表现出强大的泛化能力，并在不同设置下实现了最先进的结果。 |
| [^129] | [A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing.](http://arxiv.org/abs/2309.10532) | 基于人类认知启发，我们提出了一种新的神经结构，用于解决视觉抽象推理任务。该架构通过追求感知和概念处理之间的一致性，采用迭代、自对比的学习过程来模拟人类的认知过程。实验证明，该网络在RAVEN数据集上实现了比之前所有模型更高的准确性，并且使用了最弱的归纳偏置。我们还指出了原始数据集中存在的类不平衡问题，并提出了解决方案。 |
| [^130] | [ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots.](http://arxiv.org/abs/2308.06920) | 本文以抗可卡因成瘾药物研发为例，展示了OpenAI开发的ChatGPT在药物研发领域的创新应用。通过利用GPT-4作为虚拟导引，ChatGPT为研究人员提供战略和方法论的见解，促进了创新的药物研发方法。这项研究揭示了AI在现代药物研发中的重要作用。 |
| [^131] | [A Game-Theoretic Framework for Joint Forecasting and Planning.](http://arxiv.org/abs/2308.06137) | 本论文提出了一个新颖的博弈论框架，用于联合规划和预测，以改进机器人动作的安全性。通过使用游戏理论的方法，该框架可以学习预测人类防范对策，并通过实际算法实现了端到端的模型训练。研究结果表明，该算法在人群导航和现实世界的行人运动数据集中可以产生更安全的规划。 |
| [^132] | [Nature and the Machines.](http://arxiv.org/abs/2308.04440) | 《自然》杂志在一篇社论中呼吁我们“停止谈论明天的AI末日，而AI今天就存在风险。”这被认为是一个严重的判断失误，特别是对于有影响力的行动者来说，因为我们期望他们能够考虑到错误的后果。 |
| [^133] | [Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation.](http://arxiv.org/abs/2308.03723) | 本研究提出了一种降维方法，通过在医学图像分割中应用马氏距离后处理和主成分分析，实现对越界图像的高效检测。 |
| [^134] | [The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models.](http://arxiv.org/abs/2308.00245) | 本文详细研究了以使用前初始化错误为案例的大型语言模型辅助静态分析的开放空间，并开发了一种全自动代理程序LLift，该程序能够克服多个挑战，包括错误建模。 |
| [^135] | [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus.](http://arxiv.org/abs/2307.11760) | EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。 |
| [^136] | [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models.](http://arxiv.org/abs/2307.11224) | Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。 |
| [^137] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^138] | [Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation.](http://arxiv.org/abs/2306.17817) | Act3D是一种基于Transformer的操作策略，将6自由度关键姿势预测作为3D检测任务，并以自适应空间计算的方式进行处理。它在高度精确的机器人操纵任务中取得了显著的性能改进。 |
| [^139] | [IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors.](http://arxiv.org/abs/2306.15228) | 本研究提出了Implicit Interactive Fleet Learning (IIFL)，将隐性策略推广到交互式模仿学习中，能够解决多模态和分布转移问题。 |
| [^140] | [FuXi: A cascade machine learning forecasting system for 15-day global weather forecast.](http://arxiv.org/abs/2306.12873) | 逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。 |
| [^141] | [Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery.](http://arxiv.org/abs/2306.12802) | 本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。 |
| [^142] | [Trained Transformers Learn Linear Models In-Context.](http://arxiv.org/abs/2306.09927) | 本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。 |
| [^143] | [Description Logics with Abstraction and Refinement.](http://arxiv.org/abs/2306.03717) | 本文提出了一种扩展的描述逻辑（DLs）以支持多个抽象层次的知识表示，并证明了推理在该逻辑中是可判定的。 |
| [^144] | [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.](http://arxiv.org/abs/2306.03341) | 本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。 |
| [^145] | [Handling Realistic Label Noise in BERT Text Classification.](http://arxiv.org/abs/2305.16337) | 本文研究了BERT在现实标签噪声存在下的分类性能，发现特征相关的标签噪声和来自注释者分歧的合成标签噪声会导致BERT的分类性能下降。提出不同类型的集成和噪声清理方法以提高鲁棒性。 |
| [^146] | [Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples.](http://arxiv.org/abs/2305.15269) | 该研究使用OOD案例测试了大型语言模型的普遍演绎推理能力，并通过构建新的推理数据集进行了探究。研究结果表明，这些模型能够推广到更复杂的证明。 |
| [^147] | [SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables.](http://arxiv.org/abs/2305.13186) | SCITAB是一个具有挑战性的评估数据集，包含1.2K个经验证的科学事实和相关的科学表格，要求进行组合推理和事实验证。对于最先进的模型来说，SCITAB提出了许多独特挑战，包括表格定位、事实歧义和组合推理。所有模型中，除了GPT-4之外，性能仅略高于随机猜测。提示技术如思维链对于在SCITAB上提升性能几乎没有作用。 |
| [^148] | [GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study.](http://arxiv.org/abs/2305.13062) | 本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。 |
| [^149] | [The Scope of ChatGPT in Software Engineering: A Thorough Investigation.](http://arxiv.org/abs/2305.12138) | ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。 |
| [^150] | [Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings.](http://arxiv.org/abs/2305.12027) | 本文提出了 DUCK 方法，通过使用实体类型的先前知识在实体表示空间中注入结构信息。把盒嵌入概念引入到极坐标中，将关系表示为超球面上的盒子，将具有相似类型的实体放置在对应于它们关系的盒子内，实现聚类。在实体链接方面取得了最先进的结果，尤其在低资源环境下效果显著。 |
| [^151] | [What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability.](http://arxiv.org/abs/2305.11707) | 本文分析了神经文本生成器与人类生产变异性之间的不确定性，通过探测生成器的输出空间来测量其对人类生产变异性的校准程度，并证明用多个样本和多个参考可以更好地了解模型的不确定性表示。 |
| [^152] | [INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models.](http://arxiv.org/abs/2305.06677) | 本文提出了一种使用信息丰富的数据子集来高效预训练大型语言模型的方法，减少了训练时间和计算成本，同时保持了模型的泛化能力。 |
| [^153] | [Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden.](http://arxiv.org/abs/2305.02321) | 本文研究了自动生成新闻文章摘要中的政治偏见。研究发现，与特朗普相比，更多的美国政府集体机构（即政府）与拜登相关联。这些发现为未来的摘要偏见研究提供了一个框架。 |
| [^154] | [Q-based Equilibria.](http://arxiv.org/abs/2304.12647) | 该论文研究了基于Q的策略规则族中的均衡偏差（或 Qb-equilibria），即Q值在不同监测技术下的效果。 |
| [^155] | [Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching.](http://arxiv.org/abs/2304.03215) | 本文提出了一种带有跨设备交叉注意力的分层图神经网络(HGNN)，用于解决跨设备用户匹配问题，相对于最先进的TGCE方法，提高了5%的性能。 |
| [^156] | [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation.](http://arxiv.org/abs/2303.12570) | RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。 |
| [^157] | [CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models.](http://arxiv.org/abs/2302.12343) | 提出了CHiLL，一种用于从医疗记录中提取特征的方式，通过对大型语言模型进行查询生成特征，使医生能够用自己的专业知识制作对下游任务有临床意义的特征，并且该方法在性能和可解释性方面表现出良好的结果。 |
| [^158] | [On the Role of Morphological Information for Contextual Lemmatization.](http://arxiv.org/abs/2302.00407) | 本文研究了形态信息在六种语言的上下文词形还原器开发中的作用，并对词形还原器进行了领域外性能评估。 |
| [^159] | [The Past, Current, and Future of Neonatal Intensive Care Units with Artificial Intelligence.](http://arxiv.org/abs/2302.00225) | 本论文回顾了新生儿科学中最近发展的基于机器学习和深度学习的解决方案，系统评估了它们在新生儿科应用中的作用，并描述了最新的研究进展。 |
| [^160] | [A Scalable Technique for Weak-Supervised Learning with Domain Constraints.](http://arxiv.org/abs/2301.05253) | 我们提出了一种使用领域约束的可扩展技术，用于弱监督学习神经网络分类未标记数据。我们通过在MNIST图像分类问题上的实验证明，我们的方法比以前的方法在规模上有显著的提升。 |
| [^161] | [Progress measures for grokking via mechanistic interpretability.](http://arxiv.org/abs/2301.05217) | 该论文通过机械解释性的方法找到了连续的进展测量，以理解神经网络中 emergent behavior 的产生原因。其中，作者以“grokking”现象为案例，通过逆向工程的方式完全理解了小型transformer网络在模块化加法任务中的算法，并定义了进展测量来研究这个现象的进展性质。 |
| [^162] | [Unleashing the Power of Shared Label Structures for Human Activity Recognition.](http://arxiv.org/abs/2301.03462) | 本文提出了SHARE，一种考虑共享标签结构的HAR框架，通过建模共同的结构从而揭示不同活动之间的知识。 |
| [^163] | [Data-centric Artificial Intelligence.](http://arxiv.org/abs/2212.11854) | 数据中心人工智能是一种新兴的范式，它强调系统性地设计和构建数据对于建立有效和高效的基于人工智能的系统至关重要。 |
| [^164] | [Consistent and Truthful Interpretation with Fourier Analysis.](http://arxiv.org/abs/2210.17426) | 该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。 |
| [^165] | [Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization.](http://arxiv.org/abs/2210.05242) | 该论文提出了一种新颖的视频级语义一致性引导网络，用于视听事件定位任务。它利用事件的视频级语义信息进行语义一致性建模，通过跨模态事件表示提取器和内模态语义一致性增强器实现。与现有方法相比，该方法能更好地捕捉和利用同一视频中事件的语义一致性。 |
| [^166] | [DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation.](http://arxiv.org/abs/2207.09920) | 本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。 |
| [^167] | [Pruning-aware Sparse Regularization for Network Pruning.](http://arxiv.org/abs/2201.06776) | 本文提出了一种修剪感知的稀疏正则化的网络剪枝方法，使用精细的稀疏正则化仅对由剪枝掩模选择的特定滤波器进行处理，从而减少网络容量限制和欠拟合。 |
| [^168] | [Numerical influence of ReLU'(0) on backpropagation.](http://arxiv.org/abs/2106.12915) | 本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。 |
| [^169] | [Lifetime policy reuse and the importance of task capacity.](http://arxiv.org/abs/2106.01741) | 本文提出了生命周期策略重用算法和任务容量指标，通过优化一组近似最优策略和策略选择，实现了在终身强化学习中避免生成大量策略的目标。实验证明了生命周期策略重用和任务容量预选对多任务学习的重要性。 |
| [^170] | [On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective.](http://arxiv.org/abs/2011.11152) | 本论文发现了权重衰减在训练的最后阶段会导致大梯度范数的陷阱，为了解决这个问题，提出了Scheduled Weight Decay（SWD）方法，通过动态调整权重衰减强度并惩罚大梯度范数，可以有效缓解这个问题。 |

# 详细

[^1]: Hunayn：超越字面意义的翻译进步

    Hunayn: Elevating Translation Beyond the Literal. (arXiv:2310.13613v1 [cs.CL])

    [http://arxiv.org/abs/2310.13613](http://arxiv.org/abs/2310.13613)

    这项研究介绍了一种超越传统工具的高级英译阿拉伯语翻译器，使用赫尔辛基变压器和纯文学阿拉伯语数据集，表现出色，并强调了其在文化敏感性和语境准确性方面的优势。

    

    该项目介绍了一种超越传统工具的高级英译阿拉伯语翻译器。利用赫尔辛基变压器（MarianMT），我们的方法涉及在一个自动生成的、纯文学阿拉伯语数据集上进行微调。与谷歌翻译的评估表明，在定性评估中始终表现出色。值得注意的是，它在文化敏感性和语境准确性方面表现出色。这项研究强调了赫尔辛基变压器在使用Fusha数据集的英阿翻译中的优势。

    This project introduces an advanced English-to-Arabic translator surpassing conventional tools. Leveraging the Helsinki transformer (MarianMT), our approach involves fine-tuning on a self-scraped, purely literary Arabic dataset. Evaluations against Google Translate show consistent outperformance in qualitative assessments. Notably, it excels in cultural sensitivity and context accuracy. This research underscores the Helsinki transformer's superiority for English-to-Arabic translation using a Fusha dataset.
    
[^2]: 使您的决策有说服力！一个统一的两阶段框架：自我归因和决策制定。

    Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making. (arXiv:2310.13610v1 [cs.CL])

    [http://arxiv.org/abs/2310.13610](http://arxiv.org/abs/2310.13610)

    提出了一个名为自我归因和决策制定的统一两阶段框架（SADM），通过使用输入文本的子序列作为解释来建立更可靠的模型决策解释联系，并在ERASER基准测试中取得了竞争性的结果。

    

    在各种自然语言处理任务中，用自然语言解释黑盒模型的行为已经取得了令人印象深刻的结果。最近的研究探索了利用输入文本的子序列作为解释的方法，为用户提供支持模型决策的证据。尽管现有的框架在生成高质量解释的同时取得了高任务性能，但它们忽略了生成的解释与模型决策之间的不可靠联系。换句话说，模型可能在归因错误的情况下做出正确的决策，或者在归因正确的情况下做出糟糕的决策。为了解决这个问题，我们提出了一个称为自我归因和决策制定的统一两阶段框架（SADM）。通过对ERASER基准测试的五个推理数据集进行广泛实验，我们证明了我们的框架不仅能建立生成解释和模型决策之间更可靠的联系，而且还能取得竞争性的结果。

    Explaining black-box model behavior with natural language has achieved impressive results in various NLP tasks. Recent research has explored the utilization of subsequences from the input text as a rationale, providing users with evidence to support the model decision. Although existing frameworks excel in generating high-quality rationales while achieving high task performance, they neglect to account for the unreliable link between the generated rationale and model decision. In simpler terms, a model may make correct decisions while attributing wrong rationales, or make poor decisions while attributing correct rationales. To mitigate this issue, we propose a unified two-stage framework known as Self-Attribution and Decision-Making (SADM). Through extensive experiments on five reasoning datasets from the ERASER benchmark, we demonstrate that our framework not only establishes a more reliable link between the generated rationale and model decision but also achieves competitive results 
    
[^3]: MULTITuDE: 大规模多语言机器生成文本检测基准

    MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark. (arXiv:2310.13606v1 [cs.CL])

    [http://arxiv.org/abs/2310.13606](http://arxiv.org/abs/2310.13606)

    这篇论文介绍了MULTITuDE，一个针对多语言机器生成文本检测的基准数据集。通过比较不同的检测器在零样本和微调情况下的性能，研究了这些检测器对于未见过的语言和生成器的泛化能力，并探讨了在多语言训练下检测器的性能提升。

    

    目前对于最近的LLMs生成其他语言的令人信服的文本能力以及多语言环境下机器生成文本检测器的性能研究不足。这也反映在现有的基准数据集中，缺乏其他语言真实文本，主要涵盖较旧的生成器。为了填补这一空白，我们引入了MULTITuDE，一个新颖的多语言机器生成文本检测基准数据集，包含11种语言（阿拉伯语，加泰罗尼亚语，捷克语，德语，英语，西班牙语，荷兰语，葡萄牙语，俄语，乌克兰语和中文）生成的74,081个真实和机器生成的文本，由8个多语言LLMs生成。使用这个基准，我们比较了零样本（统计和黑盒）和微调检测器的性能。考虑到多语性，我们评估了1）这些检测器对于未见过的语言（语言上相似和不相似的语言）和未见过的LLMs的泛化能力，以及2）当在多个语言上进行训练时，检测器是否能提高其性能。

    There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple lang
    
[^4]: 基于Transformer网络和跨尺度依赖建模改进的皮肤病变分割

    Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale Dependency Modeling. (arXiv:2310.13604v1 [eess.IV])

    [http://arxiv.org/abs/2310.13604](http://arxiv.org/abs/2310.13604)

    本论文介绍了一种基于Transformer网络和跨尺度依赖建模的皮肤病变分割方法，通过替换CNN块和优化连接路径来提高网络特征的可重用性，以实现更准确的医学图像分割。

    

    早期发现可以治疗的危险性皮肤癌症类型——黑色素瘤。为了通过自动皮肤病变分割协助诊断，提出了各种使用全卷积网络（FCNs）的方法，其中突出的是U-Net架构。然而，对卷积操作的依赖使得对准确医学图像分割至关重要的长程依赖的捕捉能力受到限制。近期创建了几种基于Transformer的U-Net拓扑结构，通过将CNN块替换为不同的Transformer模块来捕捉局部和全局表示。此外，编码器和解码器之间的语义差距限制了U型结构的效果。本研究旨在通过精心构建跳过连接路径，增加网络特征的可重用性。在跳过连接路径中整合已计算的注意力亲和性可以提高网络性能。

    Melanoma, a dangerous type of skin cancer resulting from abnormal skin cell growth, can be treated if detected early. Various approaches using Fully Convolutional Networks (FCNs) have been proposed, with the U-Net architecture being prominent To aid in its diagnosis through automatic skin lesion segmentation. However, the symmetrical U-Net model's reliance on convolutional operations hinders its ability to capture long-range dependencies crucial for accurate medical image segmentation. Several Transformer-based U-Net topologies have recently been created to overcome this limitation by replacing CNN blocks with different Transformer modules to capture local and global representations. Furthermore, the U-shaped structure is hampered by semantic gaps between the encoder and decoder. This study intends to increase the network's feature re-usability by carefully building the skip connection path. Integrating an already calculated attention affinity within the skip connection path improves t
    
[^5]: MarineGPT: 向公众揭示海洋的秘密

    MarineGPT: Unlocking Secrets of Ocean to the Public. (arXiv:2310.13596v1 [cs.CL])

    [http://arxiv.org/abs/2310.13596](http://arxiv.org/abs/2310.13596)

    这项工作介绍了一种名为MarineGPT的海洋领域专用多模态大型语言模型，该模型能够以敏感、信息丰富和科学的方式回应用户的需求。

    

    大型语言模型（LLM），如ChatGPT/GPT-4，被证明是提升用户体验的强大工具，可作为人工智能助手。最近的研究提出了多模态大型语言模型（MLLM），通过构建联合语义空间（如视觉-文本空间），使LLM具备感知多模态输入的能力。尽管LLM和MLLM取得了巨大成功，但在需要领域特定知识和专业知识的领域特定应用中，特别是对于海洋领域，对LLM和MLLM的探索尚不充分。与通用目的的MLLM不同，海洋特定MLLM需要产生更加敏感、信息丰富和具有科学性的回应。在这项工作中，我们证明了现有的通过大量通用培训数据进行优化的MLLM在理解领域特定意图并生成信息丰富且满意的回应方面能力有限。

    Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be powerful tools in promoting the user experience as an AI assistant. The continuous works are proposing multi-modal large language models (MLLM), empowering LLMs with the ability to sense multiple modality inputs through constructing a joint semantic space (e.g. visual-text space). Though significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in domain-specific applications that required domain-specific knowledge and expertise has been less conducted, especially for \textbf{marine domain}. Different from general-purpose MLLMs, the marine-specific MLLM is required to yield much more \textbf{sensitive}, \textbf{informative}, and \textbf{scientific} responses. In this work, we demonstrate that the existing MLLMs optimized on huge amounts of readily available general-purpose training data show a minimal ability to understand domain-specific intents and then generate informative and satisfactory resp
    
[^6]: SPARE: 用于关系数据库的单遍神经模型

    SPARE: A Single-Pass Neural Model for Relational Databases. (arXiv:2310.13581v1 [cs.DB])

    [http://arxiv.org/abs/2310.13581](http://arxiv.org/abs/2310.13581)

    SPARE是一种单遍神经模型，用于高效训练关系数据库，并提供与图神经网络相似的准确度。

    

    尽管深度神经网络在图像和文本方面有着 extensive的研究，但是在关系数据库（RDBs）方面的深度学习仍然是一个相对未开发的领域。最近流行的一个方向是将图神经网络（GNNs）应用于RBDs。然而，在大规模的关系数据库上训练GNNs效率较低，主要是因为需要多轮训练和潜在的大而低效的表示。因此，在本文中，我们提出了SPARE（Single-Pass Relational models），一种可以在关系数据库上高效训练并且提供与GNNs相似准确度的新型神经模型。为了实现高效训练，与GNNs不同，SPARE利用了关系数据库中数据的规则结构，可以在单遍训练时利用对称性。我们的大量实证评估证明，SPARE可以显著加快训练和推理的速度，同时提供相似的准确度。

    While there has been extensive work on deep neural networks for images and text, deep learning for relational databases (RDBs) is still a rather unexplored field.  One direction that recently gained traction is to apply Graph Neural Networks (GNNs) to RBDs. However, training GNNs on large relational databases (i.e., data stored in multiple database tables) is rather inefficient due to multiple rounds of training and potentially large and inefficient representations. Hence, in this paper we propose SPARE (Single-Pass Relational models), a new class of neural models that can be trained efficiently on RDBs while providing similar accuracies as GNNs. For enabling efficient training, different from GNNs, SPARE makes use of the fact that data in RDBs has a regular structure, which allows one to train these models in a single pass while exploiting symmetries at the same time. Our extensive empirical evaluation demonstrates that SPARE can significantly speedup both training and inference while
    
[^7]: 在DAG空间中使用基于模型的强化学习进行因果发现的树搜索

    Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])

    [http://arxiv.org/abs/2310.13576](http://arxiv.org/abs/2310.13576)

    本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。

    

    识别因果结构对于许多领域至关重要，从战略决策到生物学和经济学。在这项工作中，我们提出了一种基于树搜索的因果发现的模型驱动强化学习方法，该方法逐步构建有向无环图。我们还形式化并证明了一种排除会引入循环的边的高效算法的正确性，这使得在DAG空间中进行更深入的离散搜索和采样成为可能。我们在两个实际任务中评估了我们的方法，在性能上比最先进的无模型方法和贪婪搜索取得了显著优势，这是组合方法的一个有希望的进展。

    Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
    
[^8]: 通过自适应风格技术提升指纹活体检测的泛化能力

    Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection. (arXiv:2310.13573v1 [cs.CV])

    [http://arxiv.org/abs/2310.13573](http://arxiv.org/abs/2310.13573)

    该论文介绍了一种通过自适应风格技术提升指纹活体检测泛化能力的方法，并在LivDet 2023挑战中取得了最先进的性能。

    

    我们引入了一种性能优异的指纹活体特征提取技术，该技术在LivDet 2023指纹表示挑战中获得了第一名。此外，我们还开发了一个实用的指纹识别系统，准确率达到了94.68%，在LivDet 2023动作中的活体检测中获得了第二名。通过对各种方法的研究，特别是风格转换，我们证明了在面对有限的训练数据时，准确率和泛化能力的提高。因此，我们的方法在LivDet 2023挑战中取得了最先进的性能。

    We introduce a high-performance fingerprint liveness feature extraction technique that secured first place in LivDet 2023 Fingerprint Representation Challenge. Additionally, we developed a practical fingerprint recognition system with 94.68% accuracy, earning second place in LivDet 2023 Liveness Detection in Action. By investigating various methods, particularly style transfer, we demonstrate improvements in accuracy and generalization when faced with limited training data. As a result, our approach achieved state-of-the-art performance in LivDet 2023 Challenges.
    
[^9]: 通过逻辑推理和相关性评分的检索增强型神经响应生成

    Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring. (arXiv:2310.13566v1 [cs.CL])

    [http://arxiv.org/abs/2310.13566](http://arxiv.org/abs/2310.13566)

    这项研究介绍了一种结合了检索增强语言模型和逻辑推理的知识驱动响应生成方法，通过在每个对话轮次中评估知识图中的节点和边的对话相关性，在任务型对话系统中取得了较好的效果。

    

    在任务型对话系统中构造响应通常依赖于当前对话状态或外部数据库等信息源。本文提出了一种结合了检索增强语言模型和逻辑推理的知识驱动响应生成的新方法。该方法围绕着表示当前对话状态和背景信息的知识图，并通过三个步骤来进行。首先，使用概率逻辑编程推断出逻辑推导的事实，丰富知识图。然后，在每个对话轮次中使用神经模型来评分扩展图中每个节点和边的对话相关性。最后，将相关性得分最高的元素转换成自然语言形式，并整合到用于生成系统响应的神经对话模型的提示中。我们在两个数据集（KVRET和GraphWOZ）上研究了该方法的优势。

    Constructing responses in task-oriented dialogue systems typically relies on information sources such the current dialogue state or external databases. This paper presents a novel approach to knowledge-grounded response generation that combines retrieval-augmented language models with logical reasoning. The approach revolves around a knowledge graph representing the current dialogue state and background information, and proceeds in three steps. The knowledge graph is first enriched with logically derived facts inferred using probabilistic logical programming. A neural model is then employed at each turn to score the conversational relevance of each node and edge of this extended graph. Finally, the elements with highest relevance scores are converted to a natural language form, and are integrated into the prompt for the neural conversational model employed to generate the system response.  We investigate the benefits of the proposed approach on two datasets (KVRET and GraphWOZ) along w
    
[^10]: 为更快乐的自主网络安全代理设计的奖励塑造

    Reward Shaping for Happier Autonomous Cyber Security Agents. (arXiv:2310.13565v1 [cs.LG])

    [http://arxiv.org/abs/2310.13565](http://arxiv.org/abs/2310.13565)

    本研究探讨了在计算机网络防御任务中，奖励信号的特点对深度强化学习算法的影响，并研究了奖励塑造技术以提高代理的训练效率和性能。

    

    随着机器学习模型的能力增强，它们在解决复杂任务方面展示出了增加的潜力。最有前景的方向之一是使用深度强化学习来训练计算机网络防御任务中的自主代理。本研究研究了在训练此任务时为代理提供的奖励信号的影响。由于网络安全任务的性质，奖励信号通常采用以下形式：1）以惩罚的形式（例如发生了妥协）；2）在每次防御任务中稀疏分布。这样的奖励特征不同于经典强化学习任务，其中代理定期获得进展的奖励（而不是偶尔因失败而受到惩罚）。我们研究了能够弥合这一差距的奖励塑造技术，以使代理更能有效地训练，并潜在地收敛到更好的性能。首先，我们展示了深度强化学习算法对奖励幅度的敏感性。

    As machine learning models become more capable, they have exhibited increased potential in solving complex tasks. One of the most promising directions uses deep reinforcement learning to train autonomous agents in computer network defense tasks. This work studies the impact of the reward signal that is provided to the agents when training for this task. Due to the nature of cybersecurity tasks, the reward signal is typically 1) in the form of penalties (e.g., when a compromise occurs), and 2) distributed sparsely across each defense episode. Such reward characteristics are atypical of classic reinforcement learning tasks where the agent is regularly rewarded for progress (cf. to getting occasionally penalized for failures). We investigate reward shaping techniques that could bridge this gap so as to enable agents to train more sample-efficiently and potentially converge to a better performance. We first show that deep reinforcement learning algorithms are sensitive to the magnitude of 
    
[^11]: 自我启示的大型语言模型在开放领域多跳推理中的应用

    Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning. (arXiv:2310.13552v1 [cs.CL])

    [http://arxiv.org/abs/2310.13552](http://arxiv.org/abs/2310.13552)

    本文提出了自我启示的思维链（SP-CoT）框架，在开放领域多跳推理中利用大型语言模型（LLMs），通过自动化生成高质量的CoTs，扩展了开放领域问答的能力。

    

    在开放领域问答（ODQA）中，大多数问题需要对常识进行单跳推理。为了进一步扩展这个任务，我们正式引入了开放领域多跳推理（ODMR），通过在开放领域设置中回答多跳问题并提供明确的推理步骤。最近，大型语言模型（LLMs）在无需外部语料库的情况下在促进ODQA方面发挥了重要作用。此外，思维链（CoT）提示通过手动或自动化范例，显著提升了LLMs的推理能力。然而，现有的自动化方法缺乏质量保证，而手动方法受到可扩展性和多样性的限制，阻碍了LLMs的能力。在本文中，我们提出了自我启示的思维链（SP-CoT），这是一个自动化框架，可以通过LLMs并为LLMs大规模生成高质量的CoTs。

    In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT s
    
[^12]: 探索语言模型中谄媚行为的理解

    Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])

    [http://arxiv.org/abs/2310.13548](http://arxiv.org/abs/2310.13548)

    这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。

    

    「从人类反馈中进行强化学习（RLHF）」是训练高质量AI助手的一种流行技术。然而，RLHF可能会鼓励模型通过与用户信念相符的回答来代替真实回答，这种行为被称为谄媚行为。我们研究了RLHF训练模型中谄媚行为的普遍性以及人类偏好判断是否起到了作用。首先，我们证明了五个最先进的AI助手在四个不同的自由文本生成任务中一贯表现出谄媚行为。为了理解人类偏好是否驱动了RLHF模型的这种广泛行为，我们分析了现有的人类偏好数据。我们发现，当回答与用户的观点相符时，它更有可能被选中。此外，人类和偏好模型（PMs）将有说服力的谄媚回答与正确回答相比，有时几乎可以忽略不计地选择了谄媚回答。优化模型输出以满足PMs有时也会在真实性和谄媚行为之间做出取舍。

    Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
    
[^13]: ScaleLong: 通过缩放长跳接连接实现扩散模型的更稳定训练

    ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection. (arXiv:2310.13545v1 [cs.CV])

    [http://arxiv.org/abs/2310.13545](http://arxiv.org/abs/2310.13545)

    通过缩小长跳接连接系数，扩散模型中的UNet训练更加稳定，并且在理论上证明了UNet对于输入的扰动敏感，并产生振荡效果。

    

    在扩散模型中，UNet是最受欢迎的网络骨干，因为其长跳接连接可以聚合远距离信息并减轻消失梯度问题。不幸的是，在扩散模型中，UNet经常遭受不稳定的训练，通过缩小其长跳接连接系数可以缓解该问题。然而，关于UNet在扩散模型中的不稳定性以及长跳接连接缩放的性能改进的理论理解尚未出现。为了解决这个问题，我们从理论上证明了UNet中长跳接连接系数对前向和后向传播的稳定性以及UNet的鲁棒性有很大影响。具体而言，UNet在任何层的隐藏特征和梯度都会振荡，其振荡范围实际上很大，这解释了UNet训练的不稳定性。此外，UNet对扰动输入也具有敏感性，并且会产生与期望输出相距较远的输出，从而产生振荡效果。

    In diffusion models, UNet is the most popular network backbone, since its long skip connects (LSCs) to connect distant network blocks can aggregate long-distant information and alleviate vanishing gradient. Unfortunately, UNet often suffers from unstable training in diffusion models which can be alleviated by scaling its LSC coefficients smaller. However, theoretical understandings of the instability of UNet in diffusion models and also the performance improvement of LSC scaling remain absent yet. To solve this issue, we theoretically show that the coefficients of LSCs in UNet have big effects on the stableness of the forward and backward propagation and robustness of UNet. Specifically, the hidden feature and gradient of UNet at any layer can oscillate and their oscillation ranges are actually large which explains the instability of UNet training. Moreover, UNet is also provably sensitive to perturbed input, and predicts an output distant from the desired output, yielding oscillatory 
    
[^14]: 基于结构感知图学习的正-未标记节点分类

    Positive-Unlabeled Node Classification with Structure-aware Graph Learning. (arXiv:2310.13538v1 [cs.LG])

    [http://arxiv.org/abs/2310.13538](http://arxiv.org/abs/2310.13538)

    本文提出了一种利用图结构进行正-未标记节点分类的方法。我们引入了一个距离感知的PU损失来提供更准确的监督，同时通过正则化项与图结构对齐。实验证明该方法在多种图数据集上具有卓越性能。

    

    在图上进行节点分类是一个具有许多应用的重要研究问题。现实世界中的图数据集可能并不如大多数现有工作所假定的那样平衡和准确。正-未标记（PU）节点分类是一个具有挑战性的设置，其中标记节点被限制为正节点。它具有多样的应用，例如疫情预测或网络异常检测。现有的PU节点分类工作忽视了图结构中的信息，而这可能是关键的。在本文中，我们提出了更好地利用图结构进行PU节点分类的方法。我们首先提出了一个距离感知的PU损失，利用图中的同质性引入更准确的监督。我们还提出了一个正则化项，使模型与图结构对齐。理论分析表明，最小化所提出的损失函数也导致最小化带有正负标签的期望损失。对多种图数据集进行的大量实证评估证明了其卓越的性能。

    Node classification on graphs is an important research problem with many applications. Real-world graph data sets may not be balanced and accurate as assumed by most existing works. A challenging setting is positive-unlabeled (PU) node classification, where labeled nodes are restricted to positive nodes. It has diverse applications, e.g., pandemic prediction or network anomaly detection. Existing works on PU node classification overlook information in the graph structure, which can be critical. In this paper, we propose to better utilize graph structure for PU node classification. We first propose a distance-aware PU loss that uses homophily in graphs to introduce more accurate supervision. We also propose a regularizer to align the model with graph structure. Theoretical analysis shows that minimizing the proposed loss also leads to minimizing the expected loss with both positive and negative labels. Extensive empirical evaluation on diverse graph data sets demonstrates its superior p
    
[^15]: ICCV 2023视觉连续学习挑战的技术报告：语义分割的连续测试时间适应

    Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation. (arXiv:2310.13533v1 [cs.CV])

    [http://arxiv.org/abs/2310.13533](http://arxiv.org/abs/2310.13533)

    该论文提出了一种连续测试时间适应方法，用于解决语义分割任务中视频序列中逐渐变化的领域问题。该方法在每个图像序列中单独评估，并通过逐渐改变天气条件和时段来测试模型的性能。

    

    该挑战的目标是开发一种测试时间适应方法（TTA），该方法能够适应视频序列中逐渐变化的领域，用于语义分割任务。它基于一个合成驾驶视频数据集- SHIFT。源模型是在天气晴朗的白天拍摄的图像上训练的。测试时间的领域变化主要是由于不同的天气条件和不同时段。TTA方法在每个图像序列（视频）中单独评估，意味着在下一个序列之前，模型会被重新置为源模型状态。图像一个接一个地到达，每个帧到达时都需要进行预测。每个序列由401个图像组成，从源领域开始，然后逐渐转变为不同的领域（改变天气或时段），直到序列的中间部分。在序列的后半部分，领域逐渐回归到源领域。只有SHIFT数据集的验证集提供了真实数据。

    The goal of the challenge is to develop a test-time adaptation (TTA) method, which could adapt the model to gradually changing domains in video sequences for semantic segmentation task. It is based on a synthetic driving video dataset - SHIFT. The source model is trained on images taken during daytime in clear weather. Domain changes at test-time are mainly caused by varying weather conditions and times of day. The TTA methods are evaluated in each image sequence (video) separately, meaning the model is reset to the source model state before the next sequence. Images come one by one and a prediction has to be made at the arrival of each frame. Each sequence is composed of 401 images and starts with the source domain, then gradually drifts to a different one (changing weather or time of day) until the middle of the sequence. In the second half of the sequence, the domain gradually shifts back to the source one. Ground truth data is available only for the validation split of the SHIFT da
    
[^16]: 基于测量的变分量子计算用于生成建模

    Variational measurement-based quantum computation for generative modeling. (arXiv:2310.13524v1 [quant-ph])

    [http://arxiv.org/abs/2310.13524](http://arxiv.org/abs/2310.13524)

    这项研究提出了一种基于测量的变分量子计算算法，将量子测量的随机性视为计算资源，并应用于生成建模任务。

    

    基于测量的量子计算（MBQC）提供了一种基本独特的范例来设计量子算法。在MBQC中，由于量子测量的固有随机性，自然的操作不是确定性和幺正的，而是通过概率附带的。然而，到目前为止，MBQC的主要算法应用是完全抵消这种概率性质，以模拟表达在电路模型中的幺正计算。在这项工作中，我们提出了设计MBQC算法的思路，该算法接受这种固有随机性，并将MBQC中的随机附带视为计算资源。我们考虑了随机性有益的自然应用，即生成建模，这是一个以生成复杂概率分布为中心的机器学习任务。为了解决这个任务，我们提出了一个具有控制参数的变分MBQC算法，可以直接调整允许在计算中引入的随机程度。

    Measurement-based quantum computation (MBQC) offers a fundamentally unique paradigm to design quantum algorithms. Indeed, due to the inherent randomness of quantum measurements, the natural operations in MBQC are not deterministic and unitary, but are rather augmented with probabilistic byproducts. Yet, the main algorithmic use of MBQC so far has been to completely counteract this probabilistic nature in order to simulate unitary computations expressed in the circuit model. In this work, we propose designing MBQC algorithms that embrace this inherent randomness and treat the random byproducts in MBQC as a resource for computation. As a natural application where randomness can be beneficial, we consider generative modeling, a task in machine learning centered around generating complex probability distributions. To address this task, we propose a variational MBQC algorithm equipped with control parameters that allow to directly adjust the degree of randomness to be admitted in the comput
    
[^17]: RaceLens:一种基于机器智能的赛车照片分析应用

    RaceLens: A Machine Intelligence-Based Application for Racing Photo Analysis. (arXiv:2310.13515v1 [cs.CV])

    [http://arxiv.org/abs/2310.13515](http://arxiv.org/abs/2310.13515)

    RaceLens是一种基于机器智能的赛车照片分析应用，利用先进的深度学习和计算机视觉模型对赛车照片进行全面分析。通过持续改进和增强数据集的方法，RaceLens的性能和准确性得到了提高，并成功应用于NASCAR车队，对其战略决策和表现产生了直接影响。

    

    本文介绍了RaceLens，一种利用先进的深度学习和计算机视觉模型对赛车照片进行全面分析的创新应用。我们开发的模型在多项任务中展示了高效性，包括检测赛车、识别车牌号码、检测和量化车辆细节以及识别车辆方向等。我们讨论了为训练模型所需的鲁棒数据集的收集过程，并描述了一种我们设计的持续改进和增强数据集的方法。我们的方法利用反馈循环不断改进模型，从而提高RaceLens的性能和准确性。本研究的重要部分是介绍RaceLens的实际应用，重点是其在四个赛季中由NASCAR车队成功部署的案例。我们对系统的性能及其对车队战略决策和表现的直接影响进行了全面评估。

    This paper presents RaceLens, a novel application utilizing advanced deep learning and computer vision models for comprehensive analysis of racing photos. The developed models have demonstrated their efficiency in a wide array of tasks, including detecting racing cars, recognizing car numbers, detecting and quantifying car details, and recognizing car orientations. We discuss the process of collecting a robust dataset necessary for training our models, and describe an approach we have designed to augment and improve this dataset continually. Our method leverages a feedback loop for continuous model improvement, thus enhancing the performance and accuracy of RaceLens over time. A significant part of our study is dedicated to illustrating the practical application of RaceLens, focusing on its successful deployment by NASCAR teams over four seasons. We provide a comprehensive evaluation of our system's performance and its direct impact on the team's strategic decisions and performance met
    
[^18]: 解释文本片段之间的交互

    Explaining Interactions Between Text Spans. (arXiv:2310.13506v1 [cs.CL])

    [http://arxiv.org/abs/2310.13506](http://arxiv.org/abs/2310.13506)

    本论文介绍了SpanEx，这是一个关于自然语言理解任务的人类标记范围交互解释的多注释者数据集。研究发现现有的解释方法往往只关注相邻标记或元组之间的交互，缺乏捕捉人类决策过程中必要交互的注释。通过比较大型语言模型与人类的决策过程，作者发现它们之间存在差异。最后，作者提出了一种基于社区检测的元建模方法。

    

    对来自输入的不同部分的标记范围进行推理对于自然语言理解（NLU）任务非常重要，例如事实核查（FC）、机器阅读理解（MRC）或自然语言推理（NLI）。然而，现有的基于高亮的解释主要集中在识别个别重要标记或仅在相邻标记或标记元组之间的相互作用。值得注意的是，在这些任务中缺乏捕捉人类决策过程中必要交互的注释。为了填补这一差距，我们介绍了SpanEx，一个关于NLI和FC的人类标记范围交互解释的多注释者数据集。然后，我们研究了多个精调的大型语言模型在输入的不同部分之间使用的连接的决策过程，并将其与人类推理过程进行比较。最后，我们提出了一种基于社区检测的无监督元建模方法。

    Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important tokens or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process w.r.t. the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised met
    
[^19]: 具有强化改写生成的对话问答模型的鲁棒训练

    Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])

    [http://arxiv.org/abs/2310.13505](http://arxiv.org/abs/2310.13505)

    这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。

    

    知识图谱（KG）上的对话问答（ConvQA）模型通常在黄金QA对的基准上进行训练和测试。这意味着训练仅限于在相应数据集中见到的表面形式，评估仅针对一小部分问题。通过我们的提出的框架REIGN，我们采取了几个步骤来解决这个受限的学习设置。首先，我们系统地生成训练问题的改写，以提高模型对表面形式变化的鲁棒性。这是一个特别具有挑战性的问题，因为这些问题的不完整性。其次，我们使用深度强化学习将ConvQA模型引导到更高的性能，只提供那些有助于提高回答质量的改写。第三，我们展示了在一个基准上训练主要模型组件并将其零-shot应用于另一个的可行性。最后，为了对训练模型的鲁棒性进行严格评估，我们使用和重新配置初始的改写、测试语料。

    Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
    
[^20]: 模拟比例与创造力：初步研究

    Analogical Proportions and Creativity: A Preliminary Study. (arXiv:2310.13500v1 [cs.CL])

    [http://arxiv.org/abs/2310.13500](http://arxiv.org/abs/2310.13500)

    本研究初步探讨了模拟比例在创造力中的应用，通过实验表明使用词嵌入可以更好地基于模拟比例提出新的动物。

    

    模拟比例是形式为“$a$对$b$如同$c$对$d$”的陈述，表达了元素对$(a,b)$和对$(c,d)$的比较得出相似结果。模拟比例是创造性的，因为在给定3个不同的项目的情况下，可以根据一定条件计算出与之前的项目不同的第4个项目$d$的表示，使其与之形成模拟比例。在介绍模拟比例及其特性后，论文报告了使用动物描述和类别的数据库进行的实验结果，我们试图从现有动物中“创造”新的动物，如鸭嘴兽等罕见动物。我们进行了一系列的实验，使用词嵌入和布尔特征来基于模拟比例提出新的动物，结果表明词嵌入获得更好的结果。

    Analogical proportions are statements of the form "$a$ is to $b$ as $c$ is to $d$", which expresses that the comparisons of the elements in pair $(a, b)$ and in pair $(c, d)$ yield similar results. Analogical proportions are creative in the sense that given 3 distinct items, the representation of a 4th item $d$, distinct from the previous items, which forms an analogical proportion with them can be calculated, provided certain conditions are met. After providing an introduction to analogical proportions and their properties, the paper reports the results of an experiment made with a database of animal descriptions and their class, where we try to "create" new animals from existing ones, retrieving rare animals such as platypus. We perform a series of experiments using word embeddings as well as Boolean features in order to propose novel animals based on analogical proportions, showing that word embeddings obtain better results.
    
[^21]: 留意指令：对基于提示的学习中一致性和交互性的全面评估

    Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. (arXiv:2310.13486v1 [cs.CL])

    [http://arxiv.org/abs/2310.13486](http://arxiv.org/abs/2310.13486)

    这项研究对基于提示的学习中导致预测不稳定和不一致的设计选择进行了详细分析，并通过全面评估不同因素的影响，确定了对预测影响最大、具有交互性或稳定性的因素。

    

    在当前自然语言处理中，寻找将预训练语言模型适应到任务的最佳方式是一个重大挑战。与先前一代的任务调整模型（TT）类似，通过上下文学习（ICL）适应任务的模型在某些设置下是健壮的，但在其他设置下不是。在这里，我们对导致线性语言模型（LLM）预测不稳定和不一致的设计选择进行了详细分析。首先，我们展示了输入分布与标签之间的伪相关性在提示模型中只是一个次要问题，而对于TT模型来说是已知的问题。然后，我们进行了对提示设置中影响预测的不同因素的系统的全面评估。我们在不同规模的原始和指令调整（IT）LLMs上测试了所有可能的因素组合，并对结果进行统计分析，以展示哪些因素最具影响力、交互性或稳定性。我们的结果显示了哪些因素可以在不加预防的情况下使用，哪些因素需要预防措施。

    Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -- a known issue in TT models -- form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and whi
    
[^22]: 深度学习在畜牧动物行为识别中的应用：一项系统文献综述

    Application of deep learning for livestock behaviour recognition: A systematic literature review. (arXiv:2310.13483v1 [cs.CV])

    [http://arxiv.org/abs/2310.13483](http://arxiv.org/abs/2310.13483)

    本文通过系统文献综述研究了深度学习在畜牧动物行为识别中的应用，并发现了人工智能和计算机视觉技术在畜牧业中的潜在决策工具的重要作用。

    

    传统上，畜牧动物的健康和福利监测是一项人力密集型的任务。最近的进展导致了人工智能和计算机视觉技术的引入，尤其是深度学习模型，作为畜牧业的决策工具。这些模型已被用于动物识别、追踪、身体部位识别和物种分类等任务。过去十年中，人们对使用这些模型来探索畜牧动物行为与健康问题之间的关系越来越感兴趣。尽管先前的综述研究比较通用，但目前还没有一项专门研究畜牧动物行为识别的综述研究。因此，进行了这项系统文献综述(SLR)。SLR通过对电子数据库进行初步搜索，共得到1101篇出版物。经过定义的选择标准筛选后，筛选出126篇出版物。这些出版物基于质量和相关性进行进一步筛选，以最终获得最相关的研究文献。

    Livestock health and welfare monitoring has traditionally been a labor-intensive task performed manually. Recent advances have led to the adoption of AI and computer vision techniques, particularly deep learning models, as decision-making tools within the livestock industry. These models have been employed for tasks like animal identification, tracking, body part recognition, and species classification. In the past decade, there has been a growing interest in using these models to explore the connection between livestock behaviour and health issues. While previous review studies have been rather generic, there is currently no review study specifically focusing on DL for livestock behaviour recognition. Hence, this systematic literature review (SLR) was conducted. The SLR involved an initial search across electronic databases, resulting in 1101 publications. After applying defined selection criteria, 126 publications were shortlisted. These publications were further filtered based on qu
    
[^23]: 让语言模型清理您的有噪音的翻译数据

    Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])

    [http://arxiv.org/abs/2310.13469](http://arxiv.org/abs/2310.13469)

    论文介绍了如何利用大型语言模型清理神经机器翻译中的噪声输入，通过从MTNT数据集中清理目标语句的噪声，生成了C-MTNT数据集，显著减少了噪声。

    

    Transformer模型在神经机器翻译（NMT）中展现出了出色的性能。然而，它们对噪声输入的脆弱性在实际应用中提出了重大挑战，从噪声输入中生成干净的输出至关重要。MTNT数据集被广泛用作评估NMT模型对噪声输入鲁棒性的基准。然而，由于源语句和目标语句中都存在噪声，其实用性受到限制。为解决这一限制，我们专注于清理MTNT中目标语句的噪声，使其更适用于噪声评估的基准。利用大型语言模型（LLM）的能力，我们观察到它们在去噪方面的出色能力。例如，它们可以在考虑语义含义的同时删除表情符号。此外，我们还展示了LLM能够有效地更改俚语、术语和粗口。得到的数据集被称为C-MTNT，噪声显著减少。

    Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise 
    
[^24]: 多尺度超像素结构差异图卷积网络用于视觉语言表征

    Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])

    [http://arxiv.org/abs/2310.13447](http://arxiv.org/abs/2310.13447)

    本文提出了一种多尺度超像素结构差异图卷积网络（MDGCN）用于视觉语言表征，通过聚类感知相似像素，减少了后续处理的视觉基元数量，并挖掘了更精确的拓扑关系。

    

    在多模态领域中，整合视觉和语言的关键在于建立一个良好的对齐策略。最近，受到自监督学习成功的启发，基于预训练模型的视觉和语言的多模态语义表征取得了重大进展。然而，视觉语义表征仍有改进的空间。当前基于像素或块的方法在准确提取复杂场景边界方面存在空间语义连贯性不足和对噪声的脆弱性的挑战。为此，本文将超像素作为可学习图像数据的综合紧凑表征，通过对感知相似像素进行聚类，有效地减少了后续处理的视觉基元数量。为了挖掘更精确的拓扑关系，我们提出了一种多尺度差异图卷积网络（MDGCN）。它将整个图像解析为细到粗的层次结构，从而实现了整个图像的解析。

    Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
    
[^25]: 大型语言模型在模棱两可情况下的自一致性问题

    Self-Consistency of Large Language Models under Ambiguity. (arXiv:2310.13439v1 [cs.CL])

    [http://arxiv.org/abs/2310.13439](http://arxiv.org/abs/2310.13439)

    大型语言模型在模棱两可问题中的自一致性存在，并且不需要特别训练也能在稳健性检查中保持自一致性。

    

    在需要一致性的任务中，大型语言模型（LLMs）在不同上下文下给出不一致的答案是有问题的，例如问答、解释等。我们的研究提供了一个用于评估自一致性的基准，针对存在两个或多个正确答案的情况下进行评估。我们对OpenAI模型套件进行了一系列行为实验，使用了一个模棱两可的整数序列补全任务。我们发现平均一致性范围从67％到82％不等，远远高于一个模型的一致性如果是随机的话所能预测的水平，并且随着模型能力的提升而增加。此外，我们还展示了模型在一系列稳健性检查中都倾向于保持自一致性，包括提示说话者变化和序列长度变化。这些结果表明，自一致性是一种没有特别训练也能产生的新能力。尽管如此，我们发现模型在判断自身一致性时缺少校准。

    Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency, e.g., question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67\% to 82\%, far higher than would be predicted if a model's consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consiste
    
[^26]: 随机矩阵分析在低密度分离假设下平衡监督和无监督学习之间的应用

    Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption. (arXiv:2310.13434v1 [cs.LG])

    [http://arxiv.org/abs/2310.13434](http://arxiv.org/abs/2310.13434)

    本论文提出了一种理论框架，在低密度分离假设下分析高维情况下的半监督分类，并介绍了QLDS模型，该模型在平衡监督和无监督学习方法之间建立了一个平滑的桥梁。通过应用随机矩阵理论，我们推导出了在渐近情况下分类错误的理论评估，并提出了一种能够找到最佳平衡点的超参数选择策略。

    

    我们提出了一个理论框架，用于分析高维情况下低密度分离假设下的半监督分类。具体而言，我们引入了QLDS，一个线性分类模型，其中通过二次边界最大化来实现低密度分离假设。该算法具有明确的解和丰富的理论属性，并且我们证明了我们算法的特殊情况是监督情况下的最小二乘支持向量机，完全无监督情况下的谱聚类，以及一类半监督的基于图的方法。因此，QLDS在这些监督和无监督学习方法之间建立了一个平滑的桥梁。利用随机矩阵理论的最新进展，我们正式推导了在渐近情况下的分类误差的理论评估。作为一个应用，我们推导出了一种超参数选择策略，以找到最佳的监督和无监督之间的平衡。

    We propose a theoretical framework to analyze semi-supervised classification under the low density separation assumption in a high-dimensional regime. In particular, we introduce QLDS, a linear classification model, where the low density separation assumption is implemented via quadratic margin maximization. The algorithm has an explicit solution with rich theoretical properties, and we show that particular cases of our algorithm are the least-square support vector machine in the supervised case, the spectral clustering in the fully unsupervised regime, and a class of semi-supervised graph-based approaches. As such, QLDS establishes a smooth bridge between these supervised and unsupervised learning methods. Using recent advances in the random matrix theory, we formally derive a theoretical evaluation of the classification error in the asymptotic regime. As an application, we derive a hyperparameter selection policy that finds the best balance between the supervised and the unsupervised
    
[^27]: FLTracer: 高准确性的联邦学习中的毒化攻击来源追踪

    FLTracer: Accurate Poisoning Attack Provenance in Federated Learning. (arXiv:2310.13424v1 [cs.CR])

    [http://arxiv.org/abs/2310.13424](http://arxiv.org/abs/2310.13424)

    FLTracer是第一个FL攻击来源追踪框架，能够高准确性地检测各种攻击，并追踪攻击的时间、目标、类型和被毒化的位置。与现有方法不同，FLTracer利用本地信息和全局信息的统计分析来实现。

    

    联邦学习（FL）是一种有前途的分布式学习方法，可以使多个客户端协同训练共享的全局模型。然而，最近的研究表明，FL容易受到各种毒化攻击，这些攻击可以降低全局模型的性能或在其中引入后门。本文首先对先前的FL攻击和检测方法进行了全面的研究。结果表明，所有现有的检测方法只对有限和特定的攻击有效。大多数检测方法存在高误报率，特别是在非独立和同分布（non-IID）的设置中，导致性能严重降低。为了解决这些问题，我们提出了FLTracer，这是第一个FL攻击来源追踪框架，可以准确检测各种攻击并追踪攻击的时间、目标、类型和被毒化的位置。与现有的仅依赖于跨客户端异常检测的方法不同，我们的方法利用了每个客户端的本地信息和全局信息的统计分析。

    Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we
    
[^28]: POSQA：使用尺寸比较探索LLMs的世界模型

    POSQA: Probe the World Models of LLMs with Size Comparisons. (arXiv:2310.13394v1 [cs.CL])

    [http://arxiv.org/abs/2310.13394](http://arxiv.org/abs/2310.13394)

    POSQA是一个用来探索LLMs在具身理解方面的物体大小问答数据集，研究了LLMs在零-shot情景下的表现，通过推动技术和外部知识增强了它们的极限，同时分析了其现实世界理解的来源和提示的影响。

    

    具身化语言理解强调语言理解不仅仅是大脑中的心理处理问题，还涉及与物理和社会环境的互动。随着大型语言模型（LLMs）的爆炸性增长和它们已经普遍存在于我们的日常生活中，越来越有必要验证它们在现实世界中的理解能力。受认知理论的启发，我们提出了POSQA：一个带有简单尺寸比较问题的物体大小问答数据集，以检验最新LLMs的具身理解的极端性并分析其潜在机制。我们发现即使是今天的最大LLMs在零-shot情景下表现不佳，并通过先进的提示技术和外部知识增强推动了它们的极限。此外，我们研究了它们的现实世界理解主要是来自上下文信息还是内部权重，并分析了提示对其影响。

    Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs.  We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt for
    
[^29]: 使用分布式Hebbian Temporal Memory学习继任者表示法

    Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])

    [http://arxiv.org/abs/2310.13391](http://arxiv.org/abs/2310.13391)

    本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。

    

    本文提出了一种新颖的方法来解决在线隐藏表示学习的挑战，该方法用于在不稳定的、部分可观测的环境中进行决策。所提出的算法，分布式Hebbian Temporal Memory (DHTM)，基于因子图形式和多组成神经元模型。DHTM旨在捕捉顺序数据关系并对未来观察作出累积预测，形成继任者表示。受新皮层的神经生理学模型启发，该算法利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则克服了传统时间记忆算法（如RNN和HMM）的不稳定性和慢速学习过程。实验结果表明，DHTM优于经典的LSTM，并与更先进的类似RNN的算法性能相当，在变化的环境中加速了继任者表示的时间差异学习。此外，我们还进行了比较。

    This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
    
[^30]: 具有情感基础语言习得和差异结果训练的人机互动学习系统

    A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training. (arXiv:2310.13377v1 [cs.RO])

    [http://arxiv.org/abs/2310.13377](http://arxiv.org/abs/2310.13377)

    本文提出了一种人机互动学习系统，其中机器人和人类通过学习符号语言来识别机器人的内部稳态需求。研究采用了差异结果训练协议，证明其能够提高人类的学习效率，进而实现更高效的机器人语言习得。

    

    本文提出了一种新颖的人机互动设置，用于机器人和人类学习符号语言以识别机器人的内稳态需求。机器人和人类学习使用和响应传达内稳态需求和满足这些需求的刺激的相同语言符号。我们采用了差异结果训练（DOT）协议，机器人在满足正确刺激（如饼干）时提供特定于其内部需求（如“饥饿”）的反馈。我们发现DOT可以提高人类的学习效率，进而实现更高效的机器人语言习得。研究中使用的机器人具有与处于语言“咿呀学语”阶段的人类婴儿类似的词汇量。机器人的软件架构基于情感基础语言习得模型，通过与人类的交互将词汇与内部需求（饥饿、口渴、好奇）相关联。

    This paper presents a novel human-robot interaction setup for robot and human learning of symbolic language for identifying robot homeostatic needs. The robot and human learn to use and respond to the same language symbols that convey homeostatic needs and the stimuli that satisfy the homeostatic needs, respectively. We adopted a differential outcomes training (DOT) protocol whereby the robot provides feedback specific (differential) to its internal needs (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We found evidence that DOT can enhance the human's learning efficiency, which in turn enables more efficient robot language acquisition. The robot used in the study has a vocabulary similar to that of a human infant in the linguistic ``babbling'' phase. The robot software architecture is built upon a model for affect-grounded language acquisition where the robot associates vocabulary with internal needs (hunger, thirst, curiosity) through interactions with the human
    
[^31]: VFedMH: 垂直联合学习用于训练多参与方异构模型

    VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])

    [http://arxiv.org/abs/2310.13367](http://arxiv.org/abs/2310.13367)

    VFedMH是一种垂直联合学习方法，通过在前向传播过程中聚合参与者的嵌入来处理参与者之间的异构模型，解决了现有VFL方法面临的挑战。

    

    垂直联合学习（VFL）作为一种集成样本对齐和特征合并的新型训练范式，已经引起了越来越多的关注。然而，现有的VFL方法在处理参与者之间存在异构本地模型时面临挑战，这影响了优化收敛性和泛化能力。为了解决这个问题，本文提出了一种名为VFedMH的新方法，用于训练多方异构模型。VFedMH的重点是在前向传播期间聚合每个参与者知识的嵌入，而不是中间结果。主动方，拥有样本的标签和特征，在VFedMH中安全地聚合本地嵌入以获得全局知识嵌入，并将其发送给被动方。被动方仅拥有样本的特征，然后利用全局嵌入在其本地异构网络上进行前向传播。然而，被动方不拥有标签。

    Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
    
[^32]: 通过行为测试实现对机器翻译的通用错误诊断

    Towards General Error Diagnosis via Behavioral Testing in Machine Translation. (arXiv:2310.13362v1 [cs.CL])

    [http://arxiv.org/abs/2310.13362](http://arxiv.org/abs/2310.13362)

    本论文提出了一种基于双语翻译对生成的行为测试框架，用于诊断机器翻译系统的通用错误。该框架通过自动构建高质量的测试用例及其伪参考，能够在没有人工参考的情况下进行翻译质量评估。

    

    行为测试为诊断语言错误和评估自然语言处理模型的能力提供了重要手段。然而，将行为测试应用于机器翻译系统是具有挑战性的，因为通常需要人力来制作评估这些系统在新生成的测试用例上的翻译质量的参考。现有的机器翻译行为测试工作通过在没有参考的情况下评估翻译质量来绕开这个问题，但这限制了对特定类型错误的诊断，比如单个数字或货币词的错误翻译。为了诊断通用错误，本文提出了一种基于双语翻译对生成的行为测试（BTPGBT）框架来进行机器翻译行为测试。BTPGBT的核心思想是采用一种新颖的双语翻译对生成（BTPG）方法，自动构建高质量的测试用例及其伪参考。实验结果表明，该方法能够有效诊断机器翻译系统的通用错误。

    Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on var
    
[^33]: 填补合成图像与真实图像间的鸿沟以实现多模态机器翻译

    Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation. (arXiv:2310.13361v1 [cs.CV])

    [http://arxiv.org/abs/2310.13361](http://arxiv.org/abs/2310.13361)

    本文提出了一个方法来解决多模态机器翻译中合成图像与真实图像之间的分布差异问题，通过对模型输入图像表示和输出分布进行调整，来填补这一鸿沟。

    

    多模态机器翻译（MMT）同时将源句子和相关图像作为翻译的输入。由于大多数情况下输入句子没有配对的图像可用，最近的研究建议利用强大的文本到图像生成模型提供图像输入。然而，这些模型生成的合成图像往往与真实图像有不同的分布。因此，在训练中使用真实图像，而在推理中使用合成图像可能引入分布偏移，导致推理时性能下降。为了解决这个挑战，本文将合成图像和真实图像分别输入到MMT模型中。然后，通过接近Transformer编码器的输入图像表示和Transformer解码器的输出分布，最小化合成图像与真实图像之间的差距。因此，我们缓解了合成图像引入的分布差异。

    Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images
    
[^34]: NurViD: 用于护理操作活动理解的大规模专家级视频数据库

    NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding. (arXiv:2310.13347v1 [cs.CV])

    [http://arxiv.org/abs/2310.13347](http://arxiv.org/abs/2310.13347)

    NurViD是一个专为护理操作设计的大规模视频数据库，解决了现有数据集的规模较小、缺乏专家级注释和时间本地化注释等问题。

    

    将深度学习应用于护理操作活动理解可以极大地提高护士与患者互动的质量和安全性。通过利用这种技术，我们可以促进培训和教育，改善质量控制，并实现操作合规监控。然而，由于现有适当标记的数据集稀缺，该领域自动识别系统的发展受到了阻碍。现有的视频数据集存在几个限制：1）这些数据集在规模上较小，无法支持对护理活动的全面调查；2）它们主要关注单一操作，缺乏各种护理操作和操作步骤的专家级注释；3）它们缺乏时间上的本地化注释，这阻碍了对较长视频序列中目标动作的有效定位。为了缓解这些限制，我们提出了NurViD，一个具有专家级注释的大规模视频数据集，用于护理操作。

    The application of deep learning to nursing procedure activity understanding has the potential to greatly enhance the quality and safety of nurse-patient interactions. By utilizing the technique, we can facilitate training and education, improve quality control, and enable operational compliance monitoring. However, the development of automatic recognition systems in this field is currently hindered by the scarcity of appropriately labeled datasets. The existing video datasets pose several limitations: 1) these datasets are small-scale in size to support comprehensive investigations of nursing activity; 2) they primarily focus on single procedures, lacking expert-level annotations for various nursing procedures and action steps; and 3) they lack temporally localized annotations, which prevents the effective localization of targeted actions within longer video sequences. To mitigate these limitations, we propose NurViD, a large video dataset with expert-level annotation for nursing proc
    
[^35]: 大型语言模型（LLMs）的利用中的挑战和因素

    Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs). (arXiv:2310.13343v1 [cs.CL])

    [http://arxiv.org/abs/2310.13343](http://arxiv.org/abs/2310.13343)

    大型语言模型（LLMs）在广泛应用中面临领域特异性、知识遗忘、知识复制、知识错觉和知识有毒等挑战。解决这些问题的建议包括多样化训练数据、微调模型，增强透明度和可解释性。

    

    随着GPT系列等大型语言模型（LLMs）的发展，它们在各种应用场景中的广泛使用带来了一系列挑战。本综述首先探讨了领域特异性的问题，LLMs可能难以对专业领域的特定问题提供精确答案。知识遗忘的问题是这些LLMs可能难以平衡新旧信息的重要性。知识复制现象揭示了LLMs有时可能提供过度机械化的回答，缺乏深度和独创性。此外，知识错觉描述了LLMs可能提供看似深刻但实际上肤浅的答案的情况，而知识有毒则侧重于有害或有偏见的信息输出。这些挑战凸显了LLMs的训练数据和算法设计的问题。为了解决这些问题，建议多样化训练数据，微调模型，增强透明度和可解释性。

    With the development of large language models (LLMs) like the GPT series, their widespread use across various application scenarios presents a myriad of challenges. This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes situations where LLMs might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. These challenges underscore problems in the training data and algorithmic design of LLMs. To address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretab
    
[^36]: FLAIR:一种基于多源光学影像的全国范围土地覆盖语义分割数据集

    FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery. (arXiv:2310.13336v1 [cs.CV])

    [http://arxiv.org/abs/2310.13336](http://arxiv.org/abs/2310.13336)

    FLAIR是一个全国范围的土地覆盖语义分割数据集，包含大量高分辨率航拍图像和数十亿个个体标记像素。FLAIR整合了不同空间、光谱和时间分辨率的数据，对于大规模土地覆盖分析的方法开发和评估具有重要价值。

    

    我们介绍了法国国家地理和森林信息研究所（IGN）的法国航空航天影像（FLAIR）数据集，这是一个广泛的数据集，为大规模地理空间分析提供了独特而丰富的资源。FLAIR包含高分辨率航拍图像，地面样本间距为20厘米，超过200亿个个体标记像素用于精确的土地覆盖分类。该数据集还整合了光学卫星时间序列的时间和光谱数据。因此，FLAIR将具有不同空间、光谱和时间分辨率的数据整合在法国全境817平方公里的采集中，代表了法国丰富的景观多样性。这种多样性使FLAIR成为大规模土地覆盖语义分割方法的开发和评估的宝贵资源，并在计算机视觉、数据融合和地理空间分析方面带来了重大挑战。我们还提供了强大的单传感器和多传感器基线模型，可以用来

    We introduce the French Land cover from Aerospace ImageRy (FLAIR), an extensive dataset from the French National Institute of Geographical and Forest Information (IGN) that provides a unique and rich resource for large-scale geospatial analysis. FLAIR contains high-resolution aerial imagery with a ground sample distance of 20 cm and over 20 billion individually labeled pixels for precise land-cover classification. The dataset also integrates temporal and spectral data from optical satellite time series. FLAIR thus combines data with varying spatial, spectral, and temporal resolutions across over 817 km2 of acquisitions representing the full landscape diversity of France. This diversity makes FLAIR a valuable resource for the development and evaluation of novel methods for large-scale land-cover semantic segmentation and raises significant challenges in terms of computer vision, data fusion, and geospatial analysis. We also provide powerful uni- and multi-sensor baseline models that can
    
[^37]: 民主化推理能力：从大型语言模型中定制学习

    Democratizing Reasoning Ability: Tailored Learning from Large Language Model. (arXiv:2310.13332v1 [cs.CL])

    [http://arxiv.org/abs/2310.13332](http://arxiv.org/abs/2310.13332)

    本文提出了一种定制学习方法，通过利用大型语言模型作为推理教师，并建立交互式多轮学习范式，将推理能力提取到较小的语言模型中，以促进其民主化。

    

    大型语言模型展示了令人印象深刻的自然语言处理能力，但由于巨大的计算需求和封闭源代码的特性，其民主化受到了阻碍。最近对开源较小的语言模型进行知识提取的研究在指令遵循能力方面取得了有希望的结果，但对于更具挑战性的推理能力的研究相对较少。在本文中，我们提出了一种定制学习方法，将这种推理能力提取到较小的语言模型中，以促进专属推理能力的民主化。与仅将语言模型作为数据注释器不同，我们利用语言模型作为推理教师的潜力，构建了一个交互多轮学习范式。这个范式使得学生能够将自己的不足暴露给黑箱教师，然后教师可以提供定制的训练数据作为回报。此外，为了利用推理潜力，我们设计了一系列特定推理任务，通过与黑箱教师的交互来定制学生的训练数据。

    Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of t
    
[^38]: 粗细双编码器是更好的帧识别学习器

    Coarse-to-Fine Dual Encoders are Better Frame Identification Learners. (arXiv:2310.13316v1 [cs.CL])

    [http://arxiv.org/abs/2310.13316](http://arxiv.org/abs/2310.13316)

    CoFFTEA是一种粗细框架和目标编码器体系结构，通过对比学习和双编码器来有效建模框架和目标之间的对齐，并通过粗到细的课程学习过程逐渐学会区分具有不同程度的框架。

    

    帧识别旨在找到与目标词在句子中相关的语义框架。最近的研究通过建模框架定义来衡量目标和候选框架之间的相似性或匹配分数。然而，他们要么缺乏对定义的充分表示学习，要么面临在超过1000个候选框架中有效选择最合适的框架的挑战。此外，常用的词典过滤方法（lf）获取目标的候选框架时可能会忽略词汇表外的目标，并导致框架建模不充分。在本文中，我们提出了CoFFTEA，即粗细框架和目标编码器体系结构。通过对比学习和双编码器，CoFFTEA有效地建模了框架和目标之间的对齐。通过采用粗到细的课程学习过程，CoFFTEA逐渐学会区分具有不同程度的框架。

    Frame identification aims to find semantic frames associated with target words in a sentence. Recent researches measure the similarity or matching score between targets and candidate frames by modeling frame definitions. However, they either lack sufficient representation learning of the definitions or face challenges in efficiently selecting the most suitable frame from over 1000 candidate frames. Moreover, commonly used lexicon filtering ($lf$) to obtain candidate frames for the target may ignore out-of-vocabulary targets and cause inadequate frame modeling. In this paper, we propose CoFFTEA, a $\underline{Co}$arse-to-$\underline{F}$ine $\underline{F}$rame and $\underline{T}$arget $\underline{E}$ncoders $\underline{A}$rchitecture. With contrastive learning and dual encoders, CoFFTEA efficiently and effectively models the alignment between frames and targets. By employing a coarse-to-fine curriculum learning procedure, CoFFTEA gradually learns to differentiate frames with varying degr
    
[^39]: 解码沉默的大多数：利用大型语言模型诱导信念增强的社交图进行响应预测

    Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting. (arXiv:2310.13297v1 [cs.CL])

    [http://arxiv.org/abs/2310.13297](http://arxiv.org/abs/2310.13297)

    通过利用大型语言模型，我们提出了一个名为SocialSense的框架，通过诱导信念增强的图和基于图的传播来预测新闻发布的响应。这个方法在没有用户明确个人资料或历史行为的情况下，能够有效地捕捉社交动态。

    

    新闻媒体的自动响应预测在帮助内容生产者有效预测新闻发布的影响并防止意外的负面结果（如社会冲突和道德伤害）方面起着关键作用。为了有效地预测响应，必须开发能够利用个体周围的社交动态和背景信息的措施，尤其是在用户明确的个人资料或历史行为有限的情况下（即潜在参与者）。正如先前的研究所示，97%的推文仅由最活跃的25%用户产生。然而，现有的方法对于如何处理和利用这些重要特征的最佳方式进行了有限的探索。为了弥补这一差距，我们提出了一个名为SocialSense的新型框架，利用大型语言模型在现有社交网络之上诱导出一个以信念为中心的图，以及基于图的传播来捕捉社交动态。我们假设诱导出的图可以...

    Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph 
    
[^40]: PathRL：一种基于深度强化学习的端到端路径生成方法，用于碰撞避免

    PathRL: An End-to-End Path Generation Method for Collision Avoidance via Deep Reinforcement Learning. (arXiv:2310.13295v1 [cs.RO])

    [http://arxiv.org/abs/2310.13295](http://arxiv.org/abs/2310.13295)

    这项研究提出了PathRL，一种使用深度强化学习生成导航路径的端到端方法。传统的DRL方法通过训练低级控制指令来直接指导机器人行动，但这导致速度不稳定和轨迹不平滑。相比之下，PathRL通过训练直接输出路径的DRL策略，克服了高维动作空间和多时间步骤跟踪路径的难题。通过这种方法，PathRL能够生成稳定且平滑的导航路径，用于避免碰撞和提高移动机器人性能。

    

    深度强化学习（DRL）在改善移动机器人性能方面显示出巨大潜力。然而，大多数现有的基于DRL的导航方法主要集中在训练一个直接指导机器人的策略，如线性和角速度，这导致机器人在长期执行过程中的速度不稳定和轨迹不平滑。另一种方法是训练一个直接输出导航路径的DRL策略。然而，训练输出路径的DRL策略面临两个难题：（1）潜在路径的动作空间通常涉及比低级指令更高的维度，增加了训练的难度；（2）跟踪路径需要多个时间步骤，而不是单个时间步骤，这要求路径在多个时间步骤内预测机器人与动态环境的相互作用。这反过来增加了与路径跟踪相关的挑战。

    Robot navigation using deep reinforcement learning (DRL) has shown great potential in improving the performance of mobile robots. Nevertheless, most existing DRL-based navigation methods primarily focus on training a policy that directly commands the robot with low-level controls, like linear and angular velocities, which leads to unstable speeds and unsmooth trajectories of the robot during the long-term execution. An alternative method is to train a DRL policy that outputs the navigation path directly. However, two roadblocks arise for training a DRL policy that outputs paths: (1) The action space for potential paths often involves higher dimensions comparing to low-level commands, which increases the difficulties of training; (2) It takes multiple time steps to track a path instead of a single time step, which requires the path to predicate the interactions of the robot w.r.t. the dynamic environment in multiple time steps. This, in turn, amplifies the challenges associated with tra
    
[^41]: 评估语言模型中的隐私风险：以摘要任务为案例研究

    Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks. (arXiv:2310.13291v1 [cs.CL])

    [http://arxiv.org/abs/2310.13291](http://arxiv.org/abs/2310.13291)

    本研究评估了语言模型在摘要任务中的隐私风险，发现摘要模型存在泄露数据成员身份的风险，并讨论了一些保护措施和隐私与效用之间的权衡。

    

    大型语言模型通过在各种任务上实现最先进的性能，改变了自然语言处理领域。然而，人们担心这些模型可能泄露训练数据中的信息。本研究集中在摘要任务上，调查成员推断（MI）攻击：通过给出一个样本和对模型API的黑盒访问，可以确定样本是否是训练数据的一部分。我们利用文本相似性和模型对文档修改的抵抗力作为潜在的MI信号，并评估它们在广泛使用的数据集上的有效性。我们的结果表明，摘要模型存在泄露数据成员身份的风险，甚至在没有参考摘要可用的情况下也是如此。此外，我们讨论了训练摘要模型以防止MI攻击的几种保护措施，并讨论了隐私和效用之间固有的权衡。

    Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model's resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.
    
[^42]: 通过任务超图实现推荐的统一预训练

    Unified Pretraining for Recommendation via Task Hypergraphs. (arXiv:2310.13286v1 [cs.IR])

    [http://arxiv.org/abs/2310.13286](http://arxiv.org/abs/2310.13286)

    本文提出了一种名为"通过任务超图的统一预训练推荐"的多任务预训练框架，使用任务超图将先前任务推广为超边预测，并通过过渡性注意力层学习每个先前任务与推荐之间的相关性。实验表明，这种方法在准确性和效率方面优于其他基准方法。

    

    尽管预训练在最近几年引起了广泛关注和流行，但它在基于图的推荐系统中的应用相对有限。在广泛使用的ID依赖数据集中利用预训练的先前知识是具有挑战性的。一方面，一个数据集中的用户-物品交互历史很难通过预训练转移到其他数据集中，因为ID不同。另一方面，在同一个数据集上进行预训练和微调会导致过拟合的高风险。在本文中，我们提出了一种名为通过任务超图的统一推荐预训练的新型多任务预训练框架。为了处理各种先前任务的不同要求和细微差别，我们设计了任务超图将先前任务推广为超边预测。我们设计了一种新的过渡性注意力层来有差异地学习每个先前任务与推荐之间的相关性。在三个基准数据集上进行的实验结果表明，在准确性和效率方面，我们的方法优于其他基准方法。

    Although pretraining has garnered significant attention and popularity in recent years, its application in graph-based recommender systems is relatively limited. It is challenging to exploit prior knowledge by pretraining in widely used ID-dependent datasets. On one hand, user-item interaction history in one dataset can hardly be transferred to other datasets through pretraining, where IDs are different. On the other hand, pretraining and finetuning on the same dataset leads to a high risk of overfitting. In this paper, we propose a novel multitask pretraining framework named Unified Pretraining for Recommendation via Task Hypergraphs. For a unified learning pattern to handle diverse requirements and nuances of various pretext tasks, we design task hypergraphs to generalize pretext tasks to hyperedge prediction. A novel transitional attention layer is devised to discriminatively learn the relevance between each pretext task and recommendation. Experimental results on three benchmark da
    
[^43]: 用于学习排序中特征选择的模拟退火探索性研究

    An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank. (arXiv:2310.13269v1 [cs.LG])

    [http://arxiv.org/abs/2310.13269](http://arxiv.org/abs/2310.13269)

    该研究探讨了在学习排序中使用模拟退火进行特征选择的方法，并引入了进展参数来有效遍历搜索空间。实验证明了该方法的有效性。

    

    学习排序是一种应用于监督式机器学习的领域。由于特征选择已被发现可以有效提高学习模型的准确性，因此研究将该过程应用于学习排序领域是非常有趣的。在本研究中，我们调查了一种称为模拟退火的流行元启发式方法在该任务中的应用。在模拟退火的一般框架下，我们探索了各种邻域选择策略和温度冷却方案。我们进一步引入了一个新的超参数，称为进展参数，可以有效地用于遍历搜索空间。我们的算法在五个公开基准数据集上进行了评估。为了更好地验证，我们还将基于模拟退火的特征选择算法与另一种有效的元启发式算法，即局部波束搜索进行了比较。广泛的实验结果表明了我们提出的模型的有效性。

    Learning-to-rank is an applied domain of supervised machine learning. As feature selection has been found to be effective for improving the accuracy of learning models in general, it is intriguing to investigate this process for learning-to-rank domain. In this study, we investigate the use of a popular meta-heuristic approach called simulated annealing for this task. Under the general framework of simulated annealing, we explore various neighborhood selection strategies and temperature cooling schemes. We further introduce a new hyper-parameter called the progress parameter that can effectively be used to traverse the search space. Our algorithms are evaluated on five publicly benchmark datasets of learning-to-rank. For a better validation, we also compare the simulated annealing-based feature selection algorithm with another effective meta-heuristic algorithm, namely local beam search. Extensive experimental results shows the efficacy of our proposed models.
    
[^44]: ManiCast: 基于成本感知人体预测的协同操作

    ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v1 [cs.RO])

    [http://arxiv.org/abs/2310.13258](http://arxiv.org/abs/2310.13258)

    ManiCast是一个基于成本感知的人体预测的协同操作框架，通过提供能够捕捉未来人体运动如何影响机器人计划成本的预测，实现了人机协同操纵任务的流畅执行和实时交互。

    

    紧密合作的人机操纵依赖准确的人体运动预测。尽管在大规模学习预测模型方面取得了显著进展，但当应用于操作任务时，这些模型在关键转折点处积累了较高的误差，导致下游规划性能的降低。我们的关键见解是，与其预测最有可能的人体运动，产生能够捕捉未来人体运动如何影响机器人计划成本的预测就足够了。我们提出了ManiCast，一种新颖的框架，学习成本感知的人体预测并将其提供给模型预测控制规划器以执行协同操作任务。我们的框架实现了人类和7个自由度的机械臂在如反应搅拌、物体交接和协同摆桌等多个真实任务中的流畅、实时交互。我们对运动预测和端到端的预测-规划系统进行了评估

    Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a
    
[^45]: 在低数据环境中，视觉定位有助于学习单词的含义

    Visual Grounding Helps Learn Word Meanings in Low-Data Regimes. (arXiv:2310.13257v1 [cs.CL])

    [http://arxiv.org/abs/2310.13257](http://arxiv.org/abs/2310.13257)

    在低数据环境中，使用视觉定位进行监督训练的神经语言模型可以更接近于人类的语言学习能力。

    

    现代神经语言模型（LM）是用于模拟人类句子产生和理解的强大工具，其内部表达与人类大脑中的语言表达非常吻合。然而，为了取得这些结果，LM必须以与人类完全不同的方式进行训练，需要比儿童在发育过程中接收到的语言数据多几个数量级，并且没有任何感知、行动或社交行为的基础。如果用更接近人类的方式进行训练，即依靠感知的监督，模型的语言学习是否更接近人类？我们在单词学习这一语言习得的关键子任务中研究了这个问题。我们训练了一系列不同的LM架构，并在不同规模的数据集上使用图像字幕任务的辅助监督进行训练。然后我们使用一系列广泛的测试来评估这些模型在句法类别、词汇关系、语义学等方面的学习能力。

    Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways -- requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically -- with grounded supervision -- exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models' learning of syntactic categories, lexical relations, semantic f
    
[^46]: TempGNN: 用于动态会话推荐的时间图神经网络

    TempGNN: Temporal Graph Neural Networks for Dynamic Session-Based Recommendations. (arXiv:2310.13249v1 [cs.IR])

    [http://arxiv.org/abs/2310.13249](http://arxiv.org/abs/2310.13249)

    TempGNN是一种用于动态会话推荐的时间图神经网络，可以捕捉复杂物品转换的结构和时间动态，通过对动态会话图中的节点和边进行时间嵌入操作，有效地预测下一步动作。

    

    最近，基于会话的推荐系统通过理解用户在短期会话中与物品的交互行为来预测下一步动作的方法受到了广泛关注。以前的研究主要集中在通过循环神经网络、自注意模型和最近的大多数图神经网络来捕捉会话中复杂物品转换的顺序依赖性。尽管有很多根据会话中物品顺序的不同模型，但很少有方法来更好地处理交互之间的时间意义。我们提出了一种名为Temporal Graph Neural Networks (TempGNN)的通用框架，利用节点和边上的时间嵌入操作来捕捉复杂物品转换的结构和时间动态，将其表示为定时事件序列的动态会话图。广泛的实验结果显示了所提方法的有效性和适应性。

    Session-based recommendations which predict the next action by understanding a user's interaction behavior with items within a relatively short ongoing session have recently gained increasing popularity. Previous research has focused on capturing the dynamics of sequential dependencies from complicated item transitions in a session by means of recurrent neural networks, self-attention models, and recently, mostly graph neural networks. Despite the plethora of different models relying on the order of items in a session, few approaches have been proposed for dealing better with the temporal implications between interactions. We present Temporal Graph Neural Networks (TempGNN), a generic framework for capturing the structural and temporal dynamics in complex item transitions utilizing temporal embedding operators on nodes and edges on dynamic session graphs, represented as sequences of timed events. Extensive experimental results show the effectiveness and adaptability of the proposed met
    
[^47]: FLEE-GNN：一种用于分析多种商品食品流的地理空间韧性的边缘增强图神经网络的联邦学习系统

    FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows. (arXiv:2310.13248v1 [cs.LG])

    [http://arxiv.org/abs/2310.13248](http://arxiv.org/abs/2310.13248)

    本文提出了FLEE-GNN，它是一种用于分析多种商品食品流的地理空间韧性的边缘增强图神经网络的联邦学习系统。FLEE-GNN克服了当前方法的局限性，提高了分析食品供应网络韧性的普适性、可扩展性和数据隐私保护。

    

    理解和衡量食品供应网络的韧性是解决日益严重的食品不安全问题的全球任务。然而，这些网络的复杂性以及它们的多维交互和决策提出了重大挑战。本文提出了一种名为FLEE-GNN的新型联邦学习系统，用于边缘增强的图神经网络，旨在克服这些挑战，增强多种商品食品流网络的地理空间韧性分析，它是空间网络的一种类型。FLEE-GNN解决了当前方法的局限性，例如基于熵的方法在普适性，可扩展性和数据隐私方面的限制。它将图神经网络的鲁棒性和适应性与联邦学习的数据隐私意识和分散特性相结合，用于跨地理区域的食品供应网络韧性分析。本文还讨论了FLEE-GNN的创新数据生成技术，实验设计和f

    Understanding and measuring the resilience of food supply networks is a global imperative to tackle increasing food insecurity. However, the complexity of these networks, with their multidimensional interactions and decisions, presents significant challenges. This paper proposes FLEE-GNN, a novel Federated Learning System for Edge-Enhanced Graph Neural Network, designed to overcome these challenges and enhance the analysis of geospatial resilience of multicommodity food flow network, which is one type of spatial networks. FLEE-GNN addresses the limitations of current methodologies, such as entropy-based methods, in terms of generalizability, scalability, and data privacy. It combines the robustness and adaptability of graph neural networks with the privacy-conscious and decentralized aspects of federated learning on food supply network resilience analysis across geographical regions. This paper also discusses FLEE-GNN's innovative data generation techniques, experimental designs, and f
    
[^48]: 多层对比学习用于基于剧本的角色理解

    Multi-level Contrastive Learning for Script-based Character Understanding. (arXiv:2310.13231v1 [cs.CL])

    [http://arxiv.org/abs/2310.13231](http://arxiv.org/abs/2310.13231)

    本文提出了一种多层对比学习框架用于剧本中角色的理解，从角色的话语中学习其个性和身份，并在多个角色理解子任务上通过与强大的预训练语言模型的比较进行了验证。

    

    本文针对剧本中的角色理解场景，旨在从他们的话语中学习角色的个性和身份。我们首先分析了这一场景中的几个挑战，然后提出了一种多层对比学习框架，以细粒度的方式捕捉角色的全局信息。为了验证所提出的框架，我们与强大的预训练语言模型（包括SpanBERT，Longformer，BigBird和ChatGPT-3.5）进行了三个角色理解子任务的广泛实验比较。实验结果表明，我们的方法在性能上取得了显著改进。通过深入分析，我们展示了我们的方法在解决挑战和提供更多关于角色理解场景的线索方面的有效性。我们将在github上开源我们的工作，网址为https://github.com/David-Li0406/Script-based-Character-Understanding。

    In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters' personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters' global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work on github at https://github.com/David-Li0406/Script-based-Character-Understanding.
    
[^49]: 绝对策略优化

    Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])

    [http://arxiv.org/abs/2310.13230](http://arxiv.org/abs/2310.13230)

    这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。

    

    近年来，基于信任域的在线策略强化学习在解决复杂控制任务和游戏场景方面取得了令人瞩目的结果。然而，这一类别中现有的最先进算法主要强调对预期性能的改进，缺乏对最坏情况下性能结果的控制能力。为了解决这个限制，我们引入了一个新颖的目标函数；通过优化该函数，可以确保近乎总体性能样本的下界（绝对性能）呈现单调改进。考虑到这一具有突破性的理论进展，我们通过一系列的近似对这个理论基础算法进行了改进，得到了一种实用的解决方案称为绝对策略优化（APO）。我们的实验证明了我们的方法在具有挑战性的连续控制基准任务上的有效性，并将其适用性扩展到掌握Atari游戏。我们的发现表明，APO在提高性能的同时也显著改善了最坏情况下的性能结果。

    In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
    
[^50]: ToolChain*: 使用A*搜索在大型语言模型中进行高效的动作空间导航

    ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search. (arXiv:2310.13227v1 [cs.CL])

    [http://arxiv.org/abs/2310.13227](http://arxiv.org/abs/2310.13227)

    ToolChain*是一种提供了高效动作空间导航的树搜索算法，它可以在大型语言模型中进行决策和规划，解决复杂的真实世界问题。

    

    大型语言模型（LLM）在解决复杂的真实世界问题时展现了强大的决策和规划能力。基于LLM的自主代理可以与各种工具（例如功能性API）进行交互，并生成执行一系列API函数调用的解决方案计划。候选API函数调用的多样性显著扩展了动作空间，加大了对高效动作空间导航的关键需求。然而，现有方法要么在庞大的动作空间中面临单向探索困难，陷入局部优化解，要么遭受穷尽地遍历所有潜在动作所导致的低效导航。为了解决这些问题，我们提出了ToolChain*，一种基于树搜索的LLM代理规划算法。它将整个动作空间构建为一个决策树，其中每个节点表示解决方案计划中涉及的可能的API函数调用。

    Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A
    
[^51]: 可扩展的神经网络内核

    Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])

    [http://arxiv.org/abs/2310.13225](http://arxiv.org/abs/2310.13225)

    可扩展神经网络内核（SNNKs）是一种替代常规前馈层的方法，能够近似实现常规前馈层的功能，但具有更优的计算特性。通过将内核与参数-输入向量的点积联系起来，SNNKs能够有效地解开参数与输入之间的联系，从而模拟复杂关系。此外，我们还引入了神经网络捆绑过程，将SNNKs应用于深度神经网络压缩，进一步提高了压缩效果。最终捆绑网络甚至可以绕过反向传播，通过显式公式求解最优参数。

    

    我们引入了可扩展神经网络内核（SNNKs）的概念，这是常规前馈层（FFLs）的替代品，能够近似实现后者，但具有有利的计算属性。SNNKs有效地解开了FFL中参数与输入之间的联系，并通过点积内核在最终计算中连接它们。它们也更加表达力强，能够模拟复杂关系，超出参数-输入向量的函数范围。我们还引入了神经网络捆绑过程，将SNNKs应用于压缩深度神经网络结构，从而获得额外的压缩效益。在极端情况下，它导致完全捆绑网络，其最优参数可以通过多个损失函数（例如均方误差）的显式公式来表示，从而有可能绕过反向传播。作为我们分析的副产品，我们引入了普遍性机制的机制。

    We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universa
    
[^52]: HierCas: 信息级联中的分层时序图注意力网络用于流行度预测

    HierCas: Hierarchical Temporal Graph Attention Networks for Popularity Prediction in Information Cascades. (arXiv:2310.13219v1 [cs.SI])

    [http://arxiv.org/abs/2310.13219](http://arxiv.org/abs/2310.13219)

    本论文提出了一种名为HierCas的新的框架，用于信息级联流行度预测。与传统方法不同，HierCas采用动态图建模方法，在整个级联图上进行操作，能够捕捉到连续动态信息的完整范围，并明确建模结构和时间因素之间的相互作用。

    

    信息级联流行度预测对于识别假新闻和精准推荐等许多应用非常重要。传统基于特征的方法严重依赖手工特征，这些特征是特定领域的，缺乏对新领域的泛化能力。为了解决这个问题，研究人员转向了基于神经网络的方法。然而，现有的方法采用采样建模方法，可能丢失信息扩散过程中出现的连续动态信息和结构-时间依赖关系。在本文中，我们提出了一种名为HierCas的新框架，用于级联流行度预测，它采用动态图建模方法，在整个级联图上进行操作，能够捕捉到连续动态信息的完整范围，并明确建模结构和时间因素之间的相互作用。

    Information cascade popularity prediction is critical for many applications, including but not limited to identifying fake news and accurate recommendations. Traditional feature-based methods heavily rely on handcrafted features, which are domain-specific and lack generalizability to new domains. To address this problem, researchers have turned to neural network-based approaches. However, existing methods follow a sampling-based modeling approach, potentially losing continuous dynamic information and structural-temporal dependencies that emerge during the information diffusion process. In this paper, we propose a novel framework called Hierarchical Temporal Graph Attention Networks for cascade popularity prediction (HierCas). Unlike existing methods, HierCas operates on the entire cascade graph by a dynamic graph modeling approach, enabling it to capture the full range of continuous dynamic information and explicitly model the interplay between structural and temporal factors. By lever
    
[^53]: MultiCoNER v2: 一个用于细粒度和含噪音命名实体识别的大型多语言数据集

    MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition. (arXiv:2310.13213v1 [cs.CL])

    [http://arxiv.org/abs/2310.13213](http://arxiv.org/abs/2310.13213)

    MultiCoNER v2是一个大型多语言数据集，用于细粒度和含噪音的命名实体识别。它解决了细粒度类别处理和噪音导致的性能下降的实际挑战。评估结果表明，细粒度分类具有挑战性，而实体损坏对性能影响更大。

    

    我们提出了MULTICONER V2，这是一个细粒度命名实体识别的数据集，涵盖了12种语言中的33个实体类别，包括单语和多语境的设置。该数据集旨在解决NER中的以下实际挑战：(i) 有效处理包括电影标题等复杂实体的细粒度类别，以及(ii) 由于打字错误或OCR错误而产生的噪音导致性能下降。该数据集是从维基百科和维基数据等公开资源中编译而成，并可公开获取。基于XLM-RoBERTa基准的评估突出了MULTICONER V2所面临的独特挑战：(i)细粒度分类是具有挑战性的，在所有语言中的宏F1得分较低，为0.63；以及(ii)损坏策略显著影响性能，实体损坏导致的性能比非实体损坏低9%。这突显了与上下文相关的实体噪音相比于其他噪音对性能的更大影响。

    We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition covering 33 entity classes across 12 languages, in both monolingual and multilingual settings. This dataset aims to tackle the following practical challenges in NER: (i) effective handling of fine-grained classes that include complex entities like movie titles, and (ii) performance degradation due to noise generated from typing mistakes or OCR errors. The dataset is compiled from open resources like Wikipedia and Wikidata, and is publicly available. Evaluation based on the XLM-RoBERTa baseline highlights the unique challenges posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the scores are low with macro-F1=0.63 (across all languages), and (ii) the corruption strategy significantly impairs performance, with entity corruption resulting in 9% lower performance relative to non-entity corruptions across all languages. This highlights the greater impact of entity noise in contrast to cont
    
[^54]: ChatGPT的初印象效应

    Primacy Effect of ChatGPT. (arXiv:2310.13206v1 [cs.CL])

    [http://arxiv.org/abs/2310.13206](http://arxiv.org/abs/2310.13206)

    ChatGPT在选择答案时表现出初印象效应，即更倾向于选择提示中较早位置的标签作为答案。

    

    通过在ChatGPT上进行实验和分析，我们研究了ChatGPT的初印象效应，即选择较早位置的标签作为答案的倾向。我们发现：i）ChatGPT的决策对提示中标签的顺序敏感；ii）ChatGPT更倾向于选择较早位置的标签作为答案。我们希望我们的实验和分析能为构建更可靠的基于ChatGPT的解决方案提供额外的见解。

    Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the
    
[^55]: 人工智能的不透明法律

    The opaque law of artificial intelligence. (arXiv:2310.13192v1 [cs.AI])

    [http://arxiv.org/abs/2310.13192](http://arxiv.org/abs/2310.13192)

    本文分析了算法的不透明性，重点关注人工智能在因果责任领域中的应用。通过对目前最好的生成式人工智能模型（Chat-GPT）的评估，可以了解其目前的性能以及可能的法律规制形式。

    

    本文旨在分析算法的不透明性，并将其置于人工智能因果责任的公开辩论背景下进行讨论；通过应用图灵测试中提出的对话方法，我们希望评估现有最好的生成式人工智能模型（Chat-GPT）的性能，以确定其目前的能力和可能的法律规制形式。问题分析将基于对传统法律范畴（如因果关系、意图和过失）的评论，以理解人工智能使用中的问题，特别关注人机交互。从计算机科学角度来看，文中还将提出一种针对Chat-GPT进行实际询问的方法，以找到人工智能运行过程中的一些关键问题。文章的结尾将集中讨论一些现有的立法措施。

    The purpose of this paper is to analyse the opacity of algorithms, contextualized in the open debate on responsibility for artificial intelligence causation; with an experimental approach by which, applying the proposed conversational methodology of the Turing Test, we expect to evaluate the performance of one of the best existing NLP model of generative AI (Chat-GPT) to see how far it can go right now and how the shape of a legal regulation of it could be. The analysis of the problem will be supported by a comment of Italian classical law categories such as causality, intent and fault to understand the problem of the usage of AI, focusing in particular on the human-machine interaction. On the computer science side, for a technical point of view of the logic used to craft these algorithms, in the second chapter will be proposed a practical interrogation of Chat-GPT aimed at finding some critical points of the functioning of AI. The end of the paper will concentrate on some existing leg
    
[^56]: 朝着鲁棒剪枝：一种面向语言模型的自适应知识保留剪枝策略

    Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])

    [http://arxiv.org/abs/2310.13191](http://arxiv.org/abs/2310.13191)

    本文提出了一种适应性知识保留剪枝策略，旨在提高语言模型对抗攻击的鲁棒性，并在剪枝过程中保留更多的预训练知识。与其他方法相比，该方法展现了更好的平衡。

    

    剪枝目标近期不仅仅局限于准确性和稀疏性，还包括对语言模型鲁棒性的提升。然而，现有方法在持续增加模型稀疏性时，对抗攻击的鲁棒性提升方面存在困难，并需要重新训练过程。随着人们步入大型语言模型的时代，这些问题变得越来越突出。本文提出，语言模型的鲁棒性与其涵盖的预训练知识程度成正比。因此，我们引入了一种后训练的剪枝策略，旨在在剪枝过程中保留更多预训练知识，以忠实地复制密集语言模型的嵌入空间和特征空间。在这个设置中，每一层的重构误差不仅源自自身，还包括前面层的累积误差，然后进行自适应的矫正。与其他最先进的基线方法相比，我们的方法展现了更好的平衡。

    The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
    
[^57]: 长文档中快速准确的事实不一致检测

    Fast and Accurate Factual Inconsistency Detection Over Long Documents. (arXiv:2310.13189v1 [cs.CL])

    [http://arxiv.org/abs/2310.13189](http://arxiv.org/abs/2310.13189)

    SCALE是一个用于检测长文本中事实不一致性的通用模型，通过使用新颖的块化策略和自然语言推理（NLI）实现了最先进的性能，同时还能解释决策并超过竞争系统。

    

    生成式AI模型具有巨大的潜力；然而，对于当前方法难以有效解决的较长输入中的各种任务中的幻觉是一个重大挑战。我们引入了SCALE（用于大规模不一致性评估的源块化方法），这是一个用于检测事实不一致性的通用模型，使用了一种新颖的块化策略。具体而言，SCALE是一个基于自然语言推理（NLI）的模型，它使用大的文本块对长文本进行条件化。这种方法在各种任务和长文本的事实不一致性检测方面实现了最先进的性能。此外，我们利用块化机制并采用一种新颖的算法通过相关的源语句检索来解释SCALE的决策。我们的评估结果显示，SCALE在标准基准测试和我们构建的新的长形式对话数据集ScreenEval上均优于现有方法。此外，SCALE超过了竞争系统。

    Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE's decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems 
    
[^58]: CycleNet：重新思考文本引导扩散中的循环一致性，以进行图像操作

    CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])

    [http://arxiv.org/abs/2310.13165](http://arxiv.org/abs/2310.13165)

    CycleNet是一种将循环一致性引入扩散模型的新方法，用于规范图像操作，具有优越的翻译一致性和质量，并且可以生成高质量的跨领域分布图像。

    

    扩散模型（DM）在图像合成任务中取得了突破，但缺乏一种直观的一致图像到图像（I2I）翻译接口。为解决这个问题，已经探索了各种方法，包括基于掩码的方法，基于注意力的方法和基于图像的方法。然而，如何使用预训练的DMs进行无配对的I2I翻译并保持一致性仍然是一个关键挑战。本文介绍了Cyclenet，一种新颖但简单的方法，它将循环一致性纳入DMs中，以规范图像操作。我们验证了Cyclenet在不同粒度的无配对I2I任务上的优势。除了场景和对象级别的翻译，我们还贡献了一个多领域I2I翻译数据集，用于研究物体的物理状态变化。我们的实证研究表明，Cyclenet在翻译的一致性和质量方面具有优势，并且在改变文本描述时可以生成高质量的跨领域分布图像。

    Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual 
    
[^59]: 一种分布式气象预测方法：通过联邦学习和生成对抗网络解决降水预测模型中的数据不平衡问题

    A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs. (arXiv:2310.13161v1 [cs.LG])

    [http://arxiv.org/abs/2310.13161](http://arxiv.org/abs/2310.13161)

    这项研究提出了一种分布式方法来解决气象预测模型中的数据不平衡问题，通过联邦学习和生成对抗网络技术改善了模型的性能。

    

    天气数据的分类需要将气象现象分为不同的类别，从而为农业、航空和灾害管理等各个领域提供细致的分析和准确的预测。这涉及使用机器学习模型来分析大型多维天气数据集中的模式和趋势。这些数据集可能包括温度、湿度、风速和气压等变量，这些变量对气象条件起着作用。此外，分类算法必须能够有效应对数据不平衡等挑战，其中某些天气事件（如风暴或极端温度）可能被低估。这项实证研究探索了在集中和联邦环境中解决表格状天气数据中不平衡类别的数据增强方法。采用合成少数类过采样技术或生成对抗网络等数据增强技术可以改善模型的性能。

    The classification of weather data involves categorizing meteorological phenomena into classes, thereby facilitating nuanced analyses and precise predictions for various sectors such as agriculture, aviation, and disaster management. This involves utilizing machine learning models to analyze large, multidimensional weather datasets for patterns and trends. These datasets may include variables such as temperature, humidity, wind speed, and pressure, contributing to meteorological conditions. Furthermore, it's imperative that classification algorithms proficiently navigate challenges such as data imbalances, where certain weather events (e.g., storms or extreme temperatures) might be underrepresented. This empirical study explores data augmentation methods to address imbalanced classes in tabular weather data in centralized and federated settings. Employing data augmentation techniques such as the Synthetic Minority Over-sampling Technique or Generative Adversarial Networks can improve t
    
[^60]: 条件生成建模用于图像、3D动画和视频

    Conditional Generative Modeling for Images, 3D Animations, and Video. (arXiv:2310.13157v1 [cs.CV])

    [http://arxiv.org/abs/2310.13157](http://arxiv.org/abs/2310.13157)

    本论文探索了条件生成模型的新颖形式和创新应用，提出了使用神经ODE对视频动态建模，以及连续标准流的条件变体实现高分辨率图像生成。

    

    本论文旨在推动计算机视觉中生成建模领域的创新，通过探索条件生成模型的新颖形式和图像、3D动画和视频中的创新应用。我们的研究关注于提供噪声和视觉数据的可逆变换的架构，并应用编码器-解码器架构进行生成任务和3D内容操作。在所有情况下，我们都将条件信息纳入到增强视觉数据合成中，提高了生成过程的效率和生成内容的质量。我们引入了使用神经ODE对视频动态建模的方法，利用编码器-解码器架构，尽管仅训练用于重构当前帧，但能够预测未来的视频帧。接下来，我们提出了连续标准流的条件变体，通过较低分辨率的输入实现更高分辨率的图像生成。

    This dissertation attempts to drive innovation in the field of generative modeling for computer vision, by exploring novel formulations of conditional generative models, and innovative applications in images, 3D animations, and video. Our research focuses on architectures that offer reversible transformations of noise and visual data, and the application of encoder-decoder architectures for generative tasks and 3D content manipulation. In all instances, we incorporate conditional information to enhance the synthesis of visual data, improving the efficiency of the generation process as well as the generated content.  We introduce the use of Neural ODEs to model video dynamics using an encoder-decoder architecture, demonstrating their ability to predict future video frames despite being trained solely to reconstruct current frames. Next, we propose a conditional variant of continuous normalizing flows that enables higher-resolution image generation based on lower-resolution input, achiev
    
[^61]: 在临床领域的问答模型中分析自然分布偏移的CLIFT

    CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain. (arXiv:2310.13146v1 [cs.CL])

    [http://arxiv.org/abs/2310.13146](http://arxiv.org/abs/2310.13146)

    本文介绍了一个新的临床领域问答模型测试平台CLIFT，其中包括7.5k个高质量的问答样本。在原始测试集上表现出色的深度学习模型在应用于新的测试集时性能下降，表明存在分布偏移。我们的研究结果强调了提高临床领域模型鲁棒性的必要性和潜力，并提供了一个追踪该方向进展的测试平台。

    

    本文介绍了一个新的测试平台CLIFT（临床分布偏移），用于临床领域的问答任务。该测试平台包括7.5k个高质量的问答样本，提供了一个多样化且可靠的基准。我们进行了全面的实验研究，并在该测试平台下评估了几个QA深度学习模型。尽管在原始测试集上表现出色，但在应用于新的测试集时，性能下降，表明存在分布偏移。我们的研究结果强调了在分布偏移下提高临床领域模型鲁棒性的必要性和潜力。该测试平台为追踪该方向的进展提供了一种途径。它还强调了采用考虑自然分布偏移鲁棒性的评估指标的必要性。我们计划通过添加更多样本和模型结果来扩展语料库。完整的论文和更新的基准可以在github.com/openlifescience-ai/clift找到。

    This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical domain Question-answering task. The testbed includes 7.5k high-quality question answering samples to provide a diverse and reliable benchmark. We performed a comprehensive experimental study and evaluated several QA deep-learning models under the proposed testbed. Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift. Our findings emphasize the need for and the potential for increasing the robustness of clinical domain models under distributional shifts. The testbed offers one way to track progress in that direction. It also highlights the necessity of adopting evaluation metrics that consider robustness to natural distribution shifts. We plan to expand the corpus by adding more samples and model results. The full paper and the updated benchmark are available at github.com/openlifescience-ai/clift
    
[^62]: 不妨用英文问我：基于大语言模型的医疗问题跨语言评估

    Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries. (arXiv:2310.13132v1 [cs.CL])

    [http://arxiv.org/abs/2310.13132](http://arxiv.org/abs/2310.13132)

    本文提供了一个框架，研究LLMs作为医疗问题的多语言对话系统的有效性。

    

    大语言模型（LLMs）正在改变一般公众获取和消费信息的方式。它们在关键领域如医疗保健中的影响尤为显著，普通人日常查询越来越多地使用LLMs作为对话代理。虽然LLMs展示出令人印象深刻的语言理解和生成能力，但对其安全性的关注在这些高风险领域仍然很重要。此外，LLMs的开发过于集中在英文上。这些LLMs在非英文语言环境中的表现如何仍不明确，这是确保这些系统在真实环境中使用公平性的关键。本文提供了一个框架，研究LLMs作为多语言对话系统在医疗问题中的效果。我们通过实证得出的框架XlingEval，重点关注对LLMs回答自然人撰写的与健康相关问题的评估的三个基本标准：协议

    Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems.This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: co
    
[^63]: 基于深度强化学习的优化CO2排放的智能交通信号控制

    Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions. (arXiv:2310.13129v1 [eess.SY])

    [http://arxiv.org/abs/2310.13129](http://arxiv.org/abs/2310.13129)

    本研究提出了一种基于深度强化学习的智能交通信号控制算法，通过优化CO2排放和行驶时间等指标，实现了较好的性能。

    

    如今，交通网络面临着次优控制策略的挑战，这可能对人类健康、环境产生不利影响，并 contribute to traffic congestion. 由于交通拥堵导致的空气污染水平上升和通勤时间延长，交叉路口交通信号控制器成为现代交通基础设施的关键组成部分。尽管文献中有几个自适应交通信号控制器，但对其比较性能的研究有限。此外，尽管二氧化碳（CO2）排放在全球范围内非常重要，但文献对该领域关注不够。在本报告中，我们提出了EcoLight，一种针对强化学习算法的奖励塑造方案，不仅能减少CO2排放，还能在诸如行驶时间等指标上取得有竞争力的结果。我们使用行驶时间、CO2排放、等指标比较了表格式Q-Learning、DQN、SARSA和A2C算法的性能。

    Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, wa
    
[^64]: 理解Transformer中的加法

    Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])

    [http://arxiv.org/abs/2310.13121](http://arxiv.org/abs/2310.13121)

    本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。

    

    了解像Transformer这样的机器学习模型的内部工作方式对于其安全和道德使用至关重要。本文对经过训练进行整数加法的单层Transformer模型进行了深入分析。我们揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法。我们的研究还发现该模型开始计算较晚，但执行速度非常快。我们还发现了一种罕见的高损失的使用情况，并予以解释。总体而言，我们详细解释了该模型的算法。这些发现通过严格测试和数学建模得到了验证，对于机制可解释性、人工智能安全性和对齐性等广泛研究做出了贡献。我们的方法为分析更复杂的任务和多层Transformer模型打开了大门。

    Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
    
[^65]: AVTENet: 基于音频-视觉Transformer的多专家集成网络在视频深度伪造检测中的应用

    AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])

    [http://arxiv.org/abs/2310.13103](http://arxiv.org/abs/2310.13103)

    本文提出了AVTENet框架，该框架是一个基于音频-视觉Transformer的多专家集成网络，用于在视频深度伪造检测中考虑声学和视觉操作。

    

    在社交媒体平台上广泛分享的伪造内容是一个重大社会问题，要求加强监管并给研究社区带来新的挑战。近年来，超真实的深度伪造视频的普及引起了对音频和视觉伪造威胁的关注。大多数关于检测AI生成的伪造视频的先前工作只利用了视觉模态或音频模态。虽然文献中有一些方法利用音频和视觉模态来检测伪造视频，但它们尚未在涉及声学和视觉操作的多模态深度伪造视频数据集上进行全面评估。此外，这些现有方法大多基于CNN，并且检测准确率较低。受到Transformer在各个领域的最新成功启发，为了解决深度伪造技术带来的挑战，本文提出了一种考虑声学操作的音频-视觉Transformer集成网络（AVTENet）框架。

    Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulatio
    
[^66]: 粒子引导：非独立同分布样本多样性采样与扩散模型

    Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v1 [cs.LG])

    [http://arxiv.org/abs/2310.13102](http://arxiv.org/abs/2310.13102)

    本文提出了一种粒子引导的方法，通过超越独立样本的常见假设，提高了生成模型的多样性和采样效率。在实验中，我们在条件图像生成和分子构象生成上进行了测试，并取得了显著的结果。

    

    鉴于生成模型的广泛成功，已经有大量的研究致力于加快其采样时间。然而，为了获取多样性样本，生成模型通常需要进行多次采样，这会造成与采样时间无关的成本。本文处理了如何通过超越独立样本的常见假设来提高多样性和采样效率的问题。我们提出了粒子引导，一种基于扩散的生成采样的扩展，其中的联合粒子时变位势强制实现多样性。我们从理论上分析了粒子引导产生的联合分布，以及它对位势选择的影响和与其他学科方法的联系。在实证方面，我们在条件图像生成的设置中进行了测试，我们能够增加多样性而不影响质量，并在分子构象生成中降低了平均13%的先进技术中值误差。

    In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average
    
[^67]: 不冒犯，伯特 - 我只侮辱人类！多个收件人句级攻击有毒性检测神经网络。

    No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network. (arXiv:2310.13099v1 [cs.CL])

    [http://arxiv.org/abs/2310.13099](http://arxiv.org/abs/2310.13099)

    本论文介绍了一种简单而高效的句级攻击方法，可以通过添加积极的词语或句子来改变黑盒有毒性检测模型的预测结果，该方法在多种语言上均有效。

    

    我们介绍了一种简单而高效的句级攻击方法，针对黑盒有毒性检测模型。通过在仇恨信息末尾添加一些积极的词语或句子，我们能够改变神经网络的预测结果，并通过有毒性检测系统的检查。该方法在来自三个不同语言家族的七种语言上被证明有效。我们还描述了对抗上述攻击的防御机制，并讨论了其局限性。

    We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.
    
[^68]: 无监督表示学习助力半监督元学习

    Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning. (arXiv:2310.13085v1 [cs.LG])

    [http://arxiv.org/abs/2310.13085](http://arxiv.org/abs/2310.13085)

    本论文提出了一种一次性的无监督元学习方法，通过学习训练样本的潜在表示，解决了少样本学习中的数据稀缺问题。该方法在初始化和快速适应中应用了迁移学习，并提高了准确性。

    

    少样本学习或元学习利用了机器学习中的数据稀缺问题。传统上，监督学习需要大量的样本和标签。为了解决这个问题，我们提出了一种一次性的无监督元学习方法，用于学习训练样本的潜在表示。我们在无监督元学习的训练阶段使用增强样本作为查询集。在元学习的内循环中使用温度缩放的交叉熵损失来防止无监督学习过拟合。从这一步学到的参数以迁移学习的方式应用于目标监督元学习，用于初始化和快速适应，并提高准确性。所提出的方法是与模型无关的，可以帮助任何元学习模型提高准确性。我们在Omniglot和mini-Imagenet数据集上使用模型无关元学习（MAML）和关系网络（RN）来展示所提出方法的性能。

    Few-shot learning or meta-learning leverages the data scarcity problem in machine learning. Traditionally, training data requires a multitude of samples and labeling for supervised learning. To address this issue, we propose a one-shot unsupervised meta-learning to learn the latent representation of the training samples. We use augmented samples as the query set during the training phase of the unsupervised meta-learning. A temperature-scaled cross-entropy loss is used in the inner loop of meta-learning to prevent overfitting during unsupervised learning. The learned parameters from this step are applied to the targeted supervised meta-learning in a transfer-learning fashion for initialization and fast adaptation with improved accuracy. The proposed method is model agnostic and can aid any meta-learning model to improve accuracy. We use model agnostic meta-learning (MAML) and relation network (RN) on Omniglot and mini-Imagenet datasets to demonstrate the performance of the proposed met
    
[^69]: 从多语言复杂性到情感清晰度：利用常识揭示码混合对话中的情感

    From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues. (arXiv:2310.13080v1 [cs.CL])

    [http://arxiv.org/abs/2310.13080](http://arxiv.org/abs/2310.13080)

    本研究旨在利用常识信息和对话上下文结合的创新方法，揭示码混合对话中的情感动态。

    

    理解对话中的情感是人类交流的基本要素，推动着情感识别对话（ERC）的NLP研究。尽管在识别单语对话中个体演讲者的情感方面已经进行了大量的研究，但在码混合对话中理解情感动态相对较少受到关注。这激发了我们在本研究中对码混合对话进行ERC的动机。我们认识到情感智能包含对世界性知识的理解，提出了一种创新的方法，将常识信息与对话上下文相结合，促进对情感的更深入理解。为了实现这一目标，我们设计了一个高效的流程，从现有的知识图谱中提取与码混合输入相关的常识。随后，我们开发了一种先进的融合技术，将获取的常识信息与对话表示无缝结合。

    Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained fr
    
[^70]: 利用大型语言模型进行创造性机器人工具使用研究

    Creative Robot Tool Use with Large Language Models. (arXiv:2310.13065v1 [cs.RO])

    [http://arxiv.org/abs/2310.13065](http://arxiv.org/abs/2310.13065)

    本文利用大型语言模型开发了一个系统，名为RoboTool，可以使机器人在涉及隐含物理约束和长期规划的任务中创造性地使用工具。该系统通过解析自然语言指令、生成全面的策略、计算技能参数并生成可执行的Python代码来实现。实验结果表明，RoboTool能够有效地处理任务中的物理约束和环境因素。

    

    工具使用是高级智能的标志，不仅存在于动物行为中，也体现在机器人的能力中。本文研究了将机器人赋予创造性使用工具的能力，用于涉及隐含物理约束和长期规划的任务中。通过利用大型语言模型（LLM），我们开发了RoboTool，一个系统，它接受自然语言指令并输出控制机器人在模拟和真实环境中的可执行代码。RoboTool包含四个关键组件：（i）"解析器"，用于解释自然语言以识别与任务相关的关键概念，（ii）"规划器"，根据语言输入和关键概念生成全面的策略，（iii）"计算器"，用于计算每个技能的参数，（iv）"编码器"，将这些计划转换成可执行的Python代码。我们的结果表明，RoboTool不仅可以理解明确或隐含的物理约束和环境因素，而且还可以产生有效的执行计划。

    Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an "Analyzer" that interprets natural language to discern key task-related concepts, (ii) a "Planner" that generates comprehensive strategies based on the language input and key concepts, (iii) a "Calculator" that computes parameters for each skill, and (iv) a "Coder" that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend explicit or implicit physical constraints and environmental factors bu
    
[^71]: 健壮的多模态模型具有异常特征并编码更多概念

    Robust multimodal models have outlier features and encode more concepts. (arXiv:2310.13040v1 [cs.LG])

    [http://arxiv.org/abs/2310.13040](http://arxiv.org/abs/2310.13040)

    健壮的多模态模型展示了异常特征和更多概念的编码方式。

    

    什么区分健壮模型与非健壮模型？随着大规模多模态模型（如CLIP）的出现，这个问题引起了人们的关注。这些模型在自然分布转变方面表现出了前所未有的健壮性。尽管已经证明了健壮性的差异可以追溯到训练数据上的差异，但迄今为止还不清楚这对于模型学习到了什么意味着。在这项工作中，我们通过探测12个具有不同骨干（ResNets和ViTs）和预训练集（OpenAI，LAION-400M，LAION-2B，YFCC15M，CC12M和DataComp）的健壮多模态模型的表示空间来填补这一空白。我们发现这些模型的表示空间中存在两个健壮性的特征：（1）健壮模型具有由其激活特征表征的异常特征，其中一些特征值比平均值高几个数量级。这些异常特征在模型的表示空间中引入了特权方向。我们证明了...

    What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demon
    
[^72]: Agri-GNN:一种新颖的基因型-拓扑图神经网络框架，建立在GraphSAGE上，用于优化产量预测。

    Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction. (arXiv:2310.13037v1 [cs.LG])

    [http://arxiv.org/abs/2310.13037](http://arxiv.org/abs/2310.13037)

    Agri-GNN是一种新型基因型-拓扑图神经网络框架，通过考虑作物之间的复杂空间和基因型相互作用，实现优化的农作物产量预测。

    

    农业作为人类文明的基石，不断寻求整合技术以提高生产力和可持续性。本文介绍了一种名为Agri-GNN的新型基因型-拓扑图神经网络框架，旨在捕捉作物之间复杂的空间和基因型相互作用，为优化农作物产量预测铺平道路。Agri-GNN构建了一个图G，将农田地块视为节点，并根据空间和基因型相似性方法地构建节点之间的边，通过基因型-拓扑过滤器对节点信息进行聚合。图神经网络（GNN）通过设计考虑数据点之间的关系，使其能够有效地对农业生态系统进行建模。通过利用GNN的能力，Agri-GNN可以从植物中捕捉局部和全局信息，考虑它们基于空间属性的内在关联。

    Agriculture, as the cornerstone of human civilization, constantly seeks to integrate technology for enhanced productivity and sustainability. This paper introduces $\textit{Agri-GNN}$, a novel Genotypic-Topological Graph Neural Network Framework tailored to capture the intricate spatial and genotypic interactions of crops, paving the way for optimized predictions of harvest yields. $\textit{Agri-GNN}$ constructs a Graph $\mathcal{G}$ that considers farming plots as nodes, and then methodically constructs edges between nodes based on spatial and genotypic similarity, allowing for the aggregation of node information through a genotypic-topological filter. Graph Neural Networks (GNN), by design, consider the relationships between data points, enabling them to efficiently model the interconnected agricultural ecosystem. By harnessing the power of GNNs, $\textit{Agri-GNN}$ encapsulates both local and global information from plants, considering their inherent connections based on spatial pro
    
[^73]: LASER：无线分布式优化中的线性压缩

    LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])

    [http://arxiv.org/abs/2310.13033](http://arxiv.org/abs/2310.13033)

    LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。

    

    数据并行SGD是分布式优化的事实上的算法，尤其适用于大规模机器学习。尽管它有很多优点，但通信瓶颈是其中持久存在的问题之一。大多数压缩方案要么假设通信链路无噪声，要么在实际任务中无法取得良好的性能。在本文中，我们填补了这一空白，介绍了LASER：无线分布式优化中的线性压缩。LASER利用梯度的固有低秩结构，在噪声通道上高效传输梯度。尽管享受与经典SGD相似的理论保证，LASER在各种实际基准测试中表现出持续的优势。特别是，在具有挑战性的计算机视觉和GPT语言建模任务中，它优于最先进的压缩方案。在后者中，我们相对于噪声通道上的基准模型在困惑度上获得了50-64%的提升。

    Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
    
[^74]: AI反馈促进的质量-多样性算法

    Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])

    [http://arxiv.org/abs/2310.13032](http://arxiv.org/abs/2310.13032)

    基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。

    

    在许多文本生成问题中，用户可能不仅偏好单一回复，而是希望得到多样性的高质量输出以供选择。质量-多样性（QD）搜索算法旨在通过不断改进和多样化候选人群来实现这一目标。然而，QD在创作性写作等质性领域的应用受到算法指定质量和多样性度量的困难的限制。有趣的是，最近语言模型（LMs）的发展使得通过AI反馈指导搜索成为可能，其中LMs在自然语言中被提示来评估文本的质性方面。借助这一进展，我们引入了通过AI反馈实现的质量-多样性算法（QDAIF），其中进化算法应用LMs来生成变异并评估候选文本的质量和多样性。在创作性写作领域的评估中，与非QDAIF算法相比，QDAIF更广泛地覆盖高质量样本的指定搜索空间。

    In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
    
[^75]: 将查询重写重新定义为统计机器翻译问题的用例

    A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem. (arXiv:2310.13031v1 [cs.CL])

    [http://arxiv.org/abs/2310.13031](http://arxiv.org/abs/2310.13031)

    本文提出了一种基于统计机器翻译的查询重写方法，通过学习重写阿拉伯语用户查询以改善搜索引擎的检索结果。

    

    现代搜索引擎面临的最重要挑战之一是根据用户查询检索相关的网络内容。为了实现这个挑战，搜索引擎有一个模块来重写用户查询。因此，现代网络搜索引擎利用了自然语言处理领域中使用的一些统计和神经模型。其中，统计机器翻译是一种众所周知的NLP方法。本文提出了一种基于单语机器翻译模型的查询重写流程，学习如何重写阿拉伯语用户搜索查询。本文还描述了创建用户查询和网页标题之间映射的预处理步骤。

    One of the most important challenges for modern search engines is to retrieve relevant web content based on user queries. In order to achieve this challenge, search engines have a module to rewrite user queries. That is why modern web search engines utilize some statistical and neural models used in the natural language processing domain. Statistical machine translation is a well-known NLP method among them. The paper proposes a query rewriting pipeline based on a monolingual machine translation model that learns to rewrite Arabic user search queries. This paper also describes preprocessing steps to create a mapping between user queries and web page titles.
    
[^76]: 融合梯度提升树和神经网络进行层次时间序列的点预测和概率预测

    Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series. (arXiv:2310.13029v1 [cs.LG])

    [http://arxiv.org/abs/2310.13029](http://arxiv.org/abs/2310.13029)

    本文提出了一种融合梯度提升树和神经网络的方法，有效解决了层次时间序列的点预测和概率预测问题。

    

    本文通过描述一种融合梯度提升树和神经网络方法的机器学习模型，解决了点预测和概率预测的问题。该方法在最近的M5竞赛的准确性和不确定性赛道上取得了成功。我们方法的关键点包括：a)将任务转化为对每天销售额的回归问题；b)信息丰富的特征工程；c)创建一系列最先进的机器学习模型；d)精心构建用于模型调优的验证集。我们认为，机器学习模型的多样性以及验证样本的精心选择是我们方法有效的最重要因素。尽管预测数据具有层次结构（12个层级），但我们提出的解决方案没有利用该层次结构。使用所提出的方法，我们的团队在金牌范围内排名。

    In this paper we tackle the problem of point and probabilistic forecasting by describing a blending methodology of machine learning models that belong to gradient boosted trees and neural networks families. These principles were successfully applied in the recent M5 Competition on both Accuracy and Uncertainty tracks. The keypoints of our methodology are: a) transform the task to regression on sales for a single day b) information rich feature engineering c) create a diverse set of state-of-the-art machine learning models and d) carefully construct validation sets for model tuning. We argue that the diversity of the machine learning models along with the careful selection of validation examples, where the most important ingredients for the effectiveness of our approach. Although forecasting data had an inherent hierarchy structure (12 levels), none of our proposed solutions exploited that hierarchical scheme. Using the proposed methodology, our team was ranked within the gold medal ran
    
[^77]: 可靠的学术会议问答：基于大型语言模型的研究

    Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])

    [http://arxiv.org/abs/2310.13028](http://arxiv.org/abs/2310.13028)

    本研究以大型语言模型为基础，开发了可靠的学术会议问答系统，通过组织半结构化的会议数据并进行人工标注，解决了研究人员在获取准确、最新信息时的需求。

    

    计算机科学的快速发展导致学术会议上的研究大量增加，促进了全球学术交流。研究人员在各个阶段都持续寻求关于这些事件的准确、最新信息。这种数据爆发需要一个智能的问答系统来高效解决研究人员的问题，并确保对最新进展的了解。会议信息通常在官方网站上发布，以半结构化的方式组织，并包含大量的文本。为了满足这一需求，我们开发了ConferenceQA数据集，涵盖了7个不同学术会议，并进行了人工标注。首先，我们采用手动和自动方法的组合，以半结构化的JSON格式组织学术会议数据。随后，我们为每个会议注释了近100个问题-答案对。每个对应对应了四个不同的维度分类。为了确保数据的可靠性，我们手动进行了标注。

    The rapid growth of computer science has led to a proliferation of research presented at academic conferences, fostering global scholarly communication. Researchers consistently seek accurate, current information about these events at all stages. This data surge necessitates an intelligent question-answering system to efficiently address researchers' queries and ensure awareness of the latest advancements. The information of conferences is usually published on their official website, organized in a semi-structured way with a lot of text. To address this need, we have developed the ConferenceQA dataset for 7 diverse academic conferences with human annotations. Firstly, we employ a combination of manual and automated methods to organize academic conference data in a semi-structured JSON format. Subsequently, we annotate nearly 100 question-answer pairs for each conference. Each pair is classified into four different dimensions. To ensure the reliability of the data, we manually annotate 
    
[^78]: 通过附加件变得贝叶斯，捕捉更多不确定性

    Be Bayesian by Attachments to Catch More Uncertainty. (arXiv:2310.13027v1 [cs.LG])

    [http://arxiv.org/abs/2310.13027](http://arxiv.org/abs/2310.13027)

    本文提出了一种附加结构贝叶斯神经网络(ABNN)，通过在主干网络中整合足够分布外数据的不确定性，来提高神经网络对不确定性的捕捉能力。

    

    贝叶斯神经网络(BNNs)已成为不确定性评估的有希望方法之一，由于其坚实的理论基础。然而，BNNs的性能受到捕捉不确定性的能力的影响。本文提出了一种新的附加结构贝叶斯神经网络(ABNN)，通过附加结构从足够分布外的数据(OOD)中捕捉更多的不确定性。我们首先根据先验分布为OOD数据的不确定性构建了一个数学描述，然后开发了一个附加的贝叶斯结构将OOD数据的不确定性整合到主干网络中。ABNN由期望模块和若干分布模块组成。期望模块是一个专注于原始任务的主干深度网络，而分布模块则是作为主干的附加结构的小贝叶斯结构。特别地，这些分布模块的目的是检测和传播OOD数据的不确定性，从而提高整体网络的贝叶斯性质。

    Bayesian Neural Networks (BNNs) have become one of the promising approaches for uncertainty estimation due to the solid theorical foundations. However, the performance of BNNs is affected by the ability of catching uncertainty. Instead of only seeking the distribution of neural network weights by in-distribution (ID) data, in this paper, we propose a new Bayesian Neural Network with an Attached structure (ABNN) to catch more uncertainty from out-of-distribution (OOD) data. We first construct a mathematical description for the uncertainty of OOD data according to the prior distribution, and then develop an attached Bayesian structure to integrate the uncertainty of OOD data into the backbone network. ABNN is composed of an expectation module and several distribution modules. The expectation module is a backbone deep network which focuses on the original task, and the distribution modules are mini Bayesian structures which serve as attachments of the backbone. In particular, the distribu
    
[^79]: 使用幂集多类交叉熵损失函数进行神经说话人分离

    Powerset multi-class cross entropy loss for neural speaker diarization. (arXiv:2310.13025v1 [cs.SD])

    [http://arxiv.org/abs/2310.13025](http://arxiv.org/abs/2310.13025)

    提出了一种将多标签公式改为幂集多类分类的方法，通过大量实验表明这种方法在性能和鲁棒性方面都显著优于原始方法，并消除了多标签公式中的关键超参数。

    

    自从2019年提出以来，全端到端神经说话人分离（EEND）一直将说话人分离视为一个逐帧多标签分类问题，并采用了排列不变的训练方法。尽管EEND显示出了很大的潜力，但最近的一些研究退回到了研究（本地）有监督EEND说话人分离与（全局）无监督聚类的可能组合。然而，这些混合方法并没有对原始的多标签公式产生疑问。我们提出从多标签（允许同时出现两个说话人）转换为幂集多类分类（为重叠说话人对分配专门的类别）。通过在9个不同的基准测试中进行大量实验，我们表明这种公式可以显著提高性能（主要是在重叠语音上）并且对领域不匹配具有鲁棒性，同时消除了多标签公式中关键的检测阈值超参数。

    Since its introduction in 2019, the whole end-to-end neural diarization (EEND) line of work has been addressing speaker diarization as a frame-wise multi-label classification problem with permutation-invariant training. Despite EEND showing great promise, a few recent works took a step back and studied the possible combination of (local) supervised EEND diarization with (global) unsupervised clustering. Yet, these hybrid contributions did not question the original multi-label formulation. We propose to switch from multi-label (where any two speakers can be active at the same time) to powerset multi-class classification (where dedicated classes are assigned to pairs of overlapping speakers). Through extensive experiments on 9 different benchmarks, we show that this formulation leads to significantly better performance (mostly on overlapping speech) and robustness to domain mismatch, while eliminating the detection threshold hyperparameter, critical for the multi-label formulation.
    
[^80]: 朝向随时微调：使用超网络提示进行持续预训练的语言模型

    Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt. (arXiv:2310.13024v1 [cs.CL])

    [http://arxiv.org/abs/2310.13024](http://arxiv.org/abs/2310.13024)

    本研究旨在解决持续预训练方法在未知领域上性能下降的问题，提出了一种基于超网络提示的持续预训练方法，通过使用一致性和不一致性损失来生成领域特定的提示，显著减轻了领域特异性并促进了知识的传递。

    

    在快速发展的世界中，持续预训练对于使预训练模型适应各种领域和任务变得迫切。在实践中，持续预训练模型被期望不仅在预训练领域上进行微调时具有更大的容量，而且在未知领域上的性能不会下降。在本文中，我们首先调查了现有持续预训练方法的随时微调效果，得出了对未知领域的普遍性能下降的结论。为此，我们提出了一种基于提示的持续预训练方法，通过一致性和不一致性损失训练超网络生成特定领域的提示。一致性损失最大程度地保留了预训练模型对新领域的泛化能力，而不一致性损失则保护了为每个领域生成的隐藏状态的独特性。显著的是，超网络生成的提示在微调时减轻了领域特异性并促进了知识的传递。

    Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance on unseen domains. To this end, we propose a prompt-guided continual pre-training method, where we train a hypernetwork to generate domain-specific prompts by both agreement and disagreement losses. The agreement loss maximally preserves the generalization of a pre-trained model to new domains, and the disagreement one guards the exclusiveness of the generated hidden states for each domain. Remarkably, prompts by the hypernetwork alleviate the domain identity when fine-tuning and promote knowledge
    
[^81]: GraphGPT: 大型语言模型的图指令调优

    GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])

    [http://arxiv.org/abs/2310.13023](http://arxiv.org/abs/2310.13023)

    本论文提出了GraphGPT框架，它是一种面向图结构知识的大型语言模型，通过图指令调优实现高度泛化，即使在没有下游图数据的情况下也能在不同的下游数据集和任务上取得很好的效果。

    

    通过图节点之间的递归信息交换和聚合，图神经网络（GNN）在理解图结构方面取得了进展。为了提高模型的健壮性，自监督学习（SSL）已经成为一种有前途的数据增强方法。然而，现有的用于生成预训练图嵌入的方法通常依赖于对特定下游任务标签进行微调，这限制了它们在标记数据稀缺或不可用的情况下的可用性。为了解决这个问题，我们的研究重点是提升图模型在具有挑战性的零样本学习场景中的泛化能力。受大型语言模型（LLM）的成功启发，我们的目标是开发一种面向图结构知识的LLM，即使没有来自下游图数据的任何信息，也能在不同的下游数据集和任务上实现高度泛化。在这项工作中，我们提出了GraphGPT框架，通过图指令调优将LLM与图结构知识对齐。

    Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction t
    
[^82]: 不确定性感知的参数高效自训练用于半监督语言理解

    Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding. (arXiv:2310.13022v1 [cs.LG])

    [http://arxiv.org/abs/2310.13022](http://arxiv.org/abs/2310.13022)

    本论文提出了一种不确定性感知的参数高效自训练（UPET）框架，通过利用Monte Carlo dropout来进行贝叶斯神经网络中的不确定性估计，并根据置信度和确定性选择可靠的伪标记样本，以解决标记数据稀缺问题。

    

    最近大型预训练语言模型（PLM）的成功在很大程度上依赖于大量标记数据，这在资源有限的情况下通常会产生较差的性能。为了解决这个困境，我们研究了自训练作为半监督学习（SSL）方法中的一种主要方法，利用大规模未标记数据生成合成样本。然而，太多的噪声标签会损害模型性能，而且自训练过程需要多次训练迭代，如果更新PLM的所有模型参数，会更加昂贵。本文提出了UPET，一种新颖的不确定性感知的参数高效自训练框架，以有效且高效地解决标记数据稀缺问题。具体而言，我们将蒙特卡罗（MC）dropout引入贝叶斯神经网络（BNN）以进行教师模型的不确定性估计，然后根据置信度和确定性来精确选择可靠的伪标记样本。

    The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the stude
    
[^83]: 数学的人工智能：从认知科学的角度来看

    AI for Mathematics: A Cognitive Science Perspective. (arXiv:2310.13021v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.13021](http://arxiv.org/abs/2310.13021)

    本文从认知科学的角度探讨了构建人类级数学系统的目标，并强调了认知科学对于AI从业者在此方向上的价值。结合开放性讨论和问题，认为需要多学科合作来实现更好的数学AI系统。

    

    数学是人类开发和使用的最强大的概念系统之一。自动化数学家的梦想在人工智能领域有着悠久的历史。人工智能的快速进展，尤其是在大型语言模型 (LLM) 方面的进步，引发了对构建这样的系统的广泛兴趣。在这项工作中，我们从"认知科学"的角度对这些目标进行了反思。我们指出了一些来自认知科学的经典研究方向和当前研究方向，我们认为这些方向对于寻求构建真正的人类（或超人类）级数学系统的AI从业者来说是有价值的。最后，我们提出了一些需要多学科视角的开放性讨论和问题—认知科学家与人工智能研究人员和数学家共同合作—在我们朝着更好的数学AI系统迈进的过程中，这些系统不仅可以帮助我们推动数学的前沿，还可以提供一瞥

    Mathematics is one of the most powerful conceptual systems developed and used by the human species. Dreams of automated mathematicians have a storied history in artificial intelligence (AI). Rapid progress in AI, particularly propelled by advances in large language models (LLMs), has sparked renewed, widespread interest in building such systems. In this work, we reflect on these goals from a \textit{cognitive science} perspective. We call attention to several classical and ongoing research directions from cognitive science, which we believe are valuable for AI practitioners to consider when seeking to build truly human (or superhuman)-level mathematical systems. We close with open discussions and questions that we believe necessitate a multi-disciplinary perspective -- cognitive scientists working in tandem with AI researchers and mathematicians -- as we move toward better mathematical AI systems which not only help us push the frontier of the mathematics, but also offer glimpses into 
    
[^84]: 通过DeepFool算法对深度神经网络进行有针对性的类别操纵的对抗攻击定制

    Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])

    [http://arxiv.org/abs/2310.13019](http://arxiv.org/abs/2310.13019)

    本文提出了一种增强版DeepFool算法，名为Targeted DeepFool，可以针对特定类别进行错误分类，并引入了最小置信度分数要求超参数来提高灵活性。

    

    深度神经网络（DNNs）在各个领域都取得了显著的进展，但对抗攻击的易受攻击性引起了严重关注。了解这些易受攻击性并开发有效的防御机制至关重要。DeepFool是Moosavi-Dezfooli等人（2016年）提出的一种算法，用于找到将输入图像错误分类的最小扰动。然而，DeepFool缺乏有针对性的方法，使其在特定攻击场景中的有效性较低。此外，在先前的相关工作中，研究人员主要关注的是成功率，而没有考虑图像被扭曲的程度、图像质量的完整性以及错误分类的置信度水平。因此，在本文中，我们提出了Targeted DeepFool，这是DeepFool的增强版，可以针对特定类别进行错误分类。我们还引入了一个最小置信度分数要求超参数来增强灵活性。我们的实验证明了所提方法在不同情况下的有效性和效率。

    Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
    
[^85]: 对表示一致性达成共识

    Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.13018](http://arxiv.org/abs/2310.13018)

    该论文研究了生物和人工信息处理系统的表示一致性，探讨了不同系统之间的表示是否一致以及如何调整表示以更好地匹配其他系统。为了改善领域之间的交流，提出了一个统一的框架作为共同语言。

    

    生物和人工信息处理系统构建可以用来进行分类、推理、规划、导航和决策的世界表示。这些多样化系统所构建的表示在多大程度上是一致的？即使表示不同，是否仍然能够导致相同的行为？系统如何修改它们的表示以更好地匹配另一个系统的表示？这些关于表示一致性研究的问题是当代认知科学、神经科学和机器学习中一些最活跃的研究领域的核心。不幸的是，对于对表示一致性感兴趣的研究社区之间的知识转移有限，其中大部分在一个领域的进展最终会在另一个领域独立地重新发现，而更广泛的领域间交流将是有利的。为了改善领域之间的交流，我们提出了一个统一的框架，可以作为一种共同的语言。

    Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
    
[^86]: 位置插值改进了ALiBi外推能力

    Position Interpolation Improves ALiBi Extrapolation. (arXiv:2310.13017v1 [cs.CL])

    [http://arxiv.org/abs/2310.13017](http://arxiv.org/abs/2310.13017)

    该论文提出了一种使用线性位置插值来改进ALiBi模型外推能力的方法，并且在上游语言建模和下游摘要和检索任务中取得了显著的改进。

    

    线性位置插值有助于使用旋转位置嵌入（RoPE）的预训练模型对更长的序列长度进行外推。我们提出使用线性位置插值来扩展使用带有线性偏差的注意力（ALiBi）的模型的外推范围。我们发现，位置插值显著改善了上游语言建模和下游摘要和检索任务的外推能力。

    Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.
    
[^87]: 使用基于图形的方法解决大型语言模型系统的乘法问题

    Solving the multiplication problem of a large language model system using a graph-based method. (arXiv:2310.13016v1 [cs.OH])

    [http://arxiv.org/abs/2310.13016](http://arxiv.org/abs/2310.13016)

    我们提出了一种基于图形的乘法算法，通过引入10k操作符，模拟了人类类似的数值运算，解决了GPT和其他大型语言模型的乘法挑战。该算法在100万个大数乘法任务中实现了100％的准确性。

    

    基于生成式预训练变换器（GPT）的聊天机器人软件ChatGPT具有出色的自然语言处理能力，但在解决算术问题，尤其是乘法方面不足。它的GPT结构使用计算图进行乘法操作，但在简单乘法操作之外，其准确性有限。我们开发了一种基于图形的乘法算法，模拟了类似人类的数值运算，引入了一个10k操作符，其中k表示两个输入数中较大数的以10为底的最大幂次。我们提出的算法对于100万个大数乘法任务实现了100％的准确性，有效解决了基于GPT和其他大型语言模型的乘法挑战。我们的工作强调了将简单的人类洞察力融入到人工智能算法设计中的重要性。

    The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication. Its GPT structure uses a computational graph for multiplication, which has limited accuracy beyond simple multiplication operations. We developed a graph-based multiplication algorithm that emulated human-like numerical operations by incorporating a 10k operator, where k represents the maximum power to base 10 of the larger of two input numbers. Our proposed algorithm attained 100% accuracy for 1,000,000 large number multiplication tasks, effectively solving the multiplication challenge of GPT-based and other large language models. Our work highlights the importance of blending simple human insights into the design of artificial intelligence algorithms. Keywords: Graph-based multiplication; ChatGPT; Multiplication problem
    
[^88]: Audio-AdapterFusion：一种无任务ID的高效和非破坏性多任务语音识别方法

    Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition. (arXiv:2310.13015v1 [cs.CL])

    [http://arxiv.org/abs/2310.13015](http://arxiv.org/abs/2310.13015)

    Audio-AdapterFusion提出了一种无任务ID的多任务语音识别方法，通过结合单任务适配器实现非破坏性和参数高效，且在多个测试集上表现优异。

    

    Adapter是预训练模型的高效可组合替代方案，有助于将大型ASR模型扩展到多个任务。然而，在推断期间，常常需要在输入上添加任务ID以将其路由到特定任务的单任务适配器。然而，这种方法的一个主要限制是在推断期间任务ID可能是未知的，因此不适用于大多数多任务设置。为了解决这个问题，我们提出了三种新颖的无任务ID方法来结合多任务ASR中的单任务适配器，并研究了两种用于训练的学习算法。我们在来自4个不同ASR任务的10个测试集上评估了我们的方法，并证明了我们的方法是非破坏性和参数高效的。在仅更新模型参数的17%的情况下，相对于完全微调，我们的方法可以实现8%的平均WER改进，并与任务ID适配器路由效果相当。

    Adapters are an efficient, composable alternative to full fine-tuning of pre-trained models and help scale the deployment of large ASR models to many tasks. In practice, a task ID is commonly prepended to the input during inference to route to single-task adapters for the specified task. However, one major limitation of this approach is that the task ID may not be known during inference, rendering it unsuitable for most multi-task settings. To address this, we propose three novel task-ID-free methods to combine single-task adapters in multi-task ASR and investigate two learning algorithms for training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and show that our methods are non-destructive and parameter-efficient. While only updating 17% of the model parameters, our methods can achieve an 8% mean WER improvement relative to full fine-tuning and are on-par with task-ID adapter routing.
    
[^89]: 大型语言模型的预测能力：来自现实预测竞赛的证据

    Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. (arXiv:2310.13014v1 [cs.CY])

    [http://arxiv.org/abs/2310.13014](http://arxiv.org/abs/2310.13014)

    该论文通过参与Metaculus平台举办的预测竞赛，实证测试了OpenAI的最先进大型语言模型GPT-4的概率预测能力，并发现其与人类预测相比明显不准确。

    

    准确预测未来将是人工智能能力的重要里程碑。然而，关于大型语言模型提供关于未来事件概率预测能力的研究仍处于初级阶段。为了经验性地测试这种能力，我们将OpenAI最先进的大型语言模型GPT-4纳入了Metaculus平台举办的为期三个月的预测竞赛。这场从2023年7月到10月进行的竞赛吸引了843名参与者，涵盖了包括大型科技公司、美国政治、病毒爆发和乌克兰冲突在内的各种主题。我们聚焦于二进制预测，结果显示，与人群中位数预测相比，GPT-4的概率预测明显不准确。我们发现，GPT-4的预测与将每个问题的概率分配为50%的无信息预测策略没有显著差异。我们探讨了一个潜在的解释，即GPT-4可能有倾向性地预测概率为50%。

    Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities clo
    
[^90]: 使用大型语言模型的代码交替语音识别的生成式错误校正

    Generative error correction for code-switching speech recognition using large language models. (arXiv:2310.13013v1 [cs.CL])

    [http://arxiv.org/abs/2310.13013](http://arxiv.org/abs/2310.13013)

    本论文提出了一种使用大型语言模型和自动生成的假设列表来进行代码交替语音识别的生成式错误校正方法。

    

    代码交替语音识别是指在同一句子中混合使用两种或更多语言的现象。尽管自动语音识别（ASR）的最新进展，代码交替语音识别仍然是一项具有挑战性的任务，原因是该现象的语法结构复杂以及特定训练语料库的数据稀缺性。在这项工作中，我们提出利用大型语言模型（LLM）和ASR生成的假设列表来解决代码交替问题。具体而言，我们首先使用多个经过良好训练的ASR模型进行N-best假设生成，旨在增加假设集中的多样性和信息量。接下来，我们利用LLM学习假设到转录的映射，通过添加可训练的低秩适配器。这种生成式错误校正（GER）方法根据其专业的语言知识和N-best假设直接预测准确的转录，从传统的语言模型的范式转变为。

    Code-switching (CS) speech refers to the phenomenon of mixing two or more languages within the same sentence. Despite the recent advances in automatic speech recognition (ASR), CS-ASR is still a challenging task ought to the grammatical structure complexity of the phenomenon and the data scarcity of specific training corpus. In this work, we propose to leverage large language models (LLMs) and lists of hypotheses generated by an ASR to address the CS problem. Specifically, we first employ multiple well-trained ASR models for N-best hypotheses generation, with the aim of increasing the diverse and informative elements in the set of hypotheses. Next, we utilize the LLMs to learn the hypotheses-to-transcription (H2T) mapping by adding a trainable low-rank adapter. Such a generative error correction (GER) method directly predicts the accurate transcription according to its expert linguistic knowledge and N-best hypotheses, resulting in a paradigm shift from the traditional language model r
    
[^91]: H2O开放生态系统用于最先进的大规模语言模型

    H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])

    [http://arxiv.org/abs/2310.13012](http://arxiv.org/abs/2310.13012)

    H2O推出了开放生态系统，旨在开发和测试最先进的大规模语言模型（LLMs），包括h2oGPT和H2O LLM Studio。这一开源项目提供了全面开放的替代方案，能够帮助推动人工智能的发展，使其更加可信赖和可访问。

    

    大规模语言模型（LLMs）代表了人工智能的一项革命。然而，它们也带来了许多重大风险，例如存在偏见、私有、受版权保护或有害的文本。因此，我们需要开放、透明和安全的解决方案。我们介绍了一个完整的开源生态系统，用于开发和测试LLMs。该项目的目标是推动对封闭源方法的开放式替代方案。我们发布了h2oGPT，即从70亿到700亿参数的一系列精细调整的LLMs。我们还推出了H2O LLM Studio，这是一个框架和无代码GUI，专为使用最新的先进技术进行LLMs的高效精细调整、评估和部署而设计。我们的代码和模型在完全自由的Apache 2.0许可证下授权使用。我们相信开源语言模型有助于推动人工智能的发展，并使其更可访问和可信赖。演示网址为：https://gpt.h2o.ai/

    Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. The demo is available at: https://gpt.h2o.ai/
    
[^92]: 使用基于Perceiver的序列分类器和通用语音模型检测语音异常

    Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model. (arXiv:2310.13010v1 [eess.AS])

    [http://arxiv.org/abs/2310.13010](http://arxiv.org/abs/2310.13010)

    该论文提出了一种使用基于Perceiver的序列分类器和通用语音模型检测语音异常的方法，该方法可以针对不同类别的输入建模不同的区域，同时具有数据效率性。通过在Mayo Clinic的语料库上进行广泛评估，该模型表现出色，平均准确率达到83.1%。预训练是重要的，而且意外的是，与不相关的自动语音识别（ASR）任务进行预训练也是有益的。

    

    我们提出了一种基于Perceiver的序列分类器，用于检测反映多种神经系统疾病的语音异常。我们将这个分类器与一个训练有素的通用语音模型（USM）相结合，该模型在1200万小时的多样化音频记录上进行无监督训练。我们的模型将长序列压缩为一小组特定类别的潜在表示，并使用分解投影预测输入语音的不同属性。我们的方法的好处是可以为不同类别的输入建模不同的区域，并且同时具有数据效率性。我们对Mayo Clinic的精选语料库对我们提出的模型进行了广泛评估。我们的模型在标准Transformer（80.9%）和Perceiver（81.8%）模型上表现出色，平均准确率达到83.1%。在有限的任务特定数据情况下，我们发现预训练非常重要，而且令人惊讶的是，与不相关的自动语音识别（ASR）任务进行预训练也是有益的。

    We propose a Perceiver-based sequence classifier to detect abnormalities in speech reflective of several neurological disorders. We combine this classifier with a Universal Speech Model (USM) that is trained (unsupervised) on 12 million hours of diverse audio recordings. Our model compresses long sequences into a small set of class-specific latent representations and a factorized projection is used to predict different attributes of the disordered input speech. The benefit of our approach is that it allows us to model different regions of the input for different classes and is at the same time data efficient. We evaluated the proposed model extensively on a curated corpus from the Mayo Clinic. Our model outperforms standard transformer (80.9%) and perceiver (81.8%) models and achieves an average accuracy of 83.1%. With limited task-specific data, we find that pretraining is important and surprisingly pretraining with the unrelated automatic speech recognition (ASR) task is also benefic
    
[^93]: LoBaSS：在监督微调数据中测量可学习性

    LoBaSS: Gauging Learnability in Supervised Fine-tuning Data. (arXiv:2310.13008v1 [cs.LG])

    [http://arxiv.org/abs/2310.13008](http://arxiv.org/abs/2310.13008)

    本文介绍了一种新的方法LoBaSS，利用数据的可学习性作为选择监督微调数据的主要标准。这种方法可以根据模型的能力将数据选择与模型对齐，确保高效的学习。

    

    监督微调（SFT）是将大型语言模型（LLM）与特定任务的先决条件对齐的关键阶段。微调数据的选择深刻影响模型的性能，传统上以数据质量和分布为基础。在本文中，我们引入了SFT数据选择的一个新维度：可学习性。这个新维度的动机是由LLM在预训练阶段获得的能力。鉴于不同的预训练模型具有不同的能力，适合一个模型的SFT数据可能不适合另一个模型。因此，我们引入了学习能力这个术语来定义数据对模型进行有效学习的适合性。我们提出了基于损失的SFT数据选择（LoBaSS）方法，利用数据的可学习性作为选择SFT数据的主要标准。这种方法提供了一种细致的方法，允许将数据选择与固有的模型能力对齐，确保高效的学习。

    Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring op
    
[^94]: XAI的公平效益的批判性调查

    A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v1 [cs.AI])

    [http://arxiv.org/abs/2310.13007](http://arxiv.org/abs/2310.13007)

    这个批判性调查分析了可解释的人工智能（XAI）与公平之间的关系，指出XAI在实现公平理想方面存在潜力和限制，呼吁更具体地说明XAI方法如何帮助解决公平理想。

    

    在这个批判性调查中，我们分析了关于可解释的人工智能（XAI）和公平之间关系的典型论述，以解开这两个概念之间的多维关系。通过系统文献综述和随后的定性内容分析，我们从175篇论文中识别出关于XAI的公平效益的七个典型论断。我们提出关于这些论断的重要警告，并为未来围绕XAI在特定公平理想中的潜力和限制进行讨论提供了一个切入点。虽然文献通常认为XAI是实现多个公平理想的一种手段，但我们注意到这些理想与XAI的能力之间存在不一致。我们鼓励将XAI视为应对算法公平这一多维社会技术挑战的众多工具之一，并更具体地说明哪种XAI方法如何帮助哪些人解决哪些公平理想。

    In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. While the literature often suggests XAI to be an enabler for several fairness desiderata, we notice a misalignment between these desiderata and the capabilities of XAI. We encourage to conceive XAI as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of XAI method enables whom to address which fairness desideratum.
    
[^95]: 基于生成式人工智能的软件元数据分类

    Software Metadata Classification based on Generative Artificial Intelligence. (arXiv:2310.13006v1 [cs.SE])

    [http://arxiv.org/abs/2310.13006](http://arxiv.org/abs/2310.13006)

    本文提出了一种利用生成式人工智能在二进制代码评论质量分类模型中提升性能的新方法，通过引入生成数据集，模型的准确性得到了显著改善，支持向量机模型的精确度提高了6%，人工神经网络模型的召回率提高了1.5%。

    

    本文提出了一种新颖的方法，通过应用生成式人工智能(AI)来提高二进制代码评论质量分类模型的性能。通过利用OpenAI API，将从各种GitHub仓库和开源项目中提取的1239个新生成的代码-评论对数据集标记为“有用”或“无用”，并与现有的9048个C编程语言对数据集集成。利用先进的大型语言模型架构，生成的数据集在模型准确性方面表现出明显的改善。具体来说，当集成到支持向量机(SVM)模型中时，精确度提高了6%，从0.79增加到0.85。此外，人工神经网络(ANN)模型的召回率提高了1.5%，从0.731增加到0.746。本文揭示了生成式人工智能在增强代码评论质量分类模型中的潜力。

    This paper presents a novel approach to enhance the performance of binary code comment quality classification models through the application of Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a dataset comprising 1239 newly generated code-comment pairs, extracted from various GitHub repositories and open-source projects, has been labelled as "Useful" or "Not Useful", and integrated into the existing corpus of 9048 pairs in the C programming language. Employing a cutting-edge Large Language Model Architecture, the generated dataset demonstrates notable improvements in model accuracy. Specifically, when incorporated into the Support Vector Machine (SVM) model, a 6% increase in precision is observed, rising from 0.79 to 0.85. Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5% increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the potential of Generative AI in augmenting code comment quality classification models. The res
    
[^96]: Metacognitive threshold: a computational account. (arXiv:2310.13005v1 [cs.OH])的翻译标题: 元认知阈值：一个计算模型。

    Metacognitive threshold: a computational account. (arXiv:2310.13005v1 [cs.OH])

    [http://arxiv.org/abs/2310.13005](http://arxiv.org/abs/2310.13005)

    元认知阈值的计算模型以及通过元认知训练和冥想影响该阈值的潜在认知机制。

    

    本文将探讨如何计算元认知阈值，即被感知为一种心理状态所需的最小刺激量，并讨论通过元认知训练和冥想可以影响该阈值的潜在认知机制。

    This paper will explore ways of computationally accounting for the metacognitive threshold -- the minimum amount of stimulus needed for a mental state to be perceived -- and discuss potential cognitive mechanisms by which this threshold can be influenced through metacognitive training and meditation.
    
[^97]: 渐进高效学习

    Progressively Efficient Learning. (arXiv:2310.13004v1 [cs.LG])

    [http://arxiv.org/abs/2310.13004](http://arxiv.org/abs/2310.13004)

    CEIL是一种渐进高效的学习框架，通过给学习代理提供一个抽象、动态的语言和一种内在的动机，以尽可能少的沟通代价学习，实现了类似人类逐步高效沟通的能力。在2D MineCraft领域上，CEIL展示了令人印象深刻的性能和沟通效率。

    

    助理 AI 代理应该能够迅速获取新技能并适应用户的新偏好。传统的框架如模仿学习和强化学习不能支持这种能力，因为它们只支持低级、低效的沟通形式。相比之下，人类通过定义和分享抽象意图来实现逐步高效的沟通。为了在 AI 代理中再现类似的能力，我们开发了一种名为沟通高效交互学习（CEIL）的新型学习框架。通过为学习代理提供一个抽象、动态的语言和一种内在的动机，即以尽可能少的沟通代价学习，CEIL 可以导致学习者和教师逐渐高效地交流，通过交换越来越抽象的意图。CEIL 在一个包含长期决策任务的 2D MineCraft 领域上展示了令人印象深刻的性能和沟通效率。

    Assistant AI agents should be capable of rapidly acquiring novel skills and adapting to new user preferences. Traditional frameworks like imitation learning and reinforcement learning do not facilitate this capability because they support only low-level, inefficient forms of communication. In contrast, humans communicate with progressive efficiency by defining and sharing abstract intentions. Reproducing similar capability in AI agents, we develop a novel learning framework named Communication-Efficient Interactive Learning (CEIL). By equipping a learning agent with an abstract, dynamic language and an intrinsic motivation to learn with minimal communication effort, CEIL leads to emergence of a human-like pattern where the learner and the teacher communicate progressively efficiently by exchanging increasingly more abstract intentions. CEIL demonstrates impressive performance and communication efficiency on a 2D MineCraft domain featuring long-horizon decision-making tasks. Agents trai
    
[^98]: 会话式金融信息检索模型（ConFIRM）

    Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])

    [http://arxiv.org/abs/2310.13001](http://arxiv.org/abs/2310.13001)

    ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。

    

    随着大型语言模型（LLM）的指数级增长，利用它们在金融等专门领域的新兴特性具有探索的价值。然而，金融等受监管领域具有独特的约束条件，需要具备针对该领域的优化框架。我们提出了ConFIRM，一种基于LLM的会话式金融信息检索模型，用于查询意图分类和知识库标记。ConFIRM包括两个模块：1）一种合成金融领域特定问答对的方法，以及2）评估参数高效的微调方法来进行查询分类任务。我们生成了一个包含4000多个样本的数据集，并在单独的测试集上评估了准确性。ConFIRM实现了超过90%的准确性，这对于符合监管要求至关重要。ConFIRM提供了一种数据高效的解决方案，用于提取金融对话系统的精确查询意图。

    With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
    
[^99]: 关系相关性增强的文档级关系抽取

    Document-Level Relation Extraction with Relation Correlation Enhancement. (arXiv:2310.13000v1 [cs.IR])

    [http://arxiv.org/abs/2310.13000](http://arxiv.org/abs/2310.13000)

    该论文提出了一种关系图方法，用于解决文档级关系抽取中忽视关系相关性的问题。通过构建关系图、加权处理和利用图注意力网络，可以有效地捕捉文档中关系之间的相关性。该方法可以作为现有模型的模块集成，并在实验中取得了良好的效果。

    

    文档级关系抽取是一项致力于识别文档内实体间关系的任务。然而，现有的文档级关系抽取模型往往忽视了关系之间的相关性，并缺乏对关系相关性的定量分析。为了解决这一问题并有效捕捉文档级关系抽取中的关系相关性，我们提出了一种关系图方法，旨在明确利用关系之间的相互依赖关系。首先，我们构建一个关系图，利用先前的关系知识导出的统计共现信息来建模关系之间的相关性。其次，我们采用重新加权的方法创建一个有效的关系相关性矩阵，以引导关系信息的传播。此外，我们利用图注意力网络来聚合关系嵌入。重要的是，我们的方法可以无缝地作为一个即插即用的模块集成到现有的模型中。实验结果表明，我们的方法可以增强关系抽取的效果。

    Document-level relation extraction (DocRE) is a task that focuses on identifying relations between entities within a document. However, existing DocRE models often overlook the correlation between relations and lack a quantitative analysis of relation correlations. To address this limitation and effectively capture relation correlations in DocRE, we propose a relation graph method, which aims to explicitly exploit the interdependency among relations. Firstly, we construct a relation graph that models relation correlations using statistical co-occurrence information derived from prior relation knowledge. Secondly, we employ a re-weighting scheme to create an effective relation correlation matrix to guide the propagation of relation information. Furthermore, we leverage graph attention networks to aggregate relation embeddings. Importantly, our method can be seamlessly integrated as a plug-and-play module into existing models. Experimental results demonstrate that our approach can enhanc
    
[^100]: 自适应动态规划用于节能基站小区切换

    Adaptive Dynamic Programming for Energy-Efficient Base Station Cell Switching. (arXiv:2310.12999v1 [cs.NI])

    [http://arxiv.org/abs/2310.12999](http://arxiv.org/abs/2310.12999)

    本论文提出了一种基于近似动态规划和在线优化的方法，用于在减少网络功耗的同时保持良好的服务质量。通过预测功耗和QoS，并结合适应性QoS阈值，实现了节能基站小区切换。

    

    由于对不断进化的新一代蜂窝网络的需求增加、环境和监管方面的关注以及由地缘政治紧张局势引发的潜在能源危机，无线网络的节能越来越重要。在这项工作中，我们提出了一种基于近似动态规划（ADP）的方法，结合在线优化，在维持足够的服务质量（QoS）指标的同时，切换基站的小区以减少网络的功耗。我们使用多层感知器（MLP）来预测功耗以近似ADP中的值函数，以选择能够节约期望功耗最优的动作。为了在不损害QoS的情况下尽可能节约功耗，我们还使用另一个MLP来预测QoS，并使用长短期记忆网络（LSTM）来预测切换，将其纳入在线优化算法，生成用于根据整体情况筛选小区切换动作的自适应QoS阈值。

    Energy saving in wireless networks is growing in importance due to increasing demand for evolving new-gen cellular networks, environmental and regulatory concerns, and potential energy crises arising from geopolitical tensions. In this work, we propose an approximate dynamic programming (ADP)-based method coupled with online optimization to switch on/off the cells of base stations to reduce network power consumption while maintaining adequate Quality of Service (QoS) metrics. We use a multilayer perceptron (MLP) given each state-action pair to predict the power consumption to approximate the value function in ADP for selecting the action with optimal expected power saved. To save the largest possible power consumption without deteriorating QoS, we include another MLP to predict QoS and a long short-term memory (LSTM) for predicting handovers, incorporated into an online optimization algorithm producing an adaptive QoS threshold for filtering cell switching actions based on the overall 
    
[^101]: 基于环视摄像系统的停车位分类

    Parking Spot Classification based on surround view camera system. (arXiv:2310.12997v1 [cs.CV])

    [http://arxiv.org/abs/2310.12997](http://arxiv.org/abs/2310.12997)

    本文基于环视摄像系统进行停车位分类的研究，使用了适用于各种形状停车位的多边形边界框模型，并是第一个对鱼眼摄像机上的停车位进行检测与分类的详细研究。

    

    环视鱼眼摄像机常用于自动驾驶场景中的近场感知，包括城市驾驶和汽车代客泊车。四个鱼眼摄像头分别放置在车辆的四周，足以覆盖车辆周围360度的近场区域。基于环视摄像头，近年来已经有很多关于停车位检测的研究，主要关注的是车位的占用状态，但对于空闲车位是否与自动驾驶车辆的任务兼容的研究较少，例如，一些停车位仅供残疾人或电动车使用。在本文中，我们基于环视摄像系统进行了停车位分类的研究。我们使用了目标检测神经网络YOLOv4，并采用了一种适用于各种形状停车位（如斜停车位）的新型多边形边界框模型。据我们所知，我们是第一个在鱼眼摄像机上对停车位进行检测与分类的详细研究。

    Surround-view fisheye cameras are commonly used for near-field sensing in automated driving scenarios, including urban driving and auto valet parking. Four fisheye cameras, one on each side, are sufficient to cover 360{\deg} around the vehicle capturing the entire near-field region. Based on surround view cameras, there has been much research on parking slot detection with main focus on the occupancy status in recent years, but little work on whether the free slot is compatible with the mission of the ego vehicle or not. For instance, some spots are handicap or electric vehicles accessible only. In this paper, we tackle parking spot classification based on the surround view camera system. We adapt the object detection neural network YOLOv4 with a novel polygon bounding box model that is well-suited for various shaped parking spaces, such as slanted parking slots. To the best of our knowledge, we present the first detailed study on parking spot detection and classification on fisheye ca
    
[^102]: 无监督学习在临床药物筛选中的药物反应预测

    Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening. (arXiv:2310.12996v1 [q-bio.BM])

    [http://arxiv.org/abs/2310.12996](http://arxiv.org/abs/2310.12996)

    本文提出了一种无监督学习的解决方案，可用于临床药物筛选中的药物反应预测。通过学习相似药物的先前反应数据，该方法能够增强对未标记化合物的实时预测。

    

    传统的深度学习方法通常使用有监督学习来进行药物反应的预测，这需要依赖于已标记的药物反应数据进行模型训练。然而，在临床药物筛选阶段的实际应用中，需要药物反应模型来预测新化合物的反应，这些化合物的药物反应通常是未知的，这带来了挑战，使得传统的有监督深度学习方法在这种情况下不适用。本文提出了一种在临床药物筛选中用于药物反应预测的无监督学习解决方案。具体而言，我们提出了一种多分支多源领域自适应测试增强插件(MSDA)，MSDA可以与传统的药物反应预测方法无缝集成，从相似药物的先前反应数据中学习不变特征，以增强对未标记化合物的实时预测。我们使用GDSCv2和CellMiner数据集进行实验。结果表明，MSDA能够高效地预测药物反应。

    Conventional deep learning methods typically employ supervised learning for drug response prediction (DRP). This entails dependence on labeled response data from drugs for model training. However, practical applications in the preclinical drug screening phase demand that DRP models predict responses for novel compounds, often with unknown drug responses. This presents a challenge, rendering supervised deep learning methods unsuitable for such scenarios. In this paper, we propose a zero-shot learning solution for the DRP task in preclinical drug screening. Specifically, we propose a Multi-branch Multi-Source Domain Adaptation Test Enhancement Plug-in, called MSDA. MSDA can be seamlessly integrated with conventional DRP methods, learning invariant features from the prior response data of similar drugs to enhance real-time predictions of unlabeled compounds. We conducted experiments using the GDSCv2 and CellMiner datasets. The results demonstrate that MSDA efficiently predicts drug respon
    
[^103]: 医学影像的综合多模态分割：将YOLOv8与SAM和HQ-SAM模型相结合

    Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models. (arXiv:2310.12995v1 [cs.CV])

    [http://arxiv.org/abs/2310.12995](http://arxiv.org/abs/2310.12995)

    本文介绍了一种综合方法，将YOLOv8模型与SAM和HQ-SAM模型相结合，在多种医学影像数据集中实现了精确的感兴趣区域分割，展示了其在医学图像分析中的有效性和潜力。

    

    本文介绍了一种综合方法，用于在多种医学影像数据集中分割感兴趣区域（ROI），包括超声、CT扫描和X射线图像。所提出的方法利用YOLOv8模型的能力，在不同模态下进行近似边界框检测，同时结合Segment Anything Model (SAM) 和 High Quality (HQ) SAM进行完全自动和精确的分割。为了生成边界框，使用了每种模态的100个图像和掩模的有限集合对YOLOv8模型进行训练。通过广泛的计算和分析，得出了我们方法的有效性和在医学图像分析中的潜力。使用了多种评估指标，包括精确度、召回率、F1分数和Dice分数，来量化分割结果的准确性。进行了比较分析，评估了YOLOv8、YOLOv8+SAM和YOLOv8+HQ-SAM模型的各自和组合表现。实验结果表明，该方法具有良好的分割效果和潜力。

    This paper introduces a comprehensive approach for segmenting regions of interest (ROI) in diverse medical imaging datasets, encompassing ultrasound, CT scans, and X-ray images. The proposed method harnesses the capabilities of the YOLOv8 model for approximate boundary box detection across modalities, alongside the Segment Anything Model (SAM) and High Quality (HQ) SAM for fully automatic and precise segmentation. To generate boundary boxes, the YOLOv8 model was trained using a limited set of 100 images and masks from each modality. The results obtained from our approach are extensively computed and analyzed, demonstrating its effectiveness and potential in medical image analysis. Various evaluation metrics, including precision, recall, F1 score, and Dice Score, were employed to quantify the accuracy of the segmentation results. A comparative analysis was conducted to assess the individual and combined performance of the YOLOv8, YOLOv8+SAM, and YOLOv8+HQ-SAM models. The results indicat
    
[^104]: 不同维度的分歧：揭示认知科学与人工智能中的差异和不一致性

    Dimensions of Disagreement: Unpacking Divergence and Misalignment in Cognitive Science and Artificial Intelligence. (arXiv:2310.12994v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.12994](http://arxiv.org/abs/2310.12994)

    本文探讨了认知科学和人工智能中分歧的不同维度，指出分歧既可以来源于对象评估的差异，也可以源于对对象表示方式的不一致。研究者发展了用于量化表示重叠程度的工具，深入理解分歧和不一致性如何相互作用对于促进不同类型代理之间的有效协作至关重要。

    

    人工智能代理的普及增加了管理人类和人工智能代理之间以及人工智能代理之间的分歧的需求。研究揭示了分歧的本质：过去的心理学研究常常将分歧解释为两个代理对同一对象形成不同的评估，但分歧也可以源于代理对对象的表示方式的差异。人工智能领域研究人员关注人机对齐和计算认知科学的最新研究已着重考虑后一种分歧，并开发出可以用来量化代理之间表示重叠程度的工具。了解分歧和不一致性如何相互作用产生分歧以及解决策略如何依赖于这种相互作用是促进不同类型代理之间有效协作的关键。

    The increasing prevalence of artificial agents creates a correspondingly increasing need to manage disagreements between humans and artificial agents, as well as between artificial agents themselves. Considering this larger space of possible agents exposes an opportunity for furthering our understanding of the nature of disagreement: past studies in psychology have often cast disagreement as two agents forming diverging evaluations of the same object, but disagreement can also arise from differences in how agents represent that object. AI research on human-machine alignment and recent work in computational cognitive science have focused on this latter kind of disagreement, and have developed tools that can be used to quantify the extent of representational overlap between agents. Understanding how divergence and misalignment interact to produce disagreement, and how resolution strategies depend on this interaction, is key to promoting effective collaboration between diverse types of ag
    
[^105]: 利用大型语言模型增强健康数据互操作性：一项FHIR研究

    Enhancing Health Data Interoperability with Large Language Models: A FHIR Study. (arXiv:2310.12989v1 [cs.CL])

    [http://arxiv.org/abs/2310.12989](http://arxiv.org/abs/2310.12989)

    本研究调查了利用大型语言模型(LLM)增强医疗数据互操作性的能力。实验证明，LLM不仅简化了多步自然语言处理和人工校准过程，而且在与人工标注进行比较时，精确匹配率超过90%。

    

    在这项研究中，我们研究了大型语言模型（LLM）增强医疗数据互操作性的能力。我们利用LLM将临床文本转换为对应的FHIR资源。我们在3671个临床文本片段上进行了实验，结果显示LLM不仅简化了多步自然语言处理和人工校准过程，而且在与人工标注进行比较时，准确匹配率超过90%。

    In this study, we investigated the ability of the large language model (LLM) to enhance healthcare data interoperability. We leveraged the LLM to convert clinical texts into their corresponding FHIR resources. Our experiments, conducted on 3,671 snippets of clinical text, demonstrated that the LLM not only streamlines the multi-step natural language processing and human calibration processes but also achieves an exceptional accuracy rate of over 90% in exact matches when compared to human annotations.
    
[^106]: 流形学习及其在多媒体上的应用的调研

    A survey of manifold learning and its applications for multimedia. (arXiv:2310.12986v1 [cs.MM])

    [http://arxiv.org/abs/2310.12986](http://arxiv.org/abs/2310.12986)

    这项调研涉及到流形学习及其在多媒体领域的重要应用，为了解介绍了流形学习的基本概念和应用。

    

    流形学习是机器学习的新兴研究领域。在这项工作中，我们介绍了流形学习的基本概念，并讨论了它在多媒体重要应用领域中的应用。

    Manifold learning is an emerging research domain of machine learning. In this work, we give an introduction into manifold learning and how it is employed for important application fields in multimedia.
    
[^107]: 通过替代梯度下降在脉冲神经网络中实现能源高效的目标检测

    Enabling energy-Efficient object detection with surrogate gradient descent in spiking neural networks. (arXiv:2310.12985v1 [cs.CV])

    [http://arxiv.org/abs/2310.12985](http://arxiv.org/abs/2310.12985)

    在本研究中，我们提出了一种基于当前均值解码（CMD）方法的SNN-YOLOv3模型，用于实现能源高效的目标检测。实验结果表明，在PASCAL VOC数据集上，SNN-YOLOv3具有令人瞩目的性能，只需6个时间步，mAP达到了61.87%，比SpikingYOLO提高了近10%的mAP，同时能够降低能量消耗。

    

    脉冲神经网络（SNNs）是一种生物可行的神经网络模型，具有事件驱动处理和时空信息处理的显著优势，使得SNNs成为能源高效的目标检测的吸引人选择。然而，生物神经元动力学模型的非可微性在SNNs的训练过程中提出了挑战。此外，目前缺乏适合SNNs中目标检测的解码策略。在本研究中，我们引入了当前均值解码（CMD）方法，解决了回归问题，以促进对象检测任务中深层SNNs的训练。基于梯度替代和CMD，我们提出了SNN-YOLOv3模型用于目标检测。我们的实验表明，SNN-YOLOv3在PASCAL VOC数据集上实现了令人瞩目的性能，mAP达到了61.87%，仅需6个时间步。与SpikingYOLO相比，我们成功将mAP提高了近10%，同时降低了能量消耗。

    Spiking Neural Networks (SNNs) are a biologically plausible neural network model with significant advantages in both event-driven processing and spatio-temporal information processing, rendering SNNs an appealing choice for energyefficient object detection. However, the non-differentiability of the biological neuronal dynamics model presents a challenge during the training of SNNs. Furthermore, a suitable decoding strategy for object detection in SNNs is currently lacking. In this study, we introduce the Current Mean Decoding (CMD) method, which solves the regression problem to facilitate the training of deep SNNs for object detection tasks. Based on the gradient surrogate and CMD, we propose the SNN-YOLOv3 model for object detection. Our experiments demonstrate that SNN-YOLOv3 achieves a remarkable performance with an mAP of 61.87% on the PASCAL VOC dataset, requiring only 6 time steps. Compared to SpikingYOLO, we have managed to increase mAP by nearly 10% while reducing energy consum
    
[^108]: 深度神经网络线性区域的训练动力学

    Training Dynamics of Deep Network Linear Regions. (arXiv:2310.12977v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.12977](http://arxiv.org/abs/2310.12977)

    该研究探讨了深度神经网络的线性区域训练动力学。通过对数据点周围线性区域的局部复杂性进行统计分析，发现训练过程中这种复杂性经历了几个阶段的变化。最后，通过精确的可视化方法，观察到训练的最后阶段有一个下降趋势。

    

    对于深度神经网络（DN）的训练动力学的研究主要集中在损失函数的演变上，这些演变是在训练和测试数据点附近进行评估的。事实上，许多DN现象最初是在文献中以此为基础引入的，例如，双重下降、理解。在本研究中，我们研究了由连续分段仿射DN（例如具有（泄漏的）ReLU非线性的网络）形成的输入空间划分或线性区域的训练动力学。首先，我们提出了一种新的统计量，该统计量根据数据点周围任意维度邻域中的线性区域的集中程度来囊括DN的局部复杂性（LC）。我们观察到，在训练过程中，数据点周围的LC经历了许多阶段，从初始化后的下降趋势开始，然后上升，并最终以下降趋势结束。通过使用精确的可视化方法，我们发现在训练的最后LC下降阶段，li

    The study of Deep Network (DN) training dynamics has largely focused on the evolution of the loss function, evaluated on or around train and test set data points. In fact, many DN phenomenon were first introduced in literature with that respect, e.g., double descent, grokking. In this study, we look at the training dynamics of the input space partition or linear regions formed by continuous piecewise affine DNs, e.g., networks with (leaky)ReLU nonlinearities. First, we present a novel statistic that encompasses the local complexity (LC) of the DN based on the concentration of linear regions inside arbitrary dimensional neighborhoods around data points. We observe that during training, the LC around data points undergoes a number of phases, starting with a decreasing trend after initialization, followed by an ascent and ending with a final descending trend. Using exact visualization methods, we come across the perplexing observation that during the final LC descent phase of training, li
    
[^109]: 面向焊接过程的基于深度学习的在线质量预测系统

    Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])

    [http://arxiv.org/abs/2310.12632](http://arxiv.org/abs/2310.12632)

    该论文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念，通过收集和管理多传感器数据以及实时处理和特征工程的方式，可以识别关系并预测焊接质量。

    

    制造过程的数字化为机器学习辅助的质量保证提供了有前景的应用。一个广泛应用的制造过程，可以从数据驱动的解决方案中受益匪浅，是气体金属电弧焊接（GMAW）。焊接过程以材料性质、工艺条件和焊接质量之间复杂的因果关系为特征。在频繁更改工艺参数的非实验室环境中，通过破坏性测试准确确定焊缝质量是经济上不可行的。深度学习提供了从工艺观察中识别关系并预测焊接质量的潜力。本文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念。核心概念包括由四个主要阶段组成的管线：多传感器数据（如电流和电压）的收集和管理、时间序列的实时处理和特征工程

    The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series 
    
[^110]: 学习同时语言手势用于多模态失语类型检测

    Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])

    [http://arxiv.org/abs/2310.11710](http://arxiv.org/abs/2310.11710)

    通过学习语音和手势之间的相关性，我们提出了一种多模态图神经网络，用于准确识别特定失语类型的检测。实验证明我们的方法优于现有方法，达到了最先进的结果（F1 84.2%）。

    

    失语是一种由脑损伤引起的语言障碍，需要准确识别特定的失语类型，如Broca失语和Wernicke失语，以进行有效的治疗。然而，对于开发用于检测不同类型失语的方法，人们并没有给予足够的关注。我们意识到分析同时语言手势对于区分失语类型的重要性，提出了一种多模态图神经网络，利用语音和相应的手势模式进行失语类型检测。通过学习每种失语类型的语音和手势模态之间的相关性，我们的模型可以生成对手势信息敏感的文本表示，从而实现准确的失语类型检测。大量实验证明了我们方法的优越性，实现了最先进的结果（F1 84.2%）。我们还展示了手势特征优于声学特征，突显了手势表达在检测失语类型中的重要性。

    Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We pro
    
[^111]: 从挫折中获得智慧：通过错误分析对齐大型语言模型

    Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10477](http://arxiv.org/abs/2310.10477)

    该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。

    

    大型语言模型（LLMs）的快速发展既带来了机遇，也带来了挑战，特别是在意外生成有害和有毒回应方面。传统的对齐方法致力于引导LLMs朝着期望的性能发展并保护它们免受恶意内容的侵害，而本研究提出了一种基于错误分析的全新对齐策略，通过有意暴露LLMs的缺陷输出并进行深入评估，以完全理解内部原因，通过自然语言分析。因此，有毒回应可以转化为模型对齐的指导调谐语料，LLMs不仅可以避免生成有缺陷的回应，还可以训练其进行自我批评，发挥其辨别有毒内容的内在能力。实验结果表明，所提出的方法在安全指令遵循方面优于传统的对齐技术，同时还保持了卓越的效率。

    The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
    
[^112]: 时间序列和时空数据的大型模型：综述与展望

    Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. (arXiv:2310.10196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10196](http://arxiv.org/abs/2310.10196)

    这篇综述探讨了大型模型在时间序列和时空数据中的应用。它们不仅带来了增强的模式识别和推理能力，还为人工通用智能打下了基础。

    

    时间数据，特别是时间序列和时空数据，在现实世界的应用中非常普遍。它们捕捉动态系统的测量数据，并由物理和虚拟传感器大量产生。分析这些数据类型对于利用它们所包含的丰富信息以及受益于各种下游任务非常重要。近年来，大型语言和其他基础模型的突破推动了这些模型在时间序列和时空数据挖掘中的增加使用。这样的方法不仅能够实现跨不同领域的增强模式识别和推理，还为能够理解和处理常见时间数据的人工通用智能打下了基础。在此综述中，我们提供了针对时间序列和时空数据定制的大型模型的全面和最新综述，包括数据类型、模型类别、模型范围和应用领域/任务。我们的目标是

    Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to 
    
[^113]: 动态模块扩展和自适应的终身序列生成

    Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.09886](http://arxiv.org/abs/2310.09886)

    这项研究提出了一种动态模块扩展和自适应的方法，旨在解决终身序列生成中的持续学习问题。该方法允许模型根据任务相关性动态决定获取新知识的架构，并选择相似的先前任务来帮助适应新任务。此外，还引入了动态梯度缩放来平衡学习过程，以避免对先前学到的知识的严重遗忘。

    

    终身序列生成（LSG）是持续学习中的一个问题，旨在让模型在一系列生成任务上进行持续训练，以不断学习新的生成模式并避免遗忘先前的知识。现有的LSG方法主要关注维持旧知识，而对跨任务的知识传递关注较少。相比之下，人类可以通过利用先前获取的类似任务的知识更好地学习新任务。受人类学习范式的启发，我们提出了动态模块扩展和自适应（DMEA）的方法，该方法使模型能够根据任务相关性动态确定获取新知识的架构，并选择最相似的先前任务来促进对新任务的适应。此外，由于学习过程很容易偏向于当前任务，这可能导致更严重的遗忘先前学到的知识，因此我们提出了动态梯度缩放来平衡学习过程。

    Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the lea
    
[^114]: HierarchicalContrast: 一种用于跨领域零样本插槽填充的粗粒度到细粒度对比学习框架

    HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling. (arXiv:2310.09135v1 [cs.AI])

    [http://arxiv.org/abs/2310.09135](http://arxiv.org/abs/2310.09135)

    本研究提出了一种用于零样本插槽填充的Hierarchical Contrastive Learning Framework (HiCL)，通过粗粒度到细粒度的对比学习，学习语句令牌之间的深层语义关系，并提出了一种新的迭代标签集语义推理方法，来提高在未见插槽上的泛化能力。

    

    在面向任务的对话场景中，跨领域零样本插槽填充在利用源领域知识来学习具有高泛化能力的模型方面起着重要作用，在未知目标领域中，由于缺少带注释的数据，其性能往往不理想。然而，现有的零样本插槽填充方法在目标领域中的泛化能力有限，它们只能在已见插槽上有效地进行知识转移，对未见插槽的表现较差。为了缓解这个问题，我们提出了一种新颖的Hierarchical Contrastive Learning Framework (HiCL)用于零样本插槽填充。具体来说，我们提出了一种基于高斯分布嵌入的粗粒度到细粒度对比学习方法，通过优化间隔和内部标记分布的距离，学习语句令牌之间的深层语义关系。这鼓励HiCL在训练阶段泛化到未见的插槽类型。此外，我们提出了一种新的迭代标签集语义推理方法，来对标签进行公正的推断。

    In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a novel Hierarchical Contrastive Learning Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarseto fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens, by optimizing inter- and intra-token distribution distance. This encourages HiCL to generalize to the slot types unseen at training phase. Furthermore, we present a new iterative label set semantics inference method to unbiasedly 
    
[^115]: 朝着更好的指令遵循评估：摘要中的一个案例研究

    Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization. (arXiv:2310.08394v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08394](http://arxiv.org/abs/2310.08394)

    本论文通过收集真实数据集riSum，对大型语言模型的指令遵循能力进行了评估，并提出了新的参考方法，以衡量这种能力。这些方法在性能上与需要高质量摘要的参考方法相当。

    

    尽管最近取得了一些进展，但是评估大型语言模型(LLMs)如何遵循用户指令仍然是一个开放问题。虽然语言模型的评估方法在基于提示的方法上有所增长，但是对这些方法的正确性的研究还很有限。在这项工作中，我们进行了元评估，评估了一系列指标的准确性，以衡量LLMs的指令遵循能力。我们的研究是在基于查询的摘要任务上进行的，通过收集一个新的短文形式的真实数据集riSum，其中包含300个文档指令对，每个对应3个答案。所有900个答案由3名人类标注员进行评分。利用riSum，我们分析了评估方法与人类判断之间的一致性。最后，我们提出了基于LLM的新的无参考评估方法，改进了已有的基准方法，并与需要高质量摘要的昂贵基于参考的度量方法表现相当。

    Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.
    
[^116]: 一种新的自动形式化方法

    A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])

    [http://arxiv.org/abs/2310.07957](http://arxiv.org/abs/2310.07957)

    该论文提出了一种新的方法来应对研究水平的数学自动形式化任务，通过将任务分解成更容易处理的子任务，包括未链接形式化、实体链接和类型调整。此外，还提出了一个用于未链接形式化的基准数据集 arXiv2Formal。

    

    验证数学证明是困难的，但可以通过计算机的辅助实现自动化。自动形式化是将自然语言数学自动转化为可以由程序验证的形式语言的任务。这是一项具有挑战性的任务，尤其对于研究论文中的高级数学来说。研究论文中的数学需要大量的背景和上下文。本文中，我们提出了一种应对研究水平数学自动形式化的方法，将任务分解为更易于处理的子任务：未链接形式化（包含未链接的定义和定理的形式化）、实体链接（链接到正确的定理和定义）以及调整类型以通过类型检查器。此外，我们还提出了arXiv2Formal，一个用于未链接形式化的基准数据集，其中包括从arXiv.org的论文中抽取的50个定理在Lean定理证明器中进行形式化。我们欢迎任何贡献。

    Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contribut
    
[^117]: 基于多模态电子健康记录的层次预训练方法

    Hierarchical Pretraining on Multimodal Electronic Health Records. (arXiv:2310.07871v1 [cs.AI])

    [http://arxiv.org/abs/2310.07871](http://arxiv.org/abs/2310.07871)

    该论文提出了一种针对层次多模态电子健康记录数据的新颖、通用且统一的预训练框架MEDHMP，在三个级别上展示了其有效性，并且在与十八个基准方法的比较中验证了其高效性。

    

    预训练在自然语言处理中已被证明是一种强大的技术，在各种下游任务中取得了显著的成功。然而，在医学领域，现有的电子健康记录预训练模型无法捕捉到健康记录数据的层次性，限制了它们在使用单个预训练模型跨多个下游任务的泛化能力。为了解决这个挑战，本文介绍了一种新颖、通用且统一的预训练框架MEDHMP，专门针对层次多模态的健康记录数据进行设计。通过实验结果在三个级别上展示了所提出的MEDHMP的有效性，并通过与十八个基准模型的比较进一步突出了我们方法的有效性。

    Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.
    
[^118]: 对中文大语言模型的指导调整的实证研究

    An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])

    [http://arxiv.org/abs/2310.07328](http://arxiv.org/abs/2310.07328)

    本论文进行了对中文大语言模型的指导调整的实证研究，探索了LLM基础、参数高效的方法和指令数据类型的影响，并研究了其他因素的影响，为定制能更好响应中文指令的LLM提供了有价值的发现。

    

    ChatGPT的成功验证了大语言模型（LLM）在人工通用智能（AGI）中的潜力。随后，LLM的发布引起了开源社区对指导调整的兴趣，这被认为可以加快ChatGPT的复制过程。然而，关于中文的LLM指导调整的研究仍处于早期阶段。因此，本文对中文LLM的指导调整进行了深入的实证研究，这可以作为一本提供有价值发现的烹饪书来有效地定制LLM，使其能够更好地响应中文指令。具体而言，我们系统地探索了LLM基础、参数高效的方法和指令数据类型这三个对指导调整至关重要的因素的影响。此外，我们还进行了实验来研究其他因素的影响，例如思维链数据和人类价值一致性。我们希望这个实证研究能够

    The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can
    
[^119]: PICProp：用于不确定性量化的物理信息置信传播

    PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])

    [http://arxiv.org/abs/2310.06923](http://arxiv.org/abs/2310.06923)

    本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。

    

    在深度学习和物理信息学习中，标准的不确定性量化方法存在着持久的局限性。特别是，需要对数据的可能性做出强烈的假设，性能在很大程度上取决于先验的选择，并且后验只能以近似的方式进行采样，从而由于相关的计算成本导致近似精度较差。本文提出并研究了对确定性偏微分方程的置信区间（CI）估计作为一个新的问题。即，在整个区域中以概率担保的形式传播置信度，以达到数据位置到整个域的置信度传播。我们提出了一种名为物理信息置信传播（PICProp）的方法，基于双层优化来计算一个有效的CI，而不需要进行大量的假设。我们提供了关于我们方法有效性的定理以及针对物理信息学习的计算实验。

    Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
    
[^120]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^121]: 通过基于知识的推理和大型语言模型实现可解释的主张验证

    Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. (arXiv:2310.05253v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05253](http://arxiv.org/abs/2310.05253)

    本文提出了一种基于知识的推理方法，通过大型语言模型实现可解释的主张验证。该方法不需要依赖昂贵的标注数据，能够验证复杂的主张并生成解释，对协助人工事实检查员具有重要意义。

    

    主张验证在打击虚假信息方面起着至关重要的作用。尽管现有的主张验证工作取得了令人满意的结果，但仍有一个关键问题尚未解决，那就是如何在不依赖于昂贵的人工标注数据的情况下进行主张验证。此外，模型提供全面解释以证明其决策并协助人工事实检查员也非常重要。本文提出了一种能够使用大型语言模型进行复杂主张验证和生成解释的一阶逻辑指导的知识基础的推理方法（FOLK）。FOLK利用大型语言模型的上下文学习能力将主张转化为一阶逻辑子句，并生成一组谓词，每个谓词对应一个需要验证的子主张。然后，FOLK通过一组基于知识的问答对进行一阶逻辑指导的推理，从而进行真实性验证。

    Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity pr
    
[^122]: 物理感知机器学习革命科学范式对于机器学习和基于过程的水文学的影响

    Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05227](http://arxiv.org/abs/2310.05227)

    物理感知机器学习是一种革命性方法，它将物理知识和机器学习相结合，提供了准确的水文学理解和水循环预测，对于管理水资源以应对气候变化等挑战具有重要意义。

    

    准确的水文学理解和水循环预测对于解决水资源管理中的科学和社会挑战至关重要，特别是在人为气候变化的动态影响下。现有的评论主要关注机器学习在这个领域的发展，然而水文学和机器学习作为独立的范式存在明显的区别。在这里，我们介绍了以物理感知机器学习作为一种变革性方法，克服了这种认知障碍，并革新了这两个领域。具体来说，我们提出了一个综合的物理感知机器学习方法的评论，构建了一个结构化社区（PaML），将先前的物理知识或基于物理的建模与机器学习相结合。我们系统地从物理数据引导的机器学习、物理信息处理的机器学习、物理嵌入式机器学习和物理感知混合学习四个方面分析了这些PaML方法。PaML促进了机器学习辅助的假设推导。

    Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
    
[^123]: SE(3)-蛋白质主链生成中的随机流匹配

    SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])

    [http://arxiv.org/abs/2310.02391](http://arxiv.org/abs/2310.02391)

    通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。

    

    通过基于三维刚体运动（即SE(3)群）的流匹配范式，我们引入了一系列具有不断增强建模能力的新型生成模型：FoldFlow，从而实现了对蛋白质主链的准确建模。首先，我们介绍了FoldFlow-Base，一种无需模拟的学习确定性连续时间动力学和匹配不变目标分布的方法。接下来，我们通过引入Riemannian最优传输来加速训练，创建了FoldFlow-OT，从而构建了更简单和稳定的流。最后，我们设计了FoldFlow-SFM，将Riemannian最优传输和无需模拟训练相结合，可以学习SE(3)上的随机连续时间动力学。我们的FoldFlow生成模型家族相比之前的方法具有几个关键优势。

    The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
    
[^124]: 关于自然语言处理中毒性定义的探讨

    On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])

    [http://arxiv.org/abs/2310.02357](http://arxiv.org/abs/2310.02357)

    这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。

    

    毒性检测任务中的根本问题在于毒性的定义模糊不清。谷歌旗下的团队Jigsaw是该领域的领导者之一，他们使用Dixon等人给出的毒性定义：“粗鲁、不尊重或不合理的语言，可能会让某人离开讨论”。人们可以立即看到这个定义的问题，因为它没有给出毒性的定量度量，而且涉及高度主观的文化术语。尽管存在模糊和缺陷，但这个定义已经成为许多研究者广泛采用的实际标准。在这项工作中，我们提出了一种基于定量压力的毒性定义，克服了现有的缺点。

    The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
    
[^125]: 去噪扩散桥模型

    Denoising Diffusion Bridge Models. (arXiv:2309.16948v1 [cs.CV])

    [http://arxiv.org/abs/2309.16948](http://arxiv.org/abs/2309.16948)

    本论文提出了一种去噪扩散桥模型（DDBMs），该模型通过学习扩散桥的分数，并基于学习到的分数求解微分方程来实现从一个分布到另一个分布的映射，从而将多类生成模型统一起来。

    

    扩散模型是强大的生成模型，它使用随机过程将噪声映射到数据。然而，对于许多应用，如图像编辑，模型的输入不是随机噪声的分布。因此，扩散模型必须依赖于繁琐的方法，如引导或投影采样，以在生成过程中加入这些信息。在我们的工作中，我们提出了去噪扩散桥模型（DDBMs），这是一种基于扩散桥的范例，扩散桥是一族过程，其在给定的端点下插值两个配对分布。我们的方法通过学习扩散桥的分数，基于学习到的分数求解一个（随机的）微分方程，从一个端点分布映射到另一个端点分布。我们的方法自然地统一了几类生成模型，如基于分数的扩散模型和OT-Flow-Matching，使我们能够将现有的设计和架构选择适应到更通用的模型中。

    Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general 
    
[^126]: 自我监督的无约束机器人经验中的地形表示学习

    Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience. (arXiv:2309.15302v1 [cs.RO])

    [http://arxiv.org/abs/2309.15302](http://arxiv.org/abs/2309.15302)

    提出一种名为STERLING的自我监督地形表示学习方法，通过无约束的机器人经验学习有关地形的相关表示，以实现地形感知导航。

    

    地形认知，即辨别和区分不同类型的地形，是机器人在自主越野导航中必须具备的关键能力。目前提供机器人这种认知能力的方法要么依赖昂贵的标记数据收集，要么依赖无法泛化的工程特征和成本函数，或者依赖可能无法获得的专家人类示范。为了使机器人不受这些限制地具备地形认知能力，我们引入了自我监督地形表示学习（STERLING），这是一种新颖的学习地形表示的方法，仅依赖于易于收集的、无约束（例如，非专家的）和未标记的机器人经验，对数据采集没有额外的限制。STERLING采用了一种新颖的多模态自我监督目标，通过非对比度表示学习来学习地形相关的表示以用于地形感知导航。通过实物机器人实验

    Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments
    
[^127]: 面向工业应用的XAI MLOps架构研究

    Towards an MLOps Architecture for XAI in Industrial Applications. (arXiv:2309.12756v1 [cs.SE])

    [http://arxiv.org/abs/2309.12756](http://arxiv.org/abs/2309.12756)

    该论文提出了一种面向工业应用的XAI MLOps架构，旨在解决实际部署和管理ML模型中的解释和反馈能力的挑战，并提高模型准确性和用户体验。

    

    机器学习（ML）已经成为工业领域中的一种流行工具，它有助于改善操作、增加效率和降低成本。然而，在生产环境中部署和管理ML模型可能会很复杂。这就是机器学习运营（MLOps）的作用所在。MLOps旨在简化部署和管理过程。其中一个尚未解决的MLOps挑战是对解释的需求。这些解释对于理解ML模型的推理过程至关重要，这对于信任和接受是关键的。更好地识别错误和提高模型准确性只是其中的两个结果优势。一个经常被忽视的事实是，当准确性，尤其是可解释性不满足用户期望时，部署的模型在实践中会被绕过。我们开发了一种新颖的MLOps软件架构，以解决将解释和反馈能力整合到ML开发和部署过程中的挑战。在项目EXPLAIN中，我们的架构被应用于工业应用中。

    Machine learning (ML) has become a popular tool in the industrial sector as it helps to improve operations, increase efficiency, and reduce costs. However, deploying and managing ML models in production environments can be complex. This is where Machine Learning Operations (MLOps) comes in. MLOps aims to streamline this deployment and management process. One of the remaining MLOps challenges is the need for explanations. These explanations are essential for understanding how ML models reason, which is key to trust and acceptance. Better identification of errors and improved model accuracy are only two resulting advantages. An often neglected fact is that deployed models are bypassed in practice when accuracy and especially explainability do not meet user expectations. We developed a novel MLOps software architecture to address the challenge of integrating explanations and feedback capabilities into the ML development and deployment processes. In the project EXPLAIN, our architecture is
    
[^128]: 基于音频对比的微调方法

    Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])

    [http://arxiv.org/abs/2309.11895](http://arxiv.org/abs/2309.11895)

    本论文提出了一种基于音频对比的微调方法（AudioConFit），通过借助对比学习的可转移性，该方法在各种音频分类任务中表现出强大的泛化能力，并在不同设置下实现了最先进的结果。

    

    音频分类在语音和声音处理任务中起着至关重要的作用，具有广泛的应用。在将模型拟合到训练数据（避免过拟合）并使其能够良好地泛化到新领域之间仍然存在着平衡的挑战。借助对比学习的可转移性，我们引入了基于音频对比的微调方法（AudioConFit），这种方法具有强大的泛化能力。对各种音频分类任务的实证实验表明了我们方法的有效性和鲁棒性，在不同设置下取得了最先进的结果。

    Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
    
[^129]: 基于对比感知和概念处理的认知启发神经结构用于视觉抽象推理

    A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing. (arXiv:2309.10532v1 [cs.AI])

    [http://arxiv.org/abs/2309.10532](http://arxiv.org/abs/2309.10532)

    基于人类认知启发，我们提出了一种新的神经结构，用于解决视觉抽象推理任务。该架构通过追求感知和概念处理之间的一致性，采用迭代、自对比的学习过程来模拟人类的认知过程。实验证明，该网络在RAVEN数据集上实现了比之前所有模型更高的准确性，并且使用了最弱的归纳偏置。我们还指出了原始数据集中存在的类不平衡问题，并提出了解决方案。

    

    我们引入了一种新的神经结构，用于解决视觉抽象推理任务，受人类认知的启发，具体来说是由人类抽象推理通常将感知和概念处理交替进行作为灵活、迭代和动态认知过程的一部分的观察所启发。受此原理的启发，我们的架构将视觉抽象推理建模为一种迭代的、自对比的学习过程，追求视觉刺激的感知和概念处理之间的一致性。我们解释了这个新的对比感知-概念网络（CPCNet）如何通过模拟鸦文进阶矩阵智力测试的矩阵推理问题来工作。在机器学习数据集RAVEN上进行的实验证明，CPCNet在使用最弱的归纳偏置的同时实现了比之前所有已发表模型更高的精度。我们还指出了原始RAVEN数据集中存在的大量且以前没有被注意到的类不平衡问题，并提出了一个新的解决方案。

    We introduce a new neural architecture for solving visual abstract reasoning tasks inspired by human cognition, specifically by observations that human abstract reasoning often interleaves perceptual and conceptual processing as part of a flexible, iterative, and dynamic cognitive process. Inspired by this principle, our architecture models visual abstract reasoning as an iterative, self-contrasting learning process that pursues consistency between perceptual and conceptual processing of visual stimuli. We explain how this new Contrastive Perceptual-Conceptual Network (CPCNet) works using matrix reasoning problems in the style of the well-known Raven's Progressive Matrices intelligence test. Experiments on the machine learning dataset RAVEN show that CPCNet achieves higher accuracy than all previously published models while also using the weakest inductive bias. We also point out a substantial and previously unremarked class imbalance in the original RAVEN dataset, and we propose a new
    
[^130]: ChatGPT在药物研发中的应用：以与聊天机器人开发抗可卡因成瘾药物为例的案例研究

    ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots. (arXiv:2308.06920v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.06920](http://arxiv.org/abs/2308.06920)

    本文以抗可卡因成瘾药物研发为例，展示了OpenAI开发的ChatGPT在药物研发领域的创新应用。通过利用GPT-4作为虚拟导引，ChatGPT为研究人员提供战略和方法论的见解，促进了创新的药物研发方法。这项研究揭示了AI在现代药物研发中的重要作用。

    

    ChatGPT是OpenAI开发的一种基于语言模型的先进聊天机器人，它给人工智能带来了新的纪元。然而，由于潜在的缺陷，它在严格的科学研究中的作用尚不明确。本文生动展示了它在药物研发领域中的创新应用。研究专注于开发抗可卡因成瘾药物，利用GPT-4作为虚拟导引，为研究工作在候选药物的生成模型上提供战略和方法论的见解。主要目标是生成具有所需特性的最佳药物样子分子。通过利用ChatGPT的能力，本研究引入了一种新颖的药物研发方法。人工智能与研究人员之间的这种共生伙伴关系改变了药物研发的方法。聊天机器人成为促进者，引导研究人员走向创新的方法和创造有效药物候选的有成效途径。这项研究阐明了 AI 在现代药物研发中的作用。

    The birth of ChatGPT, a cutting-edge language model-based chatbot developed by OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its role in rigorous scientific research is not clear yet. This paper vividly showcases its innovative application within the field of drug discovery. Focused specifically on developing anti-cocaine addiction drugs, the study employs GPT-4 as a virtual guide, offering strategic and methodological insights to researchers working on generative models for drug candidates. The primary objective is to generate optimal drug-like molecules with desired properties. By leveraging the capabilities of ChatGPT, the study introduces a novel approach to the drug discovery process. This symbiotic partnership between AI and researchers transforms how drug development is approached. Chatbots become facilitators, steering researchers towards innovative methodologies and productive paths for creating effective drug candidates. This research sheds light on 
    
[^131]: 一个用于联合预测和规划的博弈论框架

    A Game-Theoretic Framework for Joint Forecasting and Planning. (arXiv:2308.06137v1 [cs.AI])

    [http://arxiv.org/abs/2308.06137](http://arxiv.org/abs/2308.06137)

    本论文提出了一个新颖的博弈论框架，用于联合规划和预测，以改进机器人动作的安全性。通过使用游戏理论的方法，该框架可以学习预测人类防范对策，并通过实际算法实现了端到端的模型训练。研究结果表明，该算法在人群导航和现实世界的行人运动数据集中可以产生更安全的规划。

    

    在存在人的情况下，安全规划机器人的运动需要可靠的对未来人类运动的预测。然而，简单地预测前期交互中最可能的动作并不能保证安全。这样的预测未能模拟出可能事件的长尾部分，这些事件在有限的数据集中很少被观察到。另一方面，为最坏情况下的动作进行规划会导致过分保守的行为和"僵化的机器人"。相反，我们的目标是学习预测人类防范对策的预测模型。我们提出了一个创新的游戏理论框架，用于联合规划和预测，以规划器与演示者之间的绩效作为收益，并提出了实际算法，以端对端的方式训练模型。我们证明了我们提出的算法在人群导航模拟器和现实世界的行人运动数据集中产生了更安全的计划。我们将代码发布在https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning。

    Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a ``frozen robot''. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.
    
[^132]: 自然和机器

    Nature and the Machines. (arXiv:2308.04440v1 [cs.CY])

    [http://arxiv.org/abs/2308.04440](http://arxiv.org/abs/2308.04440)

    《自然》杂志在一篇社论中呼吁我们“停止谈论明天的AI末日，而AI今天就存在风险。”这被认为是一个严重的判断失误，特别是对于有影响力的行动者来说，因为我们期望他们能够考虑到错误的后果。

    

    人工智能(AI)是否对人类构成存在危险？一些批评家认为这个问题正在受到过多的关注，他们希望将其推到一边，转而讨论AI的即时风险。这些批评家现在包括《自然》杂志，在最近的一篇社论中敦促我们“停止谈论明天的AI末日，而AI今天就存在风险。”我们认为这是一种严重的判断失误，对于《自然》杂志来说。在科学领域，就像在日常生活中一样，我们希望有影响力的行为者能够考虑错误的后果。作为世界领先的科学期刊，《自然》杂志无疑是一个有影响力的行为者，特别是在缺乏健全全球AI监管的情况下。然而，它在这个案例中明显未能考虑到错误的代价。

    Does artificial intelligence (AI) pose existential risks to humanity? Some critics feel this question is getting too much attention, and want to push it aside in favour of conversations about the immediate risks of AI. These critics now include the journal Nature, where a recent editorial urges us to 'stop talking about tomorrow's AI doomsday when AI poses risks today.' We argue that this is a serious failure of judgement, on Nature's part. In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
    
[^133]: 提高医学图像分割中的越界检测的降维方法

    Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation. (arXiv:2308.03723v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03723](http://arxiv.org/abs/2308.03723)

    本研究提出了一种降维方法，通过在医学图像分割中应用马氏距离后处理和主成分分析，实现对越界图像的高效检测。

    

    已知临床应用的分割模型在其训练分布之外的数据上表现不佳。由于这些模型在大多数情况下表现良好，因此在推断过程中检测越界图像是至关重要的，以防止自动化偏差。本研究将马氏距离后处理应用于Swin UNETR模型的瓶颈特征，该模型用于对T1加权磁共振成像上的肝脏进行分割。通过使用主成分分析降低瓶颈特征的维数，可以高效地检测到越界图像，并且具有较小的计算负载。

    Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
    
[^134]: 程序分析指南：与大型语言模型共进之旅

    The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models. (arXiv:2308.00245v1 [cs.SE])

    [http://arxiv.org/abs/2308.00245](http://arxiv.org/abs/2308.00245)

    本文详细研究了以使用前初始化错误为案例的大型语言模型辅助静态分析的开放空间，并开发了一种全自动代理程序LLift，该程序能够克服多个挑战，包括错误建模。

    

    静态分析是软件工程中广泛使用的一种技术，用于识别和减轻错误。然而，在精确性和可扩展性之间实现微妙平衡是一个重要的障碍。大型语言模型（LLM）提供了一个有希望的替代方案，由于最近的进展在理解、生成甚至调试代码方面展示出了显著能力。然而，错误的逻辑可能很复杂，需要复杂的推理和跨多个函数的大范围分析。因此，在这一点上，LLM更适合在辅助角色中与静态分析相补充使用。本文深入研究了以使用前初始化（UBI）错误为案例研究的LLM辅助静态分析的开放空间。为此，我们开发了LLift，一个完全自动化的代理程序，可以与静态分析工具和LLM进行交互。通过精心设计代理程序和提示，我们能够克服许多挑战，包括针对具体错误的建模，

    Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated agent that interfaces with both a static analysis tool and an LLM. By carefully designing the agent and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, 
    
[^135]: EmotionPrompt: 通过情感刺激提升大型语言模型的关键心理学方法

    EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])

    [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)

    EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。

    

    大型语言模型（LLMs）在推理、语言理解和数学问题解决等许多领域取得了显著的性能，并被视为人工通用智能（AGI）的关键步骤。然而，LLMs对提示的敏感性仍然是其日常应用的主要瓶颈。本文从心理学中汲取灵感，提出了EmotionPrompt来探索情感智能以提升LLMs的性能。EmotionPrompt基于一个非常简单明了的原则：将情感刺激融入到提示中。实验结果表明，我们的方法在相同的单一提示模板上，与原始的零样本提示和Zero-shot-CoT相比，在8个任务上都显著优于多种模型：ChatGPT、Vicuna-13b、Bloom和T5。此外，观察到EmotionPrompt能够提高真实性和信息量。我们相信EmotionPrompt为探索跨学科知识开辟了一条新的道路。

    Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
    
[^136]: Jina Embeddings:一种新颖的高性能句子嵌入模型

    Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])

    [http://arxiv.org/abs/2307.11224](http://arxiv.org/abs/2307.11224)

    Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。

    

    Jina Embeddings由一组高性能的句子嵌入模型组成，能够将各种文本输入转化为数值表示，从而捕捉文本的语义本质。虽然这些模型并非专门设计用于文本生成，但在密集检索和语义文本相似性等应用中表现出色。本文详细介绍了Jina Embeddings的开发过程，从创建高质量的成对和三元数据集开始。它强调了数据清理在数据集准备中的关键作用，并对模型训练过程进行了深入探讨，最后利用Massive Textual Embedding Benchmark（MTEB）进行了全面的性能评估。

    Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
    
[^137]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^138]: Act3D：无限分辨率的机器人操作检测Transformer

    Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation. (arXiv:2306.17817v1 [cs.RO])

    [http://arxiv.org/abs/2306.17817](http://arxiv.org/abs/2306.17817)

    Act3D是一种基于Transformer的操作策略，将6自由度关键姿势预测作为3D检测任务，并以自适应空间计算的方式进行处理。它在高度精确的机器人操纵任务中取得了显著的性能改进。

    

    3D感知表征非常适用于机器人操纵，因为它们可以轻松编码遮挡情况并简化空间推理。许多操纵任务需要对末端执行器姿势预测进行高空间精度，通常需要高分辨率的3D感知网格进行计算，这在处理上非常耗时。因此，大多数操作策略直接在2D中运作，放弃了3D的归纳偏差。本文提出了Act3D，一种将6自由度关键姿势预测视为自适应空间计算的操作策略Transformer。它以一个或多个摄像机视图的未投影3D特征云作为输入，以粗-精方式在自由空间中迭代采样3D点网格，使用相对空间注意力将其特征化为物理特征云，并选择最佳特征点进行末端执行器姿势预测。Act3D在已建立的操纵基准RLbench中取得了最新的最好成绩。我们的模型在该基准中实现了10%的绝对改进。

    3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute impr
    
[^139]: IIFL: 非同质人类监督员的隐性互动车队学习

    IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors. (arXiv:2306.15228v1 [cs.RO])

    [http://arxiv.org/abs/2306.15228](http://arxiv.org/abs/2306.15228)

    本研究提出了Implicit Interactive Fleet Learning (IIFL)，将隐性策略推广到交互式模仿学习中，能够解决多模态和分布转移问题。

    

    模仿学习已经应用于各种机器人任务，但在以下情况下可能会遇到困难：（1）机器人遇到训练数据中没有代表的边缘案例（分布转移）或（2）人类演示是异质的：例如，在障碍物周围采取不同路径（多模态）。交互式车队学习（IFL）通过允许机器人在任务执行过程中访问远程人类远程操作员并从他们那里学习来减轻分布转移问题，但不能处理多模态。最近的研究提出了隐性行为克隆（IBC），它能够使用基于能量的模型（EBMs）表示多模态演示。在这项工作中，我们提出用隐性互动车队学习（IIFL）解决多模态和分布转移问题，这是隐性策略在交互式模仿学习中的第一个扩展（包括单机器人、单人类的设置）。IIFL使用Jeffreys分歧的新颖应用来量化不确定性。

    Imitation learning has been applied to a range of robotic tasks, but can struggle when (1) robots encounter edge cases that are not represented in the training data (distribution shift) or (2) the human demonstrations are heterogeneous: taking different paths around an obstacle, for instance (multimodality). Interactive fleet learning (IFL) mitigates distribution shift by allowing robots to access remote human teleoperators during task execution and learn from them over time, but is not equipped to handle multimodality. Recent work proposes Implicit Behavior Cloning (IBC), which is able to represent multimodal demonstrations using energy-based models (EBMs). In this work, we propose addressing both multimodality and distribution shift with Implicit Interactive Fleet Learning (IIFL), the first extension of implicit policies to interactive imitation learning (including the single-robot, single-human setting). IIFL quantifies uncertainty using a novel application of Jeffreys divergence to
    
[^140]: FuXi: 一个15天全球天气预报级联机器学习系统

    FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.12873](http://arxiv.org/abs/2306.12873)

    逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。

    

    近年来，随着机器学习模型在天气预报中的快速发展，最先进的机器学习模型在0.25度空间分辨率下的10天天气预报中已经表现出比欧洲中期天气预报中心(ECMWF)的高分辨率预报(HRES)更优越的性能。然而，挑战在于在15天预报中表现与ECMWF集合平均(EM)相当。以前的研究表明，缓解预报误差的积累对于有效的长期预报非常重要。尽管有许多减少积累误差的努力，包括自回归多时间步长损失，但使用单个模型发现无法在短和长导出时间上达到最佳性能。因此，我们提出了FuXi，这是一个级联机器学习天气预测系统，提供了分辨率为0.25度、时间分辨率为6小时的15天全球预测。FuXi基于级联集合模型开发，它集成了多种模型的优势，并减少了预测误差的积累。使用空气温度，比湿度和位势高度的均方根误差(RMSE)和异常相关系数(ACC)评估了FuXi的性能。结果表明，与ECMWF HRES相比，FuXi在15天预报中表现出更好的性能，并显著减少了积累误差。

    Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
    
[^141]: Otter-Knowledge：不同来源的多模态知识图谱表示学习在药物发现中的基准测试。

    Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])

    [http://arxiv.org/abs/2306.12802](http://arxiv.org/abs/2306.12802)

    本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。

    

    最近，表示学习的研究利用大量的蛋白质或分子数据库，通过无监督学习技术获得药物和蛋白质结构的知识。这些预训练表示已被证明可以显著提高后续任务的准确性，如预测药物和靶蛋白之间的亲和力。在本研究中，我们展示了通过将来自不同来源和模态的知识图谱整合到序列或SMILES表示中，可以进一步丰富表示，并在已建立的基准测试数据集上实现最先进的结果。我们提供了来自7个公共来源的预处理和整合数据，其中包括超过30M个三元组。此外，我们还提供了基于这些数据的预训练模型，以及它们在Therapeutic Data Commons (TDC)基准测试中性能报告的结果。

    Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
    
[^142]: 训练好的Transformer在上下文中学习线性模型

    Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])

    [http://arxiv.org/abs/2306.09927](http://arxiv.org/abs/2306.09927)

    本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。

    

    基于注意力的神经网络，例如Transformers，在上下文学习（ICL）方面表现出了非凡的能力：给定一个来自未见过的任务的短语序列的提示，它们可以制定相关的每个令牌和下一个令牌的预测，而不需要任何参数更新。通过将标记的训练数据和未标记的测试数据序列嵌入到提示中，这使得Transformer表现得像有监督学习算法。事实上，最近的工作表明，在随机实例上训练Transformer体系结构的线性回归问题时，这些模型的预测会模仿普通最小二乘法的预测。

    Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
    
[^143]: 具有抽象和细化的描述逻辑

    Description Logics with Abstraction and Refinement. (arXiv:2306.03717v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03717](http://arxiv.org/abs/2306.03717)

    本文提出了一种扩展的描述逻辑（DLs）以支持多个抽象层次的知识表示，并证明了推理在该逻辑中是可判定的。

    

    本文提出了一种扩展的描述逻辑（DLs），该DLs有助于支持多个抽象层次的知识表示。我们的扩展DLs将抽象层级作为第一类公民，提供了明确的操作符，用于跨多个抽象层次进行概念和角色的抽象和细化，基于合取查询。我们证明了在得到的DLs族中进行推理是可判定的，而一些看似无害的变化实际上是不可判定的。我们还明确了我们逻辑和一些相关片段的复杂性。

    Ontologies often require knowledge representation on multiple levels of abstraction, but description logics (DLs) are not well-equipped for supporting this. We propose an extension of DLs in which abstraction levels are first-class citizens and which provides explicit operators for the abstraction and refinement of concepts and roles across multiple abstraction levels, based on conjunctive queries. We prove that reasoning in the resulting family of DLs is decidable while several seemingly harmless variations turn out to be undecidable. We also pinpoint the precise complexity of our logics and several relevant fragments.
    
[^144]: 推理时间干预：从语言模型中引导出真实的答案

    Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])

    [http://arxiv.org/abs/2306.03341](http://arxiv.org/abs/2306.03341)

    本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。

    

    我们介绍了推理时间干预（ITI）技术，旨在增强大型语言模型（LLMs）的真实性。ITI通过在推理过程中沿着一组方向移动模型激活，跨越有限数量的注意力头。这种干预显着提高了LLaMA模型在TruthfulQA基准上的表现。在指令微调的LLaMA Alpaca上，ITI将其真实性从32.5％提高到65.1％。我们确定了真实性和可用性之间的权衡，并演示了如何通过调整干预强度来平衡它。ITI 取得了最低程度的干扰且计算廉价。此外，该技术在数据效率上表现优异：虽然像RLHF这样的方法需要广泛注释，但是ITI仅使用了几百个例子就能定位真实的方向。我们的研究结果表明，LLMs可能具有某种内部表示方法来表示某事是真实的可能性，即使它们在表面上产生了虚假的结果。

    We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
    
[^145]: 处理BERT文本分类中的现实标签噪声

    Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v1 [cs.CL])

    [http://arxiv.org/abs/2305.16337](http://arxiv.org/abs/2305.16337)

    本文研究了BERT在现实标签噪声存在下的分类性能，发现特征相关的标签噪声和来自注释者分歧的合成标签噪声会导致BERT的分类性能下降。提出不同类型的集成和噪声清理方法以提高鲁棒性。

    

    标签噪声是由于廉价的数据标注方法（如网络爬取或众包）导致的训练标签中的错误，这可能对监督分类器的性能有害。已经提出了几种方法来抵消有监督分类中随机标签噪声的影响，并且一些研究已经表明BERT已经对高比率的随机注入标签噪声具有鲁棒性。然而，真实的标签噪声并不是随机的，而是经常与输入特征或其他注释者特定因素相关。在本文中，我们评估了BERT在面对两种类型的现实标签噪声：特征相关的标签噪声和来自注释者分歧的合成标签噪声。我们表明这些类型噪声的存在显著降低了BERT分类性能。为了提高其鲁棒性，我们评估了不同类型的集成和噪声清理方法，并比较它们在不同数据集中对标签噪声的效果。

    Labels noise refers to errors in training labels caused by cheap data annotation methods, such as web scraping or crowd-sourcing, which can be detrimental to the performance of supervised classifiers. Several methods have been proposed to counteract the effect of random label noise in supervised classification, and some studies have shown that BERT is already robust against high rates of randomly injected label noise. However, real label noise is not random; rather, it is often correlated with input features or other annotator-specific factors. In this paper, we evaluate BERT in the presence of two types of realistic label noise: feature-dependent label noise, and synthetic label noise from annotator disagreements. We show that the presence of these types of noise significantly degrades BERT classification performance. To improve robustness, we evaluate different types of ensembles and noise-cleaning methods and compare their effectiveness against label noise across different datasets.
    
[^146]: 使用OOD案例测试大型语言模型的普遍演绎推理能力

    Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15269](http://arxiv.org/abs/2305.15269)

    该研究使用OOD案例测试了大型语言模型的普遍演绎推理能力，并通过构建新的推理数据集进行了探究。研究结果表明，这些模型能够推广到更复杂的证明。

    

    鉴于证明空间的庞大，任何具有普遍演绎推理能力的模型必须能够推理更复杂的证明。最近的研究表明，大型语言模型(LLMs)在给定推理链条提示的情况下具有某些抽象演绎推理能力。然而，它们主要是在使用莫德斯坦斯或特定大小的证明上进行测试，并且与上下文示例的分布相同。为了衡量LLM的普遍演绎推理能力，我们测试了广泛的演绎规则，并测量它们推理更复杂证明的能力，方法包括深度泛化、宽度泛化和组合泛化。为了便于系统的探索，我们构建了一个新的合成和可编程推理数据集，可以对演绎规则和证明复杂性进行控制。我们对四个具有不同大小和训练目标的LLMs进行了实验，结果显示它们能够推广到复杂的证明。

    Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to c
    
[^147]: SCITAB: 一个对科学表格进行组合推理和事实验证的具有挑战性的基准测试

    SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13186](http://arxiv.org/abs/2305.13186)

    SCITAB是一个具有挑战性的评估数据集，包含1.2K个经验证的科学事实和相关的科学表格，要求进行组合推理和事实验证。对于最先进的模型来说，SCITAB提出了许多独特挑战，包括表格定位、事实歧义和组合推理。所有模型中，除了GPT-4之外，性能仅略高于随机猜测。提示技术如思维链对于在SCITAB上提升性能几乎没有作用。

    

    当前的科学事实核查基准测试存在一些不足，例如来自众包审查的偏见和对基于文本的证据的过度依赖。我们介绍了SCITAB，一个具有挑战性的评估数据集，包含1.2K个经验证的科学事实，这些事实1）来源于真实的科学出版物，2）需要组合推理进行验证。这些事实与包含证据的科学表格进行配对，并进行了标记。通过广泛的评估，我们证明SCITAB对于最先进的模型，包括基于表格的预训练模型和大型语言模型，提出了重大挑战。除了GPT-4外，所有模型的性能仅略高于随机猜测。流行的提示技术，如思维链，对SCITAB的性能提升不大。我们的分析揭示了SCITAB提出的几个独特挑战，包括表格定位、事实的歧义性和组合推理。我们的代码和数据可供使用。

    Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are 
    
[^148]: GPT4Table：大型语言模型能理解结构化表格数据吗？一项基准测试和实证研究

    GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13062](http://arxiv.org/abs/2305.13062)

    本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。

    

    大型语言模型（LLMs）作为少样本推理器来解决与自然语言相关的任务越来越具吸引力。然而，关于LLMs对结构化数据（例如表格）的理解程度还有很多需要学习的地方。尽管可以使用表格序列化作为LLMs的输入，但目前还缺乏对LLMs是否真正能够理解这类数据的全面研究。本文通过设计一个基准测试来评估LLMs的结构理解能力（SUC）来解决这个问题。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如单元格查找、行检索和大小检测。我们对GPT-3.5和GPT-4进行了一系列评估。我们发现性能因多种输入选择而异，包括表格输入格式、内容顺序、角色提示和分区标记等。根据基准测试评估所得的见解，我们提出了“自我增强”技术以改善性能。

    Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
    
[^149]: ChatGPT在软件工程中的应用范围：全面探究

    The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])

    [http://arxiv.org/abs/2305.12138](http://arxiv.org/abs/2305.12138)

    ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。

    

    ChatGPT展示了在软件工程中转化的巨大潜力，表现出在代码和文档生成等任务中的卓越性能。然而，软件工程需要高可靠性和风险控制，使ChatGPT的缺乏可解释性成为一个问题。为了解决这个问题，我们进行了一项研究，评估了ChatGPT在软件工程中的能力和局限性。我们将AI模型应对SE任务所需的能力分为三类：1）语法理解，2）静态行为理解，和3）动态行为理解。我们的调查重点是ChatGPT理解代码语法和语义结构的能力，包括抽象语法树（AST）、控制流程图（CFG）和调用图（CG）。我们评估了ChatGPT在涉及C、Java、Python和Solidity的跨语言任务中的表现。我们的发现表明，虽然ChatGPT表现出了对理解代码语法（AST）的出色能力，但在理解代码语义和部分功能上存在问题。

    ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
    
[^150]: 极地鸭子的发现之旅：使用鸭式辨析和极坐标盒嵌入增强实体链接

    Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v1 [cs.CL])

    [http://arxiv.org/abs/2305.12027](http://arxiv.org/abs/2305.12027)

    本文提出了 DUCK 方法，通过使用实体类型的先前知识在实体表示空间中注入结构信息。把盒嵌入概念引入到极坐标中，将关系表示为超球面上的盒子，将具有相似类型的实体放置在对应于它们关系的盒子内，实现聚类。在实体链接方面取得了最先进的结果，尤其在低资源环境下效果显著。

    

    基于密集检索的实体链接方法是大规模应用中高效且广泛使用的解决方案，但它们在性能上不如生成模型，因为它们对嵌入空间的结构敏感。为解决此问题，本文提出了 DUCK 方法，通过使用实体类型的先前知识在实体表示空间中注入结构信息。受编程语言中鸭式辨析的启发，我们提议根据实体在知识图中的关系定义实体的类型。然后，将盒嵌入的概念移植到球形极坐标中，我们提议将关系表示为超球面上的盒子。我们通过将具有相似类型的实体放置在对应于它们关系的盒子内来优化模型以聚类实体。我们的实验证明，我们的方法在标准实体消歧基准测试上设置了新的最先进结果，它提高了密集检索方法的性能，并且特别不适用于生成模型不可行的低资源环境。

    Entity linking methods based on dense retrieval are an efficient and widely used solution in large-scale applications, but they fall short of the performance of generative models, as they are sensitive to the structure of the embedding space. In order to address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we propose to define the type of an entity based on the relations that it has with other entities in a knowledge graph. Then, porting the concept of box embeddings to spherical polar coordinates, we propose to represent relations as boxes on the hypersphere. We optimize the model to cluster entities of similar type by placing them inside the boxes corresponding to their relations. Our experiments show that our method sets new state-of-the-art results on standard entity-disambiguation benchmarks, it improves the perf
    
[^151]: 下一个会是什么？评估神经文本生成器的不确定性与人类生产变异性对比

    What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])

    [http://arxiv.org/abs/2305.11707](http://arxiv.org/abs/2305.11707)

    本文分析了神经文本生成器与人类生产变异性之间的不确定性，通过探测生成器的输出空间来测量其对人类生产变异性的校准程度，并证明用多个样本和多个参考可以更好地了解模型的不确定性表示。

    

    在自然语言生成（NLG）任务中，针对任何输入，存在多个可行的交际目标，并且可以用多种方式将任何目标用语言表达出来或进行生产。我们表征了人类生产在四个NLG任务中词汇、句法和语义方面的变异程度，并将人类生产变异性与不确定性联系起来。然后，我们检查了生成系统预测的概率分布和解码算法所形成的输出字符串空间，以探究其不确定性。针对每个测试输入，我们测量了生成器对人类生产变异性的校准程度。通过这种基于实例级别的方法，我们分析了NLG模型和解码策略，证明用多个样本和多个参考对生成器进行探测，提供了理解模型不确定性表示所必需的详细级别。

    In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
    
[^152]: INGENIOUS：使用信息丰富的数据子集对大型语言模型进行高效预训练

    INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])

    [http://arxiv.org/abs/2305.06677](http://arxiv.org/abs/2305.06677)

    本文提出了一种使用信息丰富的数据子集来高效预训练大型语言模型的方法，减少了训练时间和计算成本，同时保持了模型的泛化能力。

    

    大型预训练语言模型的显着特点是在其泛化能力和新能力方面随着模型容量和预训练数据集大小的增加而 achieved. 然而，必须认识到这不可避免地导致了过长的训练时间、过高的计算成本和有害的环境影响。本文提出了一种方法，即是否可能仅使用高度信息丰富的训练数据子集来训练 PTLM，并同时保持其下游性能？

    A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
    
[^153]: 自动摘要中的政治偏见特征分析：以特朗普和拜登为例的案例研究

    Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden. (arXiv:2305.02321v1 [cs.CL])

    [http://arxiv.org/abs/2305.02321](http://arxiv.org/abs/2305.02321)

    本文研究了自动生成新闻文章摘要中的政治偏见。研究发现，与特朗普相比，更多的美国政府集体机构（即政府）与拜登相关联。这些发现为未来的摘要偏见研究提供了一个框架。

    

    越来越多的文献表明，强大的NLP系统可能对社会偏见进行编码；然而，自动摘要模型的政治偏见仍相对未知。在这项工作中，我们使用实体替换方法研究了新闻文章自动生成摘要中的政治家描绘。我们基于政治实体和词汇资源开发了一个计算框架，并使用它来评估抽取式和抽象式摘要模型中有关唐纳德·特朗普和乔·拜登的偏见。我们发现了一些一致的差异，例如在与特朗普相比，更多的美国政府集体机构（即政府）与拜登相关联。当实体在源文章中重点出现时，这些摘要差异最为明显。我们的系统化特征分析提供了一个未来研究摘要偏见的框架。

    Growing literature has shown that powerful NLP systems may encode social biases; however, the political bias of summarization models remains relatively unknown. In this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news articles. We develop a computational framework based on political entities and lexical resources, and use it to assess biases about Donald Trump and Joe Biden in both extractive and abstractive summarization models. We find consistent differences, such as stronger associations of a collective US government (i.e., administration) with Biden than with Trump. These summary dissimilarities are most prominent when the entity is heavily featured in the source article. Our systematic characterization provides a framework for future studies of bias in summarization.
    
[^154]: 基于Q的均衡

    Q-based Equilibria. (arXiv:2304.12647v1 [econ.TH])

    [http://arxiv.org/abs/2304.12647](http://arxiv.org/abs/2304.12647)

    该论文研究了基于Q的策略规则族中的均衡偏差（或 Qb-equilibria），即Q值在不同监测技术下的效果。

    

    在动态环境中，Q学习是一种自适应规则，其为每个替代方案提供估计值(即Q值)，该值与之前的决策相关。一个朴素的策略是始终选择具有最高Q值的替代方案。我们考虑一族基于Q的策略规则，这些规则可能系统地支持某些替代方案而不是其他替代方案，例如包含有利合作的宽容偏差的规则。在 Compte 和 Postlewaite [2018] 的精神下，我们在这个 Q-based 规则族中寻找均衡偏差（或 Qb-equilibria）。我们研究了不同监测技术下的经典博弈。

    In dynamic environments, Q-learning is an adaptative rule that provides an estimate (a Q-value) of the continuation value associated with each alternative. A naive policy consists in always choosing the alternative with highest Q-value. We consider a family of Q-based policy rules that may systematically favor some alternatives over others, for example rules that incorporate a leniency bias that favors cooperation. In the spirit of Compte and Postlewaite [2018], we look for equilibrium biases (or Qb-equilibria) within this family of Q-based rules. We examine classic games under various monitoring technologies.
    
[^155]: 带有跨设备交叉注意力的分层图神经网络用于跨设备用户匹配

    Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching. (arXiv:2304.03215v1 [cs.LG])

    [http://arxiv.org/abs/2304.03215](http://arxiv.org/abs/2304.03215)

    本文提出了一种带有跨设备交叉注意力的分层图神经网络(HGNN)，用于解决跨设备用户匹配问题，相对于最先进的TGCE方法，提高了5%的性能。

    

    在广告、推荐系统和网络安全等众多领域，跨设备用户匹配是一个关键问题。它涉及使用序列日志来识别和链接属于同一人的不同设备。以往的数据挖掘技术难以解决日志之间的长程依赖和高阶连接问题。最近，研究人员将这个问题建模为图问题，并提出了一种两层图上下文嵌入(TGCE)神经网络架构，表现优于先前的方法。在本文中，我们提出了一种新颖的分层图神经网络架构（HGNN），它具有比TGCE更为计算效率的二级设计。此外，我们在模型中引入了一种跨设备交叉注意力（Cross-Att）机制，相对于最先进的TGCE方法，提高了5%的性能。

    Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method.
    
[^156]: RepoCoder：通过迭代检索和生成实现的代码存储库级别完成

    RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])

    [http://arxiv.org/abs/2303.12570](http://arxiv.org/abs/2303.12570)

    RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。

    

    库级别代码完成任务是基于代码库更广阔上下文中继续编写未完成代码的过程。但是对于自动完成工具而言，很难利用散布在不同文件中的有用信息。我们提出了RepoCoder，这是一个简单、通用和有效的框架，可以应对这一挑战。它通过整合基于相似性的检索器和预训练的代码语言模型简化了库级别代码完成流程，从而允许有效利用库级别信息进行代码完成，并具有不同粒度层面的代码生成能力。此外，RepoCoder 还使用了一种新的迭代检索-生成模型，弥合了检索上下文和预期完成目标之间的差距。我们还提出了一个新的RepoEval基准测试，其中包含了最新和高质量真实世界的代码库，涵盖了行、API 调用和函数体完成场景。

    The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
    
[^157]: CHiLL: 零-shot定制化、可解释的医疗记录特征提取方法与大型语言模型

    CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12343](http://arxiv.org/abs/2302.12343)

    提出了CHiLL，一种用于从医疗记录中提取特征的方式，通过对大型语言模型进行查询生成特征，使医生能够用自己的专业知识制作对下游任务有临床意义的特征，并且该方法在性能和可解释性方面表现出良好的结果。

    

    我们提出了CHiLL (Crafting High-Level Latents)，一种自然语言规范化线性模型特征的方法。CHiLL通过专家制作的查询指导LLMs生成解释性特征，用于从健康记录中训练简单的线性分类器。基于LLM的查询生成特征可以使医生利用他们的领域专业知识来制作对下游任务有临床意义的特征，而无需手动从原始电子病历中提取这些特征。我们受到一个现实世界的风险预测任务的启发，但我们使用MIMIC-III和MIMIC-CXR数据和标准的预测任务（例如，30天再入院）来评估这种方法。我们发现使用自动提取的特征的线性模型在性能上与使用参考特征的模型相当，并且比使用“词袋”特征的线性模型提供更高的可解释性。我们证实了学习到的特征权重的有效性。

    We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using "Bag-of-Words" features. We verify that learned feature weig
    
[^158]: 关于形态信息在上下文词形还原中的作用

    On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.00407](http://arxiv.org/abs/2302.00407)

    本文研究了形态信息在六种语言的上下文词形还原器开发中的作用，并对词形还原器进行了领域外性能评估。

    

    词形还原是一项自然语言处理的任务，它包括从给定的屈折词生成其规范形式或词形还原。词形还原是简化下游自然语言处理应用的基本任务之一，对于高度屈折语言尤为重要。虽然根据屈折词获得词形还原形式的过程可以通过考虑其形态句法类别来解释，但在训练上下文词形还原器时引入细粒度的形态句法信息已经成为常见做法，而无视下游绩效是否优化。为了解决这个问题，本文实证研究了在六种语言中考察形态信息在上下文词形还原器开发中的作用，这六种语言的形态复杂性各不相同，包括巴斯克语、土耳其语、俄语、捷克语、西班牙语和英语。此外，与绝大多数先前的工作不同，我们还在领域外环境中进行了词形还原器的评估。

    Lemmatization is a natural language processing (NLP) task which consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this paper we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, wh
    
[^159]: 新生儿重症监护室与人工智能的过去、现在和未来

    The Past, Current, and Future of Neonatal Intensive Care Units with Artificial Intelligence. (arXiv:2302.00225v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.00225](http://arxiv.org/abs/2302.00225)

    本论文回顾了新生儿科学中最近发展的基于机器学习和深度学习的解决方案，系统评估了它们在新生儿科应用中的作用，并描述了最新的研究进展。

    

    机器学习和深度学习是人工智能的两个子领域，涉及教授计算机从各种数据中学习和做出决策。最近在人工智能领域的大部分发展来自深度学习，在计算机视觉到健康科学等几乎所有领域都被证明是革命性的。深度学习在医学中的影响已经显著改变了传统的临床应用方式。尽管一些医学的子领域，例如儿科学，相对较慢地接受了深度学习的重要好处，但相关的儿科研究也开始积累到了一个重要的水平。因此，在本文中，我们回顾了新生儿科学中最近发展的基于机器学习和深度学习的解决方案。我们对经典机器学习和深度学习在新生儿科应用中的角色进行了系统评估，定义了方法论，包括算法发展，并且描述了最新的研究进展。

    Machine learning and deep learning are two subsets of artificial intelligence that involve teaching computers to learn and make decisions from any sort of data. Most recent developments in artificial intelligence are coming from deep learning, which has proven revolutionary in almost all fields, from computer vision to health sciences. The effects of deep learning in medicine have changed the conventional ways of clinical application significantly. Although some sub-fields of medicine, such as pediatrics, have been relatively slow in receiving the critical benefits of deep learning, related research in pediatrics has started to accumulate to a significant level, too. Hence, in this paper, we review recently developed machine learning and deep learning-based solutions for neonatology applications. We systematically evaluate the roles of both classical machine learning and deep learning in neonatology applications, define the methodologies, including algorithmic developments, and describ
    
[^160]: 一种基于领域约束的弱监督学习可扩展技术

    A Scalable Technique for Weak-Supervised Learning with Domain Constraints. (arXiv:2301.05253v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05253](http://arxiv.org/abs/2301.05253)

    我们提出了一种使用领域约束的可扩展技术，用于弱监督学习神经网络分类未标记数据。我们通过在MNIST图像分类问题上的实验证明，我们的方法比以前的方法在规模上有显著的提升。

    

    我们提出了一种新颖的可扩展的端到端流水线技术，利用符号领域知识作为约束条件，以弱监督的方式学习用于分类未标记数据的神经网络。我们的方法特别适用于数据包含不同的组（类），适宜于聚类友好的表示学习，并且可以通过考虑多个训练示例一次性使用高效的数学优化技术来重新制定领域约束。

    We propose a novel scalable end-to-end pipeline that uses symbolic domain knowledge as constraints for learning a neural network for classifying unlabeled data in a weak-supervised manner. Our approach is particularly well-suited for settings where the data consists of distinct groups (classes) that lends itself to clustering-friendly representation learning and the domain constraints can be reformulated for use of efficient mathematical optimization techniques by considering multiple training examples at once. We evaluate our approach on a variant of the MNIST image classification problem where a training example consists of image sequences and the sum of the numbers represented by the sequences, and show that our approach scales significantly better than previous approaches that rely on computing all constraint satisfying combinations for each training example.
    
[^161]: 通过机械解释性的进展测量来理解"grokking"

    Progress measures for grokking via mechanistic interpretability. (arXiv:2301.05217v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05217](http://arxiv.org/abs/2301.05217)

    该论文通过机械解释性的方法找到了连续的进展测量，以理解神经网络中 emergent behavior 的产生原因。其中，作者以“grokking”现象为案例，通过逆向工程的方式完全理解了小型transformer网络在模块化加法任务中的算法，并定义了进展测量来研究这个现象的进展性质。

    

    神经网络经常表现出新兴行为，也就是通过增加参数、训练数据或训练步骤来产生质的变化。理解新兴行为的一种方法是找到连续的“进展测量”，这些测量是看似不连续的质的变化的基础。我们认为，可以通过机械解释性来找到进展测量：将学到的行为逆向工程成其各个组成部分。作为一个案例研究，我们调查了小型transformer训练模块化加法任务时出现的“grokking”现象。我们完全逆向工程了这些网络学到的算法，该算法使用离散傅里叶变换和三角恒等式将加法转换为圆周旋转。我们通过分析激活和权重，并在傅里叶空间进行消融实验证实了这个算法。基于这种理解，我们定义了进展测量，使我们能够研究这个现象的进展性质。

    Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the 
    
[^162]: 释放共享标签结构在人类活动识别中的力量

    Unleashing the Power of Shared Label Structures for Human Activity Recognition. (arXiv:2301.03462v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03462](http://arxiv.org/abs/2301.03462)

    本文提出了SHARE，一种考虑共享标签结构的HAR框架，通过建模共同的结构从而揭示不同活动之间的知识。

    

    当前的人类活动识别（HAR）技术将活动标签视为整数类别ID，没有明确地对类别标签的语义进行建模。我们观察到不同的活动名称通常具有共享的结构。例如，“打开门”和“打开冰箱”都有“打开”作为动作；“踢足球”和“打网球”都有“球”作为物体。标签名称中的这种共享结构可以转化为感知数据的相似性，并且建模共同的结构可以帮助揭示不同活动之间的知识，特别是对于样本有限的活动。在本文中，我们提出了SHARE，一种考虑不同活动的标签名称共享结构的HAR框架。为了利用共享结构，SHARE包括一个编码器，用于从输入的感知时间序列中提取特征，并包括一个解码器，用于生成标签名称作为令牌序列。我们还提出了三种标签增强技术，以帮助模型更加有效地进行预测。

    Current human activity recognition (HAR) techniques regard activity labels as integer class IDs without explicitly modeling the semantics of class labels. We observe that different activity names often have shared structures. For example, "open door" and "open fridge" both have "open" as the action; "kicking soccer ball" and "playing tennis ball" both have "ball" as the object. Such shared structures in label names can be translated to the similarity in sensory data and modeling common structures would help uncover knowledge across different activities, especially for activities with limited samples. In this paper, we propose SHARE, a HAR framework that takes into account shared structures of label names for different activities. To exploit the shared structures, SHARE comprises an encoder for extracting features from input sensory time series and a decoder for generating label names as a token sequence. We also propose three label augmentation techniques to help the model more effecti
    
[^163]: 数据中心人工智能

    Data-centric Artificial Intelligence. (arXiv:2212.11854v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.11854](http://arxiv.org/abs/2212.11854)

    数据中心人工智能是一种新兴的范式，它强调系统性地设计和构建数据对于建立有效和高效的基于人工智能的系统至关重要。

    

    数据中心人工智能（data-centric AI）是一种新兴的范式，强调系统性地设计和构建数据对于建立有效和高效的基于人工智能的系统至关重要。本文的目的是向信息系统领域的从业者和研究人员介绍数据中心人工智能。我们定义相关术语，提供关键特征来对比数据中心范式和模型中心范式，并介绍了一个数据中心人工智能的框架。我们区分数据中心人工智能和相关概念，并讨论其对信息系统社区的长期影响。

    Data-centric artificial intelligence (data-centric AI) represents an emerging paradigm emphasizing that the systematic design and engineering of data is essential for building effective and efficient AI-based systems. The objective of this article is to introduce practitioners and researchers from the field of Information Systems (IS) to data-centric AI. We define relevant terms, provide key characteristics to contrast the data-centric paradigm to the model-centric one, and introduce a framework for data-centric AI. We distinguish data-centric AI from related concepts and discuss its longer-term implications for the IS community.
    
[^164]: 傅立叶分析实现一致且真实的模型解释

    Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2210.17426](http://arxiv.org/abs/2210.17426)

    该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。

    

    对于许多跨学科领域，机器学习的解释需要与当前案例相关的假设情景一致，即如果一个因素改变，模型会如何反应？尽管归因方法由优雅的公理系统支持，但它们主要关注单个输入，并且通常不一致。为支持假设情景，我们引入了一个称为真实解释的新概念，并应用布尔函数的傅立叶分析来获得严格的保证。实验结果表明，对于各种半径的邻域，我们的方法与其他方法相比，可以实现2倍至50倍更低的解释误差。

    For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
    
[^165]: 利用事件的视频级语义一致性进行视听事件定位

    Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization. (arXiv:2210.05242v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.05242](http://arxiv.org/abs/2210.05242)

    该论文提出了一种新颖的视频级语义一致性引导网络，用于视听事件定位任务。它利用事件的视频级语义信息进行语义一致性建模，通过跨模态事件表示提取器和内模态语义一致性增强器实现。与现有方法相比，该方法能更好地捕捉和利用同一视频中事件的语义一致性。

    

    近年来，视听事件（AVE）定位引起了很大的关注。大多数现有方法通常只能独立地对从完整视频中分离出的每个视频段进行编码和分类（可视为事件的片段级表示）。然而，它们忽视了同一完整视频中事件的语义一致性（可视为事件的视频级表示）。与现有方法不同，我们提出了一种新颖的视频级语义一致性引导网络用于AVE定位任务。具体而言，我们提出了一种事件语义一致性建模（ESCM）模块，用于探索视频级语义信息进行语义一致性建模。它由两个组件组成：跨模态事件表示提取器（CERE）和内模态语义一致性增强器（ISCE）。CERE被提出用于获取视频级事件语义信息。此外，ISCE采取视频级事件语义信息并增强一致性建模。

    Audio-visual event (AVE) localization has attracted much attention in recent years. Most existing methods are often limited to independently encoding and classifying each video segment separated from the full video (which can be regarded as the segment-level representations of events). However, they ignore the semantic consistency of the event within the same full video (which can be considered as the video-level representations of events). In contrast to existing methods, we propose a novel video-level semantic consistency guidance network for the AVE localization task. Specifically, we propose an event semantic consistency modeling (ESCM) module to explore video-level semantic information for semantic consistency modeling. It consists of two components: a cross-modal event representation extractor (CERE) and an intra-modal semantic consistency enhancer (ISCE). CERE is proposed to obtain the event semantic information at the video level. Furthermore, ISCE takes video-level event seman
    
[^166]: DESCN: 深度整体空间交叉网络用于个体治疗效果估计

    DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09920](http://arxiv.org/abs/2207.09920)

    本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。

    

    因果推断在电子商务和精准医学等领域有广泛应用，其性能严重依赖于个体治疗效果(ITE)的准确估计。传统上，通过分别在各自的样本空间中建模受治疗组和对照组的响应函数来预测ITE。然而，在实际应用中，这种方法通常遇到两个问题，即由于治疗偏差而导致的受治疗组和对照组之间的分布偏离，以及其人口规模之间的显著样本不平衡。本文提出了深度整体空间交叉网络(DESCN)，以端到端的方式建模治疗效果。DESCN通过一个跨网络以多任务学习的方式捕捉治疗倾向、响应和隐藏治疗效果的综合信息。我们的方法在整个样本空间中联合学习治疗和响应函数，以避免治疗偏差，并采用中间伪处理方式。

    Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
    
[^167]: 针对网络剪枝的修剪感知稀疏正则化

    Pruning-aware Sparse Regularization for Network Pruning. (arXiv:2201.06776v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.06776](http://arxiv.org/abs/2201.06776)

    本文提出了一种修剪感知的稀疏正则化的网络剪枝方法，使用精细的稀疏正则化仅对由剪枝掩模选择的特定滤波器进行处理，从而减少网络容量限制和欠拟合。

    

    结构化神经网络剪枝旨在通过修剪对最终输出精度不重要的滤波器，从而去除深度卷积神经网络（CNNs）中的冗余通道。为了减少剪枝后性能的下降，许多方法利用稀疏正则化的损失来产生结构化稀疏性。本文分析了这些基于稀疏正则化训练的方法，并发现对未修剪通道进行正则化是不必要的，而且限制了网络的容量，导致欠拟合。为了解决这个问题，我们提出了一种新的剪枝方法，命名为MaskSparsity，采用了修剪感知的稀疏正则化。MaskSparsity仅对由修剪掩模选择的特定滤波器施加精细的稀疏正则化，而不是对模型的所有滤波器施加。在MaskSparity的精细稀疏正则化之前，我们可以使用许多方法来获取剪枝掩模，例如运行全局稀疏正则化。

    Structural neural network pruning aims to remove the redundant channels in the deep convolutional neural networks (CNNs) by pruning the filters of less importance to the final output accuracy. To reduce the degradation of performance after pruning, many methods utilize the loss with sparse regularization to produce structured sparsity. In this paper, we analyze these sparsity-training-based methods and find that the regularization of unpruned channels is unnecessary. Moreover, it restricts the network's capacity, which leads to under-fitting. To solve this problem, we propose a novel pruning method, named MaskSparsity, with pruning-aware sparse regularization. MaskSparsity imposes the fine-grained sparse regularization on the specific filters selected by a pruning mask, rather than all the filters of the model. Before the fine-grained sparse regularization of MaskSparity, we can use many methods to get the pruning mask, such as running the global sparse regularization. MaskSparsity ach
    
[^168]: ReLU'(0)对反向传播的数值影响研究

    Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.12915](http://arxiv.org/abs/2106.12915)

    本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。

    

    在理论上，神经网络中ReLU'(0)在[0, 1]范围内的选择对反向传播和训练几乎没有影响。然而，在现实世界中，32位默认精度结合深度学习问题的规模，使其成为训练方法的超参数。我们研究了ReLU'(0)值对几种精度水平（16位，32位，64位）、各种网络（全连接、VGG、ResNet）和数据集（MNIST、CIFAR10、SVHN）的重要性。我们观察到在32位精度下，反向传播输出出现了显著的变化，这种情况大约出现了一半的时间。这种影响在双精度下消失，而在16位精度下是系统性的。对于普通的SGD训练而言，选择ReLU'(0) = 0似乎是最有效的。我们还发现，批归一化或ADAM等重新调整方法 tend to buffer ReLU'(0)值的影响。总体而言，我们想要传达的信息是，非光滑问题的算法微分可能隐藏了一些参数。

    In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
    
[^169]: 生命周期策略重用与任务容量的重要性

    Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.01741](http://arxiv.org/abs/2106.01741)

    本文提出了生命周期策略重用算法和任务容量指标，通过优化一组近似最优策略和策略选择，实现了在终身强化学习中避免生成大量策略的目标。实验证明了生命周期策略重用和任务容量预选对多任务学习的重要性。

    

    人工智能领域一直存在一个挑战，即终身强化学习。在这种学习中，学习者按顺序接收多个任务，并在任务之间传递知识，同时避免灾难性遗忘。策略重用和其他多策略强化学习技术可以学习多个任务，但可能会生成大量的策略。本文提出了两个创新贡献，即1) 生命周期策略重用，这是一种模型无关的策略重用算法，通过策略优化和自适应策略选择的组合来优化一组近似最优策略，避免生成过多策略；2) 任务容量，一种衡量策略能准确解决的最大任务数量的指标。通过比较两种先进的基础学习模型，在一个包含18个部分可观察的Pacman任务和一个最多包含125个任务的Cartpole任务中，实验结果证明了生命周期策略重用和基于任务容量的预选的重要性。

    A long-standing challenge in artificial intelligence is lifelong reinforcement learning, where learners are given many tasks in sequence and must transfer knowledge between tasks while avoiding catastrophic forgetting. Policy reuse and other multi-policy reinforcement learning techniques can learn multiple tasks but may generate many policies. This paper presents two novel contributions, namely 1) Lifetime Policy Reuse, a model-agnostic policy reuse algorithm that avoids generating many policies by optimising a fixed number of near-optimal policies through a combination of policy optimisation and adaptive policy selection; and 2) the task capacity, a measure for the maximal number of tasks that a policy can accurately solve. Comparing two state-of-the-art base-learners, the results demonstrate the importance of Lifetime Policy Reuse and task capacity based pre-selection on an 18-task partially observable Pacman domain and a Cartpole domain of up to 125 tasks.
    
[^170]: 关于权重衰减的常被忽视的陷阱及其如何缓解：梯度范数视角

    On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective. (arXiv:2011.11152v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.11152](http://arxiv.org/abs/2011.11152)

    本论文发现了权重衰减在训练的最后阶段会导致大梯度范数的陷阱，为了解决这个问题，提出了Scheduled Weight Decay（SWD）方法，通过动态调整权重衰减强度并惩罚大梯度范数，可以有效缓解这个问题。

    

    权重衰减是一种简单而强大的正则化技术，在深度神经网络（DNN）的训练中被广泛使用。尽管权重衰减引起了很多关注，但先前的研究未能发现权重衰减导致的大梯度范数的一些被忽视的陷阱。在本文中，我们发现权重衰减不幸地会导致训练的最后阶段（或终止的解）出现大梯度范数，这通常表明了糟糕的收敛和差劲的泛化性能。为了缓解以梯度范数为中心的问题，我们提出了第一个实用的权重衰减调度器，称为Scheduled Weight Decay（SWD）方法，可以根据梯度范数动态调整权重衰减强度，并在训练过程中显著惩罚大梯度范数。我们的实验也证明，SWD确实缓解了大梯度范数问题，并且通常明显优于常规的恒定权重衰减策略（AMEN）。

    Weight decay is a simple yet powerful regularization technique that has been very widely used in training of deep neural networks (DNNs). While weight decay has attracted much attention, previous studies fail to discover some overlooked pitfalls on large gradient norms resulted by weight decay. In this paper, we discover that, weight decay can unfortunately lead to large gradient norms at the final phase (or the terminated solution) of training, which often indicates bad convergence and poor generalization. To mitigate the gradient-norm-centered pitfalls, we present the first practical scheduler for weight decay, called the Scheduled Weight Decay (SWD) method that can dynamically adjust the weight decay strength according to the gradient norm and significantly penalize large gradient norms during training. Our experiments also support that SWD indeed mitigates large gradient norms and often significantly outperforms the conventional constant weight decay strategy for Adaptive Moment Es
    

