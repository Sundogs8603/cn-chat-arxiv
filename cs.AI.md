# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition.](http://arxiv.org/abs/2309.14183) | Species196是一个包含196个类别的大规模半监督数据集，用于细粒度物种识别。它提供了四种实验设置，可以用于基准测试现有模型和算法的性能。 |
| [^2] | [ODE-based Recurrent Model-free Reinforcement Learning for POMDPs.](http://arxiv.org/abs/2309.14078) | 本论文提出了一种基于ODE的循环模型结合无模型强化学习来解决部分可观察的马尔可夫决策过程（POMDPs）。实验结果表明该方法在PO连续控制和元强化学习任务中表现出了良好的效果，并且对于不规则观察具有鲁棒性。 |
| [^3] | [Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World.](http://arxiv.org/abs/2309.10987) | 本文介绍了SpikingNeRF，它通过将辐射光线与脉冲神经网络的时间维度对齐，以节省能量并减少计算量。 |
| [^4] | [Data Formulator: AI-powered Concept-driven Visualization Authoring.](http://arxiv.org/abs/2309.10094) | Data Formulator是一种基于人工智能的可视化创作工具，采用概念绑定范式，通过将高级可视化意图和低级数据转换步骤分离，利用AI代理自动转换输入数据，并生成所需的可视化。 |
| [^5] | [BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture.](http://arxiv.org/abs/2309.08036) | 本文提出了一种新型简化的集合结构BEA用于锚点目标检测模型，并通过改进置信度得分的校准与降低不确定性误差的损失函数，提高了模型准确性。 |
| [^6] | [Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck.](http://arxiv.org/abs/2309.03800) | 本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。 |
| [^7] | [Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks.](http://arxiv.org/abs/2309.02460) | 本文介绍了一种新颖的多图神经网络模型DIAM，用于有效地检测加密货币交易网络上的非法账户。该模型通过自动学习节点表示并保留平行边的内在交易模式，在大型交易网络中取得了良好的效果。 |
| [^8] | [Stabilize to Act: Learning to Coordinate for Bimanual Manipulation.](http://arxiv.org/abs/2309.01087) | 通过借鉴人类的角色分配方法，我们提出了一种稳定行动的框架，其中一个手臂用于固定物体，另一个手臂用于执行任务。通过学习到的稳定分类器和行动策略，我们实现了这一框架并对其进行了评估。 |
| [^9] | [Iterative Multi-granular Image Editing using Diffusion Models.](http://arxiv.org/abs/2309.00613) | 本研究提出了一种名为EMILIE的迭代多粒度图像编辑器，通过重新利用预训练的扩散模型来实现迭代编辑，并引入梯度控制操作来实现对图像编辑的粒度控制。 |
| [^10] | [NAS-X: Neural Adaptive Smoothing via Twisting.](http://arxiv.org/abs/2308.14864) | NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。 |
| [^11] | [Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering.](http://arxiv.org/abs/2308.13259) | 本文提出了一个名为知识驱动的思维链（KD-CoT）的框架，用于验证和修改LLMs中的推理过程，通过与外部知识的交互来解决幻觉和错误传播的问题。 |
| [^12] | [An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM.](http://arxiv.org/abs/2308.06828) | 本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。 |
| [^13] | [Toward a Better Understanding of Loss Functions for Collaborative Filtering.](http://arxiv.org/abs/2308.06091) | 现有研究已经表明，通过改进对齐和均匀性设计的损失函数可以实现显著的性能提升。本文提出了一种新的损失函数，称为MAWU，它考虑了数据集的独特模式。 |
| [^14] | [Homophily-enhanced Structure Learning for Graph Clustering.](http://arxiv.org/abs/2308.05309) | 提出了一种名为HoLe的方法，通过在图结构中增强同类性可以显著改善图聚类任务的性能。 |
| [^15] | [The Ethics of AI Value Chains.](http://arxiv.org/abs/2307.16787) | 本文提出了AI价值链的概念，以满足AI伦理研究和干预的需求。通过综合评估AI价值链涉及的伦理问题，我们提出了四个未来方向，旨在推动更具伦理的AI发展和使用。 |
| [^16] | [Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control.](http://arxiv.org/abs/2307.12388) | 本文提出了UGAT方法，通过在模拟环境中动态转换具有不确定性的行动，实现了从模拟环境到真实环境的策略转移，显著提高了在真实世界中的性能。 |
| [^17] | [HIQL: Offline Goal-Conditioned RL with Latent States as Actions.](http://arxiv.org/abs/2307.11949) | 本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。 |
| [^18] | [A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media.](http://arxiv.org/abs/2307.06775) | 本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。 |
| [^19] | [Instruction Mining: High-Quality Instruction Data Selection for Large Language Models.](http://arxiv.org/abs/2307.06290) | 本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。 |
| [^20] | [Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling.](http://arxiv.org/abs/2307.05194) | 通过对数据生成过程和模型之间的$\beta$-分解进行后验采样，我们提出了$\beta$D-Bayes，一种能够实现差分机器学习的方法。 |
| [^21] | [Enhancing Adversarial Robustness via Score-Based Optimization.](http://arxiv.org/abs/2307.04333) | 本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。 |
| [^22] | [Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems.](http://arxiv.org/abs/2307.03913) | 本研究介绍了将人工智能与人类团队协作作为一种新的发展范式的方法，强调有效的人工智能与人类团队需要充分利用双方的独特能力，同时克服挑战和限制，提高联合表现。同时，该研究指出现有研究往往未考虑到动态、适应性和协作团队环境中人工智能的功能，呼吁加强关于人工智能与人类团队协作的研究。 |
| [^23] | [Optimal Learners for Realizable Regression: PAC Learning and Online Learning.](http://arxiv.org/abs/2307.03848) | 本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。 |
| [^24] | [Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning.](http://arxiv.org/abs/2307.03406) | 本论文研究了将决策制定视为离线收集的轨迹的监督学习问题，并探究了序列建模在轨迹压缩和策略学习中的作用。通过引入目标条件预测编码（GCPC），本文提出了一种能够产生强大轨迹表示并实现高性能策略的方法。 |
| [^25] | [FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy.](http://arxiv.org/abs/2307.01217) | 提出了一种名为FedCP的个性化联邦学习方法，通过生成条件策略分离特征中的全局信息和个性化信息，并分别进行处理。实验证明该方法在计算机视觉和自然语言处理领域的性能超过了十一种最先进的方法，最高可提高6.69%。 |
| [^26] | [An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography.](http://arxiv.org/abs/2307.00904) | 本研究开发了一个名为DeepGPET的开源全自动深度学习算法，用于光学相干断层扫描中脉络膜区域分割，并且在数据准确性和处理速度方面取得了显著的改进。 |
| [^27] | [A Neural Separation Algorithm for the Rounded Capacity Inequalities.](http://arxiv.org/abs/2306.17283) | 本论文提出了一种基于学习的分割启发式算法，使用图神经网络学习精确分割问题的解，通过嵌入切平面法找到了容量VRP的下限。 |
| [^28] | [Realistic Synthetic Financial Transactions for Anti-Money Laundering Models.](http://arxiv.org/abs/2306.16424) | 本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。 |
| [^29] | [Learning non-Markovian Decision-Making from State-only Sequences.](http://arxiv.org/abs/2306.15156) | 本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。 |
| [^30] | [Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses.](http://arxiv.org/abs/2306.13104) | 本研究提出了一种“人类介入的视觉前列腺深度刺激编码优化”的方法，通过反演前向模型、实时优化编码参数等手段，显著提高了感知质量。 |
| [^31] | [Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective.](http://arxiv.org/abs/2306.13092) | SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。 |
| [^32] | [Simplifying and Empowering Transformers for Large-Graph Representations.](http://arxiv.org/abs/2306.10759) | 本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。 |
| [^33] | [Residual Q-Learning: Offline and Online Policy Customization without Value.](http://arxiv.org/abs/2306.09526) | 该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。 |
| [^34] | [Large Language Model Is Semi-Parametric Reinforcement Learning Agent.](http://arxiv.org/abs/2306.07929) | 根据人类记忆和推理机制，提出了一种新的可演化LLM智能体框架REMEMBERER，通过为LLM装备长期经验记忆，可以为不同任务提供优异的智能体，其构成了半参数RL代理。成功率超过先前SOTA 4％和2％。 |
| [^35] | [A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation.](http://arxiv.org/abs/2306.07304) | 本文提出了一个全面的理论框架，来统一定义和澄清自动概念提取和概念重要性评估，进而提供新的评估指标以实现对这些方法的比较以及推导关于这种方法的最优性的理论保证。 |
| [^36] | [Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization.](http://arxiv.org/abs/2306.06805) | 提出了一种名为MACO的简单方法，通过优化相位谱生成图像，同时保持幅度恒定，解决了特征可视化在深度神经网络上的挑战，并实现了高质量和高效的可解释特征可视化。 |
| [^37] | [ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer.](http://arxiv.org/abs/2306.06446) | ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。 |
| [^38] | [Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models.](http://arxiv.org/abs/2306.06253) | 决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。 |
| [^39] | [Prodigy: An Expeditiously Adaptive Parameter-Free Learner.](http://arxiv.org/abs/2306.06101) | 本文提出了一种基于自适应算法(如Adagrad和Adam)的学习率估计方法Prodigy和Resetting，可以快速且正确地估计到达解决方案所需的距离D，从而提高了模型的收敛速度，并在多个数据集和模型上进行了测试，实验表明该方法优于D-Adaptation并可达到手动调整Adam的测试准确度值。 |
| [^40] | [Bayesian Optimisation of Functions on Graphs.](http://arxiv.org/abs/2306.05304) | 本论文提出了一种在通用的大规模和潜在未知图上定义函数的贝叶斯优化算法，并通过学习适当的图内核，适应目标函数行为。 |
| [^41] | [Factorized Contrastive Learning: Going Beyond Multi-view Redundancy.](http://arxiv.org/abs/2306.05268) | 本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。 |
| [^42] | [Autonomous Capability Assessment of Black-Box Sequential Decision-Making Systems.](http://arxiv.org/abs/2306.04806) | 本论文提出了一种新方法，可以有效地评估黑盒SDM系统的能力，该方法使用主动学习来建立一个可解释的概率模型，能够准确地描述其能力和在随机环境中执行这些能力的可能效果和要求。 |
| [^43] | [Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks.](http://arxiv.org/abs/2306.04251) | SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。 |
| [^44] | [State Regularized Policy Optimization on Data with Dynamics Shift.](http://arxiv.org/abs/2306.03552) | 本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。 |
| [^45] | [Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity.](http://arxiv.org/abs/2306.02652) | 本文提出了一种在Early-Exit网络中实现条件单调性的方法，将深度模型转化为真正的随时分类器。 |
| [^46] | [Nonparametric Identifiability of Causal Representations from Unknown Interventions.](http://arxiv.org/abs/2306.00542) | 本文提出了一种新方法用于从未知干预数据中推断非参数因果表达式学习，并且证明了在两个因果变量的基本设置中，无法消除一些由干预数据引起的歧义问题。 |
| [^47] | [Addressing Negative Transfer in Diffusion Models.](http://arxiv.org/abs/2306.00354) | 本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。 |
| [^48] | [Approximate inference of marginals using the IBIA framework.](http://arxiv.org/abs/2306.00335) | 提出了一种新的基于IBIA框架的算法，通过将PGM转化为有界团大小的SLCTF序列，并使用启发式置信度更新算法来推导边缘。实验结果表明，该算法比当前最先进的技术更加有效和高效。 |
| [^49] | [Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images.](http://arxiv.org/abs/2306.00219) | 本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。 |
| [^50] | [Diffused Redundancy in Pre-trained Representations.](http://arxiv.org/abs/2306.00183) | 本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。 |
| [^51] | [Latent Exploration for Reinforcement Learning.](http://arxiv.org/abs/2305.20065) | 本文提出了一种名为Lattice的方法，通过向策略网络的潜在状态中注入时间相关性噪声，来解决强化学习中多作用器系统存在的探索问题。 |
| [^52] | [Replicability in Reinforcement Learning.](http://arxiv.org/abs/2305.19562) | 这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。 |
| [^53] | [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models.](http://arxiv.org/abs/2305.19308) | 本研究提出了一个基于大型语言模型（LLMs）的代理程序SheetCopilot，该程序可以通过自然语言指导软件执行电子表格数据处理等任务。该程序设计了一组抽象的电子表格软件功能原子动作以及基于状态机的任务规划框架，实现了LLMs与电子表格的鲁棒交互，可以单次正确完成44.3％的任务。 |
| [^54] | [NetHack is Hard to Hack.](http://arxiv.org/abs/2305.19240) | 本文研究了神经策略学习在NetHack中的性能差距，并分析了获胜的符号代理，提出了动作层级的优势、神经架构的改进以及强化学习与模仿学习的整合等方面可能提升性能的方法。 |
| [^55] | [On the impact of activation and normalization in obtaining isometric embeddings at initialization.](http://arxiv.org/abs/2305.18399) | 本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。 |
| [^56] | [Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks.](http://arxiv.org/abs/2305.18395) | 本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。 |
| [^57] | [Beyond Confidence: Reliable Models Should Also Consider Atypicality.](http://arxiv.org/abs/2305.18262) | 研究发现，模型的可靠性不能仅仅依靠置信度，还应考虑预测样本或类别的非典型性，特别是对于非典型输入或类别，模型预测更过于自信且准确性较低。利用这些发现，将非典型性纳入模型可以提高不确定性量化和性能。 |
| [^58] | [Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making.](http://arxiv.org/abs/2305.17588) | 该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。 |
| [^59] | [Auditing Fairness by Betting.](http://arxiv.org/abs/2305.17570) | 本文提供了一种通过赌博的方式进行公平性审计的方法，相比之前的方法，这种方法具有更高的实用性和效率，能够对不断产生的数据进行连续的监控，并处理因分布漂移导致的公平性问题。 |
| [^60] | [Causal Component Analysis.](http://arxiv.org/abs/2305.17225) | 本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。 |
| [^61] | [Inferring the Future by Imagining the Past.](http://arxiv.org/abs/2305.17195) | 本文介绍了一种蒙特卡罗算法用于从情报主体的单张图像中推断出一连串复杂的历史和未来事件，并且能够在只有少数样本的情况下扩展到各种具有挑战性的推断问题。 |
| [^62] | [Three Towers: Flexible Contrastive Learning with Pretrained Image Models.](http://arxiv.org/abs/2305.16999) | 本文提出了 3T 方法，即在视觉语言模型中引入预训练的图像分类器，从而提高对比学习的灵活性。3T 同时受益于预训练嵌入和对比训练，并在实验证明对检索任务和分类问题均取得了有竞争力的性能。 |
| [^63] | [Training Socially Aligned Language Models in Simulated Human Society.](http://arxiv.org/abs/2305.16960) | 本研究提出了一种在模拟人类社会中训练语言模型的新方法，相比于现有方法，该方法具有更大的可扩展性和高效性，并在对齐基准和人类评估中展示出更优异的性能。 |
| [^64] | [Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer.](http://arxiv.org/abs/2305.16380) | 本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。 |
| [^65] | [Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation.](http://arxiv.org/abs/2305.16289) | 通过自动语言引导图像编辑，利用大型视觉和语言模型扩充训练数据集的方法，以增强细粒度分类任务中面对领域变化的模型的泛化能力。 |
| [^66] | [How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits.](http://arxiv.org/abs/2305.15944) | 本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。 |
| [^67] | [LayoutGPT: Compositional Visual Planning and Generation with Large Language Models.](http://arxiv.org/abs/2305.15393) | LayoutGPT是一种使用大型语言模型的视觉规划和生成方法，通过从文本条件生成布局来提高用户对生成结果的控制能力。LayoutGPT在多个领域生成合理的布局，并优于其他文本到图像模型/系统的性能。 |
| [^68] | [A Mini Review on the utilization of Reinforcement Learning with OPC UA.](http://arxiv.org/abs/2305.15113) | 这项工作旨在通过提供关于强化学习和OPC UA的技术概述和综述，来解决将强化学习无缝集成到工业系统中的问题。 |
| [^69] | [A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2305.14649) | 本文提出了一种称为JTFT的方法，可以在提取时间依赖性的同时有效地减小计算需求，适用于多元时间序列预测，并具有线性复杂度。同时，采用低秩注意层以有效捕获跨维度的依赖关系，并在六个真实世界数据集上展示了比现有方法更好的表现。 |
| [^70] | [Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics.](http://arxiv.org/abs/2305.14286) | 本研究提出了一种称为EPNS的等变概率神经模拟框架，可以在系统演化中生成等变分布，并在随机时空动态方面表现出色。 |
| [^71] | [Hierarchical Prompting Assists Large Language Model on Web Navigation.](http://arxiv.org/abs/2305.14257) | 这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。 |
| [^72] | [Language Models with Rationality.](http://arxiv.org/abs/2305.14250) | 本研究提出了一种名为REFLEX的方法，在大型语言模型上添加了合理性和自反性层，以使模型的答案得以解释并消除潜在的矛盾。 |
| [^73] | [Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization.](http://arxiv.org/abs/2305.14152) | 本文提出了一种称为PEQA的方法，结合了参数高效微调和量化LLM的优点。通过仅更新量化尺度，PEQA可以高效地微调压缩大型语言模型，并减少内存开销。 |
| [^74] | [LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4.](http://arxiv.org/abs/2305.12147) | 该论文提出了LogiCoT, 一个基于GPT-4的逻辑思维指令调整数据集，用于教授模型逻辑推理和引出一般推理技能。 |
| [^75] | [STOAT: Structured Data to Analytical Text With Controls.](http://arxiv.org/abs/2305.11826) | STOAT模型是表格和推理意识的生成模型，在数字推理、常识推理、时间推理、表格知识和实体知识方面有较好的控制，提高了分析句子生成的质量和准确度。 |
| [^76] | [Language Models Meet World Models: Embodied Experiences Enhance Language Models.](http://arxiv.org/abs/2305.10626) | 本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。 |
| [^77] | [Personality Understanding of Fictional Characters during Book Reading.](http://arxiv.org/abs/2305.10156) | 本文提出了一个NLP领域内尚未研究的问题：情景和细致地理解小说人物个性，并提供了第一个标记数据集PersoNet来解决这个问题。 |
| [^78] | [Can Language Models Solve Graph Problems in Natural Language?.](http://arxiv.org/abs/2305.10037) | 本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。 |
| [^79] | [RelaMiX: Exploring Few-Shot Adaptation in Video-based Action Recognition.](http://arxiv.org/abs/2305.08420) | 本文研究了基于视频的动作识别中少样本自适应的问题，并提出了FSDA-AR方法，通过利用少量标记的目标视频实现有效的自适应。实验证明FSDA-AR与无监督领域自适应的性能相当。 |
| [^80] | [Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques.](http://arxiv.org/abs/2305.07116) | 本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。 |
| [^81] | [Information Design in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.06807) | 本文探究了多智能体强化学习中的信息设计问题及其挑战，提出了“马尔科夫信令博弈”的概念。 |
| [^82] | [The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation.](http://arxiv.org/abs/2305.06156) | The Vault是一个提供了10种流行编程语言的40百万行代码-文本对的开源数据集，旨在增强面向代码的大型语言模型（LLM）的训练，有望在代码理解和生成任务上取得显著进展。 |
| [^83] | [Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.05803) | 本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。 |
| [^84] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^85] | [Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment.](http://arxiv.org/abs/2305.03510) | 本文提出了一个通过翻译对齐的方式实现参数高效、跨语言的迁移学习框架，实验结果显示该框架能够显著减少多语言之间的性能差异。 |
| [^86] | [Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services.](http://arxiv.org/abs/2305.02109) | 本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。 |
| [^87] | [Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing.](http://arxiv.org/abs/2304.08315) | 本文调查了自然语言处理（NLP）领域的双重使用问题，提出了一份定制的双重使用定义，并讨论了当前的状况和可能的挑战。 |
| [^88] | [Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method.](http://arxiv.org/abs/2304.07056) | 本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。 |
| [^89] | [Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models.](http://arxiv.org/abs/2304.03271) | 本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。 |
| [^90] | [MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities.](http://arxiv.org/abs/2304.01969) | MEGClass是一种通过相互增强的文本粒度实现极弱监督文本分类的方法，利用分层关注机制从文档、句子和单词中提取连贯且多样的子文本，能够准确分类讨论多个主题的文档，并且在五个基准数据集上表现优于现有最先进的模型。 |
| [^91] | [Pgx: Hardware-accelerated parallel game simulation for reinforcement learning.](http://arxiv.org/abs/2303.17503) | Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。 |
| [^92] | [Black-box Backdoor Defense via Zero-shot Image Purification.](http://arxiv.org/abs/2303.12175) | 本文提出了一种黑盒后门防御框架，利用零样本图像净化有效地防御各种攻击，无需任何内部信息或先前知识，通过对受污染的图像进行线性变换和预训练的扩散模型恢复缺失语义信息实现。 |
| [^93] | [A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games.](http://arxiv.org/abs/2303.09716) | 本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。 |
| [^94] | [Transformer-based Planning for Symbolic Regression.](http://arxiv.org/abs/2303.06833) | 该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。 |
| [^95] | [Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning.](http://arxiv.org/abs/2303.05479) | 本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。 |
| [^96] | [Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction.](http://arxiv.org/abs/2303.04132) | 本文展示了利用不对称性进行合成训练数据生成的方法，可以针对结构化输出问题产生大规模、高质量的数据。在封闭信息提取任务中，我们合成生成了一个包含180万数据点的数据集，并利用此数据集对小型模型进行微调，取得了远超先前领先技术的成果。 |
| [^97] | [Learning to Influence Human Behavior with Offline Reinforcement Learning.](http://arxiv.org/abs/2303.02265) | 本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。 |
| [^98] | [SHAP-IQ: Unified Approximation of any-order Shapley Interactions.](http://arxiv.org/abs/2303.01179) | 提出了一种名为SHAP-IQ的新方法，用于计算任意阶Shapley互动，并提供了逼近质量的理论保证和方差估计。该方法在计算成本和逼近质量方面优于现有方法。 |
| [^99] | [Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework.](http://arxiv.org/abs/2302.12247) | 通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。 |
| [^100] | [Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management.](http://arxiv.org/abs/2302.10850) | 本论文研究了离线强化学习在混合专家对话管理中的应用。通过利用最新的混合专家语言模型（MoE-LMs），我们开发了针对对话规划的强化学习算法，显著减少了动作空间的大小，并提高了对话管理的有效性。 |
| [^101] | [Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels.](http://arxiv.org/abs/2302.05666) | 本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。 |
| [^102] | [Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.](http://arxiv.org/abs/2302.01560) | 本论文研究了在开放世界环境中多任务具身智能体的任务规划挑战，提出了一种基于大型语言模型的交互式规划方法(DEPS)来解决计划执行的准确性和效率问题。 |
| [^103] | [The geometry of hidden representations of large transformer models.](http://arxiv.org/abs/2302.00294) | 大型Transformer模型中的隐藏表示具有类似的几何和统计特性，随着层级的移动，它们在最初的几层中变得高维，然后在中间层中显著收缩，在模型的最后部分，保持恒定或形成第二个浅峰。在第一个峰值结束时，数据集的语义信息被更好地表达。 |
| [^104] | [Alignment with human representations supports robust few-shot learning.](http://arxiv.org/abs/2301.11990) | 论文提出少样本学习的表现与人类表征的一致性存在U形关系，并通过计算机视觉模型的实验进行了验证。高度对齐的模型更加鲁棒，对数据的利用更加有效，但与人类对齐并非必要条件。 |
| [^105] | [Characterization and Learning of Causal Graphs with Small Conditioning Sets.](http://arxiv.org/abs/2301.09028) | 本研究提出了一种使用小的条件集来表征和学习因果图的方法，用于解决约束性因果发现算法在数据有限和条件集较大时的困难。我们定义了k-马尔可夫等价的概念，该概念在不能利用所有条件独立性语句时仍然适用。 |
| [^106] | [On the Sensitivity of Reward Inference to Misspecified Human Models.](http://arxiv.org/abs/2212.04717) | 本研究从理论和实证角度研究了奖励推断对错误人类模型的敏感性，并发现存在可能构建小的对抗性偏差的情况。 |
| [^107] | [Confidence-Conditioned Value Functions for Offline Reinforcement Learning.](http://arxiv.org/abs/2212.04607) | 本文提出了一种条件值函数的学习方法，该方法在离线强化学习中处理了数据集和学习策略之间的分布偏移，并可以在训练和评估时根据保守程度动态选择策略。 |
| [^108] | [A Visual Active Search Framework for Geospatial Exploration.](http://arxiv.org/abs/2211.15788) | 本论文提出了基于视觉主动搜索框架的强化学习方法，以协助地理空间探索问题。该框架以广阔区域的图像为输入，通过有限的查询来验证目标对象示例的存在，并提供关于目标对象空间分布的信息。 |
| [^109] | [Towards Improved Input Masking for Convolutional Neural Networks.](http://arxiv.org/abs/2211.14646) | 本论文提出了一种改进的卷积神经网络的输入遮罩方法，通过层遮罩能够有效减少遮罩引起的缺失偏差，并消除或最小化了遮罩对模型预测的影响。 |
| [^110] | [Perfectly Secure Steganography Using Minimum Entropy Coupling.](http://arxiv.org/abs/2210.14889) | 本文研究了完全安全的隐写术，并发现只有最小熵耦合过程才是最高效的，提出了高可伸缩性的完美安全隐写算法，具有实际应用价值。 |
| [^111] | [Transformers over Directed Acyclic Graphs.](http://arxiv.org/abs/2210.13148) | 本文研究了在有向无环图上的Transformer。通过改进注意机制和位置编码，本方法在多种任务上表现出了很好的效果。 |
| [^112] | [Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks.](http://arxiv.org/abs/2210.12974) | 该论文提出了研究异构神经网络中神经干扰现象的理论分析和实验验证，并提出了一种通过自适应选择本地模型来执行预测的方法。实验结果表明，该方法在数据异质性方面更加鲁棒。 |
| [^113] | [On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?.](http://arxiv.org/abs/2210.12770) | 本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。 |
| [^114] | [Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition.](http://arxiv.org/abs/2210.09943) | 通过进行公平性的神经架构搜索，我们发现偏见是神经网络架构本身固有的，而不仅仅是训练数据的影响。我们的方法在人脸识别等难题上取得了比其他方法更好的准确性和公平性。 |
| [^115] | [ConSpec: honing in on critical steps for rapid learning and generalization in RL.](http://arxiv.org/abs/2210.05845) | ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。 |
| [^116] | [A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning.](http://arxiv.org/abs/2210.05448) | 本文提出了一个通用的学习框架，用于解决开放式即席团队合作问题，通过基于图的策略学习获得最优策略，并且能够适应不完全可观测情况下的团队合作。 |
| [^117] | [SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models.](http://arxiv.org/abs/2210.04802) | 本文是第一个系统研究超分布泛化问题的方法，通过模拟不同维度的源代码数据属性和微调方法，在不同场景中研究了模型的行为。 |
| [^118] | [Are All Steps Equally Important? Benchmarking Essentiality Detection of Events.](http://arxiv.org/abs/2210.04074) | 本文研究了当前模型在理解与目标事件有关的步骤事件的重要性方面的困难，并贡献了一个由专家手动注释步骤重要性的语料库。 |
| [^119] | [Learnware: Small Models Do Big.](http://arxiv.org/abs/2210.03647) | Learnware范式旨在帮助用户充分利用小型模型，实现超越原始目的的事情，以解决当前机器学习技术面临的数据量大、训练难度高、数据安全等问题。 |
| [^120] | [Hyperbolic VAE via Latent Gaussian Distributions.](http://arxiv.org/abs/2209.15217) | 这项研究提出了一种通过使用高斯流形的潜空间来改进变分自编码器(VAE)的方法。实验证明，这种GM-VAE方法在密度估计和模型驱动的强化学习任务中表现出色，具有较强的数值稳定性。 |
| [^121] | [Language models show human-like content effects on reasoning tasks.](http://arxiv.org/abs/2207.07051) | 本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。 |
| [^122] | [What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization.](http://arxiv.org/abs/2207.05161) | 本论文提出了一种分类框架，用于对不确定性量化方法标记的不确定示例进行分类。通过引入混淆密度矩阵，并将示例分为分布外、边界和高分布误分类区域的三类，本研究为评估不同的不确定性量化方法提供了新的视角和评估基准。 |
| [^123] | [Fairness and Bias in Robot Learning.](http://arxiv.org/abs/2207.03444) | 本文从技术、伦理和法律角度出发，首次调查了机器人学习中的公平性问题。提出了偏见来源的分类法和由其引起的歧视类型，并探讨了不同机器人学习领域中不公平结果的场景和缓解策略。 |
| [^124] | [Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE).](http://arxiv.org/abs/2206.14261) | SCOPE 提出了一种名为半监督对比异常值剔除的伪最大期望化方法，通过抑制混淆错误来提高半监督学习的性能。 |
| [^125] | [Interactive Visual Reasoning under Uncertainty.](http://arxiv.org/abs/2206.09203) | 该论文介绍了一个名为IVRE的交互式环境，在不确定性下评估人工智能代理的推理能力。研究人员通过在IVRE中设置不确定的动作-效果对，要求代理确定对象的角色，并鼓励代理基于观察提出有效且高效的实验来验证假设。 |
| [^126] | [Evaluating and Inducing Personality in Pre-trained Language Models.](http://arxiv.org/abs/2206.07550) | 本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。 |
| [^127] | [Complex Locomotion Skill Learning via Differentiable Physics.](http://arxiv.org/abs/2206.02341) | 本研究提出了一个实用的学习框架，通过可微分物理实现了统一的神经网络控制器，使其具备更高复杂性和多样性。与强化学习相比，该框架的学习效果更好且收敛速度更快。 |
| [^128] | [Augmentation-Aware Self-Supervision for Data-Efficient GAN Training.](http://arxiv.org/abs/2205.15677) | 本文提出一种增强感知的自监督判别器用于对生成数据及其增强参数的预测，从而提高判别器的表现和生成模型的性能，实现了数据有效的 GAN 训练。 |
| [^129] | [Treatment Learning Causal Transformer for Noisy Image Classification.](http://arxiv.org/abs/2203.15529) | 本研究提出了一种处理学习因果Transformer（TLT）架构，用于噪声图像分类。通过将"存在噪声"的信息作为处理输入，并联合估计处理效果，TLT能够提高预测准确性，并使用潜在生成模型估计鲁棒的特征表示。 |
| [^130] | [Improving Molecular Representation Learning with Metric Learning-enhanced Optimal Transport.](http://arxiv.org/abs/2202.06208) | 本文提出了一种基于最优传输的算法MROT，用于改进分子表示学习和增强其泛化能力。实验证明，MROT在化学性质预测和材料吸附选择等任务中显著优于现有模型，具有加速发现具有期望性质的新物质的潜力。 |
| [^131] | [CubeTR: Learning to Solve The Rubiks Cube Using Transformers.](http://arxiv.org/abs/2111.06036) | CubeTR提出了一种使用Transformer进行强化学习解决魔方问题的方法，并通过关注长序列动作和解决稀疏奖励的问题，实现了从任意起始状态学习如何解决魔方问题，并能够生成接近专业人员解决方案长度的解决方案。 |
| [^132] | [Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification.](http://arxiv.org/abs/2110.03894) | 本文提出了一种基于相似性映射的神经模型重编程方法，用于低资源口语命令分类。实验证明，在有限的数据条件下，该方法在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果。 |
| [^133] | [A Survey of Knowledge Enhanced Pre-trained Models.](http://arxiv.org/abs/2110.00269) | 本综述提供了关于NLP中知识增强预训练语言模型的综合概述，讨论了预训练语言模型和知识表示学习的进展，并从三个不同的角度对现有的KEPLMs进行了分类，最后概述了未来研究中KEPLMs的潜在方向。 |
| [^134] | [FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging.](http://arxiv.org/abs/2109.09658) | 本论文介绍了一系列从经验、共识和最佳实践中提炼出的指导原则，旨在引领医学影像中值得信赖的人工智能的发展，提高信任、安全性和应用水平。 |
| [^135] | [On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization.](http://arxiv.org/abs/2106.02835) | 发现因果结构的任务是重要且具有挑战性的，在连续优化中使用的最小二乘损失函数受限于高斯噪声假设。在本研究中，我们提出了一种理论上与任何噪声分布一致的基于熵的损失函数来克服这一限制。 |
| [^136] | [Evolving parametrized Loss for Image Classification Learning on Small Datasets.](http://arxiv.org/abs/2103.08249) | 本文提出了一种在小数据集上进行图像分类学习的方法，通过元学习的方式进化参数化损失函数，实验结果表明该方法能够有效地提高分类性能。 |
| [^137] | [Continual Developmental Neurosimulation Using Embodied Computational Agents.](http://arxiv.org/abs/2103.05753) | 通过使用发育Braitenberg Vehicles代理，我们提出了一种具有发育启发的学习代理设计，实现了计算自主性的具身经验和形态发生增长模拟，并考虑了发育轨迹在神经系统形态生成、发育学习和可塑性等方面的作用。 |
| [^138] | [Efficient Learning of Control Policies for Robust Quadruped Bounding using Pretrained Neural Networks.](http://arxiv.org/abs/2011.00446) | 本文提出了一种高效学习鲁棒四足跳跃控制策略的方法，通过使用预训练的神经网络和深度强化学习，考虑接触点和阶段的奖励函数以提升跳跃性能，并成功地应用于真实的四足机器人上。 |

# 详细

[^1]: Species196：一百万半监督数据集用于细粒度物种识别

    Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition. (arXiv:2309.14183v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.14183](http://arxiv.org/abs/2309.14183)

    Species196是一个包含196个类别的大规模半监督数据集，用于细粒度物种识别。它提供了四种实验设置，可以用于基准测试现有模型和算法的性能。

    

    基于基础视觉模型的发展，普通视觉识别已经达到了一个很高的水平，但是不能很好地处理专门领域中的细粒度识别，比如入侵物种分类。识别和管理入侵物种具有很强的社会和生态价值。目前，大多数入侵物种数据集的规模有限，覆盖的物种范围狭窄，这限制了基于深度学习的入侵生物计量系统的发展。为了填补这个领域的空白，我们引入了Species196，一个包含196个类别的大规模半监督数据集。它收集了超过19K带有专家级准确注释的图像Species196-L，以及120万张未标记的入侵物种图像Species196-U。该数据集提供了四种实验设置，用于基准测试现有的模型和算法，分别是监督学习、半监督学习、自监督预训练和大型多模态模型的零样本推理能力。

    The development of foundation vision models has pushed the general visual recognition to a high level, but cannot well address the fine-grained recognition in specialized domain such as invasive species classification. Identifying and managing invasive species has strong social and ecological value. Currently, most invasive species datasets are limited in scale and cover a narrow range of species, which restricts the development of deep-learning based invasion biometrics systems. To fill the gap of this area, we introduced Species196, a large-scale semi-supervised dataset of 196-category invasive species. It collects over 19K images with expert-level accurate annotations Species196-L, and 1.2M unlabeled images of invasive species Species196-U. The dataset provides four experimental settings for benchmarking the existing models and algorithms, namely, supervised learning, semi-supervised learning, self-supervised pretraining and zero-shot inference ability of large multi-modal models. T
    
[^2]: 基于ODE的无模型反馈强化学习用于POMDPs

    ODE-based Recurrent Model-free Reinforcement Learning for POMDPs. (arXiv:2309.14078v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14078](http://arxiv.org/abs/2309.14078)

    本论文提出了一种基于ODE的循环模型结合无模型强化学习来解决部分可观察的马尔可夫决策过程（POMDPs）。实验结果表明该方法在PO连续控制和元强化学习任务中表现出了良好的效果，并且对于不规则观察具有鲁棒性。

    

    神经常微分方程（ODEs）被广泛认为是建模物理机制的标准，它们有助于在未知的物理或生物环境中进行近似推断。在部分可观测（PO）环境中，如何从原始观测中推断看不见的信息困扰着代理人。通过使用具有紧凑上下文的循环策略，基于上下文的强化学习提供了一种灵活的方法来从历史转换中提取不可观察的信息。为了帮助代理人提取更多与动态相关的信息，我们提出了一种新颖的基于ODE的循环模型，并结合了无模型的强化学习框架来解决部分可观察的马尔可夫决策过程（POMDPs）。我们通过各种PO连续控制和元强化学习任务实验验证了我们方法的有效性。此外，我们的实验表明，由于ODEs具有建模不规则性的能力，我们的方法对于不规则观察具有鲁棒性。

    Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularl
    
[^3]: Spiking NeRF：使生物启发的神经网络穿透现实世界

    Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])

    [http://arxiv.org/abs/2309.10987](http://arxiv.org/abs/2309.10987)

    本文介绍了SpikingNeRF，它通过将辐射光线与脉冲神经网络的时间维度对齐，以节省能量并减少计算量。

    

    脉冲神经网络（SNN）在许多任务中取得了成功，利用其具有潜在生物学可行性的能量效率和潜力。与此同时，神经辐射场（NeRF）以大量能量消耗渲染高质量的3D场景，但很少有研究深入探索以生物启发的方法进行节能解决方案。本文提出了脉冲NeRF（SpikingNeRF），将辐射光线与SNN的时间维度对齐，以自然地适应SNN对辐射场的重建。因此，计算以基于脉冲、无乘法的方式进行，从而减少能量消耗。在SpikingNeRF中，光线上的每个采样点匹配到特定的时间步，并以混合方式表示，其中体素网格也得到维护。基于体素网格，确定采样点是否在训练和推断过程中被屏蔽以进行更好的处理。然而，这个操作也会产生不可逆性。

    Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
    
[^4]: 数据公式化: 基于人工智能驱动的概念驱动可视化创作

    Data Formulator: AI-powered Concept-driven Visualization Authoring. (arXiv:2309.10094v1 [cs.HC])

    [http://arxiv.org/abs/2309.10094](http://arxiv.org/abs/2309.10094)

    Data Formulator是一种基于人工智能的可视化创作工具，采用概念绑定范式，通过将高级可视化意图和低级数据转换步骤分离，利用AI代理自动转换输入数据，并生成所需的可视化。

    

    大多数现代可视化工具需要作者将其数据转换为整洁格式以创建所需的可视化。由于这需要具备编程或单独的数据处理工具的经验，数据转换仍然是可视化创作的一个障碍。为了解决这一挑战，我们提出了一种新的可视化范式，即概念绑定，它将高级可视化意图和低级数据转换步骤分开，并利用人工智能代理来实现。我们在Data Formulator中实现了这个范式，这是一个交互式的可视化创作工具。使用Data Formulator，作者首先使用自然语言或示例定义他们计划可视化的数据概念，然后将它们绑定到可视通道。Data Formulator然后将其人工智能代理派遣出去，自动将输入数据转换为呈现这些概念和生成所需可视化的数据。在向Data Formulator展示来自人工智能代理的结果(转换后的表格和输出的可视化)时，Data Formulator提供反馈。

    With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback
    
[^5]: BEA: 重新审视使用Budding Ensemble Architecture的基于锚点的目标检测DNN

    BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v1 [cs.CV])

    [http://arxiv.org/abs/2309.08036](http://arxiv.org/abs/2309.08036)

    本文提出了一种新型简化的集合结构BEA用于锚点目标检测模型，并通过改进置信度得分的校准与降低不确定性误差的损失函数，提高了模型准确性。

    

    本文介绍了Budding Ensemble Architecture（BEA），一种用于锚点目标检测模型的新型简化集合结构。目标检测模型在基于视觉的任务中非常重要，特别是在自主系统中。它们应该提供精确的边界框检测，并校准其预测的置信度得分，以获得更高质量的不确定性估计。然而，由于假阳性接收到高分或真阳性由于低分而被丢弃，当前的模型可能会做出错误的决策。BEA旨在解决这些问题。BEA的提出的损失函数改善了置信度得分的校准和降低了不确定性误差，从而更好地区分真阳性和假阳性，最终提高了目标检测模型的准确性。使用BEA方法和其提出的损失函数，对Base-YOLOv3和SSD模型进行了改进。BEA在KITTI数据集上训练的Base-YOLOv3结果中，精度分别提高了6%和3.7%。

    This paper introduces the Budding Ensemble Architecture (BEA), a novel reduced ensemble architecture for anchor-based object detection models. Object detection models are crucial in vision-based tasks, particularly in autonomous systems. They should provide precise bounding box detections while also calibrating their predicted confidence scores, leading to higher-quality uncertainty estimates. However, current models may make erroneous decisions due to false positives receiving high scores or true positives being discarded due to low scores. BEA aims to address these issues. The proposed loss functions in BEA improve the confidence score calibration and lower the uncertainty error, which results in a better distinction of true and false positives and, eventually, higher accuracy of the object detection models. Both Base-YOLOv3 and SSD models were enhanced using the BEA method and its proposed loss functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6% and 3.7% i
    
[^6]: 神经特征学习中的帕累托前沿：数据、计算、宽度和运气

    Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])

    [http://arxiv.org/abs/2309.03800](http://arxiv.org/abs/2309.03800)

    本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。

    

    本研究探讨了在计算统计差距存在的情况下，深度学习中微妙的算法设计选择。我们首先考虑了离线稀疏奇偶学习，这是一个有关多层感知器梯度训练的监督分类问题，其具有统计查询下界。这个下界可以解释为多资源的权衡前沿：成功学习只有在一个足够丰富（大型模型）、知识渊博（大规模数据集）、耐心（训练迭代次数多）或幸运（随机猜测次数多）的情况下才能发生。我们通过理论和实验表明，在这种情况下，稀疏初始化和增加网络宽度可以显著提高样本效率。在这里，宽度起到了并行搜索的作用：它增加了找到“幸运神经元”的概率，这些神经元可以更高效地学习稀疏特征。最后，我们表明合成稀疏奇偶任务可以作为真实问题的代理。

    This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
    
[^7]: 有效的多图神经网络用于加密货币交易网络上的非法账户检测

    Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])

    [http://arxiv.org/abs/2309.02460](http://arxiv.org/abs/2309.02460)

    本文介绍了一种新颖的多图神经网络模型DIAM，用于有效地检测加密货币交易网络上的非法账户。该模型通过自动学习节点表示并保留平行边的内在交易模式，在大型交易网络中取得了良好的效果。

    

    我们研究了在线金融市场中日益重要的加密货币交易网络上的非法账户检测。在加密货币上的非法活动激增导致了普通用户数十亿的损失。现有的解决方案要么依赖于繁琐的特征工程来获得手工特征，要么不能充分利用加密货币交易数据中丰富的语义信息，从而导致亚优化的性能。在本文中，我们将非法账户检测问题定义为带有边属性的有向多图上的分类任务，并提出了DIAM，一种新颖的多图神经网络模型，用于在大型交易网络上有效地检测非法账户。首先，DIAM包含一个Edge2Seq模块，通过同时考虑边属性和有向边序列依赖关系，自动学习有效的节点表示，保留平行边的内在交易模式。然后利用t

    We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
    
[^8]: 稳定行动：学习协调双手操作的方法

    Stabilize to Act: Learning to Coordinate for Bimanual Manipulation. (arXiv:2309.01087v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.01087](http://arxiv.org/abs/2309.01087)

    通过借鉴人类的角色分配方法，我们提出了一种稳定行动的框架，其中一个手臂用于固定物体，另一个手臂用于执行任务。通过学习到的稳定分类器和行动策略，我们实现了这一框架并对其进行了评估。

    

    在真实世界中进行丰富和灵巧的操作的关键是能够协调控制两只手。然而，虽然双手机器人系统带来了巨大的前景，但构建双臂自主系统的控制策略却面临难题。其中一个困难是双手动作空间的高维度，这给基于模型和数据驱动的方法增加了复杂性。我们从人类身上得到启发，提出了一种新的角色分配框架来应对这一挑战：一个稳定的臂将物体固定在一个位置上，简化环境，而一个活动的臂则执行任务。我们利用学习到的稳定分类器来实施 BimanUal Dexterity from Stabilization (BUDS) 框架，该分类器交替更新学习到的稳定位置以保持环境稳定，并利用示范学习到的行动策略完成任务。我们对BUDS进行了四种不同的双手任务评估。

    Key to rich, dexterous manipulation in the real world is the ability to coordinate control across two hands. However, while the promise afforded by bimanual robotic systems is immense, constructing control policies for dual arm autonomous systems brings inherent difficulties. One such difficulty is the high-dimensionality of the bimanual action space, which adds complexity to both model-based and data-driven methods. We counteract this challenge by drawing inspiration from humans to propose a novel role assignment framework: a stabilizing arm holds an object in place to simplify the environment while an acting arm executes the task. We instantiate this framework with BimanUal Dexterity from Stabilization (BUDS), which uses a learned restabilizing classifier to alternate between updating a learned stabilization position to keep the environment unchanged, and accomplishing the task with an acting policy learned from demonstrations. We evaluate BUDS on four bimanual tasks of varying compl
    
[^9]: 使用扩散模型的多粒度迭代图像编辑

    Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])

    [http://arxiv.org/abs/2309.00613](http://arxiv.org/abs/2309.00613)

    本研究提出了一种名为EMILIE的迭代多粒度图像编辑器，通过重新利用预训练的扩散模型来实现迭代编辑，并引入梯度控制操作来实现对图像编辑的粒度控制。

    

    最近，文本引导的图像合成的进展极大地改变了创意专业人员生成艺术和审美上令人愉悦的视觉资产的方式。为了充分支持这样的创意努力，该过程应具备以下能力：1）迭代地编辑生成的图像，2）控制所需变化的空间范围（全局、局部或介于两者之间）。我们将这个实用的问题设定正式化为迭代多粒度编辑。虽然在图像合成和编辑方面，基于扩散模型取得了重大进展，但它们都是一次性操作（即没有迭代编辑能力），并且不能自然产生多粒度控制（即涵盖从局部到全局编辑的全谱）。为了克服这些缺点，我们提出了EMILIE：迭代多粒度图像编辑器。EMILIE引入了一种新颖的潜在迭代策略，利用预训练的扩散模型来促进迭代编辑。同时，还引入了梯度控制操作来实现对所需变化的粒度控制。

    Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control opera
    
[^10]: NAS-X: 基于扭曲的神经自适应平滑方法

    NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])

    [http://arxiv.org/abs/2308.14864](http://arxiv.org/abs/2308.14864)

    NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。

    

    本文提出了一种名为NAS-X的神经自适应平滑方法，该方法基于重新加权的唤醒-睡眠算法进行顺序潜变量模型的学习和推断。NAS-X适用于离散和连续潜变量，并利用平滑SMC方法来拟合比传统的重新加权唤醒-睡眠方法更广泛的模型。我们在离散和连续任务上测试了NAS-X，并发现在推断和参数恢复方面，它明显优于先前的变分和基于重新加权唤醒-睡眠方法。

    We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
    
[^11]: 基于知识驱动的CoT：探索LLMs中对知识密集型问答进行忠实推理

    Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])

    [http://arxiv.org/abs/2308.13259](http://arxiv.org/abs/2308.13259)

    本文提出了一个名为知识驱动的思维链（KD-CoT）的框架，用于验证和修改LLMs中的推理过程，通过与外部知识的交互来解决幻觉和错误传播的问题。

    

    大型语言模型（LLMs）配备了思维链（CoT），在各种下游任务中展现出了令人印象深刻的推理能力。但是，由于幻觉和无法访问外部知识，LLMs在对知识密集型任务（如知识库问答）进行推理时常常会产生不正确或不忠实的中间推理步骤。为了缓解这个问题，我们提出了一个名为知识驱动的思维链（KD-CoT）的框架，通过与外部知识的交互来验证和修改CoT中的推理过程，从而克服幻觉和错误传播。具体地，我们将LLMs的CoT推理过程规范化为结构化的多轮问答格式。在每一轮中，LLMs与一个问答系统进行交互，该系统检索外部知识并基于检索到的准确答案产生忠实的推理过程。我们开发的KBQA CoT集合促进了LLMs的结构化CoT推理，它作为上下文学习的一部分。

    Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning
    
[^12]: 问题分类的集成方法：融合Electra Transformer、GloVe和LSTM

    An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.06828](http://arxiv.org/abs/2308.06828)

    本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。

    

    自然语言处理（NLP）已经成为理解和生成人类语言的关键技术，它在机器翻译、情感分析等任务中扮演着重要角色，尤其是在问题分类方面。作为自然语言处理的子领域，问题分类专注于确定所需信息的类型，这是问题回答系统等下游应用的基本步骤。本研究提出了一种创新的问题分类集成方法，将Electra、GloVe和LSTM模型的优势相结合。该模型在著名的TREC数据集上进行了严格测试，展示了如何整合这些不同技术可以得到更优越的结果。Electra提供了基于transformer的复杂语言理解能力，GloVe提供了全局向量表示以捕捉词级语义，LSTM则贡献了序列学习能力以建模长期依赖关系。

    Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
    
[^13]: 对协同过滤丢失函数的更好理解

    Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])

    [http://arxiv.org/abs/2308.06091](http://arxiv.org/abs/2308.06091)

    现有研究已经表明，通过改进对齐和均匀性设计的损失函数可以实现显著的性能提升。本文提出了一种新的损失函数，称为MAWU，它考虑了数据集的独特模式。

    

    协同过滤（CF）是现代推荐系统中的关键技术。CF模型的学习过程通常由三个组件组成：交互编码器、损失函数和负采样。尽管许多现有研究已经提出了各种CF模型来设计复杂的交互编码器，但最近的工作表明，简单地重新制定损失函数可以实现显著的性能提升。本文深入分析了现有损失函数之间的关系。我们的数学分析揭示了先前的损失函数可以解释为对齐和均匀性函数：（i）对齐匹配用户和物品表示，（ii）均匀性分散用户和物品分布。受到这个分析的启示，我们提出了一种改进对齐和均匀性设计的损失函数，考虑到数据集的独特模式，称为Margin-aware Alignment and Weighted Uniformity（MAWU）。MAWU的关键创新是

    Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
    
[^14]: 图聚类的同类性增强结构学习

    Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])

    [http://arxiv.org/abs/2308.05309](http://arxiv.org/abs/2308.05309)

    提出了一种名为HoLe的方法，通过在图结构中增强同类性可以显著改善图聚类任务的性能。

    

    图聚类是图分析中的一个基本任务，在利用图神经网络（GNNs）方面的最新进展已经取得了令人印象深刻的成果。尽管现有的基于GNN的图聚类方法取得了成功，但它们往往忽视了图结构的质量，这是由于现实世界图的稀疏性和多样性所固有的，从而导致了次优的性能。图结构学习可以通过添加缺失的连接和删除错误的连接来优化输入图。然而，以往的图结构学习工作主要集中在有监督的设置上，并且由于缺乏真实标签，不能直接应用于我们的特定聚类任务。为了弥补这个差距，我们提出了一种新颖的方法，称为同类性增强结构学习图聚类（HoLe）。我们的动机源于观察到，微妙地增强图结构中的同类性程度可以显著提升GNNs的性能。

    Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
    
[^15]: AI价值链的伦理问题

    The Ethics of AI Value Chains. (arXiv:2307.16787v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2307.16787](http://arxiv.org/abs/2307.16787)

    本文提出了AI价值链的概念，以满足AI伦理研究和干预的需求。通过综合评估AI价值链涉及的伦理问题，我们提出了四个未来方向，旨在推动更具伦理的AI发展和使用。

    

    对于对人工智能伦理感兴趣的研究人员、从业人员和政策制定者，他们需要更多综合的方法来研究和干预在多种背景和活动规模下的AI系统。本文提出了AI价值链作为一个满足这一需求的综合概念。为了更清晰地理论化AI价值链，并在概念上将其与供应链区分开来，我们回顾了战略管理、服务科学、经济地理学、行业、政府和应用研究文献中关于价值链和AI价值链的理论。然后，我们对涵盖AI价值链涉及的伦理问题的67个来源进行综合评估。根据我们综合评估的结果，我们提出了四个未来方向，研究人员、从业人员和政策制定者可以采取这些方向来推动在AI价值链上实现更具伦理的发展和使用。我们的综述和建议有助于推进研究议程的发展。

    Researchers, practitioners, and policymakers with an interest in AI ethics need more integrative approaches for studying and intervening in AI systems across many contexts and scales of activity. This paper presents AI value chains as an integrative concept that satisfies that need. To more clearly theorize AI value chains and conceptually distinguish them from supply chains, we review theories of value chains and AI value chains from the strategic management, service science, economic geography, industry, government, and applied research literature. We then conduct an integrative review of a sample of 67 sources that cover the ethical concerns implicated in AI value chains. Building upon the findings of our integrative review, we recommend four future directions that researchers, practitioners, and policymakers can take to advance more ethical practices of AI development and use across AI value chains. Our review and recommendations contribute to the advancement of research agendas, i
    
[^16]: 面向交通信号控制的不确定性感知基于实例的行动转换的模拟到实际转移

    Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control. (arXiv:2307.12388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12388](http://arxiv.org/abs/2307.12388)

    本文提出了UGAT方法，通过在模拟环境中动态转换具有不确定性的行动，实现了从模拟环境到真实环境的策略转移，显著提高了在真实世界中的性能。

    

    交通信号控制（TSC）是一个影响数百万人日常生活的复杂而重要的任务。强化学习（RL）在优化交通信号控制方面取得了有希望的结果，但当前基于RL的TSC方法主要在模拟环境中训练，存在模拟和真实世界之间性能差距的问题。本文提出了一种模拟到实际环境转移的方法，称为UGAT，通过在模拟中动态转换具有不确定性的行动，以减轻转移动态的领域差距，将在模拟环境中训练的学习策略转移到真实环境中。我们在模拟交通环境中评估了我们的方法，并表明它显著提高了转移后的RL策略在真实世界中的性能。

    Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
    
[^17]: HIQL: 以潜在状态作为动作的离线目标导向强化学习

    HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])

    [http://arxiv.org/abs/2307.11949](http://arxiv.org/abs/2307.11949)

    本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。

    

    无监督预训练最近已成为计算机视觉和自然语言处理的基石。在强化学习中，目标导向强化学习可以潜在地利用大量未标记的（无奖励）数据，提供类似于自我监督的方法。然而，构建有效的目标导向强化学习算法并直接从多样化的离线数据中进行学习是具有挑战性的，因为准确估计远期目标的价值函数很困难。然而，目标达成问题表现出一定的结构，即达到远期目标需要首先通过较近子目标。这种结构非常有用，因为评估邻近目标的动作质量通常比更远目标容易。基于这一思想，我们提出了一个基于离线数据的目标导向强化学习的分层算法。利用一个没有动作的价值函数，我们学习了两个策略，允许我们利用这种结构：一个高层策略

    Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
    
[^18]: 一种新型的与平台无关的多模态深度学习模型，用于识别社交媒体上的促进饮食紊乱内容

    A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])

    [http://arxiv.org/abs/2307.06775](http://arxiv.org/abs/2307.06775)

    本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。

    

    在过去的十年中，饮食紊乱的诊断和与之相关的死亡数量大幅增加，尤其是在新冠疫情期间。这种巨大增长部分来源于疫情的压力，但也与社交媒体的暴露增加有关，社交媒体上充斥着促进饮食紊乱的内容。这些内容可以诱发观看者的饮食紊乱。本研究旨在创建一个多模态深度学习模型，能够基于视觉和文本数据的组合判断给定的社交媒体帖子是否促进饮食紊乱。从Twitter收集了一个带有标签的推文数据集，对其进行了十二个深度学习模型的训练和测试。根据模型的性能，最有效的深度学习模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的多模态融合模型，准确率和F1分数分别达到95.9%和0.959。RoBERTa和MaxViT融合模型可以有效地识别社交媒体上的促进饮食紊乱的内容。

    Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
    
[^19]: 指令挖掘：大语言模型的高质量指令数据选择

    Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])

    [http://arxiv.org/abs/2307.06290](http://arxiv.org/abs/2307.06290)

    本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。

    

    大型语言模型通常经历预训练和微调两个训练阶段。尽管大规模预训练赋予模型强大的生成自然语言回应的能力，但这些预训练模型有时仍然无法理解人类指令。为了增强语言模型解释和响应指令的能力，指令微调已成为该领域的关键方法。最近的研究发现，即使只有少量高质量的指令跟随数据，大型语言模型也可以进行良好的微调。然而，选择用于微调语言模型的高质量数据集仍缺乏明确的指导方针。在本文中，我们提出了InstructMining，一个用于评估指令跟随数据质量的线性规则。我们使用具体的自然语言指标来进行InstructMining的建模。为了研究数据质量与这些指标之间的关系，我们还进行了广泛的细致研究。

    Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
    
[^20]: 通过$\beta$-分解一后验采样实现差分计算机学习

    Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])

    [http://arxiv.org/abs/2307.05194](http://arxiv.org/abs/2307.05194)

    通过对数据生成过程和模型之间的$\beta$-分解进行后验采样，我们提出了$\beta$D-Bayes，一种能够实现差分机器学习的方法。

    

    差分私密性确保了包含敏感数据的统计分析结果可以在不损害任何个体隐私的情况下进行发布。实现这种保证通常需要在参数估计或估计过程中直接注入噪音。而采样来自贝叶斯后验分布已被证明是指数机制的一种特殊情况，可以产生一致且高效的私密估计，而不会改变数据生成过程。然而，当前方法的应用受到较强的边界假设的限制，这些假设对于基本模型（如简单的线性回归器）并不成立。为了改善这一点，我们提出了$\beta$D-Bayes，一种从广义后验中进行后验采样的方案，目标是最小化模型与数据生成过程之间的$\beta$-分解。这提供了私密估计的方法。

    Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
    
[^21]: 通过基于分数的优化提升对抗鲁棒性

    Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04333](http://arxiv.org/abs/2307.04333)

    本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。

    

    对抗攻击有可能通过引入微小扰动来误导深度神经网络分类器。开发能够减轻这些攻击影响的算法对确保人工智能的安全使用至关重要。最近的研究表明，基于分数的扩散模型在对抗防御中是有效的。然而，现有的基于扩散的防御依赖于顺序模拟扩散模型的反向随机微分方程，这在计算效率上是低效的，并且产生次优结果。在本文中，我们介绍了一种名为ScoreOpt的新型对抗防御方案，该方案在测试时通过在由基于分数先验指导的方向上对原始干净数据进行优化来优化对抗样本。我们在多个数据集上进行了全面的实验，包括CIFAR10、CIFAR100和ImageNet。我们的实验结果表明，我们的方法在鲁棒性方面优于现有的对抗防御方法。

    Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
    
[^22]: 在发展有效的人工智能与人类团队协作中应用以人为中心的人工智能：以人工智能-人类共同认知系统的视角

    Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v1 [cs.AI])

    [http://arxiv.org/abs/2307.03913](http://arxiv.org/abs/2307.03913)

    本研究介绍了将人工智能与人类团队协作作为一种新的发展范式的方法，强调有效的人工智能与人类团队需要充分利用双方的独特能力，同时克服挑战和限制，提高联合表现。同时，该研究指出现有研究往往未考虑到动态、适应性和协作团队环境中人工智能的功能，呼吁加强关于人工智能与人类团队协作的研究。

    

    研究和应用已将人工智能与人类团队协作作为一种新的范式来发展人工智能系统。人工智能与人类团队协作认识到人工智能将作为一名队友而不仅仅是工具与人类协作。有效的人工智能与人类团队需要能够充分利用人类和人工智能的独特能力，同时克服每个成员的已知挑战和限制，增强人类能力，并将联合性能提高到任何实体之上。2023年全国人工智能研究和战略计划更新认识到，主要关注人工智能系统独立性能的研究计划往往未考虑到人工智能在动态、适应性和协作团队环境中必须提供的功能，并呼吁进一步研究人工智能与人类团队协作。然而，人们对于人工智能是否能作为人类的队友存在争议。主要的关注点在于采用"协作"范式是否与人类的认知过程相矛盾。

    Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems. HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans. Effective human-AI teams need to be capable of taking advantage of the unique abilities of both humans and AI while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. The National AI Research and Strategic Plan 2023 update has recognized that research programs focusing primarily on the independent performance of AI systems generally fail to consider the functionality that AI must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-AI teaming and collaboration. However, there has been debate about whether AI can work as a teammate with humans. The primary concern is that adopting the "teaming" paradigm contradicts the human
    
[^23]: 可实现回归的最优学习算法：PAC学习和在线学习

    Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])

    [http://arxiv.org/abs/2307.03848](http://arxiv.org/abs/2307.03848)

    本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。

    

    本研究旨在对可实现回归在PAC学习和在线学习的统计复杂度进行刻画。先前的研究已经证明了有限的fat shattering维度对于PAC学习的充分性以及有限的scaled Natarajan维度对于必要性的存在，但自从Simon 1997（SICOMP '97）的工作以来，对于更完整的刻画的进展甚少。为此，我们首先引入了一种最小化实例最优学习算法来对可实现回归进行学习，并提出了一种既定性又定量地刻画了哪些类的实数预测器可以被学习的新颖维度。然后，我们确定了一个与图维度相关的组合维度，该维度刻画了在可实现设置中的ERM可学习性。最后，我们根据与DS维度相关的组合维度建立了学习可行性的必要条件，并猜测它也可能是充分的。

    In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
    
[^24]: 使用目标条件预测编码作为离线强化学习的隐式规划器

    Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])

    [http://arxiv.org/abs/2307.03406](http://arxiv.org/abs/2307.03406)

    本论文研究了将决策制定视为离线收集的轨迹的监督学习问题，并探究了序列建模在轨迹压缩和策略学习中的作用。通过引入目标条件预测编码（GCPC），本文提出了一种能够产生强大轨迹表示并实现高性能策略的方法。

    

    最近的研究已经证明了将决策制定视为离线收集的轨迹的监督学习问题的有效性。然而，在轨迹数据上进行序列建模的好处尚不清楚。在这项工作中，我们研究了序列建模是否具备将轨迹压缩为有用表示并对策略学习有所贡献的能力。为了实现这一目标，我们采用了一个两阶段的框架，首先使用序列建模技术总结轨迹，然后利用这些表示学习策略以及一个期望的目标。这个设计使得许多现有的监督式离线强化学习方法可以被看作是我们框架的特例。在这个框架内，我们引入了目标条件预测编码（GCPC），这是一种带来强大轨迹表示并导致高性能策略的方法。我们在AntMaze，FrankaKitchen和Locomotion环境上进行了广泛的实证评估，并观察到...

    Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and obser
    
[^25]: FedCP:通过条件策略对个性化联邦学习中的特征信息进行分离

    FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])

    [http://arxiv.org/abs/2307.01217](http://arxiv.org/abs/2307.01217)

    提出了一种名为FedCP的个性化联邦学习方法，通过生成条件策略分离特征中的全局信息和个性化信息，并分别进行处理。实验证明该方法在计算机视觉和自然语言处理领域的性能超过了十一种最先进的方法，最高可提高6.69%。

    

    最近，个性化联邦学习（pFL）在隐私保护、协作学习以及解决客户端之间的统计异质性等方面引起了越来越多的关注，例如医院、移动智能手机等。大多数现有的pFL方法侧重于利用客户端级模型参数中的全局信息和个性化信息，但忽略了数据是这两种信息的源头。为了解决这个问题，我们提出了联邦条件策略（FedCP）方法，该方法为每个样本生成一个条件策略，以分离其特征中的全局信息和个性化信息，然后分别通过全局头和个性化头进行处理。与现有的pFL方法相比，FedCP更加细粒度地考虑个性化的样本特定方式。在计算机视觉和自然语言处理领域进行的大量实验表明，FedCP在性能上超过了十一种最先进的方法，最高可提高6.69%。

    Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthe
    
[^26]: 一个用于光学相干断层扫描中脉络膜的高效全自动分析的开源深度学习算法

    An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography. (arXiv:2307.00904v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.00904](http://arxiv.org/abs/2307.00904)

    本研究开发了一个名为DeepGPET的开源全自动深度学习算法，用于光学相干断层扫描中脉络膜区域分割，并且在数据准确性和处理速度方面取得了显著的改进。

    

    目的：开发一个开源全自动深度学习算法DeepGPET，用于光学相干断层扫描（OCT）数据中脉络膜区域分割。方法：我们使用了来自3个与系统性疾病相关的临床研究的715个OCT B-扫描（82名受试者，115只眼睛）的数据集。使用临床验证的半自动脉络膜分割方法高斯过程边缘追踪（GPET）生成了地面真实分割。我们对在ImageNet上预训练了MobileNetV3骨干的UNet进行了微调。标准分割一致性指标以及脉络膜厚度和面积的衍生度量被用于评估DeepGPET，同时还进行了临床眼科医生的定性评估。结果：DeepGPET在来自3个临床研究的数据上与GPET达到了很好的一致性（AUC = 0.9994，Dice = 0.9664；脉络膜厚度的皮尔逊相关系数为0.8908，脉络膜面积的皮尔逊相关系数为0.9082），同时将在标准笔记本电脑CPU上每张图像的平均处理时间缩短至34.49秒

    Purpose: To develop an open-source, fully-automatic deep learning algorithm, DeepGPET, for choroid region segmentation in optical coherence tomography (OCT) data. Methods: We used a dataset of 715 OCT B-scans (82 subjects, 115 eyes) from 3 clinical studies related to systemic disease. Ground truth segmentations were generated using a clinically validated, semi-automatic choroid segmentation method, Gaussian Process Edge Tracing (GPET). We finetuned a UNet with MobileNetV3 backbone pre-trained on ImageNet. Standard segmentation agreement metrics, as well as derived measures of choroidal thickness and area, were used to evaluate DeepGPET, alongside qualitative evaluation from a clinical ophthalmologist. Results: DeepGPET achieves excellent agreement with GPET on data from 3 clinical studies (AUC=0.9994, Dice=0.9664; Pearson correlation of 0.8908 for choroidal thickness and 0.9082 for choroidal area), while reducing the mean processing time per image on a standard laptop CPU from 34.49s (
    
[^27]: 一种用于Rounded Capacity Inequalities的神经分割算法

    A Neural Separation Algorithm for the Rounded Capacity Inequalities. (arXiv:2306.17283v1 [cs.AI])

    [http://arxiv.org/abs/2306.17283](http://arxiv.org/abs/2306.17283)

    本论文提出了一种基于学习的分割启发式算法，使用图神经网络学习精确分割问题的解，通过嵌入切平面法找到了容量VRP的下限。

    

    切平面法是成功的分支定价法和分支切割法算法的关键技术，适用于各种车辆路径问题（VRPs）的确切最优解。在各种切割中，圆角容量不等式（RCIs）是最基本的。生成RCIs需要解决分割问题，其精确解需要较长的时间获取，因此广泛使用启发式方法。我们设计了一种基于学习的带有图粗化的分割启发式算法，该算法使用图神经网络（GNN）学习精确分割问题的解，经过50到100个客户的小实例训练。我们将分割算法嵌入切平面法中，以找到容量VRP（CVRP）的下限，其中包括高达1,000个客户。我们将我们的方法与CVRPSEP进行了性能比较，CVRPSEP是用于解决VRP中各种切割问题的流行分割软件包。我们的计算结果表明，我们的方法的性能优于CVRPSEP。

    The cutting plane method is a key technique for successful branch-and-cut and branch-price-and-cut algorithms that find the exact optimal solutions for various vehicle routing problems (VRPs). Among various cuts, the rounded capacity inequalities (RCIs) are the most fundamental. To generate RCIs, we need to solve the separation problem, whose exact solution takes a long time to obtain; therefore, heuristic methods are widely used. We design a learning-based separation heuristic algorithm with graph coarsening that learns the solutions of the exact separation problem with a graph neural network (GNN), which is trained with small instances of 50 to 100 customers. We embed our separation algorithm within the cutting plane method to find a lower bound for the capacitated VRP (CVRP) with up to 1,000 customers. We compare the performance of our approach with CVRPSEP, a popular separation software package for various cuts used in solving VRPs. Our computational results show that our approach 
    
[^28]: 逼真的合成金融交易用于反洗钱模型

    Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])

    [http://arxiv.org/abs/2306.16424](http://arxiv.org/abs/2306.16424)

    本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。

    

    随着金融的广泛数字化和加密货币的日益流行，网络犯罪分子设计的欺诈方案越来越复杂。洗钱——将非法资金移动以掩盖其来源——可以跨越银行和国界，产生复杂的交易模式。联合国估计每年全球洗钱金额占全球GDP的2-5%，约为0.8-2.0万亿美元。不幸的是，通常无法获得用于训练机器学习模型来检测洗钱的真实数据，且之前的合成数据生成器存在显著缺陷。为了比较模型并推进该领域的发展，需要一个逼真、标准化、公开可用的基准数据集。为此，本文提出了一种合成金融交易数据集生成器和一组合成的反洗钱数据集。我们根据实际交易尽可能地校准了这个基于代理的生成器。

    With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
    
[^29]: 从仅状态序列学习非马尔科夫决策

    Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])

    [http://arxiv.org/abs/2306.15156](http://arxiv.org/abs/2306.15156)

    本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。

    

    传统的模仿学习假设能够获得展示者的动作，但是在自然环境中这些动作通常无法观测。此外，在这些环境中的序列决策行为可能偏离标准马尔科夫决策过程（MDP）的假设。为了解决这些挑战，我们探索了非马尔科夫决策过程（nMDP）中仅状态序列的深度生成建模，其中策略是潜在状态转移生成器的能量先验。我们开发了最大似然估计来实现基于模型的模仿，其中包括对先验进行短期MCMC采样和对后验进行重要性采样。学习的模型实现了“推理式决策”，即无模型策略执行等价于先验采样，基于模型的规划则是从策略初始化的后验采样。我们在一个具有非马尔科夫特征的原型路径规划任务中证明了所提方法的有效性。

    Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
    
[^30]: 人类介入的视觉前列腺深度刺激编码的优化

    Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses. (arXiv:2306.13104v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.13104](http://arxiv.org/abs/2306.13104)

    本研究提出了一种“人类介入的视觉前列腺深度刺激编码优化”的方法，通过反演前向模型、实时优化编码参数等手段，显著提高了感知质量。

    

    神经前列腺在恢复失去的感官功能和增强人类能力方面具有潜力，但当前设备产生的感觉通常似乎不自然或扭曲。植入器的确切位置和个体感知的差异导致刺激响应存在显着差异，使个性化刺激优化成为关键挑战。贝叶斯优化可用于优化具有有限噪声观察数据的患者专属刺激参数，但对于高维刺激不可行。而深度学习模型可以优化刺激编码策略，但通常假设有关患者特定变化的完美知识。在这里，我们提出了一种新颖的、实际可行的方法，克服了这两个基本局限性。首先，通过反演将电刺激映射到视觉感知的前向模型，训练深度编码器网络以为任何个体患者产生最佳刺激。其次，提出了一种优选贝叶斯优化算法，以实时优化编码参数，成功使知觉刺激更加逼真。我们提出的“人类介入的视觉前列腺深度刺激编码优化”方法在动物模型中进行了测试，相比最先进的方法显著提高了感知质量。

    Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian opti
    
[^31]: 从新的角度压缩ImageNet规模数据集：SRe$^2$L

    Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])

    [http://arxiv.org/abs/2306.13092](http://arxiv.org/abs/2306.13092)

    SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。

    

    我们提出了一个新的数据集压缩框架，称为Squeeze、Recover和Relabel（SRe$^2$L），它在训练期间分离了模型和合成数据的双层优化，以处理不同规模的数据集、模型体系结构和图像分辨率，从而实现有效的数据集压缩。所提出的方法展示了在不同数据集规模上的灵活性，并在合成图像任意分辨率、高分辨率训练的情况下具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力。我们在Tiny-ImageNet和完整的ImageNet-1K数据集上进行了广泛的实验。在50IPC下，我们的方法在Tiny-ImageNet和ImageNet-1K上分别实现了42.5％和60.8％的最高验证精度，较之前所有最先进方法提高了14.5％和32.9％。我们的方法在Res上也比MTT快约52倍(ConvNet-4)和16倍。

    We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
    
[^32]: 为大型图表示简化和增强Transformer

    Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10759](http://arxiv.org/abs/2306.10759)

    本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。

    

    在大型图上学习表示是一个长期存在的挑战，因为其中涉及了大量数据点之间的相互依赖关系。Transformer作为一种新兴的用于图结构数据的基本编码器类别，由于其全局注意力可以捕捉到邻节点之外的所有对影响，因此在小型图上表现出了有希望的性能。尽管如此，现有方法往往继承了Transformer在语言和视觉任务中的思想，并通过堆叠深层多头注意力来采用复杂的模型。本文通过关于节点属性预测基准的实验证明，即使只使用一层注意力也能在节点数量从千级到十亿级的范围内带来令人惊讶的竞争性能。这鼓励我们重新思考在大型图上设计Transformer的理念，其中全局注意力是一个阻碍可扩展性的计算开销。我们将提出的方案称为简化图Transformer。

    Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
    
[^33]: 残差 Q 学习：无需价值的在线和离线策略定制

    Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])

    [http://arxiv.org/abs/2306.09526](http://arxiv.org/abs/2306.09526)

    该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。

    

    模仿学习是一种广泛使用的框架，适用于从演示中学习模仿行为。当手工制作奖励函数困难或目标是模仿人类专家行为时，这种方法特别有吸引力。但是，学习的模仿策略只能遵循演示中的行为。在应用模仿策略时，我们可能需要根据不同的下游任务要求定制策略行为。同时，我们仍希望定制的策略保持其模仿性质。为此，我们提出了一种新的问题设置，称为策略定制。它将学习任务定义为训练一种策略，该策略继承先前策略的特性，同时满足目标下游任务强加的一些附加要求。我们提出了一种新颖和有原则的方法来解释和确定两个任务目标之间的权衡。具体而言，我们制定了一种动态控制残差的 Q 学习方法，该方法可以在不使用价值函数的情况下进行在线和离线策略定制。

    Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
    
[^34]: 大型语言模型是半参数强化学习智能体

    Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])

    [http://arxiv.org/abs/2306.07929](http://arxiv.org/abs/2306.07929)

    根据人类记忆和推理机制，提出了一种新的可演化LLM智能体框架REMEMBERER，通过为LLM装备长期经验记忆，可以为不同任务提供优异的智能体，其构成了半参数RL代理。成功率超过先前SOTA 4％和2％。

    

    受认知科学对人类记忆和推理机制的启发，提出了一种新的可演化LLM（大型语言模型）智能体框架REMEMBERER。通过为LLM装备长期经验记忆，REMEMBERER能够利用过去剧集的经验，甚至可以为不同的任务目标提供优异的LLM智能体，这优于具有固定实例或具有短暂工作记忆的LLM智能体。我们进一步介绍了经验记忆的强化学习（RLEM）来更新记忆。因此，整个系统可以从成功和失败的经验中学习，并在不微调LLM参数的情况下发展其能力。以此方式，所提出的REMEMBERER构成了半参数RL代理。在两个RL任务集上进行了大量实验以评估所提出的框架。不同初始化和训练集的平均结果对于成功率超过先前SOTA 4％和2％。

    Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate 
    
[^35]: 一种统一自动概念提取和概念重要性评估的全面方法

    A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation. (arXiv:2306.07304v1 [cs.LG])

    [http://arxiv.org/abs/2306.07304](http://arxiv.org/abs/2306.07304)

    本文提出了一个全面的理论框架，来统一定义和澄清自动概念提取和概念重要性评估，进而提供新的评估指标以实现对这些方法的比较以及推导关于这种方法的最优性的理论保证。

    

    近年来，基于概念的方法成为了一些最有前途的可解释方法，帮助我们解释人工神经网络（ANN）的决策。这些方法试图在两个关键步骤中发现被隐藏在ANN激活的复杂模式中的可理解的视觉“概念”：（1）概念提取，（2）重要性评估。虽然这两个步骤是各种方法之间共同的，但它们的具体实现都有所不同。在这里，我们介绍了一个统一的理论框架，全面定义和澄清了这两个步骤。该框架具有几个优点，它允许我们：（i）提出新的评估指标来比较不同的概念提取方法；（ii）利用现代归因方法和评估指标来扩展和系统地评估最先进的基于概念的方法和重要性评估技术；（iii）推导关于这种方法的最优性的理论保证。

    In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual 'concepts' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that comprehensively defines and clarifies these two steps. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii) to derive theoretical guarantees regarding the optimality of such met
    
[^36]: 用幅度受限制优化解锁更深层网络的特征可视化

    Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization. (arXiv:2306.06805v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.06805](http://arxiv.org/abs/2306.06805)

    提出了一种名为MACO的简单方法，通过优化相位谱生成图像，同时保持幅度恒定，解决了特征可视化在深度神经网络上的挑战，并实现了高质量和高效的可解释特征可视化。

    

    特征可视化在Olah等人2017年的有影响力的工作之后获得了很大的 popularity，将其确立为可解释性的重要工具。然而，由于依赖于生成可解释图像的技巧以及在将其扩展到更深的神经网络时面临的挑战，其广泛应用受到了限制。在这里，我们描述了一种简单的方法MACO来解决这些问题。主要思想是通过优化相位谱生成图像，同时保持幅度恒定，以确保生成的解释位于自然图像空间中。我们的方法在定性和定量上都取得了显着的改进，并为大型最先进神经网络提供了高效且可解释的特征可视化。我们还展示了我们的方法具有一个属性机制，可以增强特征可视化的空间重要性。我们在一个新的基准测试中验证了我们的方法。

    Feature visualization has gained substantial popularity, particularly after the influential work by Olah et al. in 2017, which established it as a crucial tool for explainability. However, its widespread adoption has been limited due to a reliance on tricks to generate interpretable images, and corresponding challenges in scaling it to deeper neural networks. Here, we describe MACO, a simple approach to address these shortcomings. The main idea is to generate images by optimizing the phase spectrum while keeping the magnitude constant to ensure that generated explanations lie in the space of natural images. Our approach yields significantly better results (both qualitatively and quantitatively) and unlocks efficient and interpretable feature visualizations for large state-of-the-art neural networks. We also show that our approach exhibits an attribution mechanism allowing us to augment feature visualizations with spatial importance. We validate our method on a novel benchmark for compa
    
[^37]: ShiftAddViT：多种乘法原语混合实现高效的视觉变换器

    ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06446](http://arxiv.org/abs/2306.06446)

    ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。

    

    视觉变换器（ViT）展示了令人印象深刻的性能，并成为多个视觉任务的统一骨干。但是，ViTs中的注意力和多层感知器（MLPs）由于密集的乘法而不够高效，导致训练和推理代价高昂。为此，我们提出了一种将预训练的ViT以多种乘法原语（例如位移和加法）重新参数化的方法，以实现全新类型的减少乘法的模型，称为ShiftAddViT，旨在实现GPU上的端到端推理加速，无需从头开始训练。具体而言，我们将查询和键映射为汉明空间中的二进制码之后，采用加法核对查询、键和值之间的MatMul进行重新参数化。剩余的MLPs或线性层则采用位移核进行重新参数化。我们利用TVM在GPU上实施并优化这些定制核，以实现实际硬件部署。我们发现，这种重新参数化方法可以显著提高推理速度，而无需从头开始训练。

    Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
    
[^38]: 通过模块化生成模型实现灵活的强化学习决策堆叠

    Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])

    [http://arxiv.org/abs/2306.06253](http://arxiv.org/abs/2306.06253)

    决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。

    

    强化学习是一种有吸引力的模型，可以推理顺序决策制定的几个不同方面，如指定复杂目标、规划未来观察和行动，以及批评其实用性。然而，这些能力的综合集成在保持最大表达能力的同时允许进行模型选择以实现高效的学习和推理，这构成了竞争性的算法挑战。我们提出了决策堆叠（Decision Stacks），这是一个生成框架，将目标条件化策略代理分解为3个生成模块。这些模块通过独立的生成模型模拟了观测、奖励和行动的时间演变，可以通过教师强制并行学习。我们的框架保证了表达能力和灵活性，在设计单个模块以考虑关键因素（如架构偏差、优化目标和动态、跨领域的可转移性和推理速度）方面具有优势。我们对一系列连续控制基准进行的实证结果表明，决策堆叠提供了一种灵活且可扩展的替代最先进的基于模型和基于模型的强化学习方法。

    Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
    
[^39]: Prodigy: 一种快速自适应零参数学习算法

    Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])

    [http://arxiv.org/abs/2306.06101](http://arxiv.org/abs/2306.06101)

    本文提出了一种基于自适应算法(如Adagrad和Adam)的学习率估计方法Prodigy和Resetting，可以快速且正确地估计到达解决方案所需的距离D，从而提高了模型的收敛速度，并在多个数据集和模型上进行了测试，实验表明该方法优于D-Adaptation并可达到手动调整Adam的测试准确度值。

    

    本文研究自适应算法(如Adagrad和Adam)中的学习率估计问题，描述了两种技术Prodigy和Resetting，可以证明地估计到达解决方案所需的距离D，以便最优设置学习率。我们的技术是基于学习率自由的D-Adaptation方法的修改，并通过$O(\sqrt{\log(D/d_0)})$的因子提高了D-Adaptation的收敛速度，其中$d_0$是$D$的初始估计值。我们在12个常见的逻辑回归基准数据集、在CIFAR10上训练的VGG11和ResNet-50、在Imagenet上训练的ViT、在IWSLT14上训练的LSTM、在Criteo数据集上训练的DLRM、在Knee MRI数据集上的VarNet，以及在BookWiki上训练的RoBERTa和GPT transformer上测试了我们的方法。我们的实验结果表明，我们的方法始终优于D-Adaptation，并达到手动调整Adam的测试准确度值。

    We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
    
[^40]: 图上函数的贝叶斯优化

    Bayesian Optimisation of Functions on Graphs. (arXiv:2306.05304v1 [cs.LG])

    [http://arxiv.org/abs/2306.05304](http://arxiv.org/abs/2306.05304)

    本论文提出了一种在通用的大规模和潜在未知图上定义函数的贝叶斯优化算法，并通过学习适当的图内核，适应目标函数行为。

    

    图结构数据的不断涌现推动了在图节点集上定义函数的优化任务。传统的图搜索算法可用于此，但它们可能样本效率低下，并且不利用关于函数值的信息；另一方面，贝叶斯优化是一类有前途的黑盒求解器，具有更高的样本效率，但它很少被应用于这样的新颖设置。为了填补这一空白，我们提出了一种新颖的贝叶斯优化框架，该框架优化在通用，大规模和潜在的未知图上定义的函数。通过学习适当的图内核，我们的框架具有适应目标函数行为的优点。局部建模方法进一步保证了我们方法的效率。在合成和真实世界图上的大量实验表明了所提出的优化框架的有效性。

    The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.
    
[^41]: 分解对比学习：超越多视角冗余

    Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])

    [http://arxiv.org/abs/2306.05268](http://arxiv.org/abs/2306.05268)

    本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。

    

    在广泛的多模态任务中，对比学习已成为一种特别吸引人的方法，因为它可以成功地学习具有丰富未标记数据的表示，只需配对信息（例如，图像标题或视频音频对）。这些方法的基础是多视角冗余的假设——跨模态间共享信息对于下游任务是必要且足够的。然而，在许多现实世界的情况下，任务相关信息也包含在跨模态唯一区域中：一种仅存在于一个模态中但与任务仍然相关的信息。如何学习自我监督的多模态表示以捕获与下游任务相关的共享和唯一信息？本文提出了一种新的多模态表示学习方法FactorCL，以超越多视角冗余。FactorCL的基础是三个新的贡献：（1）将任务相关信息分解为共享和唯一表示，（2）限制共享和唯一成分之间的交互，（3）使用因子正则化促进表示学习。

    In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
    
[^42]: 黑盒序贯决策系统的自主能力评估

    Autonomous Capability Assessment of Black-Box Sequential Decision-Making Systems. (arXiv:2306.04806v1 [cs.AI])

    [http://arxiv.org/abs/2306.04806](http://arxiv.org/abs/2306.04806)

    本论文提出了一种新方法，可以有效地评估黑盒SDM系统的能力，该方法使用主动学习来建立一个可解释的概率模型，能够准确地描述其能力和在随机环境中执行这些能力的可能效果和要求。

    

    理解AI系统的能力和局限性对于安全地使用它们是至关重要的。然而，让用户评估具有不断发展的序贯决策能力的AI系统的问题相对少有研究。本文提出了一种新方法，用于建模黑盒AI系统的能力，包括在随机环境中执行这些能力的可能效果和要求。我们提出了一种主动学习方法，可以有效地与黑盒SDM系统交互，并学习描述其能力的可解释概率模型。对该方法的理论分析确定了学习过程收敛到代理正确模型的条件；对不同代理和模拟场景的实证评估表明，该方法可以有效地描述任意黑盒SDM代理的能力，并能进行少次通用化。

    It is essential for users to understand what their AI systems can and can't do in order to use them safely. However, the problem of enabling users to assess AI systems with evolving sequential decision making (SDM) capabilities is relatively understudied. This paper presents a new approach for modeling the capabilities of black-box AI systems that can plan and act, along with the possible effects and requirements for executing those capabilities in stochastic settings. We present an active-learning approach that can effectively interact with a black-box SDM system and learn an interpretable probabilistic model describing its capabilities. Theoretical analysis of the approach identifies the conditions under which the learning process is guaranteed to converge to the correct model of the agent; empirical evaluations on different agents and simulated scenarios show that this approach is few-shot generalizable and can effectively describe the capabilities of arbitrary black-box SDM agents 
    
[^43]: 随机坍缩：如何利用梯度噪声使SGD动态趋向更简单的子网络

    Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])

    [http://arxiv.org/abs/2306.04251](http://arxiv.org/abs/2306.04251)

    SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。

    

    本文揭示了随机梯度下降（SGD）的一个强烈隐式偏好，它将过度表达的网络驱动到更简单的子网络，从而大大减少了独立参数的数量，并提高了泛化能力。为了揭示这个偏好，我们识别了不变集，或者说是SGD未修改的参数空间的子集。我们专注于两类不变集，它们对应于现代架构中常见的更简单的子网络。我们的分析揭示了SGD在这些简单不变集方面具有随机吸引性的特性。我们根据损失景观在不变集周围的曲率和随机梯度引入的噪声之间的竞争建立了一种随机吸引性的充分条件。值得注意的是，我们发现增加噪声水平会增强吸引力，导致与鞍点或训练损失的局部极大值相关的吸引不变集的出现。

    In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
    
[^44]: 数据中动态偏移的状态规范化策略优化

    State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])

    [http://arxiv.org/abs/2306.03552](http://arxiv.org/abs/2306.03552)

    本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。

    

    在许多实际场景中，强化学习算法使用的数据受到动态偏移的影响，即具有不同的环境动态。目前的大多数方法通过训练上下文编码器来识别环境参数来解决这个问题。根据其环境参数将带有动态漂移的数据分开以训练相应的策略。然而，这些方法可能会出现样本效率低下的问题，因为数据是“特定场景”使用的，针对某个环境训练的策略不能从收集在其他具有不同动态的所有其他环境中的数据中受益。本文发现，在许多具有相似结构和不同动态的环境中，最优策略具有类似的稳态分布。我们利用这种特性，并从具有动态漂移的数据中学习稳态分布，以实现高效的数据重用。这种分布用于规范新环境中训练的策略，导致了 SRPO（状态规范化策略优化）算法的出现。实验结果表明，SRPO 在具有动态偏移的任务上显著优于现有的方法。

    In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
    
[^45]: 通过强制条件单调性在Early-Exit结构中实现随时分类

    Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity. (arXiv:2306.02652v1 [cs.LG])

    [http://arxiv.org/abs/2306.02652](http://arxiv.org/abs/2306.02652)

    本文提出了一种在Early-Exit网络中实现条件单调性的方法，将深度模型转化为真正的随时分类器。

    

    现代预测模型通常部署在计算预算动态的环境中。随时算法非常适用于这种环境，因为它们在计算的任何时候都可以输出预测值，其质量是计算时间的函数。由于其能够在网络各个阶段提供中间预测结果的能力，Early-Exit神经网络在随时计算的背景下引起了人们的关注。然而，我们证明当前的Early-Exit网络并不直接适用于任何时候的设置，因为单个数据点的预测质量不能保证随着计算时间的增加而提高。为了解决这个缺陷，我们提出了一种优雅的事后修改，基于专家乘积，鼓励Early-Exit网络逐渐变得自信。这赋予了我们的深度模型条件单调性的特性——这是实现真正随时分类的重要基石。

    Modern predictive models are often deployed to environments in which computational budgets are dynamic. Anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. Early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. However, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. To address this shortcoming, we propose an elegant post-hoc modification, based on the Product-of-Experts, that encourages an early-exit network to become gradually confident. This gives our deep models the property of conditional monotonicity in the prediction quality -- an essential stepping stone towards truly an
    
[^46]: 未知干预的因果表达式的非参数识别

    Nonparametric Identifiability of Causal Representations from Unknown Interventions. (arXiv:2306.00542v1 [stat.ML])

    [http://arxiv.org/abs/2306.00542](http://arxiv.org/abs/2306.00542)

    本文提出了一种新方法用于从未知干预数据中推断非参数因果表达式学习，并且证明了在两个因果变量的基本设置中，无法消除一些由干预数据引起的歧义问题。

    

    我们研究因果表达式学习，即从变量的高维函数（“混合物”）中推断潜在的因果变量及其因果关系的任务。以前的工作依赖于弱监督，如反事实的干预观察或时间结构；对混合函数或潜在因果模型施加限制，如线性；或需要部分了解生成过程，如因果图或干预目标。我们考虑到因果模型和混合函数都是非参数的一般情况。学习信号采用来自基础因果模型中未知干预的多个数据集或环境的形式。我们的目标是将地面真实潜变量及其因果图鉴定出来，同时解决一组从干预数据无法消除的歧义问题。我们研究了两个因果变量的基本设置，并证明了...

    We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions ("mixtures") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or the intervention targets. We instead consider the general setting in which both the causal model and the mixing function are nonparametric. The learning signal takes the form of multiple datasets, or environments, arising from unknown interventions in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that
    
[^47]: 解决扩散模型中的负迁移问题

    Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])

    [http://arxiv.org/abs/2306.00354](http://arxiv.org/abs/2306.00354)

    本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。

    

    基于扩散的生成模型在各个领域都取得了显著的成功。它在同时涵盖不同噪声水平的去噪任务上训练模型，代表了一种多任务学习（MTL）的形式。然而，从MTL的角度分析和改善扩散模型仍然未被充分探索。特别地，MTL有时会导致众所周知的$\textit{负迁移}$现象，这种现象是由于任务之间存在冲突而导致某些任务的性能降低。本文旨在从MTL的角度分析扩散训练，提出了两个关键观察：$\textbf{(O1)}$ 随着噪声水平之间的差距加大，去噪任务之间的任务亲和力减弱， $\textbf{(O2)}$ 在扩散训练的背景下，负迁移也可能会出现。基于这些观察结果，我们的目标是通过减轻负迁移来增强扩散训练。为了实现这一目标，我们提出了利用现有的MTL方法、具体是正则化技术，来鼓励任务特定的特征提取并减少任务干扰。实验结果表明，我们提出的方法有效地减轻了负迁移，提高了扩散模型在一系列去噪任务上的性能。

    Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
    
[^48]: 使用IBIA框架的边缘近似推断

    Approximate inference of marginals using the IBIA framework. (arXiv:2306.00335v1 [cs.AI])

    [http://arxiv.org/abs/2306.00335](http://arxiv.org/abs/2306.00335)

    提出了一种新的基于IBIA框架的算法，通过将PGM转化为有界团大小的SLCTF序列，并使用启发式置信度更新算法来推导边缘。实验结果表明，该算法比当前最先进的技术更加有效和高效。

    

    概率图模型（PGM）中边缘的精确推断被认为是不可计算的，因此需要使用近似方法。大多数现有的变分技术在环路图中执行迭代信息传递，这对于许多基准测试来说收敛速度慢。本文提出了一种基于增量构建-推理-近似（IBIA）范例的新型边缘推断算法。我们的算法将PGM转化为具有有界团大小的一系列链接的团树森林（SLCTF），然后使用启发式置信度更新算法来推导边缘。对于贝叶斯网络的特殊情况，我们显示如果IBIA中增量构建步骤使用变量的拓扑顺序，则（a）所有SLCTF中的CTF的先验边缘一致，（b）一旦将所有证据变量添加到SLCTF中，后验边缘就是一致的。在我们的方法中，置信传递步骤是非迭代的，准确度和复杂度之间的权衡可以通过调整团的大小来控制。基准数据集上的实验结果证明了与现有技术相比我们算法的有效性和效率。

    Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity
    
[^49]: 扩散画笔：基于潜在扩散模型的AI生成图像编辑工具

    Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])

    [http://arxiv.org/abs/2306.00219](http://arxiv.org/abs/2306.00219)

    本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。

    

    文本到图像的生成模型在生成高质量图像方面取得了显著的进展。然而，由于模型限制，生成的图像经常包含不良的伪影或其他错误。现有的微调生成图像的技术要么耗时（手动编辑），要么产生不够完美的结果（修补），要么会导致整体图像产生意想不到的变化（变体选择和提示微调）。本文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地微调AI合成图像中所需的区域。我们的方法在反向扩散过程中在目标区域引入了新的随机噪声模式，使模型能够在保留其他区域原始上下文的同时，高效地对指定区域进行更改。我们通过艺术家进行的用户研究评估了我们方法的可用性和有效性，将我们的技术与其他最先进的图像修复技术进行了比较。

    Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
    
[^50]: 预训练表示中的扩散冗余

    Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])

    [http://arxiv.org/abs/2306.00183](http://arxiv.org/abs/2306.00183)

    本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。

    

    在大型数据集上预训练神经网络获得的表示已被越来越多地成功应用于各种下游任务中。在本文中，我们更加深入地研究了这种预训练表示中的特征是如何被编码的。我们发现，在给定层中学到的表示展现出一定程度的扩散冗余，即对于超过一个阈值大小的任何随机子集神经元，都与完整层具有很高的相似度，并且在各种下游任务中能够表现出与整个层相似的性能。我们在各种不同的神经架构（包括CNN和Transformer）上进行了实验，使用了ImageNet1k和ImageNet21k进行预训练，并评估了各种下游任务，如图像分类、目标检测和自然语言处理。我们的实验结果表明，可以利用预训练表示中的冗余来降低在实际部署中使用这些模型的计算和内存成本，同时仍然保持相当的性能水平。

    Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
    
[^51]: 强化学习中的潜在探索

    Latent Exploration for Reinforcement Learning. (arXiv:2305.20065v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.20065](http://arxiv.org/abs/2305.20065)

    本文提出了一种名为Lattice的方法，通过向策略网络的潜在状态中注入时间相关性噪声，来解决强化学习中多作用器系统存在的探索问题。

    

    在强化学习中，智能体通过探索和与环境互动来学习策略。由于维度灾难，学习将高维感知输入映射到运动输出的策略尤其具有挑战性。在训练过程中，最先进的方法（如SAC，PPO等）通过对作用力施加独立的高斯噪声来探索环境。尽管这种非结构化的探索方法在许多任务中证明成功，但对于过动作系统来说可能不够优化。当多个作用器（如马达或肌肉）驱动行为时，不相关的扰动可能会减少彼此的影响，或以与任务无关的方式修改行为。虽然已经存在通过引入动作扰动之间的时间相关性来解决这个问题的方法，但忽视了跨作用器之间的相关性。在本文中，我们提出了潜在时间相关探索（Lattice），一种将时间相关噪声注入到策略网络的潜在状态中的方法。

    In Reinforcement Learning, agents learn policies by exploring and interacting with the environment. Due to the curse of dimensionality, learning policies that map high-dimensional sensory input to motor output is particularly challenging. During training, state of the art methods (SAC, PPO, etc.) explore the environment by perturbing the actuation with independent Gaussian noise. While this unstructured exploration has proven successful in numerous tasks, it can be suboptimal for overactuated systems. When multiple actuators, such as motors or muscles, drive behavior, uncorrelated perturbations risk diminishing each other's effect, or modifying the behavior in a task-irrelevant way. While solutions to introduce time correlation across action perturbations exist, introducing correlation across actuators has been largely ignored. Here, we propose LATent TIme-Correlated Exploration (Lattice), a method to inject temporally-correlated noise into the latent state of the policy network, which
    
[^52]: 强化学习中的可复现性研究

    Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])

    [http://arxiv.org/abs/2305.19562](http://arxiv.org/abs/2305.19562)

    这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。

    

    我们在强化学习 (RL) 的背景下，将可复现性作为算法属性进行了数学研究。我们关注的是具有生成模型访问权的带折扣表格MDP的基本设置。受Impagliazzo等人 [2022]的启发，如果在内部随机性相同时，RL算法在从生成器抽取的两个独立和同分布的样本上执行两次并输出完全相同的策略，则表示该RL算法是可复制的。我们首先提供一个有效的$\rho$-可复制算法，用于$(\varepsilon,\delta)$-最优策略估计，其样本和时间复杂度为 $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$，其中$N$是状态-动作对的数量。然后，对于确定性算法的子类，我们提供了 $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ 阶的下限。接下来，我们研究了Kalavasis等人[2019]提出的可复制性的松弛版本，其中仅要求算法的输出接近复制算法的输出，而不是相同。我们提供了一种有效算法，其时间和样本复杂度为 $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$，用于$(\varepsilon,\delta)$意义下的可复制性，这比先前与相关问题的界限更好。最后，我们讨论了我们的结果对RL算法设计和可重复性研究的未来方向的影响。

    We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
    
[^53]: SheetCopilot: 通过大型语言模型将软件生产力提升到新的水平

    SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. (arXiv:2305.19308v1 [cs.SE])

    [http://arxiv.org/abs/2305.19308](http://arxiv.org/abs/2305.19308)

    本研究提出了一个基于大型语言模型（LLMs）的代理程序SheetCopilot，该程序可以通过自然语言指导软件执行电子表格数据处理等任务。该程序设计了一组抽象的电子表格软件功能原子动作以及基于状态机的任务规划框架，实现了LLMs与电子表格的鲁棒交互，可以单次正确完成44.3％的任务。

    

    计算机终端用户花费了数十亿小时完成诸如表格数据处理和项目时间轴调度等日常任务。这些任务大多是重复性的和容易出错的，然而大多数终端用户缺乏自动化这些繁琐工作的技能。随着大型语言模型（LLMs）的出现，用自然语言用户请求指导软件成为了一个可达成的目标。在本研究中，我们提出了一个SheetCopilot代理，该代理接受自然语言任务和控制电子表格以满足要求。我们提出了一组原子动作作为电子表格软件功能的抽象。我们进一步设计了基于状态机的任务规划框架，以便LLMs与电子表格进行鲁棒的交互。我们策划了一个包含221种电子表格控制任务的代表性数据集，并建立了一个完全自动化的评估管道，以严格评估LLMs在软件控制任务中的能力。我们的SheetCopilot单次正确完成44.3％的任务。

    Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill of automating away these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single 
    
[^54]: NetHack很难被黑客入侵。

    NetHack is Hard to Hack. (arXiv:2305.19240v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19240](http://arxiv.org/abs/2305.19240)

    本文研究了神经策略学习在NetHack中的性能差距，并分析了获胜的符号代理，提出了动作层级的优势、神经架构的改进以及强化学习与模仿学习的整合等方面可能提升性能的方法。

    

    神经策略学习方法在各种控制问题中取得了显著的成果，从Atari游戏到模拟运动。然而，在长视野任务中，尤其是在具有多模态观测的开放式环境中，比如流行的地牢探险游戏NetHack中，这些方法都面临困难。有趣的是，NeurIPS 2021 NetHack挑战赛表明，符号代理在中位游戏得分上超过神经方法四倍。在本文中，我们深入探讨了这种性能差距背后的原因，并对NetHack的神经策略学习进行了广泛研究。为了进行这项研究，我们分析了获胜的符号代理，并扩展了其代码库以跟踪内部策略选择，以生成其中一个最大的可用演示数据集。利用这个数据集，我们研究了以下几个方面：(i) 动作层级的优势；(ii) 神经架构的改进；以及 (iii) 强化学习与模仿学习的整合。

    Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning.
    
[^55]: 关于激活函数和规范化对初始化等距嵌入的影响

    On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])

    [http://arxiv.org/abs/2305.18399](http://arxiv.org/abs/2305.18399)

    本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。

    

    本文探讨了深度神经网络中倒数第二个 Gram 矩阵的结构，该矩阵包含与一批输入对应的输出之间的成对内积。在几种架构中，观察到在初始化时该 Gram 矩阵会随着深度变得退化，从而严重减缓训练速度。规范化层如批处理规范化或层规范化，在防止秩崩溃问题方面起着关键作用。然而现有的理论结果无法全面覆盖广泛用于 transformer 中的层规范化和有限深度下规范化的量化偏差。为了解决这个问题，我们证明了在初始化时，结合激活函数层使用的层规范化可以使多层感知机的 Gram 矩阵偏向指数级深度等距，并使用激活函数的 Hermite 展开来量化这个速度，从而填补了现有理论的空白。

    In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
    
[^56]: 知识增强的推理蒸馏：面向知识密集型任务的小型语言模型

    Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])

    [http://arxiv.org/abs/2305.18395](http://arxiv.org/abs/2305.18395)

    本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。

    

    大型语言模型在需要复合知识理解的知识密集型推理任务中表现出了良好的性能。但是，由于计算要求高且涉及数据隐私，将此类模型部署到现实世界的应用中可能会具有挑战性。以往的研究专注于通过微调具有标记数据或蒸馏大型语言模型来构建任务特定的小型语言模型，但是由于小型语言模型在记忆所需知识方面的能力有限，这些方法不适用于知识密集型推理任务。在理论分析的基础上，我们提出了一种名为知识增强的推理蒸馏 (KARD) 的新方法，该方法微调小型语言模型以生成从外部知识库检索到的增强知识的依据。此外，我们还提出了一个神经重排器，用于获得与依据生成相关的文档。我们实证表明，KARD在三项知识密集型任务上显着优于以前的方法，并且在模型尺寸相同的情况下可以达到与LLMs可比较的结果。

    Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
    
[^57]: 超越置信度：可靠模型还应考虑非典型性

    Beyond Confidence: Reliable Models Should Also Consider Atypicality. (arXiv:2305.18262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18262](http://arxiv.org/abs/2305.18262)

    研究发现，模型的可靠性不能仅仅依靠置信度，还应考虑预测样本或类别的非典型性，特别是对于非典型输入或类别，模型预测更过于自信且准确性较低。利用这些发现，将非典型性纳入模型可以提高不确定性量化和性能。

    

    大多数机器学习模型能够提供置信度以预测结果，然而，置信度无法完全理解预测的可靠性。例如，当输入在训练数据集中没有很好的表示或者输入 inherently 易混淆时，模型可能会给出较低的置信度。本研究探讨了样本或类别的非典型性与模型预测可靠性之间的关系。我们首先证明了非典型性与误校准和准确性之间的强相关性。具体而言，我们实证表明对于非典型的输入或非典型的类别，预测更加过于自信且准确性较低。利用这些发现，我们展示了如何将非典型性纳入不确定性量化和鉴别性神经网络以及大型语言模型的性能提升。在一个案例研究中，我们展示了利用非典型性如何提高不同肤色群体的皮肤病变分类器的性能。

    While most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction's reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical(rare) a sample or a class is and the reliability of a model's predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups witho
    
[^58]: 诊断变压器：揭示临床决策中的特征空间。 (arXiv:2305.17588v2 [cs.CL] UPDATED)

    Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17588](http://arxiv.org/abs/2305.17588)

    该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。

    

    在医学等高风险领域，为了建立信任和确保安全，模型的可解释性至关重要，而使用有限的临床记录对预训练的变压器进行微调以辅助临床决策。我们引入了一种名为SUFO的系统框架，该框架增强了微调的变压器特征空间的可解释性。SUFO利用一系列分析和可视化技术，包括监督探索、无监督相似性分析、特征动态和异常值分析，来解决关于模型信任和可解释性的关键问题。我们进行了一个案例研究，研究了预训练数据对真实世界病理分类任务的影响，并在MedNLI上验证了我们的发现。我们评估了五个110M规模的预训练变压器模型，分为通用领域（BERT, TNLR）、混合领域（BioBERT, Clinical BioBERT）和领域特定（PubMedBERT）组。我们的SUFO分析揭示了：(1)

    Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
    
[^59]: 通过赌博进行公平性审计

    Auditing Fairness by Betting. (arXiv:2305.17570v1 [stat.ML])

    [http://arxiv.org/abs/2305.17570](http://arxiv.org/abs/2305.17570)

    本文提供了一种通过赌博的方式进行公平性审计的方法，相比之前的方法，这种方法具有更高的实用性和效率，能够对不断产生的数据进行连续的监控，并处理因分布漂移导致的公平性问题。

    

    我们提供了实用、高效、非参数方法，用于审计已部署的分类和回归模型的公平性。相比之前依赖于固定样本量的方法，我们的方法是序贯的，并允许对不断产生的数据进行连续的监控，因此非常适用于跟踪现实世界系统的公平性。我们也允许数据通过概率策略进行收集，而不是从人口中均匀采样。这使得审计可以在为其他目的收集的数据上进行。此外，该策略可以随时间改变，并且不同的子人群可以使用不同的策略。最后，我们的方法可以处理因模型变更或基础人群变更导致的分布漂移。我们的方法基于最近关于 anytime-valid 推断和博弈统计学的进展，尤其是"通过赌博进行测试"框架。这些联系确保了我们的方法具有可解释性、快速和提供统计保证。

    We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics-the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, 
    
[^60]: 因果成分分析

    Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])

    [http://arxiv.org/abs/2305.17225](http://arxiv.org/abs/2305.17225)

    本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。

    

    独立成分分析(ICA)的目标是从混合观测到的变量中恢复独立的潜在变量。而因果表示学习(CRL)的目标是推断因果关系强相关性的潜在变量，以及编码它们的因果关系的未知图。我们引入了一个中间问题，称为因果成分分析(CauCA)。CauCA可以被看作是ICA的一种推广，对潜在成分之间的因果依赖建模，也是CRL的一个特例。与CRL不同的是，它预设了因果图的知识，仅关注于学习解混函数和因果机制。所有关于CauCA回收基础真相的不可能结果也适用于CRL，而可能性结果可以作为扩展CRL的基础。我们将从对潜在因果变量实施不同类型干预的多个数据集中表征CauCA的可识别性。

    Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
    
[^61]: 通过想象过去来推断未来

    Inferring the Future by Imagining the Past. (arXiv:2305.17195v1 [cs.AI])

    [http://arxiv.org/abs/2305.17195](http://arxiv.org/abs/2305.17195)

    本文介绍了一种蒙特卡罗算法用于从情报主体的单张图像中推断出一连串复杂的历史和未来事件，并且能够在只有少数样本的情况下扩展到各种具有挑战性的推断问题。

    

    漫画书中的单一画面能够展现人物的当前状态、来自何处、动机以及可能发生的事情，这启示了我们可以从情报主体的单张图像中推断出一连串复杂的历史和未来事件。本文基于最近的认知科学研究，提供一种蒙特卡罗算法用于进行此类推断。建立在计算机图形学中的蒙特卡罗路径追踪的基础上，我们借鉴了许多理念，大大提高了先前工作中的样本效率。这使得我们能够在只有少数样本的情况下扩展到各种具有挑战性的推断问题。它也表明了一定的认知合理性，事实上，我们呈现了在各种领域中，我们的算法能够匹配先前方法无法扩展的人类直觉的人类受试者研究。

    A single panel of a comic book can say a lot: it shows not only where characters currently are, but also where they came from, what their motivations are, and what might happen next. More generally, humans can often infer a complex sequence of past and future events from a *single snapshot image* of an intelligent agent.  Building on recent work in cognitive science, we offer a Monte Carlo algorithm for making such inferences. Drawing a connection to Monte Carlo path tracing in computer graphics, we borrow ideas that help us dramatically improve upon prior work in sample efficiency. This allows us to scale to a wide variety of challenging inference problems with only a handful of samples. It also suggests some degree of cognitive plausibility, and indeed we present human subject studies showing that our algorithm matches human intuitions in a variety of domains that previous methods could not scale to.
    
[^62]: 三塔：利用预训练图像模型进行灵活的对比学习

    Three Towers: Flexible Contrastive Learning with Pretrained Image Models. (arXiv:2305.16999v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16999](http://arxiv.org/abs/2305.16999)

    本文提出了 3T 方法，即在视觉语言模型中引入预训练的图像分类器，从而提高对比学习的灵活性。3T 同时受益于预训练嵌入和对比训练，并在实验证明对检索任务和分类问题均取得了有竞争力的性能。

    

    本文提出了一种名为“三塔（3T）”的灵活方法，通过将预先训练的图像分类器纳入对比学习，改进了视觉语言模型的对比学习。与通常从头开始训练对比模型不同，最近的 LiT（Zhai 等人，2022）表明了使用预训练分类器嵌入的性能提升。但是，LiT 直接用冻结的嵌入替换图像塔，排除了对图像塔进行对比训练的任何潜在好处。通过 3T，我们提出了一种更灵活的策略，允许图像塔同时受益于预训练嵌入和对比训练。为了实现这一点，我们引入了第三个塔，其中包含冻结的预训练嵌入，并鼓励该第三个塔与主要的图像-文本塔之间的对齐。在实验证明，3T 在检索任务上始终优于 LiT 和 CLIP 风格的从头开始对比学习基线。对于分类问题，3T 在从头开始基线的基础上可靠地改善，虽然在某些数据集上表现不及 LiT，但仍然实现了有竞争力的性能。总的来说，本方法凸显了将预训练分类器注入到视觉语言模型中的有效性，并提供了一种更灵活的利用它们的方法。

    We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperform
    
[^63]: 在模拟人类社会中训练社会对齐的语言模型

    Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16960](http://arxiv.org/abs/2305.16960)

    本研究提出了一种在模拟人类社会中训练语言模型的新方法，相比于现有方法，该方法具有更大的可扩展性和高效性，并在对齐基准和人类评估中展示出更优异的性能。

    

    AI系统中的社会对齐旨在确保这些模型按照既定的社会价值行事。然而，与人类不同，人们通过社交互动得出对价值判断的共识，当前的语言模型（LMs）则在孤立地复制其训练语料库时被训练出来，导致在陌生场景中表现不佳，并易受到对抗攻击。本研究提出了一种新的训练范式，允许LMs从模拟的社交互动中学习。与现有方法相比，我们的方法具有更大的可扩展性和高效性，在对齐基准和人类评估中展示出更优异的性能。这种LMs训练中的范式转变使我们离开发能够强有力且准确反映社会规范和价值的AI系统更近了一步。

    Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
    
[^64]: 扫描与拍照：理解1层Transformer中的训练动态和标记组成

    Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])

    [http://arxiv.org/abs/2305.16380](http://arxiv.org/abs/2305.16380)

    本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。

    

    Transformer架构在多个研究领域表现出了惊人的性能，并成为许多神经网络模型的基础。然而，我们对其如何工作的理解仍然有限。特别是，通过简单的预测性损失，表示如何从梯度训练动态中出现仍然是一个谜。在本文中，针对具有一个自我关注层和一个解码器层的1层Transformer，我们以数学严谨的方式分析其在下一个标记预测任务中的SGD训练动态。我们打开了自我关注层组合输入标记的动态过程的黑盒子，并揭示了底层归纳偏差的本质。具体而言，在没有位置编码、长输入序列和解码器层学习速度快于自我关注层的假设下，我们证明了自我关注层充当了“区分性扫描算法”：从均匀注意力开始，它逐渐关注到相关标记，排除不相关的标记，直到所有相关信息被扫描并总结在编码表示中。我们的分析还显示了标记频率和上下文如何影响注意权重，以及自我关注层初始化如何影响收敛速度。

    Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
    
[^65]: 用自动扩充基于扩散的方法丰富视觉数据集

    Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation. (arXiv:2305.16289v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16289](http://arxiv.org/abs/2305.16289)

    通过自动语言引导图像编辑，利用大型视觉和语言模型扩充训练数据集的方法，以增强细粒度分类任务中面对领域变化的模型的泛化能力。

    

    许多细粒度分类任务，如罕见动物识别，训练数据有限，因此在这些数据集上训练的分类器通常无法推广到领域中的变化，如天气或位置的变化。因此，我们探索了如何使用训练数据中所见领域的自然语言描述与在多样的预训练数据集上训练的大型视觉模型结合，生成训练数据的有用变化。我们引入了ALIA（自动语言引导图像增强），这是一种利用大型视觉和语言模型自动生成数据集领域的自然语言描述，并通过语言引导图像编辑来扩充训练数据的方法。为了保持数据的完整性，原始数据集训练的模型会滤除最小的图像编辑和那些损坏类相关信息的编辑。最终的数据集在视觉上与原始训练数据一致，并提供了显著增强的多样性。

    Many fine-grained classification tasks, like rare animal identification, have limited training data and consequently classifiers trained on these datasets often fail to generalize to variations in the domain like changes in weather or location. As such, we explore how natural language descriptions of the domains seen in training data can be used with large vision models trained on diverse pretraining datasets to generate useful variations of the training data. We introduce ALIA (Automated Language-guided Image Augmentation), a method which utilizes large vision and language models to automatically generate natural language descriptions of a dataset's domains and augment the training data via language-guided image editing. To maintain data integrity, a model trained on the original dataset filters out minimal image edits and those which corrupt class-relevant information. The resulting dataset is visually consistent with the original training data and offers significantly enhanced diver
    
[^66]: 如何通过概率电路将您的知识图嵌入转化为生成模型

    How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])

    [http://arxiv.org/abs/2305.15944](http://arxiv.org/abs/2305.15944)

    本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。

    

    一些成功的知识图嵌入模型可用作基于能量的模型，而这篇论文解释了这些模型的得分函数，将其重新解释成为电路形式--这是一种允许有效边际化的约束计算图。然后，我们设计了两个方法来获得有效的生成电路模型，其中一个方法是将其激活限制为非负数，另一个方法是将其输出平方。我们的解释不会影响到预测节点连边模型的性能，但电路框架使得MLE的精确学习、新三元组的有效抽样以及保证逻辑约束得以满足成为可能。此外，我们的模型在拥有数百万个实体的图上比原始的KGEs更具伸缩性。

    Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
    
[^67]: LayoutGPT: 使用大型语言模型进行组合式视觉规划和生成

    LayoutGPT: Compositional Visual Planning and Generation with Large Language Models. (arXiv:2305.15393v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.15393](http://arxiv.org/abs/2305.15393)

    LayoutGPT是一种使用大型语言模型的视觉规划和生成方法，通过从文本条件生成布局来提高用户对生成结果的控制能力。LayoutGPT在多个领域生成合理的布局，并优于其他文本到图像模型/系统的性能。

    

    在视觉生成中，实现高度的用户可控性通常需要精细的输入，如布局。然而，与简单的文本输入相比，这样的输入给用户带来了很大的负担。为了解决这个问题，我们研究了大型语言模型（LLMs）如何通过从文本条件中生成布局来充当视觉规划者，并与视觉生成模型进行协作。我们提出了LayoutGPT，一种使用样式表语言来组成上下文中的视觉示例，以增强LLMs的视觉规划能力的方法。LayoutGPT可以在多个领域生成合理的布局，从2D图像到3D室内场景。LayoutGPT还在将具有挑战性的语言概念（如数字和空间关系）转化为布局安排以进行忠实的文本到图像生成方面表现出优越的性能。当与下游图像生成模型结合时，LayoutGPT的性能优于文本到图像模型/系统20-40%，并达到可比较的性能水平。

    Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs. LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40% and achieves comparable performan
    
[^68]: 关于利用OPC UA进行强化学习的迷你评论

    A Mini Review on the utilization of Reinforcement Learning with OPC UA. (arXiv:2305.15113v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.15113](http://arxiv.org/abs/2305.15113)

    这项工作旨在通过提供关于强化学习和OPC UA的技术概述和综述，来解决将强化学习无缝集成到工业系统中的问题。

    

    强化学习（RL）是一种强大的机器学习范 paradigm，已应用于机器人技术、自然语言处理和游戏中，取得了最新的成果。针对解决顺序决策问题，它通过设计可以从经验中学习，因此适应不断变化的动态环境。这些能力使其成为控制和优化工业复杂过程的理想选择。充分利用这一潜力的关键是将RL无缝集成到现有工业系统中。工业通信标准Open Platform Communications Unified Architecture（OPC UA）可能弥补这一差距。然而，由于RL和OPC UA来自不同的领域，需要研究人员来弥合两种技术之间的差距。本研究通过提供简要的技术概述并进行半详尽的文献综述来弥合这一差距，以获得深入的见解。

    Reinforcement Learning (RL) is a powerful machine learning paradigm that has been applied in various fields such as robotics, natural language processing and game playing achieving state-of-the-art results. Targeted to solve sequential decision making problems, it is by design able to learn from experience and therefore adapt to changing dynamic environments. These capabilities make it a prime candidate for controlling and optimizing complex processes in industry. The key to fully exploiting this potential is the seamless integration of RL into existing industrial systems. The industrial communication standard Open Platform Communications UnifiedArchitecture (OPC UA) could bridge this gap. However, since RL and OPC UA are from different fields,there is a need for researchers to bridge the gap between the two technologies. This work serves to bridge this gap by providing a brief technical overview of both technologies and carrying out a semi-exhaustive literature review to gain insights
    
[^69]: 一种用于多元时间序列预测的联合时频域变换器

    A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting. (arXiv:2305.14649v1 [cs.LG])

    [http://arxiv.org/abs/2305.14649](http://arxiv.org/abs/2305.14649)

    本文提出了一种称为JTFT的方法，可以在提取时间依赖性的同时有效地减小计算需求，适用于多元时间序列预测，并具有线性复杂度。同时，采用低秩注意层以有效捕获跨维度的依赖关系，并在六个真实世界数据集上展示了比现有方法更好的表现。

    

    本文介绍了一种称为联合时频域变换器的方法，旨在提高多元时间序列的预测性能，同时最小化计算需求。该方法利用可学习频率的稀疏性，在频域有效地提取时间依赖性。除了频率域表示外，最近的有限数量的数据点也被直接编码在时间域中，以增强学习局部关系并减轻非平稳性的不利影响。JTFT具有线性复杂度，因为内部表示的长度保持独立于输入序列长度。此外，提出了一个低秩注意层，以有效捕获跨维度的依赖关系，并防止由于时间和通道建模的纠缠而导致性能降级。 对六个真实世界数据集进行的实验表明，JTFT优于现有的状态-of-the-art方法。

    To enhance predicting performance while minimizing computational demands, this paper introduces a joint time-frequency domain Transformer (JTFT) for multivariate forecasting. The method exploits the sparsity of time series in the frequency domain using a small number of learnable frequencies to extract temporal dependencies effectively. Alongside the frequency domain representation, a fixed number of the most recent data points are directly encoded in the time domain, bolstering the learning of local relationships and mitigating the adverse effects of non-stationarity. JTFT achieves linear complexity since the length of the internal representation remains independent of the input sequence length. Additionally, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies and prevent performance degradation due to the entanglement of temporal and channel-wise modeling. Experiments conducted on six real-world datasets demonstrate that JTFT outperforms state
    
[^70]: 等变神经模拟器用于随机时空动态

    Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics. (arXiv:2305.14286v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14286](http://arxiv.org/abs/2305.14286)

    本研究提出了一种称为EPNS的等变概率神经模拟框架，可以在系统演化中生成等变分布，并在随机时空动态方面表现出色。

    

    神经网络正在成为可扩展的数据驱动高维动态系统模拟工具，特别是在数值方法不可行或计算昂贵的情况下。值得注意的是，已经证明在确定性神经模拟器中引入域对称性可以大大提高其精确性、样本效率和参数效率。然而，为了将对称性纳入可以模拟随机现象的概率神经模拟器中，我们需要一个能够生成等变轨迹分布而不是等变函数逼近的模型。在本文中，我们提出了等变概率神经模拟（EPNS），这是一个用于等变分布系统演化的自回归概率建模框架。我们使用EPNS设计了一个用于随机N体系统和随机细胞动力学的模型。我们的结果表明，EPNS在p方面比现有的基于神经网络的方法表现出色。

    Neural networks are emerging as a tool for scalable data-driven simulation of high-dimensional dynamical systems, especially in settings where numerical methods are infeasible or computationally expensive. Notably, it has been shown that incorporating domain symmetries in deterministic neural simulators can substantially improve their accuracy, sample efficiency, and parameter efficiency. However, to incorporate symmetries in probabilistic neural simulators that can simulate stochastic phenomena, we need a model that produces equivariant distributions over trajectories, rather than equivariant function approximations. In this paper, we propose Equivariant Probabilistic Neural Simulation (EPNS), a framework for autoregressive probabilistic modeling of equivariant distributions over system evolutions. We use EPNS to design models for a stochastic n-body system and stochastic cellular dynamics. Our results show that EPNS considerably outperforms existing neural network-based methods for p
    
[^71]: 分层提示提升大规模语言模型在网络导航中的应用

    Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14257](http://arxiv.org/abs/2305.14257)

    这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。

    

    大规模语言模型（LLMs）在处理交互决策任务中的复杂观察时遇到困难。为了解决这个问题，我们提出了一种简单的分层提示方法。不同于以往总是把\emph{完整}观察（例如网页）放到提示中的提示方法，我们提出首先构建一个与动作相关的\emph{压缩}和\emph{相关}的观察，并使用专门的\summ提示。然后，\actor提示根据总结的观察预测下一个动作。尽管我们的方法具有广泛的适用性，但我们尤其展示了它在复杂的网络导航领域的有效性，其中完整的观察通常包含冗余和无关信息。我们的方法在任务成功率上优于先前最先进的提示机制6.2\%，展示了其在具有长时间观察轨迹的交互决策任务中的潜力。

    Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
    
[^72]: 具有合理性的语言模型

    Language Models with Rationality. (arXiv:2305.14250v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14250](http://arxiv.org/abs/2305.14250)

    本研究提出了一种名为REFLEX的方法，在大型语言模型上添加了合理性和自反性层，以使模型的答案得以解释并消除潜在的矛盾。

    

    虽然大型语言模型(Large Language Models, LLMs)在问答中非常擅长，但它们的答案与其内在的“信念”之间的关系往往不明确。这种缺乏解释性阻碍了LLMs的广泛使用。为了解决这个问题，我们的目标是使模型的信念以及它们的推理关系变得明确，并消除可能存在的矛盾，以便答案能够通过从一致的信念网络中得出的可解释的推理链来支持。我们的方法名为REFLEX，在LLM之上添加了一个具有合理性和自反性的层。首先，给定一个问题，我们使用反向链接过程构建一个信念图，以实现相关模型信念(包括对答案候选者的信念)及其推理关系。其次，我们使用形式约束推理器识别和最小化该图中的矛盾。我们发现，REFLEX显著提高了一致性(绝对值提升了8%-11%)，而不损害已有的性能。

    While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent "beliefs". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming ov
    
[^73]: 通过子4位整数量化实现压缩大型语言模型的内存高效微调

    Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. (arXiv:2305.14152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14152](http://arxiv.org/abs/2305.14152)

    本文提出了一种称为PEQA的方法，结合了参数高效微调和量化LLM的优点。通过仅更新量化尺度，PEQA可以高效地微调压缩大型语言模型，并减少内存开销。

    

    大型语言模型（LLM）面临着在微调和部署过程中由于其高内存需求和计算成本而带来的挑战。虽然参数高效微调（PEFT）方法旨在减少微调过程中优化器状态的内存使用量，但预训练LLM权重本身的大小仍然是一个紧迫的问题。尽管量化技术被广泛提出来缓解内存需求和加快LLM推理速度，但大多数这些技术都是针对部署阶段。为了弥合这一差距，本文提出了参数高效和量化感知适应（PEQA）-一种简单而有效的方法，将PEFT的优点与量化LLM结合起来。通过仅更新量化尺度，PEQA可以直接应用于量化LLM，确保平稳的任务转换。与现有的PEFT方法并行，PEQA显着减少了与优化器状态相关的内存开销。此外，它还充分利用了

    Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages o
    
[^74]: LogiCoT：基于GPT-4的逻辑思维指令调整数据收集。

    LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])

    [http://arxiv.org/abs/2305.12147](http://arxiv.org/abs/2305.12147)

    该论文提出了LogiCoT, 一个基于GPT-4的逻辑思维指令调整数据集，用于教授模型逻辑推理和引出一般推理技能。

    

    生成式预训练变压器4（GPT-4）展示了令人印象深刻的思维链推理能力。最近的自我指导调整研究（如Alpaca）侧重于增强模型的通用能力。这些指令使模型在一般任务（如开放领域文本生成和释义）上能够达到与GPT-3.5相当的性能。然而，它们不能帮助模型处理复杂的推理任务。为填补这一差距，本文提出了LogiCoT，一种新的逻辑思维指令调整数据集，用于GPT-4的逻辑思维链推理。我们详细阐述了收集指令以提示GPT-4生成思维链推理的过程。LogiCoT作为教授逻辑推理模型的指令集，并引出了一般推理技能。

    Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
    
[^75]: STOAT: 结构化数据控制性分析文本生成

    STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])

    [http://arxiv.org/abs/2305.11826](http://arxiv.org/abs/2305.11826)

    STOAT模型是表格和推理意识的生成模型，在数字推理、常识推理、时间推理、表格知识和实体知识方面有较好的控制，提高了分析句子生成的质量和准确度。

    

    最近，语言模型在结构化数据到文本生成任务中取得了巨大的进展。然而，当需要进行逻辑推理以生成描述时，这些模型仍然表现出次优的性能。在本文中，我们特别关注从结构化数据（例如表格）生成分析文本。在（Gupta et al.,2020）提出的分类基础上，我们重点关注以下推理类别的可控制表格到文本生成：数字推理、常识推理、时间推理、表格知识和实体知识。我们提出了STOAT模型，该模型具有表格和推理意识，并通过矢量量化将给定的推理类别注入输出中。我们观察到，在分析句子任务中，我们的模型在iToTTo和Infotabs的PARENT指标上分别提供了10.19％和1.13％的优化。我们还发现，与基线模型相比，我们的模型生成的描述更加准确和分析，人类评估中增加了15.3％。

    Recent language models have made tremendous progress in the structured data to text generation task. However, these models still give sub-optimal performance where logical inference is required to generate the descriptions. In this work, we specifically focus on analytical text generation from structured data such as tables. Building on the taxonomy proposed in (Gupta et al., 2020) we focus on controllable table to text generation for the following reasoning categories: numerical reasoning, commonsense reasoning, temporal reasoning, table knowledge, and entity knowledge. We propose STOAT model, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output. We observe that our model provides 10.19%, 1.13% improvement on the PARENT metric in iToTTo and Infotabs for the analytical sentence task. We also found that our model generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.
    
[^76]: 语言模型遇见世界模型：实体经验增强语言模型

    Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])

    [http://arxiv.org/abs/2305.10626](http://arxiv.org/abs/2305.10626)

    本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。

    

    虽然大型语言模型 (LMs) 在许多任务上表现出了非凡的能力，但它们常常在处理物理环境下的简单推理和规划问题时遇到困难，例如理解物体永恒或规划家庭活动。这种限制源于 LM 仅受书面语言训练，缺少必要的实体知识和技能。本文提出了一种新的增强 LM 的方法，即将其与世界模型相结合进行微调，以获得多样化的实体知识，同时保持其一般语言能力。本方法在世界模型中部署一个融入实体经验的代理，特别是一个模拟物理世界的仿真器(VirtualHome)，通过有目的的规划和随机探索获得多样化的实体经验。然后，利用这些经验微调 LM ，以教授在物理世界中的各种推理和行为能力，例如规划和完成目标、物体永恒和跟踪等。此外，我们相信我们的方法可以轻松扩展以利用其他模拟器，包括机器人或自动驾驶汽车。我们证明了我们的方法显着提高了 LM 在一系列物理推理任务中的性能，同时保留并经常提高了它们在语言基准上的表现。

    While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
    
[^77]: 阅读过程中对小说人物个性的理解

    Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])

    [http://arxiv.org/abs/2305.10156](http://arxiv.org/abs/2305.10156)

    本文提出了一个NLP领域内尚未研究的问题：情景和细致地理解小说人物个性，并提供了第一个标记数据集PersoNet来解决这个问题。

    

    理解小说人物个性是阅读故事的关键。随着读者与故事的互动，他们对一个人物的理解会根据新的事件和信息而演变；并且可以感知到多个精细的个性方面。这导致了一个自然的问题：情境和精细的个性理解。这个问题在NLP领域中没有得到研究，主要是由于缺乏模仿阅读过程的适当数据集。我们提供了第一个标记数据集PersoNet来解决这个问题。我们的新型注释策略涉及用在线阅读应用程序的用户笔记作为原始书籍的代理进行注释。实验和人体研究表明，我们的数据集构建既有效又准确；我们的任务在很大程度上依赖于长期的上下文以实现对机器和人类的准确预测。数据集可在https://github.com/Gorov/personet_acl23获得。

    Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
    
[^78]: 语言模型能否用自然语言解决图问题？

    Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])

    [http://arxiv.org/abs/2305.10037](http://arxiv.org/abs/2305.10037)

    本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。

    

    大型语言模型越来越多地应用于一些具有隐式图形结构的任务，例如机器人规划、多跳问题回答或知识探索、结构化常识推理等等。虽然LLM在这些任务中已经取得了一定的进展，但是LLM是否能够显式处理图形的文本描述，将它们映射到基于概念的空间中，并执行结构化操作仍然尚未得到足够的研究。为此，我们提出了自然语言图形(NLGraph)，它是一个设计用于自然语言的基于图形问题解决全面测试。NLGraph包含29,370个问题，涵盖了八个图形推理任务，从简单的连接和最短路径到复杂的最大流和模拟图神经网络等任务不等。我们在NLGraph基准测试上评估了LLM(GPT-3/4)，并发现1)语言模型具有相应的图形推理能力；

    Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
    
[^79]: RelaMiX: 探索视频中基于动作识别的少样本自适应

    RelaMiX: Exploring Few-Shot Adaptation in Video-based Action Recognition. (arXiv:2305.08420v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.08420](http://arxiv.org/abs/2305.08420)

    本文研究了基于视频的动作识别中少样本自适应的问题，并提出了FSDA-AR方法，通过利用少量标记的目标视频实现有效的自适应。实验证明FSDA-AR与无监督领域自适应的性能相当。

    

    领域自适应对于活动识别至关重要，以确保在不同的环境、传感器类型和数据来源下能够准确和稳健地进行性能表现。无监督领域自适应方法已被广泛研究，然而，它们需要来自目标领域的大规模无标签数据。在本文中，我们解决了基于视频的活动识别的少样本领域自适应问题（FSDA-AR），它利用了少量的标记目标视频来实现有效的自适应。这种设置对于应用程序来说是有吸引力和有前景的，因为它只需要在目标领域中记录和标记很少甚至一个样本的类别，包括那些罕见但至关重要的活动。我们使用五个已建立的数据集构建了FSDA-AR基准，考虑了不同的领域类型：UCF101，HMDB51，EPIC-KITCHEN，Sims4Action和ToyotaSmartHome。我们的结果表明，FSDA-AR与无监督领域自适应的性能相当。

    Domain adaptation is essential for activity recognition to ensure accurate and robust performance across diverse environments, sensor types, and data sources. Unsupervised domain adaptation methods have been extensively studied, yet, they require large-scale unlabeled data from the target domain. In this work, we address Few-Shot Domain Adaptation for video-based Activity Recognition (FSDA-AR), which leverages a very small amount of labeled target videos to achieve effective adaptation. This setting is attractive and promising for applications, as it requires recording and labeling only a few, or even a single example per class in the target domain, which often includes activities that are rare yet crucial to recognize. We construct FSDA-AR benchmarks using five established datasets considering diverse domain types: UCF101, HMDB51, EPIC-KITCHEN, Sims4Action, and ToyotaSmartHome. Our results demonstrate that FSDA-AR performs comparably to unsupervised domain adaptation with significantl
    
[^80]: k-匿名和合成数据技术的能量成本和机器学习准确性影响。

    Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])

    [http://arxiv.org/abs/2305.07116](http://arxiv.org/abs/2305.07116)

    本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。

    

    为了解决与隐私和气候变化有关的愈发增长的社会关切，欧盟颁布了《通用数据保护条例》(GDPR)并承诺了绿色协议。大量研究探究了运用匿名数据集训练机器学习模型的能效和准确性。最近的研究开始探究隐私增强技术（PET）对机器学习模型的能量消耗和准确性的影响，重点关注k-匿名。由于合成数据越来越受欢迎，因此本文分析了两个阶段的能量消耗和准确性：a）将隐私增强技术应用于相关数据集，b）在相关隐私增强数据集上训练模型。我们使用两种隐私增强技术：k-匿名化（使用泛化和抑制）和合成数据，以及三种机器学习模型。每个模型都在每个隐私增强数据集上进行训练。结果显示，在经过k-匿名化的数据上训练的模型具有较低的准确性，并且消耗的能量较少，与在非匿名化数据上训练的模型相比。然而，k-匿名化过程中消耗的能量非常可观，在评估其有用性时必须将其考虑在内。合成数据证明是一种有前途的选择，因为它在消耗更少能源的情况下实现了与非匿名化数据可比的准确性。

    To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
    
[^81]: 多智能体强化学习中的信息设计

    Information Design in Multi-Agent Reinforcement Learning. (arXiv:2305.06807v1 [cs.GT])

    [http://arxiv.org/abs/2305.06807](http://arxiv.org/abs/2305.06807)

    本文探究了多智能体强化学习中的信息设计问题及其挑战，提出了“马尔科夫信令博弈”的概念。

    

    强化学习（RL）模仿人类和动物与环境交互的方式。然而实际环境中存在其他有自己目标的智能体，它们会适应地与自己相互作用。因此，为了在这些环境中成功，自主智能体需要影响其他智能体以使它们的行为更有益。信息设计是影响其他智能体行为的一种方法。本文探讨了针对一组RL代理的信息设计问题，并提出了“马尔科夫信令博弈”的概念。

    Reinforcement learning (RL) mimics how humans and animals interact with the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receivers are willing to respect. We formulate the Markov signaling game, and develop the notions 
    
[^82]: The Vault：一个全面的多语言数据集，为促进代码理解和生成而设计

    The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])

    [http://arxiv.org/abs/2305.06156](http://arxiv.org/abs/2305.06156)

    The Vault是一个提供了10种流行编程语言的40百万行代码-文本对的开源数据集，旨在增强面向代码的大型语言模型（LLM）的训练，有望在代码理解和生成任务上取得显著进展。

    

    我们介绍了 The Vault，这是一个开源的大规模代码文本数据集，旨在增强面向代码的大型语言模型（LLM）的训练。现有的用于训练基于代码的LLM的开源数据集在大小、质量(由于噪声信号)和格式（仅包含代码函数和文本说明配对）方面经常面临挑战。The Vault通过提供10种流行编程语言的40百万行代码-文本对，彻底清除10种多样的问题，以及各种级别的代码-文本对，包括类、函数和代码行等级别，来克服这些限制。研究人员和从业人员可以利用The Vault来训练不同的面向代码的LLM，或者将提供的数据清洗方法和脚本合并到自己的数据集中来改进数据集。通过将The Vault作为面向代码的LLMs的训练数据集，我们预计在代码理解和生成任务上取得显著进展，促进人工智能研究和实践的发展。

    We present The Vault, an open-source, large-scale code-text dataset designed to enhance the training of code-focused large language models (LLMs). Existing open-source datasets for training code-based LLMs often face challenges in terms of size, quality (due to noisy signals), and format (only containing code function and text explanation pairings). The Vault overcomes these limitations by providing 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings, including class, function, and line levels. Researchers and practitioners can utilize The Vault for training diverse code-focused LLMs or incorporate the provided data cleaning methods and scripts to improve their datasets. By employing The Vault as the training dataset for code-centric LLMs, we anticipate significant advancements in code understanding and generation tasks, fostering progress in both artificial intelligence research and so
    
[^83]: 基于Segment Anything Model (SAM)增强伪标签的弱监督语义分割

    Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])

    [http://arxiv.org/abs/2305.05803](http://arxiv.org/abs/2305.05803)

    本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。

    

    仅使用图像级别的监督的弱监督语义分割(WSSS)由于其与像素级注释相比的低成本而越来越受到关注。大多数现有方法依赖于类激活图(CAM)生成像素级的伪标签进行监督训练。然而，众所周知，CAM经常遭受局部激活的限制-只激活最具区分性的部分，而不是整个对象区域和虚假的激活-不必要地激活物体周围的背景。本研究引入了一种简单而有效的方法来解决这些限制，即利用最近发布的Segment Anything Model (SAM)增强CAM生成高质量的伪标签。SAM是一个分割基础模型，展示了将图像分割成段落的强零-shot能力，但缺乏这些区域的语义标签。为了解决这个问题，我们采用特定类别的伪标签作为信号来选择m。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
    
[^84]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^85]: 基于翻译对齐的视觉语言模型跨语言迁移的参数高效方法

    Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])

    [http://arxiv.org/abs/2305.03510](http://arxiv.org/abs/2305.03510)

    本文提出了一个通过翻译对齐的方式实现参数高效、跨语言的迁移学习框架，实验结果显示该框架能够显著减少多语言之间的性能差异。

    

    预训练的视觉语言模型（如CLIP）在连接图像和英语文本方面取得了显著的成功。尽管最近试图扩展CLIP以支持其他语言，但由于资源不平衡，观察到了不同语言之间的性能差异。此外，当前的预训练模型的跨语言迁移方法会消耗大量资源。因此，我们提出了一种新的参数高效的跨语言迁移学习框架，利用基于翻译的对齐方法来减轻多语言差异，并探索参数高效的微调方法来实现参数高效的跨语言迁移。在XTD和Multi30K数据集上进行了广泛的实验，涵盖了零-shot、few-shot和全数据集学习场景下的11种语言，结果显示我们的框架显著减少了语言之间的多语言差异，并提高了性能。

    Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves 
    
[^86]: 联邦学习与O-RAN的协同：面向多个分布式机器学习服务的弹性虚拟化架构

    Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])

    [http://arxiv.org/abs/2305.02109](http://arxiv.org/abs/2305.02109)

    本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。

    

    联邦学习是最流行的分布式机器学习技术，但是在现代无线网络中实现联邦学习面临着许多挑战，主要包括网络条件的动态性、系统中多个联邦学习服务/任务的并存以及联邦学习服务与其他网络服务的并行执行等。针对这些挑战，本文提出了一种名为动态多服务联邦学习（DMS-FL）的联邦学习泛型架构，并通过提出一种新的分布式机器学习架构——弹性虚拟化联邦学习（EV-FL）来解决DMS-FL中的三个未探索的设计问题。

    Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
    
[^87]: 荆棘玫瑰：探究自然语言处理中的双重使用困境

    Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08315](http://arxiv.org/abs/2304.08315)

    本文调查了自然语言处理（NLP）领域的双重使用问题，提出了一份定制的双重使用定义，并讨论了当前的状况和可能的挑战。

    

    双重使用是指有意将技术和科学成果用于有害目的的问题，在自然语言处理（NLP）领域尚未明确定义。然而，随着NLP技术的不断发展和在社会中的广泛应用，其内部运行方式变得越来越不透明。因此，理解双重使用的问题以及限制双重使用的潜在方法对于减少研究和开发的潜在危害至关重要。在本文中，我们对NLP研究人员和从业者进行了调查，以了解他们对该问题的深度理解和观点，并评估现有的支持情况。根据调查结果，我们为NLP社区提供了一份定制的双重使用定义。调查结果显示，大多数研究人员对他们的研究的潜在双重使用问题表示关切，但只采取有限的行动。基于调查结果，我们讨论了当前的状况和可能的挑战。

    Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and p
    
[^88]: 面部视频压缩的感知质量评估：基准和有效方法

    Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])

    [http://arxiv.org/abs/2304.07056](http://arxiv.org/abs/2304.07056)

    本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。

    

    近年来，对面部视频压缩的需求呈指数级增长，人工智能的成功使得超出了传统的混合视频编码范围。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。然而，空间和时间域中扭曲类型的极大多样性，从传统的混合编码框架到生成模型，给压缩面部视频质量评估（VQA）带来了巨大挑战。在本文中，我们介绍了大规模压缩面部视频质量评估（CFVQA）数据库，这是系统地了解面部视频感知质量和多样化压缩失真的第一次尝试。该数据库包含 3,240 个压缩的面部视频片段，涵盖多个压缩级别，这些片段来自 135 个源视频，具有多样性。

    Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
    
[^89]: 使AI“口渴”减少的方法：揭示和解决AI模型的秘密水消耗

    Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])

    [http://arxiv.org/abs/2304.03271](http://arxiv.org/abs/2304.03271)

    本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。

    

    人工智能（AI）模型的碳足迹不断增长，特别是像GPT-3和GPT-4这样的大型模型，已经受到公众的关注。然而，同等重要且巨大的AI模型水印尚未引起人们的注意。例如，在微软最先进的美国数据中心中训练GPT-3可以直接消耗70万升清洁淡水（相当于生产370辆宝马汽车或320辆特斯拉电动汽车），如果在微软的亚洲数据中心进行训练，这个水消耗量将增加三倍，但这样的信息一直被保密。这极其令人担忧，因为淡水短缺已成为在人口迅速增长、水资源减少和老化的水基础设施的背景下，我们所有人面临的最紧迫的挑战之一。为了应对全球水资源的挑战，人工智能模型可以，而且应该，承担社会责任，以身作则解决自己的问题。

    The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
    
[^90]: MEGClass: 通过相互增强的文本粒度实现极弱监督文本分类。

    MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])

    [http://arxiv.org/abs/2304.01969](http://arxiv.org/abs/2304.01969)

    MEGClass是一种通过相互增强的文本粒度实现极弱监督文本分类的方法，利用分层关注机制从文档、句子和单词中提取连贯且多样的子文本，能够准确分类讨论多个主题的文档，并且在五个基准数据集上表现优于现有最先进的模型。

    

    文本分类通常需要大量的人工标注数据作为监督，这在动态新兴领域中是昂贵的。某些方法通过仅依赖类名表面文本作为极弱监督来解决这个问题。然而，现有方法未能考虑到单一类别文档讨论多个主题的情况。主题多样性和模糊的句子可能会引入噪声到文档的底层表示，从而影响预测类别的精度。此外，当前的方法独立地关注文档、句子或单词的文本粒度，从而限制了我们联合从所有三者中提取粗粒度或细粒度上下文的能力来识别分类的重要子文本。为了解决这个问题，我们提出了MEGClass，一种利用相互增强的文本粒度进行极弱监督文本分类的方法。具体来说，MEGClass通过分层关注机制从文档、句子和单词中提取连贯且多样的子文本，使我们能够识别和整合来自多个粒度的弱信号，以准确分类文档，即使它们讨论多个主题。在五个基准数据集上的实验结果表明，我们的方法在使用极弱监督的情况下，比现有最先进的模型有着更好的表现。

    Text classification typically requires a substantial amount of human-annotated data to serve as supervision, which is costly to obtain in dynamic emerging domains. Certain methods seek to address this problem by solely relying on the surface text of class names to serve as extremely weak supervision. However, existing methods fail to account for single-class documents discussing multiple topics. Both topic diversity and vague sentences may introduce noise into the document's underlying representation and consequently the precision of the predicted class. Furthermore, current work focuses on text granularities (documents, sentences, or words) independently, which limits the degree of coarse- or fine-grained context that we can jointly extract from all three to identify significant subtext for classification. In order to address this problem, we propose MEGClass, an extremely weakly-supervised text classification method to exploit Mutually-Enhancing Text Granularities. Specifically, MEGC
    
[^91]: Pgx:强化学习硬件加速的并行游戏模拟器

    Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])

    [http://arxiv.org/abs/2303.17503](http://arxiv.org/abs/2303.17503)

    Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。

    

    我们提出了Pgx，这是一个用JAX编写的棋盘游戏模拟器集合。由于JAX的自动向量化和即时编译功能，Pgx易于在GPU/TPU加速器上进行大规模并行执行。我们发现，在单个A100 GPU上的Pgx模拟比现有的强化学习库快10倍。Pgx实现了被认为是人工智能研究中至关重要的基准测试的游戏，如Backgammon，Shogi和Go。 Pgx可在https://github.com/sotetsuk/pgx获得。

    We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
    
[^92]: 零样本图像净化的黑盒后门防御方法

    Black-box Backdoor Defense via Zero-shot Image Purification. (arXiv:2303.12175v1 [cs.CV])

    [http://arxiv.org/abs/2303.12175](http://arxiv.org/abs/2303.12175)

    本文提出了一种黑盒后门防御框架，利用零样本图像净化有效地防御各种攻击，无需任何内部信息或先前知识，通过对受污染的图像进行线性变换和预训练的扩散模型恢复缺失语义信息实现。

    

    后门攻击会将毒数据注入训练集，导致模型推理时样本错误分类。在仅有模型预测可用的实际黑盒环境中，防御此类攻击是具有挑战性的。本文提出了一种新颖的后门防御框架，可以通过零样本图像净化（ZIP）有效地抵御各种攻击。我们的防御框架可以应用于黑盒模型，不需要任何关于受污染模型的内部信息或任何关于干净/受污染样本的先前知识。我们的防御框架包括两个步骤。首先，我们对受污染的图像应用线性变换以破坏触发模式。然后，我们使用预训练的扩散模型来恢复由变换去除的缺失语义信息。特别地，我们设计了一个新的反向过程，使用变换后的图像来引导高保真度净化图像的生成。

    Backdoor attacks inject poisoned data into the training set, resulting in misclassification of the poisoned samples during model inference. Defending against such attacks is challenging, especially in real-world black-box settings where only model predictions are available. In this paper, we propose a novel backdoor defense framework that can effectively defend against various attacks through zero-shot image purification (ZIP). Our proposed framework can be applied to black-box models without requiring any internal information about the poisoned model or any prior knowledge of the clean/poisoned samples. Our defense framework involves a two-step process. First, we apply a linear transformation on the poisoned image to destroy the trigger pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process using the transformed image to guide the generation of high-fidelity purified 
    
[^93]: 零和马尔可夫博弈中强化学习的新政策迭代算法

    A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])

    [http://arxiv.org/abs/2303.09716](http://arxiv.org/abs/2303.09716)

    本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。

    

    许多基于模型的强化学习算法可以被视为具有两个阶段: 学习阶段和规划阶段。在标准MDPs情况下，可以使用价值迭代或策略迭代来解决学习问题。但在零和马尔可夫博弈的情况下，没有有效的策略迭代算法，以前的尝试都有局限性。本文提出了一种简单的策略迭代变体，能够有效地解决这个问题。

    Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
    
[^94]: 基于Transformer的符号回归规划

    Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06833](http://arxiv.org/abs/2303.06833)

    该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。

    

    符号回归是机器学习中一项具有挑战性的任务，它涉及基于函数值查找其数学表达式。最近，符号回归的一些进展表明，预训练的基于Transformer的模型对于生成方程序列是有效的，这些模型从合成数据集的大规模预训练中获益，并在推理时间方面比基于GP的方法具有显著优势。然而，这些模型关注的是借鉴文本生成的监督预训练目标，而忽略了方程的特定目标，如准确性和复杂性。为了解决这个问题，我们提出了TPSR，一种基于Transformer的符号回归规划策略，将蒙特卡罗树搜索融入到Transformer解码过程中。与传统的解码策略不同，TPSR允许将非可微的反馈（如拟合准确性和复杂性）作为知识的外部来源融入到方程生成过程中。

    Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
    
[^95]: Cal-QL: 计算机辅助脱机强化学习预先训练用于高效在线微调

    Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05479](http://arxiv.org/abs/2303.05479)

    本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。

    

    脱机强化学习方法可以用来从现有数据集中获取策略初始化并通过有限互动进行快速在线微调。然而，现有的脱机强化学习方法在在线微调中表现较差。本文研究了保守脱机强化学习方法中的微调问题，并设计了一种方法，可以从脱机数据中学习到有效的初始化，并使其具备快速的在线微调功能。我们的方法，即Cal-QL，通过学习一个保守的值函数初始化，低估从脱机数据中学到的策略的价值，同时确保学习到的Q值在合理的范围内。实验结果表明，我们的方法在多个基准环境中具有显著的性能优势，并且也能在真实机器人问题中进行有效的在线微调。

    A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
    
[^96]: 利用不对称性进行合成训练数据生成：SynthIE和信息提取案例

    Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04132](http://arxiv.org/abs/2303.04132)

    本文展示了利用不对称性进行合成训练数据生成的方法，可以针对结构化输出问题产生大规模、高质量的数据。在封闭信息提取任务中，我们合成生成了一个包含180万数据点的数据集，并利用此数据集对小型模型进行微调，取得了远超先前领先技术的成果。

    

    大型语言模型（LLM）在合成数据生成方面有着巨大的潜力。这项工作表明，即使对于LLM无法直接解决的任务，也可以合成生成有用的数据：对于具有结构化输出的问题，可以提示LLM在反向方向上执行任务，通过为目标输出结构生成合理的输入文本。利用任务困难度的不对称性，可以生成大规模、高质量的复杂任务数据。我们在封闭信息提取方面展示了这种方法的有效性，该领域难以收集到真实数据，至今没有令人满意的数据集存在。我们合成生成了一个包含180万数据点的数据集，并在人工评估中证明其与现有数据集相比具有更好的质量，并利用该数据集对小型模型（220M和770M参数）进行微调，这些模型被称为SynthIE，以远远超过先前领先技术的水平（具有相同的模型大小）。

    Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 
    
[^97]: 通过离线强化学习学习影响人类行为

    Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02265](http://arxiv.org/abs/2303.02265)

    本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。

    This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.

    在现实世界中，学习代理与人类互动是最复杂的设置之一，因为人类往往由于复杂的偏见而表现出次优的、不可预测的行为。在这种情况下与人类互动的代理最终会影响这些人所采取的行动。我们的目标是使代理能够利用这种影响来提高人类在协作任务中的表现，随着任务的展开。与以前的工作不同，我们不假设与人员进行在线培训（这往往太昂贵和不安全），也不假设有高保真度环境模拟器的访问权限。我们的想法是，通过采用各种先前观察到的人类-人类交互数据并将其标记为任务奖励，离线强化学习（RL）可以学习组合行为的组件，并发现导致更理想的人类行为的行动。首先，我们展示了离线RL可以学习策略来影响和改善人类行为，尽管这些策略可能与人类的期望不同。

    In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
    
[^98]: SHAP-IQ: 任意阶Shapley interaction的统一逼近方法

    SHAP-IQ: Unified Approximation of any-order Shapley Interactions. (arXiv:2303.01179v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01179](http://arxiv.org/abs/2303.01179)

    提出了一种名为SHAP-IQ的新方法，用于计算任意阶Shapley互动，并提供了逼近质量的理论保证和方差估计。该方法在计算成本和逼近质量方面优于现有方法。

    

    在可解释的人工智能（XAI）研究中，Shapley值（SV）通常被应用于确定任何黑盒模型的特征重要性得分。 Shapley interaction indices将SV扩展为定义任意阶特征相互作用得分。定义独特的Shapley interaction index是一个开放性研究问题，迄今为止已经提出了三个定义，其不同之处在于所选择的公理。此外，每个定义都需要特定的逼近技术。在这里，我们提出了基于采样的有效逼近方法SHAPley Interaction Quantification（SHAP-IQ），以计算任意基数交互指数（CII）的Shapley互动。即满足线性、对称和虚拟公理的交互指数。SHAP-IQ基于一种新颖的表示方法，与现有方法相比，我们为其逼近质量提供了理论保证，以及点估计的方差估计。对于SV的特殊情况，我们的逼近方法与精确计算一致。进行了数值实验，以证明我们的方法在几个合成和实际数据集上的有效性，并显示SHAP-IQ在计算成本和逼近质量方面优于现有方法。

    Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature importance scores for any black box model. Shapley interaction indices extend the SV to define any-order feature interaction scores. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our app
    
[^99]: 量化和建模多模态交互：一种信息分解框架

    Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12247](http://arxiv.org/abs/2302.12247)

    通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。

    

    对于解决多模态任务所需的交互如何进行量化？最适合捕捉这些交互的多模态模型是什么？为了回答这些问题，我们提出了一种信息论方法来量化输入模态与输出任务之间的冗余度、独特性和协同性。我们将这三个衡量标准称为多模态分布（或简称PID）的PID统计量，并引入了两个新的PID统计估计器，适用于高维分布。为了验证PID估计，我们在已知PID的合成数据集和大规模多模态基准测试集上进行了大量实验。

    The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
    
[^100]: 离线强化学习用于混合专家对话管理

    Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10850](http://arxiv.org/abs/2302.10850)

    本论文研究了离线强化学习在混合专家对话管理中的应用。通过利用最新的混合专家语言模型（MoE-LMs），我们开发了针对对话规划的强化学习算法，显著减少了动作空间的大小，并提高了对话管理的有效性。

    

    强化学习（RL）在开发对话管理（DM）代理，实现非目标导向，进行富有内容的对话，最大化用户满意度方面表现出了巨大的潜力。尽管强化学习和语言模型（LMs）最近取得了进展，但使用强化学习驱动的对话聊天机器人仍然具有挑战性，部分原因是强化学习需要在线探索以有效学习，而收集新颖的人机交互可能既昂贵又不安全。这个问题在面对这些算法的组合动作空间时变得更为严重，因为大多数语言模型代理以词级别生成响应。我们开发了多种针对对话规划的强化学习算法，利用最新的混合专家语言模型（MoE-LMs） - 一种捕捉多样语义，生成反映不同意图的话语的模型，适用于多轮对话管理。通过利用MoE-LM结构，我们的方法显著减少了行动空间的大小，并提高了基于强化学习的对话管理的有效性。

    Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.
    
[^101]: Jaccard度量损失：使用软标签优化Jaccard指数

    Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05666](http://arxiv.org/abs/2302.05666)

    本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。

    

    IoU损失是直接优化Jaccard指数的替代品。在语义分割中，将IoU损失作为损失函数的一部分，与仅优化像素损失（如交叉熵损失）相比，对于Jaccard指数测量表现更好。最显着的IoU损失是软Jaccard损失和Lovasz-Softmax损失。然而，这些损失与机器学习中普遍存在的软标签不兼容。在本文中，我们提出了Jaccard度量损失（JMLs），它们在标准设置下与软标签兼容，与软Jaccard损失相同。使用JMLs，我们研究了两种最流行的软标签用例：标签平滑和知识蒸馏。在三个语义分割数据集（Cityscapes、PASCAL VOC和DeepGlobe Land）上，我们的实验表明，与交叉熵损失相比，我们的简单方法显著提高了性能，并且在DeepGlobe Land数据集上超过了最先进的方法。

    IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
    
[^102]: 描述、解释、规划和选择：大型语言模型启用开放世界多任务智能体的交互规划

    Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents. (arXiv:2302.01560v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.01560](http://arxiv.org/abs/2302.01560)

    本论文研究了在开放世界环境中多任务具身智能体的任务规划挑战，提出了一种基于大型语言模型的交互式规划方法(DEPS)来解决计划执行的准确性和效率问题。

    

    我们研究了开放世界环境中多任务具身智能体的任务规划挑战。我们发现了两个主要困难：1）在开放世界环境（如Minecraft）中执行计划需要准确的多步推理，因为任务是长期性的；2）由于传统规划器不考虑当前智能体完成给定子任务的难度，在复杂计划中对并行子目标进行排序可能导致计划低效甚至不可行。为此，我们提出了基于大型语言模型（LLMs）的“描述、解释、规划和选择”（DEPS）交互式规划方法。DEPS通过整合计划执行过程的描述和在规划阶段遇到失败时提供自我解释的反馈，实现了对初始LLM生成的计划的更好的错误修正。

    We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and $\underline{S}$elect" ($\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of the plan execution process and providing self-$\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore
    
[^103]: 大型Transformer模型的隐藏表示的几何学

    The geometry of hidden representations of large transformer models. (arXiv:2302.00294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00294](http://arxiv.org/abs/2302.00294)

    大型Transformer模型中的隐藏表示具有类似的几何和统计特性，随着层级的移动，它们在最初的几层中变得高维，然后在中间层中显著收缩，在模型的最后部分，保持恒定或形成第二个浅峰。在第一个峰值结束时，数据集的语义信息被更好地表达。

    

    大型Transformer模型是用于自监督数据分析的强大架构，可以处理包括蛋白质序列、图像和文本在内的各种数据类型。在这些模型中，数据集的语义结构通过一个表示与下一个表示之间的一系列变换而出现。我们表征了这些表示的几何和统计特性，以及它们在层级移动时的变化。通过分析内在维度（ID）和邻居组成，我们发现在训练在蛋白质语言任务和图像重建任务上的Transformer模型中，表示以相似的方式演化。在最初的几层中，数据流形扩展，变得高维，然后在中间层中显著收缩。在模型的最后部分，ID保持大致恒定或形成第二个浅峰。我们展示了数据集的语义信息在第一个峰值结束时更好地表达，这一现象可以被观察到。

    Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers. By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be ob
    
[^104]: 与人类表征的一致性支持鲁棒的少样本学习

    Alignment with human representations supports robust few-shot learning. (arXiv:2301.11990v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11990](http://arxiv.org/abs/2301.11990)

    论文提出少样本学习的表现与人类表征的一致性存在U形关系，并通过计算机视觉模型的实验进行了验证。高度对齐的模型更加鲁棒，对数据的利用更加有效，但与人类对齐并非必要条件。

    

    我们是否应该关心AI系统是否具有与人类相似的世界表征？我们提供了一个信息论分析，建议在少样本学习任务的表现度与人类表征的一致性之间应该存在一个U形关系。我们通过对491个计算机视觉模型的性能分析验证了这个预测的可行性，并且表明高度对齐的模型更加鲁棒于对抗攻击和域偏移。我们的结果表明，与人类对齐往往是模型有效利用有限数据、鲁棒性 以及泛化能力的充分但不必要条件。

    Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.
    
[^105]: 用小的条件集表征和学习因果图

    Characterization and Learning of Causal Graphs with Small Conditioning Sets. (arXiv:2301.09028v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09028](http://arxiv.org/abs/2301.09028)

    本研究提出了一种使用小的条件集来表征和学习因果图的方法，用于解决约束性因果发现算法在数据有限和条件集较大时的困难。我们定义了k-马尔可夫等价的概念，该概念在不能利用所有条件独立性语句时仍然适用。

    

    约束性因果发现算法通过系统地测试数据中观察到的条件独立性来学习因果图的一部分结构。这些算法，如PC算法及其变体，依赖于由Pearl提出的所谓因果图等价类的图形表征。然而，当数据有限时，约束性因果发现算法往往面临困难，因为条件独立性检验很快失去统计能力，尤其是当条件集很大时。为了解决这个问题，我们提出使用条件独立性检验，在鲁棒的因果发现中将条件集的大小上限设置为某个整数 k。然而，现有的因果图等价类的图形表征在我们不能利用所有的条件独立性语句时不适用。我们首先定义了 k-马尔可夫等价的概念：如果两个因果图得到相同的条件独立性语句，它们是 k-马尔可夫等价的。

    Constraint-based causal discovery algorithms learn part of the causal graph structure by systematically testing conditional independences observed in the data. These algorithms, such as the PC algorithm and its variants, rely on graphical characterizations of the so-called equivalence class of causal graphs proposed by Pearl. However, constraint-based causal discovery algorithms struggle when data is limited since conditional independence tests quickly lose their statistical power, especially when the conditioning set is large. To address this, we propose using conditional independence tests where the size of the conditioning set is upper bounded by some integer $k$ for robust causal discovery. The existing graphical characterizations of the equivalence classes of causal graphs are not applicable when we cannot leverage all the conditional independence statements. We first define the notion of $k$-Markov equivalence: Two causal graphs are $k$-Markov equivalent if they entail the same c
    
[^106]: 关于奖励推断对错误人类模型的敏感性

    On the Sensitivity of Reward Inference to Misspecified Human Models. (arXiv:2212.04717v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04717](http://arxiv.org/abs/2212.04717)

    本研究从理论和实证角度研究了奖励推断对错误人类模型的敏感性，并发现存在可能构建小的对抗性偏差的情况。

    

    从人类行为中推断奖励函数是与价值对齐密切相关的内容 - 确保人工智能的目标与我们人类实际想要的一致。但这需要建立人类的行为模型。经过几十年的认知科学、神经科学和行为经济学研究，获得准确的人类模型仍然是一个开放的研究课题。这引出了一个问题：模型的准确性对于奖励推断的准确性有多重要？一方面，如果模型中存在小错误就会导致推断的灾难性错误，那么奖励学习的整个框架似乎注定失败，因为我们永远无法拥有完美的人类行为模型。另一方面，如果随着模型的改进，可以保证奖励的准确性也会提高，这将证明在模型方面做更多工作的益处。我们在理论和实证方面研究了这个问题。我们确实展示了构建小的对抗性偏差是可能的

    Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases 
    
[^107]: 离线强化学习中的置信度条件值函数

    Confidence-Conditioned Value Functions for Offline Reinforcement Learning. (arXiv:2212.04607v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04607](http://arxiv.org/abs/2212.04607)

    本文提出了一种条件值函数的学习方法，该方法在离线强化学习中处理了数据集和学习策略之间的分布偏移，并可以在训练和评估时根据保守程度动态选择策略。

    

    离线强化学习（RL）承诺能够仅使用现有的静态数据集学习有效的策略，而不需要任何昂贵的在线交互。为了做到这一点，离线RL方法必须处理数据集和学习策略之间的分布偏移。最常见的方法是学习保守或下限值函数，它们低估了超出分布的行为的回报。然而，这种方法存在一个显著缺点：在这些值函数上优化的策略只能根据固定的、可能是次优的保守程度来行为。然而，如果我们能够在训练时学习不同保守程度的策略，并设计一种方法在评估时动态选择其中之一，这个问题就可以得到缓解。为此，在这项工作中，我们提出学习附加条件的值函数，这些值函数依赖于保守程度，我们将其称为置信度条件值函数。

    Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of out-of-distribution (OOD) actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Be
    
[^108]: 用于地理空间探索的视觉主动搜索框架

    A Visual Active Search Framework for Geospatial Exploration. (arXiv:2211.15788v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.15788](http://arxiv.org/abs/2211.15788)

    本论文提出了基于视觉主动搜索框架的强化学习方法，以协助地理空间探索问题。该框架以广阔区域的图像为输入，通过有限的查询来验证目标对象示例的存在，并提供关于目标对象空间分布的信息。

    

    许多问题可以被视为利用航空影像协助的地理空间搜索的形式，例如检测盗猎活动和人口贩卖等。本论文在视觉主动搜索（VAS）框架中建立了这类问题的模型，该框架以广阔区域的图像为输入，并旨在尽可能多地识别目标对象的示例。通过一系列有限的查询，VAS会验证在给定区域内是否存在示例。VAS的一个关键特征是，每个这样的查询都会提供有关目标对象空间分布的信息，超出了可视化所捕捉的内容（例如，由于空间相关性）。我们提出了针对VAS的强化学习方法，利用完全注释的搜索任务集作为训练数据来学习搜索策略，并将输入图像的特征与主动搜索状态的自然表示相结合。此外，我们提出了域自适应技术，以改善决策时的策略。

    Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which takes as input an image of a broad area, and aims to identify as many examples of a target object as possible. It does this through a limited sequence of queries, each of which verifies whether an example is present in a given region. A crucial feature of VAS is that each such query is informative about the spatial distribution of target objects beyond what is captured visually (for example, due to spatial correlation). We propose a reinforcement learning approach for VAS that leverages a collection of fully annotated search tasks as training data to learn a search policy, and combines features of the input image with a natural representation of active search state. Additionally, we propose domain adaptation techniques to improve the policy at decis
    
[^109]: 改进卷积神经网络的输入遮罩方法研究

    Towards Improved Input Masking for Convolutional Neural Networks. (arXiv:2211.14646v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14646](http://arxiv.org/abs/2211.14646)

    本论文提出了一种改进的卷积神经网络的输入遮罩方法，通过层遮罩能够有效减少遮罩引起的缺失偏差，并消除或最小化了遮罩对模型预测的影响。

    

    对于理解和解释模型预测结果来说，从机器学习模型的输入中移除特征非常重要。然而，对于视觉模型来说，这是非常困难的，因为遮罩图像的一部分通常会引起很大的分布偏移。这是因为用于遮罩的基准颜色（通常是灰色或黑色）是处于分布之外的。此外，遮罩本身的形状可以包含不需要的信号，模型可能会利用这些信号进行预测。最近，在视觉变换器中对图像遮罩的缺失偏差问题方面已经取得了一些进展。在本研究中，我们提出了一种新的CNN遮罩方法，称之为层遮罩，可以在很大程度上减少遮罩引起的缺失偏差。直观上，层遮罩将一个遮罩应用于中间激活图，使得模型只处理没有遮罩的输入。我们展示了我们的方法（i）能够消除或最小化遮罩的影响。

    The ability to remove features from the input of machine learning models is very important to understand and interpret model predictions. However, this is non-trivial for vision models since masking out parts of the input image typically causes large distribution shifts. This is because the baseline color used for masking (typically grey or black) is out of distribution. Furthermore, the shape of the mask itself can contain unwanted signals which can be used by the model for its predictions. Recently, there has been some progress in mitigating this issue (called missingness bias) in image masking for vision transformers. In this work, we propose a new masking method for CNNs we call layer masking in which the missingness bias caused by masking is reduced to a large extent. Intuitively, layer masking applies a mask to intermediate activation maps so that the model only processes the unmasked input. We show that our method (i) is able to eliminate or minimize the influence of the mask sh
    
[^110]: 使用最小熵耦合的完美安全隐写术

    Perfectly Secure Steganography Using Minimum Entropy Coupling. (arXiv:2210.14889v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.14889](http://arxiv.org/abs/2210.14889)

    本文研究了完全安全的隐写术，并发现只有最小熵耦合过程才是最高效的，提出了高可伸缩性的完美安全隐写算法，具有实际应用价值。

    

    隐写术是将秘密信息编码到无害的内容中，以便对手无法意识到隐藏的含义的实践。本文针对隐写术的安全性进行了研究，并展示了如何使用最小熵耦合实现完全安全的隐写过程。此外，本文还证明了在所有完全安全的隐写过程中，仅仅是最小熵耦合的过程是最高效的。这些结果为实现具有实用性和高可伸缩性的完美安全保障的隐写算法提供了思路。

    Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning. While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques. In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information theoretic-model of steganography if and only if it is induced by a coupling. Furthermore, we show that, among perfectly secure procedures, a procedure is maximally efficient if and only if it is induced by a minimum entropy coupling. These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees with non-trivial efficiency; additionally, these algorithms are highly scalable. To provide empirical valid
    
[^111]: 在有向无环图上的Transformer

    Transformers over Directed Acyclic Graphs. (arXiv:2210.13148v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13148](http://arxiv.org/abs/2210.13148)

    本文研究了在有向无环图上的Transformer。通过改进注意机制和位置编码，本方法在多种任务上表现出了很好的效果。

    

    最近，Transformer模型在图表示学习中变得流行起来，因为它们有能力学习超出常规图神经网络捕捉到的复杂关系。主要的研究问题是如何将图的结构偏差注入到Transformer的架构中，并针对有向无环图（DAGs）提出了一些适应性的架构改进：（1）一个比常规Transformer的二次复杂度更高效的注意机制，同时忠实地捕捉了DAGs的结构，（2）一个对DAG的偏序进行位置编码，补充了前者。我们对我们的方法在各种类型的任务上进行了严格的评估，从对源代码图的分类到对引用网络中的节点，结果显示它在两个重要的任务上是有效的。

    Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two importan
    
[^112]: 研究异构神经网络中的神经干扰

    Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks. (arXiv:2210.12974v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12974](http://arxiv.org/abs/2210.12974)

    该论文提出了研究异构神经网络中神经干扰现象的理论分析和实验验证，并提出了一种通过自适应选择本地模型来执行预测的方法。实验结果表明，该方法在数据异质性方面更加鲁棒。

    

    将在不同位置上训练的深度学习模型融合成一个全局模型是联邦学习的直接实现。尽管当前的模型融合方法在融合几乎相同结构的神经网络时在实验上是有效的，但它们很少被进行理论分析。本文从贝叶斯观点结合来自客户端的数据异质性和神经网络的特性，揭示了神经元干扰的现象，即异构本地模型的神经元相互干扰。此外，为了验证我们的发现，我们提出了一种通过自适应选择本地模型来执行预测的实验方法，称为AMS，以排除神经元干扰并融合神经网络。实验结果表明，AMS在数据异质性方面比一般的模型融合和集成方法更加鲁棒。

    Fusing deep learning models trained on separately located clients into a global model in a one-shot communication round is a straightforward implementation of Federated Learning. Although current model fusion methods are shown experimentally valid in fusing neural networks with almost identical architectures, they are rarely theoretically analyzed. In this paper, we reveal the phenomenon of neuron disturbing, where neurons from heterogeneous local models interfere with each other mutually. We give detailed explanations from a Bayesian viewpoint combining the data heterogeneity among clients and properties of neural networks. Furthermore, to validate our findings, we propose an experimental method that excludes neuron disturbing and fuses neural networks via adaptively selecting a local model, called AMS, to execute the prediction according to the input. The experiments demonstrate that AMS is more robust in data heterogeneity than general model fusion and ensemble methods. This implies
    
[^113]: 关于临床文本挖掘的跨领域预训练语言模型：在数据受限微调中它们表现如何？

    On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12770](http://arxiv.org/abs/2210.12770)

    本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。

    

    在自然语言处理领域，使用从一般或相关领域数据预训练的大型语言模型（LLMs）来将其微调到特定领域和任务上，并使用新任务中可用的有限资源进行微调，一直以来都是一个流行的实践。在本研究中，我们重新考虑了这种假设，并在临床自然语言处理领域进行了研究，具体是在药物及其相关属性的命名实体识别任务上。我们比较了从头开始学习的Transformer模型和通过微调BERT-based LLMs（包括BERT-base、BioBERT和ClinicalBERT）进行微调的模型。我们还对这些模型及其扩展模型与带有CRF层的连续学习进行了比较。我们使用n2c2-2018共享任务数据进行模型开发和评估。实验结果表明：1）CRF层对所有神经模型都起到了积极的影响；2）在使用宏平均F1对BIO-strict跨度级别进行评估时，微调的LLMs获得了0.83+的得分，而从头开始学习的TransformerCRF模型得分为0.78+，证明了微调模型的优势。

    Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
    
[^114]: 重新思考偏见缓解：更公平的架构实现更公平的人脸识别

    Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition. (arXiv:2210.09943v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09943](http://arxiv.org/abs/2210.09943)

    通过进行公平性的神经架构搜索，我们发现偏见是神经网络架构本身固有的，而不仅仅是训练数据的影响。我们的方法在人脸识别等难题上取得了比其他方法更好的准确性和公平性。

    

    人脸识别系统广泛应用于执法等安全关键应用中，但它们在性别和种族等社会人口统计维度上存在偏见。传统观点认为，模型偏见源于有偏的训练数据。因此，以往关于偏见缓解的工作主要集中在预处理训练数据、在训练过程中添加惩罚项以防止偏见影响模型，或对预测结果进行后处理以消除偏见，但这些方法在人脸识别等难题上的成功有限。在我们的工作中，我们发现偏见实际上根源于神经网络架构本身。基于这一重新定义，我们首次进行了公平性的神经架构搜索，同时进行了超参数搜索。我们的搜索输出了一系列在准确性和公平性方面均优于其他高性能架构和现有偏见缓解方法的模型。

    Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairne
    
[^115]: ConSpec: 突出强化学习中的关键步骤，实现快速学习和泛化

    ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05845](http://arxiv.org/abs/2210.05845)

    ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。

    

    在现实生活中，成功往往取决于多个关键步骤，这些步骤在时间上相距较远，与最终奖励也相距甚远。传统的强化学习方法在信用分配方面依赖Bellman方程，很难识别这些关键步骤。本文提出了一种新的强化学习算法，使用离线对比学习来确定关键步骤。这个算法被称为对比内省（ConSpec），可以添加到任何现有的强化学习算法中。ConSpec通过一种新颖的对比损失学习任务中的关键步骤的原型，并在当前状态与这些原型之一匹配时提供内在奖励。ConSpec中的原型在信用分配方面具有两个关键优势：（1）它们使得能够迅速识别所有关键步骤；（2）它们以容易解释的方式实现这一点，使得在感觉特征改变时可以进行超出分布的泛化。与其他当代的强化学习方法不同，

    In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
    
[^116]: 使用基于图的策略学习的通用学习框架进行开放式即席团队合作

    A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning. (arXiv:2210.05448v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2210.05448](http://arxiv.org/abs/2210.05448)

    本文提出了一个通用的学习框架，用于解决开放式即席团队合作问题，通过基于图的策略学习获得最优策略，并且能够适应不完全可观测情况下的团队合作。

    

    开放式即席团队合作是指训练一个单一代理与一个未知的可能随时间变化的团队高效合作的问题。对于代理来说，可变的团队组成带来了挑战，如需要适应新的团队动态和处理不断变化的状态向量大小。这些挑战在受控环境仅具有部分视图的真实应用中更加严峻。在这项工作中，我们针对完全和部分可观测性开发了一类解决方案。我们首先针对完全可观测的情况开发了一种解决方案，该解决方案利用图神经网络架构基于强化学习获得最优策略。然后，我们通过提出不同的方法来将这种解决方案扩展到部分可观测的场景中，这些方法可以在潜在的环境状态和团队组成上保持信念估计。这些信念估计与我们对完全可观测情况的解决方案相结合。

    Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully 
    
[^117]: SimSCOOD: Fine-tuned源代码模型的超分布泛化的系统分析

    SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models. (arXiv:2210.04802v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.04802](http://arxiv.org/abs/2210.04802)

    本文是第一个系统研究超分布泛化问题的方法，通过模拟不同维度的源代码数据属性和微调方法，在不同场景中研究了模型的行为。

    

    大型代码数据集已经越来越容易地用于预训练源代码模型。然而，对于微调阶段来说，获取代表性的训练数据以充分覆盖特定下游任务的代码分布仍然具有挑战性，原因是任务特定性和有限的标注资源。此外，微调预训练模型可能会导致遗忘以前获得的预训练知识。这些问题导致了超分布泛化问题，即模型的推理行为出现意外情况，这尚未进行系统研究。在本文中，我们提出了第一个系统方法，模拟了不同维度源代码数据属性的各种超分布场景，并研究了这些场景中微调模型的行为。我们研究了不同微调方法（包括全微调和低秩适应微调方法）下模型的行为。我们在各个系统上进行了全面分析。

    Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on fo
    
[^118]: 所有步骤都同等重要吗？基于事件的重要性检测的基准测试。

    Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04074](http://arxiv.org/abs/2210.04074)

    本文研究了当前模型在理解与目标事件有关的步骤事件的重要性方面的困难，并贡献了一个由专家手动注释步骤重要性的语料库。

    

    自然语言以不同的细粒度表达事件，其中粗粒度事件（目标）可以细分为更细粒度的事件序列（步骤）。理解事件过程的一个关键但常被忽视的方面是认识到并非所有步骤事件对于完成目标具有相同的重要性。本文通过研究当前模型理解与目标事件有关的步骤事件的重要性的程度来填补这一空白。认知研究表明，这种能力使机器能够模拟人类对日常任务的先决条件和必要努力的常识推理。我们贡献了一个优质的语料库（目标，步骤）对，该语料库从社区指南网站WikiHow收集，通过专家手动注释步骤的重要性。高一致性的标注者间一致性表明人类对事件重要性具有一致的理解。然而，在评估多个统计模型之后，我们发现当前的模型在理解步骤的重要性方面还存在困难。

    Natural language expresses events with varying granularities, where coarse-grained events (goals) can be broken down into finer-grained event sequences (steps). A critical yet overlooked aspect of understanding event processes is recognizing that not all step events hold equal importance toward the completion of a goal. In this paper, we address this gap by examining the extent to which current models comprehend the essentiality of step events in relation to a goal event. Cognitive studies suggest that such capability enables machines to emulate human commonsense reasoning about preconditions and necessary efforts of everyday tasks. We contribute a high-quality corpus of (goal, step) pairs gathered from the community guideline website WikiHow, with steps manually annotated for their essentiality concerning the goal by experts. The high inter-annotator agreement demonstrates that humans possess a consistent understanding of event essentiality. However, after evaluating multiple statisti
    
[^119]: Learnware: 小模型实现大作为

    Learnware: Small Models Do Big. (arXiv:2210.03647v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03647](http://arxiv.org/abs/2210.03647)

    Learnware范式旨在帮助用户充分利用小型模型，实现超越原始目的的事情，以解决当前机器学习技术面临的数据量大、训练难度高、数据安全等问题。

    

    当前机器学习技术存在训练数据量大、训练技能高、连续学习难、遗忘风险大、数据隐私和专有信息泄露等问题，而过去的大模型范式虽然在自然语言处理和计算机视觉应用中取得了惊人的结果，但并未解决这些问题，反而成为严重的碳排放源。该文概述了Learnware范式，让用户不需要从头构建机器学习模型，希望利用小型模型可以实现超越原始目的的事情，其中关键是规范，可以使训练的模型得到充分鉴别。

    There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identi
    
[^120]: 通过隐变量高斯分布的双曲 VAE

    Hyperbolic VAE via Latent Gaussian Distributions. (arXiv:2209.15217v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15217](http://arxiv.org/abs/2209.15217)

    这项研究提出了一种通过使用高斯流形的潜空间来改进变分自编码器(VAE)的方法。实验证明，这种GM-VAE方法在密度估计和模型驱动的强化学习任务中表现出色，具有较强的数值稳定性。

    

    我们提出了一种高斯流形变分自编码器(GM-VAE)，其潜空间由一组高斯分布组成。已知一维高斯分布集合在 Fisher 信息度量下形成了一个双曲空间，我们称之为高斯流形。为了学习具有高斯流形的 VAE，我们提出了基于 Kullback-Leibler 散度的伪高斯流形正态分布，它是对平方 Fisher-Rao 距离的局部近似，用于定义潜空间上的密度。在实验中，我们展示了 GM-VAE 在两个不同任务上的有效性：图像数据集密度估计和模型驱动的强化学习中的环境建模。GM-VAE 在密度估计任务上超越了其他双曲和欧几里得 VAE 的变体，并在模型驱动的强化学习中展现了竞争性的性能。我们观察到我们的模型提供了强大的数值稳定性，解决了一个常见的限制问题。

    We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of Gaussian distributions. It is known that the set of the univariate Gaussian distributions with the Fisher information metric form a hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. In experiments, we demonstrate the efficacy of GM-VAE on two different tasks: density estimation of image datasets and environment modeling in model-based reinforcement learning. GM-VAE outperforms the other variants of hyperbolicand Euclidean-VAEs on density estimation tasks and shows competitive performance in model-based reinforcement learning. We observe that our model provides strong numerical stability, addressing a common limitation reported 
    
[^121]: 语言模型显示对推理任务具有类似人类的内容效应

    Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.07051](http://arxiv.org/abs/2207.07051)

    本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。

    

    抽象推理是智能系统的关键能力。大型语言模型在抽象推理任务上实现了高于随机的性能，但存在许多不完善之处。然而，人类的抽象推理也是不完美的。例如，人类推理受到我们对真实世界的知识和信念的影响，并表现出显著的“内容效应”；当问题的语义内容支持正确的逻辑推理时，人类更可靠地进行推理。这些内容纠缠的推理模式在关于人类智能基本性质的争论中起着核心作用。在这里，我们研究了语言模型是否以类似的方式混入内容来回答逻辑问题，这些语言模型的先验期望捕捉了一些人类知识的特征。我们在三个逻辑推理任务上探索了这个问题：自然语言推理、判断三段论的逻辑有效性和Wason选择任务。我们评估了最先进的大型语言模型的性能。

    Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
    
[^122]: 不确定性量化中的标记是什么？潜在密度模型用于不确定性分类

    What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization. (arXiv:2207.05161v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05161](http://arxiv.org/abs/2207.05161)

    本论文提出了一种分类框架，用于对不确定性量化方法标记的不确定示例进行分类。通过引入混淆密度矩阵，并将示例分为分布外、边界和高分布误分类区域的三类，本研究为评估不同的不确定性量化方法提供了新的视角和评估基准。

    

    不确定性量化（UQ）对于创建可信赖的机器学习模型至关重要。近年来，出现了许多能够标记可疑样本的UQ方法，然而，这些方法究竟识别了什么内容往往不清楚。在这项工作中，我们提出了一个框架，用于对分类任务中被UQ方法标记为不确定的示例进行分类。我们引入了混淆密度矩阵——基于核的误分类密度的近似，并将其用于将被给定不确定性方法识别为可疑样本的示例分为三类：分布外（OOD）样本、边界（Bnd）样本和处于高分布误分类区域（IDM）的示例。通过大量实验，我们证明了我们的框架为评估不同不确定性量化方法之间的差异提供了一种新的独特视角，从而形成了一个有价值的评估基准。

    Uncertainty Quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods in classification tasks. We introduce the confusion density matrix -- a kernel-based approximation of the misclassification density -- and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.
    
[^123]: 机器人学习中的公平性和偏见

    Fairness and Bias in Robot Learning. (arXiv:2207.03444v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.03444](http://arxiv.org/abs/2207.03444)

    本文从技术、伦理和法律角度出发，首次调查了机器人学习中的公平性问题。提出了偏见来源的分类法和由其引起的歧视类型，并探讨了不同机器人学习领域中不公平结果的场景和缓解策略。

    

    机器学习显著增强了机器人的能力，使它们能够在人类环境中执行各种任务并适应我们不确定的真实世界。最近各个机器学习领域的研究强调了考虑公平性的重要性，以确保这些算法不会重复人类的偏见，并因此导致具有歧视性的结果。随着机器人学习系统在我们日常生活中执行越来越多的任务，了解这种偏见的影响以防止对某些人群产生意外行为至关重要。在这项工作中，我们从技术、伦理和法律挑战的跨学科角度提出了机器人学习中公平性的首次调查。我们提出了偏见来源的分类法和由它们引起的歧视类型。通过不同机器人学习领域的示例，我们探讨了不公平结果的场景和缓解策略。

    Machine learning has significantly enhanced the abilities of robots, enabling them to perform a wide range of tasks in human environments and adapt to our uncertain real world. Recent works in various machine learning domains have highlighted the importance of accounting for fairness to ensure that these algorithms do not reproduce human biases and consequently lead to discriminatory outcomes. With robot learning systems increasingly performing more and more tasks in our everyday lives, it is crucial to understand the influence of such biases to prevent unintended behavior toward certain groups of people. In this work, we present the first survey on fairness in robot learning from an interdisciplinary perspective spanning technical, ethical, and legal challenges. We propose a taxonomy for sources of bias and the resulting types of discrimination due to them. Using examples from different robot learning domains, we examine scenarios of unfair outcomes and strategies to mitigate them. We
    
[^124]: 半监督对比异常值剔除的伪最大期望化方法（SCOPE）

    Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). (arXiv:2206.14261v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14261](http://arxiv.org/abs/2206.14261)

    SCOPE 提出了一种名为半监督对比异常值剔除的伪最大期望化方法，通过抑制混淆错误来提高半监督学习的性能。

    

    半监督学习是将少量标记数据集与较大的未标记数据集相结合，训练出准确的预测模型的问题。已经开发了许多半监督深度学习方法，包括伪标签、一致性正则化和对比学习技术。然而，伪标签方法容易受到混淆的影响，在早期迭代中，错误的伪标签被认为是真实标签，导致模型加强其先前的偏见，进而无法很好地推广到强大的预测性能。我们提出了一种新方法，通过一个我们称之为半监督对比异常值剔除的伪最大期望化方法（SCOPE）来抑制混淆错误。像基本的伪标签方法一样，SCOPE与最大期望化（EM）相关，EM是一种潜变量框架，可以扩展到理解聚类假设的深度半监督算法。但是，与基本的方法不同。

    Semi-supervised learning is the problem of training an accurate predictive model by combining a small labeled dataset with a presumably much larger unlabeled dataset. Many methods for semi-supervised deep learning have been developed, including pseudolabeling, consistency regularization, and contrastive learning techniques. Pseudolabeling methods however are highly susceptible to confounding, in which erroneous pseudolabels are assumed to be true labels in early iterations, thereby causing the model to reinforce its prior biases and thereby fail to generalize to strong predictive performance. We present a new approach to suppress confounding errors through a method we describe as Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). Like basic pseudolabeling, SCOPE is related to Expectation Maximization (EM), a latent variable framework which can be extended toward understanding cluster-assumption deep semi-supervised algorithms. However, unlike basic
    
[^125]: 《不确定性下的交互式视觉推理》

    Interactive Visual Reasoning under Uncertainty. (arXiv:2206.09203v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.09203](http://arxiv.org/abs/2206.09203)

    该论文介绍了一个名为IVRE的交互式环境，在不确定性下评估人工智能代理的推理能力。研究人员通过在IVRE中设置不确定的动作-效果对，要求代理确定对象的角色，并鼓励代理基于观察提出有效且高效的实验来验证假设。

    

    人类的基本认知能力之一是通过生成假设并通过积极试验来迅速解决不确定性。当遇到伴随着模糊的因果关系的新现象时，人类对数据提出假设，通过观察进行推理，通过实验来测试他们的理论，并在不一致出现时修正命题。这些迭代过程持续到底层机制变得清晰为止。在这项工作中，我们设计了一个名为IVRE（读作"ivory"）的环境，用于评估人工智能代理在不确定性下的推理能力。IVRE是一个交互式环境，围绕Blicket检测的丰富场景展开。IVRE中的代理被放置在具有各种模糊的动作-效果对的环境中，并被要求确定每个对象的角色。他们被鼓励基于观察提出有效且高效的实验来验证他们的假设，并积极收集新的信息。

    One of the fundamental cognitive abilities of humans is to quickly resolve uncertainty by generating hypotheses and testing them via active trials. Encountering a novel phenomenon accompanied by ambiguous cause-effect relationships, humans make hypotheses against data, conduct inferences from observation, test their theory via experimentation, and correct the proposition if inconsistency arises. These iterative processes persist until the underlying mechanism becomes clear. In this work, we devise the IVRE (pronounced as "ivory") environment for evaluating artificial agents' reasoning ability under uncertainty. IVRE is an interactive environment featuring rich scenarios centered around Blicket detection. Agents in IVRE are placed into environments with various ambiguous action-effect pairs and asked to determine each object's role. They are encouraged to propose effective and efficient experiments to validate their hypotheses based on observations and actively gather new information. T
    
[^126]: 评估和诱导预训练语言模型的个性

    Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.07550](http://arxiv.org/abs/2206.07550)

    本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。

    

    个性起源于哲学探索，关注个体在思考、情感和行为方面的差异。为了构建能够与人类日常合作的社交机器，我们想知道：现有的大型语言模型(LLMs)是否拥有与人类类似的个性？如果是，我们如何评估它们？进一步地，在此评估框架的基础上，如何以可控的方式诱导具有特定个性的语言模型？为回答这些问题，我们提出了机器个性库(Machine Personality Inventory, MPI)数据集，用于评估机器的个性。MPI遵循标准化的个性测试，基于五因素人格理论和人格评估库建立。通过用MPI系统地评估LLM，我们提供了第一个证据，证明了LLM的个性。我们进一步设计了一种个性提示(Personality Prompting, P^2)方法，以可控的方式诱导LLMs具有特定的个性。

    Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
    
[^127]: 通过可微分物理进行复杂运动技能学习

    Complex Locomotion Skill Learning via Differentiable Physics. (arXiv:2206.02341v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.02341](http://arxiv.org/abs/2206.02341)

    本研究提出了一个实用的学习框架，通过可微分物理实现了统一的神经网络控制器，使其具备更高复杂性和多样性。与强化学习相比，该框架的学习效果更好且收敛速度更快。

    

    可微分物理能够通过有效的基于梯度的优化来获得神经网络控制器。然而，现有的工作通常只能提供具有有限能力和泛化能力的神经网络控制器。我们提出了一个实用的学习框架，可以输出统一的具有更高复杂性和多样性的神经网络控制器。为了系统地提高训练的鲁棒性和效率，我们对基准方法进行了一系列改进，包括周期激活函数和定制损失函数。此外，我们发现批处理和Adam优化器在训练复杂运动任务时非常有效。我们在可微分的质点弹簧和材料点法（MPM）模拟中评估了我们的框架，进行了具有挑战性的运动任务和多个机器人设计。实验表明，基于可微分物理的学习框架比强化学习具有更好的结果，并且收敛速度更快。

    Differentiable physics enables efficient gradient-based optimizations of neural network (NN) controllers. However, existing work typically only delivers NN controllers with limited capability and generalizability. We present a practical learning framework that outputs unified NN controllers capable of tasks with significantly improved complexity and diversity. To systematically improve training robustness and efficiency, we investigated a suite of improvements over the baseline approach, including periodic activation functions, and tailored loss functions. In addition, we find our adoption of batching and an Adam optimizer effective in training complex locomotion tasks. We evaluate our framework on differentiable mass-spring and material point method (MPM) simulations, with challenging locomotion tasks and multiple robot designs. Experiments show that our learning framework, based on differentiable physics, delivers better results than reinforcement learning and converges much faster. 
    
[^128]: 数据有效的 GAN 训练中的自我监督增强技术

    Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15677](http://arxiv.org/abs/2205.15677)

    本文提出一种增强感知的自监督判别器用于对生成数据及其增强参数的预测，从而提高判别器的表现和生成模型的性能，实现了数据有效的 GAN 训练。

    

    有限数据情况下训练生成式对抗网络（GAN）是具有挑战性的，因为判别器容易过拟合。先前提出的可微增强技术改善了GAN训练的数据效率。但是，增强技术隐式地引入了不良不变性因素，因为它忽略了由数据转换引起的标签空间语义变化，这可能限制了判别器的表示学习能力，并最终影响生成模型的表现。为了减轻不变性的负面影响，同时继承数据增强的好处，我们提出了一种新的增强感知的自监督判别器，该判别器可以预测增强数据的参数。特别地，真实数据和生成数据的预测目标在训练过程中需要区别开来。我们还鼓励生成器对抗地生成其增强参数可以被判别器准确预测的数据，从而获得更多信息量和更高效的判别器，提高生成模型的性能。多个数据集上的实验表明，我们的方法在数据有效的 GAN 训练中实现了最先进的性能。

    Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
    
[^129]: 处理学习因果Transformer用于噪声图像分类

    Treatment Learning Causal Transformer for Noisy Image Classification. (arXiv:2203.15529v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.15529](http://arxiv.org/abs/2203.15529)

    本研究提出了一种处理学习因果Transformer（TLT）架构，用于噪声图像分类。通过将"存在噪声"的信息作为处理输入，并联合估计处理效果，TLT能够提高预测准确性，并使用潜在生成模型估计鲁棒的特征表示。

    

    当前顶级的基于深度学习（DL）的视觉模型主要基于探索和利用训练数据样本和其关联标签之间的内在相关性。然而，已知的一个实际挑战是它们在"噪声"数据对抗下性能下降，噪声数据由于不同情况引起，比如虚假相关性、无关背景、领域转移和对抗攻击。在这项工作中，我们将"存在噪声"的二进制信息作为处理输入到图像分类任务中，通过联合估计处理效果来提高预测准确性。受因果变分推断的启发，我们提出了一种基于Transformer的架构，称为处理学习因果Transformer（TLT），它使用潜在的生成模型从当前观测输入中估计鲁棒的特征表示以进行噪声图像分类。根据估计的噪声水平（建模为二进制处理因子），TLT分配相应的推断

    Current top-notch deep learning (DL) based vision models are primarily based on exploring and exploiting the inherent correlations between training data samples and their associated labels. However, a known practical challenge is their degraded performance against "noisy" data, induced by different circumstances such as spurious correlations, irrelevant contexts, domain shift, and adversarial attacks. In this work, we incorporate this binary information of "existence of noise" as treatment into image classification tasks to improve prediction accuracy by jointly estimating their treatment effects. Motivated from causal variational inference, we propose a transformer-based architecture, Treatment Learning Causal Transformer (TLT), that uses a latent generative model to estimate robust feature representations from current observational input for noise image classification. Depending on the estimated noise level (modeled as a binary treatment factor), TLT assigns the corresponding inferen
    
[^130]: 用度量学习增强的最优传输改进分子表示学习

    Improving Molecular Representation Learning with Metric Learning-enhanced Optimal Transport. (arXiv:2202.06208v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06208](http://arxiv.org/abs/2202.06208)

    本文提出了一种基于最优传输的算法MROT，用于改进分子表示学习和增强其泛化能力。实验证明，MROT在化学性质预测和材料吸附选择等任务中显著优于现有模型，具有加速发现具有期望性质的新物质的潜力。

    

    在许多化学和生物应用中，训练数据通常有限或异构。现有的化学和材料科学的机器学习模型未能考虑训练领域之外的泛化能力。在本文中，我们开发了一种基于最优传输的新算法MROT，用于增强分子回归问题的泛化能力。MROT通过测量一种新的领域距离度量和传输计划上的后验方差正则化，来学习数据的连续标签，以弥合化学领域的差距。在下游任务中，我们考虑了无监督和半监督设置中的基本化学回归任务，包括化学性质预测和材料吸附选择。大量实验表明，MROT显著优于现有模型，在加速发现具有期望性质的新物质方面具有潜在的潜力。

    Training data are usually limited or heterogeneous in many chemical and biological applications. Existing machine learning models for chemistry and materials science fail to consider generalizing beyond training domains. In this article, we develop a novel optimal transport-based algorithm termed MROT to enhance their generalization capability for molecular regression problems. MROT learns a continuous label of the data by measuring a new metric of domain distances and a posterior variance regularization over the transport plan to bridge the chemical domain gap. Among downstream tasks, we consider basic chemical regression tasks in unsupervised and semi-supervised settings, including chemical property prediction and materials adsorption selection. Extensive experiments show that MROT significantly outperforms state-of-the-art models, showing promising potential in accelerating the discovery of new substances with desired properties.
    
[^131]: CubeTR: 使用Transformer学习解决魔方问题

    CubeTR: Learning to Solve The Rubiks Cube Using Transformers. (arXiv:2111.06036v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06036](http://arxiv.org/abs/2111.06036)

    CubeTR提出了一种使用Transformer进行强化学习解决魔方问题的方法，并通过关注长序列动作和解决稀疏奖励的问题，实现了从任意起始状态学习如何解决魔方问题，并能够生成接近专业人员解决方案长度的解决方案。

    

    自从Transformer首次出现以来，已经成功地应用于计算机视觉到自然语言处理等各种领域。最近提出将Transformer应用于强化学习，将其重新定义为序列建模问题。与其他常见的强化学习问题相比，魔方问题具有独特的挑战。魔方有着数以千万计的可能组合，但只有一个解决的状态，这导致了极度稀疏的奖励信号。提出的模型CubeTR关注于长序列的动作，并解决了稀疏奖励的问题。CubeTR能够从任意起始状态学习如何解决魔方问题，经过移动规范化后，生成的解决方案长度预期将非常接近专业人员使用的算法给出的解决方案长度。CubeTR为学习算法在更高维度魔方上的泛化能力提供了深入洞见。

    Since its first appearance, transformers have been successfully used in wide ranging domains from computer vision to natural language processing. Application of transformers in Reinforcement Learning by reformulating it as a sequence modelling problem was proposed only recently. Compared to other commonly explored reinforcement learning problems, the Rubiks cube poses a unique set of challenges. The Rubiks cube has a single solved state for quintillions of possible configurations which leads to extremely sparse rewards. The proposed model CubeTR attends to longer sequences of actions and addresses the problem of sparse rewards. CubeTR learns how to solve the Rubiks cube from arbitrary starting states without any human prior, and after move regularisation, the lengths of solutions generated by it are expected to be very close to those given by algorithms used by expert human solvers. CubeTR provides insights to the generalisability of learning algorithms to higher dimensional cubes and 
    
[^132]: 基于相似性映射的神经模型重编程用于低资源口语命令分类

    Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2110.03894](http://arxiv.org/abs/2110.03894)

    本文提出了一种基于相似性映射的神经模型重编程方法，用于低资源口语命令分类。实验证明，在有限的数据条件下，该方法在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果。

    

    本研究提出了一种新颖的对抗重编程（AR）方法，用于低资源口语命令识别（SCR），并构建了一个AR-SCR系统。AR过程旨在修改来自目标领域的声学信号，以重新调整预训练的SCR模型，从而实现重编程。为了解决源域和目标域之间的标签不匹配问题，并进一步提高AR的稳定性，我们提出了一种新颖的基于相似性的标签映射技术来对齐类别。此外，将迁移学习（TL）技术与原始AR过程相结合，以提高模型的适应能力。我们在三个低资源SCR数据集上评估了提出的AR-SCR系统，包括阿拉伯语、立陶宛语和言语障碍性普通话。实验结果表明，在大规模英语数据集上训练的预训练AM的基础上，提出的AR-SCR系统在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果，且仅使用有限数量的数据。

    In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of 
    
[^133]: 知识增强预训练模型综述

    A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.00269](http://arxiv.org/abs/2110.00269)

    本综述提供了关于NLP中知识增强预训练语言模型的综合概述，讨论了预训练语言模型和知识表示学习的进展，并从三个不同的角度对现有的KEPLMs进行了分类，最后概述了未来研究中KEPLMs的潜在方向。

    

    预训练语言模型通过自监督学习在大规模文本语料库上学习了信息丰富的词表示，在细调之后在自然语言处理领域取得了有希望的性能。然而，这些模型存在鲁棒性差和可解释性不足的问题。我们将注入知识的预训练语言模型称为知识增强预训练语言模型(KEPLMs)。这些模型表现出深入理解和逻辑推理，并引入了可解释性。在本综述中，我们提供了关于NLP中KEPLMs的综合概述。我们首先讨论了预训练语言模型和知识表示学习的进展。然后，我们从三个不同的角度系统地分类了现有的KEPLMs。最后，我们概述了一些未来研究中KEPLMs的潜在方向。

    Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.
    
[^134]: FUTURE-AI:医学影像中值得信赖的人工智能的指导原则和共识建议

    FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.09658](http://arxiv.org/abs/2109.09658)

    本论文介绍了一系列从经验、共识和最佳实践中提炼出的指导原则，旨在引领医学影像中值得信赖的人工智能的发展，提高信任、安全性和应用水平。

    

    近年来，人工智能和临床系统生成的大量数据的结合，推动了医学影像领域整个价值链上的成像人工智能解决方案的发展，包括图像重建、医学图像分割、基于图像的诊断和治疗规划。尽管医学影像中的人工智能取得了成功并有着巨大的潜力，但许多利益相关者担心成像人工智能解决方案的潜在风险和伦理问题，认为其复杂、不透明、难以理解、难以应用和难以在关键临床应用中建立信任。尽管存在这些担忧和风险，但目前尚没有具体的指导原则和最佳实践来引导未来医学影像中人工智能的发展以增加信任、安全性和采用。为了弥补这一空白，本文提出了从积累的经验、共识和最佳实践中精选出的指导原则。

    The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices f
    
[^135]: 论连续优化下基于熵损失函数在学习因果结构中的作用

    On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.02835](http://arxiv.org/abs/2106.02835)

    发现因果结构的任务是重要且具有挑战性的，在连续优化中使用的最小二乘损失函数受限于高斯噪声假设。在本研究中，我们提出了一种理论上与任何噪声分布一致的基于熵的损失函数来克服这一限制。

    

    因果发现是许多科学领域中重要且具有挑战性的任务。最近，一种名为NOTEARS的非组合有向无环约束方法将因果结构学习问题转化为使用最小二乘损失的连续优化问题。虽然最小二乘损失函数在标准高斯噪声假设下是合理的，但如果该假设不成立，它会受到限制。在这项工作中，我们从理论上证明了高斯噪声假设的违反将阻碍因果方向的识别，使因果方向完全由因果强度以及线性情况下噪声方差和非线性情况下强非高斯噪声确定。因此，我们提出了一种更一般的基于熵的损失函数，在任何噪声分布下，在理论上与似然分数一致。我们对合成数据和真实世界数据进行了大量的实证评估。

    Causal discovery from observational data is an important but challenging task in many scientific fields. Recently, a method with non-combinatorial directed acyclic constraint, called NOTEARS, formulates the causal structure learning problem as a continuous optimization problem using least-square loss. Though the least-square loss function is well justified under the standard Gaussian noise assumption, it is limited if the assumption does not hold. In this work, we theoretically show that the violation of the Gaussian noise assumption will hinder the causal direction identification, making the causal orientation fully determined by the causal strength as well as the variances of noises in the linear case and by the strong non-Gaussian noises in the nonlinear case. Consequently, we propose a more general entropy-based loss that is theoretically consistent with the likelihood score under any noise distribution. We run extensive empirical evaluations on both synthetic data and real-world d
    
[^136]: 在小数据集上学习的图像分类中进化参数化损失函数的方法

    Evolving parametrized Loss for Image Classification Learning on Small Datasets. (arXiv:2103.08249v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2103.08249](http://arxiv.org/abs/2103.08249)

    本文提出了一种在小数据集上进行图像分类学习的方法，通过元学习的方式进化参数化损失函数，实验结果表明该方法能够有效地提高分类性能。

    

    本文提出了一种元学习方法来进化参数化损失函数，称为元损失网络（MLN），用于在小数据集上进行图像分类学习。在我们的方法中，MLN被嵌入到分类学习的框架中作为一个可微的目标函数。通过用进化策略算法（ES）来优化损失函数演变MLN，使得通过最小化该损失优化的分类器能够达到很好的泛化效果。分类器在小训练数据集上使用随机梯度下降（SGD）来最小化MLN，然后MLN根据小数据集更新的分类器在大验证数据集上进行演化。为了评估我们的方法，使用FashionMNIST采样了大量小样本学习任务来训练MLN，并在FashionMNIST和CIFAR10采样的验证任务上进行测试。实验结果表明MLN能够有效地改善分类性能。

    This paper proposes a meta-learning approach to evolving a parametrized loss function, which is called Meta-Loss Network (MLN), for training the image classification learning on small datasets. In our approach, the MLN is embedded in the framework of classification learning as a differentiable objective function. The MLN is evolved with the Evolutionary Strategy algorithm (ES) to an optimized loss function, such that a classifier, which optimized to minimize this loss, will achieve a good generalization effect. A classifier learns on a small training dataset to minimize MLN with Stochastic Gradient Descent (SGD), and then the MLN is evolved with the precision of the small-dataset-updated classifier on a large validation dataset. In order to evaluate our approach, the MLN is trained with a large number of small sample learning tasks sampled from FashionMNIST and tested on validation tasks sampled from FashionMNIST and CIFAR10. Experiment results demonstrate that the MLN effectively impr
    
[^137]: 持续发展的神经仿真：基于具身计算代理的方法

    Continual Developmental Neurosimulation Using Embodied Computational Agents. (arXiv:2103.05753v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2103.05753](http://arxiv.org/abs/2103.05753)

    通过使用发育Braitenberg Vehicles代理，我们提出了一种具有发育启发的学习代理设计，实现了计算自主性的具身经验和形态发生增长模拟，并考虑了发育轨迹在神经系统形态生成、发育学习和可塑性等方面的作用。

    

    通过综合发育生物学、认知科学和计算建模，我们可以获得很多知识。我们的研究目标是基于Braitenberg Vehicles设计开发受发育启发的学习代理。利用这些代理体现了计算自主性的具身特性，不断靠近对具身经验和形态发生增长作为认知发展能力组成部分的建模。我们考虑生物和认知发展对成年表型生成和可用发展路径的影响。持续发展神经仿真使我们能够考虑发育轨迹在连接神经系统形态发生、发育学习和可塑性等相关现象中所起的作用。由于与持续学习紧密相关，我们的方法与发育具身紧密集成，可以使用一种称为发育Braitenberg Vehicles (dBVs)的代理来实现。

    There is much to learn through synthesis of Developmental Biology, Cognitive Science and Computational Modeling. Our path forward is to present a design for developmentally-inspired learning agents based on Braitenberg Vehicles. Using these agents to exemplify the embodied nature of computational autonomy, we move closer to modeling embodied experience and morphogenetic growth as components of cognitive developmental capacity. We consider biological and cognitive development which influence the generation of adult phenotypes and the contingency of available developmental pathways. Continual developmental neurosimulation allows us to consider the role of developmental trajectories in bridging the related phenomena of nervous system morphogenesis, developmental learning, and plasticity. Being closely tied to continual learning, our approach is tightly integrated with developmental embodiment, and can be implemented using a type of agent called developmental Braitenberg Vehicles (dBVs). T
    
[^138]: 高效学习使用预训练神经网络进行鲁棒四足跳跃的控制策略

    Efficient Learning of Control Policies for Robust Quadruped Bounding using Pretrained Neural Networks. (arXiv:2011.00446v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2011.00446](http://arxiv.org/abs/2011.00446)

    本文提出了一种高效学习鲁棒四足跳跃控制策略的方法，通过使用预训练的神经网络和深度强化学习，考虑接触点和阶段的奖励函数以提升跳跃性能，并成功地应用于真实的四足机器人上。

    

    跳跃是四足动物在克服障碍物时非常重要的步态之一。作者提出了一种有效的方法，可以更高效地学习鲁棒的跳跃步态，尽管其动态身体运动变化较大。作者首先利用传统的基于模型的控制器从机器人的数据中预训练神经网络，然后通过深度强化学习进一步优化预训练的神经网络。特别地，作者设计了一个考虑接触点和阶段的奖励函数，以强制跳跃步态的对称性和周期性，从而改善跳跃性能。基于神经网络的反馈控制器在仿真中学习，并成功地应用于真实的四足机器人Jueying Mini上。作者的方法在室内外环境中展示了多样化的应用。该方法表现出高效的计算和Jueying Mini四足机器人跳跃超越障碍物的良好行动结果。

    Bounding is one of the important gaits in quadrupedal locomotion for negotiating obstacles. The authors proposed an effective approach that can learn robust bounding gaits more efficiently despite its large variation in dynamic body movements. The authors first pretrained the neural network (NN) based on data from a robot operated by conventional model based controllers, and then further optimised the pretrained NN via deep reinforcement learning (DRL). In particular, the authors designed a reward function considering contact points and phases to enforce the gait symmetry and periodicity, which improved the bounding performance. The NN based feedback controller was learned in the simulation and directly deployed on the real quadruped robot Jueying Mini successfully. A variety of environments are presented both indoors and outdoors with the authors approach. The authors approach shows efficient computing and good locomotion results by the Jueying Mini quadrupedal robot bounding over une
    

